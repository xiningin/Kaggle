{"cell_type":{"8106e116":"code","a35ae041":"code","e6da7d1a":"code","748faaba":"code","1346ffef":"code","b73b5fb8":"code","4411cd55":"code","7ee4f914":"code","ec345cf1":"code","b813d262":"code","708c98b2":"code","10f53335":"code","41d2d3b7":"code","52140a0c":"code","9b9fe73c":"code","926fcd60":"code","451796b6":"code","d4153ce0":"code","c8031e06":"code","cad68d5a":"code","8f28149c":"code","6057604c":"code","73f08d49":"code","0ea6548b":"code","e9912200":"code","174dbfa8":"code","0c6268c3":"code","63ef102f":"code","64a0b9b8":"code","025be342":"code","41535091":"code","bc52116d":"code","0aa377a0":"code","14eb3ff8":"code","9f4901e0":"code","3ec88cdd":"code","22da13d4":"code","d1cc97c8":"code","eb38bba4":"code","d576ac30":"code","246890d7":"code","c9ca8b31":"code","ee4312ca":"code","5540e759":"code","64b3bf77":"code","87c03019":"code","437c85b1":"code","7ed3ddaf":"code","2abd2d3c":"code","609b6bfb":"code","a7d13bcd":"code","a1f231af":"code","b37ff105":"code","637b57ef":"code","4f471eac":"code","e953951b":"code","d4c62c13":"code","de201519":"code","f9ebf1e3":"markdown","0f9486fe":"markdown","e0cf85bd":"markdown","9eb06452":"markdown","0f085db4":"markdown","24a855d6":"markdown","d54c11b2":"markdown","5a31cdde":"markdown","42225ba1":"markdown","c5c43766":"markdown","17323280":"markdown","7a3c1fa8":"markdown","c425cc9c":"markdown","f4d5f555":"markdown","0cc47086":"markdown","3bd9f735":"markdown","acc75be3":"markdown","29935211":"markdown","d200a7c5":"markdown","2dc2f36d":"markdown","9a04e28b":"markdown","f4f08b54":"markdown","90f8e269":"markdown","300457e6":"markdown","14c89565":"markdown","8d96f787":"markdown","a973b664":"markdown","8787932a":"markdown","e5e0d12f":"markdown","1eab5888":"markdown","35e9bf0b":"markdown","5bb17031":"markdown","6e9257c5":"markdown","1cfbe87a":"markdown","4c17b1a3":"markdown","7eef9f22":"markdown","24eaf893":"markdown","a81a67c1":"markdown","a5dd201e":"markdown","5f16ec15":"markdown","04443c9a":"markdown","12fbe2be":"markdown","d8812ed2":"markdown"},"source":{"8106e116":"# to handle datasets\nimport pandas as pd\nimport numpy as np\n\n# for plotting\nimport matplotlib.pyplot as plt\n#% matplotlib inline\n\n# to display all the columns of the dataframe in the notebook\npd.pandas.set_option('display.max_columns', None)","a35ae041":"import os\nprint(os.listdir(\"..\/input\"))","e6da7d1a":"# load dataset\ndata = pd.read_csv(\"..\/input\/train.csv\")\n\n# rows and columns of the data\nprint(data.shape)\n\n# visualise the dataset\ndata.head()","748faaba":"# make a list of the variables that contain missing values\nvars_with_na = [var for var in data.columns if data[var].isnull().sum()>1]\n\n# print the variable name and the percentage of missing values\nfor var in vars_with_na:\n    print(var, np.round(data[var].isnull().mean(), 3),  ' % missing values')","1346ffef":"def analyse_na_value(df, var):\n    df = df.copy()\n    \n    # let's make a variable that indicates 1 if the observation was missing or zero otherwise\n    df[var] = np.where(df[var].isnull(), 1, 0)\n    \n    # let's calculate the mean SalePrice where the information is missing or present\n    df.groupby(var)['SalePrice'].median().plot.bar()\n    plt.title(var)\n    plt.show()\n    \nfor var in vars_with_na:\n    analyse_na_value(data, var)","b73b5fb8":"# list of numerical variables\nnum_vars = [var for var in data.columns if data[var].dtypes != 'O']\n\nprint('Number of numerical variables: ', len(num_vars))\n\n# visualise the numerical variables\ndata[num_vars].head()","4411cd55":"print('Number of House Id labels: ', len(data.Id.unique()))\nprint('Number of Houses in the Dataset: ', len(data))","7ee4f914":"# list of variables that contain year information\nyear_vars = [var for var in num_vars if 'Yr' in var or 'Year' in var]\n\nyear_vars","ec345cf1":"# let's explore the content of these year variables\nfor var in year_vars:\n    print(var, data[var].unique())\n    print()","b813d262":"data.groupby('YrSold')['SalePrice'].median().plot()\nplt.ylabel('Median House Price')\nplt.title('Change in House price with the years')","708c98b2":"# let's explore the relationship between the year variables and the house price in a bit of more details\ndef analyse_year_vars(df, var):\n    df = df.copy()\n    \n    # capture difference between year variable and year the house was sold\n    df[var] = df['YrSold'] - df[var]\n    \n    plt.scatter(df[var], df['SalePrice'])\n    plt.ylabel('SalePrice')\n    plt.xlabel(var)\n    plt.show()\n    \nfor var in year_vars:\n    if var !='YrSold':\n        analyse_year_vars(data, var)\n    ","10f53335":"#  list of discrete variables\ndiscrete_vars = [var for var in num_vars if len(data[var].unique())<20 and var not in year_vars+['Id']]\n\nprint('Number of discrete variables: ', len(discrete_vars))","41d2d3b7":"# let's visualise the discrete variables\ndata[discrete_vars].head()","52140a0c":"def analyse_discrete(df, var):\n    df = df.copy()\n    df.groupby(var)['SalePrice'].median().plot.bar()\n    plt.title(var)\n    plt.ylabel('SalePrice')\n    plt.show()\n    \nfor var in discrete_vars:\n    analyse_discrete(data, var)","9b9fe73c":"# list of continuous variables\ncont_vars = [var for var in num_vars if var not in discrete_vars+year_vars+['Id']]\n\nprint('Number of continuous variables: ', len(cont_vars))","926fcd60":"# let's visualise the continuous variables\ndata[cont_vars].head()","451796b6":"# Let's go ahead and analyse the distributions of these variables\ndef analyse_continous(df, var):\n    df = df.copy()\n    df[var].hist(bins=20)\n    plt.ylabel('Number of houses')\n    plt.xlabel(var)\n    plt.title(var)\n    plt.show()\n    \nfor var in cont_vars:\n    analyse_continous(data, var)","d4153ce0":"# Let's go ahead and analyse the distributions of these variables\ndef analyse_transformed_continous(df, var):\n    df = df.copy()\n    \n    # log does not take negative values, so let's be careful and skip those variables\n    if 0 in data[var].unique():\n        pass\n    else:\n        # log transform the variable\n        df[var] = np.log(df[var])\n        df[var].hist(bins=20)\n        plt.ylabel('Number of houses')\n        plt.xlabel(var)\n        plt.title(var)\n        plt.show()\n    \nfor var in cont_vars:\n    analyse_transformed_continous(data, var)","c8031e06":"# let's explore the relationship between the house price and the transformed variables\n# with more detail\ndef transform_analyse_continous(df, var):\n    df = df.copy()\n    \n    # log does not take negative values, so let's be careful and skip those variables\n    if 0 in data[var].unique():\n        pass\n    else:\n        # log transform\n        df[var] = np.log(df[var])\n        df['SalePrice'] = np.log(df['SalePrice'])\n        plt.scatter(df[var], df['SalePrice'])\n        plt.ylabel('SalePrice')\n        plt.xlabel(var)\n        plt.show()\n    \nfor var in cont_vars:\n    if var !='SalePrice':\n        transform_analyse_continous(data, var)","cad68d5a":"# let's make boxplots to visualise outliers in the continuous variables \n\ndef find_outliers(df, var):\n    df = df.copy()\n    \n    # log does not take negative values, so let's be careful and skip those variables\n    if 0 in data[var].unique():\n        pass\n    else:\n        df[var] = np.log(df[var])\n        df.boxplot(column=var)\n        plt.title(var)\n        plt.ylabel(var)\n        plt.show()\n    \nfor var in cont_vars:\n    find_outliers(data, var)","8f28149c":"### Categorical variables\n\ncat_vars = [var for var in data.columns if data[var].dtypes=='O']\n\nprint('Number of categorical variables: ', len(cat_vars))","6057604c":"# let's visualise the values of the categorical variables\ndata[cat_vars].head()","73f08d49":"for var in cat_vars:\n    print(var, len(data[var].unique()), ' categories')","0ea6548b":"def analyse_rare_labels(df, var, rare_perc):\n    df = df.copy()\n    tmp = df.groupby(var)['SalePrice'].count() \/ len(df)\n    return tmp[tmp<rare_perc]\n\nfor var in cat_vars:\n    print(analyse_rare_labels(data, var, 0.01))\n    print()\n","e9912200":"for var in cat_vars:\n    analyse_discrete(data, var)","174dbfa8":"# to handle datasets\nimport pandas as pd\nimport numpy as np\n\n# for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# to divide train and test set\nfrom sklearn.model_selection import train_test_split\n\n# feature scaling\nfrom sklearn.preprocessing import MinMaxScaler\n\n# to visualise al the columns in the dataframe\npd.pandas.set_option('display.max_columns', None)","0c6268c3":"# Let's separate into train and test set\n# Remember to set the seed (random_state for this sklearn function)\n\nX_train, X_test, y_train, y_test = train_test_split(data, data.SalePrice,\n                                                    test_size=0.1,\n                                                    random_state=0) # we are setting the seed here\nX_train.shape, X_test.shape","63ef102f":"# make a list of the categorical variables that contain missing values\nvars_with_na = [var for var in data.columns if X_train[var].isnull().sum()>1 and X_train[var].dtypes=='O']\n\n# print the variable name and the percentage of missing values\nfor var in vars_with_na:\n    print(var, np.round(X_train[var].isnull().mean(), 3),  ' % missing values')","64a0b9b8":"# function to replace NA in categorical variables\ndef fill_categorical_na(df, var_list):\n    X = df.copy()\n    X[var_list] = df[var_list].fillna('Missing')\n    return X","025be342":"# replace missing values with new label: \"Missing\"\nX_train = fill_categorical_na(X_train, vars_with_na)\nX_test = fill_categorical_na(X_test, vars_with_na)\n\n# check that we have no missing information in the engineered variables\nX_train[vars_with_na].isnull().sum()","41535091":"# check that test set does not contain null values in the engineered variables\n[vr for var in vars_with_na if X_train[var].isnull().sum()>0]","bc52116d":"# make a list of the numerical variables that contain missing values\nvars_with_na = [var for var in data.columns if X_train[var].isnull().sum()>1 and X_train[var].dtypes!='O']\n\n# print the variable name and the percentage of missing values\nfor var in vars_with_na:\n    print(var, np.round(X_train[var].isnull().mean(), 3),  ' % missing values')","0aa377a0":"# replace the missing values\nfor var in vars_with_na:\n    \n    # calculate the mode\n    mode_val = X_train[var].mode()[0]\n    \n    # train\n    X_train[var+'_na'] = np.where(X_train[var].isnull(), 1, 0)\n    X_train[var].fillna(mode_val, inplace=True)\n    \n    # test\n    X_test[var+'_na'] = np.where(X_test[var].isnull(), 1, 0)\n    X_test[var].fillna(mode_val, inplace=True)\n\n# check that we have no more missing values in the engineered variables\nX_train[vars_with_na].isnull().sum()","14eb3ff8":"# check that we have the added binary variables that capture missing information\nX_train[['LotFrontage_na', 'MasVnrArea_na', 'GarageYrBlt_na']].head()","9f4901e0":"# let's explore the relationship between the year variables and the house price in a bit of more details\n\ndef elapsed_years(df, var):\n    # capture difference between year variable and year the house was sold\n    df[var] = df['YrSold'] - df[var]\n    return df\nfor var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n    X_train = elapsed_years(X_train, var)\n    X_test = elapsed_years(X_test, var)\n# check that test set does not contain null values in the engineered variables\n[vr for var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'] if X_test[var].isnull().sum()>0]","3ec88cdd":"for var in ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']:\n    X_train[var] = np.log(X_train[var])\n    X_test[var]= np.log(X_test[var])\n# check that test set does not contain null values in the engineered variables\n[var for var in ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice'] if X_test[var].isnull().sum()>0]","22da13d4":"# same for train set\n[var for var in ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice'] if X_train[var].isnull().sum()>0]","d1cc97c8":"# let's capture the categorical variables first\ncat_vars = [var for var in X_train.columns if X_train[var].dtype == 'O']","eb38bba4":"def find_frequent_labels(df, var, rare_perc):\n    # finds the labels that are shared by more than a certain % of the houses in the dataset\n    df = df.copy()\n    tmp = df.groupby(var)['SalePrice'].count() \/ len(df)\n    return tmp[tmp>rare_perc].index\n\nfor var in cat_vars:\n    frequent_ls = find_frequent_labels(X_train, var, 0.01)\n    X_train[var] = np.where(X_train[var].isin(frequent_ls), X_train[var], 'Rare')\n    X_test[var] = np.where(X_test[var].isin(frequent_ls), X_test[var], 'Rare')","d576ac30":"# this function will assign discrete values to the strings of the variables, \n# so that the smaller value corresponds to the smaller mean of target\n\ndef replace_categories(train, test, var, target):\n    ordered_labels = train.groupby([var])[target].mean().sort_values().index\n    ordinal_label = {k:i for i, k in enumerate(ordered_labels, 0)} \n    train[var] = train[var].map(ordinal_label)\n    test[var] = test[var].map(ordinal_label)","246890d7":"for var in cat_vars:\n    replace_categories(X_train, X_test, var, 'SalePrice')","c9ca8b31":"# check absence of na\n[var for var in X_train.columns if X_train[var].isnull().sum()>0]","ee4312ca":"# check absence of na\n[var for var in X_test.columns if X_test[var].isnull().sum()>0]","5540e759":"# let me show you what I mean by monotonic relationship between labels and target\ndef analyse_vars(df, var):\n    df = df.copy()\n    df.groupby(var)['SalePrice'].median().plot.bar()\n    plt.title(var)\n    plt.ylabel('SalePrice')\n    plt.show()\n    \nfor var in cat_vars:\n    analyse_vars(X_train, var)","64b3bf77":"train_vars = [var for var in X_train.columns if var not in ['Id', 'SalePrice']]\nlen(train_vars)","87c03019":"X_train[['Id', 'SalePrice']].reset_index(drop=True)","437c85b1":"# fit scaler\nscaler = MinMaxScaler() # create an instance\nscaler.fit(X_train[train_vars]) #  fit  the scaler to the train set for later use\n\n# transform the train and test set, and add on the Id and SalePrice variables\nX_train = pd.concat([X_train[['Id', 'SalePrice']].reset_index(drop=True),\n                    pd.DataFrame(scaler.transform(X_train[train_vars]), columns=train_vars)],\n                    axis=1)\n\nX_test = pd.concat([X_test[['Id', 'SalePrice']].reset_index(drop=True),\n                    pd.DataFrame(scaler.transform(X_test[train_vars]), columns=train_vars)],\n                    axis=1)","7ed3ddaf":"X_train.head()","2abd2d3c":"That concludes the feature engineering section for this dataset.","609b6bfb":"# check absence of missing values\nX_train.isnull().sum()","a7d13bcd":"# capture the target\ny_train = X_train['SalePrice']\ny_test = X_test['SalePrice']\n\n# drop unnecessary variables from our training and testing sets\nX_train.drop(['Id', 'SalePrice'], axis=1, inplace=True)\nX_test.drop(['Id', 'SalePrice'], axis=1, inplace=True)","a1f231af":"# here I will do the model fitting and feature selection\n# altogether in one line of code\n\n# first, I specify the Lasso Regression model, and I\n# select a suitable alpha (equivalent of penalty).\n# The bigger the alpha the less features that will be selected.\n\n# Then I use the selectFromModel object from sklearn, which\n# will select the features which coefficients are non-zero\n# to build the models\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\nsel_ = SelectFromModel(Lasso(alpha=0.005, random_state=0)) # remember to set the seed, the random state in this function\nsel_.fit(X_train, y_train)","b37ff105":"# this command let's us visualise those features that were kept.\n# Kept features have a True indicator\nsel_.get_support()","637b57ef":"# let's print the number of total and selected features\n\n# this is how we can make a list of the selected features\nselected_feat = X_train.columns[(sel_.get_support())]\n\n# let's print some stats\nprint('total features: {}'.format((X_train.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\nprint('features with coefficients shrank to zero: {}'.format(\n    np.sum(sel_.estimator_.coef_ == 0)))","4f471eac":"# print the selected features\nselected_feat","e953951b":"# this is an alternative way of identifying the selected features \n# based on the non-zero regularisation coefficients:\nselected_feats = X_train.columns[(sel_.estimator_.coef_ != 0).ravel().tolist()]\nselected_feats","d4c62c13":"\n# here I will add this last feature, even though it was not selected in our previous step,\n# because it needs key feature engineering steps that I want to discuss further during the deployment\n# part of the course. \nselected_feats = selected_feats + ['LotFrontage'] \n","de201519":"# reduce the train and test set to the desired features\n\nX_train = X_train[selected_feats]\nX_test = X_test[selected_feats]","f9ebf1e3":"From the previous plots, we observe some monotonic associations between SalePrice and the variables to which we applied the log transformation, for example 'GrLivArea'.","0f9486fe":"The majority of the continuous variables seem to contain outliers. Outliers tend to affect the performance of linear model. So it is worth spending some time understanding if removing outliers will add performance value to our  final machine learning model.","e0cf85bd":"We see that the fact that the information is missing for those variables, is important. We will capture this information when we engineer the variables in our next section.","9eb06452":"For numerical variables, we are going to add an additional variable capturing the missing information, and then replace the missing information in the original variable by the mode, or most frequent value:","0f085db4":"## 2.6 Feature Scaling\n\nFor use in linear models, features need to be either scaled or normalised. In the next section, I will scale features between the min and max values:","24a855d6":"## 2.3 Numerical variables: Non-Gaussian to gaussian\n\nWe will log transform the numerical variables that do not contain zeros in order to get a more Gaussian-like distribution. This tends to help Linear machine learning models. ","d54c11b2":"As you can see, it refers to years.\n\nWe can also explore the evolution of the sale price with the years in which the house was sold:","5a31cdde":"### 1.3.1 Number of labels: cardinality\n\nLet's evaluate how many different categories are present in each of the variables.","42225ba1":"From the above view of the dataset, we notice the variable Id, which is an indicator of the house. We will not use this variable to make our predictions, as there is one different value of the variable per each row, i.e., each house in the dataset. See below:","c5c43766":"We see that there is a relationship between the variable numbers and the SalePrice, but this relationship is not always monotonic. \n\nFor example, for OverallQual, there is a monotonic relationship: the higher the quality, the higher the SalePrice.  \n\nHowever, for OverallCond, the relationship is not monotonic. Clearly, some Condition grades, like 5, favour better selling prices, but higher values do not necessarily do so. We need to be careful on how we engineer these variables to extract the most for a linear model. \n\nThere are ways to re-arrange the order of the discrete values of a variable, to create a monotonic relationship between the variable and the target.","17323280":"## 1.3 Categorical variables\nLet's go ahead and analyse the categorical variables present in the dataset.","7a3c1fa8":"### Identify the selected variables","c425cc9c":"## 1.4 Potential relationship between IDV and sales price(target)\nSome of the categorical variables show multiple labels that are present in less than 1% of the houses. We will engineer these variables in our next section. Labels that are under-represented in the dataset tend to cause over-fitting of machine learning models. That is why we want to remove them.\n\nFinally, we want to explore the relationship between the categories of the different variables and the house price:","f4d5f555":"#### 1.2.1.2 Continuous variables\n\nLet's go ahead and find the distribution of the continuous variables. We will consider continuous all those that are not temporal or discrete variables in our dataset.","0cc47086":"### Separate dataset into train and test\n\nBefore beginning to engineer our features, it is important to separate our data intro training and testing set. This is to avoid over-fitting. This step involves randomness, therefore, we need to set the seed.","3bd9f735":"# 2.Feature Engineering\n\n## House Prices dataset: Feature Engineering\n\nIn the following cells, we will engineer \/ pre-process the variables of the House Price Dataset. We will engineer the variables so that we tackle:\n\n1. Missing values\n2. Temporal variables\n3. Non-Gaussian distributed variables\n4. Categorical variables: remove rare labels\n5. Categorical variables: convert strings to numbers\n5. Standarise the values of the variables to the same range\n\n### Setting the seed\n\nIt is important to note that we are engineering variables and pre-processing data with the idea of deploying the model if we find business value in it. Therefore, from now on, for each step that includes some element of randomness, it is extremely important that we **set the seed**. This way, we can obtain reproducibility between our research and our development code.\n\nThis is perhaps one of the most important lessons that you need to take away from this course: **Always set the seeds**.","acc75be3":"The house price dataset contains 1460 rows, i.e., houses, and 81 columns, i.e., variables. \n**We will analyse the dataset to identify:**\n1. Data Anslysis<br>\n1.1. Missing values<br>\n1.2. Numerical variables<br>\n1.2.1. Distribution of the numerical variables<br>\n1.2.1.1. Discrete Numeric variables<br>\n1.2.1.2. Continuous Numeric variables<br>\n1.2.2. Outliers<br>\n1.3. Categorical variables<br>\n1.3.1. Cardinality of the categorical variables<br>\n1.3.2. Rare Labels<br>\n1.4. Potential relationship between the variables and the target: SalePrice","29935211":"## Machine Learning Model Building Pipeline: Data Analysis\n\n1. Data Analysis\n2. Feature Engineering\n3. Feature Selection\n4. Model Building\n\n**This is the notebook for step 1: Data Analysis**\n\n===================================================================================================\n\n## Predicting Sale Price of Houses\n\nThe aim of the project is to build a machine learning model to predict the sale price of homes based on different explanatory variables describing aspects of residential houses. \n\n### Why is this important? \n\nPredicting house prices is useful to identify fruitful investments, or to determine whether the price advertised for a house is over or underestimated, before making a buying judgment.\n\n### What is the objective of the machine learning model?\n\nWe aim to minimise the difference between the real price, and the estimated price by our model. We will evaluate model performance using the mean squared error (mse) and the root squared of the mean squared error (rmse).","d200a7c5":"## 2.4 Categorical variables: Removal of Rare variable\n\nFirst, we will remove those categories within variables that are present in less than 1% of the observations:","2dc2f36d":"We can see that these variables tend to be Qualifications or grading scales, or refer to the number of rooms, or units. Let's go ahead and analyse their contribution to the house price.","9a04e28b":"Relationship between values being missing and Sale Price\n\nLet's evaluate the price of the house for those cases where the information is missing, for each variable.","f4f08b54":"## House Prices dataset: Machine Learning Model build\n\nIn the following cells, we will finally build our machine learning models, utilising the engineered data and the pre-selected features. \n\n\n### Setting the seed\n\nIt is important to note, that we are engineering variables and pre-processing data with the idea of deploying the model if we find business value in it. Therefore, from now on, for each step that includes some element of randomness, it is extremely important that we **set the seed**. This way, we can obtain reproducibility between our research and our development code.\n\nThis is perhaps one of the most important lessons that you need to take away from this course: **Always set the seeds**.\n\nLet's go ahead and load the dataset.","90f8e269":"There has been a drop in the value of the houses. That is unusual, in real life, house prices typically go up as years go by.\n\n\nLet's go ahead and explore whether there is a relationship between the year variables and SalePrice. For this, we will capture the elapsed years between the Year variables and the year in which the house was sold:","300457e6":"## 1.1 Missing values\n\nLet's go ahead and find out which variables of the dataset contain missing values","14c89565":"### Feature Selection\n\nLet's go ahead and select a subset of the most predictive features. There is an element of randomness in the Lasso regression, so remember to set the seed.","8d96f787":"## 2.5 Categorical variable: Convert string to number","a973b664":"#### Temporal variables\n\nFrom the above view we also notice that we have 4 year variables. Typically, we will not use date variables as is, rather we extract information from them. For example, the difference in years between the year the house was built and the year the house was sold. We need to take this into consideration in our sectio, where we will engineer our features.","8787932a":"Next, we need to transform the strings of these variables into numbers. We will do it so that we capture the monotonic relationship between the label and the target:","e5e0d12f":"## 1.2 Numerical variables\n\nLet's go ahead and find out what numerical variables we have in the dataset","1eab5888":"We see that all of the above variables, are not normally distributed, including the target variable 'SalePrice'. For linear models to perform best, we need to account for non-Gaussian distributions. We will transform our variables in the next section, during our feature engineering section.\n\nLet's also evaluate here if a log transformation renders the variables more Gaussian looking:","35e9bf0b":"# 3. Feature Selection\n\nIn the following cells, we will select a group of variables, the most predictive ones, to build our machine learning models. \n\n### Why do we need to select variables?\n\n1. For production: Fewer variables mean smaller client input requirements (e.g. customers filling out a form on a website or mobile app), and hence less code for error handling. This reduces the chances of bugs.\n2. For model performance: Fewer variables mean simpler, more interpretable, less over-fitted models\n\n\n**We will select variables using the Lasso regression: Lasso has the property of setting the coefficient of non-informative variables to zero. This way we can identify those variables and remove them from our final models.**\n\n### Setting the seed\n\nIt is important to note, that we are engineering variables and pre-processing data with the idea of deploying the model if we find business value in it. Therefore, from now on, for each step that includes some element of randomness, it is extremely important that we **set the seed**. This way, we can obtain reproducibility between our research and our development code.","5bb17031":"## 2.2 Temporal variables\n\nWe remember from the previous lecture, that there are 4 variables that refer to the years in which something was built or something specific happened. We will capture the time elapsed between the that variable and the year the house was sold:","6e9257c5":"Our dataset contains a few variables with missing values. We need to account for this in our following notebook, where we will engineer the variables for use in Machine Learning Models.","1cfbe87a":"## 2.1 Missing values\n\nFor categorical variables, we will fill missing information by adding an additional category: \"missing\"","4c17b1a3":"### 1.2.2 Outliers","7eef9f22":"All the categorical variables show low cardinality, this means that they have only few different labels. That is good as we won't need to tackle cardinality during our feature engineering lecture.\n\n### 1.3.2 Rare labels:\n\nLet's go ahead and investigate now if there are labels that are present only in a small number of houses:","24eaf893":"Clearly, the categories give information on the SalePrice. In the next section, we will transform these strings \/ labels into numbers, so that we capture this information and transform it into a monotonic relationship between the category and the house price.","a81a67c1":"### 1.2.1 Distribution of numeric variable","a5dd201e":"#### 1.2.1.1 Discrete variables\n\nLet's go ahead and find which variables are discrete, i.e., show a finite number of values","5f16ec15":"# 1. Data Analysis\n\nIn the following cells, we will analyse the variables of the House Price Dataset from Kaggle. I will take you through the different aspects of the analysis that we will make over the variables, and introduce you to the meaning of each of the variables as well.\nLet's go ahead and load the dataset.","04443c9a":"We see that there is a tendency to a decrease in price, with older features.","12fbe2be":"We get a better spread of values for most variables when we use the logarithmic transformation. This engineering step will most likely add performance value to our final model.","d8812ed2":"We can now see monotonic relationships between the labels of our variables and the target (remember that the target is log-transformed, that is why the differences seem so small)."}}