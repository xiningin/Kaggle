{"cell_type":{"2aa38076":"code","aaf15f64":"code","b7ff6030":"code","765f7f42":"code","4516b500":"code","597e064e":"code","cfe61ea9":"code","d50f813f":"code","48ed8983":"code","5c192da5":"code","99efdfc8":"code","a0738df9":"code","db834230":"code","7ac72bc7":"code","1f0a437a":"code","5e08c050":"code","79788b40":"code","e1de8a9e":"code","c9a5a24a":"code","c38bf6cf":"code","24d4e924":"code","ab6ae73f":"markdown","1012f53e":"markdown","6cb03b13":"markdown","4bc7195c":"markdown","08ffef29":"markdown","18804411":"markdown","dc3fad1d":"markdown","2b2ebf29":"markdown","b21ae40d":"markdown","bc057b3c":"markdown","c9d5ffa5":"markdown","a1e60f2b":"markdown"},"source":{"2aa38076":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\nfrom keras.preprocessing.image import load_img, save_img, img_to_array\nfrom keras.applications import vgg19\nfrom keras import backend as K\n\nfrom scipy.optimize import fmin_l_bfgs_b\n\nfrom PIL import Image\nimport PIL\nimport time\n\nimport os\nprint(os.listdir(\"..\/input\"))","aaf15f64":"StylePath = '\/kaggle\/input\/styleimage\/'\nContentPath = '\/kaggle\/input\/contentimages\/'","b7ff6030":"target_image_path = ContentPath+'equilibrium.jpg'\nstyle_image_path = StylePath+'gogh1.jpg'","765f7f42":"# dimensions of the generated picture.\nwidth, height = load_img(target_image_path).size\nimg_height = 400\nimg_width = int(width * img_height \/ height)","4516b500":"fig, ax = plt.subplots(1,2, figsize=(18, 12))\nax[0].set_title('Target image', fontsize=\"20\")\nimg_target = load_img(target_image_path)\nax[0].imshow(img_target)  \nax[1].set_title('Style image', fontsize=\"20\")\nimg_style = load_img(style_image_path)\nax[1].imshow(img_style)","597e064e":"#Utility functions for opening, resizing and formating image into tensors for the model\ndef preprocess_image_VGG19(image_path):\n    img = load_img(image_path, target_size=(img_height, img_width))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = vgg19.preprocess_input(img)\n    return img\n\n# util functions to convert a tensor into a valid image\ndef deprocess_image_VGG19(x):\n    if K.image_data_format() == 'channels_first':\n        x = x.reshape((3, img_height, img_width))\n        x = x.transpose((1, 2, 0))\n    else:\n        x = x.reshape((img_height, img_width, 3))\n    # Remove zero-center by mean pixel\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    # 'BGR'->'RGB'\n    x = x[:, :, ::-1]\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x","cfe61ea9":"target_image = K.constant(preprocess_image_VGG19(target_image_path))\nstyle_reference_image = K.constant(preprocess_image_VGG19(style_image_path))\n# this will contain our generated image\nif K.image_data_format() == 'channels_first':\n    combination_image = K.placeholder((1,3,img_height, img_width))\nelse:\n    combination_image = K.placeholder((1,img_height, img_width,3))","d50f813f":"# combine the 3 images into a single Keras tensor\ninput_tensor = K.concatenate([target_image,\n                              style_reference_image,\n                              combination_image\n                              ], axis=0)","48ed8983":"input_tensor","5c192da5":"vgg19_weights = '\/kaggle\/input\/vgg19\/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\nmodel = vgg19.VGG19(input_tensor=input_tensor,\n              weights=vgg19_weights,\n              include_top = False)\nprint('Model loaded.')\n","99efdfc8":"def content_loss(base, combination):\n    return K.sum(K.square(combination - base))","a0738df9":"#import tensorflow as tf\n# the gram matrix of an image tensor (feature-wise outer product)\ndef gram_matrix(x):\n    features = K.batch_flatten(K.permute_dimensions(x,(2,0,1))) #Turn a nD tensor into a 2D tensor with same 0th dimension\n    gram = K.dot(features, K.transpose(features))\n    return gram\n\ndef style_loss(style, combination):\n    S = gram_matrix(style)\n    C = gram_matrix(combination)\n    channels = 3\n    size = img_height*img_width\n    return K.sum(K.square(S - C)) \/ (4.0 * (channels ** 2) * (size ** 2))","db834230":"# total variation loss\ndef total_variation_loss(x):\n    assert K.ndim(x) == 4\n    if K.image_data_format() == 'channels_first':\n        a = K.square(x[:, :, :img_height - 1, :img_width - 1] -\n                     x[:, :, 1:, :img_width - 1])\n        b = K.square(x[:, :, :img_height - 1, :img_width - 1] -\n                     x[:, :, :img_height - 1, 1:])\n    else:\n        a = K.square(x[:, :img_height - 1, :img_width-1, :] -\n                     x[:, 1:,              :img_width-1, :]) # (pixel i- pixel i_1)^2\n        b = K.square(x[:, :img_height - 1, :img_width-1, :] -\n                     x[:, :img_height - 1, 1:,           :]) # (pixel j- pixel j_1)^2\n    return K.sum(K.pow(a + b, 1.25))","7ac72bc7":"outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\nprint(outputs_dict['block5_conv2'])","1f0a437a":"# Content layer where will pull our feature maps\ncontent_layers = 'block5_conv2'\n\n# Style layer we are interested in\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1',\n                'block5_conv1'\n               ]\ntotal_variation_weight = 1e-4\nstyle_weight = 1.\ncontent_weight = 0.025 ","5e08c050":"loss = K.variable(0.)\nlayer_features = outputs_dict[content_layers]\ntarget_image_features = layer_features[0, :, :, :]\ncombination_features = layer_features[2, :, :, :]\nloss = loss + content_weight * content_loss(target_image_features, \n                                            combination_features)\n\nfor layer_name in style_layers:\n    layer_features = outputs_dict[layer_name]\n    style_reference_features = layer_features[1, :, :, :]\n    combination_features = layer_features[2, :, :, :]\n    sl = style_loss(style_reference_features, combination_features)\n    loss += (style_weight \/ len(style_layers)) * sl\n\nloss += total_variation_weight * total_variation_loss(combination_image)","79788b40":"# get the gradients of the generated image wrt the loss\ngrads = K.gradients(loss, combination_image)[0]\ngrads","e1de8a9e":"#Set up Keras function to retrieve the loss and gradient values given an input image:\nfetch_loss_and_grads = K.function([combination_image], [loss, grads])","c9a5a24a":"#This class wraps fetch_loss_and_grads in a way that lets you retrieve the losses and the gradients via two separate methods calls, which is required by the SciPy optimizer.\nclass Evaluator(object):\n    def __init__(self):\n        self.loss_value = None\n        self.grad_values = None\n\n    def loss(self, x):\n        assert self.loss_value is None\n        #loss_value, grad_values = eval_loss_and_grads(x)\n        x = x.reshape((1, img_height, img_width, 3))\n        outs = fetch_loss_and_grads([x])\n        loss_value = outs[0]\n        grad_values = outs[1].flatten().astype('float64')\n        self.loss_value = loss_value\n        self.grad_values = grad_values\n        return self.loss_value\n\n    def grads(self, x):\n        assert self.loss_value is not None\n        grad_values = np.copy(self.grad_values)\n        self.loss_value = None\n        self.grad_values = None\n        return grad_values\n\nevaluator = Evaluator()","c38bf6cf":"x = preprocess_image_VGG19(target_image_path)\n\niterations = 20\nfor i in range(iterations):\n    print('Start of iteration', i)\n    start_time = time.time()\n    x, min_val, info= fmin_l_bfgs_b(evaluator.loss,\n                                    x.flatten(),\n                                    fprime=evaluator.grads,\n                                    maxfun=20)\n    print('Current loss value:', min_val)\n    if i%10==0:\n        img = x.copy().reshape((img_height, img_width, 3))\n        img = deprocess_image_VGG19(img)\n        img_pil = Image.fromarray(img, 'RGB')\n        display(img_pil)\n    end_time = time.time()\n    print('Iteration %d completed in %ds' %(i, end_time-start_time))\n\nimg = x.copy().reshape((img_height, img_width, 3))\nimg = deprocess_image_VGG19(img)\nimg_pil = Image.fromarray(img, 'RGB')\ndisplay(img_pil)\n    ","24d4e924":"fig, ax = plt.subplots(1,3, figsize=(22, 15))\nax[0].set_title('Target image', fontsize=\"20\")\n#img_target = load_img(target_image_path)\nax[0].imshow(img_target)  \nax[1].set_title('Style image', fontsize=\"20\")\n#img_style = load_img(style_image_path)\nax[1].imshow(img_style)\nax[2].set_title('Combination image', fontsize=\"20\")\n#img_style = load_img(style_image_path)\nax[2].imshow(img_pil)","ab6ae73f":"## Defining the final loss as an average of previous losses","1012f53e":"## Choose target and style images","6cb03b13":"## Continuity loss","4bc7195c":"## Content loss","08ffef29":"## Style loss","18804411":"## Define some utility functions","dc3fad1d":"This notebook takes its inspiration in the DeepDream implementation example of [Keras](https:\/\/keras.io\/examples\/deep_dream\/) as well as in this [blogpost](https:\/\/asolda.github.io\/post\/deepdream\/).","2b2ebf29":"## Loading the VGG19 network and applying it to the three images","b21ae40d":"## Load libraries","bc057b3c":"## Style transfer loop","c9d5ffa5":"## Visualization","a1e60f2b":"## Setting up gradient-descent process"}}