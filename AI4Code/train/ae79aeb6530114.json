{"cell_type":{"563d9e64":"code","832a9bf0":"code","1b860271":"code","b8929564":"code","2214a1cf":"code","f08fbeca":"code","8a884eb6":"code","20b55af2":"code","e7afafee":"code","eb970961":"code","b43bbc0e":"code","626cdf15":"code","88625de4":"code","6fbf0077":"code","eda54f6f":"code","54df4e37":"code","0a653dd9":"code","1d5bb103":"code","0f7a9478":"markdown","8b44b60c":"markdown","0a732814":"markdown","d23454c9":"markdown","16e82815":"markdown","5caaf5ee":"markdown","640244dd":"markdown","631d178d":"markdown","038b0445":"markdown","6ea7324b":"markdown","7b441e36":"markdown","749e0fe3":"markdown","1e475f4b":"markdown","f21d5d4d":"markdown","db963eea":"markdown","6efc9969":"markdown","81cf3c26":"markdown"},"source":{"563d9e64":"#This is coded by Volkan \u00d6zdemir \n#Essential Libraries\nimport tensorflow as tf\nimport datetime\nimport numpy as np\nfrom tensorflow import keras\nfrom keras.layers import GaussianNoise\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pathlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom keras import optimizers\nimport tensorflow as tf\nimport tensorflow.keras \nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.utils import to_categorical\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras import backend as K\nfrom keras.layers.normalization import BatchNormalization","832a9bf0":"test_path = r\"\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\"\ntrain_path = r\"\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\"\nsubmission_path = r\"\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\"\n\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)","1b860271":"train=train.iloc[:,1:]\ntest=test.iloc[:,1:]\nprint('train shape is: ',train.shape)\nprint('test shape is: ',test.shape)\ny = train[\"loss\"]\nX = train.iloc[:,:-1]\nprint(type(X), type(y))\nprint( X.shape, y.shape)","b8929564":"train.head()","2214a1cf":"test.head()","f08fbeca":"label=y\ntrain = X\nprint(\"train shape: \",train.shape),print(\"label shape: \",label.shape)","8a884eb6":"x_train, x_test, y_train, y_test = train_test_split(train, label, test_size=0.30, random_state=42 )\nprint(\"x_train shape: \", x_train.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"x_test shape: \", x_test.shape)\nprint(\"y_test shape: \", y_test.shape)","20b55af2":"from sklearn.preprocessing import  StandardScaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\ntest = scaler.transform(test)","e7afafee":"#reshape images\nx_train = x_train.reshape(-1,10,10,1)\nx_test = x_test.reshape(-1,10,10,1)\ntest = test.reshape(-1,10,10,1)\n\nprint(\"x_train shape: \", x_train.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"x_test shape: \", x_test.shape)\nprint(\"y_test shape: \", y_test.shape)\nprint(\"test shape: \", test.shape)","eb970961":"fig = plt.figure(figsize = (21, 22))\nlabel_index=y_train[:25].to_list()\nfor i in range(25):\n    plt.subplot(5,5,1 + i)\n    plt.title(\"Image Belongs to the label: \" + \" \" + str(label_index[i]), fontname=\"Times New Roman\",fontweight=\"bold\")\n    plt.imshow(x_train[i,:,:,0], cmap=plt.get_cmap('gray'))\nplt.show()","b43bbc0e":"#Hyper Parameters\nbatch_size = 128\nepochs = 8\nlearning_rate=0.01\nactivation = 'relu'\nFully_connected_layer_nodes=86\ninput_shape = (10, 10, 1)","626cdf15":"#y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n#y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)","88625de4":"#Rehape the images for AlexNet input such: (28,28,1) --> Giri\u015fleri AlexNet'e g\u00f6re ayarl\u0131yoruz.\nimg_rows = 10\nimg_cols = 10\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)","6fbf0077":"print(\"x_train shape: \", x_train.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"x_test shape: \", x_test.shape)\nprint(\"y_test shape: \", y_test.shape)\nprint(\"input_shape: \", input_shape )","eda54f6f":"#ALEXNET STRUCTURE\nmodel = Sequential()\n\nmodel.add(Conv2D(8, kernel_size=(1, 1), activation= activation, padding=\"SAME\",input_shape=input_shape ) )\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(1, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(36, (3, 3), activation=activation,padding=\"SAME\"))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(Fully_connected_layer_nodes, activation=activation))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(Fully_connected_layer_nodes, activation=activation))\nmodel.add(BatchNormalization())\n\n\nmodel.add(Dense(1, activation='linear'))\n\n\nadam=tensorflow.keras.optimizers.Adam(lr=learning_rate)\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)      \n      \nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\nmodel.summary()","54df4e37":"history=model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[callback], verbose=1, validation_data=(x_test, y_test), shuffle=False)\nval_loss, val_acc=model.evaluate(x_test, y_test )\nprint(\"validation loss: \", val_loss)\nprint( \"<3 \")\nprint(\"validation accuracy: \", val_acc)\nprint(\"learn rate: \",learning_rate, \"epochs: \", epochs, \"activation: \",activation, \"Fully Connected Layer's Node Number :\", Fully_connected_layer_nodes)","0a653dd9":"y_submission=model.predict(test)\npred = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")\npred.loss = y_submission\npred","1d5bb103":"pred.to_csv('submission.csv', index=False)","0f7a9478":"* Read the data as test & train","8b44b60c":"* Train head is:","0a732814":"## $\\color{Pink}{\\text{Chapter 2. Creating Monochrome Images}}$ <a class=\"anchor\" id=\"chapter2\"><\/a>","d23454c9":"* Let see our magical images :D I wonder if there will be some logical outcomes..","16e82815":"* And now our part has come, we will create 10x10 matrices from 100x1 feature","5caaf5ee":"## $\\color{Pink}{\\text{Chapter 3. AlexNet CNN Training}}$ <a class=\"anchor\" id=\"chapter3\"><\/a>","640244dd":"## $\\color{Pink}{\\text{Chapter 4. Conclusion}}$ <a class=\"anchor\" id=\"chapter4\"><\/a>\n\n* Training loss continues to drop but validation loss do not after a certain epcohs. That means we have an overfitting problem.\n* Still the creating image logic can be changed\n* I have max of 7.89 RMSE score so that it seem not successfull\n* Upvote if you like","631d178d":"## Submission ","038b0445":"* Test - Train split as %30 of the data for test","6ea7324b":"## $\\color{Pink}{\\text{Chapter 1. Introduction}}$ <a class=\"anchor\" id=\"chapter1\"><\/a>","7b441e36":"* Standard scale for the scaling the features, it is nothing but substracting the mean and dividing the standard deviation of the corresponding feature","749e0fe3":"![kaggle.png](attachment:1257a057-1bfc-4e80-8117-a0f7cbf2ea0c.png)","1e475f4b":"* Test head is:","f21d5d4d":"## $\\color{Pink}{\\text{Table of Contents}}$\n\n* [Chapter 1. Introduction](#chapter1)     \n* [Chapter 2. Creating Monochrome Image](#chapter2)\n* [Chapter 3. AlexNet CNN Training](#chapter3)\n* [Chapter 4. Conclusion](#chapter4)\n\n\n\n\n\n ##### ****$\\color{pink}{\\text{If You like my work, Please upvote!}}$****","db963eea":"* Create X ( train ) and y (label) out of train dataset and get rid of the first feature \"id\"","6efc9969":"Dear friends,\n\nI would like to experiment if the tabular playground series Aug 2021 data belongs to images dataset or not. So that i would like to share this effort with the community. However there are lost of \"Magic\" notebook and competition submissions which only using blending at the high places. Still, Kaggle is for Machine Learning and everybody is free to do whatever they want.\n\nAnyway, i will take the data with 100 features and will create images 10x10x1. So that our dataset will be capable for the basic CNN architectures like AlexNet or VGG16. \n\nIf you enjoy please upvote!! ","81cf3c26":"* Since we have Regression Problem, we changed the last activation neuron from softmax to linear. That's it !\n* Our loss is 'mean_squared_error', optimizer is 'adam'and metric is 'mse' again.\n* We have callbacks as early stopping with patience of 5\n* I use small filter sizes in convolutions since the data is small 10x10, even 7 times smaller than MNIST data which is 28x28!\n* I use batch normalization since i have a huge batch size\n* I use dropout to avoid overfitting\n* Activation layers are \"ReLu\"\n* Padding is \"Same\""}}