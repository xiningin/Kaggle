{"cell_type":{"165266e2":"code","8a68742e":"code","af65bacc":"code","f6305645":"code","e29d9269":"code","6ac1de30":"code","4696a526":"code","0ec868e6":"code","399682cd":"code","4f21fe71":"code","d0613815":"code","6d1ff4d9":"code","55a5bd6f":"code","53835783":"code","02531bad":"code","ebd26ddb":"markdown","00a228db":"markdown","cadffaf2":"markdown","2ce8833d":"markdown","5f76a2ff":"markdown"},"source":{"165266e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8a68742e":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nframe = (train, test)\ndf1 = pd.concat(frame, keys = ['train','test'])","af65bacc":"df1.corr().sum().sort_values(ascending=False)","f6305645":"df2 = df1.sort_values(by = 'GrLivArea')\ndf3 = df2.isnull().sum().sort_values(ascending= False)\n\n#Find values with NaN\/missing values\ndf3 = df2.isnull().sum().sort_values(ascending= False)\ndf3.head(40)\n\n","e29d9269":"# First lets fill columns that have less than 50% NaN\/missing values \n\nfor column in ['LotFrontage', 'GarageQual', 'GarageYrBlt', 'GarageFinish',\n       'GarageCond', 'GarageType', 'BsmtExposure', 'BsmtCond', 'BsmtQual',\n       'BsmtFinType2', 'BsmtFinType1', 'MasVnrType', 'MasVnrArea', 'MSZoning',\n       'Utilities', 'Functional', 'BsmtFullBath', 'BsmtHalfBath', 'GarageArea',\n       'BsmtFinSF2', 'Exterior1st', 'TotalBsmtSF', 'GarageCars', 'BsmtUnfSF',\n       'Electrical','BsmtFinSF1','KitchenQual','SaleType','Exterior2nd']:\n    df2[column].fillna(method='ffill',inplace=True)","6ac1de30":"# Select the top 6 from the list above and drop em since they have over 50% NaN\/missing values\n\ndf3.index[0:6]\ndf2.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'],axis=1, inplace=True)","4696a526":"df2.columns","0ec868e6":"# Choose as many numerical features\n\nfig = plt.figure(figsize=(12,5)) \n\nfig.add_subplot(221)            \nplt.scatter(df2.TotalBsmtSF, y=df2.SalePrice)\n\nfig.add_subplot(222)         \nplt.scatter(df2.LotArea, y=df2.SalePrice)               \n\nfig.add_subplot(223)            \nplt.scatter(df2.LotFrontage, y=df2.SalePrice)\n\nfig.add_subplot(224)            \nplt.scatter(df2.MasVnrArea, y=df2.SalePrice)\n\nplt.show()                      ","399682cd":"# Drop outliers\n\ncolumns1 = ['LotFrontage','LotArea','TotalBsmtSF','MasVnrArea']\ndf2[columns1].sort_values(by = columns1,ascending=False)[0:4]\n\ndf2.drop(df2[df2['Id'] == 1298].index, inplace = True)\ndf2.drop(df2[df2['Id'] == 386].index, inplace = True)\ndf2.drop(df2[df2['Id'] == 934].index, inplace = True)\ndf2.drop(df2[df2['Id'] == 313].index, inplace = True)\n","4f21fe71":"# Convert Categorical features to dummy\/numerical variables\ndf2_dummy = pd.get_dummies(df2)","d0613815":"Id =  df2_dummy.loc['test']\ndftest = df2_dummy.loc['test']\ndftrain = df2_dummy.loc['train']\nId = Id.Id","6d1ff4d9":"# Drop any NaN values\ndftrain.dropna(axis = 0, inplace = True)\ndftest.drop(['SalePrice','Id'], axis =1, inplace=True)","55a5bd6f":"from sklearn.model_selection import train_test_split\nimport xgboost \n\ndftrain_X = dftrain.drop(['SalePrice','Id'], axis = 1)\ndftrain_y = dftrain['SalePrice']","53835783":"# I used hypertuning to get the optimal parameters. Left it out for simplicity.\n\nxgboost = xgboost.XGBRegressor(learning_rate=0.05,  \n                      colsample_bytree = 0.5,\n                      subsample = 0.8,\n                      n_estimators=1000, \n                      max_depth=5, \n                      gamma=5)\n\nxgboost.fit(dftrain_X, dftrain_y)\n","02531bad":"# Save it\npredictions= xgboost.predict(dftest)\nsolution = pd.DataFrame({\"Id\": Id, 'SalePrice': predictions})\nsolution.to_csv('house_pred.csv', index=False)","ebd26ddb":"# Finding and Dropping Outliers","00a228db":"# Merge train and test data","cadffaf2":"# Split the Test and Train data we merged in the beginning","2ce8833d":"# Fill in NaN values by sorting feature of highest Corr()","5f76a2ff":"# Creating and fitting XGBOOST Regressor model"}}