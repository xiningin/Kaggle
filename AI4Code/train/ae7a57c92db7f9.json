{"cell_type":{"66c84fde":"code","1d76ca55":"code","531d7e72":"code","8836a0c7":"code","8047b5e7":"code","d044dade":"code","8a35ad50":"code","7e4e0d26":"code","b2efea04":"code","b85dc856":"code","60de30d8":"code","bb0e95fc":"code","cd048f43":"code","c300edf7":"markdown","fff0d4b6":"markdown","9e8c0368":"markdown"},"source":{"66c84fde":"# importing libraries and using inbuilt dataset for this process.\nfrom sklearn.datasets import load_diabetes  \n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split","1d76ca55":"X,y = load_diabetes(return_X_y=True)","531d7e72":"print(X.shape)\nprint(y.shape)","8836a0c7":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)  # train test split","8047b5e7":"reg = LinearRegression()\nreg.fit(X_train,y_train)","d044dade":"print(reg.coef_)\nprint(reg.intercept_)","8a35ad50":"y_pred = reg.predict(X_test)\nr2_score(y_test,y_pred)","7e4e0d26":"X_train.shape","b2efea04":"class GDRegressor:\n    \n    def __init__(self,learning_rate=0.01,epochs=100):\n        \n        self.coef_ = None\n        self.intercept_ = None\n        self.lr = learning_rate\n        self.epochs = epochs\n        \n    def fit(self,X_train,y_train):\n        # init your coefficients from X_train because coefficients are equal to the number of features\n        self.intercept_ = 0\n        self.coef_ = np.ones(X_train.shape[1]) # [1] take the columns from (353, 10) <-- shape of X_train\n        \n        for i in range(self.epochs):\n            # update all the coefficients and the intercept\n            y_hat = np.dot(X_train,self.coef_) + self.intercept_\n            #print(\"Shape of y_hat\",y_hat.shape)\n            intercept_der = -2 * np.mean(y_train - y_hat)\n            self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n            \n            coef_der = -2 * np.dot((y_train - y_hat),X_train)\/X_train.shape[0]\n            self.coef_ = self.coef_ - (self.lr * coef_der)\n        \n        print(self.intercept_,self.coef_)\n    \n    def predict(self,X_test):\n        return np.dot(X_test,self.coef_) + self.intercept_","b85dc856":"gdr = GDRegressor(epochs=1000,learning_rate=0.5)  # building model","60de30d8":"gdr.fit(X_train,y_train)  # fitting data","bb0e95fc":"y_pred = gdr.predict(X_test)  # testing on X_test","cd048f43":"r2_score(y_test,y_pred)  # improved r2_score, changing the learning rate will tune r2_score","c300edf7":"# In this notebook we'll apply batch gradient descent from scratch.\n\n## Gradient Descent\n**The goal of the gradient descent is to minimise a given function which, in our case, is the loss function of the mode. To achieve this goal, it performs two steps iteratively.**\n1. Compute the slope (gradient) that is the first-order derivative of the function at the current point\n2. Move-in the opposite direction of the slope increase from the current point by the computed amount\n\n![](https:\/\/miro.medium.com\/max\/875\/1*P7z2BKhd0R-9uyn9ThDasA.png)\n\n## Batch Gradient Descent\n**In Batch Gradient Descent, all the training data is taken into consideration to take a single step. We take the average of the gradients of all the training examples and then use that mean gradient to update our parameters. So that\u2019s just one step of gradient descent in one epoch.**\n***Batch Gradient Descent is great for convex or relatively smooth error manifolds.***\n\n![](https:\/\/miro.medium.com\/max\/735\/1*44QbDJ9gJvw8tXtHNVLoCA.png)\n\n*Documentation reference is from www.towardsdatascience.com","fff0d4b6":"## Updating intercept \n![](https:\/\/miro.medium.com\/max\/1192\/0*VHSZPjAofkxFk3i2.png)\n\n**Step 1**\n\nFor Calculating y hat we'll use matrix multiplication(dot product) of coefficint(matrix) and X_train(matrix).\n\n**Step2**\n\nNow, using y hat calculate the derivative which is the mean of (y_train - y hat) which is clearly shown in formula in above image.\n\n**Step 3**\n\nNow, we've calculated the derivative of the loss function w.r.t to intercept. So, new intercept will be (i_old - learning_rate*intercept_derivative)\n\n## Updating Coefficients\n\n**Step 1**\n\nWe know that number of coefficient is equal to the number of features, so we have to calculate that many derivatives in one run.\nBy multiplying whole X_train in form of matrix using dot product with each derivative in above image will do the work.\n\n**Step 2**\n\n(coef_old - learning_rate*coef_derivative)\n\nNow we have a vector of coef_derivatives easily\n\n***IMPLEMENTATION*** ","9e8c0368":"## R2 Score\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/6b863cb70dd04b45984983cb6ed00801d5eddc94)\n\n*R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression. ... 100% indicates that the model explains all the variability of the response data around its mean*"}}