{"cell_type":{"ef643b4e":"code","822d830c":"code","c28afb08":"code","dca88481":"code","b41e331f":"code","b190106b":"code","e44407ba":"code","5df2f70d":"code","1877e88e":"code","f86647da":"code","35fdddaa":"code","7145d340":"code","2c5301f1":"code","876c5418":"code","79808ebc":"code","4e2568a2":"code","96d1bd6e":"code","e00dc9bb":"code","cda75301":"code","2cdb90bb":"code","6592aabd":"code","cc89972a":"code","4362d7ce":"code","fc3548c9":"code","5947cf95":"code","bd5c10d4":"code","3856bc3a":"code","0ab023b8":"code","9a102bc1":"code","7b26ad9c":"code","2872facc":"code","7ad4b3a6":"code","17701106":"code","79f420fb":"code","dbb1c739":"code","b3107f67":"code","b9093497":"code","f57bc8c4":"code","3f6bc882":"code","9adad5a5":"code","b46d7899":"code","6f5a8844":"code","bf9942a0":"code","a081cc96":"code","07ea354a":"code","e3f41e4b":"code","f5e378ae":"code","8490a41c":"code","41c66924":"code","19a73ae5":"code","b1e7d19d":"markdown","cbf582dd":"markdown","6069b386":"markdown","591250ba":"markdown","3349a9d9":"markdown","bb83b79c":"markdown"},"source":{"ef643b4e":"import numpy as np\nimport pandas as pd\nimport json","822d830c":"def load_json_df(filename, num_bytes = -1):\n    '''Load the first `num_bytes` of the filename as a json blob, convert each line into a row in a Pandas data frame.'''\n    fs = open(filename)\n    df = pd.DataFrame([json.loads(x) for x in fs.readlines(num_bytes)])\n    fs.close()\n    return df","c28afb08":"biz_df = load_json_df('..\/input\/yelp-dataset\/yelp_academic_dataset_business.json')\n#biz_df = pd.DataFrame([json.loads(x) for x in biz_file.readlines()])\nbiz_df.head()","dca88481":"# too many memory\n# user_df = load_json_df('..\/input\/yelp-dataset\/yelp_academic_dataset_user.json')\n# user_df.head()","b41e331f":"# too many memory\n# reviews_df = load_json_df('..\/input\/yelp-dataset\/yelp_academic_dataset_review.json')\n# reviews_df.head()","b190106b":"### 02.03-06_Counts","e44407ba":"#Example 2-3. Quantizing counts with fixed-width bins\n\n# import numpy as np\n# Generate 20 random integers uniformly between 0 and 99\nsmall_counts = np.random.randint(0, 100, 20)\nsmall_counts","5df2f70d":"# Map to evenly spaced bins 0-9 by division\nnp.floor_divide(small_counts, 10)","1877e88e":"# An array of counts that span several magnitudes\nlarge_counts = [296, 8286, 64011, 80, 3, 725, 867, 2215, 7689, 11495, 91897, 44, 28, 7971, 926, 122, 22222]","f86647da":"# Map to exponential-width bins via the log function\nnp.floor(np.log10(large_counts))","35fdddaa":"# Example 2-5. Binning counts by quantiles\n\n# Map the counts to quartiles\npd.qcut(large_counts, 4, labels=False)","7145d340":"# Compute the quantiles themselves\nlarge_counts_series = pd.Series(large_counts)\nlarge_counts_series.quantile([0.25, 0.5, 0.75])","2c5301f1":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#%matplotlib notebook","876c5418":" %matplotlib inline","79808ebc":"sns.set_style('whitegrid')\nfig, ax = plt.subplots()\nbiz_df['review_count'].hist(ax=ax, bins=100) \nax.set_yscale('log')\nax.tick_params(labelsize=14)\nax.set_xlabel('Review Count', fontsize=14) \nax.set_ylabel('Occurrence', fontsize=14)","4e2568a2":"deciles = biz_df['review_count'].quantile([.1, .2, .3, .4, .5, .6, .7, .8, .9])\ndeciles","96d1bd6e":"# Visualize the deciles on the histogram\nsns.set_style('whitegrid')\nfig, ax = plt.subplots()\nbiz_df['review_count'].hist(ax=ax, bins=100)\nfor pos in deciles:\n    handle = plt.axvline(pos, color='r')\nax.legend([handle], ['deciles'], fontsize=14)\nax.set_yscale('log')\nax.set_xscale('log')\nax.tick_params(labelsize=14)\nax.set_xlabel('Review Count', fontsize=14)\nax.set_ylabel('Occurrence', fontsize=14)","e00dc9bb":"y = np.arange(0.00001, 3, 0.01)\nx = np.power(10, y)","cda75301":"fig, ax = plt.subplots()\nplt.plot(x, y, 'b')\nax.tick_params(labelsize=14)\nax.set_xlabel('x', fontsize=14)\nax.set_ylabel('log10(x)', fontsize=14)","2cdb90bb":"log_review_count = np.log(biz_df['review_count'])\n# biz_df.columns = biz_df.columns.str.replace(' ', '')","6592aabd":"plt.figure()\nax = plt.subplot(2,1,1)\nbiz_df['review_count'].hist(ax=ax, bins=100)\nax.tick_params(labelsize=14)\nax.set_xlabel('review_count', fontsize=14)\nax.set_ylabel('Occurrence', fontsize=14)\n\nax = plt.subplot(2,1,2)\nlog_review_count.hist(ax=ax, bins=100)\nax.tick_params(labelsize=14)\nax.set_xlabel('log10(review_count)', fontsize=14)\nax.set_ylabel('Occurrence', fontsize=14)","cc89972a":"biz_df.head()","4362d7ce":"# Using the previously loaded Yelp reviews DataFrame,\n# compute the log transform of the Yelp review count.\n# Note that we add 1 to the raw count to prevent the logarithm from \n# exploding into negative infinity in case the count is zero.\nbiz_df['log_review_count'] = np.log10(biz_df['review_count'] + 1) ","fc3548c9":"!pip install -U scikit-learn","5947cf95":"# import sklearn\n# from sklearn import linear_model\n# from sklearn.sklearn.linear_model.LinearRegression import train_test_split\n# biz_train, biz_validate = cross_validation.train_test_split(biz_df, test_size=0.2)","bd5c10d4":"from sklearn import linear_model\nfrom sklearn.model_selection import cross_val_score ","3856bc3a":"# Train linear regression models to predict the average star rating of a business, \n# using the review_count feature with and without log transformation.\n# Compare the 10-fold cross validation score of the two models.\nm_orig = linear_model.LinearRegression()\nscores_orig = cross_val_score(m_orig, biz_df[['review_count']],\n                             biz_df['stars'], cv=10)\nm_log = linear_model.LinearRegression()\nscores_log = cross_val_score(m_log, biz_df[['log_review_count']],\n                             biz_df['stars'], cv=10)\nprint(\"R-squared score without log transform: %0.5f (+\/- %0.5f)\"\n% (scores_orig.mean(), scores_orig.std() * 2))\nprint(\"R-squared score with log transform: %0.5f (+\/- %0.5f)\"\n% (scores_log.mean(), scores_log.std() * 2))","0ab023b8":"loan_df = pd.read_csv('..\/input\/loandata\/Loan payments data.csv')\nloan_df.head()","9a102bc1":"plt.figure()\nloan_df['paid_off_time'].hist(bins=100)","7b26ad9c":"df = pd.read_csv('..\/input\/uci-online-news-popularity-data-set\/OnlineNewsPopularity.csv', delimiter=', ')\n\n# Take the log transform of the 'n_tokens_content' feature, which \n# represents the number of words (tokens) in a news article.\ndf['log_n_tokens_content'] = np.log10(df['n_tokens_content'] + 1)\n\ndf.head()","2872facc":"# Train two linear regression models to predict the number of shares\n# of an article, one using the original feature and the other the\n# log transformed version.\u201d\nm_orig = linear_model.LinearRegression()\nscores_orig = cross_val_score(m_orig, df[['n_tokens_content']],\n                             df['shares'], cv=10)\nm_log = linear_model.LinearRegression()\nscores_log = cross_val_score(m_log, df[['log_n_tokens_content']],\n                             df['shares'], cv=10)\nprint(\"R-squared score without log transform: %0.5f (+\/- %0.5f)\"\n% (scores_orig.mean(), scores_orig.std() * 2))\nprint(\"R-squared score with log transform: %0.5f (+\/- %0.5f)\"\n% (scores_log.mean(), scores_log.std() * 2))","7ad4b3a6":"fig, (ax1, ax2) = plt.subplots(2,1)\ndf['n_tokens_content'].hist(ax=ax1, bins=100)\nax1.tick_params(labelsize=14)\nax1.set_xlabel('Number of Words in Article', fontsize=14)\nax1.set_ylabel('Number of Articles', fontsize=14)\n\ndf['log_n_tokens_content'].hist(ax=ax2, bins=100)\nax2.tick_params(labelsize=14)\nax2.set_xlabel('Log of Number of Words', fontsize=14)\nax2.set_ylabel('Number of Articles', fontsize=14)","17701106":"plt.figure()\nax1 = plt.subplot(2,1,1)\nax1.scatter(df['n_tokens_content'], df['shares'])\nax1.tick_params(labelsize=14)\nax1.set_xlabel('Number of Words in Article', fontsize=14)\nax1.set_ylabel('Number of Shares', fontsize=14)\n\nax2 = plt.subplot(2,1,2)\nax2.scatter(df['log_n_tokens_content'], df['shares'])\nax2.tick_params(labelsize=14)\nax2.set_xlabel('Log of the Number of Words in Article', fontsize=14)\nax2.set_ylabel('Number of Shares', fontsize=14)","79f420fb":"fig, (ax1, ax2) = plt.subplots(2,1)\nax1.scatter(biz_df['review_count'], biz_df['stars'])\nax1.tick_params(labelsize=14)\nax1.set_xlabel('Review Count', fontsize=14)\nax1.set_ylabel('Average Star Rating', fontsize=14)\n\nax2.scatter(biz_df['log_review_count'], biz_df['stars'])\nax2.tick_params(labelsize=14)\nax2.set_xlabel('Log of Review Count', fontsize=14)\nax2.set_ylabel('Average Star Rating', fontsize=14)\n","dbb1c739":"### 02.12-14_Box-Cox_Transform","b3107f67":"from scipy import stats\n\n# Continuing from the previous example, assume biz_df contains # the Yelp business reviews data.\n# The Box-Cox transform assumes that input data is positive.\n# Check the min to make sure\nbiz_df['review_count'].min()\n\n# Setting input parameter lmbda to 0 gives us the log transform (without\n# constant offset)\nrc_log = stats.boxcox(biz_df['review_count'], lmbda=0)\n# By default, the scipy implementation of Box-Cox transform finds the lambda \n# parameter that will make the output the closest to a normal distribution\nrc_bc, bc_params = stats.boxcox(biz_df['review_count'])\nbc_params","b9093497":"biz_df['rc_bc'] = rc_bc\nbiz_df['rc_log'] = rc_log","f57bc8c4":"fig, (ax1, ax2, ax3) = plt.subplots(3,1)\n# original review count histogram\nbiz_df['review_count'].hist(ax=ax1, bins=100)\nax1.set_yscale('log')\nax1.tick_params(labelsize=14)\nax1.set_title('Review Counts Histogram', fontsize=14)\nax1.set_xlabel('')\nax1.set_ylabel('Occurrence', fontsize=14)\n# review count after log transform\nbiz_df['rc_log'].hist(ax=ax2, bins=100)\nax2.set_yscale('log')\nax2.tick_params(labelsize=14)\nax2.set_title('Log Transformed Counts Histogram', fontsize=14)\nax2.set_xlabel('')\nax2.set_ylabel('Occurrence', fontsize=14)# review count after optimal Box-Cox transform\nbiz_df['rc_bc'].hist(ax=ax3, bins=100)\nax3.set_yscale('log')\nax3.tick_params(labelsize=14)\nax3.set_title('Box-Cox Transformed Counts Histogram', fontsize=14)\nax3.set_xlabel('')\nax3.set_ylabel('Occurrence', fontsize=14)","3f6bc882":"fig.savefig('box-cox-hist.jpg')","9adad5a5":"fig2, (ax1, ax2, ax3) = plt.subplots(3,1)\nprob1 = stats.probplot(biz_df['review_count'], dist=stats.norm, plot=ax1)\nax1.set_xlabel('')\nax1.set_title('Probplot against normal distribution')\nprob2 = stats.probplot(biz_df['rc_log'], dist=stats.norm, plot=ax2)\nax2.set_xlabel('')\nax2.set_title('Probplot after log transform')\nprob3 = stats.probplot(biz_df['rc_bc'], dist=stats.norm, plot=ax3)\nax3.set_xlabel('Theoretical quantiles')\nax3.set_title('Probplot after Box-Cox transform')","b46d7899":"fig2.savefig('box-cox-probplot.jpg')","6f5a8844":"### 02.12_Power_Transforms.ipynb\nx = np.arange(0.001, 3, 0.01)\nlambda0 = np.log(x)\none_quarter = (x**0.25 - 1) \/ 0.25\nsquare_root = (x**0.5 - 1) \/ 0.5\nthree_quarters = (x**0.75 - 1) \/ 0.75\none_point_five = (x**1.5 - 1) \/ 1.5\n\nfig, ax = plt.subplots()\nplt.plot(x, lambda0, 'c', \n         x, one_quarter, 'r--', \n         x, square_root, 'g-.', \n         x, three_quarters, 'b:',\n         x, one_point_five, 'k')\nplt.legend(['lambda = 0', 'lambda = 0.25', 'lambda = 0.5', 'lambda = 0.75', 'lambda = 1.5'], \n           loc='lower right')\nax.tick_params(labelsize=14)\nax.set_xlabel('x', fontsize=14)\nax.set_ylabel('y', fontsize=14)\nax.set_title('Box-Cox Transforms', fontsize=14)","bf9942a0":"### 02.17_Interaction_Features_Example\n\nimport sklearn.preprocessing as preproc\n# Look at the original data - the number of words in an article\n\ndf['n_tokens_content'].to_numpy()\n\n# Min-max scaling\ndf['minmax'] = preproc.minmax_scale(df[['n_tokens_content']])\ndf['minmax'].to_numpy() #.as_matrix()\n\n# Standardization - note that by definition, some outputs will be negative\ndf['standartized'] = preproc.StandardScaler().fit_transform(df[['n_tokens_content']])\ndf['standartized'].to_numpy()\n\n# L2-normalization\ndf['l2_normalized'] = preproc.normalize(df[['n_tokens_content']], axis=0)\ndf['l2_normalized'].to_numpy()","a081cc96":"# Example 2-16. Plotting the histograms of original and scaled data\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(4,1) \nfig.tight_layout()\ndf['n_tokens_content'].hist(ax=ax1, bins=100) \nax1.tick_params(labelsize=14)\nax1.set_xlabel('Article word count', fontsize=14) \nax1.set_ylabel('Number of articles', fontsize=14)\n\ndf['minmax'].hist(ax=ax2, bins=100)\nax2.tick_params(labelsize=14)\nax2.set_xlabel('Min-max scaled word count', fontsize=14) \nax2.set_ylabel('Number of articles', fontsize=14)\n\ndf['standartized'].hist(ax=ax3, bins=100)\nax3.tick_params(labelsize=14)\nax3.set_xlabel('Standartized word count', fontsize=14) \nax3.set_ylabel('Number of articles', fontsize=14)\n\ndf['l2_normalized'].hist(ax=ax4, bins=100)\nax4.tick_params(labelsize=14)\nax4.set_xlabel('L2-normalized word count', fontsize=14) \nax4.set_ylabel('Number of articles', fontsize=14)","07ea354a":"# \u201cExample 2-17. Example of interaction features in prediction\u201d\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nimport sklearn.preprocessing as preproc\n\ndf.columns","e3f41e4b":"# Select the content-based features as singleton features in the model, \n# skipping over the derived features\nfeatures = ['n_tokens_title', 'n_tokens_content',\n    'n_unique_tokens', 'n_non_stop_words', 'n_non_stop_unique_tokens',\n    'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos',\n    'average_token_length', 'num_keywords', 'data_channel_is_lifestyle',\n    'data_channel_is_entertainment', 'data_channel_is_bus',\n    'data_channel_is_socmed', 'data_channel_is_tech',\n    'data_channel_is_world']\n\nX = df[features]\ny = df[['shares']]\n\n# Create pairwise interaction features, skipping the constant bias term\nX2 = preproc.PolynomialFeatures(include_bias=False).fit_transform(X)\nX2.shape","f5e378ae":"def evaluate_feature(X_train, X_test, y_train, y_test):\n    \"\"\"Fit a linear regression model on the training set \n    and score on the test set\"\"\"\n    model = linear_model.LinearRegression()\n    model.fit(X_train, y_train)\n    r_score = model.score(X_test, y_test)\n    return (model, r_score)","8490a41c":"# Create train\/test sets for both feature sets\nX1_train, X1_test, X2_train, X2_test, y_train, y_test = \\\ntrain_test_split(X, X2, y, test_size=0.3, random_state=17)","41c66924":"# Create train\/test sets for both feature sets\nX1_train, X1_test, X2_train, X2_test, y_train, y_test = \\\ntrain_test_split(X, X2, y, test_size=0.3, random_state=17)","19a73ae5":"# Train models and compare score on the two feature sets\n(m1, r1) = evaluate_feature(X1_train, X1_test, y_train, y_test)\n(m2, r2) = evaluate_feature(X2_train, X2_test, y_train, y_test)\nprint(\"R-squared score with singleton features: %0.5f\" % r1)\nprint(\"R-squared score with pairwise features: %0.10f\" % r2)","b1e7d19d":"## Visualize the correlation between the input and the output","cbf582dd":"## Kaggle loan data","6069b386":"## Log Transform on Online News Popularity Dataset","591250ba":"## Plot the distribution of number of tokens with and without log transform","3349a9d9":"## <font color=\"green\">Stay tuned and don't forget to <b>UPVOTE<\/b> this kernel =)<\/font>","bb83b79c":"This kernel accompanies code from 2nd chapter of [\"Feature Engineering for Machine Learning,\" by Alice Zheng and Amanda Casari. O'Reilly, 2018.](https:\/\/www.oreilly.com\/library\/view\/feature-engineering-for\/9781491953235\/).\nThere are a lot of interesting & simple Feature Engineering approaches, which you can apply in your problems, or even Kaggle competitions.\n\nThe repo does not contain the data from book because I don't have rights to disseminate them. So, I used datasets available on Kaggle. \nPlease follow the URLs given in the book to download the data.\n\nAs a result, the final solutions may differ from the originals in the book. \nBut that doesn't change the techniques described from the book, which I wrote here, right?    \n\n## <font color=\"green\">If this notebook were useful for you, please <b>UPVOTE<\/b> it =)<\/font>"}}