{"cell_type":{"91253930":"code","73e95ca4":"code","2492bc72":"code","f2bfe233":"code","24dba003":"code","49c35f1d":"code","e9c13bb0":"code","cde92c6a":"code","75c9f7ab":"code","3b066e53":"code","729cc6af":"code","7e4028e1":"code","37f380f9":"code","734489e3":"markdown","d17cdf0d":"markdown","12758c6d":"markdown","3920dd6e":"markdown","0994a923":"markdown","c8e777a2":"markdown","44b03cc9":"markdown","5cf2aca6":"markdown","0c4f0896":"markdown","e8848964":"markdown","a09a534e":"markdown","bf0bab6f":"markdown"},"source":{"91253930":"import numpy as np\nimport pandas as pd\nimport math\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, BatchNormalization, Conv2D, Dense, MaxPool2D, Flatten, Activation, Dropout, Lambda,Layer\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.python.keras.utils import tf_utils","73e95ca4":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","2492bc72":"label = train['label'].values\ny_tr = label[:40000]\ny_val = label[40000:]\n\nX_train = train.drop('label', axis = 1)\nX_tr = X_train.values.reshape((42000, 28, 28, 1))[:40000]\/255.0\nX_val = X_train.values.reshape((42000, 28, 28, 1))[40000:]\/255.0","f2bfe233":"# building basic CNN architecture\nmodel = tf.keras.models.Sequential()\nmodel.add(Conv2D(input_shape=(28,28,1),filters=10, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=10, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Conv2D(filters=20, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=20, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(50, activation = 'relu'))\nmodel.add(Dense(3, activation = 'linear'))\nmodel.add(Lambda(lambda x: K.l2_normalize(x,axis=1)))\nmodel.add(Dense(10, activation = 'softmax'))\n\nmodel.summary()","24dba003":"# model compile and fitting\nmodel.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              optimizer = 'adam', metrics = ['accuracy'])\nmodel.fit(X_tr, y_tr, validation_data = (X_val, y_val),\n          epochs = 1000, batch_size = 32,\n          callbacks = tf.keras.callbacks.EarlyStopping(patience = 3, monitor = 'val_accuracy'))","49c35f1d":"# feature extractor trained with Cross Entropy loss\nFE_CE = tf.keras.models.Model(model.layers[0].input, model.layers[-2].output)\n\n# feature extraction on validation set\nCE_feats = FE_CE.predict(X_val)\n\n# dataframe for visualization\nCE_df = pd.DataFrame(np.hstack([CE_feats, y_val.reshape(-1,1)]), columns = ['x','y','z','label'])\nCE_df = CE_df.sort_values('label', ascending = True)\nCE_df['label'] = CE_df['label'].astype(int).astype(str)","e9c13bb0":"# extract class center from weight matrix columns of last layer \ncenters = []\nfor i in range(10):\n    centers.append(model.layers[-1].weights[0].numpy()[:,i])\ncenters = np.array(centers)","cde92c6a":"def _resolve_training(layer, training):\n    if training is None:\n        training = K.learning_phase()\n    if isinstance(training, int):\n        training = bool(training)\n    if not layer.trainable:\n        # When the layer is not trainable, override the value\n        training = False\n    return training\n\nclass ArcFace(Layer):\n    \"\"\"\n    Implementation of ArcFace layer. Reference: https:\/\/arxiv.org\/abs\/1801.07698\n    \n    Arguments:\n      num_classes: number of classes to classify\n      s: scale factor\n      m: margin\n      regularizer: weights regularizer\n    \"\"\"\n    def __init__(self,\n                 num_classes,\n                 s=30.0,\n                 m=0.5,\n                 regularizer=None,\n                 name='arcface',\n                 **kwargs):\n        \n        super().__init__(name=name, **kwargs)\n        self._n_classes = num_classes\n        self._s = float(s)\n        self._m = float(m)\n        self._regularizer = regularizer\n\n    def build(self, input_shapes):\n        embedding_shape, label_shape = input_shapes\n        self._w = self.add_weight(shape=(embedding_shape[-1], self._n_classes),\n                                  initializer='glorot_uniform',\n                                  trainable=True,\n                                  regularizer=self._regularizer,\n                                  name='cosine_weights')\n    def call(self, inputs, training=None):\n        \"\"\"\n        During training, requires 2 inputs: embedding (after backbone+pool+dense),\n        and ground truth labels. The labels should be sparse (and use\n        sparse_categorical_crossentropy as loss).\n        \"\"\"\n        embedding, label = inputs\n\n        # Squeezing is necessary for Keras. It expands the dimension to (n, 1)\n        label = tf.reshape(label, [-1], name='label_shape_correction')\n\n        # Normalize features and weights and compute dot product\n        x = tf.nn.l2_normalize(embedding, axis=1, name='normalize_prelogits')\n        w = tf.nn.l2_normalize(self._w, axis=0, name='normalize_weights')\n        cosine_sim = tf.matmul(x, w, name='cosine_similarity')\n\n        training = _resolve_training(self, training)\n        if not training:\n            # We don't have labels if we're not in training mode\n            return self._s * cosine_sim\n        else:\n            one_hot_labels = tf.one_hot(label,\n                                        depth=self._n_classes,\n                                        name='one_hot_labels')\n            theta = tf.math.acos(K.clip(\n                    cosine_sim, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n            selected_labels = tf.where(tf.greater(theta, math.pi - self._m),\n                                       tf.zeros_like(one_hot_labels),\n                                       one_hot_labels,\n                                       name='selected_labels')\n            final_theta = tf.where(tf.cast(selected_labels, dtype=tf.bool),\n                                   theta + self._m,\n                                   theta,\n                                   name='final_theta')\n            output = tf.math.cos(final_theta, name='cosine_sim_with_margin')\n            return self._s * output","75c9f7ab":"input = Input(shape = (28,28,1), name = 'image')\nlabel = Input(shape = (10,), dtype = np.int32)\nx = Conv2D(filters = 10, kernel_size = (3,3), padding = 'same', activation = 'relu')(input)\nx = Conv2D(filters = 10, kernel_size = (3,3), padding = 'same', activation = 'relu')(x)\nx = MaxPool2D(pool_size=(2,2), strides=(2,2))(x)\n\nx = Conv2D(filters = 20, kernel_size = (3,3), padding = 'same', activation = 'relu')(x)\nx = Conv2D(filters = 20, kernel_size = (3,3), padding = 'same', activation = 'relu')(x)\nx = MaxPool2D(pool_size=(2,2), strides=(2,2))(x)\n\nx = Flatten()(x)\nx = Dense(50, activation = 'relu')(x)\nx = Dense(3, activation = 'linear')(x)\nx = Lambda(lambda x: K.l2_normalize(x,axis=1))(x)\nx = ArcFace(num_classes = 10, s = 30.0, m = 0.8)([x, label])\noutput = Activation('softmax')(x)\n\nmodel_arc = Model([input, label], output)\nmodel_arc.summary()","3b066e53":"# model compile and fitting\nmodel_arc.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              optimizer = 'adam', metrics = ['accuracy'])\nmodel_arc.fit([X_tr, y_tr], y_tr, validation_data = ([X_val, y_val], y_val),\n          epochs = 1000, batch_size = 32,\n          callbacks = tf.keras.callbacks.EarlyStopping(patience = 3, monitor = 'val_accuracy'))","729cc6af":"# feature extractor trained with Cross Entropy loss\nFE_ARC = tf.keras.models.Model(model_arc.layers[0].input, model_arc.layers[-4].output)\n\n# feature extraction on validation set\nARC_feats = FE_ARC.predict(X_val)\n\n# dataframe for visualization\nARC_df = pd.DataFrame(np.hstack([ARC_feats, y_val.reshape(-1,1)]), columns = ['x','y','z','label'])\nARC_df = ARC_df.sort_values('label', ascending = True)\nARC_df['label'] = ARC_df['label'].astype(int).astype(str)","7e4028e1":"fig = px.scatter_3d(CE_df, x='x', y='y', z='z', color='label')\nfig.update_traces(marker=dict(size = 1),\n                  selector=dict(mode='markers'))\nfig.show()","37f380f9":"fig = px.scatter_3d(ARC_df, x='x', y='y', z='z', color='label')\nfig.update_traces(marker=dict(size = 1),\n                  selector=dict(mode='markers'))\nfig.show()","734489e3":"## 2. Visualization of Embeddings trained with ArcFace layer","d17cdf0d":"In this notebook, I'd like to introduce *ArcFace loss*, which results in <b>feature extraction<\/b> with better <b>intra-class compactness<\/b> and <b>inter-class discrepency<\/b>\n\nI will train Convolutional digit recognizer model with both **Cross Entropy Loss** and **ArcFace Loss**, and then visualize extracted features on 3D vector space. \n\n","12758c6d":"## 1. Visualization of Embeddings trained without ArcFace layer","3920dd6e":"# 3. Training with ArcFace Layer","0994a923":"# 2. Training without ArcFace layer","c8e777a2":"## 1. Defining ArcFace layer","44b03cc9":"## 3. Feature Extraction","5cf2aca6":"# 1. Loading Data","0c4f0896":"## 1. Model training","e8848964":"# 3. Embedding Visualization","a09a534e":"## 2. Model Training","bf0bab6f":"## 2. Feature Extraction"}}