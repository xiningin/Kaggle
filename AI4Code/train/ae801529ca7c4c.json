{"cell_type":{"02e75d99":"code","52cc1841":"code","e43ba404":"code","3a934caa":"code","f0c599a6":"code","4983c557":"code","6750df27":"code","516dae78":"code","7c0c14be":"code","34b8f868":"code","b24683f7":"code","c5e9d724":"code","d0f710ae":"code","70bd7e06":"code","8c4f6afd":"code","ce285e3a":"code","2a7f9537":"code","592b4670":"code","ebcc5a2a":"code","c555edd4":"code","1d695192":"code","0dc693b7":"code","44937d6f":"code","3a626af3":"code","2c0bb24b":"code","f3a905e3":"code","036fd29c":"code","e6ca41d5":"code","32a559ed":"code","2ac333ba":"code","b6ddda3f":"code","28a9dd61":"code","01400583":"code","848e15a6":"code","25088ebb":"code","046f3202":"code","fe5bab3d":"code","486c8bd6":"code","41855ffa":"code","308c26d4":"code","fabaa56a":"code","d9934e89":"code","33dbc143":"code","c213b729":"code","693332c5":"code","633fcdde":"code","108f58dc":"code","c1e9b6cd":"code","18265617":"code","062986a2":"code","384a8e81":"code","e02d0d75":"code","06dc9c0d":"code","55340570":"code","00ed071d":"code","9bf3e85e":"code","768e474b":"code","6eceec40":"code","b9b6ac9f":"code","cc4bce00":"code","f0df0853":"markdown","5b690775":"markdown","f2b3b19f":"markdown","e3f99823":"markdown","b2c91ee2":"markdown","d52a0474":"markdown","45f0b62e":"markdown","92326dff":"markdown","7bd80961":"markdown","8aca5bf0":"markdown","fd17c3ed":"markdown","f9854722":"markdown","0a16437d":"markdown","1864f524":"markdown","80dc4ed3":"markdown","e2492b14":"markdown","ddc45147":"markdown","6f15a613":"markdown","678e521f":"markdown","c7f249c5":"markdown","c8046626":"markdown","560b1399":"markdown","29a2ad23":"markdown","793f8b44":"markdown","c09d4656":"markdown","8e83a67c":"markdown","add8da30":"markdown","66cf474f":"markdown"},"source":{"02e75d99":"! pip install eli5","52cc1841":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import make_pipeline\nimport eli5\nfrom scipy import sparse","e43ba404":"train_df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","3a934caa":"train_df.shape","f0c599a6":"train_df.head()","4983c557":"print(\"samples with disaster tweet:\", sum(train_df['target']==1))\nprint(\"samples without disaster tweet:\", sum(train_df['target']==0))","6750df27":"test_df.head()","516dae78":"count_vectorizer = feature_extraction.text.CountVectorizer()\n\n## let's get counts for the first 5 tweets in the data\nexample_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])","7c0c14be":"## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\nprint(example_train_vectors[0].todense().shape)\nprint(example_train_vectors[0].todense())","34b8f868":"train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\ntest_vectors = count_vectorizer.transform(test_df[\"text\"])","b24683f7":"clf = linear_model.RidgeClassifier()","c5e9d724":"scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","d0f710ae":"clf.fit(train_vectors, train_df[\"target\"])","70bd7e06":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","8c4f6afd":"sample_submission[\"target\"] = clf.predict(test_vectors)","ce285e3a":"sample_submission.head()","2a7f9537":"sample_submission.to_csv(\"submission_cv.csv\", index=False)","592b4670":"tfidf_vectorizer = feature_extraction.text.TfidfVectorizer(ngram_range=(1,2), min_df=2, max_df=0.5, use_idf=True)\n\n## let's get counts for the first 5 tweets in the data\nexample_train_vectors = tfidf_vectorizer.fit_transform(train_df[\"text\"][0:5])","ebcc5a2a":"## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\nprint(example_train_vectors[0].todense().shape)\nprint(example_train_vectors[0].todense())","c555edd4":"train_vectors = tfidf_vectorizer.fit_transform(train_df[\"text\"])\ntest_vectors = tfidf_vectorizer.transform(test_df[\"text\"])","1d695192":"clf = linear_model.RidgeClassifier()","0dc693b7":"scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","44937d6f":"clf.fit(train_vectors, train_df[\"target\"])","3a626af3":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","2c0bb24b":"sample_submission[\"target\"] = clf.predict(test_vectors)","f3a905e3":"sample_submission.head()","036fd29c":"sample_submission.to_csv(\"submission_tfidf.csv\", index=False)","e6ca41d5":"train_vectors = tfidf_vectorizer.fit_transform(train_df[\"text\"])\ntest_vectors = tfidf_vectorizer.transform(test_df[\"text\"])","32a559ed":"clf = RandomForestClassifier(class_weight=\"balanced\")","2ac333ba":"scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","b6ddda3f":"clf.fit(train_vectors, train_df[\"target\"])","28a9dd61":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsample_submission[\"target\"] = clf.predict(test_vectors)\nsample_submission.to_csv(\"submission_tfidf_rf.csv\", index=False)","01400583":"tfidf_vectorizer = feature_extraction.text.TfidfVectorizer(ngram_range=(1,2), use_idf=True, max_features= 2000)","848e15a6":"train_vectors = tfidf_vectorizer.fit_transform(train_df[\"text\"])","25088ebb":"# Project the tfidf vectors onto the first N principal components.\n# Though this is significantly fewer features than the original tfidf vector,\n# they are stronger features, and the accuracy is higher.\nsvd = TruncatedSVD(1000)\nlsa = make_pipeline(svd, Normalizer(copy=False))\n\n# Run SVD on the training data, then project the training data.\ntrain_vectors = lsa.fit_transform(train_vectors)\n\nexplained_variance = svd.explained_variance_ratio_.sum()\nprint(\"  Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))","046f3202":"test_vectors = tfidf_vectorizer.fit_transform(test_df[\"text\"])\ntest_vectors = lsa.fit_transform(test_vectors)","fe5bab3d":"clf = linear_model.RidgeClassifier()","486c8bd6":"scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","41855ffa":"clf.fit(train_vectors, train_df[\"target\"])","308c26d4":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","fabaa56a":"sample_submission[\"target\"] = clf.predict(test_vectors)","d9934e89":"sample_submission.head()","33dbc143":"sample_submission.to_csv(\"submission_tfidf_lsa.csv\", index=False)","c213b729":"tfidf_vectorizer = feature_extraction.text.TfidfVectorizer(ngram_range=(1,2), min_df=2, max_df=0.5, use_idf=True)","693332c5":"tfidf_vectorizer.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_vectors = tfidf_vectorizer.transform(train_df['text'].values.tolist())\ntest_vectors = tfidf_vectorizer.transform(test_df['text'].values.tolist())","633fcdde":"train_vectors","108f58dc":"train_vectors=train_vectors.toarray()\ntest_vectors=test_vectors.toarray()","c1e9b6cd":"train_vectors.shape","18265617":"from wordcloud import WordCloud, STOPWORDS\nimport string\nstopwords = set(STOPWORDS)\nmore_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown', '-', '...', '|', '&', '?', '??', 'via', '2'}\nstopwords = stopwords.union(more_stopwords)\n## Number of words in the text ##\ntrain_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\ntest_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\ntest_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\ntest_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\n\n## Number of punctuations in the text ##\ntrain_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","062986a2":"for col in ['num_words', 'num_unique_words', 'num_chars', 'num_stopwords', 'num_punctuations', 'num_words_upper', 'num_words_title', 'mean_word_len']:\n  train_vectors = np.append(train_vectors, train_df[col].values.reshape(-1,1), axis=1)\n  test_vectors = np.append(test_vectors, test_df[col].values.reshape(-1,1), axis=1)","384a8e81":"train_vectors = sparse.csr_matrix(train_vectors)\ntest_vectors = sparse.csr_matrix(test_vectors)","e02d0d75":"train_vectors","06dc9c0d":"from sklearn import metrics\ntrain_y = train_df[\"target\"].values\n\ndef runModel(train_X, train_y, test_X, test_y, test_X2):\n    model = linear_model.LogisticRegression(C=5., solver='sag')\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)[:,1]\n    pred_test_y2 = model.predict_proba(test_X2)[:,1]\n    return pred_test_y, pred_test_y2, model\n\nprint(\"Building model.\")\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0]])\nkf = model_selection.KFold(n_splits=3, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_df):\n    dev_X, val_X = train_vectors[dev_index], train_vectors[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runModel(dev_X, dev_y, val_X, val_y, test_vectors)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))","55340570":"cv_scores","00ed071d":"for thresh in np.arange(0.3, 0.55, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_val_y>thresh).astype(int))))","9bf3e85e":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","768e474b":"sample_submission['target'] = list(map(lambda x: 1 if model.predict_proba(x)[:,1] > 0.44 else 0, test_vectors))","6eceec40":"sample_submission.head()","b9b6ac9f":"sample_submission.to_csv(\"submission_tfidf_logistic.csv\", index=False)","cc4bce00":"eli5.show_weights(model, vec=tfidf_vectorizer, top=100, feature_filter=lambda x: x != '<BIAS>')","f0df0853":"# 2. TF_IDF Approach","5b690775":"Cross Validation","f2b3b19f":"# 1. Base Model using BOW (Bag of Words)\/Count Vectorizer","e3f99823":"# 5. Adding meta data points along with tf-idf using logistic regression","b2c91ee2":"Below script will add required data points to your tf-idf vectors","d52a0474":"Submission","45f0b62e":"Adding Meta Features","92326dff":"Submission","7bd80961":"Submission","8aca5bf0":"Submission","fd17c3ed":"Cross Validation","f9854722":"Cross Validation","0a16437d":"# 4. TF-IDF -> LSA","1864f524":"Fit the model","80dc4ed3":"# Analyze Data","e2492b14":"Set Threshold as per cross validation accuracy","ddc45147":"# Disaster Predict\n\n![nlp1-cover.jpg](attachment:nlp1-cover.jpg)\n\nYou are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n\nIntroduction and References\nFew approaches to begin with in NLP Text Classification. \n\nEDA and modelling using meta informations only are in seperate notebook >> [NLP_ Disaster Predict: Exploring Meta Information](https:\/\/www.kaggle.com\/vivekam101\/nlp-disaster-predict-exploring-meta-information)\n\nThis kernel includes codes and ideas from kernels below. If this kernel helps you, please upvote their work as well.\n\nReferences: \n[Simple Exploration Notebook - QIQC](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc) by [@sudalairajkumar](https:\/\/www.kaggle.com\/sudalairajkumar)\n","6f15a613":"Cross Validation using RF","678e521f":"Fit the model","c7f249c5":"tf-idf vectorisation","c8046626":"Cross Validation","560b1399":"View the features with high importance with the help of eli5 library","29a2ad23":"Submission","793f8b44":"# Load","c09d4656":"# Conclusion\n\n* tf-idf along with logistic regression worked well with LB score of 0.80416. By Chance the meta information didn't help the model to improve.\n\n* EDA and modelling using meta informations only are in seperate notebook >> [NLP_ Disaster Predict: Exploring Meta Information](https:\/\/www.kaggle.com\/vivekam101\/nlp-disaster-predict-exploring-meta-information)\n\n* We will work on BERT model on another notebook.\n\nPlease upvote if you find the notebook interesting.","8e83a67c":"Revert to sparse matrix for better computations","add8da30":"Fit the model","66cf474f":"# 3. TF-IDF Using RF Classifier"}}