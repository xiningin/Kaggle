{"cell_type":{"751f3580":"code","4cfba9d7":"code","d314a682":"code","71ae1852":"code","d1a6d5ad":"code","5b43136b":"code","645f00bb":"code","31c97664":"code","58c560a0":"code","4f86ec8c":"code","1159a890":"code","b84ba210":"code","adf1ffe7":"code","8e7b9ace":"code","f4174ae7":"code","817e2183":"code","6c683190":"code","f8df72fe":"code","496d30b3":"code","da49395d":"code","74d2bcea":"markdown","9a0d9114":"markdown","1b70655f":"markdown","72829712":"markdown","6d51a3e0":"markdown","70ccdce1":"markdown","0e9987ae":"markdown"},"source":{"751f3580":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport lightgbm as lgb\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nimport time\n\nimport datetime as datetime\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport cufflinks as cf\ncf.set_config_file(offline=True)","4cfba9d7":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d314a682":"path_file = r'\/kaggle\/input\/smart-meter-dataset\/'","71ae1852":"df_metadata_loop = pd.read_csv(os.path.join(path_file, 'metadata_loop.csv'))\ndf_metadata_loop","d1a6d5ad":"df_metadata_uid = pd.read_csv(os.path.join(path_file, 'metadata_uid.csv'))\ndf_metadata_uid","5b43136b":"df_weather = pd.read_csv(os.path.join(path_file, 'weather.csv'))\ndf_weather['ObsTime'] = pd.to_datetime(df_weather['ObsTime'])\ndf_weather","645f00bb":"df_holiday_encode = pd.read_csv(os.path.join(path_file, 'holiday_encode.csv'))\ndf_holiday_encode['date'] = pd.to_datetime(df_holiday_encode['date'])\ndf_holiday_encode","31c97664":"df_powerMeter_pivot_output = pd.read_csv(os.path.join(path_file, 'powerMeter.csv'))\ndf_powerMeter_pivot_output['\u65e5\u671f\u6642\u9593'] = pd.to_datetime(df_powerMeter_pivot_output['\u65e5\u671f\u6642\u9593'])\ndf_powerMeter_pivot_output = df_powerMeter_pivot_output.set_index('\u65e5\u671f\u6642\u9593')\ndf_powerMeter_pivot_output","58c560a0":"building_type = '\u884c\u653f\u55ae\u4f4d'\n\ndf_metadata = df_metadata_loop.merge(df_metadata_uid, on='uid')\nlist_powerMeter = df_metadata[df_metadata['buildType1C']==building_type]['\u8ff4\u8def\u7de8\u865f'].to_list()\ndf_powerMeter_pivot_output = df_powerMeter_pivot_output.loc[:, df_powerMeter_pivot_output.columns.str.contains('|'.join(list_powerMeter))]\ndf_powerMeter_pivot_output.columns","4f86ec8c":"# Normalize the energy data and take average of all meters' trends\ndf_powerMeter_pivot_output = (df_powerMeter_pivot_output-df_powerMeter_pivot_output.mean())\/df_powerMeter_pivot_output.std()\ndf_powerMeter_pivot_output[building_type + '_mean'] = df_powerMeter_pivot_output.mean(axis=1) + 1\n\nmeter_name = building_type + '_mean'\n\n# Prepare data for modeling\ndf_temp = df_powerMeter_pivot_output.loc[:, meter_name].reset_index().copy()\ndf_temp = df_temp.dropna()\n\n# Add timestamp features\ndf_temp['weekday'] = df_temp['\u65e5\u671f\u6642\u9593'].dt.weekday\ndf_temp['hour'] = df_temp['\u65e5\u671f\u6642\u9593'].dt.hour\ndf_temp['date'] =pd.to_datetime(df_temp['\u65e5\u671f\u6642\u9593'].dt.date)\n\ndf_temp = df_temp.set_index('\u65e5\u671f\u6642\u9593').drop(['date'],axis=1)\n\ndf_temp = df_temp.rename(columns={meter_name:'elec_meas'})\n\ndf_temp","1159a890":"# Weekly profiles of building energy\ndf_plot = df_temp.copy()\ndf_plot['elec_meas'].iplot()\ndf_plot['date'] = pd.to_datetime(df_plot.index.date)\ndf_plot.pivot_table(columns=['weekday','hour'], index='date', values='elec_meas').T.plot(figsize=(15,5),color='black',alpha=0.1,legend=False)","b84ba210":"traindata = df_temp.loc['2016'].copy()\ntestdata = df_temp.loc['2017'].copy()\n\ntrain_labels = traindata['elec_meas']\ntest_labels = testdata['elec_meas']\n\ntrain_features = traindata.drop('elec_meas', axis=1)\ntest_features = testdata.drop('elec_meas', axis=1) \n\nLGB_model = lgb.LGBMRegressor()\nLGB_model.fit(train_features, train_labels)\n\ntestdata['elec_pred'] = LGB_model.predict(test_features)\n\ndf_temp.loc['2017', 'elec_pred'] = testdata['elec_pred']\n\n# Calculate the absolute errors\nerrors = abs(testdata['elec_pred'] - test_labels)\n\n# Calculate mean absolute percentage error (MAPE) and add to list\nMAPE = 100 * np.mean((errors \/ test_labels))\nNMBE = 100 * (sum(testdata.dropna()['elec_meas'] - testdata.dropna()['elec_pred']) \/ (testdata.dropna()['elec_meas'].count() * np.mean(testdata.dropna()['elec_meas'])))\nCVRSME = 100 * ((sum((testdata.dropna()['elec_meas'] - testdata.dropna()['elec_pred'])**2) \/ (testdata.dropna()['elec_meas'].count()-1))**(0.5)) \/ np.mean(testdata.dropna()['elec_meas'])\nRSQUARED = r2_score(testdata.dropna()['elec_meas'], testdata.dropna()['elec_pred'])\n\nprint(\"MAPE: \"+str(round(MAPE,2)))\nprint(\"NMBE: \"+str(round(NMBE,2)))\nprint(\"CVRSME: \"+str(round(CVRSME,2)))\nprint(\"R SQUARED: \"+str(round(RSQUARED,2)))\n\ntestdata[['elec_meas', 'elec_pred']].iplot()","adf1ffe7":"df_temp = df_powerMeter_pivot_output.loc[:, meter_name].reset_index().copy()\ndf_temp = df_temp.dropna()\n\n# Add timestamp features\ndf_temp['weekday'] = df_temp['\u65e5\u671f\u6642\u9593'].dt.weekday\ndf_temp['hour'] = df_temp['\u65e5\u671f\u6642\u9593'].dt.hour\ndf_temp['date'] =pd.to_datetime(df_temp['\u65e5\u671f\u6642\u9593'].dt.date)\n\n# Add weather features\ndf_temp = df_temp.merge(df_weather[['ObsTime', 'Temperature']], left_on='\u65e5\u671f\u6642\u9593', right_on='ObsTime')\n\ndf_temp = df_temp.set_index('\u65e5\u671f\u6642\u9593').drop(['ObsTime','date'],axis=1)\n\ndf_temp = df_temp.rename(columns={meter_name:'elec_meas'})\n\ndf_temp","8e7b9ace":"# Scatter plot for energy consumptions and outdoor temperature\nplt.figure(figsize=(10,10))\ndf_plot = df_temp.copy()\ndf_plot = df_plot[df_plot['elec_meas']<3]\ndf_plot['weekday\/weekend'] = 'weekday'\ndf_plot.loc[df_plot['weekday']>4, 'weekday\/weekend'] ='weekend'\ndf_plot\nax = sns.relplot(x=\"Temperature\", y=\"elec_meas\", col=\"weekday\/weekend\", hue='hour',\n                 kind=\"scatter\", data=df_plot, alpha=0.1)","f4174ae7":"traindata = df_temp.loc['2016'].copy()\ntestdata = df_temp.loc['2017'].copy()\n\ntrain_labels = traindata['elec_meas']\ntest_labels = testdata['elec_meas']\n\ntrain_features = traindata.drop('elec_meas', axis=1)\ntest_features = testdata.drop('elec_meas', axis=1) \n\nLGB_model = lgb.LGBMRegressor()\nLGB_model.fit(train_features, train_labels)\n\ntestdata['elec_pred'] = LGB_model.predict(test_features)\n\n# Use the forest's predict method on the train data\ndf_temp.loc['2017', 'elec_pred'] = testdata['elec_pred']\n\n# Calculate the absolute errors\nerrors = abs(testdata['elec_pred'] - test_labels)\n# Print out the mean absolute error (mae)\n\n# Calculate mean absolute percentage error (MAPE) and add to list\nMAPE = 100 * np.mean((errors \/ test_labels))\nNMBE = 100 * (sum(testdata.dropna()['elec_meas'] - testdata.dropna()['elec_pred']) \/ (testdata.dropna()['elec_meas'].count() * np.mean(testdata.dropna()['elec_meas'])))\nCVRSME = 100 * ((sum((testdata.dropna()['elec_meas'] - testdata.dropna()['elec_pred'])**2) \/ (testdata.dropna()['elec_meas'].count()-1))**(0.5)) \/ np.mean(testdata.dropna()['elec_meas'])\nRSQUARED = r2_score(testdata.dropna()['elec_meas'], testdata.dropna()['elec_pred'])\n\nprint(\"MAPE: \"+str(round(MAPE,2)))\nprint(\"NMBE: \"+str(round(NMBE,2)))\nprint(\"CVRSME: \"+str(round(CVRSME,2)))\nprint(\"R SQUARED: \"+str(round(RSQUARED,2)))\n\ntestdata[['elec_meas', 'elec_pred']].iplot()","817e2183":"df_temp = df_powerMeter_pivot_output.loc[:, meter_name].reset_index().copy()\ndf_temp = df_temp.dropna()\n\n# Add timestamp features\ndf_temp['weekday'] = df_temp['\u65e5\u671f\u6642\u9593'].dt.weekday\ndf_temp['hour'] = df_temp['\u65e5\u671f\u6642\u9593'].dt.hour\ndf_temp['date'] =pd.to_datetime(df_temp['\u65e5\u671f\u6642\u9593'].dt.date)\n\n# Add weather features\ndf_temp = df_temp.merge(df_weather[['ObsTime', 'Temperature']], left_on='\u65e5\u671f\u6642\u9593', right_on='ObsTime')\n\n# Add holiday features\ndf_temp = df_temp.merge(df_holiday_encode, on='date')\n\ndf_temp = df_temp.set_index('\u65e5\u671f\u6642\u9593').drop(['ObsTime','date'],axis=1)\n\ndf_temp = df_temp.rename(columns={meter_name:'elec_meas'})\n\ndf_temp","6c683190":"traindata = df_temp.loc['2016'].copy()\ntestdata = df_temp.loc['2017'].copy()\n\ntrain_labels = traindata['elec_meas']\ntest_labels = testdata['elec_meas']\n\ntrain_features = traindata.drop('elec_meas', axis=1)\ntest_features = testdata.drop('elec_meas', axis=1) \n\nLGB_model = lgb.LGBMRegressor()\nLGB_model.fit(train_features, train_labels)\n\ntestdata['elec_pred'] = LGB_model.predict(test_features)\n\n# Use the forest's predict method on the train data\ndf_temp.loc['2017', 'elec_pred'] = testdata['elec_pred']\n\n# Calculate the absolute errors\nerrors = abs(testdata['elec_pred'] - test_labels)\n# Print out the mean absolute error (mae)\n\n# Calculate mean absolute percentage error (MAPE) and add to list\nMAPE = 100 * np.mean((errors \/ test_labels))\nNMBE = 100 * (sum(testdata.dropna()['elec_meas'] - testdata.dropna()['elec_pred']) \/ (testdata.dropna()['elec_meas'].count() * np.mean(testdata.dropna()['elec_meas'])))\nCVRSME = 100 * ((sum((testdata.dropna()['elec_meas'] - testdata.dropna()['elec_pred'])**2) \/ (testdata.dropna()['elec_meas'].count()-1))**(0.5)) \/ np.mean(testdata.dropna()['elec_meas'])\nRSQUARED = r2_score(testdata.dropna()['elec_meas'], testdata.dropna()['elec_pred'])\n\nprint(\"MAPE: \"+str(round(MAPE,2)))\nprint(\"NMBE: \"+str(round(NMBE,2)))\nprint(\"CVRSME: \"+str(round(CVRSME,2)))\nprint(\"R SQUARED: \"+str(round(RSQUARED,2)))\n\ntestdata[['elec_meas', 'elec_pred']].iplot()","f8df72fe":"testdata[['elec_meas', 'elec_pred']].resample('D').mean().iplot()","496d30b3":"testdata[['elec_pred', 'elec_meas']].resample('M').mean().iplot(kind='bar')","da49395d":"(testdata[['elec_pred', 'elec_meas']].resample('Y').mean()+1).iplot(kind='bar')","74d2bcea":"## Aggregated by different levels","9a0d9114":"### The second forecasting model with time-series and weather features","1b70655f":"## Process dataset (Target: \u884c\u653f\u55ae\u4f4d)","72829712":"## Clustering & visualizations","6d51a3e0":"### The third forecasting model with time-series, weather, and holiday features","70ccdce1":"## Load dataset","0e9987ae":"### The first forecasting model with only time-series features"}}