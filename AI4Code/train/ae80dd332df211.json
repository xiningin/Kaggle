{"cell_type":{"d8ba63df":"code","1efcb582":"code","ef251292":"code","2aecec5c":"code","080bc2c9":"code","7980086a":"code","adafa8e1":"code","48edc31e":"code","6cb38225":"code","82ac3039":"code","7380d646":"code","00cf08af":"code","f62ad423":"code","4c081f9e":"code","97ae9a59":"code","38d3edb4":"code","8dea4b15":"code","d5d347bd":"code","5420a800":"code","cd74ca01":"code","924731bb":"code","25817763":"code","b0d00fee":"code","e20ee501":"code","beebf9fe":"markdown","6e78e8f0":"markdown","a0182944":"markdown","baa62b11":"markdown","5291c0f3":"markdown","b4a7c9fd":"markdown","bf390172":"markdown","7c86a227":"markdown","c3f75325":"markdown","4ae48229":"markdown","1a1aab2d":"markdown"},"source":{"d8ba63df":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1efcb582":"# third-party libraries\nimport numpy as np\nimport pandas as pd\n\n# For preprocessing and metrics\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# hyperparameter tuning\nimport optuna\n\n# For training the models\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold\n\n# warnings should be avaoided \nimport warnings\nwarnings.filterwarnings('ignore')","ef251292":"# Read the dataset\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col = 0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col = 0)\nsubmissin = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nprint(\"Shape of the training data--->\", train.shape)\ntrain.head(5)","2aecec5c":"print(\"Shape of the testing data--->\", test.shape)\ntest.head(5)\n","080bc2c9":"# Get some information about the data\ncolumes = list(train.columns)\nprint(\"Colume names in the datatset -- -------> \",columes)\nprint(\"Total number of columes in the dataset ->\", len(columes))\n\nprint(\"=\"*80)\nprint(\"Data Information\")\nprint(train.info())","7980086a":"X = train.drop(\"target\" , axis = 1)\ny = train.target\n\nprint(\"Shape of the X_train dataset---->\", X.shape)\nprint(\"Shape of the y_train dataset---->\", y.shape)\nprint(\"Shape of the testing data ------>\", test.shape)","adafa8e1":"# find the colums\ncolumes = train.columns\n\nn = (X.dtypes == 'float64')\nnumerical_col_list = list(n[n].index) \n\ns = (X.dtypes == 'object')\ncatagorical_col_list =list( s[s].index)\n\nprint(\"Categorical columns with len({})-->{}\".format(len(catagorical_col_list) , catagorical_col_list))\nprint(\"Numerical columns with len({})-->{}\".format(len(numerical_col_list) , numerical_col_list))","48edc31e":"# Ordinal-encode categorical columns\nX_test = test.copy()\n\nordinal_encoder = OrdinalEncoder()\n\nX[catagorical_col_list] = ordinal_encoder.fit_transform(X[catagorical_col_list])\nX_test[catagorical_col_list] = ordinal_encoder.transform(X_test[catagorical_col_list])\n","6cb38225":"X_test.head()","82ac3039":"# # get the categorical cols\n# cat_cols_list = [col for col in columes if 'cat' in col]\n\n# # Encode the catetgorical data\n# ohencoder = OneHotEncoder()\n\n# X_test = test.copy()\n\n# X_train_encoded = pd.DataFrame(ohencoder.fit_transform(X_train[cat_cols_list]).toarray(), index = X_train.index)\n# X_train_rem = X_train.drop(cat_cols_list, axis = 1)\n# X_train = X_train_encoded.join(X_train_rem)\n\n# X_test_encoded = pd.DataFrame(ohencoder.fit_transform(X_test[cat_cols_list]).toarray(), index = X_test.index)\n# X_test_rem = X_test.drop(cat_cols_list, axis = 1)\n\n# X_test = X_test_encoded.join(X_test_rem)\n# X_test.head()","7380d646":"#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=0, test_size = 0.1)","00cf08af":"# Standartization\nscalar = StandardScaler()\n\nX = scalar.fit_transform(X)\n\nX_test = scalar.transform(X_test)","f62ad423":"X = pd.DataFrame(X)\nX_test = pd.DataFrame(X_test)\nX.head()","4c081f9e":"X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0, test_size = 0.1)","97ae9a59":"# Define the Model\nrf = RandomForestRegressor(random_state=42 , n_jobs = -1)\n\nrf.fit(X_train , y_train)\ny_pred_val = rf.predict(X_val)\nprint(\"MSE from Random forest: \",mean_squared_error(y_val, y_pred_val, squared=False))","38d3edb4":"# Model hyperparameters\nxgb_params = {\n    'n_estimators': 5000,\n    'learning_rate': 0.05,\n    'subsample': 0.95,\n    'colsample_bytree': 0.91,\n    'max_depth': 4,\n    'booster': 'gbtree', \n    'random_state':42,\n    'tree_method':'gpu_hist',\n    'gpu_id':0,\n    'predictor':\"gpu_predictor\",\n    'min_child_weight': 3,\n}\n\n# fit the model.\nuntuned_xgb = XGBRegressor(**xgb_params)\nuntuned_xgb.fit(X_train , y_train ,\n                verbose = False,\n                eval_metric = \"rmse\",\n                eval_set = [(X_train, y_train), (X_val, y_val)],\n                early_stopping_rounds = 100)\n\n# predict the results.\nuntuned_xgb_pred = untuned_xgb.predict(X_val)\n\nprint(\"MSE from Untuned XGBoost with some hyperparameter: \",mean_squared_error(y_val, untuned_xgb_pred, squared=False))","8dea4b15":"def objective(trial):\n    param = {\n        'n_estimators': trial.suggest_int('n_estimators', 5000, 50000, 500),\n        'max_depth': trial.suggest_int('max_depth', 2, 8),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 100),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n        'gamma': trial.suggest_loguniform('gamma', 1e-8, 100.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-8, 100.0),\n        'lambda': trial.suggest_loguniform('lambda', 1e-8, 100.0),\n        \n        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.5, log=True),\n    }\n    \n    #XGBoost model fitting.\n    xgb = XGBRegressor(**param, \n                         random_state=24,\n                         tree_method='gpu_hist' , \n                         gpu_id = 0, \n                         predictor=\"gpu_predictor\",)\n    \n    \n    xgb.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_val, y_val)],\n             eval_metric = \"rmse\",\n             early_stopping_rounds = 100)\n    \n    # precdition .\n    xgb_pred = xgb.predict(X_val)\n    \n    rmse = mean_squared_error(y_val, xgb_pred, squared=False)\n    return rmse","d5d347bd":"study = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=50) \n\nprint(\"lenth of the finished trials: \", len(study.trials))\n\n\nprint(\"Best value for the rmse:\", study.best_trial.value)\nprint(\"Best parameters:\", study.best_params)","5420a800":"best_param = study.best_params\nbest_param","cd74ca01":"X.head()","924731bb":"#Setting the kfold parameters\nn_fold = 15\nkf = KFold(n_splits = n_fold, shuffle = True, random_state = 42)\npred = 0\nresults = np.zeros((X.shape[0],))\nmean_rmse = 0\n\nmodel = XGBRegressor(**best_param,\n                     random_state=24,\n                     tree_method='gpu_hist' , \n                     gpu_id = 0, \n                     predictor=\"gpu_predictor\",)\n\n\nfor fold, (train_id, valid_id) in enumerate(kf.split(X)):\n    X_train, X_val = X.loc[train_id], X.loc[valid_id]\n    y_train, y_val = y.iloc[train_id], y.iloc[valid_id]\n    \n    \n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_val, y_val)],\n             eval_metric = \"rmse\",\n             early_stopping_rounds = 100)\n    \n    \n    #Out of Fold predictions\n    results=  model.predict(X_val) \n    \n    pred += model.predict(X_test) \/ n_fold\n    \n    fold_rmse = mean_squared_error(y_val ,results , squared=False)\n    \n    print(f\"Fold {fold} | mean squared error: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ n_fold\n    \nprint(f\"\\nOverall Accuracy: {mean_rmse}\")","25817763":"len(pred)","b0d00fee":"# Use the model to generate predictions\nsub = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsub.target = pred\n\nsub.to_csv('submission2.csv', index=False)\nsub.head()\n# # Save the predictions to a CSV file\n# output = pd.DataFrame({'id': X_test.index,\n#                        'target': pred})\n# output.to_csv('submission.csv', index=False)\nprint(\"Submission file created\")","e20ee501":"sub.head()","beebf9fe":"# Hyperparameter tuning using optuna","6e78e8f0":"# Step 4 -- > Submit the results","a0182944":"# Step 1 ---> Load the dataset","baa62b11":"# Step 3 --->  Modeling","5291c0f3":"### 2. XGBoost","b4a7c9fd":"# Step 2 ---> Preprocessing (Encoding and Standartization)","bf390172":"Next step we need to load the train and test dataset.","7c86a227":"#### next we need to separate the dataset into X and y. \nX --> will have all the values except the 'target'. y --> will have the target column","c3f75325":"### 1. Random Forest","4ae48229":"### 2.XGBoost without hyperparameter tuning","1a1aab2d":"### 1. Random Forest"}}