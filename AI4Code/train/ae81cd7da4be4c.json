{"cell_type":{"0ab9829c":"code","587a7c37":"code","3f5be5f2":"code","aa8eb911":"code","490fdf1d":"code","5a93aec4":"code","498d4e4e":"code","3f73914d":"code","8305c7b5":"code","8063199d":"code","ba46877d":"code","62d2bf5b":"code","7dd73868":"code","85a2f4b4":"code","66bc2e4c":"code","0f98f135":"code","ad724d23":"code","d71f3146":"code","d402553d":"code","5b89b640":"code","66687dea":"code","6304d1a6":"code","e735efd9":"code","c6f49153":"markdown","eeb99239":"markdown","708968d7":"markdown","a394e318":"markdown","e0b635d8":"markdown","b04ac813":"markdown","5559ac1a":"markdown","acf60ba0":"markdown","4b7aa6e4":"markdown","f70dea7a":"markdown","aff8a676":"markdown","560226f1":"markdown","6dc696c7":"markdown","559f7e97":"markdown","1149ea08":"markdown","1a6f93af":"markdown","9bdf9a43":"markdown","aac55632":"markdown","6edbbcab":"markdown","13a6b7a6":"markdown","d5bc6e38":"markdown","4331a5d7":"markdown","d0d743ab":"markdown"},"source":{"0ab9829c":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\nplt.style.use('fivethirtyeight')\n\n# Read in the data\ndf = pd.read_csv('..\/input\/amazon-reviews-unlocked-mobile-phones\/Amazon_Unlocked_Mobile.csv')\ndf.head()","587a7c37":"\nmostreviewd = (df.set_index('Product Name').groupby(level=0)['Reviews']\n    .agg(['count'])).sort_values(['count'], ascending=False)[:10]\n\n\n\nplt.figure(figsize=(12, 8))\nsns.barplot(mostreviewd.reset_index().index, y=mostreviewd['count'], hue=mostreviewd.index.str[:50] + '...', dodge=False)\nplt.ylim(1000,)\nplt.xticks([]);\nplt.ylabel('Reviews count')\nplt.title('Top 10 most reviewed products');","3f5be5f2":"bestbrand = (df[df['Rating'] > 3].set_index('Brand Name').groupby(level=0)['Reviews'].\n    agg(['count'])).sort_values(['count'], ascending=False)[:10]\n\nplt.figure(figsize=(12, 8))\nsns.barplot(bestbrand.index, y=bestbrand['count'], hue=bestbrand.index, dodge=False)\nplt.legend([])\nplt.ylabel('Positive reviews count')\nplt.title('Top 10 best brand');","aa8eb911":"# Filter out rating above 3 and get the review count\nworstproduct = (df[df['Rating'] < 3].set_index('Product Name').groupby(level=0)['Reviews'].\n    agg(['count'])).sort_values(['count'], ascending=False)[:10]\n\nplt.figure(figsize=(12, 8))\nsns.barplot(worstproduct.reset_index().index, y=worstproduct['count'], hue=worstproduct.index.str[:50] + '...', dodge=False)\nplt.ylim(250,)\nplt.xticks([]);\nplt.ylabel('Negative reviews count')\nplt.title('Top 10 worst products');","490fdf1d":"## Best budget product\nbudget = (df[(df['Rating'] > 3) & (df['Price'] < 500)].set_index('Product Name').groupby(level=0)['Price'].\n    agg(['count'])).sort_values(['count'], ascending=False)[:10]\n\ngrouped = df.set_index('Product Name').loc[budget.index].groupby(level=0)\n\nprice = pd.Series(index = budget.index)\nfor name, group in grouped:\n    price.loc[name] = group.Price.iloc[0] \n    \nbudget['Price'] = price\nbudget.reset_index(inplace=True)\n\nplt.figure(figsize=(12, 8))\nsns.barplot(x='Price', y='count', dodge=False, hue='Product Name', data=budget, palette=sns.color_palette(\"cubehelix\", 12))\nplt.ylim(750,)\nplt.ylabel('Positive reviews count')\nplt.title('Best budget products under $500');\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);","5a93aec4":"highend = (df[(df['Rating'] > 3) & (df['Price'] > 900)].set_index('Product Name').groupby(level=0)['Price'].\n    agg(['count'])).sort_values(['count'], ascending=False)[:10]\n\ngrouped = df.set_index('Product Name').loc[highend.index].groupby(level=0)\n\nprice = pd.Series(index = budget.index)\nfor name, group in grouped:\n    price.loc[name] = group.Price.iloc[0] \n    \nhighend['Price'] = price\nhighend.reset_index(inplace=True)\n\nplt.figure(figsize=(8, 8))\nsns.barplot(x='Price', y='count', dodge=False, hue='Product Name', data=highend, palette=sns.color_palette(\"cubehelix\", 12))\nplt.ylabel('Positive reviews count')\nplt.title('Best high end products under $2000');\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);","498d4e4e":"plt.figure(figsize=(10,8))\nsns.violinplot(x=\"Rating\", y=\"Price\", data=df)\nplt.title('Price vs Rating distribution');","3f73914d":"# Drop missing values\ndf.dropna(inplace=True)\n\n# Remove any 'neutral' ratings equal to 3\ndf = df[df['Rating'] != 3]\n\n# Encode 4s and 5s as 1 (rated positively)\n# Encode 1s and 2s as 0 (rated poorly)\ndf['Positively Rated'] = np.where(df['Rating'] > 3, 1, 0)\ndf.head(10)","8305c7b5":"df.describe()","8063199d":"# Get training and test data from dataset. \nfrom sklearn.model_selection import train_test_split\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['Reviews'], \n                                                    df['Positively Rated'], \n                                                    random_state=0)","ba46877d":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\nvect = TfidfVectorizer(min_df=5).fit(X_train)\nlen(vect.get_feature_names())","62d2bf5b":"X_train_vectorized = vect.transform(X_train)","7dd73868":"feature_names = np.array(vect.get_feature_names())\n\nsorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n\nprint('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\nprint('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))","85a2f4b4":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nX_train_vectorized = vect.transform(X_train)\n\nmodel = LogisticRegression(solver='saga')\nmodel.fit(X_train_vectorized, y_train)\n\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, model.decision_function(vect.transform(X_test))))\n","66bc2e4c":"sorted_coef_index = model.coef_[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","0f98f135":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\ndef PlotWordCloud(words, title):\n    wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white' \n                ).generate(words) \n                                                           \n    # plot the WordCloud image                        \n    plt.figure(figsize = (10, 10), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n    plt.title(title, fontsize=50)\n\n    plt.show() ","ad724d23":"negative = ''\nfor word in feature_names[sorted_coef_index[:100]]:\n    negative += word + ' '\nPlotWordCloud(negative, 'Most negative words')","d71f3146":"positive = ''\nfor word in feature_names[sorted_coef_index[:-101:-1]]:\n    positive += word + ' '    \nPlotWordCloud(positive, 'Most positive words')","d402553d":"print(model.predict(vect.transform(['not an issue, phone is working',\n                                    'an issue, phone is not working'])))","5b89b640":"# extracting 1-grams and 2-grams\nvect = TfidfVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n\nX_train_vectorized = vect.transform(X_train)\n\nlen(vect.get_feature_names())","66687dea":"model = LogisticRegression(solver='saga')\nmodel.fit(X_train_vectorized, y_train)\n\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, model.decision_function(vect.transform(X_test))))","6304d1a6":"feature_names = np.array(vect.get_feature_names())\n\nsorted_coef_index = model.coef_[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","e735efd9":"print(model.predict(vect.transform(['not an issue, phone is working',\n                                    'an issue, phone is not working'])))","c6f49153":"# Amazon reviews : Sentiment analysis","eeb99239":"## Introduction\nAbout this Dataset\nContext\n\nPromptCloud extracted 400 thousand reviews of unlocked mobile phones sold on Amazon.com to find out insights with respect to reviews, ratings, price and their relationships.\nContent\n\nGiven below are the fields:\n\n    Product Title\n    Brand\n    Price\n    Rating\n    Review text\n    Number of people who found the review helpful\n\nData was acquired in December, 2016 by the crawlers build to deliver our data extraction services.\nInitial Analysis\n\nIt can be accessed here: http:\/\/www.kdnuggets.com\/2017\/01\/data-mining-amazon-mobile-phone-reviews-interesting-insights.html","708968d7":"## Model","a394e318":"### Best budget product","e0b635d8":"### Feature extraction from text\nThe main and only feature for this model is **Review**. We will be parsing review and train the model. Finally, model should be able to predict whether review is positive or negative.\n\n* Feature : Reviews\n* Target : Positively Rated\n\n#### Tfidf\n\nConvert a collection of raw documents to a matrix of TF-IDF features. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html\n\n","b04ac813":"### Best high end product","5559ac1a":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Introduction<\/a><\/span><\/li><li><span><a href=\"#Data-Analysis\" data-toc-modified-id=\"Data-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Data Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Top-10-most-reviewed-products\" data-toc-modified-id=\"Top-10-most-reviewed-products-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Top 10 most reviewed products<\/a><\/span><\/li><li><span><a href=\"#Top-10-best-brand\" data-toc-modified-id=\"Top-10-best-brand-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Top 10 best brand<\/a><\/span><\/li><li><span><a href=\"#Top-10-worst-products\" data-toc-modified-id=\"Top-10-worst-products-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Top 10 worst products<\/a><\/span><\/li><li><span><a href=\"#Best-budget-product\" data-toc-modified-id=\"Best-budget-product-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;<\/span>Best budget product<\/a><\/span><\/li><li><span><a href=\"#Best-high-end-product\" data-toc-modified-id=\"Best-high-end-product-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;<\/span>Best high end product<\/a><\/span><\/li><li><span><a href=\"#Price-vs-Rating-distribution\" data-toc-modified-id=\"Price-vs-Rating-distribution-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;<\/span>Price vs Rating distribution<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Model<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Feature-extraction-from-text\" data-toc-modified-id=\"Feature-extraction-from-text-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Feature extraction from text<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Tfidf\" data-toc-modified-id=\"Tfidf-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;<\/span>Tfidf<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Logistic Regression<\/a><\/span><\/li><li><span><a href=\"#n-grams\" data-toc-modified-id=\"n-grams-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;<\/span>n-grams<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","acf60ba0":"### Price vs Rating distribution","4b7aa6e4":"The goal of this notebook is to predict wheter a review is positive or negative using Logistic Regression, MLP and NN","f70dea7a":"### Logistic Regression","aff8a676":"TfidfVectorizer created 17951 features from review text. Now we can feed this features to out model. Lets see top features extracted by TfidfVectorizer","560226f1":"Thats exactly we want our model should predict. It is now able to diffrentiate reviews based on tow words combination.","6dc696c7":"Model is wroking as expected. Lets try to give it some difficult reviews","559f7e97":"The features count reached to whopping 198917 from 17951. Lets train the model again with new features","1149ea08":"Our model's roc score is very good. Below are the lists of words from Logistic Regression model coefficiants ","1a6f93af":"As you can see most of the reviews are positive.","9bdf9a43":"**Data preparation**\n\nFirst read the dataset in panda dataframe. We are interseted in positive and negative reviews only. No such column exist in dataframe.\n\nTo create the column we will remove neutral rating i.e. 3. After that values those are below 3 will be treated negative review and above 3 will be treated as positive review.","aac55632":"If you can notice that our model is predicting both the reviews as negative. It is only considering single word. Now lets make it understand two word combination.","6edbbcab":"## Data Analysis","13a6b7a6":"### n-grams","d5bc6e38":"### Top 10 worst products","4331a5d7":"### Top 10 best brand","d0d743ab":"### Top 10 most reviewed products"}}