{"cell_type":{"35e9e61d":"code","a0c88155":"code","e6b7f0e0":"code","52801eb7":"code","1ab58947":"code","ae215231":"code","1c4d18d6":"code","53461679":"code","dd320adf":"code","213db27e":"code","fdabcbad":"code","6a09d3c6":"markdown","a9b01fab":"markdown","cd0908ba":"markdown","71d9dfbd":"markdown","afc7f53e":"markdown","1f570a28":"markdown","42197dbc":"markdown","f73c84b1":"markdown","90d98ff7":"markdown","08aa6b23":"markdown","e63b37cd":"markdown","415a277a":"markdown","1fda64f0":"markdown","3be591ec":"markdown","e42f36ef":"markdown","f9a1c7c7":"markdown","c3c31ee3":"markdown","1b1cd47e":"markdown","6b6e4bc2":"markdown"},"source":{"35e9e61d":"import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport csv\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a0c88155":"file_name = \"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\"\n\nwith open(file_name,'r') as csvfile:\n    reader = csv.DictReader(csvfile, restkey = None, restval = None, dialect = 'excel')\n\n    # list() takes a single argument. Could be a sequence (string, tuples) or collection (set, dictionary)\n    # Reads the entire file into RAM. \n    data = list(reader)\n\n    # Takes a sequence (list,string,tuple) and reorganize the order of the items\n    random.shuffle(data)\n\nlist_len = len(data)\nprint('Size of total data : {}'.format(list_len))","e6b7f0e0":"# Feature matrix \nX = np.zeros((list_len,11), dtype = float)\n# Quality vector \nY = np.zeros((list_len,1), dtype = float)\n\n# Transfer data into respective feature vectors\nfor i in range(0,list_len):\n    # data is a list of dictionaries. data[i] is a dictionary. data[i].values provides the values to the corresponding keys. list(data[i].values) converts it into a list\n    values = list(data[i].values())\n\n    X[i,0] = values[0]\n    X[i,1] = values[1]\n    X[i,2] = values[2]\n    X[i,3] = values[3]\n    X[i,4] = values[4]\n    X[i,5] = values[5]\n    X[i,6] = values[6]\n    X[i,7] = values[7]\n    X[i,8] = values[8]\n    X[i,9] = values[9]\n    X[i,10] = values[10]\n    Y[i,0] = values[11]\n    \nprint(X)\n\nprint(np.amin(X,axis=0))\n\nprint(np.amax(X,axis=0))\n    \n# Normalizing the data in feature matrix\nX = (X - np.amin(X,axis=0))\/(np.amax(X,axis=0) - np.amin(X,axis=0))\n# X = X\/np.amax(X,axis=0)\n    \n# Arranging data such that each column has one training example.\nX = X.T\n    \nprint(\"Data head :\")\nprint(X)\nprint(\"\\n\")\n\n# If quality score is >= 7 we classify it as good quality wine (1).\n# If quality score is < 7 we classify it is as poor quality wine (0).\n\n# MODIFY WHAT IS GOOD QUALITY WINE OVER HERE\n\nY = np.where(Y >= 7,1,0)\nY = Y.T\n\nprint(\"Quality vector : \")\nprint(Y)\nprint(\"\\n\")\n\nprint(\"Shape of feature matrix : \" + str(X.shape))\nprint(\"Shape of quality matrix : \" + str(Y.shape))","52801eb7":"# Enter train split (1 - 10)\ntrain_split = 7\n\nX_train = X[:,0:train_split*(X.shape[1])\/\/10]\nX_test = X[:,train_split*X.shape[1]\/\/10:X.shape[1]]\n\nY_train = Y[:,0:train_split*(Y.shape[1])\/\/10]\nY_test = Y[:,train_split*(Y.shape[1])\/\/10:Y.shape[1]]\n\nassert((X_train.shape[1] + X_test.shape[1]) == list_len)\n\nX_train_data_len = X_train.shape\nprint(' X Train data shape : {}'.format(X_train_data_len))\nX_test_data_len = X_test.shape\nprint(' X Test data shape : {}'.format(X_test_data_len))\nY_train_data_len = Y_train.shape\nprint(' Y Train data shape : {}'.format(Y_train_data_len))\nY_test_data_len = Y_test.shape\nprint(' Y Test data shape : {}'.format(Y_test_data_len))","1ab58947":"Z = np.arange(-10,10,0.1)\nplt.plot(Z,1\/(1 + np.exp(-Z)),Z,(1\/(1 + np.exp(-Z)))*(1 - 1\/(1 + np.exp(-Z))))\nplt.axvline(x=0,color='r')\nplt.axhline(y=0,color='r')\nplt.ylabel('a',fontsize=14)\nplt.xlabel('Z',fontsize=14)\nplt.legend(('a=g(Z)','da\/dZ'))\nplt.show()","ae215231":"Z = np.arange(-10,10,0.1)\nplt.plot(Z,(np.exp(Z) - np.exp(-Z))\/(np.exp(Z) + np.exp(-Z)),Z,1 - ((np.exp(Z) - np.exp(-Z))\/(np.exp(Z) + np.exp(-Z)))**2)\nplt.axvline(x=0,color='r')\nplt.axhline(y=0,color='r')\nplt.ylabel('a',fontsize=14)\nplt.xlabel('Z',fontsize=14)\nplt.legend(('a=g(Z)','da\/dZ'))\nplt.show()","1c4d18d6":"Z = np.arange(-10,10,0.1)\nrelu = np.where(Z<=0,0,Z)\nrelu_der = np.where(Z<=0,0,1)\nleaky_relu = np.where(Z<=0,0.05*Z,Z)\nleaky_relu_der = np.where(Z<=0,0.05,1)\n\nplt.plot(Z,relu,Z,relu_der)\nplt.title('ReLU')\nplt.axvline(x=0,color='r')\nplt.axhline(y=0,color='r')\nplt.ylabel('a',fontsize=14)\nplt.xlabel('Z',fontsize=14)\nplt.legend(('a=g(Z)','da\/dZ'))\nplt.show()\n            \nplt.plot(Z,leaky_relu,Z,leaky_relu_der)\nplt.title('Leaky ReLU')\nplt.axvline(x=0,color='r')\nplt.axhline(y=0,color='r')\nplt.ylabel('a',fontsize=14)\nplt.xlabel('Z',fontsize=14)\nplt.legend(('a=g(Z)','da\/dZ'))\nplt.show()\n        ","53461679":"# Computes sigmoid activation.\ndef sigmoid(Z):\n    \n    A = 1\/(1 + np.exp(-Z))\n    \n    return A\n\ndef sigmoid_der(A):\n    \n    dAdZ = A*(1-A)\n    \n    return dAdZ\n\ndef relu(Z):\n    \n    A = np.maximum(0,Z)\n    \n    return A\n\ndef relu_der(A):\n    \n    dAdZ = np.where(A>0,1,0)\n    \n    return dAdZ\n\ndef initialize_parameters(dims):\n    \n    n = len(dims)\n    params = {}\n    \n    for l in range(1,n):\n        params[\"W\"+str(l)] = np.random.randn(dims[l],dims[l-1])*0.1\n        params[\"b\"+str(l)] = np.zeros((dims[l],1),dtype=float)\n        \n    return params\n\n# Forward propagation function\n# caches architecture : [A_prev[l-1],W[l],b[l],A[l]]\ndef forward_propagation(params,A):\n    \n    # Number of layers\n    L = len(params) \/\/ 2\n    caches = []\n    \n    for l in range(1,L):\n        # 1st hidden layer to L-1th hidden layer is relu\n        A_prev = A\n        Z = np.dot(params[\"W\"+str(l)],A_prev) + params[\"b\"+str(l)]\n        linear_cache = (A_prev,params[\"W\"+str(l)],params[\"b\"+str(l)])\n        A = relu(Z)\n        cache = (linear_cache,A)\n        caches.append(cache)\n        \n    # Final output layer is sigmoid layer \n    A_prev = A\n    Z = np.dot(params[\"W\"+str(L)],A_prev) + params[\"b\"+str(L)]\n    linear_cache = (A_prev,params[\"W\"+str(L)],params[\"b\"+str(L)])\n#     print(\"Input into sigmoid function : \")\n#     print(\"\\n\")\n#     print(Z)\n    AL = sigmoid(Z)\n#     print(\"\\n\")\n#     print(\"Output from sigmoid function : \")\n#     print(\"\\n\")\n#     print(AL)\n#     print(\"\\n\")\n    cache = (linear_cache,AL)\n    caches.append(cache)\n        \n    return AL, caches\n\n# Cost function - Cross Entropy Loss Function\ndef compute_cost(AL,Y):\n    \n    # Number of training examples\n    m = Y.shape[1]\n    \n    cost = -np.sum(np.multiply(Y,np.log(AL)) + np.multiply(1-Y,np.log(1-AL)), axis=1, keepdims=True)\/m\n    \n    cost = np.squeeze(cost)\n    \n    return cost\n\n# Backward propogation function \ndef backward_propagation(AL,Y,caches):\n    \n    grads = {}\n    \n    # Number of layers\n    L = len(caches)\n    \n    # Number of training examples\n    m = Y.shape[1]\n    \n    # Derivative of Loss with respect to Activation of last layer\n    dAL = -(np.divide(Y,AL) - np.divide(1-Y,1-AL))\/m\n    # Calculating the derivatives of Loss with respect to weights and biases of last layer.\n    last_cache = caches[L-1]\n    # Extracting the linear cache and Activation in the last layer\n    linear_cache, A = last_cache\n    # Linear cache has the following components : A_prev, Weights and biases in last layer\n    A_prev, W ,b = linear_cache\n    grads[\"dA\"+str(L)] = dAL\n    grads[\"dW\"+str(L)] = np.dot(np.multiply(grads[\"dA\"+str(L)],sigmoid_der(A)),A_prev.T)\n    grads[\"db\"+str(L)] = np.sum(np.multiply(grads[\"dA\"+str(L)],sigmoid_der(A)),axis=1,keepdims=True)\n    \n    for l in range(L-1,0,-1):\n        if l == L-1:\n            grads[\"dA\"+str(l)] = np.dot(W.T,np.multiply(grads[\"dA\"+str(l+1)],sigmoid_der(A)))\n        else:\n            grads[\"dA\"+str(l)] = np.dot(W.T,np.multiply(grads[\"dA\"+str(l+1)],relu_der(A)))           \n        current_cache = caches[l-1]\n        linear_cache, A = current_cache\n        A_prev, W, b = linear_cache\n        grads[\"dW\"+str(l)] = np.dot(grads[\"dA\"+str(l)]*relu_der(A),A_prev.T)\n        grads[\"db\"+str(l)] = np.sum(grads[\"dA\"+str(l)]*relu_der(A),axis=1,keepdims=True)\n        \n    return grads\n\n# Updating the weights and biases \ndef update_parameters(params, grads, learning_rate):\n    \n    # Number of layers in model\n    L = len(params) \/\/ 2\n    \n    for l in range(1,L):\n        params[\"W\"+str(l)] = params[\"W\"+str(l)] - learning_rate*grads[\"dW\"+str(l)]\n        params[\"b\"+str(l)] = params[\"b\"+str(l)] - learning_rate*grads[\"db\"+str(l)]\n        \n    return params\n\n# Predicting the \ndef predict(params,X):\n    \n    AL,cache = forward_propagation(params,X)\n    predictions = np.where(AL>0.5,1,0)","dd320adf":"def model(X,Y,dims,learning_rate,num_iterations,print_cost=True):\n    \n    costs = []\n    \n    # Takes input only dimensions of each layer\n    params = initialize_parameters(dims)\n    \n    for i in range(0, num_iterations):\n        \n        # Forward propogation; inputs : parameters, X ; outputs : last layer activation, cache\n        AL, caches = forward_propagation(params,X)\n        \n        # Computing cost; inputs : AL, Y; outputs : cost\n        cost = compute_cost(AL,Y)\n        \n        # Backward propogation; inputs : AL, Y, cache ; outputs : grads\n        grads = backward_propagation(AL,Y,caches)\n        \n        # Update parameter; inputs : parameters, gradients; outputs : updated parameters\n        params = update_parameters(params,grads,learning_rate)\n        \n        if print_cost and i%100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i,cost))\n            costs.append(cost)\n    \n    # Plotting the cost\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('number of iterations')\n    plt.title('Learning rate : '+ str(learning_rate))\n    plt.show()\n    \n    return params\n    ","213db27e":"# You can play around with the learning rate and number of iterations and then press run all\ndims = [11,11,11,1]\nparams = model(X_train,Y_train,dims,learning_rate=0.5,num_iterations=5000,print_cost=True)","fdabcbad":"# Enter the following values\n\n# Fixed acidity range (4.6 - 15.9) ; max = 15.9\nFA = 10\n# Volatile acidity range (0.12 - 1.58) ; max = 1.58\nVA = 0.5\n# Citric acid range (0 - 1) ; max = 1\nCA = 0.8\n# Residual sugars (0.9 - 15.5) ; max = 15.5\nRS = 10\n# Chlorides (0.01 - 0.61) ;  max = 0.61\nC = 0.03\n# Free sulphur dioxide (1 - 72) ; max = 72\nFS = 30\n# Total sulphure dioxide (6 - 289) ; max = 289\nTS = 50\n# Density (0.99 - 1) ; max = 1\nD = 0.9997\n# pH (2.74 - 4.01) ; max = 4.01\nPH = 3.5\n# Sulphates (0.33 - 2) ; max = 2\nS = 1.5\n# alcohol (8.4 - 14.9) ; max = 14.9\nA = 12\n\nuser_X = np.zeros((11,1), dtype = float)\nuser_X[0] = FA\/15.9\nuser_X[1] = VA\/1.58\nuser_X[2] = CA\/1\nuser_X[3] = RS\/15.5\nuser_X[4] = C\/0.61\nuser_X[5] = FS\/72\nuser_X[6] = TS\/289\nuser_X[7] = D\/1\nuser_X[8] = PH\/4.01\nuser_X[9] = S\/2\nuser_X[10] = A\/14.9\n    \n","6a09d3c6":"## Weights and biases\n\nBefore the data is passed into the activation function it is acted upon by weights and biases. \n\n### $$ Z = WX + b $$  \n\nWeights and biases are used to segregate the input space. Weights adjust the slope while biases move the slope so that each node\/neuron in the hidden layer focuses on one aspect of the input space. \n\n****\n\n## Loss function\n\nCross entropy loss or log loss measures the performance of a classification model whose output is  a probability value between 0 and 1. Mean squared loss function is generally used in linear or polynomial regression problems. \n\nThe log loss function for binary and multi class classification is as follows :\n\n### Binary $$ -(ylog(p) + (1-y)log(1-p)) $$\n\n### Multiclass $$ -\\sum_{c=1}^{M} y_{c}log(p_{c}) $$\n\nHere c stands for class and y and p are the binary indicators and predicted probability observation. Here we will use binary classification to classify the wines into two categories good quality and bad quality.\n\n****\n\n## Why do you need non linear activation functions ?\n\nSuppose you have a linear activation function in the hidden layer and output layer. The square brackets denote the layer in you are in. 1 for the hidden layer and 2 for output layer.\n\n### $$ Z^{[1]} = W^{[1]}X + b^{[1]} $$\n### $$ a^{[1]} = Z^{[1]} $$\n### $$ Z^{[2]} = W^{[2]}(W^{[1]}X + b^{[1]}) + b^{[2]} $$\n### $$ a^{[2]} = Z^{[2]} = W^{[2]}W^{[1]}X + W^{[2]}b^{[1]} + b^{[2]} = W^{'}X + b^{'} $$\n\nSo the output is essentially a linear function of the input and does not create appropriate decision boundaries required for complex data sets. If you have only a sigmoid as the non linear activation function in the ouput then it is again a simple logistic regression problem and there is no need for any hidden leayers.\n\nReLU is a non linear activation function because it does not have a constant slope. For Z < 0 the slope of the function is 0, at 0 the slope of function is not defined and for Z > 0 the slope of function is 1.\n\n****\n\n## Different types of activation functions\n\nThe activation functions can be different for different layers.The below functions are plotted with Z as input and a as output and g(Z) is the mapping function between Z and a.\n\n### $$ Z = w^{T}X + b $$\n### $$ a = g(Z) $$\n\n### SIGMOID FUNCTION\n\n### $$ a = \\dfrac{1}{1 + e^{-Z}} $$\n\n* The Sigmoid function is not centered about 0.\n* For large value or small value of Z the slope of the function is 0 and hence the rate of learning decreases.\n* Use sigmoid in the output layer when you are doing binary classification as sigmoid outputs lie between 0 and 1 but dont use for hidden layers.\n* Derivative of sigmoid function is :\n\n### $$ \\dfrac{da}{dZ} = \\dfrac{e^{Z}}{(1 + e^{-Z})^2} = \\dfrac{1 +e^{-Z}}{(1 + e^{-Z})^2} - \\dfrac{1}{(1 + e^{-Z})^2} = a - a^2 = a(1-a) $$\n","a9b01fab":"# Data Scaling","cd0908ba":"### HYPERBOLIC TAN FUNCTION\n\n### $$ a = tanh(Z) = \\dfrac{e^{Z} - e^{-Z}}{e^{Z} + e^{-Z}} $$\n\n* The tanh function is centered about 0 and has a steeper slope.\n* For large value or small value of Z the slope of the function is 0 and hence the rate of learning decreases. \n* It is generally used in the hidden layers as it values lie between -1 to 1 hence the mean for the  hidden layer comes out to be 0 or very close to 0 and hence which helps in centering the data making the learning easier for the next layer.\n* Derivative of tanh function is :\n\n### $$ \\dfrac{da}{dZ} = \\dfrac{(e^{Z} + e^{-Z})^2 - (e^{Z} - e^{-Z})^2}{(e^{Z} + e^{-Z})^2} = 1 - a^2 $$","71d9dfbd":"Using all the above functions in the model function which the user will invoke","afc7f53e":"# Maths behind forward propagation and backward propagation","1f570a28":"### RECTIFIED LINEAR UNITS AND LEAKY RECTIFIED LINEAR UNITS\n\n### $$ a = max(0,Z) $$ \n\n* It is a peice wise function where for Z <= 0 then a = 0 and for Z > 0 then a = Z. It ranges from 0 to $ \\infty $\n* ReLU is less computationally expensive than tanh and sigmoid beacause it involves simpler mathematical operations. It avoids and rectifies the vanishing gradient problem.\n* Its limitation is that it should be used only within hidden layers of a neural network model. The default activation function for hidden layers is ReLU.\n* It avoids and rectifies the vanishing gradient problem. \n\nThe vanishing gradient problem is encountered when training neural nets with gradient based learning methods and backpropagation. In such methods, each of the neural network's weights recieve an update proportional to the partial derivative of the error function with respective to the current weight in each iteration of training Wi = Wi + learning_rate*(dJ\/dWi). The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the network from further training. For example, activation functions like tanh have gradients in the range from 0 to 1 and backpropagation computes gradients by the chain rule. This has the effect of multiplying 'n' of these small numbers to compute gradients of the \"front layers\" in an n-layer netowork, meaning the gradient decreses exponentially with n while the front layers train very slowly.\n\n* A problem with ReLU is that half of the space of Z is less than 0 and this can result in dead neurons with no weight update hence to overcome this problem leaky ReLU is used. In most cases most of the hidden units will have Z greater than 0.","42197dbc":"# Parameters vs Hyper parameters\n\nApplied deep learning is a very empirical process.\n\nParameters are weights and biases\n\nExmaples of hyperparameters are :\n\n1. Learning rate $ \\alpha $\n2. number of iterations\n3. number of hidden layers\n4. number of hidden units\n5. choice of activation function","f73c84b1":"Before we cover what neural nets are lets first understand what are supervised and unsupervised learning models are.\n\nSuppose you have input data X and target data Y. The goal of supervised learning models is to approximate a mapping function from input to ouput so that when new data is present one can predict the output for that data. Supervised learning models can be classified into two types:\n\n1. Classification : Target variable is discrete data (categorical). Examples of classification models are Neural Nets, Decision Tree. In these models one can have binary classification or multi class classification.\n\n2. Regression : Target variable is any real value. Examples of regression models are linear and polymonial regression. \n\nOn the other hand in unsupervised learning problems you only have input data X and no corresponding output data. You are training the machine on information that is neither classified nor labelled.\nUnsupervised Learning models can be classified into two types:\n\n1. Clustering : Discover inherent groupings within data. Example of a clustering algorithm is k-means\n\n2. Association : Discover rules that describe the data. Examples are Apriori algorithms for association rule learning problems.\n\nIn this tutorial we will be taking about how to build a simple neural net model.","90d98ff7":"# Libraries required","08aa6b23":"# Run model function here","e63b37cd":"# The Neural Network architecture","415a277a":"# Supervised and Unsupervised Learning Models","1fda64f0":"Feel free to refer to these amazing links which I have referred to build this model.\n\n1. Andrew NG Coursera : Neural Network and Deep Learning\n2. https:\/\/github.com\/stephencwelch\/Neural-Networks-Demystified\/blob\/master\/Part%201%20Data%20and%20Architecture.ipynb\n3. http:\/\/www.wildml.com\/2015\/09\/implementing-a-neural-network-from-scratch\/","3be591ec":"# Data split into training and testing sets","e42f36ef":"# Input your own parameters and check if wine is good quality or not","f9a1c7c7":"## Computing gradients in a three layer Neural Net and generalizing to n layer Neural Net\n\n### NOTATION\n\n1. X is the input and is also represented as activation in zeroth layer $ a$\n2. \"Z\" is a linear function operating on the input data whose output is input into a non linear function called the activation function denoted by \"a\". Different types include tanh, sigmoid and relu. \n3. $ \\sigma(Z) $ is sigmoid activation function for output layer. Sigmoid is used beacause we are doing binary classification.\n4. $ W_{3}^{[3](i)} $ the number at the top in the square brackets denote the weights in third layer and the number in the round brackets denotes the ith training example. The number at the bottom tells us which node the weights correspond to.\n5. * represents element wise multiplication.\n\n### FORWARD PROPAGATION\n\n#### Layer 1 Linear function \n### $$ Z^{[1]} = W^{[1]}X + b^{[1]} $$\n#### Layer 1 Activation function\n### $$ a^{[1]} = g(Z^{[1]}) $$\n#### Layer 2 Linear function\n### $$ Z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} $$\n#### Layer 2 Activation function \n### $$ a^{[2]} = g(Z^{[2]}) $$\n#### Layer 3 Linear function\n### $$ Z^{[3]} = W^{[3]}a^{[2]} + b^{[3]} $$\n#### Layer 3 Activation function\n### $$ a^{[3]} = \\sigma(Z^{[3]}) $$\n#### Layer 'l' Linear function\n### $$ Z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]} $$\n#### LAYER 'l' Activation function\n### $$ a^{[l]} = \\sigma(Z^{[l]}) $$\n\n#### Log Loss \/ Cross Entropy function \n### $$ L(a^{[n]},y) = -\\dfrac{1}{m} \\sum_{i=1}^{m}[y^{(i)}log(a^{[n](i)}) + (1 - y^{(i)})log(1 - a^{[n](i)})] $$\n\nThe loss is computed for each sum and is then summed across all training examples and finally divided by number of training examples.\n\n### BACKWARD PROPAGATION\n\n#### Layer 3 Backpropagation\n### $$ \\dfrac{dL}{da^{[3]}} = \\dfrac{-y}{a^{[3]}} + \\dfrac{(1-y)}{(1-a^{[3]})} = \\dfrac{-1}{m}\\dfrac{a^{[3]} - y}{a^{[3]}(1 - a^{[3]})} $$\n### $$ \\dfrac{dL}{dW^{[3]}} = \\dfrac{dL}{da^{[3]}}\\dfrac{da^{[3]}}{dZ^{[3]}}\\dfrac{dZ^{[3]}}{dW^{[3]}} = (\\dfrac{dL}{da^{[3]}}*g^{'}(Z^{[3]}))a^{[2]T} $$\n### $$ \\dfrac{dL}{db^{[3]}} = \\dfrac{dL}{da^{[3]}}\\dfrac{da^{[3]}}{dZ^{[3]}}\\dfrac{dZ^{[3]}}{db^{[3]}} = \\dfrac{dL}{da^{[3]}}*g^{'}(Z^{[3]}) $$\n\n#### Layer 2 Backpropagation\n### $$ \\dfrac{dL}{da^{[2]}} = \\dfrac{dL}{da^{[3]}}\\dfrac{da^{[3]}}{dZ^{[3]}}\\dfrac{dZ^{[3]}}{da^{[2]}} = W^{[3]T}(\\dfrac{dL}{da^{[3]}}*g^{'}(Z^{[3]})) $$\n### $$ \\dfrac{dL}{dW^{[2]}} = \\dfrac{dL}{da^{[2]}}\\dfrac{da^{[2]}}{dZ^{[2]}}\\dfrac{dZ^{[2]}}{dW^{[2]}} = (\\dfrac{dL}{da^{[2]}}*g^{'}(Z^{[2]}))a^{[1]T} $$\n### $$ \\dfrac{dL}{db^{[2]}} = \\dfrac{dL}{da^{[2]}}\\dfrac{da^{[2]}}{dZ^{[2]}}\\dfrac{dZ^{[2]}}{db^{[2]}} = \\dfrac{dL}{da^{[2]}}*g^{'}(Z^{[2]}) $$\n\n#### Layer 1 Backpropagation\n### $$ \\dfrac{dL}{da^{[1]}} = \\dfrac{dL}{da^{[2]}}\\dfrac{da^{[2]}}{dZ^{[2]}}\\dfrac{dZ^{[2]}}{da^{[1]}} = W^{[2]T}(\\dfrac{dL}{da^{[2]}}*g^{'}(Z^{[2]})) $$\n### $$ \\dfrac{dL}{dW^{[1]}} = \\dfrac{dL}{da^{[1]}}\\dfrac{da^{[1]}}{dZ^{[1]}}\\dfrac{dZ^{[1]}}{dW^{[1]}} = (\\dfrac{dL}{da^{[1]}}*g^{'}(Z^{[1]}))a^{[0]T} $$\n### $$ \\dfrac{dL}{db^{[1]}} = \\dfrac{dL}{da^{[1]}}\\dfrac{da^{[1]}}{dZ^{[1]}}\\dfrac{dZ^{[1]}}{db^{[1]}} = \\dfrac{dL}{da^{[1]}}*g^{'}(Z^{[1]}) $$\n\n#### Layer 'l' Backpropagation\n### $$ \\dfrac{dL}{da^{[l]}} = W^{[l+1]T}(\\dfrac{dL}{da^{[l+1]}}*g^{'}(Z^{[l+1]}))$$\n### $$ \\dfrac{dL}{dW^{[l]}} = (\\dfrac{dL}{da^{[l]}}*g^{'}(Z^{[l]}))a^{[l-1]T} $$\n### $$ \\dfrac{dL}{db^{[l]}} = \\dfrac{dL}{da^{[l]}}*g^{'}(Z^{[l]}) $$\n\n*** \n\n### MATRIX DIMENSIONS\n\nLet $n_x$, $n_{h1}$, $n_{h2}$, $n_y$ denote the number of nodes in input, hidden layer 1, hidden layer 2 and output layer. Let 'm' be the number of training examples. \n\nLets see the dimensions of the weight, biases and activation matrices.\n\n* X has dimensions [$n_x$, m] \n\n* Y has dimensions [$n_y$, m] \n\n* $ W^{[1]} $ has dimensions [$n_{h1}$, $n_x$]\n\n* $ b^{[1]} $ has dimensions [$n_{h1}$, 1]. \n\n* $ a^{[1]} $ has dimensions [$n_{h1}$, m]. \n\n* $ W^{[2]} $ has dimensions [$n_{h2}$, $n_{h1}$]\n\n* $ b^{[2]} $ has dimensions [$n_{h2}$, 1].\n\n* $ a^{[2]} $ has dimensions [$n_{h2}$, m].\n\n* $ W^{[3]} $ has dimensions [$n_y$, $n_{h2}$]\n\n* $ b^{[3]} $ has dimensions [$n_y$, 1]\n\n* $ a^{[3]} $ has dimensions [$n_y$, m].\n\n* **The weight matrix in the lth layer has dimensions as follows [number of hidden units in lth layer, number of hidden units in (l-1)th layer]. The bias vector in the lth layer has the dimensions as follows [number of hidden units in lth layer, 1]. The bias vector is broadcasted so that it can be added to WX when calculating Z.** \n\nLets see the dimensions of the derivatives\n\n* The dimensions of any Loss derivative with respect to activation must have the same dimensions of the activation marix in the corresponding layer. So for example $ \\dfrac{dL}{da^{[3]}} $, $ \\dfrac{dL}{da^{[2]}} $, $ \\dfrac{dL}{da^{[1]}} $ have dimensions [$n_y$, m], [$n_{h2}$, m], [$n_{h1}$, m] respectively. Let us prove this for $ \\dfrac{dL}{da^{[2]}} $. Looking at its formula we see that $ W^{[3]T} $ has dimensions [$n_{h2}$, $n_y$] while $ \\dfrac{dL}{da^{[3]}} $ has dimensions of [$n_y$, m] and $ g^{'}(Z^{[3]}) $ has dimensions [$n_y$, m]. $ \\dfrac{dL}{da^{[3]}} $ and $ g^{'}(Z^{[3]}) $ are multiplied element wise and the output undergoes dot product multiplication with $ W^{[3]T} $.\n\n* The dimensions of any Loss derivative with respect to weights must have the same dimensions of the weights matrix in the corresponding layer. So for example $ \\dfrac{dL}{dW^{[3]}} $, $ \\dfrac{dL}{dW^{[2]}} $, $ \\dfrac{dL}{dW^{[1]}} $ have dimensions [$n_y$, $n_{h2}$], [$n_{h2}$, $n_{h1}$], [$n_h1$, $n_x$] respectively. Let us prove this for $ \\dfrac{dL}{dW^{[2]}} $. Looking at its formula we see that $ \\dfrac{dL}{da^{[2]}} $ has dimensions [$n_{h2}$, m] and $ g^{'}(Z^{[2]} $ has dimensions [$n_{h2}$, m] and $ a^{[1]T} $ has dimensions [m, $n_{h1}$]. $ \\dfrac{dL}{da^{[2]}} $ and $ g^{'}(Z^{[2]} $ is multiplied element wise and the output undergoes dot product multiplication with $ a^{[1]T} $.\n\n* The dimensions of any Loss derivative with respect to bias must have the same dimensions as that of the bias vector in the corresponding layer. So for example $ \\dfrac{dL}{db^{[3]}} $, $ \\dfrac{dL}{db^{[2]}} $, $ \\dfrac{dL}{db^{[1]}} $ have dimensions [$n_y$, 1], [$n_{h2}$, 1], [$n_h1$, 1] respectively. Let us prove this for $ \\dfrac{dL}{db^{[2]}} $. Looking at its formula formula we see that $ \\dfrac{dL}{da^{[2]}} $ has dimensions [$n_{h2}$, m] and $ g^{'}(Z^{[2]} $ has dimensions [$n_{h2}$, m]. We use np.sum() function and sum it across all columns so that we get a column vector.\n\nConsider an 'n' layered neural net. When computing derivative of Loss with respect to weight $ W^{[n]} $ then you take the derivative of the activation function and multiply by input to activation whereas while taking derivative of Loss with respect to $  W^{[1]} $ then there is a compounding effect. That is why changes in weights to first layer is the slowest if sigmoid or tanh activation function is used in hidden layer giving rise to the vanishing gradient problem.\n\n****\n\n## Random initialization of weights\n\nWhen having multi layer neural net intializing all weights to 0 does not work. This can be understood by the following example. Consider a neural net with two inputs and two hidden nodes and one output. Node 1 In hidden layer has weights $ W_1^{[1]} $ [0,0] and node two in hidden layer has weights $ W_2^{[1]} $ [0,0] and b is initialized to 0 for both of them. Output node has weight $ W_1^{[2]} $ [0,0] and b is intialized to 0. The weights for the activation for node 1 is $a_1^{[1]}$ and is given by $ g(W_1^{[1]}X + b_1^{[1]}) $ similarly for $a_2^{[1]}$ it is $ g(W_2^{[1]}X + b_2^{[1]}) $. Substituting the zero weights in the activation functions we get $a_1^{[1]}$ == $a_2^{[1]}$ and $a^{[1]} = [0;0] $. From the above formula of $ \\dfrac{dL}{dW^{[1]}} $ we can see that weights in $ W^{[1]} $ will be updated the same $ W^{[1]} = W^{[1]} - \\alpha\\dfrac{dL}{dW^{[1]}} $ as they both started from same value.  Hence the two hidden units will behave as 1 hidden unit. As iterations continue both the weights of the hidden units will be updated the same. \n\nThis is solved by using $W^{[1]}$ = np.random.randn()x0.01. We multiply with 0.01 or any other small constant when using sigmoid or tanh activation function as we want the value of Z input into the activation function to be small otherwise large inputs lead to gradients close to 0 and the optimization algorithm will become slow leading to problem of vanishing gradient. However if you are not using sigmoid or tanh activation functions this is not a problem. Also if we have many deep layers the constant you choose will vary but for shallow neural nets (2 layers deep) using 0.01 is fine.\n\nHowever if Logistic Regression (no hidden layer) then can intialize all weights to 0. The first example X fed in the logistic regression will output zero but the derivatives of the Logistic Regression depend on the input X (because there's no hidden layer) which is not zero. So at the second iteration, the weights values follow X's distribution and are different from each other if X is not a constant vector. ","c3c31ee3":"In this tutorial we are going to do binary classification and classify the wine as good if it has a quality score above a certain value and if it is below that value then we will classify it as bad wine. Binary classification can be done using logistic regression or shallow neural nets. \n\nFuture works include doing a multi calss classification to determine the quality score of the wine.\n\n# Logistic Regression\n\nLogistic Regression is used for binary classification. However it can be used for multi class classification through the one vs rest scheme in which for each class a binary classification is done. It is a one layer deep neural net having one node which is the output node. The output node assigns weights to each of the inputs and is sent as input into the sigmoid function whose output (which ranges from 0 to 1) is compared with the true label (0 or 1).\n\n# Shallow neural nets\n\nShallow neural nets consist of 1 or 2 hidden layers. Understanding shallow neural net can help us understand how deep neural nets function. They are better at approximating complex decision boundaries compared to logistic regression where a single node is present and you have only a single decision boundary so if the dataset is not linearly separable, logistic regression does not perform well and has 50\/50 percent chance of predicting the correct label. ","1b1cd47e":"Normalization of data is important in binary classification because the ouput node is a sigmoid activation function. In sigmoid activation functions any large negative input results in activity becoming very close to 0. This will hamper values calculated during backpropagation. Scaling down the feature data to lie between 0 and 1 will scale down the inputs into the sigmoid activation function and hence the outputs produced will lie in the range 0 to 1. To see this for yourself comment normalization of feature data and see how the inputs into the sigmoid activation function change and the corresponding outputs.\n\n## Min-Max scaling of data\n\n### $$ z = \\dfrac{x - min(x)}{max(x) - min(x)} $$\n\nHere 'x' is input feature and 'z' represents the scaled feature. ","6b6e4bc2":"# Extracting features from CSV file"}}