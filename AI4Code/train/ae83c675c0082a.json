{"cell_type":{"8ddfc58d":"code","1382a23f":"code","bab0b1a1":"code","88b3ac7c":"code","65ca3778":"code","9ebedc65":"code","ebd3cc9c":"code","9e0271ec":"code","276cbed1":"code","6ee85275":"code","b4f513e5":"code","23afaa13":"code","f184f16e":"code","0d453d9c":"code","9d8508ea":"code","60991d49":"code","93ed3f81":"code","adc0f5f2":"code","8a625f19":"code","cf1b7309":"code","ed90a082":"code","36e6c808":"code","6fe8c981":"code","a56c9156":"code","f10cb1cf":"code","797a9d7a":"code","c52c4f1a":"code","20be7abd":"code","0c4d7d31":"code","bc06c5e4":"code","3e624488":"code","188d55bf":"code","d87bccc5":"code","0795a34f":"code","45b926b3":"code","eb5c0b70":"code","55732894":"code","d9c1470f":"code","8e96c928":"code","8b8e450a":"code","7b2d8a7e":"code","f842bd0b":"code","3edaa497":"code","727b42cc":"code","3e168622":"code","2e1e40ed":"code","65398e7b":"code","1920104d":"code","8084ac66":"code","c31559b5":"code","497aa43e":"code","8facace8":"code","4f470d56":"code","45ac4266":"code","b7d2f999":"code","eb0c7c05":"code","87e1a269":"code","b9399f53":"code","2d0ef2af":"code","b96439e8":"code","3bf0cb6d":"code","fe4ff0b2":"code","88bdcc63":"code","1d9cf77a":"code","183d8191":"code","9dadf26b":"markdown","333ba37c":"markdown","1774d499":"markdown","a985ae1d":"markdown","47724bb9":"markdown","8fdf1cf5":"markdown","492c3690":"markdown","80070d0f":"markdown","4d78e5f9":"markdown","dc217dcd":"markdown","8348b184":"markdown"},"source":{"8ddfc58d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom wordcloud import wordcloud\nfrom wordcloud import WordCloud\nimport string\nimport nltk \nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport re\nimport gensim\nfrom gensim.models import Word2Vec","1382a23f":"# when u run on the kaggle kernel just uncomment the below line\n# imdb_data = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\n\n# when u will run the code in the jupyter notebook just uncomment the below line and make sure that your data should be available in the same folder.\n# imdb_data = pd.read_csv('IMDB Dataset.csv')","bab0b1a1":"imdb_data.head()","88b3ac7c":"## Reading first five reviews entirely from the dataset\nfor review in imdb_data['review'][0:5]:\n  print(review)","65ca3778":"# as of data has too much punctuations marks, stopwords we hve to clean the data.","9ebedc65":"imdb_data.shape    #lets see the shape of the data","ebd3cc9c":"# let olot the countplot of Positive and Negative Reviews  --> to find the how many instances are there in the dataset.\nsns.countplot(imdb_data.sentiment)\nplt.title('Countplot of Sentiments.')\nplt.show()","9e0271ec":"# Great Positive and Negative labels in the datasets are balanced so it means that our dataset is balanced dataset.\n# Positive Labels = 25000 instances\n# Negative Labels = 25000 instances","276cbed1":"# Lets find out the is there any null  or missing values are existing in the dataset or not?\nimdb_data.isnull().sum()     # It seems there are no missing Values in the dataset.","6ee85275":"imdb_data.info()","b4f513e5":"# Lets make split the dataset into positive Labels and Negative labels.\nimdb_positive_reviews = imdb_data[imdb_data['sentiment']=='positive']\nimdb_negative_reviews = imdb_data[imdb_data['sentiment']=='negative']","23afaa13":"# Lets see the first five rows of the positive reviews DataFrame.\nimdb_positive_reviews.head()","f184f16e":"imdb_positive_reviews.tail()","0d453d9c":"# Lets see the first five rows of the negative reviews DataFrame.\nimdb_negative_reviews.head()","9d8508ea":"imdb_negative_reviews.tail()","60991d49":"# Let now combine all the text in positive reviews and all the text in the negative reviews.\npositive_text = ' '.join(review for review in imdb_positive_reviews.review)\nnegative_text = ' '.join(review for review in imdb_negative_reviews.review)","93ed3f81":"print(\"Length of all the positive text in all 25000 instances:\", len(positive_text))","adc0f5f2":"print(\"Length of all the negative text in all 25000 instances:\", len(negative_text ))","8a625f19":"#Lets make now wordcloud for positive_text and negative_text from the above text we extracted.\n# Lets make first for the positive Text.\n\nwordcloud_positive  = WordCloud(background_color='white').generate(positive_text)\n\nplt.figure(figsize=(10,5))\nplt.imshow(wordcloud_positive)\nplt.title('The words which was used most of the times in the positive reviews.')\nplt.axis('off')\nplt.show()","cf1b7309":"?WordCloud     # ? it gives the information about the paramter in the class and in short docstring.","ed90a082":"#Lets make wordcloud for Negative Reviews.\nwordcloud_negative  = WordCloud(background_color='black').generate(negative_text)\n\nplt.figure(figsize=(10,5))\nplt.imshow(wordcloud_negative)\nplt.title('The words which was used most of the times in the negative reviews.', fontsize=20)\nplt.axis('off')\nplt.show()","36e6c808":"# Now Lets remove the Punctuation , stopwords, noisy_texts from the reviews.\ndef remove_punctuation_mark(text):\n    filtered_words = word_tokenize(text)\n    for mark in string.punctuation:\n        filtered_words = [word.replace(mark,'') if mark in word else word for word in filtered_words] \n    filtered_text = \" \".join(filtered_words)\n    return filtered_text\n\n\nimdb_data['review']= imdb_data['review'].apply(remove_punctuation_mark)","6fe8c981":"# we have removed punctutation from the text succesfully.\nimdb_data['review'][0]","a56c9156":"imdb_data['review'] = imdb_data['review'].str.replace(' br ', '')","f10cb1cf":"imdb_data['review'][0]","797a9d7a":"# Lets remove stopwords from the reviews\neng_stopwords = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    filtered_words = text.split()\n    filtered_words = [word for word in filtered_words if word not in eng_stopwords]\n    return \" \".join(filtered_words)\n\nimdb_data['review'] = imdb_data['review'].apply(remove_stopwords)","c52c4f1a":"# let see the stopwords are removed or not`\nimdb_data['review'][0]   # yes, stopwords is removed. great almost we have clean the data.","20be7abd":"#Lets make all the characters to the lowercase.\nimdb_data['review'] = imdb_data['review'].str.lower()","0c4d7d31":"# Lets check the data is converted into the lower case or not \nimdb_data['review'][0]    # yes, great it has been converted.","bc06c5e4":"imdb_data.head()    # we have reached to the milestone we want to achieve.","3e624488":"# Lets find some HTTP word which exists in the text reviews or not?\ncount_http_tags=0\nindex=0\nindex_store=[]\nfor review in imdb_data['review']:\n    if 'http' in review:\n        count_http_tags+=1\n        index_store.append(index)\n    index+=1","188d55bf":"print('No of reviews with URL Links:', count_http_tags)\nprint(\"Index at which the UrL links appears:\\n\", index_store)","d87bccc5":"imdb_data['review'][907]  # Can see the url link in all these index.","0795a34f":"# Lets Remove the Url Links from the data reviews.\ndef remove_urllinks(text):\n    filtered_text = text\n    if 'http' in text:\n        url_content = re.findall(r'http www\\w+',filtered_text)\n        for url in url_content:\n            filtered_text = filtered_text.replace(url,'')\n    return filtered_text\n\nimdb_data['review'] = imdb_data['review'].apply(remove_urllinks)\n        \n            ","45b926b3":"# Lets check the with that index did that url links are removed or not.\nimdb_data['review'][907]","eb5c0b70":"# So now, we have performed the major data preprocessing stage where we have removed punctuation marks,\n# url links, stopwords, and also converted to lowercase and remove br tags.so by all this our data is cleaned perfectly","55732894":"# Lets make split the dataset into  positive Labels and Negative labels.\nclean_imdb_positive_reviews = imdb_data[imdb_data['sentiment']=='positive']\nclean_imdb_negative_reviews = imdb_data[imdb_data['sentiment']=='negative']\n\nclean_positive_text = ' '.join(review for review in clean_imdb_positive_reviews.review)\nclean_negative_text = ' '.join(review for review in clean_imdb_negative_reviews.review)\n\nprint(\"Length of all the cleaned positive text in all 25000 instances:\", len(clean_positive_text))\nprint(\"Length of all the cleaned negative text in all 25000 instances:\", len(clean_negative_text))","d9c1470f":"# Lets see the plot and find out how many characters are got reduced due to clean.\ncompare_df = pd.DataFrame({'Originaldata':[len(positive_text), len(negative_text)], \n                        'Cleaneddata':[len(clean_positive_text), len(clean_negative_text)]}, index =['Positive', 'Negative'])\n\ncompare_df.head()","8e96c928":"# Wordcloud for positive clean data\nwordcloud_clean_positive  = WordCloud(background_color='white').generate(clean_positive_text)\n\nplt.figure(figsize=(10,5))\nplt.imshow(wordcloud_clean_positive)\nplt.title('The words which was used most of the times in the cleaned positive reviews.', fontsize=15)\nplt.axis('off')\nplt.show()","8b8e450a":"# WordCloud for Negative clean data\nwordcloud_clean_negative  = WordCloud(background_color='black').generate(clean_negative_text)\n\nplt.figure(figsize=(10,5))\nplt.imshow(wordcloud_clean_negative)\nplt.title('The words which was used most of the times in the cleaned negative reviews.', fontsize=15)\nplt.axis('off')\nplt.show()","7b2d8a7e":"# Lets convert now words in vectors using word2Vec from Gensim Library\n# let Merge every review in one list \nall_review_words = []\nfor review in imdb_data.review:\n    review_words=word_tokenize(review)\n    all_review_words.append(review_words)\n","f842bd0b":"# Now all the reviews has been converted into words and stored in the list\nall_review_words[49999]","3edaa497":"# Lets make an object of word2vec model with passing arguments of all_review_words with min_count=1.\nw2v_model = Word2Vec(all_review_words, min_count=1)","727b42cc":"len(w2v_model.wv.index_to_key)   # total these many words are there in the review column which has been converted into vectors.","3e168622":"# Lets make the average of words of list so that we can make one single vector representation for each an every review\ndef compute_avg_vector_repr(all_review_words):\n    avg_vector_of_each_review =[]\n    for review_words in all_review_words:\n        vector_of_words = w2v_model.wv[review_words]\n        avg_vector_of_each_review.append(vector_of_words.mean())\n    \n    return avg_vector_of_each_review\n\navg_vector_repr = compute_avg_vector_repr(all_review_words)\nimdb_data['review_vectors'] = avg_vector_repr","2e1e40ed":"imdb_data.head(10)","65398e7b":"avg_vector_repr[10:50]","1920104d":"imdb_data_vectors = imdb_data.drop(columns=['review'])\nimdb_data_vectors.head()","8084ac66":"imdb_data_vectors[imdb_data_vectors['sentiment']=='positive'].plot(kind='hist', title='Distribution of positive reviews vectors.', edgecolor='black')\nplt.show()","c31559b5":"imdb_data_vectors[imdb_data_vectors['sentiment']=='negative'].plot(kind='hist', title='Distribution of negative reviews vectors.', edgecolor='black')\nplt.show()","497aa43e":"plt.figure(figsize=(10,5))\ngraph=sns.FacetGrid(hue='sentiment', data = imdb_data_vectors, height= 5)\ngraph.map(sns.histplot,'review_vectors', edgecolor='black',bins=25)\ngraph.add_legend()","8facace8":"# classes are not able to separate themselves properly therefore model is too much biases towards and negative reviews.\n# because of these SVM may not perform good.","4f470d56":"sns.scatterplot(imdb_data_vectors.index, imdb_data_vectors['review_vectors'],hue=imdb_data_vectors['sentiment'])","45ac4266":"imdb_data_vectors.isnull().sum() \n# while imputing avg values there was no missing values occured it means that every value has been imputed properly","b7d2f999":"feature = np.array(imdb_data_vectors['review_vectors']).reshape(-1,1)\ntarget = np.array(imdb_data_vectors['sentiment']).reshape(-1,1)","eb0c7c05":"feature.shape, target.shape","87e1a269":"# lets apply train_test_split from sklearn \nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train, y_test = train_test_split(feature, target, test_size=0.2, random_state=42)\nprint(x_train.shape, y_train.shape, x_test.shape,y_test.shape)","b9399f53":"from sklearn.svm import SVC\n\nsvc = SVC()    # applying default kernel i.e rbf( radial basis function )\nsvc.fit(x_train, y_train)","2d0ef2af":"# Predictions on the test data.\ny_pred = svc.predict(x_test)","b96439e8":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\ndef metrics(y_test, y_pred):\n    print('Accuracy Score:', accuracy_score(y_test, y_pred))\n    print('Classification Report:\\n',classification_report(y_test, y_pred))\n    print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))","3bf0cb6d":"metrics(y_test, y_pred)","fe4ff0b2":"# Lets change the kernel now:\nsvc1 = SVC(kernel ='linear') #lets fit the model by using the linear kernel\nsvc1.fit(x_train, y_train)\n\ny_pred1 = svc1.predict(x_test)\n\n#metrics\nmetrics(y_test,y_pred1)","88bdcc63":"# Lets change the kernel now:\nsvc2 = SVC(kernel ='sigmoid') #lets fit the model by using the linear kernel\nsvc2.fit(x_train, y_train)\n\ny_pred2 = svc2.predict(x_test)\n\n#metrics\nmetrics(y_test,y_pred2)","1d9cf77a":"# Lets change the kernel now:\nsvc3 = SVC(kernel ='poly') #lets fit the model by using the poly kernel\nsvc3.fit(x_train, y_train)\n\ny_pred3 = svc3.predict(x_test)\n\n#metrics\nmetrics(y_test,y_pred3)","183d8191":"# Visualizing the decision Boundaries.","9dadf26b":"### 60.22% accuracy has attained when we have apply svm using default kernel called 'rbf'.","333ba37c":"# Word2Vec Model","1774d499":"### while using kernel as linear the accuracy has increased to 0.02% which is almost negligible. Accuracy attained by this is 60.24%.","a985ae1d":"# Analyzing the Movie Reviews dataset.\n\n- Features in the dataset are:\n* 1) review --> The people given review on some of the movie which was collected by the IMDB. *\n* 2) sentiment --> As of review, given by the people is positive or it is negative. *","47724bb9":"# Data Preprocesing","8fdf1cf5":"### upto 10 laks to 12 lakhs characters are remove after cleaning the data which we had perform in the data preprocessing stage.","492c3690":"### Accuracy scored when we used this with kernel poly, the score we have attained is 54.76%","80070d0f":"## Model Evaluation for kernel RBF","4d78e5f9":"# Splitting the data for Feature and target variables.","dc217dcd":"# Lets Make the Wordcloud on Cleaned Data.","8348b184":"# Applying SVM classifier."}}