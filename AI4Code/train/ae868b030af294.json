{"cell_type":{"6ecf4b84":"code","9a84e995":"code","e9210572":"code","d70dd8b2":"code","c6a98faf":"code","dfcb3ca1":"code","b63dfceb":"code","fd7d9dc6":"code","b9b02117":"code","9b97d6d5":"code","b54407b9":"code","bdb3a0cc":"code","64af4561":"code","a2793864":"code","97fbb7db":"code","fc32e5cf":"code","2aa59617":"code","7f39f2f9":"code","f807c77c":"code","0fdcb417":"code","8b229f1e":"code","eacfc558":"code","53b86002":"code","a908f699":"code","c434ebd2":"code","b89be5d9":"code","ee7286b9":"code","c8a44b11":"code","eec1c252":"code","7b3680c9":"code","4ae3c637":"code","e07f26e0":"code","53d95b8f":"code","aa56532c":"code","66cdc7de":"code","4041c563":"code","2d56af7f":"code","7486118c":"code","22ad3e9f":"code","8b23d85d":"code","4f9e231b":"code","06077e43":"code","441a39d2":"code","f98cef37":"code","4ca1ae7c":"code","98f886be":"markdown","13723f78":"markdown","d7980b40":"markdown","1cd8bcca":"markdown","e9870ded":"markdown","8a36d9d4":"markdown","f055ab03":"markdown","c4ba6ffb":"markdown","1ee90b54":"markdown","7f46cf43":"markdown","813030f1":"markdown","d20b72d9":"markdown","cc75c3e2":"markdown","84e16b22":"markdown","9f457a4e":"markdown"},"source":{"6ecf4b84":"import pandas as pd # processing data\nimport matplotlib.pyplot as plt # Visualization\nimport numpy as np # Linear Algebra\nimport re # Regular Expression\nimport seaborn as sns # Visaulization\nfrom sklearn.preprocessing import LabelEncoder # Encoding object to number\nfrom sklearn.ensemble import RandomForestClassifier #Classification model\nfrom sklearn.model_selection import train_test_split #split train and validation data","9a84e995":"train_data=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data=pd.read_csv(\"..\/input\/titanic\/test.csv\")","e9210572":"train_data.head(10)","d70dd8b2":"test_data.head(10)","c6a98faf":"p = re.compile('[^\\,.$\\.]+')","dfcb3ca1":"print(p.findall(train_data.Name[0]))\nprint(p.findall(test_data.Name[0]))","b63dfceb":"train_title = pd.Series(train_data.Name.map(lambda x : p.findall(x)[1]))\ntest_title = pd.Series(test_data.Name.map(lambda x : p.findall(x)[1]))","fd7d9dc6":"train_title = pd.Series(list(map(lambda x : x.strip() ,train_title)))\ntest_title = pd.Series(list(map( lambda x : x.strip(),test_title)))","b9b02117":"plt.figure(figsize=(20,13))\nsns.barplot(x = train_title.value_counts().index,y=train_title.value_counts().values)\nplt.title(\"The title of aboarded\")\nplt.xlabel(\"title\")\nplt.ylabel(\"number of aboarded\")\nplt.show()","9b97d6d5":"plt.figure(figsize=(20,13))\nsns.barplot(x = test_title.value_counts().index,y=test_title.value_counts().values)\nplt.title(\"The title of aboarded\")\nplt.xlabel(\"title\")\nplt.ylabel(\"number of aboarded\")\nplt.show()","b54407b9":"train_data['title'] = train_title\ntest_data['title'] = test_title","bdb3a0cc":"train_data.head(10)","64af4561":"test_data.head()","a2793864":"list_for_check_train = []\nfor i in range(len(train_data)):\n    if train_data.title[i] in train_data.Name[i]:\n        list_for_check_train.append(True)\n    else:\n        list_for_check_train.append(False)\nnp.sum(list_for_check_train)","97fbb7db":"relation = train_data.corr()\nplt.figure(figsize=(16,9))\nsns.heatmap(data=relation,annot=True,cmap='YlGnBu')\nplt.show()","fc32e5cf":"train_data.count()","2aa59617":"train_data[train_data.Age.isna()].title.value_counts()","7f39f2f9":"test_data[test_data.Age.isna()].title.value_counts()","f807c77c":"train_data.groupby('title').mean().T['Mr'].Age.mean()","0fdcb417":"for title in train_data[train_data.Age.isna()].title.value_counts().index:\n    mean_age = train_data.groupby('title').mean().T[title].Age\n    mean_age_list = train_data[train_data.title == title].Age.fillna(mean_age)\n    train_data.update(mean_age_list)\ntrain_data.Age.isna().sum()","8b229f1e":"for title in test_data[test_data.Age.isna()].title.value_counts().index:\n    mean_age = train_data.groupby('title').mean().T[title].Age\n    mean_age_list = test_data[test_data.title == title].Age.fillna(mean_age)\n    test_data.update(mean_age_list)\ntest_data.Age.isna().sum()","eacfc558":"train_data.count()","53b86002":"test_data.count()","a908f699":"test_data.Fare = test_data.Fare.fillna(train_data.Fare.mean())","c434ebd2":"most_frequnt_embarked_value = train_data.Embarked.value_counts()\ntrain_data.Embarked = train_data.Embarked.fillna(most_frequnt_embarked_value.index[0])","b89be5d9":"train_data = train_data.drop(['Cabin','Ticket','PassengerId','Name'], axis= 1)\ntest_data = test_data.drop(['Cabin','Ticket','PassengerId','Name'], axis= 1)","ee7286b9":"train_data.select_dtypes(include='object').T.index","c8a44b11":"test_data.select_dtypes(include='object').T.index","eec1c252":"train_data.count()","7b3680c9":"test_data.count()","4ae3c637":"label_encoder = LabelEncoder()","e07f26e0":"for col in list(train_data.select_dtypes(include='object').T.index):\n    print(col)","53d95b8f":"train_data.head()","aa56532c":"test_data.head()","66cdc7de":"for col in train_data.select_dtypes(include='object').T.index:\n    train_data[col] = label_encoder.fit_transform(train_data[col])\n    test_data[col] = label_encoder.fit_transform(test_data[col])","4041c563":"X_train = train_data.drop('Survived',axis=1)\ny_train = train_data.Survived","2d56af7f":"X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,test_size =0.2, random_state =2045)","7486118c":"modeler = RandomForestClassifier(random_state=2045)","22ad3e9f":"modeler.fit(X_train,y_train)\npred = modeler.predict(X_valid)","8b23d85d":"np.sum(pred == y_valid)\/len(pred == y_valid)","4f9e231b":"final_pred = modeler.predict(test_data)","06077e43":"final_pred","441a39d2":"PassengerId = pd.read_csv(\"..\/input\/titanic\/test.csv\").PassengerId","f98cef37":"final = pd.DataFrame({'PassengerId':PassengerId,'Survived':final_pred})","4ca1ae7c":"final.to_csv('submission_MJ.csv', index=False)","98f886be":"## Visualization","13723f78":"## What am I gonna do?\n1. Import packages which we need \n2. See what kind of data we have\n    1. Type\n    2. Feature\n    3. leakage\n    4. etc...\n3. Preprocessing\n4. Classification\n5. Submission\n   ","d7980b40":"With this process, I got 76% score. \n\nYou and I could improve a model based on this or some other faboulaus method!\n\nPlease leave a comment, if you have a good idea and interesting topic!","1cd8bcca":"Sample extraction using re package","e9870ded":"## Visualize correalation between features\n\nIn this heatmap you can see 'Sibsp' and 'Parch' related very well and this is pretty straitfoward\n\nand look at the corr between survied and Fare. This is what we want to know\n\nFare and Survived related pretty much \n\ndid you expect it?","8a36d9d4":"You can see a space like ' mr'\n\nI wanted to delete this space","f055ab03":"I will replace missing value refer to title.\n\nBut there's something suspicious.\n\nThe result titles at the train data, which has missing value is different from the titles at the test data","c4ba6ffb":"Here is another(?) topic!\n\nI fitted the Labelencoder by using test dataset.\n\nIt's pretty similar what I mentioned about.","1ee90b54":"## Import\nI did use the package to process and analasys this data set","7f46cf43":"So, I replaced it seperatly\n\nBut what I concerned is, is it okay to replace each other?\n\nLet's think aout this\n\nWe make a model refer to train dataset not test data, which we don't know what kind of value there are\n\nAnd if we let the model learn from train dataset, which doesn't has a categorical value from test dataset, will the model predict and work well?\n\nplease leave your idea.","813030f1":"As you can see and you can even see many notebooks that plenty of people handled the 'Name' feature \n\nBecause there is a 'title' (Mr, Mrs, Miss)\n\nAnd to extract that feature I used regular expression package called 're'\n\nyou can see a reference about this package on https:\/\/docs.python.org\/3\/library\/re.html","d20b72d9":"extract all tilts from all of row at a object 'xxx_title'\n\nI will add this to raw data","cc75c3e2":"## What did I most concerned?\n\nwe could make so many mistakes in data handling. And that gives us not even only wrong insight, but also that could be a manipulation\n\nSo, I tried not to hack the test data. \n\nIt sounds very easy not to hack but it really is.\n\nLet me show you what kind of process I consider and let's talk about that\n","84e16b22":"# Titanic Prediction \n## I just do my super simple example as start of this competition and will improve myself based on this\n## I share this notebook as a example Please comment if you have a question or there is somethin wrong\n\n## I will very grateful about your feedback\n","9f457a4e":"## replace Nan\n\nAs you can see there are some missing vlaue (Age, Embarked, Cabin ...)\n\nI replaced missing value as a mean value\n\nBut not replaced Cabin column, which I droped"}}