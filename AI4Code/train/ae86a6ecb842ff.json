{"cell_type":{"3459f0ad":"code","826eaa28":"code","89b40055":"code","817ea3f0":"code","ac9bdaff":"code","f18f2b0d":"code","fc0466bd":"code","86c6087d":"code","ec7084d0":"code","f823206e":"code","65d977f1":"code","be581ac7":"code","601108a8":"code","eaf246e0":"code","a9b80334":"code","10fb3033":"code","5e653207":"code","80c2227e":"code","9ac91c10":"code","38ff386a":"code","ae43fc2e":"code","2d7e5ffc":"code","e4ebbb50":"markdown","7d6c7f50":"markdown","61e5c1b7":"markdown","25c8a8a7":"markdown","be7a7d79":"markdown","786df58f":"markdown","e4d0eba0":"markdown","614c1963":"markdown","bff4e115":"markdown","993daee2":"markdown","3a3e3733":"markdown","dcee8687":"markdown","039bdba0":"markdown","06901f55":"markdown","c1cb0304":"markdown","ceab2b10":"markdown","8bd4ccfe":"markdown","b8e5a868":"markdown","0d6660bf":"markdown","68c3272a":"markdown","ec533067":"markdown","eb22b8b4":"markdown","a3cfe857":"markdown","44bd3147":"markdown","c04d931b":"markdown","e5f45981":"markdown","f439ee7c":"markdown","6c8f4193":"markdown","786a9e71":"markdown","3a950775":"markdown","ebb20ef5":"markdown"},"source":{"3459f0ad":"import pandas as pd\nimport numpy as np\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ntrain_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')        \ntrain_identity = pd.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv')\ntest_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv')        \ntest_identity = pd.read_csv('..\/input\/ieee-fraud-detection\/test_identity.csv')\n\ntrain_transaction = train_transaction[['TransactionID','TransactionDT','TransactionAmt','ProductCD']]\ntrain_identity = train_identity[['TransactionID','id_19','id_20','id_31','DeviceInfo']]\ntest_transaction = test_transaction[['TransactionID','TransactionDT','TransactionAmt','ProductCD']]\ntest_identity = test_identity[['TransactionID','id_19','id_20','id_31','DeviceInfo']]\n\ntrain_transaction = train_transaction.merge(train_identity, how='left', left_on='TransactionID', right_on='TransactionID')\ntest_transaction = test_transaction.merge(test_identity, how='left', left_on='TransactionID', right_on='TransactionID')\n\ntotal = train_transaction.copy()\ntotal = total[total.ProductCD=='C']\ndel train_transaction\ndel test_transaction","826eaa28":"#same functions as in my previous kernel about CardID\nimport itertools\nimport math\nimport networkx as nx\n\n#function to create keys based on multiple columns\ndef create_key(df, cols, name_new_col):\n    '''\n    df: pandas dataframe\n    cols: list of columns composing the key\n    name_new_col: name given to the new column\n    '''\n    df.loc[:,name_new_col] = ''\n    for col in cols:\n        df.loc[:,name_new_col] = df.loc[:,name_new_col] + df.loc[:,col].astype(str)\n    return df  \n\ndef truncate(f, n):\n    return math.floor(f * 10 ** n) \/ 10 ** n  \n\ndef merge(list1, list2): \n    merged_list = [[p1, p2] for idx1, p1 in enumerate(list1)  \n    for idx2, p2 in enumerate(list2) if idx1 == idx2] \n    return merged_list  \n\ndef find_groups(df, groupingcriteria):   \n    a=[]\n    liste_sameamount = df.groupby(groupingcriteria)['TransactionID'].apply(list).tolist()\n    res = [list(map(a.append, map(list,zip(i, i[1:] + i[:1])))) for i in liste_sameamount]\n    return a","89b40055":"groups = pd.read_csv('..\/input\/cardid\/groups.csv')\ngroups = groups.set_index('TransactionID')\ndictgroups = groups['groups'].to_dict()\ntotal['cardID'] = total['TransactionID'].map(dictgroups)","817ea3f0":"total['day'] = total['TransactionDT']\/(3600*24)\ntotal['daytrunc'] = total['day'].apply(lambda x: truncate(x,1))\ntotal['dayround'] = total['day'].apply(lambda x: round(x,1))\ntotal['TransactionAmtround'] = total['TransactionAmt'].apply(lambda x: round(x,3))","ac9bdaff":"total1 = total[['TransactionID','TransactionAmt','TransactionAmtround','id_19','id_20','daytrunc','dayround','day']].copy()\ntotal1 = create_key(total1, ['TransactionAmtround','id_19','id_20'],'firstgroupcriteriaC')","f18f2b0d":"import gc\ntimeframe = total1.dayround.unique().tolist()\ngroup_list_C_criteria1 = []\n\nfor frame in timeframe:\n    if frame%50==0:\n        print('day',frame)\n        gc.collect()\n    \n    subset = total1[total1['dayround']==frame].copy()\n    if len(subset)==1:\n        group_list_C_criteria1.append(subset['TransactionID'].tolist())\n    else:\n        group_list_C_criteria1.extend(find_groups(subset, 'firstgroupcriteriaC'))","fc0466bd":"timeframe = total1.daytrunc.unique().tolist()\n\nfor frame in timeframe:\n    if frame%50==0:\n        print('day',frame)\n        gc.collect()\n    \n    subset = total1[total1['daytrunc']==frame].copy()\n    if len(subset)==1:\n        group_list_C_criteria1.append(subset['TransactionID'].tolist())\n    else:\n        group_list_C_criteria1.extend(find_groups(subset, 'firstgroupcriteriaC'))","86c6087d":"print(len(group_list_C_criteria1))\ngroup_list_C_criteria1 = [list(tupl) for tupl in {tuple(item) for item in group_list_C_criteria1 }]\nprint(len(group_list_C_criteria1))","ec7084d0":"group_list_C_criteria1[:10]","f823206e":"L = group_list_C_criteria1\nG = nx.Graph()\n\nG.add_nodes_from(sum(L, []))\nq = [[(s[i],s[i+1]) for i in range(len(s)-1)] for s in L]\nfor i in q:\n          G.add_edges_from(i)\n\ngroup_list = [list(i) for i in nx.connected_components(G)]\n\nmyDict = {}\n\nfor i in range(0,len(group_list)):\n    for element in group_list[i]:\n        name='group'+str(i)\n        myDict[element] = name\n    \ngroupsCAmtid1920 = pd.DataFrame.from_dict(myDict, orient='index').reset_index()\ngroupsCAmtid1920.columns=['TransactionID','groupsCAmtid1920']","65d977f1":"groupsCAmtid1920.head(5)","be581ac7":"total1 = total[['TransactionID','id_19','id_20','id_31','DeviceInfo','daytrunc','dayround','day']].copy()\ntotal1 = create_key(total1, ['id_19','id_20','id_31','DeviceInfo'],'secondgroupcriteriaC')\ntotal1 = total1[(total1['id_20'].isna()==False) & (total1['id_19'].isna()==False) & (total1['id_31'].isna()==False)]\n# this key is too indulgent if we don't get rid of missing id_19 and id_20 as many are missing, but try your experiments to find the best combination","601108a8":"import gc\ntimeframe = total1.dayround.unique().tolist()\ngroup_list_C_criteria1 = []\n\nfor frame in timeframe:\n    if frame%50==0:\n        print('day',frame)\n        gc.collect()\n    \n    subset = total1[total1['dayround']==frame].copy()\n    if len(subset)==1:\n        group_list_C_criteria1.append(subset['TransactionID'].tolist())\n    else:\n        group_list_C_criteria1.extend(find_groups(subset, 'secondgroupcriteriaC'))\n    \n## Second on Truncated data\ntimeframe = total1.daytrunc.unique().tolist()\n\nfor frame in timeframe:\n    if frame%50==0:\n        print('day',frame)\n        gc.collect()\n    \n    subset = total1[total1['daytrunc']==frame].copy()\n    if len(subset)==1:\n        group_list_C_criteria1.append(subset['TransactionID'].tolist())\n    else:\n        group_list_C_criteria1.extend(find_groups(subset, 'secondgroupcriteriaC'))       ","eaf246e0":"print(len(group_list_C_criteria1))\ngroup_list_C_criteria1 = [list(tupl) for tupl in {tuple(item) for item in group_list_C_criteria1 }]\nprint(len(group_list_C_criteria1)) ","a9b80334":"L = group_list_C_criteria1\nG = nx.Graph()\n\nG.add_nodes_from(sum(L, []))\nq = [[(s[i],s[i+1]) for i in range(len(s)-1)] for s in L]\nfor i in q:\n    G.add_edges_from(i)\n\ngroup_list = [list(i) for i in nx.connected_components(G)]\n\ngroupsCid192031Device = pd.DataFrame.from_dict(myDict, orient='index').reset_index()\ngroupsCid192031Device.columns=['TransactionID','groupsCid192031Device']","10fb3033":"total1 = total[['TransactionID','day','cardID']].copy()\n\ngroupC1 = groupsCAmtid1920.copy()\ngroupC2 = groupsCid192031Device.copy()\n\ntotal1 = total1.merge(groupC1, how='left',left_on='TransactionID',right_on='TransactionID')\ntotal1 = total1.merge(groupC2, how='left',left_on='TransactionID',right_on='TransactionID')\n\n#imputation\ntotal1['imputecol'] = [i for i in range(0,len(total1))]\ntotal1.loc[total1.groupsCid192031Device.isna(), 'groupsCid192031Device'] = total1.loc[total1.groupsCid192031Device.isna(), 'imputecol']","5e653207":"groups_C_final = []\ngroups_C_final.extend(find_groups(total1, 'cardID'))\nprint('group1done')\n\ngroups_C_final.extend(find_groups(total1, 'groupsCAmtid1920'))\nprint('group2done')\n\ngroups_C_final.extend(find_groups(total1, 'groupsCid192031Device'))\nprint('group3done')\n\nprint(len(groups_C_final))\ngroups_C_final = [list(tupl) for tupl in {tuple(item) for item in groups_C_final }]\nprint(len(groups_C_final))","80c2227e":"L = groups_C_final\nG = nx.Graph()\n\nG.add_nodes_from(sum(L, []))\nq = [[(s[i],s[i+1]) for i in range(len(s)-1)] for s in L]\nfor i in q:\n    G.add_edges_from(i)\n\ngroup_list = [list(i) for i in nx.connected_components(G)]\n\nmyDict = {}\n\nfor i in range(0,len(group_list)):\n    for element in group_list[i]:\n        name='group'+str(i)\n        myDict[element] = name\n    \ngroupsCuser = pd.DataFrame.from_dict(myDict, orient='index').reset_index()\ngroupsCuser.columns=['TransactionID','groupsCuser']\ngroupsCuser.to_csv('groupsCuser.csv',index=False)","9ac91c10":"train_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')        \ntrain_identity = pd.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv')\n\ntrain_transaction = train_transaction[['TransactionID','TransactionDT','TransactionAmt','ProductCD','isFraud']]\ntrain_identity = train_identity[['TransactionID','id_19','id_20','id_31','DeviceInfo']]\n\ntotal = train_transaction.merge(train_identity, how='left', left_on='TransactionID', right_on='TransactionID')\n\ntotal = total[total.ProductCD=='C']\ndel train_transaction","38ff386a":"#cardID\ngroups = pd.read_csv('..\/input\/cardid\/groups.csv')\ngroups = groups.set_index('TransactionID')\ndictgroups = groups['groups'].to_dict()\ntotal['cardID'] = total['TransactionID'].map(dictgroups)\n\n#New User group\ntotal = total.merge(groupsCuser, how='left',left_on='TransactionID',right_on='TransactionID')","ae43fc2e":"total['CardIDcount'] = total['cardID'].map(total.cardID.value_counts())\ntotal['CardID_fraud_sum'] = total.groupby('cardID')['isFraud'].sum()\n\ntotal['UserIDcount'] = total['groupsCuser'].map(total.groupsCuser.value_counts())\ntotal['UserID_fraud_sum'] = total.groupby('groupsCuser')['isFraud'].sum()","2d7e5ffc":"total[total.isFraud==1].to_csv('checkgroups.csv',index=False)","e4ebbb50":"### 5. Group the groups (deep heart of the kernel)\n\na. Create a dataframe with all the IDs, and the 3 different groupings by columns <br>\nb. imputation of different numbers for the group that does not cover all the IDs (where id_19 and id_20 were missing)","7d6c7f50":"d. Create all the groups based on the 3 keys","61e5c1b7":"b. Add the groups","25c8a8a7":"# Finding users\n\nThis is the second notebook about identifiers, coming after the one where I try to find unique cards. I'm going to use the results of the first one also as some of the methodology, so make sure you read it: \nhttps:\/\/www.kaggle.com\/tuttifrutti\/isolating-a-cardid\n\n### In many discussions the Card ID is called User ID. But a User can use multiple cards! and especially a frauding one!\n\n","be7a7d79":"The assumption is that a fraudster - if he is a professional in the field- uses multiple cards to perform his art. He has a set of cards, and will try to pay with each one, expecting one or all to work.\n\nIn addition, recreating the users identificators enables to create variables about User behaviour, and help the models to spot the differences between regular users and fraudsters (such as, using multiple cards, multiples devices, multiples IP adresses ect..)\n\nLet's analyze a frauding sequence, and find a way to recreate it.\n\nIn the following image, we identified a cluster of cards. The transactions were made in a short time period, the amount was exactly the same, and so are id_19 and id_20 (which I interpret as a being some sort of machine or connection address). \nI have colored the card groups identified by the previous kernel (1 card with 3 transactions, 1 with 2, and 2 with 1. The aim of this one, is to group these cards under the same user. Here the grouping is quite straightforward right? same amount, same \"IP\", same device, same browser. <p>\n\n<img src=\"https:\/\/i.imgur.com\/RvNZSo5.png\" alt=\"ex1\">\n\n\nLet's have a look at a much more interesting example. Below, we can see that the cardID algorithms identified 1 card with 6 transactions. And we have in the middle two transactions from different card.\nWe can assume that all these transactions have been made by the same user.\nGrouping by device AND IP AND Amount, you'll group the 3 last transactions. The fact that in these group of 3, one of the transactions is connected to another group (the cardID group) enables you to create a group for the User.\n\nThe last transaction, acts like a **bridge** between the group of cardID, and the group of Device x Amount x IP x Browser\n\n\n<img src=\"https:\/\/i.imgur.com\/RPXTko9.png\" alt=\"ex1\">\n\n\n\nActually here, you have to have a close look at the data and make your assumptions, my grouping keys are the following for product C:\n- **Key1**: group by Amount, id_19 and id_20 (this is pretty restrictive as the amounts in group C are very specific, so is the \"IP\" in a short timeframe)\n- **Key2**: group by id_19, id_20, id_31, Deviceinfo (this is also restrictive on a short timeframe\n- **Key3**: group by cardID (already done)\n- perform the grouping on 0.1 day (with the day truncated, and then rounded, to have a pseudo sliding window of size 0.1)\n\n**Then find transactions that act like BRIDGES between those groups** (group by intersection of groups) <br>\nAt the end of the day, all these transaction will be found in the same User group (see column 'newgroup'):\n\n<img src=\"https:\/\/i.ibb.co\/P1bkNNL\/Capture-d-cran-de-2019-10-04-12-50-34.png\" alt=\"ex1\">\n\n\n\nNow, it's time to compute it.\n\n<img src=\"https:\/\/www.incimages.com\/uploaded_files\/image\/970x450\/dirty-hands-1725x810_21760.jpg\" alt=\"handdirt\" width=500>\n","786df58f":"### 1. Load, merge, and subset, keeping the columns that we need\n\nMake sure you download Train AND Test to create the User ID, so we can find ovelapping users.\n\nIn this example I'm running run the code only on train, as it will be quicker to run. Do not do it at home!","e4d0eba0":"b. Get the matching couples of TransactionIDs based on the first key, on the **\"rounded\"** time window","614c1963":"### 8. Once your happy with the groups. Create Features\n\nThe User groups enables you to create features that make sense for the problem.\n- Number of card used by User\n- How hard the fraudster try not to be detected (changing IP, browser, device)\n\nWe have tried isolation forest based on the User variables, to detect outstanding behaviours. We used the results as a feature, and it turned out to bring information to the models.","bff4e115":"a. Load data","993daee2":"Let's check in our results in excel.\n\n**1st example**\nOn this first screenshot we can see:\n- Card1 14276, was group by CardID with another card, we cannot see the second on the screenshot, but the variable cardID_count indicates that this group contains 2 transactions (and 2 of them are fraudulent (cardID_sumFraud))\n- Same for Card1 8755.\n- These two cards were used by the same User. And are grouped with other cards. (12 transactions in total by the User, with 12 frauds)\n\n<img src=\"https:\/\/i.ibb.co\/WH1Y7NR\/Capture-d-cran-de-2019-10-04-14-54-32.png\" alt=\"ex1\">\n","3a3e3733":"**4th and most spectacular example**\n\nThis user tried at least 231 transactions (the rest might be errors of grouping).\n\nAccording to the grouping, the User made 275 transactions, out of which 231 were fraudulent.\n\nWe can see here, how this grouping by user ID can help!\n\n<img src=\"https:\/\/i.ibb.co\/9NNLxzg\/Capture-d-cran-de-2019-10-04-14-54-14.png\" alt=\"ex1\">","dcee8687":"#### Key2 group by id_19, id_20, id_31, Deviceinfo\n\nPerform the same algo as for Key 1 on another key","039bdba0":"### 2. Load the cards ID from the previous kernel","06901f55":"c. Drop duplicates, as the rounded and the truncated windows overlap","c1cb0304":"### 4. Grouping by keys (heart of the notebook)\n\nRemember, my grouping keys are the following:\n- **Key1**: group by Amount, id_19 and id_20 (this is pretty restrictive as the amounts in group C are very specific, so is the \"IP\" in a short timeframe)\n- **Key2**: group by id_19, id_20, id_31, Deviceinfo (this is also restrictive on a short timeframe\n- **Key3**: group by cardID (already done)\n\n\n#### Key1: Amount, id_19, id_20","ceab2b10":"### 6. Check the groups, and try again with other keys if not satisfied","8bd4ccfe":"### 7. Create user ID for the other products\n\nThe data available differs by product. Here my keys for the others:\n- R: 2 keys\n    - Amount x id_19 x id_20 x id_31 x id_20 x id_33 x DeviceInfo \n    - CardID\n- H: 2 keys\n    - id_19 x id_20 x id_31 x id_30 x id_33 x DeviceInfo\n    - CardID\n- S: 2 keys\n    - Amt x id_19 x id_20 x id_30 x id_31 x id_33 x DeviceInfod\n    - CardID","b8e5a868":"### 3. Create the pseudo time window.\n\nAfter some research you might find that a time window of 0.1 day is well suited (or not, you have to make your own assumptions) <br>\nSo i choosed to perform two loops:\n- One on the truncated day, so i have ranges (time windows) like [1.1-1.199] [2.4-2.499]\n- One on rounded day, time windows: [1.75001-1.84999] [34.2400001-34.349999]\n\nThis way, if 2 transactions of the same group appear on 1.999 and 2.0001, they will be together in the \"rounded\" window. However, if the group is in 1.30 and 1.39, the truncated window will catch it.","0d6660bf":"**3rd example**\n\nThis one is also nice, we can see that a card with 9 transactions, has been grouped with a single transaction card. It will be obviously easier to spot single transaction frauds thanks to this grouping!\n<img src=\"https:\/\/i.ibb.co\/LPX36rc\/Capture-d-cran-de-2019-10-04-14-54-18.png\" alt=\"ex1\">","68c3272a":"c. Create all the couples of ID from the same groups, and drop duplicates","ec533067":"#### Key3: CardID, we already have it computed from the previous kernel.","eb22b8b4":"## Takeaways:\n- Explore the data widely (Don't be ashamed of using excel, this does not make you less of a Data Scientist)\n- Run the algo on all the dataset, train+test, you'll find overlapping IDs\n- Iterate: Always check your result, and adjust your keys\n- Create your own KPIs to test your ideas (like the counts and sumFraud by group, or feed straight away your models with the outputs...)","a3cfe857":"b bis. Get the matching couples of TransactionIDs based on the first key, on the **\"truncated\"** time window","44bd3147":"**2nd example**\n\nHere we can see that a fraudulent card had 10 transactions (incl. 10 fraudulent), two others had one. <br>\nThe User grouping succeeded in grouping these cards together along with other that don't appear on this screenshot. \n\nHowever we can see, that this user has 2 non fraudulent transaction out of 30 (error of aggregation?)\n\n<img src=\"https:\/\/i.ibb.co\/BjWJzxb\/Capture-d-cran-de-2019-10-04-14-54-10.png\" alt=\"ex1\">\n","c04d931b":"**You can imagine that with this kind of feature (user having hundred of lines), you have to be very carefull about overfitting. Any unique feature for the group might become an ID**","e5f45981":"We get all couples of TransactionIDs that match based on the previous criterias","f439ee7c":"**5th example: predicting manually**\n\nThe UserID that was more represented on the test set, and very fraudulent on the train, had 144 lines on the test.\n\nThis is the training data of a very big fraudulent group. You can see that some are misclassified (look at colum isFraud), tou can easily spot them following the pattern on the C1\/C2 columns.\n\n<img src=\"https:\/\/i.ibb.co\/wBB6jsm\/Capture-d-cran-de-2019-10-01-00-40-15.png\" alt=\"ex1\">\n\nThis user had around 150 transactions in the test set. Here there is no column isFraud, but i guess you can easily spot the not-necessarily frauds looking at the C columns\n\n<img src=\"https:\/\/i.ibb.co\/K0DTfn4\/Capture-d-cran-de-2019-10-01-00-41-23.png\" alt=\"ex1\">\n","6c8f4193":"This is the resulting dataframe. Groups by the first key.","786a9e71":"c. Create your indicators","3a950775":"a. Create the key","ebb20ef5":"d. Group the groups thanks to the **bridges** (transactionIDs present in multiple groups) and save the result"}}