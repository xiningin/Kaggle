{"cell_type":{"73939b11":"code","80e81099":"code","36a4335f":"code","70033103":"code","28ed842d":"code","03eb9b77":"code","01601750":"code","5dfccbae":"code","2f3157de":"code","7fad98e4":"code","98530dfa":"code","79bec2e5":"code","c55f9125":"code","d1566e00":"code","051a4111":"code","52f8808c":"code","9f9a57e5":"code","1d27d9b9":"code","aec39f58":"code","6aa26c7d":"code","607b8c0d":"code","c059d920":"code","fa233993":"code","11544507":"code","120960c6":"markdown","47c4a6e7":"markdown","fb33dc68":"markdown","e0ea7e10":"markdown","ec3f5bde":"markdown","840e8ede":"markdown","3c575b82":"markdown","901830e3":"markdown","b2dacc85":"markdown","5f48d163":"markdown","b137a3f4":"markdown","c7228e22":"markdown","df94610a":"markdown","f14b4360":"markdown","017bf38a":"markdown","3b134940":"markdown","e245aec9":"markdown","cf47144a":"markdown","43749cbe":"markdown","7eabf65e":"markdown","c2e7d68f":"markdown","eed904f6":"markdown","187273a7":"markdown","e21fd163":"markdown","dbb9df66":"markdown","2cd18fa1":"markdown","120adaa4":"markdown","69cd6f2e":"markdown","d31bc974":"markdown","c8b4ef8d":"markdown"},"source":{"73939b11":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","80e81099":"# tensorflow hub\nimport tensorflow_hub as hub\n# tensor flow module\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n# matplotlib\nfrom matplotlib import colors\nfrom matplotlib import pyplot as plt\n\n# word vectorizor\n# first converts the text into a matrix of word counts\n# then transforms these counts by normalizing them based on the term frequency\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# used to create word encoders\nfrom sklearn import preprocessing","36a4335f":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","70033103":"train_df.groupby(\"target\")[\"id\"].nunique()","28ed842d":"print(len(test_df.loc[test_df[\"location\"].notnull(),\"location\"]))\nprint(len(test_df.loc[test_df[\"location\"].notnull(),\"location\"].unique()))","03eb9b77":"print(len(test_df.loc[test_df[\"keyword\"].notnull(),\"keyword\"]))\nprint(len(test_df.loc[test_df[\"keyword\"].notnull(),\"keyword\"].unique()))","01601750":"embed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/3\")\nX_train_embeddings = embed(train_df[\"text\"].values)\nX_test_embeddings = embed(test_df[\"text\"].values)","5dfccbae":"X_train_matrix = X_train_embeddings['outputs'].numpy()","2f3157de":"X_test_matrix = X_test_embeddings['outputs'].numpy()","7fad98e4":"y_train = tf.constant(train_df[\"target\"])","98530dfa":"class TFNaiveBayesClassifier:\n    dist = None\n    \n    # X is the matrix containing the vectors for each sentence\n    # y is the list target values in the same order as the X matrix\n    def fit(self, X, y):\n        unique_y = np.unique(y) # unique target values: 0,1\n        print(unique_y)\n        # `points_by_class` is a numpy array the size of \n        # the number of unique targets.\n        # in each item of the list is another list that contains the vector\n        # of each sentence from the same target value\n        points_by_class = np.asarray([np.asarray(\n            [np.asarray(\n                X.iloc[x,:]) for x in range(0,len(y)) if y[x] == c]) for c in unique_y])\n        mean_list=[]\n        var_list=[]\n        for i in range(0, len(points_by_class)):\n            mean_var, var_var = tf.nn.moments(tf.constant(points_by_class[i]), axes=[0])\n            mean_list.append(mean_var)\n            var_list.append(var_var)\n        mean=tf.stack(mean_list, 0)\n        var=tf.stack(var_list, 0)\n        # Create a 3x2 univariate normal distribution with the \n        # known mean and variance\n        self.dist = tfp.distributions.Normal(loc=mean, scale=tf.sqrt(var))\n        \n    def predict(self, X):\n        assert self.dist is not None\n        nb_classes, nb_features = map(int, self.dist.scale.shape)\n\n        # uniform priors\n        priors = np.log(np.array([1. \/ nb_classes] * nb_classes))\n        \n        # Conditional probabilities log P(x|c)\n        # (nb_samples, nb_classes, nb_features)\n        all_log_probs = self.dist.log_prob(\n            tf.reshape(\n                tf.tile(X, [1, nb_classes]), [-1, nb_classes, nb_features]))\n        # (nb_samples, nb_classes)\n        cond_probs = tf.reduce_sum(all_log_probs, axis=2)\n        \n        # posterior log probability, log P(c) + log P(x|c)\n        joint_likelihood = tf.add(priors, cond_probs)\n\n        # normalize to get (log)-probabilities\n        norm_factor = tf.reduce_logsumexp(\n            joint_likelihood, axis=1, keepdims=True)\n        log_prob = joint_likelihood - norm_factor\n        # exp to get the actual probabilities\n        return tf.exp(log_prob)","79bec2e5":"tf_nb = TFNaiveBayesClassifier()\ntf_nb.fit(pd.DataFrame(X_train_matrix),\n          y_train)","c55f9125":"y_pred = tf_nb.predict(X_test_matrix)","d1566e00":"predProbGivenText_df = pd.DataFrame(y_pred.numpy())\npredProbGivenText_df.head()","051a4111":"uniq_keywords = train_df[\"keyword\"].unique()[1:]\nprint(len(uniq_keywords))\nprint(uniq_keywords)","52f8808c":"def replace_keywords(df_og):\n    df = df_og.copy()\n    df[\"keyword\"] = df[\"keyword\"].replace(\"ablaze\",\"blaze\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"blazing\",\"blaze\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"annihilated\",\"annihilation\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"attacked\",\"attack\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bioterror\",\"bioterrorism\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"blown%20up\",\"blew%20up\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bloody\",\"blood\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bleeding\",\"blood\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"body%20bags\",\"body%20bag\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"body%20bagging\",\"body%20bag\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bombed\",\"bomb\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bombing\",\"bomb\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"burning%20buildings\",\"buildings%20burning\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"buildings%20on%20fire\",\"buildings%20burning\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"burned\",\"burning\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"casualties\",\"casualty\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"catastrophe\",\"catastrophic\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"collapse\",\"collapsed\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"collide\",\"collision\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"collided\",\"collision\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"crash\",\"crashed\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"crush\",\"crushed\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"dead\",\"death\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"deaths\",\"death\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"deluge\",\"deluged\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"demolished\",\"demolish\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"demolition\",\"demolish\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"derailment\",\"derail\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"derailed\",\"derail\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"desolation\",\"desolate\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"destroyed\",\"destroy\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"destruction\",\"destroy\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"detonate\",\"detonation\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"devastated\",\"devastation\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"drowned\",\"drown\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"drowning\",\"drown\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"electrocute\",\"electrocuted\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"evacuated\",\"evacuate\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"evacuation\",\"evacuate\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"explode\",\"explosion\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"exploded\",\"explosion\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"fatality\",\"fatalities\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"floods\",\"flood\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"flooding\",\"flood\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bush%20fires\",\"forest%20fire\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"forest%20fires\",\"forest%20fire\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"hailstorm\",\"hail\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"hazardous\",\"hazard\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"hijacking\",\"hijack\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"hijacker\",\"hijack\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"hostages\",\"hostage\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"injured\",\"injury\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"injures\",\"injury\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"inundated\",\"inundation\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"mass%20murderer\",\"mass%20murder\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"obliterated\",\"obliterate\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"obliteration\",\"obliterate\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"panicking\",\"panic\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"quarantined\",\"quarantine\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"rescuers\",\"rescue\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"rescued\",\"rescue\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"rioting\",\"riot\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"dust%20storm\",\"sandstorm\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"screamed\",\"screams\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"screaming\",\"screams\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"sirens\",\"siren\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"suicide%20bomb\",\"suicide%20bomber\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"suicide%20bombing\",\"suicide%20bomber\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"survived\",\"survive\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"survivors\",\"survive\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"terrorism\",\"terrorist\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"thunderstorm\",\"thunder\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"traumatised\",\"trauma\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"twister\",\"tornado\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"typhoon\",\"hurricane\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"weapons\",\"weapon\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"wild%20fires\",\"wildfire\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"wounded\",\"wounds\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"wrecked\",\"wreckage\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"wreck\",\"wreckage\")\n    return(df)","9f9a57e5":"train_df = replace_keywords(train_df)\ntest_df = replace_keywords(test_df)","1d27d9b9":"uniq_keywords = train_df[\"keyword\"].unique()[1:]\nkword_resArr = []\nprint(len(uniq_keywords))\nfor kword in uniq_keywords:\n    kword_df = train_df.loc[train_df[\"keyword\"] == kword,: ]\n    total_kword = float(len(kword_df))\n    target0_n = float(len(kword_df.loc[kword_df[\"target\"]==0,:]))\n    target1_n = float(len(kword_df.loc[kword_df[\"target\"]==1,:]))\n    kword_prob_df = pd.DataFrame({'keyword':[kword],\n                                 \"keywordPred0\": [target0_n\/total_kword],\n                                 \"keywordPred1\": [target1_n\/total_kword]})\n    kword_resArr.append(kword_prob_df)\npredProbGivenKeyWord_df= pd.concat(kword_resArr)\npredProbGivenKeyWord_df.head()","aec39f58":"test_df[\"textprob0\"]=predProbGivenText_df.loc[:,0].copy()\ntest_df[\"textprob1\"]=predProbGivenText_df.loc[:,1].copy()\ntest_df.head()","6aa26c7d":"test_df = test_df.merge(predProbGivenKeyWord_df, how='left', on=\"keyword\")","607b8c0d":"test_df[\"keywordPred0\"]=test_df[\"keywordPred0\"].fillna(0.5)\ntest_df[\"keywordPred1\"]=test_df[\"keywordPred1\"].fillna(0.5)","c059d920":"test_df[\"pred0\"]=test_df[\"textprob0\"]*test_df[\"keywordPred0\"]\ntest_df[\"pred1\"]=test_df[\"textprob1\"]*test_df[\"keywordPred1\"]\ntest_df[\"target\"]=test_df[\"pred1\"]>test_df[\"pred0\"]\ntest_df[\"target\"] = test_df[\"target\"].astype(np.int)","fa233993":"submission_df = test_df.loc[:,[\"id\",\"target\"]]\nsubmission_df.head()","11544507":"submission_df.to_csv(\"submission.csv\",index=False)","120960c6":"it looks like not even half match a previous tweet. We may eventually have to categorize locations using a different technique than string matching. For example, it may be useful to categorize cities verse suburban areas rather than just the city names.","47c4a6e7":"Tensorflow code is inspired by github repo:\nhttps:\/\/github.com\/nicolov\/naive_bayes_tensorflow\/blob\/master\/tf_iris.py\n","fb33dc68":"Here we initialize our naive bayes model and fit it using the training data","e0ea7e10":"Using the updated set of keywords let's get the probability for each target given a specific key word.","ec3f5bde":"# Exploratory\nI just want to get a general idea of the data before we predict the targets","840e8ede":"# Create submission file","3c575b82":"What about the key words?","901830e3":"Lets create numpy matricies for the features in the training and test set.","b2dacc85":"# Background\nIn this competition we have a keyword(sometimes), location(sometimes), and text to represent a Tweet. Our goal is to predict whether the tweet is describing a real disaster (1) or not (0).\n\nIn this kernel I am going to focus on the text and keyword features to predict a target. First, I must convert the text into vectors to use in my model. I will use a multinominal Naive Bayes classifier to build my model given the text features. Then I will use Naive Bayes to get predictions based on keywords.","5f48d163":"get probabilities given the tweet text","b137a3f4":"Now let's calculate the probability given the text and the keyword. Then let's choose our prediction based on which target has the higher probability.","c7228e22":"First let's create a python object that can fit and predict a naive bayes classifier","df94610a":"# Tweet Text: Sentence Embedding","f14b4360":"How many unique locations are duplicated in the tweets of the tweets that contain locations?","017bf38a":"count how many of each target is found in the training set ","3b134940":"predict probability of each target values in the test set","e245aec9":"The target is my y variable for my model.","cf47144a":"Note that in the code above I only look at the tweets themselves and the keywords, but many of the tweets do not have keywords. We could maybe create a model to infer keywords from tweets that do not have keywords. We also did not include location in the model.","43749cbe":"Next we can convert each of the texts into vectors using a TensorFlow universal-sentence-encoder.","7eabf65e":"This is amazing! **Our training set has no null targets and our targets are fairly balanced as well!**","c2e7d68f":"read in data to pandas dataframes","eed904f6":"Create submission file","187273a7":"# Keyword Naive Bayes","e21fd163":"# Import Data","dbb9df66":"Let's look at our unique key words.","2cd18fa1":"get the probabilities given the key words. Note that if there is no key word than the probability is 50% for both targets.","120adaa4":"Create a dataframe containing the probability of each target given the text in each tweet.","69cd6f2e":"I noticed that there are some keywords that are very similar to each other so I decided to manually make them the same word.","d31bc974":"# Tensor Flow Naiver Bayes","c8b4ef8d":"These may actually be useful in a naive bayes model since each key word will have a conditional probability."}}