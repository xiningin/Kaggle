{"cell_type":{"c3edbb9a":"code","b01dec1e":"code","5f5f7841":"code","f76a937f":"code","3a3824b7":"code","6228fdff":"code","fb76b96f":"code","b01704d8":"code","de41904a":"code","7daba145":"code","2114e928":"code","3a18b06b":"code","bb08221c":"code","a4835e16":"code","e4eb1ad5":"code","3c86cced":"code","af5de5c7":"code","c973c47f":"code","7fa03dc0":"code","31e9e2df":"code","79d0f682":"code","42d5b573":"code","49bfce5f":"code","a491aede":"code","0dc9db39":"code","7fd79e7a":"code","26252a17":"code","8d664caa":"code","bb2261d4":"code","0236a495":"code","9df16bcd":"markdown","8868b892":"markdown","2aa6cf57":"markdown","58d550d1":"markdown","2098b8a5":"markdown","16034aa1":"markdown","cbb19cee":"markdown","04884434":"markdown","2dca6ab1":"markdown","f76bc5de":"markdown","3f904bc1":"markdown","1a460c75":"markdown","7724b93a":"markdown","30c9003a":"markdown","9079e526":"markdown","ccda2d29":"markdown","d02a003d":"markdown","d39b9268":"markdown","0cc84e79":"markdown","4861136a":"markdown","223c15a4":"markdown","7be17243":"markdown","3e26a522":"markdown","8d6f7ddb":"markdown","b8ab65c8":"markdown","059e91ce":"markdown","3466de74":"markdown","768d2733":"markdown","f9dbadce":"markdown","7504e7fb":"markdown","77e77403":"markdown","29deec29":"markdown","6a03edfa":"markdown","271f1cc7":"markdown","8b1d1519":"markdown","529ceab6":"markdown","d8972f81":"markdown","45252235":"markdown","75ea8261":"markdown","0e06a966":"markdown","f300ec4a":"markdown","37594390":"markdown","73ac76ce":"markdown"},"source":{"c3edbb9a":"# Pandas and numpy for data manipulation\nimport warnings\nimport pandas as pd\nimport numpy as np\n\n# Matplotlib and seaborn for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Scipy for statistics\nfrom scipy import stats\n\n# os to manipulate files\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,f1_score \nfrom sklearn.preprocessing import OrdinalEncoder,OneHotEncoder, LabelEncoder\n\nfrom sklearn.model_selection import cross_validate,KFold,GridSearchCV\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# from sklearn.metrics import mean_absolute_error,r2_score\n# from sklearn import linear_model\n# from sklearn.preprocessing import PolynomialFeatures","b01dec1e":"warnings.filterwarnings(\"ignore\")\n\n# Setting seaborn style\nsns.set_style(\"darkgrid\")\nsns.set_palette(sns.color_palette(\n    [\"#7a0723\",\"#ff5b21\",\"#f7cf60\", \"#ff94c0\",\"#323133\"]\n))\n\n# Setting Pandas float format\npd.options.display.float_format = '{:,.1f}'.format\n\nSEED = 42   #The Answer to the Ultimate Question of Life, The Universe, and Everything\nnp.random.seed(SEED)","5f5f7841":"def print_categories(encoder,num_list,title):\n    n_list = np.array(num_list).reshape(-1,)\n    res = [[encoder.inverse_transform([[elem]]).reshape(-1,)[0],elem] for elem in n_list]\n    \n    print('\\nEncoding for '+title)\n    for elem in res:\n        print(' '+str(elem[0])+'\\t->\\t'+str(elem[1]))\n        \n        \ndef cross_validate_interval(results):\n    mean = results['test_score'].mean()\n    std = results['test_score'].std()\n    interval = [mean-2*std,mean+2*std]\n    print('A acur\u00e1cia m\u00e9dia \u00e9 {:.3}%'.format(100*media))\n    print('A acur\u00e1cia est\u00e1 entre {:.3}% e {:.3}%'.format(*(100*np.array(intervalo))))\n    \ndef print_score(scores):\n    mean= scores.mean() * 100\n    std = scores.std() * 100\n    print(\"Accuracy m\u00e9dio %.2f\" % mean)\n    print(\"Intervalo [%.2f, %.2f]\" % (mean - std, mean + std))\n    \ndef hyperparameter_mean(results,hyperpar):\n    str_ = 'param_'+hyperpar\n    return results[[str_,'mean_fit_time','mean_score_time','mean_test_score']].groupby(str_).mean()","f76a937f":"class Classifier:\n    '''\n    Description\n    -----------------\n    \n    Class to approach classification algorithm\n    \n    \n    Example\n    -----------------\n        classifier = Classifier(\n                 algorithm = ChooseTheAlgorith,\n                 hyperparameters_range = {\n                    'hyperparameter_1': [1,2,3],\n                    'hyperparameter_2': [4,5,6],\n                    'hyperparameter_3': [7,8,9]\n                 }\n             )\n\n        # Looking for best model\n        classifier.grid_search_fit(X,y,n_splits=10)\n        #dt.grid_search_results.head(3)\n\n        # Prediction Form 1\n        par = classifier.best_model_params\n        dt.fit(X_trn,y_trn,params = par)\n        y_pred = classifier.predict(X_tst)\n        print(accuracy_score(y_tst, y_pred))\n\n        # Prediction Form 2\n        classifier.fit(X_trn,y_trn,params = 'best_model')\n        y_pred = classifier.predict(X_tst)\n        print(accuracy_score(y_tst, y_pred))\n\n        # Prediction Form 3\n        classifier.fit(X_trn,y_trn,min_samples_split = 5,max_depth=4)\n        y_pred = classifier.predict(X_tst)\n        print(accuracy_score(y_tst, y_pred))\n    '''\n    def __init__(self,algorithm, hyperparameters_range={},random_state=42):\n        \n        self.algorithm = algorithm\n        self.hyperparameters_range = hyperparameters_range\n        self.random_state = random_state\n        self.grid_search_cv = None\n        self.grid_search_results = None\n        self.hyperparameters = self.__get_hyperparameters()\n        self.best_model = None\n        self.best_model_params = None\n        self.fitted_model = None\n        \n    def grid_search_fit(self,X,y,verbose=0,n_splits=10,shuffle=True,scoring='accuracy'):\n        \n        self.grid_search_cv = GridSearchCV(\n            self.algorithm(),\n            self.hyperparameters_range,\n            cv = KFold(n_splits = n_splits, shuffle=shuffle, random_state=self.random_state),\n            scoring=scoring,\n            verbose=verbose\n        )\n        \n        self.grid_search_cv.fit(X, y)\n        \n        col = list(map(lambda par: 'param_'+str(par),self.hyperparameters))+[\n                'mean_fit_time',\n                'mean_test_score',\n                'std_test_score',\n                'params'\n              ]\n        \n        results = pd.DataFrame(self.grid_search_cv.cv_results_)\n        \n        self.grid_search_results = results[col].sort_values(\n                    ['mean_test_score','mean_fit_time'],\n                    ascending=[False,True]\n                ).reset_index(drop=True)\n        \n        self.best_model = self.grid_search_cv.best_estimator_\n        \n        self.best_model_params = self.best_model.get_params()\n    \n    def best_model_cv_score(self,X,y,parameter='test_score',verbose=0,n_splits=10,shuffle=True,scoring='accuracy'):\n        if self.best_model != None:\n            cv_results = cross_validate(\n                self.best_model,\n                X = X,\n                y = y,\n                cv=KFold(n_splits = 10,shuffle=True,random_state=self.random_state)\n            )\n            return {\n                parameter+'_mean': cv_results[parameter].mean(),\n                parameter+'_std': cv_results[parameter].std()\n            }\n        \n    def fit(self,X,y,params=None,**kwargs):\n        model = None\n        if len(kwargs) == 0 and params == 'best_model' and self.best_model != None:\n            model = self.best_model\n            \n        elif type(params) == dict and len(params) > 0:\n            model = self.algorithm(**params)\n            \n        elif len(kwargs) >= 0 and params==None:\n            model = self.algorithm(**kwargs)\n            \n        else:\n            print('[Error]')\n            \n        if model != None:\n            model.fit(X,y)\n            \n        self.fitted_model = model\n            \n    def predict(self,X):\n        if self.fitted_model != None:\n            return self.fitted_model.predict(X)\n        else:\n            print('[Error]')\n            return np.array([])\n            \n    def predict_score(self,X_tst,y_tst,score=accuracy_score):\n        if self.fitted_model != None:\n            y_pred = self.predict(X_tst)\n            return score(y_tst, y_pred)\n        else:\n            print('[Error]')\n            return np.array([])\n        \n    def hyperparameter_info(self,hyperpar):\n        str_ = 'param_'+hyperpar\n        return self.grid_search_results[\n                [str_,'mean_fit_time','mean_test_score']\n            ].groupby(str_).agg(['mean','std'])\n        \n    def __get_hyperparameters(self):\n        return [hp for hp in self.hyperparameters_range]","3a3824b7":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6228fdff":"df1 = pd.read_csv('\/kaggle\/input\/drug-classification\/drug200.csv')#.drop('Unnamed: 0',axis=1)\n\ndf1.columns = ['age','gender','pressure','cholesterol','na_k_ratio','drug']\ndf1['drug'] = df1['drug'].str.replace('drug','Drug')\ndf1.sample(5)","fb76b96f":"df1.describe(include='all')","b01704d8":"# Set up the matplotlib figure\nf, axes = plt.subplots(2, 2, figsize=(12, 12))\n#sns.despine(left=True)\nsns.countplot(x=df1['drug'],data=df1, ax=axes[0,0])\nsns.countplot(x=df1['drug'],hue='cholesterol',data=df1, ax=axes[0,1])\nsns.countplot(x=df1['drug'],hue='gender',data=df1, ax=axes[1,0])\nsns.countplot(x=df1['drug'],hue='pressure',data=df1, ax=axes[1,1])\n\naxes[0,0].set(xlabel='', ylabel='Number of Occurrences',title='Drug Occurrences')\naxes[0,1].set(xlabel='', ylabel='Number of Occurrences',title='Relation between Drug and Cholesterol')\naxes[1,0].set(xlabel='', ylabel='Number of Occurrences',title='Relation between Drug and Gender')\naxes[1,1].set(xlabel='', ylabel='Number of Occurrences',title='Relation between Drug and Blood Pressure')\n\naxes[0,1].legend(loc=1,title='Cholesterol')\naxes[1,0].legend(loc=1,title='Gender')\naxes[1,1].legend(loc=1,title='Blood Pressure')\n\nplt.show()","de41904a":"ax1 = sns.catplot(x='drug', y='age', data = df1)\nax2 = sns.catplot(x='drug', y='na_k_ratio', data = df1)\n\nax1.set(xlabel='', ylabel='Age',title=\"People's age\")\nax2.set(xlabel='', ylabel='Na\/K Ratio',title=\"Urinary Na\/K ratio\")\n\n\nplt.show()","7daba145":"g = sns.pairplot(df1, hue=\"drug\")","2114e928":"# Splitting target from features\ny_column = 'drug'\nX_columns = list(df1.drop(columns=y_column).columns)\n\nX = df1[X_columns]\ny = df1[y_column]","3a18b06b":"Ord_Encoder = OrdinalEncoder(categories=[['LOW', 'NORMAL', 'HIGH']])\nGender_Encoder = LabelEncoder()\nDrugs_Encoder = LabelEncoder()\n\npressure_list = [[elem] for elem in X['pressure'].to_list()]\nX['pressure'] = Ord_Encoder.fit_transform(pressure_list).astype(int)\n\ncholesterol_list = [[elem] for elem in X['cholesterol'].to_list()]\nX['cholesterol'] = Ord_Encoder.fit_transform(cholesterol_list).astype(int)\n\nX['gender'] = Gender_Encoder.fit_transform(X[['gender']]).astype(int)\n\ny = Drugs_Encoder.fit_transform(y)\n\ndf_enc = pd.concat([X,pd.DataFrame(y,columns=['drug'])],axis=1)","bb08221c":"print_categories(Ord_Encoder,X['pressure'].unique(),'pressure')\nprint_categories(Ord_Encoder,X['cholesterol'].unique(),'cholesterol')\nprint_categories(Gender_Encoder,X['gender'].unique(),'gender')\nprint_categories(Drugs_Encoder,np.unique(y),'drugs')","a4835e16":"df_enc.sample(5)","e4eb1ad5":"X_trn, X_tst, y_trn, y_tst = train_test_split(\n    X, y, test_size = 0.33,stratify = y,random_state=42)","3c86cced":"from sklearn.preprocessing import StandardScaler\n\nscaler_trn = StandardScaler()\nX_scl_trn = scaler_trn.fit_transform(X_trn)\n\nscaler_tst = StandardScaler()\nX_scl_tst = scaler_trn.fit_transform(X_tst)\n\nscaler = StandardScaler()\nX_scl = scaler_trn.fit_transform(X)","af5de5c7":"X_scl = pd.concat([\n            pd.DataFrame(X_scl,columns=X.columns),\n            pd.DataFrame(y,columns=['drug'])\n        ],axis=1)\n\nX_trn_scl = pd.concat([\n            pd.DataFrame(X_scl_trn,columns=X.columns),\n            pd.DataFrame(y_trn,columns=['drug'])\n        ],axis=1)\n\nX_tst_scl = pd.concat([\n            pd.DataFrame(X_scl_tst,columns=X.columns),\n            pd.DataFrame(y_tst,columns=['drug'])\n        ],axis=1)","c973c47f":"pd.options.display.float_format = '{:,.4f}'.format\n\nModel_Scores = {}","7fa03dc0":"SEED = 40\nmodel_dummy = DummyClassifier(strategy='stratified',random_state = SEED)\n\ndummy_results = cross_validate(\n    model_dummy,\n    X = X,\n    y = y,\n    cv=KFold(n_splits = 200,shuffle=True,random_state = SEED)\n)\n\ndummy_score = dummy_results['test_score'].mean()\n\nmodel_dummy = DummyClassifier(strategy='stratified',random_state = SEED)\n\nmodel_dummy.fit(X_trn,y_trn)\ny_pred = model_dummy.predict(X_tst)\ndummy_acc_score = accuracy_score(y_tst,y_pred)\n\n\nprint('Dummy Accuracy Score: %.2f' % (dummy_acc_score))\nprint('Dummy CV Score: %.1f%%' % (100*dummy_score))\n\n\nModel_Scores['dummy'] = {\n    'model' : model_dummy,\n    'best_params' : model_dummy.get_params(),\n    'test_accuracy_score' : dummy_acc_score,\n    'cv_score' : dummy_results['test_score'].mean(),\n    'cv_score_std' : dummy_results['test_score'].std()\n}","31e9e2df":"\"\"\"\n    From previous tests, it was possible to conclude that sag and saga solvers\n    retrieve the worst results. I'll not consider this in GridSearchCV.\n\"\"\"\nSEED = 42\n\nhyperparametric_space = {\n    'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n    'C' : [5,7,10,20,30,50,100]\n}\n\ngrid_search_cv = GridSearchCV(\n    LogisticRegression(random_state=SEED),\n    hyperparametric_space,\n    cv = KFold(n_splits = 10, shuffle=True,random_state=SEED),\n    scoring='accuracy',\n    verbose=0\n)\n\ngrid_search_cv.fit(X, y)\nresults = pd.DataFrame(grid_search_cv.cv_results_)\n\npd.options.display.float_format = '{:,.5f}'.format\n\ncol = ['param_C', 'param_solver','mean_fit_time', 'mean_test_score', 'std_test_score']\n\nresults[col].sort_values(\n    ['mean_test_score','mean_fit_time'],\n    ascending=[False,True]\n).head(10)","79d0f682":"SEED = 42\n\nbest_solver = 'newton-cg'\n\nres_liblinear = results[results['param_solver']==best_solver].sort_values(\n    ['mean_test_score','mean_fit_time'], ascending=[False,True] )\n\nlogReg_model_params = res_liblinear['params'].iloc[0]\n\nlogReg_model = LogisticRegression(**logReg_model_params,random_state=SEED)\n\nlogReg_cv_results = cross_validate(\n    logReg_model,\n    X = X,\n    y = y,\n    cv=KFold(n_splits = 10,shuffle=True,random_state=SEED)\n)\n\nlogReg_cv_score = logReg_cv_results['test_score'].mean()\nlogReg_cv_std = logReg_cv_results['test_score'].std()\n\nprint('Logistic Regression Classifier CV Score: %.2f%% \u00b1 %.2f%%' % (100*logReg_cv_score,100*logReg_cv_std))","42d5b573":"SEED = 42\n\nlogReg = LogisticRegression(**logReg_model_params,random_state=SEED)\n\nlogReg.fit(X_trn,y_trn)\n\ny_pred = logReg.predict(X_tst)\n\nlogReg_score = accuracy_score(y_tst, y_pred)\nlogReg_score","49bfce5f":"Model_Scores['logistic_regression'] = {\n    'model' : logReg,\n    'best_params' : logReg.get_params(),\n    'test_accuracy_score' : logReg_score,\n    'cv_score' : logReg_cv_score,\n    'cv_score_std' : logReg_cv_std\n}","a491aede":"# Instantiating the Classifier class\nlog = Classifier(\n         algorithm = LogisticRegression,\n         hyperparameters_range = {\n            'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n            'C' : [5,7,10,20,30,50,100]\n        }\n     )\n\nlog.grid_search_fit(X,y)\n\nprint('\\nBest Model:')\nprint('\\n',log.best_model)\n\nsc_dict = log.best_model_cv_score(X,y)\nsc_list = list((100*np.array(list(sc_dict.values()))))\nprint('\\nCV Score: %.2f%% \u00b1 %.2f%%' % (sc_list[0],sc_list[1]))\n\nlog.fit(X_trn,y_trn,params = 'best_model')\npsc = log.predict_score(X_tst,y_tst)\nprint('\\nAccuracy Score: %.2f ' % psc)\n\nModel_Scores['logistic_regression'] = {\n    'model' : log.best_model,\n    'best_params' : log.best_model_params,\n    'test_accuracy_score' : psc,\n    'cv_score' : 0.01*sc_list[0],\n    'cv_score_std' : 0.01*sc_list[1]\n}\n\nlog.grid_search_results.head(5)","0dc9db39":"sv = Classifier(\n         algorithm = SVC,\n         hyperparameters_range = {\n                'kernel' : ['linear', 'poly','rbf','sigmoid'],\n                'C' : [0.1,0.5,1,3,7,10]\n            }\n     )\n\nsv.grid_search_fit(X,y)\n\nprint('\\nBest Model:')\nprint('\\n',sv.best_model)\n\nsc_dict = sv.best_model_cv_score(X,y)\nsc_list = list((100*np.array(list(sc_dict.values()))))\nprint('\\nCV Score: %.2f%% \u00b1 %.2f%%' % (sc_list[0],sc_list[1]))\n\nsv.fit(X_trn,y_trn,params = 'best_model')\npsc = sv.predict_score(X_tst,y_tst)\nprint('\\nAccuracy Score: %.2f ' % (psc))\n\n\nModel_Scores['svc'] = {\n    'model' : sv.best_model,\n    'best_params' : sv.best_model_params,\n    'test_accuracy_score' : psc,\n    'cv_score' : 0.01*sc_list[0],\n    'cv_score_std' : 0.01*sc_list[1]\n}\n\nsv.grid_search_results.head(5)","7fd79e7a":"dt = Classifier(\n         algorithm = DecisionTreeClassifier,\n         hyperparameters_range = {\n            'min_samples_split': [2,5,10],\n            'max_depth': [2,5,10],\n            'min_samples_leaf': [1,5,10]\n         }\n     )\n\ndt.grid_search_fit(X,y)\n\nprint('\\nBest Model:')\nprint('\\n',dt.best_model)\n\nsc_dict = dt.best_model_cv_score(X,y)\nsc_list = list((100*np.array(list(sc_dict.values()))))\nprint('\\nCV Score: %.2f%% \u00b1 %.2f%%' % (sc_list[0],sc_list[1]))\n\ndt.fit(X_trn,y_trn,params = 'best_model')\npsc = dt.predict_score(X_tst,y_tst)\nprint('\\nAccuracy Score: %.2f ' % (psc))\n\nModel_Scores['decision_tree'] = {\n    'model' : dt.best_model,\n    'best_params' : dt.best_model_params,\n    'test_accuracy_score' : psc,\n    'cv_score' : 0.01*sc_list[0],\n    'cv_score_std' : 0.01*sc_list[1]\n}\n\ndt.grid_search_results.head(5)","26252a17":"gnb = Classifier(\n         algorithm = GaussianNB,\n         hyperparameters_range = {\n            'var_smoothing': [1e-09,1e-08,1e-07,1e-06,1e-05,4e-05,1e-04],\n         }\n     )\n\ngnb.grid_search_fit(X,y)\n\nprint('\\nBest Model:')\nprint('\\n',gnb.best_model)\n\nsc_dict = gnb.best_model_cv_score(X,y)\nsc_list = list((100*np.array(list(sc_dict.values()))))\nprint('\\nCV Score: %.2f%% \u00b1 %.2f%%' % (sc_list[0],sc_list[1]))\n\ngnb.fit(X_trn,y_trn,params = 'best_model')\nprint('\\nAccuracy Score: %.2f ' % (gnb.predict_score(X_tst,y_tst)))\n\n\npd.options.display.float_format = '{:,.8f}'.format\n\nModel_Scores['gaussian_nb'] = {\n    'model' : gnb.best_model,\n    'best_params' : gnb.best_model_params,\n    'test_accuracy_score' : psc,\n    'cv_score' : 0.01*sc_list[0],\n    'cv_score_std' : 0.01*sc_list[1]\n}\n\ngnb.grid_search_results.head(9)","8d664caa":"knn = Classifier(\n         algorithm = KNeighborsClassifier,\n         hyperparameters_range = {\n            'n_neighbors': [2,5,10,20],\n             'weights' : ['uniform', 'distance'],\n             'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n             'p' : [2,3,4,5]\n         }\n     )\n\nknn.grid_search_fit(X,y)\n\nprint('\\nBest Model:')\nprint('\\n',knn.best_model)\n\nsc_dict = knn.best_model_cv_score(X,y)\nsc_list = list((100*np.array(list(sc_dict.values()))))\nprint('\\nCV Score: %.2f%% \u00b1 %.2f%%' % (sc_list[0],sc_list[1]))\n\nknn.fit(X_trn,y_trn,params = 'best_model')\npsc = knn.predict_score(X_tst,y_tst)\nprint('\\nAccuracy Score: %.2f ' % (psc))\n\n\npd.options.display.float_format = '{:,.3f}'.format\n\n\nModel_Scores['knn_classifier'] = {\n    'model' : knn.best_model,\n    'best_params' : knn.best_model_params,\n    'test_accuracy_score' : psc,\n    'cv_score' : 0.01*sc_list[0],\n    'cv_score_std' : 0.01*sc_list[1]\n}\n\n\nknn.grid_search_results.head(9)","bb2261d4":"rf = Classifier(\n         algorithm = RandomForestClassifier,\n         hyperparameters_range = {\n            'n_estimators': [5,10,15,20,25,30,50,100],\n             'random_state': [42]\n         }\n     )\n\nrf.grid_search_fit(X,y)\n\nprint('\\nBest Model:')\nprint('\\n',rf.best_model)\n\nsc_dict = rf.best_model_cv_score(X,y)\nsc_list = list((100*np.array(list(sc_dict.values()))))\nprint('\\nCV Score: %.2f%% \u00b1 %.2f%%' % (sc_list[0],sc_list[1]))\n\nrf.fit(X_trn,y_trn,params = 'best_model')\npsc = rf.predict_score(X_tst,y_tst)\nprint('\\nAccuracy Score: %.2f ' % (psc))\n\n\npd.options.display.float_format = '{:,.3f}'.format\n\n\nModel_Scores['random_forest'] = {\n    'model' : rf.best_model,\n    'best_params' : rf.best_model_params,\n    'test_accuracy_score' : psc,\n    'cv_score' : 0.01*sc_list[0],\n    'cv_score_std' : 0.01*sc_list[1]\n}\n\n\nrf.grid_search_results.head(9)","0236a495":"pd.DataFrame([\n    [\n        key,\n        Model_Scores[key]['test_accuracy_score'],\n        Model_Scores[key]['cv_score'],\n        Model_Scores[key]['cv_score_std']\n    ]\n    \n    for key in Model_Scores\n],columns=['model','accuracy_score','cv_score','cv_score_std'])","9df16bcd":"<a id=\"init\"><\/a>\n\n# Initializing","8868b892":"<a id=\"is-dep\"><\/a>\n\n<h2>2.1 Packages and Modules <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#second\">\u21bb<\/a><\/h2>","2aa6cf57":"**Cros-Validate Score**","58d550d1":"<a id=\"fourth\" style=\"\n  background-color: #7a0723;\n  border: none;\n  color: white;\n  padding: 2px 10px;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  font-size: 10px;\" href=\"#toc\">TOC \u21bb<\/a>\n  \n<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h1>4. Pre-Processing<\/h1>\n        <\/center>\n\n   \n<ol>\n    <li><a href=\"#pp-feat-target\" style=\"color: #7a0723;\">Defing Feature and Target Variables<\/a><\/li>\n    <li><a href=\"#pp-enc\" style=\"color: #7a0723;\">Encoding Categorical Features<\/a><\/li>\n    <li><a href=\"#pp-train-test\" style=\"color: #7a0723;\">Splitting dataset into train and test<\/a><\/li>\n    <li><a href=\"#pp-scl\" style=\"color: #7a0723;\">Scaling Features<\/a><\/li>\n<!--     <ol>\n        <li><a href=\"#ea-feat-descr\" style=\"color: #7a0723;\">Feature Description<\/a><\/li>\n        <li><a href=\"#ea-feat-cat\" style=\"color: #7a0723;\">Categorical Features<\/a><\/li>\n        <li><a href=\"#ea-feat-num\" style=\"color: #7a0723;\">Numerical Features<\/a><\/li>\n    <\/ol> -->\n<\/ol>\n\n\n<\/div>","2098b8a5":"To construct a system capable to make good predictions, it is necessary to understand how the features influence the desired variable. To make this, I will split my work in two stages:\n\n1. Exploring: An exploratory analysis, to understand the structure of data, missing values, correlations and etc, to select the best set of features to be used at the analysis.\n\n2. Modeling: In this step, I try to find the best and simplest model for describing the data. I do this by testing the influence of features on target values. Since that the definition of a best model is something complicated and subjective, I will choose the model which best efficiency in the follow approach:\n\n    > A. For each Classification Algorithm, I will build a shuffled K-Folded Cross Validation model with `n_splits`= 10\n    \n    > B. After, I'll optimize hyperparameters with a GridSearchCV. The configuration retrieving the higher cv_score will be called 'best_model' for that algorithm\n\n    > C. Split dataset into test (33%) and training (67%)\n\n    > D. Use training dataset to fit the inner parameters of best_model\n\n    > E. Use the trained model to predict the drug variable of test dataset\n\n    > F. Use predicted an original total to evaluate the accuracy_score\n\n    > G. After this, analyzing cv_score and accuracy_score, I'll choose the best estimator for the problem","16034aa1":"<a id=\"pp-train-test\"><\/a>\n\n<h2>4.3 Splitting dataset into train and test<a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fourth\">\u21bb<\/a><\/h2>\n  \n","cbb19cee":"<!-- <a id=\"first\"><\/a> -->\n\n<a id=\"first\" style=\"\n  background-color: #7a0723;\n  border: none;\n  color: white;\n  padding: 2px 10px;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  font-size: 10px;\" href=\"#toc\">TOC \u21bb<\/a>\n  \n  \n<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h1>1. Introduction<\/h1>\n        <\/center>\n\n   \n<ol>\n<!--     <li><a href=\"#ea-init\">Initializing<\/a><\/li> -->\n\n<\/ol>\n\n\n<\/div>","04884434":"<h3> B. Logistic Regression: Best Model<a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#ml-log\">\u21bb<\/a><\/h3>","2dca6ab1":"<a id=\"ml-tree\"><\/a>\n\n<h2>5.4 Decision Tree Classifier <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fifth\">\u21bb<\/a><\/h2>\n  \n  The complete call of a Support Vector Classifier is:\n```\nDecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n```","f76bc5de":"<a id=\"ml-log\"><\/a>\n\n<h2>5.2 Logistic Regression <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fifth\">\u21bb<\/a><\/h2>\n  \n  The complete call of a logistic regression is:\n```\nLogisticRegression(\n    penalty='l2', *, dual=False, tol=0.0001, C=1.0, \n    fit_intercept=True, intercept_scaling=1, \n    class_weight=None, random_state=None, solver='lbfgs',\n    max_iter=100, multi_class='auto', verbose=0, \n    warm_start=False, n_jobs=None, l1_ratio=None)\n```\n\nHere, I'll looking for the best configuration of hyperparameters `C` and `solver`","3f904bc1":"<a id=\"fifth\" style=\"\n  background-color: #7a0723;\n  border: none;\n  color: white;\n  padding: 2px 10px;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  font-size: 10px;\" href=\"#toc\">TOC \u21bb<\/a>\n  \n<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h1>5. Machine Learning: Classification Algorithms<\/h1>\n        <\/center>\n\n   \n<ol>\n    <li><a href=\"#ml-dummy\" style=\"color: #7a0723;\">Dummy Classifier<\/a><\/li>\n    <li><a href=\"#ml-log\" style=\"color: #7a0723;\">Logistic Regression<\/a><\/li>\n    <li><a href=\"#ml-svc\" style=\"color: #7a0723;\">Suport Vector Classifier<\/a><\/li>\n    <li><a href=\"#ml-tree\" style=\"color: #7a0723;\">Decision Tree Classifier<\/a><\/li>\n    <li><a href=\"#ml-gnb\" style=\"color: #7a0723;\">Gaussian Naive Bayes<\/a><\/li>\n    <li><a href=\"#ml-knn\" style=\"color: #7a0723;\">K-Nearest Neighbors Classifier<\/a><\/li>\n    <li><a href=\"#ml-rf\" style=\"color: #7a0723;\">Random Forest Classifier<\/a><\/li>\n    <li><a href=\"#ml-comp\" style=\"color: #7a0723;\">Comparison of Model Scores<\/a><\/li>\n    \n    \n\n<!--     <li><a href=\"#ea-data\" style=\"color: #7a0723;\">The Dataset<\/a><\/li> -->\n<!--     <ol>\n        <li><a href=\"#ea-feat-descr\" style=\"color: #7a0723;\">Feature Description<\/a><\/li>\n        <li><a href=\"#ea-feat-cat\" style=\"color: #7a0723;\">Categorical Features<\/a><\/li>\n        <li><a href=\"#ea-feat-num\" style=\"color: #7a0723;\">Numerical Features<\/a><\/li>\n    <\/ol> -->\n<\/ol>\n\n\n<\/div>","1a460c75":"**Observations**\n1. **DrugY**: Mainly for `Na_to_K $\\gtrsim 10$ \n2. Except for **DrugY**, all drugs exhibits `Na_to_K` $\\lesssim 10$ \n3. **DrugA** not observed in people over 55 years old (conf: ~95%)\n3. **DrugB** mainly for people over 48 years old (conf: ~95%)","7724b93a":"<a id=\"ea-data\"><\/a>\n<h2>3.1 The Dataset <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#third\">\u21bb<\/a><\/h2>","30c9003a":"<a id=\"third\" style=\"\n  background-color: #7a0723;\n  border: none;\n  color: white;\n  padding: 2px 10px;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  font-size: 10px;\" href=\"#toc\">TOC \u21bb<\/a>\n  \n<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h1>3. Exploratory Analysis<\/h1>\n        <\/center>\n\n   \n<ol>\n    <li><a href=\"#ea-data\" style=\"color: #7a0723;\">The Dataset<\/a><\/li>\n    <li><a href=\"#ea-feat-descr\" style=\"color: #7a0723;\">Feature Description<\/a><\/li>\n    <li><a href=\"#ea-feat-cat\" style=\"color: #7a0723;\">Categorical Features<\/a><\/li>\n    <li><a href=\"#ea-feat-num\" style=\"color: #7a0723;\">Numerical Features<\/a><\/li>\n\n<\/ol>\n\n\n<\/div>","9079e526":"<a id=\"ml-knn\"><\/a>\n\n<h2>5.6 K-Nearest Neighbors Classifier  <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fifth\">\u21bb<\/a><\/h2>\n  \n  The complete call of a  k-nearest neighbors  is:\n```\nKNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n```","ccda2d29":"Now, I will do the same work using the <a href=\"#class-class\" style=\"color: #7a0723;\">classifier class<\/a>.","d02a003d":"<a id=\"sixth\" style=\"\n  background-color: #7a0723;\n  border: none;\n  color: white;\n  padding: 2px 10px;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  font-size: 10px;\" href=\"#toc\">TOC \u21bb<\/a>\n  \n<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h1>6. Conclusion<\/h1>\n        <\/center>\n\n   \n<\/div>\nThe use of the `GridSearchCV` for hyperperparamers optimization proved to be very efficient since most of the algorithms are returning excellent results for predicting the target variable. Here, the model selection criteria points to the Support Vector Classifier since it exhibits the higher `cv_score`.","d39b9268":"<a id=\"ml-rf\"><\/a>\n\n<h2>5.7 Random Forest Classifier  <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fifth\">\u21bb<\/a><\/h2>\n  \n  The complete call of a Random Forest Classifier is:\n```\nRandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n```","0cc84e79":"From the results of previous section, we can see that `newton-cg` presents the best test score $ \\left( 98\\% \\pm 3\\% \\right)$. But, if we compare with the best parametric configuration for `liblinear`  $ \\left( 96\\% \\pm 4\\% \\right)$, we can see that the difference is not too big when considering standard deviation. However, there is a big advantage in use `liblinear`: the fit time is much smaller than `newton-cg`  ($0.006 << 0.16 $). I will adopt `newton-cg` with the best hyperparametric configuration as my best Logistic Regression Model but is good to keep in mind that if it were a model to work in real life,  `liblinear` is the best choice because of its fit time.","4861136a":"<a id=\"pp-enc\"><\/a>\n\n<h2>4.2 Encoding Categorical Features<a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fourth\">\u21bb<\/a><\/h2>\n  \n\nFor the case of nominal categorical variables, I will use the `OneHotEnconder` technique. However, as the unique nominal categorical feature  has only two alternatives, the encoding process gives rise to one column (avoiding the dummy variable trap). Because of this, we choose to use the `LabelEncoder`. In the case of ordinal categorical variables, the best approach will be `OrdinalEncoder`.\n\nThe target exhibits 5 different values. It needs to be encoded in a single column such that the `OneHotEncoder` method is not the most indicated one. Since that there is no concept of order in target values, th best encoding technique is `LabelEnconder`","223c15a4":"<a id=\"is-func\"><\/a>\n\n<h2>2.3 Useful Functions <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#second\">\u21bb<\/a><\/h2>","7be17243":"<a id=\"is-set\"><\/a>\n\n<h2>2.2 Settings <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#second\">\u21bb<\/a><\/h2>","3e26a522":"Import Dataset","8d6f7ddb":"<a id=\"pp-scl\"><\/a>\n\n<h2>4.4 Scaling Features<a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fourth\">\u21bb<\/a><\/h2>\n  \n","b8ab65c8":"<a id=\"ea-feat-cat\"><\/a>\n  \n<h2>3.3 Categorical Features <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#third\">\u21bb<\/a><\/h2>","059e91ce":"In this section, I will repeat the same approach for different classification algorithms. The approach is detailed for the first case (Logistic Regression) and, for the others, I will use the class implemented in the <a href=\"#class-class\" style=\"color: #7a0723;\">Class to Classifiers<\/a> section.","3466de74":"<a id=\"pp-feat-target\"><\/a>\n\n<h2>4.1 Defining Features and Target Variables <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fourth\">\u21bb<\/a><\/h2>","768d2733":"<h3> A. Logistic Regression: Grid-Search with Cross Validation<a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#ml-log\">\u21bb<\/a><\/h3>","f9dbadce":"**Test Data Score**","7504e7fb":"<a id=\"ea-feat-num\"><\/a>\n<h2>3.4 Numerical Features <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#third\">\u21bb<\/a><\/h2>","77e77403":"<a id=\"ea-feat-descr\"><\/a>\n<h2>3.2 Feature Description  <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#third\">\u21bb<\/a><\/h2>\n\n`age`(*RN*): the age of the person\n\n`sex`(*NC*): the gender of the person\n\n`pressure`(*OC*): the blood pressure\n\n`cholesterol`(*OC*): cholesterol levels in blood test\n\n`na_k_ratio`(*RN*): urinary Na\/K ratio\n\nPS.:\n* (*RN*) : real numeric variable\n* (*NC*) : nominal categorical variable\n* (*OC*) : ordinal categorical variable","29deec29":"<h3> C. Using Classifier Class<a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#ml-log\">\u21bb<\/a><\/h3>","6a03edfa":"<a id=\"ml-svc\"><\/a>\n\n<h2>5.3 Support Vector Classifier <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fifth\">\u21bb<\/a><\/h2>\n  \n  The complete call of a Support Vector Classifier is:\n```\nSVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n```","271f1cc7":"I want to define a dummy classifier to compare the efficiency of our classifcation approach with a random classification.","8b1d1519":"<a id=\"ml-gnb\"><\/a>\n\n<h2>5.5 Gaussian Naive Bayes  <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fifth\">\u21bb<\/a><\/h2>\n  \n  The complete call of a Gaussian Naive Bayes is:\n```\nGaussianNB(*, priors=None, var_smoothing=1e-09)\n```","529ceab6":"<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h2>Table of Contents<\/h2>\n        <\/center>\n\n   \n<ol>\n    <li><a href=\"#first\" style=\"color: #7a0723;\">Introduction<\/a><\/li>\n    <li><a href=\"#second\" style=\"color: #7a0723;\">Initializing Script<\/a><\/li>\n    <li><a href=\"#third\" style=\"color: #7a0723;\">Exploratory Analysis<\/a><\/li>\n    <li><a href=\"#fourth\" style=\"color: #7a0723;\">Pre-Processing<\/a><\/li>\n    <li><a href=\"#fifth\" style=\"color: #7a0723;\">Machine Learning: Classification Algorithms<\/a><\/li>\n\n<!--     <li><a href=\"#seventh\"><\/a><\/li> -->\n<!--     <li><a href=\"#eighth\">Teste t pareado para duas m\u00e9dias<\/a><\/li> -->\n<!--     <li><a href=\"#ninth\">Teste F para duas vari\u00e2ncias<\/a><\/li> -->\n<!--     <li><a href=\"#tenth\">Teste t para duas m\u00e9dias<\/a><\/li> -->\n<\/ol>\n\n\n<\/div>","d8972f81":"<a id=\"ml-comp\"><\/a>\n\n<h2>5.8 Comparison of Models  <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fifth\">\u21bb<\/a><\/h2>","45252235":"![title](https:\/\/raw.githubusercontent.com\/emdemor\/drug-classification\/master\/source\/title.png)\n<a id=\"toc\"><\/a>","75ea8261":"<a id=\"class-class\"><\/a>\n### Class to Classifiers","0e06a966":"<a id=\"ml-dummy\"><\/a>\n\n<h2>5.1 Dummy Classifier <a style=\"\n  border-radius: 10px;\n  background-color: #f1f1f1;\n  border: none;\n  color: #7a0723;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  padding: 4px 4px;\n  font-size: 14px;\" href=\"#fifth\">\u21bb<\/a><\/h2>","f300ec4a":"### Useful Functions & Classes","37594390":"**Observations**\n\n* **DrugY** is the most observed\n\n* **DrugC** is only observed in people with high cholesterol and low blood pressure\n\n* **DrugX** is observed more in people with normal cholesterol than with high cholesterol and normal bloos pressure\n\n* **DrugA** is only observed on people with high blood pressure\n\n* **DrugB** is only observed on people with high blood pressure","73ac76ce":"<a id=\"second\" style=\"\n  background-color: #7a0723;\n  border: none;\n  color: white;\n  padding: 2px 10px;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  font-size: 10px;\" href=\"#toc\">TOC \u21bb<\/a>\n\n  \n<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h1>2. Initializing Script<\/h1>\n        <\/center>\n\n   \n   \n<ol>\n    <li><a href=\"#is-dep\" style=\"color: #7a0723;\">Packages and Modules<\/a><\/li>\n    <li><a href=\"#is-set\" style=\"color: #7a0723;\">Settings<\/a><\/li>\n    <li><a href=\"#is-func\" style=\"color: #7a0723;\">Useful Functions<\/a><\/li>\n<\/ol>\n\n\n<\/div>"}}