{"cell_type":{"3653ba7a":"code","8b246cf3":"code","b281b819":"code","4307b6e4":"code","a9117d0a":"code","08ffd802":"code","4000c55b":"code","3ae7e679":"code","61fd4eab":"code","aac1d681":"code","d4a9090f":"code","349bd4b9":"code","0695ac96":"code","421a5379":"code","0797e4c7":"code","6d0ce46a":"code","643b72db":"code","a9f58d79":"code","dee88714":"code","f33b74db":"code","8226e3fa":"code","7623ea64":"code","5fd8f008":"code","b1d32ea3":"code","6fedee76":"code","e62250c3":"code","7f23d91e":"code","7928507c":"code","c7dd9ef5":"code","58dc1181":"code","d163a511":"code","fcb6fc27":"code","61f521ad":"code","04dbae32":"code","165301cb":"code","cdc0da4e":"code","25f30817":"code","02598814":"code","4a5bdddb":"code","7b9dd529":"code","d9366ca5":"code","d9d84b1e":"code","d4062eba":"code","deff9be8":"code","c080c8c4":"code","9584d1b6":"code","4b611189":"code","292eec78":"code","b3501dd4":"code","aa9078fa":"code","c4fb1bc4":"code","dc36f726":"code","738ba713":"code","06b1dca1":"markdown","80dd052d":"markdown","16770686":"markdown","c83a957d":"markdown","0a57665a":"markdown","13709dcd":"markdown","223895fd":"markdown","e200fcfd":"markdown","e57e96a3":"markdown","4e0d80c3":"markdown","5b58611d":"markdown","95f77fb0":"markdown","c3bcb773":"markdown","1e6a6c0a":"markdown","2c36a1b8":"markdown","20a4b028":"markdown","4d41dfc9":"markdown","2181b469":"markdown","9685eab3":"markdown","5166fc32":"markdown","f43042ff":"markdown","9d330c83":"markdown","2f7a8917":"markdown","c2fe2ed3":"markdown","d8cefe2a":"markdown","d30375ae":"markdown","7b6bf8c2":"markdown","3b8d57eb":"markdown","e4884067":"markdown","4659e555":"markdown","14eb4ac7":"markdown","5d4b0b34":"markdown","e6314d86":"markdown","35c6fba6":"markdown","87b835bf":"markdown","a8b7cb87":"markdown","29ceecc1":"markdown","d76886de":"markdown"},"source":{"3653ba7a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8b246cf3":"path=\"..\/input\/sms-spam-collection-dataset\/spam.csv\"\ndata=pd.read_csv(path,encoding='latin-1')\ndata.head()","b281b819":"data.info()","4307b6e4":"data.describe()","a9117d0a":"df=data[[\"v1\",'v2']].copy()\ndf.rename(columns={'v1':'Class','v2':'sms'},inplace=True)\ndf.head()","08ffd802":"sns.countplot(x=df.Class)","4000c55b":"import spacy\nnlp=spacy.load('en')","3ae7e679":"df[\"tokens\"]=df.sms.apply(lambda x: nlp(x))\ndf.head()","61fd4eab":"def stopword(txt):\n    l=[]\n    for tokens in txt:\n        if not tokens.is_stop and not tokens.is_punct:\n            l.append(tokens.lemma_.strip().lower())\n    return l        \ndf['Lmnt_text']=df.tokens.apply(stopword) \ndf.head()","aac1d681":"def final_corpus(lmt):\n    return (' '.join(lmt))\ndf['final_corpus']=df.Lmnt_text.apply(final_corpus)\ndf.head()","d4a9090f":"text = \" \".join(r for r,s in zip(df.final_corpus.astype(str),df.Class) if s == 'ham')\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width = 1200, height = 1200,\n                background_color ='white',contour_width=1, contour_color='green',\n                min_font_size = 20).generate(text)\nplt.figure(figsize=[15,20])\nplt.title(\"HAM WORD CLOUD\")\nplt.axis(\"off\")\nplt.imshow(wordcloud,interpolation='bilinear')","349bd4b9":"from collections import Counter\nf=Counter(text.split())\nprint(f.most_common(10))","0695ac96":"text = \" \".join(r for r,s in zip(df.final_corpus.astype(str),df.Class) if s == 'spam')\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width = 1200, height = 1200,\n                background_color ='black',contour_width=1, contour_color='green',\n                min_font_size = 20).generate(text)\nplt.figure(figsize=[15,20])\nplt.title(\"SPAM WORD CLOUD\")\nplt.axis(\"off\")\nplt.imshow(wordcloud,interpolation='bilinear')","421a5379":"from collections import Counter\nf=Counter(text.split())\nprint(f.most_common(10))","0797e4c7":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(use_idf=False)\nX= tfidf.fit_transform(df.final_corpus).toarray()","6d0ce46a":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nx = cv.fit_transform(df.final_corpus).toarray()","643b72db":"from sklearn.model_selection import train_test_split\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, df.Class, test_size=0.30, random_state=42)\nX_train2, X_test2, y_train2, y_test2 = train_test_split(x, df.Class, test_size=0.30, random_state=42)","a9f58d79":"Model=pd.DataFrame({\"Model\":[],\"Accuracy\":[],\"Vectorizer\":[]})\nModel","dee88714":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression().fit(X_train1,y_train1)\nscore=lr.score(X_test1,y_test1)\nModel=Model.append({\"Model\":\"Logistic Regression\",\"Accuracy\":score,\"Vectorizer\":\"tfidf\"},ignore_index=True)\nlr=LogisticRegression().fit(X_train2,y_train2)\nscore=lr.score(X_test2,y_test2)\nModel=Model.append({\"Model\":\"Logistic Regression\",\"Accuracy\":score,\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel ","f33b74db":"from sklearn import tree\ndt= tree.DecisionTreeClassifier().fit(X_train1, y_train1)\nscore=dt.score(X_test1,y_test1)\nModel=Model.append({\"Model\":\"Decision Tree\",\"Accuracy\":score,\"Vectorizer\":\"tfidf\"},ignore_index=True)\ndt=tree.DecisionTreeClassifier().fit(X_train2,y_train2)\nscore=dt.score(X_test2,y_test2)\nModel=Model.append({\"Model\":\"Decision Tree\",\"Accuracy\":score,\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel ","8226e3fa":"from sklearn.ensemble import RandomForestClassifier\nrf= RandomForestClassifier(random_state=42).fit(X_train1, y_train1)\nscore=rf.score(X_test1,y_test1)\nModel=Model.append({\"Model\":\"Random Forest\",\"Accuracy\":score,\"Vectorizer\":\"tfidf\"},ignore_index=True)\nrf=RandomForestClassifier(random_state=42).fit(X_train2,y_train2)\nscore=rf.score(X_test2,y_test2)\nModel=Model.append({\"Model\":\"Random Forest\",\"Accuracy\":score,\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel ","7623ea64":"from sklearn.neighbors import KNeighborsClassifier\nfor r in range(1,20,2):\n    knn = KNeighborsClassifier(n_neighbors=r).fit(X_train1, y_train1)\n    print(r,knn.score(X_test1,y_test1))","5fd8f008":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3).fit(X_train1, y_train1)\nscore=knn.score(X_test1,y_test1)\nModel=Model.append({\"Model\":\"KNN\",\"Accuracy\":score,\"Vectorizer\":\"tfidf\"},ignore_index=True)\nknn = KNeighborsClassifier(n_neighbors=3).fit(X_train2,y_train2)\nscore=knn.score(X_test2,y_test2)\nModel=Model.append({\"Model\":\"KNN\",\"Accuracy\":score,\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel \n","b1d32ea3":"from sklearn.naive_bayes import MultinomialNB,GaussianNB,ComplementNB,BernoulliNB\nmnb = MultinomialNB().fit(X_train1, y_train1).score(X_test1,y_test1)\ngnb = GaussianNB().fit(X_train1,y_train1).score(X_test1,y_test1)\ncnb = ComplementNB().fit(X_train1,y_train1).score(X_test1,y_test1)\nbnb = BernoulliNB().fit(X_train1,y_train1).score(X_test1,y_test1)","6fedee76":"naive_bayes=pd.DataFrame({\"Classifier\":[\"GaussianNB\",\"MultinomialNB\",\"ComplementNB\",\"BernoulliNB\"],\n                          \"Score\":[gnb,mnb,cnb,bnb]})\nModel=Model.append({\"Model\":\"Naive Bayes\",\"Accuracy\":mnb,\"Vectorizer\":\"tfidf\"},ignore_index=True)\nnaive_bayes","e62250c3":"from sklearn.naive_bayes import MultinomialNB,GaussianNB,ComplementNB,BernoulliNB\nmnb = MultinomialNB().fit(X_train2, y_train2).score(X_test2,y_test2)\ngnb = GaussianNB().fit(X_train2,y_train2).score(X_test2,y_test2)\ncnb = ComplementNB().fit(X_train2,y_train2).score(X_test2,y_test2)\nbnb = BernoulliNB().fit(X_train2,y_train2).score(X_test2,y_test2)","7f23d91e":"naive_bayes1=pd.DataFrame({\"Classifier\":[\"GaussianNB\",\"MultinomialNB\",\"ComplementNB\",\"BernoulliNB\"],\n                          \"Score\":[gnb,mnb,cnb,bnb]})\nnaive_bayes1","7928507c":"Model=Model.append({\"Model\":\"Naive Bayes\",\"Accuracy\":mnb,\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel","c7dd9ef5":"from sklearn import svm\nfor r in ['linear' , 'poly', 'rbf', 'sigmoid']:\n    sv=svm.SVC(kernel=r).fit(X_train1,y_train1).score(X_test1,y_test1)\n    print(r,sv)","58dc1181":"from sklearn import svm\nfor r in ['linear' , 'poly', 'rbf', 'sigmoid']:\n    sv=svm.SVC(kernel=r).fit(X_train2,y_train2).score(X_test2,y_test2)\n    print(r,sv)","d163a511":"from sklearn import svm\nsv=svm.SVC(kernel='linear').fit(X_train1,y_train1)\nscore=sv.score(X_test1,y_test1)\nModel=Model.append({\"Model\":\"SVM\",\"Accuracy\":score,\"Vectorizer\":\"tfidf\"},ignore_index=True)\nsv = svm.SVC(kernel=\"linear\").fit(X_train2,y_train2)\nscore=sv.score(X_test2,y_test2)\nModel=Model.append({\"Model\":\"SVM\",\"Accuracy\":score,\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel ","fcb6fc27":"from sklearn.linear_model import SGDClassifier\nfor r in [ 'hinge', 'log', 'modified_huber']:\n    sgd=SGDClassifier(loss=r).fit(X_train1,y_train1).score(X_test1,y_test1)\n    print(r,sgd)","61f521ad":"from sklearn.linear_model import SGDClassifier\nfor r in [ 'hinge', 'log', 'modified_huber']:\n    sgd=SGDClassifier(loss=r).fit(X_train2,y_train2).score(X_test2,y_test2)\n    print(r,sgd)","04dbae32":"from sklearn.linear_model import SGDClassifier\nsgd=SGDClassifier(loss='hinge').fit(X_train1,y_train1)\nscore=sgd.score(X_test1,y_test1)\nModel=Model.append({\"Model\":\"Stochastic Gradient Descent\",\"Accuracy\":score,\"Vectorizer\":\"tfidf\"},ignore_index=True)\nsgd = SGDClassifier(loss=\"hinge\").fit(X_train2,y_train2)\nscore=sgd.score(X_test2,y_test2)\nModel=Model.append({\"Model\":\"Stochastic Gradient Descent\",\"Accuracy\":score,\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel \n","165301cb":"import xgboost as xgb\nxg=xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42,use_label_encoder=True).fit(X_train1, y_train1)\nscore=xg.score(X_test1,y_test1) \nModel=Model.append({\"Model\":\"XGBoost\",\"Accuracy\":score,\"Vectorizer\":\"tfidf\"},ignore_index=True)\nxg=xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42,use_label_encoder=True).fit(X_train2, y_train2)\nscore=xg.score(X_test2,y_test2)\nModel=Model.append({\"Model\":\"XGBoost\",\"Accuracy\":score,\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel \n","cdc0da4e":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel=keras.Sequential([\n            layers.Dense(units=6, activation='relu'), \n            layers.Dense(units=6, activation='relu'),  \n            layers.Dense(units=1, activation='sigmoid')])  ","25f30817":"model.compile(\noptimizer='adamax',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy']\n)","02598814":"stop=keras.callbacks.EarlyStopping(\npatience=10,\nmin_delta=0.01,\nrestore_best_weights=True)\n\nhistory=model.fit(\n    X_train2,y_train2.replace({\"ham\":0,\"spam\":1}),\n    validation_data=(X_test2,y_test2.replace({\"ham\":0,\"spam\":1})),\n    batch_size=32,\n    epochs=100,\n    callbacks=[stop])","4a5bdddb":"model.summary()","7b9dd529":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","d9366ca5":"Model=Model.append({\"Model\":\"Artificial Neural Network\",\"Accuracy\":history_df['val_binary_accuracy'].max(),\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel","d9d84b1e":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel=keras.Sequential([\n            layers.Dense(units=6, activation='relu'), \n            layers.Dense(units=6, activation='relu'),  \n            layers.Dense(units=1, activation='sigmoid')])\nmodel.compile(\noptimizer='adamax',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'])\nstop=keras.callbacks.EarlyStopping(\npatience=10,\nmin_delta=0.01,\nrestore_best_weights=True)\n\nhistory=model.fit(\n    X_train1,y_train1.replace({\"ham\":0,\"spam\":1}),\n    validation_data=(X_test1,y_test1.replace({\"ham\":0,\"spam\":1})),\n    batch_size=32,\n    epochs=100,\n    callbacks=[stop])","d4062eba":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","deff9be8":"Model=Model.append({\"Model\":\"Artificial Neural Network\",\"Accuracy\":history_df['val_binary_accuracy'].max(),\"Vectorizer\":\"tfidf\"},ignore_index=True)\nModel","c080c8c4":"Model.sort_values(by=['Accuracy'],ascending=False)","9584d1b6":"Model.groupby([\"Vectorizer\",'Model']).min()","4b611189":"from sklearn.linear_model import SGDClassifier\nfinal_model=SGDClassifier(loss='hinge')\nfinal_model.fit(X_train2,y_train2)\nfinal_model.score(X_test2,y_test2)","292eec78":"from sklearn.metrics import classification_report\nprint(classification_report(y_test2, final_model.predict(X_test2)))","b3501dd4":"from sklearn.metrics import plot_confusion_matrix\ndisp = plot_confusion_matrix(final_model, X_test2, y_test2,\n                                 display_labels=['ham','spam'],\n                                 cmap=plt.cm.Blues)\ndisp.ax_.set_title('SGDClassifier')\nprint(disp.confusion_matrix)\nplt.show()","aa9078fa":"import pickle\nfilename = 'finalized_model.sav'\npickle.dump(final_model, open(filename, 'wb'))","c4fb1bc4":"load_model = pickle.load(open(filename, 'rb'))","dc36f726":"import spacy\nnlp=spacy.load('en')\ndef prediction(text):\n    doc=nlp(text)\n    lt=[]\n    for tokens in doc:\n        if not tokens.is_stop and not tokens.is_punct:\n            lt.append(tokens.lemma_.strip().lower())\n    corpus= ' '.join(lt) \n    f_vct = cv.transform([corpus]).toarray()\n    pred=final_model.predict(f_vct)[0]\n    return pred\n    ","738ba713":"text=\"Free tones Hope you enjoyed your new content\"\noutput=prediction(text)\nprint(f\"The sms is {output}\")","06b1dca1":"# Data Visualization","80dd052d":"# Word Cloud","16770686":"# Save the trained model for future use","c83a957d":"# Load the Model","0a57665a":"# Prediction","13709dcd":"# Artificial neural network ","223895fd":"# Text preprocessing","e200fcfd":"# Read the Dataset","e57e96a3":"## For CountVector","4e0d80c3":"# Stochastic Gradient Descent","5b58611d":"# Support Vector Machine","95f77fb0":"# K-nearest Neighbours","c3bcb773":"# Vectorization","1e6a6c0a":"# Confusion Matrix","2c36a1b8":"# CountVectorizer","20a4b028":"# Random forest","4d41dfc9":"# Import Libraries","2181b469":"# TF-IDF","9685eab3":"## Multinomial Naive Bayes yields better result","5166fc32":"### The dataset is imbalanced ","f43042ff":"# Frequently occuring words in Ham messages","9d330c83":"# Decision Tree","2f7a8917":"# Information about data","c2fe2ed3":"# Model Building","d8cefe2a":"# Lammatization after removing stopwords and punctuation","d30375ae":"# Selection of State of The Art Model","7b6bf8c2":"# Logistic Regression","3b8d57eb":"# Building State Of The Art Model","e4884067":"# Building the pipeline for the prediction","4659e555":"# XGBoost ","14eb4ac7":"# Frequently occuring word in spam message","5d4b0b34":"# Naive Bayes","e6314d86":"# Train-Test Split","35c6fba6":"# Corpus of Lammatized text","87b835bf":"## For tfidf","a8b7cb87":"# Stochastic Gradient Descent algorithm with count vectorization of corpus yields maximum of accuracy","29ceecc1":"## Tokenization","d76886de":"# Data Preprocessing"}}