{"cell_type":{"9a524506":"code","656c5dbf":"code","386bbe5c":"code","5b113af0":"code","f20e5136":"code","e8690612":"code","e23cefec":"code","f672c174":"code","863159fb":"code","9ed2aced":"code","dc2b0c82":"code","9d5d2db4":"code","5c55f3f8":"code","782214f4":"code","ff765c55":"code","9065ce6f":"code","c9e07201":"markdown","4998ff5b":"markdown","e7a88317":"markdown","54a08565":"markdown","5658215c":"markdown","52b08929":"markdown","4992db1b":"markdown","3593218d":"markdown","329500c9":"markdown","6a616491":"markdown","5f69a949":"markdown","a2723a0c":"markdown","081c9d42":"markdown","b6db0d06":"markdown","c2070b3a":"markdown","c486ba4a":"markdown"},"source":{"9a524506":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected = True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npd.set_option('max_columns', 1000)\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nimport lightgbm as lgb\nimport plotly.figure_factory as ff\nimport gc\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder","656c5dbf":"%%time\nprint('Reading train.csv file....')\ntrain = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train.csv')\nprint('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\nprint('Reading test.csv file....')\ntest = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/test.csv')\nprint('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\nprint('Reading train_labels.csv file....')\ntrain_labels = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv')\nprint('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\nprint('Reading specs.csv file....')\nspecs = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/specs.csv')\nprint('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\nprint('Reading sample_submission.csv file....')\nsample_submission = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/sample_submission.csv')\nprint('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))","386bbe5c":"# Converts timestamp feat into datatime type feature and creates new feats: data|month|hour|daysofweek\ndef get_time(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['date'] = df['timestamp'].dt.date\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    return df\n    \ntrain = get_time(train)\ntest = get_time(test)","5b113af0":"# Agreegates event_id against transaction_id segregated on different values from the input columns (argument)\n# @param df The source Dataframe\n# @param columns Column name (string) from the dataframe which will be used along wiht installation_id to summarize\n# @return A dataframe with total count of event_id against each unique combination of transaction_id and column value\ndef get_object_columns(df, columns):\n    df = df.groupby(['installation_id', columns])['event_id'].count().reset_index()\n    df = df.pivot_table(index = 'installation_id', columns = [columns], values = 'event_id')\n    df.columns = list(df.columns)\n    df.fillna(0, inplace = True)\n    return df\n\n# Printing out sample output from the method\nprint(\"Sample output of get_object_columns() method: \")\nget_object_columns(train, 'type').head()","f20e5136":"# Agreegates input columns values: mean, sum and standard deviation against each installation_id\n# @param df The source Dataframe\n# @param columns Column name (string) from the dataframe which will summarized against installation_id\n# @return A dataframe with the agreegated values of input columns (argument) against each installation_id\ndef get_numeric_columns(df, column):\n    df = df.groupby('installation_id').agg({f'{column}': ['mean', 'sum', 'std']})\n    df.fillna(0, inplace = True)\n    df.columns = [f'{column}_mean', f'{column}_sum', f'{column}_std']\n    return df\n\n# Printing out sample output from the method\nprint(\"Sample output of get_numeric_columns() method: \")\nget_numeric_columns(train, 'game_time').head()","e8690612":"# Agreegates input columns values: mean, sum and standard deviation against each activity type by each installation_id \n# @param df The source Dataframe\n# @param columns Column name (string) from the dataframe which will summarized against installation_id\n# @return A dataframe with the agreegated values of input column (argument) against each combinatin of installation_id and agg_column\ndef get_numeric_columns_2(df, agg_column, column):\n    df = df.groupby(['installation_id', agg_column]).agg({f'{column}': ['mean', 'sum', 'std']}).reset_index()\n    df = df.pivot_table(index = 'installation_id', columns = [agg_column], values = [col for col in df.columns if col not in ['installation_id', 'type']])\n    df.fillna(0, inplace = True)\n    df.columns = list(df.columns)\n    return df\n\n\n# Printing out sample output from the method\nprint(\"Sample output of get_numeric_columns_2() method: \")\nget_numeric_columns_2(train,'type', 'game_time').head()","e23cefec":"reduce_train = pd.DataFrame({'installation_id': train['installation_id'].unique()})\nreduce_train.set_index('installation_id', inplace = True)\nreduce_test = pd.DataFrame({'installation_id': test['installation_id'].unique()})\nreduce_test.set_index('installation_id', inplace = True)","f672c174":"numerical_columns = ['game_time']\n\n# Applies get_numeric_columns() mehtod on train and test dataset to add aggregated features of game_type to the respective dataframe\nfor i in numerical_columns:  \n    # Appending columns with the agreegated values of input columns (game_type) with training datasets\n    df = get_numeric_columns(train, i) \n    reduce_train = reduce_train.merge(df, left_index = True, right_index = True)\n    # Appending columns with the agreegated values of input columns (game_type) with testing datasets\n    df = get_numeric_columns(test, i)\n    reduce_test = reduce_test.merge(df, left_index = True, right_index = True)\n","863159fb":"categorical_columns = ['type', 'world']\n\n# Applies categorical_columns() mehtod on train and test dataset to add aggregated features of type and world to the respective dataframe\nfor i in categorical_columns:\n    # Appending columns with the agreegated values of input columns (type and world) with training datasets\n    df = get_object_columns(train, i)\n    reduce_train = reduce_train.merge(df, left_index = True, right_index = True)\n    # Appending columns with the agreegated values of input columns (type and world) with testing datasets\n    df = get_object_columns(test, i)\n    reduce_test = reduce_test.merge(df, left_index = True, right_index = True)","9ed2aced":"# Applies categorical_columns_2() mehtod  to append agreegates input columns values: mean, sum and standard deviation against each activity type by each installation_id  to training and testing datasets\nfor i in categorical_columns:\n    for j in numerical_columns:\n        # Appending columns with training datasets\n        df = get_numeric_columns_2(train, i, j)\n        reduce_train = reduce_train.merge(df, left_index = True, right_index = True)\n        # Appending columns with testing datasets\n        df = get_numeric_columns_2(test, i, j)\n        reduce_test = reduce_test.merge(df, left_index = True, right_index = True)","dc2b0c82":"# Printing out shape of the processed datasets\nreduce_train.reset_index(inplace = True)\nreduce_test.reset_index(inplace = True)\n    \nprint('Our training set have {} rows and {} columns'.format(reduce_train.shape[0], reduce_train.shape[1]))\nprint('Column names of the training set are: ', list(reduce_train.columns))","9d5d2db4":"    \n# get the mode of the title\nlabels_map = dict(train_labels.groupby('title')['accuracy_group'].agg(lambda x:x.value_counts().index[0]))\n# merge target\nlabels = train_labels[['installation_id', 'title', 'accuracy_group']]\n# replace title with the mode\nlabels['title'] = labels['title'].map(labels_map)\n# get title from the test set\nreduce_test['title'] = test.groupby('installation_id').last()['title'].map(labels_map).reset_index(drop = True)\n# join train with labels\nreduce_train = labels.merge(reduce_train, on = 'installation_id', how = 'left')\nprint('We have {} training rows'.format(reduce_train.shape[0]))","5c55f3f8":"categoricals = ['title']\nreduce_train = reduce_train[['installation_id', 'game_time_mean', 'game_time_sum', 'game_time_std', 'Activity', 'Assessment', \n                             'Clip', 'Game', 'CRYSTALCAVES', 'MAGMAPEAK', 'NONE', 'TREETOPCITY', ('game_time', 'mean', 'Activity'),\n                             ('game_time', 'mean', 'Assessment'), ('game_time', 'mean', 'Clip'), ('game_time', 'mean', 'Game'), \n                             ('game_time', 'std', 'Activity'), ('game_time', 'std', 'Assessment'), ('game_time', 'std', 'Clip'), \n                             ('game_time', 'std', 'Game'), ('game_time', 'sum', 'Activity'), ('game_time', 'sum', 'Assessment'), \n                             ('game_time', 'sum', 'Clip'), ('game_time', 'sum', 'Game'), ('game_time', 'mean', 'CRYSTALCAVES'), \n                             ('game_time', 'mean', 'MAGMAPEAK'), ('game_time', 'mean', 'NONE'), ('game_time', 'mean', 'TREETOPCITY'), \n                             ('game_time', 'std', 'CRYSTALCAVES'), ('game_time', 'std', 'MAGMAPEAK'), ('game_time', 'std', 'NONE'), \n                             ('game_time', 'std', 'TREETOPCITY'), ('game_time', 'sum', 'CRYSTALCAVES'), \n                             ('game_time', 'sum', 'MAGMAPEAK'), ('game_time', 'sum', 'NONE'), ('game_time', 'sum', 'TREETOPCITY'), \n                             'title', 'accuracy_group']]","782214f4":"# Fits light gradient boosting tree model on the dataset\n# @param reduce_train Training dataset\n# @param reduce_test Test dataset\n# @return Predicted probability \n\ndef run_lgb(reduce_train, reduce_test):\n    kf = KFold(n_splits=10)\n    # Taking all features except accuracy_group (target feat.) and installation_id (ID) from the reduce_train dataframe \n    features = [i for i in reduce_train.columns if i not in ['accuracy_group', 'installation_id']]\n    \n    target = 'accuracy_group'\n    # Creating colums for predicted accuracy groups (four labels)\n    oof_pred = np.zeros((len(reduce_train), 4))\n    y_pred = np.zeros((len(reduce_test), 4))\n    \n    # Applies 10 fold cross validation.\n    # Applies a for loop to repeat 10 times this process: step 01: split training set into training and validation set --> step0 02: fit lgb model -- > step 03: store prediction results\n    # @return Prediction on test data\n    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train)):\n        print('Fold {}'.format(fold + 1))\n        \n        # step 01\n        x_train, x_val = reduce_train[features].iloc[tr_ind], reduce_train[features].iloc[val_ind]\n        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n        \n        # step0 02\n        train_set = lgb.Dataset(x_train, y_train, categorical_feature=categoricals)\n        val_set = lgb.Dataset(x_val, y_val, categorical_feature=categoricals)\n\n        params = {\n            'learning_rate': 0.01,\n            'metric': 'multiclass',\n            'objective': 'multiclass',\n            'num_classes': 4,\n            'feature_fraction': 0.75,\n            'subsample': 0.75\n        }\n\n        model = lgb.train(params, train_set, num_boost_round = 100000, early_stopping_rounds = 100, \n                          valid_sets=[train_set, val_set], verbose_eval = 100)\n        \n        # step 03\n        oof_pred[val_ind] = model.predict(x_val)\n        y_pred += model.predict(reduce_test[features]) \/ 10\n    return y_pred\ny_pred = run_lgb(reduce_train, reduce_test)","ff765c55":"reduce_test = reduce_test.reset_index()\nreduce_test = reduce_test[['installation_id']]\nreduce_test['accuracy_group'] = y_pred.argmax(axis = 1)\nsample_submission.drop('accuracy_group', inplace = True, axis = 1)\nsample_submission = sample_submission.merge(reduce_test, on = 'installation_id')\nsample_submission.to_csv('submission.csv', index = False)","9065ce6f":"sample_submission['accuracy_group'].value_counts(normalize = True)","c9e07201":"**Changing the order of the columns in the training dataset**","4998ff5b":"**get_numeric_columns_2()**\n\nThis method gives us a more grannular view compared to the last two methods. \n* From get_object_columns() we get to see total number of activities performed by each user (transaction_id)\n* From get_numeric_columns() we get to see aggregated measures (mean, sum and std) of game_time spent by each user\n\nNow from get_numeric_columns_2() we'll get to see different game_time agreegated measures against each activity type per user. ","e7a88317":"# Preparing submission file and visualizing submission","54a08565":"**Creating new aggregated features for titles**\n\nIn this section title of the different games are numerically coded based on the most frequently present category of accuracy_group in respective title. And then the values are mapped on the existing values of titles for both training and testing dataset. \n","5658215c":"# Fitting model","52b08929":">> # Background\nThis notebook is an attempt to make codes from the notebook shared by [@ragnar](https:\/\/www.kaggle.com\/ragnar123) more palatable especially for the Python beginners. @ragnar's notebook is a great one especially the way it was desiged with all the utility methods! So I started looking into it and figured that adding some additional explanations on the methods will make it a lot easier to follow for anyone. Hence, I forked the notebook and started putting out comments and this is the output. \n\n>>Here's the original notebook: https:\/\/www.kaggle.com\/ragnar123\/simple-exploratory-data-analysis-and-model","4992db1b":"**<center> Thanks goes to [@ragnar](https:\/\/www.kaggle.com\/ragnar123) again for the notebook! <center\/>**","3593218d":"**Filtering out only the unique transaction_ids (users)**","329500c9":"**Applying mehtods**","6a616491":"**get_object_columns():** \n","5f69a949":"**get_numeric_columns()**","a2723a0c":"# Creating methods to aggregate features\n\nIn the dataset, there are multiple rows against each user (here 'installation_id'). So the idea in this notebook is to take agreegated measures against each unique installation_id and then using those agreegated features as the predictors in the model. \n\nTo stream line the process of feature aggregation, bunch of methods (functionally similar to functions in R) are created. In this section I'll make an effort to explain the utility functions before applying them and hopefully it'll make it easier for someone new follow the process more easily. ","081c9d42":"# Converting timestamp","b6db0d06":"# Reading Files","c2070b3a":"In @ragnar's notebook there are some primary data exploratoins steps are included before diving into the feature engineering and model building phase which I found really compact. So I'll skip the exploratoins and jump to the sections of feature engineering and model building. ","c486ba4a":"# Feature engineering\n\nIn this section, the methods created in previous section will be applied on both the training and testing datasets. "}}