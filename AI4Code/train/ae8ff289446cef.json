{"cell_type":{"eb35cadc":"code","827a37a6":"code","c8a1f1f7":"code","6b1eb85a":"code","527c041e":"code","d44d0b06":"code","53c1f743":"code","3ee7ff00":"code","3da413b0":"code","cd227699":"code","41254d26":"code","7320973d":"code","6edd0f96":"code","b1a60ecd":"code","92cfd0ce":"code","623475b2":"code","59204646":"code","977894a0":"code","679cb0a9":"code","ec4f8dae":"code","9c072a6d":"code","fb87398e":"code","dc977866":"code","2448fb54":"code","0fa6e5a4":"code","c52870bf":"code","04b8b625":"code","5944559d":"code","4db077b9":"code","b3d78420":"code","6779c7ea":"code","baf86f48":"code","d1d5fc93":"code","d800252b":"code","854e4e46":"code","664022ae":"code","14e64968":"code","e35d747e":"code","9bbc7656":"code","94f21bbe":"code","8d799f50":"code","329399ac":"code","4b5c2944":"code","95f84162":"code","f8055997":"code","ab9148b6":"code","07020c87":"code","62c68fd7":"code","6be45134":"code","1210ce63":"code","bfc907ef":"code","e030e446":"code","c4970420":"code","9dc2977c":"code","f8c36e56":"code","9eb0dc72":"code","e2e1fbc8":"code","a5ec0274":"code","926c701f":"code","f4488f63":"code","6ffa45e4":"code","bf2ec187":"code","580f7411":"code","6a1dd506":"code","18dae14e":"code","04ed9361":"code","477f22a1":"code","5b7ce508":"code","b10ea98b":"code","50572b61":"code","fc9640c7":"code","b9c1a62e":"markdown","7f38ba96":"markdown","24378b65":"markdown","469c77a8":"markdown","9a89590f":"markdown","25b18f25":"markdown","dbfc0269":"markdown","ea6cd443":"markdown","2d972a3e":"markdown","35d0ccd6":"markdown","90b7386f":"markdown","e4c5adba":"markdown","34ff7db5":"markdown","1bc2d3ab":"markdown","e0d385f4":"markdown","f81eaaf1":"markdown","3b7c3f7f":"markdown","c04826f6":"markdown","d05074e7":"markdown","bd036818":"markdown","97721f4f":"markdown","73d52cae":"markdown","158927e8":"markdown","928230d0":"markdown","9215532a":"markdown","fbed6db6":"markdown","f3b9a517":"markdown","f3526d66":"markdown","ac2754cc":"markdown","ac5aeb14":"markdown","0194a1ac":"markdown","e57030a4":"markdown","e3a473e8":"markdown","0ea9ede3":"markdown"},"source":{"eb35cadc":"import numpy as np\nimport pandas as pd \nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport torch\nimport time\nimport math\nfrom tqdm.notebook import tqdm\nimport glob","827a37a6":"IMG_SIZE = 224\nMARGIN = 10\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","c8a1f1f7":"# MTCNN\n!pip install ..\/input\/mtcnn-package\/mtcnn-0.1.0-py3-none-any.whl -q\nfrom mtcnn import MTCNN as mMTCNN\nmtcnn_detector = mMTCNN()","6b1eb85a":"# facenet_pytorch\nfrom facenet_pytorch import MTCNN as fMTCNN\nfacenet_detector = fMTCNN(image_size=150, margin=0, keep_all=True, factor=0.5, post_process=False, device=device).eval()","527c041e":"# dlib\n!pip install '\/kaggle\/input\/dlibpkg\/dlib-19.19.0'","d44d0b06":"import dlib\ndlib_hog_detector = dlib.get_frontal_face_detector()","53c1f743":"# insightface\n!pip install insightface -q\nimport insightface\ninsight_detector = insightface.app.FaceAnalysis()\ninsight_detector.prepare(ctx_id = -1, nms=0.4)","3ee7ff00":"# mobilenet\nimport tensorflow as tf\ndetection_graph = tf.Graph()\nwith detection_graph.as_default():\n    od_graph_def = tf.compat.v1.GraphDef()\n    with tf.io.gfile.GFile('..\/input\/mobilenet-face\/frozen_inference_graph_face.pb', 'rb') as fid:\n        serialized_graph = fid.read()\n        od_graph_def.ParseFromString(serialized_graph)\n        tf.import_graph_def(od_graph_def, name='')\n        config = tf.compat.v1.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess=tf.compat.v1.Session(graph=detection_graph, config=config)\n    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n    boxes_tensor = detection_graph.get_tensor_by_name('detection_boxes:0')    \n    scores_tensor = detection_graph.get_tensor_by_name('detection_scores:0')\n    num_detections = detection_graph.get_tensor_by_name('num_detections:0')","3da413b0":"def mobile_detector(image):\n    global boxes,scores,num_detections\n    (h, w)=image.shape[:-1]\n    imgs=np.array([image])\n    (boxes, scores) = sess.run(\n        [boxes_tensor, scores_tensor],\n        feed_dict={image_tensor: imgs})\n    max_=np.where(scores==scores.max())[0][0]\n    box=boxes[0][max_]\n    ymin, xmin, ymax, xmax = box\n    (left, right, top, bottom) = (xmin * w, xmax * w, \n                                  ymin * h, ymax * h)\n    left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n    return (left, right, top, bottom)","cd227699":"# blazeface\ndef prepare_blaze():\n    blazeface = BlazeFace()\n    blazeface.load_weights(\"\/kaggle\/input\/blazeface-pytorch\/blazeface.pth\")\n    blazeface.load_anchors(\"\/kaggle\/input\/blazeface-pytorch\/anchors.npy\")\n\n    # Optionally change the thresholds:\n    blazeface.min_score_thresh = 0.95\n    blazeface.min_suppression_threshold = 0.3\n    return blazeface\nblazeface = prepare_blaze()\n\ndef get_blaze_boxes(detections):\n    result = []\n    if isinstance(detections, torch.Tensor):\n        detections = detections.cpu().numpy()\n\n    if detections.ndim == 1:\n        detections = np.expand_dims(detections, axis=0)\n\n    # no detect\n    img_shape = (128, 128)\n    if len(detections) != 0:\n        for i in range(detections.shape[0]):\n            ymin = detections[i, 0] * img_shape[0]\n            xmin = detections[i, 1] * img_shape[1]\n            ymax = detections[i, 2] * img_shape[0]\n            xmax = detections[i, 3] * img_shape[1]\n            result.append((xmin, ymin, xmax, ymax))\n    return result\n\n\ndef scale_boxes(boxes, scale_w, scale_h):\n    sb = []\n    for b in boxes:\n        sb.append((b[0] * scale_w, b[1] * scale_h, b[2] * scale_w, b[3] * scale_h))\n    return sb\n\n\ndef blaze_detector(img):\n    img = cv2.resize(img, (128, 128))\n    output = blazeface.predict_on_image(img)\n    bbox = scale_boxes(get_blaze_boxes(output), 380\/128, 380\/128)\n    return bbox\n    ","41254d26":"df = pd.read_csv('..\/input\/deepfake-detection-challenge\/sample_submission.csv')\nvideo_names = df['filename']\nvideo_paths = ['..\/input\/deepfake-detection-challenge\/test_videos\/' + n for n in video_names]\nvideo_paths[:10]","7320973d":"sample_video = video_paths[0]","6edd0f96":"%%time\nbefore = time.time()\n\ndef read_video1(video_path):\n    v_cap = cv2.VideoCapture(sample_video)\n    v_int = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    all_frames = []\n    for i in range(v_int):\n        ret, frame = v_cap.read()\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        all_frames.append(frame)\n    return np.array(all_frames)\n    \nresult = read_video1(sample_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(result.shape, round(after-before, 1)))","b1a60ecd":"%%time\nbefore = time.time()\n\ndef read_video2(video_path):\n    v_cap = cv2.VideoCapture(sample_video)\n    v_int = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    all_frames = []\n    for i in range(v_int):\n        v_cap.grab()\n        ret, frame = v_cap.retrieve()\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        all_frames.append(frame)\n    return np.array(all_frames)\n    \nresult = read_video2(sample_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(result.shape, round(after-before, 1)))","92cfd0ce":"!pip install ..\/input\/imageio-ffmpeg\/imageio_ffmpeg-0.3.0-py3-none-manylinux2010_x86_64.whl","623475b2":"import imageio","59204646":"%%time\nbefore = time.time()\n\ndef read_video3(video_path):\n    reader = imageio.get_reader(video_path, 'ffmpeg')\n    v_int = reader.count_frames()\n    meta = reader.get_meta_data()\n    all_frames = []\n    for i in range(v_int):\n        img = reader.get_data(i)\n        all_frames.append(img)\n    return np.array(all_frames)\n    \nresult = read_video3(sample_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(result.shape, round(after-before, 1)))","977894a0":"frames_per_video = 72","679cb0a9":"def read_frames1(frames_per_video=60):\n    v_cap = cv2.VideoCapture(sample_video)\n    v_int = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    all_frames = []\n    sample_idx = np.linspace(0, v_int-1, frames_per_video, endpoint=True, dtype=np.int)\n    for i in range(v_int):\n        # speed reading without decode unwanted frame\n        ret = v_cap.grab()\n        \n        if ret is None: \n            print('The {} cannot be read'.format(i))\n            continue\n            \n#         # the frame we want\n        if i in sample_idx:\n            ret, frame = v_cap.retrieve()\n            if ret is None or frame is None:\n                continue\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            all_frames.append(frame)\n            \n    return all_frames","ec4f8dae":"%%time\nbefore = time.time()\n\nresult = read_frames1(frames_per_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(np.array(result).shape, round(after-before, 1)))","9c072a6d":"def read_frames2(frames_per_video=60):\n    v_cap = cv2.VideoCapture(sample_video)\n    v_int = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    all_frames = []\n    sample_idx = np.linspace(0, v_int-1, frames_per_video, endpoint=True, dtype=np.int)\n    for i in range(v_int):\n        ret, frame = v_cap.read()\n        if ret is None or frame is None:\n            continue\n        \n        if i in sample_idx:\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            all_frames.append(frame)\n        \n    return all_frames","fb87398e":"%%time\nbefore = time.time()\n\nresult = read_frames2(frames_per_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(np.array(result).shape, round(after-before, 1)))","dc977866":"def read_frames3(frames_per_video):\n    reader = imageio.get_reader(sample_video, 'ffmpeg')\n    v_int = reader.count_frames()\n    all_frames = []\n    sample_idx = np.linspace(0, v_int-1, frames_per_video, endpoint=True, dtype=np.int)\n    for i in sample_idx:\n        img = reader.get_data(i)\n        all_frames.append(img)\n    return all_frames","2448fb54":"%%time\nbefore = time.time()\n\nresult = read_frames3(frames_per_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(np.array(result).shape, round(after-before, 1)))","0fa6e5a4":"def read_frames4(frames_per_video=72):\n    v_cap = cv2.VideoCapture(sample_video)\n    v_int = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    all_frames = []\n    sample_idx = np.linspace(0, v_int-1, frames_per_video, endpoint=True, dtype=np.int)\n    for i in sample_idx:\n        v_cap.set(cv2.CAP_PROP_POS_FRAMES, i) \n        ret, frame = v_cap.read()\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        all_frames.append(frame)\n    return all_frames","c52870bf":"%%time\nbefore = time.time()\n\nresult = read_frames4(frames_per_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(np.array(result).shape, round(after-before, 1)))","04b8b625":"# unify show faces\nshow_faces = 9\nframes = read_video2(sample_video)\ncell = round(math.sqrt(9))\nrandom_idx = np.random.choice(len(frames), show_faces)","5944559d":"from mtcnn import MTCNN\ndetector = MTCNN()","4db077b9":"def detect_face(img):\n    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    final = []\n    detected_faces_raw = detector.detect_faces(img)\n    if detected_faces_raw==[]:\n        #print('no faces found')\n        return []\n    confidences=[]\n    for n in detected_faces_raw:\n        x,y,w,h=n['box']\n        final.append([x,y,w,h])\n        confidences.append(n['confidence'])\n    if max(confidences)<0.9:\n        return []\n    max_conf_coord=final[confidences.index(max(confidences))]\n    #return final\n    return max_conf_coord\n\ndef crop(img,x,y,w,h, margin, img_shape):\n    x-=margin\n    y-=margin\n    w+=margin*2\n    h+=margin*2\n    if x<0:\n        x=0\n    if y<=0:\n        y=0\n    return cv2.cvtColor(cv2.resize(img[y:y+h,x:x+w],img_shape),cv2.COLOR_BGR2RGB)","b3d78420":"def detect_faces_mtcnn(video_path):\n    total_faces = []\n    \n    # full frames\n    frames = read_video2(video_path)\n    \n    for frame in tqdm(frames):\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        bounding_box = detect_face(frame)\n        if bounding_box == []:\n            continue\n        else:\n            x,y,w,h = bounding_box\n            face = crop(frame, x, y, w, h, MARGIN, (IMG_SIZE, IMG_SIZE))\n            total_faces.append(face)\n            \n    return np.array(total_faces)\n","6779c7ea":"%%time\nbefore = time.time()\n\nfaces = detect_faces_mtcnn(sample_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(faces.shape, round(after-before, 1)))","baf86f48":"fig, ax = plt.subplots(cell, cell, figsize=(8, 8))\nfor i in range(cell):\n    for j in range(cell):\n        ax[i][j].imshow(faces[random_idx[i*cell+j]])","d1d5fc93":"!pip install ..\/input\/facenet-pytorch-vggface2\/facenet_pytorch-2.0.1-py3-none-any.whl","d800252b":"from facenet_pytorch import MTCNN\nmtcnn = MTCNN(image_size=IMG_SIZE, margin=MARGIN)","854e4e46":"def detect_faces_facenet(video_path):\n    total_faces = []\n    \n    frames = read_video2(video_path)\n    \n    for frame in tqdm(frames):\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        faces = net_facenet(Image.fromarray(frame))\n        if faces is None:\n            continue\n        else:\n            total_faces.append(faces.numpy()[0])   \n        \n    return np.array(total_faces)","664022ae":"%%time\nbefore = time.time()\n\nfaces = detect_faces_facenet(sample_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(faces.shape, round(after-before, 1)))","14e64968":"fig, ax = plt.subplots(cell, cell, figsize=(8, 8))\nfor i in range(cell):\n    for j in range(cell):\n        ax[i][j].imshow(faces[random_idx[i*cell+j]])","e35d747e":"import sys\nsys.path.insert(0, \"\/kaggle\/input\/blazeface-pytorch\")","9bbc7656":"from blazeface import BlazeFace\n\nnet = BlazeFace()\nnet.load_weights(\"\/kaggle\/input\/blazeface-pytorch\/blazeface.pth\")\nnet.load_anchors(\"\/kaggle\/input\/blazeface-pytorch\/anchors.npy\")\n\n# Optionally change the thresholds:\nnet.min_score_thresh = 0.75\nnet.min_suppression_threshold = 0.3","94f21bbe":"def plot_detections(img, detections, with_keypoints=True):\n    fig, ax = plt.subplots(1, figsize=(10, 10))\n    ax.grid(False)\n    ax.imshow(img)\n    \n    if isinstance(detections, torch.Tensor):\n        detections = detections.cpu().numpy()\n\n    if detections.ndim == 1:\n        detections = np.expand_dims(detections, axis=0)\n\n    print(\"Found %d faces\" % detections.shape[0])\n        \n    for i in range(detections.shape[0]):\n        ymin = detections[i, 0] * img.shape[0]\n        xmin = detections[i, 1] * img.shape[1]\n        ymax = detections[i, 2] * img.shape[0]\n        xmax = detections[i, 3] * img.shape[1]\n\n        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                 linewidth=1, edgecolor=\"r\", facecolor=\"none\", \n                                 alpha=detections[i, 16])\n        ax.add_patch(rect)\n\n        if with_keypoints:\n            for k in range(6):\n                kp_x = detections[i, 4 + k*2    ] * img.shape[1]\n                kp_y = detections[i, 4 + k*2 + 1] * img.shape[0]\n                circle = patches.Circle((kp_x, kp_y), radius=0.5, linewidth=1, \n                                        edgecolor=\"lightskyblue\", facecolor=\"none\", \n                                        alpha=detections[i, 16])\n                ax.add_patch(circle)\n        \n    plt.show()","8d799f50":"def tile_frames(frames, target_size):\n    num_frames, H, W, _ = frames.shape\n\n    split_size = min(H, W)\n    x_step = (W - split_size) \/\/ 2\n    y_step = (H - split_size) \/\/ 2\n    num_v = 1\n    num_h = 3 if W > H else 1\n\n    splits = np.zeros((num_frames * num_v * num_h, target_size[1], target_size[0], 3), dtype=np.uint8)\n\n    i = 0\n    for f in range(num_frames):\n        y = 0\n        for v in range(num_v):\n            x = 0\n            for h in range(num_h):\n                crop = frames[f, y:y+split_size, x:x+split_size, :]\n                splits[i] = cv2.resize(crop, target_size, interpolation=cv2.INTER_AREA)\n                x += x_step\n                i += 1\n            y += y_step\n\n    resize_info = [split_size \/ target_size[0], split_size \/ target_size[1]]\n    return splits, resize_info","329399ac":"def detect_faces_full_frame(video_path):\n    total_detections = []\n    frames = read_video2(video_path)\n    for frame in tqdm(frames):\n        frame = cv2.resize(frame, net.input_size)\n        detections = net.predict_on_image(frame)\n        \n        if len(detections) != 0:\n            total_detections.append(detections)\n    \n    return total_detections","4b5c2944":"%%time\nbefore = time.time()\n\ndetections = detect_faces_full_frame(sample_video)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(len(detections), round(after-before, 1)))","95f84162":"for frame in frames:\n    frame = cv2.resize(frame, net.input_size)\n\n    detections = net.predict_on_image(frame)\n    \n    # cannot detect\n    if len(detections) == 0:\n        continue\n    else:\n        print('Frame idx {}'.format(i))\n        plot_detections(frame, detections)\n        break","f8055997":"# show the tile effect\nfirst_frames, first_resize_info = tile_frames(frames[:1], (128, 128))\n\nfig = plt.figure(figsize=(14, 14))\ncolumns = 3\nrows = 1\nfor i in range(1, columns*rows +1):\n    img = first_frames[i-1]\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img)\nplt.show()","ab9148b6":"def resize_detections(detections, target_size, resize_info):\n    '''get detections from [0, 1] to original frame size (1080, 1080)'''\n    \n    projected = []\n    target_w, target_h = target_size  # (128, 128)\n    scale_w, scale_h = resize_info #  (1080\/128, 1080\/128)\n\n    # each frame\n    for i in range(len(detections)):\n        detection = detections[i].clone()\n\n        # ymin, xmin, ymax, xmax\n        for k in range(2): #0, 1\n            detection[:, k*2    ] = (detection[:, k*2    ] * target_h) * scale_h\n            detection[:, k*2 + 1] = (detection[:, k*2 + 1] * target_w) * scale_w\n         \n        # keypoints are x,y\n        for k in range(2, 8):\n            detection[:, k*2    ] = (detection[:, k*2    ] * target_w) * scale_w\n            detection[:, k*2 + 1] = (detection[:, k*2 + 1] * target_h) * scale_h\n\n        projected.append(detection)\n\n    return projected ","07020c87":"# show resize_detection effects\n\n# update\ntarget_size = (128, 128)\nresize_info = (1080\/128, 1080\/128)\ntarget_w, target_h = target_size\nscale_w, scale_h = resize_info \n\nfirst_originals, _ = tile_frames(frames[:1], (1080, 1080))\n\nfirst_detections = net.predict_on_batch(first_frames, apply_nms=False)\nfirst_detections = resize_detections(first_detections, target_size, resize_info)\n    \n    \nfig = plt.figure(figsize=(14, 14))\nfor i, detection in enumerate(first_detections):\n    \n    # show the img\n    ax = fig.add_subplot(1, 3, i+1, aspect='equal')\n    ax.imshow(first_originals[i])\n    \n\n    # each face\n    for detect in detection:\n#         ymin = (detect[0] * target_h) * scale_h\n#         xmin = (detect[1] * target_w) * scale_w\n#         ymax = (detect[2] * target_h) * scale_h\n#         xmax = (detect[3] * target_w) * scale_w\n        \n        ymin = detect[0]\n        xmin = detect[1]\n        ymax = detect[2] \n        xmax = detect[3] \n\n        ax.add_patch( patches.Rectangle( (xmin, ymin), xmax-xmin, ymax-ymin, fill=False, edgecolor=\"r\", alpha=detect[16])) \n        \nplt.show()","62c68fd7":"del first_originals\ndel fig","6be45134":"def untile_detections(num_frames, frame_size, detections):\n    \"\"\"With N tiles per frame, there also are N times as many detections.\n    This function groups together the detections for a given frame; it is\n    the complement to tile_frames().\n    detections: (192, ?, 17)\n    \"\"\"\n    combined_detections = []\n\n    W, H = frame_size\n    split_size = min(H, W)\n    x_step = (W - split_size) \/\/ 2\n    y_step = (H - split_size) \/\/ 2\n    num_v = 1\n    num_h = 3 if W > H else 1\n\n    i = 0\n    for f in range(num_frames):\n        detections_for_frame = []\n        y = 0\n        for v in range(num_v):\n            x = 0\n            for h in range(num_h):\n                # Adjust the coordinates based on the split positions.\n                detection = detections[i].clone()\n                if detection.shape[0] > 0:\n                    for k in range(2):\n                        detection[:, k*2    ] += y\n                        detection[:, k*2 + 1] += x\n                    for k in range(2, 8):\n                        detection[:, k*2    ] += x\n                        detection[:, k*2 + 1] += y\n\n                detections_for_frame.append(detection)\n                x += x_step\n                i += 1\n            y += y_step\n\n        combined_detections.append(torch.cat(detections_for_frame))\n\n    return combined_detections","1210ce63":"# show untile detections effect\nframe_size = (frames.shape[2], frames.shape[1])\ndetections_one = untile_detections(1, frame_size, first_detections)\ndetections_one = net.nms(detections_one)[0] # only one frame\n\nfig = plt.figure(figsize=(14, 14))\nax = fig.add_subplot(1, 1, 1, aspect='equal')\nax.imshow(frames[0])\n\n# how many faces\nfor i in range(len(detections_one)):\n    ymin = detections_one[i][0]\n    xmin = detections_one[i][1]\n    ymax = detections_one[i][2] \n    xmax = detections_one[i][3] \n\n    ax.add_patch( patches.Rectangle( (xmin, ymin), xmax-xmin, ymax-ymin, fill=False, edgecolor=\"r\", alpha=detect[16])) \n        \nplt.show()","bfc907ef":"def add_margin_to_detections(detections, frame_size, margin=0.2):\n    \"\"\"Expands the face bounding box.\n\n    NOTE: The face detections often do not include the forehead, which\n    is why we use twice the margin for ymin.\n\n    Arguments:\n        detections: a PyTorch tensor of shape (num_detections, 17)\n        frame_size: maximum (width, height)\n        margin: a percentage of the bounding box's height\n\n    Returns a PyTorch tensor of shape (num_detections, 17).\n    \"\"\"\n    offset = torch.round(margin * (detections[:, 2] - detections[:, 0])) # (ymax - ymin)*margin\n    detections = detections.clone()\n    detections[:, 0] = torch.clamp(detections[:, 0] - offset*2, min=0)            # ymin\n    detections[:, 1] = torch.clamp(detections[:, 1] - offset, min=0)              # xmin\n    detections[:, 2] = torch.clamp(detections[:, 2] + offset, max=frame_size[1])  # ymax\n    detections[:, 3] = torch.clamp(detections[:, 3] + offset, max=frame_size[0])  # xmax\n    return detections","e030e446":"# show add margin effect\n\ndetections_margin = add_margin_to_detections(detections_one, frame_size=frame_size)\n\n\nfig = plt.figure(figsize=(14, 14))\nax = fig.add_subplot(1, 1, 1, aspect='equal')\nax.imshow(frames[0])\n\n# how many faces\nfor i in range(len(detections_margin)):\n    ymin = detections_margin[i][0]\n    xmin = detections_margin[i][1]\n    ymax = detections_margin[i][2] \n    xmax = detections_margin[i][3] \n\n    ax.add_patch( patches.Rectangle( (xmin, ymin), xmax-xmin, ymax-ymin, fill=False, edgecolor=\"r\", alpha=detect[16])) \n        \nplt.show()","c4970420":"def crop_faces(frame, detections):\n    \"\"\"Copies the face region(s) from the given frame into a set\n    of new NumPy arrays.\n\n    Arguments:\n        frame: a NumPy array of shape (H, W, 3)\n        detections: a PyTorch tensor of shape (num_detections, 17)\n\n    Returns a list of NumPy arrays, one for each face crop. If there\n    are no faces detected for this frame, returns an empty list.\n    \"\"\"\n    faces = []\n    for i in range(len(detections)):\n        ymin, xmin, ymax, xmax = detections[i, :4].cpu().numpy().astype(np.int)\n        face = frame[ymin:ymax, xmin:xmax, :]\n        faces.append(face)\n    return faces","9dc2977c":"faces_blaze = crop_faces(frames[0], detections_margin)\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(1, 1, 1, aspect='equal')\nax.imshow(faces_blaze[0])\nprint(faces_blaze[0].shape)","f8c36e56":"def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n    h, w = img.shape[:2]\n    if w > h:\n        h = h * size \/\/ w\n        w = size\n    else:\n        w = w * size \/\/ h\n        h = size\n\n    resized = cv2.resize(img, (w, h), interpolation=resample)\n    return resized\n\n\ndef make_square_image(img):\n    h, w = img.shape[:2]\n    size = max(h, w)\n    t = 0\n    b = size - h\n    l = 0\n    r = size - w\n    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)","9eb0dc72":"face_resized = isotropically_resize_image(faces_blaze[0], 224)\nface_square = make_square_image(face_resized)\nplt.imshow(face_square)","e2e1fbc8":"class BlazeFaceExtractor:\n    ''' Convenient class'''\n    \n    \n    \n    def __init__(self, weights_path=None, anchors_path=None):\n        ''' load face extractor ''' \n        \n        from blazeface import BlazeFace\n\n        self.net = BlazeFace()\n        \n        \n        if weights_path is None:\n            self.net.load_weights(\"\/kaggle\/input\/blazeface-pytorch\/blazeface.pth\")\n        else:\n            self.net.load_weights(weights_path)\n            \n        if anchors_path is None:\n            self.net.load_anchors(\"\/kaggle\/input\/blazeface-pytorch\/anchors.npy\")\n        else:\n            self.net.load_anchors(anchors_path)\n            \n\n        # Optionally change the thresholds:\n        self.net.min_score_thresh = 0.75\n        self.net.min_suppression_threshold = 0.3\n\n        \n        \n        \n    def extract_faces(self, video_path, video_read_fn, output_size=224):\n        '''\n        Arguments\n            video_path: full path for one video\n        '''\n        self.video_path = video_path\n        self.video_read_fn = video_read_fn\n        self.output_size = 224\n        \n        # all or specific number of full frames for one video\n        frames = self.video_read_fn(self.video_path)\n        \n        # 1 - tile\n        tiles_frames, tiles_resize_info = tile_frames(frames, self.net.input_size)\n        \n        # 2 - predict\n        detections = net.predict_on_batch(tiles_frames, apply_nms=False)\n        \n        # 3 - resize detection\n        detections_resized = self.resize_detections(detections, self.net.input_size, tiles_resize_info)\n        \n        # 4 - untile\n        frame_size = (frames.shape[2], frames.shape[1])\n        detections_one = self.untile_detections(len(frames), frame_size, detections_resized)\n        detections_one = self.net.nms(detections_one) # \n        \n        # for each frame\n        total_faces = []\n        for i, detection_single in enumerate(detections_one):\n            # no face\n            if len(detection_single) == 0:\n                continue\n                \n                \n            # 5 - add margin\n            detections_margin = self.add_margin_to_detections(detection_single, frame_size=frame_size)\n\n            # 6 - crop\n            faces_crop = self.crop_faces(frames[i], detections_margin)\n            \n            faces_final = []\n            for face in faces_crop:\n                # 7 - resize\n                face_resized = self.isotropically_resize_image(face, self.output_size)\n            \n                # 8 - fill blank\n                face_final = self.make_square_image(face_resized)\n                faces_final.append(face_final)\n            total_faces.append(faces_final)\n        return total_faces\n                \n    \n    \n    def tile_frames(self, frames, target_size):\n        num_frames, H, W, _ = frames.shape\n\n        split_size = min(H, W)\n        x_step = (W - split_size) \/\/ 2\n        y_step = (H - split_size) \/\/ 2\n        num_v = 1\n        num_h = 3 if W > H else 1\n\n        splits = np.zeros((num_frames * num_v * num_h, target_size[1], target_size[0], 3), dtype=np.uint8)\n\n        i = 0\n        for f in range(num_frames):\n            y = 0\n            for v in range(num_v):\n                x = 0\n                for h in range(num_h):\n                    crop = frames[f, y:y+split_size, x:x+split_size, :]\n                    splits[i] = cv2.resize(crop, target_size, interpolation=cv2.INTER_AREA)\n                    x += x_step\n                    i += 1\n                y += y_step\n\n        resize_info = [split_size \/ target_size[0], split_size \/ target_size[1]]\n        return splits, resize_info\n    \n    \n    \n    def resize_detections(self, detections, target_size, resize_info):\n        '''get detections from [0, 1] to original frame size (1080, 1080)'''\n\n        projected = []\n        target_w, target_h = target_size  # (128, 128)\n        scale_w, scale_h = resize_info #  (1080\/128, 1080\/128)\n\n        # each frame\n        for i in range(len(detections)):\n            detection = detections[i].clone()\n\n            # ymin, xmin, ymax, xmax\n            for k in range(2): #0, 1\n                detection[:, k*2    ] = (detection[:, k*2    ] * target_h) * scale_h\n                detection[:, k*2 + 1] = (detection[:, k*2 + 1] * target_w) * scale_w\n\n            # keypoints are x,y\n            for k in range(2, 8):\n                detection[:, k*2    ] = (detection[:, k*2    ] * target_w) * scale_w\n                detection[:, k*2 + 1] = (detection[:, k*2 + 1] * target_h) * scale_h\n\n            projected.append(detection)\n\n        return projected \n    \n    \n    \n    def untile_detections(self, num_frames, frame_size, detections):\n        \"\"\"With N tiles per frame, there also are N times as many detections.\n        This function groups together the detections for a given frame; it is\n        the complement to tile_frames().\n        detections: (192, ?, 17)\n        \"\"\"\n        combined_detections = []\n\n        W, H = frame_size\n        split_size = min(H, W)\n        x_step = (W - split_size) \/\/ 2\n        y_step = (H - split_size) \/\/ 2\n        num_v = 1\n        num_h = 3 if W > H else 1\n\n        i = 0\n        for f in range(num_frames):\n            detections_for_frame = []\n            y = 0\n            for v in range(num_v):\n                x = 0\n                for h in range(num_h):\n                    # Adjust the coordinates based on the split positions.\n                    detection = detections[i].clone()\n                    if detection.shape[0] > 0:\n                        for k in range(2):\n                            detection[:, k*2    ] += y\n                            detection[:, k*2 + 1] += x\n                        for k in range(2, 8):\n                            detection[:, k*2    ] += x\n                            detection[:, k*2 + 1] += y\n\n                    detections_for_frame.append(detection)\n                    x += x_step\n                    i += 1\n                y += y_step\n\n            combined_detections.append(torch.cat(detections_for_frame))\n\n        return combined_detections\n        \n        \n        \n    def add_margin_to_detections(self, detections, frame_size, margin=0.2):\n        \"\"\"Expands the face bounding box.\n\n        NOTE: The face detections often do not include the forehead, which\n        is why we use twice the margin for ymin.\n\n        Arguments:\n            detections: a PyTorch tensor of shape (num_detections, 17)\n            frame_size: maximum (width, height)\n            margin: a percentage of the bounding box's height\n\n        Returns a PyTorch tensor of shape (num_detections, 17).\n        \"\"\"\n        offset = torch.round(margin * (detections[:, 2] - detections[:, 0])) # (ymax - ymin)*margin\n        detections = detections.clone()\n        detections[:, 0] = torch.clamp(detections[:, 0] - offset*2, min=0)            # ymin\n        detections[:, 1] = torch.clamp(detections[:, 1] - offset, min=0)              # xmin\n        detections[:, 2] = torch.clamp(detections[:, 2] + offset, max=frame_size[1])  # ymax\n        detections[:, 3] = torch.clamp(detections[:, 3] + offset, max=frame_size[0])  # xmax\n        return detections\n\n\n      \n    def crop_faces(self, frame, detections):\n        \"\"\"Copies the face region(s) from the given frame into a set\n        of new NumPy arrays.\n\n        Arguments:\n            frame: a NumPy array of shape (H, W, 3)\n            detections: a PyTorch tensor of shape (num_detections, 17)\n\n        Returns a list of NumPy arrays, one for each face crop. If there\n        are no faces detected for this frame, returns an empty list.\n        \"\"\"\n        faces = []\n        for i in range(len(detections)):\n            ymin, xmin, ymax, xmax = detections[i, :4].cpu().numpy().astype(np.int)\n            face = frame[ymin:ymax, xmin:xmax, :]\n            faces.append(face)\n        return faces\n        \n        \n        \n    def isotropically_resize_image(self, img, size, resample=cv2.INTER_AREA):\n        h, w = img.shape[:2]\n        if w > h:\n            h = h * size \/\/ w\n            w = size\n        else:\n            w = w * size \/\/ h\n            h = size\n\n        resized = cv2.resize(img, (w, h), interpolation=resample)\n        return resized\n        \n        \n        \n    def make_square_image(self, img):\n        h, w = img.shape[:2]\n        size = max(h, w)\n        t = 0\n        b = size - h\n        l = 0\n        r = size - w\n        return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)\n","a5ec0274":"%%time\nbefore = time.time()\n\nblaze = BlazeFaceExtractor()\ntotal_faces = blaze.extract_faces(sample_video, read_video2, IMG_SIZE)\n\nafter = time.time()\nprint('result {} with time {} seconds'.format(len(total_faces), round(after-before, 1)))","926c701f":"fig, ax = plt.subplots(cell, cell, figsize=(8, 8))\nfor i in range(cell):\n    for j in range(cell):\n        ax[i][j].imshow(total_faces[random_idx[i*cell+j]][0])","f4488f63":"def load_mobilenetv2_224_075_detector(path):\n    input_tensor = Input(shape=(224, 224, 3))\n    output_tensor = MobileNetV2(weights=None, include_top=False, input_tensor=input_tensor, alpha=0.75).output\n    output_tensor = ZeroPadding2D()(output_tensor)\n    output_tensor = Conv2D(kernel_size=(3, 3), filters=5)(output_tensor)\n\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    model.load_weights(path)\n    \n    return model","6ffa45e4":"# mobilenetv2 = load_mobilenetv2_224_075_detector(\"..\/input\/facedetection-mobilenetv2\/facedetection-mobilenetv2-size224-alpha0.75.h5\")\n# mobilenetv2.summary()","bf2ec187":"imgs_path = glob.glob('..\/input\/dfdcfaceevaluate\/*.jpg')\nimgs = np.array([cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB) for img_path in imgs_path])\nprint(imgs.shape)\n\nfig, ax = plt.subplots(4, 5, figsize=(10, 10))\nfor i in range(4):\n    for j in range(5):\n        ax[i][j].imshow(cv2.cvtColor(cv2.imread(imgs_path[i*4+j]), cv2.COLOR_BGR2RGB))","580f7411":"fig, ax = plt.subplots(4, 5, figsize=(10, 10))\nfor i in range(4):\n    for j in range(5):\n        # show origin face\n        ax[i][j].imshow(imgs[i*4+j])\n        # show detected\n        f = mtcnn_detector.detect_faces(imgs[i*4+j])\n        if len(f) <= 0:\n            continue\n        for n in f:\n            x,y,w,h = n['box']\n            rect = patches.Rectangle((x,y),w,h, linewidth=1,edgecolor='r',facecolor='none')\n            ax[i][j].add_patch(rect)\n\nplt.show()","6a1dd506":"fig, ax = plt.subplots(4, 5, figsize=(10, 10))\nfor i in range(4):\n    for j in range(5):        \n        # show detected\n        f = net_facenet(Image.fromarray(imgs[i*4+j]))\n        if f is None:\n            continue\n        else:\n            ax[i][j].imshow(imgs[i*4+j])\nplt.show()","18dae14e":"fig, ax = plt.subplots(4, 5, figsize=(10, 10))\nfor i in range(4):\n    for j in range(5):     \n        # show origin face\n        ax[i][j].imshow(imgs[i*4+j])\n        \n        # show detected\n        gray = cv2.cvtColor(imgs[i*4+j], cv2.COLOR_BGR2GRAY)\n        f = dlib_hog_detector(gray, 1)\n        if len(f) > 0:\n            f = f[0]\n            x = f.left()\n            y = f.top()\n            w = f.right() - f.left()\n            h = f.bottom() - f.top()\n           \n            rect = patches.Rectangle((x,y),w,h, linewidth=1,edgecolor='r',facecolor='none')\n            ax[i][j].add_patch(rect)\n\nplt.show()","04ed9361":"fig, ax = plt.subplots(4, 5, figsize=(10, 10))\nfor i in range(4):\n    for j in range(5):     \n        # show origin face\n        ax[i][j].imshow(imgs[i*4+j])\n        \n        # show detected\n        f = insight_detector.get(imgs[i*4+j])\n        if len(f) > 0:\n            x = f[0].bbox[0]\n            y = f[0].bbox[1]\n            w = f[0].bbox[2] - f[0].bbox[0]\n            h = f[0].bbox[3] - f[0].bbox[1]\n            \n            rect = patches.Rectangle((x,y),w,h, linewidth=1,edgecolor='r',facecolor='none')\n            ax[i][j].add_patch(rect)\n        \nplt.show()","477f22a1":"fig, ax = plt.subplots(4, 5, figsize=(10, 10))\nfor i in range(4):\n    for j in range(5):     \n        # show origin face\n        ax[i][j].imshow(imgs[i*4+j])\n        \n        # show detected\n        f = mobile_detector(imgs[4*i+j])\n        if len(f) > 0:\n            x = f[0] # left\n            y = f[2] # top\n            w = f[1] - f[0] # right - left\n            h = f[3] - f[2] # bottom - top\n            \n            rect = patches.Rectangle((x,y),w,h, linewidth=1,edgecolor='r',facecolor='none')\n            ax[i][j].add_patch(rect)\n        \nplt.show()","5b7ce508":"fig, ax = plt.subplots(4, 5, figsize=(10, 10))\nfor i in range(4):\n    for j in range(5):     \n        # show origin face\n        ax[i][j].imshow(imgs[i*4+j])\n        \n        # show detected\n        f = blaze_detector(imgs[4*i+j])\n        if len(f) > 0:\n            x = f[0][0] # xmin\n            y = f[0][1] # ymin\n            w = f[0][2] - f[0][0] # right - left\n            h = f[0][3] - f[0][1] # bottom - top\n            \n            rect = patches.Rectangle((x,y),w,h, linewidth=1,edgecolor='r',facecolor='none')\n            ax[i][j].add_patch(rect)\n        \nplt.show()","b10ea98b":"import sys\nimport torch\nsys.path.insert(0,\"\/kaggle\/input\/retina-face\/Pytorch_Retinaface\/\")\nfrom data import cfg_mnet, cfg_re50\nfrom models.retinaface import RetinaFace","50572b61":"cfg_re50['image_size'], cfg_mnet['image_size']","fc9640c7":"def get_model(modelname=\"mobilenet\"):\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    \n    torch.set_grad_enabled(False)\n    cfg = None\n    cfg_mnet['pretrain'] = False\n    cfg_re50['pretrain'] = False\n    \n    if modelname == \"mobilenet\":\n        cfg = cfg_mnet\n        pretrained_path = \"\/kaggle\/input\/retina-face\/Pytorch_Retinaface\/weights\/mobilenet0.25_Final.pth\"\n    else:\n        cfg = cfg_re50\n        pretrained_path = \"\/kaggle\/input\/retina-face\/Pytorch_Retinaface\/weights\/Resnet50_Final.pth\"\n    \n    # net and model\n    net = RetinaFace(cfg=cfg, phase='test')\n    net.load_state_dict(torch.load(pretrained_path, map_location=torch.device('cpu')))\n    net.eval().to(device)\n    return net\n\nretina_detector = get_model()","b9c1a62e":"## 3-6) Blazeface","7f38ba96":"BlazeFace github repo offical version","24378b65":"## read specific frames","469c77a8":"2. Image_ffmpeg","9a89590f":"Credits: <br>\nmobileface:  https:\/\/www.kaggle.com\/unkownhihi\/mobilenet-face-extractor-helper-code\/comments <br>\nyolo: https:\/\/www.kaggle.com\/unkownhihi\/mobilenet-face-extractor-comparison#Initialize-Yolo <br>\ndlib: https:\/\/www.kaggle.com\/carlossouza\/face-detection-in-a-couple-of-lines-with-dlib <br>\nblaze face: https:\/\/www.kaggle.com\/humananalog\/inference-demo <br>\nblaze face 2: https:\/\/www.kaggle.com\/unkownhihi\/mobilenet-face-extractor-comparison#Initialize-Yolo <br>\nfacenet_pytorch:  <br>\nMTCNN:  <br>\nRetinaFace: <br>","25b18f25":"## MTCNN","dbfc0269":"random effects","ea6cd443":"## 3-5) Mobilenet","2d972a3e":"# 2) Video Read Speed","35d0ccd6":"1. Opencv Read Video","90b7386f":"1-2 show detected result without tile image","e4c5adba":"# Yolo v2\nsource : https:\/\/www.kaggle.com\/drjerk\/detect-faces-using-yolo","34ff7db5":"detect number of blaze face","1bc2d3ab":"2. v_cap.read()","e0d385f4":"## Read all frames","f81eaaf1":"v_cap.grab() and v_cap.retrieve()","3b7c3f7f":"# 1) Preparations","c04826f6":"## 3-4) Insightface","d05074e7":"4. v_cap.cat","bd036818":"effect of blaze face, no deformed","97721f4f":"2-1 see how many faces are detected with ***tile image***","73d52cae":"# dlib","158927e8":"3. imageio_ffmpeg","928230d0":"## 3-1) MTCNN","9215532a":"# Face Detectoe Evaluate","fbed6db6":"## 3-2) facenet_pytorch","f3b9a517":"# Retina","f3526d66":"## 3-3) dlib","ac2754cc":"# BlazeFace","ac5aeb14":"# Facenet_pytorch","0194a1ac":"speed and detect number","e57030a4":"# 3) Face Detection - Imgs","e3a473e8":"1-1 without ***tile image***, some faces cannot be detectd <br>\nsee how many faces are detected","0ea9ede3":"1. v_cap.grab()"}}