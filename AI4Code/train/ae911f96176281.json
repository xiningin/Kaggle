{"cell_type":{"7c0ad437":"code","07686009":"code","a2d0ac12":"code","3eedc145":"code","b7a04b84":"code","5dde97e1":"code","9adc28d3":"code","42eaba13":"code","cbe2902b":"code","4a18a2f9":"code","035725ab":"code","828ffce2":"code","e83e735d":"code","6dad3467":"code","fe66f82f":"code","83ac0096":"code","8e214017":"code","a8bcfcfb":"code","4a1c7693":"code","5bde36e1":"code","d534f097":"code","53399902":"code","1adf2871":"code","6b4904b7":"code","64e53739":"code","17d9fc71":"code","dbb13c72":"code","0207f6af":"code","e703c427":"code","d50718e9":"code","d62cc4bc":"code","e81a31c5":"code","02f627be":"code","0e26f343":"code","28caff51":"code","ae02b4ca":"code","56b0078b":"code","7000003b":"code","4f453c84":"code","840c8999":"code","b14a6114":"code","ac7c83f7":"code","de2ffd90":"code","7539ff6c":"markdown","f2af1890":"markdown","60f518bc":"markdown","84a884c0":"markdown","0606851c":"markdown","100bd71d":"markdown"},"source":{"7c0ad437":"!pip install -U efficientnet","07686009":"!pip install -q --upgrade wandb \n\nimport wandb\nprint(wandb.__version__)\nfrom wandb.keras import WandbCallback\n\n","a2d0ac12":"#wandb.login()","3eedc145":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import StratifiedKFold\nimport tensorflow_addons as tfa\nfrom sklearn.utils import class_weight\nimport os \n\nimport efficientnet.keras as efn\nfrom sklearn.metrics import confusion_matrix\n\nimport glob","b7a04b84":"gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","5dde97e1":"AUTO_ENCODER = True\nEFFNET = False\n\ntrain_csv = \"..\/input\/seti-breakthrough-listen\/train_labels.csv\"\ntrain_df_master = pd.read_csv( train_csv)\ntrain_df_master[\"path\"] = train_df_master[\"id\"].apply( lambda x: \"..\/input\/seti-breakthrough-listen\/train\/\"+ str(x[0]) +\"\/\"+x +\".npy\" )\ntrain_df_master.head()\n","9adc28d3":"def create_heat_map( file_name,target  ):\n    data = np.load( file_name  )\n    plt.figure( figsize =( 16,8), dpi = 80 )\n    for i in range( 0, 6):\n        plt.subplot( 1, 6, i +1)\n        sns.heatmap( data [ i ] )\n        plt.suptitle ( file_name +\" = \"+ str ( target) )\n    plt.show()\n#create_heat_map ( train_df_master[\"path\"].iloc[np.random.randint (1000) ]  )","42eaba13":"for i in range ( 0,4 ):\n    if i % 2 == 0 & False :\n        shape = train_df_master[train_df_master[\"target\"] == 0 ].shape[0]\n        file_name = train_df_master[\"path\"].iloc[ shape]\n        target = 0\n    else:\n        shape = train_df_master[train_df_master[\"target\"] == 1 ].shape[0]\n        file_name = train_df_master[\"path\"].iloc[ shape ]\n        target = 1\n        \n    create_heat_map( file_name, target  )\n        ","cbe2902b":"fig = plt.figure( figsize=(10,5), dpi = 80 ) \nsns.countplot( train_df_master['target'])\nplt.show()","4a18a2f9":"## perform over sample \nif True:\n    \n    anamolly_df  = train_df_master[ train_df_master[\"target\"] == 1]\n    train_df_master = pd.concat (  [ train_df_master,anamolly_df,anamolly_df,anamolly_df ])\n    train_df_master.shape","035725ab":"fig = plt.figure( figsize=(10,5), dpi = 80 ) \nsns.countplot( train_df_master['target'])\nplt.show()","828ffce2":"CFG= {\n    \n    \"IMG_LENGTH\" :  273,#256\n    \"IMG_WIDTH\" : 256,\n    \"CHANNELS\" : 3,\n    \"RANDOM_STATE\" : 100,\n    \"BATCH_SIZE\"  :100,\n    \"FOLDS\" : 5,\n    \"LEARNING_RATE\" : 0.1\n}\n\nCFG_ENCODE_DECODE= {\n    \n    \"IMG_LENGTH\" :  256,\n    \"IMG_WIDTH\" : 256,\n    \"CHANNELS\" : 3,\n    \"RANDOM_SATE\" : 100,\n    \"BATCH_SIZE\"  :50,\n    \"FOLDS\" : 5,\n    \"LEARNING_RATE\" : 0.1\n}\n    \n    ","e83e735d":"\ntrain_df,test_df = train_test_split ( train_df_master, train_size = 0.8, random_state= CFG[\"RANDOM_STATE\"],shuffle = True,stratify = train_df_master[\"target\"])\nval_df, test_df = train_test_split ( test_df, test_size = 0.2 , random_state= CFG[\"RANDOM_STATE\"],shuffle = True,stratify = test_df[\"target\"])\n\n\nprint (\"number of samples for train data set  = {} \".format(len ( train_df) ) )\nprint (\"number of samples for test data set  = {} \".format(len ( test_df)))\nprint (\"number of samples for validation data set  = {} \".format(len ( val_df ) ) )","6dad3467":"## Creating data generator which can work on Both TPU + GPU\n\ndef decode_numpy(  ):\n    \n    def read_image(file_name ):\n        #print ( file_name)\n        np_data =  tf.io.read_file ( file_name )\n        np_data = tf.io.decode_raw( np_data, tf.float16 )\n        np_data = tf.reshape( np_data[64:], (6, 273, 256 )) # (6, 273, 256 ) is data origional shape \n        #np_data = tf.concat ( (np_data[0],np_data[2],np_data[4]), axis = 0)\n        #np_data = tf.stack([np_data[0],np_data[2],np_data[4]] ,axis = 2 ) \n        np_data = tf.stack([np_data[0],np_data[2],np_data[4]] ,axis = 2 )\n        #np_data = tf.concat([np_data[0],np_data[2],np_data[4]] ,axis = -1  )\n        #np_data = tf.stack([np_data,np_data,np_data] ,axis = 2 )\n        np_data = tf.reshape( np_data, ( CFG[\"IMG_LENGTH\"], CFG[\"IMG_WIDTH\"], CFG[\"CHANNELS\"] ))\n        #np_data = tf.image.r(np_data)\n        return np_data   #(819, 256)is output shape \n        \n    def decode( file_name, target ):\n        return read_image ( file_name ), target#tf.cast(target, tf.float32) \n    \n    return decode\n\ndef data_augmentation( ):\n    \n    def add_augmentation( image ):\n        \n        image = tf.image.random_flip_up_down( image,seed= CFG[\"RANDOM_STATE\"] )\n        image = tf.image.random_flip_left_right( image ,seed= CFG[\"RANDOM_STATE\"] )\n        image = tf.image.random_contrast( image,lower =0.1,upper = 0.5 ,seed= CFG[\"RANDOM_STATE\"] )\n        image = tf.image.random_saturation( image,lower =5,upper = 7 ,seed= CFG[\"RANDOM_STATE\"])\n        #image = tf.image.random_crop( value = image , size = (3,3), seed=CFG[\"RANDOM_STATE\"] )\n        return image\n    def call_add_augmentation( data, target ):\n        return add_augmentation( data), target \n    \n    return  call_add_augmentation\n\ndef datagenerator_rev_01(df,test = False ):\n    file_list = df[\"path\"].to_list() \n    target = df[\"target\"].to_list() \n    decode_tf = decode_numpy()\n    augment_fn = data_augmentation()\n    \n    datagen = tf.data.Dataset.from_tensor_slices( (file_list,target ))\n    datagen = datagen.map( decode_tf ,num_parallel_calls= tf.data.AUTOTUNE )\n    datagen = datagen.map(augment_fn, num_parallel_calls= tf.data.AUTOTUNE ) if not test else datagen\n    datagen = datagen.repeat() if not test else datagen\n    datagen = datagen.shuffle(1024) if not test else datagen\n    datagen = datagen.batch(CFG[\"BATCH_SIZE\"]).prefetch(tf.data.AUTOTUNE )\n    return  datagen\n\n","fe66f82f":"## Creating data generator which can work on Both TPU + GPU\n\ndef decode_numpy_auto_ecoder(  ):\n    \n    def read_image(file_name ):\n        #print ( file_name)\n        np_data =  tf.io.read_file ( file_name )\n        np_data = tf.io.decode_raw( np_data, tf.float16 )\n        np_data = tf.reshape( np_data[64:], (6, 273, 256 )) # (6, 273, 256 ) is data origional shape \n        np_data = tf.stack([np_data[1],np_data[3],np_data[5]] ,axis = 2 )\n        np_data = np_data[np.random.randint(3) ]\n        np_data = tf.stack([np_data,np_data,np_data] ,axis = 2 )\n        np_data = tf.image.resize(np_data,( CFG_ENCODE_DECODE[\"IMG_LENGTH\"], CFG_ENCODE_DECODE[\"IMG_WIDTH\"] ) )\n        return tf.data.Dataset.from_tensors( np_data[:,:,0:1] ) #(256, 256)is output shape \n        \n    def decode( file_name ):\n        return read_image ( file_name )\n    \n    return decode\n\ndef input_output():\n    \n    def auto_encode_input_output(data):\n        return data,data\n    \n    return auto_encode_input_output\n\n\ndef Auto_Encoder_Data_Generator(df,test = False,channel =None ):\n    file_list = df[\"path\"].to_list() \n    target = df[\"target\"].to_list() \n    decode_tf = decode_numpy_auto_ecoder()\n    \n    io_for_auto_encode =input_output()\n    \n    data_gen = tf.data.Dataset.from_tensor_slices( (file_list  ) )\n    data_gen = data_gen.interleave(decode_tf,num_parallel_calls=tf.data.AUTOTUNE) \n    data_gen = data_gen.map( io_for_auto_encode,num_parallel_calls=tf.data.AUTOTUNE )\n    data_gen = data_gen.shuffle( 30 ,seed = CFG_ENCODE_DECODE[\"RANDOM_SATE\"], reshuffle_each_iteration = True ) if not test else data_gen\n    data_gen = data_gen.batch( CFG_ENCODE_DECODE[\"BATCH_SIZE\"]) if not test else data_gen\n    data_gen = data_gen.prefetch( tf.data.AUTOTUNE )\n    data_gen = data_gen.repeat() if not test else data_gen\n    \n    \n    \n    return  data_gen\n\n","83ac0096":"if False:\n    \n    def read_numpy_data( file_name_result ):\n\n        #random_int = np.random.randint(3)\n        #channel = [1,3,5]\n        file_name=file_name_result\n        data = np.load( file_name.numpy() ).astype( np.float16 )    \n        data =np.vstack ( (data[0],data[2],data[4]) )\n        #data = np.vstack( (data,data,data) )\n        return  np.dstack ( [ data, data, data] ) \n\n\n\n    def resize_image( df_dict ):\n\n        [image, ] =  tf.py_function( read_numpy_data,  [ df_dict[\"path\"] ], [tf.float16] )\n        image.set_shape( ( CFG.IMG_LENGTH,CFG.IMG_WIDTH, CFG.CHANNELS ) )\n        image = tf.image.resize( image , ( CFG.IMG_LENGTH,CFG.IMG_WIDTH ) )\n        label = df_dict[\"target\"]\n        label = tf.cast( label, tf.int16 )\n\n        return  image, label \n\n    def data_augmentation( ):\n\n        def add_augmentation( image ):\n\n            #image = tf.image.random_flip_up_down( image )\n            #image = tf.image.random_flip_up_down( image )\n            #image = tf.image.random_contrast( image )\n            return image\n        def call_add_augmentation( data, target ):\n            return add_augmentation( data), target \n\n        return  call_add_augmentation\n\n\n    def create_tf_dataset( dataframe, test =False  ):\n\n        augment_fn = data_augmentation()\n\n        tf_dataset = tf.data.Dataset.from_tensor_slices ( dict ( dataframe[ [\"path\",\"target\"] ] ) ) \n        \n        if test :\n        \n            tf_dataset = ( tf_dataset\n                          .map( resize_image,num_parallel_calls= tf.data.AUTOTUNE)\n                          .batch( CFG.BATCH_SIZE )\n                          .prefetch( tf.data.AUTOTUNE)\n                         )\n        else:\n            tf_dataset = ( tf_dataset\n                          .shuffle (1024)\n                          .map( resize_image,num_parallel_calls= tf.data.AUTOTUNE)\n                          .batch( CFG.BATCH_SIZE )\n                          .prefetch( tf.data.AUTOTUNE)\n                         )\n        tf_dataset = tf_dataset.map (augment_fn,num_parallel_calls= tf.data.AUTOTUNE )\n\n        return  tf_dataset  \n\n    train_data_gen = create_tf_dataset( train_df ) \n    val_data_gen = create_tf_dataset( val_df ) \n    test_data_gen = create_tf_dataset ( test_df, test = True ) ","8e214017":"def build_lrfn(lr_start=0.00001, lr_max=0.000075, lr_min=0.000001, lr_rampup_epochs=20, lr_sustain_epochs=0, lr_exp_decay=.8):\n    lr_max = lr_max #* strategy.num_replicas_in_sync\n    \n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay ** (epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn","a8bcfcfb":"def get_model():\n    \n    \n    input_mo = tf.keras.layers.Input(shape= (  CFG[\"IMG_LENGTH\"], CFG[\"IMG_WIDTH\"], CFG[\"CHANNELS\"] ) )\n\n    efff_net =tf.keras.applications.EfficientNetB0(include_top = False, \n                                                   weights =\"imagenet\" , \n                                                   input_shape = ( CFG[\"IMG_LENGTH\"], CFG[\"IMG_WIDTH\"], CFG[\"CHANNELS\"]) ,\n                                                   input_tensor = input_mo,\n                                                   classes=2,\n                                                   pooling = True,\n                                                   #classifier_activation='softmax',\n                                                   drop_connect_rate= 0.1\n                                                  )\n\n    for layer in  efff_net.layers  :\n        #if not isinstance(layer, tf.keras.layers.BatchNormalization):\n        layer.trainable = True\n        \n\n    model = tf.keras.Sequential( [ efff_net,\n            tf.keras.layers.GlobalMaxPool2D(),\n            #tf.keras.layers.Dropout(0.25 ),\n\n            #tf.keras.layers.Dense(600, activation='relu'),\n            #tf.keras.layers.Flatten(),\n            #tf.keras.layers.Dropout(0.2 ),\n            tf.keras.layers.Dense(32, activation='relu'),\n            \n            tf.keras.layers.Dense(1, activation='sigmoid')\n            \n            ])    \n    \n    \n    #model.summary()\n    \n    #model.compile(optimizer, \n    #              loss=tfa.losses.SigmoidFocalCrossEntropy(), \n    #              metrics=[tf.keras.metrics.AUC(curve='ROC')])\n    \n    return ( model )","4a1c7693":"def get_model_effnet_07():\n    \n    \n    #input_ = layers.Input((3*273,256,3))\n    input_ = tf.keras.layers.Input((273,256,3))\n    \n    #x = efn.EfficientNetB0(input_shape=(3*273,256,3),weights='noisy-student',include_top=False)(input_)\n    x = efn.EfficientNetB7(input_shape=(273,256,3),weights='noisy-student',include_top=False)(input_)\n    #x = layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.GlobalMaxPool2D()(x)\n    x = tf.keras.layers.Dense(32)(x)\n    x = tf.keras.layers.Activation(\"relu\")(x)\n    \n    x = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n    \n    model = tf.keras.models.Model(inputs=input_, outputs=x)   \n    \n    \n    #model.summary()\n    \n    #model.compile(optimizer, \n    #              loss=tfa.losses.SigmoidFocalCrossEntropy(), \n    #              metrics=[tf.keras.metrics.AUC(curve='ROC')])\n    \n    return ( model )","5bde36e1":"#x = efn.EfficientNetB0(input_shape=(273,256,3),weights='noisy-student',include_top=False)(input_)\n\n\ndef get_model_noisy_student_weights():\n    \n    \n    input_mo = tf.keras.layers.Input(shape= (  CFG[\"IMG_LENGTH\"], CFG[\"IMG_WIDTH\"], CFG[\"CHANNELS\"] ) )\n\n    efff_net =efn.EfficientNetB0(include_top = False, \n                                                   weights =\"noisy-student\" , \n                                                   input_shape = ( CFG[\"IMG_LENGTH\"], CFG[\"IMG_WIDTH\"], CFG[\"CHANNELS\"]) ,\n                                                   input_tensor = input_mo,\n                                                   classes=2,\n                                                   pooling = True,\n                                                   #classifier_activation='softmax',\n                                                   #drop_connect_rate= 0.1\n                                                  )\n\n    for layer in  efff_net.layers  :\n        #if not isinstance(layer, tf.keras.layers.BatchNormalization):\n        layer.trainable = False\n        \n\n    model = tf.keras.Sequential( [#tf.keras.layers.GaussianNoise( stddev=0.3 ),\n                                  efff_net,\n                                  tf.keras.layers.Flatten(),\n                                  #tf.keras.layers.GlobalAveragePooling2D(),\n            \n            #tf.keras.layers.Dropout(0.25 ),\n            tf.keras.layers.Dense(1000, activation='relu'),\n            tf.keras.layers.Dropout(0.1 ),\n            tf.keras.layers.Dense(100, activation='relu'),\n            tf.keras.layers.Dropout(0.1 ),\n            tf.keras.layers.Dense(32, activation='relu'),\n            \n            tf.keras.layers.Dense(1, activation='sigmoid')\n            \n            ])    \n    \n    \n    #x = layers.GlobalAveragePooling2D()(x)\n    \n    #x = layers.Dense(32)(x)\n    #x = layers.Activation(\"relu\")(x)\n    \n    #x = layers.Dense(1, activation=\"sigmoid\")(x)\n    \n    \n    return ( model )","d534f097":"#model_1 = get_model_noisy_student_weights()\n#model_1.summary()","53399902":"if False:\n    \n\n    CFG[\"BATCH_SIZE\"] =100\n\n    lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(  patience=3,\n                                                        min_lr= 0.000001,\n                                                        monitor='val_loss', \n                                                        factor=0.6, \n                                                        verbose=1,\n                                                        mode='auto', \n                                                       )\n\n    train_df_2 = train_df[[\"path\",\"target\"]].iloc[:30000]\n    val_df_2 = val_df[[\"path\",\"target\"]].iloc[:5000]\n\n    CFG['model_name'] = 'efficientnetb0-folds'\n    CFG['group'] = 'K-Fold-EnetB0'\n    CFG['run_name'] = 'baseline-k-fold'\n    model = get_model()\n\n    class_weight_for_train = {0:1, 1: (train_df_2[train_df_2[\"target\"]==0].shape[0]\/train_df_2[train_df_2[\"target\"]==1].shape[0]) }\n    CFG[\"TRAIN_STEPS\"] = int ( train_df_2.shape[0]\/CFG[\"BATCH_SIZE\"] ) + (1 if train_df_2.shape[0]% CFG[\"BATCH_SIZE\"] != 0 else 0)\n    CFG[\"VAL_STEPS\"] = int ( val_df_2.shape[0]\/CFG[\"BATCH_SIZE\"] ) + (1 if val_df_2.shape[0]% CFG[\"BATCH_SIZE\"] != 0 else 0)\n\n    train_data_gen = datagenerator_rev_01( train_df_2 ) \n    val_data_gen = datagenerator_rev_01( val_df_2 ) \n\n\n\n    #model = get_model()\n\n    model.build( ( None, CFG[\"IMG_LENGTH\"], CFG[\"IMG_WIDTH\"], CFG[\"CHANNELS\"]) )\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0006000000284984708)#0.001) #tf.keras.optimizers.Adam(learning_rate=0.1)\n    model.compile( optimizer= optimizer,loss=\"binary_crossentropy\", metrics=[tf.keras.metrics.AUC(curve='ROC')])\n\n    checkpoint = tf.keras.callbacks.ModelCheckpoint( f'model{1}.h5', save_best_only=True, monitor='val_loss', mode='min')\n\n\n    model_history = model.fit_generator(train_data_gen,\n                                        #class_weight= class_weight_for_train,\n                                         steps_per_epoch= CFG[\"TRAIN_STEPS\"], \n                                         epochs =5, \n                                         validation_data= val_data_gen,\n                                         validation_steps = CFG[\"VAL_STEPS\"],\n                                         callbacks=[ checkpoint,lr_reducer]\n                                       )\n\n    # Initialize W&B run\n    run = wandb.init(project='kaggle-seti', \n                     config=CFG,\n                     group=CFG['group'], \n                     job_type='train',\n                     name=CFG['run_name'])\n\n    # Evaluate\n    loss, auc = model.evaluate( val_data_gen, steps = CFG[\"VAL_STEPS\"])\n    wandb.log({'Val AUC-ROC': auc})\n\n    # Save model\n    model_name = CFG['model_name']\n    MODEL_PATH= f'models\/{model_name}'\n    os.makedirs(MODEL_PATH, exist_ok=True)\n    count_models = len(os.listdir(MODEL_PATH))\n\n    model.save(f'{MODEL_PATH}\/{model_name}_{count_models}.h5')\n\n    run.finish()\n\n","1adf2871":"if False:\n    TEST_STEPS = int ( test_df.shape[0]\/CFG[\"BATCH_SIZE\"] ) + (1 if test_df.shape[0]% CFG[\"BATCH_SIZE\"] != 0 else 0)\n    test_data_gen = datagenerator_rev_01 ( test_df[[\"path\",\"target\"]],True )    \n    prediction = model.predict_classes( test_data_gen, batch_size = CFG[\"BATCH_SIZE\"] )","6b4904b7":"if False:\n    test_predict_df = test_df\n    test_predict_df[\"prediction\"]= prediction\n    test_predict_df.head()","64e53739":"if False: confusion_matrix( test_predict_df[\"target\"], test_predict_df[\"prediction\"])","17d9fc71":"def create_model():\n    #input_ = layers.Input((3*273,256,3))\n    input_ = tf.keras.layers.Input((273,256,3))\n    \n    #x = efn.EfficientNetB0(input_shape=(3*273,256,3),weights='noisy-student',include_top=False)(input_)\n    x = efn.EfficientNetB0(input_shape=(273,256,3),weights='noisy-student',include_top=False)(input_)\n    #x = layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.GlobalMaxPool2D()(x)\n    x = tf.keras.layers.Dense(32)(x)\n    x = tf.keras.layers.Activation(\"relu\")(x)\n    \n    x = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n    \n    model = tf.keras.models.Model(inputs=input_, outputs=x)\n    \n    return model","dbb13c72":"## Mode building with Efficient net with pre trained model \n\nif EFFNET :\n    \n    nosy_eff_net = create_model ()\n    #nosy_eff_net.build( ( None, CFG[\"IMG_LENGTH\"], CFG[\"IMG_WIDTH\"], CFG[\"CHANNELS\"]) )\n    nosy_eff_net.load_weights(\"..\/input\/seti-tpu-rev-02\/model_TPU_REV_02.h5\")\n\n    \n    CFG[\"BATCH_SIZE\"] =75\n\n    lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(  patience=1,\n                                                        min_lr= 0.0000001,\n                                                        monitor='val_loss', \n                                                        factor=0.6, \n                                                        verbose=1,\n                                                        mode='auto', \n                                                       )\n\n    train_df_2 = train_df[[\"path\",\"target\"]]#.iloc[:30000]\n    val_df_2 = val_df[[\"path\",\"target\"]]#.iloc[:5000]\n\n    CFG['model_name'] = 'efficientnetb0-folds_nosy_oversample'\n    CFG['group'] = 'K-Fold-EnetB0_nosy'\n    CFG['run_name'] = 'baseline-k-fold_nosy'\n\n\n    #class_weight_for_train = {0:1, 1: (train_df_2[train_df_2[\"target\"]==0].shape[0]\/train_df_2[train_df_2[\"target\"]==1].shape[0]) }\n    CFG[\"TRAIN_STEPS\"] = int ( train_df_2.shape[0]\/CFG[\"BATCH_SIZE\"] ) + (1 if train_df_2.shape[0]% CFG[\"BATCH_SIZE\"] != 0 else 0)\n    CFG[\"VAL_STEPS\"] = int ( val_df_2.shape[0]\/CFG[\"BATCH_SIZE\"] ) + (1 if val_df_2.shape[0]% CFG[\"BATCH_SIZE\"] != 0 else 0)\n\n    train_data_gen = datagenerator_rev_01( train_df_2 ) \n    val_data_gen = datagenerator_rev_01( val_df_2 ) \n\n\n\n\n    #0.0006000000284984708\n    optimizer = tf.keras.optimizers.Adam(learning_rate= 0.0003600000170990825  ) #tf.keras.optimizers.Adam(learning_rate=0.1)\n    nosy_eff_net.compile( optimizer= optimizer,loss=\"binary_crossentropy\", metrics=[tf.keras.metrics.AUC(curve='ROC')])\n\n    checkpoint = tf.keras.callbacks.ModelCheckpoint( f'model{1}.h5', save_best_only=True, monitor='val_loss', mode='min')\n\n\n    model_history = nosy_eff_net.fit_generator(train_data_gen,\n                                        #class_weight= class_weight_for_train,\n                                         steps_per_epoch= CFG[\"TRAIN_STEPS\"], \n                                         epochs =5, \n                                         validation_data= val_data_gen,\n                                         validation_steps = CFG[\"VAL_STEPS\"],\n                                         callbacks=[ checkpoint,lr_reducer]\n                                       )\n    nosy_eff_net.save(\".\/nosy_eff_net_with_guassian_noise_over_sample_part_2.h5\")\n    # Initialize W&B run\n    run = wandb.init(project='kaggle-seti', \n                     config=CFG,\n                     group=CFG['group'], \n\n                     job_type='train',\n                     name=CFG['run_name'])\n\n    # Evaluate\n    loss, auc = nosy_eff_net.evaluate( val_data_gen, steps = CFG[\"VAL_STEPS\"])\n    wandb.log({'Val AUC-ROC': auc})\n\n    # Save model\n    model_name = CFG['model_name']\n    MODEL_PATH= f'models\/{model_name}'\n    os.makedirs(MODEL_PATH, exist_ok=True)\n    count_models = len(os.listdir(MODEL_PATH))\n\n    nosy_eff_net.save(f'{MODEL_PATH}\/{model_name}_{count_models}.h5')\n\n    run.finish()","0207f6af":"if EFFNET :\n    nosy_eff_net = get_model_effnet_07()\n    nosy_eff_net.load_weights(\"..\/input\/setitpurev03\/model_TPU_REV_03.h5\")","e703c427":"if EFFNET :\n    submission_df = pd.read_csv(\"..\/input\/seti-breakthrough-listen\/sample_submission.csv\")\n    submission_df[\"path\"] = submission_df[\"id\"].apply( lambda x: \"..\/input\/seti-breakthrough-listen\/test\/\"+ str(x[0]) +\"\/\"+x +\".npy\" )\n    submission_df[\"target\"] = [1]*submission_df.shape[0]\n    submission_df.head()","d50718e9":"if EFFNET : submission_df.shape","d62cc4bc":"if EFFNET : CFG[\"BATCH_SIZE\"] = 50","e81a31c5":" if EFFNET :\n    fial_submission_data_gen = datagenerator_rev_01 ( submission_df[[\"path\",\"target\"]], True ) \n    final_prediction = nosy_eff_net.predict( fial_submission_data_gen, batch_size = CFG[\"BATCH_SIZE\"] )\n    submission_df[\"target\"]= final_prediction\n","02f627be":"if EFFNET : submission_df.head(10)","0e26f343":"if EFFNET :\n    final_submission = submission_df[[\"id\",\"target\"]]\n    final_submission.head()","28caff51":"if EFFNET : final_submission.to_csv(\".\/sample_submission.csv\",index = False)","ae02b4ca":"if EFFNET : test_data_gen = datagenerator_rev_01( test_df[[\"path\",\"target\"]], True ) ","56b0078b":"#nosy_eff_net = get_model_noisy_student_weights ()\n#nosy_eff_net.load_weights(\"..\/input\/train-model-weight-for-seti-anomally-detection\/PRETRAINED_MODEL_WEIGHTS.h5\")","7000003b":"\nif EFFNET : prediction_df = nosy_eff_net.predict_classes(test_data_gen ) ","4f453c84":"if EFFNET : test_df[\"prediction\"] = prediction_df","840c8999":"if EFFNET : confusion_matrix ( test_df[\"target\"],test_df[\"prediction\"] )","b14a6114":"encoder_decoder_model = tf.keras.Sequential(\n            [ tf.keras.layers.Input( shape=  ( CFG_ENCODE_DECODE[\"IMG_LENGTH\"],CFG_ENCODE_DECODE[\"IMG_WIDTH\"] ,1 ), name= \"encoder_input_layer\" ) ,\n              tf.keras.layers.Conv2D(filters = 256, kernel_size = (4,4), strides = 2,padding = \"valid\",activation =\"relu\",name =\"encoder_layer_01\"),\n              tf.keras.layers.Conv2D(filters = 128, kernel_size = (4,4), strides = 2,padding = \"valid\",activation =\"relu\",name =\"encoder_layer_02\"),\n              tf.keras.layers.Conv2D(filters = 64, kernel_size =  (4,4), strides = 2,padding = \"valid\",activation =\"relu\",name =\"encoder_layer_03\") ,\n              tf.keras.layers.Conv2D(filters = 32, kernel_size =  (4,4), strides = 2,padding = \"valid\",activation =\"relu\",name =\"encoder_layer_04\") ,\n              tf.keras.layers.Conv2D(filters = 16, kernel_size =  (4,4), strides = 2,padding = \"valid\",activation =\"relu\",name =\"encoder_layer_05\"),\n              #tf.keras.layers.Conv2D(filters = 8, kernel_size =  (4,4), strides = 2,padding = \"valid\",name =\"encoder_layer_65\"),\n              #tf.keras.layers.Conv1D(filters = 8, kernel_size = (4,4), strides = 3,padding = \"same\",activation =\"relu\"),\n\n              tf.keras.layers.Conv2DTranspose(filters = 16, kernel_size = (4,4), strides = 2,padding = \"valid\",activation =\"relu\",name =\"decoder_layer_01\"),\n              tf.keras.layers.Conv2DTranspose(filters = 32, kernel_size = (4,4), strides = 2,padding = \"valid\",activation =\"relu\",name =\"decoder_layer_02\"),\n              tf.keras.layers.Conv2DTranspose(filters = 64, kernel_size = (4,4), strides = 2,padding = \"valid\",activation =\"relu\",name =\"decoder_layer_03\"),\n              tf.keras.layers.Conv2DTranspose(filters = 128, kernel_size = (4,4), strides = 2,padding = \"valid\",activation =\"relu\",name =\"decoder_layer_04\"),\n              tf.keras.layers.Conv2DTranspose(filters = 256, kernel_size = (4,4), strides = 2,padding = \"valid\",activation =\"relu\",name =\"decoder_layer_05\"),\n              #tf.keras.layers.Conv2DTranspose(filters = 512, kernel_size = (4,4), strides = 2,padding = \"valid\",name =\"decoder_layer_06\"),\n              tf.keras.layers.Conv2DTranspose(filters = 1, kernel_size = 3, strides = 1,padding = \"valid\",activation =\"relu\",name =\"output_layer\")\n            ]\n                    )\nencoder_decoder_model.summary()","ac7c83f7":"train_df_2= train_df.iloc[:30000]\nval_df_2 = val_df.iloc[:4000]\ntrain_data_gen = Auto_Encoder_Data_Generator( train_df_2 )\nval_data_gen = Auto_Encoder_Data_Generator( val_df_2 )\nlr_reducer = tf.keras.callbacks.ReduceLROnPlateau(  patience=4,\n                                                        min_lr= 0.0000001,\n                                                        monitor='val_loss', \n                                                        factor=0.6, \n                                                        verbose=1,\n                                                        mode='auto', \n                                                       )\n\nCFG_ENCODE_DECODE[\"TRAIN_STEPS\"] = int ( train_df_2.shape[0]\/CFG_ENCODE_DECODE[\"BATCH_SIZE\"] ) + (1 if train_df_2.shape[0] % CFG_ENCODE_DECODE[\"BATCH_SIZE\"] != 0 else 0)\nCFG_ENCODE_DECODE[\"VAL_STEPS\"] = int ( val_df_2.shape[0]\/CFG_ENCODE_DECODE[\"BATCH_SIZE\"] ) + (1 if val_df_2.shape[0]% CFG_ENCODE_DECODE[\"BATCH_SIZE\"] != 0 else 0)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate= 0.01  )\n\nencoder_decoder_model.compile( optimizer= optimizer,loss=\"mse\", metrics=[tf.keras.metrics.MeanSquaredError()])\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint( f'model{1}.h5', save_best_only=True, monitor='val_loss', mode='min')\n\nencoder_decoder_model.fit(train_data_gen,\n                        #class_weight= class_weight_for_train,\n                         steps_per_epoch= CFG_ENCODE_DECODE[\"TRAIN_STEPS\"], \n                         epochs =5, \n                         validation_data= val_data_gen,\n                         validation_steps = CFG_ENCODE_DECODE[\"VAL_STEPS\"],\n                         callbacks=[ lr_reducer]\n                       )\n    \nencoder_decoder_model.save(\".\/encoder_decoder.h5\" )\n","de2ffd90":"CFG[\"BATCH_SIZE\"]","7539ff6c":"# How to enable data generator for TPU usage\nStep1- to use TPU : https:\/\/www.kaggle.com\/product-feedback\/163416\nhttps:\/\/www.kaggle.com\/manjunathns\/tensorflow-tpu-seti-efficientnet-train\/edit","f2af1890":"# With Efficient net, I am able to get 98% Accuracy.\n# Prior to this,I Tried to classify wihtout training network, just by using image net weights can get only 50% accuracy.\n# By Training all layers in the network I am able to ge 98 % accurcy.\n# Instead of Dense layer at the end,used Convolution + Dense Layer.\n","60f518bc":"*  Objetive is to identify anomally, and we have 6 channels.\n*  In that channels 1,3,5 are man made & channel 0,2,4 are from man made + outer space.\n*  Good part is, any channel 0,2,4 might have signal from Extra terestrial [ET]orall channels .\n*  So for auto Encoder instead of taking entire image  -->not working ,asimgaedimension + network size too big tohandle \n     1. We can start training Auto Encoder using channel bychannel\n     2. We could use all channels from target 0.\n\n*  This way we can reduce the complexity of model to very small. as we are not lookign at image in3 Dimention. Rather we are going look at it only in 2 Dimension\n* Same method will be applied by using any latest Pretrained models ","84a884c0":"# Lets Visualize few numpy files","0606851c":"#import neural_structured_learning as nsl\nCFG[\"BATCH_SIZE\"] =100\n\nlr_reducer = tf.keras.callbacks.ReduceLROnPlateau(  patience=1,\n                                                    min_lr= 0.000001,\n                                                    monitor='val_loss', \n                                                    factor=0.5, \n                                                    verbose=1,\n                                                    mode='auto', \n                                                   )\n\n#lr_reducer = build_lrfn()\n\nKFOLD = StratifiedKFold( n_splits= CFG[\"FOLDS\"],shuffle = True, random_state= CFG[\"RANDOM_STATE\"] )    \nkfold_history = []\nCFG['model_name'] = 'efficientnetb0-folds'\nCFG['group'] = 'K-Fold-EnetB0'\nCFG['run_name'] = 'baseline-k-fold'\nmodel_1 = get_model()\n\nfor i , (train_index,val_index) in enumerate( KFOLD.split( train_df,train_df[\"target\"] ) ):\n    \n    \n    train = train_df.iloc[train_index]\n    val = train_df.iloc[val_index]\n    \n    class_weight_for_train = {0:1, 1: (train[train[\"target\"]==0].shape[0]\/train[train[\"target\"]==1].shape[0]) }\n    \n    print ( \"class weight =={}\",class_weight_for_train )\n    print ( \"Curent fold =={}\".format( i))\n    \n    CFG[\"TRAIN_STEPS\"] = int ( train.shape[0]\/CFG[\"BATCH_SIZE\"] ) + (1 if train.shape[0]% CFG[\"BATCH_SIZE\"] != 0 else 0)\n    CFG[\"VAL_STEPS\"] = int ( val.shape[0]\/CFG[\"BATCH_SIZE\"] ) + (1 if val.shape[0]% CFG[\"BATCH_SIZE\"] != 0 else 0)\n\n\n    train_data_gen = datagenerator_rev_01( train[[\"path\",\"target\"]] ) \n    val_data_gen = datagenerator_rev_01( val[[\"path\",\"target\"]] ) \n    #test_data_gen = datagenerator_rev_01 ( test_df[[\"path\",\"target\"]])    \n\n    \n    model_1 = get_model()\n    \n    \n    model_1.build( ( None, CFG[\"IMG_LENGTH\"], CFG[\"IMG_WIDTH\"], CFG[\"CHANNELS\"]) )\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.05) #tf.keras.optimizers.Adam(learning_rate=0.1)\n    model_1.compile( optimizer= optimizer,loss=\"binary_crossentropy\", metrics=[tf.keras.metrics.AUC(curve='ROC')])\n\n\n    checkpoint = tf.keras.callbacks.ModelCheckpoint( f'model{i}.h5', save_best_only=True, monitor='val_loss', mode='min')\n\n\n    model_history = model_1.fit_generator(train_data_gen,\n                                                  class_weight= class_weight_for_train,#{0: 0.5514850705113055, 1: 5.355776587605202},\n                                                 steps_per_epoch= CFG[\"TRAIN_STEPS\"], \n                                                 epochs =5, \n                                                 validation_data= val_data_gen,\n                                                 validation_steps = CFG[\"VAL_STEPS\"],\n                                                callbacks=[ checkpoint,lr_reducer#tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1),\n                                                              #tf.keras.callbacks.ModelCheckpoint( \"..\/ZIP_MODEL\/model2.h5\", \n                                                               #                              monitor='loss', \n                                                               #                              verbose=0,\n                                                               #                              save_best_only=True, \n                                                               #                              save_weights_only=True,\n                                                               #                              mode='auto', \n                                                               #                              save_freq='epoch',\n                                                                                             #monitor='val_loss', \n                                                                                             #mode='min'\n                                                                #\n                                                                #                            )\n                                      ] )\n    \n    # Initialize W&B run\n    run = wandb.init(project='kaggle-seti', \n                     config=CFG,\n                     group=CFG['group'], \n                     job_type='train',\n                     name=CFG['run_name'])\n    \n    # Evaluate\n    loss, auc = model_1.evaluate( val_data_gen, steps = CFG[\"VAL_STEPS\"])\n    wandb.log({'Val AUC-ROC': auc})\n    \n    # Save model\n    model_name = CFG['model_name']\n    MODEL_PATH = f'models\/{model_name}'\n    os.makedirs(MODEL_PATH, exist_ok=True)\n    count_models = len(os.listdir(MODEL_PATH))\n    \n    model_1.save(f'{MODEL_PATH}\/{model_name}_{count_models}.h5')\n\n    # Get Prediction on validation set\n    #_oof_df = get_predictions(model, validloader, valid_df)\n    #oof_df = pd.concat([oof_df, _oof_df])\n    \n    run.finish()\n    \n    #kfold_history.append( model_1 )","100bd71d":"1. Building data generator"}}