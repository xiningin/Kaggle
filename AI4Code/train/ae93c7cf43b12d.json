{"cell_type":{"809b5f47":"code","62f65100":"code","99597884":"code","5cd5ba59":"code","a5c130ec":"markdown","ce2c2c7c":"markdown"},"source":{"809b5f47":"# This code snip fills holes inside large (blue) rocks\n\nimport cv2\nimport  numpy\nimport matplotlib.pyplot as plt\n\n# Loads a segmented image and isolates larger (blue) rocks\nground_truth = numpy.array(cv2.imread(\"..\/input\/images\/ground\/ground0001.png\"))\ndata_blue = ground_truth[:,:,0]\n\n# Creates a 10x10 kernel so as to close any hole smaller than 10x10 pixels inside blue rocks\nkernel = numpy.ones((10,10),numpy.uint8)\nclosing = cv2.morphologyEx(data_blue, cv2.MORPH_CLOSE, kernel)\n\nfig, ax = plt.subplots(1, 2, figsize=(16,9))\nax[0].axis('off')\nax[0].imshow(data_blue)\nax[0].set_title('FIG 1.1 - Original blue component')\nax[1].axis('off')\nax[1].imshow(closing)\nax[1].set_title('FIG 1.2 - Filling holes inside larger rocks')","62f65100":"from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy\n\n# Loads the segmented image and converts it to an array\nground_truth = Image.open(\"..\/input\/images\/ground\/ground0001.png\")\ndata = numpy.array(ground_truth)\n\n# Defines different thresholds deciding which large (blue) rocks to keep\ndata_blue = data[:,:,2]\nmin_blue_value_1 = 0\nmin_blue_value_2 = 50\nmin_blue_value_3 = 100\nmin_blue_value_4 = 150\n\nblue_1 = (data_blue > min_blue_value_1)\nblue_2 = (data_blue > min_blue_value_2)\nblue_3 = (data_blue > min_blue_value_3)\nblue_4 = (data_blue > min_blue_value_4)\n\nfig, ax = plt.subplots(1, 5, figsize=(16,9))\nax[0].axis('off')\nax[0].imshow(data_blue)\nax[0].set_title('FIG 2.1 - Original blue component')\nax[1].axis('off')\nax[1].imshow(blue_1)\nax[1].set_title('FIG 2.2 - Threshold = 0')\nax[2].axis('off')\nax[2].imshow(blue_2)\nax[2].set_title('FIG 2.3 -Threshold = 50')\nax[3].axis('off')\nax[3].imshow(blue_3)\nax[3].set_title('FIG 2.4 - Threshold = 100')\nax[4].axis('off')\nax[4].imshow(blue_4)\nax[4].set_title('FIG 2.5 - Threshold = 150')\n\n","99597884":"# This code snip only keeps the most relevant green rocks\n\nimport cv2\nimport  numpy\nimport matplotlib.pyplot as plt\nground_truth = numpy.array(cv2.imread(\"..\/input\/images\/ground\/ground0015.png\"))\ndata = numpy.array(ground_truth)\n\ndata_green = data[:,:,1]\nmin_value = 100\nh,w = data_green.shape\nfor y in range(0, h):\n    for x in range(0, w):\n        data_green[y, x] = 255 if data_green[y, x] >= min_value else 0\n        \nkernel = numpy.ones((15,15),numpy.uint8)\nkernel_circle = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(15,15))\nopening = cv2.morphologyEx(data_green, cv2.MORPH_OPEN, kernel_circle)\nfig, ax = plt.subplots(1, 2, figsize=(16,9))\nax[0].axis('off')\nax[0].imshow(ground_truth)\nax[0].set_title('Original')\nax[1].axis('off')\nax[1].imshow(opening)\nax[1].set_title('Green component without the smallest rocks')","5cd5ba59":"from PIL import Image\nimport csv\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\nimport numpy\n\n# Loads the bounding boxes file and extracts those from the first frame\nbounding_boxes_list = []\nwith open(\"..\/input\/bounding_boxes.csv\") as bounding_boxes_csv:\n    reader = csv.reader(bounding_boxes_csv, delimiter=',')\n    next(bounding_boxes_csv) # Skip the header\n    for row in reader:\n        if row[0] == '1':\n            bounding_boxes_list.append(row[1:5])\n        else:\n            break\n\n# Loads the first segmented image and displays its bounding boxes\nground_truth = numpy.array(Image.open(\"..\/input\/images\/ground\/ground0001.png\"))\nfig,ax = plt.subplots(1)\nax.axis('off')\nax.imshow(ground_truth)\nfor  bounding_box in bounding_boxes_list:\n    bounding_box = list(map(float, bounding_box)) \n    rect = patches.Rectangle((bounding_box[0]-0.5,bounding_box[1]-0.5),bounding_box[2],bounding_box[3],linewidth=2,edgecolor='w',facecolor='none')\n    ax.add_patch(rect)\nplt.show()","a5c130ec":"# Technical information\n* Render dimensions: 720x480 pixels\n* Camera FOV: 65\u00b0 \n* Camera film aperture: 36x24mm\n* Camera focal length: ~20mm\n* Camera orientation: Yaw between 0\u00b0 and 360\u00b0; Pitch between -30\u00b0 and 0\u00b0 (so as to limit the portion of images occupied by the sky); Roll between -20\u00b0 and 20\u00b0\n* Sun elevation: Random, uniform distribution between 20\u00b0 and 40\u00b0 for 75% of frames, between 40\u00b0 and 50\u00b0 for 25% of frames\n* Sun heading: Random, uniform distribution between 0\u00b0 and 360\u00b0\n","ce2c2c7c":"# Background and motivation\nAs we all know, it is always difficult to find good datasets for image analysis, even more so when the data of interest is difficult to collect. In the field of space robotics, firsthand data is very scarce and seldom freely accessible. To the knowledge of the authors, there exists no labelled dataset of planetary landscape images that could be used for any kind of machine learning approach to object detection or segmentation. Our goal is to openly provide a decent substitute for anyone who wishes to use such an approach on a lunar environment without manual labelling.\n\n# First Look at the Data\nBelow is a side-by-side view of the first image of the dataset (render0001.tif) and its corresponding segmented ground truth (ground0001.tif).\n\n![Render and its ground truth segmentation](https:\/\/i.imgur.com\/m5ja0BI.png)\n\nFrom this, we can make a number of observations that should be kept in mind when using this dataset, in decreasing order of importance:\n* **3 **(technically 4)** classes are considered: large rocks (in blue), smaller rocks (in green), the sky (in red)** (and everything else, in black)\n* **The segmented ground truth is not perfect**. In particular, there are some instances (like above) where a small rock is embedded in a larger rock. We recommend all users of the dataset to be aware of this when using the segmentation data, and we provide cleaned-up images of the segmentation under the images\/clean folder. For users wishing to clean up images differently also show an example of how to morphologically close holes inside large rocks, apply thresholds and removes smaller rocks in the code snips below.\n* **The camera used is noise-free, and no data augmentation is performed**. It is up to the user to decide on what data augmentation pipeline they see fit to use, though we recommend that particular attention be paid to **adding sensor noise to renders, especially if the goal is to perform validation test on real lunar images**. An example of such sensor effect augmentation can be found here: https:\/\/github.com\/alexacarlson\/SensorEffectAugmentation.\n* **Only rocks measuring more than 10cm are usually represented on the segmented image**. This is to avoid cluttering and focus on rocks that are relevant to detect;\n* **Colors go darker for distant rocks**. This is also to keep focus on relevant rocks, which are usually closest to the observer. However, the user is free to set their own threshold to determine whether or not a certain rock should be considered depending on its color intensity (cf. the blue rock). As can be seen from the images below, giving a minimum intensity threshold betweeen 50 and 200 is recommended to avoid noise from distant rocks.\n* **Bounding boxes are only drawn around blue rocks of intensity above 150 and dimensions above 20x20 pixels**. This is, again, to only consider rocks that are clearly visible while leaving aside those further away. \n\n\n# Testing on real lunar pictures\nYou may have noticed that real lunar pictures are also provided alongside the renders. Those were taken by the Chang'e 3 rover (image credit: CNSA), equipped with two cameras: PCAM and TCAM. Those cameras are quite different and we found it useful to keep their images separate so users can identify how well their segmentation works on each camera. We also hand-drew ground truth segmentation for these  images, arbitrarily selecting blue and green rocks (keep in mind that not all rocks are present in the ground truth when examining results such as false postitives). As an example, we tried using a modified UNet architecture on the dataset and used the trained result on some of Chang'e 3's images. Below are examples of both the potential and the limitations of the dataset. While some rocks are correctly and precisely detected, others (such as rocks with complex shapes, or those partially covered in sands) are not detected at all. We hope that this dataset put in the right hands will lead to even better results.\n\n![Test on real lunar pictures](https:\/\/i.imgur.com\/7GRUlcS.png)\n(from left to right: original picture, ground truth (hand-drawn), segmentation trained with the Artificial Lunar Landscape Dataset.)"}}