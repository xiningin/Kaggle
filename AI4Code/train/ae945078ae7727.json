{"cell_type":{"755b443d":"code","815e325f":"code","9d24bc37":"code","0c38acad":"code","1598fc10":"code","61ff9b4a":"code","4784e0c4":"code","800d7d9d":"code","af46c5ab":"code","dc7667ac":"code","f88471e4":"code","f83f9037":"code","618d3453":"code","54d4a6df":"code","6293de4e":"code","3bb6f657":"code","2c9d5922":"code","49a4f5f2":"code","07a0ae63":"code","900234a7":"code","6d672198":"code","2606fa14":"code","913417fa":"code","59188b56":"code","a2c2a357":"code","8dde9746":"code","89678f27":"code","88a0f9ba":"code","5de65c28":"code","f8254ec5":"code","78f530d1":"code","c21cfc6e":"code","ad52a5d2":"code","7859c25d":"code","b41f3338":"code","9a86f694":"code","0774246c":"markdown","5dfd0a80":"markdown","33179d04":"markdown","d5c9bc70":"markdown","3ba94691":"markdown","ffab3cbc":"markdown","e782bff0":"markdown","fdee5244":"markdown","99a99179":"markdown","5ed33a24":"markdown","2a3630a0":"markdown","af837eca":"markdown","5b8807f7":"markdown","5db883fb":"markdown","631ec993":"markdown","fd6b5e9d":"markdown","c49f76c5":"markdown","4bc7185d":"markdown","2ae6ac69":"markdown","324b4c45":"markdown","cabe61eb":"markdown","4e518651":"markdown"},"source":{"755b443d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\n# Python \u22653.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n\n\n# Scikit-Learn \u22650.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns #for better and easier plots\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","815e325f":"data = pd.read_csv(\"..\/input\/heart.csv\")","9d24bc37":"print(\"Shape of the data: \", data.shape) #printing out the shape of data, 303x14","0c38acad":"data.head()#let's use .head() and see what the data has for us.","1598fc10":"#only shows null values. \n## shows the percentage of null values\ndef missing_values_calculate(trainset): \n    nulldata = (trainset.isnull().sum() \/ len(trainset)) * 100\n    nulldata = nulldata.drop(nulldata[nulldata == 0].index).sort_values(ascending=False)\n    ratio_missing_data = pd.DataFrame({'Ratio' : nulldata})\n    return ratio_missing_data.head(30)","61ff9b4a":"missing_values_calculate(data) #calling the function to check the data","4784e0c4":"data.columns","800d7d9d":"data.dtypes #let's take a look at the types of each column","af46c5ab":"sns.set(rc={'figure.figsize':(11.7,8.27)}) #setting the size of the figure to make it easier to read.\nsns.countplot(data[\"age\"]) #age seems to have a positive correlation to the chance of heart disease.","dc7667ac":"g = sns.FacetGrid(data, col=\"sex\")\ng.map(plt.hist, \"age\")","f88471e4":"data['sex'].value_counts() #in this given data, we have a significantly assymetry in gender, way more mens.","f83f9037":"g = sns.FacetGrid(data, col=\"target\", hue=\"sex\")\ng.map(plt.scatter, \"age\", \"chol\", alpha=.7)\ng.add_legend()","618d3453":"sns.set(rc={'figure.figsize':(11.7,8.27)})\ng = sns.FacetGrid(data, col=\"target\", height=4, aspect=2)\ng.map(sns.barplot, \"age\", \"cp\")# I would like to see how chest pain is distributed in relationship to the target and age","54d4a6df":"corr = data.corr() #let's take a look at pearson's correlation\ncorr['target'].sort_values(ascending=False)","6293de4e":"#we can use the following sklearn method to create a training and testing sample that is stratified.\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) #20% for the testing sample\nfor train_index, test_index in split.split(data, data[\"sex\"]):\n    strat_train_set = data.loc[train_index]\n    strat_test_set = data.loc[test_index]","3bb6f657":"print(strat_train_set.shape, strat_test_set.shape, data.shape) # let's check the shape of the datasets created","2c9d5922":"#now, I am gonna create Xtrain, Ytrain, Xtest, Ytest\nXtrain = strat_train_set.drop('target', axis=1).copy()\nYtrain = strat_train_set['target'].copy()\nXtest = strat_test_set.drop('target', axis=1).copy()\nYtest = strat_test_set['target'].copy()","49a4f5f2":"print(Xtrain.shape, Ytrain.shape, Xtest.shape, Ytest.shape) #And again, let's check the final shape of our datasets.","07a0ae63":"#Gonna create a simple pipeline, for imputing future values, if NaN and standard scalling\n#from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n#this is a class to make selection of numerical atributes elegant and simple.   \nnum_pipeline = Pipeline([\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        ('scaler', StandardScaler())\n    ])","900234a7":"#and finally, I am gonna create this function and use it to apply different model and see the results, accuracy, recal and F1 score\nfrom sklearn.base import clone\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score\n\n#Generic function for making a classification model and accessing performance:\ndef classification_model(model, X_train, y_train):\n    #Fit the model:\n    model.fit(X_train,y_train)\n    n_cache = []\n    \n    train_predictions = model.predict(X_train)\n    precision = precision_score(y_train, train_predictions)\n    recall = recall_score(y_train, train_predictions)\n    f1 = f1_score(y_train, train_predictions)\n    \n    print(\"Precision \", precision)\n    print(\"Recall \", recall)\n    print(\"F1 score \", f1)\n\n    cv_score = cross_val_score(model, X_train, y_train, cv=5, scoring=\"accuracy\")\n        \n    print (\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(cv_score)))","6d672198":"#Xtrain = num_pipeline.fit_transform(Xtrain) you might want to turn on the pipeline to see whether it fits better to our model\n#Xtest_prepared = num_pipeline.fit_transform(Xtest)","2606fa14":"#ok, let's take a look at our fist model.\n#stochastic gradient descent SGD\n\n##Note: some hyperparameters will have a different defaut value in future versions of Scikit-Learn, such as max_iter and tol. \n##To be future-proof, we explicitly set these hyperparameters to their future default values.\n\nfrom sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\nclassification_model(sgd_clf,Xtrain,Ytrain)","913417fa":"#Now, logistic regression...\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(solver=\"liblinear\")\nclassification_model(log_reg, Xtrain, Ytrain)","59188b56":"#gonna use the decision tree without specifying any parameter, it's very likely to overfit the data, however, I am gonna use it\n##to take a look at feature importances and plot a graph using it.\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\nclassification_model(tree_reg, Xtrain, Ytrain)","a2c2a357":"# Plot feature importance\nfeature_importance = tree_reg.feature_importances_\n# make importances relative to max importance\nplt.figure(figsize=(40, 40))\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, strat_train_set.columns[sorted_idx], fontsize=30)\nplt.xlabel('Relative Importance', fontsize=30)\nplt.title('Variable Importance', fontsize=40)","8dde9746":"#Gradient Boosting for classification.\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nparams = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n          'learning_rate': 0.005, 'loss': 'deviance'}\n\nclf = GradientBoostingClassifier(**params)\nclassification_model(clf, Xtrain, Ytrain)","89678f27":"from sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(n_estimators=200, random_state=42)\nclassification_model(forest_clf, Xtrain, Ytrain)","88a0f9ba":"#let's train a rather simple neural network\nfrom sklearn.neural_network import MLPClassifier\n\nmlp_clf = MLPClassifier(random_state=42)\nclassification_model(mlp_clf, Xtrain, Ytrain)","5de65c28":"import xgboost #importing the package\n\nextra_parameters = {'learning_rate': 0.1, 'n_estimators': 200, 'max_depth': 5,\n                        \"min_child_weight\": 3, 'gamma': 0.2, 'subsample': 0.6, 'colsample_bytree': 1.0,\n                        'objective': 'binary:logistic', 'scale_pos_weight': 1, 'seed': 27\n                   }\n\nxgb_clf = xgboost.XGBClassifier(random_state=42)\nclassification_model(xgb_clf, Xtrain, Ytrain)","f8254ec5":"#Gonna implement an ensemble model using the voting classifier\nfrom sklearn.ensemble import VotingClassifier\n\n#gonna create a list to help us put together the models and give it a name.\n\nnamed_estimators = [\n    #(\"sgd_clf\", sgd_clf),\n    #(\"random_forest_clf\", forest_clf),\n    #(\"gdb_clf\", clf),\n    #(\"mlp_clf\", mlp_clf),\n    (\"logistic\", log_reg),\n    (\"xboost\", xgb_clf),\n]","78f530d1":"voting_clf = VotingClassifier(named_estimators)\n#voting_clf.fit(Xtrain, Ytrain)\nclassification_model(voting_clf, Xtrain, Ytrain)","c21cfc6e":"#importing relevant modules for this part\nimport eli5 #for purmutation importance\nfrom eli5.sklearn import PermutationImportance\nimport shap #for SHAP values\n#from pdpbox import pdp, info_plots #for partial plots","ad52a5d2":"# I am gonna try first with de Gradient Boosting model, \n##And then with the Random forest model, just to see the difference, if any\nperm = PermutationImportance(log_reg, random_state=1).fit(Xtest, Ytest) #gonna use logistic regression here, as the voting classifier seems to no work properly\neli5.show_weights(perm, feature_names = Xtest.columns.tolist())","7859c25d":"explainer = shap.TreeExplainer(forest_clf) #using the random forest as shap seems to no support the voting classifier\nshap_values = explainer.shap_values(Xtest)\n\nshap.summary_plot(shap_values[1], Xtest, plot_type=\"bar\")","b41f3338":"def heart_disease_risk_factors(model, patient):\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(patient)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value[1], shap_values[1], patient)","9a86f694":"#let's get a patience and take a look at the shap force plot.\np = 7 #helping variable, easily specify different patients.\ndata_for_prediction = Xtest.iloc[p,:].astype(float) #as I am using pipeline, I am using different datasets in order\nheart_disease_risk_factors(forest_clf, data_for_prediction)#again, I am using the random forest model. \n#unfortunately, not all the models used can be used for the shap force plot","0774246c":"**As the pearson's correlation revealed before, chest pain is the most important feature for our dataset **","5dfd0a80":"** a rather small increase upon using the voting classifier, but still, an improvement. let's keep going on...**","33179d04":"**The highest Cross Value so far, which might be improved later with grid_search, searching for better hyper parameters.**","d5c9bc70":"**That's fun, no null values**","3ba94691":"**> First step is to load the data and then take a overall look at the features**","ffab3cbc":"The pearson's correlation give us some insights;\n* as seen, chest pain is positively correlated to the target\n* thalach, highest heart rate registered, is as well positively correlated to our target\n* we have also some negatively correlated values, such as, ca, oldpeak and exang, which are, respectivelly, number of major vessels(ca), the oldpeak during exercise and the slope, depression, after exercise(as measured in a test)","e782bff0":"**I am gonna run the pipeline and prepare the data, I could(or should)have created a pipeline that receives the model automatically and fit the data, I am gonna implement it later**","fdee5244":"**Contrary to the common sense, High colesterol doesn't seems to correlate significantly with a higher risk of heart disease**\nPS: we have one outlier, for the group of men, above 500 cholesterol, men and target 1","99a99179":"**Let's now split the data using the stratified method, trying to keep the data more approximately distributed as the original**","5ed33a24":"So now, let's take a look at the null values we have in our data, I am gonna define a function that's gonna help us whenever it's needed to check for null values.","2a3630a0":"Angina appears way more in our right plot, target 0, It seems to be a good indicator of target 0\/1","af837eca":"**Some data exploration, let's have some fun**","5b8807f7":"**by looking at the plot, we have a normal distribution, after 59 years old, your chance of heart disease decreases**","5db883fb":"The following part was inspired by a post here [here](https:\/\/www.kaggle.com\/tentotheminus9\/what-causes-heart-disease-explaining-the-model)\n\nBefore reading it, I hadn't be aware of the permutation package and how to use it.","631ec993":"** by using our function, we had a CV of 74% and F1 75%, let's dig deeper**","fd6b5e9d":"**Gonna describe briefly the columns below**\n* **age** in years\n* **sex** (1 = male; 0 = female)\n* **cp**  chest pain type(angina)\n* **trestbpsm** resting blood pressure (in mm Hg on admission to the hospital)\n* **chol** serum cholestoral in mg\/dl\n* **fbs** (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* **restecgresting** electrocardiographic results\n* **thalach** maximum heart rate achieved\n* **exang** exercise induced angina (1 = yes; 0 = no)\n* **oldpeakST** depression induced by exercise relative to rest\n* **slope** the slope of the peak exercise ST segment\n* **ca** number of major vessels (0-3) colored by flourosopy\n* **thal** 3 = normal\/ 6 = fixed defect \/7 = reversable defect\n* **target** 1 or 0","c49f76c5":"**Gradient descent boosting, gonna use grid_search later in order to get parameters**","4bc7185d":"**way better, don't you think? Our CV is now 82%**","2ae6ac69":"all numerical data, as it has been prepared for this, ready for preparing and fitting to models.","324b4c45":"**just as I said, It's overfitting. And it was expected. now let's take a look at features importance and see how it looks**","cabe61eb":"**Again, picking from the same article as a mentioned before, gonna use the function below to apply to single individuals and measure the risk of heart disease**","4e518651":"Now let's see how the famous XGboost performs in this dataset. Gradient boosting on steroids."}}