{"cell_type":{"a454ea7b":"code","c91b44a5":"code","319b8a79":"code","6cda8442":"code","27c490c3":"code","c5b50e53":"code","f7072d7f":"code","24ab2aef":"code","39f814f5":"code","fa8f6168":"code","bcf0ee43":"code","d10b0f95":"code","4ff7f4e2":"code","e238c6a6":"code","fe8ddf68":"code","63c3944c":"code","c9b9b6cd":"code","1183788f":"code","312f5325":"code","2cf642dd":"code","cafe5e51":"code","8f58e366":"code","e82d3c12":"code","c0004878":"code","66e97bfe":"code","a2216a9a":"code","5eca1dad":"code","d414e0dc":"code","868951cc":"code","d7f6ef28":"code","64bc24cf":"code","03ec7cb7":"code","b309b34d":"code","6957cd9d":"code","ff7c1084":"code","1a913df1":"code","4b73ba7a":"code","38bc4453":"code","96995b52":"code","737b2581":"code","cd3b4ea6":"code","a07b609a":"code","e7613b86":"code","5d9a2e96":"code","7a55e90b":"code","636641ff":"code","029bc6e3":"code","2e98804f":"code","43372b24":"code","64123c07":"code","f8967d09":"code","12a772c5":"code","51a21a50":"code","1d310a04":"code","93e359c1":"code","938bfe4e":"code","18835d30":"code","9567ffcd":"code","c3615355":"code","05780109":"code","912d34d1":"code","6b58b574":"code","3072ab4f":"code","12b07e7b":"code","69bd5145":"code","49a87d8c":"code","edd1a417":"code","b1254475":"code","6999469f":"code","ed996b16":"markdown","e789cfcc":"markdown"},"source":{"a454ea7b":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm","c91b44a5":"!pip install catboost","319b8a79":"from catboost import CatBoostRegressor","6cda8442":"# train_df = pd.read_csv('..\/input\/beyond-analysis\/train.csv')\n# train_df.drop(columns = ['CATEGORY_1', 'CATEGORY_2'], axis=0, inplace = True)\n# train_df.head()","27c490c3":"# users = train_df['UNIQUE_IDENTIFIER'].unique()","c5b50e53":"# train_i = train_df[train_df['UNIQUE_IDENTIFIER'] == users[0]]\n# x_i = train_i.loc[:,'STATUS_CHECK': 'PRACTICE_WINNINGS_NUMBER']\n# y_i = train_i.loc[:,'Y1': 'Y2']\n# np.array(y_i)[0]","f7072d7f":"# x = {}\n# y = {}\n\n# for i in tqdm(users):\n#     train_i = train_df[train_df['UNIQUE_IDENTIFIER'] == i]\n#     x_i = train_i.loc[:,'STATUS_CHECK': 'PRACTICE_WINNINGS_NUMBER']\n#     y_i = train_i.loc[:,'Y1': 'Y2']\n#     x[i] = np.array(x_i)\n#     y[i] = np.array(y_i)[0]","24ab2aef":"# X = {}\n# Y = {}\n# for k in tqdm(x.keys()):\n#     X[str(k)] = x[k].tolist()\n#     Y[str(k)] = y[k].tolist()","39f814f5":"# import json\n# with open('.\/' + 'x_train.json', 'w') as fp:\n#     json.dump(X, fp)\n# with open('.\/' + 'y_train.json', 'w') as fp:\n#     json.dump(Y, fp)","fa8f6168":"# test_df = pd.read_csv('..\/input\/beyond-analysis\/test.csv')\n# test_df.drop(columns = ['CATEGORY_1', 'CATEGORY_2'], axis=0, inplace = True)\n# users_test = test_df['UNIQUE_IDENTIFIER'].unique()\n# test_df.head()\n\n# x = {}\n\n# for i in tqdm(users_test):\n#     test_i = test_df[test_df['UNIQUE_IDENTIFIER'] == i]\n#     x_i = test_i.loc[:,'STATUS_CHECK': 'PRACTICE_WINNINGS_NUMBER']\n#     x[i] = np.array(x_i)\n    \n# test_X = {}\n# for k in tqdm(x.keys()):\n#     test_X[str(k)] = x[k].tolist()\n\n# import json\n# with open('.\/' + 'x_test.json', 'w') as fp:\n#     json.dump(test_X, fp)","bcf0ee43":"# import glob\n# import random\n# import os\n# import numpy as np\n\n# import torch\n# from torch.utils.data import Dataset\n# from PIL import Image\n# import torchvision.transforms as transforms\n\n# import torch\n# from torch import nn\n# from matplotlib import pyplot as plt\n\n# import argparse\n# import os\n# import numpy as np\n# import math\n# import itertools\n# import sys\n\n# import torchvision.transforms as transforms\n# from torchvision.utils import save_image, make_grid\n\n# from torch.utils.data import DataLoader\n# from torch.autograd import Variable\n\n# import torch.nn as nn\n# import torch.nn.functional as F\n# import torch","d10b0f95":"pad_length = 15","4ff7f4e2":"# import json\n# f = open('..\/input\/techniche-sequential\/x_train.json')\n# X = json.load(f)\n# f = open('..\/input\/techniche-sequential\/y_train.json')\n# Y = json.load(f)","e238c6a6":"# users_train, users_test = train_test_split(users, test_size=0.2, shuffle=True, random_state=42)","fe8ddf68":"# class Dataset(Dataset):\n#     def __init__(self, users, X, Y, pad_size = 30):\n        \n#         self.users= users\n#         self.pad_size = pad_size\n#         self.X= X\n#         self.Y= Y\n        \n#     def __getitem__(self, index):\n#         user_id = self.users[index]\n#         x = np.array(self.X[str(user_id)])\n#         y = np.array(self.Y[str(user_id)])\n        \n#         pad_array = np.zeros((x.shape[0], self.pad_size - x.shape[1]))\n#         x = np.concatenate((x, pad_array), axis=1)\n        \n#         x = torch.tensor(x)\n#         y = torch.tensor(y)\n\n#         return {'x': x, 'y' : y}\n\n\n#     def __len__(self):\n#         return len(self.X)","63c3944c":"# train = Dataset(users_train, X, Y)\n# train = Dataset(users_test, X, Y)","c9b9b6cd":"# test[0]['x'].shape, test[0]['y']","1183788f":"import tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras import optimizers, Sequential, Model\nimport keras","312f5325":"train_df = pd.read_csv('..\/input\/beyond-analysis\/train.csv')\ntrain_df.drop(columns = ['CATEGORY_1', 'CATEGORY_2'], axis=0, inplace = True)\n\nusers = train_df['UNIQUE_IDENTIFIER'].unique()\nusers_train, users_test = train_test_split(users, test_size=0.2, shuffle=True, random_state=42)","2cf642dd":"import json\nf = open('..\/input\/techniche-lstm-version-3-data\/x_train.json')\nX = json.load(f)\nf = open('..\/input\/techniche-lstm-version-3-data\/y_train.json')\nY = json.load(f)","cafe5e51":"train_df[train_df['UNIQUE_IDENTIFIER']== users[0]]","8f58e366":"cols = list(train_df.columns)\n#i = 1\n#print(cols[i], \" => \", train_df[cols[i]].value_counts()[0])\ni = 2\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 3\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 4\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 5\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 6\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 7\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 8\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 9\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 10\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 11\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 12\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 13\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 14\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 15\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 16\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 17\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 18\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 19\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))\ni = 20\nprint(cols[i], \" => \", train_df[cols[i]].value_counts()[0]\/len(train_df))","e82d3c12":"def pad(users, X=X, Y=Y, pad_size=30):\n    final_x = []\n    final_y1 = []\n    final_y2 = []\n    for user_id in tqdm(users):\n        x = np.array(X[str(user_id)]).T\n        y = np.array(Y[str(user_id)]).T\n        if(x.shape[1]<=pad_size):\n            pad_array = np.zeros((x.shape[0], pad_size - x.shape[1]))\n            x = np.concatenate((x, pad_array), axis=1)\n        else:\n            x = x[:,:pad_size]\n        final_x.append(x)\n        final_y1.append(y[0])\n        final_y2.append(y[1])\n    final_x, final_y1, final_y2 = np.array(final_x), np.array(final_y1), np.array(final_y2)\n    final_x = np.transpose(final_x, [0, 2, 1])\n    return final_x, final_y1, final_y2","c0004878":"train_x, train_y1, train_y2 = pad(users_train)\ntest_x, test_y1, test_y2 = pad(users_test)\n#  Remove columns 5, 9, 10, 11, 12. 15, 16, 17, 18, ","66e97bfe":"train_x.shape, test_x.shape","a2216a9a":"a = train_x[..., :5]\nb = train_x[..., 6:9]\nc = train_x[..., 13:15]\ntrain_x_cat = np.concatenate((a, b, c), axis=-1)\ntrain_x_cat.shape","5eca1dad":"train_x_cat=np.log(1+np.power(train_x_cat,0.25))","d414e0dc":"a = test_x[..., :5]\nb = test_x[..., 6:9]\nc = test_x[..., 13:15]\ntest_x_cat = np.concatenate((a, b, c), axis=-1)\ntest_x_cat.shape","868951cc":"test_x_cat=np.log(1+np.power(test_x_cat,0.25))","d7f6ef28":"serie_size =  train_x_cat.shape[1] # 30\nn_features =  train_x_cat.shape[2] # 10\n\nlstm_model_y2 = Sequential()\nlstm_model_y2.add(L.LSTM(10, input_shape=(serie_size, n_features), return_sequences=True))\nlstm_model_y2.add(L.LSTM(10, activation='relu', return_sequences=True))\nlstm_model_y2.add(L.LSTM(6, activation='relu'))\nlstm_model_y2.add(L.Dense(10, activation='relu'))\nlstm_model_y2.add(L.Dense(10, activation='relu'))\nlstm_model_y2.add(L.Dense(1))\nlstm_model_y2.summary()\n","64bc24cf":"def lr_scheduler(epoch, lr):\n    decay_rate = 0.5\n    decay_step = 95\n    if epoch % decay_step == 0 and epoch>0:\n        return lr * decay_rate\n    return lr\n\ncallbacks = [\n    keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n]","03ec7cb7":"epochs = 200\nbatch = 128*16*4*4\n\nlr = 0.01\n\nadam = optimizers.Adam(lr)\nlstm_model_y2.compile(loss='mse', optimizer=adam)","b309b34d":"lstm_model_y2 = keras.models.load_model('..\/input\/techniche-lstm-version-3-data\/Y1_model.h5')\n# lstm_model_y2 = keras.models.load_model('.\/Y2_model.h5')\n","6957cd9d":"lstm_history = lstm_model_y2.fit(train_x_cat, train_y2,\n                              validation_data=(test_x_cat, test_y2), \n                              batch_size=batch, \n                              epochs=300,\n                              verbose=1,\n                                callbacks=callbacks)","ff7c1084":"intermediate_layer_model = Model(inputs=lstm_model_y2.input,\n                                 outputs=lstm_model_y2.layers[-2].output)","1a913df1":"intermediate_layer_model.summary()","4b73ba7a":"train_emb=intermediate_layer_model.predict(train_x_cat)","38bc4453":"test_emb=intermediate_layer_model.predict(test_x_cat)","96995b52":"cat_y1=CatBoostRegressor(depth=10, learning_rate=0.01)","737b2581":"cat_y1.fit(train_emb,train_y2,eval_set=(test_emb,test_y2))","cd3b4ea6":"lstm_model_y2.save('.\/Y2_model.h5')\n# model = load_model('my_model.h5')","a07b609a":"lstm_model_y2.predict(train_x_cat[:10]), train_y2[:10]","e7613b86":"serie_size =  train_x_cat.shape[1] # 30\nn_features =  train_x_cat.shape[2] # 10\n\nlstm_model_y1 = Sequential()\nlstm_model_y1.add(L.LSTM(10, input_shape=(serie_size, n_features), return_sequences=True))\nlstm_model_y1.add(L.LSTM(10, input_shape=(serie_size, n_features), return_sequences=True))\nlstm_model_y1.add(L.LSTM(6, activation='relu'))\nlstm_model_y1.add(L.Dense(10, kernel_initializer='glorot_normal', activation='relu'))\nlstm_model_y1.add(L.Dense(10, kernel_initializer='glorot_normal', activation='relu'))\nlstm_model_y1.add(L.Dense(1))\nlstm_model_y1.summary()","5d9a2e96":"epochs = 300\nbatch = 128*16*4*4\nlr = 0.01\n\nadam = optimizers.Adam(lr)\nlstm_model_y1.compile(loss='mse', optimizer=adam)","7a55e90b":"# lstm_model_y1 = keras.models.load_model('..\/input\/techniche-lstm-version-3-data\/Y2_model.h5')","636641ff":"import os\nos.mkdir(\"Y1\")","029bc6e3":"model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n    filepath=\".\/Y1\",\n    save_weights_only=True,\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True)","2e98804f":"def lr_scheduler2(epoch, lr):\n    decay_rate = 0.5\n    decay_step = 125\n    if epoch % decay_step == 0 and epoch>0:\n        return lr * decay_rate\n    return lr\n\ncallbacks2 = [\n    keras.callbacks.LearningRateScheduler(lr_scheduler2, verbose=1),\n    model_checkpoint_callback\n]","43372b24":"lstm_history = lstm_model_y1.fit(train_x_cat, train_y1,\n                              validation_data=(test_x_cat, test_y1), \n                              batch_size=batch, \n                              epochs=epochs,\n                              verbose=1,\n                                callbacks=callbacks2)","64123c07":"lstm_model_y1.load_weights('.\/Y1')\n# model = load_model('my_model.h5')","f8967d09":"intermediate_layer_model2 = Model(inputs=lstm_model_y1.input,\n                                 outputs=lstm_model_y1.layers[-2].output)","12a772c5":"y1_emb_train=intermediate_layer_model2.predict(train_x_cat)\ny1_emb_test=intermediate_layer_model2.predict(test_x_cat)","51a21a50":"cat_y1_=CatBoostRegressor(depth=6, learning_rate=0.05)","1d310a04":"cat_y1_.fit(y1_emb_train,train_y1,eval_set=(y1_emb_test,test_y1))","93e359c1":"lstm_model_y1.predict(train_x_cat[:2]), train_y1[:2]","938bfe4e":"import json\nf = open('..\/input\/techniche-lstm-version-3-data\/x_test.json')\nX_test = json.load(f)","18835d30":"def test_pad(users, X=X_test, pad_size=30):\n    final_x = []\n    for user_id in tqdm(users):\n        x = np.array(X[str(user_id)]).T\n        pad_array = np.zeros((x.shape[0], pad_size - x.shape[1]))\n        x = np.concatenate((x, pad_array), axis=1)\n        final_x.append(x)\n    final_x = np.array(final_x)\n    final_x = np.transpose(final_x, [0, 2, 1])\n    return final_x","9567ffcd":"test_df = pd.read_csv('..\/input\/beyond-analysis\/test.csv')\ntest_df.drop(columns = ['CATEGORY_1', 'CATEGORY_2'], axis=0, inplace = True)\n\nusers_sub = test_df['UNIQUE_IDENTIFIER'].unique()","c3615355":"sub_x = test_pad(users_sub)\n#  Remove columns 5, 9, 10, 11, 12. 15, 16, 17, 18, ","05780109":"sub_x.shape","912d34d1":"a = sub_x[..., :5]\nb = sub_x[..., 6:9]\nc = sub_x[..., 13:15]\nsub_x_cat = np.concatenate((a, b, c), axis=-1)\nsub_x_cat.shape","6b58b574":"pred_y1 = lstm_model_y1.predict(sub_x_cat)\nprint(pred_y1)\npred_y1.shape","3072ab4f":"y2_emb = intermediate_layer_model.predict(sub_x_cat)\n# print(pred_y2)\ny2_emb.shape","12b07e7b":"pred_y2=cat_y1.predict(y2_emb)","69bd5145":"pred_y2.shape","49a87d8c":"sub_df=pd.read_csv(\"..\/input\/cat-y1-pred\/cat(1)(3).csv\")","edd1a417":"sub_df[\"Y2\"]=pred_y2\nsub_df[\"Y1\"]=pred_y1","b1254475":"sub_df.to_csv(\"lstm-lstm-cat.csv\",index=None)","6999469f":"sub = pd.DataFrame({'UNIQUE_IDENTIFIER': users_sub.astype(int), \n                   'Y1': pred_y1[:, 0], 'Y2': pred_y2[:, 0]})\n\nsub.to_csv('sub.csv', index=False)","ed996b16":"# Y1","e789cfcc":"# Prediction"}}