{"cell_type":{"c1fab077":"code","2b1e2d12":"code","12bb8c4e":"code","70b2230b":"code","808226a6":"code","14ba8625":"code","f7a3343d":"code","f35aa425":"code","f0704483":"code","62b768c3":"code","5cf5ca03":"code","78413fe8":"code","d8f44426":"code","dc896691":"code","450727a1":"code","e5bb6325":"code","251f8ddb":"code","3d99aad1":"code","14d7bbe7":"code","766353e8":"code","43d00362":"code","fc069fd1":"code","8626d6df":"code","c0291e27":"code","5208c5ce":"code","7eb107e2":"code","64f0fa51":"code","d55d00d9":"code","73ba90e1":"code","8e04db26":"code","f891e4f8":"code","786f1be0":"code","28e2ebf9":"code","13f271d0":"code","7a5582fe":"code","a04a951d":"code","9e3fc313":"code","c7594565":"code","dea44260":"code","04a358e8":"code","5b65c298":"code","8fdc75fa":"code","97c4f82b":"code","46fec16d":"code","ad6cd55b":"code","7d29f0f3":"code","308113e4":"code","fb95f64c":"code","1734b088":"markdown","5a5b3acd":"markdown","106ca7d0":"markdown","d85f7122":"markdown","4dd6e0ab":"markdown","42dfd1c4":"markdown","92642bf2":"markdown","7647fe02":"markdown","9f2cff9c":"markdown","d1e99a95":"markdown","ed139b48":"markdown","fd49c68f":"markdown","9288b673":"markdown","fc5cccf6":"markdown","6c748952":"markdown","c25f9e2f":"markdown","b43e9e3e":"markdown","11a4b1a3":"markdown","28811c82":"markdown","b12e57c0":"markdown","f25a3f8d":"markdown"},"source":{"c1fab077":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2b1e2d12":"## import libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom pylab import rcParams\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nrcParams['figure.figsize'] = 12,7\nsns.color_palette(\"hls\", 8)","12bb8c4e":"df = pd.read_csv(\"\/kaggle\/input\/housedata\/data.csv\")  # Reading dataset","70b2230b":"df.head() # looking at the first 5 rows ","808226a6":"df.shape # looking at the shape. we have 4600 rows and 18 columns.","14ba8625":"df.info() # looking at information","f7a3343d":"df.describe()  # looking at statistical info","f35aa425":"import datetime as dt\n\ndf[\"date\"]= pd.to_datetime(df[\"date\"])\n\ndf[\"day\"] = df[\"date\"].dt.day\ndf[\"month\"] = df[\"date\"].dt.month\ndf[\"year\"] = df[\"date\"].dt.year\n\ndf = df.drop(columns=[\"date\"])\n\ndf","f0704483":"df[\"price\"]= df[\"price\"].astype(\"int64\")\ndf[\"bedrooms\"]= df[\"bedrooms\"].astype(\"int64\")\ndf[\"bathrooms\"]= df[\"bathrooms\"].astype(\"int64\")\ndf[\"floors\"]= df[\"floors\"].astype(\"int64\")\n\ndf","62b768c3":"df.isnull().sum().sort_values(ascending=False) # checking for missing observations","5cf5ca03":"price_des = df['price'].describe()\n\nplt.figure(figsize=(16,6))\nsns.displot(df['price'], kde = True)\nplt.axvline(price_des['mean'], linestyle = \"--\", color = \"red\")\nplt.title('Price distribution')\nrcParams['figure.figsize'] = 20,10","78413fe8":"a = df[[\"price\"]].mean()\n\na","d8f44426":"len(df)","dc896691":"df[\"price\"].max()","450727a1":"df[\"price\"].min()","e5bb6325":"len(df[df[\"price\"]<20000])","251f8ddb":"len(df[df[\"price\"]>1500000])","3d99aad1":"df = df[df[\"price\"]>=20000]\ndf.reset_index(inplace=True)\n\ndf = df[df[\"price\"]<=1500000]\ndf.reset_index(inplace=True)","14d7bbe7":"len(df)","766353e8":"price_des = df['price'].describe()\n\nplt.figure(figsize=(16,6))\nsns.displot(df['price'], kde = True)\nplt.axvline(price_des['mean'], linestyle = \"--\", color = \"red\")\nplt.title('Price distribution')\nrcParams['figure.figsize'] = 20,10","43d00362":"plt.hist(df[\"sqft_basement\"])","fc069fd1":"plt.hist(df[\"yr_built\"])","8626d6df":"plt.hist(df[\"yr_renovated\"])","c0291e27":"df[\"basement\"] = [\"No\" if i == 0 else \"Yes\" for i in df[\"sqft_basement\"]]\n\ndf[\"situation\"] = [\"Former\" if i <= 1990 else \"New\" for i in df[\"yr_built\"]]\n\ndf[\"renewal_status\"] = [\"Not_renewed\" if i == 0 else \"Renewed\" for i in df[\"yr_renovated\"]]\n\ndf","5208c5ce":"#!pip install plotly","7eb107e2":"import plotly.express as px\nimport plotly.graph_objects as go","64f0fa51":"keys = df['basement'].value_counts().keys().to_list()\nvalues = df['basement'].value_counts().to_list()\nfig = go.Figure(go.Pie(labels=keys,\n                      values= values, \n                      hole = 0.5))\nfig.show()","d55d00d9":"yr_built_des = df['yr_built'].describe()\n\nplt.figure(figsize=(16,6))\nsns.displot(df['yr_built'], kde = True)\nplt.axvline(yr_built_des['mean'], linestyle = \"--\", color = \"red\")\nplt.title('yr_built distribution')\nrcParams['figure.figsize'] = 20,10","73ba90e1":"keys = df['situation'].value_counts().keys().to_list()\nvalues = df['situation'].value_counts().to_list()\nfig = go.Figure(go.Pie(labels=keys,\n                      values= values, \n                      hole = 0.5))\nfig.show()","8e04db26":"keys = df['renewal_status'].value_counts().keys().to_list()\nvalues = df['renewal_status'].value_counts().to_list()\nfig = go.Figure(go.Pie(labels=keys,\n                      values= values, \n                      hole = 0.5))\nfig.show()","f891e4f8":"# plotting numerical features \nnum_variable = (df.dtypes==float) | (df.dtypes==\"int64\")\nnum_variable = df.columns[num_variable].tolist()\n\ndef plot_hist(train_df, variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(train_df[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()\n    \nfor i in num_variable:\n    plot_hist(df,i)","786f1be0":"##plotting the categorical features\ncat_variable = df.dtypes==object\ncat_variable = df.columns[cat_variable].tolist()\n\n# Count of products per keys\ndef bar_plot(data,feature):\n    print(f'There are {len(set(data[feature]))} unique {feature}')\n    print('\\n')\n    sns.countplot(x = feature,\n              data = data,\n              order = data[feature].value_counts(ascending=False)[0:20].index)\n    plt.xticks(rotation=90)\n    print(f'Count of {feature}')\n    print('\\n')\n    print(data[feature].value_counts(ascending=False)[0:20])\n    plt.show()\n    print('\\n')\n    \nfor i in cat_variable:\n    bar_plot(df,i)","28e2ebf9":"keys = df['bedrooms'].value_counts().keys().to_list()\nvalues = df['bedrooms'].value_counts().to_list()\nfig = go.Figure(go.Pie(labels=keys,\n                      values= values, \n                      hole = 0.5))\nfig.show()","13f271d0":"cat_variable = df.dtypes==object\ncat_variable = df.columns[cat_variable].tolist()","7a5582fe":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ndf[cat_variable] = df[cat_variable].apply(lambda col: lb.fit_transform(col.astype(str)))","a04a951d":"df.head()","9e3fc313":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","c7594565":"df =df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\ndf.dropna(inplace=True)","dea44260":"df","04a358e8":"df.info()","5b65c298":"plt.figure(figsize=(20,12))\nsns.heatmap(df.corr(),annot=True)","8fdc75fa":"#saleprice correlation matrix\nk = 23 #number of variables for heatmap\nplt.figure(figsize=(16,12))\ncorrmat = df.corr()\n# picking the top 15 correlated features\ncols = corrmat.nlargest(k, 'price')['price'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","97c4f82b":"from sklearn.metrics import mean_squared_error","46fec16d":"X = df.drop(columns=[\"price\",\"view\",\"waterfront\",\"country\", \"year\", \"level_0\", \"index\", \"situation\",\"basement\",\"renewal_status\"])\ny = df[\"price\"]\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2)","ad6cd55b":"def models():\n    #use logistic regression\n    from sklearn.linear_model import LinearRegression\n    lr = LinearRegression()\n    lr.fit(X_train,y_train)\n\n    #use Kneighbors\n    from sklearn.neighbors import KNeighborsRegressor\n    knn = KNeighborsRegressor()\n    knn.fit(X_train,y_train)\n\n    #use Support vector classifier (linear kernel)\n    from sklearn.svm import SVR\n    svc = SVR(kernel='linear')\n    svc.fit(X_train,y_train)\n\n    #use decision tree\n    from sklearn.tree import DecisionTreeRegressor\n    tree=DecisionTreeRegressor()\n    tree.fit(X_train,y_train)\n\n    #use Random Forest\n    from sklearn.ensemble import RandomForestRegressor\n    forest = RandomForestRegressor()\n    forest.fit(X_train,y_train)\n\n    #use GradientBoosting\n    from sklearn.ensemble import GradientBoostingRegressor\n    gb = GradientBoostingRegressor()\n    gb.fit(X_train,y_train)\n    \n    \n    from xgboost import XGBRegressor\n    xgb = XGBRegressor()\n    xgb.fit(X_train,y_train)\n\n    from lightgbm import LGBMRegressor\n    lgbm = LGBMRegressor()\n    lgbm.fit(X_train,y_train)\n    #Print the accuracy for ech model\n    print(\"Results\")\n    print('[0] Logistic Regression Test Error: ',np.sqrt(mean_squared_error(y_test,lr.predict(X_test))))\n    print('[1] K neighbors Regression Test Error: ',np.sqrt(mean_squared_error(y_test,knn.predict(X_test))))\n    print('[2] SVR linear Regression Test Error: ',np.sqrt(mean_squared_error(y_test,svc.predict(X_test))))\n    print('[3] Decision Tree Regression Test Error: ',np.sqrt(mean_squared_error(y_test,tree.predict(X_test))))\n    print('[4] Random Forest Regression Test Error: ',np.sqrt(mean_squared_error(y_test,forest.predict(X_test))))\n    print('[5] Gradient Boosting Regression Test Error: ',np.sqrt(mean_squared_error(y_test,gb.predict(X_test))))\n    print('[6] XGBoost Regression Test Error: ',np.sqrt(mean_squared_error(y_test,xgb.predict(X_test))))\n    print('[7] LightGBM Regression Test Error: ',np.sqrt(mean_squared_error(y_test,lgbm.predict(X_test)))) \n\n    return lr,knn,svc,tree,forest,gb,xgb,lgbm","7d29f0f3":"lr,knn,svc,tree,forest,gb,xgb,lgbm = models()","308113e4":"Importance = pd.DataFrame({\"Importance\": lgbm.feature_importances_*100},\n                         index = X_train.columns)","fb95f64c":"Importance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\")\n\nplt.xlabel(\"Variable Severity Levels\")","1734b088":"45.1% of the homes in our dataset have 3 bedrooms. It is also possible to have 4 bedrooms, but houses with 5, 6, 8 bedrooms are rare.","5a5b3acd":"Let's convert the variables that are float in our dataset to int.","106ca7d0":"# Data Visualization","d85f7122":"# Load the data","4dd6e0ab":"# Getting Rid of Outliers","42dfd1c4":"The price has a right-skewed distribution. To bring this closer to normal, we can subtract the data above 1000000 from our dataset.","92642bf2":"As can be seen from the graph, 3053 houses are old and 1381 houses are new. Although the construction of houses has increased in recent years, the old houses are much more.","7647fe02":"Now let's visualize our numeric and categorical variables collectively.","9f2cff9c":"# Feature Importance","d1e99a95":"40.5% of the houses in our dataset are renovated, 59.5% are not. We understand that a high percentage of the houses are both old and not renovated.","ed139b48":"When we examine this histosgaram graph, I decide to create new categories. First, the category created as Basement gives information about whether there is a basement area or not. Secondly, the situation gives information about whether the houses are old or new in the range determined according to the year of construction. It was determined as \"new\" after \"old\" before 1990. Third, renewal_status informs about the renewal status of the houses as \"renewed\" or \"not renewed\".","fd49c68f":"If you like it plase vote!","9288b673":"# Modelling\n\nLet's start the model!","fc5cccf6":"# Converting Categorical Variables to Numeric Variables","6c748952":"The model works best with LightGBM.","c25f9e2f":"There are no missing observations in our data set. ","b43e9e3e":"Using datatime, we create new variables for our date variable as day, month and year. Then we can permanently remove our date variable from our dataset.","11a4b1a3":"We see that the houses were built in 1970 on average and after 1975 they increased with an interval of 1 year. The year most houses were built was in 2005.","28811c82":" In our dataset, we see that the lowest value of the price is \"0\" and the highest value is \"26590000\". I think $0 will not be a house and a value like 26590000 will not appeal to the potential buyer. So I decided to set a range to be greater than 20000 and less than 1500000.","b12e57c0":"# Exploratory Data Analysis","f25a3f8d":"39.6% of the houses have a basement area, while 60.04% do not."}}