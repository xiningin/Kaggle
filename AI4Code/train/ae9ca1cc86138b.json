{"cell_type":{"9576afb2":"code","bbe48d11":"code","a206c6b7":"code","6d4291a5":"code","7d6f85ef":"code","3ea98070":"code","d9b9e501":"code","e81a892f":"code","d3446250":"code","eefc34b9":"code","b8a5b61d":"code","076ab1b5":"code","7bfd83d5":"code","15924ffe":"code","01c387c3":"code","3853eae6":"code","e863b387":"code","38b19f48":"code","dc3ddeba":"code","42f4cf3e":"code","10d9b923":"code","df452fcb":"code","f1493a23":"code","66c6cb67":"code","a5ad1af4":"code","686ccdbb":"code","e5b9df31":"code","54fcac27":"code","24f18ce6":"code","576c1913":"code","4a778ba3":"code","5d1d2026":"code","da155b3e":"code","f6ca99a6":"code","dce40063":"code","306e26a2":"code","c14ebbe1":"code","6d3a3ac1":"code","ad4d6be2":"code","976528e2":"code","184cfd73":"code","ecc53757":"code","65d77941":"markdown","7c6cef97":"markdown","8798f92d":"markdown","555f8b66":"markdown","584b11db":"markdown","619fd29e":"markdown","fcb7386b":"markdown","7606908a":"markdown","e4645c3b":"markdown","e43ce0c0":"markdown","88d790f1":"markdown","d1e0cd93":"markdown","faa17da2":"markdown","0dc9db90":"markdown","64b2cc48":"markdown","4d0090e8":"markdown","f595113e":"markdown","fd4a6e08":"markdown","cb37e632":"markdown","cd72d6b2":"markdown","3db71926":"markdown"},"source":{"9576afb2":"# IMPORTING LIBRARIES\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew, norm\nfrom sklearn.neighbors import KNeighborsRegressor\n%matplotlib inline\nfrom scipy import stats\n\n#MODELING import\nimport shap\nimport xgboost as xgb\nfrom catboost import Pool\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor\nfrom mlxtend.regressor import StackingRegressor\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","bbe48d11":"# IMPORTING DATA\n\nhouse_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndata_w = house_data.copy()\ndata_w.columns = data_w.columns.str.replace(' ', '') # Replacing the white spaces in columns' names if any\ndata_w.info()","a206c6b7":"data_w.head()","6d4291a5":"# Getting the main parameters of the Normal Ditribution ()\n(mu, sigma) = norm.fit(data_w['SalePrice'])\n\nplt.figure(figsize = (12,6))\nsns.distplot(data_w['SalePrice'], kde = True, hist=True, fit = norm)\nplt.title('SalePrice distribution vs Normal Distribution', fontsize = 17)\nplt.xlabel(\"House's sale Price in $\", fontsize = 14)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.show()","7d6f85ef":"# Skew and kurt\n\n\nshap_t,shap_p = stats.shapiro(data_w['SalePrice'])\n\nprint(\"Skewness: %f\" % abs(data_w['SalePrice']).skew())\nprint(\"Kurtosis: %f\" % abs(data_w['SalePrice']).kurt())\nprint(\"Shapiro_Test: %f\" % shap_t)\nprint(\"Shapiro_Test: %f\" % shap_p)","3ea98070":"# Correlation Matrix\n\nf, ax = plt.subplots(figsize=(35, 30))\nmat = data_w.corr('pearson')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","d9b9e501":"# OverallQuall - SalePrice [Pearson = 0.79]\n\nfigure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.stripplot(data=data_w, x = 'OverallQual', y='SalePrice', ax = ax[0])\nsns.violinplot(data=data_w, x = 'OverallQual', y='SalePrice', ax = ax[1])\nsns.boxplot(data=data_w, x = 'OverallQual', y='SalePrice', ax = ax[2])\nplt.show()","e81a892f":"# TotRmsAbvGrd - SalePrice [Pearson = 0.50]\n\nfigure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.stripplot(data=data_w, x = 'TotRmsAbvGrd', y='SalePrice', ax = ax[0])\nsns.violinplot(data=data_w, x = 'TotRmsAbvGrd', y='SalePrice', ax = ax[1])\nsns.boxplot(data=data_w, x = 'TotRmsAbvGrd', y='SalePrice', ax = ax[2])\nplt.show()","d3446250":"# GrLivArea vs SalePrice [corr = 0.71]\n\nPearson_GrLiv = 0.71\nplt.figure(figsize = (12,6))\nsns.regplot(data=data_w, x = 'GrLivArea', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('GrLivArea vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_GrLiv)], loc = 'best')\nplt.show()","eefc34b9":"Pearson_TBSF = 0.63\nplt.figure(figsize = (12,6))\nsns.regplot(data=data_w, x = 'TotalBsmtSF', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('TotalBsmtSF vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_TBSF)], loc = 'best')\nplt.show()","b8a5b61d":"# YearBuilt vs SalePrice\n\nPearson_YrBlt = 0.56\nplt.figure(figsize = (12,6))\nsns.regplot(data=data_w, x = 'YearBuilt', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('YearBuilt vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_YrBlt)], loc = 'best')\nplt.show()","076ab1b5":"# Median of Sale Price by Year\n\nplt.figure(figsize = (10,5))\nsns.barplot(x='YrSold', y=\"SalePrice\", data = data_w, estimator = np.median)\nplt.title('Median of Sale Price by Year', fontsize = 13)\nplt.xlabel('Selling Year', fontsize = 12)\nplt.ylabel('Median of Price in $', fontsize = 12)\nplt.show()","7bfd83d5":"# Checking outliers at 1%,5%,25%,50%,75%,90%,95%,99% and above\ndata_w.describe(percentiles=[.01,.05,.25,.5,.75,.90,.95,.98])","15924ffe":"# Cut the window in 2 parts\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)})\n# Add a graph in each part\nsns.boxplot(data_w[\"LotArea\"], ax=ax_box)\nsns.distplot(data_w[\"LotArea\"], ax=ax_hist)\n# Remove x axis name for the boxplot\nax_box.set(xlabel='')","01c387c3":"# Removing Outliers, we can clearly see that there is outliers at the end  , so let's remove outliers\n\noutliers_range_upper = data_w['LotArea'].quantile(0.98)\n# Removing values beyond 98% for LotFrontage\ndata_w = data_w[data_w[\"LotArea\"] < outliers_range_upper]\noutliers_range_lower = data_w['LotArea'].quantile(0.01)\n# Removing values under 1% for LotFrontage\ndata_w = data_w[data_w[\"LotArea\"] > outliers_range_lower]\n\n#Again Plot the chart\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)})\n# Add a graph in each part\nsns.boxplot(data_w[\"LotArea\"], ax=ax_box)\nsns.distplot(data_w[\"LotArea\"], ax=ax_hist)\n# Remove x axis name for the boxplot\nax_box.set(xlabel='')\n","3853eae6":"# Checking outliers at 1%,5%,25%,50%,75%,90%,95%,99% and above\ndata_w.describe(percentiles=[.01,.05,.25,.5,.75,.90,.95,.98])","e863b387":"data_w.info()","38b19f48":"# Separating Target and Features\n\ntarget = data_w['SalePrice']\ntest_id = test['Id']\ntest = test.drop(['Id'],axis = 1)\ndata_w2 = data_w.drop(['SalePrice'], axis = 1)\n\n\n# Concatenating train & test set\n\ntrain_test = pd.concat([data_w2,test], axis=0, sort=False)","dc3ddeba":"# Looking at NaN % within the data\n\nnan = pd.DataFrame(train_test.isna().sum(), columns = ['NaN_sum'])\nnan['feat'] = nan.index\nnan['Perc(%)'] = (nan['NaN_sum']\/1460)*100\nnan = nan[nan['NaN_sum'] > 0]\nnan = nan.sort_values(by = ['NaN_sum'])\nnan['Usability'] = np.where(nan['Perc(%)'] > 20, 'Discard', 'Keep')\nnan","42f4cf3e":"# Plotting Nan\n\nplt.figure(figsize = (15,5))\nsns.barplot(x = nan['feat'], y = nan['Perc(%)'])\nplt.xticks(rotation=45)\nplt.title('Features containing Nan')\nplt.xlabel('Features')\nplt.ylabel('% of Missing Data')\nplt.show()","10d9b923":"# Converting non-numeric predictors stored as numbers into string\n\ntrain_test['MSSubClass'] = train_test['MSSubClass'].apply(str)\ntrain_test['YrSold'] = train_test['YrSold'].apply(str)\ntrain_test['MoSold'] = train_test['MoSold'].apply(str)\n\n# Filling Categorical NaN (That we know how to fill due to the description file )\n\ntrain_test['Functional'] = train_test['Functional'].fillna('Typ')\ntrain_test['Electrical'] = train_test['Electrical'].fillna(\"SBrkr\")\ntrain_test['KitchenQual'] = train_test['KitchenQual'].fillna(\"TA\")\ntrain_test['Exterior1st'] = train_test['Exterior1st'].fillna(train_test['Exterior1st'].mode()[0])\ntrain_test['Exterior2nd'] = train_test['Exterior2nd'].fillna(train_test['Exterior2nd'].mode()[0])\ntrain_test['SaleType'] = train_test['SaleType'].fillna(train_test['SaleType'].mode()[0])\ntrain_test[\"PoolQC\"] = train_test[\"PoolQC\"].fillna(\"None\")\ntrain_test[\"Alley\"] = train_test[\"Alley\"].fillna(\"None\")\ntrain_test['FireplaceQu'] = train_test['FireplaceQu'].fillna(\"None\")\ntrain_test['Fence'] = train_test['Fence'].fillna(\"None\")\ntrain_test['MiscFeature'] = train_test['MiscFeature'].fillna(\"None\")\n\nfor col in ('GarageArea', 'GarageCars'):\n    train_test[col] = train_test[col].fillna(0)\n        \nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    train_test[col] = train_test[col].fillna('None')\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    train_test[col] = train_test[col].fillna('None')\n    \n    # Checking the features with NaN remained out\n\nfor col in train_test:\n    if train_test[col].isna().sum() > 0:\n        print(train_test[col][0])","df452fcb":"# Removing the useless variables\n\nuseless = ['GarageYrBlt','YearRemodAdd'] \ntrain_test = train_test.drop(useless, axis = 1)\n\n# Imputing with KnnRegressor (we can also use different Imputers)\n\ndef impute_knn(df):\n    ttn = train_test.select_dtypes(include=[np.number])\n    ttc = train_test.select_dtypes(exclude=[np.number])\n\n    cols_nan = ttn.columns[ttn.isna().any()].tolist()         # columns w\/ nan \n    cols_no_nan = ttn.columns.difference(cols_nan).values     # columns w\/n nan\n\n    for col in cols_nan:\n        imp_test = ttn[ttn[col].isna()]   # indicies which have missing data will become our test set\n        imp_train = ttn.dropna()          # all indicies which which have no missing data \n        model = KNeighborsRegressor(n_neighbors=5)  # KNR Unsupervised Approach\n        knr = model.fit(imp_train[cols_no_nan], imp_train[col])\n        ttn.loc[ttn[col].isna(), col] = knr.predict(imp_test[cols_no_nan])\n    \n    return pd.concat([ttn,ttc],axis=1)\n\ntrain_test = impute_knn(train_test)\n\n\nobjects = []\nfor i in train_test.columns:\n    if train_test[i].dtype == object:\n        objects.append(i)\ntrain_test.update(train_test[objects].fillna('None'))\n\n# # Checking NaN presence\n\nfor col in train_test:\n    if train_test[col].isna().sum() > 0:\n        print(train_test[col][0])","f1493a23":"train_test[\"SqFtPerRoom\"] = train_test[\"GrLivArea\"] \/ (train_test[\"TotRmsAbvGrd\"] +\n                                                       train_test[\"FullBath\"] +\n                                                       train_test[\"HalfBath\"] +\n                                                       train_test[\"KitchenAbvGr\"])\n\ntrain_test['Total_Home_Quality'] = train_test['OverallQual'] + train_test['OverallCond']\n\ntrain_test['Total_Bathrooms'] = (train_test['FullBath'] + (0.5 * train_test['HalfBath']) +\n                               train_test['BsmtFullBath'] + (0.5 * train_test['BsmtHalfBath']))\n\ntrain_test[\"HighQualSF\"] = train_test[\"1stFlrSF\"] + train_test[\"2ndFlrSF\"]\n\n# Converting non-numeric predictors stored as numbers into string\n\ntrain_test['MSSubClass'] = train_test['MSSubClass'].apply(str)\ntrain_test['YrSold'] = train_test['YrSold'].apply(str)\ntrain_test['MoSold'] = train_test['MoSold'].apply(str)\n\n# Creating dummy variables from categorical features\n\ntrain_test_dummy = pd.get_dummies(train_test)\n\n# Fetch all numeric features\n\n\nnumeric_features = train_test_dummy.dtypes[train_test_dummy.dtypes != object].index\nskewed_features = train_test_dummy[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_features[skewed_features > 0.5]\nskew_index = high_skew.index\n\n# Normalize skewed features using log_transformation\n    \nfor i in skew_index:\n    train_test_dummy[i] = np.log1p(train_test_dummy[i])","66c6cb67":"# SalePrice before transformation\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\" qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(target, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\n\nsns.distplot(target, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","a5ad1af4":"# SalePrice after transformation\n\ntarget_log = np.log1p(target)\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\"qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(target_log, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\nsns.distplot(target_log, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","686ccdbb":"# Train-Test separation\n\ntrain = train_test_dummy[0:1413] #after outlier we have only 1413 record in train data\ntest = train_test_dummy[1413:]\ntest['Id'] = test_id\n\n# Creation of the RMSE metric:\n    \ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, train, target_log, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","e5b9df31":"# 10 Fold Cross validation\n\nkf = KFold(n_splits=10, random_state=42, shuffle=True)\n\ncv_scores = []\ncv_std = []\n\nbaseline_models = ['Linear_Reg.','Bayesian_Ridge_Reg.','LGBM_Reg.','SVR',\n                   'Dec_Tree_Reg.','Random_Forest_Reg.', 'XGB_Reg.',\n                   'Grad_Boost_Reg.','Cat_Boost_Reg.','Stacked_Reg.']\n\n# Linear Regression\n\nlreg = LinearRegression()\nscore_lreg = cv_rmse(lreg)\ncv_scores.append(score_lreg.mean())\ncv_std.append(score_lreg.std())\n\n# Bayesian Ridge Regression\n\nbrr = BayesianRidge(compute_score=True)\nscore_brr = cv_rmse(brr)\ncv_scores.append(score_brr.mean())\ncv_std.append(score_brr.std())\n\n# Light Gradient Boost Regressor\n\nl_gbm = LGBMRegressor(objective='regression')\nscore_l_gbm = cv_rmse(l_gbm)\ncv_scores.append(score_l_gbm.mean())\ncv_std.append(score_l_gbm.std())\n\n# Support Vector Regression\n\nsvr = SVR()\nscore_svr = cv_rmse(svr)\ncv_scores.append(score_svr.mean())\ncv_std.append(score_svr.std())\n\n# Decision Tree Regressor\n\ndtr = DecisionTreeRegressor()\nscore_dtr = cv_rmse(dtr)\ncv_scores.append(score_dtr.mean())\ncv_std.append(score_dtr.std())\n\n# Random Forest Regressor\n\nrfr = RandomForestRegressor()\nscore_rfr = cv_rmse(rfr)\ncv_scores.append(score_rfr.mean())\ncv_std.append(score_rfr.std())\n\n# XGB Regressor\n\nxgb = xgb.XGBRegressor()\nscore_xgb = cv_rmse(xgb)\ncv_scores.append(score_xgb.mean())\ncv_std.append(score_xgb.std())\n\n# Gradient Boost Regressor\n\ngbr = GradientBoostingRegressor()\nscore_gbr = cv_rmse(gbr)\ncv_scores.append(score_gbr.mean())\ncv_std.append(score_gbr.std())\n\n# Cat Boost Regressor\n\ncatb = CatBoostRegressor()\nscore_catb = cv_rmse(catb)\ncv_scores.append(score_catb.mean())\ncv_std.append(score_catb.std())\n\n# Stacked Regressor\n\nstack_gen = StackingRegressor(regressors=(CatBoostRegressor(),\n                                          LinearRegression(),\n                                          BayesianRidge(),\n                                          GradientBoostingRegressor()),\n                              meta_regressor = CatBoostRegressor(),\n                              use_features_in_secondary = True)\n\nscore_stack_gen = cv_rmse(stack_gen)\ncv_scores.append(score_stack_gen.mean())\ncv_std.append(score_stack_gen.std())\n\nfinal_cv_score = pd.DataFrame(baseline_models, columns = ['Regressors'])\nfinal_cv_score['RMSE_mean'] = cv_scores\nfinal_cv_score['RMSE_std'] = cv_std","54fcac27":"final_cv_score","24f18ce6":"plt.figure(figsize = (12,8))\nsns.barplot(final_cv_score['Regressors'],final_cv_score['RMSE_mean'])\nplt.xlabel('Regressors', fontsize = 12)\nplt.ylabel('CV_Mean_RMSE', fontsize = 12)\nplt.xticks(rotation=45)\nplt.show()","576c1913":"# Train-Test split the data\n#to Display you must run this command in powershell\n# jupyter nbextension enable --py widgetsnbextension --sys-prefix\n\nX_train,X_val,y_train,y_val = train_test_split(train,target_log,test_size = 0.1,random_state=42)\n\n# Cat Boost Regressor\n\ncat = CatBoostRegressor()\ncat_model = cat.fit(X_train,y_train,\n                     eval_set = (X_val,y_val),\n                     plot=True,\n                     verbose = 0)","4a778ba3":"cat_pred = cat_model.predict(X_val)\ncat_score = rmse(y_val, cat_pred)\ncat_score","5d1d2026":"# Features' importance of our model\n\nfeat_imp = cat_model.get_feature_importance(prettified=True)\nfeat_imp","da155b3e":"# Plotting top 20 features' importance\n\nplt.figure(figsize = (12,8))\nsns.barplot(feat_imp['Importances'][:20],feat_imp['Feature Id'][:20], orient = 'h')\nplt.show()","f6ca99a6":"# Feature importance Interactive Plot \n\ntrain_pool = Pool(X_train)\nval_pool = Pool(X_val)\n\nexplainer = shap.TreeExplainer(cat_model) # insert your model\nshap_values = explainer.shap_values(train_pool) # insert your train Pool object\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[:200,:], X_train.iloc[:200,:])\n\n# The plot represents just a slice of the Training data (200 observations)","dce40063":"shap.summary_plot(shap_values, X_train)","306e26a2":"# Features' Interactions\n\ntrain_data = Pool(X_train)\n\ninteraction = cat_model.get_feature_importance(train_data, type=\"Interaction\")\ncolumn_names = X_train.columns.values \ninteraction = pd.DataFrame(interaction, columns=[\"feature1\", \"feature2\", \"importance\"])\ninteraction.feature1 = interaction.feature1.apply(lambda l: column_names[int(l)])\ninteraction.feature2 = interaction.feature2.apply(lambda l: column_names[int(l)])\ninteraction.head(20)","c14ebbe1":"# Catboost default paramters\n\ncat_model.get_all_params()","6d3a3ac1":"# Preforming a Random Grid Search to find the best combination of parameters\n\n#grid = {'iterations': [1000,6000],\n      #  'learning_rate': [0.05, 0.005],\n      #  'depth': [4, 6, 10],\n      #  'l2_leaf_reg': [1, 3, 5]}\n\n#final_model = CatBoostRegressor()\n#randomized_search_result = final_model.randomized_search(grid,\n#                                                   X = X_train,\n#                                                   y= y_train,\n#                                                   verbose = False,\n#                                                   plot=True)\n                                                   ","ad4d6be2":"# Final Cat-Boost Regressor\n\nparams = {'iterations': 6000,\n          'learning_rate': 0.05,\n          'depth': 6,\n          'l2_leaf_reg': 5,\n          'eval_metric':'RMSE',\n          'early_stopping_rounds': 200,\n          'verbose': 200,\n          'random_seed': 42}\n         \ncat_f = CatBoostRegressor(**params)\ncat_model_f = cat_f.fit(X_train,y_train,\n                     eval_set = (X_val,y_val),\n                     plot=True,\n                     verbose = False)\n\ncatf_pred = cat_model_f.predict(X_val)\ncatf_score = rmse(y_val, catf_pred)","976528e2":"catf_score","184cfd73":"# Test CSV Submission\n\ntest_pred = cat_f.predict(test)\nsubmission = pd.DataFrame(test_id, columns = ['Id'])\ntest_pred = np.expm1(test_pred)\nsubmission['SalePrice'] = test_pred \nsubmission.head()","ecc53757":"# Saving the results in a csv file\n\nsubmission.to_csv(\"result.csv\", index = False, header = True)","65d77941":"### SUBMISSION","7c6cef97":"###  DATA PREPROCESSING\n\nNow that we have some insights about data, we need to preprocess them for the modeling part. The main steps are:\n\n- Looking at potential NaN\n- Dealing with categorical features (e.g. Dummy coding)\n- Normalization\n\nN.B:\n\nUsually, in a real-world project, the test data are not available until the end. For this reason, test data should contain the same type of data of the training set to preprocess them in the same way. Here, the test set is available. It contains some observations not present in the training dataset and,the use of dummy coding could raise several issues (I spent a lot of time figuring out why I was not able to make predictions on the test set). The easiest way to solve this problem (that is not applicable if test data are not available) is to concatenate Train and Test sets, preprocess, and divide them again.\n","8798f92d":"### MODELING","555f8b66":"The correlation matrix is the best way to see all the numerical correlation between features. Let's see which are the feature that correlate most with our target variable.","584b11db":"The above diagram represents each observation (x-axis) for the feature presented (y-axis). The x location of each dot on the x-axis reflects the impact of that feature on the model's predictions, while the color of the dot represents the value of that feature for that exact observation. Dots that pile up on the line show density. Here we can see how features such as 'BsmtFinType1_GLQ' or 'BsmtQual_Ex', differently from 'GrLivArea' and 'OverallQual', do not contribute significantly in producing the final predictions.\n\n","619fd29e":"### DEALING WITH OUTLIERS","fcb7386b":"Now that we know which features correlates most with our target variable we can investigate them more in depth.","7606908a":"on both end we can clearly see outliers","e4645c3b":"### EDA & VISUALIZATION\n\nBefore working with data let's understand them first. A crucial step to this aim is the ***Exploratory data analysis (EDA)***: a combination of visualizations and statistical analysis (uni-variate, bi-variate, and multivariate) these steps' helps us to better understand the data we are working with and to gain insight into their relationships. So, let's explore our target variable and how the other features influence it.","e43ce0c0":"Now let's take a look at the top 20 most important variables for our model. This could give us further insight into the functioning of the algorithm and how and which data it uses most to arrive at the final prediction.","88d790f1":"Are we sure that all these nans are real missing values? Looking at the given description file, we can see how the majority of these nans reflect the absence of something, and for this reason, they are not nans. We can impute them (for numerical features) or substitute them with data in the file:","d1e0cd93":" N.B: Catboost comes with a great method: ***get_feature_importance***. This method can be used to find important interactions among features. This is a huge advantage because it can give us insights about possible new features to create that can improve the performance.  ","faa17da2":"Value looks nice now","0dc9db90":"Let's see In LotArea ","64b2cc48":"Which are the deafult parameters used by CaboostRegressor? This is our real baseline, now we need to optimize the hyperparameters trying to tune the model to obtain a better performance.","4d0090e8":"In literature, acceptable values for skewness are between -0.5 and 0.5 while -2 and 2 for Kurtosis. Looking at the plot, we can clearly see how the distribution does not seem to be normal, but highly right-skewed. The non-normality of our distribution is also supported by the Shapiro test for normality (p-value really small that allows us to reject the hypotesis of normality). Despite that, let's leave it like that for now, we'll deal with that later in the notebook. ","f595113e":"### FEATURE ENGINEERING\n\nLet's create some new features combining the ones that we already have. These could help us to increase the performance of the model!","fd4a6e08":"Now let's try to tranform our target distribution into a normal one. To do this we use a log transformation. We will use qq-plot to see the transformation effect.  ","cb37e632":"### HOUSE SALE PRICING PREDICTION\n\n![image.png](attachment:image.png)\n\nThis is a beginners notebook that covers all the main steps necessary to complete a Machine Learning project. Here below you can see a detailed table of contents of the work:\n\n**PREPROCESSING & EDA**\n\n- Importing Libraries & Data\n- Dealing with Duplicates and Nan\n- Looking at correlations\n- Data Normalization (Plots & Tests)\n\n\n**MODELING**\n\n- Baseline Models with 10-Folds CV\n- Best Model (RandomGridSearch)\n- Prediction\n- Submission","cd72d6b2":"Imputation for completing missing values using k-Nearest Neighbors.\n\nEach sample\u2019s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close.","3db71926":"### Hyperparameter Optimization"}}