{"cell_type":{"71221ce5":"code","e3a64828":"code","3cbf5b7b":"code","bd12892f":"code","bbc587cf":"code","037516b8":"code","3d8b5515":"code","5eb0bc47":"code","cc201601":"code","2685257b":"code","6cc1f236":"code","6199c97d":"code","fafa5459":"code","94cf0ba0":"code","da14e17d":"code","dafe18ab":"code","b4696312":"code","3ca5686d":"code","f856bc1f":"code","cd785916":"code","8ac9e377":"code","b389811d":"code","7978a26c":"code","9230cc37":"code","af5a2e54":"code","c44ec507":"code","94b119b4":"code","de479987":"code","729563c6":"code","ef731894":"code","e28020ae":"code","17c846c6":"code","de7a567f":"code","d571c498":"code","07df8d95":"code","2c77e44a":"markdown","7e461c87":"markdown","a4152700":"markdown","7fbae7ff":"markdown","ca3e89d9":"markdown","edf7c1ed":"markdown"},"source":{"71221ce5":"# Import packages\n\n# pandas to read data\nimport pandas as pd\n\n#numpy for linear algebra and math\nimport numpy as np\n\n# scipy stats for statistics\nfrom scipy import stats\n\n#visuals\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#set seaborn sytle\nsns.set()\n\n# display max columns\npd.set_option('display.max_columns', None)\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e3a64828":"# read the csv file\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ncredit = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","3cbf5b7b":"#print head of the credit df\ncredit.head()","bd12892f":"#print tail\ncredit.tail()","bbc587cf":"# Let's see how inbalanced our data is:\nsns.countplot('Class', data=credit)\nplt.text(s= 'No fraud : {}'.format(credit['Class'].value_counts()[0]), x = -0.3, y = (credit['Class'].value_counts()[0])\/2, color='w' )\nplt.text(s= 'Fraud : {}'.format(credit['Class'].value_counts()[1]), x = 0.8, y = (credit['Class'].value_counts()[0])\/2 )","037516b8":"credit.describe()","3d8b5515":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.special import boxcox1p #using 1p cause of 0's in the dataset\nrs= RobustScaler()\nss= StandardScaler()\ncredit['rScaled_Time']= rs.fit_transform(credit['Time'].values.reshape(-1,1))\ncredit['rScaled_Amount']= rs.fit_transform(credit['Amount'].values.reshape(-1,1))\n\ncredit['sScaled_Time']= ss.fit_transform(credit['Time'].values.reshape(-1,1))\ncredit['sScaled_Amount']= ss.fit_transform(credit['Amount'].values.reshape(-1,1))\n\ncredit['boxScaled_Time'] = boxcox1p(credit['Time'], 0.25) \ncredit['boxScaled_Amount'] = boxcox1p(credit['Amount'], 0.25) \n\ncredit.drop(['Time', 'Amount'], axis=1, inplace=True)","5eb0bc47":"for col in ['rScaled_Time', 'rScaled_Amount', 'sScaled_Time', 'sScaled_Amount', 'boxScaled_Time', 'boxScaled_Amount']:\n    sns.distplot(credit[col])\n    plt.show()","cc201601":"credit.describe().T","2685257b":"from sklearn.model_selection import train_test_split\n\nX=credit.drop('Class', axis=1)\ny=credit['Class']\n\nX_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.1, stratify=y)\n\ntrain_data= pd.concat([X_train, y_train], axis=1)\ntest_data= pd.concat([X_test, y_test], axis=1)\n\n#\u00a0Now we will split train data again (X_train an y_train), interpret  the model\n# make predictions with X_test to predict y_test","6cc1f236":"train_data.head()","6199c97d":"test_data.head()","fafa5459":"print(train_data['Class'].value_counts())\nprint(train_data['Class'].value_counts()[0]\/train_data['Class'].value_counts()[1])","94cf0ba0":"print(test_data['Class'].value_counts())\nprint(test_data['Class'].value_counts()[0]\/test_data['Class'].value_counts()[1])","da14e17d":"# Over and under sampling\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\n\nsmote= SMOTE(sampling_strategy=0.2, random_state=42)\n# Oversampling of train_data, recall train_data=pd.concat([X_train, y_train])\nover_X_train, over_y_train= smote.fit_resample(X_train, y_train)","dafe18ab":"over_X_train = pd.DataFrame(over_X_train, columns=X_train.columns )\nover_X_train","b4696312":"# undersampling\nnm= NearMiss(sampling_strategy=0.5)\n\nX_train_new, y_train_new= nm.fit_resample(over_X_train, over_y_train)","3ca5686d":"# create new DF's with column names - fit_resample is creating numpy arrays \nX_train_new = pd.DataFrame(X_train_new, columns = X_train.columns )\ny_train_new = pd.DataFrame(y_train_new, columns = ['Class'])","f856bc1f":"display(X_train_new)\ny_train_new","cd785916":"X_train_new.shape","8ac9e377":"y_train_new.value_counts()","b389811d":"for col in ['rScaled_Time', 'rScaled_Amount']:\n    sns.boxplot(X_train_new[col])\n    plt.show()","7978a26c":"# drop other scaled features and keep Rscaled time and amount\nX_train_new = X_train_new.drop(['sScaled_Time', 'sScaled_Amount', 'boxScaled_Time', 'boxScaled_Amount'], axis=1)\nX_test= X_test.drop(['sScaled_Time', 'sScaled_Amount', 'boxScaled_Time', 'boxScaled_Amount'], axis=1)","9230cc37":"X_test.head()","af5a2e54":"X_train_new","c44ec507":"y_train_new","94b119b4":"# Shuffle X and y_train new, because they are sorted by y values\ntrain_df= pd.concat([X_train_new, y_train_new], axis=1)\n\nfrom sklearn.utils import shuffle\n\ntrain_df= shuffle(train_df, random_state=42)\n\ntrain_df.head()","de479987":"X_shuffled= train_df.drop('Class', axis=1)\ny_shuffled= train_df['Class']\n\nX_train_fin, X_dev, y_train_fin, y_dev= train_test_split(X_shuffled, y_shuffled, test_size=0.2, stratify=y_shuffled)","729563c6":"print(X_train_fin.shape)\nprint(y_train_fin.shape)\nprint(X_dev.shape)\nprint(y_dev.shape)\n","ef731894":"y_shuffled[0:20]","e28020ae":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\n\n#Performance metrics\nfrom sklearn.metrics import classification_report, confusion_matrix","17c846c6":"key= ['KNeighborsClassifier', 'LogisticRegression', 'RandomForestClassifier', 'GaussianNB', 'DecisionTreeClassifier', 'XGBClassifier', 'SVC']\nvalue= [KNeighborsClassifier(), LogisticRegression(), RandomForestClassifier(), GaussianNB(), DecisionTreeClassifier(), XGBClassifier(), SVC()]\n\nmodels= dict(zip(key,value))","de7a567f":"training_scores= []\ntesting_scores=[]\n\nfor key, value in models.items():\n    value.fit(X_train_fin, y_train_fin)\n    y_train_pred= value.predict(X_train_fin)\n    y_dev_pred= value.predict(X_dev)\n    train_score= value.score(X_train_fin,  y_train_fin)\n    test_score= value.score(X_dev, y_dev)\n    training_scores.append(train_score)\n    testing_scores.append(test_score)\n    train_conf_matrix = confusion_matrix(y_train_pred,  y_train_fin)\n    dev_conf_matrix = confusion_matrix(y_dev_pred, y_dev)\n\n\n    print(f\"{key}\\n\")\n    print(f\"Training Score: {train_score}\" )\n    print(f\"Testing Score: {test_score} \\n\")\n    print(f\"Train confusion matrix: \\n {train_conf_matrix}\")\n    print(f\"Dev confusion matrix: \\n {dev_conf_matrix}\")","d571c498":"rfc= RandomForestClassifier()\n\nrfc.fit(X_train_fin, y_train_fin)\ny_pred_rfc= rfc.predict(X_test)\nprint(f\"Confusion Matrix: \\n {confusion_matrix(y_test, y_pred_rfc)}\")","07df8d95":"print(f\"Classification Report: \\n {classification_report(y_test, y_pred_rfc)}\")\n","2c77e44a":"We used so many different names, and created confusing names. Let's wrap up training and test data\n\n####  Training data: \nHere we will use X_train_fin and y_train_fin\nAnd predict(X_dev) to predict y_dev\n\n#### Test data\nWe will use X_test to predict y_test ( we did no changes on these two)","7e461c87":"Robust Scaler did better job on the features. However, I believe the amount has outliers, cause it has X range between 0 to 100, and right skewed. ","a4152700":"We have highly inbalanced data here.  We know that PCA analysis applied for privacy issues. However, it is not a barrier for us to not investigate the distribution, min, max, mean of the features.\n\n","7fbae7ff":"We have two datasets with similar target proportions. ","ca3e89d9":"# Credit Card Fraud Detection - Kaggle\n\n## Introduction \n\nCredit card fraud is an inclusive term for fraud committed using a payment card, such as a credit card or debit card. The purpose may be to obtain goods or services, or to make payment to another account which is controlled by a criminal. The Payment Card Industry Data Security Standard (PCI DSS) is the data security standard created to help businesses process card payments securely and reduce card fraud.\n\nCredit card fraud can be authorised, where the genuine customer themselves processes a payment to another account which is controlled by a criminal, or unauthorised, where the account holder does not provide authorisation for the payment to proceed and the transaction is carried out by a third party. In 2018, unauthorised financial fraud losses across payment cards and remote banking totalled \u00a3844.8 million in the United Kingdom. Whereas banks and card companies prevented \u00a31.66 billion in unauthorised fraud in 2018. That is the equivalent to \u00a32 in every \u00a33 of attempted fraud being stopped. (Wiki)\n\n## About the Data\n\nThe dataset contains transactions made by credit cards in September 2013 by European cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\nGiven the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.","edf7c1ed":"### Outliers on Amount\nAs you can see from the descriptive table, rScaled_Amount  maximum value is 358, since minimum median are gathered around -0.2 to 0.7. It would be better to remove these outliers. \n\n### Class Imbalance\n\nTo handle the imbalanced data, we will first do oversampling of minority class, then do undersampling of majority class To do that, we will have to first divide our dataset into train and test set because if we oversample the whole minority class, our test set will contain the same data that will be used to train our classifier\n\nHence, we first divide our dataset, then oversample, undersample and then train our model on train set"}}