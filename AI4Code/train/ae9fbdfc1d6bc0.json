{"cell_type":{"b626abd9":"code","453846f4":"code","f2eb1731":"code","f155b9a7":"code","0071742e":"code","c6923e1b":"code","1eb27a75":"code","c682609d":"code","05a4ffb6":"code","1b1cf998":"code","d31b16e3":"code","5b2c4b72":"code","0d45cda4":"code","55a00e43":"code","d048ca49":"code","2f8eb9bf":"code","7cbe4c2e":"code","f54eb10a":"code","5807bfa1":"code","3b847ea4":"code","dc2a27d0":"code","77091f65":"code","321a7f2d":"code","f0486496":"code","8ed29051":"code","96c5b2d9":"code","60828d7b":"code","1fc55724":"code","02ac4b6d":"code","fc0287f7":"code","dacb66c3":"code","70333989":"code","468264fb":"markdown","af27e1f2":"markdown","c3987041":"markdown","ad4024c9":"markdown","b4373904":"markdown","0e021e9e":"markdown","564314ca":"markdown","04c84144":"markdown","38cf4837":"markdown","acaf328f":"markdown","fe9df888":"markdown","5e48acb2":"markdown","2988f4a2":"markdown","302b06e0":"markdown","67166699":"markdown","185604d4":"markdown"},"source":{"b626abd9":"# import basic libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)","453846f4":"# reading train and test data\ntrain = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')","f2eb1731":"# viewing first 5 rows of our train dataset\ntrain.head()","f155b9a7":"# shape of train-data\ntrain.shape","0071742e":"# concise summary of a DataFrame.\ntrain.info()","c6923e1b":"# descriptive statistics of the data\ntrain.describe().T","1eb27a75":"# check for null values\ntrain.isna().sum()","c682609d":"# let's see how many sample each of our class has\ntrain.claim.value_counts()","05a4ffb6":"# plot a pie chart\nplt.pie(train.claim.value_counts(), labels = ['0', '1']);","1b1cf998":"train.drop('id', axis = 1, inplace = True)","d31b16e3":"for i in range(118):\n    train['f'+str(i+1)].fillna(train['f'+str(i+1)].mean(), inplace = True)","5b2c4b72":"def skew_kurt(column, data = train):\n    sns.displot(x = column, data = data, kde = True)\n    skewness=str(data[column].skew())\n    kurtosis=str(data[column].kurt())\n    plt.legend([skewness,kurtosis],title=(\"skewness and kurtosis\"))\n    plt.show()","0d45cda4":"skew_kurt('f1')","55a00e43":"skew_kurt('f2')","d048ca49":"skew_kurt('f25')","2f8eb9bf":"skew_kurt('f50')","7cbe4c2e":"skew_kurt('f100')","f54eb10a":"# dependent and independent features\nx = train.drop(['claim'], axis = 1)\ny = train['claim']","5807bfa1":"x.head()","3b847ea4":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nx = scaler.fit_transform(x)","dc2a27d0":"#splitting the dataset into train and test set.\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = .05, random_state = 31)","77091f65":"len(x_train), len(x_test), len(y_train), len(y_test)","321a7f2d":"x_train.shape, y_train.shape","f0486496":"%%time\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\nfrom lightgbm import LGBMClassifier\nlgbm = LGBMClassifier(objective= 'binary',\n                    n_estimators= 20000,\n                    random_state= 2021,\n                    learning_rate= 5e-3,\n                    subsample= 0.6,\n                    subsample_freq= 1,\n                    colsample_bytree= 0.4,\n                    reg_alpha= 10.0,\n                    reg_lambda= 1e-1,\n                    min_child_weight= 256,\n                    min_child_samples= 20).fit(x_train, y_train)\n\ny_preds = lgbm.predict_proba(x_test)[:, 1]\nroc_auc_score(y_test, y_preds)","8ed29051":"test.head()","96c5b2d9":"# descriptive statistics of test data\ntest.describe().T","60828d7b":"# check for null values\ntest.isna().sum()","1fc55724":"for i in range(118):\n    test['f'+str(i+1)].fillna(test['f'+str(i+1)].mean(), inplace = True)","02ac4b6d":"test.drop('id', axis = 1, inplace = True)","fc0287f7":"test.shape","dacb66c3":"test = scaler.transform(test)","70333989":"preds = lgbm.predict_proba(test)[:, 1]\ntestforsub = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\npreds = pd.DataFrame(preds, columns = ['claim'])\nsub = pd.concat([testforsub.id, preds] , axis = 1)\nsub.to_csv('baseline_submission.csv', index = False)","468264fb":"# Predicting and submission file","af27e1f2":"# Train data Preprocessing","c3987041":"I chose some column randomly and all of their skew and kurt are in expected range. I assume rest of the columns will be same. You may use **transformation** for better distribution.","ad4024c9":"# Test data Preprocessing","b4373904":"# Let's know our data\n\nEvery samples of this dataset is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting whether a claim will be made on an insurance policy. Although the features are anonymized, they have properties relating to real-world features.We need to predict whether a customer made a claim upon an insurance policy. The ground truth claim is binary valued, but a prediction may be any number from `0.0` to `1.0`, representing `the probability` of a claim. The features in this dataset have been anonymized and it contains missing values.\n\n**Evaluation**: Submissions are evaluated on area under the **ROC curve** between the predicted probability and the observed target.\n\n### What do we need to submit?\n\nThe submission file is expected to have an id and claim columns.\n\nOk! Now we are familar with our playground. Let's practice. We will try to score goals later","0e021e9e":"We are making a baseline. So, let's just fill them with mean. We can analysis them more later.","564314ca":"# Handling Missing Values","04c84144":"# Table of Contents\n\n* [Let's Know our data](#Let's-know-our-data)\n* [Train data Preprocessing](#Train-data-Preprocessing)\n* [Handling Missing Values](#Handling-Missing-Values)\n* [Feature Exploration](#Feature-Exploration)\n* [Feature Scaling](#Feature-Scaling)\n* [Modeling](#Modeling)\n* [Test data Processing](#Test-data_processing)\n* [Prediction & Submission](#Predicting-and-submission-file)","38cf4837":"**Skewness** is a measure of *symmetry*, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point. **Kurtosis** is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or **outliers**. If outliers is a new term for you, check out this [great notebook ](https:\/\/www.kaggle.com\/nareshbhat\/outlier-the-silent-killer) by [Naresh Bhat](#https:\/\/www.kaggle.com\/nareshbhat).\n\n#### What is acceptable skewness and kurtosis?\nThe values for asymmetry and kurtosis between -2 and +2 are considered acceptable in order to prove normal univariate distribution (George & Mallery, 2010)\n\nLet's check `Skewness` and `Kurtosis` for our data.","acaf328f":"**N.B:** I always try to import libraries right before where I use them. This process helps me and hopefully the reader to track the required library much efficiently than importing it all in one cell.","fe9df888":"# Modeling","5e48acb2":"We have nearly the same numbers of null values in every column. You may remember all values are synthetic. So this is possible.","2988f4a2":"# Feature Scaling","302b06e0":"# Feature Exploration","67166699":"# Overview:\n\nIt is always better to start with a simple model in every Machine Learning problem. Then we can apply different methods to increase the score and make a more robust model. This notebook aims to make a baseline model and make my first submission in this competition.","185604d4":"### Basic Train data Info"}}