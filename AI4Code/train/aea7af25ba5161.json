{"cell_type":{"627983aa":"code","317cdf18":"code","b1c9b362":"code","daa2b702":"code","24b100c7":"code","9621ceda":"code","cad312af":"code","5d856ac8":"code","04eb2ae3":"code","10413ff0":"code","3cb9fefc":"code","3ecbfb88":"code","0a1dee3a":"code","b16117d5":"code","b14d11d6":"code","7db7eb51":"code","27e71b8f":"code","8714888b":"code","980bd4c4":"code","661f397d":"code","95309cc0":"code","cdddac66":"code","54f094a3":"code","3be53819":"code","30cc774f":"code","b10dcf6c":"code","df2ea82b":"code","9324572d":"code","ddf74503":"code","081db5c4":"code","72810789":"code","fdfea0df":"code","74a273e7":"code","709a7c21":"code","564134eb":"code","70e25cfd":"code","055165a1":"code","c368310c":"code","dff0103a":"code","c9bf4f8f":"code","e999008e":"code","57e98da4":"code","262ba023":"code","1ccb3423":"code","58794348":"code","aaad7553":"code","65dcf383":"code","6f6d8869":"code","a1f644ef":"code","6e23eee3":"code","f82ab6a9":"code","c77c4d41":"markdown","e1edc41f":"markdown","7be0bc8a":"markdown","46adc58b":"markdown","48086199":"markdown","c7700522":"markdown","05df0c83":"markdown","d4c03f57":"markdown","c2d11a12":"markdown","0537eee8":"markdown"},"source":{"627983aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom tqdm import tqdm\nimport random\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","317cdf18":"## read the data:\nkaggle=1\n\nif kaggle==1:\n    \n    train=pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\")\n    test=pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\")\n    sample=pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/sample_submission.csv\")\n    \nelse:\n    train=pd.read_csv(\"train.csv\")\n    test=pd.read_csv(\"test.csv\")\n    sample=pd.read_csv(\"sample_submission.csv\")","b1c9b362":"train.head()","daa2b702":"train.dtypes","24b100c7":"train.isnull().any(),test.isnull().any()","9621ceda":"for c in train.columns :\n    if c not in ['id','target']:\n        print(c,\"has\",train[c].nunique(),\"unique values\\n\")","cad312af":"for col in ['nom_5','nom_6','nom_7','nom_8','nom_9']:\n    print(col,\"has \",set(train[col].unique())-set(test[col].unique()),\" unique value in train set not available in test dataset\\n\")","5d856ac8":"train.shape,test.shape","04eb2ae3":"train.columns,test.columns","10413ff0":"train.dtypes,test.dtypes","3cb9fefc":"def reduce_memory(df,col):\n    mx = df[col].max()\n    print(f'\\n reducing memory for {col}')\n    if mx<256:\n        df[col] = df[col].astype('uint8')\n    elif mx<65536:\n        df[col] = df[col].astype('uint16')\n    else:\n        df[col] = df[col].astype('uint32')","3ecbfb88":"\n\n\ndef reduce_cardi(train,test,col):\n    print(f'\\nBefore reducing the cardinality of col {c} in train was {train[c].nunique()}')\n    cv1=pd.DataFrame(train[col].value_counts().reset_index().rename({col:'train'},axis=1))\n    cv2=pd.DataFrame(test[col].value_counts().reset_index().rename({col:'test'},axis=1))\n    cv3=pd.merge(cv1,cv2,on='index',how='outer')\n    factor=len(train)\/len(test)\n    cv3['train'].fillna(0,inplace=True)\n    cv3['test'].fillna(0,inplace=True)\n    cv3['remove']=False\n    cv3['remove']=cv3['remove']|(cv3['train']<len(train)\/10000)  ## remove variables that appear 0.1 % of the total data in train.\n    cv3['new'] = cv3.apply(lambda x: x['index'] if x['remove']==False else 0,axis=1)\n    cv3['new'],_ = cv3['new'].factorize(sort=True)\n    cv3.set_index('index',inplace=True)\n    cc = cv3['new'].to_dict()\n    train[col] = train[col].map(cc)\n    \n    test[col]=test[col].map(cc)\n    #reduce_memory(test,col)","0a1dee3a":"card_cols=['nom_9','nom_8','nom_5','nom_6','nom_7']\n\nfor c in card_cols:\n    print(f'\\nReducing cardinality for col {c}')\n    reduce_cardi(train,test,c)\n    reduce_memory(train,c)\n    reduce_memory(test,c)\n    print(f'\\nAfter reducing the cardinality of col {c} in train is {train[c].nunique()}')","b16117d5":"## Creating new features after doing one round of training and understanding the gini importance\n\n# train['ord_1'+'_'+'ord_5']=train['ord_1']+\"_\"+train['ord_5']\n# test['ord_1'+'_'+'ord_5']=test['ord_1']+\"_\"+test['ord_5']\n\n# train['ord_1'+'_'+'ord_2']=train['ord_1']+\"_\"+train['ord_2']\n# test['ord_1'+'_'+'ord_2']=test['ord_1']+\"_\"+test['ord_2']\n\n# train['nom_6'+'_'+'nom_7']=train['nom_6']+'_'+train['nom_7']\n# test['nom_6'+'_'+'nom_7']=test['nom_6']+'_'+test['nom_7']","b14d11d6":"# a=train\n# b=test","7db7eb51":"## OHE columns with less cardinality\n\ncat_cols=['bin_0','bin_1','bin_2','bin_3','bin_4','ord_0','ord_1','ord_2']\n#https:\/\/github.com\/rushter\/heamy\/blob\/c330854cee3c547417eb353a4a4a23331b40b4bc\/heamy\/feature.py\n\nfor column in cat_cols:\n      \n        cate = pd.concat([train[column], test[column]]).dropna().unique()\n\n        train[column] = train[column].astype('category')\n        test[column] = test[column].astype('category')\n\ntrain = pd.get_dummies(train, columns=cat_cols, dummy_na=False, sparse=False)\ntest = pd.get_dummies(test, columns=cat_cols, dummy_na=False, sparse=False)\n","27e71b8f":"train.columns,test.columns","8714888b":"import category_encoders as ce","980bd4c4":"def frequency_encoding(variable):\n    t = train[variable].value_counts().reset_index()\n    t = t.reset_index()\n    t.loc[t[variable] == 1, 'level_0'] = np.nan\n    t.set_index('index', inplace=True)\n    max_label = t['level_0'].max() + 1\n    t.fillna(max_label, inplace=True)\n    return t.to_dict()['level_0']","661f397d":"# temp = test['nom_6_nom_7'].value_counts().reset_index()\n# temp = temp.reset_index()\n# temp.loc[temp['nom_6_nom_7'] == 1, 'level_0'] = np.nan\n# temp.set_index('index', inplace=True)\n# max_label = temp['level_0'].max() + 1\n# temp.fillna(max_label, inplace=True)\n# #temp.to_dict()['level_0'],max_label\n# temp.isna().any()","95309cc0":"ce_cols=['nom_0','nom_1','nom_2','nom_3','nom_4', 'ord_3', 'ord_4', 'ord_5', 'day', 'month']\n## Remove day,month and newly created combined columns:\n#ce_cols=['nom_0','nom_1','nom_2','nom_3','nom_4','nom_5','nom_6','nom_7','nom_8', 'nom_9', 'ord_3', 'ord_4', 'ord_5']\n## Remove cols that have been summarised earlier:\n#ce_cols = [c for c in ce_cols if c not in eda_cols]","cdddac66":"ce_cols","54f094a3":"# for c in ce_cols:\n#     print(f'\\nConverting {c} to lower case')\n#     train[c]=train[c].str.lower()\n#     test[c]=test[c].str.lower()","3be53819":"# cat_boost=ce.CatBoostEncoder(cols=ce_cols)\n# cat_boost.fit(train[ce_cols],train['target'])\n\n# train=train.join(cat_boost.transform(train[ce_cols]).add_suffix('_cb'))\n# test=test.join(cat_boost.transform(test[ce_cols]).add_suffix('_cb'))","30cc774f":"## Frequency encode variables:\nfor variable in tqdm(ce_cols):\n    freq_encod_dict=frequency_encoding(variable)\n    train[variable+'_FE']=train[variable].map(lambda x:freq_encod_dict.get(x,1))# return value as 1 if the variable does not exist in freq_encod_dict\n    test[variable+'_FE']=test[variable].map(lambda x:freq_encod_dict.get(x,1))\n    #categorical_columns.remove(variable)","b10dcf6c":"train.columns,test.columns","df2ea82b":"# test.isnull().any()","9324572d":"train.head()","ddf74503":"ce_cols","081db5c4":"train=train.drop(columns=ce_cols,axis=1)\ntest=test.drop(columns=ce_cols,axis=1)","72810789":"# train=train.drop(columns=['nom_5','nom_6','nom_7','nom_8','ord_5'],axis=1)\n# test=test.drop(columns=['nom_5','nom_6','nom_7','nom_8','ord_5'],axis=1)","fdfea0df":"# plt.figure(figsize=(30,30))\n# sns.heatmap(train[feats].corr(), cmap='RdBu_r', annot=True, center=0.0)","74a273e7":"##https:\/\/www.kaggle.com\/kyakovlev\/ieee-fe-for-local-test\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)","709a7c21":"seed_everything(1001)","564134eb":"##import required libraries:\nfrom sklearn.model_selection import StratifiedKFold,GroupKFold\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score,roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom bayes_opt import BayesianOptimization\nimport gc\nfrom tqdm import tqdm\n#import lofo as lofo","70e25cfd":"## Split into train and validation:\ny=train['target']\ntrain.drop('target',axis=1,inplace=True)","055165a1":"n_folds=5\nkf=StratifiedKFold(n_splits=n_folds,shuffle=True,random_state=1001)\nfeats=[f for f in train.columns if f not in ['id','bin_0_1',\n 'bin_1_1',\n 'bin_2_0',\n 'bin_3_F',\n 'bin_4_N', 'ord_0_1','ord_1_Novice','ord_2_Lava Hot' ]]  # remove highly correlated features","c368310c":"oof_preds=np.zeros(train.shape[0])\nsub_preds=np.zeros(test.shape[0])\n\nfeature_importance_df=pd.DataFrame()\ncategorical_features=[f for f in train.columns if f not in ['id','bin_0_1',\n 'bin_1_1',\n 'bin_2_0',\n 'bin_3_F',\n 'bin_4_N','ord_0_1','ord_1_Novice','ord_2_Lava Hot']]","dff0103a":"categorical_features","c9bf4f8f":"##https:\/\/www.kaggle.com\/cdeotte\/eda-for-columns-v-and-id\nplt.figure(figsize=(30,30))\nsns.heatmap(train[feats].corr(), cmap='RdBu_r', annot=True, center=0.0)","e999008e":"cor=train[feats].corr()\n","57e98da4":"##https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction\nc=cor.unstack()\nso=c.sort_values(kind='quicksort').reset_index().rename({0:'value'},axis=1)\nso=so[so['level_0']!=so['level_1']]\nprint(\"Negative correlated features \\n\",so.head(10),\"\\nPositive correlated features\\n\",so.tail(10))","262ba023":"#Parameters through Bayesian Optimization:\n\nparam = {'num_leaves': 40,\n         'min_data_in_leaf': 69, \n         'objective':'binary',\n         'max_depth': 4,\n         'learning_rate': 0.026,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.49,\n         \"metric\": 'auc',\n         \"lambda_l2\": 2.84,\n         \"random_state\": 100,\n         \"min_gain_to_split\":0.386,\n         \"bagging_freq\":5, ## randomly initialized\n         \"bagging_fraction\":0.5,## randomly initialized\n         \"verbosity\": -1}\n\n# {'feature_fraction': 0.49667591307631404,\n#  'lambda_l2': 2.843966944905738,\n#  'learning_rate': 0.026747986947564972,\n#  'max_depth': 4.390096819896572,\n#  'min_data_in_leaf': 68.92767604826321,\n#  'min_gain_to_split': 0.38669168478099314,\n#  'num_leaves': 39.828939074908384}\n\n# param = {'num_leaves': 60,\n#          'min_data_in_leaf': 60, \n#          'objective':'binary',\n#          'max_depth': -1,\n#          'learning_rate': 0.1,\n#          \"boosting\": \"gbdt\",\n#          \"feature_fraction\": 0.8,\n#          \"bagging_freq\": 1,\n#          \"bagging_fraction\": 0.8 ,\n#          \"bagging_seed\": 11,\n#          \"metric\": 'auc',\n#          \"lambda_l1\": 0.1,\n#          \"random_state\": 133,\n#          \"verbosity\": -1}","1ccb3423":"# ## Feature importance:\n\n# dataset=lofo.Dataset(df=train,target=\"target\",features=categorical_features)\n\n# lgbm=lgb.train(param,num_round,\n#                     verbose_eval=100,\n#                     early_stopping_rounds = 200)\n\n# lf=LOFOImportance(dataset,model=lgbm,cv=4,scoring='roc_auc',n_jobs=4)\n\n# # dataset = Dataset(df=df, target=\"binary_target\", features=features, feature_groups=feature_groups)\n\n# # lgbm = LGBMClassifier(random_state=0, n_jobs=1)\n\n# # lofo = LOFOImportance(dataset, model=lgbm, cv=4, scoring='roc_auc', n_jobs=4)","58794348":"## Using the standard cross validation format for training.\n\n\nfor n_folds,(train_idx,valid_idx) in tqdm(enumerate(kf.split(train.values,y.values))):\n    print(\"fold n\u00b0{}\".format(n_folds+1))\n    trn_data = lgb.Dataset(train.iloc[train_idx][feats],\n                           label=y.iloc[train_idx],\n                           categorical_feature=categorical_features\n                          )\n    val_data = lgb.Dataset(train.iloc[valid_idx][feats],\n                           label=y.iloc[valid_idx],categorical_feature=categorical_features\n                          )\n\n    num_round = 10000\n    clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds = 200)\n    oof_preds[valid_idx]=clf.predict(train.iloc[valid_idx][feats],num_iteration=clf.best_iteration)\n    sub_preds+=clf.predict(test[feats],num_iteration=clf.best_iteration)\/kf.n_splits\n    \n    fold_importance_df=pd.DataFrame()\n    fold_importance_df['features']=feats\n    fold_importance_df['importance']=clf.feature_importance(importance_type='gain')\n    fold_importance_df['folds']=n_folds+1\n    print(f'Fold {n_folds+1}: Most important features are:\\n')\n    for i in np.argsort(fold_importance_df['importance'])[-5:]:\n        print(f'{fold_importance_df.iloc[i,0]}-->{fold_importance_df.iloc[i,1]}')\n    \n    feature_importance_df=pd.concat([feature_importance_df,fold_importance_df],axis=0)\n    \n    print('Fold %2d AUC : %.6f' % (n_folds + 1, roc_auc_score(y.iloc[valid_idx], oof_preds[valid_idx])))\n    del clf\n    gc.collect()\n    \n\n\nprint('Full auc score %.6f' % (roc_auc_score(y,oof_preds)))\n\ntest['target']=sub_preds\n","aaad7553":"# ##https:\/\/www.kaggle.com\/cdeotte\/xgb-fraud-with-magic-0-9600\n# kf=GroupKFold(n_splits=12)","65dcf383":"# ## Using the standard cross validation format for training.\n\n\n# for n_folds,(train_idx,valid_idx) in tqdm(enumerate(kf.split(train.values,y.values,groups=train['month']))):\n    \n#     print(\"fold n\u00b0{}\".format(n_folds+1))\n#     month = train.iloc[valid_idx]['month'].iloc[0]\n#     print('\\nFold',n_folds+1,'withholding month',month)\n#     print('\\n rows of train =',len(train_idx),'rows of holdout =',len(valid_idx))\n#     trn_data = lgb.Dataset(train.iloc[train_idx][feats],\n#                            label=y.iloc[train_idx],\n#                            categorical_feature=categorical_features\n#                           )\n#     val_data = lgb.Dataset(train.iloc[valid_idx][feats],\n#                            label=y.iloc[valid_idx],categorical_feature=categorical_features\n#                           )\n\n#     num_round = 10000\n#     clf = lgb.train(param,\n#                     trn_data,\n#                     num_round,\n#                     valid_sets = [trn_data, val_data],\n#                     verbose_eval=100,\n#                     early_stopping_rounds = 200)\n#     oof_preds[valid_idx]=clf.predict(train.iloc[valid_idx][feats],num_iteration=clf.best_iteration)\n#     sub_preds+=clf.predict(test[feats],num_iteration=clf.best_iteration)\/kf.n_splits\n    \n#     fold_importance_df=pd.DataFrame()\n#     fold_importance_df['features']=feats\n#     fold_importance_df['importance']=clf.feature_importance(importance_type='gain')\n#     fold_importance_df['folds']=n_folds+1\n#     print(f'Fold {n_folds+1}: Most important features are:\\n')\n#     for i in np.argsort(fold_importance_df['importance'])[-5:]:\n#         print(f'{fold_importance_df.iloc[i,0]}-->{fold_importance_df.iloc[i,1]}')\n    \n#     feature_importance_df=pd.concat([feature_importance_df,fold_importance_df],axis=0)\n    \n#     print('Fold %2d AUC : %.6f' % (n_folds + 1, roc_auc_score(y.iloc[valid_idx], oof_preds[valid_idx])))\n#     del clf\n#     gc.collect()\n    \n\n\n# print('Full auc score %.6f' % (roc_auc_score(y,oof_preds)))\n\n# test['target']=sub_preds\n","6f6d8869":"# ## Random Forest\n# for n_folds,(train_idx,valid_idx) in tqdm(enumerate(kf.split(train.values,y.values))):\n#     print(\"fold n\u00b0{}\".format(n_folds+1))\n#     trn_X = train.iloc[train_idx][feats]\n#     trn_Y = y.iloc[train_idx]\n#     val_X=train.iloc[valid_idx][feats]\n#     val_Y=y.iloc[valid_idx]\n#     num_round = 10000\n#     clf =RandomForestClassifier(n_estimators=50,min_samples_split=10,min_samples_leaf=10,max_depth=10,random_state=100,criterion='gini',max_features='sqrt',oob_score=True)\n#     clf.fit(trn_X,trn_Y)\n#     oof_preds[valid_idx]=clf.predict(train.iloc[valid_idx][feats])\n#     sub_preds+=clf.predict(test[feats])\/kf.n_splits\n    \n#     fold_importance_df=pd.DataFrame()\n#     fold_importance_df['features']=feats\n#     fold_importance_df['importance']=clf.feature_importances_\n#     fold_importance_df['folds']=n_folds+1\n#     print(f'Fold {n_folds+1}: Most important features are:\\n')\n#     for i in np.argsort(fold_importance_df['importance'])[-5:]:\n#         print(f'{fold_importance_df.iloc[i,0]}-->{fold_importance_df.iloc[i,1]}')\n    \n#     feature_importance_df=pd.concat([feature_importance_df,fold_importance_df],axis=0)\n    \n#     print('Fold %2d AUC : %.6f' % (n_folds + 1, roc_auc_score(y.iloc[valid_idx], oof_preds[valid_idx])))\n#     del clf\n#     gc.collect()\n    \n\n\n# print('Full auc score %.6f' % (roc_auc_score(y,oof_preds)))\n\n# test['target']=sub_preds","a1f644ef":"sample['target']=sub_preds","6e23eee3":"sample.head()","f82ab6a9":"sample.to_csv(\"sample_submission.csv\",index=False)","c77c4d41":"**To try out** \n\n* Remove either the summary features or the FE features of the correlated variables and see if the score increases or decreases.\n\n* Do a summary features of most important columns https:\/\/www.kaggle.com\/tunguz\/elo-adversarial-validation\n\n* Try removing the variables with high cardinality - https:\/\/www.kaggle.com\/bogorodvo\/lightgbm-baseline-model-using-sparse-matrix","e1edc41f":"Now we will start experimenting with different encoding techniques.I will use the techniques described in this [kaggle course](https:\/\/www.kaggle.com\/matleonard\/categorical-encodings)","7be0bc8a":"* The local cv score increases by 0.000865 when we include only one of the categories in binary cardinality features and leave out using combination of features.The score was .793503 whereas including all the features the score was 0.792638.\n* After doing the summary features only for certain columns the local auc score is 0.792114 which is not a improvement over the above which gave a local cv score of 0.793503.Therefore we retain the above score.\n* We do not include the summary variables and include only encoded variables in the model and the auc score is 0.791267.\n* Removing the highly correlated variables gave auc of 0.788427\n* Model score is 0.790089 when we remove the high cardinality variables and do a FE on those variables.","46adc58b":"From the result we understand that nom_7,nom_8 and nom_9 have values which are not present in the test dataset.The local CV score without removing these variable is 0.79910 . ","48086199":"nom_5,nom_6,nom_7,nom_8,nom_9 has more cardinality compared to other variables.Lets check if there exist any unique values in test set for these columns.","c7700522":"### Modelling","05df0c83":"Our strategy is to one hot encode the binary categorical variables - bin_0 to bin_4,ordinal variables ord_0 to ord_2 , frequency encode the rest.We remove the cardinality of high cardinality variables by removing the variables present only 0.1 % of the data.","d4c03f57":"We dont have any null vales in the dataset.","c2d11a12":"## Experiement 2:","0537eee8":"* From the above correlation plot it is seen that the summary features correlate with their respective FE variables.Ord_0_1 , ord_0_2 and ord_0_3 show correlation with each other."}}