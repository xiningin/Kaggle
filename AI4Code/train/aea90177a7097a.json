{"cell_type":{"1e6a58b2":"code","dee3f5cd":"code","d00ecf83":"code","565b28bc":"code","dcd2836b":"code","cd341b13":"code","58f186e8":"code","4092e863":"code","fa03fbba":"code","13c9c72f":"code","42201d70":"code","d980a194":"code","3925effd":"code","4a936b59":"markdown","33db7ac6":"markdown","b8ca9ee3":"markdown","deac9d32":"markdown","780d1fae":"markdown","da7ee530":"markdown","95c037c9":"markdown"},"source":{"1e6a58b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dee3f5cd":"df = pd.read_csv('..\/input\/suspicious-communication-on-social-platforms\/Suspicious Communication on Social Platforms.csv', encoding='ISO-8859-2')\ndf.head().style.set_properties(**{'background-color':'Aqua',\n                                     'color': 'purple'})","d00ecf83":"#Twenty-Third row:\"that karma is a bitch  HUH'\", 1st column: comments\n\ndf.iloc[23,0]","565b28bc":"def get_sequences(texts, tokenizer, train=True, max_seq_length=None):\n    sequences = tokenizer.texts_to_sequences(texts)\n    \n    if train == True:\n        max_seq_length = np.max(list(map(lambda x: len(x), sequences)))\n    \n    sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n    \n    return sequences","dcd2836b":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    # Drop FILE_NAME column\n   # df = df.drop('FILE_NAME', axis=1)\n    \n    # Split df into X and y\n    y = df['tagging']\n    X = df['comments']\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Create tokenizer\n    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=30000)\n    \n    # Fit the tokenizer\n    tokenizer.fit_on_texts(X_train)\n    \n    # Convert texts to sequences\n    X_train = get_sequences(X_train, tokenizer, train=True)\n    X_test = get_sequences(X_test, tokenizer, train=False, max_seq_length=X_train.shape[1])\n    \n    return X_train, X_test, y_train, y_test","cd341b13":"X_train, X_test, y_train, y_test = preprocess_inputs(df)","58f186e8":"X_train","4092e863":"y_train.value_counts()","fa03fbba":"X_train.shape","13c9c72f":"inputs = tf.keras.Input(shape=(207,))\n\nembedding = tf.keras.layers.Embedding(\n    input_dim=30000,\n    output_dim=64\n)(inputs)\n\nflatten = tf.keras.layers.Flatten()(embedding)\n\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(flatten)\n\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.AUC(name='auc')\n    ]\n)\n\n\nprint(model.summary())","42201d70":"history = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=32,\n    epochs=100,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)","d980a194":"results = model.evaluate(X_test, y_test, verbose=0)\n\nprint(\"    Test Loss: {:.4f}\".format(results[0]))\nprint(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))\nprint(\"     Test AUC: {:.4f}\".format(results[2]))","3925effd":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Thanks to Gabriel Atkin, all code by Gabriel.')","4a936b59":"#Training","33db7ac6":"#Preprocessing","b8ca9ee3":"#https:\/\/www.kaggle.com\/gcdatkin\/email-spam-classification\n\n#Data Every Day. Youtube series by Gabriel Atkin. https:\/\/www.youtube.com\/watch?v=9R-0WgJh7Zw\n\n","deac9d32":"#\"That Karma is a bitch\"  23rd Comments Row.","780d1fae":"#Results","da7ee530":"#Code by Gabriel Atkin  https:\/\/www.kaggle.com\/gcdatkin\/email-spam-classification\n\n#Featured on Data Every Day, Gabriel's YouTube series where he trains models on a new dataset Each Day. https:\/\/www.youtube.com\/watch?v=9R-0WgJh7Zw\n\nUsing a TensorFlow\/Keras neural network with word embeddings to make the predictions.","95c037c9":"#Shape=207 was taken from the number above."}}