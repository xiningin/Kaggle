{"cell_type":{"fce96b8d":"code","a42992ca":"code","cb4080e3":"code","73ded6f1":"code","f1bb721b":"code","512763e5":"code","d939da0e":"code","ff9cc61b":"markdown","3736a825":"markdown"},"source":{"fce96b8d":"import sys\n!cp ..\/input\/rapids\/rapids.0.13.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path\n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","a42992ca":"import numpy as np\n# import pandas as pd\nimport cudf\nimport cupy as cp\nfrom cuml.neighbors import KNeighborsRegressor\nfrom cuml import SVR\nfrom cuml.linear_model import Ridge, Lasso\nfrom sklearn.model_selection import KFold\nfrom cuml.metrics import mean_absolute_error, mean_squared_error\n\n\ndef metric(y_true, y_pred):\n    return np.mean(np.sum(np.abs(y_true - y_pred), axis=0)\/np.sum(y_true, axis=0))","cb4080e3":"fnc_df = cudf.read_csv(\"..\/input\/trends-assessment-prediction\/fnc.csv\")\nloading_df = cudf.read_csv(\"..\/input\/trends-assessment-prediction\/loading.csv\")\n\nfnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\ndf = fnc_df.merge(loading_df, on=\"Id\")\n\n\nlabels_df = cudf.read_csv(\"..\/input\/trends-assessment-prediction\/train_scores.csv\")\nlabels_df[\"is_train\"] = True\n\ndf = df.merge(labels_df, on=\"Id\", how=\"left\")\n\ntest_df = df[df[\"is_train\"] != True].copy()\ndf = df[df[\"is_train\"] == True].copy()\n\ndf.shape, test_df.shape","73ded6f1":"# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.\nFNC_SCALE = 1\/600\n\ndf[fnc_features] *= FNC_SCALE\ntest_df[fnc_features] *= FNC_SCALE","f1bb721b":"%%time\n\nNUM_FOLDS = 7\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\n\nfeatures = loading_features + fnc_features\n\noveral_score = 0\nfor target, c, w, ff in [(\"age\", 60, 0.3, 0.55), (\"domain1_var1\", 12, 0.175, 0.2), (\"domain1_var2\", 8, 0.175, 0.2), (\"domain2_var1\", 9, 0.175, 0.22), (\"domain2_var2\", 12, 0.175, 0.22)]:    \n    y_oof = np.zeros(df.shape[0])\n    y_test = np.zeros((test_df.shape[0], NUM_FOLDS))\n\n    \n    for f, (train_ind, val_ind) in enumerate(kf.split(df, df)):\n        train_df, val_df = df.iloc[train_ind], df.iloc[val_ind]\n        train_df = train_df[train_df[target].notnull()]\n\n        model_1 = SVR(C=c, cache_size=3000.0)\n        model_1.fit(train_df[features].values, train_df[target].values)\n        model_2 = Ridge(alpha = 0.00008)\n        model_2.fit(train_df[features].values, train_df[target].values)\n        val_pred_1 = model_1.predict(val_df[features])\n        val_pred_2 = model_2.predict(val_df[features])\n        \n        test_pred_1 = model_1.predict(test_df[features])\n        test_pred_2 = model_2.predict(test_df[features])\n        \n        val_pred = (1-ff)*val_pred_1+ff*val_pred_2\n        val_pred = cp.asnumpy(val_pred.values.flatten())\n        \n        test_pred = (1-ff)*test_pred_1+ff*test_pred_2\n        test_pred = cp.asnumpy(test_pred.values.flatten())\n\n        y_oof[val_ind] = val_pred\n        y_test[:, f] = test_pred\n        \n    df[\"pred_{}\".format(target)] = y_oof\n    test_df[target] = y_test.mean(axis=1)\n    \n    score = metric(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values)\n    mae = mean_absolute_error(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values)\n    rmse = np.sqrt(mean_squared_error(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values))\n    overal_score += w*score\n    print(target, np.round(score, 8))\n    print(target, np.round(score, 4))\n    print(target, 'mean absolute error:', np.round(mae, 8))\n    print(target, ' root mean square error:', np.round(rmse, 8))\n    print()\n    \nprint(\"Overal score:\", np.round(overal_score, 8))\nprint(\"Overal score:\", np.round(overal_score, 4))","512763e5":"sub_df = cudf.melt(test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=[\"Id\"], value_name=\"Predicted\")\nsub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n\nsub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\nassert sub_df.shape[0] == test_df.shape[0]*5\nsub_df.head(10)","d939da0e":"sub_df.to_csv(\"submission_rapids_ensemble.csv\", index=False)","ff9cc61b":"This is an ensemble version of @aerdem4's excellent SVM notebook that can be found here: https:\/\/www.kaggle.com\/aerdem4\/rapids-svm-on-trends-neuroimaging\n\n[Rapids](https:\/\/rapids.ai) is an open-source GPU accelerated Data Sceince and Machine Learning library, developed and mainatained by [Nvidia](https:\/\/www.nvidia.com). It is designed to be compatible with many existing CPU tools, such as Pandas, scikit-learn, numpy, etc. It enables **massive** acceleration of many data-science and machine learning tasks, oftentimes by a factor fo 100X, or even more. \n\nRapids is still undergoing developemnt, and as of right now it's not availabel in the Kaggle Docker environment. If you are interested in installing and riunning Rapids locally on your own machine, then you should [refer to the followong instructions](https:\/\/rapids.ai\/start.html).","3736a825":"## Install Rapids for 500x faster kNN on GPUs"}}