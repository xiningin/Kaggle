{"cell_type":{"170f2c0d":"code","89594235":"code","18626669":"code","6a74dfad":"code","9ce0d421":"code","e4968ddf":"code","cfc3eef6":"code","b3ab2db2":"code","e8d44760":"code","76d8faff":"code","bfa637e7":"code","81d657ef":"code","c0df16e3":"code","a449adde":"code","86a5f32c":"code","562197e7":"code","fc635f7e":"code","68a5e798":"code","3f937a23":"code","35fede03":"code","cd6fd1be":"code","5f5d9ed7":"code","3c316545":"code","9df0fc8b":"code","5271ba90":"code","c68b63f8":"code","15e9499a":"code","fba3b346":"code","8450bd85":"code","26466449":"code","d652eef7":"code","6a154bba":"code","a8165213":"code","e29be3e9":"code","dcac40a3":"code","c3ec2627":"code","bc8ac4ed":"code","42fc2eb2":"code","252ae35a":"code","49a70b2c":"code","ca2c233d":"code","4a799314":"code","63534421":"code","2778a6fc":"code","54f52320":"code","54c2117e":"code","9ef77c50":"code","26eaf70f":"code","2ab4af08":"code","47f38871":"code","a1ff66d7":"code","af6c2584":"code","7c940966":"code","482bc9b5":"code","2794cce4":"code","ab238165":"code","14a496dc":"code","c415ccd7":"code","c35bd089":"code","9b62b40e":"code","73b6d368":"code","e3fd3b52":"code","88ae9bad":"code","9e6779c9":"markdown","f2251bf2":"markdown","3c750edb":"markdown","5833a538":"markdown","a301ed3d":"markdown","e51ff731":"markdown","a70ff587":"markdown","d9a367c1":"markdown","8b409b99":"markdown","9bf1e1ff":"markdown","1ad98049":"markdown","bb0c6e2a":"markdown","5ee40240":"markdown","6b7fc22a":"markdown","4ed1145b":"markdown","5c7c9677":"markdown","c2576c02":"markdown","41d20147":"markdown","8ca0da7c":"markdown","c8233efe":"markdown","024209bd":"markdown","0011cb72":"markdown"},"source":{"170f2c0d":"# Starting with the standard imports\nimport numpy as np\nimport pandas as pd\n\n#import pandas_profiling\n\n\n# Preprocessing data\nfrom sklearn.model_selection import train_test_split     # data-splitter\nfrom sklearn.preprocessing import StandardScaler         # data-normalization\nfrom sklearn.preprocessing import PolynomialFeatures     # for polynomials\nfrom sklearn.preprocessing import PowerTransformer       # for power-transformations\nfrom sklearn.pipeline import make_pipeline               # for pipelines\nnp.random.seed (42)                                      # for reproducible results\n\n#\n# Modeling and Metrics\n# \n# --For Regressor\nfrom sklearn.dummy import DummyRegressor                 # baseline regressor (null-hypothesis)\nfrom sklearn.linear_model import LinearRegression        # linear regression\nfrom sklearn.linear_model import ( Ridge, \n                                  Lasso, \n                                  ElasticNet,\n                                 RidgeCV, \n                                 LassoCV,\n                                 ElasticNetCV)           # regularized regressions with CV\nfrom sklearn.metrics import mean_squared_error, r2_score # model-metrics\nfrom sklearn.ensemble import RandomForestRegressor\n\n#\n# For Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.dummy import (DummyClassifier, DummyRegressor)\nfrom sklearn.ensemble import RandomForestClassifier\n#\n# For clusterers\n\nfrom sklearn.svm import SVC\nfrom scipy import stats, integrate\nimport sklearn.cluster as cluster\nfrom sklearn.cluster import (DBSCAN, KMeans)\nfrom sklearn import metrics\nfrom sklearn.datasets import make_blobs\n\n# Yellowbrick\nfrom yellowbrick.features import FeatureImportances\nfrom yellowbrick.classifier import ConfusionMatrix, ClassificationReport, ROCAUC\nfrom yellowbrick.regressor import ResidualsPlot\n\n\nfrom matplotlib import colors, cm\n#import seaborn as sns\nlist_of_cmaps=['Blues','Greens','Reds','Purples']   # some colors to use\n\n# Now the Graphical libraries imports and settings\n%matplotlib inline\nimport matplotlib.pyplot as plt                          # for plotting\nimport seaborn as sns                                    # nicer looking plots\n#import altair as alt                                     # for interactive plots\nfrom matplotlib import colors                            # for web-color specs\npd.set_option('plotting.backend', 'matplotlib')          # pandas_bokeh, plotly, etc       \nplt.rcParams[ 'figure.figsize' ] = '20,10'               # landscape format figures\nplt.rcParams[ 'legend.fontsize' ] = 13                   # legend font size\nplt.rcParams[ 'axes.labelsize' ] = 13                    # axis label font size\nplt.rcParams['figure.dpi'] = 144                         # high-dpi monitors support\nplt.style.use ('ggplot')                                 # emulate ggplot style\n\n# For latex-quality, i.e., publication quality legends and labels on graphs.\n# Warning: you must have installed LaTeX on your system.\n#from matplotlib import rc\n#rc ('font', family='serif')\n#rc ('text', usetex=False) # Enable it selectively \n#rc ('font', size=16)\n\n# For youtube video display\nfrom IPython.display import YouTubeVideo\n\nimport warnings\nwarnings.filterwarnings ('ignore')  # suppress warning\n#from pandas_profiling import ProfileReport\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npath = '\/kaggle\/input\/fish-market\/'","89594235":"\nfile_name = 'Fish.csv'\nfinal_path = path + file_name\nfinal_path","18626669":"url = \"https:\/\/dnr.wisconsin.gov\/sites\/default\/files\/topic\/images\/Questions_measure.gif\"\nfrom IPython.display import Image\nfrom IPython.core.display import HTML \nprint(\"\\tWeight Predictor\")\nImage(url= url)\n","6a74dfad":"data = pd.read_csv(final_path, low_memory=False)","9ce0d421":"data.head()","e4968ddf":"data.rename(columns= {'Length1':'Length_Ver', 'Length2':'Length_Dia', 'Length3':'Length_Cro'}, inplace=True)\ndata.head()","cfc3eef6":"data.isnull().values.any()","b3ab2db2":"data.isnull().sum()","e8d44760":"def background(cell_value):\n\n    highlight = 'background-color: darkorange;'\n    default = ''\n\n    if type(cell_value) in [float, int]:\n        if cell_value == 0:\n            return highlight\n    return default\ndata.describe().T.style.applymap(background)","76d8faff":"data[data['Weight']==0]","bfa637e7":"data.drop(data[data['Weight'] == 0].index, inplace = True)","81d657ef":"corr = data.corr()\ncorr.style.background_gradient(cmap='coolwarm')\n","c0df16e3":"corr = data.corr(method='kendall')\ncorr.style.background_gradient(cmap='coolwarm')","a449adde":"g = sns.pairplot(data, kind='scatter', hue='Species', diag_kind='kde', diag_kws=dict(fill=False))\n","86a5f32c":"\nfig, ax = plt.subplots()\ncols = [ 'Weight', 'Length_Ver', 'Length_Dia', 'Length_Cro', 'Height','Width']\nvp = ax.violinplot(data[cols],\n                   showmeans=True, showextrema=True)\n\nplt.xticks(ticks=range(len(cols)+1),labels=['']+cols)\n\nplt.show()","562197e7":"ax = sns.boxplot(y=\"Species\", x=\"Weight\", data=data,  orient=\"h\")","fc635f7e":"fish = pd.get_dummies(data,columns=[\"Species\"])  \nprint(fish.shape)\nfish.head(2)","68a5e798":"x=fish.iloc[:,1:13].values     \ny= fish[\"Weight\"].values ","3f937a23":"x_train,x_test,y_train,y_test = train_test_split(x,y, train_size = 0.7, random_state = 42)","35fede03":"reg = LinearRegression()   \nreg.fit(x_train,y_train)\nscore = reg.score(x_train,y_train)   \n\nprint(score)\nprint(\"Coeffs: \", reg.coef_)\nprint(\"Interept: \",reg.intercept_)\n\npreds = reg.predict(x_test)\npred_data = {\"y_test\":y_test,\"y_pred\":preds}\npd.DataFrame(data=pred_data).head()\nr2 = r2_score(y_test,preds)\nprint(\"R2 score: \"+str(r2))\n","cd6fd1be":"fig, ax = plt.subplots(figsize=(20,10))\nax.scatter(pred_data['y_test'], pred_data['y_pred'], alpha=0.5, s=150, color='salmon')\nax.plot(pred_data['y_test'], y_test, label=\"Model Predictions\")\nax.legend(loc='best');","5f5d9ed7":"visualizer = ResidualsPlot(reg)\n\nvisualizer.fit(x_train, y_train)  # Fit the training data to the visualizer\nvisualizer.score(x_test, y_test)  # Evaluate the model on the test data\nvisualizer.show()                 # Finalize and render the figure","3c316545":"from sklearn.model_selection import cross_val_score\ncross_val_score_train = cross_val_score(reg, x_train, y_train, cv=10, scoring='r2')\nprint(cross_val_score_train)","9df0fc8b":"cross_val_score_train.mean()","5271ba90":"reg_scalar = make_pipeline(StandardScaler(), LinearRegression())\nreg_scalar.fit(x_train,y_train)\nscore = reg_scalar.score(x_train,y_train)    #Predicting score of the model on x_train,y_train\nprint(score)\n\nprint(\"Coeffs: \", reg_scalar[1].coef_)\nprint(\"Interept: \",reg_scalar[1].intercept_)\n\npreds = reg_scalar.predict(x_test)\npred_data = {\"y_test\":y_test,\"y_pred\":preds}\npd.DataFrame(data=pred_data).head()\nr2 = r2_score(y_test,preds)\nprint(\"R2 score: \"+str(r2))","c68b63f8":"from sklearn.model_selection import cross_val_score\ncross_val_score_train = cross_val_score(reg_scalar, x_train, y_train, cv=10, scoring='r2')\nprint(cross_val_score_train)","15e9499a":"cross_val_score_train.mean()","fba3b346":"degree     = 2\npolynomial = PolynomialFeatures(degree)\nx_poly = polynomial.fit_transform(x_train)\npoly_model = LinearRegression()\n# Now, train the model\npoly_model.fit(x_poly, y_train)\nprint (\"The cofficients: {}\".format(poly_model.coef_))\nx_poly_test = polynomial.fit_transform(x_test)\nyhat = poly_model.predict(x_poly_test)\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(y_test, yhat))\nr2 = r2_score(y_test, yhat)\nprint(rf\"Coefficient of Determination (R^2):{r2}\")","8450bd85":"from sklearn.model_selection import cross_val_score\ncross_val_score_train = cross_val_score(poly_model, x_poly, y_train, cv=10, scoring='r2')\nprint(cross_val_score_train)\ncross_val_score_train.mean()","26466449":"from sklearn.model_selection import cross_val_score\ncross_val_score_train = cross_val_score(poly_model, x_train, y_train, cv=10, scoring='r2')\nprint(cross_val_score_train)\ncross_val_score_train.mean()","d652eef7":"from sklearn.model_selection import cross_val_score\ncross_val_score_train = cross_val_score(poly_model, x, y, cv=10, scoring='r2')\nprint(cross_val_score_train)\ncross_val_score_train.mean()","6a154bba":"visualizer = ResidualsPlot(poly_model)\n\nvisualizer.fit(x_poly, y_train)  # Fit the training data to the visualizer\nvisualizer.score(x_poly_test, y_test)  # Evaluate the model on the test data\nvisualizer.show()                 # Finalize and render the figure","a8165213":"data.columns","e29be3e9":"cols = ['Weight', 'Length_Ver', 'Length_Dia', 'Length_Cro', 'Height','Width']\n\n\nfor col in cols:\n    data.boxplot(column=col, figsize=(5,3), vert=False)\n    plt.show()\n\nplt.show()","dcac40a3":"def outliers(df_outlier=None):\n    df_Q1 = df_outlier.quantile(0.25)\n    df_Q3 = df_outlier.quantile(0.75)\n    df_IQR = df_Q3 - df_Q1\n    df_lowerend = df_Q1 - (1.5 * df_IQR)\n    df_upperend = df_Q3 + (1.5 * df_IQR)\n    final_outliers = df_outlier[(df_outlier < df_lowerend) | (df_outlier > df_upperend)]\n    return final_outliers","c3ec2627":"outliers_weight = outliers(data['Weight'])\noutliers_weight","bc8ac4ed":"outliers_length_ver = outliers(data['Length_Ver'])\noutliers_length_ver","42fc2eb2":"outliers_length_dia = outliers(data['Length_Dia'])\noutliers_length_dia","252ae35a":"outliers_length_cro = outliers(data['Length_Cro'])\noutliers_length_cro","49a70b2c":"outlier_rows = outliers_weight + outliers_length_ver + outliers_length_dia + outliers_length_cro\noutlier_rows","ca2c233d":"outlier_rows.index","4a799314":"data.shape","63534421":"data.drop(outlier_rows.index,inplace=True)","2778a6fc":"data.shape","54f52320":"fish = pd.get_dummies(data,columns=[\"Species\"])  \nprint(fish.shape)\nfish.head()\n","54c2117e":"x=fish.iloc[:,1:13].values     \ny= fish[\"Weight\"].values ","9ef77c50":"x_train,x_test,y_train,y_test = train_test_split(x,y, train_size = 0.7, random_state = 42)","26eaf70f":"poly_model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression(fit_intercept = False))\npoly_model.fit(x_train,y_train)\nscore = poly_model.score(x_train,y_train)    #Predicting score of the model on x_train,y_train\nprint(score)\n\nprint(\"Coeffs: \", poly_model[1].coef_)\nprint(\"Interept: \",poly_model[1].intercept_)\n\npreds = poly_model.predict(x_test)\npred_data = {\"y_test\":y_test,\"y_pred\":preds}\npd.DataFrame(data=pred_data).head()\nr2 = r2_score(y_test,preds)\nprint(\"R2 score: \"+str(r2))","2ab4af08":"degree     = 2\npolynomial = PolynomialFeatures(degree)\nx_poly = polynomial.fit_transform(x_train)\npoly_model = LinearRegression()\n# Now, train the model\npoly_model.fit(x_poly, y_train)\nprint (\"The cofficients: {}\".format(poly_model.coef_))\nx_poly_test = polynomial.fit_transform(x_test)\nyhat = poly_model.predict(x_poly_test)\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(y_test, yhat))\nr2 = r2_score(y_test, yhat)\nprint(rf\"Coefficient of Determination (R^2):{r2}\")","47f38871":"from sklearn.model_selection import cross_val_score\ncross_val_score_train = cross_val_score(poly_model, x_train, y_train, cv=10, scoring='r2')\nprint(cross_val_score_train)\ncross_val_score_train.mean()","a1ff66d7":"visualizer = ResidualsPlot(poly_model)\n\nvisualizer.fit(x_poly, y_train)  # Fit the training data to the visualizer\nvisualizer.score(x_poly_test, y_test)  # Evaluate the model on the test data\nvisualizer.show()                 # Finalize and render the figure","af6c2584":"data['length_height_width'] = data['Weight'] * data['Length_Ver'] * data['Length_Dia'] * data['Length_Cro'] * data['Height']","7c940966":"data.head(5)","482bc9b5":"fish = pd.get_dummies(data,columns=[\"Species\"])  \nprint(fish.shape)\nfish.head()\n","2794cce4":"x=fish.iloc[:,6:14].values     \ny= fish[\"Weight\"].values ","ab238165":"x_train,x_test,y_train,y_test = train_test_split(x,y, train_size = 0.7, random_state = 42)","14a496dc":"reg = LinearRegression()   \nreg.fit(x_train,y_train)\nscore = reg.score(x_train,y_train)   \n\nprint(score)\nprint(\"Coeffs: \", reg.coef_)\nprint(\"Interept: \",reg.intercept_)\n\npreds = reg.predict(x_test)\npred_data = {\"y_test\":y_test,\"y_pred\":preds}\npd.DataFrame(data=pred_data).head()\nr2 = r2_score(y_test,preds)\nprint(\"R2 score: \"+str(r2))\n","c415ccd7":"fig, ax = plt.subplots(figsize=(20,10))\nax.scatter(pred_data['y_test'], pred_data['y_pred'], alpha=0.5, s=150, color='salmon')\nax.plot(pred_data['y_test'], y_test, 'r--.', label=\"Model Predictions\")\nax.legend(loc='best');","c35bd089":"visualizer = ResidualsPlot(reg)\n\nvisualizer.fit(x_train, y_train)  # Fit the training data to the visualizer\nvisualizer.score(x_test, y_test)  # Evaluate the model on the test data\nvisualizer.show()                 # Finalize and render the figure","9b62b40e":"# Polynomial on the new feature","73b6d368":"degree     = 2\npolynomial = PolynomialFeatures(degree)\nx_poly = polynomial.fit_transform(x_train)\npoly_model = LinearRegression()\n# Now, train the model\npoly_model.fit(x_poly, y_train)\nprint (\"The cofficients: {}\".format(poly_model.coef_))\nx_poly_test = polynomial.fit_transform(x_test)\nyhat = poly_model.predict(x_poly_test)\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(y_test, yhat))\nr2 = r2_score(y_test, yhat)\nprint(rf\"Coefficient of Determination (R^2):{r2}\")","e3fd3b52":"from sklearn.model_selection import cross_val_score\ncross_val_score_train = cross_val_score(poly_model, x_train, y_train, cv=10, scoring='r2')\nprint(cross_val_score_train)\ncross_val_score_train.mean()","88ae9bad":"visualizer = ResidualsPlot(poly_model)\n\nvisualizer.fit(x_poly, y_train)  # Fit the training data to the visualizer\nvisualizer.score(x_poly_test, y_test)  # Evaluate the model on the test data\nvisualizer.show()                 # Finalize and render the figure","9e6779c9":"## One Hot Encoding for the input categorical feature - Species","f2251bf2":"## Features of the dataset\n- Species: Species name of fish\n- Weight: Weight of fish in gram\n- Length1: Vertical length in cm\n- Length2: Diagonal length in cm\n- Length3: Cross length in cm\n- Height: Height in cm\n- Width: Diagonal width in cm","3c750edb":"* Correlation - Pearson vs Kendall","5833a538":"- To ensure if data scaling is required while trying data model","a301ed3d":"## Scaling data gives no benefit","e51ff731":"# Linear Regression\n","a70ff587":"# Introduction\n\nPredict the weight of the fish.","d9a367c1":"We can observe high correlation. I choose not to drop any features at this point.","8b409b99":"# Linear with Scaling the data","9bf1e1ff":"- No null values\n- Weight is found 0 on few records -> Cleaning up the records.","1ad98049":"- The model score is pretty impressive. R2 = 93.06% \n- Let's try with Residual Plot","bb0c6e2a":"# Try introduing new Features\n* Weight can be predicted based on ( Height * Length_Ver * Length_Dia * Length_Dia * Width )","5ee40240":"# Removing obvious outliers from the data","6b7fc22a":"# EDA","4ed1145b":"* Not sure which dataset to use for cross_val_score on Polynomial Regression:\n    - x_train   0.8636\n    - x_poly   -1.1625 \n    - x        -0.6959\n* **The R^2 negative is performing worst than the NULL hypothesis (straight line)**","5c7c9677":"- We can observe a significant pattern in the residual graph.\n\n- Before we continue with next model, let's try with cross validation score.","c2576c02":"# Polynomial degree 2","41d20147":"# Try the Polynomial Regression model again on the cleaned data","8ca0da7c":"# Find the pair plot of the data color coded with Species","c8233efe":"## 'Weight', 'Length_Ver', 'Length_Dia', 'Length_Cro' have obvious outliers from boxplot\nRemoving those data point","024209bd":"* The data has obvious outlier; Let's clean the outliers","0011cb72":"## Plot distribution of the Species"}}