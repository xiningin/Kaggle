{"cell_type":{"6c324b58":"code","fd8f3ac9":"code","79e384f3":"code","4a0b7da5":"code","93f18d68":"code","d67f7d39":"code","40b4ae65":"code","a917143f":"code","f0f6395a":"code","4a6ca730":"code","92d6e749":"code","27e65150":"code","090b01ca":"code","498d5363":"code","5d56befe":"code","4a778e97":"code","bbe86f98":"code","772d04e7":"code","cba3ed93":"code","fe410b8e":"code","2c06f347":"markdown","fff36e78":"markdown","0a52e002":"markdown","75f068a2":"markdown","cb5aded9":"markdown","73c32bd0":"markdown","1e594986":"markdown","478dbbea":"markdown","e2b92727":"markdown","cfc2710a":"markdown","b42b1f10":"markdown","196f3b0f":"markdown","9758c256":"markdown","b4640fc9":"markdown"},"source":{"6c324b58":"import pandas as pd\nimport os\nimport datetime\nimport numpy as np\nimport matplotlib.pyplot as plt","fd8f3ac9":"dataset_path = '\/kaggle\/input\/car-crashes-severity-prediction\/'\n\ndf_original = pd.read_csv(os.path.join(dataset_path, 'train.csv'))\n\nprint(\"The shape of the dataset is {}.\\n\\n\".format(df_original.shape))\n\ndf_original.head()","79e384f3":"def create_datetime_cols(df_original):\n    \"\"\"\n    Convert 'timestamp' column to datetime and split it to date and time features, inplace\n    \"\"\"\n    df = df_original.copy()\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['Date'] = df['timestamp'].dt.strftime('%d.%m.%Y')\n    df['Year'] = pd.DatetimeIndex(df['timestamp']).year\n    df['Month'] = pd.DatetimeIndex(df['timestamp']).month\n    df['Day'] = pd.DatetimeIndex(df['timestamp']).day\n    df['Hour'] = pd.DatetimeIndex(df['timestamp']).hour\n    df['Date_hour'] = pd.to_datetime(df[['Year', 'Month', \"Day\", \"Hour\"]])\n    \n    return df\n\ndf = create_datetime_cols(df_original)","4a0b7da5":"import xml.etree.ElementTree as et\ndef get_XML_df(xml_file, df_cols=[\"date\", \"description\"]): \n    \"\"\"\n    Parse the input XML file and store the result in a pandas DataFrame with the given columns. \n    Date and time features are also created and split as done with the train and validation data set.\n    \"\"\"\n    \n    xtree = et.parse(xml_file)\n    xroot = xtree.getroot()\n    rows = []    \n    \n    for node in xroot: \n        date = node.find(df_cols[0]).text if node is not None else None\n        description = node.find(df_cols[1]).text if node is not None else None\n        \n        rows.append({df_cols[0]: date, df_cols[1]: description})\n    \n    df_xml = pd.DataFrame(rows, columns=df_cols)\n    df_xml['Date'] = pd.to_datetime(df_xml['date'])\n    df_xml['Year'] = pd.DatetimeIndex(df_xml['Date']).year\n    df_xml['Month'] = pd.DatetimeIndex(df_xml['Date']).month\n    df_xml['Day'] = pd.DatetimeIndex(df_xml['Date']).day\n    df_xml.drop(columns=[\"date\"], inplace=True)\n        \n    return df_xml\n\ndef add_is_holiday(df, df_xml):\n    df['is_holiday'] = df['Date'].apply(lambda date: date in df_xml['Date'])","93f18d68":"df_xml = get_XML_df(os.path.join(dataset_path, 'holidays.xml'), [\"date\", \"description\"])\nadd_is_holiday(df, df_xml)\ndf.head()\ndf.shape","d67f7d39":"# Loading weather condition\ndf_weather = pd.read_csv(os.path.join(dataset_path, 'weather-sfcsv.csv'))\ndf_weather.drop(columns='Selected', inplace=True)","40b4ae65":"# Impute missing data in Wind Speed feature\nfrom sklearn.impute import KNNImputer\ndef impute_windspeed(df_weather):\n    # Drop all nulls using all features except Wind_Speed(mph)\n    df_imputed = df_weather.dropna(subset=df_weather.columns.difference(['Wind_Speed(mph)'])).copy()\n\n    # Encode Weather_Condition column\n    df_imputed = pd.get_dummies(df_imputed, columns=['Weather_Condition'], drop_first='True', prefix='Weather_Condition')\n    # Create KNNImputer object\n    imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n    df_imputed = pd.DataFrame(imputer.fit_transform(df_imputed), columns=df_imputed.columns)\n    df_weather['Wind_Speed(mph)'] = df_imputed['Wind_Speed(mph)']\n    \nimpute_windspeed(df_weather)","a917143f":"# Create a Date_hour column to use in merging later\ndf_weather['Date_hour'] = pd.to_datetime(df_weather[['Year', 'Month', \"Day\", \"Hour\"]])\n# Keep Relevant columns with unbiased values\ndf_weather = df_weather.loc[:, df_weather.columns.isin(['Date_hour', 'Weather_Condition', 'Wind_Speed(mph)'])]\ndf_weather.dropna(inplace=True)\n\n# Unite all values with the word \"Cloud.*\"\ndf_weather['Weather_Condition'] = df_weather['Weather_Condition'].apply(lambda val : val.replace(val, \"Cloudy\") if \"Cloud\" in val else val)\n\n\n# Remove duplicated rows\ndf_weather = df_weather.loc[~df_weather.duplicated('Date_hour'), :]","f0f6395a":"# Merge both datasets\ndef merge_with_weather(df, df_weather):\n    df = pd.merge(df, df_weather, how='left', on='Date_hour')\n    df.drop(columns='Date_hour', inplace=True)\n    return df\n\ndf = merge_with_weather(df, df_weather)\ndf.head()","4a6ca730":"def encode_features(df):\n    \"\"\"Encode all bool and categorical columns in the dataset\"\"\"\n    # Rename dataframe\n    df_encoded = df\n\n    # Add a feature is_holiday based on xml data\n    df_encoded.drop(columns=['timestamp'], inplace=True)\n    df_encoded.drop(columns='Date', inplace=True)\n\n    # Fetch boolean columns' names\n    bool_columns = df_encoded.select_dtypes(include='bool').columns\n    # Encode them to 0 and 1\n    df_encoded[bool_columns] = df_encoded[bool_columns].astype('int')\n\n    # Encode the \"side\" value to 0 and 1 for \"L\" and \"R\", respectively\n    df_encoded.replace({'Side': {'L': 0, 'R': 1}}, inplace=True)\n    \n    # Encode features from weather dataset\n    df_encoded = pd.get_dummies(df, columns=['Weather_Condition'], drop_first='True', prefix='Weather_Condition')\n    \n    return df_encoded\n\ndf = encode_features(df)\ndf.shape","92d6e749":"# df_eda = df[df['Severity'].isin([1, 4])]\ndf_eda = df.copy()\ndf_eda = df_eda.select_dtypes(exclude='float64')\ndf_eda.drop(columns=[\"ID\"], inplace=True)\n#Performing Chi-Square, test of independence\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import chi2\n\ndef independence_test(table):\n    # interpret test-statistic\n    stat, p, dof, expected = chi2_contingency(table)\n    # interpret p-value\n    alpha = 0.05\n    if p <= alpha:\n        print('Dependent (reject H0)')\n    else:\n        print('Independent (fail to reject H0)')\n        \nfor col_name, col_val in df_eda.iteritems():\n    table = pd.crosstab(index=df_eda['Severity'], columns=col_val)\n    print(table)\n    print('Testing Dependency ', col_name)\n    independence_test(table)\n    print('')\n","27e65150":"def clean_data(df):\n    \"\"\"Function drops unwanted or biased features and null values\"\"\"\n    df.drop(list(df.filter(regex = '^(?!.+(Fair|Overcast)$)^Weather_Condition_.*$')), axis = 1, inplace = True)\n    df.drop(columns=[\n        'No_Exit', 'Bump', 'Roundabout', 'is_holiday', \n        \"Give_Way\", 'Wind_Speed(mph)'\n    ], inplace=True)\n    df.dropna(inplace=True)\n    \nclean_data(df)","090b01ca":"from sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Severity']) # Try adding `stratify` here\n\nX_train = train_df.drop(columns=['ID', 'Severity'])\ny_train = train_df['Severity']\n\nX_val = val_df.drop(columns=['ID', 'Severity'])\ny_val = val_df['Severity']","498d5363":"from sklearn.ensemble import RandomForestClassifier\n\n# Create an instance of the classifier\nclassifier = RandomForestClassifier(max_depth=2, random_state=0)\n\n# Train the classifier\nclassifier = classifier.fit(X_train, y_train)","5d56befe":"unique_elements, counts_elements = np.unique(classifier.predict(X_val), return_counts=True)\nprint(\"Frequency of unique values of the said array:\")\nprint(np.asarray((unique_elements, counts_elements)))\nprint(\"The accuracy of the classifier on the validation set is \", (classifier.score(X_val, y_val)))","4a778e97":"#Find most important features to the model\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfeature_imp = pd.Series(classifier.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\n\nplt.rcParams['figure.figsize'] = [18, 6]\nplt.rcParams['figure.dpi'] = 100\n\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.show()","bbe86f98":"test_df = pd.read_csv(os.path.join(dataset_path, 'test.csv'))","772d04e7":"# Prepare Test data set\ntest_df = create_datetime_cols(test_df)\nadd_is_holiday(test_df, df_xml)\ndf_xml = get_XML_df(os.path.join(dataset_path, 'holidays.xml'), [\"date\", \"description\"])\nadd_is_holiday(test_df, df_xml)\ntest_df = merge_with_weather(test_df, df_weather)\ntest_df = encode_features(test_df)","cba3ed93":"X_test = test_df.drop(columns=['ID'])\nclean_data(X_test)\n\ny_test_predicted = classifier.predict(X_test)\ntest_df['Severity'] = y_test_predicted","fe410b8e":"test_df[['ID', 'Severity']].to_csv('\/kaggle\/working\/submission.csv', index=False)","2c06f347":"## Model Training\n\nLet's train a model with the data! We'll train a Random Forest Classifier to demonstrate the process of making submissions. ","fff36e78":"The remaining steps is to submit the generated file and are as follows. \n\n1. Press `Save Version` on the upper right corner of this notebook.\n2. Write a `Version Name` of your choice and choose `Save & Run All (Commit)` then click `Save`.\n3. Wait for the saved notebook to finish running the go to the saved notebook.\n4. Scroll down until you see the output files then select the `submission.csv` file and click `Submit`.\n\nNow your submission will be evaluated and your score will be updated on the leaderboard! CONGRATULATIONS!!","0a52e002":"## Exploratory Data Analysis","75f068a2":"## Data Splitting","cb5aded9":"## Load and prepare Weather data","73c32bd0":"## Impute Missing Data","1e594986":"## Submission File Generation","478dbbea":"## Encode Features","e2b92727":"## Import the libraries","cfc2710a":"## Clean Weather data\n","b42b1f10":"## Chi-Square test\nUse Chi-square independence test to determine which features to keep","196f3b0f":"## Load xml holidays data","9758c256":"Now we're ready to generate the submission file. The submission file needs the columns `ID` and `Severity` only.","b4640fc9":"Now let's test our classifier on the validation dataset and see the accuracy."}}