{"cell_type":{"e1597ee2":"code","cabab57a":"code","f7ea024d":"code","aade4943":"code","3e29f7aa":"code","e77c54f4":"code","c87d9f8e":"code","8554d718":"code","26b539e1":"code","89e67934":"code","911c5e44":"code","7c254c2c":"code","745fdf5e":"code","eee263c3":"code","47e13712":"code","3761e002":"code","9fa3550c":"code","15a2b69e":"code","732b96d7":"code","aaa0d112":"code","5ac6d797":"code","95418a08":"code","be395e94":"code","e8a4716b":"code","3a7265c2":"code","2726210e":"code","3a5ee5bd":"code","47540d78":"code","b137b819":"code","8acb0751":"code","2a9d395d":"code","66a8af16":"code","07152e4e":"code","5971479a":"code","cd8a1e1c":"code","d4996fab":"code","2ca9bfb6":"code","f7aed163":"code","13928410":"code","ea7accf8":"code","b36a61eb":"code","3754364d":"code","290e10dc":"code","b35106f1":"code","fb005891":"markdown","8300574d":"markdown","58b60a05":"markdown","186d5f35":"markdown","a145062c":"markdown","567c67e6":"markdown","106a3571":"markdown","aa01447f":"markdown","cc8bbab5":"markdown","de7d664d":"markdown","c6536b83":"markdown","54574562":"markdown","43b98e3f":"markdown","adc4de18":"markdown","bc971d00":"markdown","609e1f2f":"markdown","13a15e75":"markdown","a79a8719":"markdown","b0ce902b":"markdown","9de19a8b":"markdown","4fb31530":"markdown"},"source":{"e1597ee2":"import pandas as pd \nimport numpy as np\nimport re \n\nimport nltk\n#nltk.download('punkt')\n\n# model imports\nfrom gensim.models.word2vec import Word2Vec\nimport gensim.downloader as api # we need an internet connection for this one \n\nfrom sklearn.model_selection import train_test_split\n\n# visualization imports\nimport umap # nonlinear dimensionality reduction\nimport matplotlib.pyplot as plt\n%matplotlib inline","cabab57a":"our_special_word = 'qwerty'\n\ndef remove_ascii_words(df):\n    \"\"\" removes non-ascii characters from the 'texts' column in df.\n    It returns the words containig non-ascii characers.\n    \"\"\"\n    non_ascii_words = []\n    for i in range(len(df)):\n        for word in df.loc[i, 'excerpt'].split(' '):\n            if any([ord(character) >= 128 for character in word]):\n                non_ascii_words.append(word)\n                df.loc[i, 'excerpt'] = df.loc[i, 'excerpt'].replace(word, our_special_word)\n    return non_ascii_words","f7ea024d":"def get_good_tokens(sentence):\n    replaced_punctation = list(map(lambda token: re.sub('[^0-9A-Za-z!?]+', '', token), sentence))\n    removed_punctation = list(filter(lambda token: token, replaced_punctation))\n    return removed_punctation","aade4943":"def w2v_preprocessing(df):\n    \"\"\" All the preprocessing steps for word2vec are done in this function.\n    All mutations are done on the dataframe itself. So this function returns\n    nothing.\n    \"\"\"\n    df['excerpt'] = df.excerpt.str.lower()\n    df['document_sentences'] = df.excerpt.str.split('.')  # split texts into individual sentences\n    df['tokenized_sentences'] = list(map(lambda sentences:\n                                         list(map(nltk.word_tokenize, sentences)),\n                                         df.document_sentences))  # tokenize sentences\n   # df['tokenized_sentences'] = list(map(lambda sentences: list(map( ,sentences)), df.tokenized_sentences))\n    df['tokenized_sentences'] = list(map(lambda sentences:\n                                         list(map(get_good_tokens, sentences)),\n                                         df.tokenized_sentences))  # remove unwanted characters\n    df['tokenized_sentences'] = list(map(lambda sentences:\n                                         list(filter(lambda lst: lst, sentences)),\n                                         df.tokenized_sentences))  # remove empty lists","3e29f7aa":"train_data =  pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain_data.head()","e77c54f4":"train_data.excerpt = train_data['excerpt'].apply(str)\nnon_ascii_words = remove_ascii_words(train_data)\n\nprint(\"Replaced {} words with characters with an ordinal >= 128 in the test data.\".format(\n    len(non_ascii_words)))","c87d9f8e":"W2Vmodel = api.load('word2vec-google-news-300')","8554d718":"w2v_preprocessing(train_data)","26b539e1":"train_data.drop(train_data[train_data.tokenized_sentences.str.len() == 0].index, inplace= True) ","89e67934":"#create dictionary with all sentences\nsentences = []\nfor sentence_group in train_data.tokenized_sentences:\n    sentences.extend(sentence_group)\n\nprint(\"Number of sentences: {}.\".format(len(sentences)))\nprint(\"Number of texts: {}.\".format(len(train_data)))","911c5e44":"def get_w2v_features(w2v_model, sentence_group):\n    \"\"\" Transform a sentence_group (containing multiple lists\n    of words) into a feature vector. It averages out all the\n    word vectors of the sentence_group.\n    \"\"\"\n    words = np.concatenate(sentence_group)  # words in text\n    index2word_set = set(w2v_model.index_to_key) # set(w2v_model.wv.vocab.keys())  # words known to model\n    \n    featureVec = np.zeros(w2v_model.vector_size, dtype=\"float32\")\n    \n    # Initialize a counter for number of words in a review\n    nwords = 0\n    # Loop over each word in the comment and, if it is in the model's vocabulary, add its feature vector to the total\n    for word in words:\n        if word in index2word_set: \n            featureVec = np.add(featureVec, w2v_model[word])\n            nwords += 1.\n\n    # Divide the result by the number of words to get the average\n    if nwords > 0:\n        featureVec = np.divide(featureVec, nwords)\n    return featureVec","7c254c2c":"train_data['w2v_features'] = list(map(lambda sen_group:\n                                     get_w2v_features(W2Vmodel, sen_group),\n                                     train_data.tokenized_sentences))\n\ntrain_data[\"w2v_resh_features\"] = train_data[\"w2v_features\"].apply(lambda x : x.reshape(1,-1) )","745fdf5e":"# save w2v features\n#train_data.to_csv(\"w2v_features.csv\")","eee263c3":"arr_w2v = train_data.w2v_resh_features[0]\nfor i in range(1, len(train_data)):\n    arr_w2v = np.vstack((arr_w2v, train_data.w2v_resh_features[i]))","47e13712":"umap_emb = umap.UMAP(n_neighbors= 15, n_components = 2, target_metric = 'l1' , n_epochs = 500).fit_transform(arr_w2v, y=train_data.target)","3761e002":"fig, ax = plt.subplots(1, figsize=(14, 10))\nplt.scatter(*umap_emb.T, s=3, c=train_data.target, cmap='Spectral', alpha=1.0)\nplt.setp(ax, xticks=[], yticks=[])\nax.patch.set_facecolor('black')\nfg_color = 'black'\ncbar = plt.colorbar()\nplt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color=fg_color)","9fa3550c":"X_train, X_val, y_train, y_val = train_test_split(\n    arr_w2v, train_data[[\"target\", \"standard_error\"]].values, test_size=0.10, random_state=42)","15a2b69e":"mapper = umap.UMAP(n_neighbors= 15, n_components = 2, target_metric = 'l1' , n_epochs = 1000).fit(X_train, y=y_train[:,0])","732b96d7":"val_embedding = mapper.transform(X_val)","aaa0d112":"fig, ax = plt.subplots(1, figsize=(14, 10))\nplt.scatter(*mapper.embedding_.T, s=3, c=y_train[:,0], cmap='Spectral', alpha=1.0)\nplt.setp(ax, xticks=[], yticks=[])\nax.patch.set_facecolor('black')\nfg_color = 'black'\ncbar = plt.colorbar()\nplt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color=fg_color)","5ac6d797":"fig, ax = plt.subplots(1, figsize=(14, 10))\nplt.scatter(*val_embedding.T, s=3, c=y_val[:,0], cmap='Spectral', alpha=1.0)\nplt.setp(ax, xticks=[], yticks=[])\nax.patch.set_facecolor('black')\nfg_color = 'black'\ncbar = plt.colorbar()\nplt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color=fg_color)","95418a08":"import xgboost as xgb\nfrom sklearn.metrics import mean_squared_error as mse","be395e94":"xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.5, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 1000, verbosity = 1)\n\nxg_reg.fit(X_train,y_train[:,0])","e8a4716b":"preds = xg_reg.predict(X_val)","3a7265c2":"np.sqrt(mse(y_val[:,0], preds)) ","2726210e":"def augment_train_w2v(X_train, y_train, y_std, times = 5):\n    \n    augmented_w2v_X = X_train.copy()\n    augmented_w2v_y = y_train.copy()\n    \n    for j in range(0, times - 1):\n        for i in range(0, X_train.shape[0]):\n        \n            new_w2v = X_train[i,:] + np.random.uniform(1,8)*1e-4*np.random.randn(300)       # np.random.uniform(1,8)*1e-3*np.random.randn(300)\n            augmented_w2v_X = np.vstack((augmented_w2v_X, new_w2v))\n            \n            new_y = y_train[i] + np.random.choice([-1, 1])*y_std[i]*0.05\n            augmented_w2v_y = np.append(augmented_w2v_y, new_y)\n        \n    return augmented_w2v_X, augmented_w2v_y","3a5ee5bd":"aug_X_train, aug_Y_train = augment_train_w2v(X_train, y_train[:,0], y_train[:,1], times = 5)","47540d78":"def shuffle_couples(a, b):\n    assert len(a) == len(b)\n    p = np.random.permutation(len(a))\n    return a[p], b[p]","b137b819":"aug_X_train, aug_Y_train = shuffle_couples(aug_X_train, aug_Y_train)","8acb0751":"mapper_2 = umap.UMAP(n_neighbors= 15, n_components = 2, min_dist = 0.1, target_metric = 'l1' , n_epochs = 1000).fit(aug_X_train)#, y=aug_Y_train)","2a9d395d":"val_embedding_2 = mapper_2.transform(X_val)","66a8af16":"fig, ax = plt.subplots(1, figsize=(14, 10))\nplt.scatter(*mapper_2.embedding_.T, s=3, c=aug_Y_train, cmap='Spectral', alpha=1.0)\nplt.setp(ax, xticks=[], yticks=[])\nax.patch.set_facecolor('black')\nfg_color = 'black'\ncbar = plt.colorbar()\nplt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color=fg_color)","07152e4e":"fig, ax = plt.subplots(1, figsize=(14, 10))\nplt.scatter(*val_embedding_2.T, s=3, c=y_val[:,0], cmap='Spectral', alpha=1.0)\nplt.setp(ax, xticks=[], yticks=[])\nax.patch.set_facecolor('black')\nfg_color = 'black'\ncbar = plt.colorbar()\nplt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color=fg_color)","5971479a":"xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.5, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 1000, verbosity = 1)\n\nxg_reg.fit(aug_X_train, aug_Y_train)","cd8a1e1c":"preds = xg_reg.predict(X_val)","d4996fab":"np.sqrt(mse(y_val[:,0], preds))","2ca9bfb6":"test_data =  pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\ntest_data.head()","f7aed163":"test_data.excerpt = test_data['excerpt'].apply(str)\nnon_ascii_words = remove_ascii_words(test_data)\n\nprint(\"Replaced {} words with characters with an ordinal >= 128 in the test data.\".format(\n    len(non_ascii_words)))","13928410":"w2v_preprocessing(test_data)","ea7accf8":"test_data.drop(test_data[test_data.tokenized_sentences.str.len() == 0].index, inplace= True) ","b36a61eb":"#create dictionary with all sentences\nsentences_test = []\nfor sentence_group in test_data.tokenized_sentences:\n    sentences_test.extend(sentence_group)\n\nprint(\"Number of sentences: {}.\".format(len(sentences)))\nprint(\"Number of texts: {}.\".format(len(test_data)))","3754364d":"test_data['w2v_features'] = list(map(lambda sen_group:\n                                     get_w2v_features(W2Vmodel, sen_group),\n                                     test_data.tokenized_sentences))","290e10dc":"test_data[\"w2v_resh_features\"] = test_data[\"w2v_features\"].apply(lambda x : x.reshape(1,-1) )","b35106f1":"arr_w2v_test = test_data.w2v_resh_features[0]\nfor i in range(1, len(test_data)):\n    arr_w2v_test = np.vstack((arr_w2v_test, test_data.w2v_resh_features[i]))","fb005891":"## Validation Set\n\nLet's use ```scikit-learn```'s ```train_test_split``` method in order to extract a validation set and try to see whether a semi-supervised ```UMAP``` can capture some structures.","8300574d":"# Test Data\n\nPreparing submission with test data by basically re-doing the previously seen operations.","58b60a05":"### Pretrained Word2Vec\nHere we can load a pretrained version of Word2Vec (on Google News) from ```Gensim```'s API. Each vector has 300 components","186d5f35":"Apparently the structure is affected by this augmentation strategy. After that we can take a look at the ```rmse``` score, maybe we can get a better result","a145062c":"# W2V Naive Augmentation \n\nIn this section, we'll try to instantiate a naive augmentation pipeline for Word2Vec features. The rationale behind the following strategy is that W2V features are essentially vectors so we can augment our \"feature set\" by perturbing these vectors with a small gaussian noise, in order to facilitate the learning procedure for our regressor of choice. Moreover, we'd like to see if by augmenting our \"feature set\" in this way we can **break** the structure extracted by the supervised ```UMAP```.\n\nOf course, this naive augmentation doesn't make a lot of sense from a word\/sentence perspective\nsince it might be that the perturbed vectors don't correspond to any word\/sentence themselves.\n\nWith the following method, ```augment_train_w2v```, we augment the training set by adding a small gaussian noise on w2v's features while slightly modifying (by exploiting the **standard deviation** of each target) the new target variable corresponding to the augmented sample.","567c67e6":"~0.69 not bad, let's see on test :) ","106a3571":"Let's take a look at what we get by plotting this new 2-d space.","aa01447f":"### Data Import\nLet's load training data as a ```pandas``` dataframe","cc8bbab5":"# CommonLit Readability Challenge - Word2Vec + UMAP \ud83d\uddfa\ufe0f\n## Introduction \n\nIn this notebook we are going to see how to apply a pretrained [Word2Vec](https:\/\/jalammar.github.io\/illustrated-word2vec\/) model (from gensim) to a text corpus of our choice and,after that, we are going to see if our embedding has captured readability by employing a dimensionality reduction algorithm named [UMAP](https:\/\/umap-learn.readthedocs.io\/en\/latest\/how_umap_works.html).\n\n``` If you're trying to locally install umap, pay attention to the fact that the correct command is \"pip install umap-learn\", indeed it is \"import umap\" while importing it```.\n","de7d664d":"Extracting Word2Vec features as a dataframe column. This cell takes quite a lot of time (~1 hr), it might be useful to parallelize the feature extraction process, you know nested ```for``` loops are always problematic in Python \ud83d\udc0d","c6536b83":"Converting Word2Vec features to ```numpy.ndarray``` for visualization purposes with ```UMAP```\n","54574562":"## Feature Extraction\n\nThe following function, ```get_w2v_features``` transforms each sentence into a feature vector by averaging words vectors. In this way we can take into account the different length of each sentence, on the other side, by averaging on different words we will lose some word-specific information.","43b98e3f":"# Dimensionality Reduction and Visualization via UMAP\n\n```UMAP``` is a nonlinear dimensionality reduction technique, something like ```PCA``` but more fancy! \nIt can be used for both *unsupervised* and *supervised* problems and also both for *regression* and *classification* problems.   \n\nSince we are facing a regression problem, i.e. our target variable is continuous, we have to impose ```target_metric = 'l1'``` (thanks Leland McInnes, here a [github issue ](https:\/\/github.com\/lmcinnes\/umap\/issues\/257) as motivation). \nWe opted for a 2-dimensional representation (```n_components = 2```) ","adc4de18":"Now let's see if the previously seen structure still exist ( you can try to change the parameter ```times``` of ```augment_train_w2v``` to see weather it affects something or not) ","bc971d00":"# XGBoost on W2V features\n\nHere we apply a ```XGBoost``` regressor on our Word2Vec features,  in order to be ready for submission","609e1f2f":"Okay, in a supervised setting ```UMAP``` is able to isolate this structure, i.e. it seems that there exist a specific subspace which is kinda parametrized by the **readability target**. Is it possible to find a similar structure in a semi-supervised or unsupervised setting? Let's dive in \ud83c\udfca\ud83e\udd3f","13a15e75":"Unfortunately the result is not what we wanted it to be. Anyway, a further investigation of the supervised representation extracted by ```UMAP``` is worth to be done even by considering it for augmentation strategies","a79a8719":"## Preprocessing\n\nHere we apply our utility functions for preprocessing","b0ce902b":"### Utilities\nHere some utility functions: \n* ```remove_ascii_words``` it might be useless in this case but it's quite useful for general purpose preprocessing, where we don't know whether there are non-ascii characters around\n\n* ```get_good_tokens``` removes useless punctuation\n\n* ``` w2v_preprocessing``` all the necessary preprocessing for gensim Word2Vec model, we basically divide each document into individual sentences, remove punctuation by using ```get_good_tokens```, tokenize every sentence and remove empty lists. \n","9de19a8b":"The result is pretty much the same, we need to devise a better augmentation strategy, maybe by exploiting the information gained with ```UMAP```\n","4fb31530":"We decided to shuffle again the new augmented training set by keeping the information on the sample-target couple. That's what the following method ```shuffle_couples``` does. "}}