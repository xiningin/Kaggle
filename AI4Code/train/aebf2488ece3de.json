{"cell_type":{"f5763d82":"code","7b6e53c1":"code","e4943426":"code","2ab8ebd6":"code","8a47f64d":"code","a38aff89":"code","58e14677":"code","f9602a4e":"code","f2ef8030":"code","6d63480c":"code","24e0dc6b":"code","c9cfca0e":"code","9ba0b7f6":"code","a86bbda4":"code","9500e931":"code","4a4d6fb7":"code","71dc47c1":"code","f4155f44":"code","ec70c661":"code","e94e233b":"code","5398b134":"code","0e61d2ad":"code","1cc31f36":"code","9c96da51":"code","0cd0c560":"code","7b957847":"code","70005f88":"markdown","d27064e5":"markdown","6c68c3f2":"markdown","f955a5e5":"markdown","1bf3deb4":"markdown","4fc8c8ee":"markdown","9fb99d5b":"markdown","54707889":"markdown","292c6917":"markdown","f9e45f82":"markdown","42352ab5":"markdown","8bf905de":"markdown"},"source":{"f5763d82":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# NLP libraries to clean the text data\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport re\n\n# Vectorization technique TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# For Splitting the dataset\nfrom sklearn.model_selection import train_test_split\n\n# Model libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Accuracy measuring library\nfrom sklearn.metrics import accuracy_score\n","7b6e53c1":"file_path = '..\/input\/fake-news-detection\/data.csv'\ndata = pd.read_csv(file_path)","e4943426":"data.shape #Returns the number of rows and columns present in the dataset","2ab8ebd6":"data.head()  # Returns the first 5 rows of the dataset","8a47f64d":"data.columns # Returns the column headings","a38aff89":"data.isnull().sum() #To check the null values in the dataset, if any","58e14677":"df = data.copy() #Creating a copy of my data, I will be working on this Dataframe","f9602a4e":"df['Body'] = df['Body'].fillna('')   # As Body is empty, just filled with an empty space","f2ef8030":"df.isnull().sum()  # No null values found","6d63480c":"df['News'] = df['Headline']+df['Body']","24e0dc6b":"df.head()","c9cfca0e":"df.columns","9ba0b7f6":"features_dropped = ['URLs','Headline','Body']\ndf = df.drop(features_dropped, axis =1)","a86bbda4":"df.columns","9500e931":"ps = PorterStemmer()\ndef wordopt(text):\n    text = re.sub('[^a-zA-Z]', ' ',text)\n    text = text.lower()\n    text = text.split()\n    text = [ps.stem(word) for word in text if not word in stopwords.words('english')]\n    text = ' '.join(text)\n    return text","4a4d6fb7":"df['News'] = df['News'].apply(wordopt) #Applying the text processing techniques onto every row data","71dc47c1":"df.head()","f4155f44":"X = df['News']\nY = df['Label']\n\n#Split the data into training and test set\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)","ec70c661":"#Vectorization\nvectorization = TfidfVectorizer()\nxv_train = vectorization.fit_transform(x_train)\nxv_test = vectorization.transform(x_test)","e94e233b":"#1. Logistic Regression - used because this model is best suited for binary classification\nLR_model = LogisticRegression()\n\n#Fitting training set to the model\nLR_model.fit(xv_train,y_train)\n\n#Predicting the test set results based on the model\nlr_y_pred = LR_model.predict(xv_test)\n\n#Calculate the accurracy of this model\nscore = accuracy_score(y_test,lr_y_pred)\nprint('Accuracy of LR model is ', score)","5398b134":"#2. Support Vector Machine(SVM) - SVM works relatively well when there is a clear margin of separation between classes.\nsvm_model = SVC(kernel='linear')\n\n#Fitting training set to the model\nsvm_model.fit(xv_train,y_train)\n\n#Predicting the test set results based on the model\nsvm_y_pred = svm_model.predict(xv_test)\n\n#Calculate the accuracy score of this model\nscore = accuracy_score(y_test,svm_y_pred)\nprint('Accuracy of SVM model is ', score)","0e61d2ad":"#3. Random Forest Classifier \nRFC_model = RandomForestClassifier(random_state=0)\n\n#Fitting training set to the model\nRFC_model.fit(xv_train, y_train)\n\n#Predicting the test set results based on the model\nrfc_y_pred = RFC_model.predict(xv_test)\n\n#Calculate the accuracy score of this model\nscore = accuracy_score(y_test,rfc_y_pred)\nprint('Accuracy of RFC model is ', score)","1cc31f36":"# As SVM is able to provide best results - SVM will be used to check the news liability\n\ndef fake_news_det(news):\n    input_data = {\"text\":[news]}\n    new_def_test = pd.DataFrame(input_data)\n    new_def_test[\"text\"] = new_def_test[\"text\"].apply(wordopt) \n    new_x_test = new_def_test[\"text\"]\n    #print(new_x_test)\n    vectorized_input_data = vectorization.transform(new_x_test)\n    prediction = svm_model.predict(vectorized_input_data)\n    \n    if prediction == 1:\n        print(\"Not a Fake News\")\n    else:\n        print(\"Fake News\")","9c96da51":"fake_news_det('U.S. Secretary of State John F. Kerry said Monday that he will stop in Paris later this week, amid criticism that no top American officials attended Sunday\u00e2\u20ac\u2122s unity march against terrorism.')","0cd0c560":"fake_news_det(\"\"\"The second Covid-19 wave in India is now on the \"downswing,\" the Centre said on Thursday, highlighting that the current number of active cases is still \"very high\" and advised states and Union territories (UTs) to not let down their guards.\"\"\")","7b957847":"fake_news_det(\"JetNation FanDuel League; Week 4 of readers think this story is Fact. Add your two cents.(Before Its News)Our FanDuel league is back again this week. Here are the details:$900 in total prize money. $250 to the winner. $10 to enter.Remember this is a one week league, pick your lineup against the salary cap and next week if you want to play again you can pick a completely different lineup if you want.Click this link to enter \u2014 http:\/\/fanduel.com\/JetNation You can discuss this with other NY Jets fans on the Jet Nation message board. Or visit of on Facebook.Source: http:\/\/www.jetnation.com\/2017\/09\/27\/jetnation-fanduel-league-week-4\/\")","70005f88":"## 3.Data-Preprocessing\n\nFor further analysis, cleaning of data is necessary. \nIn this Notebook, I will be doing 3 stages of data cleaning:\n1. Removing the Null values\n2. Adding a new field\n3. Drop features that are not needed\n4. Text Processing","d27064e5":"## 7. Manual Model Testing","6c68c3f2":"## 3.3. Drop features that are not needed","f955a5e5":"## 6. Model Fitting\nI will be fitting my data onto 3 classifications models\n1. Logistic Regression\n2. SVM\n3. RandomForestClassifier\n\nThe best one amongst the 3 will be used further","1bf3deb4":"## Splitting DataSet","4fc8c8ee":"## 3.4. Text Processing\n1. Remove symbols(',','-',...etc)\n2. Remove stop words\n3. Stemming","9fb99d5b":"## 3.2. Adding a new column\nFor ease of implementation, I combined Headline and Body Column and created a new column 'News' ","54707889":"## 2. Loading the data","292c6917":"## 5. Vectorization\n\nThis is used to handle our text data, by converting it into vectors.","f9e45f82":"## 1. Import Libraries","42352ab5":"Hello All, This is my very first notebook. I'm a novice in Machine Learning therefore, when you will go through this notebook, you may not find anything interesting. But,I'm very excited to share my understanding on FakeNewsDetction Dataset. I would really appreciate your suggestions. Happy Learning !!\n\nLittle description of this notebook: This is mainly focusing on Natural Language Processing techniques such as removal of stopwords and stemming. Suggestions to innovate this would be really appresciated. :)","8bf905de":"## 3.1. Removing the Null Values\n\nAs Body field has some empty fields, it can be handled in two ways:\n1. Drop the 21 rows\n2. Replace the null value with a dummy string\n\nHere, I will be going with the 2nd option, because although dropping 21 rows would not affect the accuracy, as it is just a minute portion of our large dataset, it is never recommended.\n\nI will be replacing the Null(Nan) values in 'Body' field with an empty string ('')"}}