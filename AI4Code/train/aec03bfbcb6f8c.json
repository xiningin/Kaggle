{"cell_type":{"eb32f934":"code","d104c2fc":"code","43b41e0b":"code","f2ff3395":"code","d740402c":"code","e58335b9":"code","eb8cf2a1":"code","90356dd5":"code","39668007":"code","26e85e5a":"markdown","bb23a6af":"markdown","cfa54837":"markdown","e8eee8c1":"markdown","681a7264":"markdown","95d41412":"markdown","d90becc5":"markdown","58fb568b":"markdown","fde09f4d":"markdown","d04186cd":"markdown","a3f435ad":"markdown","425698cb":"markdown","7b63a362":"markdown","e8cea652":"markdown","ec3708e8":"markdown","d3514779":"markdown","bb28529b":"markdown","bec09381":"markdown","afdf53dd":"markdown","3a2f1d09":"markdown","c46eae83":"markdown","64a8b1dd":"markdown","8985a470":"markdown"},"source":{"eb32f934":"import time\nimport csv\nimport os.path\nimport numpy as np \nimport pandas as pd \nimport requests \nfrom bs4 import BeautifulSoup ","d104c2fc":"def request_with_check(url):\n    \n    headers = {'User-Agent': 'Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/50.0.2661.102 Safari\/537.36 , For: a Tutorial kernel By: elamraoui sohayb'}\n    page_response = requests.get(url, headers=headers, timeout=60)\n    if page_response.status_code>299:\n        raise AssertionError(\"page content not found, status: %s\"%page_response.status_code)\n    \n    return page_response    ","43b41e0b":"page_test = request_with_check('https:\/\/www.investing.com\/news\/commodities-news')\n# Cheking the first 5000 charchters of the HTML code\npage_test.text[:5000]","f2ff3395":"def get_details(single_article):\n    \n    # A title is in <a><\/a> with the 'class' attribute set to: title\n    title = single_article.find('a',{'class':'title'})\n\n    # A safeguard against some empty articles in the deeper pages of the site\n    if title == None:\n        #print('Empty Article')\n        return None\n    \n    # the link to an article is the Href attribute\n    link = title['href']\n    \n    # A safeguarde against embedded Advertisment articles\n    if (('\/news\/'and category_name) not in link):\n        #print('Ad Article found')\n        return None       \n        \n    title = title.text\n    \n    # The first Paragraph is in <p><\/p>\n    first_p = single_article.find('p').text\n    \n    # the Source is in <span><\/span>, with Class == articleDetails\n    source_tag = single_article.find_all('span',{'class':'articleDetails'})\n    source = str(source_tag[0].span.text)\n    \n    #date is also in <span><\/span> withe the Class == date\n    date = single_article.find('span',{'class':'date'}).text\n    \n    return title, link, first_p, source, date  ","d740402c":"def single_page(Url_page,page_id = 1):\n\n    news_list = []\n\n    #Making the Http request\n    page = request_with_check(Url_page)\n    \n    #Calling the Html.parser to start extracting our data\n    html_soup = BeautifulSoup(page.text, 'html.parser')\n    \n    # The Articles Class\n    articles = html_soup.find('div',{'class':'largeTitle'})\n    \n    # The single Articles List\n    articleItems = articles.find_all('article' ,{'class':'articleItem'})\n\n    # Looping, for each single Article\n    for article in articleItems:\n        if get_details(article) == None:\n            continue\n        \n        title, link, first_p, source_tag, date = get_details(article)\n        news_list.append({'id_page':page_id,\n                          'title':title,   \n                          'date':date,\n                          'link': link,\n                          'source':source_tag,\n                          'first_p':first_p})\n\n    return news_list","e58335b9":"def dict_to_csv (filename,news_dict):\n    \n    #Setting the Dataframe headers\n    fields = news_dict[0]\n    fields = list(fields.keys())\n    \n    #Checking if the file already exists, if Exists we woulb pe appending, if Not we creat it\n    has_header = False\n    if os.path.isfile(filename):\n        with open(filename, 'r') as csvfile:\n            sniffer = csv.Sniffer()\n            has_header = sniffer.has_header(csvfile.read(2048))\n    \n    with open(filename, 'a',errors = 'ignore', encoding= 'utf-8') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fields)\n        if(has_header == False):\n            writer.writeheader()  \n        for row in range(len(news_dict)):\n            item = news_dict[row]\n            writer.writerow(item)","eb8cf2a1":"def parsing_category_pages(category_name,base_url,number_pages):\n    start_time = time.time()\n    \n    #getting the start page\n    page = request_with_check(base_url)\n\n    #Calling the Html Parser\n    html_soup = BeautifulSoup(page.text, 'html.parser')\n    \n    #Finding the Laste page\n    last_page = int(html_soup.findAll(class_='pagination')[-1].text)\n\n    if number_pages > last_page:\n        number_pages = last_page\n\n    #Looping over the specified nupber of Pages:\n    for p in range(1,number_pages,1):\n        category_page = base_url+'\/'+str(p)\n        print('Parsing: ',category_page)\n        page_news = single_page(category_page,p)\n        \n        #Saving to a CSV\n        dict_to_csv(category_name+'.csv',page_news)\n        \n        #Time sleep\n        time.sleep(10)\n    \n    print(\"--- %s seconds ---\" % (time.time() - start_time))\n    return True","90356dd5":"URL = 'https:\/\/www.investing.com\/news\/'\ncategory_name = 'commodities-news'\nbase_url = URL+category_name\nparsing_category_pages ('commodities-news',base_url,number_pages=5)","39668007":"data = pd.read_csv('..\/working\/commodities-news.csv')\ndata.head(100)","26e85e5a":"## <a id=\"7\">Up next: Starting our NLP pipline for this dataset<\/a>\n\n###                             (Coming Soon .............. )","bb23a6af":"#### Testing:","cfa54837":"## <a id=\"4\">  The Code, Finally! : <\/a>\n### <a id=\"4.1\">Environment and tools: <\/a>\n- My default environment setup is **Python 3.5** kernel in a **Jupyter Notebook**\n- We will be using: [Requests](http:\/\/docs.python-requests.org\/en\/latest\/) and [Beautifullsoup](https:\/\/www.crummy.com\/software\/BeautifulSoup\/bs4\/doc\/) as our scrapping tools:\n   - **Requests:** \" allows you to send organic, grass-fed HTTP\/1.1 requests, without the need for manual labor. There\u2019s no need to manually add query strings to your URLs, or to form-encode your POST data. Keep-alive and HTTP connection pooling are 100% automatic, thanks to urllib3.\"\n   - **Beautifullsoup:** \"is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.\"","e8eee8c1":"### <a id=\"4.7\"> Saving to CSV:<\/a>\n\n> The 3rd Bolck is saving the resulting news dictionnary in a CSV file, we are checking if the file exists (and we would append to it), or not (and we would creat it as new file) (returned by the second function.\n<br>","681a7264":"### <a id=\"4.2\">Imports<\/a>","95d41412":"## <a id=\"6\"> Future Improvements: <\/a>\n- Adding a 'start_page' and 'stop_page' parameters,\n- Adding a Resume_Block for Scrapping part by part,\n- Automating the scrapping of All categories of news\n- Automating the scrapping to access the full text of the Articles ","d90becc5":"### <a id=\"4.8\"> Looping over the Pages of the Category: (the General function) <\/a>\n> Finally the genral function, where we iterate over the number of pages, and apply the blocks defined above for each page of news.\n\n<br>\n> **Note:** at each iteration, i.e: parsing a new page, we are forcing a 10 Seconds Pause of the scrapper, so as not to overuse our acces to the site\n\n<br>","58fb568b":"## Table of Contents:","fde09f4d":"## <a id=\"3\">Pre-Code Analysis:<\/a> \n### <a id=\"3.1\">Examining the source:<\/a>\nIn this tutorial we will be collecting financial news from the **[Investing blog](https:\/\/www.investing.com\/news)**\n> **OBJECTIF:** \nAs a simple Demo, we want to collect the following in formations: \n    the title of the articl\n    the source and time of the article\n    the first paragraph\nIn addition we want to do this for all the pages in the category.\n\n<br>\n![](https:\/\/raw.githubusercontent.com\/Elamraoui-Sohayb\/Ethical_Scrapper\/master\/Ethical_Data_Collection\/img\/Screenshot1.png)\n<br> \n![](https:\/\/raw.githubusercontent.com\/Elamraoui-Sohayb\/Ethical_Scrapper\/master\/Ethical_Data_Collection\/img\/Screenshot2.png)\n<figure>\n\n<br>\n<figcaption> <center> Starting page (Simple view)<\/center><\/figcaption> \n<\/figure>\n\n### <a id=\"3.2\">Examining the HTML:<\/a>\n<br>\n![](https:\/\/raw.githubusercontent.com\/Elamraoui-Sohayb\/Ethical_Scrapper\/master\/Ethical_Data_Collection\/img\/screen4.png)\n<br>\n![](https:\/\/raw.githubusercontent.com\/Elamraoui-Sohayb\/Ethical_Scrapper\/master\/Ethical_Data_Collection\/img\/screen6.png)\n<figure><figcaption> <center> Starting page (HTML Inspection)<\/center><\/figcaption><\/figure>\n<br>\n\n**SO,** After inspecting the HTML source, our acess path is the following:\n\n    \t|<div class=\"largeTitle\">                     | The articles List\n        |---<article class=\"articleItem\" >            |--- Single article\n        |--- ---<a href=\"...\" class=\"title\">          |--- --- The Article Title\n        |--- ---<span class=\"articleDetails\">  <====> |--- --- Source and Time of the Article  \n        |--- ---<p>                            <====> |--- --- The First paragraph of the article\n        |---<article class=\"articleItem\" >            |--- Next Single article\n        |   .......                                   |    ........\n        |   .......                                   |    ........\n    \t|<a href=\"...\" class=\"pagination\">            | Page numbers\n\n","d04186cd":"### <a id=\"4.3\"> Making a request to a single page:<\/a>\nOur first function would make the HTTP requests to a given URL adress, and return the page response or an error depending on the status of the response\n<br>","a3f435ad":"## <a id=\"2\">Effecient Scrapping guidlines:<\/a> \n- **Identifiy what your looking for:** web pages can sometimes be overcharged with informations, or presented in less optimal layout, so always locate precisly the path for accesing the data you are collecting  \n\n- **DRY (Dont repeat yourself):** Scrapping is a repetitive process, so try the style of the code must be adapted, assign each task to a function call for example.\n\n- **Update the code of the scrapper regularly**, as websites chage their HTML layouts quite often.\n\n- **Make as many checks** (if statements, assertions, exeptions) to keep track of your scrapper failures, when you are scrapping a large amount of pages. ","425698cb":"1. [Ethical Scrapping:](#1)\n1. [Efficent Scrapping:](#2)\n1. [Pre-Code Analysis:](#3)\n    1. [Examining the Source](#3.1)\n    1. [Examining the HTML](#3.2)\n1. [Code:](#4)\n    1. [Envirenment and Setup](#4.1)\n    1. [Imports](#4.2)\n    1. [Making a request to a single page](#4.3)\n    1. [Code Structure](#4.4)\n    1. [Getting the details of a single Article](#4.5)\n    1. [Getting the details of a single Page: (list of Articles)](#4.6)\n    1. [Saving to CSV](#4.7)\n    1. [Looping over the Pages of the Category: (the General function)](#4.8)\n1. [Checking the resulting dataset](#5)\n1. [Future Improvements](#6)\n1. [Up next: Starting our NLP pipline for this dataset](#7)\n1. [ Ressources](#8)\n","7b63a362":"#### Testing:","e8cea652":"According to **[Kaggle survey of 2017](http:\/\/www.kaggle.com\/surveys\/2017)** , Data availabilty and quality remains among the top barriers for professionals in the field.\n<br>\n ![](https:\/\/raw.githubusercontent.com\/Elamraoui-Sohayb\/Ethical_Scrapper\/master\/Ethical_Data_Collection\/img\/Screenshot0.png)\n<figure>\n \n  <br>    \n  <figcaption> <center>Kaggle survey results(2017)<\/center><\/figcaption>\n<\/figure>\n\n<br><br>\nThis notebook deals with **the availabilty problem**, and walks through the steps of scrapping web pages and collecting the data in a responsible and efficient manner.","ec3708e8":"## Introduction: ","d3514779":"### <a id=\"4.5\"> Getting the details of a single Article: <\/a>\n>Our 1st block of operations, would be to take a single Article item from the list and extract its relevent informations as a return values:\n<br>","bb28529b":"## <a id=\"8\"> Ressources: <\/a>\n- **More on Ethical scrapping:**\n   - Legality and Ethics of Web Scraping, By *Vlad Krotov* and *Leiser Silva*  [Researchgate](https:\/\/www.researchgate.net\/publication\/324907302_Legality_and_Ethics_of_Web_Scraping)\n   - legality, ethics,web scraping, By *Sudarshan Shidore*  [LinkedIn](https:\/\/www.linkedin.com\/pulse\/legality-ethics-web-scraping-sudarshan-shidore\/)\n   - ethics in web scrapping, By *James Densmore* [TDS](https:\/\/towardsdatascience.com\/ethics-in-web-scraping-b96b18136f01)\n- **More on Effecient Production Programming:**\n   - How to write a production-level code in Data Science?, By *Venkatesh Pappakrishnan* [TDS](https:\/\/towardsdatascience.com\/how-to-write-a-production-level-code-in-data-science-5d87bd75ced)\n   - Writing clean, testable, high quality code in Python, By *Noah Gift* [IBM](https:\/\/www.ibm.com\/developerworks\/aix\/library\/au-cleancode\/index.html)\n        ","bec09381":"## <a id=\"1\"> Ethical Scrapping guidlines:<\/a> \nscrapping the web is a fairly easy anf very powerful method, therefore it must be used responsibly, you can find more in the ressource section.\n\n- **Transparency:** identify your self, and purpose or privde a contact, if the page owner wanted to contact you; this can be easly done in the UserAgent definition.\n\n- **Ownership:** the content you are collecting is Not your own, always cite the source or the orignal author.\n\n- **Overuse:** we must request data at a reasonable rate, in order to avoid stressing or crashing the server; you can set a sleep time between requests.","afdf53dd":"### <a id=\"4.6\"> Getting the details of a single Page: (list of Articles) <\/a>\n>In the 2nd block, we inspect the Page Url, finds the List of Articles to iterate over, and calls the first fuction (above) at each iteration, appending the calling results into a list of dictionnaries :\n<br>","3a2f1d09":" ### <a id=\"4.4\">Code Structure: <\/a>\n Now that we can make requests to the sites page, we should extract our data form the page code we are getting:\n  - **FOR each** Page **IN** the News Category, **DO:**\n    - **FOR each** Article **IN** the page, **DO:**\n       - **Get_Details:** title, link, date, source, first paragraphe\n     - **Write the Details Of a page to CSV File**               ","c46eae83":">### <p>&copy;<\/p>\n\n<br>\n**By:** [Elamraoui Sohayb](https:\/\/www.linkedin.com\/in\/sohayb-elamraoui\/), **Supervision of:** [Sadiq Abdelalim, Phd](https:\/\/www.linkedin.com\/in\/ACoAAARR-RkBQxLhbsUsrqHkxCRa8KwwtZnP0mA\/). Master 'Big Data & Cloud Computing', Ibn Tofail University","64a8b1dd":"# Ethical Data Collection for Financial News \n\n<br>\n![](https:\/\/raw.githubusercontent.com\/Elamraoui-Sohayb\/Ethical_Scrapper\/master\/Ethical_Data_Collection\/img\/icon-web-scraping.png)\n<figure>\n<figcaption> <center>From: <a href='http:\/\/arbisoft.com\/services\/data-services\/web-scraping-services'> arbisoft <\/center><\/figcaption>\n\n<\/figure>\n","8985a470":"## <a id=\"5\"> Checking the resulting dataset: <\/a>\n\n**Note:**  the csv resulting would be saved in \"..\/working\/name_of_file.csv\""}}