{"cell_type":{"8bd28306":"code","4aef0aaa":"code","20f5088e":"code","3b67b979":"code","89524bb8":"code","7b1b056e":"code","549036de":"code","81ac3874":"code","9d8ef78a":"code","43ef849d":"code","d166d6b8":"code","ab416815":"code","582828bf":"code","93cfecc2":"code","b67b565c":"code","fcfa83a4":"code","77b34086":"code","9ed129fd":"code","3ce54cb0":"code","ef54f425":"code","de554e82":"code","30ac746c":"code","750f42fb":"code","f8aeeb54":"code","adbb592f":"code","922ff3d7":"code","6882fc04":"code","d08b737c":"code","26246239":"code","befdbbc1":"code","e0cc0e09":"code","4b21a75b":"code","fe256c63":"code","2485ccf3":"code","c5962cff":"code","837e808e":"code","2060e9db":"code","c61cfff3":"code","0ae12d08":"code","fe4b4b42":"code","3fc07005":"code","2e0a7bec":"code","bcaadbfd":"code","b21357a0":"code","29cad0a4":"code","3707f6c8":"code","b2f712db":"code","626bc019":"code","f69b01e3":"code","087f339c":"code","d0c89d89":"code","5383ac8b":"code","856c0c48":"code","3b90a105":"code","7f3af40a":"code","789c30d4":"code","ee3989f6":"code","d52bfae4":"code","d830df56":"markdown","e484f417":"markdown","4516c41f":"markdown","65e84b89":"markdown","155650a8":"markdown","d7a5e5d5":"markdown","0a5c6487":"markdown","d5a43a19":"markdown","b7981634":"markdown","857dacf4":"markdown","90dc3f34":"markdown","1c6bca89":"markdown","e005a6fc":"markdown","c1e2741b":"markdown","9c3ea442":"markdown","68bbee8f":"markdown","ec6d8abf":"markdown","1c3b2e7b":"markdown","7d687e85":"markdown","c5ee6d4c":"markdown"},"source":{"8bd28306":"!pip install -Uqq fastbook kaggle waterfallcharts treeinterpreter dtreeviz\nimport fastbook\nfastbook.setup_book()\n\nimport seaborn as sns\nfrom fastbook import *\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG, clear_output\nfrom tqdm.auto import tqdm\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 10","4aef0aaa":"df = pd.read_csv(\"..\/input\/ventpressure1\/train_preprocessed.csv\", low_memory=False)\nto = load_pickle(\"..\/input\/ventpressure1\/to.pkl\")","20f5088e":"for col, i in tqdm(zip(df.dtypes.index, df.dtypes)): \n    i = str(i)\n    if i == \"float64\": df = df.astype({col: \"float32\"})\n    elif i == \"int64\": df = df.astype({col: \"int32\"})","3b67b979":"df.dtypes","89524bb8":"path = Path(\"..\/input\/ventilator-pressure-prediction\")\nsave_path = Path(\"\/kaggle\/working\")","7b1b056e":"def rf(xs, y, n_estimators=40, max_samples=200_000, \n      max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)","549036de":"xs, y = to.train.xs, to.train.y\nvalid_xs, valid_y = to.valid.xs, to.valid.y\nxs[\"u_in_cumsum_2\"] = xs.u_in.groupby(xs.breath_id).cumsum()\nvalid_xs[\"u_in_cumsum_2\"] = valid_xs.u_in.groupby(valid_xs.breath_id).cumsum()\n\nm = rf(xs, y)","81ac3874":"def m_mae(m, xs, y): return mean_absolute_error(y, m.predict(xs))","9d8ef78a":"m_mae(m, xs, y), m_mae(m, valid_xs, valid_y)","43ef849d":"preds = np.stack([t.predict(valid_xs) for t in m.estimators_])","d166d6b8":"mean_absolute_error(valid_y, preds.mean(0))","ab416815":"plt.plot([mean_absolute_error(valid_y, preds[:i+1].mean(0)) for i in range(40)])","582828bf":"mean_absolute_error(y, m.oob_prediction_)","93cfecc2":"preds = np.stack([t.predict(valid_xs) for t in m.estimators_])\npreds.shape","b67b565c":"preds_std = preds.std(0)\npreds_std[:5]","fcfa83a4":"def rf_feat_importance(m, df):\n    return pd.DataFrame({\"cols\": df.columns, \"imp\": m.feature_importances_}\n                       ).sort_values(\"imp\", ascending=False)","77b34086":"# show first few most important columns. \nfi = rf_feat_importance(m, xs)\nfi","9ed129fd":"# plot of feature importances\ndef plot_fi(fi): return fi.plot(\"cols\", \"imp\", \"barh\", figsize=(12, 7), legend=False)\n\nplot_fi(fi)","3ce54cb0":"to_keep = fi[fi.imp > 0.005].cols\nlen(to_keep)","ef54f425":"# Retrain using subset\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\n\nm = rf(xs_imp, y)\nm_mae(m, xs_imp, y), m_mae(m, valid_xs_imp, valid_y)","de554e82":"# And if we remove C and R also\nto_keep1 = fi[fi.imp > 0.05].cols\nxs_imp1 = xs[to_keep]\nvalid_xs_imp1 = valid_xs[to_keep]\n\nm1 = rf(xs_imp1, y)\nm_mae(m1, xs_imp1, y), m_mae(m1, valid_xs_imp1, valid_y)","30ac746c":"del to_keep1, xs_imp1, m1, valid_xs_imp1","750f42fb":"plot_fi(rf_feat_importance(m, xs_imp))","f8aeeb54":"cluster_columns(xs_imp)","adbb592f":"def get_oob(df):\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15, \n        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(df, y)\n    to_ret = m.oob_score_\n    del m\n    return to_ret","922ff3d7":"get_oob(xs_imp)","6882fc04":"get_oob(xs)","d08b737c":"to_drop = [\"u_in_cumsum\", \"new_time_step\"]\nget_oob(xs.drop(to_drop, axis=1))","26246239":"xs_final = xs_imp.drop(to_drop, axis=1)\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1)\n# xs_final = xs.drop(to_drop, axis=1)\n# valid_xs_final = valid_xs.drop(to_drop, axis=1)\n\nsave_pickle(save_path\/\"xs_final.pkl\", xs_final)\nsave_pickle(save_path\/\"valid_xs_final.pkl\", valid_xs_final)","befdbbc1":"from sklearn.inspection import plot_partial_dependence\n\nfig, ax = plt.subplots(figsize=(12, 4))\nplot_partial_dependence(m, valid_xs_final, [\"time_step\", \"u_in_cumsum_2\"],\n                       grid_resolution=20, ax=ax)","e0cc0e09":"fig, ax = plt.subplots(figsize=(12, 4))\nplot_partial_dependence(m, valid_xs_final, [\"R\"],\n                       grid_resolution=20, ax=ax)","4b21a75b":"fig, ax = plt.subplots(figsize=(12, 4))\nplot_partial_dependence(m, valid_xs_final, [\"uIn_lag1\", \"uIn_lag2\", \"uIn_lag3\"],\n                       grid_resolution=20, ax=ax)","fe256c63":"fig, ax = plt.subplots(figsize=(12, 4))\nplot_partial_dependence(m, valid_xs_final, [\"uIn_diff1\", \"uIn_diff2\", \"uIn_diff3\"],\n                       grid_resolution=20, ax=ax)","2485ccf3":"fig, ax = plt.subplots(figsize=(12, 4))\nplot_partial_dependence(m, valid_xs_final, [\"breathId_uIn_diffmax\"],\n                       grid_resolution=20, ax=ax)","c5962cff":"fig, ax = plt.subplots(figsize=(12, 4))\nplot_partial_dependence(m, valid_xs_final, [\"breathId_uIn_diffmean\"],\n                       grid_resolution=20, ax=ax)","837e808e":"import warnings\nwarnings.simplefilter(\"ignore\", FutureWarning)\n\nfrom treeinterpreter import treeinterpreter\nfrom waterfall_chart import plot as waterfall","2060e9db":"m = rf(xs_final, y)","c61cfff3":"row = valid_xs_final.iloc[:5]\nprediction, bias, contributions = treeinterpreter.predict(m, row.values)\nprediction[0], bias[0], contributions[0].sum()","0ae12d08":"waterfall(valid_xs_final.columns, contributions[0], threshold=0.08,\n         rotation_value=45, formatting=\"{:,.3f}\")","fe4b4b42":"df_dom = pd.concat([xs_final, valid_xs_final])\nis_valid = np.array([0] * len(xs_final) + [1] * len(valid_xs_final))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)","3fc07005":"m = rf(xs_final, y)\nprint(\"orig\", m_mae(m, valid_xs_final, valid_y))\n\nfor c in tqdm((\"max_cumsum breathId_uIn_max u_in_cumsum_2 breathId_uIn_diffmax breathId_uIn_diffmean\".split(\" \"))):\n    m = rf(xs_final.drop(c, axis=1), y)\n    print(c, m_mae(m, valid_xs_final.drop(c, axis=1), valid_y))","2e0a7bec":"for c in tqdm((\"uIn_diff1 uIn_diff2 uIn_diff3 uIn_lag1 uIn_lag2 uIn_lag3\".split(\" \"))):\n    m = rf(xs_final.drop(c, axis=1), y)\n    print(c, m_mae(m, valid_xs_final.drop(c, axis=1), valid_y))","bcaadbfd":"to_drop = [\"breathId_uIn_max\", \"breathId_uIn_diffmean\", \"uIn_diff1\", \"uIn_diff2\", \n          \"uIn_lag2\", \"uIn_lag3\"]\nm = rf(xs_final.drop(to_drop, axis=1), y)\nprint(\"Multi_drop: \", m_mae(m, valid_xs_final.drop(to_drop, axis=1), valid_y))","b21357a0":"to_drop = [\"breathId_uIn_diffmean\", \"uIn_diff3\", \"breathId_uIn_diffmax\"]\nm = rf(xs_final.drop(to_drop, axis=1), y)\nprint(\"Multi_drop: \", m_mae(m, valid_xs_final.drop(to_drop, axis=1), valid_y))","29cad0a4":"to_test = pd.read_csv(\"..\/input\/ventpressure1\/test_preprocessed.csv\")\nto_test[\"u_in_cumsum_2\"] = to_test[\"u_in\"].groupby(to_test[\"breath_id\"]).cumsum()\nto_drop1 = [\"u_in_cumsum\", \"new_time_step\"]\nto_test = to_test.drop(to_drop1, axis=1)\nto_test = to_test.drop(to_drop, axis=1)\n# to_test = to_test.drop(\"breath_id\", axis=1)\nto_test.to_csv(\"test_preprocessed.csv\", index=False)\ndel to_test","3707f6c8":"filt = xs.u_in_cumsum < 1500\nxs_filt = xs_final[filt]\ny_filt = y[filt]\n\nm = rf(xs_filt, y_filt)\nm_mae(m, xs_filt, y_filt), m_mae(m, valid_xs_final, valid_y)","b2f712db":"filt = xs.u_in_cumsum < 1000\nxs_filt = xs_final[filt]\ny_filt = y[filt]\n\nm = rf(xs_filt, y_filt)\nm_mae(m, xs_filt, y_filt), m_mae(m, valid_xs_final, valid_y)","626bc019":"del xs_filt, y_filt, filt, xs_imp, valid_xs_imp, preds, preds_std, to","f69b01e3":"try: del df\nexcept Exception: pass\ndf_train = pd.concat([xs_final, y], axis=1)\ndf_valid = pd.concat([valid_xs_final, valid_y], axis=1)\ndf_nn_final = pd.concat([df_train, df_valid])\ndf_nn_final = df_nn_final.sort_index()\ndf_nn_final.to_csv(\"train_preprocessed.csv\", index=False)\ndf_nn_final.dtypes","087f339c":"del df_train, df_valid, m, is_valid, df_dom, xs, y, valid_xs, valid_y\nimport gc\ngc.collect()","d0c89d89":"dep_var = \"pressure\"\ncont_nn, cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\ncont_nn","5383ac8b":"df_nn_final[cat_nn].nunique()","856c0c48":"procs_nn = [Categorify, FillMissing, Normalize]\nsplits = load_pickle(\"..\/input\/ventpressure1\/split.pkl\")\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn, splits=splits, \n                     y_names=dep_var)","3b90a105":"# Tabular models don't generally require much GPU RAM. \n# Hence we use larger batch size. \ndls = to_nn.dataloaders(1024)","7f3af40a":"y = to_nn.train.y\ny.min(), y.max()","789c30d4":"learn = tabular_learner(dls, y_range=(-2, 65), layers=[1000, 500], \n                       n_out=1, loss_func=F.mse_loss, metrics=mae)","ee3989f6":"learn.lr_find()","d52bfae4":"learn.fit_one_cycle()","d830df56":"### Removing Redundant Features. ","e484f417":"It did worse than with all features.","4516c41f":"For us, we would just remove one variable, which is `breath_id`. And looks like non-capped cumsum have better oob (although very slightly) than capped cumsum. ","65e84b89":"We might also require the splits, though we haven't saved it yet. We might update that next time. ","155650a8":"Okay, these are the ones we want to drop. Let's try filtering for cumsum less than 1500 and see if it improves accuracy. (MAE). ","d7a5e5d5":"Since we require using GPU, and this instance is best run with CPU (as Kaggle offers more CPU without GPU), we will continue in another notebook. Third, we are about to reach OOM (out of memory). ","0a5c6487":"### Partial Dependence\nWe want to understand the relationship between predictors and dependent variable (pressure). We will do so for the most predictive two variables: `u_in_cumsum_2` and `time_step`. ","d5a43a19":"# Model Interpretation\n- How confident are we in our predictions using a particular row of data?\n- For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\n- Which columns are the strongest predictors, which can we ignore?\n- Which columns are effectively redundant with each other, for purposes of prediction?\n- How do predictions vary, as we vary these columns? \n\n### Tree Variance for Prediction Confidence\nStandard deviation tells us the *relative* confidence of predictions. ","b7981634":"## Using a Neural Network","857dacf4":"# Ventilation Pressure Notebook 2\n## Random Forest\n\n\n**Note:** Due to the large size of dataset after preprocessed, it is best to use PySpark and Parquet files, which improves preprocessing and training. Here we don't use that, however, one might create another notebook on that (if that happens one will direct the link here). ","90dc3f34":"We don't have lots of columns, so we didn't see any redundant features. Of course, perhaps if we haven't remove `new_time_step` in notebook 1 we might have it here as redundant, for example. Example of one with such is as below. We create `u_in_cumsum_2` which is extremely similar to `u_in_cumsum` except there are no binning. \n\nContinue by creating function to get OOB score. ","1c6bca89":"For `time_step`, we have most of the pressure accumulated before time 1.0 (or slightly after that steep drop). This is expected because pressure is high when we breath in, and pressure low when breathing out (after the time_step steep drop). Is this *data leakage*? \n\nRemember R and C are assigned a mapping so that's why it's not its original value. \n\n### Tree Interpreter","e005a6fc":"Let's get a baseline of original random forest model MAE, and see effect of removing each of these columns in turn. ","c1e2741b":"### Removing Low-Importance Variables\nParticularly, remove `breath_id` . One could also try removing `C` and `R` just to experiment with it. ","9c3ea442":"## Extrapolation and Neural Networks\nRandom Forests don't generalize well. Neural networks generalizes better. \n\nActually, our program doesn't really requires using neural network, because we don't need to generalize too well. We cover quite a lot of range and mostly we want are interpolations. We will still look at neural networks and see how they do, though. \n\n### Finding out-of-domain data. \nTo predict whether a row is validation or training set. ","68bbee8f":"Look at the contributions per variable. ","ec6d8abf":"We have very high standard deviations, and we have varying standard deviations. \n\n### Feature Importance\n*how* it's making predictions. ","1c3b2e7b":"Doesn't seem to give better result than the original. Original is 0.885 and this is 0.909. Let's try 1000 and see really. ","7d687e85":"It's good idea to set `y_range` for regression models. Let's find min and max of variable. ","c5ee6d4c":"Not too bad predictions.\n\nTo see the impact of `n_estimators`, let's get predictions from each individual tree in forest. "}}