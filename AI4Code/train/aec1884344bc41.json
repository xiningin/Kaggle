{"cell_type":{"e472a6b1":"code","31d9f3db":"code","86d39aac":"code","e858811f":"code","9e4e5848":"code","cef170de":"code","f7d28675":"code","7b1f1cec":"code","ba6b25a7":"code","01411f6a":"code","b467e43f":"code","f82dd6d0":"code","71846955":"code","dd3e869a":"code","d9134c98":"markdown","674e0f84":"markdown","fae43854":"markdown","7288bbd1":"markdown"},"source":{"e472a6b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","31d9f3db":"arquivo = pd.read_csv('\/kaggle\/input\/wine-dataset\/wine_dataset.csv')","86d39aac":"arquivo.head()","e858811f":"arquivo['style'] = arquivo['style'].replace('red', 0)\narquivo['style'] = arquivo['style'].replace('white', 1)","9e4e5848":"arquivo","cef170de":"# Separando as vari\u00e1veis entre preditoras e vari\u00e1vel alvo\ny = arquivo['style'] # vari\u00e1vel alvo\nx = arquivo.drop('style', axis = 1) #armazena todo o restante na vari\u00e1vel x (vari\u00e1veis preditoras)","f7d28675":"from sklearn.model_selection import train_test_split\n\n# Criando conjuntos de dados de treino e teste:\nx_treino, x_teste, y_treino, y_teste = train_test_split (x, y, test_size = 0.3) #test_size: quantos % para treino\/teste","7b1f1cec":"from sklearn.ensemble import ExtraTreesClassifier\n# Cria\u00e7\u00e3o do modelo\nmodelo = ExtraTreesClassifier() # algoritmo que cria v\u00e1rias \u00e1rvores de decis\u00e3o\nmodelo.fit(x_treino, y_treino)","ba6b25a7":"# Imprimindo resultados\nresultado = modelo.score(x_teste, y_teste) # ir\u00e1 prever de acordo com o modelo que ele 'aprendeu' com os dados de treino, utilizando agora dados n\u00e3o visto(os de teste)","01411f6a":"print(\"Acur\u00e1cia: \", resultado)","b467e43f":"y_teste[205:210] #filtro de 5 amostras aleat\u00f3rias que o modelo n\u00e3o teve contato","f82dd6d0":"x_teste[205:210] # filtro de 5 amostras aleat\u00f3rias que o modelo n\u00e3o teve contato","71846955":"previsoes = modelo.predict(x_teste[205:210]) # cria\u00e7\u00e3o de vari\u00e1vel para checar se o modelo treinado acerta","dd3e869a":"previsoes","d9134c98":"Devemos lembrar que uma acur\u00e1cia alta nem sempre \u00e9 ind\u00edcio de sucesso! Muitas vezes ela pode indicar algum tipo de _overfitting_, quando o algoritmo aprende 'demais' o modelo!\n\nQuantos mais complexo o dataset, mais improv\u00e1vel uma acur\u00e1cia acima de 70%. Neste caso espec\u00edfico, o dataset \u00e9 mais simples e a solu\u00e7\u00e3o j\u00e1 foi validada.","674e0f84":"Verificando o c\u00f3digo acima:","fae43854":"Nossa vari\u00e1vel 'target', isto \u00e9, a vari\u00e1vel que desejamos prever, \u00e9 a coluna _style_, que identifica se o vinho \u00e9 tinto (red) ou branco (white). Iremos substituir esses valores por 0 e 1.","7288bbd1":"O modelo previu corretamente."}}