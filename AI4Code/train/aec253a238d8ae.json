{"cell_type":{"09c83c41":"code","9d727e30":"code","57757a6f":"code","bafb29a4":"code","d9fad1f5":"code","e28a893b":"code","775d6895":"code","3efa84fc":"code","7a42b34b":"code","7ed53777":"code","6df5cd3a":"code","1932fd4e":"code","22ea6ec4":"code","1a704593":"code","a2f83648":"code","b8f87680":"code","8a6a48c7":"markdown","8454ab8a":"markdown","0df063a2":"markdown","d0d3a5e5":"markdown","b36e0921":"markdown","c24d9a6c":"markdown","abab77ec":"markdown","fb5bcc76":"markdown","8b8bf2c4":"markdown","285ff71c":"markdown","f689637d":"markdown","74e78cb1":"markdown","f42950a8":"markdown","ebf86444":"markdown"},"source":{"09c83c41":"# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, warnings, gc, string\nwarnings.filterwarnings(\"ignore\")\n\n# SKLearn\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\n\n# Tensorflow \/ Keras\nimport tensorflow as tf\nfrom keras.preprocessing import text,sequence\nfrom keras import layers,models,optimizers\nimport tensorflow_hub as hub\n\n# XGBoost & Textblob\nimport xgboost\n\n#Gensim Library for Text Processing\nimport gensim.parsing.preprocessing as gsp\nfrom gensim import utils","9d727e30":"'''Load'''\n\n#train\nurl = '..\/input\/analytics-vidhya-identify-the-sentiments\/train.csv'\ndf = pd.read_csv(url, header='infer')\n\n#Drop Columns\ndf.drop('id', inplace=True, axis=1)\n\n#Inspect\nprint(\"Total Records (training dataset): \", df.shape[0])","57757a6f":"'''Tweet Data Cleaning Utility Function'''\n\nprocesses = [\n               gsp.strip_tags, \n               gsp.strip_punctuation,\n               gsp.strip_multiple_whitespaces,\n               gsp.strip_numeric,\n               gsp.remove_stopwords, \n               gsp.strip_short, \n               gsp.stem_text\n            ]\n\ndef proc_txt(txt):\n    text = txt.lower()\n    text = utils.to_unicode(text)\n    for p in processes:\n        text = p(text)\n    return text\n\n\n# Training Dataset\ndf['tweet_cln'] = df['tweet'].apply(lambda x: proc_txt(x))","bafb29a4":"# Training Dataset\ndf.head()","d9fad1f5":" '''Data Split (training dataset)'''\ntrain_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['tweet_cln'], df['label'])\n\n\n\n'''Feature Engineering of Training Dataset [TF-IDF Vectors] - Basic Classifiers'''\ntfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(df['tweet_cln'])\nxtrain_tfidf =  tfidf_vect.transform(train_x)\nxvalid_tfidf =  tfidf_vect.transform(valid_x)\n\n\n\n'''Feature Engineering of Training Dataset [Word Embedding] - Deep Neural'''\nembeddings_index = {}\n\nfor i, line in enumerate(open('..\/input\/wikinews300d1mvec\/wiki-news-300d-1M.vec')):  #Pretrained Word Embedding Vectors\n    values = line.split()\n    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n\n# Tokenizer \ntoken = text.Tokenizer()\ntoken.fit_on_texts(df['tweet_cln'])\nword_index = token.word_index\n\n# Text to Sequence \ntrain_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\nvalid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n\n# Token-embedding Mapping\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","e28a893b":"'''Utility Function'''\n\ndef model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    \n    if is_neural_net:\n        predictions = predictions.argmax(axis=-1)\n   \n    #free memory\n    gc.collect()\n    \n    return metrics.accuracy_score(predictions, valid_y)","775d6895":"nb_acc = model(naive_bayes.MultinomialNB(),xtrain_tfidf, train_y, xvalid_tfidf)\nprint(\"Naive Bayes(multinomial) Accuracy Achieved: \", '{:.2%}'.format(nb_acc))","3efa84fc":"ln_acc = model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\nprint(\"Logistic Reg Accuracy Achieved: \", '{:.2%}'.format(ln_acc))","7a42b34b":"rf_acc = model(ensemble.RandomForestClassifier(random_state=42), xtrain_tfidf, train_y, xvalid_tfidf)\nprint(\"Random Forest Accuracy Achieved: \", '{:.2%}'.format(rf_acc))","7ed53777":"xgb_acc = model(xgboost.XGBClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\nprint(\"XGBoost Accuracy Achieved: \", '{:.2%}'.format(xgb_acc))","6df5cd3a":"'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# Convolutional Layer\nconv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n\n# Pooling Layer\npooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\ncnn_model = models.Model(inputs=input_layer, outputs = output_layer2)\ncnn_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\ncnn_acc = model(cnn_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"CNN Model Accuracy Achieved: \", '{:.2%}'.format(cnn_acc))","1932fd4e":"'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# LSTM Layer\nlstm_layer = layers.LSTM(100)(embedding_layer)\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\nrnn_model = models.Model(inputs=input_layer, outputs = output_layer2)\nrnn_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\nrnn_acc = model(rnn_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"RNN(LSTM) Model Accuracy Achieved: \", '{:.2%}'.format(rnn_acc))","22ea6ec4":"'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# GRU Layer\ngru_layer = layers.GRU(100)(embedding_layer)\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(gru_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\nrnngru_model = models.Model(inputs=input_layer, outputs = output_layer2)\nrnngru_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\nrnngru_acc = model(rnngru_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"RNN(GRU) Model Accuracy Achieved: \", '{:.2%}'.format(rnngru_acc))","1a704593":"'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# BiDirectional Layer\nbi_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(bi_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\nrnnbi_model = models.Model(inputs=input_layer, outputs = output_layer2)\nrnnbi_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\nrnnbi_acc = model(rnnbi_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"RNN(BiDirectional-GRU) Model Accuracy Achieved: \", '{:.2%}'.format(rnnbi_acc))","a2f83648":"'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# BiDirectional Layer\nbi_layer = layers.Bidirectional(layers.LSTM(100))(embedding_layer)\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(bi_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\nrnnbil_model = models.Model(inputs=input_layer, outputs = output_layer2)\nrnnbil_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\nrnnbil_acc = model(rnnbil_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"RNN(BiDirectional-LSTM) Model Accuracy Achieved: \", '{:.2%}'.format(rnnbil_acc))","b8f87680":"'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# Recurrent Layer\nrnn_layer = layers.Bidirectional(layers.GRU(100,return_sequences=True))(embedding_layer)\n    \n# Convolutional Layer\nconv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(rnn_layer)\n\n# Pooling Layer\npooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\nrcnn_model = models.Model(inputs=input_layer, outputs = output_layer2)\nrcnn_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\nrcnn_acc = model(rcnn_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"RCNN Model Accuracy Achieved: \", '{:.2%}'.format(rcnn_acc))","8a6a48c7":"### RCNN","8454ab8a":"### RNN - GRU","0df063a2":"### Logistic Reg Classifier","d0d3a5e5":"### RNN - BiDirectional(GRU)","b36e0921":"### Data Setup","c24d9a6c":"### CNN (Keras)","abab77ec":"### RNN - BiDirectional(LSTM)","fb5bcc76":"# Build Model","8b8bf2c4":"# Setup\n\n### Libraries","285ff71c":"### XGBoost","f689637d":"### Random Forest","74e78cb1":"# Text Classification Models - An Extensive List \n\nI won't go into the details & bore you'll with the information about \"what is text classification?\". Instead I shall go straight to implementing various models for text classification [assuming thats what you're here for :-)]. \n\nI will keep the notebook fairely organised & well commented for easy reading, please do **UPVOTE** if you find it helpful.","f42950a8":"### Naive Bayes","ebf86444":"### RNN - LSTM"}}