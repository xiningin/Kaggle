{"cell_type":{"cf9908a4":"code","af5d9328":"code","26de0cee":"code","1bf46ed3":"code","2eb11373":"code","0f226f78":"code","7cae65fc":"code","b9cabcfc":"code","d67621b3":"code","9c115afb":"code","d466a8ec":"code","0c11d35a":"code","c633f20d":"code","56c7e432":"code","d89fe4fe":"code","f939afc2":"markdown"},"source":{"cf9908a4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","af5d9328":"data = pd.read_csv('\/kaggle\/input\/mnist-digit-dataset\/MNIST_DIGIT_DATSET\/train.csv')\ndata.head()","26de0cee":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport os\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nimport tensorflow as tf\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\nfrom tensorflow.keras.optimizers import RMSprop\n\n\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import load_img\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.image import ImageDataGenerator","1bf46ed3":"sns.countplot(data['label'])\n","2eb11373":"weight_file = '\/kaggle\/input\/inception-v3\/inception_v3.ckpt'","0f226f78":"pre_trained_model = InceptionV3(input_shape = (75,75,3),\n                                include_top = False,\n                               weights = 'imagenet')\n\n# pre_trained_model.load_weights(weight_file)\nfor layer in pre_trained_model.layers:\n      layer.trainable = False\nlast_layer = pre_trained_model.get_layer('mixed7')\nlast_output = last_layer.output\n","7cae65fc":"x = layers.Flatten()(last_output)\n\nx = layers.Dense(1024, activation='relu')(x)\n\nx = layers.Dropout(0.2)(x)\n\nx = layers.Dense(10, activation='softmax')(x)\n\nmodel = Model(pre_trained_model.input, x)\n\n# model.summary()\n\n# model = tf.keras.models.Sequential([\n#     # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n#     # This is the first convolution\n#     tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28, 1)),\n#     tf.keras.layers.MaxPooling2D(2, 2),\n#     # The second convolution\n#     tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n#     tf.keras.layers.MaxPooling2D(2,2),\n#     # The third convolution\n#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n#     tf.keras.layers.MaxPooling2D(2,2),\n   \n#     # Flatten the results to feed into a DNN\n#     tf.keras.layers.Flatten(),\n#     tf.keras.layers.Dropout(0.5),\n#     # 512 neuron hidden layer\n#     tf.keras.layers.Dense(512, activation='relu'),\n#     tf.keras.layers.Dense(10, activation='softmax')\n# ])\n\nmodel.compile(optimizer = RMSprop(lr = 0.0001),\n             loss = 'sparse_categorical_crossentropy',\n             metrics = ['accuracy'])\n\n","b9cabcfc":"train_folder =  '\/kaggle\/input\/mnist-digit-dataset\/MNIST_DIGIT_DATSET\/Images\/train\/'\ntest_folder =  '\/kaggle\/input\/mnist-digit-dataset\/MNIST_DIGIT_DATSET\/Images\/test\/'","d67621b3":"data['image_path'] = data.apply(lambda x: (train_folder + x['filename']), axis=1)\n","9c115afb":"train_data = np.array([img_to_array(load_img(img, target_size=(28,28)))\n                      for img in data['image_path'].values.tolist()]).astype('float32')","d466a8ec":"train_label = data['label']","0c11d35a":"# Split the data into train and validation. The stratify parm will insure  train and validation  \n# will have the same proportions of class labels as the input dataset.\nx_train, x_validation, y_train, y_validation = train_test_split(train_data, train_label, test_size=0.2, stratify=np.array(train_label), random_state=100)","c633f20d":"print('x_train shape = ',x_train.shape)\nprint('y_train shape = ',y_train.shape)\nprint('x_validation shape = ',x_validation.shape)\nprint('y_validation shape = ',y_validation.shape)","56c7e432":"train_datagen = ImageDataGenerator(rescale=1.\/255,\n                                  rotation_range=30,\n                                  width_shift_range = 0.2,\n                                  height_shift_range = 0.2)\n\nval_datagen = ImageDataGenerator(rescale = 1.\/255)\n\n\n\ntrain_gen = train_datagen.flow(x_train,\n                               y_train,\n#                                class_mode='categorical',\n                              batch_size=126)\n\n\nval_gen = val_datagen.flow(x_validation,\n                           y_validation, \n#                            class_mode='categorical',\n                           batch_size=126)\n","d89fe4fe":"history = model.fit_generator(train_gen,\n                              epochs =15,\n                   steps_per_epoch=200,\n                   validation_data = val_gen,\n                    validation_steps = 44,\n                   \n                   verbose = 2)","f939afc2":"# loading Inception model"}}