{"cell_type":{"2e5bd8ed":"code","c9e4e42f":"code","9b578dbf":"code","3625d8c1":"code","fa9b6b0b":"code","66bc50bb":"code","198d89c8":"code","1edf2e07":"code","a0078ded":"code","9aa757ec":"code","8263e329":"code","7f1f96bf":"code","ac76d5eb":"code","e350e989":"code","d8322318":"code","dae5029b":"code","4b7fcfdc":"code","735adc9d":"code","9a160d24":"code","d9f7231f":"code","38608063":"code","556a2a52":"code","e1c5d3bd":"code","d3ab2628":"code","4f27dceb":"code","a522ebff":"code","42d28b1e":"code","25941fe9":"code","5a1c9db1":"code","917f9f5e":"code","5cbd8381":"code","92110e4b":"code","836c04c4":"code","78f6fc64":"markdown","2f21635b":"markdown","ed7b9ada":"markdown","df69bcf8":"markdown","ea7fce16":"markdown","9be528e4":"markdown","48ca615e":"markdown","46710045":"markdown","d220ad98":"markdown","c0efe2a5":"markdown","6d9a52a2":"markdown","0e380532":"markdown","f1c656bd":"markdown","ae8a332c":"markdown","60bd4a3b":"markdown","b0c0c400":"markdown","62892f55":"markdown","6fb4a882":"markdown","aaae60b7":"markdown","6e73d625":"markdown","b54c75cf":"markdown","bc5e814d":"markdown","63cd548c":"markdown","70e1bda2":"markdown","619afbf8":"markdown","fceb639b":"markdown","dd37f45d":"markdown","2a78f419":"markdown","8e2b757f":"markdown","22f86512":"markdown","2922a9d5":"markdown","5fe51c77":"markdown","fba89394":"markdown","1f0e9452":"markdown"},"source":{"2e5bd8ed":"# Author: Pierre Jeanne\n# Date Created:  23 May 2021","c9e4e42f":"import numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\n# display progress bar during iteration\nfrom tqdm.notebook import tqdm\n\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\")\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.metrics import plot_confusion_matrix\n\n# stat on data\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\n# slip the data\nfrom sklearn.model_selection import train_test_split\n# scale the data\nfrom sklearn.preprocessing import MinMaxScaler\n# cross validation\nfrom sklearn.model_selection import cross_val_score\n# classification model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\n# hyperparameter tunning\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n# model evaluation\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score \nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","9b578dbf":"pip install dataprep","3625d8c1":"# LIBRARY FOR EDA\nfrom dataprep.eda import *\nfrom dataprep.eda import plot\nfrom dataprep.eda import plot_correlation\nfrom dataprep.eda import plot_missing","fa9b6b0b":"# load the data\ndf = pd.read_csv(r'..\/input\/telecom-users-dataset\/telecom_users.csv')\ndf.head(3)","66bc50bb":"# drop 'Unnamed: 0' \ndf.drop('Unnamed: 0', inplace=True, axis=1)\n# set 'customerID ' as index\ndf = df.set_index('customerID')\n# look at columns type\ndf.info()","198d89c8":"# convert TotalCharges to float64\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'],errors='coerce') ","1edf2e07":"# EDA\npp.ProfileReport(df)","a0078ded":"# find missing values\ndf.isnull().sum()","9aa757ec":"# replace missing value by mean\ndf['TotalCharges'] = df['TotalCharges'].fillna( df['tenure']*df['MonthlyCharges'] )","8263e329":"# senior citizen is concidered as an integer. convert it as object\ndf['SeniorCitizen'] = df['SeniorCitizen'].astype('object')","7f1f96bf":"# create list with varaibles object\ndatatype = df.dtypes\nobject_variables  = datatype[(datatype == 'object')].index.tolist()\n# create list with boolean  varaibles \nbool_variables = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling','Churn']\n# create list with categorical variables \ncat_variables = object_variables.copy()\nfor variable in cat_variables:\n    if variable in bool_variables:\n        cat_variables.remove(variable)\n\n# convert object dtypes to category\nfor variable in cat_variables:\n    df[variable] = df[variable].astype('category')\n\ndf.info()","ac76d5eb":"#  plot the correlations between variables and their relations with target varibales\nfig, axes = plt.subplots(2,2,figsize=(18,9))\n# fig.subplots_adjust(hspace=0.3)\nax0, ax1, ax2, ax3 = axes.flatten() \nsns.histplot(data=df, x=\"tenure\", hue=\"Churn\",ax=ax0)\nsns.histplot(data=df, x=\"MonthlyCharges\", hue=\"Churn\",ax=ax1)\nsns.histplot(data=df, x=\"TotalCharges\", hue=\"Churn\",ax=ax2)\nsns.scatterplot(data=df, x=\"TotalCharges\", y=\"tenure\", hue=\"Churn\",ax=ax3)\n\nplt.show()","e350e989":"# add new variable called 'difference':\ndf['differences'] = df['TotalCharges'] - df['tenure']*df['MonthlyCharges']","d8322318":"sns.histplot(data=df, x=\"differences\", hue=\"Churn\")\nplt.show()","dae5029b":"# initiate list to store percentage and name\npercentage_churn = []\noption_churn = []\n\nfor variable in object_variables[:-1]:\n    ct = pd.crosstab(df.Churn, df[variable], normalize=True)\n    # Among all the data: get max percentage in row 'YES'(client who left)\n    val_max = np.max(ct.loc['Yes',:])\n    # get which option associated with val_max and add it to the list\n    col_name = ct.columns[ct.isin([val_max]).any()][0]\n    option_churn.append(col_name)\n    # Among all the client who left (26.5%) calculate percentage client who left with this specific option\n    perc_val_max = val_max * 100\/0.265\n    percentage_churn.append(perc_val_max)\n    \n    \ndf_cross_tab = pd.DataFrame({'variable':object_variables[:-1],'name':option_churn,'percentage':percentage_churn})\ndf_cross_tab = df_cross_tab.sort_values('percentage',ascending=False)","4b7fcfdc":"ax = plt.figure(figsize=(15,10))\nax = sns.barplot(x=\"percentage\", y=\"variable\",data = df_cross_tab,palette= 'Dark2',orient='h')\nax.set_ylabel('Categorical variables',fontsize=16)\nax.set_xlabel('percentage of client who left',fontsize=16)\nax.set_yticklabels(ax.get_yticklabels(), size= 14)\n# ax.set_xticklabels(ax.get_xticklabels(), size = 14)\n\nlist_name = df_cross_tab['name'].tolist()\nfor p,name in zip(ax.patches,list_name):\n    ax.annotate(name, xy=(p.get_width(), p.get_y()+p.get_height()\/2),\n                xytext=(5, 0), textcoords='offset points', ha=\"left\", va=\"center\",fontweight = 'bold')\n    ax.annotate('{:.2f}%'.format(p.get_width()), xy=(p.get_width()-10, p.get_y()+p.get_height()\/2),\n                xytext=(5, 0), textcoords='offset points', ha=\"left\", va=\"center\",fontweight = 'bold')","735adc9d":"# drop unwanted columns\ndf = df.drop(['gender','MultipleLines','PhoneService','differences'],axis=1)\n\n# get dummies\ndf = pd.get_dummies(df,drop_first = True)","9a160d24":"# Displaying the count for non Deviated hole \nmajority_class = df.loc[df['Churn_Yes'] == 0].count()[0]\n\n# Showing the count for Deviated hole \nminority_class = df.loc[df['Churn_Yes'] == 1].count()[0]\n\n# Printing the classes for the deviated and non-deviated class \nprint('Clustomer who stayed: {}'.format(majority_class))\nprint('Customers who left : {}'.format(minority_class))\n\n\nsns.countplot(x=\"Churn_Yes\", data=df)\nplt.show()","d9f7231f":"from imblearn.over_sampling import SMOTE\n\n\nX = df.drop(['Churn_Yes'],axis = 1)\nysmote = df[['Churn_Yes']].values.ravel()\n# Using SMOTE to Balance the imbalanced data \nX_resampled, y_resampled = SMOTE().fit_resample(X, ysmote)\n\nX_resampled = pd.DataFrame(X_resampled, columns=X.columns ) ","38608063":"# convert y_resampled to df\ndf_y_resampled = pd.DataFrame(y_resampled,columns=['Churn_Yes'])\n\n# showing a plot of the Balanced dataset \nmajority_class = df_y_resampled.loc[df_y_resampled['Churn_Yes'] == 0].count()[0]\n\n# Showing the count for Non Hole Deviation \nminority_class = df_y_resampled.loc[df_y_resampled['Churn_Yes'] == 1].count()[0]\n\n# Print \nprint('Clustomer who stayed: {}'.format(majority_class))\nprint('Customers who left : {}'.format(minority_class))\n\nsns.countplot(x=\"Churn_Yes\", data=df_y_resampled)\nplt.show()","556a2a52":"# split the data into a training and testing dataset\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled,test_size = .3, random_state=42)\n\nprint('Shape of X_train :'+\" \"+str(X_train.shape))\nprint('Shape of X_test :'+\" \"+str(X_test.shape))\n","e1c5d3bd":"scaler = MinMaxScaler()\nnumeric_variables  = datatype[(datatype != 'object')].index.tolist()\n# fit and transform \"x_train\"\nX_train.loc[:,numeric_variables] = scaler.fit_transform(X_train[numeric_variables])\n# transform \"x_test\"\nX_test.loc[:,numeric_variables] = scaler.transform(X_test[numeric_variables])","d3ab2628":"list_scores = []\ndef plot_result_cv(list_score_train,list_score_test,model,pred,name):\n    print('mean scores on training set: {:2f}, and testing set: {:2f}'.format(np.mean(list_score_train),np.mean(list_score_test)))\n    fig = plt.figure(figsize=(15,5))\n    fig.subplots_adjust(hspace=0.4,wspace=0.3)\n    ax0 = fig.add_subplot(1,2,1)\n    ax0 = plt.plot(list_score_train,'go-',label='CV score on training set')\n    ax0 = plt.plot(list_score_test,'ro-',label='CV score on testing set')\n    ax0 = plt.xlabel('nb of fold cross-validation')\n    ax0 = plt.ylabel('score')\n    ax0 = plt.legend() \n    \n    ax1 = fig.add_subplot(1,2,2)\n    plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues,ax=ax1);  \n    \n    accuracy = accuracy_score(y_test,pred)\n    recall = recall_score(y_test,pred)\n    precision = precision_score(y_test,pred)\n    f1 = f1_score(y_test,pred)\n    \n    print('accuracy: ', accuracy)\n    print('recall: ',recall)\n    print('precision: ', precision)\n    print('f1: ', f1)\n    \n    list_scores.append({'Model Name': name, 'Accuracy': accuracy, 'Recall': recall, 'Precision': precision, 'F1':f1})\n    \n    plt.show()","4f27dceb":"clf_lr = LogisticRegression()\n\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_train = cross_val_score(clf_lr,X_train, y_train,cv=12,scoring = 'accuracy')\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_test = cross_val_score(clf_lr,X_test, y_test,cv=12,scoring = 'accuracy')\n\nclf_lr_mean_train = np.mean(cv_scores_train)\nclf_lr_mean_test = np.mean(cv_scores_test)\n\nclf_lr.fit(X_train,y_train)\npred = clf_lr.predict(X_test)\n# plot result cv\nplot_result_cv(cv_scores_train,cv_scores_test,clf_lr,pred,'logistic reg')","a522ebff":"param_grid = {'n_neighbors':np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=5)\nknn_cv.fit(X_train,y_train)\nknn_cv.best_params_","42d28b1e":"clf_knn = KNeighborsClassifier(n_neighbors=8)\n\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_train = cross_val_score(clf_knn,X_train, y_train,cv=12,scoring = 'accuracy')\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_test = cross_val_score(clf_knn,X_test, y_test,cv=12,scoring = 'accuracy')\n\nclf_knn_mean_train = np.mean(cv_scores_train)\nclf_knn_mean_test = np.mean(cv_scores_test)\n\nclf_knn.fit(X_train,y_train)\npred = clf_knn.predict(X_test)\n# plot result cv\nplot_result_cv(cv_scores_train,cv_scores_test,clf_knn, pred,'knn')","25941fe9":"# clf_svm = svm.SVC()\n\n# parameters = { 'C':np.arange(1,5,1),'gamma':[0.001, 0.005, 0.01, 0.05, 0.09, 0.1, 0.2, 0.5,1],\n#               'kernel':['rbf', 'sigmoid', 'linear', 'poly',]}\n\n# # Instantiate the grid search model\n# grid_search = GridSearchCV(estimator = clf_svm, param_grid = parameters,cv = 5, n_jobs = -1, verbose = 2,scoring = 'accuracy')\n\n# # Fit the grid search to the data\n# grid_search.fit(X_train, y_train)\n# grid_search.best_params_","5a1c9db1":"clf_svm = svm.SVC(C= 3, gamma= 0.2,kernel = 'rbf')\n\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_train = cross_val_score(clf_svm,X_train, y_train,cv=12)\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_test = cross_val_score(clf_svm,X_test, y_test,cv=12)\n\nclf_svm_mean_train = np.mean(cv_scores_train)\nclf_svm_mean_test = np.mean(cv_scores_test)\n\nclf_svm.fit(X_train,y_train)\npred = clf_svm.predict(X_test)\n# plot result cv\nplot_result_cv(cv_scores_train,cv_scores_test,clf_svm,pred,'svc')\n","917f9f5e":"# # Setup the parameters and distributions to sample from: param_dist\n# parameters = {'bootstrap': [True, False],\n#  'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n#  'max_features': ['auto', 'sqrt'],\n#  'min_samples_leaf': [1, 2, 4],\n#  'min_samples_split': [2, 5, 10],\n#  'n_estimators': [600]}\n\n# # Use the random grid to search for best hyperparameters\n# # First create the base model to tune\n# rf = RandomForestClassifier()\n# # Random search of parameters, using 3 fold cross validation, \n# # search across 100 different combinations, and use all available cores\n# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = parameters, n_iter = 400, cv = 4, verbose=2, random_state=42, n_jobs = -1)\n# # Fit the random search model\n# rf_random.fit(X_train, y_train)\n\n# # view the best parameters from fitting the random search:\n# rf_random.best_params_","5cbd8381":"# # Create the parameter grid based on the results of random search \n# param_grid = {\n#     'bootstrap': [True],\n#     'max_depth': [45,46,47,48,49,50,51,52,53,54,55],\n#     'max_features': ['sqrt'],\n#     'min_samples_leaf': [1,2,3],\n#     'min_samples_split': [2,3,4],\n#     'n_estimators': [600]\n# }\n# # Create a based model\n# rf = RandomForestClassifier()\n# # Instantiate the grid search model\n# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n#                           cv = 5, n_jobs = -1, verbose = 2)\n\n# # Fit the grid search to the data\n# grid_search.fit(X_train, y_train)\n# grid_search.best_params_","92110e4b":"clf_rf = RandomForestClassifier(bootstrap ='True',max_depth = 46,max_features = 'sqrt',\n                                min_samples_leaf = 1, min_samples_split = 3, n_estimators = 600)\n\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_train = cross_val_score(clf_rf,X_train, y_train,cv=12)\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_test = cross_val_score(clf_rf,X_test, y_test,cv=12)\n\nclf_rf_mean_train = np.mean(cv_scores_train)\nclf_rf_mean_test = np.mean(cv_scores_test)\n\nclf_rf.fit(X_train,y_train)\npred = clf_rf.predict(X_test)\n# plot result cv\nplot_result_cv(cv_scores_train,cv_scores_test,clf_rf,pred,'rf')","836c04c4":"df_scores = pd.DataFrame(list_scores)\ndf_scores.style.highlight_max(color = 'lightgreen', axis = 0)","78f6fc64":"## 2.1: Research of depencies\n### 2.1.1: Numerical variables","2f21635b":"The target data is imbalanced \n\nThe challenge of working with imbalanced datasets is that most machine learning techniques will ignore, and in turn have poor performance on, the minority class, although typically it is performance on the minority class that is most important.\n\nOne approach to addressing imbalanced datasets is to use **SMOTE**.\n\nSMOTE is an oversampling method. It works by creating synthetic samples from the minor class instead of creating copies. The algorithm selects two or more similar instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or **SMOTE** for short.","ed7b9ada":"### 3.2.4: Classification with random Forest classifier\n#### random search","df69bcf8":"# Telecom users dataset\n## Practice classification with a telco dataset\n\n<img src=\"data:image\/jpeg;base64,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\" width=\"900px\">\n\n### Goal:\nTo identify people who will and will not renew their contract","ea7fce16":"**Result grid search**\n\n{'bootstrap': True,\n 'max_depth': 46,\n 'max_features': 'sqrt',\n 'min_samples_leaf': 1,\n 'min_samples_split': 3,\n 'n_estimators': 600}","9be528e4":"**best model**","48ca615e":"<div style=\"\n           border-radius:50px;\n           background-color:#7ca4cd;\n           font-size:200%;\n           font-family:Arial;\n           letter-spacing:0.10px\">\n<p style=\"padding: 10px;\n          color:white;\n          text-align:center;\">1: Description of the data (with the calculation of basic statistics)\n<\/p>\n<\/div>","46710045":"<div style=\"\n           border-radius:50px;\n           background-color:#7ca4cd;\n           font-size:200%;\n           font-family:Arial;\n           letter-spacing:0.10px\">\n<p style=\"padding: 10px;\n          color:white;\n          text-align:center;\">2: Research of dependencies and formulation of hypotheses\n<\/p>\n<\/div>","d220ad98":"`TotalCharges` has 10 missing values, we will replace them by the mean.","c0efe2a5":"## 3.1: Preprocessing\n### 3.1.1: target variables","6d9a52a2":"Based on our observations we can proposed that a client is more susceptible to leave if :\n- he has a high `MonthlyCharges`. It is especially true if:\n    - the client is new (low `tenure` < ~15 months).\n    - and has no special services as: `online security`, `tech support`, `online backup` and\/or `device protection` \n- if the decision to leave is easy too take, meaning:\n    - no strong commitment: has a `month-to-month` contract,\n    - no other person involve in the decision: no `dependent` and\/or no `partner`,\n    - evrything can be done on internet or by phone: has `Paperless Billing` and `phone services`\n    \n\n","0e380532":"## 3.2: Machine learning for Classification","f1c656bd":"The random forest classifier is the best model with high accuracy (0.83) and high precission (0.82).","ae8a332c":"We can see that:\n- in general, clients who want to leave (churn = 'Yes'), are new clients (low `tenure` <~15 months, and so low `TotalCharges`) and with high `MonthlyCharges`  > 65$\/per month.\n- there is no linear relation between `tenure` and `TotalCharges`, so there are additional fees that need to be calculated.","60bd4a3b":"`TotalCharges` is an object, we will convert it to a float.","b0c0c400":"This figure shows that, for the customer who left:\n- 91% had 'phone services'\n- 88% had a 'month-to-month' contract,\n- 82% had no 'dependents',\n- 78% had no 'online security',\n- 77% had no 'tech support',\n- 75% had 'Paperless Billing' and are 'senior citizen`\n- 68% received internet with a fiber optic,\n- 65% had no 'online backup' and no 'device protection',\n- 64% had no partner\n- 57% paid by electronic check,\n- 50% had no 'streaming TV', were male and no 'streaming movies',\n- 45 % had 'multiplelines'\n    ","62892f55":"### 3.2.1: Classification with logistic regression","6fb4a882":"**Results gridsearch:**\n\nFitting 5 folds for each of 144 candidates, totalling 720 fits\n\n{'C': 3, 'gamma': 0.2, 'kernel': 'rbf'}","aaae60b7":"**Grid Search with Cross Validation**\n\nRandom search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. To use Grid Search, we make another grid based on the best values provided by random search:","6e73d625":"### 1.2: Asign dtypes","b54c75cf":"<div style=\"\n           border-radius:50px;\n           background-color:#7ca4cd;\n           font-size:200%;\n           font-family:Arial;\n           letter-spacing:0.10px\">\n<p style=\"padding: 10px;\n          color:white;\n          text-align:center;\">4: Comparison of the quality of the obtained models\n<\/p>\n<\/div>","bc5e814d":"### 2.1.2: Categorical variables\n#### Is there a option that push the customer to leave?","63cd548c":"### 1.1: Dealing with missing values ","70e1bda2":"### Summary\nThere are 10 Missing cells.\n\nThere are 3 Numeric variables:\n- `tenure`, `MonthlyCharges`, `TotalCharges`\n\n`Tenure`: Indicates the total amount of months that the customer has been with the company\n\nThere are 13 Categorical variables:\n- `customerID`, `gender`, `SeniorCitizen`, `MultipleLines`, `InternetService`, `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`, `StreamingTV`, `StreamingMovies`, `Contract`, `PaymentMethod`\n\nThere are 5 Boolean variables:\n- `Partner`, `Dependents`, `PhoneService`, `PaperlessBilling`,`Churn` (target variable)\n    \n    \n**26.5% of the customer present in the dataset left the company** \n\n**correlations between variables**\n- `MultipleLines` is highly correlated with `PhoneService`\n- `DeviceProtection`, `TechSupport`, `InternetService`, `StreamingTV`, `StreamingMovies`, `OnlineSecurity`, `OnlineBackup` are well corelated.\n- `Churn` has very poor corelations with `gender`, `MultipleLines` and `PhoneService`\n","619afbf8":"### 3.2.3: Classification with Support Vector Machines\n**Gridsearch**","fceb639b":"### 3.1.2: split the data","dd37f45d":"## 2.2: formulation of hypotheses","2a78f419":"## Don't forget to upvote :D","8e2b757f":"### 3.2.2: Classification with k-Nearest Neighbors","22f86512":"There is no clear relations between extra fees and Churn.","2922a9d5":"**Result Random search**\n\n{'n_estimators': 600,\n 'min_samples_split': 2,\n 'min_samples_leaf': 1,\n 'max_features': 'sqrt',\n 'max_depth': 50,\n 'bootstrap': True}","5fe51c77":"### 3.1.2: Scale the data\nHere, we normalize the continuous variables only, leaving the dummy variables alone. We also use the min-max scaler to give those continuous variables the same minimum of zero, max of one, range of 1.","fba89394":"<div style=\"\n           border-radius:50px;\n           background-color:#7ca4cd;\n           font-size:200%;\n           font-family:Arial;\n           letter-spacing:0.10px\">\n<p style=\"padding: 10px;\n          color:white;\n          text-align:center;\">3: Building models for predicting the outflow\n<\/p>\n<\/div>","1f0e9452":"**best model**"}}