{"cell_type":{"52d675f2":"code","80808521":"code","1ff8286b":"code","441202a4":"code","d8c4c88f":"code","5f3f2c48":"code","94d060a3":"code","9a440664":"code","e426a110":"code","ae55c999":"code","60aa89bb":"code","b274915d":"code","f17bdcd0":"code","593603c8":"code","8dc16a9e":"code","ab20e978":"code","2e222d99":"code","015f9d15":"code","206c5c94":"code","d1ab0820":"code","0f26edba":"markdown","26f312e1":"markdown","ec05a1ae":"markdown","fa62482a":"markdown","e4b9eb1b":"markdown","866d35b4":"markdown","35bd5548":"markdown","732e4102":"markdown","873a1c6b":"markdown","835baa81":"markdown","ff43cb3c":"markdown","d2dd8bfe":"markdown","e844d753":"markdown"},"source":{"52d675f2":"from IPython.display import Image\nImage(filename=\"..\/input\/quora-image\/quora.jpg\")","80808521":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict\nimport string\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import linear_model\nimport eli5\n\nimport os\nprint(os.listdir(\"..\/input\/quora-insincere-questions-classification\"))","1ff8286b":"train = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv')\ntest = pd.read_csv('..\/input\/quora-insincere-questions-classification\/test.csv')\nsub = pd.read_csv('..\/input\/quora-insincere-questions-classification\/sample_submission.csv')\n\nprint('Train data: \\nRows: {}\\nCols: {}'.format(train.shape[0],train.shape[1]))\nprint(train.columns)\n\nprint('\\nTest data: \\nRows: {}\\nCols: {}'.format(test.shape[0],test.shape[1]))\nprint(test.columns)\n\nprint('\\nSubmission data: \\nRows: {}\\nCols: {}'.format(sub.shape[0],sub.shape[1]))\nprint(sub.columns)","441202a4":"temp = train['target'].value_counts(normalize=True).reset_index()\n\ncolors = ['#4f92ff', '#4ffff0']\nexplode = (0.05, 0.05)\n \nplt.pie(temp['target'], explode=explode, labels=temp['index'], colors=colors,\n         autopct='%1.1f%%', shadow=True, startangle=0)\n \nfig = plt.gcf()\nfig.set_size_inches(12, 6)\nfig.suptitle('% Target Distribution', fontsize=16)\nplt.rcParams['font.size'] = 14\nplt.axis('equal')\nplt.show()","d8c4c88f":"def ngram_extractor(text, n_gram):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n# Function to generate a dataframe with n_gram and top max_row frequencies\ndef generate_ngrams(df, col, n_gram, max_row):\n    temp_dict = defaultdict(int)\n    for question in df[col]:\n        for word in ngram_extractor(question, n_gram):\n            temp_dict[word] += 1\n    temp_df = pd.DataFrame(sorted(temp_dict.items(), key=lambda x: x[1])[::-1]).head(max_row)\n    temp_df.columns = [\"word\", \"wordcount\"]\n    return temp_df\n\ndef comparison_plot(df_1,df_2,col_1,col_2, space):\n    fig, ax = plt.subplots(1, 2, figsize=(20,10))\n    \n    sns.barplot(x=col_2, y=col_1, data=df_1, ax=ax[0], color=\"palegreen\")\n    sns.barplot(x=col_2, y=col_1, data=df_2, ax=ax[1], color=\"palegreen\")\n\n    ax[0].set_xlabel('Word count', size=14, color=\"green\")\n    ax[0].set_ylabel('Words', size=14, color=\"green\")\n    ax[0].set_title('Top words in sincere questions', size=18, color=\"green\")\n\n    ax[1].set_xlabel('Word count', size=14, color=\"green\")\n    ax[1].set_ylabel('Words', size=14, color=\"green\")\n    ax[1].set_title('Top words in insincere questions', size=18, color=\"green\")\n\n    fig.subplots_adjust(wspace=space)\n    \n    plt.show()","5f3f2c48":"sincere_1gram = generate_ngrams(train[train[\"target\"]==0], 'question_text', 1, 20)\ninsincere_1gram = generate_ngrams(train[train[\"target\"]==1], 'question_text', 1, 20)\n\ncomparison_plot(sincere_1gram,insincere_1gram,'word','wordcount', 0.25)","94d060a3":"sincere_2gram = generate_ngrams(train[train[\"target\"]==0], 'question_text', 2, 20)\ninsincere_2gram = generate_ngrams(train[train[\"target\"]==1], 'question_text', 2, 20)\n\ncomparison_plot(sincere_2gram,insincere_2gram,'word','wordcount', .35)","9a440664":"sincere_3gram = generate_ngrams(train[train[\"target\"]==0], 'question_text', 3, 20)\ninsincere_3gram = generate_ngrams(train[train[\"target\"]==1], 'question_text', 3, 20)\n\ncomparison_plot(sincere_3gram,insincere_3gram,'word','wordcount', .45)","e426a110":"# Number of words in the questions\ntrain[\"word_count\"] = train[\"question_text\"].apply(lambda x: len(str(x).split()))\ntest[\"word_count\"] = test[\"question_text\"].apply(lambda x: len(str(x).split()))\n\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"word_count\", y=\"target\", data=train, ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Word Count', size=10, color=\"#0D47A1\")\nax.set_ylabel('Target', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Word Count distribution', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","ae55c999":"# Number of unique words in the questions\ntrain[\"unique_word_count\"] = train[\"question_text\"].apply(lambda x: len(set(str(x).split())))\ntest[\"unique_word_count\"] = test[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"unique_word_count\", y=\"target\", data=train, ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Unique Word Count', size=10, color=\"#0D47A1\")\nax.set_ylabel('Target', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Unique Word Count distribution', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","60aa89bb":"# Number of characters in the questions\ntrain[\"char_length\"] = train[\"question_text\"].apply(lambda x: len(str(x)))\ntest[\"char_length\"] = test[\"question_text\"].apply(lambda x: len(str(x)))\n\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"char_length\", y=\"target\", data=train, ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Character Length', size=10, color=\"#0D47A1\")\nax.set_ylabel('Target', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Character Length distribution', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","b274915d":"# Number of stop words in the questions\ntrain[\"stop_words_count\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest[\"stop_words_count\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"stop_words_count\", y=\"target\", data=train, ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Number of stop words', size=10, color=\"#0D47A1\")\nax.set_ylabel('Target', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Number of Stop Words distribution', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","f17bdcd0":"# Number of punctuations in the questions\ntrain[\"punc_count\"] = train[\"question_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest[\"punc_count\"] = test[\"question_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"punc_count\", y=\"target\", data=train[train['punc_count']<train['punc_count'].quantile(.99)], ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Number of punctuations', size=10, color=\"#0D47A1\")\nax.set_ylabel('Target', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Punctuation distribution', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","593603c8":"# Number of upper case words in the questions\ntrain[\"upper_words\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"upper_words\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"upper_words\", y=\"target\", data=train[train['upper_words']<train['upper_words'].quantile(.99)], ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Number of Upper case words', size=10, color=\"#0D47A1\")\nax.set_ylabel('Target', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Upper case words distribution', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","8dc16a9e":"# Number of title words in the questions\ntrain[\"title_words\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest[\"title_words\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"title_words\", y=\"target\", data=train[train['title_words']<train['title_words'].quantile(.99)], ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Number of Title words', size=10, color=\"#0D47A1\")\nax.set_ylabel('Target', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Title words distribution', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","ab20e978":"# Mean word length in the questions\ntrain[\"word_length\"] = train[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest[\"word_length\"] = test[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\nfig, ax = plt.subplots(figsize=(15,2))\nsns.boxplot(x=\"word_length\", y=\"target\", data=train[train['word_length']<train['word_length'].quantile(.99)], ax=ax, palette=sns.color_palette(\"RdYlGn_r\", 10), orient='h')\nax.set_xlabel('Mean word length', size=10, color=\"#0D47A1\")\nax.set_ylabel('Target', size=10, color=\"#0D47A1\")\nax.set_title('[Horizontal Box Plot] Distribution of mean word length', size=12, color=\"#0D47A1\")\nplt.gca().xaxis.grid(True)\nplt.show()","2e222d99":"# Get the tfidf vectors\ntfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\ntfidf_vec.fit_transform(train['question_text'].values.tolist() + test['question_text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train['question_text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test['question_text'].values.tolist())","015f9d15":"y_train = train[\"target\"].values\n\nx_train = train_tfidf\nx_test = test_tfidf\n\nmodel = linear_model.LogisticRegression(C=5., solver='sag')\nmodel.fit(x_train, y_train)\ny_test = model.predict_proba(x_test)[:,1]","206c5c94":"eli5.show_weights(model, vec=tfidf_vec, top=100, feature_filter=lambda x: x != '<BIAS>')","d1ab0820":"sub['prediction'] = y_test\nsub.to_csv('baseline_submission.csv',index=False)","0f26edba":"**A base model with vectorized matrix shows that insincere words are predominantly due to identification of religion, nationality, race, caste, political affiliation, etc. **","26f312e1":"**Insincere questions have more words per question**","ec05a1ae":"**More to come!!!**\n* Note to self: Questions marks are present in the words. Should remove them while processing.","fa62482a":"**Insincere questions have more characters than sincere questions**","e4b9eb1b":"**Thanks for viewing my Kernel! If you like my work and find it useful, please leave an upvote! :)**\n\n**Key Insights:** \n\n* 6.2% of the questions in training data are insincere\n* Insincere questions are dominated by words like trump, women, white, men, indian, muslims, black, americans, girls, indians, sex and india. More reference to specific groups of people of directly a person i.e. Donald Trump. \n* Top 3 bigrams in the insincere questions are 'Donald Trump', 'White People' and 'Black People'. Questions related to race are highly insincere. Presence of Chinese people, Indian muslims, Indian girls, North Indians, Indian women and White Women confirm the same.\n* Insincere questions are related to hypothetical scenarios, age, race, etc\n* Sincere questions are related to tips, advices, suggestions, facts, etc. \n* Insincere questions have more words, characters, stop words and punctuations","866d35b4":"**6.2 % of train questions are insincere**","35bd5548":"**Top 20 3-gram words in sincere and insincere questions**\n* Insincere questions are related to hypothetical scenarios, age, race, etc\n* Sincere questions are related to tips, advices, suggestions, facts, etc. ","732e4102":"Thanks to [SRK's](https:\/\/www.kaggle.com\/sudalairajkumar) exploratory [kernel](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc) for the custom functions. I modified them a bit so that I can reuse for any dataframe and n-gram combination. ","873a1c6b":"**Insincere questions have more punctuations**","835baa81":"**Top 20 2-gram words in sincere and insincere questions**\n* Top 3 bigrams in the insincere questions are 'Donald Trump', 'White People' and 'Black People'. Questions related to race are highly insincere. \n* Presence of Chinese people, Indian muslims, Indian girls, North Indians, Indian women and White Women confirm the same.\n* Sincere questions have best way, year old, will happen, etc. as the top ones. No clear trend there but 'best' is the key word to look for. ","ff43cb3c":"**More upper case words in Sincere questions**","d2dd8bfe":"**Insincere questions have more stop words than sincere questions**","e844d753":"**Top 20 1-gram words in sincere and insincere questions**\n* Sincere questions are dominated by words like best, will, people, good, one, etc. with no reference to any specific nouns.  Some of these words are high even in insincere words - meaning they are not significant to the classification. \n* Insincere questions are dominated by words like trump, women, white, men, indian, muslims, black, americans, girls, indians, sex and india. More reference to specific groups of people of directly a person i.e. Donald Trump. "}}