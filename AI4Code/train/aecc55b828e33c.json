{"cell_type":{"c1897ba1":"code","fb966310":"code","101e910b":"code","12f1622e":"code","31b21cba":"code","990b7b11":"code","ec642d4d":"code","0ec4e024":"code","472d410c":"code","fd0fdbdc":"code","74efba34":"code","f48160f2":"code","1f03f1e7":"code","e1dfc8bc":"markdown","f6c410f6":"markdown","09a13370":"markdown","deac360e":"markdown","d8ba0241":"markdown","79eee65b":"markdown","e5a4e783":"markdown","d48eb090":"markdown","fa8b4913":"markdown","f4343fa4":"markdown","eabf6b59":"markdown","237a8224":"markdown","5dc10a3c":"markdown","d160e8a2":"markdown"},"source":{"c1897ba1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n%matplotlib inline","fb966310":"from sklearn.datasets import load_boston\nboston_data = load_boston()\ndf =pd.DataFrame(boston_data.data,columns=boston_data.feature_names)","101e910b":"df.head()","12f1622e":"X = df\ny = boston_data.target","31b21cba":"X_constant = sm.add_constant(X)","990b7b11":"model = sm.OLS(y, X_constant)\nlin_reg = model.fit()","ec642d4d":"lin_reg.summary()","0ec4e024":"f_model = smf.ols(formula = 'y ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT', \n              data=df)\nf_lin_reg = f_model.fit()\nf_lin_reg.summary()","472d410c":"print(lin_reg.predict(X_constant[:10]))\nprint(f_lin_reg.predict(X_constant[:10]))","fd0fdbdc":"pd.options.display.float_format = '{:,.4f}'.format\ncorr = df.corr()\ncorr[np.abs(corr) < 0.65] = 0\nplt.figure(figsize=(16,10))\nsns.heatmap(corr, annot=True, cmap='YlGnBu')\nplt.show()","74efba34":"from sklearn.metrics import r2_score\nlinear_reg = smf.ols(formula = 'y ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT', \n              data=df)\nbase = linear_reg.fit()\nprint(r2_score(y, base.predict(df)))","f48160f2":"# WITHOUT LSTAT\nlinear_reg = smf.ols(formula = 'y ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B', \n              data=df)\nbase = linear_reg.fit()\nprint(r2_score(y, base.predict(df)))","1f03f1e7":"# WITHOUT AGE\nlinear_reg = smf.ols(formula = 'y ~ CRIM + ZN + INDUS + CHAS + NOX + RM +DIS + RAD + TAX + PTRATIO + B + LSTAT', \n              data=df)\nbase = linear_reg.fit()\nprint(r2_score(y, base.predict(df)))","e1dfc8bc":"The R2 score that I got was 0.7406 and now this will serve as my base number. If the removal of a feature changes the R2 score significantly then that feature can be termed as an important feature.\n\nFor e.g. if from the above formula, LSTAT is removed then the R2 score drops to 0.68 whereas if AGE is removed it stays at 0.74. Hence, LSTAT is an important feature.","f6c410f6":"Let\u2019s look at the correlation between the different features.\n\n","09a13370":"Now, let\u2019s add some extra constant term to allow statsmodels to calculate the bias.","deac360e":"When dealing with multiple features, simple linear regression loses its charm and so Multiple regression is necessary for encapsulating the effect of multiple features.\n\n","d8ba0241":"Now, getting the data.\n\n","79eee65b":"To see what the actual prediction for 10 rows of the features dataframe looks like, run these:","e5a4e783":"These are some ways you can check for feature importance and see if feature engineering can help make better features that uplifts the model.","d48eb090":"(Just an add-on. If you are interested about which feature is important then go through this.)","fa8b4913":"There you go! That\u2019s how you perform Multiple Regression using Python!","f4343fa4":"Let\u2019s also try using the R2 score to see the importance of features.\n\n","eabf6b59":"Now, let\u2019s instantiate and fit our model with an ordinary least square model.\n\n","237a8224":"Another way to make your model would be to actually use the formula,\n\n","5dc10a3c":"To see the results, run the following code:\n\n","d160e8a2":"Then, for simplicity, I\u2019ll be using X and y to denote the feature and target variables."}}