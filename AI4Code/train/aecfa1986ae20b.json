{"cell_type":{"0f87f7d8":"code","0e01a0e4":"code","8851b9d4":"code","11e16fd3":"code","8c9f1899":"code","443ec3b7":"code","0b7501bb":"code","716da45c":"code","7aa9409b":"code","e43c1e71":"code","20f9485b":"code","b551ce02":"code","44c7642e":"code","19022551":"code","d86c489a":"code","a74d80a7":"code","ded86e5b":"code","103a2b60":"code","68a43390":"code","d6e17211":"code","3bdd096c":"code","3582e748":"code","7994e701":"code","32c9c11d":"code","a89cd5e9":"code","d111cbd0":"code","c7cfd066":"code","c245ab0b":"markdown","d37c795e":"markdown","e09fd002":"markdown","bbcaea90":"markdown","4a7f8c13":"markdown","6dfbac44":"markdown","08727e09":"markdown","caf6f9e3":"markdown","aee45061":"markdown","445f3fae":"markdown","5cdf2c69":"markdown","0d5c6f71":"markdown","3291002a":"markdown","0ee3619f":"markdown","d9bb753d":"markdown","27f13545":"markdown","44d09f53":"markdown","e941154c":"markdown","c279c96f":"markdown","05930b7d":"markdown","cb6f1f71":"markdown","c7055f86":"markdown","7a6a3aeb":"markdown","b91d5a10":"markdown","80f86c60":"markdown","8c0dc7d0":"markdown","8155cb53":"markdown","8113cc3d":"markdown","615bc2ea":"markdown","bb6f9314":"markdown","92ac541e":"markdown","e2efa45a":"markdown"},"source":{"0f87f7d8":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimport numpy as np\nimport os\nimport sys\nimport random\nimport tensorflow as tf\nfrom pathlib import Path\nfrom six.moves import urllib\nimport tarfile\nimport shutil\n\nfrom tensorflow.keras import applications\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import metrics\nfrom tensorflow.keras import Model\n\nfrom tensorflow.keras.applications import inception_v3\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3","0e01a0e4":"target_size = (224, 224)\n\ninception_model = InceptionV3(weights='imagenet', input_shape = target_size + (3,), include_top=False)\n\n#uncomment the line below to see the layers structure of our network\n#inception_model.summary()","8851b9d4":"flat_layer = layers.Flatten()(inception_model.output)\ndense_layer_1 = layers.Dense(512, activation='relu')(flat_layer)\ndense_layer_1 = layers.BatchNormalization()(dense_layer_1)\ndense_layer_2 = layers.Dense(256, activation='relu')(dense_layer_1)\ndense_layer_2 = layers.BatchNormalization()(dense_layer_2)\ndense_layer_3 = layers.Dense(256, activation='relu')(dense_layer_2)\n\n# next I specify which layers to 'freeze' and which layers to keep training\n# freeze all convolutional InceptionV3 layers \n# and train only the layers I added at the top (initialized with random values)\nfor layer in inception_model.layers:\n    layer.trainable = False\n\n#define a model with the layers we've just connected\ntransfer_inception_model = Model(inputs = inception_model.inputs, outputs = dense_layer_3)","11e16fd3":"#our class will inherit from https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Layer\nclass SimilarityLayer(layers.Layer):\n    # compute and return the two distances:\n    # d(anchor,positive) \n    # d(anchor,negative)\n    \n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        \n    def call(self, anchor, positive, negative):\n        d1 = tf.reduce_sum(tf.square(anchor-positive), -1)\n        d2 = tf.reduce_sum(tf.square(anchor-negative), -1)\n        return(d1,d2)\n    \nanchor = layers.Input(name='anchor', shape = target_size + (3,))\npositive = layers.Input(name='positive', shape = target_size + (3,))\nnegative = layers.Input(name='negative', shape = target_size + (3,))\n\nsim_layer_output = SimilarityLayer().call(\n    transfer_inception_model(inputs = inception_v3.preprocess_input(anchor)),\n    transfer_inception_model(inputs = inception_v3.preprocess_input(positive)),\n    transfer_inception_model(inputs = inception_v3.preprocess_input(negative))\n)\n\nsiamese_model = Model(inputs=[anchor, positive,negative], outputs=sim_layer_output)","8c9f1899":"class SiameseModelClass(Model):\n    def __init__(self, siamese_model, margin = 0.5):\n        super(SiameseModelClass, self).__init__()\n        \n        self.siamese_model = siamese_model\n        self.margin = margin\n        \n        # create a Metric instance to track the loss\n        self.loss_tracker = metrics.Mean(name=\"loss\")\n        \n    def call(self, inputs):\n        return self.siamese_model(inputs)\n    \n    # customize the training process: providing our own training step\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            # call custom loss function\n            loss = self.custom_loss(data)\n            \n        # Compute gradients\n        trainable_vars = self.siamese_model.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        \n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        \n        # Update our training loss metric\n        self.loss_tracker.update_state(loss)\n        \n        return {\"loss\": self.loss_tracker.result()}\n    \n    # providing our own evaluation step\n    def test_step(self, data):\n        # call custom loss function\n        loss = self.custom_loss(data)\n        \n        # Update our test loss metric\n        self.loss_tracker.update_state(loss)\n        \n        return {\"loss\": self.loss_tracker.result()}\n    \n    # custom loss function\n    def custom_loss(self, data):\n        # get the distances tuple from the siamese model output\n        d1, d2 = self.siamese_model(data)\n        \n        # compute the triplet loss\n        loss = tf.maximum(d1 - d2 + self.margin, 0)\n        \n        return loss\n    \n    @property\n    def metrics(self):\n        # We list our `Metric` objects here so that `reset_states()` can be\n        # called automatically at the start of each epoch\n        # or at the start of `evaluate()`.\n        # If you don't implement this property, you have to call\n        # `reset_states()` yourself at the time of your choosing.\n        return [self.loss_tracker]","443ec3b7":"#helper function\ndef download_and_uncompress_tarball(tarball_url, dataset_dir):\n    \"\"\"Downloads the `tarball_url` and uncompresses it locally.\n    Args:\n    tarball_url: The URL of a tarball file.\n    dataset_dir: The directory where the temporary files are stored.\n    \"\"\"\n    filename = tarball_url.split('\/')[-1]\n    filepath = os.path.join(dataset_dir, filename)\n\n    def _progress(count, block_size, total_size):\n        sys.stdout.write('\\r>> Downloading %s %.1f%%' % (\n            filename, float(count * block_size) \/ float(total_size) * 100.0))\n        sys.stdout.flush()\n\n    filepath, _ = urllib.request.urlretrieve(tarball_url, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n    tarfile.open(filepath, 'r:gz').extractall(dataset_dir)","0b7501bb":"# URL for sourcing the funneled images\ndatabase_url = 'http:\/\/vis-www.cs.umass.edu\/lfw\/lfw-deepfunneled.tgz'\n\nroot_folder = '..\/working'\ndownload_folder = root_folder + '\/'+ 'data\/lfw_original'\nselection_folder = root_folder + '\/' + 'data\/lfw_selection'\ndownload_path = download_folder + '\/lfw-deepfunneled.tgz'\n\nif not os.path.exists(download_folder):\n    os.makedirs(download_folder)\n\nif not os.path.exists(selection_folder):\n    os.makedirs(selection_folder)\n    \nif not os.path.exists(download_path):\n    download_and_uncompress_tarball(database_url, download_folder)","716da45c":"extracted_folder = download_folder + '\/lfw-deepfunneled'\n\n# images are organized into separate folders for each person\n# get a list of subfolders \nsubfolders = [x[0] for x in os.walk(extracted_folder)]\n\n# first item is root the folder itself\nsubfolders.pop(0) ","7aa9409b":"people_list = []\n\nfor path in subfolders:\n    image_count = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n    people_list.append((path.split('\\\\')[-1], image_count))\n    #people_count.append((path, image_count))\n    \n# Sort from max to min images per person\npeople_list = sorted(people_list, key=lambda x: x[1], reverse=True)","e43c1e71":"print(f'Number of people: {len(subfolders)}')\nprint(f'Number of people with only one photo: {len([person for person, image_count in people_list if image_count==1])}')\nprint(f'Number of people with >=5 photos: {len([person for person, image_count in people_list if image_count>=5])}')","20f9485b":"# dictionary of selected persons, where \n# - key = rank\n# - value = list of names of the image files for this person\nselected_persons = {}\ni = 0\n\nfor person,image_count in people_list:\n    if image_count >=5:\n        file_list = []\n        \n        # create new folder in selected images path\n        newpath = selection_folder + '\/' + person.split('\/')[-1]\n        if not os.path.exists(newpath):\n            os.makedirs(newpath)\n        \n        # copy \/ paste first 5 images to the new location\n        files = [os.path.join(person, f) for f in os.listdir(person) if os.path.isfile(os.path.join(person, f))]\n        files = files[0:5] # select first 5 images\n        for file in files:\n            filename = file.split('\/')[-1]\n            shutil.copyfile(file, newpath + '\/' + filename)\n            file_list.append(newpath + '\/' + filename)\n            \n        selected_persons[i] = file_list\n        i = i + 1","b551ce02":"triplets = []\n\nfor item in selected_persons.items():\n    images = item[1]\n    \n    for i in range(len(images)-1):\n        for j in range(i+1,len(images)):\n            anchor = images[i]\n            positive = images[j]\n            \n            # choose a random negative\n            # first generate a random class rank and make sure we're not selecting the current class\n            random_class = item[0]\n            while random_class == item[0]:\n                random_class = random.randint(0, len(selected_persons)-1)\n            # selected a random image from the 5 that any of our classes has\n            random_image = random.randint(0, 4)\n            negative = selected_persons[random_class][random_image]\n            \n            triplets.append((anchor, positive, negative))","44c7642e":"# helper function for needed preprocessing of our jpg images\ndef preprocess_image(filename):\n    image_string = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(image_string, channels = 3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, target_size)\n    return image","19022551":"def plot_images(triplets):\n    def show(ax, image):\n        ax.imshow(image)\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n    \n    fig = plt.figure(figsize=(7,12))\n    axis = fig.subplots(5, 3)\n    \n    for i in range(0,5):\n        anchor,positive,negative = triplets[40+i]\n        show(axis[i,0], preprocess_image(anchor))\n        show(axis[i,1], preprocess_image(positive))\n        show(axis[i,2], preprocess_image(negative))","d86c489a":"plot_images(triplets)","a74d80a7":"# helper function for transforming a tuple of filenames \n# into a tuple of corresponding tensors\ndef preprocess_triplets(anchor, positive, negative):\n    \"\"\"\n    Inputs: a tuple of filenames\n    Output: a tuple of preprocessed images \n    \"\"\"\n\n    return (\n        preprocess_image(anchor),\n        preprocess_image(positive),\n        preprocess_image(negative)\n    )","ded86e5b":"rng = np.random.RandomState(seed=101)\nrng.shuffle(triplets)","103a2b60":"anchor_images = [a_tuple[0] for a_tuple in triplets]\npositive_images = [a_tuple[1] for a_tuple in triplets]\nnegative_images = [a_tuple[2] for a_tuple in triplets]\n\nanchor_dataset = tf.data.Dataset.from_tensor_slices(anchor_images)\npositive_dataset = tf.data.Dataset.from_tensor_slices(positive_images)\nnegative_dataset = tf.data.Dataset.from_tensor_slices(negative_images)\n\ndataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\ndataset = dataset.shuffle(buffer_size=1024)\ndataset = dataset.map(preprocess_triplets)","68a43390":"training_data = dataset.take(round(image_count * 0.8))\nvalidation_data = dataset.skip(round(image_count * 0.8))\n\ntraining_data = training_data.batch(32, drop_remainder=False)\ntraining_data = training_data.prefetch(8)\n\nvalidation_data = validation_data.batch(32, drop_remainder=False)\nvalidation_data = validation_data.prefetch(8)","d6e17211":"# training on CPU will take a bit longer\n# use this code only if you ran out of GPU time\n'''\nimport time\nstart_time = time.time()\n\nepochs = 15\n\nsiameze_custom_model = SiameseModelClass(siamese_model)\nsiameze_custom_model.compile(optimizer = optimizers.Adam(0.0001))\nsiameze_custom_model.fit(training_data, epochs=epochs, validation_data = validation_data)\n\nstop_time = time.time()\nprint(f'It took {(stop_time - start_time)} to train for {epochs} epochs.')\n'''","3bdd096c":"# use the GPU training instead\n\nmodel_on_GPU = SiameseModelClass(siamese_model)\nmodel_on_GPU.compile(optimizer = optimizers.Adam(0.0001))\n#model_on_GPU.fit(training_data, epochs=epochs, validation_data = validation_data)\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","3582e748":"import time\nstart = time.time()\n\nepochs = 15\nwith tf.device('\/gpu:0'):\n    history = model_on_GPU.fit(\n        training_data, \n        epochs=epochs, \n        validation_data = validation_data\n    )\nstop = time.time()\nprint(f'Training on GPU took: {(stop-start)\/60} minutes')","7994e701":"print(history.history.keys())","32c9c11d":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss during training')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","a89cd5e9":"sample = next(iter(training_data))\n#plot_images(*sample)\n\nanchor, positive, negative = sample\nanchor_embedding, positive_embedding, negative_embedding = (\n    transfer_inception_model(inputs = inception_v3.preprocess_input(anchor)),\n    transfer_inception_model(inputs = inception_v3.preprocess_input(positive)),\n    transfer_inception_model(inputs = inception_v3.preprocess_input(negative)),\n)","d111cbd0":"d1 = np. sum(np. power((anchor_embedding-positive_embedding),2))\nprint(f'Anchor-positive difference = {d1}')\n\nd2 = np. sum(np. power((anchor_embedding-negative_embedding),2))\nprint(f'Anchor-negative difference = {d2}')","c7cfd066":"cosine_similarity = metrics.CosineSimilarity()\n\npositive_similarity = cosine_similarity(anchor_embedding, positive_embedding)\nprint(\"Positive similarity:\", positive_similarity.numpy())\n\nnegative_similarity = cosine_similarity(anchor_embedding, negative_embedding)\nprint(\"Negative similarity\", negative_similarity.numpy())","c245ab0b":"*What CNN layers learn*\n\nComplementing the intuition above, <a href='https:\/\/arxiv.org\/abs\/1311.2901'>Zeiler and Fergus, 2013<\/a> introduced for the first time a method to see what CNNs layers are actually learning and proved what researchers already suspected was happening.  \n\n<img src='https:\/\/mihaelagrigore.info\/wp-content\/uploads\/2021\/05\/2-What-do-CNN-learn.png'\/>\n-- Figure 2: snippet of figures from the Zeiler and Fergus 2013 with cyan highlighting applied by myself --\n\nFigure 2 is a snippet of one of the figures presented by Zeiler and Fergus 2013. The cyan highlight is my own addition and the figure is cropped, to prevent information overload and help us stay focused on a simple take home message. On the left we have activations of a neuron in a particular layer projected back to the input pixels space and on the right we have the original input patch that the particular neuron 'sees'. A more intuitive way to think of this is that we are looking at what patterns a neuron in a certain layer has a stronger response for. And we can see in the cyan highlight that a neuron in layer 2 is mostly activated by a vertical pattern, while the highlighted neuron in layer 5 responds to the shape of dog heads. ","d37c795e":"*How many exemplars per class do we need ?*\n\nFor this problem, each training example will consist of a triplet of images: the anchor, the positive and the negative image.  \n\nThe anchor and the positive image are two different pictures from the same class.  \n\nIf one class has n exemplars, we can make this amount of anchor,positive pairs:\n\n![image.png](attachment:image.png)\n\nwhere k is 2 in our case.  \n\nSo, if we have 5 different images for a class, we will be able to compose 5! \/ ((5-2)!\\*2! = 10 anchor-positive pairs.  \n\nThen, to complete our triplet, we will select one random image from any other class expect the one we got the anchor from.","e09fd002":"These are just some transformations specific to TF data structures. ","bbcaea90":"Wrap a custom Model class aroung the siamese_model above so that we can implement a custom training loop that optimizes the difference between distances.  \n\nFor the class definition below I used <a href='https:\/\/keras.io\/guides\/customizing_what_happens_in_fit\/'>Keras' official documentation for writing your own training loop<\/a>.","4a7f8c13":"## Conclusions\n\nWe can see from the training output that validation error increased right from the start, which is definitely not a god sign. It looks like overfitting right from the start. It could be the case that I am using a too complicated network for a too simple dataset.  \n\nRemember we took a pretrained inception model, removed the top layer and added a few more Dense layers. Remember that I also fixed all other layers except for these Dense layers at the end, so I'm only training these ones. Maybe the Dense layers are too large or I used too many of them and my dataset is too simple for this. There are some standard measures to prevent overfitting, but I will address those in another Notebook.","6dfbac44":"Extract the list of people in our dataset.","08727e09":"## The Inception Model\n\nSo, I covered so far why we use pre-trained models and why transfer learning would work. Next I had to choose which model to use. I decided to use the Inception network model, introduced by <a href='https:\/\/arxiv.org\/abs\/1409.4842'>Szegedy et al 2014<\/a>. First, the Inception model is also the one chosen by the authors of the <a href='https:\/\/arxiv.org\/abs\/1503.03832'>FaceNet system<\/a>, which is the system I will use for facial recognition later on. Second, the Inception model introduced an interesting concept that should be studied by someone learning about Convolutional Neural Networks.  \n\nUp until Inception, CNN models would normally use identically shaped filters in a certain layer. For example, in Figure 1, for the first layer, I used 6 filters of shape 3x3x3 and stacked their outputs together on the depths dimension. Inception would use 3 different filter shapes (1x1, 3x3 and 5x5) and a max pooling in the same layer. \n\n<img src='https:\/\/mihaelagrigore.info\/wp-content\/uploads\/2021\/05\/3-Inception-CNN-model-1.png'\/>\n-- Figure 3: Inception module --\n\nThe Inception network is mostly composed of layers (Inception *modules*) with the structure shown in Figure 3. This figure is reproduced from the original article by <a ref='https:\/\/arxiv.org\/abs\/1409.4842'>Szegedy et al 2014<\/a>. The Inception *module* on the right contains additional 1x1 convolutions which have the role of reducing computational cost. Andrew Ng has a short video capsule explaining why this is the case, part of the course on CNN from the Deep Learning specialization offered by DeepLearning.ai on Coursera, also available on YouTube as <a href='https:\/\/www.youtube.com\/watch?v=C86ZXvgpejM'>C4W2L06<\/a>.","caf6f9e3":"Notice in the image above that, when we want:   \n\nd(anchor,positive) - d(anchor,negative) <= 0 - margin \n\nThe margin parameter is a standard method to prevent the network from learning the redundant solution: if we asked the network simply to have the left side of the equation <= 0, it could learn the solution of outputting 0 for the distance every time, regardless of the input. The margin makes sure d=0 will not be the solution the network converges to.\n\nAs the outputs of the network are embeddings (vectors), we will compute the distance between them as the L2 norm of the difference between the two vectors.\n\n![image.png](attachment:image.png)\n\nNow let's continue to build our network from where we left off, from the *transfer_inception_model*, which was built from a pretrained InceptionV3 without the top layer plus 3 additional Dense layers to be trained for the new task.  \n\nWe need to 'assemble' the network from Figure 7 now.","aee45061":"Overview of the number of images in our dataset.","445f3fae":"## Training\n\nNow we're ready to train our model.","5cdf2c69":"Remember our images were in sorted in a specific way:\n- we have 10 tuples from each class \n- classes were sorted in alphabetical order\n- images in each class were also sorted in alphabetical order  \nSo far we did not alter this ordering. But we want to start training on a randomized dataset.","0d5c6f71":"## Facial recognition\n\n**One shot learning**\n\nFacial recognition is a problem of the type <a href='https:\/\/en.wikipedia.org\/wiki\/One-shot_learning'>**one-shot learning**<\/a>. The model has to learn who this person is by only looking at one (new) picture of this person. The typical example here is that you're building a facial recognition system for the employees of a company and usually HR only has one picture for each employee. This is in contrast with tyical machine learning problem where a model learngs from hundreds or thousands of examples of a particular class.   \n\n**One way to do facial recognition**  \nTo continue with the facial recognition for employes' accessing the building, a way to implement face recognition with CNN can be as in Figure 5 below. We train a CNN on our database of images (our 5 employees) and use a softmax unit at the end with 6 outputs (1 for each employee and 1 for the option 'none of them'). \n\n![image-4.png](attachment:image-4.png)\n\n-- Figure 5: Usual approach for CNN training --\n\nIssues with the implementation above for the given problem:\n- would not work well for a small training set\n- would need retraining when we need to recognize new persons, right ? Because our output layer learned to predict a fixed number of classes. New class, new training.\n\n**Better alternative: Similarity Function**  \nThe alternative to learning what each class looks like is to learn a similarity function, a function of the degree of difference between images. \n\nThe output of such a function would be something like:  \nd(picture_1, picture_2) = the degree of difference between two pictures. \n\nAfterwards, the facial recognition function would compare the degree of difference between the new input image and an image from our database.  \nIf d < thereshold => it's the same person.","3291002a":"Generate the list of (anchor, positive, negative) triplets for our training and testing.  \n\nFor anchor and positive we generate all combinations of two from the 5 available images per person. \n\nFor the negative, we choose a random class and a random image rank (of the 5 images we have per class).","0ee3619f":"In the FaceNet paper, <a href='https:\/\/arxiv.org\/abs\/1503.03832'>Schroff et al 2015<\/a> describe a more professional method to select triplets. Ideally, the negative that accompanies a pair of anchor and positive should be a difficult negative, not just a random image. I implemented the method above only for simplicity.","d9bb753d":"### Siamese Network\n\nTo compute the degree of difference, we could compare pixel by pixel. But this would not give the best results when we deal with different pose, different illumination etc for the same person.  \n\nA better alternative is called a Siamese Network (<a href='https:\/\/www.cs.toronto.edu\/~ranzato\/publications\/taigman_cvpr14.pdf'>Taigman et al 2014<\/a>). This method uses two identical CNNs that compute **embeddings** - vector encodings of the input image. And aftwrwards it compares the **distance** between these embeddings. \n\n![image-2.png](attachment:image-2.png)\n\n-- Figure 6: Siamese Network --","27f13545":"### Faces Dataset\n\nFor transfer learning and for testing the model on the face recognition task I want to use one of common datasets for this task.  \n\nIn the article introducing the FaceNet system (which made the Triplet function popular), <a href='https:\/\/arxiv.org\/abs\/1503.03832'>Schroff et al 2015<\/a> use two datasets: <a href='http:\/\/vis-www.cs.umass.edu\/lfw\/'>Labeled Faces in the Wild (LFW)<\/a> and <a href='https:\/\/www.cs.tau.ac.il\/~wolf\/ytfaces\/'>YouTube Faces DB<\/a>. I will be using the LFW dataset. Mind that LFW official website warns that LFW is \"a public benchmark for face verification\" and not suitable for commercial-grade face recognition systems, but for the purpose of this Notebook it's good enough.    \n\nLFW consists of 13.000 images collected from the internet. Each image is labelled with the name of the person in the image.  \n\nThe number of images per person varies. I assume there are people for which only one image is available. I will thus not use the whole dataset. I will first do a basic exploration of the dataset and then decide on the threshold of minimum number of images per person for selecting images for my dataset.  \n\nAccording to <a href='http:\/\/vis-www.cs.umass.edu\/lfw\/'>LFW website<\/a>, there are multiple versions of this dataset, besides the original one. One of these versions contain \"funneled images\". <a href='http:\/\/vis-www.cs.umass.edu\/deep_funnel.html'>Deep funneling<\/a> is a method to reduce within class variability, so I'm gonna work with this one, because I have limited computing power for transfer learning training, so I want to make my task easier.","44d09f53":"Visualize training results","e941154c":"## Keras Applications\n\nFor the implementation, I will use Keras. <a href='https:\/\/keras.io\/api\/applications\/'>Keras Applications<\/a> makes available pre-trained deep learning models. I think this is more or less the same as <a href='https:\/\/github.com\/tensorflow\/models'>Tensorflow Model Garden<\/a>. So if you've used or heard of the TF Model garden before and now you're wondering why there's a Keras 'garden' too, especially since Keras is now the official high level API for Tensorflow 2, I think you can use either of the two. I decided to go with Keras because it's more convenient, easier to use and it offers everything I need for the purpose of this projects. \n\n<a href='https:\/\/keras.io\/api\/applications\/'>Keras Applications<\/a> offers two versions of the Inception network: <a href='https:\/\/arxiv.org\/abs\/1512.00567'>InceptionV3<\/a> and InceptionResNetV2. which I assume is a mix of Inception and ResNet, but I will not go into ResNet in this Notebook, to keep complexity under control. I have a separate Notebook on Kaggle on <a href='https:\/\/www.kaggle.com\/mishki\/resnet-keras-code-from-scratch-train-on-gpu'>coding a ResNet from scratch using Keras<\/a>, if interested. These CNN models have been trained on the <a href='https:\/\/imagenet.stanford.edu\/'>ImageNet dataset<\/a>.\n\nFigure 4 shows a benchmark of popular deep neural network models for visual recognition. The graph shows Top-1 accuracy and model complexity. We can see that Inception V4 has one of the highest accuracies and relatively small complexity among networks that give similar results. Unfortunately, v4 is only available in the TF Model Garden, but not in Keras Applications. So I will settle to v3.\nFigure 4 is reproduced from <a href='https:\/\/arxiv.org\/pdf\/1810.00736.pdf'>Bianco et al 2018<\/a>.\n\n<img src='https:\/\/mihaelagrigore.info\/wp-content\/uploads\/2021\/04\/Benchmark-of-representative-deep-neural-networks-architectures-small.jpg' style=\"width: 600px;\"\/>\n-- Figure 4: Ball chart reporting the Top-1 accuracy vs. computational complexity of representative Deep Neural Network architectures --","c279c96f":"Now count the number of available images per person.","05930b7d":"Visualize a few of our triplets.","cb6f1f71":"Check the embeddings distances learned by the model.  \nWe expect the difference between anchor and positive image to be larger than the distance between the anchor and the negative example.","c7055f86":"First, download the dataset.","7a6a3aeb":"Start by importing the pretrained model. For examples of how to get started with this handling this an Applications model I used the <a href='https:\/\/keras.io\/api\/applications\/'>official documentation<\/a>.\n\nI mentioned earlier that transfer learning consists of freezing the parameters (weights and biases) of a pretrained model and retraining for the current task only the last layer(s).  \n\nI will load the pretrained InceptionV3 model (trained on the ImageNet dataset), but without the fully connected layer at the top (last layer in the network). This is the layer that outputs predictions for the ImageNet dataset. I then replace this layer with three more fully connected layers, to give the model a chance to learn new features for the image recognition task. And I define a new Model based on all these layers. To see how to work with Keras Models check out the <a href='https:\/\/keras.io\/api\/models\/model\/#model-class'>Model class documentation<\/a>.\n\nFor the first two fully connected layers I will apply Batch Normalization for reasons explained <a href='https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/BatchNormalization'>here<\/a>","b91d5a10":"### What did our network learn ?\n\nOur network learned to produce embeddings that maximize the following function for a tuple of three images received as input (anchor, positive, negative):  \n\n![image.png](attachment:image.png)\n\nThat is, embeddings for the anchor and the positive image should be closer together than embeddings for the anchor and the negative image.  ","80f86c60":"<img src='https:\/\/mihaelagrigore.info\/wp-content\/uploads\/2021\/05\/1-Regular-covolutional-neural-networks.png'\/>\n\n-- Figure 1: schematic depictiong of a basic convolutional neural network --\n\nFigure 1 is a visualization for the intuition mentioned above. This is a schematic depiction of a simple CNN. As we progress through the network, an individual cell in a network's layer 'sees' (or receives information from) a wider patch of the network's input image. So we assume that deeper layers learn more complex patterns.","8c0dc7d0":"## Tansfer learning\n\nCNN models accuracy tends to increase with network complexity and training dataset size. This is a simplified summary of the general trend. Although in reality things are a bit more complicated (e.g. you can't train a very complex network on a not so large dataset because the model will overfit or the fact that networks beyond a certain depth become problematic), this simplification is a good enough approximation of the general trend. \n\nTaking for granted that more complex is generally better, which means more computing power\/time needed for training, we come to realize we need very powerful machines to train large networks on large datasets (which can reach millions of images). \n\nLuckily, research groups \/ companies make available their already trained image models, so we don't have to spend weeks or months doing this work on our own, on a personal computer.  \n\nThese pre-trained and freely available image models are usually trained on some common datasets, also freely available, which most of these research groups \/ companies tend to use (like the <a href='https:\/\/imagenet.stanford.edu\/'>ImageNet dataset<\/a>, which is used in the <a href='https:\/\/imagenet.stanford.edu\/challenges\/LSVRC\/'>ImageNet Large-Scale Visual Recognition Challenge (ILSVRC))<\/a>.\n\nThese training datasets are often different than the images we want to use these models on. For example, in this notebook I will use a model trained on the ImageNet dataset to perform facial recognition. The ImageNet dataset contains approximately 1.2 million images for 1000 different objects (1000 classes), but none of them are faces. To be able to use this pre-trained model for my task, I will apply <a href='https:\/\/www.tensorflow.org\/tutorials\/images\/transfer_learning'>transfer learning<\/a>. \n\n*Intuition behind Transfer Learning*\n\nTransfer learning consists of freezing the parameters (weights and biases) of a pretrained model and retraining for the current task only the last layer(s). The intuition behind why this approach makes sense is that models trained on images learn features that are useful in any visual classification task, regardless on the particular images on which they were trained. ","8155cb53":"Select the people with at least 5 images. Then select the first five images for each. Move them all to a new location.","8113cc3d":"### The triplet function\n\nSo we're trying to train a neural network, to learn parameters (weights and biases for each layer) so that:\n- when two different images of the same person are presented -> d(output1, output2) is small\n- when two images of different persons are presented -> d(output1, output3) is large\n\nSo our final network will look like the one in Figure 7.  \n\nThe image we need to make a judgement on is called anchor.  \nWe compare it to an image of the person we're trying tp verify it against (positive image).  \nAnd we compare it against a picture of a person who is not the same as in the anchor image (negative image).\n\n![image.png](attachment:image.png)\n-- Figure 7: CNN for facial recognition --","615bc2ea":"Download and uncompress the LFW dataset.","bb6f9314":"Also look at cosine similarity for each pair of embeddings learned by the model. This time we expect the first value to be larger than the second value (now we're talking about similarity; it's not the difference anymore)","92ac541e":"## <center> Facial Recognition with Keras, FaceNet, Inception model and transfer learning<\/center> \n\nFacial recognition is the task of looking at an image showing the face of a person and being able to identify that person from a collectiong of images already present in a database.  \n\n- the system has a database of n persons (images for each of them, most often just 1 per person, if were talking about a database of employee profile pictures)\n- the system receives as input a new, unseen, image of that person (maybe it's them checking in at the security desk every morning)\n- and should be able to recognize if this is any of the n persons already present in the database. \n\nTo build such a system, I will make use of:\n- <a href='https:\/\/en.wikipedia.org\/wiki\/Siamese_neural_network'>Siamese Networks<\/a>\n- the Triplet loss function, described in the <a href='https:\/\/arxiv.org\/abs\/1503.03832'>FaceNet article by Schroff et al 2015<\/a>\n- <a href='https:\/\/www.tensorflow.org\/tutorials\/images\/transfer_learning'>Transfer learning<\/a>, to save training time by making use of pretrained convolutional neural networks.\n- Keras interface for the TensorFlow library and <a href='https:\/\/keras.io\/api\/applications\/'>Keras Applications<\/a> pretrained models. ","e2efa45a":"Let's now split our dataset into train and validation."}}