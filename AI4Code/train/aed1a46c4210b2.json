{"cell_type":{"c75bf0e7":"code","b7c7b17d":"code","c5e62121":"code","8b7b5f13":"code","52129f0e":"code","8b1d5f30":"code","a10a516c":"code","1678f643":"code","7505aef6":"code","81fa1d66":"code","8acb0ee6":"code","0281f366":"code","a6b32c74":"code","59dc7f86":"code","a67a7920":"code","4d782a5f":"code","92d6a070":"code","94af7f8e":"code","2e0e9a85":"code","836ffd92":"code","35649dab":"code","76163271":"code","3b87dab2":"code","104c47fe":"code","6005319a":"code","1d533b85":"code","fc7b1e75":"code","c8d642cc":"code","dac703b6":"code","74461acd":"code","7b161f40":"code","b7865eaa":"code","daa6b429":"code","3a9cfae1":"code","f15d5a9c":"code","2ef48dcc":"code","63c2cfb1":"code","d3a37a55":"code","5e2b2a9a":"code","11424305":"code","e932eebf":"code","850cccb0":"code","c8e68dd0":"code","ed0e389c":"code","beced2e0":"code","ee0606ba":"code","f3da7954":"code","bddc508a":"code","776ef165":"code","bb907418":"code","ad36bf93":"code","b526ed97":"code","8a2906f2":"code","6613b040":"code","28a96d91":"code","bd34ebfa":"code","0a8e4f5a":"code","776e4096":"code","ea243ef0":"code","6953b4db":"code","9b9f0e5c":"code","46d170eb":"code","9458536f":"code","994621e5":"markdown","03f12f63":"markdown","9c837a97":"markdown","46e1e168":"markdown","e5a771f1":"markdown","d6ec8e83":"markdown","7f343949":"markdown","a3cbe7f2":"markdown","684812c6":"markdown","981a55fe":"markdown","d784dc83":"markdown","9247d78a":"markdown","bbddd6e9":"markdown","8cb6d82e":"markdown","cd0e7964":"markdown","bd3d00e6":"markdown","82ccf7c1":"markdown","5ed3cc33":"markdown","91865e11":"markdown","d822c99e":"markdown","21fbe90c":"markdown","6d543c61":"markdown","5706e65b":"markdown","b6e74061":"markdown","3d736e5f":"markdown","178954e6":"markdown","586c38dd":"markdown","4a3c4c18":"markdown","7aa477c2":"markdown","01be6273":"markdown","0be8c969":"markdown","c2e74392":"markdown","419f6384":"markdown","19eef805":"markdown","6629d051":"markdown","cab9d48e":"markdown","ca843225":"markdown","219db88f":"markdown","e81678d5":"markdown","326bcc76":"markdown","e7d17b7b":"markdown","018c682b":"markdown","d65598e6":"markdown","0ed8b893":"markdown","f05c7e47":"markdown","0af3ce3d":"markdown","bb5a00fb":"markdown","fd117682":"markdown","9cb4d105":"markdown","9ca48448":"markdown","b0f7a7f5":"markdown","de02380c":"markdown","ec0d75c1":"markdown","35e99930":"markdown","5eaa2934":"markdown","84c10881":"markdown","25043c18":"markdown","3ab289fc":"markdown","884f30ee":"markdown","6ad55c0b":"markdown","0c07ab9d":"markdown","fa3a4745":"markdown","05dc921f":"markdown","0b37eb82":"markdown","47d6a7df":"markdown","3836a16e":"markdown","0000a01b":"markdown","4dd18bef":"markdown","b8dcb805":"markdown","f9e2568f":"markdown","7ec139b5":"markdown"},"source":{"c75bf0e7":"#Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import svm\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom scipy import stats\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nsns.set()\n","b7c7b17d":"#Reading datasets\ntrain=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n#define train target\ntarget=train['SalePrice']\n\n#Combining train and test dataset for preprocessing\nall_df=pd.concat([train,test], ignore_index=True,sort=False)\n\n#Dropping SalePrice\nall_df.drop('SalePrice',axis=1,inplace=True)","c5e62121":"missings_df={}\nfor key in all_df.columns:\n    if all_df[key].isnull().sum() > 0:\n        missings_df[key]=(all_df[key].isnull().sum()  \/  len(all_df[key]) ) * 100\n\n#Create missing values dataframe\nmissings_df=pd.DataFrame(missings_df,index=['MissingValues']).T.sort_values(by='MissingValues',ascending=False)","8b7b5f13":"#Plotting the percentage of missing values per column\nplt.figure(figsize=(15,7),dpi=100)\nplt.xticks(rotation=90)\nsns.barplot(y=missings_df.MissingValues,x=missings_df.index, orient='v').set_title('The percentage of missing values per column')","52129f0e":"#Sort transposed describe() by count\nall_df.describe(include='object').T.sort_values(by=['count']).head(10)","8b1d5f30":"#Get the number of FirePlaces where Fire Place quality is missing\nall_df[['FireplaceQu','Fireplaces']][all_df.FireplaceQu.isnull()]","a10a516c":"#Countplot the number of fireplaces\nsns.countplot(x=all_df['Fireplaces'])\nplt.ylabel('The number of houses')","1678f643":"#Get Garage car capacity where GarageQual is missing\nall_df[['GarageQual','GarageCars']][all_df.GarageQual.isnull()]","7505aef6":"#Countplot the capacity of garages\nsns.countplot(x=all_df['GarageCars'])\nplt.ylabel('The number of houses')\nplt.xlabel('The car capacity of garage')","81fa1d66":"#Filling missing values for categorical features\nall_df['Alley'].fillna('NA',inplace=True)\nall_df['PoolQC'].fillna('NA',inplace=True)\nall_df['Fence'].fillna('NA',inplace=True)\nall_df['MiscFeature'].fillna('NA',inplace=True)\nall_df['FireplaceQu'].fillna('NA',inplace=True)\nall_df['GarageCond'].fillna('NA',inplace=True)\nall_df['GarageQual'].fillna('NA',inplace=True)\nall_df['GarageFinish'].fillna('NA',inplace=True)\nall_df['GarageType'].fillna('NA',inplace=True)\nall_df['BsmtExposure'].fillna('NA',inplace=True)\nall_df['BsmtFinType2'].fillna('NA',inplace=True)\nall_df['BsmtFinType1'].fillna('NA',inplace=True)\nall_df['BsmtQual'].fillna('NA',inplace=True)\nall_df['BsmtCond'].fillna('NA',inplace=True)","8acb0ee6":"#Number of missing values for MasVnrType + unique categories\nprint(\"Number of missing values : \", all_df['MasVnrType'].isnull().sum())\nprint(\"Categories:\", all_df['MasVnrType'].unique() )","0281f366":"#Fill MasVnrType missing values with 'None'\nall_df['MasVnrType'].fillna('None',inplace=True)","a6b32c74":"#Getting categories for Electrical feature\nprint(\"Number of missing values : \", all_df['Electrical'].isnull().sum())\nprint(\"Categories:\", all_df['Electrical'].unique() )","59dc7f86":"#Fill with top freq\nall_df['Electrical'].fillna('SBrkr',inplace=True)\n ","a67a7920":"#Fill with top freq\nall_df['MSZoning'].fillna('RL',inplace=True)\nall_df['Utilities'].fillna('AllPub',inplace=True)\nall_df['Functional'].fillna('Typ',inplace=True)\nall_df['SaleType'].fillna('WD',inplace=True)\nall_df['Exterior2nd'].fillna('VinylSd',inplace=True)\nall_df['Exterior1st'].fillna('VinylSd',inplace=True)\nall_df['KitchenQual'].fillna('TA',inplace=True)","4d782a5f":"#sorting transposed describe() by count \nall_df.describe().T.sort_values(by='count').head(10)","92d6a070":"#Plot Lotfrontage and LotArea distribution\nfig,ax = plt.subplots(1,2, figsize=(20,5))\nfig.suptitle('LotFrontage And LotArea Distribution',size=15)\nsns.histplot(x=all_df.LotFrontage,kde=True, ax=ax[0])\nsns.histplot(x=all_df.LotArea,kde=True, ax=ax[1])\n","94af7f8e":"#Temp dataframe\nLot_tmp=all_df[['LotFrontage','LotArea']][~all_df.LotFrontage.isnull()]\n#Calculating z-score to remove outliers\nz = np.abs(stats.zscore(Lot_tmp))\n#remove outliers\nLot_tmp_Z=Lot_tmp[(z < 3).all(axis=1)]\n","2e0e9a85":"##Plot Lotfrontage and LotArea distribution without outliers\nfig,ax = plt.subplots(1,2, figsize=(20,5))\nfig.suptitle('LotFrontage And sqrt(LotArea) Distribution Without outliers',size=15)\nsns.histplot(x=Lot_tmp_Z.LotFrontage,kde=True, ax=ax[0])\nsns.histplot(x=Lot_tmp_Z.LotArea.apply(np.sqrt),kde=True, ax=ax[1])","836ffd92":"# Plotting LotFrontage and LotArea\nplt.figure(figsize=(10,5))\nsns.regplot(x=Lot_tmp_Z.LotArea.apply(np.sqrt),y=Lot_tmp_Z.LotFrontage,\n            line_kws={\"color\": \"red\"}).set(title='Relation Between LotFrontage And Sqrt(LotArea)')","35649dab":"#Relation between BldgType and LotFrontage\nfig,ax = plt.subplots(1,2, figsize=(20,6))\nsns.boxplot(x=all_df.BldgType,y=Lot_tmp_Z.LotFrontage, ax=ax[0]).set_title('BldgType (Type of dwelling) and LotFrontage',fontsize = 20)\nsns.boxplot(x=all_df.GarageCars,y=Lot_tmp_Z.LotFrontage, ax=ax[1]).set_title('GarageCars and LotFrontage',fontsize = 20)","76163271":"# Plotting LotFrontage, LotArea and Neighborhood\nfig,ax=plt.subplots(2,1,figsize=(25,15))\nsns.boxplot(x=all_df.Neighborhood,y=Lot_tmp_Z.LotFrontage,ax=ax[0]).set_title('LotFrontage And Neighborhood',fontsize=20)\nsns.boxplot(x=all_df.Neighborhood,y=Lot_tmp_Z.LotArea.apply(np.sqrt),ax=ax[1]).set_title('LotArea And Neighborhood',fontsize=20)","3b87dab2":"fig,ax=plt.subplots(1,1,figsize=(25,8))\nsns.boxplot(x=all_df[:1460].Neighborhood , y=target).set_title('Neighborhood And SalePrice',fontsize=20)","104c47fe":"#Creating temp dataframe and converting objects to dummies\nLotFrontage_df=pd.get_dummies(all_df[['LandSlope','BldgType','Alley','LotConfig','MSZoning','Neighborhood','LotArea','LotFrontage']].copy())\n#Replace Lotarea with it's Sqrt\nLotFrontage_df['LotArea']=LotFrontage_df['LotArea'].apply(np.sqrt)\n\n#Mask to access part of the dataframe with available LotFrontage\nmask=~LotFrontage_df.LotFrontage.isnull()\n#train dataset (including LotFrontage)\nLotFrontage_train=LotFrontage_df.loc[mask]\n#test dataset (Missing LotFrontage)\nLotFrontage_Test=LotFrontage_df.loc[~mask].drop('LotFrontage',axis=1)\n#Removing outliers from Train Dataframe \nz = np.abs(stats.zscore(LotFrontage_train[['LotFrontage','LotArea']]))\nLotFrontage_train=LotFrontage_train[(z < 3).all(axis=1)]\n\n#Define scaler\nScaler_L=StandardScaler()\n#standardizing features\nX=Scaler_L.fit_transform(LotFrontage_train.drop('LotFrontage',axis=1))\n#Define target\ny=LotFrontage_train['LotFrontage']\n#Splitting Train dataframe\nx_train,x_valid,y_train,y_valid=train_test_split(X, y ,test_size=0.3 , random_state=42)\n#Defining model\nmodel=GradientBoostingRegressor(n_estimators= 100, min_samples_split= 26, min_samples_leaf= 17, max_features='auto', max_depth= None)\n#Fitting model on train data\nmodel.fit(x_train,y_train)\n#predicting validation target\ny_pred=model.predict(x_valid)\n#Storing mean absolute error for single mean, median and Gradien Boosting Regressor\nGBoost_results= mean_absolute_error(y_valid,y_pred)\nMean_result= mean_absolute_error(y_valid,[y_valid.mean()]*len(y_valid))\nMedian_result= mean_absolute_error(y_valid,[y_valid.median()]*len(y_valid))\nprint(\"MEAN ABSOLUTE ERROR : \", GBoost_results)\n","6005319a":"#Plotting train\/validation results\nfig, ax= plt.subplots(1,2,figsize=(20,6))\n#Show results\nsns.regplot(x=y_pred,y=y_valid, line_kws={'color':'red'} , ax=ax[0]).set(title='G-BOOOSTING Regressor Train and Validation Results',xlabel='LotFrontage (Predictions)',ylabel='LotFrontage (True)')\nsns.barplot(x=[\"Mean\",\"Median\",\"GradientBoostingRegressor\"],y=[Mean_result,Median_result,GBoost_results], ax=ax[1]).set(title='LotFrontage Imputation - Mean\/Median Vs. GradientBoostingRegressor',ylabel='Mean absolute error')","1d533b85":"#Predicting Missing values for Lotfrontage\nLotFrontage_Test=Scaler_L.transform(LotFrontage_Test)\nLotFrontage_missings=model.predict(LotFrontage_Test)\n\n#Fill Missing values\nall_df.loc[~mask,'LotFrontage']=LotFrontage_missings","fc7b1e75":"#Exploring dataset to discover missing values\nall_df.describe().T.sort_values(by='count').head(10)","c8d642cc":"# Cheking GarageType where GarageYrBlt is missing \nall_df[['GarageYrBlt','GarageType']].loc[all_df.GarageYrBlt.isnull()]","dac703b6":"# fill GarageYrBlt missings with the mean\nall_df['GarageYrBlt'].fillna(0, inplace=True)","74461acd":"#fill with median or 0\nall_df['BsmtHalfBath'].fillna(all_df.BsmtHalfBath.median(), inplace=True)\nall_df['BsmtFullBath'].fillna(all_df.BsmtFullBath.median(), inplace=True)\nall_df['GarageCars'].fillna(0, inplace=True)\nall_df['TotalBsmtSF'].fillna(all_df.TotalBsmtSF.median(), inplace=True)\nall_df['BsmtUnfSF'].fillna(0 , inplace=True)\nall_df['BsmtFinSF2'].fillna(all_df.BsmtFinSF2.median(), inplace=True)\nall_df['GarageArea'].fillna(0, inplace=True)\nall_df['BsmtFinSF1'].fillna(all_df.BsmtFinSF1.median(), inplace=True)\nall_df['MasVnrArea'].fillna(all_df['MasVnrArea'].median(), inplace=True)\n","7b161f40":"#Saving dataframe\nall_df.to_csv('Imputed_data_all.csv')","b7865eaa":"#Get the correlation matrix\ncorr=train.corr()\n\n# Getting the Upper Triangle of the co-relation matrix\nUpperT = np.triu(corr)\nplt.figure(figsize=(20,15))\nplt.title('Correlation Matrix')\nsns.heatmap(corr, mask=UpperT)","daa6b429":"#Getting sorted correlation between features and SalePrice \ntrain.corrwith(target).sort_values(ascending=False).head(50)","3a9cfae1":"def highly_correlated(df, features, threshold=0.5):\n    corr_df = df[features].corr() # get correlations\n    correlated_features = np.where(np.abs(corr_df) > threshold) # select ones above the abs threshold\n    correlated_features = [(corr_df.iloc[x,y], x, y) for x, y in zip(*correlated_features) if x != y and x < y] # avoid duplication\n    s_corr_list = sorted(correlated_features, key=lambda x: -abs(x[0])) # sort by correlation value\n    correlation_df={}\n    if s_corr_list == []:\n        print(\"There are no highly correlated features with correlation above\", threshold)\n    else:\n        for v, i, j in s_corr_list:\n            correlation_df[corr_df.index[i] +\" and \"+ corr_df.columns[j]]= v\n        correlation_df=pd.DataFrame(correlation_df,index=['Correlation'])\n    return  correlation_df.T.sort_values(by='Correlation',ascending=False)\n","f15d5a9c":"#print correlation between features\nhighly_correlated(all_df,all_df.columns).style.set_properties(**{'background-color': 'black','color': 'white'})","2ef48dcc":"#plotting histogram chart for highly correlated features \nfig, ax = plt.subplots(5,2,figsize=(20,22),dpi=100)\nfig.suptitle('Distribution of Highly Correlated Features',size=20)\nsns.histplot(all_df['GarageCars'],ax=ax[0,0])\nsns.histplot(all_df.GarageArea,ax=ax[0,1])\nsns.histplot(all_df.YearBuilt,ax=ax[1,0])\nsns.histplot(all_df.GarageYrBlt ,ax=ax[1,1])\nsns.histplot(all_df.GrLivArea,ax=ax[2,0])\nsns.histplot(all_df['TotRmsAbvGrd'],ax=ax[2,1])\nsns.histplot(all_df['TotalBsmtSF'],ax=ax[3,0])\nsns.histplot(all_df['1stFlrSF'],ax=ax[3,1])\nsns.histplot(all_df['BedroomAbvGr'],ax=ax[4,0])\nsns.histplot(all_df['2ndFlrSF'],ax=ax[4,1])","63c2cfb1":"#Creating a copy of data\nall_df_copy=all_df.copy()\n#Drop GarageYrBlt\nall_df_copy.drop('GarageYrBlt', axis=1,inplace=True)","d3a37a55":"all_df_copy['Log_GrLivArea'] = all_df_copy['GrLivArea'].apply(np.log)\nall_df_copy['Log_1stFlrSF'] = all_df_copy['1stFlrSF'].apply(np.log) \nall_df_copy['Log_LotFrontage']= all_df_copy['LotFrontage'].apply(np.log) \nall_df_copy['Sqrt_LotArea']=all_df_copy['LotArea'].apply(np.log)   \nall_df_copy['HouseAge']=all_df_copy['YrSold']-all_df_copy['YearBuilt']\nall_df_copy['Total_Rooms']= all_df_copy['BedroomAbvGr'] + all_df_copy['TotRmsAbvGrd']  \nall_df_copy['GrLivArea_Score']= np.sqrt(all_df_copy['OverallQual']) * all_df_copy['GrLivArea']\nall_df_copy['Total_Square_Feet'] = all_df_copy.TotalBsmtSF + all_df_copy['1stFlrSF'] + all_df_copy['2ndFlrSF']\nall_df_copy['Total_Porch'] = all_df_copy.ScreenPorch + all_df_copy.EnclosedPorch + all_df_copy.OpenPorchSF + all_df_copy.WoodDeckSF + all_df_copy['3SsnPorch']\nall_df_copy['OverallQualCond'] = (all_df_copy['OverallCond'] * all_df_copy.OverallQual)\nall_df_copy['BsmtFinSF']=all_df_copy['BsmtFinSF1'] + all_df_copy['BsmtFinSF2'] \nall_df_copy['TotalBath_Abv']=2*all_df_copy['FullBath']+all_df_copy['HalfBath']*0.5\nall_df_copy['TotalBath_Bsmt']=2*all_df_copy['BsmtFullBath']+all_df_copy['BsmtHalfBath']*0.5","5e2b2a9a":"tmp_cols=all_df_copy.columns[all_df_copy.isin(['Gd']).any()]\ntmp_cols","11424305":"#Make a dict\nmarks = {'No':0,'None':0,'NA':0,'Mn':1,'Av':3, \"Po\": 1, 'Fa': 2, \"TA\": 3, 'Gd': 4, 'Ex': 5}\n#Apply map for each column\nfor column in tmp_cols:\n    all_df_copy[column]=all_df_copy[column].map(marks)","e932eebf":"all_df_copy['Exter_Overall'] = all_df_copy['ExterCond'] * all_df_copy['ExterQual']\nall_df_copy['Garage_Overall'] = all_df_copy['GarageQual'] * all_df_copy['GarageCond']","850cccb0":"#plotting most correlated features based on SalePrice\nfig, ax = plt.subplots(3,2,figsize=(20,18))\nfig.suptitle('Important features and Sale Price',size=20)\nsns.regplot(x=target,y=all_df_copy[:1460].OverallQual,ax=ax[0,0])\nsns.regplot(x=target,y=all_df_copy[:1460].GrLivArea,ax=ax[0,1])\nsns.regplot(x=target,y=all_df_copy[:1460].GarageCars,ax=ax[1,0])\nsns.regplot(x=target,y=all_df_copy[:1460]['1stFlrSF'],ax=ax[1,1])\nsns.regplot(x=target,y=all_df_copy[:1460].FullBath,ax=ax[2,0])\nsns.regplot(x=target,y=all_df_copy[:1460]['YearBuilt'],ax=ax[2,1])","c8e68dd0":"#Plotting Sale Price distribiution\nfig,ax=plt.subplots(1,2,figsize=(20,5),dpi=100)\nfig.suptitle('Distribution of SalePrice',size=15)\nsns.histplot(target, ax=ax[0],kde=True).set_title('SalePrice')\nsns.histplot(target.apply(np.log),kde=True, ax=ax[1]).set_title('SalePrice - Log-Scale')","ed0e389c":"#taking SalePrice into logarithmic scale\ntarget=target.apply(np.log)","beced2e0":"#Create copy\nall_df_copy_en=all_df_copy.copy()\n\n#Converting objects data type to category data type\nfor key,value in all_df_copy_en.items():\n    if pd.api.types.is_string_dtype(value):\n        all_df_copy_en[key] = value.astype(\"category\").cat.as_ordered()","ee0606ba":"#Replace categories with their code\nfor key,value in all_df_copy_en.items():\n     if not pd.api.types.is_numeric_dtype(value):\n        all_df_copy_en[key] = pd.Categorical(value).codes+1\n        \n#Get Dummy variables\n#all_df_copy_D=pd.get_dummies(all_df_copy).drop('Id',axis=1) # not used","f3da7954":"#Drop id column\nall_df_copy_en.drop('Id',axis=1,inplace=True)\n#Split data after encoding\ntrain_df=all_df_copy_en[:1460].copy()\ntrain_df['SalePrice']=target\ntest_df=all_df_copy_en[1460:].copy()","bddc508a":"#columns for z-score calculation\nnumeric_columns=train_df.columns \n#Calculating z-score using stats library\nz = np.abs(stats.zscore( train_df[numeric_columns]))\n\n#train_df.select_dtypes(include=[np.number]).columns.values  \/\/ Returns only numeric columns","776ef165":"#Removing outliers on temporary dataframe\ntrain_df_z=train_df.copy()[(z < 10).all(axis=1)]\n\nprint('Rows,columns Before removing outliers : ', train_df.shape )\nprint('Rows,columns After removing outliers : ', train_df_z.shape )","bb907418":"#plotting important features\nfig, ax = plt.subplots(4,2,figsize=(18,18))\nfig.suptitle('Log-SalePrice and Important Features Without Outliers', size=20)\nsns.regplot(x=train_df_z['SalePrice'],y=train_df_z.OverallQual,ax=ax[0,0], line_kws={'color':'red'})\nsns.regplot(x=train_df_z['SalePrice'],y=train_df_z.Log_GrLivArea,ax=ax[0,1], line_kws={'color':'red'})\nsns.regplot(x=train_df_z['SalePrice'],y=train_df_z.GarageCars,ax=ax[1,0], line_kws={'color':'red'})\nsns.regplot(x=train_df_z['SalePrice'],y=train_df_z['Log_1stFlrSF'],ax=ax[1,1], line_kws={'color':'red'})\nsns.regplot(x=train_df_z['SalePrice'],y=train_df_z.TotalBsmtSF ,ax=ax[2,0], line_kws={'color':'red'})\nsns.regplot(x=train_df_z['SalePrice'],y=train_df_z['YearBuilt'],ax=ax[2,1], line_kws={'color':'red'})\nsns.regplot(x=train_df_z['SalePrice'],y=train_df_z.FullBath ,ax=ax[3,0], line_kws={'color':'red'})\nsns.regplot(x=train_df_z['SalePrice'],y=train_df_z['TotRmsAbvGrd'],ax=ax[3,1], line_kws={'color':'red'})","ad36bf93":"#Add features to dropFullBath\nto_drop=['SalePrice','Fence',\n                    'HalfBath',\n                    'FullBath',\n                    'BsmtFullBath',\n                     'BsmtHalfBath',\n                     'LotConfig',\n                     'Foundation',\n                     'BsmtFinSF2',\n                     'LotShape',\n                     'MSSubClass',\n                     'PoolArea',\n                     'LandContour',\n                     'LandSlope',\n                     'BsmtHalfBath',\n                     'MasVnrArea',\n                     'PavedDrive',\n                     'GarageFinish',\n                     'MasVnrType',\n                     'HeatingQC',\n                     'SaleType',\n                     '3SsnPorch',\n                     'PoolQC',\n                     'ScreenPorch',\n                     'MiscFeature',\n                     'Street',\n                     'MiscVal',\n                     'GarageArea',\n                     'YearRemodAdd',\n                     'YearBuilt',\n                     'LotArea',\n                     'GrLivArea',\n                     'LotFrontage',\n                     'RoofMatl',\n                     'Alley',\n                     'RoofStyle',\n                     'Heating',\n                     'BsmtFinType2',\n                     'BsmtFinType1',\n                     'LowQualFinSF',\n                     'MoSold', \n                     '1stFlrSF',\n                     '2ndFlrSF',\n                     'OverallQual',\n                     'OverallCond',\n                     'TotalBsmtSF',\n                     'YrSold',\n                     'BsmtFinSF1',\n                     'ExterCond',\n                     'ExterQual',\n                     'GarageQual',\n                     'GarageCond',\n                     'BsmtFinSF2']\n#will be dropped from test data\nto_drop_t=to_drop[1:]","b526ed97":"#Independent variables \nX=train_df_z.drop(to_drop,axis=1)\n#Dependent variable\ny=train_df_z['SalePrice']\n\n#scaling features\nscaler=StandardScaler()\nX_scaled=scaler.fit_transform(X)\n\n#spliting train dataset to train and validation\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X_scaled, y, test_size=0.25, random_state=42)","8a2906f2":"# evaluate a given model by making predictions on X_valid\ndef get_v_score(model):\n    valid_predictions=model.predict(X_valid)\n    score=np.sqrt(mean_squared_error(y_valid, valid_predictions))\n    return score\n\n# evaluate a given model using cross-validation\ndef get_cv_score(model, X, y):\n    #cv = KFold(n_splits=5, shuffle=True, random_state=1)\n    scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=5, n_jobs=-1))\n    return np.mean(scores)","6613b040":"# Models name\nmodels_name=['RandomForestRegressor',\n             'GradientBoostingRegressor',\n             'XGBRegressor',\n             'svm.SVR',\n             'ExtraTreesRegressor',\n             'Lasso']\n# Models\nmodels=[RandomForestRegressor(),\n        GradientBoostingRegressor(),\n        XGBRegressor(),\n        svm.SVR(),\n        ExtraTreesRegressor(),\n        Lasso(alpha=0.0005,tol=0.001)]","28a96d91":"#Fit and get scores for each model\nscores_list=[]\nfor model in models:\n    model.fit(X_train,y_train)\n    scores_list.append(get_v_score(model))","bd34ebfa":"#Convert list to dataframe\ndata={'Model':models_name,'RMSE': scores_list}\nscores_df=pd.DataFrame(data)\n#Sort by valid RMSLE\nscores_df.sort_values(by='RMSE').round(5).style.set_properties(**{'background-color': 'black','color': 'white'})","0a8e4f5a":"#Hyperparameter tuning with RandomizedSearchC \n#GBOOST_grid = {\"max_depth\" :  np.arange(2, 10, 1),\n               #\"splitter\" : ['best','random'],\n#               \"min_samples_split\": np.arange(2, 100, 1),\n#               \"min_samples_leaf\" : np.arange(2, 100, 1),}\n#GBOOST_model = RandomizedSearchCV(GradientBoostingRegressor(), param_distributions=GBOOST_grid, n_iter=100, cv=5, verbose=True)\n#GBOOST_model.fit(X_train, y_train)\n#Show best parameters\n#print(\"Best Params : \", GBOOST_model.best_params_)","776e4096":"# get a stacking ensemble of models\ndef get_stacking():\n   \n    # define the base models with tuned parameters\n    level0 = list()\n    level0.append(('GBOOST', GradientBoostingRegressor(min_samples_split=90, min_samples_leaf=26)))\n    level0.append(('RandomForest', RandomForestRegressor(min_samples_split=14, min_samples_leaf=8)))\n    level0.append(('XGB', XGBRegressor(colsample_bytree=0.6, gamma=0.4 )))\n    level0.append(('ExtraTrees', ExtraTreesRegressor(min_samples_split=14, min_samples_leaf=2)))\n    level0.append(('svm.SVR', svm.SVR(kernel='poly',gamma='scale',degree=1,coef0=0.30000000000000004)))\n    level0.append(('ElasticNet', ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=0,tol=0.001)))\n    level0.append(('Lasso', Lasso(alpha=0.0005, tol=0.001)))\n\n    # define meta learner model\n    level1 = LinearRegression()\n    # define the stacking ensemble\n    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n    return model\n\n#Add to models list\nmodels_name.append('StackingRegressor')\nmodels.append(get_stacking())","ea243ef0":"#Fit stacked model and get validation score\nStackedmodel=get_stacking()\nStackedmodel.fit(X_train,y_train)\nprint('RMSE : ', get_v_score(Stackedmodel))","6953b4db":"y_pred=Stackedmodel.predict(X_valid)\nsns.regplot(x=y_pred, y=y_valid).set_title(\"Stackedmodel Results\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True values\")","9b9f0e5c":"Gboost=GradientBoostingRegressor(min_samples_split=26, min_samples_leaf=29)\nGboost.fit(X_train,y_train)\n\n#create feature importance dataframe\nFI_df=pd.DataFrame({'Features': X.columns, 'Importance': Gboost.feature_importances_})\nFI_df=FI_df.sort_values(by='Importance', ascending=False).head(100)\n\n#plot feature importance\nplt.figure(figsize=(15,10),dpi=100)\nsns.barplot(x=FI_df[:30].Importance,y=FI_df[:30].Features,orient = 'h').set_title('Feature Importance')","46d170eb":"#Scaling Test data\ntest_df_Scaled=scaler.transform(test_df.drop(to_drop_t,axis=1))\n#Getting predictions\ntest_df_preds=Stackedmodel.predict(test_df_Scaled)\n\n#Adding Id column\ntest_preds=pd.DataFrame({'Id':test['Id'],'SalePrice':(np.exp(test_df_preds))})\ntest_preds","9458536f":"#Save predictions\ntest_preds.to_csv('submission.csv', index=False)","994621e5":"#### Plot features after removing outliers","03f12f63":"## Define models ","9c837a97":"## Training a model to predict missing values for LotFrontage\n#### - I'll take a few columns as predictors \n#### - Then I'll compare the Mean absolute error for imputing with simple Median\/Mean And regression Model","46e1e168":"### Making a to_drop list","e5a771f1":"## Hyperparameter tuning","d6ec8e83":"### Fill the rest of few missing values with median or 0","7f343949":"### Adding new predictors","a3cbe7f2":"## Correlation between features\n#### following function calculates correlation in pairs and returns a dataframe","684812c6":"### There are 1420 houses without fireplace","981a55fe":"<h1 id=\"1. Importing libraries and reading datasets\" style=\"color:white;background:#0087B6;padding:8px;border-radius:8px\">1. Importing libraries and reading datasets<\/h1>","d784dc83":" Correlation >  0.60 :\n\n- OverallQual : (Rates the overall material and finish of the house - 1 to 9 )\n- GrLivArea   : (Above grade (ground) living area square feet)\n- GarageCars  : (Size of garage in car capacity)\n- GarageArea  : (Size of garage in square feet)\n- TotalBsmtSF : (Total square feet of basement area)\n- 1stFlrSF    : (First Floor square feet)","9247d78a":"<h1 id=\"Checking Correlations\" style=\"color:white;background:#0087B6;padding:8px;border-radius:8px\">  3.  Checking Correlations <\/h1>","bbddd6e9":"<h1 id=\" 5. Feature engineering\" style=\"color:white;background:#0087B6;padding:8px;border-radius:8px\">  5. Feature engineering and log transformation <\/h1>","8cb6d82e":"### Importing libraries","cd0e7964":"## Get score for each model ","bd3d00e6":"#### Credits\n* [Stacking Ensemble Machine Learning With Python by Jason Brownlee](https:\/\/machinelearningmastery.com\/stacking-ensemble-machine-learning-with-python\/)\n* https:\/\/www.kaggle.com\/niekvanderzwaag\/housing-price-prediction-regression\n* https:\/\/www.kaggle.com\/lovroselic\/houseprices-ls\n* https:\/\/towardsdatascience.com","82ccf7c1":"#### I'll define a Temporary dataframe and keep the previous one safe\n\n#### With the knowledge we have gained from the data so far, By changing Z value for several times and observing results we can finally decide what should be the right value for Z ","5ed3cc33":"#### save dataframe without missing values","91865e11":"#### Mean absolute error is twice lower for Gradient Boosting Regressor compared to the simple mean or median.","d822c99e":"### Plot without outliers","21fbe90c":"## Fit stacked model and get validation score ","6d543c61":"### Splitting data to train\/test","5706e65b":"## Define score functions","b6e74061":"### Relation between LotFrontage and LotArea","3d736e5f":"<h1 id=\"8. Time to modeling\" style=\"color:white;background:#0087B6;padding:8px;border-radius:8px;text-align:left\"> 8. Time to modeling <\/h1>","178954e6":"<h1 id=\"4. Distribution of Important features\" style=\"color:white;background:#0087B6;padding:8px;border-radius:8px\">  4. Distribution of Important features<\/h1> ","586c38dd":"<h3> Replace categories with their codes  <\/h3>","4a3c4c18":"#### Calculating z-score : ","7aa477c2":" ## Missing values - numeric features","01be6273":"<h1 id=\"11. Making prediction on test data \" style=\"color:white;background:#0087B6;padding:8px;border-radius:8px;text-align:left\"> 11. Making prediction on test data <\/h1>","0be8c969":"#### According to data dictionary there are features that do not contain the 'NA' category : ","c2e74392":"### Reading datasets\n#### After reading train and test dataset, I'll combine them to for preprocessing","419f6384":"#### Since there is no garage for these houses, it's not a good idea to fill these NaNs with the mean or median, it makes sense to fill it with 0","19eef805":"## Correlation Matrix","6629d051":"<h3>  Change the type of strings into category data type <\/h3>\n","cab9d48e":"### There are 159 houses without garage","ca843225":"<h2> <a style=\"color:black\" href=\"https:\/\/www.kaggle.com\/mehrdadsadeghi\/house-price-advanced-regression-techniques\">Sort highly correlated features <\/a> with SalePrice <\/h2>","219db88f":"#### Before dealing with outliers I have to split the combined dataframes to train and test, because we want to remove outliers only from train dataset","e81678d5":"## Visualizing missing data","326bcc76":"### GarageYrBlt","e7d17b7b":"### Relation between neighborhood, LotFrontage, LotArea","018c682b":"### Get columns containing  Na, Fa, Ta, Gd, Ex,..","d65598e6":"### So let's fill categorical variables with 'Na'","0ed8b893":"## Define dependent and independent variables","f05c7e47":"#### By looking at the data description I realized that the most missing values in the categorical features were supposed to be labeled as 'NA' (Not Available) . \n\n#### There are numeric variables related to the categorical ones which can assure us to fill them with 'NA'","0af3ce3d":" ## Missing values - Categorical features","bb5a00fb":"###  LotFrontage : \n#### There are other features related to the LotFrontage such as LotArea, Neighborhood,...\n\n**Let's see how they related**","fd117682":"### What is Z-score ? \n#### Also called standard score. This score helps to understand if a data value is greater or smaller than mean and how far away it is from the mean. More specifically, Z score tells how many standard deviations away a data point is from the mean.\n\n#### Z score = (x -mean) \/ std","9cb4d105":"<b> Since there is high correlation between Yearbuilt and GarageYrBlt  I'll drop the GarageYrBlt. <\/b>\n","9ca48448":"#### so there is a rise for LotFrontage as the sqrt(LotArea) increases.","b0f7a7f5":"<h1 id=\"Predicting house prices\" style=\"color:white;background:#0087B6;padding:8px;border-radius:8px\">  Predicting house prices using advanced regression techniques <\/h1>\n\n## Problem Definition\nHow well can we predict house prices for samples in test.csv file using advanced regression techniques\n\n## Data\n- train.csv Contains House sales samples (1460 sample) with 79 variables including SalePrice which is the dependent variable\n- test.csv  Contains House sales samples (1460 sample) without SalePrice which is the variable we should predict\n- data_description.txt full description of each variable\n\n## Evaluation\n- Goal\n\nPredicting sale price for each sample (house) in test data. For each Id in the test set, we must predict the value of the SalePrice variable. \n\n- Metric\n\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n\n\n<h2 style=\"color:white;background:#0087B6;padding:8px;border-radius:8px\">  What does this notebook contain? <\/h2>\n\n\n*  Importing libraries and reading datasets\n*  Dealing with missing values\n    * Visualizing missing values percentage per column\n    * Categorical features\n    * Numeric features\n        * Predicting LotFrontage missing values\n        * Imputing other numeric features\n*  Checking correlations\n    * Correlation Matrix\n    * Sorting correlations between features and SalePrice\n    * Sorting Correlations between features\n*   Important features Distribution\n*   Feature engineering and Log Transformation\n*   Encoding Categorical Features just using pandas\n*   Discovering Outliers using Z-score\n*   Time to modeling\n    * Define variables and models\n    * Train and validation report\n    * Hyperparameter tuning with RandomizedSearchCV\n*   Stacking ensemble of models\n*   Feature importance\n*   Predicting SalePrices for test data","de02380c":"#### I dont want to drop other features for now, instead of that I'll use them to create new features like Log-scaled GrLivArea, Total_Square_Feet , ...","ec0d75c1":"### Replace marks with numbers","35e99930":"### Fill Other missing values","5eaa2934":"### Combine ExterCond,ExterQual and GarageQual,GarageCond","84c10881":"#### I've used RandomizedSearchCv to get the best parameters for each model and use them in the next section ","25043c18":"#### Feature importance by gradient boosting regressor","3ab289fc":"### Stacking Models","884f30ee":"<h1 id=\"6. Encoding Categorical Features\" style=\"color:white;background:#0087B6;padding:8px;border-radius:8px;text-align:left\"> 6. Encoding Categorical Features Using Pandas <\/h1>","6ad55c0b":"![image.png](attachment:627354eb-9f6f-4b53-af92-2d93e3bc6a14.png) ","0c07ab9d":"#### As a result of Non-normal distribution of X-axis (SalePrice) samples are concentrated on the left","fa3a4745":"<h1 id=\"10. Feature importance\" style=\"color:white;background:#0087B6;padding:8px;border-radius:8px;text-align:left\"> 10. Feature importance <\/h1>","05dc921f":"#### Overally, Houses in MeadowV, NPkVill, Blmngtn and Blueste neighborhoods have lower prices, lower LotArea and lower LotFrontage","0b37eb82":"<h1 id=\"7. Discovering Outliers\" style=\"color:white;background:#0087B6;padding:8px;border-radius:8px;text-align:left\"> 7. Discovering Outliers using Z-score <\/h1>","47d6a7df":"<h1 id=\"9. Stacking models with best parameters\" style=\"color:white;background:#0087B6;padding:8px;border-radius:8px;text-align:left\"> 9. Stacking models with best parameters <\/h1>","3836a16e":"<h1 id=\"2. Dealing with missing values\" style=\"color:white;background:#0087B6;padding:8px;border-radius:8px\">  2. Dealing with missing values <\/h1>","0000a01b":"#### since they have diffrent range of numbers I'll take the sqrt of lotArea \n","4dd18bef":"### Predict missing values for LotFrontage using trained model","b8dcb805":"## Sale Price Log Transformation","f9e2568f":"### How about SalePrice and Neighborhood","7ec139b5":"### Important features price chart"}}