{"cell_type":{"7d93827a":"code","832a155f":"code","fad60b95":"code","bd1e81a8":"markdown","e2aca551":"markdown","12f852fd":"markdown","787283ab":"markdown"},"source":{"7d93827a":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","832a155f":"# Data preparation\n\nadmissions = pd.read_csv('..\/input\/inputdatacsv\/inputdata.csv')\n\n# Make dummy variables for rank\ndata = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\ndata = data.drop('rank', axis=1)\n\n# Standarize features\nfor field in ['gre', 'gpa']:\n    mean, std = data[field].mean(), data[field].std()\n    data.loc[:,field] = (data[field]-mean)\/std\n    \n# Split off random 10% of the data for testing\nnp.random.seed(21)\nsample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\ndata, test_data = data.iloc[sample], data.drop(sample)\n\n# Split into features and targets\nfeatures, targets = data.drop('admit', axis=1), data['admit']\nfeatures_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']","fad60b95":"\n\nnp.random.seed(21)\n\ndef sigmoid(x):\n    \"\"\"\n    Calculate sigmoid\n    \"\"\"\n    return 1 \/ (1 + np.exp(-x))\n\n\n# Hyperparameters\nn_hidden = 2  # number of hidden units\nepochs = 900\nlearnrate = 0.005\n\nn_records, n_features = features.shape\nlast_loss = None\n# Initialize weights\nweights_input_hidden = np.random.normal(scale=1 \/ n_features ** .5,\n                                        size=(n_features, n_hidden))\nweights_hidden_output = np.random.normal(scale=1 \/ n_features ** .5,\n                                         size=n_hidden)\n\nfor e in range(epochs):\n    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n    for x, y in zip(features.values, targets):\n        ## Forward pass ##\n        # TODO: Calculate the output\n        hidden_input = np.dot(x, weights_input_hidden)\n        hidden_output = sigmoid(hidden_input)\n\n        output = sigmoid(np.dot(hidden_output,\n                                weights_hidden_output))\n\n        ## Backward pass ##\n        #  Calculate the network's prediction error\n        error = y - output\n\n        #  Calculate error term for the output unit\n        output_error_term = error * output * (1 - output)\n\n        ## propagate errors to hidden layer\n\n        #  Calculate the hidden layer's contribution to the error\n        hidden_error = np.dot(output_error_term, weights_hidden_output)\n\n        #  Calculate the error term for the hidden layer\n        hidden_error_term = hidden_error * hidden_output * (1 - hidden_output)\n\n        #  Update the change in weights\n        del_w_hidden_output += output_error_term * hidden_output\n        del_w_input_hidden += hidden_error_term * x[:, None]\n\n    #  Update weights\n    weights_input_hidden += learnrate * del_w_input_hidden \/ n_records\n    weights_hidden_output += learnrate * del_w_hidden_output \/ n_records\n\n    # Printing out the mean square error on the training set\n    if e % (epochs \/ 10) == 0:\n        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n        out = sigmoid(np.dot(hidden_output,\n                             weights_hidden_output))\n        loss = np.mean((out - targets) ** 2)\n\n        if last_loss and last_loss < loss:\n            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n        else:\n            print(\"Train loss: \", loss)\n        last_loss = loss\n\n# Calculate accuracy on test data\nhidden = sigmoid(np.dot(features_test, weights_input_hidden))\nout = sigmoid(np.dot(hidden, weights_hidden_output))\npredictions = out > 0.5\naccuracy = np.mean(predictions == targets_test)\nprint(\"Prediction accuracy: {:.3f}\".format(accuracy))\n","bd1e81a8":"## Let's Implement gradient descent along with back propogation\n\n### Input : datapreparation.csv \n[Student graduate school admissions data](https:\/\/stats.idre.ucla.edu\/stat\/data\/binary.csv)\n\n' I m going to have you use gradient descent to train a network on graduate school admissions data (found at http:\/\/www.ats.ucla.edu\/stat\/data\/binary.csv). This dataset has three input features: GRE score, GPA, and the rank of the undergraduate school","e2aca551":"# GRADIENT DESCENT IMPLEMENTATION","12f852fd":"### lets construct our algorithm","787283ab":"## Instructions\n### For implementing gradient descent, we need to build main four functions:\n\nThe sigmoid activation function: sigmoid;\nThe formula for the prediction: output_formula;\nThe formula for the error at a point: error_formula;\nThe function that updates the parameters with one gradient descent step: update_weights."}}