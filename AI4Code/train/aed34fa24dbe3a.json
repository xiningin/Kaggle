{"cell_type":{"e7398a23":"code","2e6ea54c":"code","c39116a0":"code","2e327c24":"code","693495c8":"code","4a3adf98":"code","cdb10204":"code","2ed4dfbf":"code","f51aa423":"code","10e025aa":"code","494c502c":"code","ea859f40":"code","3915d3de":"code","73cbeb5d":"code","2b229085":"code","fd26e348":"code","6592834c":"code","5251a147":"code","d43d22f0":"code","a906047b":"code","6c8f00ef":"code","57c61b14":"code","db400cf5":"markdown","a9f234e3":"markdown","8606d066":"markdown","dd3c6bb2":"markdown","e8bfabfa":"markdown","dd4a2b3e":"markdown"},"source":{"e7398a23":"import random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport sklearn.metrics\nfrom sklearn.metrics import confusion_matrix as cm, classification_report as cr\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nrandom.seed(0)\n%matplotlib inline","2e6ea54c":"df = pd.read_csv('..\/input\/company-bankruptcy-prediction\/data.csv')\ndf","c39116a0":"df.describe()","2e327c24":"counts = df['Bankrupt?'].value_counts()\nn_neg = counts[counts.index == 0].values[0]\nn_pos = counts[counts.index == 1].values[0]\nprint(n_neg, n_pos)","693495c8":"# ' Net Income Flag'\ndf = df.drop(columns=df.std()[(df.std() == 0)].index)","4a3adf98":"train_df, test_df = train_test_split(df, test_size=0.2)\ntrain_df, val_df = train_test_split(df, test_size=0.2)\n\n# Form np arrays of labels and features.\ny_train = np.array(train_df.pop('Bankrupt?'))\ny_val = np.array(val_df.pop('Bankrupt?'))\ny_test = np.array(test_df.pop('Bankrupt?'))\n\nx_train = np.array(train_df)\nx_val = np.array(val_df)\nx_test = np.array(test_df)","cdb10204":"scaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\n\nx_val = scaler.transform(x_val)\nx_test = scaler.transform(x_test)","2ed4dfbf":"columns = train_df.columns  # [kbest.get_support()]\ntemp_df = pd.DataFrame(x_train, columns=columns)\ntemp_df['y'] = y_train\ncorr = temp_df.corr()\nplt.figure(figsize=(15, 15))\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)\nplt.show()\nplt.close()\n\ncorr = corr['y'].drop('y').abs().sort_values(ascending=False)\nprint(corr)\nprint(len(corr[corr > 0.05]))","f51aa423":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8), sharey=True, sharex=True)\nbins = ax1.hist(temp_df[temp_df['y'] == 0][' Net Income to Total Assets'], bins='auto', density=True)\nax2.hist(temp_df[temp_df['y'] == 1][' Net Income to Total Assets'], bins=bins[1], density=True)\nplt.show()\nplt.close()","10e025aa":"fig, ax = plt.subplots()\nscatter = ax.scatter(temp_df[' Net Income to Total Assets'], temp_df['y'], c=temp_df['y'], s=1)\nlegend1 = ax.legend(*scatter.legend_elements(), loc=\"upper left\", title=\"Classes\")\nax.add_artist(legend1)\nplt.show()\nplt.close()","494c502c":"from sklearn.feature_selection import SelectKBest\n\nkbest = SelectKBest(k=50)\nx_train = kbest.fit_transform(x_train, y_train)\nx_test = kbest.transform(x_test)\nx_val = kbest.transform(x_val)","ea859f40":"class PrintLRCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        print(f'LR at end of epoch {epoch} = {tf.keras.backend.eval(self.model.optimizer.lr(self.model.optimizer.iterations))}')\n        \n\ndef plot_history(history):\n    metrics = ['loss', 'prc', 'precision', 'recall']\n    for n, metric in enumerate(metrics):\n        name = metric.replace(\"_\",\" \").capitalize()\n        plt.subplot(2,2,n+1)\n        plt.plot(history.epoch, history.history[metric], label='Train')\n        plt.plot(history.epoch, history.history['val_'+metric], linestyle=\"--\", label='Val')\n        plt.xlabel('Epoch')\n        plt.ylabel(name)\n        if metric == 'loss':\n            plt.ylim([0, plt.ylim()[1]])\n        elif metric == 'auc':\n            plt.ylim([0.8,1])\n        else:\n            plt.ylim([0,1])\n            \n        plt.legend()\n    plt.show()\n\n    \ndef report(y_true, y_pred, p=0.5):\n    y_pred = y_pred = np.where(y_pred >= 0.5, 1, 0).squeeze()\n    cm_ = cm(y_true=y_true, y_pred=y_pred)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm_, annot=True, fmt=\"d\")\n    plt.title('Confusion matrix @{:.2f}'.format(p))\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\n    print('TN: ', cm_[0][0])\n    print('FP: ', cm_[0][1])\n    print('FN: ', cm_[1][0])\n    print('TP: ', cm_[1][1])\n\n    print(cr(y_true=y_true, y_pred=y_pred))\n    \n    \ndef plot_roc(y_true, y_pred, **kwargs):\n    fp, tp, _ = sklearn.metrics.roc_curve(y_true, y_pred)\n\n    plt.plot(100*fp, 100*tp, linewidth=2, **kwargs)\n    plt.xlabel('False positives [%]')\n    plt.ylabel('True positives [%]')\n    plt.xlim([-0.5,20])\n    plt.ylim([80,100.5])\n    plt.grid(True)\n    ax = plt.gca()\n    ax.set_aspect('equal')\n    plt.show()","3915d3de":"def guess_all_neg(x):\n    return np.zeros(len(x), dtype='float32')\n\n\ny_pred = guess_all_neg(x_train)\nreport(y_train, y_pred)\n\ny_pred = guess_all_neg(x_test)\nreport(y_test, y_pred)","73cbeb5d":"BATCH_SIZE = 2048\nMETRICS = [\n      keras.metrics.TruePositives(name='tp'),\n      keras.metrics.FalsePositives(name='fp'),\n      keras.metrics.TrueNegatives(name='tn'),\n      keras.metrics.FalseNegatives(name='fn'), \n      keras.metrics.BinaryAccuracy(name='accuracy'),\n      keras.metrics.Precision(name='precision'),\n      keras.metrics.Recall(name='recall'),\n      keras.metrics.AUC(name='auc'),\n      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n]\n\n\ndef make_model(metrics=METRICS, output_bias=None, lr=1e-3, dropout=0.5):\n    if output_bias is not None:\n        output_bias = tf.keras.initializers.Constant(output_bias)\n    model = keras.Sequential([\n        keras.layers.Dense(128, activation='relu', input_shape=(x_train.shape[-1],)),\n        keras.layers.Dense(128, activation='relu'),\n        keras.layers.Dropout(dropout),\n        keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias),\n    ])\n\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n                  loss=keras.losses.BinaryCrossentropy(),\n                  metrics=metrics)\n\n    return model","2b229085":"# We want initial loss to be around n_pos\/(n_pos + n_neg)=220\/6819~0.03\n# https:\/\/karpathy.github.io\/2019\/04\/25\/recipe\/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines\n\n# To do this we set the initial_bias = log(n_pos\/n_neg)\nEPOCHS = 200\n\ninitial_bias = np.log([n_pos \/ n_neg])\nprint(initial_bias)\n\nrandom.seed(0)\nlr = 1e-3\nlr = tf.keras.experimental.CosineDecay(lr, decay_steps=EPOCHS * len(x_train)\/\/BATCH_SIZE)\nmodel = make_model(output_bias=initial_bias, lr=lr)\nmodel.summary()\nmodel.save_weights('initial_weights')\n\nresults = model.evaluate(x_train, y_train, batch_size=BATCH_SIZE, verbose=0)\nprint(results)","fd26e348":"random.seed(0)\n\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(monitor='val_prc', verbose=1, patience=20, mode='max', restore_best_weights=True),\n    # PrintLRCallback()\n    # tf.keras.callbacks.ReduceLROnPlateau(monitor='val_prc', verbose=1, patience=10, mode='max', min_lr=0, factor=0.2)\n]\n\ntotal = n_pos + n_neg\nclass_weight = {0: (total \/ n_neg)\/2.0, 1: (total \/ n_pos)\/2.0}\nprint(class_weight)\n\nhistory = model.fit(\n    x_train,\n    y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    validation_data=(x_val, y_val),\n    class_weight=class_weight,\n    verbose=1\n)\n\nresults = model.evaluate(x_train, y_train, batch_size=BATCH_SIZE, verbose=0)\nfor name, value in zip(model.metrics_names, results):\n    print(name, ': ', value)\n\nresults = model.evaluate(x_val, y_val, batch_size=BATCH_SIZE, verbose=0)\nfor name, value in zip(model.metrics_names, results):\n    print(name, ': ', value)\n\nplot_history(history)","6592834c":"y_pred = model.predict(x_train)\nreport(y_train, y_pred)\nplot_roc(y_train, y_pred)","5251a147":"y_pred = model.predict(x_test)\nreport(y_test, y_pred)\nplot_roc(y_test, y_pred)\n\nresults = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE, verbose=0)\nfor name, value in zip(model.metrics_names, results):\n    print(name, ': ', value)","d43d22f0":"from imblearn.over_sampling import SMOTE \n\nsm = SMOTE(random_state=0)\nx_train_res, y_train_res = sm.fit_resample(x_train, y_train)","a906047b":"random.seed(0)\n\nmodel = make_model()\nmodel.load_weights('initial_weights')\nmodel.layers[-1].bias.assign([0])\n\nhistory = model.fit(\n    x_train_res,\n    y_train_res,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    batch_size=BATCH_SIZE,\n    verbose=1,\n    validation_data=(x_val, y_val)\n)\n\nresults = model.evaluate(x_train_res, y_train_res, batch_size=BATCH_SIZE, verbose=0)\nfor name, value in zip(model.metrics_names, results):\n    print(name, ': ', value)\n\nresults = model.evaluate(x_val, y_val, batch_size=BATCH_SIZE, verbose=0)\nfor name, value in zip(model.metrics_names, results):\n    print(name, ': ', value)\n\nplot_history(history)","6c8f00ef":"y_pred = model.predict(x_train)\nreport(y_train, y_pred)\nplot_roc(y_train, y_pred)","57c61b14":"y_pred = model.predict(x_test)\nreport(y_test, y_pred)\nplot_roc(y_test, y_pred)\n\nresults = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE, verbose=0)\nfor name, value in zip(model.metrics_names, results):\n    print(name, ': ', value)","db400cf5":"## SMOTE","a9f234e3":"# EDA","8606d066":"## Model with class weights + bias init","dd3c6bb2":"# Model","e8bfabfa":"# Imbalanced classification using tf","dd4a2b3e":"## Stupid baselines"}}