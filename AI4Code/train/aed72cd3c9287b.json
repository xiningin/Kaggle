{"cell_type":{"4a9c7317":"code","40b6053b":"code","b4e44a2c":"code","bbdc599c":"code","cb8dbb40":"code","c0b01534":"code","0f44f263":"code","6127592d":"code","23e773ef":"code","e000091a":"code","a79ffdb8":"code","bd4cc93c":"code","b8ca7b8c":"code","e7a1520b":"code","7a8393e3":"code","5197e967":"code","dc162482":"code","9037995f":"code","de7cf36d":"code","68319564":"code","4af09ed6":"code","57fd04ae":"code","968f4c94":"code","6d5dd2f4":"code","208c9163":"code","052db234":"code","4a493f0d":"code","9084b187":"code","f79922d0":"code","428f961b":"code","40ef4be3":"code","c94826bb":"code","d2d4e1d4":"code","7848cb36":"code","83fcf242":"code","fae75461":"code","32bdda88":"code","8e50487d":"code","45bda7cb":"code","457bf2bf":"code","fd01e494":"code","5448dd7f":"code","22858e17":"code","ad889084":"code","7d80e6e5":"code","7daea658":"code","fdd389eb":"code","1393a617":"code","1ed6d9ce":"code","7ca3a95f":"code","c1b08d18":"code","63c25a93":"code","b066ad6b":"code","dba44187":"code","727f2dfc":"code","33d23241":"code","6f24811c":"markdown","c2e5035a":"markdown","00713888":"markdown","97256140":"markdown","1ba08433":"markdown","22cc803b":"markdown","9aab4b62":"markdown","ede44b81":"markdown","3273e005":"markdown","a0723eba":"markdown","ffa88be6":"markdown","8dc7e222":"markdown","e5fc3c8a":"markdown","b6cc8631":"markdown","bf78c43a":"markdown","f51fd40a":"markdown"},"source":{"4a9c7317":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns","40b6053b":"from imblearn.over_sampling import SMOTE, ADASYN\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import  OneHotEncoder as ohe\nfrom sklearn.preprocessing import StandardScaler as ss\nfrom sklearn.compose import ColumnTransformer as ct","b4e44a2c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","bbdc599c":"#import scikitplot as skplt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import average_precision_score\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import f1_score, recall_score","cb8dbb40":"# Read the CSV file\ntransactions = pd.read_csv('..\/input\/datasetForFinalAssignment.csv')\nprint (transactions.shape)\ntransactions.head()","c0b01534":"transactions[\"class\"].value_counts(normalize=True)","0f44f263":"sns.countplot(x=\"class\", data=transactions)","6127592d":"transactions[\"timeBetween\"] = (pd.to_datetime(transactions[\"signup_time\"])-pd.to_datetime(transactions[\"purchase_time\"])).dt.seconds\n\n#transactions.apply (lambda row: timeBetween(row), axis=1)\ntransactions.head(3)","23e773ef":"transactions[\"numberOfTimesDeviceUsed\"] = transactions.groupby(\"device_id\")[\"user_id\"].transform(\"count\")\ntransactions.head(3)","e000091a":"# What is the percentage of the fraudulent transactions occur when a device was used multiple times? \n\ntransactions['class'].value_counts().plot.bar()\ntransactions['class'].value_counts()\n\ntotalFraudulent = len(transactions[transactions['class'] == 1])\nprint('total number of fraudulent transactions : ', totalFraudulent)\n\ntotalNormal = len(transactions[transactions['class'] == 0])\nprint('total number of Normal transactions : ', totalNormal)\n\n## find the cases where timeBetween is zero (just signed up to make a purchase)\nzeroSecondsTransactions = transactions[(transactions['signup_time-purchase_time'] == 0)]\nfraudulentAtZeroSeconds = len(zeroSecondsTransactions[zeroSecondsTransactions['class']==1])\nnonFraudulentAtZeroSeconds = len(zeroSecondsTransactions[zeroSecondsTransactions['class']==0])\n\nprint('total number of fraudulent transactions at zero seconds : ', fraudulentAtZeroSeconds)\nprint('total number of NONfraudulent transactions at zero seconds : ', nonFraudulentAtZeroSeconds)\nprint('if the customer makes a purchase at ZERO seconds after signup, these transactions are FRAUDULENT ', round(100*fraudulentAtZeroSeconds\/len(zeroSecondsTransactions),2) ,'% of the time' )\nratio = round(100*(fraudulentAtZeroSeconds\/totalFraudulent),2)\nprint('In the given dataset ', ratio,'% of fraudulent transactions happen at ZERO seconds from signup')\n\ndeviceUsedMoreThanOnce = transactions[(transactions['N[device_id]'] > 1)]\nprint(len(deviceUsedMoreThanOnce), ' of the records show that the same device was used more than once')\n\nfraudulentMultipleUse = deviceUsedMoreThanOnce[deviceUsedMoreThanOnce['class'] == 1]\nprint( ' Of these ', len(deviceUsedMoreThanOnce), ' transactions, ', len(fraudulentMultipleUse), 'of them are fraudulent ---', round(100*len(fraudulentMultipleUse)\/len(deviceUsedMoreThanOnce),2), '% of the time')\n","a79ffdb8":"transactions.info()","bd4cc93c":"dataCopy = transactions.copy()","b8ca7b8c":"y = transactions[\"class\"]\n#transactions.drop(columns = [\"Column 1\", \"user_id\",\"device_id\",\"class\",\"signup_time\",\"purchase_time\",\"ip_address\"], inplace=True)\ntransactions.drop(columns = [\"Column 1\", \"user_id\",\"device_id\",\"class\",\"signup_time\",\"purchase_time\",\"ip_address\"], inplace=True)","e7a1520b":"transactions.info()","7a8393e3":"transactions[\"sex\"] = transactions[\"sex\"].map({'M':1,'F':0})","5197e967":"transactions.head(5)","dc162482":"all_columns = transactions.columns","9037995f":"num_columns = transactions[['signup_time-purchase_time','purchase_value','age','sex','N[device_id]']]\nstdScalar = ss()\nstdScalar.fit(num_columns)\nscaled_Transactions = stdScalar.transform(num_columns)\n","de7cf36d":"df_scaledTransactions = pd.DataFrame(scaled_Transactions) #, index=scaled_Transactions.index, columns=num_columns)\ndf_scaledTransactions.columns = ['signup_time-purchase_time','purchase_value','age','sex','N[device_id]']\n","68319564":"browser_transactions = pd.get_dummies(transactions.browser, prefix='Browser').iloc[:,1:]\nsource_transactions = pd.get_dummies(transactions.source, prefix='Source').iloc[:,1:]","4af09ed6":"X_transAndScaledData = pd.concat([df_scaledTransactions, browser_transactions, source_transactions], axis=1)","57fd04ae":"X_transAndScaledData.info()","968f4c94":"X_transAndScaledData.head()","6d5dd2f4":"X_transAndScaledData.shape","208c9163":"X_train, X_test, y_train, y_test,indicies_tr,indicies_test =  train_test_split(X_transAndScaledData,\n                                                      y,\n                                                      np.arange(X_transAndScaledData.shape[0]),\n                                                      train_size = 0.95,\n                                                      test_size = 0.05,                                                      \n                                                      )","052db234":"X_train.shape","4a493f0d":"y_train.head()","9084b187":"dataForCost = dataCopy.iloc[indicies_test]\ndataForCost.purchase_value.head()","f79922d0":"def normalize(X):\n    \"\"\"\n    Make the distribution of the values of each variable similar by subtracting the mean and by dividing by the standard deviation.\n    \"\"\"\n    for feature in X.columns:\n        X[feature] -= X[feature].mean()\n        X[feature] \/= X[feature].std()\n    return X","428f961b":"sm = SMOTE(random_state=42)\n# Normalize the data\n#X_train = normalize(X_train)\n#X_test = normalize(X_test)\n    \nX_res, y_res = sm.fit_sample(X_train, y_train)\nnp.sum(y_res)\/len(y_res)","40ef4be3":"X_res.shape, y_res.shape","c94826bb":"def PrintStats(cmat, y_test, pred):\n   # separate out the confusion matrix components\n   tpos = cmat[0][0]\n   fneg = cmat[1][0]\n   fpos = cmat[0][1]\n   tneg = cmat[1][1]\n   print(cmat)\n  \n   print('True Positive:' + str(tpos))\n   print('False Negative:' + str(fneg))\n   print('False Positive:' + str(fpos))\n   print('True Negative:' + str(tneg))\n   \n   # calculate F!, Recall scores\n   f1Score = round(f1_score(y_test, pred), 2)\n   recallScore = round(recall_score(y_test, pred), 2)\n   # calculate and display metrics  \n   print( 'Accuracy: '+ str(np.round(100*float(tpos+fneg)\/float(tpos+fneg + fpos + tneg),2))+'%')\n   #print( 'Cohen Kappa: '+ str(np.round(cohen_kappa_score(y_test, pred),3)))\n   print(\"Sensitivity\/Recall for Model : {recall_score}\".format(recall_score = recallScore))\n   print(\"F1 Score for Model : {f1_score}\".format(f1_score = f1Score))","d2d4e1d4":"def CalculateCost(actual,prediction,data):\n  #if model prediction is true, but it is actually false - this will cost $8 per customer\n  costofFalsePositive = data.purchase_value[(prediction==1) & (actual==0)].count() * 8  \n  print(\"Cost for false predicition: ${:.0f}\".format(costofFalsePositive))\n  #if model prediction is false, but it is actually true - this will cost the purchase value\n  costofFalseNegative = data.purchase_value[(prediction==0) & (actual==1)].sum()\n  print(\"Cost lost due to wrong predicution: ${:.0f}\".format(costofFalseNegative))\n  totalCost = costofFalsePositive + costofFalseNegative\n  print(\"total Cost ${:.0f}\".format(totalCost))\n  \n  #return totalCost","7848cb36":"lr = LogisticRegression()\n# Fit and predict!\nlr.fit(X_train, y_train)\nY_pred = lr.predict(X_test)\nlr_cnf_matrix = confusion_matrix(y_test, Y_pred)","83fcf242":"# And finally: show the results\nprint(classification_report(y_test, Y_pred))","fae75461":"PrintStats(lr_cnf_matrix, y_test, Y_pred)","32bdda88":"CalculateCost(y_test,Y_pred,dataForCost)","8e50487d":"lr = LogisticRegression()\n# Fit and predict!\nlr.fit(X_res, y_res)\nY_pred = lr.predict(X_test)\nlr_cnf_matrix = confusion_matrix(y_test, Y_pred)","45bda7cb":"# And finally: show the results\nprint(classification_report(y_test, Y_pred))","457bf2bf":"PrintStats(lr_cnf_matrix, y_test, Y_pred)\n","fd01e494":"CalculateCost(y_test,Y_pred,dataForCost)","5448dd7f":"Y_pred.tofile(\"submission_MinCost.csv\",sep=\",\")","22858e17":"dt = DecisionTreeClassifier()\ndt.fit(X_res, y_res)\nY_dt_pred = dt.predict(X_test)\ndt_cnf_matrix = confusion_matrix(y_test, Y_dt_pred)","ad889084":"# And finally: show the results\nprint(classification_report(y_test, Y_dt_pred))","7d80e6e5":"PrintStats(dt_cnf_matrix, y_test, Y_dt_pred)","7daea658":"CalculateCost(y_test, Y_dt_pred, dataForCost)","fdd389eb":"rf = RandomForestClassifier(n_estimators = 500, n_jobs =4)\nrf.fit(X_res, y_res)\nY_rf_pred = rf.predict(X_test)\nrf_cnf_matrix = confusion_matrix(y_test, Y_rf_pred)","1393a617":"# And finally: show the results\nprint(classification_report(y_test, Y_rf_pred))","1ed6d9ce":"PrintStats(rf_cnf_matrix, y_test, Y_rf_pred)","7ca3a95f":"CalculateCost(y_test,Y_rf_pred,dataForCost)","c1b08d18":"\n#testData =  pd.read_csv('..\/input\/datasetForFinalTest.csv')\n#testData.head()","63c25a93":"#dataTestCopy = testData.copy()\n#testData.drop(columns = [\"Column 1\", \"user_id\",\"device_id\",\"signup_time\",\"purchase_time\",\"ip_address\"], inplace=True)\n#testData[\"sex\"] = testData[\"sex\"].map({'M':1,'F':0})\n#num_columns = testData[['signup_time-purchase_time','purchase_value','age','sex','N[device_id]']]\n#stdScalar = ss()\n#stdScalar.fit(num_columns)\n#scaled_TestTransactions = stdScalar.transform(num_columns)\n#df_testscaledTransactions = pd.DataFrame(scaled_TestTransactions) #, index=scaled_Transactions.index, columns=num_columns)\n#df_testscaledTransactions.columns = ['signup_time-purchase_time','purchase_value','age','sex','N[device_id]']\n#browser_testtransactions = pd.get_dummies(testData.browser, prefix='Browser').iloc[:,1:]\n#source_testtransactions = pd.get_dummies(testData.source, prefix='Source').iloc[:,1:]\n#X_testtransAndScaledData = pd.concat([df_testscaledTransactions, browser_testtransactions, source_testtransactions], axis=1)","b066ad6b":"#X_testtransAndScaledData.info()","dba44187":"#X_testtransAndScaledData.head()","727f2dfc":"#lr = LogisticRegression()\n#lr.fit(X_res, y_res)\n#Y_pred = lr.predict(X_testtransAndScaledData)","33d23241":"#Y_pred.tofile(\"submission_FinalTest.csv\",sep=\",\")","6f24811c":"From the above diagram, we can infer that the data is highly imbalanced and any predictions will lead to non-fraudlent decisions. Hence, we need to balance the data using SMOTE library","c2e5035a":"**Model # 1 --- Logistic Regression (Balanced Data)**","00713888":"Create a new column timeBetween","97256140":"**Model # 2 - Decision Trees**","1ba08433":"**Model # 1 --- Logistic Regression (Unbalanced Data)**","22cc803b":"## Data Modeling for test data\n\nBased on the above calculations,  Logistic regression seems to be cost effective compare to other models, however, the accuracy is high for Random Forest that of Logistic regression. Hence, it would be better to go with Logistic regression as it gives more cost effective model.","9aab4b62":"Standaradize the integer variables using Standard Scaler and convert the categorical variables into integer by using dummy variable. Both can be done using column transformter","ede44b81":"Create a new column labeled numberOfTimesDeviceUsed to identify how many times teh same device is used to make purchases","3273e005":"## Data Modelling","a0723eba":"Convert object variables into integer by using dummy variables","ffa88be6":"    Our customer is an e-commerce site that sells wholesale electronics. You have been contracted to build a model that predicts whether a given transaction is fraudulent or not. You only have information about each user\u2019s first transaction on the company's website. If you fail to identify a fraudulent transaction, the company loses money equivalent to the price of the fraudulently purchased product. If you incorrectly flag a real transaction as fraudulent, it inconveniences the customers whose valid transactions are flagged\u2014a cost your client values at $8...\n    \n    So, the task is to build a model that's predictive but also minimizes to cost to the company not only by correctly flagging and identifying fraudulent transactions but also to minimize the cost of wrong predictions since each wrong prediction costs the company $8.\n    \n    The evaluation criteria will be total cost to company for the test data provided; for example if your model identifies 100 cases as fraudulent while they are normal transactions, this would cost the company  8\u2217100= 800.\n\nThere are 2 files included in this assignment:\n\n1. training data to build your models:\n    datasetForFinalAssignment.csv\n\n2. Test data to apply your models to predict if the transaction id fraudulent or not.\n    datasetForFinalTest.csv","8dc7e222":"# Build a predictive model for determining if a customer transaction is fraudulent","e5fc3c8a":"Load the data","b6cc8631":"## Data Processing","bf78c43a":"**Model # 3 Random Forest**","f51fd40a":"# Data Analysis"}}