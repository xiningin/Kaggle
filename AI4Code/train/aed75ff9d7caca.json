{"cell_type":{"cad8548b":"code","babf155e":"code","ea239ca1":"code","2126b6b0":"code","f3765b88":"code","868c7761":"code","09a0daf9":"code","788c7c27":"code","21cd121c":"code","e5707a81":"code","816b0a76":"code","4793f924":"code","454a4b6f":"code","75226006":"code","171e8b11":"code","b84d83be":"code","a75727ba":"code","d2067d87":"code","541fa6cc":"code","be0a350e":"code","76d6dbc5":"code","ba44d76e":"code","efa98ac2":"code","c9205d1d":"markdown","598e2d3d":"markdown","619cd4ab":"markdown","ec54e88c":"markdown","41077de1":"markdown","42163eb7":"markdown","edf5100b":"markdown","c59ea1af":"markdown"},"source":{"cad8548b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# There are too many paths and printing them takes\n# up too much space so I don't do this normally\nif 1==0: \n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            pass\n            #print(os.path.join(dirname, filename))","babf155e":"!pip install swifter\n#!pip install scispacy\n#!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_sm-0.2.4.tar.gz","ea239ca1":"# Progress bar\nimport tqdm\n\n# Word2Vec\nfrom gensim.models.word2vec import Word2Vec\nfrom gensim.models.doc2vec import Doc2Vec,TaggedDocument\n\nfrom nltk.tokenize import word_tokenize,sent_tokenize \nfrom scipy.spatial.distance import cdist,cosine\nimport gc\nimport swifter\nimport spacy","2126b6b0":"#nlp = spacy.load(\"en_core_web_sm\")\n#tokenizer = nlp.Defaults.create_tokenizer(nlp)","f3765b88":"all_data = pd.read_csv(\"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_pmc.csv\").drop(columns = ['bibliography','raw_bibliography','raw_authors'])\n\"\"\"\nbiorxiv_clean = pd.read_csv(\"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/biorxiv_clean.csv\").drop(columns = ['bibliography','raw_bibliography','raw_authors'])\nclean_comm_use = pd.read_csv(\"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_comm_use.csv\").drop(columns = ['bibliography','raw_bibliography','raw_authors'])\nclean_noncomm_use = pd.read_csv(\"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_noncomm_use.csv\").drop(columns = ['bibliography','raw_bibliography','raw_authors'])\nclean_pmc = pd.read_csv(\"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_pmc.csv\").drop(columns = ['bibliography','raw_bibliography','raw_authors'])\n\nall_data = pd.concat([biorxiv_clean, clean_comm_use, clean_noncomm_use, clean_pmc]).reset_index(drop=True).drop_duplicates()\ndel biorxiv_clean,clean_comm_use,clean_noncomm_use,clean_pmc\ngc.collect()\n\"\"\"\nall_data.head()","868c7761":"print(\"Number of Rows in Table: %i\" % len(all_data))\nprint(\"Number of Titles: %i \" % all_data['title'].count())\nprint(\"Number of Abstracts: %i \" % all_data['abstract'].count())\nprint(\"Number of Texts: %i \" % all_data['text'].count())","09a0daf9":"def cos_sim(text,df,model):\n    # compute similarity\n    doc_sim = (\n        1-cdist(\n            df.values,\n            [model.wv[text]],\n            'cosine'\n        )\n    )\n    # convert result to a date frame\n    document_sim_df = (\n        pd.DataFrame(doc_sim, columns=[\"cos_sim\"])\n        .assign(document_id=list(df.index))\n    )\n    # sort from most similar to least\n    document_sim_df = document_sim_df.sort_values(\"cos_sim\", ascending=False)\n    \n    # perform left-join to get information about the documents\n    doc_sim_meta_df = document_sim_df.merge(all_data,\n                      how='left',\n                     left_on='document_id',\n                     right_on='paper_id')\n    return doc_sim_meta_df","788c7c27":"def majority_voting(text,df,model):\n    # choose top 100\n    doc_sim_meta_dfs = [cos_sim(word,df,model).iloc[:100] for word in text.split()]\n    return pd.merge(*doc_sim_meta_dfs,how = 'inner',on = 'document_id')","21cd121c":"# replace empty text with empty strings\nptn = r'\\[[0-9]{1,2}\\]'","e5707a81":"def tokenize_and_tag(x):\n    return TaggedDocument(word_tokenize(x['text'].replace('\\n\\n', ' ').replace(ptn,'').strip()),[x['paper_id']])","816b0a76":"#text_documents = list(map( lambda x: tokenizer(x),all_data.text.str.replace('\\n\\n', ' ').replace(ptn,'').str.strip()))\ntext_documents = all_data.swifter.apply(tokenize_and_tag,axis = 1)","4793f924":"#text_documents = [TaggedDocument(doc, [all_data.loc[i,'paper_id']]) for i, doc in enumerate(text_documents)]","454a4b6f":"text_model = Doc2Vec(text_documents,vector_size = 200,window=10, min_count=3, workers=4)\ntext_model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)","75226006":"document_dict_text = {}\nfor idx, text_df in tqdm.tqdm(all_data[[\"paper_id\", \"text\"]].iterrows()):\n    document_dict_text[text_df['paper_id']] = text_model.docvecs[text_df['paper_id']]","171e8b11":"text_document_embeddings_df = pd.DataFrame.from_dict(document_dict_text, orient=\"index\")\ntext_document_embeddings_df.head()","b84d83be":"npi_papers = majority_voting('non-pharmaceutical interventions',text_document_embeddings_df,text_model).loc[:100,['text_x','title_x']]","a75727ba":"# identify paragraphs for each document\n# each line breaks will give us paragraph \nnpi_papers['text_x'] = npi_papers['text_x'].replace('\\n\\n', ' ').str.split('\\n')\nnpi_tagged_docs = []\nparagraph_table = []\nfor i,row in npi_papers.iterrows():\n    title = row['title_x']\n    npi_tagged_docs += [TaggedDocument(word_tokenize(text),[f\"{title} - {j}\"]) for j,text in enumerate(row['text_x']) if len(text) > 1]\n    paragraph_table += [[title,j,text] for j,text in enumerate(row['text_x']) if len(text) > 1]\nparagraph_df = pd.DataFrame(data = paragraph_table,columns = ['title','paragraph_id','text'])","d2067d87":"paragraph_model = Doc2Vec(npi_tagged_docs,vector_size = 150,window=4, min_count=3, workers=4)\nparagraph_model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)","541fa6cc":"def get_paragraph_model_dic(x,text):\n    title = x[0]\n    paragraph_id = x[1]\n    doc_vec = paragraph_model.docvecs[f\"{title} - {paragraph_id}\"]\n    cos_sim = 1 - cosine(\n            doc_vec,\n            paragraph_model.wv[text]\n        )\n    return cos_sim","be0a350e":"paragraph_df['non_ph_cos_sim'] = paragraph_df.swifter.apply(get_paragraph_model_dic,axis=1,text = 'non-pharmaceutical')\nparagraph_df['interv_cos_sim'] = paragraph_df.swifter.apply(get_paragraph_model_dic,axis=1,text = 'intervention')\ntop_paragraphs = pd.merge(paragraph_df.sort_values(by = 'non_ph_cos_sim',ascending = False).iloc[:200],\n        paragraph_df.sort_values(by = 'interv_cos_sim',ascending = False).iloc[:200],\n         on = ['title','paragraph_id'],how = 'inner')","76d6dbc5":"top_paragraphs","ba44d76e":"top_paragraphs.to_csv('top_paragraphs.csv',index = False)","efa98ac2":"# predict subtasks \nparagraph_model.infer_vector(word_tokenize(\"Methods to control the spread in communities, barriers to compliance and how these vary among different populations.\"))","c9205d1d":"The first block of code is (almost) directly from kaggle","598e2d3d":"# Task: What do we know about non-pharmaceutical interventions?","619cd4ab":"## Introduction","ec54e88c":"Now that all our libraries are loaded we need data. We explore the full text in the files using the output generated from the following notebook:\nhttps:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv","41077de1":"## Train Doc2Vec Model\nIn this notebook, I applied [Doc2Vec](https:\/\/radimrehurek.com\/gensim\/models\/doc2vec.html)\n\nThe algorithm is introduced [here](https:\/\/arxiv.org\/pdf\/1405.4053v2.pdf)","42163eb7":"## Install\/Load Packages","edf5100b":"## idenitfy most relevant paragraph\nOnce we identify the document, let's identify the most relevant paragraph to the question.\nThe paragraph is recognized by the line breaks.","c59ea1af":"Next we installl scispacy, a repo of commands to deal with scientific documents. *Note that internet access needs to be switched on for this to work!*"}}