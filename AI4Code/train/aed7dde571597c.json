{"cell_type":{"671181d6":"code","8c7ea7ae":"code","4af9b20d":"code","ca0c435d":"code","a2dcc743":"code","509055f7":"code","e63ad89d":"code","44cfe894":"code","cd080ef9":"code","7a8f2fed":"code","231110ca":"code","1eec7fef":"code","ba21f99e":"code","2c89359d":"code","48d2d938":"code","67044c2e":"code","9dad520f":"code","a8a325ce":"code","824083d2":"code","c75382c2":"code","97ae4a50":"code","ac7979cd":"code","3838533e":"code","b633fd68":"code","76c272d6":"code","5364659f":"code","b5a4558f":"code","9ada40bd":"markdown","42fab291":"markdown","6fd817e4":"markdown","30404110":"markdown","45f0ae1f":"markdown","2d7cd881":"markdown","fcfb271c":"markdown","90d6cab8":"markdown","75d780df":"markdown","ee374898":"markdown","20c61c70":"markdown","19bd0094":"markdown","c9896bfe":"markdown"},"source":{"671181d6":"# Import some libraries\n\nimport pandas as pd\npd.options.mode.chained_assignment = None\nimport numpy as np\nseed = 0\nnp.random.seed(seed)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style = 'whitegrid')\n\n!pip install twint\nimport twint\nimport nest_asyncio\nnest_asyncio.apply()\n\n\nimport datetime as dt\nimport re\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n!pip install Sastrawi\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\nfrom wordcloud import WordCloud\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Dense, Dropout, LSTM\nfrom keras.optimizers import Adam, RMSprop, SGD\nfrom keras.callbacks import EarlyStopping\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix","8c7ea7ae":"# Scraping 25000 tweets and then store to csv file using twint (https:\/\/github.com\/twintproject\/twint)\n\n# c = twint.Config()\n# c.Search = '\"kuliah online\" lang:id'\n# c.Limit = 25000\n# c.Store_csv = True\n# c.Output = 'tweet_data.csv'\n# twint.run.Search(c)","4af9b20d":"# Load data from a CSV file into pandas DataFrame\n\ntweets_data = pd.read_csv('..\/input\/25k-tweets-kuliah-online\/25k_tweets_data.csv')\ntweets = tweets_data[['id', 'username', 'created_at', 'tweet', 'replies_count', 'retweets_count', 'likes_count']]\ntweets","ca0c435d":"# Some functions for preprocessing text\n\ndef cleaningText(text):\n    text = re.sub(r'@[A-Za-z0-9]+', '', text) # remove mentions\n    text = re.sub(r'#[A-Za-z0-9]+', '', text) # remove hashtag\n    text = re.sub(r'RT[\\s]', '', text) # remove RT\n    text = re.sub(r\"http\\S+\", '', text) # remove link\n    text = re.sub(r'[0-9]+', '', text) # remove numbers\n\n    text = text.replace('\\n', ' ') # replace new line into space\n    text = text.translate(str.maketrans('', '', string.punctuation)) # remove all punctuations\n    text = text.strip(' ') # remove characters space from both left and right text\n    return text\n\ndef casefoldingText(text): # Converting all the characters in a text into lower case\n    text = text.lower() \n    return text\n\ndef tokenizingText(text): # Tokenizing or splitting a string, text into a list of tokens\n    text = word_tokenize(text) \n    return text\n\ndef filteringText(text): # Remove stopwors in a text\n    listStopwords = set(stopwords.words('indonesian'))\n    filtered = []\n    for txt in text:\n        if txt not in listStopwords:\n            filtered.append(txt)\n    text = filtered \n    return text\n\ndef stemmingText(text): # Reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n    factory = StemmerFactory()\n    stemmer = factory.create_stemmer()\n    text = [stemmer.stem(word) for word in text]\n    return text\n\ndef toSentence(list_words): # Convert list of words into sentence\n    sentence = ' '.join(word for word in list_words)\n    return sentence","a2dcc743":"# # Preprocessing tweets data\n\n# tweets['text_clean'] = tweets['tweet'].apply(cleaningText)\n# tweets['text_clean'] = tweets['text_clean'].apply(casefoldingText)\n# tweets.drop(['tweet'], axis = 1, inplace = True)\n\n# tweets['text_preprocessed'] = tweets['text_clean'].apply(tokenizingText)\n# tweets['text_preprocessed'] = tweets['text_preprocessed'].apply(filteringText)\n# tweets['text_preprocessed'] = tweets['text_preprocessed'].apply(stemmingText)\n\n# # drop duplicates\/spams tweets\n# tweets.drop_duplicates(subset = 'text_clean', inplace = True)\n\n# Export to csv file\n# tweets.to_csv(r'25k_tweets_data_clean.csv', index = False, header = True,index_label=None)\n\n#tweets","509055f7":"# Because preprocessing tweets data takes a lot time, so I load tweets data which has been preprocessed before\ntweets = pd.read_csv('..\/input\/25k-tweets-kuliah-online\/25k_tweets_data_clean.csv')\n\nfor i, text in enumerate(tweets['text_preprocessed']):\n    tweets['text_preprocessed'][i] = tweets['text_preprocessed'][i].replace(\"'\", \"\")\\\n                                            .replace(',','').replace(']','').replace('[','')\n    list_words=[]\n    for word in tweets['text_preprocessed'][i].split():\n        list_words.append(word)\n        \n    tweets['text_preprocessed'][i] = list_words   \n    \ntweets","e63ad89d":"# Determine sentiment polarity of tweets using indonesia sentiment lexicon (source : https:\/\/github.com\/fajri91\/InSet)\n\n# Loads lexicon positive and negative data\nlexicon_positive = dict()\nimport csv\nwith open('..\/input\/25k-tweets-kuliah-online\/lexicon_positive.csv', 'r') as csvfile:\n    reader = csv.reader(csvfile, delimiter=',')\n    for row in reader:\n        lexicon_positive[row[0]] = int(row[1])\n\nlexicon_negative = dict()\nimport csv\nwith open('..\/input\/25k-tweets-kuliah-online\/lexicon_negative.csv', 'r') as csvfile:\n    reader = csv.reader(csvfile, delimiter=',')\n    for row in reader:\n        lexicon_negative[row[0]] = int(row[1])\n        \n# Function to determine sentiment polarity of tweets        \ndef sentiment_analysis_lexicon_indonesia(text):\n    #for word in text:\n    score = 0\n    for word in text:\n        if (word in lexicon_positive):\n            score = score + lexicon_positive[word]\n    for word in text:\n        if (word in lexicon_negative):\n            score = score + lexicon_negative[word]\n    polarity=''\n    if (score > 0):\n        polarity = 'positive'\n    elif (score < 0):\n        polarity = 'negative'\n    else:\n        polarity = 'neutral'\n    return score, polarity","44cfe894":"# Results from determine sentiment polarity of tweets\n\nresults = tweets['text_preprocessed'].apply(sentiment_analysis_lexicon_indonesia)\nresults = list(zip(*results))\ntweets['polarity_score'] = results[0]\ntweets['polarity'] = results[1]\nprint(tweets['polarity'].value_counts())\n\n# Export to csv file\n# tweets.to_csv(r'25k_tweets_data_clean_polarity.csv', index = False, header = True,index_label=None)\n\ntweets","cd080ef9":"fig, ax = plt.subplots(figsize = (6, 6))\nsizes = [count for count in tweets['polarity'].value_counts()]\nlabels = list(tweets['polarity'].value_counts().index)\nexplode = (0.1, 0, 0)\nax.pie(x = sizes, labels = labels, autopct = '%1.1f%%', explode = explode, textprops={'fontsize': 14})\nax.set_title('Sentiment Polarity on Tweets Data \\n (total = 23699 tweets)', fontsize = 16, pad = 20)\nplt.show()","7a8f2fed":"pd.set_option('display.max_colwidth', 3000)\npositive_tweets = tweets[tweets['polarity'] == 'positive']\npositive_tweets = positive_tweets[['text_clean', 'polarity_score', 'polarity']].sort_values(by = 'polarity_score', ascending=False).reset_index(drop = True)\npositive_tweets.index += 1\npositive_tweets[0:10]","231110ca":"pd.set_option('display.max_colwidth', 3000)\nnegative_tweets = tweets[tweets['polarity'] == 'negative']\nnegative_tweets = negative_tweets[['text_clean', 'polarity_score', 'polarity']].sort_values(by = 'polarity_score', ascending=True)[0:10].reset_index(drop = True)\nnegative_tweets.index += 1\nnegative_tweets[0:10]","1eec7fef":"# Visualize word cloud\n\nlist_words=''\nfor tweet in tweets['text_preprocessed']:\n    for word in tweet:\n        list_words += ' '+(word)\n        \nwordcloud = WordCloud(width = 600, height = 400, background_color = 'black', min_font_size = 10).generate(list_words)\nfig, ax = plt.subplots(figsize = (8, 6))\nax.set_title('Word Cloud of Tweets Data', fontsize = 18)\nax.grid(False)\nax.imshow((wordcloud))\nfig.tight_layout(pad=0)\nax.axis('off')\nplt.show()","ba21f99e":"# Function to group all positive\/negative words\ndef words_with_sentiment(text):\n    positive_words=[]\n    negative_words=[]\n    for word in text:\n        score_pos = 0\n        score_neg = 0\n        if (word in lexicon_positive):\n            score_pos = lexicon_positive[word]\n        if (word in lexicon_negative):\n            score_neg = lexicon_negative[word]\n        \n        if (score_pos + score_neg > 0):\n            positive_words.append(word)\n        elif (score_pos + score_neg < 0):\n            negative_words.append(word)\n            \n    return positive_words, negative_words","2c89359d":"# Visualize positive and negative word cloud\n\nsentiment_words = tweets['text_preprocessed'].apply(words_with_sentiment)\nsentiment_words = list(zip(*sentiment_words))\npositive_words = sentiment_words[0]\nnegative_words = sentiment_words[1]\n\nfig, ax = plt.subplots(1, 2,figsize = (12, 10))\nlist_words_postive=''\nfor row_word in positive_words:\n    for word in row_word:\n        list_words_postive += ' '+(word)\nwordcloud_positive = WordCloud(width = 800, height = 600, background_color = 'black', colormap = 'Greens'\n                               , min_font_size = 10).generate(list_words_postive)\nax[0].set_title('Word Cloud of Positive Words on Tweets Data \\n (based on Indonesia Sentiment Lexicon)', fontsize = 14)\nax[0].grid(False)\nax[0].imshow((wordcloud_positive))\nfig.tight_layout(pad=0)\nax[0].axis('off')\n\nlist_words_negative=''\nfor row_word in negative_words:\n    for word in row_word:\n        list_words_negative += ' '+(word)\nwordcloud_negative = WordCloud(width = 800, height = 600, background_color = 'black', colormap = 'Reds'\n                               , min_font_size = 10).generate(list_words_negative)\nax[1].set_title('Word Cloud of Negative Words on Tweets Data \\n (based on Indonesia Sentiment Lexicon)', fontsize = 14)\nax[1].grid(False)\nax[1].imshow((wordcloud_negative))\nfig.tight_layout(pad=0)\nax[1].axis('off')\n\nplt.show()","48d2d938":"# Visualize counts of tweets created based on hours\n\npd.plotting.register_matplotlib_converters()\ntweets_created = pd.DataFrame()\ntweets_created['created_at'] = tweets['created_at'].str.split(' ', expand = True)[1]\ntweets_created['created_at'] = pd.to_datetime(tweets_created['created_at'])\ntweets_created['created_at']= tweets_created['created_at'].dt.round('H')\ntweets_created['created_at'] = tweets_created['created_at'].dt.time\ntweets_created\n\nfig, ax = plt.subplots(figsize = (12, 4))\nx_values = tweets_created['created_at'].value_counts().sort_index().index\ny_values = tweets_created['created_at'].value_counts().sort_index()\nsns.lineplot(ax = ax, data = tweets, x = x_values, y = y_values)\nax.set_title('Count of Tweets Created \\n (based on hours)', fontsize = 18)\nax.set_xlabel('Hours')\nax.set_xticks(x_values)\nax.set_xticklabels(x_values, rotation = 45)\nax.set_ylabel('Count')\nplt.show()","67044c2e":"# Make text preprocessed (tokenized) to untokenized with toSentence Function\nX = tweets['text_preprocessed'].apply(toSentence) \nmax_features = 5000\n\n# Tokenize text with specific maximum number of words to keep, based on word frequency\ntokenizer = Tokenizer(num_words=max_features, split=' ')\ntokenizer.fit_on_texts(X.values)\nX = tokenizer.texts_to_sequences(X.values)\nX = pad_sequences(X)\nX.shape","9dad520f":"# Encode target data into numerical values\npolarity_encode = {'negative' : 0, 'neutral' : 1, 'positive' : 2}\ny = tweets['polarity'].map(polarity_encode).values\n\n# Split the data (with composition data train 80%, data test 20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","a8a325ce":"# Create model function with default hyperparameter values\n\ndef create_model(embed_dim = 16, hidden_unit = 16, dropout_rate = 0.2, optimizers = Adam, learning_rate = 0.001):\n    model = Sequential()\n    model.add(Embedding(input_dim = max_features, output_dim = embed_dim, input_length = X_train.shape[1]))\n    model.add(LSTM(units = hidden_unit, activation = 'tanh'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(units = 3, activation = 'softmax'))\n    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizers(lr = learning_rate), metrics = ['accuracy'])\n    print(model.summary())\n    return model","824083d2":"# # Hyperparameter tuning (to know the best hyperparameter for this model)\n\n# # Wrapper model with default hyperparameter values\n# model = KerasClassifier(build_fn = create_model, epochs = 25, batch_size=128) \n\n# # Hyperparameters\n# embed_dim = [32, 64]\n# hidden_unit = [16, 32, 64]\n# dropout_rate = [0.2]\n# optimizers = [Adam, RMSprop]\n# learning_rate = [0.01, 0.001, 0.0001]\n# epochs = [10, 25, 50, 100]\n# batch_size = [128, 256]\n# param_grid = dict(embed_dim = embed_dim, hidden_unit = hidden_unit, dropout_rate = dropout_rate,\n#                   learning_rate = learning_rate, optimizers = optimizers, epochs = epochs, batch_size = batch_size)\n\n# # Evaluation model with GridSearchCV to know what the best hyperparameter for model \n# grid = GridSearchCV(estimator = model, param_grid = param_grid, cv = 3)\n# grid_result = grid.fit(X_train, y_train)\n\n# results = pd.DataFrame()\n# results['means'] = grid_result.cv_results_['mean_test_score']\n# results['stds'] = grid_result.cv_results_['std_test_score']\n# results['params'] = grid_result.cv_results_['params']\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# results.to_csv(r'gridsearchcv_results.csv.csv', index = False, header = True)\n# results.sort_values(by='means', ascending = False).reset_index(drop=True)","c75382c2":"# Results from hyperparameter tuning\nresults = pd.read_csv('..\/input\/25k-tweets-kuliah-online\/gridsearchcv_results.csv')\nresults.sort_values(by='means', ascending = False).reset_index(drop=True)","97ae4a50":"# From results above, we know the best hyperparameter for this model is :\n# {'batch_size': 128, 'dropout_rate': 0.2, 'embed_dim': 32, 'epochs': 10, 'hidden_unit': 16, 'learning_rate': 0.001, 'optimizers': <class 'keras.optimizers.RMSprop'>}\n\n# Create the model with the best hyperparameter which has been determined\nmodel = KerasClassifier(build_fn = create_model,\n                        # Model Parameters\n                        dropout_rate = 0.2,\n                        embed_dim = 32,\n                        hidden_unit = 16,\n                        optimizers = RMSprop,\n                        learning_rate = 0.001,\n                   \n                        # Fit Parameters\n                        epochs=10, \n                        batch_size=128,\n                        # Initiate validation data, which is 10% data from data train. It's used for evaluation model\n                        validation_split = 0.1)\n                         \n\nmodel_prediction = model.fit(X_train, y_train)","ac7979cd":"# Visualization model accuracy (train and val accuracy)\n\nfig, ax = plt.subplots(figsize = (10, 4))\nax.plot(model_prediction.history['accuracy'], label = 'train accuracy')\nax.plot(model_prediction.history['val_accuracy'], label = 'val accuracy')\nax.set_title('Model Accuracy')\nax.set_xlabel('Epoch')\nax.set_ylabel('Accuracy')\nax.legend(loc = 'upper left')\nplt.show()","3838533e":"# Predict sentiment on data test by using model has been created, and then visualize a confusion matrix\n\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint('Model Accuracy on Test Data:', accuracy)\nconfusion_matrix(y_test, y_pred)\n\nfig, ax = plt.subplots(figsize = (8,6))\nsns.heatmap(confusion_matrix(y_true = y_test, y_pred = y_pred), fmt = 'g', annot = True)\nax.xaxis.set_label_position('top')\nax.xaxis.set_ticks_position('top')\nax.set_xlabel('Prediction', fontsize = 14)\nax.set_xticklabels(['negative (0)', 'neutral (1)', 'positive (2)'])\nax.set_ylabel('Actual', fontsize = 14)\nax.set_yticklabels(['negative (0)', 'neutral (1)', 'positive (2)'])\nplt.show()","b633fd68":"# Results from prediction sentiment on data test\ntext_clean = tweets['text_clean']\ntext_train, text_test = train_test_split(text_clean, test_size = 0.2, random_state = 0)\nresult_test = pd.DataFrame(data = zip(text_test, y_pred), columns = ['text', 'polarity'])\npolarity_decode = {0 : 'Negative', 1 : 'Neutral', 2 : 'Positive'}\nresult_test['polarity'] = result_test['polarity'].map(polarity_decode)\npd.set_option('max_colwidth', 300)\nresult_test","76c272d6":"# Initializing and preprocessing new text data\notherData = pd.DataFrame()\notherData['text'] = ['enaknya kuliah online, ga perlu capek\" ke kampus dan bisa santai dirumah',\n                     'Tapi serius deh semakin kesini ngerasa kuliah online makin ga efektif, ga paham materi blasss, kopong, berasa yauda kek ga kuliah' \n                    ]\n\notherData['text_clean'] = otherData['text'].apply(cleaningText)\notherData['text_clean'] = otherData['text_clean'].apply(casefoldingText)\notherData.drop(['text'], axis = 1, inplace = True)\n\notherData['text_preprocessed'] = otherData['text_clean'].apply(tokenizingText)\notherData['text_preprocessed'] = otherData['text_preprocessed'].apply(filteringText)\notherData['text_preprocessed'] = otherData['text_preprocessed'].apply(stemmingText)\notherData","5364659f":"# Preprocessing text data\n\n# Make text preprocessed (tokenized) to untokenized with toSentence Function\nX_otherData = otherData['text_preprocessed'].apply(toSentence)\nX_otherData = tokenizer.texts_to_sequences(X_otherData.values)\nX_otherData = pad_sequences(X_otherData, maxlen = X.shape[1])\nX_otherData","b5a4558f":"# Results from prediction sentiment on text data\n\ny_pred_otherData = model.predict(X_otherData)\notherData['Result Prediction'] = y_pred_otherData\n\npolarity_decode = {0 : 'Negative', 1 : 'Neutral', 2 : 'Positive'}\notherData['Result Prediction'] = otherData['Result Prediction'].map(polarity_decode)\notherData","9ada40bd":"## Preprocessing Text Data","42fab291":"## Word Cloud","6fd817e4":"## Comparasion Sentiment Polarity on Tweets Data","30404110":"## Top 10 Positive and Negative Tweet Sentiments","45f0ae1f":"## Positive and Negative Word Cloud","2d7cd881":"## Introductions\n\n<div style=\"text-align: justify\"> \n    <br><\/br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; In this project I do sentiment analysis on the topic \u201conline lectures  (\u201ckuliah online\u201d in bahasa)\u201d. The purpose is to know what is the opinion twitter users in Indonesia about online lectures (\u201ckuliah online\u201d) activity in Indonesia, whether is more \u201ccontra\u201d than \u201cpro\u201d or otherwise ? Therefore I do analysis and classification (negative, neutral or positive) on each user tweets about  \u201conline lectures  (\u201ckuliah online\u201d in bahasa)\u201d in Indonesia.\n    <br><\/br>\n    <br><\/br>\n&nbsp;&nbsp; The data used in this project is 25.000 twitter data (tweets) that\u2019s contain topic \u201ckuliah online\u201d which scraping with twitter scraping tool Twint (https:\/\/github.com\/twintproject\/twint). After that, I do preprocessing on tweets data and determine sentiment polarity of tweets with Indonesian Sentiment Lexicon (https:\/\/github.com\/fajri91\/InSet).  Data that has been determined, will be used for sentiment analysis with model recurrent neural network which is Long Short Term Memory (LSTM). \n<\/div>\n","fcfb271c":"## Model LSTM","90d6cab8":"## Analysis and Visualization","75d780df":"## Predict with Other Data","ee374898":"## Counts of Tweets Created based on Hours","20c61c70":"# Sentiment Analysis Using LSTM","19bd0094":"# Scraping 25000 tweets about 'kuliah online'","c9896bfe":"## Determine Sentiment Polarity of Tweets with Indonesia Sentiment Lexicon"}}