{"cell_type":{"49599822":"code","bd2827ca":"code","b7d3f4c2":"code","e9de48a4":"code","605cf11f":"code","cee06e08":"code","9b8bccaa":"code","e07eab3c":"code","a5b3dc66":"code","2603bcdf":"code","d2dda432":"code","55e104d9":"markdown","668d0b06":"markdown"},"source":{"49599822":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt","bd2827ca":"df=pd.read_csv('..\/input\/chinese-mnist-digit-recognizer\/chineseMNIST.csv')","b7d3f4c2":"df.head()","e9de48a4":"cols=df.columns.tolist()\ncols.remove(\"label\")\ncols.remove(\"character\")\nX=df[cols]\ny=df[\"character\"]","605cf11f":"df['label'].replace(100, 11, inplace=True)\ndf['label'].replace(1000, 12, inplace=True)\ndf['label'].replace(10000, 13, inplace=True)\ndf['label'].replace(100000000, 14, inplace=True)","cee06e08":"X = df.drop(['label', 'character'], axis = 1)\nY = df['label']\nX=X.values","9b8bccaa":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)","e07eab3c":"from numpy import mean\nfrom numpy import std\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.utils import to_categorical","a5b3dc66":"def load_dataset():\n  # load dataset\n  (trainX, trainY), (testX, testY) = (X_train,Y_train),( X_test, Y_test)\n  # reshape dataset to have a single channel\n  trainX = trainX.reshape((trainX.shape[0], 64, 64, 1))\n  testX = testX.reshape((testX.shape[0], 64, 64, 1))\n  # one hot encode target values\n  trainY = to_categorical(trainY)\n  testY = to_categorical(testY)\n  return trainX, trainY, testX, testY\n  # scale pixels\ndef prep_pixels(train, test):\n  # convert from integers to floats\n  train_norm = train.astype('float32')\n  test_norm = test.astype('float32')\n  # normalize to range 0-1\n  train_norm = train_norm \/ 255.0\n  test_norm = test_norm \/ 255.0\n  # return normalized images\n  return train_norm, test_norm\ndef define_model():\n  model = Sequential()\n  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform',\n  input_shape=(64, 64, 1)))\n  model.add(MaxPooling2D((2, 2)))\n  model.add(Flatten())\n  model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n  model.add(Dense(15, activation='softmax'))\n  # compile model\n  opt = SGD(lr=0.01, momentum=0.9)\n  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n  return model\ndef evaluate_model(dataX, dataY, n_folds=5):\n  scores, histories = list(), list()\n  # prepare cross validation\n  kfold = KFold(n_folds, shuffle=True, random_state=1)\n  # enumerate splits\n  for train_ix, test_ix in kfold.split(dataX):\n  # define model\n    model = define_model()\n    # select rows for train and test\n    trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix],dataY[test_ix]\n    # fit model\n    history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX,testY), verbose=0)\n    # evaluate model\n    _, acc = model.evaluate(testX, testY, verbose=0)\n    print('> %.3f' % (acc * 100.0))\n    # append scores\n    scores.append(acc)\n    histories.append(history)\n  return scores, histories\n# plot diagnostic learning curves\ndef summarize_diagnostics(histories):\n  for i in range(len(histories)):\n    # plot loss\n    pyplot.subplot(211)\n    pyplot.title('Cross Entropy Loss')\n    pyplot.plot(histories[i].history['loss'], color='blue', label='train')\n    pyplot.plot(histories[i].history['val_loss'], color='orange', label='test')\n    # plot accuracy\n    pyplot.subplot(212)\n    pyplot.title('Classification Accuracy')\n    pyplot.plot(histories[i].history['accuracy'], color='blue', label='train')\n    pyplot.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n  pyplot.show()\n# summarize model performance\ndef summarize_performance(scores):\n  # print summary\n  print('Accuracy: mean=%.3f std=%.3f, n=%d'% (mean(scores)*100, std(scores)*100,\n  len(scores)))\n  # box and whisker plots of results\n  pyplot.boxplot(scores)\n  pyplot.show()\n  # run the test harness for evaluating a model\ndef run_test_harness():\n    # load dataset\n  trainX, trainY, testX, testY = load_dataset()\n  # prepare pixel data\n  trainX, testX = prep_pixels(trainX, testX)\n  # evaluate model\n  scores, histories = evaluate_model(trainX, trainY)\n  # learning curves\n  summarize_diagnostics(histories)\n  # summarize estimated performance\n  summarize_performance(scores)\n  # entry point, run the test harness\nrun_test_harness()","2603bcdf":"from tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import BatchNormalization","d2dda432":"def load_dataset():\n  # load dataset\n  (trainX, trainY), (testX, testY) = (X_train,Y_train),( X_test, Y_test)\n  # reshape dataset to have a single channel\n  trainX = trainX.reshape((trainX.shape[0], 64, 64, 1))\n  testX = testX.reshape((testX.shape[0], 64, 64, 1))\n  # one hot encode target values\n  trainY = to_categorical(trainY)\n  testY = to_categorical(testY)\n  return trainX, trainY, testX, testY\n  # scale pixels\ndef prep_pixels(train, test):\n  # convert from integers to floats\n  train_norm = train.astype('float32')\n  test_norm = test.astype('float32')\n  # normalize to range 0-1\n  train_norm = train_norm \/ 255.0\n  test_norm = test_norm \/ 255.0\n  # return normalized images\n  return train_norm, test_norm\ndef define_model():\n  model = Sequential()\n  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform',\n  padding='same', input_shape=(64,64,1)))\n  model.add(BatchNormalization())\n  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform',\n  padding='same'))\n  model.add(BatchNormalization())\n  model.add(MaxPooling2D((2, 2)))\n  model.add(Dropout(0.2))\n  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform',\n  padding='same'))\n  model.add(BatchNormalization())\n  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform',\n  padding='same'))\n  model.add(BatchNormalization())\n  model.add(MaxPooling2D((2, 2)))\n  model.add(Dropout(0.3))\n  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform',\n  padding='same'))\n  model.add(BatchNormalization())\n  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform',\n  padding='same'))\n  model.add(BatchNormalization())\n  model.add(MaxPooling2D((2, 2)))\n  model.add(Dropout(0.4))\n  model.add(Flatten())\n  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n  model.add(BatchNormalization())\n  model.add(Dropout(0.5))\n  model.add(Dense(15, activation='softmax'))\n  # compile model\n  opt = SGD(lr=0.001, momentum=0.9)\n  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n  return model\n\ndef evaluate_model(dataX, dataY, n_folds=5):\n  scores, histories = list(), list()\n  # prepare cross validation\n  kfold = KFold(n_folds, shuffle=True, random_state=1)\n  # enumerate splits\n  for train_ix, test_ix in kfold.split(dataX):\n  # define model\n    model = define_model()\n    # select rows for train and test\n    trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix],dataY[test_ix]\n    # fit model\n    history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX,testY), verbose=0)\n    # evaluate model\n    _, acc = model.evaluate(testX, testY, verbose=0)\n    print('> %.3f' % (acc * 100.0))\n    # append scores\n    scores.append(acc)\n    histories.append(history)\n  return scores, histories\n# plot diagnostic learning curves\ndef summarize_diagnostics(histories):\n  for i in range(len(histories)):\n    # plot loss\n    pyplot.subplot(211)\n    pyplot.title('Cross Entropy Loss')\n    pyplot.plot(histories[i].history['loss'], color='blue', label='train')\n    pyplot.plot(histories[i].history['val_loss'], color='orange', label='test')\n    # plot accuracy\n    pyplot.subplot(212)\n    pyplot.title('Classification Accuracy')\n    pyplot.plot(histories[i].history['accuracy'], color='blue', label='train')\n    pyplot.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n  pyplot.show()\n# summarize model performance\ndef summarize_performance(scores):\n  # print summary\n  print('Accuracy: mean=%.3f std=%.3f, n=%d'% (mean(scores)*100, std(scores)*100,\n  len(scores)))\n  # box and whisker plots of results\n  pyplot.boxplot(scores)\n  pyplot.show()\n  # run the test harness for evaluating a model\ndef run_test_harness():\n    # load dataset\n  trainX, trainY, testX, testY = load_dataset()\n  # prepare pixel data\n  trainX, testX = prep_pixels(trainX, testX)\n  # evaluate model\n  scores, histories = evaluate_model(trainX, trainY)\n  # learning curves\n  summarize_diagnostics(histories)\n  # summarize estimated performance\n  summarize_performance(scores)\n  # entry point, run the test harness\nrun_test_harness()","55e104d9":"BASELINE MODEL","668d0b06":"Baseline + Increasing Dropout + Batch Norm"}}