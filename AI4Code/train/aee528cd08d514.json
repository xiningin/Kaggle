{"cell_type":{"6dbc8329":"code","846433b3":"code","efcdd6ca":"code","bd8bd816":"code","21246291":"code","1786a8e9":"code","ac2fcea4":"code","cc1977f7":"code","9700b2f8":"code","e3781d3a":"code","8185b4a5":"code","34516e2e":"code","5c8627a9":"code","1e27780e":"code","b5f27387":"code","b3e37a23":"code","222642a1":"code","59c8031a":"code","12a1b98a":"code","7aaebcea":"code","a7e9da93":"code","ea5d6d7b":"code","54c9b9e7":"code","29f642e5":"code","2769e13f":"code","cb0c48c2":"code","8fca06e1":"code","564e0ba3":"code","4b1a7df2":"code","ff612eaf":"code","fa68a738":"code","867bf9b6":"code","50238bc3":"code","6f585f81":"code","1bc1e5ca":"code","1154fc6c":"code","843b9a6d":"code","1b0c30ea":"code","9ef5499b":"code","17f643c6":"code","193399ad":"code","456b2a5c":"code","ad3fcb5c":"code","82aabff4":"code","be68488f":"code","e55f122a":"code","c884196a":"code","25a7edeb":"code","727eecbd":"code","00e58130":"code","0a6ff5c4":"code","51fb08c1":"markdown","e957a7ce":"markdown","084b6c35":"markdown","40239c84":"markdown","f4067da9":"markdown","cae06b13":"markdown","92bdcd57":"markdown","f8484270":"markdown","7adeb4e3":"markdown","b70d7309":"markdown","fcc8cd9f":"markdown","63151dd6":"markdown","f48d11a2":"markdown","584b69a7":"markdown","b852ef4f":"markdown","796eda22":"markdown","3ed12af1":"markdown","601a14b5":"markdown","ccf63ef4":"markdown","91c21311":"markdown","9863fbd9":"markdown","61a463d2":"markdown","988ff7b0":"markdown","40903a50":"markdown","39ba8873":"markdown","12d29dfd":"markdown","0cc01d00":"markdown","a7515c9f":"markdown","30c87e2c":"markdown","bf97ade3":"markdown","85e2d013":"markdown","b9889f6f":"markdown","d7de12a0":"markdown","9569b4d8":"markdown","0048d29a":"markdown","fecf856d":"markdown","55320197":"markdown","c858b712":"markdown","12db1809":"markdown","f6a501e0":"markdown","01546977":"markdown","f6527d63":"markdown","85588924":"markdown","c82976b1":"markdown","73a24be7":"markdown","e13bda08":"markdown","9b639240":"markdown","24cf81b6":"markdown","e8123e11":"markdown","67961155":"markdown","77df3c39":"markdown","4c71fe39":"markdown","e413bba6":"markdown","793f5626":"markdown","66664e4f":"markdown","3ef16eaa":"markdown","ca9cfacb":"markdown","4da8051d":"markdown","661a15c9":"markdown","971c0420":"markdown","d33232f4":"markdown","f9113292":"markdown","39800ff5":"markdown","2c66ed38":"markdown","1fcfc118":"markdown","70851b26":"markdown","229ec25e":"markdown","4e93587e":"markdown","149dbd94":"markdown","730a22eb":"markdown","3f390c78":"markdown","218479c7":"markdown","f304c0d0":"markdown","859fd300":"markdown","eacf04c5":"markdown","b7583d08":"markdown","d05521e6":"markdown","c7d82ad7":"markdown","71c92ea8":"markdown","0eaa2ee9":"markdown"},"source":{"6dbc8329":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split,cross_val_predict,cross_val_score\nfrom sklearn.metrics import roc_auc_score,confusion_matrix,make_scorer,classification_report,roc_curve,auc\nfrom sklearn.model_selection import StratifiedKFold\nfrom imblearn.over_sampling import SMOTE, RandomOverSampler\nfrom imblearn.under_sampling import ClusterCentroids,NearMiss, RandomUnderSampler\nimport lightgbm as lgb\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn import tree\nimport graphviz\nfrom pdpbox import pdp, get_dataset, info_plots\nimport scikitplot as skplt\nfrom scikitplot.metrics import plot_confusion_matrix,plot_precision_recall_curve\n\n\nfrom scipy.stats import randint as sp_randint\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nrandom_state=42\nnp.random.seed(random_state)","846433b3":"#importing the train dataset\ntrain_df=pd.read_csv('..\/input\/train.csv')\ntrain_df.head()","efcdd6ca":"#Shape of the train dataset\ntrain_df.shape","bd8bd816":"#Summary of the dataset\ntrain_df.describe()","21246291":"%%time\n#target classes count\ntarget_class=train_df['target'].value_counts()\nprint('Count of target classes :\\n',target_class)\n#Percentage of target classes count\nper_target_class=train_df['target'].value_counts()\/len(train_df)*100\nprint('percentage of count of target classes :\\n',per_target_class)\n\n#Countplot and violin plot for target classes\nfig,ax=plt.subplots(1,2,figsize=(20,5))\nsns.countplot(train_df.target.values,ax=ax[0],palette='husl')\nsns.violinplot(x=train_df.target.values,y=train_df.index.values,ax=ax[1],palette='husl')\nsns.stripplot(x=train_df.target.values,y=train_df.index.values,jitter=True,color='black',linewidth=0.5,size=0.5,alpha=0.5,ax=ax[1],palette='husl')\nax[0].set_xlabel('Target')\nax[1].set_xlabel('Target')\nax[1].set_ylabel('Index')","1786a8e9":"%%time\n#Distribution of train attributes\ndef plot_train_attribute_distribution(t0,t1,label1,label2,train_attributes):\n    i=0\n    sns.set_style('whitegrid')\n    \n    fig=plt.figure()\n    ax=plt.subplots(10,10,figsize=(22,18))\n    \n    for attribute in train_attributes:\n        i+=1\n        plt.subplot(10,10,i)\n        sns.distplot(t0[attribute],hist=False,label=label1)\n        sns.distplot(t1[attribute],hist=False,label=label2)\n        plt.legend()\n        plt.xlabel('Attribute',)\n        sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n    plt.show()","ac2fcea4":"%%time\n#corresponding to negative class\nt0=train_df[train_df.target.values==0]\n#corresponding to positive class\nt1=train_df[train_df.target.values==1]\n#train attributes from 2 to 102\ntrain_attributes=train_df.columns.values[2:102]\n#plot distribution of train attributes\nplot_train_attribute_distribution(t0,t1,'0','1',train_attributes)","cc1977f7":"%%time\n#train attributes from 102 to 203\ntrain_attributes=train_df.columns.values[102:203]\n#plot distribution of train attributes\nplot_train_attribute_distribution(t0,t1,'0','1',train_attributes)","9700b2f8":"#importing the test dataset\ntest_df=pd.read_csv('..\/input\/test.csv')\ntest_df.head()","e3781d3a":"#Shape of the test dataset\ntest_df.shape","8185b4a5":"%%time\n#Distribution of test attributes\ndef plot_test_attribute_distribution(test_attributes):\n    i=0\n    sns.set_style('whitegrid')\n    \n    fig=plt.figure()\n    ax=plt.subplots(10,10,figsize=(22,18))\n    \n    for attribute in test_attributes:\n        i+=1\n        plt.subplot(10,10,i)\n        sns.distplot(test_df[attribute],hist=False)\n        plt.xlabel('Attribute',)\n        sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n    plt.show()","34516e2e":"%%time\n#test attribiutes from 1 to 101\ntest_attributes=test_df.columns.values[1:101]\n#plot distribution of test attributes\nplot_test_attribute_distribution(test_attributes)","5c8627a9":"%%time\n#test attributes from 101 to 202\ntest_attributes=test_df.columns.values[101:202]\n#plot the distribution of test attributes\nplot_test_attribute_distribution(test_attributes)","1e27780e":"%%time\n#Distribution of mean values per column in train and test dataset\nplt.figure(figsize=(16,8))\n#train attributes\ntrain_attributes=train_df.columns.values[2:202]\n#test attributes\ntest_attributes=test_df.columns.values[1:201]\n#Distribution plot for mean values per column in train attributes\nsns.distplot(train_df[train_attributes].mean(axis=0),color='blue',kde=True,bins=150,label='train')\n#Distribution plot for mean values per column in test attributes\nsns.distplot(test_df[test_attributes].mean(axis=0),color='green',kde=True,bins=150,label='test')\nplt.title('Distribution of mean values per column in train and test dataset')\nplt.legend()\nplt.show()\n\n#Distribution of mean values per row in train and test dataset\nplt.figure(figsize=(16,8))\n#Distribution plot for mean values per row in train attributes\nsns.distplot(train_df[train_attributes].mean(axis=1),color='blue',kde=True,bins=150,label='train')\n#Distribution plot for mean values per row in test attributes\nsns.distplot(test_df[test_attributes].mean(axis=1),color='green',kde=True, bins=150, label='test')\nplt.title('Distribution of mean values per row in train and test dataset')\nplt.legend()\nplt.show()","b5f27387":"%%time\n#Distribution of std values per column in train and test dataset\nplt.figure(figsize=(16,8))\n#train attributes\ntrain_attributes=train_df.columns.values[2:202]\n#test attributes\ntest_attributes=test_df.columns.values[1:201]\n#Distribution plot for std values per column in train attributes\nsns.distplot(train_df[train_attributes].std(axis=0),color='red',kde=True,bins=150,label='train')\n#Distribution plot for std values per column in test attributes\nsns.distplot(test_df[test_attributes].std(axis=0),color='blue',kde=True,bins=150,label='test')\nplt.title('Distribution of std values per column in train and test dataset')\nplt.legend()\nplt.show()\n\n#Distribution of std values per row in train and test dataset\nplt.figure(figsize=(16,8))\n#Distribution plot for std values per row in train attributes\nsns.distplot(train_df[train_attributes].std(axis=1),color='red',kde=True,bins=150,label='train')\n#Distribution plot for std values per row in test attributes\nsns.distplot(test_df[test_attributes].std(axis=1),color='blue',kde=True, bins=150, label='test')\nplt.title('Distribution of std values per row in train and test dataset')\nplt.legend()\nplt.show()","b3e37a23":"%%time\n#Distribution of skew values per column in train and test dataset\nplt.figure(figsize=(16,8))\n#train attributes\ntrain_attributes=train_df.columns.values[2:202]\n#test attributes\ntest_attributes=test_df.columns.values[1:201]\n#Distribution plot for skew values per column in train attributes\nsns.distplot(train_df[train_attributes].skew(axis=0),color='green',kde=True,bins=150,label='train')\n#Distribution plot for skew values per column in test attributes\nsns.distplot(test_df[test_attributes].skew(axis=0),color='blue',kde=True,bins=150,label='test')\nplt.title('Distribution of skewness values per column in train and test dataset')\nplt.legend()\nplt.show()\n\n#Distribution of skew values per row in train and test dataset\nplt.figure(figsize=(16,8))\n#Distribution plot for skew values per row in train attributes\nsns.distplot(train_df[train_attributes].skew(axis=1),color='green',kde=True,bins=150,label='train')\n#Distribution plot for skew values per row in test attributes\nsns.distplot(test_df[test_attributes].skew(axis=1),color='blue',kde=True, bins=150, label='test')\nplt.title('Distribution of skewness values per row in train and test dataset')\nplt.legend()\nplt.show()","222642a1":"%%time\n#Distribution of kurtosis values per column in train and test dataset\nplt.figure(figsize=(16,8))\n#train attributes\ntrain_attributes=train_df.columns.values[2:202]\n#test attributes\ntest_attributes=test_df.columns.values[1:201]\n#Distribution plot for kurtosis values per column in train attributes\nsns.distplot(train_df[train_attributes].kurtosis(axis=0),color='blue',kde=True,bins=150,label='train')\n#Distribution plot for kurtosis values per column in test attributes\nsns.distplot(test_df[test_attributes].kurtosis(axis=0),color='red',kde=True,bins=150,label='test')\nplt.title('Distribution of kurtosis values per column in train and test dataset')\nplt.legend()\nplt.show()\n\n#Distribution of kutosis values per row in train and test dataset\nplt.figure(figsize=(16,8))\n#Distribution plot for kurtosis values per row in train attributes\nsns.distplot(train_df[train_attributes].kurtosis(axis=1),color='blue',kde=True,bins=150,label='train')\n#Distribution plot for kurtosis values per row in test attributes\nsns.distplot(test_df[test_attributes].kurtosis(axis=1),color='red',kde=True, bins=150, label='test')\nplt.title('Distribution of kurtosis values per row in train and test dataset')\nplt.legend()\nplt.show()","59c8031a":"%%time\n#Finding the missing values in train and test data\ntrain_missing=train_df.isnull().sum().sum()\ntest_missing=test_df.isnull().sum().sum()\nprint('Missing values in train data :',train_missing)\nprint('Missing values in test data :',test_missing)","12a1b98a":"%%time\n#Correlations in train attributes\ntrain_attributes=train_df.columns.values[2:202]\ntrain_correlations=train_df[train_attributes].corr().abs().unstack().sort_values(kind='quicksort').reset_index()\ntrain_correlations=train_correlations[train_correlations['level_0']!=train_correlations['level_1']]\nprint(train_correlations.head(10))\nprint(train_correlations.tail(10))","7aaebcea":"%%time\n#Correlations in test attributes\ntest_attributes=test_df.columns.values[1:201]\ntest_correlations=test_df[test_attributes].corr().abs().unstack().sort_values(kind='quicksort').reset_index()\ntest_correlations=test_correlations[test_correlations['level_0']!=test_correlations['level_1']]\nprint(test_correlations.head(10))\nprint(test_correlations.tail(10))","a7e9da93":"%%time\n#Correlations in train data\ntrain_correlations=train_df[train_attributes].corr()\ntrain_correlations=train_correlations.values.flatten()\ntrain_correlations=train_correlations[train_correlations!=1]\n#Correlations in test data\ntest_correlations=test_df[test_attributes].corr()\ntest_correlations=test_correlations.values.flatten()\ntest_correlations=test_correlations[test_correlations!=1]\n\nplt.figure(figsize=(20,5))\n#Distribution plot for correlations in train data\nsns.distplot(train_correlations, color=\"Red\", label=\"train\")\n#Distribution plot for correlations in test data\nsns.distplot(test_correlations, color=\"Blue\", label=\"test\")\nplt.xlabel(\"Correlation values found in train and test\")\nplt.ylabel(\"Density\")\nplt.title(\"Correlation distribution plot for train and test attributes\")\nplt.legend()","ea5d6d7b":"#training and testing data\nX=train_df.drop(columns=['ID_code','target'],axis=1)\ntest=test_df.drop(columns=['ID_code'],axis=1)\ny=train_df['target']","54c9b9e7":"#Split the training data\nX_train,X_valid,y_train,y_valid=train_test_split(X,y,random_state=42)\n\nprint('Shape of X_train :',X_train.shape)\nprint('Shape of X_valid :',X_valid.shape)\nprint('Shape of y_train :',y_train.shape)\nprint('Shape of y_valid :',y_valid.shape)","29f642e5":"%%time\n#Random forest classifier\nrf_model=RandomForestClassifier(n_estimators=10,random_state=42)\n#fitting the model\nrf_model.fit(X_train,y_train)","2769e13f":"%%time\n#Permutation importance\nfrom eli5.sklearn import PermutationImportance\nperm_imp=PermutationImportance(rf_model,random_state=42)\n#fitting the model\nperm_imp.fit(X_valid,y_valid)","cb0c48c2":"%%time\n#Important features\neli5.show_weights(perm_imp,feature_names=X_valid.columns.tolist(),top=200)","8fca06e1":"%%time\n#Create the data we will plot 'var_53'\nfeatures=[v for v in X_valid.columns if v not in ['ID_code','target']]\npdp_data=pdp.pdp_isolate(rf_model,dataset=X_valid,model_features=features,feature='var_53')\n#plot feature \"var_53\"\npdp.pdp_plot(pdp_data,'var_53')\nplt.show()","564e0ba3":"%%time\n#Create the data we will plot \npdp_data=pdp.pdp_isolate(rf_model,dataset=X_valid,model_features=features,feature='var_6')\n#plot feature \"var_6\"\npdp.pdp_plot(pdp_data,'var_6')\nplt.show()","4b1a7df2":"%%time\n#Create the data we will plot \npdp_data=pdp.pdp_isolate(rf_model,dataset=X_valid,model_features=features,feature='var_146')\n#plot feature \"var_146\"\npdp.pdp_plot(pdp_data,'var_146')\nplt.show()","ff612eaf":"#Training data\nX=train_df.drop(['ID_code','target'],axis=1)\nY=train_df['target']\n#StratifiedKFold cross validator\ncv=StratifiedKFold(n_splits=5,random_state=42,shuffle=True)\nfor train_index,valid_index in cv.split(X,Y):\n    X_train, X_valid=X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid=Y.iloc[train_index], Y.iloc[valid_index]\n\nprint('Shape of X_train :',X_train.shape)\nprint('Shape of X_valid :',X_valid.shape)\nprint('Shape of y_train :',y_train.shape)\nprint('Shape of y_valid :',y_valid.shape)","fa68a738":"%%time\n#Logistic regression model\nlr_model=LogisticRegression(random_state=42)\n#fitting the lr model\nlr_model.fit(X_train,y_train)","867bf9b6":"#Accuracy of the model\nlr_score=lr_model.score(X_train,y_train)\nprint('Accuracy of the lr_model :',lr_score)","50238bc3":"%%time\n#Cross validation prediction\ncv_predict=cross_val_predict(lr_model,X_valid,y_valid,cv=5)\n#Cross validation score\ncv_score=cross_val_score(lr_model,X_valid,y_valid,cv=5)\nprint('cross_val_score :',np.average(cv_score))","6f585f81":"#Confusion matrix\ncm=confusion_matrix(y_valid,cv_predict)\n#Plot the confusion matrix\nplot_confusion_matrix(y_valid,cv_predict,normalize=False,figsize=(15,8))","1bc1e5ca":"#ROC_AUC score\nroc_score=roc_auc_score(y_valid,cv_predict)\nprint('ROC score :',roc_score)\n\n#ROC_AUC curve\nplt.figure()\nfalse_positive_rate,recall,thresholds=roc_curve(y_valid,cv_predict)\nroc_auc=auc(false_positive_rate,recall)\nplt.title('Reciver Operating Characteristics(ROC)')\nplt.plot(false_positive_rate,recall,'b',label='ROC(area=%0.3f)' %roc_auc)\nplt.legend()\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.ylabel('Recall(True Positive Rate)')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint('AUC:',roc_auc)","1154fc6c":"#Classification report\nscores=classification_report(y_valid,cv_predict)\nprint(scores)","843b9a6d":"%%time\n#Predicting the model\nX_test=test_df.drop(['ID_code'],axis=1)\nlr_pred=lr_model.predict(X_test)\nprint(lr_pred)","1b0c30ea":"%%time\nfrom imblearn.over_sampling import SMOTE\n#Synthetic Minority Oversampling Technique\nsm = SMOTE(random_state=42, ratio=1.0)\n#Generating synthetic data points\nX_smote,y_smote=sm.fit_sample(X_train,y_train)\nX_smote_v,y_smote_v=sm.fit_sample(X_valid,y_valid)","9ef5499b":"%%time\n#Logistic regression model for SMOTE\nsmote=LogisticRegression(random_state=42)\n#fitting the smote model\nsmote.fit(X_smote,y_smote)","17f643c6":"#Accuracy of the model\nsmote_score=smote.score(X_smote,y_smote)\nprint('Accuracy of the smote_model :',smote_score)","193399ad":"%%time\n#Cross validation prediction\ncv_pred=cross_val_predict(smote,X_smote_v,y_smote_v,cv=5)\n#Cross validation score\ncv_score=cross_val_score(smote,X_smote_v,y_smote_v,cv=5)\nprint('cross_val_score :',np.average(cv_score))","456b2a5c":"#Confusion matrix\ncm=confusion_matrix(y_smote_v,cv_pred)\n#Plot the confusion matrix\nplot_confusion_matrix(y_smote_v,cv_pred,normalize=False,figsize=(15,8))","ad3fcb5c":"#ROC_AUC score\nroc_score=roc_auc_score(y_smote_v,cv_pred)\nprint('ROC score :',roc_score)\n\n#ROC_AUC curve\nplt.figure()\nfalse_positive_rate,recall,thresholds=roc_curve(y_smote_v,cv_pred)\nroc_auc=auc(false_positive_rate,recall)\nplt.title('Reciver Operating Characteristics(ROC)')\nplt.plot(false_positive_rate,recall,'b',label='ROC(area=%0.3f)' %roc_auc)\nplt.legend()\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.ylabel('Recall(True Positive Rate)')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint('AUC:',roc_auc)","82aabff4":"#Classification report\nscores=classification_report(y_smote_v,cv_pred)\nprint(scores)","be68488f":"%%time\n#Predicting the model\nX_test=test_df.drop(['ID_code'],axis=1)\nsmote_pred=smote.predict(X_test)\nprint(smote_pred)","e55f122a":"#Training the model\n#training data\nlgb_train=lgb.Dataset(X_train,label=y_train)\n#validation data\nlgb_valid=lgb.Dataset(X_valid,label=y_valid)","c884196a":"#Selecting best hyperparameters by tuning of different parameters\nparams={'boosting_type': 'gbdt', \n          'max_depth' : -1, #no limit for max_depth if <0\n          'objective': 'binary',\n          'boost_from_average':False, \n          'nthread': 20,\n          'metric':'auc',\n          'num_leaves': 50,\n          'learning_rate': 0.01,\n          'max_bin': 100,      #default 255\n          'subsample_for_bin': 100,\n          'subsample': 1,\n          'subsample_freq': 1,\n          'colsample_bytree': 0.8,\n          'bagging_fraction':0.5,\n          'bagging_freq':5,\n          'feature_fraction':0.08,\n          'min_split_gain': 0.45, #>0\n          'min_child_weight': 1,\n          'min_child_samples': 5,\n          'is_unbalance':True,\n          }","25a7edeb":"num_rounds=10000\nlgbm= lgb.train(params,lgb_train,num_rounds,valid_sets=[lgb_train,lgb_valid],verbose_eval=1000,early_stopping_rounds = 5000)\nlgbm","727eecbd":"X_test=test_df.drop(['ID_code'],axis=1)\n#predict the model\n#probability predictions\nlgbm_predict_prob=lgbm.predict(X_test,random_state=42,num_iteration=lgbm.best_iteration)\n#Convert to binary output 1 or 0\nlgbm_predict=np.where(lgbm_predict_prob>=0.5,1,0)\nprint(lgbm_predict_prob)\nprint(lgbm_predict)","00e58130":"#plot the important features\nlgb.plot_importance(lgbm,max_num_features=50,importance_type=\"split\",figsize=(20,50))","0a6ff5c4":"#final submission\nsub_df=pd.DataFrame({'ID_code':test_df['ID_code'].values})\nsub_df['lgbm_predict_prob']=lgbm_predict_prob\nsub_df['lgbm_predict']=lgbm_predict\nsub_df.to_csv('submission.csv',index=False)\nsub_df.head()","51fb08c1":"**Correlation between the attributes**","e957a7ce":"Cross validation prediction of smoth_model","084b6c35":"**Let us look distribution of kurtosis values per rows and columns in train and test dataset**","40239c84":"**Let us look distribution of mean values per rows and columns in train and test dataset**","f4067da9":"**Confusion matrix**","cae06b13":"**lgbm model performance on test data**","92bdcd57":"**choosing of  hyperparameters**","f8484270":"**LightGBM:**\n\nLightGBM is a gradient boosting framework that uses tree based learning algorithms. We are going to use LightGBM model.\n","7adeb4e3":"Take aways:\n* Importance of the features decreases as we move down the top of the column.\n* As we can see the features shown in green indicate that they have a positive impact on our prediction\n* As we can see the features shown in white indicate that they have no effect on our prediction\n* As we can see the features shown in red indicate that they have a negative impact on our prediction\n* The most important feature is 'Var_81'","b70d7309":"Let us see important features,","fcc8cd9f":"Let us see first 100 train attributes will be displayed in next cell.","63151dd6":"**Take aways:**\n* The y_axis does not show the predictor value instead how the value changing with  the change in given predictor variable. \n* The blue shaded area indicates the level of confidence of 'var_53'.\n* On y-axis having a positive value means for that particular value of predictor variable it is less likely to predict the correct class and having a positive value means it has positive impact on predicting the correct class.\n","f48d11a2":"****","584b69a7":"**Correlation plot for train and test data**","b852ef4f":"**Shape of the train dataset**","796eda22":"Both Oversampling and undersampling techniques have some drawbacks. So, we are not going to use this models for this problem and also we will use other best algorithms.","3ed12af1":"**Oversample minority class:**\n* It can be defined as adding more copies of minority class.\n* It can be a good choice when we don't have a ton of data to work with.\n* Drawback is that we are adding information.This may leads to overfitting and poor performance on test data.\n","601a14b5":"**Reciever operating characteristics (ROC)-Area under curve(AUC) score and curve**","ccf63ef4":"Let us see next 100 train attributes will be displayed in next cell.","91c21311":"**Take aways:**\n* The y_axis does not show the predictor value instead how the value changing with  the change in given predictor variable. \n* The blue shaded area indicates the level of confidence of 'var_146'.\n* On y-axis having a positive value means for that particular value of predictor variable it is less likely to predict the correct class and having a positive value means it has positive impact on predicting the correct class.\n","9863fbd9":"Let us see next 100 test attributes will be displayed in next cell.","61a463d2":"**Split the train data using StratefiedKFold cross validator**","988ff7b0":"**Partial dependence plots**","40903a50":"**Let us look distribution of skewness per rows and columns in train and test dataset**","39ba8873":"We can observed that smote model is performing well on imbalance data compare to logistic regression.","12d29dfd":"**Target classes count**","0cc01d00":"**Importing the train dataset**","a7515c9f":"**Cross validation prediction of lr_model**","30c87e2c":"Permutation variable importance measure in a random forest for classification and regression.","bf97ade3":"**Partial dependence plot**","85e2d013":"When we compare the roc_auc_score and model accuracy , model is not performing well on imbalanced data.","b9889f6f":"Partial dependence plot gives a graphical depiction of the marginal effect of a variable on the class probability or classification.While feature importance shows what variables most affect predictions, but partial dependence plots show how a feature affects predictions.","d7de12a0":"Let us do some feature engineering by using\n* Permutation importance\n* Partial dependence plots","9569b4d8":"**Conclusion :**\n\nWe tried model with logistic regression,smote and lightgbm. But lightgbm model is performing well on imbalanced data compared to other models based on scores of roc_auc_score.","0048d29a":"**Importing the test dataset**","fecf856d":"**Training the lgbm model**","55320197":"No missing values are present in both train and test data.","c858b712":"**Problem statement :-**\n\nIn this challenge, we need to identify which customers will make a specific transaction in\nthe future, irrespective of the amount of money transacted.\n","12db1809":"**Take aways:**\n* We can observed that their is a considerable number of features which are significantly have different distributions. \n  For example like var_0,var_1,var_9,var_180 var_198 etc.\n* We can observed that their is a considerable number of features which are significantly have same distributions. \n  For example like var_3,var_7,var_10,var_171,var_185,var_192 etc.\n","f6a501e0":"We can observed that the correlation between the train attributes is very small.","01546977":"We can observed that f1 score is high for number of customers those who will not make a transaction then the who will make a transaction. So, we are going to change the algorithm.","f6527d63":"**Undersample majority class:**\n* It can be defined as removing some observations of the majority class.\n* It can be a good choice when we have a ton of data -think million of rows.\n* Drawback is that we are removing information that may be valuable.This may leads to underfitting and poor performance on test data.","85588924":"Let us see how baseline logistic regression model performs on synthetic data points.","c82976b1":"**Logistic Regression model**","73a24be7":"Summary of the dataset","e13bda08":"**Model performance on test data**","9b639240":"**Let us look distribution of standard deviation(std) values per rows and columns in train and test dataset**","24cf81b6":"**Let us plot the important features**","e8123e11":"**Permutation importance**","67961155":"**Handling of imbalanced data**\n\nNow we are going to explore 5 different approaches for dealing with imbalanced datasets.\n* Change the performance metric\n* Oversample minority class\n* Undersample majority class\n* Synthetic Minority Oversampling Technique(SMOTE)\n* Change the algorithm","77df3c39":"**Model performance on test data**","4c71fe39":"**Confusion matrix**","e413bba6":"Accuracy of the model is not the best metric to use when evaluating the imbalanced datasets as it may be misleading. So, we are going to change the performance metric.","793f5626":"Let us see first 100 test attributes will be displayed in next cell.","66664e4f":"**Take aways:**                   \n* We have a unbalanced data,where 90% of the data is the number of customers those will not make a transaction and 10% of the data is those who will make a transaction.\n* Look at the violin plots seems that there are no relationship between the target with the index of the train dataframe.This is more dominated by the zero targets then for the ones.\n* Look at the jitter plots with violin plots. We can observed that targets looks uniformly distributed over the indexs of the dataframe.","3ef16eaa":"**Classification report**","ca9cfacb":"Let us calculate partial dependence plots on random forest","4da8051d":"**Accuracy of model**","661a15c9":"Let us build LightGBM model","971c0420":"**Accuracy of model**","d33232f4":"**Reciever operating characteristics (ROC)-Area under curve(AUC) score and curve**","f9113292":"**Take aways:**\n* We can observed that their is a considerable number of features which are significantly have different distributions for two target variables. For example like var_0,var_1,var_9,var_198 var_180 etc.\n*  We can observed that their is a considerable number of features which are significantly have same distributions for two target variables. For example like var_3,var_7,var_10,var_171,var_185 etc.\n","39800ff5":"**Shape of the test dataset**","2c66ed38":"**Let us look distribution of train attributes**","1fcfc118":"Let us calculate weights and show important features using eli5 library.","70851b26":"**Classification report**","229ec25e":"**Synthetic Minority Oversampling Technique(SMOTE)**","4e93587e":"We can observed that the correlation between the test attributes is very small.","149dbd94":"**Feature engineering**","730a22eb":"Let us build simple model to find features which are more important.","3f390c78":"**Missing value analysis**","218479c7":"**Let us look distribution of test attributes**","f304c0d0":"**Take aways:**\n* The y_axis does not show the predictor value instead how the value changing with  the change in given predictor variable. \n* The blue shaded area indicates the level of confidence of 'var_6'.\n* On y-axis having a positive value means for that particular value of predictor variable it is less likely to predict the correct class and having a positive value means it has positive impact on predicting the correct class.\n","859fd300":"Let us see impact of the main features which are discovered in the previous section by using the pdpbox.","eacf04c5":"Now let us start with simple Logistic regression model.","b7583d08":"**Random forest classifier**","d05521e6":"We can observed from correlation distribution plot that the correlation between the train and test attributes is very very small, it means that features are independent each other.","c7d82ad7":"**Project title :- Santander customer transaction prediction using Python**","71c92ea8":"SMOTE uses a nearest neighbors algorithm to generate new and synthetic data to used for training the model.","0eaa2ee9":"**Contents:**\n\n 1. Exploratory Data Analysis\n           * Loading dataset and libraries\n           * Data cleaning\n           * Typecasting the attributes\n           * Target classes count        \n           * Missing value analysis\n        2. Attributes Distributions and trends\n           * Distribution of train attributes\n           * Distribution of test attributes\n           * Mean distribution of attributes\n           * Standard deviation distribution of attributes\n           * Skewness distribution of attributes\n           * Kurtosis distribution of attributes      \n           * Outliers analysis\n        4. Correlation matrix \n        5. Split the dataset into train and test dataset\n        7. Modelling the training dataset\n           * Logistic Regression Model\n           * SMOTE Model\n           * LightGBM Model\n        8. Cross Validation Prediction\n           * Logistic  Regression CV Prediction\n           * SMOTE CV Prediction\n           * LightGBM CV Prediction\n        9. Model performance on test dataset\n           * Logistic Regression Prediction\n           * SMOTE Prediction\n           * LightGBM Prediction\n        10. Model Evaluation Metrics\n           * Confusion Matrix\n           * ROC_AUC score\n        11. Choosing best model for predicting customer transaction"}}