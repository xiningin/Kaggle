{"cell_type":{"346b949d":"code","34f9aa63":"code","88fea99f":"code","b038158b":"code","e4a86839":"code","3375224b":"code","b1f5e215":"code","a789396e":"code","a8846cc1":"code","ac9c581c":"code","b87b34af":"code","5215bc10":"code","871ee242":"code","33793432":"code","074065f4":"code","eeb8f39a":"code","669ca758":"code","0dad4502":"code","aa195f84":"code","1f1ce24d":"code","8eab38b0":"code","d1383519":"code","c4f53634":"code","f25d3f6b":"code","bac24210":"markdown","1387e72a":"markdown","5db7a6d0":"markdown","32a91cac":"markdown","b1ac6fa2":"markdown","1dfd6581":"markdown","59d5d28e":"markdown","40eb8a2b":"markdown","ea79f754":"markdown","d6eb54d9":"markdown","f24493b4":"markdown","157319e8":"markdown","bf31084a":"markdown","4d52a503":"markdown","8fa0720e":"markdown","eeeab95d":"markdown","bd7130bf":"markdown","aa8ef18c":"markdown","011a7f32":"markdown","9ae8d77c":"markdown","5aebfafb":"markdown","1f0e17ca":"markdown","a5eecfbc":"markdown","dd5d146a":"markdown","e805ba52":"markdown"},"source":{"346b949d":"import pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt \nimport warnings\nwarnings.filterwarnings('ignore')","34f9aa63":"data= pd.read_csv('..\/input\/body-measurements-dataset\/Body Measurements _ original_CSV.csv')","88fea99f":"data.head().T","b038158b":"data.info()","e4a86839":"def count_missing_value(data):\n    null_count = data.isnull().sum()\n    nan_count = ((data== 'nan') | (data=='NaN') | (data=='Nan')).sum()\n    empty_count = ((data== ' ') | (data=='')).sum()\n    abc = pd.DataFrame({\n        'null_count' : null_count,\n        'nan_count' : nan_count,\n        'empty_count' : empty_count\n     })\n    \n    return abc\ncount_missing_value(data)","3375224b":"location= data.loc[data.Gender.isnull()]    \nprint('The empty row is : \\n')\nlocation","b1f5e215":"data2= data.drop(labels=[536] ,axis=0)     \ndata2.info()","a789396e":"count_missing_value(data2)","a8846cc1":"data2['Gender'].value_counts()","ac9c581c":"data2.describe().T","b87b34af":"label = ['male', 'female']\nfig= plt.subplots(figsize=(10,8))\nplt.pie(data2.Gender.value_counts(), labels= label, shadow= True)\nplt.show()","5215bc10":"female= data2[data2['Gender']==2]\nmale= data2[data2['Gender']==1]","871ee242":"f_ShoulderWidth_mean = round(female['ShoulderWidth'].mean(), 0)\nm_ShoulderWidth_mean = round(male['ShoulderWidth'].mean(), 0)\n\nf_TotalHeight_mean = round(female['TotalHeight'].mean(), 0)\nm_TotalHeight_mean = round(male['TotalHeight'].mean(), 0)\n\nf_above_avg_height = female[female['TotalHeight']>=f_TotalHeight_mean].TotalHeight\nf_below_avg_height = female[female['TotalHeight']<f_TotalHeight_mean].TotalHeight\nm_above_avg_height = male[male['TotalHeight']>=m_TotalHeight_mean].TotalHeight\nm_below_avg_height = male[male['TotalHeight']<m_TotalHeight_mean].TotalHeight","33793432":"plt.bar('Female', f_ShoulderWidth_mean,bottom=None)\nplt.bar('Male', m_ShoulderWidth_mean)\nplt.title('MALE\/FEMALE AVERAGE SHOULDER WIDTH')\nplt.ylim(0,16)\nplt.show()","074065f4":"fig,(ax1,ax2)= plt.subplots(1,2, figsize=(10,4))\n\nsns.distplot(f_above_avg_height, label='Above Average', kde= False, bins= 25, ax=ax1,color='r')\nsns.distplot(f_below_avg_height, label='Below Average', kde= False, bins= 25, ax=ax1,color='b')\nsns.distplot(m_above_avg_height, label='Above Average', kde= False, bins= 25, ax= ax2, color='r')\nsns.distplot(m_below_avg_height, label='Below Average', kde= False, bins= 25, ax= ax2, color='b')\n\nax1.set(title='ABOVE\/BELOW AVERAGE HEIGHT OF FEMALES', xlabel= \"Height\")\nax1.legend()\nax2.set(title='ABOVE\/BELOW AVERAGE HEIGHT OF MALES', xlabel= \"Height\")\nax2.legend()\n\nplt.tight_layout()\nplt.show()","eeb8f39a":"plt.figure(figsize=(12, 10))\nsns.heatmap(data2.corr().abs(), annot= True)\nplt.show()","669ca758":"x= data2.drop(['Gender', 'Age', 'Belly '], axis=1)\ny=data2['Age']","0dad4502":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test= train_test_split(x,y, test_size= 0.3, random_state=0)","aa195f84":"knn_scores= []\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, mean_squared_error\n\nfor i in np.arange(1,15,3):\n    \n    #knn classifier\n    c_knn = KNeighborsClassifier(n_neighbors=i, weights='distance')\n    c_train_knn = c_knn.fit(x_train, y_train)\n    c_knn_ypred = c_train_knn.predict(x_test)\n    c_accuracy = accuracy_score(y_test, c_knn_ypred)\n    c_accuracy_percentage = c_accuracy * 100 \n    \n    #knn regressor\n    r_knn= KNeighborsRegressor(n_neighbors=i)\n    r_train_knn= r_knn.fit(x_train, y_train)\n    r_knn_ypred = r_train_knn.predict(x_test)\n    r_accuracy= mean_squared_error(y_test, r_knn_ypred)\n    \n    knn_scores.append(pd.Series({'n_neighbour' : i,\n                                'knn regressor' : r_accuracy,\n                                'knn classifier' : c_accuracy_percentage\n                                }))\nknn_score = pd.concat(knn_scores, axis=1).T.set_index('n_neighbour')    ","1f1ce24d":"knn_score","8eab38b0":"fig, ax = plt.subplots(figsize=(10,12))\nax.plot(knn_score.index, knn_score['knn regressor'], label= 'Knn Regressor', marker= '^', ls= '--')\nax.plot(knn_score.index, knn_score['knn classifier'], label= 'Knn Classifier', marker= '*', ls= '--')\nax.legend()\nax.set(title='ELBOW CURVE' , xlim=(0,15), ylim=(0,55), xlabel= 'Neighbours', ylabel= 'Accuracy score')\nplt.show()","d1383519":"rf_scores= []\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\n\nfor i in [1,10,25,50,75,90,100,150,250,300,400,500]:\n    for j in np.arange(1,5,1):\n        #random forest regressor\n        r_rf= RandomForestRegressor(n_estimators=i, criterion='mse', max_depth=j)\n        r_rf_train = r_rf.fit(x_train,y_train)\n        r_rf_ypred = r_rf_train.predict(x_test)\n        r_rf_score = round(mean_squared_error(y_test, r_rf_ypred),2)\n\n        #random forest classifier\n        c_rf= RandomForestClassifier(n_estimators=i, criterion='entropy', max_depth=j)\n        c_rf_train = c_rf.fit(x_train,y_train)\n        c_rf_ypred = c_rf_train.predict(x_test)\n        c_rf_score = accuracy_score(y_test, c_rf_ypred)\n        c_rf_score_percentage = round(c_rf_score*100,2)\n\n        rf_scores.append(pd.Series({'n_estimator' : i,\n                                    'max_depth' : j,\n                                    'rf_regression_score': r_rf_score,\n                                    'rf classification score': c_rf_score_percentage\n                                   }))","c4f53634":"rf_score = pd.concat(rf_scores, axis=1).T.set_index(['n_estimator','max_depth'])\nrf_score","f25d3f6b":"print('BEST RESULT OF knn regressor : ',round(knn_score['knn regressor'].max(),2))\nprint('BEST RESULT OF knn classifier : ',round(knn_score['knn classifier'].max(),2))\nprint('BEST RESULT OF rf_regression_score : ', rf_score['rf_regression_score'].max())\nprint('BEST RESULT OF rf classification score : ', rf_score['rf classification score'].max())","bac24210":"### Re-checking for missing values.","1387e72a":"Here, we can see RF REGRESSOR is far better than RF CLASSIFIER.","5db7a6d0":"The 'GENDER' column has only one null value, so, we can remove it.\n\nFirst, we'll try to fine the row number of the Null Value.","32a91cac":"From the above distribution plot, we can interpret that more than 50% of the Males & Females have ABOVE average height. \n\nOne thing to note is that, maximum number of Males\/Females are in the VICINITY OF THE AVERAGE HEIGHT.","b1ac6fa2":"### Making variables required for plotting. ","1dfd6581":"## PLOT 3","59d5d28e":"To get the best parameters, we can use GridSearchCV because it has in-built functions for it.\n\nIf you have any other options to find the best parameters please let me know. I'll really appreciate it!","40eb8a2b":"# AIM: EDA ALONG WITH PREDICTING THE AGE.  ","ea79f754":"# IF YOU GAINED ANYTHING FROM THE CODE, DO UPVOTE IT!\n\n# THANKS!","d6eb54d9":"## EXPLORING THE DATA","f24493b4":"###  Finding the location of empty value","157319e8":"### a) KNN","bf31084a":"## PLOT 1","4d52a503":"## Dealing with missing data","8fa0720e":"Among all the 4 algorithms used in this code, RANDOM FOREST REGRESSOR gives the best results with an accuracy of 91.67.","eeeab95d":"# CONCLUSION","bd7130bf":"## PLOT 4","aa8ef18c":"Removing the less correlated columns to make a cleaner dataset for a precise prediction.","011a7f32":"### Removing the row with empty value.","9ae8d77c":"Neither of them gives a satisfactory results.\n\nBut, Regressor performs better than Classifier.","5aebfafb":"## REGRESSOR vs CLASSIFIER","1f0e17ca":"The empty value has been removed from the data.\n","a5eecfbc":"## PLOT 2","dd5d146a":"## DATA VISUALISATION","e805ba52":"### b) RANDOM FOREST "}}