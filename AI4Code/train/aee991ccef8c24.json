{"cell_type":{"afd46fc4":"code","91d8e87f":"code","4a4341b0":"code","11b742d4":"code","a1e7d283":"code","ad1feb09":"code","db8fe28d":"code","af32843e":"code","6b951e14":"code","7a7d1b55":"code","5d457bae":"code","a68187a4":"code","0a98a321":"code","32751e65":"code","6c0b694e":"code","0c2b1075":"code","763d1cb3":"code","cf84ff99":"code","87adc849":"code","a147b1e7":"code","c76cd84a":"code","a11e87f8":"code","7222cc2c":"code","36b70aa2":"code","fafbe3b3":"code","d14edf28":"code","761160a9":"code","5d5a0514":"code","3daefbf8":"markdown","4bc6d6ef":"markdown","f9d196a6":"markdown","7014cc48":"markdown","2c97e04f":"markdown","f5e18d6b":"markdown","d820a3b3":"markdown","7a00d1fe":"markdown","eb7517b4":"markdown","8b99af78":"markdown","aeb19838":"markdown","a463dd9d":"markdown","86e0feb3":"markdown","3a03f73e":"markdown","a6c42a5a":"markdown","74c0070c":"markdown"},"source":{"afd46fc4":"import itertools\nimport warnings\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.metrics import mean_absolute_error\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport statsmodels.tsa.api as smt\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n\nwarnings.filterwarnings('ignore')","91d8e87f":"data = sm.datasets.co2.load_pandas()\ny = data.data\ny.index.sort_values()\n\ny = y['co2'].resample('MS').mean()","4a4341b0":"y = y.fillna(y.bfill())\ny.head()\n\ny.plot(figsize=(15, 6))\nplt.show()","11b742d4":"# Train set.\ntrain = y[:'1997-12-01']\nlen(train)  # 478 months\n\n# Test set.\ntest = y['1998-01-01':]\nlen(test)  # 48 moths","a1e7d283":"def is_stationary(y):\n\n    # \"HO: Non-stationary\"\n    # \"H1: Stationary\"\n\n    p_value = sm.tsa.stattools.adfuller(y)[1]\n    if p_value < 0.05:\n        print(F\"Result: Stationary (H0: non-stationary, p-value: {round(p_value, 3)})\")\n    else:\n        print(F\"Result: Non-Stationary (H0: non-stationary, p-value: {round(p_value, 3)})\")\n\n\nis_stationary(y)","ad1feb09":"def ts_decompose(y, model=\"additive\", stationary=False):\n    result = seasonal_decompose(y, model=model)\n    fig, axes = plt.subplots(4, 1, sharex=True, sharey=False)\n    fig.set_figheight(10)\n    fig.set_figwidth(15)\n\n    axes[0].set_title(\"Decomposition for \" + model + \" model\")\n    axes[0].plot(y, 'k', label='Original ' + model)\n    axes[0].legend(loc='upper left')\n\n    axes[1].plot(result.trend, label='Trend')\n    axes[1].legend(loc='upper left')\n\n    axes[2].plot(result.seasonal, 'g', label='Seasonality & Mean: ' + str(round(result.seasonal.mean(), 4)))\n    axes[2].legend(loc='upper left')\n\n    axes[3].plot(result.resid, 'r', label='Residuals & Mean: ' + str(round(result.resid.mean(), 4)))\n    axes[3].legend(loc='upper left')\n    plt.show(block=True)\n\n    if stationary:\n        is_stationary(y)\n        \nfor model in [\"additive\", \"multiplicative\"]:\n    ts_decompose(y, model, stationary=True)","db8fe28d":"ses_model = SimpleExpSmoothing(train).fit(smoothing_level=0.5)\n\ny_pred = ses_model.forecast(48)\n\nmean_absolute_error(test, y_pred)\n\n\ntrain.plot(title=\"Single Exponential Smoothing\")\ntest.plot()\ny_pred.plot()\nplt.show()","af32843e":"train[\"1985\":].plot(title=\"Single Exponential Smoothing\")\ntest.plot()\ny_pred.plot()\nplt.show()","6b951e14":"def plot_co2(train, test, y_pred, title):\n    mae = mean_absolute_error(test, y_pred)\n    train[\"1985\":].plot(legend=True, label=\"TRAIN\", title=f\"{title}, MAE: {round(mae,2)}\")\n    test.plot(legend=True, label=\"TEST\", figsize=(6, 4))\n    y_pred.plot(legend=True, label=\"PREDICTION\")\n    plt.show()\n\n\nplot_co2(train, test, y_pred, \"Single Exponential Smoothing\")","7a7d1b55":"############################\n# Hyperparameter Optimization\n############################\n\n\ndef ses_optimizer(train, alphas, step=48):\n    best_alpha, best_mae = None, float(\"inf\")\n\n    for alpha in alphas:\n        ses_model = SimpleExpSmoothing(train).fit(smoothing_level=alpha)\n        y_pred = ses_model.forecast(step)\n        mae = mean_absolute_error(test, y_pred)\n\n        if mae < best_mae:\n            best_alpha, best_mae = alpha, mae\n\n        print(\"alpha:\", round(alpha, 2), \"mae:\", round(mae, 4))\n    print(\"best_alpha:\", round(best_alpha, 2), \"best_mae:\", round(best_mae, 4))\n    return best_alpha, best_mae\n\nalphas = np.arange(0.8, 1, 0.01)\nses_optimizer(train, alphas)\n\nbest_alpha, best_mae = ses_optimizer(train, alphas)","5d457bae":"############################\n# Final SES Model\n############################\n\nses_model = SimpleExpSmoothing(train).fit(smoothing_level=best_alpha)\ny_pred = ses_model.forecast(48)\n\nplot_co2(train, test, y_pred, \"Single Exponential Smoothing\")\nmean_absolute_error(test, y_pred)","a68187a4":"##################################################\n# Double Exponential Smoothing (DES)\n##################################################\n\n# DES: Level (SES) + Trend\n\ndes_model = ExponentialSmoothing(train, trend=\"add\").fit(smoothing_level=0.5,\n                                                         smoothing_trend=0.5)\n\ny_pred = des_model.forecast(48)\n\nplot_co2(train, test, y_pred, \"Double Exponential Smoothing\")\n\ndes_model.params","0a98a321":"############################\n# Hyperparameter Optimization\n############################\n\n\ndef des_optimizer(train, alphas, betas, step=48):\n    best_alpha, best_beta, best_mae = None, None, float(\"inf\")\n    for alpha in alphas:\n        for beta in betas:\n            des_model = ExponentialSmoothing(train, trend=\"add\").fit(smoothing_level=alpha,\n                                                                     smoothing_slope=beta)\n            y_pred = des_model.forecast(step)\n            mae = mean_absolute_error(test, y_pred)\n            if mae < best_mae:\n                best_alpha, best_beta, best_mae = alpha, beta, mae\n            print(\"alpha:\", round(alpha, 2), \"beta:\", round(beta, 2), \"mae:\", round(mae, 4))\n    print(\"best_alpha:\", round(best_alpha, 2), \"best_beta:\", round(best_beta, 2), \"best_mae:\", round(best_mae, 4))\n    return best_alpha, best_beta, best_mae\n\n\nalphas = np.arange(0.01, 1, 0.10)\nbetas = np.arange(0.01, 1, 0.10)\n\nbest_alpha, best_beta, best_mae = des_optimizer(train, alphas, betas)\n","32751e65":"############################\n# Final DES Model\n############################\n\nfinal_des_model = ExponentialSmoothing(train, trend=\"add\").fit(smoothing_level=best_alpha,\n                                                               smoothing_slope=best_beta)\n\ny_pred = final_des_model.forecast(48)\n\nplot_co2(train, test, y_pred, \"Double Exponential Smoothing\")","6c0b694e":"##################################################\n# Triple Exponent\u0131al Smoothing (Holt-Winters)\n##################################################\n\n\n# TES = SES + DES + Mevsimsellik\n\ntes_model = ExponentialSmoothing(train,\n                                 trend=\"add\",\n                                 seasonal=\"add\",\n                                 seasonal_periods=12).fit(smoothing_level=0.5,\n                                                          smoothing_slope=0.5,\n                                                          smoothing_seasonal=0.5)\n\n\ny_pred = tes_model.forecast(48)\n\nplot_co2(train, test, y_pred, \"Triple Exponential Smoothing\")","0c2b1075":"############################\n# Hyperparameter Optimization\n############################\n\ndef tes_optimizer(train, abg, step=48):\n    best_alpha, best_beta, best_gamma, best_mae = None, None, None, float(\"inf\")\n\n    for comb in abg:\n        tes_model = ExponentialSmoothing(train, trend=\"add\", seasonal=\"add\", seasonal_periods=12).\\\n            fit(smoothing_level=comb[0], smoothing_slope=comb[1], smoothing_seasonal=comb[2])\n        y_pred = tes_model.forecast(step)\n        mae = mean_absolute_error(test, y_pred)\n        if mae < best_mae:\n            best_alpha, best_beta, best_gamma, best_mae = comb[0], comb[1], comb[2], mae\n        print([round(comb[0], 2), round(comb[1], 2), round(comb[2], 2), round(mae, 2)])\n\n    print(\"best_alpha:\", round(best_alpha, 2), \"best_beta:\", round(best_beta, 2), \"best_gamma:\", round(best_gamma, 2),\n          \"best_mae:\", round(best_mae, 4))\n\n    return best_alpha, best_beta, best_gamma, best_mae\n\n\nalphas = betas = gammas = np.arange(0.10, 1, 0.20)\nabg = list(itertools.product(alphas, betas, gammas))\n\n\nbest_alpha, best_beta, best_gamma, best_mae = tes_optimizer(train, abg)\n\n","763d1cb3":"############################\n# Final TES Model\n############################\n\nfinal_tes_model = ExponentialSmoothing(train, trend=\"add\", seasonal=\"add\", seasonal_periods=12).\\\n            fit(smoothing_level=best_alpha, smoothing_slope=best_beta, smoothing_seasonal=best_gamma)\n\ny_pred = final_tes_model.forecast(48)\n\nplot_co2(train, test, y_pred, \"Triple Exponential Smoothing\")","cf84ff99":"from matplotlib import pyplot\nfrom statsmodels.graphics.tsaplots import plot_acf\nplot_acf(y, lags=31)\npyplot.show()","87adc849":"##################################################\n# ARIMA(p, d, q): (Autoregressive Integrated Moving Average)\n##################################################\n\n\narima_model = ARIMA(train, order=(1, 1, 1)).fit(disp=0)\narima_model.summary()\n\ny_pred = arima_model.forecast(48)[0]\ny_pred = pd.Series(y_pred, index=test.index)\n\n\nplot_co2(train, test, y_pred, \"ARIMA\")\n\narima_model.plot_predict(dynamic=False)\nplt.show()","a147b1e7":"############################\n# Hyperparameter Optimization (Model Derecelerini Belirleme)\n############################\n\n#1. Determining Model Grade Based on AIC Statistics\n#2. Determining Model Grade Based on ACF & PACF Charts\n\n############################\n# Determining Model Grade Based on AIC & BIC Statistics\n############################\n\n# producing p and q combinations\np = d = q = range(0, 4)\npdq = list(itertools.product(p, d, q))\n\ndef arima_optimizer_aic(train, orders):\n    best_aic, best_params = float(\"inf\"), None\n    for order in orders:\n        try:\n            arma_model_result = ARIMA(train, order).fit(disp=0)\n            aic = arma_model_result.aic\n            if aic < best_aic:\n                best_aic, best_params = aic, order\n            print('ARIMA%s AIC=%.2f' % (order, aic))\n        except:\n            continue\n    print('Best ARIMA%s AIC=%.2f' % (best_params, best_aic))\n    return best_params\n\n\nbest_params_aic = arima_optimizer_aic(train, pdq)\n","c76cd84a":"############################\n# Final Model\n############################\n\narima_model = ARIMA(train, best_params_aic).fit(disp=0)\ny_pred = arima_model.forecast(48)[0]\ny_pred = pd.Series(y_pred, index=test.index)\n\nplot_co2(train, test, y_pred, \"ARIMA\")","a11e87f8":"############################\n# Determining Model Grade Based on ACF & PACF Charts\n############################\n\ndef acf_pacf(y, lags=30):\n    plt.figure(figsize=(12, 7))\n    layout = (2, 2)\n    ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n    acf_ax = plt.subplot2grid(layout, (1, 0))\n    pacf_ax = plt.subplot2grid(layout, (1, 1))\n    y.plot(ax=ts_ax)\n\n    # Stationarity test (HO: Series is not Stationary. H1: Series is Stationary.)\n    p_value = sm.tsa.stattools.adfuller(y)[1]\n    ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n    plt.tight_layout()\n    plt.show()\n\nacf_pacf(y)","7222cc2c":"######################\n# Determining Model Grade with ACF and PACF Chart\n######################\n\n\n# IF ACF width \"DECREASE\" relative to the delays and PACF \"CUT\" after the p delay means it's an AR(p) pattern.\n\n# If the ACF width q \"CUT\" after the delay and the PACF width \"DECREASE\" according to the delays, it means it's a MA(q) pattern.\n\n# If the widths of ACF and PACF are decreasing according to the delays, it means it is an ARMA model.\ndf_diff = y.diff()\ndf_diff.dropna(inplace=True)\n\nacf_pacf(df_diff)","36b70aa2":"##################################################\n# SARIMA(p, d, q): (Seasonal Autoregressive Integrated Moving-Average)\n##################################################\n\n\n\nmodel = SARIMAX(train, order=(1, 0, 1), seasonal_order=(0, 0, 0, 12))\nsarima_model = model.fit(disp=0)\n\ny_pred_test = sarima_model.get_forecast(steps=48)\ny_pred = y_pred_test.predicted_mean\ny_pred = pd.Series(y_pred, index=test.index)\n\nplot_co2(train, test, y_pred, \"SARIMA\")","fafbe3b3":"############################\n# Hyperparameter Optimization\n############################\n\n\np = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\ndef sarima_optimizer_aic(train, pdq, seasonal_pdq):\n    best_aic, best_order, best_seasonal_order = float(\"inf\"), float(\"inf\"), None\n    for param in pdq:\n        for param_seasonal in seasonal_pdq:\n            try:\n                sarimax_model = SARIMAX(train, order=param, seasonal_order=param_seasonal)\n                results = sarimax_model.fit(disp=0)\n                aic = results.aic\n                if aic < best_aic:\n                    best_aic, best_order, best_seasonal_order = aic, param, param_seasonal\n                print('SARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, aic))\n            except:\n                continue\n    print('SARIMA{}x{}12 - AIC:{}'.format(best_order, best_seasonal_order, best_aic))\n    return best_order, best_seasonal_order\n\n\nbest_order, best_seasonal_order = sarima_optimizer_aic(train, pdq, seasonal_pdq)\n\n","d14edf28":"############################\n# Final Model\n############################\n\nmodel = SARIMAX(train, order=best_order, seasonal_order=best_seasonal_order)\nsarima_final_model = model.fit(disp=0)\n\ny_pred_test = sarima_final_model.get_forecast(steps=48)\n\n# MAE\ny_pred = y_pred_test.predicted_mean\ny_pred = pd.Series(y_pred, index=test.index)\n\nplot_co2(train, test, y_pred, \"SARIMA\")","761160a9":"##################################################\n# Examining the Statistical Outputs of the Model\n##################################################\nsarima_final_model.plot_diagnostics(figsize=(15, 12))\nplt.show()","5d5a0514":"##################################################\n# BONUS: SARIMA Optimization for MAE\n##################################################\n\np = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\ndef sarima_optimizer_mae(train, pdq, seasonal_pdq):\n    best_mae, best_order, best_seasonal_order = float(\"inf\"), float(\"inf\"), None\n    for param in pdq:\n        for param_seasonal in seasonal_pdq:\n            try:\n                model = SARIMAX(train, order=param, seasonal_order=param_seasonal)\n                sarima_model = model.fit(disp=0)\n                y_pred_test = sarima_model.get_forecast(steps=48)\n                y_pred = y_pred_test.predicted_mean\n                mae = mean_absolute_error(test, y_pred)\n\n                # mae = fit_model_sarima(train, val, param, param_seasonal)\n\n                if mae < best_mae:\n                    best_mae, best_order, best_seasonal_order = mae, param, param_seasonal\n                print('SARIMA{}x{}12 - MAE:{}'.format(param, param_seasonal, mae))\n            except:\n                continue\n    print('SARIMA{}x{}12 - MAE:{}'.format(best_order, best_seasonal_order, best_mae))\n    return best_order, best_seasonal_order\n\nbest_order, best_seasonal_order = sarima_optimizer_mae(train, pdq, seasonal_pdq)\n\nmodel = SARIMAX(train, order=best_order, seasonal_order=best_seasonal_order)\nsarima_final_model = model.fit(disp=0)\n\ny_pred_test = sarima_final_model.get_forecast(steps=48)\ny_pred = y_pred_test.predicted_mean\ny_pred = pd.Series(y_pred, index=test.index)\n\n\nplot_co2(train, test, y_pred, \"SARIMA\")","3daefbf8":"# 2) Single Exponential Smoothing\n\n![image.png](https:\/\/miro.medium.com\/max\/2400\/1*d8UsN4g3URJAaaEzD-zyZA.png)\n\n    Makes predictions by exponentially correcting (recursive).\n    \n    The future is more related to the recent past and using the assumption, the effects of the past are weighted.\n    \n    Estimates are made by exponentially weighting past actual values and past estimated values.","4bc6d6ef":"# SARIMA(p, d, q): (Seasonal Autoregressive Integrated Moving-Average)\n\nAfter, we add the order of integration I(d). The parameter d represents the number of differences required to make the series stationary.\nFinally, we add the final component: seasonality S(P, D, Q, s), where s is simply the season\u2019s length. Furthermore, this component requires the parameters P and Q which are the same as p and q, but for the seasonal component. Finally, D is the order of seasonal integration representing the number of differences required to remove seasonality from the series.\nCombining all, we get the SARIMA(p, d, q)(P, D, Q, s) model.\nThe main takeaway is: before modelling with SARIMA, we must apply transformations to our time series to remove seasonality and any non-stationary behaviors.\n\n\n__Summary__:\n\n>ARIMA + seasonality\n\n>It can be used in univariate series with trend and\/or seasonality.\n\n>The p,d,q parameters are the parameters from the ARIMA model. They are trend elements and ARIMA can model the trend.\n\n>p: Actual value delay number (autoregressive degree)\n\n>d: Number of difference operations (degree of difference)\n\n>q: Error delay number (Moving average degree)\n\nP,D,Q seasonal lag numbers.\n\nm is the number of time steps for a single seasonal period. Expresses the structure of seasonality.","f9d196a6":"# Dataset\n\nAtmospheric CO2 from Continuous Air Samples at Mauna Loa Observatory, Hawaii, U.S.A.\n\n__Period of Record:__ March 1958 - December 2001\n\nThis dataset includes a monthly observation of atmospheric carbon dioxide (or CO2) concentrations from the Mauna Loa Observatory (Hawaii) at a latitude of 19.5, longitude of -155.6, and elevation of 3397 meters.\n","7014cc48":"# Characteristics of Time Series\n\nWhen we analyze or build a model for a timestamp dataset, there are some characteristic needs to be considered.\n\n- Is there a __stationarity__ ?\n\n![image.png](https:\/\/miro.medium.com\/max\/2400\/1*xdblkZyg6YmmReAkZHUksw.png)\n\n- Is there a __trend__, meaning that, on average, the measurements tend to increase (or decrease) over time?\n\n![image,png](https:\/\/miro.medium.com\/max\/1276\/1*xYwv3xjv1sdq4rPUlzRarA.jpeg)\n\n- Is there __seasonality__, meaning that there is a regularly repeating pattern of highs and lows related to calendar time such as seasons, quarters, months, days of the week, and so on?\n\n![image.png](https:\/\/www.researchgate.net\/profile\/Grace-Rumantir\/publication\/2797556\/figure\/fig6\/AS:279556704489480@1443662920465\/An-example-of-a-time-series-with-a-long-term-trend-a-seasonal-e-ect-with-superimposed.png)\n\n- Are there __outliers?__ In regression, outliers are far away from your line. With time series data, your outliers are far away from your other data.\n\n- Is there a __long-run cycle or period__ unrelated to seasonality factors?\n\n- Is there constant variance over time, or is the variance non-constant?\n\n- Are there any abrupt changes to either the level of the series or the variance?","2c97e04f":"# Model Usage for Different Scenario\n\n>Stationarity: SES, AR, MA, ARMA\n\n>Trend : DES, ARIMA, SARIMA\n\n>Trend + Seasonality: TES + SARIMA","f5e18d6b":"# 4) Triple Exponential Smoothing Holt-Witers\n\n![image.png](https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Faimlzone.com%2Fwp-content%2Fuploads%2F2020%2F01%2Fimage-8.png&f=1&nofb=1)\n\n        Level(SSE) + Trend + Seasonality\n        \n        State of art method for smoothinng methods\n        \n        This method predicts dynamically using level, trend and seasonality.\n        \n        ","d820a3b3":"# 3) Double Exponential Smoothing\n\n![image.png](https:\/\/miro.medium.com\/max\/2400\/1*Ng9wsOnjHu8FAse6ba2mwg.png)\n\n    Makes predictions by exponentially correcting based on trend effect (recursive).\n    \n    DES = Level(SSE) + Trend\n    \n    The main approach is same like SSE. Additionally, trend effect is considered\n    \n    It is used for univariate time series with trend and no seasonality.\n","7a00d1fe":"# Dickey-Fuller Test\n\nWe will use Augmented Dickey-Fuller test to test our null hypothesis that our series is not stationary. If we get a significant p value, we will reject our null hypothesis and establish that our series is stationary. On the contrary, if we fail to reject our null hypothesis, we will difference the series and run the test again.","eb7517b4":"# ARMA Model(p,q) = AR(p) + MA(q)\n\nARIMA models, also called Box-Jenkins models, are models that may possibly include autoregressive terms, moving average terms, and differencing operations. Various abbreviations are used:\n\n>When a model only involves autoregressive terms it may be referred to as an AR model. When a model only involves moving average terms, it may be referred to as an MA model.\n\n>When no differencing is involved, the abbreviation ARMA may be used.\n\n__Summary:__\n\n>AutoRegressive Moving Average(ARIMA) combines AR and MA methods\n\n>It predicts with a linear combination of past values \u200b\u200band past errors.\n\n> Suitable for univariate time series without trend and seasonality","8b99af78":"# 1) Moving Average\n\nThe moving average model is probably the most naive approach to time series modelling. This model simply states that the next observation is the mean of all past observations.\n\nAlthough simple, this model might be surprisingly good and it represents a good starting point.\n\nOtherwise, the moving average can be used to identify interesting trends in the data. We can define a window to apply the moving average model to smooth the time series, and highlight different trends.\n\n![image.png](https:\/\/miro.medium.com\/max\/2400\/1*WZOyzZewXwKOhJBbN82kfg.png)","aeb19838":"# 1) AR(p): Autoregression\n\nThis is basically a regression of the time series onto itself. Here, we assume that the current value depends on its previous values with some lag. It takes a parameter p which represents the maximum lag. To find it, we look at the partial autocorrelation plot and identify the lag after which most lags are not significant.\n\nIn an autoregression model, we forecast the variable of interest using a linear combination of past values of the variable.\n\nThus, an autoregressive model of order p can be written as;\n\n>$y_{t} = c + \\phi_{1}y_{t-1} + \\phi_{2}y_{t-2} + \\dots + \\phi_{p}y_{t-p} + \\varepsilon_{t}, $\n\nwhere $\\varepsilon_t$ s white noise. This is like a multiple regression but with lagged values of yt as predictors.\n\n\n> Estimation is made by a linear combination of observations from previous time steps.\n\n> Suitable for univariate time series without trend and seasonality","a463dd9d":"# Autocorrelation (ACF)\n\nInformally, autocorrelation is the similarity between observations as a function of the time lag between them.\n\n![image.png](https:\/\/miro.medium.com\/max\/700\/1*F5wTJgw8dpteJssJVGXPcg.png)\n\nAbove is an example of an autocorrelation plot. Looking closely, you realize that the first value and the 24th value have a high autocorrelation. Similarly, the 12th and 36th observations are highly correlated. This means that we will find a very similar value at every 24 unit of time.\nNotice how the plot looks like sinusoidal function. This is a hint for seasonality, and you can find its value by finding the period in the plot above, which would give 24h.\n\nVariance is : $Var(x_t)= \\sigma^2_w(1+\\theta^2_1)$\n\nAutocorrelation function (ACF) is: $ \\rho_1 = \\dfrac{\\theta_1}{1+\\theta^2_1}, \\text{ and } \\rho_h = 0 \\text{ for } h \\ge 2 $","86e0feb3":"# Modelling the Time Series\n\nThere are several ways to model a time seris to make predictions.\n\nWe will mention the topics as below:\n\n- Moving Average\n\n- Weighted Average\n\n- Exponential Smoothing Methods\n\n     -Single Exponential Smoothing (It contains stationarity)\n     \n     -Double Exponential Smoothing (It contains Level + Trend)\n     \n     -Triple Exponential Smoothing Holt-Witers (It contains Level + Tred + Seasonality)\n     \n- ARIMA Models","3a03f73e":"# Time Series Analysis and Forecasting\n\nBasically, time series is sequential data that is indexed on timestamp.\n\nIf we want to predict trend in financial markets or weather forecasting or electricity consumption, time is an asset to be considered in our models.\n\n![image.png](https:\/\/www.vertica.com\/wp-content\/uploads\/2019\/02\/Timeseries.jpg)\n\n## Basic Objectives of the Analysis\n\nThe basic objective usually is to determine a model that describes the pattern of the time series. Uses for such a model are:\n\n- To describe the important features of the time series pattern.\n\n- To explain how the past affects the future or how two time series can \u201cinteract\u201d.\n\n- To forecast future values of the series.\n\n- To possibly serve as a control standard for a variable that measures the quality of product in some manufacturing situations.\n\n\nReference:\n\n[PennState STAT510](https:\/\/online.stat.psu.edu\/stat510\/lesson\/1\/1.1)\n\n[Medium](https:\/\/towardsdatascience.com\/the-complete-guide-to-time-series-analysis-and-forecasting-70d476bfe775)\n\n[OTexts.com](https:\/\/otexts.com\/fpp2\/AR.html)\n\n[Medium](https:\/\/towardsdatascience.com\/a-blueprint-for-time-series-9f865609bfa2)\n","a6c42a5a":"# 2) MA(q): Moving Average\n\nA moving average term in a time series model is a past error (multiplied by a coefficient).\n\n>The 1st order moving average model, denoted by MA(1) is:\n\n> $x_t = \\mu + w_t +\\theta_1w_{t-1}$\n\n>The 2nd order moving average model, denoted by MA(2) is:\n\n> $x_t = \\mu + w_t +\\theta_1w_{t-1}+\\theta_2w_{t-2}$\n\n>The qth order moving average model, denoted by MA(q) is:\n\n> $x_t = \\mu + w_t +\\theta_1w_{t-1}+\\theta_2w_{t-2}+\\dots + \\theta_qw_{t-q}$\n\n\n## ACF for General MA(q) Models\n\nA property of MA(q) models in general is that there are nonzero autocorrelations for the first q lags and autocorrelations = 0 for all lags > q.\n\nFor a time series;\n\n> $\\rho_h = \\dfrac{\\text{Covariance for lag h}}{\\text{Variance}}$\n\nApply this result to get the ACF\n\n## Partial Autocorrelation Function (PACF)\n\nIn general, a partial correlation is a conditional correlation. It is the correlation between two variables under the assumption that we know and take into account the values of some other set of variables. For instance, consider a regression context in which y is the response variable and x1, x2 and x3 are predictor variables. The partial correlation between y and x3 is the correlation between the variables determined taking into account how both y and x3 are related to x1 and x2.\n\nIn regression, this partial correlation could be found by correlating the residuals from two different regressions:\n\n>Regression in which we predict y from x1 and x2\n\n>regression in which we predict x3 from x1 and x2, Basically we correlate the parts of y and x3 that are not predicted by x1 and x2\n\n\n$\\dfrac{\\text{Covariance}(y, x_3|x_1, x_2)}{\\sqrt{\\text{Variance}(y|x_1, x_2)\\text{Variance}(x_3| x_1, x_2)}}$","74c0070c":"# Some Useful Facts About PACF and ACF Patterns\n\n__Identification of an AR model is often best done with the PACF.__\n\n> For an AR model, the theoretical PACF \u201cshuts off\u201d past the order of the model. The phrase \u201cshuts off\u201d means that in theory the partial autocorrelations are equal to 0 beyond that point. Put another way, the number of non-zero partial autocorrelations gives the order of the AR model. By the \u201corder of the model\u201d we mean the most extreme lag of x that is used as a predictor.\n\n__Identification of an MA model is often best done with the ACF rather than the PACF__\n\n>For an MA model, the theoretical PACF does not shut off, but instead tapers toward 0 in some manner. A clearer pattern for an MA model is in the ACF. The ACF will have non-zero autocorrelations only at lags involved in the model."}}