{"cell_type":{"6a1ea405":"code","e14f3ed9":"code","ef0fea2b":"code","245decb2":"code","0701b52e":"code","9b6dfd9b":"code","0076f9ba":"code","9b3457e7":"code","1f3c94a9":"code","7e729d9f":"code","02ffa4d7":"code","a9b36628":"code","1a6613bd":"code","d834ef01":"code","7b24ec88":"code","dcda8ce9":"code","5968cdd8":"code","f363a26c":"code","69d0604a":"code","76f88a1b":"code","fbda66ba":"code","e92c7192":"code","e5eee7ab":"code","72016aa2":"code","46d04a0f":"code","e8372eaa":"code","69ff65df":"code","a766cbbc":"code","c88dc915":"code","c857f2bf":"code","d1dc896d":"code","bcac0bb1":"code","d8022ba8":"code","efed8a77":"code","b7ce2f74":"code","733b44ad":"code","b042e248":"code","2b8095ff":"code","f68190d6":"code","a5d3c132":"code","2fe18258":"code","72d7e00e":"code","4a2b1ca1":"code","87f1058c":"code","1dc9910f":"code","a0964518":"code","5271037b":"code","49c1a884":"code","325e4ef6":"code","19d2e757":"code","bcd34e7b":"code","3510c80b":"code","a47345e7":"code","d5a35bfe":"code","50c5fa6c":"code","74cbee73":"code","b007b6c7":"code","6feca6b7":"code","29150afa":"code","d4021e93":"code","4b300d85":"code","620a1a70":"code","e83d33d9":"code","73df6ba5":"code","815f294f":"code","b64898ef":"code","30729b42":"code","7cc0cb57":"code","3bb37680":"code","1a4be3c8":"code","5d32c6c3":"code","cf50603f":"code","3f92cd0e":"code","e570240d":"code","0a6cc33b":"code","f016e171":"code","deefbc3e":"markdown","47eb602d":"markdown","d20a1d2a":"markdown","2de8d87f":"markdown","0f621691":"markdown","dc758200":"markdown","018f0416":"markdown","2b8455ec":"markdown","7acaf322":"markdown","3214c9e2":"markdown","47f046f1":"markdown","ac3b7732":"markdown","94e07954":"markdown","93b0bead":"markdown","09ea3e98":"markdown","b766cbbe":"markdown","4b2118a3":"markdown","9477116b":"markdown","c3be9bf5":"markdown","8e6a3abd":"markdown"},"source":{"6a1ea405":"import pandas as pd \nimport numpy as np\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nimport optuna\nfrom sklearn.metrics import log_loss\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline\nsns.set(color_codes=True)\npal = sns.color_palette(\"viridis\", 10)\nsns.set_palette(pal)","e14f3ed9":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')","ef0fea2b":"train.info()","245decb2":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain['target'] = le.fit_transform(train['target'])","0701b52e":"train.isnull().sum()","9b6dfd9b":"!pip install dataprep","0076f9ba":"from dataprep.eda import plot, plot_correlation, create_report, plot_missing","9b3457e7":"plot(train.drop(['id'],axis=1))","1f3c94a9":"# create_report(train)","7e729d9f":"skew = []\nfor i in train.drop(['id','target'],axis=1).columns:\n    skew.append(train[str(i)].skew())\n    \nskew_df = pd.DataFrame({'Feature':train.drop(['id','target'],axis=1).columns, 'Skewness': skew})\nskew_df.plot(kind='bar',figsize=(18,10))","02ffa4d7":"big_df = pd.concat([train.drop(['id','target'],axis=1), test.drop(['id'],axis=1)])","a9b36628":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nscaled_big_df = pd.DataFrame(sc.fit_transform(big_df), columns=big_df.columns)","1a6613bd":"from sklearn.cluster import KMeans\nkm = KMeans(n_clusters = 9, random_state=13).fit(scaled_big_df)","d834ef01":"scaled_big_df['Cluster'] = km.predict(scaled_big_df)","7b24ec88":"s_train = scaled_big_df.iloc[:200000,:]\ns_train = pd.concat([s_train,pd.get_dummies(s_train['Cluster'])],axis=1)","dcda8ce9":"s_test = scaled_big_df.iloc[200000:,:]\ns_test = pd.concat([s_test,pd.get_dummies(s_test['Cluster'])],axis=1)","5968cdd8":"s_train = s_train.drop(['Cluster',0],axis=1)\ns_test = s_test.drop(['Cluster',0],axis=1)","f363a26c":"X = s_train\ny = train['target']","69d0604a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test  = train_test_split(X,y,train_size=0.8,random_state=42)","76f88a1b":"from catboost import CatBoostClassifier, Pool\ntrain_pool = Pool(data=X_train, label=y_train)\ntest_pool = Pool(data=X_test, label=y_test.values) ","fbda66ba":"model = CatBoostClassifier(\n    loss_function='MultiClass',\n    eval_metric='MultiClass',\n    verbose=False,\n    task_type = 'GPU'\n)\nmodel.fit(train_pool,plot=True,eval_set=test_pool)","e92c7192":"y_pred = model.predict_proba(X_test)\nlog_loss(y_test,y_pred)","e5eee7ab":"import eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(model, random_state=13, scoring = 'neg_log_loss')\nperm.fit(X_test,y_test)","72016aa2":"feat_importance = pd.DataFrame({'Feature':X_train.columns, 'Importance':perm.feature_importances_}).sort_values(by='Importance',ascending=False)\nplt.figure(figsize= (10,15))\nsns.barplot(data = feat_importance, y = 'Feature', x= 'Importance',orient='h')","46d04a0f":"a = perm.feature_importances_\nl = []\nfor i in range(83):\n    if i>74:\n        if a[i]<0:\n            l.append(i-74)\n    else:\n        if a[i]<0:\n            l.append('feature_'+str(i))\n        \nprint('Dropped Features')\nprint(l)","e8372eaa":"train_new = s_train.drop(l,axis=1)\ntrain_new['target'] = train['target']\ntrain_new['id'] = train['id']\ntest_new =s_test.drop(l,axis=1)\nX_new = train_new.drop(['id','target'],axis=1)","69ff65df":"def fun(trial,data=X_new,target=y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2,random_state=42)\n    param = {\n        'loss_function': 'MultiClass',\n        'eval_metric': 'MultiClass',\n        'learning_rate' : trial.suggest_uniform('learning_rate',1e-3,0.1),\n        'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,30),\n        'subsample': trial.suggest_uniform('subsample',0,1),\n        'random_strength': trial.suggest_uniform('random_strength',0,1),\n        'depth': trial.suggest_int('depth',5,12),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,100),\n        'num_leaves' : trial.suggest_int('num_leaves',16,64),\n        'leaf_estimation_method' : 'Newton',\n        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,10),\n        'verbose':False,\n        'bootstrap_type': 'Bernoulli',\n        'random_state' : trial.suggest_categorical('random_state',[13,2000,7,19]),\n        'task_type' : 'GPU',\n        'grow_policy' : 'Lossguide'\n        \n    }\n    model = CatBoostClassifier(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=200,verbose=False)\n    \n    preds = model.predict_proba(test_x)\n    \n    ll = log_loss(test_y, preds)\n    \n    return ll","a766cbbc":"study = optuna.create_study(direction='minimize')\nstudy.optimize(fun, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","c88dc915":"\nbest_params_cb = study.best_params\nbest_params_cb['loss_function'] = 'MultiClass'\nbest_params_cb['eval_metric'] = 'MultiClass'\nbest_params_cb['verbose'] = False\nbest_params_cb['n_estimators'] = 10000\nbest_params_cb['bootstrap_type']= 'Bernoulli'\nbest_params_cb['leaf_estimation_method'] = 'Newton'\nbest_params_cb['task_type'] = 'GPU'\nbest_params_cb['grow_policy'] = 'Lossguide'","c857f2bf":"stacked_df = pd.DataFrame(columns = ['Class1m1', 'Class2m1','Class3m1','Class4m1','Class5m1', 'Class6m1','Class7m1','Class8m1','Class9m1','Class1m2', 'Class2m2','Class3m2','Class4m2','Class5m2', 'Class6m2','Class7m2','Class8m2','Class9m2','Class1m3', 'Class2m3','Class3m3','Class4m3','Class5m3', 'Class6m3','Class7m3','Class8m3','Class9m3','target'])","d1dc896d":"columns = train_new.drop(['id','target'],axis=1).columns\ncb_df = pd.DataFrame(columns = ['Class1m1', 'Class2m1','Class3m1','Class4m1','Class5m1', 'Class6m1','Class7m1','Class8m1','Class9m1','target'])\npreds = np.zeros((test.shape[0],9))\nkf = StratifiedKFold(n_splits = 10 , random_state = 13 , shuffle = True)\nll =[]\nn=0\n\nfor tr_idx, test_idx in kf.split(train_new[columns], train_new['target']):\n    \n    X_tr, X_val = train_new[columns].iloc[tr_idx], train_new[columns].iloc[test_idx]\n    y_tr, y_val = train_new['target'].iloc[tr_idx], train_new['target'].iloc[test_idx]\n    \n    model = CatBoostClassifier(**best_params_cb)\n    \n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    y_pred  = model.predict_proba(X_val)\n    df = pd.DataFrame(y_pred,columns=['Class1m1', 'Class2m1','Class3m1','Class4m1','Class5m1', 'Class6m1','Class7m1','Class8m1','Class9m1'])\n    df['target'] = list(y_val)\n    \n    cb_df = pd.concat([cb_df,df])\n    preds+=model.predict_proba(test_new)\/kf.n_splits\n    ll.append(log_loss(y_val, y_pred))\n    print(n+1,ll[n])\n    n+=1","bcac0bb1":"cb_df","d8022ba8":"np.mean(ll)","efed8a77":"df_kfold = pd.DataFrame(preds,columns=['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9'])\ndf_kfold['id']  = test['id']\ndf_kfold = df_kfold[['id','Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']]","b7ce2f74":"df_kfold","733b44ad":"output_3 = df_kfold.to_csv('cb_submit.csv',index=False)","b042e248":"from lightgbm import LGBMClassifier","2b8095ff":"model = LGBMClassifier(random_state= 13, objective= 'multiclass', metric = 'multi_logloss', device_type='gpu').fit(X_train, y_train)","f68190d6":"perm = PermutationImportance(model, random_state=13, scoring = 'neg_log_loss')\nperm.fit(X_test,y_test)","a5d3c132":"feat_importance = pd.DataFrame({'Feature':X_train.columns, 'Importance':perm.feature_importances_}).sort_values(by='Importance',ascending=False)\nplt.figure(figsize= (8,15))\nsns.barplot(data = feat_importance, y = 'Feature', x= 'Importance',orient='h')\n","2fe18258":"a = perm.feature_importances_\nl = []\nfor i in range(83):\n    if i>74:\n        if a[i]<0:\n            l.append(i-74)\n    else:\n        if a[i]<0:\n            l.append('feature_'+str(i))\n        \nprint('Dropped Features')\nprint(l)","72d7e00e":"train_new = s_train.drop(l,axis=1)\ntrain_new['target'] = train['target']\ntrain_new['id'] = train['id']\ntest_new =s_test.drop(l,axis=1)\nX_new = train_new.drop(['id','target'],axis=1)","4a2b1ca1":"def fun2(trial, data = X_new, target=y):\n    train_x, test_x, train_y, test_y = train_test_split(data,target,train_size=0.8,random_state=42)\n    param = {\n         'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 30.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 30.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n\n        'subsample': trial.suggest_uniform('subsample', 0,1),\n        'learning_rate': 0.01,\n        'max_depth': trial.suggest_int('max_depth', 1,100),\n        'num_leaves' : trial.suggest_int('num_leaves', 2, 1000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight' , 1e-5 , 1),\n        'cat_smooth' : trial.suggest_int('cat_smooth', 1, 100),\n        'cat_l2': trial.suggest_int('cat_l2',1,20),\n        'metric': 'multi_logloss', \n        'random_state' : trial.suggest_categorical('random_state',[13,2000,7,19]),\n        'n_estimators': 10000,\n        'objective': 'multiclass',\n        'device_type':'gpu'\n        \n    }\n    model = LGBMClassifier(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=200,verbose=False)\n    \n    pred = model.predict_proba(test_x)\n    \n    ll = log_loss(test_y, pred)\n    \n    return ll","87f1058c":"study_2 = optuna.create_study(direction='minimize')\nstudy_2.optimize(fun2, n_trials=20)\nprint('Number of finished trials:', len(study_2.trials))\nprint('Best trial:', study_2.best_trial.params)","1dc9910f":"\nbest_params_lgbm =  study_2.best_params\nbest_params_lgbm['objective'] = 'multiclass'\nbest_params_lgbm['metric'] = 'multi_logloss'\nbest_params_lgbm['learning_rate'] = 0.01\nbest_params_lgbm['n_estimators'] = 10000\nbest_params_lgbm['device_type'] : 'gpu'","a0964518":"columns = train_new.drop(['id','target'],axis=1).columns\npreds_2 = np.zeros((test.shape[0],9))\nlgbm_df = pd.DataFrame(columns  = ['Class1m2', 'Class2m2','Class3m2','Class4m2','Class5m2', 'Class6m2','Class7m2','Class8m2','Class9m2','target'])\nkf = StratifiedKFold(n_splits = 10 , random_state = 13 , shuffle = True)\nll =[]\nn=0\n\nfor tr_idx, test_idx in kf.split(train_new[columns], train_new['target']):\n    \n    X_tr, X_val = train_new[columns].iloc[tr_idx], train_new[columns].iloc[test_idx]\n    y_tr, y_val = train_new['target'].iloc[tr_idx], train_new['target'].iloc[test_idx]\n    \n    model = LGBMClassifier(**best_params_lgbm)\n    \n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    y_pred  = model.predict_proba(X_val)\n    df = pd.DataFrame(y_pred,columns = ['Class1m2', 'Class2m2','Class3m2','Class4m2','Class5m2', 'Class6m2','Class7m2','Class8m2','Class9m2'])\n    df['target'] = list(y_val)\n    \n    lgbm_df = pd.concat([lgbm_df,df])\n    preds_2+=model.predict_proba(test_new)\/kf.n_splits\n    ll.append(log_loss(y_val, y_pred))\n    print(n+1,ll[n])\n    n+=1","5271037b":"lgbm_df","49c1a884":"np.mean(ll)","325e4ef6":"df_kfold_lgbm = pd.DataFrame(preds_2,columns=['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9'])\ndf_kfold_lgbm['id']  = test['id']\ndf_kfold_lgbm = df_kfold_lgbm[['id','Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']]","19d2e757":"df_kfold_lgbm","bcd34e7b":"output_5 = df_kfold_lgbm.to_csv('lgbm_submit.csv',index=False)","3510c80b":"from xgboost import XGBClassifier","a47345e7":"model = XGBClassifier(random_State=13, tree_method ='gpu_hist',predictor= 'gpu_predictor').fit(X_train, y_train)\nperm = PermutationImportance(model, random_state=13, scoring = 'neg_log_loss')\nperm.fit(X_test,y_test)","d5a35bfe":"feat_importance = pd.DataFrame({'Feature':X_train.columns, 'Importance':perm.feature_importances_}).sort_values(by='Importance',ascending=False)\nplt.figure(figsize= (8,15))\nsns.barplot(data = feat_importance, y = 'Feature', x= 'Importance',orient='h')\n","50c5fa6c":"a = perm.feature_importances_\nl = []\nfor i in range(83):\n    if i>74:\n        if a[i]<0:\n            l.append(i-74)\n    else:\n        if a[i]<0:\n            l.append('feature_'+str(i))\n        \nprint('Dropped Features')\nprint(l)","74cbee73":"train_new = s_train.drop(l,axis=1)\ntrain_new['target'] = train['target']\ntrain_new['id'] = train['id']\ntest_new =s_test.drop(l,axis=1)\nX_new = train_new.drop(['id','target'],axis=1)","b007b6c7":"def fun3(trial, data = X_new, target = y):\n    train_x, test_x, train_y, test_y = train_test_split(data,target,train_size=0.8,random_state=42)\n\n    param = {\n       'learning_rate' : trial.suggest_uniform('learning_rate',0,1),\n        'gamma' : trial.suggest_uniform('gamma',0,100),\n        'max_depth': trial.suggest_int('max_depth', 1,100),\n        'min_child_weight' : trial.suggest_uniform('min_child_weight', 0,100),\n        'max_delta_step' : trial.suggest_uniform('max_delta_step',1,10),\n        'subsample' : trial.suggest_uniform('subsample',0,1),\n        'colsample_bytree' : trial.suggest_uniform('colsample_bytree',0,1),\n        'lambda' : trial.suggest_uniform('lambda',1e-5,30),\n        'alpha' : trial.suggest_uniform('alpha',1e-5,30),\n        'tree_method' :'gpu_hist',\n        'grow_policy':'lossguide',\n        'max_leaves': trial.suggest_int('max_leaves',16,64),\n        'random_state' : trial.suggest_categorical('random_state',[13,2000,7,19]),\n        'objective':'multi:softprob',\n        'eval_metric':'mlogloss',\n        'predictor':'gpu_predictor'\n\n        \n    }\n    model = XGBClassifier(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=200,verbose=False)\n    pred_y = model.predict_proba(test_x)\n    \n    ll = log_loss(test_y, pred_y)\n    \n    return ll\n    ","6feca6b7":"study_3 = optuna.create_study(direction='minimize')\nstudy_3.optimize(fun3, n_trials=50)\nprint('Number of finished trials:', len(study_3.trials))\nprint('Best trial:', study_3.best_trial.params)","29150afa":"best_params_xgb = study_3.best_params\nbest_params_xgb['objective'] = 'multi:softprob'\nbest_params_xgb['eval_metric'] = 'mlogloss'\nbest_params_xgb['grow_policy'] = 'lossguide'\nbest_params_xgb['n_estimators'] = 10000\nbest_params_xgb['tree_method'] ='gpu_hist'\nbest_params_xgb['predictor'] ='gpu_predictor'","d4021e93":"columns = train_new.drop(['id','target'],axis=1).columns\npreds_3 = np.zeros((test.shape[0],9))\nkf = StratifiedKFold(n_splits = 10 , random_state = 13 , shuffle = True)\nxgb_df = pd.DataFrame(columns = ['Class1m3', 'Class2m3','Class3m3','Class4m3','Class5m3', 'Class6m3','Class7m3','Class8m3','Class9m3','target'])\nll =[]\nn=0\n\nfor tr_idx, test_idx in kf.split(train_new[columns], train_new['target']):\n    \n    X_tr, X_val = train_new[columns].iloc[tr_idx], train_new[columns].iloc[test_idx]\n    y_tr, y_val = train_new['target'].iloc[tr_idx], train_new['target'].iloc[test_idx]\n    \n    model = XGBClassifier(**best_params_xgb)\n    \n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose = False)\n    y_pred  = model.predict_proba(X_val)\n    df = pd.DataFrame(y_pred,columns= ['Class1m3', 'Class2m3','Class3m3','Class4m3','Class5m3', 'Class6m3','Class7m3','Class8m3','Class9m3'])\n    df['target'] = list(y_val)\n    xgb_df = pd.concat([xgb_df,df])\n    \n    preds_3+=model.predict_proba(test_new)\/kf.n_splits\n    ll.append(log_loss(y_val, model.predict_proba(X_val)))\n    print(n+1,ll[n])\n    n+=1","4b300d85":"xgb_df","620a1a70":"np.mean(ll)","e83d33d9":"df_kfold_xgb = pd.DataFrame(preds_3,columns=['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9'])\ndf_kfold_xgb['id']  = test['id']\ndf_kfold_xgb = df_kfold_xgb[['id','Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']]","73df6ba5":"df_kfold_xgb","815f294f":"output_6 = df_kfold_xgb.to_csv('xgb_submit.csv',index=False)","b64898ef":"preds_combined = (preds+preds_2+preds_3)\/3\ndf_combined = pd.DataFrame(preds_combined,columns=['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9'])\ndf_combined['id'] = test['id']\ndf_combined = df_combined[['id','Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']]","30729b42":"df_combined","7cc0cb57":"final_output = df_combined.to_csv('blend_submit.csv',index=False)","3bb37680":"stacked_df['Class1m1'] = cb_df['Class1m1']\nstacked_df['Class2m1'] = cb_df['Class2m1']\nstacked_df['Class3m1'] = cb_df['Class3m1']\nstacked_df['Class4m1'] = cb_df['Class4m1']\nstacked_df['Class5m1'] = cb_df['Class5m1']\nstacked_df['Class6m1'] = cb_df['Class6m1']\nstacked_df['Class7m1'] = cb_df['Class7m1']\nstacked_df['Class8m1'] = cb_df['Class8m1']\nstacked_df['Class9m1'] = cb_df['Class9m1']\n\n\nstacked_df['Class1m2'] = lgbm_df['Class1m2']\nstacked_df['Class2m2'] = lgbm_df['Class2m2']\nstacked_df['Class3m2'] = lgbm_df['Class3m2']\nstacked_df['Class4m2'] = lgbm_df['Class4m2']\nstacked_df['Class5m2'] = lgbm_df['Class5m2']\nstacked_df['Class6m2'] = lgbm_df['Class6m2']\nstacked_df['Class7m2'] = lgbm_df['Class7m2']\nstacked_df['Class8m2'] = lgbm_df['Class8m2']\nstacked_df['Class9m2'] = lgbm_df['Class9m2']\n\nstacked_df['Class1m3'] = xgb_df['Class1m3']\nstacked_df['Class2m3'] = xgb_df['Class2m3']\nstacked_df['Class3m3'] = xgb_df['Class3m3']\nstacked_df['Class4m3'] = xgb_df['Class4m3']\nstacked_df['Class5m3'] = xgb_df['Class5m3']\nstacked_df['Class6m3'] = xgb_df['Class6m3']\nstacked_df['Class7m3'] = xgb_df['Class7m3']\nstacked_df['Class8m3'] = xgb_df['Class8m3']\nstacked_df['Class9m3'] = xgb_df['Class9m3']\n\n\n\nstacked_df['target'] = cb_df['target']\n\n\ntest_stacked_df = pd.DataFrame(columns = ['Class1m1', 'Class2m1','Class3m1','Class4m1','Class5m1', 'Class6m1','Class7m1','Class8m1','Class9m1','Class1m2', 'Class2m2','Class3m2','Class4m2','Class5m2', 'Class6m2','Class7m2','Class8m2','Class9m2','Class1m3', 'Class2m3','Class3m3','Class4m3','Class5m3', 'Class6m3','Class7m3','Class8m3','Class9m3'])\ntest_stacked_df['Class1m1'] = df_kfold['Class_1']\ntest_stacked_df['Class2m1'] = df_kfold['Class_2']\ntest_stacked_df['Class3m1'] = df_kfold['Class_3']\ntest_stacked_df['Class4m1'] = df_kfold['Class_4']\ntest_stacked_df['Class5m1'] = df_kfold['Class_5']\ntest_stacked_df['Class6m1'] = df_kfold['Class_6']\ntest_stacked_df['Class7m1'] = df_kfold['Class_7']\ntest_stacked_df['Class8m1'] = df_kfold['Class_8']\ntest_stacked_df['Class9m1'] = df_kfold['Class_9']\n\n\n\ntest_stacked_df['Class1m2'] = df_kfold_lgbm['Class_1']\ntest_stacked_df['Class2m2'] = df_kfold_lgbm['Class_2']\ntest_stacked_df['Class3m2'] = df_kfold_lgbm['Class_3']\ntest_stacked_df['Class4m2'] = df_kfold_lgbm['Class_4']\ntest_stacked_df['Class5m2'] = df_kfold_lgbm['Class_5']\ntest_stacked_df['Class6m2'] = df_kfold_lgbm['Class_6']\ntest_stacked_df['Class7m2'] = df_kfold_lgbm['Class_7']\ntest_stacked_df['Class8m2'] = df_kfold_lgbm['Class_8']\ntest_stacked_df['Class9m2'] = df_kfold_lgbm['Class_9']\n\n\ntest_stacked_df['Class1m3'] = df_kfold_xgb['Class_1']\ntest_stacked_df['Class2m3'] = df_kfold_xgb['Class_2']\ntest_stacked_df['Class3m3'] = df_kfold_xgb['Class_3']\ntest_stacked_df['Class4m3'] = df_kfold_xgb['Class_4']\ntest_stacked_df['Class5m3'] = df_kfold_xgb['Class_5']\ntest_stacked_df['Class6m3'] = df_kfold_xgb['Class_6']\ntest_stacked_df['Class7m3'] = df_kfold_xgb['Class_7']\ntest_stacked_df['Class8m3'] = df_kfold_xgb['Class_8']\ntest_stacked_df['Class9m3'] = df_kfold_xgb['Class_9']\n\n\n","1a4be3c8":"stacked_df","5d32c6c3":"l=[]\nfor i in stacked_df['target']:\n    l.append(int(i))\n    \nstacked_df['target'] = l","cf50603f":"preds_stacked = np.zeros((test.shape[0],9))\ncolumns = ['Class1m1', 'Class2m1','Class3m1','Class4m1','Class5m1', 'Class6m1','Class7m1','Class8m1','Class9m1','Class1m2', 'Class2m2','Class3m2','Class4m2','Class5m2', 'Class6m2','Class7m2','Class8m2','Class9m2','Class1m3', 'Class2m3','Class3m3','Class4m3','Class5m3', 'Class6m3','Class7m3','Class8m3','Class9m3']\nkf = StratifiedKFold(n_splits = 10 , random_state = 13 , shuffle = True)\nll =[]\nn=0\n\nfor tr_idx, test_idx in kf.split(stacked_df[columns], stacked_df['target']):\n    \n    X_tr, X_val = stacked_df[columns].iloc[tr_idx], stacked_df[columns].iloc[test_idx]\n    y_tr, y_val = stacked_df['target'].iloc[tr_idx], stacked_df['target'].iloc[test_idx]\n    \n    model = CatBoostClassifier(loss_function='MultiClass',eval_metric='MultiClass',verbose=False,task_type = 'GPU')    \n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=200,verbose=False)\n    y_pred  = model.predict_proba(X_val)\n    \n    preds_stacked+=model.predict_proba(test_stacked_df)\/kf.n_splits\n    ll.append(log_loss(y_val, y_pred))\n    print(n+1,ll[n])\n    n+=1","3f92cd0e":"np.mean(ll)","e570240d":"df_kfold_st = pd.DataFrame(preds_stacked,columns=['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9'])\ndf_kfold_st['id']  = test['id']\ndf_kfold_st = df_kfold_st[['id','Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']]","0a6cc33b":"df_kfold_st","f016e171":"stacked_submit = df_kfold_st.to_csv('stacked_submit.csv',index=False)","deefbc3e":"## Creating a Clustering variable!","47eb602d":"# XGBoost KFOLD Predictions ","d20a1d2a":"# Plotting + Report with Dataprep","2de8d87f":"# What is there in this notebook\n\n## *This notebook will guide you through the starting submission in this competetion*\n\n## My approach\n\n> #### 1. Did EDA using Dataprep library and found some insights, data was similar to TPS MAY.\n> #### 2. Built the Catboost Model, tuned it and trained it on 10 folds.\n> #### 3. Built the LGBM Model, tuned it and trained it on 10 folds.\n> #### 4. Built the XGboost Model, tuned it and trained it on 10 folds.\n> #### 5. Built a voting classifier combining all the probabilities.\n> #### 6. Built a stacking model of all the models (training on the predicted probabilities and labels)","0f621691":"# Tuning with OPTUNA","dc758200":"# Stacked Model","018f0416":"# Baseline CATBoost Classifier","2b8455ec":"## Insights from Report and Visualisations\n> #### 1. The data is left skewed, log transformations or box-cox can be tried\n> #### 2. Most of the features are skewed with 0 values even >90%, that means feature selection will be necessary.\n> #### 3. Baseline model can overfit because of skewness in data.\n> #### 4. Outlier Detection and removal will also be handy to improve score.\n> #### 5. No corelation means that there are some unnecessary features.\n> #### 6. Also we can gain some info by feature engineering by trying feature interaction or ratio and increase corelation.","7acaf322":"# Feature Selection with Permutation Importance","3214c9e2":"## Visualising skewness","47f046f1":"# Feature Selection with Permutation Importance","ac3b7732":"# Predictions on Kfold","94e07954":"# Making Predictions with tuned Model","93b0bead":"# XGBoost","09ea3e98":"# Tuning with OPTUNA","b766cbbe":"## Thanks, and don't forget to upvote, it will motivate me!!","4b2118a3":"# LGBM Kfold Predictions","9477116b":"# Optimizing Catboost Classifier with OPTUNA","c3be9bf5":"# Voting Classifier (Catboost+LGBM+XGBoost)","8e6a3abd":"# LGBM"}}