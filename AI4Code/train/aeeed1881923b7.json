{"cell_type":{"e56f7361":"code","5321d02d":"code","9f105fff":"code","f37dc483":"code","d3e2a5e3":"code","9bb91c15":"code","ede98fee":"code","fa77bb42":"code","ccd27de4":"code","86bdd427":"code","5660dbcb":"code","7b955798":"code","928ba580":"code","f976fb7e":"code","d59339e3":"code","49814fa8":"code","ce88ac0c":"code","67e20816":"code","ed275aa0":"code","b16f3a8d":"code","0a8afb18":"code","641e9da8":"code","339ed7fe":"code","ca9f2fb6":"code","873b841a":"code","e60e9915":"code","ee975307":"code","480f25f3":"code","963ea60a":"code","f0d5acac":"code","52d962e1":"code","9a07c81b":"code","31a39276":"code","28576ea6":"code","83adfac4":"code","6a60c187":"code","da873cc5":"code","8834729b":"code","ec6bc06b":"code","5a85a9fb":"code","ecea5882":"code","4f3eee63":"code","46280e02":"code","37dd9da0":"code","810f5f7c":"code","b11b6f9a":"code","5350d25f":"code","c12bed22":"code","844f91b9":"code","18aa0d9c":"code","dd70df59":"code","2cf2922a":"code","77c86bdf":"code","29e0b692":"code","14a8c2f2":"code","e02a2cd7":"code","a5135931":"code","3d7f7b67":"code","57ef4885":"code","d9b2ebd3":"code","a93bd67f":"code","bb6f9a3c":"code","70838e44":"code","78600488":"code","c8b2cee0":"code","81542944":"code","0f9ad440":"code","c2ae7864":"code","c7ff0e6a":"code","07c1a369":"code","56e2e8df":"code","838d2bbf":"code","2e197f35":"code","6cd950f0":"code","d4fde8a9":"code","7492a37a":"code","7dbbb456":"code","99a37912":"code","2df7f02d":"code","f364b94e":"code","7bf54d83":"code","520d575a":"code","d15df627":"code","99ef40a0":"code","964eecea":"code","7781990a":"code","c02020ef":"code","d6f5107c":"code","4915e32e":"code","75480f19":"code","a3a2f595":"code","13f0d95e":"code","a9eb8a96":"code","73386d80":"code","bbe7f0b1":"code","53dfaa1b":"code","0758c120":"code","5ef94120":"code","21f26d88":"code","6a4eda9f":"code","fec677e3":"code","a52b98d5":"code","cd6a220b":"code","bfa5f8ca":"code","d1adcacc":"code","b58fa45c":"code","cf3a50ad":"code","f295aa1f":"code","953f3feb":"markdown","48845a9b":"markdown","caa8f5be":"markdown","96ff25f0":"markdown","9fd1b163":"markdown","35ed7f27":"markdown","2cbd8da4":"markdown","609fd59f":"markdown","31f2c92d":"markdown","3238a398":"markdown","7097f52c":"markdown","b1b90262":"markdown","1ddceb72":"markdown","bd88b568":"markdown","9f3d32d2":"markdown","533d3123":"markdown","037bc875":"markdown","0cd9478d":"markdown","770ae274":"markdown","b3f01c72":"markdown","6d5b1165":"markdown","3213763d":"markdown","87767ea5":"markdown","23812a1c":"markdown","191c6266":"markdown","9091845d":"markdown","c1355227":"markdown","ebf1de57":"markdown","5d46608c":"markdown","5258456c":"markdown","76edc708":"markdown","b9d3385d":"markdown","d2d54baa":"markdown","d3149f52":"markdown","3eb33931":"markdown","7a614141":"markdown","90853e8b":"markdown","da4d334b":"markdown","ce52efe8":"markdown","77ec7b46":"markdown","674fbc5b":"markdown","aa9b9983":"markdown","4edba85d":"markdown","ddeeb441":"markdown","be7fe3a7":"markdown","759d47e0":"markdown"},"source":{"e56f7361":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\npd.options.display.max_rows = 300\npd.options.display.max_columns = 300\n\nimport warnings\nwarnings.filterwarnings('ignore')","5321d02d":"# Read the data\n\nraw_data = pd.read_csv(\"\/kaggle\/input\/telecom-churn\/telecom_churn_data.csv\")\nraw_data.head()","9f105fff":"# Shape of the data\n\nraw_data.shape","f37dc483":"# Create a copy of original data\nchurn_data = raw_data.copy()","d3e2a5e3":"all_columns = raw_data.columns.tolist()","9bb91c15":"rech_col = [col for col in all_columns if 'rech' in col]\nrech_col","ede98fee":"# Though we have total amount of recharge for talktime we dont have that number for data, instead we have average and number of rechanrges\n# so let's calculate that amount\nchurn_data['total_data_rech_amt_6'] = churn_data['av_rech_amt_data_6'] * churn_data['total_rech_data_6']\nchurn_data['total_data_rech_amt_7'] = churn_data['av_rech_amt_data_7'] * churn_data['total_rech_data_7']\nchurn_data['total_data_rech_amt_8'] = churn_data['av_rech_amt_data_8'] * churn_data['total_rech_data_8']\nchurn_data['total_data_rech_amt_9'] = churn_data['av_rech_amt_data_9'] * churn_data['total_rech_data_9']\n\n# Drop the columns already accounted for \nchurn_data.drop(['total_rech_data_6', 'total_rech_data_7', 'total_rech_data_8', 'total_rech_data_9', 'av_rech_amt_data_6',\n              'av_rech_amt_data_7', 'av_rech_amt_data_8', 'av_rech_amt_data_9'], axis=1, inplace=True)\n\n# Find Average recharge amount for good phase i.e 6th and 7th month\ngood_phase_average_rech_amt = ( churn_data['total_rech_amt_6'].fillna(0) + churn_data['total_rech_amt_7'].fillna(0) + \n                               churn_data['total_data_rech_amt_6'].fillna(0) + churn_data['total_data_rech_amt_7'].fillna(0) ) \/ 2\n\n# 70 percentile of the good_phase_average_rech_amt data\ngd_phase_avg_rech_amt_70 = np.percentile(good_phase_average_rech_amt, 70.0)\nprint(f'70 Percentile of recharge amount is : {gd_phase_avg_rech_amt_70}')\n\n# Keep high value customer data\nchurn_data = churn_data[good_phase_average_rech_amt >= gd_phase_avg_rech_amt_70]\n\nprint(f'Shape of high value customer data: {churn_data.shape}')","fa77bb42":"# Seperating columns for the 9th month\nmnth9_columns = [col for col in all_columns if '9' in col]\n\n# Seperating the ic columns \nmnth9_ic_columns = [col for col in mnth9_columns if 'ic' in col and 'mou' in col]\n\n# Seperating oc columns\nmnth9_oc_columns = [col for col in mnth9_columns if 'oc' in col and 'mou' in col]\n\n# Seperating vol columns\nmnth9_vol_columns = [col for col in mnth9_columns if 'vol' in col and 'mb' in col]","ccd27de4":"# Sum of all the ic columns\nchurn_data['ic_sum'] = raw_data[mnth9_ic_columns].sum(axis = 1)\n\n# Sum of all the oc columns\nchurn_data['oc_sum'] = raw_data[mnth9_oc_columns].sum(axis = 1)\n\n# Sum of all the vol columns\nchurn_data['vol_sum'] = raw_data[mnth9_vol_columns].sum(axis = 1)","86bdd427":"churn_tag_columns = ['vol_sum', 'oc_sum', 'ic_sum']\n\n# Create churn as the sum of newly aggregrated columns, as that will be 0 if all of the columns have 0 values\nchurn_data['churn'] = churn_data[churn_tag_columns].sum(axis = 1)\n\nchurn_data.head(15)","5660dbcb":"# If the churn value is greater than 0 that means the user used a service indicating an active user\nchurn_data['churn'] = churn_data['churn'].apply(lambda x: 0 if x > 0 else 1)\nchurn_data.head()","7b955798":"churn_data.churn.value_counts(normalize=True)","928ba580":"# Dropping Month 9 Columns\n\nchurn_data.drop( [ col for col in mnth9_columns + churn_tag_columns if col not in ['total_rech_data_9', 'av_rech_amt_data_9'] ],\n                axis=1, inplace=True)\nchurn_data.shape","f976fb7e":"# Checking for missing values\nmsng_values = round(churn_data.isna().sum() \/ len(churn_data) * 100, 2).sort_values(ascending=False)\nmsng_values","d59339e3":"# Select columns with more than 40% missing values\ndrop_msng_col = msng_values[msng_values > 40].index.tolist()\ndrop_msng_col","49814fa8":"# Drop columns > 40% missing\nchurn_data.drop(drop_msng_col, axis=1, inplace=True)\nchurn_data.shape","ce88ac0c":"# Re-check missing values\nround(churn_data.isna().sum() \/ len(churn_data) * 100, 2).sort_values(ascending=False)","67e20816":"# Drop rows with missing values\nfor col in churn_data.columns:\n    churn_data = churn_data[~churn_data[col].isna()]\n    \n# Re-check missing values\nround(churn_data.isna().sum() \/ len(churn_data) * 100, 2).sort_values(ascending=False)","ed275aa0":"# The date columns don't add any value excpet for marking the last day of the period so we will drop them\ndate_col = [col for col in churn_data.columns if 'date' in col]\nchurn_data.drop(date_col, axis=1, inplace=True)\n[col for col in churn_data.columns if 'date' in col]","b16f3a8d":"print(churn_data.shape)\nchurn_data.head(20)","0a8afb18":"# Check columns for only 1 unique value\ndrop_col_sngl_val = []\n\nfor col in churn_data.columns:\n    if churn_data[f'{col}'].nunique() == 1:\n        drop_col_sngl_val.append(col)\n\ndrop_col_sngl_val","641e9da8":"# Drop single value columns\nchurn_data.drop(drop_col_sngl_val, axis=1, inplace=True)\nchurn_data.shape","339ed7fe":"# Drop the column 'mobile_number' as it does not add any information \nchurn_data.drop('mobile_number', axis=1, inplace=True)\nchurn_data.head()","ca9f2fb6":"corr = churn_data.corr()\ncorr.loc[:, :] = np.tril(corr, -1)\ncorr = corr.stack()\nhigh_corr_value = corr[(corr > 0.60) | (corr < -0.60)]\nhigh_corr_value","873b841a":"# List of columns that are explained well by other columns\ndrop_col_corr = ['loc_og_t2m_mou_6', 'std_og_t2t_mou_6', 'std_og_t2t_mou_7', 'std_og_t2t_mou_8', 'std_og_t2m_mou_6', 'std_og_t2m_mou_7', \n                 'std_og_t2m_mou_8', 'total_og_mou_6', 'total_og_mou_7', 'total_og_mou_8', 'loc_ic_t2t_mou_6', 'loc_ic_t2t_mou_7', \n                 'loc_ic_t2t_mou_8', 'loc_ic_t2m_mou_6', 'loc_ic_t2m_mou_7', 'loc_ic_t2m_mou_8', 'std_ic_t2m_mou_6', 'std_ic_t2m_mou_7', \n                 'std_ic_t2m_mou_8', 'total_ic_mou_6', 'total_ic_mou_7', 'total_ic_mou_8', 'total_rech_amt_6', 'total_rech_amt_7', \n                 'total_rech_amt_8', 'vol_3g_mb_6', 'vol_3g_mb_7', 'vol_3g_mb_8', 'loc_og_t2t_mou_6', 'loc_og_t2t_mou_7', 'loc_og_t2t_mou_8',\n                 'loc_og_t2f_mou_6', 'loc_og_t2f_mou_7', 'loc_og_t2f_mou_8', 'loc_og_t2m_mou_6', 'loc_og_t2m_mou_7', 'loc_og_t2m_mou_8',\n                 'loc_ic_t2f_mou_6', 'loc_ic_t2f_mou_7', 'loc_ic_t2f_mou_8']\n\n# Drop the high corr columns\nchurn_data.drop(drop_col_corr, axis=1, inplace=True)\nchurn_data.shape","e60e9915":"churn_data.head()","ee975307":"# Create a total mou instead of offnet and onnet\n\nchurn_data['total_mou_6'] = churn_data['onnet_mou_6'] + churn_data['offnet_mou_6']\nchurn_data['total_mou_7'] = churn_data['onnet_mou_7'] + churn_data['offnet_mou_7']\nchurn_data['total_mou_8'] = churn_data['onnet_mou_8'] + churn_data['offnet_mou_8']\n\n# Drop the redundant columns\nchurn_data.drop(['onnet_mou_6', 'onnet_mou_7', 'onnet_mou_8', 'offnet_mou_6', 'offnet_mou_7', 'offnet_mou_8'], axis=1, inplace=True)\n\nchurn_data.head()","480f25f3":"# Seperate columns for 6th and 7th month\ncol_for_6_7 = [col[:-2] for col in churn_data.columns if '6' in col or '7' in col]\n\n# Create new feature and drop the redundant columns\nfor col in set(col_for_6_7):\n    churn_data[f'gd_ph_{col}'] = ( churn_data[f'{col}_6'] + churn_data[f'{col}_7'] ) \/ 2\n    churn_data.drop([f'{col}_6', f'{col}_7'], axis=1, inplace=True)\n        \nchurn_data.head()","963ea60a":"# Create new column\nchurn_data['gd_ph_vbc_3g'] = ( churn_data['jul_vbc_3g'] + churn_data['jun_vbc_3g'] ) \/ 2\n\n# Drop redundant column\nchurn_data.drop(['jul_vbc_3g', 'jun_vbc_3g', 'sep_vbc_3g'], axis=1, inplace=True)\n\n# Rename the august column for vbc to vbc_3g_8\nchurn_data['vbc_3g_8'] = churn_data['aug_vbc_3g']\nchurn_data.drop('aug_vbc_3g', axis=1, inplace=True)\n\nchurn_data.head()","f0d5acac":"churn_data.shape","52d962e1":"# Reset the index \nchurn_data.reset_index(inplace=True, drop=True)\nchurn_data.head()","9a07c81b":"churn_data.describe()","31a39276":"# Going through the output of describe we can filter out the features that needs a second look\n\n# List of features to be analyzed\ncol_boxplot = ['arpu_8', 'loc_og_mou_8', 'max_rech_amt_8', 'last_day_rch_amt_8', 'aon', 'total_mou_8', \n               'gd_ph_loc_ic_mou', 'gd_ph_last_day_rch_amt', 'gd_ph_std_og_mou', 'gd_ph_max_rech_amt', \n              'gd_ph_loc_og_mou', 'gd_ph_arpu']\n\n# Plot boxplots for each variable\nfig, axes = plt.subplots(6, 2, figsize=(20, 20))\n# sns.boxplot(x=churn_data['arpu_8'], ax=axes[0])\nfor index, col in enumerate(col_boxplot):\n    i, j = divmod(index, 2)\n    sns.boxplot(churn_data[col], ax=axes[i, j])\n    \nplt.subplots_adjust(hspace=0.3) \nplt.show()","28576ea6":"# Check churn based on tenure\n\nplt.figure(figsize=(15,7))\nsns.scatterplot(y=churn_data['aon'] \/ 365, x=churn_data.index, hue=churn_data.churn, alpha=0.7)\nplt.ylim(0,10)\nplt.show()","83adfac4":"# Lets check how the VBC effects the revenue\nfig, axes = plt.subplots(1, 2, sharey=True, figsize=(15, 7))\nsns.scatterplot(y='gd_ph_arpu', x='gd_ph_total_mou', data=churn_data, ax=axes[0], hue='churn', alpha=0.7)\nsns.scatterplot(y='arpu_8', x='total_mou_8', data=churn_data, ax=axes[1], hue='churn', alpha=0.7)\n\n\n# Limiting the graph to more general upper bound\nplt.ylim(0,10000)\nplt.show()","6a60c187":"# Lets check how the total_mou effects the revenue\nfig, axes = plt.subplots(1, 2, sharey=True, figsize=(15, 7))\nsns.scatterplot(y='gd_ph_arpu', x='gd_ph_vbc_3g', data=churn_data, ax=axes[0], hue='churn', alpha=0.7)\nsns.scatterplot(y='arpu_8', x='vbc_3g_8', data=churn_data, ax=axes[1], hue='churn', alpha=0.7)\n\n\n# Limiting the graph to more general upper bound\nplt.ylim(0,10000)\nplt.show()","da873cc5":"# Lets check the relation between recharge amount and local outgoing calls\n\nfig, axes = plt.subplots(1, 2, sharey=True, figsize=(15, 7))\nsns.scatterplot(x='gd_ph_max_rech_amt', y='gd_ph_loc_og_mou', data=churn_data, ax=axes[0],\n                hue='churn', alpha=0.7)\nsns.scatterplot(x='max_rech_amt_8', y='loc_og_mou_8', data=churn_data, ax=axes[1], hue='churn', alpha=0.7)\n\n# Limiting the graph to more general upper bound\naxes[0].set_xlim(0,4000)\naxes[1].set_xlim(0,4000)\nplt.ylim(0,6000)\n\nplt.show()","8834729b":"# Check the effect of max recharge amount on churn\n\nplt.figure(figsize=(15,7))\nsns.scatterplot(x=churn_data.index, y=churn_data['gd_ph_max_rech_amt'] + churn_data['max_rech_amt_8'],\n                hue=churn_data['churn'], alpha=0.7)\nplt.ylim(0,800)\nplt.show()\n","ec6bc06b":"# Incoming from the same service provider vs the recharge amount\n\nfig, axes = plt.subplots(1, 2, sharey=True, figsize=(15, 7))\nsns.scatterplot(x='gd_ph_max_rech_amt', y='gd_ph_std_ic_t2t_mou', data=churn_data, ax=axes[0],\n                hue='churn', alpha=0.7)\nsns.scatterplot(x='max_rech_amt_8', y='std_ic_t2t_mou_8', data=churn_data, ax=axes[1], hue='churn', alpha=0.7)\n\n# Limiting the graph to more general upper bound\naxes[0].set_xlim(0,2000)\naxes[1].set_xlim(0,2000)\nplt.ylim(0,2000)\n\nplt.show()","5a85a9fb":"# Distribution of target variable\n\nsns.distplot(churn_data['churn'])\nplt.show()","ecea5882":"# Cap the features with high outliers\n\ncol_upr_lmt = [('arpu_8', 7000), ('loc_og_mou_8', 4000), ('max_rech_amt_8', 1000 ), ('last_day_rch_amt_8', 1000 ), \n               ('aon', 3000), ('total_mou_8', 4000), ('gd_ph_loc_ic_mou', 3000), ('gd_ph_last_day_rch_amt', 1000 ), \n               ('gd_ph_std_og_mou', 4000), ('gd_ph_max_rech_amt', 1500), ('gd_ph_loc_og_mou', 3000), ('gd_ph_arpu', 7000)]\n\nfor col, value in col_upr_lmt:\n    churn_data[col] = churn_data[col].apply(lambda x : x if x < value else value)\n\nchurn_data[col_boxplot].head()","4f3eee63":"y = churn_data['churn']\nX = churn_data.drop('churn', axis=1)","46280e02":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX[X.columns] = scaler.fit_transform(X[X.columns])","37dd9da0":"churn_data.churn.value_counts()","810f5f7c":"# Use SMOTE to take care of class imbalance\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample(X, y)","b11b6f9a":"y_res.value_counts()","5350d25f":"sns.distplot(y_res)\nplt.show()","c12bed22":"X.shape","844f91b9":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=25)\nX_pca = pca.fit_transform(X_res)\nX_pca.shape","18aa0d9c":"# Split the data into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, train_size=0.7, test_size=0.3, random_state=25)","dd70df59":"import statsmodels.api as sm","2cf2922a":"# Initial logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","77c86bdf":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\n\n# Create LR object\nlogreg = LogisticRegression(solver='liblinear')\n\n# Run RFE for 25 features\nrfe = RFE(logreg, n_features_to_select=25)\nrfe = rfe.fit(X_train, y_train)","29e0b692":"rfe_col = X.columns[rfe.support_]\nrfe_col","14a8c2f2":"# Build model with RFE selected features\nX_train_sm = sm.add_constant(X_train[rfe_col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","e02a2cd7":"# Predict on train data\n\ny_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred_final = pd.DataFrame({'Churn':y_train.values, 'Churn_Prob':y_train_pred})\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","a5135931":"from sklearn import metrics\n\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nprint(confusion)\n\nprint()\n\n# The overall accuracy.\nprint(f'Accuracy : {metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted)}')","3d7f7b67":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[rfe_col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[rfe_col].values, i) for i in range(X_train[rfe_col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","57ef4885":"rfe_col = rfe_col.drop('total_mou_8', 1)","d9b2ebd3":"# Build new model\nX_train_sm = sm.add_constant(X_train[rfe_col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","a93bd67f":"# Predict on train data\n\ny_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred_final = pd.DataFrame({'Churn':y_train.values, 'Churn_Prob':y_train_pred})\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","bb6f9a3c":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nprint(confusion)\n\nprint()\n\n# The overall accuracy.\nprint(f'Accuracy : {metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted)}')","70838e44":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[rfe_col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[rfe_col].values, i) for i in range(X_train[rfe_col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","78600488":"# Drop 'gd_ph_total_mou'\nrfe_col = rfe_col.drop('gd_ph_total_mou', 1)","c8b2cee0":"# Build new model\nX_train_sm = sm.add_constant(X_train[rfe_col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","81542944":"# Predict on train data\n\ny_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred_final = pd.DataFrame({'Churn':y_train.values, 'Churn_Prob':y_train_pred})\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","0f9ad440":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nprint(confusion)\n\nprint()\n\n# The overall accuracy.\nprint(f'Accuracy : {metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted)}')","c2ae7864":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[rfe_col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[rfe_col].values, i) for i in range(X_train[rfe_col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","c7ff0e6a":"# Drop 'loc_ic_mou_8'\nrfe_col = rfe_col.drop('loc_ic_mou_8', 1)","07c1a369":"# Build new model\nX_train_sm = sm.add_constant(X_train[rfe_col])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","56e2e8df":"# Predict on train data\n\ny_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred_final = pd.DataFrame({'Churn':y_train.values, 'Churn_Prob':y_train_pred})\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","838d2bbf":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nprint(confusion)\n\nprint()\n\n# The overall accuracy.\nprint(f'Accuracy : {metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted)}')","2e197f35":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[rfe_col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[rfe_col].values, i) for i in range(X_train[rfe_col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","6cd950f0":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","d4fde8a9":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","7492a37a":"# Let us calculate specificity\nTN \/ float(TN+FP)","7dbbb456":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","99a37912":"# positive predictive value \nprint (TP \/ float(TP+FP))","2df7f02d":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","f364b94e":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False)\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","7bf54d83":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Churn, y_train_pred_final.Churn_Prob, drop_intermediate = False )\ndraw_roc(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","520d575a":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","d15df627":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5, 0.5, 0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","99ef40a0":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","964eecea":"X_test = X_test[rfe_col]\nX_test_sm = sm.add_constant(X_test)\n\n# Predict on test data\n\ny_test_pred = res.predict(X_test_sm)\ny_test_pred_final = pd.DataFrame({'Churn':y_test.values, 'Churn_Prob':y_test_pred})\ny_test_pred_final['predicted'] = y_test_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_test_pred_final.head()","7781990a":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_test_pred_final.Churn, y_test_pred_final.predicted )\nprint(confusion)\n\nprint()\n\n# The overall accuracy.\nprint(f'Accuracy : {metrics.accuracy_score(y_test_pred_final.Churn, y_test_pred_final.predicted)}')","c02020ef":"# Top 10 predictors\n\nabs(res.params).sort_values(ascending=False)[0:11]","d6f5107c":"X_train, X_test, y_train, y_test = train_test_split(X_pca, y_res, train_size=0.7, random_state=25)","4915e32e":"from sklearn.tree import DecisionTreeClassifier\n\n# Initial classifier\nintial_dt = DecisionTreeClassifier(random_state=42, max_depth=10)\nintial_dt.fit(X_train, y_train)\n\n# Train Accuracy\ny_train_pred = intial_dt.predict(X_train)\nprint(f'Train accuracy : {metrics.accuracy_score(y_train, y_train_pred)}')\n\ny_test_pred = intial_dt.predict(X_test)\n\n# Print the report on test data\nprint(metrics.classification_report(y_test, y_test_pred))","75480f19":"# Plot ROC curve\nfrom sklearn.metrics import plot_roc_curve\nplot_roc_curve(intial_dt, X_train, y_train, drop_intermediate=False)\nplt.show()","a3a2f595":"from sklearn.model_selection import GridSearchCV\n\ndt = DecisionTreeClassifier(random_state=42)\n\n# Define parameters\nparams = {\n    \"max_depth\": [2, 3, 5, 10, 20, 30, 40, 50, 100],\n    \"min_samples_leaf\": [5, 10, 20, 50, 100, 250, 500, 800, 1000],\n    \"min_samples_leaf\" : [1, 5, 10, 25, 50, 100]\n}\n\ngrid_search = GridSearchCV(estimator=dt,\n                           param_grid=params,\n                           cv=4,\n                           n_jobs=-1, verbose=1, scoring=\"accuracy\")\n\n# Perform gridsearch\ngrid_search.fit(X_train, y_train)","13f0d95e":"grid_search.best_score_","a9eb8a96":"# Best estimator\ndt_best = grid_search.best_estimator_\ndt_best","73386d80":"y_train_pred = dt_best.predict(X_train)\ny_test_pred = dt_best.predict(X_test)\n\n# Print the report\nprint(metrics.classification_report(y_test, y_test_pred))","bbe7f0b1":"# ROC\nplot_roc_curve(dt_best, X_train, y_train)\nplt.show()","53dfaa1b":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=15, max_depth=10, max_features=5, random_state=25, oob_score=True)\nrf.fit(X_train, y_train)\n\ny_train_pred = rf.predict(X_train)\n\n# Train Accuracy\ny_train_pred = intial_dt.predict(X_train)\nprint(f'Train accuracy : {metrics.accuracy_score(y_train, y_train_pred)}')\n\ny_test_pred = rf.predict(X_test)\n\n# Print the report\nprint(metrics.classification_report(y_test, y_test_pred))\n\n# Plotting ROC\nplot_roc_curve(rf, X_train, y_train)\nplt.show()","0758c120":"rf = RandomForestClassifier(random_state=25, n_jobs=-1)\n\n# Define parameters\nparams = {\n    'max_depth': [2, 3, 5, 10, 20, 30],\n    'min_samples_leaf': [5, 10, 20, 50, 100],\n    'n_estimators': [10, 25, 50, 100]\n}\n\ngrid_search = GridSearchCV(estimator=rf,\n                           param_grid=params,\n                           cv = 4,\n                           n_jobs=-1, verbose=1, scoring=\"accuracy\")\n\ngrid_search.fit(X_train, y_train)","5ef94120":"grid_search.best_score_","21f26d88":"rf_best = grid_search.best_estimator_\nrf_best","6a4eda9f":"y_train_pred = rf_best.predict(X_train)\ny_test_pred = rf_best.predict(X_test)\n\n# Print the report\nprint(metrics.classification_report(y_test, y_test_pred))","fec677e3":"plot_roc_curve(rf_best, X_train, y_train)\nplt.show()","a52b98d5":"from sklearn.ensemble import AdaBoostClassifier","cd6a220b":"# Create a shallow tree as weak learner\nshallow_tree = DecisionTreeClassifier(max_depth=2, random_state = 100)\n\n# Fit the shallow decision tree \nshallow_tree.fit(X_train, y_train)\n\n# Test error\ny_pred = shallow_tree.predict(X_test)\nscore = metrics.accuracy_score(y_test, y_pred)\nscore","bfa5f8ca":"# Adaboost with shallow tree as base estimator\n\n# Define the number of trees to be used or estimators\nestimators = list(range(1, 150, 10))\n\n# Loop through the estimators\nadaboost_scores = []\nfor num_est in estimators:\n    adaboost = AdaBoostClassifier(base_estimator=shallow_tree, n_estimators = num_est, random_state=25)\n    \n    adaboost.fit(X_train, y_train)\n    y_pred = adaboost.predict(X_test)\n    score = metrics.accuracy_score(y_test, y_pred)\n    adaboost_scores.append(score)","d1adcacc":"# Plot the scores corrosponding to number of estimators to find the best possible number of estimator\nplt.plot(estimators, adaboost_scores)\nplt.xlabel('n_estimators')\nplt.ylabel('accuracy')\nplt.show()","b58fa45c":"adaboost_best = AdaBoostClassifier(base_estimator=shallow_tree, n_estimators = 200, random_state=25)\nadaboost_best.fit(X_train, y_train)\ny_pred = adaboost_best.predict(X_test)\n\nprint(metrics.classification_report(y_test, y_test_pred))","cf3a50ad":"final_model = RandomForestClassifier(max_depth=30, min_samples_leaf=5, n_jobs=-1,\n                       random_state=25)","f295aa1f":"y_train_pred = rf_best.predict(X_train)\ny_test_pred = rf_best.predict(X_test)\n\n\n\n# Print the report\nprint(\"Report on train data\")\nprint(metrics.classification_report(y_train, y_train_pred))\n\nprint(\"Report on test data\")\nprint(metrics.classification_report(y_test, y_test_pred))","953f3feb":"> Since the VIF for `total_mou_8` is very high we will drop it","48845a9b":"## 3. Random Forest","caa8f5be":"## 4. Adaboost","96ff25f0":"## PCA","9fd1b163":"> #### Observation\n> * Users who have max recharge amount on the higher end and still have low incoming call mou during the good pahse, churned out more","35ed7f27":"> #### Observation\n> * Though the varible is not skwed it is higly imbalanced, the number of non-churners in the dataset is around 94%\n> * We will handle this imbalance using SMOTE algorithm","2cbd8da4":"> #### Observations\n> * From the above plots we can define following upper limits to the sepected variables\n\n> | Feature | Value |\n> | --- | --- |\n> | arpu_8 | 7000|\n> | loc_og_mou_8 | 4000|\n> | max_rech_amt_8 | 1000 |\n> | last_day_rch_amt_8 | 1000 |\n> | aon | 3000 | \n> | total_mou_8 | 4000 | \n> | gd_ph_loc_ic_mou | 3000 |\n> | gd_ph_last_day_rch_amt | 1000 |\n> | gd_ph_std_og_mou | 4000 | \n> | gd_ph_max_rech_amt | 1500 |\n> | gd_ph_loc_og_mou | 3000 |\n> | gd_ph_arpu | 7000 |\n\n> * We will make these changes post exploration of other features\n","609fd59f":"> By looking at the dataframe we can see a few of the columns, like `circle_id` have only value. These kind of feature suggest no variance and hence wont be contributing anything to our target variable. ","31f2c92d":"### Hyperparameter tuning","3238a398":"# Data Cleaning & EDA","7097f52c":"> Let us check the correlation between the features and if we find any feature with high correlation we will go ahead and drop them","b1b90262":"> We are getting an accuracy of 95% on test data, with Random forest","1ddceb72":"> Since rest of the columns are break out of totals into granualr details we will not merge them into one, instead we will create new features that will each account for the good pahse by averaging out the values for 6th and 7th month","bd88b568":"> #### Observation\n> * We can see that users who had the max recharge amount less tha 200 churned more","9f3d32d2":"# Conclusions","533d3123":"* Given our bussines probelm, to retain their customers, we need higher recall. As giving an offer to an user not going to churn will cost less as compared to loosing a customer and bring new customer, we need to have high rate of correctly identifying the true positives, hence recall.\n\n* When we compare the models trained we can see the tuned random forest and ada boost are performing the best, which is highest accuracy along with highest recall i.e. 95% and 97% respectively. So, we will go with random forest instead of adaboost as that is comparetively simpler model.\n","037bc875":"## Strategies to Manage Customer Churn\n\nThe top 10 predictors are :\n\n| Features |\n| ---------- |\n| loc_og_mou_8 |\n| total_rech_num_8 |\n| monthly_3g_8 |\n| monthly_2g_8 |\n| gd_ph_loc_og_mou |\n| gd_ph_total_rech_num |\n| last_day_rch_amt_8 |\n| std_ic_t2t_mou_8 |\n| sachet_2g_8 |\n| aon |\n\n* We can see most of the top predictors are from the action phase, as the drop in engagement is prominent in that phase\n\nSome of the factors we noticed while performing EDa which can be clubed with these inshigts are:\n1. Users whose maximum recharge amount is less than 200 even in the good pahse, should have a tag and re-evaluated time to time as they are more likely to churn\n2. Users that have been with the network less than 4 years, should be monitored time to time, as from data we can see that users who have been associated with the network for less than 4 years tend to churn more\n3. MOU is one of the major factors, but data especially VBC if the user is not using a data pack if another factor to look out","0cd9478d":" > #### Observation\n > * We can see that the users who were using very less amount of VBC data and yet were generating high revenue churned\n > * Yet again we see that the revenue is higher towards the lesser consumption side","770ae274":"The churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. The attributes you need to use to tag churners are:\n\n   - total_ic_mou_9\n\n   - total_og_mou_9\n\n   - vol_2g_mb_9\n\n   - vol_3g_mb_9","b3f01c72":"> The VIF values looks good now, we can proceed further","6d5b1165":"# Data Preperation","3213763d":"## 1. Logistic Regression","87767ea5":"High-value customers as follows: Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).","23812a1c":"# High Value Customer","191c6266":"### RFE for Feature Selection","9091845d":"> For logistic regression we will be using the unaltered X and y so that we can use RFE for feature selection instead of PCA, to find out the strong predictor of churn ","c1355227":"> #### Observation\n> * We can see almost every columns has some outliers, while most of them are becuase there are 0.0 as the service was not used some are actual outliers\n> * Since we don't have actuall bussines people to check the factfulness of the data, we will cap those features","ebf1de57":"## Standardization","5d46608c":"### Plotting ROC","5258456c":"## Handling Class Imbalance","76edc708":"> #### Observation\n> * Though we cannot see a clear pattern here, but we can notice that the mojority of churners had a tenure of less than 4 years","b9d3385d":"# Model Building","d2d54baa":"### Hyperparameter tuning","d3149f52":"## Driving new features","3eb33931":"> #### Observations\n> * Users who were recharging with high amounts were using the service for local uses less as compared to user who did lesser amounts of recharge\n> * Intuitevly people whose max recharge amount as well as local out going were very less even in the good phase churned more ","7a614141":"> We are getting an accuracy of 90% on test data, with decission tree","90853e8b":"> #### Observation\n> * We can clearly see that MOU have dropped significantly for the churners in the action pahse i.e 8th month, thus hitting the revenue generated from them\n> * It is also interesting that though the MOU is between 0-2000, the revenue is highest in that region that tells us these users had other services that were boosting the revenue","da4d334b":"# Tagging Churn","ce52efe8":"> Now the class is balanced and the target variable is not skwed ","77ec7b46":"### Optimal cutoff point","674fbc5b":"We assume that there are three phases of customer lifecycle :\n\n - The \u2018good\u2019 phase: In this phase, the customer is happy with the service and behaves as usual.\n\n - The \u2018action\u2019 phase: The customer experience starts to sore in this phase, for e.g. he\/she gets a compelling offer from a  competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the \u2018good\u2019 months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor\u2019s offer\/improving the service quality etc.)\n\n - The \u2018churn\u2019 phase: In this phase, the customer is said to have churned. We define churn based on this phase. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available to you for prediction. Thus, after tagging churn as 1\/0 based on this phase, we discard all data corresponding to this phase.\n \nIn this case, since we are working over a four-month window, the first two months are the \u2018good\u2019 phase, the third month is the \u2018action\u2019 phase, while the fourth month is the \u2018churn\u2019 phase.","aa9b9983":"> The point 0.5 is optimal for our model, so we will keep that","4edba85d":"> Since rest of columns have even less than 5% of the data missing we can drop the rows with the missing values ","ddeeb441":"## 2. Decission Tree","be7fe3a7":"> `vbc` columns doesn't have number of month as suffix so it seemed to have missed out let's avergae out the columns for this feature too","759d47e0":"> * So using Logistic regression we are geting an accuracy of 78.5% on train data and 78.8% on test data\n> * We can clearly see most of the critical features are form the `action` phase, which is inline with the bussiness understanding that `action` phase needs more attention"}}