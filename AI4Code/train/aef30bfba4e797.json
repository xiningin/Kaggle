{"cell_type":{"6cd5758f":"code","6280a175":"code","585213be":"code","b61c01c5":"code","1796f2d3":"code","ed2f164a":"code","e1b2d927":"code","9efdc06e":"code","06dbd656":"code","733a78f4":"code","9f2ca789":"code","b6f61e56":"markdown","f8414d98":"markdown"},"source":{"6cd5758f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6280a175":"#before of all we read the data that is we will use\ndata = pd.read_csv(\"..\/input\/data.csv\")","585213be":"#we can see class\/feature of our data\ndata.columns","b61c01c5":"data.head()\n#id and unnamed32 is unnecessary for our model and we change M(malignant) and B(benign) replace 0,1","1796f2d3":"#firstly we drop two columns\ndata.drop([\"id\",\"Unnamed: 32\"],axis = 1,inplace = True)\n#we change value of diognose M = 0 and B = 1(we use this diversity in logistic regression)\ndata[\"diagnosis\"].replace(\"M\",0,inplace = True)\ndata[\"diagnosis\"].replace(\"B\",1,inplace = True)","ed2f164a":"#we select x,y axis and we normalize our data\ny = data.diagnosis.values\nx_data = data.drop(\"diagnosis\",axis=1)\nx = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))","e1b2d927":"#we separate train and test data with sklearn selection model\n#You can thnk this x_train for learn and y_train is answer of x_train and finally we testing our data with x_test andy_test\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y)\n","9efdc06e":"#You can see shape of our data it is available for preparing model\nprint(\"xtrain:{}\".format((x_train).shape))\nprint(\"y_train:{}\".format((y_train).shape))\nprint(\"xtest:{}\".format((x_test).shape))\nprint(\"ytest:{}\".format((y_test).shape))","06dbd656":"#We did \nfrom sklearn.linear_model import LogisticRegression\nlgr = LogisticRegression(max_iter = 200)\nlgr.fit(x_train,y_train)\nprint(\"our accuracy is:{}\".format(lgr.score(x_test,y_test)))","733a78f4":"#We can evaluate our model so and we have y_predict and y_true(y_test)\nfrom sklearn.metrics import confusion_matrix\ny_true = y_test \ny_pred = lgr.predict(x_test) #Predict data for eveluating \ncm = confusion_matrix(y_true,y_pred)\n\n\n","9f2ca789":"#We draw heatmap for showing confusion matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nf,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True,linewidth = 1,fmt =\".0f\",ax = ax)","b6f61e56":"**Welcome**\n\nWelcome to my logistic regression kernel.We will answer questions is \"How \u0131 make logistic regression\".I hope you like my kernel any more this kernel belong us.I am sory for my mistake","f8414d98":"**Conclusion**\n\nIf you write this conclusion you understand we make lr models :) I hope you learn something to me.You don't forget write your opinion and my mistake"}}