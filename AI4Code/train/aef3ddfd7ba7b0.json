{"cell_type":{"6b2a1088":"code","7f2487d5":"code","78751364":"code","6ffb8797":"code","e7c72861":"code","05ed7443":"code","d6c43b05":"code","1e20acbd":"code","cb9e9677":"code","98c14e0b":"code","7e4cd347":"code","341287f8":"code","ffbda2f1":"code","99e165bd":"code","dda28315":"code","ef49beef":"code","1bb6da7b":"code","3702325c":"code","35a7a665":"code","dc21f766":"code","fc6b8e23":"code","8bc3cce4":"code","8b2c8630":"code","77ba268d":"code","50750432":"markdown","f0905b21":"markdown","3a43dbab":"markdown","df166266":"markdown","1b11bd33":"markdown","3b1dcfbc":"markdown","db92f31b":"markdown","0e5b255d":"markdown","cc108e9e":"markdown","27176936":"markdown","acc8ea6a":"markdown","4cbdbedb":"markdown","fd974caa":"markdown","290505ad":"markdown","40c1fafc":"markdown","45a7303d":"markdown","5471aa26":"markdown","7b9273a0":"markdown","6153b63e":"markdown","0cfc1347":"markdown","22658bd0":"markdown","4887ae05":"markdown","c91c58ad":"markdown","1bb9caee":"markdown","8e7e117c":"markdown","2099c8d7":"markdown","9884b358":"markdown"},"source":{"6b2a1088":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7f2487d5":"# Load data\ntrain = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')\nsub = pd.read_csv('..\/input\/cat-in-the-dat\/sample_submission.csv')\nprint(train.shape)\nprint(test.shape)","78751364":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\ntrain.head()","6ffb8797":"bin_cols = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']\n# loop to get column and the count of plots\nfor n, col in enumerate(train[bin_cols]): \n    plt.figure(n)\n    sns.countplot(x=col, data=train, hue='target', palette='husl')","e7c72861":"train['bin_3'] = train['bin_3'].replace(to_replace=['F', 'T'], value=['0', '1']).astype(int)\ntrain['bin_4'] = train['bin_4'].replace(to_replace=['Y', 'N'], value=['1', '0']).astype(int)\ntest['bin_3'] = test['bin_3'].replace(to_replace=['F', 'T'], value=['0', '1']).astype(int)\ntest['bin_4'] = test['bin_4'].replace(to_replace=['Y', 'N'], value=['1', '0']).astype(int)\n# train['bin_3'].astype(int)\n# train['bin_4'].astype(int)","05ed7443":"train.head(3)","d6c43b05":"#Drop ID and seperate target variable\ntarget = train['target']\ntrain_id = train['id']\ntest_id = test['id']\ntrain.drop(['target', 'id'], axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)\n\nprint(train.shape)\nprint(test.shape)","1e20acbd":"ord_cols = ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']\n\nfor i in ord_cols:\n    print(\"The number of unique values in {} column is : {}\".format(i, train[i].nunique()))\n    print(\"The unique values in {} column is : \\n {}\".format(i, train[i].value_counts()[:7]))\n    print('\\n')","cb9e9677":"# credits for mapper code : https:\/\/www.kaggle.com\/gogo827jz\/catboost-baseline-with-feature-importance\n\nmapper_ord_1 = {'Novice': 1, 'Contributor': 2, 'Expert': 3, 'Master': 4, 'Grandmaster': 5}\n\nmapper_ord_2 = {'Freezing': 1, 'Cold': 2, 'Warm': 3, 'Hot': 4,'Boiling Hot': 5, 'Lava Hot': 6}\n\nmapper_ord_3 = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, \n                'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15}\n\nmapper_ord_4 = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, \n                'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15,\n                'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, \n                'W': 23, 'X': 24, 'Y': 25, 'Z': 26}\n\nfor col, mapper in zip(['ord_1', 'ord_2', 'ord_3', 'ord_4'], [mapper_ord_1, mapper_ord_2, mapper_ord_3, mapper_ord_4]):\n    train[col+'_oe'] = train[col].replace(mapper)\n    test[col+'_oe'] = test[col].replace(mapper)\n    train.drop(col, axis=1, inplace=True)\n    test.drop(col, axis=1, inplace=True)","98c14e0b":"from sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder(categories='auto')\nencoder.fit(train.ord_5.values.reshape(-1, 1))\ntrain.ord_5 = encoder.transform(train.ord_5.values.reshape(-1, 1))\ntest.ord_5 = encoder.transform(test.ord_5.values.reshape(-1, 1))","7e4cd347":"train.ord_5[:5]","341287f8":"train[['ord_1_oe','ord_2_oe','ord_3_oe','ord_4_oe','ord_5','ord_0']].info()","ffbda2f1":"nom_cols = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n\nfor i in nom_cols:\n    print(\"The number of unique values in {} column is : {}\".format(i, train[i].nunique()) )\n        ","99e165bd":"%%time\nfrom sklearn.preprocessing import OneHotEncoder\none=OneHotEncoder()\ntrain_ohe1 = one.fit_transform(train)\ntest_ohe1 = one.fit_transform(test)\n# ohe_obj_train = one.fit(train)\n# ohe_obj_test = one.fit(test)\n\n# train_ohe1 = ohe_obj_train.transform(train)\n# test_ohe1 = ohe_obj_test.transform(test)\nprint(train_ohe1.shape)\nprint(train_ohe1.dtype)\nprint(test_ohe1.shape)\nprint(test_ohe1.dtype)","dda28315":"# %%time\n# nom_col = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8','nom_9']\n\n# traintest = pd.concat([train, test])\n# traintest_ohe = pd.get_dummies(traintest, columns=nom_col, drop_first=True, sparse=True)\n# train_ohe = traintest_ohe.iloc[:train.shape[0], :]\n# test_ohe = traintest_ohe.iloc[train.shape[0]:, :]\n\n# print(train_ohe.shape)\n# print(test_ohe.shape)","ef49beef":"def logistic(X,y):\n    X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.2)\n    lr=LogisticRegression()\n    lr.fit(X_train,y_train)\n    y_pre=lr.predict(X_test)\n    print('Accuracy : ',accuracy_score(y_test,y_pre))","1bb6da7b":"logistic(train_ohe1,target)","3702325c":"%%time\nnom_col = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8','nom_9']\nfrom sklearn import model_selection, preprocessing, metrics\nle = preprocessing.LabelEncoder()\ntraintest = pd.concat([train, test])\n\nfor col in nom_col:\n    traintest[col] = le.fit_transform(traintest[col])\n\ntrain_le = traintest.iloc[:train.shape[0], :]\ntest_le = traintest.iloc[train.shape[0]:, :]\n\nprint(train_le.shape)\nprint(test_le.shape)","35a7a665":"train_le.head()","dc21f766":"logistic(train_le,target)","fc6b8e23":"# LGBM on LabelEncoded data\nimport lightgbm as lgb\nnum_round = 50000\n\nparam = {'num_leaves': 64,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': 5,\n         'learning_rate': 0.001,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 44,\n         \"metric\": 'auc',\n         \"verbosity\": -1}\n\nX_train,X_test,y_train,y_test=train_test_split(train_le,target,random_state=42,test_size=0.2)\n\ntrain = lgb.Dataset(X_train, label=y_train)\ntest = lgb.Dataset(X_test, label=y_test)\n\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, \n                early_stopping_rounds = 500)\n","8bc3cce4":"X_train_ohe,X_test_ohe,y_train_ohe,y_test_ohe=train_test_split(train_ohe1,target,random_state=42,test_size=0.2)\n\ntrain = lgb.Dataset(X_train_ohe, label=y_train_ohe)\ntest = lgb.Dataset(X_test_ohe, label=y_test_ohe)\n\nclf_ohe = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, \n                early_stopping_rounds = 500)\n","8b2c8630":"y_preds = clf.predict(test_le)","77ba268d":"sub['target'] = y_preds\nsub.to_csv('lgb_model.csv', index=False)","50750432":"Looks Good!!\n\n### Now let's Check the dtypes for the ordinal variables after transformations","f0905b21":"## Nominal V\/S Ordinal data\n\nNominal data does not have any sort of order in the data points\/variables. For example, the results of a test could be each classified nominally as a \"pass\" or \"fail\".\n\nWhereas on the other hand, Ordinal data is grouped according to some sort of ranking system, that is, it orders the data. For example, test results could be grouped in descending order by grade: A, B, C, D, E and F.\n\n## Example of nominal data\n\n![](https:\/\/miro.medium.com\/max\/1185\/0*iKsDex5fUBQoYTju.png)\n\n## Example of ordinal data\n\n![](https:\/\/miro.medium.com\/max\/1038\/1*ychLO4DAe5cvD1UwUuvjZw.png)","3a43dbab":"# Let's take a step by step approach for each feature variable\n## 1. Binary variables (bin_0 through bin_4)","df166266":"# A detailed guide to different encoding schemes to deal with categorical features in data and their implications on the model's performance.","1b11bd33":"Converting bin_3 and bin_4 from Yes\/No (Y\/N) and True\/False (T\/F) to binary (1\/0)","3b1dcfbc":"As far as accuracy of logistic model is concerned, OHE has slightly edged LE.","db92f31b":"Checking the dataframe after transformation:","0e5b255d":"## Yet another efficient way of encoding ordinal features is: Sklearn's OrdinalEncoder\n* OrdinalEncoder falls in the same family of SKlearn's encoders alongside LabelEncoder and OneHotEncoder.\n* It is a fairly straightforward wat to encode Orninal features. \n* This technique has been applied on the ordinal features in this kernel below.","cc108e9e":"**Will try some more permutation\/combinations of encoding techniques and model architectures in further versions.**","27176936":"## Now let's fit a LGB Model on both LabelEncoded dataset as well as OneHotEncoded dataset and check their respective performance","acc8ea6a":"## Logical ordering for ordinal feautes seems to be as follows:\n\nord_0 : [1, 2, 3]\n\nord_1 : ['Novice', 'Contributor','Expert', 'Master', 'Grandmaster']\n\nord_2 : ['Freezing', 'Cold', 'Warm', 'Hot', 'Boiling Hot', 'Lava Hot']\n\nord_3 : ['a', 'b', 'c', 'd', 'e', 'f', 'g','h', 'i', 'j', 'k', 'l', 'm', 'n', 'o']\n\nord_4 : ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I','J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', ... 'Z']\n\nLet's encode ord_1 to ord_4 since the numbers of their unique values are small","4cbdbedb":"Expand the output of above cell to view the model's training and validation AUC score.\n\n### Submitting LGBM + LabelEncoder results!!","fd974caa":"**Kindly upvote if you find the kernel helpful :)  **","290505ad":"## Time to replace OneHotEncoder with LabelEncoder and checking the performance","40c1fafc":"This dataset contains only categorical features and since dealing with categorical features is such a common task and important skill to master, this kernel will focus on applying different techniques to deal with categorical data and their implications on the performance of the model.\n\nThe dataset contains the following types of categorical features:\n\n* binary features (bin_*)\n* low- and high-cardinality nominal features (nom_*)\n* low- and high-cardinality ordinal features (ord_*)\n* (potentially) cyclical features (day (of the week) and month features)\n\nWe would be predicting the probability [0, 1] of the binary target column.\n","45a7303d":"## Lets discuss the two most popular techniques for encoding categorical features:\n\n* Label Encoding\n\nLabel encoding assigns each unique value to a different integer. (good fit for ordinal features)\n\nThis approach assumes an ordering of the categories: \"Never\" (0) < \"Rarely\" (1) < \"Most days\" (2) < \"Every day\" (3).\n\nFor tree-based models (like decision trees and random forests), you can expect label encoding to work well with ordinal variables.\n\n* One-Hot Encoding\n\nOne-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data.\n\nIn contrast to label encoding, one-hot encoding does not assume an ordering of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data (e.g., \"Red\" is neither more nor less than \"Yellow\")\n\nOne-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values).\n\n![](https:\/\/chrisalbon.com\/images\/machine_learning_flashcards\/One-Hot_Encoding_print.png)","5471aa26":"Let's take a look at ord_5 after transormation","7b9273a0":"Also, let's already drop the ID column which is insignificant for us. Also, let's seperate the target column.","6153b63e":"## Let's dive in the competition dataset","0cfc1347":"### For nominal features, we will perform two types of encodings (viz, LabelEncoding and One Hot Encoding). \n\n### Moreover, we will check the performances for individual encoding techniques by applying a couple of models (LightGBM and logistic).\n","22658bd0":"Expand the output of above cell to view the model's training and validation AUC score.\n\n### Validation set's AUC using LabelEncoder in LGBM comes approximately to 0.778382\n\n## Applying LGBM on OneHotEncoded data and checking it's performance","4887ae05":"## Applying a logistic model on OneHotEncoded dataset and checking the performance","c91c58ad":"### Finding the number of unique values for each variable and taking a glimpse of the values","1bb9caee":"for ord_5, we have high cardinality. Lets apply  OrdinalEncoder with \"categories=\u2019auto\u2019\" to it.","8e7e117c":"# Now let's Get on with nominal features (nom_0 through nom_9)","2099c8d7":"### Reason of commenting above code:\n\n* I tried to perform One hot Encoding using Pandas' getdummies() as well as sklearn OneHotEncoding.\n* The wall run time for sklearn OneHotEncoding was remarkably less at at couple seconds compared to Pandas getDummies which took north of 4 minutes.","9884b358":"# Transformation of ordinal features (ord_0 through ord_5)"}}