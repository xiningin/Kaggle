{"cell_type":{"507becb6":"code","8ba4e8a2":"code","91df33d3":"code","59aa1d0d":"code","dbec30e9":"code","340fef98":"code","6230d372":"code","6f038fde":"code","7f2f3d75":"code","9378998a":"code","d66141a1":"code","80bff2af":"code","85493a23":"code","ea2afc74":"code","33c52f51":"code","95f2bb4f":"code","67c3be5e":"code","35487852":"code","abb88450":"code","a6bcef34":"code","01c6a7ef":"code","f293921d":"code","a519cf69":"code","fdc5707c":"code","a9b47d47":"code","3b7e8017":"code","476a5e21":"code","2dd04b87":"code","76a18dcb":"code","5f0d47d7":"code","f4bef6a6":"code","704eac50":"code","87b13b56":"code","9ee9ff71":"code","81c91e57":"code","63e45f96":"code","ecc9991e":"code","a250f758":"code","de94543b":"code","251a1d86":"code","f6d1c308":"code","6da1ebac":"code","690e869c":"code","73ceb0d9":"code","a684e835":"code","7c6395a8":"code","4324721a":"markdown","fdde6524":"markdown","175b5a18":"markdown","e8b16216":"markdown","482828cb":"markdown","e7e9fece":"markdown","1438e76a":"markdown","c5669879":"markdown","49030c7f":"markdown","bbafa567":"markdown","a19e9513":"markdown","9b1d5bad":"markdown","84c1a65f":"markdown","62c1888e":"markdown","afa1c861":"markdown","087cd165":"markdown","c3f417ce":"markdown","aa0c8f2e":"markdown","c019f711":"markdown","c93fbbec":"markdown","45bc556b":"markdown","791c5662":"markdown"},"source":{"507becb6":"import numpy as np\nimport pandas as pd","8ba4e8a2":"data = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")","91df33d3":"data","59aa1d0d":"X = data[\"review\"]\ny = data[\"sentiment\"]","dbec30e9":"X.shape, y.shape","340fef98":"X[0][:250]","6230d372":"import re\nfor i in range(X.shape[0]):\n    X[i] = re.sub('[^A-Za-z]', ' ', X[i])","6f038fde":"X[0][:250]","7f2f3d75":"for i in range(X.shape[0]):\n    X[i] = X[i].split(\" \")\n    X[i] = [w for w in X[i] if w != \"\"]","9378998a":"import nltk\nnltk.download(\"punkt\")","d66141a1":"from nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()","80bff2af":"for i in range(X.shape[0]):\n    X[i] = [stemmer.stem(w) for w in X[i]]","85493a23":"\" \".join(X[0][:40])","ea2afc74":"for i in range(X.shape[0]):\n    X[i] = [w.lower() for w in X[i]]","33c52f51":"\" \".join(X[0][:40])","95f2bb4f":"nltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nstopwords_ = stopwords.words(\"english\")\n\",\".join(stopwords_)","67c3be5e":"for i in range(X.shape[0]):\n    X[i] = [w for w in X[i] if w not in stopwords_]","35487852":"\" \".join(X[0][:40])","abb88450":"word_dict = {}\nfor i in range(X.shape[0]):\n    for w in X[i]:\n        if w not in word_dict:\n            word_dict[w] = 0\n        word_dict[w] += 1\nword_dict = sorted(word_dict.items(), key= lambda x : x[1], reverse=True)\n\n# print most used 20 words\n\ncnt = 0\nfor k,v in word_dict:\n    cnt += 1\n    if cnt == 21:\n        break\n    print(k,v)","a6bcef34":"text = \"\"\ncnt = 0\nfor k,v in word_dict:\n    cnt += 1\n    if cnt == 5000:\n        break\n    for i in range(v):\n        text += k + \" \"\n\nimport matplotlib.pyplot as plt\n\ndef plot_cloud(wordcloud):\n    plt.figure(figsize=(40, 30))\n    plt.imshow(wordcloud) \n    plt.axis(\"off\");\n    \nfrom wordcloud import WordCloud, STOPWORDS\nwordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False, stopwords = STOPWORDS).generate(text)\nplot_cloud(wordcloud)","01c6a7ef":"most_used_words = {}\ncnt = 0\nindex = 0\nfor word, freq in word_dict:\n    most_used_words[word] = index\n    index += 1\n    cnt += 1\n    if cnt == 5000:\n        break\nlist(most_used_words.items())[0:10]","f293921d":"X_train = []\nfor i in range(X.shape[0]):\n    list_ = []\n    for w in X[i]:\n        try:\n            list_.append(most_used_words[w])\n        except:\n            pass\n    X_train.append(list_)\nfor i in X_train[0]:\n    print(i, end=\" \")","a519cf69":"leng = 0\nfor i in range(len(X_train)):\n    leng += len(X_train[i])\nleng \/ len(X_train)","fdc5707c":"X_train = np.array(X_train)","a9b47d47":"import tensorflow as tf\nfrom tensorflow import keras\nX_train = keras.preprocessing.sequence.pad_sequences(X_train, 125)","3b7e8017":"print(X_train.shape)\nfor i in X_train[0]:\n    print(i, end=\" \")","476a5e21":"y[0:5]","2dd04b87":"y = [1 if i==\"positive\" else 0 for i in y]\ny[0:5]","76a18dcb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_train, np.array(y), test_size = 0.2)","5f0d47d7":"lstm_model = keras.Sequential()\nlstm_model.add(keras.layers.Embedding(5000, 32, input_length=125))\nlstm_model.add(keras.layers.LSTM(100, return_sequences = True))\nlstm_model.add(keras.layers.Dropout(0.5))\nlstm_model.add(keras.layers.LSTM(100, return_sequences = False))\nlstm_model.add(keras.layers.Dropout(0.5))\nlstm_model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\nlstm_model.compile(optimizer=\"rmsprop\", loss=keras.losses.binary_crossentropy, metrics=\"accuracy\")","f4bef6a6":"learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(\n    monitor = \"val_accuracy\",\n    factor = 0.5,\n    patience = 3,\n    verbose = 1,\n    min_lr = 0.00001\n)\nearly_stopping = keras.callbacks.EarlyStopping(patience=5, verbose=1)\nmcp_save = keras.callbacks.ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min', verbose = 1)","704eac50":"import matplotlib.pyplot as plt\ndef plot_history(history):\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","87b13b56":"history = lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data = (X_test, y_test),\n                   callbacks = [learning_rate_reduction, early_stopping, mcp_save])\nplot_history(history)","9ee9ff71":"lstm_model.load_weights('.mdl_wts.hdf5')","81c91e57":"gru_model = keras.Sequential()\ngru_model.add(keras.layers.Embedding(5000, 32, input_length=125))\ngru_model.add(keras.layers.GRU(100, return_sequences = True, activation = \"tanh\"))\ngru_model.add(keras.layers.Dropout(0.5))\ngru_model.add(keras.layers.GRU(100, return_sequences = False, activation = \"tanh\"))\ngru_model.add(keras.layers.Dropout(0.5))\ngru_model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\ngru_model.compile(optimizer=\"rmsprop\", loss=keras.losses.binary_crossentropy, metrics=\"accuracy\")","63e45f96":"history = gru_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data = (X_test, y_test),\n                   callbacks = [learning_rate_reduction, early_stopping, mcp_save])\nplot_history(history)","ecc9991e":"gru_model.load_weights('.mdl_wts.hdf5')","a250f758":"def predict(review):\n    #\u00a0remove non alphabetic characters\n    review = re.sub('[^A-Za-z]', ' ', review) \n    \n    # stemming\n    review = review.split(\" \")\n    review = [w for w in review if w != \"\"]\n    review = [stemmer.stem(w) for w in review]\n    \n    # remove stepwords\n    review = [w for w in review if w not in stopwords_]\n    \n    # get bag of words\n    list_ = []\n    for w in review:\n        try:\n            list_.append(most_used_words[w])\n        except:\n            pass\n    test = [list_]\n    test = np.array(test)\n    test = keras.preprocessing.sequence.pad_sequences(test, 125)\n    pred = gru_model.predict(test)[0][0]\n    print(\"Positive : \", pred)\n    print(\"Negative : \", 1-pred)","de94543b":"negative_review = \"Where to start???? Very cheap story,bad acting,cant understand other people reviews ,especially about Florence acting,she have so stubborn face exprecions,anoying voice overal. Sarcasm all the time through the movie and not funny at all in any moment. To much talking and almost none action. Avoid this at any stake,dont waste money in cinema.\"","251a1d86":"negative_review_2 = \"VERY BAD SCRIPT first of all.There are sooooo many characters that I can hardly see Natasha. What the hell is wrong with you people ?! A lot of CGI which sucks.Unrealistic fight scenes in the middle of the town where pedestrians are not reacting........really?Not to mention the fake Russian accent of the actors :))))) Thank God i didn't have to pay to see such a bad movie.You failed.\"","f6d1c308":"positive_review = \"Johansson and Pugh knock this movie completely out of the park. Their action is gripping, their grief and trauma are convincing & heart-wrenching, and their chemistry is outstanding - the best 'buddy' duo in the MCU, I'd say (outshining Falcon & the Winter Soldier, who I like, et.al.) An added plus is that they are both gorgeous.The story is well-written. Natasha has a chance to settle scores and quiet demons from her past, as well as reunite with those she thought she'd lost. Underneath it all is a sobering fictional example of the all-too-real international problem of human trafficking.If you love Marvel movies, you'll have a blast watching this one (unless, of course, you are a misogynist like some reviewers I have read on this site.)\"","6da1ebac":"positive_review_2 = \"This was really fun. Frances Pugh was SUPPOSED to carry the movie as she is in the new Hawkeye series this year. It was fun to see one of the human superheros in action, no special powers. It is like Jason Bourne which is great. I'm glad we didn't spend time watching her be tortured as a child. Very exciting and heartwarming at the same time. No avenger cameos to detract from the film. It's fun and different.\"","690e869c":"predict(negative_review)","73ceb0d9":"predict(negative_review_2)","a684e835":"predict(positive_review)","7c6395a8":"predict(positive_review_2)","4324721a":"**We will remove non alphabetic characters, best way to do that is [regex](https:\/\/docs.python.org\/3\/library\/re.html) method**","fdde6524":"## 4.Create Bag of Words for every review","175b5a18":"**By removing stop words we remove the low-level information from our text in order to give more focus to the important information. This words does not effect sentiment of the review.**","e8b16216":"# 2.Stemming","482828cb":"# 1. Remove Non Alphabetic Characters","e7e9fece":"**As you can see above in stemming operation we extract root of the word. We will get rid of prefixes and suffixes. Before stemming operation we need to seperate sentences to words.**","1438e76a":"**We will prepare our labels for deep learning algorithm**","c5669879":"![image.png](attachment:dd72bd5b-7173-4eef-a55d-b4fc92209609.png)","49030c7f":"**Sort words by frequency**","bbafa567":"We will continue with GRU (Gated recurrent unit) model","a19e9513":"**We will use embedding algorithm in our RNN deep learning model. Word embeddings are commonly used in many Natural Language Processing (NLP) tasks because they are found to be useful representations of words and often lead to better performance in the various tasks performed**","9b1d5bad":"**Get average length of lists**","84c1a65f":"**We will get 5000 most used words and store another dictionary**","62c1888e":"## 3.Remove Stop Words","afa1c861":"![image.png](attachment:67c9dc9f-f7c2-4295-a41d-d6028bdb6783.png)","087cd165":"**Sentiment Analysis very used topic in nowadays. Consider a very large company, this company will get thousands review about their products everyday. Company should automate this with basic deep learning algorithm instead of employing many people for this job. Another example politicans. Politicans review their moves with sentiment analysis by looking social media platforms. The biggest example of this [Donald Trump on US Elections](https:\/\/voxeu.org\/article\/how-twitter-affected-2016-presidential-election)**","c3f417ce":"**We will remove words not in most used word dict and we change every word with its index**","aa0c8f2e":"## 5.Train with RNN","c019f711":"**Make every word lower case**","c93fbbec":"**Average length of sentences 119, we can use 125. We will constrain size of every sentence to 125. Algorithm will complete smaller sentences to 125 by adding zeros.**","45bc556b":"**Print Word Cloud**","791c5662":"**This dataset published in 2019. I will try model with reviews of Black Widow (2021)**"}}