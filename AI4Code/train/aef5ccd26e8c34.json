{"cell_type":{"559382b8":"code","f389b972":"code","15fbd5fb":"code","4014a281":"code","e4ac4c2b":"code","cc935925":"code","89d920e3":"code","62ee1bd0":"code","d44e73dc":"code","8f3fd4dd":"code","7f777d73":"code","40600160":"code","c9cdd91f":"code","8573523a":"code","9ea6c624":"code","a95bef81":"code","90977121":"code","00ab4c7a":"code","b16d2672":"code","99ac8932":"code","d788b294":"code","7bb55c5c":"code","10eabd56":"code","ed2f5154":"code","3754e531":"code","f67bd15e":"code","039d2047":"code","b854eca6":"code","3ee89ddd":"code","f120c713":"code","8c542b29":"code","4a339e1f":"code","c6bec3a4":"code","f893ffa7":"code","6c0efa69":"code","98b77021":"code","859a1fdf":"code","28abe5b9":"code","2390e980":"code","4f7ead60":"code","c720a513":"markdown","38850b10":"markdown","dc46da40":"markdown","aed7174d":"markdown","1b5f3c01":"markdown","d863f516":"markdown","99091c7f":"markdown","99f7732d":"markdown","c7436acf":"markdown","93afe357":"markdown"},"source":{"559382b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f389b972":"train = pd.read_csv('\/kaggle\/input\/demand-forecasting-kernels-only\/train.csv')\ntest  = pd.read_csv('\/kaggle\/input\/demand-forecasting-kernels-only\/test.csv')\nsamp  = pd.read_csv('\/kaggle\/input\/demand-forecasting-kernels-only\/sample_submission.csv')","15fbd5fb":"test.date = pd.to_datetime(test.date)","4014a281":"train.loc[:,'date'] = pd.to_datetime(train.loc[:,'date'])\ntrain.info()","e4ac4c2b":"print('Store: ',train.store.unique(),'{} stores'.format(len(train.store.unique())))\nprint('Item: ',train.item.unique(),'{} items'.format(len(train.item.unique())))","cc935925":"import matplotlib.pyplot as plt\n\nfig,ax = plt.subplots(figsize = (20,10))\n\nfor j in range(3):\n    s = np.random.randint(min(train.store),max(train.store))\n    i  = np.random.randint(min(train.item),max(train.item))\n    \n    temp = train.loc[(train.store == s) & (train.item ==i),:]\n        \n    ax.plot(temp.date,temp.sales,label = 'Store {} Item {}'.format(s,i))\n\nax.set_xlabel('datetime',fontsize = 20)\nax.set_ylabel('sales',fontsize = 20)\nax.tick_params(axis='both', which='major', labelsize=20)\nplt.legend()","89d920e3":"ex = train.loc[(train.store == 9) & (train.item ==12),:]\n\nex.loc[:,'day_of_month']  = ex.loc[:,'date'].dt.day\nex.loc[:,'month_of_year'] = ex.loc[:,'date'].dt.month\nex.loc[:,'year']          = ex.loc[:,'date'].dt.year\nex.loc[:,'day_of_year']   = ex.loc[:,'date'].dt.dayofyear\nex.loc[:,'day_of_week']   = ex.loc[:,'date'].dt.dayofweek\n\nex_train = ex.loc[ex.year <  2017,:]\nex_test  = ex.loc[ex.year == 2017,:]\n\nplt.plot(ex_train.date,ex_train.sales)\nplt.plot(ex_test.date,ex_test.sales)","62ee1bd0":"temp","d44e73dc":"fig,ax = plt.subplots(figsize = (20,10))\n\nfor year in ex_train.year.unique():\n    temp = ex_train.loc[ex_train.year == year,:]\n    ax.plot(temp.day_of_year,temp.sales,label = year)\n    ax.set_xlabel('Day of a year',fontsize = 20)\n    ax.set_ylabel('Sales',fontsize = 20)\n    ax.tick_params(labelsize = 20)\nplt.legend()  ","8f3fd4dd":"# Null hypothesis: Non Stationarity exists in the series.\n# Alternative Hypothesis: Stationarity exists in the series\n\n# Data: (-1.8481210964862593, 0.35684591783869046, 0, 1954, {'10%': -2.5675580437891359, \n# '1%': -3.4337010293693235, '5%': -2.863020285222162}, 21029.870846458849\n\n# Lets break data one by one.\n# First data point: -1.8481210964862593: Critical value of the data in your case\n# Second data point: 0.35684591783869046: Probability that null hypothesis will not be rejected(p-value)\n# Third data point: 0: Number of lags used in regression to determine t-statistic. So there are no auto correlations going back to '0' periods here.\n# Forth data point: 1954: Number of observations used in the analysis.\n# Fifth data point: {'10%': -2.5675580437891359, '1%': -3.4337010293693235, '5%': -2.863020285222162}: T values corresponding to adfuller test.\n\n\n# Since critical value -1.8>-2.5,-3.4,-2.8 (t-values at 1%,5%and 10% confidence intervals), null hypothesis cannot be rejected. So there is non stationarity in your data\n# Also p-value of 0.35>0.05(if we take 5% significance level or 95% confidence interval), null hypothesis cannot be rejected.\n\n# Hence data is non stationary (that means it has relation with time)\n\n\n\nex_train_rm = ex_train.copy()\nex_train_rm.loc[:,'sales'] = ex_train_rm.loc[:,'sales'].rolling(window = 30).mean()\nplt.plot(ex_train.date,ex_train.sales)\nplt.plot(ex_train_rm.date,ex_train_rm.sales)\n\n\nfrom statsmodels.tsa.stattools import adfuller\n\nadf = adfuller(ex_train.sales,autolag='AIC')\n\nprint(adf[0],\n      adf[1],\n      adf[4])\n\nadf = adfuller(ex_train_rm.sales.dropna(),autolag='AIC')\n\nprint(adf[0],\n      adf[1],\n      adf[4])\n","7f777d73":"ex_train_log = ex_train.copy()\nex_train_log.loc[:,'sales'] = np.log(ex_train_log.loc[:,'sales'])\n\nplt.plot(ex_train.date,ex_train.sales)\nplt.plot(ex_train_log.date,ex_train_log.sales)\n\nfrom statsmodels.tsa.stattools import adfuller\n\nadf = adfuller(ex_train.sales,autolag='AIC')\n\nprint(adf[0],\n      adf[1],\n      adf[4])\n\nadf = adfuller(ex_train_log.sales,autolag='AIC')\n\nprint(adf[0],\n      adf[1],\n      adf[4])\n\n","40600160":"ex_train_diff= ex_train.copy()\nex_train_diff.loc[:,'sales'] = ex_train_diff.loc[:,'sales'].diff()\n\nplt.plot(ex_train.date,ex_train.sales)\nplt.plot(ex_train_diff.date,ex_train_diff.sales)\n\nfrom statsmodels.tsa.stattools import adfuller\nadf = adfuller(ex_train.sales,autolag='AIC')\n\nprint(adf[0],\n      adf[1],\n      adf[4])\n\nadf = adfuller(ex_train_diff.dropna().sales,autolag='AIC')\n\nprint(adf[0],\n      adf[1],\n      adf[4])","c9cdd91f":"from statsmodels.tsa import seasonal\n# seasonal trend residual\ndecompose = seasonal.seasonal_decompose(ex_train.set_index('date')['sales'],model='additive',extrapolate_trend = 'freq',period = 365)\n\nfig,ax = plt.subplots(4,1,figsize = (10,10))\n\n\nax[0].plot(decompose.observed.index,decompose.observed)\nax[1].plot(decompose.observed.index,decompose.trend,linewidth=10 )\nax[2].plot(decompose.observed.index,decompose.seasonal)\nax[3].plot(decompose.observed.index,decompose.resid)","8573523a":"decompose2 = seasonal.seasonal_decompose(decompose.seasonal,model='additive',extrapolate_trend = 'freq',period = 365)\n\nfig,ax = plt.subplots(4,1,figsize = (10,10))\n\n\nax[0].plot(decompose2.observed.index,decompose2.observed)\nax[1].plot(decompose2.observed.index,decompose2.trend )\nax[2].plot(decompose2.observed.index,decompose2.seasonal)\nax[3].plot(decompose2.observed.index,decompose2.resid)","9ea6c624":"## diff\nex_train_diff = ex_train.copy()\nex_train_diff.sales = ex_train.sales.diff()\n\n# trend,season,remove\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomposed  = seasonal_decompose(ex_train.set_index('date').sales,model = 'additive',period=365,extrapolate_trend = 'freq')\n\nplt.plot(ex_train_diff.date,ex_train_diff.sales)\nplt.plot(ex_train.date,decomposed.resid,color = 'red',linestyle = '-')\n\n# on diff data\nfrom statsmodels.tsa.stattools import adfuller, kpss\nadf = adfuller(ex_train_diff.dropna().sales,autolag='AIC')\nprint('diff',adf[1])\nprint('diff',adf[0],adf[4])\n\n#on resid\nadf = adfuller(decompose.resid,autolag='AIC')\nprint('Trend removal', adf[1])\nprint('Trend removal',adf[0],adf[4])","a95bef81":"\n\n# on row data\nfrom statsmodels.tsa.stattools import adfuller, kpss\nadf = adfuller(ex_train.sales,autolag='AIC')\nprint(adf[1])\nprint(adf[0],adf[4])\n\n#on resid\nadf = adfuller(decompose.resid,autolag='AIC')\nprint(adf[1])\nprint(adf[0],adf[4])","90977121":"from statsmodels.tsa.stattools import acf,pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nacf_50 = acf(ex_train_diff.sales.dropna(), nlags=10)\npacf_50 = pacf(ex_train_diff.sales.dropna(), nlags=10)\n\nfig, axes = plt.subplots(1,2,figsize=(16,3))\nplot_acf(ex_train_diff.sales.dropna(), lags=10, ax=axes[0])\nplot_pacf(ex_train_diff.sales.dropna(),lags=10, ax=axes[1])","00ab4c7a":"ex_train = ex_train.set_index('date')[['sales']].resample('D').mean()\nex_test = ex_test.set_index('date')[['sales']].resample('D').mean()","b16d2672":"plt.plot(ex_train)","99ac8932":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\n\nsarima = SARIMAX(ex_train,\n                 order=(1,1,1),\n                 seasonal_order=(1,1,1,7),\n                 freq='D')\nsarima_fit = sarima.fit(disp = 0)","d788b294":"fig = sarima_fit.plot_diagnostics()","7bb55c5c":"pred = sarima_fit.get_prediction(start = '2013-01-06',end = '2018-01-01',dynamic = False)\nmean_pred = pred.predicted_mean\nconf_mean = pred.conf_int()\n\nfig1,ax1 = plt.subplots(figsize = (15,10))\n\nax1.plot(ex_train.index,ex_train.values,color = 'black',linestyle = '-',linewidth = 2)\nax1.plot(mean_pred.index,mean_pred.values,color = 'red',linestyle = '-',linewidth = 2)\nax1.plot(conf_mean['2014':].index,conf_mean['2014':],color = 'blue',linestyle = '--',linewidth = 1)\n","10eabd56":"rmse = np.mean(np.abs((ex_train.values-mean_pred.values)))\nprint(rmse)","ed2f5154":"\nprint(sarima_fit.summary())","3754e531":"forecast = sarima_fit.get_forecast(steps = 365)\nmean_forecast = forecast.predicted_mean\nconf_forecast = forecast.conf_int()\n\nfig1,ax1 = plt.subplots(figsize = (15,10))\n\nax1.plot(ex_test.index,ex_test.values,color = 'black',linestyle = '-',linewidth = 2)\nax1.plot(mean_forecast.index,mean_forecast.values,color = 'red',linestyle = '-',linewidth = 2)\nax1.plot(conf_forecast.index,conf_forecast,color = 'blue',linestyle = '--',linewidth = 1)\n\nprint(np.mean(np.sqrt((ex_test.values - mean_forecast.values)**2)))","f67bd15e":"test_id = test.id\ntest.drop('id',axis = 1,inplace = True)","039d2047":"train['set'] = 'train'\ntest['set']  = 'test'","b854eca6":"data = pd.concat([train,test])","3ee89ddd":"data['day_of_month']     = data.date.dt.day\ndata['month_of_year']   = data.date.dt.month\ndata['year']    = data.date.dt.year\ndata['day_of_year'] = data.date.dt.dayofyear\ndata['day_of_week'] = data.date.dt.dayofweek\ndata['is_weekday'] = data['day_of_week'].apply(lambda x: 1 if x in (6,7) else 0)\ndata['is_month_start']   = data.date.dt.is_month_start.map({False:0,True:1})\ndata['is_month_end']     = data.date.dt.is_month_end.map({False:0,True:1})\n\n\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\nholidays = calendar().holidays(start=data.date.min(), end=data.date.max())\ndata['ia_holiday'] = data.date.isin(holidays).astype(int)","f120c713":"grouping = data.groupby(['store','item'])\nlags = [90,91,92,93,94,95,96,97,98,99,100,180,270]\nfor lag in lags:\n    col_name = 'lag-'+str(lag)\n    data[col_name] = grouping.sales.shift(lag)\n","8c542b29":"lags = [90,97,104]\n\nfor lag in lags:\n    col_name = 'rolling_mean-'+str(lag)\n    data[col_name] = grouping.sales.shift(lag).rolling(window=7).mean()\n    \nfor lag in lags:\n    col_name = 'rolling_std-'+str(lag)\n    data[col_name] = grouping.sales.shift(lag).rolling(window=7).std()","4a339e1f":"data.columns","c6bec3a4":"test  = data.loc[data.set  == 'test',:]\ntrain = data.loc[data.set == 'train',:].dropna()\n","f893ffa7":"train.sales = np.log1p(train.sales)","6c0efa69":"X = train.drop(['date','sales','set'],axis=1).dropna()\ny = train.sales\n\nX_train,X_val = X.loc[X.year < 2017],X.loc[X.year == 2017]\ny_train,y_val = y.loc[X.year < 2017],y.loc[X.year == 2017]","98b77021":"from sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\ntransformer = make_column_transformer(\n    (OneHotEncoder(),['store','item','day_of_week']),\n    (MinMaxScaler(), ['day_of_month','day_of_year']),\n    (StandardScaler(),['lag-90', 'lag-91','lag-92', 'lag-93', 'lag-94', 'lag-95', 'lag-96', 'lag-97', 'lag-98','lag-99', 'lag-100', 'lag-180', 'lag-270',\n                        'rolling_mean-90',\n                        'rolling_mean-97', 'rolling_mean-104', 'rolling_std-90',\n                        'rolling_std-97', 'rolling_std-104']),\n    remainder = 'passthrough'\n)\n\nfrom sklearn.model_selection import TimeSeriesSplit\nimport xgboost as xgb\n\nregressor = xgb.XGBRegressor(n_estimators = 500,\n                             max_depth = 5)\n\npipeline = make_pipeline(transformer,regressor)\n\npipeline.fit(X_train,y_train)\n","859a1fdf":"from sklearn.metrics import mean_absolute_error\npred_val = pipeline.predict(X_val)\npred_train = pipeline.predict(X_train)\n\nprint(mean_absolute_error(y_val,pred_val))\nprint(mean_absolute_error(y_train,pred_train))","28abe5b9":"def smape(preds, target):\n    '''\n    Function to calculate SMAPE\n    '''\n    n = len(preds)\n    masked_arr = ~((preds==0)&(target==0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds-target)\n    denom = np.abs(preds)+np.abs(target)\n    smape_val = (200*np.sum(num\/denom))\/n\n    return smape_val\n\nprint(smape(pred_val,y_val))\nprint(smape(pred_train,y_train))","2390e980":"pipeline.fit(X,y)\npred = pipeline.predict(X)\nprint(smape(pred,y))","4f7ead60":"X_test = test.copy().drop(['date','sales','set'],axis = 1)\npred_test = np.expm1(pipeline.predict(X_test))\n\nsub = pd.DataFrame({'id':test_id,'sales':np.round(pred_test)})\nsub.to_csv('submission.csv',index = False)","c720a513":"# Features","38850b10":"## predictions","dc46da40":"# Seasonality and Trend","aed7174d":"# Differenciating","1b5f3c01":"## rolling mean","d863f516":"## deseson prev season","99091c7f":"## Lags","99f7732d":"## Compare trend,season reomval with diff","c7436acf":"## Log transform","93afe357":"# Transformation"}}