{"cell_type":{"bb175ec6":"code","fada6861":"code","9b992834":"code","2afcba37":"code","44224b00":"code","b00a40ae":"code","ac7990a7":"code","24d29ffc":"code","03136571":"code","c388fd9d":"code","6e71173a":"code","24d20250":"code","0337f65e":"code","207e70ba":"code","0d887dc3":"code","28548687":"code","206c2871":"code","61636ccc":"code","1ba158ec":"code","84268f91":"code","56c61289":"code","4a22de1a":"code","86e2412c":"code","ce1fa573":"code","43486935":"code","96aa2edb":"markdown","a96c0ebb":"markdown","b14a96b8":"markdown","17d4c37b":"markdown","58dc2568":"markdown","a6a5e531":"markdown","8e7f9637":"markdown"},"source":{"bb175ec6":"import pandas as pd","fada6861":"order_reviews = pd.read_csv(\"..\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv\")","9b992834":"import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nstopwords_ptg = stopwords.words(\"portuguese\")\nportugueseStemmer=SnowballStemmer(\"portuguese\")\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer","2afcba37":"order_reviews","44224b00":"# Realizamos una copia del dataset original para efectuar los analisis y \n# conservar la informacion original\nreviews = order_reviews.copy()","b00a40ae":"# Eliminar las filas de la columna review en donde se presenten NaN\nreviews = reviews.dropna(subset=[\"review_comment_message\"])\nreviews.head(3)","ac7990a7":"print(f\"Contamos con un total de {len(reviews)} datos sobre las reviews de los \\\nclientes para realizar NLP.\")","24d29ffc":"duplicados = round(sum(reviews.duplicated(\"review_id\"))\/len(reviews)*100, 2)\nprint(f\"Hay {duplicados}% de reviews duplicados basados en los id.\")","03136571":"# Eliminacion de duplicados basado en los id\nreviews = reviews.drop_duplicates(\"review_id\")\nreviews = reviews.reset_index(drop=True)","c388fd9d":"def pre_procesado(texto):\n    \"\"\"Esta funcion toma un texto y elimina simbolos, numeros y palabras\n    vacias, retornado el texto \"cleaned\".\"\"\"\n    texto = texto.lower()\n    texto = re.sub(r\"[\\W\\d_]+\", \" \", texto)\n    texto = [palabra for palabra in texto.split() if palabra not in stopwords_ptg]\n    texto = \" \".join([portugueseStemmer.stem(palabra) for palabra in texto])\n    return texto","6e71173a":"reviews['pre-procesado'] = reviews['review_comment_message'].apply(lambda texto: pre_procesado(texto))","24d20250":"reviews.head(3)","0337f65e":"# Utilizamos la columna pre-procesado, que contiene los comentarios en formato\n# limpio (sin caracteres especiales, palabras vacias\/stopwords, simbolos\n# o numeros). Buscamos conocer cual es la cantidad de clusters mas optima\n# para realizar la agrupacion. \n\ntfidf_vect = TfidfVectorizer(\n    min_df = 5,\n    max_df = 0.99,\n    max_features = 8000,\n    stop_words = None)\ntfidf = tfidf_vect.fit_transform(reviews['pre-procesado'].values)\n\ntfidf_matrix = pd.DataFrame(data=tfidf.toarray(), columns=tfidf_vect.get_feature_names())\n\ntfidf_matrix = tfidf_matrix.T.round(3)\n\ntfidf_matrix.head(3)","207e70ba":"# Mediante el metodo del codo, buscaremos en la grafica en que cluster comienza\n# a disminuir la pendiente, este sera el numero de clusters optimo para \n# realizar la agrupacion.\n\ndef find_optimal_clusters(data, max_k):\n    iters = range(2, max_k+1, 2)\n    \n    sse = []\n    for k in iters:\n        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n        print('Fit {} clusters'.format(k))\n        \n    f, ax = plt.subplots(1, 1)\n    ax.plot(iters, sse, marker='o')\n    ax.set_xlabel('Cluster Centers')\n    ax.set_xticks(iters)\n    ax.set_xticklabels(iters)\n    ax.set_ylabel('SSE')\n    ax.set_title('SSE by Cluster Center Plot')","0d887dc3":"find_optimal_clusters(tfidf_matrix, 16)","28548687":"comments = reviews[\"pre-procesado\"].values.astype(\"U\")","206c2871":"vectorizer = TfidfVectorizer()\nfeatures = vectorizer.fit_transform(comments)","61636ccc":"# k es igual a 12, debido a que este es el numero de cluster optimo para \n# realizar el agrupamiento.\n\nk = 12\nmodelo = KMeans(n_clusters=k, init=\"k-means++\", max_iter=100, n_init=1)\nmodelo.fit(features)","1ba158ec":"# Creamos una nueva columna a nuestro dataframe con los cluster encontrados\n# por KMeans\nreviews[\"cluster\"] = modelo.labels_","84268f91":"reviews.head(3)","56c61289":"# Agrupamos nuestra dataframe con el objetivo de realizar analisis por cada\n# uno de los clusters encontrados\ngrupos = reviews.groupby(\"cluster\")","4a22de1a":"# Un breve conteo de reviews agrupadas por cada cluster\nreviews[\"cluster\"].value_counts()","86e2412c":"cluster_0 = reviews[\"cluster\"] == 0\ncluster_1 = reviews[\"cluster\"] == 1\ncluster_2 = reviews[\"cluster\"] == 2\ncluster_3 = reviews[\"cluster\"] == 3\ncluster_4 = reviews[\"cluster\"] == 4\ncluster_5 = reviews[\"cluster\"] == 5\ncluster_6 = reviews[\"cluster\"] == 6\ncluster_7 = reviews[\"cluster\"] == 7\ncluster_8 = reviews[\"cluster\"] == 8\ncluster_9 = reviews[\"cluster\"] == 9\ncluster_10 = reviews[\"cluster\"] == 10\ncluster_11 = reviews[\"cluster\"] == 11","ce1fa573":"cluster0 = reviews[cluster_0]\ncluster1 = reviews[cluster_1]\ncluster2 = reviews[cluster_2]\ncluster3 = reviews[cluster_3]\ncluster4 = reviews[cluster_4]\ncluster5 = reviews[cluster_5]\ncluster6 = reviews[cluster_6]\ncluster7 = reviews[cluster_7]\ncluster8 = reviews[cluster_8]\ncluster9 = reviews[cluster_9]\ncluster10 = reviews[cluster_10]\ncluster11 = reviews[cluster_11]","43486935":"print(\"Cluster centroids: \\n\")\norder_centroids = modelo.cluster_centers_.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names()\n\nfor i in range(k):\n    print(\"Cluster %d:\" %i)\n    print(\"-\"*10)\n    print(\"\\n\")\n    for j in order_centroids[i, :15]: # Estraemos los 15 atributos mas \n        print(\" %s\" % terms[j])       # representativos de cada cluster\n    print(\"-\"*10)","96aa2edb":"## TF-IDF\n\nEmplearemos el metodo de Text Frecuency - Inverse Document Frecuency para poder analizar de una manera mas optima los comentarios realizados por los clientes.","a96c0ebb":"Si bien en la anterior funcion **pre_procesado()** se utiliza el metodo stemming para encontrar la raiz base de cada palabra, se recomienda usar el metodo de lematizacion para encontrar la base de cada palabra ya que este logra preservar el significado de las mismas. \n\nPor ejemplo, para las siguientes palabras en ingles \"running, ran, run, runner\", el metodo de stemming nos traeria la raiz \"run\" para cada una de ellas. Sin embargo, la palabra \"runner\" perderia el valor de \"persona corriendo\". El metodo de lemmatization, para este ultimo caso, lograria diferenciar esta palabra, y preservaria a la misma como \"runner\", sin perder su significado base.\n\nPara este analisis se utilizara el metodo de Stemming debido a que el idioma portugues no cuenta con amplio desarrollo en el area de procesamiento del lenguaje natural.","b14a96b8":"Ahora crearemos un DataFrame por cada cluster, para identificar cuales\nson las caracteristicas mas importantes de cada uno.","17d4c37b":"Un aspecto a resaltar es la cantidad de reviews **(22214)** que quedaron agrupadas en el cluster 0. En este cluster encontramos palabras como *satisfeit*, *receb*, *bem* y *qualidad*, las cuales pueden indicar que las reviews agrupadas en este cluster pueden ser positivas.","58dc2568":"Al conocer el numero de clusters optimos para realizar el agrupamiento, buscaremos realizar la segmentacion con el algoritmo no supervisado KMeans (es no supervisado debido a que no le estoy dando datos con labels, o en pocas palabras, no le estoy diciendo al algoritmo a que grupo\/cluster pertenece cada comentario, sino que espero que este los encuentre).","a6a5e531":"# Natural Language Processing","8e7f9637":"Para conocer las caracteristica mas importantes de cada cluster, extraeeremos los atributos que se encuentran en el centro de cada cluster. Estos son los atributos mas representativos y nos brindara informacion sobre a que hace referencia cada uno de los cluster encontrados."}}