{"cell_type":{"492d1782":"code","add4af24":"code","ea7a6493":"code","349d098e":"code","a8c6a824":"code","b7dab001":"code","b18c2493":"code","7394fe9c":"code","7ef35bc5":"code","dcc77fb1":"code","72133916":"code","754a017b":"code","45d4a090":"code","500cbe9b":"code","5afe852c":"code","7ad01b73":"code","915de772":"code","f919474c":"code","c10cce82":"code","562078db":"code","9943923d":"code","8f1eb043":"code","2e3c0c97":"code","baea6ffc":"code","dcef888d":"code","6fa77456":"code","874e8c7f":"code","42f72248":"code","596f4e71":"code","4f81987b":"code","7aa680c6":"code","674078a9":"code","ecb32894":"code","db485605":"code","57d653cf":"code","c9e97788":"code","ed50fe97":"code","ff77ebae":"code","05ca8ebf":"code","e4774ed7":"code","569b1ea8":"code","a2f8eee8":"code","116208fe":"code","4ae5148a":"code","456653b7":"code","76db6614":"code","e4af0ed9":"code","b6b3e070":"code","dc6a5da7":"code","078580bd":"code","5f8f2af4":"code","8ca7df5c":"code","e2d6d6b9":"code","ba6bf989":"code","33d54977":"code","223a15f9":"code","c63cb4a1":"code","1eb651e2":"code","1b2232b1":"code","43f02d8c":"code","3ac03edc":"code","d4eb4f55":"code","d135b9c2":"code","b1bcc5fb":"code","08fa5471":"code","38613455":"code","335cffbb":"code","9528ef45":"code","ad16b7fb":"code","11aa0e0f":"code","5e477632":"code","08f36008":"code","a16a2bd3":"code","ed0e40d4":"code","4bffc450":"markdown","580b6868":"markdown","5dd53fbd":"markdown","f90a90a1":"markdown","ad92b696":"markdown","89fcd562":"markdown","4458a690":"markdown","5bc3b3e9":"markdown","4836edea":"markdown","fc0125a8":"markdown","a3bc893c":"markdown","48d34ec5":"markdown","2530d32a":"markdown","573fe38b":"markdown","0cf39ec7":"markdown","b6bb53e3":"markdown","5fb213c2":"markdown","647d5675":"markdown","ad4cfd19":"markdown","c355e58e":"markdown","c0099c87":"markdown","58aef46c":"markdown","307ae79d":"markdown","b0445067":"markdown","efee875e":"markdown","ff990b1d":"markdown","01f35a42":"markdown","64cd0ffe":"markdown","d6ea6e96":"markdown","a53519ab":"markdown","b2117e94":"markdown","0e0d019e":"markdown","6d7b58d0":"markdown","8742861c":"markdown","f18e3932":"markdown","88c8e104":"markdown"},"source":{"492d1782":"# data analysis\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n\n\n# kaggle settings\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","add4af24":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","ea7a6493":"train.head()","349d098e":"test.head()","a8c6a824":"train.info() #some variables have nulls","b7dab001":"test.info()","b18c2493":"train.describe() #statistics of num variables","7394fe9c":"train['Survived'].value_counts()","7ef35bc5":"train.groupby('Sex', as_index=False).Survived.aggregate(['mean', 'count', 'sum'])","dcc77fb1":"train.groupby('Pclass', as_index=False).Survived.aggregate(['mean', 'count', 'sum'])","72133916":"sns.FacetGrid(train, col='Survived').map(plt.hist, 'Age', bins=25)\nplt.ylabel('Count');","754a017b":"sns.FacetGrid(train, col='Survived').map(plt.hist, 'Fare', bins=25)\nplt.ylabel('Count');","45d4a090":"train.groupby('SibSp', as_index=False).Survived.aggregate(['mean', 'count', 'sum'])","500cbe9b":"train.groupby('Parch', as_index=False).Survived.aggregate(['mean', 'count', 'sum'])","5afe852c":"sns.FacetGrid(train, col='Survived',row='Embarked').map(plt.hist, 'Age', bins=25)\nplt.ylabel('Count');","7ad01b73":"result = []\nfor line in train['Name']:\n    result.append(line[line.find(',')+2 : line.find('.')])\ntrain['Title'] = result\n\nresulttest = []\nfor line in test['Name']:\n    resulttest.append(line[line.find(',')+2 : line.find('.')])\ntest['Title'] = resulttest","915de772":"train.tail()","f919474c":"test.head()","c10cce82":"train.groupby('Title', as_index=False).Survived.aggregate(['mean', 'count', 'sum'])","562078db":"train['Title'] = train['Title'].replace(['Lady', 'Mme', 'Mlle', 'the Countess'], 'Ms')\ntrain['Title'] = train['Title'].replace(['Sir', 'Master'], 'Mr')\ntrain['Title'] = train['Title'].replace(['Capt', 'Col', 'Don', 'Dr','Jonkheer', 'Rev', 'Major' ], 'Other')\n\ntest['Title'] = test['Title'].replace(['Lady', 'Mme', 'Mlle', 'the Countess'], 'Ms')\ntest['Title'] = test['Title'].replace(['Sir', 'Master'], 'Mr')\ntest['Title'] = test['Title'].replace(['Capt', 'Col', 'Don', 'Dr','Jonkheer', 'Rev', 'Major', 'Dona' ], 'Other')","9943923d":"train.groupby('Title', as_index=False).Survived.mean().sort_values(by='Survived', ascending=False)","8f1eb043":"np.unique(test.Title)","2e3c0c97":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain['Title'] = le.fit_transform(np.array(train['Title']).reshape(-1, 1))\ntest['Title'] = le.transform(np.array(test['Title']).reshape(-1, 1))\n# male - 1, female - 0\ntrain['Sex'] = le.fit_transform(train['Sex'])\ntest['Sex'] = le.transform(test['Sex'])","baea6ffc":"train.isnull().sum()","dcef888d":"test.isnull().sum()","6fa77456":"from sklearn.impute import SimpleImputer\nsi=SimpleImputer(strategy='mean')\ntrain['Age'] = si.fit_transform(np.array(train['Age']).reshape(-1, 1))\ntest['Age'] = si.transform(np.array(test['Age']).reshape(-1, 1))","874e8c7f":"median = test[\"Fare\"].median()\ntest['Fare'].fillna(median, inplace=True)","42f72248":"train['FamSize'] = train['SibSp'] + train ['Parch']\ntest['FamSize'] = test['SibSp'] + test ['Parch']","596f4e71":"plt.hist(x=train['Age'], bins=30);","4f81987b":"train['AgeSegm'] = train['Age'].apply(lambda x: 1 if (x >= 0) & (x < 20) else 2 if (x >= 20) & (x < 40) else 3 if (x >= 40) & (x < 60) else 4\n                                      if (x >= 60) & (x <= 80) else 0) \ntest['AgeSegm'] = test['Age'].apply(lambda x: 1 if (x >= 0) & (x < 20) else 2 if (x >= 20) & (x < 40) else 3 if (x >= 40) & (x < 60) else 4\n                                      if (x >= 60) & (x <= 80) else 0) ","7aa680c6":"train['AgeSegm'].unique()","674078a9":"plt.hist(train['Fare'], bins = 35)\nplt.xticks(np.arange(0, max(train['Fare'])+20, 40));","ecb32894":"train['FareSegm'] = train['Fare'].apply(lambda x: 1 if (x >= 0) & (x < 15) else 2 if (x >= 15) & (x < 80) else 3 if (x >= 80) else 0) \ntest['FareSegm'] = test['Fare'].apply(lambda x: 1 if (x >= 0) & (x < 15) else 2 if (x >= 15) & (x < 80) else 3 if (x >= 80) else 0) ","db485605":"train.head()","57d653cf":"train_df = train.drop(['PassengerId','Name','Ticket', 'Cabin', 'Embarked','SibSp','Parch', 'Age', 'Fare'], axis=1)\ntest_df = test.drop(['PassengerId','Name','Ticket', 'Cabin', 'Embarked','SibSp','Parch',  'Age', 'Fare'], axis=1)","c9e97788":"train_df.head()","ed50fe97":"X_train = train_df.drop(\"Survived\", axis=1)\ny_train = train_df[\"Survived\"]\nX_test  = test_df.copy()\nX_train.shape, y_train.shape, X_test.shape","ff77ebae":"tree = DecisionTreeClassifier(random_state=1)\ntree.get_params().keys()","05ca8ebf":"params = {'max_depth': range (4,10),\n          'min_samples_leaf': range(1, 9),\n          'min_samples_split': range (2,10,2)}\n\ngs = GridSearchCV(tree,params, scoring='accuracy', cv=10, n_jobs=-1, refit = True)","e4774ed7":"gs.fit(X_train, y_train)\nprint('best params: ', gs.best_params_)\nprint('best score: ', gs.best_score_)\ndt_score = round(gs.best_score_*100,2)","569b1ea8":"Y_pred_tree = gs.predict(X_test)","a2f8eee8":"lg = LogisticRegression(random_state=1)\nlg.get_params().keys()","116208fe":"params_lg = {'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n          'max_iter': range(40, 100, 10)}\n\ngs_lg = GridSearchCV(lg,params_lg, scoring='accuracy', cv=10, n_jobs=-1, refit = True)","4ae5148a":"gs_lg.fit(X_train, y_train)\nprint('best params: ', gs_lg.best_params_)\nprint('best score: ', gs_lg.best_score_)\nlg_score = round(gs_lg.best_score_ * 100,2)","456653b7":"Y_pred_lg = gs.predict(X_test)","76db6614":"knn = KNeighborsClassifier()\nknn.get_params().keys()","e4af0ed9":"params_knn = {'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n          'n_neighbors': range(3, 50, 2),\n           'leaf_size': range(15, 50, 5)\n                                       }\ngs_knn = GridSearchCV(knn,params_knn, scoring='accuracy', cv=10, n_jobs=-1, refit = True)","b6b3e070":"gs_knn.fit(X_train, y_train)\nprint('best params: ', gs_knn.best_params_)\nprint('best score: ', gs_knn.best_score_)\nknn_score = round(gs_knn.best_score_ * 100,2)","dc6a5da7":"Y_pred_knn = gs_knn.predict(X_test)","078580bd":"nb = GaussianNB()\nnb.get_params().keys()","5f8f2af4":"nb.fit(X_train, y_train)\nprint('score: ', nb.score(X_train, y_train))\nnb_score = round(nb.score(X_train, y_train) * 100,2)","8ca7df5c":"Y_pred_nb = nb.predict(X_test)","e2d6d6b9":"svm = SVC(random_state = 1)\nsvm.get_params().keys()","ba6bf989":"svm.fit(X_train, y_train)\nprint('score: ', svm.score(X_train, y_train))\nsvm_score = round(svm.score(X_train, y_train) * 100,2)","33d54977":"Y_pred_svm = svm.predict(X_test)","223a15f9":"rf = RandomForestClassifier(random_state=1)\nrf.get_params().keys()","c63cb4a1":"params_rf = {'n_estimators': range(10, 100, 10),\n           'max_depth': range (4,10),\n          'min_samples_leaf': range(1, 9),\n          'min_samples_split': range (2,10,2)\n                                       }\ngs_rf = GridSearchCV(rf,params_rf, scoring='accuracy', cv=10, n_jobs=-1, refit = True)","1eb651e2":"gs_rf.fit(X_train, y_train)\nprint('best params: ', gs_rf.best_params_)\nprint('best score: ', gs_rf.best_score_)\nrf_score = round(gs_rf.best_score_ * 100, 2)","1b2232b1":"Y_pred_rf = gs_rf.predict(X_test)","43f02d8c":"sgd = SGDClassifier(random_state = 1, early_stopping = True)\nsgd.get_params().keys()","3ac03edc":"params_sgd = {'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n           'max_iter': range(100, 1000, 20),\n            'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n              'eta0':[0.001, 0.005, 0.01]\n                                       }\ngs_sgd = GridSearchCV(sgd,params_sgd, scoring='accuracy', cv=10, n_jobs=-1, refit = True)","d4eb4f55":"gs_sgd.fit(X_train, y_train)\nprint('best params: ', gs_sgd.best_params_)\nprint('best score: ', gs_sgd.best_score_)\nsgd_score = round(gs_sgd.best_score_ * 100, 2)","d135b9c2":"Y_pred_rf = gs_sgd.predict(X_test)","b1bcc5fb":"gb = GradientBoostingClassifier(random_state = 1)\ngb.get_params().keys()","08fa5471":"params_gb = {'n_estimators': range(10, 100, 10),\n    'learning_rate': [0.1, 0.001, 0.5],\n           'max_depth': range (3,10),\n          'min_samples_leaf': range(1, 9),\n          'min_samples_split': range (2,10,2)\n                                       }\ngs_gb = GridSearchCV(gb,params_gb, scoring='accuracy', cv=10, n_jobs=-1, refit = True)","38613455":"gs_gb.fit(X_train, y_train)\nprint('best params: ', gs_gb.best_params_)\nprint('best score: ', gs_gb.best_score_)\ngb_score = round(gs_gb.best_score_ * 100, 2)","335cffbb":"Y_pred_gb = gs_gb.predict(X_test)","9528ef45":"xgb = XGBClassifier(random_state = 1)\nxgb.get_params().keys()","ad16b7fb":"params_xgb = {'n_estimators': range(10, 100, 10),\n    'learning_rate': [0.1, 0.001, 0.5],\n           'max_depth': range (3,10),\n          'min_samples_leaf': range(1, 9),\n          'min_samples_split': range (2,10,2)\n                                       }\ngs_xgb = GridSearchCV(xgb,params_xgb, scoring='accuracy', cv=10, n_jobs=-1, refit = True)","11aa0e0f":"#gs_xgb.fit(X_train, y_train)\n#print('best params: ', gs_xgb.best_params_)\n#print('best score: ', gs_xgb.best_score_)\n#xgb_score = round(gs_xgb.best_score_ * 100, 2)","5e477632":"#Y_pred_gb = gs_xgb.predict(X_test)","08f36008":"models = pd.DataFrame({\n    'Model': ['Decision Tree', 'Logistic Regression', 'kNN', \n              'Naive Bayes', 'Support Vector Machine', 'Random Forest', \n              'Stochastic Gradient Decent', 'Gradient Boosting'],\n    'Score': [dt_score, lg_score, knn_score, \n              nb_score, svm_score, rf_score, \n              sgd_score, gb_score]}).sort_values(by='Score', ascending=False)\nprint(models)","a16a2bd3":"subm = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv') \nsubm.head()","ed0e40d4":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": Y_pred_gb\n    })\nsubmission.to_csv('titanic.csv', index=False)","4bffc450":"**Main Advantages:**  \nGood in binary classification  \nFast learning process and clarity\n\n**Main disadvantages:**  \nIt can only classify relatively simple objects  \nNot suitable for dividing objects into several classes. ","580b6868":"### Exploratory analysis","5dd53fbd":"We have some missing values in Age and Fare (do not take into account Cabin feature because I'm gonna drop it).","f90a90a1":"#### kNN","ad92b696":"### Fit-Predict part","89fcd562":"**Main Advantages:**  \nEfficiency;  \nEase of implementation.  \n\n**Main disadvantages:**  \nRequires a number of hyperparameters such as the regularization parameter and the number of iterations;  \nIs sensitive to feature scaling.","4458a690":"#### Random Forest","5bc3b3e9":"### Interim conclusions ","4836edea":"The same as GradientBoostingClassifier but faster.","fc0125a8":"#### Decision Trees","a3bc893c":"Gradient boosting has the highest score.","48d34ec5":"We can also combine some age digits and fare into groups. In addition let's unite SibSp and ParCh.","2530d32a":"#### Logistic Regression","573fe38b":"**Main Advantages:**  \nHas a high prediction accuracy;  \nPractically insensitive to data outliers due to random sampling;  \nIs able to efficiently process data with a large number of features and classes;  \nRerely overfits.\n\n**Main disadvantages:**  \nDifficult to interpret;  \nIs prone to overfitting on some tasks, especially on noisy data;  \nLarger size of the resulting models.","0cf39ec7":"#### XGBoost","b6bb53e3":"#### SGD (Stochastic Gradient Descent)","5fb213c2":"In whis part I'm gonna compare some algoritms, scores.","647d5675":"#### Comparing the results","ad4cfd19":"Most likely there will be no need in id, ticket and cabin numbers.","c355e58e":"#### Naive Bayes","c0099c87":"**Main Advantages:**  \nClassification, including multi-class, is easy and fast;  \nWhen the independence assumption is met, NBA outperforms other algorithms, such as logistic regression, and requires less train data.\n\n**Main disadvantages:**  \nIf there is some categorical attribute value in the test data that did not occur in the train data, then the model will assign a zero probability to this value and will not be able to make a prediction (zero frequency can be solved using Laplace smoothing);  \nThe values of the predicted probabilities are not always accurate enough. ","58aef46c":"Now after small analysis we can say that there were total of 891 people on the ship with 314 women (74% survived) and 577 man (19% survived). In total there were more people who didn't manage to survive.  \nPeople from 1st class had more chances to survive than from 3rd.  \nPeople in the age group of ~20-35 years prevail among the survivors and those who did not survive.  \nPassangers with 1-2 siblings aboard more likely survived.","307ae79d":"Survived - is a target.","b0445067":"**Main Advantages:**  \nResistance to outliers and anomalous values, since the probability of the records containing them falling into the number of k-nearest neighbors is small;  \nSimple implementation and easy to interpret.\n\n**Main disadvantages:**  \nPoorly distinguishes objects that are too similar. Sometimes it can incorrectly determine the class of objects even after carefully calibrating the model;  \nSlower than decision tree in case of large dataset;  \nThere is no theoretical basis for choosing a certain number of neighbors. ","efee875e":"We can also do some feature engineering by creating new features.","ff990b1d":"I've been waiting for 5 hours for XGBoost to learn and I had to force stop the process(","01f35a42":"#### SVM (Support Vector Machines)","64cd0ffe":"#### Gradient Tree Boosting","d6ea6e96":"![\u0411\u0435\u0437\u044b\u043c\u044f\u043d\u043d\u044b\u0439.png](attachment:de3e77a3-76e6-4465-a3ef-229607ebcfea.png)","a53519ab":"### Feature engineering","b2117e94":"**Main Advantages:**  \nOften provides predictive accuracy;  \nFlexibility;  \nNo data pre-processing required\n\n**Main disadvantages:**  \nIt will continue improving to minimize all errors. This can overemphasize outliers and cause overfitting;  \nComputationally expensive - often require many trees (>1000) which can be time and memory exhaustive;  \nRequires a large grid search during tuning","0e0d019e":"Now let's drop some columns","6d7b58d0":"**Main Advantages:**  \nIntuitive classification model  \nFast learning process\n\n**Main disadvantages:**  \nOften overfit  \nVary a lot with a small change in the sample","8742861c":"Let's see at the distributions of survived passegers depending on different features.","f18e3932":"Let's agrergate the titles","88c8e104":"**Main Advantages:**  \nWorks well with small data;  \nWorks well with large feature space.\n\n**Main disadvantages:**  \nLong training time (for large datasets);  \nInstability to noise: outliers in the training data directly affect the construction of the separating hyperplane."}}