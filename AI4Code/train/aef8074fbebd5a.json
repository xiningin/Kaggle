{"cell_type":{"2b36bf88":"code","50d4cd12":"code","40e8c9d2":"code","c1fb93dd":"code","b92a8883":"code","39113fc6":"code","dfb6e15d":"code","fabc1c21":"code","9870765b":"code","9f8dbe6e":"code","fe3e3ea6":"code","3ec45706":"code","58916426":"code","acfc46e7":"code","295f57b9":"code","c4d60338":"code","a09fafb8":"code","a8600c35":"code","4ceea0f7":"code","6e07a365":"code","b425ccec":"code","6ca450fd":"code","fde64101":"code","de8820d2":"code","4a8eb3fd":"code","b7eeed67":"code","94a961e4":"code","8124106b":"code","6f499945":"code","3f04f317":"code","72456fef":"code","c9d6fb31":"code","5c07129f":"code","cb75592a":"code","549c52b4":"markdown","dbdba665":"markdown","5b543259":"markdown","7dbb0852":"markdown"},"source":{"2b36bf88":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","50d4cd12":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime","40e8c9d2":"df = pd.read_csv('..\/input\/wind-power-forecasting\/Turbine_Data.csv')\ndf.tail()","c1fb93dd":"df.info()","b92a8883":"ig, ax = plt.subplots()\nax.scatter(df[\"Unnamed: 0\"][:1000], df[\"ActivePower\"][:1000])","39113fc6":"df.ActivePower.plot.hist()","dfb6e15d":"df = pd.read_csv(\"..\/input\/wind-power-forecasting\/Turbine_Data.csv\",\n                 low_memory=False,\n                 parse_dates=[\"Unnamed: 0\"])","fabc1c21":"# With parse_dates... check dtype of \"Unnamed: 0\"\ndf.info()","9870765b":"# duplicate the date column to change it's name \ndf['DateTime'] = df['Unnamed: 0'] \ndf.drop('Unnamed: 0', axis=1, inplace=True)","9f8dbe6e":"df['DateTime'].head(20)","fe3e3ea6":"# Add datetime parameters \ndf['DateTime'] = pd.to_datetime(df['DateTime'], \n format = '%Y-%m-%dT%H:%M:%SZ', \n errors = 'coerce')\n\ndf['year'] = df['DateTime'].dt.year\ndf['month'] = df['DateTime'].dt.month\ndf['day'] = df['DateTime'].dt.day\ndf['hour'] = df['DateTime'].dt.hour\ndf['minute'] = df['DateTime'].dt.minute","3ec45706":"# Drop original DateTime column\ndf.drop('DateTime', axis=1, inplace= True)","58916426":"df","acfc46e7":"df.isna().sum()","295f57b9":"# Fill numeric rows with the median\nfor label, content in df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            # Add a binary column which tells if the data was missing our not\n            df[label+\"_is_missing\"] = pd.isnull(content)\n            # Fill missing numeric values with median since it's more robust than the mean\n            df[label] = content.fillna(content.median())","c4d60338":"df.isna().sum()","a09fafb8":"# Turn categorical variables into numbers\nfor label, content in df.items():\n    # Check columns which aren't numeric\n    if not pd.api.types.is_numeric_dtype(content):\n        # Add binary column to inidicate whether sample had missing value\n        df[label+\"_is_missing\"] = pd.isnull(content)\n        # We add the +1 because pandas encodes missing categories as -1\n        df[label] = pd.Categorical(content).codes+1","a8600c35":"### Testing For Stationarity\n\nfrom statsmodels.tsa.stattools import adfuller\ntest_result=adfuller(df['ActivePower'])\n#Ho: It is non stationary\n#H1: It is stationary\n\ndef adfuller_test(Power):\n    result=adfuller(Power)\n    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n    for value,label in zip(result,labels):\n        print(label+' : '+str(value) )\n    if result[1] <= 0.05:\n        print(\"strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data has no unit root and is stationary\")\n    else:\n        print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n        \nadfuller_test(df['ActivePower'])","4ceea0f7":"# Import libraries required \nimport pandas as pd\nfrom pandas import Series\nimport numpy as np\nimport datetime\nfrom pandas_datareader import data as pdr\nimport matplotlib.pyplot as plt\n\nimport seaborn\nfrom fbprophet import Prophet\nimport statsmodels.api as sm\nimport statsmodels.tsa as ts\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.arima_model import ARIMAResults\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom matplotlib import pyplot\nimport itertools as it\nfrom matplotlib import mlab","6e07a365":"#Analysis of ACF and PACF on Close Price\nfig = plt.figure(figsize=(10,6))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(df['ActivePower'], lags=30, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(df['ActivePower'], lags=30, ax=ax2)\nplt.xlabel('Time lag')\nplt.show()","b425ccec":"#Analysis of ACF and PACF on differece Close Price\nfig = plt.figure(figsize=(10,6))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(df['ActivePower'].diff().dropna(), lags=30, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(df['ActivePower'].diff().dropna(), lags=30, ax=ax2)\nplt.xlabel('Time lag')\nplt.show()","6ca450fd":"from statsmodels.tsa.arima_model import ARIMA\nmodel=ARIMA(df['ActivePower'][:5000],order=(2,0,3))\nmodel_fit=model.fit()","fde64101":"# Plot residual errors\nresiduals = pd.DataFrame(model_fit.resid)\nfig, ax = plt.subplots(1,2)\nresiduals.plot(title=\"Residuals\", ax=ax[0])\nresiduals.plot(kind='kde', title='Density', ax=ax[1])\nplt.show()","de8820d2":"# Actual vs Fitted\nmodel_fit.plot_predict(dynamic=False )\nplt.show()","4a8eb3fd":"# splitting data into test and train datasets\nfrom sklearn.model_selection import train_test_split\ntrain = df['ActivePower'][:1000]\ntest = df['ActivePower'][1000:1015]","b7eeed67":"# Forecast\nfc, se, conf = model_fit.forecast(15, alpha=0.05)  # 95% conf\n\n# Make as pandas series\nfc_series = pd.Series(fc, index=test.index)\nlower_series = pd.Series(conf[:, 0], index=test.index)\nupper_series = pd.Series(conf[:, 1], index=test.index)","94a961e4":"# Accuracy metrics\ndef forecast_accuracy(forecast, actual):\n    mape = np.mean(np.abs(forecast - actual)\/np.abs(actual))  # MAPE\n    me = np.mean(forecast - actual)             # ME\n    mae = np.mean(np.abs(forecast - actual))    # MAE\n    mpe = np.mean((forecast - actual)\/actual)   # MPE\n    rmse = np.mean((forecast - actual)**2)**.5  # RMSE\n    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n    mins = np.amin(np.hstack([forecast[:,None], \n                              actual[:,None]]), axis=1)\n    maxs = np.amax(np.hstack([forecast[:,None], \n                              actual[:,None]]), axis=1)\n    minmax = 1 - np.mean(mins\/maxs)             # minmax\n    return({'mape':mape, 'me':me, 'mae': mae, \n            'mpe': mpe, 'rmse':rmse, \n            'corr':corr, 'minmax':minmax})\n\nforecast_accuracy(fc, test.values)\n","8124106b":"\ndf = pd.read_csv('\/kaggle\/input\/wind-power-forecasting\/Turbine_Data.csv')\ndf.info()\n\ndf[\"Unnamed: 0\"] = df[\"Unnamed: 0\"].apply(lambda x : datetime.strptime(x[:19],'%Y-%m-%d %H:%M:%S'))\ndf.describe()","6f499945":"plt.figure(figsize=(18,16))\nsns.heatmap(df.corr(),square=True,annot=True,linewidths=0.1,cmap=\"coolwarm\")\nplt.show()","3f04f317":"var = df.columns.values\n\ni = 0\n\nsns.set_style('whitegrid')\nfig, ax = plt.subplots(5,4,figsize=(24,30))\n\nfor feature in var:\n    if feature in ['WindSpeed','Unnamed: 0']:\n        pass\n    else:\n        i += 1\n        plt.subplot(5,4,i)\n        sns.scatterplot(x=feature,y='WindSpeed', data=df[[feature,'WindSpeed']])\n        plt.xlabel(feature, fontsize=12)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='both', which='major', labelsize=12)\n        \nplt.show()\n","72456fef":"plt.figure(figsize=(50,5))\nsns.lineplot(x='Unnamed: 0',y='WindSpeed', data=df[['Unnamed: 0','WindSpeed']])\nplt.show()","c9d6fb31":"from sklearn.preprocessing import RobustScaler, StandardScaler\n\nrs = RobustScaler()\ncolumns = df.columns.values.tolist()\ncolumns.remove('Unnamed: 0')\n\npreprocessed = rs.fit_transform(df[columns])\npreprocessed = pd.DataFrame(preprocessed,columns=columns)\n\npreprocessed['Time'] = pd.to_datetime(df['Unnamed: 0'].astype(str).values.tolist())\n\npreprocessed.dtypes","5c07129f":"def accuracy(predicted, observed):\n    mse = abs(predicted - observed).mean()      # MSE, Mean Square Error\n    rmse = ((predicted - observed)**2).mean()**.5  # RMSE, Root Mean Square Error\n    mae = abs(predicted - observed).mean()      # MAE, Mean Absolute Error\n    mape = abs((predicted - observed)\/observed).mean()  # MAPE, Mean Absolute Percentage Error\n    smape = (abs(predicted - observed)\/((abs(predicted)+abs(observed))\/2)).mean() # SMAPE, Symmetric Mean Absolute Percentage Error\n\n    return({'MSE, Mean Square Error': mse, \n            'RMSE, Root Mean Square Error':rmse, \n            'MAE, Mean Absolute Error': mae, \n            'MAPE, Mean Absolute Percentage Error': mape , \n            'SMAPE, Symmetric Mean Absolute Percentage Error':smape})","cb75592a":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\n\n\ncolumn_list = ['ActivePower', 'AmbientTemperatue', 'BearingShaftTemperature',\n       'Blade1PitchAngle', 'Blade2PitchAngle', 'Blade3PitchAngle',\n       'GearboxBearingTemperature', 'GearboxOilTemperature', 'GeneratorRPM',\n       'GeneratorWinding1Temperature', 'GeneratorWinding2Temperature',\n       'HubTemperature', 'MainBoxTemperature', 'NacellePosition',\n       'ReactivePower', 'RotorRPM', 'WindDirection']\n\nx = preprocessed[column_list]\n\ny = \n\ntrain_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.33, random_state=100)\nmodel = DecisionTreeRegressor().fit(train_x,train_y)\npredict_y = model.predict(test_x)\n\n\nfor i in range(len(column_list)):\n    print('%s: %.5f'%(column_list[i],model.feature_importances_[i]))","549c52b4":"### Modelling","dbdba665":"## Parsing dates\nWhen working with time series data, it's a good idea to make sure any date data is the format of a datetime object (a Python data type which encodes specific information about dates).","5b543259":"****Around 2.6% MAPE implies the model is about 97.4% accurate in predicting the next 15 observations.****\n\nPlease don't forget to upvote if my notebook helped you. \ud83d\ude00","7dbb0852":"## Wind Power Forecasting\n\nIn this notebook we're going to  to predict the wind power that could be generated from the windmill for the next 15 days.  We'll use ARIMA model for forecasting. \n\nLet's start"}}