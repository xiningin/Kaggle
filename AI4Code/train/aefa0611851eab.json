{"cell_type":{"a6178968":"code","df2b2aba":"code","ebc516fc":"code","a492966a":"code","ad4bcec2":"code","8d3a1545":"code","43a571bb":"code","c048e823":"code","33a296ca":"code","251bf064":"code","74df0a7b":"code","e521c51c":"code","3418419c":"code","eacaf177":"markdown","88980160":"markdown","df436f6b":"markdown","7fced8c6":"markdown","2e16dbd1":"markdown","949e0bc9":"markdown","c5c165ba":"markdown","04a130a9":"markdown","7d411c58":"markdown","169415aa":"markdown","885caf28":"markdown","b65785f0":"markdown"},"source":{"a6178968":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df2b2aba":"# Plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Model\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","ebc516fc":"df = pd.read_csv('..\/input\/drinking-water-probability\/drinking_water_potability.csv')\ndf.head()","a492966a":"# Shape, data types, null values and unique values count\n\nprint(\"Dataframe's shape:\", df.shape)\n\nnull = list(df.isna().sum())\ntypes = list(df.dtypes)\nunique = list(df.nunique())\n\ninfo = pd.DataFrame({\"Data Types\": types, \"Null\": null, \"Unique\": unique}, index=df.columns)\n\ninfo","ad4bcec2":"# Statistical information\n\ndf.describe()","8d3a1545":"# Checking the value count of potability\n\nplt.figure(figsize=(6,4))\nsns.barplot(x=list(df['Potability'].unique()), y=df['Potability'].value_counts())\nplt.xlabel('Potability')\nplt.ylabel('Count')\nplt.title(\"Potability's Values Count\")\n\nplt.show()","43a571bb":"# Distribution of values for each feature\n\nplt.figure(figsize=(12,20))\n\nfor n, column in enumerate(df.columns):\n    plt.subplot(5, 2, n+1)\n    sns.histplot(df[column], kde=True)\n    plt.xlabel(column)\n    plt.ylabel('Count')\n    plt.title(f\"{column}'s Histogram\")\n\nplt.tight_layout()\nplt.show()","c048e823":"# I opted to take the null data out of the dataframe \n\ndf = df.dropna().reset_index(drop=True)\ndf","33a296ca":"# Establishing the independent and depedent features\n\nX = df.drop(columns='Potability')\ny = df['Potability']\n\n# Scaling the data\n\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n\n# Splitting the data between training and testing \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)","251bf064":"# Selecting the models, fitting the data and measuring their classification performance.\n\nKNN = KNeighborsClassifier()\nKNN.fit(X_train, y_train)\n\nLR = LogisticRegression()\nLR.fit(X_train, y_train)\n\nRF = RandomForestClassifier()\nRF.fit(X_train, y_train)\n\nGNB = GaussianNB()\nGNB.fit(X_train, y_train)\n\nmodels = [KNN, LR, RF, GNB]\n\ntrain_acc = []\ntest_acc = []\ntrain_logloss = []\ntest_logloss = []\ntrain_rocauc = []\ntest_rocauc = []\n\nfor model in models:\n    y_train_hat = model.predict(X_train)\n    y_test_hat = model.predict(X_test)\n    train_acc.append(round(accuracy_score(y_train, y_train_hat)*100, 2))\n    test_acc.append(round(accuracy_score(y_test, y_test_hat)*100, 2))\n    train_logloss.append(log_loss(y_train, y_train_hat))\n    test_logloss.append(log_loss(y_test, y_test_hat))\n    train_rocauc.append(round(roc_auc_score(y_train, y_train_hat), 2))\n    test_rocauc.append(round(roc_auc_score(y_test, y_test_hat), 2))\n\nmetrics = pd.DataFrame({'Train Accuracy %': train_acc, 'Test Accuracy %': test_acc, 'Train Log Loss': train_logloss,\n                       'Test Log Loss': test_logloss, 'Train ROC AUC': train_rocauc, 'Test ROC AUC': test_rocauc}, \n                   index=['K-Nearest Neighbor', 'Logistic Regression','Random Forest', 'Gaussian Naive Bayes'])\nmetrics","74df0a7b":"# Specifying the parameter grid\n\nn_estimators = list(range(200, 2000, 200))\nmax_depth = list(range(10, 110, 10))\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nparam_grid = {'n_estimators': n_estimators,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nparam_grid","e521c51c":"# Random search training: Here 100 different parameters will be tested with K-Fold of 3, equaling 300 iterations in total \n\n\nRF_random = RandomizedSearchCV(estimator = RF, param_distributions = param_grid, \n                               n_iter = 100, cv = 3, verbose=0, random_state=1984, n_jobs = -1)\n\nRF_random.fit(X_train, y_train)\n\nprint(RF_random.best_params_)","3418419c":"# Now using the best parameters found in the random search we will compare its performance to the first model\n\nRF = RandomForestClassifier(n_estimators=1200, min_samples_split=5, min_samples_leaf=4, max_depth=50, bootstrap=True)\nRF.fit(X_train, y_train)\n\ny_train_hat = RF.predict(X_train)\ny_test_hat = RF.predict(X_test)\n\ntrain_acc.append(round(accuracy_score(y_train, y_train_hat)*100, 2))\ntest_acc.append(round(accuracy_score(y_test, y_test_hat)*100, 2))\ntrain_logloss.append(log_loss(y_train, y_train_hat))\ntest_logloss.append(log_loss(y_test, y_test_hat))\ntrain_rocauc.append(round(roc_auc_score(y_train, y_train_hat), 2))\ntest_rocauc.append(round(roc_auc_score(y_test, y_test_hat), 2))\n\nmetrics = pd.DataFrame({'Train Accuracy %': train_acc, 'Test Accuracy %': test_acc, 'Train Log Loss': train_logloss,\n                       'Test Log Loss': test_logloss, 'Train ROC AUC': train_rocauc, 'Test ROC AUC': test_rocauc}, \n                   index=['K-Nearest Neighbor', 'Logistic Regression','Random Forest', 'Gaussian Naive Bayes', 'Random Forest Tuned'])\nmetrics","eacaf177":"# Looking at the metadata","88980160":"The values are not distributed as 1:1, this may affect the model negatively.","df436f6b":"As the dataframe above displays, the Random Forest Classifier was by far the most accurate in its predictions, while keeping the lowest Logarithimic Loss scores and the best proportion of the Area Under the Receiver Operating Characteristic Curve.\n\nAlthough the model got great results predicting the training data,the test data predictions came out with significantly worse performance, indicating overfitting. To solve this I'll implement Scikit-Learn\u2019s RandomizedSearchCV method with K-Fold. This method will help pinpoint the best hyperparameters for our model while trying to solve our overfitting problem.\n\nWill Koehrsen explains this technique in more detail in his article: [Hyperparameter Tuning the Random Forest in Python](https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)","7fced8c6":"As the dataframe above show the new parameters used in the model actually made the performance worst. This may mean we reached diminishing returns for hyperparameter tuning or that more iterations would be necessary to create better parameters. For this analysis the original Random Forest model was the best classifier and yielded decent results, although we didn't manage to overcome the overfitting problem. ","2e16dbd1":"First we need to specify a parameter grid to sample from during fitting. The hyperparameters I will be experimenting with will be:\n\n* n_estimastors = The number of trees in the forest\n* max_depth = max number of levels in each decision tree\n* min_samples_split = min number of data points placed in a node before the node is split\n* min_samples_leaf = min number of data points allowed in a leaf node\n* bootstrap = method for sampling data points (with or without replacement)","949e0bc9":"# Tuning the Hyperparameters","c5c165ba":"# Creating the Model","04a130a9":"All the features seem to be falling into a normal distribution, with the exception of few that are shifted a little to the sides. This information can be used later on when choosing our model.","7d411c58":"# Imports","169415aa":"# Checking the quality of the data","885caf28":"# Reading the data","b65785f0":"# Preparing the data for training"}}