{"cell_type":{"ba0d62ca":"code","8fb1fa70":"code","f3c2fd01":"code","e454c63e":"code","1311f3ea":"code","67db301f":"code","db2abc4c":"code","ab438072":"code","64d5bcdc":"code","df6fe393":"code","2641228f":"code","dd426c6a":"code","8d6cefad":"code","47523e51":"code","4b34615e":"code","d4fe794b":"code","98943f4c":"code","d1bf717e":"code","9a20a5a8":"code","4c69f74f":"code","f6a28dbe":"code","b3a27a56":"markdown","b9fabd16":"markdown","b4dff4c9":"markdown","638a9975":"markdown","bd6349ec":"markdown","eda8ca15":"markdown","2a752918":"markdown","613fa73f":"markdown","e60f06f0":"markdown"},"source":{"ba0d62ca":"!pip install inflect","8fb1fa70":"!pip install contractions","f3c2fd01":"# Data Handelling\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Data preparation and text-preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport nltk\nimport inflect\nimport contractions\nfrom bs4 import BeautifulSoup\nimport re, string, unicodedata\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer\n\n\n# Model Building\nfrom keras.layers import Dropout, Dense, GRU, Embedding, LSTM, Bidirectional, TimeDistributed, Flatten\nfrom keras.models import Sequential\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import matthews_corrcoef, confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Logging\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nlogging.basicConfig(level=logging.INFO)","e454c63e":"#### text preprocessing\n\ndef text_preprocessing_platform(df, text_col):\n    \n    ## Define functions for individual steps\n    # First function is used to denoise text\n    def denoise_text(text):\n        # Strip html if any. For ex. removing <html>, <p> tags\n        soup = BeautifulSoup(text, \"html.parser\")\n        text = soup.get_text()\n        # Replace contractions in the text. For ex. didn't -> did not\n        text = contractions.fix(text)\n        return text\n    \n    ## Next step is text-normalization\n    \n    # Text normalization includes many steps.\n    \n    # Each function below serves a step.\n    \n    \n    def remove_non_ascii(words):\n        \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n        new_words = []\n        for word in words:\n            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n            new_words.append(new_word)\n        return new_words\n    \n    \n    def to_lowercase(words):\n        \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n        new_words = []\n        for word in words:\n            new_word = word.lower()\n            new_words.append(new_word)\n        return new_words\n    \n    \n    def remove_punctuation(words):\n        \"\"\"Remove punctuation from list of tokenized words\"\"\"\n        new_words = []\n        for word in words:\n            new_word = re.sub(r'[^\\w\\s]', '', word)\n            if new_word != '':\n                new_words.append(new_word)\n        return new_words\n    \n    \n    def replace_numbers(words):\n        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n        p = inflect.engine()\n        new_words = []\n        for word in words:\n            if word.isdigit():\n                new_word = p.number_to_words(word)\n                new_words.append(new_word)\n            else:\n                new_words.append(word)\n        return new_words\n    \n    \n    def remove_stopwords(words):\n        \"\"\"Remove stop words from list of tokenized words\"\"\"\n        new_words = []\n        for word in words:\n            if word not in stopwords.words('english'):\n                new_words.append(word)\n        return new_words\n    \n    \n    def stem_words(words):\n        \"\"\"Stem words in list of tokenized words\"\"\"\n        stemmer = LancasterStemmer()\n        stems = []\n        for word in words:\n            stem = stemmer.stem(word)\n            stems.append(stem)\n        return stems\n    \n    \n    def lemmatize_verbs(words):\n        \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n        lemmatizer = WordNetLemmatizer()\n        lemmas = []\n        for word in words:\n            lemma = lemmatizer.lemmatize(word, pos='v')\n            lemmas.append(lemma)\n        return lemmas\n    \n    \n    ### A wrap-up function for normalization\n    def normalize_text(words):\n        words = remove_non_ascii(words)\n        words = to_lowercase(words)\n        words = remove_punctuation(words)\n        words = replace_numbers(words)\n        words = remove_stopwords(words)\n        #words = stem_words(words)\n        words = lemmatize_verbs(words)\n        return words\n    \n    # All above functions work on word tokens we need a tokenizer\n    \n    # Tokenize tweet into words\n    def tokenize(text):\n        return nltk.word_tokenize(text)\n    \n    \n    # A overall wrap-up function\n    def text_prepare(text):\n        text = denoise_text(text)\n        text = ' '.join([x for x in normalize_text(tokenize(text))])\n        return text\n    \n    # run every-step\n    df[text_col] = [text_prepare(x) for x in df[text_col]]\n    \n    \n    # return processed df\n    return df \n    ","1311f3ea":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\nbefore = train.head(10)\nprint(\"Before Text Preprocessing\")\ndisplay(before[['text']])\nafter = text_preprocessing_platform(before, 'text')\nprint(\"After Text Preprocessing\")\ndisplay(after[['text']])","67db301f":"def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n    np.random.seed(7)\n    text = np.concatenate((X_train, X_test), axis=0)\n    text = np.array(text)\n    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n    tokenizer.fit_on_texts(text)\n    sequences = tokenizer.texts_to_sequences(text)\n    word_index = tokenizer.word_index\n    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n    print('Found %s unique tokens.' % len(word_index))\n    indices = np.arange(text.shape[0])\n    # np.random.shuffle(indices)\n    text = text[indices]\n    print(text.shape)\n    X_train = text[0:len(X_train), ]\n    X_test = text[len(X_train):, ]\n    embeddings_index = {}\n    f = open(\"\/kaggle\/input\/glove6b50dtxt\/glove.6B.50d.txt\", encoding='utf-8')\n    for line in f:\n        try:\n            values = line.split()\n            word = values[0]\n            try:\n                coefs = np.asarray(values[1:], dtype='float32')\n            except:\n                pass\n            embeddings_index[word] = coefs\n        except UnicodeDecodeError:\n            pass\n    f.close()\n    print('Total %s word vectors.' % len(embeddings_index))\n    return (X_train, X_test, word_index,embeddings_index, tokenizer)","db2abc4c":"def Build_Model_RNN_Text(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n    # Model building\n    model = Sequential()\n    hidden_layer = 2\n    lstm_node = 32\n    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            if len(embedding_matrix[i]) != len(embedding_vector):\n                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n                      \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n                                                                \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n                exit(1)\n            embedding_matrix[i] = embedding_vector\n    model.add(Embedding(len(word_index) + 1,\n                                EMBEDDING_DIM,\n                                weights=[embedding_matrix],\n                                input_length=MAX_SEQUENCE_LENGTH,\n                                trainable=True))\n    print(lstm_node)\n    for i in range(0,hidden_layer):\n        model.add(Bidirectional(LSTM(lstm_node,return_sequences=True, recurrent_dropout=0.5)))\n        model.add(Dropout(dropout))\n    model.add(Bidirectional(LSTM(lstm_node, recurrent_dropout=0.5)))\n    model.add(Dropout(dropout))\n    #model.add(TimeDistributed(Dense(256)))\n    #model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dense(nclasses, activation='softmax'))\n    model.compile(loss='sparse_categorical_crossentropy',\n                      optimizer='adam',\n                      metrics=['accuracy'])\n    return model","ab438072":"###Utility\n\ndef get_eval_report(labels, preds):\n    mcc = matthews_corrcoef(labels, preds)\n    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n    precision = (tp)\/(tp+fp)\n    recall = (tp)\/(tp+fn)\n    f1 = (2*(precision*recall))\/(precision+recall)\n    return {\n        \"mcc\": mcc,\n        \"tp\": tp,\n        \"tn\": tn,\n        \"fp\": fp,\n        \"fn\": fn,\n        \"pricision\" : precision,\n        \"recall\" : recall,\n        \"F1\" : f1,\n        \"accuracy\": (tp+tn)\/(tp+tn+fp+fn)\n    }\n\ndef compute_metrics(labels, preds):\n    assert len(preds) == len(labels)\n    return get_eval_report(labels, preds)\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\ndef class_balance(df, target):\n  cls = df[target].value_counts()\n  cls.plot(kind='bar')\n  plt.show()","64d5bcdc":"preprocess = True\ntext = 'text'\ntarget = 'target'","df6fe393":"traindf = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntestdf = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n#traindf.head(3)","2641228f":"if preprocess:\n    traindf = text_preprocessing_platform(traindf, text)\n    testdf = text_preprocessing_platform(testdf, text)\ntrain_final = traindf\ntest_final = testdf\nprint(\"Train DataFrame\")\ndisplay(train_final.head(3))\nprint(\"Leadeboard DataFrame\")\ndisplay(test_final.head(3))","dd426c6a":"## df for training and prediction\ndf = train_final\ntest = test_final","8d6cefad":"X = df[text]\ny = df[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\nprint(\"Generating Glove ...\")\nX_train_Glove,X_test_Glove, word_index,embeddings_index, tokenizer = loadData_Tokenizer(X_train,X_test)","47523e51":"# Model Training\nwith warnings.catch_warnings():\n    print(\"Building Model ...\")\n    model_RNN = Build_Model_RNN_Text(word_index,embeddings_index, 2)\n    model_RNN.summary()\n    print(\"\\n Starting Training ... \\n\")\n    history = model_RNN.fit(X_train_Glove, y_train,\n                              validation_data=(X_test_Glove, y_test),\n                              epochs=5,\n                              batch_size=128,\n                              verbose=1)\n    warnings.simplefilter(\"ignore\")","4b34615e":"print(\"\\n Plotting results ... \\n\")\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')\nprint(\"\\n Evaluating Model ... \\n\")\npredicted = model_RNN.predict_classes(X_test_Glove)\n#print(predicted)\nprint(metrics.classification_report(y_test, predicted))\nprint(\"\\n\")\nlogger = logging.getLogger(\"logger\")\nresult = compute_metrics(y_test, predicted)\nfor key in (result.keys()):\n  logger.info(\"  %s = %s\", key, str(result[key]))","d4fe794b":"test.head()\nids = test['id']","98943f4c":"test_input = test[text]\ntest_input = np.array(test_input)\ntest_sequences = tokenizer.texts_to_sequences(test_input)\ntest_padded = pad_sequences(test_sequences, maxlen=500)\nindices = np.arange(test_padded.shape[0])\ntest_padded = test_padded[indices]\nunlabbeled_glove = test_padded","d1bf717e":"test_predictions = model_RNN.predict_classes(unlabbeled_glove)","9a20a5a8":"output = pd.DataFrame({'id':ids,'target':test_predictions})","4c69f74f":"output.set_index('id')","f6a28dbe":"output.to_csv('\/kaggle\/working\/glove6b50d_bi_lstm_p-2LAYERS.csv', index=False)","b3a27a56":"Running experiment","b9fabd16":"Embedding the textual data","b4dff4c9":"Demo of text - preprocessing platform","638a9975":"A master text-preprocessing platform","bd6349ec":"Model evaluation utilities","eda8ca15":"### A Binary Text Clasification Pipline using Bidirection LSTM and GloVe Embeddings","2a752918":"Importing required libraries","613fa73f":"Building Bidirectional LSTM Model","e60f06f0":"Making prediction on un-labbeled data submission"}}