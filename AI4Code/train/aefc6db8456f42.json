{"cell_type":{"90aabce6":"code","99d2c0be":"code","5cb9842e":"code","1945412e":"code","1c500935":"code","4ec575e9":"code","8b4a39d0":"code","33028995":"code","87eba754":"code","86b15acf":"code","b085327b":"code","2ecf33bd":"code","b03cdba6":"code","d7dbd902":"code","b09f5d34":"code","77daa6b2":"code","326944ab":"code","eb5ab7d5":"code","aaf420b6":"code","a7f6edc9":"code","5716af62":"code","95452259":"code","00a06d3a":"code","8353dd6b":"code","c2ebaa9b":"code","64c75369":"code","1e2f0c8a":"code","d26007d1":"code","579b2e27":"code","2875b8aa":"code","853d6fd0":"code","aa5c89d6":"code","4c28f5b1":"code","301693b3":"code","64012541":"code","e78090d3":"code","cd2ff7ad":"code","7ef60141":"code","eb8e4cb3":"code","b6dce8f7":"code","2426c29a":"code","75aa16d5":"code","31e027cc":"code","29927dcb":"code","252a3a41":"code","4f0090a1":"code","821173f5":"markdown","5a6b7918":"markdown","796ae5b6":"markdown","fd8f8fa4":"markdown","949cb621":"markdown","a3bdc522":"markdown","69118e2e":"markdown","3435f7f2":"markdown","57dc0d5d":"markdown","e69db1a8":"markdown","40b5e0c3":"markdown"},"source":{"90aabce6":"\n!pip install featexp\n!pip install --upgrade numpy\nfrom featexp import get_univariate_plots\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\", color_codes=True)\nsns.set(font_scale=1)\n\n# Data Prediction\nfrom sklearn import preprocessing\nimport xgboost\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\nimport lightgbm as lgb\nfrom pandas.plotting import parallel_coordinates\nfrom sklearn.preprocessing import LabelEncoder\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"..\/input\/inc-ml\/\"))\n\n","99d2c0be":"# Importing the dataset\n\ntrain_dt = pd.read_csv('..\/input\/inc-ml\/tcd-ml-1920-group-income-train.csv') \ntest_dt = pd.read_csv('..\/input\/inc-ml\/tcd-ml-1920-group-income-test.csv')\n\nY = train_dt['Total Yearly Income [EUR]']\n#Display columns and shape\nprint(train_dt.shape)\nprint(test_dt.shape)\ntrain_dt.head()\ntest_dt.head()\n","5cb9842e":"print(train_dt['Instance'].nunique())\nprint(len(train_dt['Instance']))","1945412e":"get_univariate_plots(data=train_dt, target_col='Total Yearly Income [EUR]', \n                     features_list=['Wears Glasses'], bins=10)","1c500935":"#Counting Null Function\ndef null_values(df):\n    \n    sum_null = df.isnull().sum()\n    total = df.isnull().count()\n    percent_nullvalues = 100* sum_null \/ total \n    df_null = pd.DataFrame()\n    df_null['Total'] = total\n    df_null['Null_Count'] = sum_null\n    df_null['Percent'] = round(percent_nullvalues,2)\n    df_null = df_null.sort_values(by='Null_Count',ascending = False)\n    df_null = df_null[df_null.Null_Count > 0]\n    \n    return(df_null)\n\n# Training and Testing null functions\nprint(null_values(train_dt))\nprint(null_values(test_dt))","4ec575e9":"train_dt.describe()","8b4a39d0":"test_dt.describe()","33028995":"# Seperate categorical and numeric features\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnum_df = train_dt.select_dtypes(include=numerics)\ncat_df = list(set(train_dt.columns)-set(num_df))\n\n# Display different types in training features\nfor feature in cat_df:\n    print('---'+feature+'---')\n    print(train_dt[feature].value_counts())\n    print()\n\n# Display different types in testing features\nfor feature in num_df:\n    print('---'+feature+'---')\n    print(test_dt[feature].value_counts())\n    print()","87eba754":"#Remove null values with most common values\nfor feature in train_dt.columns:\n    train_dt = train_dt.fillna({\n    feature:train_dt[feature].mode()[0]\n    })\n\n\nprint(null_values(train_dt))\n\n#Manually Encode Values\n","86b15acf":"train_dt['Gender'].value_counts()","b085327b":"train_dt['University Degree'].value_counts()","2ecf33bd":"train_dt['Hair Color'].value_counts()","b03cdba6":"train_dt['Satisfation with employer'].value_counts()","d7dbd902":"test_dt","b09f5d34":"train_dt = pd.concat([train_dt, test_dt],axis='rows').reset_index()\nfor feature in train_dt.columns:\n    train_dt = train_dt.fillna({\n    feature:train_dt[feature].mode()[0]\n    })","77daa6b2":"# Label Encode Gender, Degree, Hair-Color\n\n# Hair\nle_gender= train_dt['Gender']\nle_gender = le_gender.str.replace('female','2')\nle_gender = le_gender.str.replace('male','3')\nle_gender = le_gender.str.replace('other','0')\nle_gender = le_gender.str.replace('unknown','4')\nle_gender = le_gender.str.replace('0','4')\nle_gender = le_gender.str.replace('f','2')\nle_gender = le_gender.apply(lambda v: int(v))\nprint(le_gender.value_counts())\nle_gender = pd.DataFrame({'gender_le':le_gender}) \n\n\n# Degree\nle_degree = train_dt['University Degree']\nle_degree = le_degree.str.replace('No','0')\nle_degree = le_degree.str.replace('Bachelor','1')\nle_degree = le_degree.str.replace('Master','2')\nle_degree = le_degree.str.replace('PhD','3')\nle_degree = le_degree.str.replace('0','0')\nle_degree = le_degree.apply(lambda v: int(v))\nprint(le_degree.value_counts())\nle_degree = pd.DataFrame({'uni_le':le_degree}) \n\n\n# Hair Color\nle = LabelEncoder()\nle_hc = le.fit_transform(train_dt['Hair Color'])\nle_hc = pd.DataFrame({'Hc_le':le_hc}) \n\n# Satisfation with employer\nle_satisfaction = train_dt['Satisfation with employer']\nle_satisfaction = le_satisfaction.str.replace('Unhappy','0')\nle_satisfaction = le_satisfaction.str.replace('Average','1')\nle_satisfaction = le_satisfaction.str.replace('Somewhat Happy','2')\nle_satisfaction = le_satisfaction.str.replace('Happy','3')\nle_satisfaction = le_satisfaction.apply(lambda v: int(v))\nprint(le_satisfaction.value_counts())\nle_satisfaction = pd.DataFrame({'satisfaction_le':le_satisfaction}) \n\n\n#Additional Income\ninc = pd.DataFrame()\ninc['additionalInc'] = train_dt['Yearly Income in addition to Salary (e.g. Rental Income)']\ninc['additionalInc'] = inc['additionalInc'].str.replace('EUR', '')\ninc['additionalInc'] = pd.to_numeric(inc['additionalInc'], errors='coerce')\n","326944ab":"#Drop categorical columns\n# train_dt = train_dt.drop(['Gender','University Degree','Hair Color'],axis=1)\n\n# Add one encoded columns\ntrain_dt = pd.concat([train_dt,le_gender,le_degree,le_hc,le_satisfaction,inc['additionalInc']],axis=1)\n","eb5ab7d5":"train_dt.head(3)","aaf420b6":"# {'Antigua and Barbuda', 'Palau', 'Russia', 'Nigeria', 'Seychelles', 'Liechtenstein', 'Iran', 'Ethiopia', \n# 'San Marino', 'Philippines', 'Tonga', 'Marshall Islands', 'Monaco', 'Nauru', 'Germany', '0'}\nfor feature in ['Housing Situation','Country','Profession']:\n    print('---'+feature+'---')\n    tr_vc = train_dt[feature].unique()\n    te_vc = test_dt[feature].unique()\n    diff_vals = set(te_vc) - set(tr_vc) \n    print(diff_vals)","a7f6edc9":"train_dt['Country'].unique()","5716af62":"corr=train_dt.corr()[\"Total Yearly Income [EUR]\"]\ncorr[np.argsort(corr, axis=0)[::-1]]","95452259":"# WeirdStuff1\ncorrelations=train_dt.corr()\nattrs = correlations.iloc[:-1,:-1] # all except target\n\nthreshold = 0.005\nimportant_corrs = (attrs[abs(attrs) > threshold][attrs != 1.0]) \\\n    .unstack().dropna().to_dict()\n\nunique_important_corrs = pd.DataFrame(\n    list(set([(tuple(sorted(key)), important_corrs[key]) \\\n    for key in important_corrs])), \n        columns=['Attribute Pair', 'Correlation'])\n\n# sorted by absolute value\nunique_important_corrs = unique_important_corrs.ix[\n    abs(unique_important_corrs['Correlation']).argsort()[::-1]]\n\nunique_important_corrs","00a06d3a":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnum_df = train_dt.select_dtypes(include=numerics)\ncorrMatrix=train_dt[num_df.columns].corr()\n\nsns.set(font_scale=1.10)\nplt.figure(figsize=(10, 10))\n\nsns.heatmap(corrMatrix, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='viridis',linecolor=\"white\")\nplt.title('Correlation between features');","8353dd6b":"num_df","c2ebaa9b":"# Univariate Analysis\nfig = plt.figure(figsize=(9,3))\nfor feature in num_df:\n    sns.distplot(train_dt[feature], color=\"r\", kde=False)\n    plt.title(\"Distribution of \" + feature)\n    plt.ylabel(\"Number of Occurences\")\n    plt.xlabel(feature)\n    plt.show()","64c75369":"f = (train_dt\n         .loc[train_dt['Housing Situation'].isin(['Small House', 'Large House','Medium Apartment'])]\n         .loc[:, ['Total Yearly Income [EUR]', 'Housing Situation', 'Gender']]\n    )\n\nsns.boxplot(x=\"Gender\", y=\"Total Yearly Income [EUR]\", hue='Housing Situation', data=f)","1e2f0c8a":"train_dt['Housing Situation'].value_counts()","d26007d1":"# WeirdStuff2\nsns.lmplot(x='Year of Record', y='Total Yearly Income [EUR]', col='Housing Situation', \n           data=train_dt.loc[train_dt['Housing Situation'].isin(['Small House','Medium House','Large House'])], \n           fit_reg=False)","579b2e27":"sns.lmplot(x='Year of Record', y='Total Yearly Income [EUR]', col='Housing Situation', \n           data=train_dt.loc[train_dt['Housing Situation'].isin(['Small Apartment','Medium Apartment','Large Apartment'])], \n           fit_reg=False)","2875b8aa":"# train_dt.loc[(train_dt['Housing Situation'] == 'Castle') & (train_dt['Year of Record'] >=1978)] \n# test_dt.loc[(test_dt['Housing Situation'] == 'Large Apartment') & (test_dt['Year of Record'] >=1980) & (test_dt['Year of Record'] >=1990)] \nsns.lmplot(x='Year of Record', y='Total Yearly Income [EUR]', col='Housing Situation', \n           data=train_dt.loc[train_dt['Housing Situation'].isin(['nA','0','Castle'])], \n           fit_reg=False)","853d6fd0":"train_dt.loc[(train_dt['Housing Situation'] == 'Castle') & (train_dt['Total Yearly Income [EUR]'] >=300000)] ","aa5c89d6":"sns.lmplot(x='Satisfation with employer', y='Total Yearly Income [EUR]', col='Gender', \n           data=train_dt.loc[train_dt['Housing Situation'].isin([0,1,2,3])], \n           fit_reg=False)","4c28f5b1":"# WeirdStuff3: Gender female doing worse than f \nsns.lmplot(x='Crime Level in the City of Employement', y='Total Yearly Income [EUR]', col='Gender', \n           data=train_dt.loc[train_dt['gender_le'].isin([0,1,2,3])], \n           fit_reg=False)","301693b3":"train_dt.loc[(train_dt['Housing Situation'] == 'Castle') & (train_dt['Total Yearly Income [EUR]'] >=300000)] ","64012541":"incs = train_dt.groupby('Year of Record').mean()\nincs['Year'] = incs.index","e78090d3":"incs.columns","cd2ff7ad":"plt.scatter(incs['Year'], incs['Total Yearly Income [EUR]'])\nplt.show()","7ef60141":"countries_mean = train_dt.groupby('Country').mean()\ncountries_mean['Country'] = countries_mean.index\ncountries_mean","eb8e4cb3":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnum_df = countries_mean.select_dtypes(include=numerics)\ncorrMatrix=train_dt[num_df.columns].corr()\n\nsns.set(font_scale=1.0)\nplt.figure(figsize=(12, 12))\n\nsns.heatmap(corrMatrix, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='viridis',linecolor=\"white\")\nplt.title('Correlation between features');","b6dce8f7":"testtest = test_dt.iloc[:,:-1]\ntesttest","2426c29a":"train_dt.columns","75aa16d5":"get_univariate_plots(data=train_dt, target_col='Total Yearly Income [EUR]', \n                     features_list=['Crime Level in the City of Employement'], bins=10)\nget_univariate_plots(data=train_dt, target_col='Total Yearly Income [EUR]', \n                     features_list=['Hc_le'], bins=10)\nget_univariate_plots(data=train_dt, target_col='Total Yearly Income [EUR]', \n                     features_list=['uni_le'], bins=10)\nget_univariate_plots(data=train_dt, target_col='Total Yearly Income [EUR]', \n                     features_list=['Year of Record'], bins=10)\nget_univariate_plots(data=train_dt, target_col='Total Yearly Income [EUR]', \n                     features_list=['Wears Glasses'], bins=10)\nget_univariate_plots(data=train_dt, target_col='Total Yearly Income [EUR]', \n                     features_list=['satisfaction_le'], bins=10)\nget_univariate_plots(data=train_dt, target_col='Total Yearly Income [EUR]', \n                     features_list=['gender_le'], bins=10)\nget_univariate_plots(data=train_dt, target_col='Total Yearly Income [EUR]', \n                     features_list=['Body Height [cm]'], bins=10)","31e027cc":"num_train_df = train_dt.iloc[0:1048574,:].drop(['Housing Situation', 'Work Experience in Current Job [years]', 'Satisfation with employer', 'Gender', 'Country', 'Profession', 'University Degree', 'Hair Color', 'Yearly Income in addition to Salary (e.g. Rental Income)'],axis=1)\nnum_test_df = train_dt.iloc[1048574:,:].drop(['Housing Situation', 'Work Experience in Current Job [years]', 'Satisfation with employer', 'Gender', 'Country', 'Profession', 'University Degree', 'Hair Color', 'Yearly Income in addition to Salary (e.g. Rental Income)'],axis=1)\nX_train, X_val, y_train, y_val = train_test_split(num_train_df, Y, test_size=0.35, random_state=42)\nget_univariate_plots(data=X_train, target_col='Total Yearly Income [EUR]', data_test=X_val, features_list=['Hc_le'])\n# (1048574, 17)\n# (369438, 17)","29927dcb":"from featexp import get_trend_stats\nstats = get_trend_stats(data=X_train, target_col='Total Yearly Income [EUR]', data_test=X_val)\nstats","252a3a41":"stats = get_trend_stats(data=num_train_df, target_col='additionalInc', data_test=num_test_df)\nstats","4f0090a1":"get_univariate_plots(data=num_train_df, target_col='additionalInc', data_test=num_test_df, features_list=['satisfaction_le'])","821173f5":"# Null Values in Training and Test Set \nNot sure if its including blanks in other strings","5a6b7918":"## Adding Dummy Variables\nAdding \n- Gender\n- Uni Degree\n\nDropping Categorical variables* ","796ae5b6":"## Dummy and Label Encoding \n#### Label Encoding \n- Hair Color\n- Gender (3,2,1,0)\n- Uni Degree (3,2,1,0)\n- Satisfation with employer\t\n\n#### Dummy Enconding variables\n- ...","fd8f8fa4":"# Feature Engineering\n\nCreating new Features by means of \n1. Label Encoding\n2. Dummy Encoding","949cb621":"# EDA (Exploratory Data Analysis)\n\nThis file is to analyze the features being used for Income Prediction\n**Search WeirdStuff{num} and the visualisation\/code regarding fact should show up **\n## Interesting Finds\n\n1. WeirdStuff1: Instances is TOO highly correlated to Year of Record which in turn is highly correlated to Target Income.\n    - This may lead to doubting affect of Year of Record of if Instances may be useful in testing data as well.\n    \n2. Some houses just dont exists during longe ranges of years\n    - We may need to find a way to restrict housing impact to only certain years. Because in test set there are sets that are not in training \n    \n3. WeirdStuff3: Gender female doing worse than f \n    - Maybe best to keep them seperate\n","a3bdc522":"### Unique Countries","69118e2e":"# Unique values in each and .describe() stats of each feature\nShows distribution of values and the unique values in each feature, both training and test set","3435f7f2":"# Correlations\nChecking correlations between variables with Total yearly income and pair plots\n\n## MultiColinearity\nMulticollinearity increases the standard errors of the coefficients. That means, multicollinearity makes some variables statistically insignificant when they should be significant.\n\nTo avoid this we can do 3 things:\n\nCompletely remove those variables\nMake new feature by adding them or by some other operation.\nUse PCA, which will reduce feature set to small number of non-collinear features.","57dc0d5d":"## Difference between Training and Test Sets\nOnly missing values are \n- 0 in Housing \n- Countries ","e69db1a8":"# Trend Correlation\n\nShows if features are noisy and inconsistent throughout data","40b5e0c3":"# Filling Missing values\nReplace missing values for **all to mode of  feature**"}}