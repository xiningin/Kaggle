{"cell_type":{"7c47e60d":"code","46c25ba0":"code","5f7eb24b":"code","af9dc5c4":"code","a4611dd3":"code","b1cec076":"code","f56734d1":"code","62fc0b34":"code","347ecf01":"code","e6c426e1":"code","de584451":"code","c70ab34c":"code","c64d0f03":"code","d85122db":"code","1f284582":"code","b966ccf6":"code","c9ed5432":"code","0e41647e":"code","9bcb600f":"code","6c84c8ab":"code","7ff8f1b5":"code","3eb41616":"code","b93195bc":"code","d2df07eb":"code","1beede72":"code","539c3239":"code","02d771ea":"code","f7b83e30":"code","880b1702":"code","3432fda9":"code","ccd3823d":"code","b5fec175":"code","f963af53":"code","108ac68e":"code","260268d0":"code","8d781d1b":"code","aca26d4c":"code","501b32ee":"code","847025cf":"code","e3c53905":"code","9988a563":"code","e5177793":"code","13cf2fde":"code","b825871d":"code","fae5597c":"code","a6feb7d8":"code","3fac27c2":"code","34320ec9":"code","4849af2a":"code","ca783a84":"code","1b974a99":"code","a3ae6c21":"code","5e41ae2c":"code","99944aea":"code","d4558745":"code","387bc42b":"code","c2552501":"code","6a5397a9":"code","b1f3ec40":"code","92325eab":"code","f0fb019d":"code","5faacf59":"code","4b4ee3c6":"code","6a976e73":"code","77abacf8":"markdown","868155f7":"markdown","fdf0d1c0":"markdown","7c83ce14":"markdown","5486dcc3":"markdown","261a0e9c":"markdown","44bd3e52":"markdown","8ac584e9":"markdown"},"source":{"7c47e60d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","46c25ba0":"import seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, f1_score\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.models import load_model\nfrom sklearn.metrics import confusion_matrix, accuracy_score","5f7eb24b":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsubmission_df = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","af9dc5c4":"train_df","a4611dd3":"test_df","b1cec076":"train_df.info()","f56734d1":"train_df.dtypes","62fc0b34":"train_df.describe()","347ecf01":"train_df.drop(['PassengerId','Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","e6c426e1":"train_df","de584451":"train_df['Family_size'] = train_df['SibSp'] + train_df['Parch'] +1","c70ab34c":"train_df","c64d0f03":"train_df.drop(['SibSp', 'Parch'], axis=1, inplace = True)","d85122db":"train_df","1f284582":"train_df.isnull().sum()","b966ccf6":"train_df['Embarked'].unique()","c9ed5432":"train_df.dropna(subset=['Embarked'], inplace=True)","0e41647e":"le = LabelEncoder()","9bcb600f":"for col in ['Embarked', 'Sex']:\n    train_df[col] = le.fit_transform(train_df[col])","6c84c8ab":"train_df","7ff8f1b5":"train_df.isnull().sum()","3eb41616":"mean_age = train_df['Age'].mean()","b93195bc":"mean_age","d2df07eb":"train_df['Age'].replace(np.NaN, mean_age, inplace=True)","1beede72":"sns.heatmap(train_df.corr(), cmap='Greys', annot=True)","539c3239":"train_df.isnull().sum()","02d771ea":"X = train_df.drop(['Survived'], axis=1)\nY = train_df['Survived']","f7b83e30":"X","880b1702":"Y","3432fda9":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)","ccd3823d":"print('X_train', X_train.shape)\nprint('Y_train', Y_train.shape)\nprint('X_test', X_test.shape)\nprint('Y_test', Y_test.shape)","b5fec175":"Y_train","f963af53":"scaler = preprocessing.MaxAbsScaler()\nX = scaler.fit_transform(X)","108ac68e":"lr = LogisticRegression()\nlr.fit(X_train,Y_train)\n\nprint(lr.intercept_)\nprint(lr.coef_)","260268d0":"score = lr.score(X_test, Y_test)\nscore","8d781d1b":"y_pred = lr.predict(X_test)","aca26d4c":"print(f\"f1_score {f1_score(Y_test, y_pred, average='macro')}\")\nprint(f\"accuracy_score {accuracy_score(Y_test, y_pred)}\")","501b32ee":"model = Sequential()\nmodel.add(Dense(35, input_dim=X_train.shape[1], activation='relu',kernel_initializer='RandomUniform'))\nmodel.add(Dense(25,activation='relu',kernel_initializer='RandomUniform'))\nmodel.add(Dense(15,activation='relu',kernel_initializer='RandomUniform'))\nmodel.add(Dense(5,activation='relu',kernel_initializer='RandomUniform'))\nmodel.add(Dense(1,activation='sigmoid',kernel_initializer='RandomUniform'))","847025cf":"#Compiling Model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics =['accuracy'])\nhistory = model.fit(X_train,Y_train,verbose=2,epochs=10)","e3c53905":"test_acc = model.evaluate(X_test, Y_test,verbose=1)\nprint(test_acc)","9988a563":"acur = history.history['accuracy']","e5177793":"key_values = model.history.history.keys()","13cf2fde":"key_values","b825871d":"plt.plot(acur, 'g', label='Accuracy')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","fae5597c":"pred_nn = model.predict(X_test)\npred_nn[pred_nn<0.5]=0\npred_nn[pred_nn>=0.5]=1","a6feb7d8":"labels = [0 , 1]\n\nwidth = 5\nheight = 5\n\ntitle = \"Confusion Matrix\"","3fac27c2":"cm = confusion_matrix(Y_test, pred_nn, labels=labels)\nfig = plt.figure(figsize=(width, height))\nax = plt.subplot()\nax = sns.heatmap(cm, annot=True, ax=ax, fmt='d');\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title(title)\nax.xaxis.set_ticklabels(labels, rotation=90)\nax.yaxis.set_ticklabels(labels, rotation=0)\nbottom, top = ax.get_ylim()\nleft, right = ax.get_xlim()\ndelta = 0.0325\nax.set_ylim(bottom + delta, top - delta)\nax.set_xlim(left - delta, right + delta)\n# plt.savefig(name + \".pdf\", bbox_inches='tight')\nplt.show()\n# return ax","34320ec9":"test_df","4849af2a":"test_data = test_df.drop(['PassengerId','Name', 'Ticket', 'Cabin'], axis=1, inplace=False)","ca783a84":"test_data['Family_size'] = test_data['SibSp'] + test_data['Parch'] +1","1b974a99":"test_data","a3ae6c21":"test_data.drop(['SibSp', 'Parch'], axis=1, inplace = True)","5e41ae2c":"test_data.isnull().sum()","99944aea":"test_data['Fare']= test_data['Fare'].fillna(test_data['Fare'].mean())","d4558745":"for col in ['Embarked', 'Sex']:\n    test_data[col] = le.fit_transform(test_data[col])","387bc42b":"mean_age_test = test_data['Age'].mean()","c2552501":"test_data['Age'].replace(np.NaN, mean_age_test, inplace=True)","6a5397a9":"test_data.info()","b1f3ec40":"y_pred_test = lr.predict(test_data)","92325eab":"len(y_pred_test)","f0fb019d":"test_df['Fare'].isnull().sum()","5faacf59":"len(test_df['PassengerId'])","4b4ee3c6":"output = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': y_pred_test})\noutput.to_csv('my_submission_test.csv', index=False)","6a976e73":"output","77abacf8":"## Data Preprocessing","868155f7":"## Confusion Matrix","fdf0d1c0":"## Dealing with Null Values","7c83ce14":"## Spliting Data in Train and Test sets","5486dcc3":"# Deep Neural Network","261a0e9c":"## Making Feature set X and Target Variable Y","44bd3e52":"### Replacing null values in Age with its mean","8ac584e9":"# Logistic Regression"}}