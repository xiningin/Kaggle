{"cell_type":{"3dc2fafb":"code","2291ab4e":"code","1e0be2e0":"code","04fe7d3b":"code","02d1d288":"code","78ab065b":"code","a7d7dcfc":"code","a25bea52":"code","a4f91c7f":"code","a88446d7":"code","15a67805":"code","7d3526d7":"code","fb07a582":"code","4eee5d11":"code","3b2f7648":"code","6f8996b8":"code","37ca45a7":"code","f8c2e34b":"code","16a15805":"code","83ccfa52":"code","9efd9797":"code","f18e660c":"code","ef2a5de9":"code","1babc3bc":"code","8fe2333e":"code","c50cecef":"code","fa463663":"code","725f08bc":"code","d4790236":"code","28bc618a":"code","e0b2479e":"code","05a2e090":"code","cce5bb4c":"code","b68f033e":"code","a5682c34":"code","034c9c43":"code","b504d525":"code","966e4357":"code","8f2394f0":"code","a71e3b91":"code","298b6447":"code","350e4270":"markdown","f27eb071":"markdown","d1461e28":"markdown","a311acb6":"markdown","8550e2c5":"markdown","3b55458c":"markdown","793f50d8":"markdown","c1cbffa4":"markdown"},"source":{"3dc2fafb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2291ab4e":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.offline as pyo\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\n\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot, download_plotlyjs\n","1e0be2e0":"# Function defined to check medata of a dataframe\ndef master_dataframe(dataframe):\n    df_metadata = pd.DataFrame({'Datatype': dataframe.dtypes,\n                                \"Null Values\": dataframe.isna().sum(),  \n                                \"Null %\": round(dataframe.isna().sum()\/len(dataframe)*100, 2),\n                                \"No: Of Unique Values\": dataframe.nunique()})\n    \n    df_describe = dataframe.describe(include='all').T\n    \n    df_metadata = df_metadata.join(df_describe)  \n\n    return df_metadata","04fe7d3b":"# Import data\nraw_data = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","02d1d288":"# Check metadata by using the function master_dataframe()\nmaster_dataframe(raw_data)","78ab065b":"raw_data['Unnamed: 32'].unique()","a7d7dcfc":"# Dropping the ID & Unnamed 32 field.\ndata_col_drop = raw_data.copy()\ndata_col_drop.drop(columns = ['id', 'Unnamed: 32'], axis = 1, inplace = True)\ndata_col_drop.columns","a25bea52":"# Check the unique value of our target variable diagnosis.\ndata_col_drop['diagnosis'].unique()","a4f91c7f":"# Segregating the dependent and independent variables.\nX = data_col_drop.iloc[:, 1:].values\ny = data_col_drop.iloc[:, 0].values","a88446d7":"print(X)","15a67805":"print(y)","7d3526d7":"# Lets encode the dependent variable i.e. diagnosis using LabelEncoder as it is a nominal categorical data(unordered).\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)\nprint(y)","fb07a582":"# Let's split out dataset into train and test data before we create our model.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\nprint('Shape of X_train', X_train.shape)\nprint('Shape of X_test', X_test.shape)\nprint('Shape of y_train', y_train.shape)\nprint('Shape of y_test', y_test.shape)","4eee5d11":"# Lets apply feature scaling to our train and test independent variables as they appear to be in different scale.\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","3b2f7648":"print(X_train)","6f8996b8":"print(X_test)","37ca45a7":"# Lets create our Logistic Regression model by using sklearn library\nfrom sklearn.linear_model import LogisticRegression\nlog_classifier = LogisticRegression(random_state = 0)\nlog_classifier.fit(X_train, y_train)","f8c2e34b":"# Lets make the prediction using the model.\ny_pred = log_classifier.predict(X_test)\nprint(y_pred)","16a15805":"# Lets measure the accuracy of the model using the confusion matrix and accruacy score.\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\n\ncorr_pred = cm[0, 0] + cm[1, 1]\ntotal = cm.sum()\ncorr_pred_per = round(corr_pred\/total*100, 2)\nprint('Percentage of correct predictions: ', corr_pred_per)","83ccfa52":"# Lets check the confusion matrix\nprint(cm)","9efd9797":"# Lets check the accuracy using the accuracy_score from sklearn.\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy of the model: ', round((accuracy*100), 2))","f18e660c":"# Lets transform the encoded predicted variable back to it's original form for better understanding.\ny_pred_d_enco = le.inverse_transform(y_pred)\ny_test_d_enco = le.inverse_transform(y_test)\n\n# Also lets store them in a dataframe and check them parallely\ncomparison = pd.DataFrame()\ncomparison['Actual Values'] = y_test_d_enco\ncomparison['Predicted Values'] = y_pred_d_enco","ef2a5de9":"# Lets check all the data in the dataframe we just created.\npd.set_option('display.max_rows', 200)\ncomparison\n\n# Good to see most of them match.","1babc3bc":"# Creating the model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nknn_classifier.fit(X_train, y_train)\n\n# Predicting the outcome.\ny_pred_knn = knn_classifier.predict(X_test)","8fe2333e":"# Lets measure the accuracy of the model using the confusion matrix.\ncm = confusion_matrix(y_test, y_pred_knn)\n\ncorr_pred = cm[0, 0] + cm[1, 1]\ntotal = cm.sum()\ncorr_pred_per_knn = round(corr_pred\/total*100, 2)\nprint('Percentage of correct predictions: ', corr_pred_per_knn)","c50cecef":"# Lets check the accuracy using the accuracy_score from sklearn.\naccuracy = accuracy_score(y_test, y_pred_knn)\nprint('Accuracy of the model: ', round((accuracy*100), 2))","fa463663":"# Lets create a SVM classification model.\nfrom sklearn.svm import SVC\nsvc_classifier = SVC(kernel = 'linear')\nsvc_classifier.fit(X_train, y_train)\n\n# Predicting the outcome.\ny_pred_svm = svc_classifier.predict(X_test)","725f08bc":"# Lets measure the accuracy of the model using the confusion matrix.\ncm = confusion_matrix(y_test, y_pred_svm)\n\ncorr_pred = cm[0, 0] + cm[1, 1]\ntotal = cm.sum()\ncorr_pred_per_svm = round(corr_pred\/total*100, 2)\nprint('Percentage of correct predictions: ', corr_pred_per_svm)","d4790236":"# Lets check the accuracy using the accuracy_score from sklearn.\naccuracy = accuracy_score(y_test, y_pred_svm)\nprint('Accuracy of the model: ', round((accuracy*100), 2))","28bc618a":"k_svm_classifier = SVC(kernel = 'rbf')\nk_svm_classifier.fit(X_train, y_train)\n\n# Predicting the outcome.\ny_pred_k_svm = k_svm_classifier.predict(X_test)","e0b2479e":"# Lets measure the accuracy of the model using the confusion matrix.\ncm = confusion_matrix(y_test, y_pred_k_svm)\n\ncorr_pred = cm[0, 0] + cm[1, 1]\ntotal = cm.sum()\ncorr_pred_per_k_svm = round(corr_pred\/total*100, 2)\nprint('Percentage of correct predictions: ', corr_pred_per_k_svm)","05a2e090":"# Lets check the accuracy using the accuracy_score from sklearn.\naccuracy = accuracy_score(y_test, y_pred_k_svm)\nprint('Accuracy of the model: ', round((accuracy*100), 2))","cce5bb4c":"from sklearn.naive_bayes import GaussianNB\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train, y_train)\n\n# Predicting the outcome.\ny_pred_nb = nb_classifier.predict(X_test)","b68f033e":"# Lets measure the accuracy of the model using the confusion matrix.\ncm = confusion_matrix(y_test, y_pred_nb)\n\ncorr_pred = cm[0, 0] + cm[1, 1]\ntotal = cm.sum()\ncorr_pred_per_nb = round(corr_pred\/total*100, 2)\nprint('Percentage of correct predictions: ', corr_pred_per_nb)","a5682c34":"# Lets check the accuracy using the accuracy_score from sklearn.\naccuracy = accuracy_score(y_test, y_pred_nb)\nprint('Accuracy of the model: ', round((accuracy*100), 2))","034c9c43":"from sklearn.tree import DecisionTreeClassifier\ndt_classifier = DecisionTreeClassifier(criterion = 'entropy')\ndt_classifier.fit(X_train, y_train)\n\n# Predicting the outcome.\ny_pred_dt = dt_classifier.predict(X_test)","b504d525":"# Lets measure the accuracy of the model using the confusion matrix.\ncm = confusion_matrix(y_test, y_pred_dt)\n\ncorr_pred = cm[0, 0] + cm[1, 1]\ntotal = cm.sum()\ncorr_pred_per_dt = round(corr_pred\/total*100, 2)\nprint('Percentage of correct predictions: ', corr_pred_per_dt)","966e4357":"# Lets check the accuracy using the accuracy_score from sklearn.\naccuracy = accuracy_score(y_test, y_pred_dt)\nprint('Accuracy of the model: ', round((accuracy*100), 2))","8f2394f0":"from sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy')\nrf_classifier.fit(X_train, y_train)\n\n# Predicting the outcome.\ny_pred_rf = rf_classifier.predict(X_test)","a71e3b91":"# Lets measure the accuracy of the model using the confusion matrix.\ncm = confusion_matrix(y_test, y_pred_rf)\n\ncorr_pred = cm[0, 0] + cm[1, 1]\ntotal = cm.sum()\ncorr_pred_per_rf = round(corr_pred\/total*100, 2)\nprint('Percentage of correct predictions: ', corr_pred_per_rf)","298b6447":"# Lets check the accuracy using the accuracy_score from sklearn.\naccuracy = accuracy_score(y_test, y_pred_rf)\nprint('Accuracy of the model: ', round((accuracy*100), 2))","350e4270":"# Naive Bayes","f27eb071":"# Support Vector Machine","d1461e28":"# Kernel SVM","a311acb6":"# K Nearest Neighbors Classifier","8550e2c5":"# Decision Tree Clssifier","3b55458c":"# Random Forest Classfier","793f50d8":"# Logistic Regression","c1cbffa4":"### This work is in progress."}}