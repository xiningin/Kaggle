{"cell_type":{"5750065a":"code","2f74a8d4":"code","3e5c9904":"markdown","a24e2ad7":"markdown","bb9a8925":"markdown","31486fa3":"markdown","f535f114":"markdown","2efbd6c5":"markdown","d85bfb7a":"markdown","cf69fff5":"markdown","2535aea6":"markdown","cbf7e653":"markdown","850e6483":"markdown","a5106e64":"markdown","30a33665":"markdown","1d8eac9d":"markdown","3ab7c92c":"markdown","83b85b74":"markdown","2e4bb3a1":"markdown","d8be5273":"markdown","81b1b104":"markdown","3d1751b7":"markdown","5ec4471d":"markdown","d62869c1":"markdown","d5ae0dc8":"markdown","76aa607a":"markdown","df14542e":"markdown","6cabf02a":"markdown","c424925f":"markdown","9475eba0":"markdown","29636356":"markdown","24b1d385":"markdown","772c5655":"markdown","0cf5eefd":"markdown","bbaabb10":"markdown","e1f7ea0d":"markdown","7c5f6d8b":"markdown","005cdea5":"markdown","2bb9c415":"markdown","58497253":"markdown"},"source":{"5750065a":"import pandas as pd\nimport numpy as np\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain.head(5)","2f74a8d4":"#cut_labels = [0, 1, 2, 3]  # bin lables by numbers\ncut_labels = [\"child\", \"young\", \"adult\", \"senior\"] # bin lables by texts\ncut_bins = [0, 15.99, 25.99, 59.99, 200] #bin range\ntrain['Age_bin'] = pd.cut(train['Age'], bins=cut_bins, labels=cut_labels) # map values to bins\ntrain.head(5)","3e5c9904":"### Data normalisation\n* check whether the range of two or more numerical variables are significantly different\n* Apply a scaler to normalise data\n    *  MaxMin scaler\n    * Standard scaler ","a24e2ad7":"### Binning Numerical Features \n* Sometimes we prefer categorical rather than continuous predictors\n    * e.g. partition the numerical predictor **Age** into four bins\n        * child, young, adult and  senior\n* Partition any numerical predictors into bins\n    * e.g. using a map\n        *  $Age < 16: $ child: 0\n        *  $16\\le Age < 26: $ young: 1\n        *  $26 \\le Age < 60: $ adult: 2\n        *  $Age \\ge 60:$ senior: 3\n\n|<img src =\"https:\/\/images-na.ssl-images-amazon.com\/images\/I\/71dpNSzURtL._AC_SL1500_.jpg\" width =100>|\n|:--:|\n|[Four bins](https:\/\/images-na.ssl-images-amazon.com\/images\/I\/71dpNSzURtL._AC_SL1500_.jpg)|\n\n### Example: Binning Numerical Features\n*`Pandas.cut` function bins a numerical variable on a dataframe.","bb9a8925":"### Data Types\n* **Structured data:**  traditional spreadsheets from databases, structured with columns, rows and attribute names\n* **Unstructured data:** data without structure such as books, newspapers,  TV, websites, Youtube, Photos \n* **Semi-structured data:** data with some structure such as XML, HTML, Log files\n\n![image.png](attachment:image.png) ","31486fa3":"## Part 3: Data Preparation \n### Raw Data\n* Raw data often are incomplete, noisy and may contain:\n    * Obsolete fields\n    * Missing values\n    * Outliers\n    * Erroneous values\n    * Data format is not suitable for machine learning models","f535f114":"## Part 6 Three Models (Classifiers)\n### Model 1: k-Nearest Neighbour algorithm \n* KNN is a kind of **instance-based** learning\n    1. Given a set of instances (e.g. Patients 1 to 4)\n    1. For a new sample (e.g. Patient 0), choose $k$ instances most similar to the new sample. $k$ is a hyperparameter\n    1. Assignn the class label (e.g. drug) to the new sample based on its neighbour instances' labels. All points from $k$ neighbours have a vote  \n    \n    ![image.png](attachment:image.png)\n\n* In KNN, the model is the above algorithm.  ","2efbd6c5":"### 10 Lectures in Term 1 \n1. Introduction\n1. CRISP-DM and Case Study 1\n1. Data Preprocessing\n1. Exploratory Data Analysis\n1. Basics of Machine Learning\n1. Nearest Neighbour Algorithm\n1. Decision Tree\n1. Logistic Regression\n1. Model Selection and Hyperparameter Tuning\n1. Pipeline and Model Evaluation\n ","d85bfb7a":"### Model 2: Decision Trees \n* **Decision Tree** is a flowchart structure consiting of **decision nodes**, connected by **branches**, extending downward from **root node** to terminating **leaf nodes**\n* Beginning with root node, attributes tested at decision nodes, and each possible outcome results in branch\n* Each branch leads to decision node or leaf node \n\n ![image.png](attachment:image.png)\n\n* The tree can be interpreted as `IF THEN` rules, e.g. IF income = low is false, THEN credit risk is good","cf69fff5":"### EDA (4)\n* Explore the relationship among three variables\n    * Example: the relationship among `Day minutes`, `Evening Minutes`, `Churner`\n    \n![image.png](attachment:image.png)\n\n> Q: how to explore four variables?","2535aea6":"###  Supervised learning\n* Learning on data with labels \n    * Example: given an iris flower data set with species labels (by three colours), build a ML model for classifying an iris flower into one of three species\n\n![image.png](attachment:image.png)  ","cbf7e653":"### Module content \n![image.png](attachment:f1607106-0095-4111-8bb5-6e06c7c7566e.png) ","850e6483":"### Decision trees \nThere are different algorithms to grow a tree using different splitting criteria\n1. **Gini** index determines the purity of a specific class as a result of a decision to branch along a particular attribute\/value\n    * Used in CART (classification and regression trees)\n2. Information gain uses **entropy** to measure the extent of uncertainty or randomness of a particular attribute\/value split\n    * Used in ID3, C4.5, C5\n3. Chi-square statistics\n    * used in CHAID","a5106e64":"## Part 2 Data Understanding\n### Exploratory Data Analysis (1) \n* Exploratory data analysis (EDA) is an approach to understand data and summarizing main characteristics of data\n* Usually adopt data visualisation methods\n* Identify an outlier\n    * An **outlier** is an extreme value  that goes against the trend of the remaining data.\n    * For example, there is  an extreme point at 192.5 in the figure\n    \n    ![image.png](attachment:image.png)","30a33665":"### Hyperparameters in Decision Tree\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html \n1. `criterion{\u201cgini\u201d, \u201centropy\u201d}, default=\u201dgini\u201d`\n    * The method to calculate the purity of a node\n1. `max_depthint, default=None`\n    * The maximum depth of the tree. \n1. `splitter{\u201cbest\u201d, \u201crandom\u201d}, default=\u201dbest\u201d`\n    * The strategy used to choose the split at each node","1d8eac9d":"## Part 1 Machine Learning for Data Analysis\n### Era of Big Data\n* We are in the Era of Big Data \n\n|<img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/7c\/Hilbert_InfoGrowth.png\" width=400>|\n|:--:|\n|[Growth of and digitization of global information-storage capacity](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/7c\/Hilbert_InfoGrowth.png)|\n\n* It becomes more and more important to discover useful information and knowledge from data ","3ab7c92c":"## Part 5 Build Model and Evaluate Model\n### Overfitting and Underfitting\n![image.png](attachment:image.png) \n\n* Overfitting \n    * a model performs very well on the training data\n    * but not on the test data\n    * e.g. overfit when the tree depth = 12  \n* Underfitting  \n    * a model does not fits the training data \n    * and not on test data too \n    * e.g. underfit when the tree depth = 1","83b85b74":"### Hyperparameters in Logistic Regression\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n1. `penalty{\u2018l1\u2019, \u2018l2\u2019, \u2018elasticnet\u2019, \u2018none\u2019}, default=\u2019l2\u2019`\n    * Used to penalization on large weights\n2. `solver{\u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018sag\u2019, \u2018saga\u2019}, default=\u2019lbfgs\u2019`\n    * Algorithm to find optimal coefficients in logistic regression curve\n3. `max_iterint, default=100`\n    * Maximum number of iterations taken for the solvers to stop.\n","2e4bb3a1":"### Data transformation\n* Convert  categorical features to numerical ones because Sklearn and Python libraries work only on numerical values\n    * Onehot Encoder for nominal features\n    * Ordinal Encoder for ordinal features\n* Convert numerical features into categorical ones or bins","d8be5273":"### Hyperparamters in KNN\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\n\n1. `n_neighborsint, default=5`\n    * Number of neighbors to vote\n1. `weights, default=\u2019uniform'`\n    * voting method  \n    * `uniform`: uniform weights. All points in each neighborhood have an equal vote\n    * `distance`: a closer neighbor has a large weighted vote\n1. `metric, default=\u2019minkowski\u2019`\n    * the distance metric to measure the similarity of two points","81b1b104":"###  Model Selection and Evaluation\n* Models (classiers) Taught in Term 1\n    1. K-Nearest Neighbour Algorithm\n    1.  Decision Tree\n    1. Logistic Regression \n\n* Model selection: how to select the final model from potential candidate models based on their performance on the training dataset\n    * select a model from different types of models, e.g. from three classifiers: logistic regression, decision tree and KNN\n    * select a model from the same model with different model hyperparameters, e.g., from the decision tree model with different tree depths\n* Model evaluation: how to evaluate the performance of a candidate models using certain metrics, e.g., accuracy  \n \n![image.png](attachment:image.png)","3d1751b7":"### Automatic Tuning of Hyperparameters: Search\n* Search optimal hyperparameters\n    * For each different combination of hyperparameters, evalaute the performance of the model and then choose the best one\n* Two search methods \n    1. `GridSearchCV`  \n        * apply a brute-force exhaustive search to different combinations of hyper-parameters   \n        * evaluates the model performance for each combination \n    2. `RandomizedSearchCV` \n        * apply randomized search to different combinations of hyper-parameters  \n        * each setting is sampled from a distribution over possible combinations (but not exhaustively) \n* Grid search is based on k-fold cross-evaluation","5ec4471d":"# Review of Term 1\n[COMP20121 Machine Learning for Data Analytics](https:\/\/sites.google.com\/site\/hejunhomepage\/Teaching\/machine-learning-for-data-analytics)\n\nAuthor: Jun He","d62869c1":"### Learning objectives\n* Review the content taught in Term 1","d5ae0dc8":"### Split Data: k-fold Cross-validation\n* In k-fold cross-validation, randomly split the training dataset into k folds without replacement\n    * k \u22121 folds are used for training\n    * 1 fold is used for validation\n* This procedure is repeated k times so that we obtain k models and performance estimates.\n* Advantage:\n    * Each sample point will be part of a training and test dataset exactly once\n\n![image.png](attachment:image.png) ","76aa607a":"### Workflow of Supervised Machine Learning\n![image.png](attachment:image.png)","df14542e":"### Model Evaluation\n* **Model evaluation** is the process of evaluating a model\u2019s performance \n* In binary classification\n    * Confusion Matrix is a 2 x 2 table which reports the number of false positives, false negatives, true positives, and true negatives\n    * Accuracy and Error (=1-Accuracy)\n    * Sensitivity (TPR)\n    * Specificity and  False-positive rate  (FPR = 1- Specificity)\n    * ROC curve  (x-axis: FPR and y-axis: TPR) and AUC (area under the curve)\n    * Classification Report: precision (PTP), recall (TPR),  f1-score (weighted mean of PTP and TPR)\n* In Multi-label classification\n    * Confusion Matrix \n    * Accuracy \n    * Classification Report: precision, recall,  f1-score","6cabf02a":"## Part 4 Machine Learning for Data Analysis\n### Data mining\n* Machine learning for data analysis = data mining\n* Its purpose is to transform data into information and knowledge  \n![image.png](attachment:image.png) ","c424925f":"### Model Building: Parameters\n* A model may include two types of parameters (configurations)\n    * An internal parameter of a model can be automatically estimated from data. This kind of parameters is not required to manually tune\n    * A hyperparameter cannot be automatically estimated from data. This parameter is required to manually tune","9475eba0":"### CRISP-DM  \n* We usually follow a process to develop a ML project for data analysis\n* **CRoss-Industry Standard Process for Data Mining** (CRISP-DM)  is the most widely-used data analytics model \n* CRISP-DM consists of 6 phases\n\n|<img src = \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/b9\/CRISP-DM_Process_Diagram.png\/800px-CRISP-DM_Process_Diagram.png\" width =400>|\n|:--:| \n|[Six phases of CRISP-DM](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/b9\/CRISP-DM_Process_Diagram.png\/800px-CRISP-DM_Process_Diagram.png)|","29636356":"### Splitting Data: Holdout Method\n* Split the data into three parts\n    * Training dataset: used to find the best internal parameter of a model \n    * Validation dataset: used for model selection and hyperparameter tuning \n    * Test dataset: used to evaluate the performance of a model\n* Model selection might be different depending on the way you split dataset \n\n![image.png](attachment:image.png)","24b1d385":"### EDA (3)\n* Explore the relationship between two variables\n    * Example:  the relationship between two variables `International plan` and `Churners`\n    \n![image.png](attachment:image.png)","772c5655":"### Neighbour and Distance function\n* Neighbour instances are measured by similarity\n* **Similarity** between two instances are measure by a distance function\n \n![image.png](attachment:image.png) \n\n* Let  $x=(x_1, \\cdots, x_n)$ and $y=(y_1, \\cdots, y_n)$, i.e., two data points with $n$ numerical features\n    * **Euclidean Distance** $d(x, y)= \\sqrt{ (x_1-y_1)^2 + \\cdots (x_n-y_n)^2}$\n    * **Manhattan Distance** $d(x, y)= |x_1-y_1| + \\cdots + |x_n-y_n|$","0cf5eefd":"### Exploratory data analysis (2) \n* Explore the distribution of one variable\n\n    * Example: the distribution of `Churners`\n    \n![image.png](attachment:image.png)","bbaabb10":"###  Logistic regression curve\n* The curve is represented by a sigmoid function \n    $$ \\pi(x) = \\frac{e^{\\beta_0 +\\beta_1 x_1+ \\cdots + \\beta_n x_n}}{1+e^{\\beta_0 +\\beta_1 x+\\cdots+\\beta_n x_n}}$$\n    * $\\pi(x)$ is the probability of outcome being 1\n    * $\\beta_0$ is an intercept\n    * $\\beta_1, \\cdots, \\beta_n$ are weights on the input $x_1, \\cdots, x_n$ respectively\n* The value $\\pi(x)$ is explained as the probability of record $x$ with label $1$\n    * The higher $\\pi(x)$, the more likely outcome is 1, while the smaller $\\pi(x)$, the less likely outcome is 1\n    * For example, given a patient with $Age =50$, $\\pi(50) = 0.025$ means the probability of the patient with the disease presence = 0.025 or 2.5% ","e1f7ea0d":"### Descriptive Statistic \n* Descriptive statistic quantitatively describes a numerical variable\n    * number of samples\n    * mean\n    * median\n    * maximal and minimal values\n    * quartiles\n    * standard deviation\n* Example: describe `Account Length` with Descriptive Statistics\n\n|Statistics| Value|\n|---------------|--------|\n| count   | 3333. 000000 |\n| mean     | 101. 064806 |\n| std       | 39. 822106 |\n| min        | 1.000000 |\n| 25%        |74.000000 |\n| 50%      | 101.000000 |\n| 75%      | 127.000000 |\n| max      | 243.000000 |","7c5f6d8b":"### Unsupervised learning\n\n* Learning on data without labels\n    * Example: given an iris flower data set without species labels (same color in the figure), build a model for grouping flowers into clusters (species). Biologists enounter this learning task when then first discovered iris flowers.\n    \n![image.png](attachment:image.png)","005cdea5":"### Data Cleansing\n* Detect missing values, incorrect values, inaccurate values or irrelevant values in data \n    * Detect standard missing values: NaN values or blank values\n        * `dataframe.isnull()` function can do it\n    * Detect non-standard missing or inaccurate values\n        * mannually detect\n* Handle missing values, incorrect values, inaccurate values or irrelevant values\n    * Drop missing values\n    * Replace missing values with mean, median, mode or other specific values","2bb9c415":"### Machine Learning Models\n* A model is a computer program which is able to output some useful results after inputting some data\n\n![image.png](attachment:image.png) \n\n* A model is trained on data with some learning algorithm  \n","58497253":"### Model 3: Logistic Regression\n* Logistic regression is widely applied to binary classification with class labels 0 and 1\n* Find a S-shape curve between predictors and target variable based on observations\n* For example, the predictor is `Age`, and the target variable is `Disease` where $1$ for presence and $0$ for absense\n![image.png](attachment:image.png)"}}