{"cell_type":{"ebb17a75":"code","b61a4f21":"code","54d19566":"code","6c5b0399":"code","94e97180":"code","b2a490f4":"code","b5183e70":"code","f190cc3a":"code","035363f1":"code","d89baf26":"code","793bd163":"code","fba7c3cc":"code","e3df1a16":"code","395f4b76":"code","dae304a3":"code","90542849":"code","421b9f45":"code","0ee1cd60":"code","41266aed":"code","5f980761":"code","7cbcaea1":"code","73a4fdd7":"code","8a08fd64":"code","ede6c2cf":"code","e23546a9":"code","991cf758":"code","ffd007d1":"code","09a3829a":"code","30d273b1":"code","8972163e":"code","e6953232":"code","2b4cbf34":"code","6c31a58b":"code","f4c7b696":"code","16e02112":"code","a5b0d682":"code","6660f400":"code","172d11de":"code","20d88845":"code","ed6f4d2a":"code","4d533d6f":"code","9311889e":"code","419a62a2":"code","0a2ddd4d":"code","5cfaac10":"code","f4ab8d68":"code","5260bd60":"code","110bda65":"code","85821132":"code","d6dc7d1b":"code","01eb4e7f":"code","717dd670":"code","5a332616":"code","14822eaa":"code","e2896e12":"code","83598d3d":"code","a98c7f8c":"code","a16f9596":"code","c0d9da7d":"code","8600f317":"code","2e537f7f":"code","84ff845d":"code","c8b0122c":"code","10c29a1a":"code","bd5cdf0d":"code","4dd4ce7c":"markdown","a30ff326":"markdown","be32b263":"markdown","ad4d055f":"markdown","6c34b151":"markdown","fcc416bb":"markdown","d821017a":"markdown","edf048ac":"markdown","248b435c":"markdown","952d09d7":"markdown","55b9ad81":"markdown","b7816d77":"markdown","598fa7c6":"markdown","6a9c258c":"markdown","83873c38":"markdown","052797e1":"markdown","7a8c6068":"markdown"},"source":{"ebb17a75":"import pandas as pd\nimport seaborn as sns\nimport re\nsns.set()\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport matplotlib.ticker as ticker\nplt.rc(\"font\", size=14)\nimport warnings\nwarnings.simplefilter(action='ignore')","b61a4f21":"# seaborn color palettes \n\npalette_1 = sns.color_palette('Accent', 6)\npalette_2 = sns.color_palette('Set1', 6)\npalette_3 = sns.color_palette('BrBG')\npalette_4 = sns.color_palette('CMRmap')\npalette_5 = sns.color_palette('Paired', 6)\npalette_6 = sns.color_palette('RdYlBu')\npalette_binary_1 = sns.color_palette('Accent_r', 2)\npalette_binary_2 = sns.color_palette('Set1', 2)\npalette_binary_3 = sns.color_palette('Set2', 2)\n\nfor color in [palette_1, palette_2, palette_3, palette_4, palette_5, \n              palette_6, palette_binary_1, palette_binary_2, palette_binary_3]:\n        sns.palplot(color)","54d19566":"Train = pd.read_csv(\"..\/input\/titanic\/train.csv\", header=0)","6c5b0399":"test = pd.read_csv(\"..\/input\/titanic\/test.csv\", header=0)","94e97180":"test.head()","b2a490f4":"Train.head()","b5183e70":"train = pd.concat([Train,test],axis=0)","f190cc3a":"train.shape","035363f1":"train.describe()","d89baf26":"# let's plot pair plot to visualise the attributes all at once\nsns.pairplot(data=train, hue = 'Survived')\nplt.savefig(\"palette_binary_1\")","793bd163":"train.info() ","fba7c3cc":"train.Fare= train.Fare.fillna(train.Fare.mean()) ","e3df1a16":"# percent of missing \"Embarked\" \nprint('Percent of missing \"Embarked\" records is %.2f%%' %((train['Embarked'].isnull().sum()\/train.shape[0])*100))","395f4b76":"train.Age= train.Age.fillna(train.Age.median()) #filling missing values by median","dae304a3":"train[\"Embarked\"].fillna(train['Embarked'].value_counts().idxmax(), inplace=True)","90542849":"# percent of missing \"Cabin\" \nprint('Percent of missing \"Cabin\" records is %.2f%%' %((train['Cabin'].isnull().sum()\/train.shape[0])*100))","421b9f45":"train.info()","0ee1cd60":"plt.figure(figsize=(16, 7))\ntrain['Age_cat'] = pd.cut(train.Age, bins=[0, 5, 24, 30, 36, 40, 50, 60, 70, 80])\n\nsns.countplot(data=train, x='Age_cat', hue='Survived', palette=palette_binary_3)\n\nplt.show()","41266aed":"cat_features = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n\nplt.figure(figsize=(16, 14))\nsns.set(font_scale= 1.2)\nsns.set_style('ticks')\n\nfor i, feature in enumerate(cat_features):\n    plt.subplot(2, 3, i+1)\n    sns.countplot(data=train, x=feature, hue='Survived', palette=palette_5)  \n    \nsns.despine()","5f980761":"# percent of Survived male\nmale = train.loc[train.Sex == 'male'][\"Survived\"]\nrate_male =sum(male)\/len(male)*100\nprint(rate_male)","7cbcaea1":"# percent of Survived female\nFemale = train.loc[train.Sex == 'female'][\"Survived\"]\nrate_Female =sum(Female)\/len(Female)*100\nprint(rate_Female)","73a4fdd7":"temp = train.copy()\ntemp['Cabin'] = temp.Cabin.str.extract(pat='([A-Z])')\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 8))\nsns.set_style('ticks')\n\nsns.countplot(data=temp, x='Cabin', hue='Pclass', ax=ax[0], palette=palette_1)\nax[0].set_title('Pclass-Cabin Proportions', x=0.28, y=1.04, size=25)\n\ntemp.Cabin.fillna('missing', inplace=True)\ntemp_missing = temp.loc[temp.Cabin == 'missing']\n\nsns.countplot(data=temp_missing, x='Cabin', hue='Pclass', palette=palette_1)\nax[1].set_title('Missing Cabin proportions', x=0.27, y=1.04, size=25)\n\nsns.despine()\nplt.show()","8a08fd64":"num_features = ['Fare', 'Age']\nsns.set_style('white')\n\nplt.figure(figsize=(16, 14))\nfor i, feature in enumerate(num_features):\n    plt.subplot(2, 2, i+1)\n    plt.hist(x=[train[feature][train['Survived'] == 1], train[feature][train['Survived']==0]],\n            stacked=True, label=['Survived', 'Not Survived'], bins=20, color=['orange', 'b'])\n    plt.legend()\n    plt.xlabel(f'{feature}', fontsize=15)\n    plt.ylabel('Count', fontsize=15)","ede6c2cf":"plt.figure(figsize=(15,8))\nax = sns.kdeplot(train[\"Age\"][train.Survived == 1], color=\"red\", shade=True)\nsns.kdeplot(train[\"Age\"][train.Survived == 0], color=\"lightcoral\", shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Age for Surviving Population and Deceased Population')\nax.set(xlabel='Age')\nplt.xlim(-10,85)\nplt.show()","e23546a9":"plt.figure(figsize=(15,8))\nax = sns.kdeplot(train[\"Fare\"][train.Survived == 1], color=\"green\", shade=True)\nsns.kdeplot(train[\"Fare\"][train.Survived == 0], color=\"blue\", shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Fare for Surviving Population and Deceased Population')\nax.set(xlabel='Fare')\nplt.xlim(-10,85)\nplt.show()","991cf758":"temp = train.copy()\ntemp['Family_size'] = temp['SibSp']+temp['Parch']+1 # +1cause if there is no sibsb or parch then it would consider alone pass\nsns.set_style('ticks')\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 8))\nsns.countplot(data=temp, x='Family_size', hue='Survived', ax=ax, palette=palette_6)\nax.set_title('Family Size - Survived Plot', size=25, loc='Left', y=1.04)\n\nsns.despine()\nplt.show()","ffd007d1":"temp['Family_size_cat'] = temp['Family_size'].replace({1:'alone', 2:'small_family', 3:'small_family', 4:'small_family'\n                                                      ,5:'large_family', 6:'large_family', 7:'large_family'\n                                                      ,8:'large_family', 9:'large_family', 10:'large_family', \n                                                       11:'large_family'})\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 8))\nsns.set_style('ticks')\n\nsns.countplot(data=temp, x='Family_size_cat', hue='Survived', ax=ax, palette=palette_5)\nax.set_title('Family Category - Survived Plot', size=25, loc='Left', y=1.04)\n\nsns.despine()\nplt.show()","09a3829a":"temp = train.copy()\ntemp['Name_length'] = temp.Name.str.replace(pat='[^a-zA-Z]', repl='').str.len()\nsns.set_style('ticks')\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.histplot(data=temp, x='Name_length', hue='Survived', kde=True, fill=True, ax=ax, palette=palette_binary_2)\nax.set_title('Name Length - Survived Plot', size=20, loc='Left', y=1.03)\n\nsns.despine()\nplt.show()","30d273b1":"temp = train.copy()\n\ntemp['Title'] = temp.Name.str.extract(pat='([a-zA-Z]+\\.)')\n\ntemp.Title[~temp.Title.isin(['Mr.', 'Miss.', 'Mrs.', 'Master.'])] = 'rare'","8972163e":"fig, ax = plt.subplots(1, 1, figsize=(15, 8))\nsns.set_style('ticks')\n\nsns.countplot(data=temp, x='Title', hue='Survived', ax=ax, palette=palette_6)\nax.set_title('Title - Survived Plot', loc='Left', size=25, y=1.03)\n\nsns.despine()\nplt.show()","e6953232":"temp.head()","2b4cbf34":"df = pd.get_dummies(train, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\ndf.drop('Sex_female', axis=1, inplace=True)\ndf.drop('Ticket', axis=1, inplace=True)\ndf.drop('SibSp', axis=1, inplace=True)\ndf.drop('Parch', axis=1, inplace=True)\ndf.drop('Age_cat', axis=1, inplace=True)\ndf.drop('Cabin', axis=1, inplace=True)\ndf.drop('Name', axis=1, inplace=True)\ndf.drop('Pclass_3', axis=1, inplace=True)\ndf.drop('Embarked_C', axis=1, inplace=True)\n\ndf.head()","6c31a58b":"plt.figure(figsize=(16, 6))\nheatmap =sns.heatmap(df.corr(), annot = True, cmap= 'Greens')\nheatmap.set_title('Correlation Heatmap', fontdict = {'fontsize':20}, pad =14);","f4c7b696":"from sklearn.metrics import classification_report\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, roc_auc_score, precision_score\nfrom sklearn.metrics import roc_curve\nfrom statsmodels.tools import add_constant\nimport warnings\nwarnings.filterwarnings('ignore')","16e02112":"new_train = df.iloc[:891,:]\nnew_test = df.iloc[891:,:]","a5b0d682":"new_train[['Survived']]=new_train[['Survived']].astype('int64')","6660f400":"x = new_train.drop(['Survived'], axis=1)\ny = new_train[['Survived']]","172d11de":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42) # 70% training and 30% test","20d88845":"from sklearn.preprocessing import StandardScaler\nx_train = StandardScaler().fit_transform(x_train)\nx_test = StandardScaler().fit_transform(x_test)","ed6f4d2a":"logReg = LogisticRegression().fit(x_train, y_train)\ntrain_pred = logReg.predict(x_train)\ntest_pred = logReg.predict(x_test)\n\nprint('train set accuracy:', accuracy_score(y_train, train_pred))\nprint(' test set accuracy:', accuracy_score(y_test, test_pred))","4d533d6f":"x_cons = sm.add_constant(x)","9311889e":"result = sm.Logit(y, x_cons).fit()\nresult.summary()","419a62a2":"x.drop(['Fare'], axis=1, inplace=True)\n\nresult = sm.Logit(y, x).fit()\nresult.summary()","0a2ddd4d":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=254)\n\nlogReg = LogisticRegression().fit(x_train, y_train)\ntrain_pred = logReg.predict(x_train)\ntest_pred = logReg.predict(x_test)\n\nprint('New train set accuracy:', accuracy_score(y_train, train_pred))\nprint('New test set accuracy:', accuracy_score(y_test, test_pred))\npred_prob = logReg.predict_proba(x_test)","5cfaac10":"log_reg=accuracy_score(y_test, test_pred)\ncm=confusion_matrix(y_test,test_pred)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\n\nall_sample_title = 'Accuracy Score: {0}'.format(log_reg)\n\nplt.title(all_sample_title, size = 19)\nplt.savefig(\"pne.png\")","f4ab8d68":"from sklearn.metrics import classification_report\nimport statsmodels.api as sm\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, roc_auc_score, precision_score, auc\nfrom sklearn.metrics import roc_curve\nimport warnings\nwarnings.filterwarnings('ignore')","5260bd60":"pip install pydotplus","110bda65":"#for decision tree object\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold, StratifiedKFold\nfrom sklearn.tree import plot_tree\nfrom sklearn import tree\nfrom IPython.display import Image\nimport pydotplus","85821132":"#Decision tree\nclf = tree.DecisionTreeClassifier(criterion=\"gini\",random_state = 42, max_depth=5,\n                            min_samples_split=5,min_samples_leaf=1,min_impurity_decrease = 0.001)\n                            \nclf = clf.fit(x_train,y_train)\n\n#Predict the response\ny_pred1 = clf.predict(x_test)\nprint(\"Classification report - \\n\", classification_report(y_test,y_pred1))\nDtree=accuracy_score(y_test, y_pred1)","d6dc7d1b":"dot_data = tree.export_graphviz(clf, out_file=None, \n                              feature_names=x_train.columns, \n                              filled=True, rounded=True,  \n                              special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data)\nImage(graph.create_png())","01eb4e7f":"cm = confusion_matrix(y_test, y_pred1)\nplt.figure(figsize = (8,5))\n\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nsns.heatmap(conf_matrix, annot=True,  cmap = 'YlGnBu')\n\n# print the scores on training and test set\nall_sample_title = 'Accuracy Score: {0}'.format(accuracy_score(y_test, y_pred1))\n\nplt.title(all_sample_title, size = 19)\nplt.savefig(\"pne.png\")","717dd670":"from sklearn.ensemble import BaggingClassifier\n\n#Create Model\nbag_clf = BaggingClassifier(base_estimator=clf, n_estimators=300,\n                            bootstrap=True,bootstrap_features=True, n_jobs=-1,\n                            random_state=42)\n#fit model\nbag_clf.fit(x_train, y_train)\ny_pred2 = bag_clf.predict(x_test)\nbagging = accuracy_score(y_test, y_pred2)","5a332616":"cm = confusion_matrix(y_test,y_pred2)\nplt.figure(figsize = (8,5))\n\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nsns.heatmap(conf_matrix, annot=True,  cmap = 'YlGnBu')\n\n# print the scores on training and test set\nall_sample_title = 'Accuracy Score: {0}'.format(accuracy_score(y_test, y_pred2))\n\nplt.title(all_sample_title, size = 19)\nplt.savefig(\"pne.png\")","14822eaa":"from sklearn.ensemble import RandomForestClassifier\n#creating model\nrf_clf = RandomForestClassifier(criterion = 'gini',\n                                n_estimators=700,\n                                oob_score= True,\n                                max_features ='log2',\n                                min_samples_split=10,\n                                min_samples_leaf=3,\n                                bootstrap=True,\n                                n_jobs=-1,\n                                random_state=1)\n#fitting model\nrf_clf.fit(x_train, y_train)\ny_pred3 = rf_clf.predict(x_test)\nRF=accuracy_score(y_test, y_pred3)","e2896e12":"cm = confusion_matrix(y_test, y_pred3)\nplt.figure(figsize = (8,5))\n\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nsns.heatmap(conf_matrix, annot=True,  cmap = 'YlGnBu')\n\n# print the scores on training and test set\nall_sample_title = 'Accuracy Score: {0}'.format(accuracy_score(y_test, y_pred3))\n\nplt.title(all_sample_title, size = 19)\nplt.savefig(\"pne.png\")","83598d3d":"from sklearn.ensemble import AdaBoostClassifier\n\n# Ada boosting tunning\ndtree = DecisionTreeClassifier(criterion='gini', max_depth = 3, random_state =42)\n\nadaclass = AdaBoostClassifier(base_estimator=dtree,\n                             n_estimators = 300,\n                             learning_rate = 0.01,\n                             algorithm= 'SAMME',\n                             random_state = 42)\n\n\nadaclass.fit(x_train,y_train)\ny_pred4 = adaclass.predict(x_test)\nAda =accuracy_score(y_test, y_pred4)","a98c7f8c":"cm = confusion_matrix(y_test, y_pred4)\nplt.figure(figsize = (8,5))\n\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nsns.heatmap(conf_matrix, annot=True,  cmap = 'YlGnBu')\n\n# print the scores on training and test set\nall_sample_title = 'Accuracy Score: {0}'.format(accuracy_score(y_test, y_pred4))\n\nplt.title(all_sample_title, size = 19)\nplt.savefig(\"pne.png\")","a16f9596":"from sklearn.ensemble import GradientBoostingClassifier\n# create model\ngradclass =  GradientBoostingClassifier(learning_rate=0.3, loss='deviance', max_depth=5,\n                                         max_features='auto',min_impurity_decrease=0.001, min_impurity_split=None,\n                                         min_samples_leaf=1, min_samples_split = 4, n_estimators=300)\n                                    \n\n#fit model\ngradclass.fit(x_train,y_train)\ny_pred5 = gradclass.predict(x_test)\nGrad=accuracy_score(y_test, y_pred5)","c0d9da7d":"y_pred= gradclass.predict(x_test)\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (8,5))\n\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nsns.heatmap(conf_matrix, annot=True,  cmap = 'YlGnBu')\n\n# print the scores on training and test set\nall_sample_title = 'Accuracy Score: {0}'.format(accuracy_score(y_test, y_pred))\n\nplt.title(all_sample_title, size = 19)\nplt.savefig(\"pne.png\")","8600f317":"import xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import model_selection","2e537f7f":"xgb = XGBClassifier(\n     learning_rate = 0.1,\n     n_estimators = 1000,\n     max_depth = 3,\n     min_child_weight=1,\n     gamma = 0.1,\n     alpha = 1e-05, \n     subsample = 0.9,\n     colsample_bytree = 0.6,\n     objective = 'binary:logistic',\n     nthread = 5,\n     scale_pos_weight =1,\n     seed = 27)\n\nxgb.fit(x_train, y_train)\ny_pred6=xgb.predict(x_test)\nXGB=accuracy_score(y_test, y_pred6)\nXGB","84ff845d":"cm = confusion_matrix(y_test, y_pred6)\nplt.figure(figsize = (8,5))\n\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nsns.heatmap(conf_matrix, annot=True,  cmap = 'YlGnBu')\n\n# print the scores on training and test set\nall_sample_title = 'Accuracy Score: {0}'.format(accuracy_score(y_test, y_pred6))\n\nplt.title(all_sample_title, size = 19)\nplt.savefig(\"pne.png\")","c8b0122c":"models = pd.DataFrame({\n    'Model': ['Logistic Regreesion','Decision Tree','Bagging','Random Forest', 'Gradient boosting', \n              'Adaboost', 'XG Boost'],\n\n    'Score': [log_reg,Dtree, bagging, RF, Grad, Ada,XGB]})\nmodels.sort_values(by='Score', ascending=True)","10c29a1a":"# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n\ny_score0 = logReg.predict_proba(x_test)[:,1]\nlog_fpr, log_tpr, thresh = roc_curve(y_test, y_score0)\nauc_log = auc(log_fpr, log_tpr)\n\ny_score1 = clf.predict_proba(x_test)[:,1]\ntree_fpr, tree_tpr, thresh = roc_curve(y_test, y_score1)\nauc_tree = auc(tree_fpr, tree_tpr)\n\ny_score2 = bag_clf.predict_proba(x_test)[:,1]\nbag_fpr, bag_tpr, thresh = roc_curve(y_test, y_score2)\nauc_bag = auc(bag_fpr, bag_tpr)\n\ny_score3 = rf_clf.predict_proba(x_test)[:,1]\nrf_fpr, rf_tpr, thresh = roc_curve(y_test, y_score3)\nauc_rf = auc(rf_fpr, rf_tpr)\n\ny_score4 = adaclass.predict_proba(x_test)[:,1]\nada_fpr, ada_tpr, thresh = roc_curve(y_test, y_score4)\nauc_ada = auc(ada_fpr, ada_tpr)\n\ny_score5 = gradclass.predict_proba(x_test)[:,1]\ngrad_fpr, grad_tpr, thresh = roc_curve(y_test, y_score5)\nauc_grad = auc(grad_fpr, grad_tpr)\n\ny_score6 = xgb.predict_proba(x_test)[:,1]\nxgb_fpr, xgb_tpr, thresh = roc_curve(y_test, y_score6)\nauc_xgb = auc(xgb_fpr, xgb_tpr)\n\n\nplt.figure(figsize=(8, 5), dpi = 100)\n# plot roc curves\nplt.plot(log_fpr, log_tpr, linestyle='--',color='grey', label='logReg(auc = %0.3f)'%auc_log)\nplt.plot(tree_fpr, tree_tpr, linestyle='--',color='red', label='tree(auc = %0.3f)'%auc_tree)\nplt.plot(bag_fpr, bag_tpr, linestyle='--',color='blue', label='bag(auc = %0.3f)'%auc_bag)\nplt.plot(rf_fpr, rf_tpr, linestyle='--',color='green', label='rf(auc = %0.3f)'%auc_rf)\nplt.plot(ada_fpr, ada_tpr, linestyle='--',color='pink', label='ada(auc = %0.3f)'%auc_ada)\nplt.plot(grad_fpr, grad_tpr, linestyle='--',color='black', label='grad(auc = %0.3f)'%auc_grad)\nplt.plot(xgb_fpr, xgb_tpr, linestyle='--',color='orange', label='XGb(auc = %0.3f)'%auc_xgb)\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n\n# x label y label\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.show();","bd5cdf0d":"submit = pd.DataFrame({\"PassengerId\":x_test.PassengerId, \"Survived\":y_pred2})\nsubmit.to_csv(\"submissionT.csv\", index = False)","4dd4ce7c":"# Random Forest","a30ff326":"# Gradient Boosting","be32b263":"## Missing Vlues","ad4d055f":"# XG Boost ","6c34b151":"# Ensemble Techniques","fcc416bb":"## Logit","d821017a":"###### here cabin has 77% missing value so we ignore this clm","edf048ac":"## Confusion Matrix Logistic","248b435c":"## Logistic Regression","952d09d7":"## Visualization","55b9ad81":"###### Great!! we can see that with increase in the name length the survival rate increases!! from about name_length 26 more pople with name length more than 26 survived than not survived.","b7816d77":"# Adaboost","598fa7c6":"## Create Dummy variables","6a9c258c":"###### here most common boarding port of embarkation is S.. so we replace  Nan value by s","83873c38":"## Correlation Heatmap","052797e1":"# Bagging","7a8c6068":"# Decision Tree"}}