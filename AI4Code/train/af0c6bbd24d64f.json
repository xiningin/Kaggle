{"cell_type":{"fcad1f3d":"code","2bd49cb9":"code","9081c08a":"code","1fb6f643":"code","77acaa7d":"code","c26fbcb6":"code","2ef7afac":"code","c62e2954":"code","072669ac":"code","d060edf0":"code","4b6bee34":"code","9a612f14":"code","e2a61d16":"code","3ee7ce66":"code","d3a9c464":"code","296948ef":"code","a25b1ba5":"code","dee49897":"code","0d106444":"code","fb3f2728":"code","fbbe84cc":"code","5e40b255":"code","b4255bbe":"code","9ad8eb2a":"code","3390c7c5":"code","72767489":"code","03095454":"code","fc3e9122":"code","c7cac069":"code","5ac45b70":"code","d4f0bbbb":"code","7b1d42c8":"code","157fa489":"code","902bc495":"code","7ef74520":"code","3814cad6":"code","90b0711f":"markdown","87bd827d":"markdown","48ccc60d":"markdown","dd23a70a":"markdown"},"source":{"fcad1f3d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2bd49cb9":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_data.head()","9081c08a":"test_data.head()","1fb6f643":"#data_about_me = [10001, 3, 'X, Mr. Y', 'male', 21, 0, 0, '301922', test_data.Fare.median(), 'BE73', test_data.Embarked.mode()[0]]\n# df_about_me = pd.DataFrame([data_about_me], columns=test_data.columns)\n# test_data = test_data.append(df_about_me, ignore_index=True)\n# test_data.tail()","77acaa7d":"train_data.isnull().sum()","c26fbcb6":"test_data.isnull().sum()","2ef7afac":"# Drop unhelpful features like \"Cabin\", etc.\nfeatures_to_drop = ['Cabin', 'Ticket', 'PassengerId']\ntrain_data = train_data.drop(features_to_drop, axis=1)\ntest_data_id = test_data.PassengerId\ntest_data = test_data.drop(features_to_drop, axis=1)\n\n# Fill gaps for missing values with a median value (for quantitative features) or a mode (for qualitative features)\nall_data = train_data.drop(['Survived'], axis=1).append(test_data)\n\ntrain_data = train_data.fillna({'Age': all_data['Age'].median(), \n                                'Embarked': all_data['Embarked'].mode()[0]})\n\ntest_data = test_data.fillna({'Age': all_data['Age'].median(),\n                             'Fare': all_data['Fare'].median()})","c62e2954":"train_data.isnull().sum()","072669ac":"test_data.isnull().sum()","d060edf0":"train_data.head()","4b6bee34":"# Extract titles from names\ndef extract_title(name):\n    title = (name.split(', ')[1]).split('. ')[0]\n    \n    if title not in ['Mr', 'Mrs', 'Miss', 'Master']:\n        return 'Rare'\n    else:\n        return title\n    \ntrain_data['Title'] = train_data.Name.apply(extract_title)\ntest_data['Title'] = test_data.Name.apply(extract_title)\ntrain_data.head()","9a612f14":"# Create 'Family_Size' feature by adding values of 'SibSp' feature to 'Parch' feature + 1\ntrain_data['Family_Size'] = train_data.SibSp + train_data.Parch + 1\ntest_data['Family_Size'] = test_data.SibSp + test_data.Parch + 1\ntrain_data.head()","e2a61d16":"# Create 'Is_Alone' feature (1 if 'Family_Size' == 1, 0 otherwise)\ntrain_data['Is_Alone'] = (train_data.Family_Size == 1).astype(int)\ntest_data['Is_Alone'] = (test_data.Family_Size == 1).astype(int)\ntrain_data.head()","3ee7ce66":"# # Add 'Name_Length' feature\n# train_data['Name_Length'] = train_data.Name.apply(len)\n# test_data['Name_Length'] = test_data.Name.apply(len)\n# train_data.head()","d3a9c464":"# # Check if there is a correlation between name length and survival rate\n# import matplotlib.pyplot as plt\n\n# plt.subplot()\n# plt.hist([train_data[train_data['Survived'] == 1]['Name_Length'], \n#          train_data[train_data['Survived'] == 0]['Name_Length']], \n#          stacked=True, color = ['g','r'], label = ['Survived','Dead'], alpha=0.5)\n# plt.title('Name Length Histogram by Survival')\n# plt.xlabel('Name Length (chars)')\n# plt.ylabel('# of Passengers')\n# plt.legend()","296948ef":"# train_data.corr()","a25b1ba5":"# Drop features that we don't need anymore\nfeatures_to_drop = ['Name', 'SibSp']\ntrain_data.drop(features_to_drop, axis=1, inplace=True, errors='ignore')\ntest_data.drop(features_to_drop, axis=1, inplace=True, errors='ignore')\ntrain_data.head()","dee49897":"# Encode 'Sex' feature (1 - male, 0 - female)\nencode_sex = lambda sex: 1 if sex == 'male' else 0\ntrain_data['Sex_Code'] = train_data['Sex'].apply(encode_sex)\ntest_data['Sex_Code'] = test_data['Sex'].apply(encode_sex)\ntrain_data.head()","0d106444":"# Divide age to 5 groups (use pd.cut for equal-spaced intervals)\ntrain_data['Age_Code'] = pd.cut(train_data.Age, 5, retbins=True, labels=False)[0]\ntest_data['Age_Code'] = pd.cut(test_data.Age, 5, retbins=True, labels=False)[0]\ntrain_data.head()","fb3f2728":"# Divide 'Fare' feature to 5 groups (use pd.qcut for equal-filled bins)\ntrain_data['Fare_Code'] = pd.qcut(train_data.Fare, 5, retbins=True, labels=False)[0]\ntest_data['Fare_Code'] = pd.qcut(test_data.Fare, 5, retbins=True, labels=False)[0]\ntrain_data.head()","fbbe84cc":"# Alternative way to encode 'Embarked' and 'Title' features (dummy encoding)\ntrain_data = pd.get_dummies(train_data.drop(['Sex'], axis=1, errors='ignore'))\ntest_data = pd.get_dummies(test_data.drop(['Sex'], axis=1, errors='ignore'))\ntrain_data.head()","5e40b255":"train_data.shape, test_data.shape","b4255bbe":"test_data.head()","9ad8eb2a":"# # Encode 'Embarked' feature\n# from sklearn.preprocessing import LabelEncoder\n\n# train_data\n# train_data['Embarked_Code'] = LabelEncoder().fit_transform(train_data.Embarked)\n# test_data['Embarked_Code'] = LabelEncoder().fit_transform(test_data.Embarked)\n\n# train_data.head()","3390c7c5":"# # Encode 'Title' feature\n# train_data['Title_Code'] = LabelEncoder().fit_transform(train_data.Title)\n# test_data['Title_Code'] = LabelEncoder().fit_transform(test_data.Title)\n# train_data.head()","72767489":"# Drop features that we don't need anymore\nfeatures_to_drop = ['Sex', 'Age', 'Fare', 'Embarked', 'Title']\ntrain_data.drop(features_to_drop, axis=1, inplace=True, errors='ignore')\ntest_data.drop(features_to_drop, axis=1, inplace=True, errors='ignore')\ntrain_data.head()\n\n# features_to_drop = ['Sex', 'Embarked', 'Title']\n# train_data.drop(features_to_drop, axis=1, inplace=True, errors='ignore')\n","03095454":"# Training a decision tree classifier (DTC)from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\nX = train_data.drop(['Survived'], axis=1)\ny = train_data.Survived\n\nclf = DecisionTreeClassifier()\nparam_grid = {'max_depth': range(3, 8, 1),\n             'min_samples_split': range(2, 5, 1),\n             'min_samples_leaf': range(3, 8, 1),\n             'criterion': ['entropy']}\n\nsearch = GridSearchCV(clf, param_grid, verbose=True, n_jobs=-1)\n_ = search.fit(X, y)\n\nprint('DTC mean CV score: %0.4f' % cross_val_score(search.best_estimator_, X, y).mean())\n\n# predictions = model.predict(X_test)","fc3e9122":"search.best_params_","c7cac069":"# # Training a random forest\n# clf = RandomForestClassifier()\n\n# y = train_data.Survived\n\n# param_grid = {'n_estimators': range(50, 101, 50),\n#               'max_depth': range(3, 8, 2), \n#               'criterion': ['entropy'], \n#               'min_samples_leaf': range(3, 16, 4), \n#               'min_samples_split': range(5, 26, 4)\n#              }\n\n# search = GridSearchCV(clf, param_grid, verbose=True, n_jobs=-1)\n# _ = search.fit(X, y)\n\n# print('RFC mean CV score: %0.4f' % cross_val_score(search.best_estimator_, X, y).mean())","5ac45b70":"search.best_params_","d4f0bbbb":"# Training an SVC\nfrom sklearn.svm import SVC\nsvc = SVC()\n\ny = train_data.Survived\n\nparam_grid = {'C': np.arange(0.1, 10.1, 0.05),\n             #'kernel': ['linear', 'poly', 'rbf', 'sigmoid']#, 'precomputed']\n             }\n\nsearch = GridSearchCV(svc, param_grid, verbose=True, n_jobs=-1)\n_ = search.fit(X, y)\n\nprint('SVC mean CV score: %0.4f' % cross_val_score(search.best_estimator_, X, y).mean())","7b1d42c8":"search.best_params_","157fa489":"test_data.tail()","902bc495":"# Get predictions from the best model (SVM)\nclf = search.best_estimator_\nX_test = test_data\npredictions = clf.predict(X_test)","7ef74520":"output = pd.DataFrame({'PassengerId': test_data_id,\n              'Survived': predictions\n             })\n\n\noutput.to_csv('my_submission5.csv', index=False)\nprint('Done')","3814cad6":"output.tail()","90b0711f":"### Feature engineering","87bd827d":"### Training machine learning models","48ccc60d":"### Feature cleaning","dd23a70a":"### Encoding nominal (categorical) features"}}