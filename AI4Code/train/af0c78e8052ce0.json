{"cell_type":{"084332a5":"code","024243ce":"code","087b1e0d":"code","9ac11888":"code","6bec55b3":"code","c00f6b96":"code","94e69232":"code","f9378501":"code","4ee32de3":"code","860add02":"code","c43ece37":"code","a1222b1e":"code","1b48a565":"code","1b5e15ec":"code","84c9de73":"code","b1144141":"code","94c0d728":"code","b19b7ed2":"code","3730c267":"code","af08822c":"code","150723cf":"code","1e98482e":"code","e49922a8":"code","ed2173e6":"code","222a343f":"code","aa3578c6":"code","8ce0530c":"code","ec39af27":"markdown","4c0c8a38":"markdown","83304994":"markdown","597027b2":"markdown","42708bf5":"markdown","e8f381f2":"markdown","27590603":"markdown","46f6d461":"markdown","53bed012":"markdown","77366585":"markdown","b7c93d10":"markdown","dac7e34c":"markdown","2b74009b":"markdown","58dc81d2":"markdown","6e2b09c5":"markdown","b32ac5e4":"markdown","ef232dcc":"markdown"},"source":{"084332a5":"!pip install wordninja","024243ce":"#from sklearn.naive_bayes import MultinomialNB\nimport pandas as pd\nimport numpy as np\nimport regex as re\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nimport wordninja\nfrom tqdm.notebook import tqdm\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n%matplotlib inline","087b1e0d":"class MultinomialNB:\n    def __init__(self, alpha=1.0):\n        \"\"\"\n        alpha: float\n          Indice to avoid zero expressions (Laplace smoothing).\n        \"\"\"\n        self.alpha = alpha\n\n    def __calculate_probability_class(self, y):\n        \"\"\"\n        y: numpy.ndarray\n          Labels of training data.\n\n        Return: numpy.ndarray \n          Probability vector of all labels.\n        \"\"\"\n        self.name_class, num_class = np.unique(y, return_counts=True)\n        print(\"Start Calculate probability class\")\n        probability = num_class \/ len(y)\n        return np.array(probability)\n\n    def __calculate_probability_vocabulary(self, X, y):\n        \"\"\"\"\n        X: scipy.sparse.csr.csr_matrix or numpy.ndarray\n          Training data.\n        y: numpy.ndarray\n          Labels of training data.\n\n        Return: numpy.ndarray\n          Probability matrix of all vocabulary.\n        \"\"\"\n        classes = []\n        print(\"Create vector labels\")\n        for i in tqdm(self.name_class):\n            classes.append(np.where(y == i, 1, 0))\n\n        classes = np.array(classes)\n        matrix_sum_vocabulary = classes @ X\n        return np.array([np.log(x + self.alpha) - np.log(np.sum(x) + self.alpha*np.sum(np.ceil(x))) for x in tqdm(matrix_sum_vocabulary)])\n\n    def fit(self, X, y):\n        \"\"\"\"\n        X: scipy.sparse.csr.csr_matrix or numpy.ndarray\n          Training data.\n        y: numpy.ndarray\n          Labels of training data.\n        \"\"\"\n        y = np.array(y)\n        probability_class = self.__calculate_probability_class(y)\n        matrix_pv = self.__calculate_probability_vocabulary(X, y).T\n        self.matrix_probability_vocabulary = (probability_class + matrix_pv).T\n\n    def predict(self, X):\n        \"\"\"\"\n        X: scipy.sparse.csr.csr_matrix or numpy.ndarray\n          Test data.\n\n        Return: numpy.ndarray\n          Label corresponding to the test data\n        \"\"\"\n        temp = X @ self.matrix_probability_vocabulary.T\n        return np.array([self.name_class[list(x).index(max(x))] for x in tqdm(temp)])","9ac11888":"clf = MultinomialNB(alpha=0.001)\nX_ = [[1, 0.2, 0.3, 0.1], [1, 0.4, 0.5, 0.9], [1, 0.4, 1, 0.2], [1, 0.7, 0.5, 0.8],[1, 0, 0.3, 0.6], [1, 0.7, 0.42, 0]]\ny_ = [\"a\", \"b\", \"a\", \"d\", \"b\", \"d\"]\nX_ = np.array(X_)\ny_ = np.array(y_)\nprint(\"X shape: \", X_.shape)\nprint(\"y shape: \", y_.shape)\nclf.fit(X_, y_)\ny_pred = clf.predict(X_)\nprint(y_pred)","6bec55b3":"print(classification_report(y_,y_pred))","c00f6b96":"stop_word = [\"http\", \"www\", \"com\", \"org\", \"net\", \".ca\", \"html\", \"htm\"]","94e69232":"def remove_stop_word(data, labels):\n    \"\"\"\n    data: Dataframe, numpy.ndarray, list\n      Data to preprocessing\n    labels: Dataframe, numpy.ndarray, list\n      Labels corresponding to data\n\n    Return: numpy.ndarray, numpy.ndarray\n      Data after preprocessing \n    \"\"\"\n    data = list(data)\n    labels = list(labels)\n    for i in tqdm(range(len(data))):\n        try:\n          data[i] = re.sub(\"[^a-zA-Z0-9\\n]\", \" \", str(data[i]))\n          for j in stop_word:\n              data[i] = data[i].replace(j, \" \")\n          data[i] = ' '.join(wordninja.split(data[i]))\n          data[i] = ' '.join(data[i].split())\n          if len(data[i])<1:\n            del data[i]\n            del labels[i]\n        except:\n          break\n    return np.array(data), np.array(labels)","f9378501":"data = pd.read_csv(\"..\/input\/url-classification-dataset-dmoz\/URL Classification.csv\", names=[\"index\", \"url\", \"labels\"])","4ee32de3":"data.labels.value_counts().plot(figsize=(12,5),kind='bar',color='blue');\nplt.xlabel('Category')\nplt.ylabel('Total Number Of Individual Category')","860add02":"data, labels = remove_stop_word(data[\"url\"], data[\"labels\"])","c43ece37":"x_train, x_test, y_train, y_test = train_test_split(data, labels, random_state=42, test_size=0.2)","a1222b1e":"x_test","1b48a565":"del data\ndel labels","1b5e15ec":"model = Pipeline([\n                  ('count_vector', CountVectorizer(ngram_range=(1,2))), \n                  ('tfidf', TfidfTransformer(use_idf=True,norm='l2')),\n                ])","84c9de73":"model.fit(x_train, y_train)","b1144141":"data_train = model.transform(x_train)\ndata_train.shape","94c0d728":"clf = MultinomialNB(alpha = 0.01)\nclf.fit(data_train,y_train)","b19b7ed2":"y_train_pred = clf.predict(data_train)","3730c267":"print(classification_report(y_train,y_train_pred))","af08822c":"data_test = model.transform(x_test)","150723cf":"y_test_pred = clf.predict(data_test)","1e98482e":"print(classification_report(y_test,y_test_pred))","e49922a8":"def visualize_data_labels(y, y_pred, name_y):\n  data_and_count_y_train = np.unique(y, return_counts=True)\n  data_and_count_y_train_pred = np.unique(y_pred, return_counts=True)\n\n  labels = tuple(data_and_count_y_train[0])\n\n  x = np.arange(len(labels))  # the label locations\n  width = 0.35  # the width of the bars\n\n  fig = plt.figure(figsize = (10,5))\n  ax = fig.add_axes([0,0,1,1])\n\n  rects1 = ax.bar(x - width\/2, tuple(data_and_count_y_train[1]), width, label=name_y)\n  rects2 = ax.bar(x + width\/2, tuple(data_and_count_y_train_pred[1]), width, label='y_pred')\n  # Add some text for labels, title and custom x-axis tick labels, etc.\n  ax.set_ylabel('Numbers labels')\n  ax.set_title('Compare numbers of labels')\n  ax.set_xticks(x)\n  ax.set_xticklabels(labels)\n  ax.legend()\n\n  fig.tight_layout()\n\n  plt.show()","ed2173e6":"visualize_data_labels(y_train,y_train_pred, \"y_train\")","222a343f":"visualize_data_labels(y_train,y_train_pred, \"y_test\")","aa3578c6":"array = confusion_matrix(y_train, y_train_pred)\ncm=np.array(array)\ncm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\ndf_cm = pd.DataFrame(cm)\nplt.figure(figsize = (20,15))\nsns.heatmap(df_cm, annot=True,cmap='Greens')","8ce0530c":"array = confusion_matrix(y_test, y_test_pred)\ncm=np.array(array)\ncm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\ndf_cm = pd.DataFrame(cm)\nplt.figure(figsize = (20,15))\nsns.heatmap(df_cm, annot=True,cmap='Greens')","ec39af27":"### Split to train and test with ratio is 80:20 and shuffle","4c0c8a38":"# Conclusion","83304994":"# Import library","597027b2":"# Predict on training data","42708bf5":"# Training model","e8f381f2":"# Bag of word and re-weight with TF-IDF","27590603":"# Define multinomial Naive Bayes","46f6d461":"---\n## Explain model with test data\n### **Input**:\n\n  >$X_{6,4} =\n  \\begin{pmatrix}\n    1 & 0.2 & 0.3 & 0.1 \\\\ \n    1 & 0.4 & 0.5 & 0.9 \\\\ \n    1 & 0.4 & 1 & 0.2 \\\\\n    1 & 0.7 & 0.5 & 0.8 \\\\\n    1 & 0 & 0.3 & 0.6 \\\\\n    1 & 0.7 & 0.42 & 0\n  \\end{pmatrix}$ ; $y_{6,1} = \\begin{pmatrix} a \\\\ b \\\\ a \\\\ d \\\\ b \\\\ d\n  \\end{pmatrix}$\n\n  >$X_{6,4}$: 6 samples and 4 vocabulary (Each column is the weight corresponding to row vocabulary).\n\n  >$y_{6,1}$: 6 labels corresponding to each samples\n---\n### **Algorithm to calculate matrix naive bayes:**\n\n1.   **Calculate a probability vector of all labels:**\n  *   **Create vector of set all labels and count labels vector corresponding.**\n      >$V_{labels} = (a, b, d)$ \\\\\n      \n      >$V_{count_.labels} = (2, 2, 2)$\n\n  *   **Calculate a probability vector of all labels:**\n      >$V_{probability_.labels} = V_{count_.labels} \/ length(y_{6,1}) = (2, 2, 2) \/ 6 = (0.33, 0.33, 0.33)$\n\n2.   **Calculate a probability matrix of all vocabulary.**\n  *   **Create matrix contain same one hot vectors of labels** \\\\\n    $MoC_{3,6}$: 3 labels and 6 samples. \\\\\n    Ex: $ MoC_{1,1:6} = (1, 0, 1, 0, 0, 0)$ because this vector instead for label `a` and we have $y_{1,6} = (a,b,a,d,b,d)$ \\\\\n  >$MoC_{3,6} = \\begin{pmatrix}\n      1 & 0 & 1 & 0 & 0 & 0 \\\\ \n      0 & 1 & 0 & 0 & 1 & 0 \\\\ \n      0 & 0 & 0 & 1 & 0 & 1\n  \\end{pmatrix}$\n\n  *  **Calculate dot between $MoC_{3,6}$ and $X_{6,4}$:**\n>$MoSumClass_{3,4} = MoC_{3,6}$ x $X_{6,4} =\n  \\begin{pmatrix}\n      1 & 0 & 1 & 0 & 0 & 0 \\\\ \n      0 & 1 & 0 & 0 & 1 & 0 \\\\ \n      0 & 0 & 0 & 1 & 0 & 1\n  \\end{pmatrix}$ x $\\begin{pmatrix}\n    1 & 0.2 & 0.3 & 0.1 \\\\ \n    1 & 0.4 & 0.5 & 0.9 \\\\ \n    1 & 0.4 & 1 & 0.2 \\\\\n    1 & 0.7 & 0.5 & 0.8 \\\\\n    1 & 0 & 0.3 & 0.6 \\\\\n    1 & 0.7 & 0.42 & 0\n  \\end{pmatrix}$ \n  >$MoSumClass_{3,4} = \\begin{pmatrix}\n      2 & 0.6 & 1.3 & 0.3 \\\\ \n      2 & 0.4 & 0.8 & 1.5\\\\ \n      2 & 1.4 & 0.92 & 0.8\n  \\end{pmatrix}$ \n\n  * **Calculate probability of each vocabulary corresponding labels.** \\\\\n  Follow Multinomial Naive Bayes add Laplace smoothing technique, matrix of probability each vocabulary (MoPV) calculate by $MoSumClass_{3,4}$:\n  >$MoPV_{3,4} =\n  \\begin{pmatrix}\n   0.47574893 & 0.14289111 & 0.30932002 & 0.07156443 \\\\\n 0.42520187 & 0.08521037 & 0.17020824 & 0.31895453 \\\\\n 0.39036286 & 0.27331252 & 0.17967226 & 0.15626219\n  \\end{pmatrix}$\n\n3. **Calculate matrix multiply between $MoPV_{4,3} (MoPV_{3,4}.T) $ and $V_{probability_.labels(3)}$** \n>$M_{bayes} = V_{probability_.labels}$ x $MoPV_{3,4} =\n  \\begin{pmatrix} \n  0.15858298 & 0.04763037 & 0.10310667 & 0.02385481 \\\\\n  0.14173396 & 0.02840346 & 0.05673608 & 0.10631818 \\\\\n  0.13012095 & 0.09110417 & 0.05989075 & 0.0520874\n  \\end{pmatrix}$\n---\n### **Algorithm to calculate vector labels after predict:**\n**Calculate matrix dot between $X_{test}$ and $M_{bayes}$ then logarit this matrix:** \n>$log(X_{n,4}$ x $M_{bayes(3,4)}.T)$ \\\\\n  Select label have max weight, its label for sample corresponding. \n","53bed012":"# Visualize data","77366585":"1. CountVectorizer(ngram_range=(1,2)): `ngram_range` is select n of gram and choose 1,2 which means that use `unigram` and `bigram`.\n2. TfidfTransformer(use_idf=True,norm='l2'): `norm` is Euclid distance and `l2` Sum of squares of vector elements is 1","b7c93d10":"# Test model\n`(X_, y_)` is my create data to test model with `alpha = 0.001`\n","dac7e34c":"# Predict on test data","2b74009b":"`macro avg`: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account. \\\\\n`weighted avg`: Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters \u2018macro\u2019 to account for label imbalance; it can result in an F-score that is not between precision and recall.","58dc81d2":"The model has processed data by CountVectorizer and TfidfTransformer. Then use define Multinomial Naive Bayes to training, predict data and have accuacy quite good with time training and predict are very fast. But have much noise data so need suitable method\n> Data | Training data | Test data\n> --- | --- | ---\n> Num of samples | 1250376 | 312594\n> Time Train | 1 (s) | \n> Time Predict | 8 (s) | 2 (s)\n> Accuracy | 91 % | 55 %","6e2b09c5":"# B\u1eaft \u0111\u1ea7u th\u1ef1c hi\u1ec7n x\u1eed l\u00fd d\u1eef li\u1ec7u ","b32ac5e4":"MultinomialNB(alpha = 0.01): `alpha` is parameters with `default = 1`, to avoid zero numerator","ef232dcc":"# Visualize data"}}