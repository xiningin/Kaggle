{"cell_type":{"098a58a2":"code","6bd357cd":"code","c5d8084d":"code","c60e4ebf":"code","c70ceb85":"code","3c218b1f":"code","f82b4ac2":"code","d0c95155":"code","b55c35df":"code","12ac7929":"code","0cbc8d50":"code","74c8734a":"code","d6c47112":"code","024abbc6":"code","6e2a9d68":"code","fbe39f7d":"code","68fb33b3":"code","5c770b84":"code","27277654":"code","0ff9b63c":"code","7635e8b3":"code","4209f423":"code","50948e1d":"code","fb6f4570":"code","b63ad5be":"code","efa7a576":"code","ed0d00e3":"code","e8448f73":"code","6885e96a":"code","75cf1ece":"code","e8f9df9b":"code","87e88356":"code","6f75c00b":"code","9c8599f0":"code","41faf3b3":"code","05f30a4b":"code","569d9604":"code","906b0d4c":"markdown","566193c8":"markdown","cb3c7827":"markdown","fac63a9c":"markdown","40dfd06f":"markdown","2cb83304":"markdown"},"source":{"098a58a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6bd357cd":"from __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\n","c5d8084d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport os\nimport itertools\nimport time\nimport copy\n\nseed = 123\nnp.random.seed(seed)\n\n# Defining the show plot method\ndef show_plots(history, plot_title=None, fig_size=None):\n    \"\"\" Useful function to view plot of loss values & accuracies across the various epochs\n        Works with the history object returned by the train_model(...) call \"\"\"\n    \n    assert type(history) is dict\n    # NOTE: the history object should always have loss & acc (for training data), but MAY have\n    # val_loss & val_acc for validation data\n    loss_vals = history['loss']\n    val_loss_vals = history['val_loss'] if 'val_loss' in history.keys() else None\n    epochs = range(1, len(history['loss']) + 1)\n\n    f, ax = plt.subplots(nrows=1, ncols=2, figsize=((16, 4) if fig_size is None else fig_size))\n\n    # plot losses on ax[0]\n    ax[0].plot(epochs, loss_vals, color='navy', marker='o', linestyle=' ', label='Training Loss')\n    if val_loss_vals is not None:\n        ax[0].plot(epochs, val_loss_vals, color='firebrick', marker='*', label='Validation Loss')\n        ax[0].set_title('Training & Validation Loss')\n        ax[0].legend(loc='best')\n    else:\n        ax[0].set_title('Training Loss')\n\n    ax[0].set_xlabel('Epochs')\n    ax[0].set_ylabel('Loss')\n    ax[0].grid(True)\n\n    # plot accuracies\n    acc_vals = history['acc']  if 'acc' in history.keys() else None\n    val_acc_vals = history['val_acc'] if 'val_acc' in history.keys() else None\n    if acc_vals is not None:\n        ax[1].plot(epochs, acc_vals, color='navy', marker='o', ls=' ', label='Training Accuracy')\n    if val_acc_vals is not None:\n        ax[1].plot(epochs, val_acc_vals, color='firebrick', marker='*', label='Validation Accuracy')\n        ax[1].set_title('Training & Validation Accuracy')\n        ax[1].legend(loc='best')\n    else:\n        ax[1].set_title('Training Accuracy')\n\n    ax[1].set_xlabel('Epochs')\n    ax[1].set_ylabel('Accuracy')\n    ax[1].grid(True)\n\n    if plot_title is not None:\n        plt.suptitle(plot_title)\n\n    plt.show()\n    plt.savefig(\"metrics.png\")\n    plt.close()\n\n    # delete locals from heap before exiting (to save some memory!)\n    del loss_vals, epochs\n    if val_loss_vals is not None:\n        del val_loss_vals\n","c60e4ebf":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn \nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torchvision import datasets, transforms, models\nimport torchvision.models as models\nimport torchvision.transforms as T\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\n%matplotlib inline\nimport time","c70ceb85":"import os\nprint(os.listdir(\"..\/input\"))","3c218b1f":"# Create the path dir\nimport shutil\npath = '\/dataset'\nshutil.os.mkdir(path)\nprint('Current path:',path)","f82b4ac2":"import shutil\npath = '\/dataset\/train'\nshutil.os.mkdir(path)\nprint('Current path:',path)","d0c95155":"path = '\/dataset\/train\/'\ndata_dir = '..\/input\/v2-plant-seedlings-dataset\/'\ndef copytree(src, dst, symlinks=False, ignore=None):\n    for item in os.listdir(src):\n        s = os.path.join(src, item)\n        d = os.path.join(dst, item)\n        if os.path.isdir(s):\n            shutil.copytree(s, d, symlinks, ignore)\n        else:\n            shutil.copy2(s, d)\n\ncopytree(data_dir, path)\nprint ('Data dir:',data_dir)\nprint ('Path:',path)\n","b55c35df":"print(os.listdir(path))","12ac7929":"# Remove the nonsegmentedv2 class\nshutil.rmtree('\/dataset\/train\/nonsegmentedv2')\nprint(os.listdir(path))\n","0cbc8d50":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch import optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import train_test_split\n","74c8734a":"# Top level data directory. Here we assume the format of the directory conforms\n#   to the ImageFolder structure\ndata_dir = '\/dataset\/train\/'\n \n# Models to choose from [inception, mobilenet, resnet], but implemeting resnet\nmodel_name = \"resnet\"\n\n# Number of classes in the dataset\nnum_classes = 12\n\n# Batch size for training\nbatch_size = 64\n\n# Number of epochs to train for\nnum_epochs = 15\n\n# Flag for feature extracting. When False, we finetune the whole model,\n#   when True we only update the reshaped layer params\nfeature_extract = True\n\nprint('Dataset directory:', data_dir)\nprint('Current Model:',model_name)","d6c47112":"#Prepare transforms and dataset\n\nif model_name == 'inception':\n    #inception requires 299 cropped size\n    batchsize = 64\n    data_transforms = transforms.Compose([transforms.RandomRotation(30),\n                                           transforms.RandomResizedCrop(299),\n                                           transforms.RandomHorizontalFlip(),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize([0.485, 0.456, 0.406],\n                                                                [0.229, 0.224, 0.225])])\nelse:\n    batchsize = 64\n    data_transforms = transforms.Compose([transforms.RandomRotation(30),\n                                           transforms.RandomResizedCrop(224),\n                                           transforms.RandomHorizontalFlip(),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize([0.485, 0.456, 0.406],\n                                                                [0.229, 0.224, 0.225])])    \n\ndataset = datasets.ImageFolder(data_dir, transform=data_transforms)\nprint('Dataset Details: \\n', dataset)","024abbc6":"print('Size of dataset is:',len(dataset))\n","6e2a9d68":"#Split dataset into training and validation sets\ntrain_set, val_set = torch.utils.data.random_split(dataset, [4539, 1000])\n\n# Load the dataset using DataLoader\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(val_set, batch_size, shuffle=True)\ndataloaders_dict = {'train':train_loader, 'val':valid_loader}","fbe39f7d":"print('Training set size is:',len(train_set))\nprint('Validation set size is:',len(val_set))","68fb33b3":"def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","5c770b84":"\ndef initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n    # Initialize these variables which will be set in this if statement. Each of these\n    #   variables is model specific.\n    model_ft = None\n    input_size = 0\n\n    if model_name == \"resnet\":\n        \"\"\" Resnet18\n        \"\"\"\n        model_ft = models.resnet18(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n        \n    elif model_name == \"mobilenet\":\n        \"\"\" mobilenet_v2\n        \"\"\"\n        model_ft = models.mobilenet_v2(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[1].in_features\n        model_ft.classifier[1] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n    \n    elif model_name == \"inception\":\n        \"\"\" Inception v3\n        Be careful, expects (299,299) sized images and has auxiliary output\n        \"\"\"\n        model_ft = models.inception_v3(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        # Handle the auxilary net\n        num_ftrs = model_ft.AuxLogits.fc.in_features\n        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n        # Handle the primary net\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n        input_size = 299\n    else:\n        print(\"Invalid model name, exiting...\")\n        exit()\n\n    return model_ft, input_size","27277654":"def get_default_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n\ndef to_device(data, device):\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n    \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\" \n        for b in self.dl:\n            yield to_device(b, self.device)\n            \n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","0ff9b63c":"# check if CUDA is available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint('Working device:',device)","7635e8b3":"# Initialize the model for this run\nmodel_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n\n# Print the model we just instantiated\nprint(model_ft)","4209f423":"#Send the model to GPU\nmodel_ft = model_ft.to(device)\n\n# Gather the parameters to be optimized\/updated in this run. If we are\n#  finetuning we will be updating all parameters. However, if we are\n#  doing feature extract method, we will only update the parameters\n#  that we have just initialized, i.e. the parameters with requires_grad\n#  is True.\nparams_to_update = model_ft.parameters()\nprint(\"Params to learn:\")\nif feature_extract:\n    params_to_update = []\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            print(\"\\t\",name)\nelse:\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            print(\"\\t\",name)\n            \n# Observe that all parameters are being optimized\n#optimizer_ft = optim.Adam(params_to_update, lr=0.001)\noptimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)","50948e1d":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']","fb6f4570":"def train_model(model, dataloaders, criterion, max_lr, num_epochs=15,\n                weight_decay=0,opt_func=torch.optim.SGD, grad_clip=None, is_inception=False):\n    since = time.time()\n\n    val_acc_history = []\n    val_loss_history = []\n    \n    train_acc_history = []\n    train_loss_history = []\n    lrs = []\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.CyclicLR(optimizer, 1e-3, max_lr,\n                                                step_size_up=len(dataloaders['train']), cycle_momentum=False)\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:                \n                inputs = inputs.to(device)\n                labels = labels.to(device)                 \n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    # Get model outputs and calculate loss\n                    # Special case for inception because in training it has an auxiliary output. In train\n                    # mode we calculate the loss by summing the final output and the auxiliary output\n                    # but in testing we only consider the final output.\n                    if is_inception and phase == 'train':\n                        # From https:\/\/discuss.pytorch.org\/t\/how-to-optimize-inception-model-with-auxiliary-classifiers\/7958\n                        outputs, aux_outputs = model(inputs)\n                        loss1 = criterion(outputs, labels)\n                        loss2 = criterion(aux_outputs, labels)\n                        loss = loss1 + 0.4*loss2\n                    else:\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)                    \n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        # Record & update learning rate\n                        lrs.append(get_lr(optimizer))\n                        sched.step()\n                                        \n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss \/ len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() \/ len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n                val_loss_history.append(epoch_loss)\n            if phase == 'train':\n                train_acc_history.append(epoch_acc)\n                train_loss_history.append(epoch_loss)\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n    history = {'loss':train_loss_history, 'acc':train_acc_history, 'val_loss':val_loss_history, 'val_acc':val_acc_history, 'lr':lrs}\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model,history","b63ad5be":"epochs = 15\nmax_lr = 0.01\ngrad_clip = None  #0.1\nweight_decay = 0  #1e-4\nopt_func = torch.optim.Adam\ncriterion = F.cross_entropy\n# Train and evaluate\nmodel, history = train_model(model_ft, dataloaders_dict, criterion, max_lr, \n                             epochs,weight_decay, opt_func, grad_clip=None, is_inception=(model_name==\"inception\"))\n","efa7a576":"#getting all the predictions on the valid set\ndef eval_predict(model,dataloaders_dict, phase):\n    model.eval()\n    all_predict = torch.tensor([], device=device)\n    all_labels = torch.tensor([], device=device)\n    with torch.no_grad():\n        for inputs, labels in dataloaders_dict[phase]:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_predict = torch.cat((all_predict, preds), 0)\n            all_labels = torch.cat((all_labels, labels), 0)\n    return all_predict, all_labels\n\nphase = 'val'\ny_pred,y_true = eval_predict(model, dataloaders_dict, phase)\n\ny_pred = y_pred.cpu().numpy()\ny_true = y_true.cpu().numpy()\n\n#print('Y pred:', y_pred)\n#print('Y true:', y_true)","ed0d00e3":"from sklearn.metrics import confusion_matrix\ncnf_matrix = confusion_matrix(y_true, y_pred)\nwith open('confusion_matrix.npy', 'wb') as f:\n    np.save(f, cnf_matrix)\n\n#with open('test.npy', 'rb') as f:\n#    a = np.load(f)\n    \ndf_cm = pd.DataFrame(cnf_matrix, index = [\"black_grass\", \"charlock\",\"cleavers\", \"chickweed\",\"wheat\",\"fat_hen\",\"silky_bent\",\"maize\",\"mayweed\",\"sherperds_purse\",\"cranesbill\",\"sugarbeet\"],\n                  columns = [\"black_grass\", \"charlock\",\"cleavers\", \"chickweed\",\"wheat\",\"fat_hen\",\"silky_bent\",\"maize\",\"mayweed\",\"sherperds_purse\",\"cranesbill\",\"sugarbeet\"])\nplt.figure(figsize = (10,7))\nplot = sns.heatmap(df_cm, annot=True)\nfig = plot.get_figure()\nfig.savefig(\"confusionmatrix.png\")\n","e8448f73":"from sklearn.metrics import classification_report\ndef CLASS_Report(y_test, y_pred, labels):\n    class_report = classification_report(y_test, y_pred, target_names=labels)\n    return class_report\n\ndef extractClass(testXpred, classes):\n    Test = [] \n    for i in range(len(testXCount)):      \n        instance = testXCount[i]\n        label = testXpred[i]\n        instance.append(label)\n        Test.append(instance)\n    clusters = separate_by_class_pop_class(Test)   \n    testXpred  = []\n    for x in testXCount:\n        for clss in classes:\n            if clss in clusters:\n              if x in clusters[clss]:\n                  kmClusters = clusters[clss]                  \n        maxima = []\n        maximumLabelDict = {}\n        for value in kmClusters:\n            label = np.argmax(value)\n            maximum = value[label]\n            maxima.append(maximum)\n            maximumLabelDict[maximum] = label\n        Predictlabel = maximumLabelDict[max(maxima)]\n        testXpred.append(Predictlabel)\n    return testXpred","6885e96a":"labels = [\"black_grass\", \"charlock\",\"cleavers\", \"chickweed\",\"wheat\",\"fat_hen\",\"silky_bent\",\"maize\",\"mayweed\",\"sherperds_purse\",\"cranesbill\",\"sugarbeet\"]\nclass_report = CLASS_Report(y_true, y_pred, labels)\nprint(class_report)","75cf1ece":"#Plot loss and accuracies\nshow_plots(history, plot_title=None, fig_size=None)","e8f9df9b":"savemodel = '{}_model{}.pt'.format(model_name, num_epochs)\ntorch.save(model.state_dict(), savemodel)\nprint('Current resnet model result:',savemodel)","87e88356":"# Saving the result pf the confusion matrix\nfrom IPython.display import FileLink\nFileLink(r'confusionmatrix.png')","6f75c00b":"def plot_lrs(history):\n    plt.plot(history[\"lr\"])\n    plt.xlabel('Batch Number')\n    plt.ylabel('learning rate')\n    plt.title('Learning rate vs Batches')","9c8599f0":"plot_lrs(history)","41faf3b3":"FileLink(r'resnet_model15.pt')","05f30a4b":"FileLink(r'metrics.png')","569d9604":"FileLink(r'confusion_matrix.npy')","906b0d4c":"**Set GPU**","566193c8":"**Training**","cb3c7827":"**Train\/Test Split Dataset**","fac63a9c":"**Download Pretrained Model**","40dfd06f":"**Initialize Model**","2cb83304":"**Define Feature Extraction**"}}