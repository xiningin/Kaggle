{"cell_type":{"2b2e93d7":"code","5252ce98":"code","171e7660":"code","d1f8e3fd":"code","453f1c72":"code","9334ec87":"code","8fa27695":"code","add8e7c6":"code","376abeb0":"code","a3023f00":"code","b6d33d9f":"code","9863cda1":"code","9684254d":"code","7e536079":"code","1cf00739":"code","ac837e0c":"code","7654484c":"code","86a57627":"code","10a7f41b":"code","7d6dae8d":"code","375ab58b":"code","279e583c":"code","619fb15d":"code","d4c3b4b0":"code","5a690ad3":"code","46fc31f5":"code","dae22672":"code","11a8888f":"code","4f20c509":"code","f14d59c6":"code","34ffe4e6":"code","d3bd8734":"code","5ce82a23":"code","1313574a":"code","15ea4265":"code","613d1e51":"markdown","b8ed5db4":"markdown","5a5c341d":"markdown","7282f2d8":"markdown","a5362ef0":"markdown","b13b95e5":"markdown","d6774b31":"markdown","52c21a12":"markdown","b07b2638":"markdown","ae5275a7":"markdown","361c341e":"markdown","fd3460b2":"markdown"},"source":{"2b2e93d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as py\nimport seaborn as sns\nfrom sklearn.svm import SVR\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.preprocessing import normalize\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.linear_model import LinearRegression,SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5252ce98":"train=pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')","171e7660":"mo1=pd.concat([train,test],sort=False)\nmo1.drop('SalePrice',axis=1,inplace=True)","d1f8e3fd":"len(train.columns)","453f1c72":"py.figure(figsize=(25,5))\nsns.heatmap(train.isnull())","9334ec87":"mo1=pd.get_dummies(mo1,columns=['MSZoning','Street','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive','SaleType','SaleCondition'],drop_first=True)","8fa27695":"train1=mo1.iloc[:1460,:]\ntest1=mo1.iloc[1460:,:]","add8e7c6":"train1.drop(columns=['PoolQC','Alley','Fence','MiscFeature'],axis=1,inplace=True)\ntest1.drop(columns=['PoolQC','Alley','Fence','MiscFeature'],axis=1,inplace=True)","376abeb0":"sim=SimpleImputer(strategy='mean',missing_values=np.nan)\n","a3023f00":"X1=sim.fit_transform(train1.values)\nX2=sim.fit_transform(test1.values)","b6d33d9f":"X=pd.DataFrame(X1,columns=train1.columns)\nX_t=pd.DataFrame(X2,columns=test1.columns)","9863cda1":"X.shape","9684254d":"py.figure(figsize=(25,5))\nsns.heatmap(X.isnull())","7e536079":"X=pd.concat([X,train['SalePrice']],axis=1)\np1=X.corr(method='pearson')['SalePrice'].sort_values(ascending=False)","1cf00739":"p1=p1[p1<0.3]","ac837e0c":"p1","7654484c":"X.drop(columns=list(p1.index.values),axis=1,inplace=True)\nX_t.drop(columns=list(p1.index.values),axis=1,inplace=True)","86a57627":"X2=normalize(X)\nX=pd.DataFrame(X2,columns=X.columns)\nXt=normalize(X_t)\nX_t=pd.DataFrame(Xt,columns=X_t.columns)","10a7f41b":"x_train,x_val,y_train,y_val=train_test_split(X.drop('SalePrice',axis=1),X['SalePrice'],test_size=0.3)","7d6dae8d":"lr=LinearRegression()","375ab58b":"lr.fit(x_train,y_train)","279e583c":"y_pred=lr.predict(x_train)\ntp=np.sqrt(np.sum((np.log(y_pred+1)-np.log(y_train+1))**2)\/y_train.shape[0])\nprint('log RMSE for training set is {}'.format((tp)))\ny_pred=lr.predict(x_val)\ntp=np.sqrt(np.sum((np.log(y_pred+1)-np.log(y_val+1))**2)\/y_val.shape[0])\nprint('log RMSE for validation set is {}'.format(tp))","619fb15d":"rf=RandomForestRegressor()","d4c3b4b0":"rf.fit(x_train,y_train)","5a690ad3":"y_pred=rf.predict(x_train)\ntp=np.sqrt(np.sum((np.log(y_pred+1)-np.log(y_train+1))**2)\/y_train.shape[0])\nprint('log RMSE for training set is {}'.format((tp)))\ny_pred=rf.predict(x_val)\ntp=np.sqrt(np.sum((np.log(y_pred+1)-np.log(y_val+1))**2)\/y_val.shape[0])\nprint('log RMSE for validation set is {}'.format(tp))","46fc31f5":"sv=SVR()","dae22672":"sv.fit(x_train,y_train)","11a8888f":"y_pred=sv.predict(x_train)\ntp=np.sqrt(np.sum((np.log(y_pred+1)-np.log(y_train+1))**2)\/y_train.shape[0])\nprint('log RMSE for training set is {}'.format((tp)))\ny_pred=sv.predict(x_val)\ntp=np.sqrt(np.sum((np.log(y_pred+1)-np.log(y_val+1))**2)\/y_val.shape[0])\nprint('log RMSE for validation set is {}'.format(tp))","4f20c509":"ny=Nystroem()","f14d59c6":"x_train1=ny.fit_transform(x_train)\nx_val1=ny.fit_transform(x_val)","34ffe4e6":"sg=SGDRegressor()","d3bd8734":"sg.fit(x_train1,y_train)","5ce82a23":"y_pred=sg.predict(x_train1)\ntp=np.sqrt(np.sum((np.log(y_pred+1)-np.log(y_train+1))**2)\/y_train.shape[0])\nprint('log RMSE for training set is {}'.format((tp)))\ny_pred=sg.predict(x_val1)\ntp=np.sqrt(np.sum((np.log(y_pred+1)-np.log(y_val+1))**2)\/y_val.shape[0])\nprint('log RMSE for validation set is {}'.format(tp))","1313574a":"y_test=rf.predict(X_t)","15ea4265":"pd.DataFrame(y_test).to_csv('submission.csv')","613d1e51":"**We will remove PoolQC,Alley,Fence,MiscFeature.**","b8ed5db4":"<h1>**Training**<\/h1>\n","5a5c341d":"**We need to fill the remaining NaN values with their respective column's mean. Scikit learn provides SimpleImputer which is used to do the previously mentioned task.To use SimpleImputer,all the values in the dataframe must be mumerical. Hence,we one-hot encode the categorical values in our dataset using Pandas.get_dummies.  **","7282f2d8":"<h1>**Submission**<\/h1>","a5362ef0":"<h1>**Handling NaN values**<\/h1>","b13b95e5":"**Feature Selection is a very important step as we get to select the features that are highly correlated with the target variables.Rest of the features are eliminated so that our model doesn't have high variance.**","d6774b31":"**In order to account for the categorical values that are not in training but in test set ,we concat both training and test sets . **","52c21a12":"**Firstly, we try to visualize the presence of NaN values in the datasets so that we can handle them.\nI use the seaborn's heatmap method to get a colorful plot.**\n","b07b2638":"**Thus, we have handled the NaN values and the categorical features.**","ae5275a7":"**There are some columns that predominantly contain NaN values. We'll remove such columns**","361c341e":"**We will remove all the above columns as they have a very low correlation with the target variable. **","fd3460b2":"<h1>**Feature Selection**<\/h1>"}}