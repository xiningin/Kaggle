{"cell_type":{"7a4e4195":"code","81c7f280":"code","3371809f":"code","681e5279":"code","f9f23a08":"code","1627af75":"code","d146841d":"code","efb8a14e":"code","151ce03b":"code","ddb1177b":"code","4918305e":"code","7d23e62a":"code","209d83e7":"code","7808e1ad":"code","2e43ad52":"code","01e6e056":"code","87d990da":"code","f8889d62":"code","3dae4334":"code","177efb1a":"code","88ac6b93":"code","55448d85":"code","3f7d5e20":"markdown","10e28d11":"markdown","a66c95f5":"markdown","04bf1134":"markdown"},"source":{"7a4e4195":"# constants \/\/ data\nMIN_CASES_FIT = 1e-5\n\"\"\"threshold to start modeling, as confirmed cases over total population\"\"\"\nMIN_DAYS_FIT = 45\n\"\"\"total days of valid data needed for training\/modeling\"\"\"\nDAYS_TEST = 10\n\"\"\"number of recent days held out for prediction\/testing\"\"\"\nMAX_MISSING_DATA = 0.15\n\"\"\"acceptable portion of missing input data\"\"\"\nDENOM_SZ = 1000\n\"\"\"denominater size when computing rmse, proportion active cases, etc\"\"\"\n# constants \/\/ sir-poly\nSIR_POLY_BETA_A_MIN = -10\n\"\"\"lower bound on first order coefficient of sir-poly\"\"\"\n\n# constants \/\/ ML\nRANDOM_SEED = 1234\n\"\"\"fix seed for random number generator for reproducible results\"\"\"\nY_WIN_SZ = 15\n\"\"\"window size when computing smoothed contact rate\"\"\"\nMOBILITY_WIN_SZ = 21\n\"\"\"windows size for rolling mean when replacing missing values\"\"\"\nCROSS_FOLDS = 5\n\"\"\"number of folds to use during cross validation\"\"\"\nPARAMETERS_SVM = [\n    {\n        'kernel': ['rbf'],\n        'C': [0.01, 0.1, 1.0, 10.0],\n        'gamma': ['scale'],\n        'epsilon': [0.001, 0.005, 0.01, 0.1]\n    }\n]\n\"\"\"SVM parameter options to be tried during grid search\"\"\"\nPARAMETERS_RANDOMFOREST = [\n    {\n        'n_estimators': [100, 500, 1000],\n        'criterion': ['mse'],# 'mae'],\n        'min_samples_leaf': [100, 200, 500],\n        'max_features': ['auto', 'sqrt']\n    }\n]\n\"\"\"Random forest options to be tried during grid search\"\"\"\npass","81c7f280":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.integrate import odeint\nfrom scipy.optimize import curve_fit\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure, ColumnDataSource\nfrom bokeh.models import BoxAnnotation, Span, HoverTool, CrosshairTool\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\n\n# inline notebook viz\noutput_notebook()\n\n# set random seed\nnp.random.seed(RANDOM_SEED)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3371809f":"testing = pd.read_csv('..\/input\/hackathon\/task_2-owid_covid_data-21_June_2020.csv')\ntesting.head()","681e5279":"# load raw datasets\nconfirmed = pd.read_csv('\/kaggle\/input\/novel-corona-virus-2019-dataset\/time_series_covid_19_confirmed.csv')\nrecovered = pd.read_csv('\/kaggle\/input\/novel-corona-virus-2019-dataset\/time_series_covid_19_recovered.csv')\ndeaths = pd.read_csv('\/kaggle\/input\/novel-corona-virus-2019-dataset\/time_series_covid_19_deaths.csv')\ncountries = pd.read_csv('\/kaggle\/input\/countries-of-the-world\/countries of the world.csv', decimal=',')\nmobility = pd.read_csv('\/kaggle\/input\/covid19-mobility-data\/Global_Mobility_Report.csv')","f9f23a08":"# collapse counts by country\nconfirmed.drop(columns=['Province\/State', 'Lat', 'Long'], inplace=True)\nconfirmed = confirmed.groupby('Country\/Region').sum()\n\nrecovered.drop(columns=['Province\/State', 'Lat', 'Long'], inplace=True)\nrecovered = recovered.groupby('Country\/Region').sum()\n\ndeaths.drop(columns=['Province\/State', 'Lat', 'Long'], inplace=True)\ndeaths = deaths.groupby('Country\/Region').sum()","1627af75":"# flip for more traditional dataframe format\nconfirmed = confirmed.transpose()\nconfirmed.index.rename('Date', inplace=True)\nconfirmed.columns.rename('', inplace=True)\n\nrecovered = recovered.transpose()\nrecovered.index.rename('Date', inplace=True)\nrecovered.columns.rename('', inplace=True)\n\ndeaths = deaths.transpose()\ndeaths.index.rename('Date', inplace=True)\ndeaths.columns.rename('', inplace=True)","d146841d":"# normalize dates\nconfirmed.index = pd.to_datetime(confirmed.index)\nrecovered.index = pd.to_datetime(recovered.index)\ndeaths.index = pd.to_datetime(deaths.index)\n\n# reindex country data\ncountries['Country'] = countries['Country'].str.strip()\ncountries.set_index('Country', inplace=True)","efb8a14e":"# map any missing country names we can\ncountries.rename(\n    index={\n        'Antigua & Barbuda': 'Antigua and Barbuda',\n        'Bahamas, The': 'Bahamas',\n        'Bosnia & Herzegovina': 'Bosnia and Herzegovina',\n        'Cape Verde': 'Cabo Verde',\n        'Central African Rep.': 'Central African Republic',\n        'Czech Republic': 'Czechia',\n        'Taiwan': 'Taiwan*',\n        'Trinidad & Tobago': 'Trinidad and Tobago',\n        # TODO \/\/ more here...\n        'United States': 'US'\n    },\n    inplace=True\n)\n","151ce03b":"# validate matches between datasets\ncount = 0\nfor name in confirmed:\n    if name not in countries.index:\n        count += 1\n        #print('Missing country data on: {}'.format(name))\nprint('Missing country data for {} countries'.format(count))\n        \n# only want country-level data\nmobility = mobility[mobility[\"sub_region_1\"].isnull()]\nmobility.drop(columns=['country_region_code', 'sub_region_1', 'sub_region_2'], inplace=True)","ddb1177b":"# more usable names\nmobility.rename(\n    columns={\n        'retail_and_recreation_percent_change_from_baseline': 'retail',\n        'grocery_and_pharmacy_percent_change_from_baseline': 'grocery',\n        'parks_percent_change_from_baseline': 'parks',\n        'transit_stations_percent_change_from_baseline': 'transit',\n        'workplaces_percent_change_from_baseline': 'work',\n        'residential_percent_change_from_baseline': 'residential'\n    },\n    inplace=True\n)\n\n# reindex mobility data\nmobility['date'] = pd.to_datetime(mobility['date'])\nmobility.set_index(['country_region', 'date'], inplace=True)","4918305e":"# map any missing mobility country names we can\nmobility.rename(\n    index={\n        'The Bahamas': 'Bahamas',\n        \"C\u00f4te d'Ivoire\": \"Cote d'Ivoire\",\n        'South Korea': 'Korea, South',\n        'Taiwan': 'Taiwan*',\n        # TODO \/\/ more here...\n        'United States': 'US'\n    },\n    inplace=True\n)\n\ncount = 0\nmobility_country_names = set(mobility.index.get_level_values('country_region'))\nfor name in confirmed :\n    if name not in mobility_country_names:\n        count += 1\n        #print('Missing mobility data on: {}'.format(name))\nprint('Missing mobility data for {} countries'.format(count))","7d23e62a":"# reindex testing data\ntesting['date'] = pd.to_datetime(testing['date'])\ntesting.drop(columns=['total_cases', 'new_cases', 'total_deaths', 'new_deaths'], inplace=True)\ntesting.set_index(['location', 'date'], inplace=True)","209d83e7":"# map any missing mobility country names we can\ntesting.rename(\n    index={\n        'Cape Verde': 'Cabo Verde',\n        'Czech Republic': 'Czechia',\n        'South Korea': 'Korea, South',\n        'Taiwan': 'Taiwan*',\n        # TODO \/\/ more here...\n        'United States': 'US'\n    },\n    inplace=True\n)\n\ncount = 0\ntesting_country_names = set(testing.index.get_level_values('location'))\nfor name in confirmed :\n    if name not in testing_country_names:\n        count += 1\n        #print('Missing testing data on: {}'.format(name))\nprint('Missing testing data for {} countries'.format(count))","7808e1ad":"# examine data sample\nname = 'Italy'\nidx = confirmed[name] > MIN_CASES_FIT * countries.loc[name]['Population']\nsample_confirmed = confirmed[name][idx]\nsample_recovered = recovered[name][idx]\nsample_deaths = deaths[name][idx]\n\n# plot\np = figure(x_axis_type=\"datetime\", title=name, plot_height=400, plot_width=800)\np.xaxis.axis_label = 'Time'\np.yaxis.axis_label = 'Count'\np.line(sample_confirmed.index, sample_confirmed, color='blue', legend_label='confirmed')\np.line(sample_recovered.index, sample_recovered, color='green', legend_label='recovered')\np.line(sample_deaths.index, sample_deaths, color='red', legend_label='deaths')\np.legend.location = \"top_left\"\nshow(p)\n\n# country data\ncountries.loc[name]","2e43ad52":"data = {}\n\nfor name in confirmed:\n    if name not in mobility_country_names:\n        continue\n    if name not in testing_country_names:\n        continue\n    if name not in countries.index:\n        continue\n\n    # select data of interest\n    pop = countries.loc[name]['Population']\n    min_cases = MIN_CASES_FIT * pop\n    idx = confirmed[name] > min_cases\n    idx_dt = confirmed[idx].index\n    \n    # check for sufficient input data\n    idx_overlap = idx_dt.intersection(mobility.loc[name].index)\n    mob_missing_count = mobility.loc[name].loc[idx_overlap]['retail'].isnull().sum()\n    if mob_missing_count > len(idx) * MAX_MISSING_DATA:\n        #print('Insufficient mobility data {} {}'.format(name, mob_missing_count))\n        continue\n        \n    idx_overlap = idx_dt.intersection(testing.loc[name].index)\n    test_missing_count = testing.loc[name].loc[idx_overlap]['new_tests_smoothed'].isnull().sum()\n    if test_missing_count > len(idx) * MAX_MISSING_DATA:\n        #print('Insufficient testing data {} {}'.format(name, test_missing_count))\n        continue\n    \n    # compute SIR values\n    s = pop - confirmed[name][idx]  # susceptible\n    r = recovered[name][idx] + deaths[name][idx]  # recovered\n    i = confirmed[name][idx] - r  # infected\n    t = range(len(s))\n    \n    if len(s) < MIN_DAYS_FIT:\n        continue\n    \n    # format SIR data as numpy array\n    sir = np.zeros((len(s), 3))\n    sir[:, 0] = s\n    sir[:, 1] = i\n    sir[:, 2] = r\n                        \n    # store by country\n    data[name] = {\n        'sir': sir,\n        'dt': s.index\n    }\n\nprint('Prepared SIR data for {} countries'.format(len(data)))","01e6e056":"# differential equations for SIR\ndef deriv_sir(y, t, n, beta, gamma):\n    s, i, r = y\n    dsdt = -beta * s * i \/ n\n    didt = beta * s * i \/ n - gamma * i\n    drdt = gamma * i\n    return dsdt, didt, drdt\n    \n# compute SIR given candidate beta and gamma\ndef gen_compute_sir(s0, i0, r0, n):\n    def compute_sir(t, beta, gamma):\n        init = (s0, i0, r0)\n        pop = n\n        res = odeint(deriv_sir, init, t, args=(pop, beta, gamma))\n        return res.flatten()\n    return compute_sir","87d990da":"# helper function to visualize SIR\ndef plot_sir(\n    name=None, t=None, s=None, i=None, r=None,\n    t_fit=None, s_fit=None, i_fit=None, r_fit=None,\n    t_pred=None, s_pred=None, i_pred=None, r_pred=None\n):\n    p = figure(title=name, x_axis_type=\"datetime\", plot_height=400, plot_width=800)\n\n    # plot actual data\n    src = ColumnDataSource(data={\n        'dt': t,\n        's': s if s is not None else np.full(len(t), np.nan),\n        'i': i if i is not None else np.full(len(t), np.nan),\n        'r': r if r is not None else np.full(len(t), np.nan)\n    })\n    if s is not None:\n        ls = p.line('dt', 's', source=src, color='blue', alpha=0.3, legend_label='susceptible (actual)')\n        p.add_tools(HoverTool(renderers=[ls], tooltips=[('S', '@s{0,0}')], mode='vline'))\n    if i is not None:\n        li = p.line('dt', 'i', source=src, color='red', alpha=0.3, legend_label='infected (actual)')\n        p.add_tools(HoverTool(renderers=[li], tooltips=[('I', '@i{0,0}')], mode='vline'))\n    if r is not None:\n        lr = p.line('dt', 'r', source=src, color='green', alpha=0.3, legend_label='recovered (actual)')\n        p.add_tools(HoverTool(renderers=[lr], tooltips=[('R', '@r{0,0}')], mode='vline'))\n        \n    # plot fit data\n    if t_fit is not None:\n        src_fit = ColumnDataSource(data={\n            'dt': t_fit,\n            's': s_fit if s_fit is not None else np.full(len(t_fit), np.nan),\n            'i': i_fit if i_fit is not None else np.full(len(t_fit), np.nan),\n            'r': r_fit if r_fit is not None else np.full(len(t_fit), np.nan)\n        })\n        if s_fit is not None:\n            lsf = p.line('dt', 's', source=src_fit, color='blue', legend_label='susceptible (fit)')\n            p.add_tools(HoverTool(renderers=[lsf], tooltips=[('S (fit)', '@s{0,0}')], mode='vline'))\n        if i_fit is not None:\n            lif = p.line('dt', 'i', source=src_fit, color='red', legend_label='infected (fit)')\n            p.add_tools(HoverTool(renderers=[lif], tooltips=[('I (fit)', '@i{0,0}')], mode='vline'))\n        if r_fit is not None:\n            lrf = p.line('dt', 'r', source=src_fit, color='green', legend_label='recovered (fit)')\n            p.add_tools(HoverTool(renderers=[lrf], tooltips=[('R (fit)', '@r{0,0}')], mode='vline'))\n            \n    # plot prediction data\n    if t_pred is not None:\n        src_pred = ColumnDataSource(data={\n            'dt': t_pred,\n            's': s_pred if s_pred is not None else np.full(len(t_pred), np.nan),\n            'i': i_pred if i_pred is not None else np.full(len(t_pred), np.nan),\n            'r': r_pred if r_pred is not None else np.full(len(t_pred), np.nan)\n        })\n        if s_pred is not None:\n            lsp = p.line('dt', 's', source=src_pred, color='blue', line_dash=[4, 4], legend_label='susceptible (predicted)')\n            p.add_tools(HoverTool(renderers=[lsp], tooltips=[('S (pred)', '@s{0,0}')], mode='vline'))\n        if i_pred is not None:\n            lip = p.line('dt', 'i', source=src_pred, color='red', line_dash=[4, 4], legend_label='infected (predicted)')\n            p.add_tools(HoverTool(renderers=[lip], tooltips=[('I (pred)', '@i{0,0}')], mode='vline'))\n        if r_pred is not None:\n            lrp = p.line('dt', 'r', source=src_pred, color='green', line_dash=[4, 4], legend_label='recovered (predicted)')\n            p.add_tools(HoverTool(renderers=[lrp], tooltips=[('R (pred)', '@r{0,0}')], mode='vline'))\n\n    # additional annotation\n    p.add_tools(CrosshairTool(dimensions='height', line_alpha=0.3))\n    p.legend.location = \"top_left\"\n    if t_fit is not None and t_pred is not None:\n        t_cutoff = t_fit[-1] + (t_pred[0] - t_fit[-1]) \/ 2.0\n        p.add_layout(\n            Span(location=t_cutoff, dimension='height', line_color='gray', line_dash='dotted')\n        )\n    show(p)","f8889d62":"# test with dummy values\nt = range(100)\nres = odeint(deriv_sir, (9990, 10, 0), t, args=(10000, 0.4, 0.2))\n\n# plot\nplot_sir(name='Dumb Blonde Bokeh SIR', t=t, s=res[:,0], i=res[:,1], r=res[:,2])","3dae4334":"for name in ['Italy', 'Japan', 'US']:\n    sir = data[name]['sir']\n    dt = data[name]['dt']\n    s0, i0, r0 = sir[0, :]\n    pop = countries.loc[name]['Population']\n    t = range(sir.shape[0])\n\n    # fit SIR model\n    fx = gen_compute_sir(s0, i0, r0, pop)\n    opt_params, opt_cov = curve_fit(fx, t, sir.flatten(), bounds=(0, 1))\n    beta, gamma = opt_params\n    \n    print('{} (beta: {:.4f}, gamma: {:.4f})'.format(name, beta, gamma))\n   \n    # integrate\n    fit = odeint(deriv_sir, (s0, i0, r0), t, args=(pop, beta, gamma))\n    \n    # plot\n    plot_sir(\n        t=dt, i=sir[:, 1], r=sir[:, 2],\n        t_fit=dt, i_fit=fit[:, 1], r_fit=fit[:, 2]\n    )","177efb1a":"# differential equations for SIR w\/ polynomial contact rate\ndef deriv_sir_poly(y, t, n, beta_a, beta_b, gamma):\n    s, i, r = y\n    \n    # compute polynomial contact rate\n    x = i \/ n * DENOM_SZ  # (1000)\n    beta = beta_a * x + beta_b\n\n    dsdt = -beta * s * i \/ n\n    didt = beta * s * i \/ n - gamma * i\n    drdt = gamma * i\n    return dsdt, didt, drdt\n    \n# compute SIR w\/ polynomial contact given candidate values\ndef gen_compute_sir_poly(s0, i0, r0, n):\n    def compute_sir_poly(t, beta_a, beta_b, gamma):\n        init = (s0, i0, r0)\n        pop = n\n        res = odeint(deriv_sir_poly, init, t, args=(pop, beta_a, beta_b, gamma))\n        return res.flatten()\n    return compute_sir_poly","88ac6b93":"for name in ['Italy', 'Japan', 'US']:\n    # select data of interest\n    sir = data[name]['sir']\n    dt = data[name]['dt']\n    s0, i0, r0 = sir[0, :]\n    pop = countries.loc[name]['Population']\n    t = range(sir.shape[0])\n        \n    # fit SIR model\n    fx = gen_compute_sir_poly(s0, i0, r0, pop)\n    opt_params, opt_cov = curve_fit(\n        fx,\n        t,\n        sir.flatten(),\n        bounds=([SIR_POLY_BETA_A_MIN, 0, 0], [0, 1, 1])\n    )\n    beta_a, beta_b, gamma = opt_params\n    \n    print('{} (beta A: {:.4f}, beta B: {:.4f}, gamma: {:.4f})'.format(\n        name, beta_a, beta_b, gamma\n    ))\n        \n    # integrate\n    fit = odeint(\n        deriv_sir_poly,\n        (s0, i0, r0),\n        t,\n        args=(pop, beta_a, beta_b, gamma)\n    )\n    \n    # plot\n    plot_sir(\n        t=dt, i=sir[:, 1], r=sir[:, 2],\n        t_fit=dt, i_fit=fit[:, 1], r_fit=fit[:, 2]\n    )","55448d85":"# setup\ndebug_countries = ['Italy', 'Japan', 'US']\nrmse_normalized = []\nfor name in data:\n    sir = data[name]['sir']\n    dt = data[name]['dt']\n    pop = countries.loc[name]['Population']\n    \n    # get training split\n    train_sz = int(sir.shape[0] - DAYS_TEST)\n    train_t = range(train_sz)\n    train_data = sir[:train_sz]\n    \n    # fit SIR model against training split\n    s0, i0, r0 = sir[0, :]\n    fx = gen_compute_sir_poly(s0, i0, r0, pop)\n    opt_params, opt_cov = curve_fit(\n        fx,\n        train_t,\n        train_data.flatten(),\n        bounds=([SIR_POLY_BETA_A_MIN, 0, 0], [0, 1, 1])\n    )\n    beta_a, beta_b, gamma = opt_params\n        \n    # integrate across train split\n    fit = odeint(\n        deriv_sir_poly,\n        (s0, i0, r0),\n        train_t,\n        args=(pop, beta_a, beta_b, gamma)\n    )\n\n    # integrate across test split\n    test_t = range(train_sz, sir.shape[0])\n    predicted = odeint(\n        deriv_sir_poly,\n        (sir[train_sz, 0], sir[train_sz, 1], sir[train_sz, 2]),\n        test_t,\n        args=(pop, beta_a, beta_b, gamma)\n    )\n    \n    # score RMSE against normalized values\n    test_data = sir[train_sz:]\n    rmse = mean_squared_error(test_data, predicted, squared=False)\n    rmse_normalized.append(rmse \/ pop * DENOM_SZ)\n    \n    if name not in debug_countries:\n        continue\n        \n    print('{}'.format(name))\n    print('beta A: {:.4f}, beta B: {:.4f}, gamma: {:.4f}'.format(\n        beta_a, beta_b, gamma\n    ))\n    print('RMSE: {:.2f}, RMSE per {}: {:.6f}'.format(\n        rmse, DENOM_SZ, rmse \/ pop * DENOM_SZ\n    ))\n    \n    # plot\n    plot_sir(\n        t=dt, i=sir[:, 1], r=sir[:, 2],\n        t_fit=dt[:train_sz], i_fit=fit[:, 1], r_fit=fit[:, 2],\n        t_pred=dt[train_sz:], i_pred=predicted[:, 1], r_pred=predicted[:, 2]\n    )","3f7d5e20":"#All codes from Devin A. Conley https:\/\/www.kaggle.com\/devinaconley\/covid-19-forecasting-seir-svm\/notebook\n\nIf you want to read from the original read Devin's Notebook. There is much more meaningful content there.","10e28d11":"#All codes from Devin A. Conley https:\/\/www.kaggle.com\/devinaconley\/covid-19-forecasting-seir-svm\/notebook\n\nIf you want to read from the original read Devin's Notebook. There is much more meaningful content there. I just ran the cells (as always).","a66c95f5":"Das War's  Kaggle Notebook Runner: Mar\u00edlia Prata  @mpwolke","04bf1134":"#SIR-poly: Evaluation as a predictive model\n\nSubjectively, we can already see a clear improvement in our ability to fit and reproduce these curves.\n\nLet's now split our data into a train and test set to get some hard numbers on actual predictive capability"}}