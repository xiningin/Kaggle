{"cell_type":{"07fa42f8":"code","cb0a18bd":"code","d9cb2855":"code","31d428c4":"code","71a0c99d":"code","53509c80":"code","8732fafd":"code","d544a8e1":"markdown","8acf97b3":"markdown","eca0f591":"markdown","e8c6a5c7":"markdown","4d2a6e63":"markdown","485540b2":"markdown","d6eb48b3":"markdown","99ae493b":"markdown","20eafe0e":"markdown"},"source":{"07fa42f8":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd","cb0a18bd":"sns.set(rc={'axes.facecolor':'snow'})\nmean1 = [0, 4]\ncov1 = [[2.5, 0], [0, 2.5]]  # diagonal covariance\n\n\nx, y = np.random.multivariate_normal(mean1, cov1, 1000).T\nplt.plot(x, y, 'o',alpha=0.25, color = 'firebrick')\nplt.axis('equal')\n\nmean2 = [6, 0]\ncov2 = [[2.5, 0], [0, 2.5]]  # diagonal covariance\n\n\nx, y = np.random.multivariate_normal(mean2, cov2, 1000).T\nplt.plot(x, y, '*',alpha=0.25, color = 'royalblue')\n\nplt.text(-5,8,'Negative',size='large', bbox=dict(facecolor='firebrick', alpha=0.5))\nplt.text(9,-3,'Positive',size='large', bbox=dict(facecolor='royalblue', alpha=0.5))\nplt.grid(False)\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Two Clusters of Observations - Both Bivariate Normal Distn')\n\nplt.show()","d9cb2855":"weights = np.dot(np.array(mean2).T - np.array(mean1).T,np.linalg.inv(cov2))\n\nw2 = weights[1]\nw1 = weights[0]\n\nb = (1\/2)*(np.dot(np.dot(np.array(mean1).T,np.linalg.inv(cov1)),np.array(mean1))-np.dot(np.dot(np.array(mean2).T,np.linalg.inv(cov2)),np.array(mean2))) - np.log(0.5\/0.5)","31d428c4":"# X_mat is the design matrix\n# first 1000 observations are from one distribution, the second 1000 observations are from another distribution\n\nx1, y1 = np.random.multivariate_normal(mean1, cov1, 1000).T\nx2, y2 = np.random.multivariate_normal(mean2, cov2, 1000).T\n\nx = np.array([x1,x2]).flatten()\ny = np.array([y1,y2]).flatten()\n\nX_mat = np.stack([np.ones(2000),x,y],axis=1)","71a0c99d":"# weights from solution\n\nw = [0, w1, w2]\n\n# predicted probabilities from the sigmoid function\n\npredictions = (1\/(1+np.exp(-1*(X_mat.dot(w)+b))))\n\n# actual classes (0 for Negative, 1 for Positive)\n\nactuals = [0]*1000+[1]*1000","53509c80":"# results dataframe \n\nresults_df = pd.concat([pd.DataFrame(X_mat),pd.DataFrame(predictions),pd.DataFrame(actuals)],axis=1)\nresults_df.columns = ['bias','x1','x2','prediction','actual']\nresults_df.head()","8732fafd":"sns.set(rc={'axes.facecolor':'snow'})\nplt.figure(figsize=(9,6))\n\nax = sns.scatterplot('x1','x2',data = results_df,hue = 'prediction',palette='RdBu')\nnorm = plt.Normalize(results_df['prediction'].min(), results_df['prediction'].max())\nsm = plt.cm.ScalarMappable(cmap=\"RdBu\", norm=norm)\nsm.set_array([])\n\nax.get_legend().remove()\ncbar = ax.figure.colorbar(sm)\ncbar.ax.get_yaxis().labelpad = 15\ncbar.ax.set_ylabel('P(Y=1|X) \"Probability of Positive Class\"',rotation=270)\n\n\n# discriminating line\nplt.plot(np.linspace(-5,10,10),(-b\/w2-w1*np.linspace(-5,10,10)\/w2),linestyle='--',color='k',alpha=0.25)\n\nplt.title('Logistic Regression: Binary Classification')\nplt.text(-5,8,'Negative',size='large', bbox=dict(facecolor='firebrick', alpha=0.5))\nplt.text(9,-3,'Positive',size='large', bbox=dict(facecolor='royalblue', alpha=0.5))\nplt.grid(False)\nplt.show()","d544a8e1":"Now that we have the weights associated with our two features, x1 and x2, we can define what discriminates between the two classes.\n\nBy setting the dot product equal to 0, we can find a line to be plotted that discriminates between the two classes and plot it in the 2d plane.\n\n$$ (w^T \\cdot x + b) = 0, $$\n\n$$-b = (w_1*x_1 + w_2*x_2)$$\n\n$$x2 =-\\frac{b}{w_2} -\\frac{w_1*x_1}{w_2}$$","8acf97b3":"## Import Packages","eca0f591":"## Solving for Weights and Discriminant Line\n\nSince both covariance matrices are equal, calculating the weights to separate the two classes has an analytic solution, which can be derived (after some matrix manipulation).  Refer to this derivation to see how this solution was calculated.","e8c6a5c7":"## Predicted Probabilities\n\nThe predicted probability for each observation is returned by passing the weights, features, and bias to the sigmoid function.\n\n$$P(Y=1|X) = \\frac{1}{1 + e^{-(w^T \\cdot x + b)}}$$","4d2a6e63":"# End","485540b2":"## Problem Statement\n\nUsing the bivariate means and covariate matrices, find a discriminating line that separates the two classes of observations on a 2-d plane and plot it.","d6eb48b3":"# Binary Classification for Classes with Equal Covariances Using Logistic Regression\n\n+ Author: Casey Whorton\n+ Last edited: 1\/12\/2021\n\n**Purpose:** This notebook showcases a simple binary classification problem and how it can be solved analytically without the use of an algorithm, and the results plotted with explanation.\n\n**Description:** We look at two clusters of observations, both with a bivariate normal distribution. One of the clusters is class 0, or \"Negative\" and the other is class 1, or \"Positive\". Visually, we can detect where to draw a line to separate these two classes, but we want a solution using the data from the two distributions. We'll use the results of _this derivation_ as the solution of the weights to be applied in a sigmoid function that predicts the probability of falling into the \"Positive\" class, (class 1).","99ae493b":"## Plot Solution\n\nDots nearest to the discriminating line have predicted probabilities closer to 0.5.","20eafe0e":"## Plot the Data"}}