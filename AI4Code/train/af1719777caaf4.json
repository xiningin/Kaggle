{"cell_type":{"a6a4632e":"code","be5be12a":"code","bc1392c4":"code","f60cc6a0":"code","ab3f6ce2":"code","e302e86f":"code","a79a5918":"code","7c24c79d":"code","0d0bd266":"code","ff03acc6":"code","71a27430":"code","986ad9e9":"code","61210cfb":"code","07ddcb99":"code","13b79404":"code","da6705a3":"code","39ff99bb":"code","006b7d63":"code","84ef07dc":"code","565f41da":"code","e27b7917":"code","b1246b90":"code","702d673e":"code","876f9e2d":"code","e2e2cecf":"code","f904ca32":"code","f9eee5a3":"code","9a5d0de3":"code","1b9ad6a1":"code","d5907296":"code","4d4cae8d":"code","1e0c6eaf":"code","d9662ca9":"code","1a94ce2e":"code","07a9dd7c":"code","432a56a5":"code","391113a9":"code","bc2ae386":"code","752cbf95":"markdown","bf831a3f":"markdown","fdf443de":"markdown","d808099a":"markdown","23bca497":"markdown","d43cd02b":"markdown","94d11e3c":"markdown","9cdf3713":"markdown","80f391bf":"markdown","e3f7902b":"markdown","5fde7fea":"markdown","3105c074":"markdown","3f54f859":"markdown","84b23f8d":"markdown","670e9bcc":"markdown","01690351":"markdown","dea8b33f":"markdown","fa520b73":"markdown","9eec9680":"markdown","0e87366f":"markdown","dc36400d":"markdown","01c76664":"markdown","cca8aab1":"markdown","67e9b02f":"markdown","2baf9448":"markdown"},"source":{"a6a4632e":"import json\nimport pandas as pd\nimport numpy as np\nimport time\nimport tensorflow as tf\nimport seaborn as sns\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt","be5be12a":"vocab_size = 10000 # Vocabulary size\nsequence_length = 1024 # Sequence Length\nbatch_size = 128 # Batch size\nunk_token = \"<unk>\" # Unknownd token\nvectorizer_path = \"vectorizer.json\"\n# Use output dataset for inference\noutput_dataset_path = \"..\/input\/name-entity-recognition-with-keras-output\/\"\nmodel_path = \"model.h5\"\nembed_size = 64\nhidden_size = 64\nmodes = [\"training\", \"inference\"] # There is training and inference mode\nmode = modes[1]\nepochs = 10\ndropout = 0.2 # Dropout rate for the Model.","bc1392c4":"train = pd.read_csv(\"..\/input\/feedback-prize-2021\/train.csv\")\ntrain.head()","f60cc6a0":"submission = pd.read_csv(\"..\/input\/feedback-prize-2021\/sample_submission.csv\")\nsubmission.head()","ab3f6ce2":"train[\"file_path\"] = train[\"id\"].apply(lambda item: \"..\/input\/feedback-prize-2021\/train\/\" + item + \".txt\")\ntrain.head()","e302e86f":"submission[\"file_path\"] = submission[\"id\"].apply(lambda item: \"..\/input\/feedback-prize-2021\/test\/\" + item + \".txt\")\nsubmission.head()","a79a5918":"train[\"discourse_type\"].value_counts().plot(kind=\"bar\")","7c24c79d":"discourse_types = np.array([\"<PAD>\", \"<None>\"] + sorted(train[\"discourse_type\"].unique()))\ndiscourse_types_index = dict([(discoure_type, index) for (index, discoure_type) in enumerate(discourse_types)])\ndiscourse_types, discourse_types_index","0d0bd266":"train[\"discourse_type_num\"].value_counts().plot(kind=\"bar\")","ff03acc6":"len(train[\"id\"].unique())","71a27430":"def get_range(item):\n    locations = [int(location) for location in item[\"predictionstring\"].split(\" \")]\n    return (locations[0], locations[-1])","986ad9e9":"character_counter = defaultdict(int)\ncharacter_counter\nallow_set = set(\"'&%-_\/$+\u00c2\u00c3\u00c5\u00cb\u00d3\u00e2\u00e5\u00f3\u00fe@|~\u00a2\u00a3\u00a2\u00a3\")\ndef tokenize(text):\n    tokens = []\n    chars = []\n    for i in range(len(text)):\n        c = text[i].lower()\n        character_counter[c] += 1\n        is_valid = c.isalnum() or c in allow_set\n        if i >= 1 and i < len(text) - 1:\n            if text[i-1].isdigit() and text[i+1].isdigit():\n                is_valid = True\n            elif text[i-1].isalpha() and text[i+1].isalpha() and c == \".\":\n                is_valid = True\n        if is_valid:\n            chars.append(c)\n        if (not is_valid or i == len(text) - 1) and len(chars) > 0:\n            tokens.append(\"\".join(chars))\n            chars.clear()\n    return tokens","61210cfb":"%%time\nbegin = time.time()\nlast_id = \"\"\ncontents = []\nwrong_samples = []\ntoken_list = []\nannotation_list = []\nnum_samples = len(train)\nunmaptch_count = 0 # Number of sentences extracted from predictionstring that doesn't discourse_text\nmatch_count = 0 # Number of sentences extracted from predictionstring that matches discourse_text including shifting\ncompletely_match_count = 0 # Number of sentences extracted from predictionstring that matches discourse_text without shifting\nmismatch_count = 0\nfor i in range(len(train)):\n    item = train.iloc[i]\n    identifier = item[\"id\"] \n    discourse_type_id = discourse_types_index[item[\"discourse_type\"]]\n    if identifier != last_id:\n        last_id = identifier\n        with open(item[\"file_path\"]) as f:\n            content = \"\".join(f.readlines())\n            contents.append(content)\n            tokens = tokenize(content)\n            token_list.append(tokens)\n            annotations = [1] * len(tokens)\n            annotation_list.append(annotations)\n    annotation_range = get_range(item)\n    extracted = tokens[annotation_range[0]:annotation_range[1]+1]\n    discourse = tokenize(item[\"discourse_text\"])\n    delta = None\n    num_tokens_to_compare = min(len(discourse), 3)\n    \n    # Compare text extracted from predictionstring with discourse_text, shift discourse_text or right if needed, just compare a few words for performance\n    for j in range(10):\n        if len(extracted) < num_tokens_to_compare or len(discourse) <= j + num_tokens_to_compare:\n            break\n        if extracted[0:num_tokens_to_compare] == discourse[j:num_tokens_to_compare+j]:\n            delta = j\n            break\n    if delta == None:\n        for j in range(10):\n            if len(discourse) < num_tokens_to_compare and len(extracted) <= j + num_tokens_to_compare:\n                break\n            if discourse[0:num_tokens_to_compare] == extracted[j:num_tokens_to_compare+j]:\n                delta = -j\n                break\n    if delta == None:\n        unmaptch_count += 1\n    else:\n        not_match = False\n        for j in range(annotation_range[0] - delta, min(min(annotation_range[1] - delta + 1, len(tokens)), len(discourse) + annotation_range[0] - delta)): \n            if tokens[j] != discourse[j - annotation_range[0] + delta]:\n                mismatch_count += 1\n                not_match = True\n                break\n        if not not_match:\n            for j in range(annotation_range[0] - delta, min(min(annotation_range[1] - delta + 1, len(tokens)), len(discourse) + annotation_range[0] - delta)): \n                annotation_list[-1][j] = discourse_type_id\n            match_count += 1\n        else:\n            unmaptch_count += 1\n        if delta == 0:\n            completely_match_count += 1 \nprint(\"Unmatch count:%d Match Count: %d Completedly Match count: %d\"%(unmaptch_count, match_count, completely_match_count))\nprint(\"Mismatch count:\", mismatch_count)\nprint(token_list[0])\nprint(annotation_list[0])","07ddcb99":"useful_tokens = []\nuseful_annotations = []\nfor i in range(len(annotation_list)):\n    if np.sum(annotation_list[i]) != 0:\n        useful_tokens.append(token_list[i])\n        useful_annotations.append(annotation_list[i])\ntoken_list = useful_tokens\nannotation_list = useful_annotations","13b79404":"word_counter = defaultdict(int)\nfor tokens in token_list:\n    for token in tokens:\n        word_counter[token] += 1","da6705a3":"word_count = pd.DataFrame({\"key\": word_counter.keys(), \"count\": word_counter.values()})\n\nsns.barplot(x=\"key\", y=\"count\", data=word_count[:30])","39ff99bb":"word_count.describe()","006b7d63":"len(word_count)","84ef07dc":"(word_count[\"count\"] == 1).sum()","565f41da":"character_count = pd.DataFrame({\"key\": character_counter.keys(), \"count\": character_counter.values()})\ncharacter_count.sort_values(by=\"count\", ascending=False, inplace=True)\ncharacter_count.head(30)","e27b7917":"sns.barplot(x=\"key\", y=\"count\", data=character_count[:30])","b1246b90":"print(list(character_count['key'].unique()))","702d673e":"sentence_length = defaultdict(int)\nfor tokens in token_list:\n    length = len(tokens)\n    sentence_length[length] += 1\nsentence_length = pd.DataFrame({\"sentence_length\": sentence_length.keys(), \"count\": sentence_length.values()})\nsentence_length.head()","876f9e2d":"sentence_length.describe()","e2e2cecf":"sentence_length[(sentence_length[\"sentence_length\"] >= 1000)][\"count\"].sum()","f904ca32":"class Vectorizer:\n    \n    def __init__(self, vocab_size = None, sequence_length = None, unk_token = \"<unk>\"):\n        self.vocab_size = vocab_size\n        self.sequence_length = sequence_length\n        self.unk_token = unk_token\n        \n    def fit_transform(self, sentences):\n        word_counter = dict()\n        for tokens in sentences:\n            for token in tokens: \n                if token in word_counter:\n                    word_counter[token] += 1\n                else:\n                    word_counter[token] = 1\n        word_counter = pd.DataFrame({\"key\": word_counter.keys(), \"count\": word_counter.values()})\n        word_counter.sort_values(by=\"count\", ascending=False, inplace=True)\n        vocab = set(word_counter[\"key\"][0:self.vocab_size-1])\n        word_index = dict()\n        begin_index = 1 \n        word_index[self.unk_token] = begin_index\n        begin_index += 1\n        Xs = []\n        for i in range(len(sentences)):\n            X = []\n            for token in sentences[i]:\n                if token not in word_index and token in vocab:\n                    word_index[token] = begin_index\n                    begin_index += 1\n                if token in word_index:\n                    X.append(word_index[token])\n                else:\n                    X.append(word_index[self.unk_token])\n                if len(X) == self.sequence_length:\n                    break\n            if len(X) < self.sequence_length:\n                X += [0] * (self.sequence_length - len(X))\n            Xs.append(X)\n        self.word_index = word_index\n        self.vocab = vocab\n        return Xs\n    \n    def transform(self, sentences):\n        Xs = []\n        for i in range(len(sentences)):\n            X = []\n            for token in sentences[i]:\n                if token in self.word_index:\n                    X.append(self.word_index[token])\n                else:\n                    X.append(self.word_index[self.unk_token])\n                if len(X) == self.sequence_length:\n                    break\n            if len(X) < self.sequence_length:\n                X += [0] * (self.sequence_length - len(X))\n            Xs.append(X)\n        return Xs\n    \n    def load(self, path):\n        with open(path, 'r') as f:\n            dic = json.load(f)\n            self.vocab_size = dic['vocab_size']\n            self.sequence_length = dic['sequence_length']\n            self.unk_token = dic['unk_token']\n            self.word_index = dic['word_index']\n            \n    def save(self, path):\n        with open(path, 'w') as f:\n            data = json.dumps({\n                \"vocab_size\": self.vocab_size, \n                \"sequence_length\": self.sequence_length, \n                \"unk_token\": self.unk_token,\n                \"word_index\": self.word_index\n            })\n            f.write(data)","f9eee5a3":"%%time\nvectorizer = Vectorizer(vocab_size = vocab_size, sequence_length = sequence_length, unk_token = unk_token)\nif mode == modes[0]:\n    Xs = vectorizer.fit_transform(token_list)\n    vectorizer.save(vectorizer_path)\n\nelse:\n    vectorizer.load(output_dataset_path + vectorizer_path)\n    Xs = vectorizer.transform(token_list)\nys = []\nannotation_count = [0] * len(discourse_types_index)\nfor annotation in annotation_list:\n    if len(annotation) <= sequence_length:\n        ys.append(annotation + [0] * (sequence_length - len(annotation)))\n    else:\n        ys.append(annotation[0:sequence_length])\n    for item in ys[-1]:\n        annotation_count[item] += 1\nX_train, X_val, y_train, y_val = train_test_split(np.array(Xs), np.array(ys), test_size = 0.2, random_state=42)","9a5d0de3":"annotation_count_df = pd.DataFrame({\n    \"key\":discourse_types,\n    \"value\": list(range(len(discourse_types))),\n    \"count\": annotation_count\n})\nplt.figure(figsize=(15, 10))\nsns.barplot(x=\"key\", y=\"count\", data=annotation_count_df)","1b9ad6a1":"def make_dataset(X, y, batch_size, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices((X, y))\n    if mode == \"train\":\n        ds = ds.shuffle(512)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds","d5907296":"train_ds = make_dataset(X_train, y_train, batch_size)\nval_ds = make_dataset(X_val, y_val, batch_size, mode=\"valid\")","4d4cae8d":"model = keras.Sequential([\n    keras.layers.Embedding(vocab_size, embed_size, input_length=sequence_length),\n    keras.layers.SpatialDropout1D(dropout),\n    keras.layers.Bidirectional(keras.layers.LSTM(hidden_size, dropout=dropout, recurrent_dropout=dropout)),\n    keras.layers.RepeatVector(sequence_length),\n    keras.layers.Bidirectional(keras.layers.LSTM(hidden_size, return_sequences=True)),\n    keras.layers.TimeDistributed(keras.layers.Dense(len(discourse_types), activation=\"softmax\"))\n])\nmodel.summary()","1e0c6eaf":"keras.utils.plot_model(model, show_shapes=True, show_dtype=True)","d9662ca9":"if mode == modes[0]:\n    checkpoint = keras.callbacks.ModelCheckpoint(\n        model_path, \n        save_best_only=True,\n        save_weights_only=True\n    )\n    early_stop = keras.callbacks.EarlyStopping(\n        min_delta=1e-4, \n        patience=10\n    )\n    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n        factor=0.3,\n        patience=2, \n        min_lr=1e-7\n    )\n    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n    callbacks = [early_stop, checkpoint, reduce_lr]\n    optimizer = tf.keras.optimizers.Adam(1e-3)\n    model.compile(loss=loss, optimizer=optimizer)\n    model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=callbacks)\nelse:\n    model.load_weights(output_dataset_path + model_path)","1a94ce2e":"from sklearn.metrics import f1_score, classification_report\ndef evaluate(model, dataset):\n    all_true_tag_ids, all_predicted_tag_ids = [], []\n    for x, y in dataset:\n        output = model.predict(x)\n        predictions = np.argmax(output, axis=-1)\n        predictions = np.reshape(predictions, [-1])\n\n        true_tag_ids = np.reshape(y, [-1])\n\n        mask = (true_tag_ids > 0) & (predictions > 0)\n        true_tag_ids = true_tag_ids[mask]\n        predicted_tag_ids = predictions[mask]\n\n        all_true_tag_ids.append(true_tag_ids)\n        all_predicted_tag_ids.append(predicted_tag_ids)\n\n    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n    cls_report = classification_report(all_true_tag_ids, all_predicted_tag_ids)\n    print(cls_report)\n    f1 =  f1_score(all_true_tag_ids, all_predicted_tag_ids, average=\"micro\")\n    print(\"F1 Score:\", f1)\nevaluate(model, val_ds)","07a9dd7c":"%%time\ncontents = []\ntoken_list = []\nfor i in range(len(submission)):\n    item = submission.iloc[i]\n    identifier = item[\"id\"] \n    with open(item[\"file_path\"]) as f:\n        content = \"\".join(f.readlines())\n        contents.append(content)\n        tokens = tokenize(content)\n        token_list.append(tokens)","432a56a5":"X_test = vectorizer.transform(token_list)\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test)).batch(batch_size)\ny_pred = model.predict(test_ds)\ny_pred = np.argmax(y_pred, axis=-1)\nprint(y_pred.shape)","391113a9":"predictionstrings = []\nclasses = []\nids = []\nfor i in range(y_pred.shape[0]):\n    identifier = submission.iloc[i][\"id\"]\n    last_prediction = 0\n    indices = []\n    upper_bound = min(y_pred.shape[1], len(token_list[i]))\n    for j in range(upper_bound):\n        if last_prediction != y_pred[i, j]:\n            if len(indices) > 0:\n                ids.append(identifier)\n                predictionstrings.append(\" \".join(indices))\n                classes.append(discourse_types[last_prediction])\n                indices = []\n            last_prediction = y_pred[i, j]\n        if y_pred[i, j] > 1:\n            indices.append(str(j))\n        if j == upper_bound - 1:\n            if len(indices) > 0:\n                ids.append(identifier)\n                predictionstrings.append(\" \".join(indices))\n                classes.append(discourse_types[last_prediction])","bc2ae386":"sub_df = pd.DataFrame({\"id\": ids, \"class\": classes, \"predictionstring\": predictionstrings})\nsub_df.to_csv(\"submission.csv\", index=False)\nsub_df.head()","752cbf95":"### Ditrubtion of Sentence Length","bf831a3f":"### Name Entity Recognition  Model","fdf443de":"# Name Entity Recognition with Keras\nIn this notebook, I will build a Name Entity Recognition Model using Keras to evaluate student writing using dataset for Kaggle Competition [Feedback Prize - Evaluating Student Writing](https:\/\/www.kaggle.com\/c\/feedback-prize-2021). In this Project, Modeling part will be very easy, what's challenging is converting this dataset to Name Entity Recognization format that can be handled by Keras Name Entity Recognition Model. I also perform some Exploratory Data Analysis to find insights.\n## Import Packages","d808099a":"## Common Parameters","23bca497":"## Import Datasets","d43cd02b":"#### Number of sentences that has more than 1000 tokens","94d11e3c":"#### Words appearing only once","9cdf3713":"## Create Tensorflow Dataset","80f391bf":"### Distibution of Character Counts","e3f7902b":"## Modeling","5fde7fea":"### Number of Unique files","3105c074":"### Distribution of Word Counts","3f54f859":"## Distribution of Labels","84b23f8d":"## EDA & Preprocessing","670e9bcc":"### Vectorization","01690351":"### Add File Path to train and submission Files","dea8b33f":"### Filter samples without annotations","fa520b73":"## Distribution of discourse_type_num","9eec9680":"## Training","0e87366f":"### Model Evaluation","dc36400d":"## Submission","01c76664":"## Disturbution of annotation","cca8aab1":"### Unique Characters","67e9b02f":"### Tokenization\n\nI am trying to build a Tokenizer to tokenize sentences that extracted from predictionstring and see if it can match with the discourse_text including shifting left and right. A good Tokenizer can match more cases without shifting sentences or just a few shifting, so that it may have a better prediction on test set.","2baf9448":"#### Number of words"}}