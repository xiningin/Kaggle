{"cell_type":{"94419f63":"code","b72fa3ec":"code","5d6a7be2":"code","4579fb48":"code","6140c5e5":"code","4209ec85":"code","9fde870d":"code","030b1035":"code","44525a81":"code","9469f561":"code","38c7e19e":"code","7b9c8537":"code","8dddee72":"code","d0b7fd25":"code","1a430a40":"code","e7f843e4":"code","2ebcd27b":"code","d28c32b4":"code","d761b783":"code","d591b604":"code","ccf1058e":"code","35b3075e":"code","2c048bc8":"code","f9c06476":"code","f4f83c4f":"code","ab52f519":"code","70f46c79":"code","022e1c34":"code","0dfadd68":"code","d8c35aab":"code","7aed1929":"code","f55b7df8":"code","77c015fa":"code","5295896c":"code","3e98cb3b":"code","fb78e7c1":"code","e0411dfa":"code","14e03f6f":"code","91391982":"code","abec3699":"code","8cad98e6":"code","0aa3c3ba":"code","87439b76":"code","7bb45c4f":"code","873a7266":"code","cbe4003f":"code","5bdf8905":"code","abc81d36":"code","7345e963":"code","ad96377b":"code","249fd10d":"code","d6ecd3a5":"code","f00e80ae":"code","02b0f3e8":"code","e8987d83":"code","3b8a1155":"code","34ee910a":"code","c11fbf17":"code","d21e030e":"code","5215a314":"code","b76ae3ae":"code","3fb5bf81":"code","47998b46":"code","c9453fd8":"code","5c5e25f0":"code","42164f15":"code","10bca61d":"code","abf49a29":"code","6319b4a0":"code","f46a25c6":"code","c2e43ade":"code","4348d231":"code","0e9f6f75":"code","5bcd1127":"code","f8145bf5":"code","af457de2":"markdown","3b7523a4":"markdown","ccc9489f":"markdown"},"source":{"94419f63":"!pip install d2l","b72fa3ec":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom tqdm import tqdm\nfrom d2l import torch as d2l\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"","5d6a7be2":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.preprocessing import LabelEncoder                    #\u6807\u7b7e\u7f16\u7801\nfrom sklearn.preprocessing import RobustScaler, StandardScaler    #\u53bb\u9664\u5f02\u5e38\u503c\u4e0e\u6570\u636e\u6807\u51c6\u5316\nfrom sklearn.pipeline import Pipeline, make_pipeline              #\u6784\u5efa\u7ba1\u9053\nfrom scipy.stats import skew                                 #\u504f\u5ea6\nfrom scipy.special import boxcox1p                           # box-cox\u53d8\u6362\nfrom sklearn.decomposition import PCA","4579fb48":"pd.set_option('max_columns', 10000)      # \u63a7\u5236\u663e\u793a\u7684\u5217\u8303\u56f4\uff0c\u67e5\u770b\u6570\u636e\u7684\u65f6\u5019\uff0c\u663e\u793a\u6240\u6709\u6570\u636e\uff0c\u800c\u4e14\u6570\u636e\u8868\u4e2d\u6ca1\u6709\u7701\u7565\u53f7\npd.set_option('max_rows', 5000)","6140c5e5":"data_0 = pd.read_csv('..\/input\/california-house-prices\/train.csv')\ndata_0.head(5) #train:47439x41","4209ec85":"import pandas_profiling as ppf\n# ppf.ProfileReport(data_0)","9fde870d":"data_0.columns","030b1035":"import matplotlib.pyplot as plt\nplt.figure(figsize=(18,12))\n\nplt.subplot(2, 2, 1)\nplt.scatter(x=data_0['Last Sold Price'], y=data_0['Sold Price'],color='b') ##\u53ef\u4ee5\u7528\u6765\u89c2\u5bdf\u5b58\u5728\u7ebf\u578b\u7684\u5173\u7cfb\nplt.xlabel(\"Last Sold Price\", fontsize=13)\nplt.ylabel(\"SalePrice\", fontsize=13)\n\nplt.subplot(2, 2, 2)\nplt.scatter(x=data_0['Year built'], y=data_0['Sold Price'],color='b') ##\u53ef\u4ee5\u7528\u6765\u89c2\u5bdf\u5b58\u5728\u7ebf\u578b\u7684\u5173\u7cfb\nplt.xlabel(\"YearBuilt\", fontsize=13)\nplt.ylabel(\"SalePrice\", fontsize=13)\n\nplt.subplot(2, 2, 3)\nplt.scatter(x=data_0['Tax assessed value'], y=data_0['Sold Price'],color='b') ##\u53ef\u4ee5\u7528\u6765\u89c2\u5bdf\u5b58\u5728\u7ebf\u578b\u7684\u5173\u7cfb\nplt.xlabel(\"Tax assessed value\", fontsize=13)\nplt.ylabel(\"SalePrice\", fontsize=13)\n\nplt.subplot(2, 2, 4)\nplt.scatter(x=data_0['Total interior livable area'], y=data_0['Sold Price'],color='b') ##\u53ef\u4ee5\u7528\u6765\u89c2\u5bdf\u5b58\u5728\u7ebf\u578b\u7684\u5173\u7cfb\nplt.xlabel(\"Total interior livable area\", fontsize=13)\nplt.ylabel(\"SalePrice\", fontsize=13)    \n\nplt.show()","44525a81":"#\u5220\u9664\u5f02\u5e38\u503c\ndata_0.drop(data_0[(data_0['Year built']>9000)].index,inplace = True)\ndata_0.drop(data_0[(data_0['Year built']==0)].index,inplace = True)\ndata_0.drop(data_0[(data_0['Total interior livable area']==1)].index,inplace = True)\ndata_0.drop(data_0[(data_0['Total interior livable area']==176416380)].index,inplace = True)\ndata_0.drop(data_0[(data_0['Last Sold Price']>80000000)].index,inplace = True)\ndata_0.drop(data_0[(data_0['Tax assessed value']>40000000)].index,inplace = True)\ndata_0.drop(data_0[(data_0['Sold Price']>80000000)].index,inplace = True)\ndata_0.reset_index(drop=True, inplace=True)\ndata_0.shape","9469f561":"data_0['Last Sold Price']","38c7e19e":"import seaborn as sns\nfrom scipy.stats import *\ndef plt_distribution(data, obj_col):\n    sns.distplot(data[obj_col] , fit=norm);\n\n    # \u83b7\u53d6\u8bad\u7ec3\u96c6\u6570\u636e\u5206\u5e03\u66f2\u7ebf\u7684\u62df\u5408\u5747\u503c\u548c\u6807\u51c6\u5dee\n    (mu, sigma) = norm.fit(data[obj_col])\n    print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n    # \u7ed8\u5236\u5206\u5e03\u66f2\u7ebf\n    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n                loc='best')\n    plt.ylabel('Frequency')\n    plt.title('SalePrice distribution')\n\n    # \u7ed8\u5236\u56fe\u50cf\u67e5\u770b\u6570\u636e\u7684\u5206\u5e03\u72b6\u6001\n    fig = plt.figure()\n    res = probplot(data[obj_col], plot=plt)\n    plt.show()\n\n# plt_distribution(data_0,'Last Sold Price')\ndata_0[\"Last Sold Price\"] = np.log1p(data_0[\"Last Sold Price\"]) # \u5bf9\u6570\u53d8\u6362\n# plt_distribution(data_0, 'Last Sold Price')","7b9c8537":"data_0['Sold Price'] = np.log1p(data_0['Sold Price'])","8dddee72":"plt.scatter(x=data_0['Last Sold Price'], y=data_0['Sold Price'],color='b')","d0b7fd25":"# plt_distribution(data_0, 'Total interior livable area')\ndata_0[\"Total interior livable area\"] = np.log1p(data_0[\"Total interior livable area\"]) # \u5bf9\u6570\u53d8\u6362\n# plt_distribution(data_0, 'Total interior livable area')","1a430a40":"# plt_distribution(data_0, 'Annual tax amount')\ndata_0[\"Annual tax amount\"] = np.log1p(data_0[\"Annual tax amount\"]) # \u5bf9\u6570\u53d8\u6362\n# plt_distribution(data_0, 'Annual tax amount')","e7f843e4":"# plt_distribution(data_0, 'Zip')\ndata_0[\"Zip\"] = np.log1p(data_0[\"Zip\"]) # \u5bf9\u6570\u53d8\u6362\n# plt_distribution(data_0, 'Zip')","2ebcd27b":"# plt_distribution(data_0, 'Listed Price')\ndata_0[\"Listed Price\"] = np.log1p(data_0[\"Listed Price\"]) # \u5bf9\u6570\u53d8\u6362\n# plt_distribution(data_0, 'Listed Price')","d28c32b4":"# plt_distribution(data_0, 'Tax assessed value')\ndata_0[\"Tax assessed value\"] = np.log1p(data_0[\"Tax assessed value\"]) # \u5bf9\u6570\u53d8\u6362\n# plt_distribution(data_0, 'Tax assessed value')","d761b783":"data_0.drop(data_0[(data_0['Last Sold Price']<2.5)].index,inplace = True)","d591b604":"plt.scatter(x=data_0['Last Sold Price'], y=data_0['Sold Price'],color='b')","ccf1058e":"data_0['Last Sold Price'].fillna(0, inplace=True)\nlabel_list = []\nfor item in data_0.iterrows():\n    if item[1]['Last Sold Price'] ==0:\n        item= item[1]['Sold Price']\n        label_list.append(item)\n    else:\n        label_list.append(item[1]['Last Sold Price'])\ndata_0['Last Sold Price'] = label_list\nfor item in data_0['Last Sold Price']:\n    if item ==0:\n        print('\u5b58\u57280\u503c')","35b3075e":"data_0.isnull().sum()","2c048bc8":"data_0.head()","f9c06476":"data_1 = pd.read_csv('..\/input\/california-house-prices\/test.csv')\ndata_1.head() #test: 31626x40","f4f83c4f":"data_1.isnull().sum()","ab52f519":"data_1[\"Last Sold Price\"] = np.log1p(data_1[\"Last Sold Price\"]) # \u5bf9\u6570\u53d8\u6362\ndata_1[\"Total interior livable area\"] = np.log1p(data_1[\"Total interior livable area\"]) # \u5bf9\u6570\u53d8\u6362\ndata_1[\"Annual tax amount\"] = np.log1p(data_1[\"Annual tax amount\"]) # \u5bf9\u6570\u53d8\u6362\ndata_1[\"Zip\"] = np.log1p(data_1[\"Zip\"]) # \u5bf9\u6570\u53d8\u6362\ndata_1[\"Listed Price\"] = np.log1p(data_1[\"Listed Price\"]) # \u5bf9\u6570\u53d8\u6362\ndata_1[\"Tax assessed value\"] = np.log1p(data_1[\"Tax assessed value\"]) # \u5bf9\u6570\u53d8\u6362","70f46c79":"train_y = data_0['Sold Price']\ntrain_x = data_0.drop('Sold Price',axis = 1)\nprint(train_x.shape)   #(47439, 39)\nprint(train_y.shape)   #(47439,)\nprint(data_1.shape) #(31626,39)\n#\u5408\u5e76\u6d4b\u8bd5\u96c6\u4e0e\u8bad\u7ec3\u96c6\uff0c\u4e00\u8d77\u8fdb\u884c\u7279\u5f81\u5de5\u7a0b\ndata = pd.concat([train_x,data_1],axis=0,ignore_index=True)\ndata = data.drop('Id',axis = 1)\nprint(data.shape)#(78818,39)\ndata.head()","022e1c34":"plt.scatter(x=data['Last Sold Price'], y=data['Listed Price'],color='b')","0dfadd68":"count=data.isnull().sum().sort_values(ascending=False)\nratio=count\/len(data)\nnulldata=pd.concat([count,ratio],axis=1,keys=['count','ratio'])\nnulldata[nulldata.ratio>0]","d8c35aab":"Bedrooms_col = data['Bedrooms']\ntemp_Bedrooms = []\nfor item in Bedrooms_col:\n    if item=='nan':\n        temp_Bedrooms.append(0)\n    elif isinstance(item,str):\n        n = len(item.split(','))\n        if n ==0:\n            temp_Bedrooms.append(1)\n        else:\n            temp_Bedrooms.append(n)\n    else:\n        temp_Bedrooms.append(item)\nBedrooms_col = temp_Bedrooms\ndata['Bedrooms'] = Bedrooms_col","7aed1929":"numcol = data.dtypes[data.dtypes != 'object'].index.tolist()\nnumcol","f55b7df8":"numcol_0 = ['Lot','Garage spaces','Listed Price','Elementary School Score','Elementary School Distance','Middle School Score',\n           'Middle School Distance','High School Score','High School Distance']\n\nnumcol_1 = ['Year built','Bedrooms','Bathrooms','Full bathrooms','Total interior livable area','Total spaces','Tax assessed value','Annual tax amount','Zip']","77c015fa":"for col in numcol_0:\n    data[col].fillna(0, inplace=True)\n\ndel col    \nfor col in numcol_1:\n    data[col].fillna(data[col].mode()[0], inplace=True)\ndel col","5295896c":"strcol =  data.dtypes[data.dtypes == 'object'].index.tolist()\nstrcol","3e98cb3b":"strcol_0 = ['Summary','Heating','Cooling','Parking','Region', 'Elementary School',\n 'Middle School','High School','Flooring', 'Heating features','Cooling features', 'Appliances included',\n 'Laundry features','Parking features','Listed On','City','State','Last Sold On']\n\nfor col in strcol_0:\n    data[col].fillna(\"None\",inplace=True)\ndel col","fb78e7c1":"# ppf.ProfileReport(data)","e0411dfa":"heat = []\ntemp = []\nfor col in data['Heating']:\n    heat = col.split(',')\n    if heat.count('Central')== 1:\n        temp.append(19371)\n        \n    elif heat.count('Central Forced Air')== 1:\n        temp.append(5942)\n        \n    elif heat.count('Forced Air')== 1:\n        temp.append(5009)\n        \n    elif heat.count(' Central Forced Air - Gas')== 1:\n        temp.append(3883)\n        \n    elif heat.count('None') == 1:\n        temp.append(12207)\n        \n    else:\n        temp.append(32653)\n        \ndata['Heating'] = temp","14e03f6f":"type1 = []\ntemp = []\ni = 0\nfor col in data['Type']:\n    \n    type1 = col.split(',')\n    if type1.count('SingleFamily')== 1:\n        temp.append(53521)\n        \n    elif type1.count('Condo')== 1:\n        temp.append(12794)\n        \n    elif type1.count('Townhouse')== 1:\n        temp.append(4096)\n        \n    elif type1.count('MultiFamily')== 1:\n        temp.append(2258)\n        \n    elif type1.count('Unknown') == 1:\n        temp.append(2168)\n        \n    else:\n        temp.append(4228)\n#     print(type1)\n#     i = i+1\n#     if i == 10:\n#         break\n# temp        \ndata['Type'] = temp","91391982":"cool = []\ntemp = []\ni = 0\nfor col in data['Cooling']:\n    \n    cool = col.split(',')\n    if cool.count('None')== 1:\n        temp.append(30144)\n        \n    elif cool.count('Central Air')== 1:\n        temp.append(21526)\n        \n    elif cool.count('Central AC')== 1:\n        temp.append(6595)\n        \n    elif cool.count('Central')== 1:\n        temp.append(4217)\n        \n    elif cool.count('Wall\/Window Unit(s)') == 1:\n        temp.append(1511)\n        \n    else:\n        temp.append(15072)       \ndata['Cooling'] = temp","abec3699":"r = []\ntemp = []\ni = 0\nfor col in data['Region']:\n    \n    r = col.split(',')\n    if r.count('Los Angeles')== 1:\n        temp.append(12273)\n        \n    elif r.count('San Jose')== 1:\n        temp.append(7298)\n        \n    elif r.count('San Francisco')== 1:\n        temp.append(5743)\n        \n    elif r.count('San Diego')== 1:\n        temp.append(1100)\n        \n    elif r.count('San Mateo') == 1:\n        temp.append(910)\n        \n    else:\n        temp.append(51741)\n#     print(r)\n#     i = i+1\n#     if i == 100:\n#         break\n# temp        \ndata['Region'] = temp","8cad98e6":"f = []\ntemp = []\ni = 0\nfor col in data['Flooring']:\n    \n    f = col.split(',')\n    if f.count('None')== 1:\n        temp.append(21863)\n        \n        \n    elif f.count('Wood')== 1:\n        temp.append(5599)\n        \n        \n    elif f.count(' Laminate')== 1:\n        temp.append(3182)\n        \n        \n    elif f.count(' HardWood')== 1:\n        temp.append(2885)\n        \n        \n    elif (f.count('Carpet') == 1) and (f.count(' Tile') == 1):\n        temp.append(2863)\n        \n        \n    else:\n        temp.append(42606)\n#     print(f)\n#     i = i+1\n#     if i == 100:\n#         break\n# temp        \ndata['Flooring'] = temp","0aa3c3ba":"hf = []\ntemp = []\ni = 0\nfor col in data['Heating features']:\n    \n    hf = col.split(',')\n    if hf.count('Central')== 1:\n        temp.append(15980)\n        \n        \n    elif hf.count('None')== 1:\n        temp.append(13275)\n        \n        \n    elif (hf.count('Forced air')== 1) and (hf.count(' Gas')== 1):\n        temp.append(10096)\n        \n        \n    elif (hf.count('Forced air')== 1) or (hf.count('Forced Air')== 1):\n        temp.append(11628)\n        \n    else:\n        temp.append(27839)\n#     print(hf)\n#     i = i+1\n#     if i == 100:\n#         break\n# temp        \ndata['Heating features'] = temp","87439b76":"cf = []\ntemp = []\ni = 0\nfor col in data['Cooling features']:\n    \n    cf = col.split(',')\n    if cf.count('None')== 1:\n        temp.append(31897)\n        \n        \n    elif cf.count('Central Air')== 1:\n        temp.append(18571)\n        \n        \n    elif cf.count('Central')== 1:\n        temp.append(16849)\n        \n    else:\n        temp.append(11501)\n#     print(cf)\n#     i = i+1\n#     if i == 100:\n#         break\n# temp        \ndata['Cooling features'] = temp","7bb45c4f":"lf = []\ntemp = []\ni = 0\nfor col in data['Laundry features']:\n    \n    lf = col.split(',')\n    if lf.count('None')== 1:\n        temp.append(20705)\n        \n        \n    elif lf.count('In Garage')== 1:\n        temp.append(6824)\n        \n        \n    elif lf.count('Inside')== 1:\n        temp.append(6613)\n        \n        \n    elif lf.count('Laundry Room')== 1:\n        temp.append(3934)\n        \n        \n    elif (lf.count('Laundry Closet') == 1):\n        temp.append(2254)\n        \n        \n    else:\n        temp.append(38488)\n#     print(lf)\n#     i = i+1\n#     if i == 100:\n#         break\n# temp        \ndata['Laundry features'] = temp","873a7266":"ct = []\ntemp = []\ni = 0\nfor col in data['City']:\n    \n    ct = col.split(',')\n    if ct.count('Los Angeles')== 1:\n        temp.append(11909)\n        \n        \n    elif ct.count('San Jose')== 1:\n        temp.append(7029)\n        \n        \n    elif ct.count('San Francisco')== 1:\n        temp.append(5541)\n        \n        \n    elif ct.count('San Mateo')== 1:\n        temp.append(1125)\n        \n        \n    elif ct.count('San Diego') == 1:\n        temp.append(1107)\n        \n        \n    else:\n        temp.append(52107)\n#     print(ct)\n#     i = i+1\n#     if i == 100:\n#         break\n# temp        \ndata['City'] = temp","cbe4003f":"del col\nnumra = ['Heating','Cooling','Region','Type','Flooring','Heating features','Cooling features','Laundry features','City']\nfor col in numra:\n    data[col] = (data[col]) \/ (data.shape[0])","5bdf8905":"del col\nnumnorm = ['Year built','Last Sold Price','Total interior livable area' ,'Annual tax amount' ,'Zip','Listed Price' ,'Tax assessed value','Bedrooms','Bathrooms','Full bathrooms','Total spaces','Garage spaces','Elementary School Score','Elementary School Distance','Middle School Score','Middle School Distance','High School Score','High School Distance',]\nfor col in numnorm:\n    data[col] = (data[col] - data[col].mean()) \/ (data[col].std())\n","abc81d36":"data['Last Sold Price'].fillna(0,inplace=True)\ndata.isnull().sum()","7345e963":"delcol = ['Address','Summary','Parking','Lot','Appliances included','High School',\n          'Parking features','Listed On','Last Sold On','State','Elementary School',\n          'Middle School','Cooling','Elementary School Distance','Total spaces','Heating features',\n         'Garage spaces','Heating','Flooring','Laundry features',\n         'Region','City','Cooling features']\ndata1 = data.drop(delcol,axis =1 )\ndata1.head()","ad96377b":"data1.drop('Last Sold Price',axis = 1)","249fd10d":"n_train = data_0.shape[0]\ntrain_features = data1[:n_train]\ntest_features = data1[n_train:]\ntrain_labels = train_y\nprint(train_features.shape)\nprint(test_features.shape)","d6ecd3a5":"# n_train = data_0.shape[0]\n# train_features = torch.tensor(data1[:n_train].values,\n#                               dtype=torch.float32)\n# test_features = torch.tensor(data1[n_train:].values,\n#                              dtype=torch.float32)\n# train_labels = torch.tensor(train_y.values.reshape(-1, 1),\n#                             dtype=torch.float32)\n# print(train_features.shape)\n# print(test_features.shape)\n# data1.isnull().sum()","f00e80ae":"import xgboost as xgb\nimport pandas as pd\nimport numpy as np\nfrom xgboost.sklearn import XGBClassifier,XGBRegressor\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import plot_importance\nimport matplotlib.pyplot as plt","02b0f3e8":"train_data,test_data,train_label,test_label = train_test_split(train_features,train_labels,test_size=0.25)\n","e8987d83":"def xgboost_train(train_data,test_data,train_label,test_label):\n    \n    model = xgb.XGBRegressor(\n      max_depth = 3,\n      booster = \"gbtree\",\n      min_child_weight = 1, #\u6700\u5c0f\u53f6\u5b50\u8282\u70b9\u6743\u91cd\u548c\n      learning_rate = 0.5,\n      n_estimators = 100, #\u8fed\u4ee3\u6b21\u6570\n      silent = 1,\n      subsmaple = 0.5,\n      objective = 'reg:linear', #\u7ebf\u6027\u56de\u5f52\n      gamma = 0, \n      max_delta_step = 0,\n      reg_alpha = 0,\n      reg_lambda = 0,\n      seed = 1,\n#       tree_method = 'gpu_hist',\n      early_stopping_rounds=20,\n      \n      \n  )\n    eval_set = [(train_data, train_label), (test_data, test_label)]\n    model.fit(train_data, train_label, eval_metric = 'rmse', verbose = True, eval_set = eval_set,\n            early_stopping_rounds = 10) #eval_set:\u8fdb\u884c\u6d4b\u8bd5\u7684\u6570\u636e\u96c6\n  \n    y_pre = model.predict(test_data)\n\n    print('rmse: %f'%metrics.mean_squared_error(test_label,y_pre))\n    results = model.evals_result()\n    epochs = len(results['validation_0']['rmse'])\n    x_axis = range(0, epochs)\n# plot rmse\n    fig, ax = plt.subplots()\n    ax.plot(x_axis, results['validation_0']['rmse'], label='Train')\n    ax.plot(x_axis, results['validation_1']['rmse'], label='Test')\n    ax.legend()\n    plt.ylabel('rmse')\n    plt.title('XGBoost rmse')\n    plt.show()\n  \n\n    return model","3b8a1155":"model = xgboost_train(train_data,test_data,train_label,test_label)","34ee910a":"xgb.plot_importance(model,height=1.5)\nplt.show()\nimportance = model.feature_importances_\n","c11fbf17":"def n_estimators():\n  param_test = {\n#         'n_estimators':[100,150,200,250,300,400,500],\n      'max_depth':range(3,10,1),     #5\n      'subsmaple':[i\/10 for i in range(6,11)], #0.6\n#       'learning_rate':[i\/100 for i in range(1,20)],\n      'learning_rate':[0.01,0.02,0.03,0.05,0.1,0.15,0.2], #0.1\n      'gamma':[i\/10 for i in range(0,5)]  #0.1\n      \n  }\n  model = xgb.XGBRegressor(\n      max_depth = 5,\n      booster = \"gbtree\",\n      min_child_weight = 1, #\u6700\u5c0f\u53f6\u5b50\u8282\u70b9\u6743\u91cd\u548c\uff0c\u503c\u8fc7\u9ad8\u6b20\u62df\u5408\uff1b\u5982\u679c\u5728\u4e00\u6b21\u5206\u88c2\u4e2d\uff0c\u53f6\u5b50\u8282\u70b9\u4e0a\u6240\u6709\u6837\u672c\u7684\u6743\u91cd\u548c\u5c0f\u4e8emin_child_weight\u5247\u505c\u6b62\u5206\u88c2\uff0c\u80fd\u591f\u6709\u6548\u7684\u9632\u6b62\u8fc7\u62df\u5408\n      learning_rate = 0.1,\n      n_estimators = 100, #\u8fed\u4ee3\u6b21\u6570\n      silent = 1,\n      subsmaple = 0.6,\n      objective = 'reg:linear', #\u7ebf\u6027\u56de\u5f52\n      gamma = 0.1, \n      max_delta_step = 0,\n      reg_alpha = 0,\n      reg_lambda = 0,\n      seed = 1,\n      tree_method = 'gpu_hist',\n      early_stopping_rounds=20,\n  )\n\n  gsearch = GridSearchCV(estimator=model, param_grid=param_test,scoring = 'neg_mean_squared_error',n_jobs = -1, cv=5)\n  gsearch.fit(train_data,train_label)\n  print('\\n\u6a21\u578b\u6700\u4f73\u53d6\u503c:\\n', gsearch.best_params_, '\\n\u6700\u4f73\u6a21\u578b\u5f97\u5206:\\n', gsearch.best_score_)\n\n  return gsearch.best_params_\n","d21e030e":"n_estimators = n_estimators()","5215a314":"preds = model.predict(test_features)\npreds.shape","b76ae3ae":"preds = np.expm1(preds)\npreds","3fb5bf81":"# sub_data = pd.read_csv('..\/input\/california-house-prices\/sample_submission.csv')\n# sub_data['Sold Price'] = preds    \n# submission = sub_data\n# submission.to_csv('submission.csv', index=False)","47998b46":"train_labels1 = np.expm1(train_labels)\nn_train = data_0.shape[0]\ntrain_features1 = torch.tensor(data1[:n_train].values,\n                              dtype=torch.float32)\ntest_features1 = torch.tensor(data1[n_train:].values,\n                             dtype=torch.float32)\ntrain_labels1 = torch.tensor(train_labels1.values.reshape(-1, 1),\n                            dtype=torch.float32)\nprint(train_features1.shape)\nprint(test_features1.shape)\ndata1.isnull().sum()","c9453fd8":"loss = nn.MSELoss()\nin_features = train_features1.shape[1] #38\n\ndef get_net():\n    net = nn.Sequential(nn.Linear(in_features,16),\n                       nn.ReLU(),\n                       nn.Linear(16,8),\n                       nn.ReLU(),\n                       nn.Linear(8,1))\n    return net\n","5c5e25f0":"def log_rmse(net,features,labels):\n    clam = torch.clamp(net(features),1,float('inf'))\n    rmse = torch.sqrt(loss(torch.log(clam),torch.log(labels)))\n#     rmse = torch.sqrt(loss(clam,labels))\n    return rmse.item()","42164f15":"def train(net, train_features, train_labels, test_features, test_labels,\n          num_epochs, learning_rate, weight_decay, batch_size):\n    train_ls, test_ls = [], []\n    train_iter = d2l.load_array((train_features, train_labels), batch_size)\n    # \u8fd9\u91cc\u4f7f\u7528\u7684\u662fAdam\u4f18\u5316\u7b97\u6cd5\n    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, eps=1e-08,\n                                 weight_decay=weight_decay)\n    \n    for epoch in tqdm(range(num_epochs)):\n        for X, y in train_iter:\n            optimizer.zero_grad()\n            l = loss(net(X), y)\n            l.backward()\n            optimizer.step()\n        train_ls.append(log_rmse(net, train_features, train_labels))\n        if test_labels is not None:\n            test_ls.append(log_rmse(net, test_features, test_labels))\n        \n            \n    return train_ls, test_ls","10bca61d":"def get_k_fold_data(k, i, X, y):\n    assert k > 1\n    fold_size = X.shape[0] \/\/ k\n    X_train, y_train = None, None\n    for j in range(k):\n        \n        idx = slice(j * fold_size, (j + 1) * fold_size)\n        X_part, y_part = X[idx, :], y[idx]\n        if j == i:\n            X_valid, y_valid = X_part, y_part\n        elif X_train is None:\n            X_train, y_train = X_part, y_part\n        else:\n            X_train = torch.cat([X_train, X_part], 0)\n            y_train = torch.cat([y_train, y_part], 0)\n    return X_train, y_train, X_valid, y_valid\n\ndef k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay,\n           batch_size):\n    train_l_sum, valid_l_sum = 0, 0\n    for i in range(k):\n        data = get_k_fold_data(k, i, X_train, y_train)\n        net = get_net()\n        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n                                   weight_decay, batch_size)\n        train_l_sum += train_ls[-1]\n        valid_l_sum += valid_ls[-1]\n        if i==0:\n            d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls],\n                     xlabel='epoch', ylabel='rmse', xlim=[1, num_epochs],\n                     legend=['train', 'valid'], yscale='log')\n        print(f'fold {i + 1}, train log rmse {float(train_ls[-1]):f}, '\n              f'valid log rmse {float(valid_ls[-1]):f}')\n    return train_l_sum \/ k, valid_l_sum \/ k\n                ","abf49a29":"# k, num_epochs, lr, weight_decay, batch_size = 5, 100, 0.3, 0.001, 64\n# train_l, valid_l = k_fold(k, train_features1, train_labels1, num_epochs, lr,\n#                           weight_decay, batch_size)\n# print(f'{k}-\u6298\u9a8c\u8bc1: \u5e73\u5747\u8bad\u7ec3log rmse: {float(train_l):f}, '\n#       f'\u5e73\u5747\u9a8c\u8bc1log rmse: {float(valid_l):f}')","6319b4a0":"def train_and_pred(train_features, test_feature, train_labels, sub_data,\n                   num_epochs, lr, weight_decay, batch_size):\n    net = get_net()\n    train_ls, _ = train(net, train_features, train_labels, None, None,\n                        num_epochs, lr, weight_decay, batch_size)\n    d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel='epoch',\n             ylabel='log rmse', xlim=[1, num_epochs], yscale='log')\n    print(f'train log rmse {float(train_ls[-1]):f}')\n    # \u5c06\u7f51\u7edc\u5e94\u7528\u4e8e\u6d4b\u8bd5\u96c6\u3002\n    preds = net(test_feature).detach().numpy()\n    preds = pd.Series(preds.reshape(1,-1)[0])\n    # \u5c06\u5176\u91cd\u65b0\u683c\u5f0f\u5316\u4ee5\u5bfc\u51fa\u5230Kaggle\n#     sub_data['Sold Price'] = pd.Series(preds.reshape(1, -1)[0])\n#     submission = sub_data\n#     submission.to_csv('submission.csv', index=False)\n    return preds","f46a25c6":"sub_data = pd.read_csv('..\/input\/california-house-prices\/sample_submission.csv')\n\n\n# preds_1 = train_and_pred(train_features1, test_features1, train_labels1, sub_data,\n#                num_epochs, lr, weight_decay, batch_size)\n","c2e43ade":"# preds = preds*0.6+preds_1*0.4\npreds = preds","4348d231":"sub_data['Sold Price'] = preds    \nsubmission = sub_data\nsubmission.to_csv('submission.csv', index=False)","0e9f6f75":"# !pip install -U \"mxnet_cu101<2.0.0\"\n# !pip install autogluon","5bcd1127":"# # line 1: import libs\n# from autogluon.tabular import TabularPredictor\n# # line 2: train predictor\n# predictor = TabularPredictor(label='Sold Price').fit(train_data='..\/input\/california-house-prices\/train.csv', time_limit=60*60*4, presets='best_quality')\n# # line 3: make predict\n# predictions = predictor.predict('..\/input\/california-house-prices\/test.csv')","f8145bf5":"# pred = pd.read_csv('..\/output\/kaggle\/working\/submission.csv')\n# pred['Sold Price'] = predictions*0.7 + preds*0.3\n# pred.to_csv('submission.csv', index=False)","af457de2":"## \u8bad\u7ec3-xgboost","3b7523a4":"### k\u6298\u4ea4\u53c9\u9a8c\u8bc1","ccc9489f":"## \u8bad\u7ec3-\u57fa\u4e8e\u7ebf\u6027\u6a21\u578b"}}