{"cell_type":{"5d93a006":"code","596f4445":"code","09c5c808":"code","8ec52fce":"code","ccdab104":"code","67a3881d":"code","4f21ed6c":"code","501c4bf5":"markdown","1efe66e2":"markdown","21aa3d7a":"markdown","ee33722e":"markdown"},"source":{"5d93a006":"import torch\nimport torchvision\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom matplotlib import pyplot as plt\nimport os\nimport time\nimport pandas as pd\nimport subprocess\nimport os\nfrom datetime import datetime,timedelta\nfrom datetime import date\nimport urllib.request\nfrom PIL import Image","596f4445":"\nd = 20\nclass VAE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        \n        self.conv1 = nn.Sequential(\n        nn.Conv2d(in_channels=3,out_channels=2,kernel_size=3,padding=0,stride=1),\n            nn.ReLU(),\n        nn.BatchNorm2d(2)\n        )\n        self.fc1 = nn.Sequential(\n            ### Reduce the number of channels to 1 without changing the width and dimensions of the images\n            nn.Linear(26*26*2,128),\n            \n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Linear(128,64),\n            \n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Linear(64,32)\n            \n        )\n \n        self.encoder = nn.Sequential(\n            \n            nn.Linear(32, d ** 2),\n            nn.ReLU(),\n            nn.Linear(d ** 2, d * 2)\n        )\n        \n \n        self.decoder = nn.Sequential(\n            nn.Linear(d, d ** 2),\n            nn.ReLU(),\n            nn.Linear(d ** 2, 32)\n        )\n        self.fc2 = nn.Sequential(\n            nn.Linear(32,64),\n            nn.ReLU(),\n            nn.BatchNorm1d(64),\n            nn.Linear(64,128),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.Linear(128,26*26*2),\n            nn.ReLU(),\n            nn.BatchNorm1d(26*26*2)\n            \n        )\n        self.tconv1 = nn.Sequential(\n            nn.ConvTranspose2d(in_channels=2,out_channels=3,kernel_size=3,stride=1,padding=0),\n            \n            nn.Sigmoid()\n        \n        )\n \n    def reparameterise(self, mu, logvar):\n        if self.training:\n            ## Using log variance to ensure that we get a positive std dev\n            ## Converting to std dev in the real space\n            std = logvar.mul(0.5).exp_()\n            ### Create error term which has the same shape as std dev sampled from a N(0,1) distribution\n            eps = std.data.new(std.size()).normal_()\n            #eps = torch.zeros(std.size())\n            ### Add the mean and the std_dev \n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n \n    def forward(self, x):\n        \n        #fc1_output = self.fc1(x.view(-1, 28*28*3))\n        conv1_output = self.conv1(x)\n        #print(conv1_output.size())\n        fc1_output = self.fc1(conv1_output.view(-1,26*26*2))\n        \n        ### Convert Encoded vector into shape (N,2,d)\n        mu_logvar = self.encoder(fc1_output).view(-1, 2, d)\n        ### First vector for each image is mean of the latent distribution\n        mu = mu_logvar[:, 0, :]\n        ### Second vector for each image is log-variance of the latent distribution\n        logvar = mu_logvar[:, 1, :]\n        ### Create variable Z = mu + error * Std_dev\n        z = self.reparameterise(mu, logvar)\n        ### Get decoder output\n        decoder_output = self.decoder(z)\n        \n        fc2_output = self.fc2(decoder_output)\n        tconv1_output = self.tconv1(fc2_output.view(fc2_output.size(0),2,26,26))\n        ## Resize Decoder Output to Pass it to TransposedConv2d layer to recontruct 3 channeled image\n        #decoder_output = decoder_output.view(decoder_output.size(0),1,28,28) \n        ## Return Reconstructed Output and mean and log-variance\n        return tconv1_output, mu, logvar,z\n    \nmodel = VAE()","09c5c808":"def load_checkpoint(filepath):\n        checkpoint = torch.load(filepath)\n        model = checkpoint['model']\n        model.load_state_dict(checkpoint['state_dict'])\n        for parameter in model.parameters():\n            parameter.requires_grad = False\n\n        model.eval()\n\n        return model\n    \nmodel_inference = load_checkpoint('..\/input\/checkpoint\/checkpoint.pth')","8ec52fce":"import os\nimage_list = []\nfor root, dirs, files in os.walk('..\/input\/fruits\/fruits-360\/Test\/'):\n    for filename in files:\n        image_name = root+'\/'+filename\n        image_list.append(image_name)\n        \n","ccdab104":"import numpy as np\ntransform = transforms.Compose([transforms.Resize((28,28)),transforms.ToTensor()])\n\n#model.eval()\n\nimage_names = []\nimage_embeddings = np.zeros((len(image_list),d))\ncount = 0\nfor i in image_list:\n    image_path = i\n    img = Image.open(image_path)\n    img = transform(img)\n    img = img.view(1,3,28,28)\n    x_hat,mu,sigma,embedding_vector = model_inference(img)\n    image_names.append(i)\n    embedding_vector = mu.detach().numpy()\n    image_embeddings[count,:] = embedding_vector\n    count = count+1\nfrom numpy import array as a\nfrom numpy.linalg.linalg import norm\nfrom numpy import set_printoptions\n\n\n","67a3881d":"M = image_embeddings   # create demo matrix\n\n# dot products of rows against themselves\nDotProducts = M.dot(M.T);       \n\n# kronecker product of row norms\nNormKronecker = a([norm(M, axis=1)]) * a([norm(M, axis=1)]).T; \n\nCosineSimilarity = DotProducts \/ NormKronecker\nimport pandas as pd\ndf_vae = pd.DataFrame(CosineSimilarity)\ndf_vae.index = image_names\ndf_vae.columns = image_names\nimport random\nnum_selected = 50\nlist_of_random_items = random.sample(image_names, num_selected)\nfrom matplotlib import pyplot as plt","4f21ed6c":"for image in list_of_random_items:\n    filtered_df = df_vae[[image]].sort_values(by=image,ascending=False)\n    ### Selected top n \n    n = 15\n    filtered_df = filtered_df.head(n)\n    images_recommended = filtered_df.index\n#     print('Similar Images to :')\n#     print(image)\n#     print('---')\n    count=0\n    plt.figure(figsize=(18, 6))\n\n    for image_reco in images_recommended:\n        plt.suptitle('Similar Images to :'+ image, color='b', fontsize=16)\n\n        im = Image.open(str(image_reco))\n        plt.subplot(3,5,count+1)\n        count= count +1\n        plt.imshow(im)\n        plt.axis('off')\n","501c4bf5":"## Create embedding vectors for Test Images","1efe66e2":"## Choose 50 random images and display 15 images closest to each of them","21aa3d7a":"## Importing the  model file","ee33722e":"## Background\n\nWe shall use the  variational autoencoder model object that was trained and saved here https:\/\/www.kaggle.com\/adhok93\/unsupervised-learning-using-vae. This model will be used on the test data."}}