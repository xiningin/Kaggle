{"cell_type":{"a13fead5":"code","83868d92":"code","ca70c8f3":"code","d42e9de1":"code","db166d20":"code","423c4689":"code","99d25a30":"code","39e565bf":"code","0baaff67":"code","2d597aed":"code","b991bae1":"markdown","ae6cc7cf":"markdown","16b5a523":"markdown","7739380d":"markdown","eaa90f46":"markdown","b395657f":"markdown","44fdee1a":"markdown"},"source":{"a13fead5":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom keras import  backend as K\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score\n\nimport os\nimport random","83868d92":"def standardize(x): \n    return (x-mean_px)\/std_px\n\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest= pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nX_train = (train.iloc[:,1:].values).astype('float32') # all pixel values\ny_train = train.iloc[:,0].values.astype('int32') # only labels i.e targets digits\nX_test = test.values.astype('float32')\nmean_px = X_train.mean().astype(np.float32)\nstd_px = X_train.std().astype(np.float32) \n\ny_train = tf.keras.utils.to_categorical(y_train)\nnum_classes = y_train.shape[1]\n\n# fix random seed for reproducibility\nseed_everything(seed=42)\n\nX_test = X_test.reshape(X_test.shape[0], 28, 28,1)\nX_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n\n# cross validation\nX = X_train\ny = y_train\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=42)\n\nmnist_test = pd.read_csv(\"..\/input\/mnist-in-csv\/mnist_test.csv\")\nmnist_train = pd.read_csv(\"..\/input\/mnist-in-csv\/mnist_train.csv\")\nground_truth = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\n\ncols = test.columns\n\ntest['dataset'] = 'test'\n\ntrain['dataset'] = 'train'\n\ndataset = pd.concat([train.drop('label', axis=1), test]).reset_index()\n\nmnist = pd.concat([mnist_train, mnist_test]).reset_index(drop=True)\nlabels = mnist['label'].values\nmnist.drop('label', axis=1, inplace=True)\nmnist.columns = cols\n\nidx_mnist = mnist.sort_values(by=list(mnist.columns)).index\ndataset_from = dataset.sort_values(by=list(mnist.columns))['dataset'].values\noriginal_idx = dataset.sort_values(by=list(mnist.columns))['index'].values\n\nfor i in range(len(idx_mnist)):\n    if dataset_from[i] == 'test':\n        ground_truth.loc[original_idx[i], 'Label'] = labels[idx_mnist[i]]\n        \ndef get_test_acc(model):\n    predictions = model.predict(X_test, verbose=0)\n    predictions = np.argmax(predictions,axis=1)\n\n    submissions = pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                                \"Label\": predictions})\n    return accuracy_score(ground_truth['Label'].values, submissions['Label'].values)","ca70c8f3":"verbosity = 0\n\ndef get_model():\n    input_1 = tf.keras.layers.Input((28,28,1))\n    x = tf.keras.layers.Lambda(standardize)(input_1)\n    x = tf.keras.layers.Convolution2D(32,(3,3), activation='relu')(x)\n    x = tf.keras.layers.BatchNormalization(axis=1)(x)\n    x = tf.keras.layers.Convolution2D(32,(3,3), activation='relu')(x)\n    x = tf.keras.layers.MaxPooling2D()(x)\n    x = tf.keras.layers.BatchNormalization(axis=1)(x)\n    x = tf.keras.layers.Convolution2D(64,(3,3), activation='relu')(x)\n    x = tf.keras.layers.BatchNormalization(axis=1)(x)\n    x = tf.keras.layers.Convolution2D(64,(3,3), activation='relu')(x)\n    x = tf.keras.layers.MaxPooling2D()(x)\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dense(512, activation='relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    out = tf.keras.layers.Dense(10, activation='softmax')(x)\n    model = tf.keras.Model(inputs=input_1, outputs=out)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n    return model","d42e9de1":"ckp = tf.keras.callbacks.ModelCheckpoint(f'baseline.hdf5', monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=True, mode='max')\nmodel = get_model()\nmodel.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=verbosity, callbacks=[ckp])\n\nmodel.load_weights('baseline.hdf5') # load best weights\nno_pseudo_acc = get_test_acc(model)\nprint(f\"No pseudolabelling accuracy: {format(no_pseudo_acc, '.5g')}\")","db166d20":"model = get_model()\nmodel.load_weights('baseline.hdf5')\n\npseudolabels = model.predict(X_test, verbose=0) # create our pseudolabels\npseudolabels = np.argmax(pseudolabels,axis=1) # convert probabilities into classes\npseudolabels = tf.keras.utils.to_categorical(pseudolabels) \n\nmodel.optimizer.lr = 1e-4 # reduce learning rate since we are finetuning\n\nckp = tf.keras.callbacks.ModelCheckpoint(f'selftrain.hdf5', monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=True, mode='max')\nmodel.fit(X_test, pseudolabels, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=verbosity, callbacks=[ckp])\n\nmodel.load_weights('selftrain.hdf5') # load best weights\nself_train_acc = get_test_acc(model)\nprint(f\"Self training accuracy: {format(self_train_acc, '.5g')}\")","423c4689":"model = get_model()\nmodel.load_weights('baseline.hdf5')\n\npseudolabels = model.predict(X_test, verbose=0) # create our pseudolabels\npseudolabels = np.argmax(pseudolabels,axis=1) # convert probabilities into classes\npseudolabels = tf.keras.utils.to_categorical(pseudolabels) \ny_combined = np.concatenate([pseudolabels, y_train]) # combine our pseudolabels with labelled data\nX_combined = np.concatenate([X_test, X_train]) ","99d25a30":"ckp = tf.keras.callbacks.ModelCheckpoint('simultaneous_train.hdf5', monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=True, mode='max')\n\nmodel = get_model() # reinitialize model\nmodel.fit(X_combined, y_combined, validation_data=(X_val, y_val), epochs=20, batch_size=32, callbacks=[ckp], verbose=verbosity) # train a new model on all data together\n\nmodel.load_weights('simultaneous_train.hdf5') # load best weights\n\nsimultaneous_acc = get_test_acc(model) # get test accuracy\nprint(f\"Simultaneous training accuracy: {format(simultaneous_acc, '.5g')}\")","39e565bf":"model = get_model()\nmodel.load_weights('baseline.hdf5')\n\npseudolabels = model.predict(X_test, verbose=0) # create our pseudolabels\npseudolabels = np.argmax(pseudolabels,axis=1) # convert probabilities into classes\npseudolabels = tf.keras.utils.to_categorical(pseudolabels) ","0baaff67":"ckp = tf.keras.callbacks.ModelCheckpoint('pretrain.hdf5', monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=True, mode='max')\n\nprint(\"Pretrain on pseudolabels\")\nmodel = get_model() # reinitialize model\nmodel.fit(X_test, pseudolabels, validation_data=(X_val, y_val), epochs=15, batch_size=32, callbacks=[ckp], verbose=verbosity) # first train on pseudolabels only\n\nprint(\"Finetune on labelled data\")\nmodel.optimizer.lr = 1e-4 # reduce learning rate since we are finetuning\nmodel.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, callbacks=[ckp], verbose=verbosity) # finetune on labelled data\n\nmodel.load_weights('pretrain.hdf5') # load best weights\npretrain_acc = get_test_acc(model) # get test accuracy\nprint(f\"Pretraining accuracy: {pretrain_acc}\")","2d597aed":"print(f\"No pseudolabelling accuracy: {no_pseudo_acc}\")\nprint(f\"Self training accuracy: {self_train_acc}\")\nprint(f\"Simultaneous training accuracy: {simultaneous_acc}\")\nprint(f\"Pretraining accuracy: {pretrain_acc}\")\n\nprint(\"-\"*30)\n\nprint(f\"Percent difference from no pseudolabelling to self training: {100*(self_train_acc-no_pseudo_acc)\/no_pseudo_acc}%\")\nprint(f\"Percent difference from self training to simultaneous training: {100*(simultaneous_acc-self_train_acc)\/self_train_acc}%\")\nprint(f\"Percent difference from simultaneous training to pretraining: {100*(pretrain_acc-simultaneous_acc)\/simultaneous_acc}%\")\n\nprint(\"-\"*30)\n\nprint(f\"Percent difference from no pseudolabelling to pretraining: {100*(pretrain_acc-no_pseudo_acc)\/no_pseudo_acc}%\" )","b991bae1":"## Pretraining\nFirst, we train on labelled data, then we create pseudolabels. \n\nNext, we initialize a new model and train it on ","ae6cc7cf":"# Simultaneous training\nFirst, we train on the labelled data, then initialize a new model and train with labelled data and pseudolabels simultaneously.","16b5a523":"**Load Train and Test data and cross validation**\n============================","7739380d":"# Pydata Dec. 2020 - Stanley Zheng\n\nThis notebook is intended to explore various pseudolabelling schemes. Validation, model, data isn't hugely important here, so the cells are collapsed. Keep in mind this is a minimal example without much of the techniques discussed, and this dataset is very basic - adding augmentations, stochastic depth, etc. during training would result in better results. This is intended to be a minimal code example.\n\n### Reproducibility\nTo make this as fair a comparison as possible, I have seeded random weights and all pseudolabels are produced from the same set of weights. ","eaa90f46":"# Conclusion\n\nIn my talk, I explained the use cases for various pseudolabelling methods. Even though MNIST is not a particularly complex dataset and it's not very fit for pseudolabelling, we still see an improvement over baseline. MNIST's test set is only about half the size of the train set. ","b395657f":"# No pseudolabelling baseline","44fdee1a":"# Self training\n\nFirst, we train on the labelled data, then produce pseudolabels and finetune on the pseudolabels."}}