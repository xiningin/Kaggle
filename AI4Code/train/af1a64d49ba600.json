{"cell_type":{"c0b403d1":"code","73cc1952":"code","209293e9":"code","b30a8bd3":"code","e62162c7":"code","8a3e97ea":"code","16f36ca5":"code","01f823db":"code","0ebe83ba":"code","0f6d8d7a":"code","53ba52bc":"code","dcb85de5":"code","165b92d1":"code","f78df3ac":"code","c2fc16d2":"code","0290569d":"code","f6ebd533":"markdown","fb990ffa":"markdown","da927760":"markdown","866cfadd":"markdown","cf124726":"markdown","1a7c9302":"markdown","8911272a":"markdown","9c199abd":"markdown","905d402e":"markdown","3bc40fb8":"markdown","592fac97":"markdown","cdcdf7c2":"markdown","e3a7e2ad":"markdown","5ab143ea":"markdown","fe412f5e":"markdown","76dcec90":"markdown"},"source":{"c0b403d1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nimport string\nimport nltk\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nimport re\nfrom wordcloud import WordCloud, STOPWORDS \n\nimport time\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nfrom sklearn.metrics import classification_report\n\nimport keras\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom keras.models import Sequential\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","73cc1952":"df = pd.read_csv('\/kaggle\/input\/news-aggregator-dataset\/uci-news-aggregator.csv')\ndf.head()","209293e9":"print('Feature ',end=' ')\nif(any(df.isnull().any())):\n    print('Missing Data\\n')\n    print(df.isnull().sum())\nelse:\n    print('NO missing data')","b30a8bd3":"df['PUBLISHER'] = df['PUBLISHER'].fillna(df['PUBLISHER'].mode()[0]) # Mode- 'Reuters'\ndf.info()","e62162c7":"print('Data Size {}'.format(df.shape))\nif(any(df.duplicated())==True):\n    print('Duplicate rows found')\n    print('Number of duplicate rows= ',df[df.duplicated()].shape[0])\n    df.drop_duplicates(inplace=True,keep='first')\n    df.reset_index(inplace=True,drop=True)\n    print('Dropping duplicates\\n')\n    print(df.shape)\nelse:\n    print('NO duplicate data')","8a3e97ea":"# (b = business, t = science and technology, e = entertainment, m = health)\n\ndef label_to_name(label):\n    if(label=='e'):\n        return 'entertainment'\n    elif(label=='b'):\n        return 'business'\n    elif(label=='t'):\n        return 'science and technology'\n    else:\n        return 'health'\n    \ndf['CATEGORY'] = df['CATEGORY'].apply(label_to_name)\nprint('Distribution of labels in %\\n')\nprint(df['CATEGORY'].value_counts()\/df.shape[0]*100)\n\nsns.set(font_scale=1.2)\nplt.figure(figsize=(12,6))\nsns.countplot(df['CATEGORY']);","16f36ca5":"df.drop(columns=['ID','URL','PUBLISHER','STORY','HOSTNAME','TIMESTAMP'],inplace=True)\n\n\n# lowercasing\ndf['lower'] = df['TITLE'].str.lower()\n\n\n\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\ndf[\"punc_removed\"] = df[\"lower\"].apply(lambda text: remove_punctuation(text))\n\n\n\nSTOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\ndf[\"stopwords_removed\"] = df[\"punc_removed\"].apply(lambda text: remove_stopwords(text))\n\n\ndf.head()\n\n\n# lemmatizer = WordNetLemmatizer()\n# def lemmatize_words(text):\n#     return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n# df[\"lemmatized_without_stopwords\"] = df[\"punc_removed\"].apply(lambda text: lemmatize_words(text))\n\n# df[\"lemmatized_stopwords\"] = df[\"stopwords_removed\"].apply(lambda text: lemmatize_words(text))\n\n\n\n\n# NO EMOJI IN DATA\n# print(all(df['lemmatized'] == df['removed_emoji'])) # TRUE\n# def remove_emoji(string):\n#     emoji_pattern = re.compile(\"[\"\n#                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n#                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n#                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n#                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n#                            u\"\\U00002702-\\U000027B0\"\n#                            u\"\\U000024C2-\\U0001F251\"\n#                            \"]+\", flags=re.UNICODE)\n#     return emoji_pattern.sub(r'', string)\n# df[\"removed_emoji\"] = df[\"lemmatized\"].apply(lambda text: remove_emoji(text))\n\n\n\n\n# NO EMOTICONS IN DATA\n# print(all(df['lemmatized'] == df['removed_emoticons'])) # TRUE\n# def remove_emoticons(text):\n#     emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n#     return emoticon_pattern.sub(r'', text)\n\n# df[\"removed_emoticons\"] = df[\"lemmatized\"].apply(lambda text: remove_emoticons(text))","01f823db":"comment_words = ' '\nstopwords = set(STOPWORDS) \n  \n\nfor val in df.stopwords_removed[0:10000]:  \n    tokens = val.split()     \n    for words in tokens: \n        comment_words = comment_words + words + ' '\n  \n  \nwordcloud = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","0ebe83ba":"le = LabelEncoder()\ndf['CATEGORY']=le.fit_transform(df['CATEGORY'])\n\ndf.head(3)","0f6d8d7a":"# convert data into vectors\nvectorizer = CountVectorizer()\nx = vectorizer.fit_transform(df['stopwords_removed'])\ny = df['CATEGORY']\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42,stratify=df.CATEGORY)\nprint('Training Data ',x_train.shape,y_train.shape)\nprint('Test Data     ',x_test.shape,y_test.shape)\n\n\nresults = pd.DataFrame(columns=['Model','Accuracy','F1-score'])\n\nmodels_name = ['Logistic Regression','Decision Tree','Multinomial NaiveBayes']\n\nmodel_list = [LogisticRegression(), DecisionTreeClassifier(),MultinomialNB()]\n\nfor idx,model in enumerate(model_list):\n    clf = model.fit(x_train, y_train)\n    predictions = model.predict(x_test)\n    results.loc[idx] = [models_name[idx],accuracy_score(y_test, predictions),f1_score(y_test, predictions, average = 'weighted')]\n\nresults.sort_values(by='Accuracy',inplace=True,ascending=False)\nresults","53ba52bc":"tfidf = TfidfVectorizer()\nx = tfidf.fit_transform(df['stopwords_removed'].values)\ny = df['CATEGORY']\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42,stratify=df.CATEGORY)\nprint('Training Data ',x_train.shape,y_train.shape)\nprint('Test Data     ',x_test.shape,y_test.shape)\n\n\nresults = pd.DataFrame(columns=['Model','Accuracy','F1-score'])\n\nmodels_name = ['Logistic Regression','Decision Tree','Multinomial NaiveBayes']\n\nmodel_list = [LogisticRegression(), DecisionTreeClassifier(),MultinomialNB()]\n\nfor idx,model in enumerate(model_list):\n    clf = model.fit(x_train, y_train)\n    predictions = model.predict(x_test)\n    results.loc[idx] = [models_name[idx],accuracy_score(y_test, predictions),f1_score(y_test, predictions, average = 'weighted')]\n\nresults.sort_values(by='Accuracy',inplace=True,ascending=False)\nresults","dcb85de5":"labels = to_categorical(df['CATEGORY'], num_classes=4)\n\nn_most_common_words = 10000\nmax_len = 130\ntokenizer = Tokenizer(num_words=n_most_common_words, filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df[\"lower\"].values)\nsequences = tokenizer.texts_to_sequences(df[\"lower\"].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nX = pad_sequences(sequences, maxlen=max_len)\n\nX_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.2, random_state=42,stratify=df.CATEGORY)\n\nepochs = 10\nemb_dim = 150\nbatch_size = 256","165b92d1":"print((X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n\nmodel = Sequential()\nmodel.add(Embedding(n_most_common_words, emb_dim, input_length=X.shape[1]))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.15, recurrent_dropout=0.15))\n\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\nprint(model.summary())\n\nfilepath=\"weights.best.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n\nhistory = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_test,y_test),callbacks=callbacks_list)\n","f78df3ac":"fig1 = plt.figure(figsize=(12,5))\nplt.plot(history.history['loss'],'r',linewidth=3.0)\nplt.plot(history.history['val_loss'],'b',linewidth=3.0)\nplt.legend(['Training loss', 'Validation Loss'],fontsize=18)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.title('Loss Curves',fontsize=16)\n#fig1.savefig('loss.png')\nplt.show()","c2fc16d2":"fig2=plt.figure(figsize=(12,5))\nplt.plot(history.history['acc'],'r',linewidth=3.0)\nplt.plot(history.history['val_acc'],'b',linewidth=3.0)\nplt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Accuracy',fontsize=16)\nplt.title('Accuracy Curves',fontsize=16)\n#fig2.savefig('accuracy.png')\nplt.show()","0290569d":"print('** Results for LSTM Model **\\n')\npredictions = model.predict_classes(X_test)\nprint(\"Accuracy score: \", accuracy_score(y_test.argmax(1), predictions)) # to convert OHE vector back to label\nprint(\"F1 score: \", f1_score(y_test.argmax(1), predictions, average = 'weighted'))","f6ebd533":"# Using LSTM for training Deep Learning model","fb990ffa":"# Importing Libraries","da927760":"# Checking for Missing Data","866cfadd":"# Results using TF-IDF","cf124726":"# Distribution of 'CATEGORY' (Dependent Variable)","1a7c9302":"# Thank You","8911272a":"## Independent Features- [ID,TITLE,URL,PUBLISHER,STORY,HOSTNAME,TIMESTAMP]\n\n# Dependent Feature- 'CATEGORY'\t","9c199abd":"# Dealing with Categorical Data","905d402e":"# Further improvements-\u00b6\n**1. Extensive Hyper-parameter tuning**\n\n**2. Use Pre-trained models (Eg- BERT)**","3bc40fb8":"# CONCLUSION- Logistic Regression has the best Accuracy & F1-score.","592fac97":"# PREPROCESSING\n\n### - Dropping features [ID,URL,PUBLISHER,STORY,HOSTNAME,TIMESTAMP]\n### - Lowercasing text in 'TITLE' column\n### - Removing Punctuation\n### - Removing StopWords\n### - Lemmatizing (since Stemming may create non-existent\/incorrect words)\n### - Removing emojis if any (None were found)\n### - Removing emoticons (None were found)","cdcdf7c2":"# Results using CountVectorizer","e3a7e2ad":"# Missing Data Imputation","5ab143ea":"# WORDCLOUD","fe412f5e":"## Since, we have to classify news into respective categories based on their headlines we will drop other features for building the classifier.","76dcec90":"# Checking for Duplicates"}}