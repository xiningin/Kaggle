{"cell_type":{"61a573e1":"code","d5bf6122":"code","88c233a1":"code","dd62f0b2":"code","78c95983":"code","9375945d":"code","a0a2d3bb":"code","c8c7bb3a":"markdown","16b30772":"markdown","0e9606d3":"markdown","4dd70ff7":"markdown","9bc22ff5":"markdown"},"source":{"61a573e1":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom scipy.stats import multivariate_normal as mvn","d5bf6122":"# Before starting its better to have concept of oops\n# I will be commenting as much as I can for oops concept\n# Start by creating class\n# Refer to Bayes' Theorem, conditional probablity with Gaussian distribution (probablity density function)\nclass GaussianNaiveBayes:\n    \n    # fit methods trains the data\n    def fit(self,X,y,spar=10e-3): # here self is the variable which refers to current object of class \n        number_of_sample,number_of_features = X.shape # returns shape of X which is NxD dimensional\n        # categories contains classes in Y uniquely due to Set\n        self.categories=np.unique(y)\n        \n        # number_of_classes is the local variable\n        number_of_classes=len(self.categories)\n        \n        # Initialising mean, var and priors\n        self.gaussian_mean=np.zeros((number_of_classes,number_of_features),dtype=np.float64)\n        self.gaussian_var=np.zeros((number_of_classes,number_of_features),dtype=np.float64)\n        self.log_prior=np.zeros((number_of_classes),dtype=np.float64)\n        \n        # Calculating mean,var,prior based on categories in Y\n        for classes in self.categories:\n            X_classes=X[classes==y] # grouping into X_classes array according to category in y\n            self.gaussian_mean[classes,:]=X_classes.mean(axis=0) # mean with each row of sample belonging particular column(features)\n            self.gaussian_var[classes,:]=X_classes.var(axis=0)+spar\n            self.log_prior[classes]=np.log(X_classes.shape[0]\/float(number_of_sample)) #number of sample in a class\/ total samples\n            # i have logged prior because in posterior we will be calculation log_pdf in predict\n            \n        \n        \n        \n    \n    \n    # predict method make prediction\n    def predict(self,X):\n        # posterior probablity dimension (number of sample,number of categories)\n        posteriorS=np.zeros((X.shape[0],len(self.categories)))\n        for classes in self.categories: # calculating posterior with log of class_conditional probablity + log prior \n            posteriorS[:,classes]=mvn.logpdf(X,\n                                             mean=self.gaussian_mean[classes,:],\n                                             cov=self.gaussian_var[classes,:]) + self.log_prior[classes]\n        return np.argmax(posteriorS,axis=1)\n        \n    def accuracy(self,y_true,predicted):\n        return np.mean(y_true==predicted)\n        \n            \n        ","88c233a1":"from sklearn import datasets\nX,y=datasets.make_classification(n_samples=3000,n_features=10,n_classes=2,random_state=42)","dd62f0b2":"# Splitting data into train and test set\n# can use sklearn's train-test split but i want to keep it simple\nX_train=X[:2900,:]\nX_test=X[2900:,]\ny_train=y[:2900]\ny_test=y[2900:]","78c95983":"# Training data\nnb=GaussianNaiveBayes()\nnb.fit(X_train,y_train)","9375945d":"# predicting on test-set\npr=nb.predict(X_test)","a0a2d3bb":"# Checking Accuracy\nnb.accuracy(y_test,pr)","c8c7bb3a":"## 3 types of Naive-Bayes Classifier\n### 1. Gaussian Naive Bayes Classifier\n### 2. Multinomial Naive Bayes Classifer\n### 3. Bernoulli Naive Bayes Classifier","16b30772":"## If these Notebook helped. Do give an upvote, that means a lot. Thank you","0e9606d3":"## Naive-Bayes Classifier\n### Why Naive??\n#### Because it thinks all features ar emutually independent of each other.(so, the multiplication)","4dd70ff7":"## 1. Gausssian Naive Bayes Classifier","9bc22ff5":"### I havent done Standardisation because Gaussian Naive Bayes does it internally\n### If you think it, intuitively, it does take mean and variance and apply pdf."}}