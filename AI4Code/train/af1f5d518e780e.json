{"cell_type":{"cfd56688":"code","bfcdd7fd":"code","c3e94ef1":"code","44c6be95":"code","07907d18":"code","5433cc3d":"code","b1fd05bd":"code","4214a973":"code","27077273":"code","9a02fc6d":"markdown","2fe8e9d1":"markdown","63b7e0ae":"markdown","c1341741":"markdown"},"source":{"cfd56688":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bfcdd7fd":"# for cross-validation\nfrom sklearn.model_selection import cross_val_score\n\n# for preprocessing categorical features\nfrom sklearn.preprocessing import OneHotEncoder\n# for preprocessing numerical features\nfrom sklearn.impute import SimpleImputer\n\n# the model I'm gonna use\nfrom xgboost import XGBRegressor\n\n# package with statistical functions, it will be helpfull in searching outliers in our train dataset\nfrom scipy import stats","c3e94ef1":"# very useful function for fast checking differently made train-datasets\n# I recommend you to save it\n# Here it is not as helpful as it be in other cases, but anyway it makes my work faster\ndef score_dataset(X, y, model = XGBRegressor(random_state=0)):\n    score = cross_val_score(model, X, y, scoring = 'neg_mean_squared_error')\n    score = -1* score.mean()\n    score = np.sqrt(score)\n    return score\n\n# also very cool function, use it drop outliers in numerical features\n# the idea here is linked with statistal termin z-score\ndef drop_outliers(col, zscore= 3):\n    return col[np.abs(stats.zscore(col))<zscore]","44c6be95":"# reading our csv's\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\n\n# that cols are almost empty, so they are useless\ncols_to_drop = [ 'Alley', 'PoolQC', 'Fence', 'MiscFeature']\ntrain = train.drop(cols_to_drop, axis=1)\n\n# split into features dataframe and target column\nX = train.drop(['SalePrice'], axis=1)\ny = train.SalePrice\n\n# the code will be much clearer if we defince categorical and numerical columns explicitly \ncat_cols = X.select_dtypes(include='object').columns\nnum_cols = X.select_dtypes(exclude='object').columns","07907d18":"# cat-features\ncat_X = X[cat_cols]\n# num-features\nnum_X = X[num_cols]\n\n# create onehotencoder transformer to process cat-features\n# pay attention to attribute named 'handle_unknown'. It's important if we don't want to get errors with working with test dataset\nonehot_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\ncat_X_prep = onehot_encoder.fit_transform(cat_X)\n\n# filling nans with mean values for every column\nfor col in num_X:\n    num_X[col].fillna(num_X[col].mean(), inplace=True)\n\n# assign refined features to old columns\nfor col in num_X:\n    num_X.loc[:,col] = drop_outliers(num_X[col])\n# outliers now are nans\n# and we just use dropna method\nnum_X = num_X.dropna(axis=0)\n\n# concatenate categoricals and numericals\nX_prep = np.concatenate((num_X, cat_X_prep[num_X.index]), axis=1)\n\n# our dataset have become smaller and we must adapt target-vector to it\ny_prep = y[num_X.index]","5433cc3d":"# just for tracking changes\n# In previous generations of that code the score was around 40'000\n# now it's much smaller :)\nscore_dataset(X_prep,y_prep, XGBRegressor(random_state=0,learning_rate=0.1,n_jobs=-1\n                                             ))","b1fd05bd":"X_test = test.drop(columns=cols_to_drop, axis=1)\n\ncat_X_test= X_test[cat_cols]\nnum_X_test = X_test[num_cols]\n\n# be careful! with test-dataset you should use only 'trasform' method, without fitting\ncat_X_test_prep = onehot_encoder.transform(cat_X_test)\n\n\n\nfor col in num_X_test:\n    num_X_test[col].fillna(num_X_test[col].mean(), inplace=True)\n\n\n\nX_test_prep = np.concatenate((num_X_test, cat_X_test_prep), axis=1)","4214a973":"# learning_rate defines the speed of learning of our model\n# n_jobs is the number of cores of your processor that are used. value '-1' means using all cores\nmy_model = XGBRegressor(learning_rate=0.1, n_jobs=-1)\nmy_model.fit(X_prep,y_prep)\n\n# making prediction\ny_predicted = my_model.predict(X_test_prep)","27077273":"# I concatenate the id's of test dataset and predicted values into single dataframe\nresult = pd.DataFrame({'Id':test.index, 'SalePrice':y_predicted})\n# and then just convert it to csv file\nresult.to_csv('.\/result.csv', index=False)","9a02fc6d":"Prerprocess acordingly the test dataset, but don't drop outliers","2fe8e9d1":"## Hello, my name is George Lolaev and I will be satisfied, if you check my notebook\nI was trying to make it beginner friendly, cause I am also beginner in DataScience.\n\nIf there is anything to make better in my code, please tell me about it","63b7e0ae":"## Let's now work with the test data","c1341741":"**Thanks for watching :)\nNow let's do that by yourself !**"}}