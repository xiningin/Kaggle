{"cell_type":{"50da9062":"code","0ef98222":"code","0a75246a":"code","640bb074":"code","281d1a90":"code","b0333ab8":"code","71e7b6ea":"markdown","deb8291c":"markdown","e33c56bd":"markdown","8f71ca02":"markdown","51c71bc4":"markdown"},"source":{"50da9062":"# POC \n# Le but est d'ajouter une couche d'attention (self-attention) avant l'utilisation d'un model xyz pour \n# l'analyse de sentiments dans un texte (positif\/negatif .... neutre devra etre ajout\u00e9)\n\n# Le code du model vient du site\n# https:\/\/docs.microsoft.com\/fr-fr\/learn\/modules\/analyze-review-sentiment-with-keras\/2-build-and-train-a-neural-network\n\n# La couche d'attention utilisera tf.keras.layers.MultiHeadAttention","0ef98222":"# Chargement du dataset imdb\n# Jeu de donn\u00e9es de la classification des sentiments des critiques de films d\u2019IMDB qui est fourni avec Keras.\n# https:\/\/keras.io\/datasets\/#imdb-movie-reviews-sentiment-classification\nfrom keras.datasets import imdb\ntop_words = 10000\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)\n\n# Limite le texte a 500 mots\nfrom keras.preprocessing import sequence\nmax_review_length = 500\nx_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\nx_test = sequence.pad_sequences(x_test, maxlen=max_review_length)\n","0a75246a":"\n# Creation d'un modele simple\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Flatten\nfrom keras.layers import MultiHeadAttention\n# https:\/\/keras.io\/api\/layers\/attention_layers\/multi_head_attention\/\n#This is an implementation of multi-headed attention as described in the paper \"Attention is all you Need\" (Vaswani et al., 2017). \n#If query, key, value are the same, then this is self-attention.\n#C'est notre cas !!! On passe les vectors d'entr\u00e9e en tant que query, key et value\n#query: Query Tensor of shape (B, T, dim).\n#value: Value Tensor of shape (B, S, dim).\n#key: Optional key Tensor of shape (B, S, dim). If not given, will use value for both key and value, which is the most common case.\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import Input\n\n#Version fonctionnel avec self-attention car pas possible sinon ....\nembedding_vector_length = 32\ninputs = Input(shape=[500])\ninter1 = Embedding(top_words, embedding_vector_length, input_length=max_review_length)(inputs)\ninter2 = MultiHeadAttention(num_heads=2, key_dim=embedding_vector_length)(inter1, inter1) \n# 4x plus long ??\n#inter2 = MultiHeadAttention(num_heads=8, key_dim=embedding_vector_length)(inter1, inter1)\ninter3 = Flatten()(inter2)\ninter4 = Dense(16, activation='relu')(inter3)\ninter5 = Dense(16, activation='relu')(inter4)\ninter6 = Dense(1, activation='sigmoid')(inter5)\nmodel1 = Model(inputs=inputs, outputs=inter6)\nmodel1.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\nprint(model1.summary())\n\n#Model sans self-attention\nembedding_vector_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\nmodel.add(Flatten())\n#erreur car call du layer sans params...doit utiliser version fonctionnel\n#model.add(MultiHeadAttention(num_heads=2, key_dim=2, value_dim=2))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n\n# Entrainement\nprint('Modele sans self-attention')\nhist = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=128)\nprint('Modele avec self-attention')\nhist2 = model1.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=128)","640bb074":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns2\nimport matplotlib.pyplot as plt2\n%matplotlib inline\n\nsns.set()\nacc = hist.history['accuracy']\nval = hist.history['val_accuracy']\nepochs = range(1, len(acc) + 1)\n\nplt.figure(figsize=(10, 10))\n\nplt.plot(epochs, acc, '-', label='Training accuracy')\nplt.plot(epochs, val, ':', label='Validation accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\n\nacc2 = hist2.history['accuracy']\nval2 = hist2.history['val_accuracy']\nepochs2 = range(1, len(acc2) + 1)\n\nplt.plot(epochs2, acc2, '-', label='Training accuracy SELF')\nplt.plot(epochs2, val2, ':', label='Validation accuracy SELF')\nplt.legend(loc='upper left')\nplt.plot()","281d1a90":"import string\nimport numpy as np\n\nword_dict = imdb.get_word_index()\nword_dict = { key:(value + 3) for key, value in word_dict.items() }\nword_dict[''] = 0  # Padding\nword_dict['>'] = 1 # Start\nword_dict['?'] = 2 # Unknown word\nreverse_word_dict = { value:key for key, value in word_dict.items() }\nprint(' '.join(reverse_word_dict[id] for id in x_train[0]))\n\ndef analyze(text):\n    # Prepare the input by removing punctuation characters, converting\n    # characters to lower case, and removing words containing numbers\n    translator = str.maketrans('', '', string.punctuation)\n    text = text.translate(translator)\n    text = text.lower().split(' ')\n    text = [word for word in text if word.isalpha()]\n\n    # Generate an input tensor\n    input = [1]\n    for word in text:\n        if word in word_dict and word_dict[word] < top_words:\n            input.append(word_dict[word])\n        else:\n            input.append(2)\n    padded_input = sequence.pad_sequences([input], maxlen=max_review_length)\n\n    # Invoke the model and return the result\n    result = model.predict(np.array([padded_input][0]))[0][0]\n    return result","b0333ab8":"analyze('Easily the most stellar experience I have ever had.')","71e7b6ea":"# Function de test du modele","deb8291c":"# Visualisation","e33c56bd":"# Test","8f71ca02":"# Chargement des donn\u00e9es","51c71bc4":"# Modele"}}