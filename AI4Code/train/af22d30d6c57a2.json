{"cell_type":{"641f8bdf":"code","07584728":"code","0b04b99e":"code","c20146e4":"code","3b3afde9":"code","870c24a8":"code","36815ab8":"code","e077970f":"code","bdf43293":"code","d8ae5abf":"code","3e841ef1":"code","1a29f4e6":"code","067e2bf2":"code","279247c0":"code","cc6866cf":"code","395b3c4a":"code","888a282e":"code","72ca31e9":"code","6728fa8a":"code","78830b96":"code","02cd12a4":"code","81f067e8":"code","9476b67f":"code","118fb9b1":"code","ebc987eb":"code","2f8fa73b":"code","59ff6c0e":"code","532cd9ea":"code","400e8ff1":"code","4ccd9ae0":"code","ead4bebc":"code","9752ae6c":"code","256b39ef":"code","18c0078b":"code","611b0fc3":"code","fbeeae3d":"code","5e64a64d":"code","8b7a1d75":"code","db4af0f3":"code","cb249b4a":"code","89a7e787":"code","76042f8d":"code","ec8d4971":"code","eb66603e":"code","2c486c77":"code","2f834bfd":"markdown","0d56d7e1":"markdown","11000488":"markdown","cf7a70a1":"markdown","c3dcb4f8":"markdown","c2f65a9c":"markdown","48451637":"markdown","577ed2cd":"markdown","a7c36ce8":"markdown","09fb9ef8":"markdown","98b3953c":"markdown","004aecf7":"markdown","85f4e641":"markdown","14c7c9e9":"markdown","47f28fa1":"markdown","7fd49542":"markdown","723120b3":"markdown","ae2732ea":"markdown","bf9b2daf":"markdown","5cfdd01d":"markdown","41ca0c1e":"markdown","b78413aa":"markdown","833160a7":"markdown","cc8b7661":"markdown","ea847f7a":"markdown","bf2daf7e":"markdown"},"source":{"641f8bdf":"import pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport math\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import kpss\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.api import ExponentialSmoothing\nfrom statsmodels.tsa.api import SimpleExpSmoothing\nfrom statsmodels.tsa.api import Holt\nfrom pandas.plotting import autocorrelation_plot\nfrom pandas.plotting import lag_plot\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom math import sqrt\n\nfrom plotly.subplots import make_subplots\n\nimport warnings\nwarnings.filterwarnings('ignore')","07584728":"df = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ndf","0b04b99e":"# Change format of date feature\ndf['date'] = pd.to_datetime(df.date, format = '%d.%m.%Y')\ndf.head().style.set_properties(subset=['date'], **{\n    'background-color': 'dodgerblue'\n})","c20146e4":"# Missing values\ndf.isnull().sum()","3b3afde9":"plot = df.groupby('date')['item_cnt_day'].sum()\n\nfig = go.Figure()\nfig.add_trace(\n    go.Scatter(x=plot.index, y=plot.values)\n)\n\nfig.update_layout(\n    title='Number of sold products every day',\n    yaxis_title='Number of products',\n    xaxis_title='Month'\n)","870c24a8":"plot = df.groupby('date', as_index=False)['item_cnt_day'].sum()\nresampled_plot = plot[['date', 'item_cnt_day']].resample('7D', on='date').sum().reset_index(drop=False)\n\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=resampled_plot.date, y=resampled_plot.item_cnt_day)\n)\n\nfig.update_layout(\n    title='Number of sold products every week',\n    yaxis_title='Number of products',\n    xaxis_title='Month'\n)","36815ab8":"plot = df.groupby('date_block_num')['item_cnt_day'].sum()\n\nfig = go.Figure()\nfig.add_trace(\n    go.Scatter(x=plot.index, y=plot.values)\n)\n\nfig.update_layout(\n    title='Number of sold products every month',\n    yaxis_title='Number of products',\n    xaxis_title='Month'\n)","e077970f":"df_num_prod = pd.DataFrame(df.groupby('date', as_index=False)['item_cnt_day'].sum()).rename(columns={'item_cnt_day': 'sold_products'})\ndf_downsampled = df_num_prod[[\n    'date',\n    'sold_products'\n]].resample('7D', on='date').sum().reset_index(drop=False)\ndf_downsampled","bdf43293":"fig = go.Figure()\n\nfor i in range(2):\n    fig.add_trace(\n        go.Scatter(x=df_downsampled.date, y=df_downsampled['sold_products'].shift(-i), name='lag ' + str(i))\n    )\n    \nfig.update_xaxes(title_text='Date')\nfig.update_yaxes(title_text='Sold Products')\nfig.update_layout(title='Lag plot')\n    \nfig.show()","d8ae5abf":"# Running this function plots the temperature data (t) on the x-axis\nautocorrelation_plot(df_downsampled['sold_products'])","3e841ef1":"plot_acf(df_downsampled['sold_products'])","1a29f4e6":"plot_pacf(df_downsampled['sold_products'])","067e2bf2":"# Running this function plots the temperature data (t) on the x-axis against the temperature on the previous data \n# (t-1) on the y-axis\nlag_plot(df_downsampled['sold_products'])","279247c0":"# Here, the forecasts of all future values are equal to the average\nrang = 20\nmean = df_downsampled['sold_products'].mean()\nmeans = []\nfor i in range(rang):\n    means.append(mean)\n    \nnew = pd.date_range(df_downsampled.date.iloc[-1], periods=rang, freq='W')\nnew_df = pd.DataFrame({'date': new[1:], 'sold_products': mean})\ncopy_df = df_downsampled.copy()\n\nto_plot = pd.concat([copy_df, new_df])\n\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=to_plot.date, y=to_plot.sold_products, name='basic')\n)\n\nfig.add_trace(\n    go.Scatter(x=new_df.date, y=new_df.sold_products, name='predicted', mode='lines')\n)","cc6866cf":"# For na\u00efve forecasts, we simply set all forecasts to be the value of the last observation\nrang = 20\nmean = df_downsampled['sold_products'].iloc[-1]\nmeans = []\nfor i in range(rang):\n    means.append(mean)\n    \nnew = pd.date_range(df_downsampled.date.iloc[-1], periods=rang, freq='W')\nnew_df = pd.DataFrame({'date': new[1:], 'sold_products': mean})\ncopy_df = df_downsampled.copy()\n\nto_plot = pd.concat([copy_df, new_df])\n\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=to_plot.date, y=to_plot.sold_products, name='basic')\n)\n\nfig.add_trace(\n    go.Scatter(x=new_df.date, y=new_df.sold_products, name='predicted', mode='lines')\n)","395b3c4a":"decomp = seasonal_decompose(df_downsampled.sold_products, freq=10, model='additive', extrapolate_trend='freq')\ndf_downsampled['sold_products_trend'] = decomp.trend\ndf_downsampled['sold_products_seasonal'] = decomp.seasonal\ndf_downsampled['sold_products_residual'] = decomp.resid","888a282e":"fig = make_subplots(cols=1, rows=4, subplot_titles=(\n    'Basic',\n    'Trend',\n    'Seasonality',\n    'Residual'\n))\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products),\n    row=1,\n    col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_trend),\n    row=2,\n    col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_seasonal),\n    row=3,\n    col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_residual),\n    row=4,\n    col=1\n)\n\nfig.update_layout(height=800, title_text='Eecomposition of Sold Products', showlegend=False)\n","72ca31e9":"fig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products, name='basic')\n)\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products.rolling(10).mean(), name='rolling mean')\n)\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products.rolling(10).std(), name='rolling std')\n)","6728fa8a":"def adf_test(timeseries):\n    print('Results of Diskey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n    for key, value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)","78830b96":"def kpss_test(timeseries):\n    print('Results of KPSS Test:')\n    kpsstest = kpss(timeseries, regression='c', nlags='auto')\n    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic', 'p-value', 'Lags Used'])\n    for key, value in kpsstest[3].items():\n        kpss_output['Critical Value (%s)'%key] = value\n    print(kpss_output)","02cd12a4":"adf_test(df_downsampled.sold_products)","81f067e8":"kpss_test(df_downsampled.sold_products)","9476b67f":"# First Order Differencing\nts_diff = np.diff(df_downsampled.sold_products)\ndf_downsampled['sold_products_diff_1'] = np.append([0], ts_diff)\n\n# Second Order Differencing\nts_diff = np.diff(df_downsampled.sold_products_diff_1)\ndf_downsampled['sold_products_diff_2'] = np.append([0], ts_diff)","118fb9b1":"fig = make_subplots(cols=1, rows=2, subplot_titles=(\n    'sold_products_diff_1',\n    'sold_products_diff_2'\n))\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_1, legendgroup='basic', line=dict(color = 'blue'), name='basic'),\n    row=1,\n    col=1\n)\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_1.rolling(10).mean(), legendgroup='mean', line=dict(color = 'orange'), name='rolling mean'),\n    row=1,\n    col=1\n)\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_1.rolling(10).std(), legendgroup='std', line=dict(color = 'red'), name='rolling std'),\n    row=1,\n    col=1\n)\nfig.update_yaxes(title_text='Number of Sold Products', row=1, col=1)\n\n\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_2, legendgroup='basic', line=dict(color = 'blue'), showlegend=False),\n    row=2,\n    col=1\n)\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_2.rolling(10).mean(), legendgroup='mean', line=dict(color = 'orange'), showlegend=False),\n    row=2,\n    col=1\n)\nfig.add_trace(\n    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_2.rolling(10).std(), legendgroup='std', line=dict(color = 'red'), showlegend=False),\n    row=2,\n    col=1\n)\nfig.update_yaxes(title_text='Number of Sold Products', row=2, col=1)\n\nfig.update_layout(showlegend=False)","ebc987eb":"adf_test(df_downsampled.sold_products_diff_1)","2f8fa73b":"kpss_test(df_downsampled.sold_products_diff_1)","59ff6c0e":"plot_acf(df_downsampled.sold_products_diff_1)","532cd9ea":"plot_pacf(df_downsampled.sold_products_diff_1)","400e8ff1":"# Split dataset\nX = df_downsampled.copy()\nX = X.drop(['date'], axis=1)\nX = X.sold_products_diff_1.values\ntrain, test = X[1:len(X)-7], X[len(X)-7:]\n\n# Train autoregression\nmodel = AutoReg(train, lags=2)\nmodel_fit = model.fit()\nprint('Coefficients %s' % model_fit.params)\nprint()\n\n# Make predictions\npredictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n\nfor i in range(len(predictions)):\n    print('predicted=%f, expected=%f' % (predictions[i], test[i]))\n\nprint()\n    \nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n\n# Plot results\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=np.arange(7), y=test, name='test')\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(7), y=predictions, name='predictions')\n)","4ccd9ae0":"model = ARIMA(df_downsampled.set_index('date').sold_products, order=(5, 1, 0))\nmodel_fit = model.fit()\nprint(model_fit.summary())","ead4bebc":"X = df_downsampled.set_index('date').sold_products\nsize = int(len(X) * 0.8)\ntrain, test = X[0:size], X[size:len(X)]\nhistory = [x for x in train]\npredictions = list()\n\n# walk-forward validation\nfor t in range(len(test)):\n    model = ARIMA(history, order=(2,1,0))\n    model_fit = model.fit()\n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)\n    print('predicted=%f, expected=%f' % (yhat, obs))\n\nprint()\n# Evaluate forecast\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n\n# Plot forecast against actual outcomes\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=np.arange(30), y=test, name='test')\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(30), y=predictions, name='predictions')\n)","9752ae6c":"def evaluate_arima_model(X, arima_order):\n    # prepare training dataset\n    train_size = int(len(X) * 0.8)\n    train, test = X[0:train_size], X[train_size:]\n    history = [x for x in train]\n    # make predictions\n    predictions = list()\n    for t in range(len(test)):\n        model = ARIMA(history, order=arima_order)\n        model_fit = model.fit()\n        yhat = model_fit.forecast()[0]\n        predictions.append(yhat)\n        history.append(test[t])\n    # calculate out of sample error\n    error = mean_squared_error(test, predictions)\n    return sqrt(error)\n\n# GriSearchCV for ARIMA\ndef evaluate_models(dataset, p_values, d_values, q_values):\n    dataset = dataset.astype('float32')\n    best_score, best_cfg = float('inf'), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p, d, q)\n                try:\n                    rmse = evaluate_arima_model(dataset, order)\n                    if rmse < best_score:\n                        best_score, best_cfg = rmse, order\n                    print('ARIMA%s RMSE=%.3f' % (order, rmse))\n                except:\n                    continue\n    print('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))","256b39ef":"X = df_downsampled.set_index('date').sold_products\nfit1 = SimpleExpSmoothing(X, initialization_method='heuristic').fit(smoothing_level=0.2, optimized=False)\nfcast1 = fit1.forecast(20)\n\n\nfit2 = SimpleExpSmoothing(X, initialization_method='heuristic').fit(smoothing_level=0.4, optimized=False)\nfcast2 = fit1.forecast(20)\n\nfit3 = SimpleExpSmoothing(X, initialization_method='heuristic').fit(smoothing_level=0.6, optimized=False)\nfcast3 = fit1.forecast(20)","18c0078b":"fig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=X.index, y=X.values, name='test')\n)\n\nfig.add_trace(\n    go.Scatter(x=fit1.fittedvalues.index, y=fit1.fittedvalues.values, legendgroup='0.2', name='SES 0.2', line=dict(color = 'red'))\n)\n\nfig.add_trace(\n    go.Scatter(x=fcast1.index, y=fcast1.values, legendgroup='0.2', showlegend=False, line=dict(color = 'red'))\n)\n\nfig.add_trace(\n    go.Scatter(x=fit2.fittedvalues.index, y=fit2.fittedvalues.values, legendgroup='0.4', name='SES 0.4', line=dict(color = 'yellow'))\n)\n\nfig.add_trace(\n    go.Scatter(x=fcast2.index, y=fcast2.values, legendgroup='0.4', showlegend=False, line=dict(color = 'yellow'))\n)\n\nfig.add_trace(\n    go.Scatter(x=fit3.fittedvalues.index, y=fit3.fittedvalues.values, legendgroup='0.6', name='SES 0.6', line=dict(color='brown'))\n)\n\nfig.add_trace(\n    go.Scatter(x=fcast3.index, y=fcast3.values, legendgroup='0.6', showlegend=False, line=dict(color = 'brown'))\n)","611b0fc3":"X = df_downsampled.set_index('date').sold_products\nsize = int(len(X) * 0.8)\ntrain, test = X[0:size], X[size:len(X)]\nhistory = [x for x in train]\npredictions = list()\n\n# walk-forward validation\nfor t in range(len(test)):\n    model = SimpleExpSmoothing(history, initialization_method='heuristic').fit(smoothing_level=0.2, optimized=False)\n#     model_fit = model.fit()\n    output = model.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)\n    print('predicted=%f, expected=%f' % (yhat, obs))\n\nprint()\n# Evaluate forecast\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n\n# Plot forecast against actual outcomes\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=np.arange(30), y=test, name='test')\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(30), y=predictions, name='predictions')\n)","fbeeae3d":"X = df_downsampled.set_index('date').sold_products\nfit1 = Holt(X, initialization_method='estimated').fit(smoothing_level=0.8, smoothing_trend=0.2, optimized=False)\nfcast1 = fit1.forecast(10)\n\nfit2 = Holt(X, initialization_method='estimated').fit(smoothing_level=0.6, smoothing_trend=0.4, optimized=False)\nfcast2 = fit2.forecast(10)\n\nfit3 = Holt(X, initialization_method='estimated').fit(smoothing_level=0.4, smoothing_trend=0.6, optimized=False)\nfcast3 = fit3.forecast(10)","5e64a64d":"fig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=X.index, y=X.values, name='test')\n)\n\nfig.add_trace(\n    go.Scatter(x=fit1.fittedvalues.index, y=fit1.fittedvalues.values, legendgroup='HES 0.8 0.2', line=dict(color = 'red'), name='HES 0.8 0.2')\n)\nfig.add_trace(\n    go.Scatter(x=fcast1.index, y=fcast1.values, mode='lines', legendgroup='HES 0.8 0.2', line=dict(color='red'), showlegend=False)\n)\n\nfig.add_trace(\n    go.Scatter(x=fit2.fittedvalues.index, y=fit2.fittedvalues.values, legendgroup='HES 0.6 0.4', line=dict(color = 'yellow'), name='HES 0.6 0.4')\n)\nfig.add_trace(\n    go.Scatter(x=fcast2.index, y=fcast2.values, mode='lines', legendgroup='HES 0.6 0.4', line=dict(color='yellow'), showlegend=False)\n)\n\nfig.add_trace(\n    go.Scatter(x=fit3.fittedvalues.index, y=fit3.fittedvalues.values, legendgroup='HES 0.4 0.6', line=dict(color = 'brown'), name='HES 0.4 0.6')\n)\nfig.add_trace(\n    go.Scatter(x=fcast3.index, y=fcast3.values, mode='lines', legendgroup='HES 0.4 0.6', line=dict(color='brown'), showlegend=False)\n)","8b7a1d75":"X = df_downsampled.set_index('date').sold_products\nsize = int(len(X) * 0.8)\ntrain, test = X[0:size], X[size:len(X)]\nhistory = [x for x in train]\npredictions = list()\n\n# walk-forward validation\nfor t in range(len(test)):\n    model = Holt(history, initialization_method='estimated').fit(smoothing_level=0.8, smoothing_trend=0.2, optimized=False)\n    output = model.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)\n    print('predicted=%f, expected=%f' % (yhat, obs))\n\nprint()\n# Evaluate forecast\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n\n# Plot forecast against actual outcomes\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=np.arange(30), y=test, name='test')\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(30), y=predictions, name='predictions')\n)","db4af0f3":"simulated = model_fit.simulate(anchor='end', nsimulations=7, repetitions=100)","cb249b4a":"fig = go.Figure()\n\nfor i in range(len(simulated)):\n    fig.add_trace(\n        go.Scatter(x=np.arange(20), y=simulated[i], line=dict(color = 'gray'), showlegend=False, opacity=0.2)\n    )\n    \nfig.add_trace(\n    go.Scatter(x=np.arange(20), y=test, line=dict(color = 'red'))\n)\nfig.show()","89a7e787":"X = df_downsampled.set_index('date').sold_products\n\nfit1 = ExponentialSmoothing(X, seasonal_periods=12, trend='add', seasonal='add', use_boxcox=True, initialization_method='estimated').fit()\nfcast1 = fit1.forecast(20)","76042f8d":"fig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=X.index, y=X.values, name='data')\n)\n\nfig.add_trace(\n    go.Scatter(x=fit1.fittedvalues.index, y=fit1.fittedvalues.values, legendgroup='add add', line=dict(color = 'red'), name='predicted')\n)\n\nfig.add_trace(\n    go.Scatter(x=fcast1.index, y=fcast1.values, legendgroup='add add', line=dict(color = 'red'), showlegend=False)\n)","ec8d4971":"X = df_downsampled.set_index('date').sold_products\nsize = int(len(X) * 0.8)\ntrain, test = X[0:size], X[size:len(X)]\nhistory = [x for x in train]\npredictions = list()\n\n# walk-forward validation\nfor t in range(len(test)):\n    model = ExponentialSmoothing(history, seasonal_periods=7, trend='mul', seasonal='mul', use_boxcox=True, initialization_method='estimated')\n    model_fit = model.fit()\n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)\n    print('predicted=%f, expected=%f' % (yhat, obs))\n\nprint()\n# Evaluate forecast\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n\n# Plot forecast against actual outcomes\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(x=np.arange(30), y=test, name='test')\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(30), y=predictions, name='predictions')\n)","eb66603e":"simulated = model_fit.simulate(anchor='end', nsimulations=7, repetitions=100)","2c486c77":"fig = go.Figure()\n\nfor i in range(len(simulated)):\n    fig.add_trace(\n        go.Scatter(x=np.arange(20), y=simulated[i], line=dict(color = 'gray'), showlegend=False, opacity=0.2)\n    )\n    \nfig.add_trace(\n    go.Scatter(x=np.arange(20), y=test, line=dict(color = 'red'))\n)\n\nfig.show()","2f834bfd":"ADF test is used to determine the presense of unit root in the series, and hence helps in understanding if the series is stationary or not. The null and alternate hypothesis of this test are:\nNull Hypothesis: The series has a unit root\nAlternate Hypothesis: The series has no unit root","0d56d7e1":"# Autocorrelation","11000488":"We can see that there is no missing values","cf7a70a1":"We gona change 'date' feature to other format. It's good practice beacause our new format will be used by many functions in the future","c3dcb4f8":"KPSS is another test for checking the stationarity of a time series. The null and alternate hypothesis for the KPSS test are opposite that of the ADF test:\nNull Hypothesis: The process is trend stationary\nAlternate Hypothesis: The series has a unit root (series is not stationary)","c2f65a9c":"After differencing we can see that, the KPSS test statistic < critical value, which implies that the series is now stationary.","48451637":"Average Method ","577ed2cd":"There is no necessity to look at the daily data. Considering weekly data seems to be sufficient as well. We gona create new dataframe with number of sold_products ordered by date and we will downsample it","a7c36ce8":"After differencing we can see that, the ADF test statistic < critical value, which implies that the series is now stationary.","09fb9ef8":"If the test statistic is greater than the critical value, we reject the null hypothesis (series is not stationary). If the test statistic is less than the critical value, it fail to reject the null hypothesis (series is stationary). In our case, the test statistic > critical value, which again implies that the series is not stationary.","98b3953c":"#  Simple forecasting methods","004aecf7":"# Decomposition","85f4e641":"# Holt's Winters Seasonal Smoothing","14c7c9e9":"# Simple Exponential Smoothing","47f28fa1":"For white noise series, we expect each autocorrelation to be cose to zero. Of course, they will not be exactly equal to zero as there is some random variation. For a white noise series, we expect 95% of the spikes in the ACF to lie within plus-minus 2\/sqrt(T) where T is the lenght of the time series. In our case autocorrelation coefficients are outside of the range that is why our data are not white noise.","7fd49542":"#  Stationarity","723120b3":"If the test statistic is less than the critical value, we can reject the null hypothesis (aka the series is stationary). When the test statistic is greater than the critical value, we fail to reject the null hypothesis (which means the series is not stationary). In out case, the test statistic > critical value, which implies that the series is not stationary.","ae2732ea":"# Resampling","bf9b2daf":"# Holt's Exponential Smoothing","5cfdd01d":"Na\u00efve method","41ca0c1e":"When data have a trend, the autocorrelations for small lags tend to be large and positive because observations nearby in time are also nearby in size. So the ACF of trended time series tend to have positive values that slowly decrease as the lags increase.\nWhen data are seasonal, the autocorrelations will be larger for the seasonal lags (at multiples of the seasonal frequency) than for other lags.\nWhen data are both trended and seasonal, you see a combination of these effects.\nIn our case we can see strong trend.","b78413aa":"# Autoregression Models","833160a7":"The simplest of the exponentially smoothing methods is naturally called simple exponencial smoothing (SES). This method is suitable for forecasting data with no clear trend or seasonal pattern. Here we gona use it only to show how it is works.","cc8b7661":"# ARIMA Model","ea847f7a":"In both tests we see that our data are not stationary. That is why we will use differencing to make it stationary for later use","bf2daf7e":"Plot time series and check for trends or seasonality"}}