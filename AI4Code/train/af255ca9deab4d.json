{"cell_type":{"707becf2":"code","0d6db20d":"code","41989c9f":"code","8f23f7c7":"code","1a33d867":"code","65fa4e96":"code","e86331cb":"code","5a398f3c":"code","e4fb7b65":"code","0d0edabc":"code","0a98e034":"code","f1c9d6b5":"code","d2bc028d":"code","228d5c7d":"code","8d6cfac5":"code","cb4006e0":"code","4897c688":"code","50a15b8c":"code","c6769878":"code","63035cf9":"code","e0e2ee99":"code","0554593d":"code","40e67f70":"code","ff4cf55f":"code","085afdb1":"code","c7952c53":"code","22feef29":"code","7f4c7456":"code","6922f591":"code","1e5a1548":"code","4585fb56":"code","fd493de0":"code","fe799799":"code","b2920a8f":"code","d9ad64dd":"code","2a2eb72c":"code","c4fdb5f0":"markdown","4bdcb2f4":"markdown","2404a061":"markdown","3c9f9c26":"markdown","fda39d3e":"markdown","f79a2ab0":"markdown","d6fdee84":"markdown","f38cb70e":"markdown","c33d780b":"markdown","d2d5623e":"markdown","ab7cee2e":"markdown","58f6021a":"markdown","b5f3f861":"markdown","599f3b8a":"markdown","d7f9c992":"markdown","662124ca":"markdown","37dd897f":"markdown","7ba5463c":"markdown","f7e9700e":"markdown","429d39cc":"markdown","57b9dc83":"markdown","3e9a1d9e":"markdown","584d151f":"markdown","dd772d19":"markdown","731c1f0d":"markdown","5a9258ac":"markdown","182fcfe7":"markdown","fe1b2055":"markdown"},"source":{"707becf2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, KFold\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)","0d6db20d":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","41989c9f":"train.shape","8f23f7c7":"test.shape","1a33d867":"train.head()","65fa4e96":"#Save the 'Id' column\ntrain_ID = train['PassengerId']\ntest_ID = test['PassengerId']\n\n# Remove 'Id' for analysis\ntrain = train.drop('PassengerId', axis=1)\ntest = test.drop('PassengerId', axis=1)","e86331cb":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","5a398f3c":"train.describe()","e4fb7b65":"train['Sex'].value_counts()","0d0edabc":"train['Embarked'].value_counts()","0a98e034":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.Survived.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['Survived'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","f1c9d6b5":"#display top missing data ratio\ntotal = all_data.isnull().sum().sort_values(ascending=False)\npercent = (all_data.isnull().sum()\/all_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","d2bc028d":"all_data.drop(['Cabin'], axis=1, inplace=True)","228d5c7d":"#box plot Pclass\/Age\nvar = 'SibSp'\ndata = pd.concat([all_data['Age'], all_data[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 12))\nfig = sns.boxplot(x=var, y=\"Age\", data=data)","8d6cfac5":"#box plot Pclass\/Age\nvar = 'Pclass'\ndata = pd.concat([all_data['Age'], all_data[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 12))\nfig = sns.boxplot(x=var, y=\"Age\", data=data)","cb4006e0":"all_data['SibSp'].loc[np.isnan(all_data['Age'])].value_counts()","4897c688":"all_data[\"Age\"] = all_data.loc[all_data[\"SibSp\"]>1].groupby(\"SibSp\")[\"Age\"].transform(\n    lambda x: x.fillna(x.median()))","50a15b8c":"all_data[\"Age\"] = all_data.groupby(\"Pclass\")[\"Age\"].transform(\n    lambda x: x.fillna(x.median()))","c6769878":"all_data[\"Embarked\"].value_counts()","63035cf9":"all_data[\"Embarked\"] = all_data[\"Embarked\"].fillna(\"S\")","e0e2ee99":"all_data[\"Fare\"] = all_data.groupby(\"Pclass\")[\"Fare\"].transform(\n    lambda x: x.fillna(x.median()))","0554593d":"all_data[\"TotalRelatives\"] = all_data['SibSp'] + all_data['Parch']\n\nall_data['IsAlone'] = 1 #initialize to yes\/1 is alone\nall_data['IsAlone'].loc[all_data[\"TotalRelatives\"] > 0] = 0 # now update to no\/0 if family size is greater than 1\n\n#quick and dirty code split title from name: http:\/\/www.pythonforbeginners.com\/dictionary\/python-split\nall_data['Title'] = all_data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n\n#Continuous variable bins; qcut vs cut: https:\/\/stackoverflow.com\/questions\/30211923\/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n#Fare Bins\/Buckets using qcut or frequency bins: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.qcut.html\nall_data['FareBin'] = pd.qcut(all_data['Fare'], 4)\n\n#Age Bins\/Buckets using cut or value bins: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.cut.html\nall_data['AgeBin'] = pd.cut(all_data['Age'].astype(int), 5)","40e67f70":"final_features = pd.get_dummies(all_data).reset_index(drop=True)\nfinal_features.shape","ff4cf55f":"train = final_features[:ntrain]\ntest = final_features[ntrain:]","085afdb1":"#Validation function\nn_folds = 5\n\ndef score_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    score = cross_val_score(model, train.values, y_train, cv = kf)\n    return(\"score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","c7952c53":"score_cv(GaussianNB())","22feef29":"params = {'logisticregression__C' : [0.001,0.01,0.1,1,10,100,1000]}\npipe = make_pipeline(RobustScaler(), LogisticRegression())\ngridsearch_logistic = GridSearchCV (pipe, params, cv=10)\ngridsearch_logistic.fit(train, y_train)\nprint (\"Meilleurs parametres: \", gridsearch_logistic.best_params_)","7f4c7456":"score_cv(gridsearch_logistic.best_estimator_)","6922f591":"params = {'kneighborsclassifier__n_neighbors' : [3,4,5,6,7],\n         'kneighborsclassifier__weights' : ['uniform','distance'],\n         'kneighborsclassifier__algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']}\npipe = make_pipeline(RobustScaler(), KNeighborsClassifier())\ngridsearch_KNC = GridSearchCV (pipe, params, cv=5)\ngridsearch_KNC.fit(train, y_train)\nprint (\"Meilleurs parametres: \", gridsearch_KNC.best_params_)","1e5a1548":"score_cv(gridsearch_KNC.best_estimator_)","4585fb56":"score_cv(XGBClassifier())","fd493de0":"gradient = GradientBoostingClassifier()\ngradient.fit(train, y_train)","fe799799":"score_cv(GradientBoostingClassifier())","b2920a8f":"params = {\n    \"loss\":[\"deviance\"],\n    \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2],\n    \"min_samples_split\": np.linspace(0.1, 0.5, 4),\n    \"min_samples_leaf\": np.linspace(0.1, 0.5, 4),\n    \"max_depth\":[3,5,8],\n    \"max_features\":[\"auto\",\"log2\",\"sqrt\"],\n    \"criterion\": [\"friedman_mse\",  \"mae\"],\n    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n    \"n_estimators\":[100]\n    }\ngridsearch_gradient = RandomizedSearchCV (GradientBoostingClassifier(), params, n_iter = 500, cv=5)\ngridsearch_gradient.fit(train, y_train)\nprint (\"Meilleurs parametres: \", gridsearch_gradient.best_params_)","d9ad64dd":"score_cv(gridsearch_gradient.best_estimator_)","2a2eb72c":"pred = gridsearch_logistic.best_estimator_.predict(test)\nsub = pd.DataFrame()\nsub['PassengerID'] = test_ID\nsub['Survived'] = pred\nsub.to_csv('submission.csv',index=False)","c4fdb5f0":"* **Age**","4bdcb2f4":"It seems that there is no outliers !","2404a061":"## A. Validation method","3c9f9c26":"* **GaussianNB**","fda39d3e":"I fill in the blank by the most common value","f79a2ab0":"## C. Missing data","d6fdee84":"The huge majority of data for this feature is missing and it does not bring a lot of information for our predictions. So, I decide to remove this column.","f38cb70e":"* **LogisticRegression**","c33d780b":"## C. Create submitting file","d2d5623e":"## 1. Import data & useful libraries\n## 2. Analyse and prepare data\n### A. Dataset analyse\n### B. Outliers\n### C. Missing data\n### D. More feature engineering\n### E. Encoding of categorical features\n## 3. Applying classification algorithms\n### A. Validation method\n### B. Comparing models\n### C. Create submitting file","ab7cee2e":"## E. Encoding of categorical features","58f6021a":"## A. Dataset analyse","b5f3f861":"* **Fare**","599f3b8a":"## B. Outliers","d7f9c992":"* **GradientBoostingClassifier**","662124ca":"# 3. Applying classification algorithms","37dd897f":"* **Cabin**","7ba5463c":"## D. More feature engineering","f7e9700e":"* **KNeighborsClassifier**","429d39cc":"Waouh ! It seems that from 2 sibling and spouse, the average age is becoming very lower ! It seems normal, you don't usely travel that much with your whole brothers and sisters when you are adult.\nLet's first fill in the blanks by the median of the given SibSp, for SibSp > 1.\nFor the others, I fill in the blanks by the median of the given Pclass.","57b9dc83":"* **Embarked**","3e9a1d9e":"Ok, so we have 12 features (including the one to predict). We have data about 891 people in the train set and 418 in the test set.","584d151f":"* **XGBClassifier**","dd772d19":"The fare mainly depends on pclass","731c1f0d":"## B. Comparing models","5a9258ac":"# 1. Import data & useful libraries","182fcfe7":"We could replace the 263 missing values by mean or median. But let's check if we can do something a little bit smarter. On the correlation matrix, we can notice that Pclass and SibSp are strongly correlated to the age. Let's explore this idea.","fe1b2055":"# 2. Analyse and prepare data "}}