{"cell_type":{"8191a0be":"code","f8e97098":"code","c77552c2":"code","d5644c9a":"code","6cdc9772":"code","7836f78c":"code","df4ab6aa":"code","4023c4d3":"code","e626cf12":"code","2fa277b6":"code","f00aa674":"markdown","94d21239":"markdown","25d60c6e":"markdown","df8376d0":"markdown","1e4db503":"markdown"},"source":{"8191a0be":"import numpy as np \nimport matplotlib.pyplot as plt \nimport cv2\nimport glob\n\n# This is a bit of magic to make matplotlib figures appear inline in the notebook\n# rather than in a new window.\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (19.0, 17.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n# Some more magic so that the notebook will reload external python modules;\n# see http:\/\/stackoverflow.com\/questions\/1907993\/autoreload-of-modules-in-ipython\n%load_ext autoreload\n%autoreload 2\n","f8e97098":"data_dir = r'\/kaggle\/input\/dataset\/dataset\/'\nclasses = ['broadleaf', 'grass', 'soil', 'soybean'] \n\nnum_file = 1100 \nall_files = [] \nnum_data =num_file*len(classes)\nY = np.zeros(num_data)\n\n\nfor i, cls in enumerate(classes):\n    all_files += [f for f in glob.glob(data_dir+cls+'\/*.tif')][:num_file]\n    Y[i*num_file:(i+1)*num_file] = i # label all classes with int [0.. len(classes)]\n\n    \n# Image dimension\nim_width = 200\nim_height = 200 \nim_channel = 3\ndim = im_width * im_height * im_channel\n\nX = np.ndarray(shape=(num_data, im_width, im_height, im_channel), dtype=np.float64)\n\nfor idx, file in enumerate(all_files):\n    X[idx] = cv2.resize(cv2.imread(file), (im_width, im_height))\n\nX_train = np.empty(shape=(4000,im_width, im_height, im_channel), dtype=np.float64)\nX_val = np.empty(shape=(200,im_width, im_height, im_channel), dtype=np.float64)\nX_test = np.empty(shape=(200,im_width, im_height, im_channel), dtype=np.float64)\n\ny_train = np.empty(4000)\ny_val = np.empty(200)\ny_test = np.empty(200) \n\nfor i, cls in enumerate(classes): \n    X_test[50*i:50*(i+1)] = X[np.where(Y == i)[0][:50]]\n    X_val[50*i:50*(i+1)] = X[np.where(Y == i)[0][50:100]]\n    X_train[1000*i:1000*(i+1)] = X[np.where(Y == i)[0][100:]]\n    \n    y_test[50*i:50*(i+1)] = i\n    y_val[50*i:50*(i+1)] = i\n    y_train[1000*i:1000*(i+1)] = i\n    \ndel Y \ndel X\n\n# Extract features \n#Shuffle training index\ntrain_idxs = np.random.permutation(X_train.shape[0])\ny_train  = y_train[train_idxs].astype(int)\nX_train = X_train[train_idxs]\n\nX_train = np.reshape(X_train, (X_train.shape[0], -1)).astype('float64')\nX_test = np.reshape(X_test, (X_test.shape[0], -1)).astype('float64')\nX_val = np.reshape(X_val, (X_val.shape[0], -1)).astype('float64')\n\nX_tiny = X_train[100:110].astype('float64')\ny_tiny = y_train[100:110].astype(int)\nnum_dev = 500\n\nX_dev = X_train[0:num_dev].astype('float64')\ny_dev = y_train[0:num_dev].astype(int)\nprint(\"X_train shape\", X_train.shape, \"| y_train shape:\", y_train.shape)\nprint(\"X_test shape\", X_test.shape, \"| y_test shape:\", y_test.shape)\nprint(\"X_val shape\", X_val.shape, \"| y_val shape:\", y_val.shape)\nprint(\"X_dev shape\", X_dev.shape, \"| y_dev shape:\", y_dev.shape)\nprint(\"X_tiny shape\", X_tiny.shape, \"| y_tiny shape:\", y_tiny.shape)\n\n#Subtract out the mean image \n#first: compute the mean image\n# mean_image = np.mean(X_train, axis=0) #axis=0. stack horizontally\nmean_image = 128\n#Second subtract the mean image from train and test data \nX_train -= mean_image\nX_val -= mean_image \nX_test -= mean_image\nX_dev -= mean_image\nX_tiny -= mean_image\n\n#Third append the bias dimension using linear algebra trick\n#Not for net\n# X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n# X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n# X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n# X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n# X_tiny = np.hstack([X_tiny, np.ones((X_tiny.shape[0], 1))])\n\nprint('=====STACK BIAS term=====')\nprint(\"X_train shape\", X_train.shape)\nprint(\"X_test shape\", X_test.shape)\nprint(\"X_val shape\", X_val.shape)\nprint(\"X_dev shape\", X_dev.shape)\nprint(\"X_tiny shape\", X_tiny.shape)","c77552c2":"# Visualize some images \n# Make sure that everything when OK\nclasses = ['broadleaf', 'grass', 'soil', 'soybean']\nn_class = len(classes)\nsamples_per_class = 4\n\n\nfor y, cls in enumerate(classes):\n    idxes = np.flatnonzero(y == y_train)\n    idxes = np.random.choice(idxes, samples_per_class, replace = False)\n    for i, idx in enumerate(idxes):\n        plt_idx = i * n_class + y + 1\n        plt.subplot(samples_per_class,n_class, plt_idx)\n        plt.imshow(X_train[idx].reshape(im_width, im_height, im_channel).astype('uint8'))\n        if(i==0): plt.title(cls)\n\nplt.show()","d5644c9a":"class TwoLayerNet():\n    def __init__(self, input_size, hidden_size, output_size, std= 1e-4):\n        '''\n        std: weight initialization term\n        W1: first layer weight, shape(D x H) \n        W2: second layer weight shape(H x C) \n        C: num_classes(output_size) , H: hidden_size, D: data_dim(input_size) \n        '''\n        self.params = {}\n        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n        self.params['b1'] = np.zeros(hidden_size)\n        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n        self.params['b2'] = np.zeros(output_size)\n        \n    def loss(self, X, y = None, reg=0.0):\n        '''\n        reg: regularization strength\n        X: ndarray shape(N x C). N: num of data \n        y: vector of training label\n        '''\n        #DEfine relu activation function \n        relu = lambda x:np.maximum(0,x)\n\n        #unpack\n        W1, b1 = self.params['W1'], self.params['b1']\n        W2, b2 = self.params['W2'], self.params['b2']\n        N, D = X.shape\n\n        #Forward prop\n        layer1 = relu(X.dot(W1) + b1)  #(N,D) x (D,H) = (N,H)\n        scores = layer1.dot(W2) + b2\n\n        #if target is not given then jump out \n        if(y is None): \n            return scores\n\n        #compute the loss \n        ##Normalization trick to prevent overflow when compute exp \n        scores -= scores.max()#stack vertically\n\n        scores = np.exp(scores)\n        scores_sumexp = np.sum(scores, axis=1)#stack vertically\n\n        ##Nomalize all score \n        softmax = scores \/ scores_sumexp.reshape(N,1)  #Shape: (N, C)\n        #total loss of all training. -log of all correct score\n        loss =  (-1.0) * np.sum(np.log(softmax[range(N),y]))\n\n        ##Normalize the loss and add regularization strength \n        loss \/= N \n        loss += reg * np.sum(W1 * W1) \n        loss += reg * np.sum(W2 * W2) \n\n        #Backward pass on the net \n        grads = {}\n\n        correct_class_scores = scores[range(N), y]\n        softmax[range(N), y] = (-1.0) * (scores_sumexp - correct_class_scores)\/scores_sumexp\n        softmax \/= N\n\n\n        #Want to find dW2(dL\/dW2)\n        # Derivation: dL\/dW2 = dL\/dscore * dscore\/dW2(chain rule)\n        #dL\/dscore = softmax since L(score) = softmax(variable)\n        #dscore\/dW2 = relu_(hidden layer output)\n        grads['W2'] = layer1.T.dot(softmax)\n        grads['b2'] = np.sum(softmax, axis=0)#stack horizontally\n        grads['W2'] += reg * 2 * W2\n\n        #dL\/dW1 = dL\/dscore * dscore\/drelu(layler1) * drelu(layer1)\/dW1 \n        #dL\/dW1 = dW1 = softmax * W2 * X \n        hidden = softmax.dot(W2.T)\n\n        #derivative of a max gate\n        #Intuition: in forward pass if neuron didn't fire that mean. the derivative of that neuron \n        # is 0. This might be bad since this will kill gradient. \n        hidden[layer1 == 0] = 0 \n\n        grads['W1'] = X.T.dot(hidden) \n        grads['b1'] = np.sum(hidden, axis=0) #stack horizontally \n        grads['W1'] += reg * 2 * W1\n\n        return loss, grads\n\n    def train(self, X, y, X_val, y_val, \n              learning_rate =1e-3, learning_rate_decay=0.95, \n              reg=5e-6, num_iters=100, \n              batch_size=200, it_verbose = 1, verbose=False):\n        '''\n        Train using SGD \n        Input: \n            X: nd array shape(N x D) \n            y: vector of train label \n            X_val: nd array shape( n_VAL , D) Use as validation set after each epoch \n            y_val: vector of validation label \n        '''\n        N, D = X.shape\n        N_val = X_val.shape[0]\n        iteration_per_epoch = max(N\/batch_size, 1)\n        \n        loss_hist = []\n        train_acc_hist = []\n        val_acc_hist = []\n        \n        for it in range(num_iters):\n            sampling = np.random.choice(np.arange(N), batch_size, replace=False) \n            X_batch = X[sampling]\n            y_batch = y[sampling]\n            \n            #compute loss and gradients\n            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n            loss_hist.append(loss) \n            \n            #Update rule \n            self.params['W1'] += (-1.0) * learning_rate * grads['W1']\n            self.params['b1'] += (-1.0) * learning_rate * grads['b1']\n            self.params['W2'] += (-1.0) * learning_rate * grads['W2']\n            self.params['b2'] += (-1.0) * learning_rate * grads['b2']\n            \n            if(verbose and it%it_verbose==0):\n                print('iteration: %d \/ %d | Loss: %f' % (it, num_iters, loss)) \n            # Every epoch, check train and val accuracy and decay learning rate.\n            if (it % iteration_per_epoch == 0):\n                # Check accuracy\n                train_acc = (self.predict(X_batch) == y_batch).mean()\n                val_acc = (self.predict(X_val) == y_val).mean()\n                train_acc_hist.append(train_acc)\n                val_acc_hist.append(val_acc)\n\n                # Decay learning rate\n                learning_rate *= learning_rate_decay\n        return {\n            'loss_hist':loss_hist,\n            'train_acc_hist':train_acc_hist,\n            'val_acc_hist':val_acc_hist\n        }\n\n    def predict(self, X):\n        \"\"\"\n        Use the trained weights of this two-layer network to predict labels for\n        data points. For each data point we predict scores for each of the C\n        classes, and assign each data point to the class with the highest score.\n\n        Inputs:\n        - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n        classify.\n\n        Returns:\n        - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n        the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n        to have class c, where 0 <= c < C.\n        \"\"\"\n        y_pred = None\n        relu = lambda x:np.maximum(0,x)\n        # Unpack variables from the params dictionary\n        W1, b1 = self.params['W1'], self.params['b1']\n        W2, b2 = self.params['W2'], self.params['b2']\n\n        #Forward propagation though the network \n        layer1 = relu(X.dot(W1) + b1)\n        scores = layer1.dot(W2) + b2 #shape: (N x C)\n        y_pred = np.argmax(scores, axis=1)\n\n        return y_pred","6cdc9772":"input_size = im_width * im_height * im_channel\nhidden_size = 200\noutput_size = n_class \nstd = 1e-3 # size initialization parameter\n\nnet = TwoLayerNet(input_size, hidden_size,output_size,std )\nstats = net.train(X_dev, y_dev, X_val, y_val, \n              learning_rate =1e-5, learning_rate_decay=0.95, \n              reg=0.0, num_iters=70, \n              batch_size=100, it_verbose = 10,verbose=True)","7836f78c":"# plot loss history and train\/ validation accuracies history\nplt.rcParams['figure.figsize'] = (15.0, 15.0) # set default size of plots\nplt.subplot(2,1,1) \nplt.plot(stats['loss_hist'])\nplt.title('Loss History')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\n\nplt.subplot(2,1,2)\nplt.plot(stats['train_acc_hist'], label='train')\nplt.plot(stats['val_acc_hist'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Classfication Accuracies')\nplt.legend()\nplt.show()","df4ab6aa":"print((net.predict(X_test) == y_test).mean())","4023c4d3":"best_net = None # store the best model into this \nbest_val= -1 #highest validation accuracy\n\n\nhidden_unit = [200]\nlearn_rates = [7.6e-5]\nregularizations = [0.0]\niterations = [600]\n\nresult = {}\nbest_stats = None\n\ninput_size = im_height * im_width * im_channel # \noutput_size = 4 #4 class\nfor hidden in hidden_unit: \n    for learn in learn_rates:\n        for r in regularizations:\n            for iter in iterations:\n                tune_net = TwoLayerNet(input_size,\n                                       hidden_size=hidden,\n                                       output_size=output_size,std=1e-3)\n                stats = tune_net.train(X_train, y_train, X_val, y_val, \n                              num_iters=iter, batch_size=200, \n                              learning_rate=learn,learning_rate_decay=0.94, \n                              reg=r,  it_verbose = 100,verbose=True)\n                train_acc = stats['train_acc_hist'][-1]#get last value \n                val_acc = stats['val_acc_hist'][-1]\n                result[(hidden, learn)] = (train_acc, val_acc)\n                #print log\n                print('hs:',hidden,'learn:',learn,'reg',r,'iter',iter,'train-acc:',train_acc,'val_acc',val_acc)\n                if(val_acc > best_val):\n                    best_val = val_acc\n                    #create best net\n                    best_stats = stats\n                    best_net = tune_net\n                del tune_net\n                del stats\n\nprint(\"Accuracy on Test set\", (best_net.predict(X_test) == y_test).mean())                    ","e626cf12":"# plot loss history and train\/ validation accuracies history\nplt.rcParams['figure.figsize'] = (15.0, 15.0) # set default size of plots\nplt.subplot(2,1,1) \nplt.plot(best_stats['loss_hist'])\nplt.title('Loss History')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\n\nplt.subplot(2,1,2)\nplt.plot(best_stats['train_acc_hist'], label='train')\nplt.plot(best_stats['val_acc_hist'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Classfication Accuracies')\nplt.legend()\nplt.show()","2fa277b6":"plt.hist(best_net.predict(X_test))","f00aa674":"# Data preprocessing ","94d21239":"# Debug The net on dev set \n- Problem that I faced: when init W with `np.random.rand` -> W will be uniformly init with positive small value. But we don't want this. We want to use `randn` so the W init with negative and potive non uniform value","25d60c6e":"## Hyperparameter search\n","df8376d0":"## Check prediction statistic of the model \n- In this case, the model did learn each class uniformly. Predictions are uniformly distributed to all classes. \n- One conclude that the model learn all classes evenly. :D","1e4db503":"## 2 Layer Neural net\n- Stacked RELU activation function."}}