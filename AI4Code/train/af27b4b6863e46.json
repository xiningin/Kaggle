{"cell_type":{"cc996df9":"code","f52f64d5":"code","6deda537":"code","8d05182b":"code","f27fc016":"code","d2eed6ed":"code","13174df4":"code","4ef9ad5a":"code","e60514f8":"code","5c9ed12b":"code","b19ff744":"code","58236834":"code","16c75501":"code","536f726c":"code","ba55c895":"code","042878cf":"code","3050ed76":"code","21329b3d":"code","7fb715d3":"code","f64414a4":"code","4e8cc886":"code","48f0a310":"code","8ba5dda3":"code","ad035d37":"code","cada46b4":"code","ca1fc881":"code","4f0ef589":"code","fb36bc60":"code","159ca6e8":"code","fa8c4314":"code","862904ef":"code","ad0f8130":"code","884277f8":"code","fd123317":"code","e859c4e3":"code","280ecced":"code","f119a8e2":"code","7cca68a2":"code","d69bf112":"code","cb0d6298":"code","2a4d94f1":"code","0709dcee":"code","03617bc2":"code","160ec3ea":"markdown","234c2f5c":"markdown","6c8e1450":"markdown","1b8c3e31":"markdown","30461c88":"markdown","f874ad8a":"markdown","1fa45dec":"markdown","12fb5f2d":"markdown","7a10a979":"markdown","eeae2c90":"markdown","eea1502d":"markdown","1d7b5d6a":"markdown","5e356f0e":"markdown","db9cd917":"markdown","88ab7808":"markdown","d7ab4640":"markdown","e2129400":"markdown","36949996":"markdown","e5ab4563":"markdown","5fe2e843":"markdown"},"source":{"cc996df9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Special imports\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport scipy.cluster.hierarchy as shc\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f52f64d5":"# Reading file from the source as dataset\ndataset=pd.read_csv(\"..\/input\/iris-data\/Iris.csv\")\nIris=dataset.copy()","6deda537":"#checking dataset columns and first few rows\ndataset.head()","8d05182b":"# Descriptive Data Exploration\ndataset.describe()","f27fc016":"# Basic information about dataset\ndataset.info()","d2eed6ed":"# Check shape of data\ndataset.shape","13174df4":"# Checking Null Values in Dataset\ndataset.isnull==True","4ef9ad5a":"# Checking total null values in each given variable\ndataset.isnull().sum()","e60514f8":"# Id Column set as a index for this dataset\ndataset.set_index('Id', inplace=True)\ndata=dataset.copy()","5c9ed12b":"# Label Encoding of Categorical Data Type Variable i.e Species\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ndataset['Species'] = le.fit_transform(dataset['Species'])\nprint(\"\\nFinal Data\")\ndataset.head()","b19ff744":"dataset.tail()","58236834":"# Checking unique values of Species Variable after Label Encoding\ndataset['Species'].unique()","16c75501":"# Box Plot of Columns\n#for i in cols:\n    #dataset.boxplot([i])\n    #plt.show()","536f726c":"# Discriptive analysis of Numeric Veriable available in dataset  \ndataset.describe()","ba55c895":"# BoxPlot Visualization of gven data for Outlier Analysis\ndataset.boxplot()","042878cf":"# To remove outliers from 'sepal width (cm)'\nq1 = dataset['SepalWidthCm'].quantile(0.25)\nq3 = dataset['SepalWidthCm'].quantile(0.75)\niqr = q3 - q1\ndataset = dataset[(dataset['SepalWidthCm'] >= q1-1.5*iqr) & (dataset['SepalWidthCm'] <= q3+1.5*iqr)]\ndataset.shape # To find out the number of rows and column after outlier treatment","3050ed76":"# Boxplot for sepal width (cm) after outlier treatment\nsns.boxplot(y=dataset['SepalWidthCm'])\nplt.show()","21329b3d":"dataset.shape","7fb715d3":"# Dropping Categorical data type variable column from Data dataframe\ndata=data.drop(['Species'],axis=1)","f64414a4":"cols=data.columns\nfor i in cols:\n    sns.boxplot(x=dataset['Species'],y=(i), data=data)\n    plt.show()","4e8cc886":"dataset.corr()","48f0a310":"corr=dataset.corr()\nsns.heatmap(corr,annot=True, square=True, cmap='Blues')","8ba5dda3":"dataset.hist()","ad035d37":"sns.pairplot(dataset, hue=\"Species\", height=2.5, palette='bright')","cada46b4":"from sklearn.preprocessing import normalize\ndata_scaled = normalize(data)\ndata_scaled = pd.DataFrame(data_scaled, columns=data.columns)\ndata_scaled.head()","ca1fc881":"data_scaled.tail()","4f0ef589":"data_scaled.boxplot()","fb36bc60":"import scipy.cluster.hierarchy as shc\nplt.figure(figsize=(15, 10))  \nplt.title(\"Dendrograms\")  \ndend = shc.dendrogram(shc.linkage(dataset, method='ward'))","159ca6e8":"plt.figure(figsize=(20, 7))  \nplt.title(\"Dendrograms\")  \ndend = shc.dendrogram(shc.linkage(dataset, method='ward'))\nplt.axhline(y=10, color='b', linestyle='--')","fa8c4314":"from sklearn.cluster import AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')  \ncluster.fit_predict(dataset)","862904ef":"plt.figure(figsize=(10, 7))  \nsns.scatterplot(data=dataset, x=\"PetalLengthCm\", y=\"PetalWidthCm\", c=cluster.labels_, hue=\"Species\")\n","ad0f8130":"plt.figure(figsize=(10, 7))  \nsns.scatterplot(data=dataset, x=\"SepalLengthCm\", y=\"SepalWidthCm\", c=cluster.labels_, hue=\"Species\")","884277f8":"# Creating an empty list\ncost =[]  # cost = wcss (With in Cluster Sum of Squares)\n\ncl_num = 10 # cl_num is the heighest number of clusters we want to use in Elbow Method, It is completelt arbitary.\nfor i in range(1, cl_num):  \n    KM = KMeans(n_clusters = i, max_iter = 500)\n    KM.fit(data)\n     \n    # calculates squared error\n    # for the clustered points\n    cost.append(KM.inertia_)    \ncost","fd123317":"# plot the cost against K values\nplt.plot(range(1, cl_num), cost, color ='g', linewidth ='3')\nplt.xlabel(\"Value of K, No. of Clusters\")\nplt.ylabel(\"WCSS (Cost)\")\nplt.title('The Elbow Map')\nplt.show() # clear the plot\n \n# the point of the elbow is the\n# most optimal value for choosing k","e859c4e3":"kmeans=KMeans(3)\nkmeans.fit(data)","280ecced":"prediction=kmeans.predict(data)\nprediction","f119a8e2":"cluster=data.copy()\ncluster['Predicted Value']=prediction\ncluster","7cca68a2":"# Sepal Length and Width cluster presentation using scatter plot after K Means Clustering ML Model Prediction\nsns.scatterplot(data=cluster, x=\"SepalLengthCm\", y=\"SepalWidthCm\", hue=\"Predicted Value\", palette='muted')\nplt.title('Clustering of Species as per the model (k Means Clustering)')\nplt.show()","d69bf112":"# Sepal Length and Width cluster presentation using scatter plot with real given Iris data \nsns.scatterplot(data=Iris, x=\"SepalLengthCm\", y=\"SepalWidthCm\", hue=\"Species\", palette='muted')\nplt.title('Clustering of Species with original given data')\nplt.show()","cb0d6298":"# Similarty for Petals Length and Width\n# Petal Length and Width cluster presentation using scatter plot after K Means Clustering ML Model Prediction\nsns.scatterplot(data=cluster, x=\"PetalLengthCm\", y=\"PetalWidthCm\", hue=\"Predicted Value\", palette='muted')\nplt.title('Clustering of Species with original given data')\nplt.show()","2a4d94f1":"# Petal Length and Width cluster presentation using scatter plot with real given Iris data \nsns.scatterplot(data=Iris, x=\"PetalLengthCm\", y=\"PetalWidthCm\", hue=\"Species\", palette='muted')\nplt.title('Clustering of Species with original given data')\nplt.show()","0709dcee":"# Pair Plot Comparision between ML (K Means Clustering) Model Prediction and Iris Real data Species Clustering\n\n# Pairplot with ML (K Means Clustering) Model Prediction\nsns.pairplot(cluster, hue='Predicted Value', height=2.5, palette='bright')","03617bc2":"# Pairplot with Iris data Species based clustering\nsns.pairplot(Iris, hue='Species', height=2.5, palette='bright')","160ec3ea":"# Data Preparation for finding optimal No. of Clusters  ","234c2f5c":"### Using Hierarchical Clustering to determine optimal No. of Clusters and Using Elbow Method to Determine optimal value of K (Clusters) in K Means Clustering, I am able to determine Optimal No. Of clusters (K)=3.\n","6c8e1450":"# EDA and Data Visualization","1b8c3e31":"# 2. Determining the optimal value of K (Clusters) in K Means Clustering\n## **Using Elbow Method**","30461c88":"# Prediction of optimal No. of Clusters","f874ad8a":"**K=1 to K=2 (Steep Slope)**<br>\n**K=2 to K=3 (Gentle Slope)**<br>\n**K=3 (Elbow Point)**","1fa45dec":"# 1. Hierarchical Clustering to determine optimal No. of Clusters\n> **Here, we can see that the scale of all the variables is almost similar. Now, we are good to go. Let\u2019s first draw the dendrogram to help us decide the number of clusters for this particular problem:**","12fb5f2d":"![image.png](attachment:6e9c480d-2a97-4c69-98f6-6accf29eb292.png)","7a10a979":"# Data Presentation of K Means Clustering (ML Model) and Original Iris Data Using Scatter Plot","eeae2c90":"###  We can see the values of 0s 1s and 2s in the output since we defined 3 clusters. 0 represents the points that belong to the first cluster, 1 represents points in the second cluster, 3 represents points in the third cluster. Let\u2019s now visualize the two clusters:","eea1502d":"# Conclusion ","1d7b5d6a":"> **From the above boxplot we can say that there are outliers in the column 'sepal width (cm)'**","5e356f0e":"# PairPlot Comparision between ML (K Means Clustering) Model Prediction and Iris Real data Species Clustering","db9cd917":"# Read data from the source","88ab7808":"> **The x-axis contains the samples and y-axis represents the distance between these samples. The vertical line with maximum distance is the blue line and hence we can decide a threshold of 1.0 and cut the dendrogram:**","d7ab4640":"# Task-2 : \"Iris\" Dataset, Prediction of Optimum Number of Clusters and its Visual Presentation\n## **Prediction Using Unsupervised ML**\n## Level Beginner\n\n### **SUNIL SINGH**\n*Data Science & Business Analytics Intern*<br> \n*The Spark Foundation*","e2129400":"> **We have three clusters as this red line cuts three vertical lines of maximum distance in above dendrogram. Let\u2019s now apply hierarchical clustering for 3 clusters:**","36949996":"> **We can see that after outlier treatment the number of rows are reduced to 146 from 150**","e5ab4563":"# Creating a ML Model using K Means Clustering","5fe2e843":"# Exploratory Iris Data Analysis (EDA)"}}