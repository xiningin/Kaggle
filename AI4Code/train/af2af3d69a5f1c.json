{"cell_type":{"360e5124":"code","f1c5aa85":"code","3638427e":"code","eae77f01":"code","6a25d505":"code","aee8f454":"code","6110e327":"code","267fd642":"code","9ef1299d":"code","8c62b139":"code","7afee626":"code","926bdfe7":"code","25316047":"code","a4106239":"code","709a1005":"code","a354b307":"code","96f0b421":"code","d88016d5":"code","24c1bd48":"markdown","432e8542":"markdown","b039ffe0":"markdown","3e33550b":"markdown","db5296ac":"markdown","29fde82e":"markdown","d4ee83d2":"markdown","32a9a353":"markdown","b7d49276":"markdown","4db93049":"markdown","f8f17491":"markdown"},"source":{"360e5124":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport glob\nimport cv2","f1c5aa85":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D, Activation, MaxPool2D, BatchNormalization, Flatten, Dense, Dropout\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import SGD","3638427e":"names = pd.read_csv('..\/input\/stanford-car-dataset-images-in-224x224\/stanford-car-dataset-by-classes-folder-224\/names.csv')\nnames = names.values","eae77f01":"np.random.shuffle(names)","6a25d505":"nr_cars = 10","aee8f454":"idx_to_name = {x : names[x][0] for x in np.arange(nr_cars)}\nname_to_idx = {x:i for i,x in enumerate(idx_to_name.values())}","6110e327":"idx_to_name","267fd642":"train_path = '..\/input\/stanford-car-dataset-images-in-224x224\/stanford-car-dataset-by-classes-folder-224\/car_data\/train\/'\ntest_path = '..\/input\/stanford-car-dataset-images-in-224x224\/stanford-car-dataset-by-classes-folder-224\/car_data\/test\/'","9ef1299d":"def get_data(path):\n    train = []\n    for i, name in enumerate(name_to_idx.keys()):\n        new_path = path + name + \"\/\"\n        [train.append([i, cv2.resize(cv2.imread(img), (244,244), interpolation = cv2.INTER_AREA)]) for img in glob.glob(new_path + \"*.jpg\")]\n    return np.array(train)","8c62b139":"train = get_data(train_path)\ntest = get_data(test_path)","7afee626":"X_train = np.concatenate(train[:,1], axis=0).reshape(len(train), 244, 244, 3)\nX_train = X_train \/ 255.0\nX_train = X_train.astype('float32')\ny_train = train[:,0]\ny_train = np.eye(len(idx_to_name))[list(y_train)]\n\nX_test = np.concatenate(test[:,1], axis=0).reshape(len(test), 244, 244, 3)\nX_test = X_test \/ 255.0\nX_test = X_test.astype('float32')\ny_test = test[:,0]\ny_test = np.eye(len(idx_to_name))[list(y_test)]","926bdfe7":"# Instantiate an empty sequential model\nmodel = Sequential(name=\"Alexnet\")\n# 1st layer (conv + pool + batchnorm)\nmodel.add(Conv2D(filters= 96, kernel_size= (11,11), strides=(4,4), padding='valid', kernel_regularizer=l2(0.0005),\ninput_shape = (227,227,3)))\nmodel.add(Activation('relu'))  #<---- activation function can be added on its own layer or within the Conv2D function\nmodel.add(MaxPool2D(pool_size=(3,3), strides= (2,2), padding='valid'))\nmodel.add(BatchNormalization())\n    \n# 2nd layer (conv + pool + batchnorm)\nmodel.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='same', kernel_regularizer=l2(0.0005)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPool2D(pool_size=(3,3), strides=(2,2), padding='valid'))\nmodel.add(BatchNormalization())\n            \n# layer 3 (conv + batchnorm)      <--- note that the authors did not add a POOL layer here\nmodel.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same', kernel_regularizer=l2(0.0005)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\n        \n# layer 4 (conv + batchnorm)      <--- similar to layer 3\nmodel.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same', kernel_regularizer=l2(0.0005)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\n            \n# layer 5 (conv + batchnorm)  \nmodel.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same', kernel_regularizer=l2(0.0005)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n\n# Flatten the CNN output to feed it with fully connected layers\nmodel.add(Flatten())\n\n# layer 6 (Dense layer + dropout)  \nmodel.add(Dense(units = 4096, activation = 'relu'))\nmodel.add(Dropout(0.5))\n\n# layer 7 (Dense layers) \nmodel.add(Dense(units = 4096, activation = 'relu'))\nmodel.add(Dropout(0.5))\n                           \n# layer 8 (softmax output layer) \nmodel.add(Dense(units = len(y_train[0]), activation = 'softmax'))\n\n# print the model summary\nmodel.summary()","25316047":"# reduce learning rate by 0.1 when the validation error plateaus\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1))\n \n# set the SGD optimizer with lr of 0.01 and momentum of 0.9\noptimizer = SGD(lr = 0.01, momentum = 0.9)\n \n# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])","a4106239":"# train the model\n# call the reduce_lr value using callbacks in the training method\nwith tf.device('\/GPU:0'):\n    history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test),\n    verbose=0, callbacks=[reduce_lr])","709a1005":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","a354b307":"def predict(img):\n    to_predict = np.zeros(shape=X_train.shape)\n    to_predict[0] = img\n    \n    return idx_to_name[np.argmax(model(to_predict)[0])]","96f0b421":"predict(X_train[100])","d88016d5":"plt.imshow(X_train[100])","24c1bd48":"<a id='exploratory'><\/a>\n# Data Exploratory","432e8542":"<a id='split'><\/a>\n# Data split","b039ffe0":"<a id='selection'><\/a>\n# Data Selection\n\nSelect only a limited amount of cars to train on just because Kaggle does not support to train on all data at the same time.","3e33550b":"<img align=left src='https:\/\/dpzbhybb2pdcj.cloudfront.net\/elgendy\/v-8\/Figures\/05-06_img_0050.png'>","db5296ac":"## Table Content\n\n* [Data Exploratory](#exploratory)\n* [Data Selection](#selection)\n* [Data Split](#split)\n* [Implementation](#implementation)\n* [Analyze](#analyze)\n* [Prediction](#prediction)\n* [Reference](#reference)","29fde82e":"<a id='reference'><\/a>\n# Reference\n\nAlexNet kaggle layers design and architecture image:<br>\nhttps:\/\/www.manning.com\/books\/deep-learning-for-vision-systems","d4ee83d2":"# CNN Stanford Cars using Keras Alexnet","32a9a353":"<a id='prediction'><\/a>\n# Prediction","b7d49276":"<a id='implementation'><\/a>\n# Implementation","4db93049":"<a id='analyze'><\/a>\n# Analyze training history","f8f17491":"# Goal\n\nThe goal of this notebook is simply to show the implementation of the AlexNet and train a model to predict certain types of Cars\n\n## Definition\n\nAlexNet is the name of a convolutional neural network (CNN), designed by Alex Krizhevsky, and published with Ilya Sutskever and Krizhevsky's doctoral advisor Geoffrey Hinton.\n\nAlexNet competed in the ImageNet Large Scale Visual Recognition Challenge on September 30, 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up. The original paper's primary result was that the depth of the model was essential for its high performance, which was computationally expensive, but made feasible due to the utilization of graphics processing units (GPUs) during training.\n\nhttps:\/\/en.wikipedia.org\/wiki\/AlexNet"}}