{"cell_type":{"0bd83c04":"code","a1af8c7b":"code","3ae16787":"code","28b50948":"code","4cd65a86":"code","f3b0f1c9":"code","489ad0da":"code","1579ba9f":"code","91389152":"code","1654f092":"code","1355daca":"code","c6fa27c1":"code","a232587e":"code","c56a9ec3":"code","bb383d58":"code","f1cd481d":"code","7a37b52c":"code","08779684":"code","3eb01af3":"code","f02aa1b4":"code","b97d3961":"code","5679d849":"code","877cf0a5":"code","acad3316":"code","6235f89a":"code","5491af28":"code","5d48cd63":"code","439964e0":"code","52b753f7":"code","56c6517a":"code","2cf51991":"code","4699b119":"code","c2eff0bd":"code","993eb63a":"code","62df887d":"code","56df2785":"code","f06eb9d4":"code","38947d1d":"code","c55c1241":"code","90273026":"code","9588403c":"code","56d81511":"code","524d3a11":"code","c9c7e5c3":"code","e8677c69":"code","215be8bf":"code","ca6dd063":"code","73528d13":"code","7a4a7492":"code","78b9e081":"code","7c722a5f":"code","7498ba81":"code","37504768":"code","38a33ed8":"code","f308e6bc":"code","d8e50c29":"code","b2d9b51a":"code","1cb0d865":"code","3668a2b3":"code","b1364198":"code","83dd491a":"code","071ee8ae":"code","d81ac4b5":"code","99cc8a49":"code","f0a75c87":"code","e082a466":"code","dae59f7f":"code","4f0bb13d":"code","9739937e":"markdown","0d0f8e31":"markdown","c9a3bf16":"markdown","a92052fc":"markdown","1c0461c2":"markdown"},"source":{"0bd83c04":"# !pip install featexp","a1af8c7b":"# General use libraries\nimport numpy as np\nimport os\nimport pandas as pd\nimport pickle \nfrom scipy.stats import mode\n\n# Set numpy and pandas options for viewing\npd.options.display.max_rows = 999\npd.options.display.max_columns = 999\npd.set_option('max_colwidth',1000)\npd.options.display.max_info_columns = 999\npd.set_option('expand_frame_repr',True)\n\n# Data Visualization Libaries\nimport seaborn as sns\nimport featexp\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# keras and sklearn for machine learning \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import regularizers\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.layers.advanced_activations import LeakyReLU, PReLU\nfrom keras.utils import np_utils\nfrom keras.callbacks import Callback\nfrom sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV, KFold, learning_curve\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score, balanced_accuracy_score, classification_report\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier","3ae16787":"# import data\ntrain = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/test.csv')\nmeta = pd.read_csv('..\/input\/metadata\/Metadata.csv',index_col='variable', header=0)\nprint(f'Train data:{train.shape}, Test data: {test.shape}, Metadata: {meta.shape}')","28b50948":"# check how data is structured + datatypes\nprint('Train')\nprint(train.info(),'\\n')\nprint('Test')\nprint(test.info())","4cd65a86":"# check for duplicates\nprint(f'Train duplicates: {len(train[train.duplicated()])}\\nTest Duplicates: {len(test[test.duplicated()])}')","f3b0f1c9":"# define function to change labels columns and convert to numeric\ndef chg_lb_num(df, col_ls):\n    '''\n    ====================================\n    df: dataframe\n    col_ls: a list of column names\n\n    >>> df\n        |   a   |  b  |  c  |\n    ---------------------------\n    0   | \"yes\" |  2  |  1  |\n    1   | \"no\"  |  4  |  1  |\n    2   | \"yes\" |  6  |  1  |\n\n    >>> chg_lb_num(df,[a])\n        |  a   |  b  |  c  |\n    ---------------------------    \n    0   |  1   |  2  |  1  |\n    1   |  0   |  4  |  1  |\n    2   |  1   |  6  |  1  |\n    a:\n    [1 0 1]\n    ====================================\n    '''\n    for item in col_ls:\n        df[item] = pd.to_numeric(df[item].replace(['yes','no'],['1','0']), errors='coerce')\n        print(f'{item}:\\n{df[item].unique()}\\n')\n    return df","489ad0da":"label_chg_ls = ['dependency','edjefe','edjefa']\ntrain = chg_lb_num(train, label_chg_ls)\ntest = chg_lb_num(test, label_chg_ls)","1579ba9f":"# show metadata\nprint(meta.info())\nmeta","91389152":"# summarize train data\ntrain.describe()","1654f092":"# check for train data outliers\ntrain.describe().loc['max'][(train.describe().loc['max'] > (train.describe().loc['mean'] + 10 * train.describe().loc['std'])) == True][train.describe().loc['max']>1]","1355daca":"# check for test data outliers\ntest.describe().loc['max'][(test.describe().loc['max'] > (test.describe().loc['mean'] + 10 * test.describe().loc['std'])) == True][test.describe().loc['max']>1]","c6fa27c1":"#outlier in test set which rez_esc is 99.0\ntest.loc[test['rez_esc'] == 99.0 , 'rez_esc'] = 5","a232587e":"# print out columns with null values\nprint(f'Train:\\n{train.isnull().sum()[train.isnull().sum()>0]}\\n')\nprint(f'Test:\\n{test.isnull().sum()[test.isnull().sum()>0]}\\n')\n\n# check if columns are the same \npd.testing.assert_series_equal(train.isnull().any()[train.isnull().any()==True],test.isnull().any()[test.isnull().any()==True])\n\n# put columns with null values in a list\nnull_list = list(train.isnull().any()[train.isnull().any()==True].index)\nprint(f'Columns with Null values: {null_list}\\n')\n\n# show which columns have null values\nprint('Definitions:')\nfor item in null_list:\n    print(f'{item}\\n{meta.loc[item]}\\n')","c56a9ec3":"# create new train and test dataframes, with missing columns encoded\ntrain_n = train\ntest_n = test\n\nname = [item + '_null' for item in null_list[:3]]\ni = 0\nfor item in null_list[:3]:\n    # train data\n    train_n[item].fillna(0,inplace=True)\n    # test data\n    test_n[item].fillna(0,inplace=True)\n    i+=1\n    \npd.concat([train_n.head(), test_n.head()], sort=False)","bb383d58":"# define function for shifting cols to the end train and test \ndef shift_cols_last(df, col_index):\n    '''\n    ====================================\n    df: dataframe\n    col_ls: a list of column names\n\n    >>> df\n        |  a  |  b  |  c  |\n    ---------------------------\n    0   |  0  |  2  |  1  |\n    1   |  0  |  4  |  1  |\n    2   |  0  |  6  |  1  |\n\n    >>> shift_cols_last(df,[1])\n        |  a   |  c  |  b  |\n    ---------------------------    \n    0   |  0   |  1  |  2  |\n    1   |  0   |  1  |  4  |\n    2   |  0   |  1  |  6  |\n    ====================================\n    '''\n    cols = df.columns.tolist()\n    i=0\n    for item in col_index:\n        if i==0:\n            new_cols = cols[:item] + cols[item+1:] + [cols[item]] \n        elif i>=1:\n            new_cols = new_cols[:item] + new_cols[item+1:] + [cols[item]] \n        i+=1\n    return df[new_cols]","f1cd481d":"# show dataframes\npd.concat([train_n.head(), test_n.head()], sort=False)","7a37b52c":"# change datatype for entire dataframe for train and test\ntrain_n = train_n.astype('float64', errors= 'ignore')\ntest_n = test_n.astype('float64', errors= 'ignore')","08779684":"# check how data is structured + datatypes\nprint('Train')\nprint(train_n.info(),'\\n')\nprint('Test')\nprint(test_n.info())","3eb01af3":"# confirming y =x^2 relationship between meaneduc and SQBmeaned\ntrain_n.plot(kind='scatter', x='meaneduc', y='SQBmeaned')\nplt.title('SQBmeaned VS meaneduc')\nplt.show()","f02aa1b4":"# Fill by median values for meaneduc in both train and test set\ntr_median = train_n.meaneduc.median()\nprint(f'Train Median = {tr_median}')\ntrain_n.meaneduc = train_n.meaneduc.fillna(tr_median)\n\ntest_median = test_n.meaneduc.median()\nprint(f'Test Median = {test_median}')\ntest_n.meaneduc = test_n.meaneduc.fillna(test_median)","b97d3961":"# store index of rows with missing SQBmeaned values to list\nsqb_null_trls = train_n[train_n['SQBmeaned'].isnull()].index.tolist()\nsqb_null_tels = test_n[test_n['SQBmeaned'].isnull()].index.tolist()\nprint(f'Train SQBmeaned Null rows\\n{sqb_null_trls}\\nTest SQBmeaned Null rows\\n{sqb_null_tels}')","5679d849":"# Substitute values in SQBmeaned with the squared values of meanduc for train and test\nfor item in sqb_null_trls:\n    train_n.loc[item,'SQBmeaned']=train_n.loc[item,'meaneduc']**2 \nfor item in sqb_null_tels:\n    test_n.loc[item,'SQBmeaned']=test_n.loc[item,'meaneduc']**2 \n\n# show result\npd.concat([train_n.loc[sqb_null_trls, ['Id','meaneduc', 'SQBmeaned', 'Target']],\n           test_n.loc[sqb_null_tels, ['Id','meaneduc', 'SQBmeaned']]], sort=False)","877cf0a5":"# check if there are any remaining nas\n\nprint(f'Train data remaining NAs: {len(train_n[train_n.isnull().any(axis=1)])}')\nprint(f'Test data remaining NAs: {len(test_n[test_n.isnull().any(axis=1)])}')","acad3316":"# prepare for label encoding\npared = []\npiso = []\ntecho = []\nabasta = []\nsanitario = [] \nenergcocinar = []\nelimbasu = []\nepared = []\netecho = []\neviv = []\ngender = []\nestadocivil = []\nparentesco = []\ninstlevel = []\ntipovivi = []\nlugar = [] \narea = []\nelec = []\n\n# append items into corresponding lists\nfor item in train_n.columns:\n    if item.startswith('pared'):\n        # print(item)\n        pared.append(item)\n    elif item.startswith('piso'):\n        # print(item)\n        piso.append(item)\n    elif item.startswith('techo'):\n        # print(item)\n        techo.append(item)\n    elif item.startswith('abasta'):\n        # print(item)\n        abasta.append(item)\n    elif item.startswith('sanitario'):\n        # print(item)\n        sanitario.append(item)\n    elif item.startswith('energcocinar'):\n        # print(item)\n        energcocinar.append(item)\n    elif item.startswith('elimbasu'):\n        # print(item)\n        elimbasu.append(item)\n    elif item.startswith('epared'):\n        # print(item)\n        epared.append(item)\n    elif item.startswith('etecho'):\n        # print(item)\n        etecho.append(item)\n    elif item.startswith('eviv'):\n        # print(item)\n        eviv.append(item)\n    elif item== 'male' or item =='female':\n        # print(item)\n        gender.append(item)\n    elif item.startswith('estadocivil'):\n        # print(item)\n        estadocivil.append(item)\n    elif item.startswith('parentesco'):\n        # print(item)\n        parentesco.append(item)\n    elif item.startswith('instlevel'):\n        # print(item)\n        instlevel.append(item)\n    elif item.startswith('tipovivi'):\n        # print(item)\n        tipovivi.append(item)\n    elif item.startswith('lugar'):\n        # print(item)\n        lugar.append(item)\n    elif item.startswith('area'):\n        # print(item)\n        area.append(item)\n    elif item == 'noelec'or item =='coopele' or item == 'planpri' or item =='public':\n        elec.append(item)","6235f89a":"# store lists into a list for later for loops\nlbl_ls = [pared, piso, techo, abasta, sanitario, energcocinar, elimbasu, epared, etecho, eviv, gender, estadocivil, parentesco, \n          instlevel, tipovivi, lugar, area, elec]\nlbl_strls = ['pared', 'piso', 'techo', 'abasta', 'sanitario', 'energcocinar', 'elimbasu', 'epared', 'etecho', 'eviv', 'gender', 'estadocivil', \n             'parentesco', 'instlevel', 'tipovivi', 'lugar', 'area', 'elec']\nprint(len(lbl_ls), len(lbl_strls))","5491af28":"# reverse one-hot encoding for train and test\ni = 0\nfor item in lbl_ls:\n    trdf = train_n[item]\n    cols = trdf.columns.to_series().values\n    if i==0:\n        lbl_trdf = pd.DataFrame(np.repeat(cols[None, :], len(trdf), 0)[trdf.astype(bool).values], trdf.index[trdf.any(1)], columns = [lbl_strls[i]])\n    else:\n        lbl_trdf = pd.concat([lbl_trdf, pd.DataFrame(np.repeat(cols[None, :], len(trdf), 0)[trdf.astype(bool).values], \n                                                 trdf.index[trdf.any(1)], columns = [lbl_strls[i]])], axis = 1)\n    i+=1\n\nj = 0\nfor item in lbl_ls:\n    tedf = test_n[item]\n    cols = tedf.columns.to_series().values\n    if j==0:\n        lbl_tedf = pd.DataFrame(np.repeat(cols[None, :], len(tedf), 0)[tedf.astype(bool).values], \n                                tedf.index[tedf.any(1)], columns = [lbl_strls[j]])\n    else:\n        lbl_tedf = pd.concat([lbl_tedf, pd.DataFrame(np.repeat(cols[None, :], len(tedf), 0)[tedf.astype(bool).values], \n                                                 tedf.index[tedf.any(1)], columns = [lbl_strls[j]])], axis = 1)\n    j+=1\n\n#show resulting dataframe\nprint(lbl_trdf.shape, lbl_tedf.shape)\npd.concat([lbl_trdf.head(), lbl_tedf.head()], sort=False)","5d48cd63":"# label encode data \nfor i in range(0, len(lbl_strls)):\n    if i%10==0:\n        print(i)\n    tr_ls = lbl_trdf.iloc[:,i].tolist()\n    lbl_trdf.iloc[:,i] = LabelEncoder().fit_transform(tr_ls)\n    te_ls = lbl_tedf.iloc[:,i].tolist()\n    lbl_tedf.iloc[:,i] = LabelEncoder().fit_transform(te_ls)","439964e0":"# show encoded dataframe\npd.concat([lbl_trdf.head(), lbl_tedf.head()], sort=False)","52b753f7":"# concat lbl_df for train and test datasets\ntrain_n = pd.concat([train_n, lbl_trdf], axis = 1)\ntest_n = pd.concat([test_n, lbl_tedf], axis = 1)","56c6517a":"# delete one-hot encoded columns\nfor item in lbl_ls:\n    train_n = train_n.drop(labels=item, axis=1)\n    test_n = test_n.drop(labels=item, axis=1)","2cf51991":"# show new dataframe\npd.concat([train_n.head(), test_n.head()], sort = False)","4699b119":"# find the idhogarcolumn for train and test (they share the same column index in both datasets)\ncol_index = []\nfor c, value in enumerate(train_n.columns):\n    if value == 'Target':\n        col_index.append(c)\nprint(f'Column indexes for Target: {col_index}')","c2eff0bd":"# execute function to shift Target column the last in train\ntrain_n = shift_cols_last(train_n, col_index)\ntrain_n.head()","993eb63a":"# find the idhogar column for train and test (they share the same column indexes in both datasets)\ncol_index = []\nfor c, value in enumerate(train_n.columns):\n    if value == 'idhogar':\n        col_index.append(c)\n\nprint(f'Column index for idhogar: {col_index}')","62df887d":"# define function for shifting cols in front for train and test \ndef shift_cols_front(df, col_index):\n    '''\n    ====================================\n    df: dataframe\n    col_ls: a list of column names\n\n    >>> df\n        |  a  |  b  |  c  |\n    ---------------------------\n    0   |  0  |  2  |  1  |\n    1   |  0  |  4  |  1  |\n    2   |  0  |  6  |  1  |\n\n    >>> shift_cols_front(df,[2])\n        |  c   |  b  |  a  |\n    ---------------------------    \n    0   |  1   |  2  |  0  |\n    1   |  1   |  4  |  0  |\n    2   |  1   |  6  |  0  |\n    ====================================\n    '''\n    cols = df.columns.tolist()\n    i=0\n    for item in col_index:\n        if i==0:\n            new_cols = [cols[0]] + [cols[item]] + cols[1:item] + cols[item+1:]\n        elif item < col_index[i-1]:\n            new_cols = new_cols[0:i+1] + [new_cols[item+1]] + new_cols[1+i:item+1] + new_cols[item+2:]\n        elif item > col_index[i-1]:\n            new_cols = new_cols[0:i+1] + [new_cols[item]] + new_cols[1+i:item] + new_cols[item+1:]\n        i+=1\n    return df[new_cols]","56df2785":"# shift idhogar col in front for train and test\ntrain_n = shift_cols_front(train_n, col_index)\ntest_n = shift_cols_front(test_n, col_index)","f06eb9d4":"# shuffle the data\ndf = train_n.sample(frac=1).reset_index(drop=True)","38947d1d":"# check correlation of features\ntr_corr = df.corr()\n\nsorted_cor = tr_corr['Target'][tr_corr['Target'].isna()==False].sort_values(ascending=False)\nprint(f'Most Positive Correlations:\\n{sorted_cor.head(20)}\\n\\nMost Negative Correlations:\\n{sorted_cor.tail(20)}')","c55c1241":"# plot distribution of Target in train dataset\nstyle.use('seaborn-white')\nsns.distplot(train_n.Target, kde=False, norm_hist = True)\nplt.ylabel('Density')\nplt.title('Distribution of poverty levels in Train Data')\nplt.savefig('Initial Distribution of Target Class.png')\nplt.show()","90273026":"# increase samples where target = 1 - 3 \ntrain_class1_3 = train_n[train_n.Target!=4]\ntrain_n.drop(index = train_class1_3.index, inplace=True)\n\ntrain_class1_3 = pd.concat([train_class1_3, train_class1_3, train_class1_3], sort=False, ignore_index = True)\ntrain_n = pd.concat([train_n, train_class1_3], sort=False, ignore_index = True)\nprint(len(train_n))","9588403c":"# plot distribution of Target in train dataset\nstyle.use('seaborn-white')\nsns.distplot(train_n.Target, kde=False, norm_hist = True)\nplt.ylabel('Density')\nplt.title('Distribution of poverty levels in Train Data')\nplt.show()","56d81511":"# turn X and y into numpy arrays\nX_arr = train_n.iloc[:,2:-1].values\ny = train_n.iloc[:,-1].values\nte_X = test_n.iloc[:,2:].values","524d3a11":"# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nX_arr = scaler.fit_transform(X_arr)\nte_X = scaler.fit_transform(te_X)","c9c7e5c3":"# Check shape of data (X)\ninput_dim = X_arr.shape[1]\nprint(f'Train features\\n{input_dim}')","e8677c69":"assert not np.any(np.isnan(X_arr))","215be8bf":"# define function to plot learning curve of estimators\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    \n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring = 'f1_macro')\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","ca6dd063":"# set up model based on previously tuned features\nmodel3 = LogisticRegression(penalty='l2', solver = 'newton-cg', multi_class='multinomial', verbose = 1)\nscores3 = cross_validate(model3, X_arr, y, cv=10, n_jobs = 4, return_train_score = True, verbose=1, scoring = 'f1_macro')\nmodel3.fit(X_arr,y)","73528d13":"# show scores of model 3\nscores3","7a4a7492":"# Make prediction\npred3 = model3.predict(te_X)","78b9e081":"# store prediction from logistic regression in dataframe\npred3s = pd.DataFrame(pred3, columns = ['Target'])\npred3s = pred3s.assign(Id=test_n.Id)\npred3s = pred3s[['Id','Target']]\npred3s.Target = pd.to_numeric(pred3s.Target, downcast='signed')\npred3s.to_csv('log_reg.csv', header = True, index=False)\npred3s.head()","7c722a5f":"# plot training results\nmodel3_graph = plot_learning_curve(model3, 'Logistic_Regression', X_arr, y, cv=10,\n                        n_jobs=4, train_sizes = np.linspace(0.5, 1.0, 5))\nmodel3_graph.savefig('Log_Reg_train.png')\nmodel3_graph.show()","7498ba81":"# store calculated data into a dictionary\nmodel3_dict = {}\nfor i in range(len(model3.classes_)):\n    model3_dict.update({' Class ' + str(i+1): model3.classes_[i],'Intercept '+ str(i+1): model3.intercept_[i], 'Coefficients '+ str(i+1):model3.coef_[i]})","37504768":"# convert dictionary into dataframe\npd.DataFrame.from_dict(model3_dict, orient = 'Index', columns=['Value'])","38a33ed8":"# show correlation matrix\npd.crosstab(y, model3.predict(X_arr), rownames=['True'], colnames=['Predicted'], margins=True)","f308e6bc":"# show classification report in dataframe\npd.DataFrame.from_dict(classification_report(y, model3.predict(X_arr), output_dict=True), orient='index')","d8e50c29":"model7 = BaggingClassifier()\nscores7 = cross_validate(model7, X_arr, y, cv=10, verbose=1, return_train_score = True, n_jobs=4, scoring = 'f1_macro')\nmodel7.fit(X_arr,y)","b2d9b51a":"scores7","1cb0d865":"pred7 = model7.predict(te_X)","3668a2b3":"pred7s = pd.DataFrame(pred7, columns = ['Target'])\npred7s = pred7s.assign(Id=test_n.Id)\npred7s = pred7s[['Id','Target']]\npred7s.Target = pd.to_numeric(pred7s.Target, downcast='signed')\npred7s.to_csv('bagging.csv', header = True, index=False)\npred7s.head()","b1364198":"model7_graph = plot_learning_curve(model7, 'Bagging', X_arr, y, cv=10,\n                        n_jobs=4, train_sizes = np.linspace(0.5, 1.0, 5))\nmodel7_graph.savefig('Bagging_train.png')\nmodel7_graph.show()","83dd491a":"model10 = GradientBoostingClassifier()\nscores10 = cross_validate(model10, X_arr, y, cv=10, verbose=1, return_train_score = True, n_jobs=4, scoring = 'f1_macro')\nmodel10.fit(X_arr,y)","071ee8ae":"scores10","d81ac4b5":"pred10 = model10.predict(te_X)","99cc8a49":"pred10s = pd.DataFrame(pred10, columns = ['Target'])\npred10s = pred10s.assign(Id=test_n.Id)\npred10s = pred10s[['Id','Target']]\npred10s.Target = pd.to_numeric(pred10s.Target, downcast='signed')\npred10s.to_csv('grad_boosting.csv', header = True, index=False)\npred10s.head()","f0a75c87":"model10_graph = plot_learning_curve(model10, 'Gradient_Boosting', X_arr, y, cv=10,\n                        n_jobs=4, train_sizes = np.linspace(0.5, 1.0, 5))\nmodel10_graph.savefig('GB_train.png')\nmodel10_graph.show()","e082a466":"# attach it to a numpy array\nfinal_pred = np.array([])\nfor i in range(0,len(te_X)):\n    final_pred = np.append(final_pred, mode([pred3[i], pred7[i], pred10[i]])[0])","dae59f7f":"pred = pd.DataFrame(final_pred, columns = ['Target'])\npred = pred.assign(Id=test_n.Id)\npred = pred[['Id','Target']]\npred.Target = pd.to_numeric(pred.Target, downcast='signed')\npred.to_csv('prediction.csv', header = True, index=False)\npred.head()","4f0bb13d":"# show distribution of predictions\nsns.distplot(a=pred.Target, kde=False)\nplt.show()","9739937e":"# Part 4: Training Data","0d0f8e31":"#  Part 1: Initial Data Exploration","c9a3bf16":"# Part 3: Preparing data for training","a92052fc":" **Treating NA values** \n\nThere are five variables with null values in the column: \n1. Monthly Rent Payment\n2. Number of tablets owned by household\n3. Years behind in school\n4. Average years of education for adults (18+)\n5. Square of the mean years of education of adults (>=18) in the household\n\nWe also note that variable 5 is non-linearly dependent variable 5, with the relationship y = x^2. \n\nDue to the high occurence of missing data in the first three variables, we fill them with 0 directly. ","1c0461c2":"# Part 2: Feature engineering"}}