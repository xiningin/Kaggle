{"cell_type":{"5fb2daa3":"code","499dad14":"code","083d5aa3":"code","d2eae167":"code","cc9256d5":"code","5a441a18":"code","8b3efa4c":"code","8d3a9782":"code","f81649df":"code","c4c6381d":"code","c05acf1d":"code","8f4675dd":"code","aaece5b9":"code","517baf03":"code","261b83c3":"code","eaeec70a":"code","8ed5f364":"code","d8a76a6c":"code","9423b3f2":"code","2e18f85a":"code","ab6dceb0":"code","e9e8b768":"code","7f6d529a":"code","a4eddd6d":"code","0b60a4c8":"code","a53e680f":"code","97a02369":"code","211c2354":"code","250b1e64":"code","ea01a793":"code","f960d1fe":"code","7206beb8":"markdown","2d0746a6":"markdown","921b5716":"markdown","ff27bce6":"markdown","df725abb":"markdown","67c04979":"markdown","6078c1bf":"markdown","ee143c0b":"markdown","32f5079c":"markdown","298dda06":"markdown","d0cd6bd5":"markdown","8390ae2e":"markdown"},"source":{"5fb2daa3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","499dad14":"from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n","083d5aa3":"# loading data\ndf = pd.read_csv('\/kaggle\/input\/porto-seguro-safe-driver-prediction\/train.csv')","d2eae167":"# Let's see about data.\ndf.info()","cc9256d5":"# let's see there are null values or not.\ndf.isnull().sum()","5a441a18":"# make cat, bin, [con,or] in a list.\ncat_feature = []\nbin_feature = []\ncon_or_feature = []\nfor i in df.columns:\n    if 'cat' in i:\n        cat_feature.append(i)\n    elif 'bin' in i:\n        bin_feature.append(i)\n    else:\n        con_or_feature.append(i)","8b3efa4c":"print(cat_feature)\nprint('cat_feature count:',len(cat_feature))\nprint(bin_feature)\nprint('bin_feature count:',len(bin_feature))\nprint(con_or_feature)\nprint('con_or_feature count:',len(con_or_feature))","8d3a9782":"# -1\uc774 \uc788\ub294 \ubcc0\uc218\ub9cc \ubcf4\uae30\nnulled_data = []\nfor i in cat_feature:\n    if -1 in df[i].values:\n        print(i)\n        nulled_data.append(i)\nfor i in bin_feature:\n    if -1 in df[i].values:\n        print(i)\n        nulled_data.append(i)\nfor i in con_or_feature:\n    if -1 in df[i].values:\n        print(i)\n        nulled_data.append(i)","f81649df":"# count values\nfor i in nulled_data:\n    print(df[i].value_counts())","c4c6381d":"# 13 features has null values \nprint('ps_ind_02_cat: {:.5%}'.format(216\/len(df)))\nprint('ps_ind_04_cat: {:.5%}'.format(83\/len(df)))\nprint('ps_ind_05_cat: {:.5%}'.format(5809\/len(df)))\nprint('ps_car_01_cat: {:.5%}'.format(107\/len(df)))\nprint('ps_car_02_cat: {:.5%}'.format(5\/len(df)))\nprint('ps_car_03_cat: {:.5%}'.format(411231\/len(df)))\nprint('ps_car_05_cat: {:.5%}'.format(266551\/len(df)))\nprint('ps_car_07_cat: {:.5%}'.format(11489\/len(df)))\nprint('ps_car_09_cat: {:.5%}'.format(569\/len(df)))\nprint('ps_reg_03: {:.5%}'.format(107772\/len(df)))\nprint('ps_car_11: {:.5%}'.format(5\/len(df)))\nprint('ps_car_12: {:.5%}'.format(1\/len(df)))\nprint('ps_car_14: {:.5%}'.format(42620\/len(df)))\n","c05acf1d":"df2 = df.copy()","8f4675dd":"df2.drop(['id','ps_car_03_cat','ps_car_05_cat','ps_reg_03','ps_car_14'], axis=1, inplace=True)","aaece5b9":"df2 = df2[(df2['ps_car_11'] != -1) & (df2['ps_car_12'] != -1) & (df2['ps_car_02_cat'] != -1)\n          & (df2['ps_ind_02_cat'] != -1) & (df2['ps_ind_04_cat'] != -1) & (df2['ps_ind_05_cat'] != -1)\n          & (df2['ps_car_01_cat'] != -1) & (df2['ps_car_07_cat'] != -1) & (df2['ps_car_09_cat'] != -1)]","517baf03":"for i in df.columns:\n    if -1 in df[i].values:\n        print(df[i].value_counts())\n\nprint('--'*20)\nfor i in df2.columns:\n    if -1 in df2[i].values:\n        print(df2[i].value_counts())\n","261b83c3":"df2['target'].value_counts()","eaeec70a":"df2.columns","8ed5f364":"X,y = df2.iloc[:,1:], df2.iloc[:,:1]\nprint('Resampled dataset shape %s' % y['target'].value_counts())\n# sm = SMOTE(random_state=42)\n# X_res, y_res = sm.fit_resample(X, y)\nrus = RandomUnderSampler(random_state=42)\nX_res, y_res = rus.fit_resample(X,y)\nprint('Resampled dataset shape %s' % y_res['target'].value_counts())","d8a76a6c":"# make cat, bin, [con,or] in a list.\ncat_bin_feature = []\ncon_or_feature = []\nfor i in X_res.columns:\n    if 'cat' in i:\n        cat_bin_feature.append(i)\n    elif 'bin' in i:\n        cat_bin_feature.append(i)\n    else:\n        con_or_feature.append(i)\n\nfor i in cat_bin_feature:\n    X_res[i] = X_res[i].astype('category')","9423b3f2":"\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.2, random_state=42)\nX_train, X_test, y_train, y_test = X_train.values, X_test.values, y_train.values.ravel(), y_test.values.ravel()\n# baseline = LogisticRegression()\n# baseline.fit(X_train, y_train)\n# print(baseline.score(X_train, y_train))\n# print(baseline.score(X_test, y_test))","2e18f85a":"rf_param_grid = {'max_depth' : [6,7,8], \n                 'n_estimators' : [100,150,200],\n                 'min_samples_split' : [2,3],\n                 'min_samples_leaf' : [1,3,5]}\n\n\nrf = RandomForestClassifier()\nrf_grid = GridSearchCV(estimator=rf,\n                       param_grid=rf_param_grid,\n                       scoring='roc_auc',\n                       cv=5)","ab6dceb0":"rf_grid.fit(X_train, y_train)","e9e8b768":"best_max_depth = rf_grid.best_params_[\"max_depth\"]\nbest_min_samples_split = rf_grid.best_params_[\"min_samples_split\"]\nbest_n_estimators = rf_grid.best_params_['n_estimators']\nbest_min_samples_leaf = rf_grid.best_params_['min_samples_leaf']\n#best_learning_rate = rf_grid.best_params_['learning_rate']\n\nprint('max_depth : ',best_max_depth,'\\n',\n     'min_samples_split : ',best_min_samples_split,'\\n',\n     'n_estimators : ',best_n_estimators,'\\n')\n     #'learning_rate : ',best_learning_rate,'\\n')","7f6d529a":"rf_grid.best_params_","a4eddd6d":"rf_2 = RandomForestClassifier(max_depth=8,\n                            min_samples_leaf=1,\n                            min_samples_split=3,\n                            n_estimators=200)\n\nrf_2.fit(X_train, y_train)","0b60a4c8":"rf_2.score(X_test, y_test)\npredict2 = rf_2.predict(submit_X)\n#submission = pd.read_csv('\/kaggle\/input\/porto-seguro-safe-driver-prediction\/sample_submission.csv')\nsubmission['target'] = predict2","a53e680f":"submission['target'].value_counts()","97a02369":"submit_X = pd.read_csv('\/kaggle\/input\/porto-seguro-safe-driver-prediction\/test.csv')\nsubmit_X.drop(['id','ps_car_03_cat','ps_car_05_cat','ps_reg_03','ps_car_14'], axis=1, inplace=True)\nfor i in cat_bin_feature:\n    submit_X[i] = submit_X[i].astype('category')\nsubmit_X = submit_X.values\n\nsubmission = pd.read_csv('\/kaggle\/input\/porto-seguro-safe-driver-prediction\/sample_submission.csv')\nsubmission['target'] = predict2\nprint(submission['target'].value_counts())\n\nsubmission.to_csv('gridsearch_rf_0407_2_undersampling.csv', index=False)\nprint('\uc800\uc7a5\ud558\uc600\uc2b5\ub2c8\ub2e4.')","211c2354":"predict = baseline.predict(submit_X)\nsubmission = pd.read_csv('\/kaggle\/input\/porto-seguro-safe-driver-prediction\/sample_submission.csv')\nsubmission['target'] = predict","250b1e64":"import lightgbm as lgb\n# from lightgbm import LGBMClassifier\n# params = {'learning_rate': 0.01,\n#           'max_depth': 16,\n#           'boosting': 'gbdt',\n#           'objective': 'binary',\n#           'metric': 'auc',\n#           'is_training_metric': True, \n#           'num_leaves': 144,\n#           'feature_fraction': 0.9,\n#           'bagging_fraction': 0.7, \n#           'bagging_freq': 5, \n#           'seed':2018} \n\n# train_ds = lgb.Dataset(X_train, label = y_train)\n# val_ds = lgb.Dataset(X_test, label = y_test)\n\n# model = lgb.train(params, train_ds, 1000, val_ds, verbose_eval=10, early_stopping_rounds=100)\n\n# lgb_cf = LGBMClassifier()\n# lgb_cf.fit(X_train,y_train)\n# print(lgb_cf.score(X_test,y_test))\n\n\nparam_grid = {\n    'num_leaves': [31, 127],\n    'reg_alpha': [0.1, 0.5],\n    'min_data_in_leaf': [30, 50, 100, 300, 400],\n    'lambda_l1': [0, 1, 1.5],\n    'lambda_l2': [0, 1]\n    }\n\nlgb_estimator = lgb.LGBMClassifier(boosting_type='gbdt',  objective='binary', num_boost_round=2000, learning_rate=0.01, metric='auc')\n\ngsearch = GridSearchCV(estimator=lgb_estimator, param_grid=param_grid, cv=5)\nlgb_model = gsearch.fit(X=X_train, y=y_train)\n\nprint(lgb_model.best_params_, lgb_model.best_score_)","ea01a793":"predict_lgb = lgb_cf.predict(submit_X)\nsubmission['target'] = predict_lgb\n#submission['target'] = submission['target'].apply(lambda x: 1 if x>=0.5  else 0)\nprint(submission['target'].value_counts())\n\nsubmission.to_csv('lgb_0407_1_undersampling.csv', index=False)\nprint('\uc800\uc7a5\ud558\uc600\uc2b5\ub2c8\ub2e4.')\n","f960d1fe":"submission.to_csv('lgb_0407_2_undersampling.csv', index=False)","7206beb8":"we have think about imbalanced target data.\nshould we Oversampling? or Undersampling?\nI don't want to dump out 55 data. it is pretty much worth it. to use it.\nso, I think it is reasonable to just use SMOTE library on this dataset.","2d0746a6":"# What should I do?\nwith these nulled data???","921b5716":"there are 14 categorical data, 17 bin data, 26 con_or_data, 1 id, 1 target data","ff27bce6":"let's see.. \n### ps_car_03_cat,ps_car_05_cat,ps_reg_03, ps_car_14\n#### this 4 features are 5% above nulled data overall. \n#### so, I decided to delete id(it is not worth it to contain), 4 of those columns instead of filling it with mean or median.\n#### let's copy the origin data first","df725abb":"let's delete id data first. because, id is not important columns to predict the target data","67c04979":"Data shows us that there are 3 or 4 kinds of data type here.\n1. Categorical Data \n2. Bin Data \n3. Continuous Data \n4. Ordinal Data\n","6078c1bf":"none. there are no NaN value on this dataset. Is this good for us? no! you have to carefully see Data Description again.\n\n# Data Description\nIn this competition, you will predict the probability that an auto insurance policy holder files a claim.\n\nIn the train and test data, features that belong to similar groupings are tagged as such in the feature names (e.g., ind, reg, car, calc). In addition, feature names include the postfix bin to indicate binary features and cat to indicate categorical features. Features without these designations are either continuous or ordinal. Values of -1 indicate that the feature was missing from the observation. The target columns signifies whether or not a claim was filed for that policy holder.","ee143c0b":"There are 595212 rows in this data. we can see distinguish columns type by their names.\nand also, I don't have any knowledge about this Data, but you can easily see that there are ind, reg, car, calc is some kind of keywords about\nthe car insurance data.","32f5079c":"# *we took care of all the nulled data now~!*","298dda06":"now, we got 13-4 = 9 features to take care of. \nps_car_11,ps_car_12,ps_car_02_cat this 3 features have too little null. so I decided to delete those rows.\n","d0cd6bd5":"I think I made a big mistake.\nIt happens to me that when I did over_sampling, it gives me 0.86, 0.86 accuracy for each train and test set.\nbut, when I submitted my prediction file, it gives me 0.018 Acc score. What the heck?\n\nso, I tried it again with the under_sampling, using RandomUnderSampler.\nand it gives me 0.16 accuracy, which is pretty much good acc compare to leaderboards scoring.\nSMOTE just makes data unreasonable and I think 20000 data is pretty big enough for training set.","8390ae2e":"there were no NaN values because those values were changed to -1 values. Then, let's count -1 values.\n\njust a minute! before that, I want to divide those column names by their types"}}