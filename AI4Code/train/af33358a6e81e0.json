{"cell_type":{"d327415a":"code","25d434bb":"code","b904d33e":"code","07090fe8":"code","a641fd2f":"code","82e66dbc":"code","f2ee9e4f":"code","dcf2c63f":"code","e60965c8":"code","31fea1ea":"code","864f715e":"code","a072f9c6":"code","16c5218d":"code","2963caad":"code","fa7ae9e2":"code","f7aa37a5":"code","00f3539f":"code","1ea01007":"code","5ef7f5c2":"code","e02fbb9f":"code","85b5a895":"code","1b5ddf57":"code","568d39d4":"code","219e78fa":"code","ac189b36":"code","789efbca":"code","d2548b0c":"code","6c196943":"code","e30cf111":"code","4e85a39c":"code","52aae378":"code","1b04be6d":"code","c9b21182":"code","981b15ed":"code","4380e430":"code","8b18a330":"code","d921ee63":"code","71e2ba44":"code","b8db3cfd":"code","3b74c3a9":"code","b2b80927":"code","d7e42a57":"code","720ba012":"code","a2a63142":"code","77042c68":"code","0c539dff":"code","26ac0794":"code","c5a16bb1":"code","f8b67da9":"code","eb6ce253":"code","4f5c659d":"code","f91947ff":"code","4e62e938":"code","266936ce":"code","2bdd651e":"code","1f17b2a4":"code","945cc493":"code","7e9c6126":"code","bc74b25b":"code","6c81c8a6":"code","4c249cfb":"code","b693fefa":"markdown","11984e3c":"markdown","d56c8ed0":"markdown","5e5ab472":"markdown","b5cf0574":"markdown","17590c9c":"markdown","794f5cb9":"markdown","a508a7a5":"markdown","810488ce":"markdown","783bbb61":"markdown","4ba197e0":"markdown","f7bb45b1":"markdown","e08b9413":"markdown","97a4b535":"markdown","5ed5d5c2":"markdown","033bb626":"markdown","1d01074f":"markdown","fe6d893c":"markdown","69b0b5ae":"markdown","9cc4f729":"markdown","a3728305":"markdown","b9ff4006":"markdown","c07eb843":"markdown","4bdfc384":"markdown"},"source":{"d327415a":"import pandas as pd\nimport numpy as np","25d434bb":"data = pd.read_csv(\"..\/input\/insurance\/insurance.csv\")","b904d33e":"data.describe()","07090fe8":"data.info()","a641fd2f":"data.head()","82e66dbc":"data","f2ee9e4f":"data.region.value_counts()","dcf2c63f":"#null values\ndata.isna().sum()","e60965c8":"data.nunique()","31fea1ea":"data.region.unique()","864f715e":"#dependent,independent variables\ny = data.charges\nx= data.drop(['charges'], axis=1)","a072f9c6":"y.head()","16c5218d":"x.head()","2963caad":"#encodeyes no to 1,0\nx.smoker = x.smoker.eq('yes').mul(1)","fa7ae9e2":"x.head()","f7aa37a5":"x.sex = x.sex.eq('female').mul(1)","00f3539f":"x.head()","1ea01007":"#one_hot encoding\ncat_var = list(x.select_dtypes(include=[\"object\"]))","5ef7f5c2":"cat_var","e02fbb9f":"x = pd.get_dummies(x, columns=cat_var, drop_first = True)","85b5a895":"x.head()","1b5ddf57":"#check outliers\nimport seaborn as sns\ncolumns = list(x)","568d39d4":"columns","219e78fa":"import matplotlib.pyplot as plt\nimport warnings\nimport matplotlib.cbook\nwarnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\n\nsns.set(rc={'figure.figsize':(20,10)})\n\nfor j in range(1, 9):\n    plt.subplot(2, 4, j)\n    sns.boxplot(x =  x[columns[j-1]])\n   ","ac189b36":"from scipy import stats\nimport numpy as np\nz = np.abs(stats.zscore(x))\nprint(z)","789efbca":"threshold = 3\nprint(np.where(z > 3))\n\n#first array is rows, second is column numbers","d2548b0c":"#OLS statsmodel regression\nfrom statsmodels.stats import diagnostic\n\nimport statsmodels.api as sm\nmodel = sm.OLS(y, x).fit()\n#predictions = model.predict(x_test) \n\nprint_model = model.summary()\nprint(print_model)","6c196943":"heter#inferences\n'''\n1. R square : model explains 87.4% variance of charges\n\n2. Prob(F statistic) = 0.00, model is significant. F test is basically a test for the model significance. \n   Null hypothesis: No linear relationship between all the independent variales and the Dependent variable\n   \n3. AIC : basic idea of AIC is to penalize the inclusion of additional variables to a model. It adds a penalty that increases\n   the error when including additional terms. The lower the AIC, the better the model.\n   BIC:  Variant of AIC with a stronger penalty for including additional variables to the model.\n   BIC tends to penalise the complex models more hence, A downside of BIC is that for smaller, less representative training \n   datasets, it is more likely to choose models that are too simple. In general. it doesnt choose the more complex models\n   probability that BIC will select the true model increases with the size of the training dataset. This cannot be said for the \n   AIC score.\n   \n4. Degrees of Freedom of residuals: Degrees of freedom is the number of values in the final calculation of a statistic\n   that are free to vary.\n   df(residual) = n-k-1 (1 for the intercept)\n\n5. Degress of Freedom of Model: No. of independent variables \n\n6. Std. error of variables: Individual Estimation error by variables.\n\n7. P values of variables: all under 0.05 except Children but very close so can be avoided for now, for accuracy purposes \n   can be removed and checked later.\n   T test is done for individual variable significance.\n   Null hypothesis: No linear relationship between the Independent Variable and dependent variable.\n   T statistic = Coefficient\/Std. error\n   \n8. Omnibus: Test for Normality.\n   Null hypothesis : Data is Normally Distributed\n   P value can be used to understand if to accept or rejeheterct (Prob omnibus)\n\n9. Skew: Measure for Noramlity. Amount and direction of skew (departure from horizontal symmetry)\n   Skewness of Normal Distribution = 0 \n\n10. Kurtosis: Measure for Normality. Height and sharpness of Central peak.\n    Kurtosis of Normal Distribution = 3\n\n11. Durbin Watson: Measure of Autocorrelation.\n    Null Hypothesis: No first order correlation.\n    \n    The Durbin-Watson statistic will always have a value between 0 and 4.\n    A value of 2.0 means that there is no autocorrelation detected in the sample.\n    Values from 0 to less than 2 indicate positive autocorrelation \n    and values from 2 to 4 indicate negative autocorrelation.\n    \n    A rule of thumb is that test statistic values in the range of 1.5 to 2.5 are relatively normal. \n    \n    Any value outside this range could be a cause for concern. The Durbin\u2013Watson statistic, while displayed by many \n    regression analysis programs, is not applicable in certain situations. For instance, when lagged dependent variables \n    are included in the explanatory variables, then it is inappropriate to use this test.\n\n12. Jarque Bera: Test for Normality.\n    Null hypothesis : Data is Normally Distributed\n    P value can be used to understand if to accept or reject (Prob Jargue Bera)\n\n13. Standard Error of Regresion : The standard error of the regression model represents the average distance that the \n    observed values fall from the regression line. Conveniently, it tells you how wrong the regression model is on average \n    using the units of the response variable. Smaller values are better because it indicates that the observations are closer \n    to the fitted line.\n    \n    Unlike R-squared, you can use the standard error of the regression to assess the precision of the predictions. \n    Approximately 95% of the observations should fall within plus\/minus 2*standard error of the regression from the \n    regression line, which is also a quick approximation of a 95% prediction interval. If want to use a regression model \n    to make predictions, assessing the standard error of the regression might be more important than assessing R-squared.\n\n'''","e30cf111":"#train_test splitting \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)","4e85a39c":"#Linear Regression Model\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(x_train, y_train)\n\n# Predict\ny_pred = regressor.predict(x_test)\n\n#Accuracy\nfrom sklearn.metrics import r2_score\nr2_test = r2_score(y_test, y_pred)  \n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nmse_test = sqrt(mean_squared_error(y_test, y_pred))","52aae378":"print(r2_test)\nprint(mse_test)","1b04be6d":"y_predt = regressor.predict(x_train)\nr2_train = r2_score(y_train, y_predt) \nmse_train = sqrt(mean_squared_error(y_train, y_predt))","c9b21182":"print(r2_train)\nprint(mse_train)","981b15ed":"#find the residuals\nresidual = y_test - y_pred","4380e430":"#why is sklearn and statsmodel R square different?\n#why is test accuracy better than train accuracy?","8b18a330":"#linearity\n#plot predicted vs actual\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.scatter(y_pred,y_test)\n\n#sort of linear","d921ee63":"#linearity with different independent variables to dependent variable  \nsns.set(rc={'figure.figsize':(20,10)})\n\nfor j in range(1, 9):\n    plt.subplot(2, 4, j)\n    plt.scatter(x[columns[j-1]],y)\n","71e2ba44":"#Multicollinearity - VIF\n#VIF>10, REMOVE. - high VIF indicates high multicollinearity\n#VIF>10 indicates heavy multicollinearity so those factors should be removed, <5 is good\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = [variance_inflation_factor(x_train.values, i) for i in range(x_train.shape[1])]\npd.DataFrame({'vif':vif[0:]}, index = x_train.columns).T\n\n#based on results - BMI should be definitely removed","b8db3cfd":"#Normality of Residual\n#1 Distribution\n#2 PP plot or a QQ plot. Whats the difference? How to interpret?\n%matplotlib inline\nimport seaborn as sns\nsns.distplot(residual)","3b74c3a9":"#PP plot\nimport scipy as sp\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(6,2.5))\n_, (__, ___, r) = sp.stats.probplot(residual, plot=ax, fit=True)\n","b2b80927":"#QQ plot\nimport statsmodels.api as sm \nimport pylab as py \nsm.qqplot(residual, fit=True, line = '45') \npy.show() ","d7e42a57":"np.mean(residual)\n#should be close to zero or nearly zero in case of normal distribution","720ba012":"#homoskedasticity., there shouldnt be a pattern as such, increasing or decreasing or anything like that \n#what should be there? \n#what does it mean?\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(6,2.5))\nax.scatter(y_pred, residual)","a2a63142":"#residuals vs predicted plot\n# look for independence assumption. If the residuals are distributed uniformly randomly around the zero x-axes and \n#do not form specific clusters, then the assumption holds true.\nplt.scatter(y_pred, residual)\n","77042c68":"#autocorrelation\n#except the first line, all other lines should be inside the blue or the significance area\n#outside the blue area would mean there is some significant autocorrelation\n\nimport statsmodels.api as sm\nimport statsmodels.tsa.api as smt\n\nacf = smt.graphics.plot_acf(residual, lags=40 , alpha=0.05)\nacf.show()","0c539dff":"data.corr()['charges'].sort_values()","26ac0794":"#correlation matrix\nimport seaborn as sns\nx_cor = x.corr()\n\nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(10,10))\nsns.set(font_scale=1)\nsns.heatmap(x_cor, linewidths=3,fmt='.2f', annot=True)","c5a16bb1":"'''\nAssumption Inferences\n\nLinearity: Sort of\nAutocorrelation: No autocorrelation as such \nHeteroskedasticity: no certain pattern as such, but weird, idk\nNormality: Nearly normal, but not completely\nMulticollinearity: Should remove BMI\n\n'''\n\n#updated data after linear regression assumptions\nx_asmp = x.drop(['bmi'], axis=1)","f8b67da9":"#reapply linear regression and see if any progress\nfrom sklearn.model_selection import train_test_split\nx_tr, x_te, y_tr, y_te = train_test_split(x_asmp, y, test_size = 0.25, random_state = 0)\n\n#Linear Regression Model\nfrom sklearn.linear_model import LinearRegression\nregress = LinearRegression()\nregress.fit(x_tr, y_tr)\n\n# Predict\ny_pr = regress.predict(x_te)\n\n#Accuracy\nfrom sklearn.metrics import r2_score\nr2_te = r2_score(y_te, y_pr)  \n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nmse_te = sqrt(mean_squared_error(y_te, y_pr))","eb6ce253":"print(r2_te)\nprint(mse_te)\n\n#useless","4f5c659d":"#apply other models\n#polynomial regression\n\n# Fitting Polynomial Regression to the dataset\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree = 2)\nX_poly = poly_reg.fit_transform(x_train)\npoly_reg.fit(X_poly, y_train)\nlin_reg_2 = LinearRegression()\nlin_reg_2.fit(X_poly, y_train)\n\n# Predicting a new result with Polynomial Regression\npoly_pred = lin_reg_2.predict(poly_reg.fit_transform(x_test))\n\nr2_poly = r2_score(y_test, poly_pred)  \n\nprint(r2_poly)\n","f91947ff":"#Kaggle polynomial features\n#why? and how?\n\nX = x.drop(['region_northwest','region_southeast','region_southwest'], axis = 1)\nY = data.charges\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nquad = PolynomialFeatures (degree = 2)\nx_quad = quad.fit_transform(X)\n\nX_train,X_test,Y_train,Y_test = train_test_split(x_quad,Y, random_state = 0)\n\nplr = LinearRegression().fit(X_train,Y_train)\n\nY_train_pred = plr.predict(X_train)\nY_test_pred = plr.predict(X_test)\n\nr2_quad = r2_score(Y_test, Y_test_pred)  \n\nprint(r2_quad)","4e62e938":"#decision tree\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state = 0)\nregressor.fit(x_train, y_train)\n\n# Predicting a new result\ny_pred = regressor.predict(x_test)\n                          \nr2_dt = r2_score(y_test, y_pred)  \nmse_dt = sqrt(mean_squared_error(y_test, y_pred))\n                           \nprint(r2_dt)\nprint(mse_dt)","266936ce":"#random forest\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 10000, random_state = 0, max_depth=5)\nregressor.fit(x_train, y_train)\n\n# Predicting a new result\ny_pred = regressor.predict(x_test)\n\nr2_rf = r2_score(y_test, y_pred)  \nmse_rf = sqrt(mean_squared_error(y_test, y_pred))\n                           \nprint(r2_rf)\nprint(mse_rf)","2bdd651e":"#SVR\nfrom sklearn.svm import SVR\nregressor = SVR(kernel = 'rbf')\nregressor.fit(x_train, y_train)\n\n# Predicting a new result\ny_pred = regressor.predict(x_test)\n\nr2_svr = r2_score(y_test, y_pred)  \nmse_svr = sqrt(mean_squared_error(y_test, y_pred))\n                           \nprint(r2_svr)\nprint(mse_svr)","1f17b2a4":"#bagging, boosting and its difference\n#gradient boost\nfrom sklearn import ensemble\n\nreg = ensemble.GradientBoostingRegressor(n_estimators = 500, max_depth = 4,learning_rate = 0.01)\nreg.fit(x_train, y_train)\n\ny_pred_gbm = reg.predict(x_test)\n\nr2_gbm = r2_score(y_test, y_pred_gbm)  \nmse_gbm = sqrt(mean_squared_error(y_test, y_pred_gbm))\n\nprint(r2_gbm)\nprint(mse_gbm)\n                           ","945cc493":"#Feature Scaling\n'''\nReal world dataset contains features that highly vary in magnitudes, units, and range. Normalisation should be performed \nwhen the scale of a feature is irrelevant or misleading and not should Normalise when the scale is meaningful.\n\nThe algorithms which use Euclidean Distance measure are sensitive to Magnitudes. Here feature scaling helps to weigh all \nthe features equally.\n\nFormally, If a feature in the dataset is big in scale compared to others then in algorithms where Euclidean distance is \nmeasured this big scaled feature becomes dominating and needs to be normalized.\n\nExamples of Algorithms where Feature Scaling matters\n1. K-Means uses the Euclidean distance measure here feature scaling matters.\n2. K-Nearest-Neighbours also require feature scaling.\n3. Principal Component Analysis (PCA): Tries to get the feature with maximum variance, here too feature scaling is required.\n4. Gradient Descent: Calculation speed increase as Theta calculation becomes faster after feature scaling.\n\nNote: Naive Bayes, Linear Discriminant Analysis, and Tree-Based models are not affected by feature scaling.\nIn Short, any Algorithm which is Not Distance based is Not affected by Feature Scaling.\n\n\nSomething like millions and thousands should be taken care of. \n\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nX = scaler.fit_transform(X)\n\n'''","7e9c6126":"#normalise all independent variables and try all models. Does result improve?\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\nx_norm = scaler.fit_transform(x)\nx_norm = pd.DataFrame(x_norm)\nx_norm.columns = x.columns\nx_norm.head()","bc74b25b":"#Apply linear regression\nfrom sklearn.model_selection import train_test_split\nxtr, xte, ytr, yte = train_test_split(x_norm, y, test_size = 0.25, random_state = 0)\n\n#Linear Regression Model\nfrom sklearn.linear_model import LinearRegression\nregres = LinearRegression()\nregres.fit(xtr, ytr)\n\n# Predict\nypr = regres.predict(xte)\n\n#Accuracy\nfrom sklearn.metrics import r2_score\nr2te = r2_score(yte, ypr)  \n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nmsete = sqrt(mean_squared_error(yte, ypr))","6c81c8a6":"print(r2te)\nprint(msete)\n\n#not much difference","4c249cfb":"#support vector on scaled data\nregressor = SVR(kernel = 'rbf')\nregressor.fit(xtr, ytr)\n\n# Predicting a new result\ny_pred = regressor.predict(xte)\n\nr2_svr = r2_score(yte, ypr)  \nmse_svr = sqrt(mean_squared_error(yte, ypr))\n                           \nprint(r2_svr)\nprint(mse_svr)","b693fefa":"### Support Vector Regression","11984e3c":"### Heteroskedasticity","d56c8ed0":"# Other Regression Models = Do they have assumptions too?","5e5ab472":"### Encoding X","b5cf0574":"### Changed model based on assumptions","17590c9c":"# Statsmodel Regression and Meaning","794f5cb9":"### Multicollinearity","a508a7a5":"# Data Preparation","810488ce":"### Gradient Boost","783bbb61":"### X-Y Preparation","4ba197e0":"### Normality","f7bb45b1":"### Autocorrelation","e08b9413":"# Normalisation\/Scaling","97a4b535":"# SKlearn Regression","5ed5d5c2":"### Linearity","033bb626":"### Outliers","1d01074f":"### Polynomial Regression","fe6d893c":"### Residuals","69b0b5ae":"## Testing Assumptions","9cc4f729":"### Decision Tree","a3728305":"### Polynomial Features - Kaggle","b9ff4006":"### Support Vector Regression on scaled data","c07eb843":"### Missing Values","4bdfc384":"### Random Forest"}}