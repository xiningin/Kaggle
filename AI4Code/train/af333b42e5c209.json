{"cell_type":{"8fd310a9":"code","ad0198ad":"code","3961b059":"code","02427ffd":"code","37d9f0d2":"code","5cc165fa":"code","6eb22bfd":"code","b721e2cb":"code","0d0a8795":"code","ad16a209":"code","e2fe83da":"code","63319d10":"code","51f2e5b7":"code","5f3df0c0":"code","4e606265":"code","5693bf91":"code","243df82c":"code","399973e7":"code","8ee5e955":"code","3a6d8a07":"code","e83f0816":"code","ec6b827f":"code","664c689b":"code","52a484d8":"code","c64fe0c9":"code","beabdb9a":"code","00490f16":"code","8552e33c":"code","f4e9766a":"code","d8829644":"code","d013911f":"code","0bb07f90":"code","bf8cf8cb":"markdown","213f804f":"markdown","0064bf0e":"markdown","871cb8b1":"markdown","9f3a20fc":"markdown","0e76e399":"markdown","584f3a2e":"markdown","38590e76":"markdown","2e87ac19":"markdown","ea2455a5":"markdown","dd1eda07":"markdown","840693b2":"markdown","a5c49b9d":"markdown","1dc6f8ab":"markdown","939e780f":"markdown","3def3c0f":"markdown","9e7d383f":"markdown","0aa09018":"markdown","f4e82265":"markdown","f0cbeb1e":"markdown","f05654a9":"markdown","2fbe79a2":"markdown","23f37c2d":"markdown","61baa5e9":"markdown","2e8b793b":"markdown","6599c72d":"markdown","15bba561":"markdown","9ddcc964":"markdown","11c7138b":"markdown","38907a5b":"markdown","1dbb5133":"markdown","fbcd1dea":"markdown","0d9af135":"markdown","d60ca767":"markdown","80af54da":"markdown","927f22a2":"markdown","64c1c8c4":"markdown","1a09da8d":"markdown","8ef62d2b":"markdown","67c7e1fe":"markdown","6716579d":"markdown","5c0805af":"markdown","d825b75d":"markdown","6b017587":"markdown","662a7904":"markdown","024c8f3d":"markdown","a0472397":"markdown","fa6ec6d7":"markdown","1ff716cf":"markdown","a8dcfb24":"markdown","40c8b525":"markdown","da751feb":"markdown","a2e09e84":"markdown","34185fc3":"markdown","c1f8c600":"markdown"},"source":{"8fd310a9":"import numpy as np # Linear algebra.\nimport pandas as pd # Data processing, CSV file I\/O (e.g. pd.read_csv).\nimport datatable as dt # Data processing, CSV file I\/O (e.g. dt.fread).\n\nimport seaborn as sns # Visualization.\nimport matplotlib.pyplot as plt # Visualization.\n\n# Machine Learning block.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.pipeline import make_pipeline\n\nfrom copy import deepcopy\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f'\\n[INFO] Libraries set up has been completed.')","ad0198ad":"df_train = dt.fread('..\/input\/tabular-playground-series-nov-2021\/train.csv').to_pandas()\ndf_test = dt.fread('..\/input\/tabular-playground-series-nov-2021\/test.csv').to_pandas()\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\n\ndf_train['target'] = df_train['target'].astype('int32') # Datatable reads target as bool by default.\nprint(f'{df_train.info()}\\n\\n')\ndf_train.head(3)","3961b059":"df_target_count = df_train.target.value_counts()\n\nf, axes = plt.subplots(ncols=2, figsize=(12, 3))\nplt.subplots_adjust(wspace=0)\n\ndf_target_count.plot.pie(\n    ax=axes[0], autopct=\"%.1f%%\", \n    explode=(0.05, 0.05),label='', \n    title='Target Count', fontsize=16,\n    colors=['#BB0000', '#4DB6AC'], \n    shadow=True, startangle=140,   \n)\n\ndf_target_count.plot.barh(\n    ax=axes[1],\n    title='Target Count',\n    fontsize=16,\n    color=['#BB0000', '#4DB6AC']\n)\n\nplt.tick_params(\n    axis='x',         \n    which='both',      \n    bottom=False,      \n    labelbottom=False\n)\n\nfor i, v in enumerate(df_target_count):\n    axes[1].text(v, i+0.25, str(v), color='black', fontweight='bold')\n    axes[1].set_xlim([1, 350000])\n\nplt.show()","02427ffd":"seed = 322\ndf_train_sample = df_train.sample(n=30000, random_state=seed)\ndf_test_sample = df_test.sample(n=30000, random_state=seed)\ndf_train_sample.drop(columns='id', inplace=True)\ndf_test_sample.drop(columns='id', inplace=True)\n\nnp.random.seed(seed) \nfeatures_choice = np.random.choice(\n    df_train_sample.keys()[1:-1], size=12, replace=False\n)\n\ndf_sample_twelve = df_train_sample[sorted(features_choice.tolist()) + ['target']]\ndf_sample_twelve.head(3)","37d9f0d2":"fig, ax = plt.subplots(nrows=12, figsize=(24, 24))\n\nfor i, feature in enumerate(sorted(features_choice)):\n     sns.scatterplot(\n         data=df_sample_twelve,\n         x=df_sample_twelve.index,\n         y=feature,\n         hue='target',\n         palette=['red', '#4DB6AC'],\n         legend=True,\n         ax=ax[i]\n     )\n        ","5cc165fa":"missing_values_train = df_train.isna().any().sum()\nmissing_values_test = df_test.isna().any().sum()\n\nprint(f'\\n[INFO] {missing_values_train} missing value(s) has\/have been detected in the train dataset.')\nprint(f'\\n[INFO] {missing_values_test} missing value(s) has\/have been detected in the test dataset.')","6eb22bfd":"df_train.drop(columns='id', inplace=True)\ndf_train.head(3)","b721e2cb":"df_test.drop(columns='id', inplace=True)\ndf_test.head(3)","0d0a8795":"memory_train = sum(df_train.memory_usage()) \/ 1e6\nprint(f'[INFO] Memory usage train_before: {memory_train:.2f} MB.')\n\nmemory_test = sum(df_test.memory_usage()) \/ 1e6\nprint(f'[INFO] Memory usage test_before: {memory_test:.2f} MB.\\n')\n\n# Downcasting the traind dataset.\nfor col in df_train.columns:\n    \n    if df_train[col].dtype == \"float64\":\n        df_train[col] = pd.to_numeric(df_train[col], downcast=\"float\")\n        \n    if df_train[col].dtype == \"int64\":\n        df_train[col] = pd.to_numeric(df_train[col], downcast=\"integer\")\n        \n# Downcasting the test dataset.\nfor col in df_test.columns:\n    \n    if df_test[col].dtype == \"float64\":\n        df_test[col] = pd.to_numeric(df_test[col], downcast=\"float\")\n        \n    if df_test[col].dtype == \"int64\":\n        df_test[col] = pd.to_numeric(df_test[col], downcast=\"integer\")\n        \nmemory_train = sum(df_train.memory_usage()) \/ 1e6\nprint(f'[INFO] Memory usage train: {memory_train:.2f} MB.')\n\nmemory_test = sum(df_test.memory_usage()) \/ 1e6\nprint(f'[INFO] Memory usage test: {memory_test:.2f} MB.')","ad16a209":"df_train.iloc[:, :-1].describe().T.sort_values(by='std', ascending=False)\\\n                     .head(15)\\\n                     .style.background_gradient(cmap='GnBu')\\\n                     .bar(subset=[\"mean\",], color='green')\\\n                     .bar(subset=[\"max\"], color='#BB0000')","e2fe83da":"fig, axes = plt.subplots(10,10, figsize=(20, 12))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    \n    sns.kdeplot(\n        data=df_train_sample, ax=ax, hue='target', fill=True,\n        x=f'f{idx}', palette=['#4DB6AC', 'red'], legend=idx==0\n    )\n \n    ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel('')\n    ax.set_ylabel(''); ax.spines['left'].set_visible(False)\n    ax.set_title(f'f{idx}', loc='right', weight='bold', fontsize=10)\n\nfig.supxlabel('Probability Density Function Estimation', ha='center', fontweight='bold')\nfig.tight_layout()\nplt.show()","63319d10":"corr = df_train_sample.corr()\n\nfig, axes = plt.subplots(figsize=(20, 10))\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask=mask, linewidths=.5, cmap='ocean')\n\nplt.show()","51f2e5b7":"candles = [\n    'f0','f2','f4','f9','f12','f16','f19','f20','f23','f24','f27',\n    'f28','f30','f31','f32','f33','f35','f39','f42','f44','f46','f48',\n    'f49','f51','f52','f53','f56','f58','f59','f60','f61','f62','f63',\n    'f64','f68','f69','f72','f73','f75','f76','f78','f79','f81','f83',\n    'f84','f87','f88','f89','f90','f92','f93','f94','f95','f98','f99'\n]\n\ndf_candles_log_transform = df_train_sample[candles]\n\nmask_neg = (df_candles_log_transform < 0)\nmask_pos = (df_candles_log_transform > 0)\n\ndf_candles_log_transform[mask_neg] = np.log(np.abs(df_candles_log_transform)) * (-1)\ndf_candles_log_transform[mask_pos] = np.log(df_candles_log_transform)","5f3df0c0":"df_candles_log_transform.describe().T.sort_values(by='std', ascending=False)\\\n                     .head(15)\\\n                     .style.background_gradient(cmap='GnBu')\\\n                     .bar(subset=[\"mean\",], color='green')\\\n                     .bar(subset=[\"max\"], color='#BB0000')","4e606265":"df_candles_log_transform['target'] = df_train_sample.target\n\nfig, axes = plt.subplots(11,5, figsize=(20, 12))\naxes = axes.flatten()\n\nfor col, ax in zip(candles, axes):\n    \n    sns.kdeplot(\n        data=df_candles_log_transform, ax=ax, hue='target', fill=True,\n        x=col, palette=['#4DB6AC', 'red'], legend=idx==0\n    )\n \n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    ax.spines['left'].set_visible(False)\n    ax.set_title(col, loc='right', weight='bold', fontsize=10)\n\nfig.supxlabel('Probability Density Function Estimation', ha='center', fontweight='bold')\nfig.tight_layout()\nplt.show()","5693bf91":"agg_features = ['sum','mean','std','max','min','kurt']\nfeatures = df_train.keys().tolist()[:-1]\n\nfor ft in agg_features:\n    \n    class_method = getattr(pd.DataFrame, ft)\n    df_train_sample[ft] = class_method(df_train_sample[features], axis=1)\n    df_test_sample[ft] = class_method(df_test_sample[features], axis=1)\n\ndf_train_sample.head(3)","243df82c":"def print_shapes(X_train, y_train, X_valid, y_valid):\n  \"\"\"\n  Prints shapes of train\/valid splits.\n  :param: X_train (numpy.ndarray)\n  :param: y_train (numpy.ndarray)\n  :param: X_valid(numpy.ndarray)\n  :param: y_valid (numpy.ndarray)\n  :return: None\n  \"\"\"\n\n  print(f'\\n[INFO] shape of X_train: {X_train.shape}.')\n  print(f'[INFO] shape of y_train: {y_train.shape}.')\n  print(f'[INFO] shape of X_valid: {X_valid.shape}.')\n  print(f'[INFO] shape of y_valid: {y_valid.shape}.\\n')\n\n\ndef auc_score(y_true, y_pred):\n  \"\"\"\n  Prints shapes of train\/valid splits.\n  :param: y_true (numpy.ndarray)\n  :param: y_pred (numpy.ndarray)\n  :return: float\n  \"\"\"\n\n  fpr, tpr, _ = roc_curve(y_true, y_pred)\n  score = auc(fpr, tpr)\n\n  return score\n\n\ndef train_model(x, y, clf, clf_name, xgb=False):\n    \"\"\"\n    Trains model by using selected classifier.\n    :param: x (scaled numpy.ndarray)\n    :param: y (scaled numpy.ndarray)\n    :param: clf (model classifier)\n    :param: clf_name (str)\n    :return: clf, float\n    \"\"\"\n    \n    best_clf_auc = 0\n\n    for fold, (idx_train, idx_valid) in enumerate(skf.split(X, Y)):\n\n            X_train, y_train = X[idx_train, :], Y[idx_train]\n            X_valid, y_valid = X[idx_valid, :], Y[idx_valid]\n\n            if fold == 0:\n                print_shapes(X_train, y_train, X_valid, y_valid)\n            \n            if xgb:\n                clf.fit(\n                    X_train, y_train,\n                    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                    eval_metric='auc',\n                    early_stopping_rounds=40,\n                    verbose=100\n                )\n            else:\n                clf.fit(X_train, y_train)\n\n            y_pred = clf.predict_proba(X_valid)[:, 1]\n            clf_auc = auc_score(y_true=y_valid, y_pred=y_pred)\n            print(f'[INFO] Fold: {fold+1}. {clf_name} AUC score: {clf_auc:.6f}.')\n\n            if clf_auc > best_clf_auc:\n                best_clf = deepcopy(clf)\n                best_clf_auc = clf_auc\n    \n    return best_clf, best_clf_auc","399973e7":"seed = 1\nepochs = 40\npatience = 10\nbatch_size = 1024\nlr = 0.02\nn_folds = 5\nskf = StratifiedKFold(\n    n_splits=n_folds, \n    shuffle=True, \n    random_state=seed\n)","8ee5e955":"scaler = StandardScaler()\nX = scaler.fit_transform(df_train[features])\nY = df_train.target.astype(dtype=int) # TabNet can save model only with int64.\nX_test = scaler.fit_transform(df_test[features])","3a6d8a07":"lr_cfl = LogisticRegression(solver='liblinear')\nclf_name = 'Logistic Regression'\nbest_lr_clf, best_lr_auc = train_model(x=X, y=Y, \n                                       clf=lr_cfl, clf_name=clf_name)","e83f0816":"df_coef = pd.DataFrame(features)\ndf_coef.columns = ['Feature']\ndf_coef[\"Coefficient\"] = pd.Series(best_lr_clf.coef_[0])\ndf_coef.sort_values(by='Coefficient', ascending=False)","ec6b827f":"scaler = StandardScaler()\nX_sample = scaler.fit_transform(df_train_sample[features])\nY_sample = df_train_sample.target.astype(dtype=int) \nX_test_sample = scaler.fit_transform(df_test_sample[features])\n\nprint(f'\\n[INFO] shape of X: {X_sample.shape}.')\nprint(f'[INFO] shape of Y: {Y_sample.shape}.')\n\nlogreg = LogisticRegression(solver='liblinear')\nlogreg.fit(X_sample, Y_sample)\n\ny_pred_logreg = logreg.predict_proba(X_sample)[:, 1]\nscore = auc_score(y_true=Y_sample, y_pred=y_pred_logreg)\nprint(f'\\n[INFO] Logistic Regression AUC score: {score:.6f}.')","664c689b":"benchmark = 0.752058\ncounter = 0\nfor idx, ft in enumerate(features):\n            \n    mask = df_train[features].columns != ft\n    X_train = X_sample[:, mask]\n    \n    if idx == 0:\n        print(f'\\n[INFO] shape of X_train: {X_train.shape}\\n')\n    \n    LogisticRegression(solver='liblinear')\n    logreg.fit(X_train, Y_sample)\n    \n    \n    y_pred = logreg.predict_proba(X_train)[:, 1]\n    score = auc_score(y_true=Y_sample, y_pred=y_pred)\n    \n    if score >= benchmark:\n        auc_old = benchmark\n        benchmark = score\n        print(\n            f'\\n[INFO] Iteration #: {idx}. AUC has been increased after dropping {ft} feature.\\n'\n            f'[INFO] Iteration #: {idx}. AUC before: {auc_old:.6f} >>> AUC after: {benchmark:.6f}.\\n'\n        )\n    \n    if idx % 10 == 0:\n        print(f'[INFO] Iteration #: {idx}.')\n        \n    if score >= counter:\n        \n        counter = score\n        print(f'[INFO] Iteration #: {idx}. AUC: {counter:.6f}. Removed feature: {ft}.')","52a484d8":"lr_svc = LinearSVC(dual=False, random_state=seed, tol=1e-4)\nlr_svc = CalibratedClassifierCV(lr_svc) \nclf_name = 'Linear SVC'\n\nbest_svc_clf, best_svc_auc = train_model(x=X, y=Y, \n                                         clf=lr_svc, clf_name=clf_name)","c64fe0c9":"nb_cfl = GaussianNB()\nclf_name = 'Gaussian Naive Bayes'\nbest_nb_clf, best_nb_auc = train_model(x=X, y=Y, \n                                       clf=nb_cfl, clf_name=clf_name)","beabdb9a":"sgd_cfl = SGDClassifier(loss='log')\nclf_name = 'SGD'\nbest_sgd_clf, best_sgd_auc = train_model(x=X, y=Y, \n                                         clf=sgd_cfl, clf_name=clf_name)","00490f16":"xgb_clf = XGBClassifier(\n    max_depth=8,\n    learning_rate=0.01,\n    n_estimators=10000,\n    verbosity=1,\n    silent=None,\n    objective='binary:logistic',\n    tree_method = 'gpu_hist',\n    booster='gbtree',\n    n_jobs=-1,\n    nthread=None,\n    gamma=0,\n    min_child_weight=1,\n    max_delta_step=0,\n    subsample=0.7,\n    colsample_bytree=1,\n    colsample_bylevel=1,\n    colsample_bynode=1,\n    reg_alpha=0,\n    reg_lambda=1,\n    scale_pos_weight=1,\n    base_score=0.5,\n    random_state=0,\n    seed=None\n)\n\n\nclf_name = 'XGB'\nbest_xgb_clf, best_xgb_auc = train_model(x=X, y=Y, clf=xgb_clf,\n                                         clf_name=clf_name, xgb=True)","8552e33c":"!pip install pytorch-tabnet --quiet\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nprint(f'\\n[INFO] TabNet set up has been completed.')","f4e9766a":"tabnet_clf = TabNetClassifier(optimizer_params=dict(lr=lr))\nbest_tabnet_auc = 0\n\nfor fold, (idx_train, idx_valid) in enumerate(skf.split(X, Y)):\n\n        X_train, y_train = X[idx_train, :], Y[idx_train]\n        X_valid, y_valid = X[idx_valid, :], Y[idx_valid]\n\n        if fold == 0:\n            print_shapes(X_train, y_train, X_valid, y_valid)\n\n        tabnet_clf.fit(\n            X_train, y_train,\n            eval_set=[(X_valid, y_valid)],\n            eval_metric=['auc'],\n            max_epochs=epochs,\n            patience=patience,\n            batch_size=batch_size, \n            virtual_batch_size=128\n        )\n\n        y_pred = tabnet_clf.predict_proba(X_valid)[:, 1]\n        tabnet_auc = auc_score(y_true=y_valid, y_pred=y_pred)\n        print(f'\\n[INFO] Fold: {fold+1}. TabNet AUC score: {tabnet_auc:.6f}.\\n')\n\n        if tabnet_auc > best_tabnet_auc:\n            best_tabnet_cfl = deepcopy(tabnet_clf)\n            best_tabnet_auc = tabnet_auc","d8829644":"results = pd.DataFrame({\n    'model': [\n        'Logistic Regression', 'Linear SVC', \n        'Gaussian Naive Bayes', 'SGD',\n        'XGB', 'TabNet'\n    ],\n    'auc_score': [\n        best_lr_auc, best_svc_auc, best_nb_auc, \n        best_sgd_auc, best_xgb_auc, best_tabnet_auc\n    ]\n})\n\ncell_hover = {\n    'selector': 'td:hover',\n    'props': [('background-color', '#ffffb3')]\n}\nindex_names = {\n    'selector': '.index_name',\n    'props': 'font-style: italic; color: white; font-weight:normal;'\n}\nheaders = {\n    'selector': 'th:not(.index_name)',\n    'props': 'background-color: #16A085; color: white;'\n}\n\ns = results.sort_values(by='auc_score', ascending=False)\ns = s.style.format({'auc_score': '{:.4f}'}).hide_index()\ns.set_table_styles([cell_hover, index_names, headers])\n\ns.set_table_styles([\n    {'selector': 'th.col_heading', 'props': 'text-align: left;'},\n    {'selector': 'td', 'props': 'text-align: left;'},\n], overwrite=False)\n\ns","d013911f":"y_test = best_tabnet_cfl.predict_proba(X_test)[:, 1]\nsubmission[\"target\"] = y_test\nsubmission.head(10)","0bb07f90":"def postprocess_separate(submission, df_test, df_original=None):\n    \"\"\"\n    Updates the submission file so that the predictions of two sides of the hyperplane don't overlap.\n    :param: submission (pd.DataFrame with columns 'id' and 'target')\n    :param: df_test (pd.DataFrame competition's test data without 'id' column)\n    :return: df_original : (pd.DataFramethe competition's original training data)\n    From https:\/\/www.kaggle.com\/ambrosm\/tpsnov21-007-postprocessing\n    \"\"\"\n    \n    if df_original is None: \n        df_original = dt.fread('..\/input\/november21\/train.csv').to_pandas()\n        df_original['target'] = df_original['target'].astype('int32')\n\n    # Find the separating hyperplane for df_original, step 1\n    # Use an SVM with almost no regularization\n    model1 = make_pipeline(\n        StandardScaler(),\n        LinearSVC(C=1e5, tol=1e-7, penalty='l2', \n                  dual=False, max_iter=2000, random_state=1)\n    )\n    \n    model1.fit(df_original.drop(columns=['id', 'target']), df_original.target)\n    pure_pred = model1.predict(df_original.drop(columns=['id', 'target']))\n\n    # model1 is not perfect: it predicts the wrong class for 1 of 600000 samples.\n    # Find the separating hyperplane for df_original, step 2.\n    # Fit a second SVM to a subset of the points which contains the support vectors.\n    pure_pred = model1.decision_function(df_original.drop(columns=['id', 'target']))\n    subset_df = df_original[(pure_pred > -5) & (pure_pred < 0.9)]\n    model2 = make_pipeline(\n        StandardScaler(),\n        LinearSVC(C=1e5, tol=1e-7, penalty='l2', \n                  dual=False, max_iter=2000, random_state=1)\n    )\n               \n    model2.fit(subset_df.drop(columns=['id', 'target']), subset_df.target)\n    pure_pred = model2.predict(df_original.drop(columns=['id', 'target']))\n    \n    # model2 is perfect: it predicts the correct class for all 600000 training samples\n    pure_test_pred = model2.predict(df_test.drop(columns=['target'], errors='ignore'))\n    lmax, rmin = submission[pure_test_pred == 0].target.max(), submission[pure_test_pred == 1].target.min()\n               \n    if lmax < rmin:\n        print(\"\\n[INFO] There is no overlap. No postprocessing needed.\\n\")\n        return\n    \n    # There is overlap. Remove this overlap\n    submission.loc[pure_test_pred == 0, 'target'] -= lmax + 1\n    submission.loc[pure_test_pred == 1, 'target'] -= rmin - 1\n    print(\n        submission[pure_test_pred == 0].target.min(), \n        submission[pure_test_pred == 0].target.max(),\n        submission[pure_test_pred == 1].target.min(), \n        submission[pure_test_pred == 1].target.max()\n    )\n\n\npostprocess_separate(submission, df_test=df_test)\nsubmission.to_csv(\"submission_postprocessed.csv\", index=False)\nsubmission.head()","bf8cf8cb":"<a id='5.1'><\/a>\n## <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">5.1 Logistic Regression <\/p>","213f804f":"We have run six different models. Hyperparameters tuning and features engineering have not been used during training.\nNow it is the time to rank our models based on their AUC score. ","0064bf0e":"![Kaggle TPS '21-11 1.png](attachment:45f45d9e-997a-4ea6-9307-95d4cb912f2a.png)","871cb8b1":"## <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">Table of Contents<\/p> <a href= '#Table of Contents'><\/a>\n\n* [1. Data visualization \ud83d\udcc8\ud83d\udcc9\ud83d\udcca](#1)\n* [2. Data Preprocessing \ud83d\udee0](#2)\n    * [2.1 Handle Missing Values](#2.1)\n    * [2.2 Smoothing data \/ Reformating \/ Dropping](#2.2)\n* [3. Exploratory Data Analysis \ud83d\udcca](#3)\n* [4. Feature engineering \ud83d\udd27](#4)\n    * [4.1 Changing functional dependency](#4.1)\n    * [4.2 Creating aggregated features](#4.2)\n    * [4.3 Dropping features](#4.3)\n* [5. Modeling](#5)\n    * [5.1 LogisticRegression](#5.1)\n    * [5.2 Linear SVC](#5.2)\n    * [5.3 Gaussian Naive Bayes](#5.3)\n    * [5.4 Stochastic Gradient Decent](#5.4)\n    * [5.5 XGB](#5.5)\n    * [5.6 * Bonus Model: TabNet](#5.6)\n* [6. Conclusions](#6)\n    * [6.1 ** Bonus Postprocessing](#6.1)\n* [7. References](#7)","9f3a20fc":"### ***Model***","0e76e399":"### ***Model***","584f3a2e":"### ***Helper functions***","38590e76":"### ***CFG***","2e87ac19":"### ***Model***","ea2455a5":"<a id='2'><\/a>\n# <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">2. Data Preprocessing \ud83d\udee0 <\/p>\n\n> **[Data preprocessing](https:\/\/en.wikipedia.org\/wiki\/Data_pre-processing)** can refer to manipulation or dropping of data before it is used in order to ensure or enhance performance, and is an important step in the data mining process. The phrase \"garbage in, garbage out\" is particularly applicable to data mining and machine learning projects.","dd1eda07":"> **\"id\" and \"target\" columns are int64.** We do not really need \"id\" column. We do not really need \"target\" as int64.\nMoreover, we can downscale float64 for the sake of faster computation. We will work on it later on. \n\n> **Now let's plot our target values:**","840693b2":"# <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:180%; text-align:center\">A Complete Guide Kaggle TPS \ud83d\udcd3<\/p>\n\n>This notebook is a walk through guide for dealing with common data science competition.\n>* The **objective** of this notebook is to apply step-by-step approach to solve tabular data competition.\n>* The **subject** of this notebook is a classical classification task, based on \"[the synthetic dataset generated using a CTGAN on a real dataset](https:\/\/www.kaggle.com\/c\/tabular-playground-series-nov-2021). The original dataset deals with predicting and identifying spam emails via various extracted features from the email. The features are anonymized, they have properties relating to real-world features\".\n\n","a5c49b9d":"<a id='5'><\/a>\n# <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">5. Modeling<\/p>\n\n> Now it is modeling time. There are many models to choose from. We are going to try following ones:\n>\n>* Logistic Regression\n>* Linear SVC\n>* Gaussian Naive Bayes\n>* Stochastic Gradient Decent\n>* XGB\n>* TabNet","1dc6f8ab":"# **Import of Libraries**","939e780f":"<a id='5.4'><\/a>\n## <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">5.4 Stochastic Gradient Decent<\/p>","3def3c0f":"<a id='5.5'><\/a>\n## <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">5.5 XGB<\/p>","9e7d383f":"### Missing Values, how to handle\n\n* **Option 1: Fill NaN with Outlier or Zero**\n\nIn this specific example filling the missing value with an outlier value such as np.inf or 0 seems to be very naive. However, using values like -999, is sometimes a good idea.\n\n* **Option 2: Fill NaN with Mean Value**\n\nFilling NaNs with the mean value is also not sufficient and naive, and doesn't seems to be a good option.\n\n* **Option 3: Fill NaN with Last Value with .ffill()**\n\nFilling NaNs with the last value could be bit better.\n\n* **Option 4: Fill NaN with Linearly Interpolated Value with .interpolate()**\n\nFilling NaNs with the interpolated values is the best option in this small examlple but it requires knowledge of the neighouring values.","0aa09018":"<a id='6.1'><\/a>\n## <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">** Bonus Postprocessing<\/p>\n>\n>Speacial thanks to AMBROSM and the [notebook](https:\/\/www.kaggle.com\/ambrosm\/tpsnov21-007-postprocessing).","f4e82265":"<a id='2.1'><\/a>\n## <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">2.1 Handle Missing Values<\/p>","f0cbeb1e":"> **We have plotted Probability Density Function estimation for each feature. What does it tell us?**\n>* The features are distributed differently;\n>* More than half of the features are candle-like distributed (looks like Pearson Type 6 or even Poison distribution);\n>* The rest of the features are bell-shaped-like (e.g., Gaussian distribution);\n>* It might be a good idea to transform some of the features.","f05654a9":"### ***Model***","2fbe79a2":"> **Some chosen features are on a different scale and distribution.** (e.g. some features are distributed from 0 to 5 and \"f16\" is on a scale from -20 to 40)","23f37c2d":"> This is a pyTorch implementation of Tabnet [[2]](#8.2) by [**Dreamquark-ai**](https:\/\/github.com\/dreamquark-ai\/tabnet). \n>\n> TabNet uses a sequential attention mechanism to choose a subset of semantically meaningful features to process at each decision step.\n> According to the paper this model outperforms XGBoost:\n\n\n                   \n><style type=\"text\/css\">\n.tg  {border-collapse:collapse;border-color:#93a1a1;border-spacing:0;}\n.tg td{background-color:#fdf6e3;border-color:#93a1a1;border-style:solid;border-width:1px;color:#002b36;\n  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:5px 15px;word-break:normal;}\n.tg th{background-color:#657b83;border-color:#93a1a1;border-style:solid;border-width:1px;color:#fdf6e3;\n  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:5px 15px;word-break:normal;}\n.tg .tg-6f7u{background-color:#ffffff;border-color:#002b36;font-style:italic;font-weight:bold;text-align:center;vertical-align:top}\n.tg .tg-604u{background-color:#4db6ac;border-color:#002b36;text-align:center;vertical-align:top}\n.tg .tg-580s{background-color:#ffffff;border-color:#002b36;font-size:14px;text-align:center;vertical-align:top}\n.tg .tg-vgqm{background-color:#4db6ac;border-color:#002b36;font-size:14px;text-align:center;vertical-align:top}\n.tg .tg-xd2w{background-color:#ffffff;border-color:#002b36;text-align:center;vertical-align:top}\n<\/style>\n<table align=\"center\"class=\"tg\">\n<thead>\n  <tr>\n    <th class=\"tg-vgqm\"><span style=\"font-weight:bold;font-style:italic;color:#000\">Model<\/span><\/th>\n    <th class=\"tg-604u\"><span style=\"font-weight:bold;font-style:italic;color:#000\">Test MSE<\/span><\/th>\n  <\/tr>\n<\/thead>\n<tbody>\n  <tr>\n    <td class=\"tg-580s\">MLP<\/td>\n    <td class=\"tg-xd2w\">512.62<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-xd2w\">XGBoost<\/td>\n    <td class=\"tg-xd2w\">490.83<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-xd2w\">LightGBM<\/td>\n    <td class=\"tg-xd2w\">504.76<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-xd2w\">CatBoost<\/td>\n    <td class=\"tg-xd2w\">489.74<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-xd2w\"><span style=\"font-weight:bold;font-style:italic\">TabNet<\/span><\/td>\n    <td class=\"tg-6f7u\"><span style=\"font-weight:bold;font-style:italic\">485.12<\/span><\/td>\n  <\/tr>\n    <caption>Performance for Rossmann Store Sales dataset<\/caption>\n<\/tbody>\n<\/table>\n\n> The library provides very convinient way to fit and predict. Let's build our model:","61baa5e9":"> **We've been able to significantly shrink the magnitude of \"f2\".**\n> \n> Let's plot the Probability Density Function Estimation for the transformed features:","2e8b793b":"<a id='2.2'><\/a>\n## <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">2.2 Smoothing data \/ Reformating \/ Dropping <\/p>\n\n> **Let's drop \"id\" column since we do not really need it:**\n","6599c72d":"> **At first glance, the data is pretty heavy memory-wise.** \nNo doubts that float64 makes it heavier. let's take a closer look at dtypes:","15bba561":"### ***Model***","9ddcc964":"<a id='6'><\/a>\n# <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">6. Conclusions<\/p>","11c7138b":"### ***Dataset and Scaler***","38907a5b":"<a id='3'><\/a>\n# <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">3. Exploratory Data Analysis \ud83d\udcca <\/p>\n\n> **The problem statement of this competition says that the features are anonymized.** In reality, there are not many things we can observe and make decisions on. Let's take a closer look at the distribution of the features:","1dbb5133":"> **Let's check if we have any missing values:**","fbcd1dea":"> **Good news, our classes are well balanced!**\n\n> **Now we are going to plot some features (we go with 12) against its target values.**\n>\n> Additionally to the above, let's get 30000 sample for a faster run time.","0d9af135":"> Let's take a look at the coefficients:","d60ca767":"<a id='1'><\/a>\n# <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">1. Data visualization \ud83d\udcc8\ud83d\udcc9\ud83d\udcca<\/p>\n\n> **Let's read the data first** (I strongly recommend using 'datatable' to for faster data reading):","80af54da":"<a id='4.3'><\/a>\n## <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">4.3 Dropping features <\/p>\n\n> Another good idea is to test the feature importance by dropping it and monitoring the model performance at the same time.\n>\n> We will do it during modeling stage.","927f22a2":"<a id='4.1'><\/a>\n## <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">4.1 Changing functional dependency<\/p>\n\n> We have made and assumption that it might be a good idea to transform some of the features. Let's try to do it for all candle-like distributed features:","64c1c8c4":"> As it was proposed, base TabNet model outperformed all other models.\nOur submission to the competition resulted in scoring 248 position of 776 on the leaderboar. The competition is still running and the result might not last long. Anyway, not bad for the first attempt. Any suggestions to improve our score or performance will be greatly appreciated.\n>\n> Table Style and Table rendering can be found here: [**link**](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/style.html).\n>\n> P\/s If I have forgotten to reference someone's work, please, do not hesitate to leave your feedback.\nAny questions, suggestions or complaints are most welcome.","1a09da8d":"> The distributions of the transformed features looks more bell-shaped-like but still...\n> A good idea is to transform features separately, adding them into a model and monitor the model performance. It might be a very time consuming process taking into consideration not only log transformations. We will leave it as an exercise for the reader.","8ef62d2b":"<a id='5.6'><\/a>\n## <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">5.6 * Bonus Model: TabNet<\/p>","67c7e1fe":"> Special thanks to **EDUARDO GUTIERREZ** and his notebook for .describe method representation and styling. [**Eduardo's notebook**](https:\/\/www.kaggle.com\/eduardogutierrez\/tps-nov-21-exploratory-data-analysis). \n>\n> **What is the distribution of the features?**\n>* Features **\"f2\"**, **\"f35\"** and **\"f44\"** are of a higher magnitude comparing to other features;\n>* The standard deviation is in the range of .05 to 1.78 if we exclude above mentioned features;\n>* Some features are almost identical in terms of their scale and magnitude.","6716579d":"<a id='5.3'><\/a>\n## <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">5.3 Gaussian Naive Bayes<\/p>","5c0805af":"> **Having noticed float64 and int64 dtypes, we'd like to try downcast the dataset size for the sake of faster computation:**","d825b75d":"> If we wish to label the strength of the features association, for absolute values of correlation, **0-0.19** is regarded as very weak (our example is even weaker: **0-0.10**). ","6b017587":"> During feature engineering stage, we have **made the assumption that dropping some of the features** could improve the model performance.\n>\n> Let's go fancier (we are going to use our toy 30000 sample for a faster run time):","662a7904":"<a id='4'><\/a>\n# <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">4. Feature engineering \ud83d\udd27<\/p>\n\n> [**Feature engineering**](https:\/\/www.omnisci.com\/technical-glossary\/feature-engineering#:~:text=Feature%20engineering%20refers%20to%20the,machine%20learning%20or%20statistical%20modeling.) refers to the process of using domain knowledge to select and transform the most relevant variables from raw data.\n","024c8f3d":"> **We have substantially reduced the size of dataset:**\n>* Memory usage train: 243.00 MB. MB against 489.60 MB; \n>* Memory usage test: 218.16 MB MB against 436.32. \n> \n> It is always a good idea to bring your dataset to a right dtype format.","a0472397":"> Well done! We've been able to improve performance of the 30000 sample data set by removing 'f13' feature. \n>\n> Additionally to the above, the features can be checked in pairs recursively. We are not going to do it here.","fa6ec6d7":"> **Let's us take a look at features correlation matrix**:","1ff716cf":"<a id='4.2'><\/a>\n## <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">4.2 Creating aggregated features<\/p>\n\n> One of the naive approach to engineer features, is to aggregate them.  \n> Special thanks to **RAHUL YADAV** and his notebook. [**Rahul's notebook**](https:\/\/www.kaggle.com\/rahullalu\/tps-nov-2021-eda-and-baseline#Approach).\n>\n> The problem with aggregation is that we might encounter **multicollinearity** (e.g., the high correlation of the explanatory variables). \"It should be noted that the presence of multicollinearity does not mean that the model is\nmisspecified. You only start to talk about it when you think that it is\naffecting the regression results seriously.\" [[1]](#8.1)\n>\n> Now let's create our aggregated features:\n","a8dcfb24":"<a id='5.2'><\/a>\n## <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">5.2 Linear SVC<\/p>","40c8b525":"<a id='8.1'><\/a>\n<p >[1] C. Dougherty. Introduction to Econometrics 5th edition, pages 171-174, 2016.<\/p>\n<a id='8.2'><\/a>\n<p >[2] Arik, S. O., & Pfister, T. (2019). TabNet: Attentive Interpretable Tabular Learning. arXiv preprint arXiv:1908.07442.<\/p> \n","da751feb":"> **Let's take a closer look at the distribution of the features:**","a2e09e84":"### ***Model***","34185fc3":"<a id='7'><\/a>\n# <p style=\"background-color:#4DB6AC; font-family:newtimeroman; font-size:120%; text-align:center\">7. References<\/p>","c1f8c600":"> Special thanks to [ORMALKA](https:\/\/www.kaggle.com\/ormalka) for finding \"deepcopy\" bug. Greatly appreciated!"}}