{"cell_type":{"a7b4e5a3":"code","a1d62b71":"code","03aac16b":"code","7ce99e0c":"code","85d15971":"code","4363e83f":"code","c938e281":"code","afb3f751":"code","bc43cb43":"code","a9d17492":"code","eb0ad98a":"code","d003a897":"code","cfd763d7":"code","86fbf105":"code","427a6661":"code","e730fbb3":"code","d510b337":"code","7eb5db0b":"code","f34a303e":"code","5e83e07b":"code","b0ee7120":"code","2535d782":"code","a13a7e3f":"code","dfdb555a":"code","ee7036c7":"code","a2e52301":"code","d76e6e3a":"code","578bb68f":"code","352bfd66":"code","cd19068e":"code","a736b215":"code","017e9c55":"code","d51dde8a":"code","3c78f259":"code","1fb8b0bf":"code","dcfe9b8a":"code","e07f2e6a":"code","07418c1c":"code","be0b8b73":"code","f231e9d3":"code","157debb3":"code","e32fb177":"code","72f23b9b":"code","c9057fb1":"code","417d4a5f":"code","92af59fe":"code","b965febf":"code","50f4ff4a":"code","809128d8":"code","77b7132b":"code","b7c13b3e":"code","3adc75bd":"code","51dff181":"code","9c792b49":"code","bfbcd832":"code","c2d8e145":"code","d2b576ab":"code","d569deb2":"code","3fdcc07a":"code","0d7b8d84":"code","be8e2a2f":"code","2be7bb97":"code","0bfa5075":"code","35584135":"markdown","ccae6f4f":"markdown","c011a0f4":"markdown","6f837b2d":"markdown","460241f3":"markdown","9d60a8be":"markdown","ee070cbc":"markdown","bc12f28f":"markdown","1cf2bb17":"markdown","cf1f07b6":"markdown","5b6da262":"markdown","a8c1e744":"markdown","8d92609a":"markdown","6b5bc196":"markdown","ab6e6460":"markdown","d93e4987":"markdown","3740e6a3":"markdown","ea17b354":"markdown","e7e08ff6":"markdown","2e5a5e48":"markdown","f327b52e":"markdown","56544412":"markdown","7e282a26":"markdown","434a6266":"markdown","45d3cd48":"markdown","c28de980":"markdown","29bb10b7":"markdown","e7f0c07c":"markdown","a3d0dfdc":"markdown","24e63e12":"markdown","50869c6a":"markdown","ae5d0f94":"markdown","ba749f01":"markdown","62d7acd9":"markdown","493a4f96":"markdown","13dbfc3c":"markdown","4007f5ab":"markdown","82ccd3bf":"markdown","8276c05a":"markdown","a3c07593":"markdown","34cf8c2d":"markdown"},"source":{"a7b4e5a3":"!pip install dataprep","a1d62b71":"# Standard library\nimport math\nimport random\n\n# 3rd party library\nfrom catboost import CatBoostClassifier\nfrom catboost import Pool\nfrom dataprep import eda\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np \nimport optuna\nfrom optuna import create_study, logging\nfrom optuna.pruners import MedianPruner\nimport pandas as pd\nimport plotly.express as px\nimport seaborn as sns\nimport sklearn\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, KFold, GroupKFold, StratifiedKFold\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom optuna.integration import XGBoostPruningCallback\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.preprocessing import LabelEncoder\nimport umap\nimport xgboost as xgb","03aac16b":"# Fix seed\n\ndef fix_seed(seed):\n    # random\n    random.seed(seed)\n    # Numpy\n    np.random.seed(seed)\n\nSEED = 46\nfix_seed(SEED)","7ce99e0c":"# Load csv data of this competition.\n\nDATA = \"..\/input\/tabular-playground-series-jun-2021\"\ntrain = pd.read_csv(DATA + \"\/train.csv\")\ntest = pd.read_csv(DATA + \"\/test.csv\")","85d15971":"# Remove the ID column as it is in the way.\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)","4363e83f":"train.head()","c938e281":"test.head()","afb3f751":"train.describe().T.style.bar(subset=['mean'], color='#20c8f2')\\\n                   .background_gradient(subset=['std'], cmap='YlGn')","bc43cb43":"train.info()","a9d17492":"pd.DataFrame(train.isna().sum()\/len(train), columns=[\"missing_rate\"])\\\n                        .style.bar(subset=['missing_rate'], color='#20c8f2')","eb0ad98a":"pd.DataFrame((train==0).sum()\/len(train), columns=[\"zero_rate\"])\\\n    .style.bar(subset=['zero_rate'], color='#20c8f2')","d003a897":"test.describe().T.style.bar(subset=['mean'], color='#20c8f2')\\\n                 .background_gradient(subset=['std'], cmap='YlGn')","cfd763d7":"test.info()","86fbf105":"pd.DataFrame(test.isna().sum()\/len(test), columns=[\"missing_rate\"])\\\n                        .style.bar(subset=['missing_rate'], color='#20c8f2')","427a6661":"pd.DataFrame((test==0).sum()\/len(test), columns=[\"zero_rate\"])\\\n    .style.bar(subset=['zero_rate'], color='#20c8f2')","e730fbb3":"def multiclass_log_loss(y_pred, y_true):\n    score = sum([math.log(pred[label]) for pred, label in zip(y_pred, y_true)])\n    return - score \/ len(y_true)","d510b337":"# Separate X and y.\n\nfeature_cols = [col for col in train.columns if col != \"target\"]\ntarget_cat = train[\"target\"]\ndf = train.drop(\"target\", axis=1)","7eb5db0b":"df.head()","f34a303e":"# I'll try lavel encoding because targets are string.\nle = LabelEncoder()\ntarget = le.fit_transform(target_cat)","5e83e07b":"print(\"-\"*30)\nprint(\"Before label encoding, \")\nprint(target_cat[:10])\nprint(\"-\"*30)\nprint(\"After label encoding, \")\nprint(target[:10])\nprint(\"-\"*30)","b0ee7120":"# I refered https:\/\/www.kaggle.com\/subinium\/tps-may-categorical-eda\n\nplt.style.use(\"Solarize_Light2\")\nprint(f\"Orange is train, and blue is test data.\")\n\nfig, axes = plt.subplots(19, 4, figsize=(15, 30), gridspec_kw=dict(wspace=0.3, hspace=0.6))\nfor col, ax in zip(feature_cols, axes.flatten()):\n    \n    sns.kdeplot(x=df[col], ax=ax, alpha=0.5, fill=True, linewidth=0.6, color='orange')\n    sns.kdeplot(x=test[col], ax=ax, alpha=0.1, fill=True, linewidth=0.6)","2535d782":"plt.figure(figsize=(13, 8))\ng = sns.countplot(target_cat, order=[f\"Class_{i}\" for i in range(1, 10)])\ng.tick_params(labelsize=14)\ng.set_xlabel(\"target\",fontsize=20)\ng.set_ylabel(\"Count\",fontsize=20)\ng.set_title(\"Count plot for target of train data\",fontsize=25)","a13a7e3f":"def extract_tril_without_diagonal(corr_matrix):\n    return np.tril(corr_matrix) - np.triu(np.tril(corr_matrix))\n\nfig = px.imshow(extract_tril_without_diagonal(df.corr().values),\n                x=feature_cols, y=feature_cols, width=700, height=700)\nfig.update_layout(title='Correlation between features')\nfig.show()","dfdb555a":"eda.create_report(df,display=[\"Interactions\"])","ee7036c7":"# I refered https:\/\/www.kaggle.com\/kushal1506\/deciding-n-components-in-pca\n\npca = PCA().fit(df)\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (15,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, len(feature_cols)+1, step=1)\ny = np.cumsum(pca.explained_variance_ratio_)\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, len(feature_cols), step=2))\n\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()","a2e52301":"reducer = umap.UMAP()\nembedding = reducer.fit_transform(df)","d76e6e3a":"plt.scatter(\n    embedding[:, 0],\n    embedding[:, 1],\n    c=target,\n    s=1,\n    alpha=0.5)\nplt.gca().set_aspect('equal', 'datalim')\nplt.title('UMAP projection of train data', fontsize=15)","578bb68f":"pca = PCA(n_components=54).fit(df)\ndf_pca = pca.transform(df)\n\nreducer = umap.UMAP()\nembedding = reducer.fit_transform(df_pca)\n\nplt.scatter(\n    embedding[:, 0],\n    embedding[:, 1],\n    c=target,\n    s=1,\n    alpha=0.5)\nplt.gca().set_aspect('equal', 'datalim')\nplt.title('UMAP projection of train data after PCA', fontsize=15)","352bfd66":"# Refered Code Examples of https:\/\/optuna.org\/\n\ndef quadratic_function(x, y):\n    \"\"\"Calculate quadratic_function (x -2 )^2 + (y - 3)^2\n    \"\"\"\n    return (x - 2) ** 2 + (y - 3)**2\n\ndef objective(trial):\n    x = trial.suggest_uniform('x', -10, 10)\n    y = trial.suggest_uniform('y', -10, 10)\n    return quadratic_function(x, y)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100)","cd19068e":"study.best_params","a736b215":"x = np.arange(-1.0, 5.0, 0.1)\ny = np.arange(0., 5.0, 0.1)\nX, Y = np.meshgrid(x, y)\nZ = quadratic_function(X, Y)\n\nbest_x = study.best_params[\"x\"]\nbest_y = study.best_params[\"y\"]\nbest_z = quadratic_function(best_x, best_y)\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_zlabel(\"f(x, y)\")\nax.plot_wireframe(X, Y, Z, alpha=0.1)\nax.scatter3D([best_x], [best_y], [best_z],  c='Red', s=100);\nplt.show()","017e9c55":"display(optuna.visualization.plot_optimization_history(study))\ndisplay(optuna.visualization.plot_slice(study))\ndisplay(optuna.visualization.plot_parallel_coordinate(study))","d51dde8a":"# \u2193\u2193\u2193 You can change number of trial. Now, to keep time, I'll set small number.\n\ntrials_catboost = 5\ntrials_histgradientboost = 5\ntrials_xgboost = 5","3c78f259":"# I refered the great notebook which wrote pipeline to optimize xgboost training with optuna,\n# but I can't refer my voted notebooks list...\n# If I could find it, I'll note the URL.\n\ndef train_and_val_catboost(df, target, params, n_splits=3):\n    \"\"\"Calculate and return validation score averaged of CatBoostClassifier n_splits times tried with kfold.\n    \"\"\"\n    test_preds = None\n    train_mertics = 0\n    val_mertics = 0 \n    \n    kf = KFold(n_splits = n_splits , shuffle = True , random_state = 42)\n    for fold, (tr_index , val_index) in enumerate(kf.split(df.values , target)):\n        print(\"-\" * 50)\n        print(f\"Fold {fold + 1}\")\n    \n        x_train,x_val = df.values[tr_index] , df.values[val_index]\n        y_train,y_val = target[tr_index] , target[val_index]\n        \n        train_dataset = Pool(data=x_train,\n                     label=y_train)\n        eval_data = Pool(data=x_val,\n                     label=y_val)\n    \n        model = CatBoostClassifier(**params)\n        model.fit(train_dataset, eval_set = eval_data, verbose = 100)\n    \n        train_preds = model.predict_proba(x_train)\n        train_mertics += multiclass_log_loss(train_preds, y_train)\n        print(\"Training Metric : \" , multiclass_log_loss(train_preds, y_train))\n    \n        val_preds = model.predict_proba(x_val)\n        val_mertics += multiclass_log_loss(val_preds, y_val)\n        print(\"Validation Metric : \" , multiclass_log_loss(val_preds, y_val))\n    \n        if test_preds is None:\n            test_preds = model.predict_proba(test.values)\n        else:\n            test_preds += model.predict_proba(test.values)\n\n    print(\"-\" * 50)\n    print(\"Average Training Metric : \" , train_mertics \/ n_splits)\n    print(\"Average Validation Metric : \" , val_mertics \/ n_splits)\n\n    return val_mertics \/ n_splits","1fb8b0bf":"def objective_catboost(trial, df, target, params=dict()):\n    \"\"\" Set optimize target parameters & its' sampling\n    \"\"\"\n    \n    # Tuning target\n    params['max_depth'] = trial.suggest_int('max_depth', 2, 8)\n    params['n_estimators'] = trial.suggest_int('n_estimators', 500, 1500)\n    params['bagging_temperature'] = trial.suggest_uniform('bagging_temperature', 0.5, 10)\n    params['learning_rate'] = trial.suggest_uniform('learning_rate', 0.01, 0.15)\n\n    return train_and_val_catboost(df, target, params, n_splits=3)","dcfe9b8a":"def execute_optimization(study_name, df, target, trials,\n                                   params=dict(), direction='minimize'):\n    \"\"\" Execute optimization for objective_catboost\n    \"\"\"\n    logging.set_verbosity(logging.ERROR)\n    \n    ## We use pruner to skip trials that are NOT fruitful\n    pruner = MedianPruner(n_warmup_steps=5)\n    \n    study = create_study(direction=direction,\n                         study_name=study_name,\n                         storage=f'sqlite:\/\/\/optuna_{study_name}.db',\n                         load_if_exists=False,\n                         pruner=pruner)\n\n    study.optimize(lambda trial: objective_catboost(trial, df, target, params),\n                   n_trials=trials,\n                   n_jobs=-1)\n    \n    \n    print(\"STUDY NAME: \", study_name)\n    print('------------------------------------------------')\n    print(\"EVALUATION METRIC: \", multiclass_log_loss.__name__)\n    print('------------------------------------------------')\n    print(\"BEST CV SCORE\", study.best_value)\n    print('------------------------------------------------')\n    print(f\"OPTIMAL PARAMS: \", study.best_params)\n    print('------------------------------------------------')\n    print(\"BEST TRIAL\", study.best_trial)\n    print('------------------------------------------------')\n    \n    \n    return study.best_params, study","e07f2e6a":"params_catboost, study_catboost = execute_optimization(\"catboost_tuning\", df, target, trials_catboost)","07418c1c":"print(params_catboost)","be0b8b73":"display(optuna.visualization.plot_optimization_history(study_catboost))\ndisplay(optuna.visualization.plot_slice(study_catboost))\ndisplay(optuna.visualization.plot_parallel_coordinate(study_catboost))","f231e9d3":"def train_and_val_histgradientboost(df, target, params, n_splits=3):\n    \"\"\"Calculate and return validation score  of HistGradientBoostingClassifier averaged n_splits times tried with kfold.\n    \"\"\"\n    test_preds = None\n    train_mertics = 0\n    val_mertics = 0\n    \n    kf = KFold(n_splits = n_splits , shuffle = True , random_state = 42)\n    for fold, (tr_index , val_index) in enumerate(kf.split(df.values , target)):\n        print(\"-\" * 50)\n        print(f\"Fold {fold + 1}\")\n    \n        x_train,x_val = df.values[tr_index] , df.values[val_index]\n        y_train,y_val = target[tr_index] , target[val_index]\n    \n        model = HistGradientBoostingClassifier(**params)\n        model.fit(x_train, y_train)\n    \n        train_preds = model.predict_proba(x_train)\n        train_mertics += multiclass_log_loss(train_preds, y_train)\n        print(\"Training Metric : \" , multiclass_log_loss(train_preds, y_train))\n    \n        val_preds = model.predict_proba(x_val)\n        val_mertics += multiclass_log_loss(val_preds, y_val)\n        print(\"Validation Metric : \" , multiclass_log_loss(val_preds, y_val))\n    \n        if test_preds is None:\n            test_preds = model.predict_proba(test.values)\n        else:\n            test_preds += model.predict_proba(test.values)\n\n    print(\"-\" * 50)\n    print(\"Average Training Metric : \" , train_mertics \/ n_splits)\n    print(\"Average Validation Metric : \" , val_mertics \/ n_splits)\n\n    return val_mertics \/ n_splits\n\n\ndef objective_histgradientboost(trial, df, target, params=dict()):\n    \"\"\" Set optimize target parameters & its' sampling\n    \"\"\"  \n\n    # Tuning target\n    params['max_depth'] = trial.suggest_int('max_depth', 2, 8)\n    params['l2_regularization'] = trial.suggest_uniform('l2_regularization', 0, 1)\n    params['learning_rate'] = trial.suggest_uniform('learning_rate', 0.05, 0.5)\n\n    return train_and_val_histgradientboost(df, target, params, n_splits=3)\n\n\ndef execute_optimization(study_name, df, target, trials,\n                                   params=dict(), direction='minimize'):\n    \"\"\" Execute optimization for objective_histgradientboost\n    \"\"\"\n    \n    logging.set_verbosity(logging.ERROR)\n    \n    ## We use pruner to skip trials that are NOT fruitful\n    pruner = MedianPruner(n_warmup_steps=5)\n    \n    study = create_study(direction=direction,\n                         study_name=study_name,\n                         storage=f'sqlite:\/\/\/optuna_{study_name}.db',\n                         load_if_exists=False,\n                         pruner=pruner)\n\n    study.optimize(lambda trial: objective_histgradientboost(trial, df, target, params),\n                   n_trials=trials,\n                   n_jobs=-1)\n    \n    \n    print(\"STUDY NAME: \", study_name)\n    print('------------------------------------------------')\n    print(\"EVALUATION METRIC: \", multiclass_log_loss.__name__)\n    print('------------------------------------------------')\n    print(\"BEST CV SCORE\", study.best_value)\n    print('------------------------------------------------')\n    print(f\"OPTIMAL PARAMS: \", study.best_params)\n    print('------------------------------------------------')\n    print(\"BEST TRIAL\", study.best_trial)\n    print('------------------------------------------------')\n    \n    \n    return study.best_params, study","157debb3":"params_histgradientboost, study_histgradientboost = execute_optimization(\"histgradientboost_tuning\", df, target, trials_histgradientboost)","e32fb177":"print(params_histgradientboost)","72f23b9b":"display(optuna.visualization.plot_optimization_history(study_histgradientboost))\ndisplay(optuna.visualization.plot_slice(study_histgradientboost))\ndisplay(optuna.visualization.plot_parallel_coordinate(study_histgradientboost))","c9057fb1":"def train_and_val_xgboost(df, target, params, n_splits=3):\n    \"\"\"Calculate and return validation score  of HistGradientBoostingClassifier averaged n_splits times tried with kfold.\n    \"\"\"\n    dtrain = xgb.DMatrix(df, label=target)\n    \n    pruning_callback = XGBoostPruningCallback(trial, \"test-mlogloss\")\n    cv_scores = xgb.cv(params, dtrain, nfold=n_splits,\n                       stratified=True,\n                       metrics = \"mlogloss\",\n                       early_stopping_rounds=50,\n                       callbacks=[pruning_callback],\n                       seed=0)\n\n    return cv_scores['test-' + \"mlogloss\" + '-mean'].values[-1]\n\n\ndef objective_xgboost(trial, df, target, params=dict()):\n    \"\"\" Set optimize target parameters & its' sampling\n    \"\"\"  \n    \n    params['num_class'] = 9\n\n    # Tuning target\n    params['max_depth'] = trial.suggest_int('max_depth', 2, 10)\n    params['learning_rate'] = trial.suggest_uniform('learning_rate', 0, 0.1)\n    params['num_boost_round'] = trial.suggest_int('num_boost_round', 100, 1000)\n\n    return train_and_val_xgboost(df, target, params, n_splits=3)\n\n\ndef execute_xgboost(study_name, df, target, trials,\n                                   params=dict(), direction='minimize'):\n    \"\"\" Execute optimization for objective_histgradientboost\n    \"\"\"\n    \n    logging.set_verbosity(logging.ERROR)\n    \n    ## We use pruner to skip trials that are NOT fruitful\n    pruner = MedianPruner(n_warmup_steps=5)\n    \n    study = create_study(direction=direction,\n                         study_name=study_name,\n                         storage=f'sqlite:\/\/\/optuna_{study_name}.db',\n                         load_if_exists=False,\n                         pruner=pruner)\n\n    study.optimize(lambda trial: objective_xgboost(trial, df, target, params),\n                   n_trials=trials,\n                   n_jobs=-1)\n    \n    \n    print(\"STUDY NAME: \", study_name)\n    print('------------------------------------------------')\n    print(\"EVALUATION METRIC: \", multiclass_log_loss.__name__)\n    print('------------------------------------------------')\n    print(\"BEST CV SCORE\", study.best_value)\n    print('------------------------------------------------')\n    print(f\"OPTIMAL PARAMS: \", study.best_params)\n    print('------------------------------------------------')\n    print(\"BEST TRIAL\", study.best_trial)\n    print('------------------------------------------------')\n    \n    \n    return study.best_params, study","417d4a5f":"params_xgboost, study_xgboost = execute_optimization(\"xgboost_tuning\", df, target, trials_xgboost)","92af59fe":"print(params_xgboost)","b965febf":"display(optuna.visualization.plot_optimization_history(study_xgboost))\ndisplay(optuna.visualization.plot_slice(study_xgboost))\ndisplay(optuna.visualization.plot_parallel_coordinate(study_xgboost))","50f4ff4a":"# See https:\/\/www.kaggle.com\/nayuts\/tps-06-solution-assortment-eda-optuna-ensemble?scriptVersionId=65714768\n\nparams_catboost = {'bagging_temperature': 1.5451428810065613, 'learning_rate': 0.04814888472822457, 'max_depth': 5, 'n_estimators': 1483}\nparams_histgradientboost = {'l2_regularization': 0.5629424804207567, 'learning_rate': 0.05065982344408913, 'max_depth': 6}\nparams_xgboost = {'l2_regularization': 0.591214850198673, 'learning_rate': 0.4895203779149179, 'max_depth': 3}","809128d8":"test_preds_catboost = None\ntrain_metric = 0\nval_metric = 0\nn_splits = 7\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = test.columns\n\nkf = KFold(n_splits = n_splits , shuffle = True , random_state = 42)\nfor fold, (tr_index , val_index) in enumerate(kf.split(df.values , target)):\n    \n    print(\"-\" * 50)\n    print(f\"Fold {fold + 1}\")\n    \n    x_train,x_val = df.values[tr_index] , df.values[val_index]\n    y_train,y_val = target[tr_index] , target[val_index]\n        \n    eval_set = [(x_val, y_val)]\n    \n    model = CatBoostClassifier(**params_catboost)\n    model.fit(x_train, y_train, eval_set = eval_set, verbose = 100)\n    \n    train_preds = model.predict_proba(x_train)\n    train_metric += multiclass_log_loss(train_preds, y_train)\n    print(\"Training Metric : \" , multiclass_log_loss(train_preds, y_train))\n    \n    feature_importances[f'fold_{fold}'] = model.feature_importances_\n    \n    val_preds = model.predict_proba(x_val)\n    val_metric += multiclass_log_loss(val_preds, y_val)\n    print(\"Validation Metric : \" , multiclass_log_loss(val_preds, y_val))\n    \n    if test_preds_catboost is None:\n        test_preds_catboost = model.predict_proba(test.values)\n    else:\n        test_preds_catboost += model.predict_proba(test.values)\n\nprint(\"-\" * 50)\nprint(\"Average Training Metric : \" , train_metric \/ n_splits)\nprint(\"Average Validation Metric : \" , val_metric \/ n_splits)\n\ntest_preds_catboost \/= n_splits","77b7132b":"# Exporting results\nsub_catboost = pd.read_csv(DATA + \"\/sample_submission.csv\")\n\nsub_catboost['Class_1']=test_preds_catboost[:,0]\nsub_catboost['Class_2']=test_preds_catboost[:,1]\nsub_catboost['Class_3']=test_preds_catboost[:,2]\nsub_catboost['Class_4']=test_preds_catboost[:,3]\nsub_catboost['Class_5']=test_preds_catboost[:,4]\nsub_catboost['Class_6']=test_preds_catboost[:,5]\nsub_catboost['Class_7']=test_preds_catboost[:,6]\nsub_catboost['Class_8']=test_preds_catboost[:,7]\nsub_catboost['Class_9']=test_preds_catboost[:,8]\n\nsub_catboost.to_csv(\"CatBoost.csv\",index=False)","b7c13b3e":"# I refered https:\/\/www.kaggle.com\/gogo827jz\/catboost-baseline-with-feature-importance\n\n# Calculate the average feature importance for each feature\nfeature_importances['average'] = feature_importances[[f'fold_{fold}' for fold in range(n_splits)]].mean(axis=1)\nfeature_importances.to_csv('feature_importances_catboost.csv')\nfeature_importances.sort_values(by='average', ascending=False).head()","3adc75bd":"# Plot the feature importances with min\/max\/average using seaborn\nfeature_importances_flatten = pd.DataFrame()\nfor i in range(1, len(feature_importances.columns)-1):\n    col = ['feature', feature_importances.columns.values[i]]\n    feature_importances_flatten = pd.concat([feature_importances_flatten, feature_importances[col].rename(columns={f'fold_{i-1}': 'importance'})], axis=0)\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances_flatten.sort_values(by='importance', ascending=False), x='importance', y='feature')\nplt.title(f'Feature Importances over {n_splits} folds of CatBoostClassifier')  \nplt.savefig(\"feature_importances_catboost.png\")","51dff181":"test_preds_histgradientboost = None\ntrain_metric = 0\nval_metric = 0\nn_splits = 7\n\nkf = KFold(n_splits = n_splits , shuffle = True , random_state = 46)\nfor fold, (tr_index , val_index) in enumerate(kf.split(df.values , target)):\n    \n    print(\"-\" * 50)\n    print(f\"Fold {fold + 1}\")\n    \n    x_train,x_val = df.values[tr_index] , df.values[val_index]\n    y_train,y_val = target[tr_index] , target[val_index]\n        \n    eval_set = [(x_val, y_val)]\n    \n    model = HistGradientBoostingClassifier(**params_histgradientboost)\n    model.fit(x_train, y_train)\n    \n    train_preds = model.predict_proba(x_train)\n    train_metric += multiclass_log_loss(train_preds, y_train)\n    print(\"Training Metric : \" , multiclass_log_loss(train_preds, y_train))\n    \n    val_preds = model.predict_proba(x_val)\n    val_metric += multiclass_log_loss(val_preds, y_val)\n    print(\"Validation Metric : \" , multiclass_log_loss(val_preds, y_val))\n    \n    if test_preds_histgradientboost is None:\n        test_preds_histgradientboost = model.predict_proba(test.values)\n    else:\n        test_preds_histgradientboost += model.predict_proba(test.values)\n\nprint(\"-\" * 50)\nprint(\"Average Training Metric : \" , train_metric \/ n_splits)\nprint(\"Average Validation Metric : \" , val_metric \/ n_splits)\n\ntest_preds_histgradientboost \/= n_splits","9c792b49":"sub_histgradientboost = pd.read_csv(DATA + \"\/sample_submission.csv\")\n\nsub_histgradientboost['Class_1']=test_preds_histgradientboost[:,0]\nsub_histgradientboost['Class_2']=test_preds_histgradientboost[:,1]\nsub_histgradientboost['Class_3']=test_preds_histgradientboost[:,2]\nsub_histgradientboost['Class_4']=test_preds_histgradientboost[:,3]\nsub_histgradientboost['Class_5']=test_preds_histgradientboost[:,4]\nsub_histgradientboost['Class_6']=test_preds_histgradientboost[:,5]\nsub_histgradientboost['Class_7']=test_preds_histgradientboost[:,6]\nsub_histgradientboost['Class_8']=test_preds_histgradientboost[:,7]\nsub_histgradientboost['Class_9']=test_preds_histgradientboost[:,8]\n\nsub_histgradientboost.to_csv(\"HistGradientBoost.csv\",index=False)","bfbcd832":"print(sklearn.__version__)","c2d8e145":"test_preds_xgboost = None\ntrain_metric = 0\nval_metric = 0\nn_splits = 7\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = test.columns\n\nkf = KFold(n_splits = n_splits , shuffle = True , random_state = 46)\nfor fold, (tr_index , val_index) in enumerate(kf.split(df.values , target)):\n    \n    print(\"-\" * 50)\n    print(f\"Fold {fold + 1}\")\n    \n    x_train,x_val = df.values[tr_index] , df.values[val_index]\n    y_train,y_val = target[tr_index] , target[val_index]\n        \n    eval_set = [(x_val, y_val)]\n    \n    model = xgb.XGBClassifier(**params_xgboost, random_state=46 , n_jobs=-1)\n    model.fit(x_train, y_train)\n    \n    train_preds = model.predict_proba(x_train)\n    train_metric += multiclass_log_loss(train_preds, y_train)\n    print(\"Training Metric : \" , multiclass_log_loss(train_preds, y_train))\n    \n    feature_importances[f'fold_{fold}'] = model.feature_importances_\n    \n    val_preds = model.predict_proba(x_val)\n    val_metric += multiclass_log_loss(val_preds, y_val)\n    print(\"Validation Metric : \" , multiclass_log_loss(val_preds, y_val))\n    \n    if test_preds_xgboost is None:\n        test_preds_xgboost = model.predict_proba(test.values)\n    else:\n        test_preds_xgboost += model.predict_proba(test.values)\n\nprint(\"-\" * 50)\nprint(\"Average Training Metric : \" , train_metric \/ n_splits)\nprint(\"Average Validation Metric : \" , val_metric \/ n_splits)\n\ntest_preds_xgboost \/= n_splits","d2b576ab":"# Exporting results\n\nsub_xgboost = pd.read_csv(DATA + \"\/sample_submission.csv\")\n\nsub_xgboost['Class_1']=test_preds_xgboost[:,0]\nsub_xgboost['Class_2']=test_preds_xgboost[:,1]\nsub_xgboost['Class_3']=test_preds_xgboost[:,2]\nsub_xgboost['Class_4']=test_preds_xgboost[:,3]\nsub_xgboost['Class_5']=test_preds_xgboost[:,4]\nsub_xgboost['Class_6']=test_preds_xgboost[:,5]\nsub_xgboost['Class_7']=test_preds_xgboost[:,6]\nsub_xgboost['Class_8']=test_preds_xgboost[:,7]\nsub_xgboost['Class_9']=test_preds_xgboost[:,8]\n\nsub_xgboost.to_csv(\"XGBoost.csv\",index=False)","d569deb2":"# I refered https:\/\/www.kaggle.com\/gogo827jz\/catboost-baseline-with-feature-importance\n\n# Calculate the average feature importance for each feature\nfeature_importances['average'] = feature_importances[[f'fold_{fold}' for fold in range(n_splits)]].mean(axis=1)\nfeature_importances.to_csv('feature_importances_xgboost.csv')\nfeature_importances.sort_values(by='average', ascending=False).head()","3fdcc07a":"# Plot the feature importances with min\/max\/average using seaborn\nfeature_importances_flatten = pd.DataFrame()\nfor i in range(1, len(feature_importances.columns)-1):\n    col = ['feature', feature_importances.columns.values[i]]\n    feature_importances_flatten = pd.concat([feature_importances_flatten, feature_importances[col].rename(columns={f'fold_{i-1}': 'importance'})], axis=0)\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances_flatten.sort_values(by='importance', ascending=False), x='importance', y='feature')\nplt.title(f'Feature Importances over {n_splits} folds of XGBoost')  \nplt.savefig(\"feature_importances_xgboost.png\")","0d7b8d84":"# Zero matrix to hold the values\ntest_preds_blended =  np.zeros_like(test_preds_catboost, dtype=\"float64\")\n\n# Weights for mixing results\nweights = {\"catboost\": 0.4,\n           \"histgradientboost\": 0.3,\n           \"xgboost\": 0.3}\n\n# Ensemble targets\npreds = [test_preds_catboost, test_preds_histgradientboost, test_preds_xgboost]","be8e2a2f":"# Ensemble\n\nfor pred, weight in zip(preds, weights.values()):\n    test_preds_blended += pred * weight","2be7bb97":"# Exporting results\n\nsub_blended = pd.read_csv(DATA + \"\/sample_submission.csv\")\n\nsub_blended['Class_1']=test_preds_blended[:,0]\nsub_blended['Class_2']=test_preds_blended[:,1]\nsub_blended['Class_3']=test_preds_blended[:,2]\nsub_blended['Class_4']=test_preds_blended[:,3]\nsub_blended['Class_5']=test_preds_blended[:,4]\nsub_blended['Class_6']=test_preds_blended[:,5]\nsub_blended['Class_7']=test_preds_blended[:,6]\nsub_blended['Class_8']=test_preds_blended[:,7]\nsub_blended['Class_9']=test_preds_blended[:,8]\n\nsub_blended.to_csv(\"Blended.csv\",index=False)","0bfa5075":"sub_blended.head()","35584135":"Here is best parameter.","ccae6f4f":"Let's execute optimization.","c011a0f4":"Specifies the hyperparameters to explore and their ranges. Note whether the parameter is a real number or an integer.","6f837b2d":"<a id='4'><\/a>\n# <div class=\"alert alert-block alert-success\">Hyperparameter tuning with optuna<\/div>\n\nWe will use [CatBoost](https:\/\/catboost.ai\/) for the model. This is because I felt that CatBoost performed well in the competition in May. \n\nWe will see how to tune the hyperparameters using [Optuna](https:\/\/optuna.org\/).\n\nWith Optuna, you can efficiently search for good hyperparameter values with a small number of trials.","460241f3":"## PCA Result\n\nBecause of the large number of features, you may want to use PCA to reduce the number of dimensions. We will also check the cumulative contribution ratio. If we take roughly 54 components, we can see that there is a 95% contribution rate.","9d60a8be":"We also look at the results after reducing the dimension in PCA, but it did not change much.","ee070cbc":"It seems that there are many columns with a value of 0. However, in rare cases, there are columns that have no 0 at all.","bc12f28f":"## Metric\n\nThe metric used for evaluation is multi-class logarithmic loss.\n\n$$\n   logloss = -\\frac{1}{N}\\sum^{N}_{i-1}\\sum^{M}_{j-1}y_{ij}\\log(p_{ij})\n$$\n\nHere N is the number of rows, M is  is the number of class labels, i is the index of data and j is the index of class. \n\nIn this case, we will use predict_proba() in the classifier model as the probability of each class for output , but I made my own evaluation function because it was useful for CV.","1cf2bb17":"### Simple example\n\nLet's see simple optimization example for following quadratic_function.\n$$\nz = (x -2 )^2 + (y - 3)^2\n$$","cf1f07b6":"<a id='5'><\/a>\n# <div class=\"alert alert-block alert-success\">Train & Inference<\/div>\n\nUsing the tuned parameters, try to create the data to be submitted.","5b6da262":"<a id='2'><\/a>\n# <div class=\"alert alert-block alert-success\">Data Overview<\/div>\n\nLet's see stats, missings and zeros.\n\nThe output is vertically long and difficult to check. So we need to check it, but for those where the result is known to some extent, the Outputs are hidden. If you are interested, please open them and have a look.","a8c1e744":"The red dot is the minimum point explored by Optuna. The next optimization we do is more difficult than this one, but it does the same thing: we'll search for the hyperparameter that minimizes the CV score.","8d92609a":"Unfortunately, the HisTGradientBoostingClassifier does not seem to have feature importance yet ( [HistGradientBoostingRegressor does](https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_gradient_boosting_regression.html) ).","6b5bc196":"There are no missing values.","ab6e6460":"### For HistGradientBoostClassifier\n\nWe'll see a similar tuning for the [HisGradientBoostClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.HistGradientBoostingClassifier.html), which runs fast and has reasonably good score for this competition data.","d93e4987":"## HistGradientBoostingClassifier\n\n### Train & Inference","3740e6a3":"Optimize execute setting.","ea17b354":"# <div class=\"alert alert-block alert-success\">Preprocessing<\/div>\n\nWe'll try preprocessing for the later process is minimal but necessary.","e7e08ff6":"### <span style=\"color: orange; \">\u2193\u2193\u2193 I replace the parameters with those explored by large trial number in another notebook version. If you want to use your own tuned values, comment out this cell.<\/span>","2e5a5e48":"## Umap Result\n\nWe will also check the result of dropping the data into two dimensions with Umap. The data seems to be quite mixed up.","f327b52e":"The search results can be visualized.","56544412":"<a id='1'><\/a>\n# <div class=\"alert alert-block alert-success\">Load Data & Libraries<\/div>\n\nFirst, I'll load data and libraries. Additionally, I'll do some preprocessing.\n\n### Load data and libraries","7e282a26":"## Interactions of all features\n\nYou can see scatter plot between all features. Please choose two features with pull down and check their interactions.","434a6266":"There are no missing values.","45d3cd48":"## Countplot for target of train data\n\nThe target in the training data seems to be quite biased.","c28de980":"## For CatBoostClassifier\n\nTrain the model with the hyperparameter values assigned by Optuna and calculate the validation score. In this case, we will use kfold to split the train data into three parts and return the CV score to optuna.","29bb10b7":"\n\nWe'll specify trial times.","e7f0c07c":"It seems that there are many columns with a value of 0. However, in rare cases, there are columns that have no 0 at all.","a3d0dfdc":"<a id='3'><\/a>\n# <div class=\"alert alert-block alert-success\">EDA<\/div>\n\nI'll see standard visualizations.\n\n## kdeplot for all features\n\nCompare the distribution of the train and test data. All features seem to have a similar distribution.","24e63e12":"<div class=\"alert alert-block alert-warning\">Note:  Optuna is easy and friendly, but if you felt too difficult, you can skip this section. Remember here when you need someday!<\/div>","50869c6a":"## CatBoostClassifier\n\n### Train & Inference","ae5d0f94":"All values are Int.","ba749f01":"### For XGBoost","62d7acd9":"## Feature importance\n\nIn methods such as LightGBM and CatBoost, we can often use feature importance. Although not completely reliable, feature importance can be used to check the importance of features and in some cases to reduce the number of features that are too many.\n\nIn this case, the implementation is a bit complicated because we are using kfold to ensemble, but the point is that the feature_importances_ of the trained model contains values, so we extract and visualize.","493a4f96":"## Correlations with heatmap\n\nCorrelation is not high across the whole features. For more details, you can hover over to see the details!","13dbfc3c":"## Feature importance","4007f5ab":"Since target is list (or Series) of string, such as \"Class_6\", we will convert it to a list of number using label encoding so that it can be entered into the model.\n\nWe can easily do this with [sklearn.preprocessing.LabelEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html) .","82ccd3bf":"## Blending\n\nIn this case, we estimated result with two models, but averaging the results may improve the score. A simple average is easy to do, so let's see how to do it.","8276c05a":"## XGBoost\n\n### Train & Inference","a3c07593":"# TPS-06 Solution Assortment EDA, Optuna, ensemble\ud83c\udf6d\n\n## Motivation\n\nThe Tabular Playground Series has become a regular competition that is released on the first of every month. In previous competitions, a brilliant and wonderful variety of analyses and solutions have been published. However, since there are quite a lot of methods being shared, some people may not know where to start.\n\nIn this notebook, I hope to share examples of the use of things that are easy to get a handle on, following the methods used so far, like following:\n\n- Overviewing (Stats, Missings, Zeros)\n- EDA (distribution, correlation, PCA, Umap)\n- optuna\n- BoostingClassifier\n- Blending\n\n## Contents\n\n- [Load Data & Libraries](#1)\n- [Data Overview](#2)\n    - Stats\n    - Missings\n    - Zeros\n    - Metric\n- [EDA](#3)\n    - kdeplot for all features\n    - Correlations with heatmap\n    - Interactions of all features\n    - PCA Result\n    - Umap Result\n- [Hyperparameter tuning with optuna](#4)\n    - Simple example\n    - CatBoostClassifier\n    - HistGradinetBoostingClassifier\n    - XGBoost\n- [Train & Inference](#5)\n    - CatBoostClassifier\n    - HistGradinetBoostingClassifier\n    - XGBoost\n    - Feature importance\n    - Blending","34cf8c2d":"All values are Int."}}