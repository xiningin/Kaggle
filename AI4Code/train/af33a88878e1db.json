{"cell_type":{"1811b8d7":"code","1cb5a9f2":"code","6d835010":"code","6a36b561":"code","8288a294":"code","9763740a":"code","f2b97177":"code","65df0d53":"code","3b78415a":"code","ea6b23c8":"code","d6d1e40a":"code","974c2921":"code","42b5f2ac":"code","f069bab1":"code","14be4c8f":"code","a069c2c4":"code","d43ec426":"code","6415865e":"code","4405495d":"code","0d22fafa":"code","263c78be":"code","f7d82048":"code","38c99eb0":"code","64c1319f":"code","18647dd4":"code","f756cbf5":"code","d2bc6090":"code","ce6a8841":"code","c423c9a8":"code","6eccdd8c":"code","74d58d8d":"code","7c70f670":"code","41a7b72e":"code","1abf36de":"code","e24a64ff":"code","6f34c6a4":"code","0d2299f1":"code","e46d40a9":"code","f922b262":"code","f80481cc":"code","ae18f574":"code","006ce812":"code","e2f1fa54":"code","3eed011b":"code","33dc1c74":"code","4a0c6742":"code","cb86a41c":"code","d0da1035":"code","de5e1845":"code","17bf73e1":"code","0cdfde58":"code","b2406681":"code","e436d46c":"code","9c9911d4":"code","d2b68e52":"code","1ba7175a":"code","af2a4356":"code","c6038548":"code","aa2df1e3":"code","799b3802":"code","7eb66cdf":"code","cdb815d9":"code","8e7257f9":"code","a13a29f7":"code","eaf683dd":"code","94cb4ec0":"code","b098ed84":"code","9d73d3e8":"code","469afd02":"code","e1a48fdf":"code","05fa319f":"code","f7549794":"code","3537ec5f":"code","16db6a79":"markdown","c03d5463":"markdown","e565c13f":"markdown","739775bf":"markdown","5af2b9dd":"markdown","d560e25a":"markdown","9559bcbc":"markdown","6e1b8efa":"markdown","7a685723":"markdown","4839cc3e":"markdown","f4129bb6":"markdown","2d8b8056":"markdown","5a942b24":"markdown","d6477466":"markdown","f2709277":"markdown","58b8d858":"markdown","7355f886":"markdown","ec95f5a9":"markdown","26fd387a":"markdown","140d875d":"markdown","dac571b7":"markdown","871f6f5f":"markdown","86831a25":"markdown","b16047bc":"markdown","6913353c":"markdown","88ed5fd4":"markdown","39e7c8c7":"markdown","2c766d21":"markdown","243b6554":"markdown","a28e4fbc":"markdown","0bdd9e99":"markdown","605adb3a":"markdown","b3f37b73":"markdown","e9552541":"markdown","d69009b2":"markdown","c2eea3b9":"markdown","b2166a17":"markdown","4040599b":"markdown","61d15371":"markdown","a513a186":"markdown","f56d49b3":"markdown","189019e4":"markdown","87afd982":"markdown","8c67a77c":"markdown","88167470":"markdown","1e8821a4":"markdown","9e2ff95d":"markdown","f5c0d457":"markdown","84a1c4c1":"markdown","35283240":"markdown","25f68443":"markdown","be885081":"markdown","69a4ecaf":"markdown","07bce8fa":"markdown","1216863e":"markdown","f9113261":"markdown","683aba1a":"markdown","fff05fe2":"markdown","5c8748a2":"markdown","653b9981":"markdown","6bc9d0fd":"markdown"},"source":{"1811b8d7":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# for data visualzation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport pandas_profiling # LIBRARY TO see all the details of data\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1cb5a9f2":"train_data=pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ntest_data=pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')","6d835010":"test_data=pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\n# test_id=test_data.pop('Id')\ntest_data.shape\n","6a36b561":"print(train_data.dtypes.unique())\nprint(len(list(train_data.columns))) # we have total 81 columns with 1 target column and 80 variables\n# train_data.columns\n# train_id=train_data.pop('Id') # since Id iss not going to be used in Prediction so etter to pop it\n\nnum_col=train_data.select_dtypes(exclude='object')\ncat_col=train_data.select_dtypes(exclude=['int64','float64'])\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', 100)\nnum_col.describe(include = 'all')","8288a294":"# HEATMAP TO SEE MISSING VALUES\nplt.figure(figsize=(15,5))\nsns.heatmap(num_col.isnull(),yticklabels=0,cbar=False,cmap='viridis')","9763740a":"# num_col.profile_report() # this will show profile report only for numerical variables because we are using dataframe having only numerical variables","f2b97177":"X=num_col.copy() #  all numerical variables\ny=X.pop('SalePrice') # storing target variable in y\nX.isnull().sum()","65df0d53":"X.isna().sum().reset_index() ","3b78415a":"#correlation map\nf,ax = plt.subplots(figsize=(20,2))\nsns.heatmap(X.corr().iloc[7:8,:], annot=True, linewidths=.8, fmt= '.1f',ax=ax)\n# this shows that MasVnrArea is not highly corelated to any other feature","ea6b23c8":"sns.kdeplot(X.MasVnrArea,Label='MasVnrArea',color='g')","d6d1e40a":"X.MasVnrArea.replace({np.nan:0},inplace=True)\n","974c2921":"f,ax = plt.subplots(figsize=(20,2))\nsns.heatmap(X.corr().iloc[24:25,:], annot=True, linewidths=.8, fmt= '.1f',ax=ax)","42b5f2ac":"sns.kdeplot(X.GarageYrBlt,Label='GarageYrBlt',color='g')","f069bab1":"X.GarageYrBlt.describe()","14be4c8f":"X['GarageYrBlt'].replace({np.nan:X.GarageYrBlt.mean()},inplace=True)","a069c2c4":"f,ax = plt.subplots(figsize=(20,2))\nsns.heatmap(X.corr().iloc[1:2,:], annot=True, linewidths=.8, fmt= '.1f',ax=ax)","d43ec426":"sns.kdeplot(X.LotFrontage,Label='LotFrontage',color='g')","6415865e":"X.LotFrontage.describe()","4405495d":"X['LotFrontage'].replace({np.nan:X.LotFrontage.mean()},inplace=True)","0d22fafa":"from sklearn.feature_selection import SelectKBest # SELECT K  BEST  is used to first top k features from variables list\nfrom sklearn.feature_selection import chi2 # import chi1 function","263c78be":"# # apply SelectKBest class to extract top 30 best features\nbestfeatures = SelectKBest(score_func=chi2, k=30)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(30,'Score'))  #print  TOP 30 best features","f7d82048":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(30).plot(kind='barh',figsize=(30,10))","38c99eb0":"\nfeatures_tree=set(list(feat_importances.nlargest(30).index)) # top 35 features by tree classifier\nfeatures_chi=set(list(featureScores.Specs[:30]))# top 30 features by chi square test","64c1319f":"union_fe=features_chi.union(features_tree)\nunion_fe=list(union_fe)\n","18647dd4":"X=X[union_fe] # SELCTING TOP fEATURES FROM FEATURE SET\n","f756cbf5":"from sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.model_selection import train_test_split as tt\nfrom sklearn.ensemble import RandomForestRegressor as rr\ntrain_X,val_X,train_Y,val_Y=tt(X,y,random_state=23)\nforest_model=rr(random_state=12,max_depth=9,n_estimators=200)\nforest_model.fit(X,y)\n\nprediction=forest_model.predict(val_X)\nprint(mae(val_Y,prediction))","d2bc6090":"test_X=test_data.select_dtypes(exclude=['object']) # way to select all numerical variables\ntest_X.head()","ce6a8841":"test_X['LotFrontage'].replace({np.nan:test_X.LotFrontage.mean()},inplace=True)","c423c9a8":"test_X.MasVnrArea.replace({np.nan:0},inplace=True)","6eccdd8c":"test_X['GarageYrBlt'].replace({np.nan:test_X.GarageYrBlt.mean()},inplace=True)","74d58d8d":"sns.kdeplot(test_X.TotalBsmtSF,label='TotalBsmtSF')\nprint(test_X.TotalBsmtSF.describe())","7c70f670":"test_X['TotalBsmtSF'].replace({np.nan:test_X.TotalBsmtSF.mean()},inplace=True)","41a7b72e":"sns.kdeplot(test_X.BsmtFinSF1,label='BsmtFinSF1')\nprint(test_X.BsmtFinSF1.describe())\n","1abf36de":"test_X['BsmtFinSF1'].replace({np.nan:test_X.BsmtFinSF1.median()},inplace=True)","e24a64ff":"sns.kdeplot(test_X.BsmtFinSF2,label='BsmtFinSF2')\nprint(test_X.BsmtFinSF2.describe())","6f34c6a4":"test_X['BsmtFinSF2'].replace({np.nan:0},inplace=True)","0d2299f1":"sns.kdeplot(test_X.GarageArea,label='GarageArea')\nprint(test_X.GarageArea.describe())","e46d40a9":"test_X['GarageArea'].replace({np.nan:test_X.GarageArea.mean()},inplace=True)","f922b262":"sns.kdeplot(test_X.BsmtUnfSF,label='BsmtUnfSF')\nprint(test_X.BsmtUnfSF.describe())","f80481cc":"# BsmtUnfSF\ntest_X['BsmtUnfSF'].replace({np.nan:test_X.BsmtUnfSF.median()},inplace=True)","ae18f574":"sns.kdeplot(test_X.BsmtFullBath,label='BsmtUnfSF')\nprint(test_X.BsmtFullBath.describe())","006ce812":"test_X['BsmtFullBath'].replace({np.nan:0},inplace=True)","e2f1fa54":"# BsmtHalfBath\nsns.kdeplot(test_X.BsmtHalfBath,label='BsmtHalfBath')\nprint(test_X.BsmtHalfBath.describe())\n","3eed011b":"test_X['BsmtHalfBath'].replace({np.nan:0},inplace=True)","33dc1c74":"sns.kdeplot(test_X.GarageCars,label='BsmtHalfBath')\nprint(test_X.GarageCars.describe())\n","4a0c6742":"test_X['GarageCars'].replace({np.nan:test_X.GarageCars.median()},inplace=True)","cb86a41c":"# test_X.isnull().sum().sort_values(ascending=False)","d0da1035":"test_X.columns","de5e1845":"train_X.columns","17bf73e1":"test_X.shape","0cdfde58":"f,ax = plt.subplots(figsize=(20,20))\nsns.heatmap(test_X.corr(), annot=True, linewidths=.8, fmt= '.1f',ax=ax)","b2406681":"# test_X=test_X[union_fe]\n# make predictions which we will submit. \ntest_preds = forest_model.predict(test_X)\ntest_preds.shape\n# The lines below shows how to save predictions in format used for competition scoring\n# Just uncomment them.\n\noutput = pd.DataFrame({'Id': test_data.Id,\n                      'SalePrice': test_preds})\noutput.to_csv('submission12.csv', index=False)\n# test_X.columns","e436d46c":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\ndf = output\n\n# create a link to download the dataframe\ncreate_download_link(df)","9c9911d4":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n\ncat_col_name=[cname for cname in cat_col.columns if cat_col[cname].nunique() < 10 and \n                        cat_col[cname].dtype == \"object\"]\n","d2b68e52":"cat_col_name","1ba7175a":"num_col_name=list(X.columns)\nnum_col_name","af2a4356":"features=cat_col_name+num_col_name\nfeatures","c6038548":"train_data=train_data[features]\ntrain_data=pd.get_dummies(train_data)\n","aa2df1e3":"#idxmax() function returns index of first occurrence of maximum over requested axis.\n#While finding the index of the maximum value across any index, all NA\/null values are excluded.\n\n#These columns has only few value missing as compared to total number so i choose to replace the NaN values with the most frequent values\n\n#MasVnrType\ncat_col['MasVnrType'].replace({np.nan:cat_col.MasVnrType.value_counts().idxmax()},inplace=True)\n\n#BsmtQual\ncat_col['BsmtQual'].replace({np.nan:cat_col.BsmtQual.value_counts().idxmax()},inplace=True)\n\n#BsmtCond\ncat_col['BsmtCond'].replace({np.nan:cat_col.BsmtCond.value_counts().idxmax()},inplace=True)\n\n#BsmtExposure\ncat_col['BsmtExposure'].replace({np.nan:cat_col.BsmtExposure.value_counts().idxmax()},inplace=True)\n\n#BsmtFinType1\ncat_col['BsmtFinType1'].replace({np.nan:cat_col.BsmtFinType1.value_counts().idxmax()},inplace=True)\n\n#BsmtFinType2\ncat_col['BsmtFinType2'].replace({np.nan:cat_col.BsmtFinType2.value_counts().idxmax()},inplace=True)\n\n#Electrical\ncat_col['Electrical'].replace({np.nan:cat_col.Electrical.value_counts().idxmax()},inplace=True)\n\n#GarageType\ncat_col['GarageType'].replace({np.nan:cat_col.GarageType.value_counts().idxmax()},inplace=True)\n\n#GarageFinish\ncat_col['GarageFinish'].replace({np.nan:cat_col.GarageFinish.value_counts().idxmax()},inplace=True)\n\n#GarageQual\ncat_col['GarageQual'].replace({np.nan:cat_col.GarageQual.value_counts().idxmax()},inplace=True)\n\n#GarageCond\ncat_col['GarageCond'].replace({np.nan:cat_col.GarageCond.value_counts().idxmax()},inplace=True)\n\n\n","799b3802":"# Now filling Missing Value for FireplaceQu column\nprint(cat_col.FireplaceQu.value_counts())\n# and Missing Values in this column are 690 so we will Replace nan with 'Unknown'\ncat_col.FireplaceQu.replace({np.nan:'Unknown'},inplace=True)\n","7eb66cdf":"f, axes = plt.subplots(1, 2,figsize=(12, 8))\ng = sns.swarmplot(x=cat_col.Street,y=y,ax=axes[0]) # y is Saleprice\ng = g.set_ylabel(\"Sale Price for Diffrent Streets\")\n\nlabels=['Pave','Grvl']\nslices=[cat_col.loc[cat_col.Street==\"Pave\"].shape[0],cat_col.loc[cat_col.Street==\"Grvl\"].shape[0]]\nplt.pie(slices,labels=labels,startangle=90,shadow=1,explode=(0.5,0.7),autopct='%1.2f%%',colors=['#99ff99','#ffcc99'])\nplt.show()","cdb815d9":"list_of_col=list(cat_col.columns)\ndict_of_col={i:list_of_col[i] for i in range(len(list_of_col))}\ndict_of_col","8e7257f9":"temp_list=list(i for i in range(5))\nf,axes=plt.subplots(1, 5,figsize=(20,8))\nf.subplots_adjust(hspace=0.5)\n\nfor j in temp_list:\n        g = sns.swarmplot(x=cat_col[dict_of_col[j]],y=y,ax=axes[j]) # y is Saleprice\n        g.set_title(label=dict_of_col[j].upper(),fontsize=20)\n        g.set_xlabel(str(dict_of_col[j]),fontsize=25)\n        g.set_ylabel('SalePrice',fontsize=25)\n        plt.tight_layout() # to increase gapping between subplots\n\n    \n","a13a29f7":"f,axes=plt.subplots(1, 2,figsize=(15,8))\nf.subplots_adjust(hspace=0.5)\nfor j,i in zip(temp_list,range(5,7)):\n        g = sns.swarmplot(x=cat_col[dict_of_col[i]],y=y,ax=axes[j]) # y is Saleprice\n        g.set_title(label=dict_of_col[i].upper(),fontsize=20)\n        g.set_xlabel(str(dict_of_col[i]),fontsize=25)\n        g.set_ylabel('SalePrice',fontsize=25)\n        plt.tight_layout() # to increase gapping between subplots\n","eaf683dd":"for i in cat_col.columns:\n    print(i,'\\t',cat_col[str(i)].unique(),'\\n')\n","94cb4ec0":"from sklearn.preprocessing import LabelEncoder as le,OneHotEncoder as oe","b098ed84":"\nst_le=le()\nstreet_labels=st_le.fit_transform(cat_col.Street) # meaning of fit_transform is that it will asign appropriate numbers for each categorical variables\nstreet_mapping={index:label for index,label in enumerate(st_le.classes_)}\nstreet_mapping","9d73d3e8":"temp_col=cat_col.copy() # doing our work on a copy of datframe","469afd02":"temp_col['Street_Labels']=street_labels\ntemp_col.head()","e1a48fdf":"st_oe=oe(handle_unknown='ignore')\nst_feature_arr=st_oe.fit_transform(temp_col[['Street_Labels']]).toarray()\nst_feature_labels=list(st_le.classes_)\nst_features=pd.DataFrame(st_feature_arr,columns=st_feature_labels)","05fa319f":"temp_X_train=pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ntemp_X_test=pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')","f7549794":"# All categorical columns\nobject_cols = [col for col in temp_X_train.columns if temp_X_train[col].dtype == \"object\"]\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if \n                   set(temp_X_train[col]) == set(temp_X_test[col])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be label encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)","3537ec5f":"good_label_cols=list(set(good_label_cols)-set(['Alley','PoolQC','Fence','MiscFeature']))\nprint(len(cat_col.columns))\ncat_col=cat_col[good_label_cols]\nprint(len(cat_col.columns))","16db6a79":"### Selcting Union of Features from both the ways","c03d5463":"Looking at the Kdeplot for **GarageYrBlt** and Description we find that data in this column is not spread enough so we can use **mean** of this column to fill its Missing Values","e565c13f":"<a id=3-1><\/a> <br>\n## 3.1-UNIQUE TECHNIQUE TO SEE MISSING VALUES","739775bf":"Looking at the Kde Plot and Description  of **LotFrontage** we can replace Nan Values of this column either by **Mean** or **Median** Because data is almost Normal distribution\n\nI would Choose Mean to replace NaN values","5af2b9dd":"<a id=6><\/a> <br>\n# 6-FEATURE SELECTION TECHNIQUES FOR NUMERICAL VARIABLES\n### 1-UNIVARIATE SELECTION\n### 2-FEATURE IMPORTANCE\n### 3-CORRELATION MATRIX\n","d560e25a":"<a id=11><\/a> <br>\n## 11-WORKING WITH TEST DATA","9559bcbc":"## Label Encoding one Column with Sklearn ","6e1b8efa":"<a id=7><\/a> <br>\n## 7-DATA CLEANING","7a685723":"<a id=\"1\"><\/a> <br>\n## 1-IMPORTS","4839cc3e":"2. **GarageYrBlt**","f4129bb6":"So we are done with **data cleaning** part ","2d8b8056":"Observation of First 5 Variables with **SalePrice**","5a942b24":"<a id=3><\/a><br>\n## 3-DATA SNEAK PEAK","d6477466":"<a id=4><\/a> <br>\n## 4-DATA SCIENCE WORKFLOW\n\nThere is no template for solving a data science problem. The roadmap changes with every new dataset and new problem. But we do see similar steps in many different projects. I wanted to make a clean workflow to serve as an example to aspiring data scientists. \n\n<img src=\"https:\/\/miro.medium.com\/max\/2000\/1*3FQbrDoP1w1oibNPj9YeDw.png\"> \n\n### Overview:\n\n- Source the Data \n- Data Processing\n- Modeling\n- Model Deployment\n- Model Monitoring \n- Exploration and reporting\n\n## In this notebook we will focus mainly on <font color='red'><b>STEP-2(DATA CLEANING)<\/b><\/font>\n\n","f2709277":"10. <font color='red'>**BsmtHalfBath**<\/font>","58b8d858":"This shows that **GarageYrBlt** is highly corelated to **YearBuilt**\n\nAnd One Important Reason that we cant drop **GarageYrBlt** because it has significant corelation with our predicctor variable **SalePrice** ","7355f886":"So finally we will use **cat_col** dataframe and **good_label_cols** for one hot encoding and later will be used for prediction\nFirst of all we will remove **Alley,PoolQC,Fence,MiscFeature** from good labels list because these columns have more than 80% values as **NaN**\nand then we will use **good_label_cols** as list of features  for cat_col dataframe","ec95f5a9":"2. <font color='red'>**MasVnrArea**<\/font>\n\nUse the same criteria that we used to handle missing values of <font color='red'>MasVnrArea<\/font> in training data","26fd387a":"<a id='top'><\/a> <br>\n## NOTEBOOK CONTENT\n1. [IMPORTS](#1)\n1. [LOAD DATA](#2)\n1. [DATA SNEAK PEAK](#3)\n    1. [UNIQUE WAY TO SEE MISSING VALUES](#3-1)\n1. [DATA SCIENCE WORKFLOW](#4)\n1. [PROFILE REPORT](#5)\n1. [FEATURE SELECTION TECHNIQUES FOR NUMERICAL VARIABLES](#6)\n1. [DATA CLEANING](#7)\n1. [UNIVARIATE SELECTION](#8)\n1. [FEATURE IMPORTANCE](#9)\n1. [RANDOM FOREST](#10)\n1. [WORKING WITH TEST DATA](#11)\n1. [TO MAKE CSV FILE FOR SUBMISSION](#12)\n1. [CATEGORICAL DATA](#13)\n    1. [HANDLING MISSING CATEGORICAL DATA](#13-1)\n1. [DATA VISUALISATION FOR CATEGORICAL VARIABLES](#14)\n1. [One Hot Encoding Categorical Columns](#15)","140d875d":"<a id=\"2\"><\/a> <br>\n## 2-LOAD DATA","dac571b7":"**LotFrontage**: Linear feet of street connected to property\n\nAnd One Important Reason that we cant drop **LotFrontage** because it has significant corelation with our predicctor variable **SalePrice** \n","871f6f5f":"\n5. <font color='red'> **BsmtFinSF1**<\/font>\n\nWe can see from description of this column that there is lot of gap between 75th percentile and max value so that is the reason why mean is so high even 25 percentile is equal to zero.\nSo better to use median to replace NaN values\n","86831a25":"## <div style=\"text-align: center\">LEARN FEATURE ENGINEERING AND FEATURE SELECTION TECHNIQUES <\/div>\n<div style=\"text-align:center\"><img src=\"https:\/\/brainstation-23.com\/wp-content\/uploads\/2018\/12\/ML-real-state.png\"><\/div>","b16047bc":"<font color='red'>WARNING<\/font> The Implementation of this cell takes resources","6913353c":"Similarly we can observe each **Categorical variables**","88ed5fd4":"\n\n[GO to top](#top)","39e7c8c7":"So the heatmap shows **LotFrontage**,**MasVnrArea**,**GarageYrBlt** have the missing values","2c766d21":"11. <font color='red'>**GarageCars**<\/font>\n","243b6554":"## Next we will look how to Handle and use these categorical variables in our model","a28e4fbc":"<a id=5><\/a> <br>\n## 5-PROFILE REPORT  (PERSONAL FAV.)","0bdd9e99":"This clearly shows that **Pave** street has more Saleprices as compared to **Grvl**\nand very Interesting thing \nMost of the people(99.59%) prefer **Pave** Street Access as compared to **Grvl**\nand in **Pave** section also most of the Houses cost under **400,000 $**","605adb3a":"3. <font color='red'>**GarageYrBlt**<\/font>\n\nUse the same criteria that we used to handle missing values of <font color='red'>GarageYrBlt<\/font> in training data","b3f37b73":"## One Hot encoding a Column ith Sklearn","e9552541":"<a id=10><\/a> <br>\n## 10-RANDOM FOREST\nRandom Forest is a trademark term for an ensemble of decision trees. In Random Forest, we\u2019ve collection of decision trees (so known as \u201cForest\u201d). To classify a new object based on attributes, each tree gives a classification and we say the tree \u201cvotes\u201d for that class. The forest chooses the classification having the most votes (over all the trees in the forest).","d69009b2":"3. **LotFrontage**","c2eea3b9":"1. RL is the most choosen street zone from all 5 zones and has  the maximum prices\n1. Pave is Highly Preferred Street access than Grvl\n1. IR1 and Reg are highly preferred LotShape","b2166a17":"Delaing with **MasVnrArea**\n    So MasVnrArea is  **Masonry veneer area** of house so we can take mean of **MasVnrArea** and replace all nun values of column\n","4040599b":"<a id=9><\/a> <br>\n## 9-FEATURE IMPORTANCE\n### Extra Tree Classifier also uses positive values and is not applicable for NaN,Infinite values","61d15371":"## Street vs SalePrice","a513a186":"## I hope this kernel helpful and some <font color='red'><b>UPVOTES<\/b><\/font> would be very much appreciated","f56d49b3":"### So we have finally filled all the Missing Values in Categorical columns","189019e4":"Looking at all the **diffrent** values in each column","87afd982":"This and profilereport above shows that most of the values (nearly 60%) values of MasVnrArea have **zero** value so replace nan values here with **ZERO**","8c67a77c":"1. <font color='red'>**LotFrontage**<\/font>\n\nUse the same criteria that we used to handle missing values of <font color='red'>LotFrontage<\/font> in training data","88167470":"<a id=13-1><\/a> <br>\n## 13.1-Handling Missing  Values in Categorical data","1e8821a4":"1.**MasVnrArea**","9e2ff95d":"9. <font color='red'>**BsmtFullBath**<\/font>\n\n75 percentile of values are 0 so replace NaN with 0","f5c0d457":"PC - BRAIN STATION 23","84a1c4c1":"8. <font color='red'>**BsmtUnfSF**<\/font>\n\nAs it is highly varied data so to use median to replace NaN value","35283240":"<a id=8><\/a> <br>\n## 8-UNIVARIATE SELECTION\n**CHI2** Test for Univariate Selection\n### 1. CHI2 Test only applies for Positive Value\n### 2. CHI2 should only be applied to columns that do not have any NAN values","25f68443":"7. <font color='red'>**GarageArea**<\/font>\n\nWe have only 1 missing values in this column so just replace it with mean of the column","be885081":"## HANDLE MISSING VALUES OF <font color='red'>Test Data<\/font>","69a4ecaf":"<a id=13><\/a> <br>\n## 13-CATEGORICAL DATA","07bce8fa":"<a id=14><\/a> <br>\n## 14-DATA VISUALISATION FOR CATEGORICAL VARIABLES","1216863e":"6. <font color='red'>**BsmtFinSF2**<\/font>\n\nAs we see 75 percent of the value are zero so better to replace missing value with 0","f9113261":"<a id=12><\/a> <br>\n## 12-TO MAKE CSV FILE FOR SUBMISSION","683aba1a":"Very Important Link\n[replace pandas](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.replace.html)","fff05fe2":"Observing next 2 **variables**","5c8748a2":"## Data cleaning for test data done","653b9981":"Fitting a label encoder to a column in the training data creates a corresponding integer-valued label for each unique value that appears in the training data. In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them. Notice that the 'Condition2' column in the validation data contains the values 'RRAn' and 'RRNn', but these don't appear in the training data -- thus, if we try to use a label encoder with scikit-learn, the code will throw an error.\n\nThis is a common problem that you'll encounter with real-world data, and there are many approaches to fixing this issue. For instance, you can write a custom label encoder to deal with new categories. The simplest approach, however, is to drop the problematic categorical columns.\n\nRun the code cell below to save the problematic columns to a Python list bad_label_cols. Likewise, columns that can be safely label encoded are stored in good_label_cols.\n\n**Bad_Label_cols** are those columns that the values are not the same between the 2 dataset. In this case the **training** and **testing**\n\n","6bc9d0fd":"4. <font color='red'>**TotalBsmtSF**<\/font>\n\nWe have only 1 missing values in this column so just replace it with mean of the column\nAlthough data is varying alot but still the median and mean of data are nearly same so i chosse mean"}}