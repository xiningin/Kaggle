{"cell_type":{"6da82f9e":"code","d1362b99":"code","eafdb059":"code","404055d1":"code","b9fdb585":"code","92c6e145":"code","50cf5ad1":"markdown","24a7c710":"markdown","d1ec95dc":"markdown","689300de":"markdown","a12eddd5":"markdown","7ddab71d":"markdown","b93053af":"markdown","4977e330":"markdown","c45beaf4":"markdown"},"source":{"6da82f9e":"# Install:\n# Kaggle environments.\n!git clone -q https:\/\/github.com\/Kaggle\/kaggle-environments.git\n!cd kaggle-environments && pip install .\n\n# GFootball environment.\n!apt-get update -y -qq > \/dev\/null\n!apt-get install -y -qq libsdl2-gfx-dev libsdl2-ttf-dev\n\n# Make sure that the Branch in git clone and in wget call matches !!\n!git clone -q -b v2.8 https:\/\/github.com\/google-research\/football.git\n!mkdir -p football\/third_party\/gfootball_engine\/lib\n\n!wget -q https:\/\/storage.googleapis.com\/gfootball\/prebuilt_gameplayfootball_v2.8.so -O football\/third_party\/gfootball_engine\/lib\/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip install .","d1362b99":"!pip install rl-replicas","eafdb059":"import os\n\nimport gfootball\nimport gym\nimport torch\nimport torch.nn as nn\n\nfrom rl_replicas.algorithms import TRPO\nfrom rl_replicas.common.policy import Policy\nfrom rl_replicas.common.value_function import ValueFunction\nfrom rl_replicas.common.optimizers import ConjugateGradientOptimizer\nfrom rl_replicas.common.torch_net import mlp\n\nalgorithm_name = 'trpo'\nenvironment_name = 'GFootball-11_vs_11_kaggle-simple115v2-v0'\nepochs = 5\nsteps_per_epoch = 4000\npolicy_network_architecture = [64, 64]\nvalue_function_network_architecture = [64, 64]\nvalue_function_learning_rate = 1e-3\noutput_dir = '.\/trpo'\n\nenv: gym.Env = gym.make(environment_name)\n\npolicy_network = mlp(\n    sizes = [env.observation_space.shape[0]]+policy_network_architecture+[env.action_space.n]\n)\n\npolicy: Policy = Policy(\n    network = policy_network,\n    optimizer = ConjugateGradientOptimizer(params=policy_network.parameters())\n)\n\nvalue_function_network = mlp(\n    sizes = [env.observation_space.shape[0]]+value_function_network_architecture+[1]\n)\nvalue_function: ValueFunction = ValueFunction(\n    network = value_function_network,\n    optimizer = torch.optim.Adam(value_function_network.parameters(), lr=value_function_learning_rate)\n)\n\nmodel: TRPO = TRPO(policy, value_function, env, seed=0)\n\nprint('an experiment to: {}'.format(output_dir))\n\nprint('algorithm:           {}'.format(algorithm_name))\nprint('epochs:              {}'.format(epochs))\nprint('steps_per_epoch:     {}'.format(steps_per_epoch))\nprint('environment:         {}'.format(environment_name))\n\nprint('value_function_learning_rate: {}'.format(value_function_learning_rate))\nprint('policy network:')\nprint(policy.network)","404055d1":"model.learn(\n    epochs=epochs,\n    steps_per_epoch=steps_per_epoch,\n    output_dir=output_dir,\n    model_saving=True\n)","b9fdb585":"%%writefile .\/agent.py\nimport time\n\nimport torch\nimport gfootball\nimport gym\nfrom gfootball.env.wrappers import Simple115StateWrapper\n\nfrom rl_replicas.common.policy import Policy\nfrom rl_replicas.common.torch_net import mlp\n\nstart_setup_time: float = time.time()\n\nnum_observation = 115\nnum_action = 19\npolicy_network_architecture = [64, 64]\nmodel_location = '.\/trpo\/model.pt'\nmodel = torch.load(model_location)\n\npolicy_network = mlp(\n    sizes = [num_observation]+policy_network_architecture+[num_action]\n)\n\npolicy_network.load_state_dict(model['policy_state_dict'])\n\npolicy: Policy = Policy(\n    network = policy_network,\n    optimizer = None\n)\n\ncurrent_step: int = 0\n\nprint('Set up Time: {:<8.3g}'.format(time.time() - start_setup_time))\n\ndef agent(observation):\n    global policy\n    global current_step\n\n    start_time: float = time.time()\n    current_step += 1\n\n    raw_observation = observation['players_raw']\n    simple_115_observation = Simple115StateWrapper.convert_observation(raw_observation, fixed_positions=False)\n    observation_tensor: torch.Tensor = torch.from_numpy(simple_115_observation).float()\n\n    action = policy.predict(observation_tensor)\n    \n    if (current_step%100) == 0:\n        print('Current Step: {}'.format(current_step))\n\n    one_step_time = time.time() - start_time\n    if one_step_time >= 0.2:\n        print('One Step Time exceeded 0.2 seconds: {:<8.3g}'.format(one_step_time))\n\n    return [action.item()]\n","92c6e145":"from kaggle_environments import make\n\nenv = make(\"football\", \n           configuration={\n             \"save_video\": True, \n             \"scenario_name\": \"11_vs_11_kaggle\",\n             \"running_in_notebook\": True,\n           })\n\noutput = env.run([\".\/agent.py\", \"do_nothing\"])[-1]\n\nprint('Left player: reward = {}, status = {}, info = {}'.format(output[0]['reward'], output[0]['status'], output[0]['info']))\nprint('Right player: reward = {}, status = {}, info = {}'.format(output[1]['reward'], output[1]['status'], output[1]['info']))\n\nenv.render(mode=\"human\", width=800, height=600)","50cf5ad1":"##### Note\nTRPO uses `ConjugateGradientOptimizer` as an optimizer.","24a7c710":"install rl-replicas","d1ec95dc":"Along with the Google Research Football competition, I'm trying to implement key algorithms of reinforcement learning in [https:\/\/github.com\/yamatokataoka\/reinforcement-learning-replications](https:\/\/github.com\/yamatokataoka\/reinforcement-learning-replications).\n\nI've implemented VPG, TRPO and PPO so far.\n\nIn this notebook, you'll train a TRPO agent with the Reinforcement Learning Replications (rl-replicas).\n\nIf you're not familiar with TRPO or RL, I highly recomend to read through the [OpenAI Spinning Up](https:\/\/spinningup.openai.com\/en\/latest\/user\/introduction.html)","689300de":"In the rl-replicas, you'll set up the TRPO algorithm with `Policy` and `ValueFunction`. Those two core components have their own neural network and optimizer.\n\nThe trained model is saved in `.\/trpo\/model.pt`.\nYou'll use this for test run.","a12eddd5":"test run","7ddab71d":"install gfootball required tools","b93053af":"Start learning","4977e330":"### Train with rl-replicas","c45beaf4":"### Run inference with rl-replicas\n\nTo run inference, you'll initialize `Policy` with the trained network. The optimizer is uncessary here."}}