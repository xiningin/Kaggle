{"cell_type":{"c06de55b":"code","9d057d66":"code","9bb488d4":"code","a1ce500a":"code","773f0a0c":"code","8b0eff8e":"code","be644ce0":"code","7961ceb1":"code","522dfa24":"code","eefb396a":"code","2b72c009":"code","bece366a":"code","5e9f4366":"code","696f3294":"code","04ff58c6":"code","9de5db4e":"code","7b1e05d9":"code","9c80f407":"code","4fc008f2":"code","b183fd74":"code","358f41c4":"code","4d9ee7aa":"code","67d0d9ea":"code","8ac0e8a7":"code","8067dc89":"code","6ee4e524":"code","bef8c6cc":"code","6385d2eb":"code","ded9d027":"code","6cb456f7":"code","96f84d81":"code","213c10ef":"code","2a1edb39":"code","dbcf8150":"code","7de2ff1f":"code","047f6edc":"code","11f8244b":"code","5cd67602":"code","4240af60":"code","0eb403ee":"code","66f1ac73":"code","4d10e224":"code","3347e026":"code","0e82554b":"code","514cedf6":"code","e877c965":"code","af0940d6":"code","7cf74f45":"code","5fdf1b36":"code","c852a399":"code","262f1923":"code","369522ce":"code","a8844735":"code","c06ad0fc":"code","6f6ff4fc":"code","124995ce":"code","01bee723":"code","a923bbc9":"code","0ac077dc":"code","be011dfb":"code","14edce14":"code","aa9c4763":"code","e37ede38":"code","f99f9cfa":"code","767df21e":"code","84893862":"code","21b2c54b":"code","56c30623":"code","ed5b83d6":"code","fbb60248":"code","11011ee3":"code","b937b94e":"code","8b98fb85":"code","ae091f78":"code","ec0adc09":"code","18266340":"code","03fa67af":"code","631c328e":"code","657686f9":"code","1b27cd5e":"markdown","cc6be75e":"markdown","eb6e98db":"markdown","b4aa3d1b":"markdown","bcc5d797":"markdown","17477633":"markdown","8e5d798f":"markdown","ab9dfc56":"markdown","5de269b3":"markdown","c9e21db5":"markdown","0e9ae283":"markdown","ee835804":"markdown","802a65ad":"markdown","44c03e7b":"markdown","60787ca7":"markdown","047331d7":"markdown","14e20542":"markdown","9efbce07":"markdown","21d722d8":"markdown","70e7a047":"markdown","ca46859d":"markdown","93551d04":"markdown","4beac5da":"markdown","790373a6":"markdown"},"source":{"c06de55b":"# data analysis and wrangling\nimport numpy as np \nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","9d057d66":"# get the data\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ngender_submission_df =pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nall_data = pd.concat([train_df ,test_df ])","9bb488d4":"# view train data\ntrain_df.head(10)","a1ce500a":"train_df.Age.plot.hist()","773f0a0c":"# view test data\ntest_df.head(10)","8b0eff8e":"# view submission\ngender_submission_df.head(10)","be644ce0":"train_df.describe(include='all')","7961ceb1":"test_df.describe(include='all')","522dfa24":"print('tain_df')\ntrain_df.info()\nprint('='*50+'\\n')\nprint('test_df')\ntest_df.info()\nprint('='*50+'\\n')\nprint('the shape of tain_df is :', train_df.shape)\nprint('the shape of test_df is :', test_df.shape)\nprint('the shape of gender_submission_df is :', gender_submission_df.shape)","eefb396a":"# missing value\nprint('tain_df')\ndisplay(train_df.isna().sum())\nprint('='*50+'\\n')\nprint('test_df')\ntest_df.isna().sum()\n","2b72c009":"\nprint(train_df.Survived.value_counts())\nfig = plt.figure(figsize=(10,5))\nsns.countplot(data = train_df , y='Survived');","bece366a":"train_df[['Pclass' , 'Survived']].groupby(['Pclass'],as_index=False).mean()","5e9f4366":"fig = plt.figure(figsize=(10,5))\nsns.countplot(data = train_df , x = 'Pclass' , hue ='Survived');","696f3294":"train_df[['Embarked' , 'Survived']].groupby(['Embarked'],as_index=False).mean()","04ff58c6":"fig = plt.figure(figsize=(10,5))\nsns.countplot(data = train_df , x = 'Embarked' , hue ='Survived');","9de5db4e":"train_df[['Sex','Survived']].groupby(['Sex'], as_index=False).mean()","7b1e05d9":"fig = plt.figure(figsize=(10,5))\nsns.countplot(data = train_df , x = 'Sex' , hue ='Survived');","9c80f407":"train_df[['SibSp','Survived']].groupby('SibSp',as_index=False).mean().sort_values(by = 'Survived' , ascending = False)","4fc008f2":"fig = plt.figure(figsize=(10,5))\nsns.countplot(data = train_df , x = 'SibSp' , hue ='Survived');","b183fd74":"train_df[['Parch' , 'Survived']].groupby('Parch' , as_index=False).mean().sort_values(by = 'Survived' , ascending = False)","358f41c4":"fig = plt.figure(figsize=(10,5))\nsns.countplot(data = train_df , x = 'Parch' , hue ='Survived')\nplt.legend(title = 'Survived',loc = 'upper center');","4d9ee7aa":"train_df[['Fare', 'Survived']].groupby('Survived', as_index=False).mean()","67d0d9ea":"grid = sns.FacetGrid(train_df , col = 'Survived' , height=5)\ngrid.map(plt.hist , 'Fare',bins = 20 )\ngrid.set_ylabels('Count');","8ac0e8a7":"grid = sns.FacetGrid(train_df , col = 'Survived' , height=5)\ngrid.map(plt.hist , 'Age',bins = 30 )\ngrid.set_ylabels('Count');","8067dc89":"grid = sns.FacetGrid(train_df , col = 'Survived' , row = 'Pclass' , height = 3  , aspect=1.7)\ngrid.map(plt.hist , 'Age',bins = 30 )\ngrid.set_ylabels('Count');","6ee4e524":"grid = sns.FacetGrid(train_df , col = 'Survived' , row = 'Embarked' , height = 3  , aspect=1.7)\ngrid.map(sns.barplot, 'Sex', 'Fare' , palette='muted' ,order = ['male' , 'female'] )\ngrid.add_legend();","bef8c6cc":"grid = sns.FacetGrid(train_df, col='Embarked', height = 3, aspect=1.7)\ngrid.map(sns.violinplot, 'Pclass', 'Survived', 'Sex', palette='muted' , order = [1,2,3] ,  hue_order =['male' , 'female'] )\ngrid.add_legend();\n","6385d2eb":"all_data","ded9d027":"test_df.isna().sum() # so i will remove Cabin and fill the Age and Fare so the test data become the same raws For submision","6cb456f7":"print('the shape of the train_df ',train_df.shape)\nprint('the shape of the test_df ',test_df.shape)\nprint('the shape of the all_data ',all_data.shape)\n\nselected_data = all_data.copy() \nall_data.isna().sum() # note that the 418 survived missing data is form the test data ","96f84d81":"#remove cabin column (too many missing data )\nselected_data.drop('Cabin' , axis = 1 , inplace = True)\n\n# fill age and fare with the median\nselected_data['Age'].fillna(selected_data['Age'].dropna().median(), inplace = True)\nselected_data['Fare'].fillna(selected_data['Fare'].dropna().median() , inplace = True)\n\n# drop the missing value in Embarked column (is only two raw in training data)\nselected_data.dropna(subset =['Embarked'] , inplace = True)\n","213c10ef":"display(train_df.shape)\ndisplay(test_df.shape)\ndisplay(all_data.shape)\ndisplay(selected_data.shape)\n\n\nselected_data.isna().sum()","2a1edb39":"selected_data.head(1)","dbcf8150":"selected_data[['Sex' ,'Embarked' ]].head(5)","7de2ff1f":"display(selected_data['Sex'].value_counts())\ndisplay(selected_data['Embarked'].value_counts())","047f6edc":"selected_data['Sex'] = selected_data['Sex'].map({'male':0 , 'female':1})\nselected_data['Embarked'] = selected_data['Embarked'].map({'S':0 , 'C':1,'Q':2})","11f8244b":"selected_data[['Sex' ,'Embarked' ]].head(5)","5cd67602":"selected_data[['Name']]","4240af60":"# get the title of the names\ntemp = pd.DataFrame()\ntemp['Title']=selected_data.Name.apply(lambda x : x.split(',')[1].split('.')[0].strip())\ntemp.Title.value_counts()\n","0eb403ee":"# is the captin srvived !?\nall_data.iloc[temp.query('Title ==\"Capt\"').index]","66f1ac73":"selected_data","4d10e224":"# add name title in the data set and drop the name col\nselected_data = pd.concat([selected_data , temp] , axis = 1)\nselected_data.drop('Name' , axis = 1 , inplace = True)","3347e026":"selected_data","0e82554b":"selected_data.Ticket.value_counts() ","514cedf6":"# no so unique ticket so i wil drop the column \nselected_data.drop('Ticket', axis = 1 , inplace =True)","e877c965":"selected_data[['Age','Fare']]","af0940d6":"# get cut cut for the both the Age and Fare Columns \nselected_data['Fare'] = pd.cut(selected_data['Fare'] , 5)\nselected_data['Age'] = pd.cut(selected_data['Age'] , 5)\nselected_data[['Age','Fare']]","7cf74f45":"selected_data.isna().sum()","5fdf1b36":"Pclass_dummy = pd.get_dummies(selected_data['Pclass'],prefix='Pclass')\nAge_dummy = pd.get_dummies(selected_data['Age'],prefix='Age')\nSibSp_dummy = pd.get_dummies(selected_data['SibSp'],prefix='SibSp')\nParch_dummy = pd.get_dummies(selected_data['Parch'],prefix='Parch')\nFare_dummy = pd.get_dummies(selected_data['Fare'],prefix='Fare')\nEmbarked_dummy = pd.get_dummies(selected_data['Embarked'],prefix='Embarked')\nTitle_dummy = pd.get_dummies(selected_data['Title'],prefix='Title')\n\nall_dummies = pd.concat([Pclass_dummy ,Age_dummy  , SibSp_dummy , Parch_dummy , Fare_dummy , Embarked_dummy , Title_dummy] ,  axis = 1 )","c852a399":"# check that is every thing is ok \nall_dummies.shape , all_dummies.isna().sum() ","262f1923":"final_data = pd.concat([selected_data , all_dummies] , axis = 1)\nfinal_data","369522ce":"# drop the original columns after geting the dummies\nfinal_data.drop(['Pclass' ,'Age','SibSp', 'Parch', 'Fare' , 'Embarked' , 'Title' , 'PassengerId'] , axis = 1 , inplace = True)\nfinal_data","a8844735":"import math, time ,random , datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection , metrics\nfrom sklearn.linear_model import LinearRegression , LogisticRegression, SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n","c06ad0fc":"# now split the data to train and test with the same original test shape\nnew_train_df = final_data.iloc[0 : -(test_df.shape[0]), : ]\nnew_test_df = final_data.iloc[-(test_df.shape[0]) : , : ]","6f6ff4fc":"X_train = new_train_df.drop('Survived' , axis = 1 )\ny_train = new_train_df.Survived\n\nnew_test_df = new_test_df.drop('Survived' , axis = 1)\n\nX_train.shape , y_train.shape , new_test_df.shape","124995ce":"# Function that runs the requested algorithm and returns the accuracy metrics\ndef fit_ml_algo(algo, X_train, y_train, cv):\n    \n    model = algo.fit(X_train,y_train)\n    acc = round(model.score(X_train,y_train) *100  , 2)\n    \n    #cross Validation\n    train_pred = model_selection.cross_val_predict(algo,\n                                                  X_train,y_train,\n                                                  cv = cv , n_jobs = -1)\n    #cross-validation Acc metric\n    acc_cv = round(metrics.accuracy_score(y_train , train_pred) *100, 2)\n    \n    return train_pred,acc , acc_cv","01bee723":"start_time = time.time()\ntrain_pred_log , acc_log , acc_cv_log = fit_ml_algo(LogisticRegression() ,\n                                                    X_train, y_train ,\n                                                    10)\nlog_time = (time.time() - start_time)\nprint('Accuracy: ', acc_log)\nprint(\"Accuracy CV 10-fold\" , acc_cv_log)\nprint(\"Running Time\" , datetime.timedelta(seconds = log_time))","a923bbc9":"start_time = time.time()\ntrain_pred_knn , acc_knn , acc_cv_knn = fit_ml_algo(KNeighborsClassifier() ,\n                                                    X_train, y_train ,\n                                                    10)\nknn_time = (time.time() - start_time)\nprint('Accuracy: ', acc_knn)\nprint(\"Accuracy CV 10-fold\" , acc_cv_knn)\nprint(\"Running Time\" , datetime.timedelta(seconds = knn_time))","0ac077dc":"start_time = time.time()\ntrain_pred_gaussian, acc_gaussian, acc_cv_gaussian = fit_ml_algo(GaussianNB(), \n                                                                      X_train, \n                                                                      y_train, \n                                                                           10)\ngaussian_time = (time.time() - start_time)\nprint('Accuracy: ', acc_gaussian)\nprint(\"Accuracy CV 10-fold\" , acc_cv_gaussian)\nprint(\"Running Time\" , datetime.timedelta(seconds = gaussian_time))","be011dfb":"start_time = time.time()\ntrain_pred_svc, acc_linear_svc, acc_cv_linear_svc = fit_ml_algo(LinearSVC(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\nlinear_svc_time = (time.time() - start_time)\nprint('Accuracy: ', acc_linear_svc)\nprint(\"Accuracy CV 10-fold\" , acc_cv_linear_svc)\nprint(\"Running Time\" , datetime.timedelta(seconds = linear_svc_time))","14edce14":"start_time = time.time()\ntrain_pred_sgd, acc_sgd, acc_cv_sgd = fit_ml_algo(SGDClassifier(), \n                                                  X_train, \n                                                  y_train,\n                                                  10)\nsgd_time = (time.time() - start_time)\nprint('Accuracy: ', acc_sgd)\nprint(\"Accuracy CV 10-fold\" , acc_cv_sgd)\nprint(\"Running Time\" , datetime.timedelta(seconds = sgd_time))","aa9c4763":"start_time = time.time()\ntrain_pred_dt, acc_dt, acc_cv_dt = fit_ml_algo(DecisionTreeClassifier(), \n                                                                X_train, \n                                                                y_train,\n                                                                10)\ndt_time = (time.time() - start_time)\nprint('Accuracy: ', acc_dt)\nprint(\"Accuracy CV 10-fold\" , acc_cv_dt)\nprint(\"Running Time\" , datetime.timedelta(seconds = dt_time))","e37ede38":"start_time = time.time()\ntrain_pred_gbt, acc_gbt, acc_cv_gbt = fit_ml_algo(GradientBoostingClassifier(), \n                                                                       X_train, \n                                                                       y_train,\n                                                                       10)\ngbt_time = (time.time() - start_time)\nprint('Accuracy: ', acc_gbt)\nprint(\"Accuracy CV 10-fold\" , acc_cv_gbt)\nprint(\"Running Time\" , datetime.timedelta(seconds = gbt_time))","f99f9cfa":"models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees'],\n    'Score': [\n        acc_knn, \n        acc_log,  \n        acc_gaussian, \n        acc_sgd, \n        acc_linear_svc, \n        acc_dt,\n        acc_gbt,\n    ]})\nprint(\"---Reuglar Accuracy Scores---\")\nmodels.sort_values(by='Score', ascending=False)","767df21e":"cv_models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees'],\n    'Score': [\n        acc_cv_knn, \n        acc_cv_log,      \n        acc_cv_gaussian, \n        acc_cv_sgd, \n        acc_cv_linear_svc, \n        acc_cv_dt,\n        acc_cv_gbt,\n    ]})\nprint('---Cross-validation Accuracy Scores---')\ncv_models.sort_values(by='Score', ascending=False)","84893862":"gbt_base_model = GradientBoostingClassifier(learning_rate=0.1,\n                                            n_estimators=100,\n                                            max_depth=3,\n                                            max_features='sqrt')","21b2c54b":"param_grid = {'learning_rate':[0.1,0.05,0.15],\n              'n_estimators':[100,250,500] , \n              'max_depth':[3,4,5],\n             'max_features': ['auto','sqrt']}\n\ntuning = GridSearchCV(gbt_base_model , param_grid = param_grid , cv = 5 , verbose = True, n_jobs = -1 )\ntuning.fit(X_train,y_train)\n","56c30623":"print('Best Score: ' + str(tuning.best_score_))\nprint('Best Parameters: ' + str(tuning.best_params_))","ed5b83d6":"gbt_base_model = GradientBoostingClassifier(learning_rate=0.05,\n                                            n_estimators=100,\n                                            max_depth=3,\n                                            max_features='sqrt')\ngbt_model=gbt_base_model.fit(X_train,y_train)\nacc = round(gbt_model.score(X_train,y_train) *100  , 2)\n#cross Validation\ntrain_pred = model_selection.cross_val_predict(gbt_model,\n                                              X_train,y_train,\n                                              cv = 10 , n_jobs = -1)\n#cross-validation Acc metric\nacc_cv = round(metrics.accuracy_score(y_train , train_pred) *100, 2)\nprint(acc_cv)\n","fbb60248":"gbt_model.feature_importances_","11011ee3":"# Feature Importance\ndef feature_importance(model, data):\n    \"\"\"\n    Function to show which features are most important in the model.\n    ::param_model:: Which model to use?\n    ::param_data:: What data to use?\n    \"\"\"\n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': data.columns})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[False, False]).iloc[-30:]\n    _ = fea_imp.plot(kind='barh', x='col', y='imp' , figsize=(20, 10))\n    return fea_imp","b937b94e":"feature_importance(gbt_model, X_train)","8b98fb85":"new_test_df","ae091f78":"# Make a prediction using the DecisionTreeClassifier model on the wanted columns\npredictions = gbt_model.predict(new_test_df)\npredictions[:10]","ec0adc09":"# Create a submisison dataframe and append the relevant columns\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = test_df['PassengerId']\nsubmission['Survived']= predictions.astype(int)\nsubmission.head(5)","18266340":"# What does our submission have to look like?\ngender_submission_df.head(5)","03fa67af":"from sklearn.metrics import confusion_matrix\n\nCM = confusion_matrix(gender_submission_df['Survived'], submission['Survived'])\nCM","631c328e":"precision = round(metrics.precision_score(gender_submission_df['Survived'], submission['Survived']) *100, 2)\nrecall =  round(metrics.recall_score(gender_submission_df['Survived'] , submission['Survived'] ) *100, 2)\nacc =  round(metrics.accuracy_score(gender_submission_df['Survived'] , submission['Survived'] ) *100, 2)\n\nprint('Precision :' ,precision )\nprint('Recall :' ,recall )\nprint('Acc :' ,acc )","657686f9":"submission.to_csv('submission.csv', index=False)\nprint('Submission CSV is ready!')","1b27cd5e":"### Logistic Regression","cc6be75e":"### Decision Tree Classifier","eb6e98db":"# Building Machine Learning Models","b4aa3d1b":"## get the columns ready for Machine Learning","bcc5d797":"### Gradient Boost Trees","17477633":"## deal with the missing data","8e5d798f":"## Model Results\n","ab9dfc56":"Tree specific parameters\n\nn_estimators=100 (number of trees)<br>\nmax_depth=3 <br>\nmin_samples_split=2<br>\nmin_samples_leaf=1<br>\nsubsample=1.0<br>\n\nOther Parameters<br>\nlearning_rate=0.1 (shrinkage)<br>\nsubsample: 1<br>","5de269b3":"**deal with Age and Fare**","c9e21db5":"### Gaussian Naive Bayes","0e9ae283":"# Submission","ee835804":"# Wrangle data","802a65ad":"### K-Nearest Neighbours","44c03e7b":"**Name columns**\n","60787ca7":"**Wonder if the capt survived !**","047331d7":"**Sex & Embarked columns**","14e20542":"**Ticket columns**\n","9efbce07":"## Data Descriptions\n\n**Survival:** 0 = No, 1 = Yes\n\n**pclass (Ticket class):** 1 = 1st, 2 = 2nd, 3 = 3rd\n\n**sex:** Sex\n\n**Age:** Age in years\n\n**sibsp:** number of siblings\/spouses aboard the Titanic\n\n**parch:** number of parents\/children aboard the Titanic\n\n**ticket:** Ticket number\n\n**fare:** Passenger fare\n\n**cabin:** Cabin number\n\n**embarked:** Port of Embarkation, C = Cherbourg, Q = Queenstown, S = Southampton","21d722d8":"## Tunning the best Model","70e7a047":"R.I.P Capt. Edward Gifford","ca46859d":"**get the dummies**","93551d04":"# Analyze & Visualize data","4beac5da":"### Linear Support Vector (SVC)","790373a6":"### Stochastic Gradient Descent"}}