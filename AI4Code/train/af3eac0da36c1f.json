{"cell_type":{"418c2663":"code","35e50fc1":"code","5363a869":"code","e094d910":"code","7bc430e8":"code","625da61a":"code","868b7a17":"code","f350764a":"code","217837c4":"code","116ed5fd":"code","4c4b4050":"code","67cde814":"code","c600ea18":"code","99cfa3f2":"code","2838fd4f":"code","245a84d3":"code","d786820c":"code","f65dee94":"code","c9db5bc3":"code","43f12677":"code","bdc06603":"code","fea890d0":"code","2eae28e9":"code","5c58fd5f":"code","839206d7":"code","9145b2d8":"code","ad5c17eb":"code","04cad2b1":"code","2dca4bdd":"code","4b95c716":"code","44cf1086":"code","4f3e0304":"code","bb4237f5":"code","1e7a28a3":"code","c2fefb1f":"code","d4c6c4db":"code","7ce67cac":"code","af5fee03":"code","69d729c2":"code","89ae6e36":"code","5389a8e6":"code","e489b67b":"code","84ac31e7":"code","08ea22e2":"code","b34344b5":"code","5edadd72":"code","73d71a20":"code","e1fd85df":"code","89e9ceb2":"markdown","d8c3fa60":"markdown","49e0fc25":"markdown","40416b44":"markdown","05fefad0":"markdown","85538f75":"markdown","59bb113f":"markdown","27bcd9e1":"markdown","e9bfcedc":"markdown","0c58be5a":"markdown","9ccf8ba7":"markdown","a00a37b6":"markdown","6d681000":"markdown","b6ed1bb8":"markdown","6f48fe30":"markdown","bcfd746a":"markdown","e16d7257":"markdown","b4b25bd6":"markdown","f66c15e0":"markdown","de6f0d46":"markdown","1714d676":"markdown","75085f81":"markdown","fdaafe49":"markdown","e60931a5":"markdown","aa256f9f":"markdown","4a89d4e1":"markdown","e04b2537":"markdown","79c8e823":"markdown","1214adf3":"markdown","d62355bd":"markdown","bbe3c6f0":"markdown","13e15883":"markdown","d6fb02e2":"markdown","5c99b2ea":"markdown","59e12fcc":"markdown","70228653":"markdown","ae607424":"markdown","733ef387":"markdown","f8b4d1dd":"markdown","11142bfc":"markdown","bae4df4d":"markdown","743c5b60":"markdown","20f7e032":"markdown","c687fd2f":"markdown","0e616dc8":"markdown"},"source":{"418c2663":"import pandas as pd \nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\n\n#nlp\nimport string\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\neng_stopwords = set(stopwords.words(\"english\"))\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","35e50fc1":"sentence1 = \"To be or not to be\"","5363a869":"len(sentence1)","e094d910":"len(sentence1.split(' '))","7bc430e8":"sentence1 = sentence1.split(' ')\nprint(sentence1)\n\nsentence1 = set(sentence1)\nsentence1","625da61a":"[s for s in sentence1 if len(s) > 2]","868b7a17":"sentence2 = \"Be who #you are and say what you feel, #because those who mind don't matter, and those who matter don't #mind. @Madonna @\"","f350764a":"sentence2 = sentence2.split(' ')\nsentence2","217837c4":"sentence2 = set(sentence2)","116ed5fd":"[s for s in sentence2 if s.startswith('m')]","4c4b4050":"[s for s in sentence2 if s.endswith('e')]","67cde814":"words = ['you', 'me', 'are', 'be']\n[s for s in sentence2 if s in words]","c600ea18":"[s for s in sentence2 if s.isupper()]","99cfa3f2":"[s for s in sentence2 if s.startswith('#')]","2838fd4f":"[s for s in sentence2 if s.startswith('@')]","245a84d3":"import re\n[s for s in sentence2 if re.search('@[A-Za-z0-9_]+',s)]","d786820c":"[s for s in sentence2 if re.search('@\\w+',s)]","f65dee94":"import nltk\nnltk.help.upenn_tagset('MD')","c9db5bc3":"nltk.pos_tag(sentence1)","43f12677":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np","bdc06603":"def read_and_concat_dataset(training_path, test_path):\n    train = pd.read_csv(training_path)\n    test = pd.read_csv(test_path)\n    data = train.append(test, ignore_index=True)\n    return train, test, data\n\ntrain, test, data = read_and_concat_dataset('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv', \n                                            '..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv')","fea890d0":"data.isnull().sum()","2eae28e9":"data['normal'] = np.where((data.toxic==0) & (data.threat==0) & (data.severe_toxic==0) & \n                          (data.obscene==0) & (data.insult==0) & (data.identity_hate==0),1,0)\ndata['normal'] = np.where((data.toxic.isnull()),np.nan,data['normal'])","5c58fd5f":"def cleaning_comments(data, text):\n    import re\n    data[text] = data[text].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n    data[text] = data[text].apply(lambda x: re.sub(r\"what's\", \"what is \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"\\'s\", \" \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"\\'ve\", \" have \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"can't\", \"cannot \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"\\'re\", \" are \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"n't\", \" not \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"i'm\", \"i am \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"\\'d\", \" would \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"\\'ll\", \" will \", x))\n    data[text] = data[text].apply(lambda x: re.sub(r\"you're\", \"you are \", x))\n    \ncleaning_comments(data, 'comment_text')","839206d7":"def example_of_comment(data, kind_of_comment, name_of_category):\n    print (name_of_category, data[data[kind_of_comment]==1].iloc[0,0])","9145b2d8":"example_of_comment(data,'toxic', 'Toxic comment:')","ad5c17eb":"example_of_comment(data,'threat', 'Threat comment:')","04cad2b1":"example_of_comment(data,'severe_toxic', 'Severe toxic comment:')","2dca4bdd":"example_of_comment(data,'obscene', 'Obscene comment:')","4b95c716":"example_of_comment(data,'insult', 'Insult comment:')","44cf1086":"example_of_comment(data,'identity_hate', 'Identity hate comment:')","4f3e0304":"example_of_comment(data,'normal', 'Normal comment:')","bb4237f5":"columns_of_comment = data.drop(['comment_text','id'],axis=1)\n\ncorrelation=columns_of_comment.corr()\nplt.figure(figsize=(8,6))\nsns.heatmap(correlation,\n            xticklabels=correlation.columns.values,\n            yticklabels=correlation.columns.values, annot=True)","1e7a28a3":"def crosstab(data, columns_of_comment, main_variable):\n    corr=[]\n    for other_variables in columns_of_comment.columns:\n        confusion_matrix = pd.crosstab(columns_of_comment[main_variable], columns_of_comment[other_variables])\n        corr.append(confusion_matrix)\n    output = pd.concat(corr,axis=1,keys=columns_of_comment.columns)\n    return output\n\ncrosstab(data, columns_of_comment, \"toxic\")","c2fefb1f":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, plot_title):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=300,\n        max_font_size=60, \n        scale=3,\n        random_state=1).generate(str(data))\n    fig = plt.figure(1, figsize=(10, 10))\n    plt.axis('off')\n    plt.imshow(wordcloud)\n    plt.title(plot_title)\n    plt.show()","d4c6c4db":"show_wordcloud(data['comment_text'][data.toxic==1],\"Toxic comments\")\nshow_wordcloud(data['comment_text'][data.threat==1],\"Threat comments\")\nshow_wordcloud(data['comment_text'][data.insult==1],\"Insult comments\")\nshow_wordcloud(data['comment_text'][data.obscene==1],\"Obscene comments\")\nshow_wordcloud(data['comment_text'][data.severe_toxic==1],\"Severe toxic comments\")\nshow_wordcloud(data['comment_text'][data.identity_hate==1],\"Identity hate comments\")\nshow_wordcloud(data['comment_text'][data.normal==1],\"Normal comments\")","7ce67cac":"data_working = data.copy()\nfrequence = pd.Series(' '.join(data_working['comment_text']).split()).value_counts()[:300]\nfrequence = list(frequence.index)\ndata_working['comment_text'] = data_working['comment_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in frequence))","af5fee03":"show_wordcloud(data_working['comment_text'][data_working.toxic==1],\"Toxic comments\")\nshow_wordcloud(data_working['comment_text'][data_working.threat==1],\"Threat comments\")\nshow_wordcloud(data_working['comment_text'][data_working.insult==1],\"Insult comments\")\nshow_wordcloud(data_working['comment_text'][data_working.obscene==1],\"Obscene comments\")\nshow_wordcloud(data_working['comment_text'][data_working.severe_toxic==1],\"Severe toxic comments\")\nshow_wordcloud(data_working['comment_text'][data_working.identity_hate==1],\"Identity hate comments\")\nshow_wordcloud(data_working['comment_text'][data_working.normal==1],\"Normal comments\")","69d729c2":"def new_variables(data, variable):\n    \n    data['comment_text_words'] = data[variable].apply(lambda x: len(str(x).split(\" \")))\n    \n    data['comment_text_chars'] = data[variable].str.len()\n    \n    data['comment_text_unique_words'] = data[variable].apply(lambda x: len(set(str(x).split())))\n    \n    data['comment_text_freq_of_unique_words'] = data['comment_text_unique_words'] \/ data['comment_text_words'] * 100\n    \n    data['spammer'] = np.where(data['comment_text_freq_of_unique_words']<20,1,0)\n    \n    from nltk.corpus import stopwords\n    stop = stopwords.words('english')\n    data['comment_text_stopwords'] = data[variable].apply(lambda x: len([x for x in x.split() if x in stop]))\n    \n    data['comment_text_uppers'] = data[variable].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n    \n    data['comment_text_title'] = data[variable].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n    \n    data['comment_text_punctuation'] = data[variable].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n    \n    data['comment_text_avg_length'] = data[variable].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n    \n    data['comment_text_smile'] = data[variable].apply(lambda x: sum(x.count(w) for w in (':-)', ':)', ';-)', ';)')))\n    \n    data['comment_text_exclamation'] = data[variable].apply(lambda x: x.count('!'))\n    \n    data['comment_text_question'] = data[variable].apply(lambda x: x.count('?'))\n    \n    \nnew_variables(data, 'comment_text')\ndata.head(3)","89ae6e36":"variables = ('comment_text_words','comment_text_chars','comment_text_unique_words',\n             'comment_text_freq_of_unique_words','spammer','comment_text_stopwords',\n             'comment_text_uppers', 'comment_text_title','comment_text_punctuation',\n             'comment_text_avg_length', 'comment_text_smile','comment_text_exclamation','comment_text_question')\ntypes_of_comment = ('toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate','normal')\n\nrows = [{c:data[f].corr(data[c]) for c in types_of_comment} for f in variables]\ndata_correlations = pd.DataFrame(rows, index=variables)\ndata_correlations","5389a8e6":"def number_of_comments_by_spammer(data, types, spam_variable):\n    comments_by_spammer = pd.DataFrame({'Kind of comment': [],\n                                        'Number of comments by spammer': []})\n\n    for type_of_comment in types:\n        num_of_comm_by_spammer = data[type_of_comment][(data[spam_variable]==1) & (data[type_of_comment]==1)].count() \n        comments_by_spammer=comments_by_spammer.append({'Kind of comment': type_of_comment, \n                                                        'Number of comments by spammer': num_of_comm_by_spammer},\n                                                        ignore_index=True)\n    return comments_by_spammer\n\nnumber_of_comments_by_spammer(data, ['identity_hate','insult','obscene','severe_toxic','threat','toxic','normal'], 'spammer')","e489b67b":"def average_number_of_words_and_chars_in_each_type_of_comment(data, types):\n    avg_number_of_words_in_each_type_of_comment = pd.DataFrame({'Kind of comment': [],\n                                                                'Average number of words in comment': [],\n                                                                'Average number of chars in comment': []})\n    for type_of_comment in types:\n        number_of_words = data.comment_text_words[data[type_of_comment]==1].sum()\n        number_of_comments = data.comment_text[data[type_of_comment]==1].count()\n        avg_words_number = number_of_words\/number_of_comments\n        \n        number_of_chars = data.comment_text_chars[data[type_of_comment]==1].sum()\n        avg_chars_number = number_of_chars\/number_of_comments\n        \n        avg_number_of_words_in_each_type_of_comment = avg_number_of_words_in_each_type_of_comment.append({'Kind of comment': type_of_comment, \n                                                        'Average number of words in comment': avg_words_number,\n                                                        'Average number of chars in comment': avg_chars_number},ignore_index=True)\n    return avg_number_of_words_in_each_type_of_comment\n\naverage_number_of_words_and_chars_in_each_type_of_comment(data, ['identity_hate','insult','obscene','severe_toxic','threat','toxic','normal'])","84ac31e7":"data['username']=data[\"comment_text\"].apply(lambda x: re.findall(\"\\[\\[User(.*)\\|\",str(x)))","08ea22e2":"def fixing_values_in_variable(data, variable):\n    data[variable] = data[variable].map(lambda x: None if len(x)==0 else x)\n    data[variable] = data[variable].map(lambda x: x[0] if x!=None else x)","b34344b5":"fixing_values_in_variable(data, 'username')\ndata['link']=data[\"comment_text\"].apply(lambda x: re.findall(\"http:\/\/.*com\",str(x)))\nfixing_values_in_variable(data, 'link')\ndata['ip']=data[\"comment_text\"].apply(lambda x: re.findall(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",str(x)))\nfixing_values_in_variable(data, 'ip')","5edadd72":"data['comment_text']=data[\"comment_text\"].apply(lambda x: re.sub(\"\\[\\[(.*)\\|\",\"\",str(x)))\ndata['comment_text']=data[\"comment_text\"].apply(lambda x: re.sub(\"http:\/\/.*com\",\"\",str(x)))\ndata['comment_text']=data[\"comment_text\"].apply(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",str(x)))","73d71a20":"def pre_processing(data,variable,count):\n\n    data[variable] = data[variable].str.replace('[^\\w\\s]','')\n\n    from nltk.corpus import stopwords\n    stop = stopwords.words('english')\n    data[variable] = data[variable].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n\n    frequence = pd.Series(' '.join(data[variable]).split()).value_counts()[:count]\n    frequence = list(frequence.index)\n    data[variable] = data[variable].apply(lambda x: \" \".join(x for x in x.split() if x not in frequence))\n\n    data[variable] = data[variable].map(lambda x: x.strip())\n\n    import re\n    data[variable] = data[variable].map(lambda x: re.sub(r'\\d+', '', x))\n\n    from textblob import Word\n    data[variable] = data[variable].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n    \n    \npre_processing(data,'comment_text',100)","e1fd85df":"data.head(3)","89e9ceb2":"We cane see that every toxic comment is also severe toxic. ","d8c3fa60":"I want to add \"normal\" variable to identify comments which aren't toxic.","49e0fc25":"**Toxic Comment data set**","40416b44":"Let's see words included in MD (modal).","05fefad0":"Text mining, equivalent to text analytics, is the process of getting high-quality information from text. We can analyse comments, reviews, posts, articles and so on. For example text mining helps to check that comments or reviews are positive or negative.\n\nThis kernel will be updated and improved over time as I acquire new knowledge.\n\nFor now, this work is a collection of useful functions of text analysis using basic libraries and nltk, and a helpful source for feature engineering for other data sets.\n\nIf this kernel is useful to you, upvote and enjoy :)","85538f75":"##Text Mining","59bb113f":"\"[A-Za-z0-9_]\" can be replace with meta-character \"\\w\"","27bcd9e1":"##Simple preprocessing","e9bfcedc":"![image.jpeg](attachment:image.jpeg)","0c58be5a":"Let's look how each kind of toxic comment like","9ccf8ba7":"##Simple variables from text","a00a37b6":"Toxic comments are mainly written by spammers.","6d681000":"It is difficult to notice any clear correlation here.","b6ed1bb8":"The comments contain IP addresses, links and usernames that are unique and do not contribute to analysis. We should extract these things and remove them from comments.","6f48fe30":"**Which words start with 'm'?**","bcfd746a":"![image.png](attachment:image.png)","e16d7257":"First of all, we need to unify the text, getting rid of capital letters and shortened forms. For this purpose the function \"cleaning_comments\" will be defined.","b4b25bd6":"Adding a few new variables (with a few hypotheses):\n\n* Number of words - haters usually write longer messages\n* Number of characters (with spaces) - haters usually write longer messages\n* Stop words and number of them\n* Uppercase words - haters often use caps lock\n* Number of unique words - haters often spam\n* Frequency of unique words (comments with freq value below 30% can be something lik spam)\n* Number of title\n* Number of punctuations - haters use it rarely\n* Average length of words\n* Number of smiles - haters use it rarely\n* Number of exclamation marks - popular among haters\n* Number of question marks - haters use it rarely","f66c15e0":"**Finding hashtags**","de6f0d46":"**Any word from our list 'words'?**","1714d676":"A few very popular techniques to make text more readable fo machines:\n* *lower case*\n* *removing white spaces*\n* *removing stopwords* - a stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n* *removing punctuation*\n* *removing common words*\n* *removing numbers*\n* *tokenization* - split text to individual tokens (words)\n* *stemming * - the process of converting a word to its base form\n* *lemmatization* - the process of converting a word to its meaningful base form\n\n","75085f81":"**Which words end with 'e'?**","fdaafe49":"**A few meta-characters**:\n\n* . \u2013 wildcard, matches a single character\n* ^ - start of a string\n* $ - end of a string\n* [] \u2013 match one of the set of characters within []\n* [a-z] \u2013 letters from a to z\n* [^abc] \u2013 matches a character that is not a, b or c\n* a|b \u2013 matchers either a or b\n* () - scoping for operators\n* \\ - special characters \\n ect.\n* \\b \u2013 matches word boundary\n* \\d \u2013 any digit [0-9]\n* \\D \u2013 any non-dogit [^0-9]\n* \\s \u2013 any whitespace\n* \\S \u2013 any non-whitespace\n* \\w \u2013 alphanumeric character [A-Za-z0-9]\n","e60931a5":"In this comment \"89.205.38.27\" looks like IP address, so it's something to check later.","aa256f9f":"##**Trying text mining on real data**","4a89d4e1":"*Is there a relationship between comment length and toxicity?*\n\nI will prepare a table that will contain the types of comments, the average number of words in the comments and the average number of characters in the comments.","e04b2537":"##**NLP - Part of Speech Tagging (POS)**","79c8e823":"##Regular Expressions","1214adf3":"**Length of sentence**","d62355bd":"..but it doesn't work well every time.\n\nWe need to import Regular Expressions package and use better way to find callout.","bbe3c6f0":"Let's see how it looks in number","13e15883":"Now I want to check how it looks after dropping common words, but I don't want to drop it from data now, so let's make new dataset - data_working, drop common words and make the same word clouds.","d6fb02e2":"To show the most popular words in each category of toxic comments I'm gonna use word clouds.","5c99b2ea":"**Number of words in sentence**","59e12fcc":"Correlation between category of comment","70228653":"Any missing values in independent variables?","ae607424":"We can easily see part of speech of every word in sentence.","733ef387":"Thanks for reading.\n\nStay tuned :)","f8b4d1dd":"**Leave only unique word in sentence**\n\nFirstly we need to split sentence into list of words or signs.","11142bfc":"In nltk we have function for Stemming (**nltk.PorterStemmer**) and for Lemmatization (**nltk.WordNetLemmatizer**). \n\nTo split sentences to individual words we can use Tokenization (**nltk.word_tokenize**).\n\nSplitting text into individual sentences is possible by Sentence Tokenization (**nltk.sent_tokenize**)","bae4df4d":"Removing IP, links and usernames","743c5b60":"Now is time to basic pre-processing, because present comment text is something like a mess.\n\n* Removing punctuation\n* Removing stop words\n* Removing common words\n* Removing white spaces\n* Removing numbers\n* Lemmatization","20f7e032":"**Finding callout**","c687fd2f":"##**Natural Language Processing**\n(nltk package)","0e616dc8":"**Which words are shorter than two letters?**"}}