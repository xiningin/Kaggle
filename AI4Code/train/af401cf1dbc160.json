{"cell_type":{"b0321a6e":"code","38e4ac13":"code","d279bdf3":"code","6f911a7a":"code","3477a4ad":"code","8baa850f":"code","ad59be2f":"code","6c863634":"code","ba6c1e33":"code","23e0beea":"code","6d9f2a0e":"code","d8f74201":"code","c7d7a3df":"code","e398c718":"code","364269fd":"code","20ed0dd9":"code","dac0a28d":"code","8ba66ef7":"code","336bebc6":"code","9e41b913":"code","677ce1bf":"code","90999700":"code","a6852d14":"code","b0d5d582":"code","90406256":"code","8748fd54":"code","e55e3a55":"code","dc622e97":"code","f416bd32":"code","77cfcdfe":"code","20d7e187":"code","a0fa5c2a":"code","7f2877ab":"code","7aa4568b":"code","1361e7a3":"code","249d181e":"code","1d6ada61":"code","802a590c":"code","a1c2e40d":"code","58e91a73":"code","f4bfb0ff":"code","a06541e8":"code","b5e5e137":"code","97c507a1":"code","0c5873bd":"code","83192345":"code","d417338f":"code","bab07049":"code","2c69534d":"code","a2689b9a":"code","ca2f2fa2":"code","bdb217d1":"code","b2931d69":"code","c5de7d2c":"code","076e94cb":"code","fc60ff34":"code","83facd4d":"code","f3f5ba65":"code","a21d8c47":"code","f4df223a":"code","59e92b39":"code","d4d73a29":"code","566719d9":"code","7624b24c":"code","491436fc":"code","f05c8655":"code","3700007a":"code","5d0454e3":"code","eda4d692":"code","f6984bbb":"code","4a5bd629":"code","99d831d7":"code","f2ad8f35":"code","f8fda9c3":"code","1d003366":"code","62b8b62c":"code","abe2cb0a":"code","0729e223":"code","8df87c85":"code","2d3ca9a9":"code","f2c319fd":"code","6618de3e":"code","f18d2d87":"code","a5e1022b":"code","f45ca509":"markdown","fc368c61":"markdown","26ae2bd3":"markdown","4449116d":"markdown","b1e58bcd":"markdown","3c5dec11":"markdown","f3c707df":"markdown","f13956e0":"markdown","9874cd82":"markdown","eada4e62":"markdown","2150e6ef":"markdown","a7b02d64":"markdown","5292b0c3":"markdown","99f05ed2":"markdown","cc9113b7":"markdown","c999b2a8":"markdown","4e7f52bb":"markdown","72a3b05b":"markdown","6ec4b8f3":"markdown","0af2d0de":"markdown","43923a25":"markdown","7046062b":"markdown","a96ae864":"markdown","d5623135":"markdown","46cecf81":"markdown","85a849cf":"markdown","4d10894d":"markdown","83de38b0":"markdown","56fffda9":"markdown","12d5c259":"markdown","62286305":"markdown","67e0fcdd":"markdown","c308af62":"markdown","a0fea144":"markdown","987cef06":"markdown","b3939f04":"markdown","39df5299":"markdown","5843a1a6":"markdown","d2ab7d8e":"markdown","b705894c":"markdown","17ac5153":"markdown","578a5c27":"markdown","01117469":"markdown","5ed3309d":"markdown","1114d5e7":"markdown","226ffb7e":"markdown","08f8abec":"markdown","3d07c56f":"markdown","434a32ea":"markdown","cfa9eca5":"markdown","9735910b":"markdown","cad5b7fd":"markdown","69be9db4":"markdown","4fb08a76":"markdown","ee886e13":"markdown","40e0e381":"markdown","a7b471b2":"markdown","b36a5cb0":"markdown","cdf9330e":"markdown","bb383ffa":"markdown","3978e22e":"markdown","aeead88b":"markdown","54cd7b5a":"markdown","aa568e2e":"markdown","32dc77a9":"markdown","937c705e":"markdown","bb54e237":"markdown","c23d302c":"markdown","5f9f3ab8":"markdown","242e9287":"markdown","9a38f3eb":"markdown","1cb31f6e":"markdown","60846215":"markdown","c87c0c73":"markdown","967c6776":"markdown","a39bae73":"markdown","baf11934":"markdown","913ebd29":"markdown","6b899654":"markdown","c681dc08":"markdown","e71de608":"markdown","f640a1e0":"markdown","72691132":"markdown","da7d227b":"markdown","25fdf4b7":"markdown","3ef47489":"markdown","1088d6c9":"markdown","1aa46305":"markdown","a07d0f0e":"markdown","233210f8":"markdown","64ff9f43":"markdown","0ebd5872":"markdown","ea97f8a2":"markdown","138ac51a":"markdown"},"source":{"b0321a6e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n\n# To avoid Warning message inbetween ...\nimport warnings\nwarnings.filterwarnings('ignore')","38e4ac13":"df = pd.read_csv(\"..\/input\/Attrition1.csv\")","d279bdf3":"#Quick Analysis on Dataset : DataTypes, Rows and Columns ,Null values, Unique values ...\ndef quick_analysis(df):\n    print(\"Data Types:\")\n    print(df.dtypes)\n    print(\"\\nRows and Columns:\")\n    print(df.shape)\n    print(\"\\nColumn names:\")\n    print(df.columns)\n    print(\"\\nNull Values\")\n    print(df.apply(lambda x: sum(x.isnull()) \/ len(df)))\n    print(\"\\nUnique values\")\n    print(df.nunique())\n\nquick_analysis(df)","6f911a7a":"#Dropping the unwanted columns: Those having only one unique value.\ndf=df.drop([\"EmployeeCount\",\"Over18\",\"StandardHours\",\"EmployeeNumber\"],axis=1)","3477a4ad":"#Visual Exploratory Data Analysis (EDA) And Your First Model\n#EDA on Feature Variables\nprint(list(set(df.dtypes.tolist())))\ndf_object = df.select_dtypes(include=[\"object\"]).copy()\ndf_int = df.select_dtypes(include=['int64']).copy()\n\ncategorical = df_object.columns\nnumerical = df_int.columns\n\nprint(\"Datashape of Object Dataframe:\",df_object.shape)\nprint(\"Datashape of Interger Dataframe:\",df_int.shape)","8baa850f":"# Univariate Analysis\n# EDA with Categorical Variables\n\nfig,ax = plt.subplots(3,2, figsize=(20,20))\nfor variable,subplot in zip(categorical,ax.flatten()):\n    sns.countplot(df[variable],ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(20)","ad59be2f":"# EDA with Numerical Variables\ndf[numerical].hist(bins=50,figsize=(16,20),layout=(8,3))","6c863634":"# Bivariate analysis - Categorical (Target variable) vs Numerical ( Feature Variables)\nfig , ax =plt.subplots(4,6,figsize=(30,30))\nfor var,subplot in zip(numerical,ax.flatten()):\n    sns.boxplot(x=\"Attrition\",y=var,data=df, ax=subplot)","ba6c1e33":"#fig , ax =plt.subplots(3,8,figsize=(30,30))\nfor var,subplot in zip(numerical,ax.flatten()):\n    facet = sns.FacetGrid(df,hue=\"Attrition\",aspect=4)\n    facet.map(sns.kdeplot,var,shade= True)\n    facet.set(xlim=(0,df[var].max()))\n    facet.add_legend()\n    plt.show()\n    #sns.boxplot(x=\"Attrition\",y=var,data=df, ax=subplot) ","23e0beea":"g = sns.FacetGrid(df, col=\"Attrition\",row=\"Gender\",aspect=1,height=4,hue=\"Department\") \ng.map(sns.distplot, \"YearsAtCompany\")\ng.set(xlim=(0,df[\"YearsAtCompany\"].max()))\n#plt.xlim(0,6)\ng.add_legend()","6d9f2a0e":"g = sns.FacetGrid(df, col=\"Attrition\",row=\"Department\",hue=\"Gender\",aspect=1,height=5) \ng.map(sns.distplot, \"MonthlyIncome\")\ng.add_legend()\ng.set(xlim=(0,df[\"MonthlyIncome\"].max()))","d8f74201":"g = sns.FacetGrid(df, col=\"Attrition\",row=\"JobRole\",hue=\"Gender\",aspect=1,height=5) \ng.map(sns.distplot, \"MonthlyIncome\")\ng.add_legend()\nplt.ylim(0,0.0010)\ng.set(xlim=(0,df[\"MonthlyIncome\"].max()))","c7d7a3df":"figure = plt.figure(figsize=(20,8))\nplt.hist([df[df['Attrition'] == 1]['MonthlyIncome'], df[df['Attrition'] == 0]['MonthlyIncome']], \n         stacked=True,\n         bins = 80, label = ['Attrition','Not_Attrition'])\nplt.xlabel('MonthlyIncome')\nplt.ylabel('Number of Employees')\nplt.legend();","e398c718":"df.groupby([\"JobRole\",\"Gender\"]).Attrition.value_counts()","364269fd":"labels = df['JobRole'].astype('category').cat.categories.tolist()\ncounts = df['JobRole'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nprint(sizes)\nfig1, ax1 = plt.subplots(figsize=(6,6))\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","20ed0dd9":"# Data Manuplation in the Dataset\n# Data types changes for :Features Variables\n\n# Variables in df needs to be changed to the object type from int64.\ndf[\"Education\"] = df[\"Education\"].astype(object)\ndf[\"EnvironmentSatisfaction\"] = df[\"EnvironmentSatisfaction\"].astype(object)\ndf[\"JobInvolvement\"] = df[\"JobInvolvement\"].astype(object)\ndf[\"JobLevel\"] = df[\"JobLevel\"].astype(object)\ndf[\"JobSatisfaction\"] = df[\"JobSatisfaction\"].astype(object)\ndf[\"PerformanceRating\"] = df[\"PerformanceRating\"].astype(object)\ndf[\"RelationshipSatisfaction\"] = df[\"RelationshipSatisfaction\"].astype(object)\ndf[\"StockOptionLevel\"] = df[\"StockOptionLevel\"].astype(object)\ndf[\"TrainingTimesLastYear\"] = df[\"TrainingTimesLastYear\"].astype(object)\ndf[\"WorkLifeBalance\"] = df[\"WorkLifeBalance\"].astype(object)","dac0a28d":"df[\"TotalWorkingYears\"] =np.sqrt(df[\"TotalWorkingYears\"])\ndf[\"YearsAtCompany\"] =np.sqrt(df[\"YearsAtCompany\"])\n\n# Taking log tranformation,so the data distribution look normal Distribution ...\ndf[\"MonthlyIncome\"] = np.log(df[\"MonthlyIncome\"])","8ba66ef7":"df.loc[ df['YearsInCurrentRole'] <= 2,'YearsInCurrentRole'] = 0\ndf.loc[ (df['YearsInCurrentRole'] >=3) & (df['YearsInCurrentRole'] <= 6) ,'YearsInCurrentRole'] = 1\ndf.loc[ (df['YearsInCurrentRole'] >=7) & (df['YearsInCurrentRole'] <= 10), 'YearsInCurrentRole'] = 2\ndf.loc[ (df['YearsInCurrentRole'] >=11) & (df['YearsInCurrentRole'] <= 18), 'YearsInCurrentRole'] = 3","336bebc6":"df.loc[ df['YearsSinceLastPromotion'] <= 1,'YearsSinceLastPromotion'] = 0\ndf.loc[ (df['YearsSinceLastPromotion'] >=2) & (df['YearsSinceLastPromotion'] <= 4) ,'YearsSinceLastPromotion'] = 1\ndf.loc[ (df['YearsSinceLastPromotion'] >=5) & (df['YearsSinceLastPromotion'] <= 7), 'YearsSinceLastPromotion'] = 2\ndf.loc[ (df['YearsSinceLastPromotion'] >=8) & (df['YearsSinceLastPromotion'] <= 15), 'YearsSinceLastPromotion'] = 3","9e41b913":"df.loc[ df['YearsWithCurrManager'] < 1,'YearsWithCurrManager'] = 0\ndf.loc[ (df['YearsWithCurrManager'] >=2) & (df['YearsWithCurrManager'] <= 3) ,'YearsWithCurrManager'] = 1\ndf.loc[ (df['YearsWithCurrManager'] >=4) & (df['YearsWithCurrManager'] <= 6), 'YearsWithCurrManager'] = 2\ndf.loc[ (df['YearsWithCurrManager'] >=7) & (df['YearsWithCurrManager'] <= 9), 'YearsWithCurrManager'] = 3\ndf.loc[ (df['YearsWithCurrManager'] >=10) & (df['YearsWithCurrManager'] <= 17), 'YearsWithCurrManager'] = 4","677ce1bf":"df.loc[ df['DistanceFromHome'] <= 2,'DistanceFromHome'] = 0\ndf.loc[ (df['DistanceFromHome'] >=3) & (df['DistanceFromHome'] <= 5) ,'DistanceFromHome'] = 1\ndf.loc[ (df['DistanceFromHome'] >=6) & (df['DistanceFromHome'] <= 8), 'DistanceFromHome'] = 2\ndf.loc[ (df['DistanceFromHome'] >=9) & (df['DistanceFromHome'] <= 12), 'DistanceFromHome'] = 3\ndf.loc[ (df['DistanceFromHome'] >=13) & (df['DistanceFromHome'] <= 20), 'DistanceFromHome'] = 4\ndf.loc[ (df['DistanceFromHome'] >=21) & (df['DistanceFromHome'] <= 29), 'DistanceFromHome'] = 5","90999700":"df.loc[ df['NumCompaniesWorked'] <= 1,'NumCompaniesWorked'] = 0\ndf.loc[ (df['NumCompaniesWorked'] >=2) & (df['NumCompaniesWorked'] <= 4) ,'NumCompaniesWorked'] = 1\ndf.loc[ (df['NumCompaniesWorked'] >=5) & (df['NumCompaniesWorked'] <= 6), 'NumCompaniesWorked'] = 2\ndf.loc[ (df['NumCompaniesWorked'] >=7) & (df['NumCompaniesWorked'] <= 9), 'NumCompaniesWorked'] = 3","a6852d14":"df.loc[ df['PercentSalaryHike'] <= 12,'PercentSalaryHike'] = 0\ndf.loc[ (df['PercentSalaryHike'] >=13) & (df['PercentSalaryHike'] <= 14) ,'PercentSalaryHike'] = 1\ndf.loc[ (df['PercentSalaryHike'] >=15) & (df['PercentSalaryHike'] <= 18), 'PercentSalaryHike'] = 2\ndf.loc[ (df['PercentSalaryHike'] >=19) & (df['PercentSalaryHike'] <= 21), 'PercentSalaryHike'] = 3\ndf.loc[ (df['PercentSalaryHike'] >=22) & (df['PercentSalaryHike'] <= 25), 'PercentSalaryHike'] = 4","b0d5d582":"df[\"YearsInCurrentRole\"] = df[\"YearsInCurrentRole\"].astype(object)\ndf[\"YearsSinceLastPromotion\"] = df[\"YearsSinceLastPromotion\"].astype(object)\ndf[\"YearsWithCurrManager\"] = df[\"YearsWithCurrManager\"].astype(object)\ndf[\"DistanceFromHome\"] = df[\"DistanceFromHome\"].astype(object)\ndf[\"NumCompaniesWorked\"] = df[\"NumCompaniesWorked\"].astype(object)\ndf[\"PercentSalaryHike\"] = df[\"PercentSalaryHike\"].astype(object)","90406256":"#EDA on Feature Variables\nprint(list(set(df.dtypes.tolist())))\ndf_object = df.select_dtypes(include=[\"object\"]).copy()\ndf_int = df.select_dtypes(include=['int64','float64']).copy()\n\ncategorical = df_object.columns\nnumerical = df_int.columns\n\nprint(\"Datashape of Object Dataframe:\",df_object.shape)\nprint(\"Datashape of Interger Dataframe:\",df_int.shape)","8748fd54":"# Features encoding and scaling\n# Preprocessing packages \nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n# Model selection for Train and Test split the dataset\nfrom sklearn.model_selection import train_test_split","e55e3a55":"def feature_imp_Dataset(df):\n    #Target Columns\n    target_col = [\"Attrition\"]\n\n    #Categorical Columns\n    cat_cols = df.nunique()[df.nunique() < 10].keys().tolist()\n    cat_cols = [x for x in cat_cols if x not in target_col]\n\n    #numerical columns\n    num_cols = [x for x in df.columns if x not in cat_cols + target_col]\n\n    #Binary columns with 2 values\n    bin_cols = df.nunique()[df.nunique() == 2].keys().tolist()\n\n    #Columns more than 2 values\n    multi_cols = [i for i in cat_cols if i not in bin_cols] \n    \n    df_feature_imp = df.copy()\n    \n    #Label encoding Binary columns\n    le = LabelEncoder()\n    for i in cat_cols:\n        df_feature_imp[i] = le.fit_transform(df_feature_imp[i])\n\n    #Dulpicating columns for Multiple value columns\n    #df = pd.get_dummies(data=df,columns= multi_cols,drop_first=True)\n    df_feature_imp= pd.get_dummies(data=df_feature_imp,columns= multi_cols)\n    \n    return df_feature_imp","dc622e97":"# Feature selection\n\n# Inorder to avoid the Dummy trap, we are removing the less Importance Dummy Varaible columns...\n# For this we are using the Random Forest to select the Importance Feature...\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100, max_features='sqrt')\n\n# Loading the dataset:\ndf_feature_imp = feature_imp_Dataset(df)\n\n# Def X and Y for Unscaled Dataset\ntarget_col = [\"Attrition\"]\ny = pd.DataFrame(df_feature_imp,columns=target_col)\n#y = df_unscaled[\"Attrition\"]\nX = df_feature_imp.drop('Attrition',1)\n\n# Fit the Model with the X and y ...\nclf = clf.fit(X, y)\nfeatures = pd.DataFrame()\nfeatures['feature'] = X.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\nfeatures.plot(kind='barh', figsize=(25, 25))","f416bd32":"# As per the Importance Features Techinque With the help of Random Forest Classifier we could see the below Dummy Variables\n# has less Importance compared to other Dummy variables, so we are removing those variables.\n\nto_drop_dummy_variable_trap= ['BusinessTravel_0','Department_0','DistanceFromHome_2','Education_4','EducationField_0','EnvironmentSatisfaction_1',\n 'JobInvolvement_3','JobLevel_3','JobRole_4','JobSatisfaction_1','MaritalStatus_0','NumCompaniesWorked_1','PercentSalaryHike_3',\n 'RelationshipSatisfaction_1','StockOptionLevel_2','TrainingTimesLastYear_5','WorkLifeBalance_3','YearsInCurrentRole_3','YearsInCurrentRole_3',\n 'YearsWithCurrManager_4']\n#df = df.drop(columns=to_drop_dummy_variable_trap)","77cfcdfe":"# Correlation Matrix - Orginal Dataset ...\n\n#correlation for Orginal Dataset\ncorrelation = df.corr()\n\n#tick labels\n#matrix_cols = correlation.columns.tolist()\n#convert to array\n#corr_array  = np.array(correlation)\n\n# Viewing the Correlation with respect to Attrition ...\ncorr_list = correlation['Attrition'].sort_values(axis=0,ascending=False)#.iloc[1:]\n#corr_list","20d7e187":"from sklearn.utils.class_weight import compute_class_weight\ndef _compute_class_weight_dictionary(y):\n    # helper for returning a dictionary instead of an array\n    classes = np.unique(y)\n    class_weight = compute_class_weight(\"balanced\", classes, y)\n    class_weight_dict = dict(zip(classes, class_weight))\n    return class_weight_dict   ","a0fa5c2a":"y=df[\"Attrition\"]\nprint(\"Class Weight for the Attrition Attribute:\")\n_compute_class_weight_dictionary(y)","7f2877ab":"def unscaled_data(df):\n    #global to_drop_dummy_variable_trap\n    #Target Columns\n    target_col = [\"Attrition\"]\n\n    #Categorical Columns\n    cat_cols = df.nunique()[df.nunique() < 10].keys().tolist()\n    cat_cols = [x for x in cat_cols if x not in target_col]\n\n    #numerical columns\n    num_cols = [x for x in df.columns if x not in cat_cols + target_col]\n\n    #Binary columns with 2 values\n    bin_cols = df.nunique()[df.nunique() == 2].keys().tolist()\n\n    #Columns more than 2 values\n    multi_cols = [i for i in cat_cols if i not in bin_cols]\n    \n    df_unscaled = df.copy()\n\n    #Label encoding Binary columns\n    le = LabelEncoder()\n    for i in cat_cols:\n        df_unscaled[i] = le.fit_transform(df_unscaled[i])\n\n    #Dulpicating columns for Multiple value columns\n    #df = pd.get_dummies(data=df,columns= multi_cols,drop_first=True)\n    df_unscaled= pd.get_dummies(data=df_unscaled,columns= multi_cols)\n\n    #Dropping original values merging scaled values for numerical columns\n    #f_unscaled = df.copy()\n    \n    ###############################################################################\n    # Remove collinear features for Unscaled Dataset ...\n    # Threshold to remove correlated Variables\n    threshold = 0.7\n    #0.8 - Initailly i have taken as\n\n    # Absolute value of corelation matrix\n    corr_matrix = df_unscaled.corr().abs()\n    corr_matrix.head()\n\n    # Upper triangle of correlations\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n    upper.head()\n\n    # Select columns with correlations above threshold\n    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n    print('There are %d columns to remove:' %(len(to_drop)))\n    print(\"Threshold more than %s \\n\" %threshold ,to_drop)\n    df_unscaled = df_unscaled.drop(columns=to_drop)\n    \n    #Columns thats is to avoid dummy variable trap: to_drop_dummy_variable_trap\n    \n    to_drop_dummy_variable_trap_un = [i for i in to_drop_dummy_variable_trap if i not in to_drop]\n    df_unscaled = df_unscaled.drop(columns=to_drop_dummy_variable_trap_un)\n    print(\"\\nRemoving variables to avoid Dummy variable trap:\\n\")\n    print(to_drop_dummy_variable_trap_un)\n    print(\"\\n\")\n    \n    ###############################################################################\n    #y=df_unscaledl[\"Attrition\"]\n    #print(\"Class Weight for the Attrition Attribute:\\n\")\n    #_compute_class_weight_dictionary(y)\n    #print(\"\\n\")\n    \n    ###############################################################################\n    # Prepare dataset\n    # Define (X, y)\n\n    # Def X and Y for Unscaled Dataset\n    y = pd.DataFrame(df_unscaled,columns=target_col)\n    #y = df_unscaled[\"Attrition\"]\n    X = df_unscaled.drop('Attrition',1)\n    \n    # execute this step if you need the Orginal \"Train test split :X_train, X_test, y_train, y_test\" \n    random_state =0\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = random_state)\n    # Defining Cols variables to store the Column names of X Unscaled dataframe.\n    cols = X_train.columns\n    \n    return X_train,X_test,y_train,y_test,cols,X,y","7aa4568b":"def scaled_data(df):\n    #Target Columns\n    target_col = [\"Attrition\"]\n\n    #Categorical Columns\n    cat_cols = df.nunique()[df.nunique() < 10].keys().tolist()\n    cat_cols = [x for x in cat_cols if x not in target_col]\n\n    #numerical columns\n    num_cols = [x for x in df.columns if x not in cat_cols + target_col]\n\n    #Binary columns with 2 values\n    bin_cols = df.nunique()[df.nunique() == 2].keys().tolist()\n\n    #Columns more than 2 values\n    multi_cols = [i for i in cat_cols if i not in bin_cols]\n    \n    df_scaled = df.copy()\n    \n    #Label encoding Binary columns\n    le = LabelEncoder()\n    for i in cat_cols:\n        df_scaled[i] = le.fit_transform(df_scaled[i])\n\n    #Dulpicating columns for Multiple value columns\n    #df = pd.get_dummies(data=df,columns= multi_cols,drop_first=True)\n    df_scaled = pd.get_dummies(data=df_scaled,columns= multi_cols)\n\n    #Scaling the Numerical columns\n    std = StandardScaler()\n    scaled = std.fit_transform(df_scaled[num_cols])\n    scaled = pd.DataFrame(scaled,columns=num_cols)\n\n    #Dropping original values merging scaled values for numerical columns\n    df_scaled = df_scaled.drop(columns= num_cols,axis=1)\n    df_scaled = df_scaled.merge(scaled,left_index=True,right_index=True,how=\"left\")\n    \n    ###############################################################################\n    # Threshold to remove correlated Variables\n    threshold = 0.7\n    #0.8 - Initailly i have taken as\n\n    # Absolute value of corelation matrix\n    corr_matrix = df_scaled.corr().abs()\n    corr_matrix.head()\n\n    # Upper triangle of correlations\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n    upper.head()\n\n    # Select columns with correlations above threshold\n    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n    print('There are %d columns to remove:' %(len(to_drop)))\n    print(\"Threshold more than %s \\n\" %threshold ,to_drop)\n    df_scaled = df_scaled.drop(columns=to_drop)\n    print(to_drop)\n    \n    #Columns thats is to avoid dummy variable trap: to_drop_dummy_variable_trap\n    \n    to_drop_dummy_variable_trap_un = [i for i in to_drop_dummy_variable_trap if i not in to_drop]\n    df_scaled = df_scaled.drop(columns=to_drop_dummy_variable_trap_un)\n    print(\"\\nRemoving variables to avoid Dummy variable trap:\\n\")\n    print(to_drop_dummy_variable_trap_un)\n    print(\"\\n\")\n    \n    ###############################################################################\n    # Def X and Y for Scaled Dataset\n    y_scale = pd.DataFrame(df_scaled,columns=target_col)\n    #y = df_unscaled[\"Attrition\"]\n    X_scale = df_scaled.drop('Attrition',1)\n    \n    ###############################################################################   \n    # execute this step if you need the Scaled \"Train test split :X_train, X_test, y_train, y_test\" \n    random_state = 0\n    X_train, X_test, y_train, y_test = train_test_split(X_scale,y_scale, test_size = 0.30, random_state = random_state)\n    # Defining Cols variables to store the Column names of X scaled dataframe.\n    cols = X_train.columns\n    \n    return X_train, X_test, y_train, y_test,cols,X_scale,y_scale","1361e7a3":"def cross_validate_(model,X,y,num_validations=5):\n    accuracy_train = cross_val_score(model,X,y,scoring=\"accuracy\",cv=num_validations)\n    precision_train = cross_val_score(model,X,y,scoring=\"precision\",cv=num_validations)\n    recall_train = cross_val_score(model,X,y,scoring=\"recall\",cv=num_validations)\n    f1_train = cross_val_score(model,X,y,scoring=\"f1_weighted\",cv=num_validations)                                  \n    \n    print(\"Cross Validation of : {}\".format(model.__class__.__name__))\n    print('*********************')\n    print(\" Model :\",model)\n    #print(\"Transforming {}\".format(transformer.__class__.__name__))\n    print(\"Accuracy: \" , round(100*accuracy_train.mean(), 2))\n    print(\"Precision: \",  round(100*precision_train.mean(), 2))\n    print(\"Recall: \",  round(100*recall_train.mean(), 2))\n    print(\"F1 Score: \",  round(100*f1_train.mean(), 2))\n    print('**************************************************************************\\n')\n","249d181e":"# Modelling\n# Baseline Model\n\n# Support functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_validate\nfrom scipy.stats import uniform\n\n# Fit models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nimport statsmodels.api as sm\n\n\n# Scoring functions\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve,scorer\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import mean_squared_error\n","1d6ada61":"# Basics function for Prediction\n# Function which can predict the Accuracy and Area under the curve(AUC) of test data from Train dataset in a single shot ...\n\ndef Attrition_prediction(algorthim,train_x,test_x,train_y,test_y,cols):\n    \n    #model\n    algorthim.fit(train_x,train_y)\n    predictions = algorthim.predict(test_x)\n    #probabilities = algorthim.predict_proba(test_x)\n     \n    #roc_auc_score\n    model_roc_auc = roc_auc_score(test_y,predictions)\n    \n    #RMSE values of Train Model ...\n    train_y_pred = algorthim.predict(train_x)\n    confusion_matrix(train_y,train_y_pred)\n\n    final_mse = mean_squared_error(train_y,train_y_pred) \n    train_final_rmse = np.sqrt(final_mse)\n    \n    #RMSE values of Test Model ...\n    final_mse = mean_squared_error(test_y,predictions) \n    test_final_rmse = np.sqrt(final_mse)\n    \n    #Confusion Matrix for Train Model ...\n    confuse_train = confusion_matrix(train_y, train_y_pred)\n    \n    #Confusion Matrix for Train Model ...\n    confuse_test = confusion_matrix(test_y, predictions)\n    \n    print(\"Algorthims parameters used :\\n\\n\",algorthim)\n    print(\"\\n Classification Report :\\n\", classification_report(test_y,predictions))\n    print(\"Accuracy Score of Train :\", accuracy_score(train_y,train_y_pred),\"\\n\")\n    print(\"Accuracy Score of Test :\", accuracy_score(test_y,predictions),\"\\n\")\n    print(\"Area under the curve :\",model_roc_auc,\"\\n\")\n    print(\"RMSE of the Train Model :\",train_final_rmse,\"\\n\")\n    print(\"Confusion Matrix of the Train Model :\\n\",confuse_train)\n    print(\"RMSE of the Test Model :\",test_final_rmse,\"\\n\")\n    print(\"Confusion Matrix of the Test Model :\\n\",confuse_test)\n","802a590c":"logreg = LogisticRegression()\nlogreg_cv = LogisticRegressionCV()\nrandom_f = RandomForestClassifier()\nknn = KNeighborsClassifier()\nsvc = SVC(kernel='linear')\nxgb = XGBClassifier()\n\nclassifiers = [logreg, logreg_cv, random_f, knn, svc, xgb]","a1c2e40d":"X_train,X_test,y_train,y_test,cols,X,y=unscaled_data(df)\nprint('**************************************************************************\\n')\n#for model in classifiers:\n    #cross_validate_(model,X,y,num_validations=7)","58e91a73":"X_train,X_test,y_train,y_test,cols,X,y=scaled_data(df)\nprint('**************************************************************************\\n')\nfor model in classifiers:\n    cross_validate_(model,X,y,num_validations=7)","f4bfb0ff":"logreg = LogisticRegression()\nlogreg_cv = LogisticRegressionCV()\nrandom_f = RandomForestClassifier()\nknn = KNeighborsClassifier()\nsvm = SVC()\nxgb = XGBClassifier()\nfrom sklearn.ensemble import BaggingClassifier\nclassifiers = [logreg, logreg_cv, random_f, knn, svm, xgb]","a06541e8":"X_train,X_test,y_train,y_test,cols,X,y=unscaled_data(df)\nprint('**************************************************************************\\n')\nfor model in classifiers:\n    print(\"Bagging Techinque on :{}\".format(model.__class__.__name__))\n    print(\"**********************\")\n    print(\"Model used:\", model)\n    bag_model = BaggingClassifier(base_estimator=model,n_estimators=100,bootstrap=True)\n    bag_model = bag_model.fit(X_train,y_train)\n    ytest_pred = bag_model.predict(X_test)\n    print(\"Bagging Accuarcy :\", bag_model.score(X_test,y_test))\n    print(\"Confusin Matrix :\\n \", confusion_matrix(y_test,ytest_pred))\n    print(\"***************************************************************************\\n\")\n    ","b5e5e137":"X_train,X_test,y_train,y_test,cols,X,y=scaled_data(df)\nprint('**************************************************************************\\n')\nfor model in classifiers:\n    print(\"Bagging Techinque on :{}\".format(model.__class__.__name__))\n    print(\"**********************\")\n    print(\"Model used:\", model)\n    bag_model = BaggingClassifier(base_estimator=model,n_estimators=100,bootstrap=True)\n    bag_model = bag_model.fit(X_train,y_train)\n    ytest_pred = bag_model.predict(X_test)\n    print(\"Bagging Accuarcy :\", bag_model.score(X_test,y_test))\n    print(\"Confusin Matrix :\\n \", confusion_matrix(y_test,ytest_pred))\n    print(\"***************************************************************************\\n\")\n    ","97c507a1":"from tpot import TPOTClassifier\nfrom tpot import TPOTRegressor","0c5873bd":"tpot = TPOTClassifier(generations=5,verbosity=2)\nX_train,X_test,y_train,y_test,cols,X,y=unscaled_data(df)\ntpot.fit(X_train.values,y_train.values)","83192345":"tpot.score(X_test.values,y_test.values)","d417338f":"#tpot.export('tpot_Attrition_modeling_pipeline.py')","bab07049":"tpot = TPOTClassifier(generations=5,verbosity=2)\nX_train,X_test,y_train,y_test,cols,X,y=scaled_data(df)\ntpot.fit(X_train.values,y_train.values)","2c69534d":"tpot.score(X_test.values,y_test.values)","a2689b9a":"#tpot.export('tpot_Attrition_modeling_scaled_pipeline.py')","ca2f2fa2":"#Logistic Regression...\n\n# Using Unscaled Data set for Logistic Regression...\n\nclassifier = LogisticRegression()\nX_train,X_test,y_train,y_test,cols,X,y=unscaled_data(df)\nAttrition_prediction(classifier,X_train,X_test,y_train,y_test,cols)","bdb217d1":"#Logistic Regression...\n\n# Using Scaled Data set for Logistic Regression...\n\nclassifier = LogisticRegression()\nX_train,X_test,y_train,y_test,cols,X,y=scaled_data(df)\nAttrition_prediction(classifier,X_train,X_test,y_train,y_test,cols)","b2931d69":"# Grid Search on Logistic Regression ...\n\n# Fit the parameters for logistic regression ...\n\nparam_grid = {'C':np.logspace(-3,3,8),'penalty':[\"l1\",\"l2\"],'max_iter':[100],'intercept_scaling':[1]}\nlog_param_grid = GridSearchCV(LogisticRegression(),param_grid=param_grid,cv=10,refit=True,verbose=1)\n\nX_train,X_test,y_train,y_test,cols,X,y=unscaled_data(df)\n#Applying Grid Search on Orginal Dataset ...\nlog_param_grid.fit(X_train,y_train)\n# Find the best estimator from the model ...\nfinal_model=log_param_grid.best_estimator_\n# Predicting the Accuracy and AUC value from the function we defined above ...\nAttrition_prediction(final_model,X_train,X_test,y_train,y_test,cols)","c5de7d2c":"# Grid Search on Logistic Regression ...\n\n# Fit the parameters for logistic regression ...\n\nparam_grid = {'C':np.logspace(-3,3,8),'penalty':[\"l1\",\"l2\"],'max_iter':[100],'intercept_scaling':[0.97,0.98,1]}\nlog_param_grid = GridSearchCV(LogisticRegression(),param_grid=param_grid,cv=7,refit=True,verbose=1)\n\nX_train,X_test,y_train,y_test,cols,X,y=scaled_data(df)\n#Applying Grid Search on Orginal Dataset ...\nlog_param_grid.fit(X_train,y_train)\n# Find the best estimator from the model ...\nfinal_model=log_param_grid.best_estimator_\n# Predicting the Accuracy and AUC value from the function we defined above ...\nAttrition_prediction(final_model,X_train,X_test,y_train,y_test,cols)","076e94cb":"# KNN ...\n# Using Unscaled Data set for KNN...\nclassifier = KNeighborsClassifier()\nX_train,X_test,y_train,y_test,cols,X,y=unscaled_data(df)\nAttrition_prediction(classifier,X_train,X_test,y_train,y_test,cols)","fc60ff34":"# Using Scaled Data set for KNN...\nclassifier = KNeighborsClassifier()\nX_train,X_test,y_train,y_test,cols,X,y=scaled_data(df)\nAttrition_prediction(classifier,X_train,X_test,y_train,y_test,cols)","83facd4d":"# Grid Search on KNN Scaled Dataset ...\n\n# Fit the parameters for KNN ...\nparam_grid = {'n_neighbors':[3,5,7,9],'weights':['uniform','distance'],'metric':['euclidean','manhattan','minkowski'],\n             'leaf_size':[40,45,50,60]}\nknn_param_grid = GridSearchCV(KNeighborsClassifier(),param_grid,cv=7,refit=True,n_jobs=-1,verbose=1)\nX_train,X_test,y_train,y_test,cols,X,y=scaled_data(df)\n\n#Applying Grid Search on Orginal Dataset ...\nknn_param_grid.fit(X_train,y_train)\n# Find the best estimator from the model ...\nfinal_model=knn_param_grid.best_estimator_\n# Predicting the Accuracy and AUC value from the function we defined above ...\nAttrition_prediction(final_model,X_train,X_test,y_train,y_test,cols)","f3f5ba65":"# Grid Search on KNN unScaled Dataset ...\n\n# Fit the parameters for KNN ...\nparam_grid = {'n_neighbors':[3,5,7,9,11],'weights':['uniform','distance'],'metric':['euclidean','manhattan','minkowski'],\n             'leaf_size':[60,90,100,150,200,300]}\nknn_param_grid = GridSearchCV(KNeighborsClassifier(),param_grid,cv=7,refit=True,n_jobs=-1,verbose=1)\nX_train,X_test,y_train,y_test,cols,X,y=unscaled_data(df)\n\n#Applying Grid Search on Orginal Dataset ...\nknn_param_grid.fit(X_train,y_train)\n# Find the best estimator from the model ...\nfinal_model=knn_param_grid.best_estimator_\n# Predicting the Accuracy and AUC value from the function we defined above ...\nAttrition_prediction(final_model,X_train,X_test,y_train,y_test,cols)\n","a21d8c47":"# Random Forest \n\n#Create a Gaussian Classifier ...\n# Using scaled Data set for Random Forest ...\nclassifier = RandomForestClassifier()\nX_train,X_test,y_train,y_test,cols,X,y=scaled_data(df)\nAttrition_prediction(classifier,X_train,X_test,y_train,y_test,cols)\n","f4df223a":"#Create a Gaussian Classifier ...\n# Using Unscaled Data set for Random Forest ...\nclassifier = RandomForestClassifier()\nX_train,X_test,y_train,y_test,cols,X,y=unscaled_data(df)\nAttrition_prediction(classifier,X_train,X_test,y_train,y_test,cols)","59e92b39":"# Grid Search on Random Forest Scaled Dataset ...\n\n# Fit the parameters for Random Forest ...\nparam_grid = {'max_depth':[3,4,5,6],'max_features':['sqrt', 'auto', 'log2'],'n_estimators':[50,100],\n             'min_samples_split':[2,3,5,6,7],'bootstrap':[True,False],'min_samples_leaf':[1,3,10]}\nclass_weight = dict({0: 0.5961070559610706, 1: 3.1012658227848102})\ncross_validation = StratifiedKFold(n_splits=10)\nRF_param_grid = GridSearchCV(RandomForestClassifier(class_weight=class_weight),param_grid,cv=cross_validation,refit=True,n_jobs=-1,verbose=1)\n\nX_train,X_test,y_train,y_test,cols,X,y=scaled_data(df)\n\n#Applying Grid Search on Orginal Dataset ...\nRF_param_grid.fit(X_train,y_train)\n# Find the best estimator from the model ...\nfinal_model=RF_param_grid.best_estimator_\n# Predicting the Accuracy and AUC value from the function we defined above ...\nAttrition_prediction(final_model,X_train,X_test,y_train,y_test,cols)\n","d4d73a29":"# Grid Search on Random Forest UnScaled Dataset ...\n\n# Fit the parameters for Random Forest ...\nparam_grid = {'max_depth':[3,4,5,6],'max_features':['sqrt', 'auto', 'log2'],'n_estimators':[50,100],\n             'min_samples_split':[2,3,5,6,7],'bootstrap':[True,False],'min_samples_leaf':[1,3,10]}\ncross_validation = StratifiedKFold(n_splits=10)\nclass_weight = dict({0: 0.5961070559610706, 1: 3.1012658227848102})\nRF_param_grid = GridSearchCV(RandomForestClassifier(class_weight=class_weight),param_grid,cv=cross_validation,refit=True,n_jobs=-1,verbose=1)\n\nX_train,X_test,y_train,y_test,cols,X,y=unscaled_data(df)\n\n#Applying Grid Search on Orginal Dataset ...\nRF_param_grid.fit(X_train,y_train)\n# Find the best estimator from the model ...\nfinal_model=RF_param_grid.best_estimator_\n# Predicting the Accuracy and AUC value from the function we defined above ...\nAttrition_prediction(final_model,X_train,X_test,y_train,y_test,cols)\n","566719d9":"# Support Vector Machine\n\n# Using Unscaled Data set.\nclassifier = SVC(kernel='linear')\nX_train,X_test,y_train,y_test,cols,X,y=unscaled_data(df)\nAttrition_prediction(classifier,X_train,X_test,y_train,y_test,cols)","7624b24c":"# Using Scaled Data set ...\nclassifier = SVC(kernel='linear')\nX_train,X_test,y_train,y_test,cols,X,y=scaled_data(df)\nAttrition_prediction(classifier,X_train,X_test,y_train,y_test,cols)","491436fc":"# Grid Search on SVM Scaled Dataset ...\n\n# Fit the parameters for SVM ...\nparam_grid = {'C':[0.45,0.5,0.51,0.53,0.55,1,1.5,5],'kernel': ['linear']}\ncross_validation = StratifiedKFold(n_splits=10)\nSVC_param_grid = GridSearchCV(SVC(),param_grid,cv=cross_validation,refit=True,n_jobs=-1,verbose=1)\nX_train,X_test,y_train,y_test,cols,X,y=scaled_data(df)\n\n#Applying Grid Search on Orginal Dataset ...\nSVC_param_grid.fit(X_train,y_train)\n# Find the best estimator from the model ...\nfinal_model=SVC_param_grid.best_estimator_\n# Predicting the Accuracy and AUC value from the function we defined above ...\nAttrition_prediction(final_model,X_train,X_test,y_train,y_test,cols)","f05c8655":"# Grid Search on SVM-RBF Scaled Dataset ...\n\n# Fit the parameters for SVM ...\nparam_grid = {'C':[0.5,1,1.5,5],'gamma':[1,0.1,0.01,0.001],'probability':[True,False],'kernel': ['rbf']}\ncross_validation = StratifiedKFold(n_splits=5)\nSVC_param_grid = GridSearchCV(SVC(),param_grid,cv=cross_validation,refit=True,n_jobs=-1,verbose=1)\nX_train,X_test,y_train,y_test,cols,X,y=scaled_data(df)\n\n#Applying Grid Search on Orginal Dataset ...\nSVC_param_grid.fit(X_train,y_train)\n# Find the best estimator from the model ...\nfinal_model=SVC_param_grid.best_estimator_\n# Predicting the Accuracy and AUC value from the function we defined above ...\nAttrition_prediction(final_model,X_train,X_test,y_train,y_test,cols)","3700007a":"# Grid Search on SVM Scaled Dataset ...\n\n# Fit the parameters for SVM ...\nparam_grid = {'C':[0.5,1,1.5,5],'gamma':[1,0.1,0.01,0.001],'probability':[True,False],'kernel': ['poly'],\n             'degree':[2,3]}\ncross_validation = StratifiedKFold(n_splits=5)\nSVC_param_grid = GridSearchCV(SVC(),param_grid,cv=cross_validation,refit=True,n_jobs=-1,verbose=1)\nX_train,X_test,y_train,y_test,cols,X,y=scaled_data(df)\n\n#Applying Grid Search on Orginal Dataset ...\nSVC_param_grid.fit(X_train,y_train)\n# Find the best estimator from the model ...\nfinal_model=SVC_param_grid.best_estimator_\n# Predicting the Accuracy and AUC value from the function we defined above ...\nAttrition_prediction(final_model,X_train,X_test,y_train,y_test,cols)","5d0454e3":"# Extreme Gradient boosting classifier using Scaled Data ...\n\nclassifier = XGBClassifier()\nX_train,X_test,y_train,y_test,cols,X,y=scaled_data(df)\n# Predicting the Accuracy and AUC value from the function we defined above ...\nAttrition_prediction(classifier,X_train,X_test,y_train,y_test,cols)","eda4d692":"# Extreme Gradient boosting classifier using UnScaled Data ...\n\nclassifier = XGBClassifier()\nX_train,X_test,y_train,y_test,cols,X,y=unscaled_data(df)\n# Predicting the Accuracy and AUC value from the function we defined above ...\nAttrition_prediction(classifier,X_train,X_test,y_train,y_test,cols)","f6984bbb":"# A parameter grid for XGBoost\nparams = {\n        'n_estimators' : [100, 200, 500, 750],\n        'learning_rate' : [0.01, 0.02, 0.05, 0.1, 0.25],\n        'min_child_weight': [1, 5, 7, 10],\n        'gamma': [0.1, 0.5, 1, 1.5, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5, 10, 12]\n        }\n\nfolds = 5\nparam_comb = 800\nfrom sklearn.model_selection import RandomizedSearchCV\n# Extreme Gradient boosting classifier using UnScaled Data ...\nclassifier = XGBClassifier()\nxgb_grid = RandomizedSearchCV(classifier, param_distributions=params,n_iter=param_comb, cv=5,n_jobs=-1, refit=True, verbose=1)\n\nX_train,X_test,y_train,y_test,cols,X,y=unscaled_data(df)\n#Applying Grid Search on Orginal Dataset ...\nxgb_grid.fit(X_train,y_train)\n# Find the best estimator from the model ...\nfinal_model=xgb_grid.best_estimator_\n# Predicting the Accuracy and AUC value from the function we defined above ...\nAttrition_prediction(final_model,X_train,X_test,y_train,y_test,cols)","4a5bd629":"# Applying PCA function on training \n# and testing set of X component \n\nfrom sklearn.decomposition import PCA\nX_train,X_test,y_train,y_test,cols,X,y= scaled_data(df)\n\npca = PCA().fit(X)\n#Plotting the Cumulative Summation of the Explained Variance\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Attrition Level')\n","99d831d7":"explained_variance = pca.explained_variance_ratio_ \nlen(explained_variance)","f2ad8f35":"# execute this step if you need the \"PCA\" Scaled \"Train test split :X_train, X_test, y_train, y_test\" \npca = PCA(0.95)\nX_unscale_pca = pca.fit_transform(X)\nX_train,X_test,y_train,y_test,cols,X,y= scaled_data(df)\nrandom_state = 0\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state = random_state)\n# Defining Cols variables to store the Column names of X scaled dataframe.\n\n#Logistic Regression...\n\n# Using PCA scaled Data set for Logistic Regression...\n\nclassifier = LogisticRegression()\nAttrition_prediction(classifier,X_train,X_test,y_train,y_test,cols)\nprint(\"************************************************************\")\nclassifier = SVC(kernel='linear')\nAttrition_prediction(classifier,X_train,X_test,y_train,y_test,cols)","f8fda9c3":"# Load the Imbalance Librarires for the further processing ...\nfrom imblearn.over_sampling import SMOTE,RandomOverSampler\nfrom imblearn.under_sampling import ClusterCentroids,NearMiss,RandomUnderSampler\nfrom imblearn.combine import SMOTEENN,SMOTETomek\n#from imblearn.ensemble import BalanceCascade\n\nfrom sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report \nfrom sklearn.metrics import recall_score,accuracy_score,confusion_matrix, f1_score, precision_score, auc,roc_auc_score,roc_curve, precision_recall_curve","1d003366":"# Helper functions\n\ndef benchmark(sampling_type,X,y):\n    #clf = LogisticRegression(penalty='l2')\n    clf = model\n    param_grid = {'C':[0.01,0.1,1,10]}\n    \"\"\"LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='warn',\n          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n          tol=0.0001, verbose=0, warm_start=False)\"\"\"\n    grid_search_lr = GridSearchCV(estimator=clf,param_grid=param_grid,scoring='accuracy',cv=10,verbose=1,refit=True,n_jobs=-1)\n    grid_search_lr = grid_search_lr.fit(X.values,y.values.ravel())\n    \n    return sampling_type,grid_search_lr.best_score_,grid_search_lr.best_params_['C']\n\ndef transform(transformer,X,y):\n    print(\"Transforming {}\".format(transformer.__class__.__name__))\n    X_resampled,y_resampled =transformer.fit_sample(X.values,y.values.ravel())\n    return transformer.__class__.__name__,pd.DataFrame(X_resampled),pd.DataFrame(y_resampled)\n    \n        ","62b8b62c":"\nX_train,X_test,y_train,y_test,cols,X,y= unscaled_data(df)\n\ndatasets = []\ndatasets.append((\"base\",X_train,y_train))\ndatasets.append(transform(SMOTE(n_jobs=-1),X_train,y_train))\ndatasets.append(transform(RandomOverSampler(),X_train,y_train))\ndatasets.append(transform(NearMiss(n_jobs=-1),X_train,y_train))\ndatasets.append(transform(RandomUnderSampler(),X_train,y_train))\ndatasets.append(transform(SMOTEENN(),X_train,y_train))\ndatasets.append(transform(SMOTETomek(),X_train,y_train))\n","abe2cb0a":"benchmark_scores =[]\nfor sample_type,X,y in datasets:\n    print('__________________________________________________________________________')\n    print('{}'.format(sample_type))\n    benchmark_scores.append(benchmark(sample_type,X,y))\n    print('__________________________________________________________________________')","0729e223":"benchmark_scores","8df87c85":"# Train\/evaluate models for each of tranformed datasets\n\nscores=[]\n# Train model based on benchmark params ...\n\nfor sample_type,score,parm in benchmark_scores:\n    print(\"Training on {}\".format(sample_type))\n    clf = LogisticRegression(penalty='l1',C=parm)\n    for s_type,X,y in datasets:\n        if s_type == sample_type:\n            clf.fit(X.values,y.values.ravel())\n            pred_test = clf.predict(X_test.values)\n            pred_test_probs = clf.predict_proba(X_test.values)\n            probs = clf.decision_function(X_test.values)\n            fpr, tpr , thresholds = roc_curve(y_test.values.ravel(),pred_test)\n            p,r,t = precision_recall_curve(y_test.values.ravel(),probs)\n            scores.append((sample_type,\n                          f1_score(y_test.values.ravel(),pred_test), \n                          precision_score(y_test.values.ravel(),pred_test),\n                           recall_score(y_test.values.ravel(),pred_test),\n                           accuracy_score(y_test.values.ravel(),pred_test),\n                           auc(fpr,tpr),\n                           auc(p,r,reorder=True),\n                           confusion_matrix(y_test.values.ravel(),pred_test)))","2d3ca9a9":"#Tabulate results\nsampling_results_unscaled = pd.DataFrame(scores,columns=['Sampling Type','F1 Score','Precision','Recall','Accuracy','AUC_Score',\n                                               'AUC_PR','Confusion Matrix'])\nsampling_results_unscaled","f2c319fd":"\nX_train,X_test,y_train,y_test,cols,X,y= scaled_data(df)\n\ndatasets = []\ndatasets.append((\"base\",X_train,y_train))\ndatasets.append(transform(SMOTE(n_jobs=-1),X_train,y_train))\ndatasets.append(transform(RandomOverSampler(),X_train,y_train))\ndatasets.append(transform(NearMiss(n_jobs=-1),X_train,y_train))\ndatasets.append(transform(RandomUnderSampler(),X_train,y_train))\ndatasets.append(transform(SMOTEENN(),X_train,y_train))\ndatasets.append(transform(SMOTETomek(),X_train,y_train))","6618de3e":"benchmark_scores =[]\nfor sample_type,X,y in datasets:\n    print('__________________________________________________________________________')\n    print('{}'.format(sample_type))\n    benchmark_scores.append(benchmark(sample_type,X,y))\n    print('__________________________________________________________________________')","f18d2d87":"# Train\/evaluate models for each of tranformed datasets\n\nscores=[]\n# Train model based on benchmark params ...\n\nfor sample_type,score,parm in benchmark_scores:\n    print(\"Training on {}\".format(sample_type))\n    clf = LogisticRegression(penalty='l1',C=parm)\n    for s_type,X,y in datasets:\n        if s_type == sample_type:\n            clf.fit(X.values,y.values.ravel())\n            pred_test = clf.predict(X_test.values)\n            pred_test_probs = clf.predict_proba(X_test.values)\n            probs = clf.decision_function(X_test.values)\n            fpr, tpr , thresholds = roc_curve(y_test.values.ravel(),pred_test)\n            p,r,t = precision_recall_curve(y_test.values.ravel(),probs)\n            scores.append((sample_type,\n                          f1_score(y_test.values.ravel(),pred_test), \n                          precision_score(y_test.values.ravel(),pred_test),\n                           recall_score(y_test.values.ravel(),pred_test),\n                           accuracy_score(y_test.values.ravel(),pred_test),\n                           auc(fpr,tpr),\n                           auc(p,r,reorder=True),\n                           confusion_matrix(y_test.values.ravel(),pred_test)))","a5e1022b":"#Tabulate results\nsampling_results_scaled = pd.DataFrame(scores,columns=['Sampling Type','F1 Score','Precision','Recall','Accuracy','AUC_Score',\n                                               'AUC_PR','Confusion Matrix'])\nsampling_results_scaled","f45ca509":"##### Grid Search on SVM - Poly Scaled Dataset ...","fc368c61":"### Modelling","26ae2bd3":"####  Scaled Dataset","4449116d":"##### Export the TPOT Unscaled PIPELINE Python file","b1e58bcd":"###### Execute this step if you need the \"PCA\" Scaled \"Train test split :X_train, X_test, y_train, y_test\" ","3c5dec11":"##### Check the output of Benchmarf Functions:","f3c707df":"### Logistic Regression...","f13956e0":"### Support Vector Machine","9874cd82":"##### Using Unscaled Data set for Random Forest ..","eada4e62":"##### PercentSalaryHike from the Dataframe ... Explanation as follows\n\n- 0 > PercentSalaryHike in between 11 and 12 - Termed as Less than one Year\n- 1 > PercentSalaryHike in between 13 and 14 - Termed as between 2 and 4 year \n- 2 > PercentSalaryHike in between 15 and 18 - Termed as between 5 and 8 year \n- 3 > PercentSalaryHike in between 19 and 21 - Termed as between 9 and 15 year\n- 4 > PercentSalaryHike in between 22 and 25 - Termed as between 9 and 15 year ","2150e6ef":"##### Scaled Data","a7b02d64":"##### Apply transform functions to Scaled dataset","5292b0c3":"##### Using Scaled Data set for Random Forest ...","99f05ed2":"####  Unsacled Dataset(Dataframe as its given...)","cc9113b7":"###### Data Manuplation in the Converted Object variables\n###### Data types changes for :Features Variables","c999b2a8":"##### Libraries needed for Imbalance Data","4e7f52bb":"##### Cross Validation: Scaled Dataset","72a3b05b":"##### Calling the Tpot Classifier for Unscaled dataset...","6ec4b8f3":"Here we have created a copy of the dataset after Feature engineering.\n\nThe below steps were taken as part of this funtion:\n\n - Label Encoded the categorical columns which has less than 10 unique elements.\n - Removed the variables which has more the 0.7 correlation value with respect to independent variables.\n - Removed some of the categorical variables which we converted with LabelEncoder inorder to avoid the Dummy Variable trap.\n - Split the Dataset to train and test for further techinques.","0af2d0de":"#### Using Unscaled Data","43923a25":"# How HR can help to prevent loss of good people to organization\n\nThe key to success in any organization is attracting and retaining top talent.\n\nEmployee churn can be defined as a leak or departure of an intellectual asset from a company or organization. Alternatively, in simple words, you can say, when employees leave the organization is known as churn. Another definition can be when a member of a population leaves a population, is known as churn.\n\nIn Research, it was found that employee churn will be affected by age, tenure, pay, job satisfaction, salary, working conditions, growth potential and employee\u2019s perceptions of fairness. Some other variables such as age, gender, ethnicity, education, and marital status, were essential factors in the prediction of employee churn. In some cases such as the employee with niche skills are harder to replace.\n\nIt affects the ongoing work and productivity of existing employees.\n\nAcquiring new employees as a replacement has its costs such as hiring costs and training costs. Also, the new employee will take time to learn skills at the similar level of technical or business expertise knowledge of an older employee. Organizations tackle this problem by applying machine learning techniques to predict employee churn, which helps them in taking necessary actions.\n\nYou are an HR analyst at a company, and one of the tasks is to determine which factors keep employees at the company and which prompt others to leave.\n\nYou need to know what factors can change to prevent the loss of good people.\n\n### What data we have?\nData has various data points on our employees, but you are most interested in whether they\u2019re still with my company or whether they\u2019ve gone to work somewhere else.\n\n### Data Information\n\n- Education: 1 'Below College\u2019, 2 'College\u2019, 3 'Bachelors', 4 'Master\u2019, 5 'Doctors'\n- Environment Satisfaction: 1 'Low', 2 'Medium', 3 'High', 4 'Very High'\n- Job Involvement: 1 'Low\u2019, 2 'Medium', 3 'High\u2019, 4 'Very High'\n- Job Satisfaction: 1 'Low\u2019, 2 'Medium\u2019, 3 'High\u2019, 4 'Very High'\n- Performance Rating: 1 'Low\u2019, 2 'Good\u2019, 3 'Excellent\u2019, 4 'Outstanding'\n- Relationship Satisfaction: 1 'Low\u2019, 2 'Medium\u2019, 3 'High\u2019, 4 'Very High'\n- Work Life Balance: 1 'Bad\u2019, 2 'Good\u2019, 3 'Better\u2019, 4 'Best","7046062b":"##### Loading the libraries:","a96ae864":"##### Tabulate results","d5623135":"##### Gridsearch for XGBoost","46cecf81":"Data Manuplation in the Dataset and Feature engineering and selection.\n\nTaking sqrt tranformation,so the data distribution look normal Distribution.\n\nTaking log tranformation on Montly Income,so the data distribution look normal Distribution ...","85a849cf":"##### YearsWithCurrManager from the Dataframe ... Explanation as follows\n\n- 0 > YearsWithCurrManager in between 0 and 1 - Termed as Less than one Year \n- 1 > YearsWithCurrManager in between 2 and 3 - Termed as between 2 and 3 year \n- 2 > YearsWithCurrManager in between 4 and 6 - Termed as between 4 and 6 year \n- 3 > YearsWithCurrManager in between 7 and 9 - Termed as between 7 and 9 year \n- 4 > YearsWithCurrManager in between 10 and 17 - Termed as between 10 and 17 year ","4d10894d":"### Exploratory Data Analysis (EDA)","83de38b0":"### Load the Dataset","56fffda9":"Algorthim which can be used for *Employee Attrition Modelling* can be:\n\n- Logistic Regression:\n    - Giving Test Accuracy of 89.756\n    - PCA Accuarcy : 89.56\n- SVC:\n    - Giving Test Accuarcy of 89.34\n- XGB:\n    - Giving Test Accuarcy of 87.30\n  ","12d5c259":"The below functions used to itrate different Imbalance Techique (Transform Functions) to Techinques (Benchmark).\n\nBenchmark:\n- Performing Logistic Regression with penalty \"L2\".\n- Doing Gridsearch on the above step.\n- Fit the Dataset.\n\nTransform:\n- Transformer - Different Imbalance Techinques.\n- Fit the Imbalanced libraries on the Dataset.","62286305":"##### Helper functions","67e0fcdd":"##### Bagging Techinque: Scaled Dataset","c308af62":"##### Using Unscaled Data set...","a0fea144":"##### Evaluate the Score of the TPOT Unscaled Dateset","987cef06":"##### Train\/evaluate models for each of tranformed Scaled datasets","b3939f04":"##### Evaluate the Score of the TPOT Unscaled Dateset","39df5299":"##### Cross Validation: Unscaled Dataset:","5843a1a6":"##### DistanceFromHome from the Dataframe ... Explanation as follows\n\n- 0 > DistanceFromHome in between 0 and 2 - Termed as Less than one Year -- Very close \n- 1 > DistanceFromHome in between 3 and 5 - Termed as between 2 and 4 year -- Normal Distance\n- 2 > DistanceFromHome in between 6 and 8 - Termed as between 5 and 8 year -- Modearte DiStance\n- 3 > DistanceFromHome in between 9 and 12 - Termed as between 9 and 15 year -- Average DiStance  \n- 4 > DistanceFromHome in between 13 and 20 - Termed as between 9 and 15 year -- Little Fare\n- 5 > DistanceFromHome in between 21 and 29 - Termed as between 9 and 15 year -- Long Fare away","d2ab7d8e":"### Random Forest ","b705894c":"### Basics function for Prediction","17ac5153":"### Load Libraries","578a5c27":"##### Grid Search on SVM Scaled Dataset ...","01117469":"### Correlation Matrix : With respect to Depedent variable","5ed3309d":"#### Train\/evaluate models for each of tranformed datasets","1114d5e7":"## Let's try different base models","226ffb7e":"The below function used to Test the data on models and find the Different Matrics scores for each Imbalance Techiques.","08f8abec":"### Univariate Analysis on: Numerical Variables","3d07c56f":"### Extreme Gradient boosting classifier","434a32ea":"#### Grid Search on KNN UnScaled Dataset ...","cfa9eca5":"##### Export the TPOT Scaled PIPELINE Python file","9735910b":"### Data Manuplation in the Dataset","cad5b7fd":"### Overcome Dummy Trap ","69be9db4":"##### Using Scaled Data set...","4fb08a76":"##### Determine best hyperparameters using: Benchmark","ee886e13":"##### Grid Search on Random Forest Unscaled Dataset ...","40e0e381":"### KNN","a7b471b2":"##### Loading neccesary libraries ...","b36a5cb0":"###### Explained variance ","cdf9330e":"### Univariate Analysis","bb383ffa":"### Quick Analysis on Dataset","3978e22e":"#### Bagging different Machine Learning Techinques ...","aeead88b":"## Bagging Techinque","54cd7b5a":"##### Using Scaled Data","aa568e2e":"### Imbalanced Data","32dc77a9":"### PCA ","937c705e":"#### Grid Search on KNN Scaled Dataset ...","bb54e237":"#### Using Scaled Data","c23d302c":"### Removing Unwanted Variables","5f9f3ab8":"#### Using Scaled Dataset","242e9287":"##### YearsInCurrentRole from the Dataframe ... Explanation as follows\n\n- 0 >YearsInCurrentRole in between 0 and 2 - Termed as Fresher in the Current Role.\n- 1 > YearsInCurrentRole in between 3 and 6 - Termed as Intermidate in the Current Role.\n- 2 > YearsInCurrentRole in between 7 and 10 - Termed as Experienced in the Current Role.\n- 3 > YearsInCurrentRole in between 11 and 18 - Termed as SME in the Current Role.","9a38f3eb":"### Data Prepartion ","1cb31f6e":"#####  Apply transform functions to Unscaled dataset\n","60846215":"### Auto ML method: TPOT","c87c0c73":"##### Function which can predict the Accuracy and Area under the curve(AUC) of test data from Train dataset in a single shot ...","967c6776":"##### NumCompaniesWorked from the Dataframe ... Explanation as follows\n\n0 > NumCompaniesWorked in between 0 and 2 - Termed as Less than one Year -- Very close \n1 > NumCompaniesWorked in between 3 and 5 - Termed as between 2 and 4 year -- Normal Distance\n2 > NumCompaniesWorked in between 6 and 8 - Termed as between 5 and 8 year -- Modearte DiStance\n3 > NumCompaniesWorked in between 9 and 12 - Termed as between 9 and 15 year -- Average DiStance  \n4 > NumCompaniesWorked in between 13 and 20 - Termed as between 9 and 15 year -- Little Fare\n5 > NumCompaniesWorked in between 21 and 29 - Termed as between 9 and 15 year -- Long Fare away","a39bae73":"#### Tabulate results","baf11934":"### Conclusion ","913ebd29":"Here we have created a copy of the dataset after Feature engineering.\n\nThe below steps were taken as part of this funtion:\n\n- Label Encoded the categorical columns which has less than 10 unique elements.\n- Removed the variables which has more the 0.7 correlation value with respect to independent variables.\n- Scaled the numerical variable inorder to standardize.\n- Removed some of the categorical variables which we converted with LabelEncoder inorder to avoid the Dummy Variable trap.\n- Split the Dataset to train and test for further techinques.","6b899654":"##### Bagging Techinque: Unscaled Dataset","c681dc08":"##### Calling the Tpot Classifier for Scaled dataset...","e71de608":"### Grid Search on Logistic Regression\n\n##### Unscaled Dataset","f640a1e0":"##### Grid Search on Random Forest Scaled Dataset ...","72691132":"##### Applying PCA on the Scaled Data","da7d227b":"#### Cross Validation Scores","25fdf4b7":"##### Determine best hyperparameters using: Benchmark","3ef47489":"##### Grid Search on SVM - RBF Scaled Dataset ...","1088d6c9":"### Cross Validation","1aa46305":"In this method we will use cross validation techinque into different Machine learning algorthims by using the all dataset inorder to calculate which Algorthims shows better:\n - Accuracy\n - Precision\n - Recall\n - F1 Score","a07d0f0e":"##### Using Unscaled Data ...","233210f8":"##### Using Unscaled Dataset","64ff9f43":"##### YearsSinceLastPromotion from the Dataframe ... Explanation as follows\n\n- 0 > YearsSinceLastPromotion in between 0 and 1 - Termed as Less than one Year -- Newly Promoted\n- 1 > YearsSinceLastPromotion in between 2 and 4 - Termed as between 2 and 4 year -- Waiting for a while for Promoting \n- 2 > YearsSinceLastPromotion in between 5 and 7 - Termed as between 5 and 7 year -- Too Much Waiting for Promoting\n- 3 > YearsSinceLastPromotion in between 8 and 15 - Termed as between 8 and 15 year -- No promotions given for along while","0ebd5872":"### Bivariate analysis","ea97f8a2":"### Feature engineering ","138ac51a":"### Feature selection"}}