{"cell_type":{"c2947194":"code","28b16858":"code","430ee81d":"code","5143abee":"code","d4bbf442":"code","52624f6f":"code","e8b0485f":"code","402dcadf":"code","41388b6d":"code","4161d943":"code","ce7deb34":"code","4d52258c":"code","66a891ac":"code","69ff26d3":"code","1cc7c0a3":"code","467e495f":"markdown","3a86e875":"markdown","00750a87":"markdown","204f9c78":"markdown","8a9588ab":"markdown"},"source":{"c2947194":"# Input data files are available in the \"..\/input\/\" directory.\n# Any results you write to the current directory are saved as output.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","28b16858":"# Import libraries\n%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, ConcatDataset\nfrom torchvision import transforms\nimport torch.optim as optim","430ee81d":"# Read csv files\ntrain_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\nsubmit_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')","5143abee":"# Training data\ntrain_df.head()","d4bbf442":"# Number of pixels\nnum_pixel = len(train_df.columns) - 1\nnum_pixel","52624f6f":"# Transformers\ntransform_0 = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\ntransform_1 = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\ntransform_2 = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomAffine(degrees=15, translate=(0.1,0.1), scale=(0.8,0.8)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\ntransform_3 = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomAffine(degrees=30, scale=(1.1,1.1)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\ntransform_4 = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomAffine(degrees=30, translate=(0.1,0.1)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\ntransform_5 = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomAffine(degrees=10, shear=45),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])","e8b0485f":"# Write a class that transform a DataFrame to PyTorch Dataset\n# Your custom dataset should inherit Dataset and override the following methods:\n    # __len__ so that len(dataset) returns the size of the dataset.\n    # __getitem__ to support the indexing such that dataset[i] can be used to get i\n\nclass DataFrame_to_Dataset(Dataset):\n    \n    def __init__(self, df, transform=transform_0):\n\n        # Get features and labels\n        if len(df.columns) == num_pixel:\n            # Test dataset\n            self.features = df.values.reshape((-1,28,28)).astype(np.uint8) # .astype(np.uint8) for ToPILImage transformer\n            self.labels = None\n        else:\n            # Train dataset\n            self.features = df.iloc[:,1:].values.reshape((-1,28,28)).astype(np.uint8)\n            self.labels = torch.from_numpy(df.label.values)\n        \n        # Transformer\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, index):\n            \n        if self.labels is not None:\n            return self.transform(self.features[index]), self.labels[index]\n        else:\n            return self.transform(self.features[index])","402dcadf":"def create_dataloaders(seed, test_size=0.1, df=train_df, batch_size=32):\n    # Create training set and validation set\n    train_data, valid_data = train_test_split(df,\n                                              test_size=test_size,\n                                              random_state=seed)\n    \n    # Create Datasets\n    train_dataset_0 = DataFrame_to_Dataset(train_data)\n    train_dataset_1 = DataFrame_to_Dataset(train_data, transform_1)\n    train_dataset_2 = DataFrame_to_Dataset(train_data, transform_2)\n    train_dataset_3 = DataFrame_to_Dataset(train_data, transform_3)\n    train_dataset_4 = DataFrame_to_Dataset(train_data, transform_4)\n    train_dataset_5 = DataFrame_to_Dataset(train_data, transform_5)\n    train_dataset = ConcatDataset([train_dataset_0, train_dataset_1, train_dataset_2, train_dataset_3, train_dataset_4, train_dataset_5])\n\n    valid_dataset = DataFrame_to_Dataset(valid_data)\n    \n    # Create Dataloaders\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n\n    return train_loader, valid_loader","41388b6d":"# Create a LeNet neural network\n\nclass Net(nn.Module):\n    def __init__(self):\n        # Super function. It inherits from nn.Module and we can access everythink in nn.Module\n        super().__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True), # inplace=True helps to save some memory\n            \n            nn.Conv2d(32, 32, kernel_size=3),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            \n            nn.Conv2d(32, 32, kernel_size=5, stride=2, padding=14), # compute same padding by hand\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(2, 2),\n            nn.Dropout2d(0.25)\n        )\n        \n        self.conv2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True), # inplace=True helps to save some memory\n            \n            nn.Conv2d(64, 64, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            \n            nn.Conv2d(64, 64, kernel_size=5, stride=2, padding=6), # compute same padding by hand\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(2, 2),\n            nn.Dropout2d(0.25)\n        )\n        \n        self.conv3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=4),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.25)\n        )\n        \n        self.fc = nn.Sequential(\n            nn.Linear(128*1*1, 10)\n        )\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = x.view(-1, 128*1*1)\n        x = self.fc(x)\n        \n        return x","4161d943":"# Get the device\nuse_cuda = torch.cuda.is_available()\nprint(use_cuda)","ce7deb34":"def train(seed, num_epochs):\n    \n    # Train and valid dataloaders\n    print('Creating new dataloaders...')\n    train_loader, valid_loader = create_dataloaders(seed=seed)\n\n    # Model\n    print('Creating a new model...')\n    net = Net()\n\n    # Loss function\n    criterion = nn.CrossEntropyLoss()\n\n    # Move to GPU\n    if use_cuda:\n        net.cuda()\n        criterion.cuda()\n\n    # Optimizer\n    optimizer = optim.Adam(net.parameters(), \n                           lr=0.003, betas=(0.9, 0.999), \n                           eps=1e-08, weight_decay=0, \n                           amsgrad=False)\n\n    # Sets the learning rate of each parameter group to the initial lr decayed by gamma every step_size epochs\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n\n    print('Training the model...')\n    for epoch in range(num_epochs):\n        \n        net.train() # Training mode -> Turn on dropout\n        t0 = time.time()\n        training_loss = 0.0\n        num_samples = 0\n        \n        for features, labels in train_loader:\n            \n            # Move data to GPU\n            if use_cuda:\n                features = features.cuda()\n                labels = labels.cuda()\n                \n            # Zero the parameter gradients\n            optimizer.zero_grad()\n            \n            # Forward + Backward + Optimize\n            outputs = net(features)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            # Update loss\n            training_loss += loss.item()\n            num_samples += len(features)\n            \n        # Compute accuracy on validation set\n        net.eval() # Evaluation mode -> Turn off dropout\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for valid_features, valid_labels in valid_loader:\n                \n                # Move data to GPU\n                if use_cuda:\n                    valid_features = valid_features.cuda()\n                    valid_labels = valid_labels.cuda()\n\n                outputs = net(valid_features)\n                _, predicted = torch.max(outputs, 1)\n                total += valid_labels.size(0)\n                correct += (predicted == valid_labels).sum().item()\n        \n        scheduler.step()\n        \n        print('[model %d, epoch %d, time: %.3f seconds] train_loss: %.5f, val_acc: %4f %%' %\n              (seed + 1, epoch + 1, time.time() - t0, training_loss\/num_samples, 100 * correct \/ total))\n    \n    # Prediction\n    net.eval() # Evaluation mode -> Turn off dropout\n    test_pred = torch.LongTensor()\n    \n    if use_cuda:\n        test_pred = test_pred.cuda()\n        \n    with torch.no_grad(): # Turn off gradients for prediction, saves memory and computations\n        for features in test_loader:\n\n            if use_cuda:\n                features = features.cuda()\n\n            # Get the softmax probabilities\n            outputs = net(features)\n            # Get the prediction of the batch\n            _, predicted = torch.max(outputs, 1)\n            # Concatenate the prediction\n            test_pred = torch.cat((test_pred, predicted), dim=0)\n    \n    model_name = 'model_' + str(seed + 1)\n    ensemble_df[model_name] = test_pred.cpu().numpy()\n    print('Prediction Saved! \\n')","4d52258c":"# Create test_loader\ntest_dataset = DataFrame_to_Dataset(test_df)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nensemble_df = submit_df.copy()\n\nnum_models = 11\nnum_epochs = 6\n\nfor seed in range(num_models):\n    train(seed, num_epochs)","66a891ac":"ensemble_df.head()","69ff26d3":"# Final prediction\nfinal_pred = ensemble_df.iloc[:,2:].mode(axis=1).iloc[:,0]\nsubmit_df.Label = final_pred.astype(int)\nsubmit_df.head()","1cc7c0a3":"# Create a submission file\nsubmit_df.to_csv('submission.csv', index=False)","467e495f":"# LeNet5 with Improvements\n\n- Two stacked 3x3 filters replace the single 5x5 filters.\n- A convolution with stride 2 replaces pooling layers. These become learnable pooling layers.\n- Batch normalization is added\n- Dropout is added\n- More feature maps (channels) are added\n\n![](https:\/\/pytorch.org\/tutorials\/_images\/mnist.png)","3a86e875":"# Ensemble","00750a87":"# Import Data","204f9c78":"# Argumentation, Datasets and DataLoaders","8a9588ab":"# Prediction"}}