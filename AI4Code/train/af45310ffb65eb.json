{"cell_type":{"0577c6bd":"code","67fb230b":"code","7f73e0af":"code","c9f89db7":"code","204a5a1e":"code","3247b275":"code","5be6f339":"code","18ab37ed":"code","afcdcc8c":"code","08bd3fff":"code","4105f650":"code","31b176b4":"code","4bce47ed":"code","632aa242":"code","2f0e7c41":"code","35166bba":"code","264eb7f3":"code","8d7d01fb":"code","9b6c1bf2":"code","f1ab7562":"code","89da89bb":"code","8e0e42bf":"code","4fde19b3":"code","49b2f8da":"code","40ffd373":"code","aeddbe32":"code","6cee700b":"code","fab4d428":"code","17655925":"markdown","67dd94ae":"markdown","71d0d442":"markdown","adb4f347":"markdown","01b48065":"markdown","09cc0268":"markdown","a7d51358":"markdown","3066c597":"markdown","929411d9":"markdown","0284a8a7":"markdown"},"source":{"0577c6bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_rows', None)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","67fb230b":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7f73e0af":"X = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\")","c9f89db7":"X.head()","204a5a1e":"X.shape","3247b275":"X.info()","5be6f339":"X.describe().T","18ab37ed":"X.isnull().sum()","afcdcc8c":"np.max(X.isnull().sum() \/ X.shape[0])","08bd3fff":"plt.rcParams[\"figure.dpi\"] = 100  # the dpi can be set to enhance the resolution of the image\n# Congiguring retina format\n%config InlineBackend.figure_format = \"retina\"\nsns.heatmap(X.isnull(), cmap=\"viridis\", yticklabels=False)","4105f650":"X.duplicated().sum()","31b176b4":"fig, axes = plt.subplots(5, 4, figsize=(20,25))\nfig.suptitle(\"Box plot for features between 0 and 19\", y=1)\nfig.subplots_adjust(top=2)\nplt.tight_layout()\n\nfor num_col, col in enumerate(X.columns[:20]):\n    sns.boxplot(data=X, y=col, ax=axes[num_col \/\/ 4][num_col % 4])\n    axes[num_col \/\/ 4][num_col % 4].set_title(X[col].name)","4bce47ed":"fig, axes = plt.subplots(5, 4, figsize=(20,25))\nfig.suptitle(\"Box plot for features between 20 and 39\", y=1)\nfig.subplots_adjust(top=2)\nplt.tight_layout()\n\nfor num_col, col in enumerate(X.columns[20:40]):\n    sns.boxplot(data=X, y=col, ax=axes[num_col \/\/ 4][num_col % 4])\n    axes[num_col \/\/ 4][num_col % 4].set_title(X[col].name)","632aa242":"fig, axes = plt.subplots(5, 4, figsize=(20,25))\nfig.suptitle(\"Box plot for features between 40 and 59\", y=1)\nfig.subplots_adjust(top=2)\nplt.tight_layout()\n\nfor num_col, col in enumerate(X.columns[40:60]):\n    sns.boxplot(data=X, y=col, ax=axes[num_col \/\/ 4][num_col % 4])\n    axes[num_col \/\/ 4][num_col % 4].set_title(X[col].name)","2f0e7c41":"fig, axes = plt.subplots(5, 4, figsize=(20,25))\nfig.suptitle(\"Box plot for features between 60 and 79\", y=1)\nfig.subplots_adjust(top=2)\nplt.tight_layout()\n\nfor num_col, col in enumerate(X.columns[60:80]):\n    sns.boxplot(data=X, y=col, ax=axes[num_col \/\/ 4][num_col % 4])\n    axes[num_col \/\/ 4][num_col % 4].set_title(X[col].name)","35166bba":"fig, axes = plt.subplots(5, 4, figsize=(20,25))\nfig.suptitle(\"Box plot for features between 80 and 99\", y=1)\nfig.subplots_adjust(top=2)\nplt.tight_layout()\n\nfor num_col, col in enumerate(X.columns[80:100]):\n    sns.boxplot(data=X, y=col, ax=axes[num_col \/\/ 4][num_col % 4])\n    axes[num_col \/\/ 4][num_col % 4].set_title(X[col].name)","264eb7f3":"fig, axes = plt.subplots(5, 4, figsize=(20,25))\nfig.suptitle(\"Box plot for features between 100 and 119\", y=1)\nfig.subplots_adjust(top=2)\nplt.tight_layout()\n\nfor num_col, col in enumerate(X.columns[100:120]):\n    sns.boxplot(data=X, y=col, ax=axes[num_col \/\/ 4][num_col % 4])\n    axes[num_col \/\/ 4][num_col % 4].set_title(X[col].name)","8d7d01fb":"sns.heatmap(X.corr(), cmap=\"RdYlGn\")","9b6c1bf2":"X.corr().describe().T","f1ab7562":"X.corr()[(X.corr() < -0.06) | ((X.corr() > 0.03) & (X.corr() < 1.0))].sum().sum()","89da89bb":"fig, axes = plt.subplots(5, 4, figsize=(20,25))\nfig.suptitle(\"Density plot for features between 0 and 19\", y=1)\nfig.subplots_adjust(top=2)\nplt.tight_layout()\n\nfor num_col, col in enumerate(X.columns[:20]):\n    _, FD_bins = np.histogram(X[col].index, bins=\"fd\")\n    bin_nr = min(len(FD_bins)-1, 50)\n    sns.histplot(data=X, x=col, bins=bin_nr, ax=axes[num_col \/\/ 4][num_col % 4], stat=\"density\", alpha=0.4, kde=True, kde_kws={\"cut\": 3})","8e0e42bf":"fig, axes = plt.subplots(5, 4, figsize=(20,25))\nfig.suptitle(\"Density plot for features between 20 and 39\", y=1)\nfig.subplots_adjust(top=2)\nplt.tight_layout()\n\nfor num_col, col in enumerate(X.columns[20:40]):\n    _, FD_bins = np.histogram(X[col].index, bins=\"fd\")\n    bin_nr = min(len(FD_bins)-1, 50)\n    sns.histplot(data=X, x=col, bins=bin_nr, ax=axes[num_col \/\/ 4][num_col % 4], stat=\"density\", alpha=0.4, kde=True, kde_kws={\"cut\": 3})\n    axes[num_col \/\/ 5][num_col % 4].set_title(X[col].name)","4fde19b3":"fig, axes = plt.subplots(5, 4, figsize=(20,25))\nfig.suptitle(\"Density plot for features between 40 and 59\", y=1)\nfig.subplots_adjust(top=2)\nplt.tight_layout()\n\nfor num_col, col in enumerate(X.columns[40:60]):\n    _, FD_bins = np.histogram(X[col].index, bins=\"fd\")\n    bin_nr = min(len(FD_bins)-1, 50)\n    sns.histplot(data=X, x=col, bins=bin_nr, ax=axes[num_col \/\/ 4][num_col % 4], stat=\"density\", alpha=0.4, kde=True, kde_kws={\"cut\": 3})","49b2f8da":"fig, axes = plt.subplots(5, 4, figsize=(20,25))\nfig.suptitle(\"Density plot for features between 60 and 79\", y=1)\nfig.subplots_adjust(top=2)\nplt.tight_layout()\n\nfor num_col, col in enumerate(X.columns[60:80]):\n    _, FD_bins = np.histogram(X[col].index, bins=\"fd\")\n    bin_nr = min(len(FD_bins)-1, 50)\n    sns.histplot(data=X, x=col, bins=bin_nr, ax=axes[num_col \/\/ 4][num_col % 4], stat=\"density\", alpha=0.4, kde=True, kde_kws={\"cut\": 3})","40ffd373":"fig, axes = plt.subplots(5, 4, figsize=(20,25))\nfig.suptitle(\"Density plot for features between 80 and 99\", y=1)\nfig.subplots_adjust(top=2)\nplt.tight_layout()\n\nfor num_col, col in enumerate(X.columns[80:100]):\n    _, FD_bins = np.histogram(X[col].index, bins=\"fd\")\n    bin_nr = min(len(FD_bins)-1, 50)\n    sns.histplot(data=X, x=col, bins=bin_nr, ax=axes[num_col \/\/ 4][num_col % 4], stat=\"density\", alpha=0.4, kde=True, kde_kws={\"cut\": 3})","aeddbe32":"fig, axes = plt.subplots(5, 4, figsize=(20,25))\nfig.suptitle(\"Density plot for features between 100 and 119\", y=1)\nfig.subplots_adjust(top=2)\nplt.tight_layout()\n\nfor num_col, col in enumerate(X.columns[100:120]):\n    _, FD_bins = np.histogram(X[col].index, bins=\"fd\")\n    bin_nr = min(len(FD_bins)-1, 50)\n    sns.histplot(data=X, x=col, bins=bin_nr, ax=axes[num_col \/\/ 4][num_col % 4], stat=\"density\", alpha=0.4, kde=True, kde_kws={\"cut\": 3})","6cee700b":"X.skew()","fab4d428":"X.kurt()","17655925":"<a id=\"duplicates\"><\/a>\n## V. Checking for duplicates\n\nThere are no duplicate lines in the training set.","67dd94ae":"<a id=\"distribution\"><\/a>\n## VIII. Checking the distribution of the dataset\n\nThe seaborn distplot method was popular, but is now deprecated. We present here a method to obtain a close display of the distributions of each column using histplot.\n\nWe find that some columns are skewed, and\/or have kurtosis. If a column has a value that is between -0.5 and 0.5, then everything is fine. If a column has a value that is between -1 and 1, then we can use it as is because the problem is moderate. If, on the other hand, a column has a value that is less than -1 or greater than 1, then we must intervene on it before going any further because it can distort the results of the analysis.","71d0d442":"<a id=\"stats\"><\/a>\n## III. Summary statistics\n\nWe display some statistics concerning the 120 columns, we transpose the display for the sake of readability.","adb4f347":"<a id=\"import\"><\/a>\n## I. Importing libraries\n\nWe're content with basic libraries for this EDA.","01b48065":"<a id=\"load\"><\/a>\n## II. Loading the dataset\n\nWe load the training set to perform the EDA. We aren't supposed to know the content of the test set to avoid overfitting it. We start by displaying the first few lines and some basic information.\n\nSo we see that the training set contains 957,919 rows and 120 columns. The id column is actually the index, and the claim column is our y target: both are of type int64. Finally, we have 118 features of type float64 which constitute our X.","09cc0268":"# EDA skewness\n\nAt first glance, the train.csv file in the data tab contains heavily skewed features. Is this reality or just an impression? There are simple methods to check the skewness and kurtosis of features, but let's start by performing an initial EDA on missing values, duplicates, outliers, and correlations, before going back to them.\n\nIf you're browsing this notebook, feel free to comment. I'll take into account the advice and comments to improve it.\n\n[I. Importing libraries](#import)\n\n[II. Loading the dataset](#load)\n\n[III. Summary statistics](#stats)\n\n[IV. Checking for null values](#null)\n\n[V. Checking for duplicates](#duplicates)\n\n[VI. Checking for outliers](#outliers)\n\n[VII. Checking the correlations](#correlations)\n\n[VIII. Checking the distribution of the dataset](#distribution)\n\nFor more information on handling large volumes of data with PySpark, see as well: [EDA with PySpark](https:\/\/www.kaggle.com\/cmarquay\/eda-pyspark)","a7d51358":"<a id=\"outliers\"><\/a>\n## VI. Checking for outliers\n\nWe display the boxplots of the 120 columns to check for the presence of outliers.","3066c597":"<a id=\"null\"><\/a>\n## IV. Checking for null values\n\nWe find that only the id and claim columns don't contain missing values, however these only represent less than 2% of each column and they're distributed randomly in the training set.","929411d9":"Feel free to comment. I'll take into account the advice and comments to improve this notebook.\n\nFor more information on handling large volumes of data with PySpark, see as well: [EDA with PySpark](https:\/\/www.kaggle.com\/cmarquay\/eda-pyspark)","0284a8a7":"<a id=\"correlations\"><\/a>\n## VII. Checking the correlations\n\nWe see the absence of obvious correlations between the features and the claim column, which is a bit annoying. We also note the absence of obvious correlations between the different features, which is on the other hand a good thing. The correlations are all between -0.06 and 0.03, which is very close to 0."}}