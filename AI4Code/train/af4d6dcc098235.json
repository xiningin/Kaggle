{"cell_type":{"25f0aac3":"code","515b34d7":"code","075cd26a":"code","27b7e322":"code","e575fa10":"code","34b421a6":"code","30d9a18d":"code","e540215b":"code","fb7fd72f":"code","83d52fd5":"code","02c28fe8":"code","04f2fc65":"markdown","3ab77ebd":"markdown","a0550d77":"markdown","5bdaf382":"markdown","6ad6775c":"markdown","9ea39ba8":"markdown","91838e02":"markdown","6f666c2b":"markdown","21752ac5":"markdown","45d8dbdb":"markdown","382a26f8":"markdown","2c01a207":"markdown","a21e7a14":"markdown"},"source":{"25f0aac3":"import numpy as np\nimport matplotlib.pyplot as plt","515b34d7":"# D\u00e9finition du g\u00e9n\u00e9rateur de nombres pseudo-al\u00e9atoires \u00e0 partir d'une graine\/seed (=n'importe quel entier)\n# Cela permet de tirer au sort toujours la m\u00eame s\u00e9rie de nombres al\u00e9atoires et de faire des tests sur le sch\u00e9ma d'inversion \n# si on souhaite ensuite tester le m\u00eame sch\u00e9ma avec une nouvelle s\u00e9rie de nombre al\u00e9atoire, on doit changer la \"seed\" ou graine (valeur entre parenth\u00e8se)\nnp.random.seed(123)\n#np.random.seed(456)","075cd26a":"# **************\n# * Param\u00e8tres *\n# **************\na = 1.  # param\u00e8tres de la droite\nb = -2.\nn = 100  # nombre de points","27b7e322":"# *************\n# * Fonctions *\n# *************\ndef droite(x, a, b):\n    return a * x + b\n\n# fonction densit\u00e9 de probabilit\u00e9\ndef L(y, yfit, n, sigma):\n    return np.exp( -(1\/n) * np.sum(((y - yfit) \/ sigma ) ** 2) )\n\n# fonction r\u00e9sidu\ndef res(y, yfit, n, sigma):\n    return (1\/n) * np.sum(((y - yfit) \/ sigma ) ** 2)","e575fa10":"# ********************************************        \n# * cr\u00e9ation du vecteur donn\u00e9es synth\u00e9tiques *\n# ********************************************      \n\n# D\u00e9finition de la valeur exacte suivant la relation y=ax+b\nx = np.linspace(0, 5, n)\ny_exact = droite(x, a, b)\n\n# On ajoute du bruit\n# la fonction random.randn g\u00e9n\u00e9re des nombres al\u00e9atoires suivant une loi normale de moyenne nulle et de variance 1\nsigma = 0.2\nbruit = np.random.randn(n) * sigma\ny = y_exact + bruit\n\n# On affiche la s\u00e9rie de donn\u00e9es \nplt.plot(x, y, 'bo', label='Original data', markersize=5) # nuage de points synth\u00e9tiques\nplt.plot(x, y_exact, 'g+', label='Exact values', markersize=5) # valeurs exactes\nplt.xlabel('axe x')\nplt.ylabel('axe y')\nplt.legend()\nplt.show()","34b421a6":"# rappel des valeurs exactes : a=1 et b=-2\n# information a priori sur a :\na_inf = 0.5\na_sup = 2\n\n# information a priori sur b :\nb_inf = -3\nb_sup = -0.5","30d9a18d":"# initialisation de l'inversion \n\n# a) d\u00e9finition du mod\u00e8le initial\n# La fonction random.uniform g\u00e9n\u00e9re al\u00e9atoirement un nombre compris (par d\u00e9faut) entre 0 et 1. Toutes \n# les valeurs \u00e0 l'int\u00e9rieur de l'intervalle ont la m\u00eame probabilit\u00e9 d'\u00eatre tir\u00e9es au sort.\n\n# param\u00e8tre a0 du mod\u00e8le initial\na_0 = a_inf + np.random.uniform() * (a_sup - a_inf)\n\n# param\u00e8tre b0 du mod\u00e8le initial\nb_0 = b_inf + np.random.uniform() * (b_sup - b_inf)\n\n# b) Calcul de la fonction de densit\u00e9 de probabilit\u00e9 associ\u00e9e\n\n# calcul du mod\u00e8le initial (=y_0) \u00e0 partir de l'\u00e9quation y = ax + b avec les param\u00e8tres a_0 et b_0\ny_0 = droite(x, a_0, b_0)\nL_0 = L(y, y_0, n, sigma)\nprint(a_0, b_0, L_0)","e540215b":"# cr\u00e9ation de la liste qui contiendra toutes les valeurs des param\u00e8tres test\u00e9s\nparam_opt = list()\n\n# nombre d'it\u00e9ration \nn_iter = 500000 #int(1e6)\n\n# on se d\u00e9place dans l'espace des param\u00e8tres en perturbant le mod\u00e8le initial ant\u00e9rieur \nfor i in range(n_iter):\n\n    # Param\u00e9trisation de la perturbation des param\u00e8tres a et b:\n    pas_perturbation = 0.1\n    delta_a = pas_perturbation * (a_sup + a_inf ) \/ 2\n    delta_b = pas_perturbation * (b_sup + b_inf ) \/ 2\n\n    # on cr\u00e9e un nouveau mod\u00e8le y_p avec les param\u00e8tres a_p et b_p\n    # ces param\u00e8tres sont obtenus par perturbation du mod\u00e8le initial ant\u00e9rieur d\u00e9fini par les param\u00e8tres a_0 et b_0\n    a_p = a_0 + np.random.uniform(-1, 1) * delta_a\n    b_p = b_0 + np.random.uniform(-1, 1) * delta_b\n    y_p = droite(x, a_p, b_p)\n\n    # on calcule la densit\u00e9 de probabilit\u00e9 du nouveau mod\u00e8le\n    L_p = L(y, y_p, n, sigma)\n    # on calcul la valeur du r\u00e9sidu du nouveau mod\u00e8le\n    res_p = res(y, y_p, n, sigma)\n    # on stocke la valeur des param\u00e8tres, la valeur de la densit\u00e9 de probabilit\u00e9 et la valeur du r\u00e9sidu\n    param_opt.append((a_p, b_p, L_p, res_p))\n\n    # on compare sa densit\u00e9 de probabilit\u00e9 \u00e0 celle du mod\u00e8le initial ant\u00e9rieur\n    # si L_p > L_0, le mod\u00e8le y_p devient le nouveau mod\u00e8le initial\n    # si L_p < L_0, le mod\u00e8le y_0 reste le mod\u00e8le initial\n    if L_p > L_0 or np.random.uniform() < L_p \/ L_0:\n        a_0 = a_p\n        b_0 = b_p\n        L_0 = L_p\n\n# on transforme la liste des param\u00e8tres test\u00e9s en matrice        \nparam_opt = np.asarray(param_opt)\n\n# d\u00e9finition des param\u00e8tres test\u00e9s et densit\u00e9 de probabilit\u00e9\/r\u00e9sidu associ\u00e9s\na_opt = param_opt[:,0]\nb_opt = param_opt[:,1]\nL_opt = param_opt[:,2]\nL_opt_max = param_opt[:,2].max()\nres_opt = param_opt[:,3]\nres_opt_max = param_opt[:,3].max()\n\n#on affiche la moyenne et l'\u00e9cart-type des param\u00e8tres test\u00e9s (=les meilleurs mod\u00e8les)\nprint(\"Valeur moyenne de a_opt:\", round(np.mean(a_opt),2))\nprint(\"Ecart-type de a_opt:\", round(np.std(a_opt),2))\nprint()\nprint(\"Valeur moyenne de b_opt:\", round(np.mean(b_opt),2))\nprint(\"Ecart-type de b_opt:\", round(np.std(b_opt),2))       ","fb7fd72f":"# distribution des valeurs des deux param\u00e8tres a_opt et b_opt obtenues au cours de l'inversion\nplt.figure(1)\nplt.hist(a_opt, 10, histtype='step', lw=3, label='a')\nplt.hist(b_opt, 10, histtype='step', lw=3, label='b')\nplt.xlabel('Valeurs des param\u00e8tres')\nplt.title('Distribution des param\u00e8tres test\u00e9s')\nplt.legend(loc='best')\nplt.show()\n\n# distribution de la valeurs du r\u00e9sidu des mod\u00e8les test\u00e9s\n# le r\u00e9sidu est pond\u00e9r\u00e9 par le r\u00e9sidu maximal obtenu dans l'inversion\nplt.figure(1)\nplt.hist(res_opt\/res_opt_max, 10, histtype='step', lw=3, label='r\u00e9sidu')\nplt.xlabel('Valeurs du r\u00e9sidu')\nplt.title('Distribution du r\u00e9sidu des mod\u00e8les test\u00e9s')\nplt.legend()\nplt.show()\n\n# distribution de la densit\u00e9 de probabilit\u00e9 dans l'espace des param\u00e8tres constitu\u00e9 par a et b\n# la densit\u00e9 de probabilit\u00e9 est pond\u00e9r\u00e9 par la densit\u00e9 de probabilit\u00e9 maximale obtenue dans l'inversion (= le meilleur mod\u00e8le)\nplt.figure(1)\nplt.scatter(a_opt, b_opt, s=L_opt\/L_opt_max, marker='^', c=L_opt\/L_opt_max)\nplt.xlabel('axe a')\nplt.ylabel('axe b')\nplt.title(\"Distribution de la densit\u00e9 de probabilit\u00e9 des mod\u00e8les test\u00e9s\")\nplt.colorbar(label='Densit\u00e9 de probabilit\u00e9')\nplt.show()","83d52fd5":"# d\u00e9finition de la fonction d'autocorr\u00e9lation\ndef acf(samples, lags):\n    corr = [1. if lag == 0 else np.corrcoef(samples[lag:], samples[:-lag])[0][1] for lag in range(lags)]\n    return np.array(corr)\n\n# fonction d'autocorr\u00e9lation des param\u00e8tres a et b\nplt.plot(acf(a_opt, lags=200), label='a') # modifier lags = pour trouver acf=0\nplt.plot(acf(b_opt, lags=200), label='b') # modifier lags = pour trouver acf=0\nplt.xlabel(\"Lag\")\nplt.ylabel('ACF')\nplt.title(\"ACF des param\u00e8tres a et b\")\nplt.grid()\nplt.legend()\nplt.show()\n# valeur du lag pour acf = 0 - utilis\u00e9e pour r\u00e9-\u00e9chantillonner\nlag=150 # modifier la valeur du lag \n\n# \u00e9volution de la valeur des param\u00e8tres a et b ind\u00e9pendants pendant l'inversion\nplt.figure(1)\nplt.plot(a_opt[::lag], 'r-', markersize=10) \nplt.plot(b_opt[::lag], 'b-', markersize=10) \nplt.xlabel(\"Nombre de valeurs ind\u00e9pendantes\")\nplt.ylabel('Valeurs des param\u00e8tres')\nplt.title(\"Valeurs des param\u00e8tres a et b ind\u00e9pendants durant l'inversion\")\nplt.show()","02c28fe8":"# distribution des valeurs des deux param\u00e8tres a_opt et b_opt ind\u00e9pendants obtenues au cours de l'inversion\n# r\u00e9-\u00e9chantillonnage de a_opt et b_opt avec la valeur du lag\nplt.figure(1)\nplt.hist(a_opt[::lag], 10, histtype='step', lw=3, label='a')\nplt.hist(b_opt[::lag], 10, histtype='step', lw=3, label='b')\nplt.xlabel('Valeurs des param\u00e8tres')\nplt.title('Distribution des param\u00e8tres test\u00e9s et ind\u00e9pendants')\nplt.legend(loc='best')\nplt.show()\n\n# distribution de la densit\u00e9 de probabilit\u00e9 dans l'espace des param\u00e8tres ind\u00e9pendants constitu\u00e9 par a et b\n# la densit\u00e9 de probabilit\u00e9 est pond\u00e9r\u00e9 par la densit\u00e9 de probabilit\u00e9 maximale obtenue dans l'inversion (= le meilleur mod\u00e8le)\n# r\u00e9-\u00e9chantillonnage de a_opt et b_opt avec la valeur du lag\nplt.figure(1)\nplt.scatter(a_opt[::lag], b_opt[::lag], s=L_opt[::lag]\/L_opt_max, marker='^', c=L_opt[::lag]\/L_opt_max)\nplt.xlabel('axe a')\nplt.ylabel('axe b')\nplt.title(\"Distribution de la densit\u00e9 de probabilit\u00e9 des mod\u00e8les test\u00e9s et ind\u00e9pendants\")\nplt.colorbar(label='Densit\u00e9 de probabilit\u00e9')\nplt.show()\n\n#on affiche la moyenne et l'\u00e9cart-type des param\u00e8tres test\u00e9s (=les meilleurs mod\u00e8les) ind\u00e9pendants\n# r\u00e9-\u00e9chantillonnage de a_opt et b_opt avec la valeur du lag\nprint(\"Valeur moyenne de a_opt:\", round(np.mean(a_opt[::lag]),2))\nprint(\"Ecart-type de a_opt:\", round(np.std(a_opt[::lag]),2))\nprint()\nprint(\"Valeur moyenne de b_opt:\", round(np.mean(b_opt[::lag]),2))\nprint(\"Ecart-type de b_opt:\", round(np.std(b_opt[::lag]),2))   ","04f2fc65":"- R\u00e9sultats finaux \u00e0 interpr\u00e9ter","3ab77ebd":"- Tra\u00e7age et analyse des r\u00e9sultats\n\n>* **Distribution des mod\u00e8les.** On trace la distribution des valeurs des param\u00e8tres issues de l'inversion ainsi que la densit\u00e9 de probablit\u00e9 associ\u00e9e afin de d\u00e9terminer les param\u00e8tres recherch\u00e9s a et b.\n\n>* **Echantillonnage de l'espace des param\u00e8tres.** On peut appr\u00e9cier la capacit\u00e9 de l'algorithme \u00e0 explorer efficacement l'espace des param\u00e8tres. La distribution des valeurs des param\u00e8tres montre que cet \u00e9chantillonnage al\u00e9atoire guid\u00e9 passe la plupart du temps dans la r\u00e9gion de l'espace des param\u00e8tres de plus haute probabilit\u00e9.","a0550d77":"- Cr\u00e9ation des donn\u00e9es synth\u00e9tiques : valeur exacte suivant une droite + bruit","5bdaf382":"**Etape 3** : **Perturbation al\u00e9atoire**.\n-  On perturbe al\u00e9atoirement les param\u00e8tres $\\bf{a_0}$ et $\\bf{b_0}$ afin de g\u00e9n\u00e9rer deux nouveaux param\u00e8tres $\\bf{a_p}$ et $\\bf{b_p}$ et donc un nouveau mod\u00e8le $\\bf{y_p}$ \u00e0 tester.\n\n- Une fois le nouveau mod\u00e8le d\u00e9fini, on calcule sa **probabilit\u00e9 d'existence** (ou densit\u00e9 de probabilit\u00e9).\n\n**Etape 4** : **validation ou rejet du nouveau mod\u00e8le**\n\n- On compare la probabilit\u00e9 d'existence des mod\u00e8les $\\bf{y_0}$ et $\\bf{y_p}$. Si le mod\u00e8le $\\bf{y_p}$ est le plus probable, les param\u00e8tres $\\bf{a_p}$ et $\\bf{b_p}$ deviennent les nouveaux param\u00e8tres initiaux $\\bf{a_0}$ et $\\bf{b_0}$. Si le mod\u00e8le $\\bf{y_0}$ reste le plus probable, on le conserve comme mod\u00e8le initial. \n\n- On r\u00e9-it\u00e8re les \u00e9tapes 3 et 4 jusqu'\u00e0 atteindre le nombre d'it\u00e9ration d\u00e9fini au d\u00e9but du code. Les param\u00e8tres test\u00e9s sont au fur et \u00e0 mesure stock\u00e9s. On obtient ainsi une liste des param\u00e8tres les plus probables \u00e0 partir de laquelle nous pouvons calculer une valeur moyenne et un \u00e9cart-type. ","6ad6775c":"- D\u00e9claration des param\u00e8tres ","9ea39ba8":"- G\u00e9n\u00e9ration de nombre pseudo-al\u00e9atoire","91838e02":"> * **Convergence de l'\u00e9chantillonage MCMC.** Nous avons besoin de v\u00e9rifier que l'espace des param\u00e8tres a \u00e9t\u00e9 correctement explor\u00e9. On consid\u00e8re que la chaine de Markov a converg\u00e9 quand elle a \u00e9chantillonn\u00e9 tous les mod\u00e8les importants (i.e. dont la densit\u00e9 de probabilit\u00e9 est forte). \n\n> **1. Nombre d'it\u00e9rations et pas de la perturbation.** Le nombre d'it\u00e9rations n\u00e9cessaires pour atteindre la **convergence** d\u00e9pend fortement de la mani\u00e8re dont nous avons impl\u00e9ment\u00e9 la \"marche al\u00e9atoire guid\u00e9\". En particulier, la **taille de la perturbation** pour cr\u00e9er de nouveaux mod\u00e8les doit \u00eatre choisie avec pr\u00e9caution. La perturbation doit \u00eatre assez petite pour ne pas modifier de mani\u00e8re drastique le mod\u00e8le suivant afin de ne pas rater des mod\u00e8les int\u00e9ressants mais assez important pour \u00e9viter de bloquer l'\u00e9chantillonnage dans une zone de minimum local (en terme de r\u00e9sidu) pendant un long moment. Le choix du pas de la perturbation est d\u00e9fini apr\u00e8s une suite d'essais.\n\n> **2. Autocorr\u00e9lation et mod\u00e8les ind\u00e9pendants.** Comme le pas de la perturbation est de petite amplitude, les mod\u00e8les successives dans une chaine de Markov sont hautement corr\u00e9l\u00e9s. Pour collecter des mod\u00e8les statistiquement d\u00e9pendants, nous devons calculer pour chaque param\u00e8tre, la fonction d'autocorr\u00e9lation, qui correspond \u00e0 la corr\u00e9lation crois\u00e9e d'une chaine de Markov avec elle-m\u00eame, mais d\u00e9cal\u00e9e d'une certaine quantit\u00e9 appel\u00e9e \"lag\". La fonction d'autocorr\u00e9lation d\u00e9croit avec une augmentation du lag jusqu'\u00e0 devenir nulle. La valeur du lag pour laquelle la fonction d'autocorr\u00e9lation est nulle est la valeur du pas entre deux mod\u00e8les ind\u00e9pendants. Ainsi on r\u00e9-\u00e9chantillonne tous les \"lag\" mod\u00e8les pour obtenir une collection de mod\u00e8les ind\u00e9pendants (ie. non corr\u00e9l\u00e9s). Le nombre d'it\u00e9rations total va donc \u00e9galement d\u00e9pendre du lag. Nous devons it\u00e9rer suffisamment pour obtenir un nombre de mod\u00e8les statistiquement repr\u00e9sentatif.\n\n> **3. Second run \u00e0 partir d'une nouvelle \"seed\" de nombres al\u00e9atoires.** Enfin, nous lan\u00e7ons le code une deuxi\u00e8me fois en utilisant des nombres al\u00e9atoires diff\u00e9rents et donc en testant des mod\u00e8les diff\u00e9rents. Cette \u00e9tape permet de v\u00e9rifier si nous obtenons bien les m\u00eames r\u00e9sultats et distributions de param\u00e8tres attestant que 1) notre inversion a bien explor\u00e9 tout l'espace des param\u00e8tres et 2) validant ainsi nos choix sur le nombre d'it\u00e9rations, le pas des perturbations et la valeur du lag.","6f666c2b":"# Introduction aux probl\u00e8mes inverses\n\n# 2. La m\u00e9thode Monte Carlo Markov Chain\n\n# 2.1. Un peu de th\u00e9orie","21752ac5":"Nous allons \u00e0 nouveau consid\u00e9rer un jeu de donn\u00e9es $(x_k, y_k)$ d'erreur $\\sigma_{k}$, coh\u00e9rent et distribu\u00e9 lin\u00e9airement pouvant \u00eatre ajust\u00e9 par une droite. Pour d\u00e9terminer cette droite, nous allons cette fois-ci utiliser une **m\u00e9thode stochastique** ou **m\u00e9thode bay\u00e9sienne** ou encore **de Monte-Carlo**. Il s'agit toujours  de d\u00e9terminer les param\u00e8tres <strong>a<\/strong> et <strong>b<\/strong> d\u00e9finissant la droite <strong>y = ax + b<\/strong> passant au plus pr\u00e8s de l'ensemble des points $(x_k, y_k)$ en minimisant la quantit\u00e9 suivante :\n\n$$\n\\begin{align}\nS(a,b) &= \\sum_{k=1}^{n}\\left(\\frac{y_k-(ax_k+b)}{\\sigma_{k}}\\right)^2 \\\\\nS(a,b) &= \\sum_{k=1}^{n}\\left(\\frac{y_k-f(x_k,a,b)}{\\sigma_{k}}\\right)^2 \\\\\nS(m) &= \\sum_{k=1}^{n}\\left(\\frac{y_k-f(x_k,m)}{\\sigma_{k}}\\right)^2\n\\end{align}\n$$\n\nLa **m\u00e9thode des moindres carr\u00e9s** fournit **un seul mod\u00e8le** issu d'un **calcul complexe** faisant intervenir l'**inversion de la matrice loi physique G**. Le mod\u00e8le n'est toutefois pas unique car il d\u00e9pend fortement des choix r\u00e9alis\u00e9s durant l'inversion (information \u00e0 priori, pond\u00e9ration par l'erreur, etc.).\n\nLes m\u00e9thodes **de Monte-Carlo** se basent sur le **calcul direct** et non inverse du mod\u00e8le dans un **espace de param\u00e8tres donn\u00e9**. Dans notre exemple, les m\u00e9thodes de Monte-Carlo calculent directement la droite <strong>y = ax + b<\/strong> (le mod\u00e8le) avec une valeur de **a** et de **b** choisie **al\u00e9atoirement** dans une gamme pr\u00e9d\u00e9finie. Puis, on calcule pour cette droite (ou mod\u00e8le) une valeur de $S(m)$. Cette proc\u00e9dure est r\u00e9alis\u00e9e un certain nombre de fois afin de tester un maximum de mod\u00e8le dont l'indicateur de *qualit\u00e9* est contenu dans la valeur $S(m)$. Plus cette valeur est faible, meilleur est le mod\u00e8le. Ainsi les m\u00e9thodes de Monte-Carlo testent **un ensemble de mod\u00e8les** \u00e0 partir duquel on peut d\u00e9terminer un **sous-ensemble de meilleurs mod\u00e8les** dont la **probabilit\u00e9 d'existence** est la plus forte.\n\nLa **probabilit\u00e9 d'existence** d'un mod\u00e8le est d\u00e9finie par la fonction de **densit\u00e9 de probabilit\u00e9** $L(m)$:\n\n$$\n\\begin{align}\nL(m) &= \\exp{\\left(-\\frac{1}{n}S(m)\\right)}\\\\\nL(m) &= \\exp{\\left(-\\frac{1}{n}\\sum_{k=1}^{n}\\left(\\frac{y_k-f(x_k,m)}{\\sigma_{k}}\\right)^2\\right)}\n\\end{align}\n$$\n\navec n, le nombre de donn\u00e9es. **La densit\u00e9 de probabilit\u00e9 $L(m)$ est directement reli\u00e9e \u00e0 $S(m)$ la somme des carr\u00e9s des r\u00e9sidus entre observations et mod\u00e8le**. Plus la quantit\u00e9 $S(m)$ est petite et plus la quantit\u00e9 $L(m)$ associ\u00e9e est importante et le mod\u00e8le probable.\n\nLes m\u00e9thodes de **Monte-Carlo** diff\u00e8rent entre elles par la mani\u00e8re dont **l'espace des param\u00e8tres est parcouru**. En effet, l'espace des param\u00e8tres peut \u00eatre parcouru de mani\u00e8re syst\u00e9matique, ce qui sera co\u00fbteux en temps de calcul et impossible \u00e0 r\u00e9aliser si le nombre de param\u00e8tres \u00e0 d\u00e9terminer est important. Une alternative est d'explorer cet espace en suivant un algorithme permettant de **l'\u00e9chantillonner efficacement en s'orientant rapidement vers les meilleurs mod\u00e8les**. Les **cha\u00eenes de Markov (ou Markov Chain)** constituent une classe d'algorithmes de ce type. \n\nIci nous allons utiliser la **m\u00e9thode MCMC (Monte Carlo Markov chain)** bas\u00e9e sur l'algorithme de **Metropolis-Hastings**. Les \u00e9tapes d'impl\u00e9mentation de cette m\u00e9thode sont les suivantes :\n\n> 1 - **D\u00e9termination de l'espace des param\u00e8tres**. On d\u00e9finit les limites sup\u00e9rieure (U) et inf\u00e9rieure (L) pour la valeur des param\u00e8tres **a** et **b** \u00e0 partir d'informations *a priori* : \n\n$$\na^{L}~\\leq~a~\\leq~a^{U}\\\\\nb^{L}~\\leq~b~\\leq~b^{U}\n$$\n\n> 2 - **Initialisation de l'inversion**. On tire au sort les param\u00e8tres $\\bf{a_0}$ et $\\bf{b_0}$ du mod\u00e8le initial $\\bf{y_0}$. Le tirage al\u00e9atoire est r\u00e9alis\u00e9 dans la gamme de valeurs d\u00e9finie par les limites inf\u00e9rieure et sup\u00e9rieure de chacun des param\u00e8tres. Une fois le mod\u00e8le initial d\u00e9fini, on calcule sa **probabilit\u00e9 d'existence** (ou densit\u00e9 de probabilit\u00e9) $\\bf{L_0}$.\n\n> 3 - **Perturbation al\u00e9atoire**. On perturbe al\u00e9atoirement les param\u00e8tres $\\bf{a_0}$ et $\\bf{b_0}$ afin de g\u00e9n\u00e9rer deux nouveaux param\u00e8tres $\\bf{a_p}$ et $\\bf{b_p}$ et donc un nouveau mod\u00e8le $\\bf{y_p}$ \u00e0 tester. Une fois le nouveau mod\u00e8le d\u00e9fini, on calcule sa **probabilit\u00e9 d'existence** (ou densit\u00e9 de probabilit\u00e9) $\\bf{L_p}$.\n\n> 4 - **Validation ou rejet du nouveau mod\u00e8le**. On compare la probabilit\u00e9 d'existence des mod\u00e8les $\\bf{y_0}$ et $\\bf{y_p}$. Si le mod\u00e8le $\\bf{y_p}$ est le plus probable ($\\bf{L_p}>\\bf{L_0}$), les param\u00e8tres $\\bf{a_p}$ et $\\bf{b_p}$ deviennent les nouveaux param\u00e8tres initiaux $\\bf{a_0}$ et $\\bf{b_0}$. Si le mod\u00e8le $\\bf{y_0}$ reste le plus probable ($\\bf{L_p}<\\bf{L_0}$), on le conserve comme mod\u00e8le initial. \n\n> On r\u00e9-it\u00e8re les \u00e9tapes 3 et 4 jusqu'\u00e0 atteindre le nombre d'it\u00e9rations d\u00e9fini au d\u00e9but du code. Les param\u00e8tres test\u00e9s par cette *marche al\u00e9atoire guid\u00e9e* sont au fur et \u00e0 mesure stock\u00e9s. On obtient ainsi une liste des param\u00e8tres les plus probables \u00e0 partir de laquelle nous pouvons calculer une **valeur moyenne** et un **\u00e9cart-type**. \n\n![figure_perturbation.png](attachment:figure_perturbation.png)\n\n# 2.2. Impl\u00e9mentation num\u00e9rique","45d8dbdb":"- D\u00e9finition des fonctions n\u00e9cessaires : droite, fonction densit\u00e9 de probabilit\u00e9 et r\u00e9sidu","382a26f8":"- R\u00e9solution par la m\u00e9thode Monte Carlo Markov Chain (MCMC) avec l'algorithme de Metropolis-Hastings\n\n**Etape 1** : **D\u00e9termination de l'espace des param\u00e8tres**.\n\nOn d\u00e9finit les limites sup\u00e9rieure et inf\u00e9rieure pour la valeur des param\u00e8tres **a** et **b**. On utilise la distribution des donn\u00e9es comme information *a priori*.","2c01a207":"**Etape 2** : **Initialisation de l'inversion**\n\n- On tire au sort les param\u00e8tres $\\bf{a_0}$ et $\\bf{b_0}$ du mod\u00e8le initial $\\bf{y_0}$.\nLe tirage al\u00e9atoire est r\u00e9alis\u00e9 dans la gamme de valeurs d\u00e9finie par les limites inf\u00e9rieure et sup\u00e9rieure de chacun des param\u00e8tres.\n\n- Une fois le mod\u00e8le initial d\u00e9fini, on calcule sa **probabilit\u00e9 d'existence** (ou densit\u00e9 de probabilit\u00e9).","a21e7a14":"- Importation des librairies "}}