{"cell_type":{"bfafe323":"code","de34e287":"code","326fedc8":"code","a8f7e1e2":"code","af837e16":"code","7226f13e":"code","59c6c09c":"code","e0288403":"code","a689a9a8":"code","8df37072":"code","9650ae37":"code","cf4da0f5":"code","93a9be1e":"code","9669d41c":"code","e747765f":"code","228431b1":"code","d6a1fe89":"code","d93882db":"code","b97a65b0":"code","f6b52181":"code","ced8b5dc":"code","16f337de":"code","89c72357":"code","b09e146c":"code","fbf3744a":"code","302f8b4c":"code","aec691e2":"code","56b6f906":"code","b3e833c6":"code","38db3e8a":"code","6e4c09e5":"markdown","9012ba8a":"markdown","b8158c94":"markdown","8b7dafe3":"markdown","67863598":"markdown","998b8d81":"markdown","bf39d038":"markdown","912853a7":"markdown","31a0267e":"markdown"},"source":{"bfafe323":"!pip install mtcnn","de34e287":"from mtcnn import MTCNN\nfrom cv2 import imread\nfrom PIL import Image\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport pickle\n\nimport warnings\nwarnings.filterwarnings('ignore')","326fedc8":"# load image from file using PIL\nfilename = '..\/input\/5-celebrity-faces-dataset\/train\/jerry_seinfeld\/httpmediapopsugarassetscomfilesusersxlargejpg.jpg'\nimage = Image.open(filename)\n# convert to RGB, if needed\nimage = image.convert('RGB')\n# convert to array\npixels = np.asarray(image)\nplt.imshow(pixels)\nprint(pixels.shape)","a8f7e1e2":"# create the face detector, using default weights\ndetector = MTCNN()\n# detect faces in the image\nresults = detector.detect_faces(pixels)","af837e16":"## Assuming that only 1 face is present\n# extract the bounding box from the first face\nx1, y1, width, height = results[0]['box']\n\n# bug fix - Sometimes the library will return a negative pixel index\nx1, y1 = abs(x1), abs(y1)\nx2, y2 = x1 + width, y1 + height","7226f13e":"# extract the face. this is like cropping the face from image\nface = pixels[y1:y2, x1:x2]","59c6c09c":"# resize pixels to the model size\nimage = Image.fromarray(face)\nimage = image.resize((160, 160))\nface_array = np.asarray(image)\n# Lets show the extracted face\nplt.imshow(face_array)","e0288403":"# Function to extract a single face from a given photograph\ndef extract_face(filename=None, image_pixels=None, required_size=(160, 160)):\n    if filename is not None:\n        image = Image.open(filename)\n        image = image.convert('RGB')\n        pixels = np.asarray(image)\n    elif image_pixels is not None:\n        pixels = image_pixels\n    detector = MTCNN()\n    results = detector.detect_faces(pixels)\n    x1, y1, width, height = results[0]['box']\n    x1, y1 = abs(x1), abs(y1)\n    x2, y2 = x1 + width, y1 + height\n    face = pixels[y1:y2, x1:x2]\n    box_dimensions = (x1, y1, width, height)\n    image = Image.fromarray(face)\n    image = image.resize(required_size)\n    face_array = np.asarray(image)\n    return face_array, box_dimensions","a689a9a8":"## Lets extract faces of elton john for example\nimage_path = '..\/input\/5-celebrity-faces-dataset\/train\/elton_john'\nfiles = os.listdir(image_path)\ni=1\nfor filename in files:\n    full_path = os.path.join(image_path, filename)\n    face, _ = extract_face(filename=full_path)\n    plt.subplot(2, 7, i)\n    plt.axis('off')\n    plt.imshow(face)\n    i+=1\n    if i>14:\n        break","8df37072":"# load images and extract faces for all images in a directory\ndef load_faces(directory):\n    faces = []\n    # enumerate files\n    for filename in os.listdir(directory):\n        # path\n        path = os.path.join(directory,filename)\n        # get face\n        face, _ = extract_face(filename=path)\n        # store\n        faces.append(face)\n    return faces","9650ae37":"# load a dataset that contains one subdir for each class that in turn contains images\ndef load_dataset(directory):\n    X, y = [], []\n    for subdir in os.listdir(directory):\n        path = os.path.join(directory, subdir)\n        if not os.path.isdir(path):\n            continue\n        faces = load_faces(path)\n        labels = [subdir for _ in range(len(faces))]\n        print(f'Loaded {len(faces)} examples for class: {subdir}')\n        X.extend(faces)\n        y.extend(labels)\n    return np.asarray(X), np.asarray(y)","cf4da0f5":"# load train dataset\n#..\/input\/5-celebrity-faces-dataset\/train\/ben_afflek\/httpcsvkmeuaeccjpg.jpg\nX_train, y_train = load_dataset(r'..\/input\/5-celebrity-faces-dataset\/train')\nprint(X_train.shape, y_train.shape)\n\n# load validation dataset\nX_val, y_val = load_dataset(r'..\/input\/5-celebrity-faces-dataset\/val')\nprint(X_val.shape, y_val.shape)","93a9be1e":"# Download the Pre-Trained Keras FaceNet Model\n!wget --no-check-certificate 'https:\/\/drive.google.com\/uc?export=download&id=1PZ_6Zsy1Vb0s0JmjEmVd8FS99zoMCiN1' -O 'facenet_keras.h5'","9669d41c":"# example of loading the keras facenet model\nfrom tensorflow.keras.models import load_model\n# load the model\nmodel = load_model('facenet_keras.h5')","e747765f":"# Function to get the face embedding for one face\ndef get_embedding(model, face_pixels):\n    # scale pixel values\n    face_pixels = face_pixels.astype('float32')\n    # standardize pixel values across channels (global)\n    mean, std = face_pixels.mean(), face_pixels.std()\n    face_pixels = (face_pixels - mean) \/ std\n    # transform face into one sample\n    samples = np.expand_dims(face_pixels, axis=0)\n    # make prediction to get embedding\n    yhat = model.predict(samples)\n    return yhat[0]","228431b1":"# convert each face in the test set to an embedding\nnewTrainX = []\nfor face_pixels in X_train:\n    embedding = get_embedding(model, face_pixels)\n    newTrainX.append(embedding)\nnewTrainX = np.asarray(newTrainX)\nprint(newTrainX.shape)","d6a1fe89":"# convert each face in the test set to an embedding\nnewTestX = []\nfor face_pixels in X_val:\n    embedding = get_embedding(model, face_pixels)\n    newTestX.append(embedding)\nnewTestX = np.asarray(newTestX)\nprint(newTestX.shape)","d93882db":"from sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.svm import SVC","b97a65b0":"# Normalize input vectors - We need to normalize the face embeddings\nin_encoder = Normalizer(norm='l2')\nX_train = in_encoder.transform(newTrainX)\nX_val = in_encoder.transform(newTestX)","f6b52181":"# Label encode targets\nout_encoder = LabelEncoder()\nout_encoder.fit(y_train)\ny_train = out_encoder.transform(y_train)\ny_val = out_encoder.transform(y_val)\n#pickle.dump(model, open('LabelEncoder.sav', 'wb'))","ced8b5dc":"# fit model\nsvc_model = SVC(kernel='linear', probability=True)\nsvc_model.fit(X_train, y_train)\n#pickle.dump(model, open('SVC.sav', 'wb'))","16f337de":"# predict\nyhat_train = svc_model.predict(X_train)\nyhat_test = svc_model.predict(X_val)","89c72357":"X_train.shape","b09e146c":"score_train = accuracy_score(y_train, yhat_train)\nscore_test = accuracy_score(y_val, yhat_test)\nprint('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))","fbf3744a":"## Load the trained LabelEncoder and SVM model\nloaded_encoder = out_encoder\nloaded_predictor = svc_model\nloaded_facenet = load_model('facenet_keras.h5')","302f8b4c":"loaded_encoder.classes_","aec691e2":"# Single Function capable to process input image and return the bounding box dimensions and the prediction\ndef face_recognize(image):\n    ## Extract the face and bounding box dimensions from the image by using pretrained MTCNN model\n    faces, box_dimensions = extract_face(image_pixels=image)\n    X = np.asarray(faces)\n    ## Get the Face Embeddings for the extracted face pixels and store as numpy array\n    embedding = get_embedding(loaded_facenet, X)\n    X = []\n    X.append(embedding)\n    X = np.asarray(X)\n    ## Predict label for the face by using the pretrained models\n    prediction = loaded_predictor.predict(X)\n    predicted_label = loaded_encoder.inverse_transform([prediction])\n    return predicted_label[0], box_dimensions","56b6f906":"## Testing the model with the random image\nfilename = '..\/input\/5-celebrity-faces-dataset\/val\/madonna\/httpcdncdnjustjaredcomwpcontentuploadsheadlinesmadonnatalksparisattackstearsjpg.jpg'\nimage = Image.open(filename)\nimage = image.convert('RGB')\npixels = np.asarray(image)\nface_recognize(pixels)","b3e833c6":"# Downloading the gif for model processing\n!wget --no-check-certificate 'https:\/\/c.tenor.com\/zHK0-3w4MgYAAAAC\/ben-affleck-basketball.gif' -O 'ben_afflek.gif'","38db3e8a":"## Utilizing opencv to read the gif and imageio to write the updated gif with prediction and bounding box over the detected face\n\nimport cv2\nimport imageio\ncapture = cv2.VideoCapture('ben_afflek.gif')\nimage_lst = []\n\nwhile True:\n    IsTrue, frames = capture.read()\n    if frames is None:\n        break\n    frames = cv2.cvtColor(frames, cv2.COLOR_BGR2RGB)\n    person_name, box_dimensions = face_recognize(frames)\n    x,y,w,h = box_dimensions\n    cv2.rectangle(frames,(x,y),(x+w,y+h),(0,255,0),2)\n    cv2.putText(frames,person_name,(x+w+10,y+h),0,0.3,(0,255,0))\n    image_lst.append(frames)\n\ncapture.release()\n\n# Convert to gif using the imageio.mimsave method\nimageio.mimsave('ben_afflek_processed.gif', image_lst, fps=10)","6e4c09e5":"## Model Deployment","9012ba8a":"## Face Detection using MTCNN\n### MTCNN\nThe MTCNN (Multi-Task Cascaded Convolutional Neural Network) is popular because it achieved then state-of-the-art results on a range of benchmark datasets, and because it is capable of also recognizing other facial features such as eyes and mouth, called landmark detection.\n\nThe network uses a cascade structure with three networks; first the image is rescaled to a range of different sizes (called an image pyramid), then the first model (Proposal Network or P-Net) proposes candidate facial regions, the second model (Refine Network or R-Net) filters the bounding boxes, and the third model (Output Network or O-Net) proposes facial landmarks.\n\nThe three models are not connected directly; instead, outputs of the previous stage are fed as input to the next stage. This allows additional processing to be performed between stages","b8158c94":"#### References :-\nhttps:\/\/machinelearningmastery.com\/how-to-develop-a-face-recognition-system-using-facenet-in-keras-and-an-svm-classifier\/\n\nhttps:\/\/machinelearningmastery.com\/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras\/\n\nhttps:\/\/theailearner.com\/2021\/05\/29\/creating-gif-from-video-using-opencv-and-imageio\/","8b7dafe3":"# Face Recognization\nThis notebook is created after I was routed to the dataset from the https:\/\/machinelearningmastery.com\/ blog. \n\nThe steps in the notebook are referred from below blog pages https:\/\/machinelearningmastery.com\/how-to-develop-a-face-recognition-system-using-facenet-in-keras-and-an-svm-classifier\/\n\nhttps:\/\/machinelearningmastery.com\/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras\/","67863598":"## Face Classification Steps\n1. Vector Normalization\n2. Label Encode the Targets\n3. Use SVM model to train and predict","998b8d81":"![](https:\/\/c.tenor.com\/zHK0-3w4MgYAAAAC\/ben-affleck-basketball.gif)","bf39d038":"## Face Recognization in Video\nLet us try to check how the model performs with videos. We will be using a sample gif with ben affleck and a basketball.","912853a7":"## Face Embeddings\n### FaceNet\nFaceNet is a face recognition system developed in 2015 by researchers at Google that achieved then state-of-the-art results on a range of face recognition benchmark datasets. The FaceNet system can be used to extract high-quality features from faces, called face embeddings, that can then be used to train a face identification system.\n\nThe model is a deep convolutional neural network trained via a triplet loss function that encourages vectors for the same identity to become more similar (smaller distance), whereas vectors for different identities are expected to become less similar (larger distance). The focus on training a model to create embeddings directly (rather than extracting them from an intermediate layer of a model) was an important innovation in this work.","31a0267e":"![](.\/ben_afflek_processed.gif)"}}