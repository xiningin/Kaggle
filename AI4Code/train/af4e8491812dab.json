{"cell_type":{"78a98e58":"code","fa6e5d5b":"code","b3c3776d":"code","48ce0e2d":"code","d5b83ef7":"markdown","984e10e1":"markdown","9c10b2a2":"markdown","ab85f72a":"markdown"},"source":{"78a98e58":"import numpy as np            \nimport pandas as pd           \nimport sklearn.ensemble       \nimport sklearn.metrics        \nimport sklearn.model_selection","fa6e5d5b":"def prepare_data(df):\n    # Identify a label with less data.\n    label_minor = df['satisfied'].value_counts().idxmin()\n    # Forming of satisfied and unsatisfied datasets based on the amount of data\n    # in each category.\n    minor_df = df[df['satisfied'] == label_minor]\n    major_df = df[df['satisfied'] != label_minor]\n    # Calculating number of frames.\n    parts_num = len(major_df.index)\/\/len(minor_df.index)\n    # Forming frames.\n    frames = [\n        major_df.iloc[i*len(minor_df.index):(i+1)*len(minor_df.index)]\n        for i in range(parts_num+1)\n    ]\n    \n    result = []\n\n    for frame in frames:\n        minor_part = minor_df\n        # Since the last frame is most often smaller than the default size, \n        # we need to trim minor_part to actual size.\n        if len(frame.index) < len(minor_df.index):\n            minor_part = minor_part.iloc[:len(frame.index)]\n        frame_data = pd.concat([frame, minor_part], ignore_index=False)\n\n        frame_labels = frame_data['satisfied']\n        frame_data.drop(\n            ['jobfield', 'jobtitle', 'psychotype', 'satisfied'],\n            axis=1,\n            inplace=True\n        )\n\n        # Splitting data to train and validation part.\n        x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(\n            frame_data,\n            frame_labels,\n            stratify=frame_labels,\n            test_size=.2,\n        )\n\n        # Data standardization.\n        mean = x_train.mean(axis=0)\n        x_train -= mean\n        std = x_train.std(axis=0)\n        x_train \/= std\n        x_val -= mean\n        x_val \/= std\n\n        result.append(\n            {\n                'x_train': x_train,\n                'x_val': x_val,\n                'y_train': y_train,\n                'y_val': y_val\n            }\n        )\n\n    return result","b3c3776d":"def fit_eval_model(source_df, filter_column, filter_value, iters_num=25):\n    print(f'=== FILTER: {filter_column}\/{filter_value} ===')\n    df = source_df.copy(deep=True)\n    # Trimming data to selected job type and job field.\n    df = df.loc[df[filter_column] == filter_value]\n    # Label encoding.\n    df[\"psychotype_cat\"] = df[\"psychotype\"].astype('category').cat.codes\n    lbl_vc = df[\"satisfied\"].value_counts(sort=False)\n    print(f'> Data size: {len(df.index)}, Satisfied {lbl_vc[1]}\/{lbl_vc[0]}')\n\n    avg_score_val = 0\n    ensembles = []\n    df, x_test, _, y_test = sklearn.model_selection.train_test_split(\n        df,\n        df['satisfied'],\n        stratify=df['satisfied'],\n        test_size=.15,\n    )\n    x_test.drop(\n        ['jobfield', 'jobtitle', 'psychotype', 'satisfied'],\n        axis=1,\n        inplace=True\n    )\n    mean = x_test.mean(axis=0)\n    x_test -= mean\n    std = x_test.std(axis=0)\n    x_test \/= std\n\n    print('Iters: ', end='', flush=True)\n    for i in range(iters_num):\n        print(f'{i+1}..', end='', flush=True)\n        ensembles.append([])\n\n        # Data preparation.\n        data_frames = prepare_data(df)\n\n        # Ensemble forming.\n        for data in data_frames:\n            model = sklearn.ensemble.GradientBoostingClassifier(\n                learning_rate=.1,\n                loss='deviance',\n                max_depth=3,\n                n_estimators=1000,\n                random_state=None\n            )\n            model.fit(data['x_train'], data['y_train'])\n        \n            ensembles[-1].append({\n                'score_val': model.score(data['x_val'], data['y_val']),\n                'model': model\n            })\n\n        tmp = sum(model['score_val'] for model in ensembles[-1])\n        avg_score_val += tmp\/len(data_frames)\n    print('done.')\n\n    # Calculating and showing some statistical info.\n    tmp = avg_score_val\/iters_num\n    print(f'\\n> Average single model validation score:\\t{tmp:.3f}')\n    print(f'> Models in each ensemble:\\t\\t\\t{len(ensembles[0])}')\n    ensemble_score = 0\n    for _, ensemble in enumerate(ensembles):\n        for i, model in enumerate(ensemble):\n            ensemble_score += model['model'].score(x_test, y_test)\/len(ensemble)\n    tmp = ensemble_score\/len(ensembles)\n    print(f'> Average ensemble test data score:\\t\\t{tmp:.3f}\\n')","48ce0e2d":"df = pd.read_csv(\n    '\/kaggle\/input\/kpmi-mbti-mod-test\/kpmi_data.csv',\n    low_memory=False\n)\n\n# Build models for...\nfit_eval_model(df, 'jobfield', 'Staff and training')\nfit_eval_model(df, 'jobfield', 'Department head')","d5b83ef7":"Main function:","984e10e1":"To ensure the reproducibility of the results and minimize the impact of random_state, the training and validation operation is performed 100 times by default without random_state initialization.\n\nAt each iteration, an ensemble of models is created in the number equal to the number of frames.","9c10b2a2":"Taking into account the unbalanced data, I decided to break them down into equal numbers of satisfied and dissatisfied respondents and train a separate model for each balanced frame.\n\nData preparation function:","ab85f72a":"Job satisfaction prediction with the scikit-learn GradientBoostingClassifier ensemble.\n\nImports:"}}