{"cell_type":{"e1ec3101":"code","712b2395":"code","dbdae819":"code","f99c68e8":"code","8cc96567":"code","91beeb43":"code","fd32b917":"code","e2b5d0dc":"code","c1b2db84":"code","d0a54cd1":"code","6f7ec545":"code","8c1d5af4":"code","ba4e4ba4":"code","c5a1682c":"code","5444151a":"code","abd1d709":"code","b8c20c87":"code","e468f3cf":"code","d2b6dd4e":"code","dae6b07c":"code","2b658a63":"code","4ad9ba5f":"markdown","391dd586":"markdown","b01aa5e7":"markdown","f74196ab":"markdown","cfb8cfec":"markdown","a959bd11":"markdown","d6dd9811":"markdown","6d92c358":"markdown","87c47468":"markdown","35d4fd45":"markdown","995cb12e":"markdown","cfb766e0":"markdown","0ebdb3a8":"markdown","fe21613d":"markdown","6ed1390d":"markdown","94fc66cb":"markdown"},"source":{"e1ec3101":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","712b2395":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","dbdae819":"data = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndata.head()","f99c68e8":"data.describe()","8cc96567":"data.isnull().sum()","91beeb43":"len_live = len(data['DEATH_EVENT'][data['DEATH_EVENT'] == 0])\nlen_death = len(data['DEATH_EVENT'][data['DEATH_EVENT'] == 1])\n\narr = np.array([len_live, len_death])\nlabels = ['LIVING','DIED']\n\nprint(f'Total number of Living case:- {len_live}')\nprint(f'Total number of Death case:- {len_death}')\n\nplt.pie(arr, labels = labels, explode=[0.2,0.0], shadow = True)\nplt.show()","fd32b917":"sns.distplot(data['age'])","e2b5d0dc":"age_above_50_not_died = data['DEATH_EVENT'][data.age >=50][data.DEATH_EVENT == 0]\nage_above_50_died = data['DEATH_EVENT'][data.age >= 50][data.DEATH_EVENT == 1]\n\nlen_died = len(age_above_50_died)\nlen_not_died = len(age_above_50_not_died)\n\narr1 = np.array([len_died, len_not_died])\nlabels =['DIED','NOT DIED']\n\nprint(f'Total number of Died:- {len_died}')\nprint(f'Total number of Not Died:- {len_not_died}')\n\nplt.pie(arr1, labels=labels, explode = [0.2, 0.0], shadow= True)\nplt.show()","c1b2db84":"patient_nhave_diabetes_0 = data['DEATH_EVENT'][data.diabetes == 0][data.DEATH_EVENT ==0]\npatient_have_diabetes_1 = data['DEATH_EVENT'][data.diabetes == 1][data.DEATH_EVENT == 1]\n\nlen_d_died = len(patient_have_diabetes_1)\nlen_d_alive = len(patient_nhave_diabetes_0)\n\narr2 = np.array([len_d_died, len_d_alive])\nlabels = ['Died with diabetes', 'Not died with diabetes']\n\nprint(f'Total number of Died with diabetes:- {len_d_died}')\nprint(f'Total number of Not died with diabetes: {len_d_alive}')\n\nplt.pie(arr2, labels=labels, explode = [0.2,0.0], shadow = True)\nplt.show()","d0a54cd1":"corr = data.corr()\nplt.subplots(figsize=(15,10))\nsns.heatmap(corr, annot=True)","6f7ec545":"data.corr().style.background_gradient(cmap='coolwarm')","8c1d5af4":"from sklearn.model_selection import train_test_split\n\nX = data.drop('DEATH_EVENT', axis = 1)\ny = data['DEATH_EVENT']\n\nX_train,X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state= 0)\n\nprint(f'Shape of the X_train {X_train.shape}')\nprint(f'Shape of the X_test {X_test.shape}')\nprint(f'Shape of the y_train {y_train.shape}')\nprint(f'Shape of the y_test {y_test.shape}')","ba4e4ba4":"def add_interactions(X):\n    features = X.columns\n    m = len(features)\n    X_int = X.copy(deep = True)\n    \n    for i in range(m):\n        feature_i_name = features[i]\n        feature_i_data = X[feature_i_name]\n        \n        for j in range(i+1, m):\n            feature_j_name = features[j]\n            feature_j_data = X[feature_j_name]\n            feature_i_j_name = feature_i_name + '_X_' + feature_j_name\n            X_int[feature_i_j_name] = feature_i_data * feature_j_data\n    return X_int\n\nX_train_mod = add_interactions(X_train)\nX_test_mod = add_interactions(X_test)","c5a1682c":"X_train_mod","5444151a":"X_test_mod","abd1d709":"from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ndef evaluating_model(y_test, y_pred):\n    '''\n    Function for evaliating our models\n    '''\n    print(f'Accuracy Score:- {accuracy_score(y_test, y_pred)}')\n    print(f'Precision Score:- {precision_score(y_test,y_pred)}')\n    print(f'Recall Score:- {recall_score(y_test,y_pred)}')\n    print(f'Confusion Matrix:- \\n{confusion_matrix(y_test, y_pred)}')","b8c20c87":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nlr_model_pip = make_pipeline(StandardScaler(), LogisticRegression())\nlr_model_pip.fit(X_train, y_train)\n\ny_pred1 = lr_model_pip.predict(X_test)\nevaluating_model(y_test, y_pred1)","e468f3cf":"from xgboost import XGBClassifier\n\nxgb1 = XGBClassifier(colsample_bytree = 1.0, \n                    learning_rate = 0.1,\n                    max_depth = 4,\n                    n_estimators = 4,\n                    subsample = 1.0)\n\neval_set = [(X_test, y_test)]\nxgb1.fit(X_train, y_train, early_stopping_rounds = 10, eval_metric = 'logloss', eval_set = eval_set, verbose = True)","d2b6dd4e":"pred = xgb1.predict(X_test)\nevaluating_model(y_test, pred)","dae6b07c":"from xgboost import plot_importance\nplot_importance(xgb1)\nplt.show()","2b658a63":"import joblib\njoblib.dump(xgb1, 'model.pkl')\nmodel = joblib.load('model.pkl')\nmodel.predict(X_test)","4ad9ba5f":"## Exploratory data analysis","391dd586":"### Building XGBoost","b01aa5e7":"### Number of null value:-","f74196ab":"* ### Seeing the distribution of classes,this will help us to identify which types","cfb8cfec":"## Detaset Development","a959bd11":"## Loading and Exploring Data","d6dd9811":"### You can do the some as here","6d92c358":"### Describing the data:-","87c47468":"## Checking correlation of our variables\n* ###  -1 indicates a perfectly negative linear correlation between two variables\n* ### 0 indicates no linear correlation between two variables\n* ### 1 indicates a perfectly positive linear correlation between two variables","35d4fd45":"### Choosing the best model and saving them","995cb12e":"* ### Selecting columns that are above age 50 and seeing died or not","cfb766e0":"## Author of this notebook: **Bhavik Jikadara**\n\n* Dataset Link: https:\/\/www.kaggle.com\/andrewmvd\/heart-failure-clinical-data","0ebdb3a8":"## Model Building","fe21613d":"### Building logistic regression with StandardScaler","6ed1390d":"## Feature Engineering \n* Now,We will do feature engineering. We will add interaction items. Interaction items are the product items are the product of two feautres. so below is the function prepared for interaction items.","94fc66cb":"* ### Seeing the distribution of age"}}