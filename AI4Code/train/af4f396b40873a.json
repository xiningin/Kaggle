{"cell_type":{"1daa22f3":"code","95f55911":"code","5f84929c":"code","1b5960bc":"code","b9037448":"code","c6b9a0c5":"code","9a325890":"code","0a27b859":"code","93757099":"code","367fbc75":"code","95984ea9":"code","c6f1730f":"code","3a86b439":"code","832705ea":"code","0271c12a":"code","78661d4f":"code","14740e54":"code","fa7dfbd4":"code","6d6485fc":"code","606cd1bb":"code","51c0231c":"code","57cce4fe":"code","9e1a73a0":"code","a0bc494c":"code","3e899b19":"code","5ee00c05":"code","530ce88e":"code","b7c69a69":"code","aab0ce16":"code","6fe517ac":"code","b81dd4e9":"code","0be43985":"code","c493b690":"code","4f5f9b21":"code","b7fb9de3":"code","7ae88924":"code","01692cd0":"code","807bdcf7":"code","0d03f324":"markdown","0e2f7515":"markdown","d90933b0":"markdown","98e81e2b":"markdown","559fbc0b":"markdown","c79767f9":"markdown","a6fa8b86":"markdown","f935c3af":"markdown","8cf8bc0b":"markdown","98378772":"markdown","62ac744b":"markdown","47695e5d":"markdown","05ddfa5e":"markdown","b11afdaa":"markdown","d0a15ea7":"markdown","8ec41fc3":"markdown","6c339371":"markdown","a0b9f311":"markdown","ac0f5a2c":"markdown","02638d07":"markdown","03bfaf6f":"markdown","32bec837":"markdown","42951d28":"markdown","cc41d6ce":"markdown","32708a17":"markdown","424ca970":"markdown","05e0f19f":"markdown","f218ceba":"markdown"},"source":{"1daa22f3":"#Importing librairies\n\nimport pandas as pd \nimport numpy as np\n\n# Scikit-learn library: For SVM\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import svm\n\nimport itertools\n\n# Matplotlib library to plot the charts\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\n\n# Library for the statistic data vizualisation\nimport seaborn\n\n%matplotlib inline\n\n","95f55911":"data = pd.read_csv('..\/input\/creditcard.csv') # Reading the file .csv\ndf = pd.DataFrame(data) # Converting data to Panda DataFrame","5f84929c":"df = pd.DataFrame(data) # Converting data to Panda DataFrame","1b5960bc":"df.describe() # Description of statistic features (Sum, Average, Variance, minimum, 1st quartile, 2nd quartile, 3rd Quartile and Maximum)","b9037448":"df_fraud = df[df['Class'] == 1] # Recovery of fraud data\nplt.figure(figsize=(15,10))\nplt.scatter(df_fraud['Time'], df_fraud['Amount']) # Display fraud amounts according to their time\nplt.title('Scratter plot amount fraud')\nplt.xlabel('Time')\nplt.ylabel('Amount')\nplt.xlim([0,175000])\nplt.ylim([0,2500])\nplt.show()","c6b9a0c5":"nb_big_fraud = df_fraud[df_fraud['Amount'] > 1000].shape[0] # Recovery of frauds over 1000\nprint('There are only '+ str(nb_big_fraud) + ' frauds where the amount was bigger than 1000 over ' + str(df_fraud.shape[0]) + ' frauds')","9a325890":"number_fraud = len(data[data.Class == 1])\nnumber_no_fraud = len(data[data.Class == 0])\nprint('There are only '+ str(number_fraud) + ' frauds in the original dataset, even though there are ' + str(number_no_fraud) +' no frauds in the dataset.')","0a27b859":"print(\"The accuracy of the classifier then would be : \"+ str((284315-492)\/284315)+ \" which is the number of good classification over the number of tuple to classify\")","93757099":"df_corr = df.corr() # Calculation of the correlation coefficients in pairs, with the default method:\n                    # Pearson, Standard Correlation Coefficient","367fbc75":"plt.figure(figsize=(15,10))\nseaborn.heatmap(df_corr, cmap=\"YlGnBu\") # Displaying the Heatmap\nseaborn.set(font_scale=2,style='white')\n\nplt.title('Heatmap correlation')\nplt.show()","95984ea9":"rank = df_corr['Class'] # Retrieving the correlation coefficients per feature in relation to the feature class\ndf_rank = pd.DataFrame(rank) \ndf_rank = np.abs(df_rank).sort_values(by='Class',ascending=False) # Ranking the absolute values of the coefficients\n                                                                  # in descending order\ndf_rank.dropna(inplace=True) # Removing Missing Data (not a number)","c6f1730f":"# We seperate ours data in two groups : a train dataset and a test dataset\n\n# First we build our train dataset\ndf_train_all = df[0:150000] # We cut in two the original dataset\ndf_train_1 = df_train_all[df_train_all['Class'] == 1] # We seperate the data which are the frauds and the no frauds\ndf_train_0 = df_train_all[df_train_all['Class'] == 0]\nprint('In this dataset, we have ' + str(len(df_train_1)) +\" frauds so we need to take a similar number of non-fraud\")\n\ndf_sample=df_train_0.sample(300)\ndf_train = df_train_1.append(df_sample) # We gather the frauds with the no frauds. \ndf_train = df_train.sample(frac=1) # Then we mix our dataset","3a86b439":"X_train = df_train.drop(['Time', 'Class'],axis=1) # We drop the features Time (useless), and the Class (label)\ny_train = df_train['Class'] # We create our label\nX_train = np.asarray(X_train)\ny_train = np.asarray(y_train)","832705ea":"############################## with all the test dataset to see if the model learn correctly ##################\ndf_test_all = df[150000:]\n\nX_test_all = df_test_all.drop(['Time', 'Class'],axis=1)\ny_test_all = df_test_all['Class']\nX_test_all = np.asarray(X_test_all)\ny_test_all = np.asarray(y_test_all)","0271c12a":"X_train_rank = df_train[df_rank.index[1:11]] # We take the first ten ranked features\nX_train_rank = np.asarray(X_train_rank)","78661d4f":"############################## with all the test dataset to see if the model learn correctly ##################\nX_test_all_rank = df_test_all[df_rank.index[1:11]]\nX_test_all_rank = np.asarray(X_test_all_rank)\ny_test_all = np.asarray(y_test_all)","14740e54":"class_names=np.array(['0','1']) # Binary label, Class = 1 (fraud) and Class = 0 (no fraud)","fa7dfbd4":"# Function to plot the confusion Matrix\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = 'd' \n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","6d6485fc":"classifier = svm.SVC(kernel='linear') # We set a SVM classifier, the default SVM Classifier (Kernel = Radial Basis Function)","606cd1bb":"classifier.fit(X_train, y_train) # Then we train our model, with our balanced data train.","51c0231c":"prediction_SVM_all = classifier.predict(X_test_all) #And finally, we predict our data test.","57cce4fe":"cm = confusion_matrix(y_test_all, prediction_SVM_all)\nplot_confusion_matrix(cm,class_names)","9e1a73a0":"print('Our criterion give a result of ' \n      + str( ( (cm[0][0]+cm[1][1]) \/ (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]\/(cm[1][0]+cm[1][1])) \/ 5))","a0bc494c":"print('We have detected ' + str(cm[1][1]) + ' frauds \/ ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\nprint('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]\/(cm[1][1]+cm[1][0])))\nprint(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) \/ (sum(cm[0]) + sum(cm[1]))))","3e899b19":"classifier.fit(X_train_rank, y_train) # Then we train our model, with our balanced data train.\nprediction_SVM = classifier.predict(X_test_all_rank) #And finally, we predict our data test.","5ee00c05":"cm = confusion_matrix(y_test_all, prediction_SVM)\nplot_confusion_matrix(cm,class_names)","530ce88e":"print('Our criterion give a result of ' \n      + str( ( (cm[0][0]+cm[1][1]) \/ (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]\/(cm[1][0]+cm[1][1])) \/ 5))","b7c69a69":"print('We have detected ' + str(cm[1][1]) + ' frauds \/ ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\nprint('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]\/(cm[1][1]+cm[1][0])))\nprint(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) \/ (sum(cm[0]) + sum(cm[1]))))","aab0ce16":"classifier_b = svm.SVC(kernel='linear',class_weight={0:0.60, 1:0.40})","6fe517ac":"classifier_b.fit(X_train, y_train) # Then we train our model, with our balanced data train.","b81dd4e9":"prediction_SVM_b_all = classifier_b.predict(X_test_all) #We predict all the data set.","0be43985":"cm = confusion_matrix(y_test_all, prediction_SVM_b_all)\nplot_confusion_matrix(cm,class_names)","c493b690":"print('Our criterion give a result of ' \n      + str( ( (cm[0][0]+cm[1][1]) \/ (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]\/(cm[1][0]+cm[1][1])) \/ 5))","4f5f9b21":"print('We have detected ' + str(cm[1][1]) + ' frauds \/ ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\nprint('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]\/(cm[1][1]+cm[1][0])))\nprint(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) \/ (sum(cm[0]) + sum(cm[1]))))","b7fb9de3":"classifier_b.fit(X_train_rank, y_train) # Then we train our model, with our balanced data train.\nprediction_SVM = classifier_b.predict(X_test_all_rank) #And finally, we predict our data test.","7ae88924":"cm = confusion_matrix(y_test_all, prediction_SVM)\nplot_confusion_matrix(cm,class_names)","01692cd0":"print('Our criterion give a result of ' \n      + str( ( (cm[0][0]+cm[1][1]) \/ (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]\/(cm[1][0]+cm[1][1])) \/ 5))","807bdcf7":"print('We have detected ' + str(cm[1][1]) + ' frauds \/ ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\nprint('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]\/(cm[1][1]+cm[1][0])))\nprint(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) \/ (sum(cm[0]) + sum(cm[1]))))","0d03f324":"However in this case given the fact that a PCA was previously performed, if the dimension reduction is effective then the PCA wasn't computed in the most effective way. Another way to put it is that no dimension reduction should be computed on a dataset on which a PCA was computed correctly.","0e2f7515":"So now, we'll use a SVM model classifier, with the scikit-learn library.","d90933b0":"In this case we are gonna try to minimize the number of errors in our prediction results. Errors are on the anti-diagonal of the confusion matrix.  But we can infer that being wrong about an actual fraud is far worse than being wrong about a non-fraud transaction. ","98e81e2b":"This dataset is unbalanced which means using the data as it is might result in unwanted behaviour from a supervised classifier.\nTo make it easy to understand if a classifier were to train with this data set trying to achieve the best accuracy possible it would most likely label every transaction as a non-fraud","559fbc0b":"# Model Selection","c79767f9":"# Data Selection","a6fa8b86":"However as we haven't infinite time nor the patience, we are going to run the classifier with the undersampled training data (for those using the undersampling principle if results are really bad just rerun the training dataset definition)","f935c3af":"As we can notice, most of the features are not correlated with each other. This corroborates the fact that a PCA was previously performed on the data.","8cf8bc0b":"OVERSAMPLING","98378772":" # Models Rank","62ac744b":"# Confusion Matrix","47695e5d":"# Testing the model","05ddfa5e":" # Models Rank","b11afdaa":"UNDERSAMPLING","d0a15ea7":"One way to do oversampling is to replicate the under-represented class tuples until we attain a correct proportion between the class","8ec41fc3":"# Data Visualization","6c339371":"# Testing the model","a0b9f311":"What can generally be done on a massive dataset is a dimension reduction.\nBy picking th emost important dimensions, there is a possiblity of explaining most of the problem, thus gaining\na considerable amount of time while preventing the accuracy to drop too much.","ac0f5a2c":"**Unbalanced data**","02638d07":"To answer this problem we could use the oversampling principle or the undersampling principle\nThe undersampling principle should be used only if we can be sure that the selected few tuples (in this case non-fraud) are representative of the whole non-fraud transactions of the dataset.","03bfaf6f":"There is a need to compute the fit method again, as the dimension of the tuples to predict went from 29 to 10 because of the dimension reduction","32bec837":"In this previously used SVM model, the weigh of each class was the same, which means that missing a fraud is as bad as misjudging a non-fraud. The objective, for a bank, is to maximize the number of detected frauds! Even if it means considering more non-fraud tuple as fraudulent operation. So, we need to minimize the False positives : the number of no detected frauds.\n\nIndeed, by modifying the class_weight parameter, we can chose which class to give more importance during the training phase. In this case, the class_1 which describes the fraudulent operations will be considered more important than the class_0 (non-fraud operation). However, in this case we will give more importance to the class_0 due to the large number of misclassed non-fraud operation. Of course the goal is to lose as little effective fraud as possible in the process.\n","42951d28":"We notive, first of all, the time doesn't impact the frequency of frauds. Moreover, the majority of frauds are small amounts.","cc41d6ce":"# Re-balanced class weigh :","32708a17":"That is why  using the accuracy as only classification criterion could be considered unthoughtful. \nDuring the remaining part of this study our criterion will consider precision on the real fraud 4 times more important than the general accuracy.\nEven though the final tested result is accuracy.","424ca970":"Then we define training and testing set after applying a dimension reduction to illustrate the fact that nothing will be gained because a PCA was previously computed","05e0f19f":"# Correlation of features","f218ceba":"We can see that the study using the reduced data is far from unrelevant, which means that the last step of the previously computed PCA could have been done in a more efficient way. Indeed one of the main question we have with the PCA once we calculated the principals components direction, is how many of this component are we gonna keep. This means that some of the 30 dimensions are do not discriminate classes that much."}}