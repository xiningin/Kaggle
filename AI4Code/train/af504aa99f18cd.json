{"cell_type":{"495353ff":"code","65f92fe4":"code","b077afe8":"code","1fac6445":"code","a134ad1d":"code","99868203":"code","06e1b239":"code","78279152":"code","673bcb68":"code","9bd8c0e8":"code","dc2d664e":"code","172ae35a":"code","1dbe208e":"code","91b6c09d":"code","3555a9ee":"code","3e502451":"code","3135ccd2":"code","b541084a":"code","77ebc386":"code","0a98fb5e":"code","e8ced816":"code","40977f06":"code","e590c014":"code","5b8580a4":"code","b618d793":"code","7c61de3a":"code","c7832cc0":"code","26c04fff":"code","de00e190":"code","3dbc19ae":"code","770fcc6f":"code","7e78368e":"code","364b5992":"code","ca1d69b0":"code","54a8a94d":"code","21b056ca":"code","d92bd388":"code","1c011590":"code","985977a8":"code","7004d756":"code","bfd14986":"code","04ad5067":"code","8cf3bd20":"code","98ae0294":"code","ac059420":"code","dffcb93d":"code","d571029f":"code","b9f45aa3":"code","642cacb9":"code","d531f4d2":"code","67876166":"markdown","7badde34":"markdown","603abbae":"markdown","6486fe5a":"markdown","bc92716a":"markdown","a5589f96":"markdown","b8620b43":"markdown","10409af1":"markdown","b9d00c59":"markdown","056c1248":"markdown","b8ce065f":"markdown","f66c5158":"markdown","9c8385f7":"markdown","8c681d64":"markdown","d39e2a9f":"markdown","e2c5c722":"markdown","c958e6d2":"markdown","18658db0":"markdown","6c584197":"markdown","0fcc63fe":"markdown","80635899":"markdown","9689af59":"markdown","084d3113":"markdown","755a9dc2":"markdown","efebcd80":"markdown","31d68faf":"markdown","66e0c8d8":"markdown","0ecf2d61":"markdown","a7132a2a":"markdown","9f6e797e":"markdown","767077b8":"markdown","1d14abd0":"markdown"},"source":{"495353ff":"import h5py\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n%matplotlib inline\r\n\r\n# used to supress display of warnings\r\nimport warnings\r\n\r\nfrom sklearn.metrics import precision_recall_curve,accuracy_score,f1_score,precision_score,recall_score","65f92fe4":"# suppress display of warnings\r\nwarnings.filterwarnings('ignore')","b077afe8":"import os\nsource_dir=os.path.join('\/kaggle','input','pins-face-recognition','105_classes_pins_dataset')","1fac6445":"class IdentityMetadata():\n    def __init__(self, base, name, file):\n        self.base = base\n        # identity name\n        self.name = name\n        # image file name\n        self.file = file\n\n    def __repr__(self):\n        return self.image_path()\n\n    def image_path(self):\n        return os.path.join(self.base, self.name, self.file) \n    \ndef load_metadata(path):\n    metadata = []\n    for i in os.listdir(path):\n        for f in os.listdir(os.path.join(path, i)):\n            # Check file extension. Allow only jpg\/jpeg' files.\n            ext = os.path.splitext(f)[1]\n            if ext == '.jpg' or ext == '.jpeg':\n                metadata.append(IdentityMetadata(path, i, f))\n    return np.array(metadata)\n\n# metadata = load_metadata('images')\nmetadata = load_metadata(source_dir)","a134ad1d":"print('metadata shape :', metadata.shape)","99868203":"metadata[1500]","06e1b239":"type(metadata[1500]), metadata[1500].image_path()","78279152":"import cv2\r\ndef load_image(path):\r\n    img = cv2.imread(path, 1)\r\n    # OpenCV loads images with color channels\r\n    # in BGR order. So we need to reverse them\r\n    return img[...,::-1]","673bcb68":"load_image('\/kaggle\/input\/pins-face-recognition\/105_classes_pins_dataset\/pins_Emilia Clarke\/Emilia Clarke247_998.jpg')","9bd8c0e8":"from tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation\r\n\r\ndef vgg_face():\t\r\n    model = Sequential()\r\n    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\r\n    model.add(Convolution2D(64, (3, 3), activation='relu'))\r\n    model.add(ZeroPadding2D((1,1)))\r\n    model.add(Convolution2D(64, (3, 3), activation='relu'))\r\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\r\n    \r\n    model.add(ZeroPadding2D((1,1)))\r\n    model.add(Convolution2D(128, (3, 3), activation='relu'))\r\n    model.add(ZeroPadding2D((1,1)))\r\n    model.add(Convolution2D(128, (3, 3), activation='relu'))\r\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\r\n    \r\n    model.add(ZeroPadding2D((1,1)))\r\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\r\n    model.add(ZeroPadding2D((1,1)))\r\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\r\n    model.add(ZeroPadding2D((1,1)))\r\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\r\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\r\n    \r\n    model.add(ZeroPadding2D((1,1)))\r\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\r\n    model.add(ZeroPadding2D((1,1)))\r\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\r\n    model.add(ZeroPadding2D((1,1)))\r\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\r\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\r\n    \r\n    model.add(ZeroPadding2D((1,1)))\r\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\r\n    model.add(ZeroPadding2D((1,1)))\r\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\r\n    model.add(ZeroPadding2D((1,1)))\r\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\r\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\r\n    \r\n    model.add(Convolution2D(4096, (7, 7), activation='relu'))\r\n    model.add(Dropout(0.5))\r\n    model.add(Convolution2D(4096, (1, 1), activation='relu'))\r\n    model.add(Dropout(0.5))\r\n    model.add(Convolution2D(2622, (1, 1)))\r\n    model.add(Flatten())\r\n    model.add(Activation('softmax'))\r\n    return model","dc2d664e":"model = vgg_face()\n\nmodel.load_weights('..\/input\/vgg-face-weights\/vgg_face_weights.h5')","172ae35a":"model.layers[0], model.layers[-2]","1dbe208e":"from tensorflow.keras.models import Model\r\nvgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)","91b6c09d":"type(vgg_face_descriptor)","3555a9ee":"vgg_face_descriptor.inputs, vgg_face_descriptor.outputs","3e502451":"# Get embedding vector for first image in the metadata using the pre-trained model\r\nimg_path = metadata[0].image_path()\r\nimg = load_image(img_path)\r\n\r\n# Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\r\nimg = (img \/ 255.).astype(np.float32)\r\nimg = cv2.resize(img, dsize = (224,224))\r\nprint(img.shape)\r\n\r\n# Obtain embedding vector for an image\r\n# Get the embedding vector for the above image using vgg_face_descriptor model and print the shape \r\nembedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]\r\nprint(embedding_vector.shape)","3135ccd2":"embedding_vector[0], type(embedding_vector), type(embedding_vector[0])","b541084a":"embedding_vector[2], embedding_vector[98], embedding_vector[-2]","77ebc386":"total_images = len(metadata)\r\n\r\nprint('total_images :', total_images)","0a98fb5e":"embeddings = np.zeros((metadata.shape[0], 2622))\r\nfor i, m in enumerate(metadata):\r\n    img_path = metadata[i].image_path()\r\n    img = load_image(img_path)\r\n    img = (img \/ 255.).astype(np.float32)\r\n    img = cv2.resize(img, dsize = (224,224))\r\n    embedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]\r\n    embeddings[i]=embedding_vector","e8ced816":"print('embeddings shape :', embeddings.shape)","40977f06":"embeddings[0], embeddings[988], embeddings[988].shape","e590c014":"embeddings[8275]","5b8580a4":"def distance(emb1, emb2):\r\n    return np.sum(np.square(emb1 - emb2))","b618d793":"def show_pair(idx1, idx2):\n    plt.figure(figsize=(8,3))\n    plt.suptitle(f'Distance between {idx1} & {idx2}= {distance(embeddings[idx1], embeddings[idx2]):.2f}')\n    plt.subplot(121)\n    plt.imshow(load_image(metadata[idx1].image_path()))\n    plt.subplot(122)\n    plt.imshow(load_image(metadata[idx2].image_path()));    \n\nshow_pair(900, 901)\nshow_pair(900, 1001)","7c61de3a":"show_pair(1100, 1101)\nshow_pair(1100, 1300)","c7832cc0":"show_pair(1407, 1408)\nshow_pair(1408, 1602)","26c04fff":"train_idx = np.arange(metadata.shape[0]) % 9 != 0     #every 9th example goes in test data and rest go in train data\r\ntest_idx = np.arange(metadata.shape[0]) % 9 == 0\r\n\r\n# one half as train examples of 10 identities\r\nX_train = embeddings[train_idx]\r\n\r\n# another half as test examples of 10 identities\r\nX_test = embeddings[test_idx]\r\ntargets = np.array([m.name for m in metadata])\r\n\r\n#train labels\r\ny_train = targets[train_idx]\r\n\r\n#test labels\r\ny_test = targets[test_idx]","de00e190":"print('X_train shape : ({0},{1})'.format(X_train.shape[0], X_train.shape[1]))\r\nprint('y_train shape : ({0},)'.format(y_train.shape[0]))\r\nprint('X_test shape : ({0},{1})'.format(X_test.shape[0], X_test.shape[1]))\r\nprint('y_test shape : ({0},)'.format(y_test.shape[0]))","3dbc19ae":"y_test[0], y_train[988]","770fcc6f":"len(np.unique(y_test)), len(np.unique(y_train))","7e78368e":"from sklearn.preprocessing import LabelEncoder\r\n\r\nle = LabelEncoder()\r\ny_train_encoded = le.fit_transform(y_train)","364b5992":"print(le.classes_)\r\ny_test_encoded = le.transform(y_test)","ca1d69b0":"print('y_train_encoded : ', y_train_encoded)\r\nprint('y_test_encoded : ', y_test_encoded)","54a8a94d":"# Standarize features\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\nscaler = StandardScaler()\r\nX_train_std = scaler.fit_transform(X_train)","21b056ca":"X_test_std = scaler.transform(X_test)","d92bd388":"print('X_train_std shape : ({0},{1})'.format(X_train_std.shape[0], X_train_std.shape[1]))\r\nprint('y_train_encoded shape : ({0},)'.format(y_train_encoded.shape[0]))\r\nprint('X_test_std shape : ({0},{1})'.format(X_test_std.shape[0], X_test_std.shape[1]))\r\nprint('y_test_encoded shape : ({0},)'.format(y_test_encoded.shape[0]))","1c011590":"from sklearn.decomposition import PCA\r\n\r\npca = PCA(n_components=128)\r\nX_train_pca = pca.fit_transform(X_train_std)\r\nX_test_pca = pca.transform(X_test_std)","985977a8":"from sklearn.svm import SVC\r\n\r\nclf = SVC(C=5., gamma=0.001)\r\nclf.fit(X_train_pca, y_train_encoded)","7004d756":"y_predict = clf.predict(X_test_pca)","bfd14986":"print('y_predict : ',y_predict)\r\nprint('y_test_encoded : ',y_test_encoded)","04ad5067":"y_predict_encoded = le.inverse_transform(y_predict)","8cf3bd20":"print('y_predict_encoded : ',y_predict_encoded)","98ae0294":"print('y_predict shape : ', y_predict.shape)\r\nprint('y_test_encoded shape : ', y_test_encoded.shape)","ac059420":"y_test_encoded[32:49]","dffcb93d":"# Find the classification accuracy\r\naccuracy_score(y_test_encoded, y_predict)","d571029f":"example_idx = 401\r\n\r\nexample_image = load_image(metadata[test_idx][example_idx].image_path())\r\nexample_prediction = y_predict[example_idx]\r\nexample_identity =  y_predict_encoded[example_idx]\r\n\r\nplt.imshow(example_image)\r\nplt.title(f'Identified as {example_identity}');","b9f45aa3":"example_idx = 900\n\nexample_image = load_image(metadata[test_idx][example_idx].image_path())\nexample_prediction = y_predict[example_idx]\nexample_identity =  y_predict_encoded[example_idx]\n\nplt.imshow(example_image)\nplt.title(f'Identified as {example_identity}');","642cacb9":"example_idx = 317\r\n\r\nexample_image = load_image(metadata[test_idx][example_idx].image_path())\r\nexample_prediction = y_predict[example_idx] \r\nexample_identity =  y_predict_encoded[example_idx] \r\n\r\nplt.imshow(example_image)\r\nplt.title(f'Identified as {example_identity}');","d531f4d2":"example_idx = -27\r\n\r\nexample_image = load_image(metadata[test_idx][example_idx].image_path())\r\nexample_prediction = y_predict[example_idx]\r\nexample_identity =  y_predict_encoded[example_idx]\r\n\r\nplt.imshow(example_image)\r\nplt.title(f'Identified as {example_identity}');","67876166":"<a id = '5.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 5. Model Building and Validation <\/h2> ","7badde34":"<p style = \"font-size:20px; color: #007580 \"><strong> Load a sample image<\/strong><\/p>\r\n- Load one image using the function \"load_image\"","603abbae":"### Data Description:\n\n**Aligned Face Dataset from Pinterest**\n\nThis dataset contains 17534 images for 100 people. All images are taken from 'Pinterest' and aligned using dlib library.","6486fe5a":"<p style = \"font-size:20px; color: #007580 \"><strong> Define a function to load an image <\/strong><\/p>\r\n- Define a function to load image from the metadata","bc92716a":"<a id = '4.4'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.4 Create train and test sets <\/strong><\/p>\r\n- Create X_train, X_test and y_train, y_test\r\n- Use train_idx to seperate out training features and labels\r\n- Use test_idx to seperate out testing features and labels","a5589f96":"<p style = \"font-size:20px; color: #007580 \"><strong> Function to calculate distance between given 2 pairs of images <\/strong><\/p>\r\n\r\n- Consider distance metric as \"Squared L2 distance\"\r\n- Squared l2 distance between 2 points (x1, y1) and (x2, y2) = (x1-x2)^2 + (y1-y2)^2","b8620b43":"<a id = '3.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 3. Data Collection <\/h2> ","10409af1":"<p style = \"font-size:20px; color: #007580 \"><strong> Standardize the feature values <\/strong><\/p> \r\n- Scale the features using StandardScaler","b9d00c59":"<a id = '1.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 1. Overview <\/h2> ","056c1248":"<a id = '4.2'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.2 Generate embeddings for each image in the dataset <\/strong><\/p>\n\r\n- Given below is an example to load the first image in the metadata and get its embedding vector from the pre-trained model. ","b8ce065f":"#### Setting Options","f66c5158":"### Objective:\r\n\r\nIn this problem, we use a pre-trained model trained on Face recognition to recognize similar faces. Here, we are particularly interested in recognizing whether two given faces are of the same person or not.","9c8385f7":"<a id = '6.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 6. Conclusion <\/h2> ","8c681d64":"<p style = \"font-size:20px; color: #007580 \"><strong> Load the model <\/strong><\/p>\n\r\n- Load the model defined above\r\n- Then load the given weight file named \"vgg_face_weights.h5\"","d39e2a9f":"<a id = '4.5'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.5 Reduce dimensions using PCA <\/strong><\/p> \r\n- Reduce feature dimensions using Principal Component Analysis\r\n- Set the parameter n_components=128","e2c5c722":"<p style = \"font-size:30px; color: #007580 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Here are my other notebooks, please have a look and definitely you will find it useful. Happy reading \ud83d\ude42<\/strong><\/p>\n<ol>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/industrial-safety-complete-solution\">Industrial Safety - Complete Solution<\/a><\/li>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/eda-statistical-analysis-hypothesis-testing\">EDA - Statistical Analysis - Hypothesis Testing<\/a><\/li>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/random-forest-with-bootstrap-sampling-for-beginner\">Random Forest with Bootstrap Sampling for beginner<\/a><\/li>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/amazon-electronics-eda-recommender-system\">Amazon Electronics - EDA - Recommender System<\/a><\/li>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/personal-loan-logistic-reg-accuracy-90-41\">Personal Loan - Logistic Reg - Accuracy = 90.41%<\/a><\/li>\n<\/ol>","c958e6d2":"<a id = '2.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 2. Import the necessary libraries <\/h2> ","18658db0":"1. This dataset contains 17534 images for 100 people. All images are taken from 'Pinterest' and aligned using dlib library.\n2. Generated embeddings for all images using pre-trained VGG Face model.\n3. Used \"Squared L2 distance\" to calculate the distance between given 2 pairs of images.\n4. Encoded the target variables, standardize the features and reduced dimensions using PCA.\n5. Used SVM classifier to predict the celebrity in a given image and achived a 96.455% accuracy.\n\n- Reference Link for Template used in this notebook - https:\/\/www.kaggle.com\/bhuvanchennoju\/ancient-roots-of-agriculture-a-data-overview\n\n<p style = \"font-size:30px; color: #007580 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Thanks for reading \ud83d\ude42<\/strong><\/p>","6c584197":"<a id = '5.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.1 Build a Machine Learning Classifier <\/strong><\/p> \n\r\n- Use SVM Classifier to predict the person in the given image\r\n- Fit the classifier and print the score","0fcc63fe":"**Accuracy Score: 96.455%**","80635899":"<br>\n<h2 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Celebrity Face Recognition using VGGFace Model <\/h2> \n<br>","9689af59":"<p style = \"font-size:20px; color: #007580 \"><strong> Function to load images <\/strong><\/p> \r\n- Define a function to load the images from the extracted folder and map each image with person id \r\n","084d3113":"<a id = '5.2'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.2 Validate Celebrity Images <\/strong><\/p> \n\n- Take  401th  image from test set and plot the image\n- Report to which person(folder name in dataset) the image belongs to","755a9dc2":"<a id = '4.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 4. Feature Engineering <\/h2> ","efebcd80":"<p style = \"font-size:20px; color: #007580 \"><strong> Encode the Labels <\/strong><\/p>\r\n- Encode the targets\r\n- Use LabelEncoder","31d68faf":"<a id = '4.3'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.3 Plot images and get distance between the pairs <\/strong><\/p>\n\n- 900, 901 and 900, 1001\n- 1100, 1101 and 1100, 1300\n- 1407, 1408 and 1408, 1602","66e0c8d8":"<a id = '4.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.1 VGG Face model <\/strong><\/p>\r\n- Here we are giving you the predefined model for VGG face","0ecf2d61":"<p style = \"font-size:20px; color: #007580 \"><strong> Generate embeddings for all images <\/strong><\/p>\n\r\n- Write code to iterate through metadata and create embeddings for each image using `vgg_face_descriptor.predict()` and store in a list with name `embeddings`\r\n\r\n- If there is any error in reading any image in the dataset, fill the emebdding vector of that image with 2622-zeroes as the final embedding from the model is of length 2622.","a7132a2a":"<a id = '0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #007580; color : #fed049; border-radius: 5px 5px; text-align:center; font-weight: bold\" >Table of Contents<\/h2> \n\n1. [Overview](#1.0)\n2. [Import the necessary libraries](#2.0)\n3. [Data Collection](#3.0)\n4. [Feature Engineering](#4.0)\n\t- [VGG Face model](#4.1)\n\t- [Generate embeddings for each image in the dataset](#4.2)\n\t- [Plot images and get distance between the pairs](#4.3)\n\t- [Create train and test sets](#4.4)\n\t- [Reduce dimensions using PCA](#4.5)\n5. [Model Building and Validation](#5.0)\n    - [Build a Machine Learning Classifier](#5.1)\n    - [Validate Celebrity Images](#5.2)\n6. [Conclusion](#6.0)","9f6e797e":"<p style = \"font-size:20px; color: #007580 \"><strong> Get vgg_face_descriptor <\/strong><\/p>","767077b8":"[![Celebrity.png](https:\/\/i.postimg.cc\/5yp4715n\/Celebrity.png)](https:\/\/postimg.cc\/xNJV8w4z)","1d14abd0":"### Project Description:\r\n\r\nIn this hands-on project, the goal is to build a face identification model to recognize faces."}}