{"cell_type":{"b77c4f29":"code","82a8c542":"code","a8cd92d3":"code","360ad6bf":"code","3690e994":"code","343e3dcf":"code","d4633c5e":"code","62384f9c":"code","770aa059":"code","daff5348":"code","de5f043c":"code","2c2460b3":"code","b8af9e4b":"code","67174025":"code","049db56b":"code","b35fff12":"markdown","8495553a":"markdown","bcb5516f":"markdown","05eee124":"markdown","1143e02a":"markdown","8adc6cd4":"markdown","1bc54c72":"markdown","07acb944":"markdown","748f0514":"markdown","b8c24c67":"markdown","a4c7bb9a":"markdown","2c23f83a":"markdown","f554a18a":"markdown","fd66f30c":"markdown","93a1eade":"markdown","e23b32c9":"markdown","9d248cf3":"markdown","ae6bc768":"markdown"},"source":{"b77c4f29":"import pandas as pd \nimport seaborn as sns \nimport numpy as np \nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit","82a8c542":"dataset = pd.read_csv('..\/input\/call-center-metrics-dataset\/call_metrics_dataset.csv', delimiter = ';')\ndataset.index = pd.to_datetime(dataset['date'])\ndel(dataset['date'])\ndataset['avg_aht'] = dataset['avg_aht'].str.replace('.', '', regex = True)\ndataset['avg_aht'] = dataset['avg_aht'].str.replace(',', '.', regex = True).astype(float)\ndataset.head()","a8cd92d3":"corr = dataset.astype('float64').corr()\nax = sns.heatmap(corr, annot = True)","360ad6bf":"x = dataset.iloc[:, 0:6].values\ny = dataset.iloc[:, -1]. values\nx","3690e994":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nx_train, x_test = sc_x.fit_transform(x_train), sc_x.transform(x_test)","343e3dcf":"from sklearn.decomposition import PCA\npca = PCA(n_components = 3)\nx_train = pca.fit_transform(x_train)\nx_test = pca.transform(x_test)\nexplained_variance = pca.explained_variance_ratio_\nexplained_variance","d4633c5e":"from xgboost import XGBClassifier\nclassifier = XGBClassifier(use_label_encoder = False, eval_metric = 'mlogloss').fit(x_train, y_train)\ny_classifier = classifier.predict(x_test)\naccuracy_score(y_test, y_classifier)","62384f9c":"from sklearn.ensemble import RandomForestClassifier\nclassifier2 = RandomForestClassifier(n_estimators = 10).fit(x_train, y_train)\ny_classifier2 = classifier2.predict(x_test)\naccuracy_score(y_test, y_classifier2)","770aa059":"from sklearn.svm import SVC\nclassifier3 = SVC(gamma = 1).fit(x_train, y_train)\ny_classifier3 = classifier3.predict(x_test)\naccuracy_score(y_test, y_classifier3)","daff5348":"from sklearn.linear_model import LogisticRegression\nclassifier4 = LogisticRegression(max_iter = 200).fit(x_train, y_train)\ny_classifier4 = classifier4.predict(x_test)\naccuracy_score(y_test, y_classifier4)","de5f043c":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","2c2460b3":"title = \"Curva de aprendizaje\"\ncv = ShuffleSplit(n_splits = 10, test_size = 0.3)\nplot_learning_curve(classifier, title, x_train, y_train, ylim = (0.6, 1.0), cv = cv, n_jobs = 1)","b8af9e4b":"plot_learning_curve(classifier2, title, x_train, y_train, ylim = (0.6, 1.0), cv = cv, n_jobs = 1)","67174025":"plot_learning_curve(classifier3, title, x_train, y_train, ylim = (0.6, 1.0), cv = cv, n_jobs = 1)","049db56b":"plot_learning_curve(classifier4, title, x_train, y_train, ylim = (0.6, 1.0), cv = cv, n_jobs = 1)","b35fff12":"**LOGISTIC REGRESSION**","8495553a":"**XGBOOST CLASSIFIER**","bcb5516f":"**LOGISTIC REGRESSION**","05eee124":"**Loading and inspection   - - - - - - - - -  Carga e Inspecci\u00f3n**","1143e02a":"**RANDOM FOREST**","8adc6cd4":"**Splitting the dataset      - - - - - - - - - - - - - - -        Dividiendo el dataset**","1bc54c72":"**XGBOOST**","07acb944":"# **Selection Models- - - - - - - - - - - - - - - - - - - Selecci\u00f3n de Modelos**","748f0514":"# **Learning curve**","b8c24c67":"**SVM**","a4c7bb9a":"**Ingles:**\n\n*We can see that within our models a good performance is displayed, however this may be due to the few data we have and although all the models are giving us very similar predictions, we can observe that in the learning curves who is best adapting is XGBOOST, likewise we can highlight the performance that Logistic Regression is giving us although its accuracy is ~ 4% (approximate (~)) below XGBOOST, therefore the recommendation is that if more data were obtained one of the models to consider first will be XGBOOST*\n\n**Espa\u00f1ol:**\n\n*Podemos observar que dentro de nuestros modelos se visualiza un buen desempe\u00f1o, sin embargo esto se puede deber a los pocos datos con los que contamos y aunque todos los modelos esten dandonos predicciones muy similares podemos observar que en las curvas de aprendizaje quien mejor se esta adaptando es XGBOOST, as\u00ed mismo podemos resaltar el desempe\u00f1o que nos esta dando Logistic Regression aunque su accuracy esta en un ~4% (aproximado(~)) por debajo de XGBOOST, por tanto la recomendaci\u00f3n es que si se consiguieran m\u00e1s datos uno de los modelos a considerar primeramente sera XGBOOST*\n","2c23f83a":"**SUPPORT VECTOR MACHINE**","f554a18a":"**Correlation - - - - - - - - - - - - - - Correlaci\u00f3n**","fd66f30c":"# **Conclusions - - - - - - - - - - - - - - - - Conclusiones**","93a1eade":"**RANDOM FOREST**","e23b32c9":"**Dividing into training and testing and normalizing the variables - - - - - - - - - - - - - - Dividiendo en entrenamiento y test y normalizando las variables**","9d248cf3":"**Using variable reduction (PCA) - - - - - - - - - - - Usando reducci\u00f3n de variables (PCA)**","ae6bc768":"# **Exploratory Analysis - - - - - - - - - - - - Analisis Exploratorio**"}}