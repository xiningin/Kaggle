{"cell_type":{"fd64e3cc":"code","ce5ae45c":"code","5a56fd1f":"code","2277f187":"code","eb493eb9":"code","e66919a8":"code","18345fc5":"code","055b4463":"code","be4cdd37":"code","c8ca0698":"code","b04dabd5":"code","0a7c518c":"code","ef319690":"code","bd67cd34":"code","117331e8":"code","b8958f05":"code","ca9da8e4":"code","3e363d35":"code","b2377fd7":"code","6c00c911":"code","a1000311":"code","6f5f6e3e":"code","f942a8cf":"code","686a07d5":"code","923a70d3":"code","d7111192":"code","327d1a07":"code","4735a8b5":"code","32ad252a":"code","a6ab3bf1":"code","ba2206bd":"code","deb2296b":"code","04e2c1df":"code","5ca558d9":"code","743006ce":"code","cdef77c0":"code","e2c41486":"code","b7fa0def":"code","c94848a3":"code","763fcaa6":"code","1c43185a":"code","4dea9ae5":"code","b7c900f6":"code","0841d596":"code","120bff24":"code","57d6de5e":"code","2cb9ca22":"code","e1bee564":"code","3bf1c9af":"code","4a6e2c06":"code","096dba7f":"code","67153aff":"code","fef62112":"code","86b1a719":"code","a8938761":"code","3a9fa1b2":"code","7a2923c1":"code","b1ee7c74":"code","6757bdae":"code","d46309d7":"code","b98ad9d1":"code","579a0241":"code","bd9d3642":"code","eea06099":"code","3dfa6e30":"code","10341d0d":"code","8aef8725":"code","778147f6":"code","83889302":"code","38f03635":"code","03e71d20":"code","4eb139a9":"code","70614dbe":"code","312e0eb5":"code","75414868":"code","f50232e7":"code","2b9575b4":"code","b9f10af7":"markdown","4b909f74":"markdown","0bc6086c":"markdown","9fb49838":"markdown","7e55ace0":"markdown","836c0632":"markdown","6f9b088c":"markdown","48d9f4dd":"markdown","2f883018":"markdown","46e2c75c":"markdown","17195d3a":"markdown","75e03b7e":"markdown","3c015af7":"markdown","865c02ae":"markdown","4b43011c":"markdown","6e9310e4":"markdown","4bdfce00":"markdown","1b511ed6":"markdown"},"source":{"fd64e3cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ce5ae45c":"train_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","5a56fd1f":"train_data.info()","2277f187":"\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,8))\nsns.heatmap(train_data.corr(), center = 0)\nplt.title(\"Correlations Between Columns\")\nplt.show()","eb493eb9":"y = train_data.SalePrice\nX = train_data.drop(columns=[\"SalePrice\"], axis=1)","e66919a8":"y.shape, X.shape, test_data.shape","18345fc5":"corr_matrix = train_data.corr()","055b4463":"corr_matrix['SalePrice'][(corr_matrix[\"SalePrice\"] > 0.40) | (corr_matrix[\"SalePrice\"] < -0.40)]","be4cdd37":"important_num_cols = list(corr_matrix['SalePrice'][(corr_matrix[\"SalePrice\"] > 0.5) | (corr_matrix[\"SalePrice\"] < -0.5)].index)\n\nimportant_num_cols.remove('SalePrice')\nlen(important_num_cols)","c8ca0698":"important_num_cols","b04dabd5":"X_num_only = X[important_num_cols]","0a7c518c":"X_num_only.shape","ef319690":"plt.figure(figsize=(10,8))\nsns.heatmap(X_num_only.corr(), center = 0)\nplt.title(\"Correlations Between Columns\")\nplt.show()","bd67cd34":"corr_X = X_num_only.corr()\nlen(corr_X)","117331e8":"\nfor i in range(0, len(corr_X) - 1):\n    for j in range(i + 1, len(corr_X)):\n        if(corr_X.iloc[i, j] < -0.6 or corr_X.iloc[i, j] > 0.6):\n            print(corr_X.iloc[i, j], i, j, corr_X.index[i], corr_X.index[j])\n            ","b8958f05":"# Based on the above information, we further discard the features 1stFlrSF, FullBath, TotRmsAbvGrd, GarageArea\n#num_cols = [i for i in X_modified.columns if i not in ['1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'GarageArea']]\nnum_cols = [i for i in X_num_only.columns if i not in ['1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'GarageArea']]\n","ca9da8e4":"# Categorical columns - choose the important ones\n\ncat_cols = [\"MSZoning\", \"Utilities\",\"BldgType\",\"Heating\",\"KitchenQual\",\"SaleCondition\",\"LandSlope\"]","3e363d35":"X_final = X[num_cols]","b2377fd7":"X_final.shape","6c00c911":"X_final['YearRemodAdd'] = X_final['YearRemodAdd'] - X_final['YearBuilt']","a1000311":"X_final.head()","6f5f6e3e":"X_final.isna().sum()","f942a8cf":"#X_final['MasVnrArea'] = X_final['MasVnrArea'].fillna(X_final['MasVnrArea'].median())","686a07d5":"X[cat_cols].isna().sum()","923a70d3":"X_categorical_df = pd.get_dummies(X[cat_cols], columns=cat_cols)","d7111192":"X_categorical_df","327d1a07":"# Create final dataframe","4735a8b5":"X_final = X_final.join(X_categorical_df)","32ad252a":"X_final","a6ab3bf1":"from sklearn import preprocessing\nstandardize = preprocessing.StandardScaler().fit(X_final[num_cols])","ba2206bd":"#See mean per column\nstandardize.mean_","deb2296b":"#transform\nX_final[num_cols] = standardize.transform(X_final[num_cols])","04e2c1df":"X_final","5ca558d9":"X_final.head()","743006ce":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_final, y, test_size=0.2, random_state=1)","cdef77c0":"X_train.shape, X_val.shape, y_train.shape, y_val.shape","e2c41486":"from sklearn.metrics import r2_score \nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import PolynomialFeatures","b7fa0def":"perf = []\nmethod = []","c94848a3":"from sklearn.metrics import mean_squared_log_error","763fcaa6":"# Linear Regression\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\npredictions = lin_reg.predict(X_val)\n\nr_squared = r2_score(predictions, y_val)\n\nprint(\"R2 Score:\", r_squared)\nrmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\nprint(\"RMSLE:\", rmsle)\nmethod.append('Linear Regression')\nperf.append(rmsle)\n","1c43185a":"# Ridge regression\nridge = Ridge()\nridge.fit(X_train, y_train)\npredictions = ridge.predict(X_val)\n\nr_squared = r2_score(predictions, y_val)\n\nprint(\"R2 Score:\", r_squared)\nmethod.append('Ridge Regression')\n\nrmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\nprint(\"RMSLE:\", rmsle)\nperf.append(rmsle)","4dea9ae5":"# Ridge regression\nlasso = Lasso()\nlasso.fit(X_train, y_train)\npredictions = lasso.predict(X_val)\n\nr_squared = r2_score(predictions, y_val)\n\nprint(\"R2 Score:\", r_squared)\nmethod.append('Lasso Regression')\n\nrmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\nprint(\"RMSLE:\", rmsle)\nperf.append(rmsle)","b7c900f6":"# support vector regression\nfrom sklearn.svm import SVR\nsvr = SVR(C=1000000)\nsvr.fit(X_train, y_train)\npredictions = svr.predict(X_val)\n\nr_squared = r2_score(predictions, y_val)\n\nprint(\"R2 Score:\", r_squared)\n#method.append('SVM')\nrmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\nprint(\"RMSLE:\", rmsle)\n#perf.append(rmsle)","0841d596":"svr_rbf = SVR(kernel=\"rbf\", C=1000000, gamma=0.01, epsilon=0.1)\nsvr_rbf.fit(X_train, y_train)\npredictions = svr_rbf.predict(X_val)\n\nr_squared = r2_score(predictions, y_val)\n\nprint(\"R2 Score:\", r_squared)\n\nmethod.append('SVR')\nrmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\nprint(\"RMSLE:\", rmsle)\nperf.append(rmsle)","120bff24":"#Random forest regressor\nfor i in range(50 , 500, 50):\n    random_forest = RandomForestRegressor(n_estimators=i)\n    random_forest.fit(X_train, y_train)\n    predictions = random_forest.predict(X_val)\n\n    r_squared = r2_score(predictions, y_val)\n\n    print(\"R2 Score:\", r_squared)\n    method.append('Random Forest Regressor')\n    rmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\n    print(\"RMSLE:\", rmsle)\n    perf.append(rmsle)","57d6de5e":"# xgboost\nfrom xgboost import XGBRegressor\nxgb = XGBRegressor(n_estimators=1000, learning_rate=0.01)\nxgb.fit(X_train, y_train)\npredictions = xgb.predict(X_val)\n\nr_squared = r2_score(predictions, y_val)\n\nprint(\"R2 Score:\", r_squared)\nmethod.append('XGBoost Regressor')\nrmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\nprint(\"RMSLE:\", rmsle)\nperf.append(rmsle)","2cb9ca22":"# ANN\n'''\nimport math\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.losses import MeanSquaredLogarithmicError\n\n\n\nhidden_units1 = 400\n#hidden_units2 = 480\nhidden_units3 = 256\nlearning_rate = 0.01\n# Creating model using the Sequential in tensorflow\ndef build_model_using_sequential():\n    model = Sequential([\n        Dense(hidden_units1, kernel_initializer='normal', activation='relu'),\n        Dropout(0.2),\n        Dense(hidden_units3, kernel_initializer='normal', activation='relu'),\n        Dense(1, kernel_initializer='normal', activation='linear')\n      ])\n    return model\n# build the model\nmodel = build_model_using_sequential()\n\n# loss function\nmsle = MeanSquaredLogarithmicError()\nmodel.compile(\n    loss=msle, \n    optimizer=Adam(learning_rate=learning_rate), \n    metrics=[msle]\n)\n\n# train the model\nhistory = model.fit(\n    X_final.values, \n    y.values, \n    epochs=1000, \n    batch_size=64,\n    validation_split=0.2\n)\npredictions = model.predict(X_val)\nrmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\nprint(\"RMSLE:\", rmsle)\nmethod.append('ANN')\nperf.append(rmsle)\n'''","e1bee564":"# Compare performances of models\nplt.barh(method, perf)\nplt.title('RMSLE comparison of models')","3bf1c9af":"# Test Data Preprocessing\n\nX_test = test_data[num_cols + cat_cols]\nX_test['YearRemodAdd'] = X_test['YearRemodAdd'] - X_test['YearBuilt']","4a6e2c06":"X_test.shape","096dba7f":"# Encode categorical similar to train\nX_test = pd.get_dummies(X_test)","67153aff":"X_test","fef62112":"# Add missed columns missed due to get dummies on X_test\nX_test = X_test.reindex(columns = X_final.columns, fill_value=0)","86b1a719":"X_test","a8938761":"#transform\nX_test[num_cols] = standardize.transform(X_test[num_cols])","3a9fa1b2":"X_test","7a2923c1":"X_test.isna().sum()","b1ee7c74":"# we will use median for missing values\nX_test['TotalBsmtSF'] = X_test['TotalBsmtSF'].fillna(train_data['TotalBsmtSF'].median())","6757bdae":"# mode for cars\nX_test['GarageCars'] = X_test['GarageCars'].fillna(train_data['GarageCars'].mode()[0])","d46309d7":"# Submission using SVR\n\npreds = svr_rbf.predict(X_test)\nsubmit = pd.DataFrame({'Id': test_data['Id'], 'SalePrice': preds})\nsubmit.to_csv('submission.csv',index=False)\n","b98ad9d1":"# Submission using ANN\n'''\npreds = model.predict(X_test)\npreds_2 = [i[0] for i in preds]\nout = pd.DataFrame({'Id': test_data['Id'], 'SalePrice': preds_2}) \nout.to_csv('submission.csv',index=False)\n'''","579a0241":"X_numerical = X[num_cols]","bd9d3642":"X_numerical","eea06099":"X_categorical_df = X[cat_cols]","3dfa6e30":"X_categorical_df","10341d0d":"datatypes = X_categorical_df.dtypes\nencodings = {}\n\nfor col, dt in datatypes.iteritems():\n    if(str(dt) not in ['float64', 'int64']):\n        \n        #print(col, dt)\n        X_categorical_df[col] = X_categorical_df[col].astype(\"category\")\n        encodings[col] = X_categorical_df[col].cat.codes\n        X_categorical_df[col] = encodings[col]","8aef8725":"X_categorical_df\n","778147f6":"X_final_2 = X_numerical.join(X_categorical_df)","83889302":"X_final_2","38f03635":"X_final_2['YearRemodAdd'] = X_final_2['YearRemodAdd'] - X_final_2['YearBuilt']","03e71d20":"standardize = preprocessing.StandardScaler().fit(X_final_2)\n#transform\nX_final_2 = standardize.transform(X_final_2)\nX_train_2, X_val_2, y_train_2, y_val_2 = train_test_split(X_final_2, y, test_size=0.2, random_state=1)","4eb139a9":"X_train_2","70614dbe":"#Random forest regressor\nfor i in range(50 , 500, 50):\n    random_forest = RandomForestRegressor(n_estimators=i)\n    random_forest.fit(X_train_2, y_train_2)\n    predictions = random_forest.predict(X_val_2)\n\n    r_squared = r2_score(predictions, y_val_2)\n\n    print(\"R2 Score:\", r_squared)\n    method.append('Random Forest Regressor')\n    rmsle = np.sqrt(mean_squared_log_error(predictions, y_val_2))\n    print(\"RMSLE:\", rmsle)\n    perf.append(rmsle)","312e0eb5":"# xgboost\nfrom xgboost import XGBRegressor\nxgb = XGBRegressor(n_estimators=1000, learning_rate=0.01)\nxgb.fit(X_train_2, y_train_2)\npredictions = xgb.predict(X_val_2)\n\nr_squared = r2_score(predictions, y_val_2)\n\nprint(\"R2 Score:\", r_squared)\nmethod.append('XGBoost Regressor')\nrmsle = np.sqrt(mean_squared_log_error(predictions, y_val))\nprint(\"RMSLE:\", rmsle)\nperf.append(rmsle)","75414868":"#pip install lazypredict","f50232e7":"'''\nfrom lazypredict.Supervised import LazyRegressor\nreg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\nmodels, predictions = reg.fit(X_train, X_val, y_train, y_val)\nprint(models)'''","2b9575b4":"'''                               Adjusted R-Squared  R-Squared       RMSE  \\\nModel                                                                     \nGradientBoostingRegressor                    0.89       0.91   25630.04   \nBaggingRegressor                             0.89       0.90   26123.02   \nRandomForestRegressor                        0.89       0.90   26324.15   \nPoissonRegressor                             0.87       0.89   28297.38   \nExtraTreesRegressor                          0.86       0.88   29108.04   \nLGBMRegressor                                0.86       0.88   29311.39   \nHistGradientBoostingRegressor                0.86       0.88   29370.53   \nXGBRegressor                                 0.86       0.87   30029.86   \nAdaBoostRegressor                            0.83       0.85   32620.73   \nLassoCV                                      0.81       0.84   34059.91   \nLassoLarsCV                                  0.81       0.84   34064.17   \nLarsCV                                       0.81       0.84   34094.51   \nLassoLarsIC                                  0.81       0.84   34171.60   \nLassoLars                                    0.81       0.83   34353.80   \nLars                                         0.81       0.83   34356.05   \nHuberRegressor                               0.81       0.83   34389.33   \nTransformedTargetRegressor                   0.81       0.83   34394.09   \nLinearRegression                             0.81       0.83   34394.09   \nLasso                                        0.81       0.83   34398.37   \nRidge                                        0.81       0.83   34403.95   \nRidgeCV                                      0.81       0.83   34449.75   \nPassiveAggressiveRegressor                   0.81       0.83   34450.98   \nBayesianRidge                                0.81       0.83   34521.54   \nOrthogonalMatchingPursuitCV                  0.80       0.83   34962.43   \nRANSACRegressor                              0.80       0.83   35026.20   \nOrthogonalMatchingPursuit                    0.78       0.81   36603.98   \nGammaRegressor                               0.78       0.80   37373.70   \nElasticNet                                   0.77       0.80   38043.20   \nDecisionTreeRegressor                        0.75       0.78   39247.87   \nKNeighborsRegressor                          0.72       0.76   41542.03   \nTweedieRegressor                             0.72       0.76   41548.79   \nGeneralizedLinearRegressor                   0.72       0.76   41548.79   \nExtraTreeRegressor                           0.62       0.67   48387.85   \nElasticNetCV                                -0.05       0.09   80708.65   \nSGDRegressor                                -0.05       0.08   80970.68   \nNuSVR                                       -0.15      -0.00   84465.81   \nDummyRegressor                              -0.15      -0.01   84695.70   \nSVR                                         -0.16      -0.02   85098.78   \nKernelRidge                                 -4.55      -3.85  185944.47   \nMLPRegressor                                -5.02      -4.25  193564.51   \nLinearSVR                                   -5.04      -4.27  193955.44   \nGaussianProcessRegressor                 -2305.38   -2012.13 3789135.85   \n'''","b9f10af7":"## Handling missing values in test data","4b909f74":"# Normalizing the data","0bc6086c":"## There is not much change in the performance when we change the categorical labelling method.","9fb49838":"## Split training data into training and validation","7e55ace0":"# Regression Using Machine Learning ","836c0632":"# Alternatively: Using LazyRegressor to compare all the models\nTrying LazyRegressor Library to compare performance of different regression models","6f9b088c":"# Feature Engineering","48d9f4dd":"## Remove the feautures which are highly correlated with each other","2f883018":"# Encoding Categorical data","46e2c75c":"## Split input and target variables","17195d3a":"# Effect of Label Encondings for Categorical Features","75e03b7e":"# Exploratory Data Analysis","3c015af7":"# Testing","865c02ae":"## Load the data","4b43011c":"## Modify 'YearRemodAdd' feature - make it more informative","6e9310e4":"We used one-hot encoding for categorical features. Lets see the effect on performance for label encodings which can be used for Random Forest, XGBoost","4bdfce00":"## Choose only the significant features, discard those with correlation score < 0.5 with the target variable","1b511ed6":"# Handling missing data"}}