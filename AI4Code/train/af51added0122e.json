{"cell_type":{"24f25cd8":"code","46db8c1a":"code","525a7646":"code","575d1071":"code","4e9c860d":"code","2590a3a7":"code","64219328":"code","52c05604":"code","63931742":"code","6d389585":"code","2de18983":"code","77d02adf":"code","de3e0a36":"code","e0819e62":"code","45efa1e0":"code","406dd063":"code","77730c1e":"code","00bb7bb0":"code","5f441b83":"code","52017b9d":"code","432559b0":"code","e9ace563":"code","53528147":"code","ccd6aa13":"code","e2129e3f":"code","da5fba6f":"code","18764f13":"code","b5957f87":"code","c7ed074c":"code","14b333fd":"code","cc557a75":"code","51c870b7":"code","e574bae1":"markdown","9605b552":"markdown","22f345b8":"markdown","8028263d":"markdown","12c23246":"markdown","f6f7fd59":"markdown","df9a83c9":"markdown","1a515865":"markdown","8f8d6160":"markdown","980397f1":"markdown","98ef900d":"markdown","5f586579":"markdown","d19b337a":"markdown","1d7b9789":"markdown","0b4c9339":"markdown","cef67570":"markdown","51a3af62":"markdown","3faf7a2e":"markdown","bb637c33":"markdown","323be71a":"markdown","237e02b7":"markdown"},"source":{"24f25cd8":"import warnings\nwarnings.filterwarnings(\"ignore\")","46db8c1a":"import os\nimport numpy as np\nimport pandas as pd \nimport tensorflow as tf\nimport math\nimport random\nimport shutil\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport keras.backend as K\n\nfrom PIL import Image\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom keras.utils import np_utils\n\nfrom sklearn.model_selection import train_test_split\nfrom IPython.display import display, Markdown, Latex","525a7646":"# matplotlib\nplt.rc('font', size=15)\nplt.rc('axes', titlesize=18)  \nplt.rc('xtick', labelsize=10)  \nplt.rc('ytick', labelsize=10)\n\n# seaborn\nsns.set(font_scale = 1.2)\nsns.set_style(\"whitegrid\")\n\n# Tensorflow\/Keras\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","575d1071":"class Cfg:\n    RANDOM_STATE = 2021\n    TRAIN_DATA = '..\/input\/petfinder-pawpularity-score\/train.csv'\n    TEST_DATA = '..\/input\/petfinder-pawpularity-score\/test.csv'\n    SUBMISSION = '..\/input\/petfinder-pawpularity-score\/sample_submission.csv'    \n    IMG_FOLDER = '..\/input\/petfinder-pawpularity-score\/train'\n    IMG_TEST_FOLDER = '..\/input\/petfinder-pawpularity-score\/test'\n    IMG_RESIZE_FOLDER = '.\/resized'\n    SUBMISSION_FILE = '.\/submission.csv'\n    \n    SAMPLE_FRAC = 1\n    NUM_EPOCHS = 10\n    LEARNING_RATE = 0.0001\n    TEST_SIZE = 0.3\n    BATCH_SIZE = 64\n    IMG_SIZE = 128\n    \n    INDEX = 'Id'\n    TARGET = 'Pawpularity'\n    FEATURES = [\n        'Subject Focus', \n        'Eyes', \n        'Face', \n        'Near', \n        'Action', \n        'Accessory', \n        'Group', \n        'Collage', \n        'Human', \n        'Occlusion', \n        'Info', \n        'Blur'\n    ]","4e9c860d":"# create folder for resized images\nif not os.path.isdir(Cfg.IMG_RESIZE_FOLDER):\n    os.makedirs(Cfg.IMG_RESIZE_FOLDER)","2590a3a7":"def read_data(\n    train_file:str=Cfg.TRAIN_DATA, \n    test_file:str=Cfg.TEST_DATA\n) -> (pd.DataFrame, pd.DataFrame):\n    \"\"\"Reads the csv files `train.csv` and `test.csv` and returns \n       them as pandas data frames.\n    \"\"\"\n    # read csv files\n    train_df = pd.read_csv(Cfg.TRAIN_DATA, index_col=Cfg.INDEX)\n    test_df = pd.read_csv(Cfg.TEST_DATA, index_col=Cfg.INDEX)\n\n    return train_df, test_df\n\n\ntrain_df, test_df = read_data()","64219328":"train_df","52c05604":"test_df","63931742":"train_df.describe().drop('count')","6d389585":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n\nsns.histplot(\n    data=train_df,\n    x=Cfg.TARGET,\n    bins=30,\n    legend=True,\n    kde=True,\n    ax=ax[0])\n\nax[0].set_title('Target Distribution')\n\nax[0].set_xlabel('Pawpularity')\nax[0].set_ylabel('Count')\n\nsns.boxplot(\n    data=train_df,\n    y=Cfg.TARGET,\n    ax=ax[1]\n)\n\nplt.show()","2de18983":"fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(20, 15))\n\nfor f, ax in zip(Cfg.FEATURES, axes.flatten()):\n    sns.histplot(\n        data=train_df,\n        x=Cfg.TARGET,\n        bins=30,\n        hue=f,\n        legend=True,\n        kde=True,\n        ax=ax,\n        alpha=0.3)\n\nfig.tight_layout()    \nplt.show()","77d02adf":"fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(20, 15))\n\nfor f, ax in zip(Cfg.FEATURES, axes.flatten()):\n    sns.countplot(\n        data=train_df,\n        x=f,\n        alpha=0.8,\n        ax=ax)\n\nfig.tight_layout()\nplt.show()","de3e0a36":"corr_df = train_df.corr()\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nmask = np.triu(np.ones_like(corr_df, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(\n    corr_df, \n    mask=mask, \n    cmap=cmap, \n    vmin = -0.75, \n    vmax = 0.75,\n    center=0,\n    square=True,\n    annot = True,\n    fmt=\"0.0\",\n    linewidths=.5)\n\nfig.tight_layout()\nplt.show()","e0819e62":"def get_image(image_id, image_folger=Cfg.IMG_FOLDER, data=train_df, resize=True):\n    resized_path = os.path.join(Cfg.IMG_RESIZE_FOLDER, '{}.jpg'.format(image_id))\n    if os.path.isfile(resized_path):\n        img = Image.open(resized_path)\n        return img\n    \n    img_path = os.path.join(image_folger, '{}.jpg'.format(image_id))\n    \n    img = Image.open(img_path)\n    img = img.resize((Cfg.IMG_SIZE, Cfg.IMG_SIZE))\n    img.save(resized_path)\n        \n    return img","45efa1e0":"def plot_images(data, nrows=5, ncols=5, figsize=(15, 15)):\n    \"\"\"\n    \"\"\"\n    indices = data.sample(nrows * ncols).index\n    \n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n    for index, ax in zip(indices, axes.flatten()):\n        img = get_image(index)\n        ax.imshow(img)\n\n        ax.set_title(train_df.loc[index][Cfg.TARGET], fontsize=18)\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n\n    fig.tight_layout()\n    plt.show()","406dd063":"pawpularity_range = [0, 20, 40, 60, 80, 100]\nquery = lambda i: '{} < Pawpularity and Pawpularity <= {}'.format(pawpularity_range[i], pawpularity_range[i+1])\n\nfor i in range(0, 5):\n    display(Markdown('### Pawpularity `{}` - `{}`'\n        .format(pawpularity_range[i], pawpularity_range[i+1])))\n    \n    df = train_df.query(query(i))\n    plot_images(df, nrows=1, ncols=5, figsize=(15, 5))","77730c1e":"class DataGenerator(Sequence):\n    \"\"\"\n    \"\"\"\n    def __init__(\n        self, \n        data, \n        target=None, \n        img_folder=Cfg.IMG_FOLDER, \n        batch_size=Cfg.BATCH_SIZE\n    ):\n        self.data = data\n        self.target = target\n        self.batch_size = batch_size\n        self.img_folder = img_folder\n    \n    def __len__(self):\n        return math.ceil(len(self.data) \/ self.batch_size)\n    \n    def __getitem__(self, idx):\n        start_idx = idx * self.batch_size\n        end_idx = (idx + 1) * self.batch_size\n        ids = self.data[start_idx : end_idx].index.values\n        \n        images = np.array([np.array(get_image(id, self.img_folder)) for id in ids])\n        meta = np.array(self.data[start_idx : end_idx][Cfg.FEATURES]).astype(np.float32)\n        \n        if self.target is None or not self.target.any():\n            return [images, meta]\n        \n        target = np.array(self.target[start_idx : end_idx]).astype(np.float32)\n        return [images, meta], target","00bb7bb0":"def get_image_model(img_size=Cfg.IMG_SIZE, n_channel=3):\n    \"\"\"\n    \"\"\"\n    inputs = layers.Input((img_size, img_size, n_channel))\n    x = inputs\n    \n    x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    \n    x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    \n    x = layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu')(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = layers.Conv2D(filters=256, kernel_size=(3, 3), activation='relu')(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    \n    x = layers.Flatten()(x)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n    \n    outputs = x\n    model = keras.Model(\n        inputs=inputs, \n        outputs=outputs, \n        name='image_cnn_model')\n\n    return model    ","5f441b83":"image_model = get_image_model()\nimage_model.summary()","52017b9d":"def get_meta_model(n_meta_features=12):\n    \"\"\"\n    \"\"\"\n    inputs = layers.Input(shape=((n_meta_features, )))\n    x = inputs\n    \n    x = layers.Dense(12, activation='relu')(x) \n    x = layers.Dense(24, activation='relu')(x)\n    x = layers.Dense(12, activation='relu')(x) \n    \n    outputs = x\n    model = keras.Model(\n        inputs=inputs, \n        outputs=outputs, \n        name='meta_nn_model')\n\n    return model","432559b0":"meta_model = get_meta_model()\nmeta_model.summary()","e9ace563":"def get_model(image_model, meta_model):\n    \"\"\"\n    \"\"\"\n    x = layers.Concatenate(axis=1)([image_model.output, meta_model.output])\n    x = layers.Dense(1, activation='linear')(x)\n    output = x\n\n    model = keras.Model(inputs=[image_model.input, meta_model.input], outputs=output)\n    return model","53528147":"model = get_model(image_model, meta_model)\nmodel.summary()","ccd6aa13":"data = train_df.sample(frac=Cfg.SAMPLE_FRAC)\n\n# Spit data into train and validation data sets\nX_train, X_val, y_train, y_val = train_test_split(\n    data[Cfg.FEATURES],\n    data[Cfg.TARGET],\n    test_size=Cfg.TEST_SIZE, \n    random_state=Cfg.RANDOM_STATE\n)\n\ntrain_generator = DataGenerator(X_train, y_train)\nval_generator = DataGenerator(X_val, y_val)","e2129e3f":"# Compile model\nmodel.compile(\n    optimizer=keras.optimizers.Adam(\n        learning_rate=Cfg.LEARNING_RATE,\n    ), \n    loss = keras.losses.MeanSquaredError(),\n    metrics=[ \n        keras.metrics.RootMeanSquaredError(name='rmse')\n    ]\n)\n    \ncallbacks = [\n    keras.callbacks.EarlyStopping(\n        monitor='loss', \n        patience=3)\n]","da5fba6f":"%%time\n\nresult = model.fit(\n    train_generator,\n    epochs=Cfg.NUM_EPOCHS,\n    validation_data=val_generator,\n    callbacks=callbacks\n)","18764f13":"y_pred = model.predict(val_generator).reshape(-1)\n\ndf = pd.DataFrame({'y_pred': y_pred, 'y_val': y_val})\ndf['error'] = np.abs(y_pred - y_val)","b5957f87":"fig, ax = plt.subplots(1, 4, figsize=(23, 5))\n\n# plot model rmse\nax[0].plot(result.history['rmse'])\nax[0].plot(result.history['val_rmse'])\n\nax[0].set_title('Model RMSE')\nax[0].set_ylabel('RMSE')\nax[0].set_xlabel('Epoch')\nax[0].legend(['train', 'val'], loc='upper right')\n\n# plot regression \nsns.regplot(\n    data=df,\n    x='y_val',\n    y='y_pred',\n    ax=ax[1],\n    x_estimator=np.mean, \n    x_bins=30\n)\n\nax[1].set_title('Regression True vs. Pred')\nax[1].set_ylabel('Pred (val)')\nax[1].set_xlabel('True (val)')\n\nsns.histplot(\n    data=df,\n    x='y_pred',\n    bins=30,\n    legend=True,\n    kde=True,\n    ax=ax[2])\n\nax[2].set_title('Pred Target Distribution')\n\nax[2].set_xlabel('Pred Pawpularity')\nax[2].set_ylabel('Count')\n\n# plot residuals\nsns.scatterplot(data=df, x='y_pred', y='error', ax=ax[3])\nax[3].set_title('Residuals')\n\nax[3].set_xlabel('Prediction')\nax[3].set_ylabel('Absolute error')\n\nplt.tight_layout()\nplt.show()","c7ed074c":"test_generator = DataGenerator(test_df, img_folder=Cfg.IMG_TEST_FOLDER)\ny_pred_submission = model.predict(test_generator).reshape(-1)","14b333fd":"submission_df = pd.DataFrame({\n    Cfg.INDEX: test_df.index,\n    Cfg.TARGET: y_pred_submission,\n}).set_index(Cfg.INDEX)\n\nsubmission_df","cc557a75":"# save submission file\nsubmission_df.to_csv(Cfg.SUBMISSION_FILE)","51c870b7":"# cleanup\nshutil.rmtree(Cfg.IMG_RESIZE_FOLDER)","e574bae1":"# Model","9605b552":"## Model","22f345b8":"## Target `Pawpularity`","8028263d":"# Visualization","12c23246":"# Train","f6f7fd59":"## Meta data features","df9a83c9":"## Correletaion","1a515865":"## Image CNN model","8f8d6160":"## Files\n\n*   `train\/` -    Folder containing training set photos of the form {id}.jpg, where {id} is a unique Pet Profile ID.\n*   `train.csv` - Metadata (described below) for each photo in the training set as well as the target, the photo's Pawpularity score. The `Id` column gives the photo's unique Pet Profile ID corresponding the photo's file name.\n*    `test\/` - Folder containing randomly generated images in a format similar to the training set photos. The actual test data comprises about 6800 pet photos similar to the training set photos.\n*    `test.csv` - Randomly generated metadata similar to the training set metadata.\n*    `sample_submission.csv` - A sample submission file in the correct format.","980397f1":"# Exploratory data analysis","98ef900d":"# Submission","5f586579":"# Prediction","d19b337a":"##  Photo Metadata\n\nThe `train.csv` and `test.csv` files contain metadata for photos in the training set and test set, respectively. \nEach pet photo is labeled with the value of `1` (Yes) or `0` (No) for each of the following features:\n\n*    `Focus` - Pet stands out against uncluttered background, not too close \/ far.\n*    `Eyes` - Both eyes are facing front or near-front, with at least 1 eye \/ pupil decently clear.\n*    `Face` - Decently clear face, facing front or near-front.\n*    `Near` - Single pet taking up significant portion of photo (roughly over 50% of photo width or height).\n*    `Action` - Pet in the middle of an action (e.g., jumping).\n*    `Accessory` - Accompanying physical or digital accessory \/ prop (i.e. toy, digital sticker), excluding collar and leash.\n*    `Group` - More than 1 pet in the photo.\n*    `Collage` - Digitally-retouched photo (i.e. with digital photo frame, combination of multiple photos).\n*    `Human` - Human in the photo.\n*    `Occlusion` - Specific undesirable objects blocking part of the pet (i.e. human, cage or fence). Note that not all blocking objects are considered occlusion.\n*    `Info` - Custom-added text or labels (i.e. pet name, description).\n*    `Blur` - Noticeably out of focus or noisy, especially for the pet\u2019s eyes and face. For Blur entries, \u201cEyes\u201d column is always set to 0.\n","1d7b9789":"### Summary\n\n* The mean and std of the target are: $\\mu=38.039$ and $\\sigma=20.592$.\n* The distribution of the target shows that there are some outliers in the upper area.","0b4c9339":"# Setup","cef67570":"### Summary\n\n* The training set consists of 9912 rows and 13 columns. The target Variable is `Pawpularity`. \n* The test data set consists of 8 roes and the same columns as the training sets with the expection of the target variable.\n","51a3af62":"## Meta NN Model","3faf7a2e":"### Summary\n\n* There is no significant correlation between the features. ","bb637c33":"# Load and prepare data","323be71a":"# Overview","237e02b7":"# Data generator"}}