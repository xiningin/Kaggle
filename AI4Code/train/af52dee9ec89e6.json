{"cell_type":{"1e375800":"code","1146a6a5":"code","f06b99c3":"code","b4e77d9b":"code","c3eb91f8":"code","9fd0e25c":"code","15b3b34f":"code","4ff91271":"code","c2bf06ac":"code","784d5333":"code","04461be0":"code","66a45164":"markdown","79f123e8":"markdown","a7c3e0fa":"markdown","c7c21489":"markdown","59e314bd":"markdown"},"source":{"1e375800":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import MDS, TSNE\n\npd.set_option('max.rows',500)\npd.set_option('max.columns',80)","1146a6a5":"preprocessed_train = pd.read_csv('..\/input\/preprocessed-train-data\/preprocessed_train_data.csv')\nx_train, y_train = preprocessed_train[preprocessed_train.columns[:-1]], preprocessed_train[preprocessed_train.columns[-1]]\npreprocessed_train.head()","f06b99c3":"def PCA_FE(x_train,y_train, n):\n    \"\"\"PCA - Feature Extraction\"\"\"\n    pca = PCA(n_components= n)\n    column_list = []\n    for i in range(1,n+1):\n        column_list = column_list + ['pc'+str(i)]\n    x_train_pca = pd.DataFrame(pca.fit_transform(x_train,y_train),columns = column_list,index=x_train.index)\n    return x_train_pca, pca\n\n'''PCA with two dimensions'''\nx_train_pca, pca = PCA_FE(x_train, y_train, 2)\ndisplay(x_train_pca.head())\n\n''' PCA with three dimensions'''\nx_train_pca_3, pca = PCA_FE(x_train, y_train, 3)\ndisplay(x_train_pca_3.head())","b4e77d9b":"def plot_pca(x_train_pca):\n    '''2D data visualization for PCA'''\n    plt.figure(1, figsize=(10,10))\n    plt.title('Scatter Plot for PCA',fontsize = 15)\n    plt.xlabel('PC1',fontsize =12)\n    plt.ylabel('PC2',fontsize =12)\n    plt_obj = plt.scatter(x_train_pca['pc1'], x_train_pca['pc2'])\n    \nplot_pca(x_train_pca)","c3eb91f8":"'''3D data visualization for PCA'''\npx.scatter_3d(x_train_pca_3, x = 'pc1', y ='pc2', z= 'pc3')","9fd0e25c":"def MDS_FE(x_train, y_train, n):\n    \"\"\"MDS - Feature Extraction\"\"\"\n    mds = MDS(n_components = n)\n    column_list = []\n    for i in range(1,n+1):\n        column_list = column_list + ['component'+str(i)]\n    x_train_mds = pd.DataFrame(mds.fit_transform(x_train,y_train),columns = column_list,index=x_train.index)\n    return x_train_mds, mds\n\n'''MDS with two dimensions'''\nx_train_mds, mds = MDS_FE(x_train, y_train, 2)\ndisplay(x_train_mds.head())\n\n''' MDS with three dimensions'''\nx_train_mds_3, mds = MDS_FE(x_train, y_train, 3)\ndisplay(x_train_mds_3.head())","15b3b34f":"def plot_mds(x_train_mds):\n    '''2D data visualization for MDS'''\n    plt.figure(1, figsize=(10,10))\n    plt.title('Scatter Plot for MDS',fontsize = 15)\n    plt.xlabel('Component1',fontsize =12)\n    plt.ylabel('Component2',fontsize =12)\n    plt_obj = plt.scatter(x_train_mds['component1'], x_train_mds['component2'])\n\nplot_mds(x_train_mds)","4ff91271":"'''3D data visualization for MDS'''\npx.scatter_3d( x_train_mds_3, x='component1', y ='component2', z='component3')","c2bf06ac":"def TSNE_FE(x_train, y_train, n):\n    \"\"\"T-SNE - Feature Extraction\"\"\"\n    tsne = TSNE(n_components = n)\n    column_list = []\n    for i in range(1,n+1):\n        column_list = column_list + ['component'+str(i)]\n    x_train_tsne = pd.DataFrame(tsne.fit_transform(x_train,y_train),columns = column_list,index=x_train.index)\n    return x_train_tsne, tsne\n\n'''TSNE with two dimensions'''\nx_train_tsne, tsne = TSNE_FE(x_train, y_train, 2)\ndisplay(x_train_tsne.head())\n\n'''TSNE with three dimensions'''\nx_train_tsne_3, tsne = TSNE_FE(x_train, y_train, 3)\ndisplay(x_train_tsne_3.head())","784d5333":"def plot_tsne(x_train_tsne):\n    '''2D data visualization for MDS'''\n    plt.figure(1, figsize=(10,10))\n    plt.title('Scatter Plot for TSNE',fontsize = 15)\n    plt.xlabel('Component1',fontsize =12)\n    plt.ylabel('Component2',fontsize =12)\n    plt_obj = plt.scatter(x_train_tsne['component1'], x_train_tsne['component2'])\n\nplot_mds(x_train_tsne)","04461be0":"'''3D data visualization for TSNE'''\npx.scatter_3d( x_train_tsne_3, x='component1', y ='component2', z='component3')","66a45164":"# 2. Multi-Dimensional Scaling (MDS)\n\nMDS is similar to PCA. MDS is a **Linear Dimensionality Reduction** technique as well.\n* The eigen vectors are found for the Dissimilarity matrix as opposed to the covariance matrix in PCA\n\n## MDS algorithm:\n1. Compute dissimilarity matrix\n2. Find eigen vectors and eigen values for the dissimilarity matrix\n3. The eigen vector with high eigen value explains the information well that means the variance of the data is high\n4. Project the data points to the eigen vector.","79f123e8":"# 1. Principal Component Analysis (PCA)\n\nPCA is a **Linear Dimensionality Reduction** technique. \n* PCA uses eigen vectors of the covariance matrix to orthogonally project the data points to the eigen vector.\n\n**Eigen vector:** Eigen vector is a vector whose direction doesn't change even if a linear transformation is applied to it. \n\n## PCA Algorithm:\n1. Compute covariance matrix\n2. Find eigen vectors and eigen values for the covariance matrix\n3. The eigen vector with high eigen value explains the information well that means the variance of the data is high\n4. Project the data points to the eigen vector.\n","a7c3e0fa":"# Feature Extraction\n* Feature Extraction is one of the types in Dimensionality Reduction techniques. \n* Feature extraction attempts to reduce the features by transforming the existing features into required number of features. It doesn't discard the features as opposed to **Feature Selection.**\n* Refer my notebook to know more about **Feature Selection Techniques**\n\n[Notebook Link](https:\/\/www.kaggle.com\/srivignesh\/feature-selection-techniques)\n\n## Feature Extraction techniques\n1. Principal Component Analysis (PCA)\n2. Multi-Dimensional Scaling (MDS)\n3. T -distributed Stochastic Neighbor Embedding (T-SNE )\n4. Locally Linear Embedding (LLE)\n\n**Advantages:** Feature Extraction Techniques are used for data visualization purposes and to prevent overfitting.\n\n**Disadvantage:** Feature Extraction leads to Information loss.\n","c7c21489":"# Load the preprocessed data\nThe training data has been preprocessed already. The preprocessing steps involved are,\n\n* MICE Imputation\n* Log transformation\n* Square root transformation\n* Ordinal Encoding\n* Target Encoding\n* Z-Score Normalization\n\nFor detailed implementation of the above mentioned steps refer my notebook on data preprocessing:\n\n[Notebook Link](https:\/\/www.kaggle.com\/srivignesh\/data-preprocessing-for-house-price-prediction)","59e314bd":"# 3. T-distributed Stochastic Neighbor Embedding (TSNE)\nTSNE is an algorithm that identifies the non-linear relationships in the data. It is a tool to visualize data.\n\nT-SNE differs from PCA by preserving only small pairwise distances or local similarities whereas PCA is concerned with preserving large pairwise distances to maximize variance.\n\n* First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a very low probability. \n* Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback\u2013Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map. While the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this can be changed as appropriate.\n\nKL-Divergence is also called relative entropy which is a measure of how two distributions vary.\n\n**Reference:** https:\/\/en.wikipedia.org\/wiki\/T-distributed_stochastic_neighbor_embedding"}}