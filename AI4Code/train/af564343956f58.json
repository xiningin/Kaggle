{"cell_type":{"d2ac6922":"code","bb86ae9a":"code","dba53a4b":"code","3df7ef08":"code","58ba6f2d":"code","13ef1b24":"code","2b00f400":"code","871ca3a1":"code","d1f8c7b6":"code","4d5c5605":"code","407b0e9b":"code","33493491":"code","0377f444":"code","926f2c9a":"code","05f24e11":"code","856becfe":"code","c76dcdb6":"code","fc49104b":"code","3f5f6f3c":"code","9f6752b8":"code","c76acfdf":"code","23d6765a":"code","4a275d8c":"code","18672a25":"code","bb17f433":"code","02bd3200":"code","f8cab3a7":"code","295096bf":"code","c8e89d19":"code","3662b13b":"code","e403fe63":"code","fde345bf":"code","a33fe5fd":"code","48194033":"code","7618fb4d":"code","e09c8047":"code","464c4b1e":"code","4b6206a1":"code","873639d5":"code","9a75a29a":"code","af4d5563":"code","b3c18e2e":"code","4cc2ff01":"code","d269a914":"code","1c443e2b":"code","41eb2da8":"code","3f930d57":"code","41373783":"code","35475f9f":"code","fadefbad":"code","b3fa48d4":"code","9578407d":"code","a8b34ba5":"code","0ad767f3":"code","51000ebd":"code","28f23c59":"code","83a6c398":"code","de5640fa":"code","f91314c3":"code","09f1d0d2":"code","48dc5804":"code","f4e56cb0":"code","79b652c4":"code","d90c93ec":"code","9b83cce6":"code","b6ad956c":"code","60541b6a":"code","9be626f8":"code","5e9f753e":"code","aaeed8fb":"code","af0cdf29":"code","7c67c28d":"markdown","b5853ff0":"markdown","652bba31":"markdown","4a8d6357":"markdown","c7eaefe1":"markdown","d77dd2b6":"markdown","68589f99":"markdown","9683e6c4":"markdown","43f6f4d1":"markdown","44dbdc9a":"markdown","d344a9fe":"markdown","c724ffc7":"markdown","7e88d352":"markdown","68a80d18":"markdown","96a18c52":"markdown","f6043349":"markdown","f4aeb704":"markdown","9f6f0ddd":"markdown","206bd987":"markdown","1b525941":"markdown","80187afe":"markdown","647d769f":"markdown","d1b81da7":"markdown","7a4a88cc":"markdown","f8d8b1ff":"markdown","1964fc41":"markdown","a94afc58":"markdown","b3c82712":"markdown","c7603b98":"markdown","9141ba1e":"markdown","012074c3":"markdown","e692253b":"markdown","407b70a2":"markdown","c00b5693":"markdown","109aa4d4":"markdown","e91d16bf":"markdown","d66dedb2":"markdown","9355871f":"markdown","6e394d4e":"markdown","6cb933c1":"markdown","b7be074a":"markdown","c8ca399a":"markdown","4e04f8e9":"markdown","228117f7":"markdown","0ed33c73":"markdown","bf6a9e65":"markdown","651d41c5":"markdown","ebbd6030":"markdown"},"source":{"d2ac6922":"# Machine Learning Libraries#\nimport pandas as pd\nimport numpy as np\nimport warnings\n\nfrom sklearn import tree\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics, linear_model\nfrom sklearn.metrics import accuracy_score, confusion_matrix,r2_score, mean_squared_error\n\n\n\nfrom sklearn.decomposition import PCA\n\nfrom scipy.spatial.distance import pdist, squareform\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix \n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import tree\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.impute import SimpleImputer  #Remplazar valores null\n\n\nimport warnings","bb86ae9a":"#Importamos el dataset\n\ndf = pd.read_csv('\/kaggle\/input\/productivity-prediction-of-garment-employees\/garments_worker_productivity.csv')\ndf.head()\n\n","dba53a4b":"df.info()","3df7ef08":"print(df.shape)\ndf.describe()","58ba6f2d":"df.isnull().sum()\n#Encontramos 506 valores nulos en la columna wip","13ef1b24":"#Pasamos a eliminar los datos nulos\ndf.sample(10)\n#Observamos que en la estructura de los datos si tienen Header","2b00f400":"df.dtypes","871ca3a1":"imp = SimpleImputer(missing_values = np.nan, strategy = 'wip')\n\ndf","d1f8c7b6":"df['team'].unique()  #Sacamos un listado de los items contenidos en esta columna, con el fin de evaluar su posible distribuci\u00f3n\n\n","4d5c5605":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","407b0e9b":"#dropear quarter y day para \ndf2 = df.drop(['quarter','day'], axis = 1)\n#\ndf2['team'] = df2['team'].apply(lambda x:str(x))\n\n#Department\n#df2['department'] = df['department'].apply(lambda x: 'finishing' if x == ('finishing ' or 'finishing' ) else 'sewing' )\n\n#Tomar date extraigo el dia y el mes, despues lo dropeo\ndf2['day'] = df2['date'].apply(lambda x:int(x.split(\"\/\")[0]))\ndf2['month'] = df2['date'].apply(lambda x:int(x.split(\"\/\")[1]))\n\n\ndf2 = df2.drop(['date'], axis = 1)\ndf2.head()","33493491":"df2 = pd.get_dummies(df2)","0377f444":"df2.sample(5)","926f2c9a":"from sklearn.impute import KNNImputer  # Con esto imputamos los valores null utilizando el algoritmo de knn \"El mas proximo\"\n\nkimp = KNNImputer(n_neighbors = 3)   #\ndf2_clean = kimp.fit_transform(df2)\n","05f24e11":"df2_clean = pd.DataFrame(df2_clean, columns = df2.columns)","856becfe":"df2_clean.isnull().sum()","c76dcdb6":"correlation_matrix = df2_clean.corr().round(2)\n\nfig, ax = plt.subplots(figsize = (15,10))\nsns.heatmap(data = correlation_matrix, annot = True, ax = ax, cmap = 'coolwarm')","fc49104b":"sns.pairplot(df2_clean,diag_kind='kde');","3f5f6f3c":"plt.scatter(df2_clean['no_of_workers'], df2_clean['smv'], label = 'Cluster 1');\nplt.scatter(df2_clean['no_of_workers'], df['team'], label = 'Cluster 2', marker = 'x');\nplt.scatter(df2_clean['no_of_workers'], df2_clean['over_time'], label = 'Cluster 3');\nplt.legend()\n","9f6752b8":"plt.scatter(df2_clean['no_of_workers'], df2_clean['over_time'], label = 'Cluster 3');\nplt.legend()","c76acfdf":"sns.set_style('darkgrid')\nsns.displot(df2_clean['actual_productivity'])","23d6765a":"df2_clean.actual_productivity.hist()","4a275d8c":"df2_clean.skew().sort_values(ascending=True)","18672a25":"from scipy.stats.mstats import normaltest\nnormaltest(df.actual_productivity.values)","bb17f433":"log_price = np.log(df.actual_productivity)\nlog_price.hist()","02bd3200":"plt.figure(figsize=(5,5))\nsns.boxplot(data=df2_clean,y='actual_productivity')","f8cab3a7":"import seaborn as sns\nfig, ax = plt.subplots(1,1,figsize = (10,3))\nsns.distplot(df2_clean['no_of_workers'], bins = 40, ax = ax)\nplt.show()","295096bf":"import seaborn as sns\nfig, ax = plt.subplots(1,1,figsize = (10,3))\nsns.distplot(df2_clean['over_time'], bins = 40, ax = ax)\nplt.show()","c8e89d19":"plt.figure(figsize = (20, 5))\n\nX = ['no_of_workers','smv']\nY = df2_clean['actual_productivity']\n\nfor i, col in enumerate(X):\n    plt.subplot(1, len(X) , i + 1)\n    x = df2_clean[col]\n    y = Y\n    plt.scatter(x, y, marker = 'o')\n    plt.title(\"actual_productivity en funci\u00f3n de \" + col)\n    plt.xlabel(col)\n    plt.ylabel('actual_productivity')\n    \n    \n","3662b13b":"X = df2_clean.drop(['actual_productivity'], axis = 1)\nY = df2_clean['actual_productivity']","e403fe63":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.4, random_state = 123)\n\nprint(X_train.shape), print(X_test.shape), print(Y_train.shape), print(Y_test.shape);","fde345bf":"from sklearn.decomposition import PCA\nfrom sklearn import preprocessing\n\nX = df2_clean.drop(['actual_productivity'], axis = 1)\npca = PCA()","a33fe5fd":"PCs = preprocessing.StandardScaler().fit_transform(pca.fit_transform(X))\n\n['PC' + str(i) for i in range(1, X.shape[1] + 1)]","48194033":"print(np.cumsum(pca.explained_variance_ratio_))\nfig, ax = plt.subplots(figsize = (20, 5))\nax.bar(x = range(1, X.shape[1] + 1), \n        height = np.round(pca.explained_variance_ratio_ * 100, decimals = 1), \n        tick_label = ['PC' + str(i) for i in range(1, X.shape[1] + 1)]);","7618fb4d":"X_pca = pd.DataFrame(PCs, \n                     columns = ['PC' + str(i) for i in range(1, X.shape[1] + 1)])\nX_pca.head()","e09c8047":"df.columns","464c4b1e":"fig, ax = plt.subplots(2,2, figsize = (15,7))\n\nim1 = ax[0,0].scatter(X_pca['PC1'], X_pca['PC2'], c = df['no_of_workers']) \nfig.colorbar(im1, ax = ax[0,0])\nim2 = ax[0,1].scatter(X_pca['PC1'], X_pca['PC2'], c = df['actual_productivity'])\nfig.colorbar(im2, ax = ax[0,1])\nim3 = ax[1,0].scatter(X_pca['PC1'], X_pca['PC2'], c = df['over_time'])\nfig.colorbar(im3, ax = ax[1,0])\nim4 = ax[1,1].scatter(X_pca['PC1'], X_pca['PC2'], c = df['smv'])\nfig.colorbar(im4, ax = ax[1,1])\n\nfig.colorbar(im4, ax = ax[1,1])\n \nax[0,0].set_ylabel(\"PC2\") \nax[0,1].set_ylabel(\"PC2\")\nax[1,0].set_xlabel(\"PC1\"), ax[1,0].set_ylabel(\"PC2\")\nax[1,1].set_xlabel(\"PC1\"), ax[1,1].set_ylabel(\"PC2\")\n\nax[0,0].set_title(\"Profiling Number of workers\")\nax[0,1].set_title(\"Profiling Actual Productivity\")\nax[1,0].set_title(\"Profiling Over Time\")\nax[1,1].set_title(\"Profiling index of Standard Minute Value\");","4b6206a1":"fig, ax = plt.subplots(1,1, figsize = (7,4))\n\nim1 = ax.scatter(X_pca['PC1'], X_pca['PC2'], c = df['targeted_productivity'], marker = 'o') \nfig.colorbar(im1, ax = ax)\n\nax.set_xlabel(\"PC1\"), ax.set_ylabel(\"PC2\")\nax.set_title(\"Profiling target productivity\");","873639d5":"correlation_matrix = X_pca.corr().round(2)\n\nfig, ax = plt.subplots(figsize = (15,10))\nsns.heatmap(data = correlation_matrix, annot = True, ax = ax, cmap = 'coolwarm');","9a75a29a":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nmodelo_lineal = LinearRegression()\nmodelo_lineal.fit(X_train, Y_train);","af4d5563":"modelo_lineal.coef_","b3c18e2e":"prediccion_train_lineal_simple = modelo_lineal.predict(X_train)\nmean_squared_error(Y_train, prediccion_train_lineal_simple) ","4cc2ff01":"prediccion_test_lineal_simple = modelo_lineal.predict(X_test)\nmean_squared_error(Y_test, prediccion_test_lineal_simple) ","d269a914":"from sklearn.model_selection import cross_val_score\n\nresultados = []\nnombres = []\n\nscores = -cross_val_score(modelo_lineal, X_train, Y_train, cv = 5, scoring = 'neg_mean_squared_error')\nprint(scores.mean())\n\nresultados.append(scores)\nnombres.append(\"Lineal simple\")","1c443e2b":"from sklearn.model_selection import learning_curve\n\nfig, ax = plt.subplots(1, 1, figsize = (10, 5))\ntrain_sizes, train_scores, test_scores = learning_curve(\n                                                estimator = LinearRegression(),\n                                                X = X_train,\n                                                y = Y_train, \n                                                train_sizes = np.linspace(0.1, 1.0, 100), \n                                                cv = 3,\n                                                scoring = 'neg_mean_squared_error')\ntrain_scores *= -1 \ntest_scores *= -1\n\ntrain_mean = np.mean(train_scores, axis = 1)\ntrain_std = np.std(train_scores, axis = 1)\n\ntest_mean = np.mean(test_scores, axis = 1)\ntest_std = np.std(test_scores, axis = 1)\n\nax.plot(train_sizes, np.mean(train_scores, 1), color = 'blue', label = 'training score')\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color = \"#DDDDDD\", alpha = 0.2)\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color = \"#DDDD00\", alpha = 0.2)\n\nax.plot(train_sizes, np.mean(test_scores, 1), color = 'red', label = 'validation score')\nax.hlines(np.mean([train_scores[-1], test_scores[-1]]), train_sizes[0], train_sizes[-1],\n                 color = 'gray', linestyle = 'dashed')\n\n#ax.set_ylim(0, 1)\nax.set_xlim(train_sizes[0], train_sizes[-1])\nax.set_xlabel('training size')\nax.set_ylabel('score')\nax.set_title('Learning curves', size = 14)\nax.legend(loc = 'best')","41eb2da8":"from sklearn.linear_model import Ridge\n\nmodelo_ridge = Ridge(alpha = 1.0)\nmodelo_ridge.fit(X_train, Y_train)\n\nprediccion_train = modelo_ridge.predict(X_train)\nprint(mean_squared_error(Y_train, prediccion_train)) \n\nprediccion_test = modelo_ridge.predict(X_test)\nprint(mean_squared_error(Y_test, prediccion_test))","3f930d57":"scores = -cross_val_score(modelo_ridge, X_train, Y_train, cv = 5, scoring = 'neg_mean_squared_error')\nprint(scores.mean())\n\nresultados.append(scores)\nnombres.append(\"Lineal ridge\")","41373783":"\nscaler = preprocessing.StandardScaler()\nX_train_s = scaler.fit(X_train).transform(X_train)\nX_test_s = scaler.fit(X_test).transform(X_test)\n\nmodelo_ridge = Ridge(alpha = 1.0)\nmodelo_ridge.fit(X_train_s, Y_train)\n\n\nprediccion_train = modelo_ridge.predict(X_train_s)\nprint(mean_squared_error(Y_train, prediccion_train)) # root mean squared error\n\nprediccion_test = modelo_ridge.predict(X_test_s)\nprint(mean_squared_error(Y_test, prediccion_test))\n\nscores = -cross_val_score(modelo_ridge, X_train_s, Y_train, cv = 5, scoring = 'neg_mean_squared_error')\nprint(scores.mean())\n\nresultados.append(scores)\nnombres.append(\"Lineal ridge std.\")","35475f9f":"np.logspace(-6, 6, 13)","fadefbad":"modelo_ridge = linear_model.RidgeCV(alphas = np.logspace(-6, 6, 13), cv = 5)\nmodelo_ridge.fit(X_train_s, Y_train)\n\nmodelo_ridge.alpha_\n","b3fa48d4":"prediccion_train = modelo_ridge.predict(X_train_s)\nprint(mean_squared_error(Y_train, prediccion_train))\n\nprediccion_test = modelo_ridge.predict(X_test_s)\nprint(mean_squared_error(Y_test, prediccion_test))","9578407d":"modelo_lasso = linear_model.Lasso(alpha = 0.1)\nmodelo_lasso.fit(X_train_s, Y_train)\n\nprediccion_train = modelo_lasso.predict(X_train_s)\nprint(mean_squared_error(Y_train, prediccion_train))\n\nprediccion_test = modelo_lasso.predict(X_test_s)\nprint(mean_squared_error(Y_test, prediccion_test))\n\nscores = -cross_val_score(modelo_lasso, X_train_s, Y_train, cv = 5, scoring = 'neg_mean_squared_error')\nprint(scores.mean())\n\nresultados.append(scores)\nnombres.append(\"Lineal lasso std.\")","a8b34ba5":"modelo_lasso = linear_model.LassoCV(alphas = np.logspace(-6, 6, 13), cv = 5)\nmodelo_lasso.fit(X_train_s, Y_train)\n\nmodelo_lasso.alpha_","0ad767f3":"prediccion_train = modelo_ridge.predict(X_train_s)\nprint(mean_squared_error(Y_train, prediccion_train))\n\nprediccion_test = modelo_ridge.predict(X_test_s)\nprint(mean_squared_error(Y_test, prediccion_test))","51000ebd":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\n\npoly = Pipeline([('poly', PolynomialFeatures(degree = 2)),\n                 ('linear', LinearRegression(fit_intercept = True))])\npoly.fit(X_train, Y_train)\n\nprediccion_train = poly.predict(X_train)\nprint(mean_squared_error(Y_train, prediccion_train))\n\nprediccion_test = poly.predict(X_test)\nprint(mean_squared_error(Y_test, prediccion_test))\n\nscores = -cross_val_score(poly, X_train_s, Y_train, cv = 5, scoring = 'neg_mean_squared_error')\nprint(scores.mean())","28f23c59":"from sklearn.linear_model import Lasso\nfrom sklearn.pipeline import Pipeline\n\npoly = Pipeline([('poly', PolynomialFeatures(degree = 2)),\n                 ('linear', Lasso(fit_intercept = True ,alpha = 10))])\npoly.fit(X_train, Y_train)\n\nprediccion_train = poly.predict(X_train)\nprint(mean_squared_error(Y_train, prediccion_train))\n\nprediccion_test = poly.predict(X_test)\nprint(mean_squared_error(Y_test, prediccion_test))\n\nscores = -cross_val_score(poly, X_train_s, Y_train, cv = 5, scoring = 'neg_mean_squared_error')\nprint(scores.mean())","83a6c398":"from sklearn import metrics\nfrom sklearn.model_selection import RepeatedKFold\n\nrkf = RepeatedKFold(n_splits = 5, n_repeats = 20, random_state = 123)\nX = df2_clean.drop(['actual_productivity'], axis = 1)\n","de5640fa":"warnings.filterwarnings('ignore')\n\nerr = []\nerr2 = []\nini = 1\nend = 20\nfor deg in range(ini, end):\n    poly = Pipeline([('poly', PolynomialFeatures()),\n                 ('linear', Lasso(fit_intercept = False, alpha = deg))])\n    poly.fit(X_train, Y_train)\n\n    prediccion_train = poly.predict(X_train)\n    err.append(mean_squared_error(Y_train, prediccion_train))\n    prediccion_test = poly.predict(X_test)\n    err2.append(mean_squared_error(Y_test, prediccion_test))\n\nplt.plot(np.arange(ini, end, 1.0), err, label = 'train')\nplt.plot(np.arange(ini, end, 1.0), err2, label = 'test')\nplt.xticks(np.arange(ini, end, 1.0))\nplt.vlines(np.argmin(err2) + ini, 0, max(err2),  color = 'gray', linestyle = 'dashed')\nplt.yscale('log')\nplt.ylabel('error')\nplt.xlabel('degree');","f91314c3":"from sklearn import metrics\nfrom sklearn.model_selection import RepeatedKFold\n\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\n\n\nresultados=[]\n\npipelines = []\n\nnombres = []\n\n\npipelines.append(('scaled_linear', Pipeline([('scaler', StandardScaler()),\n                                        ('linear', LinearRegression())\n                                       ])))\npipelines.append(('scaled_linear_ridge', Pipeline([('scaler', StandardScaler()),\n                                        ('ridge', Ridge(alpha = 15))\n                                       ])))\npipelines.append(('scaled_linear_Lasso', Pipeline([('scaler', StandardScaler()),\n                                        ('lasso', Lasso(alpha = 1))\n                                       ])))\n\npipelines.append(('scaled_poly_ridge', Pipeline([('scaler', StandardScaler()),\n                                        ('poly', PolynomialFeatures(degree = 2)),\n                                        ('ridge', Ridge(alpha = 15))\n                                       ])))\npipelines.append(('scaled_poly_lasso', Pipeline([('scaler', StandardScaler()),\n                                        ('poly', PolynomialFeatures(degree = 2)),\n                                        ('lasso', Lasso(alpha = 0.05))\n                                       ])))\npipelines.append(('poly_ridge', Pipeline([('poly', PolynomialFeatures(degree = 2)),\n                                        ('ridge', Ridge(alpha = 15))\n                                       ])))\n\npipelines.append(('poly_lasso', Pipeline([('poly', PolynomialFeatures(degree = 2)),\n                                        ('lasso', Lasso(alpha = 0.05))\n                                       ])))\n\npipelines.append(('SVR_Linear', Pipeline([( \"SVR\",SVR(kernel = 'linear', C = 1, max_iter=100))\n                        \n                                       ])))\npipelines.append(('SVR_Poly', Pipeline([( \"SVR\",SVR(kernel = 'poly', C = 1,degree = 2))\n                        \n                                       ])))\n\npipelines.append(('MLPRegresor', Pipeline([( \"MLPRegressor\",MLPRegressor(max_iter=10, hidden_layer_sizes=(25,)))\n                        \n                                       ])))\n\nfor nombre, modelo in pipelines:\n    \n    \n    #prediccion_train = modelo.predict(X_train)\n    #prediccion_test = modelo.predict(X_test)\n    \n    rkf = RepeatedKFold(n_splits = 5, n_repeats = 20, random_state = 123)\n    resultados_nk = []\n    \n    \n    for train_index, test_index in rkf.split(X):\n        X_train_nk, X_test_nk = X.values[train_index], X.values[test_index]\n        Y_train_nk, Y_test_nk = Y.values[train_index], Y.values[test_index]\n        scores = -cross_val_score(modelo, X_train_nk, Y_train_nk, cv = 5, scoring = 'neg_mean_squared_error')\n        resultados_nk.append(scores.mean())\n        \n    resultados.append(resultados_nk)\n    nombres.append(nombre)\n    print(\"%s: %f (%f)\" % (nombre, np.mean(resultados_nk), np.std(resultados_nk)))","09f1d0d2":"fig, ax = plt.subplots(1, 1, figsize = (20,15)) \nax.set_title('Comparaci\u00f3n de algoritmos')\nax.boxplot(resultados)\nax.set_xticklabels(nombres)\nplt.show()","48dc5804":"import seaborn as sns\n\nfig, ax = plt.subplots(1, 1, figsize = (20,15)) \nax.set_title('Comparaci\u00f3n de algoritmos')\nsns.boxplot(data = resultados, palette = \"Set2\", ax = ax);\nax.set_xticklabels(nombres);","f4e56cb0":"fig, ax = plt.subplots(1, 1, figsize = (20,15)) \nax.set_title('Comparaci\u00f3n de algoritmos')\nsns.swarmplot(data = resultados, color = \"black\", ax = ax)\nsns.boxplot(data = resultados, palette = \"Set2\", ax = ax)\nax.set_xticklabels(nombres);","79b652c4":"resultados=[]\n\npipelines = []\n\nnombres = []\n\n\npipelines.append(('scaled_linear', Pipeline([('scaler', StandardScaler()),\n                                        ('linear', LinearRegression())\n                                       ])))\npipelines.append(('scaled_linear_ridge', Pipeline([('scaler', StandardScaler()),\n                                        ('ridge', Ridge(alpha = 15))\n                                       ])))\npipelines.append(('scaled_linear_Lasso', Pipeline([('scaler', StandardScaler()),\n                                        ('lasso', Lasso(alpha = 1))\n                                       ])))\n\npipelines.append(('SVR_Poly', Pipeline([( \"SVR\",SVR(kernel = 'poly', C = 1,degree = 2))\n                        \n                                       ])))\n\n\n\nfor nombre, modelo in pipelines:\n    \n    \n    #prediccion_train = modelo.predict(X_train)\n    #prediccion_test = modelo.predict(X_test)\n    \n    rkf = RepeatedKFold(n_splits = 5, n_repeats = 20, random_state = 123)\n    resultados_nk = []\n    \n    \n    for train_index, test_index in rkf.split(X):\n        X_train_nk, X_test_nk = X.values[train_index], X.values[test_index]\n        Y_train_nk, Y_test_nk = Y.values[train_index], Y.values[test_index]\n        scores = -cross_val_score(modelo, X_train_nk, Y_train_nk, cv = 5, scoring = 'neg_mean_squared_error')\n        resultados_nk.append(scores.mean())\n        \n    resultados.append(resultados_nk)\n    nombres.append(nombre)\n    print(\"%s: %f (%f)\" % (nombre, np.mean(resultados_nk), np.std(resultados_nk)))","d90c93ec":"fig, ax = plt.subplots(1, 1, figsize = (20,15)) \nax.set_title('Comparaci\u00f3n de algoritmos')\nax.boxplot(resultados)\nax.set_xticklabels(nombres)\nplt.show()\n","9b83cce6":"import seaborn as sns\n\nfig, ax = plt.subplots(1, 1, figsize = (15,5)) \nax.set_title('Comparaci\u00f3n de algoritmos')\nsns.boxplot(data = resultados, palette = \"Set2\", ax = ax);\nax.set_xticklabels(nombres);","b6ad956c":"fig, ax = plt.subplots(1, 1, figsize = (15,5)) \nax.set_title('Comparaci\u00f3n de algoritmos')\nsns.swarmplot(data = resultados, color = \"black\", ax = ax)\nsns.boxplot(data = resultados, palette = \"Set2\", ax = ax)\nax.set_xticklabels(nombres);","60541b6a":"resultados=[]\n\npipelines = []\n\nnombres = []\n\n\n\npipelines.append(('scaled_linear_Lasso', Pipeline([('scaler', StandardScaler()),\n                                        ('lasso', Lasso(alpha = 1))\n                                       ])))\n\npipelines.append(('SVR_Poly', Pipeline([( \"SVR\",SVR(kernel = 'poly', C = 1,degree = 2))\n                        \n                                       ])))\n\nfor nombre, modelo in pipelines:\n    \n    \n    #prediccion_train = modelo.predict(X_train)\n    #prediccion_test = modelo.predict(X_test)\n    \n    rkf = RepeatedKFold(n_splits = 5, n_repeats = 20, random_state = 123)\n    resultados_nk = []\n    \n    \n    for train_index, test_index in rkf.split(X):\n        X_train_nk, X_test_nk = X.values[train_index], X.values[test_index]\n        Y_train_nk, Y_test_nk = Y.values[train_index], Y.values[test_index]\n        scores = -cross_val_score(modelo, X_train_nk, Y_train_nk, cv = 5, scoring = 'neg_mean_squared_error')\n        resultados_nk.append(scores.mean())\n        \n    resultados.append(resultados_nk)\n    nombres.append(nombre)\n    print(\"%s: %f (%f)\" % (nombre, np.mean(resultados_nk), np.std(resultados_nk)))","9be626f8":"fig, ax = plt.subplots(1, 1, figsize = (20,10)) \nax.set_title('Comparaci\u00f3n de los mejores algoritmos')\nsns.swarmplot(data = resultados, color = \"black\", ax = ax)\nsns.boxplot(data = resultados, palette = \"Set3\", ax = ax)\nax.set_xticklabels(nombres);","5e9f753e":"\nfrom sklearn.tree import DecisionTreeRegressor\n\nregressor = DecisionTreeRegressor(random_state = 123, max_depth = 3)\n\nscores = -cross_val_score(regressor, X, Y, cv = 5, scoring = 'neg_mean_squared_error')\n\nprint(scores.mean() * 100)\nprint(scores.std())\n\n\nregressor.fit(X_train, Y_train)\nfig, ax = plt.subplots(1, 1, figsize = (20, 15))\ntree.plot_tree(regressor, \n               impurity = True,\n               class_names = Y.unique(),\n               feature_names = X.columns\n              );\n\n","aaeed8fb":"from sklearn.ensemble import RandomForestRegressor\n\nbosque = RandomForestRegressor(n_estimators = 100,\n                                criterion = 'mse',\n                                max_depth = None,\n                                max_features = 'sqrt',\n                                oob_score = True,\n                                random_state = 123)\n\nbosque.fit(X_train, Y_train)\nprint(bosque.feature_importances_)\nprint('OOB error: {}'.format(1 - bosque.oob_score_))","af0cdf29":"fig, ax = plt.subplots(1, 1, figsize = (20, 15))\ntree.plot_tree(regressor, \n               impurity = True,\n               class_names = Y.unique(),\n               feature_names = X.columns\n              );","7c67c28d":"Pregunta: Por que mi variable team aparece como false?","b5853ff0":"#### We include the necessary libraries in the project\n#### Incluimos las librerias necesarias en el proyecto","652bba31":"# Edison Jair Bejarano Sepulveda","4a8d6357":"# 8. Decision tree regressor","c7eaefe1":"k-fold CV con varias repeticiones (as\u00ed la variancia de los resultados es m\u00e1s acurada). Escogemos unos cuantos pipelines y hacemos nk-fold CV para escoger el mejor modelo.","d77dd2b6":"Random Forest Regression is a supervised learning algorithm that uses ensemble learning method for regression. Ensemble learning method is a technique that combines predictions from multiple machine learning algorithms to make a more accurate prediction than a single model.","68589f99":"### Context\nThe Garment Industry is one of the key examples of the industrial globalization of this modern era. It is a highly labour-intensive industry with lots of manual processes. Satisfying the huge global demand for garment products is mostly dependent on the production and delivery performance of the employees in the garment manufacturing companies. So, it is highly desirable among the decision makers in the garments industry to track, analyse and predict the productivity performance of the working teams in their factories\n\n### Content\nThis dataset includes important attributes of the garment manufacturing process and the productivity of the employees which had been collected manually and also been validated by the industry experts.\n\n-------------------------------------------------------------------------------------------------------------------\n### Contexto\nLa industria de la confecci\u00f3n es uno de los ejemplos clave de la globalizaci\u00f3n industrial de esta era moderna. Es una industria que requiere mucha mano de obra y muchos procesos manuales. Satisfacer la enorme demanda mundial de productos de confecci\u00f3n depende principalmente del rendimiento de producci\u00f3n y entrega de los empleados en las empresas de fabricaci\u00f3n de prendas de vestir. Por lo tanto, es muy deseable entre los tomadores de decisiones en la industria de la confecci\u00f3n rastrear, analizar y predecir el desempe\u00f1o productivo de los equipos de trabajo en sus f\u00e1bricas.\n\n### Contenido\nEste conjunto de datos incluye atributos importantes del proceso de fabricaci\u00f3n de prendas de vestir y la productividad de los empleados que se recopilaron manualmente y tambi\u00e9n fueron validados por los expertos de la industria.","9683e6c4":"# 3. Pre-processing of data","43f6f4d1":"Despu\u00e9s de intentar con multiples modelos, se establece que el mas idoneo es el Scaled linear Lasso","44dbdc9a":"Nuestro PC1 y PC2 son los que representan la mayor parte de todos.","d344a9fe":"Este paso anterior se puede realizar para todos los modelos y asi encontrar su alpha ideal","c724ffc7":"# 1. Explicaci\u00f3n de variables\n\n1.Fecha: fecha en MM-DD-AAAA\n\n2.D\u00eda: d\u00eda de la semana\n\n3.Trimestre: una parte del mes. Un mes se dividi\u00f3 en cuatro trimestres.\n\n4.Departamento: Departamento asociado a la instancia\n\n5.TeamNumber: n\u00famero de equipo asociado a la instancia\n\n6.No_of_workers: N\u00famero de trabajadores en cada equipo\n\n7.No_of_stylechange: n\u00famero de cambios en el estilo de un producto en particular\n\n8.Productividad objetivo: Productividad objetivo establecida por la Autoridad para cada equipo para cada d\u00eda.\n\n9.smv: Valor minuto est\u00e1ndar, es el tiempo asignado para una tarea\n\n10.wip: trabajo en curso. Incluye el n\u00famero de elementos sin terminar de los productos.\n\n11.Tiempo extra: representa la cantidad de tiempo extra de cada equipo en minutos\n\n12.Incentivo: Representa la cantidad de incentivo financiero (en BDT) que habilita o motiva un curso de acci\u00f3n en particular.\n\n13.Idletime: la cantidad de tiempo en que se interrumpi\u00f3 la producci\u00f3n debido a varias razones\n\n14.Idlemen: el n\u00famero de trabajadores que estuvieron inactivos debido a la interrupci\u00f3n de la producci\u00f3n\n\n15.Productividad_real: El% real de productividad que entregaron los trabajadores. Va de 0 a 1.","7e88d352":"Se implementa este metodo para encontrar las variables optimas para obtener un error inferior ","68a80d18":"##  Predict the productivity range\n\n### Task Details\nIt is highly desirable among the decision makers in the garments industry to track, analyse and predict the productivity performance of the working teams in their factories.\n\n### Expected Submission\nThis dataset can be used for regression purpose by predicting the productivity range (0-1) or for classification purpose by transforming the productivity range (0-1) into different classes.\n\n\n\n## Sumary\n\n1. Explanation of variables\n2. Data analysis\n3. Pre-processing of data\n4. Correlation in our variables\n5. Data visualization\n6. Application of models\n7. Model comparison\n8. Decision tree regressor\n9. Randon forest regressor\n10. Conclusion and results\n\n\n\n\n-------------------------------------------------------------------------------------------------------------------\n\n## Predecir el rango de productividad\n\n### Detalles de la tarea\nEs muy deseable entre los tomadores de decisiones en la industria de la confecci\u00f3n rastrear, analizar y predecir el desempe\u00f1o productivo de los equipos de trabajo en sus f\u00e1bricas.\n\n### Env\u00edo esperado\nEste conjunto de datos se puede utilizar con fines de regresi\u00f3n al predecir el rango de productividad (0-1) o con fines de clasificaci\u00f3n al transformar el rango de productividad (0-1) en diferentes clases.\n\n\n\n## Resumen\n\n1. Explicaci\u00f3n de variables\n2. An\u00e1lisis de datos\n3. Tratamiento previo de datos\n4. Visualizaci\u00f3n de datos\n5. Correlaci\u00f3n en nuestras variables\n6. Aplicaci\u00f3n de modelos\n7. Comparaci\u00f3n de modelos\n8. Regresor del \u00e1rbol de decisi\u00f3n\n9. Regresor del bosque de Randon\n10. Conclusi\u00f3n y resultados\n\n\n\n\n\n\n\n\n\n","96a18c52":"## Plot de comparaci\u00f3n de algoritmos","f6043349":"# 4. Correlation in our variables","f4aeb704":"Por Lasso","9f6f0ddd":"Observamos que el head() que invocamos ya no tiene las columnas dropeadas y por lo contrario tenemos dos nuevas que son 'day' y 'month', extraidas de la columna 'date', que tambi\u00e9n ya no se encuentra.","206bd987":"Finalmente observamos las nuevas columnas de 'team', las cuales son 12 que son la cantidad de equipos en nuestro dataset.\n\nA continuaci\u00f3n utilizamos el algoritmo Knn que es denominado como el mas proximo, para remplazar los valores null que tengamos en nuestro dataset.","1b525941":"Sacamos los PCA","80187afe":"# 7. Model comparison","647d769f":"# Productivity Prediction of Garment Employees\n\n# Predicci\u00f3n de  productividad de los empleados de  confecci\u00f3n","d1b81da7":"### Scaled linear Lasso: \n0.031084 (0.000715)\n### Support Vector Regression (SVR): \n0.035161 (0.005364)","7a4a88cc":"Observamos que los datos que se contienen en la columna 'team' deben ser clasificados y distribuidos, para lo cual es necesario aplicar el metodo apply(lambda) --> esto aplicado como string.\n\nDe igual manera, a continuaci\u00f3n procederemos a dropear la columna quarter y day, para luego utilizar la columna 'date' y tomar el primer y segundo elemento, con el fin de que posteriormente con un get_dummies ingresarlos junto con las nuevas columnas de team a nuestro nuevo dataset.","f8d8b1ff":"#### Definimos las variables a ingresar en nuestros modelos  'X' y 'Y' para el Train y el Test","1964fc41":"Nuevo modelo a entrenar","a94afc58":"### Valores nulos\n\n\nMCAR (Missing Completely At Random): los valores faltantes realmente \n    son aleatorios. No sabemos la causa de su ausencia y no hay otras \n    variables que la expliquen. No podemos estimar su valor. Si no son\n    muchos valores faltantes, los podemos eliminar. Aunque a medida \n    que vayamos eliminando m\u00e1s datos, perderemos precisi\u00f3n en nuestros \n    modelos, por eso ser\u00eda m\u00e1s interesante estimar los valores.\n\n\n\n","b3c82712":"# 6. Application of models","c7603b98":"Realizaremos el calculo de todos los modelos para compararlos, adicionamos una peque\u00f1a red neuronal para incluirla en nuestro estudio.\n\n\nMulti-layer Perceptron regressor=>This model optimizes the squared-loss using LBFGS or stochastic gradient descent.\n\n","9141ba1e":"## 1. Explanation of variables\n\n1.date : Date in MM-DD-YYYY\n\n2.day : Day of the Week\n\n3.quarter : A portion of the month. A month was divided into four quarters\n\n4.department : Associated department with the instance\n5.teamNumber : Associated team number with the instance\n6.noofworkers : Number of workers in each team\n7.noofstylechange : Number of changes in the style of a particular product\n\n8.targetedproductivity : Targeted productivity set by the Authority for each team for each day.\n\n9.smv : Standard Minute Value, it is the allocated time for a task\n\n10.wip : Work in progress. Includes the number of unfinished items for products\n\n11.overtime : Represents the amount of overtime by each team in minutes\n\n12.incentive : Represents the amount of financial incentive (in BDT) that enables or motivates a particular course of action.\n\n13.idletime : The amount of time when the production was interrupted due to several reasons\n\n14.idlemen : The number of workers who were idle due to production interruption\n\n15.actual_productivity : The actual % of productivity that was delivered by the workers. It ranges from 0-1.\n\n\n\n\n\n","012074c3":"# 5. Data visualization","e692253b":"# 9. Randon forest regressor","407b70a2":"# 10. Conclusion and results","c00b5693":"Notamos una correlaci\u00f3n del 94% en las variables 'department_sweing' y 'no_of_workers', asi mismo otra correlaci\u00f3n que podemos destacar es la que existe entre 'no_of_workers' y 'smv' con un 91%.\n\nOtras encontradas:\n\n-'department_sweing' AND 'smv' -->87%\n\n-'no_of_workers' AND 'over_time' -->73%\n\n-'department_sweing' AND 'over_time' -->68%\n\n## Modelos implementados en nuestro dataset y sus resultados\n\n\n\nscaled_linear: 0.376954 (0.151149)\n\nscaled_linear_ridge: 0.288120 (0.071774)\n\nscaled_linear_Lasso: 0.031084 (0.000715)\n\nscaled_poly_ridge: 2164.370534 (2193.214479)\n\nscaled_poly_lasso: 88.653517 (44.022341)\n\npoly_ridge: 54.857675 (31.694947)\n\npoly_lasso: 3.983882 (2.867972)\n\nSVR_Linear: 2641.121353 (9649.024103)\n\nSVR_Poly: 0.035161 (0.005364)\n\nMLPRegresor: 221262.839704 (300972.414711)-->En este utilizamos 25 neuronas para encontrar el mejor valor para este\n\n\n## Los mejores resultados de nuestros modelos son:\n\n### Scaled linear Lasso: \n0.031084 (0.000715)\n### Support Vector Regression (SVR): \n0.035161 (0.005364)\n\n\n### Decision Tree Regressor\n\n\n","109aa4d4":"Finalmente ya remplazamos los valores null que teniamos y procedemos a guardarlos en un nuevo dataset que llamaremos df2_clean. Para comprobarlo, realizamos un \".isnull().sum()\" para sumar todos nuestro valores nulos,con lo cual observaremos que no tenemos ninguno.\n\nEn teoria, ya terminamos de limpiar nuestro dataset y podemos proceder con esto.","e91d16bf":"#### General","d66dedb2":"# 2. Data analysis","9355871f":"### Evaluamos el modelo simple\n\nRealizamos los planteamientos para nuestros modelo, para ello describiremos uno a uno los modelos simples y luego implementaremos probar todos para establecer cual es el mejor modelo.","6e394d4e":"The get_dummies () function is used to convert categorical variable into dummy \/ indicator variables. Fecha of which to get dummy indicators. String to append DataFrame column names. ","6cb933c1":"Obtenemos todos los resultados de los modelos planteados y procedemos a plotearlos, se destaca que en el paso anterior conseguimos establecer cual es el mejor y peor modelo para nuestro Dataset.","b7be074a":"Con todo lo anterior establecemos que, los mejores resultados son:\n\n\n1.scaled_linear_Lasso: 0.031084 (0.000715)\n\n2.SVR_Poly: 0.035161 (0.005364)\n\n3.scaled_linear_ridge: 0.288120 (0.071774)\n\n4.scaled_linear: 0.376954 (0.151149)\n\n\n\n","c8ca399a":"Notamos una correlaci\u00f3n del 94% en las variables 'department_sweing' y 'no_of_workers', asi mismo otra correlaci\u00f3n que podemos destacar es la que existe entre 'no_of_workers' y 'smv' con un 91%.\n\nOtras encontradas:\n\n-'department_sweing' AND 'smv' -->87%\n\n-'no_of_workers' AND 'over_time' -->73%\n\n-'department_sweing' AND 'over_time' -->68%\n\n\n","4e04f8e9":"https:\/\/www.kaggle.com\/ishadss\/productivity-prediction-of-garment-employees","228117f7":"Comenzamos con el estudio de nuestros modelos","0ed33c73":"### Recordando los resultados obtenidos previamente:\n\n\n###### scaled_linear: 0.376954 (0.151149)\n\n###### scaled_linear_ridge: 0.288120 (0.071774)\n\n###### scaled_linear_Lasso: 0.031084 (0.000715)\n\nscaled_poly_ridge: 2164.370534 (2193.214479)\n\nscaled_poly_lasso: 88.653517 (44.022341)\n\npoly_ridge: 54.857675 (31.694947)\n\npoly_lasso: 3.983882 (2.867972)\n\nSVR_Linear: 2641.121353 (9649.024103)\n\n###### SVR_Poly: 0.035161 (0.005364)\n\nMLPRegresor: 221262.839704 (300972.414711)-->En este utilizamos 25 neuronas para encontrar el mejor valor para este\n","bf6a9e65":"#### We set our variables to train and plot in our models\n\n#### Planteamos nuestras variables a entrenar y plotear en nuestros modelos","651d41c5":"\n### Randon forest regressor\n\n\n\n0.003976886617684627\n\n\nOOB error: 0.4639779970632426","ebbd6030":"####  Buscamos el alpha ideal para cada uno de los modelos"}}