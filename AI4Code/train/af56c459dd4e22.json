{"cell_type":{"af42001e":"code","0fcb6b8f":"code","ad9e0c7e":"code","1bbf79d2":"code","167412aa":"code","d933d49f":"code","8a8e04c4":"code","caf6ab89":"code","239931b8":"code","19ff822b":"code","d9564770":"code","b34d026f":"code","7c5c09fc":"code","6eaba513":"code","18072771":"code","ce6f754d":"code","9b843301":"code","5c64c41b":"code","19b632f3":"code","0613dde3":"code","68c36ab8":"code","c48daec3":"code","e6a80e6b":"code","47ecba87":"code","0208b413":"code","9a3ab879":"code","46eb5dfd":"code","04b3eef2":"code","a5382ef8":"code","3bbe4ff3":"code","52d0f351":"code","605efdae":"code","2b822a59":"markdown","8b5a7d0b":"markdown","a5468328":"markdown","2fdadf74":"markdown","8b4aa7a6":"markdown","fc5dc9e9":"markdown","467fc4d2":"markdown","46738834":"markdown","6262f1e2":"markdown","1d4bcd11":"markdown","17ff1fd8":"markdown","8605dd72":"markdown","8ba9073a":"markdown","dafbb9da":"markdown","e5ea35f6":"markdown","fdb8c960":"markdown","925a1b53":"markdown","f7983b1c":"markdown","75ba0f21":"markdown","95670580":"markdown","162a28d8":"markdown","072f113f":"markdown","e637af0f":"markdown","acc47185":"markdown","88c5be2a":"markdown","a3a7487d":"markdown","923cde10":"markdown","6a415a91":"markdown","ab183b8b":"markdown","4004d173":"markdown","cee8ef0a":"markdown","03737202":"markdown","c040bf47":"markdown"},"source":{"af42001e":"import pandas as pd\nfrom pandas import set_option\nfrom pandas.plotting import scatter_matrix","0fcb6b8f":"import numpy as np\nfrom numpy import set_printoptions","ad9e0c7e":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nsns.set()","1bbf79d2":"from sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score,recall_score\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.decomposition import PCA \nfrom sklearn.manifold import Isomap","167412aa":"import os\nimport sys\nimport pprint","d933d49f":"import ipywidgets as widgets\nfrom IPython.display import Image\nfrom IPython.display import display, Math, Latex\nfrom IPython.core.interactiveshell import InteractiveShell  ","8a8e04c4":"def label_encoding(dataset,input_headers):\n    \n    for i in input_headers:\n        \n        the_data_type=dataset[i].dtype.name\n        if (the_data_type=='object'):\n            lable_enc=preprocessing.LabelEncoder()\n            lable_enc.fit(dataset[i])\n            labels=lable_enc.classes_   #this is an array\n            labels=list(labels) #converting the labels array to a list\n            print(labels)\n            dataset[i]=lable_enc.transform(dataset[i])\n\n            return labels\n    \n        else:\n            c=list(np.unique(dataset[i]))\n            return [str(x) for x in c]","caf6ab89":"def feature_scaling(X_train,X_test):\n    \n    sc_X=StandardScaler()\n    X_train=sc_X.fit_transform(X=X_train,y=None)\n    X_test=sc_X.fit_transform(X=X_test,y=None)\n\n    print(sc_X.fit(X_train))\n    print(X_train[0:5])\n    \n    \n    \n    return X_train, X_test","239931b8":"def plot_of_data_space(dataset,data,labels,input_headers):\n    \n    xx_1=pd.DataFrame(data[:,0]) \n    xx_2=pd.DataFrame(data[:,1]) \n    y=pd.DataFrame(labels)\n    \n    plt.figure(figsize=(15,10)) \n    b=plt.scatter(xx_1[y==0],xx_2[y==0],color='b') \n    r=plt.scatter(xx_1[y==1],xx_2[y==1],color='r')\n    g=plt.scatter(xx_1[y==2],xx_2[y==2],color='g') \n    bl=plt.scatter(xx_1[y==3],xx_2[y==3],color='black')\n    \n    plt.xlabel(input_headers[0])\n    plt.ylabel(input_headers[1])\n\n    plt.grid(b=True)\n    plt.legend((b,r,g,bl),tuple(np.unique(labels)))\n    plt.show()\n","19ff822b":"def feature_distributions(df,target_header,*args):\n    \"\"\"Histrogram plots of the input variables for each target class\"\"\"\n    \n    data=df.drop(target_header,axis=1,inplace=False)\n\n    num_plot_rows=len(data.columns)\n\n    print (classes)\n    \n    label_encoder = preprocessing.LabelEncoder()\n    df[target_header]=label_encoder.fit_transform(df[target_header])\n    labels=label_encoder.classes_   #this is an array\n    labels=list(labels) #converting the labels array to a list\n    print (labels)\n\n    fig = plt.figure(figsize = (20,num_plot_rows*4))\n    j = 0\n\n    ax=[]\n    colors=['b','r','g','black']\n    for i in data.columns:\n        plt.subplot(num_plot_rows, 4, j+1)\n        j += 1\n        for k in range(len(labels)):\n    #         print(k)\n            a=sns.distplot(data[i][df[target_header]==k], color=colors[k], label = str(labels[k])+classes[k]);\n            ax.append(a)\n        plt.legend(loc='best')\n    \n    print('Feature Distribution Plots: \\n')\n#     fig.suptitle(target_header+ 'Feature Distribution Plots',fontsize=16)\n    fig.tight_layout()\n    # fig.subplots_adjust(top=0.95)\n    plt.show()","d9564770":"def split_the_dataset(dataset,input_headers,target_header):\n    \n    X=dataset[input_headers]\n    y=dataset[target_header]\n    \n    X.head()\n    \n    return X,y","b34d026f":"def replacing_zeros(dataset,the_headers):\n    \"\"\"Function used to replace zeros with the mean\"\"\"\n\n    for header in the_headers:\n        dataset[header]=dataset[header].replace(0,np.nan)\n        mean=int(dataset[header].mean(skipna=True))\n        dataset[header]=dataset[header].replace(np.nan,mean)\n        \n    return dataset","7c5c09fc":"def correlation_matrix(dataset,input_headers,target_header):\n    \"\"\"Correlation matrix (matrix,heatmap and pairplot)\"\"\"\n    \n    correlation_threshold=.7\n    \n#     dataset.drop([target_header[0]],axis=1,inplace=True)\n    feature_matrix=dataset[input_headers+target_header]\n    corr=feature_matrix.corr().abs()\n    corr\n    \n    plt.figure(figsize=(10,10))\n    corr_plot=sns.heatmap(corr,cmap=\"Reds\",annot=True)\n    \n    corr_to_target=corr[target_header[0]].sort_values(ascending=False)\n    print(f'Correlations with respect to target:\\n{corr_to_target}\\n')\n    \n#     high_corr_drop=[i for i in ctt.index if any (ctt.iloc[i]>.50)]\n    high_corr_drop=[]\n    for x in range(len(corr_to_target)):\n        if (corr_to_target.iloc[x]>correlation_threshold):\n            high_corr_drop.append(corr_to_target.index[x])\n    print(f'Recommended features to drop due to high correlation (greater than {correlation_threshold}) to target variable:\\n{high_corr_drop}')\n    \n    \n#     corr_pair=sns.pairplot(dataset,hue=target_header[0])\n    plt.show()\n    \n#     return corr_plot  #corr_pair \n    ","6eaba513":"def pca(dataset,input_headers,target_header,*args):\n    \"\"\"Dimensionality reduction via PCA. This function is called when the there are more than 2 predictor variables in the dataset.\"\"\"\n    \n    feature_matrix=dataset[input_headers]\n    model = PCA(n_components=2)            # 2. Instantiate the model with hyperparameters\n    model.fit(feature_matrix)  # 3. Fit to data. Notice y is not specified!\n    X_2D = model.transform(feature_matrix)         # 4. Transform the data to two dimensions\n\n    dataset['PCA1'] = X_2D[:, 0]\n    dataset['PCA2'] = X_2D[:, 1]\n    \n#     f,ax=plt.subplots(figsize=(8, 8))\n#     cmap = sns.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\n#     sns.scatterplot(\"PCA1\", \"PCA2\", hue=target_header[0],style=target_header[0],\n#                     data=dataset,palette=\"Set1\",label = None,legend=False,ax=ax).set_title(\"PCA Variables per Target Class\");\n    ax=sns.lmplot(\"PCA1\", \"PCA2\", hue=target_header[0], data=dataset, fit_reg=False);\n    plt.title(\"PCA Variables per Target Class\")\n    plt.legend(title=target_header[0], loc='lower right', labels=classes)","18072771":"if __name__ == \"__main__\":\n    \n    location='..\/input\/Iris.csv'\n\n    dataset=pd.read_csv(location)\n    dataset.info()","ce6f754d":"dataset.head()","9b843301":"dataset.describe()","5c64c41b":"target_header=['Species']\ninput_headers=['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']\n\ntarget_label=label_encoding(dataset,target_header)\nclasses=target_label\ntest_label=label_encoding(dataset,input_headers)\n\ndataset=dataset[input_headers+target_header]\nX,y=split_the_dataset(dataset,input_headers,target_header)\n\nX.head()","19b632f3":"if (X.values.shape[1]==2):\n    plot_of_data_space(dataset,X.values,y.values,input_headers)\nelse:\n    pca(dataset,input_headers,target_header,classes)","0613dde3":"feature_distributions(dataset,target_header[0],classes)","68c36ab8":"    X.head()","c48daec3":"correlation_matrix(dataset,input_headers,target_header)","e6a80e6b":"test_data_size=.2\nXtrain,Xtest,ytrain,ytest=train_test_split(X,y,test_size=test_data_size,random_state=42)","47ecba87":"print(f'Xtrain sample count: {Xtrain.shape[0]}')\nprint(f'ytrain sample count: {ytrain.shape[0]}')\nprint(f'Xtest sample count: {Xtest.shape[0]}')\nprint(f'ytest sample count: {ytest.shape[0]}')","0208b413":"#Scale the data    \nXtrain, Xtest=feature_scaling(Xtrain,Xtest)","9a3ab879":"model=RandomForestClassifier(n_estimators=100, criterion='gini',max_depth=None,\n                             random_state=42)","46eb5dfd":"ytrain=ytrain.values.reshape(ytrain.size,)\nmodel.fit(Xtrain,ytrain)","04b3eef2":"y_model=model.predict(Xtest)\ny_model","a5382ef8":"y_model_prob=model.predict_proba(Xtest)\ny_model_prob[0:5]","3bbe4ff3":"accur=accuracy_score(ytest,y_model)\nrecall=recall_score(ytest, y_model,average=None)\nprecision=precision_score(ytest, y_model,average=None)\nprint (f'MODEL RESULTS WITH DATASET SPLIT AT {(1-test_data_size)*100:.1f}% TRAINING DATA AND {test_data_size*100:.1f}% TEST DATA\\n')\nprint(f'Model Accuracy:{accur*100:.1f}%\\n')\n\nfor i,k in enumerate (classes):\n    print(f'{k} Recall:{recall[i]*100:.1f}%\\n')\n    print(f'{k} Precision:{precision[i]*100:.1f}%\\n')","52d0f351":"cm=confusion_matrix(ytest, y_model)\ncm=pd.DataFrame(data=cm,columns=classes,index=classes)\nsns.heatmap(cm,square=True,annot=True,cbar=True)\nplt.title('CONFUSION MATRIX')\nplt.xlabel('predicted value')\nplt.ylabel('true value')\nplt.show()","605efdae":"y=y.values.reshape(y.size,)   # reshape y to a 1-d array\nk_fold=10\nscore=cross_val_score(model,X,y,cv=k_fold)\nscore.mean();\n\nprint(f'Cross Validation Scores:\\n{score}\\n')\nprint(f'Mean Score:{score.mean()*100:.2f}%\\nStandard Deviation:{score.std():.2f}')\n\nsns.boxplot(x=score,orient='v')\nplt.title(f'{k_fold} Fold Cross Validation Results')\nplt.ylabel('Model Accuracy')\nplt.show()","2b822a59":"## Feature Correlations","8b5a7d0b":"## Label Encoding","a5468328":"## Principal Component Analysis (PCA)","2fdadf74":"## Pandas","8b4aa7a6":"### Model score & performance","fc5dc9e9":"### Fit model to training data","467fc4d2":"## Visualization","46738834":"### Feature Distributions (histograms)","6262f1e2":"### Cross Validation","1d4bcd11":"### Data space","17ff1fd8":"## notebook widgets","8605dd72":"## Feature Scaling","8ba9073a":"## Correlation Matrix","dafbb9da":"## Splitting the Train-Test data","e5ea35f6":"### Plot the data space (scatter)","fdb8c960":"## Get Data","925a1b53":"## Replacing Zeros","f7983b1c":"# MAIN PROGRAM","75ba0f21":"## System","95670580":"### Model prediction on test data","162a28d8":"## Numpy","072f113f":"## sklearn","e637af0f":"# FUNCTIONS","acc47185":"## Scale the data","88c5be2a":"# RANDOM FOREST","a3a7487d":"## Random Forest Model","923cde10":"# LOAD THE DEPENDANCIES","6a415a91":"## Selecting inputs and targets","ab183b8b":"### Confusion Matrix","4004d173":"## Matplotlib & Seaborn","cee8ef0a":"## Data Visualizations","03737202":"## Preprocessing: Splitting the dataset","c040bf47":"### Feature distributions"}}