{"cell_type":{"95b6178b":"code","57f05c7e":"code","361dc5f3":"code","6dd5a41b":"code","8f99d34f":"code","d7118642":"code","d875d1d8":"code","cb66ee49":"code","f45678a1":"code","ce0beb98":"code","5479f3a4":"markdown","84bfb684":"markdown","da846293":"markdown","dcc5a386":"markdown","aa4d7184":"markdown","09485d58":"markdown","1a219fc9":"markdown","861dc7ac":"markdown","1d560ccb":"markdown"},"source":{"95b6178b":"# Libraries.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Path\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","57f05c7e":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","361dc5f3":"train_df.head(), test_df.head()","6dd5a41b":"train_df.info(), test_df.info()","8f99d34f":"corrmat = train_df.corr()\ntop_corr_features = corrmat.index[abs(corrmat['SalePrice'])>0.5]\ntop_corr_features","d7118642":"# heatmap\nplt.figure(figsize=(13,10))\nplt_corr = sns.heatmap(train_df[top_corr_features].corr(), annot=True)","d875d1d8":"# Feature selection\n#train_df= train_df[top_corr_features]\n#test_df = test_df[top_corr_features.drop(['SalePrice'])]\n\n# Split target\ntrain_y_label = train_df['SalePrice']\ntrain_df.drop(['SalePrice'], axis=1, inplace=True)","cb66ee49":"boston_df = pd.concat((train_df, test_df), axis=0)\nboston_df_index = boston_df.index\n\nboston_df.head()\n#print(len(boston_df))","f45678a1":"#boston_df_without = boston_df.drop(['SalePrice'], axis=1)\nnulltotal=boston_df.isnull().sum().sort_values(ascending=False)\nnullpercent = (boston_df.isnull().sum()\/ len(boston_df)).sort_values(ascending=False)\nnullpoint=pd.concat([nulltotal, nullpercent], axis=1, keys=['Total number of null', 'Percent of null'])\nnullpoint.head()","ce0beb98":"remove_cols = nullpercent[nullpercent >= 0.5].keys()\nboston_df = boston_df.drop(remove_cols, axis=1)\nboston_df.head()","5479f3a4":"**1. Load Data**","84bfb684":"And,","da846293":"**2. Calculation of correlation for features**","dcc5a386":"We select only features of top correlation replace of data for new data. \n\n","aa4d7184":"To apply same feature engineering between train and test, I merge the data set.","09485d58":"**Introduction**\n\nThis is the third work of ML for me. It is interesting for begginer like me to study regressiion techniques and know house prices as well as real estate business. Let me do for begginer step by step. If you are interesting this notebook. Pleases write the commnets and Upvote.","1a219fc9":"We need how to take the NaN data. First of all, we remove the NaN column over ratio of 0.5.","861dc7ac":"**Purpose**\n\nThe boston house price is very basic data set. However, The data set given in kaggle has the much various variations. It needs **feature engineering**.\n\n1. Load Data\n2. Feature Selection and Correlation\n3. Check NaN, Object, Numeric variables, \n4. Change and input values.\n5. Modeling","1d560ccb":"The important thing is correlation of \"SalePrice\". The highest correlation is overallquall like \"0.79\". And, the other high correlation has 0.82 between \"1stFlrSf\" and \"TotalBsmtSF\", 0.83 between \"TotRmsAbvGrd\" and GrLivArea. 0.88 between \"GarageCars\" and \"GarageArea\".\n"}}