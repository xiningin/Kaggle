{"cell_type":{"28b2d0d8":"code","d7b02a89":"code","f0943697":"code","27477888":"code","9e27c2f6":"code","10403b1c":"code","4a98e7cb":"code","cfaf134f":"code","9dcdf9cd":"code","8aa9d1db":"code","de4ceb0b":"code","1dd8265f":"code","8d5ae218":"code","deddda92":"code","f74b8be3":"code","3cd5f744":"code","b7d0a876":"code","383a5dc8":"code","4a5d4be3":"code","06ad1b99":"code","100a7ad2":"code","103e55e4":"code","8e565bba":"code","385b0d14":"code","37ecb798":"code","2d6c1fc0":"markdown","ac6a333b":"markdown","120f51c3":"markdown","0fa5a8fb":"markdown","eb0954f3":"markdown","82d60743":"markdown","cf6c749a":"markdown","892dc344":"markdown","40cd7aae":"markdown","58560b49":"markdown","de7dfe01":"markdown","f476b7bf":"markdown","0d0c0b9d":"markdown","4c23a980":"markdown","668a3121":"markdown","d4ded26f":"markdown","60f55edd":"markdown","e18935ba":"markdown","fc942cb5":"markdown","707b5247":"markdown"},"source":{"28b2d0d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d7b02a89":"# runtime\nimport timeit\n\n# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# preprocessing\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n\n# Ml model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE\nfrom matplotlib import pyplot\nfrom numpy import where\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n\nnp.warnings.filterwarnings('ignore')","f0943697":"data_df=pd.read_csv(\"\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")\ndata_df.head()","27477888":"data_df.info()","9e27c2f6":"display(data_df.isnull().sum())\nprint(\"====\/\/\/====\")\ndisplay(data_df.duplicated().sum())\nprint(\"====\/\/\/====\")\ndisplay(data_df.info())","10403b1c":"data_df=data_df.dropna(axis='rows')\n","4a98e7cb":"# Data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nch = {1: 'Having Stroke', 0: 'Not Having Stroke'}\nanalyze_data=data_df.copy()\nanalyze_data['stroke'] = data_df['stroke'].map(ch)\n\ngender = analyze_data.groupby(['gender', 'stroke']).agg({'stroke': 'count'}).rename(columns = {'stroke': 'count'}).reset_index()\ngender","cfaf134f":"fig = px.sunburst(gender, path = ['gender', 'stroke'], values = 'count', color = 'gender', title = 'Affect of Age on Stroke', width = 600, height = 600)\nfig.update_traces(textinfo = 'label + percent parent')\nfig.show()","9dcdf9cd":"fig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\n\nvalue = analyze_data[analyze_data['stroke']=='Having Stroke']['age']\n\nax.hist(value, bins=10)\nplt.title('Distribution of Age of People Having Stroke')\n\nplt.show()","8aa9d1db":"fig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\n\nvalue = analyze_data[analyze_data['stroke']=='Having Stroke']['bmi']\n\nax.hist(value, bins=10)\nplt.title('Distribution of BMI of People Having Stroke')\n\nplt.show()","de4ceb0b":"Residence_type = analyze_data.groupby(['Residence_type', 'stroke']).agg({'stroke': 'count'}).rename(columns = {'stroke': 'count'}).reset_index()\n\nfig = px.sunburst(Residence_type, path = ['Residence_type', 'stroke'], values = 'count', color = 'Residence_type', title = 'Affect of Residence_type on Stroke', width = 600, height = 600)\nfig.update_traces(textinfo = 'label + percent parent')\nfig.show()","1dd8265f":"smoking_status = analyze_data.groupby(['smoking_status', 'stroke']).agg({'stroke': 'count'}).rename(columns = {'stroke': 'count'}).reset_index()\n\nfig = px.sunburst(smoking_status, path = ['smoking_status', 'stroke'], values = 'count', color = 'smoking_status', title = 'Affect of smoking_status on Stroke', width = 600, height = 600)\nfig.update_traces(textinfo = 'label + percent parent')\nfig.show()","8d5ae218":"data_df.head()","deddda92":"ax = sns.countplot(x=\"stroke\", data=data_df)","f74b8be3":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder","3cd5f744":"X = data_df.drop(['stroke','id'], axis = 1)\ny = data_df['stroke']\n\n# Split feature as categorical and continuous features\n\ncat_features=X.select_dtypes(exclude=np.number).columns\ncont_features=X.select_dtypes(include=[np.number,'float64','int64']).columns\n","b7d0a876":"# Label Encoder for Categorical Features\ndef label_encoder(df):\n    for i in cat_features:\n        le = LabelEncoder()\n        df[i] = le.fit_transform(df[i])\n    return df","383a5dc8":"# Standard Scaler for Continuous Features\nsc = StandardScaler()\nX[cont_features] = sc.fit_transform(X[cont_features])\n\n# Label encoding for Categorical Features\nX = label_encoder(X)\n\nX.head(3)","4a5d4be3":"data_df2=X.join(y)\ncorrmat = data_df2.corr(method='pearson')\nf, ax = plt.subplots(figsize=(24, 16))\nsns.heatmap(corrmat, ax=ax, cmap=\"YlGnBu\", linewidths=0.1, annot=True)\ndata_df2.head()","06ad1b99":"oversample = SMOTE()\nX, y = oversample.fit_resample(X, y)\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.15,random_state=0)","100a7ad2":"Model=XGBClassifier(eval_metric=\"logloss\")\n#pipeModel=Pipeline([('scaler', StandardScaler()), ('model', Model)])\n\n# the benefit of using K-Fold is that we could calculate the cross validation value using some of the methods of scoring \nkfold = KFold(n_splits=10, shuffle=True, random_state=0)\ncv_results = cross_val_score(Model, X_train, y_train, cv=kfold)\n\nModel.fit(X_train, y_train)\n# this is the scaled XGBClassifier\nprint('Score for XGBClassifier method:', Model.score(X_test, y_test))\n\n# the mean result (10 data) of negative mean squared error\nprint('Score for XGBClassifier method using cross_val_score:', cv_results.mean())\ny_pred = Model.predict(X_test)\ny_prob = Model.predict_proba(X_test)[:,1]\n\n#print(mean_squared_error(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\nprint(\"roc_auc score:\", roc_auc_score(y_test,y_prob))\n\n# Roc curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nprint('\\n\\n-----------------------------------------------------\\n\\n')\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\nprint('\\n\\n-----------------------------------------------------\\n\\n')\nplt.figure(figsize = (8, 8))\nfrom matplotlib import pyplot\nfrom xgboost import plot_importance\nplot_importance(Model, max_num_features=10) # top 10 most important features\nplt.show()\n\nprint('\\n\\n-----------------------------------------------------\\n\\n')\narr=confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(arr, range(2), range(2))\nplt.figure(figsize = (8, 8))\nsns.heatmap(arr, cmap = 'Blues', annot = True, fmt = 'd', annot_kws = {'fontsize': 15},\n           yticklabels = ['Stayed', 'Left'], xticklabels = ['Predicted stayed', 'Predicted left'])\nplt.show()","103e55e4":"Model=LogisticRegression(random_state = 22)\n#pipeModel=Pipeline([('scaler', StandardScaler()), ('model', Model)])\n\n# the benefit of using K-Fold is that we could calculate the cross validation value using some of the methods of scoring \nkfold = KFold(n_splits=10, shuffle=True, random_state=0)\ncv_results = cross_val_score(Model, X_train, y_train, cv=kfold)\n\nModel.fit(X_train, y_train)\n\nprint('Score for Logistic Regression method:', Model.score(X_test, y_test))\n\n# the mean result (10 data) of negative mean squared error\nprint('Score for Logistic Regression method using cross_val_score:', cv_results.mean())\ny_pred = Model.predict(X_test)\ny_prob = Model.predict_proba(X_test)[:,1]\n\n#print(mean_squared_error(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\nprint(\"roc_auc score:\", roc_auc_score(y_test,y_prob))\n\n# Roc curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nprint('\\n\\n-----------------------------------------------------\\n\\n')\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\nprint('\\n\\n-----------------------------------------------------\\n\\n')\nplt.figure(figsize = (8, 8))\n\nfeature_importance = abs(Model.coef_[0])\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\n\nfeatfig = plt.figure()\nfeatax = featfig.add_subplot(1, 1, 1)\nfeatax.barh(pos, feature_importance[sorted_idx], align='center')\nfeatax.set_yticks(pos)\nfeatax.set_yticklabels(np.array(X.columns)[sorted_idx], fontsize=8)\nfeatax.set_xlabel('Relative Feature Importance')\n\nplt.tight_layout()   \nplt.show()\n\nprint('\\n\\n-----------------------------------------------------\\n\\n')\narr=confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(arr, range(2), range(2))\nplt.figure(figsize = (8, 8))\nsns.heatmap(arr, cmap = 'Blues', annot = True, fmt = 'd', annot_kws = {'fontsize': 15},\n           yticklabels = ['Stayed', 'Left'], xticklabels = ['Predicted stayed', 'Predicted left'])\nplt.show()","8e565bba":"Model=GradientBoostingClassifier()\n#pipeModel=Pipeline([('scaler', StandardScaler()), ('model', Model)])\n\n# the benefit of using K-Fold is that we could calculate the cross validation value using some of the methods of scoring \nkfold = KFold(n_splits=10, shuffle=True, random_state=0)\ncv_results = cross_val_score(Model, X_train, y_train, cv=kfold)\n\nModel.fit(X_train, y_train)\n\nprint('Score for GBC method:', Model.score(X_test, y_test))\n\n# the mean result (10 data) of negative mean squared error\nprint('Score for GBC method using cross_val_score:', cv_results.mean())\ny_pred = Model.predict(X_test)\ny_prob = Model.predict_proba(X_test)[:,1]\n\n#print(mean_squared_error(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\nprint(\"roc_auc score:\", roc_auc_score(y_test,y_prob))\n\n# Roc curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nprint('\\n\\n-----------------------------------------------------\\n\\n')\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\nprint('\\n\\n-----------------------------------------------------\\n\\n')\nplt.figure(figsize = (8, 8))\n\nfeature_importance = abs(Model.feature_importances_)\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\n\nfeatfig = plt.figure()\nfeatax = featfig.add_subplot(1, 1, 1)\nfeatax.barh(pos, feature_importance[sorted_idx], align='center')\nfeatax.set_yticks(pos)\nfeatax.set_yticklabels(np.array(X.columns)[sorted_idx], fontsize=8)\nfeatax.set_xlabel('Relative Feature Importance')\n\nplt.tight_layout()   \nplt.show()\n\nprint('\\n\\n-----------------------------------------------------\\n\\n')\narr=confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(arr, range(2), range(2))\nplt.figure(figsize = (8, 8))\nsns.heatmap(arr, cmap = 'Blues', annot = True, fmt = 'd', annot_kws = {'fontsize': 15},\n           yticklabels = ['Stayed', 'Left'], xticklabels = ['Predicted stayed', 'Predicted left'])\nplt.show()\n","385b0d14":"#List Hyperparameters yang akan diuji\n#penalty = ['l1', 'l2']\n#C = np.logspace(-4,4,20)\n\n#Menjadikan ke dalam bentuk dictionary\n#hyperparameters = dict(penalty=penalty, C=C)\n\nparam_tuning = {\n    'learning_rate': [0.01, 0.1],\n    'max_depth': [3, 5, 7, 10],\n    'min_child_weight': [1, 3, 5],\n    'subsample': [0.5, 0.7],\n    'colsample_bytree': [0.5, 0.7],\n    'n_estimators' : [100, 200, 500],\n    'objective': ['reg:squarederror']\n}\n\n#Membuat Object Logistic Regression\nlogreg = XGBClassifier(eval_metric=\"logloss\")\n\n#Memasukan ke Grid Search\n#CV itu Cross Validation\n#Menggunakan 10-Fold CV\nclf = GridSearchCV(logreg, param_grid = param_tuning, cv=10)\n\n#Fitting Model\nbest_model = clf.fit(X,y)\n\n#Nilai hyperparameters terbaik\nprint('Best Params:', best_model.best_estimator_.get_params())\n\n#Prediksi menggunakan model baru\n#y_pred = best_model.predict(x_test)\n\n#Check performa dari model\n#print(classification_report(y_test, y_pred))\n#roc_auc_score(y_test, y_pred)","37ecb798":"Model=XGBClassifier(eval_metric=\"logloss\", learning_rate=best_model.best_estimator_.get_params()['learning_rate'],\n                        max_depth=best_model.best_estimator_.get_params()['max_depth'],\n                        min_child_weight=best_model.best_estimator_.get_params()['min_child_weight'],\n                        subsample=best_model.best_estimator_.get_params()['subsample'],\n                        colsample_bytree=best_model.best_estimator_.get_params()['colsample_bytree'],\n                        n_estimators=best_model.best_estimator_.get_params()['n_estimators'],\n                        objective=best_model.best_estimator_.get_params()['objective'])\n#pipeModel=Pipeline([('scaler', StandardScaler()), ('model', Model)])\n\n# the benefit of using K-Fold is that we could calculate the cross validation value using some of the methods of scoring \nkfold = KFold(n_splits=10, shuffle=True, random_state=0)\ncv_results = cross_val_score(Model, X_train, y_train, cv=kfold)\n\nModel.fit(X_train, y_train)\n\nprint('Score for XGBoost method:', Model.score(X_test, y_test))\n\n# the mean result (10 data) of negative mean squared error\nprint('Score for XGBoost method using cross_val_score:', cv_results.mean())\ny_pred = Model.predict(X_test)\ny_prob = Model.predict_proba(X_test)[:,1]\n\n#print(mean_squared_error(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\nprint(\"roc_auc score:\", roc_auc_score(y_test,y_prob))\n\n# Roc curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nprint('\\n\\n-----------------------------------------------------\\n\\n')\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\nprint('\\n\\n-----------------------------------------------------\\n\\n')\n\nplt.figure(figsize = (8, 8))\nfrom matplotlib import pyplot\nfrom xgboost import plot_importance\nplot_importance(Model, max_num_features=10) # top 10 most important features\nplt.show()\n\nprint('\\n\\n-----------------------------------------------------\\n\\n')\narr=confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(arr, range(2), range(2))\nplt.figure(figsize = (8, 8))\nsns.heatmap(arr, cmap = 'Blues', annot = True, fmt = 'd', annot_kws = {'fontsize': 15},\n           yticklabels = ['Stayed', 'Left'], xticklabels = ['Predicted stayed', 'Predicted left'])\nplt.show()","2d6c1fc0":"<a id=\"2\"><\/a>\n    \n# <font size=\"+2\" color=\"indigo\"><b>2. Load Dataset<\/b><\/font><br>","ac6a333b":"## <font size=\"+1\" color=\"indigo\"><b>Affect of Gender on Stroke<\/b><\/font><br> ","120f51c3":"<a id=\"8\"><\/a>\n    \n# <font size=\"+2\" color=\"indigo\"><b>8. Tuning Hyperparameter<\/b><\/font><br>","0fa5a8fb":"<center>\n    <h1>   \n         Clinical Features Model to Predict Stroke \n    <\/h1>\n<\/center>\n \n<center><img src=\"https:\/\/image-cdn.medkomtek.com\/_A7vRjeLyfZqshjKM71BAmEmmKU=\/673x373\/smart\/klikdokter-media-buckets\/medias\/2309095\/original\/050687900_1572589916-Stroke-infarct-By-peterschreiber.media-Shutterstock_1423084877.jpg\" alt=\"Stroke\" width=\"600\" ><\/center>\n\n<center> <h3> Please Upvote if you like the work \ud83d\ude0a <\/h3><\/center>\n\n<center>Don't hesitate to give your comment on this notebook. It gives me motivation to improve the analysis in the future<\/center>\n\n# Introduction\nThis program is used to find the right model for the Telecomunication dataset. \n\n# Contents\nContents:\n\n* [1. Load Packages](#1)\n* [2. Load Dataset](#2)\n* [3. Drop NaN and duplicates values](#3)\n* [4. Do Explanatory Data Analysis](#4)\n* [5. Mapping Sentiment](#5) \n* [6. Modelling](#6)","eb0954f3":"<a id=\"5\"><\/a>\n    \n# <font size=\"+2\" color=\"indigo\"><b>5. Label Encoding<\/b><\/font><br>","82d60743":"<a id=\"7\"><\/a>\n    \n# <font size=\"+2\" color=\"indigo\"><b>7. Modelling<\/b><\/font><br>\n## <font size=\"+1\" color=\"indigo\"><b>Data Splitting<\/b><\/font><br>","cf6c749a":"## <font size=\"+1\" color=\"indigo\"><b>Gradient Boosting Classifier<\/b><\/font><br>","892dc344":"<a id=\"1\"><\/a>\n    \n# <font size=\"+2\" color=\"indigo\"><b>1. Load Packages<\/b><\/font><br>","40cd7aae":"## <font size=\"+1\" color=\"indigo\"><b>Distribution of BMI of People Having Stroke<\/b><\/font><br> \n\nFor certain individuals, BMI is a reliable measure of body fatness. It is used to scan for weight ranges that could be associated with health issues.","58560b49":"## <font size=\"+1\" color=\"indigo\"><b>Affect of Residence_type on Stroke<\/b><\/font><br> ","de7dfe01":"> We need to use SMOTE method in order to overcome the problem of unbalanced data","f476b7bf":"## <font size=\"+1\" color=\"indigo\"><b>XGBClassifier<\/b><\/font><br>","0d0c0b9d":"<a id=\"3\"><\/a>\n    \n# <font size=\"+2\" color=\"indigo\"><b>3. Find Null and Duplicate Values<\/b><\/font><br>","4c23a980":"<a id=\"4\"><\/a>\n    \n# <font size=\"+2\" color=\"indigo\"><b>4. Exploratory Data Analysis<\/b><\/font><br>","668a3121":"<a id=\"6\"><\/a>\n    \n# <font size=\"+2\" color=\"indigo\"><b>6. Exploratory Data Analysis (Part 2)<\/b><\/font><br>","d4ded26f":"> XGBoost gives best results based on accuracy. The hyperparameter need to be tuned.","60f55edd":"## <font size=\"+1\" color=\"indigo\"><b>Distribution of Age of People Having Stroke<\/b><\/font><br> ","e18935ba":"## <font size=\"+1\" color=\"indigo\"><b>Logistic Regression<\/b><\/font><br>","fc942cb5":"> Found null values in bmi columns --> needs to be treated","707b5247":"## <font size=\"+1\" color=\"indigo\"><b>Affect of smoking on Stroke<\/b><\/font><br> "}}