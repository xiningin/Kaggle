{"cell_type":{"b5927e9a":"code","dfe5a0a6":"code","3902c477":"code","09462c82":"code","0e61bba6":"code","301a3224":"code","0886330f":"code","9d56c290":"code","fc6eeb3b":"code","b33e02d0":"code","b0e86845":"code","b8aee9ad":"code","1cf4a508":"code","8e68cdcb":"code","02da0822":"code","38e73d9d":"code","a5398f6a":"code","48f3b52a":"code","3a332082":"code","2914aa11":"code","e7ae5d04":"code","2b8e85d4":"code","a667eea5":"code","e101d4a4":"code","2aef04ea":"code","c8a9ed17":"code","fbaa0ed4":"code","a1fe1a76":"code","00bff8b4":"code","d614096c":"code","67dcbc85":"code","2e73403e":"code","75df929a":"code","cc2d3e7f":"code","88b1db5d":"code","259fa4d2":"code","53d5d60f":"code","812ec400":"code","08720cdc":"code","2e3d6ab2":"code","4ae52be7":"code","37e4f987":"markdown","7e1983e0":"markdown","ea60244c":"markdown","59fbd964":"markdown","f6c3f058":"markdown","398bf983":"markdown","81acd50d":"markdown","38557ec3":"markdown","01c78095":"markdown","0d8db1e3":"markdown","13b9f50f":"markdown","b28566e9":"markdown","3f22aa45":"markdown","0b1e2621":"markdown","f8e292e9":"markdown","e680d3f1":"markdown","85c2c1e5":"markdown","30e4878b":"markdown"},"source":{"b5927e9a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dfe5a0a6":"# We need to install a wide variety of libraries. For this we will install pandas, numpy, seaborn and matplotlib libraries.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport string\nimport re\nsns.set()\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n\nimport matplotlib.pyplot as plt\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Graphics in retina format are more sharp and legible\n%config InlineBackend.figure_format = 'retina'","3902c477":"df = pd.read_csv(\"\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv\", encoding=\"latin-1\")\n\ndf = df.dropna(how=\"any\", axis=1)\ndf.columns = ['target', 'message']\n\ndf.head()","09462c82":"df.describe()","0e61bba6":"df['message_n_chars'] = df.message.apply(len) # count all chars in each sentence\ndf['message_n_words'] = df.message.apply(lambda sent: len(sent.split())) # count number of words in each sentence\ndf.head()","301a3224":"max(df.message_n_chars), max(df.message_n_words)","0886330f":"# Convert target to numerical variable\ndf['lable'] = df.target.map({'ham': 0,\n                            'spam': 1})\ndf.head()","9d56c290":"df['target'].value_counts(normalize= True)","fc6eeb3b":"sns.countplot(data = df, x = 'target');","b33e02d0":"sns.histplot(data= df, x= 'message_n_words', hue= 'target', multiple= 'stack');","b0e86845":"stop_words = stopwords.words('english')\nstemmer    = nltk.SnowballStemmer(\"english\")","b8aee9ad":"def clean_text(text):\n    '''\n        Make text lowercase, remove text in square brackets,remove links,remove punctuation\n        and remove words containing numbers.\n    '''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text) # remove urls\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","1cf4a508":"def preprocess_data(text):\n    text = clean_text(text)                                                     # Clean puntuation, urls, and so on\n    text = ' '.join(word for word in text.split() if word not in stop_words)    # Remove stopwords\n    text = ' '.join(stemmer.stem(word) for word in text.split())                # Stemm all the words in the sentence\n    return text\n","8e68cdcb":"df['message_clean'] = df['message'].apply(preprocess_data)\ndf.head()\n","02da0822":"X = df['message_clean']\ny = df['lable']\n\nX.shape, y.shape","38e73d9d":"# split into train and test sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,  random_state=42)\nlen(X_train), len(y_train), len(X_test), len(y_test)","a5398f6a":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\n\nX_train_cv = cv.fit_transform(X_train)\nX_test_cv = cv.transform(X_test)","48f3b52a":"from sklearn.feature_extraction.text import TfidfTransformer\ntfidf = TfidfTransformer()\nX_train_tfidf = tfidf.fit_transform(X_train_cv)","3a332082":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(X_train_cv, y_train)","2914aa11":"# predict classes\ny_pred = nb.predict(X_test_cv)\ny_pred_proba = nb.predict_proba(X_test_cv)[:, 1]","e7ae5d04":"from sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score","2b8e85d4":"p, r, f, _ = precision_recall_fscore_support(y_test, y_pred,\n                                                 average='macro')\nprint(f'precision is  -> {round(p,2)}')\nprint(f'recall is -> {round(r,2)}')\nprint(f'f1_score is -> {round(f,2)}')","a667eea5":"# calculate the accuracy \nacc = accuracy_score(y_test, y_pred)\nprint(f'Accuracy is -> {round(acc,2)}')","e101d4a4":"cm = confusion_matrix(y_test, y_pred)\nprint('The confusion_matrix is: \\n', cm);","2aef04ea":"from mlxtend.plotting import plot_confusion_matrix\n\nfig, ax = plot_confusion_matrix(conf_mat= cm, colorbar=True)\nplt.show()","c8a9ed17":"roc_auc = roc_auc_score(y_test, y_pred_proba)\nprint(f'The roc_auc Score is -> {round(roc_auc,2)}')","fbaa0ed4":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([('bow', CountVectorizer()), \n                 ('tfidf', TfidfTransformer()),  \n                 ('model', MultinomialNB())])\n\npipe.fit(X_train, y_train)","a1fe1a76":"y_pred = pipe.predict(X_test)","00bff8b4":"accuracy_score(y_test, y_pred)","d614096c":"confusion_matrix(y_test, y_pred)","67dcbc85":"import xgboost as xgb\n\npipe = Pipeline([\n                ('bow', CountVectorizer()), \n                ('tfidf', TfidfTransformer()),  \n                ('model', xgb.XGBClassifier(\n                    learning_rate=0.1,\n                    max_depth=7,\n                    n_estimators=80,\n                    use_label_encoder=False,\n                    eval_metric='auc'))\n                ])\n","2e73403e":"# Fit the pipeline with the data\npipe.fit(X_train, y_train)\n\ny_pred = pipe.predict(X_test)\n# y_pred_train = pipe.predict(X_train)\n\n# print('Train: {}'.format(accuracy_score(y_train, y_pred_train)))\nprint(f'Accuracy Score For Test Data: -> {round(accuracy_score(y_test, y_pred),2)}')\nprint(f'Confusion Matrix Score For Test Data:\\n {confusion_matrix(y_test, y_pred)}')","75df929a":"# calculate the accuracy \nacc = accuracy_score(y_test, y_pred)\nprint(f'Accuracy is -> {round(acc,2)}')","cc2d3e7f":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer\n","88b1db5d":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","259fa4d2":"def bert_encode(input_text, max_len):\n    input_ids = []\n    attension_masks = []\n    for text in input_text:\n        output_dict = tokenizer.encode_plus(\n            text, \n            add_special_tokens = True,\n            truncation=True,\n            max_length = max_len,\n            pad_to_max_length = True,\n            return_attention_mask = True\n        )\n        input_ids.append(output_dict['input_ids'])\n        attension_masks.append(output_dict['attention_mask'])\n    return np.array(input_ids), np.array(attension_masks)","53d5d60f":"text = df['message_clean']\ntarget = df['lable']\ntrain_input_ids, train_attention_masks = bert_encode(text, 60)\n","812ec400":"def create_model(bert_model):\n    input_ids = tf.keras.Input(shape= (60,), dtype= 'int32')\n    attention_masks = tf.keras.Input(shape= (60,), dtype= 'int32')\n    \n    output = bert_model([input_ids, attention_masks])\n    output = output[1]\n    output = tf.keras.layers.Dense(32, activation= 'relu')(output)\n    output = tf.keras.layers.Dropout(0.2)(output)\n    output = tf.keras.layers.Dense(1, activation= 'sigmoid')(output)\n    \n    model = tf.keras.models.Model(inputs= [input_ids, attention_masks], outputs= output)\n    model.compile(Adam(lr=1e-5), loss= 'binary_crossentropy', metrics= ['accuracy'])\n    return model","08720cdc":"from transformers import TFBertModel\nbert_model = TFBertModel.from_pretrained('bert-base-uncased')","2e3d6ab2":"model = create_model(bert_model)\nmodel.summary()","4ae52be7":"%%time\n\nhistory = model.fit(\n    [train_input_ids, train_attention_masks],\n    target, \n    validation_split = 0.2,\n    epochs = 3,\n    batch_size = 10\n)","37e4f987":"**The distribution of number of chars and number of words in each sentence**","7e1983e0":"**CountVectorizer**","ea60244c":"## BERT \"Bidirectional Encoder Representations from Transformers\"","59fbd964":"### XGboost","f6c3f058":"## 2- Exploratory Data Analysis (EDA)","398bf983":"I can also using `AutoTokenizer`\n\nthe `AutoTokenizer` class **will grab the proper tokenizer class in the library based on the checkpoint** name, and can be used directly with any checkpoint:\n\n","81acd50d":"### BOOOM!!\nThe accuracy score of *BERT* reach to **`0.99`**","38557ec3":"## 3- Data Pre-processing\n* Cleaning \n* removing stop words\n* stemming or lemma\n","01c78095":"**Tf-idf**\n\n`TF-IDF` **Term Frequency\u2013Inverse Document Frequency**, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\nThe tf\u2013idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.","0d8db1e3":"## 4- Vectorization\n\nCurrently, we have the messages as lists of tokens and now we need to convert each of those messages into a `vector` the models can work with.\n\nFirst, we need to split data into train data and test data for applying modeling on it.","13b9f50f":"**Stemming\/ Lematization**\n\n\nFor grammatical reasons, documents are going to use different forms of a word, such as write, writing and writes. Additionally, there are families of derivationally related words with similar meanings. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n\n* **Stemming** usually refers to a process that chops off the ends of words in the hope of achieving goal correctly most of the time and often includes the removal of derivational affixes.\n\n* **Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base and dictionary form of a word\n\n","b28566e9":"### Make it by pipline","3f22aa45":"The **multinomial Naive Bayes** and the **XGboost** have the same accuracy score *`0.97`*","0b1e2621":"## 1- Loading the data.","f8e292e9":"**`XGboost with TF-idf`**","e680d3f1":"**`MultinomialNB with TF-idf`**","85c2c1e5":"### multinomial Naive Bayes\nWe will use `multinomial Naive Bayes`\n\n\nThe multinomial Naive Bayes classifier is suitable for classification with discrete features","30e4878b":"## 5- Modeling"}}