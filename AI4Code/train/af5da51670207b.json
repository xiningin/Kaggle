{"cell_type":{"7de9099d":"code","d800603b":"code","842ff29a":"code","564c6173":"code","f0407196":"code","9d98e7dc":"code","5db89cb7":"code","455612eb":"code","c00a9fa5":"code","293e4389":"code","90b9b19b":"code","0dc48802":"code","e24fec6d":"code","0b5518dd":"code","4c12f2c6":"code","ef637c6f":"code","79e60a97":"code","e5401e39":"code","a982b900":"code","3f0a4206":"code","a7b40564":"code","a65e055c":"code","5a8d7a2b":"code","73c878fe":"code","45bcfed7":"code","b99d4e13":"code","83a57633":"code","f7277553":"code","8f801b5d":"code","75cd7a85":"code","6f9f5604":"code","c63c37a7":"code","dc0b78d5":"code","b5afeaf7":"code","0997394f":"code","2bf1dcde":"code","d0b3a6bf":"code","ade702da":"code","40ada038":"code","753a235f":"markdown","deaacb46":"markdown","0c0919a8":"markdown","10cbd8f9":"markdown","2e9a5a11":"markdown","f0d2e4b3":"markdown"},"source":{"7de9099d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nimport matplotlib.pyplot as plt\nimport datetime\nfrom sklearn import metrics\nimport seaborn as sns\nimport sklearn.ensemble as ske\nimport gc\nfrom sklearn.linear_model import LinearRegression,SGDRegressor,ElasticNet,Ridge\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nDATA_PATH = \"..\/input\/ashrae-energy-prediction\/\"\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d800603b":"Weather_Train = pd.read_csv(DATA_PATH + 'weather_train.csv')\nWeather_Test= weather_df = pd.read_csv(DATA_PATH + 'weather_test.csv')\nTest= pd.read_csv(DATA_PATH +'test.csv')\nTrain= pd.read_csv(DATA_PATH +'train.csv')\nBuilding= pd.read_csv(DATA_PATH +'building_metadata.csv')","842ff29a":"#Function that prints Shape of the data, Columns and their datatype, Head of the data, Description of the data, number of nulls in the data\n# and Year of the data and The Percentage of missing values in the dataset\ndef Data_info(filename):\n    path = \"..\/input\/ashrae-energy-prediction\"\n    df = pd.read_csv('{0}\/{1}'.format(path,filename))\n    print(\"~~~~~~Shape of the data~~~~~~ : \",df.shape)\n    print(\"~~~~~~Columns and their datatype~~~~~~ : \")\n    print(df.info())\n    print(\"~~~~~~Quick Look at the data~~~~~~ : \")\n    print(df.head())\n    print(\"~~~~~~Description of the data~~~~~~ : \")\n    print(df.describe())\n    print(\"~~~~~~number of nulls in the data~~~~~~ : \")\n    print(df.isna().sum())\n    if 'timestamp' in df.columns: \n        df['timestamp'] = pd.to_datetime(df['timestamp'],format = \"%Y-%m-%d %H:%M:%S\")\n        print(\"~~~~~~Year of the data~~~~~~ :\")\n        print(df.timestamp.dt.year.unique())\n    print(\"~~~~~~Percentage of missing values in the dataset~~~~~~\")\n    print(df.isna().sum()\/len(df)*100)\n    return df","564c6173":"#Using (\"Data_info\") function to get 'train.csv' info\ntrain = Data_info('train.csv')","f0407196":"#Using (\"Data_info\") function to get 'test.csv' info\ntest = Data_info('test.csv')","9d98e7dc":"#Using (\"Data_info\") function to get 'weather_train.csv' info\nwtrain = Data_info('weather_train.csv')","5db89cb7":"#Using (\"Data_info\") function to get 'weather_test.csv' info\nwtest = Data_info('weather_test.csv')","455612eb":"#Using (\"Data_info\") function to get 'building_metadata.csv' info\nbuilding = Data_info('building_metadata.csv')","c00a9fa5":"#Function That reduces the used memory\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    return df","293e4389":"#using reduce_mem_usage function to reduce memory usage\nTrain = reduce_mem_usage(Train)\nTest = reduce_mem_usage(Test)\nWeather_Train = reduce_mem_usage(Weather_Train)\nWeather_Test = reduce_mem_usage(Weather_Test)\nBuilding = reduce_mem_usage(Building)","90b9b19b":"#Merging tables\nresults = Building.merge(Train,left_on='building_id',right_on='building_id',how='left')\ndata = results.merge(Weather_Train,left_on=['site_id','timestamp'],right_on=['site_id','timestamp'],how='left')\ndata.columns","0dc48802":"#Function that sets up the hist plot and its dimension\ndef plot_hist(df,var_name):\n    plt.figure(figsize=(17,8))\n    plt.hist(df[var_name],bins = 50)\n    plt.title(f\"Histogram - {var_name}\")\n    plt.show()","e24fec6d":"#Hist plot for 'year_built'\nplot_hist(Building,'year_built')","0b5518dd":"#Hist plot for 'floor_count'\nplot_hist(Building,'floor_count')","4c12f2c6":"#Hist plot for 'cloud_coverage'\nplot_hist(data,'cloud_coverage')","ef637c6f":"#Hist plot for 'wind_speed'\nplot_hist(data,'wind_speed')","79e60a97":"#Hist plot for 'dew_temperature','air_temperature' & 'wind_speed'\nfor var in ['dew_temperature','air_temperature','wind_speed']:\n    plot_hist(Weather_Train,var)","e5401e39":"#Scatter plot between Air Temperature and Meter Reading\nsns.scatterplot(x='air_temperature', y='meter_reading', data=data)","a982b900":"#Scatter plot between Cloud Coverage and Meter Reading\n#sns.scatterplot(x='cloud_coverage', y='meter_reading', hue='year_built',data=data)\n#it works but it takes a lot of time to run","3f0a4206":"#Boxplots for Meter reading for Floorcounts\nfig, axes = plt.subplots(1, 1, figsize=(14, 6))\nsns.boxplot(x='floor_count', y='meter_reading', data=data, showfliers=False);","a7b40564":"#Boxplots for 'site_id' and 'dew_temperature'\nfig, axes = plt.subplots(1, 1, figsize=(14, 6))\nsns.boxplot(x='site_id', y='dew_temperature', data=data, showfliers=False);","a65e055c":"#Heatmap for the correlation between the columns and themselvs\ncorrmat=data.corr()\nfig,ax=plt.subplots(figsize=(12,10))\nsns.heatmap(corrmat,annot=True,annot_kws={'size': 12})","5a8d7a2b":"#dropping unnecessary columns\ndata = data.drop(columns=['year_built', 'floor_count', 'wind_direction', 'dew_temperature'])","73c878fe":"#fixing timestamp and taking only the day and the month and then dropping timestamp column\ndata[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\ndata[\"day\"]= data[\"timestamp\"].dt.day\ndata[\"month\"]= data[\"timestamp\"].dt.month\ndata= data.drop(\"timestamp\", axis = 1)","45bcfed7":"#Printing the \"Percentage of missing values\" to see what is going to be dropped and what is going to be filled\nprint(\"Percentage of missing values in the data dataset\")\ndata.isna().sum()\/len(data)*100","b99d4e13":"# Since there is a problem in printing the columns, instead printing head of the data besides the columns inside (\"data\")\ndata.head()","83a57633":"#setting a list of integer ranging from 0 to length of data as index\ndata = data.reset_index()","f7277553":"#Change data type to float 32 for filling NA value before transforming them into int for smooth modeling processing\ndata['wind_speed'] = data['wind_speed'].astype('float32')\ndata['air_temperature'] = data['air_temperature'].astype('float32')\ndata['precip_depth_1_hr'] = data['precip_depth_1_hr'].astype('float32')\ndata['cloud_coverage'] = data['cloud_coverage'].astype('float32')","8f801b5d":"#Filling Null Values\ndata['precip_depth_1_hr'].fillna(data['precip_depth_1_hr'].mean(), inplace = True)\ndata['cloud_coverage'].fillna(data['cloud_coverage'].mean(), inplace = True)\ndata['wind_speed'].fillna(data['wind_speed'].mean(), inplace=True)\ndata['air_temperature'].fillna(data['air_temperature'].mean(), inplace=True)\n\n# Printing the sum of nulls inside the columns\ndata.isnull().sum()","75cd7a85":"#printing the shape of the \"results\"\nresults.shape","6f9f5604":"#printing the columns of the \"results\"\nresults.columns","c63c37a7":"#printing the shape of the \"data\"\ndata.shape","dc0b78d5":"#printing the shape of the \"data\"\ndata.columns","b5afeaf7":"# Here column 'primaty_use' was treated by get_dummies function and get_dummies is used for data manipulation\ndata_linearR = pd.get_dummies(data, columns=['primary_use'])","0997394f":"#printing the columns of \"data_linearR\"\ndata_linearR.columns","2bf1dcde":"#Using the important features \nX =data_linearR[['building_id', 'meter', 'air_temperature', 'wind_speed', 'precip_depth_1_hr', 'cloud_coverage',\n       'square_feet', 'primary_use_Education', 'primary_use_Entertainment\/public assembly',\n       'primary_use_Food sales and service', 'primary_use_Healthcare',\n       'primary_use_Lodging\/residential',\n       'primary_use_Manufacturing\/industrial', 'primary_use_Office',\n       'primary_use_Other', 'primary_use_Parking',\n       'primary_use_Public services', 'primary_use_Religious worship',\n       'primary_use_Retail', 'primary_use_Services',\n       'primary_use_Technology\/science', 'primary_use_Utility',\n       'primary_use_Warehouse\/storage', 'month', 'day']]\n\n# Create target variable\ny = data_linearR['meter_reading']\n\n# Train, test, split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .20, random_state= 0)","d0b3a6bf":"# Create linear regression object\nregressor = LinearRegression()\n\n# Fit model to training data\nregressor.fit(X_train,y_train)","ade702da":"# Predicting test set results\ny_pred = regressor.predict(X_test)","40ada038":"#Printing R^2 and the accuracy of the linear model\nprint('Accuracy %d', regressor.score(X_test, y_test))\nprint('R^2 =', metrics.explained_variance_score(y_test,y_pred))\nregressor.score(X_train,y_train)","753a235f":"* **Features Engineering**","deaacb46":"* **** About the data","0c0919a8":"Abdulrahman Emad 18101010\nMohamed Yasser Anwar 18100585\nMohamed Ashraf 18100765","10cbd8f9":"* **EDA**","2e9a5a11":"* **Linear Model**","f0d2e4b3":"* **** Loading Data"}}