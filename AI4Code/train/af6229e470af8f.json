{"cell_type":{"d1860060":"code","a4dd57ca":"code","6c56938b":"code","b2d7317a":"code","8a1fb495":"code","35c48c2d":"code","a2e03ebc":"code","b381b109":"code","bc3debad":"code","8d7e7d6a":"code","7abde482":"code","3ad9bb02":"code","79ed4adb":"code","5533a86c":"code","9395437a":"code","4715abc0":"code","8f6d72af":"code","01b8cb00":"code","9447cd51":"code","5c9ae90f":"code","ead7fa51":"code","85a5f18d":"code","d67c695c":"markdown","fb2275a1":"markdown","57921c40":"markdown","9367916a":"markdown","d0a614f9":"markdown","c5d6e057":"markdown","f2a879ee":"markdown"},"source":{"d1860060":"#Packages\nimport numpy as np \nimport pandas as pd \nfrom matplotlib import pyplot as plt \nimport seaborn as sns \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer","a4dd57ca":"train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\nssub = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","6c56938b":"train.head(5)","b2d7317a":"print(f'train shape: {train.shape}, test shape: {test.shape}') #\u0421\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0437\u043c\u0435\u0440\u044b, \u043d\u0430 \u0432\u0441\u044f\u043a\u0438\u0439 \u0441\u043b\u0443\u0447\u0430\u0439, \u0432\u0434\u0440\u0443\u0433 \u0447\u0442\u043e \u0442\u043e \u043f\u043e\u0439\u0434\u0435\u0442 \u043d\u0435 \u0442\u0430\u043a","8a1fb495":"plt.rcParams['figure.dpi'] = 600\nfeatures = [col for col in train.columns if col not in ['id', 'target']]#['f0','f1','f2','f3', 'f4','f5','f6','f7', 'f8', 'f9']\nfeatures = features[:50]\nfig = plt.figure(figsize=(18, 20), facecolor='#f6f5f5')\ngs = fig.add_gridspec(10, 5)\ngs.update(wspace=0.3, hspace=0.3)\nbackground_color = \"#f6f5f5\"\nrun_no = 0\nfor row in range(0, 10):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n        \nax0.text(-4.5, 520500, 'First 50 train-test distributions', fontsize=16, fontweight='bold')\nax0.text(-4.5, 460000, 'feature_0 - feature_49', fontsize=10, fontweight='light')        \n        \nrun_no = 0\nfor col in features:\n    sns.set_palette(['#00A4CCFF'])\n    sns.histplot(x=train[col], ax=locals()[\"ax\"+str(run_no)], color='#ff5573',zorder=2, linewidth=0.6, alpha=0.9, label='Train data', bins=50)\n    sns.histplot(x=test[col], ax=locals()[\"ax\"+str(run_no)], label='Test data',zorder=2, linewidth=0.6, alpha=0.9, bins=50)\n    locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=8, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=5, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1","35c48c2d":"plt.rcParams['figure.dpi'] = 600\nfeatures = [col for col in train.columns if col not in ['id', 'target']]#['f0','f1','f2','f3', 'f4','f5','f6','f7', 'f8', 'f9']\nfeatures = features[50:]\nfig = plt.figure(figsize=(18, 20), facecolor='#f6f5f5')\ngs = fig.add_gridspec(10, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nbackground_color = \"#f6f5f5\"\nrun_no = 0\nfor row in range(0, 10):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n        \nax0.text(-2, 30500, 'Second 50 train-test distributions', fontsize=16, fontweight='bold')\nax0.text(-2, 27000, 'feature_50 - feature_99', fontsize=10, fontweight='light')        \n        \nrun_no = 0\nfor col in features:\n    sns.set_palette(['#00A4CCFF'])\n    sns.histplot(x=train[col], ax=locals()[\"ax\"+str(run_no)], color='#ff5573',zorder=2, linewidth=0.6, alpha=0.9, label='Train data', bins=50)\n    sns.histplot(x=test[col], ax=locals()[\"ax\"+str(run_no)], label='Test data',zorder=2, linewidth=0.6, alpha=0.9, bins=50)\n    locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=8, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=5, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n    ","a2e03ebc":"print(f'Count of missing values in train: {train.isna().sum().sum()}')\nprint(f'Count of missing values in test: {test.isna().sum().sum()}')","b381b109":"fig = plt.figure(figsize=(12, 6), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 2)\n\nax0 = fig.add_subplot(gs[0, 1])\nax0.set_facecolor(background_color)\nfor s in [\"top\",\"right\"]:\n            ax0.spines[s].set_visible(False)\nsns.barplot(ax=ax0, y=train['target'].value_counts(), x=train['target'].unique())\nax0.text(-0.5, 350000, 'Target values balance', fontsize=16, fontweight='bold')\n#ax0.text(-0.5, 335000, 'feature_50 - feature_99', fontsize=10, fontweight='light')        \n","bc3debad":"fig = plt.figure(figsize=(18, 8), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 2)\n\nax0 = fig.add_subplot(gs[0, 1])\nax0.set_facecolor(background_color)\nsns.heatmap(train.corr(), ax=ax0)\nax0.text(0, -4, 'Correlation matrix', fontsize=16, fontweight='bold')\n","8d7e7d6a":"correl = train.corr()['target']","7abde482":"abs(correl).sort_values(ascending=False).head(10)","3ad9bb02":"train.var()\nhigh_var_col_list = ['f2', 'f35', 'f44', 'id', 'is_test']\n\ndef get_preparation(train, test, is_quntile=True):\n    \"\"\"\nDocstring:\nFunction to get data preparation. \n\nParameters\n----------\ntrain: :class:`pandas.DataFrame`\n        Input data structure, a train data that can be \n        transform or scalable\ntest: test df\n        Input data structure, a test data that can be \n        transform or scalable\nis_quntile: bool, default=True\n        Set to False to switch quantileTransformation \n        to StandardScale\n\nReturns\n-------\ntrain: :class:`pandas.DataFrame`\n    Inputed prepared train dataframe\ntest: :class:`pandas.DataFrame`\n    Inputed prepared test dataframe\n    \"\"\"\n    target = train['target']\n    train = train.drop(columns='target')\n    train.loc[:, 'is_test'] = 0\n    test.loc[:, 'is_test'] = 1\n    df = pd.concat([train, test])\n    if is_quntile == True:\n        colNames=[col for col in test.columns if col not in high_var_col_list]\n        for col in colNames:\n            qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n            df[col] = qt.fit_transform(df[[col]])\n            #test[col] = qt.transform(test[[col]])\n            #print(df[col])\n        test=df[df['is_test']==1]\n        train=df[df['is_test']==0]\n        train.loc[:, 'target'] = target\n    else:\n        scaler = StandardScaler()\n        colNames=[col for col in test.columns if col not in 'id']\n        train[cols] = scaler.fit_transform(train[cols])\n        test[cols] = scaler.transform(test[cols])\n    \n    return train, test","79ed4adb":"train, test = get_preparation(train, test)","5533a86c":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n","9395437a":"reduce_mem_usage(train)\nreduce_mem_usage(test)","4715abc0":"#!pip install umap-learn\n#import umap\n#colNames=[col for col in test.columns if col not in 'id']\n#reducer = umap.UMAP(random_state=42, n_components=2, low_memory=True)\n#embedding = reducer.fit_transform(train)\n\n#\u041d\u0435 \u0445\u0432\u0430\u0442\u0430\u0435\u0442 \u041e\u0417\u0423\n'''\nTo perform a dimension reduction using UMAP, you need from 16GB of RAM \n(This dataset may need more than 24)\nYou can use low_memory=True, but it also doesn't always help\n'''","8f6d72af":"colNames=[col for col in test.columns if col not in ['id', 'target']]\nX = train[colNames]\ny = train['target']\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","01b8cb00":"skf.get_n_splits(X, y)","9447cd51":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.linear_model import LogisticRegression\n#\u0414\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f, \u0432\u0437\u044f\u043b \u0443 \u043a\u043e\u0433\u043e \u0442\u043e \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u0430\u044f \u0438\u0437 sklearn \u043a\u0440\u0438\u0432\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442\ndef get_auc(y_true, y_hat):\n    fpr, tpr, _ = roc_curve(y_true, y_hat)\n    score = auc(fpr, tpr)\n    return score\n\n#\u041a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\u044f, \u0442\u043e\u0436\u0435 \u0441\u0443\u043f\u0435\u0440 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e. \u041e\u0434\u043d\u0430\u043a\u043e \u0435\u0441\u043b\u0438 \u0432\u044b \u043d\u0435 \u0434\u0435\u043b\u0430\u043b\u0438 \u043c\u043e\u0434\u0435\u043b\u044c\u0438\u043d\u0433, \u043e\u043d\u0430 \u0431\u0443\u0434\u0435\u0442 \u0440\u0443\u0433\u0430\u0442\u044c\u0441\u044f, \u0447\u0442\u043e \u0432\u044b \u0437\u0430\u0441\u0442\u0440\u044f\u043b\u0438 \u0432 \u043b\u043e\u043a. \u043c\u0438\u043d\u0438\u043c\u0443\u043c\u0435\npred = []\nfor fold, (idx1, idx2) in enumerate(skf.split(X, y)):\n    Xtrain, ytrain = X.iloc[idx1], y.iloc[idx1]\n    Xvalid, yvalid = X.iloc[idx2], y.iloc[idx2]\n    \n    lr = LogisticRegression(max_iter=20000,penalty='l2')\n    \n    lr.fit(Xtrain, ytrain)\n    \n    valid_pred = lr.predict(Xvalid)\n    test_pred = lr.predict(test[colNames])\n    \n    pred.append(test_pred)\n    \n    #acc_score = accuracy_score(yvalid, valid_pred)\n    \n    print(f'Fold: {fold} | roc auc score: {get_auc(yvalid, valid_pred)} | Model: Logisctic regression')","5c9ae90f":"lr_cv_preds = pd.DataFrame({f'fold {i}': pred for i, pred in zip(range(5), pred)}) # \u0414\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0438\u043a\u0442 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0444\u043e\u043b\u0434\u0430 \u0438 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\nlr_cv_preds.shape #\u041f\u0440\u043e\u0432\u0435\u0440\u043e\u0447\u043a\u0430 \u043d\u0430 \u0432\u0441\u044f\u043a\u0438\u0439 \u0441\u043b\u0443\u0447\u0430\u0439\n","ead7fa51":"ssub['target'] = lr_cv_preds['fold 4'] #\u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u043b\u0443\u0447\u0448\u0438\u0439 \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0435 \u0438\u0437 \u0444\u043e\u043b\u0434\u043e\u0432, \u0430 \u0442\u0430\u043a \u0436\u0435 \u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0447\u0442\u043e \u043d\u0430\u0448\u0438 \u0444\u043e\u043b\u0434\u044b \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b, \u0437\u043d\u0430\u0447\u0438\u0442 \u0434\u0430\u043d\u043d\u044b\u0435 \u0445\u043e\u0440\u043e\u0448\u043e \u043f\u043e\u0447\u0438\u0441\u0442\u0438\u043b\u0438\nssub.set_index('id').to_csv('submit_lr_cv1.csv')","85a5f18d":"ssub['target']","d67c695c":"### EDA\nHere we look at our data and try to explore it.  \nWhat can we understand from our data:\n1. There is a lot of data, you will need to deal with it somehow, there will be problems with RAM\n2. The data is distributed non-linearly, we see some lognormal and bimodal distributions.\n3. The test and train data are practically not distorted\n\n__Miniconclusion:__  \nBinomial distributions are dangerous, they can become a problem for further learning. We may get stuck in a local minimum. We can get rid of it, but lose a little in quality. It is also not clear about the data with a large excess, I want to scale them somehow.\n\n__\u0421\u0441\u044b\u043b\u043a\u0438:__   [Rus]  \nhttps:\/\/www.kaggle.com\/ankitkalauni\/tps-nov-21-logistic-regression-autoviz - \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 EDA   \nhttps:\/\/www.kaggle.com\/dwin183287 - \u041a\u0440\u0430\u0441\u0438\u0432\u044b\u0435 \u0433\u0440\u0430\u0444\u0438\u043a\u0438  \nhttps:\/\/www.kaggle.com\/subinium\/all-you-need-is-pandas-benchmark-viz - \u041d\u0435\u043c\u043d\u043e\u0433\u043e \u0430\u043d\u0430\u043b\u0438\u0437\u0430","fb2275a1":"# Tabular Playground Series - Nov 2021\n## Part 1 - baseline \n__Content:__  \n1. EDA \n2. Data cleaning\n3. Data preparation \n4. Getting baseline model\n \n_This dataset is a binary classification task with nonlinear data, basically the problem will be modeling and stacking models, as well as testing neural networks (which do well in classification tasks)_\n\n__Links:__  \nTelegram: https:\/\/t.me\/notedatascience\n\n__Some links for beginners__ [Rus lang]  \n\u042d\u0442\u043e\u0442 \u043d\u043e\u0443\u0442\u0431\u0443\u043a \u0434\u043b\u044f \u0442\u0435\u0445, \u043a\u0442\u043e \u0443\u0436\u0435 \u0447\u0442\u043e \u0442\u043e \u0443\u043c\u0435\u0435\u0442 \u0432 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u043c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435. \u041d\u043e \u0435\u0441\u043b\u0438 \u0432\u044b \u043d\u043e\u0432\u0438\u0447\u0435\u043a, \u0432 \u044d\u0442\u043e\u043c \u0432\u0441\u0435\u043c \u043b\u0435\u0433\u043a\u043e \u0440\u0430\u0437\u043e\u0431\u0440\u0430\u0442\u044c\u0441\u044f.\n\u042f \u043e\u0441\u0442\u0430\u0432\u043b\u044e \u043f\u0430\u0440\u0443 \u043f\u043e\u043b\u0435\u0437\u043d\u044b\u0445 \u0441\u0441\u044b\u043b\u043e\u043a \u043a\u0430\u043a \u0434\u043b\u044f \u043d\u0430\u0447\u0438\u043d\u0430\u044e\u0449\u0438\u0445 \u0442\u0430\u043a \u0436\u0435 \u0438 \u0434\u043b\u044f \u0442\u0435\u0445 \u043a\u0442\u043e \u0443\u0436\u0435 \u043f\u0440\u0435\u0438\u0441\u043f\u043e\u043b\u043d\u0438\u043b\u0441\u044f \u0432 \u0441\u0432\u043e\u0435\u043c \u0441\u043e\u0437\u043d\u0430\u043d\u0438\u0438.\n1. https:\/\/www.youtube.com\/c\/GlebMikhaylov\/videos - \u0421\u0443\u043f\u0435\u0440 \u043a\u0440\u0443\u0442\u043e\u0439 \u0447\u0443\u0432\u0430\u043a, \u0437\u0430\u0445\u043e\u0434\u0438\u0442\u0435 \u0432 \u043f\u043b\u0435\u0439\u043b\u0438\u0441\u0442\u044b \u0442\u0430\u043c \u043a\u0443\u0447\u0430 \u0432\u0441\u0435\u0433\u043e\n2. https:\/\/www.youtube.com\/channel\/UCCbgOIWdmYncvYMbl3LjvBQ - \u041f\u0440\u0438\u043a\u043e\u043b\u044c\u043d\u043e \u043e\u0431\u044a\u044f\u0441\u043d\u044f\u0435\u0442\n3. https:\/\/www.youtube.com\/c\/CompscicenterRu\/videos - \u0413\u0430\u0439\u0434\u044b \u0444\u0443\u043d\u0434\u0430\u043c\u0435\u043d\u0442\u0430\u043b\u044c\u043d\u043e, \u043d\u043e \u0438\u043d\u043e\u0433\u0434\u0430 \u0441\u043a\u0443\u0447\u043d\u043e\n4. https:\/\/www.youtube.com\/channel\/UCoTBTCesuoWxHFHQlhmdaiQ - \u041f\u0440\u0438\u043a\u043e\u043b\u044c\u043d\u044b\u0439 \u0447\u0435\u043b, \u043d\u043e \u043d\u0430 \u0442\u0440\u043e\u0435\u0447\u043a\u0443  \n__\u041a\u0443\u0440\u0441\u044b:__   \n5. https:\/\/www.youtube.com\/watch?v=HLg4EpeqZP0&list=PLEwK9wdS5g0pAJyqWc9bKj1JjARM_jnzx - \u0424\u041a\u041d \u0412\u0428\u042d, \u043a\u0440\u0443\u0442\u043e\u0439 \u0438 \u043f\u043e\u043d\u044f\u0442\u043d\u044b\u0439 \u043a\u0443\u0440\u0441 \u043e\u0442 \u0416\u0435\u043d\u0438 \u0421\u043e\u043a\u043e\u043b\u043e\u0432\u0430, \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0440\u0435\u0441\u043f\u0435\u043a\u0442  \n6. https:\/\/www.coursera.org\/specializations\/machine-learning-from-statistics-to-neural-networks? - \u041f\u0440\u0438\u044f\u0442\u043d\u044b\u0439 \u043a\u0443\u0440\u0441, \u043d\u043e \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0431\u0430\u0433\u043d\u0443\u0442\u044b\u0439, \u0432\u0440\u044f\u0434 \u043b\u0438 \u0432\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u0435 \u0441\u0435\u0440\u0442\u0438\u0444\u0438\u043a\u0430\u0442, \u043d\u043e \u043c\u043d\u043e\u0433\u043e\u043c\u0443 \u043d\u0430\u0443\u0447\u0438\u0442\u0435\u0441\u044c. \u041f\u043e \u043a\u0440\u0430\u0439\u043d\u0435\u0439 \u043c\u0435\u0440\u0435 \u0422\u0412\u0418\u041c\u0421 \u044f \u0432\u044b\u0443\u0447\u0438\u043b \u0431\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u044f \u044d\u0442\u043e\u043c\u0443 \u043a\u0443\u0440\u0441\u0443","57921c40":"# Models generation\n### baseline \nWe get the baseline model - this is the very first and standard model from which we will move on.  \nStandard - Logistic regression  \nWe will take StratifiedKFold - also standard, there is nothing to invent bicycles here.  \nAUC is the metric of this competition, so we will use it.  \n__\u0421\u0441\u044b\u043b\u043a\u0438:__  \nhttps:\/\/habr.com\/ru\/company\/io\/blog\/265007\/  \nhttps:\/\/habr.com\/ru\/post\/550978\/ - \u0414\u043b\u044f \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438  \nhttps:\/\/habr.com\/ru\/company\/ods\/blog\/328372\/ - \u041c\u0435\u0442\u0440\u0438\u043a\u0438  \nhttps:\/\/webiomed.ai\/blog\/osnovnye-metriki-zadach-klassifikatsii-v-mashinnom-obuchenii\/ - \u041c\u0435\u0442\u0440\u0438\u043a\u0438  \n\nhttps:\/\/www.kaggle.com\/motloch\/nov21-mislabeled-25 - \u041f\u0440\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0443 \u043d\u0430 \u044d\u0442\u043e\u043c \u0445\u0430\u043a\u0430\u0442\u043e\u043d\u0435 \u0438 25% \u043b\u043e\u0441\u0441","9367916a":"# Data preparation","d0a614f9":"And so we want to get rid of the bimodal distribution. To do this, I chose QuantileTransformer, but if it doesn't work, I will filter by StandartScaler as standard.\nQuantileTransformer - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.QuantileTransformer.html  \nStandartScaler - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n\nIn order for QuantileTransformer to work most efficiently, we will remove unnecessary columns, namely those that have the greatest variation (I check this in the first two lines)","c5d6e057":"## To be continued...\nIn the second part, I want to analyze more complex models, feature selection and models generation. Also deal with Hyperparameters searching and model blending.","f2a879ee":"# Cogort analisys\n__\u0421\u0441\u044b\u043b\u043a\u0438__:  \n\nhttps:\/\/www.kaggle.com\/kavehshahhosseini\/tps-nov-2021-pca-and-kmeans-feature-eng - PCA + kmeans  \nhttps:\/\/www.kaggle.com\/lucasmorin\/data-exploration-with-umap-hdbscan - UMAP + hdbscan  \nhttps:\/\/www.kaggle.com\/lucamassaron\/t-sne-and-umap-for-eda - t-SNE \/ UMAP  \n\nhttps:\/\/habr.com\/ru\/company\/newprolab\/blog\/350584\/ - \u041f\u043e\u0447\u0438\u0442\u0430\u0442\u044c \u0447\u0442\u043e \u0431\u044b \u043f\u043e\u043d\u044f\u0442\u044c"}}