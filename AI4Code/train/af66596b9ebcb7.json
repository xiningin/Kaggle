{"cell_type":{"c6c2ece3":"code","9235ec66":"code","36f8ff04":"code","7153a83f":"code","a6368469":"code","60487244":"code","1e23a68e":"code","9b895d1f":"code","c0e70fa2":"code","2d0b7721":"code","8a1f6e7f":"code","0b08ed8b":"code","0c33f4f2":"code","d97b5be5":"code","cf0e5df2":"code","5a4e82a2":"code","a7254ef2":"code","b78f1dae":"code","d5105484":"code","2cc06bc7":"code","73a6c2f9":"code","4a614729":"code","6cce2737":"code","94b035d9":"code","e539da1e":"code","8ae418c1":"code","7ec2852b":"code","79b1f3d7":"code","39fa6bab":"code","7250cfa1":"code","37af1d38":"code","012e58e8":"code","1854d8b1":"code","782122fb":"code","e35b34e5":"code","a7902218":"code","02e9f8a1":"code","29ff0ac0":"code","4b1c6bfc":"code","9a1f2d17":"code","603f6a06":"code","22d34813":"code","93ff5c84":"code","4ca2205a":"code","58c8476b":"code","e864aae5":"code","8e806434":"code","27d9e5d4":"code","afb44204":"code","6cd0f09e":"code","fe2b8e84":"code","1775af0b":"code","eda0441b":"code","a225e50f":"code","6b0f82b4":"code","5486c3fc":"code","f258c603":"code","f3eb9b81":"code","72b412fc":"code","5127b825":"code","528e32dc":"code","54e93d3c":"code","bba7aee2":"code","65709569":"code","e17b4c63":"code","390f82bc":"markdown","a11e61cd":"markdown","08f28fc4":"markdown","591ca2ca":"markdown","695b8371":"markdown","70f5b4c3":"markdown","d600b656":"markdown","337e9e91":"markdown","ad3a4834":"markdown","2e5379f4":"markdown","b25231bc":"markdown","3e4db26e":"markdown","1e16243f":"markdown","9b3e2a53":"markdown","23f3abb2":"markdown","b3f29ef0":"markdown","6a26c90d":"markdown","b3a5da2c":"markdown","0e56ccc0":"markdown"},"source":{"c6c2ece3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n\npd.options.display.max_colwidth = 200\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n#warnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames[:5]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9235ec66":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","36f8ff04":"#!pip install --upgrade ipykernel","7153a83f":"data = pd.read_csv('\/kaggle\/input\/d\/aggarwalrahul\/nlp-specialization-data\/Cleaned_POS_Medical_Notes.csv') #for excel file use read_excel\ndata","a6368469":"from sklearn.feature_extraction.text import CountVectorizer","60487244":"vector = CountVectorizer(lowercase=True, #this will convert all the tokens into lower case\n                         stop_words='english', #remove english stopwords from vocabulary. if we need the stopwords this value should be None\n                         analyzer='word', #tokens should be words. we can also use char for character tokens\n                         max_features=50000 #maximum vocabulary size to restrict too many features\n                        )","1e23a68e":"sample_text = data.clean_text.iloc[:5]","9b895d1f":"vectorized_corpus = vector.fit_transform(sample_text)","c0e70fa2":"print(len(vector.get_feature_names()))\nvector.get_feature_names()[40:50]","2d0b7721":"vectorized_corpus.toarray()","8a1f6e7f":"print (vectorized_corpus.shape)","0b08ed8b":"vector = CountVectorizer(lowercase=True, #this will convert all the tokens into lower case\n                         stop_words='english', #remove english stopwords from vocabulary. if we need the stopwords this value should be None\n                         analyzer='word', #tokens should be words. we can also use char for character tokens\n                         max_features=500, #maximum vocabulary size to restrict too many features\n                         max_df=.50, #if some word is in more than 50% of the documents, remove them\n                         min_df=2 #words need to be in atleast 2 documents\n                        )","0c33f4f2":"vectorized_corpus = vector.fit_transform(sample_text) #fit_transform method fit and then transform the data. We can also fit and transform separately.","d97b5be5":"print (vectorized_corpus.shape)","cf0e5df2":"vector = CountVectorizer(lowercase=True, #this will convert all the tokens into lower case\n                         stop_words='english', #remove english stopwords from vocabulary. if we need the stopwords this value should be None\n                         analyzer='word', #tokens should be words. we can also use char for character tokens\n                         max_features=50000, #maximum vocabulary size to restrict too many features\n                         max_df=.5, #if some word is in more than 50% of the documents, remove them\n                         min_df=2, #words need to be in atleast 2 documents\n                         ngram_range=(1,3) #change ngram_range for n-grams\n                        )","5a4e82a2":"vectorized_corpus = vector.fit_transform(sample_text)","a7254ef2":"vector.get_feature_names()[:50]","b78f1dae":"print (vectorized_corpus.shape)","d5105484":"from sklearn.feature_extraction.text import TfidfVectorizer","2cc06bc7":"tfidf_vector = TfidfVectorizer(lowercase=True, #this will convert all the tokens into lower case\n                         stop_words='english', #remove english stopwords from vocabulary. if we need the stopwords this value should be None\n                         analyzer='word', #tokens should be words. we can also use char for character tokens\n                         max_features=50000 #maximum vocabulary size to restrict too many features\n                        )","73a6c2f9":"tfidf_vectorized_corpus = tfidf_vector.fit_transform(sample_text)","4a614729":"tfidf_vectorized_corpus.toarray()","6cce2737":"data = data.dropna(subset=['clean_text'])\n\ntfidf_vector = TfidfVectorizer(lowercase=True, #this will convert all the tokens into lower case\n                         stop_words='english', #remove english stopwords from vocabulary. if we need the stopwords this value should be None\n                         analyzer='word', #tokens should be words. we can also use char for character tokens\n                         max_features=50000, #maximum vocabulary size to restrict too many features\n                         min_df = 5,\n                         max_df = .4\n                        )\n\ntfidf_vectorized_corpus = tfidf_vector.fit_transform(data.clean_text)\n\nprint (tfidf_vectorized_corpus.shape)","94b035d9":"top_words = tfidf_vector.get_feature_names()\ntop_words_df = pd.DataFrame(tfidf_vectorized_corpus.toarray(),columns=top_words)\ntop_words_df['speciality'] = data.label","e539da1e":"top_words_df.head(5)","8ae418c1":"top_words_df_per_label = top_words_df.groupby(['speciality']).mean().reset_index(drop=False)\ntop_words_df_per_label = top_words_df_per_label.melt(id_vars=['speciality'],value_vars=top_words)\ntop_words_df_per_label.columns = ['speciality','top_word','word_weight']\ntop_words_df_per_label = top_words_df_per_label.sort_values(by=['speciality','word_weight'],ascending=[False,False]).reset_index(drop=True)\ntop_words_df_per_label = top_words_df_per_label.groupby(['speciality']).head(20).reset_index(drop=True)\ntop_words_df_per_label.head(5)","7ec2852b":"from IPython.display import HTML, Image\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\nimport plotly.express as px\n#init_notebook_mode(connected=True)\n\nall_colors = px.colors.qualitative.Plotly\nprint (all_colors)","79b1f3d7":"traces = []\n\nfor i, s in enumerate(list(set(top_words_df_per_label.speciality))):\n    traces.append(go.Bar(\n        x = top_words_df_per_label[top_words_df_per_label.speciality == s]['top_word'],\n        y = top_words_df_per_label[top_words_df_per_label.speciality == s]['word_weight'],\n        name=s,\n        marker_color=all_colors[i%len(all_colors)]\n    ))\n    \nlayout = dict(\n        width=1050,\n        height=600,\n        title = 'Top 20 Words for each Speciality', \n        yaxis=dict(\n        title='Word Weight',\n        titlefont_size=16,\n        tickfont_size=14,\n        ),  \n    xaxis=dict(\n        title='Top Words',\n        titlefont_size=16,\n        tickfont_size=14,\n        ),  \n    )\n\nbuttons = []\nvisibility = [False]*len(list(set(top_words_df_per_label.speciality)))\nfor i, s in enumerate(list(set(top_words_df_per_label.speciality))):\n    visibility_ = visibility.copy()\n    visibility_[i] = True\n    buttons.append(\n        dict(\n            args = [{'visible': visibility_}],\n            label=s,\n            method='update',\n        ))\n    \nupdatemenus = list([\n    dict(buttons=buttons,\n    direction=\"down\",\n        pad = {'r':10, \"t\":10},\n        showactive=True,\n        x=0.3,\n        y=1.15,\n        yanchor='top')\n])\n\nlayout['updatemenus'] = updatemenus\n\nfig = dict(data=traces,layout=layout)\niplot(fig)","39fa6bab":"from sklearn.decomposition import LatentDirichletAllocation","7250cfa1":"n_topics = 5\nlda = LatentDirichletAllocation(n_components=n_topics)\nlda_features = lda.fit_transform(tfidf_vectorized_corpus)","37af1d38":"lda_features","012e58e8":"lda_features.shape","1854d8b1":"np.argmax(np.array([0.2,.34,.56,.09,.1]))","782122fb":"np.argmax(np.array([0.92,.34,.56,.09,.1]))","e35b34e5":"text_topics = lda_features.argmax(axis=1)\ndata['topic'] = text_topics\ndata","a7902218":"# pd.crosstab(data['topic'],data['label'])","02e9f8a1":"def print_top_words(model, feature_names, n_top_words):\n    topic_df = pd.DataFrame()\n    topic_df['topic_id'] = np.arange(n_topics)\n    temp = []\n    \n    for topic_idx, topic in enumerate(model.components_):\n        topic \/= topic.sum()\n        message = \"Topic #%d: \" % (topic_idx)\n        message += \" + \".join([\"{:.3f} * {}\".format(topic[i],feature_names[i])\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n        \n        temp.append(\",\".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n\n    topic_df['top_words'] = temp\n    \n    print()\n    \n    return topic_df","29ff0ac0":"top_words = print_top_words(lda, tfidf_vector.get_feature_names(), n_top_words=5)","4b1c6bfc":"data_topic_proportions = data.groupby(['topic'])['label'].value_counts(normalize=True).reset_index(name='topic_proportion')\ndata_topic_proportions.columns = ['topic','speciality','topic_proportion']\ndata_topic_proportions = data_topic_proportions.sort_values(['topic'],ascending=[True]).reset_index(drop=True)\n#data_topic_proportions","9a1f2d17":"traces = []\nfor i in data_topic_proportions.speciality.unique():\n    traces.append(go.Bar(name=i, x=data_topic_proportions[data_topic_proportions.speciality == i]['topic'], y=data_topic_proportions[data_topic_proportions.speciality == i]['topic_proportion']))\n\nlayout = dict(\n        width=1200,\n        height = 600,\n        title = 'Top disease category for each topic', \n        yaxis=dict(\n        title='Proportion',\n        titlefont_size=16,\n        tickfont_size=14,\n        ),  \n    xaxis=dict(\n        title='Topic ID',\n        titlefont_size=16,\n        tickfont_size=14,\n        ),  \n    )\n\nfig = go.Figure(data=traces,layout=layout)\n\nfig.update_layout(barmode='stack')\nfig.show()","603f6a06":"import pyLDAvis\n#import pyLDAvis.gensim\nimport pyLDAvis.sklearn\nfrom IPython.display import HTML","22d34813":"viz = pyLDAvis.sklearn.prepare(lda_model=lda,vectorizer=tfidf_vector,dtm=tfidf_vectorized_corpus)","93ff5c84":"pyLDAvis.save_html(viz,'vis.html')\nHTML(filename='vis.html')","4ca2205a":"import gensim\nimport gensim.corpora as corpora\nfrom gensim.models import CoherenceModel\nfrom gensim.models import ldamodel","58c8476b":"from nltk.tokenize import word_tokenize\ndata['reviews_token'] = [word_tokenize(sent) for sent in data.clean_text]","e864aae5":"data","8e806434":"id2word = corpora.Dictionary(data.reviews_token)\ntexts = data.reviews_token\ncorpus = [id2word.doc2bow(text) for text in texts]","27d9e5d4":"?id2word.id2token","afb44204":"lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=8, \n                                           random_state=42,\n                                           passes=10,\n                                           per_word_topics=True)","6cd0f09e":"for i in lda_model.print_topics():\n    print(i)\n    print(\"\\n\")","fe2b8e84":"# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","1775af0b":"from sklearn.decomposition import NMF","eda0441b":"nmf_model = NMF(n_components=n_topics, l1_ratio=.5) #beta_loss='kullback-leibler'","a225e50f":"nmf_features = nmf_model.fit_transform(tfidf_vectorized_corpus)","6b0f82b4":"top_words_nmf = print_top_words(nmf_model, tfidf_vector.get_feature_names(), n_top_words=5)","5486c3fc":"text_topics_nmf = nmf_features.argmax(axis=1)\ndata['topic_nmf'] = text_topics_nmf\n\ndata_topic_proportions = data.groupby(['topic_nmf'])['label'].value_counts(normalize=True).reset_index(name='topic_proportion')\ndata_topic_proportions.columns = ['topic_nmf','speciality','topic_proportion']\ndata_topic_proportions = data_topic_proportions.sort_values(['topic_nmf'],ascending=[True]).reset_index(drop=True)\n#data_topic_proportions","f258c603":"traces = []\nfor i in data_topic_proportions.speciality.unique():\n    traces.append(go.Bar(name=i, x=data_topic_proportions[data_topic_proportions.speciality == i]['topic_nmf'], y=data_topic_proportions[data_topic_proportions.speciality == i]['topic_proportion']))\n\nlayout = dict(\n        width=1200,\n        height=600,\n        title = 'Top disease category for each topic', \n        yaxis=dict(\n        title='Topic Id',\n        titlefont_size=16,\n        tickfont_size=14,\n        ),  \n    xaxis=dict(\n        title='Proportion',\n        titlefont_size=16,\n        tickfont_size=14,\n        ),  \n    )\n\nfig = go.Figure(data=traces,layout=layout)\n\nfig.update_layout(barmode='stack')\nfig.show()","f3eb9b81":"viz_nmf = pyLDAvis.sklearn.prepare(lda_model=nmf_model,vectorizer=tfidf_vector,dtm=tfidf_vectorized_corpus)\npyLDAvis.save_html(viz_nmf,'vis_nmf.html')\nHTML(filename='vis_nmf.html')","72b412fc":"from sklearn.manifold import TSNE","5127b825":"tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca', learning_rate=200)\ntsne_lda = tsne_model.fit_transform(lda_features)","528e32dc":"tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca', learning_rate=200)\ntsne_nmf = tsne_model.fit_transform(nmf_features)","54e93d3c":"vis_data = pd.DataFrame()\nvis_data['x'] = tsne_lda[:,0]\nvis_data['y'] = tsne_lda[:,1]\nvis_data['speciality'] = data.label\nvis_data['topic_id'] = data.topic\nvis_data = pd.merge(vis_data,top_words,how='inner')\nvis_data.topic_id = vis_data.topic_id.astype(str)\n\nfig = px.scatter(vis_data, x=\"x\", y=\"y\", color=vis_data.speciality, hover_data=['topic_id'], title=\"Projection of Clinical Texts (based on LDA)\", height=600, width=1200) #hover_data=['top_words']\nfig.show()","bba7aee2":"fig = px.scatter(vis_data, x=\"x\", y=\"y\", color=vis_data.topic_id, hover_data=['top_words'], title=\"Projection of Clinical Texts (based on LDA)\", height=600, width=1200) #hover_data=['top_words']\nfig.show()","65709569":"vis_data = pd.DataFrame()\nvis_data['x'] = tsne_nmf[:,0]\nvis_data['y'] = tsne_nmf[:,1]\nvis_data['speciality'] = data.label\nvis_data['topic_id'] = data.topic_nmf\nvis_data = pd.merge(vis_data,top_words_nmf,how='inner')\nvis_data.topic_id = vis_data.topic_id.astype(str)\n\nfig = px.scatter(vis_data, x=\"x\", y=\"y\", color=vis_data.speciality,  hover_data=['topic_id'], title=\"Projection of Clinical Texts (based on NMF)\", height=600, width=1200) #hover_data=['top_words']\nfig.show()","e17b4c63":"fig = px.scatter(vis_data, x=\"x\", y=\"y\", color=vis_data.topic_id,  hover_data=['top_words'], title=\"Projection of Clinical Texts (based on NMF)\", height=600, width=1200) #hover_data=['top_words']\nfig.show()","390f82bc":"The numbers learned in lda follows probability distribution. Let us see the top topic for each text and interpret each topic.","a11e61cd":"<a id='mf'><\/a>\n\n# [Optional] Matrix Factorization \n\nMatrix decomposition\/factorization is an essential statistical technique to \n\n* Reduce dimensionality\n* Feature extraction\n* Semantic analysis\n\nMatrix factorization can also be used to extract topical information.\n\nIn this notebook we use matrix decomposition to extract representation from texts","08f28fc4":"<a id='nmf'><\/a>\n\n#### Non-negative Matrix Factorization\n\nIn NMF we split the matrix into two matrices with non-negative entries. \n\n![image.png](attachment:image.png)\n\nDue to the constraint, NMF can not be solved like typical matrix decomposition. NMF is solved by optimizing the above loss function using optimization techniques like - gradient descent.","591ca2ca":"<a id='tfidf'><\/a>\n\n## Tf-idf \n\nCountvectorizer uses just count of words in each of the document, which do not represent the relative strength of the words. Tf-idf addresses that issue by normalizing the counts by total number of words in the document and how many documents that particular word in","695b8371":"## [Optional] Topic Model Using LDA (Gensim Library) & Compute Coherence Score\n\n* https:\/\/towardsdatascience.com\/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n","70f5b4c3":"There are total 818 medical texts and 3831 is vocab size","d600b656":"<a id='cv'><\/a>\n\n## Countvectorizer\n\nIn Countvectorizer, we use counts to represent each word in each of the texts","337e9e91":"<a id='tm'><\/a>\n\n# Topic Modelling\n\nTopic modeling is a type of statistical modeling for discovering the abstract \u201ctopics\u201d that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.\n\n![image.png](attachment:image.png)","ad3a4834":"<a id='bow'><\/a>\n\n# 1. Bag of Words representation of texts\n\nA bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms.\n\nThe approach is very simple and flexible, and can be used in a myriad of ways for extracting features from documents.\n\nA bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n\n1. A vocabulary of known words.\n2. A measure of the presence of known words.\n\nIt is called a \u201cbag\u201d of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.","2e5379f4":"### References for further reading\n\n<strong> Visualization <\/strong>\n\nhttps:\/\/matplotlib.org\/tutorials\/index.html\n\nhttps:\/\/plotly.com\/python\n\nhttps:\/\/www.kaggle.com\/thebrownviking20\/intermediate-visualization-tutorial-using-plotly\n\n<strong> Representation Learning <\/strong>\n\nhttps:\/\/www.oreilly.com\/library\/view\/applied-text-analysis\/9781491963036\/ch04.html\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html\n\n<strong> Topic Modelling <\/strong>\n\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/applications\/plot_topics_extraction_with_nmf_lda.html\n\nhttps:\/\/machinelearningmastery.com\/introduction-to-matrix-decompositions-for-machine-learning\/\n\nhttps:\/\/medium.com\/nanonets\/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05\n\n<strong> T-SNE <\/strong>\n\nhttps:\/\/distill.pub\/2016\/misread-tsne\/","b25231bc":"Vectorized corpus is in sparse scipy matrix. We need to convert it into numpy array for display. However, if the vocab size is too big, numpy array can eat a lot of memory. Its wise to use sparse matrix for computation for big datasets","3e4db26e":"get_feature_names attribute will show all the words in the vocabulary","1e16243f":"Now let us add bi-gram and tri-gram to add more words into our vocabulary","9b3e2a53":"### Visualizing Top words from Tf-idf representation\n\nUnder Tf-idf representation, words that carry high weightage are important for each text. Let us visualize top word based on tf-idf score for each of the specialties.","23f3abb2":"Let us apply tf-idf on the whole dataset","b3f29ef0":"<a id='viz'><\/a>\n\n# [Optional] Visualizing Representations\n\nWe can use quantitative methods for evaluating representations learned from different model. However, as the task is unsupervised, most of the evaluations are based on similarity based metrics. As we see below, some of the representations preserve local properties i.e - similar texts have similar representation and cluster together. On the other hand, some methods look for global structures. Usually, people evaluate representations using downstream task specific metrics.\n\nAs we can not visualize 100-dim vectors, we use t-SNE embeddings to reduce the dimensions into 2. t-Distributed Stochastic Neighbor Embedding (t-SNE) is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets.","6a26c90d":"The numbers denote the relative strength of each word in each of the topics. These numbers are calculated during the process of LDA. We can extract the hidden aspect for each topic from these words. In practice, we maintain reverse dictionary to get the understanding behind each topic. However, as topics are meant for hidden semantics, often topics can demonstrate abstract concepts, which may not be comprehended by human judgment.","b3a5da2c":"As we can not feed words directly into most of the models, we need numeric values for each text. In this notebook we discuss different techniques to represent a text document into numeric vectors. \n\n## Table of Contents\n\n* [Bag of Words representation of texts](#bow)\n    * [Countvectorizer](#cv)\n    * [Tf-idf](#tfidf)\n* [Topic Modelling](#tm)\n* [Matrix Decomposition](#mf)\n* [Visualizing Representations](#viz)","0e56ccc0":"If we want to remove too frequent or too rare we can use max_df and min_df arguments"}}