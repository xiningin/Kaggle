{"cell_type":{"fd79ac18":"code","9ee42911":"code","7b426bf2":"code","6991e215":"code","fb0bb91c":"code","2532b2ac":"code","9f0d00dd":"code","7315d5d2":"code","e5096262":"code","52bc01c0":"code","301c5a5e":"code","1a85fe58":"code","5bff3b91":"code","1c3bca4c":"code","2d12a739":"code","d0025c72":"code","c38f10e5":"code","01214bfd":"code","746a7707":"markdown","b6676bb0":"markdown","ab517c1a":"markdown","916bbc58":"markdown","45fda594":"markdown","8aa20467":"markdown","c4ad9390":"markdown","8ea1fcab":"markdown","38a1903f":"markdown","0fd7614b":"markdown","6d65fc65":"markdown","3c215aee":"markdown"},"source":{"fd79ac18":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ee42911":"train = pd.read_csv('..\/input\/rsna-str-pulmonary-embolism-detection\/train.csv')\ntest = pd.read_csv('..\/input\/rsna-str-pulmonary-embolism-detection\/test.csv')\n\nprint('train.shape', train.shape, 'test.shape', test.shape)","7b426bf2":"train.head(5)","6991e215":"train.groupby('StudyInstanceUID')['SeriesInstanceUID'].nunique().max(), test.groupby('StudyInstanceUID')['SeriesInstanceUID'].nunique().max()","fb0bb91c":"np.intersect1d(train.StudyInstanceUID.unique(), test.StudyInstanceUID.unique())","2532b2ac":"train_image_num_per_patient = train.groupby('StudyInstanceUID')['SOPInstanceUID'].nunique()\ntest_image_num_per_patient = test.groupby('StudyInstanceUID')['SOPInstanceUID'].nunique()","9f0d00dd":"train_image_num_per_patient.describe()","7315d5d2":"test_image_num_per_patient.describe()","e5096262":"import matplotlib.pyplot as plt\nplt.title('image_num_per_patient')\nplt.hist(train_image_num_per_patient, bins=100, label='train', density=True)\nplt.hist(test_image_num_per_patient, bins=100, label='test', density=True)\nplt.legend()\nplt.show()","52bc01c0":"FOLD_NUM = 20\ntarget_cols = [c for i, c in enumerate(train.columns) if i > 2]","301c5a5e":"# build summary of image num and target variables for each patient\ntrain_per_patient_char = pd.DataFrame(index=train_image_num_per_patient.index, columns=['image_per_patient'], data=train_image_num_per_patient.values.copy())\nfor t in target_cols:\n    train_per_patient_char[t] = train_per_patient_char.index.map(train.groupby('StudyInstanceUID')[t].mean())\n\ntrain_per_patient_char.head(10)","1a85fe58":"# make image_per_patient and pe_present_on_image into bins\nbin_counts = [40] #, 20]\ndigitize_cols = ['image_per_patient'] #, 'pe_present_on_image']\nnon_digitize_cols = [c for c in train_per_patient_char.columns if c not in digitize_cols]","5bff3b91":"for i, c in enumerate(digitize_cols):\n    bin_count = bin_counts[i]\n    percentiles = np.percentile(train_per_patient_char[c], q=np.arange(bin_count)\/bin_count*100.)\n    #print(percentiles)\n    print(train_per_patient_char[c].value_counts())\n    train_per_patient_char[c+'_digitize'] = np.digitize(train_per_patient_char[c], percentiles, right=False)\n    print(train_per_patient_char[c+'_digitize'].value_counts())\n    plt.hist(train_per_patient_char[c+'_digitize'], bins=bin_count)\n    plt.show()","1c3bca4c":"train_per_patient_char['key'] = train_per_patient_char[digitize_cols[0]+'_digitize'].apply(str)\nfor c in digitize_cols[1:]:\n    train_per_patient_char['key'] = train_per_patient_char['key']+'_'+train_per_patient_char[c+'_digitize'].apply(str)\n\ntrain_per_patient_char['key'].value_counts()","2d12a739":"from sklearn.model_selection import StratifiedKFold\nfolds = FOLD_NUM\nkfolder = StratifiedKFold(n_splits=folds, shuffle=True, random_state=719)\nval_indices = [val_indices for _, val_indices in kfolder.split(train_per_patient_char['key'], train_per_patient_char['key'])]\n\ntrain_per_patient_char['fold'] = -1\nfor i, vi in enumerate(val_indices):\n    patients = train_per_patient_char.index[vi]\n    train_per_patient_char.loc[patients, 'fold'] = i\ntrain_per_patient_char['fold'].value_counts()","d0025c72":"# check each fold for the distribution of the number of images per patients\nfor col in digitize_cols:\n    fig, axs = plt.subplots(nrows=4, ncols=int(np.floor(folds\/4)), constrained_layout=False, sharex=True, sharey=True)\n    fig.set_figheight(10)\n    fig.set_figwidth(20)\n    axs = axs.flat\n    for i, vi in enumerate(val_indices):\n        patients = train_per_patient_char.index[vi]\n        axs[i].set_title(col+' fold_'+str(i))\n        axs[i].hist(train_per_patient_char.loc[patients, col], bins=20, range=(train_per_patient_char[col].min(), train_per_patient_char[col].max()))\n    plt.show()","c38f10e5":"# check each fold for the target distribution\nfor col in non_digitize_cols:\n    fig, axs = plt.subplots(nrows=4, ncols=int(np.floor(folds\/4)), constrained_layout=False, sharex=True, sharey=True)\n    fig.set_figheight(10)\n    fig.set_figwidth(20)\n    axs = axs.flat\n    for i, vi in enumerate(val_indices):\n        patients = train_per_patient_char.index[vi]\n        axs[i].set_title(col+' fold_'+str(i))\n        axs[i].hist(train_per_patient_char.loc[patients, col], bins=20, range=(train_per_patient_char[col].min(), train_per_patient_char[col].max()))\n    plt.show()","01214bfd":"train_per_patient_char.to_csv('rsna_train_splits_fold_{}.csv'.format(FOLD_NUM))","746a7707":"### **From the above 2 observations, it is important to split train and validation set based on StudyInstanceUID\\SeriesInstanceUID to simulate train-test split**","b6676bb0":"# Overview\nIn this kernel, I'm going to demonstrate how to build a stratified validation splits while doing some preliminary EDA on both train and test set","ab517c1a":"**Further usage of this kernel:**\n* You could use the output csv directly to do patient level subsampling (ex. select fold=1-5 to do 5 fold cross-validation) \n* You could modify FOLD_NUM above to create different number of stratified folds yourself \n* You could modify bin_counts+digitize_cols above to digitize columns with designated bin counts, which will be futher incorporated into the new \"key\" to do the stratified validation splits","916bbc58":"> only **pe_present_on_image** is image level, that's why only it is the only patient-level value with floating number after averaging","45fda594":"### We will do validation splits based on\n1. Patient: Same patient should be in the same validation split\n2. Number of image per patient: Distribution should be similar across all validation splits","8aa20467":"> No repeated StudyInstanceUID between train and test","c4ad9390":"> The StudyInstanceUID and SeriesInstanceUID is 1-1 mapping in both train and test set","8ea1fcab":"> As we could see, the number of images per patient in train and test are pretty close, except some slight right shift in train set (comparing the 25%-50%-75% from the describe() method)","38a1903f":"# Create Stratified Validation Splits","0fd7614b":"## Each fold looks similar in the distribution of the number of images per patient now","6d65fc65":"# EDA and Observations","3c215aee":"### **From the above, it is better to also do *stratified split* based on the *image number per patient* to simulate train-test split**"}}