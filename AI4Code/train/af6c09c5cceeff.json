{"cell_type":{"4b8cb052":"code","47210723":"code","094a8938":"code","553d70b2":"code","cc1a34f9":"code","515e07b9":"code","beb2c58f":"code","c1e91bb1":"code","af2e1569":"code","1cc61090":"code","46334054":"code","34a6e21a":"code","d477716d":"code","4fd10bb7":"code","c716fe42":"code","fb1fe8d5":"code","f5a0c7f1":"code","5b78931f":"markdown","01c62b30":"markdown","eada4039":"markdown","5cc5bba1":"markdown","6ec28758":"markdown","75f1a11a":"markdown","6a578742":"markdown"},"source":{"4b8cb052":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nkeras = tf.keras\nimport matplotlib.pyplot as plt\nimport pathlib\nfrom sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clickinga run or pressing Shift+Enter) will list all files under the input directory\n\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\ndata_dir = pathlib.Path('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/train')\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\ntest_dir = pathlib.Path('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/test')","47210723":"batch_size = 32\nimg_height = 300\nimg_width = 300\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    labels='inferred',\n    validation_split=0.2,\n    subset=\"training\",\n    seed=123,\n    image_size=(img_height, img_width),\n    batch_size=batch_size)\n#     color_mode = 'grayscale')\n\n# Since original validation dataset contains only 16 images we will take 20% of training and use for validation\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=123,\n    image_size=(img_height, img_width),\n    batch_size=batch_size)\n\ntest_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    test_dir,\n    seed=123,\n    image_size=(img_height, img_width),\n    batch_size=batch_size)","094a8938":"from keras.preprocessing.image import img_to_array\n# Normalize image\ndef format_image(image, label):\n    \"\"\"\n    returns an image that is reshaped to IMG_SIZE\n    \"\"\"\n    image = tf.cast(image, tf.float32)\n    image = (image\/127.5) - 1\n#     image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n\n#     # convert the image pixels to a numpy array\n#     image = img_to_array(image).ref()\n#     # reshape data for the model\n#     image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n#     # prepare the image for the xception model\n#     image = keras.applications.xception.preprocess_input(image)\n    return image, label\n\n\n\ntrain = train_ds.map(format_image)\nvalidation = val_ds.map(format_image)\ntest = test_ds.map(format_image)","553d70b2":"import PIL\nxrays_norm = list(data_dir.joinpath('NORMAL').glob('*.jpeg'))\nxrays_pneum = list(data_dir.joinpath('PNEUMONIA').glob('*.jpeg'))\n\nnorm = PIL.Image.open(str(xrays_norm[0]))\npneum = PIL.Image.open(str(xrays_pneum[0]))\n\nfig = plt.figure(figsize = (15, 20))\nax1 = fig.add_subplot(1,2,1)\nax1.title.set_text('NORMAL')\nplt.imshow(norm, cmap='gray')\nax2 = fig.add_subplot(1,2,2)\nax2.title.set_text('PNEUMONIA')\nplt.imshow(pneum, cmap='gray')\nplt.show()","cc1a34f9":"class_names = train_ds.class_names\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(3):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","515e07b9":"data_augmentation = keras.Sequential(\n  [\n    keras.layers.experimental.preprocessing.RandomRotation(0.01),\n    keras.layers.experimental.preprocessing.RandomZoom(0.02),\n    keras.layers.experimental.preprocessing.RandomTranslation(height_factor=(-0.02, 0.02), width_factor=(-0.02, 0.02))\n  ]\n)","beb2c58f":"plt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n    for i in range(9):\n        augmented_images = data_augmentation(images)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n        plt.axis(\"off\")","c1e91bb1":"# Import pretrained model\nIMG_SHAPE = (img_height, img_width, 3)\n# Create the base model from the pre-trained model MobileNet V2\nbase_model = tf.keras.applications.MobileNet(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet',\n#                                                pooling='max',\n                                               classes=2)\nbase_model.trainable = False\nbase_model.summary()","af2e1569":"# Add fully-connected (Dense) layer at the end of the imported NN to adapt it for our use case\n\nglobal_average_layer = tf.keras.layers.GlobalAveragePooling2D() #keras.layers.Flatten(input_shape=base_model.output_shape[1:]) \nhidden_prediction_layer = keras.layers.Dense(500, activation='relu')\ndropout_layer = keras.layers.Dropout(0.2)\nhidden_prediction_layer2 = keras.layers.Dense(50, activation='relu')\nprediction_layer = keras.layers.Dense(1, activation='sigmoid')\nmodel = tf.keras.Sequential([\n    data_augmentation,\n    base_model,\n    global_average_layer,\n#     hidden_prediction_layer,\n#     dropout_layer,\n    hidden_prediction_layer2,\n    prediction_layer\n])\nbase_learning_rate = 0.001\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate), # https:\/\/machinelearningmastery.com\/adam-optimization-algorithm-for-deep-learning\/\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n# This function keeps the initial learning rate for the first ten epochs\n# and decreases it exponentially after that.\ndef scheduler(epoch, lr):\n    return lr \/ 2 #* tf.math.exp(-0.1)\nscheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\nearly_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=4,restore_best_weights=True)","1cc61090":"# Now we can train it on our images\nmodel.fit(train,validation_data=validation, epochs=8, callbacks=[early_stopping_callback])","46334054":"# model.save('3_epochs')\n# model = tf.keras.models.load_model('3_epochs')","34a6e21a":"# Unfreeze all weights\n# model.trainable = True\nfor layer in model.layers:\n    layer.trainable = True\n\nmodel.fit(train,validation_data=validation, epochs=10, callbacks=[scheduler_callback, early_stopping_callback])","d477716d":"# model.evaluate(validation)","4fd10bb7":"def plot_predictions(validation_labels, pred, title=''):\n    num_samples_to_show = 100#len(preds)\n    plt.figure()\n    plt.figure(figsize=(30,10))\n    plt.plot(range(num_samples_to_show), pred[:num_samples_to_show], 'ys', label='Predicted_value')\n    plt.plot(range(num_samples_to_show), validation_labels[:num_samples_to_show], 'r*', label='Test_value')\n\n\n    plt.tick_params(axis='x', which='both', bottom=False, top=False,labelbottom=False)\n    plt.ylabel('predicted')\n    plt.xlabel(' samples')\n    plt.legend(loc=\"best\")\n    plt.title(title + 'Truth vs predicted')\n    plt.show()\n    \ndef plot_auc(y_labels, pre, title=''):\n    #Print Area Under Curve\n    false_positive_rate, recall, thresholds = roc_curve(y_labels, pre)\n    roc_auc = auc(false_positive_rate, recall)\n    plt.figure()\n    plt.title(title + ' Receiver Operating Characteristic (ROC)')\n    plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1], [0,1], 'r--')\n    plt.xlim([0.0,1.0])\n    plt.ylim([0.0,1.0])\n    plt.ylabel('Recall')\n    plt.xlabel('Fall-out (1-Specificity)')\n    plt.show()","c716fe42":"# Extract validation and test labels and make predictions\nvalidation_labels = []\nvalidation_images = []\nfor images, labels in validation.take(len(validation)):\n    validation_labels += [x for x in labels.numpy()]\n    validation_images += [x for x in images.numpy()]\ntest_labels = []\ntest_images = []\nfor images, labels in test.take(len(test)):\n    test_labels += [x for x in labels.numpy()]\n    test_images += [x for x in images.numpy()]\n\npred_val = model.predict(np.stack(validation_images))\npred_test = model.predict(np.stack(test_images))\n\nplot_auc(validation_labels, pred_val, 'Validation')\nplot_auc(test_labels, pred_test, 'Test')\nplot_predictions(test_labels, pred_test, 'Test ')","fb1fe8d5":"import itertools\ndef plot_confusion_matrix(cm, class_names, title=''):\n    \"\"\"\n    Returns a matplotlib figure containing the plotted confusion matrix.\n\n    Args:\n    cm (array, shape = [n, n]): a confusion matrix of integer classes\n    class_names (array, shape = [n]): String names of the integer classes\n    \"\"\"\n    figure = plt.figure(figsize=(4, 4))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(title + \" Confusion matrix\")\n    plt.colorbar()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names, rotation=45)\n    plt.yticks(tick_marks, class_names)\n\n    # Compute the labels from the normalized confusion matrix.\n    labels = np.around(cm.astype('int'))# \/ cm.sum(axis=1)[:, np.newaxis], decimals=2)\n\n    # Use white text if squares are dark; otherwise black.\n    threshold = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        color = \"white\" if cm[i, j] > threshold else \"black\"\n        plt.text(j, i, labels[i, j], horizontalalignment=\"center\", color=color)\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n#     return figure\n\nround_predictions = [int(x) for x in np.round(pred_val)]\ncm = tf.math.confusion_matrix(validation_labels, round_predictions, num_classes=2).numpy()\nplot_confusion_matrix(cm, ['NORMAL', 'PNEUMONIA'], 'Validation')\n\nround_predictions = [int(x) for x in np.round(pred_test)]\ncm = tf.math.confusion_matrix(test_labels, round_predictions, num_classes=2).numpy()\nplot_confusion_matrix(cm, ['NORMAL', 'PNEUMONIA'], 'Test')","f5a0c7f1":"# TODO:\n# 1. Measure AUC during training\n# +2. Measure validation error during training\n# +3. Early stop\n# 4. Preprocess images - resnet\/imagenet tf.keras.applications.resnet_v2.preprocess_input\n# +5. Data augmentation https:\/\/www.tensorflow.org\/guide\/keras\/transfer_learning","5b78931f":"# Evaluate predictions","01c62b30":"# Import chest dataset","eada4039":"import matplotlib.pyplot as plt\nimport keras.backend as K\nfrom keras.callbacks import Callback\nfrom keras.callbacks import LearningRateScheduler\n# https:\/\/www.jeremyjordan.me\/nn-learning-rate\/\nclass LRFinder(Callback):\n    \n    '''\n    A simple callback for finding the optimal learning rate range for your model + dataset. \n    \n    # Usage\n        ```python\n            lr_finder = LRFinder(min_lr=1e-5, \n                                 max_lr=1e-2, \n                                 steps_per_epoch=np.ceil(epoch_size\/batch_size), \n                                 epochs=3)\n            model.fit(X_train, Y_train, callbacks=[lr_finder])\n            \n            lr_finder.plot_loss()\n        ```\n    \n    # Arguments\n        min_lr: The lower bound of the learning rate range for the experiment.\n        max_lr: The upper bound of the learning rate range for the experiment.\n        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size\/batch_size)`. \n        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n        \n    # References\n        Blog post: jeremyjordan.me\/nn-learning-rate\n        Original paper: https:\/\/arxiv.org\/abs\/1506.01186\n\n    '''\n    \n    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n        super().__init__()\n        \n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.total_iterations = steps_per_epoch * epochs\n        self.iteration = 0\n        self.history = {}\n        \n    def clr(self):\n        '''Calculate the learning rate.'''\n        x = self.iteration \/ self.total_iterations \n        return self.min_lr + (self.max_lr-self.min_lr) * x\n        \n    def on_train_begin(self, logs=None):\n        '''Initialize the learning rate to the minimum value at the start of training.'''\n        logs = logs or {}\n        K.set_value(self.model.optimizer.lr, self.min_lr)\n        \n    def on_batch_end(self, epoch, logs=None):\n        '''Record previous batch statistics and update the learning rate.'''\n        logs = logs or {}\n        self.iteration += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.iteration)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n            \n        K.set_value(self.model.optimizer.lr, self.clr())\n \n    def plot_lr(self):\n        '''Helper function to quickly inspect the learning rate schedule.'''\n        plt.plot(self.history['iterations'], self.history['lr'])\n        plt.yscale('log')\n        plt.xlabel('Iteration')\n        plt.ylabel('Learning rate')\n        plt.show()\n        \n    def plot_loss(self):\n        '''Helper function to quickly observe the learning rate experiment results.'''\n        plt.plot(self.history['lr'], self.history['loss'])\n        plt.xscale('log')\n        plt.xlabel('Learning rate')\n        plt.ylabel('Loss')\n        plt.show()\n\nlr_finder = LRFinder(min_lr=1e-5, \n                     max_lr=1e-2, \n                     steps_per_epoch=len(train_ds), \n                     epochs=3)\nmodel.fit(train, callbacks=[lr_finder])\n\nlr_finder.plot_loss()\n","5cc5bba1":"# Data augmentation","6ec28758":"Overfitting generally occurs when there are a small number of training examples. Data augmentation takes the approach of generating additional training data from your existing examples by augmenting them using random transformations that yield believable-looking images. This helps expose the model to more aspects of the data and generalize better.\n\nYou will implement data augmentation using experimental Keras Preprocessing Layers. These can be included inside your model like other layers, and run on the GPU.\n\nhttps:\/\/www.tensorflow.org\/tutorials\/images\/classification","75f1a11a":"# Train and evaluate a model","6a578742":"from tensorflow.keras.callbacks import *\nfrom tensorflow.keras import backend as K\nimport numpy as np\n# https:\/\/github.com\/bckenstler\/CLR\/blob\/master\/clr_callback.py\n\nclass CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https:\/\/arxiv.org\/abs\/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w\/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi\/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1\/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary\/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations\/(2*self.step_size))\n        x = np.abs(self.clr_iterations\/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())\n        \nlr_finder = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n# model.fit(train, callbacks=[lr_finder])\n"}}