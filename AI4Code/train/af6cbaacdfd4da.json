{"cell_type":{"87e391dd":"code","89200a5a":"code","0b9b4f18":"code","2523c1cf":"code","8afbccfc":"code","13c203f0":"code","611d1767":"code","14d9f1e5":"code","3a78d546":"code","add7613e":"code","178ca646":"code","f09ce778":"code","8249a0f4":"markdown","44d3e6e5":"markdown","993f8d68":"markdown","a98c86c9":"markdown","872777dd":"markdown","26cf9930":"markdown","fe92e289":"markdown","907bf472":"markdown","3c269ea6":"markdown","49f04462":"markdown","ae8d8af0":"markdown","9bae2f22":"markdown"},"source":{"87e391dd":"# import librairies\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport math as math\nimport time \nplt.style.use('seaborn')\nplt.rcParams['figure.figsize'] = [14,14]","89200a5a":"# load the data\ndf = pd.read_csv('\/kaggle\/input\/netflix-shows\/netflix_titles.csv')\n# convert to datetime\ndf[\"date_added\"] = pd.to_datetime(df['date_added'])\ndf['year'] = df['date_added'].dt.year\ndf['month'] = df['date_added'].dt.month\ndf['day'] = df['date_added'].dt.day\n# convert columns \"director, listed_in, cast and country\" in columns that contain a real list\n# the strip function is applied on the elements\n# if the value is NaN, the new column contains a empty list []\ndf['directors'] = df['director'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\ndf['categories'] = df['listed_in'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\ndf['actors'] = df['cast'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\ndf['countries'] = df['country'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\n\ndf.head()","0b9b4f18":"print(df.shape)","2523c1cf":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom sklearn.cluster import MiniBatchKMeans\n\n# Build the tfidf matrix with the descriptions\nstart_time = time.time()\ntext_content = df['description']\nvector = TfidfVectorizer(max_df=0.4,         # drop words that occur in more than X percent of documents\n                             min_df=1,      # only use words that appear at least X times\n                             stop_words='english', # remove stop words\n                             lowercase=True, # Convert everything to lower case \n                             use_idf=True,   # Use idf\n                             norm=u'l2',     # Normalization\n                             smooth_idf=True # Prevents divide-by-zero errors\n                            )\ntfidf = vector.fit_transform(text_content)\n\n# Clustering  Kmeans\nk = 200\nkmeans = MiniBatchKMeans(n_clusters = k)\nkmeans.fit(tfidf)\ncenters = kmeans.cluster_centers_.argsort()[:,::-1]\nterms = vector.get_feature_names()\n\n# print the centers of the clusters\n# for i in range(0,k):\n#     word_list=[]\n#     print(\"cluster%d:\"% i)\n#     for j in centers[i,:10]:\n#         word_list.append(terms[j])\n#     print(word_list) \n    \nrequest_transform = vector.transform(df['description'])\n# new column cluster based on the description\ndf['cluster'] = kmeans.predict(request_transform) \n\ndf['cluster'].value_counts().head()\n","8afbccfc":"# Find similar : get the top_n movies with description similar to the target description \ndef find_similar(tfidf_matrix, index, top_n = 5):\n    cosine_similarities = linear_kernel(tfidf_matrix[index:index+1], tfidf_matrix).flatten()\n    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n    return [index for index in related_docs_indices][0:top_n]  ","13c203f0":"G = nx.Graph(label=\"MOVIE\")\nstart_time = time.time()\nfor i, rowi in df.iterrows():\n    if (i%1000==0):\n        print(\" iter {} -- {} seconds --\".format(i,time.time() - start_time))\n    G.add_node(rowi['title'],key=rowi['show_id'],label=\"MOVIE\",mtype=rowi['type'],rating=rowi['rating'])\n#    G.add_node(rowi['cluster'],label=\"CLUSTER\")\n#    G.add_edge(rowi['title'], rowi['cluster'], label=\"DESCRIPTION\")\n    for element in rowi['actors']:\n        G.add_node(element,label=\"PERSON\")\n        G.add_edge(rowi['title'], element, label=\"ACTED_IN\")\n    for element in rowi['categories']:\n        G.add_node(element,label=\"CAT\")\n        G.add_edge(rowi['title'], element, label=\"CAT_IN\")\n    for element in rowi['directors']:\n        G.add_node(element,label=\"PERSON\")\n        G.add_edge(rowi['title'], element, label=\"DIRECTED\")\n    for element in rowi['countries']:\n        G.add_node(element,label=\"COU\")\n        G.add_edge(rowi['title'], element, label=\"COU_IN\")\n    \n    indices = find_similar(tfidf, i, top_n = 5)\n    snode=\"Sim(\"+rowi['title'][:15].strip()+\")\"        \n    G.add_node(snode,label=\"SIMILAR\")\n    G.add_edge(rowi['title'], snode, label=\"SIMILARITY\")\n    for element in indices:\n        G.add_edge(snode, df['title'].loc[element], label=\"SIMILARITY\")\nprint(\" finish -- {} seconds --\".format(time.time() - start_time))        ","611d1767":"def get_all_adj_nodes(list_in):\n    sub_graph=set()\n    for m in list_in:\n        sub_graph.add(m)\n        for e in G.neighbors(m):        \n                sub_graph.add(e)\n    return list(sub_graph)\ndef draw_sub_graph(sub_graph):\n    subgraph = G.subgraph(sub_graph)\n    colors=[]\n    for e in subgraph.nodes():\n        if G.nodes[e]['label']==\"MOVIE\":\n            colors.append('blue')\n        elif G.nodes[e]['label']==\"PERSON\":\n            colors.append('red')\n        elif G.nodes[e]['label']==\"CAT\":\n            colors.append('green')\n        elif G.nodes[e]['label']==\"COU\":\n            colors.append('yellow')\n        elif G.nodes[e]['label']==\"SIMILAR\":\n            colors.append('orange')    \n        elif G.nodes[e]['label']==\"CLUSTER\":\n            colors.append('orange')\n\n    nx.draw(subgraph, with_labels=True, font_weight='bold',node_color=colors)\n    plt.show()\n\n","14d9f1e5":"list_in=[\"Ocean's Twelve\",\"Ocean's Thirteen\"]\nsub_graph = get_all_adj_nodes(list_in)\ndraw_sub_graph(sub_graph)","3a78d546":"def get_recommendation(root):\n    commons_dict = {}\n    for e in G.neighbors(root):\n        for e2 in G.neighbors(e):\n            if e2==root:\n                continue\n            if G.nodes[e2]['label']==\"MOVIE\":\n                commons = commons_dict.get(e2)\n                if commons==None:\n                    commons_dict.update({e2 : [e]})\n                else:\n                    commons.append(e)\n                    commons_dict.update({e2 : commons})\n    movies=[]\n    weight=[]\n    for key, values in commons_dict.items():\n        w=0.0\n        for e in values:\n            w=w+1\/math.log(G.degree(e))\n        movies.append(key) \n        weight.append(w)\n    \n    result = pd.Series(data=np.array(weight),index=movies)\n    result.sort_values(inplace=True,ascending=False)        \n    return result;","add7613e":"result = get_recommendation(\"Ocean's Twelve\")\nresult2 = get_recommendation(\"Ocean's Thirteen\")\nresult3 = get_recommendation(\"The Devil Inside\")\nresult4 = get_recommendation(\"Stranger Things\")\nprint(\"*\"*40+\"\\n Recommendation for 'Ocean's Twelve'\\n\"+\"*\"*40)\nprint(result.head())\nprint(\"*\"*40+\"\\n Recommendation for 'Ocean's Thirteen'\\n\"+\"*\"*40)\nprint(result2.head())\nprint(\"*\"*40+\"\\n Recommendation for 'Belmonte'\\n\"+\"*\"*40)\nprint(result3.head())\nprint(\"*\"*40+\"\\n Recommendation for 'Stranger Things'\\n\"+\"*\"*40)\nprint(result4.head())","178ca646":"reco=list(result.index[:4].values)\nreco.extend([\"Ocean's Twelve\"])\nsub_graph = get_all_adj_nodes(reco)\ndraw_sub_graph(sub_graph)","f09ce778":"reco=list(result4.index[:4].values)\nreco.extend([\"Stranger Things\"])\nsub_graph = get_all_adj_nodes(reco)\ndraw_sub_graph(sub_graph)","8249a0f4":"# KMeans clustering with TF-IDF","44d3e6e5":"# Recommendation engine with a graph\n>The purpose is to build a recommendation engine based on graph by using the Adamic Adar measure.<br\/> The more the measure is high, the closest are the two nodes.<br\/> The measures between all movies are NOT pre-calculated, in order to determine the list of recommendation films, we are going to explore the neighborhood of the target film\n\n# How to take in account of the description ?\n\n#### First idea ...\n> In order to take in account the description, the movie are clustered by applying a KMeans clustering with TF-IDF weights <br\/> So two movies that belong in a group of description will share a node.<br\/> The fewer the number of films in the group, the more this link will be taken into account \n\n**but it doesn't work because clusters are too unbalanced*\n\n#### Second idea ...\n> In order to take in account the description, calcul the TF-IDF matrix <br\/> and for each film, take the top 5 of similar descriptions and create a node Similar_to_this. This node will be taken in account in the Adamic Adar measure.\n\nI have publish a notebook that explains what is \"KMeans clustering with TF-IDF\" here :\nhttps:\/\/www.kaggle.com\/yclaudel\/find-similar-articles-with-tf-idf","993f8d68":"# Let's test it ...","a98c86c9":"<div class=\"alert alert-block alert-warning\"><span>&#171;<\/span>column cluster are not going to be used because clusters are two unbalanced <br\/> But tfidf will be used in order to find similar description<span>&#187;<\/span><\/div>","872777dd":"![strangerT.jpg](attachment:strangerT.jpg)","26cf9930":"# Draw top recommendations, to see the common nodes","fe92e289":"# Load the data","907bf472":"# To see what's going on,a sub-graph with only two movies ...","3c269ea6":"# The recommendation function\n<div class=\"alert alert-block alert-info\">\n<li> Explore the neighborhood of the target film <span>&#8594;<\/span> this is a list of actor, director, country, categorie<\/li>\n<li> Explore the neighborhood of each neighbor <span>&#8594;<\/span> discover the movies that share a node with the target field<\/li>\n<li> Calcul Adamic Adar measure <span>&#8594;<\/span> final results<\/li>\n<\/div>","49f04462":"<div class=\"alert alert-block alert-info\"><b><span>&#171;<\/span> please don't forget to upvote, that will keep me motivated <span>&#187;<\/span><\/b><\/div> ","ae8d8af0":"# Load the graph (undirected graph)\nNodes are :\n* Movies\n* Person ( actor or director)\n* Categorie\n* Countrie\n* Cluster (description)\n* Sim(title) top 5 similar movies in the sense of the description\n\nEdges are :\n* ACTED_IN : relation between an actor and a movie\n* CAT_IN : relation between a categrie and a movie\n* DIRECTED : relation between a director and a movie\n* COU_IN : relation between a country and a movie\n* DESCRIPTION : relation between a cluster and a movie\n* SIMILARITY in the sense of the description\n\n<span>&#171;<\/span>so, two movies are not directly connected, but they share persons, categories,clusters and countries<span>&#187;<\/span>\n","9bae2f22":"# Adamic Adar measure\nIt is a measure used to compute the closeness of nodes based on their shared neighbors.\n\n* x and y are 2 nodes (2 Movies)\n* N(one_node) is a function that return the set of adjacent nodes  to one_node\n\n$$ adamicAdar(x,y)=  \\sum_{ u \\in N(x) \\cap N(y)} \\frac{1}{log(N(u))}  $$\n\n<span>&#171;<\/span>say otherwise, for each node u in common to x and y, add to the measure 1\/log(N(u))<span>&#187;<\/span>\n\nThe quantity $ \\frac{1}{log(N(u))} $ determine the importance of u in the measure.\n* if x and y share a node u that has a lot of adjacent nodes, this node is not really relevant. <span>&#8594;<\/span> N(u) is high <span>&#8594;<\/span> 1\/log(N(u)) is not high\n* if x and y share a node u that **not** has a lot of adjacent nodes, this node is **really** relevant. <span>&#8594;<\/span> N(u) is **not** high <span>&#8594;<\/span> 1\/log(N(u)) is higher\n"}}