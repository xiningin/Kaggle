{"cell_type":{"c84d7dd6":"code","4fc73655":"code","87114121":"code","a78c1b5b":"code","ad93f166":"code","0124eafb":"code","01389ba4":"code","54e8fcd6":"code","e115db13":"code","0a64f409":"code","1a9c3d75":"code","348f0cd7":"code","092fd789":"code","63658b25":"code","a1855477":"code","f10ec396":"code","bafdf076":"code","daee5645":"code","e3359716":"code","29dbce1d":"code","710d475b":"code","25a11bdf":"code","a29f6d84":"code","4ba7b7c0":"code","be5fff24":"code","9bbbdce3":"code","4fd0b1fa":"code","09d737fc":"code","dd09c4be":"code","ff7c4f7a":"code","10068948":"code","9c170c15":"code","780c83ab":"code","a0e6a06e":"code","a0a7e22d":"code","d4cd9c22":"code","93c35a04":"code","428dd425":"code","1de342de":"code","2df4d544":"code","7705d4f9":"code","93b6ef90":"code","068d9bc8":"code","3d5a7908":"code","61e3b1aa":"code","adf769af":"code","e47029df":"code","bd9b9e06":"code","0e5af5b0":"code","cbb809db":"code","0de6f2c1":"code","bb1e05b3":"code","af4b9abe":"code","53800e54":"code","b3077b07":"code","ee7c908a":"code","fc8c7eb9":"code","cbfc00a3":"markdown","90abdbba":"markdown","50529913":"markdown","1fdbfa95":"markdown","660506ec":"markdown","daa2a410":"markdown","b3313c48":"markdown","30f66e1f":"markdown","e7f7ba4f":"markdown","7ae2aed6":"markdown","6d68b576":"markdown","6bc1c87e":"markdown","aa255934":"markdown","a272404f":"markdown","9107c204":"markdown","a879bacf":"markdown","db59f0cb":"markdown"},"source":{"c84d7dd6":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4fc73655":"data = pd.read_csv('..\/input\/airbnb-recruiting-new-user-bookings\/train_users_2.csv.zip')\nprint(data.shape)\ndata.head()","87114121":"data_explore = data.copy()","a78c1b5b":"data_explore = data_explore.drop(['id'], axis=1)","ad93f166":"data_explore.info()","0124eafb":"dac = np.vstack(data_explore.date_account_created.astype(str).apply(lambda x: list(map(int, x.split('-')))).values)\ndata_explore['dac_year'] = dac[:,0]\ndata_explore['dac_month'] = dac[:,1]\ndata_explore['dac_day'] = dac[:,2]\ndata_explore = data_explore.drop(['date_account_created'], axis=1)","01389ba4":"data_explore[data_explore['country_destination']!='NDF']['date_first_booking'].isna().sum()","54e8fcd6":"data_explore.date_first_booking = data_explore.date_first_booking.fillna('2000-01-01')\nfirst_booking = np.vstack(data_explore.date_first_booking.astype(str).apply(lambda x: list(map(int, x.split('-')))).values)\ndata_explore['first_booking_year'] = first_booking[:,0]\ndata_explore['first_booking_month'] = first_booking[:,1]\ndata_explore['first_booking_day'] = first_booking[:,2]\ndata_explore = data_explore.drop(['date_first_booking'], axis=1)","e115db13":"data_explore.nunique()","0a64f409":"data_explore.describe()","1a9c3d75":"data_explore.isna().sum()","348f0cd7":"age_values = data_explore.age.values\ndata_explore['age'] = np.where(age_values>1000, np.random.randint(28, 43), age_values)\ndata_explore['age'] = data_explore['age'].fillna(np.random.randint(28, 43))\n\ndata_explore['first_affiliate_tracked'] = data_explore['first_affiliate_tracked'].fillna(data_explore['first_affiliate_tracked'].mode().values[0])","092fd789":"data_explore['language'].value_counts()[:10]","63658b25":"def plot_histogram(data):\n    ax = plt.gca()\n    counts, _, patches = ax.hist(data)\n    for count, patch in zip(counts, patches):\n        if count>0:\n            ax.annotate(str(int(count)), xy=(patch.get_x(), patch.get_height()+5))\n    if data.name:\n        plt.xlabel(data.name)","a1855477":"plt.figure(figsize=(8, 5))\nplot_histogram(data_explore['age'])\nplt.xlim(15, 100)\nplt.show()","f10ec396":"plt.figure(figsize=(15, 6))\nplt.subplot(1, 3, 1)\ngrp = data_explore[['gender', 'age']].groupby(by='gender').count()\nplt.pie(grp.values, labels=list(grp.index), shadow=True, startangle=0,\n        autopct='%1.1f%%', wedgeprops={'edgecolor':'black'})\nplt.title('Gender')\nplt.subplot(1, 3, 2)\ngrp = data_explore[['dac_year', 'age']].groupby(by='dac_year').count()\nplt.pie(grp.values, labels=list(grp.index), shadow=True, startangle=0,\n        autopct='%1.1f%%', wedgeprops={'edgecolor':'black'})\nplt.title('Account Created: Year')\nplt.subplot(1, 3, 3)\ngrp = data_explore[['dac_month', 'age']].groupby(by='dac_month').count()\nplt.pie(grp.values, labels=list(grp.index), shadow=True, startangle=0,\n        autopct='%1.1f%%', wedgeprops={'edgecolor':'black'})\nplt.title('Account Created: Month')\nplt.show()","bafdf076":"ax = sns.countplot(x='affiliate_channel', data=data_explore)\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+10))\nplt.xticks(rotation=-45)\nplt.show()","daee5645":"plt.figure(figsize=(16, 7))\nplt.subplot(1, 2, 1)\ngrp = data_explore[['affiliate_channel', 'age']].groupby(by='affiliate_channel').count()\nplt.pie(grp.values, labels=list(grp.index), shadow=True, startangle=0,\n        autopct='%1.1f%%', wedgeprops={'edgecolor':'black'})\nplt.title('Affiliate Channels')\nplt.subplot(1, 2, 2)\nax = sns.countplot(x='affiliate_provider', data=data_explore)\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+10))\nplt.xticks(rotation=-45)\nplt.xlim(-0.5, 10.5)\nplt.title('Affiliate Providers')\nplt.show()","e3359716":"plt.figure(figsize=(10, 6))\nax = sns.countplot(x='country_destination', data=data_explore)\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+10))","29dbce1d":"data_explore_booked = data_explore[data_explore['country_destination']!='NDF']\ndata_explore.shape, data_explore_booked.shape","710d475b":"plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplot_histogram(data_explore_booked['dac_year'])\nplt.subplot(1, 2, 2)\nplot_histogram(data_explore_booked['dac_month'])\nplt.show()","25a11bdf":"plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplot_histogram(data_explore_booked['first_booking_year'])\nplt.subplot(1, 2, 2)\nplot_histogram(data_explore_booked['first_booking_month'])\nplt.show()","a29f6d84":"plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplot_histogram(data_explore_booked[data_explore_booked['country_destination']=='US']['first_booking_year'])\nplt.title('# of Booking in US')\nplt.subplot(1, 2, 2)\nplot_histogram(data_explore_booked[data_explore_booked['country_destination']=='US']['first_booking_month'])\nplt.title('# of Booking in US')\nplt.show()","4ba7b7c0":"plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplot_histogram(data_explore_booked[data_explore_booked['country_destination']=='FR']['first_booking_year'])\nplt.title('# of Booking in France')\nplt.subplot(1, 2, 2)\nplot_histogram(data_explore_booked[data_explore_booked['country_destination']=='FR']['first_booking_month'])\nplt.title('# of Booking in France')\nplt.show()","be5fff24":"plt.figure(figsize=(12, 6))\nsns.countplot(x='country_destination', hue='gender', data=data_explore_booked)\nplt.title('Geneder distribution across destination countries')\nplt.show()","9bbbdce3":"plt.figure(figsize=(12, 6))\nsns.countplot(x='first_booking_year', hue='gender', data=data_explore_booked[data_explore_booked['country_destination']=='US'])\nplt.title('# of Travellers to USA')\nplt.show()","4fd0b1fa":"plt.figure(figsize=(12, 6))\nsns.countplot(x='first_booking_year', hue='gender', data=data_explore_booked[data_explore_booked['country_destination']=='FR'])\nplt.title('# of Travellers to France')\nplt.show()","09d737fc":"plt.figure(figsize=(15, 6))\nsns.boxplot(x='country_destination', y='age', hue='gender', data=data_explore_booked)\nplt.ylim(15, 60)\nplt.legend(loc='lower right')\nplt.show()","dd09c4be":"plt.figure(figsize=(15, 6))\nsns.boxplot(x='dac_year', y='age', hue='gender', data=data_explore_booked)\nplt.ylim(15, 60)\nplt.legend(loc='lower right')\nplt.show()","ff7c4f7a":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer","10068948":"X = data.drop(columns=['country_destination'], axis=1).copy()\ny = data['country_destination'].copy()\n\nlabel_enc = LabelEncoder()\ny = label_enc.fit_transform(y)\nX.shape, y.shape","9c170c15":"label_enc.classes_","780c83ab":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train.shape, X_test.shape","a0e6a06e":"cat_attrs = ['gender', 'language', 'affiliate_channel', 'affiliate_provider']","a0a7e22d":"pre_process = ColumnTransformer([('drop_cols', 'drop', ['id', 'date_first_booking', 'date_account_created', 'signup_method', 'timestamp_first_active', \n                                                        'signup_app', 'first_device_type', 'first_browser', 'first_affiliate_tracked', 'signup_flow']),\n                                 ('num_imputer', SimpleImputer(strategy='median'), ['age']),\n                                 ('cat_imputer', SimpleImputer(strategy='most_frequent'), cat_attrs)], remainder='passthrough')\n\nX_train_transformed = pre_process.fit_transform(X_train)\nX_test_transformed = pre_process.transform(X_test)\nX_train_transformed.shape, X_test_transformed.shape","d4cd9c22":"X_train_transformed = pd.DataFrame(X_train_transformed, columns=['age', 'gender', 'language', 'affiliate_channel', 'affiliate_provider'])\nX_test_transformed = pd.DataFrame(X_test_transformed, columns=['age', 'gender', 'language', 'affiliate_channel', 'affiliate_provider'])\nX_train_transformed.shape, X_test_transformed.shape","93c35a04":"from sklearn.model_selection import GridSearchCV, KFold, cross_val_score","428dd425":"kf = KFold(n_splits=5, shuffle=True, random_state=42)","1de342de":"from sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import make_scorer, ndcg_score\nndcg_scorer = make_scorer(ndcg_score, needs_proba=True, k=5)\n\ndef dcg_score(y_true, y_score, k=5):\n    \"\"\"Discounted cumulative gain (DCG) at rank K.\n\n    Parameters\n    ----------\n    y_true : array, shape = [n_samples]\n        Ground truth (true relevance labels).\n    y_score : array, shape = [n_samples, n_classes]\n        Predicted scores.\n    k : int\n        Rank.\n\n    Returns\n    -------\n    score : float\n    \"\"\"\n    order = np.argsort(y_score)[::-1]\n    y_true = np.take(y_true, order[:k])\n\n    gain = 2 ** y_true - 1\n\n    discounts = np.log2(np.arange(len(y_true)) + 2)\n    return np.sum(gain \/ discounts)\n\n\ndef ndcg_score(ground_truth, predictions, k=5):\n    \"\"\"Normalized discounted cumulative gain (NDCG) at rank K.\n\n    Normalized Discounted Cumulative Gain (NDCG) measures the performance of a\n    recommendation system based on the graded relevance of the recommended\n    entities. It varies from 0.0 to 1.0, with 1.0 representing the ideal\n    ranking of the entities.\n\n    Parameters\n    ----------\n    ground_truth : array, shape = [n_samples]\n        Ground truth (true labels represended as integers).\n    predictions : array, shape = [n_samples, n_classes]\n        Predicted probabilities.\n    k : int\n        Rank.\n\n    Returns\n    -------\n    score : float\n\n    Example\n    -------\n    >>> ground_truth = [1, 0, 2]\n    >>> predictions = [[0.15, 0.55, 0.2], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]\n    >>> score = ndcg_score(ground_truth, predictions, k=2)\n    1.0\n    >>> predictions = [[0.9, 0.5, 0.8], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]\n    >>> score = ndcg_score(ground_truth, predictions, k=2)\n    0.6666666666\n    \"\"\"\n    lb = LabelBinarizer()\n    lb.fit(range(len(predictions) + 1))\n    T = lb.transform(ground_truth)\n\n    scores = []\n\n    # Iterate over each y_true and compute the DCG score\n    for y_true, y_score in zip(T, predictions):\n        actual = dcg_score(y_true, y_score, k)\n        best = dcg_score(y_true, y_true, k)\n        score = float(actual) \/ float(best)\n        scores.append(score)\n\n    return np.mean(scores)\n\n\n# NDCG Scorer function\nndcg_scorer = make_scorer(ndcg_score, needs_proba=True, k=5)","2df4d544":"def grid_search(model, grid_param):\n    print(\"Obtaining Best Model for {}\".format(model.__class__.__name__))\n    grid_search = GridSearchCV(model, grid_param, cv=kf, scoring=ndcg_scorer, return_train_score=True, n_jobs=-1)\n    grid_search.fit(X_train_transformed, y_train)\n    \n    print(\"Best Parameters: \", grid_search.best_params_)\n    print(\"Best Score: \", grid_search.best_score_)\n    \n    cvres = grid_search.cv_results_\n    print(\"Results for each run of {}...\".format(model.__class__.__name__))\n    for train_mean_score, test_mean_score, params in zip(cvres[\"mean_train_score\"], cvres[\"mean_test_score\"], cvres[\"params\"]):\n        print(train_mean_score, test_mean_score, params)\n        \n    return grid_search.best_estimator_","7705d4f9":"results = []\n    \ndef performance_measures(model, store_results=True):\n    train_ndcg = cross_val_score(model, X_train_transformed, y_train, scoring=ndcg_scorer, cv=kf, n_jobs=-1)\n    test_ndcg = cross_val_score(model, X_test_transformed, y_test, scoring=ndcg_scorer, cv=kf, n_jobs=-1)\n    print(\"Mean Train NDGC: {}\\nMean Test NDGC: {}\".format(train_ndcg.mean(), test_ndcg.mean()))","93b6ef90":"def plot_feature_importance(feature_columns, importance_values,top_n_features=0):\n    feature_imp = [ col for col in zip(feature_columns, importance_values)]\n    feature_imp.sort(key=lambda x:x[1], reverse=True)\n\n    if top_n_features:\n        imp = pd.DataFrame(feature_imp[0:top_n_features], columns=['feature', 'importance'])\n    else:\n        imp = pd.DataFrame(feature_imp, columns=['feature', 'importance'])\n    plt.figure(figsize=(10, 8))\n    sns.barplot(y='feature', x='importance', data=imp, orient='h')\n    plt.title('Most Important Features', fontsize=16)\n    plt.ylabel(\"Feature\", fontsize=16)\n    plt.xlabel(\"\")\n    plt.show()","068d9bc8":"from catboost import CatBoostClassifier\n\n\ncatboost_grid_params = [{'iterations':[500, 1000, 1500], 'depth':[4, 6, 8, 10],}]\n\ncatboost_clf = CatBoostClassifier(task_type=\"GPU\", loss_function='MultiClass', bagging_temperature=0.3, \n                                  cat_features=[1, 2, 3, 4], random_state=42, verbose=0)\n\ngrid_search_results = catboost_clf.grid_search(catboost_grid_params,\n            X_train_transformed,\n            y_train,\n            cv=5,\n            partition_random_seed=42,\n            calc_cv_statistics=True,\n            search_by_train_test_split=True,\n            refit=True,\n            shuffle=True,\n            stratified=None,\n            train_size=0.8,\n            verbose=0,\n            plot=False)","3d5a7908":"grid_search_results['params']","61e3b1aa":"catboost_clf.is_fitted()","adf769af":"catboost_clf.feature_importances_","e47029df":"plot_feature_importance(['age', 'gender', 'language', 'affiliate_channel', 'affiliate_provider'], catboost_clf.feature_importances_)","bd9b9e06":"performance_measures(catboost_clf, store_results=False)","0e5af5b0":"X_trasformed = pre_process.transform(X)\npredicted_country = catboost_clf.predict(X_trasformed)\npredicted_country = label_enc.inverse_transform(predicted_country)\ndata['predicted_country'] = predicted_country","cbb809db":"plt.figure(figsize=(10, 10))\nplt.subplot(2, 1, 1)\nax = sns.countplot(x='country_destination', data=data)\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+10))\nplt.subplot(2, 1, 2)\nax = sns.countplot(x='predicted_country', data=data)\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+10))","0de6f2c1":"final_model = Pipeline([('pre_process', pre_process),\n                        ('catboost_clf', catboost_clf)])\nfinal_model.fit(X_train, y_train)","bb1e05b3":"test_data = pd.read_csv('..\/input\/airbnb-recruiting-new-user-bookings\/test_users.csv.zip')\ntest_data.head()","af4b9abe":"test_data.info()","53800e54":"predictions = final_model.predict_proba(test_data)","b3077b07":"#Taking the 5 classes with highest probabilities\nid_test = list(test_data.id)\nids = []\ncountries = []\nfor i in range(len(id_test)):\n    idx = id_test[i]\n    ids += [idx] * 5\n    countries += label_enc.inverse_transform(np.argsort(predictions[i])[::-1])[:5].tolist()","ee7c908a":"output = pd.DataFrame(np.column_stack((ids, countries)), columns=['id', 'country'])\noutput.head()","fc8c7eb9":"output.to_csv(\".\/submission.csv\", index=False)","cbfc00a3":"## Step 1: Frame the Problem","90abdbba":"- I will drop columns which gives information about users first activity on website, the device that has been used and date of first booking, Since all that inforamtion is reduandant for making predictions.","50529913":"## Step 5: Prediction Analysis","1fdbfa95":"- <b>Objective: <\/b>In this challenge, we have a list of users along with information of their activity on website such as date of account created, time when user first active on the website, the country for which user has done booking etc. We also got have some personal information about each user. Our task is to build a machine learning model which will predict which country a new user's first booking destination will be.\n\n\n- <b>Data: <\/b>Following are the features present in training dataset:\n    - id: user id\n    - date_account_created: the date of account creation\n    - timestamp_first_active: timestamp of the first activity, note that it can be earlier than date_account_created or date_first_booking because a user can search before signing up\n    - date_first_booking: date of first booking\n    - gender\n    - age\n    - signup_method: whether user has signup from website or by using facebook, gmail etc.\n    - signup_flow: the page a user came to signup up from\n    - language: international language preference\n    - affiliate_channel: what kind of paid marketing\n    - affiliate_provider: where the marketing is e.g. google, craigslist, other\n    - first_affiliate_tracked: whats the first marketing the user interacted with before the signing up\n    - signup_app\n    - first_device_type\n    - first_browser\n    - country_destination: this is the target variable. There are 12 possible outcomes of the destination country: 'US', 'FR', 'CA', 'GB', 'ES', 'IT', 'PT', 'NL','DE', 'AU', 'NDF' (no destination found), and 'other'. \n    <br>Note: \n        - 'other' means there was a booking, but is to a country not included in the list\n        - 'NDF' means there wasn't a booking.\n\n\n- There other 3 files given along with train and test dataset.\n    1. sessions.csv - this file contain all web sessions log for each user\n    2. countries.csv - summary statistics of destination countries in this dataset and their locations\n    3. age_gender_bkts.csv - summary statistics of users' age group, gender, country of destination","660506ec":"- In further analysis I will focus on the users which have done the booking.","daa2a410":"- Lets evaluate model's prediction on overall dataset.","b3313c48":"- Observation:\n    - Median age of people who are creating account is decreasing which indicates that many young peoples are attracted to website.\n    - In all years the median age of females is higher than males.","30f66e1f":"- This clears that there no missing value for first booking date column when there is booking done.","e7f7ba4f":"- There are several categorical columns. Lets explore them.","7ae2aed6":"- The dataset contains many categorical fetures. Performing one hot encoding on them will increase the dimensionality and inturn will increase the training time. \n- For dataset which has many categorical features, most suitable algorithm will be the CatBoost. CatBoost algorithm handles categorical features automatically using various statistical methods.\n- Evaluation metric will be Normalized Discounted Cumulative Gain (NDCG).","6d68b576":"## Step 3: Data Preprocessing","6bc1c87e":"Thanks to [NDCG Scorer](https:\/\/www.kaggle.com\/davidgasquez\/ndcg-scorer) kernel from where the scorer function for NDCG is taken.","aa255934":"## Step 6: Make submission","a272404f":"## Step 4: Modelling","9107c204":"## Step 2: Data Exploration","a879bacf":"- Max age value is 2014 which not valid. Hence I will replace all those values above 2000 by median age.","db59f0cb":"# Airbnb New User Bookings"}}