{"cell_type":{"f8723b6b":"code","7723d97b":"code","a74d6d40":"code","eb3dc60f":"code","ea6bd535":"code","b800200b":"code","004636d8":"code","c70f0e3b":"code","cf50928f":"code","80a00856":"code","1bce5d50":"code","79470dc7":"code","6f942765":"code","e355c602":"code","f22fdb82":"code","c12be53c":"code","f44f8018":"code","5ba0212e":"code","b04e85ed":"code","175c5426":"code","5a7472ff":"code","603a1022":"code","271ad472":"code","be543914":"code","0f49babb":"code","7a4c127f":"code","da82c2a3":"code","dc739021":"code","64db5532":"code","b4694e5c":"code","84e0a55e":"code","9f325953":"code","a8946026":"code","bb277d89":"code","539f3e23":"code","088361df":"code","eecb0049":"code","9c7dd5b7":"code","7f752d0f":"code","0b08be0f":"code","44e80b8d":"code","105edcaa":"code","3366a367":"code","73cc569b":"code","4b07a050":"code","4e447af3":"code","75b7e38c":"code","96fe226e":"code","ebbd7ba4":"code","38f222e6":"code","7027b935":"code","459d47f6":"code","066a576f":"code","40570ce1":"code","7c324533":"code","1c6e0b31":"code","741265d8":"code","68db7a65":"code","a9476324":"code","3c14c8e6":"code","8136bdf1":"code","56e1b7b4":"code","c5e6538a":"markdown","dce6767a":"markdown","2553ba70":"markdown","adcef6a9":"markdown","f336dc6b":"markdown","4b75ac68":"markdown","349c0202":"markdown","1e584692":"markdown","8e3febc7":"markdown","9b5af72e":"markdown","d32ca1d3":"markdown","35110bc5":"markdown","d1b45152":"markdown","fd621656":"markdown","fb1197bd":"markdown","7708b57d":"markdown","a99ffd07":"markdown","52f5c886":"markdown","972891a4":"markdown"},"source":{"f8723b6b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7723d97b":"train = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')","a74d6d40":"# Preview of the train dataset\ntrain.head()","eb3dc60f":"# Identify the number of numeric and non-numeric columns\nprint(train.select_dtypes(include='object').shape[1])\nprint()\nprint(train.select_dtypes(exclude='object').shape[1])","ea6bd535":"# Statistics summary of the train dataset\ntrain.describe()","b800200b":"# The 23 nominal categorical variables\nnominal_variables = ['MSSubClass', 'MSZoning', 'Street', 'Alley','LandContour',\n                     'LotConfig', 'Neighborhood', 'Condition1','Condition2', 'BldgType',\n                     'HouseStyle', 'RoofStyle', 'RoofMatl','Exterior1st', 'Exterior2nd',\n                     'MasVnrType', 'Foundation', 'Heating', 'CentralAir', 'GarageType',\n                     'MiscFeature', 'SaleType', 'SaleCondition']","004636d8":"# The 23 ordinal categorical variables\nordinal_variables = ['LotShape', 'Utilities', 'LandSlope', 'OverallQual', 'OverallCond',\n                     'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n                     'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'Electrical', 'KitchenQual',\n                     'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond',\n                     'PoolQC', 'Fence', 'PavedDrive']","c70f0e3b":"# The 20 continuous variables\ncontinuous_variables = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n                        'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                        'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch',\n                        '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'SalePrice']","cf50928f":"# The 14 discrete variables\ndiscrete_variables = ['YearBuilt', 'YearRemodAdd', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n                      'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n                      'GarageYrBlt', 'GarageCars', 'MoSold', 'YrSold']","80a00856":"# import visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set(style='darkgrid')","1bce5d50":"# Plot scatter plot of SalePrice vs. GrLivArea\nplt.figure(figsize=(10,8))\nsns.scatterplot(train['GrLivArea'], train['SalePrice'])\nplt.axvline(x=4000, c='r', linewidth=2)\nplt.title('Scatter Plot of Sale Price vs. Ground Living Area')\nplt.show()","79470dc7":"# We can now drop those rows\ntrain = train.drop(train[train['GrLivArea'] > 4000].index)\nprint(train.shape)\nprint()\nprint(test.shape)","6f942765":"# highly correlated features\ncorrelation = train.corr()\ntop_correlation = correlation.index[abs(correlation[\"SalePrice\"])>0.5]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(train[top_correlation].corr(),annot=True,cmap=\"coolwarm\")","e355c602":"# Plot histogram and probability plot before log transform\nplt.figure(figsize=(8,10))\nplt.subplot(2,1,1)\nsns.distplot(train['SalePrice'], bins=30)\nplt.axvline(x=train['SalePrice'].mean(), c='k', linewidth=2)\nplt.title('Histogram of Sale Prices')\nplt.show()\n\nplt.figure(figsize=(10,10))\nplt.subplot(2,2,1)\nstats.probplot(train['SalePrice'], plot=plt)\nplt.title('Probability plot of Sale Prices')\nplt.show()","f22fdb82":"# Check skewness of the target variable\ntrain['SalePrice'].skew()","c12be53c":"# Create a copy and perform log transform\ntrain_copy1 = train.copy()\ntrain_copy1['SalePrice'] = np.log(train_copy1['SalePrice'])","f44f8018":"# Import scipy special's boxcox library\nfrom scipy.special import boxcox1p\n\n# we will not go into detail on which lambda to select but the idea is\n# the lambda will affect the transformed data's skewness\ntrain_copy2 = train.copy()\nlam = 0.15\ntrain_copy2['SalePrice'] = boxcox1p(train_copy2['SalePrice'], lam)","5ba0212e":"# Plot histogram and probability plots\nplt.figure(figsize=(20, 10))\nplt.subplot(2,2,1)\nsns.distplot(train_copy1['SalePrice'], bins=30)\nplt.axvline(train_copy1['SalePrice'].mean(), c='k', linewidth=2)\nplt.title('Histogram of Log Transformed Sale Prices')\n\nplt.figure(figsize=(20, 10))\nplt.subplot(2,2,2)\nsns.distplot(train_copy2['SalePrice'], bins=30)\nplt.axvline(train_copy2['SalePrice'].mean(), c='k', linewidth=2)\nplt.title('Histogram of Box-Cox Transformed Sale Prices')\n\nplt.figure(figsize=(20,10))\nplt.subplot(2,2,3)\nstats.probplot(train_copy1['SalePrice'], plot=plt)\nplt.title('Probability plot of Log Transformed Sale Prices')\n\nplt.figure(figsize=(20,10))\nplt.subplot(2,2,4)\nstats.probplot(train_copy2['SalePrice'], plot=plt)\nplt.title('Probability plot of Box-Cox Transformed Sale Prices')\n\nplt.show()","b04e85ed":"# Skew values after transformation\nlog_skew = train_copy1['SalePrice'].skew()\nbc_skew = train_copy2['SalePrice'].skew()\n\nprint('Log Transform: {:.3f}\\nBox-Cox Transform: {:.3f}'.format(log_skew, bc_skew))","175c5426":"train['SalePrice'] = np.log(train['SalePrice'])","5a7472ff":"# look at the null values in train set\ntrain.isnull().sum().sort_values(ascending=False)[:19]","603a1022":"# look at null values in test set\ntest.isnull().sum().sort_values(ascending=False)[:33]","271ad472":"y = train[(train['BsmtExposure'].isnull()) & (train['BsmtQual'].notnull())].index\ntrain.loc[y, 'BsmtExposure'] = train.loc[y, 'BsmtExposure'].fillna('No')\n\nx = train[(train['BsmtFinType2'].isnull()) & (train['BsmtQual'].notnull())].index\ntrain.loc[x, 'BsmtFinType2'] = train.loc[x, 'BsmtFinType2'].fillna('Unf')","be543914":"# label the columns with 'NA' as a category. With an exception for 'GarageYrBlt' as those with no GarageQual\n# means there isnt a garage to begin with which will be filled with zeros.\ncol_to_fill_NA = ['PoolQC', 'MiscFeature', 'Alley', 'Fence',\n                  'FireplaceQu', 'GarageCond', 'GarageType', 'GarageFinish',\n                  'GarageQual', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n                  'BsmtFinType2', 'BsmtFinType1', 'MasVnrType']\n\ntrain[col_to_fill_NA] = train[col_to_fill_NA].fillna('NA')","0f49babb":"train['GarageYrBlt'] = train['GarageYrBlt'].fillna(0)\ntrain['MasVnrArea'] = train['MasVnrArea'].fillna(0)\ntrain['Electrical'] = train['Electrical'].fillna('SBrkr')","7a4c127f":"train['LotFrontage'] = train.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.fillna(x.median()))","da82c2a3":"id1 = test[test['Id'] == 2127].index\nid2 = test[test['Id'] == 2577].index","dc739021":"test.loc[id1,'GarageYrBlt'] = test.loc[id1, 'GarageYrBlt'].fillna(test.loc[id1,'YearBuilt'].values[0])\ntest.loc[id1,'GarageFinish'] = (test\n                                .groupby('GarageType')['GarageFinish']\n                                .apply(lambda x: x.fillna(x.mode().values[0]))\n                               )\ntest.loc[id1,'GarageQual'] = (test\n                              .groupby(['GarageYrBlt', 'GarageType'])['GarageQual']\n                              .apply(lambda x: x.fillna(x.mode().values[0]))\n                             )\ntest.loc[id1,'GarageCond'] = (test\n                              .groupby(['GarageYrBlt', 'GarageType'])['GarageCond']\n                              .apply(lambda x: x.fillna(x.mode().values[0]))\n                             )","64db5532":"test.loc[id2,'GarageYrBlt'] = test.loc[id2, 'GarageYrBlt'].fillna(test.loc[id1,'YearBuilt'].values[0])\ntest.loc[id2,'GarageFinish'] = (test\n                                .groupby('GarageType')['GarageFinish']\n                                .apply(lambda x: x.fillna(x.mode().values[0]))\n                               )\ntest.loc[id2,'GarageQual'] = (test\n                              .groupby(['GarageYrBlt', 'GarageType'])['GarageQual']\n                              .apply(lambda x: x.fillna(x.mode().values[0]))\n                             )\ntest.loc[id2,'GarageCond'] = (test\n                              .groupby(['GarageYrBlt', 'GarageType'])['GarageCond']\n                              .apply(lambda x: x.fillna(x.mode().values[0]))\n                             )\ntest.loc[id2, 'GarageCars'] = (test\n                               .groupby(['GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond'])['GarageCars']\n                               .apply(lambda x: x.fillna(x.median()))\n                              )\ntest.loc[id2, 'GarageArea'] = (test\n                               .groupby(['GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond'])['GarageArea']\n                               .apply(lambda x: x.fillna(x.median()))\n                              )","b4694e5c":"id3 = test[test['MasVnrArea'].notnull() & test['MasVnrType'].isnull()].index\ntest.loc[id3, 'MasVnrType'] = test.groupby('MasVnrArea')['MasVnrType'].apply(lambda x: x.fillna(x.mode().values[0]))","84e0a55e":"test.loc[:,['BsmtHalfBath', 'BsmtFullBath']] = test.loc[:,['BsmtHalfBath', 'BsmtFullBath']].fillna(0)\ntest[['Exterior1st', 'Exterior2nd']] = test[['Exterior1st', 'Exterior2nd']].fillna('VinylSd')\ntest['MSZoning'] = test['MSZoning'].fillna('RL')\ntest['Utilities'] = test['Utilities'].fillna('AllPub')\ntest['Functional'] = test['Functional'].fillna('Typ')\ntest['SaleType'] = test['SaleType'].fillna('WD')\ntest['KitchenQual'] = test['KitchenQual'].fillna('TA')\ntest['MasVnrArea'] = test['MasVnrArea'].fillna(0)","9f325953":"id4 = test[test['Id'] == 2121].index\ntest.loc[id4, ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']] = test.loc[id4, ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']].fillna(0)","a8946026":"# locate the ids with either BsmtQual is missing but with BsmtCond values and vice versa\nid5 = test[(\n            test['BsmtCond'].isnull() & test['BsmtQual'].notnull()\n            ) |  (\n                  test['BsmtCond'].notnull() & test['BsmtQual'].isnull()\n                  )\n          ].index\n\ntest.loc[id5, 'BsmtQual'] = test.loc[id5, 'BsmtQual'].fillna('TA')\ntest.loc[id5, 'BsmtCond'] = test.loc[id5, 'BsmtQual'].fillna('TA')","bb277d89":"id6 = test[(test['BsmtExposure'].isnull()) & (test['BsmtQual'].notnull())].index\ntest.loc[id6, 'BsmtExposure'] = test.loc[id6, 'BsmtExposure'].fillna('No')","539f3e23":"test[col_to_fill_NA] = test[col_to_fill_NA].fillna('NA')","088361df":"test['GarageYrBlt'] = test['GarageYrBlt'].fillna(0)","eecb0049":"test['LotFrontage'] = test.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.fillna(x.median()))","9c7dd5b7":"# Double checking the datasets\nprint(train.isnull().sum())\nprint()\nprint(test.isnull().sum())","7f752d0f":"# Combine both data sets and drop the 'Id' column\ndf = train.append(test)\ndf = df.drop('Id', axis=1)","0b08be0f":"# Convert these variables to str type\ndf[['MSSubClass', 'OverallQual', 'OverallCond']] = df[['MSSubClass', 'OverallQual', 'OverallCond']].astype(str)\ndf[['MoSold', 'YrSold']] = df[['MoSold', 'YrSold']].astype(str)","44e80b8d":"# Creating new features and lowering the cardinality of the dataset\ndf['TotalHouseSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\ndf['TotalPorchSF'] = df['OpenPorchSF'] + df['EnclosedPorch'] + df['3SsnPorch'] + df['ScreenPorch']\ndf['TotalBathrooms'] = df['BsmtFullBath'] + df['BsmtHalfBath'] + df['FullBath'] + df['HalfBath']\ndf['HasPool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndf['HasGarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndf['HasBasement'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndf['HasFireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ndf['HasWoodDeck'] = df['WoodDeckSF'].apply(lambda x: 1 if x > 0 else 0)","105edcaa":"# Drop the unwanted columns\ndrop_col = ['1stFlrSF', '2ndFlrSF', 'BsmtFullBath',\n            'BsmtHalfBath', 'FullBath', 'HalfBath', 'OpenPorchSF',\n            'EnclosedPorch', '3SsnPorch', 'ScreenPorch']\ndf = df.drop(drop_col, axis=1)","3366a367":"# Create a new dataframe consist of only numeric variables\nnum_df = df.select_dtypes(exclude = 'object')\n\n# Check skew of all numerical features\nnum_skew = num_df.apply(lambda x: x.skew()).sort_values(ascending=False)\nskew_df = pd.DataFrame({'Skew': num_skew})\nskew_df","73cc569b":"high_skew_df = skew_df[(skew_df['Skew']>0.5) | (skew_df['Skew']<-0.5)]\n\n# Exclude new features and year columns\nexclude_features = ['HasPool', 'HasGarage', 'HasBasement', 'HasFireplace', 'HasWoodDeck', 'YearBuilt', 'GarageYrBlt']\nhigh_skew_df = high_skew_df[high_skew_df.index.isin(exclude_features) == False]\nhigh_skew_features = high_skew_df.index\n\n# Perform box-cox transformation with specified lambda\nlam = 0.15\nfor feat in high_skew_features:\n    df[feat] = boxcox1p(df[feat], lam)","4b07a050":"# perform label encoding\nfrom sklearn.preprocessing import LabelEncoder\n    \nfor col in ordinal_variables:\n    df[col] = LabelEncoder().fit_transform(df[col])","4e447af3":"df.head()","75b7e38c":"# Perform one hot encoding\nfrom sklearn.preprocessing import OneHotEncoder\n\nnominal_variables = ['MSSubClass', 'MSZoning', 'Street', 'Alley','LandContour',\n                     'LotConfig', 'Neighborhood', 'Condition1','Condition2', 'BldgType',\n                     'HouseStyle', 'RoofStyle', 'RoofMatl','Exterior1st', 'Exterior2nd',\n                     'MasVnrType', 'Foundation', 'Heating', 'CentralAir', 'GarageType',\n                     'MiscFeature', 'SaleType', 'SaleCondition', 'MoSold', 'YrSold']\n\nencoded_features = []\nfor col in nominal_variables:\n    encoded_feat = OneHotEncoder().fit_transform(df[col].values.reshape(-1, 1)).toarray()\n    n = df[col].nunique()\n    cols = ['{}_{}'.format(col, n) for n in range(1, n + 1)]\n    encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n    encoded_df.index = df.index\n    encoded_features.append(encoded_df)\n\ndf = pd.concat([df, *encoded_features], axis=1).drop(nominal_variables, axis=1)\n\n","96fe226e":"df.head()","ebbd7ba4":"# Now we can split the data into train and test sets again\ndf_train = df[:1456]\ndf_test = df[1456:]","38f222e6":"# Compute training and test variables\nX_train = df_train.drop('SalePrice', axis=1)\ny_train = df_train['SalePrice']\nX_test = df_test.drop('SalePrice', axis=1)","7027b935":"# Just to confirm that both datasets has the same amount of columns\nprint(X_train.shape)\nprint(X_test.shape)","459d47f6":"# import necessary libraries\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score, KFold, train_test_split","066a576f":"# Create variables for all import regression models\nlin = LinearRegression()\nridge = Ridge()\nlasso = Lasso()\nrf = RandomForestRegressor()\ngb = GradientBoostingRegressor()","40570ce1":"# Define a function to calculate rsme for different models\n\nkfold = KFold(n_splits=10)\ndef rmsle_cv(model):\n    kfold = KFold(n_splits=10)\n    rmse= np.sqrt(abs(cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = kfold)))\n    return(rmse)","7c324533":"# Linear Regression\nscore = rmsle_cv(lin)\nprint('Linear Regression score: {:.3f}'.format(score.mean()))","1c6e0b31":"# Ridge Regression\nscore = rmsle_cv(ridge)\nprint('Ridge Regression score: {:.3f}'.format(score.mean()))","741265d8":"# Lasso Regression\nscore = rmsle_cv(lasso)\nprint('Lasso Regression score: {:.3f}'.format(score.mean()))","68db7a65":"# Gradient Boosting\nscore = rmsle_cv(gb)\nprint('Gradient Boosting score: {:.3f}'.format(score.mean()))","a9476324":"# Random Forest\nscore = rmsle_cv(rf)\nprint('Random Forest score: {:.3f}'.format(score.mean()))","3c14c8e6":"# Fitting the ridge model\nridge_model = ridge.fit(X_train, y_train)\n\n# Predicting prices\nX_pred = ridge_model.predict(X_test)","8136bdf1":"X_pred = np.expm1(X_pred)","56e1b7b4":"# Compute submission dataframe\n\noutput = pd.DataFrame()\noutput['Id'] = test['Id']\noutput['SalePrice'] = X_pred\noutput.to_csv('submission.csv',index=False)","c5e6538a":"# Handling the null values in both datasets\n\n**TRAIN SET**\n- PoolQC, MiscFeature, Alley, Fence, FireplaceQu, GarageCond, GarageType, GarageFinish, GarageQual, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType2, BsmtFinType1, MasVnrType have 'NA' as an option\n- LotFrontage has roughly 18% of nan values. We cannot drop this column as it holds some correlation with LotArea (0.426095) and SalePrice (0.351799). Intrinsically we can groupby the neighbourhood and compute the median value.\n- In BsmtExposure, theres one row where the exposure is labelled as nan but there is a basement. So we will replace this value with 'No'.\n- In BsmtFinType2, theres one row where it is labelled as nan but there is a basement. This cell will be replaced with the mode. ('Unf')\n- Replace all nan values in MasVnrArea to zero.\n- Replace missing Electrical row with the mode. ('SBrkr')\n\n**TEST SET**\n\n- LotFrontage has roughly 16% of nan values. We cannot drop this column as it holds high correlation with LotArea (0.644608). We can use the same way to fill the missing rows.\n- Replace missing MSZoning rows with the mode. ('RL')\n- Replace missing cells in BsmtHalfBath and BsmtFullBath in test set with zero.\n- Replace missing cells in Utilities in test set with 'AllPub'.\n- Replace missing cells in Functional in test set with 'Typ'. (Assume typical unless deductions are warranted)\n- Replace missing cells in BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF with 0 as there is no basement recorded in (ID = 2121).\n- For ID = 2218 & 2219, replace BsmtQual missing cells with the mode groupby BsmtCond. ('TA')\n- For ID = 2041, 2186 & 2525, replace BsmtCond missing cells with the mode groupby BsmtQual. ('TA')\n- For ID = 1488 & 2349, replace BsmtExposure missing cells with 'No'.\n- Replace missing cells in Exterior1st and Exterior 2nd with the mode. ('VinylSd')\n- Replace missing cell in SaleType with the mode. ('WD')\n- Replace missing cell in KitchenQual with the mode. ('TA')\n- GarageType has 76 null valued cells while GarageCond has 78 null valued cells. Locate the IDs for the 2 cells\n    - ID 2577\n    - ID 2127\n- For ID = 2127, the GarageYrblt will be replaced with the year of the house was built, GarageFinish will be filled with the mode groupby GarageType, GarageQual and GarageCond will be replaced with the mode grouped by GarageYrBlt and GarageType.\n- For ID = 2577, the GarageYrblt will be replaced with the year of the house was built, GarageFinish will be filled with the mode groupby GarageType, GarageQual and GarageCond will be replaced with the mode grouped by GarageYrBlt and GarageType. Replace missing cells in GarageArea and GarageCars with the median values based on the GarageYrblt, GarageFinish, GarageQual and GarageCond. \n- Replace missing MasVnrType cell in ID = 2611 with the mode groupby on MasVnrArea.","dce6767a":"We can see the distribution of SalePrice is heavily right skewed (showing a long right tail). And since this project focuses on machine learning with the assumption of normality in the distribution of the dataset, we simply just can't assume that the data we are working with is of normal distribution.A good approach is to transform the skewed data. Transformation technique is useful to stabilize variance, make the data more normal distribution-like, which improves the validity of measures of association. Such transformation methods are like the log-transformation, box-cox transformation, square-root transformation etc.\n\nWe can perform log transform to the target variable and plot the histogram and probabilty plot again. And from the histogram and probability plots below we can see that the sale price now is more towards being normalized at only roughly 0.065 skewness. One thing to note is that, it is very **IMPORTANT** to also transform numeric features that are skewed.\n\nWe will compare log transformation with box-cox transformation and we will choose the method that yields the better result.","2553ba70":"# Analyzing and dealing with skewed data when conducting regression analyses (target variable)","adcef6a9":"# Perform One Hot Encoding to all nominal variables","f336dc6b":"Now we have to re-transform the predicted sale prices back to their inital state","4b75ac68":"# So now we have sorted all the null values, we can then proceed to create new features\nKnowing that the prices for houses are mostly affected by the location, condition, size and type of the house, the facilities it provides, the year it was built and the year it was sold. Based on these intrinsic features that we can think off from a very basic point of view, we can now create and simplify the features that we need.\n\nTo make things easier, we are going to combine both train and test datasets for this part of the analysis and then separate them again before fitting it to our machine learning models. We can actually perform this step before starting the data cleaning as well. I personally prefer to work the datasets separately when handling null values.\n\nWe should also convert the categorical variables - ['MSSubClass', 'OverallQual', 'OverallCond'] from integers to string type. The year and month sold variables are converted into string type as well.","349c0202":" It seems that we yield much better skew using log transformation. We will proceed with using this method for the dependent variable.","1e584692":"So it seems like ridge regression is performing the best with the lowest RMSE. We will proceed with fitting the model and predict the housing sale prices.","8e3febc7":"# Analyzing correlation between target variable and numerical features\n\nFrom the heatmap below we can clearly see that the variables, OverallQual (0.8), GrLivArea (0.72) and TotalBsmtSF (0.65) show significant correlation with SalePrice.","9b5af72e":"# Lets move on to checking skewness found in the feature variables\n\n","d32ca1d3":"Lets first analyze our target variable, which is the SalePrice and see what information that we can gather. For this we can utilize seaborn's distplot function to view the histogram and scipy's stats probplot to view the probability plot. \n\n*For the most part, the normal P-P plot is better at finding deviations from normality in the center of the distribution, and the normal Q-Q plot is better at finding deviations in the tails. Q-Q plots tend to be preferred in research situations. Both Q-Q and P-P plots can be used for distributions other than normal.","35110bc5":"# Loading the datasets using Pandas built in read_csv function","d1b45152":"# We can then perform box-cox transformation on the independent variables as well.\n\nOnly select the features with relatively high skew (>0.5) and exclude the new features that we created.","fd621656":"# Checking for outliers specifically in the GrLivArea column\nAs per mentioned by the author himself, it is recommended to remove outliers for GrLivArea > 4000.\nLets have a quick scatter plot to visualize the outliers. We can easily notice a few outliers in the plot below and lets drop them from the training set.","fb1197bd":"# Perform label encoding to all ordinal variables","7708b57d":"# Introduction to the training dataset\n\nFrom the author, he mentioned that there are 23 nominal (Not ordered) and 23 ordinal (in specific orders, i.e. Overall Condition) categorical variables, 20 continuous variables and 14 discrete variables. The **nominal** variables typically identify various types of dwellings, garages, materials, and environmental conditions while the **ordinal** variables typically rate various items within the property. \n\nSo what are they? We can identify these variables simply by reading the documentation. Also we could have a quick look at the dataset to identify the continuous and discrete variables. Although we might notice that the OverallQual, OverallCond and MSSubClass is classified as numbers, but we do know that these 3 variables fall under the categorical variable group.","a99ffd07":"# Null in test data","52f5c886":"# So now we have our dataset ready, we will fit several regression models and predict the housing sale prices.\n\nThe models used are:\n* LinearRegression\n* Lasso\n* Ridge\n* RandomForestRegressor\n* GradientBoostingRegressor\n\nThe models are kept at their default state as i will not go into details for parameter tuning. This can be done using GridSearchCV. I will probably further tune it in the near future.","972891a4":"# Null in train data"}}