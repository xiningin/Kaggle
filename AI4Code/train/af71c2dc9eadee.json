{"cell_type":{"5e9903fd":"code","24e84066":"code","c5ee375e":"code","75191b61":"code","8c771a56":"code","d81aa586":"code","3bacd513":"code","bf870db8":"code","0c9b961f":"code","da37a44d":"code","f3af44f7":"code","7c7db2fe":"code","c114beda":"code","810165d4":"code","fcecfe41":"code","b39548af":"code","15ae27f7":"code","0f20691e":"code","5aea867c":"code","26f100a6":"code","7be17bb0":"code","9c0b6e38":"code","10d2b09f":"code","70255901":"code","89d4304c":"code","7317db8b":"code","69a17152":"code","a727a8d8":"code","1ec17ee6":"code","0f586773":"code","d312326b":"code","2977abb0":"code","47dcebfb":"code","99e86b71":"code","e9158ff4":"code","4ce63132":"code","74437eda":"markdown","a467b052":"markdown","c1737275":"markdown","9d30de18":"markdown","455765e2":"markdown","23dd03b8":"markdown","45941fd0":"markdown","dd83b78e":"markdown","c261e788":"markdown","6477594b":"markdown","5e0cf2e8":"markdown","cefefb71":"markdown","25391029":"markdown","d4ce1df1":"markdown","92cb646b":"markdown","e4041942":"markdown","08b6e30b":"markdown","67b56edb":"markdown","afe0482b":"markdown","19aeeac8":"markdown","870d65d6":"markdown","a8c205a4":"markdown","57bda309":"markdown","19db0d9f":"markdown","f159081f":"markdown","921f7e28":"markdown","bd406176":"markdown","081f6f6c":"markdown","064eff79":"markdown","f1ba776b":"markdown","3a50d8d7":"markdown","c91d0222":"markdown","eb543e69":"markdown","97a2f0bf":"markdown","b9ba58c2":"markdown","a685d077":"markdown","cf3970cc":"markdown","bf63c728":"markdown","eeee45de":"markdown","4eef9958":"markdown","7e091d8b":"markdown","9e6ffd4a":"markdown","9304e09d":"markdown","9b506a06":"markdown","c963c3a3":"markdown","83bd0800":"markdown","dc9da7c9":"markdown","08bdca9e":"markdown","eaf7ab4e":"markdown","be66e6de":"markdown","68e5bcdc":"markdown","0fa939a3":"markdown","c0d05478":"markdown","481b3813":"markdown","2b98edfa":"markdown","d5314d36":"markdown","69772b8a":"markdown","5f37fce6":"markdown","db3f7002":"markdown","8da5c597":"markdown","08ba1f46":"markdown","3bf9b70d":"markdown","af2e2e4b":"markdown","e0fb19ec":"markdown","350b5e71":"markdown","4a76c56d":"markdown","66432122":"markdown","a7a294e9":"markdown","61efa6dd":"markdown","344abedb":"markdown","3966119d":"markdown","2fb85ad4":"markdown","edd38c86":"markdown","667c0437":"markdown","59515971":"markdown","cec73dd2":"markdown","4ccd357d":"markdown","a91865ff":"markdown","24ea590e":"markdown","0af20a78":"markdown","b7308bdd":"markdown"},"source":{"5e9903fd":"%%javascript\n$.getScript('https:\/\/kmahelona.github.io\/ipython_notebook_goodies\/ipython_notebook_toc.js')","24e84066":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split, cross_val_predict\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.decomposition import PCA\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n%matplotlib inline\n\nSMALL_SIZE = 10\nMEDIUM_SIZE = 12\n\nplt.rc('font', size=SMALL_SIZE)\nplt.rc('axes', titlesize=MEDIUM_SIZE)\nplt.rc('axes', labelsize=MEDIUM_SIZE)\nplt.rcParams['figure.dpi']=150","c5ee375e":"#sdss_df = pd.read_csv('Skyserver_SQL2_27_2018 6_51_39 PM.csv', skiprows=1)\nsdss_df = pd.read_csv('..\/input\/Skyserver_SQL2_27_2018 6_51_39 PM.csv', skiprows=0)","75191b61":"sdss_df.head()","8c771a56":"sdss_df.info()","d81aa586":"sdss_df.describe()","3bacd513":"sdss_df['class'].value_counts()","bf870db8":"sdss_df.columns.values","0c9b961f":"sdss_df.drop(['objid', 'run', 'rerun', 'camcol', 'field', 'specobjid'], axis=1, inplace=True)\nsdss_df.head(1)","da37a44d":"fig, axes = plt.subplots(nrows=1, ncols=3,figsize=(16, 4))\nax = sns.distplot(sdss_df[sdss_df['class']=='STAR'].redshift, bins = 30, ax = axes[0], kde = False)\nax.set_title('Star')\nax = sns.distplot(sdss_df[sdss_df['class']=='GALAXY'].redshift, bins = 30, ax = axes[1], kde = False)\nax.set_title('Galaxy')\nax = sns.distplot(sdss_df[sdss_df['class']=='QSO'].redshift, bins = 30, ax = axes[2], kde = False)\nax = ax.set_title('QSO')","f3af44f7":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(16, 4))\nax = sns.lvplot(x=sdss_df['class'], y=sdss_df['dec'], palette='coolwarm')\nax.set_title('dec')","7c7db2fe":"fig, axes = plt.subplots(nrows=1, ncols=3,figsize=(16, 4))\nfig.set_dpi(100)\nax = sns.heatmap(sdss_df[sdss_df['class']=='STAR'][['u', 'g', 'r', 'i', 'z']].corr(), ax = axes[0], cmap='coolwarm')\nax.set_title('Star')\nax = sns.heatmap(sdss_df[sdss_df['class']=='GALAXY'][['u', 'g', 'r', 'i', 'z']].corr(), ax = axes[1], cmap='coolwarm')\nax.set_title('Galaxy')\nax = sns.heatmap(sdss_df[sdss_df['class']=='QSO'][['u', 'g', 'r', 'i', 'z']].corr(), ax = axes[2], cmap='coolwarm')\nax = ax.set_title('QSO')","c114beda":"sns.lmplot(x='ra', y='dec', data=sdss_df, hue='class', fit_reg=False, palette='coolwarm', size=6, aspect=2)\nplt.title('Equatorial coordinates')","810165d4":"sdss_df_fe = sdss_df\n\n# encode class labels to integers\nle = LabelEncoder()\ny_encoded = le.fit_transform(sdss_df_fe['class'])\nsdss_df_fe['class'] = y_encoded\n\n# Principal Component Analysis\npca = PCA(n_components=3)\nugriz = pca.fit_transform(sdss_df_fe[['u', 'g', 'r', 'i', 'z']])\n\n# update dataframe \nsdss_df_fe = pd.concat((sdss_df_fe, pd.DataFrame(ugriz)), axis=1)\nsdss_df_fe.rename({0: 'PCA_1', 1: 'PCA_2', 2: 'PCA_3'}, axis=1, inplace = True)\nsdss_df_fe.drop(['u', 'g', 'r', 'i', 'z'], axis=1, inplace=True)\nsdss_df_fe.head()","fcecfe41":"scaler = MinMaxScaler()\nsdss = scaler.fit_transform(sdss_df_fe.drop('class', axis=1))","b39548af":"X_train, X_test, y_train, y_test = train_test_split(sdss, sdss_df_fe['class'], test_size=0.33)","15ae27f7":"knn = KNeighborsClassifier()\ntraining_start = time.perf_counter()\nknn.fit(X_train, y_train)\ntraining_end = time.perf_counter()\nprediction_start = time.perf_counter()\npreds = knn.predict(X_test)\nprediction_end = time.perf_counter()\nacc_knn = (preds == y_test).sum().astype(float) \/ len(preds)*100\nknn_train_time = training_end-training_start\nknn_prediction_time = prediction_end-prediction_start\nprint(\"Scikit-Learn's K Nearest Neighbors Classifier's prediction accuracy is: %3.2f\" % (acc_knn))\nprint(\"Time consumed for training: %4.3f seconds\" % (knn_train_time))\nprint(\"Time consumed for prediction: %6.5f seconds\" % (knn_prediction_time))","0f20691e":"from sklearn.preprocessing import MaxAbsScaler\nscaler_gnb = MaxAbsScaler()\nsdss = scaler_gnb.fit_transform(sdss_df_fe.drop('class', axis=1))\nX_train_gnb, X_test_gnb, y_train_gnb, y_test_gnb = train_test_split(sdss, sdss_df_fe['class'], test_size=0.33)\n\ngnb = GaussianNB()\ntraining_start = time.perf_counter()\ngnb.fit(X_train_gnb, y_train_gnb)\ntraining_end = time.perf_counter()\nprediction_start = time.perf_counter()\npreds = gnb.predict(X_test_gnb)\nprediction_end = time.perf_counter()\nacc_gnb = (preds == y_test_gnb).sum().astype(float) \/ len(preds)*100\ngnb_train_time = training_end-training_start\ngnb_prediction_time = prediction_end-prediction_start\nprint(\"Scikit-Learn's Gaussian Naive Bayes Classifier's prediction accuracy is: %3.2f\" % (acc_gnb))\nprint(\"Time consumed for training: %4.3f seconds\" % (gnb_train_time))\nprint(\"Time consumed for prediction: %6.5f seconds\" % (gnb_prediction_time))","5aea867c":"xgb = XGBClassifier(n_estimators=100)\ntraining_start = time.perf_counter()\nxgb.fit(X_train, y_train)\ntraining_end = time.perf_counter()\nprediction_start = time.perf_counter()\npreds = xgb.predict(X_test)\nprediction_end = time.perf_counter()\nacc_xgb = (preds == y_test).sum().astype(float) \/ len(preds)*100\nxgb_train_time = training_end-training_start\nxgb_prediction_time = prediction_end-prediction_start\nprint(\"XGBoost's prediction accuracy is: %3.2f\" % (acc_xgb))\nprint(\"Time consumed for training: %4.3f\" % (xgb_train_time))\nprint(\"Time consumed for prediction: %6.5f seconds\" % (xgb_prediction_time))","26f100a6":"rfc = RandomForestClassifier(n_estimators=10)\ntraining_start = time.perf_counter()\nrfc.fit(X_train, y_train)\ntraining_end = time.perf_counter()\nprediction_start = time.perf_counter()\npreds = rfc.predict(X_test)\nprediction_end = time.perf_counter()\nacc_rfc = (preds == y_test).sum().astype(float) \/ len(preds)*100\nrfc_train_time = training_end-training_start\nrfc_prediction_time = prediction_end-prediction_start\nprint(\"Scikit-Learn's Random Forest Classifier's prediction accuracy is: %3.2f\" % (acc_rfc))\nprint(\"Time consumed for training: %4.3f seconds\" % (rfc_train_time))\nprint(\"Time consumed for prediction: %6.5f seconds\" % (rfc_prediction_time))","7be17bb0":"svc = SVC()\ntraining_start = time.perf_counter()\nsvc.fit(X_train, y_train)\ntraining_end = time.perf_counter()\nprediction_start = time.perf_counter()\npreds = svc.predict(X_test)\nprediction_end = time.perf_counter()\nacc_svc = (preds == y_test).sum().astype(float) \/ len(preds)*100\nsvc_train_time = training_end-training_start\nsvc_prediction_time = prediction_end-prediction_start\nprint(\"Scikit-Learn's Support Vector Machine Classifier's prediction accuracy is: %3.2f\" % (acc_svc))\nprint(\"Time consumed for training: %4.3f seconds\" % (svc_train_time))\nprint(\"Time consumed for prediction: %6.5f seconds\" % (svc_prediction_time))","9c0b6e38":"results = pd.DataFrame({\n    'Model': ['KNN', 'Naive Bayes', \n              'XGBoost', 'Random Forest', 'SVC'],\n    'Score': [acc_knn, acc_gnb, acc_xgb, acc_rfc, acc_svc],\n    'Runtime Training': [knn_train_time, gnb_train_time, xgb_train_time, rfc_train_time, \n                         svc_train_time],\n    'Runtime Prediction': [knn_prediction_time, gnb_prediction_time, xgb_prediction_time, rfc_prediction_time,\n                          svc_prediction_time]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Model')\nresult_df","10d2b09f":"from sklearn.model_selection import cross_val_score\nrfc_cv = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rfc_cv, X_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","70255901":"xgb_cv = XGBClassifier(n_estimators=100)\nscores = cross_val_score(xgb_cv, X_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","89d4304c":"importances = pd.DataFrame({\n    'Feature': sdss_df_fe.drop('class', axis=1).columns,\n    'Importance': xgb.feature_importances_\n})\nimportances = importances.sort_values(by='Importance', ascending=False)\nimportances = importances.set_index('Feature')\nimportances","7317db8b":"importances.plot.bar()","69a17152":"scaler = MinMaxScaler()\nsdss = pd.DataFrame(scaler.fit_transform(sdss_df_fe.drop(['mjd', 'class'], axis=1)), columns=sdss_df_fe.drop(['mjd', 'class'], axis=1).columns)\nsdss['class'] = sdss_df_fe['class']","a727a8d8":"sdss.head()","1ec17ee6":"sdss.to_csv('sdss_data.csv')","0f586773":"X_train, X_test, y_train, y_test = train_test_split(sdss.drop('class', axis=1), sdss['class'],\n                                                   test_size=0.33)","d312326b":"xgboost = XGBClassifier(max_depth=5, learning_rate=0.01, n_estimators=100, gamma=0, \n                        min_child_weight=1, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.005)\n\nxgboost.fit(X_train, y_train)\npreds = xgboost.predict(X_test)\n\naccuracy = (preds == y_test).sum().astype(float) \/ len(preds)*100\n\nprint(\"XGBoost's prediction accuracy WITH optimal hyperparameters is: %3.2f\" % (accuracy))","2977abb0":"xgb_cv = XGBClassifier(n_estimators=100)\nscores = cross_val_score(xgb_cv, X_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","47dcebfb":"unique, counts = np.unique(sdss['class'], return_counts=True)\ndict(zip(unique, counts))","99e86b71":"predictions = cross_val_predict(xgb, sdss.drop('class', axis=1), sdss['class'], cv=3)\nconfusion_matrix(sdss['class'], predictions)","e9158ff4":"print(\"Precision:\", precision_score(sdss['class'], predictions, average='micro'))\nprint(\"Recall:\",recall_score(sdss['class'], predictions, average='micro'))","4ce63132":"print(\"F1-Score:\", f1_score(sdss['class'], predictions, average='micro'))","74437eda":"Recall: u, g, r, i, z represent the different wavelengths which are used to capture the observations.\n\nLet's find out how much they are correlated.","a467b052":"The most objects (50%) are galaxies, a little less (40%) are stars and only around (10%) of the rows are classified as QSOs.","c1737275":"To start the univariate analysis we will plot histograms for the 'redshift' feature column for each class.\n\nThis will tell us how the redshift values are distributed over their range.","9d30de18":"### About the SDSS","455765e2":"The best parameters for prediction as found by the tuning tests are:\n\n- max_depth = 5\n- min_child_weight = 1\n- gamma = 0\n- subsample = 0.8\n- colsample_bytree = 0.8\n- reg_alpha = 0.005","23dd03b8":"Let's drop the column from the dataframe and rescale it. Since XGBoost requires the class to be discrete, we will re-add it manually afterwards.","45941fd0":"**SELECT TOP 10000** <br\/>\np.objid,p.ra,p.dec,p.u,p.g,p.r,p.i,p.z, p.run, p.rerun, p.camcol, p.field,  <br\/>\ns.specobjid, s.class, s.z as redshift, s.plate, s.mjd, s.fiberid  <br\/>\n**FROM** PhotoObj **AS** p <br\/>\n   **JOIN** SpecObj **AS** s **ON** s.bestobjid = p.objid <br\/>\n**WHERE** <br\/>\n   p.u **BETWEEN** 0 **AND** 19.6 <br\/>\n   **AND** g **BETWEEN** 0 **AND** 20 <br\/>\n\n","dd83b78e":"#### XGBoost","c261e788":"Precision is the fraction of events where the algorithm classified an object of type **t** correctly out of all occurences of the algorithm classifying objects of type **t**.\n\nRecall is the fraction of events where the algorithm classified an object of type **t** correctly when the true type of that object was actually **t**.\n\nPrecision in our case:\n\nFor every class its calculated how many objects were classified as stars (or galaxies or quasars) in relation to the amount of correct star (or galaxies or quasars) predictions. The results are averaged --> 99.36%.\n\nRecall in our case:\n\nFor every class its calculated how many objects were classified as stars (or galaxies or quasars) in relation to the total amount of predictions where the object actually was a star (or galaxy or quasar). The results are averaged --> 99.36%.\n\nIn both cases our algorithm did a very good job. The highest precision or recall value a predictor can have is 1.0.","6477594b":"The parameter tuning did not improve the accuracy as excpected. We will therefore do a cross validation to test to get a more reliable result.","5e0cf2e8":"### Query","cefefb71":"#### dec","25391029":"**First of all: what does this plot tell us?**\n\nThe Letter value (LV) Plot show us an estimate of the distribution of the data. It shows boxes which relate to the amount of values within the range of values inside the box.\n\nIn this case we can observe a clear distinction between Stars and the other two classes. The difference between Galaxies and Quasars is smaller.\n\n* **Star:** The largest part of the data points lay within a 0 to 10 range. Another large part consists of values between about 10 to 55. Only small amounts of the data are lower or higher than these ranges.\n\n* **Galaxy:** The largest part of values lays between 0 and 45. There is a smaller amount of values in the range of 45 to 60. The rest of the data has smaller or higher values.\n\n* **QSO:** This plot looks quite similiar to the GALAXY plot. Only the amount of data points in the range of 0 to 60 is even bigger.\n\nSide Note: The fact that the distribution of dec values of galaxies und quasar objects is almost the same might indicate that one can find both galaxies and quasars at smiliar positions in the night sky.","d4ce1df1":"### K Fold Cross Validation","92cb646b":"We trained different machine learning models to solve this classification problems. Without any further hyperparameter tuning XGBoost and Scikit-Learn's Random Forest Classifier performed the best.\n\nAs XGBoost showed a little higher accuracy in most of the tests, we will continue only with this classifier.","e4041942":"The Sloan Digital Sky Survey is a project which offers public data of space observations. Observations have been made since 1998 and have been made accessible to everyone who is interested. \n\nFor this purpose a special 2.5 m diameter telescope was built at the Apache Point Observatory in New Mexico, USA. The telescope uses a camera of 30 CCD-Chips with 2048x2048 image points each. The chips are ordered in 5 rows with 6 chips in each row. Each row observes the space through different optical filters (u, g, r, i, z) at wavelengths of approximately 354, 476, 628, 769, 925 nm.\n\nThe telescope covers around one quarter of the earth's sky - therefore focuses on the northern part of the sky.\n\n**For more information about this awesome project - please visit their website:**\n\nhttp:\/\/www.sdss.org\/\n\n![alt text](http:\/\/www.fingerprintdigitalmedia.com\/wp-content\/uploads\/2014\/08\/sdss1.jpg)","08b6e30b":"#### Right ascension (ra) and declination (dec) ","67b56edb":"In this notebook learned how to get data from the SDSS RD14, analyze the data (we learned some very interesting facts about our space along the way), how to build a machine learning model to predict for unseen data from this data set and how to improve its performance (even though there was only a slight improvent).\nWe used XGBoost for predicting and evaluated its result.\n\nThis project was very interesting to work on as I'm also interested in space and astronomy.","afe0482b":"As we can clearly observe the equatorial coordinates do not differ significantly between the 3 classes. There are some outliers for stars and galaxies but for the bigger part the coordinates are within the same range.\n\nWhy is that?\n\nAll SDSS images cover the same area of the sky. The plot above tells us that stars, galaxies and quasars are observed equally at all coordinates within this area. So whereever the SDSS \"looks\" - the chance of observing a star or galaxy or quasar is always the same.  \n\n**This contradicts our interpretation of the letter value plot of dec from the univariate analysis.**","19aeeac8":"There is no need to know everything about stars, galaxy or quasars - yet we can already tell which features are **unlikely** to be related to the target variable 'class'.\n\n**objid** and **specobjid** are just identifiers for accessing the rows back when they were stored in the original databank. Therefore we will not need them for classification as they are not related to the outcome.\n\nEven more: The features 'run', 'rerun', 'camcol' and 'field' are values which describe parts of the camera at the moment when making the observation, e.g. 'run' represents the corresponding scan which captured the oject.\n\nSource: http:\/\/www.sdss3.org\/dr9\/imaging\/imaging_basics.php\n\nWe will drop these columns as any correlation to the outcome would be coincidentally.","870d65d6":"This is an interesting result.\n\nWe can cleary tell that the redshift values for the classes quite differ. \n\n* **Star:** The histogram looks like a truncated zero-centered normal distribution.\n\n* **Galaxy:** The redshift values may come from a slightly right-shifted normal distribution which is centered around 0.075.\n\n* **QSO:** The redshift values for QSOs are a lot more uniformly distributed than for Stars or Galaxies. They are roughly evenly distributed from 0 to 3, than the occurences decrease drastically. For 4 oder ~5.5 there are some outliers.\n\n**The redshift can be an estimate(!) for the distance from the earth to a object in space.**\n\nHence the distplot tells us that most of the stars observed are somewhat closer to the earth than galaxies or quasars. Galaxies tend to be a little further away and quasars are distant from very close to very far.  \n\nPossible rookie explanation: Since galaxies and quasars radiate stronger due to their size and physical structure, they can be observed from further away than \"small\" stars.\n\nAs we can distinct the classes from each other just based on this column - 'redshift' is very likely to be helping a lot classifying new objects.","a8c205a4":"### Confusion Matrix","57bda309":"We can tell that we have all the features as described in the above query. \n\nWe notice that there are no categorical features at all - besides the class column. As some machine learning models can't handle categorical feature columns at all, we will encode this column to be a numerical column later on.","19db0d9f":"Let's lvplot the values of dec (Recall: position on celestial equator)!","f159081f":"Decision Trees have the unique property of being able to order features by their ability to split between the classes.\n\nWe will now visualize the features and their splitting ability.","921f7e28":"Now it's time to look for the optimal hyperparameters - what does this mean?\n\nWe will test our chosen model with different values for (almost) each of its tuning parameters and give back the parameters with which the model performed best.\n\n**The actual searching for optimal parameters is not done in this notebook since the operations can take some time and parallel editing would not be possible.\n\nWe will write our transformed data set to disk so the tuning scripts can access it. ","bd406176":"### First Data Filtering","081f6f6c":"### Univariate Analysis","064eff79":"## XGBoost - Evaluation","f1ba776b":"### Feature Description","3a50d8d7":"We will  split the data into a training and a test part. The models will be trained on the training data set and tested on the test data set","c91d0222":"The dataset has 10000 examples, 17 feature columns and 1 target column. 8 of the 17 features are 64 bit integers, 1 feature is an unsigned 64 bit integer, 8 are 64 bit floats and the target column is of the type object. ","eb543e69":"### u, g, r, i, z","97a2f0bf":"## XGBoost - Testing optimal hyperparameters","b9ba58c2":"### F1-Score","a685d077":"We will now reduce the amount of dimensions by replacing the different bands 'u', 'g', 'r', 'i' and 'z' by a linear combination with only 3 dimensions using **Principal Component Analysis**.\n\n**Principal Component Analysis:**\n\nn observations with p features can be interpreted as n points in a p-dimensional space. PCA aims to project this space into a q-dimensional subspace (with q<p) with as little information loss as possible. \n\nIt does so by finding the q directions in which the n points vary the most (--> the principal components). It then projects the original data points into the q-dimensional subspace. PCA returns a n x q dimensional matrix. \n\nUsing PCA on our data will decrease the amount of operations during training and testing.","cf3970cc":"We will no perform k fold cross valdiation for the top 2 classifiers, i.e. XGBoost & Random Forest.\n\nWe do this to get a more realistic result by testing the performance for 10 different train and test datasets and averaging the results. \n\nCross validation ensures that the above result is not arbitary and gives a more reliable performance check.","bf63c728":"As precision and recall have the same value the F1-Score has automatically the same value too. Again, we are very close to 1.0 which indicates strong performance.","eeee45de":"We can see that both XGBoost and Scikit-Learn's Random Forest Classifier could achieve very high accuracy.\n\nGaussian Naive Bayes achieves just a little less accuracy but needs a very little amount of time to both train and predict data.\n\nKNN performs about 5% worse than Naive Bayes.\n\nThe Support Vector Machine Classifier has the worst accuracy, plus takes the most of time for its operations.","4eef9958":"# Sloan Digital Sky Survey Classification\n## Classification of Galaxies, Stars and Quasars based on the RD14 from the SDSS","7e091d8b":"We will now plot the right ascension versus the declination depending on the class ","9e6ffd4a":"Cross validating the models showed that the accuracy values were in fact not arbitary and proofed that both models are performing very well. \n\nXGBoost showed a higher mean and lower standard deviation than the Scikit-Learn RFC.\n\nA high mean corresponds to a more stable performance and a low standard deviation corresponds to smaller range of results. ","9304e09d":"From the above table we can tell that are no missing values at all. This means: **no imputing!**\n\nWe also notice that most of the features stay within a reasonable scale when comparing values within **only one** column. We can recognize this from the min, max and quartil rows.","9b506a06":"Let's take a first look at our dataset to see what we're working with!","c963c3a3":"## Feature Engineering","83bd0800":"### Precision & Recall","dc9da7c9":"Right of the top we observe that the correlation matrices look very similiar for every class.\n\nWe can tell that there are high correlations between the different bands. This feels not really suprising - intuitively one would think that if one of the bands captures some object, the other bands should capture something aswell.\n\nTherefore it is interesting to see that band 'u' is less correlated to the other bands. \n\nRemember: u, g, r, i, z capture light at wavelengths of 354, 476, 628, 769 and 925 nm.\n\nThis might indicates that galaxies, stars and quasar objects shine brighter at wavelengths from 476 - 925 nm. Don't quote me on that though.\n\n**But:** as we can see - the correlation is roughly the same for every class...the different bands behave the same for the different classes!","08bdca9e":"#### Scitkit-Learn's Random Forest Classifier","eaf7ab4e":"#### u,g,r,i,z filters","be66e6de":"#### Feature Scaling","68e5bcdc":"## XGBoost - Finding the best hyperparameters","0fa939a3":"Thanks to [Adithya Raman's](https:\/\/www.kaggle.com\/christodieu) comment as he proposed to use a MaxAbsScaler for the Naive Bayes classifier. Naive Bayes assumes the data to be normally distributed which can be achieved by scaling using the MaxAbsScaler. Many thanks!","c0d05478":"The above query joins two tables (actually views): The image table (PhotoObj) which contains all image objects and the spectral table (SpecObj) which contains corresponding spectral data. ","481b3813":"Let's compare the results. We will create a table for a more comprehensive overview.","2b98edfa":"In this notebook we will try to classify observations of space to be either stars, galaxies or quasars. We will try to have a complete cycle of the data science workflow including querying the database to get the dataset, data analysis and building machine learning models to predict for new data.\n\nWe are using data from the Sloan Digital Sky Survey (**Release 14**). \n\n**I followed Niklas Donges' Titanic classification approach for this notebook as I found it really useful and comprehensive!**  ","d5314d36":"#### K Nearest Neighbors","69772b8a":"### Basic stats about our dataset","5f37fce6":"### Importing Libraries","db3f7002":"## Machine Learning Models - Training","8da5c597":"The following model implements the best performing model with optimal parameters evaluated by the hyperparameter tuning. We will expect the model to perform even better than before.\n\nAnalytics Vidhya presented a really nice guide for tuning XGBoost. \n\nPlease read more: https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/","08ba1f46":"#### Redshift","3bf9b70d":"#### Support Vector Machine Classifier","af2e2e4b":"### About the notebook","e0fb19ec":"#### Scikit-Learn's Random Forest Classifier","350b5e71":"One can combine precision and recall into one score, which is called the F-score. The F-score is computed with the harmonic mean of precision and recall. Note that it assigns much more weight to low values. As a result of that, the classifier will only get a high F-score, if both recall and precision are high.","4a76c56d":"## Data Exploration","66432122":"## Summary","a7a294e9":"### Summary","61efa6dd":"Let's find out about the types of columns we have:","344abedb":"### Feature Importance","3966119d":"#### Naive Bayes","2fb85ad4":"## Data Acquisition","edd38c86":"Here we can clearly see how PCA helped to improve the performance of our predictors as 2 of the principal components are in the top 3 features.\n\nThe best (in terms of being able to split classes) is redshift.\n\nMjd is the feature with the lowest importance during the classification process, we will therefore drop it from the dataframe.","667c0437":"#### XGBoost","59515971":"#### View \"PhotoObj\"\n* objid = Object Identifier\n* ra = J2000 Right Ascension (r-band)\n* dec = J2000 Declination (r-band)\n\nRight ascension (abbreviated RA) is the angular distance measured eastward along the celestial equator from the Sun at the March equinox to the hour circle of the point above the earth in question. When paired with declination (abbreviated dec), these astronomical coordinates specify the direction of a point on the celestial sphere (traditionally called in English the skies or the sky) in the equatorial coordinate system.\n\nSource: https:\/\/en.wikipedia.org\/wiki\/Right_ascension\n\n* u = better of DeV\/Exp magnitude fit\n* g = better of DeV\/Exp magnitude fit\n* r = better of DeV\/Exp magnitude fit\n* i = better of DeV\/Exp magnitude fit\n* z = better of DeV\/Exp magnitude fit\n\nThe Thuan-Gunn astronomic magnitude system. u, g, r, i, z represent the response of the 5 bands of the telescope.\n\nFurther education: https:\/\/www.astro.umd.edu\/~ssm\/ASTR620\/mags.html\n\n* run = Run Number\n* rereun = Rerun Number\n* camcol = Camera column\n* field = Field number\n\nRun, rerun, camcol and field are features which describe a field within an image taken by the SDSS. A field is basically a part of the entire image corresponding to 2048 by 1489 pixels. A field can be identified by:\n- **run** number, which identifies the specific scan,\n- the camera column, or \"**camcol**,\" a number from 1 to 6, identifying the scanline within the run, and\n- the **field** number. The field number typically starts at 11 (after an initial rampup time), and can be as large as 800 for particularly long runs.\n- An additional number, **rerun**, specifies how the image was processed. \n\n#### View \"SpecObj\"\n\n* specobjid = Object Identifier\n* class = object class (galaxy, star or quasar object)\n\nThe class identifies an object to be either a galaxy, star or quasar. This will be the response variable which we will be trying to predict.\n\n* redshift = Final Redshift\n* plate = plate number\n* mjd = MJD of observation\n* fiberid = fiber ID\n\nIn physics, **redshift** happens when light or other electromagnetic radiation from an object is increased in wavelength, or shifted to the red end of the spectrum. \n\nEach spectroscopic exposure employs a large, thin, circular metal **plate** that positions optical fibers via holes drilled at the locations of the images in the telescope focal plane. These fibers then feed into the spectrographs. Each plate has a unique serial number, which is called plate in views such as SpecObj in the CAS.\n\n**Modified Julian Date**, used to indicate the date that a given piece of SDSS data (image or spectrum) was taken.\n\nThe SDSS spectrograph uses optical fibers to direct the light at the focal plane from individual objects to the slithead. Each object is assigned a corresponding **fiberID**. \n\n**Further information on SDSS images and their attributes:** \n\nhttp:\/\/www.sdss3.org\/dr9\/imaging\/imaging_basics.php\n\nhttp:\/\/www.sdss3.org\/dr8\/glossary.php","cec73dd2":"Public data from the SDSS can be accessed through multiple ways - I used the **CasJobs** website which offers a **SQL-based interface** which lets you query their database which contains the released data.\n\nFor more information about how to get data from the SDSS see their Data Access Guide:\n\nhttp:\/\/www.sdss.org\/dr14\/data_access\/\n\nI used the sample query given by the **CasJobs** to receive the data. Find the exact query below:","4ccd357d":"We will now train different models on this dataset. \n\nScaling all values to be within the (0, 1) interval will reduce the distortion due to exceptionally high values and make some algorithms converge faster.","a91865ff":"The first row shows that out of 4998 stars, **4962 were classified correctly as stars**. 29 stars were classified incorrectly as galaxies and 7 stars were classified incorrectly as quasars.\n\nThe second row shows out of 850 quasars **827 were classified correctly**. 22 qsos were classified incorrectly as stars and 1 quasar was classified as galaxy.\n\nThe last row tells us that out of 4152 galaxies **4147 were classified correctly.**. 5 galaxies were classified incorrectly as star.\n\nIn total: \n\nWe have only 64 objects which were classified incorrectly. Most of the objects were recognized as what they are.","24ea590e":"### Multivariate Analysis","0af20a78":"<h1 id=\"tocheading\">Table of Contents<\/h1>\n<div id=\"toc\"><\/div>","b7308bdd":"Depending on the run the cross validation results vary from a little lower and a little better than before. \n\nThis indicates that the parameter tuning was not as effective as expected - this could mean that XGBoost was actually close to its maximum performance capability on this data set.\n\nAs we still have a good performance we will now continue with further evaluation of the performance of our model!"}}