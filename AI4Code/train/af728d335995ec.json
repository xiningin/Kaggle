{"cell_type":{"ba402524":"code","75249a0e":"code","32eb240f":"code","e8a59c2f":"code","1f5d3b74":"code","fd5b0ff9":"code","5dbf9738":"code","2f48e0aa":"code","012ddb8f":"code","d312208e":"code","b1f43281":"code","2ad4f657":"code","77e2ab22":"code","f9881706":"code","49e61793":"code","832cb680":"code","3f71486a":"code","fbfcc3b4":"code","bcc481b6":"code","ff8faa39":"code","df47d5db":"code","d4f82538":"code","2dc229b8":"code","bdf1be07":"code","8815fe2c":"code","c936cc9e":"code","667f8e44":"code","c0763991":"code","105504b4":"code","6d784f87":"code","ed204d3a":"code","652da7c4":"code","c80a40a0":"code","6e0a294f":"code","09b8f7f6":"code","7c33623c":"code","9b65ceb3":"code","6049f251":"code","575ba35a":"code","a55573e1":"code","e75884c7":"code","98650b6f":"code","5d3b6729":"code","830a0203":"code","fde219e7":"code","13ca9c5d":"code","ea58d5a9":"code","94b10ffb":"code","19a74191":"code","710b5e5b":"code","26e420b9":"code","7148d14f":"code","883b195f":"code","033783e3":"code","32611072":"code","af02d231":"code","cd36289b":"code","44b89955":"code","d8c4df81":"code","515370cc":"code","2071e793":"code","f117e874":"code","0db2a06f":"code","238e8f47":"code","47ce0a68":"code","18325c22":"code","63b374f6":"code","b70853ab":"code","344cb06b":"code","86afee44":"code","48939942":"code","dd232d88":"code","40af47ac":"code","ed034a9e":"code","c9c7f89a":"code","6cb9eb9d":"code","4ed0f941":"code","c0aad7c0":"code","b81e4a66":"code","e070cd73":"code","427d049e":"code","d22134bd":"code","2723f6d8":"code","21a38873":"code","a2aeda81":"markdown","6c401388":"markdown","9f291d80":"markdown","a239d6d9":"markdown","9f59c4dc":"markdown","98e8fbc6":"markdown","b8878c1f":"markdown","dd248904":"markdown","d706a7a4":"markdown","fd1ce89e":"markdown","ac628196":"markdown","a0ee2481":"markdown","6bf96bac":"markdown","5cfc81a7":"markdown","0df484b1":"markdown","45431fa4":"markdown","675d0fd2":"markdown","4a61111d":"markdown","0280fcff":"markdown","f14a7ba0":"markdown","dbf44dd5":"markdown","397bbf37":"markdown","ff64778e":"markdown","bcb90b0c":"markdown","4bfbcffa":"markdown","55e61867":"markdown"},"source":{"ba402524":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.pyplot import figure\n\nimport math as mt\nimport missingno as msno\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score,roc_auc_score, roc_curve\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n","75249a0e":"titanic_ = pd.read_csv('..\/input\/titanic-dataset\/titanic.csv')\ntitanic_df = titanic_.copy()\ntitanic_df.head()","32eb240f":"def upper_col_name(dataframe):\n    upper_cols = [col.upper() for col in dataframe.columns]\n    dataframe.columns = upper_cols\n    return dataframe.head()","e8a59c2f":"upper_col_name(titanic_df)","1f5d3b74":"titanic_df.info()","fd5b0ff9":"titanic_df.describe().T","5dbf9738":"#Selection of Categorical and Numerical Variables:\n\ndef grab_col_names(dataframe, cat_th=5, car_th=20):\n    \"\"\"\n    This function to perform the selection of numeric and categorical variables in the data set in a parametric way.\n    Note: Variables with numeric data type but with categorical properties are included in categorical variables.\n\n    Parameters\n    ----------\n    dataframe: dataframe\n        The data set in which Variable types need to be parsed\n    cat_th: int, optional\n        The threshold value for number of distinct observations in numerical variables with categorical properties.\n        cat_th is used to specify that if number of distinct observations in numerical variable is less than\n        cat_th, this variables can be categorized as a categorical variable.\n\n    car_th: int, optional\n        The threshold value for categorical variables with  a wide range of cardinality.\n        If the number of distinct observations in a categorical variables is greater than car_th, this\n        variable can be categorized as a categorical variable.\n\n    Returns\n    -------\n        cat_cols: list\n            List of categorical variables.\n        num_cols: list\n            List of numerical variables.\n        cat_but_car: list\n            List of categorical variables with  a wide range of cardinality.\n\n    Examples\n    ------\n        import seaborn as sns\n        df = sns.load_dataset(\"iris\")\n        print(grab_col_names(df))\n\n    Notes\n    ------\n        Sum of elements in lists the cat_cols,num_cols  and  cat_but_car give the total number of variables in dataframe.\n    \"\"\"\n\n    # cat cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\" and\n                   dataframe[col].nunique() < cat_th]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\" and\n                   dataframe[col].nunique() > car_th]\n\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\" and \"ID\" not in col.upper()]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    return cat_cols,num_cols,cat_but_car\n","2f48e0aa":"grab_col_names(titanic_df)","012ddb8f":"cat_cols, num_cols, cat_but_car = grab_col_names(titanic_df)","d312208e":" # General Exploration for Categorical Variables:\n\ndef cat_summary(dataframe, plot=False):\n    for col_name in cat_cols:\n        print(\"############## Unique Observations of Categorical Data ###############\")\n        print(\"The unique number of \"+ col_name+\": \"+ str(dataframe[col_name].nunique()))\n\n        print(\"############## Frequency of Categorical Data ########################\")\n        print(pd.DataFrame({col_name : dataframe[col_name].value_counts(),\n                            \"Ratio\": dataframe[col_name].value_counts()\/len(dataframe)}))\n        if plot == True:\n            rgb_values = sns.color_palette(\"Set2\", 6)\n            sns.set_theme(style=\"darkgrid\")\n            ax = sns.countplot(x=dataframe[col_name], data=dataframe, palette=rgb_values)\n            for p in ax.patches:\n                ax.annotate(f'\\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='white', size=10)\n            plt.show()\n","b1f43281":"cat_summary(titanic_df, plot=True)","2ad4f657":"# General Exploration for Numerical Variables:\n\ndef num_summary(dataframe,  plot=False):\n    quantiles = [0.25, 0.50, 0.75, 1]\n    for col_name in num_cols:\n        print(\"########## Summary Statistics of \" +  col_name + \" ############\")\n        print(dataframe[col_name].describe(quantiles).T)\n\n        if plot:\n            sns.histplot(data=dataframe, x=col_name  )\n            plt.xlabel(col_name)\n            plt.title(\"The distribution of \"+ col_name)\n            plt.grid(True)\n            plt.show(block=True)\n","77e2ab22":"num_summary(titanic_df, plot=True)","f9881706":"# Only passengers have cabin numbers, so \"Deck\" feature can be extracted by using Cabin feature:  \ntitanic_df[\"NEW_DECK\"] = titanic_df[\"CABIN\"].notnull().astype('int')\n\n# Name word count\ntitanic_df[\"NEW_NAME_WORD_COUNT\"] = titanic_df[\"NAME\"].apply(lambda x: len(str(x).split(\" \")))\n\n# Name that includes \"Dr\"\ntitanic_df[\"NEW_NAME_DR\"] = titanic_df[\"NAME\"].apply(lambda x: len([x for x in x.split() if x.startswith(\"Dr. \")]))\n\n# Family size:\ntitanic_df[\"NEW_FAMILY_SIZE\"] = titanic_df[\"SIBSP\"] + titanic_df[\"PARCH\"] + 1\n\n# Fare per passenger:\ntitanic_df['NEW_FARE_PER_PERSON'] = titanic_df['FARE'] \/ (titanic_df['NEW_FAMILY_SIZE'])\n\n# Title:\ntitanic_df['NEW_TITLE'] = titanic_df.NAME.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# Ticket:\ntitanic_df['NEW_TICKET'] = titanic_df['TICKET'].str.isalnum().astype('int')\n\n# Age & Pclass\ntitanic_df[\"NEW_AGE_PCLASS\"] = titanic_df[\"AGE\"] * titanic_df[\"PCLASS\"]\n\n# Is Alone?\ntitanic_df[\"NEW_IS_ALONE\"] = np.where(titanic_df['SIBSP'] + titanic_df['PARCH'] > 0, \"NO\", \"YES\") \n    \n# Age Level \ntitanic_df.loc[(titanic_df['AGE'] < 18), 'NEW_AGE_CAT'] = 'Young'\ntitanic_df.loc[(titanic_df['AGE'] >= 18) & (titanic_df['AGE'] < 56), 'NEW_AGE_CAT'] = 'Mature'\ntitanic_df.loc[(titanic_df['AGE'] >= 56), 'NEW_AGE_CAT'] = 'Senior'\n\n # Age & Sex\ntitanic_df.loc[(titanic_df['SEX'] == 'male') & (titanic_df['AGE'] <= 21), 'NEW_SEX_CAT'] = 'Young_Male'\ntitanic_df.loc[(titanic_df['SEX'] == 'male') & ((titanic_df['AGE'] > 21) & (titanic_df['AGE']) < 50), 'NEW_SEX_CAT'] = 'Mature_Male'\ntitanic_df.loc[(titanic_df['SEX'] == 'male') & (titanic_df['AGE'] > 50), 'NEW_SEX_CAT'] = 'Senior_Male'\ntitanic_df.loc[(titanic_df['SEX'] == 'female') & (titanic_df['AGE'] <= 21), 'NEW_SEX_CAT'] = 'Young_Female'\ntitanic_df.loc[(titanic_df['SEX'] == 'female') & ((titanic_df['AGE'] > 21) & (titanic_df['AGE']) < 50), 'NEW_SEX_CAT'] = 'Mature_Female'\ntitanic_df.loc[(titanic_df['SEX'] == 'female') & (titanic_df['AGE'] > 50), 'NEW_SEX_CAT'] = 'Senior_Female'","49e61793":"titanic_df.columns","832cb680":"titanic_df.drop(columns=[\"PASSENGERID\",\"NAME\",\"TICKET\",\"CABIN\"], axis=1, inplace=True)","3f71486a":"titanic_df.head(3)","fbfcc3b4":"def outlier_thresholds(dataframe, col_name, q1 = 0.25, q3 = 0.75):\n    Q1 = dataframe[col_name].quantile(q1)\n    Q3 = dataframe[col_name].quantile(q3)\n    IQR = Q3 - Q1\n    low_limit = Q1 - 1.5 * IQR\n    up_limit = Q3 + 1.5 * IQR\n    \n    return low_limit, up_limit","bcc481b6":"cat_cols, num_cols, cat_but_car = grab_col_names(titanic_df)","ff8faa39":"for col in num_cols:\n    print(col,\":\",outlier_thresholds(titanic_df,col))","df47d5db":"def check_outlier(dataframe, q1=0.25, q3=0.75):\n    for col_name in num_cols:\n        low_limit, up_limit = outlier_thresholds(dataframe, col_name, q1, q3)\n        if dataframe[(dataframe[col_name] < low_limit) | (dataframe[col_name] > up_limit)].any(axis=None):\n            sns.boxplot(x=dataframe[col_name])\n            plt.show()\n        else:\n            return False","d4f82538":"check_outlier(titanic_df)","2dc229b8":"check_outlier(titanic_df, q1=0.05, q3=0.95)","bdf1be07":"msno.matrix(titanic_df, figsize=(10,10), fontsize=10, labels=8)\nplt.show()","8815fe2c":"msno.heatmap(titanic_df, figsize=(8,8), fontsize=12)\nplt.show()","c936cc9e":"# Check the features containing NaN values:\n\ndef missing_values_df(dataframe, na_col_name=False):\n    na_cols = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    na_cols_number = dataframe[na_cols].isnull().sum()\n    na_cols_ratio = dataframe[na_cols].isnull().sum() \/ dataframe.shape[0]\n    missing_values_table = pd.DataFrame({\"Missing_Values (#)\": na_cols_number, \\\n                                         \"Ratio (%)\": na_cols_ratio * 100,\n                                         \"Type\" : dataframe[na_cols].dtypes})\n    print(missing_values_table)\n    print(\"************* Number of Missing Values *************\")\n    print(dataframe.isnull().sum().sum())\n    if na_col_name:\n        print(\"************* Nullable variables *************\")\n        return na_cols\n","667f8e44":"missing_values_df(titanic_df)","c0763991":"def missing_cat_cols_fill(dataframe):\n    na_cols = [col for col in titanic_df.columns if titanic_df[col].isnull().sum() > 0 and titanic_df[col].dtype == \"O\"]\n    for col in na_cols:\n        dataframe[col] = dataframe[col].fillna(dataframe[col].mode()[0])\n        return dataframe.head()\n","105504b4":"missing_cat_cols_fill(titanic_df)","6d784f87":"missing_values_df(titanic_df)","ed204d3a":"def observe_missing_values(dataframe, na_col, related_col, target, target_method=\"mean\", na_col_method=\"median\"):\n    print(dataframe.groupby(related_col).agg({target : target_method, \n                                               na_col : na_col_method}))","652da7c4":"cat_cols = [col for col in cat_cols if col not in \"SURVIVED\"]\nfor col in cat_cols:\n    observe_missing_values(titanic_df, \"AGE\",col,\"SURVIVED\")","c80a40a0":"titanic_df.drop(columns=\"NEW_NAME_DR\",axis=1, inplace=True)","6e0a294f":"titanic_df[\"AGE\"] = titanic_df[\"AGE\"].fillna(titanic_df.groupby(\"NEW_TITLE\")[\"AGE\"].transform(\"median\"))","09b8f7f6":"# We need to update features which have been derived with AGE:\n    \n# Age & Pclass\ntitanic_df[\"NEW_AGE_PCLASS\"] = titanic_df[\"AGE\"] * titanic_df[\"PCLASS\"]\n\n# Is Alone?\ntitanic_df[\"NEW_IS_ALONE\"] = np.where(titanic_df['SIBSP'] + titanic_df['PARCH'] > 0, \"NO\", \"YES\") \n    \n# Age Level \ntitanic_df.loc[(titanic_df['AGE'] < 18), 'NEW_AGE_CAT'] = 'Young'\ntitanic_df.loc[(titanic_df['AGE'] >= 18) & (titanic_df['AGE'] < 56), 'NEW_AGE_CAT'] = 'Mature'\ntitanic_df.loc[(titanic_df['AGE'] >= 56), 'NEW_AGE_CAT'] = 'Senior'\n\n # Age & Sex\ntitanic_df.loc[(titanic_df['SEX'] == 'male') & (titanic_df['AGE'] <= 21), 'NEW_SEX_CAT'] = 'Young_Male'\ntitanic_df.loc[(titanic_df['SEX'] == 'male') & ((titanic_df['AGE'] > 21) & (titanic_df['AGE']) < 50), 'NEW_SEX_CAT'] = 'Mature_Male'\ntitanic_df.loc[(titanic_df['SEX'] == 'male') & (titanic_df['AGE'] > 50), 'NEW_SEX_CAT'] = 'Senior_Male'\ntitanic_df.loc[(titanic_df['SEX'] == 'female') & (titanic_df['AGE'] <= 21), 'NEW_SEX_CAT'] = 'Young_Female'\ntitanic_df.loc[(titanic_df['SEX'] == 'female') & ((titanic_df['AGE'] > 21) & (titanic_df['AGE']) < 50), 'NEW_SEX_CAT'] = 'Mature_Female'\ntitanic_df.loc[(titanic_df['SEX'] == 'female') & (titanic_df['AGE'] > 50), 'NEW_SEX_CAT'] = 'Senior_Female'\n    ","7c33623c":"missing_values_df(titanic_df)","9b65ceb3":"# Let's take a head at the dataset again:\n\ntitanic_df.info()","6049f251":"cat_cols, num_cols, cat_but_car = grab_col_names(titanic_df)\ndf = titanic_df[num_cols]","575ba35a":"df.head()","a55573e1":"clf = LocalOutlierFactor(n_neighbors=20)\nclf.fit_predict(df)\ndf_scores = clf.negative_outlier_factor_\ndf_scores[0:5]","e75884c7":"# Visualization: \n\nscores = pd.DataFrame(np.sort(df_scores))\nscores.plot(stacked=True, xlim=[0, 20], style='.-')\nplt.show()\n","98650b6f":"# Let's determine the threshold by using Elbow Method\n\nth = np.sort(df_scores)[8]\n\ndf[df_scores < th]\n","5d3b6729":"titanic_df.drop(df[df_scores < th].index, inplace=True)","830a0203":"titanic_df.shape","fde219e7":"# Defining binary cols:\n\ndef binary_cols(dataframe):\n    binary_col_names = [col for col in dataframe.columns if ((dataframe[col].dtype == \"O\") and (dataframe[col].nunique() == 2))]\n    return binary_col_names\n","13ca9c5d":"binary_col_names = binary_cols(titanic_df)","ea58d5a9":"binary_col_names","94b10ffb":"# Label Encoding:\n\ndef label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe","19a74191":"for col in binary_col_names:\n    label_encoder(titanic_df, col)","710b5e5b":"titanic_df.head()","26e420b9":"def rare_analyser(dataframe, target):\n    cat_cols, num_cols, cat_but_car = grab_col_names(dataframe)\n    cat_cols = [col for col in cat_cols if  col != target in cat_cols]\n\n    for col in cat_cols:\n        print(col, \":\", dataframe[col].nunique())\n        print(\"dtype:\", dataframe[col].dtype)\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(), \\\n                            \"RATIO (%)\": dataframe[col].value_counts() \/ dataframe.shape[0], \\\n                            \"TARGET_MEAN (%) \": dataframe.groupby(col)[target].mean() * 100}))\n\n","7148d14f":"rare_analyser(titanic_df, \"SURVIVED\")","883b195f":"# Rare Encoder: \n\ndef rare_encoder(dataframe, rare_perc=0.0100):\n    rare_df = dataframe.copy()\n\n    rare_columns = [col for col in rare_df.columns if rare_df[col].dtypes == 'O'\n                    and (rare_df[col].value_counts() \/ rare_df.shape[0] <= rare_perc).any(axis=None)]\n\n    for col in rare_columns:\n        tmp = rare_df[col].value_counts() \/ rare_df.shape[0]\n        rare_labels = tmp[tmp <= rare_perc].index\n        rare_df[col] = np.where(rare_df[col].isin(rare_labels), 'Rare', rare_df[col])\n\n    return rare_df\n","033783e3":"new_titanic_df = rare_encoder(titanic_df)","32611072":"rare_analyser(new_titanic_df, \"SURVIVED\")","af02d231":"def useless_cols(dataframe, rare_perc=0.01):\n    useless_cols = [col for col in dataframe.columns if dataframe[col].nunique() == 2\n                    and (dataframe[col].value_counts() \/ len(dataframe) <= rare_perc).any(axis=None)]\n    new_df = dataframe.drop(useless_cols, axis=1)\n    return useless_cols ","cd36289b":"# It has been observed that there is no variable which can be considered as useless variable. \n\nuseless_cols(new_titanic_df)","44b89955":"def ohe_cols(dataframe):\n    ohe_cols = [col for col in dataframe.columns if (dataframe[col].dtype == \"O\" and 10 >= dataframe[col].nunique() > 2)]\n    return ohe_cols","d8c4df81":"ohe_col_names = ohe_cols(new_titanic_df)","515370cc":"   \ndef one_hot_encoder(dataframe, ohe_col_names, drop_first=True):\n    dms = pd.get_dummies(dataframe[ohe_col_names], drop_first=drop_first)    \n    df_ = dataframe.drop(columns=ohe_col_names, axis=1)              \n    dataframe = pd.concat([df_, dms],axis=1)                     \n    return dataframe","2071e793":"new_titanic_df = one_hot_encoder(new_titanic_df, ohe_col_names)","f117e874":"new_titanic_df.head()","0db2a06f":"upper_col_name(new_titanic_df)","238e8f47":"new_titanic_df.head(3)","47ce0a68":"cat_cols, num_cols, cat_but_car = grab_col_names(new_titanic_df)","18325c22":"num_cols","63b374f6":"scaler = StandardScaler()\nnew_titanic_df[num_cols] = scaler.fit_transform(new_titanic_df[num_cols])","b70853ab":"new_titanic_df.head()","344cb06b":"def high_correlated_cols(dataframe, plot=False, corr_th=0.75):\n    cat_cols, num_cols, cat_but_car = grab_col_names(dataframe)\n    cor_matrix = dataframe[num_cols].corr().abs()\n    #corr = dataframe.corr()\n    #cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n    if plot:\n        import seaborn as sns\n        import matplotlib.pyplot as plt\n        sns.set(rc={'figure.figsize': (12, 12)})\n        sns.set(font_scale=1) \n        sns.heatmap(cor_matrix, cmap=\"RdBu\",annot=True)\n        plt.show()\n    return drop_list","86afee44":"high_correlated_cols(new_titanic_df, plot=True)","48939942":"drop_list = [\"FARE\",\"SIBSP\",\"PARCH\"]\n\n# drop_list = high_correlated_cols(new_titanic_df)","dd232d88":"new_titanic_df = new_titanic_df.drop(drop_list, axis=1)","40af47ac":"new_titanic_df.head()","ed034a9e":"# Since different variables related to the \"AGE\" variable are derived, the \"AGE\" variable will be excluded and the modeling stage will be started.\n\nnew_titanic_df = new_titanic_df.drop(columns=\"AGE\",axis=1)","c9c7f89a":"X = new_titanic_df.drop(columns=\"SURVIVED\",axis=1)\ny = new_titanic_df[[\"SURVIVED\"]]\n\n# Train- test split:\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=112)\n\n# Model Training\n\nlog_model = LogisticRegression().fit(X_train,y_train)\n\n# Prediction\n\ny_pred = log_model.predict(X_test)","6cb9eb9d":"# Accuracy Score:\nprint(\"Accuracy Score:\",accuracy_score(y_test,y_pred))\n\n# Precision:\nprint(\"Precision Score:\", precision_score(y_test,y_pred))\n\n# Recall:\nprint(\"Recall Score:\" ,recall_score(y_test,y_pred))\n\n# F1 Score:\nprint(\"F1 Score:\", f1_score(y_test,y_pred))","4ed0f941":"#ROC CURVE \n\nAUC = logit_roc_auc =roc_auc_score(y_test,y_pred)\n\nplt.figure(figsize=(6,6))\nfpr ,tpr,thresholds= roc_curve(y_test,log_model.predict_proba(X_test)[:,1])\nplt.plot(fpr,tpr,label =\"AUC (area=%0.2f)\" % logit_roc_auc)\nplt.plot([0, 1], [0, 1], color='orange', linestyle='--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic\")\nplt.legend()\nplt.savefig(\"Log_ROC\")\nplt.show();\n","c0aad7c0":"cross_val_score(log_model, X_test,y_test,cv=10,scoring= \"neg_mean_squared_error\")\nnp.mean(cross_val_score(log_model, X_test,y_test,cv=10))","b81e4a66":"feature_importance = pd.DataFrame(X_train.columns, columns = [\"feature\"])\nfeature_importance[\"importance\"] = pow(mt.e, log_model.coef_[0])\nfeature_importance = feature_importance.sort_values(by = [\"importance\"], ascending=False)\n \n# Visualization \nax = feature_importance.plot.barh(x='feature', y='importance', figsize=(12,12), fontsize=10)\nplt.xlabel('Importance', fontsize=14)\nplt.ylabel('Features', fontsize=14)\nplt.show()","e070cd73":"feature_importance[0:10]","427d049e":"new_features = feature_importance[0:10]\ncols = [col for col in new_features[\"feature\"]]","d22134bd":"X_ = new_titanic_df[cols]\ny_ = new_titanic_df[[\"SURVIVED\"]]\n\n\nX_train_, X_test_, y_train_, y_test_ = train_test_split(X_, y_, \n                                                    test_size=0.20, \n                                                    random_state=112)\n\n\nlog_model_ = LogisticRegression().fit(X_train_,y_train_)\n\n\ny_pred_ = log_model_.predict(X_test_)\n\n# Accuracy Score:\n\nprint(\"Accuracy Score:\",accuracy_score(y_test_,y_pred_))\n\n# Precision:\nprint(\"Precision Score:\", precision_score(y_test_,y_pred_))\n\n# Recall:\nprint(\"Recall Score:\" ,recall_score(y_test_,y_pred_))\n\n# F1 Score:\nprint(\"F1 Score:\", f1_score(y_test_,y_pred_))","2723f6d8":"#ROC CURVE \n\nAUC = logit_roc_auc =roc_auc_score(y_test_,y_pred_)\n\nfpr ,tpr,thresholds= roc_curve(y_test_,log_model_.predict_proba(X_test_)[:,1])\nplt.figure(figsize=(6,6))\nplt.plot(fpr,tpr,label =\"AUC (area=%0.2f)\" % logit_roc_auc)\nplt.plot([0, 1], [0, 1], color='orange', linestyle='--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic\")\nplt.legend()\nplt.savefig(\"Log_ROC\")\nplt.show();\n","21a38873":"# Model Validation \n\ncross_val_score(log_model, X_test,y_test,cv=10,scoring= \"neg_mean_squared_error\")\n#print(cross_val_score(log_model, X_test,y_test,cv=10))    \nnp.mean(cross_val_score(log_model, X_test,y_test,cv=10))","a2aeda81":"**STANDARDIZATION**","6c401388":"* ***Outlier Detection:***","9f291d80":"As it can be seen above from two graphs, there is no \"Nullity Correlation\" in the variables other than the variables derived from each other (etc Age).","a239d6d9":"**DATA PREPROCESSING & FEATURE ENGINEERING**","9f59c4dc":"**Dataset Story** \n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nTitanic dataset contains information about the people involved in the Titanic shipwreck.  \n\n**Goal**\n\nPredict if a passenger survived the sinking of the Titanic or not. \n\n**Variables Description**\n* PassengerID : ID of the Passenger.\n* Survived: Survival (0 = No; 1 = Yes)\n* Pclass: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n* Name : Name of the Passenger\n* Sex: Sex of the Passenger (Female \/ Male)\n* Age: Age of the Passenger.\n* Sibsp: Number of siblings\/spouses aboard\n* Parch: Number of parents\/children aboard\n* Ticket : Ticket number.\n* Fare: Passenger fare (British pound)\n* Cabin: Cabin number\n* Embarked: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n \n \n**Steps**\n\n* Exploratory Data Analysis\n* Data Preprocessing & Feature Engineering\n* Encoding ( Label Encoding \/ One Hot Encoding \/ Rare Encoding\n* Model Buildind & Performance Metrics\n* Model Validation\n* Summary\n\n\n **References**\n \n http:\/\/rstudio-pubs-static.s3.amazonaws.com\/278621_8ab6e10f7b6941dba0dc8968955e73fe.html\n \n https:\/\/towardsdatascience.com\/importance-of-feature-engineering-methods-73e4c41ae5a3#:~:text=To%20improve%20the%20performance%20of,existing%20features%20into%20new%20features.\n \n ","98e8fbc6":"# **MODELING**","b8878c1f":"# SUMMARY\n\n**1. Dataset was read.**\n\n**2. Exploratory Data Analysis :** \n\n    * Data exploration stage has been completed by examining descriptive statistics and seperating categorical and numeric columns.\n\n**3. Data Preprocessing:**\n\n    * New features have been extracted from existing features.\n    \n    * The noisy variables (Name, Ticket, Cabin) using for newly derived features, have been removed.\n    \n    * Outliers have been checked by using Boxplot.\n    \n    * Variables with missing values have been handled. By creating Nullity Correlation Matrix, it has been checked whether it was a correlation between missing values.\n    \n    * Missing values in the numeric variables have been filled with the median value by grouping the basis of categorical variables. \n    \n    * Missing values of categorical variables have been filled by  the mode of the data.\n    \n    * Rare analysis have been applied for categorical variables, if any class distribution of these variables below 1%.\n    \n    * Useless features have been removed.\n    \n    * Outliers have been detected by LOF were dropped.\n    \n    * Dummy variables have been created.\n    \n    * Numerical variables have been standardized.\n    \n    * By examining the correlation between the variables, one of the highly correlated variables have been deleted. \n\n\n**4. Model Building:**\n\n    * Survival of Titanic passengers have been predicted by using 19 dependent variables and Logistic Regression Model. \n    \n    * Accuracy, Precision, Recall and F1 scores demonstrating the explanatory and performance of the model have been calculated. \n    \n    * AUC has been calculated by drawing the ROC curve.\n    \n    * Feature Importance has been calculated for selection of features that contributes the most in predicting the target variable.\n    \n**5. Model Evaluation:**\n \n     * Cross-validation has been used to estimate the performance of a model for both models (Before and After Feature selection)\n     \n     * Logistic regression model has been created both with all dependent variables and after Feature selection, and model performance metrics have calculated for both cases.   \n     \n     * The best Recall, Precision and F1 scores have been obtained by creating model with all dependent variables. \n     \n\n\nThank you for your comments and votes:)","dd248904":"Since 2-class variables with frequency less than 1% do not carry any information, we can delete these variables.","d706a7a4":"* ***Rare Encoding***\n\nLet's examine the class frequencies of categorical variables, if  any class distribution of these variables below 1%, we can combine  then as the \"Rare\" category.","fd1ce89e":"***Model Validation***","ac628196":"\n* ***Feature Extraction & Interactions***\n\nFeature Engineering is beneficial step is a data preparation process, that increase the performance of models. \n\nThis step will be performed first in order to deal with the missing values and outliers together with the newly derived features. ","a0ee2481":"**Import Libraries & Setting Configurations**","6bf96bac":"***Feature Importane***","5cfc81a7":"# Titanic Survival Model","0df484b1":"* ***Label Encoding:***","45431fa4":"***Model Performance Metrics***","675d0fd2":"***Correlation***","4a61111d":"* ***One-Hot-Encoding***","0280fcff":"# Logistic Regression","f14a7ba0":"**LocalOutlierFactor**","dbf44dd5":"* The maximum age value in the dataset appears to be 80 whereas it is not an impossible situation. So this value may not be considered an outlier for the relevant dataset.\n\n* If we examine the upper and lower limit values for other variables and consider them from this point of view, we can recheck outliers by replacing q1 value as 0.05 and q3  value as 0.95.\n\n* In the following steps, we will examine whether the variables together form an outlier by using Local Outlier Factor (LOF).","397bbf37":"***Showing Outliers with Boxplot:***","ff64778e":"**Import Data**","bcb90b0c":"**ENCODING**","4bfbcffa":"**Exploratory Data Analysis**","55e61867":"* ***Missing Values:***\n\nIf we know that the missing values are random, NaN values can be removed or filled. Bu if there is no randomness, that is, if there is nullity correlation between the variables, applying the fill\/delete operations will break the structure of the data set. "}}