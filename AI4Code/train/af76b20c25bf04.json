{"cell_type":{"2146a56d":"code","76f56234":"code","c6f7d82d":"code","bdd49c78":"code","1b73a2df":"code","e323c6be":"code","5daaf90c":"code","205135dc":"code","177511f0":"code","c01e9dde":"code","f2674a7e":"code","8f71634d":"code","1ebab197":"code","d2d75e33":"code","96339d87":"code","ae5b89c3":"code","569ed34e":"code","2340f898":"code","7bc66963":"code","f3d5ec3e":"code","98f4a381":"code","a0782da7":"code","e984065f":"code","da4395d3":"code","46ea0587":"code","752531a3":"code","1f6d7758":"code","4c886bd7":"code","38343da7":"code","ea8d3c9c":"code","a6f560ec":"code","ecf1ff8d":"code","9fee9851":"code","0ac920a8":"code","2b487be2":"code","5a3b0ac1":"code","d119b9ff":"code","2ab3bc21":"code","877692a2":"code","cd57bf37":"code","637f00dd":"code","752c3ea4":"code","374cf4d9":"code","62a807f0":"code","a9540257":"code","fcb0a2e0":"code","046935d2":"code","37a7fe7e":"code","8e01c5e8":"code","e6fd18fd":"code","6bde530f":"code","e3433e06":"code","67680a8c":"code","5b42d29b":"code","46cc5dfb":"code","b874f7b4":"code","40756d34":"code","7136c70c":"code","05f91453":"code","71711819":"code","331e964d":"code","8a3b6b92":"code","59c1234a":"code","b3b4d249":"code","2ff3b58d":"code","9cd1ae66":"code","f390ed14":"code","25992b87":"code","111455ec":"code","deba8013":"code","29f82c80":"code","d32ad64c":"code","f6ed7a70":"code","3097f16f":"code","6e7ddd82":"code","21100c33":"code","31051668":"code","6e6f9ea1":"code","8edaa210":"code","44d279e3":"code","41373860":"code","1021c131":"code","27e23d82":"code","d3d02778":"code","be234561":"code","94e29c09":"code","fd145fa4":"code","ae974d68":"code","08cd21d4":"markdown","1e8a6c09":"markdown","9a299277":"markdown","ecb882f9":"markdown","31d327a6":"markdown","e77ea8c3":"markdown","eb8fcc46":"markdown","5459d9fc":"markdown","34976195":"markdown","b5b4cf2a":"markdown","1f457d4e":"markdown","c9f478bc":"markdown","2bae51f8":"markdown","e36503e9":"markdown","cc672a40":"markdown","8c6a6b43":"markdown","9d2f7a36":"markdown","94cea51f":"markdown","1f0572ef":"markdown","8d9582ea":"markdown","4323fccd":"markdown","f63f0e71":"markdown","0460329f":"markdown","f4c04124":"markdown","b9ba73c6":"markdown","7ac4cc58":"markdown","524b1c78":"markdown","40cb9f4a":"markdown","b7a9a9d1":"markdown","c7fd2aed":"markdown","1cfc2b0a":"markdown","3e8abc95":"markdown","899e2b34":"markdown","055713d2":"markdown","6f00a701":"markdown","5abec735":"markdown","f44fe9de":"markdown","bb3b9038":"markdown","b45946f5":"markdown","dd6a889b":"markdown","f2783c6e":"markdown","ae1c4af0":"markdown","8627824d":"markdown","a52cbe1f":"markdown","505f2dc5":"markdown","286f6ce1":"markdown","8352d5c9":"markdown","313113dd":"markdown","a3ae1b24":"markdown","728b8126":"markdown","3905b5b8":"markdown","6e7b4134":"markdown","c1e25e07":"markdown","04445460":"markdown","380cef26":"markdown","bed136e3":"markdown","3584dbc3":"markdown","c1a44839":"markdown","7f12da1a":"markdown","5e9ca020":"markdown","e1f189fb":"markdown","339d3e42":"markdown","5d351d8b":"markdown","c962750f":"markdown","e8a19c94":"markdown","db84a8c1":"markdown","242d06c0":"markdown"},"source":{"2146a56d":"import numpy as np #working with matrices, arrays, data science-friendly arrays\nimport pandas as pd #data processing, CSV file I\/O, preprocessing\nimport matplotlib.pyplot as plt  #data viz library\n#jupyter notebook magic function to make plots show in a notebook cell\n%matplotlib inline  \nplt.style.use('seaborn-whitegrid') #set my default matplotlib style to 'seaborn-whitegrid'\n\nimport seaborn as sns  #additional data viz helper library\nimport scipy.stats as st  #used to fit non-normal distributions with seaborn\nimport missingno as msno  #visualize missing values in the dataset\n\nimport os  #working with the operating system, filepaths, folders,etc.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer #replace missing values with the mean\nfrom sklearn.feature_selection import mutual_info_regression #used to create a ranking with a feature utility metric \nfrom xgboost import XGBClassifier #first classifier model\nfrom sklearn.metrics import roc_auc_score","76f56234":"#This is default from Kaggle.  Basically uses os.walk to recursively \n#print the full filepath and filename for all files stored in the kaggle\/input folder.\n#Some people modify this to not just print the full filepath but to also read them into a dataframe.  \n#I'm just going to use the print statement to inform my pd.read_csv function call later.\n\n####### DEFAULT COMMENTS AND CODE FROM KAGGLE ###############\n\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n####### DEFAULT COMMENTS AND CODE FROM KAGGLE ###############","c6f7d82d":"#using the above print statements, set the filepath of the csv files I want to read.\ntrain_filepath = \"..\/input\/tabular-playground-series-sep-2021\/train.csv\"\ntest_filepath = \"..\/input\/tabular-playground-series-sep-2021\/test.csv\"\nsample_solution_filepath = \"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\"\n\n#read train.csv, test.csv, sample_solution.csv into a pandas dataframe\n#Need to use index_col = 0 because the id column is located at column index 0.  I found this below\n#after reading in the dataset and looking at the head of the dataframe.\ntrain_df = pd.read_csv(train_filepath, index_col = 0)  \ntest_df = pd.read_csv(test_filepath, index_col =0)\nss_df = pd.read_csv(sample_solution_filepath)","bdd49c78":"# training dataset dimensions using shape method\ntrain_df.shape","1b73a2df":"# test dataset dimensions using shape method\ntest_df.shape","e323c6be":"# sample submission dataset dimensions using shape method\nss_df.shape","5daaf90c":"#training dataset head and tail to get a feel for what the data looks like\n\ntrain_df.head()","205135dc":"train_df.tail()","177511f0":"#training dataset head and tail to get a feel for what the data looks like\ntest_df.head()","c01e9dde":"test_df.tail()","f2674a7e":"ss_df.head()","8f71634d":"ss_df.tail()","1ebab197":"## the sample solution dataframe is no longer need. delete the variable from memory to conserve a little RAM\ndel ss_df","d2d75e33":"train_df['claim'].head()","96339d87":"train_df['claim'].unique()","ae5b89c3":"train_df['claim'].dtype","569ed34e":"#value counts of 'claim' column\ntrain_df['claim'].astype('str').value_counts()","2340f898":"print('There is a {}% difference between \"0\" counts and \"1\" counts in the target \"claim\" column.'.format(((train_df['claim'].astype('str').value_counts()[0] - train_df['claim'].astype('str').value_counts()[1])\/train_df['claim'].astype('str').value_counts()[0])*100))","7bc66963":"plt.subplots(1,1, figsize=(5,5))\nplt.subplot(1,1,1)\ntarget_data_obj = train_df['claim'].astype('str').value_counts()\nplt.bar(x=target_data_obj.index, height=target_data_obj.values, alpha=0.75, color='#7571B0')\nplt.title(\"Target ('claim') Column Value Counts\", fontsize=12,fontweight='bold')","f3d5ec3e":"del target_data_obj","98f4a381":"print(\"There are {} columns in the training dataset.\".format(len(train_df.columns)))\ntrain_df.columns","a0782da7":"print(\"There are {} columns in the test dataset.\".format(len(test_df.columns)))\ntest_df.columns","e984065f":"#find any columns in train that do not appear in test\ncol_diff_list = [x for x in train_df.columns if x not in test_df.columns]\ncol_diff_list2 = [x for x in test_df.columns if x not in train_df.columns]\nprint('The column(s) that are in the training dataset but not in the test dataset are: {}'.format(col_diff_list))\nprint('The column(s) that are in the test dataset but not in the train dataset are: {}'.format(col_diff_list2))","da4395d3":"train_df.describe()","46ea0587":"test_df.describe()","752531a3":"train_df.iloc[:,:round(len(train_df.columns)\/3)].info()","1f6d7758":"train_df.iloc[:,round(len(train_df.columns)\/3):round(len(train_df.columns) * (2\/3))].info()","4c886bd7":"train_df.iloc[:,round(len(train_df.columns) * (2\/3)):round(len(train_df.columns) * (3\/3))].info()","38343da7":"train_df['claim'].dtype","ea8d3c9c":"test_df.iloc[:,:round(len(test_df.columns)\/3)].info()","a6f560ec":"test_df.iloc[:,round(len(test_df.columns)\/3):round(len(test_df.columns) * (2\/3))].info()","ecf1ff8d":"test_df.iloc[:,round(len(test_df.columns) * (2\/3)):round(len(test_df.columns) * (3\/3))].info()","9fee9851":"print('There are {} column(s) in {} with NULL values.'.format(len([col for col in train_df.columns if train_df[col].isnull().any()]),'train_df'))","0ac920a8":"print('There are {} column(s) in {} with NULL values.'.format(len([col for col in test_df.columns if train_df[col].isnull().any()]),'test_df'))","2b487be2":"#nullity matrix on training dataset to understand which columns have NULL values and where NULL values are dispersed throughtout the dataset.  Is there a pattern?\nmsno.matrix(train_df,color=(0.27, 0.52, 1.0))","5a3b0ac1":"#nullity matrix test dataset\nmsno.matrix(test_df,color=(0.27, 0.52, 1.0))","d119b9ff":"msno.bar(train_df.iloc[:,:round(len(train_df.columns)\/3)],color=(0.27, 0.52, 1.0))","2ab3bc21":"msno.bar(train_df.iloc[:,round(len(train_df.columns)\/3):round(len(train_df.columns) * (2\/3))],color=(0.27, 0.52, 1.0))","877692a2":"msno.bar(train_df.iloc[:,round(len(train_df.columns) * (2\/3)):],color=(0.27, 0.52, 1.0))","cd57bf37":"msno.bar(test_df.iloc[:,:round(len(test_df.columns)\/3)],color=(0.27, 0.52, 1.0))","637f00dd":"msno.bar(test_df.iloc[:,round(len(test_df.columns)\/3):round(len(train_df.columns) * (2\/3))],color=(0.27, 0.52, 1.0))","752c3ea4":"msno.bar(test_df.iloc[:,round(len(test_df.columns) * (2\/3)):],color=(0.27, 0.52, 1.0))","374cf4d9":"percent_missing_train_df = train_df.isnull().sum() * 100 \/ len(train_df)\nmissing_value_train_df = pd.DataFrame({'column_name': train_df.columns,\n                                 'percent_missing': percent_missing_train_df})\n\npercent_missing_test_df = test_df.isnull().sum() * 100 \/ len(test_df)\nmissing_value_test_df = pd.DataFrame({'column_name': test_df.columns,\n                                 'percent_missing': percent_missing_test_df})","62a807f0":"missing_value_train_df.sort_values('percent_missing', inplace=True, ascending=True)\nmissing_value_test_df.sort_values('percent_missing', inplace=True, ascending=True)","a9540257":"missing_value_train_df.plot.barh(x='column_name', y='percent_missing', rot=5,figsize=(10, 40),alpha=0.85,legend=False,color='#4F66AF')\nplt.title('Training Dataframe Percent Missing Values By Column Descending Order')","fcb0a2e0":"missing_value_test_df.plot.barh(x='column_name', y='percent_missing', rot=5,figsize=(10, 40),alpha=0.85,legend=False,color='#4F66AF')\nplt.title('Test Dataframe Percent Missing Values By Column Descending Order')","046935d2":"#delete the above dataframes from memory to help conserve RAM\ndel percent_missing_train_df\ndel missing_value_train_df\ndel percent_missing_test_df\ndel missing_value_test_df","37a7fe7e":"train_df_missing_values_count = train_df.isnull().sum()\ntrain_df_total_cells = np.product(train_df.shape)\ntrain_df_total_missing = train_df_missing_values_count.sum()\ntrain_df_percent_missing = (train_df_total_missing\/train_df_total_cells) * 100\nprint(\"There are {} missing data points in the training dataset out of {} total possible cells.\".format(train_df_total_missing,train_df_total_cells))\nprint(\"{}% of the training dataset is missing.\".format(round(train_df_percent_missing,3)))","8e01c5e8":"test_df_missing_values_count = test_df.isnull().sum()\ntest_df_total_cells = np.product(test_df.shape)\ntest_df_total_missing = test_df_missing_values_count.sum()\ntest_df_percent_missing = (test_df_total_missing\/test_df_total_cells) * 100\nprint(\"There are {} missing data points in the training dataset out of {} total possible cells.\".format(test_df_total_missing,test_df_total_cells))\nprint(\"{}% of the training dataset is missing.\".format(round(test_df_percent_missing,3)))","e6fd18fd":"#delete the above variables to help conserve RAM\ndel train_df_missing_values_count\ndel train_df_total_cells\ndel train_df_total_missing\ndel train_df_percent_missing\n\ndel test_df_missing_values_count\ndel test_df_total_cells\ndel test_df_total_missing\ndel test_df_percent_missing","6bde530f":"print(\"{}% of the rows would remain in the training dataset if we simply dropped all rows with any missing value!\".format(round((train_df.dropna().shape[0]\/train_df.shape[0])*100),2))","e3433e06":"print(\"{}% of the rows would remain in the test dataset if we simply dropped all rows with any missing value!\".format(round((test_df.dropna().shape[0]\/test_df.shape[0])*100),2))","67680a8c":"feature_cols = [col for col in train_df.columns if col != 'claim']","5b42d29b":"#define a function to loop over numeric feature columns and plot their distribution\ndef plot_feature_distributions(figrows,figcols,colstart,colend,collist,df_to_plot):\n    plt.figure(1)\n    plt.subplots(figrows,figcols, figsize=(20,20))\n    for i, item in enumerate(collist[colstart:colend]):\n        plt.subplot(figrows,figcols,i+1)\n        plt.hist(x=df_to_plot[item],color='#7571B0',alpha=0.75)\n        plt.title(item)\n        plt.grid(True)\n    plt.subplots_adjust(top=1.5, bottom=0.2, left=0.10, right=0.95, hspace=0.3,\n        wspace=0.35)","46cc5dfb":"#plot first 39 columns of train_df\nplot_feature_distributions(13,3,0,39,feature_cols,train_df)","b874f7b4":"#plot columns 40-78 of train_df\nplot_feature_distributions(13,3,39,78,feature_cols,train_df)","40756d34":"#plot columns 79-119 of train_df\nplot_feature_distributions(14,3,78,119,feature_cols,train_df)","7136c70c":"#plot first 39 columns of test_df\nplot_feature_distributions(13,3,0,39,feature_cols,test_df)","05f91453":"#plot columns 40-78 of test_df\nplot_feature_distributions(13,3,39,78,feature_cols,test_df)","71711819":"#plot columns 79-119 of test_df\nplot_feature_distributions(14,3,78,119,feature_cols,test_df)","331e964d":"skewness_df = pd.DataFrame(train_df.skew(),columns=['skewness'])\nskewness_test_df = pd.DataFrame(test_df.skew(),columns=['skewness'])","8a3b6b92":"#which features are extremely skewed either positively or negatively?\nskewness_df[(skewness_df['skewness'] < -1) | (skewness_df['skewness'] > 1)]","59c1234a":"#which features are extremely skewed either positively or negatively?\nskewness_test_df[(skewness_test_df['skewness'] < -1) | (skewness_test_df['skewness'] > 1)]","b3b4d249":"kurt_df = pd.DataFrame(train_df.kurt(),columns=['kurtosis'])\nkurt_test_df = pd.DataFrame(test_df.kurt(),columns=['kurtosis'])","2ff3b58d":"kurt_df[kurt_df['kurtosis'] < 0].shape","9cd1ae66":"#how many features have a kurtosis less than 0?\nkurt_df[kurt_df['kurtosis'] < 0]","f390ed14":"#how many features have a kurtosis less than 0?\nkurt_test_df[kurt_test_df['kurtosis'] < 0]","25992b87":"def highlight_abs_max(s):\n    '''\n    highlight the absolute maximum in a Series yellow.\n    '''\n    is_max = s == s.abs().max()\n    return ['background-color: yellow' if v else '' for v in is_max]","111455ec":"correlations =train_df.corr()\ncorrs_sorted = correlations['claim'].sort_values(ascending=False, key=abs).to_frame(name='Correlations With Target')\ncorrs_sorted[~corrs_sorted.index.isin(['claim','id'])].style.apply(highlight_abs_max)","deba8013":"#correlations of subsections\ncorrelations = train_df.iloc[:,:round(len(train_df.columns)\/4)].corr()\nf , ax = plt.subplots(figsize = (14,14))\nplt.title('Correlation of Numeric Variables f1 - f30',y=1,size=16)\nsns.heatmap(correlations,square = True,  vmax=0.8, cmap='viridis',linewidths=0.01,annot=True,annot_kws = {'size':7})","29f82c80":"#correlations of subsections\ncorrelations = train_df.iloc[:,round(len(train_df.columns)\/4):round(len(train_df.columns) * (2\/4))].corr()\nf , ax = plt.subplots(figsize = (14,14))\nplt.title('Correlation of Numeric Variables f31 - f60',y=1,size=16)\nsns.heatmap(correlations,square = True,  vmax=0.8, cmap='viridis',linewidths=0.01,annot=True,annot_kws = {'size':7})","d32ad64c":"#correlations of subsections\ncorrelations = train_df.iloc[:,round(len(train_df.columns) * (2\/4)):round(len(train_df.columns) * (3\/4))].corr()\nf , ax = plt.subplots(figsize = (14,14))\nplt.title('Correlation of Numeric Variables f61 - f89',y=1,size=16)\nsns.heatmap(correlations,square = True,  vmax=0.8, cmap='viridis',linewidths=0.01,annot=True,annot_kws = {'size':8})","f6ed7a70":"#correlations of subsections\ncorrelations = train_df.iloc[:,round(len(train_df.columns) * (3\/4)):round(len(train_df.columns) * (4\/4))].corr()\nf , ax = plt.subplots(figsize = (14,14))\nplt.title('Correlation of Numeric Variables f90 - claim',y=1,size=16)\nsns.heatmap(correlations,square = True,  vmax=0.8, cmap='viridis',linewidths=0.01,annot=True,annot_kws = {'size':8})","3097f16f":"#copy the training dataset\nX = train_df.copy()\ny = X.pop('claim')","6e7ddd82":"#split the dataset into a training\/validation set \nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0,train_size=0.8, test_size=0.2)","21100c33":"float_cols = [col for col in train_df if col != 'claim']","31051668":"#save the minmax scaler function as a variable\nmm_scaler = MinMaxScaler()","6e6f9ea1":"#min-max scale the numeric columns only.  In this case, that is every column.\n\n#fit and transform the training df\nscaled_cols_train = pd.DataFrame(mm_scaler.fit_transform(X_train[float_cols]),index = X_train.index, columns = X_train.columns)\n\n#just transform the validation and test df.  \nscaled_cols_valid = pd.DataFrame(mm_scaler.transform(X_valid[float_cols]),index = X_valid.index, columns = X_valid.columns)\nscaled_cols_test = pd.DataFrame(mm_scaler.transform(test_df),index= test_df.index, columns = test_df.columns)","8edaa210":"#1.6% of the dataset is missing, however, 62% of the rows and 100% of the columns have at least 1 missing value.  This means\n#I will impute rather than drop.\n\n# Imputing AFTER min-max scaling so the mean imputation is on the same scale.\n\n#set simple imputer variable.  By default, this imputs using the mean to replace missing values\nmy_imputer = SimpleImputer()\n\n#fit and transform the training df\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(scaled_cols_train), index=X_train.index)\n\nimputed_X_valid = pd.DataFrame(my_imputer.transform(scaled_cols_valid), index=X_valid.index)\nimputed_X_test = pd.DataFrame(my_imputer.transform(scaled_cols_test), index=test_df.index)\n\n\n# Imputation removed column names; put them back\n\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\nimputed_X_test.columns = test_df.columns","44d279e3":"# deleting some variables to save memory\ndel train_df\ndel test_df\ndel X_train\ndel X_valid\ndel scaled_cols_valid\ndel scaled_cols_train\ndel scaled_cols_test\n","41373860":"baseline_model = XGBClassifier(random_state=0, verbosity=0, tree_method='gpu_hist',use_label_encoder=False,n_estimators=500,learning_rate=0.05,n_jobs=4)\nbaseline_model.fit(imputed_X_train, y_train,\n             verbose = False,\n             eval_set = [(imputed_X_valid, y_valid)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 200)\npreds_valid = baseline_model.predict_proba(imputed_X_valid)[:,1]\nprint(roc_auc_score(y_valid, preds_valid))","1021c131":"#commented out:  Score:  0.78903  Rank:  1117\n\n\npredictions = baseline_model.predict_proba(imputed_X_test)[:,1]\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': imputed_X_test.index,\n                       'claim': predictions})\noutput.to_csv('submission.csv', index=False)\n","27e23d82":"def get_score(x_t,y_t,x_v,y_v,n_estimator_var):\n    \"\"\"Return the area under the curve for each model\n\n    \"\"\"\n    \n    baseline_model = XGBClassifier(random_state=0, verbosity=0, tree_method='gpu_hist',use_label_encoder=False,n_estimators=n_estimator_var,learning_rate=0.05,n_jobs=4)\n    baseline_model.fit(x_t,y_t,\n             verbose = False,\n             eval_set = [(x_v, y_v)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 200)\n    preds_valid = baseline_model.predict_proba(x_v)[:,1]\n    print(roc_auc_score(y_v, preds_valid))\n    return(roc_auc_score(y_v, preds_valid))","d3d02778":"#function commented out to save on runtime when saving.  results saved below in results variable.\n#results = dict((key, get_score(imputed_X_train, y_train, imputed_X_valid, y_valid, key)) for key in range(50,5000,500))\n\nresults = {50: 0.6903504543569663, 550: 0.7900936067312764, 1050: 0.7938331780560485, 1550: 0.7941154722257339, 2050: 0.7941154722257339, 2550: 0.7941154722257339, 3050: 0.7941154722257339, 3550: 0.7941154722257339, 4050: 0.7941154722257339, 4550: 0.7941154722257339}","be234561":"#plotting the results of all get_score() results found above.  Plotting number of trees vs. auc\nplt.plot(list(results.keys()), list(results.values()))\nplt.title(\"XG Boost Classifier Model N Trees Vs. Area under the ROC Curve\")\nplt.xlabel(\"N Trees\")\nplt.ylabel(\"AUC\")\nplt.show()","94e29c09":"results","fd145fa4":"final_model = XGBClassifier(random_state=0, verbosity=0, tree_method='gpu_hist',use_label_encoder=False,n_estimators=1550,learning_rate=0.05,n_jobs=4)\nfinal_model.fit(imputed_X_train, y_train,\n             verbose = False,\n             eval_set = [(imputed_X_valid, y_valid)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 200)\npreds_valid = final_model.predict_proba(imputed_X_valid)[:,1]\nprint(roc_auc_score(y_valid, preds_valid))","ae974d68":"predictions = final_model.predict_proba(imputed_X_test)[:,1]\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': imputed_X_test.index,\n                       'claim': predictions})\noutput.to_csv('submission.csv', index=False)","08cd21d4":"<br>\n\n<a id=\"36\"><\/a>\n\n### Final Baseline Prediction","1e8a6c09":"#### Test Dataset","9a299277":"#### There isn't really documentation or column name clues to tell me why there are missing values in this dataset.  Overall, there is only about 1.6% missing values in botht the training and test datasets which is pretty balanced.  It may be reasonable to just drop these NULL values and move forward.","ecb882f9":"<br>\n<br>\n\n<a id=\"target\"><\/a>\n\n### Target Column (\"claim\")","31d327a6":"<a id=\"21\"><\/a>\n\n### Kurtosis\n\n##### This is the degree of presence of outliers in the distribution","e77ea8c3":"#### Train","eb8fcc46":"#### Lots of purple meaning very low correlation less than 0.1 on the color scale.  Across all subsections I could not find any columns even mildly correlated with another.","5459d9fc":"#### There are 67 columns in both the training and testing datasets that are extremely skewed!","34976195":"<br>\n<br>\n\n<a id=intro><\/a>\n\n## Introduction","b5b4cf2a":"The goal of this notebook is to explore the Tabular Playground Series - Sep 2021 dataset, make some decisions about pre-processing steps based on my exploration, look for promising feature engineering opportunties and then make a benchmark submission to the competition.\n\nThis is only my second public notebook so I'm sure mistakes will be made.  I'm open to suggestions for improvement.  Thanks!\n\n***From the TPS September 2021 Competition Description Page:***\n\n*The dataset used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting whether a claim will be made on an insurance policy. Although the features are anonymized, they have properties relating to real-world features.*\n\n*Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.*\n\n*For each id in the test set, you must predict a probability for the claim variable. The file should contain a header and have the following format:  id, claim*","1f457d4e":"<br>\n\n<a id=\"30\"><\/a>\n\n### Baseline Predictions","c9f478bc":"#### All features columns are float65 type with the target \"claim\" column being an int64 type column.","2bae51f8":"<br>\n\n<a id=\"12\"><\/a>\n\n### Nullity Bar Charts","e36503e9":"<br>\n<br>\n\n<a id=\"15\"><\/a>\n\n### What If We Just Drop All Rows With Missing Values From The Datasets?","cc672a40":"<a id=\"target2\"><\/a>\n\n### Unique Values In The Target Column","8c6a6b43":"<a id=\"18\"><\/a>\n\n#### Test","9d2f7a36":"<a id='6'><\/a>\n\n### Dataset Head\/Tail","94cea51f":"<a id=\"target4\"><\/a>\n\n### Value Counts Of The Target Column","1f0572ef":"<br>\n\n#### Which columns have missing values?","8d9582ea":"#### Due to the large number of columns, the Missingno library nullity bar charts cannot handle so many columns by default.  I'm going to break up the dataset into thirds to get a better look.  Using this plot to understand, approximately, how many NULLS are in each column.","4323fccd":"<br>\n<br>\n\n<a id=\"19\"><\/a>\n\n### Feature Column Skewness and Kurtosis","f63f0e71":"<br>\n\n<a id=\"26\"><\/a>\n\n### Min-Max Scaling\n\nWe saw above all feature columns are float64 type with the target column (\"claim\") being a binary integer column.  The numeric feature columns vary greatly in scale so I would like to put all the feature columns on the same scale","0460329f":"#### 38 columns have kurtosis less than 0 and some have a kurtosis less than -1 which may indicate outliers.","f4c04124":"<br>\n\n<a id=\"23\"><\/a>\n\n### Correlations of Subsets","b9ba73c6":"<br>\n<br>\n\n<a id=\"29\"><\/a>\n\n### Baseline Model","7ac4cc58":"<a id=\"8\"><\/a>\n\n### Describe","524b1c78":"#### OK, huge dataset.  Time to use the GPU.  Sklearn's RandomForest classifier is out because it does not allow GPU usage.  So maybe, XGBoost Classifier, Catboost,  with gpu enabled or RAPIDS package version of RandomForest classifier instead of sklearn's.  Using Sklearn will take too long to train models on a dataset this size.  ","40cb9f4a":"<a id=\"target1\"><\/a>\n\n### Head","b7a9a9d1":"<br>\n\n<a id=\"27\"><\/a>\n\n\n### Impute Using Mean\n","c7fd2aed":"<br>\n<br>\n\n<a id=\"4\"><\/a>\n\n## Exploratory Data Analysis (EDA)","1cfc2b0a":"#### The target \"claim\" column is a binary 1 or 0 integer column.  It looks like the claim column in the  submission file should be a probability between 0 and 1.\n#### The target \"claim\" column is very balanced with only a 0.6% difference in counts between the two classes (0 or 1).","3e8abc95":"<a id=\"10\"><\/a>\n\n### Missing Values","899e2b34":"<a id=\"20\"><\/a>\n\n### Skewness","055713d2":"#### With this many columns, it is sort of hard to wrap my mind around all the different distributions in the dataset.  In short, the 118 numeric feature columns have a variety of different distributions.  Some that look pretty normal and some that are pretty skewed and some even look sort of binary or like some kind of categorical column with most values falling into a narrow bin.\n\n####  All the different columns appear to be on different scales so some sort of scaling such as min-max scaling could be useful with this dataset. Some features are on a 0-1 scale while some are in the tens of thousands.  Some are have negative numbers while some have only positive numbers.\n\n#### Spot-checking a few features in both the train and test seem to show similar distributions in both train and test which makes me feel the train\/test split is reasonably balanced.  For example, the distribution for f1 and f57 look pretty similar in both the training dataset and the test dataset.\n\n#### With so many skewed-looking columns, we could try some log, exponential, boxcox transformations during feature engineering to see if that may improve the model.","6f00a701":"<br>\n<br>\n\n<a id=\"16\"><\/a>\n\n### Feature Column Distributions\n\n##### We know all feature columns are numeric so we don't have any categorical columns in the intitial dataset to look at.  Let's look at each feature's distribution to see if we could possibly transform any.","5abec735":"<a id=\"target5\"><\/a>\n\n### How Balanced Is The Target Column?","f44fe9de":"<br>\n\n<a id=\"7\"><\/a>\n\n### Column Names And Check For Differences Between Train and Test","bb3b9038":"<br>\n<br>\n\n# Table of Contents\n\n* [Introduction](#intro)\n* [Load Libraries](#1)\n* [Find Data Files In Input Folder](#2)\n* [Read In The Data Files](#3)\n* [Exploratory Data Analysis (EDA)](#4)\n    * [Dataset Dimensions](#5)\n    * [Dataset Head\/Tail](#6)\n    * [Target Column (\"claim\")](#target)\n        * [Head](#target1)\n        * [Unique Values In The Target Column](#target2)\n        * [Target Column Data Type](#target3)\n        * [Value Counts Of The Target Column](#target4)\n        * [How Balanced Is The Target Column?](#target5)\n    * [Column Names And Check For Differences Between Train and Test](#7)\n    * [Describe](#8)\n    * [Info](#9)\n    * [Missing Values](#10)\n        * [Nullity Matrices](#11)\n        * [Nullity Bar Charts](#12)\n        * [Percent Missing In Each Column Bar Charts](#13)\n        * [How Many Missing Data Points Are There Total?  What Percent of Total Is Missing?](#14)\n        * [What If We Just Drop All Rows With Missing Values From The Datasets?](#15)\n    * [Feature Column Distributions](#16)\n        * [Train](#17)\n        * [Test](#18)\n    * [Feature Column Skewness and Kurtosis](#19)\n        * [Skewness](#20)\n        * [Kurtosis](#21)\n    * [Correlations](#22)\n        * [Correlations of Subsets](#23)\n* [TLDR: EDA Findings\/Conclusions](#24)\n* [Preprocess](#25)\n    * [Train\/Test Split](#28)\n    * [Min-Max Scaling](#26)\n    * [Impute Using Mean](#27)\n* [Baseline Model](#29)\n    * [Baseline Predictions](#30)\n    * [Define A Scoring Function](#31)\n    * [Choosing N Estimators](#32)\n    * [Tuning Results](#33)\n    * [Model Selection](#34)\n    * [Final Model](#35)\n    * [Final Baseline Prediction](#36)\n    ","b45946f5":"<a id=\"33\"><\/a>\n\n### Tuning Results","dd6a889b":"<a id=\"target3\"><\/a>\n\n### Target Column Data Type","f2783c6e":"<br>\n<br>\n\n<a id=\"14\"><\/a>\n\n### How Many Missing Data Points Are There Total?  What Percent of Total Is Missing?","ae1c4af0":"#### Every feature column is a float64 type column.  All columns have, at least, some missing values.  No column has more than 2% missing values in either the training or test datasets.\n\n#### There are no missing values in the target \"claim\" column\n\n#### f31 and f46 have the most NaN values but still not more than 2%","8627824d":"<br>\n<br>\n\n<a id=2><a\/>\n\n## Find Data Files In Input Folder","a52cbe1f":"#### Sample Submission ","505f2dc5":"<br>\n<br>\n<a id=1><a\/>\n    \n## Load Libraries  ","286f6ce1":"<br>\n\n<a id=\"34\"><\/a>\n\n### Model Selection\n\nBased on the above chart, performance stopped improving at 1550 estimators.\n\n{50: 0.6903504543569663,\n 550: 0.7900936067312764,\n 1050: 0.7938331780560485,\n 1550: 0.7941154722257339,\n 2050: 0.7941154722257339,\n 2550: 0.7941154722257339,\n 3050: 0.7941154722257339,\n 3550: 0.7941154722257339,\n 4050: 0.7941154722257339,\n 4550: 0.7941154722257339}","8352d5c9":"<a id=\"32\"><\/a>\n\n### Choosing N Estimators","313113dd":"<br>\n\n<a id=\"28\"><\/a>\n\n### Train\/Test Split","a3ae1b24":"<br>\n\n<a id=\"17\"><\/a>\n\n#### Train","728b8126":"#### Wow, so 62% of the rows have at least 1 missing value.  We also know from above missing value analysis that all (100%) columns also have at least one missing value.  Therefore, simply dropping all rows or all columns with a missing value would remove too much of the original dataset.  In this case, it's better to impute, in my opinion.","3905b5b8":"#### Based on the above, there appears to be, at least, some NULL values in each feature column and no NULL values in the target 'claim' column.  The NULL values appears to be randomly spread out throughout the datset in each column and appear to represent a very small percentage of each column's data.","6e7b4134":"<br>\n\n<a id=\"13\"><\/a>\n\n### Percent Missing In Each Column Bar Charts","c1e25e07":"<br>\n<br>\n\n<a id=\"25\"><\/a>\n\n### Preprocess","04445460":"<a id=\"9\"><\/a>\n\n### Info\n\nThere are lots of columns so breaking the dataset into 3 subsets","380cef26":"1. Training Dataset Shape:  ***(957919, 119)***\n2. Test Dataset Shape: ***(493474, 118)***\n3. This is a ***huge*** dataset that will likely test the default Kaggle CPU and RAM allocation.  GPU will likely be needed for faster iteration.  \n4. Due to necessity of GPU, models such as sklearn's RandomForest Classifier may not be appropriate due to its inability to work with GPU. Better choices might be XGBoost, Catboost, RAPIDS RandomForest, and other GPU-friendly classifier models\/packages.  Check out this discussion for more [tips on using GPU in Kaggle](https:\/\/www.kaggle.com\/c\/tabular-playground-series-sep-2021\/discussion\/271900#1511854)\n5. The target column \"claim\" is a binary integer column with no missing values.  This is a probabilistic classification problem.  Models such as logistic regression, tree based models such as DecisionTrees, RandomForest, XGBoostClassifier, etc.\n6. All feature columns are float64 datatype\n7. All feature columns have at least one missing value with the most sparse column only missing 1.6% of its data.\n8. 62% of the rows have at least one missing value.\n9. The target \"claim\" column has balanced classes.  Only a 0.6% difference between class value counts.\n10. The training and test datasets have the same columns, column types and similar distributions of all feature columns.\n11. All 118 feature columns have at least one missing value.  Missing values appear randomly dispersed throughout the dataset.\n12. There are 67 feature columns with extremely skewed distributions.  Some distributions even look like categorical due to appeared binning of values with short ranges.\n13. Several feature columns have very negative kurtosis indicating possible outliers.\n14. None of the columns are correlated with each other (correlation is very small).\n15. None of the feature columns are correlated with the target column (correlation is very small).\n16. The feature columns (all numeric) appear to be on different scales.  For example, some features are on a 0-1 scale, some are in the 10,000s, some features have negative values.\n17. Ideas for feature engineering:  Lots of skewed numeric columns.  I'd like to try some transformations to see if those would improve the model.  Binning numeric columns.  Clustering the feature columns to created a new cluster feature.","bed136e3":"#### Test","3584dbc3":"<br>\n<br>\n\n<a id =\"3\"><\/a>\n\n## Read In The Data Files","c1a44839":"<a id=\"5\"><\/a>\n\n### Dataset Dimensions","7f12da1a":"<br>\n\n<a id=\"11\"><\/a>\n\n### Nullity Matrices","5e9ca020":"<br>\n<br>\n\n<a id=\"22\"><\/a>\n\n### Correlations","e1f189fb":"<br>\n\n<a id=\"35\"><\/a>\n\n### Final Model","339d3e42":"#### Training dataset","5d351d8b":"<br>\n\n<a id=\"31\"><\/a>\n\n### Define A Scoring Function","c962750f":"#### Both the training and test datasets appear to have same columns except the training dataset also includes the target \"claim\" column.","e8a19c94":"#### At first glance, it appears the NULL values are relatively randomly dispersed throughout all feature columns.\n\n#### Row 105 and 119 appear to have the maximum nullity in the training dataset.\n\n#### Visually, it does not appear a large percentage of the dataset is NULL, but all feature columns appear to have at least have some missing values.","db84a8c1":"<br>\n<br>\n\n<a id=\"24\"><\/a>\n\n### TLDR: EDA Findings\/Conclusions","242d06c0":"#### Performing a correlation heatmap on the entire dataset is very difficult to see so I was not able to make one big correlation matrix between all 118 features plus the 'claim' column.  However, the largest correlation with the target 'claim' column is only -0.021.  None of the feature columns are very correlated with the 'claim' column"}}