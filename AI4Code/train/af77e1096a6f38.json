{"cell_type":{"e2a7ccd3":"code","e4de8289":"code","b04d17b5":"code","211f5f32":"code","d2cb3012":"code","adf45e11":"code","f707818d":"code","bf1ae1b0":"code","71e53945":"code","5566c323":"code","573e8abb":"code","5657c90e":"code","cdf80c0f":"code","ff7a39f3":"code","f9388eed":"code","d641fe9a":"code","c1fb4ecd":"code","b629d573":"code","5f72f3c8":"code","11ef9423":"code","ec767012":"code","e9c1d1e8":"code","477a2a70":"markdown","9e382a6f":"markdown","f667b353":"markdown","284be5d0":"markdown","a06e3a92":"markdown","8fa03b33":"markdown","aed613df":"markdown","1845816e":"markdown"},"source":{"e2a7ccd3":"import numpy as np\nimport pandas as pd\n\nimport os\nprint(os.listdir(\"..\/input\"))","e4de8289":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\nX = np.array(train.drop([\"label\"], axis=1)) \/ 255.\ny = np.array(train[\"label\"])\n\nfrom tensorflow.keras.utils import to_categorical\ny = to_categorical(y)\n\nX_test = np.array(test) \/ 255.\n\nprint(X.shape, y.shape, X_test.shape)","b04d17b5":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)\n\nprint(X_train.shape, X_val.shape, y_train.shape, y_val.shape)","211f5f32":"from tensorflow.keras.layers import Input, Dense, Dropout, Flatten\nfrom tensorflow.keras.models import Model","d2cb3012":"inp = Input(shape=(X_train.shape[1], ))\nfc_1 = Dense(128, activation=\"relu\")(inp)\nfc_2 = Dense(64, activation=\"relu\")(fc_1)\nflatten = Flatten()(fc_2)\noutp = Dense(y_train.shape[1], activation=\"softmax\")(flatten)\n\nmodel_nodrop = Model(inp, outp)\n\nmodel_nodrop.summary()\n\nmodel_nodrop.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])","adf45e11":"from tensorflow.keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint('nodrop.h5', \n                             monitor='val_acc', \n                             verbose=1, \n                             save_best_only=True, \n                             mode='max', \n                             save_weights_only = True)\n\nhist = model_nodrop.fit(X_train, \n                        y_train,\n                        verbose=0,\n                        batch_size=64, \n                        epochs=100, \n                        validation_data=(X_val, y_val), \n                        callbacks=[checkpoint])","f707818d":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (6,6)\n\nacc = hist.history['acc']\nval_acc = hist.history['val_acc']\nloss = hist.history['loss']\nval_loss = hist.history['val_loss']\nepochs = range(1, len(acc)+1)\n\nplt.figure()\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()","bf1ae1b0":"max(val_acc)","71e53945":"inp = Input(shape=(X_train.shape[1], ))\nfc_1 = Dense(128, activation=\"relu\")(inp)\nfc_1 = Dropout(0.5)(fc_1)\nfc_2 = Dense(64, activation=\"relu\")(fc_1)\nfc_2 = Dropout(0.5)(fc_2)\nflatten = Flatten()(fc_2)\noutp = Dense(y_train.shape[1], activation=\"softmax\")(flatten)\n\nmodel_drop = Model(inp, outp)\n\nmodel_drop.summary()\n\nmodel_drop.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])","5566c323":"checkpoint = ModelCheckpoint('dropout.h5', \n                             monitor='val_acc', \n                             verbose=1, \n                             save_best_only=True, \n                             mode='max', \n                             save_weights_only = True)\n\nhist = model_drop.fit(X_train, \n                      y_train, \n                      verbose=0,\n                      batch_size=64, \n                      epochs=100, \n                      validation_data=(X_val, y_val),\n                      callbacks=[checkpoint])","573e8abb":"acc = hist.history['acc']\nval_acc = hist.history['val_acc']\nloss = hist.history['loss']\nval_loss = hist.history['val_loss']\nepochs = range(1, len(acc)+1)\n\nplt.figure()\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()","5657c90e":"max(val_acc)","cdf80c0f":"# https:\/\/github.com\/andry9454\/KerasDropconnect\/blob\/master\/ddrop\/layers.py\n\nfrom tensorflow.keras.layers import Wrapper\nimport tensorflow.keras.backend as K\n\nclass DropConnect(Wrapper):\n    def __init__(self, layer, prob=1., **kwargs):\n        self.prob = prob\n        self.layer = layer\n        super(DropConnect, self).__init__(layer, **kwargs)\n        if 0. < self.prob < 1.:\n            self.uses_learning_phase = True\n\n    def build(self, input_shape):\n        if not self.layer.built:\n            self.layer.build(input_shape)\n            self.layer.built = True\n        super(DropConnect, self).build()\n\n    def compute_output_shape(self, input_shape):\n        return self.layer.compute_output_shape(input_shape)\n\n    def call(self, x):\n        if 0. < self.prob < 1.:\n            self.layer.kernel = K.in_train_phase(K.dropout(self.layer.kernel, self.prob), self.layer.kernel)\n            self.layer.bias = K.in_train_phase(K.dropout(self.layer.bias, self.prob), self.layer.bias)\n        return self.layer.call(x)","ff7a39f3":"inp = Input(shape=(X_train.shape[1], ))\nfc_1 = DropConnect(Dense(128, activation=\"relu\"), prob=0.5)(inp)\nfc_2 = DropConnect(Dense(64, activation=\"relu\"), prob=0.5)(fc_1)\nflatten = Flatten()(fc_2)\noutp = Dense(y_train.shape[1], activation=\"softmax\")(flatten)\n\nmodel_dropconn = Model(inp, outp)\n\nmodel_dropconn.summary()\n\nmodel_dropconn.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])","f9388eed":"checkpoint = ModelCheckpoint('dropconn.h5', \n                             monitor='val_acc', \n                             verbose=1, \n                             save_best_only=True, \n                             mode='max', \n                             save_weights_only = True)\n\nhist = model_dropconn.fit(X_train, \n                          y_train, \n                          verbose=0,\n                          batch_size=64, \n                          epochs=100, \n                          validation_data=(X_val, y_val),\n                          callbacks=[checkpoint])","d641fe9a":"acc = hist.history['acc']\nval_acc = hist.history['val_acc']\nloss = hist.history['loss']\nval_loss = hist.history['val_loss']\nepochs = range(1, len(acc)+1)\n\nplt.figure()\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()","c1fb4ecd":"max(val_acc)","b629d573":"sub_nodrop = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub_drop = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub_dropconn = pd.read_csv(\"..\/input\/sample_submission.csv\")","5f72f3c8":"model_nodrop.load_weights(\"nodrop.h5\")\nmodel_drop.load_weights(\"dropout.h5\")\nmodel_dropconn.load_weights(\"dropconn.h5\")","11ef9423":"y_test = model_nodrop.predict(X_test, batch_size=1024, verbose=0)\nsub_nodrop.Label = np.argmax(y_test, axis=1)\nsub_nodrop.to_csv(\"submission_nodrop.csv\", index=False)","ec767012":"y_test = model_drop.predict(X_test, batch_size=1024, verbose=0)\nsub_drop.Label = np.argmax(y_test, axis=1)\nsub_drop.to_csv(\"submission_drop.csv\", index=False)","e9c1d1e8":"y_test = model_dropconn.predict(X_test, batch_size=1024, verbose=0)\nsub_dropconn.Label = np.argmax(y_test, axis=1)\nsub_dropconn.to_csv(\"submission_dropconn.csv\", index=False)","477a2a70":"# predict and submit","9e382a6f":"# 1.  No drop","f667b353":"# load dataset","284be5d0":"LB result:\n1. no drop: 0.97457\n1. dropout: 0.97228\n1.  dropconnect: 0.97728","a06e3a92":"# 3. DropConnect","8fa03b33":"# train test split","aed613df":"Hi, Kagglers.\n\nIn this kernel, I am trying to test [DropConnect](https:\/\/cs.nyu.edu\/~wanli\/dropc\/) with two simple Dense layers.\n\nAnd the result shows DropConnect works better than Dropout.","1845816e":"# 2. Dropout"}}