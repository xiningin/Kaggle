{"cell_type":{"42fd91f0":"code","f684784f":"code","f36a158f":"code","e85cfd41":"code","6c8d07ae":"code","72d99902":"code","f3eea52b":"code","f84ff85a":"code","97e34cf6":"code","3893564a":"code","68df94b7":"code","52ca26fd":"markdown"},"source":{"42fd91f0":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f684784f":"%%bash\n\npip install pip install iterative-stratification\n\nmkdir models\n","f36a158f":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport os\nimport gc\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import log_loss\n\nimport category_encoders as ce\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nDEBUG = True\n\n\ndef get_logger(filename='models'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\n\nlogger = get_logger()\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything(seed=42)","e85cfd41":"BASE_PATH = \"..\/input\/lish-moa\/\"\ntrain_features = pd.read_csv(f'{BASE_PATH}train_features.csv')\ntrain_targets_scored = pd.read_csv(f'{BASE_PATH}train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(f'{BASE_PATH}train_targets_nonscored.csv')\ntest_features = pd.read_csv(f'{BASE_PATH}test_features.csv')\nsubmission = pd.read_csv(f'{BASE_PATH}sample_submission.csv')","6c8d07ae":"train = train_features.merge(train_targets_scored, on='sig_id')\ntarget_cols = [c for c in train_targets_scored.columns if c not in ['sig_id']]\ncols = target_cols + ['cp_type']\n\nprint(train_features.shape, test_features.shape)\ntrain = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\nprint(train.shape, test.shape)\n\nfolds = train.copy()\nFold = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor n, (train_index, val_index) in enumerate(Fold.split(folds, folds[target_cols])):\n    folds.loc[val_index, 'fold'] = int(n)\nfolds['fold'] = folds['fold'].astype(int)\nprint(folds.shape)","72d99902":"\nclass TrainDataset(Dataset):\n    def __init__(self, df, num_features, cat_features, labels):\n        self.cont_values = df[num_features].values\n        self.cate_values = df[cat_features].values\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.cont_values)\n\n    def __getitem__(self, idx):\n        cont_x = torch.FloatTensor(self.cont_values[idx])\n        cate_x = torch.LongTensor(self.cate_values[idx])\n        label = torch.tensor(self.labels[idx]).float()\n\n        return cont_x, cate_x, label\n\n\nclass TestDataset(Dataset):\n    def __init__(self, df, num_features, cat_features):\n        self.cont_values = df[num_features].values\n        self.cate_values = df[cat_features].values\n\n    def __len__(self):\n        return len(self.cont_values)\n\n    def __getitem__(self, idx):\n        cont_x = torch.FloatTensor(self.cont_values[idx])\n        cate_x = torch.LongTensor(self.cate_values[idx])\n\n        return cont_x, cate_x\n\n\ncat_features = ['cp_dose']\nnum_features = [c for c in train.columns if train.dtypes[c] != 'object']\nnum_features = [c for c in num_features if c not in cat_features]\nnum_features = [c for c in num_features if c not in target_cols]\ntarget = train[target_cols].values\n\n\ndef cate2num(df):\n    \"\"\"hours converts to days\"\"\"\n    df['cp_time'] = df['cp_time'].map({24: 0, 48: 1, 72: 2})\n    df['cp_dose'] = df['cp_dose'].map({'D1': 3, 'D2': 4})\n    return df\n\n\ntrain = cate2num(train)\ntest = cate2num(test)\n\n\nclass CFG:\n    max_grad_norm = 1000\n    gradient_accumulation_steps = 1\n    hidden_size = 1024\n    hidden_dim = 256\n    dropout = 0.5\n    lr = 1e-2\n    weight_decay = 1e-6\n    batch_size = 256\n    epochs = 20\n    # total_cate_size=5\n    # emb_size=4\n    num_features = num_features\n    cat_features = cat_features\n    target_cols = target_cols\n","f3eea52b":"class LSTMClassifier(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.gru_hidden_size = 64\n        self.lstm_hidden_size = 873\n\n        self.embedding_dropout = nn.Dropout2d(0.2)\n        self.lstm = nn.LSTM(self.lstm_hidden_size, cfg.hidden_dim, batch_first=True, bidirectional=True)\n\n        self.gru = nn.GRU(cfg.hidden_dim * 2, self.gru_hidden_size, bidirectional=True, batch_first=True)\n        self.linear = nn.Linear(384, self.gru_hidden_size * 6)\n        self.cls = nn.Linear(cfg.hidden_dim, len(cfg.target_cols))\n\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.1)\n        self.out = nn.Linear(self.gru_hidden_size * 6, len(cfg.target_cols))\n        # self.softmax = nn.LogSoftmax()\n\n    def forward(self, cont_x, cate_x):\n        cont_x = torch.unsqueeze(cont_x, 1)\n        h_lstm, lstm_out = self.lstm(cont_x)\n        h_gru, hh_gru = self.gru(h_lstm)\n        hh_gru = hh_gru.view(-1, self.gru_hidden_size * 2)\n\n        avg_pool = torch.mean(h_gru, 1)\n        max_pool, _ = torch.max(h_gru, 1)\n        conc = torch.cat((hh_gru, avg_pool, max_pool), 1)\n\n        dropped = self.dropout(conc)\n        out = self.out(dropped)\n        return out","f84ff85a":"\ndef train_fn(train_loader, model, optimizer, epoch, scheduler, device):\n    losses = AverageMeter()\n\n    model.train()\n\n    for step, (cont_x, cate_x, y) in enumerate(train_loader):\n\n        cont_x, cate_x, y = cont_x.to(device), cate_x.to(device), y.to(device)\n        batch_size = cont_x.size(0)\n\n        pred = model(cont_x, cate_x)\n\n        loss = nn.BCEWithLogitsLoss()(pred, y)\n        losses.update(loss.item(), batch_size)\n\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss \/ CFG.gradient_accumulation_steps\n\n        loss.backward()\n\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            scheduler.step()\n            optimizer.step()\n            optimizer.zero_grad()\n\n    return losses.avg\n\n\ndef validate_fn(valid_loader, model, device):\n    losses = AverageMeter()\n\n    model.eval()\n    val_preds = []\n\n    for step, (cont_x, cate_x, y) in enumerate(valid_loader):\n\n        cont_x, cate_x, y = cont_x.to(device), cate_x.to(device), y.to(device)\n        batch_size = cont_x.size(0)\n\n        with torch.no_grad():\n            pred = model(cont_x, cate_x)\n\n        loss = nn.BCEWithLogitsLoss()(pred, y)\n        losses.update(loss.item(), batch_size)\n\n        val_preds.append(pred.sigmoid().detach().cpu().numpy())\n\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss \/ CFG.gradient_accumulation_steps\n\n    val_preds = np.concatenate(val_preds)\n\n    return losses.avg, val_preds\n\n\ndef inference_fn(test_loader, model, device):\n    model.eval()\n    preds = []\n\n    for step, (cont_x, cate_x) in enumerate(test_loader):\n        cont_x, cate_x = cont_x.to(device), cate_x.to(device)\n\n        with torch.no_grad():\n            pred = model(cont_x, cate_x)\n\n        preds.append(pred.sigmoid().detach().cpu().numpy())\n\n    preds = np.concatenate(preds)\n\n    return preds\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\ndef run_single_nn(cfg, train, test, folds, num_features, cat_features, target, device, fold_num=0, seed=42):\n    # Set seed\n    # if not DEBUG:\n    logger.info(f'Set seed {seed}')\n    seed_everything(seed=seed)\n\n    # loader\n    trn_idx = folds[folds['fold'] != fold_num].index\n    val_idx = folds[folds['fold'] == fold_num].index\n    train_folds = train.loc[trn_idx].reset_index(drop=True)\n    valid_folds = train.loc[val_idx].reset_index(drop=True)\n    train_target = target[trn_idx]\n    valid_target = target[val_idx]\n    train_dataset = TrainDataset(train_folds, num_features, cat_features, train_target)\n    valid_dataset = TrainDataset(valid_folds, num_features, cat_features, valid_target)\n    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n                              num_workers=4, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=cfg.batch_size, shuffle=False,\n                              num_workers=4, pin_memory=True, drop_last=False)\n\n    # model\n    # model = TabularNN(cfg)\n    model = LSTMClassifier(cfg)\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n                                              max_lr=1e-2, epochs=cfg.epochs, steps_per_epoch=len(train_loader))\n\n    # log\n    log_df = pd.DataFrame(columns=(['EPOCH'] + ['TRAIN_LOSS'] + ['VALID_LOSS']))\n\n    # train & validate\n    best_loss = np.inf\n    for epoch in range(cfg.epochs):\n        train_loss = train_fn(train_loader, model, optimizer, epoch, scheduler, device)\n        valid_loss, val_preds = validate_fn(valid_loader, model, device)\n        log_row = {\n            'EPOCH': epoch,\n            'TRAIN_LOSS': train_loss,\n            'VALID_LOSS': valid_loss,\n        }\n        log_df = log_df.append(pd.DataFrame(log_row, index=[0]), sort=False)\n        # logger.info(log_df.tail(1))\n        if valid_loss < best_loss:\n            # if not DEBUG:\n            logger.info(f'epoch{epoch} save best model... {valid_loss}')\n            best_loss = valid_loss\n            oof = np.zeros((len(train), len(cfg.target_cols)))\n            oof[val_idx] = val_preds\n            # if not DEBUG:\n            torch.save(model.state_dict(), f\"models\/fold{fold_num}_seed{seed}.pth\")\n\n    # predictions\n    test_dataset = TestDataset(test, num_features, cat_features)\n    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n                             num_workers=4, pin_memory=True)\n    # model = TabularNN(cfg)\n    model = LSTMClassifier(cfg)\n    model.load_state_dict(torch.load(f\"models\/fold{fold_num}_seed{seed}.pth\"))\n    model.to(device)\n    predictions = inference_fn(test_loader, model, device)\n\n    # del\n    torch.cuda.empty_cache()\n\n    return oof, predictions\n\n\ndef run_kfold_nn(cfg, train, test, folds, num_features, cat_features, target, device, n_fold=5, seed=42):\n    oof = np.zeros((len(train), len(cfg.target_cols)))\n    predictions = np.zeros((len(test), len(cfg.target_cols)))\n\n    for _fold in range(n_fold):\n        # if not DEBUG:\n        logger.info(\"Fold {}\".format(_fold))\n        _oof, _predictions = run_single_nn(cfg,\n                                           train,\n                                           test,\n                                           folds,\n                                           num_features,\n                                           cat_features,\n                                           target,\n                                           device,\n                                           fold_num=_fold,\n                                           seed=seed)\n        oof += _oof\n        predictions += _predictions \/ n_fold\n\n    score = 0\n    for i in range(target.shape[1]):\n        _score = log_loss(target[:, i], oof[:, i])\n        score += _score \/ target.shape[1]\n    # if not DEBUG:\n    logger.info(f\"CV score: {score}\")\n\n    return oof, predictions","97e34cf6":"\n# Seed Averaging for solid result\noof = np.zeros((len(train), len(CFG.target_cols)))\npredictions = np.zeros((len(test), len(CFG.target_cols)))\n\nSEED = [0]\nfor seed in SEED:\n    _oof, _predictions = run_kfold_nn(CFG,\n                                      train, test, folds,\n                                      num_features, cat_features, target,\n                                      device,\n                                      n_fold=5, seed=seed)\n    oof += _oof \/ len(SEED)\n    predictions += _predictions \/ len(SEED)\n\nscore = 0\nfor i in range(target.shape[1]):\n    _score = log_loss(target[:, i], oof[:, i])\n    score += _score \/ target.shape[1]\nif not DEBUG:\n    logger.info(f\"Seed Averaged CV score: {score}\")\n","3893564a":"train[target_cols] = oof\ntest[target_cols] = predictions\n# Final result with 'cp_type'=='ctl_vehicle' data\nresult = train_targets_scored.drop(columns=target_cols)\\\n            .merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\ny_true = train_targets_scored[target_cols].values\ny_pred = result[target_cols].values\nscore = 0\nfor i in range(y_true.shape[1]):\n    _score = log_loss(y_true[:,i], y_pred[:,i])\n    score += _score \/ y_true.shape[1]\nlogger.info(f\"Final result: {score}\")","68df94b7":"sub = submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)\nsub.head()","52ca26fd":"### LSTM Model\n\nI did the modeling that was used in Jigsaw.\n\n\n### Future works\n\n- EMA\n- LSTM and NN multimodal learning\n- Features engineering\n- etc.(read other solutions)\n\n```python\ndef forward(self, cont_x, cate_x):\n    cont_x = torch.unsqueeze(cont_x, 1)\n    h_lstm, lstm_out = self.lstm(cont_x)\n    h_gru, hh_gru = self.gru(h_lstm)\n    hh_gru = hh_gru.view(-1, self.gru_hidden_size * 2)\n\n    avg_pool = torch.mean(h_gru, 1)\n    max_pool, _ = torch.max(h_gru, 1)\n    conc = torch.cat((hh_gru, avg_pool, max_pool), 1)\n\n    dropped = self.dropout(conc)\n    out = self.out(dropped)\n    return out\n```\n\n\nI used these two NOTEBOOKs as a reference.\n\nhttps:\/\/www.kaggle.com\/yasufuminakama\/moa-pytorch-nn-starter  \nhttps:\/\/www.kaggle.com\/sakami\/single-lstm-3rd-place"}}