{"cell_type":{"d88c70b1":"code","a0a418c1":"code","5ebd83cb":"code","8b73b7e7":"code","6d2a54bb":"code","18ed1265":"code","27677204":"code","8cb0d501":"code","f49a2f3b":"code","8be2e7a6":"code","a41d1f06":"code","238908ae":"code","4efe28f1":"code","c3892487":"code","88f7339a":"code","a0f67c04":"code","34a01df0":"code","1db7174e":"code","c2b2749e":"code","2a17ad33":"code","67fe3e8d":"code","e246ecba":"code","b8727e0e":"code","071e8384":"code","2ebc9aba":"code","4a39d080":"code","2ea97c46":"code","7cdf4707":"code","8496d5e9":"markdown","088776bc":"markdown","558b6712":"markdown","19f6adee":"markdown","895fb48d":"markdown","5bdb31b3":"markdown","224de54d":"markdown","2fdefb15":"markdown","6c93a8ae":"markdown","13bf36ae":"markdown","1f2113aa":"markdown","d7ee63f9":"markdown","c20acfb6":"markdown"},"source":{"d88c70b1":"!pip show keras","a0a418c1":"!pip freeze > requirements.txt","5ebd83cb":"import random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# keras\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, BatchNormalization\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\n# plt\nimport matplotlib.pyplot as plt\n#\u0443\u0432\u0435\u043b\u0438\u0447\u0438\u043c \u0434\u0435\u0444\u043e\u043b\u0442\u043d\u044b\u0439 \u0440\u0430\u0437\u043c\u0435\u0440 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\n#\u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u0432 svg \u0432\u044b\u0433\u043b\u044f\u0434\u044f\u0442 \u0431\u043e\u043b\u0435\u0435 \u0447\u0435\u0442\u043a\u0438\u043c\u0438\n%config InlineBackend.figure_format = 'svg' \n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8b73b7e7":"# seed values\nSEED = 42\nrandom.seed = SEED\nnp.random.seed(seed=SEED)","6d2a54bb":"# MODEL\nBATCH_SIZE  = 128\nEPOCH       = 10\nVAL_SPLIT   = 0.15  #15%\n\n# TOKENIZER\n# The maximum number of words to be used. (most frequent)\nMAX_WORDS = 20000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 150\n\nDATA_PATH = '\/kaggle\/input\/sf-dl-movie-genre-classification\/'","18ed1265":"train_df = pd.read_csv(DATA_PATH+'train.csv',)","27677204":"train_df.head()","8cb0d501":"train_df.info()","f49a2f3b":"train_df.genre.value_counts().plot(kind='bar',figsize=(12,4),fontsize=10)\nplt.xticks(rotation=60)\nplt.xlabel(\"Genres\",fontsize=10)\nplt.ylabel(\"Counts\",fontsize=10)","8be2e7a6":"test_df = pd.read_csv(DATA_PATH+'test.csv')\ntest_df.head()","a41d1f06":"Y = pd.get_dummies(train_df.genre)\nCLASS_NUM = Y.shape[1]\nprint('Shape of label tensor:', Y.shape)","238908ae":"Y.head()","4efe28f1":"# \u0434\u0430\u043d\u043d\u044b\u0435 \u0443 \u043d\u0430\u0441 \u0438 \u0442\u0430\u043a \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0447\u0438\u0441\u0442\u044b\u0435","c3892487":"# \u0434\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0441\u043b\u043e\u0432\u0430\u0440\u044f \u043c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0432\u0435\u0441\u044c \u0442\u0435\u043a\u0441\u0442\nall_text = train_df.text.append(test_df.text, ignore_index=True)","88f7339a":"%%time\ntokenize = Tokenizer(num_words=MAX_WORDS)\ntokenize.fit_on_texts(all_text)","a0f67c04":"%%time\nsequences = tokenize.texts_to_sequences(train_df.text)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=MAX_SEQUENCE_LENGTH)\nprint(sequences_matrix.shape)","34a01df0":"# \u0432\u043e\u0442 \u0442\u0430\u043a \u0442\u0435\u043f\u0435\u0440\u044c \u0432\u044b\u0433\u043b\u044f\u0434\u0438\u0442 \u043d\u0430\u0448 \u0442\u0435\u043a\u0441\u0442\nprint(train_df.text[1])\nprint(sequences_matrix[1])","1db7174e":"def RNN():\n    inputs = Input(name='inputs',shape=[MAX_SEQUENCE_LENGTH])\n    layer = Embedding(MAX_WORDS,50,input_length=MAX_SEQUENCE_LENGTH)(inputs)\n    layer = LSTM(100)(layer)\n    layer = Dense(256, activation='relu', name='FC1')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(CLASS_NUM, activation='sigmoid', name='out_layer')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","c2b2749e":"model = RNN()\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","2a17ad33":"history = model.fit(sequences_matrix,Y,\n                    batch_size=BATCH_SIZE,\n                    epochs=EPOCH,\n                    validation_split=VAL_SPLIT)","67fe3e8d":"model.save('keras_nlp_lstm.h5')","e246ecba":"plt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.show();","b8727e0e":"plt.title('Accuracy')\nplt.plot(history.history['acc'], label='train')\nplt.plot(history.history['val_acc'], label='test')\nplt.show();","071e8384":"test_sequences = tokenize.texts_to_sequences(test_df.text)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=MAX_SEQUENCE_LENGTH)","2ebc9aba":"%%time\npredict_proba = model.predict(test_sequences_matrix)","4a39d080":"# \u043d\u0430 \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044f\u0445 \u0432\u0441\u0435\u0433\u0434\u0430 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0439\u0442\u0435 predict_proba, \u0447\u0442\u043e\u0431 \u043f\u043e\u0442\u043e\u043c \u043c\u043e\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u044c \u0440\u0435\u0448\u0435\u043d\u0438\u0439\npredict_proba = pd.DataFrame(predict_proba, columns=Y.columns)\npredict_proba.to_csv('predict_proba.csv', index=False)\npredict_proba.head()","2ea97c46":"predict_genre = Y.columns[np.argmax(predict_proba.values, axis=1)]","7cdf4707":"submission = pd.DataFrame({'id':range(1, len(predict_genre)+1), \n                           'genre':predict_genre}, \n                          columns=['id', 'genre'])\n\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","8496d5e9":"### Target","088776bc":"### TEST","558b6712":"## \u0427\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c, \u0447\u0442\u043e\u0431 \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442:\n* \u041f\u043e\u0434\u043e\u0431\u0440\u0430\u0442\u044c LR, optimizer, loss\n* \u041f\u043e\u0438\u0433\u0440\u0430\u0442\u044c\u0441\u044f \u0441 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u043e\u0439\n* \u041f\u043e\u0434\u043e\u0431\u0440\u0430\u0442\u044c \u0434\u0440\u0443\u0433\u0438\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 (MAX_WORDS, MAX_SEQUENCE_LENGTH, \u0431\u0430\u0442\u0447 \u0438 \u0442\u043f)\n* \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043f\u043e\u043b\u0438\u0442\u0438\u043a\u0443 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n* \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0432 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0444\u0438\u043b\u044c\u043c\u0430 (\u0434\u043b\u044f pro: \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0439\u0442\u0435 \u044d\u0442\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0447\u0435\u0440\u0435\u0437 concatenate \u0441\u043b\u043e\u0439)\n* \u041f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u0443\u0436\u0435 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u0412\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0442\u043e\u0440\u044b (GLOVE, W2V \u0438 \u0442\u0434)\n* \u041f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u0443\u0436\u0435 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 (BERT, TRANFORMER \u0438 \u0442\u0434)\n* \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u044c \u0438\u0437 \u0440\u0430\u0437\u043d\u044b\u0445 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440","19f6adee":"# Model\n\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430 \u0441\u0435\u0442\u0438 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043f\u0440\u043e\u0441\u0442\u0430\u044f. \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0439\u0442\u0435 \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u043e\u0438\u0433\u0440\u0430\u0442\u044c\u0441\u044f \u0441 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 \u0441\u0435\u0442\u0438 \u0438 \u0441\u043e\u0441\u0442\u0430\u0432\u043e\u043c \u0441\u043b\u043e\u0435\u0432. \n","895fb48d":"# Submission","5bdb31b3":"### Clean Data","224de54d":"# Preprocessing","2fdefb15":"# SETUP\n\u0412 setup \u0432\u044b\u043d\u043e\u0441\u0438\u043c \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438","6c93a8ae":"\u0442\u0435\u043f\u0435\u0440\u044c \u043f\u0435\u0440\u0435\u0432\u0435\u0434\u0435\u043c \u043d\u0430\u0448 \u0442\u0435\u043a\u0441\u0442 \u0432 \u0432\u0435\u043a\u0442\u043e\u0440","13bf36ae":"### \u0423\u0434\u0430\u0447\u0438 \u0432 \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u0438!","1f2113aa":"# Data\n#### TRAIN","d7ee63f9":"### Tokenize data and convert the text to sequences\n\n\u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u2014 \u044d\u0442\u043e \u0441\u0430\u043c\u044b\u0439 \u043f\u0435\u0440\u0432\u044b\u0439 \u0448\u0430\u0433 \u043f\u0440\u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0442\u0435\u043a\u0441\u0442\u0430. \u0417\u0430\u043a\u043b\u044e\u0447\u0430\u0435\u0442\u0441\u044f \u0432 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0438 (\u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0438) \u0434\u043b\u0438\u043d\u043d\u044b\u0445 \u0441\u0442\u0440\u043e\u043a \u0442\u0435\u043a\u0441\u0442\u0430 \u0432 \u0431\u043e\u043b\u0435\u0435 \u043c\u0435\u043b\u043a\u0438\u0435: \u0430\u0431\u0437\u0430\u0446\u044b \u0434\u0435\u043b\u0438\u043c \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f, \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u043d\u0430 \u0441\u043b\u043e\u0432\u0430. \u0412 \u0438\u0442\u043e\u0433\u0435 \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043d\u0435\u043a\u0438\u0439 \u0441\u043b\u043e\u0432\u0430\u0440\u044c, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u0442 \u043f\u0440\u0435\u0432\u0440\u0430\u0442\u0438\u0442\u044c \u043d\u0430\u0448 \u0442\u0435\u043a\u0441\u0442 \u0432 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u0441\u0435\u0442\u0438.\n\n\u0432 Keras \u0435\u0441\u0442\u044c \u0437\u0430\u043c\u0435\u0447\u0430\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0443\u0442\u0438\u0434\u043b\u0438\u0442\u0430 [Tokenizer](https:\/\/keras.io\/preprocessing\/text\/) \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0432\u0441\u044e \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u0431\u0443\u043a\u0432\u0430\u043b\u044c\u043d\u043e \u0432 \u043f\u0430\u0440\u0443 \u0441\u0442\u0440\u043e\u0447\u0435\u043a \u043a\u043e\u0434\u0430! \n\u0411\u043e\u043b\u044c\u0448\u0435 \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432 \u0432 \u043a\u043e\u0434\u0435 \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u0439\u0442\u0438 \u0442\u0443\u0442:  \nhttps:\/\/machinelearningmastery.com\/prepare-text-data-deep-learning-keras\/","c20acfb6":"\u0413\u043e\u0442\u043e\u0432\u043e!"}}