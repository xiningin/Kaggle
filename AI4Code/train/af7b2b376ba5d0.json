{"cell_type":{"558f2326":"code","8ebdd39b":"code","c7d0b2b9":"code","c0cbe250":"code","a44d7499":"code","53b508af":"code","b7c9c469":"code","e5a87d3e":"code","9119ab77":"code","538d83e1":"code","1c65a3e3":"code","2baea6ec":"code","9827bc2e":"code","1ba17b99":"code","abf24475":"code","68af656d":"code","b187a400":"code","b7205779":"code","5904e198":"code","f1b2305e":"code","8ba5ff2a":"code","68d4fa80":"code","11325ce3":"code","75564c3d":"code","b6a7b1c5":"code","8597377a":"markdown","b75c1ed5":"markdown","9e57453d":"markdown","6dd31b6a":"markdown","4443dbb8":"markdown","618a76a1":"markdown","91206c28":"markdown","feb4cf2b":"markdown","e3c8d252":"markdown","7c382e17":"markdown","dcb89b5d":"markdown","0b356837":"markdown","2d7bdfbd":"markdown","f8e87177":"markdown","e74908e5":"markdown","c48525d1":"markdown","7e601bd3":"markdown","dbb83489":"markdown","2456715c":"markdown","0a3689a7":"markdown"},"source":{"558f2326":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nraw_data = pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\nraw_data.head()","8ebdd39b":"raw_data.describe(include='all')","c7d0b2b9":"#mapping senior citizen 1 to yes and 0 to no for visulization purposes and check info to identify data types\nraw_data['SeniorCitizen']=raw_data['SeniorCitizen'].map({1:'Yes', 0:'No'})\nraw_data.info()","c0cbe250":"#Seems No null values but some elements in TotalCharges columns are spaces (''), those do not not showup as nulls \nraw_data.isnull().sum()","a44d7499":"# change '' to NaN\nraw_data_with_nan  = raw_data.replace(' ', np.nan)\n# now we have 11 null elements\nraw_data_with_nan.isnull().sum()","53b508af":"#mising values (null) percentage is small(0.15%). We can drop all those rows\nprint (\"Missing Values Percentage: {}%\".format(11\/raw_data_with_nan.shape[0]*100))\ndata_no_mv = raw_data_with_nan.dropna(axis=0)\n#change data type of TotalCharges, object to float for further analysis \ndata_no_mv['TotalCharges'] = data_no_mv['TotalCharges'].astype(float)\n\n#customer id does not effect on analysis. It will drop from the dataset\ndata_no_mv_no_id = data_no_mv.drop(['customerID'],axis=1)","b7c9c469":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n#collect all columns with datatype 'Object'\nobject_cols = list(data_no_mv_no_id.select_dtypes(include=['object']).columns)\n#remove \"Churn\" column\nobject_cols.remove('Churn')\nfig, axes = plt.subplots(4, 4, figsize=(20, 20), sharex=True)\ni=0\nfor colname in object_cols:\n  i=i+1\n  ax1 = fig.add_subplot(4,4,i)\n  sns.countplot(x='Churn', hue=colname, data=data_no_mv_no_id)\nplt.show()","e5a87d3e":"plt.figure(figsize=(20,5))\ntenure_count = data_no_mv_no_id['tenure'].value_counts()\nsns.barplot(tenure_count.index, tenure_count.values, alpha=0.9)\nplt.title('Frequency distribution of Tenures', fontsize='17')\nplt.xlabel('Number of months', fontsize='15')\nplt.ylabel('Number of occurences',fontsize='15')\nplt.show()","9119ab77":"churn_count = data_no_mv_no_id['Churn'].value_counts()\nplt.figure(figsize=(4,2))\nplt.bar(churn_count.index,churn_count.values)\nplt.xlabel('Churn')\nplt.ylabel('Number of occurences')\nplt.title('frequency of target values')\nplt.show()","538d83e1":"sns.distplot(data_no_mv_no_id['MonthlyCharges'])\nplt.show()\nsns.distplot(data_no_mv_no_id['TotalCharges'])\nplt.show()","1c65a3e3":"# change categorical variables to numerical variables (one-hot). Drop first column for each \n# category to avoid extra correlinearity.\ndata_pre_processed = pd.get_dummies(data_no_mv_no_id,drop_first=True)\n#separate input and targets\ninputs = data_pre_processed.drop('Churn_Yes', axis=1)\ntargets = data_pre_processed['Churn_Yes']","2baea6ec":"# Import the scaling module to scale data\nfrom sklearn.preprocessing import StandardScaler\n# Create a scaler object\nscaler = StandardScaler()\n# Fit the inputs (calculate the mean and standard deviation feature-wise)\nscaler.fit(inputs)\n# scale input data\ninputs_scaled = scaler.transform(inputs)\n# Import the module for the split\nfrom sklearn.model_selection import train_test_split\n# Split the variables with an 80-20 split and some random state \nx_train, x_test, y_train, y_test = train_test_split(inputs_scaled, targets, test_size=0.2, random_state=1)","9827bc2e":"from sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression()\n#fit data to logistic model\nlog_model.fit(x_train,y_train)\n# get prediction on train data itself to measure the performance of the model \ny_hat = log_model.predict(x_train)\n#import confusion matrix\nfrom sklearn.metrics import confusion_matrix\n#create confusion matrix on train data\nprint('confusion matrix for training data = ', confusion_matrix(y_hat,y_train))\n#import accurracy score \nfrom sklearn.metrics import accuracy_score\n#compute accuray score of model on training data\nprint('acuracy score for training data = ', accuracy_score(y_hat,y_train))","1ba17b99":"#prediction on test data \npredictions = log_model.predict(x_test)\nprint('confusion matrix for test data = ', confusion_matrix(predictions,y_test))\nlogistic_acc = accuracy_score(predictions,y_test)\nprint('acuracy score for training data = ',logistic_acc)","abf24475":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nleaf_nodes_list = range(2,400)\nscore_list = []\nfor n_nodes in leaf_nodes_list:\n  rf_clf = RandomForestClassifier( max_leaf_nodes=n_nodes)\n  scores = cross_val_score(rf_clf, inputs, targets, scoring = \"neg_mean_squared_error\", cv=10)\n  score_list.append(-scores.mean())\nprint('minimum validation error = ',min(score_list))\nrfc_leaf_node = leaf_nodes_list[score_list.index(min(score_list))]\nprint('minimum error reaches for the number of leaf_nodes = ', rfc_leaf_node)\nplt.plot(leaf_nodes_list,score_list)\nplt.xlabel('Number of leaf nodes',fontsize=15)\nplt.ylabel('Validation Score',fontsize=15)\nplt.title('Random Forest Model',fontsize=15)\nplt.show()","68af656d":"from sklearn.ensemble import RandomForestClassifier\n#pre-test shows minmum between 50 and 200\nleaf_nodes_list = range(50,200)\nscore_list = []\nstd_list=[]\nfor n_nodes in leaf_nodes_list:\n    rf_clf = RandomForestClassifier( max_leaf_nodes=n_nodes)\n    scores = cross_val_score(rf_clf, inputs, targets, scoring = \"neg_mean_squared_error\", cv=10)\n    score_list.append(-scores.mean())\n    std_list.append(scores.std())\nprint('minimum validation error = ',min(score_list))\nrfc_min_score_leaf_node = leaf_nodes_list[score_list.index(min(score_list))]\nprint('minimum error reaches for the number of leaf_nodes = ', rfc_min_score_leaf_node )\nprint('minimum validation std = ',min(std_list))\nrfc_min_std_leaf_node=leaf_nodes_list[std_list.index(min(std_list))]\nprint('minimum std reaches for the number of leaf_nodes = ', rfc_min_std_leaf_node)\nfig, axes = plt.subplots(1, 2, figsize=(15, 5), sharex=False)\nfig.tight_layout()\nax1 = fig.add_subplot(1,2,1)\nplt.plot(leaf_nodes_list,score_list)\nplt.xlabel('Number of leaf nodes',fontsize=15)\nplt.title('Mean Validation Score',fontsize=15)\nax1 = fig.add_subplot(1,2,2)\nplt.plot(leaf_nodes_list,std_list)\nplt.xlabel('Number of leaf nodes',fontsize=15)\nplt.title('STD of Validation Score',fontsize=15)\nplt.show()","b187a400":"Tree_x_train, Tree_x_test, Tree_y_train, Tree_y_test = train_test_split(inputs, targets, test_size=0.3, random_state=15)\nrf_clf = RandomForestClassifier(random_state=30,max_leaf_nodes=rfc_min_score_leaf_node)\n#fit data to random forest model\nrf_clf.fit(Tree_x_train,Tree_y_train)\n#make predictions of train data itself\nrf_clf_hat = rf_clf.predict(Tree_x_test)\nprint('Prediction with the leaf node corresponding to the minimum validation score')\n#accuracy score\ndisplay(accuracy_score(rf_clf_hat,Tree_y_test))\n#confusion matrix\nprint(confusion_matrix(rf_clf_hat,Tree_y_test))","b7205779":"Tree_x_train, Tree_x_test, Tree_y_train, Tree_y_test = train_test_split(inputs, targets, test_size=0.3, random_state=15)\nrf_clf = RandomForestClassifier(random_state=30,max_leaf_nodes=rfc_min_std_leaf_node)\n#fit data to random forest model\nrf_clf.fit(Tree_x_train,Tree_y_train)\n#make predictions of train data itself\nrf_clf_hat = rf_clf.predict(Tree_x_test)\nprint('Prediction with the leaf node corresponding to the minimum standard diviation of validation score')\n#accuracy score\ndisplay(accuracy_score(rf_clf_hat,Tree_y_test))\n#confusion matrix\nprint(confusion_matrix(rf_clf_hat,Tree_y_test))","5904e198":"from sklearn.tree import DecisionTreeClassifier\nleaf_nodes_list = range(2,100)\nscore_list = []\nfor n_nodes in leaf_nodes_list:\n  dt_clf = DecisionTreeClassifier( max_leaf_nodes=n_nodes)\n  scores = cross_val_score(dt_clf, inputs, targets, scoring = \"neg_mean_squared_error\", cv=10)\n  score_list.append(-scores.mean())\nprint('minimum validation error = ',min(score_list))\ndtc_leaf_node = leaf_nodes_list[score_list.index(min(score_list))]\nprint('minimum error reaches for the number of leaf_nodes = ', dtc_leaf_node)\nplt.plot(leaf_nodes_list,score_list)\nplt.xlabel('Number of leaf nodes',fontsize=15)\nplt.ylabel('Validation Score',fontsize=15)\nplt.title('Decission Tree Classifier')\nplt.show()","f1b2305e":"dt_clf = DecisionTreeClassifier(random_state = 1, max_leaf_nodes=dtc_leaf_node)\n#fit data to the decision tree model\ndt_clf.fit(Tree_x_train,Tree_y_train)\n#make a prediction\ndt_clf_hat= dt_clf.predict(Tree_x_test)\nprint('predictions using the number of leaf nodes corresponding to the minimum validation score ')\n#accuray score\nprint('accuary score = ' ,accuracy_score(Tree_y_test,dt_clf_hat))\n#confusion matrix\nprint('confusion matrix = ',confusion_matrix(dt_clf_hat,Tree_y_test))","8ba5ff2a":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nleaf_nodes_list = range(2,100)\nscore_list = []\nfor n_nodes in leaf_nodes_list:\n  dec_tree_model = DecisionTreeRegressor( max_leaf_nodes=n_nodes)\n  scores = cross_val_score(dec_tree_model, inputs, targets, scoring = \"neg_mean_squared_error\", cv=10)\n  score_list.append(-scores.mean())\nprint('minimum validation error = ',min(score_list))\ndtr_leaf_node = leaf_nodes_list[score_list.index(min(score_list))]\nprint('minimum error reaches for the number of leaf_nodes = ', dtr_leaf_node)\nplt.plot(leaf_nodes_list,score_list)\nplt.xlabel('Number of leaf nodes',fontsize=15)\nplt.ylabel('Validation Score',fontsize=15)\nplt.title('Decission Tree Regressor')\nplt.show()","68d4fa80":"dt_reg = DecisionTreeRegressor(random_state = 1, max_leaf_nodes=dtr_leaf_node)\n#fit data to the decision tree model\ndt_reg.fit(Tree_x_train,Tree_y_train)\n#make a prediction\ndt_reg_hat= dt_reg.predict(Tree_x_test).round()\nprint('predictions using the number of leaf nodes corresponding to the minimum validation score ')\n#accuray score\nprint('accuracy score = ', accuracy_score(Tree_y_test,dt_reg_hat.round()))\n#confusion matrix\nprint('confusion matrix = ' ,confusion_matrix(dt_reg_hat.round(),Tree_y_test))","11325ce3":"feature_cols = list(inputs.columns)\nfrom sklearn.tree import export_graphviz\nfrom sklearn import tree\nfrom graphviz import Source\nfrom IPython.display import SVG,display\n\ngraph = Source(tree.export_graphviz(dt_reg,out_file=None,\n                                        rounded=True,proportion = False,\n                                        feature_names = feature_cols, \n                                        precision  = 2,\n                                        class_names=[\"Not churn\",\"Churn\"],\n                                        filled = True                         \n                                       )\n                  )\n\ndisplay(graph)","75564c3d":"from sklearn.ensemble import RandomForestRegressor\nleaf_nodes_list = range(2,200)\nscore_list = []\nfor n_nodes in leaf_nodes_list:\n  rf_reg = RandomForestRegressor( max_leaf_nodes=n_nodes)\n  scores = cross_val_score(rf_reg, inputs, targets, scoring = \"neg_mean_squared_error\", cv=10)\n  score_list.append(-scores.mean())\nprint('minimum validation error = ',min(score_list))\nrfr_leaf_node = leaf_nodes_list[score_list.index(min(score_list))]\nprint('minimum error reaches for the number of leaf_nodes = ', rfr_leaf_node)\nplt.plot(leaf_nodes_list,score_list)\nplt.xlabel('Number of leaf nodes',fontsize=15)\nplt.ylabel('Validation Score',fontsize=15)\nplt.title('Random Forest Regressor',fontsize=15)\nplt.show()","b6a7b1c5":"rf_reg = RandomForestRegressor(random_state=1,max_leaf_nodes=rfr_leaf_node)\n#fit data to random forest model\nrf_reg.fit(Tree_x_train,Tree_y_train)\n#make predictions \nrf_reg_hat = rf_reg.predict(Tree_x_test)\nprint('predictions using the number of leaf nodes corresponding to the minimum validation score ')\n#accuracy score\nprint('accuracy socre = ',accuracy_score(rf_reg_hat.round(),Tree_y_test))\n#confusion matrix\nprint('confusion matrix = ',confusion_matrix(rf_reg_hat.round(),Tree_y_test))","8597377a":"## <a id='4.1'>4.1. Deal with Categorical Variables<\/a>","b75c1ed5":" # <a id='9'>9. Random Forest Regression<\/a> ","9e57453d":"## <a id='3.1'>3.1.Churn Ratio Comparison<\/a>","6dd31b6a":" # <a id='2'>2. Data Cleansing<\/a>","4443dbb8":"# <a id='1'>1.Data<\/a>","618a76a1":" # <a id='6'>6. Random Forest Classifier<\/a>","91206c28":"## <a id='4.2'>4.2. Scaling and Splitting<\/a>","feb4cf2b":"# <a id='7'>7. Decision Tree Classifier<\/a>","e3c8d252":" ## <a id='8.1'>8.1. Decision Tree<\/a>","7c382e17":"# <a id='4'>4. Data Preprocessing<\/a>","dcb89b5d":"## <a id='3.2'>3.2. Frequency Distribution of Tenure<\/a>","0b356837":" ## <a id='6.2'>6.2. Predictions<\/a>","2d7bdfbd":" # <a id='8'>8. Decision Tree Regression<\/a>","f8e87177":" # <a id='3'>3. Exploratory Analysis<\/a>","e74908e5":"## <a id='1.1'>1.1. Data overview<\/a>","c48525d1":"- <a href='#1'>1. Data<\/a>\n    - <a href='#1.1'>1.1. Data overview<\/a>\n- <a href='#2'>2. Data Cleansing<\/a>\n- <a href='#3'>3. Exploratory Analysis<\/a>\n    - <a href='#3.1'>3.1.Churn Ratio Comparison<\/a>\n    - <a href='#3.2'>3.2. Frequency Distribution of Tenure<\/a>\n    - <a href='#3.3'>3.3. Frequency Distribution of Targets<\/a>\n    - <a href='#3.4'>3.4. Distribution Plot of Monthly and Total Charges <\/a>\n- <a href='#4'>4. Data Preprocessing<\/a>\n    - <a href='#4.1'>4.1. Deal with Categorical Variables<\/a>\n    - <a href='#4.2'>4.2. Scaling and Splitting<\/a>\n- <a href='#5'>5. Logistic Regression Model<\/a>\n- <a href='#6'>6. Random Forest Classifier<\/a>\n    - <a id='6.1'>6.1. Number of Leaf Nodes- Grid Search<\/a>\n    - <a id='6.2'>6.2. Predictions<\/a>\n- <a href='#7'>7. Decision Tree Regression<\/a>\n- <a href='#8'>8. Decision Tree Regression<\/a>\n    - <a href='#8.1'>8.1. Decision Tree<\/a>\n- <a href='#9'>9. Random Forest Regression<\/a> ","7e601bd3":" ## <a id='6.1'>6.1. Number of Leaf Nodes- Grid Search<\/a>","dbb83489":"## <a id='3.4'>3.4. Distribution Plot of Monthly Charges<\/a>","2456715c":"## <a id='3.3'>3.3. Frequency Distribution of Targets<\/a>","0a3689a7":"# <a id='5'>5. Logistic Regression Model<\/a>"}}