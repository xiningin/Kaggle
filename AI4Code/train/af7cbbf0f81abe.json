{"cell_type":{"fbd62439":"code","f2853c37":"code","4424ebe3":"code","4e52337a":"code","f06d8754":"code","c5024da6":"code","31997ec9":"code","b1f9a753":"code","7cd98d6c":"code","42549114":"code","c8768619":"code","496b4255":"code","f1db0432":"code","6e981dc0":"markdown","ff9b5ca1":"markdown","faf35296":"markdown","2f8ab729":"markdown","a8cacd73":"markdown"},"source":{"fbd62439":"!wget https:\/\/dl.fbaipublicfiles.com\/MLQA\/MLQA_V1.zip","f2853c37":"import zipfile\nwith zipfile.ZipFile('\/kaggle\/working\/MLQA_V1.zip') as zip_ref:\n    zip_ref.extractall('\/kaggle\/working\/')","4424ebe3":"import os\nimport sys\nimport random\nimport argparse\nimport json\nimport nltk\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\n# sys.setdefaultencoding('utf8')\nrandom.seed(42)\nnp.random.seed(42)","4e52337a":"mlqa_train_data = '\/kaggle\/working\/MLQA_V1\/dev\/dev-context-hi-question-hi.json'\nmlqa_test_data = '\/kaggle\/working\/MLQA_V1\/test\/test-context-hi-question-hi.json'\n\nwith open(mlqa_train_data, 'r') as file_input:\n    train_file = json.load(file_input)\n    \nwith open(mlqa_test_data, 'r') as file_input:\n    test_file = json.load(file_input)","f06d8754":"def preprocess(dataset, tier):\n    num_exs = 0 \n    examples = []\n\n    for articles_id in tqdm(range(len(dataset['data'])), desc=\"Preprocessing {}\".format(tier)):\n        article_paragraphs = dataset['data'][articles_id]['paragraphs']\n        for pid in range(len(article_paragraphs)):\n            context = article_paragraphs[pid]['context']\n            context = context.replace(\"''\", '\" ')\n            context = context.replace(\"``\", '\" ')\n            qas = article_paragraphs[pid]['qas'] \n            for qn in qas:\n                question = qn['question'] \n                ans_text = qn['answers'][0]['text']\n                ans_start_charloc = qn['answers'][0]['answer_start']\n                ans_end_charloc = ans_start_charloc + len(ans_text)\n                examples.append(\n                    {\n                        # 'id':articles_id,\n                        'context':context, \n                        'question':question, \n                        'answer_text':ans_text, \n                        'answer_start':ans_start_charloc, \n                        # 'answer_end':ans_end_charloc\n                    }\n                )\n\n                num_exs += 1\n    print(num_exs)    \n    return examples","c5024da6":"examples_train = preprocess(train_file, 'dev')\nexamples_test = preprocess(test_file, 'test')","31997ec9":"examples = examples_train + examples_test\nmlqa = pd.DataFrame(examples)\nmlqa['language'] = 'hindi'","b1f9a753":"!git clone https:\/\/github.com\/deepmind\/xquad.git","7cd98d6c":"xquad_train_file = '\/kaggle\/working\/xquad\/xquad.hi.json'\n\nwith open(xquad_train_file, 'r') as file_input:\n    train_file = json.load(file_input)\n    \nexamples_train = preprocess(train_file, 'dev')\nxquad = pd.DataFrame(examples_train)\nxquad['language'] = 'hindi'","42549114":"import os, shutil\nfolder = '\/kaggle\/working\/'\nfor filename in os.listdir(folder):\n    file_path = os.path.join(folder, filename)\n    try:\n        if os.path.isfile(file_path) or os.path.islink(file_path):\n            os.unlink(file_path)\n        elif os.path.isdir(file_path):\n            shutil.rmtree(file_path)\n    except Exception as e:\n        print('Failed to delete %s. Reason: %s' % (file_path, e))","c8768619":"mlqa.to_csv('mlqa_hindi.csv', index=False)\nxquad.to_csv('xquad.csv', index=False)","496b4255":"xquad.head(5)","f1db0432":"mlqa.head(5)","6e981dc0":"### Remove downloaded files","ff9b5ca1":"### Save Data","faf35296":"### XQUAD","2f8ab729":"<h2>chaii QA - 5 Fold XLMRoberta Training + Inference in Torch w\/o Trainer API<\/h2>\n    \n<h3><span \"style: color=#444\">Introduction<\/span><\/h3>\n\nThis kernel preprocesses MLQA, XQUAD Hindi Corpus. For more information check the finetuning notebook.\n\nThis is a three part kernel,\n\n- [External Data - MLQA, XQUAD Preprocessing](https:\/\/www.kaggle.com\/rhtsingh\/external-data-mlqa-xquad-preprocessing) which preprocesses the Hindi Corpus of MLQA and XQUAD. I have used these data for training.\n\n- [chaii QA - 5 Fold XLMRoberta Torch | FIT](https:\/\/www.kaggle.com\/rhtsingh\/chaii-qa-5-fold-xlmroberta-torch-fit\/edit) This kernel showcases Finetuning (FIT) on competition + external data combining different strategies.\n\n- [chaii QA - 5 Fold XLMRoberta Torch | Infer](https:\/\/www.kaggle.com\/rhtsingh\/chaii-qa-5-fold-xlmroberta-torch-infer) The Inference kernel where we ensemble our 5 Fold XLMRoberta Models and do the submission.","a8cacd73":"## MLQA"}}