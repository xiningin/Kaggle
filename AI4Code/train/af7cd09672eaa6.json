{"cell_type":{"de399584":"code","6295af4b":"code","4e57337f":"code","8674b42b":"code","fcaa7dfb":"code","34e0ed67":"code","489583be":"code","f8c15d2f":"code","7bddc6ba":"code","b3824e27":"code","07f305a4":"code","c5fabde3":"code","0e4b5e6f":"code","2b77aea6":"code","f1fefb06":"code","79b0fd7a":"code","eeb1bc9a":"code","4dfe65af":"markdown","a7da791e":"markdown","60035015":"markdown","8fcfbce0":"markdown","26902ef3":"markdown","6ad4abf1":"markdown","d38386cb":"markdown","19aef1dc":"markdown"},"source":{"de399584":"import copy\nimport json\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split as sk_train_test_split\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Embedding\n\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.preprocessing.text import Tokenizer","6295af4b":"def get_categories(df):\n    return df['category'].unique()","4e57337f":"# Load the Keras tokenizer\n# Note that it will use only the most \"num_words\" used words\ndef load_tokenizer(X_data, num_words=150000):\n    tokenizer = Tokenizer(num_words=num_words)\n    tokenizer.fit_on_texts(X_data)\n    return tokenizer","8674b42b":"def data_to_sequences(X_data, tokenizer, max_sequence_length):\n    X_data = tokenizer.texts_to_sequences(X_data)\n    X_data = sequence.pad_sequences(X_data, maxlen=max_sequence_length)\n    return X_data","fcaa7dfb":"def train_test_split(X_data, Y_data, tokenizer, max_sequence_length):\n    X_data = data_to_sequences(X_data, tokenizer, max_sequence_length)\n    \n    Y_data = Y_data.astype(np.int32)\n    X_train, X_test, Y_train, Y_test = sk_train_test_split(X_data, Y_data, test_size=0.3)\n    \n    X_train = np.array(X_train)\n    Y_train = np.array(Y_train)\n    X_test = np.array(X_test)\n    Y_test = np.array(Y_test)\n    \n    return X_train, X_test, Y_train, Y_test","34e0ed67":"df = pd.read_csv('..\/input\/bbc-articles-cleaned\/tfidf_dataset.csv')\ndf.head()","489583be":"X_data = df[['text']].to_numpy().reshape(-1)\nY_data = df[['category']].to_numpy().reshape(-1)","f8c15d2f":"category_to_id = {}\ncategory_to_name = {}\n\nfor index, c in enumerate(Y_data):\n    if c in category_to_id:\n        category_id = category_to_id[c]\n    else:\n        category_id = len(category_to_id)\n        category_to_id[c] = category_id\n        category_to_name[category_id] = c\n    \n    Y_data[index] = category_id\n\n# Display dictionary\ncategory_to_name","7bddc6ba":"MAX_SEQUENCE_LENGTH = 1000\n\nn_texts = len(X_data)\nprint('Texts in dataset: %d' % n_texts)\n\nn_categories = len(get_categories(df))\nprint('Number of categories: %d' % n_categories)\n\nprint('Loading tokenizer...')\ntokenizer = load_tokenizer(X_data)\n\nprint('Loading train dataset...')\nX_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, tokenizer, MAX_SEQUENCE_LENGTH)\n\nprint('Done!')","b3824e27":"def load_embedding_matrix(tokenizer):\n    embedding_dim = 100\n    embeddings_index = {}\n\n    f = open('..\/input\/glove6b\/glove.6B.100d.txt')\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n\n    word_index = tokenizer.word_index\n    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix, embedding_dim","07f305a4":"def create_lstm_model(tokenizer, input_length, n_categories):\n    word_index = tokenizer.word_index\n    embedding_matrix, embedding_dim = load_embedding_matrix(tokenizer)\n\n    model = Sequential()\n    model.add(Embedding(input_dim=len(word_index) + 1,\n                        output_dim=embedding_dim,\n                        weights=[embedding_matrix],\n                        input_length=input_length,\n                        trainable=True))\n    model.add(SpatialDropout1D(0.3))\n    model.add(LSTM(64,\n                   activation='tanh',\n                   dropout=0.2,\n                   recurrent_dropout=0.5))\n    model.add(Dense(n_categories, activation='softmax'))\n    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    \n    return model","c5fabde3":"EPOCHS = 10\n\nmodel = create_lstm_model(tokenizer, MAX_SEQUENCE_LENGTH, n_categories)\nhistory = model.fit(X_train,\n                    Y_train,\n                    epochs=EPOCHS,\n                    validation_data=(X_test, Y_test),\n                    verbose=1)","0e4b5e6f":"def plot_confusion_matrix(X_test, Y_test, model):\n    Y_pred = model.predict_classes(X_test)\n    con_mat = tf.math.confusion_matrix(labels=Y_test, predictions=Y_pred).numpy()\n\n    con_mat_norm = np.around(con_mat.astype('float') \/ con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n    label_names = list(range(len(con_mat_norm)))\n\n    con_mat_df = pd.DataFrame(con_mat_norm,\n                              index=label_names, \n                              columns=label_names)\n\n    figure = plt.figure(figsize=(10, 10))\n    sns.heatmap(con_mat_df, cmap=plt.cm.Blues, annot=True)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","2b77aea6":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nx_labels = range(1, EPOCHS + 1)\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(x_labels, acc, color='b', linestyle='-', label='Training acc')\nplt.plot(x_labels, val_acc, color='b', linestyle='--', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(x_labels, loss, color='b', linestyle='-', label='Training acc')\nplt.plot(x_labels, val_loss, color='b', linestyle='--', label='Validation acc')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","f1fefb06":"plot_confusion_matrix(X_test, Y_test, model)","79b0fd7a":"scores = model.evaluate(X_test, Y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1] * 100))","eeb1bc9a":"category_to_name","4dfe65af":"The global accuracy (i.e. taking into account all categories) looks pretty good for this selection of model and parameters: $95.96\\%$.\n\nIf you take a closer look into the confussion matrix, you can see that most of mismatches are in the categories $2$ and $4$ which, according to the dictionary of category-ID, are `tech` and `business`.","a7da791e":"## Create model and train\n\nI have used the loss function $SparseCategoricalCrossentropy$ since I have a multi-class problem. This model returns **a vector of probabilities** where each value is the probability for each *category* (hence I must pick the highest one).\n\nIn the first layer of the model, I have used a **pre-trained set of embedding vectors** (so I should get more accurate results). These embeddings have been trained over Wikipedia 2014 database using the top $400K$ words (source: http:\/\/nlp.stanford.edu\/data\/glove.6B.zip). However, some words might not be present in that embedding set, so I have initialized the vectors (of those \"missing words\") with zeros.\n\nSince the classes are not highly unbalanced, I have used **accuracy** to measure each model and find the best one. In other cases, you should consider to use a different metric, e.g. F1-score.","60035015":"In this version of the notebook, I did not add the function to tunne the parameters (**hyperparameter optimization**), I just wrote the best parameter values that I found. If you are interested in it the process to obtain them, take a look to my github repository: https:\/\/github.com\/DimasDMM\/text-classification","8fcfbce0":"I must to something similar with the categories: I only can **predict classes in a numeric format**, I had to transform each category into a number.","26902ef3":"## Prepare dataset\n\nSince the LSTM layer process sequences of number, I needed to **transform the texts into numbers**, so I used Keras Tokenizer to transform them. The Tokenizer contains a dictionary word-number, so every text we tokenize will be the same.","6ad4abf1":"## Evaluation","d38386cb":"I have **padded the short texts** with zeros and, in case that there are texts longer than a certain length (I fixed this value to $1000$ words), cut them (so all texts are the same length).","19aef1dc":"# Simple text classification with LSTM\n\nIn this approach, I chose to use a **Long Short Term Memory** (aka. LSTM) model as it is a good model to make predictions in numeric series. You can read below some additional descriptions of this approach.\n\nPlease, note that the data that I use in this notebook is already cleaned and ready to be used as input."}}