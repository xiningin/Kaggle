{"cell_type":{"5a6a180c":"code","4fb4f6fe":"code","467bdb47":"code","8065037d":"code","fc89947e":"code","95053736":"code","fc861ccf":"code","eb78a4fa":"code","c90f4e1c":"code","5f7c1b5c":"code","1f2bb37c":"code","9ab4308e":"code","85981db7":"code","b412b767":"code","c3df8a52":"code","6813d42c":"code","e0572d30":"code","be15a8e5":"code","837ef32a":"code","1e283405":"code","8fad99d4":"code","31fabc9f":"code","2e643195":"code","954b00ae":"code","ced97f01":"code","72ae53fd":"code","5b49d17b":"code","d1e32338":"code","387624c1":"code","4d91c7da":"code","50cca219":"code","7ba8857f":"code","ab62143e":"code","2c0f5f88":"code","c3b60ec5":"code","ca1f236e":"code","7e0aa844":"code","f4f57284":"code","d478a9a2":"code","5ea6a019":"code","ece30d8c":"code","a00bd372":"code","c330d36e":"code","8e10bbd0":"code","0b2ad54d":"code","2f9ebe23":"code","39abb83d":"code","db2912dd":"code","21af8842":"code","b6b4f4f5":"code","2b9eab5f":"code","293e26a7":"code","e8dc5375":"code","7bf6d9c7":"code","721aa21a":"code","479da28f":"code","fe3b3dd5":"code","968aaf0d":"code","1ea7eae0":"code","b44a882f":"code","052df256":"code","2e82fbdc":"code","f665910c":"code","f9f55e0d":"code","c874593d":"code","c3b38010":"code","55b2df5f":"code","6dc3c49a":"code","51020e9b":"code","38eba368":"markdown","e3d07e46":"markdown","7a44bdc6":"markdown","98fd6338":"markdown","52cb8a80":"markdown","04a22e0f":"markdown","3aba0f75":"markdown","54f5156d":"markdown","df30d90e":"markdown","3a59909a":"markdown","5744cc67":"markdown","b870f2ad":"markdown","a4f55c5f":"markdown","e4c9726a":"markdown","e304f4ae":"markdown","e356eabf":"markdown","825a655c":"markdown","a4dfb511":"markdown","f858ae8c":"markdown","a0c75a4e":"markdown","287653f5":"markdown","20b52b9e":"markdown","89e47598":"markdown","6ce69e44":"markdown","aed5833a":"markdown","c8c8fb81":"markdown","733e94b3":"markdown","4dd026a3":"markdown","9fd25555":"markdown","d17a24f2":"markdown","3e035cc0":"markdown","77b514be":"markdown","2a65911f":"markdown","57b3e365":"markdown","65edd4ce":"markdown","1875176b":"markdown","6b295518":"markdown","7501493f":"markdown","f3d0d80b":"markdown","61781b7c":"markdown","95ff7b8a":"markdown","d5e17caa":"markdown","6bf56863":"markdown","2658f7b9":"markdown","38cf4513":"markdown","31f28f09":"markdown"},"source":{"5a6a180c":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import plot_tree\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report,confusion_matrix,roc_curve, roc_auc_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder","4fb4f6fe":"wine = pd.read_csv('..\/input\/beginner-datasets\/beginner_datasets\/wine.csv')\n\nwine # display the first and last 5 rows","467bdb47":"wine.info() # we would like to see if we have null values in the df","8065037d":"wine.shape # 6497 rows and 13 columns","fc89947e":"wine.isnull().sum() #for NaN","95053736":"wine.duplicated().sum() # duplicates may point on the same wine, it can make our model unstable","fc861ccf":"wine = wine.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)","eb78a4fa":"wine.shape # 5320 rows and 13 columns","c90f4e1c":"count_cat = wine.nunique() # it shows us how much unique examples we have in each feature","5f7c1b5c":"count_cat","1f2bb37c":"wine[\"type\"].value_counts() # it let us understand the number of the examples we have in the dataset","9ab4308e":"# The distribution of the examples in our dataset\nwine[\"type\"].value_counts(\"white\")*100 # in %\nwine[\"type\"].value_counts(\"red\")*100 # in %","85981db7":"plt.figure(figsize = (10,5))\nsns.countplot(x = wine['type']); # we will plot it for better illustration","b412b767":"# Pie chart\nlabels = ['White', 'Red'] # every label represnts a type\npie_x = 0.753886 # the distribution\npie_y = 0.246114 # the distribution\nsizes = [pie_x,pie_y] # we have two types and their distribution\nexplode = (0, 0.15) # the distance between the pic slicers\ncol_pie = [\"silver\",\"red\"] # for the colors\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels,colors=col_pie, autopct='%1.1f%%',\n        shadow=True, startangle=93)\n# equal aspect ratio ensures that pie is drawn as a circle\nplt.title(\"Types division:\", size=20, color=\"blue\")\nax1.axis('equal')  \nplt.tight_layout()\nplt.show()","c3df8a52":"# another plot to show us more intersting details about the dataset\nplt.figure(figsize = (16,6))\nplt.title(\"The quality distribution of the wine according to it's type\", size=18, color='b')\nsns.countplot(wine['quality'], hue = wine['type']);","6813d42c":"wine[\"quality\"].value_counts() # the amount of wine we have from each quality number","e0572d30":"red_per_quality = wine.loc[wine[\"type\"]=='red'][\"quality\"].value_counts() # only the red wine, the sum is sorted by quality\nwhite_per_quality = wine.loc[wine[\"type\"]=='white'][\"quality\"].value_counts() # only white...\nquality_dist = pd.DataFrame({ # new df for our two new series\n        'red': red_per_quality,\n        'white':white_per_quality})\nquality_dist.fillna(value = 0, inplace=True) # if we dont have any data = we dont't have wine from that quality\n\nquality_dist","be15a8e5":"red_wine = wine.loc[wine[\"type\"]=='red'] # we chose only red wine\nred_wine_5 = red_wine.loc[red_wine[\"quality\"]==6] # we chose only quality 6\nred_wine_5","837ef32a":"df_types = wine.groupby('type') #by type\ndf_types","1e283405":"df_types.quality.mean() # the mean of the quality for each type","8fad99d4":"encoder = LabelEncoder()\nwine['type'] = encoder.fit_transform(wine['type'])\n# red = 0\n# white = 1","31fabc9f":"wine[\"type\"].value_counts()","2e643195":"wine","954b00ae":"#I want to know the correalation between each feature\nplt.figure(figsize=(20, 10))\nsns.heatmap(wine.corr(), square=True, annot=True, cmap=\"Blues\");","ced97f01":"wine.groupby('type')['total sulfur dioxide'].plot(title=\"Type correlation with total sulfur dioxide\", legend=True);\n# the most important thing here is to watch the difference between the red and white wine\n# we can also see that there are a few outliers ","72ae53fd":"wine.groupby('type')['free sulfur dioxide'].plot(title=\"Type correlation with free sulfur dioxide\", legend=True);","5b49d17b":"wine.groupby('type')['residual sugar'].plot(title=\"Type correlation with residual sugar\", legend=True);","d1e32338":"wine.describe() # more info about our dataset before the models part","387624c1":"features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\nss = StandardScaler()\ndf = wine\ndf[features] = ss.fit_transform(df[features])\ndf.head(3)","4d91c7da":"#df['type']=df['type'].map({'red':0,'white':1})\n#df","50cca219":"target=df['type'] # the feature we would like to predict, the type of the wine\ndata=df.drop(['type'], axis = 1) # we will drop y from x, because we want to predict it\nX_train,X_test,y_train,y_test = train_test_split(data,target,random_state=0) #train\\test split","7ba8857f":"X_train.shape # 3990 rows, 12 columns","ab62143e":"X_test.shape # 1330 rows, 12 columns","2c0f5f88":"y_train.shape # 3990 rows, one column","c3b60ec5":"y_test.shape # 1330 rows, one columns","ca1f236e":"# our options: 'most_frequent', 'stratified', 'uniform', 'constant', 'prior')\n# I chose the best dummy classifier from all those options (please, believe me)\n\ndm = DummyClassifier(strategy='most_frequent', random_state=0)\n# random_state = 0 assure the result will be the same every time we run the model (no random results)\ndm.fit(X_train,y_train) # training the model\ndm","7e0aa844":"y_pred = dm.predict(X_test) # predict the result","f4f57284":"print(classification_report(y_test,y_pred))","d478a9a2":"cm_dm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_dm,annot=True,annot_kws = {'size':15},fmt=\".0f\")\nplt.xlabel(\"Predict\")\nplt.ylabel(\"Actual\") #up and right = FP , down and right = FN","5ea6a019":"fpr, tpr, _= roc_curve(y_test, y_pred)\nauc = roc_auc_score(y_test, y_pred)\nplt.plot(fpr, tpr, label=\"auc=\"+str(auc), color='r')\nplt.box(False)\nplt.title('ROC CURVE Dummy Classifier')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid(True)\nplt.show()\n\ndummy_auc = round(auc,3)*100\n \nprint(f\"The score for the ROC Curve is: {round(auc,3)*100}%\")","ece30d8c":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn","a00bd372":"y_pred = knn.predict(X_test)","c330d36e":"print (classification_report(y_test, y_pred))","8e10bbd0":"k_range = list(range(3,51)) # we will test k values in range 3-50\nweight_op = ['uniform', 'distance'] # we will test the knn methods uniform & distance\nd = {'n_neighbors' :k_range, 'weights': weight_op}","0b2ad54d":"grid_temp = GridSearchCV(knn, d, cv=10, scoring='accuracy') # we chose model, d(range, methods), num of cv groups and scoring method\ngrid_temp.fit(data, target)\nprint(\"score:\",grid_temp.best_score_,\" params:\",grid_temp.best_params_)","2f9ebe23":"knn = KNeighborsClassifier(n_neighbors = 6, weights = 'distance')\nknn.fit(X_train,y_train)\nknn","39abb83d":"y_pred = knn.predict(X_test)\ny_pred","db2912dd":"print (classification_report(y_test,y_pred))","21af8842":"cm_knn = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_knn,annot=True,annot_kws = {'size':15},fmt=\".0f\")\nplt.xlabel(\"Predict\")\nplt.ylabel(\"Actual\") #up and right = FP , down and right = FN","b6b4f4f5":"fpr, tpr, _= roc_curve(y_test, y_pred)\nauc= roc_auc_score(y_test, y_pred)\nplt.plot(fpr, tpr, label=\"auc=\"+str(auc), color='g')\nplt.box(False)\nplt.title('ROC CURVE KNN')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid(True)\nplt.show()\n\nknn_auc = round(auc,3)*100\n\nprint(f\"The score for the ROC Curve is: {round(auc,3)*100}%\")","2b9eab5f":"dct = DecisionTreeClassifier(random_state=0) # gini and best are the default\ndct = dct.fit(X_train,y_train)\ndct","293e26a7":"y_pred = dct.predict(X_test)","e8dc5375":"print(classification_report(y_test,y_pred))","7bf6d9c7":"path = dct.cost_complexity_pruning_path(X_train,y_train)\nalphas = path.ccp_alphas\nimpurities = path.impurities\n# it returns the effective alphas and the corresponding total leaf impurities at each step of the pruning process\nalphas","721aa21a":"d = {'ccp_alpha':alphas}\nd","479da28f":"fig, ax = plt.subplots()\nax.plot(alphas[:-1], impurities[:-1], marker = 'o', drawstyle = \"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\");\n#  as alpha increases, more of the tree is pruned, which increases the total impurity of its leaves","fe3b3dd5":"grid_dct = GridSearchCV(dct, d, cv=10, scoring='accuracy')\ngrid_dct.fit(data, target)\nprint(\"score:\", grid_dct.best_score_, \" params:\", grid_dct.best_params_)","968aaf0d":"dct = DecisionTreeClassifier(ccp_alpha = 0.0004761904761904761)\ndct = dct.fit(X_train, y_train)\ndct","1ea7eae0":"y_pred = dct.predict(X_test)\ny_pred","b44a882f":"print(classification_report(y_test,y_pred))","052df256":"cm_dct = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_dct,annot=True,annot_kws = {'size':15},fmt=\".0f\")\nplt.xlabel(\"Predict\")\nplt.ylabel(\"Actual\") #up and right = FP , down and right = FN","2e82fbdc":"plt.figure(figsize=(15,7.5))\nplot_tree(dct,filled=True);","f665910c":"fpr, tpr, _= roc_curve(y_test, y_pred)\nauc= roc_auc_score(y_test, y_pred)\nplt.plot(fpr, tpr, label=\"auc=\"+str(auc), color='b')\nplt.box(False)\nplt.title('ROC CURVE Decision Tree')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid(True)\nplt.show()\n\ndct_auc = round(auc,3)*100\n \nprint(f\"The score for the ROC Curve is: {round(auc,3)*100}%\")","f9f55e0d":"lr = LogisticRegression(random_state=0)\nlr.fit(X_train, y_train)\npredictions = lr.predict(X_test)","c874593d":"print(classification_report(y_test,predictions))","c3b38010":"cm = confusion_matrix(y_test, predictions)\nsns.heatmap(cm,annot=True,annot_kws = {'size':15},fmt=\".0f\")\nplt.xlabel(\"Predict\")\nplt.ylabel(\"Actual\") #up and right = FP , down and right = FN","55b2df5f":"print(metrics.accuracy_score(y_test, predictions))","6dc3c49a":"fpr, tpr, _= roc_curve(y_test, predictions)\nauc= roc_auc_score(y_test, predictions)\nplt.plot(fpr, tpr, label=\"auc=\"+str(auc), color='orange')\nplt.box(False)\nplt.title('ROC CURVE Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid(True)\nplt.show()\n\nlr_auc = round(auc,3)*100\n \nprint(f\"The score for the ROC Curve is: {round(auc,3)*100}%\")","51020e9b":"print(\"The score for the models:\\n\")\nprint(\"Dummy Classifer:      \",dummy_auc,\"%\")\nprint(\"KNN:                  \",knn_auc,\"%\")\nprint(\"Decision Tree:        \",dct_auc,\"%\")\nprint(\"Logistic Regression:  \",lr_auc,\"%\")","38eba368":"At first, we had 6,497 rows. Now we have 5,320 rows.","e3d07e46":"## ROC Curve","7a44bdc6":"We have 1,177 duplicated rows, we can infer that it referrence to the same wine. Let's drop those rows for better results for our model.","98fd6338":"It seems that we have much more white wine than red wine. It will be usefull for our dummy classifier.","52cb8a80":"We would like to see the impact of total sulfur dioxide, free sulfur dioxide and residual sugar on the wine's type. Let's go back to the graphs.","04a22e0f":"# Logistic Regression","3aba0f75":"It let us understand how much \"categories\" we have for each column.","54f5156d":"As we can see, the grid search found the optimal alpha for us. We will use that alpha to improve our model results","df30d90e":"As we can see, the white wine that we have in the dataset, has a little bit better quality than the red wine.","3a59909a":"## ROC Curve","5744cc67":"Heatmap is a super important tool, because it tells us the bigger story.\nAs we can see, total sulfur dioxide has a huge effect on the type of the wine (0.7 of correalation). free sulfur dioxide takes the 2nd place with 0.47 of correalation. After it, the residual sugar with 0.35 of correalation.\nAnother intersting thing is the fact that the quality of the wine has a connection to the type of the wine. It will be intresting to find out which one of the types has the higher quality.","b870f2ad":"#### A graph showing the performance of an ambiguous classifier, due to the decision threshold set for it. The curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) under different acceptance thresholds.","a4f55c5f":"The most frequent strategy goes always after the majority. In our case, that every wine is white, because 75% of the wine is white.","e4c9726a":"#### A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. Criterion is the function to measure the quality of a split. Supported criteria are \u201cgini\u201d for the Gini impurity and \u201centropy\u201d for the information gain. Splitter is the strategy used to choose the split at each node. Supported strategies are \u201cbest\u201d to choose the best split and \u201crandom\u201d to choose the best random split.","e304f4ae":"We can also display this by a dataframe:","e356eabf":"# Conclusion","825a655c":"![image.png](attachment:image.png)","a4dfb511":"### Quick review:\n##### Link to the dataset: https:\/\/www.kaggle.com\/ahmettezcantekin\/beginner-datasets\nI Chose wine as my dataset. I would like to create a model which will predict the type of the wine(red\/white), according to it's features.\nAt first, I thought that this model might be a little stupid. We have eyes, we also have mouth. There is no problem to recognize the difference between white and red wine. However, a model has no senses. It can't taste the wine or see it color. The idea that we can use only data to predict at first the color of the wine, and after it to connect person to specific wine, is a big thing. This model may looks a little boring to you, but to me it is the first step of a long journey.\n\nFirstly, we will upload the dataset:","f858ae8c":"### Training - Testing split","a0c75a4e":"## ROC Curve","287653f5":"![image.png](attachment:image.png)","20b52b9e":"Let's take a look at the decision tree of our model:","89e47598":"# KNN - K-Nearest Neighbors","6ce69e44":"We can see that we have some quality for each wine. Let's take a look on it:","aed5833a":"### Cross validation:","c8c8fb81":"Explantion for the features:\n- **fixed acidity:** most acids involved with wine or fixed or nonvolatile (do not evaporate readily).\n- **volatile acidity:** the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste.\n- **citric acid:** found in small quantities, citric acid can add 'freshness' and flavor to wines.\n- **residual sugar:** the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram\/liter and wines with greater than 45 grams\/liter are considered sweet\n- **chlorides:** the amount of salt in the wine.\n- **free sulfur dioxide:** the free form of $SO_2$ exists in equilibrium between molecular $SO_2$ (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine.\n- **total sulfur dioxide:** amount of free and bound forms of $SO_2$; in low concentrations, $SO_2$ is mostly undetectable in wine, but at free $SO_2$ concentrations over 50 ppm, $SO_2$ becomes evident in the nose and taste of wine.\n- **density:** the density of water is close to that of water depending on the percent alcohol and sugar content.\n- **pH:** describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale.\n- **sulphates:** a wine additive which can contribute to sulfur dioxide gas $(SO_2)$ levels, wich acts as an antimicrobial and antioxidant.\n- **alcohol:** the percent alcohol content of the wine.\n- **quality:** score between 0 and 10.\n- **type:** red\/white.\n","733e94b3":"Let's try something else, we will groupby the wine into type groups to see which one has a greater quality:","4dd026a3":"# Dummy Classifier","9fd25555":"![image.png](attachment:image.png)","d17a24f2":"The default split is 75% train , 25% test.","3e035cc0":"## It can be concluded that KNN is the most accurate model for our dataset, with 98.7 percentages of success.","77b514be":"#### A statistical model that describes a possible relationship between a qualitative \/ categorical variable, known as the \"explained variable\", and other variables called \"explanatory variables\". The explanatory variables can be qualitative or quantitative. The model makes it possible to estimate the extent of the effect of a change in its value on each of the variables that explain the value of the explained variable. In other words, the model makes it possible to estimate correlations between the explanatory variables and the explained variable.","2a65911f":"Here's a reminder for a confusion matrix values:","57b3e365":"\u03b1 (alpha) is a tuning parameter that we finding using cross validation.","65edd4ce":"# Wine Type Classifier\n#####  @ Haim Goldfisher","1875176b":"## ROC Curve","6b295518":"#### Our model must be better than this model.","7501493f":"#### The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset.\n\n\n##### For illustration:","f3d0d80b":"#### The output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors","61781b7c":"# Decision Tree","95ff7b8a":"We can also take only the red wine which it's quality is 6:","d5e17caa":"As we can see, the grid search found the ideal k (6) and knn strategy (distance) for us. Let's use them for better performance for our model.","6bf56863":"![image.png](attachment:image.png)","2658f7b9":"# Scaling","38cf4513":"### Cross validation:","31f28f09":"![image.png](attachment:image.png)"}}