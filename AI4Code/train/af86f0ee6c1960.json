{"cell_type":{"89da388d":"code","2832482d":"code","3cc4af41":"code","0525a894":"code","21492f3b":"code","4eac1395":"code","734a0760":"code","666d975e":"code","e0ad0b51":"code","5e496e8b":"code","e19cbd67":"code","7d39b564":"code","27784a57":"code","9802b24e":"code","3d12c823":"code","6011b925":"code","e5f841b0":"code","38c59b6b":"code","7798815e":"code","b0290de2":"code","4c70650e":"markdown","a85e91ec":"markdown","b21aba11":"markdown","7230a0cf":"markdown","59bf2f9f":"markdown"},"source":{"89da388d":"# Import Dependencies\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport os\nimport random\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport transformers\n\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.simplefilter('ignore')","2832482d":"# Settings\nconfig = {\n    'train_batch_size': 16,\n    'valid_batch_size': 32,\n    'max_len': 314,\n    'nfolds': 4,\n    'seed': 42,\n    'epochs': 10,\n}\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'{device} is used')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.detarministic = True \n    torch.backends.cudnn.benchmark = True \n\nseed_everything(seed=config['seed'])\n\ndef rmse_score(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","3cc4af41":"# Load the data\ntrain_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","0525a894":"# k-fold\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:, 'bins'] = pd.cut(train_data['target'], bins=num_bins, labels=False)\n\ntrain_data['fold'] = -1\nkfold = StratifiedKFold(n_splits=config['nfolds'],\n                        shuffle=True,\n                        random_state=config['seed'])\nfor k, (train_idx, valid_idx) in enumerate(kfold.split(X=train_data, y=train_data.bins)):\n    train_data.loc[valid_idx, 'fold'] = k\n","21492f3b":"# Dataset, DataLoader\nclass CLRPDataset(Dataset):\n    def __init__(self, df, tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.target = df['target'].to_numpy()\n        self.tokenizer = tokenizer \n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, idx):\n        sentence = self.excerpt[idx]\n        sentence = sentence.replace('\\n', ' ')\n        bert_sens = tokenizer.encode_plus(sentence,\n                                          add_special_tokens=True,\n                                          max_length=config['max_len'],\n                                          pad_to_max_length=True,\n                                          truncation=True,\n                                          return_attention_mask=True)\n        ids = torch.tensor(bert_sens['input_ids'])\n        mask = torch.tensor(bert_sens['attention_mask'])\n        targets = torch.tensor(self.target[idx], dtype=torch.float)\n        return {'ids': ids, 'mask': mask, 'targets': targets}\n        ","4eac1395":"model_path = '..\/input\/roberta-base'\n\ntokenizer = transformers.RobertaTokenizer.from_pretrained(model_path)\n\np_fold = 0\np_train = train_data.query(f'fold != {p_fold}').reset_index(drop=True)\np_valid = train_data.query(f'fold == {p_fold}').reset_index(drop=True)\n\ntrain_dataset = CLRPDataset(p_train, tokenizer)\nvalid_dataset = CLRPDataset(p_valid, tokenizer)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=config['train_batch_size'],\n                              shuffle=True, num_workers=4, pin_memory=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=config['valid_batch_size'],\n                              shuffle=False, num_workers=4, pin_memory=True)","734a0760":"class RobertaCLRPClassificationHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n\n        self.dense = nn.Linear(in_features*4, hidden_dim)\n        self.dropout = nn.Dropout(0.1, inplace=False)\n        self.out_proj = nn.Linear(hidden_dim, 1)\n\n        nn.init.normal_(self.dense.weight, std=0.02)\n        nn.init.normal_(self.dense.bias, 0)\n        nn.init.normal_(self.out_proj.weight, std=0.02)\n        nn.init.normal_(self.out_proj.bias, 0)\n\n    def forward(self, x):\n        x = self.dense(x)\n        x = self.dropout(x)\n        x = self.out_proj(x)\n\n        return x\n\n\nclass CommonLitModel(nn.Module):\n\n    def __init__(self, model_path):\n        super(CommonLitModel, self).__init__()\n        self.roberta = transformers.RobertaModel.from_pretrained(model_path)        \n        self.classifier = RobertaCLRPClassificationHead(768, 768)\n        \n    def forward(self, ids, mask):\n        all_encoder_layers = self.roberta(ids, mask, output_hidden_states=True)['hidden_states']\n        \n        vec1 = all_encoder_layers[-1][:, 0, :]\n        vec2 = all_encoder_layers[-2][:, 0, :]\n        vec3 = all_encoder_layers[-3][:, 0, :]\n        vec4 = all_encoder_layers[-4][:, 0, :]\n\n        roberta_output = torch.cat([vec1, vec2, vec3, vec4], dim=1)\n        output = self.classifier(roberta_output)\n \n        return output   # torch.Size([16, 1])\n\n\nmodel = CommonLitModel(model_path)\nmodel.to(device)\n","666d975e":"# freezing parameters\n\nfor param in model.parameters():\n    param.requires_grad = False\n    \nfor param in model.classifier.parameters():\n    param.requires_gard = True\n    \nfor param in model.roberta.encoder.layer[11].parameters():\n    param.requires_grad = True\n    \nfor param in model.roberta.encoder.layer[10].parameters():\n    param.requires_grad = True\n    \nfor param in model.roberta.encoder.layer[9].parameters():\n    param.requires_grad = True\n    \nfor param in model.roberta.encoder.layer[8].parameters():\n    param.requires_grad = True\n","e0ad0b51":"# optimizer, scheduler and criterion\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\noptimizer = AdamW([\n    {'params': model.classifier.parameters(), 'lr': 2e-5},\n    {'params': model.roberta.encoder.layer[11].parameters(), 'lr': 1e-5},\n    {'params': model.roberta.encoder.layer[10].parameters(), 'lr': 4e-6},\n    {'params': model.roberta.encoder.layer[9].parameters(), 'lr': 2e-6},\n    {'params': model.roberta.encoder.layer[8].parameters(), 'lr': 1e-6},\n], betas=(0.9, 0.98), weight_decay=1e-2)\n\nepochs = config['epochs']\n\ntrain_steps = int(len(p_train) \/ config['train_batch_size'] * epochs)\nnum_steps = int(train_steps * 0.5)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n\nscaler = torch.cuda.amp.GradScaler()\n\ncriterion = nn.MSELoss()\n","5e496e8b":"scaler = torch.cuda.amp.GradScaler()\nepochs = config['epochs']\n\ndef training(train_dataloader, model, optimizer, scheduler=None):\n\n    model.train()\n    \n    all_preds = []\n    all_targets = []\n    losses = []\n\n    for a in train_dataloader:\n\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast():\n\n            ids = a['ids'].to(device, non_blocking=True)\n            mask = a['mask'].to(device, non_blocking=True)\n\n            output = model(ids, mask)\n            output = output.squeeze(-1)\n            \n            target = a['targets'].to(device, non_blocking=True)\n\n            loss = criterion(output, target)\n\n            losses.append(loss.item())\n            all_preds.append(output.detach().cpu().numpy())\n            all_targets.append(target.detach().squeeze(-1).cpu().numpy())\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        del loss\n        torch.cuda.empty_cache()\n\n        if scheduler:\n            scheduler.step()\n\n    all_preds = np.concatenate(all_preds)\n    all_targets = np.concatenate(all_targets)\n    losses = np.mean(losses)\n    train_score = rmse_score(all_targets, all_preds)\n\n    return losses, train_score","e19cbd67":"def validating(valid_dataloader, model):\n\n    model.eval()\n\n    all_preds = []\n    all_targets = []\n    losses = []\n\n    for b in valid_dataloader:\n\n        with torch.no_grad():\n\n            ids = b['ids'].to(device, non_blocking=True)\n            mask = b['mask'].to(device, non_blocking=True)\n\n            output = model(ids, mask)\n            output = output.squeeze(-1)\n\n            target = b['targets'].to(device, non_blocking=True)\n\n            loss = criterion(output, target)\n            losses.append(loss.item())\n            all_preds.append(output.detach().cpu().numpy())\n            all_targets.append(target.detach().squeeze(-1).cpu().numpy())\n            \n            del loss\n            torch.cuda.empty_cache()\n\n    all_preds = np.concatenate(all_preds)\n    all_targets = np.concatenate(all_targets)\n\n    losses = np.mean(losses)\n    valid_score = rmse_score(all_targets, all_preds)\n    \n    return all_preds, all_targets, losses, valid_score\n","7d39b564":"# training\ntrain_losses = []\nvalid_losses = []\nbest_score = None \n\ntrain_scores = []\nvalid_scores = []\n\nfor epoch in tqdm(range(epochs)):\n    print(\"---------------\" + str(epoch) + \"start-------------\\n\")\n\n    train_loss, train_score = training(train_dataloader, model, optimizer, scheduler)\n    train_losses.append(train_loss)\n    train_scores.append(train_score)\n    print(f'train_score is {train_score}\\n')\n\n    preds, targets, valid_loss, valid_score = validating(valid_dataloader, model)\n    valid_losses.append(valid_loss)\n    valid_scores.append(valid_score)\n    print(f'valid_score is {valid_score}\\n')\n\n    if best_score is None:\n        best_score = valid_score\n        torch.save(model.state_dict(), 'model0.pth')\n        print('Save the first model')\n\n    elif best_score > valid_score:\n        best_score = valid_score\n        torch.save(model.state_dict(), 'model0.pth')        \n        print('found better point')\n\n    else:\n        pass\n        ","27784a57":"# visualization of results\nfig = plt.figure(figsize=(6, 10))\nax1 = fig.add_subplot(2, 1, 1)\nax2 = fig.add_subplot(2, 1, 2)\nplt.subplots_adjust(wspace=1.0)\n\nax1.scatter(targets, preds)\nax1.plot([-4, 2], [-4, 2])\nax1.set_xlabel('targets')\nax1.set_ylabel('preds')\n\nx = np.arange(epochs)\nax2.plot(x, train_losses, label='train_losses')\nax2.plot(x, valid_losses, label='valid_losses')\nax2.legend()\nax2.set_xlabel('epochs')\nax2.set_ylabel('losses')","9802b24e":"# remaining k-fold\n\nbest_scores = []\nbest_scores.append(best_score)\n\nfor p_fold in range(1, config['nfolds']):\n    # initializing the data\n    p_train = train_data.query(f'fold != {p_fold}').reset_index(drop=True)\n    p_valid = train_data.query(f'fold == {p_fold}').reset_index(drop=True)\n\n    train_dataset = CLRPDataset(p_train, tokenizer)\n    valid_dataset = CLRPDataset(p_valid, tokenizer)\n    train_dataloader = DataLoader(train_dataset, batch_size=config['train_batch_size'],\n                                  shuffle=True, num_workers=4, pin_memory=True)\n    valid_dataloader = DataLoader(valid_dataset, batch_size=config['valid_batch_size'],\n                                  shuffle=False, num_workers=4, pin_memory=True)\n    \n    model = CommonLitModel(model_path)\n    model.to(device)\n\n    for name, param in model.named_parameters():\n        param.requires_grad = False\n    for param in model.classifier.parameters():\n        param.requires_gard = True\n    for name, param in model.roberta.encoder.layer[11].named_parameters():\n        param.requires_grad = True\n    for name, param in model.roberta.encoder.layer[10].named_parameters():\n        param.requires_grad = True\n    for name, param in model.roberta.encoder.layer[9].named_parameters():\n        param.requires_grad = True\n    for name, param in model.roberta.encoder.layer[8].named_parameters():\n        param.requires_grad = True\n    \n    # optimizer, scheduler and criterion\n    from transformers import AdamW\n    from transformers import get_linear_schedule_with_warmup\n    \n    optimizer = AdamW([\n                {'params': model.classifier.parameters(), 'lr': 2e-5},\n                {'params': model.roberta.encoder.layer[11].parameters(), 'lr': 1e-5},\n                {'params': model.roberta.encoder.layer[10].parameters(), 'lr': 4e-6},\n                {'params': model.roberta.encoder.layer[9].parameters(), 'lr': 2e-6},\n                {'params': model.roberta.encoder.layer[8].parameters(), 'lr': 1e-6},\n                ], betas=(0.9, 0.98), weight_decay=1e-2)\n    \n    epochs = 10\n    train_steps = int(len(p_train) \/ config['train_batch_size'] * epochs)\n    num_steps = int(train_steps * 0.5)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n    \n    scaler = torch.cuda.amp.GradScaler()\n    criterion = nn.MSELoss()\n\n    # training\n    train_losses = []\n    valid_losses = []\n    best_score = None\n\n    train_scores = []\n    valid_scores = []\n\n    for epoch in tqdm(range(epochs)):\n        print(\"---------------\" + str(epoch) + \"start-------------\\n\")\n\n        train_loss, train_score = training(train_dataloader, model, optimizer, scheduler)\n        train_losses.append(train_loss)\n        train_scores.append(train_score)\n        print(f'train_score is {train_score}\\n')\n\n        preds, targets, valid_loss, valid_score = validating(valid_dataloader, model)\n        valid_losses.append(valid_loss)\n        valid_scores.append(valid_score)\n        print(f'valid_score is {valid_score}\\n')\n\n        if best_score is None:\n            best_score = valid_score\n            torch.save(model.state_dict(), f'model{p_fold}.pth')\n            print('Save the first model')\n\n        elif best_score > valid_score:\n            best_score = valid_score\n            torch.save(model.state_dict(), f'model{p_fold}.pth')\n            print('found better point')\n\n        else:\n            pass\n\n    best_scores.append(best_score)\n","3d12c823":"model_path = '..\/input\/roberta-base'\ntokenizer = transformers.RobertaTokenizer.from_pretrained(model_path)\n\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","6011b925":"# Dataset and DataLoader for inference\nclass CLRPInferenceDataset(Dataset):\n\n    def __init__(self, df, tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, idx):\n        sentence = self.excerpt[idx]\n        sentence = sentence.replace('\\n', ' ')\n        roberta_sens = tokenizer.encode_plus(sentence,\n                                            add_special_tokens=True,\n                                            max_length=config['max_len'],\n                                            pad_to_max_length=True,\n                                            truncation=True,\n                                            return_attention_mask=True)\n        ids = torch.tensor(roberta_sens['input_ids'])\n        mask = torch.tensor(roberta_sens['attention_mask'])\n        return {'ids': ids, 'mask': mask}\n\ntest_dataset = CLRPInferenceDataset(test_data, tokenizer)\ntest_dataloader = DataLoader(test_dataset, batch_size=config['valid_batch_size'],\n                             shuffle=False, num_workers=4, pin_memory=True)\n","e5f841b0":"model = CommonLitModel(model_path)\n\nfinetune_result_path = '.\/'\nmodel_names = [s for s in os.listdir(finetune_result_path) if '.pth' in s]\npthes = [os.path.join(finetune_result_path, s) for s in os.listdir(finetune_result_path) if '.pth' in s]\n\ndef clrp_inference(test_dataloader, model, model_names, pthes):\n    all_preds = []\n    all_models = []\n    for model_name, state in zip(model_names, pthes):\n        model.load_state_dict(torch.load(state))\n        model.to(device)\n        model.eval()\n\n        preds = []\n        all_valid_loss = 0\n\n        with torch.no_grad():\n            for a in test_dataloader:\n                ids = a['ids'].to(device)\n                mask = a['mask'].to(device)\n\n                output = model(ids, mask)\n                output = output.squeeze(-1)\n\n                preds.append(output.cpu().numpy())\n\n            preds = np.concatenate(preds)\n            all_preds.append(preds)\n            all_models.append(model_name)\n\n    print('\\npredicted!')\n    return all_preds, all_models\n\nall_preds, all_models = clrp_inference(test_dataloader, model, model_names, pthes)","38c59b6b":"preds_df = pd.DataFrame(all_preds).T\npreds_df.columns = all_models\n\npreds_df\n","7798815e":"fin_preds = preds_df.mean(axis=1)\nsample['target'] = fin_preds\nsample\n","b0290de2":"sample.to_csv('submission.csv', index=False)\n","4c70650e":"## Inference","a85e91ec":"## Model","b21aba11":"**\u3010Updated Points\u3011**\n\n\u30fbI attached a classification module to RobertaModel.\n\n\u30fbI used the RobertaModel outputs of [the last 4 encoding layers' CLS tokens][1].\n\n\u30fbI adjusted learning rates, layer by layer.\n\n---\n\n**My previous note**\n\n[CLRP: RoBERTa simple finetune baseline-1][2]\n\n---\n\n**Comments**: Thanks to previous great Notebooks.\n\n1. [Pytorch BERT beginner's room][3]\n\n2. [CLRP: Pytorch Roberta Finetune][4]\n\n\n\n[1]: https:\/\/www.kaggle.com\/c\/google-quest-challenge\/discussion\/123770\n\n[2]: https:\/\/www.kaggle.com\/masatomurakawamm\/clrp-roberta-simple-finetune-baseline-1\n\n[3]: https:\/\/www.kaggle.com\/chumajin\/pytorch-bert-beginner-s-room\n\n[4]: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune","7230a0cf":"## Training","59bf2f9f":"## Datasets and DataLoaders"}}