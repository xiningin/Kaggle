{"cell_type":{"998fb1ef":"code","90c3ee1e":"code","b7d17f40":"code","c4bf608e":"code","673fdb43":"code","30f4cc35":"code","27978653":"code","dab1b4fa":"code","ffa7274e":"code","d7d73ca2":"code","7b858631":"code","612fe7fb":"code","f67c8b99":"code","6ac6564b":"code","4b8b0086":"code","a1ff19f6":"code","e4de1ab8":"code","8443e86c":"code","74ba8730":"code","8da36149":"code","ffc529bd":"code","266aed8f":"code","0e6377cf":"code","b276f0e3":"code","20fddf77":"code","f423bd59":"code","ee9a8028":"code","7de1e65e":"code","fa6acffc":"code","1e275f64":"code","d1a0f29b":"code","a70a66bb":"code","069874bb":"code","e7a894ab":"code","8b05b3c5":"code","7e150183":"code","2c16d534":"code","5735cb92":"code","ddc79915":"code","400456ac":"code","4cf6ffda":"code","d85c1169":"code","3902cd22":"code","4a3990ee":"code","0a174c1f":"code","0bc97bfb":"code","b8166489":"code","49af6794":"code","5227598a":"code","091c17d2":"code","371c1833":"code","42549860":"code","dab67c41":"code","53eb9688":"code","c5ffab7e":"code","ae0739c7":"code","f59821e7":"code","7021503d":"markdown","40e21f33":"markdown","0532e87b":"markdown","607488d1":"markdown","ceb82660":"markdown","7099cca7":"markdown","f89a0490":"markdown","2da6f594":"markdown","36293589":"markdown","47dd125b":"markdown","c5913cdd":"markdown","7865ec80":"markdown","613be15d":"markdown","b8a6e76a":"markdown","f8c063ef":"markdown","744bdfb4":"markdown","6c542b3e":"markdown","41870e17":"markdown","3512fa4a":"markdown","8b7ce10a":"markdown","5ce02c54":"markdown","9495fb93":"markdown","fdd4200c":"markdown","0023ffec":"markdown","b02e97d5":"markdown","5addbc87":"markdown","3e686207":"markdown","a4d934d4":"markdown","89984ca3":"markdown","50e183d2":"markdown","869368ec":"markdown","2d2a98ec":"markdown","4f602bf6":"markdown","a793741a":"markdown","db7f1646":"markdown","002004c6":"markdown","4c24ef11":"markdown","96db9fc9":"markdown","f42b29bc":"markdown","edce729a":"markdown","b1403f7b":"markdown","4ad1fb77":"markdown","880bd6a1":"markdown","fad97dbe":"markdown","9ca2dc8c":"markdown","b099b8da":"markdown","09603c77":"markdown","35317872":"markdown","aed5f132":"markdown","809fbddd":"markdown","f6c12c64":"markdown","7994904c":"markdown","1199d577":"markdown","2020aae4":"markdown","09a6a218":"markdown","89979d44":"markdown","2f444fd6":"markdown","6d8d8c98":"markdown","7200a0c0":"markdown","f3d7f673":"markdown","992d7db9":"markdown"},"source":{"998fb1ef":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","90c3ee1e":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain = train_data.copy()\ntest = test_data.copy()","b7d17f40":"train.head(5)","c4bf608e":"train.drop(['PassengerId'],axis=1,inplace=True)\ntest.drop(['PassengerId'],axis=1,inplace=True)\npred = train_data['Survived']","673fdb43":"train.head(5)\n","30f4cc35":"train.isnull().sum()","27978653":"test.isnull().sum()","dab1b4fa":"plt.figure(figsize=(8, 5))\nsns.set_style('whitegrid')\nsns.countplot(x = 'Sex',hue='Survived',data=train_data,palette='rainbow')","ffa7274e":"sex1 = pd.get_dummies(train['Sex'])\nsex2 = pd.get_dummies(test['Sex'])\n\ntrain.drop(['Sex'],axis=1,inplace=True)\ntest.drop(['Sex'],axis=1,inplace=True)\n\ntrain = pd.concat([train,sex1],axis=1)\ntest = pd.concat([test,sex2],axis=1)","d7d73ca2":"train.drop(['female'],axis=1,inplace=True) \ntest.drop(['female'],axis=1,inplace=True) ","7b858631":"plt.figure(figsize=(10, 9))\nsns.boxplot(x='Pclass',y='Age',data=train_data,palette='rainbow')\n","612fe7fb":"plt.figure(figsize=(8, 5))\nsns.set_style('whitegrid')\nsns.countplot(x = 'Pclass',hue='Survived',data=train_data,palette='deep')","f67c8b99":"plt.figure(figsize=(16, 5))\nfor x in [1,2,3]:    ## for 3 classes\n    sns.kdeplot(data=train_data.Age[train_data.Pclass == x],cut = 0, clip=(0,200)).grid(False)\n    \nplt.title(\"Age vs Pclass\")\nplt.legend((\"1st\",\"2nd\",\"3rd\"))\nplt.xlabel(\"Age\")\nplt.ylabel(\"Density\")","6ac6564b":"train['Age'].describe()","4b8b0086":"train[\"Age\"].fillna(train['Age'].describe().loc[['50%']][0], inplace = True) \ntest[\"Age\"].fillna(test['Age'].describe().loc[['50%']][0], inplace = True) ","a1ff19f6":"f, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(train[\"Fare\"], color=\"orange\",ax = axes)\nplt.title(\"Fare distribution for all the people\")","e4de1ab8":"Fare_0 = []\nFare_1 = []\nfor i in range(0,891):\n    if train_data[\"Survived\"][i] == 0:\n        Fare_0.append(train[\"Fare\"][i])\n    else:\n        Fare_1.append(train[\"Fare\"][i])","8443e86c":"f, axes = plt.subplots(1,1, figsize = (16, 3))\ng1 = sns.distplot(Fare_0, color=\"red\",ax = axes)\nplt.title(\"Fare distribution for the people who did not survive\")\n\nf, axes = plt.subplots(1,1, figsize = (16, 3))\ng1 = sns.distplot(Fare_1, color=\"blue\",ax = axes)\nplt.title(\"Fare distribution for the people who survived\")\n\nplt.show()","74ba8730":"test[\"Fare\"].fillna(test['Fare'].describe().loc[['50%']][0], inplace = True) ","8da36149":"plt.figure(figsize=(8, 5))\nsns.set_style('whitegrid')\nsns.countplot(x = 'Embarked',hue='Survived',data=train_data, palette = \"Set2\" )","ffc529bd":"train[\"Embarked\"].fillna(\"S\", inplace = True) \ntest[\"Embarked\"].fillna(\"S\", inplace = True) ","266aed8f":"embark1 = pd.get_dummies(train['Embarked'])\nembark2 = pd.get_dummies(test['Embarked'])\n\ntrain.drop(['Embarked'],axis=1,inplace=True)\ntest.drop(['Embarked'],axis=1,inplace=True)\n\ntrain = pd.concat([train,embark1],axis=1)\ntest = pd.concat([test,embark2],axis=1)","0e6377cf":"plt.figure(figsize=(14, 6))\nax = sns.countplot(y=\"Survived\", hue=\"SibSp\", data=train ,color = \"Orange\" )\n\nplt.figure(figsize=(14, 6))\nax = sns.countplot(y=\"Survived\", hue=\"Parch\", data=train , color = \"Green\" )\n\nplt.show()","b276f0e3":"def fam(x):\n    if  (x['SibSp'] + x['Parch'])  > 0:\n        return 1\n    else:\n        return 0\n\ntrain['Family'] = train.apply(fam, axis = 1)\ntest['Family'] = test.apply(fam, axis = 1)","20fddf77":"train = train.drop(['SibSp','Parch'],axis=1)\ntest = test.drop(['SibSp','Parch'],axis=1)","f423bd59":"plt.figure(figsize=(6, 6))\nsns.set_style('whitegrid')\nsns.countplot(x = 'Family',hue='Survived',data=train, palette=\"YlOrBr\" )\nplt.legend((\"Not Survived\",\"Survived\"))","ee9a8028":"train[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train['Cabin'] ])\ntest[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in test['Cabin'] ])","7de1e65e":"plt.figure(figsize=(8, 5))\ng = sns.catplot(y=\"Survived\",x=\"Cabin\",data=train,kind=\"bar\",order=['A','B','C','D','E','F','G','X'])\n","fa6acffc":"train[\"Cabin\"] = train[\"Cabin\"].map({\"X\":0, \"A\":1, \"B\" : 2 , \"C\":3, \"D\":4, \"E\":5, \"F\":6, \"G\":7,\"T\":0})\ntrain[\"Cabin\"] = train[\"Cabin\"].astype(int)\ntest[\"Cabin\"] = test[\"Cabin\"].map({\"X\":0, \"A\":1, \"B\" : 2 , \"C\":3, \"D\":4, \"E\":5, \"F\":6, \"G\":7,\"T\":0})\ntest[\"Cabin\"] = test[\"Cabin\"].astype(int)","1e275f64":"train_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in train[\"Name\"]]\ntrain[\"Title\"] = pd.Series(train_title)\ntest_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in test[\"Name\"]]\ntest[\"Title\"] = pd.Series(test_title)","d1a0f29b":"train = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)","a70a66bb":"plt.figure(figsize=(14, 6))\ng = sns.countplot(x=\"Title\",data=train)\ng = plt.setp(g.get_xticklabels(), rotation=45) ","069874bb":"train[\"Title\"] = train[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain[\"Title\"] = train[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ntrain[\"Title\"] = train[\"Title\"].astype(int)\ntest[\"Title\"] = test[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest[\"Title\"] = test[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ntest[\"Title\"] = test[\"Title\"].astype(int)","e7a894ab":"Ticket1 = []\nfor i in list(train.Ticket):\n    if not i.isdigit() :\n        Ticket1.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket1.append(\"X\")\ntrain[\"Ticket\"] = Ticket1\n\nTicket2 = []\nfor j in list(test.Ticket):\n    if not j.isdigit() :\n        Ticket2.append(j.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket2.append(\"X\")\ntest[\"Ticket\"] = Ticket2","8b05b3c5":"train[\"Ticket\"].unique()","7e150183":"test[\"Ticket\"].unique()","2c16d534":" np.union1d(train[\"Ticket\"], test[\"Ticket\"])","5735cb92":"train= pd.get_dummies(train, columns = [\"Ticket\"], prefix=\"T\")\ntest = pd.get_dummies(test, columns = [\"Ticket\"], prefix=\"T\")","ddc79915":"train = train.drop(['T_SP','T_SOP','T_Fa','T_LINE','T_SWPP','T_SCOW','T_PPP','T_AS','T_CASOTON'],axis = 1)\ntest = test.drop(['T_SCA3','T_STONOQ','T_AQ4','T_A','T_LP','T_AQ3'],axis = 1)","400456ac":"train.drop(['Survived'],axis=1,inplace=True)","4cf6ffda":"train.head(5)","d85c1169":"print(train.isnull().sum())\nprint(\"Number of columns are :\",train.isnull().sum().count())","3902cd22":"print(test.isnull().sum())\nprint(\"Number of columns are :\",test.isnull().sum().count())","4a3990ee":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ntrain2 = sc.fit_transform(train)\ntest2 = sc.transform(test)","0a174c1f":"from sklearn.pipeline import Pipeline\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, cross_val_score\n\nKFold_Score = pd.DataFrame()\nclassifiers = ['Linear SVM', 'Radial SVM', 'LogisticRegression', \n               'RandomForestClassifier', 'AdaBoostClassifier', \n               'XGBoostClassifier', 'KNeighborsClassifier','GradientBoostingClassifier']\nmodels = [svm.SVC(kernel='linear'),\n          svm.SVC(kernel='rbf'),\n          LogisticRegression(max_iter = 1000),\n          RandomForestClassifier(n_estimators=200, random_state=0),\n          AdaBoostClassifier(random_state = 0),\n          xgb.XGBClassifier(n_estimators=100),\n          KNeighborsClassifier(),\n          GradientBoostingClassifier(random_state=0)\n         ]\nj = 0\nfor i in models:\n    model = i\n    cv = KFold(n_splits=5, random_state=0, shuffle=True)\n    KFold_Score[classifiers[j]] = (cross_val_score(model, train, np.ravel(pred), scoring = 'accuracy', cv=cv))\n    j = j+1","0bc97bfb":"mean = pd.DataFrame(KFold_Score.mean(), index= classifiers)\nKFold_Score = pd.concat([KFold_Score,mean.T])\nKFold_Score.index=['Fold 1','Fold 2','Fold 3','Fold 4','Fold 5','Mean']\nKFold_Score.T.sort_values(by=['Mean'], ascending = False)","b8166489":"col_name1 = list(train.columns)\ncol_name2 = list(test.columns)","49af6794":"col_name1[0],col_name1[2] = col_name1[2],col_name1[0]\ncol_name2[0],col_name2[2] = col_name2[2],col_name2[0]","5227598a":"train_new = train[col_name1]\ntest_new = test[col_name2]","091c17d2":"train_new = train_new.drop(['Cabin'],axis = 1)\ntest_new = test_new.drop(['Cabin'],axis = 1)","371c1833":"sc = StandardScaler()\ntrain3 = sc.fit_transform(train_new)\ntest3 = sc.transform(test_new)","42549860":"rfc = RandomForestClassifier(random_state=0)","dab67c41":"param_grid = { \n    'n_estimators': [ 200,300],\n    'max_features': ['auto', 'sqrt'],\n    'max_depth' : [6,7,8],\n    'criterion' :['gini', 'entropy']\n}","53eb9688":"from sklearn.model_selection import GridSearchCV\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\nCV_rfc.fit(train3,pred )\nCV_rfc.best_params_","c5ffab7e":"rfc1=RandomForestClassifier(random_state=0, n_estimators= 200, criterion = 'gini',max_features = 'auto',max_depth = 8)\nrfc1.fit(train3, pred)","ae0739c7":"pred3= rfc1.predict(test3)\nprint(pred3)","f59821e7":"pred_test = pred3\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': pred_test})\noutput.to_csv('.\/submission.csv', index=False)","7021503d":"### This is our final training set after preprocessing.","40e21f33":"### In this section we will look at all the features in our dataset. There will be visualization of data using different graphs and preprocessing on the training and test set.\n\n### Note: Similar preprocessing will be done on training and test set. ","0532e87b":"#### Here we will be taking the prefix values of the cabin number. The missing values will be replaced with 'X'.","607488d1":"### This column has the ticket number of all the passengers. Here we will be taking the ticket prefix.","ceb82660":"### These above tickets are common in both the sets.","7099cca7":"## Feature Scaling with Standardization\n  ","f89a0490":"# Hyperparameter Tuning ","2da6f594":"### Applying standarization as part of our data normalization.","36293589":"#### We will be splitting the title from the name.","47dd125b":"## 3. Age","c5913cdd":"## Final Note:\n### I have experimented a lot with this project and tried many different things.I also used Neural Netowrks and Ensembling models to get a better prediction result. But most of my efforts have been in Hyperparamter tuning. Initially I had included more parameters thinking that it would give me better result. But if you do a lot of hyperparamter tuning the model will not give a good result on the test set as it will be fine-tuned for the training set. \n### So try your best in getting a better prediction and let me know what gave you a better result :)","7865ec80":"### This column holds the embarkation records for all the passengers.\n#### They stand for:\n* S - Southampton\n* C - Cherbourg\n* Q - Queenstown","613be15d":"#### Here we have our titles mapped with numeric values.","b8a6e76a":"# Features","f8c063ef":"### Note: These 2 columns were not giving any valuable information\/trend that could have helped in getting accurate prediction, hence they were combined.","744bdfb4":"### This is the list of our missing values from the training and test set.","6c542b3e":"### Filling the missing values with 'S' as it is the most frequently occuring value.","41870e17":"### \"Fare\" column tells us about the amount of money paid by the passengers. Here we can see that passengers had a greater probability of surviving if they had payed more.","3512fa4a":"## Important Note:\n### I had done this project 3 months before creating this notebook. After completing it I experimented with 'Hyperparamter Tuning' as well as 'Principle Component Analysis' to get a better score. When I finally got my best result I decided to make a notebook on kaggle. While creating this project I had made a few changes in the order of preprocessing, due to which the \"Fare\" column and \"Pclass\" column had their positions interchanged. I did not realise it before but on training the model with the same hyperparamters I was getting a lower accuracy. I was suprised by the fact that even with the columns interchanging we get a different prediction result. Hence inorder to overcome this problem I interchanged the 2 columns.   ","8b7ce10a":"### In this project I tried to make the best model for **Survival Prediction**.I experimented with various different ways of preprocessing ,  filling missing values , model training and hyperparameter tuning.\n### I will be presenting all the things that I did in this project along with the changes I made to get a better prediction result which got me to **top 4%**.\n### Throughout the project there will be some important **notes**.","5ce02c54":"### Here RandomForestClassifier is giving the highest accuracy.","9495fb93":"### In this dataset we have a \"Name\" column mentioning the name of every passenger. These names also have a title along with them which can be useful.","fdd4200c":"### The missing values have to be filled with the median value.","0023ffec":"### Applying RandomForestClassifier with hyperparameter tuning on our new training set.","b02e97d5":"### Here we can see that a passenger having no family had a lesser chance of survival.","5addbc87":"### Using K-Folds Cross validation to evaluate the performance of our models","3e686207":"### Out of these all values, '**50%**' gives us the median value.","a4d934d4":"### This is a picture of the Titanic which has mapped various locations such as dining room ,quarters ,etc. of the ship. In an actual sinking emergency (just like the one here) all the passengers irrespective of their cabin would gather around at the port side and starboard side of the ship for evacuation. Similar thing must have happened with Titanic. Therefore it does not matter which cabin you are occupying.\n### Women and children had the first preference, along with them there were people who belonged to the higher class. This data is covered under the \"Sex\" , \"Pclass\" and \"Age\" column. ","89984ca3":"#### This result got me **Top 4%**.","50e183d2":"### In order to maintain the same number of columns I had to tamper the test set and had to remove these unique tickets. Same was done with training set.\n\n### These are the following unique tickets which are dropped.","869368ec":"### Here  \".get_dummies()\"  will convert this column and make 2 dummy columns of male and female. This is done in order to convert the categorical data into numerical.","2d2a98ec":"## 8. Name & Titles","4f602bf6":"### Here we will be combining the SibSp and Parch column into one column and determining whether the passenger has a family or not.","a793741a":"### We can now proceed further as there are no missing values in the training and test set.","db7f1646":"### We will be dropping 1 column as we get all the necessary information from the other one.","002004c6":"## Missing Values","4c24ef11":"# Model Training ","96db9fc9":"### Here we can see that people from higher class(**1 being the highest**) had a better chance of survival.","f42b29bc":"### Here is a tricky part. The training set and test set have a few tickets which are unique to themselves.","edce729a":"## Checking for missing values for the new dataframes","b1403f7b":"### Note: I have performed numerous permutations with various hyperparameters but the given following are the ones which gave me the best result.","4ad1fb77":"## Important Note:\n### As I had mentioned earlier in feature preprocessing, the \"Cabin\" column does not help us in getting a better prediction and here is why:-\n### First of all we need to understand that this is a \"Survival Prediction\". Many things took place at the actually event which we don't have an account of. ","880bd6a1":"### Note : This column has **not** helped me much ( I have explained about it later).","fad97dbe":"# Plan of Action\n### We will be looking at the following things:\n  *     Data visualization\n  *     Data Preprocessing\n  *     Filling in missing values\n  *     Encoding","9ca2dc8c":"### SibSp tells us about the passengers' siblings and spouse.\n### Parch tells us about the passengers' parents and children.","b099b8da":"### Considering this I droped the \"Cabin\" column.","09603c77":"## 5. Embarked","35317872":"### 1. Sex","aed5f132":"![Titanic](https:\/\/hips.hearstapps.com\/hmg-prod.s3.amazonaws.com\/images\/launch-titanic-1523648780.jpg)\n ##  Titanic Survival Prediction","809fbddd":"#### This graph shows the density of people who belong to the 3 class along with the age.","f6c12c64":"## 9. Ticket","7994904c":"## 6. SibSp and Parch","1199d577":"#### These cabins will be mapped with a numeric value.","2020aae4":"![](http:\/\/www.historyonthenet.com\/wp-content\/uploads\/2014\/08\/MGY_Cutaway2.JPG)","09a6a218":"### \"Cabin\" is an interesting column telling us about the cabin which was occupied by the passenger.","89979d44":"### Note: In the above feature preprocessing, the values that I have used for filling missing values were chosen after experimenting with different values. I took these values as they gave me the best result. Median values are best suited for missing values in most of the Machine-Learning models.","2f444fd6":"## 4. Fare","6d8d8c98":"### This is the most important section of this project. Here, the ultimate goal is to find an optimal combination of hyperparameters that minimizes a predefined loss function to give better results.","7200a0c0":"## 7. Cabin","f3d7f673":"## 2. Pclass","992d7db9":"### In this section we will be training various models using different classifiers. Out of them all, we will be choosing the best classifier to give us the most accurate prediction."}}