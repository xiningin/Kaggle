{"cell_type":{"0135d389":"code","c8a323a5":"code","a09c138e":"code","7897f906":"code","e0eb7478":"code","29a13cbb":"code","fcd3f159":"code","e278fb1d":"code","8e92c211":"code","a39f3d7e":"code","6ccf2052":"code","972e2898":"code","3e31b8c2":"code","04ad047c":"markdown","854259f6":"markdown","3ab12f91":"markdown","c7e712f4":"markdown","922206be":"markdown","fb2684a7":"markdown","a16bdc1b":"markdown","6abf5441":"markdown","1d1ee52b":"markdown","d1e28e86":"markdown","608e22e6":"markdown","0353f988":"markdown"},"source":{"0135d389":"%%writefile submission.py\n\nimport pandas as pd\nimport numpy as np\nimport json\n\n\n# base class for all agents, random agent\nclass agent():\n    def initial_step(self):\n        return np.random.randint(3)\n    \n    def history_step(self, history):\n        return np.random.randint(3)\n    \n    def step(self, history):\n        if len(history) == 0:\n            return int(self.initial_step())\n        else:\n            return int(self.history_step(history))\n    \n# agent that returns (previousCompetitorStep + shift) % 3\nclass mirror_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['competitorStep'] + self.shift) % 3\n    \n    \n# agent that returns (previousPlayerStep + shift) % 3\nclass self_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['step'] + self.shift) % 3    \n\n\n# agent that beats the most popular step of competitor\nclass popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['competitorStep'] for x in history])\n        return (int(np.argmax(counts)) + 1) % 3\n\n    \n# agent that beats the agent that beats the most popular step of competitor\nclass anti_popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['step'] for x in history])\n        return (int(np.argmax(counts)) + 2) % 3\n    \n    \n# simple transition matrix: previous step -> next step\nclass transition_matrix(agent):\n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3)) + self.init_value\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type]), int(history[i+1][self.step_type])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type])]\/matrix[int(history[-1][self.step_type])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n    \n\n# similar to the transition matrix but rely on both previous steps\nclass transition_tensor(agent):\n    \n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type1 = 'step' \n            self.step_type2 = 'competitorStep'\n        else:\n            self.step_type2 = 'step' \n            self.step_type1 = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3, 3)) + 0.1\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type1]), int(history[i][self.step_type2]), int(history[i+1][self.step_type1])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])]\/matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n\n        \n# looks for the same pattern in history and returns the best answer to the most possible counter strategy\nclass pattern_matching(agent):\n    def __init__(self, steps = 3, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        self.steps = steps\n        \n    def history_step(self, history):\n        if len(history) < self.steps + 1:\n            return self.initial_step()\n        \n        next_step_count = np.zeros(3) + self.init_value\n        pattern = [history[i][self.step_type] for i in range(- self.steps, 0)]\n        \n        for i in range(len(history) - self.steps):\n            next_step_count = (next_step_count - self.init_value)\/self.decay + self.init_value\n            current_pattern = [history[j][self.step_type] for j in range(i, i + self.steps)]\n            if np.sum([pattern[j] == current_pattern[j] for j in range(self.steps)]) == self.steps:\n                next_step_count[history[i + self.steps][self.step_type]] += 1\n        \n        if next_step_count.max() == self.init_value:\n            return self.initial_step()\n        \n        if  self.deterministic:\n            step = np.argmax(next_step_count)\n        else:\n            step = np.random.choice([0,1,2], p = next_step_count\/next_step_count.sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n        \n# if we add all agents the algorithm will spend more that 1 second on turn and will be invalidated\n# right now the agens are non optimal and the same computeations are repeated a lot of times\n# the approach can be optimised to run much faster\nagents = {\n    'mirror_0': mirror_shift(0),\n    'mirror_1': mirror_shift(1),  \n    'mirror_2': mirror_shift(2),\n    'self_0': self_shift(0),\n    'self_1': self_shift(1),  \n    'self_2': self_shift(2),\n    'popular_beater': popular_beater(),\n    'anti_popular_beater': anti_popular_beater(),\n    'random_transitison_matrix': transition_matrix(False, False),\n    'determenistic_transitison_matrix': transition_matrix(True, False),\n    'random_self_trans_matrix': transition_matrix(False, True),\n    'determenistic_self_trans_matrix': transition_matrix(True, True),\n    'random_transitison_tensor': transition_tensor(False, False),\n    'determenistic_transitison_tensor': transition_tensor(True, False),\n    'random_self_trans_tensor': transition_tensor(False, True),\n    'determenistic_self_trans_tensor': transition_tensor(True, True),\n    \n    'random_transitison_matrix_decay': transition_matrix(False, False, decay = 1.05),\n    'random_self_trans_matrix_decay': transition_matrix(False, True, decay = 1.05),\n    'random_transitison_tensor_decay': transition_tensor(False, False, decay = 1.05),\n    'random_self_trans_tensor_decay': transition_tensor(False, True, decay = 1.05),\n    \n    'determenistic_transitison_matrix_decay': transition_matrix(True, False, decay = 1.05),\n    'determenistic_self_trans_matrix_decay': transition_matrix(True, True, decay = 1.05),\n    'determenistic_transitison_tensor_decay': transition_tensor(True, False, decay = 1.05),\n    'determenistic_self_trans_tensor_decay': transition_tensor(True, True, decay = 1.05),\n    \n#     'random_transitison_matrix_decay2': transition_matrix(False, False, decay = 1.001),\n#     'random_self_trans_matrix_decay2': transition_matrix(False, True, decay = 1.001),\n#     'random_transitison_tensor_decay2': transition_tensor(False, False, decay = 1.001),\n#     'random_self_trans_tensor_decay2': transition_tensor(False, True, decay = 1.001),\n    \n#     'determenistic_transitison_matrix_decay2': transition_matrix(True, False, decay = 1.001),\n#     'determenistic_self_trans_matrix_decay2': transition_matrix(True, True, decay = 1.001),\n#     'determenistic_transitison_tensor_decay2': transition_tensor(True, False, decay = 1.001),\n#     'determenistic_self_trans_tensor_decay2': transition_tensor(True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_1': pattern_matching(1, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_1': pattern_matching(1, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_1': pattern_matching(1, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_1': pattern_matching(1, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_2': pattern_matching(2, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_2': pattern_matching(2, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_2': pattern_matching(2, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_2': pattern_matching(2, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_3': pattern_matching(3, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_3': pattern_matching(3, False, True, decay = 1.001),\n    'determenistic_pattern_matching_decay_3': pattern_matching(3, True, False, decay = 1.001),\n    'determenistic_self_pattern_matching_decay_3': pattern_matching(3, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_4': pattern_matching(4, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_4': pattern_matching(4, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_4': pattern_matching(4, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_4': pattern_matching(4, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_5': pattern_matching(5, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_5': pattern_matching(5, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_5': pattern_matching(5, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_5': pattern_matching(5, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_6': pattern_matching(6, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_6': pattern_matching(6, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_6': pattern_matching(6, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_6': pattern_matching(6, True, True, decay = 1.001),\n}\n\nhistory = []\nbandit_state = {k:[1,1] for k in agents.keys()}\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    \n    # bandits' params\n    step_size = 3 # how much we increase a and b \n    decay_rate = 1.05 # how much do we decay old historical data\n    \n    global history, bandit_state\n    \n    def log_step(step = None, history = None, agent = None, competitorStep = None, file = 'history.csv'):\n        if step is None:\n            step = np.random.randint(3)\n        if history is None:\n            history = []\n        history.append({'step': step, 'competitorStep': competitorStep, 'agent': agent})\n        if file is not None:\n            pd.DataFrame(history).to_csv(file, index = False)\n        return step\n    \n    def update_competitor_step(history, competitorStep):\n        history[-1]['competitorStep'] = int(competitorStep)\n        return history\n    \n    # load history\n    if observation.step == 0:\n        pass\n    else:\n        history = update_competitor_step(history, observation.lastOpponentAction)\n        \n        # updating bandit_state using the result of the previous step\n        # we can update all states even those that were not used\n        for name, agent in agents.items():\n            agent_step = agent.step(history[:-1])\n            bandit_state[name][1] = (bandit_state[name][1] - 1) \/ decay_rate + 1\n            bandit_state[name][0] = (bandit_state[name][0] - 1) \/ decay_rate + 1\n            \n            if (history[-1]['competitorStep'] - agent_step) % 3 == 1:\n                bandit_state[name][1] += step_size\n            elif (history[-1]['competitorStep'] - agent_step) % 3 == 2:\n                bandit_state[name][0] += step_size\n            else:\n                bandit_state[name][0] += step_size\/2\n                bandit_state[name][1] += step_size\/2\n            \n    # we can use it for analysis later\n    with open('bandit.json', 'w') as outfile:\n        json.dump(bandit_state, outfile)\n    \n    \n    # generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in bandit_state.keys():\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    step = agents[best_agent].step(history)\n    \n    return log_step(step, history, best_agent)","c8a323a5":"%%writefile copy_opponent.py\n    \ndef copy_opponent_agent(observation, configuration):\n    if observation.step > 0:\n        return observation.lastOpponentAction\n    else:\n        return 0","a09c138e":"from kaggle_environments import evaluate, make, utils\nenv = make(\"rps\", debug=True)","7897f906":"env.reset()\nenv.run([\"submission.py\", \"copy_opponent.py\"])\nenv.render(mode=\"ipython\", width=800, height=700)","e0eb7478":"import json","29a13cbb":"with open('bandit.json') as json_file:\n    bandit_state = json.load(json_file)\nbandit_state","fcd3f159":"%%writefile anti_mirror.py\n\nimport pandas as pd\n\nhistory = []\n    \ndef anti_copy_opponent_agent (observation, configuration):\n    \n    global history\n    \n    if observation.step == 0:\n        history.append({'step': 1, 'competitorStep': None})\n        return 1\n    else:\n        history[-1]['competitorStep'] = observation.lastOpponentAction\n        step = (history[-1]['step'] + 1) % 3\n        history.append({'step': step, 'competitorStep': None})\n        return step","e278fb1d":"env.reset()\nenv.run([\"submission.py\", \"anti_mirror.py\"])\nenv.render(mode=\"ipython\", width=800, height=700)","8e92c211":"with open('bandit.json') as json_file:\n    bandit_state = json.load(json_file)\nbandit_state","a39f3d7e":"%%writefile pattern_matching.py\n\nimport random\n\n\n# current memory of the agent\ncurrent_memory = []\n# maximum steps in the pattern\nsteps_max = 6\n# minimum steps in the pattern\nsteps_min = 3\n# memory length of patterns in first group\n# steps_max is multiplied by 2 to consider both my_agent's and opponent's actions\ngroup_memory_length = steps_max * 2\n# list of groups of memory patterns\ngroups_of_memory_patterns = []\nfor i in range(steps_max, steps_min - 1, -1):\n    groups_of_memory_patterns.append({\n        # how many steps in a row are in the pattern\n        \"memory_length\": group_memory_length,\n        # list of memory patterns\n        \"memory_patterns\": []\n    })\n    group_memory_length -= 2\n\n    \ndef find_pattern(memory_patterns, memory, memory_length):\n    \"\"\" find appropriate pattern in memory \"\"\"\n    for pattern in memory_patterns:\n        actions_matched = 0\n        for i in range(memory_length):\n            if pattern[\"actions\"][i] == memory[i]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return pattern\n    # appropriate pattern not found\n    return None\n\ndef my_agent(obs, conf):\n    \"\"\" your ad here \"\"\"\n    # action of my_agent\n    my_action = None\n    # if it's not first step, add opponent's last action to agent's current memory\n    if obs[\"step\"] > 0:\n        current_memory.append(obs[\"lastOpponentAction\"])\n    for group in groups_of_memory_patterns:\n        # if length of current memory is bigger than necessary for a new memory pattern\n        if len(current_memory) > group[\"memory_length\"]:\n            # get momory of the previous step\n            previous_step_memory = current_memory[:group[\"memory_length\"]]\n            previous_pattern = find_pattern(group[\"memory_patterns\"], previous_step_memory, group[\"memory_length\"])\n            if previous_pattern == None:\n                previous_pattern = {\n                    \"actions\": previous_step_memory.copy(),\n                    \"opp_next_actions\": [\n                        {\"action\": 0, \"amount\": 0, \"response\": 1},\n                        {\"action\": 1, \"amount\": 0, \"response\": 2},\n                        {\"action\": 2, \"amount\": 0, \"response\": 0}\n                    ]\n                }\n                group[\"memory_patterns\"].append(previous_pattern)\n            for action in previous_pattern[\"opp_next_actions\"]:\n                if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                    action[\"amount\"] += 1\n            # delete first two elements in current memory (actions of the oldest step in current memory)\n            del current_memory[:2]\n            # if action was not yet found\n            if my_action == None:\n                pattern = find_pattern(group[\"memory_patterns\"], current_memory, group[\"memory_length\"])\n                # if appropriate pattern is found\n                if pattern != None:\n                    my_action_amount = 0\n                    for action in pattern[\"opp_next_actions\"]:\n                        # if this opponent's action occurred more times than currently chosen action\n                        # or, if it occured the same amount of times and this one is choosen randomly among them\n                        if (action[\"amount\"] > my_action_amount or\n                                (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                            my_action_amount = action[\"amount\"]\n                            my_action = action[\"response\"]\n    # if no action is found\n    if my_action == None:\n        my_action = random.randint(0, 2)\n    current_memory.append(my_action)\n    return my_action","6ccf2052":"env.reset()\nenv.run([\"submission.py\", \"pattern_matching.py\"])\nenv.render(mode=\"ipython\", width=800, height=700)","972e2898":"with open('bandit.json') as json_file:\n    bandit_state = json.load(json_file)\nbandit_state","3e31b8c2":"env.reset()\nenv.run([\"submission.py\", \"submission.py\"])\nenv.render(mode=\"ipython\", width=800, height=700)","04ad047c":"Let's now create an agent acting as the self_1 agent (counter strategy for the mirroring algorithm https:\/\/www.kaggle.com\/ilialar\/beating-mirror-agent) and compare it to our bandit. ","854259f6":"Let's beat the mirroring agent using our solutioin:","3ab12f91":"You can add your algorithms to the `agents` dict. If one of your agents works well against the current opponent, a multi-armed bandit will find it out and use it to win.","c7e712f4":"We can see that self_1 was applied much often than other strategies, and it is a clear winner. It is a counter-strategy for the simple mirroring strategy. That is why a multi-armed bandit algorithm used it much more often than any other agent.","922206be":"And finally let's try to beat a strong public baseline (https:\/\/www.kaggle.com\/yegorbiryukov\/rock-paper-scissors-with-memory-patterns)","fb2684a7":"Multi-armed bandit is a widely used RL-algorithm because it is very balanced in terms of exploitation\/exploration.\n\nAlgorithm logic:\n- Create N agents with different logic\n- At each step for each agent, generate a random number from B(a+1, b+1). B - beta-distribution, a - number of historical wins of this agent, b - number of this agent's historical losses.\n- Select the agent with the largest generated number and use it to generate the next step.\n- Repeat","a16bdc1b":"Validation run:","6abf5441":"Our bandit is also the winner, but it mostly used the mirror_2 strategy that is the optimal counter-strategy to the self_1 agent.","1d1ee52b":"This notebook shows how to use multi-armed bandits to build the agent to beat many (partially)deterministic agents.\n\nFor every public strategy (except random), it is easy to construct an algorithm that beats it. But you don't know in advance what method is used by the opponent.\n\nThe multi-armed bandit approach is a Reinforcement learning technic that allows us to select the most promising counter-strategy and use it to beat the opponent.","d1e28e86":"In this notebook, I also show how to create the following types of agents:\n- Mirror + shift. This class of agents takes the opponent's last step and return (step + shift) % 3. E.g., with shift=0 - it is a mirroring agent, with shift=1 agent that beats the opponent's previous hand.\n- Self + shift. These agents add some shifts to their previous steps. With shift=0, it is a constant agent. With shift = 1 or 2, it just repeats all 3 possible steps in direct or reversed order.\n- Popular beater - returns the step that beats the most popular action of the opponent so far.\n- Anti-popular beater - return the step that beats the step that wins the most popular step )\n- Transition matrix - computes the historical probability to go from one state to another (both for opponent and yourself) and returns the step that beats the most probable step of the opponent.\n- Transition tensor - the same thing but considers both player's and opponent's previous steps. The last two agents have both deterministic and random versions and a decay option - higher weight for the more recent rounds.\n- Pattern mathcing - is a development of the transition matrix idea that takes into account n last steps. It is inspired by https:\/\/www.kaggle.com\/yegorbiryukov\/rock-paper-scissors-with-memory-patterns.","608e22e6":"It is much harder but our agent wins.","0353f988":"Let's look at the final state of the bandit."}}