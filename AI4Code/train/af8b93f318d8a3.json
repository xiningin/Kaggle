{"cell_type":{"127d3743":"code","95def0bf":"code","8dfa2c9e":"code","3f45289b":"code","4164eeda":"code","80c677fd":"code","98efe9ad":"code","8d1a7f09":"code","13d28156":"code","f3cf62e7":"code","323f7078":"code","91d717cf":"code","137a51d2":"code","3badc85c":"code","0fed5d89":"code","1a30a4a9":"code","2e161673":"code","73d7442b":"code","4ff903e2":"code","b405d152":"code","c4cf34c9":"code","89c83f90":"code","d32a33e3":"code","df412f66":"code","c07264f6":"code","8e7cff06":"code","0ac7507b":"code","fd4f28b0":"code","831dbb1a":"code","3a26428f":"code","de258ab3":"code","c5e86590":"code","e522ba3c":"code","584bb4b4":"code","45db7f9c":"code","3d3392ab":"code","c317a18a":"code","c35b9892":"code","8b172be8":"code","cb4e2946":"code","c233cc55":"code","9b600b41":"code","a64d5b1e":"code","ea772336":"code","d162dc8d":"code","f004442b":"code","d7379b93":"code","9dfb1a70":"code","12035c51":"code","e18a071e":"code","917afffd":"code","8d75b17c":"code","3866362a":"code","e021bc09":"code","f93e4553":"markdown","15cea2e7":"markdown","fd3e4739":"markdown","4d3fcc8f":"markdown","db7198f6":"markdown","f6b74933":"markdown","dbb678d4":"markdown","e35b0ce3":"markdown","c5a46de6":"markdown","81f01bbc":"markdown","eadff9ce":"markdown","4ef77647":"markdown","af6ac1f7":"markdown","c79c067e":"markdown","47c0f01b":"markdown","1bab5560":"markdown","e6d4e045":"markdown","64b66693":"markdown","1cd1f731":"markdown","24f25d23":"markdown","50449668":"markdown","3a113459":"markdown","ed8dd38e":"markdown","47dd037d":"markdown","8a2c01ba":"markdown","29f2f5c5":"markdown","0f82babc":"markdown","512592d7":"markdown","bf50aee7":"markdown","e3f2a5ed":"markdown","2ffc6a12":"markdown","843f8686":"markdown","fe39a545":"markdown","cf30384a":"markdown","2386b596":"markdown","8d9f69a6":"markdown","d35b5323":"markdown","f1a7bd40":"markdown","ba1d940b":"markdown","9c584855":"markdown","ea6b34ba":"markdown","64e59526":"markdown","c10e25d4":"markdown","a6692416":"markdown","2ef66ee4":"markdown","1f0049ef":"markdown","90d1c80a":"markdown","93d399e2":"markdown","cf3898a9":"markdown","dae06cb9":"markdown","be2439d8":"markdown","89aba191":"markdown","c8e3cec2":"markdown","7634975a":"markdown","de46aacb":"markdown","8b1d20a6":"markdown","b660e049":"markdown","4736ab11":"markdown"},"source":{"127d3743":"!pip install -U scikit-learn > \/dev\/null","95def0bf":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nfrom matplotlib import pyplot as plt\nplt.rcParams['figure.figsize'] = (10, 8)\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport collections\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score","8dfa2c9e":"# Create dataframe with dummy variables\ndef create_df(dic, feature_list):\n    out = pd.DataFrame(dic)\n    out = pd.concat([out, pd.get_dummies(out[feature_list])], axis = 1)\n    out.drop(feature_list, axis = 1, inplace = True)\n    return out\n\n# Some feature values are present in train and absent in test and vice-versa.\ndef intersect_features(train, test):\n    common_feat = list( set(train.keys()) & set(test.keys()))\n    return train[common_feat], test[common_feat]","3f45289b":"features = ['Looks', 'Alcoholic_beverage','Eloquence','Money_spent']","4164eeda":"df_train = {}\ndf_train['Looks'] = ['handsome', 'handsome', 'handsome', 'repulsive',\n                         'repulsive', 'repulsive', 'handsome'] \ndf_train['Alcoholic_beverage'] = ['yes', 'yes', 'no', 'no', 'yes', 'yes', 'yes']\ndf_train['Eloquence'] = ['high', 'low', 'average', 'average', 'low',\n                                   'high', 'average']\ndf_train['Money_spent'] = ['lots', 'little', 'lots', 'little', 'lots',\n                                  'lots', 'lots']\ndf_train['Will_go'] = LabelEncoder().fit_transform(['+', '-', '+', '-', '-', '+', '+'])\n\ndf_train = create_df(df_train, features)\ndf_train","80c677fd":"df_test = {}\ndf_test['Looks'] = ['handsome', 'handsome', 'repulsive'] \ndf_test['Alcoholic_beverage'] = ['no', 'yes', 'yes']\ndf_test['Eloquence'] = ['average', 'high', 'average']\ndf_test['Money_spent'] = ['lots', 'little', 'lots']\ndf_test = create_df(df_test, features)\ndf_test","98efe9ad":"# Some feature values are present in train and absent in test and vice-versa.\ny = df_train['Will_go']\ndf_train, df_test = intersect_features(train=df_train, test=df_test)\ndf_train","8d1a7f09":"df_test","13d28156":"dt = DecisionTreeClassifier(criterion='entropy', random_state=17)\ndt.fit(df_train, y);","f3cf62e7":"plot_tree(dt, feature_names=df_train.columns, filled=True,\n         class_names=[\"Won't go\", \"Will go\"]);","323f7078":"balls = [1 for i in range(9)] + [0 for i in range(11)]","91d717cf":"# two groups\nballs_left  = [1 for i in range(8)] + [0 for i in range(5)] # 8 blue and 5 yellow\nballs_right = [1 for i in range(1)] + [0 for i in range(6)] # 1 blue and 6 yellow","137a51d2":"from math import log\n    \ndef entropy(a_list):\n    lst = list(a_list)\n    size = len(lst) \n    entropy = 0\n    set_elements = len(set(lst))\n    if set_elements in [0, 1]:\n        return 0\n    for i in set(lst):\n        occ = lst.count(i)\n        entropy -= occ\/size * log (occ\/size,2)\n    return entropy","3badc85c":"print(entropy(balls)) # 9 blue \u0438 11 yellow\nprint(entropy(balls_left)) # 8 blue \u0438 5 yellow\nprint(entropy(balls_right)) # 1 blue \u0438 6 yellow\nprint(entropy([1,2,3,4,5,6])) # entropy of a fair 6-sided die","0fed5d89":"# information gain calculation\ndef information_gain(root, left, right):\n    ''' root - initial data, left and right - two partitions of initial data'''\n        \n    return entropy(root) - 1.0 * len(left) \/ len(root) * entropy(left) \\\n                         - 1.0 * len(right) \/ len(root) * entropy(right)\n\n\nprint(information_gain(balls, balls_left, balls_right))","1a30a4a9":"def information_gains(X, y):\n    '''Outputs information gain when splitting with each feature'''\n    out = []\n    for i in X.columns:\n        out.append(information_gain(y, y[X[i] == 0], y[X[i] == 1]))\n    return out","2e161673":"information_gains(df_train, y)","73d7442b":"def btree(X, y, feature_names):\n    clf = information_gains(X, y)\n    best_feat_id = clf.index(max(clf))\n    best_feature = feature_names[best_feat_id]\n    print (f'Best feature to split: {best_feature}')\n    \n    x_left = X[X.iloc[:, best_feat_id] == 0]\n    x_right = X[X.iloc[:, best_feat_id] == 1]\n    print (f'Samples: {len(x_left)} (left) and {len(x_right)} (right)')\n    \n    y_left = y[X.iloc[:, best_feat_id] == 0]\n    y_right = y[X.iloc[:, best_feat_id] == 1]\n    entropy_left = entropy(y_left)\n    entropy_right = entropy(y_right)\n    print (f'Entropy: {entropy_left} (left) and {entropy_right} (right)')\n    print('_' * 30 + '\\n')\n    if entropy_left != 0:\n        print(f'Splitting the left group with {len(x_left)} samples:')\n        btree(x_left, y_left, feature_names)\n    if entropy_right != 0:\n        print(f'Splitting the right group with {len(x_right)} samples:')\n        btree(x_right, y_right, feature_names)\n\n\n\nbtree (df_train, y, df_train.columns)","4ff903e2":"data_train = pd.read_csv('..\/input\/adult_train.csv')","b405d152":"data_train.tail()","c4cf34c9":"data_test = pd.read_csv('..\/input\/adult_test.csv')","89c83f90":"data_test.tail()","d32a33e3":"# necessary to remove rows with incorrect labels in test dataset\ndata_test = data_test[(data_test['Target'] == ' >50K.') | (data_test['Target']==' <=50K.')]\n\n# encode target variable as integer\ndata_train.loc[data_train['Target']==' <=50K', 'Target'] = 0\ndata_train.loc[data_train['Target']==' >50K', 'Target'] = 1\n\ndata_test.loc[data_test['Target']==' <=50K.', 'Target'] = 0\ndata_test.loc[data_test['Target']==' >50K.', 'Target'] = 1","df412f66":"data_test.describe(include='all').T","c07264f6":"data_train['Target'].value_counts()","8e7cff06":"fig = plt.figure(figsize=(25, 15))\ncols = 5\nrows = np.ceil(float(data_train.shape[1]) \/ cols)\nfor i, column in enumerate(data_train.columns):\n    ax = fig.add_subplot(rows, cols, i + 1)\n    ax.set_title(column)\n    if data_train.dtypes[column] == np.object:\n        data_train[column].value_counts().plot(kind=\"bar\", axes=ax)\n    else:\n        data_train[column].hist(axes=ax)\n        plt.xticks(rotation=\"vertical\")\nplt.subplots_adjust(hspace=0.7, wspace=0.2)","0ac7507b":"data_train.dtypes","fd4f28b0":"data_test.dtypes","831dbb1a":"data_test['Age'] = data_test['Age'].astype(int)","3a26428f":"data_test['fnlwgt'] = data_test['fnlwgt'].astype(int)\ndata_test['Education_Num'] = data_test['Education_Num'].astype(int)\ndata_test['Capital_Gain'] = data_test['Capital_Gain'].astype(int)\ndata_test['Capital_Loss'] = data_test['Capital_Loss'].astype(int)\ndata_test['Hours_per_week'] = data_test['Hours_per_week'].astype(int)","de258ab3":"# choose categorical and continuous features from data\n\ncategorical_columns = [c for c in data_train.columns \n                       if data_train[c].dtype.name == 'object']\nnumerical_columns = [c for c in data_train.columns \n                     if data_train[c].dtype.name != 'object']\n\nprint('categorical_columns:', categorical_columns)\nprint('numerical_columns:', numerical_columns)","c5e86590":"# we see some missing values\ndata_train.info()","e522ba3c":"# fill missing data\n\nfor c in categorical_columns:\n    data_train[c].fillna(data_train[c].mode()[0], inplace=True)\n    data_test[c].fillna(data_train[c].mode()[0], inplace=True)\n    \nfor c in numerical_columns:\n    data_train[c].fillna(data_train[c].median(), inplace=True)\n    data_test[c].fillna(data_train[c].median(), inplace=True)","584bb4b4":"# no more missing values\ndata_train.info()","45db7f9c":"data_train = pd.concat([data_train[numerical_columns],\n    pd.get_dummies(data_train[categorical_columns])], axis=1)\n\ndata_test = pd.concat([data_test[numerical_columns],\n    pd.get_dummies(data_test[categorical_columns])], axis=1)","3d3392ab":"set(data_train.columns) - set(data_test.columns)","c317a18a":"data_train.shape, data_test.shape","c35b9892":"data_test['Country_ Holand-Netherlands'] = 0","8b172be8":"set(data_train.columns) - set(data_test.columns)","cb4e2946":"data_train.head(2)","c233cc55":"data_test.head(2)","9b600b41":"X_train = data_train.drop(['Target'], axis=1)\ny_train = data_train['Target']\n\nX_test = data_test.drop(['Target'], axis=1)\ny_test = data_test['Target']","a64d5b1e":"tree = DecisionTreeClassifier(max_depth=3, random_state=17)\ntree.fit(X_train, y_train)","ea772336":"tree_predictions = tree.predict(X_test) ","d162dc8d":"accuracy_score(y_test, tree_predictions)","f004442b":"tree_params = {'max_depth': range(2,11)}\n\nlocally_best_tree = GridSearchCV(DecisionTreeClassifier(random_state=17),\n                                 tree_params, cv=5)                    \n\nlocally_best_tree.fit(X_train, y_train)","d7379b93":"print(\"Best params:\", locally_best_tree.best_params_)\nprint(\"Best cross validaton score\", locally_best_tree.best_score_)","9dfb1a70":"tuned_tree = DecisionTreeClassifier(max_depth=9, random_state=17)\ntuned_tree.fit(X_train, y_train)\ntuned_tree_predictions = tuned_tree.predict(X_test)\naccuracy_score(y_test, tuned_tree_predictions)","12035c51":"rf = RandomForestClassifier(n_estimators=100, random_state=17)\nrf.fit(X_train, y_train)","e18a071e":"cv_scores = cross_val_score(rf, X_train, y_train, cv=3)\ncv_scores, cv_scores.mean()","917afffd":"forest_predictions = rf.predict(X_test)\naccuracy_score(y_test,forest_predictions)","8d75b17c":"forest_params = {'max_depth': range(10, 16),\n                 'max_features': range(5, 105, 20)}\n\nlocally_best_forest = GridSearchCV(\n    RandomForestClassifier(n_estimators=10, random_state=17, n_jobs=4),\n    forest_params, cv=3, verbose=1, n_jobs=4)\n\nlocally_best_forest.fit(X_train, y_train)","3866362a":"print(\"Best params:\", locally_best_forest.best_params_)\nprint(\"Best cross validaton score\", locally_best_forest.best_score_)","e021bc09":"tuned_forest_predictions = locally_best_forest.predict(X_test) \naccuracy_score(y_test,tuned_forest_predictions)","f93e4553":"Make predictions for the test data and assess accuracy.","15cea2e7":"We'll go through a toy example of binary classification - Person A is deciding whether they will go on a second date with Person B. It will depend on their looks, eloquence, alcohol consumption (only for example), and how much money was spent on the first date.","fd3e4739":"7\\. What is the test set accuracy of a decision tree with maximum depth of 9 and **random_state = 17**?","4d3fcc8f":"Let's take a sneak peek of upcoming lectures and try to use a random forest for our task. For now, you can imagine a random forest as a bunch of decision trees, trained on slightly different subsets of the training data.","db7198f6":"#### Reading train and test data","f6b74933":"[Dataset](http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/adult) UCI Adult (no need to download it, we have a copy in the course repository): classify people using demographical data - whether they earn more than \\$50,000 per year or not.","dbb678d4":"Make a prediction with the trained model on the test data.","e35b0ce3":"Consider the following warm-up example: we have 9 blue balls and 11 yellow balls. Let ball have label **1** if it is blue, **0** otherwise.","c5a46de6":"#### Test data","81f01bbc":"Next split the balls into two groups:","eadff9ce":"#### Creating the dataset","4ef77647":"**Target** \u2013 earnings level, categorical (binary) feature.","af6ac1f7":"Your goal is to figure out how decision trees work by walking through a toy problem. While a single decision tree does not yield outstanding results, other performant algorithms like gradient boosting and random forests are based on the same idea. That is why knowing how decision trees work might be useful.","c79c067e":"1\\. What is the entropy $S_0$ of the initial system? By system states, we mean values of the binary feature \"Will_go\" - 0 or 1 - two states in total.","47c0f01b":"# <center> Assignment #3 (demo)\n## <center>  Decision trees with a toy task and the UCI Adult dataset \n\nSame assignment as a [Kaggle Kernel](https:\/\/www.kaggle.com\/kashnitsky\/a3-demo-decision-trees) + [solution](https:\/\/www.kaggle.com\/kashnitsky\/a3-demo-decision-trees-solution). Fill in the answers in the [web-form](https:\/\/docs.google.com\/forms\/d\/1wfWYYoqXTkZNOPy1wpewACXaj2MZjBdLOL58htGWYBA\/edit).","1bab5560":"<img src='https:\/\/habrastorage.org\/webt\/bd\/aq\/5w\/bdaq5wi3c4feezaexponvin8wmo.png'>","e6d4e045":"#### Draw a decision tree (by hand or in any graphics editor) for this dataset. Optionally you can also implement tree construction and draw it here.","64b66693":"Train a decision tree **(DecisionTreeClassifier, random_state = 17).** Find the optimal maximum depth using 5-fold cross-validation **(GridSearchCV)**.","1cd1f731":"### Part 2. Functions for calculating entropy and information gain.","24f25d23":"Tests","50449668":"#### Fill in missing data for continuous features with their median values, for categorical features with their mode.","3a113459":"#### Additional: display the resulting tree using graphviz. You can use pydot or [web-service](https:\/\/www.coolutils.com\/ru\/online\/DOT-to-PNG) dot2png.","ed8dd38e":"6\\. What is the test set accuracy of a decision tree with maximum tree depth of 3 and **random_state = 17**?","47dd037d":"0.84","8a2c01ba":"2\\. Let's split the data by the feature \"Looks_handsome\". What is the entropy $S_1$ of the left group - the one with \"Looks_handsome\". What is the entropy $S_2$ in the opposite group? What is the information gain (IG) if we consider such a split?","29f2f5c5":"5\\. What is the information gain from splitting the initial dataset into **balls_left** and **balls_right** ?","0f82babc":"### 3.1 Decision tree without parameter tuning","512592d7":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\">\n    \n## [mlcourse.ai](https:\/\/mlcourse.ai) - Open Machine Learning Course\n\nAuthors: [Maria Sumarokova](https:\/\/www.linkedin.com\/in\/mariya-sumarokova-230b4054\/), and [Yury Kashnitsky](https:\/\/www.linkedin.com\/in\/festline\/). Translated and edited by Gleb Filatov, Aleksey Kiselev, [Anastasia Manokhina](https:\/\/www.linkedin.com\/in\/anastasiamanokhina\/), [Egor Polusmak](https:\/\/www.linkedin.com\/in\/egor-polusmak\/), and [Yuanyuan Pao](https:\/\/www.linkedin.com\/in\/yuanyuanpao\/). All content is distributed under the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license.","bf50aee7":"#### Dataset description:","e3f2a5ed":"<img src = 'https:\/\/habrastorage.org\/webt\/mu\/vl\/mt\/muvlmtd2njeqf18trbldenpqvnm.png'>","2ffc6a12":"Let's start by loading all necessary libraries:","843f8686":"#### Checking data types","fe39a545":"0.961","cf30384a":"Also we'll cast all **float** features to **int** type to keep types consistent between our train and test data.","2386b596":"3\\. What is the entropy of the state given by the list **balls_left**?","8d9f69a6":"0.16","d35b5323":"### 3.4 (Optional) Random forest with parameter tuning","f1a7bd40":"### Part 3. The \"Adult\" dataset","ba1d940b":"Make predictions for the test data and assess accuracy.","9c584855":"#### There is no Holland in the test data. Create new zero-valued feature.","ea6b34ba":"#### Train a decision tree using sklearn on the training data. You may choose any depth for the tree.","64e59526":"### 3.3 (Optional) Random forest without parameter tuning","c10e25d4":"Train a decision tree with maximum depth of 9 (it is the best **max_depth** in my case), and compute the test set accuracy. Use parameter **random_state = 17** for reproducibility.","a6692416":"- **Age** \u2013 continuous feature\n- **Workclass** \u2013  continuous feature\n- **fnlwgt** \u2013 final weight of object, continuous feature\n- **Education** \u2013  categorical feature\n- **Education_Num** \u2013 number of years of education, continuous feature\n- **Martial_Status** \u2013  categorical feature\n- **Occupation** \u2013  categorical feature\n- **Relationship** \u2013 categorical feature\n- **Race** \u2013 categorical feature\n- **Sex** \u2013 categorical feature\n- **Capital_Gain** \u2013 continuous feature\n- **Capital_Loss** \u2013 continuous feature\n- **Hours_per_week** \u2013 continuous feature\n- **Country** \u2013 categorical feature","2ef66ee4":"Train a random forest **(RandomForestClassifier)**. Set the number of trees to 100 and use **random_state = 17**.","1f0049ef":"Feature descriptions:","90d1c80a":"Train a decision tree **(DecisionTreeClassifier)** with a maximum depth of 3, and evaluate the accuracy metric on the test data. Use parameter **random_state = 17** for results reproducibility.","93d399e2":"### Part 1. Toy dataset \"Will They? Won't They?\"","cf3898a9":"#### Primary data analysis","dae06cb9":"### 3.2 Decision tree with parameter tuning","be2439d8":"0.847","89aba191":"4\\. What is the entropy of a fair dice? (where we look at a dice as a system with 6 equally probable states)?","c8e3cec2":"#### Training data","7634975a":"Train a random forest **(RandomForestClassifier)**. Tune the maximum depth and maximum number of features for each tree using **GridSearchCV**. ","de46aacb":"#### Implement a function to calculate the Shannon Entropy","8b1d20a6":"#### Optional:\n- Implement a decision tree building algorithm by calling **best_feature_to_split** recursively\n- Plot the resulting tree","b660e049":"As we see, in the test data, age is treated as type **object**. We need to fix this.","4736ab11":"We'll dummy code some categorical features: **Workclass**, **Education**, **Martial_Status**, **Occupation**, **Relationship**, **Race**, **Sex**, **Country**. It can be done via pandas method **get_dummies**"}}