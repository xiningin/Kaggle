{"cell_type":{"cfe6648e":"code","b180f430":"code","454a9c02":"code","be00ddb0":"code","85c5aecd":"code","1615e33e":"code","761d475f":"code","2189819f":"code","f197ee4c":"code","e4341c82":"code","f06fd7bb":"code","5ddb6c44":"code","b1b30172":"code","c5f8e023":"code","5ef20114":"code","7a4670c1":"code","ef157bdd":"code","0475ed97":"code","00caf42f":"code","7f7063e7":"code","1002e52c":"code","ec528ea7":"code","ee1430a8":"code","7a3ad56e":"code","50f9bc92":"code","40f08ac6":"code","a568b427":"code","c465b6cc":"code","47d237ff":"code","f810a295":"code","21c1ade9":"code","7d410ae4":"code","983679d9":"code","9a7d723b":"code","a1c61f6a":"code","b72f3fca":"code","c35a8cd8":"markdown","e769b8c7":"markdown","ce4bd29c":"markdown","e857e3f6":"markdown","635478a7":"markdown","f216fe31":"markdown","0cc01f5a":"markdown","db4afd37":"markdown","8b77aa90":"markdown","5b5821c8":"markdown","8d802631":"markdown","88ca4a55":"markdown","94332879":"markdown","950fc1e6":"markdown","4d4827b3":"markdown","a2796052":"markdown","7881602a":"markdown","187aa22a":"markdown","e70dbbb5":"markdown","f071be2e":"markdown","3b38c166":"markdown","c087f9f2":"markdown","4832e137":"markdown","5a78e4d9":"markdown"},"source":{"cfe6648e":"# Importing data wrangling and visualization libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","b180f430":"# importing the train dataset\ndf = pd.read_csv('..\/input\/train.csv')\n\n# importing the test dataset\ndf_pred = pd.read_csv('..\/input\/test.csv')\n\n# And let's check how the data looks like!\ndf.sample(10)","454a9c02":"# Let's check the datatypes\ndf.info()","be00ddb0":"# Copy dataframe\ndf1 = df.iloc[:, :]\ndf_pred1 = df_pred.iloc[:, :]\n\n# Create a list of both dataframes\nboth = [df1, df_pred1]","85c5aecd":"# For the Train Set\ndf1.isnull().sum()","1615e33e":"# and the Test Set\ndf_pred1.isnull().sum()","761d475f":"for dataset in both:\n    dataset['Age'].fillna(dataset['Age'].median(), inplace=True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace=True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace=True)\n\n# Let's delete the cabin attribute and also PassengerId and Ticket, already stated above\ndf1.drop(['Cabin', 'PassengerId', 'Ticket'], axis=1, inplace=True)","2189819f":"# Let's check the null values again\nprint(df1.isnull().sum())\nprint('-'*20)\nprint(df_pred1.isnull().sum())","f197ee4c":"for dataset in both:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    dataset['IsAlone'] = dataset['FamilySize'] == 1\n    dataset['IsAlone'] = dataset['IsAlone'].astype('int')\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    dataset['FareBin'] = pd.cut(dataset['Fare'], 4)\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype('int'), 5)","e4341c82":"df1.info()","f06fd7bb":"df1.sample(10)","5ddb6c44":"df1['Title'].value_counts()","b1b30172":"# Selects the titles to delete in both datasets\ntitle_del = (pd.concat([df1, df_pred1], sort=False)['Title'].value_counts() < 10)\n\n# Replace them with \"Other\"\nfor dataset in both:\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Other' if title_del.loc[x] == True else x)\n\n# Let's see how it looks like\ndf1['Title'].value_counts()","c5f8e023":"# Import the library\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder","5ef20114":"# Crete the encoder object\nencoder = LabelEncoder()\n\nto_encode = [('Sex', 'Sex_Coded'), ('Embarked', 'Embarked_Coded'), ('Title', 'Title_Coded'), \n             ('FareBin', 'Fare_Coded'), ('AgeBin', 'Age_Coded')]\n\n# Fit and transform using the Train set and transform the test set\nfor dataset in both:\n    for a in to_encode:\n        dataset[a[1]] = encoder.fit_transform(dataset[a[0]])","7a4670c1":"df1.sample(5)","ef157bdd":"df1_dummy = pd.get_dummies(df1[['Sex', 'Embarked', 'Title']])\ndf_pred1_dummy = pd.get_dummies(df_pred1[['Sex', 'Embarked', 'Title']])\ndf1_dummy.head()","0475ed97":"# List the features to be used on the analysis\nfeatures = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone']\n\n# Set the target\ntarget = ['Survived']","00caf42f":"# Let's check the Survived mean for each non continuous attribute\nfor feature in features:\n    if df1[feature].dtype != 'float64' :\n        print(df1[[feature, target[0]]].groupby(feature, as_index=False).mean())\n        print('-'*20, '\\n')","7f7063e7":"# Let's get visual for the continuous attributes\nplt.figure(figsize=[16,12])\n\n# Plot Fare as boxplot to identify outliers\nplt.subplot(231)\nplt.boxplot(x=df1['Fare'])\nplt.title('Fare')\nplt.ylabel('Fare ($)')\n\n# Also let's plot Age in a boxplot\nplt.subplot(232)\nplt.boxplot(df1['Age'])\nplt.title('Age')\nplt.ylabel('Age (Years)')\n\n# How about family size?\nplt.subplot(233)\nplt.boxplot(df1['FamilySize'])\nplt.title('Family Size')\nplt.ylabel('Family Size (#)')\n\n# Now how would Fare affect survivability?\nplt.subplot(234)\nplt.hist(x = [df1[df1['Survived']==1]['Fare'], df1[df1['Survived']==0]['Fare']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Fare by Survival')\nplt.xlabel('Fare ($)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\n# Age and survivability\nplt.subplot(235)\nplt.hist(x = [df1[df1['Survived']==1]['Age'], df1[df1['Survived']==0]['Age']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Age by Survival')\nplt.xlabel('Age (Years)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\n# Family size and survivability\nplt.subplot(236)\nplt.hist(x = [df1[df1['Survived']==1]['FamilySize'], df1[df1['Survived']==0]['FamilySize']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Family Size by Survival')\nplt.xlabel('Family Size (#)')\nplt.ylabel('# of Passengers')\nplt.legend()","1002e52c":"#Now let's visualize the \nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=df1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=df1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=df1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=df1, ax = saxis[1,0])\nsns.pointplot(x = 'AgeBin', y = 'Survived',  data=df1, ax = saxis[1,1])\nsns.pointplot(x = 'FamilySize', y = 'Survived', data=df1, ax = saxis[1,2])","ec528ea7":"#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = df1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = df1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = df1, ax = axis3)\naxis3.set_title('Pclass vs Family Size Survival Comparison')","ee1430a8":"#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=df1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=df1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=df1, ax  = qaxis[2])\naxis1.set_title('Sex vs IsAlone Survival Comparison')","7a3ad56e":"#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=df1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=df1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis2)","50f9bc92":"#how does embark port factor with class, sex, and survival compare\n#facetgrid: https:\/\/seaborn.pydata.org\/generated\/seaborn.FacetGrid.html\ne = sns.FacetGrid(df1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()","40f08ac6":"#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(df1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()","a568b427":"df1.head()","c465b6cc":"pred_atributes = ['Pclass', 'Embarked_Coded', 'IsAlone', 'Sex_Coded', 'Title_Coded', 'Age_Coded']\nX = df1[pred_atributes]\nY = df1['Survived']\nX_pred = df_pred1[pred_atributes]","47d237ff":"# first let us split the dataset into Train and Test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)","f810a295":"# Importing the library\nfrom sklearn import ensemble","21c1ade9":"# Using random forest model anf fit with Train Set\nforest = ensemble.RandomForestClassifier(n_estimators=20)\nforest.fit(X_train, Y_train)","7d410ae4":"# Now let's predict the Test set\nYhat_test = forest.predict(X_test)\nYhat_test[:6]","983679d9":"# Now let us compare\ncompare = Y_test == Yhat_test\ncompare.mean()","9a7d723b":"# Now let's fit the whole dataset\nforest.fit(X, Y)\nYhat = forest.predict(X)\n\nfinal = Y == Yhat\nfinal.mean()","a1c61f6a":"# Now let us predict the Predict Dataset\nY_pred = forest.predict(X_pred)\n\n# Create the output dataframe\noutput = pd.concat([df_pred[['PassengerId']], pd.Series(Y_pred)], axis=1)\noutput.rename(columns={0 : 'Survived'}, inplace=True)\noutput.head()","b72f3fca":"# Let's export the CSV\noutput.to_csv('output.csv', index=None)","c35a8cd8":"Some observations of the data:\n- The ___Survived___ attribute is the target variable. Binary, 1 for Yes (Survived) and 0 for No (Not Survived)\n- ___PassangerId___ and _Ticket_ seems to be random, and won't have any relation to the outcome whatsoever\n- The ___Pclass___ is a ordinal for the ticket class, proxy for a socialeconomic class (1 - Upper, 2 - Mid and 3 - Lower)\n- ___Name___ could be used with feature engineering to give some details on gender, status, family size, etc\n- ___Sex___ and ___Embarked___ are nominal and therefore should be converted to dummy variables for our predictive model\n- ___Age___ and ___Fare___ are continuous quantitative data\n- ___SibSp___ represents the number of sibilings or spouse on board, while ___Parch___ represents the number of parents and children on board, could be used to create family size and a \"is alone\" dummy\n- ___Cabin___ is a nominal variable, might be useful to determine the location on the ship, however it appears we have a lot of missing values (204 values out of 891 rows)! Therefore should be excluded from the model in order to not create any bias","e769b8c7":"### 3. Create new features","ce4bd29c":"## Building the model","e857e3f6":"Let us create some new features:\n- Family Size\n- Is Alone\n- Title\n- Fare Bin\n- Age Bin","635478a7":"Le's see how the Title attribute is distributed","f216fe31":"#### Competition Description\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.","0cc01f5a":"Now let us create a copy of the dataset for messing arround it, and also a referencing variable so we can perform operations on both the train and test dataframes","db4afd37":"We will get back to it latter!","8b77aa90":"# Titanic Survival  \n## Competition by Kaggle\n### Juliano Garcia","5b5821c8":"For statistical purpose, let's eliminate the titles with fewer than __10__ entries","8d802631":"## Step 3: Data Cleaning","88ca4a55":"## Step 1: Define the problem!","94332879":"## Exploratory Data Analysis (EDA)","950fc1e6":"## Step 2: Import data and basic understanding","4d4827b3":"It does not appear we have any aberant values, however we might have some outliers on the ___Age___ and ___Fare___ attributes, both since we didn't see any absurd values on the .describe() method we will waint until we finish our exploratory analysis to tackle it","a2796052":"1. Correct aberant values and outliers\n2. Complete missing information\n3. Create new features\n4. Casting variables to correct format","7881602a":"Now let's create our dummies!","187aa22a":"As per the competition description we need to develop an algorithm to predict the survival of a list of passengers","e70dbbb5":"Let's convert the categorical data to dummies for our mathematical analysis","f071be2e":"- ___Age___ we will complete with the mediam\n- ___Fare___ we will complete with the mediam\n- ___Cabin___ we will exclude from the dataframe\n- ___Embarked___ we will replace with the mode","3b38c166":"### 4. Correcting formats","c087f9f2":"### 2. Complete missing information","4832e137":"Let's check the count of null values","5a78e4d9":"### 1. Correct aberant values and outliers"}}