{"cell_type":{"a186eac9":"code","6116f65c":"code","3da0be23":"code","6e9a77b7":"code","a64fe1f9":"code","7b51e8e3":"code","53a81724":"code","5e054803":"code","84aff2c4":"code","c0e6320f":"code","0c82dcd3":"code","b12b8763":"code","00ff90d9":"code","e2ea2135":"code","0f39b867":"code","f238b6ae":"markdown","03885015":"markdown","486f736b":"markdown","c3f7fb38":"markdown","96c5c7e4":"markdown","7b9f9632":"markdown","3345db2c":"markdown","44f5efe1":"markdown","999f10e0":"markdown"},"source":{"a186eac9":"import random\nimport os\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.stats import gaussian_kde\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import fcluster\nfrom sklearn.model_selection import train_test_split\n!pip install similaritymeasures &>\/dev\/null\nfrom similaritymeasures import dtw\n\npd.options.display.max_rows = 500\npd.options.display.max_columns = 500\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6116f65c":"df = pd.read_csv(\n    '\/kaggle\/input\/pga-tour-20102018-data\/PGA_Data_Historical.csv', \n)\ndf.date = pd.to_datetime(df.date)\ndf = df[df.date > '2015-01-01']\n\nStatVar = df['statistic'] + ' - (' + df['variable'] + ')'\ndel df['variable']\ndf['variable'] = StatVar\n\nwide_df = df.set_index(['player_name', 'variable', 'date', 'tournament'])['value'].unstack('variable').reset_index()","3da0be23":"for col in  wide_df.columns[2:]:\n    if 'Money' in col:\n        wide_df[col] = wide_df[col].str.replace('$','')\n        wide_df[col] = wide_df[col].str.replace(',','')\n    try:\n        wide_df[col] = wide_df[col].astype(float)\n    except ValueError:\n        pass\nplt.rcParams[\"figure.figsize\"] = (8,8)\nsns.heatmap(wide_df.corr(), square=True)","6e9a77b7":"wide_df.describe()","a64fe1f9":"player_to_acc_dynamics = {\n    name: group['Accuracy Rating - (RATING)'].dropna().astype(float).array\n    for name, group in wide_df.groupby('player_name')\n    if group.count()['date'] > 15\n}","7b51e8e3":"from scipy import ndimage\n\n\ndef normalize_series(dynamics, target_length=100):\n    \"Turn `dynamics` of length n to length `num_chunks` using linear interpolation\"\n    length = len(dynamics)\n    return ndimage.gaussian_filter1d(np.interp(np.linspace(0, length-1, target_length), range(length), dynamics), sigma=2.5)\n\ndef get_matrix(player_to_acc_dynamics, target_length=50):\n    matrix = np.ndarray((len(player_to_acc_dynamics), target_length))\n    for i, (_, acc_dynamics) in enumerate(player_to_acc_dynamics.items()):\n        acc_dynamics = normalize_series(acc_dynamics, target_length=target_length)\n        matrix[i, :] = acc_dynamics\n    return matrix\n\nmatrix = get_matrix(player_to_acc_dynamics)\nfor row in matrix[:10]:\n    plt.plot(row)\nplt.legend(player_to_acc_dynamics.keys(), bbox_to_anchor=(0.4, 1, 1., .102))\nplt.title('Arcs on a common (quantile-based) x axis')","53a81724":"num_players = matrix.shape[0]\ndistances = np.ndarray((num_players, num_players))\nfor i in range(num_players):\n    for j in range(num_players):\n        distances[i, j], _ = dtw(np.expand_dims(matrix[i], axis=0), \n                                 np.expand_dims(matrix[j], axis=0))\nplayer_names = list(player_to_acc_dynamics.keys())\nsns.heatmap(distances[:25, :25], square=True, xticklabels=player_names[:25], yticklabels=player_names[:25], cbar=False)","5e054803":"model = AgglomerativeClustering(distance_threshold=0, n_clusters=None, affinity='precomputed', linkage='average')\nmodel = model.fit(distances)\ndistance = np.arange(model.children_.shape[0])\nno_of_observations = np.arange(2, model.children_.shape[0]+2)\nlinkage_matrix = np.column_stack([model.children_, model.distances_, no_of_observations]).astype(float)\n\ng = sns.clustermap(col_linkage=linkage_matrix, row_cluster=True, col_cluster=False, data=matrix, cbar=False, \n                   yticklabels=[], \n                   figsize=(15, 10), cmap=\"YlGnBu\")","84aff2c4":"small_model = AgglomerativeClustering(distance_threshold=0, n_clusters=None, affinity='precomputed', linkage='average')\nsmall_model = small_model.fit(distances[:25, :25])\ndistance = np.arange(model.children_.shape[0])\nno_of_observations = np.arange(2, small_model.children_.shape[0]+2)\nlinkage_matrix_small = np.column_stack([small_model.children_, small_model.distances_, no_of_observations]).astype(float)\n\ng = sns.clustermap(col_linkage=linkage_matrix_small, row_cluster=True, col_cluster=False, data=matrix[:25], cbar=False, \n                   yticklabels=player_names[:25], \n                   figsize=(10, 5), cmap=\"YlGnBu\")","c0e6320f":"sns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (20,8)\ndendrogram(linkage_matrix, color_threshold=4, orientation='left')\nclusters = fcluster(linkage_matrix, 3.3 ,'distance')","0c82dcd3":"print(f'Num clusters: {len(set(clusters))}')\nfor cluster, player in zip(clusters[:25], player_names[:25]):\n    # printing just 25 first players\n    print(f'{player} belongs to cluster {cluster}')","b12b8763":"dataset = wide_df.groupby('player_name').agg('mean')\ndataset = dataset[dataset['Accuracy Rating - (RATING)'].notna()]\nX, y = dataset.drop(columns=['Accuracy Rating - (RATING)']), dataset['Accuracy Rating - (RATING)']\nX_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=0.2)\nprint(X_dev.shape, X_test.shape)","00ff90d9":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.model_selection import cross_validate\n\nbaseline = DummyRegressor(strategy='mean')\n\npreprocessing = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')), # Filling in nans with mean value of each column\n    ('scaler', MinMaxScaler()),\n])\n\ndef make_ridge(alpha):\n    return Pipeline([\n        ('preprocessing', preprocessing),\n        ('regression', Ridge(alpha=alpha))\n    ])\n\ndef make_lasso(alpha):\n    return Pipeline([\n        ('preprocessing', preprocessing),\n        ('regression', Lasso(alpha=alpha))\n    ])\n\ndef make_forrest(num_estimators):\n    return Pipeline([\n        ('preprocessing', preprocessing),\n        ('regression', RandomForestRegressor(n_estimators=num_estimators))\n    ])\n\ndef make_gradient_boosting(n_estimators, max_depth):\n    return Pipeline([\n        ('preprocessing', preprocessing),\n        ('regression', GradientBoostingRegressor(n_estimators=n_estimators, max_depth=max_depth))\n    ])\n\ndef make_adaboost(n_estimators):\n    return Pipeline([\n        ('preprocessing', preprocessing),\n        ('regression', AdaBoostRegressor(n_estimators=n_estimators))\n    ])\n\nresults = pd.DataFrame(columns=['model name', '$R^2$', 'seed', 'mode'])\nMODELS = {\n    'always predict mean': baseline,\n    'ridge regression $\\\\alpha = 0.0001$': make_ridge(0.0001),\n    'ridge regression $\\\\alpha = 0.00001$': make_ridge(0.00001),\n    'ridge regression $\\\\alpha = 0.0000001$': make_ridge(0.000005),\n    'lasso regression $\\\\alpha = 0.01$': make_lasso(0.01),\n    'lasso regression $\\\\alpha = 0.001$': make_lasso(0.001),\n    'lasso regression $\\\\alpha = 0.0005$': make_lasso(0.0005),\n    'random forrest of 15 trees': make_forrest(15),\n    'random forrest of 25 trees': make_forrest(25),\n    'random forrest of 50 trees': make_forrest(50),\n    'gradient_boosting 50 trees depth 5': make_gradient_boosting(n_estimators=50, max_depth=5),\n    'gradient_boosting 200 trees depth 5': make_gradient_boosting(n_estimators=200, max_depth=5),\n    'gradient_boosting 50 trees depth 2': make_gradient_boosting(n_estimators=50, max_depth=5),\n    'gradient_boosting 200 trees depth 2': make_gradient_boosting(n_estimators=200, max_depth=2),\n    'gradient_boosting 200 trees depth 3': make_gradient_boosting(n_estimators=250, max_depth=3),\n    'adaboost 100 trees': make_adaboost(100),\n    'adaboost 200 trees': make_adaboost(200),\n}\n\nfor name, model in MODELS.items():\n    result_dict = cross_validate(model, X_dev, y_dev, return_train_score=True)\n    test_scores = result_dict['train_score'].mean()\n    for seed, score in enumerate(result_dict['train_score']):\n        results.loc[len(results)] = (name, score, seed, 'train')\n        \n    test_scores = result_dict['test_score'].mean()\n    for seed, score in enumerate(result_dict['test_score']):\n        results.loc[len(results)] = (name, score, seed, 'test') \n    # print(f'{name}, R^2 = {test_score:.2f}')\nresults","e2ea2135":"plt.rcParams[\"figure.figsize\"] = (30, 6)\nplt.xticks(rotation=90)\nplt.ylim(0.6, 1)\n_ = sns.barplot(data=results, x='model name', y='$R^2$', hue='mode')","0f39b867":"best_model = make_lasso(0.01)\nbest_model.fit(X_dev, y_dev)\nbest_model.score(X_test, y_test)","f238b6ae":"After some experiments, I decided to use agglomerative clustering with average linkage on my pre-computer distance matrix. Agglomerative clustering stars with assigning each player to their unique cluster and iteratively merging the clusters. Average linkage means that mering clusters A and B is done by considering the average distance $d(a, b)$ between every pair of points $a \\in A, b \\in B$. At each iteration we merge clusters A and B with the highest average distance.\n\nBelow I display a dendrogram plotted over my data representations. The colorful part of the figure is not a heatmap (I deliberately changed the color to blue to indicate that). Each row represents a sigle player encoded as a time series of their accuracy scores. Each columns is a percentile of player's recorder career. The dendrogram clusters similar career curves from bottom up. The rows on the plot are order in such a way that similar ones are next to each other. Therefore, trends are clearbly visible as darker or lighter regions spread diagonally, which correspond to both players having a similar accuracy at similar points of their careers. Let this plot be a visual justification that my clustering algorithm does indeed rely on meaningful signal in the data.","03885015":"As we can see, there are w lot of heavily correlated features in the dataset.","486f736b":"## Close-up","c3f7fb38":"The performance of gradient boosting and adaboost does not depend much on the number of trees (if there are at least 20 trees). Good performance of Lasso compared to Ridge suggest that a small subset of the features is really important. Overall, I would recommend to stick with linear regression with L1 regularisation (Lasso) for its simplicity. We can now evaluate this final model on the held out test set.\n\n","96c5c7e4":"# Clustering\n\nClustering of heterogenous tabular data such as this one is inherently difficult as it's hard to come up with a meaninful notion of distance over tabular data. Categorical variables require vector representation while ordinal and numeric variables are no better: they can (and, in our case, do) vastly differ in magnitude, variance and relative importance. Simply treating a set of numeric variables (such as total earning and total putting) as an Euclidean space will rarely work.\n\nOne idea is to use represent each player by a series of their accuracy ratings over time. I will focus on players playing on at at least 15 distinct dates. Because this yields vectors of different length across players, I will normalise them to the same length - 50 values - using linear interpolation. I will also apply smoothing by applying a Gaussian kernel with $\\sigma = 2.5$. Then, each player is represented by a curve desribing the dynamics of their accuracy. The hypothesis is that an arc desrcibes at which point of their career a player is. We would like to match players at similar points of their careers.","7b9f9632":"# Supervised learning part\n\nI will focus on on numeric columns as features and I will try to predict the accuracy rate of each player averaged over their carrer. I will held out a test set and run a 5-fold cross-validation on the remaining dev set. ","3345db2c":"A concrete clustering is obtained by cutting a dendgregram at a given height. There are principled methods of selecting it quantitatively, but here I have just visually chosen 3.3 based on a plotted dendogram.","44f5efe1":"Since the arcs have the same length, I could have used Euclidean distance for comparing them. However, I've found experimentally that distance metrics optimised for time series yield more meaningful scores. Here I use [dynamic time warping](https:\/\/en.wikipedia.org\/wiki\/Dynamic_time_warping), which account for non-linear warping (intuitively, accelerations and decelerations) of the arcs. ","999f10e0":"# Exploratory data analysis"}}