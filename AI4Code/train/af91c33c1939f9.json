{"cell_type":{"5988bb2e":"code","150f4f43":"code","5b7e52e1":"code","e80cc842":"code","8d288d8b":"code","24c93564":"code","d4efd92d":"code","d558e758":"code","4917feaf":"code","cbc2f44f":"code","cdf495e4":"code","d26deb55":"code","7bede05a":"code","b17f1baf":"code","4157b61e":"code","98a17718":"code","57b31aca":"code","1d7e02ea":"code","035d5a35":"code","52cb5720":"code","c8643bf3":"code","9e1c29b7":"code","29c01b28":"code","dc198833":"code","8854a560":"code","023f6565":"markdown","98cf58b9":"markdown","ef767b09":"markdown","aa3d4c41":"markdown","456687ae":"markdown","4f3028cd":"markdown","2f2feb33":"markdown","504668a8":"markdown","1c5adb11":"markdown","facbc3b6":"markdown","81b57f84":"markdown","c4ab8f7c":"markdown","ce1d8f26":"markdown","b449d3b4":"markdown","ffaed4b7":"markdown","812eb2e5":"markdown","d2de96e6":"markdown","ac9c59f8":"markdown","59aad7a9":"markdown","c7078cd1":"markdown","ac4d1201":"markdown"},"source":{"5988bb2e":"! pip install gensim==3.4.0","150f4f43":"import os\nimport pandas as pd\nfrom gensim.models.fasttext import FastText as FT_gensim\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\nfrom uuid import uuid4\nimport torch\nimport re\nimport json\nfrom tqdm import tqdm\nimport datetime\nimport pprint\nimport random\nimport string\nimport sys","5b7e52e1":"# main folder of Covid19 Dataset\ndirs = [\"biorxiv_medrxiv\", \"comm_use_subset\", \"custom_license\", \"noncomm_use_subset\"]\ndocs = []\nbase_path = \"\/kaggle\/input\/CORD-19-research-challenge\"\nfor d in dirs:\n    for file in tqdm(os.listdir(f\"{base_path}\/{d}\/{d}\/pdf_json\")):\n        file_path = f\"{base_path}\/{d}\/{d}\/pdf_json\/{file}\"\n        json_file = json.load(open(file_path,\"rb\"))\n        \n        title = json_file[\"metadata\"][\"title\"]\n        try : \n            abstract = \"\\n\\n\".join([text[\"text\"] for text in json_file[\"abstract\"]])\n        except : \n            abstract = \"\"\n        full_text = \"\\n\\n\".join([text[\"text\"] for text in json_file[\"body_text\"]])\n        docs.append([title, abstract, full_text])\n# Pandas Dataframe containing the title, abstract and body text\npapers_df = pd.DataFrame(docs, columns = [\"title\", \"abstract\", \"full_text\"])","e80cc842":"papers_df = papers_df.dropna()\npapers_df","8d288d8b":"# Upload the model \nmodel_load_name = 'final_model_gensim.pt'\npath = F\"\/kaggle\/input\/similaritymodels2new\/Similarity\/FinalModel\/{model_load_name}\"\nmodel = FT_gensim.load(path)","24c93564":"def token_similarity(token1, token2): \n    \"\"\"\n    calculate similarity between sentences based on their embeddings\n        ----------------\n    Args : \n        token1 : String\n        token1 : String\n        ---------------\n    returns:\n        float between 0 and 100, representing the percentage of similarity\n    \"\"\"\n    try :\n        token1 = re.sub('[^a-zA-z0-9\\s]', '' , token1).lower()\n        token2 = re.sub('[^a-zA-z0-9\\s]', '' , token2).lower()\n        return model.similarity(token1, token2)\n    except : \n        return 0\n\ndef get_context(query, search_on, model = model, df = papers_df):\n    \"\"\"\n    maps similarity function for given query to either all paper abstracts or to all paper titles to extract the closes paper to answer the query.\n        ----------------\n    Args : \n        query : String\n        search_on : String\n        model : Gensim model Object\n        df : pandas Dataframe\n        ---------------\n    returns:\n        Tuple containing full_text, title and similarity degree to closeset paper to query.\n    \"\"\"\n    \n    if search_on in [\"title\", \"abstract\"]:\n        df[\"similarity_to_query\"] = df[search_on].apply(lambda x : token_similarity(x, query))\n        result = df.nlargest(1, ['similarity_to_query']).reset_index(drop = True)\n        return result[\"full_text\"][0].replace(\"\\n\", \" \"), result[\"title\"][0], result[\"similarity_to_query\"][0]\n    else :\n        raise Exception(\"search_on argument should be in ['title', 'abstract']\")","d4efd92d":"query = \"what are risk factors COVID-19?\"\nsearch_on = \"title\"\nimport time\nt1 = time.time()\ncontext = get_context(query, search_on, model, papers_df)\nt2 = time.time()\nprint(f\"query took {t2-t1} seconds\")\nprint(context)","d558e758":"\nquestion_list = \"\"\" What is known about transmission, incubation, and environmental stability\nWhat do we know about natural history, transmission, and diagnostics for the virus \nWhat have we learned about infection prevention and control\nWhat is Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\nWhat is Prevalence of asymptomatic shedding and transmission (e.g., particularly children).\nWhat is Seasonality of transmission.\nWhat is Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\nWhat is Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\nWhat is Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\nWhat is Natural history of the virus and shedding of it from an infected person\nWhat is Implementation of diagnostics and products to improve clinical processes\nWhat is Disease models, including animal models for infection, disease and transmission\nWhat is Tools and studies to monitor phenotypic change and potential adaptation of the virus\nWhat is Immune response and immunity\nWhat is Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\nWhat is Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\nWhat is Role of the environment in transmission\n\"\"\"\n\nquestion_list = question_list.split(\"\\n\")\nqa_dataframe = pd.DataFrame({\"Questions\": question_list})","4917feaf":"qa_dataframe[\"Context\"] = qa_dataframe[\"Questions\"].apply(lambda x : get_context(x, \"title\", model, papers_df)[0])","cbc2f44f":"qa_dataframe","cdf495e4":"!pip install tensorflow==1.15.2","d26deb55":"import tensorflow as tf\nprint(tf.version)","7bede05a":"covidQuestions = qa_dataframe","b17f1baf":"! mkdir temp","4157b61e":"covidQuestions.to_json('temp\/QnAInput.json')","98a17718":"#Test File\n\nimport json\nfrom uuid import uuid4\n\ndef restructure_json_Test(input_data):  \n    \n    \"\"\"\n    Restructure input data to be compatible with BIOBERT Qna test data format.\n        ----------------\n    Args : \n        input_data : String\n        ---------------\n    returns:\n        formatted data in dictionary\n    \"\"\"\n    output = dict()\n    data = []\n    paragraphs = []\n    for i in input_data[\"Questions\"].keys():\n        qas = []\n        answers = []\n      \n        qas.append({\"id\" : str(uuid4()),\n                   \"question\" : input_data[\"Questions\"][i],\n                   })\n        paragraphs = paragraphs + [{\"qas\" : qas, \"context\" : input_data[\"Context\"][i]}]\n    data = data + [{\"paragraphs\" : paragraphs, \"title\" : \"BioASQ6b\"}]\n    output[\"data\"] = data\n    output[\"version\"] = \"BioASQ6b\"\n    return output","57b31aca":"with open('temp\/QnAInput.json',encoding='ISO-8859-1' ) as f:\n    input_data = json.load(f)","1d7e02ea":"outputData = restructure_json_Test(input_data)\nwith open('temp\/QnAInputTest.json', 'w',  encoding='ISO-8859-1') as json_file:\n    json.dump(outputData, json_file)","035d5a35":"! ls -l '\/kaggle\/input\/biobertcode\/GitRepo\/BIOBERT\/bioasq-biobert\/run_factoid.py'\n! ls -l '\/kaggle\/input\/biobertconfig\/biobertconfig\/vocab.txt'\n! ls -l '\/kaggle\/input\/biobertconfig\/biobertconfig\/bert_config.json'\n! ls -l '\/kaggle\/input\/biobertmodel2\/BERT-pubmed-1000000-SQuAD2\/model.ckpt-14470.index'\n! ls -l 'temp\/QnAInputTest.json'","52cb5720":"! python \/kaggle\/input\/biobertcode\/GitRepo\/BIOBERT\/bioasq-biobert\/run_factoid.py \\\n     --do_train=False \\\n     --do_predict=True \\\n     --vocab_file=\/kaggle\/input\/biobertconfig\/biobertconfig\/vocab.txt \\\n     --bert_config_file=\/kaggle\/input\/biobertconfig\/biobertconfig\/bert_config.json \\\n     --init_checkpoint=\/kaggle\/input\/biobertmodel2\/BERT-pubmed-1000000-SQuAD2\/model.ckpt-14470.index \\\n     --max_seq_length=512 \\\n     --max_answer_length=200 \\\n     --train_batch_size=12 \\\n     --learning_rate=5e-6 \\\n     --doc_stride=128 \\\n     --num_train_epochs=1.0 \\\n     --do_lower_case=False \\\n     --train_file=$BIOASQ_INPUT_DIR\/BioASQ-6b\/train\/Full-Abstract\/BioASQ-train-factoid-6b-full-annotated.json \\\n     --predict_file=temp\/QnAInputTest.json \\\n     --output_dir=\/kaggle\/output\/kaggle\/factoid_output\/prediction\/","c8643bf3":"ls -l \/kaggle\/output\/kaggle\/factoid_output\/prediction\/predictions.json","9e1c29b7":"with open('\/kaggle\/output\/kaggle\/factoid_output\/prediction\/predictions.json',encoding='ISO-8859-1' ) as f:\n    output_data = json.load(f)","29c01b28":"output_data","dc198833":"with open('temp\/QnAInputTest.json',encoding='ISO-8859-1' ) as f:\n    output_data_questions = json.load(f)","8854a560":"paragraphs = output_data_questions['data'][0]['paragraphs']\n\nfor index, item in enumerate(paragraphs):\n    id = item['qas'][0]['id']\n    question = item['qas'][0]['question']\n    answer = output_data[id]\n    print(\"Question \", str(index + 1), \" : \" , question ,\"\\n Answer : \", answer)\n    print(\"\\n\")","023f6565":"### Since BERT model was trained using tf1.x, we can not use transfer learning of using pre-trained weights on tensorflow 2.0 so need to installing Tensorflow version 1.x","98cf58b9":"### Function to campare similary between input query and each paper title or paper abstract","ef767b09":" # BioBERT - BERT model trained on corpus: \n ## This repository provides the code for fine-tuning BioBERT, a biomedical language representation model designed for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering, Github code: https:\/\/github.com\/dmis-lab\/biobert\n### We have used question answering side of BioBERT model to find answer in COVID-19 Challenge","aa3d4c41":"### Lets go through Task 2 Questions 1 by 1 and predictions are mapped to output side by side answers","456687ae":"Save the restructured data in JSON format","4f3028cd":"Format the dataframe to create a JSON as input to BIOBERT QnA model","2f2feb33":"Extracting context from the data ingestion module above using Fast Text model","504668a8":"### Indexing all 45,000 research papers into pandas dataframe with Title, Abstract and Full text body","1c5adb11":"# Test Data preparation","facbc3b6":"Create temp directory to store temporary files","81b57f84":"### Upload Pretrained FastText Embeddings Model","c4ab8f7c":"### Eliminate empty papers","ce1d8f26":"Verify all files used for prediction","b449d3b4":"### Test the query","ffaed4b7":"# Prediction using Fine Tuned BIOBERT model\n\nBioBERT model has been uploaded to https:\/\/www.kaggle.com\/varshnes\/biobertmodel2","812eb2e5":"COVID-19 Open Research Dataset (CORD-19) Analysis\n======\n\nCOVID-19 Open Research Dataset (CORD-19) is a free resource of scholarly articles, aggregated by a coalition of leading research groups, about COVID-19 and the coronavirus family of viruses. The dataset can be found on [Semantic Scholar](https:\/\/pages.semanticscholar.org\/coronavirus-research) and there is a research challenge on [Kaggle](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge).\n\nBERT - Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks: https:\/\/github.com\/google-research\/bert, We use derivation of BERT model as BioBERT to come up with answers for set of questions listed in the challenge, First step is to use sentence embeddings by using FastText in gensim and come up with context for questions.\n\n\nSecond step is to use that context and feed set of questions and their respective context in BioBERT model","d2de96e6":"### Task-1 Questions to be answered","ac9c59f8":"### Installing Gensim library which will be used to load FastText embeddings for Sentence vectors to find context, \n\nNote: need to maintain version as 3.8.0 had compaitibility issues","59aad7a9":"TPU support is not available for Tensorflow version 1.x in Kaggle.","c7078cd1":"Check the prediction file in output folder","ac4d1201":"## BIOBERT Fine-tuning and Prediction"}}