{"cell_type":{"6fcf1dfb":"code","a7568802":"code","37398475":"code","b780a7bd":"code","bb50bd49":"code","9cfb75b3":"code","f0512b7a":"code","8be64e30":"code","e0deac4c":"code","0f18d1a6":"code","ba9c49da":"code","85400d6e":"code","86bf798b":"code","8f592318":"code","1ef51ab3":"code","ce24fc93":"code","2db6d0c0":"code","68cbf5a8":"code","a3380f7c":"code","9b5088f0":"code","4ce1c176":"code","35040653":"code","33cecc83":"code","8a8808fb":"code","a3c57076":"code","21d52f1a":"code","3edb9e02":"code","f2631af1":"code","ec50f0bb":"code","9c898d22":"code","a76c4736":"code","5e8c6c02":"code","2f022892":"code","1f39337c":"code","f8067f59":"code","5100fd37":"code","270524b0":"code","acec8fe0":"code","8b560e7a":"code","92556e55":"code","fe7143cc":"code","ec1e56f8":"code","f18c204d":"markdown","97a92778":"markdown","0f3afd94":"markdown","cc864d0f":"markdown","3cd1e0a7":"markdown","08f28fd6":"markdown","bd5e22ba":"markdown","08ac0ccf":"markdown","3ecc0661":"markdown","7fbd4e41":"markdown","6a3a3634":"markdown","c414d8ea":"markdown","4cfef0c9":"markdown","c6913dad":"markdown"},"source":{"6fcf1dfb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nimport os\nimport time","a7568802":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","37398475":"tweet_train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntweet_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntarget = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","b780a7bd":"tweet_train.head()","bb50bd49":"null_vals = pd.DataFrame(columns = {\"train\",\"test\"})\nnull_vals[\"train\"] = tweet_train.isnull().sum()\nnull_vals[\"test\"] = tweet_test.isnull().sum()\nprint(null_vals)","9cfb75b3":"tweet_train.isnull().sum()","f0512b7a":"tweet_train.head()","8be64e30":"tweet_train.to_csv('tweet_train_data.csv', index=False)","e0deac4c":"tweet_test.isnull().sum()","0f18d1a6":"tweet_train[\"target\"].value_counts()","ba9c49da":"real = len(tweet_train[tweet_train[\"target\"] == 1])\nfake = len(tweet_train[tweet_train[\"target\"] == 0])\n\ndf_count_pie = pd.DataFrame({'Class' : ['Real', 'Not Real'], \n                             'Counts' : [real, fake]})\ndf_count_pie.Counts.groupby(df_count_pie.Class).sum().plot(kind='pie',autopct = '%1.1f%%')\nplt.axis('equal')\nplt.title(\"Tweeta which are Real or Not\")\nplt.show()","85400d6e":"tweet_train[\"text\"][:3]","86bf798b":"stopword = stopwords.words('english')","8f592318":"def text_processing(text):\n    text = re.sub(\"[^\\w\\d\\s]+\",' ',text)\n    text = text.lower()\n    tok = nltk.word_tokenize(text)\n    words = [word for word in tok if word not in stopword]\n    return words","1ef51ab3":"def join_words(words):\n    words = ' '.join(words)\n    return words","ce24fc93":"#preprocess the train text data\ntweet_train[\"text_pre\"] = tweet_train[\"text\"].apply(lambda x: text_processing(x))\ntweet_train[\"text\"] = tweet_train[\"text_pre\"].apply(lambda x: join_words(x))\n#preprocess the test text data\ntweet_test[\"text_pre\"] = tweet_test[\"text\"].apply(lambda x: text_processing(x))\ntweet_test[\"text\"] = tweet_test[\"text_pre\"].apply(lambda x: join_words(x))","2db6d0c0":"tweet_train.head(3)","68cbf5a8":"tweet_train.drop(\"text_pre\",axis = 1,inplace = True)","a3380f7c":"tweet_test.drop(\"text_pre\",axis = 1,inplace = True)","9b5088f0":"tweet_train.head()","4ce1c176":"tweet_test.head()","35040653":"tweet_train.to_csv('tweet_train_data.csv', index=False)","33cecc83":"tweet_test.to_csv('tweet_test_data.csv', index=False)","8a8808fb":"\ndataa = pd.read_csv(\"..\/input\/datasets1\/train_data.csv\")\ndataa[:3]","a3c57076":"#REPLACE THIS WITH YOUR OWN GOOGLE PROJECT ID\nPROJECT_ID = 'tweets-real-fake'\n#REPLACE THIS WITH A NEW BUCKET NAME. NOTE: BUCKET NAMES MUST BE GLOBALLY UNIQUE\nBUCKET_NAME = 'tweets-automl-project1'\n#Note: the bucket_region must be us-central1.\nBUCKET_REGION = 'us-central1'","21d52f1a":"import os\n#os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"..\/input\/google-creden\/tweets-real-fake-f2c2c00c9276.json\"\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"..\/input\/fakenewsgc\/tweets-real-fake-f73343fb0388.json\"","3edb9e02":"from google.cloud import storage, automl_v1beta1 as automl\n\nstorage_client = storage.Client(project=PROJECT_ID)\ntables_gcs_client = automl.GcsClient(client=storage_client, bucket_name=BUCKET_NAME)\nautoml_client = automl.AutoMlClient()\n# Note: AutoML Tables currently is only eligible for region us-central1. \nprediction_client = automl.PredictionServiceClient()\n# Note: This line runs unsuccessfully without each one of these parameters\ntables_client = automl.TablesClient(project=PROJECT_ID, region=BUCKET_REGION, client=automl_client, gcs_client=tables_gcs_client, prediction_client=prediction_client)","f2631af1":"bucket = storage.Bucket(storage_client, name=BUCKET_NAME)\nif not bucket.exists():\n    bucket.create(location=BUCKET_REGION)","ec50f0bb":"def upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Uploads a file to the bucket. https:\/\/cloud.google.com\/storage\/docs\/ \"\"\"\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n    \ndef download_to_kaggle(bucket_name,destination_directory,file_name,prefix=None):\n    \"\"\"Takes the data from your GCS Bucket and puts it into the working directory of your Kaggle notebook\"\"\"\n    os.makedirs(destination_directory, exist_ok = True)\n    full_file_path = os.path.join(destination_directory, file_name)\n    blobs = storage_client.list_blobs(bucket_name,prefix=prefix)\n    for blob in blobs:\n        blob.download_to_filename(full_file_path)","9c898d22":"#upload_blob(BUCKET_NAME, '\/kaggle\/working\/tweet_train_data.csv', 'train.csv')\n#upload_blob(BUCKET_NAME, '\/kaggle\/working\/tweet_test_data.csv', 'test.csv')\nupload_blob(BUCKET_NAME, '..\/input\/datasets1\/train_data.csv', 'train.csv')\nupload_blob(BUCKET_NAME, '..\/input\/datasets1\/test.csv', 'test.csv')","a76c4736":"dataset_display_name = 'fake_news'\nnew_dataset = False\ntry:\n    dataset = tables_client.get_dataset(dataset_display_name=dataset_display_name)\nexcept:\n    new_dataset = True\n    dataset = tables_client.create_dataset(dataset_display_name)","5e8c6c02":"if new_dataset:\n    gcs_input_uris = ['gs:\/\/' + BUCKET_NAME + '\/train.csv']\n\n    import_data_operation = tables_client.import_data(\n        dataset=dataset,\n        gcs_input_uris=gcs_input_uris\n    )\n    print('Dataset import operation: {}'.format(import_data_operation))\n\n    # Synchronous check of operation status. Wait until import is done.\n    import_data_operation.result()\nprint(dataset)","2f022892":"model_display_name = 'tutorial_model_automl7'\nTARGET_COLUMN = 'label'\nID_COLUMN = 'index'\n\n# TODO: File bug: if you run this right after the last step, when data import isn't complete, you get a list index out of range\n# There might be a more general issue, if you provide invalid display names, etc.\n\ntables_client.set_target_column(\n    dataset=dataset,\n    column_spec_display_name=TARGET_COLUMN\n)","1f39337c":"for col in tables_client.list_column_specs(PROJECT_ID,BUCKET_REGION,dataset.name):\n    if TARGET_COLUMN in col.display_name or ID_COLUMN in col.display_name:\n        continue\n    tables_client.update_column_spec(PROJECT_ID,\n                                     BUCKET_REGION,\n                                     dataset.name,\n                                     column_spec_display_name=col.display_name,\n                                     type_code=col.data_type.type_code,\n                                     nullable=True)","f8067f59":"TRAIN_BUDGET = 4000\nprint(\"Training started\")\nmodel = None\ntry:\n    model = tables_client.get_model(model_display_name=model_display_name)\nexcept:\n    response = tables_client.create_model(\n        model_display_name,\n        dataset=dataset,\n        train_budget_milli_node_hours=TRAIN_BUDGET,\n        exclude_column_spec_names=[ID_COLUMN, TARGET_COLUMN]\n    )\n    print('Create model operation: {}'.format(response.operation))\n    # Wait until model training is done.\n    model = response.result()\n    \nprint(model)\nprint(\"Training completed\")\n","5100fd37":"gcs_input_uris = 'gs:\/\/' + BUCKET_NAME + '\/test.csv'\ngcs_output_uri_prefix = 'gs:\/\/' + BUCKET_NAME + '\/predictions-4'\n\nbatch_predict_response = tables_client.batch_predict(\n    model=model, \n    gcs_input_uris=gcs_input_uris,\n    gcs_output_uri_prefix=gcs_output_uri_prefix,\n)\nprint('Batch prediction operation: {}'.format(batch_predict_response.operation))\n# Wait until batch prediction is done.\nbatch_predict_result = batch_predict_response.result()\nbatch_predict_response.metadata","270524b0":"gcs_output_folder = batch_predict_response.metadata.batch_predict_details.output_info.gcs_output_directory.replace('gs:\/\/' + BUCKET_NAME + '\/','')\ndownload_to_kaggle(BUCKET_NAME,'\/kaggle\/working','tables_1.csv', prefix=gcs_output_folder)","acec8fe0":"preds_df = pd.read_csv(\"tables_1.csv\")\nsub_automl_2 = pd.DataFrame()\nsub_automl_2[\"id\"] = preds_df['id']\nsub_automl_2['target'] = np.where((preds_df['target_0_score'] >= preds_df['target_1_score']),0,1)\nsub_automl_2.to_csv('submission_automl6.csv', index=False)\nprint(sub_automl_2[:3])","8b560e7a":"wrd_vec = CountVectorizer()\nword_vector = wrd_vec.fit_transform(tweet_train[\"text\"])\ntest_vector = wrd_vec.transform(tweet_test[\"text\"])","92556e55":"gnb = GaussianNB()\ngnb.fit(word_vector.toarray(),tweet_train[\"target\"])","fe7143cc":"pred = gnb.predict(test_vector.toarray())","ec1e56f8":"log_score = cross_val_score(gnb,word_vector.toarray(),tweet_train[\"target\"],cv = 3)\nprint(log_score)","f18c204d":"### Exploratory data analysis\n\n1. Real v\/s Fake tweets: Lets see how many real and fake tweets are present in the train dataset.","97a92778":"### Text processing using NLP\n\nPreprocessing the text data in the train and test dataset.\n","0f3afd94":"### Automl ","cc864d0f":"## Real or Not? NLP with Disaster Tweets\n\nStudy shows vast majority of people using Twitter like platform during disasters and breaking news events spread false information without ever getting it a second thought. Researchers examined three types of behavior in these users: they could either spread the false news, seek to confirm it, or cast doubt upon it. It becomes evident to predict if the tweet is real or fake to prevent false information reaching out to the mass especially during natural disasters.\n\n![](http:\/\/)This notebook tries to implement different ML algorithms to predcit real or fake tweets.\n\n","3cd1e0a7":"The output shows location has many null values and keyword with few null values in both datasets.","08f28fd6":"The above output shows the text column which needs pre-processing like convert the words to lowercase, remove puntuations and stopwords, tokenize the text to words.","bd5e22ba":"Initialize the clients and move your data to GCS","08ac0ccf":"Run the model for 4hours","3ecc0661":"convert the tweet_train to csv file ","7fbd4e41":"Train an AutoML Model","6a3a3634":"Read the train data","c414d8ea":"### Import all necessary libraries.","4cfef0c9":"### Load the train, test and submission data ","c6913dad":"### Data cleaning\nLets create a dataframe and find the number of missing values in train and test datasets."}}