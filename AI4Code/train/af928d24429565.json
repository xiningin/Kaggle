{"cell_type":{"5477dc79":"code","7aff5d18":"code","df46aff5":"code","9cebef28":"code","3f8eca18":"code","dccc55fe":"code","318fc5f4":"code","bd818471":"code","31a45e72":"code","8da54537":"code","ec8dde33":"code","7b3432e6":"code","e6dd8220":"code","3d3ca958":"code","a1cf1219":"code","e1666670":"code","d310328b":"code","755a1619":"code","99152279":"code","a7b4711d":"code","7d0cd8da":"code","1bee7ba8":"code","ac15f756":"code","87fe1833":"code","e671c71e":"code","93bad05f":"code","d32bfbf7":"code","bf21122b":"code","be988244":"code","b7cfd4fc":"code","aaba3b5a":"code","7a1f8189":"code","df8f6933":"code","1a0efdbd":"code","50110b9e":"code","979af40f":"code","a2b00b82":"code","b19acb3f":"code","71d09358":"markdown","23b285d9":"markdown","b21aeb65":"markdown","de082d79":"markdown","a5563cfd":"markdown","d3553c1e":"markdown","5750d7ad":"markdown","11a6dff6":"markdown","0a6c8228":"markdown","7a035f94":"markdown","e7d999e3":"markdown","2e1ce27b":"markdown","ae72a0ee":"markdown","661b95e2":"markdown","5de12e1f":"markdown"},"source":{"5477dc79":"import joblib\nimport pandas as pd\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')","7aff5d18":"TRAINING_PATH='..\/input\/mushroom-classification\/mushrooms.csv'\n\ndf=pd.read_csv(TRAINING_PATH)\ndf.head()","df46aff5":"TRAINING_FOLDS_PATH='.\/'\n\nimport pandas as pd\n\ndf_train=pd.read_csv(TRAINING_PATH)\ndf_train.head()\n\ndf_train['class'].value_counts()\n\ndf_train['kfolds']=-1\ndf_train=df_train.sample(frac=1).reset_index(drop=True)\ndf_train.head()\n\nfrom sklearn import model_selection\n\nstrat_kf=model_selection.StratifiedKFold(n_splits=5)\n\nfor fold,(trn_,val_) in enumerate(strat_kf.split(X=df_train,y=df_train['class'])):\n  df_train.loc[val_,'kfolds']=fold\ndf_train.head()\n\ndf_train.to_csv(TRAINING_FOLDS_PATH+'train_folds.csv')","9cebef28":"TRAINING_PATH='.\/train_folds.csv'\nMODEL_PATH='.\/'\nSUBMISSION_FILES_PATH='.\/Submissions\/'","3f8eca18":"df=pd.read_csv(TRAINING_PATH)\ndf.head()","dccc55fe":"df.describe()","318fc5f4":"# Count the number of null values in each column\ndf.isna().sum()","bd818471":"# Total number of unique values in each column\ndf.nunique()","31a45e72":"len(df)","8da54537":"df=df.drop(['Unnamed: 0'],axis=1)\ndf.head()","ec8dde33":"# Check for class imbalance \ndf['class'].value_counts()","7b3432e6":"# Checking for any numerical data. If present, it has to be scaled etc.\n\ncolumns = df.columns\nnumerical_columns = df._get_numeric_data().columns\nnumerical_columns","e6dd8220":"import pandas as pd\nimport numpy as np\nimport scipy.stats as ss\nimport seaborn as sns\n\n\ndef cramers_v(confusion_matrix):\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum()\n    phi2 = chi2 \/ n\n    r, k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))\/(n-1))\n    rcorr = r - ((r-1)**2)\/(n-1)\n    kcorr = k - ((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr \/ min((kcorr-1), (rcorr-1)))","3d3ca958":"rows= []\n\nfor var1 in df:\n  col = []\n  for var2 in df :\n    confusion_matrix = pd.crosstab(df[var1], df[var2])\n     # Cramer's V test\n    col.append(round(cramers_v(confusion_matrix.values),2)) # Keeping of the rounded value of the Cramer's V  \n  rows.append(col)\n  \ncramers_results = np.array(rows)\ndf_corr = pd.DataFrame(cramers_results, columns = df.columns, index =df.columns)\n\n\n\ndf_corr","a1cf1219":"# Since veil-type has NaN values in the correlation matrix\ndf_corr = df_corr.drop(['veil-type'],axis=0)\ndf_corr = df_corr.drop(['veil-type'],axis=1)\n\n# Since kfolds is not required for correlation\ndf_corr = df_corr.drop(['kfolds'],axis=0)\ndf_corr = df_corr.drop(['kfolds'],axis=1)\ndf_corr","e1666670":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(30,15))\nsns.heatmap(df_corr, annot=True, cmap=plt.cm.CMRmap_r)\n\nplt.show()","d310328b":"# with the following function we can select highly correlated features\n# it will remove the first feature that is correlated with anything other feature\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","755a1619":"corr_features = correlation(df_corr, 0.7)\nprint(f\"No of features which can be removed : {len(set(corr_features))}\")\nprint(f\"Removable features : {corr_features}\")","99152279":"df = df.drop(list(corr_features), axis=1)\ndf.head()","a7b4711d":"columns_to_one_hot_encode = list(df.columns)\ncolumns_to_one_hot_encode.remove('class')\ncolumns_to_one_hot_encode.remove('kfolds')\ncolumns_to_one_hot_encode ","7d0cd8da":"# One hot encode the categorical columns - All except the target column \"class\" and the \"kfolds\" column\n\ndf=pd.get_dummies(data=df,columns=columns_to_one_hot_encode)\ndf.head()","1bee7ba8":"df['class'] = df['class'].replace({'e':0,'p':1})\ndf.head()","ac15f756":"# Move the target and kfolds column to the last\n\ndf=df[[column for column in df if column not in['class','kfolds']]+['class','kfolds']]\ndf.head()","87fe1833":"def run(fold,df,models,target_name, save_model, print_details=False):\n  \n  # print(df.head())\n  # Training and validation sets\n  df_train=df[df['kfolds']!=fold].reset_index(drop=True)\n  df_valid=df[df['kfolds']==fold].reset_index(drop=True)\n\n\n  # x and y of training dataset\n  x_train=df_train.drop(target_name,axis=1).values\n  y_train=df_train[target_name].values\n\n  # x and y of validation dataset\n  x_valid=df_valid.drop(target_name,axis=1).values\n  y_valid=df_valid[target_name].values\n\n  # accuracy => will store accuracies of the models  (same for confusion_matrices)\n  accuracy=[]\n  confusion_matrices=[]\n  classification_report=[]\n\n  for model_name,model_constructor in list(models.items()):\n    clf=model_constructor\n    clf.fit(x_train,y_train)\n\n    # preds_train, preds_valid => predictions when training and validation x are fed into the trained model\n    preds_train=clf.predict(x_train)\n    preds_valid=clf.predict(x_valid)\n\n    acc_train=metrics.accuracy_score(y_train,preds_train)\n    acc_valid=metrics.accuracy_score(y_valid,preds_valid)\n\n    f1_train = metrics.f1_score(y_train,preds_train)\n    f1_valid = metrics.f1_score(y_valid,preds_valid)\n\n    conf_matrix=metrics.confusion_matrix(y_valid,preds_valid)\n    class_report=metrics.classification_report(y_valid,preds_valid)\n\n    accuracy.append(acc_valid)\n    confusion_matrices.append(conf_matrix)\n    classification_report.append(class_report)\n\n    if(print_details==True):\n      print(f'Model => {model_name} => Fold = {fold} => Training Accuracy = {acc_train} => Validation Accuracy = {acc_valid}')\n\n    if(save_model==True):\n      joblib.dump(clf, f\"{MODEL_PATH}{model_name}_F1_{f1_valid}_ACC_{acc_valid}_FOLD_{fold}.bin\")\n\n  if(print_details==True):\n    print('\\n--------------------------------------------------------------------------------------------\\n')\n    \n  return accuracy,confusion_matrices,classification_report","e671c71e":"import optuna\nfrom functools import partial\n\ndef optimize_rfc(trial,df,total_folds,target_name):\n    criterion = trial.suggest_categorical(\"criterion\", ['gini','entropy'])\n    n_estimators = trial.suggest_int('n_estimators', 100, 1500)\n    max_depth = trial.suggest_int(\"max_depth\", 3, 30)\n    max_features = trial.suggest_uniform(\"max_features\", 0.01, 1.0)\n    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 100)\n    \n    model = RandomForestClassifier(\n        n_estimators = n_estimators, \n        max_depth = max_depth, \n        max_features = max_features, \n        min_samples_leaf = min_samples_leaf,\n        min_samples_split = min_samples_split,\n        criterion = criterion\n    )\n    \n    accuracies = []\n    \n    for fold in range(total_folds):\n        \n        df_train=df[df['kfolds']!=fold].reset_index(drop=True)\n        df_valid=df[df['kfolds']==fold].reset_index(drop=True)\n\n\n        # x and y of training dataset\n        x_train=df_train.drop(target_name,axis=1).values\n        y_train=df_train[target_name].values\n\n        # x and y of validation dataset\n        x_valid=df_valid.drop(target_name,axis=1).values\n        y_valid=df_valid[target_name].values\n        \n        model.fit(x_train, y_train)\n        preds= model.predict(x_valid)\n        \n        fold_acc = metrics.accuracy_score(y_valid, preds)\n        accuracies.append(fold_acc)\n        \n    return np.mean(accuracies)\n\noptimization_function_rfc = partial(optimize_rfc, df = df, total_folds = 5,target_name = 'class')\nstudy_rfc = optuna.create_study(direction = 'maximize')\nstudy_rfc.optimize(optimization_function_rfc, n_trials=15)","93bad05f":"rfc_best_params = study_rfc.best_trial.params\nrfc_best_params","d32bfbf7":"def optimize_xgb(trial,df,total_folds,target_name):\n    \n    learning_rate = trial.suggest_uniform(\"learning_rate\", 0.01, 1.0)\n    gamma = trial.suggest_uniform(\"gamma\", 0.05, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 3, 30)\n    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 10)\n    subsample = trial.suggest_uniform(\"subsample\", 0.5, 1.0)\n    colsample_bytree = trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0)\n    reg_lambda = trial.suggest_uniform(\"reg_lambda\", 0.01, 1.0)\n    reg_alpha = trial.suggest_uniform(\"reg_alpha\", 0.01, 1.0)\n    \n    model = XGBClassifier(\n        learning_rate = learning_rate,\n        gamma = gamma,\n        max_depth = max_depth,\n        min_child_weight = min_child_weight,\n        subsample = subsample,\n        colsample_bytree = colsample_bytree,\n        reg_lambda = reg_lambda,\n        reg_alpha = reg_alpha\n    )\n    \n    accuracies = []\n    \n    for fold in range(total_folds):\n        \n        df_train=df[df['kfolds']!=fold].reset_index(drop=True)\n        df_valid=df[df['kfolds']==fold].reset_index(drop=True)\n\n\n        # x and y of training dataset\n        x_train=df_train.drop(target_name,axis=1).values\n        y_train=df_train[target_name].values\n\n        # x and y of validation dataset\n        x_valid=df_valid.drop(target_name,axis=1).values\n        y_valid=df_valid[target_name].values\n        \n        model.fit(x_train, y_train)\n        preds= model.predict(x_valid)\n        \n        fold_acc = metrics.accuracy_score(y_valid, preds)\n        accuracies.append(fold_acc)\n        \n    return np.mean(accuracies)\n\noptimization_function_xgb = partial(optimize_xgb, df = df, total_folds = 5,target_name = 'class')\nstudy_xgb = optuna.create_study(direction = 'maximize')\nstudy_xgb.optimize(optimization_function_xgb, n_trials=15)","bf21122b":"xgb_best_params = study_xgb.best_trial.params\nxgb_best_params","be988244":"def optimize_svc(trial,df,total_folds,target_name):\n    \n    C = trial.suggest_uniform(\"C\", 0.001, 1000)\n    gamma = trial.suggest_categorical(\"gamma\", ['auto'])\n    class_weight = trial.suggest_categorical(\"class_weight\", ['balanced'])\n    \n    model = SVC(\n        C = C,\n        gamma = gamma,\n        class_weight = class_weight\n    )\n    \n    accuracies = []\n    \n    for fold in range(total_folds):\n        \n        df_train=df[df['kfolds']!=fold].reset_index(drop=True)\n        df_valid=df[df['kfolds']==fold].reset_index(drop=True)\n\n\n        # x and y of training dataset\n        x_train=df_train.drop(target_name,axis=1).values\n        y_train=df_train[target_name].values\n\n        # x and y of validation dataset\n        x_valid=df_valid.drop(target_name,axis=1).values\n        y_valid=df_valid[target_name].values\n        \n        model.fit(x_train, y_train)\n        preds= model.predict(x_valid)\n        \n        fold_acc = metrics.accuracy_score(y_valid, preds)\n        accuracies.append(fold_acc)\n        \n    return np.mean(accuracies)\n\noptimization_function_svc = partial(optimize_svc, df = df, total_folds = 5,target_name = 'class')\nstudy_svc = optuna.create_study(direction = 'maximize')\nstudy_svc.optimize(optimization_function_svc, n_trials=15)","b7cfd4fc":"svc_best_params = study_svc.best_trial.params\nsvc_best_params","aaba3b5a":"def optimize_dt(trial,df,total_folds,target_name):\n    criterion = trial.suggest_categorical(\"criterion\", ['gini','entropy'])\n    max_depth = trial.suggest_int(\"max_depth\", 3, 30)\n    max_features = trial.suggest_uniform(\"max_features\", 0.01, 1.0)\n    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 100)\n    \n    model = DecisionTreeClassifier(\n        max_depth = max_depth, \n        max_features = max_features, \n        min_samples_leaf = min_samples_leaf,\n        min_samples_split = min_samples_split,\n        criterion = criterion\n    )\n    \n    accuracies = []\n    \n    for fold in range(total_folds):\n        \n        df_train=df[df['kfolds']!=fold].reset_index(drop=True)\n        df_valid=df[df['kfolds']==fold].reset_index(drop=True)\n\n\n        # x and y of training dataset\n        x_train=df_train.drop(target_name,axis=1).values\n        y_train=df_train[target_name].values\n\n        # x and y of validation dataset\n        x_valid=df_valid.drop(target_name,axis=1).values\n        y_valid=df_valid[target_name].values\n        \n        model.fit(x_train, y_train)\n        preds= model.predict(x_valid)\n        \n        fold_acc = metrics.accuracy_score(y_valid, preds)\n        accuracies.append(fold_acc)\n        \n    return np.mean(accuracies)\n\noptimization_function_dt = partial(optimize_dt, df = df, total_folds = 5,target_name = 'class')\nstudy_dt = optuna.create_study(direction = 'maximize')\nstudy_dt.optimize(optimization_function_dt, n_trials=15)","7a1f8189":"dt_best_params = study_dt.best_trial.params\ndt_best_params","df8f6933":"XGB_model=XGBClassifier(**xgb_best_params)\nSVM_model=SVC(**svc_best_params)\nRFC_model=RandomForestClassifier(**rfc_best_params)\nDT_model=DecisionTreeClassifier(**dt_best_params)\nmodels={\n    'XGB Classifier' : XGB_model,\n    'SVM Classifier' : SVM_model,\n    'Random Forest Classifier' : RFC_model,\n    'Decision Tree Classifier' : DT_model\n    }\n\naccuracies,confusion_matrices,classification_reports=[],[],[]\nfor f in range(5):\n  accuracy,confusion_matrix,classification_report=run(f,df,models=models,target_name='class', save_model= True, print_details=True)\n  accuracies.append(accuracy)\n  confusion_matrices.append(confusion_matrix)\n  classification_reports.append(classification_report)","1a0efdbd":"def plot_confusion_matrix(fold_num, models, title):\n    \n    classifier_num = list(models.keys()).index(title)\n    \n    cf_matrix = confusion_matrices[fold_num][classifier_num]\n    group_names = ['True Neg','False Pos','False Neg','True Pos']\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                    cf_matrix.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cf_matrix.flatten()\/np.sum(cf_matrix)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n              zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n\n    plt.figure(figsize=(10,6))\n    plt.title(title, fontsize=20)\n    sns.heatmap(cf_matrix, annot=labels, fmt='', annot_kws={\"fontsize\" : 20})\n    plt.show()","50110b9e":"plot_confusion_matrix(fold_num = 0, models = models, title = \"XGB Classifier\")","979af40f":"plot_confusion_matrix(fold_num = 0, models = models, title = \"SVM Classifier\")","a2b00b82":"plot_confusion_matrix(fold_num = 0, models = models, title = \"Random Forest Classifier\")","b19acb3f":"plot_confusion_matrix(fold_num = 0, models = models, title = \"Decision Tree Classifier\")","71d09358":"# Creating 5 Stratified K Fold cross validation sets","23b285d9":"### 3. SVM Classifier","b21aeb65":"### Move the class and kfolds column to the end","de082d79":"### 4. Decision Tree Classifier","a5563cfd":"### 2. XGBoost Classifier","d3553c1e":"# Conclusion : Since the dataset was already cleaned and it was an easy dataset so all the 4 models ie. XGBoost, Decision Tree, Random Forest and SVM gave 100% accuracies on both train and test data","5750d7ad":"# One hot encode categorical features","11a6dff6":"# Hyperparameter Tuning for different models using Optuna\nModels :\n\n1. XGB Classifier\n2. SVM Classifier\n3. Random Forest Classifier\n4. Decision Tree Classifier","0a6c8228":"### 1. Random Forest","7a035f94":"# Heatmap of the Confusion Matrix","e7d999e3":"# File Paths","2e1ce27b":"# Fit and Predict the models","ae72a0ee":"# Feature Selection : Removing some categorical features based on Cramer's V\n\n*References :*\n1. https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9\n2. https:\/\/www.kaggle.com\/chrisbss1\/cramer-s-v-correlation-matrix","661b95e2":"# Data Exploration\u00b6\n1. Null Values\n2. Number of unique values","5de12e1f":"### High correlation between 2 features indicate some sort of duplication or that 1 feature can be represented in the form of other. So we only need 1 of those 2 features and we can remove 1. We can set a threshold (here, 0.7); above which if 2 features have correlation, we can drop 1."}}