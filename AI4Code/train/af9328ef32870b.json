{"cell_type":{"2239ad80":"code","9cd1b47f":"code","1ebdd2c0":"code","25bc0d6b":"code","878b8858":"code","9f29aae6":"code","00b337bd":"code","10564b09":"code","78198401":"code","4a3af59f":"code","cbbe44a6":"code","7440655a":"code","1fc5b82c":"code","e17783d2":"code","023ae53d":"code","6170f4c4":"code","104d816c":"code","3e6cb46e":"code","187e6c0f":"code","f2d2e74e":"code","7bcadc10":"code","d7afd93f":"code","b51b47e2":"code","a89261e9":"code","1a4c5618":"code","d01b44ff":"code","abad017f":"markdown","86dfb3ef":"markdown","d6c4fd97":"markdown","9befa3c8":"markdown","7cbac8d7":"markdown","ba051c3e":"markdown","f71dc19b":"markdown","723fe7e7":"markdown","f8719e08":"markdown","b6b51cc4":"markdown","53b1af34":"markdown","001d2806":"markdown","1c59d455":"markdown"},"source":{"2239ad80":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9cd1b47f":"import matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.model_selection import (KFold, StratifiedKFold,\n                                     cross_val_predict, cross_val_score,\n                                     train_test_split)\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import GaussianNB\n\n","1ebdd2c0":"input_data = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ninput_data","25bc0d6b":"input_data.isnull()","878b8858":"input_data.describe()","9f29aae6":"input_data.isin([0]).any()","00b337bd":"from fancyimpute.knn import KNN\n\n\nfeatures_with_zero_values = ['BMI', 'BloodPressure', 'Glucose', 'Insulin', 'SkinThickness']\ninput_data[features_with_zero_values] = input_data[features_with_zero_values].replace(0, np.nan)\ndata = KNN(k=5).fit_transform(input_data.values)\ninputdata = pd.DataFrame(data, columns=input_data.columns)","10564b09":"inputdata.isin([0]).any()  ","78198401":"inputdata.isna().any()  ","4a3af59f":"inputdata.isnull().any()  ","cbbe44a6":"inputdata.hist(figsize=(12, 12))\nplt.show()","7440655a":"X = inputdata.drop('Outcome', axis=1)   # input feature vector\ny = inputdata['Outcome']                # labelled target vector\n\nscaler = StandardScaler()                # scaling \nX_scaled = scaler.fit_transform(X)\nX = pd.DataFrame(X_scaled, columns=X.columns)\nX.head()","1fc5b82c":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=10, stratify=y)\nX_train.head()","e17783d2":"y_train.head()","023ae53d":"# cross validation kFold\n\nkfold = StratifiedKFold(n_splits=10, random_state = 10)\n","6170f4c4":"# KNN model\nclf = KNeighborsClassifier(n_neighbors=3)\n\n# model fitting\nclf.fit(X_train, y_train)","104d816c":"scores = cross_val_score(clf, X_train, y_train, cv=kfold)\nscores.mean()","3e6cb46e":"# prediction on validation data\n\ny_pred = clf.predict(X_valid)\ny_pred","187e6c0f":"# confusion matrix\n\ncfm = confusion_matrix(y_pred, y_valid)\ncfm\n","f2d2e74e":"# accuracy score\nprint('accuracy of KNN: ',accuracy_score(y_pred,y_valid))","7bcadc10":"# model\n\nclf_g = GaussianNB()\nclf_g","d7afd93f":"# validation score\n\nscores = cross_val_score(clf_g, X_train, y_train, cv=kfold)\nscores.mean()","b51b47e2":"# model fitting\nclf_g.fit(X_train, y_train)","a89261e9":"# prediction on validation data\n\ny_pred = clf_g.predict(X_valid)\ny_pred","1a4c5618":"# confusion matrix\n\ncfm = confusion_matrix(y_pred, y_valid)\ncfm","d01b44ff":"# accuracy score\nprint('accuracy of GaussianNB: ',accuracy_score(y_pred,y_valid))","abad017f":"### Naive Bayes\n\n* Using Gaussian NB on the same dataset to compare with KNN.","86dfb3ef":"The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.","d6c4fd97":"Now, all our input features are at the same scale. ","9befa3c8":"### EDA","7cbac8d7":"No Null values. ","ba051c3e":"Pima indian diabetes dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. \nThe **objective** of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.","f71dc19b":"Find out zeros values in dataset:\nisin([0]) function gives us the features having '0' as a value, which is doesn't make sense. All the features are medically critical, so cannot have zero readings in a healthy human being.  \n\nWe need to impute zeroes with np.nan so that in one go we can impute nan with possibles values, maybe mean or median. ","723fe7e7":"No zero values after imputation of zeros ","f8719e08":"### KNN","b6b51cc4":"### Conclusion\n\nGaussian Naive Bayes performed better than the KNearestNeighbours(KNN) on the same Pima Indian Diabetes dataset. \n\n","53b1af34":"From the above histograms on various features, we observes that range of DiabetesPedigreefunction is very small in comparison to range of insulin. \n\nThis meant that all the features are not at the same scale. It is preferred to scale the features down to the same magnitude by a process known as feature scaling.S tandardization is one method of feature scaling and does so by replacing the values with their z scores. We will apply standardization to our features, but before doing that, we will split our df into feature (X) and outcome (y) then scale the features.","001d2806":"No NA values ","1c59d455":"### Model Fitting and Analysis "}}