{"cell_type":{"0ffce4f7":"code","90f8f02a":"code","7ae7c213":"code","c5864d03":"code","086fadc8":"code","a847898f":"code","edb793bc":"code","eb48ea74":"code","e4f9b7a9":"code","0b936431":"code","6414a6ff":"code","8800c238":"code","42d2c9f5":"code","f418005e":"code","6c81dac2":"code","854025e0":"code","5421c1dd":"code","4c5a3334":"code","c8415fc5":"markdown","8840fb57":"markdown","139aa707":"markdown","f03e9421":"markdown","85609ce6":"markdown","69b9d515":"markdown","94ee2a9b":"markdown","0f272e66":"markdown","14e764e8":"markdown","35b0cd50":"markdown","a00186fd":"markdown"},"source":{"0ffce4f7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport joblib\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","90f8f02a":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nprint(train.shape) ","7ae7c213":"train.head()","c5864d03":"train.isnull().sum()","086fadc8":"(train.Survived.value_counts() \/ len(train)).to_frame()","a847898f":"y_train = train.Survived.values\ntrain.drop(['Survived'], axis=1, inplace=True)","edb793bc":"train['FamSize'] = train['SibSp'] + train['Parch']\n\ndef set_deck(cabin):\n    if str(cabin) == 'nan':\n        return 'Missing'\n    return cabin[0]\n\ntrain['Deck'] = train['Cabin'].map(set_deck)\n\ntrain.head()","eb48ea74":"num_features = ['Age', 'FamSize', 'Fare']\ncat_features = ['Sex', 'Pclass', 'Deck', 'Embarked']\n\nfeatures = num_features + cat_features\n\n\nnum_transformer = Pipeline(\n    steps = [\n        ('imputer', SimpleImputer(strategy='mean')),\n        ('scaler', StandardScaler())  \n    ]\n)\n\n\ncat_transformer = Pipeline(\n    steps = [\n        ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ]\n)\n\n\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('num', num_transformer, num_features),\n        ('cat', cat_transformer, cat_features)\n    ]\n)","e4f9b7a9":"preprocessor.fit(train)\n\nX_train = preprocessor.transform(train)\n\nprint(X_train.shape)\nprint(y_train.shape)","0b936431":"%%time \n\nlr_clf = LogisticRegression(max_iter=3000, solver='saga', penalty='elasticnet')\n\nlr_parameters = {\n    'l1_ratio':[0, 0.5, 1],\n    'C': [0.075, 0.05, 0.025, 0.15, 0.01]\n}\n\nlr_grid = GridSearchCV(lr_clf, lr_parameters, cv=10, refit='True', n_jobs=-1, verbose=10, scoring='accuracy')\nlr_grid.fit(X_train, y_train)\n\nlr_model = lr_grid.best_estimator_\n\nprint('Best Parameters:', lr_grid.best_params_)\nprint('Best CV Score:  ', lr_grid.best_score_)\nprint('Training Acc:   ', lr_model.score(X_train, y_train))","6414a6ff":"lr_summary = pd.DataFrame(lr_grid.cv_results_['params'])\nlr_summary['cv_score'] = lr_grid.cv_results_['mean_test_score']\n\nfor r in lr_parameters['l1_ratio']:\n    temp = lr_summary.query(f'l1_ratio == {r}')\n    plt.plot(temp.C, temp.cv_score, label=r)\nplt.xscale('log')\nplt.ylim([0.75, 0.82])\nplt.xlabel('Regularization Parameter (C)')\nplt.ylabel('CV Score')\nplt.legend(title='L1 Ratio', loc='lower right')\nplt.grid()\nplt.show()\n\nprint(lr_summary.to_string(index=False))","8800c238":"%%time \n\ndt_clf = DecisionTreeClassifier(random_state=1)\n\ndt_parameters = {\n    'max_depth': range(1,33),\n    'min_samples_leaf': range(1, 17)\n}\n\ndt_grid = GridSearchCV(dt_clf, dt_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='accuracy')\ndt_grid.fit(X_train, y_train)\n\ndt_model = dt_grid.best_estimator_\n\nprint('Best Parameters:', dt_grid.best_params_)\nprint('Best CV Score:  ', dt_grid.best_score_)\nprint('Training Acc:   ', dt_model.score(X_train, y_train))","42d2c9f5":"dt_summary = pd.DataFrame(dt_grid.cv_results_['params'])\ndt_summary['cv_score'] = dt_grid.cv_results_['mean_test_score']\n\nfor ms in dt_parameters['min_samples_leaf']:\n    temp = dt_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(dt_summary.to_string(index=False))","f418005e":"%%time \n\nrf_clf = RandomForestClassifier(random_state=1, n_estimators=100)\n\nrf_parameters = {\n    'max_depth': range(1,33),\n    'min_samples_leaf': range(1, 17)\n}\n\nrf_grid = GridSearchCV(rf_clf, rf_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='accuracy')\nrf_grid.fit(X_train, y_train)\n\nrf_model = rf_grid.best_estimator_\n\nprint('Best Parameters:', rf_grid.best_params_)\nprint('Best CV Score:  ', rf_grid.best_score_)\nprint('Training Acc:   ', rf_model.score(X_train, y_train))","6c81dac2":"rf_summary = pd.DataFrame(rf_grid.cv_results_['params'])\nrf_summary['cv_score'] = rf_grid.cv_results_['mean_test_score']\n\nfor ms in rf_parameters['min_samples_leaf']:\n    temp = rf_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(rf_summary.to_string(index=False))","854025e0":"print(rf_grid.best_params_)","5421c1dd":"final_model = RandomForestClassifier(random_state=1, n_estimators=100, max_depth=21, min_samples_leaf=2)\nfinal_model.fit(X_train, y_train)\n\nprint(final_model.score(X_train, y_train))","4c5a3334":"joblib.dump(preprocessor, 'titanic_preprocessor_01.joblib')\njoblib.dump(final_model, 'titanic_rf_model_01.joblib')\nprint('Model written to file.')","c8415fc5":"# Check Label Distribution","8840fb57":"# Preprocessing","139aa707":"# Check for Missing Values","f03e9421":"# Load Training Data","85609ce6":"# **Titanic Training Notebook**","69b9d515":"# Save Pipeline and Model","94ee2a9b":"# Import Statements","0f272e66":"## Random Forests","14e764e8":"## Logistic Regression","35b0cd50":"## Decicion Trees","a00186fd":"# Model Selection"}}