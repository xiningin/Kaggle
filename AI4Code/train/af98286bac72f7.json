{"cell_type":{"930be14a":"code","6d93d950":"code","3c6fe9bd":"code","ce8ac18d":"code","cad30fc0":"code","301f6e85":"code","c4ce5ec9":"code","0bcda2c6":"code","2cfea367":"code","2c9cef30":"code","93182b7e":"code","7b4a08cf":"code","5c9a6af9":"code","ea0874e3":"code","0a3d2744":"code","b83e7f78":"code","86fc09ac":"code","9e5311d6":"code","18348b87":"code","2075aa47":"code","0d213eb1":"code","7d7e3392":"code","8036515c":"code","ca63bed9":"code","26a22422":"code","ba803bd3":"code","604a48a3":"code","229edf09":"code","45bcf3c9":"code","4e12c01e":"code","32f2d136":"code","a066c453":"code","f595e3f4":"code","98e7d943":"code","1d097847":"code","0f669e5c":"code","9cda81da":"code","7639690c":"code","8761cf40":"code","6d521403":"code","d6a499f2":"code","296ae771":"code","02f36330":"code","4f195e3a":"code","81e094b5":"code","9fc74da7":"code","d7a80adc":"code","8086b7bf":"code","b129c3b1":"code","18f11dea":"code","ffe60de8":"code","9f2f58f8":"code","2a9b0ef9":"code","45c65c42":"code","9108b54c":"code","dd58e4af":"code","906a0353":"code","87134c72":"code","9d164bf5":"code","1903a25a":"code","289ac6c8":"code","3b96d9bf":"markdown","44e04fc3":"markdown","7e97f1d7":"markdown","59623f1f":"markdown","c538b2a3":"markdown","54c7d493":"markdown"},"source":{"930be14a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6d93d950":"df=pd.read_csv('\/kaggle\/input\/spam-mails-dataset\/spam_ham_dataset.csv')","3c6fe9bd":"df.head()","ce8ac18d":"df.columns","cad30fc0":"df.loc[0]['text']","301f6e85":"df.loc[3]['text']","c4ce5ec9":"df.isnull().sum()","0bcda2c6":"df['label'].value_counts()","2cfea367":"ham=df[df['label']=='ham']\nham.head()","2c9cef30":"spam=df[df['label']=='spam']\nspam.head()","93182b7e":"ham.shape,spam.shape","7b4a08cf":"ham=ham.sample(spam.shape[0])\nham.shape","5c9a6af9":"data=ham.append(spam)\ndata.shape","ea0874e3":"data.head()","0a3d2744":"data['length']=data['text'].apply(lambda x:len(x))\ndata.head()","b83e7f78":"import matplotlib.pyplot as plt\n%matplotlib inline","86fc09ac":"plt.hist(data[data['label']=='ham']['length'],bins=100,alpha=0.7)\nplt.hist(data[data['label']=='spam']['length'],bins=100,alpha=0.7)\nplt.xlim(0,5000)\nplt.show()","9e5311d6":"from sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer","18348b87":"from sklearn.model_selection import train_test_split","2075aa47":"data.head()","0d213eb1":"X_train, X_test,y_train,y_test=train_test_split(data['text'],data['label'],test_size=0.2,random_state=0,shuffle=True,stratify=data['label'])","7d7e3392":"tfidf=TfidfVectorizer()\n#X_train=tfidf.fit_transform(X_train)","8036515c":"X_train.shape","ca63bed9":"X_train","26a22422":"clf=Pipeline([('tfidf',TfidfVectorizer()),('clf',RandomForestClassifier(n_estimators=200,n_jobs=-1))])","ba803bd3":"clf.fit(X_train,y_train)","604a48a3":"y_pred=clf.predict(X_test)","229edf09":"cm=confusion_matrix(y_test,y_pred)","45bcf3c9":"cm","4e12c01e":"report =classification_report(y_test,y_pred)\nprint(report)","32f2d136":"acc=accuracy_score(y_test,y_pred)","a066c453":"acc","f595e3f4":"clf=Pipeline([('tfidf',TfidfVectorizer()),('clf',SVC(C=1000))])","98e7d943":"clf.fit(X_train,y_train)","1d097847":"y_pred=clf.predict(X_test)","0f669e5c":"cm=confusion_matrix(y_test,y_pred)","9cda81da":"cm","7639690c":"report =classification_report(y_test,y_pred)\nprint(report)","8761cf40":"acc=accuracy_score(y_test,y_pred)","6d521403":"acc","d6a499f2":"df.head()","296ae771":"df.shape","02f36330":"import spacy\nnlp=spacy.load('en_core_web_sm')","4f195e3a":"def get_vector(x):\n    doc=nlp(x)\n    return doc.vector.reshape(-1,1)","81e094b5":"vec=get_vector(\"hello this is shubham\")\nvec.shape","9fc74da7":"df['vector']=df['text'].apply(lambda x:get_vector(x))","d7a80adc":"X=np.concatenate(df['vector'],axis=1)\nX=np.transpose(X)\nX.shape","8086b7bf":"y=df['label_num']\ny=y.values.reshape(len(y),1)\ny.shape","b129c3b1":"import tensorflow as tf\ntf.keras.backend.clear_session() ","18f11dea":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout,Conv1D,Flatten\nfrom tensorflow.keras.utils import to_categorical","ffe60de8":"y_oh=to_categorical(y)\ny_oh.shape","9f2f58f8":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y_oh,test_size=0.2,random_state=0,shuffle=True)","2a9b0ef9":"model=Sequential([\n    Dense(128,activation='relu',input_shape=([96])),\n    \n   #Conv1D(64,5,activation='relu'),\n    #Flatten(),\n    Dense(64,activation='relu'),\n \n    Dense(128,activation='relu'),\n    \n    Dense(2,activation='sigmoid')\n])","45c65c42":"model.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer='adam')\nhistory=model.fit(X_train,y_train,batch_size=8,epochs=20,validation_data=[X_test,y_test])","9108b54c":"import matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nacc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot(epochs, acc, 'r')\nplt.plot(epochs, val_acc, 'b')\nplt.title('Training and validation accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"Accuracy\", \"Validation Accuracy\"])\n\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(epochs, loss, 'r')\nplt.plot(epochs, val_loss, 'b')\nplt.title('Training and validation loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\", \"Validation Loss\"])\n\nplt.figure()\n","dd58e4af":"y_pred=model.predict(X_test)\ny_pred=np.argmax(y_pred,axis=1)\ny_test=np.argmax(y_test,axis=1)\ny_pred.shape, y_test.shape","906a0353":"from sklearn.metrics import classification_report,confusion_matrix,accuracy_score","87134c72":"report=classification_report(y_test,y_pred)\ncm=confusion_matrix(y_test,y_pred)\nacc=accuracy_score(y_test,y_pred)","9d164bf5":"print(report)","1903a25a":"print(cm)","289ac6c8":"print(acc)","3b96d9bf":"# pipeline with rfc","44e04fc3":"# data prepration ","7e97f1d7":"# using word embedding and neural network","59623f1f":"## this means data is imbalanced","c538b2a3":"## pipeline with SVM","54c7d493":"# EDA"}}