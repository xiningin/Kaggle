{"cell_type":{"d6c42130":"code","9ee9415d":"code","22f44b0a":"code","30cfee4f":"code","dcafb3ac":"code","21e99ecd":"code","3bfd2fb9":"code","49031b7c":"code","06c89688":"code","b94f9736":"code","83561a32":"code","fd380886":"code","7a63a9dd":"code","d3156357":"code","ea94bd7a":"code","6aa6f389":"code","61ac599f":"code","c3cd58f0":"code","0c5ad7a7":"code","19e03e0c":"code","551824c6":"code","7d62af82":"code","4a422242":"code","6eccd0e0":"code","229e2417":"code","666708e9":"code","4749f3ac":"code","4f7640da":"code","034a605a":"code","83989019":"code","ad538ed4":"code","54fecd29":"code","a50ed9b6":"code","3eb02a59":"code","bd01ecc6":"code","9bc088dd":"code","2b6be584":"code","9f3ded83":"code","cb588edd":"code","966db0aa":"code","3f42d7ad":"code","9edf73ef":"code","7f31b693":"code","b39e3b1b":"code","f4184739":"code","21d4275a":"code","c6436d97":"code","57f4ca25":"code","8bed2f1d":"code","a541a170":"code","2696bbf3":"code","41d4e4f4":"code","cc26e8f5":"code","37c54ada":"code","f574fa8f":"code","ce701b3a":"code","8510d03a":"code","f1db2ca7":"code","91751bc9":"code","1c7afe58":"code","af0da521":"code","00123694":"code","6db9ecf1":"code","7b2bd936":"code","dbf3ce83":"code","b7377b78":"code","58bb7673":"code","bc0972eb":"code","0b57c057":"code","166f848d":"code","227adb96":"code","fa3c261a":"code","dd81f127":"code","01d6887a":"code","832b3a58":"code","9f170f53":"code","e6493d6b":"code","0d1e8683":"code","65aaf4e0":"code","fd3d56eb":"code","f5fc146c":"code","9e0e1b7a":"code","fbf57f1a":"code","caf2f296":"code","515157d6":"code","08e0209e":"code","bccdec2a":"code","e6decc07":"code","0c8d8dcf":"code","2f2d3e2a":"code","bc2aaf78":"code","879e6e53":"code","1b903b3c":"code","785037e2":"code","37fe6fc5":"code","cdba4830":"code","27c146f4":"code","970eb4a2":"code","07e28266":"code","42712ab0":"code","375ad742":"code","4bdc0715":"code","7ad1475e":"code","a215fa8a":"code","13b32852":"code","3ba594d3":"code","bcbfab20":"code","daba8533":"code","bc6fcc50":"code","cceedc84":"code","dad0071b":"code","87b01b81":"code","d9160941":"code","a107e19b":"code","d3873059":"code","6b3567ba":"code","56c8bfd5":"code","54c46e5d":"markdown","9410377e":"markdown","f71495af":"markdown","c804d661":"markdown","8425b276":"markdown","f856792f":"markdown","350f1283":"markdown","b66a04a7":"markdown","9ba8bbf8":"markdown","fa4aebc6":"markdown","a6645ae8":"markdown","29410cb8":"markdown","1d7debc5":"markdown","88bfda39":"markdown","30df2c4c":"markdown","98d3e86d":"markdown"},"source":{"d6c42130":"import os\npath = os.getcwd()","9ee9415d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn import metrics\n%matplotlib inline","22f44b0a":"pima = pd.read_csv('..\/input\/diabetes\/diabetes.csv', \n                   header = None, sep = \",\",\n                   names=['Pregnancy', 'Glucose', 'BloodPressure' ,'SkinfoldThickness', 'Insulin', 'BodyMassIndex', 'DiabetesPedigreeFunction', 'Age', 'Class'])   ","30cfee4f":"pima.head(5)","dcafb3ac":"pima.info()","21e99ecd":"pima.describe() ","3bfd2fb9":"pima[pima.isnull().any(axis=1)] ","49031b7c":"pima.isnull().values.any() ","06c89688":"# An\u00e1lise dos dados\ncolumns=pima.columns[:8]\nplt.subplots(figsize=(18,15))\nlength=len(columns)\nfor i,j in zip(columns,range(length)):\n    plt.subplot((length\/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    pima[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","b94f9736":"# An\u00e1lise de casos de diabetes\npima1=pima[pima['Class']==1]\ncolumns=pima.columns[:8]\nplt.subplots(figsize=(18,15))\nlength=len(columns)\nfor i,j in zip(columns,range(length)):\n    plt.subplot((length\/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    pima1[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","83561a32":"pima.plot(kind= 'box' , subplots=True, layout=(3,3),figsize=(14,10))","fd380886":"pima['Class'].value_counts().plot(kind='bar', figsize=(6,6))\nplt.title('pima_indians_diabetes - Class')\nplt.xlabel('Class')\nplt.ylabel('Frequency')\nplt.show()","7a63a9dd":"pima.boxplot(column='Insulin',by='Class')\nplt.show()","d3156357":"pima.boxplot(column='Pregnancy',by='Class')\nplt.show()","ea94bd7a":"pima.boxplot(column='Glucose',by='Class')\nplt.show()","6aa6f389":"plt.figure(figsize=(14,3))\nInsulin_plt = pima.groupby(pima['Insulin']).Class.count().reset_index()\nsns.distplot(pima[pima.Class == 0]['Insulin'], color='red', kde=False, label='Diabetic')\nsns.distplot(pima[pima.Class == 1]['Insulin'], color='green', kde=False, label='Non-Diabetic')\nplt.legend()\nplt.title('Histogram of Insulin values depending in the class')\nplt.show()","61ac599f":"plt.figure(figsize=(20,5))\nglucose_plt = pima.groupby('Glucose').Class.mean().reset_index()\nsns.barplot(glucose_plt.Glucose, glucose_plt.Class)\nplt.title('Percentual de chance de ser diagnosticado com diabetes por leitura de glicose')\nplt.show()","c3cd58f0":"corr = pima.corr()\n_ , ax = plt.subplots( figsize =( 12 , 10 ) )\ncmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n_ = sns.heatmap(corr, cmap = cmap, square=True, cbar_kws={ 'shrink' : .9 }, ax=ax, annot = True, annot_kws = {'fontsize' : 12 })","0c5ad7a7":"import seaborn as sns\nsns.pairplot(pima,hue='Class',palette='coolwarm')","19e03e0c":"# Check for zero values\nprint(\"Number of missing values : \" + repr(pima[pima.Glucose == 0].shape[0]))\nprint(pima[pima.Glucose == 0].groupby('Class')['Class'].count())","551824c6":"# Replace zero value with the mean value of the classes\nGlucose_0 = pima[(pima['Glucose']== 0)]\npima[(pima['Glucose']== 0) & (pima['Class'] == 0)] = Glucose_0[Glucose_0['Class']== 0].replace(0, pima[(pima['Class']== 0)].mean())\npima[(pima['Glucose']== 0) & (pima['Class'] == 1)] = Glucose_0[Glucose_0['Class']== 1].replace(0, pima[(pima['Class']== 1)].mean())","7d62af82":"# Check for zero values\nprint(\"Number of missing values : \" + repr(pima[pima.BloodPressure == 0].shape[0]))\nprint(pima[pima.BloodPressure == 0].groupby('Class')['Class'].count())","4a422242":"# Replace zero value with the mean value of the classes\nBloodPressure_0 = pima[(pima['BloodPressure']== 0)]\npima[(pima['BloodPressure']== 0) & (pima['Class'] == 0)] = BloodPressure_0[BloodPressure_0['Class']== 0].replace(0, pima[(pima['Class']== 0)].mean())\npima[(pima['BloodPressure']== 0) & (pima['Class'] == 1)] = BloodPressure_0[BloodPressure_0['Class']== 1].replace(0, pima[(pima['Class']== 1)].mean())","6eccd0e0":"# Check for zero values\nprint(\"Number of missing values : \" + repr(pima[pima.SkinfoldThickness == 0].shape[0]))\nprint(pima[pima.SkinfoldThickness == 0].groupby('Class')['Class'].count())","229e2417":"# Replace zero value with the mean value of the classes\nSkinfoldThickness_0 = pima[(pima['SkinfoldThickness']== 0)]\npima[(pima['SkinfoldThickness']== 0) & (pima['Class'] == 0)] = SkinfoldThickness_0[SkinfoldThickness_0['Class']== 0].replace(0, pima[(pima['Class']== 0)].mean())\npima[(pima['SkinfoldThickness']== 0) & (pima['Class'] == 1)] = SkinfoldThickness_0[SkinfoldThickness_0['Class']== 1].replace(0, pima[(pima['Class']== 1)].mean())","666708e9":"# Check for zero values\nprint(\"Number of abnormal cases in skinfold thickness : \" + repr(pima[pima.SkinfoldThickness > 60].shape[0]))\nprint(pima[pima.SkinfoldThickness > 60]['SkinfoldThickness'])\nprint(pima[pima.SkinfoldThickness > 60].groupby('Class')['Class'].count())","4749f3ac":"# imputing impossible value with mean value\npima['SkinfoldThickness'].iloc[579] = pima['SkinfoldThickness'].mean()","4f7640da":"# Check for zero values\nprint(\"Number of missing values : \" + repr(pima[pima.Insulin == 0].shape[0]))\nprint(pima[pima.Insulin == 0].groupby('Class')['Class'].count())","034a605a":"# Replace zero value with the mean value of the classes\nInsulin_0 = pima[(pima['Insulin']== 0)]\npima[(pima['Insulin']== 0) & (pima['Class'] == 0)] = Insulin_0[Insulin_0['Class']== 0].replace(0, pima[(pima['Class']== 0)].mean())\npima[(pima['Insulin']== 0) & (pima['Class'] == 1)] = Insulin_0[Insulin_0['Class']== 1].replace(0, pima[(pima['Class']== 1)].mean())","83989019":"# Check for zero values\nprint(\"Number of missing values : \" + repr(pima[pima.BodyMassIndex == 0].shape[0]))\nprint(pima[pima.BodyMassIndex == 0].groupby('Class')['Class'].count())","ad538ed4":"# Replace zero value with the mean value of the classes\nBodyMassIndex_0 = pima[(pima['BodyMassIndex']== 0)] \npima[(pima['BodyMassIndex']== 0) & (pima['Class'] == 0)] = BodyMassIndex_0[BodyMassIndex_0['Class']== 0].replace(0, pima[(pima['Class']== 0)].mean())\npima[(pima['BodyMassIndex']== 0) & (pima['Class'] == 1)] = BodyMassIndex_0[BodyMassIndex_0['Class']== 1].replace(0, pima[(pima['Class']== 1)].mean())","54fecd29":"pima.describe() ","a50ed9b6":"columns=pima.columns[:8]\nplt.subplots(figsize=(18,15))\nlength=len(columns)\nfor i,j in zip(columns,range(length)):\n    plt.subplot((length\/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    pima[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","3eb02a59":"pima.plot(kind= 'box' , subplots=True, layout=(3,3),figsize=(14,10))","bd01ecc6":"pima.Class.value_counts()","9bc088dd":"random_state=12342","2b6be584":"#data = pima.copy().iloc[:, 0:8].values\n#target = pima.copy().iloc[:, 8:9].Class.values","9f3ded83":"!pip install imblearn","cb588edd":"import imblearn\n# Obs: pode ser necess\u00e1rio reiniciar o Jupyter para poder carregar o pacote","966db0aa":"# Oversampling data is given with a subscript of 'o'\nnp.random.seed(75)\nfrom imblearn.over_sampling import SMOTE, ADASYN\ndata_o, target_o = SMOTE().fit_sample(pima, pima.Class)","3f42d7ad":"data_o.shape","9edf73ef":"target_o.shape","7f31b693":"import collections\ncollections.Counter(target_o)","b39e3b1b":"from sklearn.model_selection import train_test_split","f4184739":"Xo_train, Xo_test, yo_train, yo_test = train_test_split(data_o, target_o, test_size=0.20, random_state=4)","21d4275a":"Xo_train.shape","c6436d97":"yo_train.shape","57f4ca25":"Xo_test.shape","8bed2f1d":"yo_test.shape","a541a170":"data=pima[pima.columns[:8]]\ntarget=pima['Class']","2696bbf3":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ncols = data.iloc[:, 0:8].columns\ndata[cols] = scaler.fit_transform(data)\ndata.head()","41d4e4f4":"#data[cols] = preprocessing.scale(data)","cc26e8f5":"train,test=train_test_split(pima,test_size=0.20,random_state=437,stratify=pima['Class'])# stratify the outcome\n\nX_train=train[train.columns[:8]]\nX_test=test[test.columns[:8]]\ny_train=train['Class']\ny_test=test['Class']","37c54ada":"from sklearn.neighbors import KNeighborsClassifier","f574fa8f":"knn = KNeighborsClassifier(n_neighbors=3)","ce701b3a":"knn.fit(Xo_train, yo_train)","8510d03a":"pred = knn.predict(Xo_test)","f1db2ca7":"pred.shape","91751bc9":"from sklearn.metrics import confusion_matrix\nprint (confusion_matrix(yo_test,pred))","1c7afe58":"from sklearn.metrics import classification_report","af0da521":"print (classification_report(yo_test,pred))","00123694":"from sklearn import preprocessing\ndata_on = preprocessing.scale(data_o)\nXon_train, Xon_test, yon_train, yon_test = train_test_split(data_on, target_o, test_size=0.20, random_state=4)","6db9ecf1":"knn.fit(Xon_train, yon_train)\npred = knn.predict(Xon_test)\n\nprint (confusion_matrix(yon_test,pred))","7b2bd936":"print (classification_report(yon_test,pred))","dbf3ce83":"import numpy as np \nimport matplotlib.pyplot as plt\n\nerror_rate = []\n\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(Xo_train,yo_train)\n    pred_i = knn.predict(Xo_test)\n    error_rate.append(np.mean(pred_i != yo_test))\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","b7377b78":"# NOW WITH K=1\n\nknn = KNeighborsClassifier(n_neighbors=1, weights='distance',p=1)\n\nknn.fit(Xo_train,yo_train)\npred = knn.predict(Xo_test)\n\nprint('WITH K=1')\nprint('\\n')\nprint('Confusion Matrix')\ncm_knn = confusion_matrix(yo_test,pred)\nprint(cm_knn)\nprint('\\n')\nrpt_knn = classification_report(yo_test,pred)\nprint(rpt_knn)","58bb7673":"from sklearn.utils import shuffle\nnew_Ind = []","bc0972eb":"cur_MaxScore = 0.0","0b57c057":"col_num = 8","166f848d":"col_Ind_Random = shuffle(range(0,col_num), random_state=13)","227adb96":"\nfor cur_f in range(0, col_num):\n    new_Ind.append(col_Ind_Random[cur_f])\n    newData = data.values[:, new_Ind]\n    Xs_train, Xs_test, ys_train, ys_test = train_test_split(newData, target, test_size=0.2, random_state=1987)\n    clf = KNeighborsClassifier(1)\n    fit = clf.fit(Xs_train, ys_train)\n    cur_Score = clf.score(Xs_test, ys_test)\n    if cur_Score < cur_MaxScore:\n        new_Ind.remove(col_Ind_Random[cur_f])\n    else:\n        cur_MaxScore = cur_Score\n        print (\"Score with \" + str(len(new_Ind)) + \" selected features: \" + str(cur_Score))\n","fa3c261a":"\nerror_rate = []\nrandom_state=19\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(Xs_train,ys_train)\n    pred_i = knn.predict(Xs_test)\n    error_rate.append(np.mean(pred_i != ys_test))\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","dd81f127":"knn = KNeighborsClassifier(n_neighbors=9, weights='distance',p=1)\n\nknn.fit(Xs_train,ys_train)\npred = knn.predict(Xs_test)\n\nprint('WITH K=31')\nprint('\\n')\nprint('Confusion Matrix')\ncm_knn = confusion_matrix(ys_test,pred)\nprint(cm_knn)\nprint('\\n')\nrpt_knn = classification_report(ys_test,pred)\nprint(rpt_knn)","01d6887a":"error_rate = []\n\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","832b3a58":"knn = KNeighborsClassifier(n_neighbors=30, weights='distance',p=1)\n\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\n\nprint('WITH K=31')\nprint('\\n')\nprint('Confusion Matrix')\ncm_knn = confusion_matrix(y_test,pred)\nprint(cm_knn)\nprint('\\n')\nrpt_knn = classification_report(y_test,pred)\nprint(rpt_knn)","9f170f53":"from sklearn.tree import DecisionTreeClassifier\nrandom_state=234\ndtree = DecisionTreeClassifier(random_state=998)","e6493d6b":"dtree.fit(X_train,y_train)","0d1e8683":"pred = dtree.predict(X_test)\nprint(\"Accuracy for Decision treeclassifier is\",metrics.accuracy_score(pred,y_test))","65aaf4e0":"print('Confusion Matrix')\ncm_dtree = confusion_matrix(y_test,pred)\nprint(cm_dtree)\nprint('\\n')\nrpt_dtree = classification_report(y_test,pred)\nprint(rpt_dtree)","fd3d56eb":"from IPython.display import Image\nfrom sklearn import tree\n\ndot_data = tree.export_graphviz(dtree, out_file='tree.dot', \n                         filled=True, rounded=True,  \n                         special_characters=True) ","f5fc146c":"feat_names = pima.copy().iloc[:, 0:8].columns\ntarg_names = ['Yes','No']","9e0e1b7a":"!pip install graphviz","fbf57f1a":"import graphviz\nimport os\nfrom sklearn import tree\nos.environ[\"PATH\"] += os.pathsep + path\nfrom sklearn.tree import DecisionTreeClassifier,export_graphviz\n\nimport graphviz\n\ndata1 = export_graphviz(dtree,out_file=None,feature_names=feat_names,class_names=targ_names,   \n                         filled=True, rounded=True,  \n                         special_characters=True)\ngraph = graphviz.Source(data1)\ngraph","caf2f296":"from sklearn.ensemble import RandomForestClassifier","515157d6":"# number of base decision tree estimators\nn_est = 100\n# maximum depth of any given decision tree estimator\nmax_depth = 5\n# random state variable\nrstate = 42\n# initialize a random forest algorithm\n\nrf = RandomForestClassifier(n_estimators=n_est, \n                             max_depth=max_depth,\n                             random_state=rstate)","08e0209e":"rf.fit(X_train,y_train)","bccdec2a":"pred = rf.predict(X_test)","e6decc07":"print('Confusion Matrix')\ncm_rf = confusion_matrix(y_test,pred)\nprint(cm_rf)\nprint('\\n')\nrpt_rf = classification_report(y_test,pred)\nprint(rpt_rf)","0c8d8dcf":"# list of columns to be used for training each model\nfeatures = [col for col in list(X_train) ]\nprint('%i features: %s' % (len(features), features))","2f2d3e2a":"# report the most important featuers for predicting each target\n\n# collect ranking of most \"important\" features for E\nimportances =  rf.feature_importances_\ndescending_indices = np.argsort(importances)[::-1]\nsorted_importances = [importances[idx] for idx in descending_indices]\nsorted_features = [features[idx] for idx in descending_indices]\nprint('Most important feature for diabetes energy is %s' % sorted_features[0])","bc2aaf78":"# plot the feature importances\n\ndef plot_importances(X_train, sorted_features, sorted_importances):\n    \"\"\"\n    Args:\n        X_train (nd-array) - feature matrix of shape (number samples, number features)\n        sorted_features (list) - feature names (str)\n        sorted_importances (list) - feature importances (float)\n    Returns:\n        matplotlib bar chart of sorted importances\n    \"\"\"\n    axis_width = 1.5\n    maj_tick_len = 6\n    fontsize = 14\n    bar_color = 'lightblue'\n    align = 'center'\n    label = '__nolegend__'\n    ax = plt.bar(range(X_train.shape[1]), sorted_importances,\n                 color=bar_color, align=align, label=label)\n    ax = plt.xticks(range(X_train.shape[1]), sorted_features, rotation=90)\n    ax = plt.xlim([-1, X_train.shape[1]])\n    ax = plt.ylabel('Feature Importance', fontsize=fontsize)\n    ax = plt.tick_params('both', length=maj_tick_len, width=axis_width, \n                         which='major', right=True, top=True)\n    ax = plt.xticks(fontsize=fontsize)\n    ax = plt.yticks(fontsize=fontsize)\n    ax = plt.tight_layout()\n    return ax\n\nfig1 = plt.figure(1, figsize=(10,8))\n\nax = plot_importances(X_train, sorted_features, sorted_importances)\n\n# plt.tight_layout()\nplt.show()\nplt.close()","879e6e53":"temp=[]\nclassifier=['Decision Tree','Random Forest','KNN','KNN (Smote)']\nmodels=[DecisionTreeClassifier(random_state=998),RandomForestClassifier(n_estimators=n_est, \n                             max_depth=max_depth,\n                             random_state=rstate),KNeighborsClassifier(n_neighbors=6),\"SMOTE\"]\nfor i in models:\n    model = i\n    if model == \"SMOTE\":\n        model = KNeighborsClassifier(n_neighbors=1,weights='distance',p=1)\n        model.fit(Xo_train,yo_train)\n        pred1=model.predict(Xo_test)\n        temp.append(metrics.accuracy_score(pred1,yo_test))\n    else:\n        model.fit(X_train,y_train)\n        prediction=model.predict(X_test)    \n        temp.append(metrics.accuracy_score(prediction,y_test))\n        \nmodels_dataframe=pd.DataFrame(temp,index=classifier)   \nmodels_dataframe.columns=['Accuracy']\nmodels_dataframe","1b903b3c":"diab2=pima[['Pregnancy','Glucose','SkinfoldThickness','Insulin','BodyMassIndex','Age','Class']]\n\ntrain1,test1=train_test_split(diab2,test_size=0.20,random_state=437,stratify=diab2['Class'])\n\nX_train=train1[train1.columns[:6]]\nX_test=test1[test1.columns[:6]]\ny_train=train1['Class']\ny_test=test1['Class']\n\n# SMOTE\nnp.random.seed(795)\ndata1, target1 = SMOTE().fit_sample(diab2, diab2.Class)\nXo_train, Xo_test, yo_train, yo_test = train_test_split(data1, target1, test_size=0.20, random_state=4)","785037e2":"temp=[]\nclassifier=['Decision Tree','Random Forest','KNN','KNN (Smote)']\nmodels=[DecisionTreeClassifier(random_state=998),RandomForestClassifier(n_estimators=n_est, \n                             max_depth=max_depth,\n                             random_state=rstate),KNeighborsClassifier(n_neighbors=6),\"SMOTE\"]\nfor i in models:\n    model = i\n    if model == \"SMOTE\":\n        model = KNeighborsClassifier(n_neighbors=1,weights='distance',p=1)\n        model.fit(Xo_train,yo_train)\n        pred1=model.predict(Xo_test)\n        temp.append(metrics.accuracy_score(pred1,yo_test))\n    else:\n        model.fit(X_train,y_train)\n        prediction=model.predict(X_test)    \n        temp.append(metrics.accuracy_score(prediction,y_test))\n        \nmodels_dataframe=pd.DataFrame(temp,index=classifier)   \nmodels_dataframe.columns=['Accuracy']\nmodels_dataframe","37fe6fc5":"from sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score \nkfold = KFold(n_splits=10, random_state=998) ","cdba4830":"temp=[]\naccuracy=[]\nclassifiers=['KNN','KNN (SMOTE)','Decision Tree','Random Forest']\nmodels=[KNeighborsClassifier(n_neighbors=6),\"SMOTE\", DecisionTreeClassifier(),RandomForestClassifier(n_estimators=n_est, \n                             max_depth=max_depth,\n                             random_state=938)]\nfor i in models:\n    model = i\n    \n    if model == \"SMOTE\":\n        model = KNeighborsClassifier(n_neighbors=1,weights='distance',p=1)\n        cv_result = cross_val_score(model,data_o,target_o, cv = kfold,scoring = \"accuracy\")\n        temp.append(cv_result.mean())\n        accuracy.append(cv_result)\n       \n    else:\n        cv_result = cross_val_score(model,data,target, cv = kfold,scoring = \"accuracy\")\n        temp.append(cv_result.mean())\n        accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame(temp,index=classifiers)   \nnew_models_dataframe2.columns=['CV Mean']    \nnew_models_dataframe2","27c146f4":"box=pd.DataFrame(accuracy,index=[classifiers])\nfig3 = plt.figure(1, figsize=(12,8))\nsns.boxplot(data=box.T, orient=\"h\", palette=\"Set1\")\nplt.show()","970eb4a2":"from pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","07e28266":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","42712ab0":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","375ad742":"rf_random.best_params_","4bdc0715":"def evaluate(model, X_test, y_test):\n    predictions = model.predict(X_test)\n    errors = abs(predictions - y_test)\n    mape = 100 * np.mean(errors)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy","7ad1475e":"base_model = RandomForestClassifier(random_state = 82)\nbase_model.fit(X_train, y_train)\nbase_accuracy = evaluate(base_model, X_test, y_test)","a215fa8a":"best_random = rf_random.best_estimator_\nrandom_accuracy = evaluate(best_random, X_test, y_test)","13b32852":"print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) \/ base_accuracy))","3ba594d3":"from pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(dtree.get_params())","bcbfab20":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\nrandom_state=294\n\n# Create the random grid\nrandom_grid = {\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               }\npprint(random_grid)","daba8533":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\ndtree = DecisionTreeClassifier()\nrandom_state=194\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\ndtree_random = RandomizedSearchCV(estimator = dtree, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=294, n_jobs = -1)\n# Fit the random search model\ndtree_random.fit(X_train, y_train)","bc6fcc50":"dtree_random.best_params_","cceedc84":"base_model =  DecisionTreeClassifier(min_samples_leaf=2, min_samples_split=3,random_state=474)\nbase_model.fit(X_train, y_train)\nbase_accuracy = evaluate(base_model, X_test, y_test)","dad0071b":"pprint(base_model.get_params())","87b01b81":"best_dtree_random = dtree_random.best_estimator_\nbest_dtree_random.fit(X_train, y_train)\nrandom_accuracy = evaluate(best_dtree_random, X_test, y_test)","d9160941":"print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) \/ base_accuracy))","a107e19b":"best_dtree_random","d3873059":"dot_data = tree.export_graphviz(best_dtree_random, out_file='tree2.dot', \n                         filled=True, rounded=True,  \n                         special_characters=True) ","6b3567ba":"temp=[]\naccuracy=[]\nclassifiers=['KNN','KNN (SMOTE)','Decision Tree','Random Forest']\nmodels=[KNeighborsClassifier(n_neighbors=6),\"SMOTE\", best_dtree_random,best_random]\nfor i in models:\n    model = i\n    \n    if model == \"SMOTE\":\n        model = KNeighborsClassifier(n_neighbors=1,weights='distance',p=1)\n        cv_result = cross_val_score(model,data_o,target_o, cv = kfold,scoring = \"accuracy\")\n        temp.append(cv_result.mean())\n        accuracy.append(cv_result)\n       \n    else:\n        cv_result = cross_val_score(model,data,target, cv = kfold,scoring = \"accuracy\")\n        temp.append(cv_result.mean())\n        accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame(temp,index=classifiers)   \nnew_models_dataframe2.columns=['CV Mean']    \nnew_models_dataframe2","56c8bfd5":"box=pd.DataFrame(accuracy,index=[classifiers])\nfig3 = plt.figure(1, figsize=(12,8))\nsns.boxplot(data=box.T, orient=\"h\", palette=\"Set1\")\nplt.show()","54c46e5d":"## Explora\u00e7\u00e3o","9410377e":"### Sele\u00e7\u00e3o para usar o m\u00e9todo Hill climbing","f71495af":"## Modelagem","c804d661":"## K-Nearest Neighbours Classifier","8425b276":"## Random Forest Classifier","f856792f":"## Cross validation","350f1283":"### Relacionamento","b66a04a7":"# Parametric Tuning for decision tree","9ba8bbf8":"### Normaliza\u00e7\u00e3o","fa4aebc6":"## Clean data","a6645ae8":"# Parametric Tuning for random forest","29410cb8":"## Decision Tree Classifier","1d7debc5":"### K-value Optimisation","88bfda39":"## Comparison of the models","30df2c4c":"### KNN without SMOT","98d3e86d":"fun\u00e7\u00e3o de peso usada na previs\u00e3o. Valores poss\u00edveis:\n\n\"Uniforme\": pesos uniformes. Todos os pontos em cada neighborhood s\u00e3o ponderados igualmente.\n\u2018Dist\u00e2ncia\u2019: pontos de pondera\u00e7\u00e3o pelo inverso da dist\u00e2ncia. Nesse caso, os vizinhos mais pr\u00f3ximos de um ponto de consulta ter\u00e3o uma influ\u00eancia maior do que os vizinhos mais distantes.\n(callable): uma fun\u00e7\u00e3o definida pelo usu\u00e1rio que aceita uma matriz de dist\u00e2ncias e retorna uma matriz da mesma forma que cont\u00e9m os pesos.\n\nPar\u00e2metro de pot\u00eancia para a m\u00e9trica Minkowski. Quando p = 1, isso equivale a usar manhattan_distance (l1) e euclidean_distance (l2) para p = 2. Para p arbitr\u00e1rio, minkowski_distance (l_p) \u00e9 usado."}}