{"cell_type":{"7a97667e":"code","4a2081a0":"code","898c53f8":"code","c03d7d50":"code","f79f2742":"code","b1fb5e49":"code","ba193b90":"code","663ea429":"code","ff42c8ac":"code","29309373":"code","dfde37b0":"markdown","69fe9bfe":"markdown","63e21ed1":"markdown","c53e95a0":"markdown","c2c1f565":"markdown","a67a4362":"markdown","8481f5cc":"markdown","8245c78d":"markdown","33026541":"markdown","b8e01842":"markdown","c9fe881c":"markdown"},"source":{"7a97667e":"import warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.cluster import KMeans\nimport plotly.express as px","4a2081a0":"train = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntest = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\nGENES = [col for col in train.columns if 'g-' in col]\nCELLS = [col for col in train.columns if 'c-' in col]","898c53f8":"g_SSE = []\nfor i in tqdm(range(5,250,5)):\n    k_train = train[GENES].copy()\n    k_test = test[GENES].copy()\n    k_data = pd.concat([k_train, k_test], axis = 0)\n    kmeans = KMeans(n_clusters = i, init='k-means++', n_init=5, max_iter=50, tol=1e-04,random_state = 77)\n    kmeans.fit(k_data)\n    g_SSE.append(kmeans.inertia_)","c03d7d50":"df_g = pd.DataFrame({\"gene_SSE\":g_SSE,\n                    'num':list(range(5,250,5))})\n\nfig = px.line(df_g, x = 'num', y = \"gene_SSE\",\n             title = \"gene's SSE of some clusters\")\nfig.show()","f79f2742":"from sklearn.metrics import silhouette_score\n\nsil_scores = []\nfor i in tqdm(range(5,250,5)):\n    k_train = train[GENES].copy()\n    k_test = test[GENES].copy()\n    k_data = pd.concat([k_train, k_test], axis = 0)\n    kmeans = KMeans(n_clusters = i, init='k-means++', n_init=5, max_iter=50, tol=1e-04,random_state = 77)\n    kmeans.fit(k_data)\n    labels=kmeans.predict(k_data)\n    sil_scores.append(silhouette_score(k_data, labels))","b1fb5e49":"df_g = pd.DataFrame({\"silhouette scores\":sil_scores,\n                    'num':list(range(5,250,5))})\n\nfig = px.line(df_g, x = 'num', y = \"silhouette scores\",\n             title = \"gene's Silhoute Score of some clusters\")\nfig.show()","ba193b90":"c_SSE = []\nfor i in tqdm(range(2, 100, 2)):\n    k_train = train[CELLS].copy()\n    k_test = test[CELLS].copy()\n    k_data = pd.concat([k_train, k_test], axis = 0)\n    kmeans = KMeans(n_clusters = i, init='k-means++', n_init=5, max_iter=50, tol=1e-04,random_state = 77)\n    kmeans.fit(k_data)\n    c_SSE.append(kmeans.inertia_)","663ea429":"df_c = pd.DataFrame({\"cell_SSE\":c_SSE,\n                    'num':list(range(2, 100, 2))})\n\nfig = px.line(df_c, x = 'num', y = \"cell_SSE\",\n             title = \"cell's SSE of some clusters\")\nfig.show()","ff42c8ac":"from sklearn.metrics import silhouette_score\n\nsil_scores = []\nfor i in tqdm(range(2, 100, 2)):\n    k_train = train[CELLS].copy()\n    k_test = test[CELLS].copy()\n    k_data = pd.concat([k_train, k_test], axis = 0)\n    kmeans = KMeans(n_clusters = i, init='k-means++', n_init=5, max_iter=50, tol=1e-04,random_state = 77)\n    kmeans.fit(k_data)\n    labels=kmeans.predict(k_data)\n    sil_scores.append(silhouette_score(k_data, labels))","29309373":"df_c = pd.DataFrame({\"silhouette scores\":sil_scores,\n                    'num':list(range(2, 100, 2))})\n\nfig = px.line(df_c, x = 'num', y = \"silhouette scores\",\n             title = \"cell's Silhoute Score of some clusters\")\nfig.show()","dfde37b0":"# GENES","69fe9bfe":"# CELLS","63e21ed1":"The 8-16 value that we worked out before, doesn't look good now because 2 cluster separation works very well.\n\nBut as we are looking for adding features to the data, I think we can settle for a little less silhouette score.  ","c53e95a0":"Clearly you can see that the silhouette score graph gives peaks at **20** and **30** number of clusters, hence that should be the ideal number of clusters if you are using the cluster centroid as features.\n","c2c1f565":"# Silhouette Coefficient\n\nSilhouette Coefficient or silhouette score is a metric used to calculate the goodness of a clustering technique. \n\nIts value ranges from -1 to 1 with 1 being the best and -1 being the worst.\n\n<img src=\"https:\/\/miro.medium.com\/max\/700\/1*cUcY9jSBHFMqCmX-fp8BvQ.jpeg\"><\/img>\n\n**Silhouette Score = ${\\dfrac{b-a}{max(a,b)}}$**\n\nwhere\n\n**a** = average intra-cluster distance i.e the average distance between each point within a cluster.\n\n**b** = average inter-cluster distance i.e the average distance between all clusters\n\n[Source](https:\/\/towardsdatascience.com\/silhouette-coefficient-validating-clustering-techniques-e976bb81d10c)","a67a4362":"## Note \n\nPlease note that I am a Beginner and If you find any problem or error in this, feel completely free to point it out to me. \n\nI am always looking for suggestions to improve and if you find the kernel useful, **CONSIDER UPVOTING**","8481f5cc":"Thankfully we don't need to implement this ourselves, Scikit-learn provides us a utility to calculate this","8245c78d":"## Choosing No. of clusters for KMeans","33026541":"You can see that the number of clusters in this case around 8-16, we can take any value in this range.\n\nLets check out the Silhouette scores","b8e01842":"As you can see, with elbow method we cannot clearly determine the number clusters to be used for KMeans.\n\nLets introduce, **Silhouette Coefficient**","c9fe881c":"First lets try out the Elbow method which is usually used. We will compute the SSE scores for a range of cluster sizes and plot them."}}