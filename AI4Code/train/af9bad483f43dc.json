{"cell_type":{"c43a677d":"code","baca4883":"code","d06ce4d7":"code","6391a1d5":"code","51837669":"code","a77b39c3":"code","d352b251":"code","67873767":"code","41a9c312":"code","7e6f34d4":"code","4f727a97":"code","d9d0f221":"code","ec5a764d":"code","573e6e53":"code","cd6fdb0b":"code","98d81484":"code","a180084f":"code","c006041f":"code","794b1404":"code","664a2a02":"code","63b54d71":"code","dbbacf8f":"code","17c7d317":"code","60612808":"code","3e3cd029":"code","76cc72a0":"code","d865e3a1":"markdown","5af36b1a":"markdown","82a145c1":"markdown","27ba837b":"markdown","c5f7ef8d":"markdown","c28fa408":"markdown","027a1253":"markdown","24cece35":"markdown","c7a34894":"markdown","9c5f86b2":"markdown","06ebf47e":"markdown","0c6b5b52":"markdown","9e268b96":"markdown","c7f850ae":"markdown","737023db":"markdown","d3d40f64":"markdown"},"source":{"c43a677d":"#Various imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.feature_selection import RFE, SelectFromModel,mutual_info_regression,VarianceThreshold\nfrom sklearn.metrics import mean_squared_log_error,accuracy_score\nfrom sklearn.decomposition import PCA\nfrom lightgbm import LGBMRegressor\nimport zipfile\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm \nfrom sklearn.preprocessing import MinMaxScaler\nfrom xgboost import XGBRegressor\nsns.set_style('whitegrid')","baca4883":"# data import \ndf_train = pd.read_csv('..\/input\/train.csv')","d06ce4d7":"df_train.head()","6391a1d5":"#scale data using Min Max scaler\nscaler = MinMaxScaler()\ndf_train[df_train.columns[2:]] = scaler.fit_transform(df_train[df_train.columns[2:]])\ndf_train.head()","51837669":"# No missing data \nprint(df_train.isnull().any().value_counts())","a77b39c3":"X = df_train.drop(['ID','target'],axis=1)\ny = np.log1p(df_train.target)\n# y = df_train.target","d352b251":"#complete zeros feature\n# There is a lot of columns that are always the same value, drop them\n# col_drop = X.iloc[:,np.where(np.all(X==0,axis=0) == True)[0]].columns\n# X = X.drop(col_drop.values,axis=1)\n\n# def drop_novar(X):\n#     col_drop = X.columns[X.nunique() == 1]\n#     X_dropped = X.drop(col_drop,axis=1)\n#     return X_dropped\n# X = drop_novar(X)\n# print(X.shape)","67873767":"def run_rf(X,y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    rf = RandomForestRegressor(n_estimators=500,n_jobs=-1)\n    rf.fit(X_train,y_train)\n    pred_test = rf.predict(X_test)\n    pred_train = rf.predict(X_train)\n    print('Test error: ',mean_squared_log_error(y_test,pred_test))\n    print('Train error: ',mean_squared_log_error(y_train,pred_train))\n    print('Test error: ',mean_squared_log_error(np.expm1(y_test),np.expm1(pred_test)))\n    return rf \n\ndef run_lgb(X,y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    lgb = LGBMRegressor(n_estimators=1500,n_jobs=-1,max_depth=10,learning_rate=0.005,metric='rmse')\n    lgb.fit(X_train,y_train)\n    pred_test = lgb.predict(X_test)\n    pred_train = lgb.predict(X_train)\n    print('Test error: ',mean_squared_log_error(y_test,pred_test))\n    print('Train error: ',mean_squared_log_error(y_train,pred_train))\n    print('Test error: ',mean_squared_log_error(np.expm1(y_test),np.expm1(pred_test)))\n    return lgb\n\ndef custom_select(model,X,num_features):\n    #credit to https:\/\/www.kaggle.com\/alexpengxiao\/preprocessing-model-averaging-by-xgb-lgb-1-39\n    col = pd.DataFrame({'importance': model.feature_importances_, 'feature': X.columns}).sort_values(\n                        by=['importance'], ascending=False)[:num_features]['feature'].values\n    return X[col]\n\ndef plot_imp(model):\n    imp = sorted(model.feature_importances_,reverse=True)\n    plt.figure(figsize=(15,6)) # tune this to your aesthetical preference :)\n    sns.lineplot(np.array(range(len(imp))),imp)\n    plt.title('Feature importance plot')\n    plt.xlabel('Features')\n    plt.ylabel('Importance')\n    plt.show()","41a9c312":"# run lgb on all features\n%time lgb_all = run_lgb(X,y)","7e6f34d4":"plot_imp(lgb_all)","4f727a97":"#run lgb on 1200 selected\nX4 = custom_select(lgb_all,X,1200)\nprint(X4.shape)\n\n%time lgb_select = run_lgb(X4,y)","d9d0f221":"plot_imp(lgb_select)","ec5a764d":"# run on feature selected by SKLEARN\nmodel = SelectFromModel(lgb_all, prefit=True)\nX3 = model.transform(X)\nprint(X3.shape)\n%time lgb_select2 = run_lgb(X3,y)","573e6e53":"plot_imp(lgb_select2)","cd6fdb0b":"# select 1200 best features using PCA\npca = PCA(n_components=900)\nX_pca = pca.fit_transform(X)\nX_pca.shape","98d81484":"%time lgb_pca = run_lgb(X_pca,y)","a180084f":"plot_imp(lgb_pca)","c006041f":"#compute var \nvar_ser = np.var(X,axis=0)\n\nplt.figure(figsize=(12,6))\nsns.lineplot(np.array(range(len(var_ser))),sorted(var_ser))\nplt.title('Variance plot')\nplt.xlabel('Features')\nplt.ylabel('Variance')\nplt.show()","794b1404":"#filtering by variance, choose the threshold wisely\ncol_var = var_ser[var_ser >= 0.0005].index.values\nX_var = X[col_var]\nprint(X_var.shape)","664a2a02":"#run lgb on variance selected features\n%time lgb_var = run_lgb(X_var,y)","63b54d71":"plot_imp(lgb_var)","dbbacf8f":"def add_stat_feat(X):\n    X['mean'] = np.mean(X,axis=1)\n    X['median'] = np.median(X,axis=1)\n    X['std']  = np.std(X,axis=1)\n    X['max'] = np.max(X,axis=1)\n    X['min'] = np.min(X,axis=1)\n    X['skew'] = X.skew(axis=1)\n    X['kurtosis'] = X.kurtosis(axis=1)\n    return X","17c7d317":"X = add_stat_feat(X)\nlgb_ex = run_lgb(X,y)","60612808":"model2 = SelectFromModel(lgb_ex, prefit=True)\nX4 = model2.transform(X)\nprint(X4.shape)\n%time lgb_select3 = run_lgb(X4,y)","3e3cd029":"# load data in chunks \nbest_model = lgb_select3\nchunk_size = 1000\ntest = pd.read_csv(('..\/input\/test.csv'),chunksize=chunk_size)\n    \npreds = []\nIDs = []\nfor chunk in test:\n    chunk[chunk.columns[1:]] = scaler.fit_transform(chunk.drop('ID',axis=1))\n    features = model2.transform(add_stat_feat(chunk.drop('ID',axis=1)))\n    preds = np.append(preds,best_model.predict(features))\n    IDs += list(chunk['ID'])","76cc72a0":"sub_pred = np.expm1(preds)\nsub = pd.DataFrame()\nsub['ID'] = IDs\nsub['target'] = sub_pred\nsub.to_csv('submission.csv',index=False)","d865e3a1":"A large part of algorithm compute distance, so it is necessary to bring all features to some even playing ground. If not, too large features will have a greater impact on the prediction than tiny features, which maybe contains important information.","5af36b1a":"### Data exploration","82a145c1":"We do here basic preprocessing, feature selection and modelling.","27ba837b":"We can also ask sklearn to do it for us, using SelectFromModel function. It is implemented for every model that has a coef_ or a feature_importance_ attribute","c5f7ef8d":"### Feature selection and Modelling","c28fa408":"We can also select features by thresholding the variance. Here it is only filtering so we keep the same features than before","027a1253":"We can see that most of the features are not addding any value to the prediction, a lot of features are not even considered by the model. Let's select 1200 most important features","24cece35":"Statistical features","c7a34894":"Use Log on the target, we don't really care about the exact transaction value, but mostly about the general volume. The Log transform the target column to more grouped values.","9c5f86b2":"The big winner of this part is SelectFromModel function that optimize the feature selection and computation time.","06ebf47e":"The data is composed of only numerical features","0c6b5b52":"PCA is a nice way to reduce the feature space, it transforms the feature space while maximizing the variance of the data. We guess there is approximately 1200 main features but this value can be tuned according to the empirical data. Note that after PCA you do not have the same features than before, but some kind of decomposition of the feature matrix into principal components. We try this to catch the most information we can from the features.","9e268b96":"## Satander Predict Value Challenge","c7f850ae":"Some features have the same value for all entries, meaning they do not carry any information, we can drop them.","737023db":"We reduced the compuation time significantly ! and didn't lose so much accuracy.","d3d40f64":"### Submission"}}