{"cell_type":{"e43b3130":"code","5e611e9e":"code","e2970395":"code","e16e6c67":"code","22b11af1":"code","6d07e068":"code","9ea52ad2":"code","8a452e88":"code","19bc568b":"code","932f7373":"code","49101a20":"code","f5b2fd33":"code","2a39c32e":"code","b5b31c3a":"code","dcdf43d7":"code","9e914d9b":"code","5e14e1e7":"code","06e0fbac":"code","e2167878":"code","ac31b691":"code","607fbeff":"code","5b5bf59c":"code","84dcaa64":"code","91a6194b":"code","e50dcc6a":"code","260fcb36":"code","bcbce5d9":"code","7bbc7641":"code","7b50e55c":"code","64fde0b5":"code","1b3743ff":"code","ef7bb50c":"code","f16f553d":"code","ee230f3c":"code","b3d93603":"code","9c055b31":"code","2e609e6c":"code","1c90f760":"code","6af4ba1d":"code","7acef6cd":"code","154dd6b2":"code","d9daf345":"code","bdedc56d":"code","a89bfa4c":"code","62b69e40":"code","eaae3b29":"code","dde680d3":"code","cb7ecc8c":"code","d53ffe08":"code","448b188c":"code","efb4b495":"code","5c47818d":"code","539d95ca":"markdown","4b73663f":"markdown","58386060":"markdown","cca20796":"markdown","4916fd22":"markdown","75b9fe5e":"markdown","37f28242":"markdown","4a23bae4":"markdown","89e48a58":"markdown","e6d3cae6":"markdown","8eced6bd":"markdown","973a834e":"markdown","25fac120":"markdown","4d6ab4d9":"markdown","07c8b420":"markdown","59465952":"markdown","0dc3153d":"markdown","6da47cbc":"markdown","d6794791":"markdown","0553e678":"markdown","e4b9fc72":"markdown","f6be0e6d":"markdown","f94b102a":"markdown","8967720e":"markdown","27d9d1b0":"markdown","fedd788e":"markdown","ad4d25d2":"markdown","6811df36":"markdown"},"source":{"e43b3130":"#! pip install -q kaggle\n#! mkdir ~\/.kaggle\n#! cp kaggle.json ~\/.kaggle\/\n#! chmod 600 ~\/.kaggle\/kaggle.json","5e611e9e":"! kaggle competitions download -c nlp-getting-started","e2970395":"import requests, zipfile, io\nzip_file_url = \"https:\/\/dl.fbaipublicfiles.com\/fasttext\/vectors-english\/wiki-news-300d-1M.vec.zip\"\nr = requests.get(zip_file_url)\nz = zipfile.ZipFile(io.BytesIO(r.content))\nz.extractall()","e16e6c67":"#For Pre-Processing\nfrom tqdm import tqdm\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.tokenize import RegexpTokenizer \nfrom nltk.tokenize import word_tokenize\nimport os, re, csv, math, codecs\n\n\n# For Training\nimport keras\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom keras.utils import plot_model\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\n\n# For array, dataset, and visualizing\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"whitegrid\")\nnp.random.seed(0)\n\nMAX_NB_WORDS = 100000\ntokenizer = RegexpTokenizer(r'\\w+')\nstop_words = set(stopwords.words('english'))\nstop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])","22b11af1":"print('loading word embeddings...')\n\nembeddings_index = {}\nf = codecs.open('wiki-news-300d-1M.vec', encoding='utf-8')\n\nfor line in tqdm(f):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('found %s word vectors' % len(embeddings_index))","6d07e068":"#load data\ntrain_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', sep=',', header=0)\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv', sep=',', header=0)\ntest_df = test_df.fillna('_NA_')\n\nprint(\"Number of training data \", train_df.shape[0])\nprint(\"Number of testing data: \", test_df.shape[0])","9ea52ad2":"label_names = [\"target\"]\ny_train = train_df[label_names].values\ntrain_df['doc_len'] = train_df['text'].apply(lambda words: len(words.split(\" \")))\nmax_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n\nsns.distplot(train_df['doc_len'], hist=True, kde=True, color='b', label='doc len')\nplt.axvline(x=max_seq_len, color='k', linestyle='--', label='max len')\nplt.title('comment length'); plt.legend()\nplt.show()","8a452e88":"from sklearn.utils import shuffle \n\nraw_docs_train = train_df['text'].tolist()\nraw_docs_test = test_df['text'].tolist() \nnum_classes = len(label_names)\n\nprint(\"pre-processing train data...\")\n\nprocessed_docs_train = []\nfor doc in tqdm(raw_docs_train):\n    tokens = word_tokenize(doc)\n    filtered = [word for word in tokens if word not in stop_words]\n    processed_docs_train.append(\" \".join(filtered))\n#end for\n\nprocessed_docs_test = []\nfor doc in tqdm(raw_docs_test):\n    tokens = word_tokenize(doc)\n    filtered = [word for word in tokens if word not in stop_words]\n    processed_docs_test.append(\" \".join(filtered))\n#end for\n\nprint(\"tokenizing input data...\")\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\ntokenizer.fit_on_texts(processed_docs_train + processed_docs_test)  #leaky\nword_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\nword_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\nword_index = tokenizer.word_index\nprint(\"dictionary size: \", len(word_index))\n\n#pad sequences\nword_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\nword_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)","19bc568b":"#training params\nbatch_size = 256 \nnum_epochs = 40\n\n#model parameters\nnum_filters = 64 \nembed_dim = 300 \nweight_decay = 1e-4","932f7373":"#embedding matrix\n\nprint('preparing embedding matrix...')\n\nwords_not_found = []\nnb_words = min(MAX_NB_WORDS, len(word_index)+1)\nembedding_matrix = np.zeros((nb_words, embed_dim))\n\nfor word, i in word_index.items():\n    if i >= nb_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if (embedding_vector is not None) and len(embedding_vector) > 0:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n    else:\n        words_not_found.append(word)\nprint('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))","49101a20":"print(\"sample words not found: \", np.random.choice(words_not_found, 10))","f5b2fd33":"from keras.layers import BatchNormalization\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential()\n\nmodel.add(Embedding(nb_words,embed_dim,input_length=max_seq_len, weights=[embedding_matrix],trainable=False))\n\nmodel.add(Dropout(0.3))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.summary()","2a39c32e":"from keras.optimizers import RMSprop\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","b5b31c3a":"es_callback = EarlyStopping(monitor='val_loss', patience=3)\n\nhistory = model.fit(word_seq_train, y_train, batch_size=256,\n          epochs=num_epochs, validation_split=0.3, callbacks=[es_callback], shuffle=False)","dcdf43d7":"#generate plots\nplt.figure()\nplt.plot(history.history['loss'], lw=2.0, color='b', label='train')\nplt.plot(history.history['val_loss'], lw=2.0, color='r', label='val')\nplt.title('LSTM sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Cross-Entropy Loss')\nplt.legend(loc='upper right')\nplt.show()","9e914d9b":"plt.figure()\nplt.plot(history.history['accuracy'], lw=2.0, color='b', label='train')\nplt.plot(history.history['val_accuracy'], lw=2.0, color='r', label='val')\nplt.title('LSTM sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\nplt.show()","5e14e1e7":"predictions = model.predict_classes(word_seq_test)","06e0fbac":"# sample=pd.read_csv('sample_submission.csv')\n# sample['target']= (predictions>0.5).astype(int)\n# sample.to_csv(\"submission.csv\",index=False, header=True)","e2167878":"# sample.head(20)","ac31b691":"# i'll try to submit this and know the accuracy if applied to real test\n# !kaggle competitions submit -c nlp-getting-started -f submission.csv -m \"Using LSTM with fastText Word-Embedding\"","607fbeff":"# we don't want the model to overwrite, dont we?\nkeras.backend.clear_session()","5b5bf59c":"#CNN architecture\nprint(\"training CNN ...\")\nmodel = Sequential()\nmodel.add(Embedding(nb_words, embed_dim,\n          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\nmodel.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\nmodel.add(MaxPooling1D(2))\nmodel.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Dense(num_classes, activation='sigmoid'))  #multi-label (k-hot encoding)\n\nadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\nmodel.summary()","84dcaa64":"from keras.optimizers import RMSprop\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","91a6194b":"es_callback = EarlyStopping(monitor='val_loss', patience=3)\n\nhistory = model.fit(word_seq_train, y_train, batch_size=256,\n          epochs=num_epochs, validation_split=0.3, callbacks=[es_callback], shuffle=False)","e50dcc6a":"#generate plots\nplt.figure()\nplt.plot(history.history['loss'], lw=2.0, color='b', label='train')\nplt.plot(history.history['val_loss'], lw=2.0, color='r', label='val')\nplt.title('CNN sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Cross-Entropy Loss')\nplt.legend(loc='upper right')\nplt.show()","260fcb36":"plt.figure()\nplt.plot(history.history['accuracy'], lw=2.0, color='b', label='train')\nplt.plot(history.history['val_accuracy'], lw=2.0, color='r', label='val')\nplt.title('CNN sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\nplt.show()","bcbce5d9":"predictions = model.predict_classes(word_seq_test)","7bbc7641":"# sample=pd.read_csv('sample_submission.csv')\n# sample['target']= (predictions>0.5).astype(int)\n# sample.to_csv(\"submission.csv\",index=False, header=True)","7b50e55c":"# sample.head(20)","64fde0b5":"# i'll try to submit this and know the accuracy if applied to real test\n# !kaggle competitions submit -c nlp-getting-started -f submission.csv -m \"Using CNN with fastText Word-Embedding\"","1b3743ff":"import requests, zipfile, io\nzip_file_url = \"http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\"\nr = requests.get(zip_file_url)\nz = zipfile.ZipFile(io.BytesIO(r.content))\nz.extractall()","ef7bb50c":"print('loading word embeddings...')\n\nembeddings_index = {}\nf = codecs.open('glove.6B.300d.txt', encoding='utf-8')\n\nfor line in tqdm(f):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('found %s word vectors' % len(embeddings_index))","f16f553d":"from sklearn.utils import shuffle \n\nraw_docs_train = train_df['text'].tolist()\nraw_docs_test = test_df['text'].tolist() \nnum_classes = len(label_names)\n\nprint(\"pre-processing train data...\")\n\nprocessed_docs_train = []\nfor doc in tqdm(raw_docs_train):\n    tokens = word_tokenize(doc)\n    filtered = [word for word in tokens if word not in stop_words]\n    processed_docs_train.append(\" \".join(filtered))\n#end for\n\nprocessed_docs_test = []\nfor doc in tqdm(raw_docs_test):\n    tokens = word_tokenize(doc)\n    filtered = [word for word in tokens if word not in stop_words]\n    processed_docs_test.append(\" \".join(filtered))\n#end for\n\nprint(\"tokenizing input data...\")\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\ntokenizer.fit_on_texts(processed_docs_train + processed_docs_test)  #leaky\nword_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\nword_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\nword_index = tokenizer.word_index\nprint(\"dictionary size: \", len(word_index))\n\n#pad sequences\nword_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\nword_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)","ee230f3c":"#training params\nbatch_size = 256 \nnum_epochs = 40\n\n#model parameters\nnum_filters = 64 \nembed_dim = 300 \nweight_decay = 1e-4","b3d93603":"#embedding matrix\n\nprint('preparing embedding matrix...')\n\nwords_not_found = []\nnb_words = min(MAX_NB_WORDS, len(word_index)+1)\nembedding_matrix = np.zeros((nb_words, embed_dim))\n\nfor word, i in word_index.items():\n    if i >= nb_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if (embedding_vector is not None) and len(embedding_vector) > 0:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n    else:\n        words_not_found.append(word)\nprint('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))","9c055b31":"print(\"sample words not found: \", np.random.choice(words_not_found, 10))","2e609e6c":"# we don't want the model to overwrite, dont we?\nkeras.backend.clear_session()","1c90f760":"#CNN architecture\nprint(\"training CNN ...\")\nmodel = Sequential()\nmodel.add(Embedding(nb_words, embed_dim,\n          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\nmodel.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\nmodel.add(MaxPooling1D(2))\nmodel.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Dense(num_classes, activation='sigmoid'))  #multi-label (k-hot encoding)\n\nadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\nmodel.summary()","6af4ba1d":"from keras.optimizers import RMSprop\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","7acef6cd":"es_callback = EarlyStopping(monitor='val_loss', patience=3)\n\nhistory = model.fit(word_seq_train, y_train, batch_size=256,\n          epochs=num_epochs, validation_split=0.3, callbacks=[es_callback], shuffle=False)","154dd6b2":"predictions = model.predict_classes(word_seq_test)","d9daf345":"# sample=pd.read_csv('sample_submission.csv')\n# sample['target']= (predictions>0.5).astype(int)\n# sample.to_csv(\"submission.csv\",index=False, header=True)","bdedc56d":"# sample.head(20)","a89bfa4c":"# i'll try to submit this and know the accuracy if applied to real test\n# !kaggle competitions submit -c nlp-getting-started -f submission.csv -m \"Using CNN with Glove Word-Embedding\"","62b69e40":"# we don't want the model to overwrite, dont we?\nkeras.backend.clear_session()","eaae3b29":"from keras.layers import BatchNormalization\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential()\n\nmodel.add(Embedding(nb_words,embed_dim,input_length=max_seq_len, weights=[embedding_matrix],trainable=False))\n\nmodel.add(Dropout(0.3))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.summary()","dde680d3":"from keras.optimizers import RMSprop\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","cb7ecc8c":"es_callback = EarlyStopping(monitor='val_loss', patience=3)\n\nhistory = model.fit(word_seq_train, y_train, batch_size=256,\n          epochs=num_epochs, validation_split=0.3, callbacks=[es_callback], shuffle=False)","d53ffe08":"predictions = model.predict_classes(word_seq_test)","448b188c":"# sample=pd.read_csv('sample_submission.csv')\n# sample['target']= (predictions>0.5).astype(int)\n# sample.to_csv(\"submission.csv\",index=False, header=True)","efb4b495":"# sample.head(20)","5c47818d":"# i'll try to submit this and know the accuracy if applied to real test\n# !kaggle competitions submit -c nlp-getting-started -f submission.csv -m \"Using LSTM with Glove Word-Embedding\"","539d95ca":"defining variables that used on training","4b73663f":"well, the result is **79.5%**! it's the almost the same with cnn using fastText. on training, it shows a higher number on accuracy tho..\n\n###**anyway i'm not satisfied yet so im just gonna tried LSTM using glove.**","58386060":"#**Downloading pre-trained fastText, preparing datasets, and pre-processing**\n\nas i\u2019ve mentioned, i am using fastText and i\u2019m going to download the pre-trained model that fastText offered, directly to my session, and then unzip it to use.","cca20796":"My first notebook with kaggle dataset!\n\nThis time i'm trying to apply the method of word-embedding that i used on one of my paper in uni: fastText. i tried it to classify this interesting dataset from kaggle. The dataset consisted of tweets and classifies whether a tweet is using a disaster words as to inform a real disaster or merely just use it metaphorically.\n\nThis was tested\/built with Google Colab, so it'll work fine and just the same if you try it on Colab.\n\nthank you to https:\/\/www.kaggle.com\/vsmolyakov\/keras-cnn-with-fasttext-embeddings!\n\nafter trying on LSTM, i will compare it with CNN. and then to fastText performance, i will also compare it with the performance of golve with lstm and cnn also.","4916fd22":"tokenizing the data with tokenizer from tensorflow","75b9fe5e":"# **Download Data from kaggle**\n\nFirst, the data. first things first is installing kaggle to my environment i dont have to download the data to my drive\/local. It's super efficient.\n\nTo get the API, go to your kaggle profile and download the JSON file!","37f28242":"defining variables that used on training","4a23bae4":"let's start training! here are the layers. i have been doing some test and modification on using the layer, unit cells, etc. and so far it works the best for me.","89e48a58":"\nConverting all the words to index in number, to the embedding index in pre-trained model and converted all the missing words to 0,","e6d3cae6":"building the embedding matrix for the weights in Embedding Layer on training. more about embedding matrix: https:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/\n\nall words that aren't in the pre-trained model from fasttext would be changed to 0. the words are basically the ones with typos or names, the words mostly dont matter so much to the pattern. so it's nicer to just weights it 0.","8eced6bd":"#**Evaluation**","973a834e":"# **LET'S COMPARE!**","25fac120":"which one is more interesting to compare first? the using of lstm vs cnn? or the fasttext vs glove? let's try to compare the easiest one--for me at least dont judge me!--the model.\n\nnext up i'll show the comparation of fasttext and glove using the model that works better.","4d6ab4d9":"**well, so far from my experiments, fastText and LSTM showed the best performance. but it's still not very sure if it's really like that because validation splitting influence the performance too, also how i build the layers, choosing batch size, optimizer, and stuff.**\n\n**thank you if youre reading the comments too! i hope you have a great day!**","07c8b420":"wait.. fastText has more null words?","59465952":"**it's 79.6%!**","0dc3153d":"tokenizing the data with tokenizer from tensorflow","6da47cbc":"#**Shape and Train with LSTM**","d6794791":"evaluation on the training for each epoch with this model","0553e678":"evaluation on the training for each epoch","e4b9fc72":"building the embedding matrix for the weights in Embedding Layer on training. more about embedding matrix: https:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/","f6be0e6d":"\nConverting all the words to index in number, to the embedding index in pre-trained model and converted all the missing words to 0,","f94b102a":"Read both the train data and test data","8967720e":"it's **80%**! not so good but also not that bad, huh?","27d9d1b0":"it's **79.4%**! but this one was using less epoch. what do you think?","fedd788e":"# **Sentiment Analysis Using LSTM & CNN with fastText (and Gloe Word-Embedding!**","ad4d25d2":"download the dataset directly by copying the API command on the dataset page","6811df36":"###### **next up is using glove! im excited! with what i promised, i'll use the better performance. with the time and result, i'll just use cnn again!**"}}