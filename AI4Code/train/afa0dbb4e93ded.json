{"cell_type":{"6e4ccefd":"code","232cac43":"code","a17ab158":"code","19897af5":"code","8b6bc56b":"code","5402584a":"code","65abe5d0":"code","6d9eeba3":"code","8ba5145a":"code","3c9151c8":"code","9f308f7a":"code","058081bd":"code","4cb89d4a":"code","6e294012":"code","04693e54":"code","a30d4e26":"code","5d9ebf17":"code","6b13ba6d":"code","2e974fc8":"code","401826f1":"code","2dfde852":"code","bff7aff1":"code","648b49d1":"code","5fa04543":"code","11c412a5":"code","06df2b80":"code","45041c7c":"code","8305ffa7":"code","31651017":"code","8851e704":"code","1e56cc54":"code","588e10ce":"code","b1098cd7":"code","65c1baa5":"code","321fc59c":"code","033f6066":"code","4e098f25":"code","42de3d95":"code","808ab438":"code","b0cdff6a":"markdown","1b55f674":"markdown","dba16969":"markdown","11155c9d":"markdown","1628ed76":"markdown","5e44a6b9":"markdown","65215022":"markdown","1953df0b":"markdown","453840c1":"markdown","6da4b8ea":"markdown","3db947b2":"markdown","a9418fa9":"markdown","a1f8d363":"markdown","6abf83b0":"markdown","adfbd85b":"markdown","793c36e4":"markdown","e58e0c14":"markdown","7ce8ff68":"markdown","06733173":"markdown","58d26be2":"markdown","d0776cd5":"markdown","781795d0":"markdown"},"source":{"6e4ccefd":"# Import the file\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","232cac43":"import pandas as pd\nfrom patsy import dmatrices","a17ab158":"# Read the data\ndf = pd.read_csv('\/kaggle\/input\/dataset\/data.csv')","19897af5":"df.head()","8b6bc56b":"from matplotlib import pyplot as plt ","5402584a":"# Scatter plot between X1 and Output\nplt.scatter(df.X1, df.Output)","65abe5d0":"# Scatter plot between X2 and Output\nplt.scatter(df.X2, df.Output)","6d9eeba3":"# Scatter plot between X3 and Output\nplt.scatter(df.X3, df.Output)","8ba5145a":"# Scatter plot between X4 and Output\nplt.scatter(df.X4, df.Output)","3c9151c8":"df.head(2)","9f308f7a":"df.columns","058081bd":"# Consider only X variables\ndf1 = df[['X1', 'X2', 'X3', 'X4']]\ndf1.head(2)","4cb89d4a":"from statsmodels.stats.outliers_influence import variance_inflation_factor","6e294012":"VIF = pd.DataFrame()\nVIF['feature'] = df1.columns","04693e54":"VIF['VIF'] = [variance_inflation_factor(df1.values, i) \n                          for i in range(len(df1.columns))] \nVIF","a30d4e26":"model = 'Output ~  X1 + X2 + X3 + X4'\n# in Above expression, Output is Y varialble and remaning are X variables.","5d9ebf17":"y, X = dmatrices(model, df, return_type='dataframe')","6b13ba6d":"y.head(2)","2e974fc8":"X.head(2)","401826f1":"# Now split the (train\/test) data into (70\/30)\nimport numpy as np\nsplit_num = np.random.rand(len(X)) < 0.7","2dfde852":"X_train = X[split_num]\ny_train = y[split_num]\nX_test = X[~split_num]\ny_test = y[~split_num]","bff7aff1":"X_train.shape","648b49d1":"# Build the model\nimport statsmodels.api as sm\nLR = sm.OLS(y_train, X_train).fit()","5fa04543":"LR.summary()","11c412a5":"# Predict 'Y' for the test data\ny_test_pred = LR.predict(X_test)","06df2b80":"y_test_pred = pd.DataFrame(y_test_pred)\ny_test_pred.head()","45041c7c":"# Rename the column\ny_test_pred = y_test_pred.rename(columns = {0:'Output'})","8305ffa7":"y_test_pred.head()","31651017":"# Calculate the residual\nResidual = y_test - y_test_pred\nResidual","8851e704":"# Residual vs Predicted value\nplt.scatter(x = y_test_pred, y = Residual)\nplt.xlabel('Predicted values')\nplt.ylabel('Residuals')","1e56cc54":"from statsmodels.compat import lzip\nimport statsmodels.stats.api as sms","588e10ce":"# 'Jarque Bera test' for finding the normality of Residuals\nname = ['Jarque-Bera test', 'p-value', 'Skewness', 'Kurtosis']","b1098cd7":"# Perform 'Jarque-Bera' test\nJB_test = sms.jarque_bera(Residual)","65c1baa5":"# Print\nlzip(name, JB_test)","321fc59c":"Residual.hist(bins = 40)\nplt.show()","033f6066":"# White test \nfrom statsmodels.stats.diagnostic import het_white","4e098f25":"keys = ['Lagrange Multiplier statistic:', 'LM test\\'s p-value:', 'F-statistic:', 'F-test\\'s p-value:']","42de3d95":"result = het_white(Residual, X_test)","808ab438":"lzip(keys, result)","b0cdff6a":"# Assumptions of Linear Regression\n1. X and Y are linearly related\n2. No Multicollinearity between X variables\n3. No Auto-correlation between residuals\n4. Residuals follow Normal distribution\n5. Residuals follow homoscedastic","1b55f674":"**from above plots it is clear that, X1 variable and X2 variable are almost Linear to Y variable**","dba16969":"**Residuals follow homoscedastic**\n* Homo means Same, scedastic means variance - so homoscedastic means same variance (Constant variance)\n* Heteroscedastic means - Different Variance","11155c9d":"**No Auto correlation between Residuals**\n* Residuals are random and independent.","1628ed76":"# Regression Definition \n**Regression is a statistical method used to determine the strength of the relationship between one dependent variable (Y) and a series of other changing variables (X).**\n\n**If the relationship is linear between Y and X, then it is Linear Regression**","5e44a6b9":"**Check if residuals are independent**\n\n* sometimes patterns can be detected in the plot of 'residual errors' vs 'predicted values' or         'residual errors' vs 'actual values'.\n* Durbin-Watson test is used to check if residuals are independent or not.\n* It measures degree of correlation of each residual error with the \u2018previous\u2019 residual error","65215022":"**Residuals are Normally Distributed**\n1. 'Skewness' and 'Kurtosis' is used to measure the normality of Residuals.\n2. Skewness of a perfectly normal distribution is 0 and its kurtosis is 3.0\n3. Statistical tests like 'Jarque Bera' or Omnibus can be used.\n4. If p value <= 0.05, then we can say distribution is normal with >= 95% cofidence.","1953df0b":"# Assumption 3","453840c1":"# Assumption 5","6da4b8ea":"**Residuals are Random**\n\n1. For a given sample (X_train, y_train) from a Dataset, run the linear regression model.\n2. predict the output = y(pred)\n3. difference between Y and Y(pred) is Residual.\n4. If we pick another sample of (X_train, y_train), model will generate the output = y(pred) which is different from previous one. \n5. Now we have different set of Residuals. \n6. So, Residuals are Random.","3db947b2":"No Multicollinearity between X variables\n* There should not any significant relationship between X variables\n* VIF (Variance Inflation factor) can be used to find out the relationship between X variables\n* In VIF, each X variable is picked and regress with other X variables.\n* VIF = 1\/(1-R2)\n* Higher the R2, greater the VIF.\n* In general, VIF > 5 or VIF > 10 indicates high multicollinearity.","a9418fa9":"**Below are few tests to find out homoscedasticity**\n1. Park test\n2. Glejser test\n3. Breusch\u2013Pagan test\n4. White test\n5. Goldfeld\u2013Quandt test","a1f8d363":"**F test Hypothesis**\n* Null Hypothesis :     Residuals are homoscedasticity\n* Alternate hypothesis: Residuals are heteroscedasticity","6abf83b0":"**From above it is clear that, Skewness is close to 0 and Kurtosis is close to 3**\n\n**We can confirm by plotting frequency of Residuals**","adfbd85b":"**From above it is clear that, VIF values are > 10. It means X variables are highly correlated.**","793c36e4":"**Residuals are Independent**\n\n* Two random variables are independent if the probability of one of them taking up some value doesn\u2019t depend on what value the other variable has taken\n\n   **Example:**\n\n* When you roll a die twice, the probability of its coming up as (1,2,3,4,5,6) in the second throw does not depend on the value it came up on the first throw. So the two throws are independent random variables that can each take a value from (1,6), independent of the other throw","e58e0c14":"# Assumption 2","7ce8ff68":"**Above graph illustrates a linear pattern at the end**\n\n**If Durbin Watson value is around 2, then there is no auto correlation between residuals**","06733173":"# Assumption 4","58d26be2":"**Above graph illustrates that Residuals are almost normally distributed**","d0776cd5":"**From above, it is clear that p-value <=0.05. Hence null hypothesis is rejected.** \n\n**So Residuals follow heteroscedasticity**","781795d0":"# Assumption 1\n\n**Relationship between X variable and Y variable is Linear**\n\n1. Plot the scatter plot for each X with Y\n2. Look for approximate linear relationship\n3. if it's not linear, then try to \"Drop X variable\" or \"Change the technique\" or \"apply transformation log(X)\".\n\n"}}