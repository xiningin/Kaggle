{"cell_type":{"9b777493":"code","13cc87e8":"code","0d6f4a0b":"code","2e55974c":"code","8bf9e587":"code","b8b43608":"code","103e7c61":"code","58d23bcd":"code","f659a886":"code","18573ce6":"code","6f5f44ea":"code","08d95ffc":"code","c452372d":"code","2482335d":"code","271dc38c":"code","5559d422":"code","25502455":"code","9d050779":"code","39ba3a28":"code","a7ecc8c2":"code","f080c69d":"code","163618a3":"code","92fbebcf":"code","04c2c354":"code","cb86a587":"code","85ceef56":"code","d3c5e8b3":"code","40148c44":"code","8fc1402c":"code","89c9081a":"code","dab7c722":"code","61701a88":"code","99d86a78":"code","dffd4f6a":"code","7540d66c":"code","3755bebd":"code","66ebab06":"code","6d8e125a":"code","0a797f01":"code","58f04cc8":"code","49965422":"code","599249f1":"code","2f85768d":"markdown","6d9f2b97":"markdown","ea528e93":"markdown","2091e572":"markdown","61e681ca":"markdown","b889ab8c":"markdown","0d038491":"markdown","3970fa26":"markdown","a461927b":"markdown","79250ed6":"markdown","f425776a":"markdown","cab04dc3":"markdown","2ef25363":"markdown","a04b6f0f":"markdown"},"source":{"9b777493":"#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","13cc87e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n!ls '\/kaggle\/input'\n","0d6f4a0b":"!ls '\/kaggle\/input\/kernel9afbfd12ff'\n!cp \/kaggle\/input\/kernel9afbfd12ff\/* .\n!rm .\/model_weights.h5\n!ls","2e55974c":"!cat .\/checkpoint","8bf9e587":"from __future__ import absolute_import, division, print_function, unicode_literals\n\ntry:\n  # %tensorflow_version only exists in Colab.\n  %tensorflow_version 2.x\nexcept Exception:\n  pass\nimport tensorflow as tf\n\nimport os\nimport time\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output","b8b43608":"BUFFER_SIZE = 400\nBATCH_SIZE = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256","103e7c61":"mount_point = '\/kaggle\/'\n\ninput_file = 'input\/sketch2pokemon\/pokemon_pix2pix_dataset\/'\ninput_file = mount_point + input_file\nPATH = input_file\n\nimport os\nif os.path.isdir(mount_point):\n  print(mount_point + ' has been already mounted.')","58d23bcd":"def load(image_file):\n  image = tf.io.read_file(image_file)\n  image = tf.image.decode_jpeg(image)\n\n  w = tf.shape(image)[1]\n\n  w = w \/\/ 2\n  #real_image = image[:, :w, :]\n  input_image = image[:, :w, :]\n  #input_image = image[:, w:, :]\n  real_image = image[:, w:, :]\n\n  input_image = tf.cast(input_image, tf.float32)\n  real_image = tf.cast(real_image, tf.float32)\n\n  return input_image, real_image","f659a886":"print('path = '+ str(PATH))\ninp, re = load(PATH+'train\/100.jpg')\n# casting to int for matplotlib to show the image\nplt.figure()\nplt.imshow(inp\/255.0)\nplt.figure()\nplt.imshow(re\/255.0)","18573ce6":"def resize(input_image, real_image, height, width):\n  input_image = tf.image.resize(input_image, [height, width],\n                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n  real_image = tf.image.resize(real_image, [height, width],\n                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n  return input_image, real_image","6f5f44ea":"def random_crop(input_image, real_image):\n  stacked_image = tf.stack([input_image, real_image], axis=0)\n  cropped_image = tf.image.random_crop(\n      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n\n  return cropped_image[0], cropped_image[1]","08d95ffc":"# normalizing the images to [-1, 1]\n\ndef normalize(input_image, real_image):\n  input_image = (input_image \/ 127.5) - 1\n  real_image = (real_image \/ 127.5) - 1\n\n  return input_image, real_image","c452372d":"@tf.function()\ndef random_jitter(input_image, real_image):\n  # resizing to 286 x 286 x 3\n  input_image, real_image = resize(input_image, real_image, 286, 286)\n\n  # randomly cropping to 256 x 256 x 3\n  input_image, real_image = random_crop(input_image, real_image)\n\n  if tf.random.uniform(()) > 0.5:\n    # random mirroring\n    input_image = tf.image.flip_left_right(input_image)\n    real_image = tf.image.flip_left_right(real_image)\n\n  return input_image, real_image","2482335d":"# As you can see in the images below\n# that they are going through random jittering\n# Random jittering as described in the paper is to\n# 1. Resize an image to bigger height and width\n# 2. Randomnly crop to the original size\n# 3. Randomnly flip the image horizontally\n\nplt.figure(figsize=(6, 6))\nfor i in range(4):\n  rj_inp, rj_re = random_jitter(inp, re)\n  plt.subplot(2, 2, i+1)\n  plt.imshow(rj_inp\/255.0)\n  plt.axis('off')\nplt.show()","271dc38c":"def load_image_train(image_file):\n  input_image, real_image = load(image_file)\n  input_image, real_image = random_jitter(input_image, real_image)\n  input_image, real_image = normalize(input_image, real_image)\n\n  return input_image, real_image","5559d422":"def load_image_test(image_file):\n  input_image, real_image = load(image_file)\n  input_image, real_image = resize(input_image, real_image,\n                                   IMG_HEIGHT, IMG_WIDTH)\n  input_image, real_image = normalize(input_image, real_image)\n\n  return input_image, real_image","25502455":"train_dataset = tf.data.Dataset.list_files(PATH+'train\/*.jpg')\ntrain_dataset = train_dataset.map(load_image_train,\n                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\ntrain_dataset = train_dataset.cache().shuffle(BUFFER_SIZE)\ntrain_dataset = train_dataset.batch(1)","9d050779":"test_dataset = tf.data.Dataset.list_files(PATH+'test\/*.jpg')\ntest_dataset = test_dataset.map(load_image_test)\ntest_dataset = test_dataset.batch(1)","39ba3a28":"OUTPUT_CHANNELS = 3","a7ecc8c2":"def downsample(filters, size, apply_batchnorm=True):\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  result = tf.keras.Sequential()\n  result.add(\n      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n\n  if apply_batchnorm:\n    result.add(tf.keras.layers.BatchNormalization())\n\n  result.add(tf.keras.layers.LeakyReLU())\n\n  return result","f080c69d":"down_model = downsample(3, 4)\ndown_result = down_model(tf.expand_dims(inp, 0))\nprint (down_result.shape)","163618a3":"def upsample(filters, size, apply_dropout=False):\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  result = tf.keras.Sequential()\n  result.add(\n    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n                                    padding='same',\n                                    kernel_initializer=initializer,\n                                    use_bias=False))\n\n  result.add(tf.keras.layers.BatchNormalization())\n\n  if apply_dropout:\n      result.add(tf.keras.layers.Dropout(0.5))\n\n  result.add(tf.keras.layers.ReLU())\n\n  return result","92fbebcf":"up_model = upsample(3, 4)\nup_result = up_model(down_result)\nprint (up_result.shape)","04c2c354":"def Generator():\n  down_stack = [\n    downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n    downsample(128, 4), # (bs, 64, 64, 128)\n    downsample(256, 4), # (bs, 32, 32, 256)\n    downsample(512, 4), # (bs, 16, 16, 512)\n    downsample(512, 4), # (bs, 8, 8, 512)\n    downsample(512, 4), # (bs, 4, 4, 512)\n    downsample(512, 4), # (bs, 2, 2, 512)\n    downsample(512, 4), # (bs, 1, 1, 512)\n  ]\n\n  up_stack = [\n    upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n    upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n    upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n    upsample(512, 4), # (bs, 16, 16, 1024)\n    upsample(256, 4), # (bs, 32, 32, 512)\n    upsample(128, 4), # (bs, 64, 64, 256)\n    upsample(64, 4), # (bs, 128, 128, 128)\n  ]\n\n  initializer = tf.random_normal_initializer(0., 0.02)\n  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n                                         strides=2,\n                                         padding='same',\n                                         kernel_initializer=initializer,\n                                         activation='tanh') # (bs, 256, 256, 3)\n\n  concat = tf.keras.layers.Concatenate()\n\n  inputs = tf.keras.layers.Input(shape=[None,None,3])\n  x = inputs\n\n  # Downsampling through the model\n  skips = []\n  for down in down_stack:\n    x = down(x)\n    skips.append(x)\n\n  skips = reversed(skips[:-1])\n\n  # Upsampling and establishing the skip connections\n  for up, skip in zip(up_stack, skips):\n    x = up(x)\n    x = concat([x, skip])\n\n  x = last(x)\n\n  return tf.keras.Model(inputs=inputs, outputs=x)","cb86a587":"generator = Generator()\n\ngen_output = generator(inp[tf.newaxis,...], training=False)\nplt.imshow(gen_output[0,...])","85ceef56":"def Discriminator():\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  inp = tf.keras.layers.Input(shape=[None, None, 3], name='input_image')\n  tar = tf.keras.layers.Input(shape=[None, None, 3], name='target_image')\n\n  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n\n  down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n  down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n  down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n\n  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n                                kernel_initializer=initializer,\n                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n\n  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n\n  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n\n  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n\n  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n  return tf.keras.Model(inputs=[inp, tar], outputs=last)","d3c5e8b3":"discriminator = Discriminator()\ndisc_out = discriminator([inp[tf.newaxis,...], gen_output], training=False)\nplt.imshow(disc_out[0,...,-1], vmin=-20, vmax=20, cmap='RdBu_r')\nplt.colorbar()","40148c44":"LAMBDA = 100","8fc1402c":"loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)","89c9081a":"def discriminator_loss(disc_real_output, disc_generated_output):\n  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n\n  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n\n  total_disc_loss = real_loss + generated_loss\n\n  return total_disc_loss","dab7c722":"def generator_loss(disc_generated_output, gen_output, target):\n  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n\n  # mean absolute error\n  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n\n  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n\n  return total_gen_loss","61701a88":"generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","99d86a78":"#checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint_prefix = \".\/ckpt\"\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)\nprint('checkpoint = ' + str(checkpoint))\nprint('checkpoint_prefix = ' + str(checkpoint_prefix))\n!pwd\n","dffd4f6a":"EPOCHS = 128","7540d66c":"def generate_images(model, test_input, tar):\n  # the training=True is intentional here since\n  # we want the batch statistics while running the model\n  # on the test dataset. If we use training=False, we will get\n  # the accumulated statistics learned from the training dataset\n  # (which we don't want)\n  prediction = model(test_input, training=True)\n  plt.figure(figsize=(15,15))\n\n  display_list = [test_input[0], tar[0], prediction[0]]\n  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n\n  for i in range(3):\n    plt.subplot(1, 3, i+1)\n    plt.title(title[i])\n    # getting the pixel values between [0, 1] to plot it.\n    plt.imshow(display_list[i] * 0.5 + 0.5)\n    plt.axis('off')\n  plt.show()","3755bebd":"@tf.function\ndef train_step(input_image, target):\n  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n    gen_output = generator(input_image, training=True)\n\n    disc_real_output = discriminator([input_image, target], training=True)\n    disc_generated_output = discriminator([input_image, gen_output], training=True)\n\n    gen_loss = generator_loss(disc_generated_output, gen_output, target)\n    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n\n  generator_gradients = gen_tape.gradient(gen_loss,\n                                          generator.trainable_variables)\n  discriminator_gradients = disc_tape.gradient(disc_loss,\n                                               discriminator.trainable_variables)\n\n  generator_optimizer.apply_gradients(zip(generator_gradients,\n                                          generator.trainable_variables))\n  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n                                              discriminator.trainable_variables))","66ebab06":"def fit(train_ds, epochs, test_ds):\n  for epoch in range(epochs):\n    start = time.time()\n\n    # Train\n    for input_image, target in train_ds:\n      train_step(input_image, target)\n\n    clear_output(wait=True)\n    # Test on the same image so that the progress of the model can be \n    # easily seen.\n    for example_input, example_target in test_ds.take(1):\n      generate_images(generator, example_input, example_target)\n\n    # saving (checkpoint) the model every 20 epochs\n    if (epoch + 1) % 20 == 0:\n      ckpt_save_path = ckpt_manager.save()\n      print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n        \n    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n                                                        time.time()-start))","6d8e125a":"#!ls \"{checkpoint_dir}\"\n!ls","0a797f01":"# restoring the latest checkpoint in checkpoint_dir\nckpt_manager = tf.train.CheckpointManager(checkpoint, \".\/\", max_to_keep=2)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n  checkpoint.restore(ckpt_manager.latest_checkpoint)\n  print ('Latest checkpoint restored!!')\n\n# Run the trained model on the entire test dataset\nfor inp, tar in test_dataset.take(5):\n  generate_images(generator, inp, tar)","58f04cc8":"fit(train_dataset, EPOCHS, test_dataset)","49965422":"# Run the trained model on 10 images from the entire test dataset\nfor inp, tar in test_dataset.take(10):\n  generate_images(generator, inp, tar)","599249f1":"#model_weights_path = os.path.join(output_dir, 'model_weights.h5')\nmodel_weights_path = '.\/model_weights.h5'\nmodel_weights_path\nprint(str(model_weights_path))\ntf.keras.models.save_model(generator, model_weights_path)   \n\nprint(\"Saved model weights to disk\")","2f85768d":"## Settings","6d9f2b97":"## Checkpoints (Object-based saving)","ea528e93":"## Training\n\n* We start by iterating over the dataset\n* The generator gets the input image and we get a generated output.\n* The discriminator receives the input_image and the generated image as the first input. The second input is the input_image and the target_image.\n* Next, we calculate the generator and the discriminator loss.\n* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n* This entire procedure is shown in the images below.\n\n![Discriminator Update Image](https:\/\/github.com\/tensorflow\/docs\/blob\/master\/site\/en\/tutorials\/generative\/images\/dis.png?raw=1)\n\n\n---\n\n\n![Generator Update Image](https:\/\/github.com\/tensorflow\/docs\/blob\/master\/site\/en\/tutorials\/generative\/images\/gen.png?raw=1)\n\n## Generate Images\n\n* After training, its time to generate some images!\n* We pass images from the test dataset to the generator.\n* The generator will then translate the input image into the output we expect.\n* Last step is to plot the predictions and **voila!**","2091e572":"## Input Pipeline","61e681ca":"##### Copyright 2019 The TensorFlow Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");","b889ab8c":"## Restore the latest checkpoint and test","0d038491":"## Import TensorFlow and other libraries","3970fa26":"## Build the Discriminator\n  * The Discriminator is a PatchGAN.\n  * Each block in the discriminator is (Conv -> BatchNorm -> Leaky ReLU)\n  * The shape of the output after the last layer is (batch_size, 30, 30, 1)\n  * Each 30x30 patch of the output classifies a 70x70 portion of the input image (such an architecture is called a PatchGAN).\n  * Discriminator receives 2 inputs.\n    * Input image and the target image, which it should classify as real.\n    * Input image and the generated image (output of generator), which it should classify as fake.\n    * We concatenate these 2 inputs together in the code (`tf.concat([inp, tar], axis=-1)`)","a461927b":"To learn more about the architecture and the hyperparameters you can refer the [paper](https:\/\/arxiv.org\/abs\/1611.07004).","79250ed6":"## Train","f425776a":"**Sketch2Pokemon**","cab04dc3":"## Build the Generator\n  * The architecture of generator is a modified U-Net.\n  * Each block in the encoder is (Conv -> Batchnorm -> Leaky ReLU)\n  * Each block in the decoder is (Transposed Conv -> Batchnorm -> Dropout(applied to the first 3 blocks) -> ReLU)\n  * There are skip connections between the encoder and decoder (as in U-Net).\n\n","2ef25363":"## Define the loss functions and the optimizer\n\n* **Discriminator loss**\n  * The discriminator loss function takes 2 inputs; **real images, generated images**\n  * real_loss is a sigmoid cross entropy loss of the **real images** and an **array of ones(since these are the real images)**\n  * generated_loss is a sigmoid cross entropy loss of the **generated images** and an **array of zeros(since these are the fake images)**\n  * Then the total_loss is the sum of real_loss and the generated_loss\n\n* **Generator loss**\n  * It is a sigmoid cross entropy loss of the generated images and an **array of ones**.\n  * The [paper](https:\/\/arxiv.org\/abs\/1611.07004) also includes L1 loss which is MAE (mean absolute error) between the generated image and the target image.\n  * This allows the generated image to become structurally similar to the target image.\n  * The formula to calculate the total generator loss = gan_loss + LAMBDA * l1_loss, where LAMBDA = 100. This value was decided by the authors of the [paper](https:\/\/arxiv.org\/abs\/1611.07004).","a04b6f0f":"## Export current generator model weights"}}