{"cell_type":{"d709c85a":"code","894ea0ad":"code","fbf9a3db":"code","c7a370d2":"code","f6f30f68":"code","3fa31916":"code","2f32a7fe":"code","9fe2d9bd":"code","b2e9a228":"code","7b5a81ac":"code","ca17e3e0":"code","abf79f22":"code","d2804d61":"code","f9f127af":"code","47d89851":"code","4494a009":"code","372b2d3b":"code","07c8fb94":"code","b88dc853":"code","1633056e":"code","2190910d":"code","eb558203":"code","966cc1f7":"code","a09f6e4d":"code","80cf3cd4":"code","323f3f39":"code","370185e9":"code","32462df0":"code","d6b1484c":"code","68b255dc":"code","fec1a3cf":"code","282920b8":"code","9d1e1bca":"code","e11d136a":"code","afa1f92a":"code","b198eb8f":"code","bd09f931":"code","fd81491d":"markdown","eb371b5b":"markdown","939f7c7b":"markdown","ab66999e":"markdown","614460e6":"markdown","268a19bd":"markdown","5d42739d":"markdown","f7639aa4":"markdown","808d6c25":"markdown","ca155b6d":"markdown","dd1cfd11":"markdown","3dcffe08":"markdown","968147d7":"markdown","07fcc4aa":"markdown","15895b70":"markdown","94a7de37":"markdown","47ba72e4":"markdown","89ece4b2":"markdown","b2e6c142":"markdown","0b6e418a":"markdown","32ee9f1c":"markdown","f28eef3f":"markdown","4e92ec8d":"markdown","61ea12fe":"markdown"},"source":{"d709c85a":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import load_files\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC","894ea0ad":"# Uncomment this to load data from downloaded tar.gz archive.\n# change this path\n#PATH_TO_IMDB = '\/Users\/y.kashnitsky\/Documents\/Machine_learning\/datasets\/aclImdb\/'\n#reviews_train = load_files(os.path.join(PATH_TO_IMDB, \"train\"),\n#                           categories=['pos', 'neg'])\n#text_train, y_train = reviews_train.data, reviews_train.target\n# change the path to the file\n#reviews_test = load_files(os.path.join(PATH_TO_IMDB, \"test\"),\n#                          categories=['pos', 'neg'])\n#text_test, y_test = reviews_test.data, reviews_test.target","fbf9a3db":"# # Alternatively, load data from previously pickled objects. \nimport pickle\nwith open('..\/input\/imdb_text_train.pkl', 'rb') as f:\n     text_train = pickle.load(f)\nwith open('..\/input\/imdb_text_test.pkl', 'rb') as f:\n     text_test = pickle.load(f)\nwith open('..\/input\/imdb_target_train.pkl', 'rb') as f:\n     y_train = pickle.load(f)\nwith open('..\/input\/imdb_target_test.pkl', 'rb') as f:\n     y_test = pickle.load(f)","c7a370d2":"print(\"Number of documents in training data: %d\" % len(text_train))\nprint(np.bincount(y_train))\nprint(\"Number of documents in test data: %d\" % len(text_test))\nprint(np.bincount(y_test))","f6f30f68":"print(text_train[1])","3fa31916":"y_train[1] # bad review","2f32a7fe":"text_train[2]","9fe2d9bd":"y_train[2] # good review","b2e9a228":" #import pickle\n #with open('..\/input\/imdb_text_train.pkl', 'wb') as f:\n #    pickle.dump(text_train, f)\n #with open('..\/input\/imdb_text_test.pkl', 'wb') as f:\n #    pickle.dump(text_test, f)\n #with open('..\/input\/imdb_target_train.pkl', 'wb') as f:\n #    pickle.dump(y_train, f)\n #with open('..\/input\/imdb_target_test.pkl', 'wb') as f:\n #    pickle.dump(y_test, f)","7b5a81ac":"cv = CountVectorizer()\ncv.fit(text_train)\n\nlen(cv.vocabulary_)","ca17e3e0":"print(cv.get_feature_names()[:50])\nprint(cv.get_feature_names()[50000:50050])","abf79f22":"X_train = cv.transform(text_train)\nX_train","d2804d61":"print(text_train[19726])","f9f127af":"X_train[19726].nonzero()[1]","47d89851":"X_train[19726].nonzero()","4494a009":"X_test = cv.transform(text_test)","372b2d3b":"%%time\n%env JOBLIB_TEMP_FOLDER=\/tmp\nlogit = LogisticRegression(solver='lbfgs', n_jobs=-1, random_state=7)\nlogit.fit(X_train, y_train)","07c8fb94":"round(logit.score(X_train, y_train), 3), round(logit.score(X_test, y_test), 3),","b88dc853":"def visualize_coefficients(classifier, feature_names, n_top_features=25):\n    # get coefficients with large absolute values \n    coef = classifier.coef_.ravel()\n    positive_coefficients = np.argsort(coef)[-n_top_features:]\n    negative_coefficients = np.argsort(coef)[:n_top_features]\n    interesting_coefficients = np.hstack([negative_coefficients, positive_coefficients])\n    # plot them\n    plt.figure(figsize=(15, 5))\n    colors = [\"red\" if c < 0 else \"blue\" for c in coef[interesting_coefficients]]\n    plt.bar(np.arange(2 * n_top_features), coef[interesting_coefficients], color=colors)\n    feature_names = np.array(feature_names)\n    #plt.xticks(np.arange(1, 1 + 2 * n_top_features), feature_names[interesting_coefficients], rotation=60, ha=\"right\");","1633056e":"def plot_grid_scores(grid, param_name):\n    plt.plot(grid.param_grid[param_name], grid.cv_results_['mean_train_score'],\n        color='green', label='train')\n    plt.plot(grid.param_grid[param_name], grid.cv_results_['mean_test_score'],\n        color='red', label='test')\n    plt.legend();\n    ","2190910d":"visualize_coefficients(logit, cv.get_feature_names())","eb558203":"%%time\nfrom sklearn.pipeline import make_pipeline\n\ntext_pipe_logit = make_pipeline(CountVectorizer(),\n                                # for some reason n_jobs > 1 won't work \n                                # with GridSearchCV's n_jobs > 1\n                                LogisticRegression(solver='lbfgs', \n                                                   n_jobs=1,\n                                                   random_state=7))\n\ntext_pipe_logit.fit(text_train, y_train)\nprint(text_pipe_logit.score(text_test, y_test))","966cc1f7":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid_logit = {'logisticregression__C': np.logspace(-5, 0, 6)}\ngrid_logit = GridSearchCV(text_pipe_logit, param_grid_logit, \n                          cv=3, n_jobs=-1)\n\ngrid_logit.fit(text_train, y_train)","a09f6e4d":"grid_logit.best_params_, grid_logit.best_score_","80cf3cd4":"plot_grid_scores(grid_logit, 'logisticregression__C')","323f3f39":"grid_logit.score(text_test, y_test)","370185e9":"from sklearn.ensemble import RandomForestClassifier","32462df0":"forest = RandomForestClassifier(n_estimators=200, \n                                n_jobs=-1, random_state=17)","d6b1484c":"%%time\nforest.fit(X_train, y_train)","68b255dc":"round(forest.score(X_test, y_test), 3)","fec1a3cf":"# creating dataset\nrng = np.random.RandomState(0)\nX = rng.randn(200, 2)\ny = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)","282920b8":"plt.scatter(X[:, 0], X[:, 1], s=30, c=y, cmap=plt.cm.Paired);","9d1e1bca":"def plot_boundary(clf, X, y, plot_title):\n    xx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n                     np.linspace(-3, 3, 50))\n    clf.fit(X, y)\n    # plot the decision function for each datapoint on the grid\n    Z = clf.predict_proba(np.vstack((xx.ravel(), yy.ravel())).T)[:, 1]\n    Z = Z.reshape(xx.shape)\n\n    image = plt.imshow(Z, interpolation='nearest',\n                           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n                           aspect='auto', origin='lower', cmap=plt.cm.PuOr_r)\n    contours = plt.contour(xx, yy, Z, levels=[0], linewidths=2,\n                               linetypes='--')\n    plt.scatter(X[:, 0], X[:, 1], s=30, c=y, cmap=plt.cm.Paired)\n    plt.xticks(())\n    plt.yticks(())\n    plt.xlabel(r'$x_1$')\n    plt.ylabel(r'$x_2$')\n    plt.axis([-3, 3, -3, 3])\n    plt.colorbar(image)\n    plt.title(plot_title, fontsize=12);","e11d136a":"plot_boundary(LogisticRegression(), X, y,\n              \"Logistic Regression, XOR problem\")","afa1f92a":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline","b198eb8f":"logit_pipe = Pipeline([('poly', PolynomialFeatures(degree=2)), \n                       ('logit', LogisticRegression())])","bd09f931":"plot_boundary(logit_pipe, X, y,\n              \"Logistic Regression + quadratic features. XOR problem\")","fd81491d":"**Tercero, aplicaremos las mismas operaciones al conjunto de prueba.**","eb371b5b":"Para el conjunto de validaci\u00f3n:","939f7c7b":"**Para empezar, descarga el conjunto de datos [aqui](http:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/aclImdb_v1.tar.gz) (enlace de descarga directa). El conjunto de datos se describe brevemente [aqui](http:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/). Hay 12.5k de buenas y malas cr\u00edticas en los sets de prueba y entrenamiento.**","ab66999e":"Pero si uno tuviera que dar caracter\u00edsticas polinomiales como entrada (aqu\u00ed, hasta 2 grados), entonces el problema est\u00e1 resuelto.","614460e6":"**Imprimamos mejor $C$ y cv-score usando este hiperpar\u00e1metro:**","268a19bd":"**El siguiente paso es entrenar la Regresi\u00f3n Log\u00edstica.**","5d42739d":"### XOR-Problem\nConsideremos ahora un ejemplo donde los modelos lineales son peores.\n\nLos m\u00e9todos de clasificaci\u00f3n lineal siguen definiendo una superficie de separaci\u00f3n muy simple: un hiperplano. El ejemplo de juguete m\u00e1s famoso donde las clases no se pueden dividir por un hiperplano (o l\u00ednea) sin errores es \"el problema XOR\".\n\nXOR es el \"OR exclusivo\", una funci\u00f3n booleana con la siguiente tabla de verdad:\n\n\n\n<img src='https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/XOR_table.gif'>\n\nXOR es el nombre dado a un problema de clasificaci\u00f3n binaria simple en el que las clases se presentan como nubes de puntos de intersecci\u00f3n extendidas diagonalmente.","f7639aa4":"**Si observa los ejemplos de \"palabras\" (llam\u00e9moslos tokens), puede ver que hemos omitido muchos de los pasos importantes en el procesamiento de texto (el procesamiento autom\u00e1tico de texto puede ser una serie de art\u00edculos completamente separados).**","808d6c25":"<center>\n<img src=\"https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/ods_stickers.jpg\" \/>\n    \n## [mlcourse.ai](mlcourse.ai), open Machine Learning course \n\nAuthor: [Yury Kashnitskiy](https:\/\/yorko.github.io). Translated and edited by [Christina Butsko](https:\/\/www.linkedin.com\/in\/christinabutsko\/), [Nerses Bagiyan](https:\/\/www.linkedin.com\/in\/nersesbagiyan\/), [Yulia Klimushina](https:\/\/www.linkedin.com\/in\/yuliya-klimushina-7168a9139), and [Yuanyuan Pao](https:\/\/www.linkedin.com\/in\/yuanyuanpao\/). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license. Free use is permitted for any non-commercial purpose.","ca155b6d":"### Recursos \u00fatiles\n- Medium [\"historia\"](https:\/\/medium.com\/open-machine-learning-course\/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220) basado en este cuaderno\n- Materiales del curso como un [conjunto de datos de Kaggle](https:\/\/www.kaggle.com\/kashnitsky\/mlcourse)\n- Si lees ruso: un [art\u00edculo] (https:\/\/habrahabr.ru\/company\/ods\/blog\/323890\/) en Habrahabr con ~ el mismo material. Y una [conferencia](https:\/\/youtu.be\/oTXGQ-_oqvI) en YouTube\n- En el libro [\"Aprendizaje profundo\"](http:\/\/www.deeplearningbook.org) (I. Goodfellow, Y. Bengio y A. Courville) se ofrece una descripci\u00f3n general agradable y concisa de los modelos lineales.\n- Los modelos lineales est\u00e1n cubiertos pr\u00e1cticamente en todos los libros de ML. Recomendamos \u201cReconocimiento de patrones y aprendizaje autom\u00e1tico\u201d (C. Bishop) y \u201cAprendizaje autom\u00e1tico: una perspectiva probabil\u00edstica\u201d (K. Murphy).\n- Si prefiere una visi\u00f3n general del modelo lineal desde el punto de vista de un estad\u00edstico, mire \"Los elementos del aprendizaje estad\u00edstico\" (T. Hastie, R. Tibshirani y J. Friedman).\n- El libro \u201cAprendizaje de m\u00e1quinas en acci\u00f3n\u201d (P. Harrington) lo guiar\u00e1 a trav\u00e9s de las implementaciones de algoritmos ML cl\u00e1sicos en Python puro.\n- [Scikit-learn](http:\/\/scikit-learn.org\/stable\/documentation.html) biblioteca. Estos chicos trabajan duro para escribir documentaci\u00f3n muy clara.\n- Scipy 2017 [tutorial de scikit-learn](https:\/\/github.com\/amueller\/scipy-2017-sklearn) por Alex Gramfort y Andreas Mueller.\n- Uno m\u00e1s [curso de ML](https:\/\/github.com\/diefimov\/MTH594_MachineLearning) con muy buenos materiales.\n- [Implementaciones](https:\/\/github.com\/rushter\/MLAlgorithms) de muchos algoritmos ML. B\u00fasqueda de regresi\u00f3n lineal y regresi\u00f3n log\u00edstica.","dd1cfd11":"**Para mejorar nuestro modelo, podemos optimizar el coeficiente de regularizaci\u00f3n para la 'Regresi\u00f3n log\u00edstica'. Usaremos `sklearn.pipeline` porque` CountVectorizer` solo debe aplicarse a los datos de entrenamiento (para no \"echar un vistazo\" al conjunto de prueba y no contar las frecuencias de palabras all\u00ed). En este caso, `pipeline` determina la secuencia correcta de acciones: aplique` CountVectorizer`, luego entrene `Logistic Regression`.**","3dcffe08":"Obviamente, uno no puede dibujar una l\u00ednea recta para separar una clase de otra sin errores. Por lo tanto, la regresi\u00f3n log\u00edstica funciona mal con esta tarea.","968147d7":"**Ahora hagamos lo mismo con el bosque al azar. Vemos que, con regresi\u00f3n log\u00edstica, logramos una mayor precisi\u00f3n con menos esfuerzo.**","07fcc4aa":"**Veamos c\u00f3mo funcion\u00f3 nuestra transformaci\u00f3n.**","15895b70":"# <center>Topic 4. Clasificaci\u00f3n lineal y regresi\u00f3n\n## <center> Part 4. Donde la regresi\u00f3n log\u00edstica es buena y donde no es","94a7de37":"**Aqu\u00ed hay algunos ejemplos de las cr\u00edticas.**","47ba72e4":"**Primero, crearemos un diccionario de todas las palabras usando CountVectorizer**","89ece4b2":"### An\u00e1lisis de cr\u00edticas de pel\u00edculas IMDB","b2e6c142":"## Un simple recuento de palabras","0b6e418a":"**Echemos un vistazo a la precisi\u00f3n en el entrenamiento y los conjuntos de pruebas.**","32ee9f1c":"\u00a1Ahora para un poco de pr\u00e1ctica! Queremos resolver el problema de la clasificaci\u00f3n binaria de cr\u00edticas de pel\u00edculas IMDB. Tenemos un conjunto de entrenamiento con comentarios marcados, 12500 comentarios marcados como buenos, otros 12500 malos. Aqu\u00ed, no es f\u00e1cil comenzar con el aprendizaje autom\u00e1tico de inmediato porque no tenemos la matriz $X$; Necesitamos prepararla. Usaremos un enfoque simple: modelo de bolsa de palabras. Las caracter\u00edsticas de la revisi\u00f3n estar\u00e1n representadas por los indicadores de la presencia de cada palabra de todo el corpus en esta revisi\u00f3n. El corpus es el conjunto de todas las opiniones de los usuarios. La idea est\u00e1 ilustrada por una imagen.\n\n<img src=\"https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/bag_of_words.svg\" width=80% \/>","f28eef3f":"**En segundo lugar, estamos codificando las oraciones de los textos de los conjuntos de entrenamiento con los \u00edndices de las palabras entrantes. Usaremos el formato disperso.**","4e92ec8d":"Here, logistic regression has still produced a hyperplane but in a 6-dimensional feature space $1, x_1, x_2, x_1^2, x_1x_2$ and $x_2^2$. When we project to the original feature space, $x_1, x_2$, the boundary is nonlinear.\n\nAqu\u00ed, la regresi\u00f3n log\u00edstica todav\u00eda ha producido un hiperplano pero en un espacio de caracter\u00edsticas de 6 dimensiones $1, x_1, x_2, x_1^2, x_1x_2$. Cuando proyectamos al espacio de la caracter\u00edstica original, $x_1, x_2$, el l\u00edmite no es lineal.\n\nEn la pr\u00e1ctica, las caracter\u00edsticas polinomiales ayudan, pero es computacionalmente ineficiente construirlas expl\u00edcitamente. SVM con el truco del kernel funciona mucho m\u00e1s r\u00e1pido. En este enfoque, solo se calcula la distancia entre los objetos (definidos por la funci\u00f3n del kernel) en un espacio dimensional alto, y no hay necesidad de producir un n\u00famero combinado de caracter\u00edsticas.","61ea12fe":"**Los coeficientes del modelo se pueden mostrar maravillosamente.**"}}