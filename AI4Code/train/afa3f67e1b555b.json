{"cell_type":{"062500de":"code","9e07c9fb":"code","cba518ab":"code","f2528826":"code","f309a77b":"code","f85eedef":"code","d19e1516":"code","c673750d":"code","8d3dd1e6":"code","d0b24f9b":"code","a170dec2":"code","6eb6c7d1":"code","fd245b79":"code","a6e86317":"code","c62f11b4":"code","ded5c450":"code","7d793ebc":"code","256e684d":"code","aae383fe":"code","20708123":"code","dc8b0330":"markdown","3cd36eeb":"markdown","a1bc630d":"markdown","cf651a0b":"markdown","0047a0b0":"markdown","e0578ce3":"markdown"},"source":{"062500de":"import numpy as np\nfrom numpy import ndarray\n\nimport pandas as pd\nfrom typing import Dict, Tuple\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","9e07c9fb":"df = pd.read_csv('..\/input\/autompg-dataset\/auto-mpg.csv')\ndf = df.dropna()\ndf = df[df['horsepower'] != '?']\ndf.head()","cba518ab":"# calculate the correlation matrix\ncorr = df[['cylinders', 'displacement', 'horsepower', 'weight', 'mpg']].corr()\n\n# plot the heatmap\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns)","f2528826":"df_features = df[['cylinders', 'displacement', 'horsepower', 'weight']]\ndf_features = df_features.apply(pd.to_numeric)\n\ndf_labels = df['mpg']","f309a77b":"# normalize features\ndf_features = (df_features - df_features.mean()) \/ df_features.std()\ndf_features.head()","f85eedef":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_features.values, df_labels.values, test_size=0.2, random_state=0)","d19e1516":"Batch = Tuple[ndarray, ndarray]\n\ndef generate_batch(X: ndarray, \n                   y: ndarray,\n                   start: int = 0,\n                   batch_size: int = 10) -> Batch:\n    '''\n    Generate batch from X and y, given a start position\n    '''\n\n    if start+batch_size > X.shape[0]:\n        batch_size = X.shape[0] - start\n    \n    X_batch, y_batch = X[start:start+batch_size], y[start:start+batch_size]\n    \n    return X_batch, y_batch","c673750d":"def forward(X: ndarray,\n           y: ndarray,\n           weights: Dict[str, ndarray]) -> Tuple[Dict[str, ndarray], float]:\n    '''\n    Forward propagation\n    '''\n    N = np.dot(X, weights['W'])\n    P = N + weights['B']\n\n    loss = np.mean(np.power(y - P, 2))\n\n    forward_info: Dict[str, ndarray] = {}\n    forward_info['X'] = X\n    forward_info['N'] = N\n    forward_info['P'] = P\n    forward_info['y'] = y\n\n    return forward_info, loss","8d3dd1e6":"def backward(forward_info: Dict[str, ndarray],\n             weights: Dict[str, ndarray]) -> Dict[str, ndarray]:\n    '''\n    Compute dLdW and dLdB for the step-by-step linear regression model.\n    '''\n    batch_size = forward_info['X'].shape[0]\n\n    dLdP = -2 * (forward_info['y'].reshape(forward_info['y'].shape[0], 1) - forward_info['P'])\n\n    dPdN = np.ones_like(forward_info['N'])\n\n    dPdB = np.ones_like(weights['B'])\n\n    dLdN = dLdP * dPdN\n\n    dNdW = np.transpose(forward_info['X'], (1, 0))\n        \n    dLdW = np.dot(dNdW, dLdN)  \n    dLdB = (dLdP * dPdB).sum(axis=0)\n\n    loss_gradients: Dict[str, ndarray] = {}\n    loss_gradients['W'] = dLdW\n    loss_gradients['B'] = dLdB\n\n    return loss_gradients","d0b24f9b":"def permute_data(X: ndarray, y: ndarray):\n    '''\n    Permute X and y, using the same permutation, along axis=0\n    '''\n    perm = np.random.permutation(X.shape[0])\n    return X[perm], y[perm]","a170dec2":"def to_2d_np(a: ndarray, \n             type: str = \"col\") -> ndarray:\n    '''\n    Turns a 1D Tensor into 2D\n    '''\n\n    assert a.ndim == 1, \\\n    \"Input tensors must be 1 dimensional\"\n    \n    if type == \"col\":        \n        return a.reshape(-1, 1)\n    elif type == \"row\":\n        return a.reshape(1, -1)","6eb6c7d1":"def init_weights(n_in: int) -> Dict[str, ndarray]:\n    '''\n    Initialize weights on first forward pass of model.\n    '''\n    \n    weights: Dict[str, ndarray] = {}\n    W = np.random.randn(n_in, 1)\n    B = np.random.randn(1, 1)\n    \n    weights['W'] = W\n    weights['B'] = B\n\n    return weights","fd245b79":"def train(X: ndarray, \n          y: ndarray, \n          n_iter: int = 300,\n          learning_rate: float = 0.01,\n          batch_size: int = 16,\n          return_losses: bool = False, \n          return_weights: bool = False, \n          seed: int = 1) -> None:\n    '''\n    Train model for a certain number of epochs.\n    '''\n    if seed:\n        np.random.seed(seed)\n    start = 0\n\n    # Initialize weights\n    weights = init_weights(X.shape[1])\n\n    # Permute data\n    X, y = permute_data(X, y)\n    \n    if return_losses:\n        losses = []\n\n    for i in range(n_iter):\n\n        # Generate batch\n        if start >= X.shape[0]:\n            X, y = permute_data(X, y)\n            start = 0\n        \n        X_batch, y_batch = generate_batch(X, y, start, batch_size)\n        start += batch_size\n    \n        # Train net using generated batch\n        forward_info, loss = forward(X_batch, y_batch, weights)\n\n        if return_losses:\n            losses.append(loss)\n\n        loss_grads = backward(forward_info, weights)\n        for key in weights.keys():\n            weights[key] -= learning_rate * loss_grads[key]\n\n    if return_weights:\n        return losses, weights\n    \n    return None","a6e86317":"train_info = train(X_train, y_train,\n                   n_iter = 300,\n                   learning_rate = 0.001,\n                   batch_size=16, \n                   return_losses=True, \n                   return_weights=True, \n                   seed=180708)\nlosses = train_info[0]\nweights = train_info[1]","c62f11b4":"plt.plot(list(range(300)), losses);","ded5c450":"def predict(X: ndarray,\n            weights: Dict[str, ndarray]):\n    '''\n    Generate predictions from the step-by-step linear regression model.\n    '''\n\n    N = np.dot(X, weights['W'])\n\n    return N + weights['B']","7d793ebc":"def mae(preds: ndarray, actuals: ndarray):\n    '''\n    Compute mean absolute error.\n    '''\n    return np.mean(np.abs(preds - actuals))","256e684d":"def rmse(preds: ndarray, actuals: ndarray):\n    '''\n    Compute root mean squared error.\n    '''\n    return np.sqrt(np.mean(np.power(preds - actuals, 2)))","aae383fe":"preds = predict(X_test, weights)\n\nprint(\"Mean absolute error:\", round(mae(preds, y_test), 4), \"\\n\"\n      \"Root mean squared error:\", round(rmse(preds, y_test), 4))","20708123":"plt.plot(y_test, preds, '.')\n\n# plot a line, a perfit predict would all fall on this line\nx = np.linspace(0, 47, 9)\ny = x\nplt.plot(x, y)\nplt.show()","dc8b0330":"<h3 align='center' style='border: 1px dotted red'>Prepare Data<\/h3>","3cd36eeb":"We can deduct a strong correlation between mpg and (cylinders, displacement, weight).","a1bc630d":"<h3 align='center' style='border: 1px dotted red'>Analyze Predictions<\/h3>","cf651a0b":"<h3 align='center' style='border: 1px dotted red'>Linear Regression<\/h3>","0047a0b0":"<h3 align='center' style='border: 1px dotted red'>Auto MPG Linear Regression from Scratch<\/h3>","e0578ce3":"<h3 align='center' style='border: 1px dotted red'>Exploratory Data Analysis<\/h3>"}}