{"cell_type":{"acddb639":"code","40c85a5c":"code","c7435c45":"code","55a668e8":"code","e7adb308":"code","21660a31":"code","e77bbce9":"code","7a681fa6":"code","64a7058b":"code","32731f9b":"code","38d16fed":"code","61d3c406":"code","7f3debaa":"code","8703f797":"code","ea92febd":"code","fb2b86c1":"code","402ed7e4":"code","94fce165":"code","8be1ca0a":"code","f929bfaf":"code","5be04160":"code","5aa89208":"code","baf9dd05":"code","a62ccff5":"code","2a5763c1":"code","ebac4cee":"code","78d38f96":"code","240a562f":"code","0b7b8240":"code","bdc72ab1":"code","48d1bb23":"code","d8ddb290":"code","81fa0341":"code","c902b98c":"code","eddf7924":"markdown","0f6e1830":"markdown","8a6c405e":"markdown","9da9536c":"markdown","58890578":"markdown","d302e564":"markdown","41cfb4e6":"markdown","f3fe8bc8":"markdown","203d03ff":"markdown","136f692e":"markdown","89ea3b7d":"markdown","a57b813e":"markdown","1c4bb35c":"markdown","139e0e9c":"markdown","1e3ffbe0":"markdown","4ea3171d":"markdown","ae0e5b14":"markdown","7b33a40c":"markdown","1016b119":"markdown","dad268bf":"markdown","60dc656a":"markdown","db59c603":"markdown","08280639":"markdown","a6ef0662":"markdown","3cd24387":"markdown","cb8f85ce":"markdown","dcb2c3fd":"markdown","5755648f":"markdown","8c956ff0":"markdown","5122692e":"markdown","d756dc7f":"markdown","7f0fddac":"markdown","0ab3dbab":"markdown","0194f74d":"markdown","3ba93c98":"markdown"},"source":{"acddb639":"import tensorflow as tf\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \nos.environ[\"KMP_SETTINGS\"] = \"false\"\n\nimport numpy as np\nimport glob\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Model, Sequential\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, LeakyReLU, Conv2DTranspose, Dropout, ReLU, Input, Concatenate, concatenate\nfrom tqdm import tqdm\n\nimport pathlib\nimport datetime\nfrom IPython import display","40c85a5c":"def read_png(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_png(img, channels=3)\n    return img","c7435c45":"def split_image(image):\n    w = tf.shape(image)[1]\n    w = w \/\/ 2\n    sketch_image = image[:, :w, :]\n    colored_image = image[:, w:, :]\n    sketch_image = tf.image.resize(sketch_image, (256, 256))\n    colored_image = tf.image.resize(colored_image, (256, 256))\n    return sketch_image, colored_image","55a668e8":"def flip_image(image):\n    return tf.image.flip_left_right(image)","e7adb308":"def random_jitter(sketch_image, colored_image):\n#     sketch_image, colored_image = resize(sketch_image, colored_image, 286, 286)\n\n#     sketch_image, colored_image = random_crop(sketch_image, colored_image)\n\n    if tf.random.uniform(()) > 0.5:\n        sketch_image = flip_image(sketch_image)\n        colored_image = flip_image(colored_image)\n\n    return sketch_image, colored_image\n    ","21660a31":"def normalize(sketch_image, colored_image):\n    sketch_image = tf.cast(sketch_image, tf.float32)\/127.5 - 1\n    colored_image = tf.cast(colored_image, tf.float32)\/127.5 - 1\n    return sketch_image, colored_image","e77bbce9":"def load_image_train(image_path):\n    image = read_png(image_path)\n    sketch_image, colored_image = split_image(image)\n    sketch_image, colored_image = random_jitter(sketch_image, colored_image)\n    sketch_image, colored_image = normalize(sketch_image, colored_image)\n\n    return colored_image, sketch_image","7a681fa6":"train_path = '..\/input\/anime-sketch-colorization-pair\/data\/train\/'\ntrain_images_path = [os.path.join(train_path, image_name) for image_name in os.listdir(train_path)]\nprint(len(train_images_path))","64a7058b":"BATCH_SIZE = 4\nBUFFER_SIZE = 400\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_images_path)\ntrain_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n\nfor sketch, color in train_dataset.take(1):\n    plt.subplot(1,2,1)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(sketch[0]))\n    plt.subplot(1,2,2)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(color[0]))","32731f9b":"def load_image_test(image_path):\n    image = read_png(image_path)\\\n    \n    sketch_image, colored_image = split_image(image)\n    sketch_image, colored_image = normalize(sketch_image, colored_image)\n\n    return colored_image, sketch_image","38d16fed":"test_path = '..\/input\/anime-sketch-colorization-pair\/data\/val\/'\ntest_images_path = [os.path.join(test_path, image_name) for image_name in os.listdir(test_path)]\nprint(len(test_images_path))","61d3c406":"test_dataset = tf.data.Dataset.from_tensor_slices(test_images_path)\ntest_dataset = test_dataset.map(load_image_test)\ntest_dataset = test_dataset.batch(BATCH_SIZE)\n\nfor sketch, color in test_dataset.take(1):\n    plt.subplot(1,2,1)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(sketch[0]))\n    plt.subplot(1,2,2)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(color[0]))","7f3debaa":"OUTPUT_CHANNELS = 3","8703f797":"def downsample(filters, size, apply_batchnorm=True):\n    block = Sequential()\n    block.add(Conv2D(filters, size, strides=2, padding='same', use_bias=False))\n\n    if apply_batchnorm:\n        block.add(BatchNormalization())\n        \n    block.add(LeakyReLU())\n\n    return block","ea92febd":"def upsample(filters, size, apply_dropout=False):\n    block = Sequential()\n    block.add(Conv2DTranspose(filters, size, strides=2, padding='same', use_bias=False))\n    block.add(BatchNormalization())\n\n    if apply_dropout:\n        block.add(Dropout(0.5))\n\n    block.add(ReLU())\n\n    return block","fb2b86c1":"def Generator():\n    inp = Input(shape=[256,256,3])\n    x = inp\n    \n    down_stack = [\n        downsample(64, 4, apply_batchnorm=False),\n        downsample(128, 4),\n        downsample(256, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n    ]\n    \n    bottleneck = downsample(512, 4)\n    # decoder stack\n    up_stack = [\n        upsample(512, 4, apply_dropout=True),\n        upsample(512, 4, apply_dropout=True),\n        upsample(512, 4, apply_dropout=True),\n        upsample(512, 4),\n        upsample(256, 4),\n        upsample(128, 4),\n        upsample(64, 4),\n    ]\n\n    last_layer = Conv2DTranspose(OUTPUT_CHANNELS, 4, strides=2, padding='same', activation='tanh')\n    \n    # Downsampling\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n    \n    x = bottleneck(x)\n    skips.reverse()\n\n    # Upsampling + creating skip connections for the i-th encoder and (n-i)-th decoder\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = Concatenate()([x, skip])\n\n    x = last_layer(x)\n\n    return Model(inputs=inp, outputs=x)","402ed7e4":"generator = Generator()\ntf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)","94fce165":"def Discriminator():\n    inp = Input(shape=[256, 256, 3], name='sketch_image')\n    target = Input(shape=[256, 256, 3], name='colored_image')\n\n    x = concatenate([inp, target])\n    \n    block_stack = [\n        downsample(64, 4, apply_batchnorm=False),\n        downsample(128, 4),\n        downsample(256, 4),\n        downsample(512, 4)\n    ]\n    \n    last_layer = Conv2D(1, 4, strides=1, padding='same')\n    \n    for block in block_stack:\n        x = block(x)\n    \n    x = last_layer(x)\n\n    return Model(inputs=[inp, target], outputs=x)","8be1ca0a":"discriminator = Discriminator()\ntf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)","f929bfaf":"loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\nLAMBDA = 100\n\ndef generator_loss(disc_generated_output, gen_output, target):\n    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n\n    return gan_loss + (LAMBDA * l1_loss)","5be04160":"def discriminator_loss(disc_real_output, disc_generated_output):\n    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n\n    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n    \n    return real_loss + generated_loss","5aa89208":"generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","baf9dd05":"def generate_images(model, test_input, target):\n    prediction = model(test_input, training=True)\n    plt.figure(figsize=(16,16))\n    \n    # rearrange value of each image from [-1,1] to [0,1]\n    # y = x\/127.5 - 1 -> this is the formula to normalize image from [0, 255] to [-1,1]\n    # x = (y + 1)*127.5 -> this return the normalized value from [-1, 1] to [0, 255]\n    # x = 0.5y + 0.5 -> divide by 255 to normalize it into [0, 1]\n\n    plt.subplot(1, 3, 1)\n    plt.title(\"Sketch Image\")\n    plt.imshow(test_input[0]*0.5+0.5)\n    plt.axis('off')\n\n    plt.subplot(1, 3, 2)\n    plt.title(\"Target Image\")\n    plt.imshow(target[0]*0.5+0.5)\n    plt.axis('off')\n\n    plt.subplot(1, 3, 3)\n    plt.title(\"Predicted Image\")\n    plt.imshow(prediction[0]*0.5+0.5)\n    plt.axis('off')\n    \n    plt.show()","a62ccff5":"# generator=tf.keras.models.load_model(\"..\/input\/pix2pix-model\/pix2pix_generator.h5\")\n# discriminator=tf.keras.models.load_model(\"..\/input\/pix2pix-model\/pix2pix_discriminator.h5\")","2a5763c1":"EPOCHS = 30","ebac4cee":"@tf.function\ndef train_step(sketch_image, ground_truth, epoch):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generator_output = generator(sketch_image, training=True)\n\n        disc_real_output = discriminator([sketch_image, ground_truth], training=True)\n        disc_generated_output = discriminator([sketch_image, generator_output], training=True)\n\n        gen_loss = generator_loss(disc_generated_output, generator_output, ground_truth)\n        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n\n    generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n    return gen_loss, disc_loss","78d38f96":"epoch_loss_avg_gen = tf.keras.metrics.Mean('g_loss')\nepoch_loss_avg_disc = tf.keras.metrics.Mean('d_loss')","240a562f":"generator_mean_losses = []\ndiscriminator_mean_losses = []","0b7b8240":"def fit(train_ds, epochs, test_ds):\n    for epoch in range(1, epochs+1):\n        for example_input, example_target in test_ds.take(1):\n            generate_images(generator, example_input, example_target)\n        print(\"Epoch: \", epoch)\n        \n        for n, (sketch_image, colored_image) in tqdm(train_ds.enumerate()):\n            g_loss, d_loss = train_step(sketch_image, colored_image, epoch)\n            epoch_loss_avg_gen(g_loss)\n            epoch_loss_avg_disc(d_loss)\n             \n        print()\n        print(\"Generator Loss: %.2f\" % epoch_loss_avg_gen.result().numpy())\n        print(\"Discriminator Loss: %.2f\" % epoch_loss_avg_disc.result().numpy())\n        print(\"=====================================================\")\n        \n        generator_mean_losses.append(epoch_loss_avg_gen.result().numpy())\n        discriminator_mean_losses.append(epoch_loss_avg_disc.result().numpy())\n        \n        epoch_loss_avg_gen.reset_states()\n        epoch_loss_avg_disc.reset_states()","bdc72ab1":"fit(train_dataset, EPOCHS, test_dataset)","48d1bb23":"plt.plot(range(1,EPOCHS+1), generator_mean_losses)\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.title('Generator Loss Plot')\nplt.show()","d8ddb290":"plt.plot(range(1,EPOCHS+1), discriminator_mean_losses)\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.title('Discriminator Loss Plot')\nplt.show()","81fa0341":"for n, data in enumerate(test_dataset):\n    if(n>35 and n<40):\n        sketch_image = data[0]\n        ground_truth = data[1]\n        generate_images(generator, sketch_image, ground_truth)\n    elif (n>40):\n        break","c902b98c":"generator.save('pix2pix_generator.h5')\ndiscriminator.save('pix2pix_discriminator.h5')","eddf7924":"# Defining Pix2Pix Custom Training Step\n\nIn this project i used custom training step that referenced from keras website\n\nsource: [https:\/\/keras.io\/guides\/writing_a_training_loop_from_scratch\/](https:\/\/keras.io\/guides\/writing_a_training_loop_from_scratch\/)\n\n1. Generate the losses for both generator and discriminator\n2. Get the gradient of losses\n3. Apply those to the optimizer","0f6e1830":"**Defining function for flipping the image**","8a6c405e":"# Defining Custom Discriminator Loss Function\n\nDiscriminator loss function used in the paper are the sum of the binary cross entropy loss when the discriminator is fed with the real image and when the discriminator is fed with generated image\n\ndiscriminator_loss = real_loss + generated_loss","9da9536c":"# Defining Preprocessing function for the image\n\n**Defining function for reading the image and convert it into tensor array**","58890578":"**Creating train data pipeline using tensorflow Data**\n\nBatch size = 4 and buffer size = 400 used for this dataset are same with the one that are used in the paper for edge -> shoes task.","d302e564":"**Defining function for splitting image into colored image(512x512) and sketch image(512x512)**","41cfb4e6":"**Defining function for random jitter and image flip function as described in the pix2pix paper**\n\nIn the paper there will be 2 step, first one is random cropping the corner of the image and the second one is flipping the image left and right (so all the images won't be looking to the right or to the left)\n\nIn this project, i didn't use random cropping as the paper say since most of the images aren't full of object and the object are centered.","f3fe8bc8":"# Conclusion\n1. Many of the colored image generated by the model are already acceptable\/looks real\n2. The model could already differentiate skin and clothes from the sketch\n3. Since many girl in the images from dataset use stockings and the sketch make it look like it's only skin make it harder to differentiate between skin and stocking\n4. The sketch\/anime variation in the dataset varies so widely that makes the generated colored image different from the target. Example: Hair color, eyes color, clothes color, etc.\n5. I think that this model will do better if the variation is greatly reduced, we could use Manga to Color dataset if there are some (since i didn't find any). Since manga characters will be limited and not varies widely like this dataset.","203d03ff":"# Load Dataset that will be used in the Project\n\nDataset that will be used is anime-sketch-colorization-pair from kaggle\n\nlink to dataset: [https:\/\/www.kaggle.com\/ktaebum\/anime-sketch-colorization-pair](https:\/\/www.kaggle.com\/ktaebum\/anime-sketch-colorization-pair)\n\nThis dataset contains 14200 pairs of sketch and colored image used for training and 3545 pairs of sketch and colored image used for validation formatted as (.png) file. Since this dataset sized 6.5 GB, I used kaggle notebook so i don't need to download the dataset.\n\nAll image in the dataset look like this\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/91717\/212894\/data\/train\/1002008.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20220130%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220130T154055Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=2ba3fb23e84062cbee6d2048ff240ea012fc5031f98c3377a8f7463cebdae29d8a922d4f97913e9a368627db7c1ece04bf6890fce7813af615801141bb35429fd38abf5b36be77f3fcd98d227491a87f2edcdedd2c8403f49cf5a74ffb779b43e64cbc16caf9acff82c224b309709164f62e2d131f12d6516bbccaf16444db2b556170fc6989fc331432de4e6bdc4182f635347d4e9dfdb81f405d780eda22af3ee9a33a785aa329a735b3bacceca35f093c50953bbe39a38b53344db23f8255206f7ed37cf8f914cff3d7d5903a8567b9c8363d5432fcf0d19518a1e8c3efc04dea931ad84d2a913a190596df4c69952378b15f7ba80e65e2019fc4d7dd565f\">\n\nEach image will be sized 1024x512 containing 2 image (colored and sketch). Colored image will take half of the size of full image and sketch image will take the other half. That's why I need to split the image into 2 (512x512 color image and 512x512 sketch image).","136f692e":"**Creating test data pipeline using tensorflow Data**","89ea3b7d":"**Resulting U-net model could be seen from the image below**","a57b813e":"**Defining function to normalize all image into [-1 to 1] range**","1c4bb35c":"# Save the model","139e0e9c":"**Defining function to create encoder block**","1e3ffbe0":"# Result after training","4ea3171d":"**Defining function for loading test image and apply preprocess**","ae0e5b14":"# Training Loop\n* In every epoch we will generate images using the generator to see the result from training.\n* Do train step for each batch in train dataset\n* Calculate the average loss for both generator and discriminator","7b33a40c":"# Plot loss for discriminator model","1016b119":"**Defining function to build the generator model**","dad268bf":"# 2301865741 - Edgard Jonathan Putra Pranoto\n\n# Project Title: Sketch2Color using Conditional GAN\n\nIn this project i tried to implement Conditional GAN based on paper, the task that i chose is coloring sketch using the conditional gan architecture described in the paper. The generator model that will be used is U-net which is Encoder and Decoder with skip connection and the Discriminator that will be used is PatchGAN which classify if each patch from the image is real or fake.\n\npaper link: [https:\/\/arxiv.org\/pdf\/1611.07004.pdf](https:\/\/arxiv.org\/pdf\/1611.07004.pdf)","60dc656a":"**Defining function for generating image**","db59c603":"# Building the Generator Model\n\nGenerator model that are used in this project are U-net model identical with the paper, which is basically encoder and decoder with skip connection for the i and n-i layer. We can also use autoencoder model which doesn't have skip as the paper said if we want.\n\n*note: Architecture for the generator model is the same with the one that the paper proposed.\n\n<img src=\"data:image\/png;base64,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\">\n\n**Detail for the encoder and decoder for U-net model**\n\nEncoder:\n\nC64-C128-C256-C512-C512-C512-C512-C512\n\nDecoder:\n\nCD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n\n* Each encoder block contains convolution layer, batch_normalization layer (except for the first block), and leaky relu layer\n* Each decoder block contains transposed convolution layer, dropout layer (for the first 3 block), and relu layer\n* There will be skip connection between i-th encoder and (n-i)-th decoder\n* Last layer uses a single convolutional layer with three channels and tanh activation function which is commonly used for GAN Model","08280639":"# Building input pipeline for the dataset","a6ef0662":"# Plot loss for the generator model","3cd24387":"**Getting the test image path**","cb8f85ce":"# Building the Discriminator Model\n\nDiscriminator model that are used in this project is PatchGAN identical with the paper, which basically classifies if each patch from the image is real or fake (0\/1). In the research paper, the author tried pixelGAN and imageGAN also, but the patchGAN generate the best result and ran faster for pix2pix.\n\n*note: Architecture for the discriminator model is the same with the one that the paper proposed.\n\n<img src=\"data:image\/jpeg;base64,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\">\n\n**Detail for the PatchGAN model**\n\nC64-C128-C256-C512\n\n* Each block contains convolution layer, batch_normalization layer, and leaky relu layer.\n* The output will be 16x16x1 map which each value represent a patch from the image.","dcb2c3fd":"**Defining function to create encoder block**","5755648f":"# Defining Custom Generator Loss Function\n\nIn the paper, the author used custom generator loss function which is combination of binary crossentropy of generated image and array of 1 (all pixel is real) and mean absolute error (l1 loss)\n\nThe formula for the generator loss is:\ngenerator loss = gan_loss + lambda * l1_loss\n\n*note: lambda used for this is 100 same as the paper","8c956ff0":"# Import Libraries\n\nImport some libraries that will be used in this project","5122692e":"**Defining function for loading train image and apply preprocess**\n\nSince the preprocessing for train and test are difference I defined different function","d756dc7f":"**Epochs**\n\nIn this task I trained the model for 30 epochs. In the Pix2Pix paper, for edge to shoes task the author trained the model for 15 epochs only. When i tried 15 epochs, i didn't get good result so i tried to run 30 epochs","7f0fddac":"**Getting the train image path**","0ab3dbab":"Defining the optimizer for the model","0194f74d":"**Train the model**","3ba93c98":"**Resulting U-net model could be seen from the image below**"}}