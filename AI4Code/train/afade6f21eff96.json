{"cell_type":{"ac7a314f":"code","676939fd":"code","5589aedf":"code","9c43cc2a":"code","2f6673a4":"code","5907bcfc":"code","557923c5":"code","aae4355c":"code","38d20946":"code","ed693305":"code","064a406f":"code","3d53b78f":"code","629bbbff":"code","f23f9144":"code","ae757e82":"code","5fa27861":"code","bf634180":"code","b24a6349":"code","22c40626":"code","2759feb1":"code","713c39ff":"code","1123e074":"code","d7b8ff9c":"code","7acd6bae":"code","04ff0820":"code","f7c7feba":"code","4a3a5502":"code","adfcf7a0":"code","a11ba14d":"code","b5a903a2":"code","bc37b5b7":"code","e5aeb698":"code","9fa5301b":"markdown","9f86dd5f":"markdown","05dbd2a0":"markdown","1c1d12cd":"markdown","f362f20f":"markdown","b5cf1637":"markdown","2c368aca":"markdown","7a2bb176":"markdown","9aa3ac4d":"markdown","ba9aaa95":"markdown","cc65b2a7":"markdown","413b4379":"markdown"},"source":{"ac7a314f":"import pandas as pd\nimport sklearn\nimport os\nimport numpy as np\nnp.random.seed(42)\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)","676939fd":"# Loading dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=500, noise=0.30, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","5589aedf":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nlog_clf = LogisticRegression()\nrnd_clf = RandomForestClassifier()\nsvm_clf = SVC()\nvoting_clf = VotingClassifier(\n estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n voting='hard')\nvoting_clf.fit(X_train, y_train)\n","9c43cc2a":"from sklearn.metrics import accuracy_score\n\nfor clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))","2f6673a4":"log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\nrnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\nsvm_clf = SVC(gamma=\"auto\", probability=True, random_state=42) ## Probability = True, SVC will have predict_proba()\n\nvoting_clf = VotingClassifier(\n    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n    voting='soft')\nvoting_clf.fit(X_train, y_train)","5907bcfc":"from sklearn.metrics import accuracy_score\n\nfor clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n##91.2% accuracy!","557923c5":"# Bagging\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nbag_clf = BaggingClassifier(\n    DecisionTreeClassifier(random_state=42), n_estimators=500,\n    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\nbag_clf.fit(X_train, y_train)\ny_pred = bag_clf.predict(X_test)","aae4355c":"# Decision Tree\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_clf.fit(X_train, y_train)\ny_pred_tree = tree_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred_tree))","38d20946":"# Accuracy\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, y_pred) *100)","ed693305":"# Pasting\npast_clf = BaggingClassifier(\n    DecisionTreeClassifier(random_state=42), n_estimators=500,\n    max_samples=100, bootstrap=False, n_jobs=-1, random_state=42) #bootstrap=False\npast_clf.fit(X_train, y_train)\ny_pred = past_clf.predict(X_test)","064a406f":"# Accuracy\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, y_pred) *100)","3d53b78f":"# Plotting\nfrom matplotlib.colors import ListedColormap\n\ndef plot_decision_boundary(clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True):\n    x1s = np.linspace(axes[0], axes[1], 100)\n    x2s = np.linspace(axes[2], axes[3], 100)\n    x1, x2 = np.meshgrid(x1s, x2s)\n    X_new = np.c_[x1.ravel(), x2.ravel()]\n    y_pred = clf.predict(X_new).reshape(x1.shape)\n    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n    if contour:\n        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n    plt.axis(axes)\n    plt.xlabel(r\"$x_1$\", fontsize=18)\n    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)","629bbbff":"# Bagging Plotting\nplt.figure(figsize=(11,4))\nplt.subplot(121)\nplot_decision_boundary(tree_clf, X, y)\nplt.title(\"Decision Tree\", fontsize=14)\nplt.subplot(122)\nplot_decision_boundary(bag_clf, X, y)\nplt.title(\"Decision Trees with Bagging\", fontsize=14)\nplt.show()","f23f9144":"# Pasting Plotting\nplt.figure(figsize=(11,4))\nplt.subplot(121)\nplot_decision_boundary(tree_clf, X, y)\nplt.title(\"Decision Tree\", fontsize=14)\nplt.subplot(122)\nplot_decision_boundary(past_clf, X, y)\nplt.title(\"Decision Trees with Pating\", fontsize=14)\nplt.show()","ae757e82":"from sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\nrnd_clf.fit(X_train, y_train)\n\ny_pred_rf = rnd_clf.predict(X_test)","5fa27861":"np.sum(y_pred == y_pred_rf) \/ len(y_pred) ","bf634180":"# ExtraTreesClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\next_clf = ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\next_clf.fit(X_train, y_train)\n\ny_pred_ext = ext_clf.predict(X_test)","b24a6349":"# One can't say where will RandomForest perform better or ExtraTreesClassifiers\nnp.sum(y_pred == y_pred_ext) \/ len(y_pred) # 97.6 % slightly increase","22c40626":"# On iris data\nfrom sklearn.datasets import load_iris\niris = load_iris()\nrnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\nrnd_clf.fit(iris[\"data\"], iris[\"target\"])\nfor name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n    print(name, score)\n# most important features are the petal length (44%) and width (42%) ","2759feb1":"# On MNIST Data\ntry:\n    from sklearn.datasets import fetch_openml\n    mnist = fetch_openml('mnist_784', version=1)\n    mnist.target = mnist.target.astype(np.int64)\nexcept ImportError:\n    from sklearn.datasets import fetch_mldata\n    mnist = fetch_mldata('MNIST original')","713c39ff":"rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\nrnd_clf.fit(mnist[\"data\"], mnist[\"target\"])","1123e074":"def plot_digit(data):\n    image = data.reshape(28, 28)\n    plt.imshow(image, cmap = mpl.cm.hot,\n               interpolation=\"nearest\")\n    plt.axis(\"off\")","d7b8ff9c":"plot_digit(rnd_clf.feature_importances_)","7acd6bae":"from sklearn.ensemble import AdaBoostClassifier\n\nada_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\nada_clf.fit(X_train, y_train)","04ff0820":"plot_decision_boundary(ada_clf, X, y)","f7c7feba":"np.random.seed(42)\nX = np.random.rand(100, 1) - 0.5\ny = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)","4a3a5502":" from sklearn.tree import DecisionTreeRegressor\n\ntree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\ntree_reg1.fit(X, y)","adfcf7a0":"y2 = y - tree_reg1.predict(X)\ntree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\ntree_reg2.fit(X, y2)","a11ba14d":"y3 = y2 - tree_reg2.predict(X)\ntree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\ntree_reg3.fit(X, y3)","b5a903a2":"X_new = np.array([[0.8]])","bc37b5b7":"y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\ny_pred","e5aeb698":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nX_train, X_val, y_train, y_val = train_test_split(X, y)\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\ngbrt.fit(X_train, y_train)\n\nerrors = [mean_squared_error(y_val, y_pred)\n for y_pred in gbrt.staged_predict(X_val)]\n\nbst_n_estimators = np.argmin(errors)\n\ngbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\ngbrt_best.fit(X_train, y_train)","9fa5301b":"### Bagging and Pasting\n\n#### Bootstrapping\n- Bootstrapping refers to a resample method that consists of repeatedly drawn, with replacement, samples from data to form other smaller datasets, called bootstrapping samples.\n- if we have this set of observation **[2, 4, 32, 8, 16]**\n- n=3: [32, 4, 4], [8, 16, 2], [2, 2, 2]\u2026\n- n=4: [2, 32, 4, 16], [2, 4, 2, 8], [8, 32, 4, 2]\u2026\n\n#### Bagging & Pasting\n- Use the same training algorithm for every predictor, but to train them on different random subsets of the training set. When sampling is performed **with replacement**.\n- We first bootstrap our data and for each bootstrap sample we train one model.\n- When it is **without replacement** its pasting.\n\n#### Bagging vs pasting\n-  Bagging ends up with a **slightly higher bias** than pasting\n-  Predictors end up being less correlated so the **ensemble\u2019s variance** is reduced.\n-  Bagging often results in better models.","9f86dd5f":"#### Early Stooping\n- A simpler way to train GBRT ensembles is to use Scikit-Learn\u2019s **GradientBoostingRegressor** class.\n- To implement early stopping **staged_predict()** method: it returns an iterator over the predictions made by the ensemble at each stage of training.","05dbd2a0":"#### Hard voting","1c1d12cd":"**[Link to the video](https:\/\/www.youtube.com\/watch?v=gPciUPwWJQQ&t=552s)**\n\n### XGBoost Classifier\n- Calculate the probability and find the residual with the output feature.\n- Construct Decision Tree with root.\n- Calculate the similarity weight for all the leaf node.\n- Calculate Gain to decide which would be the better split.\n- This will only be the step for one decision tree.\n- Again find the probability using the sigmoid function and find the residual.","f362f20f":"#### AdaBoost\n- A new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted.\n- New predictors focus more and more on the hard case.\n- After each prediction the weights are updated and then trained on new predictor.\n- Scikit-Learn actually uses a multiclass version of AdaBoost called SAMME.\n- When there is only two class then its SAMME or SAMME.R (having predict_proba()).","b5cf1637":"### Boosting\n- Boosting (originally called hypothesis boosting) refers to any Ensemble method that can combine several weak learners into a strong learner.\n- Boosting methods is to train predictors sequentially, each trying to correct its predecessor.","2c368aca":"### Ensemble Learning\n- A group of predictors is called an ensemble; thus, this technique is called Ensemble Learning.\n\n#### Voting Classifiers\n- Aggregate the predictions of each classifier and predict the class that gets the most votes.\n- This majority-vote classifier is called a hard voting classifier.\n- Even if each classifier is a weak learner the ensemble can still be a strong learner.","7a2bb176":"### Random Forest\n- Instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features.\n-  This results in a greater tree diversity, which (once again) trades a higher bias for a lower variance.\n- \nThe following code trains a\nRandom Forest classifier with 500 trees (each limited to maximum 16 nodes), using\nall available CPU cores","9aa3ac4d":"#### Feature Importance\n- Measures a feature\u2019s importance by looking at how much the tree nodes that use that feature reduce impurity on average.","ba9aaa95":"#### Soft voting","cc65b2a7":"#### Gradient Boosting\n- Sequentially adding predictors to an ensemble, each one correcting its predecessor.\n- This method tries to fit the new predictor to the **residual errors** made by the previous predictor.\n- We first train one decision tree as a base predictor.\n- Then the **residual error** is calculated and again that is fed to the decision tree.\n- This continues till we reach the decision tree limit or any other limit.\n- Now we have an ensemble containing three trees.","413b4379":"### Random Patches and Random Subspaces"}}