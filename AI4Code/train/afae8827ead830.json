{"cell_type":{"f2b2a20b":"code","8a2f7057":"code","ad70e6a2":"code","4b360e9f":"code","22665307":"code","b997b5f8":"code","b489dfd6":"code","c3b9880d":"code","2560fb23":"code","3aed6dce":"code","67d34c14":"code","b7311f46":"code","64cb8bee":"code","b45b2dc6":"code","ebd1557c":"code","6e00c28c":"code","0f321fa4":"code","b2525abc":"code","5c6678ee":"code","ba7010e5":"code","c81ab939":"code","eb9cd64a":"code","edeb6c21":"code","751ad92a":"code","ffda59a0":"code","5c3ed327":"code","7e3707d2":"code","2096f905":"code","f1c51ef9":"code","8dada32e":"code","bb580e37":"code","d80b64a0":"code","6df8deff":"code","911cfeb7":"code","ed78ba26":"code","36ad5f45":"code","0abf3aa6":"code","08c0c986":"code","2e1a51c1":"code","13e2375f":"code","65d5404d":"code","85f16f08":"code","b3526e60":"code","21d42a74":"code","690a9730":"code","6cce2c03":"code","b2efd8c2":"code","e764faaf":"code","ef6d116a":"code","19db17a8":"code","313b4981":"code","f61894b3":"code","7811cb0d":"code","f1bb2532":"code","d83f6f9b":"code","3b0b0f90":"code","04aace07":"code","8ebcf2c3":"code","72dfb6db":"code","ee2cbc37":"code","26ca2fbb":"code","87151e5c":"code","b7d329a8":"code","20b5e4a6":"code","d7620788":"code","1f6d46d9":"code","12f12430":"code","79a053d9":"code","3dbc822b":"code","a6bda3f3":"code","66b2caae":"code","581afe5c":"code","fd49940d":"code","dc6b03e0":"code","48b9dd99":"code","a9b5459e":"code","a2433aee":"code","6ad664d8":"code","59f211b2":"code","8c7e3087":"code","15d1e3a3":"code","6a3ace15":"code","10772daa":"code","637f3ad3":"code","854f72df":"code","da1f7b7a":"code","948c7caf":"code","6c1b0bd3":"code","b69635cb":"code","12f13219":"code","7f9eda5f":"code","ce598603":"code","582bd925":"code","6cdd283e":"code","9c5b9409":"code","f4f04b57":"code","b97b01f8":"code","64be7768":"code","9747be24":"code","88d4f4e6":"code","422c6e8e":"code","4bd8ed83":"code","7e46e898":"code","f7c90901":"code","d8cf1c39":"code","27c6078a":"code","54817c51":"code","a386bd34":"code","59151dbd":"code","8dc2f17b":"code","c20a9714":"code","2a023a49":"code","b230c325":"code","f56b9c16":"code","2f90a3ec":"code","ae07c2af":"code","04ddac10":"code","ab1d136f":"code","61bce215":"code","2a2fe5e9":"code","9a2ec419":"code","94c63dda":"code","b8b69efa":"code","671df684":"code","c29a34e5":"code","1426b7fb":"code","2a080501":"code","c6ae7e96":"code","007908e5":"code","0e34b934":"code","506166e5":"code","57e98b54":"code","958ef8fe":"code","b83c05a2":"markdown","fa2d0636":"markdown","b8e98da6":"markdown","c5c1a000":"markdown","5e3e0f2a":"markdown","8fde0340":"markdown","ac3ab048":"markdown","f696615f":"markdown","176bd882":"markdown","29d1b7c4":"markdown","0ec1b946":"markdown","3105f01a":"markdown","b9076c53":"markdown","3eab6f02":"markdown","7c76e4ca":"markdown","f9b603ab":"markdown","f1f03163":"markdown","7df3deb6":"markdown","2cb2ff05":"markdown","6495ba0c":"markdown","dc000366":"markdown","3c47abbc":"markdown","7a78ee5c":"markdown","01dd10b7":"markdown","0a7710ea":"markdown","c0fa194f":"markdown","5c32ebc9":"markdown","72308775":"markdown","26668e5d":"markdown","29cadece":"markdown","69b203f1":"markdown","613a9dd5":"markdown","05709f98":"markdown","5b339e96":"markdown","1d2be64a":"markdown","4718a61f":"markdown","2dfb567d":"markdown","a5adb4db":"markdown","baa0cd38":"markdown","b1155fa1":"markdown","fdaf2ad9":"markdown","5aed52ba":"markdown","98302a49":"markdown","0cfd62f9":"markdown","78941c9a":"markdown","b7359aaf":"markdown","54fbfa15":"markdown","7bbc0d4c":"markdown","b2a6d292":"markdown","ad7930e9":"markdown","1cd67b23":"markdown","257ebb26":"markdown","45eecab1":"markdown","4ce9369b":"markdown","da4b0f21":"markdown"},"source":{"f2b2a20b":"# importing Important libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings\nwarnings.filterwarnings(\"ignore\")","8a2f7057":"# Read Dataframe\n\ndf = pd.read_csv('..\/input\/loan-data\/train.csv')","ad70e6a2":"df.head(10)","4b360e9f":"# Dataframe Shape\n\nprint('There are {} rows and {} columns in Dataframe'.format(df.shape[0],df.shape[1]))","22665307":"# Dataframe Shape\n\ndf.shape","b997b5f8":"# Columns in dataframe\n\ndf.columns","b489dfd6":"# Target column to see numbers in each class\n\ndf['Loan_Status'].value_counts()","c3b9880d":"#change Name for some columns\n\ndf.rename(columns={'ApplicantIncome': 'Applicant_Income'} , inplace=True)","2560fb23":"df.columns","3aed6dce":"#to replace each ' ' to '_' in columns only \n\ndf.columns = df.columns.str.replace(' ', '_')","67d34c14":"# Missing Data\n\ndf.isnull().sum()","b7311f46":"# dataframe describe\n\ndf.describe()","64cb8bee":"# Dataframe correlation\n\ndf.corr()","b45b2dc6":"df.columns","ebd1557c":"# drop some columns \n\ndf.drop(['Loan_ID'] ,axis= 1 , inplace = True)","6e00c28c":"df.columns","0f321fa4":"# Missing Data\n\ndf.isnull().sum()","b2525abc":"#df['Credit_History'] = df['Credit_History'].fillna(df['Credit_History'].mode()[0])","5c6678ee":"df['Gender'].isnull().sum()","ba7010e5":"df['Gender'].value_counts()","c81ab939":"df['Gender'].mode()","eb9cd64a":"df['Gender'].head()","edeb6c21":"df['Gender'] = df['Gender'].fillna('Male')\n#df['Gender'] = df['Gender'].fillna(df['Gender'].mode()[0])","751ad92a":"df['Gender'].isnull().sum()","ffda59a0":"df['Married'].isnull().sum()","5c3ed327":"df['Married'].value_counts()","7e3707d2":"df['Married'].mode()","2096f905":"df['Married'] = df['Married'].fillna(df['Married'].mode()[0])","f1c51ef9":"df['Married'].isnull().sum()","8dada32e":"df['Dependents'].isnull().sum()","bb580e37":"df['Dependents'].value_counts()","d80b64a0":"df['Dependents'] = df['Dependents'].fillna(df['Dependents'].mode()[0])","6df8deff":"df['Self_Employed'].isnull().sum()","911cfeb7":"df['Self_Employed'].value_counts()","ed78ba26":"df['Self_Employed'] = df['Self_Employed'].fillna(df['Self_Employed'].mode()[0])","36ad5f45":"df['LoanAmount'].isnull().sum()","0abf3aa6":"df['LoanAmount'].value_counts()","08c0c986":"df['LoanAmount'].head()","2e1a51c1":"df['LoanAmount'] = df['LoanAmount'].fillna(df['LoanAmount'].mean())","13e2375f":"#df = df.loc[df['LoanAmount'] <= 500]","65d5404d":"sns.boxplot(df['LoanAmount'])","85f16f08":"df['Loan_Amount_Term'].isnull().sum()","b3526e60":"df['Loan_Amount_Term'].value_counts()","21d42a74":"df['Loan_Amount_Term'] = df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0])","690a9730":"df['Credit_History'].isnull().sum()","6cce2c03":"df['Credit_History'].value_counts()","b2efd8c2":"df['Credit_History'] = df['Credit_History'].fillna(df['Credit_History'].mode()[0])","e764faaf":"# check fill missing Data\n\ndf.isnull().sum()","ef6d116a":"df.head()","19db17a8":"df_oneHotEncoder = df.copy()","313b4981":"df_oneHotEncoder = pd.get_dummies(df)\ndf_oneHotEncoder.head()","f61894b3":"df.info()","7811cb0d":"df[['Gender' , 'Married' , 'Education' , 'Self_Employed' , 'Property_Area' , 'Loan_Status' ]].head()","f1bb2532":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","d83f6f9b":"#df['Gender'] = le.fit_transform(df['Gender'])\n#df['Married'] = le.fit_transform(df['Married'])\n#df['Education'] = le.fit_transform(df['Education'])\n#df['Self_Employed'] = le.fit_transform(df['Self_Employed'])\n#df['Property_Area'] = le.fit_transform(df['Property_Area'])\n#df['Loan_Status'] = le.fit_transform(df['Loan_Status'])","3b0b0f90":"cols = ['Gender' , 'Married' , 'Education' , 'Self_Employed' , 'Property_Area' , 'Loan_Status' , 'Dependents']","04aace07":"df[cols] = df[cols].apply(LabelEncoder().fit_transform)","8ebcf2c3":"df.head()","72dfb6db":"df[['Gender' , 'Married' , 'Education' , 'Self_Employed' , 'Property_Area' , 'Loan_Status' ]].head()","ee2cbc37":"df['Loan_Status'].value_counts()","26ca2fbb":"graph = ['Applicant_Income', 'CoapplicantIncome', 'LoanAmount',\n       'Loan_Amount_Term', 'Credit_History', 'Loan_Status']","87151e5c":"sns.pairplot(df[graph], hue=\"Loan_Status\", markers=[\"o\", \"s\"])","b7d329a8":"df.columns","20b5e4a6":"sns.countplot(x='Gender',data=df ,hue= 'Loan_Status')\nplt.show()","d7620788":"sns.countplot(x='Married',data=df ,hue= 'Loan_Status')\nplt.show()","1f6d46d9":"sns.countplot(x='Dependents',data=df ,hue= 'Loan_Status')\nplt.show()","12f12430":"sns.countplot(x='Education',data=df ,hue= 'Loan_Status')\nplt.show()","79a053d9":"sns.countplot(x='Self_Employed',data=df ,hue= 'Loan_Status')\nplt.show()","3dbc822b":"sns.countplot(x='Property_Area',data=df ,hue= 'Loan_Status')\nplt.show()","a6bda3f3":"sns.countplot(x='Loan_Amount_Term',data=df ,hue= 'Loan_Status')\nplt.show()","66b2caae":"sns.countplot(x='Credit_History',data=df ,hue= 'Loan_Status')\nplt.show()","581afe5c":"sns.countplot(x='Loan_Status',data=df)\nplt.show()","fd49940d":"sns.distplot(df['Applicant_Income'], color = 'red')\nplt.title('Applicant Income')","dc6b03e0":"df['Applicant_Income'].head()","48b9dd99":"df.loc[df['Applicant_Income'] >= 30000]","a9b5459e":"df = df.loc[df['Applicant_Income'] <= 30000]","a2433aee":"sns.distplot(df['Applicant_Income'], color = 'red')\nplt.title('Applicant Income')","6ad664d8":"sns.distplot(df['CoapplicantIncome'], color = 'orange')\nplt.title('Coapplicant Income')","59f211b2":"df.loc[df['CoapplicantIncome'] >= 10000]","8c7e3087":"df = df.loc[df['CoapplicantIncome'] <= 10000]","15d1e3a3":"sns.distplot(df['CoapplicantIncome'], color = 'orange')\nplt.title('Coapplicant Income')","6a3ace15":"sns.distplot(df['LoanAmount'], color = 'blue')\nplt.title('Loan Amount')","10772daa":"df.loc[df['LoanAmount'] >= 500]","637f3ad3":"df = df.loc[df['LoanAmount'] <= 500]","854f72df":"sns.distplot(df['LoanAmount'], color = 'blue')\nplt.title('Loan Amount')","da1f7b7a":"ds = df.copy()","948c7caf":"#from sklearn.preprocessing import MinMaxScaler\n\n#scaling = MinMaxScaler()\n#scaling.fit_transform(df[[ '' , '' ]])","6c1b0bd3":"df.head()","b69635cb":"cols_to_norm = ['Applicant_Income' , 'CoapplicantIncome' , 'LoanAmount' , 'Loan_Amount_Term' ]","12f13219":"df[['CoapplicantIncome' , 'CoapplicantIncome' , 'LoanAmount']].head()","7f9eda5f":"df[cols_to_norm] = df[cols_to_norm].apply(lambda x: (x - x.min()) \/ (x.max() - x.min()))","ce598603":"df[['Applicant_Income' , 'CoapplicantIncome' , 'LoanAmount' , 'Loan_Amount_Term']].head()","582bd925":"ds[['Applicant_Income' , 'CoapplicantIncome' , 'LoanAmount' , 'Loan_Amount_Term']].head()","6cdd283e":"from sklearn.preprocessing import StandardScaler","9c5b9409":"ds[['Applicant_Income' , 'CoapplicantIncome' , 'LoanAmount' , 'Loan_Amount_Term']] = StandardScaler().fit_transform(ds[['Applicant_Income' , 'CoapplicantIncome' , 'LoanAmount' , 'Loan_Amount_Term']])","f4f04b57":"ds[['Applicant_Income' , 'CoapplicantIncome' , 'LoanAmount' , 'Loan_Amount_Term']].head()","b97b01f8":"df.columns","64be7768":"df.columns","9747be24":"features = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed',\n       'Applicant_Income', 'CoapplicantIncome', 'LoanAmount',\n       'Loan_Amount_Term', 'Credit_History', 'Property_Area', 'Loan_Status']","88d4f4e6":"# Separating out the features\nx = df.loc[:, features].values\n# Separating out the target\ny = df.loc[:,['Loan_Status']].values","422c6e8e":"from sklearn.decomposition import PCA\npca = PCA(n_components=4)\nprincipalComponents = pca.fit_transform(x)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2' , 'principal component 3' , 'principal component 4'])","4bd8ed83":"principalDf.head()","7e46e898":"finalDf = pd.concat([principalDf, df[['Loan_Status']]], axis = 1)","f7c90901":"finalDf.head()","d8cf1c39":"X_uni = df.drop(columns=['Loan_Status'] , axis=1 )\ny_uni = df['Loan_Status']","27c6078a":"df.shape","54817c51":"## Apply SelectBest Algorithm\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\norder_applied_feature = SelectKBest(score_func=chi2 , k=11)\norder_features = order_applied_feature.fit(X_uni,y_uni)","a386bd34":"order_features","59151dbd":"order_features.scores_","8dc2f17b":"df_scores = pd.DataFrame(order_features.scores_,columns=['Score'])\ndf_columns = pd.DataFrame(X_uni.columns)","c20a9714":"feature_Rank = pd.concat([df_columns , df_scores] , axis=1)","2a023a49":"feature_Rank.columns=['Features' , 'Scores']","b230c325":"feature_Rank","f56b9c16":"## to give me the top 10\nfeature_Rank.nlargest(10 , 'Scores')","2f90a3ec":"df.corr()","ae07c2af":"corr = df.corr()\ntop_features = corr.index\nplt.figure(figsize=(40,40))\nsns.heatmap(df[top_features].corr(),annot=True)","04ddac10":"### threshold = 0.2\n\nthreshold = 0.0","ab1d136f":"#to remove if the correlation greater than 0.8\ndef correlation (dataset ,  threshold):\n    col_corr         = set()\n    corr_matrix      = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i , j]) > threshold:\n                colname  = corr_matrix.columns[i]\n                col_corr.add(colname)\n            \n    return col_corr","61bce215":"correlation(df.iloc[:,:-1],threshold)","2a2fe5e9":"from sklearn.feature_selection import mutual_info_classif","9a2ec419":"X = df.drop(columns=['Loan_Status'] , axis=1 )\ny = df['Loan_Status']","94c63dda":"mutual_info = mutual_info_classif(X,y)","b8b69efa":"mutual_data = pd.Series(mutual_info , index = X.columns)\nmutual_data.sort_values(ascending = False)","671df684":"df['Loan_Status'].value_counts()","c29a34e5":"df.columns","1426b7fb":"from collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE\nfrom matplotlib import pyplot\nfrom numpy import where\n\n# use our preprocesses dataset\nX = df.drop('Loan_Status', axis=1)\ny = df['Loan_Status']\n\n\n# summarize class distribution\ncounter = Counter(y)\nprint(counter)\n\n\n# transform the dataset\noversample = SMOTE()\nX, y = oversample.fit_resample(X, y)\n\n\n# summarize the new class distribution\ncounter = Counter(y)\nprint(counter)","2a080501":"X.shape","c6ae7e96":"y.shape","007908e5":"df_New = pd.concat([X, y], axis=1)","0e34b934":"df_New.shape","506166e5":"df_New.head()","57e98b54":"sns.countplot(x='Loan_Status',data=df)\nplt.show()","958ef8fe":"sns.countplot(x='Loan_Status',data=df_New)\nplt.show()","b83c05a2":"##### Visualization for Categorical Columns","fa2d0636":"---","b8e98da6":"###### Applicant_Income","c5c1a000":"#### Univariant Selection","5e3e0f2a":"SMOTE : Synthetic Minority Oversampling Technique","8fde0340":"##### Loan_Amount_Term","ac3ab048":"![Screen%20Shot%202021-07-10%20at%202.01.33%20AM.png](attachment:Screen%20Shot%202021-07-10%20at%202.01.33%20AM.png)","f696615f":"![Screen%20Shot%202021-07-10%20at%202.00.17%20AM.png](attachment:Screen%20Shot%202021-07-10%20at%202.00.17%20AM.png)","176bd882":"What is Exploratory Data Analysis?\n\n- Exploratory Data Analysis (EDA) is the first step in your data analysis process developed by \u201cJohn Tukey\u201d in the 1970s. In statistics, exploratory data analysis is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. By the name itself, we can get to know that it is a step in which we need to explore the data set.\n","29d1b7c4":"#### Correlation","0ec1b946":"https:\/\/www.youtube.com\/watch?v=mnKm3YP56PY&t=331s","3105f01a":"#### PCA","b9076c53":"![Screen%20Shot%202021-07-10%20at%202.18.21%20AM.png](attachment:Screen%20Shot%202021-07-10%20at%202.18.21%20AM.png)","3eab6f02":"### Feature Scaling for Machine Learning in feature Engineering","7c76e4ca":"### Handling imbalanced dataset","f9b603ab":"##### we don't meed to add another category for Dependants so we will use mode also to fill missing","f1f03163":"### Handling missing data","7df3deb6":"### Encoder","2cb2ff05":"##### Label Encoder","6495ba0c":"###### CoapplicantIncome","dc000366":"imbalanced data sets are a special case for classification problem where the class distribution is not uniform among the classes. Typically, they are composed by two classes: The majority (negative) class and the minority (positive) class","3c47abbc":"https:\/\/www.youtube.com\/watch?v=dP170R-wO-8","7a78ee5c":"###### Normalization & Standarization","01dd10b7":"##### Dependents","0a7710ea":"##### Normalization:\n\nhelps you to scal down your feature's value between 0 and 1","c0fa194f":"##### Married","5c32ebc9":"What are Exploratory Data Analysis Steps ?\n\n- Description of data\n- Handling missing data\n- Label Encoder for Categorical Data\n- Understanding relationships and new insights through plots","72308775":"#### information gain algorithm","26668e5d":"##### Class weights in the models","29cadece":"---\n#### EDA Steps !","69b203f1":"##### Visualization for Numerical Columns","613a9dd5":"https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/feature-scaling-machine-learning-normalization-standardization\/","05709f98":"##### Credit_History","5b339e96":"##### Random Undersampling and Oversampling","1d2be64a":"### Description of data","4718a61f":"##### Gender","2dfb567d":"#### Accuracy Fails for Imbalanced Classification\n\n\nWhen the skew in the class distributions are severe, accuracy can become an unreliable measure of model performance.\nbecuase it can easy biased to one of the two classes and predict with high accuracy result but without feel that dataset containg another class ! called minority class and it contain small number","a5adb4db":"##### Standarization: \n\nhelps you to scal down your feature based on standard Normal Distribution (the mean is usually 0 and standard deviation is usually 1)","baa0cd38":"##### distribution between Numerical columns and Target column ( pairplot )","b1155fa1":"##### Link for     ( Learning from Imbalanced Data Sets! )     Book\n\nhttps:\/\/www.linkedin.com\/feed\/update\/urn:li:activity:6739451149843222528\/","fdaf2ad9":"### Visualization","5aed52ba":"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/missing_data.html","98302a49":"![1*cd6AorHoJYMFyj7IZd2nOg.png](attachment:1*cd6AorHoJYMFyj7IZd2nOg.png)","0cfd62f9":"#### What is Exploratory Data Analysis?","78941c9a":"# Exploratory Data Analysis","b7359aaf":"###### LoanAmount","54fbfa15":"https:\/\/www.youtube.com\/watch?v=dkXB8HH_4-k&t=499s","7bbc0d4c":"### Feature Selection & Elimination","b2a6d292":"https:\/\/towardsdatascience.com\/exploratory-data-analysis-topic-that-is-neglected-in-data-science-projects-9962ae078a56","ad7930e9":"##### Self_Employed","1cd67b23":"This is a significant obstacle as a few machine learning algorithms are highly sensitive to these features.\nFor example, one feature is entirely in kilograms while the other is in grams, another one is liters, and so on. How can we use these features when they vary so vastly in terms of what they\u2019re presenting?\n","257ebb26":"##### LoanAmount","45eecab1":"https:\/\/www.kdnuggets.com\/2020\/01\/5-most-useful-techniques-handle-imbalanced-datasets.html","4ce9369b":"##### One Hot Encoder","da4b0f21":"#####  Oversampling using SMOTE:"}}