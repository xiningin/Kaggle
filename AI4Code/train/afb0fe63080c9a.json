{"cell_type":{"2195d36d":"code","91e61741":"code","525e3e96":"code","e673f334":"code","b7807d49":"code","81f5c090":"code","68ca3af4":"code","fe7d4fa6":"code","1ec28871":"code","97714925":"code","c1a51737":"code","57e1df20":"code","c95ecebe":"code","e4d3d15b":"code","1625d071":"code","02605717":"code","34a34ee7":"code","8eb3dd39":"code","c73dd3e6":"code","6ef0cd82":"code","aae2c8e5":"code","3f8ef42f":"code","efee0869":"code","ee347eb7":"code","5a89c1f5":"code","fa987ce4":"code","3c7045ad":"code","5f0f7876":"code","0f3f3acc":"code","f52f135a":"code","3490d8ae":"code","f766f960":"code","eb5c4e51":"code","5d91e3bd":"markdown","f7829cfa":"markdown","85b09b4d":"markdown","149769ae":"markdown","c40db425":"markdown","b3f4d508":"markdown","b4b06f98":"markdown","4d51548d":"markdown","6c4ede9b":"markdown"},"source":{"2195d36d":"import numpy as np\nimport pandas as pd\nimport plotly.express as px\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf","91e61741":"data = pd.read_csv('..\/input\/nys-environmental-remediation-sites\/environmental-remediation-sites.csv')","525e3e96":"data","e673f334":"data.info()","b7807d49":"data.isna().sum()","81f5c090":"null_columns = data.loc[:, data.isna().sum() > 0.25 * data.shape[0]]\n\ndata = data.drop(null_columns, axis=1)","68ca3af4":"data","fe7d4fa6":"data.isna().sum()","1ec28871":"unneeded_columns = ['Program Number', 'Project Name', 'Program Facility Name', 'Address1',\n                    'Locality', 'ZIPCode', 'SWIS Code', 'Owner Name', 'Owner Address1',\n                    'Owner City', 'Owner State', 'Owner ZIP', 'Location 1']\n\ndata = data.drop(unneeded_columns, axis=1)","97714925":"data","c1a51737":"def get_uniques(df, columns):\n    return {column: list(df[column].unique()) for column in columns}\n\ndef get_categorical_columns(df):\n    return [column for column in df.columns if df.dtypes[column] == 'object']","57e1df20":"get_uniques(data, get_categorical_columns(data))","c95ecebe":"data['Project Completion Date']","e4d3d15b":"data['Project Completion Date'] = data['Project Completion Date'].apply(lambda x: x[0:7] if str(x) != 'nan' else x)\n\ndata['Year'] = data['Project Completion Date'].apply(lambda x: np.float(x[0:4]) if str(x) != 'nan' else x)\ndata['Month'] = data['Project Completion Date'].apply(lambda x: np.float(x[5:7]) if str(x) != 'nan' else x)\n\ndata = data.drop('Project Completion Date', axis=1)","1625d071":"data","02605717":"data.isna().sum()","34a34ee7":"for column in ['New York Zip Codes', 'Counties', 'Year', 'Month']:\n    data[column] = data[column].fillna(data[column].mean())","8eb3dd39":"data.isna().sum()","c73dd3e6":"data","6ef0cd82":"def onehot_encode(df, column):\n    df = df.copy()\n    dummies = pd.get_dummies(df[column])\n    df = pd.concat([df, dummies], axis=1)\n    df = df.drop(column, axis=1)\n    return df","aae2c8e5":"nominal_features = get_categorical_columns(data)\nnominal_features.remove('Program Type')\n\nfor feature in nominal_features:\n    data = onehot_encode(data, feature)","3f8ef42f":"data","efee0869":"(data.dtypes == 'object').sum()","ee347eb7":"label_encoder = LabelEncoder()\n\ndata['Program Type'] = label_encoder.fit_transform(data['Program Type'])\n\ntarget_mappings = {index: column for index, column in enumerate(label_encoder.classes_)}\ntarget_mappings","5a89c1f5":"y = data['Program Type']\nX = data.drop('Program Type', axis=1)","fa987ce4":"scaler = StandardScaler()\n\nX = pd.DataFrame(scaler.fit_transform(X), index=X.index, columns=X.columns)","3c7045ad":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)","5f0f7876":"X.shape","0f3f3acc":"y.value_counts()","f52f135a":"inputs = tf.keras.Input(shape=(117,))\nx = tf.keras.layers.Dense(64, activation='relu')(inputs)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\noutputs = tf.keras.layers.Dense(5, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n\nbatch_size = 64\nepochs = 60\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=[tf.keras.callbacks.ReduceLROnPlateau()],\n    verbose=0\n)","3490d8ae":"fig = px.line(\n    history.history, y=['loss', 'val_loss'],\n    labels={'index': \"Epoch\", 'value': \"Loss\"},\n    title=\"Training and Validation Loss\"\n)\n\nfig.show()","f766f960":"model.evaluate(X_test, y_test)","eb5c4e51":"for label in range(5):\n    label_indices = y_test[y_test == label].index\n    label_acc = model.evaluate(X_test.loc[label_indices, :], y_test.loc[label_indices], verbose=0)\n    print(f\"Class {label} Accuracy: {label_acc[1]}\")","5d91e3bd":"# Training","f7829cfa":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/35a7YOV_HTU","85b09b4d":"# Getting Started","149769ae":"## Splitting and Scaling","c40db425":"# Results","b3f4d508":"# Preprocessing","b4b06f98":"## Extracting Month and Year Features from Project Completion Date Column","4d51548d":"# Task for Today  \n***\n## Predicting Program Type\n\nGiven the data, let's see if we can correctly classify the **program type** of a environmental remediation project.  \n  \nWe will use a TensorFlow neural network to make our predictions.","6c4ede9b":"## Encode"}}