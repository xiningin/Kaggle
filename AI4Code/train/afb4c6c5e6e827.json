{"cell_type":{"623d7827":"code","c032352d":"code","de0a6b4f":"code","f589af0b":"code","ea71e218":"code","8c62885e":"code","e09b41b0":"code","b5c042f7":"code","c0aca804":"code","0bf9a148":"code","994afa36":"code","b6c2ed3c":"code","fca685db":"code","5b6b87cb":"code","780fe375":"code","0ca78043":"code","523c21dd":"code","6d8cd9dc":"code","ad1eb291":"code","d24cbd97":"code","529e49cf":"code","8669ac5d":"code","d3148db1":"code","b42d34aa":"code","c1ca3bba":"code","be906bb0":"code","683d2477":"code","f660b554":"code","6e09368b":"code","95430895":"code","126a2852":"code","1dfa808d":"code","6644cd17":"code","471cbeff":"code","a0ccc3f5":"code","57497a2d":"code","30e48eed":"code","c1745adb":"code","6743d0a6":"code","e35ec2b3":"code","1c2633b2":"code","c77ad5b6":"code","7980cfda":"code","6ad94c9b":"code","0963b6a2":"code","151564a8":"code","96ea8209":"code","9c93783c":"code","2729e190":"code","af44e163":"code","8110c7a4":"code","fbc829e1":"code","d3fc85c2":"markdown","e5e15736":"markdown","11f333e2":"markdown","63b0b5ab":"markdown","5eec06bb":"markdown","5a9c0e1e":"markdown","73f974f5":"markdown","e206f62e":"markdown","99f7be5b":"markdown","dece62dd":"markdown","0e3637a6":"markdown","9759dd68":"markdown","65679869":"markdown","50322f1c":"markdown","e003b6ae":"markdown","c7daff5d":"markdown","3660d375":"markdown","7ce4fd8b":"markdown","2c88adbf":"markdown","eb637d47":"markdown","28cac202":"markdown","8d5b3e78":"markdown","01077c22":"markdown","993b08ba":"markdown","ae9d96ec":"markdown","22cd9afc":"markdown","5f03d343":"markdown","b7c5f23d":"markdown","6f1b5442":"markdown","658b69b7":"markdown","f96d4d03":"markdown"},"source":{"623d7827":"# Importando as bibliotecas utilizadas\n\nfrom scipy.stats import expon, gamma, randint, uniform\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, plot_roc_curve\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold, train_test_split, cross_val_score, cross_validate\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, RobustScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.utils.fixes import loguniform\n\nfrom skopt import BayesSearchCV\nfrom skopt.space import Integer, Real\n\nfrom xgboost import XGBClassifier\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nfrom pprint import pprint\n%matplotlib inline","c032352d":"# Importando os dados\ndf_training = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/train_data.csv\", sep=r'\\s*,\\s*', engine='python', index_col=['Id'], na_values=\"?\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/test_data.csv\", sep=r'\\s*,\\s*', engine='python', index_col=['Id'], na_values=\"?\")\n\n# Renomeando as colunas: removendo espa\u00e7os em branco, substituindo pontos por '_' e passando para caixa baixa\ndf_training.columns= df_training.columns.str.strip().str.lower().str.replace('.','_')\ndf_test.columns= df_test.columns.str.strip().str.lower().str.replace('.','_')\n\n# Analisando as 5 primeiras linhas do DataFrame\ndf_training.head()","de0a6b4f":"# Extraindo as informa\u00e7\u00f5es do dataframe\ndf_training.info()","f589af0b":"df_training.describe()","ea71e218":"# Criando uma c\u00f3pia do DataFrame para manipula\u00e7\u00e3o e visualiza\u00e7\u00e3o\ndf_viz = df_training.copy()","8c62885e":"sns.countplot(data=df_viz, y='income')","e09b41b0":"# Transformando a vari\u00e1vel 'income' em num\u00e9rica\n# Instanciando o LabelEncoder\nle = LabelEncoder()\n\n# Modificando o nosso dataframe, transformando a vari\u00e1vel de classe em 0s e 1s\ndf_viz['income_num'] = le.fit_transform(df_viz['income'])","b5c042f7":"sns.histplot(data=df_viz, x='age', hue='income', bins=20, kde=True)","c0aca804":"sns.histplot(data=df_viz, x='education_num', hue='income', bins=8, kde=True)","0bf9a148":"sns.violinplot(x=\"income\", y=\"hours_per_week\", data=df_viz)","994afa36":"sns.violinplot(data=df_viz, x='capital_gain', y='income')","b6c2ed3c":"sns.violinplot(data=df_viz, x='capital_loss', y='income')","fca685db":"df_viz['capital'] = df_viz['capital_gain'] - df_viz['capital_loss']\nsns.violinplot(data=df_viz, x='capital', y='income')","5b6b87cb":"grouped_sex = df_viz.groupby(['sex','income']).size().reset_index().pivot(columns='income', index='sex', values=0)\ngrouped_sex","780fe375":"grouped_sex.plot(kind='bar', stacked=True)","0ca78043":"sns.boxplot(data=df_viz, x='sex',y='education_num')","523c21dd":"# Seguindo o mesmo agrupamento anterior\ndf_viz.groupby(['race','income']).size().reset_index().pivot(columns='income', index='race', values=0).plot(kind='bar', stacked=True)","6d8cd9dc":"# Substituindo os valores nulos da vari\u00e1vel 'race' pela moda da vari\u00e1vel ('White')\ndf_viz['race_treated'] = df_viz['race'].fillna(df_viz['race'].value_counts().index[0])\ndf_viz.loc[df_viz['race_treated'] != 'White','race_treated'] = 'Others'\ndf_viz.groupby(['race_treated','income']).size().reset_index().pivot(columns='income', index='race_treated', values=0).plot(kind='bar', stacked=True)","ad1eb291":"# Comparando as colunas 'education' com 'education_num'\ndf_viz.groupby(['education','education_num']).size().reset_index().sort_values('education_num').rename(columns={0:'Qtd.'})","d24cbd97":"df_viz.workclass.value_counts().plot(kind=\"bar\")","529e49cf":"df_viz.occupation.value_counts().plot(kind=\"bar\")","8669ac5d":"df_viz.marital_status.value_counts().plot(kind=\"bar\")","d3148db1":"df_viz.native_country.value_counts().plot(kind=\"bar\")","b42d34aa":"# Plotando a matriz de correla\u00e7\u00e3o\nplt.figure(figsize=(16, 6))\n\nheatmap = sns.heatmap(df_viz.corr(), vmin=-1, vmax=1, annot=True)\nheatmap.set_title('Mapa de correla\u00e7\u00e3o', fontdict={'fontsize':12}, pad=12)","c1ca3bba":"_ = sns.pairplot(data=df_viz, hue=\"income\", diag_kind=\"kde\")\nplt.show()","be906bb0":"df_training.isnull().sum()","683d2477":"# Percebe-se que sempre que workclass est\u00e1 vazia, 'occupation' tamb\u00e9m est\u00e1, indicando uma correla\u00e7\u00e3o nos dados faltantes\n(pd.isna(df_training.workclass)&pd.isna(df_training.occupation)).sum()","f660b554":"def data_treatment(df):\n    # Removendo colunas que julga-se n\u00e3o serem importantes\n    df = df.drop(columns=['education','fnlwgt'])\n    \n    # Substituindo para a classe \"Others\" os valores menos frequentes\n    df.loc[df['native_country']=='United-States','native_country'] = 'US'\n    df.loc[df['native_country']!='US','native_country'] = 'Others'\n    df.loc[df['race']!='White','race'] = 'Others'\n    \n    # Transformando o income em vari\u00e1vel bin\u00e1ria para o bom funcionamento do c\u00f3digo\n    # Instanciando o LabelEncoder\n    le = LabelEncoder()\n\n    # Modificando o nosso dataframe, transformando a vari\u00e1vel de classe em 0s e 1s\n    if 'income' in df.columns:\n        df['income'] = le.fit_transform(df['income'])\n    return df\n\ndf_training = data_treatment(df_training)\ndf_test = data_treatment(df_test)","6e09368b":"df_training['capital'] = df_training['capital_gain'] - df_training['capital_loss']\ndf_test['capital'] = df_test['capital_gain'] - df_test['capital_loss']","95430895":"# Seleciona as vari\u00e1veis num\u00e9ricas\nnumerical_cols = ['age', 'education_num', 'hours_per_week']\n\n# Seleciona as vari\u00e1veis num\u00e9ricas esparsas\nsparse_cols = ['capital', 'capital_gain', 'capital_loss']\n\n# Seleciona as vari\u00e1veis categ\u00f3ricas\ncategorical_cols = ['workclass', 'sex', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n\n# Verificando se n\u00e3o h\u00e1 vari\u00e1veis esquecidas\nfor col in numerical_cols+sparse_cols+categorical_cols:\n    if col not in df_training.columns:\n        print(col)\n\nfor col in df_training.columns:\n    if (col not in numerical_cols) and (col not in categorical_cols) and (col not in sparse_cols):\n        print(col)","126a2852":"# Cria a Pipeline de tratamento das vari\u00e1veis categ\u00f3ricas\n# Substituindo dados faltantes pela moda dos dados\n# E codificando os valores no formato one-hot\ncategorical_pipeline = Pipeline(steps = [\n    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n    ('onehot', OneHotEncoder(drop='if_binary'))\n])","1dfa808d":"# Cria a Pipeline de tratamento das vari\u00e1veis num\u00e9ricas\n# Substitui os dados faltantes por meio de um modelo de KNN\n# E normaliza os dados em seguida\nnumerical_pipeline = Pipeline(steps = [\n    ('imputer', KNNImputer(n_neighbors=10, weights=\"uniform\")),\n    ('scaler', StandardScaler())\n])","6644cd17":"# Cria a Pipeline de tratamento das vari\u00e1veis esparsas\n# E codificando os valores no formato one-hot\n# E realiza uma normaliza\u00e7\u00e3o robusta\nsparse_pipeline = Pipeline(steps = [\n    ('imputer', KNNImputer(n_neighbors=10, weights=\"uniform\")),\n    ('scaler', RobustScaler())\n])","471cbeff":"# Cria o Pr\u00e9-Processador\n\n# Cada pipeline est\u00e1 associada a suas respectivas colunas no datast\npreprocessor = ColumnTransformer(transformers = [\n    ('numerical', numerical_pipeline, numerical_cols),\n    ('sparse', sparse_pipeline, sparse_cols),\n    ('categorical', categorical_pipeline, categorical_cols)\n])","a0ccc3f5":"# Removendo dados duplicados dos dados de treino\ndf_training = df_training.drop_duplicates()\n\n# Separando a vari\u00e1vel de interesse das vari\u00e1veis de entrada\ny_training = df_training.income\nX_training = df_training.drop(columns='income')\n\n# Realizando a separa\u00e7\u00e3o dos dados de treino e valida\u00e7\u00e3o\nX_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.2, random_state=42)\n\n# Tratando a Base de dados\nX_train = preprocessor.fit_transform(X_train)\nX_val = preprocessor.transform(X_val)\nX_training = preprocessor.transform(X_training)\nX_test = preprocessor.transform(df_test)\n\nprint(\"Tamanho do conjunto de treino:\", \"X:\", X_train.shape, \"y:\", y_train.shape)\nprint(\"Tamanho do conjunto de valida\u00e7\u00e3o:\", \"X:\", X_val.shape, \"y:\", y_val.shape)\nprint(\"Tamanho do conjunto de teste:\", \"X:\", X_test.shape)\n\n# Para valida\u00e7\u00e3o cruzada, usa-se StratifiedKFold,\n# pois o Dataset possui um desbalanceamento das classes de interesse\ncv_skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=13)","57497a2d":"# Vari\u00e1vel para armazenamento dos scores\nmodels_cv_score = []","30e48eed":"# Tunando os hiperpar\u00e2metros do modelo\n# Cria objeto com o modelo\nknn = KNeighborsClassifier()\n\n# Cria o intervalo entre os par\u00e2metros\nparams = {\n    'n_neighbors': Integer(10, 50)\n}\n\n# Cria um objeto de grade de busca com Bayes Search\nknn_search_cv = BayesSearchCV(\n    estimator=knn,\n    search_spaces=params,\n    scoring='accuracy',\n    cv=cv_skf,\n    n_jobs=-1,\n    n_iter=15,\n    random_state=13\n)\n \n# Treinar o modelo com o Bayes search\n%timeit -n 1 -r 1 knn_search_cv.fit(X_train, y_train)\n\n# Printa resultados\nprint('Melhor modelo: {}'.format(knn_search_cv.best_estimator_))\nprint(f'Acur\u00e1cia m\u00e9dia do melhor modelo: {knn_search_cv.best_score_:.4f}')","c1745adb":"# Usando valida\u00e7\u00e3o cruzada para estimar o desempenho do melhor modelo\nknn_cv_scores = cross_validate(\n    estimator=knn_search_cv.best_estimator_,\n    X=X_train,\n    y=y_train,\n    cv=cv_skf,\n    scoring=('accuracy', 'f1', 'roc_auc'))\n\n# Armazenando os resultados na lista models_cv_score\nresults = {\n    'model_type': 'KNN',\n    'best_model': knn_search_cv.best_estimator_,\n    'mean_accuracy': knn_cv_scores['test_accuracy'].mean(),\n    'mean_f1': knn_cv_scores['test_f1'].mean(),\n    'mean_auc': knn_cv_scores['test_roc_auc'].mean(),\n}\n\npprint(results)\nmodels_cv_score.append(results)","6743d0a6":"# Tunando os hiperpar\u00e2metros do modelo\n# Cria objeto com o modelo\nsvm = SVC()\n\n# Cria o intervalo entre os par\u00e2metros\nparams = {\n    'C': np.logspace(-2,2),\n    'gamma': np.logspace(-4,-3),\n    'kernel': ['linear','poly','rbf','sigmoid']\n}\n\n# Cria um objeto de grade de busca com Random Search\nsvm_search_cv = RandomizedSearchCV(\n    estimator=svm,\n    param_distributions=params,\n    scoring='accuracy',\n    cv=cv_skf,\n    n_jobs=-1,\n    n_iter=2,\n    random_state=13\n)\n \n# Treinar o modelo com RandomizedSearchCV\n%timeit -n 1 -r 1 svm_search_cv.fit(X_train, y_train)\n\n# Printa resultados\nprint('Melhor modelo: {}'.format(svm_search_cv.best_estimator_))\nprint(f'Acur\u00e1cia m\u00e9dia do melhor modelo: {svm_search_cv.best_score_:.4f}')","e35ec2b3":"# Usando valida\u00e7\u00e3o cruzada para estimar o desempenho do melhor modelo\nsvm_cv_scores = cross_validate(\n    estimator=svm_search_cv.best_estimator_,\n    X=X_train,\n    y=y_train,\n    cv=cv_skf,\n    scoring=('accuracy', 'f1', 'roc_auc'))\n\nresults = {\n    'model_type': 'SVM',\n    'best_model': svm_search_cv.best_estimator_,\n    'mean_accuracy': svm_cv_scores['test_accuracy'].mean(),\n    'mean_f1': svm_cv_scores['test_f1'].mean(),\n    'mean_auc': svm_cv_scores['test_roc_auc'].mean(),\n}\n\n# Armazenando os resultados na lista models_cv_score\npprint(results)\nmodels_cv_score.append(results)","1c2633b2":"# Tunando os hiperpar\u00e2metros do modelo\n# Cria objeto com o modelo\nrf = RandomForestClassifier()\n\n# Cria o intervalo entre os par\u00e2metros\nparams = {\n    'bootstrap': [True, False],\n    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n    'max_features': ['auto', 'sqrt'],\n    'n_estimators': np.arange(100, 1000, 100),\n    'min_samples_leaf': np.arange(1, 5),\n    'criterion': ['gini', 'entropy'],\n}\n\n# Cria um objeto de grade de busca com Bayes Search:\nrf_search_cv = RandomizedSearchCV(\n    estimator=rf,\n    param_distributions=params,\n    scoring='accuracy',\n    cv=cv_skf,\n    n_jobs=-1,\n    n_iter=6,\n    random_state=13\n)\n \n# Treinar o modelo com RandomizedSearchCV\n%timeit -n 1 -r 1 rf_search_cv.fit(X_train, y_train)\n\n# Printa resultados\nprint('Melhor modelo: {}'.format(rf_search_cv.best_estimator_))\nprint(f'Acur\u00e1cia m\u00e9dia do melhor modelo: {rf_search_cv.best_score_:.4f}')","c77ad5b6":"# Usando valida\u00e7\u00e3o cruzada para estimar o desempenho do melhor modelo\nrf_cv_scores = cross_validate(\n    estimator=rf_search_cv.best_estimator_,\n    X=X_train,\n    y=y_train,\n    cv=cv_skf,\n    scoring=('accuracy', 'f1', 'roc_auc'))\n\n# Armazenando os resultados na lista models_cv_score\nresults = {\n    'model_type': 'RandomForest',\n    'best_model': rf_search_cv.best_estimator_,\n    'mean_accuracy': rf_cv_scores['test_accuracy'].mean(),\n    'mean_f1': rf_cv_scores['test_f1'].mean(),\n    'mean_auc': rf_cv_scores['test_roc_auc'].mean(),\n}\n\npprint(results)\nmodels_cv_score.append(results)","7980cfda":"# Tunando os hiperpar\u00e2metros do modelo\n# Cria objeto com o modelo\nxgb = XGBClassifier(objective='binary:logistic', eval_metric='error', use_label_encoder=False)\n\n# Cria o intervalo entre os par\u00e2metros\nparams = {\n    'n_estimators': randint(100, 1000),\n    'learning_rate': loguniform(1e-3, 1),\n    'max_depth': randint(1, 20),\n    'reg_alpha': loguniform(1e-7, 1e1),\n    'reg_lambda': loguniform(1e-7, 1e1),\n    'subsample': np.arange(0.5, 1.0, 0.1),\n    'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n    'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n}\n\n# Cria um objeto de grade de busca com Bayes Search:\n\nxgb_search_cv = RandomizedSearchCV(\n    estimator=xgb,\n    param_distributions=params,\n    scoring='accuracy',\n    cv=cv_skf,\n    n_jobs=-1,\n    n_iter=5,\n    random_state=13\n)\n \n# Treinar o modelo com RandomizedSearchCV\n%timeit -n 1 -r 1 xgb_search_cv.fit(X_train, y_train)\n\n# Printa resultados\nprint('Melhor modelo: {}'.format(xgb_search_cv.best_estimator_))\nprint(f'Acur\u00e1cia m\u00e9dia do melhor modelo: {xgb_search_cv.best_score_:.4f}')","6ad94c9b":"# Usando valida\u00e7\u00e3o cruzada para estimar o desempenho do melhor modelo\nxgb_cv_scores = cross_validate(\n    estimator=xgb_search_cv.best_estimator_,\n    X=X_train,\n    y=y_train,\n    cv=cv_skf,\n    scoring=('accuracy', 'f1', 'roc_auc'))\n\n# Armazenando os resultados na lista models_cv_score\nresults = {\n    'model_type': 'XGBoost',\n    'best_model': xgb_search_cv.best_estimator_,\n    'mean_accuracy': xgb_cv_scores['test_accuracy'].mean(),\n    'mean_f1': xgb_cv_scores['test_f1'].mean(),\n    'mean_auc': xgb_cv_scores['test_roc_auc'].mean(),\n}\n\npprint(results)\nmodels_cv_score.append(results)","0963b6a2":"# Tunando os hiperpar\u00e2metros do modelo\n# Cria objeto com o modelo\nmlp = MLPClassifier(max_iter=100, early_stopping=True)\n\n# Cria o intervalo entre os par\u00e2metros\nparams = {\n    'hidden_layer_sizes': [(2 ** i, 2 ** j) for j in np.arange(4, 10) for i in np.arange(4, 8)],\n    'alpha': loguniform(1e-7, 1e-1),\n    'activation': ['tanh', 'relu'],\n    'solver': ['sgd', 'adam'],\n    'learning_rate': ['constant','adaptive']\n}\n\n# Cria um objeto de grade de busca com RandomizedSearchCV:\nmlp_search_cv = RandomizedSearchCV(\n    estimator=mlp,\n    param_distributions=params,\n    scoring='accuracy',\n    cv=cv_skf,\n    n_jobs=-1,\n    n_iter=2,\n    random_state=13\n)\n \n# Treinar o modelo com o Bayes search\n%timeit -n 1 -r 1 mlp_search_cv.fit(X_train, y_train)\n\n# Printa resultados\nprint('Melhor modelo: {}'.format(mlp_search_cv.best_estimator_))\nprint(f'Acur\u00e1cia m\u00e9dia do melhor modelo: {mlp_search_cv.best_score_:.4f}')","151564a8":"# Usando valida\u00e7\u00e3o cruzada para estimar o desempenho do melhor modelo\nmlp_cv_scores = cross_validate(\n    estimator=mlp_search_cv.best_estimator_,\n    X=X_train,\n    y=y_train,\n    cv=cv_skf,\n    scoring=('accuracy', 'f1', 'roc_auc'))\n\n# Armazenando os resultados na lista models_cv_score\nresults = {\n    'model_type': 'Neural networks',\n    'best_model': mlp_search_cv.best_estimator_,\n    'mean_accuracy': mlp_cv_scores['test_accuracy'].mean(),\n    'mean_f1': mlp_cv_scores['test_f1'].mean(),\n    'mean_auc': mlp_cv_scores['test_roc_auc'].mean(),\n}\n\npprint(results)\nmodels_cv_score.append(results)","96ea8209":"# Verificando qual o melhor modelo dentre os treinados\ndf_models_cv_score = pd.DataFrame(models_cv_score)\n\n# Mostrando os resultados para cada modelo\ndf_models_cv_score.drop(columns=['best_model']).sort_values(by=['mean_accuracy'], ascending=False)","9c93783c":"# Melhor modelo possui maior acur\u00e1cia m\u00e9dia\nbest_type = df_models_cv_score.loc[df_models_cv_score.mean_accuracy.idxmax(), 'model_type']\nbest_model = df_models_cv_score.loc[df_models_cv_score.mean_accuracy.idxmax(), 'best_model']\n\nprint(\"Melhor modelo:\", best_type)\nprint(\"Melhores par\u00e2metros:\", best_model)\n\n# Fita o modelo com o todo o dataset de treino\nbest_model.fit(X_train, y_train)\n\n# Usa o modelo para prever o conjunto de valida\u00e7\u00e3o\ny_pred_val = best_model.predict(X_val)\n\nprint(\"\\nDesempenho do classificador no conjunto de valida\u00e7\u00e3o: \")\nprint(classification_report(y_val, y_pred_val))","2729e190":"# Plotando a curva ROC\nplot_roc_curve(best_model, X_val, y_val)\nplt.show()","af44e163":"# Treinando o melhor modelo para todos os dados de treino dispon\u00edveis\nbest_model.fit(X_training, y_training)\n\n# Realizando a previs\u00e3o dos dados de teste\ny_pred = best_model.predict(X_test)\n\n# Transformando novamente os labels em string para a sumiss\u00e3o\ny_pred = le.inverse_transform(y_pred)","8110c7a4":"submission = pd.DataFrame(\n    {\n        'Id': df_test.index,\n        'income': y_pred\n    }\n)","fbc829e1":"submission.to_csv('submission.csv',index = False)","d3fc85c2":"Nota-se uma maior propor\u00e7\u00e3o de homens do que de mulheres no Dataset. Al\u00e9m disso, no conjunto de dados, os homens possuem maior propor\u00e7\u00e3o de pessoas com mais de 50K de ganho do que mulheres.\n\nOutro fator relevante \u00e9 que o grau de escolaridade das mulheres apresentou uma distribui\u00e7\u00e3o mais centrada em torno da m\u00e9dia, com menor desvio padr\u00e3o, enquanto nos homens, esta vari\u00e1vel \u00e9 mais dispersa e com assimetria para valores mais baixos de escolaridade","e5e15736":"<a id=\"section-44\"><\/a>\n## 3. XGBoost Classifier","11f333e2":"## Analisando os dados faltantes","63b0b5ab":"Ambas as vari\u00e1veis `capital_gain` e `capital_loss` possuem uma distribui\u00e7\u00e3o altamente enviesada, com `outliers` extremamente superiores ao restante da distribui\u00e7\u00e3o. Pode-se perceber, ainda, que mais de 75% dos indiv\u00edduos do conjunto de dados apresentam ambos os valores dessas vari\u00e1veis iguais a 0.0 .\n\nDessa forma, talvez essas vari\u00e1veis n\u00e3o possuam uma relev\u00e2ncia boa para o modelo de classifica\u00e7\u00e3o. Outra vari\u00e1vel poss\u00edvel \u00e9 gerar uma vari\u00e1vel bin\u00e1ria (valor >0)","5eec06bb":"Percebe-se que a maioria dos indiv\u00edduos possuem a vari\u00e1vel `education_num` entre 8 e 10, tanto para pessoas que ganham acima de 50K quanto abaixo. Contudo, a propor\u00e7\u00e3o de pessoas com escolaridade superior a 10 no grupo que ganha acima de 50K \u00e9 maior do que o outro grupo. Al\u00e9m disso, a grande maioria das pessoas do conjunto de dados que possuem ganho maior que 50K possuem escolaridade acima de determinado n\u00edvel.","5a9c0e1e":"## Pr\u00e9-processamento dos dados","73f974f5":"<a id=\"section-5\"><\/a>\n# 5. Avalia\u00e7\u00e3o do desempenho do melhor modelo\n\nVerifica-se qual modelo teve o melhor desempenho dentre os modelos treinados.\nConsidera-se como melhor modelo, aquele que teve maior acur\u00e1cia m\u00e9dia nos conjuntos de dados da valida\u00e7\u00e3o cruzada\n","e206f62e":"## Escola Polit\u00e9cnica da USP\n### PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es","99f7be5b":"<a id=\"section-3\"><\/a>\n# 3. Tratamento dos dados","dece62dd":"<a id=\"section-2\"><\/a>\n# 2. An\u00e1lise explorat\u00f3ria dos dados","0e3637a6":"Percebe-se que o dataset possui 15 colunas, sendo uma delas de \u00edndice (`id`) e outra a vari\u00e1vel de interesse (`income`).\nO restante das vari\u00e1veis dividem-se entre vari\u00e1veis num\u00e9ricas, que s\u00e3o descritas por valores num\u00e9ricos, e vari\u00e1veis categ\u00f3ricas, que s\u00e3o descritas por um n\u00famero cont\u00e1vel de classes. Estas s\u00e3o:\n1) Vari\u00e1veis num\u00e9ricas\n- age\n- fnlwgt\n- education_num\n- capital_gain\n- capital_loss\n- hours_per_week\n\n2) Vari\u00e1veis categ\u00f3ricas\n- workclass\n- sex\n- education\n- marital_status\n- occupation\n- relationship\n- race\n- sex\n- native_country","9759dd68":"<a id=\"section-1\"><\/a>\n# 1. Setup e importa\u00e7\u00f5es\n## Importando bibliotecas","65679869":"## Feature Engineering","50322f1c":"## Split dos dados de treino e holdout (para avalia\u00e7\u00e3o do modelo)","e003b6ae":"Finalmente, para a submiss\u00e3o, treina-se o melhor modelo com todos os dados de treino dispon\u00edveis (treino + *hold-out*) para o ajuste do modelo para a maior quantidade de dados poss\u00edvel","c7daff5d":"<a id=\"section-4\"><\/a>\n# 4. Treinamento dos modelos","3660d375":"Como a propor\u00e7\u00e3o de indiv\u00edduos dos estados unidos \u00e9 extremamente superior ao restante, esta vari\u00e1vel ser\u00e1 substitu\u00edda por uma classe bin\u00e1ria entre nativo dos EUA ou n\u00e3o.","7ce4fd8b":"## Importando dados\n\nPrimeiro importa-se os dados de treino e teste a serem trabalhados. Em seguida, analisa-se os dados com os principais m\u00e9todos do pandas para uma compreens\u00e3o inicial dos dados.","2c88adbf":"# Sum\u00e1rio\n1. [Setup e importa\u00e7\u00f5es](#section-1)\n2. [An\u00e1lise explorat\u00f3ria dos dados (EDA)](#section-2)\n3. [Tratamento dos dados](#section-3)\n4. [Treinamento dos modelos](#section-4)\n    0. [Baseline (KNN)](#section-41)\n    1. [SVM Classifier](#section-42)\n    2. [Random Forest Classifier](#section-43)\n    3. [XGBoost Classifier](#section-44)\n    4. [Redes neurais](#section-45)\n5. [Avalia\u00e7\u00e3o do desempenho do melhor modelo](#section-5)","eb637d47":"Percebe-se que pessoas que ganham acima de 50K possuem uma distribui\u00e7\u00e3o de idades mais sim\u00e9trica e inclinada para a esquerda, enquanto a idade de pessoas que ganham abaixo de 50K possui uma maior frequ\u00eancia em idades mais baixas. Nota-se que a idade m\u00ednima \u00e9 de 17 anos, sendo poss\u00edvelmente uma limita\u00e7\u00e3o para quem pode responder a pesquisa.","28cac202":"Percebe-se que, em geral, pessoas que ganham mais de 50K trabalham mais de 40h por semana.","8d5b3e78":"<a id=\"section-42\"><\/a>\n## 1. SVM Classifier","01077c22":"<a id=\"section-41\"><\/a>\n## 0. Baseline (KNN)","993b08ba":"<a id=\"section-43\"><\/a>\n## 2. Random Forest Classifier","ae9d96ec":"Percebe-se que as colunas `education` e `education_num` s\u00e3o redundantes, e apresentam as mesmas informa\u00e7\u00f5es. Assim, para o modelo, ser\u00e1 utilizada a vari\u00e1vel `education_num` por possuir os dados de forma num\u00e9rica e ordenada por grau de escolaridade.","22cd9afc":"### An\u00e1lise de correla\u00e7\u00e3o entre as vari\u00e1veis","5f03d343":"Agora, deseja-se estimar quais as m\u00e9tricas do melhor modelo no conjunto de valida\u00e7\u00e3o. Para isso, treina-se o modelo para todos os dados de treino e realiza-se a previs\u00e3o no conjunto de valida\u00e7\u00e3o (*hold-out*)","b7c5f23d":"Nota-se que o classificador apresenta pior desempenho para prever a vari\u00e1vel 1 (pessoas com income >=50k).\n\nAgora, plota-se a curva ROC do classificador","6f1b5442":"## Submiss\u00e3o do c\u00f3digo","658b69b7":"Analisa-se que, nos dados do conjunto de treino, existe uma quantidade significativamente superior de pessoas brancas. Estas, tamb\u00e9m, possuem uma maior propor\u00e7\u00e3o de pessoas que ganham acima do que 50K do que outras etnias.\n\nAssim, pode-se substituir os valores nulos da vari\u00e1vel `race` por `'White'`. Por\u00e9m, \u00e9 preciso atentar-se se n\u00e3o h\u00e1 uma grade altera\u00e7\u00e3o da propor\u00e7\u00e3o da vari\u00e1vel `income` nessa classe, o que ocorre ap\u00f3s o tratamento.\n\nOutro tratamento para essa vari\u00e1vel pode ser criar uma classe secund\u00e1ria que agrege todas as outras classes que n\u00e3o s\u00e3o 'White'","f96d4d03":"<a id=\"section-45\"><\/a>\n## 4. Redes neurais"}}