{"cell_type":{"00499ac4":"code","692f5e77":"code","eb90538f":"code","45fe9346":"code","b08639ab":"code","5ae403c2":"code","b5e29030":"code","3b3ac0de":"code","92c3f592":"code","428678e7":"code","39c15470":"code","5b5caa43":"code","24342ec1":"code","713e265d":"code","63341496":"code","dd25c4cf":"code","9031a058":"code","5e31b3ff":"code","b79d379d":"code","8b62a4d0":"code","e546b1a0":"code","20622bb0":"code","fe0b39e9":"code","cce0738f":"code","c64b9d17":"code","950c5a63":"code","b785b154":"markdown","4264b4ca":"markdown","c8630c08":"markdown","2824d059":"markdown","d12e890d":"markdown","bbc360a5":"markdown","a755adf5":"markdown","f471a55d":"markdown","29e70ff8":"markdown","744f13a8":"markdown","5838e8a8":"markdown"},"source":{"00499ac4":"## It is cleaned data ,which is normally not gonna happen in real world. \n## But thanks to UCI, we can skip the data-cleaning job in this report.\n\n# Basic Part\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n# Modeling Part\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz \nfrom sklearn.metrics import roc_curve, auc \nfrom sklearn.metrics import classification_report \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.model_selection import train_test_split \nimport eli5 \nfrom eli5.sklearn import PermutationImportance\nimport shap \nfrom pdpbox import pdp, info_plots \n\n# Set a seed for tracing back and reproducing\nnp.random.seed(101) #ensure reproducibility\n\n","692f5e77":"df = pd.read_csv('..\/input\/heart.csv')\ndf.head()","eb90538f":"# or, a fast and easy way\ndf.describe()","45fe9346":"# Age Distribution\nsns.violinplot(df.age,palette = 'Set2',bw = .1, cut =1)\nplt.title('Age Distribution')","b08639ab":"# Chest Pain Type Distribution\nsns.countplot(x = 'cp', data = df)\nplt.title('Chest Pain Type Distribution')","5ae403c2":"# if you want to quickly take a glance of regression relationship, it is a fast way\n## sns.pairplot(hue = 'target',data = df)","b5e29030":"df = pd.get_dummies(df,drop_first = True)","3b3ac0de":"y = df.target\nX_train, X_test, y_train, y_test = train_test_split(df.drop('target', 1), y, test_size = .2, random_state=101)\nmodel = RandomForestClassifier(max_depth=5)\nmodel.fit(X_train, y_train)","92c3f592":"estimator = model.estimators_[1]\nfeature_names = [i for i in X_train.columns]\n\ny_train_str = y_train.astype('str')\ny_train_str[y_train_str == '0'] = 'Neg'\ny_train_str[y_train_str == '1'] = 'Pos'\ny_train_str = y_train_str.values","428678e7":"export_graphviz(estimator, out_file='tree.dot', \n                feature_names = feature_names,\n                class_names = y_train_str,\n                rounded = True, proportion = True, \n                label='root',\n                precision = 2, filled = True)\n\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\nfrom IPython.display import Image\nImage(filename = 'tree.png')","39c15470":"# The very manually way: (but it is good to understand the concept and the function)\ndef plot_feature_importances(n):\n    n_features = X_train.shape[1]\n    plt.figure(figsize = (10,10))\n    plt.barh(range(n_features), n.feature_importances_, align='center',color = 'm',alpha =0.6)\n    plt.yticks(np.arange(n_features), X_train.columns)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n    plt.ylim(-1, n_features)","5b5caa43":"plot_feature_importances(model)","24342ec1":"# The faster way:\nperm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","713e265d":"y_predict = model.predict(X_test)\ny_pred_quant = model.predict_proba(X_test)[:, 1]\ny_pred_bin = model.predict(X_test)","63341496":"confusion_matrix = confusion_matrix(y_test, y_pred_bin)\n\ntotal=sum(sum(confusion_matrix))\n\nsensitivity = confusion_matrix[0,0]\/(confusion_matrix[0,0]+confusion_matrix[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = confusion_matrix[1,1]\/(confusion_matrix[1,1]+confusion_matrix[0,1])\nprint('Specificity : ', specificity)","dd25c4cf":"fpr, tpr, thresholds = roc_curve(y_test, y_pred_quant)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls='--', c='r')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for RandomForest classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","9031a058":"auc(fpr, tpr)\n# See, it works quite well.","5e31b3ff":"sns.countplot(x = 'target',data = df)","b79d379d":"features = df.columns.values.tolist()\nfeatures.remove('target')\nfeat_name = 'ca'\npdp_dist = pdp.pdp_isolate(model=model, dataset=X_test, model_features = features, feature = feat_name)\npdp.pdp_plot(pdp_dist, feat_name)","8b62a4d0":"# how about the maximum heart beat?\nfeat_name = 'thalach'\npdp_dist = pdp.pdp_isolate(model=model, dataset=X_test, model_features = features, feature = feat_name)\npdp.pdp_plot(pdp_dist, feat_name)","e546b1a0":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values[1], X_test)","20622bb0":"def heart_disease_risk_predict(model, patient_Id):\n\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(patient_Id)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value[1], shap_values[1], patient_Id)","fe0b39e9":"p1 = X_test.iloc[1,:].astype(float)\nheart_disease_risk_predict(model, p1)","cce0738f":"p5 = X_test.iloc[5,:].astype(float)\nheart_disease_risk_predict(model, p5)","c64b9d17":"p10 = X_test.iloc[10,:].astype(float)\nheart_disease_risk_predict(model, p10)","950c5a63":"\nshap_values = explainer.shap_values(X_train.iloc[20:50])\nshap.force_plot(explainer.expected_value[1], shap_values[1], X_test.iloc[20:50])","b785b154":"Let's try to predict any individual patient and see how the different variables are affecting their outcomes.","4264b4ca":"### Key Point: ROC Graph\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/3\/36\/ROC_space-2.png)","c8630c08":"The higher max heart beat, the higher risk. \n\n","2824d059":"### What happened in machine but out of our sight ?","d12e890d":"Now Let's dig deeper. The follows are the important features we knew from above. \nI'd like to explore each influence on target.\n\n* ca: number of major vessels (0-3) colored by flourosopy\n* oldpeak: ST depression induced by exercise relative to rest\n* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n* thalach: maximum heart rate achieved\n","bbc360a5":"## Diagnose and Predict Heart Disease\n![](https:\/\/cld.activebeat.com\/image\/upload\/t_tn,f_auto,q_auto,$h_360,$w_580\/ab\/2016\/06\/shutterstock_259237739-580x360.jpg)\n\nTarget:\n1. Fast Exploration \n2. Find the key features\n3. Modeling an Predicting\n\n","a755adf5":"There is a marvelous function which shows the predictions andfactors for amounts of  patients all together.","f471a55d":"### The End\n\nThe dataset might be a little outdated and small but it is very good source to learn and practice.\nThe more important thing is to understand concept and to know how.\n\nData science is a life journey. It is awesome to learn and try things new everyday.","29e70ff8":"As what we see, the data is very clean and transformatted.\nAccording to the describe, it means as follow:\n1. age: age in years\n2. sex: (1 = male; 0 = female)\n3. cp: chest pain type\n4. trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n5. chol: serum cholestoral in mg\/dl\n6. fbs:(fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n7. restecg: resting electrocardiographic results\n8. thalach: maximum heart rate achieved\n9. exang: exercise induced angina (1 = yes; 0 = no)\n10. oldpeak: ST depression induced by exercise relative to rest\n11. slope: the slope of the peak exercise ST segment\n12. ca: number of major vessels (0-3) colored by flourosopy\n13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n14. target: 1 or 0\n\nNow we just need to check how many unique value in each observation and continue transformatting based on our demand.","744f13a8":"### Note:\nROC works well in binary classification and the data is not asysmmetric. \nThe \"credit card fraud detection\" is inapproprite due to the extremely skewed distribution. It looks like\n![](http:\/\/nycdatascience.com\/blog\/wp-content\/uploads\/2018\/11\/pic1.png)\nEven you do nothing the ROC performance still looks perfect, even higher than 95% but make non-sense. We know in real world only a few people will defaut or cheat. If that is not true, most of the banks already bankrupted and we had to suffer extremely high interest.\n\nBe sure to check the distribution before choosing models","5838e8a8":"### Key Point: Sensitivity and Specificity \n![](https:\/\/www.researchgate.net\/profile\/Diana_Carvajal5\/publication\/49650721\/figure\/fig1\/AS:277352501268483@1443137397711\/Calculation-of-sensitivity-specificity-and-positive-and-negative-predictive.png)\n![](https:\/\/i2.wp.com\/emcrit.org\/wp-content\/uploads\/2017\/06\/12.png)"}}