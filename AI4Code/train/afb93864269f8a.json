{"cell_type":{"e4eadc08":"code","93018027":"code","1dc5f8e6":"code","7bd70b0d":"code","d2d18105":"code","3529be22":"code","7b687a99":"code","52389acd":"code","a7d4cc35":"code","3e5aa078":"code","1e9196c8":"code","e43b8686":"code","6a05c03b":"code","99ca2e32":"code","7b2e5950":"code","cc97d84f":"code","223b4d06":"code","98314452":"code","ba221c85":"code","99d66220":"code","794bdb1b":"code","6e093fc4":"code","c91ff127":"code","fe303b7d":"code","f3682b42":"code","257d1a68":"code","6fa2d02e":"code","b88f08b3":"code","7b86bebe":"code","2e926cc7":"code","d4e65afb":"code","d2ebfd7c":"code","afcee769":"code","948bd0df":"code","3dad7081":"markdown","3c5c0e8e":"markdown","5eb4fce5":"markdown","120c0963":"markdown","7929291e":"markdown","5f184c9e":"markdown","1ccad359":"markdown","6a9c0371":"markdown","78864b75":"markdown","0cbd02b4":"markdown","2f6e51e4":"markdown","c70a8534":"markdown","4c9d3c77":"markdown"},"source":{"e4eadc08":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","93018027":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ntest['Survived']= submission['Survived']","1dc5f8e6":"df = pd.concat([test.assign(ind=\"test\"), train.assign(ind=\"train\")])","7bd70b0d":"df.head(2)","d2d18105":"[train.shape , test.shape , submission.shape]","3529be22":"df.info()","7b687a99":"sns.barplot(data=df, x='Sex', y= 'Survived')","52389acd":"sns.countplot(data=df, x='Survived')","a7d4cc35":"sns.scatterplot(data=df, x='Age', y='Fare', hue='Survived')","3e5aa078":"# Deal with \"Cabin\" Column\ndf = df.drop(['Cabin'] , axis=1)\n\n# Some columns have no effect on survival so we remove them:\ndf = df.drop(['PassengerId'] , axis=1)\ndf = df.drop(['Ticket'] , axis=1)\ndf = df.drop(['Name'] , axis=1)\n\n# Deal with \"Age\" Column\ndf = df.dropna(axis=0, subset=['Age'])\n\n# We have some null in Embarked and Fare columns:\ndf = df.dropna(axis=0, subset=['Embarked'])\ndf = df.dropna(axis=0, subset=['Fare'])\n\n# Dealing with Categorical Data\ndf['Pclass'] = df['Pclass'].apply(str)\n\n# Convert All Object type to One hot encoding\n\n# START ONE HOT ENCODING\ndf_num = df.select_dtypes(exclude='object')\ndf_obj = df.select_dtypes(include='object')\nnon_dummy_cols = ['ind']\ndummy_cols = list(set(df_obj.columns) - set(non_dummy_cols))\ndf_obj = pd.get_dummies(df_obj, columns=dummy_cols, drop_first=True)\ndf = pd.concat([df_num, df_obj], axis = 1)\n# END ONE HOT ENCODING\n\n# Split Test data from df\ntest, train = df[df[\"ind\"].eq(\"test\")], df[df[\"ind\"].eq(\"train\")]\n\n# We should Drop indicator Column from test and train dataframes:\ntest= test.drop(['ind'], axis=1)\ntrain= train.drop(['ind'], axis=1)","1e9196c8":"df.head(2)","e43b8686":"# Determine the Features & Target Variable\n# Split the Data to Train & Test\nX_test, y_test = test.drop(columns='Survived').copy(), test['Survived'].copy()\nX_train, y_train = train.drop(columns='Survived').copy(), train['Survived'].copy()","6a05c03b":"from sklearn.preprocessing import StandardScaler\nscaler= StandardScaler()\nscaler.fit(X_train)\nscaled_X_train= scaler.transform(X_train)\nscaled_X_test= scaler.transform(X_test)","99ca2e32":"from sklearn.neighbors import KNeighborsClassifier\nknn_model= KNeighborsClassifier(n_neighbors=1)\nknn_model.fit(scaled_X_train, y_train)","7b2e5950":"y_pred= knn_model.predict(scaled_X_test)\n#The prediction Value VS Actual Value of Test Data\npd.DataFrame({'Y_Test':y_test, 'Y_Pred': y_pred})","cc97d84f":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\naccuracy_score(y_test, y_pred)","223b4d06":"confusion_matrix(y_test, y_pred)","98314452":"print(classification_report(y_test, y_pred))","ba221c85":"test_error_rate= []\n\n\nfor k in range (1, 30):\n    knn_model = KNeighborsClassifier(n_neighbors=k)\n    knn_model.fit(scaled_X_train, y_train)\n    \n    y_pred_test = knn_model.predict(scaled_X_test)\n    \n    test_error=1- accuracy_score(y_test, y_pred_test)\n    test_error_rate.append(test_error)","99d66220":"test_error_rate","794bdb1b":"plt.figure(figsize=(10, 6))\nplt.plot(range(1, 30), test_error_rate, label='Test Error')\nplt.legend()\nplt.ylabel('Error Rate')\nplt.xlabel('K Value')","6e093fc4":"scaler= StandardScaler()\nknn= KNeighborsClassifier()\nknn.get_params().keys()","c91ff127":"operations= [('scaler', scaler), ('knn', knn)]","fe303b7d":"from sklearn.pipeline import Pipeline\npipe= Pipeline(operations)\nfrom sklearn.model_selection import GridSearchCV\nk_values= list(range(1, 20))\nparam_grid= {'knn__n_neighbors': k_values}\nfull_cv_classifier= GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')\nfull_cv_classifier.fit(X_train, y_train)","f3682b42":"full_cv_classifier.best_estimator_.get_params()","257d1a68":"full_cv_classifier.cv_results_.keys()","6fa2d02e":"scaler= StandardScaler()\nknn14= KNeighborsClassifier(n_neighbors=14)\noperations= [('scaler', scaler), ('knn14', knn14)]","b88f08b3":"pipe= Pipeline(operations)\npipe.fit(X_train, y_train)","7b86bebe":"pipe_pred= pipe.predict(X_test)\nprint(classification_report(y_test, pipe_pred))","2e926cc7":"sample= X_test.iloc[35]\nsample","d4e65afb":"sample.values","d2ebfd7c":"sample.values.reshape(1, -1)","afcee769":"pipe.predict(sample.values.reshape(1, -1))","948bd0df":"pipe.predict_proba(sample.values.reshape(1, -1))","3dad7081":"## Creating a Pipeline to find K value","3c5c0e8e":"## Data Cleaning\nI did this part with details on [this notebook](https:\/\/www.kaggle.com\/sajjadnajafi\/logistic-regression-titanic\/).","5eb4fce5":"## Train the Model","120c0963":"## Final Model","7929291e":"## Import Dataset","5f184c9e":"## Import Libraries","1ccad359":"### Combine Train and Test Datasets for Data Cleaning","6a9c0371":"## Evaluating the Model","78864b75":"## Elbow Method for Choosing Reasonable K Values","0cbd02b4":"# k-Nearest Neighbors\n\nThe k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement, non-parametric, lazy learning, supervised machine learning algorithm that can be used to solve both classification and regression problems using feature similarity.\n\nLearning KNN machine learning algorithm is a great way to introduce yourself to machine learning and classification in general. At its most basic level, it is essentially classification by finding the most similar data points in the training data, and making an educated guess based on their classifications.\n\nAlthough very simple to understand and implement, this method has seen wide application in many domains, such as in recommendation systems, semantic searching, and anomaly detection.\n\n## What is K- Nearest Neighbors?\n\n**K- Nearest Neighbors is a**\n\n* **Non parametric** as it does not make an assumption about the underlying data distribution pattern\n* **Lazy algorithm** as KNN does not have a training step. All data points will be used only at the time of prediction. With no training step, prediction step is costly.\n* **Supervised machine learning algorithm** as target variable is known\n* Used for both **Classification** and **Regression**\n* Uses **feature similarity\/nearest neighbors** to predict the cluster that the new point will fall into.\n\n(Read Full Article [Here](https:\/\/medium.com\/@rndayala\/k-nearest-neighbors-a76d0831bab0))","2f6e51e4":"### Scaling the Features","c70a8534":"## Predicting Test Data","4c9d3c77":"## Data Overview"}}