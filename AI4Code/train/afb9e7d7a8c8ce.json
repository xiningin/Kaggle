{"cell_type":{"da8dc515":"code","1245afaa":"code","447970a2":"code","7ec3d1f6":"code","a975d5ed":"code","d422c561":"code","3317414c":"code","f461ed03":"code","99820b2e":"code","0448a124":"code","171bb0ae":"code","84df5a62":"code","91081776":"code","6e294dcd":"code","9c9d1128":"code","84d398f2":"code","bb926aa3":"code","4ac1f718":"code","b802defd":"markdown","57a0996f":"markdown","ae14b02c":"markdown","51a55fa4":"markdown","8193e101":"markdown","3846501e":"markdown","a5d043e3":"markdown","f3cebc2b":"markdown","81039c38":"markdown"},"source":{"da8dc515":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.neighbors import  KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix, confusion_matrix\nimport pickle\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","1245afaa":"DATA_PATH = \"..\/input\/crop-recommendation-dataset\/Crop_recommendation.csv\"","447970a2":"data = pd.read_csv(DATA_PATH)\ndata.head()","7ec3d1f6":"data.info()","a975d5ed":"labels = data[\"label\"].unique()\ndata[\"label\"].value_counts().plot(kind=\"bar\")\nplt.show()","d422c561":"all_columns = data.columns[:-1]\n\nplt.figure(figsize=(15,13))\ni = 1\nfor column in all_columns[:-1]:\n    plt.subplot(3,3,i)\n    sns.histplot(data[column])\n    i+=1\nplt.show()\n\nsns.histplot(data[all_columns[-1]])\nplt.show()","3317414c":"for column in all_columns:\n    plt.figure(figsize=(19,7))\n    sns.barplot(x = \"label\", y = column, data = data)\n    plt.xticks(rotation=90)\n    plt.title(f\"{column} vs Crop Type\")\n    plt.show()","f461ed03":"plt.figure(figsize=(19,17))\nsns.pairplot(data, hue = \"label\")\nplt.show()","99820b2e":"plt.figure(figsize = (13,11))\nsns.heatmap(data.corr(), center = 0, annot = True)\nplt.show()","0448a124":"label_encoder = LabelEncoder()\nX = data[all_columns]\ny = label_encoder.fit_transform(data[\"label\"])\nprint(X.shape, y.shape)","171bb0ae":"label_dict = {}\nfor i in range(22):\n    label_dict[i] = label_encoder.inverse_transform([i])[0]\nlabel_dict","84df5a62":"X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size = 0.2, random_state = 0)\nprint(f\"Train Data: {X_train.shape}, {y_train.shape}\")\nprint(f\"Train Data: {X_test.shape}, {y_test.shape}\")","91081776":"error_rate = []\nfor i in range(1, 50):\n    pipeline = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors = i))\n    pipeline.fit(X_train, y_train)\n    predictions = pipeline.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    print(f\"Accuracy at k = {i} is {accuracy}\")\n    error_rate.append(np.mean(predictions != y_test))\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,50),error_rate,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nprint(\"Minimum error:-\",min(error_rate),\"at K =\",error_rate.index(min(error_rate))+1)","6e294dcd":"# Optimal K value is 4\nknn_pipeline = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors = 4))\nknn_pipeline.fit(X_train, y_train)\n\n# Test Data Metrics\npredictions = knn_pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy on Test Data: {accuracy*100}%\")\nplt.figure(figsize = (15,9))\nsns.heatmap(confusion_matrix(y_test, predictions), annot = True)\nplt.title(\"Confusion Matrix for Test Data\")\nplt.show()\n\nprint()\n\n# Whole Data Metrics\npredictions = knn_pipeline.predict(X.values)\naccuracy = accuracy_score(y, predictions)\nprint(f\"Accuracy on Whole Data: {accuracy*100}%\")\nplt.figure(figsize = (15,9))\nsns.heatmap(confusion_matrix(y, predictions), annot = True)\nplt.title(\"Confusion Matrix for Whole Data\")\nplt.show()","9c9d1128":"rf_pipeline = make_pipeline(StandardScaler(), RandomForestClassifier(random_state = 18))\nrf_pipeline.fit(X_train, y_train)\n\n# Accuray On Test Data\npredictions = rf_pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy on Test Data: {accuracy*100}%\")\nplt.figure(figsize = (15,9))\nsns.heatmap(confusion_matrix(y_test, predictions), annot = True)\nplt.title(\"Confusion Matrix for Test Data\")\nplt.show()\n\nprint()\n\n# Accuray On Whole Data\npredictions = rf_pipeline.predict(X.values)\naccuracy = accuracy_score(y, predictions)\nprint(f\"Accuracy on Whole Data: {accuracy*100}%\")\nplt.figure(figsize = (15,9))\nsns.heatmap(confusion_matrix(y, predictions), annot = True)\nplt.title(\"Confusion Matrix for Whole Data\")\nplt.show()","84d398f2":"xgb_pipeline = make_pipeline(StandardScaler(), XGBClassifier(random_state = 18))\nxgb_pipeline.fit(X_train, y_train)\n\n# Accuray On Test Data\npredictions = xgb_pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy on Test Data: {accuracy*100}%\")\nplt.figure(figsize = (15,9))\nsns.heatmap(confusion_matrix(y_test, predictions), annot = True)\nplt.title(\"Confusion Matrix for Test Data\")\nplt.show()\n\nprint()\n\n# Accuray On Whole Data\npredictions = xgb_pipeline.predict(X.values)\naccuracy = accuracy_score(y, predictions)\nprint(f\"Accuracy on Whole Data: {accuracy*100}%\")\nplt.figure(figsize = (15,9))\nsns.heatmap(confusion_matrix(y, predictions), annot = True)\nplt.title(\"Confusion Matrix for Whole Data\")\nplt.show()","bb926aa3":"print(xgboost.__version__)\nimport sklearn\nprint(sklearn.__version__)","4ac1f718":"pickle.dump(knn_pipeline, open(\"knn_pipeline.pkl\", \"wb\"))\npickle.dump(rf_pipeline, open(\"rf_pipeline.pkl\", \"wb\"))\npickle.dump(xgb_pipeline, open(\"xgb_pipeline.pkl\", \"wb\"))\npickle.dump(label_dict, open(\"label_dictionary.pkl\", \"wb\"))\nprint(\"Saved All Models\")","b802defd":"From the paiplot we can see that different clusters are formed based on the feautures in the dataset. First let us experiment with KNearestNeighbors model.","57a0996f":"## KNN Classifier","ae14b02c":"## Bivariate Analysis","51a55fa4":"### Attributes Description\n\n1. N - ratio of Nitrogen content in soil\n2. P - ratio of Phosphorous content in soil\n3. K - ratio of Potassium content in soil\n4. temperature - temperature in degree Celsius\n5. humidity - relative humidity in %\n6. ph - ph value of the soil\n7. rainfall - rainfall in mm","8193e101":"## XGBoost Classifier","3846501e":"<b>Observation: <\/b>The dataset is completely balanced. There is no need to balance the data.","a5d043e3":"## Exploring the Data","f3cebc2b":"## Univariate Analysis","81039c38":"## Random Forest Classifier"}}