{"cell_type":{"6107611d":"code","844a3e12":"code","9ef7b004":"code","f52d05cf":"code","59f91daa":"code","bc43154e":"code","14c397eb":"code","214dd27f":"code","192faa3b":"markdown"},"source":{"6107611d":"from IPython.display import Image\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom os import listdir\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model, layers, backend as K\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import confusion_matrix","844a3e12":"train_dir = '..\/input\/dataset1\/dataset1\/train\/'\ntest_dir = '..\/input\/dataset1\/dataset1\/test\/'\nvalid_dir = '..\/input\/dataset1\/dataset1\/valid\/'\nalpha = 1 # Variable used to decay dropout and data augmentation.\nepochs = 75\nbatch_size = 128\nsize = 100\nseed = 1234","9ef7b004":"def customNorm(input_layer):\n    units = input_layer.shape[-1]\n    \n    inputs = layers.Input(input_layer.shape[1:])\n    mean = layers.Lambda(lambda x: K.mean(x, (1,2), True))(inputs)\n    std = layers.Lambda(lambda x: K.std(x, (1,2), True) + 1e-8)(inputs)\n    norm = layers.Lambda(lambda x: (x[0] - x[1]) \/ x[2])([inputs, mean, std])\n    \n    info = layers.Reshape((units,))(mean)\n    info = layers.Dense(units\/\/2, kernel_initializer='he_normal')(info)\n    info = layers.LeakyReLU(1e-3)(info)\n    \n    mul = layers.Dense(units, kernel_initializer='zeros', bias_initializer='ones')(info)\n    mul = layers.Reshape((1,1,units))(mul)\n    shift = layers.Dense(units, kernel_initializer='zeros', bias_initializer='zeros')(info)\n    shift = layers.Reshape((1,1,units))(shift)\n    out = layers.Lambda(lambda x: x[0] * x[1] + x[2])([norm, mul, shift])\n    \n    custom_norm = Model(inputs, out)\n    \n    return custom_norm(input_layer)","f52d05cf":"def model():\n    x = layers.Input((size, size, 3))\n    \n    skip = layers.Conv2D(16, 1)(x)\n    conv = layers.Conv2D(16, 3, padding='same')(skip)\n    conv = layers.LeakyReLU(1e-3)(conv)\n    skip = layers.Add()([skip, conv])\n    skip = customNorm(skip)\n    conv = layers.Conv2D(16, 3, padding='same')(skip)\n    conv = layers.LeakyReLU(1e-3)(conv)\n    skip = layers.Add()([skip, conv])\n    skip = layers.MaxPool2D()(skip)\n    skip = customNorm(skip)\n    \n    skip = layers.Concatenate()([skip, skip])\n    conv = layers.Conv2D(32, 3, padding='same')(skip)\n    conv = layers.LeakyReLU(1e-3)(conv)\n    skip = layers.Add()([skip, conv])\n    skip = customNorm(skip)\n    conv = layers.Conv2D(32, 3, padding='same')(skip)\n    conv = layers.LeakyReLU(1e-3)(conv)\n    skip = layers.Add()([skip, conv])\n    skip = layers.MaxPool2D()(skip)\n    skip = customNorm(skip)\n    \n    skip = layers.Concatenate()([skip, skip])\n    conv = layers.Conv2D(64, 3, padding='same')(skip)\n    conv = layers.LeakyReLU(1e-3)(conv)\n    skip = layers.Add()([skip, conv])\n    skip = customNorm(skip)\n    conv = layers.Conv2D(64, 3, padding='same')(skip)\n    conv = layers.LeakyReLU(1e-3)(conv)\n    skip = layers.Add()([skip, conv])\n    skip = layers.MaxPool2D()(skip)\n    skip = customNorm(skip)\n    \n    skip = layers.Concatenate()([skip, skip])\n    conv = layers.Conv2D(128, 3, padding='same')(skip)\n    conv = layers.LeakyReLU(1e-3)(conv)\n    skip = layers.Add()([skip, conv])\n    skip = customNorm(skip)\n    conv = layers.Conv2D(128, 3, padding='same')(skip)\n    conv = layers.LeakyReLU(1e-3)(conv)\n    skip = layers.Add()([skip, conv])\n    skip = layers.MaxPool2D()(skip)\n    skip = customNorm(skip)\n    \n    skip = layers.Concatenate()([skip, skip])\n    conv = layers.Conv2D(256, 3, padding='same')(skip)\n    conv = layers.LeakyReLU(1e-3)(conv)\n    skip = layers.Add()([skip, conv])\n    skip = customNorm(skip)\n    conv = layers.Conv2D(256, 3, padding='same')(skip)\n    conv = layers.LeakyReLU(1e-3)(conv)\n    skip = layers.Add()([skip, conv])\n    \n    y = layers.GlobalAvgPool2D()(skip)\n    y = layers.Dropout(.2, name='dropout')(y)\n    y = layers.Dense(128)(y)\n    y = layers.LeakyReLU(1e-3)(y)\n    y = layers.Dense(1, activation='sigmoid')(y)\n    \n    return Model(x, y)\n\n# 1.8M parameters.\nmodel = model()\nmodel.compile(SGD(2e-2, .9, True), 'binary_crossentropy', metrics=['acc'])\n\ntf.keras.utils.plot_model(model, 'model.png', show_shapes=True, show_layer_names=False, dpi=54)\nImage('model.png')","59f91daa":"train_loss = []\ntest_loss = []\ntrain_acc = []\ntest_acc = []\nbest_acc = 0\n\nfor e in range(epochs):\n    train_idg = ImageDataGenerator(rotation_range = .25*alpha,\n                                   width_shift_range = .2*alpha,\n                                   height_shift_range = .2*alpha,\n                                   zoom_range = .2*alpha,\n                                   horizontal_flip = True,\n                                   preprocessing_function = lambda x: x\/127.5-1)\n    train_idg = train_idg.flow_from_directory(train_dir,\n                                              (size, size),\n                                              class_mode = 'binary',\n                                              batch_size = batch_size,\n                                              seed = seed)\n    \n    test_idg = ImageDataGenerator(preprocessing_function = lambda x: x\/127.5-1)\n    test_idg = test_idg.flow_from_directory(test_dir,\n                                            (size, size),\n                                            class_mode = 'binary',\n                                            batch_size = batch_size,\n                                            shuffle = False)\n    \n    h = model.fit_generator(train_idg,\n                            len(train_idg),\n                            validation_data = test_idg,\n                            validation_steps = len(test_idg),\n                            shuffle = False).history\n    \n    train_loss.append(h['loss'][0])\n    test_loss.append(h['val_loss'][0])\n    train_acc.append(h['acc'][0])\n    test_acc.append(h['val_acc'][0])\n    \n    if test_acc[-1] > best_acc:\n        best_acc = test_acc[-1]\n        model.save('Model.h5')\n        \n    alpha *= .95","bc43154e":"print('Best accuracy %.2f' % (best_acc*100), '%', sep='')\n\nplt.figure(figsize=(8, 6))\nplt.plot(train_loss, 'b-', label='train_loss')\nplt.plot(test_loss, 'r-', label='test_loss')\nplt.title('Losses')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(8, 6))\nplt.plot(train_acc, 'b-', label='train_acc')\nplt.plot(test_acc, 'r-', label='test_acc')\nplt.title('Accuracies')\nplt.legend()\nplt.show()","14c397eb":"test_idg = ImageDataGenerator(preprocessing_function = lambda x: x\/127.5-1)\ntest_idg = test_idg.flow_from_directory(test_dir,\n                                        (size, size),\n                                        class_mode = 'binary',\n                                        batch_size = batch_size,\n                                        shuffle = False)\n\ny_pred = np.round(model.predict_generator(test_idg, len(test_idg)))\ncm = confusion_matrix(test_idg.classes, y_pred)\n\nfig, ax = plt.subplots()\nim = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nax.figure.colorbar(im, ax=ax)\n# We want to show all ticks...\nax.set(xticks = np.arange(cm.shape[1]),\n       yticks = np.arange(cm.shape[0]),\n       # ... and label them with the respective list entries\n       xticklabels = ['Man', 'Woman'], yticklabels = ['Man', 'Woman'],\n       title = 'Test Confusion Matrix',\n       ylabel = 'True label',\n       xlabel = 'Predicted label')\n\n# Rotate the tick labels and set their alignment.\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n# Loop over data dimensions and create text annotations.\nfmt = 'd'\nthresh = cm.max() \/ 2.\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        ax.text(j, i, format(cm[i, j], fmt),\n                ha=\"center\", va=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\")\nfig.tight_layout()\nplt.show()","214dd27f":"valid_idg = ImageDataGenerator(preprocessing_function = lambda x: x\/127.5-1)\nvalid_idg = valid_idg.flow_from_directory(valid_dir,\n                                         (size, size),\n                                         class_mode = 'binary',\n                                         batch_size = batch_size,\n                                         shuffle = False)\n\ny_pred = np.round(model.predict_generator(valid_idg, len(valid_idg)))\ncm = confusion_matrix(valid_idg.classes, y_pred)\n\nfig, ax = plt.subplots()\nim = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nax.figure.colorbar(im, ax=ax)\n# We want to show all ticks...\nax.set(xticks = np.arange(cm.shape[1]),\n       yticks = np.arange(cm.shape[0]),\n       # ... and label them with the respective list entries\n       xticklabels = ['Man', 'Woman'], yticklabels = ['Man', 'Woman'],\n       title = 'Valid Confusion Matrix',\n       ylabel = 'True label',\n       xlabel = 'Predicted label')\n\n# Rotate the tick labels and set their alignment.\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n# Loop over data dimensions and create text annotations.\nfmt = 'd'\nthresh = cm.max() \/ 2.\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        ax.text(j, i, format(cm[i, j], fmt),\n                ha=\"center\", va=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\")\nfig.tight_layout()\nplt.show()","192faa3b":"Plot confusion matrix for test and validation data. Code copied from scikit-learn's documentation."}}