{"cell_type":{"896cdaa1":"code","3f1a5144":"code","a7ca8349":"code","1144a36b":"code","96071e68":"code","ef9954bd":"code","60d69beb":"code","e47b5253":"code","5c1db9d1":"code","fdeeddf3":"code","3ab4eb91":"code","f0d3e7c8":"code","07122126":"code","4b71d688":"code","5e633c58":"code","0e425929":"code","97edbc49":"code","30c6d245":"code","7f9a35ff":"code","00ac44bc":"code","22e8de6d":"code","396ea5c2":"code","8cb68bc4":"code","2931ae7f":"code","4f56e74d":"code","51121bd2":"code","ab33f2b1":"code","b0266d48":"code","de5e60ed":"code","8349882d":"code","80977776":"code","dcca687e":"code","5415ce93":"code","43b245f5":"code","229d57e5":"code","dff4ef33":"code","9986679a":"code","fd484c29":"code","3a1245b8":"code","9353f657":"code","be103deb":"code","5ba6d7ea":"code","9385a811":"code","bf99a775":"code","616f0324":"code","e98efa3b":"code","27fc5d59":"code","b18837d7":"code","50b25b3a":"code","9d174a08":"code","86801c87":"code","9109c1dc":"code","f6923175":"code","d2bd5eb4":"code","ca082e8a":"code","114c8384":"code","bb565301":"code","5c357b1b":"code","345ca8f2":"code","400d2da2":"code","048acacc":"code","df7abf78":"code","7d5771d9":"code","aad6b7a0":"code","97a7cc45":"code","a1c9e277":"code","1155642c":"code","34a734e0":"code","e223f6a4":"code","6293a413":"code","054fbf0c":"code","f3451a5f":"code","7cae53e8":"code","7a496b83":"code","19d5de54":"code","dad85a0a":"code","342f51e8":"code","6c5270b2":"code","58d0985c":"code","23136178":"code","03231b58":"code","4f7f9548":"code","49ff9011":"code","ad92047a":"code","6dcfd06f":"code","f7490c96":"code","9889e217":"code","e1e3e117":"code","b11984d1":"code","93f9f04e":"code","9a8afcfa":"code","23b0636a":"code","5687c7b1":"code","a86aeb28":"markdown","9fd9648b":"markdown","390858cd":"markdown","8b1c005a":"markdown","31b43d6d":"markdown","5680d3ef":"markdown","6646f380":"markdown","ee184fd9":"markdown","efb70553":"markdown","03a459a5":"markdown","6d769e8c":"markdown","677a7acf":"markdown","1ff9506b":"markdown","f6cd149d":"markdown","4105e05f":"markdown","13b07a61":"markdown","56935ba7":"markdown","cde109c6":"markdown","f3a23d3d":"markdown","2c4911b8":"markdown","151d3974":"markdown","13194fdc":"markdown","7630c34a":"markdown","970cd565":"markdown","06bc763e":"markdown"},"source":{"896cdaa1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3f1a5144":"sub_file = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nsub_file.head()","a7ca8349":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain.head()","1144a36b":"val = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nval.head()","96071e68":"train.columns","ef9954bd":"val.columns","60d69beb":"train.isnull().mean()","e47b5253":"val.isnull().mean()","5c1db9d1":"train.shape","fdeeddf3":"train.describe()","3ab4eb91":"val.describe()","f0d3e7c8":"def impute_na_numeric(train,val,var):\n    mean = train[var].mean()\n    median = train[var].median()\n    \n    train[var+\"_mean\"] = train[var].fillna(mean)\n    train[var+\"_median\"] = train[var].fillna(median)\n    \n    var_original = train[var].std()**2\n    var_mean = train[var+\"_mean\"].std()**2\n    var_median = train[var+\"_median\"].std()**2\n    \n    print(\"Original Variance: \",var_original)\n    print(\"Mean Variance: \",var_mean)\n    print(\"Median Variance: \",var_median)\n    \n    if((var_mean < var_original) | (var_median < var_original)):\n        if(var_mean < var_median):\n            train[var] = train[var+\"_mean\"]\n            val[var] = val[var].fillna(mean)\n        else:\n            train[var] = train[var+\"_median\"]\n            val[var] = val[var].fillna(median)\n    else:\n        val[var] = val[var].fillna(median)\n    train.drop([var+\"_mean\",var+\"_median\"], axis=1, inplace=True)","07122126":"impute_na_numeric(train,val,\"Age\")","4b71d688":"impute_na_numeric(train,val,\"Fare\")","5e633c58":"train[\"Embarked\"].mode().values[0]","0e425929":"def impute_na_non_numeric(train,val,var):\n    mode = train[var].mode().values[0]\n    train[var] = train[var].fillna(mode)\n    val[var] = val[var].fillna(mode)","97edbc49":"impute_na_non_numeric(train,val,\"Embarked\")","30c6d245":"def impute_na_max_missing(train,val,var,prefix):\n    train[prefix+\"_\"+var] = np.where(train[var].isna(),0,1)\n    train.drop([var],axis=1,inplace=True)\n    val[prefix+\"_\"+var] = np.where(val[var].isna(),0,1)\n    val.drop([var],axis=1,inplace=True)","7f9a35ff":"impute_na_max_missing(train,val,\"Cabin\",\"had\")","00ac44bc":"train.head()","22e8de6d":"train[\"Family_Size\"] = train[\"SibSp\"] + train[\"Parch\"]\nval[\"Family_Size\"] = val[\"SibSp\"] + val[\"Parch\"]","396ea5c2":"train[\"Salutation\"] = train[\"Name\"].map(lambda x: x.split(',')[1].split()[0])","8cb68bc4":"train[\"Salutation\"].unique()","2931ae7f":"val[\"Salutation\"] = val[\"Name\"].map(lambda x: x.split(',')[1].split()[0])","4f56e74d":"val[\"Salutation\"].unique()","51121bd2":"val[val[\"Salutation\"] == \"Dona.\"]","ab33f2b1":"def transform_with_target_probs(train,val,var,target):\n    var_dict = train.groupby([var])[target].mean().to_dict()\n    train[var] = train[var].map(var_dict)\n    val[var] = val[var].map(var_dict)","b0266d48":"transform_with_target_probs(train,val,\"Pclass\",\"Survived\")","de5e60ed":"transform_with_target_probs(train,val,\"Sex\",\"Survived\")","8349882d":"transform_with_target_probs(train,val,\"Embarked\",\"Survived\")","80977776":"train[\"Salutation\"] = train[\"Salutation\"].apply(lambda x: x.split('.')[0])\nval[\"Salutation\"] = val[\"Salutation\"].apply(lambda x: x.split('.')[0])","dcca687e":"def get_salutation_map(df,var,rare):\n    sal_dict = {}\n    for sal, count in df[var].value_counts().to_dict().items():\n        count = int(count)\n        if count < 10:\n            sal_dict[sal] = rare\n        else:\n            sal_dict[sal] = sal\n    return sal_dict","5415ce93":"transform_with_target_probs(train,val,\"Salutation\",\"Survived\")","43b245f5":"# Explore Age distibution \ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0)], color=\"Red\", shade = True)\ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1)], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","229d57e5":"# Explore Age distribution \ng = sns.distplot(train[\"Age\"], color=\"m\", label=\"Skewness : %.2f\"%(train[\"Age\"].skew()))\ng = g.legend(loc=\"best\")","dff4ef33":"train[\"Fare\"].describe()","9986679a":"# Explore Fare distribution \ng = sns.distplot(train[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(train[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","fd484c29":"import warnings\nwarnings.filterwarnings('ignore')","3a1245b8":"# Apply log to Fare to reduce skewness distribution\ntrain[\"Fare\"] = train[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","9353f657":"g = sns.factorplot(x=\"Survived\", y = \"Age\", hue = \"had_Cabin\", data = train, kind=\"violin\")","be103deb":"train = pd.get_dummies(train, columns=[\"had_Cabin\"], drop_first=True)\nval = pd.get_dummies(val, columns=[\"had_Cabin\"], drop_first=True)","5ba6d7ea":"drop_cols = ['PassengerId', 'Name', 'SibSp','Parch', 'Ticket']","9385a811":"train.drop(drop_cols,axis=1).drop([\"Survived\"],axis=1).values","bf99a775":"train.drop(drop_cols,axis=1).drop([\"Survived\"],axis=1).columns","616f0324":"X = train.drop(drop_cols,axis=1).drop([\"Survived\"],axis=1).values\ny = train[\"Survived\"].values","e98efa3b":"val[\"Salutation\"] = val[\"Salutation\"].fillna(val[\"Salutation\"].mode().values[0])\nval_test = val.drop(drop_cols,axis=1).values","27fc5d59":"from sklearn.model_selection import train_test_split","b18837d7":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=101)","50b25b3a":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()","9d174a08":"mms.fit(X_train)","86801c87":"X_train_mms = mms.transform(X_train)","9109c1dc":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()","f6923175":"ss.fit(X_train)","d2bd5eb4":"X_train_ss = ss.transform(X_train)","ca082e8a":"# For Age\n\nsns.jointplot(X_train[:,2], X_train_mms[:,2], kind='kde')","114c8384":"# For Age\n\nsns.jointplot(X_train[:,2], X_train_ss[:,2], kind='kde')","bb565301":"# For Fare\n\nsns.jointplot(X_train[:,3], X_train_mms[:,3], kind='kde')","5c357b1b":"# For Fare\n\nsns.jointplot(X_train[:,3], X_train_ss[:,3], kind='kde')","345ca8f2":"X_test_ss = ss.transform(X_test)","400d2da2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,VotingClassifier","048acacc":"from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score","df7abf78":"classification_models = ['LogisticRegression',\n                         'SVC',\n                         'DecisionTreeClassifier',\n                         'RandomForestClassifier',\n                         'AdaBoostClassifier']","7d5771d9":"cm = []\nacc = []\nprec = []\nrec = []\nf1 = []\nmodels = []\nestimators = []","aad6b7a0":"for classfication_model in classification_models:\n    \n    model = eval(classfication_model)()\n    \n    model.fit(X_train_ss,y_train)\n    y_pred = model.predict(X_test_ss)\n    \n    models.append(type(model).__name__)\n    estimators.append((type(model).__name__,model))\n    cm.append(confusion_matrix(y_test,y_pred))\n    acc.append(accuracy_score(y_test,y_pred))\n    prec.append(precision_score(y_test,y_pred))\n    rec.append(recall_score(y_test,y_pred))\n    f1.append(f1_score(y_test,y_pred))","97a7cc45":"vc = VotingClassifier(estimators)\nvc.fit(X_train_ss,y_train)","a1c9e277":"y_pred = vc.predict(X_test_ss)\n    \nmodels.append(type(vc).__name__)\n\ncm.append(confusion_matrix(y_test,y_pred))\nacc.append(accuracy_score(y_test,y_pred))\nprec.append(precision_score(y_test,y_pred))\nrec.append(recall_score(y_test,y_pred))\nf1.append(f1_score(y_test,y_pred))","1155642c":"model_dict = {\"Models\":models,\n             \"CM\":cm,\n             \"Accuracy\":acc,\n             \"Precision\":prec,\n             \"Recall\":rec,\n             \"f1_score\":f1}","34a734e0":"model_df = pd.DataFrame(model_dict)\nmodel_df","e223f6a4":"model_df.sort_values(by=['Accuracy','f1_score','Recall','Precision'],ascending=False,inplace=True)\nmodel_df","6293a413":"val_test = ss.transform(val_test)","054fbf0c":"y_pred_sub = vc.predict(val_test)","f3451a5f":"sub_df = pd.concat([val['PassengerId'],\n                    pd.DataFrame(y_pred_sub,columns=[\"Survived\"])],\n                   axis=1)\nsub_df.head()","7cae53e8":"sub_df.to_csv(\"Stacked_Ensemble_Baseline_Submission.csv\", index=False)","7a496b83":"model_param_grid = {}","19d5de54":"model_param_grid['LogisticRegression'] = {'penalty' : ['l1', 'l2'],\n                                          'C' : np.logspace(0, 4, 10)}","dad85a0a":"model_param_grid['SVC'] = [{'kernel': ['rbf'], \n                            'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n                            'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n                           {'kernel': ['sigmoid'],\n                            'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n                            'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n                           {'kernel': ['linear'], \n                            'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n                           {'kernel': ['poly'], \n                            'degree' : [0, 1, 2, 3, 4, 5, 6]}\n                          ]","342f51e8":"model_param_grid['DecisionTreeClassifier'] = {'criterion' : [\"gini\",\"entropy\"],\n                                              'max_features': ['auto', 'sqrt', 'log2'],\n                                              'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n                                              'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11]}","6c5270b2":"model_param_grid['RandomForestClassifier'] = {'n_estimators' : [25,50,75,100],\n                                              'criterion' : [\"gini\",\"entropy\"],\n                                              'max_features': ['auto', 'sqrt', 'log2'],\n                                              'class_weight' : [\"balanced\", \"balanced_subsample\"]}","58d0985c":"model_param_grid['AdaBoostClassifier'] = {'n_estimators' : [25,50,75,100],\n                                          'learning_rate' : [0.001,0.01,0.05,0.1,1,10],\n                                          'algorithm' : ['SAMME', 'SAMME.R']}","23136178":"from sklearn.model_selection import GridSearchCV\ndef tune_parameters(model_name,model,params,cv,scorer,X,y):\n    best_model = GridSearchCV(estimator = model,\n                              param_grid = params,\n                              scoring = scorer,\n                              cv = cv,\n                              n_jobs = -1).fit(X, y)\n    print(\"Tuning Results for \", model_name)\n    print(\"Best Score Achieved: \",best_model.best_score_)\n    print(\"Best Parameters Used: \",best_model.best_params_)\n    return best_model","03231b58":"from sklearn.metrics import make_scorer\n\n# Define scorer\ndef f1_metric(y_test, y_pred):\n    score = f1_score(y_test, y_pred)\n    return score","4f7f9548":"# Scorer function would try to maximize calculated metric\nf1_scorer = make_scorer(f1_metric,greater_is_better=True)","49ff9011":"best_estimators = []","ad92047a":"for m_name, m_obj in estimators:\n    best_estimators.append((m_name,tune_parameters(m_name,\n                                                   m_obj,\n                                                   model_param_grid[m_name],\n                                                   10,\n                                                   f1_scorer,\n                                                   X_train_ss,\n                                                   y_train)))","6dcfd06f":"tuned_estimators = []","f7490c96":"tuned_lr = LogisticRegression(C=2.7825594022071245, \n                              penalty = 'l1')\ntuned_lr.fit(X_train_ss,y_train)\ntuned_estimators.append((\"LogisticRegression\",tuned_lr))","9889e217":"tuned_svc = SVC(C = 10, gamma = 0.01, kernel = 'rbf', probability=True)\ntuned_svc.fit(X_train_ss,y_train)\ntuned_estimators.append((\"SVC\",tuned_svc))","e1e3e117":"tuned_dt = DecisionTreeClassifier(criterion = 'entropy', \n                                  max_features = 'log2', \n                                  min_samples_leaf = 5, \n                                  min_samples_split = 11)\ntuned_dt.fit(X_train_ss,y_train)\ntuned_estimators.append((\"DecisionTreeClassifier\",tuned_dt))","b11984d1":"tuned_rf = RandomForestClassifier(class_weight = 'balanced_subsample', \n                                  criterion = 'gini', \n                                  max_features = 'sqrt', \n                                  n_estimators = 100)\ntuned_rf.fit(X_train_ss,y_train)\ntuned_estimators.append((\"RandomForestClassifier\",tuned_rf))","93f9f04e":"tuned_adb = AdaBoostClassifier(algorithm = 'SAMME', \n                                  learning_rate = 0.1, \n                                  n_estimators = 75)\ntuned_adb.fit(X_train_ss,y_train)\ntuned_estimators.append((\"AdaBoostClassifier\",tuned_adb))","9a8afcfa":"tuned_vc = VotingClassifier(tuned_estimators)\ntuned_vc.fit(X_train_ss,y_train)","23b0636a":"y_pred_tuned_sub = tuned_vc.predict(val_test)\ntuned_sub_df = pd.concat([val['PassengerId'],\n                          pd.DataFrame(y_pred_tuned_sub,columns=[\"Survived\"])],\n                         axis=1)\ntuned_sub_df.head()","5687c7b1":"tuned_sub_df.to_csv(\"Stacked_Ensemble_Tuned_Submission.csv\", index=False)","a86aeb28":"**** Step By Step Data Preparation \n1) Find if any null coloumns are present in the data ","9fd9648b":"##### Achieved Kaggle Score = 0.72248","390858cd":"** Missing Values imputed **","8b1c005a":"#### Run iterations for all the trained baseline models","31b43d6d":"#### Check Distribution after Scaling","5680d3ef":"### Stacking Ensemble","6646f380":"### Feature Scaling","ee184fd9":"**Which features are categorical?**\n\n* These values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\n\n* Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.","efb70553":"**Which features are numerical?**\n\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\n\n1. Continous: Age, Fare. Discrete: SibSp, Parch.","03a459a5":"### Scale Test file data","6d769e8c":"#### Outlier Removal","677a7acf":"**Analyze by visualizing data**\n* Now we can continue confirming some of our assumptions using visualizations for analyzing the data.\n* \n* Correlating numerical features\n* Let us start by understanding correlations between numerical features and our solution goal (Survived).\n* \n* A histogram chart is useful for analyzing continous numerical variables like Age where banding or ranges will help identify useful patterns. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. This helps us answer questions relating to specific bands (Did infants have better survival rate?)\n* \n* Note that x-axis in historgram visualizations represents the count of samples or passengers.\n* \n**Observations.**\n\n* Infants (Age <=4) had high survival rate.\n* Oldest passengers (Age = 80) survived.\n* Large number of 15-25 year olds did not survive.\n* Most passengers are in 15-35 age range.\n* Decisions.\n\n**This simple analysis confirms our assumptions as decisions for subsequent workflow stages.**\n\n* We should consider Age (our assumption classifying #2) in our model training.\n* Complete the Age feature for null values (completing #1).\n* We should band age groups (creating #3).","1ff9506b":"**Analyze by pivoting features**\n* To confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.\n* Pclass We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying #3). We decide to include this feature in our model.\n* Sex We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying #1).\n* SibSp and Parch These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1).","f6cd149d":"**Which features are mixed data types?**\n\n* Numerical, alphanumeric data within same feature. These are candidates for correcting goal.\n\n* Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanumeric.\n* **Which features may contain errors or typos?**\n\n* This is harder to review for a large dataset, however reviewing a few samples from a smaller dataset may just tell us outright, which features may require correcting.\n\n> Name feature may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names.","4105e05f":"### Explore Numeric Data","13b07a61":"#### Standard Scaling","56935ba7":"#### Define custom Scorer function","cde109c6":"#### MinMaxScaling","f3a23d3d":"**Explore the Feature Engineering **\n1. We can create one new coloumn with name \"Salutation\" from \"Name\" coloumn. Where all the names in \"Name\" column are unique but still we can apply feature engineering steps for this. Observe the coloumn and we can see that some of them are Mr. , Mrs. , Dr. With feature engineering we can extract these and make the prediction of survival on the basis of this.","2c4911b8":"**Combination of Two coloumns**","151d3974":"#### Function to perform Grid Search with Cross Validation","13194fdc":"****Read the data through csv file :- \n\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.","7630c34a":"### Hyper parameter Tuning","970cd565":"**** As we get Null value columns in the data \n Folllow the procedure step by step \n1.  1) For continous variable , calculate the mean and median for inittial step of data preparation","06bc763e":"##### Titanic Solution and approach\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\n\nStep By Step Processing of data :-\n\n1) Understanding the Problem\n\n2) Acquire training and testing dataI\n3) Data preparation \n4) Identify the patterne and explore the data\n5) Model, predict and solve the problem.\n6) Visualize, report, and present the problem solving steps and final solution\n7) Supply or submit the results."}}