{"cell_type":{"211bf30b":"code","29e786c7":"code","2563456f":"code","33a11313":"code","5e7294e9":"code","8ebfa6e3":"code","7899dd6d":"code","7de10a02":"code","8b142e8f":"code","71092dda":"code","154c48af":"code","2151d1db":"code","b41f2a29":"code","16697d9a":"code","98281f36":"code","753b0163":"code","b19ee28b":"code","576fa87f":"code","a63a59d0":"code","d4d6b48e":"code","02fbb83b":"code","1c007498":"code","c3082ff4":"code","9c34dfe7":"code","0b6cedb8":"code","118f7b3f":"code","4f5fbb1e":"code","fd535370":"code","debe4f58":"code","307397f4":"code","f5a5aecb":"code","307f759c":"code","260736f5":"code","8403000c":"code","ecd8938d":"code","fb4269b7":"code","c2eb04b3":"code","c1887b44":"code","b9c6bb3c":"code","2f007e93":"code","db238228":"markdown","73ee9282":"markdown","3683c766":"markdown","0ecc74e0":"markdown","0b5bcafa":"markdown","da1b45f3":"markdown","c8a67038":"markdown","419c2173":"markdown","6980e392":"markdown","ad2617c1":"markdown","9bea7816":"markdown","5bf5808b":"markdown","eaec21fb":"markdown","390c4f10":"markdown","20cb7666":"markdown","037bce9d":"markdown","da5a9acb":"markdown","aa433ce5":"markdown","77d26c9d":"markdown","179bee10":"markdown"},"source":{"211bf30b":"import gc\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport lightgbm as lgb\n\nfrom datetime import time\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score, make_scorer\n\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode()\nimport plotly.graph_objs as go\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance","29e786c7":"%time\n# Enable automatic garbage collection.\ngc.enable()\n#For Repreduciton the \nseed = 127\nnp.random.seed(seed)","2563456f":"%time\n# train_df = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/train.csv\",nrows=3000)\n# test_df = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/test.csv\",nrows=3000) \n\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\") ","33a11313":"# Show all the columns in train set\ntrain_df.shape, train_df.columns","5e7294e9":"# All features in Test set and sha\ntest_df.shape ,test_df.columns","8ebfa6e3":"train_df.shape, test_df.shape","7899dd6d":"pd.DataFrame(train_df.dtypes).transpose()","7de10a02":"pd.DataFrame(test_df.dtypes).transpose()","8b142e8f":"def missingcount(count, samples):\n        \"\"\"\n        :rtype: object\n        \"\"\" \n        return (int(count) \/samples) * 100\n    \ndef missingPercent(df):\n        \"\"\"\n        This Mehtod will return  mising value in respect to perticular feature\n        :rtype: object|\n        \"\"\" \n        count_df = pd.DataFrame(df.isnull().sum(axis=0).reset_index())\n        count_df.columns = ['FEATURE', 'MISSING_COUNT']\n        count_df[\"MISSING_%\"] = count_df[\"MISSING_COUNT\"].map(lambda x: missingcount(x, df.shape[0] ))\n        return count_df.set_index('FEATURE').T","71092dda":"# Training set\nmissingPercent(train_df) ","154c48af":"# Testing \nmissingPercent(test_df)","2151d1db":"train_df.describe()","b41f2a29":"train_df.describe()","16697d9a":"# Calculate percentage classe\ndef sample_percentage_per_class(label, values, title=\"Target Classes Percentage\"):\n    trace=go.Pie(labels=label, values=values) \n    layout=go.Layout(title=title, height=600, margin=go.Margin(l=0, r=200, b=100, t=100, pad=4))   # Margins - Left, Right, Top Bottom, Padding)\n    fig=go.Figure(data=[trace], layout=layout)\n    iplot(fig)","98281f36":"labels=[str(x) for x in list(train_df.target.unique())]\nvalues=[(len(train_df[train_df.target == 0])\/len(train_df))*100,(len(train_df[train_df.target > 0])\/len(train_df))*100]\nsample_percentage_per_class(labels,values)","753b0163":"sns.countplot(x='target',data=train_df)","b19ee28b":"train_df.var_0.hist(bins=20)","576fa87f":"train_df.var_1.hist()","a63a59d0":"# Compute the Correlation matrix\ncorr=train_df.iloc[:,:20].corr()","d4d6b48e":"# Set the matplotlib figure \nfig, ax=plt.subplots(figsize=(20,20))\n# Generate a custom diverging colormap\nCmap=sns.diverging_palette(220,10,as_cmap=True)\n\n# Draw the heatmap\n_= sns.heatmap(corr,  ax=ax, annot=True,   vmin=-1, linewidths=0.2)\n","02fbb83b":"rf = RandomForestClassifier( max_depth=10, verbose=3, n_estimators=20)\nparameters = {'n_estimators': [120, 100], 'max_depth':[3,5,]}\ngrid = GridSearchCV(rf, parameters, cv=3, n_jobs=-1, verbose=3, scoring=make_scorer(roc_auc_score))","1c007498":"df_train_FS=train_df\ngrid.fit(df_train_FS.drop([\"target\",'ID_code'], axis=1).values, df_train_FS.target.values)","c3082ff4":"grid.best_score_","9c34dfe7":"n_top=10\nimportance_feature_df=grid.best_estimator_.feature_importances_\nidx=np.argsort(importance_feature_df)[::-1][0:n_top]\nfeatures_name=df_train_FS.drop([\"target\",'ID_code'], axis=1).columns.values\n\nplt.figure(figsize=(20,5))\nsns.barplot(x=features_name[idx], y=importance_feature_df[idx],n_boot=10)\n# plt.tab","0b6cedb8":"X=df_train_FS.drop([\"target\",'ID_code'], axis=1)\nY=df_train_FS.target","118f7b3f":"x_train,x_test,y_train, y_test = train_test_split(X, Y, random_state=41)\nrfmodel=RandomForestClassifier( max_depth=10, n_estimators=20,verbose=0).fit(x_train, y_train)","4f5fbb1e":"perm = PermutationImportance(rfmodel, random_state=41, scoring=make_scorer(roc_auc_score)).fit(x_test,y_test)","fd535370":"eli5.show_weights(perm, feature_names = x_test.columns.tolist(), top=80)","debe4f58":"# ## check unique values in both train and test dataset feature\n# train_df.v","307397f4":"y = train_df.target\ntrain_df= train_df.iloc[:, 2:]","f5a5aecb":"# Adding list of Ids to a new dataframe  for submission \nids = test_df.ID_code.to_frame()\n# Removing Id_code feature from Test data\ntest_df.drop('ID_code', inplace=True, axis=1)","307f759c":"%%time\nidx = features = train_df.columns.values[2:202]\nfor df in [test_df, train_df]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)","260736f5":"test_df.shape, train_df.shape","8403000c":"# Normalization of features\nsc = StandardScaler(seed, with_std=True)\nsc.fit(train_df)\ntrain_df= sc.transform(train_df) \ntest_df = sc.transform(test_df)","ecd8938d":"train_df.shape,test_df.shape","fb4269b7":"# List fo HyperParam for LightGBM \nparams = {\n    'tree_learner': 'serial',\n    'objective': 'binary',\n    'learning_rate': 0.00742,\n    'num_leaves': 3,  # Lower value for better accuracy\n    'bagging_freq': 5,\n    'bagging_fraction': 0.33,\n    'boost_from_average':True,\n    'boost': 'gbrt',\n    'feature_fraction': 0.04,    \n    'max_depth': -1,\n    'metric':'auc',\n    'min_data_in_leaf': 85,\n    'min_sum_hessian_in_leaf': 0.80,\n    'num_threads': 16,\n    'verbosity': 1\n}","c2eb04b3":"num_round = 250000\n# Cross-validation\nfolds = StratifiedKFold(n_splits=7, shuffle=True, random_state=seed)\nlstCV=folds.split(train_df, y)","c1887b44":"# Train and Test Predication Vector\ntrain_pred = np.zeros(len(train_df))\ntest_pred = np.zeros(len(test_df))","b9c6bb3c":"%time\nfor fold_, (trn_idx, val_idx) in enumerate(lstCV):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train_df[trn_idx], label=y[trn_idx])\n    val_data = lgb.Dataset(train_df[val_idx], label=y[val_idx])\n    clf = lgb.train(params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3500)\n    train_pred[val_idx] = clf.predict(train_df[val_idx], num_iteration=clf.best_iteration)\n    test_pred += clf.predict(test_df, num_iteration=clf.best_iteration) \/ folds.n_splits\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(y, train_pred)))","2f007e93":"sub_df = pd.DataFrame({\"ID_code\": ids[\"ID_code\"]})\nsub_df[\"target\"] = test_pred\nsub_df.to_csv(\"submission.csv\", index=False)","db238228":"#### Data type\nWe will check the datatype of all the feaures in both train and test. ","73ee9282":"Pandas describe funcation will help in getting general statistics about the dataset. Like Min, max STD and count etc.","3683c766":"- Both train and Test dataset have same number of samples 20000, but both are having different number features. Because train dataset has target feature.\n\n\n> Train Set:\n    * ID_code: Identifier for the transaction or customer \n    * Target: Label feature for training \n    * 202: are training features for model like var_1.... var_199\n>Test Set:\n    * ID_code: Identifier for the transaction or customer \n    * 202: are testing  features same as training features for model like var_1.... var_199\n    \n- Dataset shape\/dimission\n- Dataset is imblanced as class 0 has aprox 175000 sample and class 1 has near about 25000 sample. ","0ecc74e0":"As per the above analysis, Train and Test don't have any missing value. So we don't need to perform any imputation for handling the  missing values.","0b5bcafa":"### Histograms\nA histogram is a plot that lets you discover, and show, the underlying frequency distribution.\n\n[Histograms](https:\/\/statistics.laerd.com\/statistical-guides\/understanding-histograms.php)","da1b45f3":"This is clearly showing that we are working or we have highly imblanced dataset.","c8a67038":" >** Showing top 10 features **","419c2173":"### Load Dataset\n\nIn this section, We will load all the train and test dataset. Which we will use for our EDA and model training.","6980e392":"#### Check unique values of feature \n- ","ad2617c1":"### Feature Selection \nFeature selection is the process of selection the features, which are more relevent for our model and not carring unnecessary noise to final model. Also it can help in giving idea about the new feature(Drived Features).\nI will use RandomForest for it, it will help us to detect the features importance.","9bea7816":"### Feature Engineering \n\nIn feature Engineering, We will create new feature and may be we can also remove some feature. ","5bf5808b":"### Missing Values\n> Analysing the missing value of features. So we can do imputaiton if any missing value present in any feature. \n> As data set is imblance, we will use cross-validation instead of traditional train test split. ","eaec21fb":"### Problem Statement\n---\n>  This is a binary classification competition, in this competition, we need to predict that the customer will do the transaction or not. The dataset that is shared by the organizer is having numeric data fields.","390c4f10":"# Customer Transaction Prediction and EDA\n---\n\n ![download.png](attachment:download.png)\n\nSantander published this competition to predict that the particular customer will do the transaction or not. \n\n- [Problem Statement](intLink)\n- [Import libraries](## Import libraries)\n- [Load Dataset](intLink)\n- [EDA](intLink)\n    - [Data Visualization](intLink)\n    - [Data Prepration](intLink)\n    - [Missing values](intLink)\n    - [Feature Selection](intLink)\n    - [Feature Correlation](intLink)\n    - [Data Prepration](intLink)\n- [Feature Engineering](intLink)\n- [Model Training ](intLink)\n- [Submission](intLink)\n- [References](intLink)\n\n","20cb7666":"### Data Visualization \nVisualization will help us to understand distribution. It's most important part of EDA. Every thing in the world, we can understand better if we have Visualization for it.  ","037bce9d":"## Exploratry Data Analysis(EDA)\nEDA is the most important component of data scinece  project workflow. Before  going to model, we need to understand the data from different angles.\n   \n    1- Type of feature\n    2- Data types of featues \n    3- Missing Values.","da5a9acb":"### Import libraries\n---\n\nWe are importing all the libraries, which will be used in our model training.  ","aa433ce5":"### Calculate which features are important for price prediction\nCalcualte importance and show the importance of feature. ","77d26c9d":"This is giving more insight of data. \n    -  Standard Deviation is relatively large for both train and test set.\n    -  Max, Min and STD are quite close in both train and test data set.\n    -  Min is distibuted over the large number of range.\n","179bee10":"### Data Relationships\nVisualizaiton of data relationship, will help us to understand the degree of correlation between features and the dependencies. \nWith the help of scatterplot and heatmap we can show the relationship of features."}}