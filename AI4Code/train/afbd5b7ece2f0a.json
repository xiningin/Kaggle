{"cell_type":{"b59c21dc":"code","3597fe76":"code","93c681a9":"code","505a9fb7":"code","77ab1c69":"code","44d4f3ee":"code","af2b646d":"code","33e80be9":"code","713400ad":"markdown"},"source":{"b59c21dc":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport librosa\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(r\"..\/input\/spoken-digit-dataset\"))\n\n# Any results you write to the current directory are saved as output.","3597fe76":"from keras.utils import to_categorical\nfrom tqdm import tqdm\n\ndef wav2mfcc(file_path, augment = False, max_pad_len=11):\n    wave, sr = librosa.load(file_path, mono=True, sr=8000, duration = 1.024)\n    \n    if augment == True:\n        bins_per_octave = 12\n        pitch_pm = 4\n        pitch_change =  pitch_pm * 2*(np.random.uniform())   \n        wave = librosa.effects.pitch_shift(wave, \n                                          8000, n_steps=pitch_change, \n                                          bins_per_octave=bins_per_octave)\n        \n        speed_change = np.random.uniform(low=0.9,high=1.1)\n        wave = librosa.effects.time_stretch(wave, speed_change)\n        wave = wave[:8192]\n\n    duration = wave.shape[0]\/sr\n    speed_change = 2.0* duration\/1.024\n    wave = librosa.effects.time_stretch(wave, speed_change)\n    wave = wave[:4096]\n    \n    wave = librosa.util.normalize(wave)\n    mfcc = librosa.feature.mfcc(wave, sr=sr, n_mfcc=40, hop_length=int(0.048*sr), n_fft=int(0.096*sr))\n    mfcc -= (np.mean(mfcc, axis=0) + 1e-8)\n    #print(\"shape=\",mfcc.shape[1], wave.shape[0])\n    pad_width = max_pad_len - mfcc.shape[1]\n    mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n    #mfcc = mfcc[2:24,:]\n    return mfcc, duration, sr\n\ndef get_data(dir = '', augment= False):\n    labels = []\n    mfccs = []\n    durations = []\n    sampling_rates = []\n    filenames = []\n\n    for f in tqdm(os.listdir(dir)):\n        if f.endswith('.wav'):\n            mfcc, duration, sr = wav2mfcc(dir + \"\/\" + f, augment)\n            mfccs.append(mfcc)\n            durations.append(duration)\n            sampling_rates.append(sr)\n            # List of labels\n            label = f.split('_')[0]\n            labels.append(label)\n            filenames.append(dir + \"\/\" + f)\n    return filenames, np.asarray(mfccs), np.asarray(durations), np.asarray(sampling_rates), to_categorical(labels), labels","93c681a9":"filenames, mfccs, durations, sampling_rates, labels, cls_true = get_data('..\/input\/spoken-digit-dataset\/free-spoken-digit-dataset-master\/free-spoken-digit-dataset-master\/recordings')","505a9fb7":"import matplotlib.pyplot as plt\nimport librosa.display\n\ndef plot_images(images, cls_true, cls_pred=None):\n    assert len(images) == len(cls_true) == 9\n    \n    # Create figure with 3x3 sub-plots.\n    fig, axes = plt.subplots(3, 3, figsize=(15,15))\n    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n    \n    max = np.max(images)\n    min = np.min(images)\n\n    for i, ax in enumerate(axes.flat):\n        # Plot image.\n        #ax.imshow(images[i].reshape(img_shape), cmap='binary')\n        im = librosa.display.specshow(images[i], ax=ax, vmin=min, vmax=max)\n\n        # Show true and predicted classes.\n        if cls_pred is None:\n            xlabel = \"True: {0}\".format(cls_true[i])\n        else:\n            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n\n        ax.set_xlabel(xlabel)\n    # Ensure the plot is shown correctly with multiple plots\n    # in a single Notebook cell.\n    plt.show()","77ab1c69":"plot_images(mfccs[100:109], cls_true[100:109])","44d4f3ee":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n\n#from ann_visualizer.visualize import ann_viz\n\ndef get_cnn_model(input_shape, num_classes):\n    model = Sequential()\n\n    model.add(Conv2D(32, kernel_size=(4, 4), activation='relu', input_shape=input_shape))\n    model.add(BatchNormalization())\n\n    model.add(Conv2D(48, kernel_size=(3, 3), activation='relu'))\n    model.add(BatchNormalization())\n\n    model.add(Conv2D(120, kernel_size=(3, 3), activation='relu'))\n    model.add(BatchNormalization())\n\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n\n    model.add(Dense(128, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n    model.add(Dense(64, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n\n    return model","af2b646d":"from sklearn.model_selection import train_test_split\n\ndef get_all():\n    filenames, mfccs, durations, sampling_rates, labels, cls_true = get_data('..\/input\/spoken-digit-dataset\/free-spoken-digit-dataset-master\/free-spoken-digit-dataset-master\/recordings', augment = False)\n    \n    filenames_a, mfccs_a, durations_a, sampling_rates_a, labels_a, cls_true_a = get_data('..\/input\/spoken-digit-dataset\/free-spoken-digit-dataset-master\/free-spoken-digit-dataset-master\/recordings', augment = True)\n\n    mfccs = np.append(mfccs, mfccs_a, axis=0)\n    labels = np.append(labels, labels_a, axis =0)\n    \n    dim_1 = mfccs.shape[1]\n    dim_2 = mfccs.shape[2]\n    channels = 1\n    classes = 10\n    \n    print(\"sampling rate (max) = \", np.max(sampling_rates))\n    print(\"sampling rate (min) = \", np.min(sampling_rates))\n    print(\"duration (max) = \", np.max(durations))\n    print(\"duration (avg) = \", np.average(durations))\n    print(\"duration (min) = \", np.min(durations))\n    print(\"mffc matrix = \", mfccs.shape)\n\n    X = mfccs\n    X = X.reshape((mfccs.shape[0], dim_1, dim_2, channels))\n    y = labels\n\n    input_shape = (dim_1, dim_2, channels)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n    model = get_cnn_model(input_shape, classes)\n\n    return X_train, X_test, y_train, y_test, model","33e80be9":"X_train, X_test, y_train, y_test, cnn_model = get_all()\n\nprint(cnn_model.summary())\n\ncnn_model.fit(X_train, y_train, batch_size=64, epochs=500, verbose=1, validation_split=0.1)","713400ad":"**So, the last accuracy value is %97.08**"}}