{"cell_type":{"941173b4":"code","4ca33d96":"code","d2c2b428":"code","5de4cb10":"code","43919ae1":"code","2491b697":"code","efcfc903":"code","3db1a0ea":"code","bbccec70":"code","33c0c85f":"code","7206cbc4":"code","e1b544fd":"code","db41500c":"code","aedbbc56":"code","c9f1c0e9":"code","02978821":"code","1152b988":"code","7f0fb8e6":"code","191ee3b5":"code","2026b717":"code","716f67d8":"code","13d60a51":"code","409271b0":"code","aa603b0d":"code","1c0fa51c":"code","bdba03d8":"code","4cad78bf":"code","235ce228":"code","a1d75ab9":"code","0b31271a":"code","c94ee56b":"code","22eb222a":"code","43578bb4":"code","f5a23887":"code","6e25309e":"code","093ad2df":"code","e1321fef":"code","25b01d98":"code","7cf7e198":"markdown","a4d14774":"markdown","f9774136":"markdown","2f73ebc8":"markdown","8f424cfb":"markdown","68e62cd7":"markdown","11a2eab6":"markdown","33c5da1c":"markdown","f80731f0":"markdown","796a31a2":"markdown","34b6aa8b":"markdown","0278a9ba":"markdown","de92c025":"markdown","3e4a205d":"markdown","5b3c9903":"markdown","f717ed9f":"markdown"},"source":{"941173b4":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4ca33d96":"train = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv', index_col='id')","d2c2b428":"train.shape","5de4cb10":"train.head().T","43919ae1":"test = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv', index_col='id')","2491b697":"test.shape","efcfc903":"fa_features = [\n    'bin_0', 'bin_1',\n    'nom_5', 'nom_6'\n]\n\ntrain_fa = train[fa_features].copy()\ntest_fa = test[fa_features].copy()","3db1a0ea":"def summary(df):\n    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name', 'dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n    return summary\n\n\nsummary(train)","bbccec70":"ohe_features = [\n    'bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4',\n    'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4',\n    'ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4',\n    'day', 'month'\n]\n\nle_features = list(set(test.columns) - set(ohe_features))","33c0c85f":"train_part = len(train)\ndf = pd.get_dummies(pd.concat([train, test], axis=0), columns=ohe_features)\ntrain = df[:train_part]\ntest = df[train_part:].drop('target', axis=1)\ndel df","7206cbc4":"from sklearn.preprocessing import LabelEncoder\n\n\ndef encode_categorial_features_fit(df, columns_to_encode):\n    encoders = {}\n    for c in columns_to_encode:\n        if c in df.columns:\n            encoder = LabelEncoder()\n            encoder.fit(df[c].astype(str).values)\n            encoders[c] = encoder\n    return encoders\n\ndef encode_categorial_features_transform(df, encoders):\n    out = pd.DataFrame(index=df.index)\n    for c in encoders.keys():\n        if c in df.columns:\n            out[c] = encoders[c].transform(df[c].astype(str).values)\n    return out\n\n\n# le_features = test.columns\n\ncategorial_features_encoders = encode_categorial_features_fit(\n    pd.concat([train, test], join='outer', sort=False), le_features)","e1b544fd":"temp = encode_categorial_features_transform(train, categorial_features_encoders)\ncolumns_to_drop = list(set(le_features) & set(train.columns))\ntrain = train.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","db41500c":"temp = encode_categorial_features_transform(test, categorial_features_encoders)\ncolumns_to_drop = list(set(le_features) & set(test.columns))\ntest = test.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","aedbbc56":"from category_encoders import TargetEncoder","c9f1c0e9":"te_features = [\n    'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'\n]","02978821":"te = TargetEncoder(cols=te_features, drop_invariant=True, return_df=True, min_samples_leaf=2, smoothing=1.0)\nte.fit(train[te_features], train['target'])","1152b988":"temp = te.transform(train[te_features])\ncolumns_to_drop = list(set(te_features) & set(train.columns))\ntrain = train.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","7f0fb8e6":"temp = te.transform(test[te_features])\ncolumns_to_drop = list(set(te_features) & set(test.columns))\ntest = test.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","191ee3b5":"le_features = fa_features\n\ncategorial_features_encoders = encode_categorial_features_fit(\n    pd.concat([train_fa, test_fa], join='outer', sort=False), le_features)","2026b717":"temp = encode_categorial_features_transform(train_fa, categorial_features_encoders)\ncolumns_to_drop = list(set(le_features) & set(train_fa.columns))\ntrain_fa = train_fa.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","716f67d8":"temp = encode_categorial_features_transform(test_fa, categorial_features_encoders)\ncolumns_to_drop = list(set(le_features) & set(test_fa.columns))\ntest_fa = test_fa.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","13d60a51":"def make_aggregates(df, feature_to_group_by, feature):\n    out = pd.DataFrame(index=df.index)\n    agg = df.groupby([feature_to_group_by])[feature].value_counts(normalize=True)\n    freq = lambda row: agg.loc[row[feature_to_group_by], row[feature]]\n    out[feature + '__' + feature_to_group_by + '_freq'] = df.apply(freq, axis=1)\n    return out\n\n\nfor feature in ['nom_5__bin_0', 'nom_6__bin_1']:\n    feature_1, feature_2 = feature.split('__')\n    print('Add feature:', feature, '\/ aggregates of', feature_2, 'by', feature_1)\n    \n    agg = make_aggregates(train_fa, feature_2, feature_1)\n    train = train.merge(agg, how='left', left_index=True, right_index=True)\n    del agg\n    \n    agg = make_aggregates(test_fa, feature_2, feature_1)\n    test = test.merge(agg, how='left', left_index=True, right_index=True)\n    del agg\n\ndel train_fa\ndel test_fa","409271b0":"# From https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#        else:\n#            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","aa603b0d":"y_train = train['target'].copy()\nx_train = train.drop('target', axis=1)\ndel train\n\nx_test = test.copy()\ndel test","1c0fa51c":"from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom bayes_opt import BayesianOptimization","bdba03d8":"def train_model(num_leaves, min_data_in_leaf, max_depth, bagging_fraction, feature_fraction, lambda_l1, lambda_l2):\n    \n    params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'boosting_type': 'gbdt',\n        'is_unbalance': False,\n        'boost_from_average': True,\n        'num_threads': 4,\n        \n        'num_leaves': int(num_leaves),\n        'min_data_in_leaf': int(min_data_in_leaf),\n        'max_depth': int(max_depth),\n        'bagging_fraction' : bagging_fraction,\n        'feature_fraction' : feature_fraction,\n        'lambda_l1': lambda_l1,\n        'lambda_l2': lambda_l2\n    }\n    \n    scores = []\n    \n    cv = KFold(n_splits=10, random_state=42)\n    for train_idx, valid_idx in cv.split(x_train, y_train):\n        \n        x_train_train = x_train.iloc[train_idx]\n        y_train_train = y_train.iloc[train_idx]\n        x_train_valid = x_train.iloc[valid_idx]\n        y_train_valid = y_train.iloc[valid_idx]\n        \n        lgb_train = lgb.Dataset(data=x_train_train.astype('float32'), label=y_train_train.astype('float32'))\n        lgb_valid = lgb.Dataset(data=x_train_valid.astype('float32'), label=y_train_valid.astype('float32'))\n        \n        lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=100)\n        y = lgb_model.predict(x_train_valid.astype('float32'), num_iteration=lgb_model.best_iteration)\n        \n        score = roc_auc_score(y_train_valid.astype('float32'), y)\n        print('Fold score:', score)\n        scores.append(score)\n    \n    average_score = sum(scores) \/ len(scores)\n    print('Average score:', average_score)\n    return average_score\n\n\nbounds = {\n    'num_leaves': (31, 100),\n    'min_data_in_leaf': (20, 100),\n    'max_depth':(-1, 100),\n    'bagging_fraction' : (0.1, 0.9),\n    'feature_fraction' : (0.1, 0.9),\n    'lambda_l1': (0, 2),\n    'lambda_l2': (0, 2)\n}\n\nbo = BayesianOptimization(train_model, bounds, random_state=42)\nbo.maximize(init_points=20, n_iter=20, acq='ucb', xi=0.0, alpha=1e-6)","4cad78bf":"bo.max","235ce228":"params = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'is_unbalance': False,\n    'boost_from_average': True,\n    'num_threads': 4,\n    \n    'num_iterations': 10000,\n    'learning_rate': 0.006,\n    'early_stopping_round': 100,\n    \n    'num_leaves': int(bo.max['params']['num_leaves']),\n    'min_data_in_leaf': int(bo.max['params']['min_data_in_leaf']),\n    'max_depth': int(bo.max['params']['max_depth']),\n    'bagging_fraction' : bo.max['params']['bagging_fraction'],\n    'feature_fraction' : bo.max['params']['feature_fraction'],\n    'lambda_l1': bo.max['params']['lambda_l1'],\n    'lambda_l2': bo.max['params']['lambda_l2']\n    \n#    'num_leaves': 94,\n#    'min_data_in_leaf': 61,\n#    'max_depth': 31,\n#    'bagging_fraction' : 0.12033530139527615,\n#    'feature_fraction' : 0.18631314159464357,\n#    'lambda_l1': 0.0628583713734685,\n#    'lambda_l2': 1.2728208225275608\n}","a1d75ab9":"n_splits = 10\n\ny = np.zeros(x_test.shape[0])\noof = np.zeros(x_train.shape[0])\nfeature_importances = []\n\ncv = KFold(n_splits=n_splits, random_state=42)\nfor train_idx, valid_idx in cv.split(x_train, y_train):\n    \n    x_train_train = x_train.iloc[train_idx]\n    y_train_train = y_train.iloc[train_idx]\n    x_train_valid = x_train.iloc[valid_idx]\n    y_train_valid = y_train.iloc[valid_idx]\n    \n    lgb_train = lgb.Dataset(data=x_train_train.astype('float32'), label=y_train_train.astype('float32'))\n    lgb_valid = lgb.Dataset(data=x_train_valid.astype('float32'), label=y_train_valid.astype('float32'))\n    \n    lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=100)\n    \n    y_part = lgb_model.predict(x_test.astype('float32'), num_iteration=lgb_model.best_iteration)\n    y += y_part \/ n_splits\n    \n    oof_part = lgb_model.predict(x_train_valid.astype('float32'), num_iteration=lgb_model.best_iteration)\n    oof[valid_idx] = oof_part\n    \n    score = roc_auc_score(y_train_valid.astype('float32'), oof_part)\n    print('Fold score:', score)\n    \n    feature_importances.append(lgb_model.feature_importance())","0b31271a":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfeature_importance_df = pd.concat([\n    pd.Series(x_train.columns),\n    pd.Series(np.mean(feature_importances, axis=0))], axis=1)\nfeature_importance_df.columns = ['featureName', 'importance']\n\ntemp = feature_importance_df.sort_values(by=['importance'], ascending=False)\n\nplt.figure(figsize=(12, 20))\nsns.barplot(x=\"importance\", y=\"featureName\", data=temp)\nplt.show()","c94ee56b":"temp = feature_importance_df.sort_values(by=['importance'], ascending=False).head(15)\nmost_important_features = temp['featureName'].values","22eb222a":"plt.figure(figsize=(20,20))\ncor = x_train[most_important_features].corr()\nsns.heatmap(cor, annot=True, annot_kws={\"size\": 8}, cmap=plt.cm.Reds)\nplt.show()","43578bb4":"# From https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","f5a23887":"classes = pd.Series([0,1])\n\nplot_confusion_matrix(y_train, oof.round(), classes=classes, normalize=True, title='Confusion matrix')\n\nplt.show()","6e25309e":"submission = pd.read_csv('..\/input\/cat-in-the-dat\/sample_submission.csv', index_col='id')\nsubmission['target'] = y\nsubmission.to_csv('lightgbm.csv')","093ad2df":"submission.head()","e1321fef":"oof_df = pd.DataFrame(index=pd.read_csv('..\/input\/cat-in-the-dat\/train.csv', index_col='id').index)\noof_df['oof'] = oof\noof_df.to_csv('oof.csv')","25b01d98":"oof_df.head()","7cf7e198":"### Add feature aggregates","a4d14774":"## Feature correlation map","f9774136":"## Target encoder","2f73ebc8":"## LightGBM","8f424cfb":"## Features","68e62cd7":"## One hot encoder","11a2eab6":"## Label encoder","33c5da1c":"## Feature aggregates","f80731f0":"**Idea:**\n* LabelEncoder for true\/false features\n* OneHotEncoder for features with number of unique values <=30\n* TargetEncoder for features with many unique values\n* LabelEncoder for other features","796a31a2":"## Submit predictions","34b6aa8b":"## Confusion matrix","0278a9ba":"## Free memory","de92c025":"## Feature importance","3e4a205d":"## Save OOF","5b3c9903":"## Extract target variable","f717ed9f":"Save columns that we will use for feature aggregates"}}