{"cell_type":{"c0578687":"code","e5479d74":"code","11aac18d":"code","1b82f254":"code","3fe54ae6":"code","43eec3be":"code","4596339a":"code","301f1326":"code","d326bbb8":"code","8baf7b5b":"code","fad5d92f":"code","828d20ac":"code","30d566e6":"code","f0884a27":"code","a49afc42":"code","ef346d34":"code","180d7166":"code","600df05c":"code","a0d8d83c":"code","249be216":"code","8b3549a2":"code","dda1af8b":"code","9e8281cd":"code","f9dc27cd":"code","3fbc6531":"code","e099867a":"code","1d6dd619":"code","723aa396":"code","30a9ae5e":"code","3061f1d6":"code","6e539b7f":"code","321ff96d":"code","7a9f6923":"code","f6eee172":"code","27a12802":"code","25237dcc":"code","aea08dea":"code","aa8ba866":"code","f593cfb9":"code","79102586":"code","c31996c4":"code","de02be5b":"code","d0b6febd":"code","4eb01911":"code","6865eb8d":"code","5983c0ba":"code","5c9b8d6f":"code","a2a703a8":"code","aee989bb":"code","40459de7":"code","77082aa5":"markdown","cfc231c4":"markdown","94088519":"markdown","dc31bf8f":"markdown","ded2e64b":"markdown","288cbc95":"markdown","449db169":"markdown","6fb88fd3":"markdown","0b091e9e":"markdown","d5481715":"markdown","352f1a74":"markdown","a7acabd0":"markdown","4ab73d17":"markdown","d03f9aa6":"markdown","54d1a0ed":"markdown","c5e88e8d":"markdown","7a7afa99":"markdown","e0e97adf":"markdown","ddcb0428":"markdown","a8bac53f":"markdown","6d3c168f":"markdown","9ffee6c2":"markdown","f8cacec4":"markdown","c4767c66":"markdown","7b21d218":"markdown","8ac175d1":"markdown","04463132":"markdown","34c7f6cd":"markdown","0ac631d0":"markdown","664a56dd":"markdown","68b8e16a":"markdown","738a7490":"markdown","27e85512":"markdown","6c337351":"markdown","d129eaeb":"markdown","b45227f9":"markdown","1ce46a1d":"markdown","d16d7beb":"markdown","23eb23c3":"markdown"},"source":{"c0578687":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport matplotlib\nmatplotlib.use('Agg')\n#matplotlib.style.use('ggplot')\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import preprocessing\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom xgboost import cv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot","e5479d74":"orders = pd.read_csv('..\/input\/instacart-market-basket-analysis\/orders.csv')\nproducts = pd.read_csv('..\/input\/instacart-market-basket-analysis\/products.csv')\norder_products_prior = pd.read_csv('..\/input\/instacart-market-basket-analysis\/order_products__prior.csv')\norder_products_train = pd.read_csv('..\/input\/instacart-market-basket-analysis\/order_products__train.csv')\nproducts = pd.read_csv('..\/input\/instacart-market-basket-analysis\/products.csv')\naisles = pd.read_csv('..\/input\/instacart-market-basket-analysis\/aisles.csv')\ndepartments = pd.read_csv('..\/input\/instacart-market-basket-analysis\/departments.csv')","11aac18d":"orders.head(10)","1b82f254":"order_products_prior.head(10)","3fe54ae6":"products.head(10)","43eec3be":"aisles.head(10)","4596339a":"departments.head(10)","301f1326":"#### Remove triple quotes to trim your dataset and experiment with your data\n### COMMANDS FOR CODING TESTING - Get 10% of users \norders = orders.loc[orders.user_id.isin(orders.user_id.drop_duplicates().sample(frac=0.1, random_state=25))] ","d326bbb8":"#Merge the orders DF with order_products_prior by their order_id, keep only these rows with order_id that they are appear on both DFs\nop = orders.merge(order_products_prior, on='order_id', how='inner')\nop.head(10)","8baf7b5b":"user = op.groupby('user_id')['order_id'].nunique().to_frame('u_total_orders').reset_index()\nuser.head(10)\n","fad5d92f":"# Get the same result using a different aggregation function e.g. max()","828d20ac":"u_reorder = op.groupby('user_id')['reordered'].mean().to_frame('u_reordered_ratio').reset_index()\nu_reorder.head(10)","30d566e6":"user = user.merge(u_reorder, on='user_id', how='left')\nuser.head(15)","f0884a27":"prd = op.groupby('product_id')['product_id'].count().to_frame('p_total_purchases').reset_index()\nprd.head(10)","a49afc42":"#Get the same result using a different column","ef346d34":"p_reord = op.groupby('product_id').filter(lambda x: x.shape[0] >40)\np_reord.head(10)","180d7166":"p_reorder = p_reord.groupby('product_id')['reordered'].mean().to_frame('p_reorder_ratio').reset_index()\np_reorder.head(10)","600df05c":"prd = prd.merge(p_reorder, on='product_id', how='left')\n#delete the reorder DataFrame\ndel p_reorder\nprd.head(10)","a0d8d83c":"prd['p_reorder_ratio'] = prd['p_reorder_ratio'].fillna(value=0)\nprd.head(10)","249be216":"#Get the names of the products\nprd_names=pd.merge(prd, products[['product_id', 'product_name']], on='product_id', how='left')\nprd_names.head(10)","8b3549a2":"prd_names.sort_values('p_reorder_ratio', ascending=False)","dda1af8b":"uxp = op.groupby(['user_id', 'product_id'])['order_id'].count().to_frame('uxp_total_bought').reset_index()\nuxp.head(10)","9e8281cd":"times = op.groupby(['user_id', 'product_id'])[['order_id']].count()\ntimes.columns = ['Times_Bought']\ntimes.head()","f9dc27cd":"total_orders = op.groupby('user_id')['order_number'].max().to_frame('total_orders')\ntotal_orders.head()","3fbc6531":"first_order_no = op.groupby(['user_id', 'product_id'])['order_number'].min().to_frame('first_order_number').reset_index()\nfirst_order_no.head()","e099867a":"span = pd.merge(total_orders, first_order_no, on='user_id', how='right')\nspan['Order_Range'] = span.total_orders - span.first_order_number + 1\n# Remove the two unneeded columns\nspan = span.drop(columns=['total_orders', 'first_order_number'])\nspan.head()","1d6dd619":"uxp_ratio = pd.merge(times, span, on=['user_id', 'product_id'], how='left')\nuxp_ratio['uxp_ratio'] = uxp_ratio.Times_Bought \/ uxp_ratio.Order_Range\nuxp_ratio.head()","723aa396":"uxp_ratio = uxp_ratio.drop(columns=['Times_Bought', 'Order_Range'])\nuxp_ratio.head()","30a9ae5e":"#Remove temporary DataFrames\ndel [times, first_order_no, span]","3061f1d6":"uxp = uxp.merge(uxp_ratio, on=['user_id', 'product_id'], how='left')\n\ndel uxp_ratio\nuxp.head()","6e539b7f":"op['order_number_back'] = op.groupby('user_id')['order_number'].transform(max) - op.order_number +1 \nop.head(15)","321ff96d":"op5 = op[op.order_number_back <= 5]\nop5.head(15)","7a9f6923":"last_five = op5.groupby(['user_id','product_id'])[['order_id']].count()\nlast_five.columns = ['times_last5']\nlast_five.head(10)","f6eee172":"uxp = uxp.merge(last_five, on=['user_id', 'product_id'], how='left')\n\ndel [op5 , last_five]\nuxp.head()","27a12802":"uxp = uxp.fillna(0)\nuxp.head()","25237dcc":"data = uxp.merge(user, on='user_id', how='left')\ndata = data.merge(prd, on='product_id', how='left')\ndel op, user, prd, uxp","aea08dea":"data.head()","aa8ba866":"orders_future = orders[((orders.eval_set=='train') | (orders.eval_set=='test'))]\norders_future = orders_future[ ['user_id', 'eval_set', 'order_id'] ]\norders_future.head(10)","f593cfb9":"data = data.merge(orders_future, on='user_id', how='left')\ndata.head(10)","79102586":"#Keep only the customers who we know what they bought in their future order\ndata_train = data[data.eval_set=='train']\ndata_train.head()","c31996c4":"#Get from order_products_train all the products that the train users bought bought in their future order\ndata_train = data_train.merge(order_products_train[['product_id','order_id', 'reordered']], on=['product_id','order_id'], how='left' )\ndata_train.head(10)","de02be5b":"#Where the previous merge, left a NaN value on reordered column means that the customers they haven't bought the product. We change the value on them to 0.\ndata_train['reordered'] = data_train['reordered'].fillna(0)\ndata_train.head(10)","d0b6febd":"#We set user_id and product_id as the index of the DF\ndata_train = data_train.set_index(['user_id', 'product_id'])\n#We remove all non-predictor variables\ndata_train = data_train.drop(['eval_set', 'order_id'], axis=1)\ndata_train.head()","4eb01911":"#Keep only the future orders from customers who are labelled as test\ndata_test = data[data.eval_set=='test']\n#Set user_id and product_id as the index of the DF\ndata_test = data_test.set_index(['user_id', 'product_id'])\n#Remove all non-predictor variables\ndata_test = data_test.drop(['eval_set','order_id'], axis=1)\n#Check if the data_test DF, has the same number of columns as the data_train DF, excluding the response variable\ndata_test.head()","6865eb8d":"X, y = data_train.drop('reordered', axis=1), data_train.reordered","5983c0ba":" X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","5c9b8d6f":"paramGrid = {\n         \"learning_rate\":[0.1],\n         \"n_estimators\":[600],\n         \"max_depth\": [6],\n         \"subsample\": [0.8],\n         \"colsample_bytree\": [0.6],\n         \"colsample_bylevel\": [0.6]\n            }  \n\nmodel = XGBClassifier(nthread=10)#, tree_method='gpu_hist', gpu_id=0)\n\ncv = StratifiedKFold()\n\ngridsearch = GridSearchCV(model, paramGrid, scoring='roc_auc', cv=cv, verbose=2)\n\nfit = gridsearch.fit(X_train, y_train)\n\nprint(fit.best_score_)\n\nprint(fit.best_params_)","a2a703a8":"fit.score(X_test, y_test)","aee989bb":"best = fit.best_estimator_","40459de7":"%matplotlib inline\n\nfig, ax = pyplot.subplots(figsize=(10, 8), dpi=180)\n\n\nplot_importance(best, height=0.4, importance_type='gain', max_num_features=30, show_values=False, ax=ax)\n\npyplot.show()","77082aa5":"### 2.3.2.1 Calculate the numerator ('Times_Bought')\n\nTo answer this question we .groupby( ) user_id & product_id and we count the instances of order_id for each group.","cfc231c4":"With the use of order_number_back we can now select to keep only the last five orders of each customer:","94088519":"Having kept the last 5 orders for each user, we perform a .groupby( ) on user_id & product_id. With .count( ) we get how many times each customer bought a product.","dc31bf8f":"We split the datasets in train and test sets in order to ensure that the evaluation of the model is unbiased. We tune the model using the train set and then we evaluate the final model in the test set. We hold 25% of the data for the test set. \n\nIMPORTANT: We split Kaggle's Train Set into train (75%) and test sets (25%). ","ded2e64b":"## 1.2 Load data from the CSV files\nInstacart provides 6 CSV files, which we have to load into Python. Towards this end, we use the .read_csv() function, which is included in the Pandas package. Reading in data with the .read_csv( ) function returns a DataFrame.\n\n","288cbc95":"## 2.4 Merge all features\nWe now merge the DataFrames with the three types of predictors that we have created (i.e., for the users, the products and the combinations of users and products).\n\nWe will start from the **uxp** DataFrame and we will add the user and prd DataFrames. We do so because we want our final DataFrame (which will be called **data**) to have the following structure: \n\n<img style=\"float: left;\" src=\"https:\/\/i.imgur.com\/mI5BbFE.jpg\" >\n","449db169":"### 2.3.2 How frequently a customer bought a product after its first purchase\nThis ratio is a metric that describes how many times a user bought a product out of how many times she had the chance to buy it (starting from her first purchase of the product): **(Times_Bought)\/(Order_Range)**\n\n* Times_Bought = Times a user bought a product\n* Order_Range = Total orders placed since the first user's order of a product\n\nFor example, we examine user 1 (user_id:1) and product 13032 (product_id:13032). User 1 made 10 orders in total. She bought the product 13032 **for first time in her 2nd order** and she has bought the same product 3 times in total. The user was able to buy the product 9 times (starting from her 2nd order until her last order). As a result, she has bought it 3 out of 9 times, meaning reorder_ratio=3\/9= 0,333.\n\nThe Order_Range variable is created using two supportive variables:\n* Total_orders = Total number of orders of each user\n* First_order_number = The order number where the customer bought a product for first time\n\nIn the next blocks we show how we create:\n1. The numerator 'Times_Bought'\n2. The denumerator 'Order_Range' with the use of the supportive variables 'Total_orders' & 'First_order_number' \n3. Our final ratio 'uxp_ratio'","6fb88fd3":"### 2.2.2.2 Group products, calculate the mean of reorders\n\nTo calculate the reorder probability we will use the aggregation function mean() to the reordered column. In the reorder data frame, the reordered column indicates that a product has been reordered when the value is 1.\n\nThe .mean() calculates how many times a product has been reordered, divided by how many times has been ordered in total. \n\nWe calculate the ratio for each product. The aggregation function is limited to column 'reordered' and it calculates the mean value of each group.","0b091e9e":"# Introduction\nThis is the 2021 version of the notebook created by the [Information Systems Lab](http:\/\/islab.uom.gr) to introduce students of the [University of Macedonia](http:\/\/www.uom.gr\/index.php?tmima=2&categorymenu=2), Greece to Machine Learning & Data Science.\n\nThe notebook has been created to use in a RAPIDS environment. In particular in has been developed in a 16 vCPUs, 60 GB RAM, and 1 NVIDIA Tesla T4 GPU in Google Cloud Platform.\n\n## The Instacart competition\nInstacart is an American company that operates as a same-day grocery delivery service. Customers select groceries through a web application from various retailers and delivered by a personal shopper. Instacart's service is mainly provided through a smartphone app, available on iOS and Android platforms, apart from its website.\n\nIn 2017 Instacart organised a Kaggle competition and provided to the community a sample dataset of over 3 million grocery orders from more than 200,000 Instacart users. The orders include 32 million basket items and 50,000 unique products. The objective of the competition was participants to **predict which previously purchased products will be in a user\u2019s next order**.\n\n## Objective\nThe objective of this notebook is to introduce students to predictive business analytics with Python through the Instacart case. \n\nBy the time you finish this example, you will be able to:\n* Describe the steps of creating a predictive analytics model\n* Use Python and Pandas package to manipulate data\n* Use Python and Pandas package to create, combine, and delete DataFrames\n* Use Random Forests to create a predictive model\n* Apply the predictive model in order to make a prediction\n* Create a submission file for the competition of Instacart\n\n## Problem definition\nThe data that Instacart opened up include orders of 200,000 Instacart users with each user having between 4 and 100 orders. Instacart indicates each order in the data as prior, train or test. Prior orders describe the **past behaviour** of a user while train and test orders regard the **future behaviour that we need to predict**. \n\nAs a result, we want to predict which previously purchased products (prior orders) will be in a user\u2019s next order (train and test orders). \n\nFor the train orders Instacart reveals the results (i.e., the ordered products) while for the test orders we do not have this piece of information. Moreover, the future order of each user can be either train or test meaning that each user will be either a train or a test user. \n\nThe setting of the Instacart problem is described in the figure below (orders with yellow color denotes future orders of a user). \n\n<img src=\"https:\/\/i.imgur.com\/S0Miw3m.png\" width=\"350\">\n\nEach user has purchased various products during their prior orders. Moreover, for each user we know the order_id of their future order. The goal is to predict which of these products will be in a user's future order. \n\nThis is a **classification problem** because we need to predict whether each pair of user and product is a reorder or not. This is indicated by the value of the reordered variable, i.e. reordered=1 or reordered=0 (see figure below). \n\n<img src=\"https:\/\/i.imgur.com\/SxK2gsR.png\" width=\"350\">\n\nAs a result we need to come up and calculate various **predictor variables (X)** that will describe the characteristics of a product and the behaviour of a user regarding one or multiple products. We will do so by analysing the prior orders of the dataset. We will then use the train users to create a predictive model and the test users to make our actual prediction. As a result we create a table as the following one and we train an algorithm based on predictor variables (X) and response variable (Y).\n\n<img src=\"https:\/\/i.imgur.com\/Yb1CKAF.png\" width=\"600\">\n\n## Method\nOur method includes the following steps:\n1. <b>Import and reshape data<\/b>: This step includes loading CSV files into pandas DataFrames, tranform character variables to categorical variables, and create a supportive table.\n2. <b>Create predictor variables<\/b>: This step includes identifying and calculating predictor variables (aka features) from the initial datasets provided by Instacart. \n3. <b>Create train and test DataFrames<\/b>: In this step we create two distinct pandas DataFrames that will be used in the creation and the use of the predictive model.\n4. <b>Create predictive model (fit)<\/b>: In this step we train a predictive model through the train dataset.\n5. <b>Apply predictive model (predict)<\/b>: This step includes applying the model to predict the 'reordered' variable for the test dataset.\n6. <b>Create submission file<\/b>: In this final step we create the submission file with our predictions for Instacart's competition.\n7. <b>Get F1 score<\/b>: In this step we submit the produced and file and get the F1 score describing the accuracy of our prediction model.","d5481715":"#### 2.3.3.6 Fill NaN values\nThe uxp DataFrame has NaN values for our new feature. This happens as there might be products that the customer did not buy on its last five orders. For these cases, we turn NaN values into zero (0) with .fillna(0) method.","352f1a74":"## 2.2.2 What is the probability for a product to be reordered\nIn this section we want to find the products which have the highest probability of being reordered. Towards this end it is necessary to define the probability per productas below:\n(number of product's reorders)\/(total number of purchaces of a product) \n\nExample: The product with product_id=2 has been purchaced 90 times but only 12 of these are reorders.\n\n### 2.2.2.1 Remove products with less than 40 purchases - Filter with .shape[0]\nWe first remove products having less than 40 purchases in order the calculated variable to be meaningful.\n\nUsing .groupby() we create groups for each product and using .filter( ) we keep only groups with more than 40 rows. Towards this end, we indicate a lambda function.","a7acabd0":"## 3.2 Prepare the train DataFrame\nIn order to prepare the train Dataset, which will be used to create our prediction model, we need to include also the response (Y) and thus have the following structure:\n\n<img style=\"float: left;\" src=\"https:\/\/i.imgur.com\/PDu2vfR.jpg\" >\n\nTowards this end:\n1. We keep only the customers who are labelled as \"train\" from the competition\n2. For these customers we get from order_products_train the products that they have bought, in order to create the response variable (reordered:1 or 0)\n3. We make all the required manipulations on that dataset and we remove the columns that are not predictors\n\nSo now we filter the **data** DataFrame so to keep only the train users:","4ab73d17":"### 2.2.2.3 Merge the new feature on prd DataFrame\nThe new feature will be merged with the prd DataFrame (section 2.2.1) which keep all the features based on products. We perform a left join as we want to keep all the products that we have created on the prd DataFrame.","d03f9aa6":"The new feature will be merged with the user DataFrame (section 2.1.1) which keep all the features based on users. We perform a left join as we want to keep all the users that we have created on the user DataFrame","54d1a0ed":"# 3. Create train and test DataFrames\n## 3.1 Include information about the last order of each user\n\nThe **data** DataFrame that we have created should include two more columns which define the type of user (train or test) and the order_id of the future order.\nThis information can be found on the **orders** DataFrame: \n\n<img style=\"float: left;\" src=\"https:\/\/i.imgur.com\/jbatzRY.jpg\" >\n\n\nTowards this end:\n1. We select the **orders** DataFrame to keep only the future orders (labeled as \"train\" & \"test). \n2. Keep only the columns of our desire ['eval_set', 'order_id'] <span style=\"color:red\">**AND** <\/span> 'user_id' as is the matching key with our **data** DataFrame\n2. Merge **data** DataFrame with the information for the future order of each customer using as matching key the 'user_id'","c5e88e8d":"## 2.2 Create product predictors\nWe create the following predictors:\n- 2.2.1 Number of purchases for each product\n- 2.2.2 What is the probability for a product to be reordered\n\n### 2.2.1 Number of purchases for each product\nWe calculate the total number of purchases for each product (from all customers). We create a **prd** DataFrame to store the results.","7a7afa99":"### 2.3.3 How many times a customer bought a product on its last 5 orders\nFor this feature, we keep the last five orders of each customer and we calculate how many times they bought a product. To achieve this we need to:\n* Create a new variable ('order_number_back') which keeps the order_number for each order in reverse order\n* Keep only the last five orders for each order\n* Perform a .groupby( ) on users and products to get how many times each customer bought a product.","e0e97adf":"Our goal is to create a new column ('order_number_back') which indicates the last order as first, the second from the end as second and so on. To achieve this, we get the highest order_number (max) for user_id==1 and we subtract the order_number of each order from it. \n\nTowards this end, we .groupby( ) op by the user_id and we select the column order_number. With .transform(max) we request to get the highest number of the column order_number for each group & with minus (-) op.order_number we substract the order_number of each row. Finally we add 1.","ddcb0428":"### 2.1.2 How frequent a customer reorders products\n\nThis feature is a ratio for each user revealing how many reorders has had in their basket.\nSo we create the following ratio per user: **(reordered basket items)\/(total basket items)**\n\nThe nominator is a counter for all the times a user has reordered products (value on reordered=1), the denominator is a counter of all the products that have been purchased on all user's orders (reordered=0 & reordered=1).\n\nE.g., for a user that has ordered 6 products in total, where 3 times were reorders, the ratio will be:\n\nTo create the above ratio we .groupby() order_products_prior by each user and then calculate the mean of reordered.","a8bac53f":"## 1.3 Create a DataFrame with the orders and the products that have been purchased on prior orders (op)\nWe create a new DataFrame, named <b>op<\/b> which combines (merges) the DataFrames <b>orders<\/b> and <b>order_products_prior<\/b>. Bear in mind that <b>order_products_prior<\/b> DataFrame includes only prior orders, so the new DataFrame <b>op<\/b>  will contain only these observations as well. Towards this end, we use pandas' merge function with how='inner' argument, which returns records that have matching values in both DataFrames. ","6d3c168f":"### 2.3.2.2 Calculate the denumerator ('Order_Range')\nTo calculate the denumerator, we first calculate the total orders of each user & first order number for each user and every product purchase.\n\nIn order to calculate the total number of orders of each cutomer ('total_orders') we .groupby( ) only by the user_id, we keep the column order_number and we get its highest value with the aggregation function .mean()","9ffee6c2":"#### 2.3.3.5 Merge the final feature with uxp DataFrame\nThe new feature will be merged with the uxp DataFrame (section 2.3.1) which keep all the features based on combinations of user-products. We perform a left join as we want to keep all the user-products that we have created on the uxp DataFrame","f8cacec4":"# 4. Create predictive model (fit)\nThe Machine Learning model that we are going to create is based on the XGBoost algorithm.\n\n\n## 4.1 Train XGBoost\nXGBoost stands for e**X**treme **G**radient **Boost**ing, an algorithm that is used in many winning solutions for Kaggle competitions [(ref)](https:\/\/github.com\/dmlc\/xgboost\/tree\/master\/demo#machine-learning-challenge-winning-solutions). \n\nXGBoost is an implementation of gradient boosted decision trees designed for speed and performance.\n\nGradient boosting is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.\n","c4767c66":"# 2. Create Predictor Variables\nWe are now ready to identify and calculate predictor variables based on the provided data. We can create various types of predictors such as:\n* <b>User predictors<\/b> describing the behavior of a user e.g. total number of orders of a user.\n* <b>Product predictors<\/b> describing characteristics of a product e.g. total number of times a product has been purchased.\n* <b>User & product predictors<\/b> describing the behavior of a user towards a specific product e.g. total times a user ordered a specific product.","7b21d218":"## 2.3 Create user-product predictors\nWe create the following predictors:\n- 2.3.1 How many times a user bought a product\n- 2.3.2 How frequently a customer bought a product after its first purchase\n- 2.3.3 How many times a customer bought a product on its last 5 orders\n\n### 2.3.1 How many times a user bought a product\nWe create different groups that contain all the rows for each combination of user and product. With the aggregation function .count( ) we get how many times each user bought a product. We save the results on new **uxp** DataFrame.","8ac175d1":"We select to keep only the 'user_id', 'product_id' and the final feature 'uxp_reorder_ratio'","04463132":"## 2.1 Create user predictors\nWe create the following predictors:\n- 2.1.1 Number of orders per customer\n- 2.1.2 How frequent a customer has reordered products\n\n### 2.1.1 Number of orders per customer\nWe calculate the total number of placed orders per customer. We create a **user** DataFrame to store the results.","34c7f6cd":"We initially create and finetune the model using the train set. In order to ensure that our model will have low variance and bias we employ repeated k-fold Cross-Validation. Cross-validation (CV) is a popular strategy for algorithm selection. The main idea behind CV is to split data, once or several times, for estimating the risk of each algorithm. Part of data (the training sample) is used for training each algorithm, and the remaining part (the validation sample) is used for estimating the risk of the algorithm. We create stratified folds when performing cross-validation. This has the effect of enforcing the same distribution of classes in each fold as in the whole training dataset when performing the cross-validation evaluation.  We perform 5-fold validation, meaning that we create and evaluate 5 models in each round and the final score is the average score of these models\n\nIMPORTANT: We employ GPU by setting: tree_method='gpu_hist', gpu_id=0","0ac631d0":"We incorporate on **data** DataFrame the information for the last order of each customer. The matching key here is the user_id and we select a left join as we want to keep all the observations from **data** DataFrame.","664a56dd":"### 2.3.2.3 Create the final ratio \"uxp_ratio\"\n\nWe merge **times** DataFrame, which contains the numerator, and **span** DataFrame, which contains the denumerator of our desired ratio. **As both variables derived from the combination of users & products, any type of join will keep all the combinations.**\n\nWe divide the Times_Bought by the Order_Range for each user and product.","68b8e16a":"This step results in the following DataFrames:\n* <b>orders<\/b>: This table includes all orders, namely prior, train, and test. It has single primary key (<b>order_id<\/b>).\n* <b>order_products_train<\/b>: This table includes training orders. It has a composite primary key (<b>order_id and product_id<\/b>) and indicates whether a product in an order is a reorder or not (through the reordered variable).\n* <b>order_products_prior <\/b>: This table includes prior orders. It has a composite primary key (<b>order_id and product_id<\/b>) and indicates whether a product in an order is a reorder or not (through the reordered variable).\n* <b>products<\/b>: This table includes all products. It has a single primary key (<b>product_id<\/b>)\n* <b>aisles<\/b>: This table includes all aisles. It has a single primary key (<b>aisle_id<\/b>)\n* <b>departments<\/b>: This table includes all departments. It has a single primary key (<b>department_id<\/b>)","738a7490":"We now use the .head( ) method in order to visualise the first 10 rows of these tables. Click the Output button below to see the tables.","27e85512":"We merge the first order number with the total_orders DataFrame. As total_orders refers to all users, where first_order_no refers to unique combinations of user & product, we perform a right join.\n\nThe denominator ('Order_Range') now can be created with simple operations between the columns of span DataFrame:","6c337351":"### 2.3.2.4 Merge the final feature with uxp DataFrame\nThe new feature will be merged with the uxp DataFrame (section 2.3.1) which keep all the features based on combinations of user-products. We perform a left join as we want to keep all the user-products that we have created on the uxp DataFrame","d129eaeb":"In order to identify the order number where the user bought a product for first time ('first_order_number') we .groupby( ) both user_id & product_id and from the order_number column we retrieve the **.min( ) value**.","b45227f9":"# 1. Import and Reshape Data \nFirst we load the necessary Python packages and then we import the CSV files that were provided by Instacart.\n\n## 1.1 Import the required packages\nThe **\"as\"** reserved word is to define an alias to the package. The alias help us to call easier a package in our code.","1ce46a1d":"## 3.3 Prepare the test DataFrame\nThe test DataFrame must have the same structure as the train DataFrame, excluding the \"reordered\" column (as it is the label that we want to predict).\n<img style=\"float: left;\" src=\"https:\/\/i.imgur.com\/lLJ7wpA.jpg\" >\n \n To create it, we:\n- Keep only the customers who are labelled as test\n- Set as index the column(s) that uniquely describe each row (in our case \"user_id\" & \"product_id\")\n- Remove the columns that are predictors (in our case:'eval_set', 'order_id')","d16d7beb":"2.2.2.4 Fill NaN values\n\nAs you may notice, there are product with NaN values. This regards the products that have been purchased less than 40 times from all users and were not included in the p_reorder DataFrame. As we performed a left join with prd DataFrame, all the rows with products that had less than 40 purchases from all users, will get a NaN value.\n\nFor these products we their NaN value with zero (0):\n","23eb23c3":"The last columm (reordered) is the response variable (y). \nThere are combinations of User X Product which they were ordered (1) in last order where other were not (NaN value).\n\nNow we manipulate the data_train DataFrame, to bring it into a structure for Machine Learning (X1,X2,....,Xn, y):\n- Fill NaN values with value zero (regards reordered rows without value = 1)\n- Set as index the column(s) that describe uniquely each row (in our case \"user_id\" & \"product_id\")\n- Remove columns which are not predictors (in our case: 'eval_set','order_id')"}}