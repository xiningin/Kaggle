{"cell_type":{"494b0a6d":"code","14653528":"code","c5d96d89":"code","dc26bab4":"code","9810302a":"code","ff9eabc3":"code","86b454c6":"code","3b4a82ba":"code","359ef31a":"code","a5d92ea4":"code","7174f351":"code","a8e2d335":"code","776be677":"code","e063b313":"code","92cee5bd":"code","e49e9a01":"code","233954b3":"code","7a29a6ba":"code","bc5d5e94":"code","7001cf5f":"code","3b976215":"code","0968591f":"code","ececae6c":"code","470de1de":"code","09c98a75":"code","8fef4bac":"code","9b070831":"code","8b249fde":"code","0f7de4cf":"code","376331df":"code","24ae59f8":"markdown","37810367":"markdown","9df59eb9":"markdown","58780541":"markdown","b8095a9a":"markdown","dd59bf44":"markdown","c00aef3d":"markdown","60cd54cf":"markdown","a49d5672":"markdown","de7dba7b":"markdown","827f0cee":"markdown","e0ac0a47":"markdown","592559a7":"markdown","fda7adae":"markdown","d01e35fe":"markdown","67f997e5":"markdown"},"source":{"494b0a6d":"!pip install -q --upgrade pip\n!pip install -q efficientnet","14653528":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport matplotlib.pyplot as plt\nimport efficientnet.tfkeras as efn\nimport seaborn as sns\n\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport sys\nimport glob\nimport math\nimport gc\nimport time\n\nprint(f'tensorflow version: {tf.__version__}')\nprint(f'tensorflow keras version: {tf.keras.__version__}')\nprint(f'python version: P{sys.version}')","c5d96d89":"AUTO = tf.data.experimental.AUTOTUNE\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')\n\n# # set half precision policy\nmixed_precision.set_policy('mixed_bfloat16')\n\n# enable XLA optmizations\ntf.config.optimizer.set_jit(True)\n\nprint(f'Compute dtype: {mixed_precision.global_policy().compute_dtype}')\nprint(f'Variable dtype: {mixed_precision.global_policy().variable_dtype}')","dc26bab4":"IMG_HEIGHT = 600\nIMG_WIDTH = 800\n\nIMG_SIZE = 600\nIMG_TARGET_SIZE = 512\nN_CHANNELS = 3\n\nN_TRAIN_IMGS = 17118\nN_VAL_IMGS = 4280\nBATCH_SIZE_VAL = 107 * REPLICAS # 4280 \/ 5\n\nN_LABELS = 5\nN_FOLDS = 1\nEPOCHS = 15\n\nBATCH_SIZE_BASE = 32\nBATCH_SIZE = BATCH_SIZE_BASE * REPLICAS\n\nTARGET_DTYPE = tf.bfloat16\n\n# ImageNet mean and standard deviation\nIMAGENET_MEAN = tf.constant([0.485, 0.456, 0.406], dtype=tf.float32)\nIMAGENET_STD = tf.constant([0.229, 0.224, 0.225], dtype=tf.float32)","9810302a":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('cassava-leaf-disease-tfrecords-600x600')","ff9eabc3":"def decode_tfrecord_train(record_bytes):\n    features = tf.io.parse_single_example(record_bytes, {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([], tf.int64),\n    })\n\n    image = tf.io.decode_jpeg(features['image'])\n    image = tf.reshape(image, [IMG_HEIGHT, IMG_WIDTH, N_CHANNELS])\n    \n    # get random square of 600*600 pixels from the original 800*600 image\n    offset = tf.random.uniform(shape=(), minval=0, maxval=IMG_WIDTH-IMG_SIZE, dtype=tf.int32)\n    image = tf.slice(image, [0, offset, 0], [IMG_SIZE, IMG_SIZE, N_CHANNELS])\n    \n    # cast label to int8\n    label = tf.cast(features['label'], tf.uint8)\n    \n    return image, label","86b454c6":"# chance of x in y to return true, used for conditional data augmentation\ndef chance(x, y):\n    return tf.random.uniform(shape=[], minval=0, maxval=y, dtype=tf.int32) < x","3b4a82ba":"def augment_image(image, label):\n    # flip image horizontally\n    image = tf.image.random_flip_left_right(image)\n    # flip image vertically\n    image = tf.image.random_flip_up_down(image)\n    \n    # transpose\n    if chance(1,2):\n        image = tf.image.transpose(image)\n    \n    # random crop between 75%-100% of orignal image size\n    crop_size = tf.random.uniform(shape=(), minval=IMG_SIZE*0.75, maxval=IMG_SIZE)\n    image = tf.image.random_crop(image, [crop_size, crop_size, N_CHANNELS])\n    \n    # resize image to target size\n    image = tf.image.resize(image, [IMG_TARGET_SIZE, IMG_TARGET_SIZE])\n    \n    # normalize according to imagenet mean and standard deviation\n    image \/= 255.0\n    image = (image - IMAGENET_MEAN) \/ IMAGENET_STD\n    \n    # cast to bfloat16 to reduce memory consumption and speedup computations\n    image = tf.cast(image, tf.bfloat16)\n    \n    # one hot encode label\n    label = tf.one_hot(label, N_LABELS, dtype=tf.float32)\n    \n    return image, label","359ef31a":"# function to decode and augment image in one call, speeds up dataset as only 1 map is needed instead of 2\ndef read_augment_image(record_bytes):\n    image, label = decode_tfrecord_train(record_bytes)\n    image, label = augment_image(image, label)\n    \n    return image, label","a5d92ea4":"def get_mix_img_idx(labels_idxs, idx):\n    idx_candidates = tf.where(labels_idxs != idx)\n    r = tf.random.uniform(minval=0, maxval=len(idx_candidates), shape=[], dtype=tf.int32)\n    idx = tf.gather(idx_candidates, r)\n    idx = tf.cast(idx, tf.int32)\n    idx = tf.squeeze(idx)\n    \n    return idx","7174f351":"def mixup(images, labels):\n    l = len(images)\n    # get image factors a and b by using the beta distribution and resize to l*N_LABELS\n    a = tfp.distributions.Beta(0.4, 0.4).sample(l)\n    a_label = tf.reshape(a, shape=(l,1))\n    a_label = tf.tile(a_label, [1, N_LABELS])\n    b_label = 1 - a_label # b is 1 minus ratio of a as they need to add up to 1.0\n    \n    # get image factors a and b and resize to l * IMG_TARGET_SIZE * IMG_TARGET_SIZE * N_CHANNELS\n    a_image = tf.reshape(a, shape=(l,1,1,1))\n    a_image = tf.tile(a_image, [1, IMG_TARGET_SIZE, IMG_TARGET_SIZE ,N_CHANNELS])\n    a_image = tf.cast(a_image, tf.bfloat16)\n    b_image = 1 - a_image # b is 1 minus ratio of a as they need to add up to 1.0\n    \n    # get MixUp image indices for images b, thus the images used to mixup images a\n    if l == 2:\n        idxs = tf.constant([1, 0])\n    else:\n        labels_idxs = tf.range(len(labels))\n        idxs = tf.map_fn(lambda idx: get_mix_img_idx(labels_idxs, idx), tf.range(len(labels)))\n    \n    # get the images and labels using the indices\n    images_mixup = tf.gather(images, idxs)\n    labels_mixup = tf.gather(labels, idxs)\n    \n    # mixup images and labels\n    images =  images * a_image + images_mixup * b_image\n    labels = labels * a_label + labels_mixup * b_label\n    \n    return images, labels","a8e2d335":"def create_cutmix_mask(a):\n    # create random mask size and coordinates\n    r_w = tf.cast(IMG_TARGET_SIZE * tf.math.sqrt(1 - a), tf.int32)\n    r_h = tf.cast(IMG_TARGET_SIZE * tf.math.sqrt(1 - a), tf.int32)\n    \n    if r_w == IMG_TARGET_SIZE:\n        r_x = 0\n    else:\n        r_x = tf.random.uniform(minval=0, maxval=IMG_TARGET_SIZE - r_w, shape=[], dtype=tf.int32)\n        \n    if r_h == IMG_TARGET_SIZE:\n        r_y = 0\n    else:\n        r_y = tf.random.uniform(minval=0, maxval=IMG_TARGET_SIZE - r_w, shape=[], dtype=tf.int32)\n\n    # compute padding sizes\n    pad_left = r_x\n    pad_right = IMG_TARGET_SIZE - (r_x + r_w)\n    pad_top = r_y\n    pad_bottom = IMG_TARGET_SIZE - (r_y + r_h)\n    \n    # create mask_a and mask_b\n    mask_a = tf.ones(shape=[r_w, r_h], dtype=tf.bfloat16)\n    mask_a = tf.pad(mask_a, [[pad_left, pad_right], [pad_top, pad_bottom]], mode='CONSTANT', constant_values=0)\n    mask_a = tf.expand_dims(mask_a, axis=2)\n    \n    return mask_a\n\ndef cutmix(images, labels):\n    l = len(images)\n    a_float32 = tfp.distributions.Beta(1.0, 1.0).sample([l])\n\n    a_bfloat16 = tf.cast(a_float32, tf.bfloat16) \n    mask_b = tf.map_fn(create_cutmix_mask, a_bfloat16)\n    mask_a = tf.math.abs(mask_b - 1)\n    \n    # images_idxs\n    if l == 2:\n        idxs = tf.constant([1, 0])\n    else:\n        labels_idxs = tf.range(len(labels))\n        idxs = tf.map_fn(lambda idx: get_mix_img_idx(labels_idxs, idx), tf.range(len(labels)))\n    \n    images_cutmix = tf.gather(images, idxs)\n    labels_cutmix = tf.gather(labels, idxs)\n    \n    a_float32_labels = tf.expand_dims(a_float32, axis=1)\n    a_float32_labels = tf.repeat(a_float32_labels, N_LABELS, axis=1)\n    labels_factor = a_float32_labels\n    labels_cutmix_factor = 1 - a_float32_labels\n    \n    # cutmix images and labels\n    images = images * mask_a + images_cutmix * mask_b\n    labels = labels * labels_factor + labels_cutmix * labels_cutmix_factor\n    \n    return images, labels","776be677":"def gridmask(images, labels):\n    l = len(images)\n    \n    d = tf.random.uniform(minval=int(IMG_TARGET_SIZE * (96\/224)), maxval=IMG_TARGET_SIZE, shape=[], dtype=tf.int32)\n    grid = tf.constant([[[0], [1]],[[1], [0]]], dtype=tf.bfloat16)\n    grid = tf.image.resize(grid, [d, d], method='nearest')\n    \n    # 50% chance to rotate mask\n    if chance(1, 2):\n        grid = tf.image.rot90(grid, 1)\n\n    repeats = IMG_TARGET_SIZE \/\/ d + 1\n    grid = tf.tile(grid, multiples=[repeats, repeats, 1])\n    grid = tf.image.random_crop(grid, [IMG_TARGET_SIZE, IMG_TARGET_SIZE, 1])\n    grid = tf.expand_dims(grid, axis=0)\n    grid = tf.tile(grid, multiples=[l, 1, 1, 1])\n\n    images = images * grid\n    \n    return images, labels","e063b313":"def augment_batch(images, labels, augmentations=None):\n    # quarter of images will be original\n    if augmentations is None:\n        r = tf.random.uniform(minval=0, maxval=4, shape=[], dtype=tf.int32)\n    else:\n        r = tf.random.uniform(minval=0, maxval=len(augmentations), shape=[], dtype=tf.int32)\n        r = tf.gather(augmentations, r)\n        \n    if r == 0:\n        return images, labels\n    elif r == 1:\n        return mixup(images, labels)\n    elif r == 2:\n        return cutmix(images, labels)\n    elif r == 3:\n        return gridmask(images, labels)\n    else:\n        return images, labels","92cee5bd":"def get_train_dataset(bs=BATCH_SIZE, fold=0, augmentations=None):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n    \n    FNAMES_TRAIN_TFRECORDS = tf.io.gfile.glob(f'{GCS_DS_PATH}\/fold_{fold}\/train\/*.tfrecords')\n    train_dataset = tf.data.TFRecordDataset(FNAMES_TRAIN_TFRECORDS, num_parallel_reads=AUTO)\n    train_dataset = train_dataset.with_options(ignore_order)\n    train_dataset = train_dataset.prefetch(BATCH_SIZE_BASE)\n    train_dataset = train_dataset.repeat()\n    train_dataset = train_dataset.map(read_augment_image, num_parallel_calls=AUTO)\n    train_dataset = train_dataset.batch(BATCH_SIZE_BASE)\n    train_dataset = train_dataset.map(lambda images, labels: augment_batch(images, labels, augmentations=augmentations), num_parallel_calls=REPLICAS)\n    train_dataset = train_dataset.unbatch()\n    train_dataset = train_dataset.shuffle(bs)\n    train_dataset = train_dataset.batch(bs)\n    train_dataset = train_dataset.prefetch(1)\n    \n    return train_dataset\n\ntrain_dataset = get_train_dataset()","e49e9a01":"def benchmark(num_epochs=3):\n    dataset = get_train_dataset()\n    start_time = time.perf_counter()\n    for epoch_num in range(num_epochs):\n        epoch_start = time.perf_counter()\n        for idx, (images, labels) in enumerate(dataset.take(10)):\n            if idx is 1:\n                print(images.shape, labels.shape)\n            pass\n        print(f'epoch {epoch_num} took: {round(time.perf_counter() - epoch_start, 2)}')\n    print(\"Execution time:\", round(time.perf_counter() - start_time, 2))\n    \nbenchmark()","233954b3":"def show_first_train_batch(augmentations=None, print_info=False, rows=4, cols=4):\n    # log info of batch and first few train images\n    imgs, lbls = next(iter(get_train_dataset(augmentations=augmentations)))\n    if print_info:\n        print(f'Number of train images: {N_TRAIN_IMGS}')\n        print(f'imgs.shape: {imgs.shape}, images.dtype: {imgs.dtype}, lbls.shape: {lbls.shape}, lbls.dtype: {lbls.dtype}')\n        img0 = imgs[0].numpy().astype(np.float32)\n        print('img0 mean: {:.3f}, img0 std {:.3f}, img0 min: {:.3f}, img0 max: {:.3f}'.format(img0.mean(), img0.std(), img0.min(), img0.max()))\n        print(f'first label: {lbls[0]}')\n\n    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(cols*6, rows*6))\n    for r in range(rows):\n        for c in range(cols):\n            img = imgs[r*rows+c].numpy().astype(np.float32)\n            lbl = lbls[r*rows+c].numpy().astype(np.float32).tolist()\n            \n            # add title with image information\n            lbl_str = '[' + ', '.join(['%.3f' % i for  i in lbl]) + ']'\n            axes[r, c].set_title('mean: {:.3f}, std {:.3f}, min: {:.3f}, max: {:.3f}\\n label: {}'.format(img.mean(), img.std(), img.min(), img.max(), lbl_str))\n            axes[r, c].axhline(y=IMG_TARGET_SIZE \/\/ 2, color='r')\n            axes[r, c].axvline(x=IMG_TARGET_SIZE \/\/ 2, color='r')\n            \n            img += abs(img.min())\n            img \/= img.max()\n            axes[r, c].imshow(img)\n\nshow_first_train_batch(augmentations=[1,2,3], print_info=True)","7a29a6ba":"# MixUp examples\nshow_first_train_batch(augmentations=[1], rows=2, cols=3)","bc5d5e94":"# CutMix examples\nshow_first_train_batch(augmentations=[2], rows=2, cols=3)","7001cf5f":"# GridMask examples\nshow_first_train_batch(augmentations=[3], rows=2, cols=3)","3b976215":"def show_data_augmentations():\n    FNAMES_TRAIN_TFRECORDS = tf.io.gfile.glob(f'{GCS_DS_PATH}\/fold_0\/train\/*.tfrecords')\n    dataset = tf.data.TFRecordDataset(FNAMES_TRAIN_TFRECORDS)\n    dataset = dataset.map(decode_tfrecord_train)\n    dataset = dataset.batch(BATCH_SIZE)\n\n    imgs, lbls = next(iter(dataset))\n    \n    # to test data augmentation\n    rows, cols = 4, 4\n    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(cols*6, rows*6))\n    for r in range(rows):\n        for c in range(cols):\n            img, _ = augment_image(imgs[14], -1)            \n            img = img.numpy().astype(np.float32)\n            \n            # add title with image information\n            axes[r, c].set_title('mean: {:.3f}, std {:.3f}, min: {:.3f}, max: {:.3f}'.format(img.mean(), img.std(), img.min(), img.max()))\n            \n            img += abs(img.min())\n            img \/= img.max()\n            \n            axes[r, c].imshow(img)\n                \nshow_data_augmentations()","0968591f":"def decode_tfrecord_val(record_bytes):\n    features = tf.io.parse_single_example(record_bytes, {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([], tf.int64),\n    })\n\n    image = tf.io.decode_jpeg(features['image'])\n    image = tf.reshape(image, [IMG_HEIGHT, IMG_WIDTH, N_CHANNELS])\n    \n    offset = (IMG_WIDTH - IMG_SIZE) \/\/ 2\n    image = tf.slice(image, [0, offset, 0], [IMG_SIZE, IMG_SIZE, N_CHANNELS])\n    \n    # resize to target size\n    image = tf.image.resize(image, [IMG_TARGET_SIZE, IMG_TARGET_SIZE])\n    \n    # normalize according to imagenet mean and std\n    image \/= 255.0\n    image = (image - IMAGENET_MEAN) \/ IMAGENET_STD\n    \n    # cast to bfloat16\n    image = tf.cast(image, tf.bfloat16)\n    \n    label = tf.cast(features['label'], tf.int32)\n    \n    # one hot encode label\n    label = tf.one_hot(label, N_LABELS, dtype=tf.int32)\n    \n    return image, label","ececae6c":"def get_val_dataset(bs=BATCH_SIZE, fold=0):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n    \n    FNAMES_VAL_TFRECORDS = tf.io.gfile.glob(f'{GCS_DS_PATH}\/fold_{fold}\/val\/*.tfrecords')\n    val_dataset = tf.data.TFRecordDataset(FNAMES_VAL_TFRECORDS, num_parallel_reads=AUTO)\n    val_dataset = val_dataset.with_options(ignore_order)\n    \n    val_dataset = val_dataset.repeat()\n    val_dataset = val_dataset.map(decode_tfrecord_val, num_parallel_calls=AUTO)\n    # drop remainder to get the exact same images each epoch\n    val_dataset = val_dataset.batch(bs, drop_remainder=True)\n    val_dataset = val_dataset.prefetch(AUTO)\n    \n    return val_dataset\n\nval_dataset = get_val_dataset()","470de1de":"# Show batch info and first few test images\ndef show_first_val_batch():\n    imgs, lbls = next(iter(val_dataset))\n    \n    print(f'Number of val images: {N_VAL_IMGS}')\n    print(f'imgs.shape: {imgs.shape}, images.dtype: {imgs.dtype}, lbls.shape: {lbls.shape}, lbls.dtype: {lbls.dtype}')\n    img0 = imgs[0].numpy().astype(np.float32)\n    print('img0 mean: {:.3f}, img0 std {:.3f}, img0 min: {:.3f}, img0 max: {:.3f}'.format(img0.mean(), img0.std(), img0.min(), img0.max()))\n    print(f'first label: {lbls[0]}')\n\n    rows, cols = 4, 4\n    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(cols*6, rows*6))\n    for r in range(rows):\n        for c in range(cols):\n            img = imgs[r * (rows + 1) + c].numpy().astype(np.float32)\n            \n            # add title with image information\n            axes[r, c].set_title('mean: {:.3f}, std {:.3f}, min: {:.3f}, max: {:.3f}'.format(img.mean(), img.std(), img.min(), img.max()))\n            \n            img += abs(img.min())\n            img \/= img.max()\n\n            axes[r, c].imshow(img)\n            \nshow_first_val_batch()","09c98a75":"def lrfn(epoch, bs=BATCH_SIZE, epochs=EPOCHS):\n    # Config\n    LR_START = 1e-6\n    LR_MAX = 2e-4\n    LR_FINAL = 1e-6\n    LR_RAMPUP_EPOCHS = 4\n    LR_SUSTAIN_EPOCHS = 0\n    DECAY_EPOCHS = epochs  - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n    LR_EXP_DECAY = (LR_FINAL \/ LR_MAX) ** (1 \/ (EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1))\n\n    if epoch < LR_RAMPUP_EPOCHS: # exponential warmup\n        lr = LR_START + (LR_MAX + LR_START) * (epoch \/ LR_RAMPUP_EPOCHS) ** 2.5\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS: # sustain lr\n        lr = LR_MAX\n    else: # cosine decay\n        epoch_diff = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        decay_factor = (epoch_diff \/ DECAY_EPOCHS) * math.pi\n        decay_factor= (tf.math.cos(decay_factor).numpy() + 1) \/ 2        \n        lr = LR_FINAL + (LR_MAX - LR_FINAL) * decay_factor\n\n    return lr","8fef4bac":"# plots the learning rate schedule\ndef show_lr_schedule(bs=BATCH_SIZE, epochs=EPOCHS):\n    rng = [i for i in range(epochs)]\n    y = [lrfn(x, bs=bs, epochs=epochs) for x in rng]\n    x = np.arange(epochs)\n    x_axis_labels = list(map(str, np.arange(1, epochs+1)))\n    print('init lr {:.1e} to {:.1e} final {:.1e}'.format(y[0], max(y), y[-1]))\n    \n    plt.figure(figsize=(30, 10))\n    plt.xticks(x, x_axis_labels, fontsize=16) # set tick step to 1 and let x axis start at 1\n    plt.yticks(fontsize=16)\n    plt.plot(rng, y)\n    plt.grid()\n    plt.show()\n    \nshow_lr_schedule()","9b070831":"def get_model():\n    # reset to free memory and training variables\n    tf.keras.backend.clear_session()\n    \n    with strategy.scope():\n        \n        net = efn.EfficientNetB4(\n            include_top=False,\n            weights='noisy-student',\n            input_shape=(IMG_TARGET_SIZE, IMG_TARGET_SIZE, N_CHANNELS),\n        )\n        \n        # freeze the batch normalisation layers\n        for layer in reversed(net.layers):\n            if isinstance(layer, tf.keras.layers.BatchNormalization):\n                layer.trainable = False\n            else:\n                layer.trainable = True\n        \n        # dropout to prevent overfitting\n        model = tf.keras.Sequential([\n            net,\n            tf.keras.layers.Dropout(0.25),\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dropout(0.25),\n            tf.keras.layers.Dense(N_LABELS, activation='softmax', dtype=tf.float32),\n        ])\n\n        # add metrics\n        metrics = [\n            tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n        ]\n\n        optimizer = tf.keras.optimizers.Adam()\n        loss = tf.keras.losses.CategoricalCrossentropy()\n\n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n        return model","8b249fde":"def show_validation_report_per_class(model, dataset, steps, name, bs):\n    print(f'--- {name} REPORT ---')\n    # classification report\n    y = np.ndarray(shape=steps * bs, dtype=np.uint16)\n    y_pred = np.ndarray(shape=steps * bs, dtype=np.uint16)\n    for idx, (images, labels) in tqdm(enumerate(dataset.take(steps)), total=steps):\n        with tf.device('cpu:0'):\n            y[idx*bs:(idx+1)*bs] = np.argmax(labels, axis=1)\n            y_pred[idx*bs:(idx+1)*bs] = np.argmax(model.predict(images).astype(np.float32), axis=1)\n            \n    print(classification_report(y, y_pred))\n    \n    # Confusion matrix\n    fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n    cfn_matrix = confusion_matrix(y, y_pred, labels=range(N_LABELS))\n    cfn_matrix = (cfn_matrix.T \/ cfn_matrix.sum(axis=1)).T\n    df_cm = pd.DataFrame(cfn_matrix, index=np.arange(N_LABELS), columns=np.arange(N_LABELS))\n    ax = sns.heatmap(df_cm, cmap='Blues', annot=True, fmt='.3f', linewidths=.5, annot_kws={'size':14}).set_title(f'{name} CONFUSION MATRIX')\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.xlabel('PREDICTED', fontsize=24, labelpad=10)\n    plt.ylabel('ACTUAL', fontsize=24, labelpad=10)\n    plt.show()","0f7de4cf":"def plot_history_metric(history, metric):\n    TRAIN_EPOCHS = len(history.history['loss'])\n    x = np.arange(TRAIN_EPOCHS)\n    x_axis_labels = list(map(str, np.arange(1, TRAIN_EPOCHS+1)))\n    val = 'val' in ''.join(history.history.keys())\n    # summarize history for accuracy\n    plt.figure(figsize=(20, 10))\n    plt.plot(history.history[metric])\n    if val:\n        plt.plot(history.history[f'val_{metric}'])\n    \n    plt.title(f'Model {metric}', fontsize=24)\n    plt.ylabel(metric, fontsize=20)\n    plt.yticks(fontsize=16)\n    plt.xlabel('epoch', fontsize=20)\n    plt.xticks(x, x_axis_labels, fontsize=16) # set tick step to 1 and let x axis start at 1\n    plt.legend(['train'] + ['test'] if val else [], loc='upper left')\n    plt.grid()\n    plt.show()","376331df":"print(f'TRAINING FOR {EPOCHS} EPOCHS WITH BATCH SIZE {BATCH_SIZE}\\n')\nprint(f'TRAIN IMAGES: {N_TRAIN_IMGS}, VAL IMAGES: {N_VAL_IMGS}\\n')\n\naugmentations_dic = dict({ 0: 'None', 1: 'MixUp', 2: 'CutMix', 3: 'GridMask' })\naugmentations = [2, 3] # use CutMix and GridMask\nepochs = EPOCHS\nMEAN_VAL_ACC = []\n    \nfor idx, fold in enumerate(range(N_FOLDS)):\n    # callbacks\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=1)\n    show_lr_schedule()\n    \n    # get the model\n    model = get_model()\n    \n    if idx is 0:\n        # model summary\n        model.summary()\n        # compute and variable data types\n        print(f'Compute dtype: {mixed_precision.global_policy().compute_dtype}')\n        print(f'Variable dtype: {mixed_precision.global_policy().variable_dtype}')\n        \n    print('\\n')\n    print(f'FOLD {fold}\\n')\n    print(f'augmenations: {\" AND \".join([augmentations_dic.get(i) for i in augmentations])}\\n')\n    \n    train_dataset = get_train_dataset(bs=BATCH_SIZE, fold=0, augmentations=augmentations)\n    val_dataset = get_val_dataset(bs=BATCH_SIZE_VAL, fold=0)\n\n    history = model.fit(\n        train_dataset,\n        steps_per_epoch = N_TRAIN_IMGS \/\/ BATCH_SIZE,\n\n        validation_data = val_dataset,\n        validation_steps = N_VAL_IMGS \/\/ BATCH_SIZE_VAL,\n\n        epochs = EPOCHS,\n        callbacks = [\n            lr_callback,\n        ],\n        verbose=1,\n    )\n    \n    # add val accuracy to list\n    MEAN_VAL_ACC.append(history.history['val_accuracy'])\n    \n    # plot training histories\n    plot_history_metric(history, 'loss')\n    plot_history_metric(history, 'accuracy')\n    \n    # show train and validation report\n    show_validation_report_per_class(model, val_dataset, N_VAL_IMGS \/\/ BATCH_SIZE_VAL, 'VALIDATION', BATCH_SIZE_VAL)\n    show_validation_report_per_class(model, train_dataset, N_TRAIN_IMGS \/\/ BATCH_SIZE, 'TRAIN', BATCH_SIZE)\n\n    # save the model\n    model.save_weights(f'model_fold_{fold}_weights.h5')\n    model.save(f'eff_b4_model_{fold}.h5')\n    \n    del model, train_dataset, val_dataset\n    gc.collect()","24ae59f8":"# CutMix implementation\n\nThis cutmix implementation differs from the one in version 3. This implementation is closer to the method described in the paper. A random mask whose size is defined by the beta destribution with $Beta(\\alpha, \\alpha)|\\alpha=1$. The original image will be masked with another image, the label will be updated accordingly to label and size of the masking image.","37810367":"# Validation Report","9df59eb9":"# Validation Dataset","58780541":"# MixUp implementation","b8095a9a":"Hello fellow Kagglers,\n\nThis notebook demonstrates how MixUp and CutMix augmentations are applied to achieve a cross validation accuracy of >0.90.\n\n\n**[MixUp paper](https:\/\/arxiv.org\/abs\/1710.09412)**\n\nMixUp combines two images, *a* and *b*, into one image. Each pixel is *x%* image *a* and *y%* image b, as is the label. The idea behind this augmentation method is to train the model on images with a label which lies in between two classes.\n\n**[CutMix paper](https:\/\/arxiv.org\/abs\/1905.04899)**\n\nCutMix also combines two images, *a* and *b*, and uses complete parts of different images to create a new image without overlap. For example, the left part of image *a* and the right part of image *b*, this results the label to be 0.50 label *a* + 0.50 label *b*. The idea behind this augmentation method is to roughly the same as with MixUp, train the model on a combination of 2 images with a mixed label. In contrast to MixUp, this method only uses original pixels and applies a regional dropout of the image as only a certain part of the original image is used.\n\n**[GridMask paper](https:\/\/arxiv.org\/pdf\/2001.04086.pdf)**\n\nGridMask is one of many image cutout methods, but it distinguishes itself by using a grid shaped mask, hence the name GridMask. The size of the grid is in a certain range and also positioned over the image with a random top and left offset.\n\nIf these augmentation methods sound abstract, don't worry, examples will be shown in this notebook.\n\nValidation is performed on a stratified kfold for n=5, thus 20% of the training data is used for validation with equal proportions of samples per class for the training and validation dataset.\nAlthough a cross validation accuracy of >0.90 is achieved the leaderboard score is 0.893, test time augmentation and comining multiple models from different fold could improve this score.\n\n\n**V4** Added GridMask and changed the batch size from 64 to 32. Also reduced the number of epochs from 25 to 15. Using only CutMix and GridMask augmentations, as this gave the best results. All together, LB score improved from 0.893 to 0.896.","dd59bf44":"This next function returns a random index from another image in the batch. This method performs better than using a random image with another label. This could be due to the fact the class inbalance is changed by choosing an image from another class. Images from the most dominant class, class 3, will be always mixed with an image from another class, thus the other classes will be more present in the training data.","c00aef3d":"# Training\n\nThis is the training loop, the training metrics and confusion matrix are displayed after each fold.\n\nFrom the validation report it can be observed the model predicts label 3 with very high precision and accuracy. This is not surprising as label 3 is by far the most common label and the model will therefore most likely get biased towards this label. Label 0 is the least common label and also has the lowest precision and recall. Label 0 is more than 10 times less common than label 3, making the dataset highly unbalanced.\n\nThe confusion matrix shows how the model mixes up labels. Label 0 is mostly confused with label 4 and 1. Moreover, label 2 is often confused with label 3.","60cd54cf":"# Model","a49d5672":"# Training History","de7dba7b":"This function shows the per image augmentation. This is the augmentation before CutMix or MixUp is applied ans shows how images can differ each epoch.","827f0cee":"# Batch Example\n\nThe next function plots examples of the final augmented images. The title of each image shows the RGB and label information.","e0ac0a47":"# TPU and  bfloat16 Configuration\n\nA bfloat16 is a 16 bits float with the range of a 32 bits float, but with a lower precision. Using a bfloat16 instead of a float32 reduces memory consumption and speeds up training and augmentation. The loss in numerical precision is in practice not a problem for machine learning models, as performance, in most cases, won't be affected by a loss of precision after the 3rd decimal number.\n\n**[Some background knowledge on bfloat16](https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/bfloat16-the-secret-to-high-performance-on-cloud-tpus)**","592559a7":"# Train Dataset\n\nA public dataset is used where jpegs are combioned into TFRecords. This allows for a faster data pipeline, as images do not have to be read one-by-one, but 1024 at a time. The original jpegs are used for data augmentation purposes. The original images are 800\\*600 pixels and each epoch a random square is used.","fda7adae":"# GridMask implementation","d01e35fe":"Improved dataset pipeline speed by adding a prefetch for the TFRecords samples and a static number of parallel calls for the batch augmentations","67f997e5":"# Learning Rate Scheduler\n\nThe learning rate used is a exponential warmup with cosine decay. The warmup is used to prevent the model from early overfitting on the first images. When the model starts learning the loss will be high as the model is trained on ImageNet, not on the training dataset. When starting with a high learning rate the model will learn the first few batches very well due to the high loss and could overfit on those samples. When starting with a very low learning rate the model will see all training images and make small adjustment to the weights and therefore learn from all training images equally when the loss is high and weights are modified strongly."}}