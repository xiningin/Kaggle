{"cell_type":{"cc6ff0e0":"code","8d67b48d":"code","dd931715":"code","06adb364":"code","805793a4":"code","4e59b51b":"code","2384905b":"code","bbfc298c":"code","db378048":"code","dadd4edf":"code","d9d53cf6":"code","4b852205":"code","15555100":"code","c94d2ff4":"code","55248a10":"code","ae6ba2bc":"code","0b141d7b":"code","87e28c8f":"code","fb02795f":"code","beab9bd1":"code","c0c6a7c6":"code","0f87ea1a":"code","6f93a22c":"code","53df17e0":"code","579d1129":"code","8e8e1815":"code","80d86fb8":"code","f576b8c9":"code","662ce811":"code","564c0797":"code","c43e1ab7":"code","59a624a8":"code","ca4d5242":"code","92b4690c":"code","d57eade4":"code","8259bf83":"code","4804b90d":"code","4af56b0b":"code","1a41639c":"code","9437eac7":"code","2f279a0c":"code","661bcc1b":"code","f188e7c3":"code","ef5385cb":"code","d9ccb85c":"code","61b3f677":"code","be228a77":"code","c3898ab8":"code","e29de50c":"code","8ac3d789":"code","1d74a805":"code","8da8a14f":"code","f55fca9c":"code","09c78d8a":"code","ba80e840":"code","c72e7436":"code","628aeb6b":"code","0362f4bb":"code","88cf152b":"code","602d1497":"code","0efaa6a8":"code","63d7d740":"code","a86e999d":"code","bfb73d9f":"code","8c6dcd3e":"code","1255f30c":"code","0b92d528":"code","937df725":"code","f3376373":"code","e05fe135":"code","79e5e4fe":"code","8eff5d83":"code","48f29a66":"code","92b3bc5b":"code","18c84218":"code","383a9750":"code","e830b1fe":"code","cb26fbc6":"code","60617786":"code","aee8e488":"code","f440c5c0":"code","b4342767":"code","2da3464a":"code","74840064":"code","02e434cb":"code","4f08c157":"code","c8d42124":"code","910a6e98":"code","e87b73e7":"code","258155f1":"markdown","26596dd2":"markdown","47d3dfc5":"markdown","dcc8c87b":"markdown","644c4077":"markdown","aab5bcb4":"markdown","19e535bd":"markdown","f6c4161a":"markdown","05200ec2":"markdown","2a07b8b2":"markdown","b40178c1":"markdown","ff0ba011":"markdown","af075bdb":"markdown","f5a28967":"markdown","3ffd6017":"markdown","f2755b6f":"markdown","cd79db60":"markdown","f3dbd6f6":"markdown","65cf0b64":"markdown","6bb345ae":"markdown","a6dd8e44":"markdown","68a2cc9d":"markdown","b86eb9f1":"markdown","0d70003a":"markdown","159b254b":"markdown","65978bbb":"markdown","dda490da":"markdown","ce31e0de":"markdown","177d05cd":"markdown","82aca9e5":"markdown","bc5ef5ec":"markdown","5faa4f77":"markdown","ea01969c":"markdown","1b113d17":"markdown","52f2bc8f":"markdown","8313d799":"markdown","9715e778":"markdown","5fa6631f":"markdown","51409267":"markdown","35ba93ff":"markdown","c24faf1e":"markdown","947ddb53":"markdown","fa7614e7":"markdown","69a15472":"markdown","08a087fb":"markdown","4f8a0850":"markdown","2db8739e":"markdown","cead18d1":"markdown","b0e0a22d":"markdown","0f5cc844":"markdown","e29d8702":"markdown","9b94aa05":"markdown","167605a0":"markdown","2bcc97fe":"markdown","a5cf8bff":"markdown","7c111a6d":"markdown","0acab540":"markdown","1584f095":"markdown","51f82a2d":"markdown","416c971a":"markdown","b14b3983":"markdown","0c102643":"markdown","0923b8e3":"markdown","b76bd1e1":"markdown","97ad2759":"markdown","01c72f95":"markdown","bbea5b21":"markdown","c609b117":"markdown","64d71369":"markdown","27e842fd":"markdown","71268c8b":"markdown","bb0b349d":"markdown","2c5119f0":"markdown","f120e71f":"markdown","5db49e92":"markdown","1816bbc7":"markdown","3549ce38":"markdown","e79c4c77":"markdown"},"source":{"cc6ff0e0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\n#plt.style.available (you can use this code to learn different template of seaborn library, I will use whitegrid)\nimport plotly.express as px\n\nfrom collections import Counter\n\n#to close warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8d67b48d":"train_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest_PassengerId = test_df[\"PassengerId\"]","dd931715":"train_df.describe()","06adb364":"#train_df.columns","805793a4":"#train_df.info()","4e59b51b":"fig, axarr =plt.subplots(2,3, figsize=(30, 10), sharey=True)\n\nsns.countplot(train_df['Survived'], ax=axarr[0][0])\nsns.countplot(train_df['Sex'], ax=axarr[0][1])\nsns.countplot(train_df['Pclass'], ax=axarr[0][2])\nsns.countplot(train_df['Embarked'], ax=axarr[1][0])\nsns.countplot(train_df['SibSp'], ax=axarr[1][1])\nsns.countplot(train_df['Parch'], ax=axarr[1][2])\nfig.show()","2384905b":"fig, ax =plt.subplots(1,2, figsize=(30, 10), sharey=False)\n\nsns.distplot(train_df['Age'], ax=ax[0])\nsns.distplot(train_df['Fare'], ax=ax[1])\nfig.show()\n","bbfc298c":"train_df[['Sex','Survived']].groupby(['Sex'],as_index = False).mean().sort_values(by='Survived',ascending=False)","db378048":"fig = px.sunburst(train_df, path=['Sex', 'Survived'], values='PassengerId')\nfig.show()","dadd4edf":"train_df[['Pclass','Survived']].groupby(['Pclass'],as_index = False).mean().sort_values(by='Survived',ascending=False)","d9d53cf6":"g = sns.factorplot(x = \"Pclass\", y = \"Survived\", data = train_df, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","4b852205":"train_df[['Embarked','Survived']].groupby(['Embarked'],as_index = False).mean().sort_values(by='Survived',ascending=False)","15555100":"train_df[['SibSp','Survived']].groupby(['SibSp'],as_index = False).mean().sort_values(by='Survived',ascending=False)","c94d2ff4":"g = sns.factorplot(x = \"SibSp\", y = \"Survived\", data = train_df, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","55248a10":"train_df[['Parch','Survived']].groupby(['Parch'],as_index = False).mean().sort_values(by='Survived',ascending=False)","ae6ba2bc":"g = sns.factorplot(x = \"Parch\", y = \"Survived\", kind = \"bar\", data = train_df, size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","0b141d7b":"#train_df.Age.value_counts(ascending=False, bins=4)\n# I use this code try to find right bins number.","87e28c8f":"bin = [0,10,20,40,60,80] \n#use pd.cut function can attribute the values into its specific bins \ncategory = pd.cut(train_df.Age,bin) \ncategory = category.to_frame() \ncategory.columns = ['AgeRange'] \n#concatenate age and its bin \ndf_new = pd.concat([train_df,category],axis = 1) ","fb02795f":"df_new[['AgeRange','Survived']].groupby(['AgeRange'],as_index = False).mean()","beab9bd1":"g = sns.FacetGrid(train_df, col = \"Survived\")\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","c0c6a7c6":"train_df.Fare.value_counts(ascending=False, bins=6)\n# I use this code try to find right bins number.","0f87ea1a":"bin = [ 0,40,80,170, 255,340,513] \n#use pd.cut function can attribute the values into its specific bins \ncategory = pd.cut(train_df.Fare,bin) \ncategory = category.to_frame() \ncategory.columns = ['FareRange'] \n#concatenate age and its bin \ndf_new = pd.concat([train_df,category],axis = 1) ","6f93a22c":"df_new[['FareRange','Survived']].groupby(['FareRange']).mean()","53df17e0":"g = sns.FacetGrid(train_df, col = \"Survived\", row = \"Pclass\", size = 3)\ng.map(plt.hist, \"Age\", bins = 25)\ng.add_legend()\nplt.show()","579d1129":"g = sns.FacetGrid(train_df, row = \"Embarked\", size = 3)\ng.map(sns.pointplot, \"Pclass\",\"Survived\",\"Sex\")\ng.add_legend()\nplt.show()","8e8e1815":"g = sns.FacetGrid(train_df, row = \"Embarked\", col = \"Survived\", size = 2.3)\ng.map(sns.barplot, \"Sex\", \"Fare\")\ng.add_legend()\nplt.show()","80d86fb8":"#function that finds outliers\n\ndef outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2) #to find if sample has 2 or more outliers\n    \n    return multiple_outliers","f576b8c9":"#detect outliers\ntrain_df.loc[outliers(train_df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])]","662ce811":"# drop outliers\ntrain_df = train_df.drop(outliers(train_df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]),axis = 0).reset_index(drop = True)","564c0797":"train_df_len = len(train_df) # not to lose train data set\ntrain_df = pd.concat([train_df,test_df],axis = 0).reset_index(drop = True)","c43e1ab7":"#finding missing values\ntrain_df.columns[train_df.isnull().any()]","59a624a8":"train_df.isnull().sum() # it shows how many missing values we have each column","ca4d5242":"train_df[train_df[\"Embarked\"].isnull()] #the rows Embarked has missing values.","92b4690c":"import plotly.express as px\n\nfig = px.box(train_df, x=\"Embarked\", y=\"Fare\",template='plotly_white' )\nfig.show()","d57eade4":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"S\")\ntrain_df[train_df[\"Embarked\"].isnull()] #to check","8259bf83":"\n\nfig = px.box(train_df, x=\"Fare\",template='plotly_white' )\nfig.show()","4804b90d":"train_df[train_df[\"Fare\"].isnull()]","4af56b0b":"np.mean(train_df[train_df[\"Pclass\"] == 3])","1a41639c":"train_df[\"Fare\"] = train_df[\"Fare\"].fillna(24.816367)","9437eac7":"train_df[train_df[\"Fare\"].isnull()] #to check","2f279a0c":"train_df[train_df[\"Age\"].isnull()]","661bcc1b":"sns.factorplot(x = \"Sex\", y = \"Age\", hue = \"Pclass\",data = train_df, kind = \"box\")\nplt.show()","f188e7c3":"sns.factorplot(x = \"Parch\", y = \"Age\", data = train_df, kind = \"box\")\nsns.factorplot(x = \"SibSp\", y = \"Age\", data = train_df, kind = \"box\")\nplt.show()","ef5385cb":"train_df.head()","d9ccb85c":"train_df[\"Sex\"] = [1 if i == \"male\" else 0 for i in train_df[\"Sex\"]] # to convert categorical sex variable to numerical one.","61b3f677":"sns.heatmap(train_df[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(), annot = True)\nplt.show()","be228a77":"index_nan_age = list(train_df[\"Age\"][train_df[\"Age\"].isnull()].index) #make list that inludes all mising values in age column \nfor i in index_nan_age: # lets fill these values under the below conditions\n    age_pred = train_df[\"Age\"][((train_df[\"SibSp\"] == train_df.iloc[i][\"SibSp\"]) &(train_df[\"Parch\"] == train_df.iloc[i][\"Parch\"])& (train_df[\"Pclass\"] == train_df.iloc[i][\"Pclass\"]))].median()#fill same median with Parch,Pclass and SibSp variables are same\n    age_med = train_df[\"Age\"].median()# if not find any same variable, use age median\n    if not np.isnan(age_pred):\n        train_df[\"Age\"].iloc[i] = age_pred\n    else:\n        train_df[\"Age\"].iloc[i] = age_med","c3898ab8":"train_df['CabinN'] = train_df['Cabin'].str[:1] #first I extracted first letter of Cabin","e29de50c":"f = sns.countplot(x=\"CabinN\", data = train_df)\nplt.show()","8ac3d789":"train_df['Deck'] = train_df['CabinN']\ntrain_df['Deck'] = train_df['Deck'].fillna('M')","1d74a805":"train_df['Deck'] = train_df['Deck'].replace(['A', 'B', 'C'], 'ABC')\ntrain_df['Deck'] = train_df['Deck'].replace(['D', 'E'], 'DE')\ntrain_df['Deck'] = train_df['Deck'].replace(['F', 'G'], 'FG')\n\ntrain_df['Deck'].value_counts()\n","8da8a14f":"train_df.drop(['Cabin','CabinN'],axis=1, inplace=True)","f55fca9c":"train_df.info()","09c78d8a":"train_df[\"Name\"].head(10)","ba80e840":"# example to understand separation:\n#s = \" Allen, Mr. William Henry\"\n#s.split(\".\") # this separete with dot\n#s.split(\".\")[0] # and choose first one like 'Allen, Mr'\n#s.split(\".\")[0].split(',') # and we separe new object with comma, so we have 'Allen' and ' Mr'\n#s.split(\".\")[0].split(',')[-1] # and we choose last object of it, it is : ' Mr'\n#s.split(\".\")[0].split(',')[-1].strip() # and we use .strip() if we have any space to get rid of it.","c72e7436":"name = train_df[\"Name\"]\ntrain_df[\"Title\"] = [i.split(\".\")[0].split(\",\")[-1].strip() for i in name]","628aeb6b":"train_df.Title.value_counts()","0362f4bb":"train_df[\"Title\"] = train_df[\"Title\"].replace([\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Jonkheer\",\"Dona\"],\"other\")\ntrain_df[\"Title\"] = [0 if i == \"Master\" else 1 if i == \"Miss\" or i == \"Ms\" or i == \"Mlle\" or i == \"Mrs\" or i == \"Lady\" or i == \"the Countess\"  else 2 if i == \"Mr\" or i == \"Sir\" else 3 for i in train_df[\"Title\"]]","88cf152b":"f = sns.countplot(x=\"Title\", data = train_df)\nf.set_xticklabels([\"Master\",\"Female's\",\"Male's\",\"Other\"])\n\nplt.show()","602d1497":"g = sns.factorplot(x = \"Title\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_xticklabels([\"Master\",\"Female's\",\"Male's\",\"Other\"])\ng.set_ylabels(\"Survival Probability\")\nplt.show()","0efaa6a8":"train_df.drop(labels = [\"Name\"], axis = 1, inplace = True)","63d7d740":"train_df = pd.get_dummies(train_df,columns=[\"Title\"])","a86e999d":"train_df[\"Fsize\"] = train_df[\"SibSp\"] + train_df[\"Parch\"] + 1","bfb73d9f":"g = sns.factorplot(x = \"Fsize\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","8c6dcd3e":"train_df[\"family_size\"] = [1 if i < 5 else 0 for i in train_df[\"Fsize\"]]","1255f30c":"train_df = pd.get_dummies(train_df, columns= [\"family_size\"])","0b92d528":"train_df = pd.get_dummies(train_df, columns=[\"Embarked\"])","937df725":"train_df[\"Ticket\"].head()","f3376373":"tickets = [] # new empty list\nfor i in list(train_df.Ticket): # to look each data in Ticket column\n    if not i.isdigit(): # if data is not just a digit number\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0])# split with '.' and '\/' and drop all space than append the list the first item\n    else: # if just a digit number, write 'x'\n        tickets.append(\"x\")\ntrain_df[\"Ticket\"] = tickets","e05fe135":"sns.countplot(x=\"Ticket\", data = train_df)\nplt.xticks(rotation = 90)\nplt.show()","79e5e4fe":"g = sns.factorplot(x = \"Ticket\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.xticks(rotation = 90)\nplt.show()","8eff5d83":"train_df = pd.get_dummies(train_df, columns= [\"Ticket\"], prefix = \"T\")# prefix = 'T' means that each dummy column begins with 'T' not 'Tickets'","48f29a66":"train_df[\"Pclass\"] = train_df[\"Pclass\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns= [\"Pclass\"])","92b3bc5b":"train_df[\"Sex\"] = train_df[\"Sex\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns=[\"Sex\"])","18c84218":"train_df.head()","383a9750":"train_df = pd.get_dummies(train_df, columns= [\"Deck\"])","e830b1fe":"train_df.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)","cb26fbc6":"\ntrain_df['Infants'] = train_df['Age'].apply(lambda x: 1 if x < 1 else 0)\ntrain_df['Child'] = train_df['Age'].apply(lambda x: 1 if x < 18 else 0)\ntrain_df['Adult'] = train_df['Age'].apply(lambda x: 1 if x >= 18 and x < 65 else 0)\ntrain_df['Elderly'] = train_df['Age'].apply(lambda x: 1 if x >= 65 else 0)\n ","60617786":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier","aee8e488":"train_df_len","f440c5c0":"test = train_df[train_df_len:]\ntest.drop(labels = [\"Survived\"],axis = 1, inplace = True)","b4342767":"train = train_df[:train_df_len]\nX_train = train.drop(labels = \"Survived\", axis = 1)\ny_train = train[\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.33, random_state = 42)\nprint(\"X_train\",len(X_train))\nprint(\"X_test\",len(X_test))\nprint(\"y_train\",len(y_train))\nprint(\"y_test\",len(y_test))\nprint(\"test\",len(test))","2da3464a":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nacc_log_train = round(logreg.score(X_train, y_train)*100,2) #to see percentage of regression score, we multiple 100 and getting two decimal\nacc_log_test = round(logreg.score(X_test,y_test)*100,2)\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_test))","74840064":"random_state = 42 # random state model number, we choose exact number to find same result each of run\n#our models;\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC(random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier(),\n             XGBClassifier(random_state=random_state),\n             GradientBoostingClassifier(random_state=random_state),\n             AdaBoostClassifier(random_state=random_state)]\n\n# decision tree parameters\ndt_param_grid = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\n\n#support vector classifier parameters\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"C\": [1,10,50,100,200,300,1000]}\n\n#random forest parameter\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,5,30],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,600],\n                \"criterion\":[\"entropy\"]}\n\n#logistic regression parameters\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\n#k-nearest neighboor parameters\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\n\nxgb_param_grid = {\"max_depth\": range(1,20,2)}\n\ngbc_param_grid = {\"n_estimators\":[10,30]}\n\nadab_param_grid = {\"n_estimators\":[10,30]}\n\n\n\n\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid,\n                   xgb_param_grid,\n                   gbc_param_grid,\n                   adab_param_grid]\n","02e434cb":"cv_result = [] # cross validation result: it will add each result\nbest_estimators = [] #choosing best results\nfor i in range(len(classifier)):  #this loop try to choose best results according to above models\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1) # verbose show us all results when code is running\n    clf.fit(X_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])","4f08c157":"cv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \"ML Models\":[\"DecisionTreeClassifier\", \"SVM\",\"RandomForestClassifier\",\n             \"LogisticRegression\",\n             \"KNeighborsClassifier\", \"XGBClassifier\", \"GradientBoostingClassifier\", \"AdaBoostClassifier\"]})\n\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")","c8d42124":"print(cv_results)","910a6e98":"votingC = VotingClassifier(estimators = [(\"rfc\",best_estimators[2]),(\"gb\",best_estimators[6])],voting = \"soft\", n_jobs = -1)\nvotingC = votingC.fit(X_train, y_train)\nprint(accuracy_score(votingC.predict(X_test),y_test))","e87b73e7":"test_survived = pd.Series(votingC.predict(test), name = \"Survived\").astype(int)\nresults = pd.concat([test_PassengerId, test_survived],axis = 1)\nresults.to_csv(\"submission.csv\", index = False)","258155f1":"Interestingly, people who embarked Cherbourg port has more chance to survival.","26596dd2":"**Age:** Age distrubition looks like normal distribution except a youngest age interval.  Generally, it looks like the mean of it is similar to the mod of it and it is about in the 20-40 interval.\n**Fare:** These variables have density of the lowest fare value,and it reminds me gamma distribution. ","47d3dfc5":"Our available Data Length(X_train + X_test + test): 590 + 291 + 418=1299 <br>\nTesting (test) : 418 <br>\nour new available Data(X_train + X_test): 590 + 291=881 <br>\nValidation(X_test): 291 <br>","dcc8c87b":"loading libraries that are needed for prediction","644c4077":"<a id = '6'><\/a><br>\n### Numerical Variables:\n  * float64: PassengerId, Age, Fare\n\n\nFor numerical variables, firstly I wonder distrubution of them, so I will use histogram for it. However, passangerid is unique variable for each passenger, I will not use this variable, this is just important to show our predicted values for Submission, so we can forget it for now.","aab5bcb4":"> ### Pclass & Sex","19e535bd":"### Age","f6c4161a":"Generally,female passengers have higher survival rate than males however males who embarked C port have better survival rate than women.","05200ec2":"Age distrubition no show clear pattern to use filling missing values. However, according to classes, each classes have different median age so we can use that to fill missing values. like;\n* 1 class:40\n* 2 class:30\n* 3 class:25","2a07b8b2":"### Ticket","b40178c1":"As you can see, the highest correlation is between age and Pclass, also there is correlation between Parch, Sibsp and Age. however, age and sex correlation is near to zero. we fill age missing values according to this correlation matrix. If we have enough correlation, fill the median age of this category, so on so forth.","ff0ba011":"we dropped all outliers if it has two or more outliers. we neglected if sample has just one outliers.","af075bdb":"## Cabin Missing Values","f5a28967":"passenger who paid higher fare had higher chance to survive. Fare can be converted to categorical variable to use in analyse.","3ffd6017":"#### Split our test data","f2755b6f":"Firstly looking at some information in [google](https:\/\/www.encyclopedia-titanica.org\/cabins.html), this web site examine all cabin codes detailly and I realized that filling cabin information is very challenging, no enough information about that, lets dive in !","cd79db60":"We decided to use models that is higher score than 80%.","f3dbd6f6":"Lets look at age distribution according to survived and pclass to see clearly relation.","65cf0b64":"<a id = '7'><\/a><br>\n# Some Remarkable Probabilities","6bb345ae":"### Family Size","a6dd8e44":"lets look Embarked and Fare missing values:","68a2cc9d":"### Embarked and Fare missing values","b86eb9f1":"Cabin has no show remarkable change with other variables, However we know that A, B, C and T are belong to first class. To simplize it, I changed all missing value as 'M', all A,B,C as ABC, and D, E as 'DE' and F, G as 'FG'.So I added one more feature in my analysis.","0d70003a":"As you see, all names have titles like Mr or Mrs, can we extract these titles to look at relations with survival category. So, all titles in the second place after the comma and all have a dot,  let separete according to this order.","159b254b":"It looks like having zero parent, children or siblings are nearly 25-30 years old. Additionally, having 3 or more children has older age than having 3 or more siblings. People who had one or two parents are younger than people who have one or two siblings or spouse. Lets look at corralation between these variables:\n","65978bbb":"If you find useful please vote up! , \nPlease let me know if you have any questions, comments, or suggestions!","dda490da":"* Age: 256 missing value\n* Fare: 1 missing value\n* Cabin: 1007 missing value\n* Embarked: 2 missing value\n\nwe will start with Embarked and Fare because they are little bit easier than others because just have 1 and 2 missing values.","ce31e0de":"to find age interval probabilities, I used 'pd.cut' and calculated probabilities of survival. It looks like young age interval has higher chance to survival. we have missing values in age. We can use this distrubution to fill missing values.","177d05cd":"to sum SibSp and Parch, we can find exact family size, and to count passenger herself\/himself, we have to add one.lets do it;","82aca9e5":"<a id = '5'><\/a><br>\n### **Polytomous Variables;** <br> \nCategorical variables with more than two possible values\n\n**Pclass:** As you know, third class is the cheapest class and the number of them is the highest. However, interestingly, the number of sceond class is lower than first class. Personelly, I had expected that first class numbers had minimum.\n\n**Embarked:** As you can see, many people embarked on Southampton port. I wonder if these are that people who have third class ticket. we will examine it later.\n\n**SibSp:** The majortiy of passangers did not have any siblings or spouse in the ship.\n\n**Parch:** like SibSp variable; The majortiy of passangers did not have any parents or children in the ship.\n\nLastly, we did not plot Name, Ticket and Cabin variables because these have so many categories, we will use these in analysis differently.","bc5ef5ec":"Passengers who had five or more children had died. We can merge Sibsp and Parch and extract new feature with threshold=3. ","5faa4f77":"to find all missing values include test data frame we concat test and train data frames:","ea01969c":"<a id = '8'><\/a><br>\n# Missing Values and Outliers","1b113d17":"### Embarked","52f2bc8f":"<a id = '10'><\/a><br>\n# MODEL","8313d799":"### Passenger ID & Cabin","9715e778":"Even if number of passengers in 3 class is higher than others, survival numbers for third class lower than others. It is important information.","5fa6631f":"Passenger ID is unique number for each passenger so it is meanningless for analysis and Cabin has lots of missing value and I can find any logic to fill them. So I will drop them.","51409267":"### Train Test Split","35ba93ff":"Passengers who had five or more siblings had died. Having between zero and two SibSp means more survival chance so we can extract a new feature for this.","c24faf1e":"We set new title column as 1,2,3 and 4 , however actually, these are not show any degree of title, so we have to use this colum with dummies. Lets convert this column with dummies.","947ddb53":"We use soft voting, because hard voting uses just 'survive or not survive' condition while soft voting uses probability of survive.","fa7614e7":"Some ticket numbers have string values, like 'A\/5', 'PC', these will be a clue for survival, so lets get these codes and extract new variable;","69a15472":"<a id = '1'><\/a><br>\n# Load and Check Data","08a087fb":"1. PassengerId: unique id number to each passenger\n1. Survived: passenger survive(1) or died(0)\n1. Pclass: passenger ticket class\n1. Name: passenger name\n1. Sex: gender of passenger\n1. Age: age of passenger \n1. SibSp: number of siblings\/spouses\n1. Parch: number of parents\/children \n1. Ticket: ticket number\n1. Fare: ticket money as a dolar\n1. Cabin: cabin category\n1. Embarked: port where passenger embarked(C=Cherbourg, Q=Queenstown, S=Southampton)","4f8a0850":"We have lots of Mr, Miss, Mrs and Master titles and the others have relatively small numbers. Let categorize them like female titles, male titles, master and others.","2db8739e":"we concanated train and test before , and we calculate train data frame length. Now, we split again according to this len. And as you remember, our survival column in test dataframe have filled with NaN values because of concanatation.So,  We will drop survival column also.","cead18d1":"####  Model Components\n![image.png](attachment:image.png)","b0e0a22d":"### Prediction and Submission","0f5cc844":"Note that we use median variables to fill missing values because there are some outliers have, and we donot want mean affect our data distribution.","e29d8702":"<a id = '9'><\/a><br>\n# Feature Engineering","9b94aa05":"we have new column named Fsize, lets look at survival probablities of them;","167605a0":"We donot do any convertion method for Pclass and Sex features, but These also are had to covert dummies.","2bcc97fe":"As you can see, the survival chance of a woman is higher than a man.","a5cf8bff":"We detailly examined this variable before. We donot extract any new column for it, however because embarked column categorical and doesnot have any degree, we have to convert it dummies.","7c111a6d":"<a id = '11'><\/a><br>\n## Simple Logistic Regression","0acab540":"These two rows have fare of 80 dolar. And, when we look at fare and embarked boxplot, only C port covers 80dolar fare. Even if filling these nan values with 'C' seems to logical, when [I googled ](https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/martha-evelyn-stone.html)the name of passengers, I found that they embarked from Southampton port so I will fill with 'S'.","1584f095":"<a id = '3'><\/a><br>\n### Categorical Variables: \n   * object: Name, Sex, Ticket, Cabin, Embarked\n   * float64: Survived\n   * int64: Pclass, SibSp, Parch\n\nAs you can see clearly, some categorical variables are given integer or float, these are ready for analysis. However, the object ones needs to convert integer type. Before do that, lets look at the categorical variables closely:","51f82a2d":"![image.png](attachment:image.png)","416c971a":"### Name and Title Relation","b14b3983":"we will try to tune **best hyperparameters** with **grid search method**. to evaluate these results, we will use **stratified cross validation**.","0c102643":"<a id = '12'><\/a><br>\n## Comparision of Decision Tree & SVM & KNN & Logistic Regression & XGBoost & Gradient Boost & Ada Boost","0923b8e3":"## Age missing values","b76bd1e1":"<a id = '13'><\/a><br>\n# Conclusion","97ad2759":"While, the first graph show the number of titles, the second one shows that probabilty of survivals. And clearly seen that, even if male's title so many, the survival rate of it is very low. <br>\nwe will use title column for analysis so we donot need name column anymore, lets drop it.","01c72f95":"### Visualize scores","bbea5b21":"Even if there are lots of x value, survival probability of x is not high. Lets convert dummies to this column too;","c609b117":"<a id = '13'><\/a><br>\n# Ensemble Modeling","64d71369":"## Outliers","27e842fd":"Because test data no have 'Survived' column, it looks like that it have missing value but actually, there is no missing value in 'Survived' column for train data set.","71268c8b":"# Introduction\n\n[Sinking of the Titanic](https:\/\/en.wikipedia.org\/wiki\/Sinking_of_the_Titanic)\n\n<font color = 'blue'>\nContent: \n    \n1. [Load and Check Data](#1)\n1. [Exploratory Data Analysis](#2)\n    * [Categorical Variables](#3)\n       * [Binary(Dichotomous) Variables](#4)\n       * [Polytomous Variables](#5)\n    * [Numerical Variables](#6)\n1. [Some Remarkable Probabilities](#7)\n1. [Missing Values and Outliers](#8)\n1. [Feature Engineering](#9)\n1. [Model](#10)\n    * [Simple Logistic Regression](#11)\n    * [Comparision of Decision Tree & SVM & KNN & Logistic Regression & XGBoost & Gradient Boost & Ada Boost](#12)\n    * [Ensemble Modeling](#13)\n1. [Conclusion](#14)\n    ","bb0b349d":"<a id = '4'><\/a><br>\n### **Binary(Dichotomous) Variables;** <br> \nThese types of variables have just two possible results.<br>\n\n**Survived :** as you can see first graph, the majority of passengers have died. It shows that this is an unbalanced data, zero counts are higher than ones.<br>\n**Sex:** This is also an unbalanced data, we can say that passengers are nearly 75% male, 25% female.\n\n","2c5119f0":"## Missing Values","f120e71f":"People who had first class ticket had higher survival probability than the other classes.","5db49e92":"<a id = '2'><\/a><br>\n# Exploratory Data Analysis\n\n* ## Variable Description","1816bbc7":"It looks like while care is increasing, survival rate is also increasing but it is just a guess.","3549ce38":"We just have one missing value in Fare. to not change distribution of Fare, fill missing value with median(Because there are some outliers) of it will be good idea. To predict more accuarete, I  will use  third pclass of passengers median age.","e79c4c77":"After family size reached 4, survival rate decreases dramatically, so we can split this data set in two as high survival probability and low survival probablity and also convert this new column to dummies."}}