{"cell_type":{"a7fa4239":"code","ba3c3dcd":"code","01afc8ce":"code","f0cee354":"code","1cdb0d62":"code","9b9b2a18":"code","f7e2c825":"code","6b1d3525":"code","590d6ff2":"code","08fc4b7b":"code","483eb44e":"code","4a750c8d":"code","afd8fc5e":"code","405a53c7":"code","bc50abee":"code","aa277fda":"code","11a042be":"code","3b33a97d":"code","4ecf4429":"code","a7365404":"code","6728ad1e":"code","55262c09":"code","4cb257fc":"code","4c5350cf":"markdown","6a82acf5":"markdown","29616bf9":"markdown","f833932d":"markdown","7aab92a8":"markdown","2a297b7b":"markdown","62da6ca1":"markdown","6069d4a6":"markdown","16dad8d6":"markdown","d871015c":"markdown","2f08e46c":"markdown","f585003b":"markdown","c51f0473":"markdown","90e946e2":"markdown","22b30bd5":"markdown","32593717":"markdown","7e6feb2e":"markdown","e6fad4f3":"markdown","1d087016":"markdown","e7a942ce":"markdown","78d7225e":"markdown","36cfa4f9":"markdown","e8018c64":"markdown","f0473f7b":"markdown","17877491":"markdown","867af077":"markdown","11820bd7":"markdown","063e7bf3":"markdown"},"source":{"a7fa4239":"import numpy as np\nimport pandas as pd","ba3c3dcd":"dataset = pd.read_csv(\"\/kaggle\/input\/google-stock-price\/Google_Stock_Price_Train.csv\")","01afc8ce":"dataset.isnull().sum()","f0cee354":"dataset.shape","1cdb0d62":"dataset.head()","9b9b2a18":"data = dataset.iloc[:, 1:2].values","f7e2c825":"print(data[:5])","6b1d3525":"data.shape","590d6ff2":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\ndata = scaler.fit_transform(data)\n","08fc4b7b":"X_train = []\ny_train = []\nfor i in range(50, 1258):\n    X_train.append(data[i-50:i, 0])\n    y_train.append(data[i, 0])\nX_train, y_train = np.array(X_train), np.array(y_train)\n","483eb44e":"X_train.shape","4a750c8d":"X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))","afd8fc5e":"X_train.shape","405a53c7":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import SimpleRNN\nfrom keras.layers import Dropout","bc50abee":"regressor = Sequential()\nregressor.add(SimpleRNN(units=50,return_sequences = True,input_shape = (X_train.shape[1],1)))\nregressor.add(Dropout(0.2))\nregressor.add(SimpleRNN(units = 50,return_sequences = True))\nregressor.add(Dropout(0.2))\nregressor.add(SimpleRNN(units = 50,return_sequences = True))\nregressor.add(Dropout(0.2))\nregressor.add(SimpleRNN(units = 50))\nregressor.add(Dropout(0.2))\nregressor.add(Dense(units = 1))","aa277fda":"regressor.summary()","11a042be":"regressor.compile(optimizer = 'adam',loss = 'mean_squared_error')","3b33a97d":"regressor.fit(X_train,y_train,epochs = 10, batch_size = 32)\n","4ecf4429":"X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))","a7365404":"X_train.shape","6728ad1e":"import torch.nn as nn\nimport torch\nfrom torch.autograd import Variable","55262c09":"INPUT_SIZE = 50\nHIDDEN_SIZE = 40\nNUM_LAYERS = 2\nOUTPUT_SIZE = 1","4cb257fc":"class RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super(RNN, self).__init__()\n\n        self.RNN = nn.RNN(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers\n        )\n        self.out = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, h_state):\n        r_out, hidden_state = self.RNN(x, h_state)\n        \n        hidden_size = hidden_state[-1].size(-1)\n        r_out = r_out.view(-1, hidden_size)\n        outs = self.out(r_out)\n\n        return outs, hidden_state\n\nRNN = RNN(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE)\n\noptimiser = torch.optim.Adam(RNN.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\n\nhidden_state = None\n\nfor epoch in range(100):\n    inputs = Variable(torch.from_numpy(X_train).float())\n    labels = Variable(torch.from_numpy(y_train).float())\n\n    output, hidden_state = RNN(inputs, hidden_state) \n\n    loss = criterion(output.view(-1), labels)\n    optimiser.zero_grad()\n    loss.backward(retain_graph=True)                     # back propagation\n    optimiser.step()                                     # update the parameters\n    \n    print('epoch {}, loss {}'.format(epoch,loss.item()))\n","4c5350cf":"Checking if there are any null values","6a82acf5":"Viewing the dataset","29616bf9":"Now we will compile the model. We used the adam optimizer and the loss function is mean_squared_error as we have a regression problem.","f833932d":"We first create an instance of the Sequential class, which is the class of Keras, the Sequential model is a linear stack of layers.","7aab92a8":"Now we scale the data into a range of -1 and 1 using the scikit-learn library","2a297b7b":"Now we start building our model.","62da6ca1":"We start by loading necessary data pre-processing and computation libraries.","6069d4a6":"Now we train or fit the model. We would train for 100 epochs, with a batch size of 32.","16dad8d6":"# LSTM Implementation","d871015c":"We check the shape of X_train","2f08e46c":"We stack up these layers.\n","f585003b":"Now we convert the data into sequences. This is done as the RNN layer only accepts in this form. We take the open price of the first 50 days into X_train and then the 51th day's open price is stored in y_train. We repeatedly performed this and store them into a numpy arrays X_train and y_train.","c51f0473":"Then we add to the model which named as regressor an LSTM layer. This layer has 50 units or cells of LSTM, we set return_sequences TRUE to return_sequences TRUE to tell that the LSTM cell need to return the last state, so that it can be used in the next cell. Then we tell the shape of thee input sequence that we would be giving to the layer.","90e946e2":"But the RNN does not accept this type of shape. So we need to reshape this so that it is suitable for RNN.","22b30bd5":"Now we get the summary of the model that we just built.","32593717":"Now we start the building our model. We start by importing the necessary libraries.[](http:\/\/)","7e6feb2e":"Then we add a dropout in order to prevent overfitting.","e6fad4f3":"# RNN Implementation in Keras\nWe are given the opening prices of the google stock for about 1000 days. Our task is to predict the opening prices of the following days.","1d087016":"Check dataset shape","e7a942ce":"Next we add a Dropout layer and finally add a Dense Layer to get the output.","78d7225e":"So this is the new shape.","36cfa4f9":"In this we create a network composed of multiple LSTM cells stacked up together. The number of LSTM cells is defined by num_layers. The nn.Linear layer is the final layer that gives us the output.\nIn the forward function we perform the forward propogation of the network. First we store the output and the hidden state of the LSTM in r_out and hidden_state. Then we compute the hidden_size, as this would be used later. Now we need to get the output. We only take the output from the final timetep. So we need to take out the last hidden sate. We then pass it to the Linear layer and get the ouput.\n\nNow we will train the network. We train it for 100 epochs.\nFirst we convert the X_train and y_train to PyTorch Variables.  Then we calculate the output.  And then find the loss on this output. zero_grad is a PyTorch function. It sets our gradients to zero as PyTorch accumulates the gradients on subsequent backward passes. Then using the backward() function we backpropagate the Neural Network. The step() function updates the parameters using the gradients calculated.\nThen we print the loss after every epoch.\n","e8018c64":"Checking the shape of the data","f0473f7b":"Then we add the last LSTM layer. Since this is last layer we do not want to take into consideration the last state of the cell, so we skip it as the default is FALSE.\n","17877491":"Loading the dataset","867af077":"Viewing the data","11820bd7":"But the RNN in PyTorch does not accept this type of shape. So we need to reshape this so that it is suitable for RNN.","063e7bf3":"Out of the various values given, we will be predicting just the opening prices of the stock."}}