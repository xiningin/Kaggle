{"cell_type":{"35ffafda":"code","f3366c4f":"code","40ffdd98":"code","6231d5e7":"code","b02f061f":"code","696be651":"code","d5bcca49":"code","47443a6b":"code","e8aaf8a9":"code","9111d747":"code","734dbe49":"code","d55ee9c3":"code","571f8aac":"code","f33260ec":"code","6b655656":"code","ddd3ca92":"code","21197e49":"code","8b7ef811":"code","ce7254e6":"code","6e424e45":"code","13ac1df5":"code","a3018bee":"code","db3f6b4f":"code","6d59d82a":"code","cea9af8d":"code","8be98bd2":"code","ff0a4b04":"code","50e30332":"code","e45c8212":"code","534bb753":"code","ad0db570":"code","16154f31":"code","01aa5227":"code","8c7463b8":"code","5dbbbc84":"code","534dfd18":"code","9a8d1e72":"code","95d31b13":"code","e3945850":"markdown","001171a0":"markdown","2fec7dae":"markdown","e994fe5c":"markdown","73aca2dd":"markdown","7ee9f401":"markdown","aa85d030":"markdown","b6bf9947":"markdown","1c6b08bb":"markdown"},"source":{"35ffafda":"import numpy as np # linear algebra\nimport pandas as pd\nimport cv2\nimport os\nimport math\nimport imageio\nimport seaborn as sns\nfrom glob import glob\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\n\nfrom sklearn.model_selection import train_test_split","f3366c4f":"import seaborn as sns\nimport plotly.express as px\nfrom IPython.display import SVG\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n%matplotlib inline","40ffdd98":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.applications import VGG16,inception_v3\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\nfrom tensorflow.keras.layers import Input, Lambda,Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                                    Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\n                                    LSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape,\\\n                                    Conv2DTranspose, LeakyReLU, Conv1D, AveragePooling1D, MaxPooling1D\n\n\n","6231d5e7":"Main_Video_Path = Path(\"..\/input\/real-life-violence-situations-dataset\/Real Life Violence Dataset\")\nVideo_Path = list(Main_Video_Path.glob(r\"*\/*.mp4\"))\nVideo_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"MP4\").astype(str)\nVideo_Labels_Series = pd.Series(Video_Labels,name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","b02f061f":"Violence_Data = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"Violence\"]\nNonViolence_Data = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"NonViolence\"]\n\nViolence_Data = Violence_Data.reset_index()\nNonViolence_Data = NonViolence_Data.reset_index()","696be651":"Violence_Data","d5bcca49":"Main_Video_Path = Path(\"..\/input\/violencedetectionsystem\")\nVideo_Path = list(Main_Video_Path.glob(r\"*\/*.mp4\"))\nVideo_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"MP4\").astype(str)\nVideo_Labels_Series = pd.Series(Video_Labels,name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","47443a6b":"Main_MP4_Data[\"CATEGORY\"].replace({'fight':'Violence','noFight':'NonViolence'}, inplace=True)\nVD = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"Violence\"]\nNVD = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"NonViolence\"]\n\n","e8aaf8a9":"Violence_Data = Violence_Data.append(VD,ignore_index=True, sort=False)\nNonViolence_Data = NonViolence_Data.append(NVD,ignore_index=True, sort=False)","9111d747":"Violence_Data","734dbe49":"NonViolence_Data","d55ee9c3":"Main_Video_Path3 = Path(\"..\/input\/ucf-crime-full\/Normal_Videos_for_Event_Recognition\")\nVideo_Path = list(Main_Video_Path3.glob(r\"*.mp4\"))\n# Video_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"mp4\").astype(str)\nVideo_Labels_Series = pd.Series('NonViolence',name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","571f8aac":"Main_MP4_Data['CATEGORY'] = Main_MP4_Data['CATEGORY'].fillna(\"NonViolence\")","f33260ec":"Main_MP4_Data.rename(columns = {'mp4':'MP4'}, inplace = True)","6b655656":"Main_MP4_Data","ddd3ca92":"NonViolence_Data = NonViolence_Data.append(Main_MP4_Data,ignore_index=True, sort=False)","21197e49":"NonViolence_Data","8b7ef811":"Main_Video_Path3 = Path(\"..\/input\/ucf-crime-full\/Fighting\")\nVideo_Path = list(Main_Video_Path3.glob(r\"*.mp4\"))\n# Video_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"mp4\").astype(str)\nVideo_Labels_Series = pd.Series('Violence',name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","ce7254e6":"Main_MP4_Data['CATEGORY'] = Main_MP4_Data['CATEGORY'].fillna(\"Violence\")","6e424e45":"Main_MP4_Data.rename(columns = {'mp4':'MP4'}, inplace = True)","13ac1df5":"Main_MP4_Data","a3018bee":"Violence_Data = Violence_Data.append(Main_MP4_Data,ignore_index=True, sort=False)","db3f6b4f":"Violence_Data","6d59d82a":"NonViolence_Data","cea9af8d":"Cdata = NonViolence_Data","8be98bd2":"Cdata = Cdata.append(Violence_Data,ignore_index=True, sort=False)","ff0a4b04":"Cdata","50e30332":"Cdata[\"CATEGORY\"].replace({'Violence':1,'NonViolence':0}, inplace=True)","e45c8212":"# Cdata.CATEGORY[Cdata.CATEGORY == 'Violence']","534bb753":"IMG_SIZE = 224\nBATCH_SIZE = 64\n\nMAX_SEQ_LENGTH = 20\nNUM_FEATURES = 2048","ad0db570":"def crop_center_square(frame):\n    y, x = frame.shape[0:2]\n    min_dim = min(y, x)\n    start_x = (x \/\/ 2) - (min_dim \/\/ 2)\n    start_y = (y \/\/ 2) - (min_dim \/\/ 2)\n    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n\n\ndef load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n    cap = cv2.VideoCapture(path)\n    frames = []\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = crop_center_square(frame)\n            frame = cv2.resize(frame, resize)\n            frame = frame[:, :, [2, 1, 0]]\n            frames.append(frame)\n\n            if len(frames) == max_frames:\n                break\n    finally:\n        cap.release()\n    return np.array(frames)","16154f31":"# load_video(Cdata.MP4[1000])","01aa5227":"def build_feature_extractor():\n    feature_extractor = keras.applications.InceptionV3(\n        weights=\"imagenet\",\n        include_top=False,\n        pooling=\"avg\",\n        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n    )\n    preprocess_input = keras.applications.inception_v3.preprocess_input\n\n    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n    preprocessed = preprocess_input(inputs)\n\n    outputs = feature_extractor(preprocessed)\n    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n\n\nfeature_extractor = build_feature_extractor()","8c7463b8":"def prepare_all_videos(df, root_dir):\n    num_samples = len(df)\n    video_paths = df[\"MP4\"].values.tolist()\n    labels = df[\"CATEGORY\"].values\n    labels = labels.reshape(len(labels),1)\n#     labels = label_processor(labels[..., None]).numpy()\n\n    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n    # masked with padding or not.\n    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n    frame_features = np.zeros(\n        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n    )\n\n    # For each video.\n    for idx, path in enumerate(video_paths):\n        # Gather all its frames and add a batch dimension.\n        frames = load_video(os.path.join(root_dir, path))\n        frames = frames[None, ...]\n\n        # Initialize placeholders to store the masks and features of the current video.\n        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n        temp_frame_features = np.zeros(\n            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n        )\n\n        # Extract features from the frames of the current video.\n        for i, batch in enumerate(frames):\n            video_length = batch.shape[0]\n            length = min(MAX_SEQ_LENGTH, video_length)\n            for j in range(length):\n                temp_frame_features[i, j, :] = feature_extractor.predict(\n                    batch[None, j, :]\n                )\n            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n\n        frame_features[idx,] = temp_frame_features.squeeze()\n        frame_masks[idx,] = temp_frame_mask.squeeze()\n\n    return (frame_features, frame_masks), labels\n\n\ntrain_data, train_labels = prepare_all_videos(Cdata, \"train\")\n# test_data, test_labels = prepare_all_videos(test_df, \"test\")\n\nprint(f\"Frame features in train set: {train_data[0].shape}\")\nprint(f\"Frame masks in train set: {train_data[1].shape}\")\n","5dbbbc84":" train_labels","534dfd18":"def get_sequence_model():\n \n\n    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n\n    # Refer to the following tutorial to understand the significance of using `mask`:\n    # https:\/\/keras.io\/api\/layers\/recurrent_layers\/gru\/\n    x = keras.layers.GRU(16, return_sequences=True)(\n        frame_features_input, mask=mask_input\n    )\n    x = keras.layers.GRU(8)(x)\n    x = keras.layers.Dropout(0.4)(x)\n    x = keras.layers.Dense(8, activation=\"relu\")(x)\n    output = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n\n    rnn_model = keras.Model([frame_features_input, mask_input], output)\n\n    rnn_model.compile(\n        loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n    )\n    return rnn_model\n\n\n# Utility for running experiments.\ndef run_experiment():\n    filepath = \"\/tmp\/video_classifier\"\n    checkpoint = keras.callbacks.ModelCheckpoint(\n        filepath, save_weights_only=True, save_best_only=True, verbose=1\n    )\n\n    seq_model = get_sequence_model()\n    history = seq_model.fit(\n        [train_data[0], train_data[1]],\n        train_labels,\n        validation_split=0.1,\n        epochs=50,\n        callbacks=[checkpoint]\n    )\n\n    seq_model.load_weights(filepath)\n#     _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n#     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n\n    return history, seq_model\n\n\n_, sequence_model = run_experiment()","9a8d1e72":"print(_.history.keys())\nsns.set()\n# summarize history for accuracy\nplt.plot(_.history['accuracy'])\nplt.plot(_.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(_.history['loss'])\nplt.plot(_.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","95d31b13":"sequence_model.save('keras_io_video_classification_model.hdf5')","e3945850":"### **Merging all different Data**","001171a0":"## **Data Importing from different datasets**","2fec7dae":"##  **Import Libraries**","e994fe5c":"### **Building Feature Extractor by using InceptionV3**","73aca2dd":"# keras_io_video_classification_model","7ee9f401":"**We can use a pre-trained network to extract meaningful features from the extracted frames. The Keras Applications module provides a number of state-of-the-art models pre-trained on the ImageNet-1k dataset. We will be using the InceptionV3 model for this purpose.**","aa85d030":"## **Video Preprocessing**","b6bf9947":"**Now, we can feed this data to a sequence model consisting of recurrent layers like GRU.**","1c6b08bb":"**Define Hyperparmeters**"}}