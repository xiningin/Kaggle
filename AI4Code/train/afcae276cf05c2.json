{"cell_type":{"a264180d":"code","dbe51db1":"code","0f76dcca":"code","15a211a7":"code","531cf0d9":"code","de7936d8":"code","192b9428":"code","11c6c25c":"code","fd78b97e":"code","ba898c77":"code","17a949e0":"code","d2b629db":"code","89967f29":"code","4d7aa359":"code","ba54359c":"code","364782c2":"code","7c4f416b":"code","32da9342":"code","e74ca00e":"code","5c245b57":"code","bd1c7184":"code","44c1081c":"code","c5a878f8":"code","29ca5f09":"code","0686b6a6":"code","b765054a":"code","0b588c94":"code","9f879a66":"code","f168539b":"code","b507c7d9":"code","0eb5118e":"code","8214280c":"code","ee76d21d":"code","1c754ce0":"code","17df4e11":"code","eee473e5":"code","1e0257df":"code","e562adf7":"code","dce17ce6":"code","e82507f0":"code","de19f19e":"code","6b530020":"code","ac551cae":"code","cf20ad90":"code","d5a2a5fc":"code","06194312":"code","b8be224f":"code","b2e5ad7c":"markdown","e8aa89dd":"markdown","0f5a9075":"markdown","7ceecbcc":"markdown","2e36ac2d":"markdown","8264f0d4":"markdown","8edc4453":"markdown","b6d47227":"markdown","246eb861":"markdown","0a81a2dd":"markdown","2cce02b5":"markdown","9066ab2a":"markdown","3a4794cb":"markdown","a647099e":"markdown","6f3a3f45":"markdown","435d4043":"markdown"},"source":{"a264180d":"import math\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nfrom itertools import product","dbe51db1":"class OptVis(object):\n    def __init__(self,\n                 model, layer, filter, neuron=False,\n                 size=[128, 128], fft=True, scale=0.01):\n        \"\"\" Create a model for use with an optimization visualization of some part of the network. Currently supported are filters and neurons. \"\"\"\n        # Create activation model\n        activations = model.get_layer(layer).output\n        if len(activations.shape) == 4:\n            activations = activations[:,:,:,filter]\n        else:\n            raise ValueError(\"Activation shapes other than 4 not implemented.\")\n        if neuron:\n            _, y, x = activations.shape\n            # find center\n            # TODO: need to compute this from selected size, not activations\n            yc = int(round(y\/2))\n            xc = int(round(x\/2))\n            activations = activations[:, yc, xc]\n        self.activation_model = keras.Model(\n            inputs=model.inputs,\n            outputs=activations\n        )\n\n        # Create random initialization buffer\n        self.shape = [1, *size, 3]\n        self.fft = fft\n        self.image = init_buffer(height=size[0], width=size[1], fft=fft, scale=scale)\n        self.fft_scale = fft_scale(size[0], size[1], decay_power=1.0)\n\n    def __call__(self):\n        # Preprocessing\n        # \n\n        image = self.activation_model(self.image)\n        \n        return image\n\n    def compile(self, optimizer):\n        self.optimizer = optimizer\n\n    @tf.function\n    def train_step(self):\n        # Compute loss\n        with tf.GradientTape() as tape:\n            image = self.image\n            if self.fft:\n                image = fft_to_rgb(shape=self.shape,\n                                   buffer=image,\n                                   fft_scale=self.fft_scale)\n            image = to_valid_rgb(image)\n            image = random_transform(\n                tf.squeeze(image),\n                jitter=8, \n                scale=1.1,\n                rotate=1.0,\n                fill_method='reflect')\n            image = tf.expand_dims(image, 0)\n            loss = clip_gradients(score(self.activation_model(image)))\n    \n        # Apply gradient\n        grads = tape.gradient(loss, self.image)\n        self.optimizer.apply_gradients([(-grads, self.image)])\n        \n        return {'loss': loss}\n\n    @tf.function\n    def fit(self, epochs=1, log=False):\n        for epoch in tf.range(epochs):\n            loss = self.train_step()\n            if log: print('Score: {}'.format(loss['loss']))\n        \n        image = self.image\n        if self.fft:\n            image = fft_to_rgb(shape=self.shape,\n                               buffer=image,\n                               fft_scale=self.fft_scale)\n        return to_valid_rgb(image)","0f76dcca":"def score(x):\n    s = tf.math.reduce_mean(x)\n    return s\n\n@tf.custom_gradient\ndef clip_gradients(y):\n    def backward(dy):\n        return tf.clip_by_norm(dy, 1.0)\n    return y, backward\n\n# unused\ndef normalize_gradients(grads, method='l2'):\n    if method is 'l2':\n        grads = tf.math.l2_normalize(grads)\n    elif method is 'std':\n        grads \/= tf.math.reduce_std(grads) + 1e-8\n    elif method is 'clip':\n        grads = tf.clip_by_norm(grads, 1.0)\n    return grads","15a211a7":"# ImageNet statistics\ncolor_correlation_svd_sqrt = np.asarray(\n    [[0.26, 0.09, 0.02],\n     [0.27, 0.00, -0.05],\n     [0.27, -0.09, 0.03]]\n).astype(\"float32\")\nmax_norm_svd_sqrt = np.max(np.linalg.norm(color_correlation_svd_sqrt, axis=0))\ncolor_correlation_normalized = color_correlation_svd_sqrt \/ max_norm_svd_sqrt\ncolor_mean = np.asarray([0.485, 0.456, 0.406])\ncolor_std = np.asarray([0.229, 0.224, 0.225])\n\ndef correlate_color(image):\n    image_flat = tf.reshape(image, [-1, 3])\n    image_flat = tf.matmul(image_flat, color_correlation_normalized.T)\n    image = tf.reshape(image_flat, tf.shape(image))\n    return image\n\ndef normalize(image):\n    return (image - color_mean) \/ color_std\n\ndef to_valid_rgb(image, crop=False):\n    if crop:\n        image = image[:, 25:-25, 25:-25, :]\n    image = correlate_color(image)\n    image = tf.nn.sigmoid(image)\n    return image","531cf0d9":"@tf.function\ndef random_transform(image, jitter=0, rotate=0, scale=1, **kwargs):\n    jx = tf.random.uniform([], -jitter, jitter)\n    jy = tf.random.uniform([], -jitter, jitter)\n    r = tf.random.uniform([], -rotate, rotate)\n    s = tf.random.uniform([], 1.0, scale)\n    image = apply_affine_transform(\n        image,\n        theta=r,\n        tx=jx, ty=jy,\n        zx=s, zy=s,\n        **kwargs,\n    )\n    return image\n\n@tf.function\ndef apply_affine_transform(x,\n                           theta=0, tx=0, ty=0, shear=0, zx=1, zy=1,\n                           row_axis=0, col_axis=1, channel_axis=2,\n                           fill_method='reflect', cval=0.,\n                           interpolation_method='nearest'):\n    \"\"\" Apply an affine transformation to an image x. \"\"\"\n\n    theta = tf.convert_to_tensor(theta, dtype=tf.float32)\n    tx = tf.convert_to_tensor(tx, dtype=tf.float32)\n    ty = tf.convert_to_tensor(ty, dtype=tf.float32)\n    shear = tf.convert_to_tensor(shear, dtype=tf.float32)\n    zx = tf.convert_to_tensor(zx, dtype=tf.float32)\n    zy = tf.convert_to_tensor(zy, dtype=tf.float32)\n\n    transform_matrix = _get_inverse_affine_transform(\n        theta,\n        tx, ty,\n        shear,\n        zx, zy,\n    )\n\n    x = _apply_inverse_affine_transform(\n        x,\n        transform_matrix,\n        fill_method=fill_method,\n        interpolation_method=interpolation_method,\n    )\n\n    return x\n\n\n# adapted from https:\/\/github.com\/keras-team\/keras-preprocessing\/blob\/master\/keras_preprocessing\/image\/affine_transformations.py\n# MIT License: https:\/\/github.com\/keras-team\/keras-preprocessing\/blob\/master\/LICENSE\n@tf.function\ndef _get_inverse_affine_transform(theta, tx, ty, shear, zx, zy):\n    \"\"\" Construct the inverse of the affine transformation matrix with the given transformations. \n    \n    The transformation is taken with respect to the usual right-handed coordinate system.\"\"\"\n\n    transform_matrix = tf.eye(3, dtype=tf.float32)\n\n    if theta != 0:\n        theta = theta * math.pi \/ 180 # convert degrees to radians\n        # this is \n        rotation_matrix = tf.convert_to_tensor(\n            [[tf.math.cos(theta), tf.math.sin(theta), 0],\n             [-tf.math.sin(theta), tf.math.cos(theta), 0],\n             [0, 0, 1]],\n            dtype=tf.float32)\n        transform_matrix = rotation_matrix\n\n    if tx != 0 or ty != 0:\n        shift_matrix = tf.convert_to_tensor(\n            [[1, 0, -tx],\n             [0, 1, -ty],\n             [0, 0, 1]],\n            dtype=tf.float32)\n        if transform_matrix is None:\n            transform_matrix = shift_matrix\n        else:\n            transform_matrix = tf.matmul(transform_matrix, shift_matrix)\n\n    if shear != 0:\n        shear = shear * math.pi \/ 180 # convert degrees to radians\n        shear_matrix = tf.convert_to_tensor(\n            [[1, tf.math.sin(shear), 0],\n             [0, tf.math.cos(shear), 0],\n             [0, 0, 1]],\n            dtype=tf.float32)\n        if transform_matrix is None:\n            transform_matrix = shear_matrix\n        else:\n            transform_matrix = tf.matmul(transform_matrix, shear_matrix)\n\n    if zx != 1 or zy != 1:\n        # need to assert !=0\n        zoom_matrix = tf.convert_to_tensor(\n            [[1\/zx, 0, 0],\n             [0, 1\/zy, 0],\n             [0, 0, 1]],\n            dtype=tf.float32)\n        if transform_matrix is None:\n            transform_matrix = zoom_matrix\n        else:\n            transform_matrix = tf.matmul(transform_matrix, zoom_matrix)\n            \n    return transform_matrix\n\n@tf.function\ndef _apply_inverse_affine_transform(A, Ti, fill_method, interpolation_method):\n    \"\"\"Perform an affine transformation of the image A defined by a\ntransform whose inverse is Ti. The matrix Ti is assumed to be in\nhomogeneous coordinate form.\n\n    Available fill methods are \"replicate\" and \"reflect\" (default).\n    Available interpolation method is \"nearest\".\n\n    \"\"\"\n    nrows, ncols, _ = A.shape\n\n    # Create centered coordinate grid\n    x = tf.range(ncols*nrows) % ncols\n    x = tf.cast(x, dtype=tf.float32) - ((ncols-1)\/2) # center\n    y = tf.range(ncols*nrows) \/\/ ncols\n    y = tf.cast(y, dtype=tf.float32) - ((nrows-1)\/2) # center\n    y = -y # left-handed to right-handed coordinates\n    z = tf.ones([ncols*nrows], dtype=tf.float32)\n    grid = tf.stack([x, y, z])\n\n    # apply transformation\n    # x, y, _ = tf.matmul(Ti, grid)\n    xy = tf.matmul(Ti, grid)\n    x = xy[0, :]\n    y = xy[1, :]\n    \n    # convert coordinates to (approximate) indices\n    i = -y + ((nrows-1)\/2)\n    j = x + ((ncols-1)\/2)\n\n    # replicate: 111|1234|444\n    if fill_method is 'replicate':\n        i = tf.clip_by_value(i, 0.0, nrows-1)\n        j = tf.clip_by_value(j, 0.0, ncols-1)\n    # reflect: 432|1234|321\n    elif fill_method is 'reflect':\n        i = _reflect_index(i, nrows-1)\n        j = _reflect_index(j, ncols-1)\n        \n    # nearest neighbor interpolation\n    grid = tf.stack([i, j])\n    grid = tf.round(grid)\n    grid = tf.cast(grid, dtype=tf.int32)\n    B = tf.gather_nd(A, tf.transpose(grid))\n    B = tf.reshape(B, A.shape)\n\n    return B\n\n@tf.function\ndef _reflect_index(i, n):\n    \"\"\"Reflect the index i across dimensions [0, n].\"\"\"\n    i = tf.math.floormod(i-n, 2*n)\n    i = tf.math.abs(i - n)\n    return tf.math.floor(i)","de7936d8":"# Adapted from https:\/\/github.com\/tensorflow\/lucid\/blob\/master\/lucid\/optvis\/param\/spatial.py\n# and https:\/\/github.com\/elichen\/Feature-visualization\/blob\/master\/optvis.py\ndef rfft2d_freqs(h, w):\n    \"\"\"Computes 2D spectrum frequencies.\"\"\"\n\n    fy = np.fft.fftfreq(h)[:, np.newaxis]\n    # when we have an odd input dimension we need to keep one additional\n    # frequency and later cut off 1 pixel\n    if w % 2 == 1:\n        fx = np.fft.fftfreq(w)[: w \/\/ 2 + 2]\n    else:\n        fx = np.fft.fftfreq(w)[: w \/\/ 2 + 1]\n        \n    return np.sqrt(fx * fx + fy * fy)\n\ndef fft_scale(h, w, decay_power=1.0):\n    freqs = rfft2d_freqs(h, w)\n    scale = 1.0 \/ np.maximum(freqs, 1.0 \/ max(w, h)) ** decay_power\n    scale *= np.sqrt(w * h)\n    return tf.convert_to_tensor(scale, dtype=tf.complex64)\n\ndef fft_to_rgb(shape, buffer, fft_scale):\n    \"\"\"Convert FFT spectrum buffer to RGB image buffer.\"\"\"\n    \n    batch, h, w, ch = shape\n\n    spectrum = tf.complex(buffer[0], buffer[1]) * fft_scale\n    image = tf.signal.irfft2d(spectrum)\n    image = tf.transpose(image, (0, 2, 3, 1))\n    \n    # in case of odd spatial input dimensions we need to crop\n    image = image[:batch, :h, :w, :ch]\n    image = image \/ 4.0  # TODO: is that a magic constant?\n    \n    return image","192b9428":"def init_buffer(height, width=None, batches=1, channels=3, scale=0.01, fft=True):\n    \"\"\"Initialize an image buffer.\"\"\"\n    width = width or height\n    shape = [batches, height, width, channels]\n    fn = init_fft if fft else init_pixel\n    \n    buffer = fn(shape, scale)\n    \n    return tf.Variable(buffer, trainable=True)\n\ndef init_pixel(shape, scale=None):\n    batches, h, w, ch = shape\n#     initializer = tf.initializers.VarianceScaling(scale=scale)\n    initializer = tf.random.uniform\n    buffer = initializer(shape=[batches, h, w, ch],\n                         dtype=tf.float32)\n    return buffer\n\n\ndef init_fft(shape, scale=0.1):\n    \"\"\"Initialize FFT image buffer.\"\"\"\n    \n    batch, h, w, ch = shape\n    freqs = rfft2d_freqs(h, w)\n    init_val_size = (2, batch, ch) + freqs.shape\n\n    buffer = np.random.normal(size=init_val_size, scale=scale).astype(np.float32)\n    return buffer","11c6c25c":"def visualize(model, layer, filter, neuron=False, size=[150, 150], fft=True, lr=0.05, epochs=500, log=False):\n    optvis = OptVis(model, layer, filter, neuron=neuron, size=size, fft=fft)\n    optvis.compile(\n        optimizer=tf.optimizers.Adam(lr)\n    )\n    image = optvis.fit(epochs=epochs, log=log)\n    \n    plt.imshow(tf.squeeze(image).numpy())\n    plt.axis('off')\n    \ndef visualize_layer(model, layer, init_filter=0, neuron=False, size=[150, 150],\n                    fft=True, lr=0.05, epochs=500, log=False,\n                    rows=2, cols=4, width=16):\n    gs = gridspec.GridSpec(rows, cols, wspace=0.01, hspace=0.01)\n    plt.figure(figsize=(width, (width * rows) \/ cols))\n    for f, (r, c) in enumerate(product(range(rows), range(cols))):\n        optvis = OptVis(model, layer, f+init_filter, neuron=neuron, size=size, fft=fft)\n        optvis.compile(optimizer=tf.optimizers.Adam(lr))\n        image = optvis.fit(epochs=epochs)\n        plt.subplot(gs[r, c])\n        plt.imshow(tf.squeeze(image))\n        plt.axis('off')","fd78b97e":"img = init_buffer(120, scale=1.0)\nscale = fft_scale(120, 120)\nimg = fft_to_rgb([1, 120, 120, 3], img, scale)\nimg = to_valid_rgb(img)\nplt.imshow(tf.squeeze(img))\nplt.axis('off')\nplt.show();","ba898c77":"model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=[128, 128, 3])","17a949e0":"visualize(model, layer=\"block2_conv2\", filter=0, neuron=True)","d2b629db":"visualize_layer(model, layer=\"block2_conv2\", neuron=True, rows=1, cols=4, epochs=250)","89967f29":"visualize_layer(model, layer=\"block1_conv1\", rows=1, cols=4, epochs=100)","4d7aa359":"visualize_layer(model, layer=\"block1_conv1\", neuron=True, rows=3, cols=4, epochs=100)","ba54359c":"visualize_layer(model, layer=\"block2_conv2\", rows=1, cols=4, epochs=250)","364782c2":"visualize_layer(model, layer=\"block3_conv2\", rows=1, cols=4, epochs=250)","7c4f416b":"visualize_layer(model, layer=\"block4_conv2\", rows=1, cols=4, epochs=250)","32da9342":"visualize_layer(model, layer=\"block5_conv1\", rows=1, cols=4, epochs=250)","e74ca00e":"visualize_layer(model, layer=\"block5_conv3\", rows=1, cols=4, epochs=250)","5c245b57":"resnet50 = tf.keras.applications.ResNet50V2(weights='imagenet', input_shape=[150, 150, 3], include_top=False)","bd1c7184":"visualize(resnet50, layer=\"conv4_block1_out\", filter=0, neuron=True, epochs=1000)","44c1081c":"layer = 'conv3_block1_out'\nvisualize_layer(resnet50, layer=layer, epochs=500, rows=1, cols=4)","c5a878f8":"layer = 'conv3_block4_out'\nvisualize_layer(resnet50, layer=layer, epochs=500, rows=1, cols=4)","29ca5f09":"layer = 'conv4_block3_out'\nneuron = True\nsize=[224, 224]\nvisualize_layer(resnet50, layer=layer, epochs=500, rows=1, cols=4, size=size)","0686b6a6":"layer = 'conv5_block1_out'\nsize=[224, 224]\nvisualize_layer(resnet50, layer=layer, epochs=500, rows=1, cols=4, size=size)","b765054a":"layer = 'conv5_block3_out'\nsize=[224, 224]\nvisualize_layer(resnet50, layer=layer, epochs=500, rows=1, cols=4, size=size)","0b588c94":"# layer = 'conv5_block3_out'\n# size = [1024, 1024]\n# plt.figure(figsize=(24, 24))\n# visualize(resnet50, layer=layer, filter=0, epochs=200, size=size)","9f879a66":"inceptionv3 = tf.keras.applications.InceptionV3(weights='imagenet')","f168539b":"size = [224, 224]\nlayer = 'mixed1'\nvisualize_layer(inceptionv3, layer=layer, epochs=500, rows=1, cols=4, size=size)","b507c7d9":"size = [224, 224]\nlayer = 'mixed2'\nvisualize_layer(inceptionv3, layer=layer, epochs=500, rows=1, cols=4, size=size)","0eb5118e":"size = [224, 224]\nlayer = 'mixed3'\nvisualize_layer(inceptionv3, layer=layer, epochs=500, rows=1, cols=4, size=size)","8214280c":"size = [224, 224]\nlayer = 'mixed4'\nvisualize_layer(inceptionv3, layer=layer, epochs=500, rows=1, cols=4, size=size)","ee76d21d":"size = [224, 224]\nlayer = 'mixed5'\nvisualize_layer(inceptionv3, layer=layer, epochs=500, rows=1, cols=4, size=size)","1c754ce0":"size = [224, 224]\nlayer = 'mixed6'\nvisualize_layer(inceptionv3, layer=layer, epochs=500, rows=1, cols=4, size=size)","17df4e11":"size = [224, 224]\nlayer = 'mixed7'\nvisualize_layer(inceptionv3, layer=layer, epochs=500, rows=1, cols=4, size=size)","eee473e5":"size = [224, 224]\nlayer = 'mixed8'\nvisualize_layer(inceptionv3, layer=layer, epochs=500, rows=1, cols=4, size=size)","1e0257df":"size = [224, 224]\nlayer = 'mixed9'\nvisualize_layer(inceptionv3, layer=layer, epochs=500, rows=1, cols=4, size=size)","e562adf7":"size = [224, 224]\nlayer = 'mixed10'\nvisualize_layer(inceptionv3, layer=layer, epochs=500, rows=1, cols=4, size=size)","dce17ce6":"xception = tf.keras.applications.Xception(weights='imagenet')","e82507f0":"size = [224, 224]\nlayer = 'block1_conv1'\nvisualize_layer(xception, layer=layer, epochs=200, rows=1, cols=4, size=size)","de19f19e":"size = [224, 224]\nlayer = 'block1_conv2'\nvisualize_layer(xception, layer=layer, epochs=200, rows=1, cols=4, size=size)","6b530020":"size = [224, 224]\nlayer = 'add_1'\nvisualize_layer(xception, layer=layer, epochs=250, rows=1, cols=4, size=size)","ac551cae":"size = [224, 224]\nlayer = 'add_3'\nvisualize_layer(xception, layer=layer, epochs=250, rows=1, cols=4, size=size)","cf20ad90":"size = [224, 224]\nlayer = 'add_5'\nvisualize_layer(xception, layer=layer, epochs=400, rows=1, cols=4, size=size)","d5a2a5fc":"size = [224, 224]\nlayer = 'add_7'\nvisualize_layer(xception, layer=layer, epochs=500, rows=1, cols=4, size=size)","06194312":"size = [224, 224]\nlayer = 'add_9'\nvisualize_layer(xception, layer=layer, epochs=500, rows=1, cols=4, size=size)","b8be224f":"size = [224, 224]\nlayer = 'add_11'\nvisualize_layer(xception, layer=layer, epochs=500, rows=1, cols=4, size=size)","b2e5ad7c":"# ResNet50V2 #","e8aa89dd":"# Imports #","0f5a9075":"## Single Neurons ##","7ceecbcc":"# InceptionV3 #","2e36ac2d":"# Loss and Gradients #","8264f0d4":"# Xception #","8edc4453":"# Color Transforms #","b6d47227":"# Spatial Transforms #","246eb861":"# Example of Initial FFT Buffer #","0a81a2dd":"## Layers ##","2cce02b5":"# Optimization Visualization #\n\nWe visualize structures in a convolutional network by creating images optimized for maximal activation. We do this by creating an **activation model**, a model whose outputs are the outputs of whatever part of the network we wish to visualize.","9066ab2a":"# Affine Transforms #","3a4794cb":"# Buffer Initializers #","a647099e":"# Optimization Model #","6f3a3f45":"# VGG16 #","435d4043":"# Plotting #"}}