{"cell_type":{"f7289ed5":"code","aee84cef":"code","51c71845":"code","b2b2f10e":"code","e52cf75d":"code","268423d7":"code","0bd33ebd":"code","d8c2a308":"code","b129f327":"code","b0f26c16":"code","f1bea05d":"code","a6f27807":"code","a905e4c9":"code","6d482e63":"code","6344bb79":"code","3f8bec44":"code","b0941a95":"code","4abaa3e6":"code","c63739ff":"code","82517f42":"code","ee3da6a9":"code","48cf58c1":"markdown","2ae0d246":"markdown","29c5eaf7":"markdown","d9ddf7dd":"markdown","6ef07ddc":"markdown","b408398c":"markdown","3ad76d4c":"markdown","bdab0d82":"markdown","82621bab":"markdown","664607cc":"markdown","5511b029":"markdown","9a12c104":"markdown","b81bbac6":"markdown","5de57d22":"markdown","08a9524b":"markdown","ebbe2ae9":"markdown","c0fe2540":"markdown","03a9fa38":"markdown","63831ab8":"markdown"},"source":{"f7289ed5":"%matplotlib inline","aee84cef":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom fastai.tabular import *\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import f1_score","51c71845":"orig_df = pd.read_csv('..\/input\/creditcard.csv')\norig_df.head()","b2b2f10e":"orig_df.loc[orig_df['Class'] == 0].shape[0], orig_df.loc[orig_df['Class'] == 1].shape[0]","e52cf75d":"smote = SMOTE()\nX, y = smote.fit_sample(orig_df.drop(columns=['Class']), orig_df['Class'])\ny = y[..., np.newaxis]\nXy = np.concatenate((X, y), axis=1)\ndf = pd.DataFrame.from_records(Xy, columns=list(orig_df))\ndf.Class = df.Class.astype(int)\ndf.head()","268423d7":"df.loc[df['Class'] == 0].shape[0], df.loc[df['Class'] == 1].shape[0]","0bd33ebd":"#procs = [FillMissing, Categorify, Normalize]\nprocs = [Normalize]","d8c2a308":"idx = np.arange(df.shape[0])\ntrain, valid, train_idx, valid_idx = train_test_split(df, idx, test_size=0.05, random_state=42)\ntrain_idx.shape, valid_idx.shape","b129f327":"dep_var = 'Class'\ncat_names = []","b0f26c16":"data = TabularDataBunch.from_df('.', df, dep_var, valid_idx=valid_idx, procs=procs, cat_names=cat_names)\nprint(data.train_ds.cont_names)  # `cont_names` defaults to: set(df)-set(cat_names)-{dep_var}","f1bea05d":"learn = tabular_learner(data, layers=[1000, 500], ps=[0.001, 0.01], emb_szs={}, metrics=[])","a6f27807":"learn.lr_find(stop_div=True, num_it=100)","a905e4c9":"learn.recorder.plot()","6d482e63":"learn.fit_one_cycle(1, 1e-2)","6344bb79":"learn.recorder.plot_lr()","3f8bec44":"[train_preds, train_targets] = learn.get_preds(ds_type=DatasetType.Train)\ntrain_preds = to_np(train_preds)[:, 1]\ntrain_targets = to_np(train_targets)","b0941a95":"precision, recall, thresholds = precision_recall_curve(train_targets, train_preds)\nplt.plot(recall, precision, marker='.')\nauprc = auc(recall, precision)\nauprc","4abaa3e6":"[valid_preds, valid_targets] = learn.get_preds(ds_type=DatasetType.Valid)\nvalid_preds = to_np(valid_preds)[:, 1]\nvalid_targets = to_np(valid_targets)","c63739ff":"precision, recall, thresholds = precision_recall_curve(valid_targets, valid_preds)\nplt.plot(recall, precision, marker='.')\nauprc = auc(recall, precision)\nauprc","82517f42":"F1 = [2*p*r\/(p+r) for p, r in zip(precision, recall)]\nidx = np.argmax(F1)\nnp.max(F1), precision[idx], recall[idx], thresholds[idx]","ee3da6a9":"preds = valid_preds.copy()\npreds[preds >= thresholds[idx]] = 1\npreds[preds < thresholds[idx]] = 0\nf1_score(valid_targets, preds)","48cf58c1":"Find max F1 score as well as precision, recall, and threshold at max F1.","2ae0d246":"Define Transforms that will be applied to our variables.  We don't have missing values, so we don't need FillMissing.  We don't have any categorical variable, so we don't need Categorify.","29c5eaf7":"Print number of rows with Class == 0 and Class == 1.","d9ddf7dd":"Import the libraries we need.","6ef07ddc":"Find learning rate to use.","b408398c":"Split our data into training and validation sets.","3ad76d4c":"Does the model do well on the training data?  Accuracy is not meaningful for unbalanced classification.  To find how well the model do, plot and compute AUPRC.  See https:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/.  Tweak the model until it does well on the training data.","bdab0d82":"Does the model do well on the validation data?","82621bab":"Optional: Plot learning rate schedule.","664607cc":"Print number of rows with Class == 0 and Class == 1.","5511b029":"# Credit Card Fraud with fastai","9a12c104":"Looks like 1e-2 is a good value to use.  Let's train our model for 1 epoch.","b81bbac6":"Based on https:\/\/docs.fast.ai\/tabular.html.","5de57d22":"Looks like we have an imbalanced dataset.  Use Synthetic Minority Oversampling TEchnique (SMOTE) to oversample the minority class.  See https:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets.","08a9524b":"Split our variables into dependent and independent variables.  Then split independent variables into categorical and continuous variables.  \"fastai will assume all variables that aren't dependent or categorical are continuous, unless we explicitly pass a list to the cont_names parameter when constructing our DataBunch.\"  We don't have any categorical variable.","ebbe2ae9":"Create a Learner.  We don't have any categorical variable, so emb_szs is empty.  fastai doesn't have AUPRC, so metrics is empty.","c0fe2540":"Create DataBunch.","03a9fa38":"As a sanity check, make sure we get the same max F1 score using the threshold we found.","63831ab8":"Read csv file."}}