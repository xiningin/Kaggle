{"cell_type":{"d77bcbb8":"code","fe771f7a":"code","e19ebab7":"code","2648f7aa":"code","0b251173":"code","3dcd390a":"code","9faae267":"code","c7e9f625":"code","0ccb321f":"code","39faa260":"code","da435d4e":"code","b56a3352":"code","9f091ae7":"code","7324b1fb":"code","2a68a39c":"code","53501c1f":"code","22774a54":"code","147973eb":"code","724c75b6":"code","766e2a07":"code","dc15549b":"code","3e207ec6":"code","b8cbe094":"code","3354ee68":"code","27ff267e":"code","5530c281":"code","3b3846aa":"code","5d1e1f00":"code","184b36a3":"code","a528feeb":"code","3e40a7ae":"code","791cb6b6":"code","f52f257b":"code","e491ea86":"code","758836ed":"code","e8261ad8":"code","a455fe22":"code","60e02d3d":"code","4c3a9c82":"code","8ae8a1aa":"code","049d8ca1":"code","1692b2c7":"code","86979042":"code","60e06aa6":"code","d86cdd57":"code","8434c056":"code","5de5bdc6":"code","6f597728":"code","bb2b173f":"code","e0fce34f":"code","4bc63c3b":"code","aae0fcdf":"code","f402c8bc":"code","4c6eb09d":"code","64326590":"markdown","92f3792c":"markdown","ec4025fa":"markdown","8283899b":"markdown","e3291aee":"markdown","c2094209":"markdown","3034de6a":"markdown","71a68afc":"markdown","fee3a77d":"markdown","baa78038":"markdown","bc33ebca":"markdown","8f286ad6":"markdown","ae111f3e":"markdown","b7005de8":"markdown","e88d9b03":"markdown","2efb0d3a":"markdown","65daad06":"markdown","99a7a501":"markdown","08824140":"markdown","e6fb440d":"markdown","c16e055d":"markdown","02e06d4f":"markdown","ac35789a":"markdown","94c3ba14":"markdown","70fc5f77":"markdown","c200e72b":"markdown","8fad5e75":"markdown","ac24e6bd":"markdown","5a4af95d":"markdown","a33833f5":"markdown","90bf8bb5":"markdown","6d7341a0":"markdown","612ec11a":"markdown","760f7418":"markdown"},"source":{"d77bcbb8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport scipy as sp\nimport string\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","fe771f7a":"data=pd.read_csv('\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv')\n","e19ebab7":"data.head()","2648f7aa":"data","0b251173":"data.var()","3dcd390a":"data.mean()","9faae267":"data.describe().transpose()\n","c7e9f625":"data.describe()","0ccb321f":"data.info()","39faa260":"data.shape   #5572 rows and 5 columns in our dataset","da435d4e":"data.value_counts()","b56a3352":"data.dtypes","9f091ae7":"data.columns","7324b1fb":"data.isnull().sum()","2a68a39c":"data.isnull().any()","53501c1f":"data.isnull().all()","22774a54":"data=data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)","147973eb":"data","724c75b6":"data=data.rename({'v1':'Class','v2':'Message'},axis=1)\n             \n","766e2a07":"data.head()","dc15549b":"data.columns","3e207ec6":"\nplt.figure(figsize=(6,6))\n\nx= data.Class.value_counts()\nsns.countplot(x= \"Class\",data= data)\n","b8cbe094":"plt.figure(figsize=(8,12))\n\nlabel= [\"Class\",\"Message\"]\n\nplt.pie(x.values, labels= label ,autopct= \"%1.1f%%\") # visualizing using pie\nplt.show()   ","3354ee68":"import nltk\nimport scikitplot as skplt\nimport re\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nSTOPWORDS = stopwords.words('english')\n","27ff267e":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^0-9a-zA-Z]', ' ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    text = \" \".join(word for word in text.split() if word not in STOPWORDS)\n    return text\n","5530c281":"data['clean_text'] = data['Message'].apply(clean_text)\ndata.head()\n","3b3846aa":"X = data['clean_text']\ny = data['Class']\n","5d1e1f00":"# importing the PorterStemmer\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nps=PorterStemmer\nwords=word_tokenize('clean_text')\n","184b36a3":"#importing the CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n#lemmatizer=WordNetLemmatizer()","a528feeb":"#define a function to get rid of stopwords present in the messages\ndef message_text_process(mess):\n    # Check characters to see if there are punctuations \n    no_punctuation=[char for char in mess if char not in string.punctuation]\n    # now form the sentence\n    no_punctuation=''.join(no_punctuation)\n    # Now eliminate any stopwords\n    return[word for word in no_punctuation.split() if word.lower() not in stopwords.words('english')]","3e40a7ae":"# to verify that function is working\ndata['Message'].head(5).apply(message_text_process)","791cb6b6":"# start text processing with vectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer","f52f257b":"# bag of words by applying the function and fit the data(message) into it\nbag_of_words_transformer=CountVectorizer(analyzer=message_text_process).fit(data['Message'])","e491ea86":"# print the length of bag of words stored in vocabulary_attribute\nprint(len(bag_of_words_transformer.vocabulary_))","758836ed":"#store bag of words for messages using transform method\nmessage_bagofwords=bag_of_words_transformer.transform(data['Message'])","e8261ad8":"#apply tfidf transformer and fit the bag of words into it(transformed version)\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer=TfidfTransformer().fit(message_bagofwords)","a455fe22":"#print shape of tfidf\nmessage_tfidf=tfidf_transformer.transform(message_bagofwords)\nprint(message_tfidf.shape)","60e02d3d":"# choose naive bayes model to detect the spam and fit the tfidf data into it\nfrom sklearn.naive_bayes import MultinomialNB\nspam_detection_model=MultinomialNB().fit(message_tfidf,data['Class'])","4c3a9c82":"# check model for prediction and expected value say for message#2 and message#5\nmessage=data['Message'][4]\nbag_of_words_for_message=bag_of_words_transformer.transform([message])\ntfidf=tfidf_transformer.transform(bag_of_words_for_message)\n\nprint('predicted',spam_detection_model.predict(tfidf)[0])\n\n#print('expected',data.response[4])","8ae8a1aa":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer","049d8ca1":"message=data['Message'][4]\n# check model for prediction and expected value say for message#2 and message#5\nbag_of_words_for_message=bag_of_words_transformer.transform([message])\ntfidf=tfidf_transformer.transform(bag_of_words_for_message)\n\nprint('predicted',spam_detection_model.predict(tfidf)[0])\n#print('expected',data.label[4])","1692b2c7":"#importing PCA for the dimensionality reduction \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.decomposition import PCA","86979042":"#function for the model building and prediction\ndef Model(model, X, y):\n#training and testing the data\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=30)\n    # model building using CountVectorizer and TfidfTransformer\n    pipeline_model = Pipeline([('vect', CountVectorizer()),\n                              ('tfidf', TfidfTransformer()),\n                              ('clf', model)])\n    pipeline_model.fit(x_train, y_train)\n    \n    \n\n\n    y_pred = pipeline_model.predict(x_test)\n    y_probas =pipeline_model.predict_proba(x_test)\n    skplt.metrics.plot_roc(y_test,y_probas,figsize=(12,8),title_fontsize=12,text_fontsize=16)\n    plt.show()\n    skplt.metrics.plot_precision_recall(y_test,y_probas,figsize=(12,8),title_fontsize=12,text_fontsize=16)\n    plt.show()\n    print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n    print(\"Classification Report is:\\n\",classification_report(y_test, y_pred))\n    print('Accuracy:', pipeline_model.score(x_test, y_test)*100)\n    print(\"Training Score:\\n\",pipeline_model.score(x_train,y_train)*100)\n    \n\n","60e06aa6":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nModel(model, X, y)\n\n","d86cdd57":"from sklearn.neighbors import KNeighborsClassifier\nmodel=KNeighborsClassifier(n_neighbors=7)\nModel(model,X,y)\n","8434c056":"from sklearn.svm import SVC\nmodel = SVC(probability=True )\nModel(model, X, y)\n","5de5bdc6":"from sklearn.naive_bayes import BernoulliNB\nmodel = BernoulliNB()\nModel(model, X, y)","6f597728":"from sklearn import tree\ntree_clf = tree.DecisionTreeClassifier(max_depth=6, random_state=123,criterion='entropy')\nModel(tree_clf,X,y)\n\n","bb2b173f":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nModel(model, X, y)\n","e0fce34f":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(base_estimator = None)\nModel(model, X, y)\n","4bc63c3b":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()\nModel(model, X, y)\n\n","aae0fcdf":"from xgboost import XGBClassifier\n\nxgb =XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\nModel(model, X, y)\n","f402c8bc":"from sklearn.ensemble import ExtraTreesClassifier\netc = ExtraTreesClassifier(n_estimators=100, random_state=0)\nModel(model,X,y)","4c6eb09d":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn import tree\nmodel = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\nModel(model,X,y)","64326590":"# Exploratory Data Analysis","92f3792c":"**So we need to drop the columns that are : Unnamed:2, Unnamed:3, Unnamed:4**","ec4025fa":"![](https:\/\/www.sms77.io\/wp-content\/uploads\/SMS-Spam-Header.jpg)","8283899b":"# 4. Naive Bayes","e3291aee":"**So we get a accuracy score of 96.69 % using Naive Bayes**","c2094209":"# 5. DECISION TREE CLASSIFIER","3034de6a":"# 2. KNeighborsClassifier","71a68afc":"# 6. RandomForestClassifier","fee3a77d":"# IMPORTING THE LIBRARIES","baa78038":"**So we get a accuracy score of 94.90 % using DecisionTreeClassifier**","bc33ebca":"**So we get a accuracy score of 97.63 % using RandomForestClassifier**","8f286ad6":"# 10. ExtraTreesClassifier","ae111f3e":"**So we get a accuracy score of 97.27 % using ExtraTreesClassifier**","b7005de8":"# SMS Spam Collection Dataset","e88d9b03":"**So we get a accuracy score of 97.70 % using XGBClassifier**","2efb0d3a":"**So we get a accuracy score of 96.19 % using LogisticRegression**","65daad06":"# **If you like my work, please appreciate it with a upvote!!**","99a7a501":"# 8. Gradient Boosting Classifier","08824140":"# 11. Bagging Classifier","e6fb440d":"**For betterment of columns(v1,v2) we can rename them respectively.**","c16e055d":"**Checking Null Values**","02e06d4f":"**Conclusion :**\n**We get a good accuracy score of 98 % using Random Forest , Ada Boost and Extra Trees Classifier**","ac35789a":"# 9. XGBClassifier","94c3ba14":"# 7. AdaBoostClassifier","70fc5f77":"**So we get a accuracy score of 96.26 % using Bagging Classifier**","c200e72b":"# Model Building","8fad5e75":"**So we get a accuracy score of 97.70 % using Gradient Boosting Classifier**","ac24e6bd":"# 3. SVC","5a4af95d":"# 1. Logistic Regression","a33833f5":"# LOADING THE DATASET","90bf8bb5":"**So we get a accuracy score of 97.55 % using AdaBoostClassifier**","6d7341a0":"**So we get a accuracy score of 90.16 % using KNeighborsClassifier**","612ec11a":"**So we get a accuracy score of 97.84 % using SVC**","760f7418":"**SMS Spam**\n\n**SMS spam (sometimes called cell phone spam) is any junk message delivered to a mobile phone as text messaging through the Short Message Service (SMS). The practice is fairly rare in North America, but has been common in Japan for years.**\n"}}