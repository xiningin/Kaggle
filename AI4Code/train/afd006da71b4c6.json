{"cell_type":{"bc13ddd7":"code","3970fba1":"code","e1fe01e2":"code","d1867882":"code","0f47d29a":"code","427ecf07":"code","a82e599b":"code","168c54ac":"code","a60fb457":"code","f380cd5e":"code","c561089b":"code","03206f79":"code","75bf44ef":"code","fb4e3f11":"code","c9ef6a9b":"code","a1f3a85f":"code","70d828bc":"code","23f52edc":"code","8a7dfc59":"code","539bbdaa":"code","bda17fdc":"code","e57a2a94":"code","9a7d71cc":"code","f64f9879":"markdown","4febe5c0":"markdown","74b1e7c2":"markdown","5f0c1134":"markdown","688b8da3":"markdown","75cc9232":"markdown","251ae5f3":"markdown","cbfdde17":"markdown","683178be":"markdown","087c5faf":"markdown","98f082a0":"markdown","e839f177":"markdown","d1b82162":"markdown","bad418e6":"markdown","cfc8666b":"markdown","56cedfb9":"markdown","85992c15":"markdown","0b651da5":"markdown"},"source":{"bc13ddd7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.util import ngrams\nimport string\nimport re\nfrom collections import Counter\nfrom textblob import TextBlob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.set_option('max_colwidth', 400)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3970fba1":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\n\nprint('Pct Actual Disasters:', train[train['target']==1]['target'].sum()\/train['target'].count())\nprint('Pct Not Disasters:', train[train['target']==0]['target'].count()\/train['target'].count())\n\npcts = train['target'].value_counts(normalize=True)\nsns.barplot(x=pcts.index, y=pcts)\nplt.title('Non-Disaster vs Disaster Pcts')\nplt.xticks(ticks = [0, 1], labels = ['Non-Disaster', 'Disaster'])\nplt.show()\n","e1fe01e2":"def clean(text):\n    text = text.lower()\n    text = re.sub(r'\\W+', ' ', text)\n    text = re.sub(r'\\d+', ' ', text)\n    text = re.sub(r'\\[^\\w\\s]', '', text)\n    wrd_lem = WordNetLemmatizer()\n    text = wrd_lem.lemmatize(text)\n    return text\n\ntrain['clean_text'] = train['text'].apply(clean)\ntest['clean_text'] = test['text'].apply(clean)\n","d1867882":"STOPS = stopwords.words('english')\nSTOPS.append(\"'the\")\nSTOPS.append(\"'i\")\nSTOPS.append('co')\nSTOPS.append('http')\nSTOPS.append('\u00fb_')\n\ndef bow(text):\n    tokens = nltk.word_tokenize(text) \n    no_stops = [t for t in tokens if not t in STOPS]\n    wrdnet_lemmas = WordNetLemmatizer()\n    lemma = [wrdnet_lemmas.lemmatize(t) for t in no_stops]\n    bow=Counter(lemma)\n    return bow.most_common(30)","0f47d29a":"ddf = train[train['target']==1]\nsdf = train[train['target']==0]\n\ntblob = []\nfor index, item in ddf.iterrows():\n    tblob.append(item['clean_text'])\n    \nblob = []\nfor index, item in sdf.iterrows():\n    blob.append(item['clean_text'])\n    \ntblob = str(tblob)\nblob = str(blob)\nbow_d = bow(tblob)\nbow_s = bow(blob)","427ecf07":"print(bow_d)\nprint(bow_s)","a82e599b":"def get_ngrams(text, num=2):\n    n_grams = ngrams(nltk.word_tokenize(text), num)\n    return [' '.join(grams) for grams in n_grams]\n\ndef get_tokens(text):\n    tokens = nltk.word_tokenize(text)\n    return tokens\n\ntrain['Bigrams'] = train['clean_text'].apply(get_ngrams)\ntrain['tokens'] = train['clean_text'].apply(get_tokens)\ntest['Bigrams'] = test['clean_text'].apply(get_ngrams)\ntest['tokens'] = test['clean_text'].apply(get_tokens)","168c54ac":"knull = train[train['keyword'].isnull()]\ntrain.dropna(subset=['keyword'], inplace=True)\nkeys = list(train.keyword.unique())\n\ntweets = []\nfor index, item in knull.iterrows():\n    new = []\n    for i in item['tokens']:\n        if i in keys:\n            new.append(i)\n        elif i not in keys:\n            new.append('')\n            \n    tweets.append(new)\n            \nnew_key = []\nfor tweet in tweets:\n    if tweet.count('') == len(tweet):\n        new_key.append('NaN')\n    else:\n        new_key.append(next(i for i in tweet if i))\n    \nknull['keyword'] = new_key\n            \ntrain = pd.concat([train, knull])","a60fb457":"train.replace('NaN', np.nan, inplace=True)\nknull = train[train['keyword'].isnull()]\ntrain.dropna(subset=['keyword'], inplace=True)\n\nfinal_kwords = ['heat', 'man', 'fruits', 'summer', 'car', 'goal', 'ridiculous', 'London', 'skiing', 'wonderful', 'lol', 'shit', 'New York', 'girlfirend', 'cool', 'pasta', 'end', 'heat', 'bomber', 'bang', 'oil', 'bomber', 'fire']\nknull['keyword'] = final_kwords\n\ntrain = pd.concat([train, knull])","f380cd5e":"knull = test[test['keyword'].isnull()]\ntest.dropna(subset=['keyword'], inplace=True)\nkeys = list(test.keyword.unique())\n\ntweets = []\nfor index, item in knull.iterrows():\n    new = []\n    for i in item['tokens']:\n        if i in keys:\n            new.append(i)\n        elif i not in keys:\n            new.append('')\n            \n    tweets.append(new)\n            \nnew_key = []\nfor tweet in tweets:\n    if tweet.count('') == len(tweet):\n        new_key.append('NaN')\n    else:\n        new_key.append(next(i for i in tweet if i))\n    \nknull['keyword'] = new_key\n            \ntest = pd.concat([test, knull])\n\ntest.replace('NaN', np.nan, inplace=True)\nknull = test[test['keyword'].isnull()]\ntest.dropna(subset=['keyword'], inplace=True)\n\nfinal_kwords = ['Arsenal', 'hey', 'hat', 'fuck', 'cold', 'no', 'no', 'what', 'awesome']\nknull['keyword'] = final_kwords\n\ntest = pd.concat([test, knull])","c561089b":"kddf = pd.DataFrame({'Count':ddf['keyword'].value_counts()})\nsddf = pd.DataFrame({'Count':sdf['keyword'].value_counts()})\n\nfig, ax = plt.subplots(1,2, figsize=(20,8), constrained_layout=True)\nsns.barplot(y=kddf[0:30].index, x=kddf[0:30]['Count'], ax=ax[0], palette='Reds_d')\nax[0].set_title('Most Common Disaster Keywords')\nsns.barplot(y=sddf[0:30].index, x=sddf[0:30]['Count'], ax=ax[1], palette='Greens_d')\nax[1].set_title('Most Common Non-Disaster Keywords')\nplt.show()","03206f79":"tweet_len = []\n\nfor index, item in train.iterrows():\n    tweet_len.append(len(item['Bigrams'])+1)\n    \ntrain['text_word_count'] = tweet_len\n\nmn_word_lens = []\nfor index, item in train.iterrows():\n    words = nltk.word_tokenize(item['clean_text'])\n    word_lens = [len(w) for w in words]\n    \n    avg_word_lens = np.mean(word_lens)\n    mn_word_lens.append(avg_word_lens)\n    \ntrain['mean_word_lengths'] = mn_word_lens\n\n\nddf = train[train['target']==1]\nsdf = train[train['target']==0]","75bf44ef":"tweet_len = []\n\nfor index, item in test.iterrows():\n    tweet_len.append(len(item['Bigrams'])+1)\n    \ntest['text_word_count'] = tweet_len\n\nmn_word_lens = []\nfor index, item in test.iterrows():\n    words = nltk.word_tokenize(item['clean_text'])\n    word_lens = [len(w) for w in words]\n    \n    avg_word_lens = np.mean(word_lens)\n    mn_word_lens.append(avg_word_lens)\n    \ntest['mean_word_lengths'] = mn_word_lens\n","fb4e3f11":"sns.set_palette('tab10')\nsns.barplot(x=['Non-Disaster', 'Disaster'], y=[sdf['text_word_count'].mean(), ddf['text_word_count'].mean()])\nplt.title('Average Tweet Word Count')\nplt.ylabel('Avg. Words')\n\nplt.figure(figsize=(8, 6))\nsns.set_palette('Greens_d')\nsns.histplot(x='mean_word_lengths', data=sdf)\nplt.title('Non-Disaster Avg. Word Length Distribution')\nplt.xlabel('Avg. Word Length')\nplt.ylabel('Frequency')\n\nplt.figure(figsize=(8, 6))\nsns.set_palette('Reds_d')\nsns.histplot(x='mean_word_lengths', data=ddf)\nplt.title('Disaster Avg. Word Length Distribution')\nplt.xlabel('Avg. Word Length')\nplt.ylabel('Frequency')\n\n\nprint('Avg. Non-Disaster Word Count:', sdf['text_word_count'].mean())\nprint('Avg. Disaster Word Count:', ddf['text_word_count'].mean())\nprint('Avg. Non-Disaster Word Length:', sdf['mean_word_lengths'].mean())\nprint('Avg. Disaster Word Length:', ddf['mean_word_lengths'].mean())","c9ef6a9b":"pol = lambda x: TextBlob(x).sentiment.polarity\nsub = lambda x: TextBlob(x).sentiment.subjectivity\n\ntrain['Polarity'] = train['clean_text'].apply(pol)\ntrain['Subjectivity'] = train['clean_text'].apply(sub)\ntest['Polarity'] = test['clean_text'].apply(pol)\ntest['Subjectivity'] = test['clean_text'].apply(sub)\n\nddf = train[train['target']==1]\nsdf = train[train['target']==0]\n\nsns.set_palette('tab10')\nsns.barplot(x=['Non-Disaster', 'Disaster'], y=[sdf['Polarity'].mean(), ddf['Polarity'].mean()])\nplt.title('Average Tweet Polarity')\nplt.ylabel('Polarity')\n\nplt.figure()\nsns.barplot(x=['Non-Disaster', 'Disaster'], y=[sdf['Subjectivity'].mean(), ddf['Subjectivity'].mean()])\nplt.title('Average Tweet Subjectivity')\nplt.ylabel('Subjectivity')","a1f3a85f":"kword = pd.DataFrame({'Count':train[['keyword', 'target']].value_counts()})\n\nkword.reset_index(inplace=True)\nkword['D-sum'] = kword['target']*kword['Count']\n\ndf = kword.groupby('keyword').sum()\ndf.reset_index(inplace=True)\ndf['Disaster_Prob'] = df['D-sum']\/df['Count']\n\ndisplay(df.sort_values('Disaster_Prob', ascending=False).head(20))\nlowest_prob = df.sort_values('Disaster_Prob', ascending=True)\nlowest_prob = lowest_prob[lowest_prob['Count']>=5]\ndisplay(lowest_prob.head(20))","70d828bc":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\nX = train['clean_text']\ny = train['target']\n\ncv = CountVectorizer(stop_words='english')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\ncv_train = cv.fit_transform(X_train.values)\ncv_test = cv.transform(X_test.values)\n\nnb = MultinomialNB()\n\nmodel = nb.fit(cv_train, y_train)\n\ny_pred = model.predict(cv_test)\n\nacc = accuracy_score(y_test, y_pred)\nprint(acc)\n\n\n","23f52edc":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import Lasso, RidgeClassifier, LogisticRegression\nfrom sklearn.svm import SVC\n\nX = train['clean_text']\ny = train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\ntf = TfidfVectorizer(stop_words='english', ngram_range=(1,1))\n\ntf_train = tf.fit_transform(X_train.values)\ntf_test = tf.transform(X_test.values)\n\nrf = RandomForestClassifier()\ndtc = DecisionTreeClassifier()\nlog = LogisticRegression()\nsvc = SVC()\nrid = RidgeClassifier()\n\nclfs = [rf, dtc, log, svc, rid]\nnames = ['Random Forest', 'Decision Tree', 'Logistic Regression', 'Support Vector', 'Ridge']\n\n\nfor (i, n) in zip(clfs, names):\n    model = i.fit(tf_train, y_train)\n    y_pred = model.predict(tf_test)\n    acc = accuracy_score(y_test, y_pred)\n    print(n, acc)","8a7dfc59":"X = train['clean_text']\ny = train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\ncv = CountVectorizer(stop_words='english', ngram_range=(1,1))\n\ncv_train = cv.fit_transform(X_train.values)\ncv_test = cv.transform(X_test.values)\n\nrf = RandomForestClassifier()\ndtc = DecisionTreeClassifier()\nlog = LogisticRegression()\nsvc = SVC()\n\nclfs = [rf, dtc, log, svc, rid]\nnames = ['Random Forest', 'Decision Tree', 'Logistic Regression', 'Support Vector', 'Ridge']\n\n\nfor (i, n) in zip(clfs, names):\n    model = i.fit(cv_train, y_train)\n    y_pred = model.predict(cv_test)\n    acc = accuracy_score(y_test, y_pred)\n    print(n, acc)","539bbdaa":"X = train[['mean_word_lengths', 'text_word_count', 'Polarity', 'Subjectivity', 'keyword']]\nX = pd.get_dummies(X)\ny = train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\nrf = RandomForestClassifier()\ndtc = DecisionTreeClassifier()\nlog = LogisticRegression()\nsvc = SVC()\nrid = RidgeClassifier()\n\nclfs = [rf, dtc, log, svc, rid]\nnames = ['Random Forest', 'Decision Tree', 'Logistic Regression', 'Support Vector', 'Ridge']\n\n\nfor (i, n) in zip(clfs, names):\n    model = i.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    print(n, acc)","bda17fdc":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import Lasso, RidgeClassifier, LogisticRegression\nfrom sklearn.svm import SVC\n\n\"\"\"X = train[['clean_text', 'mean_word_lengths', 'text_word_count', 'Polarity', 'Subjectivity']]\ny = train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\ntf = TfidfVectorizer(stop_words='english')\n\ntf_train = tf.fit_transform(X_train['clean_text'])\ntf_test = tf.transform(X_test['clean_text'])\nX_train_tf = pd.concat([X_train[['mean_word_lengths', 'text_word_count', 'Polarity', 'Subjectivity']].reset_index(drop=True),\\\n                        pd.DataFrame(tf_train.toarray(), columns=tf.get_feature_names())], axis=1)\nX_test_tf = pd.concat([X_test[['mean_word_lengths', 'text_word_count', 'Polarity', 'Subjectivity']].reset_index(drop=True),\\\n                        pd.DataFrame(tf_test.toarray(), columns=tf.get_feature_names())], axis=1)\n\nrf = RandomForestClassifier()\ndtc = DecisionTreeClassifier()\nlog = LogisticRegression(max_iter=1000)\nsvc = SVC()\nrid = RidgeClassifier()\n\nclfs = [rf, dtc, log, rid, svc]\nnames = ['Random Forest', 'Decision Tree', 'Logistic Regression', 'Ridge', 'Support Vector']\n\n\nfor (i, n) in zip(clfs, names):\n    model = i.fit(X_train_tf, y_train)\n    y_pred = model.predict(X_test_tf)\n    acc = accuracy_score(y_test, y_pred)\n    print(n, acc)\"\"\"","e57a2a94":"\"\"\"X = train[['clean_text', 'mean_word_lengths', 'text_word_count', 'Polarity', 'Subjectivity']]\ny = train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\ncv = CountVectorizer(stop_words='english')\n\ncv_train = cv.fit_transform(X_train['clean_text'])\ncv_test = cv.transform(X_test['clean_text'])\nX_train_cv = pd.concat([X_train[['mean_word_lengths', 'text_word_count', 'Polarity', 'Subjectivity']].reset_index(drop=True),\\\n                        pd.DataFrame(cv_train.toarray(), columns=cv.get_feature_names())], axis=1)\nX_test_cv = pd.concat([X_test[['mean_word_lengths', 'text_word_count', 'Polarity', 'Subjectivity']].reset_index(drop=True),\\\n                        pd.DataFrame(cv_test.toarray(), columns=cv.get_feature_names())], axis=1)\n\nrf = RandomForestClassifier()\ndtc = DecisionTreeClassifier()\nlog = LogisticRegression(max_iter=1000)\nsvc = SVC()\nrid = RidgeClassifier()\n\nclfs = [rf, dtc, log, rid, svc]\nnames = ['Random Forest', 'Decision Tree', 'Logistic Regression', 'Ridge', 'Support Vector']\n\n\nfor (i, n) in zip(clfs, names):\n    model = i.fit(X_train_cv, y_train)\n    y_pred = model.predict(X_test_cv)\n    acc = accuracy_score(y_test, y_pred)\n    print(n, acc)\"\"\"","9a7d71cc":"test['clean_text'] = test['text'].apply(clean)\n\nX_train = train['clean_text']\ny_train = train['target']\nX_test = test['clean_text']\n\ntf = TfidfVectorizer(stop_words='english')\n\ntf_train = tf.fit_transform(X_train.values)\ntf_test = tf.transform(X_test.values)\n\nsvc = SVC()\n\nmodel = svc.fit(tf_train, y_train)\n\ny_pred = model.predict(tf_test)\n\noutput = pd.DataFrame({'id':test.id, 'target':y_pred})\noutput1 = output.to_csv('DisasterSubmissionFinal.csv', index=False)","f64f9879":"Random Forest 0.7783613445378151\n\nDecision Tree 0.6901260504201681\n\nLogistic Regression 0.8098739495798319\n\nRidge 0.7809873949579832\n\nSupport Vector 0.7158613445378151\n","4febe5c0":"Next I define a function to make all the text lower case and to strip all numbers and punctuation.","74b1e7c2":"Here I filled in the null keyword values. First I looked to see if any of the tweets with a null value already had one of the other keywords in it, and most of them did. The remaining ones were few enough that I could just manually pass them in.","5f0c1134":"ngram_range=(1,2)\n\nRandom Forest 0.7752100840336135\n\nDecision Tree 0.7074579831932774\n\nLogistic Regression 0.7878151260504201\n\nSupport Vector 0.789390756302521\n\nRidge 0.7935924369747899","688b8da3":"Here I tried running a few different classifiers with both Count and Tfidf Vectorizer. I also tried each one with ngram_range = (1,2) but the results weren't much better. Support Vector, Logisitic Regression, and Ridge all seem very close in accuracy so they are all worth trying on the test set.","75cc9232":"No surprise here, disaster tweets have a much lower average polarity as they are describing negative events. They are also a but less subjective which I think also makes sense since they are often times simply news reports describing the disaster.","251ae5f3":"Here is the disaster frequency of each keyword. It seems like anything having to do with bombs, 'debris', or 'wreckage' are very likely actual disaster tweets. In the lower frequency table we see some familiar words again like 'ruin', 'body bags', and 'wrecked'. It's interesting that 'wreckage' has a high disaster frequency but 'wrecked' does not.","cbfdde17":"Here I define a function to get the bigrams and tokens for each tweet.","683178be":"First I import the data and look at the frequency of disasters vs non-disasters. Seems relatively even, so that's good.","087c5faf":"Random Forest 0.7878151260504201\n\nDecision Tree 0.6906512605042017\n\nLogistic Regression 0.803046218487395\n\nRidge 0.8035714285714286\n\nSupport Vector 0.6197478991596639","98f082a0":"These both took very long to run but I tried combining the other variables with the matrices produced by Count and Tfidf Vectorizer. Overall there was no reall improvement in accuracy so I didn't use it on the final model. ","e839f177":"Using a SVC model with a Tfidf Vectorizer wound up giving me my most accurate results. I thought incorporating other features besides just the clean text would improve the score but it didn't. I also found this interesting because it logistic regression was more accurate on the train set.","d1b82162":"Here I made a bag of words for the disaster and the non-disaster tweets, with the disaster one on top. It seems 'fire' and 'news' are the most common words for real disasters while 'like' and 'get' are the most common for non-disasters.","bad418e6":"ngram_range = (1,2)\n\nRandom Forest 0.7683823529411765\n\nDecision Tree 0.7605042016806722\n\nLogistic Regression 0.803046218487395\n\nSupport Vector 0.7809873949579832\n\nRidge 0.7899159663865546","cfc8666b":"I'm not particularly surprised about the top non-disaster keywords. 'Body bag', 'wrecked', and 'ruin' are often used when roasting someone online, and 'Armageddon' has many pop-culuture uses. In fact, I'd be surprised if anyone ever used that word for a real disaster. I am a bit surprised to see 'harm' as high as it is.","56cedfb9":"I then make a bag of words by removing stop words. I added a couple words that weren't included in the default stop word list but appeared frequently in the tweets.","85992c15":"Here I plotted the difference in tweet lengths and average word lengths in each tweet. Disaster tweets seem to be a bit longer and use slightly longer words on average, which I think is expected since they usually have more information to tell. However the difference isn't as big as I expected.","0b651da5":"This was pure experimentation and I didn't really expect it to work but Logisitic Regression and Ridge actually did a fair job here without even looking at the actual text, and only the other variables"}}