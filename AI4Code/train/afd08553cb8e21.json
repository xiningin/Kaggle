{"cell_type":{"9b77f309":"code","bb73fc1a":"code","d94f877e":"code","2b1513fc":"code","425557df":"code","2d2cd354":"code","160bcf9e":"code","742118e7":"code","61f323c3":"code","1ec76525":"code","bb572310":"code","78c7c8d3":"code","b237e2f5":"code","e951f33b":"code","1a9eaa07":"code","a367d992":"code","e633532f":"code","7afae6e9":"code","8cb4d816":"code","0780c299":"code","052bbcc1":"code","e1f00d4b":"code","a8c02ee8":"code","8f427b80":"code","9496c0b1":"code","ef40c2f7":"code","88a20741":"code","2828c126":"code","be23f7fc":"code","e7bd779e":"code","89aac4a6":"code","bc9a96b5":"code","f81aebac":"code","cc188ec6":"code","676dd314":"code","8c7a034a":"code","a3522766":"code","3757aa7a":"code","1b142a25":"code","989ceadc":"code","793641d9":"code","74c7a41f":"code","b8658d26":"markdown","f3dd520f":"markdown","eb218733":"markdown","f4b2b4f2":"markdown","ce66569e":"markdown","41d4a0f8":"markdown","e2dcdd1d":"markdown","d44797d4":"markdown","288a55b6":"markdown","a7078cee":"markdown","4b927749":"markdown","750bd0d1":"markdown","cccf4fcb":"markdown","4e2e4f4d":"markdown","0ad50d14":"markdown","5f5ab85d":"markdown","7dcda43b":"markdown","e4cc333e":"markdown","e9674955":"markdown","8248101b":"markdown","4fb9d719":"markdown","cee066c4":"markdown","1a52a8f1":"markdown"},"source":{"9b77f309":"import numpy as np # linear algebra\nimport pandas as pd # data processing,\nimport matplotlib.pyplot as plt # information ploting","bb73fc1a":"dataset = pd.read_csv(\"..\/input\/breast-cancer-coimbra-data-set\/dataR2.csv\")\ndataset.head()","d94f877e":"dataset.info()","2b1513fc":"y = dataset.iloc[:,-1].values\nX = dataset.iloc[:,:-1]","425557df":"y[np.where(y == 1)] = 0 # labeled as healthy\ny[np.where(y == 2)] = 1 # labeled as sick\ny","2d2cd354":"X.mean(axis=0)","160bcf9e":"def unison_shuffle(a, b):\n  # make a shuffle index array to make a fixed shuffling order for both x, y\n  inx = np.random.permutation(a.shape[0])  \n  return a.iloc[inx].reset_index(drop=True), b[inx]","742118e7":"X, y = unison_shuffle(X, y)","61f323c3":"X","1ec76525":"from seaborn import heatmap\n\nplt.figure(figsize=(9,9))\nheatmap(X.corr(), linewidth=0.5, annot=True);","bb572310":"fig = plt.figure(figsize=(10,10))\n\nfor index, feature in enumerate(X,1):\n    plt.subplot(3, 3, index)\n    plt.scatter(np.arange(116)[y==1], X[feature][y==1], c='r')\n    plt.scatter(np.arange(116)[y==0], X[feature][y==0], c='b')\n    plt.title(feature)\n    \nfig.tight_layout()\nplt.show()","78c7c8d3":"features_ls = [[feature1, feature2] for feature1 in X for feature2 in X if feature1!=feature2];\nfeatures_ls;","b237e2f5":"color = np.where(y==1, 'r', 'b')\nfig = plt.figure(figsize=(30,30))\n\nfor index, features in enumerate(features_ls):\n    plt.subplot(X.shape[1], X.shape[1]-1, index+1)\n    plt.scatter(X[features[0]], X[features[1]], c=color)\n    \n    # corr(x,y) = cov(x,y)\/(std(x)*std(y))\n    corr = round(X.corr()[features[0]][features[1]], 4)\n    plt.title(corr)\n    plt.xlabel(features[0])\n    plt.ylabel(features[1])\n    \n    #finding linear relationships\n    if(corr > 0.8 or corr < -0.8):\n        m, b = np.polyfit(X[features[0]], X[features[1]],1)\n        y_corr = m * X[features[0]] + b\n        plt.plot(X[features[0]], y_corr)\n\nfig.tight_layout()\nplt.show()","e951f33b":"color = np.where(y==1, 'r', 'b')\n\nplt.figure(figsize=(6,6))\nplt.scatter(X.HOMA, X.Insulin,c=color)","1a9eaa07":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Standardization\nX_new = StandardScaler().fit_transform(X.to_numpy())\n\n# PCA\nX_new = PCA(.9).fit_transform(X_new)\n","a367d992":"print('considering new dataset:', X_new[:5])\nprint('shape of the new dataset:', X_new.shape)","e633532f":"ds_new = pd.DataFrame(X_new, columns=['PC1','PC2','PC3','PC4','PC5','PC6'])\nds_new","7afae6e9":"from sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report","8cb4d816":"X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=.2,random_state=1)","0780c299":"X_train[:5]","052bbcc1":"rbf_tuning_parameters = [{'kernel': ['rbf'], \n                       'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n                       'C': [1, 10, 100, 1000]}]\n\nrbf_svm_clf = GridSearchCV(SVC(), rbf_tuning_parameters, cv=5)","e1f00d4b":"rbf_svm_clf.fit(X_train, y_train)","a8c02ee8":"print(rbf_svm_clf.best_params_)","8f427b80":"y_pred_rbf_clf = rbf_svm_clf.predict(X_test)\nprint(classification_report(y_test, y_pred_rbf_clf))\nprint(accuracy_score(y_test, y_pred_rbf_clf))","9496c0b1":"lr_tuning_parameters = [{'kernel': ['linear'], \n                        'C': [1, 10, 100, 1000]}]\nlr_svm_clf = GridSearchCV(SVC(), lr_tuning_parameters, cv=5)","ef40c2f7":"lr_svm_clf.fit(X_train, y_train)","88a20741":"print(lr_svm_clf.best_params_)","2828c126":"y_pred_lr_clf = lr_svm_clf.predict(X_test)\nprint(classification_report(y_test, y_pred_lr_clf))\nprint(accuracy_score(y_test, y_pred_lr_clf))","be23f7fc":"dt_tuning_parameters = [{'criterion':['gini', 'entropy'],\n                         'max_depth':range(1,10),\n                       }]\n\nds_clf = GridSearchCV(DecisionTreeClassifier(), dt_tuning_parameters, cv=5)","e7bd779e":"ds_clf.fit(X_train, y_train)","89aac4a6":"print(ds_clf.best_params_)","bc9a96b5":"y_pred_ds_clf = ds_clf.predict(X_test)\nprint(classification_report(y_test, y_pred_ds_clf))\nprint(accuracy_score(y_test, y_pred_ds_clf))","f81aebac":"knn_tuning_parameters = [{'n_neighbors': range(3,10),\n                         'weights':['uniform', 'distance'] ,\n                       }]\n\nknn_clf = GridSearchCV(KNeighborsClassifier(), knn_tuning_parameters, cv=5)","cc188ec6":"knn_clf.fit(X_train, y_train)","676dd314":"print(knn_clf.best_params_)","8c7a034a":"y_pred_knn_clf = knn_clf.predict(X_test)\nprint(classification_report(y_test, y_pred_knn_clf))\nprint(accuracy_score(y_test, y_pred_knn_clf))","a3522766":"ls_reg_tuning_parameters = [{'penalty':['l1', 'l2', 'elasticnet'],\n                             'C':np.logspace(-4,4,10),\n                             'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n                             }]\n\nls_reg = GridSearchCV(LogisticRegression(), ls_reg_tuning_parameters, cv=5)","3757aa7a":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nls_reg.fit(X_train, y_train)","1b142a25":"print(ls_reg.best_params_)","989ceadc":"y_pred_ls_reg = ls_reg.predict(X_test)\nprint(classification_report(y_test, y_pred_ls_reg))\nprint(accuracy_score(y_test, y_pred_ls_reg))","793641d9":"def model_report(X_train, X_test, y_trian, y_test, models, models_name):\n    for model, model_name in zip(models, models_name):\n        y_pred_train = model.predict(X_train)\n        y_pred_test = model.predict(X_test)\n        print('Classification Report of {}: '.format(model_name))\n        print('Train set accuracy:', accuracy_score(y_train, y_pred_train))\n        print('validation set accuracy:', accuracy_score(y_test, y_pred_test))\n        print()","74c7a41f":"models = [rbf_svm_clf, lr_svm_clf, ds_clf, knn_clf, ls_reg]\nmodels_name = ['SVM(RBF Kernel)', 'SVM(Linear Kernel)', 'Decision Tree', 'KNN', 'Logistic Regression']\nmodel_report(X_train, X_test, y_train, y_test, models, models_name)","b8658d26":"### train\/validation split","f3dd520f":"## Conclusion","eb218733":"### Logistic Regression","f4b2b4f2":"### KNN","ce66569e":"so we can see that the SVM model with RBF kernel have the best result","41d4a0f8":"1. SVC(RBF Kernel)\n2. SVC(Linear Kernel)\n3. Decision Tree\n4. KNN\n5. Logistic Regression","e2dcdd1d":"as we can see we have high correlation between HOMA and Insulin feature\n\nSolution:\n1. we can omit one of the features\n\n'but we can use PCA instead foe feature extraction in the following sections'","d44797d4":"among the models, above we have to count on the models that : \n1. the train set accuracy and validation set accuracy are nearer to each other\n2. having high accuracy both in train set and validation set","288a55b6":"### Customized Model Report","a7078cee":"now lets have a broader look at :\n1. first, one feature data distibution\n2. second, two feature data distribution\nwith scatter plot to see wheathre we have any observable class seprabability based on one or two feature(s)","4b927749":"## Importing Dataset","750bd0d1":"### shuffle the dataset","cccf4fcb":"## Models Prediction\n","4e2e4f4d":"### Decision Tree","0ad50d14":"for each of the models we'll use GridSearch hyperparameter tuning on 5-fold crossvalidation","5f5ab85d":"again, we can not surely rely on these distributions, excepts for the colinearality of HOMA and Insulin features ","7dcda43b":"## Importing Basic Libararies","e4cc333e":"### SVC( Linear Kernel )","e9674955":"## Feature Engineering","8248101b":"### PCA","4fb9d719":"### SVC( RBF Kernel )","cee066c4":"seemingly we do not have any good seprabability with one feature.\n\nlet's see for two feature:","1a52a8f1":"Heatmap"}}