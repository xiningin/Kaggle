{"cell_type":{"82611870":"code","a71fa31a":"code","d8c5270b":"code","17b532b2":"code","8e89e64c":"code","35355f02":"code","9c3f2947":"code","02a66ba0":"code","1c7640ad":"code","49e16070":"code","f28705ff":"code","8a01ba2b":"code","29d523e5":"code","ba2d7c2b":"code","841798c2":"markdown","3d3fdc80":"markdown","630e7c71":"markdown","723b61e9":"markdown","e36ef62a":"markdown","fac90969":"markdown","c47a8dac":"markdown","a4e33dfe":"markdown","9332526c":"markdown","edd21a93":"markdown","40c24b58":"markdown","5865a7fb":"markdown","95052f6a":"markdown","82768102":"markdown"},"source":{"82611870":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport random\nimport os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.nn.functional as F\nimport torch.optim as optim","a71fa31a":"# Telling pytorch to use GPU\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","d8c5270b":"transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n     transforms.Resize((64, 64))])\n\nbatch_size = 64\n\ntrainset = torchvision.datasets.ImageFolder(\n    root='..\/input\/cell-images-parasitized-or-not\/cell_images\/train',\n    transform=transform\n)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.ImageFolder(\n    root='..\/input\/cell-images-parasitized-or-not\/cell_images\/test',\n    transform=transform\n)\n\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\ntrainset.class_to_idx","17b532b2":"# Saving Dict to a list for later use\nclasses = list(trainset.class_to_idx)","8e89e64c":"# functions to show an image\ndef imshow(img):\n    plt.figure(figsize=(10, 10))\n    img = img \/ 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# get some random training images\ndataiter = iter(trainloader)\nimages, labels = dataiter.next()\n\n# show images\nimshow(torchvision.utils.make_grid(images, 8))","35355f02":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 10, 5)\n        self.fc1 = nn.Linear(10 * 13 * 13, 8000)\n        self.fc2 = nn.Linear(8000, 4000)\n        self.fc3 = nn.Linear(4000, 2000)\n        self.fc4 = nn.Linear(2000, 1000)\n        self.fc5 = nn.Linear(1000, 500)\n        self.fc6 = nn.Linear(500, 250)\n        self.fc7 = nn.Linear(250, 2)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc4(x))\n        x = F.relu(self.fc5(x))\n        x = F.relu(self.fc6(x))\n        x = self.fc7(x)\n        return x\n    \nmodel = Model().to(device)","9c3f2947":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","02a66ba0":"total_loss = []\n\nfor epoch in range(8):  # loop over the dataset multiple times\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs.to(device))\n        loss = criterion(outputs, labels.to(device))\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n       \n        if i % 100 == 99:\n            print('[epoch, batch] =  [%d, %5d] loss: %.6f' %(epoch + 1, i, running_loss \/ 100))\n            total_loss.append(running_loss \/ 100)\n            running_loss = 0.0\n\nprint('Finished Training')","1c7640ad":"plt.plot(total_loss)\nplt.title(\"LOSS Graph\")\nplt.show()","49e16070":"correct = 0\ntotal = 0\n\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = model(images.to(device))\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels.to(device)).sum().item()\n\nprint('Accuracy of the network on the test images: %d %%' % (\n    100 * correct \/ total))","f28705ff":"# prepare to count predictions for each class\ncorrect_pred = {classname: 0 for classname in classes}\ntotal_pred = {classname: 0 for classname in classes}\n\n# again no gradients needed\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = model(images.to(device))\n        _, predictions = torch.max(outputs, 1)\n        # collect the correct predictions for each class\n        for label, prediction in zip(labels.to(device), predictions):\n            if label == prediction:\n                correct_pred[classes[label]] += 1\n            total_pred[classes[label]] += 1\n\n\n# print accuracy for each class\nfor classname, correct_count in correct_pred.items():\n    accuracy = 100 * float(correct_count) \/ total_pred[classname]\n    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n                                                   accuracy))","8a01ba2b":"file = random.choice([x for x in os.listdir(\"..\/input\/cell-images-parasitized-or-not\/cell_images\/train\/uninfected\")])\nimage = Image.open(f'..\/input\/cell-images-parasitized-or-not\/cell_images\/train\/uninfected\/{file}')\nplt.imshow(image)\nimage = transform(image)\nimage = image.unsqueeze(0).to(device)\nmodel.eval()\noutput = model(image)\noutput = output.squeeze().tolist()\nclasses[output.index(max(output))]","29d523e5":"file = random.choice([x for x in os.listdir(\"..\/input\/cell-images-parasitized-or-not\/cell_images\/train\/parasitized\")])\nimage = Image.open(f'..\/input\/cell-images-parasitized-or-not\/cell_images\/train\/parasitized\/{file}')\nplt.imshow(image)\nimage = transform(image)\nimage = image.unsqueeze(0).to(device)\nmodel.eval()\noutput = model(image)\noutput = output.squeeze().tolist()\nclasses[output.index(max(output))]","ba2d7c2b":"folder = random.choice(['parasitized', 'uninfected'])\nfile = random.choice([x for x in os.listdir(f\"..\/input\/cell-images-parasitized-or-not\/cell_images\/train\/{folder}\")])\nimage = Image.open(f'..\/input\/cell-images-parasitized-or-not\/cell_images\/train\/parasitized\/{file}')\nplt.imshow(image)\nimage = transform(image)\nimage = image.unsqueeze(0).to(device)\nmodel.eval()\noutput = model(image)\noutput = output.squeeze().tolist()\nprint('Predicted: '+classes[output.index(max(output))]+'.\\nGround: '+folder)","841798c2":"![image.png](attachment:a5acddaa-c368-44e1-a3b9-507fa6a37079.png)","3d3fdc80":"## Testing per CLASS:","630e7c71":"# Importing Libraries","723b61e9":"# THANK  YOU.","e36ef62a":"### RANDOM PICK","fac90969":"<img alt=\"Thumbs Up Gif - IceGif\" class=\"n3VNCb\" src=\"https:\/\/www.icegif.com\/wp-content\/uploads\/thumbs-up-icegif-12.gif\" data-noaft=\"1\" jsname=\"HiaYvf\" jsaction=\"load:XAeZkd;\" style=\"width: 800px; height: 400px; margin: 0px;\">","c47a8dac":"##  Defining optimizer\n![image.png](attachment:17e404a6-648b-4f84-be9c-ff13937bcfca.png)","a4e33dfe":"## Testing on Whole MODEL:","9332526c":"### Randomly on infected:","edd21a93":"# Loading DataSet and Applying Transformation","40c24b58":"# CNN MODEL","5865a7fb":"### Randomly on uninfected:","95052f6a":"# Testing Model:","82768102":"![image.png](https:\/\/miro.medium.com\/max\/2250\/0*Ra55_QqeYjDWH0ZR.gif)"}}