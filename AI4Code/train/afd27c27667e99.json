{"cell_type":{"2baeeef5":"code","932a8c7d":"code","d9af5981":"code","a36af43e":"code","ca2d47e4":"code","e4049df6":"code","692e1383":"code","a8c596d5":"code","09872bcc":"code","6caa33bc":"code","b144ea33":"code","6b9202d0":"code","8297ca43":"code","b09791fc":"code","e7ba01e7":"markdown","54934fec":"markdown","e2b71c1c":"markdown","1640ed72":"markdown","38d05841":"markdown","36b3faea":"markdown","f6b75854":"markdown","6a323b92":"markdown","34133809":"markdown","2d6f4441":"markdown","4086f5c1":"markdown","ae1060e7":"markdown"},"source":{"2baeeef5":"!pip install pytorch_transformers\n!pip install pymorphy2\n!pip install autocorrect","932a8c7d":"import os\nimport re\nimport time\nimport random\nimport datetime\n\nimport nltk\nimport torch\nimport pymorphy2\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom nltk.corpus import words\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer\nfrom transformers import get_linear_schedule_with_warmup\nfrom transformers import BertForSequenceClassification, AdamW\nfrom autocorrect import Speller","d9af5981":"class DataPreparation():\n    \n    def __init__(self):\n        \n        #\n        self.stop_words = None\n        self.stop_words_excep = ['\u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442', '\u043e\u043f\u0435\u0440\u0430\u0442\u043e\u0440', '\u0442\u0430\u0440\u0438\u0444', '\u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0430',\n                                 '\u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442', '\u043d\u0435\u0442']\n        \n        self.stop_words_extra = ['\u0434\u043e\u0431\u0440\u044b\u0439', '\u0434\u043e\u0431\u0440\u043e\u0433\u043e', '\u0434\u043e\u0431\u0440\u044b', '\u0434\u043e\u0431\u0440\u043e\u0439', '\u0434\u043e\u0431\u0440\u043e\u0435', '\u0434\u043e\u0431\u0440\u044b\u0435',\n                                 '\u0437\u0434\u0440\u0430\u0432\u0441\u0442\u0432\u0443\u0439\u0442\u0435', '\u0437\u0434\u0440\u0430\u0441\u0442\u0435', '\u0437\u0434\u0440\u0430\u0432\u0447\u0442\u0432\u0443\u0439\u0442\u0435\u0442', '\u0437\u0434\u0440\u0430\u0441\u0442\u0438\u0432\u0438\u0442\u0438',\n                                 '\u0437\u0434\u0440\u0430\u0432\u0441\u0442\u0432\u0443\u0439', '\u0437\u0434\u0440\u0430\u0441\u0442\u0443\u0432\u0443\u0439\u0442\u0435', '\u0437\u0434\u0440\u0430\u0441\u0442\u0432\u0443\u0442\u0439\u0442\u0435', '\u0437\u0434\u0440\u0441\u0442\u0438', \n                                 '\u0437\u0434\u0440\u044c\u0442\u0435', '\u0432\u043e\u043f\u0440\u043e\u0441\u0437\u0434\u0440\u0430\u0432\u0441\u0442\u0432\u0443\u0439\u0442\u0435', '\u0432\u0432\u0435\u0434\u0438\u0442\u0435\u0437\u0434\u0440\u0430\u0432\u0441\u0442\u0432\u0443\u0439\u0442\u0435', \n                                 '\u0437\u0434\u0440\u0430\u0441\u044c\u0442\u0435', '\u0437\u0434\u0440\u0430\u0441\u0442\u0443\u0439\u0442\u0435', '\u0437\u0434\u0440\u0430\u0430\u0441\u0442\u0430\u0443\u0439\u0442\u0435', '\u0432\u0432\u0435\u0434\u0437\u0434\u0440\u0430\u0432\u0441\u0442\u0432\u0443\u0439\u0442\u0435',\n                                 '\u0437\u0434\u0440\u0430\u0432\u0441\u0442\u0435', '\u043f\u0440\u0438\u0432\u0435\u0442', '\u043f\u0440\u0438\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e', '\u0438\u043b\u0438', '\u043a\u043e\u0442\u043e\u0440\u044b\u0439',\n                                 '\u043e\u0447\u0435\u043d\u044c', '\u0435\u0449\u0435', '\u044d\u0442\u043e']\n        \n        # \u0421\u043b\u043e\u0432\u0430\u0440\u044c \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0445 \u0441\u043b\u043e\u0432\n        nltk.download('words')\n        self.vocab_en = set(w.lower() for w in words.words())\n        \n        \n    def load_csv(self, path_in, sep=',', encoding='utf-8', engine='python'):\n            \"\"\" \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c CSV-\u0444\u0430\u0439\u043b\n            \"\"\"\n            if not engine == 'python':\n                engine = None \n            return  pd.read_csv(path_in, sep=sep, encoding=encoding, engine=engine)\n    \n    \n    def save_csv(self, df, path_out, sep=',', encoding='utf-8', index=False, header=True):\n            \"\"\" \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c CSV-\u0444\u0430\u0439\u043b\n            \"\"\"\n            path_out = path_out.replace('\\\\','\/')\n            if not os.path.exists('\/'.join(path_out.split('\/')[0:-1])):\n                os.makedirs('\/'.join(path_out.split('\/')[0:-1]))\n            df.to_csv(path_out, index=index, header=header, sep=sep, encoding=encoding) \n    \n    \n    def fill_na(self, df, column_str, replace_to=''):\n            \"\"\" \u0417\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432\n            \"\"\"\n            df[column_str] = df[column_str].fillna(replace_to)\n            \n    def clear_chars(self, text):\n            \"\"\" \u0423\u0434\u0430\u043b\u044f\u0435\u043c \u043b\u0438\u0448\u043d\u0438\u0435 \u0441\u0438\u043c\u0432\u043e\u043b\u044b, \u0432\u043a\u043b\u044e\u0447\u0430\u044f \u043b\u0438\u0448\u043d\u0438\u0435 \u043f\u0440\u043e\u0431\u0435\u043b\u044b \u0438 \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u044b \u0441\u0442\u0440\u043e\u043a\n            \"\"\"\n            text = re.sub('[^a-zA-Z\u0430-\u044f\u0410-\u042f 0-9]+', ' ', text) \n            return ' '.join(text.split())\n    \n    def do_lower(self, text):\n            \"\"\" \u041f\u0440\u0438\u0432\u043e\u0434\u0438\u043c \u0442\u0435\u043a\u0441\u0442 \u043a \u043d\u0438\u0436\u043d\u0435\u043c\u0443 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0443\n            \"\"\"\n            return text.lower()\n    \n    \n    def replace_empty_to_max_freq_label(self, df, text_str, label_str, empty_str=''):\n            \"\"\" \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u0443\u044e \u043c\u0435\u0442\u043a\u0443 \u0434\u043b\u044f \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n                \u0438 \u043f\u0440\u0438\u0441\u0432\u0430\u0438\u0432\u0430\u0435\u043c \u0435\u0451 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432.\n            \"\"\"\n            df_empty = df[df[text_str] == empty_str]\n            df_empty_uniq = pd.value_counts(df_empty.values.ravel())[1:]\n            try:\n                max_freq_label = df_empty_uniq.index[0]\n                df_empty_indx = df_empty.index.to_list()\n                for empty_indx in df_empty_indx:\n                    df.loc[empty_indx,label_str] = max_freq_label\n            except:\n                pass\n    \n    \n    def tokenize_by_rules(self, string):\n            \"\"\" \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0441\u0442\u0440\u043e\u043a\u0443 \u043f\u043e \u0437\u0430\u0434\u0430\u043d\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u0435(\u043f\u0440\u0430\u0432\u0438\u043b\u0430\u043c \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u044f)\n            \"\"\"\n            token = ''\n            tokens = []\n            category = None\n            categories = ['0123456789',\n                          '\u0430\u0431\u0432\u0433\u0434\u0435\u0451\u0436\u0437\u0438\u0439\u043a\u043b\u043c\u043d\u043e\u043f\u0440\u0441\u0442\u0443\u0444\u0445\u0446\u0447\u0448\u0449\u044a\u044b\u044c\u044d\u044e\u044f',\n                          'abcdefghijklmnopqrstuvwxyz']\n            for char in string:\n                if token:\n                    if category and char.lower() in category:\n                         token += char\n                    else:\n                         if not token == ' ':\n                             tokens.append(token)\n                         token = char\n                         category = None\n                         for cat in categories:\n                             if char.lower() in cat:\n                                 category = cat\n                                 break\n                else:\n                     category = None\n                     if not category:\n                         for cat in categories:\n                             if char.lower() in cat:\n                                 category = cat\n                                 break\n                     token += char\n            if token:\n                 if not token == ' ':\n                     tokens.append(token)\n            return ' '.join(tokens)\n    \n        \n    def fix_text_case(self, text_etalon, text_current, fix_aggressive=False):\n            \"\"\" \u0421\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0435\u043c \u0434\u0432\u0430 \u0442\u0435\u043a\u0441\u0442\u0430 \u0438 \u043f\u044b\u0442\u0430\u0435\u043c\u0441\u044f \u0432\u043e\u0441\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c \u0440\u0435\u0433\u0438\u0441\u0442\u0440 ()\n                \u2022 aggressive - \u0435\u0441\u043b\u0438 False, \u0442\u043e \u043c\u0435\u043d\u044f\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0435\u0440\u0432\u044b\u0439 \u0441\u0438\u043c\u0432\u043e\u043b; True - \u0432\u0441\u0435 \u0441\u0438\u043c\u0432\u043e\u043b\u044b\n            \"\"\"\n            \n            if not fix_aggressive:\n                \n                text_etalon = text_etalon.split()\n                text_current = text_current.split()\n                if not len(text_etalon) == len(text_current):\n                    return ' '.join(text_current)\n    \n                i = 0\n                text_new = text_current\n                for t_bef,t_aft in zip(text_etalon,text_current):\n                    case_etalon = 'Upper' if t_bef[0].istitle() else 'Lower'\n                    case_current = 'Upper' if t_aft[0].istitle() else 'Lower'\n            \n                    if not case_etalon == case_current: \n                        if case_etalon == 'Upper' and case_current == 'Lower':\n                            text_new[i] = t_aft[0].upper() + t_aft[1:]\n                        elif case_etalon == 'Lower' and case_current == 'Upper':\n                            text_new[i] = t_aft[0].lower() + t_aft[1:]\n                    i += 1\n                return ' '.join(text_new)\n    \n            else:\n                \n                text_new = ''\n                if not len(text_etalon) == len(text_current):\n                    return text_current\n                \n                for t_bef,t_aft in zip(text_etalon,text_current):\n                    case_etalon = 'Upper' if t_bef.istitle() else 'Lower'\n                    case_current = 'Upper' if t_aft.istitle() else 'Lower'\n            \n                    if not case_etalon == case_current: \n                        if case_etalon == 'Upper' and case_current == 'Lower':\n                            text_new += t_aft.upper()\n                        elif case_etalon == 'Lower' and case_current == 'Upper':\n                            text_new += t_aft.lower()\n                    else:\n                        text_new += t_aft\n                return text_new\n            \n    def do_lemmatization(self, text, fix_case=True, fix_aggressive=False):\n            \"\"\" \u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0442\u043e\u0440 \u0434\u043b\u044f \u0440\u0443\u0441\u0441\u043a\u0438\u0445 \u0442\u0435\u043a\u0441\u0442\u043e\u0432 (pymorphy)\n            \"\"\"\n            new_sentence = [self.morph_ru.parse(word)[0].normal_form for word in text.split()]\n            if fix_case:\n                fix_sentence = []\n                old_sentence = text.split()\n                for old,new in zip(old_sentence,new_sentence):\n                    fix_sentence.append(self.fix_text_case(old, new, fix_aggressive))\n                return ' '.join(fix_sentence) \n            return ' '.join(new_sentence)\n    \n    \n    def fix_spell(self,text):\n        return spell(text.lower())\n        \n    \n    \n    def creat_stop_words(self, df, text_str, label_str, \n                         stop_words_excep, stop_words_extra, min_labels_count=None):\n            \"\"\" \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432 (\u0441\u043b\u043e\u0432\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0442\u0441\u044f \u0432 \u0437\u0430\u0434\u0430\u043d\u043d\u043e\u043c \u0447\u0438\u0441\u043b\u0435 \u043c\u0435\u0442\u043e\u043a)\n                \u2022 min_labels_count - \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b-\u0432\u043e \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0439,\n                                     \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0434\u043e\u043b\u0436\u043d\u043e \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c\u0441\u044f \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u043e\n            \"\"\"\n    \n            # \u0421\u043f\u0438\u0441\u043e\u043a \u043c\u0435\u0442\u043e\u043a\n            labels = list(set(df[label_str].to_list()))\n            \n            # \u041a\u043e\u043b-\u0432\u043e \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0439 \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0434\u043e\u043b\u0436\u043d\u043e \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c\u0441\u044f \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u043e\n            if min_labels_count is None:\n                min_labels_count = len(labels)\n    \n            #\u0421\u043f\u0438\u0441\u043e\u043a \u0441\u043b\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043c\u0435\u0442\u043a\u0438\n            labels_word = {}\n            for label in labels:\n                labels_word[label]= self.get_dict(df[df[label_str] == label], text_str)\n              \n            # \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432  \n            stop_words = []\n            uniq_words = self.get_dict(df, text_str)\n              \n            for word in uniq_words:\n                count = 0\n                for label in labels:\n                    if word in labels_word[label]:\n                        count += 1\n                if count >= min_labels_count and word not in stop_words_excep:\n                    stop_words.append(word)\n            \n            stop_words.extend(stop_words_extra)\n            return stop_words \n            \n    def delete_stopwords(self, text):\n            \"\"\" \u041e\u0447\u0438\u0441\u0442\u043a\u0430 \u043e\u0442 \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\n                \u2022 stops - \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u0442\u043e\u043f \u0441\u043b\u043e\u0432. \u0415\u0441\u043b\u0438 \u043f\u0443\u0441\u0442\u043e\u0439 \u0442\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0435\n            \"\"\"\n            if self.stop_words is None:\n                self.stop_words = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n            new_sentence = [word for word in text.split() if not word.lower() in self.stop_words]\n            return ' '.join(new_sentence)    \n        \n    def word_is_en(self, word):\n            return word.lower() in self.vocab_en\n    \n    def word_is_ru(self, word):\n            return self.morph_ru.word_is_known(word.lower(), strict_ee=False)\n    \n    def get_dict(self, df, column_str='', uniq=True):\n            \"\"\" \u0421\u043f\u0438\u0441\u043e\u043a \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 \u0432 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u043c \u0441\u0442\u043e\u043b\u0431\u0446\u0435\n            \"\"\"\n            if not isinstance(df,list):\n                rows = df[column_str].to_list()\n            else:\n                rows = df\n            dictionary = []\n            for row in rows:\n                dictionary.extend(row.split())\n            return list(set(dictionary)) if uniq else dictionary        \n\n\n\n    def preprocessing(self, do_clen, do_lower_case, replace_empty_label,\n                      do_tokenize, drop_dupl, do_lemm, del_stops,fix_spelling):\n    \n        # \u0417\u0430\u043c\u0435\u043d\u044f\u0435\u043c \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438\n        self.fill_na(csv_train, 'text', replace_to='')\n        self.fill_na(csv_test, 'text', replace_to='')\n        \n        # \u0423\u0434\u0430\u043b\u044f\u0435\u043c \u043b\u0438\u0448\u043d\u0438\u0435 \u0441\u0438\u043c\u0432\u043e\u043b\u044b\n        if do_clen:\n            print('\u0423\u0434\u0430\u043b\u044f\u0435\u043c \u043b\u0438\u0448\u043d\u0438\u0435 \u0441\u0438\u043c\u0432\u043e\u043b\u044b..')\n            csv_train['text'] = csv_train['text'].apply(self.clear_chars)\n            csv_test['text'] = csv_test['text'].apply(self.clear_chars)\n        \n        # \u041f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u0432 \u043d\u0438\u0436\u043d\u0438\u0439 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\n        if do_lower_case:\n            print('\u041f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u0432 \u043d\u0438\u0436\u043d\u0438\u0439 \u0440\u0435\u0433\u0438\u0441\u0442..')\n            csv_train['text'] = csv_train['text'].apply(self.do_lower)\n            csv_test['text'] = csv_test['text'].apply(self.do_lower)\n        \n        # \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u0443\u044e \u043c\u0435\u0442\u043a\u0443 \u0434\u043b\u044f \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 \n        # \u0438 \u043f\u0440\u0438\u0441\u0432\u0430\u0438\u0432\u0430\u0435\u043c \u0435\u0451 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 \u0432 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n        if replace_empty_label:\n            print('\u0417\u0430\u043c\u0435\u043d\u044f\u0435\u043c \u043f\u0443\u0441\u0442\u044b\u0435 \u043c\u0435\u0442\u043a\u0438..')\n            self.replace_empty_to_max_freq_label(csv_train, text_str='text', \n                                                 label_str='label', empty_str='')\n\n        # \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0442\u0435\u043a\u0441\u0442 (\u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c \u0447\u0438\u0441\u043b\u0430 \u043e\u0442 \u0442\u0435\u043a\u0441\u0442\u0430 \u0438 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0439 \u0442\u0435\u043a\u0441\u0442 \u043e\u0442 \u0440\u0443\u0441\u0441\u043a\u043e\u0433\u043e)\n        if do_tokenize:\n            print('\u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f..')\n            csv_train['text'] = csv_train['text'].apply(self.tokenize_by_rules)\n            csv_test['text'] = csv_test['text'].apply(self.tokenize_by_rules)\n            \n        # \u0418\u0441\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u043f\u0435\u0447\u0430\u0442\u043e\u043a\n        if fix_spelling:\n            csv_train['text'] = csv_train['text'].apply(self.fix_spell)\n            csv_test['text'] = csv_test['text'].apply(self.fix_spell)            \n        \n        # \u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f\n        if do_lemm:\n            print('\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f..')\n            self.morph_ru = pymorphy2.MorphAnalyzer() # \u042d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440 \u043a\u043b\u0430\u0441\u0441\u0430 pymorphy2\n            csv_train['text'] = csv_train['text'].apply(self.do_lemmatization)\n            csv_test['text'] = csv_test['text'].apply(self.do_lemmatization) \n            \n        # \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432 (\u0441\u043b\u043e\u0432\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0442\u0441\u044f \u0432 \u0437\u0430\u0434\u0430\u043d\u043d\u043e\u043c \u0447\u0438\u0441\u043b\u0435 \u043c\u0435\u0442\u043e\u043a)\n        if del_stops:\n            print('\u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432..')\n            self.stop_words = self.creat_stop_words(csv_train, 'text', 'label',\n                                                    self.stop_words_excep,\n                                                    self.stop_words_extra,\n                                                    min_labels_count=None)\n        \n            # \u0423\u0434\u0430\u043b\u044f\u0435\u043c \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430\n            print('\u0423\u0434\u0430\u043b\u044f\u0435\u043c \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430..')\n            csv_train['text'] = csv_train['text'].apply(self.delete_stopwords)\n            csv_test['text'] = csv_test['text'].apply(self.delete_stopwords)    \n            \n            \n        # \u0423\u0434\u0430\u043b\u044f\u0435\u043c \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b  \n        if drop_dupl:\n            print('\u0423\u0434\u0430\u043b\u044f\u0435\u043c \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b..')\n            csv_train.drop_duplicates(inplace=True)\n        \n        # \u0417\u0430\u043c\u0435\u043d\u044f\u0435\u043c \u043f\u0443\u0441\u0442\u044b\u0435 \u0441\u0442\u0440\u043e\u043a\u0438 \u043d\u0430 \u043f\u0440\u043e\u0431\u0435\u043b (\u0444\u0438\u043a\u0441 \u043e\u0448\u0438\u0431\u043a\u0438 \u043f\u0440\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438 BERT)\n        print('\u0417\u0430\u043c\u0435\u043d\u044f\u0435\u043c \u043f\u0443\u0441\u0442\u044b\u0435 \u0441\u0442\u0440\u043e\u043a\u0438 \u043d\u0430 \u043f\u0440\u043e\u0431\u0435\u043b..')\n        csv_train['text'] = csv_train['text'].replace('', ' ')\n        csv_test['text'] = csv_test['text'].replace('', ' ')   \n        \n        variant = 'clen'+str(do_clen)[0]+\\\n                  '_replaceempty'+str(replace_empty_label)[0]+\\\n                  '_tokenized'+str(do_tokenize)[0]+\\\n                  '_lower'+str(do_lower_case)[0]+\\\n                  '_stops'+str(del_stops)[0]+\\\n                  '_dropdupl'+str(drop_dupl)[0]+\\\n                  '_lemm'+str(do_lemm)[0]+\\\n                  '_fix_spelling'+str(fix_spelling)[0]\n            \n        return variant","a36af43e":"class EarlyStopping:\n\n    def __init__(self, path, patience=1, delta=0):\n        self.patience = patience\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.inf\n        self.delta = delta\n        self.path = path\n        \n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n            \n    def save_checkpoint(self, val_loss, model):\n        print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).')\n        save_pretrained_model(model,self.path)         \n        self.val_loss_min = val_loss","ca2d47e4":"def check_device():\n\n    # If there's a GPU available...\n    if torch.cuda.is_available():    \n        device = torch.device(\"cuda\")\n        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n        print('We will use the GPU:', torch.cuda.get_device_name(0))\n        device_type = 'gpu'\n    else:\n        print('No GPU available, using the CPU instead.')\n        device = torch.device(\"cpu\") \n        device_type = 'cpu'\n        \n    return device,device_type\n    \n    \ndef set_seed(seed_value=42):\n    \"\"\" \u0417\u0430\u0434\u0430\u0435\u043c seed \u0434\u043b\u044f \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u043e\u0432\n    \"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n    \n\n\ndef check_BPE_maxlen(sentences,print_str=''):\n    \"\"\" \u041d\u0430\u0445\u043e\u0434\u0438\u043c \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u0432 \u0441\u043f\u0438\u0441\u043a\u0435 \u0438\u0437 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439\n    \"\"\"\n    max_len = 0\n    for sent in sentences:\n        input_ids = tokenizer.encode(sent, add_special_tokens=True)\n        max_len = max(max_len, len(input_ids))\n    print('Max '+print_str+' length: ', max_len)\n    return max_len\n\n\ndef get_tokenized_tensors(sentences, labels=None, max_length=512, show_example=False):\n    \"\"\" \u0422\u043e\u043a\u0435\u043d\u0435\u0437\u0438\u0440\u0443\u0435\u043c \u0432\u0441\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0438 \u0441\u043e\u043f\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u043e\u043a\u0435\u043d\u044b \u0441 \u0438\u0445 id \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u0435.\n    \"\"\"\n    input_ids = []\n    attention_masks = []\n\n    for sent in sentences:\n        encoded_dict = tokenizer.encode_plus(\n                            sent,                      \n                            add_special_tokens = True, \n                            max_length = max_length,           \n                            pad_to_max_length = True,\n                            return_attention_mask = True,   \n                            return_tensors = 'pt',     \n        )\n        \n        # \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0438\u0437 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0435\u0439 \u0447\u0438\u0441\u0435\u043b, \n        # \u043e\u0442\u043e\u0436\u0434\u0435\u0441\u0442\u0432\u043b\u044f\u044e\u0449\u0438\u0445 \u043a\u0430\u0436\u0434\u044b\u0439 \u0442\u043e\u043a\u0435\u043d \u0441 \u0435\u0433\u043e \u043d\u043e\u043c\u0435\u0440\u043e\u043c \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u0435.    \n        input_ids.append(encoded_dict['input_ids'])\n        \n        # \u0421\u043f\u0438\u0441\u043e\u043a \u0438\u0437 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0435\u0439 \u043d\u0443\u043b\u0435\u0439 \u0438 \u0435\u0434\u0438\u043d\u0438\u0446, \n        # \u0433\u0434\u0435 \u0435\u0434\u0438\u043d\u0438\u0446\u044b \u043e\u0431\u043e\u0437\u043d\u0430\u0447\u0430\u044e\u0442 \u0442\u043e\u043a\u0435\u043d\u044b \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f, \u043d\u0443\u043b\u0438 - \u043f\u0430\u0434\u0434\u0438\u043d\u0433.\n        # \u041f\u0430\u0434\u0434\u0438\u043d\u0433 \u043d\u0443\u0436\u0435\u043d \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b BERT \u043c\u043e\u0433 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f\u043c\u0438 \u0440\u0430\u0437\u043d\u043e\u0439 \u0434\u043b\u0438\u043d\u044b.\n        attention_masks.append(encoded_dict['attention_mask'])\n    \n    # \u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0441\u043f\u0438\u0441\u043a\u0438 \u0432 pytorch \u0442\u0435\u043d\u0437\u043e\u0440\u044b\n    tensor_input_ids = torch.cat(input_ids, dim=0)\n    tensor_attention_masks = torch.cat(attention_masks, dim=0)\n    tensor_labels = torch.tensor(labels) if not labels is None else None\n        \n    \n    # \u0412\u044b\u0432\u043e\u0434\u0438\u043c \u043f\u0440\u0438\u043c\u0435\u0440 (\u043f\u043e \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u044e)\n    if show_example:\n        print('Original: ', sentences[0])\n        print('Token IDs:', input_ids[0])\n    \n    return tensor_input_ids,tensor_attention_masks,tensor_labels\n\n\n\ndef creat_dataloader(train_dataset, valid_dataset, batch_size=32):\n    \"\"\"  \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445\n    \"\"\"\n    train_dataloader = DataLoader(\n                train_dataset,  \n                sampler = RandomSampler(train_dataset), # SequentialSampler\n                batch_size = batch_size \n    )\n    \n    valid_dataloader = DataLoader(\n                valid_dataset, \n                sampler = SequentialSampler(valid_dataset), \n                batch_size = batch_size \n    )\n    \n    return train_dataloader, valid_dataloader\n\n\n\n\ndef get_skf_dataloader(sentences, labels, batch_size=32):\n    \"\"\" \u041f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u043c \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 (StratifiedKFold) \u043d\u0430 \u0433\u0443\u0440\u043f\u043f\u044b\n        \u0438 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\u044b \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0438\u0437 \u0441\u043e\u0437\u0434\u0430\u043d\u043d\u044b\u0445 \u0433\u0440\u0443\u043f\u043f.\n    \"\"\"\n    for train_index, val_index in skf.split(sentences,labels):\n        X_train = [sentences[i] for i in train_index]\n        Y_train = [labels[i] for i in train_index]\n        X_valid = [sentences[i] for i in val_index]\n        Y_valid = [labels[i] for i in val_index]\n        \n\n        # \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438 \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0432 pytorch \u0442\u0435\u043d\u0441\u043e\u0440\u044b: \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\n        train_input_ids,train_attention_masks,train_labels = get_tokenized_tensors(\n                                                         sentences = X_train,\n                                                         labels = Y_train,\n                                                         max_length = MAX_LEN,\n                                                         show_example = False) \n        \n        # \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438 \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0432 pytorch \u0442\u0435\u043d\u0441\u043e\u0440\u044b: \u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\n        valid_input_ids,valid_attention_masks,valid_labels = get_tokenized_tensors(\n                                                         sentences = X_valid,\n                                                         labels = Y_valid,\n                                                         max_length = MAX_LEN,\n                                                         show_example = False)\n        # \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\u044b \u0434\u0430\u043d\u043d\u044b\u0445\n        train_dataset = TensorDataset(train_input_ids, \n                                      train_attention_masks,\n                                      train_labels)\n        \n        valid_dataset = TensorDataset(valid_input_ids,\n                                      valid_attention_masks, \n                                      valid_labels)\n        \n        train_dataloader, valid_dataloader = creat_dataloader(train_dataset,\n                                                              valid_dataset,\n                                                              batch_size = batch_size)\n        \n        yield train_dataloader, valid_dataloader\n        \n\n\n\ndef load_pretrained_model(model_name):\n    \"\"\" \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0443\u044e BERT \u043c\u043e\u0434\u0435\u043b\u044c\n    \"\"\"\n    model = BertForSequenceClassification.from_pretrained(\n        model_name, \n        num_labels = NUM_LABELS)\n    \n    if device_type == 'gpu':\n        model.cuda()\n    return model\n\n\n\ndef prepare_optimizer(lr=2e-5, eps=1e-8, weight_decay=0.01):\n    \"\"\" \u041d\u0430\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\n    \"\"\"\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n         'weight_decay': weight_decay},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=eps)\n    \n    return optimizer\n\n\n\ndef prep_scheduler(num_warmup_steps=0):\n    \"\"\" \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0438 \u043d\u0430\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u0435\u043c \u043f\u043b\u0430\u043d\u0438\u0440\u043e\u0432\u0449\u0438\u043a \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n    \"\"\"\n    # \u041e\u0431\u0449\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0448\u0430\u0433\u043e\u0432 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n    total_steps = len(train_dataloader) * EPOCHS\n    \n    # \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043f\u043b\u0430\u043d\u0438\u0440\u043e\u0432\u0449\u0438\u043a \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n    scheduler = get_linear_schedule_with_warmup(optimizer, \n                                                num_warmup_steps = num_warmup_steps,\n                                                num_training_steps = total_steps)\n    return scheduler\n\n\ndef get_f1_score(preds, labels, average='micro'):\n    \"\"\" \u0420\u0430\u0441\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u043c\u0435\u0442\u0440\u0438\u043a\u0443 F1-score\n    \"\"\"\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(pred_flat, labels_flat, average=average)\n\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n\n\n\ndef train():\n    # \u041d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435:\n    # https:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py\n    \n    # \u0421\u043f\u0438\u0441\u043e\u043a \u0434\u043b\u044f \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n    training_stats = []\n    \n    # \u0417\u0430\u0441\u0435\u043a\u0430\u0435\u043c \u0432\u0440\u0435\u043c\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0432\u0441\u0435\u0445 \u044d\u043f\u043e\u0445\n    total_t0 = time.time()\n    \n    for epoch_i in range(0, EPOCHS):\n        \n        # ========================================\n        #               Training\n        # ========================================\n        print('\\n======= KFold {:} \/ {:} ======='.format(current_kfold, CROSS_VALID_FOLDS))\n        print('======= Epoch {:} \/ {:} ======='.format(epoch_i + 1, EPOCHS))\n        print('Training...')\n    \n        # \u0417\u0430\u0441\u0435\u043a\u0430\u0435\u043c \u0432\u0440\u0435\u043c\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u043e\u0434\u043d\u043e\u0439 \u044d\u043f\u043e\u0445\u0438\n        t0 = time.time()\n    \n        # \u041e\u0431\u043d\u0443\u043b\u044f\u0435\u043c \u043e\u0448\u0438\u0431\u043a\u0443 \u043d\u0430 \u043a\u0430\u0436\u0434\u043e\u0439 \u044d\u043f\u043e\u0445\u0435\n        total_train_loss = 0\n    \n        # \u041f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0432 \u0440\u0435\u0436\u0438\u043c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n        model.train()\n    \n        for step, batch in enumerate(train_dataloader):\n            # \u0412\u044b\u0432\u043e\u0434\u0438\u043c \u043b\u043e\u0433\u0438 \u043a\u0430\u0436\u0434\u044b\u0435 40 \u0431\u0430\u0442\u0447\u0435\u0439\n            if step % 40 == 0 and not step == 0:\n                elapsed = format_time(time.time() - t0) # \u0432\u0440\u0435\u043c\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f\n                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n    \n            # \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0431\u0430\u0442\u0447 \u0434\u043b\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u043d\u0430 GPU\n            batch = tuple(t.to(device) for t in batch)\n    \n            # \u0420\u0430\u0441\u043f\u0430\u043a\u043e\u0432\u044b\u0432\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 dataloader\n            b_input_ids, b_input_mask, b_labels = batch\n            \n            # \u041e\u0431\u043d\u0443\u043b\u044f\u0435\u043c \u0440\u0430\u043d\u0435\u0435 \u0440\u0430\u0441\u0447\u0438\u0442\u0430\u043d\u043d\u044b\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u044b\n            model.zero_grad()        \n    \n            # \u041f\u0440\u044f\u043c\u043e\u0439 \u043f\u0440\u043e\u0445\u043e\u0434. \u0420\u0430\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u043b\u043e\u0433\u0438\u0442\u044b\n            loss, logits = model(b_input_ids, \n                                 token_type_ids=None, \n                                 attention_mask=b_input_mask, \n                                 labels=b_labels)\n    \n            # \u0421\u0443\u043c\u043c\u0438\u0440\u0443\u0435\u043c \u043e\u0448\u0438\u0431\u043a\u0443 \u043f\u043e \u0432\u0441\u0435\u043c \u0431\u0430\u0442\u0447\u0430\u043c\n            total_train_loss += loss.item()\n    \n            # \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043e\u0431\u0440\u0430\u0442\u043d\u044b\u0439 \u043f\u0440\u043e\u0445\u043e\u0434, \u0447\u0442\u043e\u0431\u044b \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u044b.\n            loss.backward()\n    \n            # \u0411\u043e\u0440\u0435\u043c\u0441\u044f \u0441\u043e \"\u0432\u0437\u0440\u044b\u0432\u0430\u044e\u0449\u0438\u043c\u0438\u0441\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430\u043c\u0438\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    \n            # \u041e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0441 \u0443\u0447\u0435\u0442\u043e\u043c \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u043d\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430\n            optimizer.step()\n    \n            # \u041e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c learning rate.\n            scheduler.step()\n    \n        # \u0421\u0435\u0434\u043d\u044f\u044f \u043e\u0448\u0438\u0431\u043a\u0430 \u043d\u0430 \u043e\u0434\u043d\u043e\u0439 \u044d\u043f\u043e\u0445\u0435\n        avg_train_loss = total_train_loss \/ len(train_dataloader)            \n        \n        # \u0412\u0440\u0435\u043c\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043e\u0434\u043d\u043e\u0439 \u044d\u043f\u043e\u0445\u0438\n        training_time = format_time(time.time() - t0)\n    \n        print(\"\")\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"  Training epo\u0441h took: {:}\".format(training_time))\n        \n        \n        \n        # ========================================\n        #               Validation\n        # ========================================\n        print(\"\\nRunning Validation...\")\n    \n        t0 = time.time()\n    \n        # \u0412\u044b\u0432\u043e\u0434\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0438\u0437 \u0440\u0435\u0436\u0438\u043c\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n        model.eval()\n    \n        # \u041e\u0442\u0441\u043b\u0435\u0436\u0438\u0432\u0430\u0435\u043c\u044b\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \n        total_eval_f1 = 0\n        total_eval_loss = 0\n    \n        \n        for batch in valid_dataloader:\n\n            batch = tuple(t.to(device) for t in batch)\n            b_input_ids, b_input_mask, b_labels = batch\n            \n            # \u0423\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0435 \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0442\u044c \u0438 \u043d\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u044b\n            with torch.no_grad():        \n    \n                # \u041f\u0440\u044f\u043c\u043e\u0439 \u043f\u0440\u043e\u0445\u043e\u0434. \u0420\u0430\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u043b\u043e\u0433\u0438\u0442\u044b\n                (loss, logits) = model(b_input_ids, \n                                       token_type_ids=None, \n                                       attention_mask=b_input_mask,\n                                       labels=b_labels)\n                \n            # \u0421\u0443\u043c\u043c\u0438\u0440\u0443\u0435\u043c \u043e\u0448\u0438\u0431\u043a\u0443 \u043f\u043e \u0432\u0441\u0435\u043c \u0431\u0430\u0442\u0447\u0430\u043c\n            total_eval_loss += loss.item()\n    \n            # \u041f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u043b\u043e\u0433\u0438\u0442\u044b \u0438 \u043c\u0435\u0442\u043a\u0438 \u043d\u0430 CPU\n            logits = logits.detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n    \n            # \u0420\u0430\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u043c\u0435\u0442\u0440\u0438\u043a\u0443 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0438 \u0441\u0443\u043c\u043c\u0438\u0440\u0443\u0435\u043c \u0435\u0451 \u043f\u043e \u0432\u0441\u0435\u043c \u0431\u0430\u0442\u0447\u0430\u043c\n            total_eval_f1 += get_f1_score(logits,label_ids)\n            \n            \n    \n        \n        # \u041c\u0435\u0442\u0440\u0438\u043a\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043d\u0430 \u043e\u0434\u043d\u043e\u0439 \u044d\u043f\u043e\u0445\u0435\n        avg_val_f1 = total_eval_f1 \/ len(valid_dataloader)\n        print(\"  F1-score: {0:.2f}\".format(avg_val_f1))\n    \n        # \u041e\u0448\u0438\u0431\u043a\u0430 \u043d\u0430 \u043e\u0434\u043d\u043e\u0439 \u044d\u043f\u043e\u0445\u0435\n        avg_val_loss = total_eval_loss \/ len(valid_dataloader)\n        \n        # \u0412\u0440\u0435\u043c\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u043e\u0434\u043d\u043e\u0439 \u044d\u043f\u043e\u0445\u0438\n        validation_time = format_time(time.time() - t0)\n        \n        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n        print(\"  Validation took: {:}\".format(validation_time))\n        \n        # \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0443\n        training_stats.append(\n            {\n                'epoch': epoch_i + 1,\n                'Training Loss': avg_train_loss,\n                'Valid. Loss': avg_val_loss,\n                'Valid. F1-score.': avg_val_f1,\n                'Training Time': training_time,\n                'Validation Time': validation_time\n            }\n        )\n        \n        # \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043d\u0430 \u0440\u0430\u043d\u043d\u044e\u044e \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0443. \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043b\u0443\u0447\u0448\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0444\u043e\u043b\u0434\u0430\n        early_stopping(avg_val_loss, model)\n        if early_stopping.early_stop:\n            break\n    \n    print(\"\\nTraining complete!\")\n    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n    return training_stats\n\n\n\ndef predict_on_test_set(model,prediction_dataloader):\n    \"\"\" \u0414\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n    \"\"\"\n    \n    # \u0412\u044b\u0432\u043e\u0434\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0438\u0437 \u0440\u0435\u0436\u0438\u043c\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n    model.eval()\n    \n    # \u041e\u0442\u0441\u043b\u0435\u0436\u0438\u0432\u0430\u0435\u043c\u044b\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435  \n    predictions,predictions_flat = [],[]\n    \n\n    for n,batch in enumerate(prediction_dataloader):\n      print('\\r[%d]'%(n), end=\"\",flush=True)\n\n      batch = tuple(t.to(device) for t in batch)\n      b_input_ids, b_input_mask = batch\n      \n      # \u0423\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0435 \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0442\u044c \u0438 \u043d\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u044b\n      with torch.no_grad():\n          # \u041f\u0440\u044f\u043c\u043e\u0439 \u043f\u0440\u043e\u0445\u043e\u0434. \u0420\u0430\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u043b\u043e\u0433\u0438\u0442\u044b\n          outputs = model(b_input_ids, token_type_ids=None, \n                          attention_mask=b_input_mask)\n    \n      logits = outputs[0]\n    \n      # \u041f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u043b\u043e\u0433\u0438\u0442\u044b \u043d\u0430 CPU\n      logits = logits.detach().cpu().numpy()\n    \n      # \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043c\u0435\u0442\u043a\u0438\n      predictions_flat = np.argmax(logits, axis=1).flatten()\n    \n      # \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u0443\u044e \u043c\u0435\u0442\u043a\u0443\n      predictions.extend(predictions_flat)\n    \n    print('    DONE.')\n    return predictions\n\n\ndef save_pretrained_model(model,output_dir):\n    \"\"\" \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c\n    \"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    print(\"Saving model to %s\" % output_dir)\n    model_to_save = model.module if hasattr(model, 'module') else model\n    model_to_save.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n\ndef save_result(output_file):\n    \"\"\" \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438\n    \"\"\"\n    def get_test_labels_id(x):\n        for index,row in df_labels.iterrows():\n            if x == row['label_id']:\n                return df_labels.loc[index, 'label']\n\n    # \u0421\u043b\u043e\u0432\u0430\u0440\u044c \u043c\u0435\u0442\u043e\u043a \u0438 \u0438\u0445 id\n    df_labels = csv_train[['label', 'label_id']].drop_duplicates()\n\n    # \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u0432\u044b\u0445\u043e\u0434\u043d\u043e\u0439 DataFrame        \n    result = pd.DataFrame()   \n    result['id'] = csv_test['id']\n    result['label'] = predictions\n    result['label'] = result['label'].apply(get_test_labels_id)\n\n    # \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c\n    DP.save_csv(result, output_file)\n\n    print('Result csv-file was saved to:' + output_file)","e4049df6":"# \u0423\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u043e\ndevice, device_type = check_device()\n\n# \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b\nSEED = 42\nBERT_PRETRAINED_MODEL = \"DeepPavlov\/rubert-base-cased-conversational\"\nCROSS_VALID_FOLDS = 5\nBATCH_SIZE = 32\nBATCH_SIZE_TEST = 32  \nEPOCHS = 3\nLR = 2e-5 \nEPS = 1e-8\nWEIGHT_DECAY = 0.01\nNUM_WARMUP_STEPS = '10%' # n, 'n%' \n\n# \u0417\u0430\u0434\u0430\u0435\u043c seed \u0434\u043b\u044f \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\nset_seed(SEED)","692e1383":"# \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440 \u043a\u043b\u0430\u0441\u0441\u0430 \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445\nDP = DataPreparation()\n\n\nspell = Speller(lang='ru')\n\n\n# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\ncsv_train = DP.load_csv(\"\/kaggle\/input\/ocrv-intent-classification\/train.csv\")\ncsv_train = csv_train[['text', 'label']]\ncsv_test = DP.load_csv(\"\/kaggle\/input\/ocrv-intent-classification\/test.csv\")\ncsv_test = csv_test[['id', 'text']] \n\n\n# \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\ndata_prep_settings = DP.preprocessing(\n                                    do_clen = False,\n                                    do_lower_case = False,\n                                    replace_empty_label = True,\n                                    do_tokenize = False,\n                                    drop_dupl = False,\n                                    do_lemm = False,\n                                    del_stops = False,\n                                    fix_spelling = True\n                                    )","a8c596d5":"# \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439\nsentences = csv_train['text'].values\n\n# \u0418\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u043c\u0435\u0442\u043e\u043a (\u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u043c\u0435\u0442\u043a\u0438 \u0432 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438)\ncsv_train['label_id'] = pd.factorize(csv_train['label'])[0]\nlabels = csv_train['label_id'].values\nNUM_LABELS = len(set(labels))","09872bcc":"# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c BERT \u0442\u043e\u043a\u0435\u043d\u0430\u0439\u0437\u0435\u0440\ntokenizer = BertTokenizer.from_pretrained(BERT_PRETRAINED_MODEL,\n                                          do_lower_case = False)\n\n# \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u0432 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0438\nMAX_LEN_train = check_BPE_maxlen(sentences, print_str='train')\nMAX_LEN_test = check_BPE_maxlen(csv_test['text'].values, print_str='test')\nMAX_LEN = min(512,max(MAX_LEN_train, MAX_LEN_test))\nprint('MAX_LEN =', MAX_LEN)","6caa33bc":"# \u041a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f\nskf = StratifiedKFold(n_splits=CROSS_VALID_FOLDS,\n                      shuffle=True,\n                      random_state=SEED)\n\n# \u0413\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0443\u0447\u0435\u0442\u043e\u043c \u0440\u0430\u0437\u0431\u0438\u0432\u043a\u0438 \u043d\u0430 \u0433\u0440\u0443\u043f\u043f\u044b\nskf_dataloader = get_skf_dataloader(sentences,\n                                    labels,\n                                    batch_size=BATCH_SIZE)","b144ea33":"# \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u0441\u0442\u0440\u043e\u043a\u0443 \u0441 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u043c \u043c\u043e\u0434\u0435\u043b\u0438\ntraining_settings = 'bs'+str(BATCH_SIZE)+\\\n                    '_ep'+str(EPOCHS)+\\\n                    '_lr'+str(LR)+\\\n                    '_eps'+str(EPS)+\\\n                    '_wd'+str(WEIGHT_DECAY)+\\\n                    '_nws'+str(NUM_WARMUP_STEPS)\n\nmodel_variant = data_prep_settings + '__' + training_settings\n\n\n\n\n\ncurrent_kfold = 0\ntraining_stats = []\nfor train_dataloader, valid_dataloader in skf_dataloader:\n    current_kfold += 1\n    \n    # \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0443\u044e BERT \u043c\u043e\u0434\u0435\u043b\u044c\n    model = load_pretrained_model(BERT_PRETRAINED_MODEL)\n    \n    # \u041d\u0430\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\n    optimizer = prepare_optimizer(lr=LR, eps=EPS, weight_decay=WEIGHT_DECAY)\n\n    # \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043f\u043b\u0430\u043d\u0438\u0440\u043e\u0432\u0449\u0438\u043a \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n    if '%' in str(NUM_WARMUP_STEPS):\n        NUM_WARMUP_STEPS = int(EPOCHS * len(train_dataloader) * float(NUM_WARMUP_STEPS.split('%')[0])\/100)\n    scheduler = prep_scheduler(num_warmup_steps = NUM_WARMUP_STEPS)\n\n    # \u041a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u0440\u0430\u043d\u043d\u0435\u0439 \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n    early_stopping = EarlyStopping(path = '\/kaggle\/working\/models\/'+model_variant+'_kfold'+str(current_kfold),\n                                   patience = 1, \n                                   delta = 0)\n    \n    # \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\n    train_stats = train()\n    \n    # \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u043c \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u043a\u0438\u0443 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n    training_stats.append(train_stats)","6b9202d0":"# DataFrame \u0441\u043e \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u043e\u0439 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\ndf_stats_total = []\npd.set_option('precision', 2)\nfor stats in training_stats:\n    df_stats = pd.DataFrame(data=stats)\n    df_stats = df_stats.set_index('epoch')\n    df_stats_total.append(df_stats)\n\n# \u0412\u0430\u0440\u0438\u0430\u043d\u0442 \u043c\u043e\u0434\u0435\u043b\u0438\nprint('Model variant:',model_variant)\n\n# \u0421\u0440\u0435\u0434\u043d\u044f\u044f \u043c\u0435\u0442\u0440\u043a\u0438\u0430 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e \u0432\u0441\u0435\u043c \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\nf1_cum = 0\nfor df_stats in enumerate(df_stats_total):\n    f1_cum += float(df_stats[1].iloc[-1,2])\nprint('Avg. valid. F1-score:',f1_cum\/len(df_stats_total))\n\n# \u0421\u0440\u0435\u0434\u043d\u044f\u044f \u043e\u0448\u0438\u0431\u043a\u0430 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e \u0432\u0441\u0435\u043c \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\nloss_cum = 0\nfor df_stats in enumerate(df_stats_total):\n    loss_cum += float(df_stats[1].iloc[-1,1])\nprint('Avg. valid. loss:',loss_cum\/len(df_stats_total))","8297ca43":"sentences_test = csv_test['text'].values\n\n# \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438 \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0432 pytorch \u0442\u0435\u043d\u0441\u043e\u0440\u044b: \u0422\u0435\u0441\u0442\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\ninput_ids_test,attention_masks_test,_ = get_tokenized_tensors(sentences_test,\n                                                              max_length=MAX_LEN)\n\n# \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0437\u0430\u0433\u0440\u0443\u0437\u0447\u0438\u043a \u0434\u0430\u043d\u043d\u044b\u0445\nprediction_data = TensorDataset(input_ids_test, attention_masks_test)\nprediction_dataloader = DataLoader(prediction_data, sampler=SequentialSampler(prediction_data), batch_size=BATCH_SIZE_TEST)\n\n\nfor k in range(1,CROSS_VALID_FOLDS+1):\n    # \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u043d\u0443\u044e BERT \u043c\u043e\u0434\u0435\u043b\u044c (\u043a\u0430\u0436\u0434\u044b\u0439 \u0432\u0430\u0440\u0438\u0430\u043d\u0442 \u0440\u0430\u0437\u0431\u0438\u0432\u043a\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430)\n    model_path = model_variant + '_kfold' + str(k)\n    model = load_pretrained_model('\/kaggle\/working\/models\/'+model_path)\n\n    # \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\n    predictions = predict_on_test_set(model,prediction_dataloader)\n    save_result('\/kaggle\/working\/result_' + model_path + '.csv')\n\n\n    \n  ","b09791fc":"# # \u041d\u0430\u0445\u043e\u0434\u0438\u043c 2 \u043c\u043e\u0434\u0435\u043b\u0438: \u0441 \u043d\u0430\u0438\u043c\u0435\u043d\u044c\u0448\u0435\u0439 \u043e\u0448\u0438\u0431\u043a\u043e\u0439 \u0438 \u043d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0435\u0439 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c\u044e\n# best_kfold_loss_index = 0\n# best_kfold_score_index  = 0\n# loss_min = np.inf\n# score_max = 0\n\n# for i,stats in enumerate(df_stats_total):\n#     loss = stats.iloc[-1,1]\n#     if loss < loss_min:\n#         loss_min = loss\n#         best_kfold_loss_index = i\n        \n#     score = stats.iloc[-1,2]\n#     if score > score_max:\n#         score_max = score\n#         best_kfold_score_index = i\n\n# best_model_by_loss = model_variant + '_kfold' + str(best_kfold_loss_index)\n# best_model_by_score = model_variant + '_kfold' + str(best_kfold_score_index)\n# if best_model_by_loss == best_model_by_score:\n#     print('Best model:', best_model_by_loss)\n# else:\n#     print('Best model by loss:', best_model_by_loss)\n#     print('Best model by score:', best_model_by_score)\n\n\n\n# if best_model_by_loss == best_model_by_score:\n#     # \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043b\u0443\u0447\u0448\u0438\u044e \u0438\u0437 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u043d\u044b\u0445 BERT \u043c\u043e\u0434\u0435\u043b\u0435\u0439\n#     model = load_pretrained_model('\/kaggle\/working\/models\/'+best_model_by_loss)\n\n#     # \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\n#     predictions = predict_on_test_set(model,prediction_dataloader)\n#     save_result('\/kaggle\/working\/result_' + best_model_by_loss + '.csv')\n# else:\n\n#     # \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0441 \u043d\u0430\u0438\u043c\u0435\u043d\u044c\u0448\u0435\u0439 \u043e\u0448\u0438\u0431\u043a\u043e\u0439. \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\n#     model = load_pretrained_model('\/kaggle\/working\/models\/'+best_model_by_loss)\n#     predictions = predict_on_test_set(model,prediction_dataloader)\n#     save_result('\/kaggle\/working\/result_bestloss_' + best_model_by_loss + '.csv')\n    \n#     # \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0441 \u043d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0435\u0439 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c. \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\n#     model = load_pretrained_model('\/kaggle\/working\/models\/'+best_model_by_score)\n#     predictions = predict_on_test_set(model,prediction_dataloader)\n#     save_result('\/kaggle\/working\/result_bestscore_' + best_model_by_score + '.csv')\n\n","e7ba01e7":"<h4>\u0417\u0430\u0434\u0430\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b:<\/h4>","54934fec":"<h4>\u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445.<\/h4> \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0434\u0435\u043b\u0430\u044e\u0442\u0441\u044f \u0432\u0441\u0435\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u043c\u0438 \u0432 \u0445\u043e\u0434\u0435 \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438","e2b71c1c":"<h4>\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445<\/h4>\n\u0418\u043c\u0435\u0435\u0442\u0441\u044f \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c:\n<ul>\n<li>\u0423\u0434\u0430\u043b\u0438\u0442\u044c \u043b\u0438\u0448\u043d\u0438\u0435 \u0441\u0438\u043c\u0432\u043e\u043b\u044b, \u0432\u043a\u043b\u044e\u0447\u0430\u044f \u043b\u0438\u0448\u043d\u0438\u0435 \u043f\u0440\u043e\u0431\u0435\u043b\u044b \u0438 \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u044b \u0441\u0442\u0440\u043e\u043a;<\/li>\n<li>\u041f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u0442\u0435\u043a\u0441\u0442 \u043a \u043d\u0438\u0436\u043d\u0435\u043c\u0443 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0443;<\/li>\n<li>\u0423\u0434\u0430\u043b\u0438\u0442\u044c \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430;<\/li>\n<li>\u0418\u0441\u043f\u0440\u0430\u0432\u0438\u0442\u044c \u043e\u0448\u0438\u0431\u043e\u0447\u043d\u0443\u044e \u0440\u0430\u0441\u043a\u043b\u0430\u0434\u043a\u0443;<\/li>\n<li>\u0418\u0441\u043f\u0440\u0430\u0432\u0438\u0442\u044c \u043e\u043f\u0435\u0447\u0430\u0442\u043a\u0438;<\/li>\n<li>\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430;<\/li>\n<li>\u0423\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u0432 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445;<\/li>\n<li>\u0417\u0430\u043c\u0435\u043d\u0430 \u043c\u0435\u0442\u043a\u0438 \u0443 \u043f\u0443\u0441\u0442\u044b\u0445 \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432 \u043d\u0430 \u0441\u0430\u043c\u0443\u044e \u0447\u0430\u0441\u0442\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0449\u0443\u044e\u0441\u044f \u043c\u0435\u0442\u043a\u0443 \u0443 \u0442\u0430\u043a\u0438\u0445 \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432.<\/li>\n<\/ul>    ","1640ed72":"<h4>\u041e\u0431\u044a\u044f\u0432\u043b\u044f\u0435\u043c \u0432\u0441\u0435 \u0442\u0440\u0435\u0431\u0443\u0435\u043c\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438:<\/h4>","38d05841":"<h4>\u041f\u043e\u0434\u043a\u043b\u044e\u0447\u044f\u0435\u043c \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438:<\/h4>","36b3faea":"<h4>\u0423\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438.<\/h4>\nPymorphy2 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u044c \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u0434\u0430\u043d\u043d\u044b\u0445, \u043d\u043e \u0432 \u043a\u043e\u043d\u0435\u0447\u043d\u043e\u043c \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0435 \u043e\u043d \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f.","f6b75854":"<h4>\u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445:<\/h4>","6a323b92":"<h4>\u041f\u043e\u0434\u0433\u043e\u0442\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043a \u043f\u043e\u0434\u0430\u0447\u0435 \u0432 BERT:<\/h4>","34133809":"<h4>\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u043e\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043b\u0435\u043d\u0438\u044f \u0440\u0430\u043d\u043d\u0435\u0439 \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f.<\/h4","2d6f4441":"<h4>\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c:<\/h4>","4086f5c1":"<h4>\u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0443\u0441\u0440\u0435\u0434\u043d\u0451\u043d\u043d\u0443\u044e \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0438 \u043e\u0448\u0438\u0431\u043a\u0443 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e \u0432\u0441\u0435 \u0441\u043e\u0437\u0434\u0430\u043d\u043d\u044b\u043c \u0433\u0440\u0443\u043f\u043f\u0430\u043c:<\/h4>","ae1060e7":"<h4>\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0443\u0447\u0451\u0442\u043e\u043c \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438:<\/h4>"}}