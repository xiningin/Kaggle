{"cell_type":{"bf972941":"code","04983602":"code","7c36110d":"code","ad214e24":"code","120166ae":"code","447de646":"code","698cf654":"code","33cd7b3e":"code","29c2cdae":"code","a9555e91":"code","c36c5009":"code","49f49156":"code","44aef385":"code","fd227565":"code","2613fef1":"code","b1316dc6":"code","fd68f2c8":"code","5c511dd2":"code","a9581f3f":"code","a9800c05":"code","0609fd4d":"code","a578ee98":"code","5695c1c6":"code","9c2e78e4":"code","240e3df5":"code","dadd3b0e":"code","6c86fcbb":"code","eced2165":"code","bfd1923f":"code","753e75e0":"code","84461c5b":"code","38982892":"code","599a9d0d":"code","460e4a66":"code","748a2ffb":"code","190a7329":"code","3c081aeb":"code","35828606":"code","e68474a1":"code","26321b30":"code","3b4601df":"code","62b54c86":"code","8c1dabca":"code","3a7e7117":"code","305ed6de":"markdown","6dcf6b49":"markdown","c943c369":"markdown","e16803d3":"markdown","7edab4e1":"markdown","655ddfcc":"markdown","226a70e1":"markdown","931978c3":"markdown","b11327a8":"markdown","91f53108":"markdown","80f8220b":"markdown","6f6e8094":"markdown","9859148d":"markdown","4e89018d":"markdown","9ea06bd9":"markdown","2d7bedea":"markdown","22d76ff5":"markdown","4ed432e7":"markdown","2a5eab00":"markdown","1cf53714":"markdown","4b1a9eda":"markdown","15d7910f":"markdown","39d94d59":"markdown","4908236a":"markdown","65527eb0":"markdown","19777ff3":"markdown","0a007c59":"markdown","a608a80e":"markdown","34bfb11a":"markdown","2b105e54":"markdown","5a2307cf":"markdown","e1e4ca20":"markdown","c6b1733c":"markdown","e030a1e5":"markdown","af2973bb":"markdown","95fbe2ca":"markdown","fea0f4ce":"markdown","a0a35a74":"markdown","e47537c9":"markdown","e3e37eda":"markdown","231be2bf":"markdown","14375b30":"markdown","78a08b95":"markdown","5248f2d5":"markdown","4e31aadd":"markdown","0d19c487":"markdown","d4cd4a92":"markdown","4298529c":"markdown","f7b4e337":"markdown","094ba942":"markdown","cd878b6a":"markdown","d66cb37f":"markdown","3b72ab9d":"markdown"},"source":{"bf972941":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","04983602":"df = pd.read_csv('..\/input\/paysim1\/PS_20174392719_1491204439457_log.csv')","7c36110d":"df.shape","ad214e24":"df","120166ae":"# Correcting inconsistency in column name\ndf = df.rename(columns={'oldbalanceOrg':'oldbalanceOrig'})","447de646":"df.isnull().values.any()","698cf654":"print('The proportion of total transactions labeled as fraudulent is',df['isFraud'].value_counts()[1]\/df['isFraud'].size)","33cd7b3e":"df['isFraud'].value_counts().plot(kind='bar')\ndf['isFraud'].value_counts()","29c2cdae":"df['step'].hist()","a9555e91":"df['amount'].hist(bins=30)","c36c5009":"print('The average transaction amount is',df['amount'].mean())","49f49156":"df['type'].value_counts().plot(kind='barh')","44aef385":"df[['oldbalanceOrig','newbalanceOrig','oldbalanceDest','newbalanceDest']].hist(bins=30)","fd227565":"print('Average balance values:')\n\nwith pd.option_context('display.float_format','{:.2f}'.format):\n    print(df[['oldbalanceOrig','newbalanceOrig','oldbalanceDest','newbalanceDest']].mean())","2613fef1":"import matplotlib.pyplot as plt\nimport seaborn as sns","b1316dc6":"corr = df.corr()\nsns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values)","fd68f2c8":"fig, ax = plt.subplots(1,2)\n\nax[0].set_xlim([-10,100])\n\nsns.scatterplot(data=df, x='oldbalanceOrig', y='amount', ax=ax[0])\nsns.scatterplot(data=df, x='newbalanceOrig', y='amount', ax=ax[1])\n\nfig.show()","5c511dd2":"# Mode of amount values\ndf['amount'].mode()[0]","a9581f3f":"# What are the types of transactions for those with amounts greater than 10,000,000?\ndf[df['amount']>10000000]['type'].value_counts()","a9800c05":"# Are any of these transactions fraudulent?\ndf[df['amount']>10000000]['isFraud'].value_counts()","0609fd4d":"df[df['amount']>10000000].head()","a578ee98":"fig, ax = plt.subplots(1,2)\n\nsns.scatterplot(data=df, x='oldbalanceDest', y='amount', ax=ax[0])\nsns.scatterplot(data=df, x='newbalanceDest', y='amount', ax=ax[1])\n\nfig.show()","5695c1c6":"df['merchant'] = df['nameDest'].str.contains('M')\n\ndf.head(10)","9c2e78e4":"df[['isFraud','merchant']].value_counts()","240e3df5":"df[df['isFraud']==1].head(10)","dadd3b0e":"# Counts of each transaction type for fraudulent transactions\ndf[df['isFraud']==1]['type'].value_counts()","6c86fcbb":"df['balancediffOrig'] = df['newbalanceOrig'] - df['oldbalanceOrig']\ndf['balancediffDest'] = df['newbalanceDest'] - df['oldbalanceDest']\n\ndf.head(10)","eced2165":"sns.boxplot(x=\"isFraud\", y=\"balancediffOrig\", data=df,showfliers=False)","bfd1923f":"sns.boxplot(x=\"isFraud\", y=\"balancediffDest\", data=df, showfliers=False)","753e75e0":"features = ['step',\n            'type',\n            'amount',\n            'oldbalanceOrig',\n            'newbalanceOrig',\n            'oldbalanceDest',\n            'newbalanceDest',\n            'balancediffOrig',\n            'balancediffDest',\n            'merchant']\n\nlabel = ['isFraud']","84461c5b":"X = df[features]\ny = df[label]","38982892":"# Before encoding\nX.head()","599a9d0d":"# After encoding (scroll right to see new columns)\nX = X.join(pd.get_dummies(X[['type']], prefix='type')).drop(['type'], axis=1)\nX.head()","460e4a66":"from sklearn.model_selection import train_test_split ","748a2ffb":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","190a7329":"from sklearn.tree import DecisionTreeClassifier","3c081aeb":"dt_clf = DecisionTreeClassifier(random_state=1)\n\ndt_clf = dt_clf.fit(X_train,y_train)\n\ny_pred = dt_clf.predict(X_test)","35828606":"result = pd.DataFrame({'actual':y_test['isFraud'], 'predicted':y_pred})\nresult[result['actual']==1]","e68474a1":"from sklearn import metrics ","26321b30":"print(metrics.classification_report(y_test, y_pred))\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\nprint('AUC:', metrics.auc(fpr, tpr))","3b4601df":"# Confusion matrix follows the form:\n# [True Negative   |   False Positive]\n# [False Negative  |   True Positive ]\n\nmetrics.plot_confusion_matrix(dt_clf, X_test, y_test)","62b54c86":"from sklearn.tree import plot_tree\nfig = plt.figure(figsize=(17,10))\n_ = plot_tree(dt_clf, max_depth=2, feature_names=list(X_train.columns), filled=True, fontsize=12)","8c1dabca":"fi = pd.DataFrame({'features':X_train.columns,'importance':dt_clf.feature_importances_}).sort_values(by=['importance'], ascending=False)\nfi","3a7e7117":"sns.barplot(x=\"importance\", y=\"features\", data=fi)","305ed6de":"We will first split the data into training and test sets, with a 70\/30 split. This is done to measure and avoid possible overfitting, an issue in which the model is too specific to the dataset it trains on, and is not as generalizable to out-of-sample data. By measuring the model's performance on separate data, we have a more fair assessment of the model's ability to extrapolate its predictions to new or unseen information.","6dcf6b49":"## Features","c943c369":"As is often the case with fraud data like this, the dataset is highly imbalanced. Over 99.8% of the transaction records are non-fraudulent. Because only a tiny fraction of the dataset represents fraud, fraudulent transactions are likely to be under-represented in our model. If unaddressed, data imbalance can cause issues, such as misleading accuracy metrics in the model. The model may attempt to label every transaction as non-fraudulent, and since any randomly given transaction is highly likely to be non-fraudulent, a high accuracy will be reported, despite incorrectly classifying all fraudulent transactions as non-fraudulent. We should take care to compare model performance metrics considering fraudulent and non-fraudulent data independently.","e16803d3":"We have established a basic process for exploratory data analysis and modeling that has resulted in an acceptable predictor for our transaction data. While I will end our work here, there are other modeling techniques that we could try, such as SVM and Logistic Regression. There are also additional techniques to increase model performance, as well as better ways to measure performance, such as hyper-parameter tuning and cross-fold validation. It would be prudent to use these methods if we were tasked with informing business decisions that could affect large amounts of revenue or costs.","7edab4e1":"Now that the model has been fit, we can create predictions for the test set and see how they compare to the actual data. A sample of the predictions can be seen below.","655ddfcc":"As mentioned above, in dealing with imbalanced data like this, overall accuracy is not a particularly useful metric. However, we can see the metrics for the `isFraud==1` class by itself. A recall of 0.87 on this class essentially tells us that we correctly predicted 87% of the truly fraudulent data.","226a70e1":"### oldbalanceOrg, newbalanceOrig, oldbalanceDest, newbalanceDest","931978c3":"Let's look at a confusion matrix. A confusion matrix shows us the distribution of correct predictions according to the class label. Here we can see the actual counts per predicted and actual class.","b11327a8":"As stated earlier, merchant account numbers are denoted by the 'M' prefix. We will capture this information in a boolean variable that the model can use as a feature.","91f53108":"## Feature Correlations","80f8220b":"## Modeling","6f6e8094":"Below, we can see in these pairwise plots between `amount` and `oldbalanceDest` \/ `newbalanceDest` that they exhibit more evenly random distributions. There appears to be a very weak positive relationship between amounts and destination account balances, which we observed earlier in the correlation matrix.","9859148d":"As described in the dataset documentation, these transactions typically involve the fraud agent pulling large amounts out of the targeted accounts. Let's see if we can demonstrate some of these characteristics by creating and observing columns that show the change in account balances due to a transaction. We will use the convention of subtracting old balance from new, so that the difference value is positive if the account balance is higher after the transaction, and vice versa.","4e89018d":"## Feature Engineering","9ea06bd9":"We have no nulls in the table. This will save us the headache of having to fill in or impute missing values, or otherwise exclude incomplete data rows, if required by the model.","2d7bedea":"Let's look at a sample of rows where `isFraud==1`.","22d76ff5":"The columns are:\n\n* `step`: A timestamp \/ date variable that has been made arbitrary for data privacy.\n* `type`: The type of transaction. `type` is our only categorical independent variable. Categories include [cash_in, cash_out, debit, payment, transfer].\n* `amount`: Size of transaction.\n* Next 6 columns are account level info:\n    * Suffix `*Orig` refers to the originating account of the transaction.\n    * Suffix `*Dest` refers to destination account of the transaction.\n    * Prefix `name*` refers to Account ID number. An 'M' prefix, e.g. 'M1979...' denotes a merchant account. The name will not be directly used as a model feature, although we can extract the merchant prefix to create a boolean indicator variable. One thing to note is that balance data is not available for merchants. For merchants, the placeholder value for balances is zero.\n    * Prefix `oldbalance*` refers to account balance before transaction. \n    * Prefix `newbalance*` refers to account balance after transaction (and also great pair of shoes!)\n* `isFraud`: Our label \/ dependent variable\u2014whether the transaction was made by a fraudulent agent. 1 for fraudulent, 0 for not fraudulent.\n* `isFlaggedFraud`: Whether the transaction was flagged as fraud by the \"business model\". Since we are doing our own prediction here, we will ignore this variable.","4ed432e7":"Sklearn also provides a function for creating a visual output for the decision tree model. For brevity, only the first couple of levels are printed here; the entire tree can be much larger. This output illustrates the nature of the algorithm and particularly the exact set of conditions required to predict one class or the other.","2a5eab00":"In the box plots below, we can see that between the fraudulent and non-fraudulent data, `balancediffOrig` shows very contrasting distributions. For `isFraud==1`, the values tend to be much more negative, implying that these transactions are indeed moving large amounts of funds out of the originating account. This feature could end up having significant impact on the model predictions.","1cf53714":"There are also quite a few transactions where the amount is greater than 10,000,000, along the left edge of the scatter plots. These transactions are exclusively transfers where the destination account receives the change of the transaction amount and the originating account field has zero balance. None of these transactions are labeled as fraudulent.","4b1a9eda":"This tells us that the most influential features to the model are balance changes and `newbalanceDest`. Whether or not the transaction is a transfer is also important. This confirms our earlier supposition based on the data documentation that fraudulent transactions are often characterized by transfers of large amounts. On the other hand, if a transaction is not a transfer, `type` appears to be mostly irrelevant. Whether or not a merchant account is involved is also unimportant.","15d7910f":"## Preprocessing","39d94d59":"Let's look at some metrics commonly used for measuring model performance. The `sklearn.metrics.classification_report` function gives us a nice printout for assessing performance, including precision, recall, and f1-score for each of the classes independently. The separation for the classes is important especially when dealing with imbalanced data, as we are now. All of these metrics range from 0 to 1, higher numbers representing better accuracy. \n\n* **Precision:** The proportion of values selected by the model that should be selected. Precision is penalized by having more false positives.\n* **Recall:** The proportion of values that should be selected, that are actually selected by the model. Recall is penalized by having more false negatives.\n* **F1 score:** The harmonic mean of precision and recall.\n\nAnother measure we can use is Area Under Curve (AUC), which refers to the area under the ROC curve, which is a plot of the true positive rate against the false positive rate. AUC also ranges from 0 to 1 and is frequently used as a quick way to compare model performance.","4908236a":"We can see here that transactions involving merchant accounts are never fraudulent, while those transactions not involving merchants are sometimes fraudulent.","65527eb0":"At a high level, a decision tree model works by sequentially splitting the dataset along value thresholds of the predictor variables. The splits are determined by a criteria that quantifies how different two sets of data are (by default, Gini impurity). For example, if fraudulent transactions most frequently occur with amounts over 1,000,000, while non-fraudulent transactions are more frequently of lower amounts, our decision tree is likely to use `amount` as a node that splits our dataset into transactions of those over 1,000,000 and those that are less. The decision tree then considers each of those two groups and attempts to perform another split on one of the other features, and so on, until the split groups have zero impurity (all the members of each group have the same classification). While in actuality the data does not contain such explicit separations, this is the general logic used by the algorithm. In a basic sense, it can be thought of as categorizing the data based on its features. It repeats the process iteratively on each branch until an end (leaf) is reached, hence the \"tree\".\n\nAn advantage of using decision trees is that the results are highly interpretable and easy to visualize. This means we will be able to see exactly how the decision tree constructs itself, and what features it prioritizes in its construction. It will show us how the data is being split and by what values for each feature will lead to a certain classification. The same cannot be said for certain \"black box\" methods of modeling, such as neural networks\u2014which despite their potential for high performance, are often too complex to be intuitively understood and explained, even by the model developer.\n\nLet's implement the decision tree and measure its performance.","19777ff3":"### step","0a007c59":"### amount","a608a80e":"We can create some additional columns derived from existing data for features that may improve the model performance.","34bfb11a":"Let's look at correlation between the independent variables. Do any features follow similar patterns?","2b105e54":"We will now prepare the data for model input. Let's subset the dataframe to only select the variables we will use as model features, as well as the label for classification.","5a2307cf":"### What else is different about the fraudulent transactions?","e1e4ca20":"`step` is our time variable. Most transactions occur in the first half of our time sample range.","c6b1733c":"Given an account, `oldbalance*` and `newbalance*` are highly correlated with each other. That makes sense, because transaction amounts are typically a small proportion of account balances. In other words, account balances have high correlations with themselves.\n\nWhat's odd is that while `amount` has some correlation with `oldbalanceDest` and `newbalanceDest`, it has very low correlations with `oldbalanceOrig` and `newbalanceOrig`. Let's check pairwise scatterplots for these variables, starting with `amount` vs. `oldbalanceOrig` and `newbalanceOrig`.","e030a1e5":"The data has about 6.3 million rows and 11 columns. Each row represents a transaction.","af2973bb":"### Decision Tree","95fbe2ca":"### Data Imbalance","fea0f4ce":"All fraudulent transactions fall under the CASH_OUT or TRANSFER type.","a0a35a74":"For transaction type, cash withdrawals and payment transactions are the most common, while transfers and debits make up a smaller fraction of the data.","e47537c9":"### One-Hot Encoding","e3e37eda":"In order to objectively determine each features' influence on the predictions, we can calculate, summed up for each feature, the amount by which it reduces impurity in each node it represents. Sklearn provides these values in the `feature_importances_` attribute of `DecisionTreeClassifier`.","231be2bf":"### type","14375b30":"### Nulls","78a08b95":"Balance numbers show patterns similar to amount, i.e. heavily skewed to the right. We will touch on correlation between these variables next.","5248f2d5":"Amounts are skewed right\u2014the vast majority of transactions are low amounts.","4e31aadd":"Some interesting patterns here. There is a notably high frequency of transactions with the amount of exactly 10,000,000.","0d19c487":"The difference is less stark with `balancediffDest`, and the difference values also tend to be more positive, indicating that the transactions tend to move funds into the destination account.","d4cd4a92":"Let's look at frequency plots for the feature variables.","4298529c":"`type` is a categorical variable, which will need to be encoded as separate boolean dummy variables for each unique value of account type to be used by the model.","f7b4e337":"A commonality in these last two plots was that the balance difference values for `isFraud==1` had more variance and spread. This may be because our sample size for fraudulent data is much smaller. By nature, smaller samples are likely to exhibit higher variability.","094ba942":"# Predicting Fraudulent Transactions\n### February 2021\n#### Stephen So\n\nWe are presented with a labeled dataset of financial transactions, some of which are fraudulent. We will be performing exploratory data analysis on this data, and then creating a classifier model to predict whether a transaction is fraudulent given the included features. The objective of this project is to explain my thought processes in solving this problem, as well as addressing some of the issues that inherently face machine learning models. *(\"All models are wrong, but some are useful.\")* Using this notebook, I hope to focus primarily on transparency and clarity rather than raw predictive performance, and readability for an audience without a specialization in data science.","cd878b6a":"## Potential Improvements","d66cb37f":"## Data Input and EDA","3b72ab9d":"### Merchant Accounts"}}