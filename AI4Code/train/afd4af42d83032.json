{"cell_type":{"57017044":"code","547a5329":"code","aeb0bc89":"code","fc7b1f3a":"code","e761a4eb":"code","534d944e":"code","8f95b90d":"code","5a00d30d":"code","48ccfdc7":"code","bbb1b5fc":"code","f575a72d":"code","5d10fde1":"code","b7fb8236":"code","62777d85":"code","38bf1dba":"code","8ee5b86e":"markdown","d94dde53":"markdown","809f451f":"markdown","156f7d6d":"markdown","7aa580c4":"markdown","c19440ce":"markdown","4c9074b9":"markdown"},"source":{"57017044":"!pip install pydotplus","547a5329":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nfrom pydotplus import graph_from_dot_data\n","aeb0bc89":"def gini(p):\n    return (p)*(1 - (p)) + (1 - p)*(1 - (1 - p))\n\ndef entropy(p):\n    return - p*np.log2(p) - (1 - p)*np.log2((1 - p))\n\ndef error(p):\n    return 1 - np.max([p, 1 - p])","fc7b1f3a":"print('max value of gini is', gini(0.5))\nprint('max value of entropy is', entropy(0.5))\nprint('max value of classification error is', error(0.5))","e761a4eb":"x = np.arange(0, 1, 0.01)\n\n# Entropy list without None.\n## If there is 0 in entropy, None is printed as a result.\n### Create sc_ent variable for comparing with gini.\nent = [entropy(p) if p != 0 else None for p in x ]\nsc_ent = [e* 0.5 if e else None for e in ent]\n\n# Error list\n## If x is inputed into the classification error function, 0 is printed as a result.\nerr = [error(i) for i in x]\n\n# draw plot\nfig = plt.figure()\nax = plt.subplot(111)\nfor i, lab, ls, c, in zip([gini(x), ent, sc_ent, err],\n                         ['Gini Impurity', 'Entropy',\n                         'Entropy (scaled)', 'Classification Error'],\n                         ['--', '-', '-', '-.'],\n                         ['black', 'lightgray', 'red', 'green']):\n    line = ax.plot(x, i, label = lab, linestyle = ls,\n                  lw = 2, color = c)\n\nax.legend(loc = 'upper center', bbox_to_anchor = (0.5, 1.15),\n         ncol = 5, fancybox = True, shadow = False)\nax.axhline(y=0.5, linewidth = 1, color = 'k', linestyle = '--')\nax.axhline(y=1, linewidth = 1, color = 'k', linestyle = '--')\nplt.ylim([0, 1.1])\nplt.xlabel('p(i=1)')\nplt.ylabel('Impurity Index')\nplt.show()","534d944e":"# load dataset\niris = datasets.load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\nprint('Class label :', np.unique(y))","8f95b90d":"# split training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1, stratify = y)","5a00d30d":"X_combined = np.vstack((X_train, X_test)) # vlookup combine\ny_combined = np.hstack((y_train, y_test))  # hlookup combine","48ccfdc7":"tree = DecisionTreeClassifier(criterion = 'gini',\n                             max_depth = 4,\n                             random_state = 1)\ntree.fit(X_train, y_train)","bbb1b5fc":"# define function about visualizing decision_regions\ndef plot_decision_regions(X, y, classifier, test_idx = None, resolution = 0.02):\n    \n    # set marker and colormap\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n    \n    # draws a decision boundary.\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                          np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha = 0.3, cmap = cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n    \n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(X[y == cl, 0], X[y == cl, 1],\n                   alpha = 0.8, c = colors[idx],    # alpha : size of marker\n                   marker = markers[idx], label = cl,\n                   edgecolor = 'black')\n        \n    if test_idx:\n        X_test, y_test = X[test_idx, :], y[test_idx]\n        \n        plt.scatter(X_test[:, 0], X_test[:, 1],\n                   c = '', edgecolor = 'black', alpha = 1,\n                   s = 100, label = 'test set')","f575a72d":"# Create decision_regions graph\nplot_decision_regions(X_combined, y_combined,\n                     classifier = tree,\n                     test_idx = range(105, 150))\nplt.xlabel('petal length [cm]')\nplt.ylabel('petal width [cm]')\nplt.legend(loc = 'upper left')\nplt.tight_layout()\nplt.show()","5d10fde1":"dot_data = export_graphviz(tree, filled = True,\n                          rounded = True,\n                          class_names = ['Setosa',\n                                        'Versicolor',\n                                        'Virginica'],\n                          feature_names = ['petal length',\n                                          'petal width'],\n                          out_file = None)\ngraph = graph_from_dot_data(dot_data)\ngraph.write_png('tree.png')","b7fb8236":"Decision_Tree = plt.imread('.\/tree.png')\nplt.figure(figsize = (15, 8))\nplt.imshow(Decision_Tree)\nplt.tight_layout()\nplt.show()","62777d85":"# Creating RandomForestClassifier\nforest = RandomForestClassifier(criterion = 'gini',\n                               n_estimators = 25, # The number of Decision tree\n                               random_state = 1,\n                               n_jobs = 2) # Parallelizing model training\nforest.fit(X_train, y_train)","38bf1dba":"# Create decision_regions graph\nplot_decision_regions(X_combined, y_combined,\n                     classifier = forest,\n                     test_idx = range(105, 150))\nplt.xlabel('petal length [cm]')\nplt.ylabel('petal width [cm]')\nplt.legend(loc = 'upper left')\nplt.tight_layout()\nplt.show()","8ee5b86e":"### Learning Process for Random Forests\n\n1. Draw n random bootstrap samples by allowing redundancy in the training set.\n2. Learn the decision tree from the bootstrap sample.\n    - Choose d characteristics randomly without allowing redundancy.\n    - Nodes are split using properties that make the best classification based on objective functions such as information gain.\n3. Repeat steps 1 and 2 k times.\n4. Gather predictions from each decision tree and assign class labels by **majority vote**.\n\n\n### Characteristics of Random Forests\n\n- Unlike decision trees, they learn using only random d characteristics when learning.\n- Generally, pruning is not necessary.\n- The parameter that should be considered when tuning the hyperparameters is 'number of trees in step 3 k'.\n","d94dde53":"### Creating Decision tree","809f451f":"##### We can check it by adding a value (p = 0.5) that maximizes impurity.","156f7d6d":"### Information gain\n\nThe criteria for classifying decision trees are information gain. Information gains can be determined based on impurity. As the name suggests, impurity is an indicator of how various classes are mixed into the node.\n\n##### Information Gain\n$$ IG(D_p,f) = I(D_p) - \\sum_{j=1}^{m}\\frac{N_j}{N_p}I(D_j) $$\n\n\nSimply put, the information gain is determined by the sum of the impurity of the child nodes and the difference between the parent nodes. The lower the impurity of the child nodes, the greater the information gain.\n\n\nThe frequently used indicators of impurity in decision trees are **Entropy**, **Gini impurity**, and **Classification error**.","7aa580c4":"# Principle of Decision Tree\n\nThis notebook goal:\n- Understanding principle of Decision tree.\n- Understanding Impurities(Gini, entropy, classification error) definition.\n- Comparing with Impurities of Decision tree.\n- Using RandomForestclassification.\n\nYou can learn about Decision tree principle in [my blog](https:\/\/konghana01.tistory.com\/20).","c19440ce":"## RandomForests","4c9074b9":"### Comparison of impurity criteria in binary classification\n\n\n##### Entropy\n$$ I_H(t) = -\\sum_{i=1}^{c}P(i|t)log_2P(i|t)$$\n\n##### Gini impurity\n$$ I_G(t) = \\sum_{i=1}^{c}P(i|t)(1-P(i|t)) = 1 - \\sum_{i=1}^{c}(P(i|t))^2 $$\n\n##### Classification error\n$$ I_E = 1 - max(p(i|t)) $$"}}