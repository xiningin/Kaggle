{"cell_type":{"bb609159":"code","68e754f3":"code","0a73d9d2":"code","66ea8e8a":"code","4e8480f3":"code","ff08216c":"code","a8d694d0":"code","ba506ea3":"code","7a70c43c":"code","b6051a47":"code","fc09442c":"code","e2736454":"code","304d4609":"code","5ac0e032":"code","21a8c8cd":"code","e6a7c1db":"code","4ea1d7f3":"code","1dc6e0b6":"code","c56797ea":"code","4fc112fa":"code","ca0125a1":"code","12184524":"code","2ec3e3e8":"code","a20b1383":"code","29da83d6":"code","0cb9b064":"code","d5c1b397":"code","1c213794":"code","3b72ed60":"code","d4b0a6ea":"code","9642dd06":"code","d1974644":"code","95c1d86b":"code","8ac1404c":"code","98878957":"code","ac1b53bd":"code","87aa2838":"code","c4fcca7f":"code","e35aeae6":"code","480a4e90":"code","2013a55c":"code","924ed3f6":"code","cc0f4783":"code","1a6e35a2":"code","51fe7426":"code","5f1a7fb6":"code","08b4f114":"code","8049b3ca":"code","6204886a":"code","878e86b7":"code","2a06cf6d":"code","3158ce5b":"code","699db7f5":"code","e83b124c":"code","debfc6dc":"code","c97a5e7b":"code","f95b153f":"code","201ba68c":"code","ef5f595c":"code","5925bc3d":"code","2395fa39":"code","c06d9bb9":"code","4b4a457d":"code","e89ddeb4":"code","346b778c":"code","3c89daed":"code","d8a035dc":"code","0d21aac4":"code","f6e87f46":"code","53f88043":"code","858629e6":"code","5ddbba7c":"code","5f652f2d":"code","78143162":"code","3c05f49e":"code","2cc626ef":"code","0a4c6c2c":"code","7bca9769":"code","b03c42e3":"code","5030d00d":"code","627eba1b":"code","807196d5":"code","8a3c4aa8":"code","745d3d7a":"code","48baa692":"code","8531bb18":"code","eeb6e6fe":"code","8089b315":"code","71b0a139":"markdown","ccf6b1cf":"markdown","5a4c860d":"markdown","9fb110af":"markdown","c3a1e5a6":"markdown","869cbf40":"markdown","00291f07":"markdown","575c8677":"markdown","01e29474":"markdown","feab5714":"markdown","64fc0a87":"markdown","b363b437":"markdown","87ddf60a":"markdown","f83f4062":"markdown","4bcd0d14":"markdown","445ce83a":"markdown","b016e4ba":"markdown","034c1f30":"markdown","38e7dd61":"markdown","e6a771be":"markdown","7675fb08":"markdown","ee135194":"markdown","b363bc14":"markdown","87953054":"markdown","3184e478":"markdown","7eb207f6":"markdown","ce8929cb":"markdown","57a96ddc":"markdown","43bef021":"markdown","74390b2c":"markdown","c67c80cd":"markdown","26751fa6":"markdown","ebc36b31":"markdown","e37235cd":"markdown","873ddd04":"markdown","e9b4b3ce":"markdown","7eb79892":"markdown","c29dacf4":"markdown","e30a6704":"markdown","0a7d1170":"markdown","0c18db98":"markdown"},"source":{"bb609159":"import numpy as np \nimport pandas as pd \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport seaborn as sns\nsns.set_palette('Set2')\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Supress Scientific notation in python\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\n# Display all columns of long dataframe\npd.set_option('display.max_columns', None)\n\nimport re\n\nfrom math import sqrt \nfrom sklearn.metrics import mean_squared_log_error\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\nimport pandas_profiling","68e754f3":"# Import datasets\ntrain = pd.read_excel('..\/input\/Data_Train.xlsx')\ntest = pd.read_excel('..\/input\/Data_Test.xlsx')","0a73d9d2":"# Checkout the shape of datasets\ntrain.shape, test.shape","66ea8e8a":"train.profile_report()","4e8480f3":"train.sample(5)","ff08216c":"test.sample(5)","a8d694d0":"# Define categorical features\ncategorical_feature = ['Name','Location','Fuel_Type','Transmission','Owner_Type']","ba506ea3":"def missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"+\"There are \" + str(mis_val_table_ren_columns.shape[0]) +\" columns that have missing values.\")        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","7a70c43c":"missing_values_table(train)","b6051a47":"missing_values_table(test)","fc09442c":"train[categorical_feature].nunique()","e2736454":"test[categorical_feature].nunique()","304d4609":"# Check the Fuel_Type which is not in test set\nlist(train.Fuel_Type[~train.Fuel_Type.isin(test.Fuel_Type)].unique())","5ac0e032":"train = train[train.Fuel_Type != 'Electric']","21a8c8cd":"train[\"Full_name\"] = train.Name.copy()\ntest[\"Full_name\"] = test.Name.copy()\ntrain.Name.sample(20)","e6a7c1db":"temp1 = list(train.Name.str.split(' ').str[0].unique())\ntemp2 = list(test.Name.str.split(' ').str[0].unique())\ntemp3 = [item for item in temp1 if item not in temp2]\ntemp3","4ea1d7f3":"train = train[~train.Name.str.contains('|'.join(temp3))]","1dc6e0b6":"def remove_year(data):\n    result = re.search(\"([0-9]+[-]+[0-9]+)\",data)\n    if result:\n        arr = data.replace(result.group(1),\"\")\n        return arr\n    else:\n        return data","c56797ea":"train.Name = train.Name.apply(lambda x: remove_year(x))\ntest.Name = test.Name.apply(lambda x: remove_year(x))","4fc112fa":"def remove_char(str):\n    arr = ' '.join(str.split()) #replace multiple spaces to single space\n    arr = re.sub(r\"[-(){}<>\/\\.,]\",\"\", arr) #remove special characters\n    return arr.lower() #lowercase all characters","ca0125a1":"train.Full_name = train.Name.apply(lambda x: remove_char(x))\ntest.Full_name = test.Name.apply(lambda x: remove_char(x))","12184524":"train.Name = train.Full_name.apply(lambda x: \" \".join(x.split(' ')[:2]))\ntest.Name = test.Full_name.apply(lambda x: \" \".join(x.split(' ')[:2]))\n\n# Filter brand name for a more generic aggregation in further calculations\ntrain['brand'] = train.Name.apply(lambda x: \" \".join(x.split(' ')[:1]))\ntest['brand'] = test.Name.apply(lambda x: \" \".join(x.split(' ')[:1]))\n","2ec3e3e8":"train.Name.sample(10)","a20b1383":"# Define function to correct the New_Price value\ndef price_correct(x):\n    if str(x).endswith('Lakh'):\n        return float(str(x).split()[0])*100000\n    elif str(x).endswith('Cr'):\n        return float(str(x).split()[0])*10000000\n    else:\n        return x\n\ntrain.New_Price = train.New_Price.apply(price_correct)\ntest.New_Price = test.New_Price.apply(price_correct)","29da83d6":"train.Mileage = train.Mileage.replace('0.0 kmpl', np.NaN).apply(lambda x: str(x).split()[0]).astype(float).round(2) # Convert 0 value to Nan, remove unit and convert to float type and round off to 2 decimal place.\ntrain.Engine = train.Engine.apply(lambda x: str(x).split()[0]).astype(float) # Remove the CC part\ntrain.Power = train.Power.replace('null bhp', np.NaN).apply(lambda x: str(x).split()[0]).astype(float).round(2) # convert null value to NaN than as above\n\ntest.Mileage = test.Mileage.replace('0.0 kmpl', np.NaN).apply(lambda x: str(x).split()[0]).astype(float).round(2)\ntest.Engine = test.Engine.apply(lambda x: str(x).split()[0]).astype(float)\ntest.Power = test.Power.replace('null bhp', np.NaN).apply(lambda x: str(x).split()[0]).astype(float).round(2)","0cb9b064":"# Fill missing values aggregating by Name mean and median\ntrain.Engine = train.groupby('Name').Engine.apply(lambda x: x.fillna(x.median()))\ntrain.Power = train.groupby('Name').Power.apply(lambda x: x.fillna(x.mean()))\ntrain.Mileage = train.groupby('Name').Mileage.apply(lambda x: x.fillna(x.mean()))\ntrain.Seats = train.groupby('Name').Seats.apply(lambda x: x.fillna(x.median()))\ntrain.New_Price = train.groupby('Name').New_Price.apply(lambda x: x.fillna(x.mean()))\n\ntest.Engine = test.groupby('Name').Engine.apply(lambda x: x.fillna(x.median()))\ntest.Power = test.groupby('Name').Power.apply(lambda x: x.fillna(x.mean()))\ntest.Mileage = test.groupby('Name').Mileage.apply(lambda x: x.fillna(x.mean()))\ntest.Seats = test.groupby('Name').Seats.apply(lambda x: x.fillna(x.median()))\ntest.New_Price = test.groupby('Name').New_Price.apply(lambda x: x.fillna(x.mean()))\n\n# Fill remaining missing values aggregating by brand mean and median\ntrain.Power = train.groupby('brand').Power.apply(lambda x: x.fillna(x.mean()))\ntrain.Mileage = train.groupby('brand').Mileage.apply(lambda x: x.fillna(x.mean()))\ntrain.Seats = train.groupby('brand').Seats.apply(lambda x: x.fillna(x.median()))\ntrain.New_Price = train.groupby('brand').New_Price.apply(lambda x: x.fillna(x.mean()))\n\ntest.Power = test.groupby('brand').Power.apply(lambda x: x.fillna(x.mean()))\ntest.New_Price = test.groupby('brand').New_Price.apply(lambda x: x.fillna(x.mean()))\n\n# Fill remaining missing values aggregating by whole column mean\ntrain.New_Price = train.New_Price.fillna(train.New_Price.mean())\ntest.New_Price = test.New_Price.fillna(test.New_Price.mean())\n","d5c1b397":"missing_values_table(train)","1c213794":"missing_values_table(test)","3b72ed60":"test[test.Power.isnull()]","d4b0a6ea":"test.Power.fillna(test[test.Engine.between(1900,2000)].Power.mean(), inplace=True)","9642dd06":"# Define a function to plot the distribution of various features\ndef count_plot(data,col,figx,figy,rotate = 'N', order = 'Y'):\n    plt.figure(figsize=(figx, figy));\n    if order == 'Y':\n        g = sns.countplot(x=col, data=data, order = data[col].value_counts().index)\n    else:\n        g = sns.countplot(x=col, data=data)\n    plt.title('Distribution of %s' %col);\n    if rotate == 'Y':\n        plt.xticks(rotation=45);\n    ax=g.axes\n    for p in ax.patches:\n         ax.annotate(f\"{p.get_height() * 100 \/ data.shape[0]:.2f}%\",\n                     (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                     ha='center', \t# horizontal alignment\n                     va='top',\t\t# Vertical alignment\n                     fontsize=10,\t# Fontsize\n                     color='black',\t# Color set\n                     rotation=0,\t# Rotation type\n                     xytext=(0,10),\t# caption position\n                     textcoords='offset points' # Caption placement\n                    ) ","d1974644":"# Check the Year make distribution of the Training data\ncount_plot(train,'Year',20,5,rotate = 'Y', order = 'N')","95c1d86b":"# Check the Year make distribution of the Test data\ncount_plot(test,'Year',20,5,rotate = 'Y', order = 'N')","8ac1404c":"# Check the Fule type distribution of the Training data\ncount_plot(train,'Fuel_Type',6,6)","98878957":"# Check the Fule type distribution of the Test data\ncount_plot(test,'Fuel_Type',6,6)","ac1b53bd":"# Check the Transmission distribution of the Training data\ncount_plot(train,'Transmission',5,8)","87aa2838":"# Check the Transmission distribution of the Test data\ncount_plot(test,'Transmission',5,8)","c4fcca7f":"# Check the Transmission distribution of the Training data\ncount_plot(train,'Owner_Type',7,8)","e35aeae6":"# Check the Transmission distribution of the Test data\ncount_plot(test,'Owner_Type',7,8)","480a4e90":"# Check the Transmission distribution of the Training data\ncount_plot(train,'Location',20,7)","2013a55c":"# Check the Transmission distribution of the Test data\ncount_plot(test,'Location',20,7)","924ed3f6":"# Check the Transmission distribution of the Train data\ncount_plot(train,'Seats',10,6)","cc0f4783":"# Check the Transmission distribution of the Test data\ncount_plot(test,'Seats',10,6)","1a6e35a2":"train[train.Seats.isin([0,2,9,10])]","51fe7426":"train.loc[3999,'Seats'] = 5\n\ntrain.Seats[train.Seats == 9] = 10","5f1a7fb6":"# Define function for the next set of graph distributions.\ndef dist_plot(data, col, bins, color, figx, figy, kde = True):\n    plt.figure(figsize=(figx,figy))\n    sns.distplot(data[col].values, bins=bins, color=color, kde_kws={\"shade\": True}, label=\"Low\", kde=kde)\n    plt.title(\"Histogram of %s Distribution\"%col)\n    plt.xlabel('%s'%col, fontsize=12)\n    plt.ylabel('Vehicle Count', fontsize=12)\n    plt.show();","08b4f114":"# Check the Power distribution of the Train data\ndist_plot(train,'Power', 50, 'blue', 20, 5)","8049b3ca":"# Check the Power distribution of the Test data\ndist_plot(test,'Power', 50, 'blue', 20, 5)","6204886a":"# Check the Power distribution of the Train data\ndist_plot(train,'Engine', 25, 'brown', 20, 5)","878e86b7":"# Check the Power distribution of the Test data\ndist_plot(test,'Engine', 25, 'brown', 20, 5)","2a06cf6d":"# Check the Power distribution of the Train data\ndist_plot(train,'Mileage', 50, 'green', 20, 5)","3158ce5b":"# Check the Power distribution of the Test data\ndist_plot(test,'Mileage', 50, 'green', 20, 5)","699db7f5":"# Check the Power distribution of the Training data\ndist_plot(train,'Kilometers_Driven', 50, 'magenta', 20, 5)","e83b124c":"# Check the Power distribution of the Test data\ndist_plot(test,'Kilometers_Driven', 50, 'magenta', 20, 5)","debfc6dc":"col = 'Kilometers_Driven'\nfrom scipy import stats\noutliers = train[col][(np.abs(stats.zscore(train[col])) > 3)]\noutliers","c97a5e7b":"train[train.Kilometers_Driven.isin(outliers)]","f95b153f":"train = train[~train.Kilometers_Driven.isin(outliers)]","201ba68c":"dist_plot(train,'Kilometers_Driven', 50, 'magenta', 20, 5, kde = True)","ef5f595c":"# Record the age of the car\nimport datetime\ntrain['Age'] = datetime.datetime.now().year - train['Year']\ntest['Age'] = datetime.datetime.now().year - test['Year']","5925bc3d":"# Record the number of words in the Full_name of the car\ntrain['Name_length'] = train.Full_name.apply(lambda x: len(str(x).split(' ')))\ntest['Name_length'] = test.Full_name.apply(lambda x: len(str(x).split(' ')))","2395fa39":"# Define categorical features\ncategorical_features = ['Location','Fuel_Type','Transmission','Owner_Type','Seats']\n\n# Define function for dummy operation\ndef get_dummies(dataframe,feature_name):\n  dummy = pd.get_dummies(dataframe[feature_name], prefix=feature_name)\n  dummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\n  return pd.concat([dataframe,dummy], axis = 1)\n\n# Dummify categorical features\nfor i in categorical_features:\n    train = get_dummies(train, i)\n    test = get_dummies(test, i)\n","c06d9bb9":"# Define function to aggregate metrics for different features\ndef aggregate_features(data):   \n    \n    aggregate_dict = {  'Age' : ['count'],\n                        'Mileage' : ['sum','max','min','mean','std','median','skew'],\n                        'Power' : ['sum','max','min','mean','std','median','skew'],\n                        'Engine' : ['sum','max','min','mean','std','median','skew']}\n    \n    data_agg = data.groupby(['Name']).agg(aggregate_dict)\n    data_agg.columns = ['_'.join(col).strip() for col in data_agg.columns.values]\n    data_agg.reset_index(inplace=True)    \n    data_agg = pd.merge(data, data_agg, on='Name', how='left')    \n    return data_agg","4b4a457d":"# Create aggregated features\ntrain = aggregate_features(train)\ntest = aggregate_features(test)","e89ddeb4":"missing_values_table(train)","346b778c":"missing_values_table(test)","3c89daed":"train.Mileage_skew = train.Mileage_skew.fillna(train.Mileage_skew.mean())\ntrain.Power_skew = train.Power_skew.fillna(train.Power_skew.mean())\ntrain.Engine_skew = train.Engine_skew.fillna(train.Engine_skew.mean())\ntrain.Mileage_std = train.Mileage_std.fillna(train.Mileage_std.mean())\ntrain.Power_std = train.Power_std.fillna(train.Power_std.mean())\ntrain.Engine_std = train.Engine_std.fillna(train.Engine_std.mean())\n\ntest.Mileage_skew = test.Mileage_skew.fillna(test.Mileage_skew.mean())\ntest.Power_skew = test.Power_skew.fillna(test.Power_skew.mean())\ntest.Engine_skew = test.Engine_skew.fillna(test.Engine_skew.mean())\ntest.Mileage_std = test.Mileage_std.fillna(test.Mileage_std.mean())\ntest.Power_std = test.Power_std.fillna(test.Power_std.mean())\ntest.Engine_std = test.Engine_std.fillna(test.Engine_std.mean())","d8a035dc":"import gensim\nimport multiprocessing\ncores = multiprocessing.cpu_count()\n\n# Define function to add\/aggregate embeddings of single token text\ndef word_vector(tokens, size):\n    vec = np.zeros(size).reshape((1, size))\n    count = 0\n    for word in tokens:\n        try:\n            vec += model_w2v.wv[word].reshape((1, size))\n            count += 1\n        except KeyError:  # handling the case where the token is not in vocabulary\n            continue\n    if count != 0:\n        vec \/= count\n    return vec","0d21aac4":"total_names = pd.concat([train.Full_name, test.Full_name], ignore_index=True)\ntokens = total_names.apply(lambda x: x.split()) # tokenizing text\ntrain_tokens = train.Full_name.apply(lambda x: x.split()) # tokenizing text\ntest_tokens = test.Full_name.apply(lambda x: x.split()) # tokenizing text\ntokens_size = len(tokens)\ntrain_tokens_size = len(train_tokens)\ntest_tokens_size = len(test_tokens)","f6e87f46":"model_w2v = gensim.models.Word2Vec(\n            tokens,\n            size=200, # desired no. of features\/independent variables\n            window=2, # context window size\n            min_count=2,\n            sg = 1, # 1 for skip-gram model\n            hs = 0, # off heirarchichal softmax\n            negative = 1, # for negative sampling\n            workers= cores-1, # no.of cores\n#             sample=.1,\n            alpha=0.009, \n            min_alpha=0.0009,\n#             seed=0,\n#             hashfxn=hash\n) \n\nmodel_w2v.train(tokens,\n                total_examples= tokens_size,\n                epochs=40)","53f88043":"wordvec_train_array = np.zeros((train_tokens_size, 200)) \nwordvec_test_array = np.zeros((test_tokens_size, 200))\n\nfor i in range(train_tokens_size):\n    wordvec_train_array[i,:] = word_vector(train_tokens[i], 200)\nwordvec_train_df = pd.DataFrame(wordvec_train_array)\n\nfor i in range(test_tokens_size):\n    wordvec_test_array[i,:] = word_vector(test_tokens[i], 200)\nwordvec_test_df = pd.DataFrame(wordvec_test_array)","858629e6":"wordvec_train_df.shape, wordvec_test_df.shape","5ddbba7c":"model_w2v.wv.most_similar(positive=\"hyundai\")","5f652f2d":"# Define function for multiple aggregation on a dataframe rows\ndef agg_df(df):\n    return pd.DataFrame(\n                        {'Name_sum':df.sum(axis=1),\n                         'Name_mean':df.mean(axis=1),\n                         'Name_std':df.std(axis=1),\n                         'Name_max':df.max(axis=1),\n                         'Name_min':df.min(axis=1),\n                         'Name_median':df.median(axis=1),\n                         'Name_skew':df.skew(axis=1)\n                        }\n                       )","78143162":"# Add aggregated features on the Name word2vec vctors.\ntrain = pd.concat([train, agg_df(wordvec_train_df)], axis=1) \ntest = pd.concat([test, agg_df(wordvec_test_df)], axis=1) ","3c05f49e":"train.head()","2cc626ef":"features = ['Year','Kilometers_Driven','Mileage','Engine','Power','Age','Price']\n\n# Through CORRMAT\nfrom mlens.visualization import corrmat\ncorrmat(train[features].corr(), inflate=False)\nplt.show();","0a4c6c2c":"# Engine and Power \nplt.figure(figsize=(8,6))\nplt.scatter(train.Power, train.Engine, c='blue')\nplt.xlabel('Power(bhp)', fontsize=12)\nplt.ylabel('Engine(cc)', fontsize=12)\nplt.show();","7bca9769":"# Engine and Mileage \nplt.figure(figsize=(8,6))\nplt.scatter(train.Mileage, train.Engine, c='blue')\nplt.xlabel('Mileage(kmpl)', fontsize=12)\nplt.ylabel('Engine(cc)', fontsize=12)\nplt.show();","b03c42e3":"# Power and Price\nplt.figure(figsize=(8,6))\nplt.scatter(train.Power, train.Price, c='red')\nplt.xlabel('Power(bhp)', fontsize=12)\nplt.ylabel('Price(lacs)', fontsize=12)\nplt.show();","5030d00d":"# Engine and Price\nplt.figure(figsize=(8,6))\nplt.scatter(train.Engine, train.Price, c='green')\nplt.xlabel('Engine(cc)', fontsize=12)\nplt.ylabel('Price(lacs)', fontsize=12)\nplt.show();","627eba1b":"# Age and Price\nplt.figure(figsize=(8,6))\nplt.scatter(train.Age, train.Price, c='orange')\nplt.xlabel('Age(years)', fontsize=12)\nplt.ylabel('Price(lacs)', fontsize=12)\nplt.show();","807196d5":"# Take backup before dropping some features\ntrain_backup = train.copy() \ntest_backup = test.copy() \n\n# Drop irrelevant features\ndrop_features = ['Location','Fuel_Type','Transmission','Owner_Type','Seats','Full_name','Name','brand']\nbackup_train = train.drop(drop_features, axis=1, inplace=True)\nbackup_test = test.drop(drop_features, axis=1, inplace=True)","8a3c4aa8":"# Assign values to variables for training and testing\n\nX_train = train.drop(labels=['Price'], axis=1) # Assign all features except Price to X\ny_train = np.log1p(train['Price'].values) # Convert Price to log scale\nX_test = test.copy()","745d3d7a":"# Scale the train and test set before feeding to the model\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = pd.DataFrame(sc.fit_transform(X_train),columns = X_train.columns)\nX_test = pd.DataFrame(sc.transform(X_test),columns = X_test.columns)","48baa692":"train_X = X_train.copy()\ntrain_y = y_train.copy()\ntest_X = X_test.copy()\n\n# Define LGBM function\ndef runLGB(train_X, train_y, val_X=None, val_y=None, test_X=None, dep=-1, seed=0, data_leaf=5):\n    params = {}\n    params[\"objective\"] = \"regression\"\n    params['metric'] = 'l2_root'\n    params['boosting'] = 'gbdt'\n#     params[\"max_depth\"] = dep\n#     params[\"num_leaves\"] = 39\n#     params[\"min_data_in_leaf\"] = data_leaf\n    params[\"learning_rate\"] = 0.009\n    params[\"bagging_fraction\"] = 0.75\n    params[\"feature_fraction\"] = 0.75\n    params[\"feature_fraction_seed\"] = seed\n    params[\"bagging_freq\"] = 1\n    params[\"bagging_seed\"] = seed\n#     params[\"lambda_l2\"] = 5\n#     params[\"lambda_l1\"] = 5\n    params[\"silent\"] = True\n    params[\"random_state\"] = seed,\n    num_rounds = 3000\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n\n    if val_y is not None:\n        lgtest = lgb.Dataset(val_X, label=val_y)\n        model = lgb.train(params, lgtrain, num_rounds, valid_sets=[lgtest], early_stopping_rounds=50, verbose_eval=100)\n    else:\n        lgtest = lgb.DMatrix(val_X)\n        model = lgb.train(params, lgtrain, num_rounds)\n\n    pred_val_y = model.predict(val_X, num_iteration=model.best_iteration)\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n  \n    loss = 0\n    \n    if val_y is not None:\n        loss = sqrt(mean_squared_log_error(np.expm1(val_y), np.expm1(pred_val_y)))\n        return model, loss, pred_test_y\n    else:\n        return model, loss, pred_test_y\n\n## K-FOLD train\n\ncv_scores = [] # array for keeping cv-scores for each fold.\npred_test_full = 0 # array to keep predictions of each fold.\npred_train = np.zeros(train_X.shape[0])\nn_fold = 10\nprint(f\"Building model over {n_fold} folds\\n\")\nkf = KFold(n_splits=n_fold, shuffle=True, random_state=4)\n\nfeature_importance = pd.DataFrame()\nfor fold_n, (dev_index, val_index) in enumerate(kf.split(train_X, train_y)):    \n    dev_X, val_X = train_X.iloc[dev_index,:], train_X.iloc[val_index,:]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n\n    model, loss, pred_t = runLGB(dev_X, dev_y, val_X, val_y, test_X, dep=8, seed=0)\n      \n    pred_test_full += pred_t\n    print(f\"\\n>>>>RMSLE for fold {fold_n+1} is: {loss}<<<<\\n\")\n    cv_scores.append(loss)\n    \n    # feature importance aggregation over n folds\n    fold_importance = pd.DataFrame()\n    fold_importance[\"feature\"] = X_train.columns\n    fold_importance[\"importance\"] = model.feature_importance()\n    fold_importance[\"fold\"] = fold_n + 1\n    feature_importance = pd.concat([feature_importance, fold_importance], axis=0)","8531bb18":"print(f\"Mean RMSLE score over folds is: {np.mean(cv_scores)}\")\n\n# Aggregate mean prediction over 10 folds.\npred_test_full \/= n_fold\npred_test_final = np.expm1(pred_test_full)","eeb6e6fe":"# Plot feature importance mean aggregated over 10 folds\nplt.figure(figsize=(20, 20));\nfeature_importance = pd.DataFrame(feature_importance.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False).reset_index())[:50]\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importance);\nplt.title('Feature Importance (average over folds)');","8089b315":"# Create submission file.\nPredict_submission = pd.DataFrame(data=pred_test_final, columns=['Price'])\nwriter = pd.ExcelWriter('Output.xlsx', engine='xlsxwriter')\nPredict_submission.to_excel(writer,sheet_name='Sheet1', index=False)\nwriter.save()","71b0a139":"Lets have a look at the correlation between various relevant features through Heatmap","ccf6b1cf":"That's a very legit correlation between **Engine** and **Mileage** i.e. as the Engine capacity increases its mileage decreases","5a4c860d":"Lets remove the year content in the names","9fb110af":"# Predicting The Costs Of Used Cars - Hackathon By Imarticus Learning\n\n![](https:\/\/i.imgur.com\/Q0UdbCH.png)\n\nDriverless cars are getting closer to reality and at a faster pace than ever. But it is still a bit far fetched dream to have one in your garage. For the time being, there are still a lot of combustion and hybrid cars that roar around the road, for some it chills. Though the overall data on sales of automobiles shows a huge drop in sales in the last couple of years, cars are still a big attraction for many. Cars are more than just a utility for many. They are often the pride and status of the family. We all have different tastes when it comes to owning a car or at least when thinking of owning one.\n\nWell here of course as the name suggests we are not concentrating on a new car, rather our interest is in knowing the prices of used cars across the country whether it is a royal l luxury sedan or a cheap budget utility vehicle. In this hackathon, you will be predicting the costs of used cars given the data collected from various sources and distributed across various locations in India.\n\nLet\u2019s see if your data science skills can help you predict the price of a used car based on a given set of features discussed below.\n\nSize of training set: 6,019 records\n\nSize of test set: 1,234 records\n\n## Features:\n\n*     Name: The brand and model of the car.\n*     Location: The location in which the car is being sold or is available for purchase.\n*     Year: The year or edition of the model.\n*     Kilometers_Driven: The total kilometres driven in the car by the previous owner(s) in KM.\n*     Fuel_Type: The type of fuel used by the car.\n*     Transmission: The type of transmission used by the car.\n*     Owner_Type: Whether the ownership is Firsthand, Second hand or other.\n*     Mileage: The standard mileage offered by the car company in kmpl or km\/kg\n*     Engine: The displacement volume of the engine in cc.\n*     Power: The maximum power of the engine in bhp.\n*     Seats: The number of seats in the car.\n*     New_Price: The price of a new car of the same model.\n*     Price: The price of the used car in INR Lakhs.\n\n## Evaluation Metric:\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the predicted value and observed score values. The final score calculation is done in the following way:\n\nSubmissions are evaluated on Root-Mean-Squared-Log-Error (RMSLE) error = RMSLE (error)\n\nScore = 1 \u2013 error","c3a1e5a6":"Crazy naming convention. Lets find unique car brands in the train set which are **not** in test set","869cbf40":"No other ambiguity found except the KM driven values. Let's remove these and check the distribution","00291f07":"Mileage records are also similarly distributed in both the sets. Lets check Kilometers_Driven data now","575c8677":"Let's take a look at the train dataset after feature aggregation","01e29474":"Very less number of values are missing. Let's fill these values with the mean of the respective column","feab5714":"Now lets check the distribution of each relevant feature","64fc0a87":"We observed that there are no vehicles with **0** or **9** seats in the test set. Lets check cars with **0,2,9 and 10** seats in the training data and adjust the training data to align with the test set.","b363b437":"Let's remove special characters like **[- . \/()]** etc","87ddf60a":"#### Here we can observe below things\n\n* Power and Engine are highly correlated.\n* Price has good correlation with Engine and Power.\n* We can see that Price is negatively correlated with Mileage, KMs_Driven and Age which is legit\n\nSo, let's see actual distribution as per the above correlation.","f83f4062":"Word2Vec vectors are giving appropriate results. That's nice!!","4bcd0d14":"Lets keep only the **first two relevant words** in the car names and remove the rest to be consistent across the dataset","445ce83a":"Let's check the same records for these outlier values","b016e4ba":"Let's move ahead and create aggregated vectors for the vehicle name in each record","034c1f30":"Both the features **Engine** and **Power** are almost **linearly** corelated.","38e7dd61":"Let's checkout the missing values again.","e6a771be":"Lets check the record for missing Power in test dataset","7675fb08":"Now the Kilometers_Driven distribution of the training set looks more meaningful and in-sync with the test set\n\nLet's move on and create new features from the existing ones.","ee135194":"Here we can see that the data in **training** set is **highly skewed** which is a pointer to outliers. Lets check those black birds","b363bc14":"Let's have a look at the car names after processing","87953054":"There are **no** Electric vehicles in the **Test** set. Hence remove the rows from training set where **Fuel_type** is **Electric**","3184e478":"lets remove the records which contains these brands from the train set first","7eb207f6":"Coming to Names column, Lets check the naming pattern of different cars in the Dataset","ce8929cb":"Lets checkout the power, Mileage and Engine distribution across the Training and the test set","57a96ddc":"Moving on to the other metrics, we will correct the feature values as per below strategy \n\n* Remove the New_price currency unit from each value and convert into relevant amount\n* Remove the Engine capacity unit from each value\n* Remove the Mileage unit and comvert all \"0.0 kmpl\" value to NaN\n* Remove the units of Power and convert \"null bhp\" to NaN\n\n After all this filtering, we will fill the missing values with the mean or median value respective to each feature","43bef021":"That's right!! Aeging vehicles sells for lesser price as compared to the younger vehicles.","74390b2c":"Engine Data seems to be consistent across both the sets. Lets check the Mileage distribution","c67c80cd":"Lets fill this one value with the mean value of the cars having similar Engine capacity","26751fa6":"Clear difference in the uniqueness of **Name** and **Fuel_type** feature. Lets check and remove the redundant records from the training set","ebc36b31":"### Model training and prediction\n\nWe will use **LGB** because it generally **trains** much **faster** as compared to **XGBoost** and mostly gives **equivalent** or **better results**.","e37235cd":"Of course, the increase in **Power** comes with the increase in vehicle **price**. But the price factor seems to have **plateaued** after a certain Power range of 250 bhp","873ddd04":"Now lets fill the missing values relevant to there Brand and model","e9b4b3ce":"So this was it... This kernel is open to suggestions for improvement over this score through any means other than stacking and hyper-parameter tuning. If you have such idea ... please share and we will try to improve on current score.\n\nThanks for reading guys !!","7eb79892":"Power Data distribution is similar. Lets check the Engine distribution","c29dacf4":"The data seems legit except for only some records with 0 and 9 seats. Lets fill the 0 audi a4 seat with 5 seats and the other vehicle having 9 seats with 10 seats.\n\n","e30a6704":"Let's check the missing values after creating the aggregated features","0a7d1170":"**Engine** also shows similar pattern as of Power when shocased with the **Price**.","0c18db98":"Let's have a look at the different aspects of the dataset through amazing pandas profiling tool"}}