{"cell_type":{"ea7127ce":"code","fff0007e":"code","0ba72723":"code","ecf2bfa9":"code","bbf52b08":"code","714695a4":"code","5e9b5e02":"code","f9893e81":"markdown","15be3c4a":"markdown","4337209b":"markdown","db1dbd84":"markdown","15d1898f":"markdown"},"source":{"ea7127ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fff0007e":"# Load and read train.csv data\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","0ba72723":"# Load and read test.csv data\n\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","ecf2bfa9":"# Load and read gender.csv data\n\ngender_data = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ngender_data.head()","bbf52b08":"# How many % of the woman on board survived?\n\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","714695a4":"# How many % of the man on board survived?\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","5e9b5e02":"# Import RandomForest Classifier from sklearn. Look for features \"Pclass\", \"Sex\", \"SibSp\", \"Parch\"\n# Create 100 trees, with max depth of 5\n# Print the prediction in a new file called \"submission.csv\"\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","f9893e81":"# Explore a pattern\n\nWe'll check if this pattern holds true in the data (in train.csv).","15be3c4a":"Score: 0.77511","4337209b":"# Load the Data\n\nLoad the Data in the input file an show the first 5 rows.","db1dbd84":"# Creating a Machine Learning Model\n\nTo discover more patterns, we have to construct not only one decision tree, but severals. 3 decision trees might be reasonable, but we'll construct 100. In this case, we are speaking of a **random forest model**. With the random forest model, hundreds of decision trees will be created and analyzied. The outcome with the most votes will win.\n\nFor the next step, we will create a random forest model with the following features: **(\"Pclass\", \"Sex\", \"SibSp\", and \"Parch\")**.\nThe new predictions will be saved in a CSV file **submission.csv**.","15d1898f":"# Conclusion\n\nAlmost 75% of the women on board and 19% of the man on board survived. In this case, gender is a pattern we can work with. But the data contains multiple columns so looking for other patterns might be reasonable."}}