{"cell_type":{"ea13a4fb":"code","9445b5c3":"code","7959af39":"code","40df53c9":"code","b371e61b":"code","72fb480d":"code","e517e8ce":"code","052eb94a":"code","31902266":"code","72a32654":"code","b633543d":"code","1895904e":"code","94bd604a":"code","691d4d37":"code","1fd7f04d":"code","1f780e99":"code","18b261e0":"code","f040ea2d":"code","e500cea7":"code","d9606750":"code","b38df095":"code","73411a6e":"code","e91a0be5":"code","9b89f8c1":"code","afc7eb56":"code","1c94eeb6":"code","0d0b23bd":"code","ae7b189a":"code","7fb0decd":"markdown","c8041d90":"markdown","33e98db8":"markdown","4d8338e2":"markdown","df98c8a8":"markdown","6d0e4d4c":"markdown","92969b61":"markdown","690ff1d8":"markdown"},"source":{"ea13a4fb":"import numpy as np\nimport pandas as pd\nimport os\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import layers, losses","9445b5c3":"data = fetch_20newsgroups(shuffle=True, random_state=42)\ndata.keys()","7959af39":"# example of targets\ntarget_names = data['target_names']\ntarget_names[:5]","40df53c9":"# creating single dataframe\ndf = pd.DataFrame()\ndf['text'] = data['data']\ndf['target'] = data['target']\ndf['target'] = df['target'].apply(lambda x: target_names[x])\ndf['target_wide'] = df['target'].apply(lambda x: x.split('.')[0])\nprint(df.shape)\ndf.head(3)","b371e61b":"# we have got 7 general topics (alt, comp, misc...)\n# let's filter such topics with only one smaller topic (alt, misc, soc)\n# need it to perform complex cross-validation\ndf.groupby('target_wide')['target'].unique()","72fb480d":"filter_mask = df.groupby('target_wide')['target'].nunique() > 1\nneeded_cats = filter_mask[filter_mask].keys()\nprint('Num of rows before:', df.shape[0])\ndf = df[df['target_wide'].isin(needed_cats)]\nprint('Num of rows after:', df.shape[0])","e517e8ce":"# number of general\/small topics\ndf['target_wide'].nunique(), df['target'].nunique()","052eb94a":"# let's take 'comp' category as an example\ndf_crop = df[df['target_wide']=='comp']\ndf_crop['target'].value_counts(1)","31902266":"# 80\/20 split\nX_train, X_test = train_test_split(df_crop, test_size=0.2, random_state=42)\nX_train.shape[0], X_test.shape[0]","72a32654":"# we want to find new topic in test set,\n# which is not in the train set ('comp.graphics')\ntrain_sub_mask = X_train['target']!='comp.graphics'\nX_train_sub = X_train[train_sub_mask]\n\nX_train_sub = X_train_sub.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\nprint(X_train_sub.shape[0])","b633543d":"# using tf-idf to convert text into numerical features\nMAX_FEATS = 512\nvectorizer = TfidfVectorizer(max_df=0.1,\n                             min_df=5,\n                             stop_words='english',\n                             analyzer='word',\n                             lowercase=True,\n                             max_features=MAX_FEATS\n                            )\nX_train_vec = vectorizer.fit_transform(X_train_sub['text']).todense()\nX_test_vec = vectorizer.transform(X_test['text']).todense()","1895904e":"class AnomalyDetector(Model):\n    def __init__(self):\n        super(AnomalyDetector, self).__init__()\n        activ = 'relu'\n        self.encoder = tf.keras.Sequential([\n          layers.Dense(128, activation=activ),\n          layers.Dense(64, activation=activ),\n          layers.Dense(32, activation=activ)])\n\n        self.decoder = tf.keras.Sequential([\n          layers.Dense(64, activation=activ),\n          layers.Dense(128, activation=activ),\n          layers.Dense(MAX_FEATS)\n            ])\n\n    def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded","94bd604a":"autoencoder = AnomalyDetector()\nautoencoder.compile(optimizer='adam', loss='mse')","691d4d37":"history = autoencoder.fit(X_train_vec, X_train_vec, \n          epochs=20,\n          batch_size=32,\n          validation_data=(X_test_vec, X_test_vec),\n          shuffle=True)","1fd7f04d":"plt.plot(history.history[\"loss\"], label=\"Training Loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\nplt.legend();","1f780e99":"# sample element from positive&negative class (new\/old topic)\npos_ind = X_test[X_test['target']!='comp.graphics'].sample(1).index.values[0]\nneg_ind = X_test[X_test['target']=='comp.graphics'].sample(1).index.values[0]\npos_ind, neg_ind","18b261e0":"norm_data = np.array(X_test_vec[pos_ind])\nencoded_data = autoencoder.encoder(norm_data).numpy()\ndecoded_data = autoencoder.decoder(encoded_data).numpy()\n\nabnorm_data = np.array(X_test_vec[neg_ind])\nab_encoded_data = autoencoder.encoder(abnorm_data).numpy()\nab_decoded_data = autoencoder.decoder(ab_encoded_data).numpy()","f040ea2d":"fig, ax = plt.subplots(1,2, figsize=(15,4))\nax[0].plot(norm_data[0], 'b')\nax[0].plot(decoded_data[0], 'r')\nax[0].fill_between(np.arange(MAX_FEATS), decoded_data[0], norm_data[0], color='lightcoral')\nax[0].legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\nax[0].set_title('Old Topic')\n\nax[1].plot(abnorm_data[0], 'b')\nax[1].plot(ab_decoded_data[0], 'r')\nax[1].fill_between(np.arange(MAX_FEATS), ab_decoded_data[0], abnorm_data[0], color='lightcoral')\nax[1].legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\nax[1].set_title('New Topic')\nplt.show()","e500cea7":"# splitting test set into positive&negative class\ntest_mask = X_test['target']=='comp.graphics'\npos_mask = X_test[~test_mask].index.values\nneg_mask = X_test[test_mask].index.values\npos_test, neg_test = X_test_vec[pos_mask], X_test_vec[neg_mask]","d9606750":"# distributiom of train\/test loss\nreconstructions = autoencoder.predict(X_train_vec)\ntrain_loss = tf.keras.losses.mse(reconstructions, X_train_vec)\n\nreconstructions_neg_test = autoencoder.predict(neg_test)\ntest_loss = tf.keras.losses.mse(reconstructions_neg_test, neg_test)\n\nfig, ax = plt.subplots(1,2, figsize=(15,4))\nax[0].hist(test_loss[None, :], bins=50)\nax[0].set_title(\"Test loss\")\nax[1].hist(train_loss[None,:], bins=50)\nax[1].set_title(\"Train loss\")\nplt.show()","b38df095":"# unused functions right now(\ndef predict(model, data, threshold):\n    reconstructions = model(data)\n    loss = tf.keras.losses.mae(reconstructions, data)\n    return tf.math.less(loss, threshold)\n\ndef print_stats(predictions, labels):\n    print(\"Accuracy = {:.4f}\".format(accuracy_score(labels, predictions)))\n    print(\"Precision = {:.4f}\".format(precision_score(labels, predictions)))\n    print(\"Recall = {:.4f}\".format(recall_score(labels, predictions)))\n    print(\"F1 = {:.4f}\".format(f1_score(labels, predictions)))","73411a6e":"# measuring different loss statistics,\n# through which we can range our topics\nreconstructions = autoencoder(X_test_vec)\n\nmax_loss = np.array(np.max(np.abs(reconstructions.numpy()-X_test_vec), axis=1))\nmean_loss = np.array(np.mean(np.abs(reconstructions.numpy()-X_test_vec), axis=1))\nstd_loss = np.array(np.std(np.abs(reconstructions.numpy()-X_test_vec), axis=1))\nmedian_loss = np.array(np.median(np.abs(reconstructions.numpy()-X_test_vec), axis=1))","e91a0be5":"# final metric\ntest_labels = test_mask.astype(int)\n\nroc_auc = roc_auc_score(test_labels, std_loss)\nprint(f\"ROC-AUC = {roc_auc:.4f}\")","9b89f8c1":"# here we want to check how AE can range new topics\n# across a single general topic","afc7eb56":"res_df = pd.DataFrame()\nfor wide_target in df['target_wide'].unique():\n    print(f'\\nGeneral topic: {wide_target}')\n    df_crop = df[df['target_wide']==wide_target]\n    X_train, X_test = train_test_split(df_crop, test_size=0.2, random_state=42)\n    for sub_top in X_train['target'].unique():\n        train_sub_mask = X_train['target']!=sub_top\n        X_train_sub = X_train[train_sub_mask]\n\n        X_train_sub = X_train_sub.reset_index(drop=True)\n        X_test = X_test.reset_index(drop=True)\n\n        vectorizer = TfidfVectorizer(max_df=0.15,\n                                     min_df=10,\n                                     stop_words='english',\n                                     analyzer='word',\n                                     lowercase=True,\n                                     max_features=MAX_FEATS\n                                    )\n        X_train_vec = vectorizer.fit_transform(X_train_sub['text']).todense()\n        X_test_vec = vectorizer.transform(X_test['text']).todense()\n\n        autoencoder = AnomalyDetector()\n        autoencoder.compile(optimizer='adam', loss='mse')\n        history = autoencoder.fit(X_train_vec, X_train_vec, \n                  epochs=20,\n                  batch_size=32,\n                  validation_data=(X_test_vec, X_test_vec),\n                  shuffle=True,\n                  verbose=False\n                )\n\n        reconstructions = autoencoder(X_test_vec)\n\n        std_loss = np.array(np.std(np.abs(reconstructions.numpy()-X_test_vec), axis=1))\n        test_labels = (X_test['target']==sub_top).astype(int)\n        \n        roc_auc = roc_auc_score(test_labels, std_loss)\n        print(f\"New topic: {sub_top}; ROC-AUC: {roc_auc:.4f}\")\n        tdf = pd.DataFrame({'target': [sub_top], 'target_wide':[wide_target], 'roc_auc':[roc_auc]})\n        res_df = res_df.append(tdf, ignore_index=True)","1c94eeb6":"res_df['target_crop'] = res_df['target'].apply(lambda x: '.'.join(x.split('.')[1:]))\n\nfig, ax = plt.subplots(2,2, figsize=(18,8), sharey=True)\nfor ind, cat in enumerate(res_df['target_wide'].unique()):\n    x, y = ind\/\/2, ind%2\n    sns.barplot(data=res_df[res_df['target_wide']==cat],\n            x='target_crop',\n            y='roc_auc',\n            ax=ax[x][y]\n           )\n    ax[x][y].set(xlabel=None)\n    ax[x][y].set_title(cat)","0d0b23bd":"res_df.boxplot(by='target_wide', figsize=(8,5));","ae7b189a":"# As we can see, results are quite unstable\n# The 'comp' category has the worst metrics\n# (mb due to inexistence of special topic-related words)","7fb0decd":"# ToDo\n* add text cleaning\n* tune tf-idf (num tokens, words->n-grams, max\/min df, etc.)\n* tune AE architecture (num layers, activation function...)\n* use different text representation module (tf-idf->transformers)","c8041d90":"# Importing Libraries","33e98db8":"# Modelling","4d8338e2":"# Cross-Validation Loop","df98c8a8":"# Preprocessing","6d0e4d4c":"# Pipeline on train-test split","92969b61":"# Analyze Results","690ff1d8":"# Visualisations and metrics"}}