{"cell_type":{"53b091d8":"code","3d590d57":"code","830d983f":"code","dd34c229":"code","2b82704e":"code","3f619e33":"code","472f6490":"code","2dd3de73":"code","4b203c5b":"code","28d14a9c":"code","c11e113b":"code","2380ff76":"code","8d2abb8f":"code","a352a093":"code","6c37a64f":"code","90ea7f3e":"code","b5d48274":"code","d824edfc":"code","6d5ee63d":"code","5919cccd":"code","abb60afe":"code","7c9d835f":"code","588f26ad":"code","e04168ea":"code","fd1a8e18":"code","81ee2bec":"code","109b99d8":"code","e35d0c62":"code","6456fd94":"code","bf0647ef":"code","a25371a2":"code","685df7c6":"code","08d5a191":"markdown","72a4567b":"markdown","cc217b83":"markdown","7df550d5":"markdown","595703e8":"markdown","9344414c":"markdown","68b91bf5":"markdown","5f141631":"markdown","3c3c9039":"markdown","0f35e048":"markdown","36e86ef5":"markdown","c5774b92":"markdown","6c1df2f1":"markdown","a5baf8be":"markdown","0074dadb":"markdown","070f8036":"markdown","2dde9818":"markdown","cbb29056":"markdown","3e752c96":"markdown","42a4c524":"markdown","6efea622":"markdown"},"source":{"53b091d8":"!pip install lyft-dataset-sdk --quiet\n# Load the dataset\n# Adjust the dataroot parameter below to point to your local dataset path.\n# The correct dataset path contains at least the following four folders (or similar): images, lidar, maps, v1.0.1-train\n!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train_images images\n!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train_maps maps\n!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train_lidar lidar","3d590d57":"import os\nimport json\nfrom pprint import pprint\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation, rc\nfrom IPython.display import HTML\n\nfrom lyft_dataset_sdk.lyftdataset import LyftDataset","830d983f":"BASE_PATH = '\/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/'","dd34c229":"# Thanks to Nanashi!\nlyft_data = LyftDataset(\n    data_path='.',\n    json_path='\/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train_data', \n    verbose=True\n)","2b82704e":"train = pd.read_csv('\/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train.csv')\nsub = pd.read_csv('\/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/sample_submission.csv')\nprint(train.shape)\ntrain.head()","3f619e33":"os.listdir(BASE_PATH + \"\/train_images\")[:10]","472f6490":"os.listdir(BASE_PATH + \"\/train_data\")","2dd3de73":"with open(BASE_PATH + '\/train_data\/sample_data.json') as f:\n    data_json = json.load(f)\n\nprint(\"There are\", len(data_json), \"records in sample_data.json\")\n\nprint(\"\\nBelow is a record containing lidar data:\")\npprint(data_json[0])\n\nprint('\\n This one contains information about image data:')\npprint(data_json[2])","4b203c5b":"with open(BASE_PATH + '\/train_data\/scene.json') as f:\n    scene_json = json.load(f)\n\nprint(\"There are\", len(scene_json), \"records in sample_data.json\")\n\npprint(scene_json[0])","28d14a9c":"lidar_data = []\nimage_data = []\n\nfor record in data_json:\n    if record['fileformat'] == 'jpeg':\n        image_data.append(record)\n    else:\n        lidar_data.append(record)","c11e113b":"lidar_df = pd.DataFrame(lidar_data)\nimage_df = pd.DataFrame(image_data)\n\nprint(lidar_df.shape)\nprint(image_df.shape)","2380ff76":"lidar_df.head()","8d2abb8f":"image_df.head()","a352a093":"image_df['host'] = image_df['filename'].apply(lambda st: st.strip('images\/host-').split('_')[0])\nimage_df['cam'] = image_df['filename'].apply(lambda st: st.split('_')[1])\nimage_df['timestamp'] = image_df['filename'].apply(lambda st: st.split('_')[2].strip('.jpeg'))","6c37a64f":"image_df.head()","90ea7f3e":"image_df.to_csv(\"sample_data_images.csv\")\nlidar_df.to_csv(\"lidar_data_images.csv\")","b5d48274":"image_df['host'].value_counts()","d824edfc":"image_df['cam'].value_counts()","6d5ee63d":"def display_host_sample(host, n_images, jumps=1):\n    cams = list(sorted(image_df['cam'].unique()))\n    \n    fig, axs = plt.subplots(\n        n_images, len(cams), figsize=(3*len(cams), 3*n_images), \n        sharex=True, sharey=True, gridspec_kw = {'wspace':0.1, 'hspace':0.1}\n    )\n    \n    for i in range(n_images):\n        for c, cam in enumerate(cams):\n            if i == 0:\n                axs[i, c].set_title(cam)\n            \n            mask1 = image_df.cam == cam\n            mask2 = image_df.host == host\n            image_path = image_df[mask1 & mask2]\n            image_path = image_path.sort_values('timestamp')['filename'].iloc[i*jumps]\n            \n            img = cv2.imread(BASE_PATH + '\/train_' + image_path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (200, 200))\n            \n            axs[i, c].imshow(img)\n            axs[i, c].axis('off')","5919cccd":"display_host_sample('017', 5, jumps=1)","abb60afe":"display_host_sample('009', 5, jumps=5)","7c9d835f":"display_host_sample('012', 5, jumps=10)","588f26ad":"def generate_next_token(scene):\n    scene = lyft_data.scene[scene]\n    sample_token = scene['first_sample_token']\n    sample_record = lyft_data.get(\"sample\", sample_token)\n    \n    while sample_record['next']:\n        sample_token = sample_record['next']\n        sample_record = lyft_data.get(\"sample\", sample_token)\n        \n        yield sample_token\n\ndef animate_images(scene, frames, pointsensor_channel='LIDAR_TOP', interval=1):\n    cams = [\n        'CAM_FRONT',\n        'CAM_FRONT_RIGHT',\n        'CAM_BACK_RIGHT',\n        'CAM_BACK',\n        'CAM_BACK_LEFT',\n        'CAM_FRONT_LEFT',\n    ]\n\n    generator = generate_next_token(scene)\n\n    fig, axs = plt.subplots(\n        2, len(cams), figsize=(3*len(cams), 6), \n        sharex=True, sharey=True, gridspec_kw = {'wspace': 0, 'hspace': 0.1}\n    )\n    \n    plt.close(fig)\n\n    def animate_fn(i):\n        for _ in range(interval):\n            sample_token = next(generator)\n            \n        for c, camera_channel in enumerate(cams):    \n            sample_record = lyft_data.get(\"sample\", sample_token)\n\n            pointsensor_token = sample_record[\"data\"][pointsensor_channel]\n            camera_token = sample_record[\"data\"][camera_channel]\n            \n            axs[0, c].clear()\n            axs[1, c].clear()\n            \n            lyft_data.render_sample_data(camera_token, with_anns=False, ax=axs[0, c])\n            lyft_data.render_sample_data(camera_token, with_anns=True, ax=axs[1, c])\n            \n            axs[0, c].set_title(\"\")\n            axs[1, c].set_title(\"\")\n\n    anim = animation.FuncAnimation(fig, animate_fn, frames=frames, interval=interval)\n    \n    return anim","e04168ea":"%%time\nanim = animate_images(scene=0, frames=100, interval=1)\nHTML(anim.to_jshtml(fps=8))","fd1a8e18":"anim = animate_images(scene=10, frames=100, interval=1)\nHTML(anim.to_jshtml(fps=8))","81ee2bec":"anim = animate_images(scene=50, frames=100, interval=1)\nHTML(anim.to_jshtml(fps=8))","109b99d8":"anim = animate_images(scene=100, frames=100, interval=1)\nHTML(anim.to_jshtml(fps=8))","e35d0c62":"def animate_lidar(scene, frames, pointsensor_channel='LIDAR_TOP', with_anns=True, interval=1):\n    generator = generate_next_token(scene)\n\n    fig, axs = plt.subplots(1, 1, figsize=(8, 8))\n    plt.close(fig)\n\n    def animate_fn(i):\n        for _ in range(interval):\n            sample_token = next(generator)\n        \n        axs.clear()\n        sample_record = lyft_data.get(\"sample\", sample_token)\n        pointsensor_token = sample_record[\"data\"][pointsensor_channel]\n        lyft_data.render_sample_data(pointsensor_token, with_anns=with_anns, ax=axs)\n\n    anim = animation.FuncAnimation(fig, animate_fn, frames=frames, interval=interval)\n    \n    return anim","6456fd94":"%%time\nanim = animate_lidar(scene=0, frames=100, interval=1)\nHTML(anim.to_jshtml(fps=8))","bf0647ef":"%%time\nanim = animate_lidar(scene=10, frames=100, interval=1)\nHTML(anim.to_jshtml(fps=8))","a25371a2":"anim = animate_lidar(scene=50, frames=100, interval=1)\nHTML(anim.to_jshtml(fps=8))","685df7c6":"%%time\nanim = animate_lidar(scene=100, frames=100, interval=1)\nHTML(anim.to_jshtml(fps=8))","08d5a191":"## Taking a look at train_images","72a4567b":"# Creating Useful Files\n\n## Converting sample_data.json to dataframes","cc217b83":"# Animating LIDAR","7df550d5":"We can safely store the two types into separate dataframes:","595703e8":"## Augment image_df\nFinally, we will augment the image_df with some information about its host and camera:","9344414c":"# Animating the images\n\nNote: Please use the \"-\" button to slow down the animation speed.\n\nUnhide below for the definition of `animate_images(scene, frames, pointsensor_channel='LIDAR_TOP', interval=1)`","68b91bf5":"### Host 009, skip 5 frames","5f141631":"### sample_data.json\nLet's load `sample_data.json`, since it contains information about our training data.","3c3c9039":"Let's store the dataframes we just created:","0f35e048":"Unhide below to see the definition of `animate_lidar(scene, frames, pointsensor_channel='LIDAR_TOP', with_anns=True, interval=1)`.","36e86ef5":"# Exploring images","c5774b92":"## Exploring JSON files","6c1df2f1":"# About this kernel\n\nThis kernel takes a look at the dataset for the Lyft competition, with some visual exploration (display images, animations, etc.). I also convert some of the JSON files into CSV, so feel free to use this kernel output as supplementary data.\n\n## Updates\n\nV3: Added Animations! Please go to the \"Animating the images\" section at the end!\n\n## References\n\n* Starter Devkit Lyft3D: https:\/\/www.kaggle.com\/jesucristo\/starter-devkit-lyft3d\n* Devkit for the public 2019 Lyft Level 5 AV Dataset: https:\/\/github.com\/lyft\/nuscenes-devkit","a5baf8be":"It seems like there is only 12 hosts, of which there are up to 6 cameras. We also notice that each of the cameras are getting an equal amount of pictures (therefore there are little to no defects, or pictures being \"thrown away\"). We also notice that each host contains a different number of pictures, ranging from 882 to 44k. This could be caused by some of the cars being used on the road for longer than others.","0074dadb":"Thanks for [Nanashi's notebook](https:\/\/www.kaggle.com\/jesucristo\/starter-devkit-lyft3d) for going over the setup for the sdk!","070f8036":"### Host 012, skips 10 frames","2dde9818":"## Displaying sample image by host\n\nIn the sample below, we display all the images in a given timeframe (10 pictures taken from the first timestamp), in the order they were taken. Each column are the images given by each camera. We do so for a few hosts.\n\nUnhide the cell below to see the definition of `display_host_sample(host, n_images, jumps=1)`.","cbb29056":"### Exploring scene.json","3e752c96":"# Preliminary Exploration\n\n## Load CSV files","42a4c524":"### Host 017, no frame skip","6efea622":"Let's take a look at what they look like:"}}