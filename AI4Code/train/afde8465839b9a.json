{"cell_type":{"42de4857":"code","6614c78d":"code","be392e84":"code","06e787f1":"code","2dfc7a0e":"code","46c7c192":"code","dc8cbca2":"code","01b5abaa":"code","452d4aa5":"code","bf362efb":"code","a47c2f30":"code","84401179":"code","5c982a02":"code","872697cd":"code","46a7ec3c":"code","662d5c2d":"code","cae9b2dd":"code","783b0c73":"code","9438c9a6":"code","8cb70dfc":"code","1f6762ea":"code","3be2328e":"code","1ee353a5":"code","11664916":"code","1a1b9fbd":"code","0e0dbbe7":"code","c0de2358":"code","f26f89c8":"code","ff2f3866":"code","fd2ab0e8":"code","a39d43cf":"code","1008ea79":"code","4c00da36":"code","8ee5dc44":"code","9c3e5fce":"code","fc4c40ef":"code","ea1fb049":"code","eb0e0d0d":"code","2363ed17":"markdown","6a68388c":"markdown","ac0393ce":"markdown","76791403":"markdown","e158a698":"markdown","84176cbc":"markdown","dbf9b55e":"markdown","8798f6d2":"markdown","07ac1ed5":"markdown","4b6c07cb":"markdown","c48c029a":"markdown","6e203cf3":"markdown","169cd180":"markdown","463e8e66":"markdown","f4945b1f":"markdown","01e3acec":"markdown","4b19dbd9":"markdown","04bf0b79":"markdown","e348137b":"markdown","92d5f869":"markdown","6f860088":"markdown","348dc2fb":"markdown","24a70bbb":"markdown"},"source":{"42de4857":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6614c78d":"data_df=pd.read_csv(\"\/kaggle\/input\/boston-house-prices\/housing.csv\")","be392e84":"data_df.head()","06e787f1":"column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndata_df=pd.read_csv('\/kaggle\/input\/boston-house-prices\/housing.csv', header=None, delim_whitespace=True, names=column_names) #, delimiter=r\"\\s+\")","2dfc7a0e":"data_df.head()","46c7c192":"cat_features=data_df.select_dtypes(exclude=np.number).columns\ncont_features=data_df.select_dtypes(include=[np.number,'float64','int64']).columns","dc8cbca2":"data_df.columns","01b5abaa":"data_df.info()","452d4aa5":"data_df.describe()","bf362efb":"data_df.head(20)","a47c2f30":"import seaborn as sns","84401179":"import matplotlib.pyplot as plt","5c982a02":"#plt.pie(data_df['CHAS'].value_counts(), labels=data_df['CHAS'].value_counts().index);\nplt.pie(data_df['CHAS'].value_counts(), labels=data_df['CHAS'].value_counts());","872697cd":"cont_features = data_df.drop('CHAS', axis=1)\n\n\n\ndata_df.MEDV=data_df.MEDV.astype('int64')\n\nsns.countplot(data_df['MEDV'])","46a7ec3c":"sns.distplot(data_df['MEDV'],kde=False, bins=40)#, kde=True)","662d5c2d":"corrmat = data_df.corr(method='pearson')\nf, ax = plt.subplots(figsize=(12, 10))\nsns.heatmap(corrmat, ax=ax, cmap=\"YlGnBu\", linewidths=0.1, annot=True)","cae9b2dd":"sns.set(rc={'figure.figsize':(20,12)})\nsns.boxplot(x=data_df['MEDV'],y=data_df['LSTAT'],showfliers=False)\n","783b0c73":"#ax = sns.distplot(data_df[col], kde=True)","9438c9a6":"import numpy as np\nimport seaborn as sns\ncont_dummy=cont_features#.drop('store_and_fwd_flag')\nplt.figure(figsize=(20,16))\nfor index, col in enumerate(cont_dummy, start=1):\n    plt.subplot(3,5,index)\n    plt.title(col)\n    ax = sns.distplot(data_df[col], kde=True)\n    #kde_kws={'bw':0.05})\n    #ax.set_yscale('log')","8cb70dfc":"import scipy\nfrom scipy.stats import norm, kurtosis\ndata = norm.rvs(size=1000, random_state=3)\nfrom scipy.stats import kurtosis\n\n\nfor index, col in enumerate(cont_dummy, start=1):\n    print(\"Kurtosis untuk \",col,\": \",kurtosis(data_df[col]))\n    print(\"Skew untuk \",col,\": \",scipy.stats.skew(data_df[col]))\n    print(\"stdev untuk \",col,\": \",np.std(data_df[col]))\n    print(\"\")","1f6762ea":"import numpy as np\nimport seaborn as sns\ncont_dummy=cont_features#.drop('store_and_fwd_flag')\nplt.figure(figsize=(20,16))\nfor index, col in enumerate(cont_dummy, start=1):\n    plt.subplot(3,5,index)\n    plt.title(col)\n    ax = sns.boxplot(data_df[col])\n    #kde_kws={'bw':0.05})\n    #ax.set_yscale('log')","3be2328e":"corrmat = data_df.corr(method='pearson')\nf, ax = plt.subplots(figsize=(12, 10))\nsns.heatmap(corrmat, ax=ax, cmap=\"YlGnBu\", linewidths=0.1, annot=True)","1ee353a5":"#sns.pairplot(data_df)","11664916":"import numpy as np\nimport seaborn as sns\ncont_dummy=cont_features#.drop('store_and_fwd_flag')\nplt.figure(figsize=(20,16))\nfor index, col in enumerate(cont_dummy, start=1):\n    plt.subplot(3,5,index)\n    plt.title(col)\n    ax = sns.boxplot(data_df[col])\n    #kde_kws={'bw':0.05})\n    #ax.set_yscale('log')","1a1b9fbd":"data_df.isnull().sum()","0e0dbbe7":"#Q1 = data_df.quantile(0.25)\n#Q3 = data_df.quantile(0.75)\n#IQR = Q3 - Q1\n\n#data_df = data_df[~((data_df < (Q1 - 1.5 * IQR)) |(data_df > (Q3 + 1.5 * IQR))).any(axis=1)]\n#data_df.shape","c0de2358":"from scipy import stats\nimport numpy as np\nz = np.abs(stats.zscore(data_df))\nprint(z)\n\nthreshold = 3\n#print(np.where(z > 3))\n\n\n\n#print(IQR)\n#print(data_df < (Q1 - 1.5 * IQR)) |(data_df > (Q3 + 1.5 * IQR))\n\ndata_df2 = data_df[(z < 3).all(axis=1)]\n\nprint(data_df2.shape)\n\ndata_df2.head()","f26f89c8":"\"\"\"\nAnother way to handling outliers\n\ndef get_lower_upper_bound(x):\n    q1 = np.percentile(x, 25)\n    q3 = np.percentile(x, 75)\n    iqr = q3-q1\n    lower_bound = q1 - (iqr * 1.5)\n    upper_bound = q3 + (iqr * 1.5)\n    return lower_bound, upper_bound\n\ndef get_outliers_iqr(x):\n    lower_bound, upper_bound = get_lower_upper_bound(x)\n    return x[np.where((x > upper_bound) | (x < lower_bound))]\n\ndef get_inliers_iqr(x):\n    lower_bound, upper_bound = get_lower_upper_bound(x)\n    return x[np.where((x < upper_bound) & (x > lower_bound))]\n\n\n#handling outliers\ndef ub(s):\n    iqr = (np.quantile(s, 0.75))-(np.quantile(s, 0.25))\n    upper_bound = np.quantile(s, 0.75)+(1.5*iqr)\n    return upper_bound\ndef lb(s):\n    iqr = (np.quantile(s, 0.75))-(np.quantile(s, 0.25))\n    lower_bound = np.quantile(s, 0.25)-(1.5*iqr)\n    return lower_bound\n# function cap\ndef cap(s):\n    s = np.where(s > ub(s), np.median(s), s)\n    s = np.where(s < lb(s), np.median(s), s)\n    return s\n    \nget_outliers_iqr(data_df.values)\n\ndata_df_2 = pd.DataFrame(get_inliers_iqr(data_df.values))     \n\ndata2 =pd.DataFrame(cap(data_df))\ndata2.head()\n\n# using Z Score\nnilai_z = np.abs(scipy.stats.zscore(data_df))\n# outliers_df = pd.DataFrame(np.where(outliers>3))\noutliers_df = data_df[(nilai_z>2).all(axis=1)]\noutliers_df.head()\n\n\"\"\"","ff2f3866":"#from sklearn.feature_extraction import DictVectorizer\n#vec = DictVectorizer(sparse=False, dtype=int)\n#vec.fit_transform(data)","fd2ab0e8":"# Template One-Hot Encoding\n\n#categorical_columns = train_df.select_dtypes(exclude=np.number).columns\n#df2 = pd.get_dummies(data=train_df, prefix=categorical_columns, drop_first=True)","a39d43cf":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nXGBReg=xgb.XGBRegressor()\n\n\nX=data_df.drop(\"MEDV\",axis=1)\ny=data_df['MEDV']\n\n\nresults = []\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n\n","1008ea79":"import shap\nimport lightgbm as lgb\ntrain_data=lgb.Dataset(X_train,label=y_train)\nparams = {'learning_rate':0.001}\nmodel= lgb.train(params, train_data, 100)\ny_pred=model.predict(X_test)\nfrom sklearn.metrics import mean_squared_error\nrmse=mean_squared_error(y_pred,y_test)**0.5\nmodel.params[\"objective\"] = \"regression\"\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X)\nshap.summary_plot(shap_values, X)\n\nprint(mean_squared_error(y_pred,y_test))","4c00da36":"import xgboost\nimport shap\nfrom sklearn.metrics import mean_squared_error\n#\n\nfor i, col in enumerate([LinearRegression(),Lasso(),KNeighborsRegressor(),ElasticNet(),GradientBoostingRegressor(),RandomForestRegressor(),DecisionTreeRegressor(),XGBReg,AdaBoostRegressor()]):\n    print(col)\n    pipeModel=Pipeline([('scaler', StandardScaler()), ('model', col)])\n    \n    # the benefit of using K-Fold is that we could calculate the cross validation value using some of the methods of scoring \n    kfold = KFold(n_splits=10, shuffle=True, random_state=0)\n    cv_results = cross_val_score(pipeModel, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n\n    pipeModel.fit(X_train, y_train)\n    # this is the scaled LR\n    print('Skor untuk metoda',col,':', pipeModel.score(X_test, y_test))\n    # the mean result (10 data) of negative mean squared error\n    print('Skor untuk metoda',col,'menggunakan cross_val_score:', cv_results.mean())\n    y_pred=pipeModel.predict(X_test)\n    print(mean_squared_error(y_test, y_pred))\n\n    #print(col)\n    print('')\n    if col==GradientBoostingRegressor():\n        explainer = shap.TreeExplainer(col)\n        shap_values = explainer.shap_values(X)\n        shap.summary_plot(shap_values, X)\n","8ee5dc44":"from sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import GradientBoostingRegressor\nparameters = {'learning_rate': [0.1,0.3,1],\n              'subsample'    : [1.0, 0.9, 0.5],\n              'n_estimators' : [50,100,200],\n              'max_depth'    : [4,6,8]\n             }\n#pipeModel=Pipeline([('scaler', StandardScaler()), ('model', GradientBoostingRegressor())])\n\nkfold = KFold(n_splits=10, shuffle=True, random_state=0)\n\nclf=GridSearchCV(GradientBoostingRegressor(),param_grid=parameters,cv=kfold, refit=True, n_jobs=-1, verbose = 1, scoring='neg_mean_squared_error')\n\n#grid_GBR = GridSearchCV(estimator=pipeModel, param_grid = parameters, cv = kfold, n_jobs=-1, verbose = 1)\nclf.fit(X_train, y_train)\ny_pred=clf.predict(X_test)\nprint(mean_squared_error(y_test, y_pred))\nprint(clf.best_params_)\n\n\n","9c3e5fce":"clf.score(X_test, y_test)","fc4c40ef":"\"\"\"\nFeature Importances for Lasso:\npipeModel.named_steps[\"model\"].coef_.flatten()\n\n\"\"\"","ea1fb049":"\"\"\"\n\nfrom sklearn.ensemble import AdaBoostClassifier\nmodel = AdaBoostClassifier(random_state=1)\nmodel.fit(x_train, y_train)\nmodel.score(x_test,y_test)\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodel= GradientBoostingClassifier(learning_rate=0.01,random_state=1)\nmodel.fit(x_train, y_train)\nmodel.score(x_test,y_test)\n\nimport xgboost as xgb\nmodel=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\nmodel.fit(x_train, y_train)\nmodel.score(x_test,y_test)\n\nimport lightgbm as lgb\ntrain_data=lgb.Dataset(x_train,label=y_train)\n#define parameters\nparams = {'learning_rate':0.001}\nmodel= lgb.train(params, train_data, 100) \ny_pred=model.predict(x_test)\n\nfrom catboost import CatBoostClassifier\nmodel=CatBoostClassifier()\ncategorical_features_indices = np.where(df.dtypes != np.float)[0]\nmodel.fit(x_train,y_train,cat_features=([ 0,  1, 2, 3, 4, 10]),eval_set=(x_test, y_test))\nmodel.score(x_test,y_test)\n\"\"\"","eb0e0d0d":"\"\"\"\nfrom sklearn.ensemble import AdaBoostRegressor\nmodel = AdaBoostRegressor()\nmodel.fit(x_train, y_train)\nmodel.score(x_test,y_test)\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nmodel= GradientBoostingRegressor()\nmodel.fit(x_train, y_train)\nmodel.score(x_test,y_test)\n\nimport xgboost as xgb\nmodel=xgb.XGBRegressor()\nmodel.fit(x_train, y_train)\nmodel.score(x_test,y_test)\n\nimport shap\nimport lightgbm as lgb\ntrain_data=lgb.Dataset(X_train,label=y_train)\nparams = {'learning_rate':0.001}\nmodel= lgb.train(params, train_data, 100)\ny_pred=model.predict(X_test)\nfrom sklearn.metrics import mean_squared_error\nrmse=mean_squared_error(y_pred,y_test)**0.5\nmodel.params[\"objective\"] = \"regression\"\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X)\nshap.summary_plot(shap_values, X)\n\nfrom catboost import CatBoostRegressor\nmodel=CatBoostRegressor()\ncategorical_features_indices = np.where(df.dtypes != np.float)[0]\nmodel.fit(x_train,y_train,cat_features=([ 0,  1, 2, 3, 4, 10]),eval_set=(x_test, y_test))\nmodel.score(x_test,y_test)\n\"\"\"","2363ed17":"z_scores = scipy.stats.zscore(data_df)\n\nprint(z_scores)\n\nabs_z_scores = np.abs(z_scores)\nfiltered_entries = (abs_z_scores < 3).all(axis=1)\nnew_df = data_df[filtered_entries]\n\nimport numpy as np\nimport seaborn as sns\n#cont_dummy=cont_features#.drop('store_and_fwd_flag')\nplt.figure(figsize=(20,16))\nfor index, col in enumerate(new_df.columns, start=1):\n    plt.subplot(3,5,index)\n    plt.title(col)\n    ax = sns.boxplot(new_df[col])\n","6a68388c":"The kurtosis value is a measure of the sharpness of the curve to the normal distribution curve. When the value of kurtosis is zero, the curve type is a normal distribution curve. Meanwhile, when a kurtosis value is positive, then the shape of the curves and shoulders is sharper than the normal distribution. Conversely, when the value of kurtosis is negative, the shape of the crest and shoulders of the curve is more gentle compared to the normal distribution.","ac0393ce":"# Important Notes Below","76791403":"# Business Understanding\n\n## Business Problems\n\n* availability of land\n* property is one of the favorite investments\n\n## Objectives\nEDA\n* How much it costs for a house in Boston?\n* What are the variables that could be influence the price of house of Boston\n* Where are the areas that are cheaper?\nML Model\n* Mean Squared Error as smallest as possible\n\nMean Squared Error is calculated with: \n\n\n\n## Information Needed\n\nEach record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. The attributes are de\ufb01ned as follows (taken from the UCI Machine Learning Repository): \n\n* CRIM: per capita crime rate by town\n\n* ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n\n* INDUS: proportion of non-retail business acres per town\n\n* CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n* NOX: nitric oxides concentration (parts per 10 million)\n\n* RM: average number of rooms per dwelling\n\n* AGE: proportion of owner-occupied units built prior to 1940\n\n* DIS: weighted distances to \ufb01ve Boston employment centers\n\n* RAD: index of accessibility to radial highways\n\n* TAX: full-value property-tax rate per $10,000\n\n* PTRATIO: pupil-teacher ratio by town \n\n* B: 1000(Bk\u22120.63)2 where Bk is the proportion of blacks by town \n\n* LSTAT: % lower status of the population\n\n* MEDV: Median value of owner-occupied homes in $1000s\n\nWe can see that the input attributes have a mixture of units.\n\n## Comments\n*Will be added.*","e158a698":"# Boston House Price based on CRISP-DM Framework","84176cbc":"Needs to compare the performance of ML model due to the absence and the presence of outliers.","dbf9b55e":"## Step 3: Detect Aberrant and Missing Values","8798f6d2":"Strategi Feature Engineering\n* Menambah atau membuang feature, pilih feature yang signifikan mempengaruhi nilai akhir.\n \n* Menggabungkan beberapa fitur menjadi 1 fitur, agar model lebih simple. Contoh, ukuran panjang dan lebar tanah, maka cukup digabungkan menjadi 1 fitur saja, yaitu luas tanah yang merupakan perhitungan dari panjang x lebar yang digunakan.\n\n* Binning, mengganti angka numerical menjadi kategori yang lebih luas, agar model lebih simple. Contoh, data ukuran kolam renang, diubah menjadi boolean, True jika ada kolam renang, dan False jika tidak ada.\n\n* One-hot encoding, cara mempresentasikan data tipe kategori sebagai numeric dimana dapat dipahami model machine learning.\n\n***Feature Engineering Tidak Dilakukan karena Tidak Ada Data Bertipe Kategorikal***","07ac1ed5":"From the plot above, it can be seen that the majority of Bostonians live not near the river. This is inferred from data where 35 to 471 people are near rivers in Boston.","4b6c07cb":"## Step 5: Feature Engineering","c48c029a":"## Hypertuning Gradient Boosting Regressor","6e203cf3":"Dari heatmap diatas, yang memiliki korelasi negatif tertinggi terhadap MEDV adalah LSTAT, PTRATIO, TAX, INDUS. Sedangkan yang memiliki korelasi positif tertinggi terhadap MEDV adalah RM.\n\nSementara itu, dari data korelasi diatas, luas tanah industri INDUS berbanding lurus dengan jumlah polutan atau konsentrasi NOX di udara. Selain itu, proporsi unit yang ditempati pemilik yang dibangun sebelum 1940 AGE berbanding lurus dengan konsentrasi NOX di udara. Secara implisit, tempat tinggal orang yang sudah memiliki rumah sebelum tahun 1940 (AGE) ternyata memiliki jarak yang jauh dari 5 tempat pusat bisnis di boston (DIS). Status sosialpun (LSTAT) ternyata memiliki hubungan positif terhadap kepemilikan rumah sebelum tahun 1940 (AGE).\n\nLuasan kawasan industri (INDUS) berbanding lurus dengan pajak yang harus dibayarkan oleh pemilik (TAX). Selain itu, harga rumah yang memiliki jarak aksesibilitas dekat dengan jalan tol (RAD), memiliki harga yang lebih rendah (TAX) tetapi perlu dijabarkan lebih lanjut dikhawatirkan terdapat data outlayer yang dapat mempengaruhi hipotesis ini.\n\nSelain itu, jumlah NOX dan TAX memiliki perbandingan lurus. Ini mungkin terjadi mengingat ada hubungan dekat antara NOX dan dan INDUS. Jarak rumah terhadap jalan tol berbanding terbalik dengan kawasan industri INDUS, konsentrasi polutan NOX, serta rumah yang dimiliki sejak sebelum 1940.","169cd180":"## Step 2: Bi and Multivariate Analysis","463e8e66":"***Hipotesis: one-hot encoding tidak diperlukan karena tidak ada categorical columns***","f4945b1f":"Seems that Gradient Boosting Regressor has the best result compared to the others.","01e3acec":"# Data Understanding\n## **STEP 1: UNIVARIATE ANALYSIS**","4b19dbd9":"# Model Training","04bf0b79":"# Data Preparation","e348137b":"From the heatmap above, the ones with the highest negative correlation to MEDV are LSTAT, PTRATIO, TAX, INDUS. Meanwhile, the one with the highest positive correlation to MEDV was RM.\n\nMeanwhile, from the correlation data above, the industrial land area of \u200b\u200bINDUS is directly proportional to the number of pollutants or NOX concentrations in the air. In addition, the proportion of owner-occupied units built prior to 1940 AGE is directly proportional to the NOX concentration in the air. Implicitly, the residence of people who already owned a house before 1940 (AGE) turns out to be far from 5 business centers in Boston (DIS). Even social status (LSTAT) has a positive relationship with home ownership before 1940 (AGE).\n\nThe area of the industrial estate (INDUS) is directly proportional to the tax that must be paid by the owner (TAX). In addition, the price of houses that have an accessibility distance close to toll roads (RAD), has a lower price (TAX) but it is necessary to further elaborate it is feared that there is outlayer data that could influence this hypothesis.\n\nApart from that, the NOX and TAX numbers have a straight comparison. This is possible given the close relationship between NOX and and INDUS. The distance between the house and the toll road is inversely proportional to the INDUS industrial area, the concentration of NOX pollutants, and houses owned since before 1940.","92d5f869":"From the MEDV data distribution above, it can be seen that the MEDV mode value is in the range of about 23 with positive skewness.","6f860088":"Dari data diatas, dapat dilihat bahwa nilai MEDV dari 5-29 secara umum memiliki variansi yang besar. Sementara itu, secara umum, nilai MEDV dari 30 hingga 50 memiliki variansi yang lebih lebih kecil dari data yang lain. Dari grafik pula, dapat dilihat bahwa korelasi antara LSTAT berbanding MEDV memiliki hubungan terbalik.","348dc2fb":"From the data above, it can be seen that there are several data that have a high degree of variability which can be influenced by outliers: B, LSTAT, MEDV, CRIM, ZN, RM.","24a70bbb":"## Step 4: Detect Outliers"}}