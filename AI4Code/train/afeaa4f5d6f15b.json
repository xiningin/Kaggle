{"cell_type":{"28252b14":"code","4fde9e16":"code","ffbe3332":"code","05df4bba":"code","8e988164":"code","f2934e14":"code","8707d691":"code","577654db":"code","8594d58a":"code","0280e08a":"code","c5c23ee5":"markdown","2333974f":"markdown","998b6aab":"markdown","7e36fae7":"markdown","2c924ecb":"markdown","55f2ec7b":"markdown","aaa56992":"markdown","1d20a7a9":"markdown","3c87d325":"markdown"},"source":{"28252b14":"import pandas as pd\nimport numpy as np\n\ndataTrain = pd.read_csv('..\/input\/train.csv', encoding='iso-8859-1')\ndataTest = pd.read_csv('..\/input\/test.csv', encoding='iso-8859-1')\nfullData = pd.concat([dataTrain, dataTest], ignore_index=True, sort=False)\n\n","4fde9e16":"print(dataTrain.shape)\nprint(dataTest.shape)\n","ffbe3332":"for column in fullData.columns:\n    if fullData[column].isna().sum() > 0:\n        print(column)\n        print('N\u00famero de missings en la variable: %d' % (fullData[column].isna().sum()))\n        print()\n","05df4bba":"fullData['EDAD'] = fullData['EDAD'].fillna(dataTrain['EDAD'].mean())\nfullData['PROVEEDOR'] = fullData['PROVEEDOR'].fillna('SIN MAIL')\nfullData['DOMINIO'] = fullData['DOMINIO'].fillna('SINDOMINIO')\nfullData['R_VOCCONS_MISSING'] = fullData['R_VOCCONS'].isna().astype(int)\nfullData['R_VOCCONS'] = fullData['R_VOCCONS'].fillna(0)\nfullData['LON_MAIL1_MISSING'] = fullData['LON_MAIL1'].isna().astype(int)\nfullData['LON_MAIL1'] = fullData['LON_MAIL1'].fillna(0)","8e988164":"columnsToDrop = [column for column in dataTrain.columns if dataTrain[column].nunique() == 1]\nprint(columnsToDrop)\nfullData = fullData.drop(columnsToDrop, axis=1)","f2934e14":"for column in fullData.columns:\n    if fullData[column].dtype == 'object':\n        print(column, dataTrain[column].nunique())","8707d691":"for column in fullData.columns.drop(['ID', 'FRAUDE', 'HORA']): \n    if (fullData[column].dtype == 'object'):\n        if (dataTrain[column].nunique() < 40):\n            fakeDf = pd.get_dummies(fullData[column], prefix=column)     \n            fullData = fullData.drop(column, axis=1)\n            fullData = fullData.join(fakeDf)\n        else: \n            fakeDf = dataTrain.groupby(column)['ID'].count().reset_index()\n            fakeDf = fakeDf[fakeDf['ID'] > 500]\n            listValues = fakeDf[column].tolist()\n            fullData[column].loc[~(fullData[column].isin(listValues))] = 'RESTO'\n            fakeDf2 = pd.get_dummies(fullData[column], prefix=column)     \n            fullData = fullData.drop(column, axis=1)\n            fullData = fullData.join(fakeDf2)","577654db":"fullData['HORA'] = fullData['HORA'].str.partition(':')\nfullData['HORA'] = fullData['HORA'].astype(int)","8594d58a":"corrMatrix = fullData[~(fullData['FRAUDE'].isna())].corr().abs()\nupperMatrix = corrMatrix.where(np.triu(\n                               np.ones(corrMatrix.shape),\n                               k=1).astype(np.bool))\ncorrelColumns = [c for c in upperMatrix.columns\n                 if any(upperMatrix[c] > 0.98)]\nprint(correlColumns)\nfullData = fullData.drop(correlColumns, axis=1)","0280e08a":"fullData.to_csv('fullData_preProcessed.csv', index=False)","c5c23ee5":"Ahora tenemos los dos datasets (train y test) sin ning\u00fan missing, con lo que minimizamos problemas con algunos de los m\u00e9todos que queramos utilizar despu\u00e9s.\n\n## Eliminaci\u00f3n de variables constantes\n\nSe puede dar el caso de variables que solo tengan un \u00fanico valor. Estas variables son incapaces de aportar nada al modelo en la fase de training, as\u00ed que las eliminamos de la muestra.","2333974f":"# Data preprocessing\n\nEl objetivo de este notebook es mostrar algunos an\u00e1lisis \/ tratamientos iniciales que es bueno hacer a las variables de nuestro data set antes de iniciar la fase de modelizaci\u00f3n. Quede claro que son \"algunos\" an\u00e1lisis, hay inifinitud de cosas que se pueden hacer y no se muestran aqu\u00ed, pero al menos con esto podemos empezar a trabajar.\n\n## Carga de datos\n\nCargamos librer\u00edas y los datos train y test y creamos la tabla conjunta fullData","998b6aab":"## Eliminaci\u00f3n de variables correlacionadas\n\nPuede ser que en nuestro dataset tengamos variables muy correlacionadas entre s\u00ed. En ese caso, tener las dos variables en nuestra muestra aporta relativamente poco. Imaginad el caso en que las variables est\u00e1n perfectamente correlacionadas, es decir, que son la misma variable: los modelos (cualquiera que se use) no podr\u00e1n sacar m\u00e1s informaci\u00f3n de una \n\nPara hacer esto, pondremos un umbral (arbitrario) de 0.98, es decir, los pares de variables que tengan una correlaci\u00f3n entre s\u00ed del 0.98 o m\u00e1s quedar\u00e1n reducidas a una sola variable. \n\n\n","7e36fae7":"## Check de missings\n\nMiramos para cada variable el volumen de nulos que existen para evaluar si se tira la variable (si hay excesivos nulos) o si sustituimos los missings por alg\u00fan valor","2c924ecb":"Para las variables con pocas respuestas categ\u00f3ricas crearemos una variable binaria para cada una de las posibles respuestas. Es decir, si por ejemplo la variable X puede tomar los valores 'SI', 'NO' o 'NOSABE', crearemos las variables 'X_SI', 'X_NO', 'X_NOSABE', que ser\u00e1n variables binarias 0\/1. En ese caso, la variable X ya no ser\u00e1 informativa y, por lo tanto, la eliminaremos del set.\n\nSin embargo, esta binarizaci\u00f3n, que se llama One-Hot Encoding, solo tiene sentido hacerlo para variables con relativamente pocos valores posibles. Por ejemplo, para la variable poblaci\u00f3n, que puede tomar como valores nombres de pueblos que solo aparezcan una vez en la muestra no tiene sentido crear tantas variables dummies que, muy probablemente, no van a ser relevantes. As\u00ed que, de momento, nos centramos en el one-hot encoding de las variables de nuestro set con menos de 40 valores posibles (n\u00famero arbitrario con el que pod\u00e9is ir jugando). \n\nPara el resto de variables, con m\u00e1s de 40 valores posibles, haremos one-hot encoding de los valores m\u00e1s relevantes y agruparemos el resto en una \u00fanica categor\u00eda. Por ejemplo, en el caso de poblaci\u00f3n, para el que hay 4.168 valores posibles, podemos dejar 'MADRID' y 'BARCELONA' separados y agrupar todo el resto en una \u00fanica categor\u00eda 'RESTO'. Sobre esta nueva variable con 3 categor\u00edas, haremos el one-hot encoding. Para no hacer tratamientos individuales en cada variable, generalizaremos esta estrategia y separaremos como categor\u00edas independientes aquellas que tengan m\u00e1s de 500 ocurrencias (aprox 1% de la muestra) y agruparemos el resto. De nuevo, esto se puede optimizar much\u00edsimo a trav\u00e9s de an\u00e1lisis individual de cada variable o de variaci\u00f3n en el n\u00famero de ocurrencias para ser categor\u00eda independiente. ","55f2ec7b":"### Variable HORA\n\nPara la variable HORA por su especial formato haremos un tratamiento especial. Esta variable viene informada como HH:MM:SS. Para esta variable simplemente nos quedaremos con la hora (eliminando minutos y segundos) para convertirla directamente en num\u00e9rica:","aaa56992":"## Codificaci\u00f3n de variables categ\u00f3ricas\n\nEn general, las variables categ\u00f3ricas son aquellas variables no num\u00e9ricas, lo cual puede generar problemas para algunos algoritmos. Por ello, existen varias maneras de codificarlas a n\u00fameros (sobre todo, como veremos aqu\u00ed transform\u00e1ndolas en variables binarias 0\/1). \n\nIdentificamos las variables categ\u00f3ricas en nuestros datos y el n\u00famero de posibles valores que toma la variable: ","1d20a7a9":"La variable FRAUDE es el flag por lo que nos sale missing para todas las observaciones del test, no hay problema. \n\nEstrategia de reemplazo de missings para el resto de variables (existen muchas alternativas a esto):\n- Reemplazamos las variables num\u00e9ricas con pocos missings (EDAD) por la media de la variable correspondiente. Para ser estrictos con la observaci\u00f3n de los datos, se reemplazan tanto el train como el test con la media observada en el Train. En nuestro caso tiene un efecto \u00ednfimo, pero en la vida real, el test ir\u00e1 creciendo continuamente con lo cual si usamos datos de las dos muestras para hacer la media, tendr\u00edamos que actualizar el modelo continuamente tambi\u00e9n.\n- Para la variable categ\u00f3rica POBLACION_1, sustituimos por 'ZZZ' dado que ya hay muchas observaciones con ese valor y parece indicar algo parecido a missing.\n- Para la variable categ\u00f3rica PROVEEDOR, sustituimos por 'SIN MAIL'. Solo es una observaci\u00f3n y posiblemente el hecho de que sea missing es porque no tenga mail. \n- Para la variable categ\u00f3rica DOMINIO, creamos una nueva categoria \"SIN DOMINIO\".\n- Para las dos variables con elevado n\u00famero de missings, opto por reemplazarlo por 0 y crear una variable nueva identificadora de si la variable es missing o no originalmente.","3c87d325":"## Guardamos los datos\nCon estos cambios, muy optimizables todos, ya tenemos una primera base de variables num\u00e9ricas con las que, a priori, ning\u00fan modelo deber\u00eda dar problemas. Guardamos en un nuevo fichero CSV y pasamos al an\u00e1lisis de los datos."}}