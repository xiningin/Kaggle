{"cell_type":{"85debb70":"code","3e22e592":"code","e925e2d2":"code","0821c139":"code","44c709b3":"code","ad7ddbc5":"code","941f1c0c":"code","0b059313":"code","5f7c9078":"code","4e5ba93e":"code","a71a6234":"code","fbdcc8a9":"code","d027bab7":"code","440c3a1b":"code","0a6d4831":"code","c6196477":"code","fe3065cc":"code","2a3dbabb":"code","b485bad0":"code","4aa70487":"code","6b6c1b91":"code","9c3cec7e":"code","7a4eabc0":"code","2049e4a8":"code","65368d55":"code","e580c5c7":"code","e76af30f":"code","b366f92e":"markdown","f5d77e9b":"markdown","4289f64f":"markdown","c30c8f9a":"markdown","60299182":"markdown","689a162b":"markdown","215fd777":"markdown","ea7f30a0":"markdown","d5ffc290":"markdown","17ff65d3":"markdown","1f9ca2fe":"markdown","bacbaa5b":"markdown","e57ba0be":"markdown","6dd9e257":"markdown","5888b219":"markdown","63cc3410":"markdown","bb3735ab":"markdown","3870aad8":"markdown","0e09b194":"markdown","4b5a0e65":"markdown","54b25c69":"markdown","b954be8b":"markdown","d4ccff06":"markdown","debc5c20":"markdown","01e8da53":"markdown","66dddb81":"markdown","bc1d20bd":"markdown","527b141c":"markdown","9d99b7e1":"markdown","91d5275a":"markdown","ea3763c9":"markdown","fd247983":"markdown","e3b435b4":"markdown","496dfc55":"markdown","31da7293":"markdown","82a33f0d":"markdown","efcbcda7":"markdown"},"source":{"85debb70":"import itertools\nimport matplotlib.pyplot as plt \nimport numpy as np \nimport pandas as pd\nimport scipy","3e22e592":"testfile='..\/input\/data_set_ALL_AML_independent.csv'\ntrainfile='..\/input\/data_set_ALL_AML_train.csv'\npatient_cancer='..\/input\/actual.csv'\n\ntrain = pd.read_csv(trainfile)\ntest = pd.read_csv(testfile)\npatient_cancer = pd.read_csv(patient_cancer)","e925e2d2":"train.head()","0821c139":"  #  Celda propia  #\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","44c709b3":"# Remove \"call\" columns from training a test dataframes\ntrain_keepers = [col for col in train.columns if \"call\" not in col]\ntest_keepers = [col for col in test.columns if \"call\" not in col]\n\ntrain = train[train_keepers]\ntest = test[test_keepers]","ad7ddbc5":"train.head()","941f1c0c":"# Transpose the columns and rows so that genes become features and rows become observations\ntrain = train.T\ntest = test.T\ntrain.head()","0b059313":"# Clean up the column names for training data\ntrain.columns = train.iloc[1]\ntrain = train.drop([\"Gene Description\", \"Gene Accession Number\"]).apply(pd.to_numeric)\n\n# Clean up the column names for training data\ntest.columns = test.iloc[1]\ntest = test.drop([\"Gene Description\", \"Gene Accession Number\"]).apply(pd.to_numeric)\n\ntrain.head()","5f7c9078":"# Reset the index. The indexes of two dataframes need to be the same before you combine them\ntrain = train.reset_index(drop=True)\n\n# Subset the first 38 patient's cancer types\npc_train = patient_cancer[patient_cancer.patient <= 38].reset_index(drop=True)\n\n# Combine dataframes for first 38 patients: Patient number + cancer type + gene expression values\ntrain = pd.concat([pc_train,train], axis=1)\n\n\n# Handle the test data for patients 38 through 72\n# Clean up the index\ntest = test.reset_index(drop=True)\n\n# Subset the last patient's cancer types to test\npc_test = patient_cancer[patient_cancer.patient > 38].reset_index(drop=True)\n\n# Combine dataframes for last patients: Patient number + cancer type + gene expression values\ntest = pd.concat([pc_test,test], axis=1)\n","4e5ba93e":"  #  Celda propia  #\n\ntrain","a71a6234":"sample = train.iloc[:,2:].sample(n=100, axis=1)\nsample[\"cancer\"] = train.cancer\nsample.describe().round()","fbdcc8a9":"from sklearn import preprocessing","d027bab7":"sample = sample.drop(\"cancer\", axis=1)\nsample.plot(kind=\"hist\", legend=None, bins=20, color='k')\nsample.plot(kind=\"kde\", legend=None);","440c3a1b":"sample_scaled = pd.DataFrame(preprocessing.scale(sample))\nsample_scaled.plot(kind=\"hist\", normed=True, legend=None, bins=10, color='k')\nsample_scaled.plot(kind=\"kde\", legend=None);","0a6d4831":"# StandardScaler to remove mean and scale to unit variance\nfrom sklearn.preprocessing import StandardScaler","c6196477":"scaler = StandardScaler().fit(train.iloc[:,2:])\nscaled_train = scaler.transform(train.iloc[:,2:])\nscaled_test = scaler.transform(test.iloc[:,2:])\n\nx_train = train.iloc[:,2:]\ny_train = train.iloc[:,1]\nx_test = test.iloc[:,2:]\ny_test = test.iloc[:,1]","fe3065cc":"  #  Celda propia  #\n\nx_train.head()","2a3dbabb":"  #  Celda propia  #\n\ny_train.head()","b485bad0":"  #  Celda propia  #\n\nx_test.head()","4aa70487":"  #  Celda propia  #\n\ny_test.head()","6b6c1b91":"# Grid Search for tuning parameters\nfrom sklearn.model_selection import GridSearchCV\n# RandomizedSearch for tuning (possibly faster than GridSearch)\nfrom sklearn.model_selection import RandomizedSearchCV\n# Bayessian optimization supposedly faster than GridSearch\nfrom bayes_opt import BayesianOptimization\n\n# Metrics\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, log_loss\n\n## Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier","9c3cec7e":"# CHERCHEZ FOR PARAMETERS\ndef cherchez(estimator, param_grid, search):\n    \"\"\"\n    This is a helper function for tuning hyperparameters using teh two search methods.\n    Methods must be GridSearchCV or RandomizedSearchCV.\n    Inputs:\n        estimator: Logistic regression, SVM, KNN, etc\n        param_grid: Range of parameters to search\n        search: Grid search or Randomized search\n    Output:\n        Returns the estimator instance, clf\n    \n    \"\"\"   \n    try:\n        if search == \"grid\":\n            clf = GridSearchCV(\n                estimator=estimator, \n                param_grid=param_grid, \n                scoring=None,\n                n_jobs=-1, \n                cv=10, \n                verbose=0,\n                return_train_score=True\n            )\n        elif search == \"random\":           \n            clf = RandomizedSearchCV(\n                estimator=estimator,\n                param_distributions=param_grid,\n                n_iter=10,\n                n_jobs=-1,\n                cv=10,\n                verbose=0,\n                random_state=1,\n                return_train_score=True\n            )\n    except:\n        print('Search argument has to be \"grid\" or \"random\"')\n        sys.exit(0)\n        \n    # Fit the model\n    clf.fit(X=scaled_train, y=y_train)\n    \n    return clf   ","7a4eabc0":"# Function for plotting the confusion matrices\ndef plot_confusion_matrix(cm, title=\"Confusion Matrix\"):\n    \"\"\"\n    Plots the confusion matrix. Modified verison from \n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    Inputs: \n        cm: confusion matrix\n        title: Title of plot\n    \"\"\"\n    classes=[\"AML\", \"ALL\"]    \n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.bone)\n    plt.title(title)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    thresh = cm.mean()\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j]), \n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] < thresh else \"black\")    ","2049e4a8":"# Logistic Regression\n# Paramaters\nlogreg_params = {} \nlogreg_params[\"C\"] =  [0.01, 0.1, 10, 100]\nlogreg_params[\"fit_intercept\"] =  [True, False]\nlogreg_params[\"warm_start\"] = [True,False]\nlogreg_params[\"random_state\"] = [1]\n\nlr_dist = {}\nlr_dist[\"C\"] = scipy.stats.expon(scale=.01)\nlr_dist[\"fit_intercept\"] =  [True, False]\nlr_dist[\"warm_start\"] = [True,False]\nlr_dist[\"random_state\"] = [1]\n\nlogregression_grid = cherchez(LogisticRegression(), logreg_params, search=\"grid\")\nacc = accuracy_score(y_true=y_test, y_pred=logregression_grid.predict(scaled_test))\ncfmatrix_grid = confusion_matrix(y_true=y_test, y_pred=logregression_grid.predict(scaled_test))\nprint(\"**Grid search results**\")\nprint(\"Best training accuracy:\\t\", logregression_grid.best_score_)\nprint(\"Test accuracy:\\t\", acc)\n\nlogregression_random = cherchez(LogisticRegression(), lr_dist, search=\"random\")\nacc = accuracy_score(y_true=y_test, y_pred=logregression_random.predict(scaled_test))\ncfmatrix_rand = confusion_matrix(y_true=y_test, y_pred=logregression_random.predict(scaled_test))\nprint(\"**Random search results**\")\nprint(\"Best training accuracy:\\t\", logregression_random.best_score_)\nprint(\"Test accuracy:\\t\", acc)\n\nplt.subplots(1,2)\nplt.subplots_adjust(left=-0.5, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\nplot_confusion_matrix(cfmatrix_rand, title=\"Random Search Confusion Matrix\")\nplt.subplot(121)\nplot_confusion_matrix(cfmatrix_grid, title=\"Grid Search Confusion Matrix\")","65368d55":"# SVM\nsvm_param = {\n    \"C\": [.01, .1, 1, 5, 10, 100],\n    \"gamma\": [0, .01, .1, 1, 5, 10, 100],\n    \"kernel\": [\"rbf\"],\n    \"random_state\": [1]\n}\n\nsvm_dist = {\n    \"C\": scipy.stats.expon(scale=.01),\n    \"gamma\": scipy.stats.expon(scale=.01),\n    \"kernel\": [\"rbf\"],\n    \"random_state\": [1]\n}\n\nsvm_grid = cherchez(SVC(), svm_param, \"grid\")\nacc = accuracy_score(y_true=y_test, y_pred=svm_grid.predict(scaled_test))\ncfmatrix_grid = confusion_matrix(y_true=y_test, y_pred=svm_grid.predict(scaled_test))\nprint(\"**Grid search results**\")\nprint(\"Best training accuracy:\\t\", svm_grid.best_score_)\nprint(\"Test accuracy:\\t\", acc)\n\nsvm_random = cherchez(SVC(), svm_dist, \"random\")\nacc = accuracy_score(y_true=y_test, y_pred=svm_random.predict(scaled_test))\ncfmatrix_rand = confusion_matrix(y_true=y_test, y_pred=svm_random.predict(scaled_test))\nprint(\"**Random search results**\")\nprint(\"Best training accuracy:\\t\", svm_random.best_score_)\nprint(\"Test accuracy:\\t\", acc)\n\nplt.subplots(1,2)\nplt.subplots_adjust(left=-0.5, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\nplot_confusion_matrix(cfmatrix_rand, title=\"Random Search Confusion Matrix\")\nplt.subplot(121)\nplot_confusion_matrix(cfmatrix_grid, title=\"Grid Search Confusion Matrix\")","e580c5c7":"# KNN\nknn_param = {\n    \"n_neighbors\": [i for i in range(1,30,5)],\n    \"weights\": [\"uniform\", \"distance\"],\n    \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n    \"leaf_size\": [1, 10, 30],\n    \"p\": [1,2]\n}\n\nknn_dist = {\n    \"n_neighbors\": scipy.stats.randint(1,33),\n    \"weights\": [\"uniform\", \"distance\"],\n    \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n    \"leaf_size\": scipy.stats.randint(1,1000),\n    \"p\": [1,2]\n}\n\nknn_grid = cherchez(KNeighborsClassifier(), knn_param, \"grid\")\nacc = accuracy_score(y_true=y_test, y_pred=knn_grid.predict(scaled_test))\ncfmatrix_grid = confusion_matrix(y_true=y_test, y_pred=svm_grid.predict(scaled_test))\nprint(\"**Grid search results**\")\nprint(\"Best training accuracy:\\t\", knn_grid.best_score_)\nprint(\"Test accuracy:\\t\", acc)\n\nknn_random = cherchez(KNeighborsClassifier(), knn_dist, \"random\")\nacc = accuracy_score(y_true=y_test, y_pred=knn_random.predict(scaled_test))\ncfmatrix_rand = confusion_matrix(y_true=y_test, y_pred=knn_random.predict(scaled_test))\nprint(\"**Random search results**\")\nprint(\"Best training accuracy:\\t\", knn_random.best_score_)\nprint(\"Test accuracy:\\t\", acc)\n\nplt.subplots(1,2)\nplt.subplots_adjust(left=-0.5, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\nplot_confusion_matrix(cfmatrix_rand, title=\"Random Search Confusion Matrix\")\nplt.subplot(121)\nplot_confusion_matrix(cfmatrix_grid, title=\"Grid Search Confusion Matrix\")","e76af30f":"# Decision tree classifier\ndtc_param = {\n    \"max_depth\": [None],\n    \"min_samples_split\": [2],\n    \"min_samples_leaf\": [1],\n    \"min_weight_fraction_leaf\": [0.],\n    \"max_features\": [None],\n    \"random_state\": [4],\n    \"max_leaf_nodes\": [None], # None = infinity or int\n    \"presort\": [True, False]\n}\n\n\ndtc_grid = cherchez(DecisionTreeClassifier(), dtc_param, \"grid\")\nacc = accuracy_score(y_true=y_test, y_pred=dtc_grid.predict(scaled_test))\ncfmatrix_grid = confusion_matrix(y_true=y_test, y_pred=dtc_grid.predict(scaled_test))\nprint(\"**Grid search results**\")\nprint(\"Best training accuracy:\\t\", dtc_grid.best_score_)\nprint(\"Test accuracy:\\t\", acc)\n\nplot_confusion_matrix(cfmatrix_grid, title=\"Decision Tree Confusion Matrix\")","b366f92e":">Discusison:<br>\nAs with the other models, an automated search for hyperparameters with KNN just overfits the training data and doesn't generalize to the test data. The one difference here is that a randomized search produced a model which predicted that at least some of the patients had ALL rather than predicting that all patients have AML.\n\n>Conclusion:<br>\nMy main problem is that the GridSearch and RandomSearch use the model that fits the training data the best without taking into account how well it will generalizes to the test data. ","f5d77e9b":">Depending on the random sample and the histogram-bin-kung-fu, the data usually has a long skinny tail to the right. This KDE plot shows the distribution of indivudal features, but it's not very helpful here. This will change after standardizing the data. ","4289f64f":"En este caso, hay que tener en cuenta posteriormente que hay que eliminar la columna `patient`, ya que si no una divisi\u00f3n con:\n* patient < 28 --> ALL\n* patient >= 28 --> AML\nObtendr\u00eda un 100% de `accuracy` en los datos de entrenamiento.","c30c8f9a":"># Process the full set\n---","60299182":"De nuevo, no comenta la importancia de detectar verdaderos positivos en problemas de clasificaci\u00f3n en el campo de la medicina. S\u00ed que dice que no elegir\u00eda este modelo para clasificar posibles casos de c\u00e1ncer, pero no explica por qu\u00e9. Seg\u00fan su explicaci\u00f3n, alguien puede pensar que es un buen modelo. Como hemos comentado anteriormente, para evitar que la selecci\u00f3n de par\u00e1metros acabe haciendo un `Zero-R` con la clase mayoritaria para as\u00ed obtener un buen `Accuracy`, se podr\u00edan haber seleccionado los par\u00e1metros en base a otras m\u00e9tricas como `Recall`","689a162b":"## Consideraciones finales\n\nA modo de resumen, vamos a indicar los mayores fallos o cosas a mejorar que se han cometido en esta libreta:\n* No establecer una semilla para garantizar la reproducibilidadad\n* No realizar una validaci\u00f3n cruzada iterada para evitar varianza en los resultados.\n* No usar m\u00e1s clasificadores, para tener m\u00e1s opciones a elegir. Destaca sobre todo no haber probdao ning\u00fan *ensemble*.\n* Se deber\u00eda mostrar el tiempo empleado en el proceso de aprendizaje, tanto con `GridSearchCV` como con `RandomizedSearchCV`, ya que podr\u00eda ser una gran diferencia entre ambos m\u00e9todos.\n* Adem\u00e1s, viendo los conjuntos de hiperpar\u00e1metros elegidos para la selecci\u00f3n era bastante probable que `GridSearchCV` y `RandomizedSearchCV` obtuviesen resultados parecidos, ya que los conjuntos para la selecci\u00f3n aleatoria estaban bastante acotados.\n* No ha usado alguna otra m\u00e9trica aparte del `Accuracy`, como podr\u00edan ser `AUC` o `Recall`. Esto es un fallo bastante grande, teniendo en cuenta que est\u00e1 tratando de diagnosticar casos de c\u00e1ncer y que adem\u00e1s \u00e9l mismo se da cuenta de ello en el an\u00e1lisis (por lo que podr\u00eda haberlo establecido si hubiese querido). De hecho, hacerlo no aparece siquiera en la lista de tareas futuras a realizar.","215fd777":"Aunque comenta la tasa de acierto de verdaderos positivos, no comenta la importancia que \u00e9stos tienen, siendo un problema de clasificaci\u00f3n de c\u00e1nceres. Adem\u00e1s, sabiendo que dicho valor es realmente importante, deber\u00eda de haber usado otras m\u00e9tricas para realizar la selecci\u00f3n de variables en lugar del `accuracy`, o una media de varias m\u00e9tricas.","ea7f30a0":"># Classifiers\n---","d5ffc290":"Bien de nuevo, facilitando la comprensi\u00f3n y el manejo de los datos mediante la eliminaci\u00f3n de la descripci\u00f3n de los genes.","17ff65d3":"Efectivamente, se ven mucho mejor las variables despu\u00e9s de estandarizar los datos.","1f9ca2fe":">Discussion:<br>\nIn this case, Logistic Regression wasn't very good at classifying between AML and ALL. In both Grid and Random searches, the differences between the training and test scores show that the model is overfitting to the training data. In fact, the random search results actually overfit more than the grid search which is not what I expected.  Neither search type improved actual prediction results. Comparing the accuracy in confusion matrices the true positives (top left and bottm right corners) are almost the same for each type of search.\n\n>Conclusion: <br>\nA randomized parameter search resulted in higher variance. Possible caveats are that the dataset is rather small and I really haven't tried to choose proper parameters to tune so there may be room for a *little* improvement. \n<br><br>","bacbaa5b":">### Combine the data (gene expression) with class labels (patient numbers)","e57ba0be":"># Introduction\nI have a background in bioinformatics and this dataset was one of my favorites in grad school. For one of my projects, I analyzed the data in R but I have since adopted Python as my language of chocie. The aim of this kernel was to refresh myself and implement machine learning algorithms in Python. I accomplished what I wanted but I keep coming back to try new things.  The current state of this kernel is a comparison of GridSearchCV and RandomizedSearchCV for tuning hyperparameters with 10-fold cross-validation and eventually I would like to implement BayesianSearch for parameters.\n<br><br>\nA heafty chunk of this kernel is just processing the data, so feel free to fork the kernel and use that part. The last half of the kernel is the comparison of the two parameter search methods. All in all, I found that both of these methods led to overfitting of the training data -- none of the models generalized to test data at all. That's the gist of what I learned but there is a little more discussion about it at the end of the kernel.\n<br><br>\nEnjoy!\n\n\n># About the original study\nI need to re-read the study, but from what I recall, these gene expression values come from cancer  patients with either acute lymphocytic leukemia (ALL) or acute myeloid leukemia (AML). In the original study, this datset was used to classify which type of cancer each patient had based on measurements of their gene expressions. The study was published in 1999 and was the first(?) to show that cancer types can be determined based on gene expressions alone. \n\n># About the data\n- Each row represents a different gene\n- Columns 1 and 2 are descriptions about that gene \n- Each numbered column is a patient\n- Each patient has 7129 gene expression values - i.e each patient has one value for each gene \n- The **training** data contain gene expression values for patients 1 through 38\n- The **test** data contain gene expression values for patients 39 through 72\n<br><br>\n\n\n># To do list:\n>### General cleanup\n- Write more functions for redundant testing (maybe, long term wish list)\n\n>### Dimentionality reduction\n- Kanavanand has a great PCA analysis [here](https:\/\/www.kaggle.com\/kanav0183\/pca-analysis-for-geneclassification)\n\n>### Tuning Hyperparamters:\n- Compare GridSearchCV to Bayesian Optimzation (I heard it's more efficient)\n","6dd9e257":"># Data processing steps\n1. Remove columns that contain \"Call\" data, I'm not sure what they are but they doesn't seem useful\n2. Transpose the dataframe so that each row is a patient and each column is a gene\n3. Remove gene description and set the gene accession numbers as the column headers\n4. Merge the data (expression values) with the class labels (patient numbers)","5888b219":"Estar\u00eda bien comprobar que las divisiones se han realizado correctamente (usamos `head()` ya que no hay ninguna semilla establecida):","63cc3410":"Efectivamente, no se aprecian muy bien las distintas variables (aunque como no se ha establecido ninguna semilla, depende de la ejecuci\u00f3n).","bb3735ab":">### Distribution of the random sample before standardizing\n---","3870aad8":">### Distribution of the random sample after standardizing\n---","0e09b194":"># EDA\nThere's a bunch of data, so to speed things up I'm only using a small sample of the training data for the EDA.\n","4b5a0e65":"># Helper functions","54b25c69":">There's quite a difference after standardizing the features. The KDE plot is much more useful in showing the individual distributions too. This is the result of subtracting the mean. Subtracting the mean from each feature centers them on zero. Neat!","b954be8b":"Podr\u00eda haber usado algunos clasificadores m\u00e1s, sobre todo alguno basado en *ensembles* en vez de un \u00e1rbol de decisi\u00f3n simple. Por ejemplo, *AdaBoost* o *Gradient Boosting* usando la t\u00e9cnica de *Boosting*; o *Random Forest* o *Bagging* usando la t\u00e9cnica de *Bagging*.","d4ccff06":"Estandarizar consiste en dejar todas las variables con media 0 y varianza 1. Sin embargo, escalar no es lo mismo que estandarizar ya que escalar consiste en cuadrar los datos para que encajen en un rango espec\u00edfico de valores, por ejemplo, entre 0 y 100. S\u00ed que es verdad que en `scikit`, la estandarizaci\u00f3n se realiza con la clase `sklearn.preprocessing.scale`.","debc5c20":"># Models being tested\n1. Logisitc Regresison\n  - Using Grid search and Randomized search for tuning hyperparameters\n2. C-Support Vector Classification (SVM)\n  - Using Grid search and Randomized search for tuning hyperparameters\n3. K-Nearest Neighbors Classifier\n  - Using Grid search and Randomized search for tuning hyperparameters\n4. Decision Tree Classifier\n  - Using only Grid search","01e8da53":"El autor realiza una buena introducci\u00f3n a la libreta, sus objetivos, y algo de informaci\u00f3n sobre el dataset utilizado. ","66dddb81":"># To standardize or not to standardize\nThis is for a visual reference on how data changes after scaling. And for the record, I use the words standardize and scale interchangably. I think it's technically \"standardizing\", but scikit-learn calls it scaling. \n<br><br>\nStandardize  = For each value, subtract the mean and scale to unit variance\n\n\n","bc1d20bd":"Est\u00e1 muy bien haber transpuesto la matriz ya que ahora cada instancia s\u00ed representa a un conjunto de expresiones de los genes, por lo que era necesario hacerlo.","527b141c":"Efectivamente, la divisi\u00f3n se ha realizado correctamente.","9d99b7e1":"># Sources:\nGolub et al: https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/10521349\n<br><br>\nBayesian optimization: https:\/\/arxiv.org\/pdf\/1012.2599v1.pdf\n","91d5275a":"# Estudio de un kernel de Kaggle\n\n### Pablo Torrijos Arenas y Alejandro Fern\u00e1ndez Arjona\n\nEn esta libreta vamos a realizar el estudio del kernel de Kaggle: [Hyperparameter Search Comparison (Grid vs Random)](https:\/\/www.kaggle.com\/crawford\/hyperparameter-search-comparison-grid-vs-random).\n\nEl aprendizaje y selecci\u00f3n de modelos para [Breast Cancer Wisconsin (Diagnostic) Data Set](https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data) se encuentra en el siguiente kernel de Kaggle: [Selecci\u00f3n de modelos Wisconsin](https:\/\/www.kaggle.com\/pablotorrijosarenas\/selecci-n-de-modelos-wisconsin-grupo-h).\n\nEl aprendizaje y selecci\u00f3n de modelos para [Pima Indians Diabetes Database](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database) se encuentra en el siguiente kernel de Kaggle: [Selecci\u00f3n de modelos Diabetes](https:\/\/www.kaggle.com\/alexfer\/aprendizaje-de-modelos-diabetes-grupo-h).\n\n---\n\nHemos escogido este *notebook* porque est\u00e1 bastante completo, pero no todo es perfecto. Vamos a ir comentando tanto las cosas que se han realizado correctamente, como las cosas que se podr\u00edan hacer mejor y las cosas que directamente no se hacen y se deber\u00edan haber hecho. El objetivo del cuaderno original es comparar `GridSearchCV` y `RandomizedSearchCV` en algunos algoritmos para ver si hay diferencias entre ellos.","ea3763c9":">Discussion:<br>\nWell, this example goes to show that if you just predict that every patient has AML, you'll be correct more often than wrong. The model predicted that all 34 patients in the test data had AML and was absolutely right for 20 of them but absolutely wrong for the other 14 patients. \n\n>Conclusion<br>\nI wouldn't use this particular model to classify cancer patients.\n<br><br>","fd247983":">Discussion:<br>\nThis model accurately predicted more ALL patients than any of the other models and had the lowest false positive rate, but frankly that's just painting it in a positive light and I'm not even bothering to calculate it. The model still has poor accuracy overall, doesn't generalize to the test data, and has a huge variance problem\n\n>Conclusion:<br>\nThis experiment makes a good case for splitting your data into train, validation, and test sets. It would be nice if GridSearch and RandomSearch could refit the best predictors that minimize the variance between the accuracy of train and validation sets. As it is, overfitting the training data has been a problem for each of these models. It's possible that I could investigate which parameters would be better to tune and it's possible that there isn't enough data, and it's also possible that this isn't a good classification problem. Maybe I need to approach this as a clustering problem. What do you think?","e3b435b4":"El primer fallo que vemos, es que al igual que pasaba en nuestra pr\u00e1ctica, no se pueden ver todas las columnas de los dataframes, por lo que ser\u00eda interesante solucionarlo. Basta con la siguiente celda de c\u00f3digo (marcamos con un comentario especial al principio las celdas que a\u00f1adamos):","496dfc55":"Ha identificado que est\u00e1 sobreajustando sobre el conjunto de entrenamiento (normal trat\u00e1ndose del algoritmo de los vecinos m\u00e1s cercanos), lo que hace que obtenga malos resultados. Utilizar otras m\u00e9tricas podr\u00eda ayudar a evitar en parte este problema.","31da7293":"Aqu\u00ed hay que destacar que, como no se ha fijado ning\u00fan tipo de semilla, no se puede garantizar la reproducibilidad de esta parte de la libreta. As\u00ed, cada vez que ejecutemos esta celda obtendremos unos datos distintos, y posteriormente al evaluar si hay que estandarizar o no, las gr\u00e1ficas que se obtendr\u00e1n ser\u00e1n distintas cada vez.","82a33f0d":"En cuanto a la eliminaci\u00f3n de esas columnas, est\u00e1 bien hecha, pero se podr\u00eda haber realizado con la librer\u00eda `sklearn` mediante `make_column_transformer`, indicando con un `drop` las columnas a eliminar. Adem\u00e1s, en ","efcbcda7":"El propio programador de la libreta ha indicado varios errores sobre sus modelos. Aunque este modelo, usando un \u00e1rbol de decisi\u00f3n, ha predicho menos falsos positivos que otros modelos, no tiene una buena precisi\u00f3n en general, no generaliza para el conjunto de test y tiene mucha varianza, por lo que deber\u00eda realizar un *refit* utilizando otro tipo de m\u00e9tricas.\n\nAdem\u00e1s, en este caso no tiene mucho sentido que realize el proceso de selecci\u00f3n de hiperpar\u00e1metros ya que \u00fanicamente ha seleccionado `presort`, d\u00e1ndole al resto solo un valor posible."}}