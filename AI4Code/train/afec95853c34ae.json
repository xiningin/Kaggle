{"cell_type":{"a892d563":"code","da30f819":"code","02850725":"code","78cf3e03":"code","8ba537b5":"code","b2b28780":"code","13edd973":"code","6bad7dde":"code","fba2e230":"code","33ec93df":"code","33152348":"markdown"},"source":{"a892d563":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da30f819":"import os, time\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.metrics import make_scorer\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom scipy.special import expit, logit\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch import nn\nimport torch.utils.data as utils\n\nimport matplotlib.pyplot as plt\n","02850725":"data_root = \"\/kaggle\/input\/lish-moa\/\"\ntrain_X = pd.read_csv(data_root + 'train_features.csv')\ntrain_Y = pd.read_csv(data_root + 'train_targets_scored.csv')\ntest_X = pd.read_csv(data_root + 'test_features.csv')\nsample_submission = pd.read_csv(data_root + 'sample_submission.csv')","78cf3e03":"# cp_type and cp_dose have both 2 categories each\n# OneHotEncoder will return 4 columns, \n# one for each of the 4 unique categorical values\nohe = OneHotEncoder(sparse=False)\nonehotfeat = ohe.fit_transform(train_X[['cp_type', 'cp_dose', 'cp_time']])\n# print(onehotfeat.shape, ohe.categories_)\n\n# add these 4 features into the original train dataset\ntrain_X['cp_type_0'] = onehotfeat[:,0]\ntrain_X['cp_type_1'] = onehotfeat[:,1]\ntrain_X['cp_dose_0'] = onehotfeat[:,2]\ntrain_X['cp_dose_1'] = onehotfeat[:,3]\ntrain_X['cp_time_24'] = onehotfeat[:,4]\ntrain_X['cp_time_48'] = onehotfeat[:,5]\ntrain_X['cp_time_72'] = onehotfeat[:,6]\n\n# dot the sane thing to the test dataset\nonehotfeat = ohe.transform(test_X[['cp_type', 'cp_dose', 'cp_time']])\ntest_X['cp_type_0'] = onehotfeat[:,0]\ntest_X['cp_type_1'] = onehotfeat[:,1]\ntest_X['cp_dose_0'] = onehotfeat[:,2]\ntest_X['cp_dose_1'] = onehotfeat[:,3]\ntest_X['cp_time_24'] = onehotfeat[:,4]\ntest_X['cp_time_48'] = onehotfeat[:,5]\ntest_X['cp_time_72'] = onehotfeat[:,6]\n\n# drop the original cp_type and cp_dose columns from the dataset\ntrain_X.drop(['cp_type', 'sig_id', 'cp_dose', 'cp_time'], axis=1, inplace=True)\ntest_X.drop(['cp_type', 'sig_id', 'cp_dose', 'cp_time'], axis=1, inplace=True)\ntrain_Y.drop(['sig_id'],axis=1,inplace=True)\n\nprint(train_X, test_X)\nprint(train_X.columns, test_X.columns)\nprint(train_X.shape, train_Y.shape, test_X.shape)\n","8ba537b5":"# we only scale the real-valued features and not the categorical ones\ng_cols = [col for col in train_X.columns if col.startswith('g-')]\nc_cols = [col for col in train_X.columns if col.startswith('c-')]\ntransform_feature_list = g_cols + c_cols\n\n\ndef scale_and_PCA(pca_num_components, train, test, cols_to_transform, transformed_col_name):\n    # create data by stacking rows from both train and test, for the required columns\n    data = pd.concat([train[cols_to_transform], test[cols_to_transform]], axis=0).reset_index(drop=True)\n    n = train.shape[0]\n    \n    # scale\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(data)\n\n    # PCA\n    pca = PCA(pca_num_components)\n    pca_data = pca.fit_transform(scaled_data)\n\n    train_trans = pca_data[:n, :]\n    test_trans = pca_data[n:, :]\n\n    return train_trans, test_trans\n\n\n# first scale and PCA on \"g-\" features\n# we use top 30 \"g-\" features after PCA\ntrain_X_g, test_X_g = scale_and_PCA(30, train_X, test_X, g_cols, 'g_pca')\n\n# next, scale and tranform the \"c-\" features\n# we use top 10 \"c-\" features after PCA\ntrain_X_c, test_X_c = scale_and_PCA(10, train_X, test_X, c_cols, 'c_pca')\n\n# concatenate the \"g-\" and \"c-\" columns\ndata_train = np.concatenate((train_X_g, train_X_c), axis=1)\ndata_test = np.concatenate((test_X_g, test_X_c), axis=1)\n\n# now concatenate train and test rows\nn = data_train.shape[0]\ndata = np.concatenate((data_train, data_test), axis=0)\n\n# categorical columns are the last 7 columns of train\/test data (after the preprocessing that we did earlier)\ncat_data = np.concatenate((train_X.iloc[:, -7:].to_numpy(), test_X.iloc[:, -7:].to_numpy()), axis=0)\n\n# add back the 7 categorical columns in front of the real-valued columns\ntransformed_data = np.concatenate((cat_data, data), axis=1)\n\n# seaparate the train\/test data\ntransformed_train_data = transformed_data[:n, :]\ntransformed_test_data = transformed_data[n:, :]\ntransformed_train_targets = train_Y.values\n\nprint(transformed_train_data.shape, transformed_test_data.shape)","b2b28780":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nbatch_size = 64\n\n\ndef average_log_loss (y_true, y_pred):\n    print(y_true.shape, y_pred.shape)\n    num_samples, num_outputs = y_true.shape\n    loss = 0.00\n    for i in range(num_outputs):\n        loss += log_loss(y_true[:, i], y_pred[:, i])\n    loss \/= num_outputs\n    return loss\n\n\nclass NNet(nn.Module):\n    def __init__(self):\n        super(NNet, self).__init__()\n        self.mlp = nn.Sequential(\n            torch.nn.Linear(47, 512),\n            torch.nn.ReLU(),\n            torch.nn.Linear(512, 256),\n            torch.nn.Dropout(0.3),\n            torch.nn.Linear(256, 206))\n\n    def forward(self, x):\n        return self.mlp(x)\n\n\nloss_fn = torch.nn.BCEWithLogitsLoss()","13edd973":"def train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    correct = 0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        # print(data.shape, target.shape)\n        optimizer.zero_grad()\n        output = model(data.float())\n        # print(output.shape, output)\n        target = target.type_as(output)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 100 == 0:  # Print loss every 100 batch\n            print('Train Epoch: {}\\tLoss: {:.6f}'.format(\n                epoch, loss.item()))\n    test(model, device, train_loader)\n    return None\n\ndef test(model, device, test_loader):\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data.float())\n\n'''\nkfold = KFold(n_splits=5)\n\nfor train_index, val_index in kfold.split(transformed_train_data, train_Y.values):  \n    print(train_index, val_index)\n'''","6bad7dde":"n = transformed_train_data.shape[0]\n\n# transform to torch tensors\n# 3:1 split for train:val\ntensor_train_x = torch.tensor(transformed_train_data, device=device)\ntensor_train_y = torch.tensor(transformed_train_targets, device=device)\n'''tensor_val_x = torch.tensor(transformed_train_data[:n\/\/4, :], device=device)\ntensor_val_y = torch.tensor(transformed_train_targets[:n\/\/4, :], device=device)'''\ntensor_test_x = torch.tensor(transformed_test_data, device=device)\n    \n# create dataset and dataloader for training\/validation data\ntrain_dataset = utils.TensorDataset(tensor_train_x, tensor_train_y)\ntrain_loader = utils.DataLoader(train_dataset, batch_size=batch_size)\n\ntest_loader = utils.DataLoader(tensor_test_x, shuffle=False)\n'''val_dataset = utils.TensorDataset(tensor_val_x, tensor_val_y)\nval_loader = utils.DataLoader(val_dataset, batch_size=batch_size)'''","fba2e230":"model = NNet().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nfor epoch in range(20):\n    train(model, device, train_loader, optimizer, epoch)\n    print(epoch)","33ec93df":"predictions = model(tensor_test_x.float())\npredictions = expit(predictions.detach().cpu().numpy())\nprint(predictions, predictions.shape)\n\nsample_submission[sample_submission.columns.to_list()[1:]] = predictions\nsample_submission.to_csv('submission.csv',index=False)\n# !rm .\/submission.csv","33152348":"# Torch the data!"}}