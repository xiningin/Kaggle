{"cell_type":{"927e8750":"code","8d5280e2":"code","3380e6ca":"code","a922b5b6":"code","2f47b9c6":"code","23fca129":"code","ca10af71":"code","ff89bd22":"code","c5d3623f":"code","9f5a9780":"code","3ecef64f":"code","7a348e18":"code","fa899183":"code","e0387d2e":"code","d6dec2de":"code","f1bc87fc":"code","38042bfa":"code","68dfb9ff":"code","1287b9c1":"code","e2415912":"code","42eb6b60":"code","852f1c34":"code","ab0f60b3":"code","0ea946a0":"code","c1dd643c":"markdown","2ebfa6ce":"markdown","9689d175":"markdown","3af6a5af":"markdown","d8ea1311":"markdown","75ea3275":"markdown","eb0080b9":"markdown","a81e2620":"markdown"},"source":{"927e8750":"import regex as re\nimport string\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import f1_score\n\nimport spacy","8d5280e2":"import nltk\nnltk.download('stopwords')","3380e6ca":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntrain_df.head()","a922b5b6":"print (train_df.info())\nprint (\"# of training records: \", len(train_df))","2f47b9c6":"train_df['target'].value_counts(normalize=True)","23fca129":"test_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest_df.head()","ca10af71":"train_df[~train_df['keyword'].isnull()][['keyword', 'text']]","ff89bd22":"len(train_df['location'].unique())","c5d3623f":"train_df.drop(columns=['keyword', 'location'], inplace=True)","9f5a9780":"tokenizer = TweetTokenizer()\nspacy_en = spacy.load('en_core_web_sm', disable=['ner', 'parser'])","3ecef64f":"def preprocess_text(text):\n  cleaned_text = text.lower()\n  \n  #remove words like hashtags, web addresses\n  cleaned_text = re.sub(r'#\\S+|@\\S+|<.*?>|http\\S+|[0-9,.]+\\S+', '', cleaned_text)\n\n  cleaned_text = tokenizer.tokenize(cleaned_text)\n  cleaned_text = ' '.join([word for word in cleaned_text if word not in string.punctuation and word not in stopwords.words('english')])\n\n  doc = spacy_en(cleaned_text)\n  cleaned_text = [token.lemma_ for token in doc if token.lemma_ not in ['-PRON-'] and token.pos_ not in ['DET','-PRON']]\n  return cleaned_text","7a348e18":"train_df['cleaned_text'] = train_df.apply(lambda row : (preprocess_text(row['text']), row['target']), axis=1)","fa899183":"train_tweets = train_df['cleaned_text'].map(lambda elem: elem[0])","e0387d2e":"def get_all_words(all_tweets):\n  all_words = []\n  for elem in all_tweets:\n    all_words.extend(elem)\n  return all_words\n\ndef get_word_features(all_words):\n  word_count = nltk.FreqDist(all_words)\n  word_features = word_count.keys()\n  return word_features\n\nvocab = get_word_features(get_all_words(train_tweets))\nprint(\"Total unique features: \", len(vocab))","d6dec2de":"def extract_features(tweets):\n  features = {}\n  tweet_words = set(tweets)\n  for word in vocab:\n    features['contains(%s)' % word] = (word in tweet_words)\n  return features","f1bc87fc":"slice_index = int(len(train_df) * 0.8)\nmessages_set = train_df['cleaned_text']\nrandom.shuffle(messages_set)\n\ntrain_messages, val_messages = messages_set[0:slice_index].tolist(), messages_set[slice_index:].tolist()","38042bfa":"training_set = nltk.classify.apply_features(extract_features, train_messages)\nvalidation_set = nltk.classify.apply_features(extract_features, val_messages)","68dfb9ff":"classifier = nltk.NaiveBayesClassifier.train(training_set)\nprint(\"Train accuracy: \" , nltk.classify.accuracy(classifier, training_set))\nprint(\"Validation accuracy: \", nltk.classify.accuracy(classifier, validation_set))","1287b9c1":"full_train_messages = messages_set.tolist()\nfull_training_set = nltk.classify.apply_features(extract_features, full_train_messages)\nclassifier = nltk.NaiveBayesClassifier.train(full_training_set)","e2415912":"test_df.drop(columns=['keyword', 'location'], inplace=True)\ntest_df['cleaned_text'] = test_df.apply(lambda row:preprocess_text(row['text']), axis=1)\ntest_df['cleaned_text'][0:5]","42eb6b60":"test_predictions = test_df['cleaned_text'].apply(lambda tweet: classifier.classify(extract_features(tweet)))","852f1c34":"test_predictions[0:5]","ab0f60b3":"submission = pd.DataFrame({'id':test_df.id, 'target':test_predictions})\nsubmission.head()","0ea946a0":"submission.to_csv('.\/submission.csv', index=False)","c1dd643c":"**Data Reading and Understanding**","2ebfa6ce":"**Prediction on test set to be evaluated**","9689d175":"**Dropping the features: keyword and location**","3af6a5af":"**Checking- if features keyword and location are useful and should be retained**","d8ea1311":"**Data Cleaning and Preprocessing**","75ea3275":"**NLTK Naive Bayes Classifier using SnowballStemmer**","eb0080b9":"**Training on full messages set to be used for test prediction**","a81e2620":"**Modelling**"}}