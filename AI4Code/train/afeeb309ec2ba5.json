{"cell_type":{"5eeecfe7":"code","5c5f069b":"code","3d60f798":"code","2203018b":"code","ac66b58b":"code","73c15a61":"code","a9d012a7":"code","18fcc730":"code","4270061b":"code","6fcbca41":"code","d6b9546e":"code","3964ff15":"code","af460d57":"code","2e6f8f49":"code","3e5c1db1":"code","ccc76640":"code","f444c24e":"code","042ce8ec":"code","9db27aa9":"code","a2d05e73":"code","3fbbe369":"code","1516145b":"code","957d0616":"code","a54535b9":"code","f8913b37":"code","2a7869f5":"code","5c012521":"code","f8dbcc61":"code","876b2283":"code","5bebe602":"code","6aa440ce":"code","acbb7ca7":"code","932c3595":"code","e083766d":"code","7753e781":"code","90fb3b84":"code","fb0da871":"code","d4572a8b":"code","5ed5c793":"code","d57e78dc":"code","1a3e14df":"code","71f01b79":"markdown","a702233e":"markdown","7cd2f33c":"markdown","3a4b4ae6":"markdown","07c2f963":"markdown","1d8a3c70":"markdown","d777f112":"markdown","2b2e9fe4":"markdown","e902d4a5":"markdown","085af655":"markdown","04b55d24":"markdown","7cca6ba5":"markdown","38a501c8":"markdown","9a76695e":"markdown","b05283cf":"markdown","50d1476a":"markdown","81e48ce1":"markdown","01860533":"markdown","0cbe050f":"markdown","e0cd9607":"markdown","4f14de87":"markdown","004ebfc0":"markdown","f150c8d3":"markdown","26aab99e":"markdown","f0a767e2":"markdown","3e97a0bf":"markdown","cdade6b7":"markdown","429c56b9":"markdown","fecf1e2d":"markdown","844ed63d":"markdown"},"source":{"5eeecfe7":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n%matplotlib inline","5c5f069b":"df = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","3d60f798":"df.head(5)","2203018b":"test.head(5)","ac66b58b":"df.info()\ntest.info()","73c15a61":"#Count the number of duplicated rows\ndf.duplicated().sum()","a9d012a7":"#Count the number of NaN values for each column\ndf.isna().sum()","18fcc730":"df.describe()","4270061b":"#Visualize univariate outliers\nplt.subplots(figsize=(18,6))\nplt.title(\"Outliers visualization\")\ndf.boxplot();","6fcbca41":"#Only keep trips that lasted less than 5900 seconds\ndf = df[(df.trip_duration < 5900)]","d6b9546e":"#Only keep trips with passengers\ndf = df[(df.passenger_count > 0)]","3964ff15":"#Plot pickup positions to visualize outliers\npickup_longitude = list(df.pickup_longitude)\npickup_latitude = list(df.pickup_latitude)\nplt.subplots(figsize=(18,6))\nplt.plot(pickup_longitude, pickup_latitude, '.', alpha = 1, markersize = 10)\nplt.xlabel('pickup_longitude')\nplt.ylabel('pickup_latitude')\nplt.show()","af460d57":"#Plot dropoff positions to visualize outliers\ndropoff_longitude = list(df.dropoff_longitude)\ndropoff_latitude = list(df.dropoff_latitude)\nplt.subplots(figsize=(18,6))\nplt.plot(dropoff_longitude, dropoff_latitude, '.', alpha = 1, markersize = 10)\nplt.xlabel('dropoff_longitude')\nplt.ylabel('dropoff_latitude')\nplt.show()","2e6f8f49":"#Remove position outliers\ndf = df[(df.pickup_longitude > -100)]\ndf = df[(df.pickup_latitude < 50)]\n#df = df[(df.dropoff_longitude < -70) & (df.dropoff_longitude > -80)]\n#df = df[(df.dropoff_latitude < 50)]","3e5c1db1":"#Visualize the distribution of trip_duration values\nplt.subplots(figsize=(18,6))\nplt.hist(df['trip_duration'].values, bins=100)\nplt.xlabel('trip_duration')\nplt.ylabel('number of train records')\nplt.show()","ccc76640":"#Log-transformation\nplt.subplots(figsize=(18,6))\ndf['trip_duration'] = np.log(df['trip_duration'].values)\nplt.hist(df['trip_duration'].values, bins=100)\nplt.xlabel('log(trip_duration)')\nplt.ylabel('number of train records')\nplt.show()","f444c24e":"#One-hot encoding binary categorical features\ndf = pd.concat([df, pd.get_dummies(df['store_and_fwd_flag'])], axis=1)\ntest = pd.concat([test, pd.get_dummies(test['store_and_fwd_flag'])], axis=1)\n\ndf.drop(['store_and_fwd_flag'], axis=1, inplace=True)\n\ndf = pd.concat([df, pd.get_dummies(df['vendor_id'])], axis=1)\ntest = pd.concat([test, pd.get_dummies(test['vendor_id'])], axis=1)\n\ndf.drop(['vendor_id'], axis=1, inplace=True)","042ce8ec":"#Datetyping the dates\ndf['pickup_datetime'] = pd.to_datetime(df.pickup_datetime)\ntest['pickup_datetime'] = pd.to_datetime(test.pickup_datetime)\n\ndf.drop(['dropoff_datetime'], axis=1, inplace=True) #as we don't have this feature in the testset\n\n#Date features creations and deletions\ndf['month'] = df.pickup_datetime.dt.month\ndf['week'] = df.pickup_datetime.dt.week\ndf['weekday'] = df.pickup_datetime.dt.weekday\ndf['hour'] = df.pickup_datetime.dt.hour\ndf['minute'] = df.pickup_datetime.dt.minute\ndf['minute_oftheday'] = df['hour'] * 60 + df['minute']\ndf.drop(['minute'], axis=1, inplace=True)\n\ntest['month'] = test.pickup_datetime.dt.month\ntest['week'] = test.pickup_datetime.dt.week\ntest['weekday'] = test.pickup_datetime.dt.weekday\ntest['hour'] = test.pickup_datetime.dt.hour\ntest['minute'] = test.pickup_datetime.dt.minute\ntest['minute_oftheday'] = test['hour'] * 60 + test['minute']\ntest.drop(['minute'], axis=1, inplace=True)\n\ndf.drop(['pickup_datetime'], axis=1, inplace=True)\n\ndf.info()","9db27aa9":"#Function aiming at calculating distances from coordinates\ndef ft_haversine_distance(lat1, lng1, lat2, lng2):\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    AVG_EARTH_RADIUS = 6371 #km\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n    return h\n\n#Add distance feature\ndf['distance'] = ft_haversine_distance(df['pickup_latitude'].values,\n                                                 df['pickup_longitude'].values, \n                                                 df['dropoff_latitude'].values,\n                                                 df['dropoff_longitude'].values)\ntest['distance'] = ft_haversine_distance(test['pickup_latitude'].values, \n                                                test['pickup_longitude'].values, \n                                                test['dropoff_latitude'].values, \n                                                test['dropoff_longitude'].values)","a2d05e73":"#Function aiming at calculating the direction\ndef ft_degree(lat1, lng1, lat2, lng2):\n    AVG_EARTH_RADIUS = 6371 #km\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    return np.degrees(np.arctan2(y, x))\n\n#Add direction feature\ndf['direction'] = ft_degree(df['pickup_latitude'].values,\n                                df['pickup_longitude'].values,\n                                df['dropoff_latitude'].values,\n                                df['dropoff_longitude'].values)\ntest['direction'] = ft_degree(test['pickup_latitude'].values,\n                                  test['pickup_longitude'].values, \n                                  test['dropoff_latitude'].values,\n                                  test['dropoff_longitude'].values)","3fbbe369":"#Visualize distance outliers\ndf.boxplot(column='distance', return_type='axes');","1516145b":"#Remove distance outliers\ndf = df[(df.distance < 200)]","957d0616":"#Create speed feature\ndf['speed'] = df.distance \/ df.trip_duration","a54535b9":"#Visualize speed feature\ndf.boxplot(column='speed', return_type='axes');","f8913b37":"#Remove speed outliers\ndf = df[(df.speed < 30)]\ndf.drop(['speed'], axis=1, inplace=True)","2a7869f5":"#Correlations between variables\nfig, ax = plt.subplots(figsize=(14,5))  \nsns.heatmap(data=df.corr(), annot=True, cmap = plt.cm.RdYlBu_r, linewidths=.1, ax=ax).set_title('Correlations between variables');","5c012521":"#Split the labeled data frame into two sets: features and target\ny = df[\"trip_duration\"]\ndf.drop([\"trip_duration\"], axis=1, inplace=True)\ndf.drop(['id'], axis=1, inplace=True)\nX = df\n\nX.shape, y.shape","f8dbcc61":"#Split the labeled data frame into two sets to train then test the models\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","876b2283":"from sklearn.metrics import mean_squared_error as MSE","5bebe602":"#%%time\n#Try GradientBoosting\n#from sklearn.ensemble import GradientBoostingRegressor\n\n#gb = GradientBoostingRegressor()\n#gb.fit(X_train, y_train)\n#print(gb.score(X_train, y_train), gb.score(X_test, y_test))\n#print(np.sqrt(MSE(y_test, gb.predict(X_test))))\n    \n#Output\n    #0.7454771059776502 0.7443578507676307\n    #0.39291173774102295\n    #CPU times: user 3min 48s, sys: 328 ms, total: 3min 48s\n    #Wall time: 3min 48s","6aa440ce":"#%%time\n#Try RandomForest\n#from sklearn.ensemble import RandomForestRegressor\n\n#rf = RandomForestRegressor()\n#rf.fit(X_train, y_train)\n#print(rf.score(X_train, y_train), rf.score(X_test, y_test))\n#print(np.sqrt(MSE(y_test, rf.predict(X_test))))\n\n#Output:\n    #0.9601197799928392 0.7790255381297454\n    #0.36530012047088345\n    #CPU times: user 3min, sys: 792 ms, total: 3min 1s\n    #Wall time: 3min 1s","acbb7ca7":"#%%time\n#Try LightGBM ----------------------------\nimport lightgbm as lgb\n\n#lgb_params = {\n#    'metric': 'rmse',\n#    'is_training_metric': True}\n\n#lgb_train = lgb.Dataset(X_train, y_train)\n#lgb_test = lgb.Dataset(X_test, y_test)\n#lgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=100, valid_sets=[lgb_train, lgb_test], early_stopping_rounds=5)\n\n#Output\n    #[100]\tvalid_0's rmse: 0.362209\tvalid_1's rmse: 0.3629\n    #CPU times: user 40.9 s, sys: 332 ms, total: 41.2 s\n    #Wall time: 21.2 s\n\n#Try LightGBM with sklearn API ------------\n#from lightgbm import LGBMRegressor\n\n#lgbm = lgb.LGBMRegressor()\n#lgbm.fit(X, y)\n#print(lgbm.score(X_train, y_train), lgbm.score(X_test, y_test))\n#print(np.sqrt(MSE(y_test, lgbm.predict(X_test))))\n\n#Output:\n    #0.7812886118508641 0.7827256176145024\n    #0.3623481127815768\n    #CPU times: user 42 s, sys: 1.08 s, total: 43 s\n    #Wall time: 22.5 s","932c3595":"#Cross-validation on LightGBM model --------------------------\n#lgb_df = lgb.Dataset(X, y)\n#lgb.cv(lgb_params, lgb_df, stratified=False) #False is needed as it only works with classification\n\n#Cross-validation on LightGBM model (sklearn API) ------------\n#from sklearn.model_selection import cross_val_score\n\n#cv_score = cross_val_score(lgbm, X, y, cv=5)\n#print(cv_score)\n#print(np.mean(cv_score))\n\n#Output:\n    #[0.77872018 0.7801329  0.77988107 0.78049745 0.77904688]\n    #0.7796556968369478","e083766d":"#Hyperparameters tuning using RandomizedSearchCV\n#from sklearn.model_selection import RandomizedSearchCV\n\n#n_estimators = [int(x) for x in np.linspace(start = 5, stop = 20, num = 16)]\n#max_features = ['auto', 'sqrt']\n#max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n#max_depth.append(None)\n#min_samples_split = [2, 5, 10]\n#min_samples_leaf = [1, 2, 4]\n#bootstrap = [True, False]\n\n#random_grid = {'n_estimators': n_estimators, 'max_features': max_features, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\n\n#random_cv = RandomizedSearchCV(estimator = m1, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n#print(random_cv.best_params_)","7753e781":"#Test the following parameters\nlgb_params = {\n    #'metric' : 'rmse',\n    'learning_rate': 0.1,\n    'max_depth': 25,\n    'num_leaves': 1000, \n    'objective': 'regression',\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.5,\n    'max_bin': 1000 }\n\n#lgb_train = lgb.Dataset(X_train, y_train)\n#lgb_test = lgb.Dataset(X_test, y_test)\n#lgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=1500, valid_sets=[lgb_train, lgb_test], early_stopping_rounds=5)","90fb3b84":"#%%time\n#Training on all labeled data using the best parameters in hyperparameters tuning\n#rf = RandomForestRegressor(n_estimators=300, min_samples_leaf=10, min_samples_split=15, max_features='auto', max_depth=90, bootstrap=True)\n#rf.fit(X, y)","fb0da871":"#%%time\n#Training on all labeled data using the best parameters (sklearn API version)\n#from lightgbm import LGBMRegressor\n\n#lgbm = lgb.LGBMRegressor(n_estimators=500, num_leaves=1000, max_depth=25, objective='regression')\n#lgbm.fit(X, y)","d4572a8b":"%%time\n#Training on all labeled data using the best parameters\nlgb_df = lgb.Dataset(X, y)\nlgb_model = lgb.train(lgb_params, lgb_df, num_boost_round=1500)","5ed5c793":"#Make predictions on test data frame\ntest_columns = X.columns\npredictions = lgb_model.predict(test[test_columns])","d57e78dc":"#Create a data frame designed a submission on Kaggle\nsubmission = pd.DataFrame({'id': test.id, 'trip_duration': np.exp(predictions)})\nsubmission.head()","1a3e14df":"#Create a csv out of the submission data frame\nsubmission.to_csv(\"sub.csv\", index=False)","71f01b79":"Our LightGBM model is stable.","a702233e":"### d. Distance and speed creations <a id=\"three-d\"><\/a>","7cd2f33c":"### b. Metrics <a id=\"four-b\"><\/a>","3a4b4ae6":"We clearly see `trip_duration` takes strange values for `min` and `max`. Let's have a quick visualization with a boxplot.","07c2f963":"There are outliers for `trip_duration`. I can't find a proper interpretation and it will probably damage our model, so I choose to get rid of them.","1d8a3c70":"There are no duplicated or missing values.","d777f112":"### a. Duplicated and missing values <a id=\"two-a\"><\/a>","2b2e9fe4":"## I. Data loading and overview <a id=\"one\"><\/a>","e902d4a5":"### a. Split <a id=\"four-a\"><\/a>","085af655":"### b. Deal with categorical features <a id=\"three-b\"><\/a>","04b55d24":"### a. Loading the data <a id=\"one-a\"><\/a>","7cca6ba5":"## V. Hyperparameters tuning <a id=\"five\"><\/a>","38a501c8":"## II. Data cleaning <a id=\"two\"><\/a>","9a76695e":"### c. Models <a id=\"four-c\"><\/a>","b05283cf":"To fine-tune the hyperparameters in LGB, you can take a look at its documentation which provides some great advice: https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html.","50d1476a":"For this specific problematic, we'll measure the error using the RMSE (Root Mean Squared Error).","81e48ce1":"**LightGBM** is blazingly fast compared to RandomForest and classic GradientBoosting, while fitting better. It is our clear winner.","01860533":"## III. Features engineering <a id=\"three\"><\/a>","0cbe050f":"### e. Correlations and dimensionality reductions <a id=\"three-e\"><\/a>","e0cd9607":"The competition dataset is based on the 2016 NYC Yellow Cab trip record data made available in Big Query on Google Cloud Platform. The data was originally published by the NYC Taxi and Limousine Commission (TLC). The data was sampled and cleaned for the purposes of this playground competition. Based on individual trip attributes, participants should **predict the duration of each trip in the test set.**\n\n### File descriptions\n\n**train.csv** - the training set (contains 1458644 trip records)  \n**test.csv** - the testing set (contains 625134 trip records)  \n**sample_submission.csv** - a sample submission file in the correct format\n\n### Table of Contents:\n**[I. Data loading and overview](#one)**\n- [a. Loading the data](#one-a)\n- [b. Overview](#one-b)\n\n**[II. Data cleaning](#two)**\n- [a. Duplicated and missing values](#two-a)\n- [b. Deal with outliers](#two-b)\n\n**[III. Features engineering](#three)**\n- [a. Target](#three-a)\n- [b. Deal with categorical features](#three-b)\n- [c. Deal with dates](#three-c)\n- [d. Distance and speed creations](#three-d)\n- [e. Correlations and dimensionality reductions](#three-e)\n\n**[IV. Model selection](#four)**\n- [a. Split](#four-a)\n- [b. Metrics](#four-b)\n- [c. Models](#four-c)\n- [d. Cross-validation](#four-d)\n\n**[V. Hyperparameters tuning](#five)**\n\n**[VI. Training and predictions](#six)**","4f14de87":"### c. Deal with dates <a id=\"three-c\"><\/a>","004ebfc0":"### d. Cross-validation <a id=\"four-d\"><\/a>","f150c8d3":"# New York City Taxi Trip Duration","26aab99e":"### b. Overview <a id=\"one-b\"><\/a>","f0a767e2":"The distribution is **right-skewed** so we can consider a log-transformation of `trip_duration` column.","3e97a0bf":"### a. Target <a id=\"three-a\"><\/a>","cdade6b7":"### b. Deal with outliers <a id=\"two-b\"><\/a>","429c56b9":"## VI. Training and predictions <a id=\"six\"><\/a>","fecf1e2d":"Colonne | Description\n------- | -------\n**id** | a unique identifier for each trip  \n**vendor_id** | a code indicating the provider associated with the trip record  \n**pickup_datetime** | date and time when the meter was engaged  \n**dropoff_datetime** | date and time when the meter was disengaged  \n**passenger_count** | the number of passengers in the vehicle (driver entered value)  \n**pickup_longitude** | the longitude where the meter was engaged  \n**pickup_latitude** | the latitude where the meter was engaged  \n**dropoff_longitude** | the longitude where the meter was disengaged  \n**dropoff_latitude** | the latitude where the meter was disengaged  \n**store_and_fwd_flag** | This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server (Y=store and forward; N=not a store and forward trip)  \n**trip_duration** | duration of the trip in seconds  \n\n*Disclaimer: The decision was made to not remove dropoff coordinates from the dataset order to provide an expanded set of variables to use in Kernels.*","844ed63d":"## IV. Model selection <a id=\"four\"><\/a>"}}