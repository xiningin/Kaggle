{"cell_type":{"d1be5c4a":"code","339776c5":"code","ccef0f17":"code","35874ebf":"code","396238fd":"code","b5946701":"code","c42777c4":"code","82794cae":"code","bd73a776":"code","2fadcf09":"code","7d96bfe8":"code","79522d8a":"code","e679d677":"code","c3636fac":"code","396dfc4b":"code","0ff67762":"code","c198dfb2":"code","96c20a95":"code","05cdb5eb":"code","92097ae2":"code","77e8e362":"code","56ebbcbe":"code","2aa83870":"code","bde24ce4":"code","9fcb08e1":"code","2b6d4efa":"code","b15304a3":"code","01a461b9":"code","0a450dcb":"code","2d782c0a":"code","59dd68ed":"code","31847757":"code","f3983c0b":"code","f563b1c8":"code","bde58d74":"markdown","a61fef90":"markdown","63dc7cb7":"markdown","c52b00ed":"markdown","b8a0fde6":"markdown","df491c4a":"markdown","f3a0f8c8":"markdown","7f7de02a":"markdown","61cd5fc1":"markdown","19815b46":"markdown","62e9ea88":"markdown"},"source":{"d1be5c4a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom xgboost import XGBRegressor\nimport random\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","339776c5":"train = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\", low_memory=False)\ntest = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\", low_memory=False)\ntrain.info(memory_usage=\"deep\")","ccef0f17":"test.info(memory_usage=\"deep\")","35874ebf":"train.head(10)","396238fd":"plt.figure(figsize=(25, 25))\nfor i, col in enumerate(list(train.columns)):\n    plt.subplot(7, 4, i+1)\n    sns.histplot(train[col], kde=True, bins=10)","b5946701":"plt.figure(figsize=(25, 25))\nfor i, col in enumerate(list(train.columns)):\n    plt.subplot(7, 4, i+1)\n    col_target = train.groupby(col,as_index=False)['target'].mean()\n    sns.scatterplot(data = col_target,x=col,y='target')","c42777c4":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","82794cae":"# Comparing the datasets length\nfig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","bd73a776":"# Statistical description of the train dataset\ntrain.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]).T","2fadcf09":"# Checking if there are missing values in the datasets\ntrain.isna().sum().sum(), test.isna().sum().sum()","7d96bfe8":"fig, ax = plt.subplots(figsize=(16, 8))\n\nbars = ax.hist(train[\"target\"],\n               bins=100,\n               color=\"palevioletred\",\n               edgecolor=\"black\")\nax.set_title(\"Target distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","79522d8a":"print(f\"{(train['target'] < 5).sum() \/ len(train) * 100:.3f}% of the target values are less than 5\")","e679d677":"# Lists of categorical and numerical feature columns\ncat_features = [\"cat\" + str(i) for i in range(10)]\nnum_features = [\"cont\" + str(i) for i in range(14)]","c3636fac":"# Combined dataframe containing numerical features only\ndf = pd.concat([train[num_features], test[num_features]], axis=0)\ncolumns = df.columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.3)\n\n# Plots counter\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns): # If there is no more data columns to make plots from\n            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n        else:\n            # Train data histogram\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            # Test data histogram\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\n# plt.suptitle(\"Numerical feature values distribution in both datasets\", y=0.99)\nplt.show();","396dfc4b":"# Combined dataframe containing categorical features only\ndf = pd.concat([train[cat_features], test[cat_features]], axis=0)\ncolumns = df.columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.2, wspace=0.25)\n\n# Plots counter\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(cat_features): # If there is no more data columns to make plots from\n            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n        else:\n\n            values = df[cat_features[i]].value_counts().sort_index(ascending=False).index\n            bars_pos = np.arange(0, len(values))\n            if len(values)<4:\n                height=0.1\n            else:\n                height=0.3\n\n            bars1 = axs[r, c].barh(bars_pos+height\/2,\n                                   [train[train[cat_features[i]]==x][cat_features[i]].count() for x in values],\n                                   height=height,\n                                   color=\"teal\",\n                                   edgecolor=\"black\",\n                                   label=\"Train Dataset\")\n            bars2 = axs[r, c].barh(bars_pos-height\/2,\n                                   [test[test[cat_features[i]]==x][cat_features[i]].count() for x in values],\n                                   height=height,\n                                   color=\"salmon\",\n                                   edgecolor=\"black\",\n                                   label=\"Test Dataset\")\n            y_labels = [str(x) for x in values]\n\n            axs[r, c].set_title(cat_features[i], fontsize=14, pad=1)\n            axs[r, c].set_xlim(0, len(train[\"id\"])+50)\n            axs[r, c].set_yticks(bars_pos)\n            axs[r, c].set_yticklabels(y_labels)\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"x\")\n            axs[r, c].legend(fontsize=12)\n            axs[r, c].margins(0.1, 0.02)\n                                  \n        i+=1\n\n#plt.suptitle(\"Categorical feature values distribution in both datasets\", y=0.99)\nplt.show();","0ff67762":"# Bars position should be numerical because there will be arithmetical operations with them\nbars_pos = np.arange(len(cat_features))\n\nwidth=0.3\nfig, ax = plt.subplots(figsize=(14, 6))\n# Making two bar objects. One is on the left from bar position and the other one is on the right\nbars1 = ax.bar(bars_pos-width\/2,\n               train[cat_features].nunique().values,\n               width=width,\n               color=\"darkorange\", edgecolor=\"black\")\nbars2 = ax.bar(bars_pos+width\/2,\n               train[cat_features].nunique().values,\n               width=width,\n               color=\"steelblue\", edgecolor=\"black\")\nax.set_title(\"Amount of values in categorical features\", fontsize=20, pad=15)\nax.set_xlabel(\"Categorical feature\", fontsize=15, labelpad=15)\nax.set_ylabel(\"Amount of values\", fontsize=15, labelpad=15)\nax.set_xticks(bars_pos)\nax.set_xticklabels(cat_features, fontsize=12)\nax.tick_params(axis=\"y\", labelsize=12)\nax.grid(axis=\"y\")\nplt.margins(0.01, 0.05)","c198dfb2":"# Checking if test data doesn't contain categories that are not present in the train dataset\nfor col in cat_features:\n    print(set(train[col].value_counts().index) == set(test[col].value_counts().index))","96c20a95":"# Plot dataframe\ndf = train.drop(\"id\", axis=1)\n\n# Encoding categorical features with OrdinalEncoder\nfor col in cat_features:\n    encoder = OrdinalEncoder()\n    df[col] = encoder.fit_transform(np.array(df[col]).reshape(-1, 1))\n\n# Calculatin correlation values\ndf = df.corr().round(2)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(14,14))\nax = sns.heatmap(df, annot=True, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"normal\", \"fontsize\":9})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","05cdb5eb":"columns = train.drop([\"id\", \"target\"], axis=1).columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 4\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.3)\n# train['target'] = train['target'].apply(lambda x: -x)\n\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            scatter = axs[r, c].scatter(train[columns[i]].values,\n                                        train['target'].apply(lambda x: -x),\n                                        color=random.choice(colors))\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n                                  \n        i+=1\n# plt.suptitle(\"Features vs target\", y=0.99)\nplt.show();","92097ae2":"# Encoding categorical features with OrdinalEncoder\nfor col in cat_features:\n    encoder = OrdinalEncoder()\n    train[col] = encoder.fit_transform(np.array(train[col]).reshape(-1, 1))\n    test[col] = encoder.transform(np.array(test[col]).reshape(-1, 1))","77e8e362":"train[cat_features].head()","56ebbcbe":"test[cat_features].head()","2aa83870":"train['target'] = train['target'].apply(lambda x: -x)\ntrain['target'].hist()","bde24ce4":"# test['exp_y'] = test['target'].apply(lambda x: np.expm1(x))","9fcb08e1":"X = train.drop([\"id\", \"target\"], axis=1)\nX_test = test.drop([\"id\"], axis=1)\ny = train[\"target\"]","2b6d4efa":"# Model hyperparameters\nxgb_params = {'n_estimators': 60000,\n              'learning_rate': 0.35,\n              'subsample': 0.926,\n              'colsample_bytree': 0.84,\n              'max_depth': 7,\n              'booster': 'gbtree', \n              'reg_lambda': 35.1,\n              'reg_alpha': 34.9,\n              'random_state': 100,\n              'n_jobs': 4,\n             'tree_method': \"gpu_hist\",\n             }","b15304a3":"%%time\n# Setting up fold parameters\nsplits = 10\nskf = KFold(n_splits=splits, shuffle=True, random_state=100)\n\n# Creating an array of zeros for storing \"out of fold\" predictions\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\ntotal_mean_rmse = 0\n\n# Generating folds and making training and prediction for each of 10 folds\nfor num, (train_idx, valid_idx) in enumerate(skf.split(X)):\n    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n              verbose=False,\n              # These three parameters will stop training before a model starts overfitting \n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              )\n    \n    # Getting mean test data predictions (i.e. devided by number of splits)\n    preds += model.predict(X_test) \/ splits\n#     preds = min(model.predict(X_test))\n\n    # Getting mean feature importances (i.e. devided by number of splits)\n    model_fi += model.feature_importances_ \/ splits\n    \n    # Getting validation data predictions. Each fold model makes predictions on an unseen data.\n    # So in the end it will be completely filled with unseen data predictions.\n    # It will be used to evaluate hyperparameters performance only.\n    oof_preds[valid_idx] = model.predict(X_valid)\n    \n    # Getting score for a fold model\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx]))\n    print(f\"Fold {num} RMSE: {fold_rmse}\")\n\n    # Getting mean score of all fold models (i.e. devided by number of splits)\n    total_mean_rmse += fold_rmse \/ splits\n    \nprint(f\"\\nOverall RMSE: {total_mean_rmse}\")","01a461b9":"# plt.figure(figsize=(10,7))\n# plt.subplot(1,2,1)\n# # ax1.set(title='y_valid')\n# ax1 = plt.hist(y_valid)\n# plt.subplot(1,2,2)\n# # ax2.set(title='y_pred')\n# ax2 = plt.hist(oof_preds)\n# plt.show()\n# # y_valid.hist()\n","0a450dcb":"# oof_preds[:30000].shape","2d782c0a":"# lab = model.predict(X_valid)","59dd68ed":"# plt.scatter(lab, y_valid);plt.ylabel('Expected');plt.xlabel('Predicted');plt.show()\n","31847757":"# Creating a dataframe to be used for plotting\ndf = pd.DataFrame()\ndf[\"Feature\"] = X.columns\n# Extracting feature importances from the trained model\ndf[\"Importance\"] = model_fi \/ model_fi.sum()\n# Sorting the dataframe by feature importance\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)","f3983c0b":"fig, ax = plt.subplots(figsize=(13, 10))\nbars = ax.barh(df[\"Feature\"], df[\"Importance\"], height=0.4,\n               color=\"mediumorchid\", edgecolor=\"black\")\nax.set_title(\"Feature importances\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature name\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(df[\"Feature\"])\nax.set_yticklabels(df[\"Feature\"], fontsize=15)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\n# Adding labels on top\nax2 = ax.secondary_xaxis('top')\nax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\n\n# Inverting y axis direction so the values are decreasing\nplt.gca().invert_yaxis()","f563b1c8":"predictions = pd.DataFrame()\npredictions[\"id\"] = test[\"id\"]\npreds = preds * (-1)\npredictions[\"target\"] = preds\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","bde58d74":"# **Predictions submission**","a61fef90":"# **Model training**","63dc7cb7":"# **EDA**","c52b00ed":"# **Feature importances**","b8a0fde6":"So the datasets are pretty well balanced. Let's look at feature correlation.","df491c4a":"As you can see, target column is very weakly correlated with all features.\n\nLet's visualize each feature vs target.","f3a0f8c8":"Let's check if the datasets have different amount of categories in categorical features.","7f7de02a":"# **Data preprocessing**","61cd5fc1":"# **Data import**","19815b46":"The dataset contains categorical and numerical values. Let's see values distribution for these categories.","62e9ea88":"There are no missing value in the both datasets.\n\nLet's check target distribution."}}