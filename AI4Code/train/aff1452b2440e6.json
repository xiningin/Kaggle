{"cell_type":{"93fdf2dd":"code","a9ae771b":"code","861ad504":"code","9b6fb2e6":"code","3fac892e":"code","7e7b30ef":"code","561428e8":"code","90e9dff0":"code","1f29c7ae":"code","6d0227dd":"code","67bcbdef":"code","801ea85f":"code","9a88a1e5":"code","57b38c81":"code","82836b41":"code","e95c919e":"code","0982e9ab":"code","7a32d7cb":"code","e13bef52":"code","32f514e3":"code","d3b9cb20":"code","21434367":"code","0b947c54":"code","d8ca8224":"code","67bd962c":"code","1538f315":"markdown","ca38e93e":"markdown","97ab6539":"markdown","d91999c7":"markdown","005fe3f6":"markdown","86829a6e":"markdown","fa46563a":"markdown","1b6f9917":"markdown","42931ae5":"markdown","e06d0c09":"markdown","b6f56650":"markdown","257ff81a":"markdown","d2c9a836":"markdown","8a125b60":"markdown","6d0a8387":"markdown","56afb939":"markdown","894e32d1":"markdown","1b268e01":"markdown","0f213fdc":"markdown"},"source":{"93fdf2dd":"import pandas as pd\nimport seaborn as sns \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom numpy.linalg import eig \nfrom sklearn.decomposition import PCA \nimport plotly.express as px\n%matplotlib inline","a9ae771b":"df = pd.read_csv(\"..\/input\/class12\/Class2.csv\") \ndf1 = pd.read_csv(\"..\/input\/class12\/Class2.csv\")\ndf","861ad504":"df.drop([\"id\",\"gender\"],axis=1,inplace=True)","9b6fb2e6":"df","3fac892e":"df_std = df-df.mean()\/df.std()","7e7b30ef":"df_std","561428e8":"df.cov()#covariance matrix","90e9dff0":"df.var()#variance of matrix","1f29c7ae":"a = np.array(df).reshape(200,2)","6d0227dd":"values,vectors = eig(df.cov())","67bcbdef":"values","801ea85f":"vectors","9a88a1e5":"eigen_vectors = pd.DataFrame(vectors,columns=['e1','e2','e3','e4']) \neigen_vectors","57b38c81":"eigen_values = pd.DataFrame(values.reshape(1,4),columns=['test1','test2','test3','test4'])","82836b41":"eigen_values","e95c919e":"vectors = eigen_vectors[['e1','e2']].to_numpy()","0982e9ab":"pca_components = pd.DataFrame(np.array(df)@np.array(vectors),columns=['pc1','pc2'])","7a32d7cb":"pca_components","e13bef52":"pca = PCA(n_components=4)","32f514e3":"components = pca.fit_transform(df) \ncomponents","d3b9cb20":"labels =  pca.explained_variance_ratio_*100","21434367":"labels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n} \nprint(labels)","0b947c54":"len(components[0:,2])","d8ca8224":"component = pd.DataFrame(components,columns=['A','B','C','D']) ","67bd962c":"fig = px.scatter_matrix(components,labels = labels,dimensions=range(4),color=df1['gender']) \nfig.show()","1538f315":"# 3. Calculate eigen values and eigen vectors :  \n\nAn eigenvector is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it. The corresponding eigenvalue is the factor by which the eigenvector is scaled. \n       \nLet A be a square matrix (in our case the covariance matrix), \u03bd a vector and \u03bb a scalar that satisfies A\u03bd = \u03bb\u03bd, then \u03bb is called eigenvalue associated with eigenvector \u03bd of A. \n    \nRearranging the above equation, \n    \n                         A\u03bd-\u03bb\u03bd =0 ; (A-\u03bbI)\u03bd = 0\n","ca38e93e":"# Step 4: Sort eigenvalues and their corresponding eigenvectors.","97ab6539":"Listed below are the 5 general steps for performing a linear discriminant analysis; we will explore them in more detail in the following sections.\n\n\n1.Compute the d-dimensional mean vectors for the different classes from the dataset.\n    \n2.Compute the scatter matrices (in-between-class and within-class scatter matrix).\n    \n3.Compute the eigenvectors (ee1,ee2,...,eed) and corresponding eigenvalues (\u03bb\u03bb1,\u03bb\u03bb2,...,\u03bb\u03bbd) for the scatter matrices.\n    \n4.Sort the eigenvectors by decreasing eigenvalues and choose k eigenvectors with the largest eigenvalues to form a d\u00d7k dimensional matrix W (where every column represents an eigenvector).\n    \n5.Use this d\u00d7k eigenvector matrix to transform the samples onto the new subspace. This can be summarized by the matrix multiplication: YY=XX\u00d7WW (where XX is a n\u00d7d-dimensional matrix representing the n samples, and yy are the transformed n\u00d7k-dimensional samples in the new subspace).","d91999c7":"# **LDA TOPIC WILL BE UPDATED SOON...***","005fe3f6":"<img src=\"https:\/\/lh4.googleusercontent.com\/6NfVmmG39n41HvHiER7x-mBs8sjIDtAZnzZdt4cBUVU2Jw4chLOVEgYs28eqFq2w6P3Ow2sSDpFFkJ3VwCfqcEEqs_lbkEhjPZ36hOu-gAh6adJ5kgSnVgCA0LzDrCP4WeIhXAM\">","86829a6e":"<img src=\"https:\/\/miro.medium.com\/max\/333\/1*X4YeGxtzOhnnOWBfoBBJfA.png\">","fa46563a":"# Steps Involved in the PCA : \n\nStep 1: Standardize the dataset. \n    \nStep 2: Calculate the covariance matrix for the features in the dataset. \n    \nStep 3: Calculate the eigenvalues and eigenvectors for the covariance matrix. \n    \nStep 4: Sort eigenvalues and their corresponding eigenvectors. \n    \nStep 5: Pick k eigenvalues and form a matrix of eigenvectors. \n    \nStep 6: Transform the original matrix.\n","1b6f9917":"<img src=\"https:\/\/sebastianraschka.com\/images\/blog\/2014\/linear-discriminant-analysis\/lda_1.png\"><\/img>","42931ae5":"# Principal Component  Analysis :","e06d0c09":"First, we need to standardize the dataset and for that, we need to calculate the mean and standard deviation for each feature.","b6f56650":"# Steps involved in the LDA approach :","257ff81a":"<img src=\"https:\/\/builtin.com\/sites\/default\/files\/inline-images\/Principal%20Component%20Analysis%20second%20principal.gif\">","d2c9a836":"As there are as many principal components as there are variables in the data, principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. For example, let\u2019s assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? Yes, it\u2019s approximately the line that matches the purple marks because it goes through the origin and it\u2019s the line in which the projection of the points (red dots) is the most spread out. Or mathematically speaking, it\u2019s the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin).","8a125b60":"# Step 6: Transform the original matrix.","6d0a8387":"# Step 5: Pick k eigenvalues and form a matrix of eigenvectors","56afb939":"# Linear Discriminant Analysis : \n\nLinear Discriminant Analysis (LDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (\u201ccurse of dimensionality\u201d) and also reduce computational costs. \n    \nBoth Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are linear transformation techniques that are commonly used for dimensionality reduction. PCA can be described as an \u201cunsupervised\u201d algorithm, since it \u201cignores\u201d class labels and its goal is to find the directions (the so-called principal components) that maximize the variance in a dataset. In contrast to PCA, LDA is \u201csupervised\u201d and computes the directions (\u201clinear discriminants\u201d) that will represent the axes that that maximize the separation between multiple classes.","894e32d1":"# Assumptions of LDA :   \nLDA assumes:\n\n1.Each feature (variable or dimension or attribute) in the dataset is a gaussian distribution. In other words, each feature in the dataset is shaped like a bell-shaped curve. \n\n2. Each feature has the same variance, the value of each feature varies around the mean with the same amount on average. \n\n\n3. Each feature is assumed to be randomly sampled.\n\n4. Lack of multicollinearity in independent features. Increase in correlations between independent features and the power of prediction decreases.","1b268e01":"# 1. Standardize the Dataset :\nAssume we have the below dataset which has 4 features and a total of 100 training examples.","0f213fdc":"# Step 2: Create the covariance matrix :\n\nNext, we\u2019ll create the covariance matrix for this dataset using the numpy function cov(), specifying that bias = True so that we are able to calculate the population covariance matrix. "}}