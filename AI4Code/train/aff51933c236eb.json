{"cell_type":{"43b65504":"code","89531e82":"code","7d7f9dc3":"code","c0ad50c5":"code","3da6c352":"code","75a0583a":"code","87f6f6f8":"code","7cb7fc0a":"code","4be76280":"code","92b740aa":"code","e6265c58":"code","cd54d33b":"code","cd9c1667":"code","eebdbd65":"code","dedc9210":"code","72923ef8":"code","f8652f03":"code","f998b522":"code","602271ac":"code","ce06c4b3":"code","d9000add":"code","87daf3f7":"code","54447290":"code","97d206bb":"code","0560cc5d":"markdown","ee208cae":"markdown","c5f9117e":"markdown","e95116dd":"markdown","bc35ff3f":"markdown","56b9eca7":"markdown","ba9385d0":"markdown","89fe8b81":"markdown","35a8baa2":"markdown","9f3c05fd":"markdown"},"source":{"43b65504":"import pandas as pd\nimport numpy as np\nimport keras\nimport tensorflow as tf\nfrom tqdm import tqdm \nimport cv2\nimport os, datetime\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow.keras as K\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint ","89531e82":"\ntrain=pd.read_csv('\/kaggle\/input\/arabic-hwr-ai-pro-intake1\/train.csv',dtype={'id': str,'label':str})\ntest=pd.read_csv('\/kaggle\/input\/arabic-hwr-ai-pro-intake1\/test.csv',dtype={'id': str})\ndisplay(train.head())\nprint(train.shape)\nprint(test.shape)\nprint(train.dtypes)\n# train=train.astype(str)\n# test=test.astype(str)\n","7d7f9dc3":"train.isnull().sum()","c0ad50c5":"train['label'].value_counts()","3da6c352":"X=[]\nY=[]","75a0583a":"# def load_data(path):\n#     for image in tqdm(os.listdir(path)):\n#         image_path = os.path.join(path,image)\n#         img = cv2.imread(image_path)\n#         X_train.append(np.array(img))\n#     return X_train","87f6f6f8":"def load_data(path):\n    for image in train['id']:\n        image_path = path+image+'.png'\n        img = cv2.imread(image_path)\n        X.append(np.array(img))\n    return X","7cb7fc0a":"X=load_data('\/kaggle\/input\/arabic-hwr-ai-pro-intake1\/train\/')","4be76280":"Y=train['label'].astype(int).tolist()","92b740aa":"plt.figure(figsize=(10,10))\nplt.imshow(X[31])\nY[31]","e6265c58":"X_train, X_valid, y_train,y_valid = train_test_split(X, Y,test_size=0.33, random_state=0,shuffle=True,stratify=Y)\nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_valid = np.array(X_valid)\n\ny_valid = np.array(y_valid)\n\ny_train=to_categorical(y_train-1, num_classes=28)\ny_valid=to_categorical(y_valid-1, num_classes=28)\n\n\ninput_t = K.Input(shape=(224, 224, 3))\nres_model = K.applications.ResNet50(weights='imagenet',include_top=False,input_tensor=input_t)\nfor layer in res_model.layers[:143]:\n        layer.trainable = False\n\nto_res = (224, 224)\n\nmodel = K.models.Sequential()\nmodel.add(K.layers.Lambda(lambda image: tf.image.resize(image, to_res)))\nmodel.add(res_model)\nmodel.add(K.layers.Flatten())\nmodel.add(K.layers.BatchNormalization())\nmodel.add(K.layers.Dense(256, activation='relu'))\nmodel.add(K.layers.Dropout(0.5))\nmodel.add(K.layers.BatchNormalization())\nmodel.add(K.layers.Dense(128, activation='relu'))\nmodel.add(K.layers.Dropout(0.5))\nmodel.add(K.layers.BatchNormalization())\nmodel.add(K.layers.Dense(64, activation='relu'))\nmodel.add(K.layers.Dropout(0.5))\nmodel.add(K.layers.BatchNormalization())\nmodel.add(K.layers.Dense(28, activation='softmax'))\n\n\nlogdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=K.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0),\n              metrics=['accuracy'])","cd54d33b":"from keras.preprocessing.image import ImageDataGenerator\n\n# create and configure augmented image generator\ndatagen_train = ImageDataGenerator(\n   featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        #rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n# fit augmented image generator on data\ndatagen_train.fit(X_train)","cd9c1667":"from keras.callbacks import ModelCheckpoint   \nfrom keras.callbacks import ReduceLROnPlateau\nbatch_size = 64\nepochs = 80\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n# train the model\ncheckpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\nhistory=model.fit_generator(datagen_train.flow(X_train,y_train, batch_size=batch_size),\n                    steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n                    epochs=epochs, verbose=2, callbacks=[learning_rate_reduction],\n                    validation_data=(X_valid, y_valid))\nmodel.save(\"model.h5\")","eebdbd65":"%matplotlib inline\n\nimport matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\nacc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(len(acc))\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Training and validation accuracy')\nplt.legend(['train', 'val'], loc='upper left')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.figure()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Training and validation loss')\nplt.legend(['train', 'val'], loc='upper left')\nplt.ylabel('Loss')\nplt.xlabel('epoch')\n\n\nplt.title('Training and validation loss')","dedc9210":"# %load_ext tensorboard\n# %tensorboard --logdir logs\n# %reload_ext tensorboard","72923ef8":"X=[]","f8652f03":"test.head()","f998b522":"def load_data(path):\n    for image in test['id']:\n        image_path = path+image+'.png'\n        img = cv2.imread(image_path)\n        X.append(np.array(img))\n    return X","602271ac":"test.head()","ce06c4b3":"X_test=load_data('\/kaggle\/input\/arabic-hwr-ai-pro-intake1\/test\/')","d9000add":"X_test=np.array(X_test)","87daf3f7":"pred=model.predict(X_test)\npreds=np.argmax(pred,axis=-1)","54447290":"test['label']=preds\ntest['label']=test['label']+1","97d206bb":"test.to_csv('\/kaggle\/working\/submission.csv', index=False)","0560cc5d":"# \u2728 Hi There","ee208cae":"# \u2728 Character Recognition Model","c5f9117e":"# \u2728 Data Augmentation","e95116dd":"# We're Serial Kernels \ud83d\udc32","bc35ff3f":"# \u2728 Exploring","56b9eca7":"# \u2728 Training","ba9385d0":"# \u2728 Loading Data","89fe8b81":"# \u2728 Importing Libraries","35a8baa2":"# \u2728 Generating Submission File","9f3c05fd":"# \u2728 Testing"}}