{"cell_type":{"3aba5500":"code","c4a2b603":"code","07c6790e":"code","3bd82c44":"code","9e2f3adf":"code","cbbd1dbd":"code","daea865e":"code","83a8b3f7":"code","06e61c5a":"code","a2dd1e8f":"code","19896945":"code","ff235c5c":"code","33727ef7":"code","29199f42":"code","29f09f5b":"code","9908f2d5":"code","2275419c":"code","94e4fa6d":"code","464152f4":"code","b104b041":"code","c15df2e9":"code","9c7e3be7":"code","a6872ed1":"code","78adb916":"code","7c5bd539":"code","b6640a2f":"code","5743e552":"code","ea8eda71":"code","4a6ab888":"code","1efa4ff9":"code","2b9ab8e2":"code","7eb5def1":"code","03e9404d":"code","b3afb14e":"code","408cb437":"code","4e384d38":"code","1af0bfd4":"code","268695bc":"code","b341844a":"code","188aae42":"code","80b29a60":"code","0ab5e63c":"code","a9221ace":"code","c42af7f9":"code","c504c69c":"code","36487d00":"code","288e39c1":"code","6dcc783b":"code","c2402c82":"code","36e038a2":"code","97026dc6":"code","712199a0":"code","c8fa59e2":"code","4513b10d":"code","d7930f1b":"code","d29a33b3":"code","d4ac4dd2":"code","40186db4":"code","dd291917":"code","9c66c304":"code","4e8cc4bd":"code","06dae2ee":"code","1cd3c8e1":"code","04538dbe":"code","94d671d4":"code","2bc63f45":"code","415401d6":"code","428bc587":"code","94714fef":"code","c0e6c4a9":"code","466abf48":"code","0d8e54a6":"code","a1481fd7":"code","308898b9":"code","63faa2bc":"code","2eab9782":"code","68391799":"code","52b8af2d":"markdown","01f23f58":"markdown","3039da68":"markdown","15fa035d":"markdown","55ac3354":"markdown","bde0a341":"markdown","81487e3d":"markdown","626b1f90":"markdown","ce659015":"markdown","5be82f39":"markdown","b357d3a8":"markdown","988291cb":"markdown","aee0185c":"markdown","70002cdb":"markdown","95cc0a69":"markdown","a18c1bcc":"markdown","1d9e31a2":"markdown","57497a77":"markdown","765f45e7":"markdown","4b0ce35f":"markdown","4edb29b2":"markdown","cad6d891":"markdown","20fe9289":"markdown","5d042efb":"markdown","284a2178":"markdown","83270598":"markdown","f737ef94":"markdown","ccb18471":"markdown"},"source":{"3aba5500":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib\n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nfrom IPython.display import display\n","c4a2b603":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","07c6790e":"df_train.shape","3bd82c44":"df_test.shape","9e2f3adf":"df_train.info()","cbbd1dbd":"df_train.describe()","daea865e":"pd.options.display.max_rows = None\ndisplay(100-(df_train.isnull().sum()*100\/len(df_train)))","83a8b3f7":"pd.options.display.max_rows = None\ndisplay(100-(df_test.isnull().sum()*100\/len(df_test)))","06e61c5a":"df_train.columns","a2dd1e8f":"plt.figure(figsize=(20,8))\n\nplt.subplot(1,2,1)\nplt.title('House Price Distribution Plot')\nsns.distplot(df_train.SalePrice,color ='red')\n\nplt.subplot(1,2,2)\nplt.title('House Price Spread')\nax = sns.boxplot(y = df_train.SalePrice ,color = 'pink' )\n\nplt.show()","19896945":"plt.figure(figsize = (30, 30))\nsns.heatmap(df_train.corr(), annot = True, cmap=\"PiYG\")\nplt.show()","ff235c5c":"sol = df_test['Id']","33727ef7":"#Dropping columns with less than 60% missing data\ndf_train = df_train.drop(columns=['Id'])\ndf_train = df_train.drop(columns=['Alley'])\ndf_train = df_train.drop(columns=['FireplaceQu'])\ndf_train = df_train.drop(columns=['PoolQC'])\ndf_train = df_train.drop(columns=['Fence'])\ndf_train = df_train.drop(columns=['MiscFeature'])\n\ndf_test = df_test.drop(columns=['Id'])\ndf_test = df_test.drop(columns=['Alley'])\ndf_test = df_test.drop(columns=['FireplaceQu'])\ndf_test = df_test.drop(columns=['PoolQC'])\ndf_test = df_test.drop(columns=['Fence'])\ndf_test = df_test.drop(columns=['MiscFeature'])","29199f42":"#Imputing numerical data with mean value\ndf_train[\"LotFrontage\"] = df_train[\"LotFrontage\"].replace(np.NaN, df_train[\"LotFrontage\"].mean())\ndf_train[\"MasVnrArea\"] = df_train[\"MasVnrArea\"].replace(np.NaN, df_train[\"MasVnrArea\"].mean())\ndf_train[\"GarageYrBlt\"] = df_train[\"GarageYrBlt\"].replace(np.NaN, df_train[\"GarageYrBlt\"].mean())\n\ndf_test[\"LotFrontage\"] = df_test[\"LotFrontage\"].replace(np.NaN, df_test[\"LotFrontage\"].mean())\ndf_test[\"MasVnrArea\"] = df_test[\"MasVnrArea\"].replace(np.NaN, df_test[\"MasVnrArea\"].mean())\ndf_test[\"GarageYrBlt\"] = df_test[\"GarageYrBlt\"].replace(np.NaN, df_test[\"GarageYrBlt\"].mean())\ndf_test[\"BsmtFinSF1\"] = df_test[\"BsmtFinSF1\"].replace(np.NaN, df_test['BsmtFinSF1'].mean())\ndf_test[\"BsmtFinSF2\"] = df_test[\"BsmtFinSF2\"].replace(np.NaN, df_test['BsmtFinSF2'].mean())\ndf_test[\"BsmtFullBath\"] = df_test[\"BsmtFullBath\"].replace(np.NaN, df_test['BsmtFullBath'].mean())\ndf_test[\"BsmtHalfBath\"] = df_test[\"BsmtHalfBath\"].replace(np.NaN, df_test['BsmtHalfBath'].mean())\ndf_test[\"GarageCars\"] = df_test[\"GarageCars\"].replace(np.NaN, df_test['GarageCars'].mean())\ndf_test[\"GarageArea\"] = df_test[\"GarageArea\"].replace(np.NaN, df_test[\"GarageArea\"].mean())\ndf_test[\"BsmtUnfSF\"] = df_test[\"BsmtUnfSF\"].replace(np.NaN, df_test['BsmtUnfSF'].mean())\ndf_test[\"TotalBsmtSF\"] = df_test[\"TotalBsmtSF\"].replace(np.NaN, df_test['TotalBsmtSF'].mean())","29f09f5b":"#Imputin missing categorical data values with mode value\ndf_train[\"BsmtQual\"] = df_train[\"BsmtQual\"].replace(np.NaN, df_train[\"BsmtQual\"].mode()[0][:])\ndf_train[\"BsmtExposure\"] = df_train[\"BsmtExposure\"].replace(np.NaN, df_train[\"BsmtExposure\"].mode()[0][:])\ndf_train[\"BsmtFinType1\"] = df_train[\"BsmtFinType1\"].replace(np.NaN, df_train[\"BsmtFinType1\"].mode()[0][:])\ndf_train[\"BsmtCond\"] = df_train[\"BsmtCond\"].replace(np.NaN, df_train[\"BsmtCond\"].mode()[0][:])\ndf_train[\"BsmtFinType2\"] = df_train[\"BsmtFinType2\"].replace(np.NaN, df_train[\"BsmtFinType2\"].mode()[0][:])\ndf_train[\"Electrical\"] = df_train[\"Electrical\"].replace(np.NaN, df_train[\"Electrical\"].mode()[0][:])\ndf_train[\"GarageType\"] = df_train[\"GarageType\"].replace(np.NaN, df_train[\"GarageType\"].mode()[0][:])\ndf_train[\"GarageFinish\"] = df_train[\"GarageFinish\"].replace(np.NaN, df_train[\"GarageFinish\"].mode()[0][:])\ndf_train[\"GarageQual\"] = df_train[\"GarageQual\"].replace(np.NaN, df_train[\"GarageQual\"].mode()[0][:])\ndf_train[\"GarageCond\"] = df_train[\"GarageCond\"].replace(np.NaN, df_train[\"GarageCond\"].mode()[0][:])\ndf_train[\"MasVnrType\"] = df_train[\"MasVnrType\"].replace(np.NaN, df_train['MasVnrType'].mode()[0][:])\n\ndf_test[\"BsmtQual\"] = df_test[\"BsmtQual\"].replace(np.NaN, df_test[\"BsmtQual\"].mode()[0][:])\ndf_test[\"BsmtExposure\"] = df_test[\"BsmtExposure\"].replace(np.NaN, df_test[\"BsmtExposure\"].mode()[0][:])\ndf_test[\"BsmtFinType1\"] = df_test[\"BsmtFinType1\"].replace(np.NaN, df_test[\"BsmtFinType1\"].mode()[0][:])\ndf_test[\"BsmtCond\"] = df_test[\"BsmtCond\"].replace(np.NaN, df_test[\"BsmtCond\"].mode()[0][:])\ndf_test[\"BsmtFinType2\"] = df_test[\"BsmtFinType2\"].replace(np.NaN, df_test[\"BsmtFinType2\"].mode()[0][:])\ndf_test[\"Electrical\"] = df_test[\"Electrical\"].replace(np.NaN, df_test[\"Electrical\"].mode()[0][:])\ndf_test[\"GarageType\"] = df_test[\"GarageType\"].replace(np.NaN, df_test[\"GarageType\"].mode()[0][:])\ndf_test[\"GarageFinish\"] = df_test[\"GarageFinish\"].replace(np.NaN, df_test[\"GarageFinish\"].mode()[0][:])\ndf_test[\"GarageQual\"] = df_test[\"GarageQual\"].replace(np.NaN, df_test[\"GarageQual\"].mode()[0][:])\ndf_test[\"GarageCond\"] = df_test[\"GarageCond\"].replace(np.NaN, df_test[\"GarageCond\"].mode()[0][:])\ndf_test[\"MasVnrType\"] = df_test[\"MasVnrType\"].replace(np.NaN, df_test['MasVnrType'].mode()[0][:])\ndf_test[\"Utilities\"] = df_test[\"Utilities\"].replace(np.NaN, df_test['Utilities'].mode()[0][:])\ndf_test[\"MSZoning\"] = df_test[\"MSZoning\"].replace(np.NaN, df_test['MSZoning'].mode()[0][:])\ndf_test[\"SaleType\"] = df_test[\"SaleType\"].replace(np.NaN, df_test['SaleType'].mode()[0][:])\ndf_test[\"Exterior1st\"] = df_test[\"Exterior1st\"].replace(np.NaN, df_test['Exterior1st'].mode()[0][:])\ndf_test[\"Exterior2nd\"] = df_test[\"Exterior2nd\"].replace(np.NaN, df_test['Exterior2nd'].mode()[0][:])\ndf_test[\"KitchenQual\"] = df_test[\"KitchenQual\"].replace(np.NaN, df_test['KitchenQual'].mode()[0][:])\ndf_test[\"Functional\"] = df_test[\"Functional\"].replace(np.NaN, df_test['Functional'].mode()[0][:])","9908f2d5":"pd.options.display.max_rows = None\ndisplay(100-(df_test.isnull().sum()*100\/len(df_test)))\n#no missing data","2275419c":"pd.options.display.max_rows = None\ndisplay(100-(df_train.isnull().sum()*100\/len(df_train)))\n#no missing data","94e4fa6d":"cat_features = list(df_train.select_dtypes(include='object').columns)\nprint(cat_features)","464152f4":"for col in cat_features:\n    print(col,\"   \", df_train[col].unique())","b104b041":"for col in cat_features:\n    print(col,\" : \",len(df_train[col].unique()),\" labels\")","c15df2e9":"top_12 = [x for x in df_train['Neighborhood'].value_counts().sort_values(ascending=False).head(12).index]\ntop_12","9c7e3be7":"#Applying one hot encoding on the most frequently occuring top 12 results and labeling 1 if the condition is true\nfor label in top_12:\n    df_train[label] = np.where(df_train['Neighborhood']==label , 1, 0)\n    df_test[label] = np.where(df_test['Neighborhood']==label , 1, 0)\n\n","a6872ed1":"def one_hot_top_x(df, variable, top_x_labels):\n    for label in top_x_labels:\n        df[variable+'_'+label] = np.where(df[variable]==label , 1, 0)\n\n        \none_hot_top_x(df_train, 'Neighborhood',top_12)\ndf_train = df_train.drop(columns=['Neighborhood'])\n\none_hot_top_x(df_test, 'Neighborhood',top_12)\ndf_test = df_test.drop(columns=['Neighborhood'])\n\ndf_train.head()","78adb916":"#Applying one hot encoding on the most frequently occuring top 5 results and labeling 1 if the condition is true\nfeat_top_5 = [ 'Exterior1st', 'Exterior2nd', 'SaleType', 'Condition1', 'Condition2', 'HouseStyle', 'RoofMatl']\n\nfor feat in feat_top_5:\n    top_5 = [x for x in df_train[feat].value_counts().sort_values(ascending=False).head(5).index]\n    one_hot_top_x(df_train, feat ,top_5)\n    one_hot_top_x(df_test, feat ,top_5)\n\ndf_train = df_train.drop(columns=feat_top_5)\ndf_test = df_test.drop(columns=feat_top_5)","7c5bd539":"df_train.shape","b6640a2f":"df_test.shape","5743e552":"feat_one_hot=['MSZoning','Street','LotShape', 'LandContour', 'Utilities', 'LotConfig',\n 'LandSlope', 'BldgType', 'RoofStyle', 'MasVnrType', 'Foundation', 'BsmtFinType1',\n 'BsmtFinType2', 'Heating', 'CentralAir','Electrical','Functional',\n 'GarageType', 'PavedDrive', 'SaleCondition','GarageFinish']\n\n\ndef dummies(x,df):\n    temp = pd.get_dummies(df[x], prefix = x , drop_first = True).astype('int32')\n    df = pd.concat([df, temp], axis = 1)\n    df.drop([x], axis = 1, inplace = True)\n    return df\n\nfor ft in feat_one_hot:\n    df_train = dummies(ft,df_train)\n    df_test = dummies(ft,df_test)","ea8eda71":"df_train.shape","4a6ab888":"df_test.shape","1efa4ff9":"for x in df_train.columns:\n        if x not in df_test.columns:\n            print(x)","2b9ab8e2":"df_train.drop(columns = ['Utilities_NoSeWa','Heating_GasA','Heating_OthW','Electrical_Mix'], inplace=True)","7eb5def1":"df_train['BsmtExposure'].unique()","03e9404d":"ordinal_feat = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',\n                'HeatingQC', 'KitchenQual', 'GarageQual', 'GarageCond']\n\nqual_map={\n    'Ex': 5,\n    'Gd': 4,\n    'TA': 3,\n    'Fa': 2,\n    'Po': 1\n}\n\n#Here, Ex:'Excellent' Gd:'Good' Fa:'Fair' TA:'Typical' Po:'Poor'\n\ndef ord_encode(df):\n    for qual in ordinal_feat:\n        df[qual+'_ord'] = df[qual].map(qual_map)\n        df.drop(columns=[qual], inplace=True)\n    \nord_encode(df_train)\nord_encode(df_test)","b3afb14e":"qual_map={\n    'Gd': 5,\n    'Av': 4,\n    'Mn': 3,\n    'No': 2,\n    'NB': 1\n}\n\n#Here, Gd:'Good Exposure' Av:'Average Exposure' Mn:'Minimum Exposure' No:'No Exposure' NB:'No Basement'\n\ndef ord_encode_Expo(df):\n        df['BsmtExposure'+'_ord'] = df['BsmtExposure'].map(qual_map)\n        df.drop(columns=['BsmtExposure'], inplace=True)\n    \nord_encode_Expo(df_train)\nord_encode_Expo(df_test)","408cb437":"df_train.shape","4e384d38":"df_test.shape","1af0bfd4":"num_features= ['MSSubClass',\n'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n'2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n'HalfBath', 'BedroomAbvGr','KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', \n'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch','3SsnPorch',\n'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold'] ","268695bc":"# Threshold=0 means it will remove all the low variance features\nvar_thres = VarianceThreshold(threshold=0)\nvar_thres.fit(df_train[num_features])","b341844a":"constant_columns = [x for x in num_features\n                    if x not in df_train[num_features].columns[var_thres.get_support()]]\n\nprint(len(constant_columns))","188aae42":"print(var_thres.get_support())","80b29a60":"df_x = df_train[num_features]\ny_train = df_train['SalePrice']","0ab5e63c":"#Pearson Correlation Chart\nplt.figure(figsize = (30, 30))\nsns.heatmap(df_x.corr(), annot = True, cmap=\"PiYG\")\nplt.show()","a9221ace":"# Finding out how many features are correlated to each other to avoid having \n# duplicate features in our model.\n\ndef correlation(df, thres):\n    col_corr = set()\n    corr_matrix = df.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if corr_matrix.iloc[i,j] > thres:\n                colname = corr_matrix.columns[i]\n                col_corr.add(colname)\n    return col_corr\n\ncorr_feat = correlation(df_x, 0.7)\nlen(set(corr_feat))","c42af7f9":"# Duplicate Features\nprint(corr_feat)","c504c69c":"df_train = df_train.drop(corr_feat, axis=1)\ndf_test = df_test.drop(corr_feat, axis=1)","36487d00":"mutual_info = mutual_info_regression(df_x, y_train)\nmutual_info","288e39c1":"mutual_info = pd.Series(mutual_info)\nmutual_info.index = df_x.columns\nmutual_info.sort_values(ascending=False)","6dcc783b":"mutual_info.sort_values(ascending=False).plot.bar(figsize=(15,5))","c2402c82":"selected_top_columns = SelectPercentile(mutual_info_regression, percentile=40)\nselected_top_columns.fit(df_x, y_train)","36e038a2":"selected_top_columns.get_support()","97026dc6":"df_x.columns[selected_top_columns.get_support()]","712199a0":"unimp_columns = [x for x in num_features\n                    if x not in df_x.columns[selected_top_columns.get_support()]]\nprint(unimp_columns)","c8fa59e2":"for x in unimp_columns:\n    df_train.drop(columns=[x], axis=1, inplace=True)\n    df_test.drop(columns=[x], axis=1, inplace=True)","4513b10d":"num_vars = ['MSSubClass', 'LotFrontage', 'OverallQual', 'YearBuilt', 'YearRemodAdd',\n        'TotalBsmtSF', '2ndFlrSF', 'GrLivArea', 'FullBath', 'GarageCars', 'ExterQual_ord', \n        'ExterCond_ord', 'BsmtQual_ord', 'BsmtCond_ord','BsmtExposure_ord',\n        'HeatingQC_ord', 'KitchenQual_ord', 'GarageQual_ord', 'GarageCond_ord']","d7930f1b":"z = np.abs(stats.zscore(df_train[num_vars]))\nthreshold = 3\nprint(np.where(z > 3))","d29a33b3":"df = pd.concat((df_train.drop(columns=['SalePrice']), df_test))","d4ac4dd2":"matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nprices = pd.DataFrame({\"price\":df_train[\"SalePrice\"], \"log(price + 1)\":np.log1p(df_train[\"SalePrice\"])})\nprices.hist()","40186db4":"numeric_cols = ['MSSubClass', 'LotFrontage', 'OverallQual', 'YearBuilt', 'YearRemodAdd',\n        'TotalBsmtSF', '2ndFlrSF', 'GrLivArea', 'FullBath', 'GarageCars']\n\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\n\nskewed_feats = df_train[numeric_cols].apply(lambda x: skew(x)) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\ndf[skewed_feats] = np.log1p(df[skewed_feats])","dd291917":"x_train = df[:df_train.shape[0]]\nx_test = df[df_train.shape[0]:]\ny_train = df_train.SalePrice","9c66c304":"x_test.head()","4e8cc4bd":"def rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, x_train, y_train, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","06dae2ee":"mlr = LinearRegression()\nmlr.fit(x_train,y_train)\nmlr_preds = mlr.predict(x_test)","1cd3c8e1":"rmse_cv(mlr).min()","04538dbe":"alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]","94d671d4":"cv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Validation Curve\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")","2bc63f45":"cv_ridge.min()","415401d6":"ridge = Ridge(alpha = 3).fit(x_train, y_train)","428bc587":"ridge_preds = np.expm1(ridge.predict(x_test))","94714fef":"model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(x_train, y_train)model_ridge","c0e6c4a9":"rmse_cv(model_lasso).mean()","466abf48":"lasso_preds = np.expm1(model_lasso.predict(x_test))","0d8e54a6":"dtrain = xgb.DMatrix(x_train, label = y_train)\ndtest = xgb.DMatrix(x_test)\n\nparams = {\"max_depth\":2, \"eta\":0.1}\nmodel = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)","a1481fd7":"model.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()","308898b9":"model_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) #the params were tuned using xgb.cv\nmodel_xgb.fit(x_train, y_train)","63faa2bc":"xgb_preds = np.expm1(model_xgb.predict(x_test))","2eab9782":"preds = 0.7*lasso_preds + 0.3*xgb_preds","68391799":"solution = pd.DataFrame({\"id\":sol, \"SalePrice\":preds})\nsolution.to_csv(\"solution.csv\", index = False)","52b8af2d":"### Outlier Detection\n","01f23f58":"# Model Building\n\nNow we are going to use regularized linear regression models from the scikit learn module. I'm going to try both l_1(Lasso) and l_2(Ridge) regularization. I'll also define a function that returns the cross-validation rmse error so we can evaluate our models and pick the best tuning parameter.","3039da68":"## 1. Multiple linear Regression Model","15fa035d":"2. One Hot Encoding for ordinary categorical Features","55ac3354":"## Feature Engineering \n\nFeature engineering is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data. A feature is a property shared by independent units on which analysis or prediction is to be done. Features are used by predictive models and influence results. \n\n### Handling Missing Data\nThere can be 4 ways to handle the missing data:\n- Deleting the rows (loss of information)\n- Replace with the most frequent values\n- Apply classifier\/regressor model to predict missing values\n- Apply unsupervised machine learning to predict (clustering)","bde0a341":"### Transforming the skewed features\n\nNormally distributed features are an assumption in Statistical algorithms. Deep learning & Regression-type algorithms also benefit from normally distributed data.\nTransformation is required to treat the skewed features and make them normally distributed. Right skewed features can be transformed to normality with Square Root\/ Cube Root\/ Logarithm transformation.","81487e3d":"## 4. XGBoost Model","626b1f90":"## 2. Ridge Regression","ce659015":"# Data Preprocessing\n","5be82f39":"### Preparing the dataset for model fitting and Splitting the data into training set and test set","b357d3a8":"Features with large number of categories: 'Neighourhood', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Condition1',\n    'Condition2', 'HouseStyle', 'RoofMatl'","988291cb":"- No Duplicate or constant features present in the dataset.","aee0185c":"1. One Hot Encoding for Features with large number of Categories","70002cdb":"3. Encoding Ordinal features using Label encoding ","95cc0a69":"### Feature Scaling\nFeature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization. When to use feature scaling:\n- Algorithms which use gradient descent or euclidean distance for eg: Knn or K means clustering etc\n\nWhen to not use feature scaling:\n- Algorithms like Decision Tree, Random Forest, or XGBoost etc.\n\nNOTE: We are gonna skip scaling in this model as we have applied logarithmic transformation to treat skewed features. Also, we mostly are gonna use xgboost model and it is recommended not to use feature scaling along with XGB model.","a18c1bcc":"# Exploratory Data Analysis(EDA)","1d9e31a2":"Note the U-ish shaped curve above. When alpha is too large the regularization is too strong and the model cannot capture all the complexities in the data. If however we let the model be too flexible (alpha small) the model begins to overfit. A value of alpha = 3 is about right based on the plot above.","57497a77":"1. Removing constant features using Variance Threshold\n","765f45e7":"## 3. Lasso Regression","4b0ce35f":"Ordinal Categorical Features: 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'HeatingQC', 'KitchenQual', 'GarageQual', 'GarageCond' \n\nNominal Categorical features: 'MSZoning','Street','LotShape', 'LandContour', 'Utilities', 'LotConfig',\n 'LandSlope', 'BldgType', 'RoofStyle', 'MasVnrType', 'Foundation', 'BsmtFinType1',\n 'BsmtFinType2', 'Heating', 'CentralAir','Electrical','Functional',\n 'GarageType', 'PavedDrive', 'SaleCondition'","4edb29b2":"There are features with a lot of missing values.","cad6d891":"3. Using Information gain - mutual information for Feature Selection\n\nI(X ; Y) = H(X) \u2013 H(X | Y) and IG(S, a) = H(S) \u2013 H(S | a)\n\nAs such, mutual information is sometimes used as a synonym for information gain. Technically, they calculate the same quantity if applied to the same data.","20fe9289":"### Encoding Categorical Data\nTypes of Encoding: \n1. Nominal Encoding : (Categorical features where rank is not important)\n    - One Hot encoding\n    - One Hot encodng with many categories - We take 10(or more) most frequently occuring categories and group them into 1       category.\n    - Mean encoding - Replace the label with mean Eg; Pincode\n2. Ordinal Encoding : (Categorical features where rank is important)\n    - Label encoding\n    - Target guided ordinal encoding\n3. Count\/Frequency Encoding: Can be used for both nominal and ordinal features","5d042efb":"## Feature Selection for numerical Data","284a2178":"2. Dropping Features Using Pearson Correlation","83270598":"The main tuning parameter for the Ridge model is alpha - a regularization parameter that measures how flexible our model is. The higher the regularization the less prone our model will be to overfit. However it will also lose flexibility and might not capture all of the signal in the data.","f737ef94":"Variable that are highly correlated to the Target variable: 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF', 'Heating', '1stFlrSF', 'GrLivArea', 'FullBath', 'TotRmsAbvGrd', 'GarageCars', 'GarageArea'","ccb18471":"This problem uses the mean squared error, mse as the scoring metric. However, this metric returns negative values. Therefore, we need to use abs(mse) to get positive values.\n\nThe mse takes the errors: difference between the actual values and those predicted by the model, and find the mean of the squares.\n\nIt isn\u2019t null and the negative sign does not make it ineffective. A high mse means that the error is large."}}