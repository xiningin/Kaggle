{"cell_type":{"468cb22a":"code","b0b69b31":"code","8dd33952":"code","aa2026d7":"code","871bdf1e":"code","77bdcee2":"code","fcff660b":"code","b0138717":"code","27a858e1":"code","24f14079":"code","5f1b23d8":"code","a9fc07a8":"code","8a122f70":"markdown"},"source":{"468cb22a":"#Getting the inputs and targets from the data files as well as the test file which will be used later on.\nimport pandas as pd\n\nurl = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\ndata = pd.read_csv(url, index_col='Id')\nurl = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\nX_test = pd.read_csv(url, index_col='Id')\n\ny = data.SalePrice\nX = data.drop(['SalePrice'], axis=1)","b0b69b31":"#Defined a dictionary to map out the MSSubClass feature (which is numerical) to adecuate string values.\ndic = {20: \"1-story 1946 & N\", 30: \"1-story 1945 & O\", 40: \"1-story with f attic\", 45: \"1-1\/2 story unfinished\", 50: \"1-1\/2 story finished\",\n       60: \"2 story 1946 & N\", 70: \"2 story 1945 & O\", 75: \"2-1\/2 story\", 80: \"split or multilevel\", 85: \"split foyer\", 90: \"duplex\", 120: \"1 story pud\", \n       150: \"1-1\/2 story pud\", 160: \"2 story pud\", 180: \"pud multilevel\", 190: \"2 family conversion\"}\n\nX['MSSubClass'] = X['MSSubClass'].map(dic)\nX_test['MSSubClass'] = X_test['MSSubClass'].map(dic)","8dd33952":"#Splitting the input and target data into train and validation datasets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","aa2026d7":"#Selecting categorical colums with relative low cardinality\ncategorical_cols = [cname for cname in X_train.columns if X_train[cname].nunique() < 20 and \n                        X_train[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_valid = X_valid[my_cols].copy()\nX_test = X_test[my_cols].copy()","871bdf1e":"#In this cell I apply imputation to the data using a mean strategy for numerical data and a most frequen strategy to categorical data\nfrom sklearn.impute import SimpleImputer\n\n# Imputation\nimp_num = SimpleImputer(strategy='mean')\nimp_cat = SimpleImputer(strategy=\"most_frequent\")\n\nimputed_X_train_cat = pd.DataFrame(imp_cat.fit_transform(X_train[categorical_cols]), index=X_train.index)\nimputed_X_train_num = pd.DataFrame(imp_num.fit_transform(X_train[numerical_cols]), index=X_train.index)\nimputed_X_valid_cat = pd.DataFrame(imp_cat.transform(X_valid[categorical_cols]), index=X_valid.index)\nimputed_X_valid_num = pd.DataFrame(imp_num.transform(X_valid[numerical_cols]), index=X_valid.index)\nimputed_X_test_cat = pd.DataFrame(imp_cat.transform(X_test[categorical_cols]), index=X_test.index)\nimputed_X_test_num = pd.DataFrame(imp_num.transform(X_test[numerical_cols]), index=X_test.index)\n\nimputed_X_train = pd.concat([imputed_X_train_cat, imputed_X_train_num], axis=1)\nimputed_X_valid = pd.concat([imputed_X_valid_cat, imputed_X_valid_num], axis=1)\nimputed_X_test = pd.concat([imputed_X_test_cat, imputed_X_test_num], axis=1)\n\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\nimputed_X_test.columns = X_test.columns","77bdcee2":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(imputed_X_train[categorical_cols]), index = imputed_X_train.index)\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(imputed_X_valid[categorical_cols]), index = imputed_X_valid.index)\nOH_cols_test = pd.DataFrame(OH_encoder.transform(imputed_X_test[categorical_cols]), index = imputed_X_test.index)\n\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = imputed_X_train.drop(categorical_cols, axis=1)\nnum_X_valid = imputed_X_valid.drop(categorical_cols, axis=1)\nnum_X_test = imputed_X_test.drop(categorical_cols, axis=1)\n\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\nOH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)","fcff660b":"#Training and evaluating a linear model\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn import linear_model\nlin_model = linear_model.LinearRegression()\nlin_model.fit(OH_X_train, y_train)\npreds = lin_model.predict(OH_X_valid)\nprint(mean_absolute_error(y_valid, preds))","b0138717":"#Same for a random forest model\nfrom sklearn.ensemble import RandomForestRegressor\nforest_model = RandomForestRegressor(n_estimators=500, criterion='mae', random_state=1)\nforest_model.fit(OH_X_train, y_train)\npreds = forest_model.predict(OH_X_valid)\nprint(mean_absolute_error(y_valid, preds))","27a858e1":"#Same for an XGB Regressor\nfrom xgboost import XGBRegressor\n\nxgb_model = XGBRegressor(n_estimators=50000, learning_rate=0.01)\nxgb_model.fit(OH_X_train, y_train, \n             early_stopping_rounds=10, \n             eval_set=[(OH_X_valid, y_valid)],\n             verbose=False)\npreds = xgb_model.predict(OH_X_valid)\nprint(mean_absolute_error(y_valid, preds))","24f14079":"#Scaling the numerical data before using it to train\/evaluate on a NN model\nfrom sklearn.preprocessing import scale\nscaled_X_train = pd.DataFrame(scale(OH_X_train[numerical_cols]), index=OH_X_train.index)\nscaled_X_valid = pd.DataFrame(scale(OH_X_valid[numerical_cols]), index=OH_X_valid.index)\n\nscaled_X_train.columns = OH_X_train[numerical_cols].columns\nscaled_X_valid.columns = OH_X_valid[numerical_cols].columns\n\nOH_X_train[numerical_cols] = scaled_X_train[numerical_cols]\nOH_X_valid[numerical_cols] = scaled_X_valid[numerical_cols]","5f1b23d8":"#Training using a simple Neural Network and evaluating its performance\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\nmodel = keras.Sequential([\n    layers.Dense(1024, activation='relu', input_shape=[272]),\n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1),\n])\nmodel.compile(\n    optimizer='adam',\n    loss='mae',\n)\n\n\nmodel.fit(\n    OH_X_train, y_train,\n    validation_data=(OH_X_valid, y_valid),\n    batch_size=256,\n    epochs=500,\n    callbacks=[early_stopping], # put your callbacks in a list\n    verbose=0,  \n)\n\npreds = model.predict(OH_X_valid)\nprint(mean_absolute_error(y_valid, preds))","a9fc07a8":"# Generate test predictions using the XGB model since it seemed to permorf better than the rest\npreds_test = xgb_model.predict(OH_X_test)\n\n# Save predictions in format used for competition scoring\noutput = pd.DataFrame({'Id': OH_X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","8a122f70":"# This is my first notebook and first attempt on the House Prices competition. I'm still a beginner and this is definitely far from the ideal approach but I figured it wouldn't hurt to publish my work. Any comments or suggestions are welcome, thanks."}}