{"cell_type":{"d02380ae":"code","4bf3032c":"code","ac663dc2":"code","21e1d27d":"code","594f6cf4":"code","c944e342":"code","06cacdfb":"code","580d2ade":"code","47d38e28":"code","658358e3":"code","f2a9958d":"code","1dc0aae0":"code","c081bcf5":"code","32b9d56b":"code","f78c5cb6":"code","161f8270":"code","2a53447c":"code","7b7043e8":"code","dcba1717":"code","881832a8":"code","65aba2b4":"code","76d4ed63":"code","d72663cd":"code","b389b972":"code","983d621c":"code","28397c98":"code","592c778b":"code","f7bb8b7d":"code","5738b66f":"code","2e5093a9":"code","6425c7f6":"code","e2889aae":"code","0025c2aa":"code","bb072406":"code","18f6f47f":"code","44b63dd5":"code","088110fa":"code","f74797f9":"code","c5a1c204":"code","790b86b9":"code","833d35ba":"code","e62e1a15":"code","b3691222":"code","8ce3e016":"code","9b90022b":"code","f33a58f1":"code","c7b2da46":"code","72913ae9":"code","c0861020":"code","17f8788b":"code","9c1819a6":"code","f6333fb3":"code","dd97a37f":"code","ca34f344":"code","a67dd4b3":"code","619a740e":"code","aaf86092":"code","443bde21":"code","65edbfea":"code","8e04e60a":"code","1e0d7740":"code","4c32ef94":"code","30e5728b":"code","e329f8ec":"markdown","29b7af19":"markdown","6308018f":"markdown","c6e2cff2":"markdown","47a7734c":"markdown","2fc00ab3":"markdown","c8b479a0":"markdown","1e62824b":"markdown","dfbc6392":"markdown","c14547da":"markdown","71acdff0":"markdown"},"source":{"d02380ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\nimport scipy.stats as ss\nfrom scipy.stats import zscore\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport warnings\nimport plotly.graph_objs as go\nwarnings.filterwarnings(\"ignore\")\n","4bf3032c":"# Road the data\nNewData = pd.read_csv(\"..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\nNewData","ac663dc2":"# Exploring the data by.info() .describe()\n# - 1-1) Check the Data shape\n# - 1-2) Check the Data type\n# - 1-3) Check the Missing Values\n# Exploring interesting themes by Visualization \n# - Tables \/ Plots \/ Correlation between the metrics \n# - Explore interesting themes \n    # Wealthy survive? \n    # By location \n    # Age scatterplot with ticket price \n    # Young and wealthy Variable? \n    # Total spent? \n# Feature engineering \n# preprocess data together or use a transformer? \n    # use label for train and test   \n# Scaling?\n# Model Baseline \n# Model comparison with CV ","21e1d27d":"# 1-1) Road the Data \/ Check the Data Shape\nprint (\"Rows     : \" ,NewData.shape[0]) # Check the Rows\nprint (\"Columns  : \" ,NewData.shape[1]) # Check the Columns\nprint (\"Data     : \" ,NewData.head()) # Check the data columns","594f6cf4":"# 1-2) Check the Data types\n# Check the data types and found \"TotalCharges\" type is wrong. It changes \"obejct\" to \"float64\"\nNewData.TotalCharges = pd.to_numeric(NewData.TotalCharges, errors='coerce') \nNewData.TotalCharges.astype(float) # Change TotalCharges \"obejct\" to \"float64\"\nprint(NewData.dtypes) ","c944e342":"# 1-3) Check the missing values\n# Check the columns with missing values \nNewData[NewData.isnull().any(axis=1)].head()\nprint(NewData.isnull().sum()) \n# \"TotalCharges\" column has 11 missing values. I dropped all the NAs\n# This is because the missing values' count is less than 5% of total counts\nNewData = NewData.dropna()\nprint(NewData.isnull().sum()) ","06cacdfb":"# 1-4) To better understand the numeric data, I want to use the .describe() method. \n# This gives me an understanding of the central tendencies of the data\nprint(NewData.shape)\nprint(NewData.describe())","580d2ade":"# 2-1-1) Table1: Comparing <contract vs churn>\ndf_cat_plot = NewData.drop(['customerID','tenure','MonthlyCharges','TotalCharges'],axis=1)\nchurn_table1 = pd.crosstab(index=df_cat_plot['Contract'],\n                          columns=df_cat_plot['Churn'])\nchurn_table1","47d38e28":"# 2-1-2) Table2 : Colored Table \n# cat dataframe -> recode using factorize \ndf_cat_plot = NewData.drop(['customerID','tenure','MonthlyCharges','TotalCharges','Churn'],axis=1)\ndf_cat =df_cat_plot.apply(lambda x : pd.factorize(x)[0])+1\ndf_cat\n\ndf_cont = NewData[['tenure','MonthlyCharges','TotalCharges','Churn']]\ndf_cont\ndf_cat = df_cat.merge(df_cont,left_index=True,right_index=True)\ndf_cat\n\ntable1 = df_cat.groupby(['Churn']).mean()\nth_props = [\n  ('font-size', '12px'),\n  ('text-align', 'left'),\n  ('background-color', '#f7f7f9')\n  ]\n\n# CSS properties for td elements in dataframe\ntd_props = [\n  ('font-size', '11px')\n  ]\nstyles = [\n  dict(selector=\"th\", props=th_props),\n  dict(selector=\"td\", props=td_props)\n  ]\n\ntable1 = table1.style.background_gradient(cmap='PuBu').set_table_styles([{'selector': 'th', 'props': [('font-size', '10pt')]}]).set_table_styles(styles)\ntable1","658358e3":"# 2-2-1) Plot1 : Check the Churn column count\ndf = sns.catplot(y=\"Churn\", kind = \"count\", data=NewData, height = 3.0,\n                 palette=\"Set1\",\n                 aspect = 2.5, orient = 'h')\n\n# A Frequency table based on number of \ntb1 = pd.crosstab([NewData.Churn], \n                  columns='Number',\n                  colnames =[' '],\n                  margins = False) \nprint(tb1)","f2a9958d":"# 2-2-2) Plot2 : Check the Churn %\nvalues = NewData.Churn.value_counts()\nplt.figure(figsize=(6,6))\nplt.pie(values, explode = (0,0.1),autopct='%1.1f%%',labels=['Stay','Leave'], shadow=False,startangle=90,colors=['teal','gold'])\nplt.show()","1dc0aae0":"# 2-2-3) Plot 3 : Check the relationship between Churn and continous coloumns\nfig, axs = plt.subplots(ncols=2,figsize=(8,5))\nsns.set(style=\"whitegrid\", color_codes=True)\nnp.random.seed(2017)\n\nax1= sns.pointplot(x=\"Contract\", y=\"TotalCharges\", hue=\"Churn\", data=NewData,\npalette={\"No\": \"g\", \"Yes\": \"m\"},\nmarkers=[\"^\", \"o\"], linestyles=[\"-\", \"--\"],\nax=axs[0])\nax1.set_title(\"Total charges of 3 Contracts\")\n\nax2=sns.pointplot(x=\"InternetService\", y=\"TotalCharges\", hue=\"Churn\", data=NewData,\n              palette={\"No\": \"r\", \"Yes\": \"y\"},\n              markers=[\"^\", \"o\"], linestyles=[\"-\", \"--\"],ax=axs[1])\nax2.set_title(\"Total charges of 3 Internet Service\")\nplt.tight_layout()\nplt.show()","c081bcf5":"# 3) Make a heatmap\ndf_cat['Churn'] = np.where(df_cat['Churn']=='Yes', 1, 0)\nprint(df_cat.head())\nprint(df_cat.dtypes)\n\ncorr = df_cat.corr()\nsns.set(rc={'figure.figsize':(16,10)})\ncorrelation_matrix = df_cat.corr().round(2)\nsns.heatmap(data=correlation_matrix, annot=True)","32b9d56b":"# 4-1) Demographic analysis\n# Data preparation\ndf_cat_plot = df_cat_plot.merge(df_cont, right_index=True,left_index=True) #original data without factorize\ndf_cat_plot['Churn'] = np.where(df_cat_plot['Churn']=='Yes', 1, 0)\ndf_cat_plot\n\nsns.catplot(x=\"SeniorCitizen\", y=\"MonthlyCharges\", kind=\"box\", data=df_cat_plot);\n# Plot tells that the senior citizen tend to have higher montly charges as compared to younger population\n\nsns.catplot(x=\"gender\", y=\"MonthlyCharges\", kind=\"box\", data=df_cat_plot);\n\n# Male and females largely have same avg monthly bills\n\nsns.catplot(x=\"SeniorCitizen\", y=\"tenure\", kind=\"box\", data=df_cat_plot);\n\n# Senior citizen have higher mean tenure as compared to young population young people churn often\n\nsns.catplot(x=\"Dependents\", y=\"MonthlyCharges\", kind=\"box\", data=df_cat_plot);\n\n# People with no dependents have higher avg. monthly bills\n\nsns.catplot(x=\"Partner\", y=\"MonthlyCharges\", kind=\"box\", data=df_cat_plot);\n","f78c5cb6":"plt.figure(figsize=(15, 15))\n\nplt.subplot(3, 2, 1)\nsns.countplot('gender', data=NewData, hue='Churn')\n\nplt.subplot(3, 2, 2)\nsns.countplot('SeniorCitizen', data=NewData, hue='Churn')\n\nplt.subplot(3, 2, 3)\nsns.countplot('Partner', data=NewData, hue='Churn')\n\nplt.subplot(3, 2, 4)\nsns.countplot('Dependents', data=NewData, hue='Churn')\n\nplt.subplot(3, 2, 5)\nsns.countplot('PhoneService', data=NewData, hue='Churn')\n\nplt.subplot(3, 2, 6)\nsns.countplot('PaperlessBilling', data=NewData, hue='Churn')","161f8270":"# People with partners have higher avg. monthly bills\n\nsns.catplot(x=\"MultipleLines\", y=\"MonthlyCharges\", kind=\"box\", data=df_cat_plot);\n\nsns.catplot(x=\"InternetService\", y=\"MonthlyCharges\", kind=\"box\", data=df_cat_plot);\n\n# Fiber optics service has very high monthly charges\nsns.catplot(x=\"PaymentMethod\", y=\"MonthlyCharges\", kind=\"box\", data=df_cat_plot);\n","2a53447c":"plt.figure(figsize=(14, 14))\n\nplt.subplot(3, 2, 1)\nNewData[NewData.Churn == 'No'].tenure.hist(bins=35, alpha=0.6, label='Churn=No')\nNewData[NewData.Churn == 'Yes'].tenure.hist(bins=35, alpha=0.6, label='Churn=Yes')\nplt.legend()\nplt.xlabel('Number of months with company')\n\nplt.subplot(3, 2, 2)\nNewData[NewData.Churn == 'No'].tenure.value_counts().hist(bins=50, alpha=0.6, label='Churn=No')\nNewData[NewData.Churn == 'Yes'].tenure.value_counts().hist(bins=50, alpha=0.6, label='Churn=Yes')\nplt.legend()\n# plt.xlabel() Ziru? please see what goes in Label over here!\n\nplt.subplot(3, 2, 3)\nNewData[NewData.Churn == 'No'].MonthlyCharges.hist(bins=35, alpha=0.6, label='Churn=No')\nNewData[NewData.Churn == 'Yes'].MonthlyCharges.hist(bins=35, alpha=0.6, label='Churn=Yes')\nplt.xlabel('Monthly Payment')\nplt.legend()\n\nplt.subplot(3, 2, 4)\nNewData[NewData.Churn == 'No'].TotalCharges.hist(bins=35, alpha=0.6, label='Churn=No')\nNewData[NewData.Churn == 'Yes'].TotalCharges.hist(bins=35, alpha=0.6, label='Churn=Yes')\nplt.xlabel('Total Payment')\nplt.legend()","7b7043e8":"# Check the distribution of some categorical variables that indicates high correlation in the heat map\n# Oneline BackUp, Online Security, DeviceProtection, Streaming TV, Streaming Movies and Techsupport \n# These columns have the same range of answers \nplt.figure(figsize=(17, 17))\n\nplt.subplot(3, 3, 6)\nsns.countplot('OnlineBackup', data=NewData, hue='Churn')\n\nplt.subplot(3, 3, 1)\nsns.countplot('OnlineSecurity', data=NewData, hue='Churn')\n\nplt.subplot(3, 3, 2)\nsns.countplot('StreamingTV', data=NewData, hue='Churn')\n\nplt.subplot(3, 3, 3)\nsns.countplot('StreamingMovies', data=NewData, hue='Churn')\n\nplt.subplot(3, 3, 4)\nsns.countplot('DeviceProtection', data=NewData, hue='Churn')\n\nplt.subplot(3, 3, 5)\nplt.tight_layout\nsns.countplot('TechSupport', data=NewData, hue='Churn')\n \n# So for these conditions, we'd like to keep only one variable.","dcba1717":"plt.figure(figsize=(15, 18))\nplt.subplot(3, 2, 3)\ng = sns.countplot('PaymentMethod', data=NewData, hue='Churn')\ng.set_xticklabels(g.get_xticklabels(), rotation=45);\n\nplt.subplot(3, 2, 4)\ng = sns.countplot('Contract', data=NewData, hue='Churn')\ng.set_xticklabels(g.get_xticklabels(), rotation=45);","881832a8":"# Make tenure to categorical column\ndef tenure_lab(NewData) :\n    \n    if NewData[\"tenure\"] <= 12 :\n        return \"1year\"\n    elif (NewData[\"tenure\"] > 12) & (NewData[\"tenure\"] <= 24 ):\n        return \"2years\"\n    elif (NewData[\"tenure\"] > 24) & (NewData[\"tenure\"] <= 48) :\n        return \"3years\"\n    elif (NewData[\"tenure\"] > 48) & (NewData[\"tenure\"] <= 60) :\n        return \"4years\"\n    elif NewData[\"tenure\"] > 60 :\n        return \"over5years\"\nNewData[\"Tenure_Category\"] = NewData.apply(lambda NewData:tenure_lab(NewData),\n                                     axis = 1)","65aba2b4":"churn     = NewData[NewData[\"Churn\"] == \"Yes\"]\nnot_churn = NewData[NewData[\"Churn\"] == \"No\"]","76d4ed63":"NewData1 = NewData.groupby([\"Tenure_Category\",\"Churn\"])[[\"MonthlyCharges\",\n                                                    \"TotalCharges\"]].mean().reset_index()\n\n#function for tracing \ndef mean_charges(column,aggregate) :\n    tracer = go.Bar(x = NewData1[NewData1[\"Churn\"] == aggregate][\"Tenure_Category\"],\n                    y = NewData1[NewData1[\"Churn\"] == aggregate][column],\n                    name = aggregate,marker = dict(line = dict(width = 1)),\n                    text = \"Churn\"\n                   )\n    return tracer\n\n#function for layout\ndef layout_plot(title,xaxis_lab,yaxis_lab) :\n    layout = go.Layout(dict(title = title,\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',title = xaxis_lab,\n                                         zerolinewidth=1,ticklen=5,gridwidth=2),\n                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',title = yaxis_lab,\n                                         zerolinewidth=1,ticklen=5,gridwidth=2),\n                           )\n                      )\n    return layout\n    \n\n#plot1 - mean monthly charges by tenure groups\ntrace1  = mean_charges(\"MonthlyCharges\",\"Yes\")\ntrace2  = mean_charges(\"MonthlyCharges\",\"No\")\nlayout1 = layout_plot(\"Average Monthly Charges by Tenure groups\",\n                      \"Tenure group\",\"Monthly Charges\")\ndata1   = [trace1,trace2]\nfig1    = go.Figure(data=data1,layout=layout1)\n\n#plot2 - mean total charges by tenure groups\ntrace3  = mean_charges(\"TotalCharges\",\"Yes\")\ntrace4  = mean_charges(\"TotalCharges\",\"No\")\nlayout2 = layout_plot(\"Average Total Charges by Tenure groups\",\n                      \"Tenure group\",\"Total Charges\")\ndata2   = [trace3,trace4]\nfig2    = go.Figure(data=data2,layout=layout2)\n\nfig1.show()\nfig2.show()","d72663cd":"bins = [0,12,36,72]\nname = ['Less than 1 year', '1-3 years', 'More than 3 years']\nNewData['Duration'] = pd.cut(NewData.tenure, bins, labels=name)\n\ntable1 = pd.crosstab(index=NewData.Contract, columns=NewData.Churn)\ntable2 = pd.crosstab(index=NewData.PaymentMethod, columns=NewData.Churn)\ntable2.index = pd.Series(['Bank transfer','Credit card','Electronic check','Mailed check'])\ntable3 = pd.crosstab(index=NewData.InternetService, columns=NewData.Churn)\ntable4 = pd.crosstab(index=NewData.Duration, columns=NewData.Churn)\n\nfig = plt.figure(figsize=(20,12))\nplt.style.use('seaborn-darkgrid')\nax1= fig.add_subplot(2,2,1)\ntable1.plot(ax=ax1, kind=\"bar\", stacked=True)\nplt.xticks(rotation=0)\nax2 = plt.subplot(2,2,2)\ntable2.plot(ax=ax2,kind=\"bar\", stacked=True)\nplt.xticks(rotation=0)\nax3 = plt.subplot(2,2,3)\ntable3.plot(ax=ax3,kind=\"bar\", stacked=True)\nplt.xticks(rotation=0)\nax4 = plt.subplot(2,2,4)\ntable4.plot(ax=ax4,kind=\"bar\", stacked=True)\nplt.xticks(rotation=0)\nplt.show()","b389b972":"#1. Do one hot encoding\n#2. Do interaction terms\n#3. Do polynomial features and log features for numeric\n# pca is not suitable for categorical data!\n\n# Create poly faetures\ny = NewData['Churn']\nprint(y.shape)\ny = pd.DataFrame(y)\n\n#resetting index\ny.reset_index(inplace=True,drop=True)\nX = NewData.drop('Churn',axis=1)\nprint(X.shape)\n\n# Drop Customer ID\nX = X.drop('customerID',axis=1)\n\n# Seperate numeric columns\nX_num = X[['tenure','MonthlyCharges','TotalCharges']]","983d621c":"# Create polynomial features\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree=4)\nX_poly = poly_features.fit_transform(X_num)\ntmp1 = pd.DataFrame(X_poly)","28397c98":"# Create log features\nX_log = np.log(X_num)\ntmp = pd.DataFrame(X_log)\nprint(tmp.shape) # same number of columns, but they've changed.\ntmp\n\n# reset the index\ntmp.reset_index(inplace=True,drop=True)\n\n# Rename columns and adding _log\ntmp.columns = [col+'_'+'log' for col in tmp.columns]","592c778b":"# Make interaction term \n# I found there are interesting relationship between Tenure and MonthlyCharges, So I made another column for these two variables\nnew_col = X_num['tenure'] * X_num['MonthlyCharges']\nX_num.insert(loc=3, column='tenure*monthlycharges', value=new_col)","f7bb8b7d":"# Merge X_num, tmp1(poly features) and tmp(log features)\n# Reset the index of X_num\nX_num.reset_index(inplace=True,drop=True)\n\n# Smoosh the two dataframes together\nX_smoosh = pd.concat([X_num,tmp1, tmp], axis=1)\nX_smoosh.shape # Look at how many more columns there are","5738b66f":"X_smoosh  #this is the dataframe which has the original numeric columns from the actual dataset and the log features and polynomial features","2e5093a9":"# Feature Engineering for Categorical Variables\nX_cat = X.drop(['tenure','MonthlyCharges','TotalCharges','Tenure_Category','Duration'],axis=1)\nX_cat.reset_index(drop=True,inplace=True)\nX_cat.reset_index(drop=False,inplace=True)\nX_cat.head() #This has only categorical variables!","6425c7f6":"X_cat.columns","e2889aae":"cat_df = X_cat.iloc[:,1:]\n\ncolumns = cat_df.columns\ndf_final = pd.DataFrame(X_cat['index'])\nfor col in columns:\n  df = pd.DataFrame(cat_df[col])\n  one_hot = pd.get_dummies(cat_df[col])\n  df = df.join(one_hot)\n  df_final = df_final.merge(df,left_index=True,right_index=True)\n\n    \ndf_final = df_final.iloc[:,1:]\ndf_final.head()","0025c2aa":"B = ['gender','Female','Male','SeniorCitizen','Not_A_SeniorCitizen','SeniorCitizen','Partner','DoesNotHaveaPartner','HasPartner','Dependent','NoDependents','Has_Dependents',\n                      'PhoneService','No_PhoneService','Has_PhoneService','MultipleLines','No_ML','NoPhoneService','HasML','InternetService','DSL_Internet','fiberOptics','NoInternet','OnlineSecurity','NotOptedforOnlineSec',\n                      'NotApplicable(noInternet)','OptedforOnlineSec','OnlineBackup','NotOptedforOnlineBackup','NotApplicable(NoInternet)','OptedforOnlineBackup','DeviceProtection','NotOptedforDeviceProtection','NotApplicable(NoInternet)','OptedforDevicePro','TechSupport','notOPtedforTechSupport','NotApplicable','optedforTechssupport','StreamingTV','NotOptedTV','NotApplcable','OPtedTV','StreamingMovies',\n                      'NotOptedMovies','NotApplicable','OptedMovies','Contract','Month-to-month','One year','Two year','PaperlessBilling','PaperBilling','PaperlessBilling','PaymentMethod','Bank transfer(auto)','Credit Card(auto)',\n                      'Electronic check','Mailed check']\nB = pd.DataFrame(B)\nB.shape","bb072406":"df_final.columns=['gender','Female','Male','SeniorCitizen','Not_A_SeniorCitizen','SeniorCitizen','Partner','DoesNotHaveaPartner','HasPartner','Dependent','NoDependents','Has_Dependents',\n                      'PhoneService','No_PhoneService','Has_PhoneService','MultipleLines','No_ML','NoPhoneService','HasML','InternetService','DSL_Internet','fiberOptics','NoInternet','OnlineSecurity','NotOptedforOnlineSec',\n                      'NotApplicable(noInternet)','OptedforOnlineSec','OnlineBackup','NotOptedforOnlineBackup','NotApplicable(NoInternet)','OptedforOnlineBackup','DeviceProtection','NotOptedforDeviceProtection','NotApplicable(NoInternet)','OptedforDevicePro','TechSupport','notOPtedforTechSupport','NotApplicable','optedforTechssupport','StreamingTV','NotOptedTV','NotApplcable','OPtedTV','StreamingMovies',\n                      'NotOptedMovies','NotApplicable','OptedMovies','Contract','Month-to-month','One year','Two year','PaperlessBilling','PaperBilling','Paperless Billing','PaymentMethod','Bank transfer(auto)','Credit Card(auto)',\n                      'Electronic check','Mailed check']\ndf_final.head()","18f6f47f":"# Join all features engineered for categorical and continuous\ndf_modeling = df_final.merge(X_smoosh,left_index=True,right_index=True)\n\ndf_modeling.head() # This is the merger of categorical and numeric feature engineering\n#Though,still need to standardize numeric data and get rid of original categorical variables!","44b63dd5":"y['Churn'].value_counts()\ndf_modeling.iloc[:,39:].head()","088110fa":"# Data Standardization\nfrom sklearn import preprocessing\n\nstd = X_smoosh.values \n\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(std)\nx_scaled","f74797f9":"# Let's convert that x_scaled, numpy array to a pandas dataframe\n# Note that x_scaled has no column labels\ndf_mmstd = pd.DataFrame(x_scaled, columns=X_smoosh.columns)\ndf_mmstd.head()\n# df_mmstd has the all the numeric columns standardized","c5a1c204":"df_final.head()","790b86b9":"df_final_without = df_final.drop(['gender','SeniorCitizen','Partner','Dependent','PhoneService','MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies',\n'Contract','PaymentMethod','PaperlessBilling'], axis=1,inplace=False)","833d35ba":"df_final_without.shape","e62e1a15":"# Merge Standardization with Categorical\ndf_std_modelling = df_final_without.merge(df_mmstd, left_index=True, right_index=True)\ndf_std_modelling.head() #megered dataframe for modeling (standardized numeric cols and dummy vars for categorical)","b3691222":"col = np.array(df_std_modelling.columns)\ncol","8ce3e016":"df_std_modelling.dtypes","9b90022b":"df_std_modelling","f33a58f1":"y","c7b2da46":"import imblearn.under_sampling as u\n\ndf_std_modelling,y = make_classification(n_features = 84, n_samples=2000) \n# Make classification default = 20, so I need to set the number\nros = u.RandomUnderSampler(sampling_strategy='majority')\nX_resampled, Y_resampled = ros.fit_resample(df_std_modelling, y)","72913ae9":"X_resampled = pd.DataFrame(X_resampled)\nprint(X_resampled.shape)\n\nY_resampled = pd.DataFrame(Y_resampled)\nprint(Y_resampled.shape)","c0861020":"Y_resampled.head()","17f8788b":"Y_resampled = Y_resampled.rename(columns={0:'Churn'})","9c1819a6":"seed = 7\nX_train, X_test, y_train, y_test =\\\n  train_test_split(X_resampled, Y_resampled, \n                   stratify=Y_resampled, \n                   test_size=0.2, random_state=seed)","f6333fb3":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","dd97a37f":"X_train.head()","ca34f344":"from pandas import read_csv\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n#import warnings\n#warnings.filterwarnings(\"ignore\")","a67dd4b3":"# Spot-checking \nmodels = []\nmodels.append(('LR', LogisticRegression(max_iter=1000000)))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('DT', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('Bagging', BaggingClassifier()))\nmodels.append(('RandomForest', RandomForestClassifier()))\nmodels.append(('ExtraTree', ExtraTreesClassifier()))\nmodels.append(('GradientBoosting', GradientBoostingClassifier()))\n\n##################################################\n# evaluate each model in turn\n\nresults = []\nnames = []\n\n# store preds\nfor name, model in models:\n  cv_results = cross_val_score(model,X_train,y_train, scoring='accuracy')\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n  print(msg)","619a740e":"# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names, rotation=45)\npyplot.show()","aaf86092":"# Make predictions on validation dataset (Logistic Regression)\nlr = LogisticRegression(max_iter=100000)\nlr.fit(X_train, y_train)\npredictions = lr.predict(X_test)\nprint(accuracy_score(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))","443bde21":"## Hyper parameter tuning\n#Hypertuning with Grid Search\n\ngrid_params_LDA = [{'solver':['svd','lsqr','eigen'], 'tol':[0.0001,0.0002,0.0003]}]\n\ngrid_params_LR = [{'penalty':['l1', 'l2', 'elasticnet', 'none'],'solver':['newton-cg','lbfgs', 'liblinear', 'sag', 'saga']}]\n\ngs_LDA = GridSearchCV(estimator=LinearDiscriminantAnalysis(), param_grid=grid_params_LDA, scoring='accuracy', cv=10)\n\ngs_LR = GridSearchCV(estimator=LogisticRegression(), param_grid=grid_params_LR, scoring='accuracy', cv=10)\n\ngrids = [gs_LR,gs_LDA]\ngrid_dict = {0:'LogisticRegression',1:'LinearDiscriminantAnalysis'}","65edbfea":"best_acc = 0.0\nbest_clf = 0\nbest_gs = ''\nfor idx, gs in enumerate(grids):\n\tprint('\\nEstimator: %s' % grid_dict[idx])\t\n\tgs.fit(X_train, y_train)\n\tprint('Best params: %s' % gs.best_params_)\n\tprint('Best training accuracy: %.3f' % gs.best_score_)\n\ty_pred = gs.predict(X_test)\n\tprint('Test set accuracy score for best params: %.3f ' % accuracy_score(y_test, y_pred))\n\tif accuracy_score(y_test, y_pred) > best_acc:\n\t\tbest_acc = accuracy_score(y_test, y_pred)\n\t\tbest_gs = gs\n\t\tbest_clf = idx\nprint('\\nClassifier with best test set accuracy: %s' % grid_dict[best_clf])","8e04e60a":"# Make predictions on validation dataset\nlr = gs.best_estimator_\nlr.fit(X_train, y_train)\npredictions = lr.predict(X_test)\nprint(accuracy_score(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))","1e0d7740":"cm = confusion_matrix(y_test,predictions)\ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ncategories = ['No', 'Yes']\n\n\n# import one function: make_confusion_matrix\ndef make_confusion_matrix(cf,\n                          group_names=None,\n                          categories='auto',\n                          count=True,\n                          percent=True,\n                          cbar=True,\n                          xyticks=True,\n                          xyplotlabels=True,\n                          sum_stats=True,\n                          figsize=None,\n                          cmap='Blues',\n                          title=None):\n    '''\n    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n    Arguments\n    ---------\n    cf:            confusion matrix to be passed in\n    group_names:   List of strings that represent the labels row by row to be shown in each square.\n    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n    count:         If True, show the raw number in the confusion matrix. Default is True.\n    normalize:     If True, show the proportions for each category. Default is True.\n    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n                   Default is True.\n    xyticks:       If True, show x and y ticks. Default is True.\n    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n    sum_stats:     If True, display summary statistics below the figure. Default is True.\n    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n                   See http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                   \n    title:         Title for the heatmap. Default is None.\n    '''\n\n\n    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n    blanks = ['' for i in range(cf.size)]\n\n    if  len(group_names) ==cf.size:\n        group_labels = [\"{}\\n\".format(value) for value in group_names]\n    else:\n        group_labels = blanks\n\n    if count:\n        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n    else:\n        group_counts = blanks\n\n    if percent: \n        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()\/np.sum(cf[1])]\n    else:\n        group_percentages = blanks\n\n    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n\n\n    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n    if sum_stats:\n        #Accuracy is sum of diagonal divided by total observations\n        accuracy  = np.trace(cf) \/ float(np.sum(cf))\n\n        #if it is a binary confusion matrix, show some more stats\n        if len(cf)==2:\n            #Metrics for Binary Confusion Matrices\n            precision = cf[1,1] \/ sum(cf[:,1])\n            recall    = cf[1,1] \/ sum(cf[1,:])\n            f1_score  = 2*precision*recall \/ (precision + recall)\n            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n                accuracy,precision,recall,f1_score)\n        else:\n            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n    else:\n        stats_text = \"\"\n\n\n    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n    if figsize==None:\n        #Get default figure size if not set\n        figsize = plt.rcParams.get('figure.figsize')\n\n    if xyticks==False:\n        #Do not show categories if xyticks is False\n        categories=False\n\n\n    # MAKE THE HEATMAP VISUALIZATION\n    plt.figure(figsize=(10,7))\n    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=\"bone\",cbar=cbar,xticklabels=categories,yticklabels=categories)\n\n    if xyplotlabels:\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label' + stats_text)\n    else:\n        plt.xlabel(stats_text)\n    \n    if title:\n        plt.title(title)\n\n\nmake_confusion_matrix(cm, \n                      group_names=group_names,\n                      categories=categories, \n                      cmap='bone')","4c32ef94":"#Feature Importance for LR\n\nfrom sklearn.inspection import permutation_importance\n\nresults = permutation_importance(lr,X_train, y_train, scoring='neg_root_mean_squared_error')\n\nplt.figure(figsize=(10,8))\n\n#get importance\nimportance = results.importances_mean\nsorted_idx = np.argsort(importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.barh(pos, importance[sorted_idx],height=0.4,align='center')\n\nplt.yticks(pos, X_train.columns[sorted_idx],fontsize=10)\nplt.xlabel('Permutation Feature Importance Scores', fontsize=10)\n#plt.xticks(fontsize=100)\nplt.title('Permutation Feature Importance for Logistic Regression', fontsize=20)\n\nplt.tight_layout()\n\nplt.show()","30e5728b":"#Feature Importance for LR\n\nfrom sklearn.inspection import permutation_importance\n\nresults = permutation_importance(lr,X_train, y_train, scoring='neg_root_mean_squared_error')\n\nplt.figure(figsize=(10,8))\n\n#get importance\nimportance = results.importances_mean\nsorted_idx = np.argsort(importance)[:30]\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.barh(pos[:30], importance[sorted_idx],height=0.4,align='center')\n\nplt.yticks(pos[:30], X_train.columns[sorted_idx],fontsize=10)\nplt.xlabel('Permutation Feature Importance Scores', fontsize=10)\n#plt.xticks(fontsize=100)\nplt.title('Permutation Feature Importance for Logistic Regression', fontsize=20)\n\nplt.tight_layout()\n\nplt.show()","e329f8ec":"## Modeling spot-checking","29b7af19":"## Data Preparation\n\nThe dataset on Telecom Churn analysis had 7043 rows and 21 columns. The predictor variables in the dataset had information about customer demographics (like gender, age etc.),information about the services customers have subscribed for and the cost incurred by the customer. The dataset had few missing values which were dropped reducing the dataset to 7032 rows and 21 columns.\n\n#### 1-1) Check the Data shape\n#### 1-2) Check the Data type\n#### 1-3) Check the Missing Values","6308018f":"## Exploratory Data Analysis (Demographic Analysis)\nDuring Exploratory Data Analysis, I found some interesting details like senior citizens on average pay higher monthly bills than others and people with annual contracts are less likely to churn as compared to people with monthly contracts. PointPlots illustrated that the customer who churned paid significantly higher total charges, irrespective of the contract type and type of internet service, as compared to people who did not churn. CountPlot highlighted that people who are not senior citizens churned significantly more than senior citizens. Exploratory data analysis gave me a clear insight that people tend to churn less with increase in tenure.\n\n\n#### 4-1) Demographic Analysis\n","c6e2cff2":"## Overview\n#### 1) Data Preparation\n#### 2) Data Exploration (Visualization)\n#### 3) Exploratory Data Analysis (Demographic Analysis)\n#### 4) Feature Engineering\n#### 5) Model Building (Spot Checking)\n#### 6) Model Tuning (Hyperparameter tuning: Grid research on LDA and LR)\n#### 7) Permutation Importance test on LR\n#### 8) Refine Models(Building NN and LR) \n#### 9) Results","47a7734c":"## Telecom Churn Analysis\n \nBackground : Telecom companies and the telecom industry are growing at an unprecedented rate. With a sharp increase in customer base, the telecom companies face stiffer challenge of retaining a customer (preventing churn) due to ease of availability of great plans with almost all the telecom companies.\n\nGoal : My project focuses on analyzing the factors which could help predict if a customer would churn or not. The results of analysis could be leveraged by the telecom companies to minimize the churn by focusing on customers who are most likely to churn by offering customized plans. [](http:\/\/)\n\n### Best Results : 91% accuracy","2fc00ab3":"## Data Exploration (Visualization)\n#### 1) Tables\n#### 2) plots\n#### 3) Heat Map\n","c8b479a0":"## Feature Engineering\n\n#### 5-1) Data preprocessing for Feature Engineering\n\n#### 5-2) Data Splitting\n\n","1e62824b":"### Data Spliting\nResampling\nA widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and \/ or adding more examples from the minority class (over-sampling).","dfbc6392":"## Permutation Importance Test on LR","c14547da":"## Project Planning\nWhen starting any project, I like to outline the steps that I plan to take. Below is the rough outline that I created for this project using commented cells.","71acdff0":"# Bonus!! Nueral Network Modeling"}}