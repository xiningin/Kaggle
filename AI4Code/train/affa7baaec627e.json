{"cell_type":{"9de49bc5":"code","0e229b7f":"code","f96b21b4":"code","6993c64a":"code","b4bf8723":"code","d18fcbf5":"code","342c92ed":"code","ce1361bd":"code","46fa09e1":"code","b8b99886":"code","d41a0e64":"code","e2259f42":"markdown","f2e4b3cc":"markdown","1f490826":"markdown","fb321cb0":"markdown","00307b66":"markdown"},"source":{"9de49bc5":"from glob import glob\nfrom tqdm.notebook import tqdm\nimport math\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fastai.layers import SigmoidRange","0e229b7f":"def rmspe_metric(y_true, y_pred):\n\n    \"\"\"\n    Calculate root mean squared percentage error between ground-truth and predictions\n\n    Parameters\n    ----------\n    y_true [array-like of shape (n_samples)]: Ground-truth\n    y_pred [array-like of shape (n_samples)]: Predictions\n\n    Returns\n    -------\n    rmspe (float): Root mean squared percentage error\n    \"\"\"\n\n    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n    return rmspe\n\n\ndef evaluate_predictions(predictions_column, evaluate_stock=False):\n    \n    for fold in sorted(df_train['fold'].unique()):\n\n        _, val_idx = df_train.loc[df_train['fold'] != fold].index, df_train.loc[df_train['fold'] == fold].index\n        fold_score = rmspe_metric(df_train.loc[val_idx, 'target'], df_train.loc[val_idx, predictions_column])\n        print(f'Fold {fold} - RMSPE: {fold_score:.6}')\n\n    oof_score = rmspe_metric(df_train['target'], df_train[predictions_column])\n    print(f'{\"-\" * 30}\\nOOF RMSPE: {oof_score:.6}\\n{\"-\" * 30}')\n\n    if evaluate_stock:\n        for stock_id in df_train['stock_id'].unique():\n            df_stock = df_train.loc[df_train['stock_id'] == stock_id, :]\n            stock_oof_score = rmspe_metric(df_stock['target'], df_stock[predictions_column])\n            print(f'Stock {stock_id} - OOF RMSPE: {stock_oof_score:.6}')\n","f96b21b4":"class PreprocessingPipeline:\n    \n    def __init__(self, df_train, df_test):\n        \n        self.df_train = df_train.copy(deep=True)\n        self.df_test = df_test.copy(deep=True)\n        \n    def _label_encode(self):\n\n        # Encoding stock_id for embeddings\n        le = LabelEncoder()\n        self.df_train['stock_id_encoded'] = le.fit_transform(self.df_train['stock_id'].values)\n        self.df_test['stock_id_encoded'] = le.transform(self.df_test['stock_id'].values)\n    \n    def _get_folds(self):\n        \n        # Load pre-computed folds\n        self.df_train['fold'] = pd.read_csv('..\/input\/optiver-realized-volatility-dataset\/folds.csv')['fold_group']\n        self.df_train['fold'] = self.df_train['fold'].astype(np.uint8)\n        \n    def transform(self):\n        \n        self._label_encode()\n        self._get_folds()\n        \n        return self.df_train, self.df_test\n","6993c64a":"train_test_dtypes = {\n    'stock_id': np.uint8,\n    'time_id': np.uint16,\n    'target': np.float64\n}\n\ndf_train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv', dtype=train_test_dtypes)\ndf_test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv', usecols=['stock_id', 'time_id'], dtype=train_test_dtypes)\n\n# Using only first row of test set if it is the placeholder\n# Other rows break the pipeline since some of the time buckets don't exist in book data\nif df_test.shape[0] == 3:\n    df_test = df_test.head(1)\n\npreprocessing_pipeline = PreprocessingPipeline(df_train, df_test)\ndf_train, df_test = preprocessing_pipeline.transform()\n\nprint(f'Training Set Shape: {df_train.shape} - Memory Usage: {df_train.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\nprint(f'Test Set Shape: {df_test.shape} - Memory Usage: {df_test.memory_usage().sum() \/ 1024 ** 2:.2f} MB')","b4bf8723":"class Conv1dBlock(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, skip_connection=False):\n\n        super(Conv1dBlock, self).__init__()\n\n        self.skip_connection = skip_connection\n        self.conv_block = nn.Sequential(\n            nn.Conv1d(\n                in_channels,\n                out_channels,\n                kernel_size=(kernel_size,),\n                stride=(stride,),\n                padding=(kernel_size \/\/ 2,),\n                padding_mode='replicate',\n                bias=True\n            ),\n            nn.BatchNorm1d(out_channels),\n            nn.ReLU(),\n        )\n        self.downsample = nn.Sequential(\n            nn.Conv1d(\n                in_channels,\n                out_channels,\n                kernel_size=(1,),\n                stride=(stride,),\n                bias=False\n            ),\n            nn.BatchNorm1d(out_channels)\n        )\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n\n        output = self.conv_block(x)\n        if self.skip_connection:\n            x = self.downsample(x)\n            output += x\n        output = self.relu(output)\n\n        return output\n\n\nclass Conv1dLayers(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, depth_scale, width_scale, skip_connection, initial=False):\n\n        super(Conv1dLayers, self).__init__()\n\n        depth = int(math.ceil(2 * depth_scale))\n        width = int(math.ceil(out_channels * width_scale))\n\n        if initial:\n            layers = [\n                Conv1dBlock(\n                    in_channels=in_channels,\n                    out_channels=width,\n                    kernel_size=kernel_size,\n                    stride=2,\n                    skip_connection=skip_connection\n                )\n            ]\n        else:\n            layers = [\n                Conv1dBlock(\n                    in_channels=(int(math.ceil(in_channels * width_scale))),\n                    out_channels=width,\n                    kernel_size=kernel_size,\n                    stride=2,\n                    skip_connection=skip_connection\n                )\n            ]\n\n        for _ in range(depth - 1):\n            layers += [\n                Conv1dBlock(\n                    in_channels=width,\n                    out_channels=width,\n                    kernel_size=kernel_size,\n                    stride=1,\n                    skip_connection=skip_connection\n                )\n            ]\n\n        self.conv_layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.conv_layers(x)\n    \n\nclass NonLocalBlock1d(nn.Module):\n\n    def __init__(self, in_channels, inter_channels=None, mode='embedded'):\n\n        super(NonLocalBlock1d, self).__init__()\n\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n        self.mode = mode\n\n        if self.inter_channels is None:\n            self.inter_channels = in_channels \/\/ 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        self.g = nn.Conv1d(\n            in_channels=self.in_channels,\n            out_channels=self.inter_channels,\n            kernel_size=(1,),\n            stride=(1,),\n            padding=0\n        )\n        self.W_z = nn.Sequential(\n            nn.Conv1d(\n                in_channels=self.inter_channels,\n                out_channels=self.in_channels,\n                kernel_size=(1,),\n                stride=(1,),\n                padding=0\n            ),\n            nn.BatchNorm1d(self.in_channels)\n        )\n        nn.init.constant_(self.W_z[1].weight, 0)\n        nn.init.constant_(self.W_z[1].bias, 0)\n\n        if self.mode == 'embedded' or self.mode == 'dot' or self.mode == 'concatenate':\n\n            self.theta = nn.Conv1d(\n                in_channels=self.in_channels,\n                out_channels=self.inter_channels,\n                kernel_size=(1,),\n                stride=(1,),\n                padding=0\n            )\n            self.phi = nn.Conv1d(\n                in_channels=self.in_channels,\n                out_channels=self.inter_channels,\n                kernel_size=(1,),\n                stride=(1,),\n                padding=0\n            )\n\n        if self.mode == 'concatenate':\n\n            self.W_f = nn.Sequential(\n                nn.Conv2d(\n                    in_channels=self.inter_channels * 2,\n                    out_channels=1,\n                    kernel_size=(1, 1),\n                    stride=(1, 1),\n                    padding=0,\n                    bias=False\n                ),\n                nn.ReLU()\n            )\n\n    def forward(self, x):\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        if self.mode == 'gaussian':\n\n            theta_x = x.view(batch_size, self.in_channels, -1)\n            phi_x = x.view(batch_size, self.in_channels, -1)\n            theta_x = theta_x.permute(0, 2, 1)\n            f = torch.matmul(theta_x, phi_x)\n\n        elif self.mode == 'embedded' or self.mode == 'dot':\n\n            theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n            phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n            theta_x = theta_x.permute(0, 2, 1)\n            f = torch.matmul(theta_x, phi_x)\n\n        elif self.mode == 'concatenate':\n\n            theta_x = self.theta(x).view(batch_size, self.inter_channels, -1, 1)\n            phi_x = self.phi(x).view(batch_size, self.inter_channels, 1, -1)\n            h = theta_x.size(2)\n            w = phi_x.size(3)\n            theta_x = theta_x.repeat(1, 1, 1, w)\n            phi_x = phi_x.repeat(1, 1, h, 1)\n            concat = torch.cat([theta_x, phi_x], dim=1)\n            f = self.W_f(concat)\n            f = f.view(f.size(0), f.size(2), f.size(3))\n\n        if self.mode == 'gaussian' or self.mode == 'embedded':\n            f_div_C = F.softmax(f, dim=-1)\n        elif self.mode == 'dot' or self.mode == 'concatenate':\n            N = f.size(-1)\n            f_div_C = f \/ N\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n\n        W_y = self.W_z(y)\n        z = W_y + x\n\n        return z\n\n\nclass CNN1DModel(nn.Module):\n\n    def __init__(self, in_channels, out_channels, use_stock_id, stock_embedding_dims, alpha, beta, phi):\n\n        super(CNN1DModel, self).__init__()\n\n        # Stock embeddings\n        self.use_stock_id = use_stock_id\n        self.stock_embedding_dims = stock_embedding_dims\n        self.stock_embeddings = nn.Embedding(num_embeddings=113, embedding_dim=self.stock_embedding_dims)\n        self.dropout = nn.Dropout(0.25)\n\n        # Model scaling\n        depth_scale = alpha ** phi\n        width_scale = beta ** phi\n        self.out_channels = int(math.ceil(out_channels * width_scale))\n\n        # Convolutional layers\n        self.conv_layers1 = Conv1dLayers(\n            in_channels=in_channels,\n            out_channels=32,\n            kernel_size=5,\n            depth_scale=depth_scale,\n            width_scale=width_scale,\n            skip_connection=False,\n            initial=True\n        )\n        self.conv_layers2 = Conv1dLayers(\n            in_channels=32,\n            out_channels=64,\n            kernel_size=7,\n            depth_scale=depth_scale,\n            width_scale=width_scale,\n            skip_connection=False,\n            initial=False\n        )\n        self.conv_layers3 = Conv1dLayers(\n            in_channels=64,\n            out_channels=128,\n            kernel_size=9,\n            depth_scale=depth_scale,\n            width_scale=width_scale,\n            skip_connection=False,\n            initial=False\n        )\n        self.conv_layers4 = Conv1dLayers(\n            in_channels=128,\n            out_channels=self.out_channels,\n            kernel_size=11,\n            depth_scale=depth_scale,\n            width_scale=width_scale,\n            skip_connection=False,\n            initial=False\n        )\n        \n        # Non-local blocks\n        self.nl_block1 = NonLocalBlock1d(in_channels=32, mode='embedded')\n        self.nl_block2 = NonLocalBlock1d(in_channels=64, mode='embedded')\n        self.nl_block3 = NonLocalBlock1d(in_channels=128, mode='embedded')\n        self.nl_block4 = NonLocalBlock1d(in_channels=self.out_channels, mode='embedded')\n        \n        self.pooling = nn.AdaptiveAvgPool1d(1)\n        self.head = nn.Sequential(\n            nn.Linear(256 + self.stock_embedding_dims, 1, bias=True),\n            SigmoidRange(0, 0.1)\n        )\n\n    def forward(self, stock_ids, sequences):\n\n        x = torch.transpose(sequences, 1, 2)\n        x = self.conv_layers1(x)\n        x = self.nl_block1(x)\n        x = self.conv_layers2(x)\n        x = self.nl_block2(x)\n        x = self.conv_layers3(x)\n        x = self.nl_block3(x)\n        x = self.conv_layers4(x)\n        x = self.nl_block4(x)\n        x = self.pooling(x)\n        x = x.view(-1, x.shape[1])\n\n        if self.use_stock_id:\n            embedded_stock_ids = self.stock_embeddings(stock_ids)\n            x = torch.cat([x, self.dropout(embedded_stock_ids)], dim=1)\n\n        output = self.head(x)\n        return output.view(-1)\n","d18fcbf5":"class SelfAttention(nn.Module):\n\n    def __init__(self, attention_size):\n\n        super(SelfAttention, self).__init__()\n\n        self.attention_weights = nn.Parameter(torch.FloatTensor(attention_size))\n        nn.init.uniform_(self.attention_weights.data, -0.005, 0.005)\n\n        self.softmax = nn.Softmax(dim=-1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n\n        attentions = self.relu(x.matmul(self.attention_weights))\n        attentions = self.softmax(attentions)\n        weighted = torch.mul(x, attentions.unsqueeze(-1).expand_as(x))\n        representations = weighted.sum(1).squeeze()\n\n        return representations, attentions\n\n    \nclass RNNModel(nn.Module):\n\n    def __init__(self, input_size, hidden_size, num_layers, use_stock_id, stock_embedding_dims):\n\n        super(RNNModel, self).__init__()\n\n        # Stock embeddings\n        self.use_stock_id = use_stock_id\n        self.stock_embedding_dims = stock_embedding_dims\n        self.stock_embeddings = nn.Embedding(num_embeddings=113, embedding_dim=self.stock_embedding_dims)\n        self.dropout = nn.Dropout(0.25)\n\n        # Recurrent neural network\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n        self.gru = nn.GRU(\n            input_size=self.input_size,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=0,\n            bidirectional=False,\n            batch_first=True\n        )\n\n        # Sequence self attention\n        self.attention = SelfAttention(attention_size=self.hidden_size)\n        \n        self.head = nn.Sequential(\n            nn.Linear(self.hidden_size + self.stock_embedding_dims, 1, bias=True),\n            SigmoidRange(0, 0.1)\n        )\n\n    def forward(self, stock_ids, sequences):\n        \n        h_n0 = torch.zeros(self.num_layers, sequences.size(0), self.hidden_size).to(self.device)\n        gru_output, h_n = self.gru(sequences, h_n0)\n        representations, attentions = self.attention(gru_output)\n\n        if self.use_stock_id:\n            embedded_stock_ids = self.stock_embeddings(stock_ids)\n            x = torch.cat([representations.view(1, -1), self.dropout(embedded_stock_ids)], dim=1)\n\n        output = self.head(x)\n        return output.view(-1)\n","342c92ed":"# Normalizing sequences with global means and stds\nbook_means = np.array([\n    # Raw sequences\n    0.99969482421875, 1.000321388244629, 0.9995064735412598, 1.0005191564559937,\n    769.990177708821, 766.7345672818379, 959.3416027831918, 928.2202512713748,\n    # Absolute log returns of raw sequences\n    5.05890857311897e-05, 5.1026330766035244e-05, 5.74059049540665e-05, 5.8218309277435765e-05,\n    0.3967152245253066, 0.39100519899866804, 0.3239659116907835, 0.31638538484106116,\n    # Weighted average prices\n    1.0000068043192514, 1.0000055320253616, 1.000006872969592,\n    # Absolute log returns of weighted average prices\n    8.211420490291096e-05, 0.00011112522790786203, 8.236187150264073e-05\n])\nbook_stds = np.array([\n    # Raw sequences\n    0.0036880988627672195, 0.003687119111418724, 0.0037009266670793295, 0.0036990800872445107,\n    5354.051690318169, 4954.947103063445, 6683.816183660414, 5735.299917793827,\n    # Absolute log returns of raw sequences\n    0.00016576898633502424, 0.00016801751917228103, 0.0001837657910073176, 0.0001868011022452265,\n    0.9121719707304721, 0.8988021131995019, 0.8415323589617927, 0.8244750862945265,\n    # Weighted average prices\n    0.003689893218043926, 0.00370745215558702, 0.0036913980961173682,\n    # Absolute log returns of weighted average prices\n    0.00021108155612872302, 0.00029320157822289604, 0.00019975085953727163\n])\ntrade_means = np.array([0.999971866607666, 352.9736760331942, 4.1732040971227145])\ntrade_stds = np.array([0.004607073962688446, 1041.9441951057488, 7.79955795393431])\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# CNN 1D\ncnn_parameters = {\n    'in_channels': 25,\n    'out_channels': 256,\n    'use_stock_id': True,\n    'stock_embedding_dims': 16,\n    'alpha': 1,\n    'beta': 1,\n    'phi': 1\n}\n\nprint(f'CNN1D Models\\n{\"-\" * 12}')\ncnn_models = []\nfor model_path in sorted(glob(f'..\/input\/optiver-realized-volatility-dataset\/cnn1d\/*.pt')):\n    print(f'Loading model {model_path} into memory')\n    model = CNN1DModel(**cnn_parameters)\n    model.load_state_dict(torch.load(model_path))\n    model.to(device)\n    model.eval()\n    cnn_models.append(model)\n    \n# RNN\nrnn_parameters = {\n    'input_size': 25,\n    'hidden_size': 64,\n    'num_layers': 3,\n    'use_stock_id': True,\n    'stock_embedding_dims': 16,\n}\n\nprint(f'\\nRNN Models\\n{\"-\" * 10}')\nrnn_models = []\nfor model_path in sorted(glob(f'..\/input\/optiver-realized-volatility-dataset\/rnn\/*.pt')):\n    print(f'Loading model {model_path} into memory')\n    model = RNNModel(**rnn_parameters)\n    model.load_state_dict(torch.load(model_path))\n    model.to(device)\n    model.eval()\n    rnn_models.append(model)\n","ce1361bd":"# Loading pre-computed train predictions and evaluate them\ndf_train['cnn1d_predictions'] = pd.read_csv('..\/input\/optiver-realized-volatility-dataset\/cnn1d\/cnn1d_predictions.csv').values\ndf_train['rnn_predictions'] = pd.read_csv('..\/input\/optiver-realized-volatility-dataset\/rnn\/rnn_predictions.csv').values\n\nprint(f'CNN1D\\n{\"-\" * 5}')\nevaluate_predictions('cnn1d_predictions')\n\nprint(f'\\nRNN\\n{\"-\" * 3}')\nevaluate_predictions('rnn_predictions')\n\nprint(f'\\nBlend\\n{\"-\" * 5}')\ndf_train['blend_predictions'] = (df_train['cnn1d_predictions'] * 0.5) + (df_train['rnn_predictions'] * 0.5)\nevaluate_predictions('blend_predictions')","46fa09e1":"book_features = ['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2','bid_size1', 'ask_size1', 'bid_size2', 'ask_size2']\ntrade_features = ['price', 'size', 'order_count']\nstock_id_mapping = df_train.set_index('stock_id')['stock_id_encoded'].to_dict()\n\nfor stock_id in tqdm(df_test['stock_id'].unique()):\n    \n    df_stock = df_test.loc[df_test['stock_id'] == stock_id]\n    df_book = pd.read_parquet(f'..\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/stock_id={stock_id}')\n    df_trade = pd.read_parquet(f'..\/input\/optiver-realized-volatility-prediction\/trade_test.parquet\/stock_id={stock_id}')\n    stock_time_buckets = df_test.loc[df_test['stock_id'] == stock_id, 'time_id'].reset_index(drop=True)\n    missing_time_buckets = stock_time_buckets[~stock_time_buckets.isin(df_trade['time_id'])]\n    df_trade = df_trade.merge(missing_time_buckets, how='outer')\n    \n    # Iterating over time_ids\n    for time_id in df_stock['time_id'].unique():\n        \n        # Resample order book to 600 seconds, forward fill and back fill for edge cases\n        df_book_time_bucket = df_book.loc[df_book['time_id'] == time_id]\n        df_book_time_bucket = df_book_time_bucket.set_index(['seconds_in_bucket'])\n        df_book_time_bucket = df_book_time_bucket.reindex(np.arange(0, 600), method='ffill').fillna(method='bfill')\n        \n        # Sequences from book data\n        book_sequences = df_book_time_bucket.reset_index(drop=True)[book_features].values\n        \n        # Absolute log returns of raw sequences\n        book_bid_price1_log = np.log(book_sequences[:, 0])\n        book_bid_price1_absolute_log_returns = np.abs(np.diff(book_bid_price1_log, prepend=[book_bid_price1_log[0]]))\n        book_ask_price1_log = np.log(book_sequences[:, 1])\n        book_ask_price1_absolute_log_returns = np.abs(np.diff(book_ask_price1_log, prepend=[book_ask_price1_log[0]]))\n        book_bid_price2_log = np.log(book_sequences[:, 2])\n        book_bid_price2_absolute_log_returns = np.abs(np.diff(book_bid_price2_log, prepend=[book_bid_price2_log[0]]))\n        book_ask_price2_log = np.log(book_sequences[:, 3])\n        book_ask_price2_absolute_log_returns = np.abs(np.diff(book_ask_price2_log, prepend=[book_ask_price2_log[0]]))\n        book_bid_size1_log = np.log(book_sequences[:, 4])\n        book_bid_size1_absolute_log_returns = np.abs(np.diff(book_bid_size1_log, prepend=[book_bid_size1_log[0]]))\n        book_ask_size1_log = np.log(book_sequences[:, 5])\n        book_ask_size1_absolute_log_returns = np.abs(np.diff(book_ask_size1_log, prepend=[book_ask_size1_log[0]]))\n        book_bid_size2_log = np.log(book_sequences[:, 6])\n        book_bid_size2_absolute_log_returns = np.abs(np.diff(book_bid_size2_log, prepend=[book_bid_size2_log[0]]))\n        book_ask_size2_log = np.log(book_sequences[:, 7])\n        book_ask_size2_absolute_log_returns = np.abs(np.diff(book_ask_size2_log, prepend=[book_ask_size2_log[0]]))\n\n        # Weighted average prices\n        book_wap1 = (book_sequences[:, 0] * book_sequences[:, 5] + book_sequences[:, 1] * book_sequences[:, 4]) \/\\\n                    (book_sequences[:, 4] + book_sequences[:, 5])\n        book_wap2 = (book_sequences[:, 2] * book_sequences[:, 7] + book_sequences[:, 3] * book_sequences[:, 6]) \/\\\n                    (book_sequences[:, 6] + book_sequences[:, 7])\n        book_wap3 = ((book_sequences[:, 0] * book_sequences[:, 5] + book_sequences[:, 1] * book_sequences[:, 4]) +\n                     (book_sequences[:, 2] * book_sequences[:, 7] + book_sequences[:, 3] * book_sequences[:, 6])) \/\\\n                    (book_sequences[:, 4] + book_sequences[:, 5] + book_sequences[:, 6] + book_sequences[:, 7])\n\n        # Absolute log returns of weighted average prices\n        book_wap1_log = np.log(book_wap1)\n        book_wap1_absolute_log_returns = np.abs(np.diff(book_wap1_log, prepend=[book_wap1_log[0]]))\n        book_wap2_log = np.log(book_wap2)\n        book_wap2_absolute_log_returns = np.abs(np.diff(book_wap2_log, prepend=[book_wap2_log[0]]))\n        book_wap3_log = np.log(book_wap3)\n        book_wap3_absolute_log_returns = np.abs(np.diff(book_wap3_log, prepend=[book_wap3_log[0]]))\n\n        book_sequences = np.hstack([\n            book_sequences,\n            book_bid_price1_absolute_log_returns.reshape(-1, 1),\n            book_ask_price1_absolute_log_returns.reshape(-1, 1),\n            book_bid_price2_absolute_log_returns.reshape(-1, 1),\n            book_ask_price2_absolute_log_returns.reshape(-1, 1),\n            book_bid_size1_absolute_log_returns.reshape(-1, 1),\n            book_ask_size1_absolute_log_returns.reshape(-1, 1),\n            book_bid_size2_absolute_log_returns.reshape(-1, 1),\n            book_ask_size2_absolute_log_returns.reshape(-1, 1),\n            book_wap1.reshape(-1, 1),\n            book_wap2.reshape(-1, 1),\n            book_wap3.reshape(-1, 1),\n            book_wap1_absolute_log_returns.reshape(-1, 1),\n            book_wap2_absolute_log_returns.reshape(-1, 1),\n            book_wap3_absolute_log_returns.reshape(-1, 1),\n        ])\n        book_sequences = (book_sequences - book_means) \/ book_stds\n        \n        # Resample trade data to 600 seconds and fill missing values with 0\n        df_trade_time_bucket = df_trade.loc[df_trade['time_id'] == time_id]\n        df_trade_time_bucket = df_trade_time_bucket.set_index(['seconds_in_bucket'])\n        df_trade_time_bucket = df_trade_time_bucket.reindex(np.arange(0, 600)).fillna(0)\n        \n        # Sequences from trade data\n        trade_sequences = df_trade_time_bucket.reset_index(drop=True)[trade_features].values\n        # Not normalizing zero values in trade data\n        trade_sequences[trade_sequences[:, 0] != 0, :] = (trade_sequences[trade_sequences[:, 0] != 0, :] - trade_means) \/ trade_stds\n        \n        # Concatenate book and trade sequences\n        sequences = np.hstack([book_sequences, trade_sequences])\n        sequences = torch.as_tensor(sequences.reshape(1, 600, 25), dtype=torch.float)\n        sequences = sequences.to(device)\n        \n        stock_id_encoded = torch.as_tensor([stock_id_mapping[stock_id]], dtype=torch.long)\n        stock_id_encoded = stock_id_encoded.to(device)\n        \n        cnn_prediction = 0\n        for model in cnn_models:\n            with torch.no_grad():\n                cnn_model_prediction = model(stock_id_encoded, sequences).detach().cpu().numpy()\n                cnn_prediction += (cnn_model_prediction[0] \/ 5)\n        df_test.loc[(df_test['stock_id'] == stock_id) & (df_test['time_id'] == time_id), 'cnn1d_predictions'] = cnn_prediction\n                \n        rnn_prediction = 0\n        for model in rnn_models:\n            with torch.no_grad():\n                rnn_model_prediction = model(stock_id_encoded, sequences).detach().cpu().numpy()\n                rnn_prediction += (rnn_model_prediction[0] \/ 5)\n        df_test.loc[(df_test['stock_id'] == stock_id) & (df_test['time_id'] == time_id), 'rnn_predictions'] = rnn_prediction\n","b8b99886":"df_test['target'] = (df_test['cnn1d_predictions'] * 0.5) + (df_test['rnn_predictions'] * 0.5)\ndf_test['row_id'] = df_test['stock_id'].astype(str) + '-' + df_test['time_id'].astype(str)\ndf_test[['row_id', 'target']].to_csv('submission.csv', index=False)","d41a0e64":"df_test[['row_id', 'target']]","e2259f42":"## Preprocessing","f2e4b3cc":"## 1D CNN","1f490826":"## Models","fb321cb0":"## Evaluation","00307b66":"## RNN"}}