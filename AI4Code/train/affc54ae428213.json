{"cell_type":{"f28e5951":"code","7ab75147":"code","5da80d24":"code","b4710f48":"code","e98c94d2":"code","049cfb89":"code","fd4af2d8":"code","96f568b1":"code","5956f76a":"code","28c52fe2":"code","9b9d40a6":"code","870a34ed":"code","91875fb2":"code","bd56227f":"code","f8ff3ab4":"code","53dd29c2":"code","8cfcb4d5":"code","1df566e5":"markdown","8facd72a":"markdown","5ca21e9a":"markdown","82aa1a3b":"markdown","137beb35":"markdown","46a79977":"markdown"},"source":{"f28e5951":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport random\nfrom sklearn.pipeline import Pipeline\nfrom pathlib import Path\n\n\nfrom os.path import join, isfile\nfrom os import path, scandir, listdir\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Embedding,  Flatten\nfrom tensorflow.keras.models import Model, Sequential\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.optimizers import RMSprop\nimport keras_tuner as kt\n\nfrom tensorflow.data import Dataset\nfrom sklearn.preprocessing import QuantileTransformer,  KBinsDiscretizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow import keras\nfrom sklearn import metrics\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import roc_auc_score\n\nimport gc\nimport warnings\nwarnings.simplefilter('ignore')","7ab75147":"target = 'target'\n\nDEBUG = False\n\nif DEBUG:\n    N_ESTIMATORS = 1\n    N_SPLITS = 2\n    SEED = 2017\n    CVSEED = 2017\n    EARLY_STOPPING_ROUNDS = 1\n    VERBOSE = 100\n    BINS = 128\n    #N_ITERS = 2\nelse:\n    N_SPLITS = 5\n    N_ESTIMATORS = 20000\n    EARLY_STOPPING_ROUNDS = 300\n    VERBOSE = 1000\n    SEED = 2017\n    CVSEED = 2017\n    BINS = 128\n    #N_ITERS = 10","5da80d24":"def set_seed(seed=2017):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.compat.v1.set_random_seed(seed)\n    \nset_seed(SEED)","b4710f48":"INPUT = Path(\"..\/input\/tabular-playground-series-oct-2021\")\n\ntrain = pd.DataFrame(pd.read_csv(INPUT \/ \"train.csv\")[target])\n#test = pd.read_csv(INPUT \/ \"test.csv\")\ntest = pd.DataFrame()\nsub = pd.read_csv(INPUT \/ \"sample_submission.csv\")","e98c94d2":"def list_all_files(location='..\/input\/tps-oct-lv0', pattern=None, recursive=True):\n    \"\"\"\n    This function returns a list of files at a given location (including subfolders)\n    \n    - location: path to the directory to be searched\n    - pattern: part of the file name to be searched (ex. pattern='.csv' would return all the csv files)\n    - recursive: boolean, if True the function calls itself for every subdirectory it finds\n    \"\"\"\n    subdirectories= [f.path for f in scandir(location) if f.is_dir()]\n    files = [join(location, f) for f in listdir(location) if isfile(join(location, f))]\n    if recursive:\n        for directory in subdirectories:\n            files.extend(list_all_files(directory))\n    if pattern:\n        files = [f for f in files if pattern in f]\n    return files","049cfb89":"names = ['bizen', 'henke', 'hamza', '28smiles','kashif', 'kosta', 'kaveh', 'dlaststark', 'pca', 'xgb2']","fd4af2d8":"namesec = ['lonnie', 'hgb', 'xgb_d2s', 'ctb_d2s']","96f568b1":"pred = list_all_files(pattern='oof')\n\n\nfor i in range(len(names)):\n    avv = []\n    \n    for file in pred:\n        if names[i] in file.split('\/')[3]:\n            avv.append(np.load(file))\n    train[names[i]] = np.mean(avv, axis=0)\n            \npred = list_all_files(location='..\/input\/tps-oct-lv0-sec', pattern='oof')\n\nfor i in range(len(namesec)):\n    avv = []\n    \n    for file in pred:\n        if namesec[i] in file.split('\/')[3]:\n            avv.append(np.load(file))\n    train[namesec[i]] = np.mean(avv, axis=0)\n            \n            \n    \ntrain.columns","5956f76a":"pred = list_all_files(pattern='pred')\n\n\nfor i in range(len(names)):\n    avv = []\n    \n    for file in pred:\n        if names[i] in file.split('\/')[3]:\n            avv.append(np.load(file))\n    test[names[i]] = np.mean(avv, axis=0)\n            \npred = list_all_files(location='..\/input\/tps-oct-lv0-sec', pattern='pred')\n\nfor i in range(len(namesec)):\n    avv = []\n    \n    for file in pred:\n        if namesec[i] in file.split('\/')[3]:\n            avv.append(np.load(file))\n    test[namesec[i]] = np.mean(avv, axis=0)\n    \ntest.columns","28c52fe2":"avv = []\nfor i in range(5):\n    avv.append(np.load(\"..\/input\/tps-oct-lv0\/\"+str(i+2017)+\"lgb_oof.npy\"))\ntrain['lgb'] = np.mean(avv, axis=0)\n\navv = []\nfor i in range(5):\n    avv.append(np.load(\"..\/input\/tps-oct-lv0\/agg\"+str(i+1)+\"_xgb_oof.npy\"))\ntrain['xgb'] = np.mean(avv, axis=0)\n\navv = []\nfor i in range(5):\n    avv.append(np.load(\"..\/input\/tps-oct-lv0\/\"+str(i+2017)+\"lgb_pred.npy\"))\ntest['lgb'] = np.mean(avv, axis=0)\n\navv = []\nfor i in range(5):\n    avv.append(np.load(\"..\/input\/tps-oct-lv0\/agg\"+str(i+1)+\"_xgb_pred.npy\"))\ntest['xgb'] = np.mean(avv, axis=0)","9b9d40a6":"features = train.columns[1:]\n\npipe = Pipeline([\n       # ('imputer', SimpleImputer(strategy='median',missing_values=np.nan)),\n        (\"scaler\", QuantileTransformer(n_quantiles=BINS,output_distribution='normal')),\n        ('bin', KBinsDiscretizer(n_bins=BINS, encode='ordinal',strategy='uniform'))\n        ])\n#train[features] = pipe.fit_transform(train[features])\n#test[features] = pipe.transform(test[features])","870a34ed":"test[features]","91875fb2":"train[target]","bd56227f":"def make_model():\n    \n    lr = 0.0157\n    dropout =0.125\n    embed_dim =4\n    hidden_dim = 144\n    n_layers = 3\n    act = 'swish'\n    #dstep = hp.Int('decay_steps', min_value=2000, max_value=4000, step=200)\n    drate = 0.885\n    eps =3.6e-08\n        \n    \n    inputs = Input(train[features].shape[1:])\n    X = Embedding(input_dim=BINS, output_dim=embed_dim, embeddings_initializer = \"glorot_normal\")(inputs)\n    X = Dropout(dropout)(X)\n    #X = BatchNormalization()(X)\n    X = Flatten()(X)\n    \n    for i in range(n_layers):\n        #units = hp.Int('units_{i}'.format(i=i), min_value=8, max_value=256, step=8)\n        X = layers.Dense(hidden_dim\/(2**i), activation=act, kernel_initializer=tf.keras.initializers.GlorotNormal())(X)\n        X = Dropout(dropout)(X)\n        #X = BatchNormalization()(X)\n    outputs = layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.GlorotNormal())(X)\n    model = keras.Model(inputs, outputs)\n    \n    #learning_rate = hp.Float('learning_rate', min_value=3e-4, max_value=3e-3)\n    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=lr,\n        decay_steps = 400,\n        decay_rate= drate)\n\n    optimizer = keras.optimizers.Adam(learning_rate=lr_schedule, epsilon=eps)\n    model.compile(loss=keras.losses.binary_crossentropy,\n                  optimizer=optimizer,\n                  metrics=[tf.keras.metrics.AUC(name='aucroc')])\n    #model.summary()\n    return model","f8ff3ab4":"def prediction(x, y, batch_size=1024, epochs=100):\n    cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=CVSEED)\n    val_losses = np.zeros(x.shape[0])\n    nn_pred = np.zeros(test.shape[0])\n    \n    for fold, (train_indices, val_indices) in enumerate(cv.split(x, y)):\n      print(f\"===== fold {fold} =====\")\n      x_train, x_valid = x.iloc[train_indices], x.iloc[val_indices]\n      y_train, y_valid = y.iloc[train_indices], y.iloc[val_indices]\n      x_test = test[features]\n    \n      gc.collect()\n        \n      x_train[features] = pipe.fit_transform(x_train[features])\n      x_valid[features] = pipe.transform(x_valid[features])\n      x_test[features] = pipe.transform(x_test[features])\n        \n      \n    \n      model = make_model()\n      model.fit( x_train[features], y_train, \n                validation_data=(x_valid[features] , y_valid),\n                shuffle=True,\n                verbose=0,\n                #callbacks=[model_checkpoint_callback],\n                callbacks=[\n                #tf.keras.callbacks.ReduceLROnPlateau(monitor='val_aucroc', mode='max', patience=2),\n                tf.keras.callbacks.EarlyStopping(monitor='val_aucroc', mode='max', patience=5)  ],\n                batch_size=batch_size, \n                epochs=epochs)\n      val_losses[val_indices] += model.predict(x_valid[features] )[:,-1]\n      nn_pred += model.predict(x_test[features] )[:,-1]\n\n      auc = roc_auc_score(y_valid, val_losses[val_indices])\n      print(f\"fold {fold} - nn auc: {auc:.6f}\")   \n    \n      del model\n      gc.collect()\n     \n    nn_pred \/= N_SPLITS\n    print(f\"oof nn_auc = {roc_auc_score(y, val_losses)}\")\n    \n    np.save(\"nn_oof.npy\", val_losses)\n    np.save(\"nn_pred.npy\", nn_pred)\n\n    gc.collect()\n    return nn_pred","53dd29c2":"nn_pred = prediction(x=train[features], y=train[target], \n             batch_size=1024, \n             epochs=100,\n             #validation_data=(x_val, y_val),\n            )","8cfcb4d5":"sub[target]=nn_pred\nsub.to_csv('submission.csv', index=False)\n\nsub","1df566e5":"# Parameters","8facd72a":"# NN","5ca21e9a":"# Model","82aa1a3b":"# Preprocessing","137beb35":"# Load Dataset","46a79977":"# Log\n\n\/\/\/\/\/\/\/ all average \/\/\/\/\/\/\/\/\/\n\n\/\/\/ 128 quant normal kbins uniform with dropout, dstep=400, decreasing hidden units \/\/\/\n2017 ver1\n"}}