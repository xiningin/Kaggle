{"cell_type":{"af3164e0":"code","88f175b9":"code","ce36d895":"code","e174a874":"code","780cae5e":"code","00a9d73b":"code","009b5b5f":"code","2300cb10":"markdown","5a7e61ac":"markdown","b100095a":"markdown","91ae29f9":"markdown","e10cfd0e":"markdown"},"source":{"af3164e0":"\nimport numpy as np\nimport pandas as pd \nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport os\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import  f1_score\nfrom sklearn import preprocessing ,decomposition, model_selection,metrics,pipeline\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn import tree\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, plot_confusion_matrix\nfrom sklearn.svm import LinearSVC, SVC, NuSVC\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nimport random","88f175b9":"df = pd.read_csv('\/kaggle\/input\/wine-quality-binary-classification\/wine.csv')\ndf_data = df.replace(['good', 'bad'], [0,1])\ntarget = df_data.quality\nfeatures = df.columns\ndf_data.drop(columns = ['quality'], inplace=True)\ndf_data = preprocessing.scale(df_data)\ndf_data = pd.DataFrame(data=df_data, columns=features[0:11])","ce36d895":"X_train, X_test, y_train, y_test = train_test_split(df_data, target, random_state = 1, shuffle = True)","e174a874":"def show_cm(classifier, X_test, y_test):\n    plt.style.use('default')\n    class_names = ['Bad', 'Good']\n    titles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\n    for title, normalize in titles_options:\n        disp = plot_confusion_matrix(classifier, X_test, y_test,\n                                 display_labels=class_names,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize,\n                                 xticks_rotation = 30)\n        plt.title(title)\n        plt.show()","780cae5e":"# Logistic Regression\nlr = LogisticRegression()\nlr.fit(X_train ,y_train)\nlr_pred = lr.predict(X_test)\nlr_score = accuracy_score(y_test, lr_pred)\nprint('LogisticRegression Score: ', lr_score)\nshow_cm(lr, df_data, target)\n\n# XGBoost\nxgb_classifier=xgb.XGBClassifier(objective='binary:logistic',learning_rate = 0.1,gamma=0.01,max_depth = 10,booster=\"gbtree\")\nxgb_classifier.fit(X_train ,y_train)\nxgb_pred = xgb_classifier.predict(X_test)\nxgb_score = accuracy_score(y_test,xgb_pred)\nprint('XGBoost Score: ',xgb_score)\nshow_cm(xgb_classifier, df_data, target)\n\n# Decision Tree\ndt_clf = tree.DecisionTreeClassifier()\ndt_clf.fit(X_train ,y_train)\ndt_pred = dt_clf.predict(X_test)\ndt_score = accuracy_score(y_test,dt_pred)\nprint('Decision Tree Score: ',dt_score)\nshow_cm(dt_clf, df_data, target)\n\n# AdaBoost\nfrom sklearn.ensemble import AdaBoostClassifier\nada_clf = AdaBoostClassifier(n_estimators=1500,learning_rate=1,algorithm='SAMME.R')\nada_clf.fit(X_train ,y_train)\nada_pred = ada_clf.predict(X_test)\nada_score = accuracy_score(y_test,ada_pred)\nprint('AdaBoost Decision Tree Score: ',ada_score)\nshow_cm(ada_clf, df_data, target)\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(max_depth=10, random_state=0)\nrf_clf.fit(X_train ,y_train)\nrf_pred = rf_clf.predict(X_test)\nrf_score = accuracy_score(y_test,rf_pred)\nprint('Random Forest Score: ',rf_score)\nshow_cm(rf_clf, df_data, target)\n\n# Linear Support Vector Classification\nlsvc = LinearSVC()\nlsvc.fit(X_train ,y_train)\nlsvc_pred = lsvc.predict(X_test)\nlsvc_score = accuracy_score(y_test,lsvc_pred)\nprint('LinearSVC Score: ', lsvc_score)\nshow_cm(lsvc, df_data, target)\n\n# Support Vector Classification\nsvc = SVC()\nsvc.fit(X_train ,y_train)\nsvc_pred = svc.predict(X_test)\nsvc_score = accuracy_score(y_test,svc_pred)\nprint('SVC Score: ', svc_score)\nshow_cm(svc, df_data, target)\n\n# Nu-Support Vector Classification\nnusvc = NuSVC()\nnusvc.fit(X_train ,y_train)\nnusvc_pred = nusvc.predict(X_test)\nnusvc_score = accuracy_score(y_test, nusvc_pred)\nprint('NuSVC Score: ', nusvc_score)\nshow_cm(nusvc, df_data, target)\n\n# Stochastic Gradient Descent\nsgd = SGDClassifier()\nsgd.fit(X_train ,y_train)\nsgd_pred = sgd.predict(X_test)\nsgd_score = accuracy_score(y_test,sgd_pred)\nprint('SGD Score: ', sgd_score)\nshow_cm(sgd, df_data, target)\n\nmodels = pd.DataFrame({\n    'Model': ['Linear Support Vector Classification', 'Support vector Classification', 'Nu-Support Vector Classification', \n              'Stochastic Gradient Decent', 'Logistic Regression','XGBoost','AdaBoost', 'Decision Tree', 'Random Forest'],\n    'Score': [ \n              lsvc_score, svc_score, nusvc_score, \n              sgd_score, lr_score, xgb_score, ada_score, dt_score, rf_score]})\nmodels = models.sort_values(by=\"Score\",ascending=False)\nprint(models.to_string())\n","00a9d73b":"\nfeatures = df.columns.values\nimportances = xgb_classifier.get_booster().get_score(importance_type=\"gain\")\nimportances = {k: v for k, v in sorted(importances.items(), key=lambda item: item[1])}\n\nplt.style.use('fivethirtyeight')\nplt.title('Feature Importances')\nplt.barh(list(importances.keys()), list(importances.values()), color='b', align='center')\nplt.xlabel('Relative Importance')\nplt.show()\n\n","009b5b5f":"from xgboost import plot_tree\n\nplot_tree(xgb_classifier, num_trees=0, rankdir='LR')\nfig = plt.gcf()\nfig.set_size_inches(150, 200)\nplt.show()","2300cb10":"Let's now visualize a portion of the decision trees.","5a7e61ac":"Splitting our dataset into training and testing","b100095a":"Let's dive deeper into our best classifier, XGBoost; specifically, how important each of the features are.","91ae29f9":"# Training Models","e10cfd0e":"# Data Parsing and Cleaning"}}