{"cell_type":{"07be72d4":"code","431800e6":"code","db837865":"code","a127971c":"code","3a758f70":"code","63a40697":"code","82b10cba":"code","14382305":"code","118c9d91":"code","b7307d06":"code","269262ae":"code","ee0738f3":"code","2ac4f7ac":"code","1d6dec8b":"markdown","8fd214d2":"markdown","7ebf4bd4":"markdown","97c52273":"markdown","8d0c0963":"markdown","d6034ded":"markdown","9cec495f":"markdown","a8bc04b9":"markdown","8040d40b":"markdown","5bfbeeac":"markdown","76fc5fc3":"markdown","ed40351e":"markdown","4f61175a":"markdown","d4759c74":"markdown","b07d995e":"markdown","0fef73cf":"markdown","7dbb87bf":"markdown","90e53851":"markdown","b352cbea":"markdown"},"source":{"07be72d4":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport pandas as pd","431800e6":"# Example of a squential model\n# Define Sequential model with 3 layers\nmodel = keras.Sequential(\n    [\n        layers.Dense(2, activation=\"relu\", name=\"layer1\"),\n        layers.Dense(3, activation=\"relu\", name=\"layer2\"),\n        layers.Dense(4, name=\"layer3\"),\n    ]\n)\n# Call model on a test input\nx = tf.ones((3, 3))\ny = model(x)","db837865":"data = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\", delimiter=',')\ndata = data.to_numpy()\nnp.random.shuffle(data)\nprint(data.shape)\n","a127971c":"x_train = data[:int(data.shape[0]*0.75),:-1]\ny_train = data[:int(data.shape[0]*0.75),-1]\n\nx_test = data[int(data.shape[0]*0.75):,:-1]\ny_test = data[int(data.shape[0]*0.75):,-1]\n\nprint(x_train.shape)\nprint(x_test.shape)","3a758f70":"inputs = keras.Input(shape=(13,), name=\"features\")\nx = layers.Dense(8, activation=\"relu\", name=\"dense_1\")(inputs)\nx = layers.Dense(4, activation=\"relu\", name=\"dense_2\")(x)\noutputs = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nmodel.summary()","63a40697":"model.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),  # Optimizer\n    # Loss function to minimize\n    loss=keras.losses.BinaryCrossentropy(),\n    # List of metrics to monitor\n    metrics=[keras.metrics.BinaryAccuracy()],\n)","82b10cba":"print(\"Fit model on training data\")\nhistory = model.fit(\n    x_train,\n    y_train,\n    batch_size=25,\n    epochs=500,\n    # We pass some validation for\n    # monitoring validation loss and metrics\n    # at the end of each epoch\n    validation_data=(x_test, y_test),\n)","14382305":"hist = pd.DataFrame(history.history)\nhist","118c9d91":"import matplotlib.pyplot as plt\n\nplt.plot(hist['loss'], label='Training set loss')\nplt.plot(hist['val_loss'], label='Validation set loss')\nplt.legend()\nplt.title(\"Loss\")\nplt.show()","b7307d06":"plt.plot(hist['binary_accuracy'],label='Training set accuracy')\nplt.plot(hist['val_binary_accuracy'],label='Validation set accuracy')\nplt.title(\"Accuarcy\")\nplt.legend()\nplt.show()","269262ae":"print(\"Evaluate on test data\")\nresults = model.evaluate(x_test, y_test, batch_size=128)\nprint(\"test loss, test acc:\", results)\n","ee0738f3":"# Generate predictions (probabilities -- the output of the last layer)\n# on new data using `predict`\nprint(\"Generate predictions for 3 samples\")\npredictions = model.predict(x_test[:3])\nprint(\"predictions shape:\", predictions.shape)","2ac4f7ac":"print(f'Predictions: {predictions}')\nprint(f'True Values: {y_test[:3]}')","1d6dec8b":"The returned history object holds a record of the loss values and metric values during training:","8fd214d2":"# Input Layer\n\nInput() is used to instantiate a Keras tensor.\n\nA Keras tensor is a symbolic tensor-like object, which we augment with certain attributes that allow us to build a Keras model just by knowing the inputs and outputs of the model.\n\n#### Long story short, it just instantiates the input to the network\n\nThe line of code could look something like this:\n`x = Input(shape=(32,))`\n","7ebf4bd4":"## Loading the data","97c52273":"## Defining a Keras Model\n\nSo there are multiple ways to define the model and end up with the exact same model. We will proceed in a simmilar fashion to the [Keras Documentation Tutorial](https:\/\/www.tensorflow.org\/guide\/keras\/train_and_evaluate)\n\nThe first thing: \n* Ensure the input layer has the right number of input features. \nThis can be specified when creating the first layer with the `shape` argument and setting it to 13 for the 13 input variables.\n\n### How do we know the number of layers and their types?\n\nThis is a very hard question. There are heuristics that we can use and often the best network structure is found through a process of trial and error experimentation. \n\nHere we will use a fully-connected network structure with three layers.\n\nWe will use the rectified linear unit activation function referred to as ReLU on the first two layers and the Sigmoid function in the output layer.","8d0c0963":"****A lot of these classes and methods allow for cutomizations**","d6034ded":"# Keras Layers\n\n### Keras supports many many many Layers\nCore layers:\n* **Input object**\n* **Dense layer**\n* **Activation layer**\n* Embedding layer\n* Masking layer\n* Lambda layer\n\nConvolution layers\n* Conv1D layer\n* Conv2D layer\n* Conv3D layer\n\nPooling layers:\n* MaxPooling1D layer\n* MaxPooling2D layer\n* MaxPooling3D layer\n* AveragePooling1D layer\n* AveragePooling2D layer\n* AveragePooling3D layer\n* GlobalMaxPooling1D layer\n* GlobalMaxPooling2D layer\n* GlobalMaxPooling3D layer\n* GlobalAveragePooling1D layer\n* GlobalAveragePooling2D layer\n* GlobalAveragePooling3D layer\n\nRecurrent layer:\n* LSTM layer\n* GRU layer\n* SimpleRNN layer\n* TimeDistributed layer\n* Bidirectional layer\n* ConvLSTM2D layer\n* Base RNN layer\n\nNormalization layers:\n* BatchNormalization layer\n* LayerNormalization layer\n* Regularization layers\n* Dropout layer\n* Attention layer\n* Reshape layer\n* Flatten layer\n* UpSampling1D layer\n* UpSampling2D layer\n* UpSampling3D layer\n* Average layer\n* Maximum layer\n* Minimum layer\n\nActivation layers:\n* **ReLU layer**\n* **Softmax layer**\n* **LeakyReLU layer**\n* **PReLU layer**\n* **ELU layer**\n* **ThresholdedReLU layer**\n\n## And these are not all of them!!!!!!!\n\n###We are going to focus on just a few **Core Layers** today:\n* **Input object**\n* **Dense layer**\n* **Activation layer**\n","9cec495f":"# Dense Layer\n\nJust your regular densely-connected\/fully-connect NN layer.\n\nEvery node in the layer previous has a connection to every node in the layer.\n\n> Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True). These are all attributes of Dense.\" [Keras Documentation](https:\/\/keras.io\/api\/layers\/core_layers\/dense\/)\n\n## Dense class\n            tf.keras.layers.Dense(\n            units,\n            activation=None,\n            use_bias=True,\n            kernel_initializer=\"glorot_uniform\",\n            bias_initializer=\"zeros\",\n            kernel_regularizer=None,\n            bias_regularizer=None,\n            activity_regularizer=None,\n            kernel_constraint=None,\n            bias_constraint=None,\n            **kwargs)\n\n### A few important parameters for Dense()\n* **units**: Positive integer, dimensionality of the output space.\n* **activation**: Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n* **use_bias**: Boolean, whether the layer uses a bias vector.\n\n\n","a8bc04b9":"## Compile Keras Model\n\nSo we have to complile the model. \n\nCompiling the model uses the efficient numerical libraries under the covers (the so-called backend). The backend automatically chooses the best way to represent the network for training and making predictions to run on your hardware, such as CPU or GPU or even distributed.\n\nWhen compiling, we must specify some additional properties required when training the network. \n\nWe must specify:\n* The loss function to use to evaluate a set of weights\n* The optimizer is used to search through different weights for the network\n* Any optional metrics we would like to collect and report during training\n\nIn this case, we will use:\n* Binary Cross Entropy for our loss funtion\n* The optimizer will be the efficient stochastic gradient descent algorithm \u201cadam\u201c. \n* We will collect and report the classification accuracy, defined via the metrics argument.\n\nEssentially we are specifying the training configurations (optimizer, loss, metrics).\n\n\n### Optimizers available:\n* SGD\n* RMSprop\n* Adam\n* Adadelta\n* Adagrad\n* Adamax\n* Nadam\n* Ftrl\n\nThey basically apply some flavour of gradient descent\n\nLoss functions and metrics need to be choosen carfully based on the problem. \n* Is it Classification problem or a Regression problem. \n* Is there a large class imbalance\n\n### Loss Funtions available:\nProbabilistic losses:\n* BinaryCrossentropy class\n* CategoricalCrossentropy class\n* SparseCategoricalCrossentropy class\n\nRegression losses:\n* MeanSquaredError class\n* MeanAbsoluteError class\n* MeanAbsolutePercentageError class\n* MeanSquaredLogarithmicError class\n* CosineSimilarity class\n\nAnd many more\n\n\n### Metrics available\nAccuracy metrics:\n* Accuracy class\n* BinaryAccuracy class\n* CategoricalAccuracy class\n* BinaryCrossentropy class\n\nRegression metrics:\n* MeanSquaredError class\n* RootMeanSquaredError class\n* MeanAbsoluteError class\n\nClassification metrics based on True\/False positives & negatives:\n* AUC class\n* Precision class\n* Recall class\n\nAnd many more\n\nAgain these can be customized\n\n\n","8040d40b":"# OKAY LET'S ACTUALLY GET STARTED\n## Problem Domain\nSo since we using the basic Keras NN compnnents we are only going to work with tabular data.\n\nThe dataset we are working with contains 13 attributes that should help us predict whether or not the individual has heart disease.\n\nSome of the features include:\n* age\n* sex\n* chest pain type (4 values)\n* resting blood pressure\n* serum cholestoral in mg\/dl\n* fasting blood sugar > 120 mg\/dl\n* etc.\n\n","5bfbeeac":"## Fit a Keras Model\n\nWe can train or fit our model on our loaded data by calling the `fit()` function on the model.\n\nTraining occurs over epochs and each epoch is split into batches.\n* Epoch: One pass through all of the rows in the training dataset.\n* Batch: One or more samples considered by the model within an epoch before weights are updated.\n\nOne epoch is comprised of one or more batches, based on the chosen batch size and the model is fit for many epochs. \n","76fc5fc3":"Lets use 75% of the data for train and the rest for test","ed40351e":"# Activation Layer\nApplies an activation function to an output.\n>activation: Activation function, such as tf.nn.relu, or string name of built-in activation function, such as \"relu\".\n\nActivation layer options:\n* relu\n* sigmoid\n* softmax\n* softplus\n* softsign\n* tanh function\n* selu function\n* elu function\n* exponential function\n\n","4f61175a":"# Intro to Neural Networks\nDeep Learning is the technology behind countless innovations in the past few years with applications in autonomous driving, biomedical informatics, image recognition and natural language processing. But what is deep learning, and how does it work? In this workshop, we will go over the foundational concepts of deep learning, particularly neural networks and how simple parts can build to form a complex system capable of modelling complex relationships. \n\n### In this workshop we will cover:\n1. Loading Data\n2. Defining a Keras Model\n3. Compiling a Keras Model\n4. Fitting a Keras Model\n5. Evaluating Keras Model\n6. And Making Predictions\n\n### Reasources used to develop this workshop include:\n* [Your First Deep Learning Project in Python with Keras Step-By-Step](https:\/\/machinelearningmastery.com\/tutorial-first-neural-network-python-keras\/) by [Jason Brownlee](https:\/\/machinelearningmastery.com\/author\/jasonb\/)\n* [Training and evaluation with the built-in methods](https:\/\/www.tensorflow.org\/guide\/keras\/train_and_evaluate) From the [Keras Documentation](https:\/\/www.tensorflow.org\/guide)","d4759c74":"We can plot the loss and accuracy over time for the training set and validation\/test set","b07d995e":"# Thats All Folks!\n\n\nNext week will be an interactive workshop where you will build your very own Neural Network! Be sure to be ready to follow along and participate!\n\n\nMake sure to check us out on Facebook, Instagram, and our website!","0fef73cf":"Making predictions is as easy as calling the `predict()` function on the model. We are using a sigmoid activation function on the output layer, so the predictions will be a probability in the range between 0 and 1.\n\nYou could round the output to produce values equal to 0 or 1. Essentially enforcing a 0.5 confidence threshold. Or you could programatically apply your own threshold.","7dbb87bf":"### First we need to import the relevant modules","90e53851":"## Evaluate Keras Model And Make Predictions\n\nWe evaluate the model on the test data via `evaluate()`\n\nThis will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics you have configured, such as accuracy.\n\nThe `evaluate()` function will return a list with **two** values. \n* The first will be the loss of the model on the dataset\n* The second will be the accuracy of the model on the dataset","b352cbea":"# Keras Model Types\n\n### Keras supports two model types\n* The `Sequential` model\n* The `Functional` model\n\nThe `Sequential` model is used for a model that is a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n\nA Sequential model is not appropriate when:\n\n* Any of your layers has multiple inputs or multiple outputs\n* You need to do layer sharing\n* You want non-linear topology (e.g. a residual connection, a multi-branch model)\n\nFor any of these reasons you would use a `Funtional` model"}}