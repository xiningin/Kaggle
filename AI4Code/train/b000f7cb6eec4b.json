{"cell_type":{"74a99b69":"code","9ffa2446":"code","b5c43af5":"code","ffd02c9e":"code","f29378bb":"code","ab28ee38":"code","8c883de6":"code","c2376dc4":"code","e35e1c96":"code","61b8a050":"code","7c6e2dfd":"code","5724dd58":"code","866eb1aa":"code","2eb3b203":"code","24f284dd":"code","fbe62227":"code","c434955b":"code","0b157375":"code","12c5f859":"code","8f41e5b5":"code","76e6f1ab":"code","af4d1293":"code","38176b5b":"code","1c36175e":"code","cf00d4ff":"code","6d555d22":"code","b00ae63c":"code","a969a5e6":"code","62771b28":"code","0419b4df":"code","86647798":"code","cbb8682a":"code","7dfb3505":"code","029de6d9":"code","471fb20d":"code","e83f004b":"code","1dcb3f1c":"code","a9991880":"code","a8e47c61":"code","33ed4fdf":"code","007af6e3":"code","a37a321f":"code","ed8897fb":"code","1df3988f":"code","30178f60":"markdown","375b2c06":"markdown","35277438":"markdown","88d3a389":"markdown","4c66bae6":"markdown","aa52a4d6":"markdown","b01770b1":"markdown","4c09d02a":"markdown","5b3b16f6":"markdown","b7375698":"markdown","61845a14":"markdown","7d21a6d4":"markdown","f7e26788":"markdown","91503f0c":"markdown","1b7f4e1e":"markdown","a9dbf1ef":"markdown","9b3c1cdc":"markdown","79140cd9":"markdown","7fbe4aa0":"markdown","fb066628":"markdown","b7e89008":"markdown","65c73e2d":"markdown","47e007e6":"markdown","76892ad9":"markdown","7cc4ecb1":"markdown","cefd3909":"markdown","24d93c07":"markdown","79502682":"markdown","f7f5bf62":"markdown","950ffaaa":"markdown","07c7b225":"markdown"},"source":{"74a99b69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ffa2446":"# import packages\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport missingno as msno\nimport scipy.stats as stats \nfrom scipy.special import boxcox1p # for normalization purpose\nimport warnings\n\n## Filter warnings\nwarnings.filterwarnings(\"ignore\")\n","b5c43af5":"## Load the data\ndf_train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","ffd02c9e":"## Take a quick peak on the train data\nprint(\"First 5 rows :\\n{}\".format(df_train.head()))\nprint(df_train.describe().T)","f29378bb":"## Take a quick peak on the test data\nprint(\"First 5 rows :\\n{}\".format(df_test.head()))\nprint(df_test.describe().T)","ab28ee38":"# Drop ID column as we are not using it\ndf_train = df_train.drop(\"Id\",axis=1)","8c883de6":"#Quantitative & Qualitative data\nquan_features = df_train.select_dtypes(include=np.number)\nqual_features = df_train.select_dtypes(include=np.object)\n\nprint(\"Quantitative data:\\n\", quan_features)\nprint(\"Quanlitative data:\\n\", qual_features)","c2376dc4":"# Temporal features\ntemporal_features = [feat for feat in df_train if \"Year\" in feat or \"Yr\" in feat]\n\nprint(temporal_features)\n\n# Visualization\nfor feature in temporal_features:\n    if feature != \"YrSold\":\n        sns.scatterplot(x=feature,y=\"SalePrice\",data=df_train)\n        plt.title(feature)\n        plt.show()","e35e1c96":"# Discrete data\ndiscrete_features = [feature for feature in quan_features if df_train[feature].nunique() < 25 and feature not in temporal_features]\n# Continuous data\ncontinuous_features = [feature for feature in quan_features if feature not in discrete_features and feature not in temporal_features]\n\nprint(\"Discrete_Features:\\n\",discrete_features)\nprint(\"Continuous_Features:\\n\",continuous_features)","61b8a050":"#Function for Scatterplot Visualization\ndef scatterplot(df,feature,target_feature):\n    \"\"\" Take in dataframe, feature and target feature to draw a \n    scatterplot of target_feature against the feature \"\"\"\n    \n    ## Setting the size of diagram\n    plt.figure(constrained_layout=True)\n    sns.scatterplot(df[feature],df[target_feature])\n    plt.title(feature)\n    plt.show()\n\n## Visualization for discrete_features\nfor feat in discrete_features:\n    scatterplot(df_train,feat,\"SalePrice\")\n","7c6e2dfd":"## Visualization for continuous_features\nfor feat in continuous_features:\n    scatterplot(df_train,feat,\"SalePrice\")","5724dd58":"## Check on the missing value of train dataset\nmsno.matrix(df_train,labels=True)","866eb1aa":"## Check on the number of missing value and its missing value percentage \ndef missing_value(df):\n    \"\"\" \n    Takes in a dataframe and returns\n    the number of missing value and percentage of\n    missing value.\n    \"\"\"\n    # Number of missing values\n    number = df.isnull().sum().sort_values(ascending=False)\n    number = number[number > 0]\n    \n    # Percentage of missing values\n    percentage = df.isnull().sum() *100 \/ df.shape[0]\n    percentage = percentage[percentage > 0].sort_values(ascending=False)\n    \n    return  pd.concat([number,percentage],keys=[\"Total\",\"Percentage\"],axis=1)\n    \n     \n# Missing data on train set\nmissing_value(df_train)","2eb3b203":"## Check on the missing value of test dataset\nmsno.matrix(df_test,labels=True)","24f284dd":"## Check on the number of missing value and its missing value percentage \nmissing_value(df_test)","fbe62227":"## Create a lower triangle heatmap\nmask = np.zeros_like(df_train.corr(),dtype=bool)\nmask[np.triu_indices_from(mask)] = True\n\n## Heatmap visualization\nplt.figure(figsize=(30,20))\nsns.heatmap(df_train.corr(),\n            annot=True,\n            fmt=\".3f\",\n            annot_kws = {\"size\":10},\n            cmap=sns.cubehelix_palette(),\n            mask=mask)\n\n\n","c434955b":"## Zoomed Heatmap to identify the top 10 most correlated features with SalePrice \ncorr_feat = df_train.corr().nlargest(10,\"SalePrice\")[\"SalePrice\"].index\ncmap = np.corrcoef(df_train[corr_feat].values.T)\nmask = np.zeros_like(cmap,dtype=bool)\nmask[np.triu_indices_from(mask)] = True\n\n##Visualization\nplt.figure(figsize=(20,10))\nsns.heatmap(cmap,\n            annot=True,\n            fmt=\".3f\",\n            annot_kws = {\"size\":10},\n            cmap=sns.cubehelix_palette(),\n            xticklabels = corr_feat.values,\n            yticklabels = corr_feat.values,\n            mask=mask)","0b157375":"## Extract the SalePrice out\ny = df_train[\"SalePrice\"]\n\n## Combining the train and test dataset\ndf_all = pd.concat([df_train,df_test],axis=0).reset_index(drop=True)\n\n## drop the SalePrice & Id columns\ndf_all = df_all.drop([\"SalePrice\",\"Id\"],axis=1)\n\n\n##Check the shape\ndf_all.shape","12c5f859":"# Check on the missing value of all the dataset\nmissing_value(df_all)","8f41e5b5":"## Missing value represent absence of that feature\nmissing_col = [\"Alley\", \"PoolQC\", \"MiscFeature\",\"Fence\",\n               \"FireplaceQu\",\"GarageType\",\"GarageFinish\",\n               \"GarageQual\",\"GarageCond\",'BsmtQual','BsmtCond',\n               'BsmtExposure','BsmtFinType1','BsmtFinType2',\n               'MasVnrType']\n\nfor col in missing_col:\n    df_all[col] = df_all[col].fillna(\"None\") #None represents the absence\n\n## Neighborhood base feature: \n #LotFrontage (Houses in the same neighborhood would have similar lotfrontage area)\ndf_all[\"LotFrontage\"] = df_all.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x:x.fillna(x.median()))\n #MasVnrArea (Same apply to the MasVnrArea)\ndf_all[\"MasVnrArea\"] = df_all.groupby(\"Neighborhood\")[\"MasVnrArea\"].transform(lambda x:x.fillna(x.median()))\n\n## MSSubClass\ndf_all[\"MSZoning\"] = df_all.groupby(\"MSSubClass\")[\"MSZoning\"].transform(lambda x: x.fillna(x.mode()[0]))\n\n## GarageYrBlt\ndf_all.loc[df_all[\"GarageFinish\"] == \"None\" , \"GarageYrBlt\"] = df_all[\"YearBuilt\"]\n\n## Check on the missing value\nmissing_value(df_all)","76e6f1ab":"### for the rest of the missing value\n ## 1.categorical feature are replaced with the mode value\n ## 2.numerical feature are replaced with the median value\n\nmissing_feat = missing_value(df_all).index\n\n## categorical feature\nmissing_cat = [feat for feat in missing_feat if df_all[feat].dtype == np.object]\n\nfor feat in missing_cat:\n    df_all[feat] = df_all[feat].transform(lambda x: x.fillna(x.mode()[0]))\n\n## numerical feature\nmissing_num = [feat for feat in missing_feat if feat not in missing_cat]\n\nfor feat in missing_num:\n    df_all[feat] = df_all[feat].transform(lambda x: x.fillna(x.median()))\n\n    \n### Check on the missing value\nmissing_value(df_all)\n","af4d1293":"### Months ans years should be consider as categorical features\ndf_all[\"MoSold\"] = df_all[\"MoSold\"].astype(str)\ndf_all[\"YrSold\"] = df_all[\"YrSold\"].astype(str)\ndf_all[\"YearBuilt\"] = df_all[\"YearBuilt\"].astype(str)","38176b5b":"## Visualization\nfig = plt.figure(constrained_layout=True, figsize=(12,8))\ngrid = gridspec.GridSpec(ncols=3, nrows=4, figure=fig)\n # Histrogram\nax1 = fig.add_subplot(grid[0,:])\nsns.distplot(y,ax=ax1)\nax1.set_title(\"Histrogram of SalePrice\")\n # QQplot\nax2 = fig.add_subplot(grid[2:,:2])\nstats.probplot(y,plot=ax2)\nax2.set_title(\"QQplot of SalePrice\")\n # Boxplot\nax3 = fig.add_subplot(grid[2:,2])\nsns.boxplot(y,ax=ax3,orient=\"v\")\nax3.set_title(\"Boxplot of SalePrice\")\n\nplt.show()","1c36175e":"##Check on the kurtosis & the skewness of SalePrice\nprint(\"Kurtosis: {}\".format(y.kurt()))\nprint(\"Skewness: {}\".format(y.skew()))","cf00d4ff":"## Normalize the Dependant Variable(SalePrice)\ny = np.log1p(y)\n\n## Visualize of SalePrice after the normalization\nfig,(ax1,ax2) = plt.subplots(2,1,constrained_layout=True,figsize=(12,9))\n\n # Histrogram\nsns.distplot(y,ax=ax1)\nax1.set_title(\"Histrogram of SalePrice\")\n # QQplot\nstats.probplot(y,plot=ax2)\nax2.set_title(\"QQplot of SalePrice\")\n\nplt.show()","6d555d22":"## Kurtosis and skewness of SalePrice\nprint(\"Kurtosis: {}\".format(y.kurt()))\nprint(\"Skewness: {}\".format(y.skew()))","b00ae63c":"## Check on the skewness and the kurtosis on continuos data only\nnumerical_feats = [feat for feat in df_all.columns if df_all[feat].dtype != np.object]\nskewness = df_all[numerical_feats].skew().sort_values(ascending=False)\nkurtosis = df_all[numerical_feats].kurt().sort_values(ascending=False)\n\ndf_norm = pd.concat([skewness,kurtosis],axis=1,keys=[\"Skewness\",\"Kurtosis\"])\n\ndf_norm","a969a5e6":"### Feature with skewness greater than 0.5 or lower than -0.5 are considered highly skewed\nhigh_skew = skewness[abs(skewness) > 0.5].sort_values(ascending=False)\n\n## Visualization of TotalBsmtSF\nplt.figure(figsize=(8,6))\nsns.distplot(df_all[\"TotalBsmtSF\"])\nplt.show()","62771b28":"## Look at its kurtosis and skewness value\nprint(\"Kurtosis: {}\".format(df_all[\"TotalBsmtSF\"].kurt()))\nprint(\"Skewness: {}\".format(df_all[\"TotalBsmtSF\"].skew()))","0419b4df":"## import packages\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n\n## Normalization of independant variables\nfor feat in high_skew.index:\n    df_all[feat] = boxcox1p(df_all[feat], boxcox_normmax(df_all[feat] + 1))\n## Visualization of TotalBsmtSF after normalization\nplt.figure(figsize=(8,6))\nsns.distplot(df_all[\"TotalBsmtSF\"])\nplt.show()\n","86647798":"## Look at its kurtosis and skewness value after the normalization\nprint(\"Kurtosis: {}\".format(df_all[\"TotalBsmtSF\"].kurt()))\nprint(\"Skewness: {}\".format(df_all[\"TotalBsmtSF\"].skew()))","cbb8682a":"## TotalHouseSF: The total Square Foot of the house\ndf_all[\"TotalHouseSF\"] = df_all[\"TotalBsmtSF\"] + df_all[\"1stFlrSF\"] + df_all[\"2ndFlrSF\"]\n\n## TotalBath: The total number of bathrooms in the house\ndf_all[\"TotalBath\"] = df_all[\"BsmtFullBath\"] + df_all[\"BsmtFullBath\"]*0.5 + df_all[\"FullBath\"] + df_all[\"HalfBath\"]*0.5\n\n## TotalPorchSF: The total square foot of porch area of the house\ndf_all[\"TotalPorchSF\"] = df_all[\"WoodDeckSF\"] + df_all[\"OpenPorchSF\"] + df_all[\"EnclosedPorch\"] + df_all[\"3SsnPorch\"] + df_all[\"ScreenPorch\"] \n\n## HouseRemodAge: Number of years the house being remodded to the time it was sold\ndf_all[\"HouseRemodAge\"] = df_all[\"YrSold\"].astype(int) - df_all[\"YearRemodAdd\"]\ndf_all.loc[df_all[\"HouseRemodAge\"] < 0, \"HouseRemodAge\"] = 0 \n\n\n## function \npresence = lambda x: 1 if x > 0 else 0\n\n## HasPool: Presence of pool\ndf_all[\"HasPool\"] = df_all[\"PoolArea\"].transform(presence)\n\n## Has2ndFlr: Presence of second floor\ndf_all[\"Has2ndFlr\"] = df_all[\"2ndFlrSF\"].transform(presence)\n\n## HasGarage: Presence of garage\ndf_all[\"HasGarage\"] = df_all[\"GarageArea\"].transform(presence)\n\n## HasBsmt: Presence of basement\ndf_all[\"HasBsmt\"] = df_all[\"TotalBsmtSF\"].transform(presence)\n\n## HasFirePlace: Presence of fireplace\ndf_all[\"HasFirePlace\"] = df_all[\"Fireplaces\"].transform(presence)","7dfb3505":"## Bias feature reducer\n\nbias_feat = []\nfor feat in df_all.columns:\n    counts = df_all[feat].value_counts().iloc[0] ## mode value counts\n    if counts \/ len(df_all) * 100 > 99.94:\n        bias_feat.append(feat)\n\nbias_feat","029de6d9":"## Remove the bias feature from the dataset\ndf_all = df_all.drop(bias_feat,axis=1)","471fb20d":"## Dummy Variable\ndf_all = pd.get_dummies(df_all).reset_index(drop=True)","e83f004b":"## Split the dataset back into train and test\nn = len(y)\ndf_train = df_all[:n]\ndf_test = df_all[n:]","1dcb3f1c":"## import necessary package\nfrom sklearn.model_selection import train_test_split\n\n## Split the data into train and test set\nX_train, X_test, y_train, y_test =  train_test_split(df_train,y,test_size=0.33,random_state=42)\n\n\n## Check on the dataset shape\nprint(\"Shapes: \", X_train.shape, X_test.shape, y_train.shape, y_test.shape)","a9991880":"## import packages for the models\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet, HuberRegressor, BayesianRidge\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor, VotingRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n## import necessary packages for constructing the models\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n## Create an empty list\npipeline_models = []\n\n# Assign all models into the list\nseed = 42\nmodels = [Ridge(tol=10,random_state=seed),\n          Lasso(tol=1,random_state=seed),\n          ElasticNet(random_state=seed),\n          HuberRegressor(),\n          BayesianRidge(),\n          RandomForestRegressor(random_state=seed),\n          ExtraTreesRegressor(random_state=seed),\n          BaggingRegressor(random_state=seed),\n          GradientBoostingRegressor(),\n          XGBRegressor(),\n          DecisionTreeRegressor(),\n          KNeighborsRegressor(),\n          AdaBoostRegressor(random_state=seed)]\n\nmodel_names = [\"Ridge\",\"Lasso\",\"Elastic\",\"Hub_Reg\",\"BayRidge\",\"RFR\",\"ETR\",\"BR\",\"GBoost_Reg\",\"XGB_Reg\",\"DT_Reg\",\"KNN_Reg\",\"Ada_Reg\"] # All models' labels\n\n## Assign each model to a pipeline\nfor name, model in zip(model_names,models):\n    pipeline = (\"Scaled_\"+ name,\n                Pipeline([(\"Scaler\",StandardScaler()),\n                          (name,model)\n                         ]))\n    pipeline_models.append(pipeline)","a8e47c61":"## import package\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n## Create a dataframe to store all the models' cross validation score\nevaluate = pd.DataFrame(columns=[\"model\",\"cv\",\"std\"])\n\n\n## Encoded dataset\nfor name,model in pipeline_models:\n    kfold = KFold(n_splits=7,shuffle=True,random_state=42)\n    cv = cross_val_score(model, X_train, y_train, cv=kfold, n_jobs=-1, scoring=\"r2\")\n    \n    row = evaluate.shape[0]\n    evaluate.loc[row,\"model\"] = name\n    evaluate.loc[row,\"cv\"] = round(cv.mean(),3)\n    evaluate.loc[row,\"std\"] = \"+\/- {}\".format(round(cv.std(),4))\n    \n    evaluate = evaluate.sort_values(\"cv\",ascending=False)","33ed4fdf":"## Visualization\nfig, ax = plt.subplots(1,1,sharey=False,figsize=(16,9))\n\n## Encoded dataset\nbar = sns.barplot(evaluate[\"model\"], evaluate[\"cv\"],ax=ax,palette = sns.cubehelix_palette(evaluate.shape[0]))\nfor rec in bar.patches:\n    height = rec.get_height()\n    ax.text(rec.get_x() + rec.get_width()\/2, height*1.02,height,ha=\"center\")\nax.set_title(\"Cross Validate Score\")\nax.set_xticklabels(evaluate[\"model\"].to_list(),rotation = 45)","007af6e3":"## Creating a lsit for all combinations models\nvotings = []\n\n## GradientBoostingRegressor only, Current best model\nvotings.append((\"Scaled_GBRegressor\",Pipeline([(\"Scaler\",StandardScaler()),(\"GBRegressor\",GradientBoostingRegressor())])))\n\n\n##  All models\nvotings.append((\"Scaled_all_models\",Pipeline([(\"Scaler\",StandardScaler()),\n                                      (\"Votings\",VotingRegressor([(\"GBRegressor\",GradientBoostingRegressor()),\n                                                                  (\"XGBR\",XGBRegressor()),\n                                                                  (\"RFR\",RandomForestRegressor(random_state=seed)),\n                                                                 ])\n                                    \n                                    )])))\n\n\n## Combination of GBRegressor with XGBR\nvotings.append((\"Scaled_GBRegressor_XGBR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                      (\"Votings\",VotingRegressor([(\"GBRegressor\",GradientBoostingRegressor()),\n                                                                  (\"XGBR\",XGBRegressor()),\n                                                                 ])\n                                    \n                                    )])))\n\n\n## Combination of GBRegressor with RFR\nvotings.append((\"Scaled_GBRegressor_RFR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                      (\"Votings\",VotingRegressor([(\"GBRegressor\",GradientBoostingRegressor()),\n                                                                  (\"RFR\",RandomForestRegressor(random_state=seed)),\n                                                                 ])\n                                    \n                                    )])))\n\n\n## Combination of XGBR with RFR\nvotings.append((\"Scaled_XGBR_RFR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                      (\"Votings\",VotingRegressor([(\"XGBR\",XGBRegressor()),\n                                                                  (\"RFR\",RandomForestRegressor(random_state=seed)),\n                                                                 ])\n                                    \n                                    )])))\n","a37a321f":"## Create dataframe for the cross validate score\nevaluate_vote = pd.DataFrame(columns=[\"model\",\"cv\",\"std\"])\n\n## Fitting all the combination model\nfor name, model in votings:\n    kfold = KFold(n_splits=7,shuffle=True,random_state=42)\n    \n    cv = cross_val_score(model,X_train,y_train, cv=kfold, scoring=\"r2\",n_jobs=-1)\n    \n    row = evaluate_vote.shape[0]\n    \n    evaluate_vote.loc[row,\"model\"] = name\n    evaluate_vote.loc[row,\"cv\"] = round(cv.mean(),4)\n    evaluate_vote.loc[row,\"std\"] = \"+- {}\".format(round(cv.std(),5))\n    \nevaluate_vote = evaluate_vote.sort_values(\"cv\",ascending=False)","ed8897fb":"## Visualization\nfig, ax = plt.subplots(1,1,figsize=(16,9))\nbar = sns.barplot(evaluate_vote[\"model\"],evaluate_vote[\"cv\"],ax=ax,palette = sns.cubehelix_palette(evaluate.shape[0]))\n\nfor rec in bar.patches:\n    height = rec.get_height()\n    ax.text(rec.get_x() + rec.get_width() \/2, height *1.02, height, ha=\"center\")\nax.set_title(\"Cross Validate Score\",fontsize=14)\nax.set_xticklabels(evaluate_vote[\"model\"].to_list(),rotation=45)","1df3988f":"## Best Model : Scaled_GBRegressor_XGBR\nbest_model = Pipeline([(\"Scaler\",StandardScaler()),\n                                      (\"Votings\",VotingRegressor([(\"GBRegressor\",GradientBoostingRegressor()),\n                                                                  (\"XGBR\",XGBRegressor()),\n                                                                 ])\n                                    \n                                    )])\n## Fit the model \nbest_model = best_model.fit(X_train,y_train)\n\n## Submission\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(best_model.predict(df_test)))\nsubmission.to_csv('submission', index=False)","30178f60":"### Imputing Missing Values","375b2c06":"### Multicollinearity \nHeatmap is an excellent way to check on the correclation between each independant variable.\n","35277438":"We can see that we have a total of 81 features, where 37 features are quantitative\/numerical and 43 features are qualitative\/categorical. Multiple features contain missing values. This dataset contains 1461 houses data. \n\nBelow are the data description provided:\n1. ID : Assigned ID of the house\n1. MSSubClass : Identifies the type of dwelling involved in the sale\n1. MSZoning: Identifies the general zoning classification of the sale\n1. LotFrontage: Linear feet of street connected to property\n1. LotArea: Lot size in square feet\n1. Street: Type of road access to property\n1. Alley: Type of alley access to property\n1. LotShape: General shape of property\n1. LandContour: Flatness of the property\n1. Utilities: Type of utilities available\n1. LotConfig: Lot configuration\n1. LandSlope: Slope of property\n1. Neighborhood: Physical locations within Ames city limits\n1. Condition1: Proximity to various conditions\n1. Condition2: Proximity to various conditions (if more than one is present)\n1. BldgType: Type of dwelling\n1. HouseStyle: Style of dwelling\n1. OverallQual: Rates the overall material and finish of the house\n1. OverallCond: Rates the overall condition of the house\n1. YearBuilt: Original construction date\n1. YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)\n1. RoofStyle: Type of roof\n1. RoofMatl: Roof material\n1. Exterior1st: Exterior covering on house\n1. Exterior2nd: Exterior covering on house (if more than one material)\n1. MasVnrType: Masonry veneer type\n1. MasVnrArea: Masonry veneer area in square feet\n1. ExterQual: Evaluates the quality of the material on the exterior\n1. ExterCond: Evaluates the present condition of the material on the exterior\n1. Foundation: Type of foundation\n1. BsmtQual: Evaluates the height of the basement\n1. BsmtCond: Evaluates the general condition of the basement\n1. BsmtExposure: Refers to walkout or garden level walls\n1. BsmtFinType1: Rating of basement finished area\n1. BsmtFinSF1: Type 1 finished square feet\n1. BsmtFinType2: Rating of basement finished area (if multiple types)\n1. BsmtFinSF2: Type 2 finished square feet\n1. BsmtUnfSF: Unfinished square feet of basement area\n1. TotalBsmtSF: Total square feet of basement area\n1. Heating: Type of heating\n1. HeatingQC: Heating quality and condition\n1. CentralAir: Central air conditioning\n1. Electrical: Electrical system\n1. 1stFlrSF: First Floor square feet\n1. 2ndFlrSF: Second floor square feet\n1. LowQualFinSF: Low quality finished square feet (all floors)\n1. GrLivArea: Above grade (ground) living area square feet\n1. BsmtFullBath: Basement full bathrooms\n1. BsmtHalfBath: Basement half bathrooms\n1. FullBath: Full bathrooms above grade\n1. HalfBath: Half baths above grade\n1. Bedroom: Bedrooms above grade (does NOT include basement bedrooms)\n1. Kitchen: Kitchens above grade\n1. KitchenQual: Kitchen quality\n1. TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n1. Functional: Home functionality (Assume typical unless deductions are warranted)\n1. Fireplaces: Number of fireplaces\n1. FireplaceQu: Fireplace quality\n1. GarageType: Garage location\n1. GarageYrBlt: Year garage was built\n1. GarageFinish: Interior finish of the garage\n1. GarageCars: Size of garage in car capacity\n1. GarageArea: Size of garage in square feet\n1. GarageQual: Garage quality\n1. GarageCond: Garage condition\n1. PavedDrive: Paved driveway\n1. WoodDeckSF: Wood deck area in square feet\n1. OpenPorchSF: Open porch area in square feet\n1. EnclosedPorch: Enclosed porch area in square feet\n1. 3SsnPorch: Three season porch area in square feet\n1. ScreenPorch: Screen porch area in square feet\n1. PoolArea: Pool area in square feet\n1. PoolQC: Pool quality\n1. Fence: Fence quality\n1. MiscFeature: Miscellaneous feature not covered in other categories\n1. MiscVal: $Value of miscellaneous feature\n1. MoSold: Month Sold (MM)\n1. YrSold: Year Sold (YYYY)\n1. SaleType: Type of sale\n1. SaleCondition: Condition of sale\n1. ****SalePrice : The price the house was sold for (Our target variable)****","88d3a389":"### Constructing Models\n","4c66bae6":"### Missing Values\n","aa52a4d6":"## Model Building\n### Train Test Split\nThe dataset are split into X_train,X_test, y_train, y_test.","b01770b1":"1. ****Skewness****: Defined as the degree of distortion from the symmetrical bell curve or the normal curve.\n1. ****Kurtosis****: Defined as the measuer of the extreme values (also known as outliers) present in the distribution.\n\n\n As indicated in the three charts above, ****SalePrice**** is postively-skewed. ****SalePrice**** is drawn from a ****Leptokurtic**** (distributions with wider tails, greater profusion of outliers) distributions.","4c09d02a":"Great**!** that's much better.  \nNow let's check on the kurtosis and skewness value of ****SalePrice****","5b3b16f6":"### Submission","b7375698":"## Data Processing\nCombine both train and test dataset","61845a14":"### Missing Value\n","7d21a6d4":"Newer houses are sold at a higher prices.","f7e26788":"Temporal data are data that represents a state in time such as 16 June 2019","91503f0c":"### It's been my pleasure to share my notebook with you. \n### If this notebook do helped you in any way, please hit that \"upvote\" button. Thank you !","1b7f4e1e":"### Creating Dummy Variables\n","a9dbf1ef":"# Outline of this notebook:\n## Exploratory Data Analysis (EDA): This where we explore the dataset through visualization and analyzation to summarize thier main characteristics :\n\n* [Quick Peak](#Quick-Peak)\n* [Quantitative Data & Qualitative Data](#Quantitative-Data-&-Qualitative-Data)\n* [Missing Values](#Missing-Values)\n* [Multicollinearity](#Multicollinearity)\n\n## Data Processing: This is where we manipulate the explored and analyzed data to covert into meaningful information that can be used my our estimators:\n\n* [Imputing Missing Values](#Imputing-Missing-Values)\n* [Normalization of Dependant Variable (SalePrice)](#Normalization-of-Dependant-Variable-(SalePrice))\n* [Normalization of Independant Variables](#Normalization-of-Independant-Variables)\n* [Adding New Features](#Adding-New-Features)\n* [Eliminating Biased Features](#Eliminating-Biased-Features)\n* [Creating Dummy Variables](#Creating-Dummy-Variables)\n\n## Models Building: We would first train all of the models to search for high cv score models. These models are then ensemble together using VotingRegressor to further increase the cross validate score.\n* [Train Test Split](#Train-Test-Split)\n* [Constructing Models](#Constructing-Models)\n* [Models Training](#Models-Training)\n* [Ensemble Models with VotingRegressor](#Ensemble-Models-with-VotingRegressor)\n* [Submission](#Submission)\n\nThese models are:\n1. Ridge\n1. Lasso\n1. Elastic Net \n1. RandomForestRegressor\n1. ExtraTreeRegressor\n1. BaggingRegressor\n1. HuberRegressor\n1. BayesianRidge\n1. XGBRegressor\n1. DecisionTreeRegressor\n1. KNeighborsRegressor\n1. GradientBoostingRegressor","9b3c1cdc":"### Normalization of Independant Variables","79140cd9":"### Normalization of Dependant Variable (SalePrice)","7fbe4aa0":"# Exploratory Data Analysis\n### Quick Peak","fb066628":"Two types of Quantitative data:\n\n1. Discrete data: Numerical data that has specific values. A great example would be the number of cats. The number of cats are ****counted****. 1 cat, 2 cats, 3 cats. There is no such thing as 0.5 cat. \n\n2. Continuous data: Numerical data that can take on any values. A great example would be the weight of a person. Ryan is 83kg or 182.984 lbs, his weight is ****measured****.","b7e89008":"From the heatmap, there are multiple multicollinearity:\n1. ****YearBuilt**** and ****GarageYrBlt****   : ****0.826****\n1. ****TotalBsmtSF**** and ****1stFlrSF****    : ****0.820****\n1. ****TotRmsAbvGrd**** and ****GrLivArea****  : ****0.825****\n1. ****GarageArea**** and ****GarageCars****   : ****0.882****\n\nWe deal with this later in the feature engineering.\n","65c73e2d":"As we can see from the graphs, **OverallQual**, **OverallCond**, **FullBath**, **TotRmsAbvGrd** and **GarageCars** have stong correlation with **SalePrice**","47e007e6":"Great! We normalized all the data.","76892ad9":"### Ensemble Models with VotingRegressor\nA voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction\n\nNote: VotingRegressor works well with models that are not identical, for example: XGBRegressor with ExtraTreesRegressor. ExtraTreesRegressor and RandomTreesRegressor are highly identifical so we would just pick the one with a higher cv score (in this case RandomForestRegressor)","7cc4ecb1":"The three grahps above show us:\n1. The ****SalePrice**** is drawn from a normal distribution\n2. The ****SalePrice**** is right skewed\/ postively skewed, which indicates that most people are able to afford lower priced house.\n3. Present some mutliple outliers in ****SalePrice****","cefd3909":"### Quantitative Data & Qualitative Data\nTwo types of data:\n\n1. Quantitative data: Data that can be measured the quantity of something. A great example would be someone's height, Ryan is 183cm \/ 6 foot tall. The height can be measured precisely using a stadiometers.\n\n1. Qualitative data: Data that describes the information of something that cannot be measured. A great example would the color of an apple - green and red. ","24d93c07":"### Eliminating Biased Features\n","79502682":"As we can see from the graphs, **TotalBsmtSF**, **1stFlrSF**, **GrLivArea** and **GarageArea** have stong correlation with **SalePrice**","f7f5bf62":"From the chart above, there are 4 models have cross validate score greater than 0.85. All these models would be use in the ensemble VotingRegressor to further increase the cv score.\n\nThese models are:\n* GradientBoostingRegressor\n* XGBRegressor\n* RandomForestRegressor\n* ExtraTreesRegressor","950ffaaa":"### Adding New Features","07c7b225":"### Models Training"}}