{"cell_type":{"8279de4b":"code","a02b157b":"code","e1c23a1a":"code","65867efa":"code","62cde41f":"code","4f21d211":"code","7f0ea0ed":"code","5e797c73":"code","c45f8afa":"code","ab3968f1":"markdown","ac50a1ff":"markdown","fe21718f":"markdown","1fc8d20f":"markdown"},"source":{"8279de4b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics import mean_squared_error\nfrom transformers import AdamWeightDecay\nfrom transformers import AutoTokenizer, TFAutoModel,TFAutoModelForSequenceClassification, TFRobertaModel","a02b157b":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\nprint('Number of replicas:', strategy.num_replicas_in_sync)","e1c23a1a":"train = pd.read_csv('..\/input\/clrp-train-folded\/train_folds.csv') # this dataset was created after looking at the work of Abhishek Thakur's Create folds notebook.\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","65867efa":"def split(num):\n    train_df = train.loc[train.fold!=num]\n    X1 = list(train_df.excerpt.values)\n    Y1 = list(train_df.target.values)\n    val_df = train.loc[train.fold==num]\n    X2 = list(val_df.excerpt.values)\n    Y2 = list(val_df.target.values)\n    return X1,X2,Y1,Y2","62cde41f":"def encode(train_enc,target):\n    return {'input_ids':train_enc['input_ids'],'attention_mask':train_enc['attention_mask']},tf.cast(target, tf.float32)\n\ndef encode_bert(train_enc,target):\n    return {'input_ids':train_enc['input_ids'],'attention_mask':train_enc['attention_mask'],'token_type_ids':train_enc['token_type_ids']},tf.cast(target, tf.float32)\n \ndef tensors(X1,Y1,X2,Y2,batch_size,enc):\n    train = tf.data.Dataset.from_tensor_slices((dict(X1),list(Y1)))\n    val = tf.data.Dataset.from_tensor_slices((dict(X2),list(Y2)))\n    train_tensor = (train.repeat().shuffle(1024).map(enc,num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE))\n    valid_tensor = (val.map(enc,num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE))\n    return train_tensor,valid_tensor","4f21d211":"def build_model(path,optimizer,token_ids):\n    \n    with strategy.scope():\n        \n        input_ids = tf.keras.Input(shape=(256,),name='input_ids',dtype=tf.int32)\n        attention_mask = tf.keras.Input(shape=(256,),name='attention_mask',dtype=tf.int32)\n        \n        if token_ids==True: # I have used token_type_ids for bert model only\n            \n            token_type_ids = tf.keras.Input(shape=(256,),name='token_type_ids',dtype=tf.int32)\n            model = TFAutoModel.from_pretrained(path)\n            model = model([input_ids,attention_mask,token_type_ids])[0]\n            out = model[:,0,:]\n            #out=tf.keras.layers.Dropout(0.1)(out)\n            out = tf.keras.layers.Dense(1,activation='linear')(out)\n            Model = tf.keras.Model(inputs = [input_ids,attention_mask,token_type_ids],outputs = out)\n            Model.compile(optimizer = optimizer,loss = tf.keras.losses.MeanSquaredError(),metrics = [tf.keras.metrics.RootMeanSquaredError()])\n            return Model\n        \n        else:\n            \n            model = TFAutoModel.from_pretrained(path)\n            model = model([input_ids,attention_mask])[0]\n            out = model[:,0,:]\n            #out=tf.keras.layers.Dropout(0.1)(out)\n            out = tf.keras.layers.Dense(1,activation='linear')(out)\n            Model = tf.keras.Model(inputs=[input_ids,attention_mask],outputs = out)\n            Model.compile(optimizer = optimizer,loss = tf.keras.losses.MeanSquaredError(),metrics = [tf.keras.metrics.RootMeanSquaredError()])\n            return Model","7f0ea0ed":"def prediction(model_path,weights_path,batch_size,optimizer,token_ids,enc,tok):\n    \n    train_preds = np.zeros((train.shape[0],1))\n    test_pred = np.zeros((test.shape[0],1))\n    test_excerpt = tok([text.strip() for text in test.excerpt.values],padding='max_length',max_length=256,truncation=True,return_attention_mask=True)\n    test_enc = tf.data.Dataset.from_tensor_slices((dict(test_excerpt),list(np.zeros((test.shape[0],1)))))\n    test_input = (test_enc.map(enc,num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE))\n\n    for fold in range(5):\n        \n        tf.keras.backend.clear_session()\n        model = build_model(model_path,optimizer,token_ids)\n        X1,X2,Y1,Y2 = split(fold)\n        train_enc = tok([text.strip() for text in X1],padding='max_length',max_length=256,truncation=True,return_attention_mask=True)\n        val_enc = tok([text.strip() for text in X2],padding='max_length',max_length=256,truncation=True,return_attention_mask=True)\n        train1,valid1 = tensors(train_enc,Y1,val_enc,Y2,batch_size,enc)\n    \n        model.load_weights(weights_path+str(fold)+'.h5')\n        train_preds[train.index[train.fold==fold]] = model.predict(valid1)\n        loss,rmse = model.evaluate(valid1)\n        test_pred += model.predict(test_input)\/5\n        print('fold ',fold,' ',rmse)\n        \n    print('oof_rmse ', np.sqrt(mean_squared_error(train_preds,train.target)))\n    \n    return test_pred","5e797c73":"roberta_preds = prediction('..\/input\/robertabasemodel', '..\/input\/robertabasemodels\/modelrobertabase_', 8,\n                        AdamWeightDecay(learning_rate=3e-5,weight_decay_rate=0.1,epsilon=1e-6,beta_1=0.9,beta_2=0.98),\n                        False,encode,AutoTokenizer.from_pretrained('..\/input\/robertabasetokenizer'))[:,0]\n\nbert_preds = prediction('..\/input\/huggingface-bert-variants\/bert-base-cased\/bert-base-cased', '..\/input\/bertbase\/modelbertbase_',8,\n                     AdamWeightDecay(learning_rate=3e-5,weight_decay_rate=0.1,epsilon=1e-6,beta_1=0.9,beta_2=0.999),\n                     True,encode_bert,AutoTokenizer.from_pretrained('..\/input\/huggingface-bert-variants\/bert-base-cased\/bert-base-cased'))[:,0]\n\ndistilbert_preds = prediction('..\/input\/huggingface-bert-variants\/distilbert-base-uncased\/distilbert-base-uncased', '..\/input\/distlbert-models\/modeldistilbertbase_',24,\n                           tf.keras.optimizers.Adam(3e-5),False,encode,AutoTokenizer.from_pretrained('..\/input\/huggingface-bert-variants\/distilbert-base-uncased\/distilbert-base-uncased'))[:,0]\n\nxlnet_preds = prediction('..\/input\/xlnet-base-tensorflow-20', '..\/input\/xlnet-base\/modelxlnetbase_', 12,\n                      tf.keras.optimizers.Adam(3e-5),False,encode,AutoTokenizer.from_pretrained('..\/input\/xlnet-base-tensorflow-20'))[:,0]","c45f8afa":"final_preds = ( roberta_preds + bert_preds + distilbert_preds + xlnet_preds ) \/ 4\n\npd.DataFrame({\n    'id':test.id,\n    'target':final_preds\n}).to_csv('submission.csv',index=False)","ab3968f1":"Do upvote if you like it and any suggestions or doubts are welcome in the comment section.\n* Happy Kaggling!!","ac50a1ff":"# Models Inference and Simple ensemble","fe21718f":"Here are some of the great kernels that helped me during this competition\n\n* [ragnar](https:\/\/www.kaggle.com\/ragnar123)'s kernel 1 [CommonLit Readability Roberta TF](https:\/\/www.kaggle.com\/ragnar123\/commonlit-readability-roberta-tf)\n\n* [ragnar](https:\/\/www.kaggle.com\/ragnar123)'s kernel 2 [CommonLit Readability Roberta TF Inference](https:\/\/www.kaggle.com\/ragnar123\/commonlit-readability-roberta-tf-inference)\n\n* [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek)'s [Step 1: Create Folds](https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds)\n\n* [Ayush Thakur](https:\/\/www.kaggle.com\/ayuraj)'s [Transformer Baseline with TF\/Keras and W&B](https:\/\/www.kaggle.com\/ayuraj\/how-to-use-tf-data-to-train-hf-transformer)\n\nThere are lots of amazing notebooks out there and I might be missing some of those too. Do check them out!!","1fc8d20f":"This notebook is all about inference and ensemble of all the models that I have tried and each model has 5 sub models i.e for each fold. None of my single models reached LB score of less than 0.5 except roberta base which managed to reach 0.488. But ensembling helped me get to LB score of 0.482 which is a great boost. Later by adding more and more different models I have realised more the models better the score you get."}}