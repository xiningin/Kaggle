{"cell_type":{"ca017590":"code","3e45e009":"code","3ffc9f5f":"code","5cfafcc4":"code","38c1d0bc":"code","15249b1a":"code","c3d19ae5":"code","d3f02b41":"code","06719403":"code","6757b2a4":"code","cdb5751b":"code","b6d2f021":"markdown","e1acae09":"markdown","5aa4b0f2":"markdown","bccd648d":"markdown","ee0f214a":"markdown","535e51ae":"markdown"},"source":{"ca017590":"import time\nimport math\nimport pandas as pd\nimport numpy as np\nimport torch as T\nimport random\nimport matplotlib.pyplot as plt\nimport gym\nfrom gym import error, spaces, utils\nfrom gym.utils import seeding\nimport os\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom pandas.api.types import is_numeric_dtype\nfrom collections import namedtuple, deque","3e45e009":"class config():\n    def __init__(self):\n        self.device = T.device(\"cuda\") if T.cuda.is_available() else T.device(\"cpu\")\n        self.bs = 256\n        self.seq_len = 1\n        self.epochs = 10\n        self.cash = 25_000\n        self.lr = 1e-3\n        self.gamma = 0.99\n        self.epsilon = 1.\n        self.eps_dec = .9999\n        self.eps_min = 1e-2\n        self.mem_size = 1000\n        self.replace_cnt = 1000\n        \nconfig = config()","3ffc9f5f":"class DeepQNetwork(nn.Module):\n    def __init__(self, model, n_actions, n_cont, emb_dims, hidden_size, nlayers, name, chkpt_dir):\n        # https:\/\/yashuseth.blog\/2018\/07\/22\/pytorch-neural-network-for-tabular-data-with-categorical-embeddings\/\n        super(DeepQNetwork, self).__init__()\n        self.model = model\n        self.emb_layers = nn.ModuleList([nn.Embedding(i,j) for i,j in emb_dims])\n        \n        self.n_embs = sum([j for _,j in emb_dims])\n        self.n_cont = n_cont\n        n_feats = self.n_cont + self.n_embs\n        self.hidden_size = hidden_size\n        self.nlayers = nlayers\n        \n        self.checkpoint_dir = chkpt_dir\n        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)\n        \n        if model=='lstm':\n            self.lstm = nn.LSTM(n_feats, hidden_size, self.nlayers, batch_first=True)\n            self.hn = T.zeros((self.nlayers,config.seq_len,self.hidden_size), device=config.device)\n            self.cn = T.zeros((self.nlayers,config.seq_len,self.hidden_size), device=config.device)\n        elif model=='gru':\n            self.gru = nn.GRU(n_feats, hidden_size, self.nlayers, batch_first=True)\n            self.hn = T.zeros((self.nlayers,config.seq_len,self.hidden_size), device=config.device)\n            \n        self.fc_1 = nn.Linear(hidden_size, 512)\n        self.fc_2 = nn.Linear(512, 256)\n        self.fc_3 = nn.Linear(256, n_actions)\n        self.relu = nn.ReLU()\n        \n        self.optimizer = optim.Adam(self.parameters(), lr=config.lr)\n        \n        self.to(config.device)\n            \n    def forward(self, state):\n        bs = state.size(0)\n        cat_data = state[:,:15].long()\n        cont_data = state[:,15:]\n\n        emb = [emb_layer(cat_data[:,i]) for i,emb_layer in enumerate(self.emb_layers)]\n        emb_ = T.cat(emb,1)\n        \n        input_ = T.cat([emb_, cont_data],1).float()\n        input_ = input_.unsqueeze(0)\n        if self.model=='lstm':\n            output, (self.hn, self.cn) = self.lstm(input_, (self.hn.detach(),self.cn.detach())) # self.hn, self.cn\n        elif self.model=='gru':\n            output, self.hn = self.gru(input_, self.hn.detach()) # self.hn\n        output = output.contiguous().view(bs,-1)\n        out = self.relu(output)\n        out = self.fc_1(out)\n        out = self.relu(out)\n        out = self.fc_2(out)\n        out = self.relu(out)\n        out = self.fc_3(out)\n        return T.softmax(out,1)\n    \n    def save_checkpoint(self):\n        print('... saving checkpoint ...')\n        T.save(self.state_dict(), self.checkpoint_file)\n\n    def load_checkpoint(self):\n        print('... loading checkpoint ...')\n        self.load_state_dict(T.load(self.checkpoint_file))","5cfafcc4":"Transitions = namedtuple('transitions',\n                       ('states','actions','rewards','states_','dones'))\n\nclass ReplayBuffer():\n    def __init__(self):\n        self.memory = deque(maxlen=config.mem_size)\n\n    def store_transition(self, states, actions, rewards, states_, dones):\n        '''\n        Using states_.size instead of states.size because a the shape mismatch when states_ is on the last step and thus isn't explicitily equal to the batch size\n        This also just ignores the last bit of states since there's no other info to append with it.\n        '''\n        for idx in range(states_.size(0)):\n            try:\n                self.memory.append(Transitions(states[idx], actions[idx], rewards[idx], states_[idx], dones[idx]))\n            except Exception:\n                pass\n\n    def sample_buffer(self):\n        global s\n        s = random.sample(self.memory,config.bs)\n\n        states = s[0].states.unsqueeze(0)\n        actions = s[0].actions.unsqueeze(0)\n        rewards = s[0].rewards.unsqueeze(0)\n        states_ = s[0].states_.unsqueeze(0)\n        dones = s[0].dones.unsqueeze(0)\n        \n        for x in range(1,len(s)):\n            states = T.cat((states,s[x].states.unsqueeze(0)),dim=0)\n            actions = T.cat((actions,s[x].actions.unsqueeze(0)),dim=0)\n            rewards = T.cat((rewards,s[x].rewards.unsqueeze(0)),dim=0)\n            states_ = T.cat((states_,s[x].states_.unsqueeze(0)),dim=0)\n            dones = T.cat((dones,s[x].dones.unsqueeze(0)),dim=0)\n            \n        return states, actions, rewards, states_, dones\n\n    def __len__(self):\n        return len(self.memory)","38c1d0bc":"# https:\/\/www.mlq.ai\/deep-reinforcement-learning-pytorch-implementation\/\nclass DDQNAgent(object):\n    def __init__(self, model, n_actions, cons, cats, emb_dims, algo=None, env_name=None, chkpt_dir='tmp\/dqn'):\n        self.epsilon = config.epsilon\n        self.n_actions = n_actions\n        self.cols = cons.shape[0] + cats.shape[0]\n        self.input_dims = (1, self.cols)\n        self.algo = algo\n        self.env_name = env_name\n        self.chkpt_dir = chkpt_dir\n        self.action_space = [i for i in range(n_actions)]\n        self.learn_step_counter = 0\n        self.mse = nn.MSELoss()\n        self.loss = 0\n\n        self.memory = ReplayBuffer()\n\n        self.q_eval = DeepQNetwork(model, self.n_actions, n_cont=cons.shape[0], emb_dims=emb_dims, hidden_size=200, nlayers=2, name=f'{self.env_name}_{self.algo}_q_eval', chkpt_dir=self.chkpt_dir)\n        self.q_next = DeepQNetwork(model, self.n_actions, n_cont=cons.shape[0], emb_dims=emb_dims, hidden_size=200, nlayers=2, name=f'{self.env_name}_{self.algo}_q_next', chkpt_dir=self.chkpt_dir)\n\n    def store_transition(self, states, actions, rewards, states_, dones):\n         self.memory.store_transition(states, actions, rewards, states_, dones)\n            \n    def sample_memory(self):\n        state,action,reward,state_,done = self.memory.sample_buffer()\n\n        states  =  state.squeeze().to(config.device)\n        states_ = state_.squeeze().to(config.device)\n        actions = action.unsqueeze(-1).to(config.device)\n        rewards = reward.unsqueeze(-1).to(config.device)\n        dones   =   done.unsqueeze(-1).to(config.device)\n\n        return states, actions, rewards, states_, dones\n        \n    def choose_action(self, observations):\n        actionlist = [None] * observations.size(0)\n        states = observations.to(config.device)\n        actions = self.q_eval.forward(states)\n        actions = T.argmax(actions, dim=1)\n        for idx in range(actions.size(0)):\n            if np.random.random_sample() > self.epsilon:\n                action = actions[idx].item()\n            else:\n                action = np.random.choice(self.action_space)\n            actionlist[idx] = action\n        actions = T.tensor(actionlist)\n        return actions\n\n    def replace_target_network(self):\n        if self.learn_step_counter % config.replace_cnt == 0:\n            self.q_next.load_state_dict(self.q_eval.state_dict())\n\n    def decrement_epsilon(self):\n        self.epsilon = self.epsilon * config.eps_dec if self.epsilon > config.eps_min else config.eps_min\n\n    def learn(self):\n        if len(self.memory) < config.bs:\n            return\n\n        self.replace_target_network()\n        s,a,r,s_,d = self.sample_memory()\n        \n        a = a.long()        \n        q = self.q_eval.forward(s).gather(-1,a)\n        \n        a_ = T.argmax(self.q_eval.forward(s_),1).unsqueeze(-1)\n        \n        q_ = self.q_next.forward(s_).gather(-1,a_.long())\n        q_ = r + config.gamma * q_ * (1-d)\n        \n        loss = self.mse(q, q_)\n        self.q_eval.optimizer.zero_grad()\n        loss.backward(retain_graph=True)\n        self.q_eval.optimizer.step()\n        self.loss = loss.item()\n        self.learn_step_counter += 1\n\n        self.decrement_epsilon()\n\n    def save_models(self):\n        self.q_eval.save_checkpoint()\n        self.q_next.save_checkpoint()\n\n    def load_models(self):\n        self.q_eval.load_checkpoint()\n        self.q_next.load_checkpoint()","15249b1a":"class MarketEnv(gym.Env):\n    metadata = {'render.modes': ['human']}\n\n    def __init__(self, nfeats):\n        super(MarketEnv, self).__init__()\n        self.n_actions = 3 # {Buy: 0, Sell: 1, Hold: 2}\n        self.action_space = spaces.Discrete(self.n_actions)\n        self.observation_space = spaces.Box(low=0, high=1, shape=(nfeats,config.bs))\n\n        self.cash = config.cash\n        self.shares_owned = 0\n        self.reward = 0\n        self.done = 0\n        self.n_trades = 0\n        self.close = None\n        self.prev_close = 0.672762 # First close value (normalized)\n        self.risum = 0\n        self.rfsum = 0\n        self.initial_bh = self.prev_close # initial buy and hold price\n        \n    def step(self, actions, states):\n        num_shares = 10\n        buy = False\n        sel = False\n        reward = [None] * actions.size(0)\n        done = [None] * actions.size(0)\n        trades = [None] * actions.size(0)\n        self.acc_values = [None] * actions.size(0)\n        closes = states[:,19]\n        \n        for idx,element in enumerate(zip(actions, closes)):\n            action, self.close = element\n\n            if action==0 and not buy: # Buy\n                buy = True\n                sel = False\n                self.n_trades += 1\n                self.cash -= self.close * num_shares # Cost basis\n                self.shares_owned += num_shares\n            elif action==1 and not sel: # Sell\n                buy = False\n                sel = True\n                self.n_trades += 1\n                self.cash += num_shares * self.close\n                self.shares_owned -= num_shares\n            elif action==2:\n                buy = False\n                sel = False\n\n#             self.reward = self.reward_math(buy, sel)\n            if self.acc_values[-1] < (self.cash + self.shares_owned * self.close):\n                self.reward = -1\n            elif self.acc_values[-1] > (self.cash + self.shares_owned * self.close):\n                self.reward = 1\n            else:\n                self.reward = 0\n            self.done = 1 if self.cash < 0 else 0 # Need enough cash to open a position\n            self.acc_values[idx] = self.cash + self.shares_owned * self.close\n            \n            reward[idx] = self.reward\n            done[idx] = self.done\n            self.prev_close = self.close\n        \n        rewards = T.tensor(reward)\n        dones = T.tensor(done)\n        \n        return [rewards, dones]\n\n#     https:\/\/ai.stackexchange.com\/questions\/10082\/suitable-reward-function-for-trading-buy-and-sell-orders\/10912\n    def reward_math(self, buy, sel):\n        fees = 0.0025 # Fees associated with making a trade. Set to 0.25% per trade\n        excess_trading_loss = math.log((1-fees)\/(1+fees))\n        log_close = math.log(self.close)\n        log_prev_close = math.log(self.prev_close)\n        self.risum += buy * (log_close - log_prev_close)\n        self.rfsum += sel * 0.1 * self.prev_close \/ 525_600 # 10% * previous minute's closing price \/ minutes per year (i.e. Baseline growth per share)\n        r = self.risum + self.rfsum\n        rbh = self.close - self.initial_bh + excess_trading_loss\n        return math.tanh(0.01*(r - rbh))\n\n    def reset(self):\n        self.cash = config.cash\n        self.acc_value = config.cash\n        self.n_trades = 0\n        self.shares_owned = 0\n        self.done = 0\n        self.risum = 0\n        self.rfsum = 0\n        \n        return T.zeros(config.bs,nfeats)\n    \n    def render(self, baseline, accv, loss=None):\n        if type(loss)==dict:\n            fig = plt.figure(figsize=(35,10))\n            gs = fig.add_gridspec(2, 1, hspace=0, wspace=0)\n            (ax1, ax2) = gs.subplots(sharex='col')\n            for key in loss.keys():\n                ax1.plot(np.linspace(0,len(accv[key]),len(loss[key])),loss[key], label=f'Epoch {key}')\n                ax2.plot(np.linspace(0,len(accv[key]),len(accv[key])),accv[key], label=f'Epoch {key}')\n            ax2.plot(np.linspace(0,len(baseline),len(baseline)),baseline, color='gray', label='Baseline');\n            ax2.set_xlabel('Minutes')\n            ax2.set_xlim(xmin=-3)\n            fig.suptitle(f'Loss and Account Value over {key+1} epochs')\n            ax1.set_ylabel('Loss')\n            ax2.set_ylabel('Account Value')\n            ax2.legend(bbox_to_anchor=(1.0, 2.03), loc='upper left')\n            plt.show()\n            \n        else:\n            fig = plt.figure(figsize=(35,10))\n            plt.plot(np.linspace(0,len(accv['test']),len(accv['test'])), accv['test'], color='red', label='Test')\n            plt.plot(np.linspace(0,len(baseline),len(baseline)), baseline, color='gray', label='Baseline');\n            plt.title('Trained Models vs Baseline Buy and Hold')\n            plt.xlabel('Minutes')\n            plt.xlim(xmin=-3)\n            plt.ylabel('Account Value')\n            plt.legend(bbox_to_anchor=(1.0, 1.03), loc='upper left')\n            plt.show()","c3d19ae5":"class FastTensorDataLoader:\n    \"\"\"\n    A DataLoader-like object for a set of tensors that can be much faster than\n    TensorDataset + DataLoader because dataloader grabs individual indices of\n    the dataset and calls cat (slow).\n    Source: https:\/\/discuss.pytorch.org\/t\/dataloader-much-slower-than-manual-batching\/27014\/6\n    \"\"\"\n    def __init__(self, *tensors, shuffle=False):\n        \"\"\"\n        Initialize a FastTensorDataLoader.\n        :param *tensors: tensors to store. Must have the same length @ dim 0.\n        :param batch_size: batch size to load.\n        :param shuffle: if True, shuffle the data *in-place* whenever an\n            iterator is created out of this object.\n        :returns: A FastTensorDataLoader.\n        \"\"\"\n        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n        self.tensors = tensors\n\n        self.dataset_len = self.tensors[0].shape[0]\n        self.shuffle = shuffle\n\n        n_batches, remainder = divmod(self.dataset_len, config.bs)\n        if remainder > 0:\n            n_batches += 1\n        self.n_batches = n_batches\n    def __iter__(self):\n        if self.shuffle:\n            r = T.randperm(self.dataset_len)\n            self.tensors = [t[r] for t in self.tensors]\n        self.i = 0\n        return self\n\n    def __next__(self):\n        if self.i >= self.dataset_len:\n            raise StopIteration\n        batch = tuple(t[self.i:self.i+config.bs] for t in self.tensors)\n        self.i += config.bs\n        return batch[0]\n\n    def __len__(self):\n        return self.n_batches\n    \n    \ndef categorify(df1, cats):\n    df1.loc[:,cats] = df1.loc[:,cats].astype('int')\n    embs = {}\n    for col in cats:\n        embs[col] = df1.loc[:,col].unique().tolist()\n        embs[col].sort()\n\n    for key in embs:\n        for idx,val in enumerate(embs[key]):\n            df1.loc[:,key].replace(val,idx,inplace=True)\n    df1.loc[:,cats] = df1.loc[:,cats].astype('category')\n    return df1\n\ndef normalize(df1, conts):\n    mean,std = None,None\n    for col in conts:\n        assert is_numeric_dtype(df1[col]), \"Column is not numeric\"\n        mn,mx = df1[col].min(), df1[col].max()\n        df1[col] = (df1[col] - mn)\/(mx-mn) + 1e-9\n    return df1\n\ndef test_train_split(df, split_idx):\n    return df.iloc[:split_idx,:].copy(), df.iloc[split_idx:,:].copy()\n        \ndef prep_data(df, split_idx):\n    cont_feats = df.columns[15:]\n    cat_feats = df.columns[:15]\n    \n    df = categorify(df, cat_feats)\n    traindf, testdf = test_train_split(df,split_idx)\n    cat_nuniq = [df.loc[:,x].nunique() for x in cat_feats]\n    emb_dims = [(x, min(50, (x + 1) \/\/ 2)) for x in cat_nuniq]\n    \n    traindf = normalize(traindf, cont_feats)\n    testdf = normalize(testdf, cont_feats)\n\n    trainT = T.tensor(traindf.values)\n    testT = T.tensor(testdf.values)\n    traindl = FastTensorDataLoader(trainT)\n    testdl = FastTensorDataLoader(testT)\n\n    return traindl, testdl, (cont_feats, cat_feats), emb_dims","d3f02b41":"def train(baseline, traindl, env, agent):\n    start = time.time()\n    loss = {}\n    account_value = {}\n    space = 15\n    acc_sum = 0\n    max_acc = 0\n    states_ = None\n    for i in range(config.epochs):\n        estart = time.time()\n        loss[i] = []\n        account_value[i] = []\n        states = env.reset()\n        if i==0:\n            states_ = states\n        for step,states in enumerate(traindl):\n            actions = agent.choose_action(states)\n            rewards, dones = env.step(actions, states)\n            \n            agent.store_transition(states, actions, rewards, states_, dones)\n            agent.learn()\n            states_ = states\n            account_value[i] += env.acc_values\n            loss[i].append(agent.loss)\n        avg_acc_val = sum(account_value[i])\/len(account_value[i])\n        max_acc_val = max(account_value[i])\n        emin,esec = divmod(time.time()-estart,60)\n        t = f'{emin:02.0f}:{esec:02.0f}'\n        print(f'Epoch: {i} | Acc value: ${account_value[i][-1]:,.2f} | Avg acc value: ${avg_acc_val:,.2f} | Max acc value: ${max_acc_val:,.2f} | Epsilon: {agent.epsilon:.5f} | Max Loss: {max(loss[i].5f)} | Elapsed: {t}')\n\n    m, s = divmod(time.time()-start, 60)\n    h, m = divmod(m, 60)\n    t = f'{h:02.0f}:{m:02.0f}:{s:02.0f}'\n    print(f'Total training time: {t:^10}\\n\\n')\n    env.render(baseline, account_value, loss)\n    \ndef test(baseline, testdl, env, agent):\n    start = time.time()\n    account_value = {'test':[]}\n    space = 15\n    acc_sum = 0\n    max_acc = 0\n    states = env.reset()\n    \n    for states in testdl:\n        actions = agent.choose_action(states)\n        rewards, dones = env.step(actions, states)\n        account_value['test'] += env.acc_values\n\n    avg_acc_val = sum(account_value['test'])\/len(account_value['test'])\n    max_acc_val = max(account_value['test'])\n    print(f'Acc value: ${account_value[\"test\"][-1]:,.2f} | Avg acc value: ${avg_acc_val:,.2f} | Max acc value: ${max_acc_val:,.2f}')\n\n    env.render(baseline, account_value)","06719403":"plt.rcParams.update({'font.size': 25})\n\ntraindf = pd.read_pickle('..\/input\/d\/filipinogambino\/historicals\/first_500000.pkl')\ntestdf = pd.read_pickle('..\/input\/d\/filipinogambino\/historicals\/test_set.pkl')\nfig = plt.figure(figsize=(35,10))\nplt.plot(traindf.index, traindf.close_spy, color='red', label='train');\nplt.plot(testdf.index, testdf.close_spy, color='gray', label='test');\nplt.title('SPY Closing Price on 1min Candles')\nplt.xlabel('Time (min)')\nplt.ylabel('SPY Value ($)')\nplt.legend()\nplt.show()","6757b2a4":"%%time\n\ntraindf = pd.read_pickle('..\/input\/d\/filipinogambino\/historicals\/first_500000.pkl').iloc[:,:-4] # OHCLV historical data\ntestdf = pd.read_pickle('..\/input\/d\/filipinogambino\/historicals\/test_set.pkl').iloc[:,:-4]\nbaseline_shares = config.cash \/ traindf.loc[0,'close_spy']\nbaseline_values = baseline_shares * traindf['close_spy']\ndf = pd.concat([traindf,testdf]).reset_index(drop=True)\ntraindl, testdl, feats, emb_dims = prep_data(df, split_idx=int(len(traindf)*.75))\nnfeats = sum([x.shape[0] for x in feats])\n\nenv = MarketEnv(nfeats=nfeats)\n\nload_checkpoint = False\nif load_checkpoint:\n    agent.load_models()\nelse:\n    agent = DDQNAgent(model='gru', cons=feats[0], cats=feats[1], emb_dims=emb_dims, n_actions=env.action_space.n, chkpt_dir='.\/', algo='DDQNAgent', env_name='MarketEnv')\n\ntrain(baseline_values, traindl, env, agent)\n        \nagent.save_models()","cdb5751b":"%%time\n\ntestdf = pd.read_pickle('..\/input\/d\/filipinogambino\/historicals\/test_set.pkl').iloc[:,:-4]\ntestdf = testdf.reset_index(drop=True)\nbaseline_shares = config.cash \/ testdf.loc[0,'close_spy']\nbaseline_values = baseline_shares * testdf['close_spy']\n\nenv = MarketEnv(nfeats=nfeats)\nagent.load_models()\n\ntest(baseline_values, testdl, env, agent)","b6d2f021":"# Environment","e1acae09":"# Modeling","5aa4b0f2":"# Replay Buffer","bccd648d":"# Agent","ee0f214a":"# Preparing the data","535e51ae":"# DDQN"}}