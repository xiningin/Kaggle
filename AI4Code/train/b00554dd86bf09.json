{"cell_type":{"8c5ef062":"code","13f92b7e":"code","97f54250":"code","9d39969b":"code","e5743e84":"code","1f37e996":"code","0f2fc4ad":"code","78593da4":"code","f2cce629":"code","8d6000d8":"code","87fe5d66":"code","231612df":"code","611d8b0d":"code","0fe923dc":"code","af8ab390":"code","8786e22c":"code","21dc3d10":"code","3dcedd55":"code","0b8358a0":"code","d01c47fd":"code","c37e049f":"code","49a741d3":"code","7b04f98c":"code","f4c98d27":"code","53a293e4":"code","caf3441c":"code","4b2d5dd8":"code","2a3bc473":"code","0ce479b7":"code","6008dcbb":"code","7c2f2d1e":"code","a73880ce":"code","496731cb":"code","edbe3fd9":"code","4e0a573c":"code","b0d8435f":"code","22487b9d":"code","cefe82f7":"code","a961e6cd":"code","742306b6":"code","2f141a00":"code","ac7f8105":"code","6acfeccc":"code","1c11df40":"code","c9a86793":"code","a240aea1":"code","58a57a77":"code","5a2fd39a":"code","fa63d3cf":"code","3a7de341":"code","041bb739":"code","5541365f":"code","d8086826":"code","bc0a46e4":"code","458fa291":"code","f23e3020":"code","7fa3f4a4":"markdown","98fdf73a":"markdown","311987de":"markdown","84f72dac":"markdown","145034a9":"markdown","a93a4e23":"markdown","3b0d543f":"markdown","0862d1d7":"markdown","ef0339ce":"markdown","96bc6b01":"markdown","3954a240":"markdown","57372cef":"markdown","122e1022":"markdown","a6c98b1b":"markdown","1556a719":"markdown","b12f8091":"markdown","5ec1a861":"markdown","60368af3":"markdown","798d68a9":"markdown","da276621":"markdown","f830d279":"markdown","65c9914c":"markdown","b4e890a6":"markdown","0d4ac370":"markdown","47fabec7":"markdown"},"source":{"8c5ef062":"import pandas as pd\nimport numpy as np\n\nfrom scipy.stats import boxcox\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline \nplt.rcParams['figure.figsize']=[20,6]\nplt.rcParams['image.cmap'] = 'gray'\nsns.set_theme(style='darkgrid')\n\nimport warnings\nwarnings.filterwarnings('ignore')","13f92b7e":"train = pd.read_csv('..\/input\/rossmann-store-sales\/train.csv', parse_dates=['Date'])\ntest  = pd.read_csv('..\/input\/rossmann-store-sales\/test.csv', parse_dates=['Date'])\nstore = pd.read_csv('..\/input\/rossmann-store-sales\/store.csv')","97f54250":"# train.isna().sum()\ntest.isna().sum()","9d39969b":"store.isna().sum()","e5743e84":"test['Open'] = test.groupby('Date')['Open'].apply(lambda x: x.fillna(x.mode()[0]))\n\nstore['CompetitionDistance'] = store['CompetitionDistance'].fillna(store['CompetitionDistance'].median())\n# store['Promo2SinceWeek'] = store['Promo2SinceWeek'].fillna(0)\n# store['Promo2SinceYear'] = store['Promo2SinceYear'].fillna(0)\n# store['PromoInterval'] = store['PromoInterval'].fillna(0)\n\nstore.fillna(0, inplace=True)\nc = [i for i in store.columns if store[i].dtype != 'object']\nstore[c] = store[c].astype(int)\n\nstore['CompetitionOpenSinceMonth'].replace(0,1, inplace=True)\nstore['CompetitionOpenSinceYear'].replace(0,2013, inplace=True)\nstore['CompetitionSince'] = pd.to_datetime(store['CompetitionOpenSinceYear'].astype(str) +'\/' +\\\n                store['CompetitionOpenSinceMonth'].astype(str) + '\/01')\n\nstore['Promo2SinceWeek'] = (store['Promo2SinceWeek']\/int(4)).astype(int)\nstore['Promo2SinceWeek'].replace(0,12, inplace=True)\nstore['Promo2SinceYear'].replace(0,2000, inplace=True)\nstore['promo2Since'] = pd.to_datetime(store['Promo2SinceYear'].astype(str) + '\/' + store['Promo2SinceWeek'].astype(str) + '\/01')\nstore.drop(['CompetitionOpenSinceMonth','CompetitionOpenSinceYear','Promo2SinceWeek','Promo2SinceYear'], axis=1, inplace=True)","1f37e996":"\n\ndef statHol(df):\n    \"\"\"\n    Not idempotent\n    \"\"\"\n    # a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n    # Label encoding the state holidays\n\n\n\n    df['StateHoliday'] = np.where(df['StateHoliday'] == 'a', 1,\n                                    np.where(df['StateHoliday'] == 'b', 2,\n                                            np.where(df['StateHoliday'] == 'c', 3, df['StateHoliday'])))\n\n    df['StateHoliday'] = df['StateHoliday'].astype(int)\n    \n    return df\n\ntrain = statHol(train)\ntest  = statHol(test)\n# df_all = pd.concat([train, test])\ndf = pd.merge(train,store, on='Store', how='left') \ndft = pd.merge(test, store, on='Store', how='left')\n# df.info()\n","0f2fc4ad":"class RandomCheck:\n    \n    def __init__(self):\n        pass\n    \n    \n    def randomStore():\n        \n        return np.random.choice(train.Store.unique())\n    \n    def eda(*args):\n        \n        if args:\n            nb = args[0]\n        else:\n#         nb = np.random.choice(train.Store.unique())\n            nb = RandomCheck.randomStore()\n    \n        df = train[train['Store']==nb]\n        df = df.sort_values(by='Date')\n        \n        store_type = store[store['Store']==nb]['StoreType']\n        \n        fig, (ax1, ax2, ax3) = plt.subplots(1,3)\n        \n        fig.suptitle(f\"STORE NUMBER:{nb}; TYPE:{store_type.iloc[0].upper()}\", fontsize=22, fontweight='bold')\n        \n        ax1.set_title(f\"Open vs Closed\")\n        ax1 = sns.stripplot(x='Open', y='DayOfWeek', data=df, ax=ax1)\n        ax1.set_xticklabels(['Closed', 'Open'])\n        \n        ax2.set_title(\"Sales over the week\")\n        data = pd.DataFrame(df.groupby('DayOfWeek')['Sales'].mean())\n        ax2 = sns.barplot(x=data.index, y='Sales', data=data, ax=ax2)\n        \n        ax3.set_title(\"Customers over the week\")\n        data = pd.DataFrame(df.groupby('DayOfWeek')['Customers'].mean())\n        ax3 = sns.barplot(x=data.index, y='Customers', data=data, ax=ax3)\n        \n        plt.tight_layout()\n        \n        fig, (ax1, ax2, ax3) = plt.subplots(1,3)\n        \n        ax1.set_title(\"Promo effect\")\n        data = pd.DataFrame(df.groupby('Promo')['Sales'].mean())\n        ax1 = sns.barplot(x=data.index, y='Sales', data=data, ax=ax1)\n        \n        ax2.set_title(\"School Holiday effect\")\n        data = pd.DataFrame(df.groupby('SchoolHoliday')['Customers'].mean())\n        ax2 = sns.barplot(x=data.index, y='Customers', data=data, ax=ax2)\n        \n        ax3.set_title(\"State Holiday effect\")\n        data = pd.DataFrame(df.groupby(['StateHoliday'])['Sales'].mean())\n        ax3 = sns.barplot(x=data.index, y='Sales', data=data, ax=ax3)\n        ax3.set_xticklabels(['No holiday', 'Public holiday', 'Easter', 'Christmas'])\n        \n        plt.tight_layout()\n        \n        l = ['Sales', 'Customers']\n        \n        fig,ax = plt.subplots(1,2)\n        \n        fig.suptitle(\"Univariate Analysis\", fontsize=18, fontweight='bold')\n        \n        for i,j in enumerate(l):\n            \n            sns.kdeplot(df[j], ax=ax[i])\n        \n        plt.tight_layout()\n        \n        df = df[df['Open']==1]\n        df['Sales'], _ = boxcox(df['Sales'].replace(0,1))\n        df['Customers'], _ = boxcox(df['Customers'].replace(0,1))\n        \n        fig,ax = plt.subplots(1,2)\n        \n        fig.suptitle(\"Normalized\", fontsize=14, fontweight='bold')\n        \n        for i,j in enumerate(l):\n            \n            sns.kdeplot(df[j], ax=ax[i])\n        \n        plt.tight_layout()\n        \n        return nb\n        \n    def printnb():\n        \n        nb = RandomCheck.randomStore()\n        print(nb)\n\n    def store(nb):\n        \n        data = train[train['Store']==nb]\n        data = data[['Date', 'Sales']]\n        data = data.loc[data['Sales']!=0]\n        data.set_index('Date', inplace=True)\n        v = store[store['Store']==nb]['StoreType'].iloc[0]\n        plt.figure(figsize=[14,4])\n#         data[:30].plot(title=f\"STORE NUMBER: {nb}\");\n#         data[:300].plot();\n#         data.plot();\n        data.resample('W').mean().plot(title=f\"Store:#{nb}|| Type:{v}\\n Weekly view\")\n        data.resample('M').mean().plot(title=\"Monthly view\")\n        data.resample('Y').mean().plot(title=\"Yearly view\")\n        \n    def eda2():\n        \n        plt.title(\"Store Type\")\n        data = pd.DataFrame(df.groupby('StoreType')['Sales'].mean())\n        ax = sns.barplot(x=data.index, y='Sales', data=data)\n        \n        plt.title(\"Assortment Type\")\n        data = pd.DataFrame(df.groupby('Assortment')['Sales'].mean())\n        ax = sns.barplot(x=data.index, y='Sales', data=data)\n        \n    def competition(*args):\n                    \n        arr =[] \n        fig, ax = plt.subplots(2,2)\n        \n        for h,i in enumerate(ax.flatten()):\n            \n            if not args:\n                nb = RandomCheck.randomStore()\n            else:\n                nb = args[0][h]\n\n            arr.append(nb)\n                \n            data=df[df['Store']==nb]\n            data=data[['Date','Sales']].set_index('Date').sort_index()\n            data=data[data['Sales']!=0]\n            data['26DayMovingAverage'] = data.rolling(window=26).mean()\n            \n            idx = store[store['Store']==nb]['CompetitionSince'].iloc[0]\n            \n            \n            if idx > pd.to_datetime('2013\/01\/01'):\n              \n                data.plot(ax=i, rot=0, title=f\"Store #{nb}\")\n                i.axvline(idx, color='r')\n                i.text(idx, 6000,'Competition enters', fontweight='heavy', rotation=45)\n                \n            else:\n                \n                idx = data.index[int(len(data)\/2)]\n                idy = 0.8 * (data.Sales.max())\n                data.plot(ax=i, rot=0, title=f\"Store #{nb}\")\n                i.text(idx, idy,'No competition', fontweight='heavy')\n                \n            plt.tight_layout()\n            \n        return arr\n            \n    def competitionDis(arr):\n        \n        dis = {}\n        \n        for i in arr:\n            \n            dis[i] = store[store['Store']==i]['CompetitionDistance'].iloc[0]\n        pd.DataFrame.from_dict(dis, orient='index').plot.barh(xlabel='Store#',\n                                                              rot=0, \n                                                              legend=False,\n                                                             figsize=[18,2])\n    def storetype(*args):\n        \n        if not args:\n            args = ['a','b','c','d']\n        l=[]\n        for n, typ in enumerate(args):\n\n            nb = np.random.choice(df[df['StoreType']==typ]['Store'].unique())\n            c=['red','blue','green','black']\n            l.append(f\"Store#{nb}||Type#{typ}\")\n            data = df[df['Store']==nb]\n            data = data[data['Sales']!=0]\n            data['12MA'] = data['Sales'].rolling(window=12).mean()\n            plt.plot(data['Date'], data['12MA'], c=c[n], linewidth=2)\n        plt.ylabel('Sales')\n        plt.xlabel('Date')\n        plt.xticks([])\n        plt.yticks([])\n        plt.legend(labelcolor=c, labels=l)\n        \n    def promo(arr):\n        \n        for nb in arr:\n            \n            data = df[df['Store']==nb]\n            data = data[['Date', 'Promo', 'Sales']]\n            data = data[data['Sales']!=0]\n            dat1 = data[data['Promo']==1].set_index('Date').sort_index()\n            dat2 = data[data['Promo']==0].set_index('Date').sort_index()\n            data['26MovingAverage'] = data['Sales'].rolling(window=26).mean()\n            dat1['Sales'].plot.area(title=f'Promo vs No Promo: Store#{nb}')\n            plt.scatter(dat2.index, dat2.Sales, c='orange')\n            plt.plot(data.Date, data['26MovingAverage'], c='r', linewidth=6)\n            plt.xticks([])\n            plt.yticks([])\n            plt.ylabel('Sales')\n            plt.legend(labelcolor=['orange', 'r', 'b'], labels=['No promo', 'Moving Average', 'Promo'], framealpha=0.0,\n                      frameon=True, shadow=True, loc='upper left')\n            plt.figure()\n        plt.tight_layout()\n    \n    def promo2(*args):\n        \n        if args:\n            nb=args[0]\n        else:\n            nb = np.random.choice(store[store['Promo2']==1]['Store'].unique())\n        \n        v1 = store[store['Store']==nb]['promo2Since'].iloc[0]\n        v2 = v1 + pd.DateOffset(months=1)\n        v3 = store[store['Store']==nb]['PromoInterval'].iloc[0]\n        data = train[train[\"Store\"]==nb]\n        data = data[data['Sales']!=0]\n\n        plt.title(f\"Store:#{nb} || Promo2 Interval:{v3} || Promo2 start:{v1}\")\n        plt.plot(data['Date'], data['Sales'])\n        if v1>pd.to_datetime('2013-01'):\n            plt.axvspan(v1, v2, alpha=0.5, color='red')\n        plt.legend(labelcolor=['blue','red'], labels=['sales','promo2'])\n        plt.yticks([])\n        plt.ylabel('Sales')\n        \n    def fft(*args):\n        \n        if args:\n            nb=args[0]\n        else:\n            nb = np.random.choice(store['Store'].unique())\n            \n        stype = store[store['Store']==nb]['StoreType'].iloc[0]\n\n        data = train[train['Store']==nb]\n        data = data[['Date', 'Sales']].set_index('Date').sort_index()\n\n        fft = np.abs(np.fft.rfft(data.Sales.values))\n        n_years = len(data)\/365.2425\n        f_years = np.arange(0, len(fft)) \/ n_years\n        \n        plt.figure(figsize=[6,2])\n        plt.title(f\"SEASONALITY \\nStore:{nb} StoreType:{stype}\")\n        plt.step(f_years, fft)\n        plt.ylim(0,2000000)\n        plt.xscale('log')\n        plt.yticks([])\n        plt.xticks([1,12,52,365.24], labels=['1\/year','1\/months','1\/weeks','1\/days'])\n        plt.show();","78593da4":"nb = RandomCheck.eda()","f2cce629":"RandomCheck.store(nb)","8d6000d8":"RandomCheck.fft(nb)","87fe5d66":"## to validate the first point where stores are closed on Day7\n\nnsales  = train[(train['Sales']==0) & (train['Open']!=0)]\nprint(f\"Stores open but no sales: {len(nsales)}\")\n# train = train[(train['Sales']!=0) & (train['Open']==1)]\n# print(f\"Training data after dropping the days where there is no sales:{len(train)}\")","231612df":"for i,j in enumerate(list(df.StoreType.unique())):\n    \n    k = len([df.groupby('StoreType')['Store'].unique()][0][i])\n    print(f\"Total number of '{j}' type stores is {k}\")\n    \nfor i,j in enumerate(list(df.Assortment.unique())):\n    \n    if j == 'a':\n        j='basic'\n    elif j=='b':\n        j='extra'\n    else:\n        j='extended'\n    k = len([df.groupby('Assortment')['Store'].unique()][0][i])\n    print(f\"Total number of '{j}' assortment type is {k}\") ","611d8b0d":"\ndata1 = pd.DataFrame(df.groupby(['StoreType'])['Sales', 'Customers'].mean()).sort_values(by='StoreType')\ndata2 = pd.DataFrame(df.groupby(['Assortment'])['Sales', 'Customers'].mean()).sort_values(by='Assortment')\ndata3 = pd.DataFrame(df.groupby(['StoreType'])['Sales', 'Customers'].sum()).sort_values(by='StoreType')\ndata4 = pd.DataFrame(df.groupby(['Assortment'])['Sales', 'Customers'].sum()).sort_values(by='Assortment')\n\n# fig, (ax1,ax2,ax3,ax4) = plt.subplots(2,2)\n\n# for i in ax.flatten():\n    \nax1 = plt.subplot(221)\nplt.title(\"Mean values\")\ndata1.plot.bar(ax=ax1, rot=0)\nax2 = plt.subplot(222)\nplt.title(\"Mean values\")\ndata2.plot.bar(ax=ax2, rot=0)\nax2.set_xticklabels(['basic', 'extra', 'extended'])\n\nax1 = plt.subplot(223)\nplt.title(\"Summed values\")\ndata3.plot.bar(ax=ax1, rot=0)\nax2 = plt.subplot(224)\nplt.title(\"Summed values\")\ndata4.plot.bar(ax=ax2, rot=0)\nax2.set_xticklabels(['basic', 'extra', 'extended'])\nplt.tight_layout()","0fe923dc":"RandomCheck.storetype()","af8ab390":"store['StoreType'].value_counts().sort_index()","8786e22c":"ax1 = plt.subplot(131)\ndf.groupby('StoreType')['Sales'].sum().plot.pie(ax=ax1, title='Revenue share', legend=True, autopct='%1.1f%%', shadow=True)\nax2 = plt.subplot(132)\ndf.groupby('StoreType')['Customers'].sum().plot.pie(ax=ax2, title='Customer share', legend=True, autopct='%1.1f%%', shadow=True)\nax3 = plt.subplot(133)\nstore['StoreType'].value_counts().sort_index().plot.pie(rot=0, title='Store types', autopct='%1.1f%%', shadow=True, legend=True)","21dc3d10":"arr = RandomCheck.competition([1086,983,675,578])","3dcedd55":"RandomCheck.competitionDis(arr)","0b8358a0":"RandomCheck.promo(arr)","d01c47fd":"store.info()","c37e049f":"RandomCheck.promo2()","49a741d3":"RandomCheck.fft()","7b04f98c":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.stattools import adfuller, grangercausalitytests\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.statespace.varmax import VARMAX\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.api import ExponentialSmoothing\nfrom tqdm import tqdm_notebook\nfrom itertools import product","f4c98d27":"class Model:\n    \n    def __init__():\n        pass\n    \n    def optimizeSARIMAX():\n        \n        # --> (p,q,P,Q)\n        \n        p=q=Q=P =range(0,4,1)\n        combo = list(product(p,q,P,Q))\n        d=D=0\n        s=12\n        \n        res = []\n\n        for i in tqdm_notebook(combo):\n\n            try:\n                \n#                 print(i[0], i[2])\n\n                model = SARIMAX(endogTrain['Sales'], exog=exogTrain, order=(i[0],0,i[1]),\n                               seasonal_order=(i[2],0,i[3],s), simple_differencing=False).fit(disp=False)\n\n            except:\n\n                continue\n            aic = model.aic\n            res.append([i,aic])\n\n        resDF = pd.DataFrame(res,columns=['combo','AIC']).sort_values(by='AIC').reset_index(drop=True)\n\n        return resDF\n\n    def optimizeVAR(endog, exog):\n\n        p = range(0,7,1)\n        q = range(0,7,1)\n\n        combo = list(product(p,q))\n\n        res=[]\n\n        for i in tqdm_notebook(combo):\n\n            try:\n\n                model = VARMAX(endog=endog, exog=exog, order=i, enforce_stationarity=True).fit(disp=False)\n\n            except:\n\n                continue\n\n            mse = model.mse\n            aic = model.aic\n\n            res.append([i,mse,aic])\n\n        varDF = pd.DataFrame(res, columns=['combo','mse','aic']).sort_values(by='mse')\n\n        return varDF\n\n    def statTests(df):\n\n        print(f\"p-value:{adfuller(df)[1]}\")\n        \n    def causeTest(df,n):\n        \n        print(\"Sales causes customers\")\n        grangercausalitytests(endog[['Sales', 'Customers']], 4)\n        print(\"#####################################################################\")\n        print(\"\\nCustomer causes sales\")\n        grangercausalitytests(endog[['Customers', 'Sales']], 4)\n        \n    def preWithZero(*args):\n        if args:\n            nb = args\n        else:\n            nb = np.random.choice(len(store))      \n        dat = train[train['Store']==nb]\n        dat = dat.sort_values(by=\"Date\").reset_index(drop=True)\n        endog = dat[['Sales','Customers']]\n        dt = dat.pop('Date')\n        exog  = dat.drop(['Store','Sales','Customers'], axis=1)\n        endog = endog.applymap(float)\n        split = int(0.8*len(endog))\n        endogTrain = endog[:split]\n        exogTrain  = exog[:split]\n        endogTest = endog[split:]\n        exogTest  = exog[split:]\n        \n        return dt, endogTrain, exogTrain, endogTest, exogTest\n    \n    \n    def preWithoutZero(*args):\n        if args:\n            nb = args\n        else:\n            nb = np.random.choice(len(store))      \n        dat = train[train['Store']==nb]\n        dat = dat[dat['Sales']!=0]\n        dat = dat.sort_values(by=\"Date\").reset_index(drop=True)\n        endog = dat[['Sales','Customers']]\n        dt = dat.pop('Date')\n        exog  = dat.drop(['Store','Sales','Customers'], axis=1)\n        endog = endog.applymap(float)\n        split = int(0.8*len(endog))\n        endogTrain = endog[:split]\n        exogTrain  = exog[:split]\n        endogTest = endog[split:]\n        exogTest  = exog[split:]\n        \n        return dt, endogTrain, exogTrain, endogTest, exogTest\n    \n    def plotPred(model):\n        \n        preds = model.get_prediction(end=len(dt)-1, exog=exogTest)\n        predDF = preds.predicted_mean\n        plt.plot(dt[-50:], endogTrain['Sales'][-50:], c='g')\n        plt.plot(dt[-50:], predDF.iloc[-50:], c='r')\n        plt.legend(labelcolor=['green', 'red'], labels=['Actual', 'Forecast'])\n        return predDF\n        \n    def metrics(predDF):\n        \n        e = predDF - endogTest[\"Sales\"]\n        esq = e**2\n        mse = esq.mean()\n        rmse = np.sqrt(mse)\n        rmspe = (np.sqrt(np.mean(np.square((endogTest[\"Sales\"] - predDF) \/ (endogTest[\"Sales\"]))))) * 100\n        mape = np.mean(np.abs((predDF - endogTest[\"Sales\"] )\/ endogTest[\"Sales\"])) * 100\n        print(f\" MSE:{np.round(mse,2)}\\n RMSE:{np.round(rmse,2)}\\n MAPE:{np.round(mape,2)}%\\n RMSPE:{np.round(rmspe,2)}%\")\n        \n","53a293e4":"dt, endogTrain, exogTrain, endogTest, exogTest = Model.preWithZero(104)","caf3441c":"dt, endogTrain, exogTrain, endogTest, exogTest = Model.preWithoutZero(104)\nModel.statTests(endogTrain['Sales'])","4b2d5dd8":"\nplot_acf(endogTrain['Sales']);\nplot_pacf(endogTrain['Sales']);","2a3bc473":"seasonal_decompose(endogTrain.Sales, period=15).plot();","0ce479b7":"seasonal_decompose(endogTrain.Sales, period=313).plot();","6008dcbb":"p = endogTrain.tail(1).iloc[0][0]\npredNaive = np.repeat(p, len(endogTest))\nplt.plot(dt[-50:], endogTest['Sales'][-50:], c='g')\nplt.plot(dt[-50:], predNaive[-50:], c='r')\nplt.legend(labelcolor=['green', 'red'], labels=['Actual', 'Forecast'])","7c2f2d1e":"Model.metrics(predNaive)","a73880ce":"modelHW = ExponentialSmoothing(endogTrain['Sales'], \n                               seasonal_periods=7, \n                               trend='add', seasonal='add',\n                              initialization_method=\"heuristic\",).fit()\n# modelHW.summary()\npredHW = modelHW.forecast(len(exogTest))","496731cb":"plt.plot(dt[-len(endogTest):], endogTest['Sales'], c='g')\nplt.plot(dt[-len(endogTest):], predHW, c='r')\nplt.legend(labelcolor=['green', 'red'], labels=['Actual', 'Forecast'])","edbe3fd9":"Model.metrics(predHW)","4e0a573c":"# modelSarimax = SARIMAX(endog=endogTrain['Sales'], exog=exogTrain, order=(4,0,4),\n#                       seasonal_order=(4,0,4,7)).fit(disp=False)\n# modelSarimax.plot_diagnostics();","b0d8435f":"# preds = Model.plotPred(modelSarimax)","22487b9d":"# Model.metrics(preds)","cefe82f7":"# df = optimizeVAR(endog, exog)  #### takes close to 20 mins","a961e6cd":"warnings.filterwarnings('ignore')\nmodel = VARMAX(endogTrain, exogTrain, order=(4,4), enforce_stationarity=True).fit(disp=False) ### Not feasble one \nmodel.plot_diagnostics();","742306b6":"preds = Model.plotPred(model)","2f141a00":"Model.metrics(preds['Sales'])","ac7f8105":"# stores   = list(store.Store.unique())\n# allPreds = test[['Id', 'Store', 'Date']]\n# allPreds['pred'] = 0\n# dt = list(test.Date.unique())\n# dt.sort()\n# pdf = pd.DataFrame(columns=['Date', 'Store', 'preds'])\n\n# for st in tqdm_notebook(stores):\n    \n#     _, endogTrain, _, _, _ = Model.preWithZero(st)\n    \n#     modelHW = ExponentialSmoothing(endogTrain['Sales'], \n#                                seasonal_periods=7, \n#                                trend='add', seasonal='add',\n#                               initialization_method=\"heuristic\").fit()\n\n#     predHW = modelHW.forecast(48)\n    \n#     ndf = pd.DataFrame(columns=['Date', 'Store', 'preds'])\n#     ndf['Date']  = dt\n#     ndf['preds'] = predHW.values\n#     ndf['Store'] = st\n    \n#     pdf = pdf.append(ndf)\n    \n# subDF = pd.merge(test,pdf,on=['Date','Store'],how='left')\n# subDF['preds'] = np.where(subDF['Open']==0,0,subDF['preds'])    \n    ","6acfeccc":"from fbprophet import Prophet\ndef prophetDataProces(nb):\n    \n    wa = pd.DataFrame()\n    d = test.Date.unique()\n    d.sort()\n    wa['ds'] = d\n    \n    dat = train[train['Store']==nb]\n    dat = dat.sort_values(by=\"Date\").reset_index(drop=True)\n    dat = dat[['Date','Sales']]\n    dat.columns = ['ds','y']\n    return dat,wa\n","1c11df40":"pdf = pd.DataFrame(columns=['Date', 'Store', 'preds'])\nstores   = list(store.Store.unique())\n\nfor st in tqdm_notebook(stores):\n    \n    data, wa = prophetDataProces(st)\n    \n    model = Prophet()\n    model.fit(data)\n    \n    fcast = model.predict(wa)\n    fcast['Store'] = st\n    fcast = fcast[['ds', 'Store', 'yhat']]\n    fcast.columns = ['Date', 'Store', 'preds']\n    pdf = pdf.append(fcast)\n    \nsubDF = pd.merge(test,pdf,on=['Date','Store'],how='left')\nsubDF['preds'] = np.where(subDF['Open']==0,0,subDF['preds']) ","c9a86793":"# import warnings\n# warnings.filterwarnings('ignore')\n# from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n# from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae\n# import tensorflow as tf","a240aea1":"# ###### Processing data -- Univaraite ##########\n\n# def window(arr, window, horizon):\n    \n#     idx = np.expand_dims(np.arange(window+horizon), axis=0) + \\\n#                     np.expand_dims(np.arange(len(arr)-(window+horizon-1)), axis=0).T\n#     arrWindowed = arr[idx]  \n#     return arrWindowed[:,:-1], arrWindowed[:,-1:]\n\n# dfProcessed = df.drop(['Promo2','PromoInterval','CompetitionSince', 'promo2Since', 'Assortment'], axis=1).sort_values(by='Date')\n\n# df_train = dfProcessed[dfProcessed['Date']<pd.to_datetime('2015-07-01')]\n# df_test  =  dfProcessed[dfProcessed['Date']>pd.to_datetime('2015-06-30')]\n# colsToScale = ['Sales', 'Customers', 'CompetitionDistance']\n# # labelEncode = ['StoreType']\n# dfProcessed = df.drop(['Promo2','PromoInterval','CompetitionSince', 'promo2Since', 'Assortment'], axis=1)\n\n# encoder = LabelEncoder()\n# encoder.fit(df_train['StoreType'])\n# scaler = MinMaxScaler()\n# scaler.fit(df_train[colsToScale])\n\n# df_train[colsToScale] = scaler.fit_transform(df_train[colsToScale])\n# df_test[colsToScale]  = scaler.fit_transform(df_test[colsToScale])\n\n# df_train['StoreType'] = encoder.fit_transform(df_train['StoreType'])\n# df_test['StoreType'] = encoder.fit_transform(df_test['StoreType'])\n\n# trainWindows, trainLabels = window(np.asarray(df_train['Sales']), 7, 1)\n# testWindows, testLabels = window(np.asarray(df_test['Sales']), 7, 1)","58a57a77":"# ### Multivariate \n\n# dfProcessed = df.drop(['Customers','Promo2','PromoInterval','CompetitionSince', 'promo2Since', 'Assortment'],\n#                       axis=1).sort_values(by='Date')\n# dft = dft.drop([ 'Promo2','PromoInterval','CompetitionSince', 'promo2Since', 'Assortment'], axis=1).set_index('Date')\n# # dfProcessed = df.drop(['Promo2','PromoInterval','CompetitionSince', 'promo2Since', 'Assortment'], axis=1).sort_values(by='Date')\n# dfWindowed = dfProcessed.copy()\n# dfWindowed.set_index('Date', inplace=True)\n\n\n\n# for i in range(7):\n    \n#     dfWindowed[f\"Sales{i+1}\"] = dfWindowed['Sales'].shift(periods=i+1)\n    \n# dfWindowed.dropna(inplace=True)\n# yFeat = ['Sales', 'Sales1', 'Sales2', 'Sales3', 'Sales4', 'Sales5', 'Sales6', 'Sales7']\n# y = dfWindowed[yFeat]\n# X = dfWindowed.drop(yFeat, axis=1)\n\n# X_train = X[X.index<pd.to_datetime('2015-07-01')]\n# X_test  =  X[X.index>pd.to_datetime('2015-06-30')]\n# y_train = y[y.index<pd.to_datetime('2015-07-01')]\n# y_test  =  y[y.index>pd.to_datetime('2015-06-30')]\n\n# # minMaxed = ['CompetitionDistance']\n# # minMaxed = ['Customers', 'CompetitionDistance', 'Sales1', 'Sales2', 'Sales3', 'Sales4', 'Sales5', 'Sales6', 'Sales7']\n\n# encoder = LabelEncoder()\n# encoder.fit(X_train['StoreType'])\n# X_train['StoreType'] = encoder.transform(X_train['StoreType'])\n# X_test['StoreType'] = encoder.transform(X_test['StoreType'])\n# dft['StoreType'] = encoder.transform(dft['StoreType'])\n\n\n# scaler = MinMaxScaler()\n# scaler.fit(np.asarray(X_train['CompetitionDistance']).reshape(-1,1))\n# X_train['CompetitionDistance'] = scaler.transform(np.asarray(X_train['CompetitionDistance']).reshape(-1,1))\n# X_test['CompetitionDistance'] = scaler.transform(np.asarray(X_test['CompetitionDistance']).reshape(-1,1))\n# dft['CompetitionDistance'] = scaler.transform(np.asarray(dft['CompetitionDistance']).reshape(-1,1))\n\n# # y_train = y_train.values.reshape(-1,1)\n# scaler2 = MinMaxScaler()\n# scaler2.fit(y_train[yFeat])\n# y_train[yFeat] = scaler2.transform(y_train[yFeat])\n# y_test[yFeat] = scaler2.transform(y_test[yFeat])\n\n\n# # mx = y_train.max()\n# # mn = y_train.min()\n# # y_train = (y_train - mn) \/ (mx - mn)\n# # y_test = (y_test - mn) \/ (mx - mn)","5a2fd39a":"# model1 = tf.keras.Sequential()\n# model1.add(tf.keras.layers.Dense(64, activation='relu'))\n# model1.add(tf.keras.layers.Dense(256, activation='relu'))\n# model1.add(tf.keras.layers.Dense(256, activation='relu'))\n# model1.add(tf.keras.layers.Dense(8))\n\n# model1.compile(optimizer=tf.keras.optimizers.Adam(),\n#               loss=tf.keras.losses.mae,\n#               metrics=['mae', 'mse'])\n\n# history1 = model1.fit(X_train, y_train, validation_data=(X_test, y_test),\n#                      epochs=3, batch_size=128, verbose=1)","fa63d3cf":"# pd.DataFrame(history1.history).plot()","3a7de341":"# preds = model1.predict(X_test)\n# preds = preds[:,0]\n# y_test = y_test.values[:,0]","041bb739":"# print(f\"MSE:{mse(y_test, preds)}\")\n# f\"MSE:{mse(y_test, preds)}, MAE:{mae(y_test, preds)}\"","5541365f":"# try:\n#     e = np.finfo(float).eps\n#     rmspe = np.sqrt(np.mean(np.square((y_test-preds)\/y_test+e))) * 100\n# except ZeroDivisionError:\n#     print(\"Div by zero\")","d8086826":"# model2 = tf.keras.Sequential()\n# model2.add(tf.keras.layers.LSTM(64, return_sequences=True))\n# model2.add(tf.keras.layers.LSTM(32, return_sequences=False))\n# model2.add(tf.keras.layers.Dense(8, activation='linear'))\n\n# model2.compile(optimizer=tf.keras.optimizers.Adam(),\n#               loss=tf.keras.losses.mae,\n#               metrics=['mae', 'mse'])\n\n# history2 = model2.fit(X_train, y_train, validation_data=(X_test, y_test),\n#                      epochs=3, batch_size=128, verbose=1)","bc0a46e4":"\n\n#f\"MSE:{mse(y_test, predLSTM)}, MAE:{mae(y_test, predLSTM)}\"","458fa291":"submission = subDF[['Id','preds']]\nsubmission.rename(columns={'preds':'Sales'}, inplace=True)\n\nsubmission = submission.set_index('Id')\nsubmission.to_csv(\"sub.csv\")","f23e3020":"# ids = dft.pop('Id')\n# subPred = model1.predict(dft)\n# subPred = scaler2.inverse_transform(subPred)\n# subPred = subPred[:,0]\n# subDF = pd.DataFrame(ids.reset_index(drop=True))\n# subDF['Sales'] = subPred\n# subDF = subDF.set_index('Id')\n# subDF.to_csv(\"sub.csv\")","7fa3f4a4":"# Forecast using traditional TS models\n* VAR family method: We use this as VAR enables multivariate prediction which means we can predict Sales as well as Customers. In a relistic scenario multivariate process is what we would want.\n* ARIMA process: We can just do a univariate prediction as this is what the competition demands. We will see how we can integrate the exogenous variables in this process.","98fdf73a":"example: 1086,983,675,578","311987de":"## VARMAX model","84f72dac":"This is quite a nice finding\n\nAll the above stores were affected by the competition but the store 578 did not go through a negative effect which others went through because the competition was way too far to affect it. This effect can be verified running some more random checks and it quite an important finding.\n\n","145034a9":"Stores like 1086, 983, 675 showcase that after the competition enters the sales have gone down.\n\nThere are stores like 578 where the opposite has happened.","a93a4e23":"### Observations:\n* Most (or maybe all) stores are closed on Day7; need to check if there is\/are any store(s) which is open on Day7\n* Worth checking stores like #512 which seem to operate daily, even on holidays; it even has a consistent linear trend which most stores dont. Actually, this is a trait of b type stores.\n* Stores like #231 & #181 have had periods of 4-6 months of no business; need to check for more like this\n* Stores like #109 are examples of how majority of stores are; we must get some info by clustering based on Store type which hasn't been explored yet.\n* Problem with store #494 -- Never closed; need to check if there are more like this\n* There are a few stores such as #977 which are open even on public holidays; need to check more stores like this\n* Apart from the weekly timeframe which is quite evident from the plots, there seems to be no clear seasonality for others.","3b0d543f":"Randomized plot gives you details of a new store everytime it is executed and is very useful to get the feel of data one is dealing with.","0862d1d7":"# Submission","ef0339ce":"## Naive Benchmark","96bc6b01":"# Forecast using FB Prophet","3954a240":"\n### Get the data in the correct format & impute missing vals","57372cef":"# EDA-II\n\nThe main idea is to achieve the followings:\n* Analyze effect of store type\n* effect of assortment type\n* effect of competition presence","122e1022":"So, the pie chart provides a lot of info;\n\n1. Customer\/Store - a&c=1; b=3.3; d=0.8\n1. Sales\/Store - a,c&d=1; b=1.8;\n1. Sales\/Customer - a&c=1; b=0.6; d=1.3\n\n* a & c have similar per store per customer sales figures. Simply put every store has 1 customer for a unit of sale.\n* b looked good intially with the highest average sales but has 3 customers for 2 units of sale. The good thing about this store type is that per store revenue generation(almost 2X) is the most than any other type.\n* d looks like they could use more promotion to get more customers as they have 1 customer for 1.2 units of sale which is good value. \n\n\n\nDefinitely, this analyis is more meaningful if the product range is same across the store types.","a6c98b1b":"Another reason why dropping the 0 sales makes sense is for the fact that its 0 and not non-zero. In case, where the holiday effect leads to a different type of change such as power consumpition etc., it would be a bad idea to drop holiday rows.","1556a719":"### Forecast with Neural Network ","b12f8091":"# Conclusion of EDA\n1. Store type b are trending the entire time. In general, other store types have an upward trajectory but there are a few which are in a state of slump.\n1. Store type effects on customers & revenue is discussed and elaborated above. Based on that it seems that there are quite a lot of opportunities in store type 'b' & 'd' as they add some value in terms of attracting more number of customer per store & more sales per customer, respectively. Store type a & c are quite similar in terms of \"per customer and per store\" sales numbers and just because of the sheer volume of a type stores they have best gross numbers. \n2. Also, it seems that in b type store the product family is quite different if compared to others as the revenue per store is subsatantially more than the others.\n2. Majority of stores have Christmas holiday seasonality.\n3. Promo seems to ad a postive lift in the number of sales.\n4. Presence of competition has an interesting effect. In case, the competition is present with close quarters (lets say median or mean or withing 1st & 3rd quantile) then it results in loss of some revenue. Otherwise, the competition is practically too far away to affect in loss of some revenue (inturn, leads to increase of sales in some cases).\n1. It seems that promo 2 is pointless :(\n\n","5ec1a861":"#### Ex: Store#104","60368af3":"## SARIMAX \n\n##### Commented as it takes a lot of time","798d68a9":"# EDA-I\n\nSince there are a lot of stores and its immpossible to check each and ever store, hence its a better idea to use the power of randomization and check random stores.","da276621":"It seems quite clear that a store's sales during a promo clearly sees a surge compared to when there is no promo. Lets check promo2 also.\n","f830d279":"### Conclusion\n\nOnly exponential smoothing shows promise\nUn comment cell below to make prediction using Exp Smoothing","65c9914c":"So, it looks like b type stores on an average have much more sales as well as customers, but the surprising thing is they don't account much of the total revenue.","b4e890a6":"## Holt's Winter","0d4ac370":"#### Baseline: Simple DNN","47fabec7":"### LSTM"}}