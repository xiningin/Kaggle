{"cell_type":{"119c1ad6":"code","e8f16250":"code","02d8583c":"code","6397a5d8":"code","86b378b3":"code","4ddcf7d4":"code","15c04b16":"code","c1b09fc4":"code","62e1b7e1":"code","a938bedd":"code","74fa22cc":"code","1d1431c3":"code","adccf62c":"code","9b376883":"code","aa8c09cc":"code","f4083f2f":"code","afc1ed4d":"code","079229fd":"code","a3b2110f":"code","a8ca6772":"code","73340cb8":"code","ccb68243":"code","eb1600f8":"code","b753bbcd":"code","c62f505a":"code","294573db":"code","4d3cc8cb":"code","2260a695":"code","2b93f4d5":"code","ffe9b846":"code","d4a84b6a":"code","1a4e9859":"code","ccc339ac":"code","127880a7":"code","984948de":"code","a0eef5ae":"code","38043916":"code","2bdf3cfa":"code","eaa0347f":"code","2fecf062":"code","9806a47c":"code","75658d0c":"code","551669fb":"code","53ff0f0c":"code","1b1d58e8":"code","39d47656":"code","cd731953":"code","2bc9652e":"code","76b1c807":"code","69a7813f":"code","53b9f313":"code","5c33ab1f":"code","c0c8133a":"code","12992b86":"code","d7032767":"code","e3a18d74":"code","a4c0bc77":"markdown","00dd8592":"markdown","a87d0667":"markdown","8fb7d771":"markdown","780785d3":"markdown","f8ebaa90":"markdown","cebd079e":"markdown","93cd7e03":"markdown","05dd97db":"markdown","45100fc3":"markdown","316fa5fc":"markdown","39cd5975":"markdown","30fb23ff":"markdown","3ae571bd":"markdown","6c7be20a":"markdown","a26b4aeb":"markdown","895adc0a":"markdown","5e54f165":"markdown","7f2cd876":"markdown","7dbc443d":"markdown"},"source":{"119c1ad6":"#Loading libraries \/ methods:\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\nfrom sklearn.metrics import log_loss, classification_report, confusion_matrix\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, Input\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras import regularizers\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow_addons.layers import WeightNormalization\nfrom tensorflow_addons.optimizers import Lookahead\nfrom tqdm.keras import TqdmCallback\n\nsns.set()\n%matplotlib inline","e8f16250":"#Loading datasets (we'll ignore the non-scored dataset):\n\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')","02d8583c":"#Visualization of the train data:\ntrain_features.head()","6397a5d8":"#Visualization of the test data:\ntest_features.head()","86b378b3":"#Visualization of the target data:\ntrain_targets.head()","4ddcf7d4":"#Dimension of the datasets:\n\nprint('Number of rows in training set: ', train_features.shape[0])\nprint('Number of columns in training set: ', train_features.shape[1])\nprint('Number of rows in test set: ', test_features.shape[0])\nprint('Number of columns in test set: ', test_features.shape[1])\nprint('Number of rows in target set: ',train_targets.shape[0])\nprint('Number of columns in target set: ',train_targets.shape[1])\n\n#The dataset is split into the following setting: 85% into the training \n#set (23814), 15% into the test set (3982).","15c04b16":"#There are 772 gene expression features (g-0 to g-771):\ntrain_features.iloc[:, 4:776].head()","c1b09fc4":"#There are 100 cell viability features (c-0 to c-99):\ntrain_features.iloc[:, 776:876].head()","62e1b7e1":"#Gene expression:\ntrain_gene = pd.DataFrame(train_features.iloc[:, 4:776])\n\n#Cell Viability:\ntrain_cell = pd.DataFrame(train_features.iloc[:, 776:876])\n\n#Gene expression and Cell Viability:\ntrain_gc = pd.DataFrame(train_features.iloc[:, 4:876])\ntrain_gc.head(5)","a938bedd":"#Data preprocessing (test_features)\n\n\n#Converting cp_type and cp_dose as binary:\ndef Preprocess(data):\n    data = data.copy()\n    data.loc[:, 'cp_type'] = data.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    data.loc[:, 'cp_dose'] = data.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del data['sig_id']\n    return data\n\n#train = Preprocess(train_features)\ntest = Preprocess(test_features)\n\n#Delete the sig_id column:\n#del train['sig_id']\n\n#train_targets = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\n#train = train.loc[train['cp_type']==0].reset_index(drop=True)\n","74fa22cc":"#Loading csv files\nx_train_orig = pd.read_csv('\/kaggle\/input\/orig-data\/orig_data\/x_train_orig.csv')\ny_train_orig = pd.read_csv('\/kaggle\/input\/orig-data\/orig_data\/y_train_orig.csv')\nx_test_orig = pd.read_csv('\/kaggle\/input\/orig-data\/orig_data\/x_test_orig.csv')\ny_test_orig = pd.read_csv('\/kaggle\/input\/orig-data\/orig_data\/y_test_orig.csv')\n\nx_train_orig = np.asmatrix(x_train_orig)\ny_train_orig = np.asmatrix(y_train_orig)\nx_test_orig = np.asmatrix(x_test_orig)\ny_tes_orig = np.asmatrix(y_test_orig)\n\nprint(x_train_orig.shape, y_train_orig.shape, x_test_orig.shape, y_test_orig.shape)","1d1431c3":"#Loading csv files\nx_train_wv = pd.read_csv('\/kaggle\/input\/reddata\/red_data\/x_train_wv.csv')\ny_train_wv = pd.read_csv('\/kaggle\/input\/reddata\/red_data\/y_train_wv.csv')\nx_test_wv = pd.read_csv('\/kaggle\/input\/reddata\/red_data\/x_test_wv.csv')\ny_test_wv = pd.read_csv('\/kaggle\/input\/reddata\/red_data\/y_test_wv.csv')\n\nx_train_wv = np.asmatrix(x_train_wv)\ny_train_wv = np.asmatrix(y_train_wv)\nx_test_wv = np.asmatrix(x_test_wv)\ny_test_wv= np.asmatrix(y_test_wv)\n\nprint(x_train_wv.shape, y_train_wv.shape, x_test_wv.shape, y_test_wv.shape)","adccf62c":"#Loading csv files\nx_train_pca = pd.read_csv('\/kaggle\/input\/pca-data\/pca_data\/x_train_pca.csv')\ny_train_pca = pd.read_csv('\/kaggle\/input\/pca-data\/pca_data\/y_train_pca.csv')\nx_test_pca = pd.read_csv('\/kaggle\/input\/pca-data\/pca_data\/x_test_pca.csv')\ny_test_pca = pd.read_csv('\/kaggle\/input\/pca-data\/pca_data\/y_test_pca.csv')\n\nx_train_pca = np.asmatrix(x_train_pca)\ny_train_pca = np.asmatrix(y_train_pca)\nx_test_pca = np.asmatrix(x_test_pca)\ny_test_pca = np.asmatrix(y_test_pca)\n\nprint(x_train_pca.shape, y_train_pca.shape, x_test_pca.shape, y_test_pca.shape)","9b376883":"#Loading csv files\nx_train_pca_wv = pd.read_csv('\/kaggle\/input\/pca-wv-data\/pca_wv_data\/x_train_pca_wv.csv')\ny_train_pca_wv = pd.read_csv('\/kaggle\/input\/pca-wv-data\/pca_wv_data\/y_train_pca_wv.csv')\nx_test_pca_wv = pd.read_csv('\/kaggle\/input\/pca-wv-data\/pca_wv_data\/x_test_pca_wv.csv')\ny_test_pca_wv = pd.read_csv('\/kaggle\/input\/pca-wv-data\/pca_wv_data\/y_test_pca_wv.csv')\n\nx_train_pca_wv = np.asmatrix(x_train_pca_wv)\ny_train_pca_wv = np.asmatrix(y_train_pca_wv)\nx_test_pca_wv = np.asmatrix(x_test_pca_wv)\ny_tes_pca_wv = np.asmatrix(y_test_pca_wv)\n\nprint(x_train_pca_wv.shape, y_train_pca_wv.shape, x_test_pca_wv.shape, y_test_pca_wv.shape)","aa8c09cc":"#Standardization\n\nscaler=StandardScaler()\nscaler.fit(train_gc)\n\nscaled_data=scaler.transform(train_gc)","f4083f2f":"#PCA: Gene expression & Cell vialbility\n\n#n_components=0.95: Keep features explaining at least 95% of the data.\n\npca_gc = PCA(n_components=10) #the first ten PC\npca_gc.fit(scaled_data)\n\nprint(pca_gc.explained_variance_ratio_)","afc1ed4d":"gc_pca = pca_gc.transform(scaled_data)\nprint(scaled_data.shape, gc_pca.shape)","079229fd":"#Scree plot:\nper_var = np.round(pca_gc.explained_variance_ratio_* 100, decimals=1)\nlabels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n \nplt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\nplt.ylabel('Percentage of Explained Variance')\nplt.xlabel('Principal Component')\nplt.title('Scree Plot')\nplt.show()","a3b2110f":"#Plotting the 521 features w.r.t. CP1 and CP2:\n\nplt.figure(figsize=(8,6))\nplt.scatter(gc_pca[:,0],gc_pca[:,1])\nplt.xlabel('First principle component')\nplt.ylabel('Second principle component')","a8ca6772":"#Building the Logistic Regression model (neural network without hidden layers):\n\ndef LR_Model(inputsize):\n    classifier = Sequential()\n    classifier.add(Dense(206, activation=\"sigmoid\", \n                              kernel_regularizer=regularizers.l1(0),\n                              input_dim=inputsize))\n    \n    classifier.compile(optimizer='adam', loss = 'binary_crossentropy', \n                       metrics = ['accuracy'])\n    \n    return classifier","73340cb8":"LR_Model_orig = LR_Model(875)\n\nhistory_lr_orig = LR_Model_orig.fit(x_train_orig, \n                              y_train_orig,\n                              validation_split=0.3,\n                              epochs=300, \n                              batch_size=2300,\n                              verbose=0,\n                              callbacks=[TqdmCallback()])","ccb68243":"#Training \/ Validation Loss (original data)\n\ntrg_loss_orig = history_lr_orig.history['loss']\nval_loss_orig = history_lr_orig.history['val_loss']\nepochs = range(1, 301)\n\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_orig, 'b',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_orig, 'r',  linewidth=3, label='Validation Loss')\nplt.title(\"Training \/ Validation Loss - LR model (original data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","eb1600f8":"LR_Model_orig.evaluate(x_test_orig, y_test_orig)\nmin(history_lr_orig.history[\"val_loss\"])","b753bbcd":"LR_Model_red = LR_Model(875)\n\nhistory_lr_red = LR_Model_red.fit(x_train_wv, \n                              y_train_wv,\n                              validation_split=0.3,\n                              epochs=300, \n                              batch_size=2300,\n                              verbose=0,\n                              callbacks=[TqdmCallback()])","c62f505a":"#Training \/ Validation Loss (reduced data)\n\ntrg_loss_red = history_lr_red.history['loss']\nval_loss_red = history_lr_red.history['val_loss']\nepochs = range(1, 301)\n\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_red, 'k',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_red, 'y',  linewidth=3, label='Validation Loss')\nplt.title(\"Training \/ Validation Loss - LR model (reduced data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","294573db":"LR_Model_red.evaluate(x_test_wv, y_test_wv)\nmin(history_lr_red.history[\"val_loss\"])","4d3cc8cb":"LR_Model_pca = LR_Model(490)\n\nhistory_lr_pca = LR_Model_pca.fit(x_train_pca, \n                              y_train_pca,\n                              validation_split=0.3,\n                              epochs=300, \n                              batch_size=2300,\n                              verbose=0,\n                              callbacks=[TqdmCallback()])","2260a695":"#Training \/ Validation Loss (PCA data)\n\ntrg_loss_pca = history_lr_pca.history['loss']\nval_loss_pca = history_lr_pca.history['val_loss']\nepochs = range(1, 301)\n\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_pca, 'g',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_pca, 'm',  linewidth=3, label='Validation Loss')\nplt.title(\"Training \/ Validation Loss - LR model (PCA data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","2b93f4d5":"LR_Model_pca.evaluate(x_test_pca, y_test_pca)\nmin(history_lr_pca.history[\"val_loss\"])","ffe9b846":"LR_Model_pca_wv = LR_Model(490)\n\nhistory_lr_pca_wv = LR_Model_pca.fit(x_train_pca_wv, \n                              y_train_pca_wv,\n                              validation_split=0.3,\n                              epochs=300, \n                              batch_size=2300,\n                              verbose=0,\n                              callbacks=[TqdmCallback()])","d4a84b6a":"#Training \/ Validation Loss (reduced PCA data)\n\ntrg_loss_pca_wv = history_lr_pca_wv.history['loss']\nval_loss_pca_wv = history_lr_pca_wv.history['val_loss']\nepochs = range(1, 301)\n\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_pca_wv, 'c',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_pca_wv, 'darkgreen',  linewidth=3, label='Validation Loss')\nplt.title(\"Training \/ Validation Loss - LR model (reduced PCA data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","1a4e9859":"LR_Model_pca_wv.evaluate(x_test_pca_wv, y_test_pca_wv)\nmin(history_lr_pca_wv.history[\"val_loss\"])","ccc339ac":"#Validation Loss (original, reduced, PCA, and reduced PCA data)   \n\nfig = plt.figure(figsize=(20, 8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, val_loss_orig, 'r',  linewidth=3, label='Validation Loss (original)')\nplt.plot(epochs, val_loss_red, 'y',  linewidth=3, label='Validation Loss (reduced)')\nplt.plot(epochs, val_loss_pca, 'm',  linewidth=3, label='Validation Loss (PCA)')\nplt.plot(epochs, val_loss_pca_wv, 'darkgreen',  linewidth=3, label='Validation Loss (reduced PCA)')\nplt.title(\"Validation Loss - LR model (original, reduced, PCA, and reduced PCA data)  \")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","127880a7":"#Hyperparameter optimization (tuning)\n#Neural network model: \n\n'''\ndef NeuralNet(optimizer, kernel_initializer, activation, neurons, drop_rate):\n    classifier = Sequential()\n    classifier.add(Dense(units = neurons, activation=activation,\n                        kernel_initializer=kernel_initializer, input_dim=875))\n    classifier.add(BatchNormalization())\n    classifier.add(Dropout(drop_rate))\n    classifier.add(Dense(units = neurons, activation=activation,\n                        kernel_initializer=kernel_initializer))\n    classifier.add(BatchNormalization())\n    classifier.add(Dropout(drop_rate))\n    classifier.add(Dense(units = neurons, activation=activation,\n                        kernel_initializer=kernel_initializer))\n    classifier.add(BatchNormalization())\n    classifier.add(Dense(206, activation=\"sigmoid\"))\n    classifier.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    return classifier\n\nclassifier = KerasClassifier(build_fn = NeuralNet)\n'''\n\n#Grid Search method:\n\n'''\nparameters = {'batch_size': [1000, 2300],\n              'epochs': [50, 100, 300],\n              'optimizer': ['adam', 'adamw'],\n              'kernel_initializer': ['random_uniform', 'normal'], \n              'activation': ['relu', 'leaky-relu', 'elu'],\n              'neurons': [1048, 512, 256, 128],\n              'drop_rate': [0.2, 0.3, 0.4]}\n\ngrid_search = GridSearchCV(estimator = classifier, \n                           param_grid = parameters, \n                           scoring = 'accuracy', cv = 5)\n\ngrid_search = grid_search.fit(x_train_orig, y_train_orig)\n\nbest_param = grid_search.best_params_\nbest_param\nbest_acc = grid_search.best_score_\nbest_acc\n'''","984948de":"#Hyperparameters were set by the Grid Search method (previous code):\n\nbatchsize_hyp = 2300\nepochs_hyp = 300\nhiddenlayers_hyp = 512\ndropout_hyp = 0.3\nopt_hyp = 'adam'","a0eef5ae":"#Building the network:\n\ndef DNN_Model(inputsize):\n    classifier = Sequential()\n    classifier.add(WeightNormalization(Dense(512, activation=\"relu\", input_dim=inputsize)))\n    classifier.add(BatchNormalization())\n    classifier.add(Dropout(0.3))\n    classifier.add(WeightNormalization(Dense(256, activation=\"relu\"))) \n    classifier.add(BatchNormalization())\n    classifier.add(WeightNormalization(Dense(128, activation=\"relu\"))) \n    classifier.add(BatchNormalization())\n    classifier.add(WeightNormalization(Dense(206, activation=\"sigmoid\"))) \n    \n    classifier.compile(optimizer='adam', loss = 'binary_crossentropy', \n                       metrics = ['accuracy'])\n    \n    return classifier","38043916":"DNN_Model_orig = DNN_Model(875)\n\nhistory_dnn_orig = DNN_Model_orig.fit(x_train_orig, \n                              y_train_orig,\n                              validation_split=0.3,\n                              epochs=300, \n                              batch_size=2300,\n                              verbose=0,\n                              callbacks=[TqdmCallback()])","2bdf3cfa":"#10-fold cross-validation:\n\n'''\nmodels = []\nhistory = {}\nverbosity = 0\n\nkfold = KFold(n_splits=10, shuffle=True)\nfor j, (train_idx, val_idx) in enumerate(kfold.split(x_train_orig)):\n    model = DNN_Model(875)\n    history[j] = model.fit(x_train_orig.values[train_idx], y_train_orig.values[train_idx], \n                validation_data=(x_train_orig.values[val_idx], y_train_orig.values[val_idx]), \n                           batch_size=2300, epochs=12, verbose=verbosity)\n    scores = model.evaluate(x_train_orig.values[val_idx], y_train_orig.values[val_idx], \n                            verbose=verbosity)\n    print('Fold %d: %s %.6f' % (j,model.metrics_names[0],scores))\n    models.append(model)\n'''","eaa0347f":"'''\nplt.figure(figsize=(15,7))\nfor k,v in history.items():\n    plt.plot(v.history[\"loss\"], color='#d13812', label=\"Loss Fold \"+str(k))\n    plt.plot(v.history[\"val_loss\"], color='#6dbc1e', label=\"ValLoss Fold \"+str(k))\n\nplt.xlabel('Epochs')\nplt.ylabel('Error')\nplt.title('Folds Error Compound')\nplt.legend()\nplt.show()\n'''","2fecf062":"#Training \/ Validation Loss (original data)\n\ntrg_loss_dnn_orig = history_dnn_orig.history['loss']\nval_loss_dnn_orig = history_dnn_orig.history['val_loss']\nepochs = range(1, 301)\n\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_dnn_orig, 'b',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_dnn_orig, 'r',  linewidth=3, label='Validation Loss')\nplt.title(\"Training \/ Validation Loss - DNN model (original data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","9806a47c":"DNN_Model_orig.evaluate(x_test_orig, y_test_orig)\nmin(history_dnn_orig.history[\"val_loss\"])","75658d0c":"DNN_Model_red = DNN_Model(875)\n\nhistory_dnn_red = DNN_Model_red.fit(x_train_wv, \n                              y_train_wv,\n                              validation_split=0.3,\n                              epochs=300, \n                              batch_size=2300,\n                              verbose=0,\n                              callbacks=[TqdmCallback()])","551669fb":"#Training \/ Validation Loss (reduced data)\n\ntrg_loss_dnn_red = history_dnn_red.history['loss']\nval_loss_dnn_red = history_dnn_red.history['val_loss']\nepochs = range(1, 301)\n\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_dnn_red, 'k',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_dnn_red, 'y',  linewidth=3, label='Validation Loss')\nplt.title(\"Training \/ Validation Loss - DNN model (reduced data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","53ff0f0c":"DNN_Model_red.evaluate(x_test_wv, y_test_wv)\nmin(history_dnn_red.history[\"val_loss\"])","1b1d58e8":"DNN_Model_pca = DNN_Model(490)\n\nhistory_dnn_pca = DNN_Model_pca.fit(x_train_pca, \n                              y_train_pca,\n                              validation_split=0.3,\n                              epochs=300, \n                              batch_size=2300,\n                              verbose=0,\n                              callbacks=[TqdmCallback()])","39d47656":"#Training \/ Validation Loss (PCA data)\n\ntrg_loss_dnn_pca = history_dnn_pca.history['loss']\nval_loss_dnn_pca = history_dnn_pca.history['val_loss']\nepochs = range(1, 301)\n\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_dnn_pca, 'g',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_dnn_pca, 'm',  linewidth=3, label='Validation Loss')\nplt.title(\"Training \/ Validation Loss - DNN model (PCA data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","cd731953":"DNN_Model_pca.evaluate(x_test_pca, y_test_pca)\nmin(history_dnn_pca.history[\"val_loss\"])","2bc9652e":"DNN_Model_pca_wv = DNN_Model(875)\n\nhistory_dnn_pca_wv = DNN_Model_pca_wv.fit(x_train_pca_wv, \n                              y_train_pca_wv,\n                              validation_split=0.3,\n                              epochs=300, \n                              batch_size=2300,\n                              verbose=0,\n                              callbacks=[TqdmCallback()])","76b1c807":"#Training \/ Validation Loss (original data)\n\ntrg_loss_dnn_pca_wv = history_dnn_pca_wv.history['loss']\nval_loss_dnn_pca_wv = history_dnn_pca_wv.history['val_loss']\nepochs = range(1, 301)\n\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_dnn_pca_wv, 'c',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_dnn_pca_wv, 'darkgreen',  linewidth=3, label='Validation Loss')\nplt.title(\"Training \/ Validation Loss - DNN model (original data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","69a7813f":"DNN_Model_pca_wv.evaluate(x_test_pca_wv, y_test_pca_wv)\nmin(history_dnn_pca_wv.history[\"val_loss\"])","53b9f313":"#Validation Loss (original, reduced, PCA, and reduced PCA data)   \n\nfig = plt.figure(figsize=(20, 8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, val_loss_dnn_orig, 'r',  linewidth=3, label='Validation Loss (original)')\nplt.plot(epochs, val_loss_dnn_red, 'y',  linewidth=3, label='Validation Loss (reduced)')\nplt.plot(epochs, val_loss_dnn_pca, 'm',  linewidth=3, label='Validation Loss (PCA)')\nplt.plot(epochs, val_loss_dnn_pca_wv, 'darkgreen',  linewidth=3, label='Validation Loss (reduced PCA)')\nplt.title(\"Validation Loss - DNN model (original, reduced, PCA, and reduced PCA data)  \")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","5c33ab1f":"#Validation Loss - DNN vs LR model (origina and PCA data)   \n\nfig = plt.figure(figsize=(20, 8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, val_loss_orig, 'r',  linewidth=3, label='Validation Loss (LR - original)')\nplt.plot(epochs, val_loss_pca, 'y',  linewidth=3, label='Validation Loss (LR - PCA)')\nplt.plot(epochs, val_loss_dnn_orig, 'm',  linewidth=3, label='Validation Loss (DNN - original)')\nplt.plot(epochs, val_loss_dnn_pca, 'darkgreen',  linewidth=3, label='Validation Loss (DNN - PCA)')\nplt.title(\"Validation Loss - DNN vs LR model (original and PCA data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","c0c8133a":"sample_submit = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\nsample_submit.head()","12992b86":"submit_pred = DNN_Model_orig.predict(test)","d7032767":"sample_submit.iloc[:,1:] = submit_pred\nsample_submit.head()","e3a18d74":"sample_submit.to_csv(\"submission.csv\", index=False, header=True) ","a4c0bc77":"### 4.4. Predictions for the PCA data","00dd8592":"<div id=\"intro\"><\/div>\n\n## 1. Introduction\n\nThe [Connectivity Map](https:\/\/clue.io\/), a project within the Broad Institute of MIT and [Harvard, the Laboratory for Innovation Science at Harvard (LISH)](https:\/\/lish.harvard.edu\/), and the NIH Common Funds Library of Integrated Network-Based Cellular Signatures (LINCS), present this challenge with the goal of advancing drug development through improvements to MoA prediction algorithms.\n\n\n<b>What is the Mechanism of Action (MoA) of a drug? And why is it important?<\/b>\n\nIn the past, scientists derived drugs from natural products or were inspired by traditional remedies. Very common drugs, such as paracetamol, known in the US as acetaminophen, were put into clinical use decades before the biological mechanisms driving their pharmacological activities were understood. Today, with the advent of more powerful technologies, drug discovery has changed from the serendipitous approaches of the past to a more targeted model based on an understanding of the underlying biological mechanism of a disease. In this new framework, scientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activity of a given molecule, scientists assign a label referred to as mechanism-of-action or MoA for short.\n\n<br>\n\n<div id=\"datadec\"><\/div>\n\n### 1.1. Data description\n\n<code>train_features.csv<\/code>: Features for the training set. Features g- signify gene expression data, and c- signify cell viability data. cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low).\n\n<code>cp_type (categorical)<\/code>: Samples treated with a compound or with a control perturbation. Categories include \"trt_cp\" and \"ctl_vehicle\", respectively. There is no MoA for \"ctl_vehicle\".\n\n<code>cp_time (categorical)<\/code>: Treatment duration in hours. Categories include 24, 48, and 72 hours.\n\n<code>cp_dose (categorical)<\/code>: Drug dose. Categories include \"D1\", \"D2\" for low and high dose, respectively.\n\n<code>g-[0-771] (continous)<\/code>: Gene expression data - a measure of activation in a given gene after the drug is applied.\n\n<code>c-[0-99] (continous)<\/code>: Cell viability. Basically count of live cells after the drug is applied.\n    \n\n<code>train_targets_scored.csv<\/code>: The binary MoA targets that are scored. There are 206 MoA targets.\n\n<code>test_features.csv<\/code>: Features for the test data. We must predict the probability of each scored MoA for each row in the test data.\n    \n\n<hr>","a87d0667":"### 5.4. Predictions for the PCA data","8fb7d771":"<div id=\"pca\"><\/div>\n\n## 3. Dimensionality Reduction via PCA","780785d3":"<div id=\"data\"><\/div>\n\n## 2. Data overview and data preprocessing","f8ebaa90":"### 5.6. Comparing the results","cebd079e":"<div id=\"#hyper\"><\/div>\n\n### 5.1. Hyperparameter optimization","93cd7e03":"<div id=\"lr\"><\/div>\n\n## 4. A Logistic Regression Model (neural networks without hidden layers)","05dd97db":"### 4.3. Predictions for the reduced data","45100fc3":"### 4.6. Comparing the results","316fa5fc":"# Mechanisms of Action Prediction\n\n<br>\n\n### Contents\n\n* <a href=\"#intro\">1. Introduction<\/a>\n   * <a href=\"#datadec\">1.1. Data description<\/a>\n- <a href=\"#data\">2. Data overview and data preprocessing<\/a>\n- <a href=\"#pca\">3. Dimensionality reduction via PCA<\/a>\n- <a href=\"#lr\">4. A Logistic Regression Model (neural networks without hidden layers)<\/a>\n   - <a href=\"#hyper-lr\">4.1. Lambda optimization<\/a>\n   - <a href=\"#train-original-lr\">4.2. Predictions for the original data<\/a>\n   - <a href=\"#train-red-lr\">4.3. Predictions for the reduced data<\/a>\n   - <a href=\"#train-pca-lr\">4.4. Predictions for the PCA data<\/a>\n   - <a href=\"#train-pca_wv-lr\">4.5. Predictions for the reduced PCA data\n   - <a href=\"#comp-lr\">4.6. Comparing the results<\/a>\n- <a href=\"#dnn\">5. Multi-label classification via Deep Neural Networks<\/a>\n   - <a href=\"#hyper\">5.1. Hyperparamter optimization<\/a>\n   - <a href=\"#train-original\">5.2. Predictions for the original data<\/a>\n   - <a href=\"#train-red\">5.3. Predictions for the reduced data<\/a>\n   - <a href=\"#train-pca\">5.4. Predictions for the PCA data<\/a>\n   - <a href=\"#train-pca\">5.5. Predictions for the reduced PCA data<\/a>\n   - <a href=\"#comp\">5.6. Comparing the results<\/a>\n   - <a href=\"#comp_models\">5.7. Comparing the models<\/a>\n- <a href=\"#submit\">6. Submission<\/a>","39cd5975":"### 5.5. Predictions for the reduced PCA data","30fb23ff":"<div id=\"dnn\"><\/div>\n\n## 5. Multi-label classification via Deep Neural Networks","3ae571bd":"### 4.1. Lambda optimization\n\nhttps:\/\/www.kaggle.com\/heitorbaldo\/moa-prediction-pca-dnn-r","6c7be20a":"<div id=\"#train-original\"><\/div>\n\n### 5.2. Predictions for the original data","a26b4aeb":"### 5.7. Comparing the models","895adc0a":"### 4.2. Predictions for the original data","5e54f165":"### 5.3. Predictions for the reduced data","7f2cd876":"<div id=\"#submit\"><\/div>\n\n## 6. Submission","7dbc443d":"### 4.5. Predictions for the reduced PCA data"}}