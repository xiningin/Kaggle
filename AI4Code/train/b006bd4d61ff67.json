{"cell_type":{"f904dce2":"code","c2ffd743":"code","f6e191a3":"code","352b0ed9":"code","9ccffb2c":"code","736f1df7":"code","94c712ff":"code","abe03eab":"code","d60d991c":"code","15da1b4a":"code","4b6af6c1":"code","e2a139bb":"code","432c48f0":"code","92670cd8":"code","7d84edfb":"code","0f8c7c78":"code","756d4b46":"code","d015fea1":"code","152132d8":"code","18b81055":"code","7061b3bc":"code","9ae8f623":"code","ad6e3c38":"code","403a9411":"code","3fcfd475":"code","3ea87cb8":"code","82550881":"code","c1553cc3":"code","c8880d95":"code","5ad7273f":"code","0fe17f82":"code","96e3f2e5":"code","75233dd4":"code","1b13793e":"code","f07e2fa3":"code","2815388e":"code","29e8fe2e":"code","6aa19b94":"code","22612bc7":"code","47e58dd7":"code","e6be8624":"code","ba183eba":"code","7fc4c82d":"code","5e7d1721":"code","3893b67b":"markdown","7d5c6461":"markdown","43c2ea76":"markdown","e9ee5068":"markdown","4fa661f1":"markdown","5e761e6d":"markdown","6432ccad":"markdown","19ced5c9":"markdown","96098336":"markdown","fc9e80d9":"markdown","0f12d8f8":"markdown","0e09e612":"markdown","59e864f3":"markdown","16a6f986":"markdown","0282b16d":"markdown","03b07ac7":"markdown","8362547d":"markdown","d217939a":"markdown","a567e2b0":"markdown","357fb1ca":"markdown","daca65e7":"markdown"},"source":{"f904dce2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c2ffd743":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf","f6e191a3":"college = pd.read_csv('..\/input\/forbes-americas-top-colleges-2019\/ForbesAmericasTopColleges2019.csv')","352b0ed9":"college.head()","9ccffb2c":"college.shape","736f1df7":"college.nunique()","94c712ff":"clg = college.drop(columns = ['Name','City','State','Website'])","abe03eab":"clg.shape","d60d991c":"clg.isnull().sum()","15da1b4a":"clg['Net Price'] = clg['Net Price'].interpolate() \nclg['Average Grant Aid'] = clg['Average Grant Aid'].interpolate()\nclg['Alumni Salary'] = clg['Alumni Salary'].interpolate()\nclg['Acceptance Rate'] = clg['Acceptance Rate'].interpolate()\nclg['SAT Lower']= clg['SAT Lower'].interpolate()\nclg['SAT Upper']=clg['SAT Upper'].interpolate()\nclg['ACT Lower']=clg['ACT Lower'].interpolate()\nclg['ACT Upper']=clg['ACT Upper'].interpolate()","4b6af6c1":"clg.isnull().sum()","e2a139bb":"sns.relplot(data=clg, x=\"Rank\", y=\"Undergraduate Population\", hue=\"Acceptance Rate\", col=\"Public\/Private\")","432c48f0":"sns.lmplot(data=clg, x=\"Total Annual Cost\", y=\"Undergraduate Population\", col=\"Public\/Private\", hue = 'Public\/Private')","92670cd8":"sns.stripplot(x='Public\/Private', y='Alumni Salary', alpha = 0.5, data=clg)","7d84edfb":"sns.lineplot(data = clg, x='Total Annual Cost', y = 'Average Grant Aid', hue = 'Public\/Private')","0f8c7c78":"from sklearn.preprocessing import LabelBinarizer","756d4b46":"lb = LabelBinarizer()","d015fea1":"clgs = lb.fit_transform(clg['Public\/Private'])","152132d8":"clgs","18b81055":"x = clg.iloc[:,2:].values","7061b3bc":"y = clgs","9ae8f623":"from sklearn.model_selection import train_test_split","ad6e3c38":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)","403a9411":"from sklearn.preprocessing import StandardScaler","3fcfd475":"sc = StandardScaler()","3ea87cb8":"x_train_1 = sc.fit_transform(x_train)","82550881":"x_test_1 = sc.fit_transform(x_test)","c1553cc3":"ann = tf.keras.models.Sequential()","c8880d95":"ann.add(tf.keras.layers.Dense(units = 4, activation = 'relu'))","5ad7273f":"ann.add(tf.keras.layers.Dense(units = 4, activation = 'relu'))","0fe17f82":"ann.add(tf.keras.layers.Dense(units = 4, activation = 'relu'))","96e3f2e5":"ann.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))","75233dd4":"ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","1b13793e":"ann.fit(x_train_1, y_train, batch_size = 28, epochs = 65)","f07e2fa3":"y_pred = ann.predict(x_test_1)","2815388e":"y_pred.shape","29e8fe2e":"y_pred = (y_pred > 0.5)","6aa19b94":"print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.reshape(len(y_test),1)),1))","22612bc7":"from sklearn.metrics import confusion_matrix, accuracy_score","47e58dd7":"cm = confusion_matrix(y_test, y_pred)","e6be8624":"cm","ba183eba":"accuracy_score(y_test, y_pred)","7fc4c82d":"ann.weights","5e7d1721":"plt.scatter(y_test, y_pred, color = 'black')\nplt.plot(x_test_1, y_test, color = 'blue', linewidth=3, alpha = 0.2)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","3893b67b":"# **Creating activation model**","7d5c6461":"# **Weights in model(ann)**","43c2ea76":"# **Scaling x_train and x_test using Standard Scaler**","e9ee5068":"# **3rd hidden layer**","4fa661f1":"# **Visualization**","5e761e6d":"# **Output layer**","6432ccad":"# **Handling null values with interpolation**","19ced5c9":"# **Splitting data into Independent variable (x) Dependent variable (y)**","96098336":"# **Using label binarizer**","fc9e80d9":"# **Importing dataset and analysing it**","0f12d8f8":"# **2nd hidden layer**","0e09e612":"# **Splitting data into training and testing sets**","59e864f3":"# **1st hidden layer**","16a6f986":"# **Dropping unwanted columns**","0282b16d":"# **Checking for null values**","03b07ac7":"# **Defining optimizer and loss function**","8362547d":"# **Plotting graph based on our prediction**","d217939a":"# **Training our model**","a567e2b0":"# **Importing required libraries**","357fb1ca":"# **Checking Accuracy**","daca65e7":"# **Prediction on testing data**"}}