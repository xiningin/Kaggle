{"cell_type":{"c0bf7e50":"code","2d16bde3":"code","32d13ea3":"code","38a3265a":"code","4db3da94":"code","261df34e":"code","960af8de":"code","8d7acc59":"code","7121921f":"code","b85fc1c0":"code","6b70ad5a":"code","788fce10":"code","dc389ff9":"code","ac6ee355":"code","b3438531":"code","c9153b7b":"code","bca49b39":"code","b8d3b6ae":"code","ca2d8f3d":"code","5bb74f38":"code","9b6c6e24":"code","31312d46":"code","33702ac5":"code","449d5af5":"code","ee2f0a44":"code","7061d4df":"code","00420265":"code","eff2b7a9":"code","53116430":"code","505c6a9c":"code","2c4397f8":"code","5b0c232d":"code","02413bee":"code","25c8b6d0":"code","9163e706":"code","f63b76dd":"markdown","11b49a87":"markdown","cca5ba97":"markdown","2ac1a7e6":"markdown","53969099":"markdown","901994c7":"markdown","b5959171":"markdown","50bc15fa":"markdown","531d16a6":"markdown","39c255ed":"markdown","11708202":"markdown","bd03ccec":"markdown","fc42df29":"markdown","6b221886":"markdown","ca40db70":"markdown","f612eeee":"markdown","0eac905c":"markdown","c442c7de":"markdown","e0346c76":"markdown","ee560b22":"markdown","94b37384":"markdown","15c9e071":"markdown","76314161":"markdown","eff4b407":"markdown","ec98cea1":"markdown"},"source":{"c0bf7e50":"import numpy as np                               \nimport pandas as pd                              \nimport matplotlib.pyplot as plt                  \nimport seaborn as sns                            \nfrom dateutil.relativedelta import relativedelta \nfrom scipy.optimize import minimize              \nimport statsmodels.formula.api as smf            \nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\nfrom itertools import product                    \nfrom tqdm import tqdm_notebook\n\nfrom statsmodels.tsa.arima_model import ARMA\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_process import ArmaProcess\nfrom statsmodels.tsa.arima_model import ARIMA\nimport math\n\n%matplotlib inline","2d16bde3":"## Evaluation Metrics\n\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","32d13ea3":"data = pd.read_csv(\"..\/input\/sandp500\/all_stocks_5yr.csv\")","38a3265a":"df = data.copy()[data.Name=='AAPL'][['date','close']]","4db3da94":"df['date_dt'] = pd.to_datetime(df.date)","261df34e":"df.info()","960af8de":"# set the datetime column as the index\ndf.set_index('date_dt', inplace=True)","8d7acc59":"# datetimeindex with daily frequency\ndti_df = pd.date_range(start='7\/1\/20', end='7\/8\/20')\n\ndti_df","7121921f":"# Creating a datetimeindex with monthly frequency\ndti_mf = pd.date_range(start='1\/1\/20', end='7\/1\/20', freq='M')\n\ndti_mf","b85fc1c0":"df[\"close\"].asfreq('D').plot(legend=True)\nlagged_plot = df[\"close\"].asfreq('D').shift(90).plot(legend=True)\nlagged_plot.legend(['Apple Inc.','Apple Inc. Lagged'])","6b70ad5a":"# Downsampling (from daily to monthly; aggregation method: mean)\n\ndf['close'].resample('M').mean()","788fce10":"# Upsampling (from monthly mean to daily frequency: In this case, filling daily values of the same month with the mean value for that month)\n\nmonthly = df['close'].resample('M').mean()\nmonthly.resample('D').pad()","dc389ff9":"# Percent Change from previous value (Stock price today \/ stock price yesterday)\ndf['ch'] = df.close.div(df.close.shift())\ndf['ch'].plot(figsize=(13,5))","ac6ee355":"# Returns\ndf.close.pct_change().mul(100).plot(figsize=(15,6))","b3438531":"# Absolute change in price\ndf.close.diff().plot(figsize=(12,5))","c9153b7b":"# Using the rolling( ) function\nrolling = df.close.rolling('200D').mean()\ndf.close.plot()\nrolling.plot()\nplt.legend(['Close Price','Rolling Close Price Average'])","bca49b39":"# using the expanding( ) function\n\nexpanding_mean = df.close.expanding(90).mean() # average of 90 previous values and itself\nexpanding_std = df.close.expanding(90).std() # std of 90 previous values and itself\n\ndf.close.plot()\nexpanding_mean.plot()\nexpanding_std.plot()\nplt.legend(['Close Price','Expanding Mean','Expanding Standard Deviation'])","b8d3b6ae":"# Calculate average of last n obervations Using self-made function\n\ndef moving_average(series, n):\n    \"\"\"\n        Calculate average of last n observations\n    \"\"\"\n    return np.average(series[-n:])\n\nmoving_average(df.close, 90)","ca2d8f3d":"# Filter only Mondays\nmondays = df[df.index.dayofweek.isin([0])]","5bb74f38":"mondays.head(10)","9b6c6e24":"df['index'] = df.index","31312d46":"df[\"Year\"] = df[\"index\"].dt.year\ndf[\"Month\"] = df[\"index\"].dt.month\ndf[\"Day\"] = df[\"index\"].dt.day\ndf[\"Hour\"] = df[\"index\"].dt.hour\ndf[\"Minute\"] = df[\"index\"].dt.minute\ndf[\"Second\"] = df[\"index\"].dt.second\ndf[\"Nanosecond\"] = df[\"index\"].dt.nanosecond\ndf[\"Date\"] = df[\"index\"].dt.date\ndf[\"Time\"] = df[\"index\"].dt.time\ndf[\"Time_Time_Zone\"] = df[\"index\"].dt.timetz\ndf[\"Day_Of_Year\"] = df[\"index\"].dt.dayofyear\ndf[\"Week_Of_Year\"] = df[\"index\"].dt.weekofyear\ndf[\"Week\"] = df[\"index\"].dt.week\ndf[\"Day_Of_week\"] = df[\"index\"].dt.dayofweek\ndf[\"Week_Day\"] = df[\"index\"].dt.weekday\n# df[\"Week_Day_Name\"] = df[\"index\"].dt.weekday_name\ndf[\"Quarter\"] = df[\"index\"].dt.quarter\ndf[\"Days_In_Month\"] = df[\"index\"].dt.days_in_month\ndf[\"Is_Month_Start\"] = df[\"index\"].dt.is_month_start\ndf[\"Is_Month_End\"] = df[\"index\"].dt.is_month_end\ndf[\"Is_Quarter_Start\"] = df[\"index\"].dt.is_quarter_start\ndf[\"Is_Quarter_End\"] = df[\"index\"].dt.is_quarter_end\ndf[\"Is_Leap_Year\"] = df[\"index\"].dt.is_leap_year","33702ac5":"def plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False):\n\n    \"\"\"\n        series - dataframe with timeseries\n        window - rolling window size \n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n\n    \"\"\"\n    rolling_mean = series.rolling(window=window).mean()\n\n    plt.figure(figsize=(15,5))\n    plt.title(\"Moving average\\n window size = {}\".format(window))\n    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n\n    # Plot confidence intervals for smoothed values\n    if plot_intervals:\n        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n        deviation = np.std(series[window:] - rolling_mean[window:])\n        lower_bond = rolling_mean - (mae + scale * deviation)\n        upper_bond = rolling_mean + (mae + scale * deviation)\n        plt.plot(upper_bond, \"r--\", label=\"Upper Bond \/ Lower Bond\")\n        plt.plot(lower_bond, \"r--\")\n        \n        # Having the intervals, find abnormal values\n        if plot_anomalies:\n            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n            anomalies[series<lower_bond] = series[series<lower_bond]\n            anomalies[series>upper_bond] = series[series>upper_bond]\n            plt.plot(anomalies, \"ro\", markersize=10)\n        \n    plt.plot(series[window:], label=\"Actual values\")\n    plt.legend(loc=\"upper left\")\n    plt.grid(True)","449d5af5":"plotMovingAverage(df[['close']], 90, plot_intervals=True, scale=1.96, plot_anomalies=True)","ee2f0a44":"data['date_dt'] = pd.to_datetime(data.date)\ndata.set_index('date_dt',inplace=True)","7061d4df":"data_appl = data.copy()[data.Name == 'AAPL']","00420265":"plt.style.use('fivethirtyeight') \n%matplotlib inline\nfrom pylab import rcParams\nfrom plotly import tools\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n# OHLC chart of Feb 2018\ntrace = go.Ohlc(x=data_appl['2013-02'].index,\n                open=data_appl['2013-02'].open,\n                high=data_appl['2013-02'].high,\n                low=data_appl['2013-02'].low,\n                close=data_appl['2013-02'].close)\ndataa = [trace]\niplot(dataa, filename='simple_ohlc')","eff2b7a9":"# Candlestick chart of Feb 2018\ntrace = go.Candlestick(x=data_appl['2013-02'].index,\n                open=data_appl['2013-02'].open,\n                high=data_appl['2013-02'].high,\n                low=data_appl['2013-02'].low,\n                close=data_appl['2013-02'].close)\ndataa = [trace]\niplot(dataa, filename='simple_candlestick')","53116430":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf","505c6a9c":"plot_acf(df[\"close\"], lags=50, title=\"AutoCorrelation Plot\")","2c4397f8":"# using pandas plotting lib\npd.plotting.autocorrelation_plot(df.close)","5b0c232d":"plot_pacf(df[\"close\"],lags=50)","02413bee":"# The original non-stationary plot\ndf.close.plot(figsize=(13,6))","25c8b6d0":"sm.tsa.seasonal_decompose(df.close, period=365).plot()\nprint(\"Dickey-Fuller criterion: p=%f\" % \n      sm.tsa.stattools.adfuller(df.close)[1])","9163e706":"sm.tsa.seasonal_decompose(df['diff1'].dropna(), period=365).plot()\n\nprint(\"Dickey-Fuller criterion: p=%f\" % \n      sm.tsa.stattools.adfuller(df['diff1'].dropna())[1])","f63b76dd":"We can see that the newly created \"date_dt\" column is not comprised of string values. Instead, they are all datetime values. We can easily apply datetime operations on datatime datatypes (and that is why we often change string dates to datetime dates)","11b49a87":"A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time.\n* Strong stationarity:  is a stochastic process whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance also do not change over time.\n* Weak stationarity: is a process where mean, variance, autocorrelation are constant throughout the time\n\nStationarity is important as non-stationary series that depend on time have too many parameters to account for when modelling the time series. diff() method can easily convert a non-stationary series to a stationary series.\n","cca5ba97":"## Import Libraries","2ac1a7e6":"## .dt functions \/ operations","53969099":"## OHLC and CandleStick Charts for Stock Visualization","901994c7":"## Read in Data","b5959171":"## Resampling","50bc15fa":"## Converting Date that is in string\/object datatype to datetime datatype","531d16a6":"## Rolling\/Moving\/Running Average","39c255ed":"We will only use date and the closing price of Apple Inc. stocks for the purpose of the tutorial.","11708202":"**Credits and source from https:\/\/www.kaggle.com\/kashnitsky\/topic-9-part-1-time-series-analysis-in-python**","bd03ccec":"# References \/ Credits to \/ Thanks to","fc42df29":"## If this post was helpful, please upvote!","6b221886":"## Creating dattimeindex with certain frequencies","ca40db70":"The bars up until lag = 50 are well beyond the confidence interval (marked \/ colored in blue) which indicates autocorrelation exists and is statistically significant","f612eeee":"- Autocorrelation (ACF) measures how a series is correlated with itself at different lags.\n- Partial Autocorrelation : The partial autocorrelation function can be interpreted as a regression of the series against its past lags. The terms can be interpreted the same way as a standard linear regression, that is the contribution of a change in that particular lag while holding others constant. If all lags are either close to 1 or at least greater than the confidence interval, they are statistically significant.\n- These are for checking randomness + Used in the model identification stage for Box-Jenkins autoregressive, moving average time series models\n- Each spike that rises above or falls below the dashed lines is considered to be statistically significant. This means the spike has a value that is significantly different from zero. If a spike is significantly different from zero, that is evidence of autocorrelation. If random, such autocorrelations should be near zero for any and all time-lag separations. If non-random, then one or more of the autocorrelations will be significantly non-zero. But data that does not show significant autocorrelation can still exhibit non-randomness in other ways\n","0eac905c":"## Filter only certain days (e.g. Monday, Tuesday)","c442c7de":"## Percentage Change Calculations","e0346c76":"## Plot Moving Average (including anomalies and confidence intervals)","ee560b22":"Null hypothesis for dickey-fuller test has been rejected (p value smaller than typical threshold of 0.05) anddf[diff1] is stationary (which makes it appropriate for models like ARIMA and SARIMA)","94b37384":"## AutoCorrelation and Partial AutoCorrelation","15c9e071":"- https:\/\/www.datacamp.com\/community\/tutorials\/time-series-analysis-tutorial\n- https:\/\/machinelearningmastery.com\/time-series-forecasting-methods-in-python-cheat-sheet\/\n- https:\/\/www.kaggle.com\/rohanrao\/a-modern-time-series-tutorial\n- https:\/\/www.kaggle.com\/thebrownviking20\/everything-you-can-do-with-a-time-series\n- https:\/\/www.kaggle.com\/census\/population-time-series-data\n- https:\/\/www.kaggle.com\/kashnitsky\/topic-9-part-1-time-series-analysis-in-python","76314161":"## Creating lagged plots","eff4b407":"## Stationarity","ec98cea1":"## More to come!! (Classic Time Series Models like ARIMA, SARIMA, VAG...) Feedback and comments are welcome!"}}