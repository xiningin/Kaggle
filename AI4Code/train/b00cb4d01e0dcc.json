{"cell_type":{"8667f55f":"code","da37621a":"code","c2c00ceb":"code","1139109f":"code","1ee8f990":"code","b0c97602":"code","f7f5bc72":"code","a20cc0ab":"code","d948e6b4":"code","bccb3209":"code","f0519f00":"code","23afba26":"code","573b8fbc":"code","6826cc8c":"code","e43b21d3":"code","ac922977":"code","8a88d0f3":"code","2d432700":"code","5d0a7bd2":"code","428ca282":"code","d21f68ec":"code","a98c78d4":"code","85816105":"code","2fa22614":"code","755307bc":"code","8b6c3a8a":"code","cb9fd9b4":"code","b68c8fd5":"code","9f32d375":"code","a52e25c2":"code","f04c5e03":"code","40393fd7":"code","1b955799":"code","26ede8fc":"code","78d11ded":"code","e4a5cc15":"code","7cc7f322":"code","c21ec933":"code","252c6b03":"code","d4334762":"code","8898610d":"code","1af8b386":"code","da082762":"code","3a6d13a8":"code","8e9d5a18":"code","c52f324c":"code","7069274a":"code","5330d96a":"code","ad2bea3a":"code","b799188c":"code","8bea666b":"code","eb65edd4":"code","af79a670":"code","5775d589":"code","554b209a":"code","30f7de7f":"markdown","50e8fb28":"markdown","44c9b9a6":"markdown"},"source":{"8667f55f":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns',None)","da37621a":"import matplotlib.pyplot as plt\nimport seaborn as sns","c2c00ceb":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report","1139109f":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","1ee8f990":"import warnings\nwarnings.filterwarnings(action='ignore')","b0c97602":"#display all\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","f7f5bc72":"#  reading train data\ndata=pd.read_csv('..\/input\/bnp-paribas\/train.csv')","a20cc0ab":"data","d948e6b4":"data.info()","bccb3209":"X=data.drop('ID',axis=1)","f0519f00":"X.isna().sum()","23afba26":"X.isna().mean()","573b8fbc":"%matplotlib inline\nimport matplotlib.pyplot as plt\n# counting Null values\n# plt.yticks\nh=data.isna().sum().sort_values()\nh=h.to_frame()\nh.columns\nh.plot(kind='barh',figsize=(150,150))","6826cc8c":"data.groupby('target')['target'].count().plot(kind='barh')","e43b21d3":"df_obj_cat_cols1 = X.select_dtypes('object').columns\ndf_obj_cat_cols = X[df_obj_cat_cols1]\ndf_obj_cat_cols","ac922977":"# Find mean of each object col with isna\ndf_obj_cat_cols.isna().mean()","8a88d0f3":"#  obj cols with isna more than 40%  \nmore_nan_cols = df_obj_cat_cols.columns[df_obj_cat_cols.isnull().mean() > 0.4]\nmore_nan_cols","2d432700":"# remove v30 and v113 cols\ndf_obj_cat_cols = df_obj_cat_cols.drop(['v30', 'v113'],axis=1)\ndf_obj_cat_cols","5d0a7bd2":"#  Extract list of cat_cols where cat upto 4 :\ndg = (df_obj_cat_cols.nunique() < 5)  \ndg    # All True are cat and all False are num","428ca282":"#columns with unique cat <=4 \ncat_cols = dg[dg==True].index.tolist()\ndf_cat_cols = df_obj_cat_cols[cat_cols]\ndf_cat_cols","d21f68ec":"dummies = pd.get_dummies(df_cat_cols)\ndummies","a98c78d4":"# we now remove all obj cols and add only above df_cat_cols\nobj_cols = list(X.select_dtypes(include = 'object').columns)\nobj_cols\nlen(obj_cols)  \n\n# remove all object cols \ndf_without_obj_col = X.drop(obj_cols, axis = 1)\ndf_without_obj_col\n\n# concatenate dummies with dummies_without_obj_col\n# remove all object cols \nmerged =  pd.concat([df_without_obj_col,dummies], axis='columns')\nmerged","85816105":"merged.dtypes","2fa22614":"merged.fillna(0, inplace=True)\nmerged","755307bc":"y = merged.target\ny\nX = merged.drop('target', axis=1)\nX","8b6c3a8a":"# train test split\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7, random_state=1)\nX_train","cb9fd9b4":"y_train.value_counts()","b68c8fd5":"#scaling\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\nX_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\nX_train","9f32d375":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\n\nmodel.score(X_test, y_test)","a52e25c2":"cor = merged.corr()","f04c5e03":"def correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","40393fd7":"corr_features = correlation(merged, 0.9)\nlen(set(corr_features))","1b955799":"merged_corr = merged.drop(corr_features,axis=1)\n","26ede8fc":"import matplotlib.pyplot as plt\nplt.figure(figsize=(24,20))\ncor1 = X_train.corr()\nsns.heatmap(cor1, annot=True, cmap=plt.cm.CMRmap_r)\nplt.show()","78d11ded":"y = merged_corr.target\ny\nX = merged_corr.drop('target', axis=1)\nX","e4a5cc15":"# train test split\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7, random_state=1)\nX_train","7cc7f322":"y_train.value_counts()","c21ec933":"#scaling\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\nX_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\nX_train","252c6b03":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\n\nmodel.score(X_test, y_test)","d4334762":"from sklearn.ensemble import RandomForestClassifier\n\nregressor = RandomForestClassifier(n_estimators=200, random_state=0)\nregressor.fit(X_train, y_train)\ny_pred = regressor.predict(X_test)\n\n","8898610d":"from sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\nprint('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))","1af8b386":"from xgboost import XGBClassifier\nXGBClassifier()","da082762":"model = XGBClassifier()\nmodel.fit(X_train,y_train)\ny_predict = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, y_predict))","3a6d13a8":"clr = classification_report(y_test, y_predict, labels=[0,1], target_names=['No', 'Yes'] )\nprint(\"Classification report:\\n---------------------------------------------------\\n\", clr)","8e9d5a18":"df = merged\ndf","c52f324c":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscaler.fit(df)\n\nscaled_data=scaler.transform(df)\n\nscaled_data","7069274a":"from sklearn.decomposition import PCA\n\npca=PCA(n_components=30)\n\npca.fit(scaled_data)","5330d96a":"PCA(copy=True, iterated_power='auto', n_components=70, random_state=None,svd_solver='auto', tol=0.0, whiten=False)","ad2bea3a":"x_pca=pca.transform(scaled_data)\nscaled_data.shape\nx_pca.shape","b799188c":"x_pca_df = pd.DataFrame(x_pca, columns = ['col1','col2','col3','col4','col5','col6','col7','col8','col9','col10','col11','col12','col13','col14','col15','col16','col17','col18','col19','col20','col21','col22','col23','col24','col25','col26','col27','col28','col29','col30'])\n#,'col31','col32','col33','col34','col35','col36','col37','col38','col39','col40','col41','col42','col43','col44','col45','col46','col47','col48','col49','col50','col51','col52','col53','col54','col55','col56','col57','col58','col59','col60','col61','col62','col63','col64','col65','col66','col67','col68','col69','col70'","8bea666b":"finalDf = pd.concat([x_pca_df, df[['target']]], axis = 1)","eb65edd4":"print(pca.explained_variance_ratio_.sum())   #shows the over variance expressed by the first 35 principal components\nprint(pca.explained_variance_ratio_*100)   #shows variance expressed by each of the first 35 principal components","af79a670":"%matplotlib inline\na1=pca.explained_variance_ratio_*100\nb1=a1\nfor i in range(len(a1)):\n    if i>0:\n        b1[i]=b1[i]+b1[i-1]\n    else:\n        continue\n\n\n\nplt.plot(b1)\nplt.xlabel('Number of PCs')\nplt.ylabel('Variance explained')\nplt.title('Variance explained by Principal Component Analysis')\nplt.show()","5775d589":"# Implementing Random Forest\n\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import MinMaxScaler","554b209a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x_pca_df, df[['target']], test_size=0.3, random_state=0)\nmodel = XGBClassifier()\nmodel.fit(X_train,y_train)\ny_predict = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, y_predict))","30f7de7f":"sns.set_style('whitegrid')\n%matplotlib inline\n\n# plt.rcParams['figure.max_open_warning']=300\n\nfor col in data.columns:\n    print(col, \"\\n\", )\n    try:\n        data[col].hist()\n        plt.show()\n    except:\n        data[col].value_counts().plot(kind=\"bar\")\n        plt.show()","50e8fb28":"#### Find out how many object columns","44c9b9a6":"# Preprocessing"}}