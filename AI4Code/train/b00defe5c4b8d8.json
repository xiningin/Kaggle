{"cell_type":{"bd9c4e3d":"code","76684280":"code","ef9b0e5f":"code","4dde789a":"code","3916e5ea":"code","d0aa0a09":"code","71731a95":"code","ef9b0d4f":"code","20ec7e48":"code","9d2eae7c":"code","3a0d157b":"code","fbb33774":"code","0bf557a6":"code","f2317b49":"code","6215327f":"code","c2159ff0":"code","17da6126":"code","a1c6fd9a":"code","22eefc66":"code","c1aa3331":"code","22456407":"code","e8323a50":"code","3a45df4e":"code","b5d12e5f":"code","9eca1f00":"code","7e0c9bc3":"code","b100bc11":"code","02cb8869":"code","8c2233ad":"code","a9b6abb6":"code","5cac7c64":"code","93108f15":"code","3f6bcae7":"code","0b7be524":"code","360c0f08":"code","3fda0374":"code","43c66603":"code","75d64b87":"code","d35f08ed":"code","70442264":"code","a41a678e":"code","210d03b0":"code","3e90b91d":"code","d1802079":"code","28b89a49":"code","b16f5b12":"code","99223209":"code","2cf72294":"code","539c3243":"code","4e979156":"code","06c76390":"code","1ebf97a7":"code","d6579f0a":"markdown","75a0e085":"markdown","21c65a88":"markdown","f05dab17":"markdown","35dd8430":"markdown","0f42d31f":"markdown","029d1dbc":"markdown","3165f6b0":"markdown","f23826b5":"markdown","8d276d96":"markdown","a243f509":"markdown","186f374d":"markdown","37d6018a":"markdown","578a4064":"markdown","531091bf":"markdown","66be365e":"markdown","f8b440c3":"markdown","56b9836e":"markdown","afc41859":"markdown","96847979":"markdown","c493f8d4":"markdown","062b6d45":"markdown","5ba32f66":"markdown","bb7d306b":"markdown","553694e6":"markdown","f0f66472":"markdown","244aab27":"markdown","435bf4a1":"markdown","664c3c66":"markdown","19aaf842":"markdown","e33078ed":"markdown","17a4f825":"markdown"},"source":{"bd9c4e3d":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy import stats \nimport scipy.stats as st\n\n# !pip install pandas-profiling\nfrom pandas_profiling import ProfileReport\n\n# !pip install cufflinks\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nimport os\nimport re","76684280":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ef9b0e5f":"TRAIN_DATA_URL = \"\/kaggle\/input\/are-your-employees-burning-out\/train.csv\"\nTEST_DATA_URL = \"\/kaggle\/input\/are-your-employees-burning-out\/test.csv\"\nSAMPLE_DATA_URL = \"\/kaggle\/input\/are-your-employees-burning-out\/sample_submission.csv\"\n\ndf = pd.read_csv(TRAIN_DATA_URL)\ndf_test = pd.read_csv(TEST_DATA_URL)\nprint(df.shape)\ndf.tail()","4dde789a":"profile = ProfileReport(df, title='Pandas Profiling Report')\nprofile.to_file(\".\/BurnOut_Profiling.html\")\nprofile.to_widgets()","3916e5ea":"num_cols = [\"Designation\", \"Resource Allocation\", \"Mental Fatigue Score\", \"Burn Rate\"]\ndf[num_cols].iplot()","d0aa0a09":"df.describe()","71731a95":"df.info()","ef9b0d4f":"df.isna().sum()","20ec7e48":"df.dropna(subset = [\"Burn Rate\"], inplace=True)\nprint(df.shape)","9d2eae7c":"df = df.fillna(df.median())\nprint(\"Are there any value missing now? \"+str(df.isna().any().any()))","3a0d157b":"print(\"Numerical valued features counts:----------\", end=\"\\n\\n\")\n\nprint(df[\"Designation\"].value_counts(), end=\"\\n\\n\")\nprint(df[\"Resource Allocation\"].value_counts(), end=\"\\n\\n\")\nprint(df[\"Mental Fatigue Score\"].value_counts(), end=\"\\n\\n\")","fbb33774":"sns_plot = sns.pairplot(df, height=2.5)\nsns_plot.savefig(\"pairplot.png\")","0bf557a6":"def normalize_features(original_data):\n    fitted_data, fitted_lambda = stats.boxcox(original_data) \n    fig, ax = plt.subplots(1, 2) \n\n    # plotting the original data(non-normal) and  \n    # fitted data (normal) \n    sns.distplot(original_data, hist = False, kde = True, \n                kde_kws = {'shade': True, 'linewidth': 2},  \n                label = \"Non-Normal\", color =\"green\", ax = ax[0]) \n\n    sns.distplot(fitted_data, hist = False, kde = True, \n                kde_kws = {'shade': True, 'linewidth': 2},  \n                label = \"Normal\", color =\"green\", ax = ax[1]) \n\n    # adding legends to the subplots \n    plt.legend(loc = \"upper right\") \n\n    # rescaling the subplots \n    fig.set_figheight(5) \n    fig.set_figwidth(10)\n    return fitted_data","f2317b49":"original_data = df.drop(df[df[\"Mental Fatigue Score\"] <= 0.0].index)[\"Mental Fatigue Score\"]\nnormalize_features(original_data)","6215327f":"original_data = df.drop(df[df[\"Designation\"] <= 0.0].index)[\"Designation\"]\nnormalize_features(original_data)","c2159ff0":"original_data = df.drop(df[df[\"Resource Allocation\"] <= 0.0].index)[\"Resource Allocation\"]\nnormalize_features(original_data)","17da6126":"original_data = df.drop(df[df[\"Burn Rate\"] <= 0.0].index)[\"Burn Rate\"]\nnormalize_features(original_data)","a1c6fd9a":"def categorize_designation(data):\n    if data[\"Designation\"] <= 1.0:\n        return 0\n    if data[\"Designation\"] > 1.0 and data[\"Designation\"] <= 2.0:\n        return 1\n    if data[\"Designation\"] > 2.0 and data[\"Designation\"] <= 5.0:\n        return 2\n    return -1\n\n\ndef categorize_resource(data):\n    if data[\"Resource Allocation\"] <= 3.0:\n        return 0\n    if data[\"Resource Allocation\"] > 3.0 and data[\"Resource Allocation\"] <= 5.0:\n        return 1\n    if data[\"Resource Allocation\"] > 5.0 and data[\"Resource Allocation\"] <= 10.0:\n        return 2\n    return -1\n    \n\ndef categorize_Mental_Fatigue(data):\n    if data[\"Mental Fatigue Score\"] <= 4.0:\n        return 0\n    if data[\"Mental Fatigue Score\"] > 4.0 and data[\"Mental Fatigue Score\"] <= 5.0:\n        return 1\n    if data[\"Mental Fatigue Score\"] > 5.0 and data[\"Mental Fatigue Score\"] <= 6.0:\n        return 2\n    if data[\"Mental Fatigue Score\"] > 6.0 and data[\"Mental Fatigue Score\"] <= 7.0:\n        return 3\n    if data[\"Mental Fatigue Score\"] > 7.0:\n        return 4\n    return -1\n\n\n\ndf[\"categorize_designation\"] = df.apply(categorize_designation, axis=1)\ndf[\"categorize_resource\"] = df.apply(categorize_resource, axis=1)\ndf[\"categorize_Mental_Fatigue\"] = df.apply(categorize_Mental_Fatigue, axis=1)\n\ndf_test[\"categorize_designation\"] = df_test.apply(categorize_designation, axis=1)\ndf_test[\"categorize_resource\"] = df_test.apply(categorize_resource, axis=1)\ndf_test[\"categorize_Mental_Fatigue\"] = df_test.apply(categorize_Mental_Fatigue, axis=1)","22eefc66":"print(\"Cetegorized valued features values:----------\", end=\"\\n\\n\")\n\nprint(df[\"categorize_designation\"].value_counts(), end=\"\\n\\n\")\nprint(df[\"categorize_resource\"].value_counts(), end=\"\\n\\n\")\nprint(df[\"categorize_Mental_Fatigue\"].value_counts(), end=\"\\n\\n\")","c1aa3331":"current_date = pd.to_datetime('today')\n\ndf[\"Date of Joining\"] = pd.to_datetime(df[\"Date of Joining\"])\ndf_test[\"Date of Joining\"] = pd.to_datetime(df_test[\"Date of Joining\"])","22456407":"def create_days_count(data):\n    return (current_date - data[\"Date of Joining\"])\n\ndf[\"days_count\"] = df.apply(create_days_count, axis=1)\ndf[\"days_count\"] = df[\"days_count\"].dt.days\n\ndf_test[\"days_count\"] = df_test.apply(create_days_count, axis=1)\ndf_test[\"days_count\"] = df_test[\"days_count\"].dt.days","e8323a50":"print(df[\"Gender\"].value_counts(), end=\"\\n\\n\")\nprint(df[\"Company Type\"].value_counts(), end=\"\\n\\n\")\nprint(df[\"WFH Setup Available\"].value_counts(), end=\"\\n\\n\")","3a45df4e":"one = 1\nzero = 0\n\ndef gender_encoder(data):\n    if data[\"Gender\"] == \"Female\":\n        return one\n    return zero\n\n\ndef wfh_setup_encoder(data):\n    if data[\"WFH Setup Available\"] == \"Yes\":\n        return one\n    return zero\n\n\ndef company_encoder(data):\n    if data[\"Company Type\"] == \"Service\":\n        return one\n    return zero\n\n\n\ndf[\"Gender\"] = df.apply(gender_encoder, axis=1)\ndf[\"WFH Setup Available\"] = df.apply(wfh_setup_encoder, axis=1)\ndf[\"Company Type\"] = df.apply(company_encoder, axis=1)\n\ndf_test[\"Gender\"] = df_test.apply(gender_encoder, axis=1)\ndf_test[\"WFH Setup Available\"] = df_test.apply(wfh_setup_encoder, axis=1)\ndf_test[\"Company Type\"] = df_test.apply(company_encoder, axis=1)","b5d12e5f":"norm_cols = [\"Designation\", \"Resource Allocation\", \"Mental Fatigue Score\"]\n#              + [\"days_count\", \"categorize_designation\", \"categorize_resource\", \"categorize_Mental_Fatigue\"]\n\ntrain_df_min = df[norm_cols].min()\ntrain_df_max = df[norm_cols].max()\n\ndf[norm_cols] = (df[norm_cols] - train_df_min)\/(train_df_max - train_df_min)\ndf_test[norm_cols] = (df_test[norm_cols] - train_df_min)\/(train_df_max - train_df_min)","9eca1f00":"df.head()","7e0c9bc3":"df.drop(['Date of Joining', \"Employee ID\"], axis=1, inplace=True)\nclean_df_test = df_test.drop(['Date of Joining', \"Employee ID\"], axis=1)","b100bc11":"df.corr()","02cb8869":"plt.figure(figsize=(16, 6))\nheatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);\nplt.savefig(\"correlation_heatmap.png\")","8c2233ad":"# df = df.loc[:, [\"WFH Setup Available\", \"Designation\", \"Resource Allocation\", \"Mental Fatigue Score\", \"Burn Rate\"]]\n# clean_df_test = df_test.loc[:, [\"WFH Setup Available\", \"Designation\", \"Resource Allocation\", \"Mental Fatigue Score\"]]","a9b6abb6":"clean_df = df.copy()\n\ndf.to_csv(\"clean_df_train.csv\", index=False)\ntrain_file_path = \".\/clean_df_train.csv\"\nnew_df = pd.read_csv(train_file_path)\n\nclean_df_test.to_csv(\"clean_df_test.csv\", index=False)\ntest_file_path = \".\/clean_df_test.csv\"\nnew_df_test = pd.read_csv(test_file_path)\n\nnew_df_test.head()","5cac7c64":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(clean_df.loc[:, clean_df.columns != \"Burn Rate\"],\n                                                    clean_df.loc[:, clean_df.columns == \"Burn Rate\"],\n                                                    test_size=0.2, \n                                                    random_state=42)","93108f15":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor\nfrom sklearn.neural_network import MLPRegressor\n\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport xgboost\n\n\nfrom sklearn.metrics import r2_score","3f6bcae7":"def print_r2_score(y_train, train_pred, y_test, test_pred):\n    r2_train = r2_score(y_train, train_pred)\n    print(\"Score LR Train: \"+str(round(100*r2_train, 4))+\" %\")\n\n    r2_test = r2_score(y_test, test_pred)\n    print(\"Score LR Test: \"+str(round(100*r2_test, 4))+\" %\")","0b7be524":"sub = pd.read_csv(TEST_DATA_URL)\nsub = sub.loc[:, [\"Employee ID\"]]","360c0f08":"lr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\n\ntrain_pred_linear = lr_model.predict(X_train)\ntest_pred_linear = lr_model.predict(X_test)\nprint_r2_score(y_train, train_pred_linear, y_test, test_pred_linear)\n\nlr_main_pred = lr_model.predict(clean_df_test)\n\nsub[\"Burn Rate\"] = lr_main_pred\nsub.to_csv('submission_lr.csv', index=False)","3fda0374":"ridge_model = Ridge()\nridge_model.fit(X_train, y_train)\n\ntrain_pred_ridge = ridge_model.predict(X_train)\ntest_pred_ridge = ridge_model.predict(X_test)\nprint_r2_score(y_train, train_pred_ridge, y_test, test_pred_ridge)\n\nridge_main_pred = ridge_model.predict(clean_df_test)\n\nsub[\"Burn Rate\"] = ridge_main_pred\nsub.to_csv('submission_lasso.csv', index=False)","43c66603":"lasso_model = Lasso(alpha=0.1)\nlasso_model.fit(X_train, y_train)\n\ntrain_pred_lasso = lasso_model.predict(X_train)\ntest_pred_lasso = lasso_model.predict(X_test)\nprint_r2_score(y_train, train_pred_lasso, y_test, test_pred_lasso)\n\nlasso_main_pred = lasso_model.predict(clean_df_test)\n\nsub[\"Burn Rate\"] = ridge_main_pred\nsub.to_csv('submission_ridge.csv', index=False)","75d64b87":"elastic_model = ElasticNet()\nelastic_model.fit(X_train, y_train)\n\ntrain_pred_elastic = elastic_model.predict(X_train)\ntest_pred_elastic = elastic_model.predict(X_test)\nprint_r2_score(y_train, train_pred_elastic, y_test, test_pred_elastic)\n\nelastic_main_pred = elastic_model.predict(clean_df_test)\n\nsub[\"Burn Rate\"] = elastic_main_pred\nsub.to_csv('submission_elastic.csv', index=False)","d35f08ed":"svr_model = SVR(C=1, gamma=1e-6)\nsvr_model.fit(X_train, y_train)\n\ntrain_pred_svr = svr_model.predict(X_train)\ntest_pred_svr = svr_model.predict(X_test)\nprint_r2_score(y_train, train_pred_svr, y_test, test_pred_svr)\n\nsvr_main_pred = svr_model.predict(clean_df_test)\n\nsub[\"Burn Rate\"] = svr_main_pred\nsub.to_csv('submission_svr.csv', index=False)","70442264":"rf_model = RandomForestRegressor()\nrf_model.fit(X_train, y_train)\n\ntrain_pred_rf = rf_model.predict(X_train)\ntest_pred_rf = rf_model.predict(X_test)\nprint_r2_score(y_train, train_pred_rf, y_test, test_pred_rf)\n\nrf_main_pred = rf_model.predict(clean_df_test)\n\nsub[\"Burn Rate\"] = rf_main_pred\nsub.to_csv('submission_rf.csv', index=False)","a41a678e":"params = {  \n    \"n_estimators\": range(1, 500, 50),\n    \"max_depth\": range(1, 20, 2),\n    \"learning_rate\": st.uniform(0.1, 0.9)     \n}\n\nxgbreg = xgboost.XGBRegressor(nthread=-1, objective='reg:squarederror', seed=42)  \ngs = RandomizedSearchCV(xgbreg,params,n_jobs=-1, n_iter=15, cv=10, verbose=3, random_state=42)  \ngs.fit(X_train, y_train) \nrf_best_params = gs.best_params_\nprint(rf_best_params, end=\"\\n\\n\")\n\nlr_main_pred = gs.predict(clean_df_test)\n\n# \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\nxgb_model = xgboost.XGBRegressor(\n    n_estimators=rf_best_params[\"n_estimators\"] , \n    max_depth=rf_best_params[\"max_depth\"] , \n    learning_rate=rf_best_params[\"learning_rate\"])\n\nxgb_model.fit(X_train, y_train)\n\ntrain_pred_xgb = xgb_model.predict(X_train)\ntest_pred_xgb = xgb_model.predict(X_test)\nprint_r2_score(y_train, train_pred_xgb, y_test, test_pred_xgb)\n\nxgb_main_pred = xgb_model.predict(clean_df_test)\n \nsub[\"Burn Rate\"] = lr_main_pred\nsub.to_csv('submission_xgb.csv', index=False)","210d03b0":"abr_model = AdaBoostRegressor() \nabr_model.fit(X_train, y_train)\n\ntrain_pred_abr = abr_model.predict(X_train)\ntest_pred_abr = abr_model.predict(X_test)\nprint_r2_score(y_train, train_pred_abr, y_test, test_pred_abr)\n\nabr_main_pred = abr_model.predict(clean_df_test)\n\nsub[\"Burn Rate\"] = abr_main_pred\nsub.to_csv('submission_abr.csv', index=False)","3e90b91d":"cat_model = CatBoostRegressor()\ncat_model.fit(X_train, y_train)\n\ntrain_pred_cat = cat_model.predict(X_train)\ntest_pred_cat = cat_model.predict(X_test)\nprint_r2_score(y_train, train_pred_cat, y_test, test_pred_cat)\n\ncat_main_pred = cat_model.predict(clean_df_test)\n\nsub[\"Burn Rate\"] = cat_main_pred\nsub.to_csv('submission_cat.csv', index=False)","d1802079":"gbr_model = GradientBoostingRegressor() \ngbr_model.fit(X_train, y_train)\n\ntrain_pred_gbr = gbr_model.predict(X_train)\ntest_pred_gbr = gbr_model.predict(X_test)\nprint_r2_score(y_train, train_pred_gbr, y_test, test_pred_gbr)\n\ngbr_main_pred = gbr_model.predict(clean_df_test)\n\nsub[\"Burn Rate\"] = gbr_main_pred\nsub.to_csv('submission_gbr.csv', index=False)","28b89a49":"mlp_model = MLPRegressor(random_state=42) \nmlp_model.fit(X_train, y_train)\n\ntrain_pred_mlp = mlp_model.predict(X_train)\ntest_pred_mlp = mlp_model.predict(X_test)\nprint_r2_score(y_train, train_pred_mlp, y_test, test_pred_mlp)\n\nmlp_main_pred = mlp_model.predict(clean_df_test)\n\nsub[\"Burn Rate\"] = mlp_main_pred\nsub.to_csv('submission_mlp.csv', index=False)","b16f5b12":"estimators = [('lr', LinearRegression()),\n              ('ridge', Ridge()), \n              ('rf', RandomForestRegressor()),\n              ('xgb', xgboost.XGBRegressor(nthread=-1, learning_rate=0.1185260448662222, max_depth=3, n_estimators=351)),\n              ('mlp', MLPRegressor()),\n              ('ada', AdaBoostRegressor()),\n              ('gbr', GradientBoostingRegressor()),\n              ('cat', CatBoostRegressor())]\n\n\nstacking_model = StackingRegressor(estimators=estimators, final_estimator=GradientBoostingRegressor(random_state=42))\nstacking_model.fit(X_train, y_train)\n\ntrain_pred_stacking = stacking_model.predict(X_train)\ntest_pred_stacking = stacking_model.predict(X_test)\nprint_r2_score(y_train, train_pred_stacking, y_test, test_pred_stacking)\n\nstacking_main_pred = stacking_model.predict(clean_df_test)\n\nsub[\"Burn Rate\"] = stacking_main_pred\nsub.to_csv('submission_stacking.csv', index=False)","99223209":"# !pip install tensor-dash\n\n# from tensordash.tensordash import Tensordash\n# histories = Tensordash(\n#     ModelName = 'burnout-1')","2cf72294":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline","539c3243":"model = Sequential()\nmodel.add(Dense(4, input_dim=10, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(2670, activation='relu'))\nmodel.add(Dense(1, activation='linear'))\nmodel.summary()","4e979156":"model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\nhistory=model.fit(clean_df.loc[:, clean_df.columns != \"Burn Rate\"], \n                  clean_df.loc[:, clean_df.columns == \"Burn Rate\"], \n                  epochs=100, \n                  batch_size=150, \n                  verbose=1, \n                  validation_split=0.08)\n\nneural_main_pred = model.predict(clean_df_test)","06c76390":"sub[\"Burn Rate\"] = neural_main_pred\nsub.to_csv('submission_neural.csv', index=False)","1ebf97a7":"print(history.history.keys())\n# \"Loss\"\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","d6579f0a":"## Ridge","75a0e085":"## AdaBoostRegressor","21c65a88":"## StackingRegressor","f05dab17":"<center><h2>Project Under Development<\/h2><\/center>\n<img src=\"https:\/\/cdn1.iconfinder.com\/data\/icons\/construction-220\/64\/43-512.png\" width=100 height=100>\n<center><h4>I hope it was helpful!!<\/h4><\/center>\n","35dd8430":"## Importing Libraries","0f42d31f":"## GradientBoostingRegressor","029d1dbc":"## Date of Joining","3165f6b0":"## CatBoostRegressor","f23826b5":"## Checking Data Normality","8d276d96":"## Encoding Features","a243f509":"## Elastic net","186f374d":"## Categorize features","37d6018a":"# Getting and Understanding Data","578a4064":"## Understanding Data","531091bf":"## XGB (with tuning)","66be365e":"# <center style=\"background-color:#63809e; color:white;\">Employee Burn Rate Prediction<\/center>\n\n<center><img src=\"https:\/\/smallville.com.au\/wp-content\/uploads\/2019\/12\/10-Questions-To-Ask-Yourself-To-Monitor-Your-Mental-HealthAsset-1@4x-100.jpg\" ><\/center>\n\n<br><br>\n## <center style=\"background-color:#6abada; color:white;\">About<\/center>\n<div style=\"text-align: justify;\">Understanding what will be the Burn Rate for the employee working in an organization based on the current pandemic situation where work from home is a boon and a bane. How are employees' Burn Rate affected based on various conditions provided? Through this notebook, we are going to understand and observe the mental health of all the employees for a company with the dataset provided. So, we need to predict the burn-out rate of employees based on the provided features thus helping the company to take appropriate measures for their employees' health and keep measures to improve their throughput.<\/div> \n<br>\n\n\n<div style=\"text-align: justify;\">Globally, World Mental Health Day is celebrated on <b>October 10<\/b> each year. The objective of this day is to raise awareness about mental health issues around the world and mobilize efforts in support of mental health. According to an anonymous survey, about <b>450 million<\/b> people live with mental disorders that can be one of the primary causes of poor health and disability worldwide. These days when the world is suffering from a pandemic situation, it becomes really hard to maintain mental fitness.\n <\/div>","f8b440c3":"## <center style=\"background-color:#6abada; color:white;\">Featues in our Data<\/center>\n\n* `Employee ID`: The unique ID allocated for each employee (example: **fffe390032003000**)\n* `Date of Joining`: The date time when the employee has joined the organization (example: **2008-12-30**)\n* `Gender`: The gender of the employee (**Male\/Female**) \n* `Company Type`: The type of company where the employee is working (**Service\/Product**)\n* `WFH Setup Available`: Is the work from home facility available for the employee (**Yes\/No**)\n* `Designation`: The designation of the employee of work in the organization.\n    * In the range of **[0.0, 5.0]** bigger is higher designation.\n* `Resource Allocation`: The amount of resource allocated to the employee to work, ie. number of working hours. \n    * In the range of **[1.0, 10.0]** (higher means more resource)\t\n* `Mental Fatigue Score`: The level of fatigue mentally the employee is facing. \n    * In the range of **[0.0, 10.0]** where 0.0 means no fatigue and 10.0 means completely fatigue.\n* `Burn Rate`: The value we need to predict for each employee telling the rate of Bur out while working.\n    * In the range of **[0.0, 1.0]** where the higher the value is more is the burn out.","56b9836e":"## SVR","afc41859":"# Feature Engineering","96847979":"## Dealing with missing values","c493f8d4":"# Understand Correlation","062b6d45":"# Model Training and Predicitons\n\n<center><img src=\"https:\/\/media.giphy.com\/media\/JstFYY8FwlBm48n7De\/giphy.gif\" width=70%><\/center>","5ba32f66":"# Exploratory Data Analysis","bb7d306b":"## Random Forest Regression","553694e6":"## Linear Regression","f0f66472":"## Normalize Data","244aab27":"## Profiling Data ","435bf4a1":"## MLPRegressor","664c3c66":"## Removing useless columns","19aaf842":"## Lasso","e33078ed":"## Getting Data","17a4f825":"## Working with clean data"}}