{"cell_type":{"cc434454":"code","402eb2a5":"code","b3923002":"code","871cdf62":"code","e12c1b60":"code","12e8ac2a":"code","416e3249":"code","cae3551b":"code","0393449a":"code","c6b406b8":"code","fd19445a":"code","51f38433":"code","0cdb187d":"code","d73b16ab":"code","1c855d72":"code","c58b5c90":"code","8f08aa44":"code","2f99fc37":"code","dff55173":"code","f156b727":"code","8aefc76a":"code","b119784a":"code","c62fae7b":"code","2ff71043":"code","d452f712":"markdown","e26e9593":"markdown","b1be7e88":"markdown","1ab94001":"markdown","08732ef1":"markdown","56ffd9de":"markdown","483860eb":"markdown","0c72c33c":"markdown","74638348":"markdown","e469ec80":"markdown","3698554e":"markdown","313c5057":"markdown","b4f7dd3e":"markdown","f619b6ad":"markdown"},"source":{"cc434454":"import numpy as np \nimport pandas as pd \nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","402eb2a5":"df = pd.read_csv('..\/input\/diabetes-dataset\/diabetes.csv')","b3923002":"df.head()","871cdf62":"corr = df.corr()\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)","e12c1b60":"corr[abs(corr['Outcome']) > 0.2]['Outcome']","12e8ac2a":"corr[abs(corr['Outcome']) > 0.1]['Outcome']","416e3249":"small_df=df[['Pregnancies', 'Glucose', 'BMI', 'Age']]\nmed_df=df[['Pregnancies', 'Glucose', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']]\nlarge_df = df.drop(columns=['Outcome'])","cae3551b":"xs = small_df\nxm = med_df\nxl = large_df\ny = df['Outcome']","0393449a":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nlr=LogisticRegression(max_iter=10000)","c6b406b8":"x_train,x_test,y_train,y_test=train_test_split(xs,y,random_state=1,test_size=0.2)\nlr.fit(x_train,y_train)\np1=lr.predict(x_test)\ns1=accuracy_score(y_test,p1)\nprint(\"Small DF Linear Regression Success Rate :\", \"{:.2f}%\".format(100*s1))","fd19445a":"x_train,x_test,y_train,y_test=train_test_split(xm,y,random_state=1,test_size=0.2)\nlr.fit(x_train,y_train)\np1=lr.predict(x_test)\ns1=accuracy_score(y_test,p1)\nprint(\"Medium DF Linear Regression Success Rate :\", \"{:.2f}%\".format(100*s1))","51f38433":"x_train,x_test,y_train,y_test=train_test_split(xl,y,random_state=1,test_size=0.2)\nlr.fit(x_train,y_train)\np1=lr.predict(x_test)\ns1=accuracy_score(y_test,p1)\nprint(\"Large DF Linear Regression Success Rate :\", \"{:.2f}%\".format(100*s1))","0cdb187d":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()","d73b16ab":"x_train,x_test,y_train,y_test=train_test_split(xs,y,random_state=1,test_size=0.2)\ngbc.fit(x_train,y_train)\np2=gbc.predict(x_test)\ns2=accuracy_score(y_test,p2)\nprint(\"Small DF Gradient Booster Classifier Success Rate :\", \"{:.2f}%\".format(100*s2))","1c855d72":"x_train,x_test,y_train,y_test=train_test_split(xm,y,random_state=1,test_size=0.2)\ngbc.fit(x_train,y_train)\np2=gbc.predict(x_test)\ns2=accuracy_score(y_test,p2)\nprint(\"Medium DF Gradient Booster Classifier Success Rate :\", \"{:.2f}%\".format(100*s2))","c58b5c90":"x_train,x_test,y_train,y_test=train_test_split(xl,y,random_state=1,test_size=0.2)\ngbc.fit(x_train,y_train)\np2=gbc.predict(x_test)\ns2=accuracy_score(y_test,p2)\nprint(\"Large DF Gradient Booster Classifier Success Rate :\", \"{:.2f}%\".format(100*s2))","8f08aa44":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()","2f99fc37":"x_train,x_test,y_train,y_test=train_test_split(xs,y,random_state=1,test_size=0.2)\nrfc.fit(x_train,y_train)\np3=rfc.predict(x_test)\ns3=accuracy_score(y_test,p3)\nprint(\"Small DF Random Forest Classifier Success Rate :\", \"{:.2f}%\".format(100*s3))","dff55173":"x_train,x_test,y_train,y_test=train_test_split(xm,y,random_state=1,test_size=0.2)\nrfc.fit(x_train,y_train)\np3=rfc.predict(x_test)\ns3=accuracy_score(y_test,p3)\nprint(\"Medium DF Random Forest Classifier Success Rate :\", \"{:.2f}%\".format(100*s3))","f156b727":"x_train,x_test,y_train,y_test=train_test_split(xl,y,random_state=1,test_size=0.2)\nrfc.fit(x_train,y_train)\np3=rfc.predict(x_test)\ns3=accuracy_score(y_test,p3)\nprint(\"Large DF Random Forest Classifier Success Rate :\", \"{:.2f}%\".format(100*s3))","8aefc76a":"from sklearn.svm import SVC\nsvm=SVC()","b119784a":"x_train,x_test,y_train,y_test=train_test_split(xs,y,random_state=1,test_size=0.2)\nsvm.fit(x_train,y_train)\np4=svm.predict(x_test)\ns4=accuracy_score(y_test,p4)\nprint(\"Small DF Support Vector Classifier Success Rate :\", \"{:.2f}%\".format(100*s4))","c62fae7b":"x_train,x_test,y_train,y_test=train_test_split(xm,y,random_state=1,test_size=0.2)\nsvm.fit(x_train,y_train)\np4=svm.predict(x_test)\ns4=accuracy_score(y_test,p4)\nprint(\"Medium DF Support Vector Classifier Success Rate :\", \"{:.2f}%\".format(100*s4))","2ff71043":"x_train,x_test,y_train,y_test=train_test_split(xl,y,random_state=1,test_size=0.2)\nsvm.fit(x_train,y_train)\np4=svm.predict(x_test)\ns4=accuracy_score(y_test,p4)\nprint(\"Large DF Support Vector Classifier Success Rate :\", \"{:.2f}%\".format(100*s4))","d452f712":"# Initial Conclusion","e26e9593":"Following is a heatmat of correlation between various health variables and diabetes. The lighter the color, the more correlated a specific health variable is to being diabetic.","b1be7e88":"The following code will use four machine learning tools to determine if one tool is better than others in predicting diabetes in Pima Women, based on the data supplied. ","1ab94001":"## Linear Regression","08732ef1":"## Gradient Boosting","56ffd9de":"Using Support Vector Classifier, all three sets returned the same amount - 78.57% accuracy. ","483860eb":"In terms of Gradient Boosting, all of the values vary with each run, where there is no clear leader; often the large set has the highest accuracy or is tied with the small set, whereas both the small and medium sets also often return the same values. Correlation seems to not be helpful in this situation.","0c72c33c":"In terms of Linear Regression, both the small and large set return with the same or nearly the same accuracy - not terrible, but not outstanding either.","74638348":"We're going to run Linear Regression, Gradient Boosting, Random Forest Classifier, and Support Vector Machine learnings tools from Scikit on three data frames: a \"small\" data frame, which will be anything that is 20% or more correlated to diabetes, a \"medium\" data frame, which will be anything that is 10% or more correlated, and a large data frame, which is all of the available data. By running all three, we can see if, in the case of this data set, focusing simply on correlation is beneficial or not in determining diabetes among Pima women.","e469ec80":"# Diabetes Predictions in Pima Women","3698554e":"Based on the four machine learning tools used, using the whole dataset with a Random Forest Classifier nets the best result - usually just above 80% accuracy.\n\n80% and above isn't bad, but leaves room for improvement. ","313c5057":"## Random Forest Classifier","b4f7dd3e":"In terms of using the Random Forest Classifier, the less correlation seems to be the better. The large set netted the highest accuracy (the best of any tool).","f619b6ad":"## Support Vector Classifier "}}