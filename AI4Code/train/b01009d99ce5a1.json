{"cell_type":{"b69dc4b5":"code","8781466d":"code","f08931b1":"code","5250035e":"code","30b5938f":"code","35a1974d":"code","6af58931":"code","afd6b6c4":"code","d740c087":"code","6cb37d36":"code","482ed653":"code","b1e9d770":"code","e17f62a8":"code","ecc6eefd":"code","7527f07e":"code","471ce35b":"code","aabb1798":"code","a3639596":"code","a59e3d8a":"code","5c89b410":"code","dedeb151":"markdown","9b033c45":"markdown","dfb545f8":"markdown","d8deafcd":"markdown","1b2208a0":"markdown","a64a2cde":"markdown","6ab1be42":"markdown","a1abba6c":"markdown","dcdfe8eb":"markdown","808b3867":"markdown","3f868fb0":"markdown","140d75f0":"markdown","40ffcd63":"markdown","20ed0644":"markdown","82555af9":"markdown","bf68a7d3":"markdown","60defe0d":"markdown","b7b68fcf":"markdown","490afaa4":"markdown","32d9b076":"markdown","2b65c848":"markdown","85214a4d":"markdown"},"source":{"b69dc4b5":"import pandas as pd\nimport numpy as np\nimport tensorflow.contrib.keras as keras\nimport matplotlib.pyplot as plt\nimport os\nimport sys\nimport pickle\nimport itertools\nimport gensim\nfrom sklearn.model_selection import train_test_split\nfrom numpy import zeros\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom gensim.models import Word2Vec\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.layers import GlobalMaxPooling1D","8781466d":"# Importing dataset\nreviews_df = pd.read_csv('..\/input\/Hotel_Reviews.csv')\nprint(reviews_df.dtypes)","f08931b1":"def clean(text):\n    '''\n    '''\n    text = text.lower()\n    text = text.replace(\"ain't\", \"am not\")\n    text = text.replace(\"aren't\", \"are not\")\n    text = text.replace(\"can't\", \"cannot\")\n    text = text.replace(\"can't've\", \"cannot have\")\n    text = text.replace(\"'cause\", \"because\")\n    text = text.replace(\"could've\", \"could have\")\n    text = text.replace(\"couldn't\", \"could not\")\n    text = text.replace(\"couldn't've\", \"could not have\")\n    text = text.replace(\"should've\", \"should have\")\n    text = text.replace(\"should't\", \"should not\")\n    text = text.replace(\"should't've\", \"should not have\")\n    text = text.replace(\"would've\", \"would have\")\n    text = text.replace(\"would't\", \"would not\")\n    text = text.replace(\"would't've\", \"would not have\")\n    text = text.replace(\"didn't\", \"did not\")\n    text = text.replace(\"doesn't\", \"does not\")\n    text = text.replace(\"don't\", \"do not\")\n    text = text.replace(\"hadn't\", \"had not\")\n    text = text.replace(\"hadn't've\", \"had not have\")\n    text = text.replace(\"hasn't\", \"has not\")\n    text = text.replace(\"haven't\", \"have not\")\n    text = text.replace(\"haven't\", \"have not\")\n    text = text.replace(\"haven't\", \"have not\")\n    text = text.replace(\"haven't\", \"have not\")\n    text = text.replace(\"he'd\", \"he would\")\n    text = text.replace(\"haven't\", \"have not\")\n    text = text.replace(\"he'd've\", \"he would have\")\n    text = text.replace(\"'s\", \"\")\n    text = text.replace(\"'t\", \"\")\n    text = text.replace(\"'ve\", \"\")\n    text = text.replace(\".\", \" . \")\n    text = text.replace(\"!\", \" ! \")\n    text = text.replace(\"?\", \" ? \")\n    text = text.replace(\";\", \" ; \")\n    text = text.replace(\":\", \" : \")\n    text = text.replace(\",\", \" , \")\n    text = text.replace(\"\u00b4\", \"\")\n    text = text.replace(\"\u2018\", \"\")\n    text = text.replace(\"\u2019\", \"\")\n    text = text.replace(\"\u201c\", \"\")\n    text = text.replace(\"\u201d\", \"\")\n    text = text.replace(\"\\'\", \"\")\n    text = text.replace(\"\\\"\", \"\")\n    text = text.replace(\"-\", \"\")\n    text = text.replace(\"\u2013\", \"\")\n    text = text.replace(\"\u2014\", \"\")\n    text = text.replace(\"[\", \"\")\n    text = text.replace(\"]\",\"\")\n    text = text.replace(\"{\",\"\")\n    text = text.replace(\"}\", \"\")\n    text = text.replace(\"\/\", \"\")\n    text = text.replace(\"|\", \"\")\n    text = text.replace(\"(\", \"\")\n    text = text.replace(\")\", \"\")\n    text = text.replace(\"$\", \"\")\n    text = text.replace(\"+\", \"\")\n    text = text.replace(\"*\", \"\")\n    text = text.replace(\"%\", \"\")\n    text = text.replace(\"#\", \"\")\n    text = text.replace(\"\\n\", \" \\n \")\n    text = text.replace(\"\\n\", \"\")\n    text = text.replace(\"_\", \" _ \")\n    text = text.replace(\"_\", \"\")\n    text = ''.join([i for i in text if not i.isdigit()])\n\n    return text\n\npositive_reviews = reviews_df['Positive_Review'].values\nnegative_reviews = reviews_df['Negative_Review'].values\n\ncleaned_positive_reviews = [clean(r) for r in positive_reviews] \ncleaned_negative_reviews = [clean(r) for r in negative_reviews] \n\nreviews_df['Positive_Review'] = cleaned_positive_reviews\nreviews_df['Negative_Review'] = cleaned_negative_reviews","5250035e":"# Shuffling data\nreviews_df = reviews_df.sample(frac=1).reset_index(drop=True)\n\n# Extracting all text\npositive_reviews = reviews_df['Positive_Review'].values\nnegative_reviews = reviews_df['Negative_Review'].values\nreviews_text = []\n\nfor p,n in zip(positive_reviews, negative_reviews) : \n    if p in ['na', 'nothing', 'none', 'n a', 'no', 'no positive', 'no negative'] : \n        reviews_text.append(n)\n    elif n in ['na', 'nothing', 'none', 'n a', 'no', 'no positive', 'no negative'] : \n        reviews_text.append(p)\n    else : \n        reviews_text.append(n)\n        reviews_text.append(p)\n\n# Preprocessing training data\ntraining_df = reviews_df.loc[:1000]\npositive_reviews_filtered = training_df['Positive_Review'].values\nnegative_reviews_filtered = training_df['Negative_Review'].values\ntraining_reviews = []\nlabels = []\n\nfor idx,(p,n) in enumerate(zip(positive_reviews_filtered, negative_reviews_filtered)) : \n    if p in ['na', 'nothing', 'none', 'n a', 'no', 'no positive', 'no negative'] : \n        training_reviews.append(n)\n        labels.append(0)\n    elif n in ['na', 'nothing', 'none', 'n a', 'no', 'no positive', 'no negative'] :\n        training_reviews.append(p)\n        labels.append(1)\n    else :\n        training_reviews.append(n)\n        labels.append(0)\n        training_reviews.append(p)\n        labels.append(1)\n\n# Creating datasets\ndict1 ={\n    'reviews' : training_reviews,\n    'labels' : labels\n}\nsentiment_df = pd.DataFrame.from_dict(dict1)\n\n\ndict2 ={\n    'reviews_text' : reviews_text\n}\nreviews_text_df = pd.DataFrame.from_dict(dict2)\n","30b5938f":"text_reviews = [str(r) for r in reviews_text_df['reviews_text'].values]\n\nsentences = []\n\nfor review in text_reviews:\n    words = text_to_word_sequence(review)\n    sentences.append(words)\n\nembeddings_model = Word2Vec(sentences, min_count=1, sg=1, size=128)\nwords = list(embeddings_model.wv.vocab)\nprint('{} WORDS '.format(len(words)))\nprint('Printing first 100:')\nprint(words[:100])","35a1974d":"# Querying SQLlite database to extract needed words embeddings\ntokenizer = keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(text_reviews)\nvocabSize = len(tokenizer.word_index) + 1","6af58931":"# Recreating embeddings index based on Tokenizer vocabulary\nword2vec_vocabulary = embeddings_model.wv.vocab\nembeddingIndex = dict()\ncounter = 0\nfor word, i in tokenizer.word_index.items():\n    if word in word2vec_vocabulary :\n        embeddingIndex[word] = embeddings_model[word]\n    else:\n        counter += 1\n\nprint(\"{} words without pre-trained embedding!\".format(counter))\n    \n# Prepare embeddings matrix\nembeddingMatrix = zeros((vocabSize, 128))\nfor word, i in tokenizer.word_index.items():\n    embeddingVector = embeddingIndex.get(word)\n    if embeddingVector is not None:\n        embeddingMatrix[i] = embeddingVector","afd6b6c4":"reviews = [ str(r) for r in sentiment_df['reviews'].values]\nlabels = sentiment_df['labels'].values\n\noneHotReviews = tokenizer.texts_to_sequences(reviews)\nencodedReviews = keras.preprocessing.sequence.pad_sequences(oneHotReviews, maxlen=40, padding='post')\n\nX_train, X_test, y_train, y_test = train_test_split(encodedReviews, labels, test_size=0.33, random_state=42)","d740c087":"# define neural network\nCNN = keras.models.Sequential()\nCNN.add(keras.layers.Embedding(vocabSize, 128,weights=[embeddingMatrix], input_length=40, trainable=True))\nCNN.add(Conv1D(128, 2, activation='relu'))\nCNN.add(GlobalMaxPooling1D())\nCNN.add(Flatten())\nCNN.add(Dense(1, activation='sigmoid'))\nCNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\nCNN.fit(X_train, y_train, epochs=5, verbose=1)","6cb37d36":"loss, accuracy = CNN.evaluate(X_test, y_test, verbose=1)\nprint('Test Loss: {}'.format(loss))\nprint('Test Accuracy: {}'.format(accuracy))","482ed653":"predictions = CNN.predict_classes(X_test)\n\ncm = confusion_matrix(y_test, predictions, labels=[0,1])\ntitle = 'Confusion matrix'\ncmap = plt.cm.Blues\nclasses=[\"negative\",\"positive\"]\nplt.imshow(cm, interpolation='nearest', cmap=cmap)\nplt.title(title)\nplt.colorbar()\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\n\nfmt = '.2f'\nthresh = cm.max() \/ 2.\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j, i, format(cm[i, j], fmt),\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] > thresh else \"black\")\n\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.tight_layout()\nplt.show()","b1e9d770":"report = classification_report(y_test, predictions, target_names=['0','1'])\nprint(report)","e17f62a8":"positive_reviews = [str(r) for r in reviews_df['Positive_Review'].values]\n\nfor idx, review in enumerate(positive_reviews):\n    words = text_to_word_sequence(review)     \n    if(len(words) > 40): \n        words = words[:40]\n        positive_reviews[idx] = ' '.join(words)\n\noneHotPositiveReviews = tokenizer.texts_to_sequences(positive_reviews)\nencodedPositiveReviews = keras.preprocessing.sequence.pad_sequences(oneHotPositiveReviews, maxlen=40, padding='post')\n\n\npositivity_predictions = CNN.predict_proba(encodedPositiveReviews)\n\nplt.hist(positivity_predictions, bins=30)\nplt.xlabel('Positivity scores');\nplt.show() ","ecc6eefd":"negative_reviews = [str(r) for r in reviews_df['Negative_Review'].values]\n\nfor idx, review in enumerate(negative_reviews):\n    words = text_to_word_sequence(review)      \n    if(len(words) > 40): \n        words = words[:40]\n        negative_reviews[idx] = ' '.join(words)\n\noneHotNegativeReviews = tokenizer.texts_to_sequences(negative_reviews)\nencodedNegativeReviews = keras.preprocessing.sequence.pad_sequences(oneHotNegativeReviews, maxlen=40, padding='post')\n\n\nnegativity_predictions = CNN.predict_proba(encodedNegativeReviews)\n    \n# print(negativity_predictions)\nplt.hist(negativity_predictions, bins=30)\nplt.xlabel('Negativity scores');\nplt.show() \n","7527f07e":"negative_reviews = reviews_df['Negative_Review'].values\npositive_reviews = reviews_df['Positive_Review'].values\n\nmissing = ['na', 'nothing', 'none', 'n a', 'no', 'no positive', 'no negative']\n\nfor idx, (text,score) in enumerate(zip(positive_reviews, positivity_predictions)):\n    if text in missing : positivity_predictions[idx] = 0.0\n    elif score < 0.5 : positivity_predictions[idx] = 0.501\n\nfor idx, (text,score) in enumerate(zip(negative_reviews, negativity_predictions)):\n    if text in missing : negativity_predictions[idx] = 1.0\n    elif score > 0.5 : negativity_predictions[idx] = 0.499\n\n# Printing final distributions\nfig, axes = plt.subplots(nrows=1, ncols=2, constrained_layout=True)\naxes[0].hist(negativity_predictions, color='red', bins=30)\naxes[1].hist(positivity_predictions, color='green', bins=30)\nplt.show() ","471ce35b":"reviews_df['Negative_Review_Score'] = negativity_predictions\nreviews_df['Positive_Review_Score'] = positivity_predictions\nreviews_df['Sentiment_Review_Score'] = (positivity_predictions+negativity_predictions)\/2\n\npositive_reviews = reviews_df['Positive_Review']\nnegative_reviews = reviews_df['Negative_Review']\nsentiment_scores = reviews_df['Sentiment_Review_Score']\nreviewer_score = reviews_df['Reviewer_Score']\n\nprint('Positive review: {}'.format(reviews_df['Positive_Review'][100]))\nprint('Negative review: {}'.format(reviews_df['Negative_Review'][100]))\nprint('Sentiment score: {}'.format(reviews_df['Sentiment_Review_Score'][100]))\nprint('Reviewer score: {}'.format(reviews_df['Reviewer_Score'][100]))","aabb1798":"target = []\nfinal_scores = reviews_df['Sentiment_Review_Score'].values\n\nfor f in final_scores : \n    if f >= 0.7 : target.append(4)\n    elif f < 0.7 and f >= 0.5 : target.append(3)\n    elif f < 0.5 and f >= 0.3 : target.append(2)\n    else: target.append(1)\n\nreviews_df['Sentiment_Review_Class'] = target","a3639596":"reviews_best = reviews_df[reviews_df['Sentiment_Review_Class'] == 4]['Positive_Review'].values\nprint('Number of best reviews:  {}'.format(len(reviews_best)))\n      \nreviews_good = reviews_df[reviews_df['Sentiment_Review_Class'] == 3]['Positive_Review'].values\nprint('Number of good reviews:  {}'.format(len(reviews_good)))\n      \nreviews_bad = reviews_df[reviews_df['Sentiment_Review_Class'] == 2]['Negative_Review'].values\nprint('Number of bad reviews:   {}'.format(len(reviews_bad)))\n      \nreviews_worst = reviews_df[reviews_df['Sentiment_Review_Class'] == 1]['Negative_Review'].values\nprint('Number of worst reviews: {}'.format(len(reviews_worst)))\n\nfig, ax = plt.subplots()\nx = ['Worst','Bad', 'Good', 'Best']\ny = [len(reviews_worst),len(reviews_bad),len(reviews_good),len(reviews_best)]\nvert_bars = ax.bar(x, y, color='steelblue', align='center')\nplt.show()","a59e3d8a":"worst_hotels = reviews_df.groupby('Hotel_Name')['Sentiment_Review_Score'].mean().sort_values(ascending=False).head(10)\nworst_hotels.plot(kind=\"bar\",color=\"DarkGreen\")\n_=plt.xlabel('Best Hotels according to Reviews')\n_=plt.ylabel('Average Review Score')\nplt.show()\n\nreviews_df.groupby('Hotel_Name')['Sentiment_Review_Score'].mean().sort_values(ascending=False).head(10)","5c89b410":"worst_hotels = reviews_df.groupby('Hotel_Name')['Sentiment_Review_Score'].mean().sort_values(ascending=True).head(10)\nworst_hotels.plot(kind=\"bar\",color=\"DarkRed\")\n_=plt.xlabel('Worst Hotels according to Reviews')\n_=plt.ylabel('Average Review Score')\nplt.show()\n\nreviews_df.groupby('Hotel_Name')['Sentiment_Review_Score'].mean().sort_values(ascending=True).head(10)","dedeb151":"Calculate positive scores","9b033c45":"### <span style=\"color:steelblue\">__Extracting sentiment classes__ <\/span>\n\nWe're going to divide reviews in four classes:\n1. __best__ : final_score >= 0.7 \n2. __good__ : final_score < 0.7 AND final_score >= 0.5 \n3. __bad__ :  final_score < 0.5 AND final_score >= 0.3 \n4. __worst__ : final_score < 0.3","dfb545f8":"### <span style=\"color:steelblue\">__Importing needed dependencies__ <\/span>\n- The multiplayer perceptron will be built with __tensorflow keras__ libraries.\n- Useful function from __sklearn__ will be used to evaluate the performances.\n- Google's Word2Vec skipgram model will be extracted from __gensim libraries__.\n- __Numpy__ will be used to create the embeddings matrix of weigths out of the skipgram model.","d8deafcd":"# <span style=\"color:steelblue\">Sentiment analysis explained <\/span>\n\nThe goal is to train a multilayer perceptron on the reviews text to perform a sentiment analysis task.\nIn particular we want to be able to extract a positivity\/negativity score expressed in the range from 0 to 1.\n\n0. Extremly negative\n1. Extremly positive\n\nTo do this, we will take advantage of the Google's Word2Vec skip-gram algorithm.\nThanks to this simple model we'll pre-calculate word embeddings for the entire vocabulary used in the reviews, mapping in a vectorial space all words that appears in the same context.\n\nThe pre-trained embeddings will let the Multilayer Perceptron to be trained with very small amount of data.\n\nJust 20000 reviews out of 515000 will be used for training.","1b2208a0":"### <span style=\"color:steelblue\">__Clean text__ <\/span>","a64a2cde":"### <span style=\"color:steelblue\">__Extract truth value__<\/span>\n\nTo train the multilayer perceptron model we need to extract reviews text and assign them a truth value.\n - *\"The hotel was a disaster\"* : 0\n - *\"We had a lovely holiday\"* : 1\n\nNegative and positive reviews are already divided in the given dataset, so we just need to read data and create new columns with truth values.\n\nTwo dataset will be created:\n   1. sentiment_task_reviews, containing all reviews with truth\n   2. reviews_text containing all reviews (to train embeddings)\n   ","6ab1be42":"Importing preprocessed data in Pandas DataFrames.","a1abba6c":"### <span style=\"color:steelblue\">__Train the model__ <\/span>\n\n5 epochs of training will be enough to get an accuracy over 90%","dcdfe8eb":"Calculate negative scores","808b3867":"### <span style=\"color:steelblue\">Create a vocabulary<\/span>\n\nA __Keras Tokenizer__ will extract the vocabulary of all words that appear in the 515000 reviews.","3f868fb0":"### <span style=\"color:steelblue\">__Test the model__ <\/span>","140d75f0":"### <span style=\"color:steelblue\">__Train embeddings__ <\/span>\n\nThe __Word2Vec skip-gram__ model is a simple Neural Network that performs a *fake task* of predict the nearest words given another one that appears in a sentence. The goal of this fake task is to train the network so that we can then extract the weights matrix in which each row will be the vectorial representation of a word.\n\nWe will train the network 128 neurons in the hidden layer, so the words will have 128 dimensional representation.","40ffcd63":"<span style=\"color:MediumAquaMarine\">*__Observations__*<\/span><br\/>\n\nThis final visualization show us that while the big part of positive scores seems to tend to 1, negative once seem to be more distributed in the [0, 0.5] range. This could mean that a big part of negative reviews are not so negative in the end. Maybe some guests who leave a positive review, leave also an \"advice\" as negative one.<br\/>\n\nMore of that the number of people who leaves only a positive review is way bigger than the number of who decide to leave just a negative one.\n\nTo conclude we can add these two numerical features to our dataset.<br\/>\nSince every reviewer leaves a negative review and positive one, a __final average sentiment score__ is extracted.\n\nThis value will be used for further analysis instead of the actual reviewer score.\n\n","20ed0644":"### <span style=\"color:steelblue\">__Importing hotel reviews dataset__ <\/span>\n\nNot all types are recognized because of missing values, but we will take care of that later because the goal is train the Multilayer Perceptron just on text.","82555af9":"Let's visualize the number of reviews for each class:","bf68a7d3":"### <span style=\"color:steelblue\">__Extracting sentiment score__ <\/span>\n\nThanks to the Multilayer Perceptron we've built in the previous Notebook, we can extract a positivity\/negativity scores from text reviews.<br\/>\nThe goal is to use these values as Numerical features in subsequent analisis.","60defe0d":"### <span style=\"color:steelblue\">__Classification report__ <\/span>\n\nWe want a better visualization of classification results, that are more important than accuracy in this particular task.","b7b68fcf":"### <span style=\"color:steelblue\">__Confusion matrix__ <\/span>\n\nWe want a better visualization of classification results, that are more important than accuracy in this particular task.","490afaa4":"<span style=\"color:MediumAquaMarine\">*__Observations__*<\/span><br\/>\n\nThe first thing we must notice is that a small amount of data are misclassified, in fact some negative texts appear to have a positive score and viceversa. Since we know a priori if a review is positive or not, we can simply put these values in the middle.\n\nAnother problem we have to manage is related to possibility of a guest to give two reviews, one positive and one negative. In particular is someone leaves just one of the two we need to consider the worst case.<br\/>\nLet's make and example:\n\n- (\"The worst hotel\", \"No Positive\") --> (0.345, ?) \n\nIn this case the value of No Positive has to be zero, because if in subsequent analisis we'll consider the average score of the two reviews, this is the only way to describe a complete non-positive experience. The same concept is valid for \"No Negative\" reviews.","32d9b076":"### <span style=\"color:steelblue\">__Printing best and worst hotels according to sentiment analysis__ <\/span>","2b65c848":"### <span style=\"color:steelblue\">__Extract Embeddings matrix__ <\/span>\n\nNow that the word2vec model is trained we can combine its vectorial representation of words to create and embeddings matrix:\n- vocabularySize __x__ embeddingDim\n\nThese representation will be used to intitliaze the weights of the embeddings layer in the multilayer perceptron.","85214a4d":"### <span style=\"color:steelblue\">__Preprocess training text__ <\/span>\n\nThanks to the tokenizer we can map each word in a review to a one hot representation.<br\/>\nThen we want to pad the reviews with zeros to make them all of the same size, which will be __40 words__ because the average length is ~33.<br\/>\nIn the end we just divide in training and test sets. "}}