{"cell_type":{"166fe676":"code","a14abf98":"code","1f77be83":"code","52e2a82b":"code","e6abaaa1":"code","e89dcb27":"code","6fe03a1e":"code","68fcaec3":"code","82e461f6":"code","33e01a08":"code","ed2d3a9e":"code","c51de6e1":"code","dcf8dc43":"code","35ca1fd3":"code","168528e0":"code","0dc9db34":"code","51169e3e":"code","4189a77f":"code","18faedee":"code","39addbf4":"code","a0a1eb44":"code","7bbed0ca":"markdown","38899f5c":"markdown","0e1954d2":"markdown","d80d4dff":"markdown","9a83db3f":"markdown","20df2def":"markdown","4c4adaeb":"markdown","db4ab1fd":"markdown","15c70f28":"markdown","87c24534":"markdown","2a8eff4b":"markdown","b79e4d11":"markdown","89c1ccb9":"markdown","e5310a36":"markdown","ee494e9e":"markdown"},"source":{"166fe676":"import albumentations\nimport numpy as np\nimport pandas as pd\nimport os\nimport glob\nimport random\nfrom pprint import pprint\nfrom tqdm import tqdm\n\n# PyTorch Model\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n# Dataset Loading\nfrom PIL import Image\nfrom PIL import ImageFile\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Model Training\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","a14abf98":"# Configurations for the files\nDIR = \"..\/input\/captcha-images\/\"\nBATCH_SIZE = 16\nIMG_HEIGHT = 75\nIMG_WIDTH = 300\nEPOCHS = 150\nNUM_WORKERS = 8\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {DEVICE}\")","1f77be83":"paths = []\nlabels = []\nfor image in os.listdir(DIR):\n    paths.append(os.path.join(DIR, image))\n    labels.append(image.split(\".\")[0])\n\ndf = pd.DataFrame({\n    \"paths\": paths,\n    \"labels\": labels\n})\n\ndf.head()","52e2a82b":"def show_random_images(df, column_name):\n    f = plt.figure(figsize=(10,10))\n    i=1\n    for i in range(16):\n        i += 1\n        ax = f.add_subplot(4,4,i)\n        sample = random.choice(df[column_name])\n        image = mpimg.imread(sample)\n        ax.set_title(sample.split(\"\/\")[-1])\n        plt.imshow(image)\n","e6abaaa1":"show_random_images(df, \"paths\")","e89dcb27":"def get_loss_function(x, bs, targets):\n    log_softmax_values = F.log_softmax(x, 2)\n\n    input_lengths = torch.full(\n        size=(bs,), fill_value=log_softmax_values.size(0), dtype=torch.int32\n    )\n\n    target_lengths = torch.full(\n        size=(bs,), fill_value=targets.size(1), dtype=torch.int32\n    )\n\n    return nn.CTCLoss(blank=0)(log_softmax_values, targets, input_lengths, target_lengths)","6fe03a1e":"class MyCaptchaModel(nn.Module):\n    def __init__(self, num_chars):\n        super(MyCaptchaModel, self).__init__()\n        \n        # CNN Layer\n        self.conv1 = nn.Conv2d(3, 128, kernel_size=(3,3), padding=(1,1))\n        self.maxpool1 = nn.MaxPool2d(kernel_size=(2,2))\n        \n        self.conv2 = nn.Conv2d(128, 64, kernel_size=(3,3), padding=(1,1))\n        self.maxpool2 = nn.MaxPool2d(kernel_size=(2,2))\n        \n        # RNN Layer Preprocess\n        self.linear1 = nn.Linear(1152, 64)\n        self.drop1 = nn.Dropout(0.2)\n        \n        # LSTM GRU\n        self.gru = nn.GRU(64, 32, bidirectional=True, num_layers=2, dropout=0.25, batch_first=True)\n        self.output = nn.Linear(64, num_chars+1)\n        \n        \n    def forward(self, images, targets=None):  \n        bs, channel, height, width = images.size()\n        \n        x = F.relu(self.conv1(images))\n        x = self.maxpool1(x)\n\n        x = F.relu(self.conv2(x))\n        x = self.maxpool2(x)\n\n        x = x.permute(0, 3, 1, 2)\n        x = x.view(bs, x.size(1), -1)\n\n        x = self.linear1(x)\n        x = self.drop1(x)\n\n        x, _ = self.gru(x)\n        x = self.output(x)\n        x = x.permute(1, 0, 2)\n\n        if targets is not None:\n            loss = get_loss_function(x, bs, targets)\n            return x, loss\n        \n        return x, None","68fcaec3":"def train_function(model, data_loader, optimizer):\n    model.train()\n    fin_loss = 0\n    tk = tqdm(data_loader, total=len(data_loader))\n    for data in tk:\n        for k, v in data.items():\n            data[k] = v.to(DEVICE)\n        \n        optimizer.zero_grad()\n        _, loss = model(**data)\n        loss.backward()\n        optimizer.step()\n        fin_loss += loss.item()\n\n    return fin_loss \/ len(data_loader)\n\n\ndef eval_function(model, data_loader):\n    model.eval()\n    fin_loss = 0\n    fin_preds = []\n    with torch.no_grad(): \n        tk = tqdm(data_loader, total=len(data_loader))\n        for data in tk:\n            for k, v in data.items():\n                data[k] = v.to(DEVICE)\n            \n            batch_preds, loss = model(**data)\n            fin_loss += loss.item()\n            fin_preds.append(batch_preds)\n\n        return fin_preds, fin_loss \/ len(data_loader)","82e461f6":"def encode_targets():\n  # Load images from files\n  image_files = glob.glob(os.path.join(DIR, \"*.jpg\"))\n  image_files_png = glob.glob(os.path.join(DIR, \"*.png\"))\n  image_files.extend(image_files_png)\n  targets_orig = [x.split(\"\/\")[-1].split(\".\")[0] for x in image_files]\n  targets = [[c for c in x] for x in targets_orig]\n  targets_flat = [c for clist in targets for c in clist] # squeeze\n\n  # Encode images\n  lbl_enc = LabelEncoder()\n  lbl_enc.fit(targets_flat)\n\n  targets_enc = [lbl_enc.transform(x) for x in targets]\n  targets_enc = np.array(targets_enc) + 1 # transform to np and remove 0 index\n\n  return image_files, targets_enc, targets_orig, lbl_enc","33e01a08":"# Train-test split\nimage_files, targets_enc, targets_orig, lbl_enc = encode_targets()\n\n(train_imgs, test_imgs, train_targets, test_targets, _, test_orig_targets) = train_test_split(\n    image_files, targets_enc, targets_orig, test_size=0.1, random_state=0)","ed2d3a9e":"ImageFile.LoadTruncatedImages = True\n\nclass DatasetClassifier:\n    def __init__(self, image_paths, targets, resize=None):\n        self.image_paths = image_paths\n        self.targets = targets\n        self.resize = resize\n        self.aug = albumentations.Compose(\n            [\n             albumentations.Normalize(always_apply=True)\n            ]\n            )\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, item):\n        image = Image.open(self.image_paths[item]).convert(\"RGB\")\n        target = self.targets[item]\n        \n        if self.resize is not None:\n            image = image.resize((self.resize[1], self.resize[0]), resample=Image.BILINEAR)\n        \n        image = np.array(image)\n        augmented = self.aug(image=image)\n        image = augmented[\"image\"]\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        \n        return {\n            \"images\": torch.tensor(image, dtype=torch.float),\n            \"targets\": torch.tensor(target, dtype=torch.long)\n        }","c51de6e1":"# Classify images, load using pytorch's DataLoader\ntrain_dataset = DatasetClassifier(\n    image_paths=train_imgs, targets=train_targets, resize=(IMG_HEIGHT, IMG_WIDTH)\n)\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n    shuffle=True,\n)\n\ntest_dataset = DatasetClassifier(\n    image_paths=test_imgs, targets=test_targets, resize=(IMG_HEIGHT, IMG_WIDTH)\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n    shuffle=False\n)","dcf8dc43":"# Load the models\nmodel = MyCaptchaModel(num_chars=len(lbl_enc.classes_))\nmodel.to(DEVICE)\n\n# Create optimizer and callbacks\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, factor=0.8, patience=5, verbose=True\n)","35ca1fd3":"def early_stopping(patience, count, prev_loss, current_loss, threshold):\n    if abs(prev_loss - current_loss) < threshold and count >= patience:\n        return \"stop\" \n    elif abs(prev_loss - current_loss) < threshold:\n        return \"count\"\n    else:\n        return False","168528e0":"def decode_predictions(preds, encoder):\n    preds = preds.permute(1, 0, 2)\n    preds = torch.softmax(preds, 2)\n    preds = torch.argmax(preds, 2)\n    preds = preds.detach().cpu().numpy()\n    cap_preds = []\n    for j in range(preds.shape[0]):\n        temp = []\n        for k in preds[j,:]:\n            k = k - 1\n            if k == -1:\n                temp.append(\"-\")\n            else:\n                temp.append(encoder.inverse_transform([k])[0])\n        tp = \"\".join(temp)\n        cap_preds.append(tp)\n    return cap_preds","0dc9db34":"patience = 6\ncount = 0\nprev_train_loss = 0\nthreshold = 0.05\nloss = []\n\nfor epoch in range(EPOCHS):\n    train_loss = train_function(model, train_loader, optimizer)\n    valid_preds, valid_loss = eval_function(model, test_loader)\n    valid_cap_preds = []\n\n    for vp in valid_preds:\n        current_preds = decode_predictions(vp, lbl_enc)\n        valid_cap_preds.extend(current_preds)\n        \n    pprint(list(zip(test_orig_targets, valid_cap_preds))[15:20])\n    print(f\"Epoch: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n    \n    res = early_stopping(patience, count, prev_train_loss, train_loss, threshold)\n    \n    loss.append(train_loss)\n    \n    if res == \"stop\":\n        print(\"Early Stopping Implemented.\")\n        final_epoch = epoch\n        break\n    elif res == \"count\" and train_loss < 0.2:\n        count += 1\n        print(f\"Patience at {patience-count}\")\n    else:\n        prev_train_loss = train_loss","51169e3e":"torch.save(model.state_dict(), \".\/model.bin\")","4189a77f":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf_pytorch = pd.DataFrame({\"loss\": loss})\nplt.figure(figsize=(15,5))\nplt.grid()\nplt.xlabel(\"Epoch No.\")\nplt.ylabel(\"Loss Value\")\nplt.title(\"Loss During Epoch Training\")\nsns.lineplot(data=df_pytorch, palette=\"tab10\", linewidth=2.5)","18faedee":"def get_image(image_path=None):\n    if image_path == None:\n        img = random.choice(df[\"paths\"])\n        return [img]\n    return [image_path]\n\n\ndef get_sample_photo(image_path=None):\n    img = get_image(image_path)\n    eval_dataset = DatasetClassifier(\n        image_paths=img, targets=[np.array([x for x in np.arange(10)])], resize=(IMG_HEIGHT, IMG_WIDTH)\n    )\n\n    eval_loader = torch.utils.data.DataLoader(\n        eval_dataset,\n        batch_size=BATCH_SIZE,\n        num_workers=NUM_WORKERS,\n        shuffle=False\n    )\n    \n    return img, eval_loader\n\n\ndef predict_function(model, data_loader):\n    model.eval()\n    fin_preds = []\n    with torch.no_grad(): \n        for data in data_loader:\n            for k, v in data.items():\n                data[k] = v.to(DEVICE)\n            \n            batch_preds, _ = model(**data)\n            fin_preds.append(batch_preds)\n\n        return fin_preds","39addbf4":"image_path, eval_loader = get_sample_photo()\nprint(image_path)\n\ndef clean_decoded_predictions(unclean_predictions):\n    cleaned_predictions = []\n    for i in unclean_predictions:\n        if i != \"-\":\n            cleaned_predictions.append(i)\n            \n    cleaned_predictions = \"\".join(cleaned_predictions)\n    \n    if len(cleaned_predictions) == 10:\n        return cleaned_predictions   \n    \n    else:\n        prev = \"-\"\n        new_cleaned_predictions = []\n        for char in cleaned_predictions:\n            if char == prev:\n                continue\n            new_cleaned_predictions.append(char)\n            prev = char\n        res = \"\".join(new_cleaned_predictions)\n        return res\n    \ndef predict_captcha(model, eval_loader, image_path):\n    plt.figure(figsize=(15,5))\n    image = mpimg.imread(image_path[0])\n    target = image_path[0].split(\"\/\")[-1].split(\".\")[0]\n    plt.title(image_path[0].split(\"\/\")[-1])\n    plt.imshow(image)\n    \n    valid_preds = predict_function(model, eval_loader)\n    for vp in valid_preds:\n        current_preds = decode_predictions(vp, lbl_enc)\n    \n    preds = clean_decoded_predictions(current_preds[0])\n    \n    success = True if preds == target else False\n\n    return{\n        \"success\": success,\n        \"prediction\": preds,\n        \"real\": target\n    }","a0a1eb44":"preds = predict_captcha(model, eval_loader, image_path)\npreds","7bbed0ca":"# Parse Images & Encode Labels\n\nNow, we get the correct labels for each image. The targets are found in the images' filename, so we parse these and convert them into lists. Once everything is converted into a list, we use sklearn's train_test_split to shuffle and split our train and test sets.","38899f5c":"# Configuration Set-up\n\nWe first set up the different variables and load up the necessary libraries that would be needed. ","0e1954d2":"Next, we create the train and evaluation functions. These are basically the functions that run for each step, and output the loss of the function. The train function also implements backpropagation and adjusts the weights of the model.","d80d4dff":"# Create Model, Optimizers, Callbacks\n\nWe now load the model class we developed earlier. We use the Adam Optimizer with a learning rate of 3e-4. We also add callback functions that would adjust the learning rate when it plateaus, and implement early stopping whenever the loss doesn't decrease anymore. These callback functions prevent overfitting of the model.","9a83db3f":"From what we can see, each captcha image contains 10 letters. There isn't much distortion like a usual captcha would have, but the challenge here would be correctly parsing the each character sequentially and dealing with the blurry images.","20df2def":"# Sample Predictions\n\nThe function now samples a random image. It also tells us if the prediction is correct by comparing the predictions to the file name.","4c4adaeb":"## Running the training model\nThis process loads in everything we developed earlier. We load the data into the data_loader, and then create predictions based on the CRNN model. The CTCLoss gives out the loss, and then we adjust the weights to create better predictions in the following epoch. Once the loss plateaus, we save the model and use this for predictions.","db4ab1fd":"## Decoding the Predictions\nNext, we create a function to interpret the predictions. As the predictions are still using the label encoder from earlier, we use the inverse transform for these. If the prediction is not found in the encoder's classes, we assign it as a \"filler\" and will clean it later on.","15c70f28":"# Serving Predictions\n\nWe then use the trained model weights to create predictions. Similar to the train and evaluation functions earlier, we implement the model. However, we no longer calculate for the loss and adjust the weights. We simply decode the predictions and clean out the results.","87c24534":"# Image Analysis\nBefore we get started, let's take a look at the different captcha samples that we have. I created a function that gets a random sample from the dataset and shows it to you. To get an idea of what the captcha images look like, let's create subplots of random samples.","2a8eff4b":"## Dataset Classifier Class\n\nTo fully utilize PyTorch's features, we create a DatasetClassifier Class, and then use PyTorch's data_loader. This makes the training process smoother and makes it easier to separate the train set by the configured batch size.","b79e4d11":"# Captcha Model Predictor\nThis notebook elaborates on the process of using PyTorch to create a captcha decoder model. \n\nI also used Abhishek Thakur's video as a base for this, which [you can find here](https:\/\/www.youtube.com\/watch?v=IcLEJB2pY2Y). However, I implemented some of my own changes such as:\n\n- refactoring the code as a notebook\n- using a different dataset\n- adding early stopping\n- model performance analysis","89c1ccb9":"# Model Definition\n\nWe define our model here. The architecture is pretty simple: we use two CNN layers to get information from the images, and then use a GRU to sequentially parse each letter available. \n\nAs for the loss function, we use the CTCLoss to find the error sequentially.","e5310a36":"# Model Performance\n\nWe then create a lineplot to see the change of the loss value throughout the training process. As we can see, the loss goes down and then once it flattens we implement early stopping to prevent overfitting.","ee494e9e":"The libraries loaded here are divided into different parts. First, we have the preprocessing, formatting, and file system libraries. Next, we load the library for the PyTorch model, ImageLoaders, and some sklearn pre-processing and evaluation libraries."}}