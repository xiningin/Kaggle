{"cell_type":{"f75f96bd":"code","b3a33320":"code","4eae850f":"code","2eb0d8cf":"code","81cbd47f":"code","114e03cd":"code","c0231117":"code","7565c0ab":"code","00916510":"code","b53341eb":"code","73205583":"code","59863dbf":"code","cf0b4f12":"code","0ca609c8":"code","7235c966":"code","e6ca2b95":"code","97500325":"code","721d7e56":"code","09694cb0":"code","c38757a5":"code","33f3d98d":"code","55d4dbc8":"code","125913d9":"code","8c35829f":"code","31516a6e":"code","a3347470":"code","376a1565":"code","58a14c10":"code","af6e86f0":"code","cba4b3a3":"code","d91c4449":"code","6b62c5fa":"code","882f8874":"code","dd6b7c99":"code","a71525f4":"code","68d15fd4":"code","38c7042f":"code","2a62b7e8":"code","a0db2e14":"code","25796941":"code","f7572f80":"code","1199a2d8":"code","011e34cb":"code","cda793c8":"code","8810698c":"code","68f5ae10":"code","2653b764":"code","ad873a46":"code","ad043054":"code","c6b9651e":"code","0c6c4ada":"code","02b5fd88":"code","a6dab758":"code","e9e2bb0c":"code","d5a5dfb4":"code","5b088566":"code","1f00ff9d":"code","c02f82d9":"code","907b5392":"code","e1be1c92":"code","8308e89f":"code","b7904949":"code","cd963964":"code","916230e0":"markdown","2aca3892":"markdown","c50bf0da":"markdown","ac93f5b0":"markdown","35905852":"markdown","6170326f":"markdown","b4261837":"markdown","97b0a9e7":"markdown","f5a177b1":"markdown","95d6f80b":"markdown","95b6249b":"markdown","ddf8ddc7":"markdown","56189f82":"markdown","2fd9396d":"markdown","02b23e93":"markdown","45235bc1":"markdown","d1ac1b9b":"markdown","4b2d927c":"markdown","a10f610a":"markdown","254396a4":"markdown","f01dde01":"markdown","9187bf51":"markdown","6a5e1d0b":"markdown","45d64e56":"markdown","1513127d":"markdown","b885548f":"markdown","cf847e05":"markdown","f8c9da62":"markdown","5a632bbb":"markdown","22b6eb02":"markdown","c16cdacf":"markdown"},"source":{"f75f96bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b3a33320":"import numpy as np                      #for numerical computationss\nimport pandas as pd                     #for working with dataframes\nimport matplotlib.pyplot as plt         #for plotting informations \nimport seaborn as sns\nimport sklearn.metrics as metric        #Model selection and evaluation","4eae850f":"train1=pd.read_csv(\"\/kaggle\/input\/cost-to-ship\/train.csv\")\ntest1=pd.read_csv(\"\/kaggle\/input\/cost-to-ship\/test.csv\")","2eb0d8cf":"train1.head()","81cbd47f":"test1.head()","114e03cd":"train1.shape","c0231117":"test1.shape","7565c0ab":"target='Cost'","00916510":"train1.drop(['Customer Id','Artist Name'],axis=1,inplace=True)             # inplace=True, prevent copies from being created\ntest1.drop('Artist Name',axis=1,inplace=True)","b53341eb":"numerical_features=train1.select_dtypes(include=[np.number])                          # np.number will return numerical features\ncategorical_features=train1.select_dtypes(include=[np.object])                        # np.object will return categorical features","73205583":"train1.info()","59863dbf":"numerical_features.info()","cf0b4f12":"categorical_features.info()","0ca609c8":"# correlation heatmap for all features\ncorr = train1.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask = mask, annot=True)\nplt.show()","7235c966":"train1.isnull().sum()","e6ca2b95":"test1.isnull().sum()","97500325":"def fillNan(df, col, value):\n    df[col].fillna(value, inplace=True)            ####df=data,col=coloumn name,value=mean or median\n\n#########################    \nfillNan(train1,'Artist Reputation',train1['Artist Reputation'].median())\nfillNan(test1,'Artist Reputation',test1['Artist Reputation'].median())\ntrain1['Artist Reputation'].isna().any()\n#########################\nfillNan(train1,'Height',train1['Height'].median())\nfillNan(test1,'Height',test1['Height'].median())\ntrain1['Height'].isna().any()\n#########################\nfillNan(train1,'Width',train1['Width'].median())\nfillNan(test1,'Width',test1['Width'].median())\ntrain1['Width'].isna().any()\n#########################\nfillNan(train1,'Weight',train1['Weight'].median())\nfillNan(test1,'Weight',test1['Weight'].median())\ntrain1['Weight'].isna().any()\n#########################\nfillNan(train1,'Material',train1['Material'].mode()[0])\nfillNan(test1,'Material',test1['Material'].mode()[0])\ntrain1['Material'].isna().any()\n#########################\nfillNan(train1,'Transport',train1['Transport'].mode()[0])\nfillNan(test1,'Transport',test1['Transport'].mode()[0])\ntrain1['Transport'].isna().any()\n#########################\nfillNan(train1,'Remote Location',train1['Remote Location'].mode()[0])\nfillNan(test1,'Remote Location',test1['Remote Location'].mode()[0])\ntrain1['Remote Location'].isna().any()","721d7e56":"train1.isnull().sum()","09694cb0":"test1.isnull().sum()","c38757a5":"train1.head()","33f3d98d":"train1['State location']=train1['Customer Location'].map(lambda x:x.split()[-2])\ntrain1.drop('Customer Location',axis=1,inplace=True)","55d4dbc8":"test1['State location']=test1['Customer Location'].map(lambda x:x.split()[-2])\ntest1.drop('Customer Location',axis=1,inplace=True)","125913d9":"from datetime import date\ntrain1['Scheduled Date']= pd.to_datetime(train1['Scheduled Date'])\ntrain1['Delivery Date']= pd.to_datetime(train1['Delivery Date'])\n#######################\ntest1['Scheduled Date']= pd.to_datetime(test1['Scheduled Date'])\ntest1['Delivery Date']= pd.to_datetime(test1['Delivery Date'])","8c35829f":"train1['Diff']=(train1['Delivery Date']-train1['Scheduled Date']).map(lambda x:str(x).split()[0])\ntrain1['Diff']=pd.to_numeric(train1['Diff'])\n#################\ntest1['Diff']=(test1['Delivery Date']-test1['Scheduled Date']).map(lambda x:str(x).split()[0])\ntest1['Diff']=pd.to_numeric(test1['Diff'])","31516a6e":"train1['dday'] = train1['Delivery Date'].dt.day\ntrain1['dmonth'] = train1['Delivery Date'].dt.month\ntrain1['dyear'] = train1['Delivery Date'].dt.year\ntrain1['ddayofweek'] = train1['Delivery Date'].dt.dayofweek\n#############################\ntest1['dday'] = test1['Delivery Date'].dt.day\ntest1['dmonth'] = test1['Delivery Date'].dt.month\ntest1['dyear'] = test1['Delivery Date'].dt.year\ntest1['ddayofweek'] = test1['Delivery Date'].dt.dayofweek","a3347470":"train1.head()","376a1565":"test1.head()","58a14c10":"train1.drop(['Scheduled Date','Delivery Date'],inplace=True,axis=1)\ntest1.drop(['Scheduled Date','Delivery Date'],inplace=True,axis=1)","af6e86f0":"train1.head()","cba4b3a3":"numerical_features=train1.select_dtypes(include=[np.number])\ncategorical_features=train1.select_dtypes(include=[np.object])","d91c4449":"numerical_features.head()","6b62c5fa":"categorical_features.head()","882f8874":"train1['Transport'].value_counts()","dd6b7c99":"def Encoding(data, f, data_test=None):\n    feat = data[f].unique()\n    feat_idx = [x for x in range(len(feat))]\n\n    data[f].replace(feat, feat_idx, inplace=True)\n    if data_test is not None:\n        data_test[f].replace(feat, feat_idx, inplace=True)\n","a71525f4":"for col in categorical_features.columns:\n    Encoding(train1, col, test1)","68d15fd4":"val=['Transport']\nfor feat in categorical_features.columns:\n    if len(train1[feat].unique()) > 2 and feat in val:\n        dummy = pd.get_dummies(train1[feat], drop_first=True, prefix=feat+\"_\")\n        train1 = pd.concat([train1, dummy], axis=1)\n        train1.drop(feat, axis=1, inplace=True)\n###########################\n# create dummy features\ncustom_feat = ['Transport']\nfor feat in categorical_features.columns:\n    if len(test1[feat].unique()) > 2 and feat in custom_feat:\n        dummyVars = pd.get_dummies(test1[feat], drop_first=True, prefix=feat+\"_\")\n        test1 = pd.concat([test1, dummyVars], axis=1)\n        test1.drop(feat, axis=1, inplace=True)\n\ntest1.head()","38c7042f":"def Datashape(data):\n    rows,cols=data.shape\n    print('The dataframe has',rows,'Rows and',cols,'Columns.')\n    \ndef getRmse(y_train, y_train_pred):\n    print(metric.mean_squared_error(y_train, y_train_pred))\n    \ndef log1p(vec):\n    return np.log1p(abs(vec))\n\ndef expm1(x):\n    return np.expm1(x)\n\ndef clipExp(vec):\n    return np.clip(expm1(vec), 0, None)\n\ndef getRmse(y_train, y_train_pred):\n    print(metric.mean_squared_error(y_train, y_train_pred))","2a62b7e8":"Datashape(train1)","a0db2e14":"Datashape(test1)","25796941":"train1[target]=np.log1p(abs(train1[target]))","f7572f80":"train1[target].min()","1199a2d8":"seed=12\nnp.random.seed(seed)\ntrain_shuffle = train1.sample(frac=1, random_state=seed).reset_index(drop=True)\ny=train_shuffle.pop(target)                               #Prediction target","011e34cb":"x=train_shuffle","cda793c8":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(x,y,train_size=0.8,random_state=seed)","8810698c":"from sklearn.preprocessing import MinMaxScaler\nscale=MinMaxScaler()\nX_train = pd.DataFrame(scale.fit_transform(X_train), columns=X_train.columns)\nX_test = pd.DataFrame(scale.transform(X_test), columns=X_train.columns)         #It will keep all the train coloumns only ","68f5ae10":"X_train.head()","2653b764":"X_test.head()","ad873a46":"X_train.describe()","ad043054":"import sklearn.ensemble as ens\n\ngb_model = ens.GradientBoostingRegressor()\n\ngb_model.fit(X_train, Y_train)\n\n# predict\ny_train_pred = gb_model.predict(X_train)\ny_test_pred = gb_model.predict(X_test)\ngetRmse(Y_train, y_train_pred)\ngetRmse(Y_test, y_test_pred)","c6b9651e":"gb_model.score(X_test,Y_test)                                #Test score","0c6c4ada":"gb_model.score(X_train,Y_train)                              #Train Score","02b5fd88":"from sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt','log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 1000,10)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10,14]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4,6,8]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'subsample':[.5,.75,1],\n               'learning_rate':[.001,0.01,.1],\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","a6dab758":"print(random_grid)","e9e2bb0c":"rf_randomcv=RandomizedSearchCV(estimator=gb_model,param_distributions=random_grid,n_iter=100,cv=3,verbose=2,\n                               random_state=100,n_jobs=-1)","d5a5dfb4":"rf_randomcv.fit(X_train,Y_train)","5b088566":"best_grid=rf_randomcv.best_estimator_\nprint(best_grid)","1f00ff9d":"y_train_pred = best_grid.predict(X_train)\ny_test_pred = best_grid.predict(X_test)\ngetRmse(Y_train, y_train_pred)\ngetRmse(Y_test, y_test_pred)","c02f82d9":"best_grid.score(X_train,Y_train)","907b5392":"best_grid.score(X_test,Y_test)","e1be1c92":"def getTestResults(model=None, roundOff=False):\n    df_final = train1.sample(frac=1, random_state=1).reset_index(drop=True)\n    test_cols = [x for x in df_final.columns if target not in x]\n    df_final_test = test1[test_cols]\n    df_y = df_final.pop(target)\n    df_X = df_final\n\n    scaler = MinMaxScaler()\n\n    df_X = pd.DataFrame(scaler.fit_transform(df_X), columns=df_X.columns)\n    X_test = pd.DataFrame(scaler.transform(df_final_test), columns=df_X.columns)\n    \n    \n    rf_randomcv.fit(df_X, df_y)\n    \n    best_grid=rf_randomcv.best_estimator_\n\n    # Predict\n    \n    y_train_pred = best_grid.predict(df_X)\n    y_test_pred = best_grid.predict(X_test)\n    \n    if roundOff:\n        y_train_pred = np.round(y_train_pred)\n        y_test_pred = np.round(y_test_pred)\n                \n    if type(y_test_pred[0]) == np.ndarray:\n        y_test_pred = np.ravel(y_test_pred)\n\n    getRmse(df_y, y_train_pred)\n    return clipExp(y_test_pred)","8308e89f":"# Machine Learning models\nresults = getTestResults(roundOff=False)","b7904949":"submission = pd.DataFrame({\n    'Customer Id': test1['Customer Id'],\n    target: results,\n})\nsubmission.head()","cd963964":"submission.to_csv('.\/submission_Ensemble1.csv', index=False)","916230e0":"**Lets's move into the final step**","2aca3892":"***Checking the shape of train and test data***","c50bf0da":"**Checking the correlation in our train data**","ac93f5b0":"Loading the data ","35905852":"**\"Dropping Scheduled Date\tDelivery Date from train and test**","6170326f":"***Converting any negative values in our target(Cost) to positive***","b4261837":"**Handle Nan values in our train and test data**","97b0a9e7":"**Taking the best results for our parameters**","f5a177b1":"**We can see in our dataset there are numerical and categorical features are present,so will divide them and explore **","95d6f80b":"**Task**\n\nYou work for a company that sells sculptures that are acquired from various artists around the world. Your task is to predict the cost required to ship these sculptures to customers based on the information provided in the dataset.","95b6249b":"**Our target value is Cost, present in train1.csv**","ddf8ddc7":"# ***Improving our prediction with hyperparameter tuning***\n\n![image](https:\/\/builtin.com\/sites\/default\/files\/styles\/ckeditor_optimize\/public\/inline-images\/gradient-descent-convex-function.png)\n\nRefernce: https:\/\/ruder.io\/optimizing-gradient-descent\/","56189f82":"# Description of the dataset \ud83d\udcdd","2fd9396d":"**We will check for numerical and categorical datatypes**","02b23e93":"**Checking the datatypes of our dataset**","45235bc1":"***Creating more data from the given data in train and test to improve machine prediction***\n**We will use Scheduled Date and Delivery Date for this purpose\t**\n\nReference : [https:\/\/realpython.com\/python-datetime\/](http:\/\/)","d1ac1b9b":"**Taking Difference b\/w Scheduled Date and Delivery Date**","4b2d927c":"**Transform features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n\nThe transformation is given by:\n\nX_std = (X - X.min(axis=0)) \/ (X.max(axis=0) - X.min(axis=0))\nX_scaled = X_std * (max - min) + min**\n\n\nReference : https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html","a10f610a":"## Importing necessary libraries","254396a4":"**For numerical data we will use median of that particular coloumns to replace Nan values** and \n**For Categorical data we will mode of that particular coloumns to replace Nan values**\n\nReference : [https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/missing_data.html](http:\/\/)","f01dde01":"**Problem**\n\nIt can be difficult to navigate the logistics when it comes to buying art. These include, but are not limited to, the following:\n\nEffective collection management\nShipping the paintings, antiques, sculptures, and other collectibles to their respective destinations after purchase\nThough many companies have made shipping consumer goods a relatively quick and painless procedure, the same rules do not always apply while shipping paintings or transporting antiques and collectibles.","9187bf51":"**DataFrame.sample**\n\nReference : [https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.sample.html](http:\/\/)","6a5e1d0b":"**Dropping some irrelevant data from train and test**","45d64e56":"#Check top five values in our data","1513127d":"**In train and test data we can extract some information from Customer Location['OH','WY'........]**","b885548f":"**np.triu_indices_from(mask) returns the indices for the upper triangle of the array.**\n**Now, we set the upper triangle to True.\nmask[np.triu_indices_from(mask)]= True**\n\n\nReference : [https:\/\/www.kdnuggets.com\/2019\/07\/annotated-heatmaps-correlation-matrix.html](http:\/\/)","cf847e05":"# \ud83e\udded Exploring the data \ud83e\udded","f8c9da62":"**We can see data get normalize in train and test**","5a632bbb":"**Performing Encoding on train and test**","22b6eb02":"##### \u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0fDISCLAIMER: This notebook is beginner friendly, so don't worry if you don't know much about Gradient Descent algorithm everything is explained clearly and concisely.","c16cdacf":"##  HackerEarth Machine Learning Challenge: Exhibit A(rt) \n\n*Leaderboard Ranking 35 "}}