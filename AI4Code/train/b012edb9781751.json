{"cell_type":{"90acdad9":"code","ccb666c2":"code","603beeff":"code","07da0ff9":"code","8c0cd41d":"code","2fd4ce6a":"code","1c7204b7":"code","357cb969":"code","c27b3feb":"code","78dcbaf2":"code","f88babaf":"code","0ad04fce":"code","ba551860":"code","4a909fc4":"code","42ceb3dc":"code","72dee237":"code","e64e9aef":"code","0b826a06":"code","9d5929b2":"code","309c8f69":"code","af659b96":"code","5d609af7":"code","6f59a756":"code","6405e578":"code","76f0f779":"code","e239c2d3":"code","55c6ecc1":"code","959ea4a2":"code","0a8aa427":"code","724b6b29":"code","539a84e1":"code","0ef21834":"code","1afd2d29":"code","1f3eb629":"code","ce83a457":"code","09984042":"code","63070bdc":"code","9fb09920":"code","95f15ba0":"code","88d01f11":"code","b61079ba":"code","18f13b15":"code","7609221e":"code","033c1868":"code","051a6e3c":"code","0a05ed9b":"code","f0d1fac0":"code","1fc444c6":"code","58de0363":"code","a0185c80":"code","a87ce1d6":"code","014ed87f":"code","4e2fc2ce":"code","4a66bf69":"code","b7030021":"code","ed676da3":"code","96776b72":"code","cd0703f4":"code","28b204ae":"code","3219d873":"code","93d92ca6":"code","42ae07eb":"code","442576c4":"code","9c86e2ed":"code","6cfbcbb8":"code","b0707596":"code","a0433d49":"code","b67d991c":"code","d3334cc3":"code","9efb0abb":"code","10954cc2":"code","cc7d3e97":"code","bf6a557d":"markdown","37483f4c":"markdown","2d230e15":"markdown","4e3f40a2":"markdown","1a444d9e":"markdown","08e7462e":"markdown","7832c068":"markdown","84cd7721":"markdown","8de31ace":"markdown","3b4a3c92":"markdown","11dd809a":"markdown","20414785":"markdown","abcd2a1f":"markdown","228c4b31":"markdown","fc5f0c48":"markdown","83cb0721":"markdown","c78625bd":"markdown","ce0d4183":"markdown","338ef80b":"markdown","256ebe20":"markdown","acb394c4":"markdown","8a60b356":"markdown","13b9161c":"markdown","0b88aaa2":"markdown","5b61897f":"markdown","e24c8ac5":"markdown","d7c83b08":"markdown","c330867f":"markdown","fb86db3a":"markdown","06eac2da":"markdown","41186c16":"markdown","0b2e7835":"markdown","0e41ed73":"markdown","bdf4e815":"markdown","106ba48f":"markdown","6ff9674c":"markdown","7640c47b":"markdown","3f42c793":"markdown","e0705851":"markdown","a313bbd8":"markdown","e031cc76":"markdown","89d011a9":"markdown","59adf369":"markdown","b97376e1":"markdown","ad9ed5ee":"markdown","776a2402":"markdown","5e94cbe7":"markdown","0731d0ad":"markdown","d811390c":"markdown"},"source":{"90acdad9":"import numpy as np \nimport pandas as pd \n\n%matplotlib inline\nimport matplotlib.pyplot as plt # Visualization \nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings      # to ignore warnings\nwarnings.filterwarnings(\"ignore\")\n\n","ccb666c2":"from tqdm.notebook import tqdm, trange","603beeff":"\npath=\"\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\"\ndf=pd.read_csv(path)\n#df1=pd.read_csv(path1)\ndf.head()  # Top 5 rows\ndf.tail()  # Bottom 5 rows\ndf.sample(5)  # Random 5 rows\ndf.sample(5) # random fractional numbers rows  of total no of rows","07da0ff9":"print(\"Shape of the data\",df.shape)","8c0cd41d":"data=df.copy()","2fd4ce6a":"import pandas_profiling as pp\ndata=df.copy()\nreport=pp.ProfileReport(data, title='Pandas Profiling Report')  # overview and quick data analysis\nreport","1c7204b7":"print(\"Columns of the data are:\",df.columns)","357cb969":"# columns Renaming\ndf=df.rename(columns={'Serial No.':'SerialNo', 'GRE Score':'GRE', 'TOEFL Score':'TOEFL',\n                      'University Rating':'UniversityRating','LOR ':'LOR','Chance of Admit ':'ChanceOfAdmit'})\ndf.columns","c27b3feb":"#Drop the column \"Serial No.\" \ndf=df.drop(columns=\"SerialNo\")","78dcbaf2":"df.isnull().sum()","f88babaf":"df.info()","0ad04fce":"df.isnull().values.any() # check the null values in whole of the data set if any.\n","ba551860":"missing_data=df.isnull()\nfor column in missing_data.columns.values.tolist():\n    print(column)\n    print(missing_data[column].value_counts())","4a909fc4":"def missing_data_percentage(data):\n    total=data.isnull().sum().sort_values(ascending=False)\n    percent=np.round(total\/len(data)*100,2)\n    return pd.concat([total,percent],axis=1,keys=[\"Total\",\"Percent\"])\nmissing_data_percentage(df)","42ceb3dc":"#check any duplicates data in dataframe.\ndf.duplicated().any()","72dee237":"#statistical summary of the data\ndf.describe()","e64e9aef":"# Groupby the data by \"University rating\".\ndf.groupby(\"UniversityRating\").mean()","0b826a06":"print(\" Minimum requirements for more than 85% chance to get admission.\\n\",df[(df['ChanceOfAdmit']>0.85)].min())\n","9d5929b2":"df.pivot_table(values=['GRE','TOEFL'],index=['UniversityRating'],columns='Research',aggfunc=np.median)\n","309c8f69":"plt.figure(figsize=(16,9))\ndf['ChanceOfAdmit'].value_counts().plot.bar()\nplt.show()","af659b96":"#relashionship between the variables of the data in scatter form.\npd.plotting.scatter_matrix(df,figsize=(12,12)) # Scatter matrix for the data.\nplt.show()","5d609af7":"sns.pairplot(df)\nplt.show()","6f59a756":"df.hist(figsize=(10,10),edgecolor=\"k\")\nplt.tight_layout()\nplt.show()","6405e578":"plt.figure(figsize=(15,12))\ncol_list=df.columns\n \nfor i in range(len(df.columns)):\n    plt.subplot(3,3,i+1)\n    plt.hist(df[col_list[i]],edgecolor=\"w\")\n    plt.title(col_list[i],color=\"g\",fontsize=15)\n\n\nplt.show()","76f0f779":"\n#Boxplot for all variables.\n\"\"\"for col in df.columns:\n    df[[col]].boxplot()\n    plt.show()\"\"\"\ndf.plot(kind='box',subplots=True,layout=(2,4),grid=True,figsize=(12,8))\nplt.tight_layout()\n\nplt.show()\n\n\n","e239c2d3":"cols= df.columns\n\n#KDE Plot for all features\nfor i in range(len(df.columns)):\n    plt.subplot(2,4,i+1)\n    sns.kdeplot(df['ChanceOfAdmit'],df[cols[i]],cmap='Blues',shade=True,shade_lowest =False)\nplt.show()","55c6ecc1":"# Correlation Between the data features.\ndf.corr()","959ea4a2":"#heatmap of the correlation of the data variables.\nmask = np.zeros_like(df.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(12,8))\nsns.heatmap(df.corr(),mask=mask,annot=True,linewidths=1.0)\nplt.show()","0a8aa427":"\nsns.pairplot(df,x_vars=['GRE','TOEFL','UniversityRating','CGPA','SOP','LOR','Research'],\n             y_vars='ChanceOfAdmit')\nplt.tight_layout()","724b6b29":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error,SCORERS\nfrom sklearn.linear_model import LinearRegression","539a84e1":"X=df.iloc[:,:-1]\nY=df.iloc[:,-1]\nx_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=2)","0ef21834":"sc=StandardScaler()\nX_train=sc.fit_transform(x_train)\nX_test=sc.transform(x_test)\n","1afd2d29":"lr=LinearRegression()\nlr.fit(X_train,y_train)\n\n#Training data score\nytrain_pred=lr.predict(X_train)\nr2_score(y_train,ytrain_pred),mean_squared_error(y_train,ytrain_pred)\n\n","1f3eb629":"#Testdata score\ny_pred=lr.predict(X_test)\nr2_score(y_test,y_pred),mean_squared_error(y_test,y_pred)\n","ce83a457":"\nprint(\"Intercept of Linear Regression is:\\n,\",lr.intercept_,\"Coefficients of Linear Regression are:\\n,\",lr.coef_)","09984042":"from sklearn.ensemble import RandomForestRegressor\nrf_model = RandomForestRegressor(n_estimators = 150,max_depth=4,random_state = 42,criterion=\"mse\")\nrf_model.fit(X_train,y_train)\ny_pred_rf=rf_model.predict(X_test)\nr2_score(y_test,y_pred_rf),mean_squared_error(y_test,y_pred_rf)\n","63070bdc":"feature_importance = pd.DataFrame(rf_model.feature_importances_, X.columns)\nfeature_importance","9fb09920":"feature_importance = feature_importance.sort_values(by=0,ascending =True)\nfeature_importance.plot(kind='barh')\nplt.plot()","95f15ba0":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor(random_state = 4,max_depth=4)\ndt.fit(X_train,y_train)\ny_pred_dt = dt.predict(X_test) \nprint(r2_score(y_test,y_pred_dt),mean_squared_error(y_test,y_pred_dt))\n    ","88d01f11":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor(random_state = 1)\ndt.fit(x_train,y_train)\ny_pred_dt = dt.predict(x_test) \nr2_score(y_test,y_pred_dt),mean_squared_error(y_test,y_pred_dt)","b61079ba":"sorted(SCORERS.keys())","18f13b15":"#classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n \n\n#regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n#model selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n#preprocessing\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder\n\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score ","7609221e":"models=[LinearRegression(),\n        RandomForestRegressor(n_estimators=150,max_depth=4),\n        DecisionTreeRegressor(random_state=42,max_depth=4),GradientBoostingRegressor(),AdaBoostRegressor(),\n        KNeighborsRegressor(n_neighbors=35),\n        BaggingRegressor(),Ridge(alpha=1.0),RidgeCV(),SVR()]\nmodel_names=['LinearRegression','RandomForestRegressor','DecisionTree','GradientBoostingRegressor','AdaBoost','kNN',\n             'BaggingReg','Ridge','RidgeCV',\"SVR\"]\n\nR2_SCORE=[]\nMSE=[]\n      \nfor model in range(len(models)):\n    print(\"*\"*35,\"\\n\",model_names[model])\n    reg=models[model]\n    reg.fit(X_train,y_train)\n    pred=reg.predict(X_test)\n    r=r2_score(y_test,pred)\n    mse=mean_squared_error(y_test,pred)\n    R2_SCORE.append(r)\n    MSE.append(mse)\n    print(\"R2 Score\",r)\n    print(\"MSE\",mse)\n","033c1868":"df_model=pd.DataFrame({'Modelling Algorithm':model_names,'R2_score':R2_SCORE,\"MSE\":MSE})\ndf_model=df_model.sort_values(by=\"R2_score\",ascending=False).reset_index()\nprint(df_model)\n\n\nplt.figure(figsize=(10,10))\nsns.barplot(y=\"Modelling Algorithm\",x=\"R2_score\",data=df_model)\n\nplt.xlim(0.35,0.95)\nplt.grid()\nplt.tight_layout()","051a6e3c":"df_model.head(5)","0a05ed9b":"\nlr=LinearRegression()\nlr.fit(X_train,y_train)\ny_lr_pred=lr.predict(X_test)\nprint(\"MSE:\",mean_squared_error(y_test,y_lr_pred),\"R2 SCORE:\",r2_score(y_test,y_lr_pred))","f0d1fac0":"ridge=Ridge()\nridge.fit(X_train,y_train)\ny_ridge=ridge.predict(X_test)\nprint(\"MSE:\",mean_squared_error(y_test,y_ridge),\"R2 SCORE:\",r2_score(y_test,y_ridge))","1fc444c6":"r_CV=RidgeCV()\nr_CV.fit(X_train,y_train)\ny_rCV=r_CV.predict(X_test)\nprint(\"MSE:\",mean_squared_error(y_test,y_rCV),\"R2 SCORE:\",r2_score(y_test,y_rCV))\n","58de0363":"tp=pd.DataFrame({\"TEST_value\":y_test,\"LR_predict_value\": y_lr_pred,\"RIDGE_predict_value\": y_ridge,\"RCV_predict_value\": y_rCV,\"DIFF(TEST_value-LR_predict_value)\": (y_test-y_lr_pred)})\ntp.head()","a0185c80":"plt.figure(figsize=(10,10),dpi=75)\nx=np.arange(len(tp[\"TEST_value\"]))\ny=tp[\"TEST_value\"]\nz=tp[\"LR_predict_value\"]\nplt.plot(x,y)\nplt.plot(x,z,color='r')\n","a87ce1d6":"print(\"Score of Linear Regression:\",r2_score(y_test,y_lr_pred))","014ed87f":"from collections import Counter \nCounter([i-i%0.1+0.1 for i in df[\"ChanceOfAdmit\"]])","4e2fc2ce":"df['Label']= np.where(df['ChanceOfAdmit']<=0.72,0,1)\nprint(df['Label'].value_counts())\ndf.sample(5)","4a66bf69":"df[\"Label\"].value_counts()","b7030021":"sns.countplot(df['Label'])","ed676da3":"sns.pairplot(df,hue = \"Label\")\nplt.show()","96776b72":"X=df.drop(columns = [\"ChanceOfAdmit\",\"Label\"])\ny=df[\"Label\"]","cd0703f4":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)\nprint(\" X_train Shape {},\\n y_train Shape {},\\n X_test Shape {},\\n y_test Shape {},\"\n      .format(X_train.shape,X_test.shape,y_train.shape,y_test.shape))","28b204ae":"from sklearn.metrics import roc_curve,auc,roc_auc_score, confusion_matrix,classification_report\nfrom sklearn.model_selection import cross_val_score","3219d873":"def plot_roc(FP_rate,TP_rate,roc_auc):\n    plt.plot(FP_rate,TP_rate,color='r',label ='AUC =%0.2f' %roc_auc)\n    plt.title(\"ROC\")\n    plt.plot([0,1],linestyle='--')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.axis(\"tight\")\n    plt.show()\n    \n    \ndef Score(model):\n    print(\"*\"*50)\n    print(\"Train classification report: \")\n    print(\"*\"*50)\n    print(classification_report(model.predict(X_train), y_train))\n    print(confusion_matrix(model.predict(X_train), y_train))\n\n    print()\n    print(\"*\"*50)\n    print(\"Test classification report: \")\n    print(\"*\"*50)\n    print(classification_report(model.predict(X_test), y_test))\n    print(confusion_matrix(model.predict(X_test), y_test))\n    \n    \n    \n    print()\n    print(\"*\"*50)\n    \n    y_pred = model.predict(X_test)\n    y_proba=model.predict_proba(X_test)\n    \n    print('Accurancy Score :',accuracy_score(y_test, y_pred))\n\n    cm=confusion_matrix(y_test,y_pred)\n    print(cm)\n\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_proba[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n\n    plot_roc(false_positive_rate, true_positive_rate, roc_auc)\n    print('AUC Score :',roc_auc_score(y_test, y_pred))","93d92ca6":"clf = LogisticRegression()\nclf.fit(X_train, y_train)\n\nScore (clf)","42ae07eb":"parameters = [{'penalty' : ['l1', 'l2', 'elasticnet'],'C' : [0.1, 0.4, 0.5],'l1_ratio':[0,1],'solver':['saga'],'random_state' : [42]}]\n\ngscv = GridSearchCV(LogisticRegression(),parameters,scoring='accuracy')\ngscv.fit(X_train, y_train)\n\nprint('Best parameters set:')\nprint(gscv.best_params_)\nprint()\n\nprint(\"*\"*50)\nprint(\"Train classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(X_train), y_train))\nprint(confusion_matrix(gscv.predict(X_train), y_train))\n\nprint()\nprint(\"*\"*50)\nprint(\"Test classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(X_test), y_test))\nprint(confusion_matrix(gscv.predict(X_test), y_test))\n\n#Crossvalidation:\ncvs = cross_val_score(estimator = LogisticRegression(), \n                      X = X_train, y = y_train, cv = 12)\n\nprint()\nprint(\"*\"*50)\nprint(\"Cross Validation Score Mean\", cvs.mean())\nprint(\"\",\"Cross Validation Score std\",cvs.std())","442576c4":"lr = LogisticRegression(C= 0.1, penalty= 'l2', random_state= 0)\nlr.fit(X_train,y_train)\n\ny_pred = lr.predict(X_test)\ny_proba=lr.predict_proba(X_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nplot_roc(false_positive_rate, true_positive_rate, roc_auc)\n\nprint('Accurancy Score :',accuracy_score(y_test, y_pred))\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)","9c86e2ed":"clf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\nScore (clf)","6cfbcbb8":"parameters = [\n    {\n        'criterion' : ['gini', 'entropy'],\n        'max_depth' : [3, 4, 5],\n        'min_samples_split' : [10, 20, 5],\n        'random_state': [0], \n    }\n]\n\ngscv = GridSearchCV(DecisionTreeClassifier(),parameters,scoring='accuracy')\ngscv.fit(X_train, y_train)\n\nprint('Best parameters set:')\nprint(gscv.best_params_)\nprint()\n\nprint(\"*\"*50)\nprint(\"Train classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(X_train), y_train))\nprint(confusion_matrix(gscv.predict(X_train), y_train))\n\nprint()\nprint(\"*\"*50)\nprint(\"Test classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(X_test), y_test))\nprint(confusion_matrix(gscv.predict(X_test), y_test))\n\n#Crossvalidation:\ncvs = cross_val_score(estimator = DecisionTreeClassifier(), \n                      X = X_train, y = y_train, cv = 12)\n\nprint()\nprint(\"*\"*50)\nprint(\"Cross Validation Score Mean\", cvs.mean())\nprint(\"\",\"Cross Validation Score std\",cvs.std())","b0707596":"dt = DecisionTreeClassifier(criterion= 'entropy', max_depth= 5, min_samples_split= 20, \n                            random_state= 0)\ndt.fit(X_train,y_train)\n\ny_pred = dt.predict(X_test)\ny_proba=dt.predict_proba(X_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nplot_roc(false_positive_rate, true_positive_rate, roc_auc)\n\nprint('Accurancy Score :',accuracy_score(y_test, y_pred))\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)","a0433d49":"clf = RandomForestClassifier()\nclf.fit(X_train, y_train)\n\nScore (clf)","b67d991c":"parameters = [\n    {\n        'n_estimators': np.arange(10, 40, 5),\n        'criterion' : ['gini', 'entropy'],\n        'max_depth' : [3, 4, 5],\n        'min_samples_split' : [10, 20, 5],\n        'random_state': [0],\n    }\n]\n\ngscv = GridSearchCV(RandomForestClassifier(),parameters,scoring='accuracy')\ngscv.fit(X_train, y_train)\n\nprint('Best parameters set:')\nprint(gscv.best_params_)\nprint()\n\nprint(\"*\"*50)\nprint(\"Train classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(X_train), y_train))\nprint(confusion_matrix(gscv.predict(X_train), y_train))\n\nprint()\nprint(\"*\"*50)\nprint(\"Test classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(X_test), y_test))\nprint(confusion_matrix(gscv.predict(X_test), y_test))\n\n#Crossvalidation:\ncvs = cross_val_score(estimator = RandomForestClassifier(), \n                      X = X_train, y = y_train, cv = 12)\n\nprint()\nprint(\"*\"*50)\nprint(\"Cross Validation Score Mean\", cvs.mean())\nprint(\"\",\"Cross Validation Score std\",cvs.std())","d3334cc3":"rf = RandomForestClassifier(criterion= 'entropy', max_depth= 4, \n                            min_samples_split= 20, n_estimators= 20, \n                            random_state= 0)\nrf.fit(X_train,y_train)\n\ny_pred = rf.predict(X_test)\ny_proba=rf.predict_proba(X_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nplot_roc(false_positive_rate, true_positive_rate, roc_auc)\n\nprint('Accurancy Score :',accuracy_score(y_test, y_pred))\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)","9efb0abb":"clf =GradientBoostingClassifier()\nclf.fit(X_train, y_train)\n\nScore (clf)","10954cc2":"parameters = [\n    {\n        'learning_rate': [0.01, 0.02, 0.002],\n        'n_estimators' : np.arange(10, 100, 5),\n        'max_depth' : [3, 4, 5],\n        'min_samples_split' : [10, 20, 5],\n        'random_state': [0],\n    }\n]\n\ngscv = GridSearchCV(GradientBoostingClassifier(),parameters,scoring='accuracy')\ngscv.fit(X_train, y_train)\n\nprint('Best parameters set:')\nprint(gscv.best_params_)\nprint()\n\nprint(\"*\"*50)\nprint(\"Train classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(X_train), y_train))\nprint(confusion_matrix(gscv.predict(X_train), y_train))\n\nprint()\nprint(\"*\"*50)\nprint(\"Test classification report: \")\nprint(\"*\"*50)\nprint(classification_report(gscv.predict(X_test), y_test))\nprint(confusion_matrix(gscv.predict(X_test), y_test))\n\n#Crossvalidation:\ncvs = cross_val_score(estimator = GradientBoostingClassifier(), \n                      X = X_train, y = y_train, cv = 12)\n\nprint()\nprint(\"*\"*50)\nprint(\"Cross Validation Score Mean\", cvs.mean())\nprint(\"\",\"Cross Validation Score std\",cvs.std())","cc7d3e97":"gbm = GradientBoostingClassifier(learning_rate= 0.02, max_depth= 5, \n                                 min_samples_split= 10, n_estimators= 70, \n                                 random_state= 0)\ngbm.fit(X_train,y_train)\n\nScore (gbm)","bf6a557d":"**Histogram**\n> Distribution Of the data by visualise the histograms for all features of the data.","37483f4c":"\n\n\nAll columns in the data and shape of the data(columns,rows).We will list all the columns for all data by\n*df.columns*. We will check all columns, are there any spelling mistake?\nIf we found any spelling mistake we will correct it.\n","2d230e15":"## Machine Learning Models","4e3f40a2":"### DecisionTreeRegressor","1a444d9e":"Now,we have 252 observations in class 0 and 248 observations in class 1. Which is a good balance.","08e7462e":" Rename the columns name which have to required change.","7832c068":"**Box Plots for all features**","84cd7721":"### Linear Regression Model\n* Fit the model\n* Find the predicted Values with the test data  applied on model.\n* R2 value and Mean Square Error with test data and predicted data.","8de31ace":" average requirements of all features  to get admission for all universities on the basis of their Ratings.","3b4a3c92":"* Decision tree model without standardised data.","11dd809a":"Some important statistical summary of the data e.g. Mean,std,minimum , maximum value, 25,50,75 pecentiles of the data.","20414785":"##  Classification Models\n\n Till now we have solve our problem with Regression Models because data has continous target variable. We have predicted how much chances of admission.\n We can also find out that the student will get admission or not by using classification problem.","abcd2a1f":"## Decision Tree\n","228c4b31":"# Quick EDA with pandas_profiling\nPandas Profiling library is very useful and easy for the detailed overview and EDA ofthe data.","fc5f0c48":"### Function For Evaluation Metrices","83cb0721":"### Logistic Regression Classification","c78625bd":"#### Hyperparameter Tuning for Gradient Boosting Classifier","ce0d4183":"### Regression","338ef80b":"There are no missing values in the dataset. It makes our data pre-processing very much easier.\nIt can also be found in the following way.\n                  \n> df.info() --> give the information about the data as Index columns, datatypes of the variables,null values,memory used by data.","256ebe20":"There are 500 observations and 8 features in the data.","acb394c4":" ### Random Forest Regressor","8a60b356":"So we can convert out training data target variable in two classes , yes (1), or no(0). below 0.72 we will assign label 0 and above 0.72 we assign label 1. Means if student have chances of admission above 72% , will get admission.","13b9161c":"#### Hyperparameter Tuning for Decision Tree Classifier","0b88aaa2":"#### Hyperparameter Tuning for Logistic Regression","5b61897f":"**Pivot Table**","e24c8ac5":" Minimum requirements for more than 85% chance of the admission.\n \n\n\tGRE Score\t        320.00\n\tTOEFL Score \t    108.00\n    University Rating\t2.00\n\tSOP\t               3.00\n\tLOR                3.00\n   \tCGPA\t          8.94\n\tResearch        \t0.00\n\tChance of Admit \t0.86","d7c83b08":"**Pairplot**\n\nScatter diagram between the variables of the data and \"Chance of Admit\".","c330867f":"CGPA  is the most important feature to determine Chance of admission.","fb86db3a":"### Defining The class Labels for Classification\n\nFor ease of working with the classifier ,it will be nice to have 50-50 split on the data.\n\nFor class balance ,lets assume thatthe bottom 50% of the observations fall on class 0 and the top 50% of the observations fall in class 1.\n\nBinning the `ChanceOfAdmit` variable and see where 50% lies.","06eac2da":"Same matrix plot can be plot as below:-","41186c16":"Importing all the necessary libraries.","0b2e7835":"**Most useful Machine Learning Libraries import**","0e41ed73":"So Top 5 models which have Best R2 Score :-\n1. Linear Regressioin\n2. Ridge Regression\n3. RidgeCV Regression\n4. Random Forest Regressor\n5. Gradient Boosting Regressor\n\n      ","bdf4e815":"**If you find this kernal helpful and useful, Kindly comments your suggestion and upvote the kernal.**\n\n\n## THANKYOU","106ba48f":"Shape of the data.","6ff9674c":"Alterate way for histogram.","7640c47b":"**Load the data.**\n> Load the data and look into  sample of the data.Pandas module is used for reading files. We have our data in '.csv' format. We will use `read_csv()` function for loading the data.","3f42c793":"tqdm is a Python library that allows you to output a smart progress bar by wrapping around any iterable. A tqdm progress bar not only shows you how much time has elapsed, but also shows the estimated time remaining for the iterable.\n\n\n\nThe resulting tqdm progress bar gives us information that includes the task completion percentage, number of iterations complete, time elapsed, estimated time remaining, and the iterations completed per second.\n\n\n\nIn this case, tqdm allows for further optimization by using trange(100) in place of the tqdm(range(100)).","e0705851":"Import the important libraries for machine learning.","a313bbd8":"#### Hyperparameter Tuning for Random Forest Classifier","e031cc76":"### Random Forest Classifier","89d011a9":"**Correlation Between the data features**\n\nWe can check how each features are reated with others using corr() function.\n\nThe correlation value ranges between -1 to 1.When it is close to 1, it means that there is a strong positive correlation. When the coefficient is close to \u20131, it means that there is a strong negative correlation. Finally, coefficients close to zero mean that there is no linear correlation. We can observe the detail information using correlation matrix","59adf369":"### Gradient Boosting Classifier","b97376e1":"**Heatmap of the correlation**","ad9ed5ee":"**Bar Plot**","776a2402":"We will fit the various models with training data ,predict target feature values with test data and will find the which model provide the best R2 Value and MSE value, so that our prediction are more likely to true values.","5e94cbe7":"## Loading Libraries\nAll Python capabilities are not loaded to our working environment by default (even they are already installed in your system). So, we import each and every library that we want to use.\n\nIn data science, numpy and pandas are most commonly used libraries. Numpy is required for calculations like means, medians, square roots, etc. Pandas is used for data processin and data frames. We chose alias names for our libraries for the sake of our convenience (numpy --> np and pandas --> pd).","0731d0ad":"\n\nCheck the sum of null values in every column.","d811390c":"### Data Preprocessing-\n\n* SPLIT the data into train and test data.\n"}}