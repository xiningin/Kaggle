{"cell_type":{"99c9ea1e":"code","3f7738f9":"code","9e38f200":"code","f7dd48de":"code","7ed6fe0a":"code","32541227":"code","edf47734":"code","3ebe52b2":"code","bb5dee38":"code","b184c710":"code","1443f751":"code","e4a1d229":"code","74470149":"code","131ceed2":"code","50031f3f":"code","66b4eae8":"code","09abf3fa":"code","2319e862":"code","63a51a2e":"code","c7edf7bb":"code","230b3532":"code","531dacc7":"code","449b0163":"markdown"},"source":{"99c9ea1e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nfrom glob import glob \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport shutil\n\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom keras.utils import to_categorical\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom skimage.io import imread\nimport gc\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\nfrom keras.layers import Conv2D, MaxPool2D\n\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.utils import shuffle\n\nfrom IPython.display import clear_output\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3f7738f9":"base_train_title_dir='..\/input\/train_images\/train_images\/'\ndf=pd.DataFrame({'path': glob(os.path.join(base_train_title_dir,'*.jpg'))})\ndf['image_id']=df.path.map(lambda x: x.split('\/')[4])\ndf.head()","9e38f200":"labels =pd.read_csv( '..\/input\/traininglabels.csv')\nlabels.head()","f7dd48de":"df_data = df.merge(labels, on = \"image_id\")\ndf_data.drop('score',axis=1,inplace=True)\ndf_data.head(3)\n#df.drop(df[df.score < 0.75].index,inplace=True)","7ed6fe0a":"ax = sns.countplot(x=\"has_oilpalm\", data=df_data)","32541227":"df_data.has_oilpalm.value_counts()","edf47734":"SAMPLE_SIZE=942\n#0\ndf_0 = df_data[df_data['has_oilpalm'] == 0].sample(SAMPLE_SIZE, random_state = 101)\n#1\ndf_1 = df_data[df_data['has_oilpalm'] == 1].sample(SAMPLE_SIZE, random_state = 101)\n# concat the dataframes\ndf_data = shuffle(pd.concat([df_0, df_1], axis=0).reset_index(drop=True))\n\n#balance data\ny = df_data['has_oilpalm']\ndf_train, df_val = train_test_split(df_data, test_size=0.10, random_state=101, stratify=y)\n\n# Create directories\ntrain_path = '..input\/train'\nvalid_path = '..input\/valid'\ntest_path = '..\/input\/leaderboard_test_data'\nfor fold in [train_path, valid_path]:\n    for subf in [\"0\", \"1\"]:\n        os.makedirs(os.path.join(fold, subf),exist_ok=True)\n","3ebe52b2":"# Set the id as the index in df_data\ndf_data.set_index('image_id', inplace=True)\ndf_data.head()","bb5dee38":"df_train.head()","b184c710":"for image in df_train['image_id'].values:\n    # the id in the csv file does not have the .tif extension therefore we add it here\n    fname = image \n    label = str(df_data.loc[image,'has_oilpalm']) # get the label for a certain image\n    src = os.path.join('..\/input\/train_images\/train_images\/', fname)\n    dst = os.path.join(train_path, label, fname)\n    shutil.copyfile(src, dst)","1443f751":"for image in df_val['image_id'].values:\n    fname = image \n    label = str(df_data.loc[image,'has_oilpalm']) # get the label for a certain image\n    src = os.path.join('..\/input\/train_images\/train_images\/', fname)\n    dst = os.path.join(valid_path, label, fname)\n    shutil.copyfile(src, dst)","e4a1d229":"from keras.preprocessing.image import ImageDataGenerator\n\nIMAGE_SIZE = 256\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 32\nval_batch_size = 32\n\ntrain_steps = np.ceil(num_train_samples \/ train_batch_size)\nval_steps = np.ceil(num_val_samples \/ val_batch_size)\n\ndatagen = ImageDataGenerator(preprocessing_function=lambda x:(x - x.mean()) \/ x.std() if x.std() > 0 else x,\n                            horizontal_flip=True,\n                            vertical_flip=True)\n\ntrain_gen = datagen.flow_from_directory(train_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=train_batch_size,\n                                        class_mode='binary')\n\nval_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=val_batch_size,\n                                        class_mode='binary')\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=1,\n                                        class_mode='binary',\n                                        shuffle=False)","74470149":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\nfrom keras.layers import Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop, Adam\n\nkernel_size = (3,3)\npool_size= (2,2)\nfirst_filters = 32\nsecond_filters = 64\nthird_filters = 128\n\ndropout_conv = 0.3\ndropout_dense = 0.5\n\nmodel = Sequential()\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)))\nmodel.add(Conv2D(first_filters, kernel_size, use_bias=False))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPool2D(pool_size = pool_size)) \nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(second_filters, kernel_size, use_bias=False))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(Conv2D(second_filters, kernel_size, use_bias=False))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPool2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(third_filters, kernel_size, use_bias=False))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(Conv2D(third_filters, kernel_size, use_bias=False))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPool2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\n#model.add(GlobalAveragePooling2D())\nmodel.add(Flatten())\nmodel.add(Dense(256, use_bias=False))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(dropout_dense))\nmodel.add(Dense(1, activation = \"sigmoid\"))\n\n# Compile the model\nmodel.compile(Adam(0.01), loss = \"binary_crossentropy\", metrics=[\"accuracy\"])","131ceed2":"from keras.callbacks import EarlyStopping, ReduceLROnPlateau\nearlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)\nreducel = ReduceLROnPlateau(monitor='val_loss', patience=1, verbose=1, factor=0.1)\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                    validation_data=val_gen,\n                    validation_steps=val_steps,\n                    epochs=2,#very low\n                   callbacks=[reducel, earlystopper])","50031f3f":"test_gen","66b4eae8":"from sklearn.metrics import roc_curve, auc, roc_auc_score\nimport matplotlib.pyplot as plt\n\n# make a prediction\ny_pred_keras = model.predict_generator(test_gen, steps=len(df_val), verbose=1)\nfpr_keras, tpr_keras, thresholds_keras = roc_curve(test_gen.classes, y_pred_keras)\nauc_keras = auc(fpr_keras, tpr_keras)\nauc_keras","09abf3fa":"plt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_keras, tpr_keras, label='area = {:.3f}'.format(auc_keras))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()","2319e862":"base_test_dir = '..\/input\/leaderboard_test_data\/leaderboard_test_data'\nbase_test_dir2 = '..\/input\/leaderboard_holdout_data\/leaderboard_data'\ntest_file1 = glob(os.path.join(base_test_dir,'*.jpg'))\ntest_file2 = glob(os.path.join(base_test_dir2,'*.jpg'))\ntest_files=test_file1 + test_file2\nsubmission = pd.DataFrame()\nfile_batch = 5000\nmax_idx = len(test_files)","63a51a2e":"test_df = pd.DataFrame({'path': test_files})","c7edf7bb":"test_df","230b3532":"for idx in range(0, max_idx):\n    test_df['image_id'] = test_df.path.map(lambda x: x.split('\/')[4])\n    test_df['image'] = test_df['path'].map(imread)\n    K_test = np.stack(test_df[\"image\"].values)\n    K_test = (K_test - K_test.mean()) \/ K_test.std()\n    predictions = model.predict(K_test)\n    test_df['has_oilpalm'] = predictions\n    submission = pd.concat([submission, test_df[[\"image_id\", \"has_oilpalm\"]]])\nsubmission.head()","531dacc7":"test_df","449b0163":"## Train test split"}}