{"cell_type":{"15385525":"code","69317107":"code","7f598be1":"code","2b39c6c4":"code","d39848df":"code","87d8efa1":"code","9cbd25f3":"code","41b5acd6":"code","cdc94f08":"code","91a10bd0":"code","01751bed":"code","ad0a5120":"code","fbdaf9c4":"code","94239d43":"code","28869e5e":"code","0b37ccbf":"code","8b5ddc43":"code","4b88914d":"code","639c2319":"code","9d8dc575":"code","73d04b98":"code","7bf13283":"code","653c63b3":"code","98f64dd0":"code","cdd05452":"code","7aaf508a":"code","213dedfc":"code","c55629a2":"markdown","f05fff20":"markdown","41522190":"markdown","5ed35a2e":"markdown","4fe4c8f3":"markdown","01437a21":"markdown","5d6df498":"markdown","09337f5e":"markdown","7db06ec3":"markdown","cb48a8b9":"markdown","4302d9aa":"markdown","8856ac20":"markdown","fa31c4d4":"markdown","217cda1c":"markdown"},"source":{"15385525":"import re    # for regular expressions \nimport nltk  # for text manipulation \nimport string # for text manipulation \nimport warnings \nimport numpy as np \nimport pandas as pd # for data manipulation \nimport matplotlib.pyplot as plt\n\npd.set_option(\"display.max_colwidth\", 200) \nwarnings.filterwarnings(\"ignore\") #ignore warnings\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n%matplotlib inline","69317107":"data = pd.read_csv(\"\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\",encoding='latin-1')\ndata.head()","7f598be1":"DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"TweetText\"]\ndata.columns = DATASET_COLUMNS\ndata.head()","2b39c6c4":"data.drop(['ids','date','flag','user'],axis = 1,inplace = True)","d39848df":"data.head()","87d8efa1":"positif_data = data[data.target==4].iloc[:25000,:]\nprint(positif_data.shape)\nnegative_data = data[data.target==0].iloc[:1000,:]\nprint(negative_data.shape)","9cbd25f3":"data = pd.concat([positif_data,negative_data],axis = 0)\nprint(data.shape)\ndata.head()","41b5acd6":"data['Clean_TweetText'] = data['TweetText'].str.replace(\"@\", \"\") \ndata.head()","cdc94f08":"data['Clean_TweetText'] = data['Clean_TweetText'].str.replace(r\"http\\S+\", \"\") \ndata.head()","91a10bd0":"data['Clean_TweetText'] = data['Clean_TweetText'].str.replace(\"[^a-zA-Z]\", \" \") \ndata.head()","01751bed":"stopwords=nltk.corpus.stopwords.words('english')","ad0a5120":"def remove_stopwords(text):\n    clean_text=' '.join([word for word in text.split() if word not in stopwords])\n    return clean_text","fbdaf9c4":"data['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda text : remove_stopwords(text.lower()))\ndata.head()","94239d43":"data['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda x: x.split())\ndata.head()","28869e5e":"from nltk.stem.porter import * \nstemmer = PorterStemmer() \ndata['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda x: [stemmer.stem(i) for i in x])\ndata.head()","0b37ccbf":"data['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda x: ' '.join([w for w in x]))\ndata.head()","8b5ddc43":"data['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ndata.head()","4b88914d":"all_words = ' '.join([text for text in data['Clean_TweetText']])\n\nfrom wordcloud import WordCloud \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words) \n\nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","639c2319":"positive_words =' '.join([text for text in data['Clean_TweetText'][data['target'] == 4]]) \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(positive_words)\n\nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","9d8dc575":"depressive_words =' '.join([text for text in data['Clean_TweetText'][data['target'] == 0]]) \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(depressive_words)\n\nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","73d04b98":"from xgboost import XGBClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer","7bf13283":"count_vectorizer = CountVectorizer(stop_words='english') \ncv = count_vectorizer.fit_transform(data['Clean_TweetText'])\ncv.shape","653c63b3":"X_train,X_test,y_train,y_test = train_test_split(cv,data['target'] , test_size=.2,stratify=data['target'], random_state=42)","98f64dd0":"xgbc = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3)\nxgbc.fit(X_train,y_train)\nprediction_xgb = xgbc.predict(X_test)\nprint(accuracy_score(prediction_xgb,y_test))","cdd05452":"rf = RandomForestClassifier(n_estimators=1000, random_state=42)\nrf.fit(X_train,y_train)\nprediction_rf = rf.predict(X_test)\nprint(accuracy_score(prediction_rf,y_test))","7aaf508a":"lr = LogisticRegression()\nlr.fit(X_train,y_train)\nprediction_lr = lr.predict(X_test)\nprint(accuracy_score(prediction_lr,y_test))","213dedfc":"svc = svm.SVC()\nsvc.fit(X_train,y_train)\nprediction_svc = svc.predict(X_test)\nprint(accuracy_score(prediction_svc,y_test))","c55629a2":"Removing Twitter Handles (@user)","f05fff20":"Removing links","41522190":"Now let\u2019s stitch these tokens back together","5ed35a2e":"Removing small words","4fe4c8f3":"# Data Cleaning","01437a21":"What are the most common words in the entire dataset?","5d6df498":"In this section we will visualize the tweets using wordclouds.\nA wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes.","09337f5e":"Removing Punctuations, Numbers, and Special Characters","7db06ec3":"Text Tokenization and Normalization","cb48a8b9":"What are the most common words in the dataset for Positive and Depressive tweets, respectively?","4302d9aa":"Remove stop words","8856ac20":"# Model building","fa31c4d4":"Model Training","217cda1c":"Data Vizualisation"}}