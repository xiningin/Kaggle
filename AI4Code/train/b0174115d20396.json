{"cell_type":{"a2ad034f":"code","6002af62":"code","8a30ad6e":"code","6bbbe132":"code","f71acc3a":"code","6f91bcc8":"code","f13e53eb":"code","f4648500":"code","b277f093":"code","3a87d26f":"code","f254ed82":"code","b40f1ce4":"code","6adfc924":"code","02445a9f":"code","d82aaf5a":"code","a083e83b":"code","117d869c":"code","96dd55f3":"code","d7d5f4e7":"code","ce9cbb34":"code","e52ffceb":"code","6105f3db":"code","5ee5c310":"code","c7449496":"code","00c5180f":"code","30b3e152":"code","0f397ff4":"markdown","1e5339a8":"markdown","365c2d0d":"markdown","c1d9ee89":"markdown","7841fad7":"markdown","0515f9dc":"markdown","06c93690":"markdown","c6c7d1dc":"markdown"},"source":{"a2ad034f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport os\nfrom tqdm import tqdm\nimport gc\nimport pickle","6002af62":"train = pd.read_csv(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/train.csv\")\nsample_submission = pd.read_csv(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/sample_submission.csv\")","8a30ad6e":"train.head()","6bbbe132":"train.shape","f71acc3a":"train.isna().sum(axis=0)","6f91bcc8":"sns.distplot(train['time_to_eruption'])","f13e53eb":"sample_submission.head()","f4648500":"sample_submission.shape","b277f093":"# take 1000015382.csv for example\ntest_data = pd.read_csv('..\/input\/predict-volcanic-eruptions-ingv-oe\/train\/1000015382.csv')","3a87d26f":"print('shape of 1000015382.csv', test_data.shape)\ntest_data.head()","f254ed82":"fig, axs = plt.subplots(2, 5, figsize=(50, 30))\n\nfor i, ax in enumerate(axs.ravel()):\n    ax.plot(test_data['sensor_'+str(i+1)])\n    ax.set_title('sensor_'+str(i+1))","b40f1ce4":"sns.distplot(test_data['sensor_1'], label='sensor_1')\nsns.distplot(test_data['sensor_2'], label='sensor_2')\nsns.distplot(test_data['sensor_3'], label='sensor_3')\nplt.legend()","6adfc924":"print(train.loc[train['segment_id']==1000015382])","02445a9f":"test_data2 = pd.read_csv('..\/input\/predict-volcanic-eruptions-ingv-oe\/test\/1001028887.csv')\n\nfig, axs = plt.subplots(2, 5, figsize=(50, 30))\n\nfor i, ax in enumerate(axs.ravel()):\n    ax.plot(test_data2['sensor_'+str(i+1)])\n    ax.set_title('sensor_'+str(i+1))","d82aaf5a":"sns.distplot(test_data2['sensor_1'], label='sensor_1')\nsns.distplot(test_data2['sensor_4'], label='sensor_4')\nsns.distplot(test_data2['sensor_3'], label='sensor_3')\nplt.legend()","a083e83b":"test_data3 = pd.read_csv('..\/input\/predict-volcanic-eruptions-ingv-oe\/train\/1000745424.csv')\n\nfig, axs = plt.subplots(2, 5, figsize=(50, 30))\n\nfor i, ax in enumerate(axs.ravel()):\n    ax.plot(test_data3['sensor_'+str(i+1)])\n    ax.set_title('sensor_'+str(i+1))","117d869c":"sns.distplot(test_data3['sensor_1'], label='sensor_1')\nsns.distplot(test_data3['sensor_2'], label='sensor_2')\nsns.distplot(test_data3['sensor_3'], label='sensor_3')\nplt.legend()","96dd55f3":"print(train.loc[train['segment_id']==1000745424])","d7d5f4e7":"# generate feature\n# collect mean \/ std \/ 5 \/ 10 \/ 20 \/ 40 percentile \/ min \/ max \/ +5000 \/ +10000 \/ +20000 self-corr\ndef generate_feature():\n    \n    def helper(path):\n        data = []\n        for file in tqdm(os.listdir(path)):\n            tmp = []\n            file_path = os.path.join(path, file)\n            d = pd.read_csv(file_path)\n            tmp.append(eval(file[:-4]))\n            # mean\n            tmp += d.mean(axis=0).values.astype('float32').tolist()\n            # std\n            tmp += d.std(axis=0).values.astype('float32').tolist()\n            # min\n            tmp += d.min(axis=0).values.astype('float32').tolist()\n            # max\n            tmp += d.max(axis=0).values.astype('float32').tolist()\n            # 5 percentile\n            tmp += d.quantile(0.05, axis=0).values.astype('float32').tolist()\n            # 10 percentile\n            tmp += d.quantile(0.1, axis=0).values.astype('float32').tolist()\n            # 20 percentile\n            tmp += d.quantile(0.2, axis=0).values.astype('float32').tolist()\n            # 40 percentile\n            tmp += d.quantile(0.4, axis=0).values.astype('float32').tolist()\n            # shift\n            for col in d:\n                d[col+'_5000'] = d[col].shift(5000)\n                d[col+'_10000'] = d[col].shift(5000)\n                d[col+'_20000'] = d[col].shift(5000)\n            # +5000 \/ +10000 \/ +20000 self-corr\n            for col in d.columns[:10]:\n                col1 = col+'_5000'\n                col2 = col+'_10000'\n                col3 = col+'_20000'\n                tmp1 = d.loc[:, [col, col1]].dropna()\n                tmp2 = d.loc[:, [col, col2]].dropna()\n                tmp3 = d.loc[:, [col, col3]].dropna()\n                tmp += [tmp1[col].corr(tmp1[col1]), tmp2[col].corr(tmp2[col2]), tmp3[col].corr(tmp3[col3])]\n                \n            data.append(tmp)\n        return data\n                   \n    print('train_part: ')\n    train_part_fea = helper('..\/input\/predict-volcanic-eruptions-ingv-oe\/train')\n    print('test_part: ')\n    test_part_fea = helper('..\/input\/predict-volcanic-eruptions-ingv-oe\/test')\n    \n    return train_part_fea, test_part_fea","ce9cbb34":"#def na_mark(data, file_has_na_name):\n#    name = set([eval(i[:-4]) for i in file_has_na_name])\n#    data['na_mark'] = 0\n#    data.loc[data['segment_id'].isin(name), 'na_mark'] = 1","e52ffceb":"train_part_fea, test_part_fea = generate_feature()\n\nwith open('train_part_fea.pkl', 'wb') as f1:\n    pickle.dump(train_part_fea, f1)\n    \nwith open('test_part_fea.pkl', 'wb') as f2:\n    pickle.dump(test_part_fea, f2)","6105f3db":"#train_part_fea = pd.read_pickle('..\/input\/ingv-eda-basemodel\/train_part_fea.pkl')\n#test_part_fea = pd.read_pickle('..\/input\/ingv-eda-basemodel\/test_part_fea.pkl')","5ee5c310":"base_colname = ['sensor_'+str(i) for i in range(1, 11)]\nfea_colname = ['segment_id'] + [j + '_mean' for j in base_colname] + [j + '_std' for j in base_colname] + \\\n                [j + '_min' for j in base_colname] + [j + '_max' for j in base_colname] + \\\n                    [j + '_5_quant' for j in base_colname] + [j + '_10_quant' for j in base_colname] + \\\n                        [j + '_20_quant' for j in base_colname] + [j + '_40_quant' for j in base_colname] + \\\n                    [j + i for j in base_colname for i in ['_5000_self_corr', '_10000_self_corr', '_20000_self_corr']]\n\ntrain = pd.merge(train, pd.DataFrame(train_part_fea, columns=fea_colname), on='segment_id', how='left')\nsample_submission = pd.merge(sample_submission, pd.DataFrame(test_part_fea, columns=fea_colname), on='segment_id', how='left')","c7449496":"X_train, X_val, y_train, y_val = train_test_split(train.drop(['segment_id', 'time_to_eruption'], axis=1).values, \n                                                    train['time_to_eruption'].values, \n                                                    test_size=0.25, random_state=42)","00c5180f":"import lightgbm as lgb\n\ntrain_data = lgb.Dataset(X_train, label=y_train)\nval_data = lgb.Dataset(X_val, y_val, reference=train_data,)\n\n#params = {'objective': 'mae', \n#          'num_iterations': ,\n#          'learning_rate': , \n#          'num_leaves': ,\n#          'seed': ,\n#          'metric': 'mae'}\n\nparams = { 'num_leaves': 85,\n          'n_estimators': 6000,\n    'min_data_in_leaf': 10, \n    'objective':'mae',\n    'max_depth': -1,\n    'learning_rate': 0.01,\n    'max_bins': 2048,\n    \"boosting\": \"gbdt\",\n    \"feature_fraction\": 0.91,\n    \"bagging_freq\": 1,\n    \"bagging_fraction\": 0.91,\n    \"bagging_seed\": 42,\n    \"metric\": 'mae',\n    \"lambda_l1\": 0.1,\n    \"verbosity\": -1,\n    \"nthread\": -1,\n    \"random_state\": 42}\n\nmodel = lgb.train(params=params, train_set=train_data, valid_sets=[train_data, val_data], valid_names=['train', 'val'], \n                  early_stopping_rounds=50)","30b3e152":"submission = pd.DataFrame({'segment_id': sample_submission['segment_id'].values, \n    'time_to_eruption': model.predict(sample_submission.iloc[:, 2:].values)})\nsubmission.to_csv('submission.csv', index=False)","0f397ff4":"### example 3 in train","1e5339a8":"##  the datas in the ten minutes of logs are very different","365c2d0d":"### example1 in train","c1d9ee89":"***MODEL***","7841fad7":"### example2 in test","0515f9dc":"# train and submission","06c93690":"## predict","c6c7d1dc":"# Data files"}}