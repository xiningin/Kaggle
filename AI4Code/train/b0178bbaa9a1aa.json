{"cell_type":{"31782907":"code","b3318b50":"code","18df6d47":"code","f1b65ae7":"code","8906198d":"code","58539b9e":"code","a7fd54f6":"code","48a8c5b3":"code","7d7a0a94":"code","d8a7108d":"code","2126f620":"code","acd0761c":"code","65d80cc4":"code","c95ccb09":"code","2b809e2c":"code","da1bed46":"code","4b00f88e":"code","4417309c":"code","dc4fc7a8":"code","bed2fb95":"code","732da6bd":"code","f3efbe4b":"code","6d8cdc45":"code","7447861c":"code","b4a6ae40":"code","a51da526":"code","9a78a1bb":"code","51d2c5fa":"code","dfe2d608":"code","61d8be1b":"code","8a48c043":"code","cd84087a":"code","dc24905f":"code","b9295d17":"code","7f5ef06c":"code","92c2b60a":"code","60ca0ec9":"code","a4159d93":"code","4b16b38f":"code","350ecee2":"code","47532c24":"code","9f24eb02":"code","ee683898":"code","d41c4ecb":"code","56bfc611":"code","d7d32a5c":"code","13bd3bdd":"code","c9739be6":"code","473a92f7":"markdown","1da5cb8a":"markdown","bd471960":"markdown","efcd16ef":"markdown","383d9079":"markdown","9a6a5096":"markdown","7f40a11a":"markdown","97b34db0":"markdown","77cdfffb":"markdown","fdbd8f21":"markdown","679d6d50":"markdown","205e2c9a":"markdown","fd98e306":"markdown","723f40c1":"markdown","9068d8a9":"markdown","f39de85c":"markdown","49ed7930":"markdown"},"source":{"31782907":"%cd \/kaggle\/\n%ls","b3318b50":"!mkdir training\n%cd training","18df6d47":"# Download YOLOv5\n!git clone https:\/\/github.com\/USC-InfoLab\/rddc2020.git  # clone repo\n!cp rddc2020\/* .\/ -r\n!rm rddc2020 -r\n# Install dependencies\n%pip install -qr requirements.txt  # install dependencies\n%cd yolov5\n\n%cd ..\/\n\nimport torch\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")","f1b65ae7":"# Import lib\nimport os\nimport gc\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport json\nimport yaml\nfrom shutil import copyfile\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","8906198d":"%cd \/kaggle\/training\/\n%ls","58539b9e":"# Firstly, We will transfer the coco format to yolo format\n\n#load json file\njson_file_path = '\/kaggle\/input\/cowboyoutfits\/train.json'\n\ndata = json.load(open(json_file_path, 'r'))\nyolo_anno_path = '\/kaggle\/training\/yolo_anno\/'\n\nif not os.path.exists(yolo_anno_path):\n    os.makedirs(yolo_anno_path)","a7fd54f6":"# \u9700\u8981\u6ce8\u610f\u4e0b\u56e0\u4e3a\u6211\u4eec\u7684annotation lable\u662f\u4e0d\u8fde\u7eed\u7684,\u4f1a\u5bfc\u81f4\u540e\u9762\u62a5\u9519,\u6240\u4ee5\u8fd9\u91cc\u751f\u6210\u4e00\u4e2amap\u6620\u5c04\ncate_id_map = {}\nnum = 0\nfor cate in data['categories']:\n    cate_id_map[cate['id']] = num\n    num+=1","48a8c5b3":"cate_id_map","7d7a0a94":"#\u5bf9\u6bd4\u4e0b\ndata['categories']","d8a7108d":"# convert the bounding box from COCO to YOLO format.\n\ndef cc2yolo_bbox(img_width, img_height, bbox):\n    dw = 1. \/ img_width\n    dh = 1. \/ img_height\n    x = bbox[0] + bbox[2] \/ 2.0\n    y = bbox[1] + bbox[3] \/ 2.0\n    w = bbox[2]\n    h = bbox[3]\n \n    x = x * dw\n    w = w * dw\n    y = y * dh\n    h = h * dh\n    return (x, y, w, h)","2126f620":"# transfer the annotation, and generated a train dataframe file\nf = open('train.csv','w')\nf.write('id,file_name\\n')\nfor i in tqdm(range(len(data['images']))):\n    filename = data['images'][i]['file_name']\n    img_width = data['images'][i]['width']\n    img_height = data['images'][i]['height']\n    img_id = data['images'][i]['id']\n    yolo_txt_name = filename.split('.')[0] + '.txt' #remove .jpg\n    \n    f.write('{},{}\\n'.format(img_id, filename))\n    yolo_txt_file = open(os.path.join(yolo_anno_path, yolo_txt_name), 'w')\n    \n    for anno in data['annotations']:\n        if anno['image_id'] == img_id:\n            yolo_bbox = cc2yolo_bbox(img_width, img_height, anno['bbox']) # \"bbox\": [x,y,width,height]        \n            yolo_txt_file.write('{} {} {} {} {}\\n'.format(cate_id_map[anno['category_id']], yolo_bbox[0], yolo_bbox[1], yolo_bbox[2], yolo_bbox[3]))\n    yolo_txt_file.close()\nf.close()","acd0761c":"# generate training dataframe\ntrain = pd.read_csv('\/kaggle\/training\/train.csv')\ntrain.head()","65d80cc4":"train_df, valid_df = train_test_split(train, test_size=0.01, random_state=42)\n\nprint(f'Size of total training images: {len(train)}, training images: {len(train_df)}. validation images: {len(valid_df)}')\n\n# \u8bf4\u660e\u4e0b\uff0c\u8fd9\u91cc\u7ed9\u7684validation set \u5c310.01\uff0c\u662f\u56e0\u4e3a\u8fd9\u6b21\u7684\u6bd4\u8d5b\u6311\u6218\u4e4b\u4e00\u5c31\u662f\u6570\u636e\u96c6\u5f88\u5c0f\uff0c\u8fd8\u662f\u5e0c\u671b\u80fd\u591f\u7ed9\u66f4\u591a\u7684\u6570\u636e\u6765\u8bad\u7ec3\u3002\n# \u5176\u6b21\uff0c\u8001\u5e08\u5df2\u7ecf\u5212\u5206\u597d\u4e86validation\u96c6\u4e86\uff0c\u8fd9\u91cc\u76840.01\u7684\u9a8c\u8bc1\u66f4\u591a\u662f\u6d4b\u8bd5\u81ea\u5df1\u7b97\u6cd5\u662f\u5426\u6709\u9519\u8bef\uff0c\u771f\u6b63\u770bperformance\u8fd8\u662f\u63d0\u4ea4\u4e0a\u53bb\u770b\u597d\u3002\n# 100\u6b21\u7684\u63d0\u4ea4\u4e5f\u8db3\u591f\u9a8c\u8bc1performance\u4e86.","c95ccb09":"# generate new train data frame with spliter mark\n\ntrain_df.loc[:, 'split'] = 'train'\nvalid_df.loc[:, 'split'] = 'valid'\ndf = pd.concat([train_df, valid_df]).reset_index(drop=True)\ndf.sample(10)","2b809e2c":"%cd \/kaggle\/training\/\n%ls","da1bed46":"# mdke directory for traning section\nos.makedirs('\/kaggle\/training\/cowboy\/images\/train', exist_ok=True)\nos.makedirs('\/kaggle\/training\/cowboy\/images\/valid', exist_ok=True)\n\nos.makedirs('\/kaggle\/training\/cowboy\/labels\/train', exist_ok=True)\nos.makedirs('\/kaggle\/training\/cowboy\/labels\/valid', exist_ok=True)\n\n%ls","4b00f88e":"# move the images and annotations to relevant splited folders\n\nfor i in tqdm(range(len(df))):\n    row = df.loc[i]\n    name = row.file_name.split('.')[0]\n    if row.split == 'train':\n        copyfile(f'\/kaggle\/input\/cowboyoutfits\/images\/{name}.jpg', f'\/kaggle\/training\/cowboy\/images\/train\/{name}.jpg')\n        copyfile(f'\/kaggle\/training\/yolo_anno\/{name}.txt', f'\/kaggle\/training\/cowboy\/labels\/train\/{name}.txt')\n    else:\n        copyfile(f'\/kaggle\/input\/cowboyoutfits\/images\/{name}.jpg', f'\/kaggle\/training\/cowboy\/images\/valid\/{name}.jpg')\n        copyfile(f'\/kaggle\/training\/yolo_anno\/{name}.txt', f'\/kaggle\/training\/cowboy\/labels\/valid\/{name}.txt')","4417309c":"# Create  yaml file\n\ndata_yaml = dict(\n    train = '..\/cowboy\/images\/train\/',\n    val = '..\/cowboy\/images\/valid',\n    nc = 5,\n    names = ['belt', 'sunglasses', 'boot', 'cowboy_hat', 'jacket']\n)\n\n# we will make the file under the yolov5\/data\/ directory.\nwith open('\/kaggle\/training\/yolov5\/data\/data.yaml', 'w') as outfile:\n    yaml.dump(data_yaml, outfile, default_flow_style=True)\n    \n%cat \/kaggle\/training\/yolov5\/data\/data.yaml # show your YAML file","dc4fc7a8":"# Hyperparameters for COCO training from scratch\n# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300\n# See tutorials for hyperparameter evolution https:\/\/github.com\/ultralytics\/yolov5#tutorials\n\n\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.937  # SGD momentum\/Adam beta1\nweight_decay: 0.0005  # optimizer weight decay 5e-4\nwarmup_epochs: 3.0  # warmup epochs (fractions ok)\nwarmup_momentum: 0.8  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\niou_t: 0.20  # IoU training threshold\nanchor_t: 4.0  # anchor-multiple threshold\n# anchors: 3  # anchors per output layer (0 to ignore)\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nhsv_h: 0.015  # image HSV-Hue augmentation (fraction)\nhsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.4  # image HSV-Value augmentation (fraction)\ndegrees: 0.0  # image rotation (+\/- deg)\ntranslate: 0.1  # image translation (+\/- fraction)\nscale: 0.5  # image scale (+\/- gain)\nshear: 0.0  # image shear (+\/- deg)\nperspective: 0.0  # image perspective (+\/- fraction), range 0-0.001\nflipud: 0.0  # image flip up-down (probability)\nfliplr: 0.5  # image flip left-right (probability)\nmosaic: 1.0  # image mosaic (probability)\nmixup: 0.0  # image mixup (probability)\ncopy_paste: 0.0  # segment copy-paste (probability)","bed2fb95":"IMG_SIZE = 640  # the default image size in yolo is 640, it will automated resize our image during training and valudation.\nBATCH_SIZE = 16 # wisely choose, please check my W&B page for differencet performance with size [16,32,64]\nEPOCHS = 20\nMODEL = 'yolov5x.pt'  #","732da6bd":"# Create  yaml file\n\nhyper_yaml = dict(\n    lr0=0.01,  # initial learning rate (SGD=1E-2, Adam=1E-3)\n    lrf=0.2,  # final OneCycleLR learning rate (lr0 * lrf)\n    momentum=0.937,  # SGD momentum\/Adam beta1\n    weight_decay=0.0005,  # optimizer weight decay 5e-4\n    warmup_epochs=3.0,  # warmup epochs (fractions ok)\n    warmup_momentum=0.8,  # warmup initial momentum\n    warmup_bias_lr=0.1,  # warmup initial bias lr\n    giou=0.05,  # box loss gain\n    cls=0.5,  # cls loss gain\n    cls_pw=1.0,  # cls BCELoss positive_weight\n    obj=1.0,  # obj loss gain (scale with pixels)\n    obj_pw=1.0,  # obj BCELoss positive_weight\n    iou_t=0.20,  # IoU training threshold\n    anchor_t=4.0,  # anchor-multiple threshold\n    # anchors: 3  # anchors per output layer (0 to ignore)\n    fl_gamma=1.0,  # focal loss gamma (efficientDet default gamma=1.5)\n    hsv_h=0.015,  # image HSV-Hue augmentation (fraction)\n    hsv_s=0.7,  # image HSV-Saturation augmentation (fraction)\n    hsv_v=0.4,  # image HSV-Value augmentation (fraction)\n    degrees=0,  # image rotation (+\/- deg)\n    translate=0.1,  # image translation (+\/- fraction)\n    scale=0.5,  # image scale (+\/- gain)\n    shear=0.0,  # image shear (+\/- deg)\n    perspective=0.0,  # image perspective (+\/- fraction), range 0-0.001\n    flipud=0.0,  # image flip up-down (probability)\n    fliplr=0.5,  # image flip left-right (probability)\n    mosaic=1.0,  # image mosaic (probability)\n    mixup=0.0  # image mixup (probability)\n)\n\n# we will make the file under the yolov5\/data\/ directory.\nwith open('\/kaggle\/training\/yolov5\/data\/hyper.yaml', 'w') as outfile:\n    yaml.dump(hyper_yaml, outfile, default_flow_style=True)\n    \n%cat \/kaggle\/training\/yolov5\/data\/hyper.yaml # show your YAML file","f3efbe4b":"# we are ready to training our model with w&b\n%cd \/kaggle\/training\/yolov5\/","6d8cdc45":"# !python train.py --batch {BATCH_SIZE} \\\n#                  --epochs {EPOCHS} \\\n#                  --data data.yaml \\\n#                  --weights {MODEL} \\\n#                  --img-size {IMG_SIZE}\\\n#                  --hyp hyper.yaml\\\n# #                  --multi-scale","7447861c":"# In the Dev phase, we will only use the valid data for predicition. \n# Don't forget change it to test data in the Final pahse.\n\nvalid_df = pd.read_csv('\/kaggle\/input\/cowboyoutfits\/valid.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/cowboyoutfits\/test.csv')\nprint(valid_df.head())\nprint(test_df.head())","b4a6ae40":"valid_df.shape","a51da526":"%cd \/kaggle\/training\/\n%ls","9a78a1bb":"# make directory to store the validation data.\nos.makedirs('\/kaggle\/inference\/valid', exist_ok=True)\nos.makedirs('\/kaggle\/inference\/test', exist_ok=True)","51d2c5fa":"# copy the validation image to inference folder for detection process\nfor i in tqdm(range(len(valid_df))):\n    row = valid_df.loc[i]\n    name = row.file_name.split('.')[0]\n#     print('Copying ', f'\/kaggle\/inference\/valid\/{name}.jpg')\n    copyfile(f'\/kaggle\/input\/cowboyoutfits\/images\/{name}.jpg', f'\/kaggle\/inference\/valid\/{name}.jpg')\n    \nfor i in tqdm(range(len(test_df))):\n    row = test_df.loc[i]\n    name = row.file_name.split('.')[0]\n#     print('Copying ', f'\/kaggle\/inference\/test\/{name}.jpg')\n    copyfile(f'\/kaggle\/input\/cowboyoutfits\/images\/{name}.jpg', f'\/kaggle\/inference\/test\/{name}.jpg')","dfe2d608":"VALID_PATH = '\/kaggle\/inference\/valid\/'\nTEST_PATH = '\/kaggle\/inference\/test\/'\nMODEL_PATH = '\/kaggle\/input\/yolov5x-640-16-gamma1\/yolov5x_640_16_gamma1_best.pt'\nIMAGE_PATH = '\/kaggle\/input\/cowboyoutfits\/images\/'","61d8be1b":"CONF = 0.4\nIOU = 0.9","8a48c043":"# go to yolov5 main folder for detection\n%cd \/kaggle\/training\/yolov5\/","cd84087a":"!python detect.py --weights {MODEL_PATH} \\\n                  --source {VALID_PATH} \\\n                  --conf {CONF} \\\n                  --iou-thres {IOU} \\\n                  --save-txt \\\n                  --output \/kaggle\/inference\/output\/valid \\\n#                   --save-conf \\\n                  --augment","dc24905f":"len(os.listdir('\/kaggle\/inference\/output\/valid'))\/2","b9295d17":"!python detect.py --weights {MODEL_PATH} \\\n                  --source {TEST_PATH} \\\n                  --conf {CONF} \\\n                  --iou-thres {IOU} \\\n                  --save-txt \\\n                  --output \/kaggle\/inference\/output\/test \\\n#                   --save-conf \\\n                  --augment","7f5ef06c":"print(len(os.listdir('\/kaggle\/inference\/output\/valid'))\/2)\nprint(len(os.listdir('\/kaggle\/inference\/output\/test'))\/2)","92c2b60a":"# read the output log , indicated our prediction result was saved under `runs\/detect\/exp\/`\n\nPRED_PATH = '\/kaggle\/inference\/output\/'","60ca0ec9":"import random\nf_name = valid_df.sample(1)['file_name'].item().split('.')[0]\nprint(f_name)\n# f_name = random.sample(os.listdir(PRED_PATH + 'valid\/'),1)[0].split('.')[0]\nwith open(PRED_PATH + 'valid\/' + f_name+'.txt', 'r') as file:\n    for line in file:\n        print(line)","a4159d93":"from PIL import Image\nImage.open(PRED_PATH + 'valid\/' + f_name+'.jpg')","4b16b38f":"# list our prediction files path\nprediction_files_v = os.listdir(PRED_PATH + 'valid\/')\nprediction_files_t = os.listdir(PRED_PATH + 'test\/')\nprint('Number of valid images with detections: ', len(prediction_files_v))\nprint('Number of test images with detections: ', len(prediction_files_t))","350ecee2":"# convert yolo to coco annotation format\ndef yolo2cc_bbox(img_width, img_height, bbox):\n    x = (bbox[0] - bbox[2] * 0.5) * img_width\n    y = (bbox[1] - bbox[3] * 0.5) * img_height\n    w = bbox[2] * img_width\n    h = bbox[3] * img_height\n    \n    return (x, y, w, h)","47532c24":"# reverse the categories numer to the origin id\nre_cate_id_map = dict(zip(cate_id_map.values(), cate_id_map.keys()))\n\nprint(re_cate_id_map)","9f24eb02":"def make_submission(df, PRED_PATH, IMAGE_PATH, prediction_files):\n    output = []\n    for i in tqdm(range(len(df))):\n        row = df.loc[i]\n        image_id = row['id']\n        file_name = row['file_name'].split('.')[0]\n#         print(file_name)\n        if f'{file_name}.txt' in prediction_files:\n            img = Image.open(f'{IMAGE_PATH}\/{file_name}.jpg')\n            width, height = img.size\n            with open(f'{PRED_PATH}\/{file_name}.txt', 'r') as file:\n                for line in file:\n                    preds = line.strip('\\n').strip().split(' ') \n                    preds = list(map(float, preds)) #conver string to float\n                    cc_bbox = yolo2cc_bbox(width, height, preds[1:])\n                    result = {\n                        'image_id': image_id,\n                        'category_id': re_cate_id_map[preds[0]],\n                        'bbox': cc_bbox,\n                        'score': preds[-1]\n                    }\n\n                    output.append(result)\n    return output","ee683898":"sub_data_v = make_submission(valid_df, PRED_PATH + 'valid\/', IMAGE_PATH, prediction_files_v)\nsub_data_t = make_submission(test_df, PRED_PATH + 'test\/', IMAGE_PATH, prediction_files_t)","d41c4ecb":"op_pd_v = pd.DataFrame(sub_data_v)\nop_pd_t = pd.DataFrame(sub_data_t)","56bfc611":"op_pd_v.head()","d7d32a5c":"import zipfile \n\nop_pd_v.to_json('\/kaggle\/working\/answer.json',orient='records')\nzf = zipfile.ZipFile('\/kaggle\/working\/answer_valid.zip', 'w')\nzf.write('\/kaggle\/working\/answer.json', 'answer.json')\nzf.close()\n\nop_pd_t.to_json('\/kaggle\/working\/answer.json',orient='records')\nzf = zipfile.ZipFile('\/kaggle\/working\/answer_test.zip', 'w')\nzf.write('\/kaggle\/working\/answer.json', 'answer.json')\nzf.close()","13bd3bdd":"# mkdir \/kaggle\/working\/weights","c9739be6":"# cp {MODEL_PATH} \/kaggle\/working\/weights\/yolov5x_640_16_gamma1_best.pt","473a92f7":"# weights has been saved in dataset","1da5cb8a":"# Transfer annotations bbox from COCO to YOLO format","bd471960":"# Inference Hyperparameter\nYOLOv5 also provided a lot of hyperparameters for inference process, we can check the detect.py file for detail\n\nHere I list some parameters:\n* --weights {MODEL_PATH} \\ # path to the best model.\n* --source {TEST_PATH} \\ # absolute path to the test images.\n* --img {IMG_SIZE} \\ # Size of image\n* --conf 0.25 \\ # Confidence threshold (default is 0.25)\n* --iou-thres 0.45 \\ # IOU threshold (default is 0.45)\n* --max-det 100 \\ # Number of detections per image (default is 1000) \n* --save-txt \\ # Save predicted bounding box coordinates as txt files\n* --save-conf # Save the confidence of prediction for each bounding box\n* --augment # augmented inference, TTA\n* --project 'runs\/detect'  # save results to project\/name\n* --name 'exp'  # save results to project\/name\n* --half False  # use FP16 half-precision inference","efcd16ef":"## visualize our prediction","383d9079":"# Create .yaml file\n\nThe `data.yaml` file is the dataset configuration file that contains information about the datast like path of images, annotaions. \nWe should specific the following items:\n*     the path of our training and validation data\n*     the number of classes to be detected\n*     the names corresponding to those classes\n\n> \u203c\ufe0f in this competition, the categories_id provided in the train.json is not a sequential number, we need re_map it.or we will get an error here.\n\n> !! we can put our `YAML` file anywhere, since we can reference the path later, but can also simply put it in the `data` folder under `yolov5`","9a6a5096":"# Prepare required data folder structure \n\nThe Yolov5 requires a specific directory structure for custom training, you can get more information from the [official example](https:\/\/github.com\/ultralytics\/yolov5\/wiki\/Train-Custom-Data)\n\n```\n\/training    --temp_traning space\n    \/dataset --dataset fold, split into training and validation\n         \/images\n         \/labels\n    \/yolov5  --yolov5 main\n```","7f40a11a":"# GO Detection","97b34db0":"there are tons of information in the json file,  you should check it by your self. \n\n>Example code like: \n`data['info']`\n`data['image']`\n`data['annotations']`\n`data['categories']`\n\nIt can help us get better understand about the json structure. Some features might help like `data['annotations'][i][iscrowd]`. \n","77cdfffb":"# Make Submission","fdbd8f21":"# GO TRAIN","679d6d50":"We will use the default settings for inference. Here, we will simply introduced one example to pick the right confidence score which is based on our F1 score which is showed below. It can be access from our W&B result page. You can get access it from [here](https:\/\/wandb.ai\/momo233\/kaggle-cwoboy\/runs\/2rgyap5g?workspace=user-momo233). From the F1 score, we just pick the confidence with 0.546. it seems a little bit high, but can cover all the classes.\n\n<img src=\"https:\/\/api.wandb.ai\/files\/momo233\/kaggle-cwoboy\/2rgyap5g\/media\/images\/Results_20_2.png\" alt=\"drawing\" width=\"600\"\/>","205e2c9a":"# Splitting data into training and validation","fd98e306":"# Inference Section\n\nOnce we finished the training section, we should refer to [W&B Artifacts tab](https:\/\/wandb.ai\/momo233\/kaggle-cwoboy\/artifacts\/model\/run_2rgyap5g_model\/4141ab8a754564a37bb1) to choose our target model for inference.\n\n>Download the `best` performance model and upload to kaggle.","723f40c1":"# Training Hyperparameters","9068d8a9":"There are about 25 hyperparameters in tranining setting.You can get access it from `yolov5\/data\/hyps\/hyp.scratch.yaml`. Here is useful [discussion](https:\/\/github.com\/ultralytics\/yolov5\/issues\/607) about the hyperparameters in yolov5. \nHere is the example content:","f39de85c":"# Cowboy Outfits Detection\nIn this notebook, base library used is yolov5 for rddc2020 lib which enables model ensemble <br>\nThe data transfer refers to https:\/\/www.kaggle.com\/sheepwang\/training-cowboy-object-detection-yolov5?rvi=1","49ed7930":"# Save Weights"}}