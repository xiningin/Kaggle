{"cell_type":{"01654b64":"code","c51eee12":"code","4cdcb4b0":"code","5303f9b7":"code","ab9dcaf4":"code","07f56bd4":"code","616abdbd":"code","5744896c":"code","94f5c1fb":"code","ba71e5a1":"code","5b2fc3ca":"code","5141d45e":"code","9bb35e89":"code","e684d8f8":"code","0e81504d":"code","ae1d339d":"code","c8bbc699":"code","c29883a6":"code","dba16c4e":"code","b6a2db65":"code","3f823cd6":"code","7b14fc0f":"code","ffdc2cf8":"code","4516ad73":"code","4e6cadca":"code","c09f5202":"code","702ccf29":"code","feef4401":"code","5d3cef92":"code","9cf974c8":"code","fcf4d8bb":"code","cf2c3750":"code","72fb0221":"code","05b74573":"code","5b5bfb31":"code","9770372e":"code","e32d422b":"code","1b9d9d6e":"code","168d7c82":"code","64b17d38":"code","fa013943":"code","fc683d49":"code","d361e3c1":"code","6e29d7a4":"code","0f99fd5a":"code","64591061":"code","1f1c41b3":"code","e678f273":"code","389dff9f":"code","510b51d7":"code","0eec6c4a":"code","6463d63c":"code","1b28a541":"code","b5de1ace":"code","b0071159":"code","1662240b":"code","dc85d4a2":"code","c411926b":"code","5598c6bf":"code","49af38e3":"code","a7eb0e86":"code","274fd308":"code","a5b41655":"markdown","59cf87cd":"markdown","2f1f7de3":"markdown","220f75cd":"markdown","374c9b10":"markdown","e5c85239":"markdown","ac0bdafb":"markdown","ed12eefc":"markdown","708ba838":"markdown","1e98832f":"markdown","77acd8de":"markdown","0dde4a2e":"markdown","3fd78042":"markdown","e2e680ed":"markdown","9cd75d25":"markdown","8fe6ab63":"markdown","163d98f0":"markdown","66d23189":"markdown","53094d7f":"markdown","e6a4b4f7":"markdown","33d84137":"markdown","7231b2fa":"markdown","7490316d":"markdown","938ef8c6":"markdown","0a427faa":"markdown","19df6ca2":"markdown","9b42194c":"markdown","38f74dd3":"markdown","ef10de05":"markdown","4caca8a3":"markdown","4bfde3e6":"markdown","b73cdbc7":"markdown","c7933beb":"markdown","0476563b":"markdown","1dbe1d16":"markdown","692657b6":"markdown","20b743cd":"markdown","0b79367f":"markdown","1ee63ad0":"markdown","d966bb13":"markdown","efb21a3c":"markdown","24264100":"markdown","d10810ca":"markdown","f37c1cd6":"markdown","82131566":"markdown","89deb654":"markdown","83e71fb7":"markdown","cd32cac0":"markdown","8a45c378":"markdown","d41fa835":"markdown","fecacc9c":"markdown","226e5fc6":"markdown","595d62cb":"markdown"},"source":{"01654b64":"%%capture\n!pip install pycaret[full]","c51eee12":"import os \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nimport missingno as msno\nfrom scipy import stats\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4cdcb4b0":"from pycaret.classification import *","5303f9b7":"from collections import Counter\nfrom sklearn.model_selection import StratifiedKFold , train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve,auc\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import AdaBoostClassifier , GradientBoostingClassifier \n\nimport optuna\n","ab9dcaf4":"MAIN_PATH = \"..\/input\/song-popularity-prediction\/\"\ntdf = pd.read_csv(MAIN_PATH + 'train.csv')\nTdf = pd.read_csv(MAIN_PATH + 'test.csv')\nSdf = pd.read_csv(MAIN_PATH + 'sample_submission.csv')","07f56bd4":"tdf.head(3)\n","616abdbd":"EDA = setup(data = tdf , target = 'song_popularity', profile =True ,silent =True ,session_id =47 ) \n## profile --plots pycaret profile \n## silent -- removes checkout buttons i.e. won't ask for 'enter'\n## session_id -- work as seed","5744896c":"ada = create_model('ada' ,fold =5  )","94f5c1fb":"plot_model(ada ,'learning')","ba71e5a1":"plot_model(ada ,plot ='feature_all')","5b2fc3ca":"col = list(tdf.columns)\nprint(col)","5141d45e":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer ,KNNImputer ,SimpleImputer\nfrom sklearn.decomposition import PCA","9bb35e89":"\nt_df = tdf.copy()  # making copy of Train \nt_df = t_df.drop(columns ='id',axis =1)","e684d8f8":"t_df.info()","0e81504d":"X1 = t_df[['song_duration_ms','liveness',  'danceability',  'instrumentalness','energy' ,'acousticness' ,'loudness']] \nX2 = t_df[[ 'key']]\n\niter_im  = IterativeImputer(random_state =47 ,max_iter = 60)\nX1_out = iter_im.fit_transform(X1)\n\nknn_im = KNNImputer(n_neighbors=5, weights='uniform')\nX2_out = knn_im.fit_transform(X2)","ae1d339d":"t_df[['song_duration_ms','liveness',  'danceability',  'instrumentalness','energy' ,'acousticness' ,'loudness']] = X1_out\nt_df[[ 'key']] = X2_out\nt_df.head()\n","c8bbc699":"t_df.info()","c29883a6":"missing_col =['song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', \n               'key', 'liveness', 'loudness']\ndef feature_plotter(data , col_list):\n    fig , ax = plt.subplots(4 ,2 ,figsize=(20,16))\n    for i , c in enumerate(col_list):\n        if i< 4:\n            sns.histplot(data=data , x = c  , ax= ax[i,0] ) \n            ax[i,0].axvline(data[c].median(),color='r', linestyle=':')\n            ax[i,0].axvline(np.mean(data[c]),color='b', linestyle='-')\n#             ax[i,0].axvline(stats.mode(data[c])[0][0],color='g', linestyle='-.',linewidth=2)\n          \n        else : \n            sns.histplot(data=data , x = c , ax= ax[i-4,1] )\n            ax[i-4,1].axvline(data[c].median(),color='r', linestyle=':')\n            ax[i-4,1].axvline(np.mean(data[c]),color='b', linestyle='-')\n#             ax[i-4,1].axvline(stats.mode(data[c])[0][0],color='g', linestyle='-.')\n\nfeature_plotter(t_df ,missing_col)\n","dba16c4e":"clf= setup(data = t_df , target = 'song_popularity' ,silent =True ,session_id =47 )\nada= create_model('ada' , fold = 5)","b6a2db65":"scalar = StandardScaler()\npca = PCA(n_components=3)\nX = t_df.drop('song_popularity',axis =1)\n\nscaled_tdf = scalar.fit_transform(X)\nX_pca = pca.fit_transform(scaled_tdf)\nt_df[['pca1','pca2','pca3']] = X_pca\n\nprint(f\"shape of X :{X.shape}\\nshape of X_pca{X_pca.shape}\")","3f823cd6":"clf= setup(data = t_df , target = 'song_popularity' ,silent =True ,session_id =47  )\nada= create_model('ada' , fold = 5)","7b14fc0f":"t_df['log_norm_EL'] = t_df['energy']+ np.exp(t_df['loudness'])*100\nt_df['deNorm_acoust'] = t_df['acousticness']*t_df['log_norm_EL']\nt_df.head(3)","ffdc2cf8":"clf= setup(data = t_df , target = 'song_popularity' ,silent =True ,session_id =47  )\nada= create_model('ada' , fold = 5)","4516ad73":"plot_model(ada ,'feature_all')","4e6cadca":"# tdf = tdf.drop(columns =['key'])","c09f5202":"t_df.columns.shape","702ccf29":"fig , ax = plt.subplots(6 ,3 ,figsize=(25,13))\n\n\nfor i , c in enumerate(list(t_df.columns)):\n        if i<6: sns.boxplot(data = t_df ,x=c, hue = \"song_popularity\" , ax=ax[i][0])\n        elif i<12 : sns.boxplot(data = t_df ,x=c, hue = \"song_popularity\" , ax=ax[i-6][1])\n        elif i<18: sns.boxplot(data = t_df ,x=c, hue = \"song_popularity\" , ax=ax[i-12][2])\n        \nplt.tight_layout(pad=3)\nplt.show()","feef4401":"from collections import Counter\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (15%)\n        Q1 = np.percentile(df[col], 5)\n        # 3rd quartile (85%)\n        Q3 = np.percentile(df[col],95)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.2 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   ","5d3cef92":"col = list(t_df.columns)\nprint(col)","9cf974c8":"to_drop = detect_outliers(t_df, 1, col) \nprint(len(t_df.loc[to_drop]))\n# Drop outliers\n","fcf4d8bb":"t_df = t_df.drop(to_drop, axis = 0).reset_index(drop=True)\nt_df.info()","cf2c3750":"fig , ax = plt.subplots(5 ,3 ,figsize=(25,13))\n\n\nfor i , c in enumerate(list(t_df.columns)):\n        if i<5: sns.boxplot(data = t_df ,x=c, hue = \"song_popularity\" , ax=ax[i][0])\n        elif i<10 : sns.boxplot(data = t_df ,x=c, hue = \"song_popularity\" , ax=ax[i-5][1])\n        elif i<15: sns.boxplot(data = t_df ,x=c, hue = \"song_popularity\" , ax=ax[i-10][2])\n        \nplt.tight_layout(pad=3)\nplt.show()","72fb0221":"def FE( data , fix_missing_value = True ,generate_new_feature = True,do_pca =True  ,pca_comp = 3 ,  drop_irrelavnt = True):\n    if fix_missing_value :\n        X1 = data[['song_duration_ms','liveness',  'danceability',  'instrumentalness','energy' ,'acousticness' ,'loudness']] \n        X2 = data[[ 'key']]\n\n        iter_im  = IterativeImputer(random_state =47 ,max_iter = 60)\n        X1_out = iter_im.fit_transform(X1)\n\n        knn_im = KNNImputer(n_neighbors=5, weights='uniform')\n        X2_out = knn_im.fit_transform(X2)\n        \n        data[['song_duration_ms','liveness',  'danceability',  'instrumentalness','energy' \n              ,'acousticness' ,'loudness']] = X1_out\n        data[[ 'key']] = X2_out\n        \n    if generate_new_feature :\n        data['log_norm_EL'] = data['energy']+ np.exp(data['loudness'])*100\n        data['deNorm_acoust'] = data['log_norm_EL']*data['acousticness']\n        \n    if do_pca :\n        scalar = StandardScaler()\n        pca = PCA(n_components=pca_comp)\n        X = data\n\n        scaled_tdf = scalar.fit_transform(X)\n        X_pca = pca.fit_transform(scaled_tdf)\n        data = pd.concat([data , pd.DataFrame(X_pca)],axis=1)\n        [data.rename(columns = {i : ('pca' +str(i+1)) }, inplace = True ) for i in range(3)]\n        print(f\"shape of X :{X.shape}\\nshape of X_pca{X_pca.shape}\")\n#     if drop_irrelant :\n#         data = data.drop(columns = ['key'])\n    \n    return data\n        ","05b74573":"clf= setup(data = t_df , target = 'song_popularity' ,silent =True ,session_id = 47 )\nbest = compare_models(sort='AUC',exclude =['xgboost','catboost','gbc'],fold =5,n_select=5) \n# excluding time consuming models check model efficiency in version 5 of the notebook","5b5bfb31":"best","9770372e":"plot_model(best[2] ,'feature_all')","e32d422b":"plot_model(best[0])","1b9d9d6e":"# blended =blend_models(estimator_list= best ,fold =5) ","168d7c82":"T_df = Tdf.drop(columns = 'id',axis =1)\nT_df = FE( T_df ,\n          fix_missing_value = True ,\n          generate_new_feature = True,\n          do_pca =True  ,pca_comp = 3,\n          drop_irrelavnt = True)\n","64b17d38":"T_df.columns","fa013943":"T_df.head(3)","fc683d49":"# pred = predict_model(blended ,data =T_df )\n# pred.head(3)","d361e3c1":"feature = [col for col in t_df.columns if col not in (\"id\", \"song_popularity\")]\nscores = []\nX = t_df[feature]\ny = t_df['song_popularity']\n# # finding the column index of categorical columns\n# cat_cols = [\"key\", \"audio_mode\", \"time_signature\"]\n\n# cat_indices = []\n# for col in cat_cols:\n#     idx = list(X.columns).index(col)\n#     cat_indices.append(idx)\n# cat_indices","6e29d7a4":"def objective(trial, data=X,target=y):\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=47)\n    \n    params = {\n                \n                'n_estimators':trial.suggest_int('n_estimators',50,80,step=2),\n                'learning_rate': trial.suggest_float('learning_rate', 0.1,2.5,step = 0.0000005),\n                'algorithm':'SAMME.R', \n                'random_state':47\n            }\n    \n    clf = AdaBoostClassifier(**params) \n    clf.fit(X_train, y_train)\n    \n    y_proba = clf.predict_proba(X_valid)[:, 1]\n    auc = roc_auc_score(y_valid, y_proba)\n    return auc","0f99fd5a":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)","64591061":"study.best_params","1f1c41b3":"print(\"Best parameters:\")\nprint(\"*\"*50)\nfor param, val in study.best_trial.params.items():\n    print(f\"{param} :\\t {val}\")\nprint(\"*\"*50)\nprint(f\"Best AUC score: {study.best_value}\")","e678f273":"X_test =T_df[feature]\nskf = StratifiedKFold(n_splits=20, shuffle=True, random_state=47)","389dff9f":"from time import sleep","510b51d7":"\npredictions = []\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    \n    X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n    y_train , y_valid = y.iloc[train_idx], y.iloc[val_idx]\n    \n    params = study.best_params\n    clf = AdaBoostClassifier(**params)\n    clf.fit(X_train, y_train)\n    \n    y_proba = clf.predict_proba(X_valid)[:, 1]\n    \n    \n    score = roc_auc_score(y_valid, y_proba)\n    \n    for frame in '>>'*20:\n        print(frame ,sep ='',end ='',flush =True)\n        sleep(0.02)\n    print(f\"\\nFold: {fold}, AUC: {score}\")\n    print(\"_\"*40)\n    test_pred = clf.predict_proba(X_test)[:,1]\n    predictions.append(test_pred)","0eec6c4a":"preds = np.mean(np.column_stack(predictions), axis=1)\nsub = pd.DataFrame(columns = ['id', 'song_popularity'])\nsub['id'] = Tdf['id']\nsub['song_popularity'] = preds\nsub","6463d63c":"submission =[]\nsubmission.append(preds)","1b28a541":"submission.append(preds)","b5de1ace":"predictions = []\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    \n    X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n    y_train , y_valid = y.iloc[train_idx], y.iloc[val_idx]\n    params = {'n_estimators': 74, 'learning_rate': 1.472356,\n             'algorithm':'SAMME', \n                'random_state':47}\n    \n    clf = AdaBoostClassifier(**params)\n    clf.fit(X_train, y_train)\n    \n    y_proba = clf.predict_proba(X_valid)[:, 1]\n    \n    \n    score = roc_auc_score(y_valid, y_proba)\n    \n    for frame in '>>'*20:\n        print(frame ,sep ='',end ='',flush =True)\n        sleep(0.02)\n    print(f\"\\nFold: {fold}, AUC: {score}\")\n    print(\"_\"*40)\n    test_pred = clf.predict_proba(X_test)[:,1]\n    predictions.append(test_pred)","b0071159":"preds = np.mean(np.column_stack(predictions), axis=1)\nsub = pd.DataFrame(columns = ['id', 'song_popularity'])\nsub['id'] = Tdf['id']\nsub['song_popularity'] = preds\nsub","1662240b":"submission.append(preds)","dc85d4a2":"predictions = []\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    \n    X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n    y_train , y_valid = y.iloc[train_idx], y.iloc[val_idx]\n    params = {'n_estimators': 68, 'learning_rate': 0.3225445,\n             'algorithm':'SAMME', \n                'random_state':47}\n    \n    clf = AdaBoostClassifier(**params)\n    clf.fit(X_train, y_train)\n    \n    y_proba = clf.predict_proba(X_valid)[:, 1]\n    \n    \n    score = roc_auc_score(y_valid, y_proba)\n    \n    for frame in '>>'*20:\n        print(frame ,sep ='',end ='',flush =True)\n        sleep(0.02)\n    print(f\"\\nFold: {fold}, AUC: {score}\")\n    print(\"_\"*40)\n    test_pred = clf.predict_proba(X_test)[:,1]\n    predictions.append(test_pred)","c411926b":"preds = np.mean(np.column_stack(predictions), axis=1)\nsub = pd.DataFrame(columns = ['id', 'song_popularity'])\nsub['id'] = Tdf['id']\nsub['song_popularity'] = preds\nsub","5598c6bf":"submission.append(preds)","49af38e3":"np.shape(submission)","a7eb0e86":"preds = np.mean(np.column_stack(submission), axis=1)\nfinal_sub = pd.DataFrame(columns = ['id', 'song_popularity'])\nfinal_sub['id'] = Tdf['id']\nfinal_sub['song_popularity'] = preds\nfinal_sub","274fd308":"final_sub.to_csv(\"submission.csv\",index=False)","a5b41655":"ok we can see lot of dots there but are they really outliers ? , let's check using IQR","59cf87cd":"Now let's compare some models to do ensembling","2f1f7de3":"\n\n#  \ud83c\udfb9 Song Popularity Prediction \ud83c\udfb8 <br>\n\n\n![image](attachment:5dc7aeb9-f227-409d-a943-669c683d44e1.png)\n<br>\n\nThis Notebook will be some experimentation of few libraries , hope you will llike it , <br>\n\n<a id =\"welcome\"\/><br>\n\n##  Table of Content <br>\n* 1. [welcome](#welcome)<br>\n* 2. [Intro](#intro)<br>\n* 3. [Data loading and preparation](#dlp)<br>\n    * 3.1. [Import Libraries](#il)\n    * 3.2. [Import Data](#id)\n    * 3.3. [Pycaret EDA](#pe)\n* 4. [Feature Engineering](#FE)\n    * 4.1. [Missing Value Analysis](#mva)\n    * 4.2. [new feature generation](#nfg)\n    * 4.3. [PCA](#pca)\n    * 4.4. [dropping irrelevant features](#dif)\n    * 4.5. [outlier fixing (if any )](#oa)\n* 5. [Pycaret Model Analysis](#pma) \n* 6. [Model ](#m)\n* 7. [prediction](#p)\n* 8. [conclusion](#c)<br>\nlet's start the game then\ud83c\udfca\u200d  \n\n\n( in the end tell me in comments what you feel about this mixed  use of pycaret + Optuna ","220f75cd":"#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","374c9b10":"we can see there are lot of `missing values` in the data in `columns :` `song_duration_ms`, `acousticness`, `danceability`, `energy`, `instrumentalness`, `key`, `liveness`, `loudness` (to `visiualize` the missing value we can use` msno.matrix`)\n\nwe will fix half of the data with `iterative imputer` and half of the data with `KNNimputer` <br>\nthose features which are `highly correlated` we will impute them `using iterative imputer` , and those which are `categorical` we will use `KNNImputer`\n\nHighly correlated features are : `energy` ,`acousticness` ,`loudness`","e5c85239":"#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","ac0bdafb":"we can do EDA using pandas profile but Pycaret EDA is little easy and more descriptive\ud83d\ude05","ed12eefc":"what a curve \ud83e\uddd0","708ba838":"<a id ='FE' \/> <br>\n\n## \ud83d\udca0 Feature Engineering","1e98832f":"<a id=\"il\"\/><br>\n\n## \ud83d\udd05 Import libraries  \n\n","77acd8de":"Everything looks fine till now , \n\n#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","0dde4a2e":"I am using 5 fold just to decrerase runtime of notebook ,you can use any fold,\n\nwe can see improvement in the model after feature engineering","3fd78042":"### \u2b55 Preparing test data for prediction","e2e680ed":"#### \u2b55Trial prediction 3 ","9cd75d25":"<a id=\"intro\"\/><br>\n\n# \ud83d\udca0 Intro \n\nCan we use Pycaret and Otuna together \ud83e\udd14,Is it a good idea? <br>\nlet's See , I can guarantee you will enjoy reading this notebook ,I tried my best to add comments at each step.","8fe6ab63":"<a id='oa' \/><br>\n\n### \ud83d\udeec Outlier Analysis","163d98f0":"<a id='dif' \/><br>\n\n### \u2694 Droping irrelevant features","66d23189":"<a id=\"p\"\/><br>\n\n# \ud83d\udca0 Predictions ","53094d7f":"#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd\n\nIf you got some good info do like , share and comment ...see you! ","e6a4b4f7":"Outlier analysis Credits : [YASSINE GHOUZAM](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling#2.2-Outlier-detection)","33d84137":"Now we got  best model, Now we are going to use Optuna for auto tuning the model","7231b2fa":"let's create a quick model using pycaret of ada boost to check the feature importance","7490316d":"so definitely these are supporting the model","938ef8c6":"#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","0a427faa":"we got best models , I am using ,`ADA` to tune model","19df6ca2":"<a id =\"pma\"\/><br>\n\n## \ud83c\udfdd Pycaret Model Analysis ","9b42194c":"<a id=\"c\"\/><br>\n\n# \ud83c\udf40 Conclusion ","38f74dd3":"\nlet's blend the models what if we get some more good score","ef10de05":"#### \u2b55Trial Prediction 2 ","4caca8a3":"<a id ='mva' \/><br>\n\n### \ud83e\udde9 Missing Value","4bfde3e6":"In this part we will do :\n1. missing value analysis\n2. new feature generation\n3. dropping irrelevant features \n4. outlier fixing (if any )\n\nafter each step we will compare the model accuracy by quick model of pycaret , it will help us to tell which sstep is right or wrong","b73cdbc7":"<a id=\"pe\"\/><br>\n\n## \ud83d\udca5 Pycaret EDA","c7933beb":"<a id=\"id\"\/><br>\n\n## \ud83d\udd05 Import data ","0476563b":"so currently  we are having few significant  features , but  our model is currently a bogus model ,let's fix some missing value , do some feature engineering then we will check how features are improving their significance\ud83d\ude43\n\n`ada` is good model but we will check the best model after some feature engineering","1dbe1d16":"<a id ='pca' \/><br>\n\n### \ud83c\udfafPCA","692657b6":", Now we got an idea what features are important , what techniques we should be using and all , now let's tune the model using optuna , pycaret tunning is time consuming \ud83d\ude05\n#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","20b743cd":"#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","0b79367f":"we can see  these features are decreasing the score but let's check it using feature importance ","1ee63ad0":"1. Pycaret is easy to use for checking models\n2. Adaboost can also be used for training the model but it's not the most efficient solution\n3. Imputers are really good\n4. thanks to all people who helped me to reach upto this solution\n\nhope you liked this combination of pycaret + optuna \ud83d\ude05\n\nlet's meet in next notebook ...\ud83d\ude4b\u200d\u2642\ufe0f","d966bb13":"<a id=\"dlp\"\/><br>\n\n## \ud83d\udca0 Data Loading and preperation \n\nIn this part  we will look for basic EDA and learn pycaret ","efb21a3c":"#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","24264100":"These are audio feature , but it's very bad that we got only features of audio and not any audio data \u2639  (\u091c\u093e\u0909\u0926\u094d\u092f\u093e )   it's ok ,it makes problem more easy then \ud83d\ude05","d10810ca":"we got little change but it's very less \ud83d\ude42, let's enhance the model using high correlation features\n\n#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","f37c1cd6":"#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","82131566":"### \u2b55 Conclusion of EDA : \n1. High Correlation in few feature  : `energy` ,`acousticness` ,`loudness`\n2. High Missing values : `song_duration_ms`, `acousticness`, `danceability`, `energy`, `instrumentalness`, `key`, `liveness`, `loudness`\n3. Few Features are irrelevant: `id`\n4. To understand data properly reader need prior experince , which I don't have \ud83d\ude42 ( if you don't understand anything from above profile then first do EDA manually )\n\n#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","89deb654":"<a id ='nfg' \/><br>\n\n### \ud83d\udcd1 New Feature Generation\n\nwe know loudness is log decibal form of energy so obviously those two features will have high correlation , let's denormalize them to get new feature, and acousticness is just digital form of energy spectrum so let's restructure it aswell","83e71fb7":"#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","cd32cac0":"#### \ud83d\udcdaPN : 5\n\ud83d\udcda[PN : 4](https:\/\/www.kaggle.com\/sarabhian\/spp-free-time-learning-optuna-auto-tuning)","8a45c378":"<a id =\"m\"\/><br>\n\n## \ud83c\udf0a  Model ","d41fa835":"ok all of them are looking fine , let's quick check the change in model","fecacc9c":"refer some work in  other running challenges <br>[TPS Jan 22 series](https:\/\/www.kaggle.com\/sarabhian\/free-time-learning)<br>\n[tensorflow GBR](https:\/\/www.kaggle.com\/sarabhian\/gbr-extremely-beginner-level-guide-1)<br>\n[SPP Lightgbm](https:\/\/www.kaggle.com\/sarabhian\/spp-free-time-learning-optuna-auto-tuning)","226e5fc6":"even if we just predict model using ada we will get `AUC` score of `0.56` \ud83d\ude05 , interesting isn't it ?","595d62cb":"#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd\n\nLet's Define a function Feature Engineering so that we can use it on Test data "}}