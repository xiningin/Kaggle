{"cell_type":{"3a4d5e25":"code","6480d5a6":"code","3067db15":"code","497c06cf":"code","f68ba812":"code","643848c8":"code","58733389":"code","3c2ad0c2":"code","90028bc2":"code","b3deef39":"code","e439b560":"code","4b8e52cb":"code","187c595e":"code","b95aa1f6":"code","c9a2d6f5":"code","e4def443":"code","805b463d":"code","1512d33d":"code","86673649":"code","35f23ee3":"code","ea7ec140":"code","79e70bea":"code","9dc2bea4":"code","25a59f25":"code","58e35a1a":"code","7d86da44":"code","b2cbcc33":"code","0f01a1f4":"code","df983bc4":"code","4d3d9272":"code","6cec4e62":"code","4b0df13f":"code","53135c0a":"code","6afc9d6a":"markdown","5415729c":"markdown","7481a389":"markdown","40180903":"markdown","51df02c3":"markdown","9ff56394":"markdown","7b4899c3":"markdown","9425064b":"markdown","a8912bf5":"markdown","1a9c0f88":"markdown","3b923454":"markdown","ef31c6d5":"markdown","d5d9d037":"markdown","1204dc63":"markdown","a2ba9eb5":"markdown","5eedc34a":"markdown"},"source":{"3a4d5e25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6480d5a6":"# importing data to data frame\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\ndf=pd.read_csv('\/kaggle\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')","3067db15":"df.head()","497c06cf":"df.isna().any()","f68ba812":"import seaborn as sns","643848c8":"#dropping the customer id as it is irrelevant\ndf.drop(columns=['CustomerID'],inplace=True)\n","58733389":"# Converting the gender values to 1(Male) and 0(Female)\ngender = {'Male': 1, 'Female':0}\ndf['Gender'] = [gender[item] for item in df.Gender]","3c2ad0c2":"sns.heatmap(df.corr())","90028bc2":"sns.pairplot(df)","b3deef39":"#Lets check our hypothesis:\ndf_before_50=df[df['Age']<=50]\ndf_after_50=df[df['Age']>50]\n\n\n#Lets check hypothesis 1\nprint(\"\\tChecking for hypothesis: Average Annual Income is less after age of 50.\")\na=df_before_50['Annual Income (k$)'].mean()\nb=df_after_50['Annual Income (k$)'].mean()\nprint(\"Average Annual income before 50: \"+str(a))\nprint(\"Average Annual income after 50: \"+str(b))\nif a>b:\n  print(\"Our Assumption is right.\")\nelse:\n  print(\"Our Assumption is wrong\")\n\n#Lets Check hypothesis 2:\nprint(\"\\n\\tChecking for hypothesis: Average Spending score is less after age of 50.\")\na=df_before_50['Spending Score (1-100)'].mean()\nb=df_after_50['Spending Score (1-100)'].mean()\nprint(\"Average Spending Score before 50: \"+str(a))\nprint(\"Average Spending Score after 50: \"+str(b))\nif a>b:\n  print(\"Our Assumption is right.\")\nelse:\n  print(\"Our Assumption is wrong\")","e439b560":"# using elbow method to detect optimal number of clusters.\n\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom scipy.spatial.distance import cdist\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ninertias = []\n\nK = range(1, 10)\nX=df[['Annual Income (k$)','Spending Score (1-100)']]\nfor k in K:\n    # Building and fitting the model\n    kmeanModel = KMeans(n_clusters=k).fit(X)\n    kmeanModel.fit(X)\n\n    inertias.append(kmeanModel.inertia_)\n\n#plotting the curve\nplt.plot(K, inertias, 'bx-')\nplt.xlabel('Values of K')\nplt.ylabel('Inertia')\nplt.title('The Elbow Method using Inertia')\nplt.show()","4b8e52cb":"#Hence the optimal number of clusters is 5\nkm=KMeans(n_clusters=5).fit(X)\ndf_Kmeans=df\ndf_Kmeans['Labels']=km.labels_\nsns.pairplot(df_Kmeans,hue='Labels',palette='deep')","187c595e":"sns.scatterplot(x=df['Annual Income (k$)'],y=df['Spending Score (1-100)'],hue=df_Kmeans['Labels'],palette='Accent_r')","b95aa1f6":"\ndf_cluster2=df_Kmeans[df_Kmeans['Labels']==2]\ndf_cluster4=df_Kmeans[df_Kmeans['Labels']==4]\n\nprint(\"\\tChecking Assumption 1\")\nif df_cluster2['Age'].mean()<30:\n  print(\"The average Age of cluster2 is: \"+str(df_cluster2['Age'].mean()))\n  print(\"Hypothesis is True.\")\nelse:\n  print(\"The average Age of cluster2 is: \"+str(df_cluster2['Age'].mean()))\n  print(\"Hypothesis is False.\")\n\nprint(\"\\n\\tChecking Assumption 2\")\nif df_cluster4['Age'].mean()>30:\n  print(\"The average Age of cluster4 is: \"+str(df_cluster4['Age'].mean()))\n  print(\"Hypothesis is True.\")\nelse:\n  print(\"The average Age of cluster4 is: \"+str(df_cluster4['Age'].mean()))\n  print(\"Hypothesis is False.\")","c9a2d6f5":"# using elbow method to detect optimal number of clusters.\n\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom scipy.spatial.distance import cdist\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ninertias = []\n\nK = range(1, 10)\nX=df\nfor k in K:\n    # Building and fitting the model\n    kmeanModel = KMeans(n_clusters=k).fit(X)\n    kmeanModel.fit(X)\n\n    inertias.append(kmeanModel.inertia_)\n\n#plotting the curve\nplt.plot(K, inertias, 'bx-')\nplt.xlabel('Values of K')\nplt.ylabel('Inertia')\nplt.title('The Elbow Method using Inertia')\nplt.show()","e4def443":"# optimal clusters seem to be 6 but not completely sure \n# using elbow method to detect optimal number of clusters using Distortions\n\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom scipy.spatial.distance import cdist\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndistortions = []\n\nK = range(1, 10)\nX=df\nfor k in K:\n    # Building and fitting the model\n    kmeanModel = KMeans(n_clusters=k).fit(X)\n    kmeanModel.fit(X)\n\n    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n                                        'euclidean'), axis=1)) \/ X.shape[0])\n\n#plotting the curve\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('Values of K')\nplt.ylabel('Distortions')\nplt.title('The Elbow Method using Distortions')\nplt.show()","805b463d":"#Hence the optimal number of clusters is 6\nkm=KMeans(n_clusters=6).fit(X)\ndf_Kmeans_new=df\ndf_Kmeans_new['Labels']=km.labels_\nsns.pairplot(df_Kmeans_new,hue='Labels',palette='deep')","1512d33d":"\nx=df_Kmeans_new[(df_Kmeans_new['Labels']==2)|(df_Kmeans_new['Labels']==3)]\nsns.scatterplot(x['Spending Score (1-100)'],x['Annual Income (k$)'],hue=x['Labels'],palette='Accent_r')","86673649":"sns.scatterplot(x['Age'],x['Spending Score (1-100)'],hue=x['Labels'],palette='Accent_r')","35f23ee3":"import scipy.cluster.hierarchy as shc\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 7))  \nplt.title(\"Dendrograms\")  \ndend = shc.dendrogram(shc.linkage(df, method='ward'))","ea7ec140":"#from above system we get a threshold of 450\nplt.figure(figsize=(10, 7))  \nplt.title(\"Dendrograms\")  \ndend = shc.dendrogram(shc.linkage(df, method='ward'))\nplt.axhline(y=300, color='r', linestyle='--')","79e70bea":"from sklearn.cluster import AgglomerativeClustering\n# applying Agglomerative clustering with total clusters as 3\ncluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')  \ncluster.fit_predict(df)\ndf_agg=df\n\ndf_agg['Labels']=cluster.labels_\n\n#Visualizing the clusters\nsns.pairplot(df_agg, hue='Labels', palette='Accent_r')","9dc2bea4":"df.drop(columns=['Labels'],inplace=True)","25a59f25":"# we can see that we have to form 4 different clusters\nfrom sklearn.cluster import KMeans\nimport numpy as np","58e35a1a":"#if we have no clusters, it is equivalent to have one single cluster.\nsns.pairplot(df)","7d86da44":"# now lets split the data in 2 clusters using kmeans\nkmeans = KMeans(n_clusters=2, random_state=0).fit(df)\n# here we have clustered and hence we have reached level 1.","b2cbcc33":"#saving the level one clusters\nlabels=kmeans.labels_\ndf_divisive=df\ndf_divisive['label_level_1']=kmeans.labels_\n\n#viewing the level one clusters: cluster 0, cluster 1\nsns.pairplot(df_divisive,hue='label_level_1')","0f01a1f4":"centres=kmeans.cluster_centers_\ncentres","df983bc4":"# for deciding which cluster to split, we would calculate intra cluster distance.\n# cluster with higher intra cluster distance would be selected for splitting\n\ndistances = [0,0]\n\nc1w, c1x, c1y, c1z=centres[0]\nc2w, c2x, c2y, c2z=centres[0]\n\nfor index, row in df_divisive.iterrows():\n  w=row['Gender']\n  x=row['Age']\n  y=row['Annual Income (k$)']\n  z=row['Spending Score (1-100)']\n  c=row['label_level_1']\n  if c==0:\n    distances[0]+=np.sqrt((x-c1x)**2+(y-c1y)**2+(w-c1w)**2+(z-c1z)**2)\n  else:\n    distances[1]+=np.sqrt((x-c2x)**2+(y-c2y)**2+(w-c2w)**2+(z-c2z)**2)\n\n    \nprint(\"Intra Cluster distace of cluster 0:\"+str((distances[0]\/df_divisive[df_divisive['label_level_1']==0].shape[0])))\n\nprint(\"Intra Cluster distace of cluster 1:\"+str((distances[1]\/df_divisive[df_divisive['label_level_1']==1].shape[0])))","4d3d9272":"#select the cluster 1.\ndf_x_1=df_divisive[df_divisive['label_level_1']==1].iloc[:,0:4]\n\n# breaking the cluster 1 into 2 clusters\ndf_x=df_x_1\nkmeans = KMeans(n_clusters=2, random_state=0).fit(df_x)\n# here we have clustered and hence we have reached level 2.\n\n# saving the labels \nlabels=kmeans.labels_\nlabels+=2\n\n#visualising the 2 clusters from cluster1 : cluster 2, cluster3\nsns.scatterplot(df_x_1['Annual Income (k$)'],df_x_1['Spending Score (1-100)'], hue=labels)","6cec4e62":"dfc1=df_divisive[df_divisive['label_level_1']==0]\ndfc2=df_divisive[df_divisive['label_level_1']==1]","4b0df13f":"dfc1['label_level_2']=0\ndfc2['label_level_2']=labels","53135c0a":"#visualizing all the clusters\ndf_final=pd.DataFrame(dfc1)\ndf_final=df_final.append(dfc2,ignore_index=True)\ndf_final.drop(columns=['label_level_1'],inplace=True)\nsns.pairplot(df_final, hue='label_level_2', palette=\"deep\")","6afc9d6a":"# Kmeans clustering","5415729c":"Here along with spending score and annual income, age is also considered. The clearer distintion can be seen in plot of age vs annual income.  From previous section we can see that the cluster of medium income, medium spending is divided in two sections.\nLets have a detailed look over there.","7481a389":"# Data Preprocessing and EDA","40180903":"##Clustering using all the features","51df02c3":"Assumptions:\n1. The people in cluster (High spending, Low  income) may be young as they earn less but family can provide money.\n2. The People in cluster(High Income, Low Spending) may be old as they earn a lot but have less needs and hence spend less.\n","9ff56394":"We can see 5 different clusters here.\n1.   Cluster; High Income, Low Spending; Careful.\n2.   Cluster;  High income, High spending; Targeted for marketing.\n3.   Cluster; Medium income, Medium spending; General category, maximum population.\n4.   Cluster; High spending, Low  income; careless.\n5.   Cluster; Low income, Low Spending; Sensible.\n\n\n\n\n\n","7b4899c3":"# Hierarchical clustering","9425064b":"Hence we would split cluster 1","a8912bf5":"## Lets apply clustering using only Spending Score and annual income.","1a9c0f88":"Here fist the cluster split as per Spending Score then it split as per annual income.","3b923454":"# Importing Data\n\n","ef31c6d5":"Here we can see the Cluster 1 in previous cluster is then split in 2 clusters based on Age. \nIf Age is less than 40 then one cluster, If Age is greater than 40 then different cluster. This can be easily seen in above diagram.\n","d5d9d037":"From Above grapg we can have 2 major conclusions:\n\n1.   Average Annual Income is less after age of 50.\n2.   Average Spending score is also less after age of 50.\n\n\n\n","1204dc63":"## Agglomerative clustering","a2ba9eb5":"## Divisive Clustering","5eedc34a":"Here we can guess that first the split was due to the difference in annual income and then the second and third clusters are because of difference in spending score."}}