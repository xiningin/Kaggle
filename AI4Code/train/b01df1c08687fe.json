{"cell_type":{"2d7a7a5f":"code","2396f10b":"code","b8c66ba1":"code","95547387":"code","f13a14ad":"code","4a09da2e":"code","ab386b22":"code","1ad509ee":"code","435c6a82":"code","9bba4ace":"code","de348703":"code","03e63bd7":"code","043480b0":"code","b8e892ae":"code","677304da":"code","14e466f7":"code","3067a6cd":"code","8a8cc736":"code","3c88a5f2":"code","70efbcbe":"code","c9e747a2":"code","4765d82b":"code","38ccf413":"code","8a3831ac":"code","9b7364b7":"code","af802b08":"code","3117e15d":"code","e54bba8b":"code","5462f4ae":"code","a7e8ca79":"code","eb57baf8":"code","dbf9253a":"code","6a8f4518":"code","be3fcbb4":"code","1522d670":"code","a3119a58":"code","908226af":"code","3a7cb980":"markdown","86e406c1":"markdown","02233b75":"markdown","ff171212":"markdown","65fd616d":"markdown","b8cb00c0":"markdown","4888d338":"markdown","059762f7":"markdown","ecd55f71":"markdown","18493801":"markdown","646e216f":"markdown","f5a4f85e":"markdown","cabc8ddb":"markdown","481e4159":"markdown","937b363d":"markdown","46248c06":"markdown","dba7b09e":"markdown","3afc8eb9":"markdown","7bc7ef67":"markdown","f1e66abc":"markdown","f5dd4c7f":"markdown"},"source":{"2d7a7a5f":"# libraries\nimport riiideducation\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport os\nfrom typing import List, Dict, Optional\nimport numpy as np\nfrom sklearn.model_selection import RepeatedKFold\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport math\nimport time\nimport random\nimport lightgbm as lgb\nimport gc\nimport os\nfrom sklearn.preprocessing import LabelEncoder\nfrom numba import jit\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics","2396f10b":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage(deep=True).sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n    \n\n@jit\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https:\/\/www.kaggle.com\/c\/microsoft-malware-prediction\/discussion\/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc \/= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True","b8c66ba1":"!wc -l \/kaggle\/input\/riiid-test-answer-prediction\/train.csv","95547387":"path = '\/kaggle\/input'\n\ntrain = pd.read_csv(f'{path}\/riiid-test-answer-prediction\/train.csv',\n                    usecols=['timestamp', 'user_id', 'content_id', 'content_type_id', 'user_answer', 'answered_correctly',\n                             'prior_question_elapsed_time', 'prior_question_had_explanation'],\n                       dtype={'timestamp': 'int64',\n                              'user_id': 'int32',\n                              'content_id': 'int16',\n                              'content_type_id': 'int8',\n                              'user_answer': 'int8',\n                              'answered_correctly': 'int8',\n                              'prior_question_elapsed_time': 'float32', \n                              'prior_question_had_explanation': 'boolean',\n                             }\n                      )\nquestions = pd.read_csv(f'{path}\/riiid-test-answer-prediction\/questions.csv')\nlectures = pd.read_csv(f'{path}\/riiid-test-answer-prediction\/lectures.csv')\nprint('Train shapes: ', train.shape)","f13a14ad":"train.head()","4a09da2e":"train['answered_correctly'].value_counts()","ab386b22":"questions.head()","1ad509ee":"lectures.head()","435c6a82":"plt.hist(train['timestamp'], bins=40);","9bba4ace":"train.groupby(['user_id'])['timestamp'].max().sort_values(ascending=False).head()","de348703":"train['user_id'].value_counts()","03e63bd7":"train['content_type_id'].value_counts()","043480b0":"train.loc[train['content_type_id'] == 1, 'user_id'].nunique()","b8e892ae":"train['content_id'].value_counts()","677304da":"train.loc[train['content_id'] == 6116]","14e466f7":"train.loc[train['content_id'] == 6116, 'user_answer'].value_counts()","3067a6cd":"questions.loc[questions['question_id'] == 6116]","8a8cc736":"# filter out lectures\ntrain = train.loc[train['answered_correctly'] != -1].reset_index(drop=True)\ntrain = train.drop(['timestamp','content_type_id'], axis=1)\ntrain['prior_question_had_explanation'] = train['prior_question_had_explanation'].fillna(value = False).astype(bool)","3c88a5f2":"user_answers_df = train.groupby('user_id').agg({'answered_correctly': ['mean', 'count'],\n                                               'prior_question_elapsed_time':[\"mean\",\"max\"], 'prior_question_had_explanation':\"mean\"}).copy()\nuser_answers_df.columns = ['mean_user_accuracy', 'questions_answered',\n                          \"mean_prior_question_elapsed_time\",\"max_prior_question_elapsed_time\",\"mean_prior_question_had_explanation\"] # New feats - Dan\n\ncontent_answers_df = train.groupby('content_id').agg({'answered_correctly': ['mean', 'count']}).copy()\ncontent_answers_df.columns = ['mean_accuracy', 'question_asked']\n\nuser_content_answers_df = train.groupby(['user_id', 'content_id']).agg({'answered_correctly': ['mean', 'count']}).copy()\nuser_content_answers_df.columns = ['mean_user_content_accuracy', 'content_questions_answered']","70efbcbe":"train = train.iloc[90000000:,:]","c9e747a2":"train = train.merge(user_answers_df, how = 'left', on = 'user_id')\ntrain = train.merge(content_answers_df, how = 'left', on = 'content_id')\ntrain = train.merge(user_content_answers_df, how = 'left', on = ['user_id', 'content_id'])","4765d82b":"train.fillna(value = 0.5, inplace = True)","38ccf413":"train['mean_diff1'] = train['mean_user_accuracy'] - train['mean_user_content_accuracy']\ntrain['mean_diff2'] = train['mean_accuracy'] - train['mean_user_content_accuracy']","8a3831ac":"train.head()","9b7364b7":"le = LabelEncoder()\ntrain[\"prior_question_had_explanation\"] = le.fit_transform(train[\"prior_question_had_explanation\"])","af802b08":"train = train.sort_values(['user_id'])","3117e15d":"y = train['answered_correctly']\n\ncolumns = ['mean_user_accuracy', 'questions_answered', 'mean_accuracy', 'question_asked',\n           'prior_question_had_explanation', 'mean_diff1', 'mean_diff2',\n           \"mean_prior_question_elapsed_time\",\"max_prior_question_elapsed_time\",\"mean_prior_question_had_explanation\", # dan new feats\n           ]\nX = train[columns]","e54bba8b":"del train","5462f4ae":"scores = []\nfeature_importance = pd.DataFrame()\nmodels = []","a7e8ca79":"params = {'num_leaves': 32,\n          'max_bin': 300,\n          'objective': 'binary',\n          'max_depth': 13,\n          'learning_rate': 0.03,\n          \"boosting_type\": \"gbdt\",\n          \"metric\": 'auc',\n         }","eb57baf8":"folds = StratifiedKFold(n_splits=4, shuffle=False) # was 5\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n    print(f'Fold {fold_n} started at {time.ctime()}')\n    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    model = lgb.LGBMClassifier(**params, n_estimators=700, n_jobs = 1)\n    model.fit(X_train, y_train, \n            eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=eval_auc,\n            verbose=1000, early_stopping_rounds=10)\n    score = max(model.evals_result_['valid_1']['auc'])\n    \n    models.append(model)\n    scores.append(score)\n\n    fold_importance = pd.DataFrame()\n    fold_importance[\"feature\"] = columns\n    fold_importance[\"importance\"] = model.feature_importances_\n    fold_importance[\"fold\"] = fold_n + 1\n    feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n    break","dbf9253a":"print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))","6a8f4518":"feature_importance[\"importance\"] \/= 1\ncols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].index\n\nbest_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\nplt.figure(figsize=(16, 12));\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\nplt.title('LGB Features (avg over folds)');\n","be3fcbb4":"del X, y","1522d670":"env = riiideducation.make_env()","a3119a58":"iter_test = env.iter_test()","908226af":"for (test_df, sample_prediction_df) in iter_test:\n    y_preds = []\n    test_df = test_df.merge(user_answers_df, how = 'left', on = 'user_id')\n    test_df = test_df.merge(content_answers_df, how = 'left', on = 'content_id')\n    test_df = test_df.merge(user_content_answers_df, how = 'left', on = ['user_id', 'content_id'])\n    test_df['mean_diff1'] = test_df['mean_user_accuracy'] - test_df['mean_user_content_accuracy']\n    test_df['mean_diff2'] = test_df['mean_accuracy'] - test_df['mean_user_content_accuracy']\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(value = False).astype(bool)\n    test_df = test_df.loc[test_df['content_type_id'] == 0].reset_index(drop=True)\n    test_df.fillna(value = 0.5, inplace = True)\n    test_df[\"prior_question_had_explanation_enc\"] = le.fit_transform(test_df[\"prior_question_had_explanation\"])\n\n    for model in models:\n        y_pred = model.predict_proba(test_df[columns], num_iteration=model.best_iteration_)[:, 1]\n        y_preds.append(y_pred)\n\n    y_preds = sum(y_preds) \/ len(y_preds)\n    test_df['answered_correctly'] = y_preds\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","3a7cb980":"### helper functions","86e406c1":"Does low numbers mean that only one person answered this question? I wonder why there are such question.","02233b75":"## Exploring the features","ff171212":"More than 100m rows in train dataset!","65fd616d":"### content_id\n\nId of the content - question or lecture","b8cb00c0":"There are 1749 unique user ids in out data. Some have little data, some have a lot of data. But please notice that I loaded only a small part of the data.","4888d338":"Let's have a look at the most popular question","059762f7":"### user_id\n\nObviously this is an unique user id.","ecd55f71":"We can see that a lot of people made mistakes answering this question.","18493801":"## Data overview","646e216f":"## Making predictions.\n\nCode is taken from https:\/\/www.kaggle.com\/sishihara\/riiid-lgbm-5cv-benchmark","f5a4f85e":"`answered_correctly` is our target! `-1` is a special value, we'll talk about it later.","cabc8ddb":"## General information\n\nIn this competition, we will create algorithms for \"Knowledge Tracing,\" the modeling of student knowledge over time. The goal is to accurately predict how students will perform on future interactions.\nIf successful, it\u2019s possible that any student with an Internet connection can enjoy the benefits of a personalized learning experience, regardless of where they live. We can build a better and more equitable model for education in a post-COVID-19 world.\n\nOur challenge in this competition is to predict whether students are able to answer their next questions correctly.\n\nThis competition is similar to Two Sigma competition, where we got test data using special API.\n\n![](https:\/\/i.imgur.com\/Ko5MxFQ.png)","481e4159":"## Feature engineering\n\nLet's generate more features.\n\nsome code is taken from https:\/\/www.kaggle.com\/ilialar\/simple-eda-and-baseline https:\/\/www.kaggle.com\/lgreig\/simple-lgbm-baseline","937b363d":"Now we will use only a part of data for training, to avoid leaks and memory error","46248c06":"I'll continue EDA later.","dba7b09e":"Some users have really huge activity time!","3afc8eb9":"### timestamp\n\nIt is imprtant to remember that this is the time between this **user** interaction and the first event from that **user**. So starting time could be different for each user","7bc7ef67":"Hm, interesting. It seems that not all people watched lectures.","f1e66abc":"### content_type_id\n\n0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture.\n\nSo we will be mainly using `content_type_id` equal to `1` at first, but generating some features based on the lectures should be also useful.","f5dd4c7f":"### importing libraries"}}