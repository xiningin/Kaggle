{"cell_type":{"3dbfea4f":"code","96bc5e15":"code","00195dec":"code","024e76dc":"code","77b91075":"code","bb936a14":"code","44da3347":"code","f6d323c2":"code","4a8b527c":"code","f8d9e5cc":"code","fee05960":"code","ff04d4b7":"code","2a6246ba":"code","901ea973":"code","3627de7d":"code","494c8796":"code","99596f45":"code","7a6701c6":"code","cc32d39a":"code","70bb8280":"code","aa28524d":"code","cbc9c57a":"code","53fe521c":"code","9f6ee8c6":"code","e54ffa26":"code","8696e44b":"code","3f267b5d":"code","e4ec55f5":"code","694c6fe9":"code","1519f546":"code","08f9d17a":"code","546cf1a3":"code","a23696ad":"code","74d34d5d":"code","a92d1379":"code","09350666":"code","56b6cd80":"code","587cff3d":"code","c64c13f2":"code","410d1e69":"code","883975b9":"code","b9d03bec":"code","74ea825e":"markdown","ad19d3e9":"markdown","503cbf5f":"markdown","3e648e5c":"markdown","53b40ded":"markdown","8ab80659":"markdown","95fb6748":"markdown","02a85832":"markdown","25496c84":"markdown","179b7c27":"markdown","2aaaf795":"markdown","9a0e471b":"markdown","4085fe57":"markdown","a1b5d2b5":"markdown","21d4050a":"markdown","bfd1b44f":"markdown","552155da":"markdown","28f331f8":"markdown","1840f992":"markdown","1d4827be":"markdown","f05822ec":"markdown","ccf838f2":"markdown","5f4e417c":"markdown","326e400c":"markdown","91bd1f6f":"markdown","239c6e45":"markdown","2651188d":"markdown","0fb6d99a":"markdown","4e57ae42":"markdown","6cae78e7":"markdown","b7d5f05d":"markdown","df2eeff4":"markdown","d2dd7b2d":"markdown","8d00b63b":"markdown","f5fd87e7":"markdown","489dff8f":"markdown","a2ca554d":"markdown","90483d37":"markdown","1e958eea":"markdown","c7fdb4f4":"markdown","28ef7fd3":"markdown","421f4ab2":"markdown","3be5d699":"markdown","831f340d":"markdown","7e261083":"markdown","940106e2":"markdown","d47da35a":"markdown","44b79817":"markdown","a91bb6c1":"markdown","777bb350":"markdown","36213ded":"markdown","92164791":"markdown","19386c0f":"markdown","bdea7fd3":"markdown","41a94582":"markdown","a75e5bb9":"markdown","eb1209ab":"markdown","d9c0c060":"markdown","dc1813a0":"markdown","c27d76d1":"markdown","5599fbf4":"markdown"},"source":{"3dbfea4f":"#### PIP INSTALLS ####\n\n#################################################################\n######################## Light Version ##########################\n#################################################################\n# - This installs a very light version of \ud83e\udd17 Transformers. \n# - In particular, no specific machine learning frameworks are installed. \n# - Since we\u2019ll be using a lot of different features of the library, \n#   we recommend installing the development version, which comes \n#   with all the required dependencies for pretty much \n#   any imaginable use case.\n# - Therefore we use transformers[sentencepiece] instead of transformers\n# - Note*: We use a -q argument to quiet the output that is displayed\n#################################################################\n# !pip install --upgrade transformers\n\n# Full Development Version\n!pip install -q --upgrade transformers[sentencepiece]\n\n# Install Flair NLP library - https:\/\/github.com\/flairNLP\/flair\n!pip install -q --upgrade flair","96bc5e15":"# Import the pipeline module\nfrom transformers import pipeline\n\n# Instantiate a classifier for sentiment analysis \nclassifier = pipeline(\"sentiment-analysis\")\nprint(classifier([\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"I hate this so much!\",\n]))","00195dec":"from transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nraw_inputs = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"I hate this so much!\",\n]\n\nprint(\"\\n\\n... TOKENIZER ...\\n\")\nprint(tokenizer)\n\nprint(\"\\n\\n\\n... RAW INPUTS ...\\n\")\nprint(raw_inputs)","024e76dc":"tf_inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n\nprint(\"\\n\\n... TF INPUTS ...\\n\")\nprint(tf_inputs)","77b91075":"pt_inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n\nprint(\"\\n\\n... PT INPUTS ...\\n\")\nprint(pt_inputs)","bb936a14":"np_inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"np\")\nprint(\"\\n\\n... NP INPUTS ...\\n\")\nprint(np_inputs)","44da3347":"# Import Tensorflow version of AutoModel\nfrom transformers import TFAutoModel\n\n# Specify the model checkpoint name\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n\n# Load the tensorflow version of the model from the checkpoint\ntf_model = TFAutoModel.from_pretrained(checkpoint)\n\nprint(\"\\n... TF MODEL ...\\n\")\nprint(tf_model)","f6d323c2":"# Import PyTorch version of AutoModel\nfrom transformers import AutoModel\n\n# Specify the model checkpoint name\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n\n# Load the pytorch version of the model from the checkpoint\npt_model = AutoModel.from_pretrained(checkpoint)\n\nprint(\"\\n... PT MODEL ...\\n\")\nprint(pt_model)","4a8b527c":"# `Call` the tensorflow model\ntf_outputs = tf_model(tf_inputs)\n\n# Print the output shape\nprint(\"\\n\\n... TF OUTPUT SHAPE ...\\n\")\nprint(tf_outputs.last_hidden_state.shape)\n\n# Print the raw outputs\nprint(\"\\n\\n\\n... TF RAW OUTPUTS ...\\n\")\nprint(tf_outputs)","f8d9e5cc":"# `Call` the pytorch model\npt_outputs = pt_model(**pt_inputs)\n\n# Print the output shape\nprint(\"\\n\\n... PT OUTPUT SHAPE ...\\n\")\nprint(pt_outputs.last_hidden_state.shape)\n\n# Print the raw outputs\nprint(\"\\n\\n\\n... PT RAW OUTPUTS ...\\n\")\nprint(pt_outputs)","fee05960":"# Import tensorflow version of the module required \n# to generate the sequence classification model \nfrom transformers import TFAutoModelForSequenceClassification\n\n# Specify the checkpoint for which we will load the required model\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n\n# Load the tensorflow model for sequence classification\ntf_model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n\n# `Call` the model\ntf_outputs = tf_model(tf_inputs)\n\nprint(\"\\n\\n... TF OUTPUT LOGITS' SHAPE ...\\n\")\nprint(tf_outputs.logits.shape)\n\nprint(\"\\n\\n\\n... TF RAW OUTPUTS ...\\n\")\nprint(tf_outputs)","ff04d4b7":"# Import pytorch version of the module required \n# to generate the sequence classification model \nfrom transformers import AutoModelForSequenceClassification\n\n# Specify the checkpoint for which we will load the required model\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n\n# Load the pytorch model for sequence classification\npt_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n\n# `Call` the model\npt_outputs = pt_model(**pt_inputs)\n\nprint(\"\\n\\n... PT OUTPUT LOGITS' SHAPE ...\\n\")\nprint(pt_outputs.logits.shape)\n\nprint(\"\\n\\n\\n... PT RAW OUTPUTS ...\\n\")\nprint(pt_outputs)","2a6246ba":"# Import Tensorflow\nimport tensorflow as tf\n\n# Calculate the softmax of the output logits\ntf_predictions = tf.math.softmax(tf_outputs.logits, axis=-1)\n\n# Print the label map\nprint(\"\\n\\n\\n... MODEL LABEL MAP ...\\n\")\nprint(tf_model.config.id2label)\n\n# Print the prediction probabilities\nprint(\"\\n\\n... TF OUTPUT PROBABILITIES ...\\n\")\nprint(tf_predictions)\n\n# Print the label map\nprint(\"\\n\\n... TF OUTPUT PROBABILITIES WITH LABEL TITLES ...\\n\")\nfor i, pred in enumerate(tf_predictions): print(f\"Sentence #{i+1} - Classification Probabilities:\\n\\t{tf_model.config.id2label[0]}: %{100*pred[0]:.3f}\\n\\t{tf_model.config.id2label[1]}: %{100*pred[1]:.3f}\\n\")","901ea973":"# AutoModelForSequenceClassification\ndef tf_sentiment_classification(model, tokenizer, sentences, return_lbl_map=True):\n    \"\"\" Function to perform sentiment analysis with pipeline in Tensorflow \n    \n    Args:\n        model (TFAutoModelForSequenceClassification): Pretrained model to use \n            for sentiment classification.\n        tokenizer (PreTrainedTokenizerFast): Tokenizer (agnostic to TF or PT)\n        sentences (list of strs): Sentences to perform sentiment classification on\n        return_lbl_map (bool, optional): Dictionary mapping id to string\n        \n    Returns:\n        Tensor containing probabilities of each particular sentiment \n        for the respective sentences. Optionally return the model label map.\n    \"\"\"\n    \n    # Create the pipeline\n    #     1. Preprocessing (tokenizers)\n    #     2. Model Inference\n    #     3. Postprocessing (softmax)\n    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"tf\")\n    raw_outputs = model(inputs)\n    predictions = tf.math.softmax(raw_outputs.logits, axis=-1)\n    \n    if return_lbl_map:\n        return predictions, model.config.id2label\n    else:\n        return predictions\n    \nex_sentences = [\"I really hate this song. You can't dance to it!\",\n                \"You're definitely not a bad guy.\",\n                \"The capital of California is Sacremento.\",\n                \"The capital of Canada is Ottawa.\"]\ntf_ex_preds, ex_lbl_map = tf_sentiment_classification(tf_model, tokenizer, ex_sentences)\nfor i, tf_pred in enumerate(tf_ex_preds): \n    print(f\"Sentence #{i+1} - '{ex_sentences[i]}'\\n\\t\" \\\n          f\"--> {ex_lbl_map[0]}: %{100*tf_pred[0]:.3f}\\n\\t\" \\\n          f\"--> {ex_lbl_map[1]}: %{100*tf_pred[1]:.3f}\\n\")","3627de7d":"# Import PyTorch\nimport torch\n\n# Calculate the softmax of the output logits\npt_predictions = torch.nn.functional.softmax(pt_outputs.logits, dim=-1)\n\n# Print the label map\nprint(\"\\n\\n\\n... MODEL LABEL MAP ...\\n\")\nprint(pt_model.config.id2label)\n\n# Print the prediction probabilities\nprint(\"\\n\\n... PT OUTPUT PROBABILITIES ...\\n\")\nprint(pt_predictions)\n\n# Print the label map\nprint(\"\\n\\n... PT OUTPUT PROBABILITIES WITH LABEL TITLES ...\\n\")\nfor i, pred in enumerate(pt_predictions): print(f\"Sentence #{i+1} - Classification Probabilities:\\n\\t{pt_model.config.id2label[0]}: %{100*pred[0]:.3f}\\n\\t{pt_model.config.id2label[1]}: %{100*pred[1]:.3f}\\n\")","494c8796":"def pt_sentiment_classification(model, tokenizer, sentences, return_lbl_map=True):\n    \"\"\" Function to perform sentiment analysis with pipeline in PyTorch \n    \n    Args:\n        model (AutoModelForSequenceClassification): Pretrained model to use \n            for sentiment classification.\n        tokenizer (PreTrainedTokenizerFast): Tokenizer (agnostic to TF or PT)\n        sentences (list of strs): Sentences to perform sentiment classification on\n        return_lbl_map (bool, optional): Dictionary mapping id to string\n        \n    Returns:\n        Tensor containing probabilities of each particular sentiment \n        for the respective sentences. Optionally return the model label map.\n    \"\"\"\n    \n    # Create the pipeline\n    #     1. Preprocessing (tokenizers)\n    #     2. Model Inference\n    #     3. Postprocessing (softmax)\n    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n    raw_outputs = model(**inputs)\n    predictions = torch.nn.functional.softmax(raw_outputs.logits, dim=-1)\n    if return_lbl_map:\n        return predictions, model.config.id2label\n    else:\n        return predictions\n    \npt_ex_preds, ex_lbl_map = pt_sentiment_classification(pt_model, tokenizer, ex_sentences)\nfor i, pt_pred in enumerate(pt_ex_preds): \n    print(f\"Sentence #{i+1} - '{ex_sentences[i]}'\\n\\t\" \\\n          f\"--> {ex_lbl_map[0]}: %{100*pt_pred[0]:.3f}\\n\\t\" \\\n          f\"--> {ex_lbl_map[1]}: %{100*pt_pred[1]:.3f}\\n\")","99596f45":"# Imports\nfrom transformers import BertConfig, TFBertModel\n\n# Building the config\nconfig = BertConfig()\n\n# Building the model from the config\ntf_model = TFBertModel(config)\n\nprint(\"\\n\\n\\n... TF BERT MODEL ...\\n\")\nprint(tf_model)\n\nprint(\"\\n\\n... BERT CONFIG ...\\n\")\nprint(config)","7a6701c6":"# Imports\nfrom transformers import BertConfig, BertModel\n\n# Building the config\nconfig = BertConfig()\n\n# Building the pt model from the config\npt_model = BertModel(config)\n\nprint(\"\\n\\n\\n... PT BERT MODEL ...\\n\")\nprint(pt_model)\n\nprint(\"\\n\\n... BERT CONFIG ...\\n\")\nprint(config)","cc32d39a":"from transformers import BertConfig, TFBertModel\n\n# Model is randomly initialized!\nconfig = BertConfig()\ntf_model = TFBertModel(config)\n\n# Checkpoint is downloaded and model loads from it (no config needed)\ntf_model = TFBertModel.from_pretrained(\"bert-base-cased\")\n\nprint(\"\\n\\n\\n... CONENTS OF `~\/.cache\/huggingface\/transformers` DIRECTORY ...\\n\")\n!ls -sh ~\/.cache\/huggingface\/transformers","70bb8280":"from transformers import BertConfig, BertModel\n\n# Model is randomly initialized!\nconfig = BertConfig()\npt_model = BertModel(config)\n\n# Checkpoint is downloaded and model loads from it (no config needed)\npt_model = BertModel.from_pretrained(\"bert-base-cased\")\n\nprint(\"\\n\\n\\n... CONENTS OF `~\/.cache\/huggingface\/transformers` DIRECTORY ...\\n\")\n!ls -sh ~\/.cache\/huggingface\/transformers","aa28524d":"import os\n\n###############################################\n#      Tensorflow Code (same as pytorch)      #\n###############################################\nos.makedirs(\"\/kaggle\/working\/tf_models\", exist_ok=True)\ntf_model.save_pretrained(\"\/kaggle\/working\/tf_models\")\nprint(\"\\n\\n\\n... CONTENTS OF `\/kaggle\/working\/tf_models` ...\\n\")\n!ls -sh \/kaggle\/working\/tf_models\n###############################################\n\n\n###############################################\n#      PyTorch Code (same as tensorflow)      #\n###############################################\nos.makedirs(\"\/kaggle\/working\/pt_models\", exist_ok=True)\npt_model.save_pretrained(\"\/kaggle\/working\/pt_models\")\nprint(\"\\n\\n\\n... CONTENTS OF `\/kaggle\/working\/pt_models` ...\\n\")\n!ls -sh \/kaggle\/working\/pt_models\n###############################################","cbc9c57a":"# Define the initial sequences (raw)\nsequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\nprint(f\"\\n\\n\\n... SEQUENCES --> {sequences}\")\nfor seq in sequences: print(f\"\\t--> {seq}\")\n\n# Tokenize the sequences (encoded)\nencoded_sequences = tokenizer(sequences)[\"input_ids\"]\nprint(f\"\\n\\n\\n... ENCODED SEQUENCES --> {encoded_sequences}\")\nfor enc_seq in encoded_sequences: print(f\"\\t--> {enc_seq}\")\n\n###############################################\n#               Tensorflow Code               #\n###############################################\ntf_model_inputs = tf.constant(encoded_sequences)\nprint(\"\\n\\n\\n... TENSORFLOW MODEL INPUTS - TENSORS ...\\n\")\nprint(tf_model_inputs)\n###############################################\n    \n###############################################\n#                 PyTorch Code                #\n###############################################\npt_model_inputs = torch.tensor(encoded_sequences)\nprint(\"\\n\\n\\n... PYTORCH MODEL INPUTS - TENSORS ...\\n\")\nprint(pt_model_inputs)\n###############################################","53fe521c":"###############################################\n#               Tensorflow Code               #\n###############################################\ntf_output = tf_model(tf_model_inputs)\nprint(\"\\n\\n\\n... TENSORFLOW MODEL OUTPUTS ...\\n\")\nprint(tf_output)\n###############################################\n\n###############################################\n#                 PyTorch Code                #\n###############################################\npt_output = pt_model(pt_model_inputs)\nprint(\"\\n\\n\\n... PYTORCH MODEL OUTPUTS ...\\n\")\nprint(pt_output)\n###############################################","9f6ee8c6":"# Define sentences to test on\ntext_1 = \"Jim Henson was a puppeteer\"\ntext_2 = \"Let's do tokenization!\"\n\nprint(f\"\\n\\n\\n\\n... ORIGINAL TEXT 1\\n\\t--> '{text_1}'\\n\")\nprint(f\"... ORIGINAL TEXT 2\\n\\t--> '{text_2}'\\n\")\n\nwb_tokens_1 = text_1.split()\nwb_tokens_2 = text_2.split()\n\nprint(f\"\\n\\n... WORD-BASED TOKENS FROM TEXT 1\\n\\t--> {wb_tokens_1}\\n\")\nprint(f\"... WORD-BASED TOKENS FROM TEXT 2\\n\\t--> {wb_tokens_2}\\n\")\n\ncb_tokens_1 = [x for x in text_1 if x is not \" \"]\ncb_tokens_2 = [x for x in text_2 if x is not \" \"]\n\nprint(f\"\\n\\n... CHARACTER-BASED TOKENS FROM TEXT 1\\n\\t--> {cb_tokens_1}\\n\")\nprint(f\"... CHARACTER-BASED TOKENS FROM TEXT 2\\n\\t--> {cb_tokens_2}\\n\")\n\nsw_tokens_1 = tokenizer.tokenize(text_1)\nsw_tokens_2 = tokenizer.tokenize(text_2)\n\nprint(f\"\\n\\n... SUBWORD-BASED TOKENS FROM TEXT 1\\n\\t--> {sw_tokens_1}\\n\")\nprint(f\"... SUBWORD-BASED TOKENS FROM TEXT 2\\n\\t--> {sw_tokens_2}\\n\")\n","e54ffa26":"from transformers import BertTokenizer\nfrom transformers import AutoTokenizer\n\n# Option 1\ntokenizer_1 = BertTokenizer.from_pretrained(\"bert-base-cased\")\ntokenizer_1.save_pretrained(\"\/kaggle\/working\/tokenizer_1\")\nprint(\"\\n\\n\\n... OPTION 1 - BertTokenizer ...\\n\")\nprint(tokenizer_1(\"Using a Transformer network is simple\"))\nprint(\"\\n\\n... AFTER SAVING - FILES LOCATED IN --> `\/kaggle\/working\/tokenizer_1` ...\\n\")\n!ls -sh \/kaggle\/working\/tokenizer_1\n\n# Option 2\ntokenizer_2 = AutoTokenizer.from_pretrained(\"bert-base-cased\")\ntokenizer_2.save_pretrained(\"\/kaggle\/working\/tokenizer_2\")\nprint(\"\\n\\n\\n... OPTION 2 - AutoTokenizer ...\\n\")\nprint(tokenizer_2(\"Using a Transformer network is simple\"))\nprint(\"\\n\\n... AFTER SAVING - FILES LOCATED IN --> `\/kaggle\/working\/tokenizer_2` ...\\n\")\n!ls -sh \/kaggle\/working\/tokenizer_2","8696e44b":"from transformers import AutoTokenizer\n# This tokenizer is a subword tokenizer: \n#      - it splits the words until it obtains tokens that can be represented by its vocabulary. \n#      - That\u2019s the case here with 'transformer', which is split into two tokens: \n#           --> transform\n#           --> ##er\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\nsequence = \"Using a Transformer network is simple\"\ntokens = tokenizer.tokenize(sequence)\n\nprint(f\"\\n\\n\\n... SEQUENCE\\n\\t--> '{sequence}'\\n\")\nprint(f\"... TOKENS (SUBWORD)\\n\\t--> {tokens}\\n\")","3f267b5d":"print(f\"\\n\\n{'-'*85}\\n\\t\\t\\tCONTINUING EXAMPLE FROM PREVIOUS CELL\\n{'-'*85}\")\nids = tokenizer.convert_tokens_to_ids(tokens)\nprint(f\"\\n\\n... SEQUENCE\\n\\t--> '{sequence}'\\n\")\nprint(f\"... TOKENS (SUBWORD)\\n\\t--> {tokens}\\n\")\nprint(f\"... INPUT IDS\\n\\t--> {ids}\\n\")\n\nprint(f\"\\n\\n{'-'*80}\\n\\t\\t\\tEXAMPLES FROM PREVIOUS SECTION\\n{'-'*80}\")\nprev_seq_1 = \"I\u2019ve been waiting for a HuggingFace course my whole life.\"\nprev_gen_tokens_1 = [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102]\nprev_seq_2 = \"I hate this so much!\"\nprev_gen_tokens_2 = [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\nfor i, (seq, pg_seq_ids) in enumerate(zip([prev_seq_1, prev_seq_2], [prev_gen_tokens_1, prev_gen_tokens_2])):\n    seq_tokens = tokenizer.tokenize(seq)\n    seq_ids = tokenizer.convert_tokens_to_ids(seq_tokens)\n    print(f\"\\n\\n... SEQUENCE\\n\\t--> '{seq}'\\n\")\n    print(f\"... TOKENS (SUBWORD)\\n\\t--> {seq}\\n\")\n    print(f\"... INPUT IDS (previously generated)\\n\\t--> {pg_seq_ids}\\n\")\n    print(f\"... INPUT IDS (newly generated)\\n\\t--> {seq_ids}\\n\")","e4ec55f5":"print(f\"\\n\\n{'-'*85}\\n\\t\\t\\tCONTINUING EXAMPLE FROM PREVIOUS CELL\\n{'-'*85}\")\ndecoded_sequence = tokenizer.decode(ids)\nprint(f\"\\n\\n... SEQUENCE\\n\\t--> '{sequence}'\\n\")\nprint(f\"... TOKENS (SUBWORD)\\n\\t--> {tokens}\\n\")\nprint(f\"... INPUT IDS\\n\\t--> {ids}\\n\")\nprint(f\"... DECODED SEQUENCE\\n\\t--> '{decoded_sequence}'\\n\")","694c6fe9":"# Import a tokenizer\nfrom transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nsequence = \"I've been waiting for a HuggingFace course my whole life.\"\ntokens = tokenizer.tokenize(sequence)\nids = tokenizer.convert_tokens_to_ids(tokens)","1519f546":"# Tensorflow specific import, input coercion and model instantiation\nfrom transformers import TFAutoModelForSequenceClassification\ntf_model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\ntf_input_ids = tf.constant(ids)\n\n# ********** This line will fail **********\n#  - Call the model on the inputs.\n# ********** This line will fail **********\ntry:\n    print(tf_model(tf_input_ids))\nexcept:\n    print(\"You would have seen this error if we hadn't `try\/except` captured it.\\n\\n\")\n    print(\"\\tInvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]\")\n# ********** This line will fail **********","08f9d17a":"# PyTorch specific import, input coercion and model instantiation\nfrom transformers import AutoModelForSequenceClassification\npt_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\npt_input_ids = torch.tensor(ids)\n\n# ********** This line will fail **********\n#  - Call the model on the inputs.\n# ********** This line will fail **********\ntry:\n    print(pt_model(pt_input_ids))\nexcept:\n    print(\"You would have seen this error if we hadn't `try\/except` captured it.\\n\\n\")\n    print(\"\\tIndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)\\n\\n\")\n# ********** This line will fail **********","546cf1a3":"tf_input_ids = tf.constant([ids])\ntf_output = tf_model(tf_input_ids)\n\nprint(\"\\n\\n... TENSORFLOW MODEL ...\")\nprint(\"\\n\\t--> Input IDs:\\n\\t\\t\", tf_input_ids)\nprint(\"\\n\\t--> Logits:\\n\\t\\t\", tf_output.logits)\nprint(\"\\n\\t--> Logits Shape:\\n\\t\\t\", tf_output.logits.shape)","a23696ad":"pt_input_ids = torch.tensor([ids])\npt_output = pt_model(pt_input_ids)\n\nprint(\"\\n\\n... PYTORCH MODEL ...\")\nprint(\"\\n\\t--> Input IDs:\\n\\t\\t\", pt_input_ids)\nprint(\"\\n\\t--> Logits:\\n\\t\\t\", pt_output.logits)\nprint(\"\\n\\t--> Logits Shape:\\n\\t\\t\", pt_output.logits.shape)","74d34d5d":"# Batch the ids as a list\nbatched_ids = [ids, ids]\n\n###############################################\n#               Tensorflow Code               #\n###############################################\ntf_input_ids = tf.constant(batched_ids)\ntf_output = tf_model(tf_input_ids)\n\nprint(\"\\n\\n... TENSORFLOW MODEL ...\")\nprint(\"\\n\\t--> Input IDs:\\n\\t\\t\", tf_input_ids)\nprint(\"\\n\\t--> Logits:\\n\\t\\t\", tf_output.logits)\nprint(\"\\n\\t--> Logits Shape:\\n\\t\\t\", tf_output.logits.shape)\n###############################################\n    \n###############################################\n#                 PyTorch Code                #\n###############################################\npt_input_ids = torch.tensor(batched_ids)\npt_output = pt_model(pt_input_ids)\n\nprint(\"\\n\\n... PYTORCH MODEL ...\")\nprint(\"\\n\\t--> Input IDs:\\n\\t\\t\", pt_input_ids)\nprint(\"\\n\\t--> Logits:\\n\\t\\t\", pt_output.logits)\nprint(\"\\n\\t--> Logits Shape:\\n\\t\\t\", pt_output.logits.shape)\n###############################################","a92d1379":"# Initialize the sequences as described above\nsequence1_ids = [[200, 200, 200]]\nsequence2_ids = [[200, 200]]\nbatched_ids = [\n    [200, 200, 200],\n    [200, 200, tokenizer.pad_token_id], #tokenizer.pad_token_id=0\n]\n\n# Pass the various sequences through the model and compare the logits\nprint(\"\\n\\n\\n... TENSORFLOW MODEL...\")\ntf_model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\nprint(f\"\\n\\tSEQUENCE ONE LOGITS\\n\\t\\t--> {tf_model(tf.constant(sequence1_ids)).logits}\")\nprint(f\"\\n\\tSEQUENCE TWO LOGITS\\n\\t\\t--> {tf_model(tf.constant(sequence2_ids)).logits}\")\nprint(f\"\\n\\tBATCHED SEQUENCE LOGITS\\n\\t\\t--> {tf_model(tf.constant(batched_ids)).logits}\\n\")","09350666":"# Initialize the sequences as described above\nsequence1_ids = [[200, 200, 200]]\nsequence2_ids = [[200, 200]]\nbatched_ids = [\n    [200, 200, 200],\n    [200, 200, tokenizer.pad_token_id], #tokenizer.pad_token_id=0\n]\n\n# Pass the various sequences through the model and compare the logits\nprint(\"\\n\\n\\n... PYTORCH MODEL...\")\npt_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\nprint(f\"\\n\\tSEQUENCE ONE LOGITS\\n\\t\\t--> {pt_model(torch.tensor(sequence1_ids)).logits}\")\nprint(f\"\\n\\tSEQUENCE TWO LOGITS\\n\\t\\t--> {pt_model(torch.tensor(sequence2_ids)).logits}\")\nprint(f\"\\n\\tBATCHED SEQUENCE LOGITS\\n\\t\\t--> {pt_model(torch.tensor(batched_ids)).logits}\\n\")","56b6cd80":"batched_ids = [\n    [200, 200, 200],\n    [200, 200, tokenizer.pad_token_id],\n]\nattention_mask = [\n    [1, 1, 1],\n    [1, 1, 0],\n]\n\n###############################################\n#               Tensorflow Code               #\n###############################################\ntf_outputs = tf_model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))\nprint(\"\\n\\n\\n... TENSORFLOW OUTPUTS ...\\n\")\nprint(tf_outputs.logits)\n###############################################\n    \n###############################################\n#                 PyTorch Code                #\n###############################################\npt_outputs = pt_model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\nprint(\"\\n\\n... PYTORCH OUTPUTS ...\\n\")\nprint(pt_outputs.logits)\n###############################################","587cff3d":"checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# tin = try_it_now\ntin_seq_1 = \"I\u2019ve been waiting for a HuggingFace course my whole life.\" \ntin_seq_2 = \"I hate this so much!\"\n\ntin_tokens_1 = tokenizer.tokenize(tin_seq_1)\ntin_ids_1 = tokenizer.convert_tokens_to_ids(tin_tokens_1)\n\ntin_tokens_2 = tokenizer.tokenize(tin_seq_2)\ntin_ids_2 = tokenizer.convert_tokens_to_ids(tin_tokens_2)\n\n# Pad either side respectively with the pad_token_id\nbatched_tin_ids = [\n    tin_ids_1+[tokenizer.pad_token_id,]*max(0, (len(tin_ids_2)-len(tin_ids_1))),\n    tin_ids_2+[tokenizer.pad_token_id,]*max(0, (len(tin_ids_1)-len(tin_ids_2))),\n]\n\ntin_attention_mask = [\n    [1,]*len(tin_ids_1)+[0,]*max(0, (len(tin_ids_2)-len(tin_ids_1))),\n    [1,]*len(tin_ids_2)+[0,]*max(0, (len(tin_ids_1)-len(tin_ids_2))),\n]\n\nprint(\"\\n\\n\\n... GENERAL INFO SEQUENCE 1 ...\\n\")\nprint(f\"\\n\\tSEQUENCE ONE STRING\\n\\t\\t--> '{tin_seq_1}'\")\nprint(f\"\\n\\tSEQUENCE ONE TOKENS\\n\\t\\t--> {tin_tokens_1}\")\nprint(f\"\\n\\tSEQUENCE ONE INPUT IDS\\n\\t\\t--> {tin_ids_1}\\n\")\n\nprint(\"\\n\\n... GENERAL INFO SEQUENCE 2 ...\\n\")\nprint(f\"\\n\\tSEQUENCE TWO STRING\\n\\t\\t--> '{tin_seq_2}'\")\nprint(f\"\\n\\tSEQUENCE TWO TOKENS\\n\\t\\t--> {tin_tokens_2}\")\nprint(f\"\\n\\tSEQUENCE TWO INPUT IDS\\n\\t\\t--> {tin_ids_2}\\n\")\n\nprint(\"\\n\\n... GENERAL INFO BATCHED SEQUENCES ...\\n\")\nprint(f\"\\n\\tBATCHED SEQUENCES\\n\\t\\t--> {[tin_seq_1, tin_seq_2]}\")\nprint(f\"\\n\\tBATCHED TOKENS\\n\\t\\t--> {[tin_tokens_1, tin_tokens_2]}\")\nprint(f\"\\n\\tBATCHED & PADDED INPUT IDS\\n\\t\\t--> {batched_tin_ids}\")\nprint(f\"\\n\\tATTENTION MASK FOR MODEL INPUT\\n\\t\\t--> {tin_attention_mask}\\n\")\n\nprint(\"\\n\\n\\n... MODEL INFORMATION ...\\n\")\n###############################################\n#               Tensorflow Code               #\n###############################################\ntf_model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\ntf_tin_unbatched_output_1 = tf_model(tf.constant([tin_ids_1,])).logits\ntf_tin_unbatched_output_2 = tf_model(tf.constant([tin_ids_2,])).logits\ntf_tin_batched_output = tf_model(tf.constant(batched_tin_ids),  attention_mask=tf.constant(tin_attention_mask)).logits\nprint(\"\\n\\n... TENSORFLOW MODEL ...\\n\")\nprint(f\"\\n\\tSEQUENCE ONE UNBATCHED - OUTPUT LOGITS\\n\\t\\t--> {tf_tin_unbatched_output_1}\")\nprint(f\"\\n\\tSEQUENCE TWO UNBATCHED - OUTPUT LOGITS\\n\\t\\t--> {tf_tin_unbatched_output_2}\")\nprint(f\"\\n\\tSEQUENCES BATCHED - OUTPUT LOGITS\\n\\t\\t--> {tf_tin_batched_output}\\n\")\n###############################################\n\n# ###############################################\n# #                 PyTorch Code                #\n# ###############################################\npt_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\npt_tin_unbatched_output_1 = pt_model(torch.tensor([tin_ids_1,])).logits\npt_tin_unbatched_output_2 = pt_model(torch.tensor([tin_ids_2,])).logits\npt_tin_batched_output = pt_model(torch.tensor(batched_tin_ids), attention_mask=torch.tensor(tin_attention_mask)).logits\nprint(\"\\n\\n... PYTORCH MODEL ...\\n\")\nprint(f\"\\n\\tSEQUENCE ONE UNBATCHED - OUTPUT LOGITS\\n\\t\\t--> {pt_tin_unbatched_output_1}\")\nprint(f\"\\n\\tSEQUENCE TWO UNBATCHED - OUTPUT LOGITS\\n\\t\\t--> {pt_tin_unbatched_output_2}\")\nprint(f\"\\n\\tSEQUENCES BATCHED - OUTPUT LOGITS\\n\\t\\t--> {pt_tin_batched_output}\\n\")\n# ###############################################","c64c13f2":"from transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\nsequence = \"I've been waiting for a HuggingFace course my whole life.\"\nsequences_same_length = [\"Been holding out for a HuggingFace course my entire life.\", \"I've waited for a HuggingFace course so long!\"]\nsequences_diff_length = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n\n\nprint(\"\\n... WHAT CAN THE TOKENIZER OBJECT DO? ...\\n\")\n\n############################################################################\n#                        TOKENIZE A SINGLE SEQUENCE                        #\n############################################################################\nprint(\"\\n\\n\\tTOKENIZE A SINGLE SEQUENCE (RETURN TF COMPATIBLE TENSORS)...\\n\")\nprint(f\"\\t\\t--> {tokenizer(sequence, return_tensors='tf')}\\n\")\nprint(\"\\n\\tTOKENIZE A SINGLE SEQUENCE (RETURN PT COMPATIBLE TENSORS)...\\n\")\nprint(f\"\\t\\t--> {tokenizer(sequence, return_tensors='pt')}\\n\")\n############################################################################\n\n############################################################################\n#                TOKENIZE MULTIPLE SEQUENCES - SAME LENGTH                 #\n############################################################################\nprint(\"\\n\\n\\tTOKENIZE MULTIPLE SEQUENCES (SAME LENGTH)(RETURN TF COMPATIBLE TENSORS)...\\n\")\nprint(f\"\\t\\t--> {tokenizer(sequences_same_length, return_tensors='tf')}\\n\")\nprint(\"\\n\\tTOKENIZE MULTIPLE SEQUENCE (SAME LENGTH)(RETURN PT COMPATIBLE TENSORS)...\\n\")\nprint(f\"\\t\\t--> {tokenizer(sequences_same_length, return_tensors='pt')}\\n\")\n############################################################################\n\n############################################################################\n#              TOKENIZE MULTIPLE SEQUENCES - DIFFERENT LENGTH              #\n############################################################################\nprint(\"\\n\\n\\tTOKENIZE MULTIPLE SEQUENCES (DIFF LENGTH W\/ PADDING)(RETURN TF COMPATIBLE TENSORS)...\\n\")\nprint(f\"\\t\\t--> {tokenizer(sequences_diff_length, return_tensors='tf', padding=True)}\\n\")\nprint(\"\\n\\tTOKENIZE MULTIPLE SEQUENCE (DIFF LENGTH W\/ PADDING)(RETURN PT COMPATIBLE TENSORS)...\\n\")\nprint(f\"\\t\\t--> {tokenizer(sequences_diff_length, return_tensors='pt', padding=True)}\\n\")\n############################################################################\n\n############################################################################\n#                     PADDING UP TO SEQUENCE MAX LENGTH                    #\n############################################################################\n# Will pad the sequences up to the maximum sequence length\nprint(\"\\n\\n\\tTOKENIZE MULTIPLE SEQUENCES W\/ PADDING UP TO SEQUENCE MAX LENGTH (RETURN TF COMPATIBLE TENSORS)...\\n\")\nprint(f\"\\t\\t--> {tokenizer(sequences_diff_length, return_tensors='tf', padding='longest')}\\n\")\nprint(\"\\n\\tTOKENIZE MULTIPLE SEQUENCE W\/ PADDING UP TO SEQUENCE MAX LENGTH (RETURN PT COMPATIBLE TENSORS)...\\n\")\nprint(f\"\\t\\t--> {tokenizer(sequences_diff_length, return_tensors='pt', padding='longest')}\\n\")\n############################################################################\n\n############################################################################\n#                      PADDING UP TO MODEL MAX LENGTH                      #\n############################################################################\n# Will pad the sequences up to the model max length\n# (512 for BERT or DistilBERT)\nprint(\"\\n\\n\\tTOKENIZE MULTIPLE SEQUENCES W\/ PADDING UP TO MODEL MAX LENGTH (RETURN TF COMPATIBLE TENSORS)...\\n\")\nprint(f\"\\t\\t--> {tokenizer(sequences_diff_length, return_tensors='tf', padding='max_length')}\\n\")\nprint(\"\\n\\tTOKENIZE MULTIPLE SEQUENCE W\/ PADDING UP TO MODEL MAX LENGTH (RETURN PT COMPATIBLE TENSORS)...\\n\")\nprint(f\"\\t\\t--> {tokenizer(sequences_diff_length, return_tensors='pt', padding='max_length')}\\n\")\n############################################################################\n\n############################################################################\n#                      PADDING UP TO SPECIFIED LENGTH                      #\n############################################################################\n# Will pad the sequences up to the specified max length\nprint(\"\\n\\n\\tTOKENIZE MULTIPLE SEQUENCES W\/ PADDING UP TO SPECIFIED LENGTH (RETURN TF COMPATIBLE TENSORS)...\\n\")\nprint(f\"\\t\\t--> {tokenizer(sequences_diff_length, return_tensors='tf', padding='max_length', max_length=8, truncation=True)}\\n\")\nprint(\"\\n\\tTOKENIZE MULTIPLE SEQUENCE W\/ PADDING UP TO SPECIFIED LENGTH (RETURN PT COMPATIBLE TENSORS)...\\n\")\nprint(f\"\\t\\t--> {tokenizer(sequences_diff_length, return_tensors='pt', padding='max_length', max_length=8, truncation=True)}\\n\")\n############################################################################\n\n############################################################################\n#                       TRUNCATE TO MODEL MAX LENGTH                       #\n############################################################################\n# Will truncate the sequences that are longer than the model max length\n# (512 for BERT or DistilBERT)\nprint(\"\\n\\n\\tTOKENIZE MULTIPLE SEQUENCES W\/ TRUNCATION TO MODEL MAX LENGTH (RETURN TF COMPATIBLE TENSORS)...\\n\")\nprint(f\"\\t\\t--> {tokenizer(sequences_diff_length, return_tensors='tf', truncation=True, padding=True)}\\n\")\nprint(\"\\n\\tTOKENIZE MULTIPLE SEQUENCES W\/ TRUNCATION TO MODEL MAX LENGTH (RETURN PT COMPATIBLE TENSORS)...\\n\")\nprint(f\"\\t\\t--> {tokenizer(sequences_diff_length, return_tensors='pt', truncation=True, padding=True)}\\n\")\n############################################################################\n\n############################################################################\n#                      TRUNCATE TO A SPECIFIED LENGTH                      #\n############################################################################\n# Will truncate the sequences that are longer than the specified max length\nprint(\"\\n\\n\\tTOKENIZE MULTIPLE SEQUENCES W\/ PADDING UP TO SPECIFIED LENGTH (RETURN TF COMPATIBLE TENSORS)...\\n\")\nprint(f\"\\t\\t--> {tokenizer(sequences_diff_length, return_tensors='tf', padding=True, truncation=True, max_length=8)}\\n\")\nprint(\"\\n\\tTOKENIZE MULTIPLE SEQUENCE W\/ PADDING UP TO SPECIFIED LENGTH (RETURN PT COMPATIBLE TENSORS)...\\n\")\nprint(f\"\\t\\t--> {tokenizer(sequences_diff_length, return_tensors='pt', padding=True, truncation=True, max_length=8)}\\n\")\n############################################################################","410d1e69":"sequence = \"I've been waiting for a HuggingFace course my whole life.\"\nprint(\"\\n\\n\\n... SEQUENCE ...\")\nprint(f\"\\t--> '{sequence}'\")\n\nmodel_inputs = tokenizer(sequence)\nprint(\"\\n\\n... MODEL INPUT IDS (STYLE 1 - FOR MODEL CONSUMPTION) ...\")\nprint(\"\\t-->\", model_inputs[\"input_ids\"])\n\ntokens = tokenizer.tokenize(sequence)\nids = tokenizer.convert_tokens_to_ids(tokens)\nprint(\"\\n\\n... MODEL INPUT IDS (STYLE 2 - DIRECT CONVERSION NOT READY FOR MODEL) ...\")\nprint(\"\\t-->\", ids)\n\ndecoded_style_1 = tokenizer.decode(model_inputs[\"input_ids\"])\nprint(\"\\n\\n... MODEL INPUT IDS DECODED (STYLE 1 - FOR MODEL CONSUMPTION) ...\")\nprint(f\"\\t--> '{decoded_style_1}'\")\n\ndecoded_style_2 = tokenizer.decode(ids)\nprint(\"\\n\\n... MODEL INPUT IDS DECODED (STYLE 2 - DIRECT CONVERSION NOT READY FOR MODEL) ...\")\nprint(f\"\\t--> '{decoded_style_2}'\")","883975b9":"# Step 1 - Imports\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n\n# Step 2 - Initialize Tokenizer and Model\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n\n# Step 3 - Create a list of sentences\nsequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n\n# Step 4 - Tokenize the sentences and pass them to the model\ntokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\noutput = model(**tokens)\n\n# Step 5 - Print the output\nprint(f\"\\n\\n\\n... TENSORFLOW MODEL OUTPUT\\n\\n{output}\")","b9d03bec":"# Step 1 - Imports\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# Step 2 - Initialize Tokenizer and Model\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n\n# Step 3 - Create a list of sentences\nsequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n\n# Step 4 - Tokenize the sentences and pass them to the model\ntokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\noutput = model(**tokens)\n\n# Step 5 - Print the output\nprint(f\"\\n\\n\\n... PYTORCH MODEL OUTPUT\\n\\n{output}\")","74ea825e":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b><b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\"> &nbsp;&&nbsp; <\/b><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","ad19d3e9":"<a id=\"2_2\"><\/a>\n\n<br><h3 style=\"font-family: Georgia; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">2.2 MODELS&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#chapter_2\">&#10514;<\/a><\/h3>\n\n---\n\n- <a href=\"https:\/\/www.youtube.com\/watch?v=d3JVgghSOew\" style=\"font-family: Georgia; color: #ff6f00; font-weight: bold;\">[TENSORFLOW] &nbsp;&nbsp; VIDEO LINK - INSTANTIATE A TRANSFORMER MODEL - HUGGING FACE CHANNEL<\/a>\n- <a href=\"https:\/\/www.youtube.com\/watch?v=AhChOFRegn4\" style=\"font-family: Georgia; color: #EE4C2C; font-weight: bold;\">[PYTORCH] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VIDEO LINK - INSTANTIATE A TRANSFORMER MODEL - HUGGING FACE CHANNEL<\/a><br>\n\n<p style=\"font-family: Georgia;\">In this section we\u2019ll take a closer look at creating and using a model. We\u2019ll use the <b>[<b style=\"color: #ff6f00;\">TFAutoModel<\/b>|<b style=\"color: #EE4C2C;\">AutoModel<\/b>]<\/b> class, which is handy when you want to instantiate any model from a checkpoint.<\/p>\n\n<p style=\"font-family: Georgia;\">The <b>[<b style=\"color: #ff6f00;\">TFAutoModel<\/b>|<b style=\"color: #EE4C2C;\">AutoModel<\/b>]<\/b> class and all of its relatives are actually simple wrappers over the wide variety of models available in the library. It\u2019s a clever wrapper as it can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture.<\/p>\n\n<p style=\"font-family: Georgia;\">However, if you know the type of model you want to use, you can use the class that defines its architecture directly. Let\u2019s take a look at how this works with a <b><a href=\"https:\/\/arxiv.org\/abs\/1810.04805\">BERT<\/a><\/b> model.<\/p><br>\n\n<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">Creating a Transformer<\/b>\n\n<p style=\"font-family: Georgia;\">The first thing we\u2019ll need to do to initialize a BERT model is load a configuration object. The configuration contains many attributes that are used to build the model. While you haven\u2019t seen what all of these attributes do yet, you should recognize some of them.<\/p>\n\n<ul style=\"font-family: Georgia;\">\n    <li>The <b><code>hidden_size<\/code><\/b> attribute defines the size of the hidden_states<\/code><\/b> vector<\/li>\n    <li>The <b><code>num_hidden_layers<\/code><\/b> defines the number of layers the Transformer model has.<\/li>\n<\/ul>","503cbf5f":"<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">PADDING THE INPUTS<\/b>\n\n<p style=\"font-family: Georgia;\">The following list of lists cannot be converted to a tensor:<\/p>\n\n<pre style=\"font-weight: bold; white-space: pre-wrap; background-color: #eee; border: 1px dashed #999; display: block; padding: 16px; line-height:160%\">\n    batched_ids = [\n        <span style=\"color: brown;\">[200, 200, 200]<\/span>,\n        <span style=\"color: brown;\">[200, 200]<\/span>,\n    ]\n<\/pre>\n\n<p style=\"font-family: Georgia;\">In order to work around this, we\u2019ll use padding to make our tensors have a rectangular shape. <b><i>Padding<\/i><\/b> makes sure all our sentences have the same length by adding a special word called the <b><i>padding token<\/i><\/b> to the sentences with fewer values. For example, if you have 10 sentences with 10 words and 1 sentence with 20 words, <b><i>padding<\/i><\/b> will ensure all the sentences have 20 words. In our example, the resulting tensor looks like this:<\/p>\n\n<pre style=\"font-weight: bold; white-space: pre-wrap; background-color: #eee; border: 1px dashed #999; display: block; padding: 16px; line-height:160%\">\n    padding_id = <span style=\"color: brown;\">100<\/span>\n    batched_ids = [\n        <span style=\"color: brown;\">[200, 200, 200]<\/span>,\n        <span style=\"color: brown;\">[200, 200, padding_id]<\/span>,\n    ]\n<\/pre>\n\n\n<p style=\"font-family: Georgia;\">The padding token ID can be found in <b><code>tokenizer.pad_token_id<\/code><\/b> (0 in this case). Let\u2019s use it and send our two sentences through the model individually and batched together.<\/p>","3e648e5c":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","53b40ded":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","8ab80659":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">A high-Dimensional vector?<\/b>\n\n<p style=\"font-family: Georgia;\">The vector output by the Transformer module is usually large. It generally has three dimensions:<\/p>\n\n<ol style=\"font-family: Georgia;\">\n    <li>\n        <b>Batch Size:<\/b> The number of sequences processed at a time (2 in our example).\n    <\/li>\n    <li>\n        <b>Sequence Length:<\/b> The length of the numerical representation of the sequence (16 in our example).\n    <\/li>\n    <li>\n        <b>Hidden Size:<\/b> The vector dimension of each model input.\n    <\/li>\n<\/ol>\n\n<p style=\"font-family: Georgia;\">It is said to be <b><i>\u201chigh dimensional\u201d<\/i><\/b> because of the last value. The hidden size can be very large (<b><code>768<\/code><\/b> is common for smaller models, and in larger models this can reach <b><code>3072<\/code><\/b> or more).<\/p>\n\n<p style=\"font-family: Georgia;\">We can see this below when we feed the inputs we preprocessed to our model. Note that the outputs of \ud83e\udd17 Transformers models behave like <a href=\"https:\/\/www.geeksforgeeks.org\/namedtuple-in-python\/\"><b><code>namedtuples<\/code><\/b><\/a> or <a href=\"https:\/\/www.w3schools.com\/python\/python_dictionaries.asp\"><b><code>dictionaries<\/code><\/b><\/a>. You can access the elements by attributes (like we did) or by key (<b><code>outputs[\"last_hidden_state\"]<\/code><\/b>), or even by index if you know exactly where the thing you are looking for is (<b><code>outputs[0]<\/code><\/b>).<\/p>","95fb6748":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","02a85832":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","25496c84":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","179b7c27":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">USING A TRANSFORMER MODEL FOR INFERENCE<\/b>\n\n<p style=\"font-family: Georgia;\">Now that you know how to load and save a model, let\u2019s try using it to make some predictions. Transformer models can only process numbers \u2014 numbers that the tokenizer generates. But before we discuss tokenizers, let\u2019s explore what inputs the model accepts.<\/p>\n\n<p style=\"font-family: Georgia;\">Tokenizers can take care of casting the inputs to the appropriate framework\u2019s tensors, but to help you understand what\u2019s going on, we\u2019ll take a quick look at what must be done before sending the inputs to the model.<\/p>\n\n<p style=\"font-family: Georgia;\">Let\u2019s say we have a couple of sequences. The tokenizer converts these to vocabulary indices which are typically called <b><i>input IDs.<\/i><\/b> Each sequence will now be a list of numbers! The resulting output is a list of encoded sequences: a list of lists. Tensors only accept rectangular shapes (think matrices). This \u201carray\u201d is already of rectangular shape, so converting it to a tensor is easy.<\/p>","2aaaf795":"<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">LONGER SEQUENCES<\/b>\n\n<p style=\"font-family: Georgia;\">With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. There are two solutions to this problem:\n\n<ul style=\"font-family: Georgia;\">\n    <li>Use a model with a longer supported sequence length.<\/li>\n    <li>Truncate your sequences.<\/li>\n<\/ul>\n\nModels have different supported sequence lengths, and some specialize in handling very long sequences. <b><a href=\"https:\/\/huggingface.co\/transformers\/model_doc\/longformer.html\">Longformer<\/a><\/b> is one example, and another is <b><a href=\"https:\/\/huggingface.co\/transformers\/model_doc\/led.html\">LED<\/a><\/b>. If you\u2019re working on a task that requires very long sequences, we recommend you take a look at those models.<\/p>\n\n<p style=\"font-family: Georgia;\">Otherwise, we recommend you truncate your sequences by specifying the <b><code>max_sequence_length<\/code><\/b> parameter<\/p>\n\n<pre style=\"font-weight: bold; white-space: pre-wrap; background-color: #eee; border: 1px dashed #999; display: block; padding: 16px; line-height:160%\">\n    sequence = sequence[:max_sequence_length]\n<\/pre>","9a0e471b":"<br><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","4085fe57":"<a id=\"2_1\"><\/a>\n\n<br><h3 style=\"font-family: Georgia; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">2.1 BEHIND THE PIPELINE&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#chapter_2\">&#10514;<\/a><\/h3>\n\n---\n\n<center><div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px; \">\n    <br><b>\u26a0\ufe0f&nbsp;&nbsp;This is the first section where the content is slightly different depending on whether you use PyTorch and TensorFlow. In the course you can toggle to see both versions of the code... in this notebook we will use both libraries simultaneously as a learning tool to compare and contrast Tensorflow and PyTorch<\/b><br><br>\n<\/div><\/center>\n\n- <a href=\"https:\/\/www.youtube.com\/watch?v=wVN12smEvqg&t=1s\" style=\"font-family: Georgia; color: #ff6f00; font-weight: bold;\">[TENSORFLOW] &nbsp;&nbsp; VIDEO LINK - WHAT HAPPENS INSIDE THE PIPELINE FUNCTION - HUGGING FACE CHANNEL<\/a>\n- <a href=\"https:\/\/www.youtube.com\/watch?v=1pedAIvTWXk\" style=\"font-family: Georgia; color: #EE4C2C; font-weight: bold;\">[PYTORCH] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VIDEO LINK - WHAT HAPPENS INSIDE THE PIPELINE FUNCTION - HUGGING FACE CHANNEL<\/a><br>\n\n\n\n<p style=\"font-family: Georgia;\">Let\u2019s start with a complete example, taking a look at what happened behind the scenes when we executed the following code in <a href=\"#chapter_1\"><b>Chapter 1<\/b><\/a>:<\/p>","a1b5d2b5":"<img src=\"https:\/\/repository-images.githubusercontent.com\/155220641\/a16c4880-a501-11ea-9e8f-646cf611702e\"><\/img>\n\n---\n\n<p style=\"font-family: Georgia;\">This collection of notebooks will walk you through the entire <a src=\"https:\/\/huggingface.co\/course\/chapter0\/1?fw=tf\" style=\"font-weight: bold;\"><b>Hugging Face Transformers Course:<\/b><\/a><\/p>\n\n---\n\n<b style=\"font-family: Georgia;\">Links To Chapter Notebooks&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style=\"color: red; font-size: 11px;\">(This Notebook Will Cover Chapter 2)<\/span><\/b>\n<ul style=\"font-family: Georgia;\">\n    <li><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-1-tf-torch#chapter_0\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">0<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">SEtUP<\/span><\/b><\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-1-tf-torch#chapter_1\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">1<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">Transformer Models<\/span><\/b><\/a><\/li>\n    <li><a href=\"#toc\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">2<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">Using \ud83e\udd17 Transformers<\/span><\/b><\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-3-tf-torch\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">3<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">Fine-Tuning A Pretrained Model<\/span><\/b><\/a><\/li>\n    <li><a href=\"#\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">4<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">Sharing Models And Tokenizers<\/span><\/b><\/a><\/li>\n    <li><a href=\"#\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">5<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">The \ud83e\udd17 Datasets Library<\/span><\/b><\/a><\/li>\n    <li><a href=\"#\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">6<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">The \ud83e\udd17 Tokenizers Library<\/span><\/b><\/a><\/li>\n    <li><a href=\"#\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">7<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">Main NLP Tasks<\/span><\/b><\/a><\/li>\n    <li><a href=\"#\"><b>Chapter <span style=\"font-family: Courier New; font-size: 16px;\">8<\/span> &nbsp;&nbsp; - &nbsp;&nbsp;<span style=\"letter-spacing: 0.15em; text-transform: uppercase;\">How To Ask For Help<\/span><\/b><\/a><\/li>    \n<\/ul>\n\n---\n\n<p style=\"font-family: Georgia;\">To find the original course please <a src=\"https:\/\/huggingface.co\/course\/chapter0\/1?fw=tf\" style=\"font-weight: bold; text-decoration: underline;\">>>>click here<<<<\/a><\/p>\n    \n---\n    \n<br>\n\n<center><div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px;\">\n    <br><b>\u26a0\ufe0f&nbsp;&nbsp;The vast majority of the text in this notebook will come directly from the HuggingFace Transformers course. If I would like to add in (or change anything), I will insert the information in a blue-box similar to this one.<\/b><br><br>\n<\/div><\/center>\n   \n    \n<center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px;\">\n    <br><b>\u26a0\ufe0f&nbsp;&nbsp;Hugging Face uses green blocks to inject notation into their course.<\/b><br><br>\n<\/div><\/center>\n","21d4050a":"<br><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","bfd1b44f":"<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">From tokens to input IDs<\/b>\n\n<p style=\"font-family: Georgia;\">The conversion from tokens to input IDs is handled by the <code><b>convert_tokens_to_ids()<\/b><\/code> tokenizer method. This will yield outputs that, once converted to the appropriate framework tensor, can then be used as inputs to a model as seen earlier in this chapter.<\/p>\n\n<center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px; \">\n    <br><b>\u270f\ufe0f&nbsp;&nbsp;<b style=\"color: black;\">TRY IT OUT!<\/b>&nbsp;&nbsp;&nbsp;&nbsp;<br><br>Replicate the two last steps (tokenization and conversion to input IDs) on the input sentences we used in <a href=\"#2_4\"><b>Section 2.4<\/b><\/a> (\u201cI\u2019ve been waiting for a HuggingFace course my whole life.\u201d and \u201cI hate this so much!\u201d). Check that you get the same input IDs we got earlier!<\/b><br><br>\n<\/div><\/center>\n","552155da":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","28f331f8":"<br><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","1840f992":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b><b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\"> &nbsp;&&nbsp; <\/b><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","1d4827be":"<a id=\"2_4\"><\/a>\n\n<br><h3 style=\"font-family: Georgia; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">2.4 TOKENIZER BASICS&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#chapter_2\">&#10514;<\/a><\/h3>\n\n---\n\n<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">Tokenizer Loading and saving<\/b>\n\n<p style=\"font-family: Georgia;\">Loading and saving tokenizers is as simple as it is with models. Actually, it\u2019s based on the same two methods: <code><b>from_pretrained()<\/b><\/code> and <code><b>save_pretrained()<\/b><\/code>. These methods will load or save the algorithm used by the tokenizer (a bit like the <b><i>architecture<\/i><\/b> of the model) as well as its vocabulary (a bit like the <b><i>weights<\/i><\/b> of the model).<\/p>\n<ul style=\"font-family: Georgia;\">\n    <li>Loading the <b><a href=\"https:\/\/arxiv.org\/abs\/1810.04805\">BERT<\/a><\/b> tokenizer trained with the same checkpoint as <b><a href=\"https:\/\/arxiv.org\/abs\/1810.04805\">BERT<\/a><\/b> is done the same way as loading the model, except we use the <b><code>BertTokenizer<\/code><\/b> class.<\/li>\n    <li>Similar to <b>[<b style=\"color: #ff6f00;\">TFAutoModel<\/b>|<b style=\"color: #EE4C2C;\">AutoModel<\/b>]<\/b>, the <b><code>AutoTokenizer<\/code><\/b> class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint.<\/li>\n    <li>Using the tokenizer is as simple as what was shown in the previous section<\/li>\n    <li>Saving a tokenizer is identical to saving a model<\/li>\n<\/ul>\n\n<p style=\"font-family: Georgia;\">We\u2019ll talk more about <b><code>token_type_ids<\/code><\/b> in <b><a href=\"\">Chapter 3<\/a><\/b>, and we\u2019ll explain the <b><code>attention_mask<\/code><\/b> key a little later. First, let\u2019s see how the <b><code>input_ids<\/code><\/b> are generated. To do this, we\u2019ll need to look at the intermediate methods of the tokenizer.<\/p>\n","f05822ec":"<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">Encoding<\/b>\n\n- <a href=\"https:\/\/youtu.be\/Yffk5aydLzg\" style=\"font-family: Georgia; color: darkred; font-weight: bold;\">VIDEO LINK - THE TOKENIZATION PIPELINE - HUGGING FACE CHANNEL<\/a><br>\n\n<p style=\"font-family: Georgia;\">Translating text to numbers is known as <b><i>encoding<\/i><\/b>. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs.<\/p>\n\n<p style=\"font-family: Georgia;\">As we\u2019ve seen, the first step is to split the text into words (or parts of words, punctuation symbols, etc.), usually called <b><i>tokens<\/i><\/b>. There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules that were used when the model was pretrained.<\/p>\n\n<p style=\"font-family: Georgia;\">The second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a <b><i>vocabulary<\/i><\/b>, which is the part we download when we instantiate it with the <b><code>from_pretrained()<\/code><\/b> method. Again, we need to use the same <b><i>vocabulary<\/i><\/b> used when the model was pretrained.<\/p>\n\n<p style=\"font-family: Georgia;\">To get a better understanding of the two steps, we\u2019ll explore them separately. Note that we will use some methods that perform parts of the tokenization pipeline separately to show you the intermediate results of those steps, but in practice, you should call the tokenizer directly on your inputs (as shown in the <b><a href=\"#2_1\">Section 2.1 - Behind The Pipeline<\/a><\/b>).<\/p><br>\n\n<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">Tokenization<\/b>\n\n<p style=\"font-family: Georgia;\">The tokenization process is done by invoking the <b><code>tokenize()<\/code><\/b> method of the tokenizer. This will yield an output that is a list of strings, or tokens.\n<\/p>\n","ccf838f2":"<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">ATTENTION MASKS<\/b>\n\n<p style=\"font-family: Georgia;\"><b>Hmmm, wait!<\/b> There was something wrong with the logits in our batched predictions: the second row should be the same as the logits for the second sentence, but we got completely different values!<\/p>\n\n<p style=\"font-family: Georgia;\">This is because the key feature of Transformer models is attention layers that <b><i>contextualize<\/i><\/b> each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an <b><i>attention mask<\/i><\/b>.<\/p>\n\n<p style=\"font-family: Georgia;\"><b><i>Attention masks<\/i><\/b> are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s:<\/p> \n<ul style=\"font-family: Georgia;\">\n    <li><b><code>1<\/code><\/b> indicates the corresponding tokens that should be attended to.<\/li>\n    <li><b><code>0<\/code><\/b> indicates the corresponding tokens that should not be attended to (i.e., they should be ignored by the attention layers of the model).<\/li>\n<\/ul>\n\n<p style=\"font-family: Georgia;\">Now we will complete the previous example with an attention mask. Because we are using an attention mask, we will get the same logits for the second sentence in the batch. Pay attention to how the last value of the second sequence is a padding ID, (which is a 0 value in the attention mask.)<\/p>\n\n<center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px; \">\n    <br><b>\u270f\ufe0f&nbsp;&nbsp;<b style=\"color: black;\">TRY IT OUT!<\/b><br><br>Apply the tokenization manually on the two sentences used in <a href=\"#2_4\"><b>Section 2.4<\/b><\/a> (\u201cI\u2019ve been waiting for a HuggingFace course my whole life.\u201d and \u201cI hate this so much!\u201d). Pass them through the model and check that you get the same logits as in <a href=\"#2_4\"><b>Section 2.4<\/b><\/a>.<br><br>Now batch them together using the padding token, then create the proper attention mask. Check that you obtain the same results when going through the model!<\/b><br><br>\n<\/div><\/center>","5f4e417c":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">Post-Processing The Output<\/b>\n\n<p style=\"font-family: Georgia;\">The values we get as output from our model don\u2019t necessarily make sense by themselves. Our model predicted <b><code>[-1.5607, 1.6123]<\/code><\/b> for the first sentence and <b><code>[4.1692, -3.3464]<\/code><\/b> for the second one. <b>Those are not probabilities but logits<\/b>, the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a <b><i>SoftMax Layer<\/i><\/b> (all \ud83e\udd17 Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as <b><i>SoftMax<\/i><\/b>, with the actual loss function, such as <b><i>Cross Entropy<\/i><\/b>).<\/p>\n\n<p style=\"font-family: Georgia;\">*After we run the output through a softmax function, we will see that the model predicts <b><code>[0.0402, 0.9598]<\/code><\/b> for the first sentence and <b><code>[0.9995, 0.0005]<\/code><\/b> for the second one. These are recognizable <b><i>probability scores<\/i><\/b>.\n\n<p style=\"font-family: Georgia;\">To get the labels corresponding to each position, we can inspect the <b><code>id2label<\/code><\/b> attribute of the model config (more on this in the next section). Using this mapping, we can conclude that the model predicted the following. After the demonstrating this below,we will  have successfully reproduced the three steps of the pipeline: preprocessing with tokenizers, passing the inputs through the model, and postprocessing!  We will now take some time to dive deeper into each of those steps.<\/p>\n\n<center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px; \">\n    <br><b>\u270f\ufe0f&nbsp;&nbsp;<b style=\"color: black;\">TRY IT OUT!<\/b>&nbsp;&nbsp;&nbsp;&nbsp; Choose two (or more) texts of your own and run them through the sentiment-analysis pipeline. Then replicate the steps you saw here yourself and check that you obtain the same results!<\/b><br><br>\n<\/div><\/center>","326e400c":"<a id=\"2_3\"><\/a>\n\n<br><h3 style=\"font-family: Georgia; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">2.3 TOKENIZER BACKGROUND&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#chapter_2\">&#10514;<\/a><\/h3>\n\n---\n\n- <a href=\"https:\/\/www.youtube.com\/watch?v=VFp38yj8h3A\" style=\"font-family: Georgia; color: darkred; font-weight: bold;\">VIDEO LINK - TOKENIZERS OVERVIEW - HUGGING FACE CHANNEL<\/a><br>\n\n<p style=\"font-family: Georgia;\">Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. <b><i>Models can only process numbers, so tokenizers need to convert our text inputs to numerical data<\/i><\/b>. In this section, we\u2019ll explore exactly what happens in the tokenization pipeline.<\/p>\n\n<p style=\"font-family: Georgia;\">In NLP tasks, the data that is generally processed is raw text. Here\u2019s an example of such text:<\/p>\n\n<pre style=\"font-weight: bold; white-space: pre-wrap; background-color: #eee; border: 1px dashed #999; display: block; padding: 16px; line-height:160%\">\n    Jim Henson was a puppeteer\n<\/pre>\n    \n<p style=\"font-family: Georgia;\">However, models can only process numbers, so we need to find a way to convert the raw text to numbers. That\u2019s what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation \u2014 that is, the one that makes the most sense to the model \u2014 and, if possible, the smallest representation.<\/p>\n\n<p style=\"font-family: Georgia;\">Let\u2019s take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization.<\/p><br>\n\n<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">WORD BASED TOKENIZERS<\/b>\n\n- <a href=\"https:\/\/www.youtube.com\/watch?v=nhJxYji1aho\" style=\"font-family: Georgia; color: darkred; font-weight: bold;\">VIDEO LINK - WORD BASED TOKENIZERS - HUGGING FACE CHANNEL<\/a><br>\n\n<p style=\"font-family: Georgia;\">The first type of tokenizer that comes to mind is word-based. It\u2019s generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:<\/p> \n\n<br><center><img src=\"https:\/\/huggingface.co\/course\/static\/chapter2\/word_based_tokenization.png\" width=95%><\/center><br>\n\n<p style=\"font-family: Georgia;\">There are different ways to split the text. For example, we could could use whitespace to tokenize the text into words by applying Python\u2019s <b><code>split()<\/code><\/b> function<\/p>\n\n<pre style=\"font-weight: bold; white-space: pre-wrap; background-color: #eee; border: 1px dashed #999; display: block; padding: 16px; line-height:160%\">\n    tokenized_text = \"Jim Henson was a puppeteer\".split()\n    print(tokenized_text)\n<\/pre>\n\n<pre style=\"font-weight: bold; white-space: pre-wrap; background-color: #eee; border: 1px dashed #999; display: block; padding: 16px; line-height:160%\">\n    ['Jim', 'Henson', 'was', 'a', 'puppeteer']\n<\/pre><br>\n\n<p style=\"font-family: Georgia;\">There are also variations of word tokenizers that have extra rules for punctuation. With this kind of tokenizer, we can end up with some pretty large <b><i>\u201cvocabularies,\u201d<\/i><\/b> where a  <b><i>vocabulary<\/i><\/b> is defined by the total number of independent tokens that we have in our corpus.<\/p>\n\n<p style=\"font-family: Georgia;\">Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word.<\/p>\n\n<p style=\"font-family: Georgia;\">If we want to completely cover a language with a word-based tokenizer, we\u2019ll need to have an identifier for each word in the language, which will generate a huge amount of tokens. For example, there are over 500,000 words in the English language, so to build a map from each word to an input ID we\u2019d need to keep track of that many IDs. Furthermore, words like \u201cdog\u201d are represented differently from words like \u201cdogs\u201d, and the model will initially have no way of knowing that \u201cdog\u201d and \u201cdogs\u201d are similar: it will identify the two words as unrelated. The same applies to other similar words, like \u201crun\u201d and \u201crunning\u201d, which the model will not see as being similar initially.<\/p>\n\n<p style=\"font-family: Georgia;\">Finally, we need a custom token to represent words that are not in our vocabulary. This is known as the \u201cunknown\u201d token, often represented as \u201d[UNK]\u201d or \u201d\u201d. It\u2019s generally a bad sign if you see that the tokenizer is producing a lot of these tokens, as it wasn\u2019t able to retrieve a sensible representation of a word and you\u2019re losing information along the way. The goal when crafting the vocabulary is to do it in such a way that the tokenizer tokenizes as few words as possible into the unknown token.<\/p>\n\n<p style=\"font-family: Georgia;\">One way to reduce the amount of unknown tokens is to go one level deeper, using a <b><i>character-based tokenizer<\/i><\/b>.<\/p><br>\n\n<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">CHARACTER-BASED TOKENIZERS<\/b>\n\n- <a href=\"https:\/\/www.youtube.com\/watch?v=ssLq_EK2jLE\" style=\"font-family: Georgia; color: darkred; font-weight: bold;\">VIDEO LINK - CHARACTER-BASED TOKENIZERS - HUGGING FACE CHANNEL<\/a><br>\n\n<p style=\"font-family: Georgia;\">Character-based tokenizers split the text into characters, rather than words. This has two primary benefits:<\/p>\n\n<ul style=\"font-family: Georgia;\">\n    <li>The vocabulary is much smaller.<\/li>\n    <li>There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.<\/li>\n<\/ul>\n\n<p style=\"font-family: Georgia;\">But here too some questions arise concerning spaces and punctuation:<\/p> \n\n<br><center><img src=\"https:\/\/huggingface.co\/course\/static\/chapter2\/character_based_tokenization.png\" width=95%><\/center><br>\n\n<p style=\"font-family: Georgia;\">This approach isn\u2019t perfect either. Since the representation is now based on characters rather than words, one could argue that, intuitively, it\u2019s less meaningful: each character doesn\u2019t mean a lot on its own, whereas that is the case with words. However, this again differs according to the language; in Chinese, for example, each character carries more information than a character in a Latin language.<\/p>\n\n<p style=\"font-family: Georgia;\">Another thing to consider is that we\u2019ll end up with a very large amount of tokens to be processed by our model: whereas a word would only be a single token with a word-based tokenizer, it can easily turn into 10 or more tokens when converted into characters.<\/p>\n\n<p style=\"font-family: Georgia;\">To get the best of both worlds, we can use a third technique that combines the two approaches: <b><i>subword tokenization<\/i><\/b>.<\/p><br>\n\n<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">SUBWORD-BASED TOKENIZERS<\/b>\n\n- <a href=\"https:\/\/www.youtube.com\/watch?v=zHvTiHr506c\" style=\"font-family: Georgia; color: darkred; font-weight: bold;\">VIDEO LINK - SUBWORD-BASED TOKENIZERS - HUGGING FACE CHANNEL<\/a><br>\n\n<p style=\"font-family: Georgia;\">Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.<\/p>\n\n<p style=\"font-family: Georgia;\">For instance, \u201cannoyingly\u201d might be considered a rare word and could be decomposed into \u201cannoying\u201d and \u201cly\u201d. These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of \u201cannoyingly\u201d is kept by the composite meaning of \u201cannoying\u201d and \u201cly\u201d.<\/p>\n\n<p style=\"font-family: Georgia;\">Here is an example showing how a subword tokenization algorithm would tokenize the sequence \u201cLet\u2019s do tokenization!\u201c:<\/p>\n\n<br><center><img src=\"https:\/\/huggingface.co\/course\/static\/chapter2\/bpe_subword.png\" width=95%><\/center><br>\n\n<p style=\"font-family: Georgia;\">These subwords end up providing a lot of semantic meaning: for instance, in the example above \u201ctokenization\u201d was split into \u201ctoken\u201d and \u201cization\u201d, two tokens that have a semantic meaning while being space-efficient (only two tokens are needed to represent a long word). This allows us to have relatively good coverage with small vocabularies, and close to no unknown tokens.<\/p>\n\n<p style=\"font-family: Georgia;\">This approach is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.<\/p><br>\n\n<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">OTHER TOKENIZER TYPES<\/b>\n\n<p style=\"font-family: Georgia;\">Unsurprisingly, there are many more techniques out there. To name a few:<\/p>\n\n<ul style=\"font-family: Georgia;\">\n    <li><b>Byte-level BPE<\/b>, as used in <b><a href=\"https:\/\/d4mucfpksywv.cloudfront.net\/better-language-models\/language_models_are_unsupervised_multitask_learners.pdf\">GPT-2<\/a><\/b><\/li>\n    <li><b>WordPiece<\/b>, as used in <b><a href=\"https:\/\/arxiv.org\/abs\/1810.04805\">BERT<\/a><\/b><\/li>\n    <li><b>SentencePiece<\/b> or <b>Unigram<\/b>, as used in several multilingual models<\/li>\n<\/ul>\n\n<p style=\"font-family: Georgia;\">You should now have sufficient knowledge of how tokenizers work to get started with the API.<\/p>","91bd1f6f":"<br><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","239c6e45":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","2651188d":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b><b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\"> &nbsp;&&nbsp; <\/b><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","0fb6d99a":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase;\">PIPELINE BUILDING BLOCKS<\/b>\n\n\n<p style=\"font-family: Georgia;\">As we saw in <a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-1-tf-torch\"><b>Chapter 1<\/b><\/a>, this pipeline groups together three steps:<\/p>\n<ul style=\"font-family: Georgia;\">\n    <li>\n        Preprocessing\n    <\/li>\n    <li>\n        Passing the Inputs Through the Model\n    <\/li>\n    <li>\n        Postprocessing\n    <\/li>\n<\/ul>\n\n<center><img src=\"https:\/\/huggingface.co\/course\/static\/chapter2\/full_nlp_pipeline.png\" width=80%><\/center>\n\n<br><p style=\"font-family: Georgia;\">Let\u2019s quickly go over each of these steps:<\/p>","4e57ae42":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","6cae78e7":"<p style=\"font-family: Georgia;\"><b>Oh no! Why did this fail?<\/b> We followed the steps from the pipeline in <a href=\"#2_4\"><b>Section 2.4<\/b><\/a><\/p>\n\n<p style=\"font-family: Georgia;\">The problem is that we sent a single sequence to the model, whereas \ud83e\udd17 Transformers models expect multiple sentences by default. Here we tried to do everything the tokenizer did behind the scenes when we applied it to a sequence, but if you look closely, you\u2019ll see that it didn\u2019t just convert the list of input IDs into a tensor, it added a dimension on top of it.<\/p>\n    \n<p style=\"font-family: Georgia;\">Let\u2019s try again and add a new dimension. We will also print the new input IDs as well as the resulting logits for each modelling approach.<\/p>","b7d5f05d":"<br><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","df2eeff4":"<!-- <br><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b><b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\"> & <\/b><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\"> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b> -->\n\n<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","d2dd7b2d":"<br><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","8d00b63b":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b><b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\"> &nbsp;&&nbsp; <\/b><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","f5fd87e7":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">Using the tensors as inputs to the model<\/b>\n\n<p style=\"font-family: Georgia;\">Making use of the tensors with the model is extremely simple \u2014 we just call the model with the inputs.<\/p> \n\n<p style=\"font-family: Georgia;\">While the model accepts a lot of different arguments, only the <b><i>input IDs<\/i><\/b> are necessary. We\u2019ll explain what the other arguments do and when they are required later, but first we need to take a closer look at the tokenizers that build the inputs that a Transformer model can understand.<\/p>","489dff8f":"<br><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","a2ca554d":"<a id=\"2_5\"><\/a>\n\n<br><h3 style=\"font-family: Georgia; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">2.5 HANDLING MULTIPLE SEQUENCES&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#chapter_2\">&#10514;<\/a><\/h3>\n\n---\n\n- <a href=\"https:\/\/www.youtube.com\/watch?v=ROxrFOEbsQE\" style=\"font-family: Georgia; color: #ff6f00; font-weight: bold;\">[TENSORFLOW] &nbsp;&nbsp; VIDEO LINK - INSTANTIATE A TRANSFORMER MODEL - HUGGING FACE CHANNEL<\/a>\n- <a href=\"https:\/\/www.youtube.com\/watch?v=M6adb1j2jPI\" style=\"font-family: Georgia; color: #EE4C2C; font-weight: bold;\">[PYTORCH] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VIDEO LINK - INSTANTIATE A TRANSFORMER MODEL - HUGGING FACE CHANNEL<\/a><br>\n\n<p style=\"font-family: Georgia;\">In the previous section, we explored the simplest of use cases: doing inference on a single sequence of a small length. However, some questions emerge already:<\/p>\n\n<ul style=\"font-family: Georgia;\">\n    <li>How do we handle multiple sequences?<\/li>\n    <li>How do we handle multiple sequences of <b><i>different lengths?<\/i><\/b><\/li>\n    <li>Are vocabulary indices the only inputs that allow a model to work well?<\/li>\n    <li>Is there such a thing as too long a sequence?<\/li>\n<\/ul>\n\n<p style=\"font-family: Georgia;\">Let\u2019s see what kinds of problems these questions pose, and how we can solve them using the \ud83e\udd17 Transformers API.<\/p>\n\n<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">MODELS EXPECTS A BATCH OF INPUTS<\/b>\n\n<p style=\"font-family: Georgia;\">In the previous exercise you saw how sequences get translated into lists of numbers. Let\u2019s convert this list of numbers to a tensor and send it to the model:<\/p>\n\n<center><div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px;\">\n    <br><b>\u26a0\ufe0f&nbsp;&nbsp;I think the authors expect the <span style=\"color: #ff6f00;\">TensorFlow<\/span> version to fail (as the <span style=\"color: #EE4C2C;\">PyTorch<\/span> version does). However, it appears that the library has been updated to allow for a single example to be passed without the additional batch dimension.<\/b><br><br>\n<\/div><\/center>","90483d37":"<br><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","1e958eea":"<br><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","c7fdb4f4":"<br><b style=\"color: navy; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">NUMPY<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>\n\nWe won't normally show the <b style=\"color: navy;\">NumPy<\/b> version. However, I wanted to highlight that you can pass this option. You can also occasionally pass other options for other respective libraries (<b style=\"color: darkgreen;\">Trax<\/b>, <b style=\"color: darkred;\">Flax<\/b>, etc.)","28ef7fd3":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">GOING THROUGH THE MODEL<\/b>\n\n<p style=\"font-family: Georgia;\">We can download our pretrained model the same way we did with our tokenizer. \ud83e\udd17 Transformers provides an <b>[<b style=\"color: #ff6f00;\">TFAutoModel<\/b>|<b style=\"color: #EE4C2C;\">AutoModel<\/b>]<\/b> class which also has a <b><code>from_pretrained<\/code><\/b> method:<\/p>\n\n<p style=\"font-family: Georgia;\">In *[the code below, we download the same checkpoint] we used in our pipeline before (it should actually *[be]  cached already) and *[instantiate] a model with it.<\/p>\n\n<p style=\"font-family: Georgia;\">This architecture contains only the base Transformer module: given some inputs, it will output what we call <b><i>hidden states<\/i><\/b>, also known as <b><i>features<\/i><\/b>. For each model input, we\u2019ll retrieve a high-dimensional vector representing the <b>contextual understanding of that input by the Transformer model<\/b>.<\/p>\n\n<p style=\"font-family: Georgia;\">While these hidden states can be useful on their own, they\u2019re usually inputs to another part of the model, known as the <b><i>head<\/i><\/b>. In <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-1-tf-torch\">Chapter 1<\/a><\/b>, the different tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it.<\/p>","421f4ab2":"<a id=\"2_8\"><\/a>\n\n<br><h3 style=\"font-family: Georgia; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">2.8 CHAPTER QUIZ RECAP&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#chapter_2\">&#10514;<\/a><\/h3>\n\n---\n\n<center><div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px;\">\n    <br><b>\u26a0\ufe0f&nbsp;&nbsp;I am taking the end-of-chapter quiz that is included in the course and simply retrieving the relevant information.<\/b><br><br>\n<\/div><\/center><br>\n\n    \n<ol style=\"font-family: Georgia;\">\n    <li style=\"font-family: Georgia; font-weight: bold; text-decoration: none; text-transform: uppercase; font-size: 15px;\">The order of the language modeling pipeline<\/li>\n    <ul style=\"font-family: Georgia;\">\n        <li>The tokenizer handles text and returns IDs<\/li>\n        <li>Then the model handles these IDs and outputs a prediction<\/li>\n        <li>Finally, the tokenizer can then be used again to convert these predictions back to some text<\/li>\n    <\/ul>\n    <center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 0.8em; font-family: Georgia; font-size: 12px;\">\n    <br><b>\ud83d\udcda\ud83d\udcda\ud83d\udcda&nbsp;&nbsp;&nbsp;&nbsp;The tokenizer can be used for both tokenizing and de-tokenizing&nbsp;&nbsp;&nbsp;&nbsp;\ud83d\udcda\ud83d\udcda\ud83d\udcda<\/b><br><br>\n<\/div><\/center><br>\n    <li style=\"font-family: Georgia; font-weight: bold; text-decoration: none; text-transform: uppercase; font-size: 15px;\">The dimensions of the tensor outputed by the base Transformer model\n<\/li>\n    <ul style=\"font-family: Georgia;\">\n        <li>The sequence length<\/li>\n        <li>The batch size<\/li>\n        <li>The hidden size<\/li>\n    <\/ul><br><br>\n    <li style=\"font-family: Georgia; font-weight: bold; text-decoration: none; text-transform: uppercase; font-size: 15px;\">an example of subword tokenization\n<\/li>\n    <ul style=\"font-family: Georgia;\">\n        <li><b><a href=\"https:\/\/arxiv.org\/abs\/1508.07909\">BPE<\/a><\/b><\/li>\n        <li><b><a href=\"https:\/\/arxiv.org\/abs\/2106.02289\">Unigram<\/a><\/b><\/li>\n        <li><b><a href=\"https:\/\/arxiv.org\/pdf\/1609.08144.pdf\">WordPiece<\/a><\/b><\/li>\n    <\/ul><br><br>\n    <li style=\"font-family: Georgia; font-weight: bold; text-decoration: none; text-transform: uppercase; font-size: 15px;\">The definition of a 'model head'\n<\/li>\n    <ul style=\"font-family: Georgia;\">\n        <li>An additional component, usually made up of one or a few layers, to convert the transformer predictions to a task-specific output<\/li>\n    <\/ul>\n    <center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 0.8em; font-family: Georgia; font-size: 12px;\">\n    <br><b>\ud83d\udcda\ud83d\udcda\ud83d\udcda&nbsp;&nbsp;&nbsp;&nbsp;Adaptation heads, also known simply as heads, come up in different forms: language modeling heads, question answering heads, sequence classification heads...&nbsp;&nbsp;&nbsp;&nbsp;\ud83d\udcda\ud83d\udcda\ud83d\udcda<\/b><br><br>\n<\/div><\/center><br>\n    <li style=\"font-family: Georgia; font-weight: bold; text-decoration: none; text-transform: uppercase; font-size: 15px;\">The definition of a <b>[<b style=\"color: #ff6f00;\">TFAutoModel<\/b>|<b style=\"color: #EE4C2C;\">AutoModel<\/b>]<\/b>\n<\/li>\n    <ul style=\"font-family: Georgia;\">\n        <li>An object that returns the correct architecture based on the checkpoint<\/li>\n    <\/ul>\n    <center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 0.8em; font-family: Georgia; font-size: 12px;\">\n    <br><b>\ud83d\udcda\ud83d\udcda\ud83d\udcda&nbsp;&nbsp;&nbsp;&nbsp;The <b>[<b style=\"color: #ff6f00;\">TFAutoModel<\/b>|<b style=\"color: #EE4C2C;\">AutoModel<\/b>]<\/b> only needs to know the checkpoint from which to initialize to return the correct architecture.&nbsp;&nbsp;&nbsp;&nbsp;\ud83d\udcda\ud83d\udcda\ud83d\udcda<\/b><br><br>\n<\/div><\/center><br>\n    <li style=\"font-family: Georgia; font-weight: bold; text-decoration: none; text-transform: uppercase; font-size: 15px;\">The different techniques to be aware of when batching sequences of different lengths together\n<\/li>\n    <ul style=\"font-family: Georgia;\">\n        <li>Truncating<\/li>\n        <li>Padding<\/li>\n        <li>Attention Masking<\/li>\n    <\/ul><br>\n    <li style=\"font-family: Georgia; font-weight: bold; text-decoration: none; text-transform: uppercase; font-size: 15px;\">The point of applying a SoftMax function to the logits output by a sequence classification model\n<\/li>\n    <ul style=\"font-family: Georgia;\">\n        <li>It applies a lower and upper bound so that they're understandable.<\/li>\n        <li>The total sum of the output is then 1, resulting in a possible probabilistic interpretation.<\/li>\n    <\/ul><br>\n    <li style=\"font-family: Georgia; font-weight: bold; text-decoration: none; text-transform: uppercase; font-size: 15px;\">The method that most of the tokenizer API is centered around\n<\/li>\n    <ul style=\"font-family: Georgia;\">\n        <li>Calling the tokenizer object directly<\/li>\n    <\/ul>\n    <center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 0.8em; font-family: Georgia; font-size: 12px;\">\n        <br><b>\ud83d\udcda\ud83d\udcda\ud83d\udcda&nbsp;&nbsp;&nbsp;&nbsp;The <code style=\"color: darkgreen;\">__call__<\/code> method of the tokenizer is a powerful method which can handle pretty much anything. It is also the method used to retrieve predictions from a model.&nbsp;&nbsp;&nbsp;&nbsp;\ud83d\udcda\ud83d\udcda\ud83d\udcda<\/b><br><br>\n<\/div><\/center><br>\n\n<\/ol>\n\n","3be5d699":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">Model heads: Making sense out of numbers<\/b>\n\n<p style=\"font-family: Georgia;\">The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers.<\/p>\n\n<br><center><img src=\"https:\/\/huggingface.co\/course\/static\/chapter2\/transformer_and_head.png\" width=80%><\/center><br>\n\n<p style=\"font-family: Georgia;\">The output of the Transformer model is sent directly to the model head to be processed.<\/p>\n\n<p style=\"font-family: Georgia;\">In this diagram, the model is represented by its embeddings layer and the subsequent layers. The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token. The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences.<\/p>\n\n<p style=\"font-family: Georgia;\">There are many different architectures available in \ud83e\udd17 Transformers, with each one designed around tackling a specific task. Here is a non-exhaustive list:<\/p>\n\n<ul style=\"font-family: Georgia;\">\n    <li>\n        <b><code>Model<\/code><\/b> (retrieve the hidden states)\n    <\/li>\n    <li>\n        <b><code>ForCausalLM<\/code><\/b>\n    <\/li>\n    <li>\n        <b><code>ForMaskedLM<\/code><\/b>\n    <\/li>\n    <li>\n        <b><code>ForMultipleChoice<\/code><\/b>\n    <\/li>\n    <li>\n        <b><code>ForQuestionAnswering<\/code><\/b>\n    <\/li>\n    <li>\n        <b><code>ForSequenceClassification<\/code><\/b>\n    <\/li>\n    <li>\n        <b><code>ForTokenClassification<\/code><\/b>\n    <\/li>\n    <li>\n        and others \ud83e\udd17\n    <\/li>\n<\/ul>\n\n<p style=\"font-family: Georgia;\">For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won\u2019t actually use the <b>[<b style=\"color: #ff6f00;\">TFAutoModel<\/b>|<b style=\"color: #EE4C2C;\">AutoModel<\/b>]<\/b> class, but <b>[<b style=\"color: #ff6f00;\">TFAutoModelForSequenceClassification<\/b>|<b style=\"color: #EE4C2C;\">AutoModelForSequenceClassification<\/b>]<\/b><\/p>\n\n<p style=\"font-family: Georgia;\">Now if we look at the shape of our inputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label). Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2.<\/p>\n\n","831f340d":"<br><b style=\"color: gray; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">LIBRARY AGNOSTIC<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","7e261083":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","940106e2":"<br><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","d47da35a":"<a id=\"2_6\"><\/a>\n\n<br><h3 style=\"font-family: Georgia; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">2.6 PUTTING IT ALL TOGETHER&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#chapter_2\">&#10514;<\/a><\/h3>\n\n---\n\n<p style=\"font-family: Georgia;\">In the last few sections, we\u2019ve been trying our best to do most of the work by hand. We\u2019ve explored how tokenizers work and looked at tokenization, conversion to input IDs, padding, truncation, and attention masks.<\/p>\n\n<p style=\"font-family: Georgia;\">However, as we saw in <b><a href=\"#2_2\">Section 2.2<\/a><\/b>, the \ud83e\udd17 Transformers API can handle all of this for us with a high-level function that we\u2019ll dive into here. When you call your <b><code>tokenizer<\/code><\/b> directly on the sentence, you get back inputs that are ready to pass through your model<\/p>\n\n<p style=\"font-family: Georgia;\">The <b><code>tokenizer<\/code><\/b> will return an inputs variable containing everything that\u2019s necessary for a model to operate well. For DistilBERT, that includes the input IDs as well as the attention mask. Other models that accept additional inputs will also have those outputs returned by the tokenizer object.<\/p><br>\n\n<p style=\"font-family: Georgia;\">As we\u2019ll see in some examples below, this method is very powerful.<\/p>\n\n<ul style=\"font-family: Georgia;\">\n    <li>First, it can tokenize a single sequence<\/li>\n    <li>It also handles multiple sequences at a time, with no change in the API<\/li>\n    <li>It can pad according to several objectives<\/li>\n    <li>It can also truncate sequences:<\/li>\n<\/ul>\n\n<p style=\"font-family: Georgia;\">The <b><code>tokenizer<\/code><\/b> object can handle the conversion to specific framework tensors, which can then be directly sent to the model. For example, in the following code sample we are prompting the tokenizer to return tensors from the different frameworks \u2014 \"pt\" returns <b style=\"color: #EE4C2C;\">PyTorch<\/b> tensors, \"tf\" returns <b style=\"color: #ff6f00;\">TensorFlow<\/b> tensors, and \"np\" returns <b style=\"color: navy;\">NumPy<\/b> arrays:<\/p>","44b79817":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">Different Loading Methods<\/b>\n\n<p style=\"font-family: Georgia;\">Creating a model from the default configuration initializes it with random values.<\/p>\n\n<p style=\"font-family: Georgia;\">The model can be used in this state, but it will output gibberish; it needs to be trained first. We could train the model from scratch on the task at hand, but as you saw in <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-1-tf-torch\">Chapter 1<\/a><\/b>, this would require a long time and a lot of data, and it would have a non-negligible environmental impact. To avoid unnecessary and duplicated effort, it\u2019s imperative to be able to share and reuse models that have already been trained.<\/p>\n\n<p style=\"font-family: Georgia;\">Loading a Transformer model that is already trained is simple \u2014 we can do this using the <b><code>from_pretrained()<\/code><\/b> method:<\/p>\n\n<p style=\"font-family: Georgia;\">As you saw earlier, we could replace <b>[<b style=\"color: #ff6f00;\">TFBertModel<\/b>|<b style=\"color: #EE4C2C;\">BertModel<\/b>]<\/b> with the equivalent <b>[<b style=\"color: #ff6f00;\">TFAutoModel<\/b>|<b style=\"color: #EE4C2C;\">AutoModel<\/b>]<\/b> class. We\u2019ll do this from now on as this produces checkpoint-agnostic code; if your code works for one checkpoint, it should work seamlessly with another. This applies even if the architecture is different, as long as the checkpoint was trained for a similar task (for example, a sentiment analysis task).<\/p>\n\n<p style=\"font-family: Georgia;\">In the code sample *below we don\u2019t use <b><code>BertConfig<\/code><\/b>, and instead loaded a pretrained model via the <b><code>bert-base-cased<\/code><\/b> identifier. This is a model checkpoint that was trained by the authors of <b><a href=\"https:\/\/arxiv.org\/abs\/1810.04805\">BERT<\/a><\/b> themselves; you can find more details about it in its <b><a href=\"https:\/\/huggingface.co\/bert-base-cased\">model card<\/a><\/b>.<\/p>\n\n<p style=\"font-family: Georgia;\">After the model is initialized with the weights from the checkpoint, it can be used directly for inference on the tasks it was trained on or it can be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results.<\/p>\n\n<p style=\"font-family: Georgia;\">The weights have been downloaded and cached (so future calls to the <b><code>from_pretrained()<\/code><\/b> method won\u2019t re-download them) in the cache folder, which defaults to <code><i>~\/.cache\/huggingface\/transformers<\/i><\/code>. You can customize your cache folder by setting the <b><code>HF_HOME<\/code><\/b> environment variable.<\/p>\n\n<p style=\"font-family: Georgia;\">The identifier used to load the model can be the identifier of any model on the <b><a href=\"https:\/\/huggingface.co\/models\">Model Hub<\/a><\/b>, as long as it is compatible with the <b><a href=\"https:\/\/arxiv.org\/abs\/1810.04805\">BERT<\/a><\/b> architecture. The entire list of available <b><a href=\"https:\/\/arxiv.org\/abs\/1810.04805\">BERT<\/a><\/b> checkpoints can be found <b><a href=\"https:\/\/huggingface.co\/models?filter=bert\">here<\/a><\/b>.<\/p>","a91bb6c1":"<br><b style=\"color: gray; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">LIBRARY AGNOSTIC<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","777bb350":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">PREPROCESSING WITH A TOKENIZER<\/b>\n\n<p style=\"font-family: Georgia;\">Like other neural networks, Transformer models can\u2019t process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a tokenizer, which will be responsible for:<\/p>\n\n<ul style=\"font-family: Georgia;\">\n    <li>\n        Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n    <\/li>\n    <li>\n        Mapping each token to an integer\n    <\/li>\n    <li>\n        Adding additional inputs that may be useful to the model\n    <\/li>\n<\/ul>\n\n\n\n<p style=\"font-family: Georgia;\">All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the <b><a href=\"https:\/\/huggingface.co\/models\">Model Hub<\/a><\/b>. To do this, we use the <b><code>AutoTokenizer<\/code><\/b> class and its <b><code>from_pretrained()<\/code><\/b> method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model\u2019s tokenizer and cache it (so it\u2019s only downloaded the first time you run the code below).<\/p>\n\n<p style=\"font-family: Georgia;\">Since the default checkpoint of the sentiment-analysis pipeline is <b><code>distilbert-base-uncased-finetuned-sst-2-english<\/code><\/b> (you can see its model card <b><a href=\"https:\/\/huggingface.co\/distilbert-base-uncased-finetuned-sst-2-english\">here<\/a><\/b>), we will pass that checkpoint. Once we have the tokenizer, we can directly pass our sentences to it and we\u2019ll get back a dictionary that\u2019s ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors.<\/p>\n\n<p style=\"font-family: Georgia;\">You can use \ud83e\udd17 Transformers without having to worry about which ML framework is used as a backend; it might be <b style=\"color: #EE4C2C;\">PyTorch<\/b> or <b style=\"color: #ff6f00;\">TensorFlow<\/b>, <b style=\"color: darkgreen;\">Trax<\/b>, <b style=\"color: darkred;\">Flax<\/b>, etc. for some models. However, Transformer models only accept <b>tensors<\/b> as input. If this is your first time hearing about <b>tensors<\/b>, you can think of them as <b>NumPy arrays<\/b> instead. A <b>NumPy array<\/b> can be a scalar (0D), a vector (1D), a matrix (2D), or have more dimensions. It\u2019s effectively a <b>tensor<\/b>; other ML frameworks\u2019 <b>tensors<\/b> behave similarly, and are usually as simple to instantiate as <b>NumPy arrays<\/b>.<\/p>\n\n<p style=\"font-family: Georgia;\">To specify the type of tensors we want to get back (<b style=\"color: #EE4C2C;\">PyTorch<\/b>, <b style=\"color: #ff6f00;\">TensorFlow<\/b>, or plain <b style=\"color: navy;\">NumPy<\/b>), we use the return_tensors argument. Don\u2019t worry about padding and truncation just yet; we\u2019ll explain those later. The main things to remember here are that you can pass one sentence or a list of sentences, as well as specifying the type of tensors you want to get back (if no type is passed, you will get a list of lists as a result).<\/p>\n\n<p style=\"font-family: Georgia;\">The output itself is a dictionary containing two keys, <b><code>input_ids<\/code><\/b> and <b><code>attention_mask<\/code><\/b>. <b><code>input_ids<\/code><\/b> contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence. We\u2019ll explain what the <b><code>attention_mask<\/code><\/b> is later in this chapter.<\/p>","36213ded":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","92164791":"<a id=\"2_0\"><\/a>\n\n<br><h3 style=\"font-family: Georgia; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">2.0 INTRODUCTION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#chapter_2\">&#10514;<\/a><\/h3>\n\n---\n\n<p style=\"font-family: Georgia;\">As you saw in <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-1-tf-torch\">Chapter 1<\/a><\/b>, Transformer models are usually very large. With millions to tens of billions of parameters, training and deploying these models is a complicated undertaking. Furthermore, with new models being released on a near-daily basis and each having its own implementation, trying them all out is no easy task.<\/p>\n\n<p style=\"font-family: Georgia;\">The \ud83e\udd17 Transformers library was created to solve this problem. Its goal is to provide a single API through which any Transformer model can be loaded, trained, and saved. The library\u2019s main features are:<\/p>\n    \n<ul style=\"font-family: Georgia;\">\n    <li>\n        <b>Ease of Use:<\/b> Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code.\n    <\/li>\n    <li>\n        <b>Flexibility:<\/b> At their core, all models are simple PyTorch nn.Module or TensorFlow tf.keras.Model classes and can be handled like any other models in their respective machine learning (ML) frameworks.\n    <\/li>\n    <li>\n        <b>Simplicity:<\/b> Hardly any abstractions are made across the library. The \u201cAll in one file\u201d is a core concept: a model\u2019s forward pass is entirely defined in a single file, so that the code itself is understandable and hackable.\n    <\/li>\n<\/ul>\n\n<p style=\"font-family: Georgia;\">This last feature makes \ud83e\udd17 Transformers quite different from other ML libraries. The models are not built on modules that are shared across files; instead, each model has its own layers. In addition to making the models more approachable and understandable, this allows you to easily experiment on one model without affecting others.<\/p>\n\n<p style=\"font-family: Georgia;\">This chapter will begin with an end-to-end example where we use a model and a tokenizer together to replicate the <b><code>pipeline()<\/code><\/b> function introduced in <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-1-tf-torch\">Chapter 1<\/a><\/b>. Next, we\u2019ll discuss the model API: we\u2019ll dive into the model and configuration classes, and show you how to load a model and how it processes numerical inputs to output predictions.<\/p>\n\n<p style=\"font-family: Georgia;\">Then we\u2019ll look at the tokenizer API, which is the other main component of the pipeline() function. Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed. Finally, we\u2019ll show you how to handle sending multiple sentences through a model in a prepared batch, then wrap it all up with a closer look at the high-level <b><code>tokenizer()<\/code><\/b> function.<\/p>\n\n<center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px; \">\n    <br><b>\u26a0\ufe0f&nbsp;&nbsp;In order to benefit from all features available with the Model Hub and \ud83e\udd17 Transformers, we recommend <a href=\"https:\/\/huggingface.co\/join\">creating an account<\/a>.<\/b><br><br>\n<\/div><\/center>\n \n\n","19386c0f":"<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">DECODING<\/b>\n\n<p style=\"font-family: Georgia;\"><b><i>Decoding<\/i><\/b> is going the other way around: given vocabulary indices, we want to retrieve the representative string. This can be done with the <b><code>decode()<\/code><\/b> method of our tokenizer object.<\/p>\n\n<p style=\"font-family: Georgia;\">Note that the decode method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence. This behavior will be extremely useful when we use models that predict new text (either text generated from a prompt, or for sequence-to-sequence problems like translation or summarization).<\/p>\n\n<p style=\"font-family: Georgia;\">By now you should understand the atomic operations a tokenizer can handle: tokenization, conversion to IDs, and converting IDs back to a string. However, we\u2019ve just scraped the tip of the iceberg. In the following <b><a href=\"https:\/\/www.kaggle.com\/dschettler8845\/transformers-course-chapter-3-tf-torch\">section (Chapter 3)<\/a><\/b>, we\u2019ll take our approach to its limits and take a look at how to overcome them.<\/p>","bdea7fd3":"<br><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","41a94582":"<br><b style=\"color: #ff6f00; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">TENSORFLOW<\/b><b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\"> &nbsp;&&nbsp; <\/b><b style=\"color: #EE4C2C; font-family: Verdana; font-size: 18px; letter-spacing: 0.4em;\">PYTORCH<\/b> <b style=\"color: black; font-family: Verdana; font-size: 18px; letter-spacing: 0.2em;\">&nbsp;&nbsp;&nbsp;CODE<\/b>","a75e5bb9":"<a id=\"chapter_2\"><\/a>\n\n<p id=\"toc\"><\/p>\n\n<h1 style=\"font-family: Georgia; font-size: 30px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: black; background-color: #ffffff;\">TABLE OF CONTENTS<\/h1>\n\n---\n\n<h2 style=\"text-indent: 10vw; font-family: Georgia; font-size: 24px; font-style: normal; font-weight: bolder; text-decoration: none; text-transform: none; letter-spacing: 2px; color:  navy; background-color: #ffffff;\"><a href=\"#toc\">CHAPTER &nbsp;#2&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp;USING \ud83e\udd17 TRANSFORMERS<\/a><\/h2>\n\n<h3 style=\"text-indent: 10vw; font-family: Georgia; font-size: 18px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp; <a href=\"#2_0\">2.0 INTRODUCTION<\/a><\/h3>\n\n<h3 style=\"text-indent: 10vw; font-family: Georgia; font-size: 18px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp; <a href=\"#2_1\">2.1 BEHIND THE PIPELINE<\/a><\/h3>\n\n<h3 style=\"text-indent: 10vw; font-family: Georgia; font-size: 18px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp; <a href=\"#2_2\">2.2 MODELS<\/a><\/h3>\n\n<h3 style=\"text-indent: 10vw; font-family: Georgia; font-size: 18px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp; <a href=\"#2_3\">2.3 TOKENIZER BACKGROUND<\/a><\/h3>\n\n<h3 style=\"text-indent: 10vw; font-family: Georgia; font-size: 18px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp; <a href=\"#2_4\">2.4 TOKENIZER BASICS<\/a><\/h3>\n\n<h3 style=\"text-indent: 10vw; font-family: Georgia; font-size: 18px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp; <a href=\"#2_5\">2.5 HANDLING MULTIPLE SEQUENCES<\/a><\/h3>\n\n<h3 style=\"text-indent: 10vw; font-family: Georgia; font-size: 18px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp; <a href=\"#2_6\">2.6 PUTTING IT ALL TOGETHER<\/a><\/h3>\n\n<h3 style=\"text-indent: 10vw; font-family: Georgia; font-size: 18px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp; <a href=\"#2_7\">2.7 BASIC USAGE COMPLETED<\/a><\/h3>\n\n<h3 style=\"text-indent: 10vw; font-family: Georgia; font-size: 18px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp; <a href=\"#2_8\">2.8 CHAPTER QUIZ RECAP<\/a><\/h3>","eb1209ab":"<b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">Saving Methods<\/b>\n\n<p style=\"font-family: Georgia;\">Saving a model is as easy as loading one \u2014 we use the <b><code>save_pretrained()<\/code><\/b> method, which is analogous to the <b><code>from_pretrained()<\/code><\/b> method. This method will save two files to your disk, <code><b>config.json<\/b><\/code> and <b>[<b style=\"color: #ff6f00;\">tf_model.h5<\/b>|<b style=\"color: #EE4C2C;\">pytorch_model.bin<\/b>]<\/b>. These two files go hand in hand; the configuration is necessary to know your model\u2019s architecture, while the model weights are your model\u2019s parameters.<\/p>\n\n<ul style=\"font-family: Georgia;\">\n    <li><b><code>config.json<\/code><\/b><ul>\n        <li>This file contains the attributes necessary to build the model architecture.<\/li> \n        <li>This file also contains some metadata, such as where the checkpoint originated and what \ud83e\udd17 Transformers version you were using when you last saved the checkpoint.<\/li><\/ul>\n    <li><b>[<b style=\"color: #ff6f00;\">tf_model.h5<\/b>|<b style=\"color: #EE4C2C;\">pytorch_model.bin<\/b>]<\/b><ul>\n        <li>This file is the <b><i>state dictionary<\/i><\/b>; it contains all your model\u2019s weights. <\/li>\n<\/ul>\n\n","d9c0c060":"<p style=\"font-family: Georgia;\"><b>Batching is the act of sending multiple sentences through the model, all at once.<\/b> If you only have one sentence, you can just build a batch with a single sequence:<\/p>\n\n<pre style=\"font-weight: bold; white-space: pre-wrap; background-color: #eee; border: 1px dashed #999; display: block; padding: 16px; line-height:160%\">\n    batched_ids = [ids, ids]\n<\/pre>\n\n<p style=\"font-family: Georgia;\">This is a batch of two identical sequences!<\/p>\n\n<center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 1.5em; font-family: Georgia; font-size: 12px; \">\n    <br><b>\u270f\ufe0f&nbsp;&nbsp;<b style=\"color: black;\">TRY IT OUT!<\/b>&nbsp;&nbsp;&nbsp;&nbsp;Convert this <code style=\"color:darkgreen;\">batched_ids<\/code> list into a tensor and pass it through your model. Check that you obtain the same logits as before (but twice)!<\/b><br><br>\n<\/div><\/center>\n\n<p style=\"font-family: Georgia;\">Batching allows the model to work when you feed it multiple sentences. Using multiple sequences is just as simple as building a batch with a single sequence. There\u2019s a second issue, though. When you\u2019re trying to batch together two (or more) sentences, they might be of different lengths. If you\u2019ve ever worked with tensors before, you know that they need to be of rectangular shape, so you won\u2019t be able to convert the list of input IDs into a tensor directly. To work around this problem, we usually <b><i>pad<\/i><\/b> the inputs.<\/p>\n\n","dc1813a0":"<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">SPECIAL TOKENS<\/b>\n\n<p style=\"font-family: Georgia;\">If we take a look at the input IDs returned by the tokenizer, we will see they are a tiny bit different from what we had earlier. One token ID was added at the beginning, and one at the end. The tokenizer added the special word <b><code>[CLS]<\/code><\/b> at the beginning and the special word <b><code>[SEP]<\/code><\/b> at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. Note that some models don\u2019t add special words, or add different ones; models may also add these special words only at the beginning, or only at the end. In any case, the tokenizer knows which ones are expected and will deal with this for you.<\/p>\n","c27d76d1":"<a id=\"2_7\"><\/a>\n\n<br><h3 style=\"font-family: Georgia; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">2.7 BASIC USAGE COMPLETED!&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#chapter_2\">&#10514;<\/a><\/h3>\n\n---\n\n<p style=\"font-family: Georgia;\">Great job following the course up to here! To recap, in this chapter you:<\/p>\n\n<ul style=\"font-family: Georgia;\">\n    <li>Learned the basic building blocks of a Transformer model.<\/li>\n    <li>Learned what makes up a tokenization pipeline.<\/li>\n    <li>Saw how to use a Transformer model in practice.<\/li>\n    <li>Learned how to leverage a tokenizer to convert text to tensors that are understandable by the model.<\/li>\n    <li>Set up a tokenizer and a model together to get from text to predictions.<\/li>\n    <li>Learned the limitations of input IDs, and learned about attention masks.<\/li>\n    <li>Played around with versatile and configurable tokenizer methods.<\/li>\n<\/ul>\n\n<p style=\"font-family: Georgia;\">From now on, you should be able to freely navigate the \ud83e\udd17 Transformers docs: the vocabulary will sound familiar, and you\u2019ve already seen the methods that you\u2019ll use the majority of the time.<\/p>","5599fbf4":"<br><b style=\"font-family: Georgia; text-decoration: underline; text-transform: uppercase; font-size: 15px;\">Wrapping up: From tokenizer to model<\/b>\n\n<p style=\"font-family: Georgia;\">Now that we\u2019ve seen all the individual steps the tokenizer object uses when applied on texts, let\u2019s see one final time how it can handle multiple sequences (padding!), very long sequences (truncation!), and multiple types of tensors with its main API<\/p>\n"}}