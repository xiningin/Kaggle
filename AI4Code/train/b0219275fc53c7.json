{"cell_type":{"eed3f780":"code","a47975d7":"code","f6ed1ecb":"code","8f4a476a":"code","25b3c458":"code","815168d4":"code","2ce803d9":"code","7bfa8a4d":"code","97144b10":"code","c8a5c52a":"code","fec7d379":"code","b6f6f1a7":"code","dfa21cb7":"code","74c9972a":"code","fb004c25":"code","112a0c03":"code","d177a6c3":"code","f8e91ccd":"code","a7f8fbeb":"code","655bfdcf":"code","cc68f3a1":"code","f6b40d20":"code","b046cbe9":"code","413351f7":"code","8c0a631c":"code","04ae2cdf":"code","75e9d478":"code","ec5925ba":"code","afd2f341":"code","88f09d5c":"code","35a6c989":"code","a9df512d":"code","13c18d8b":"code","d39184ad":"code","5076e4c0":"code","e29bc2e6":"code","83f97bc2":"code","71864913":"code","5a0f5515":"code","87dee342":"code","77f59a2d":"code","c44e097b":"code","05ce5e24":"code","6b387712":"code","3fee2d12":"code","1d42f53f":"code","d33af890":"code","19927207":"code","a29aeeb5":"code","1a379b33":"code","b5f3bb00":"code","aebf4497":"code","4c8f5992":"code","bb3810b3":"code","5b8223e9":"code","d1533893":"code","bc1ddcee":"code","123b6594":"code","9bbc75a1":"code","d3aa5246":"code","06dd62d6":"code","a92d0e67":"code","bdbf26a7":"code","7e6572c7":"code","ee635340":"code","70ed0121":"code","69265f08":"code","363790ec":"code","15971d34":"code","f5e6467d":"code","b9ce3bfb":"code","f6beb797":"code","93815a2f":"code","dda6d0a3":"code","1dedcd64":"code","249facf6":"code","d3378dd1":"code","b930acd4":"code","26686e28":"code","da529bd7":"code","8ed449fc":"code","63913e45":"code","597b63b4":"code","1971d927":"code","80e1d455":"code","3b8c1bc6":"code","5f9d0b26":"code","f2ac6681":"code","6cdb9c02":"code","c3c7751f":"code","7e6ca65e":"code","3d575a32":"code","f04e83a6":"code","887cd16b":"code","7af9cb1a":"code","4eaa4eb7":"code","ad8bc062":"code","66f55465":"code","5ff95c0c":"code","a8ed86c6":"code","06de901f":"code","3b548c15":"markdown","5c9ff601":"markdown","38f7cf77":"markdown","d89289a8":"markdown","ee3244f7":"markdown","cda01ab4":"markdown","27e3bc30":"markdown","4bdd8430":"markdown","28caee47":"markdown","0053852d":"markdown"},"source":{"eed3f780":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom scipy import stats\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport seaborn as sns\n\nimport os\n\n# Any results you write to the current directory are saved as output.","a47975d7":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.linear_model import RidgeClassifierCV\nfrom sklearn import preprocessing\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","f6ed1ecb":"train_data=pd.read_csv(r\"..\/input\/bank-train.csv\")\ntest_data=pd.read_csv(r\"..\/input\/bank-test.csv\")","8f4a476a":"train_data.head(1)","25b3c458":"train_data.isnull().sum()","815168d4":"#Check individual attributes \n#id\n#Check if it's a unique id for every single row \ntrain_data[\"id\"].value_counts(ascending=False)[0]\n#Unique for each row, not useful for classification, might remove it later ","2ce803d9":"#Age\n#Age is a numeric data. \ntrain_data[\"age\"].isnull().sum()\/train_data.shape[0]\n#No null data ","7bfa8a4d":"#Check the distribution of age \ntrain_data[\"age\"].hist()\n#right skewed --- whether should we transform it ? \n","97144b10":"#Let us make one variable using the log transformation for age \n#Later we can compare the result of both features and see which one helps to improve classfication accuracy \ntrain_data[\"log_age\"]=np.log(train_data[\"age\"])\ntrain_data[\"log_age\"].hist()\n#Normal distribution ","c8a5c52a":"train_data[\"job\"].value_counts()\n#Many cateogiries -- > maybe merge some of the categories ? \n#Let us check if these categories have some relationships with the response variable.","fec7d379":"train_data.groupby(\"job\").y.value_counts(normalize=True).unstack()[1].plot(kind=\"bar\")\nplt.title(\"Subscription Rate by Job Categories\")\n#There are some differences between the job categories \n#If we want to limit the levels, how can we merge them? ","b6f6f1a7":"train_data[\"marital\"].value_counts(normalize=True)","dfa21cb7":"train_data[train_data[\"marital\"]==\"unknown\"].shape","74c9972a":"train_data.groupby(\"marital\").y.value_counts(normalize=True).unstack()[1].plot(kind=\"bar\")\nplt.title(\"Subscription Rate by Marital Status\")\n#There are some differences between the marital status \n#Unkown marital status have obviously higher subscription rate, \n#We may not change it. ","fb004c25":"train_data[\"education\"].value_counts(normalize=True)","112a0c03":"train_data.groupby(\"education\").y.value_counts(normalize=True).unstack()[1].plot(kind=\"bar\")\nplt.title(\"Subscription Rate by Education\")\n#kind of wanna merge basic 6-yr and 9yr, and 4y and high school and professional course \n#How to justify the decision? ","d177a6c3":"train_data.groupby(\"default\").y.value_counts(normalize=True).unstack()[1].plot(kind=\"bar\")\nplt.title(\"Subscription Rate by Credit Default Status\")\n#This might be a very important feature ","f8e91ccd":"train_data.groupby(\"housing\").y.value_counts(normalize=True).unstack()[1].plot(kind=\"bar\")\nplt.title(\"Subscription Rate by Housing Loan Status\")","a7f8fbeb":"train_data.groupby(\"loan\").y.value_counts(normalize=True).unstack()[1].plot(kind=\"bar\")\nplt.title(\"Subscription Rate by Personal Loan Status\")","655bfdcf":"train_data.groupby(\"contact\").y.value_counts(normalize=True).unstack()[1].plot(kind=\"bar\")\nplt.title(\"Subscription Rate by Contact Method\")","cc68f3a1":"train_data[\"month\"].value_counts(normalize=True)","f6b40d20":"train_data.groupby(\"month\").y.value_counts(normalize=True).unstack()[1].plot(kind=\"bar\")\nplt.title(\"Subscription Rate by Last Contact Month\")\n#Group some months? How to justify it? ","b046cbe9":"train_data[\"day_of_week\"].value_counts(normalize=True)","413351f7":"train_data.groupby(\"day_of_week\").y.value_counts(normalize=True).unstack()[1].plot(kind=\"bar\")\nplt.title(\"Subscription Rate by Day of Week\")\n#Group some months? How to justify it? ","8c0a631c":"train_data.drop(\"duration\",axis=1,inplace=True)","04ae2cdf":"train_data[\"campaign\"].hist()\n#Highly right skewed ?? ","75e9d478":"train_data[\"test_campaign\"]=train_data['campaign'].apply(lambda x: np.floor(x\/3)+1)","ec5925ba":"train_data.groupby(\"test_campaign\").y.value_counts(normalize=True).unstack()[1].plot(kind=\"bar\")\nplt.title(\"Subscription Rate by # of Contacts (Bins=3) \")\n#Group some months? How to justify it? ","afd2f341":"train_data[\"test_campaign\"].hist()","88f09d5c":"#drop the over 10s \ntrain_data=train_data[train_data[\"test_campaign\"]<=10.0]\n#DELETE OUTLIERS AT HERE ","35a6c989":"#Transform the test campaign varaible \ntrain_data[\"test_campaign\"].hist()\n#How to transform it more normal ","a9df512d":"train_data[train_data[\"pdays\"]!=999][\"pdays\"].hist()\n#Do we need to do something for 999? ","13c18d8b":"train_data[train_data[\"pdays\"]!=999][\"pdays\"].max()","d39184ad":"#Transform pdays to a category: \nbins=[0,7,14,21,28,1000]\ntrain_data[\"pdays_category\"]=pd.cut(train_data[\"pdays\"],bins, labels=[\"OneWeek\", \"TwoWeek\", \"ThreeWeek\",\"FourWeek\",\"NoPreviousCampaign\"],right=False)","5076e4c0":"train_data[\"pdays_category\"].value_counts(normalize=True)","e29bc2e6":"train_data.groupby(\"pdays_category\").y.value_counts(normalize=True).unstack()[1].plot(kind=\"bar\")\n","83f97bc2":"train_data[\"previous\"].hist()","71864913":"train_data.groupby(\"previous\").y.value_counts(normalize=True).unstack()[1].plot(kind=\"bar\")\n#The number of campaigns matters \nplt.title(\"Success Rate by # of Previous Campaigns\")","5a0f5515":"train_data=train_data[train_data[\"previous\"]!=7]\n###DELETE OUTLIERS","87dee342":"train_data.groupby(\"poutcome\").y.value_counts(normalize=True).unstack()[1].plot(kind=\"bar\")\n#The number of campaigns matters \nplt.title(\"Success Rate by # of Pevious Campaign Outcome\")","77f59a2d":"train_data.groupby(\"emp.var.rate\").y.value_counts(normalize=True).unstack()[1].plot(kind=\"bar\")\n#The number of campaigns matters \nplt.title(\"Success Rate by # of Economic Variation\" )","c44e097b":"sns.heatmap(train_data[[\"emp.var.rate\",\"cons.price.idx\",\"cons.conf.idx\",\"euribor3m\"]].corr())\n#These variables are highly correlated. \n#Think about how to manage them if doing logistic regression. ","05ce5e24":"train_data.head(1)","6b387712":"y_train=train_data[\"y\"]\nX_train=train_data.drop(\"y\",axis=1)\ncutoff=X_train.shape[0]\nX_train.drop([\"id\",\"log_age\",\"test_campaign\",\"pdays_category\"],axis=1,inplace=True)","3fee2d12":"test_data.drop(\"duration\",axis=1,inplace=True)\ntest_data_new=test_data.drop(\"id\",axis=1)\nX=X_train.append(test_data_new)","1d42f53f":"X_dummies=pd.get_dummies(X,drop_first=True)","d33af890":"X=X_dummies.iloc[:cutoff]\nX_valid=X_dummies.iloc[cutoff:]\ny=y_train\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","19927207":"base_clf = LogisticRegression(random_state=0).fit(X_train, y_train)\ny_pred=base_clf.predict(X_test)\ny_score = base_clf.fit(X_train, y_train).decision_function(X_test)\n# Compute ROC curve and ROC area for each class\nfpr, tpr, _ = roc_curve(y_test, y_score)\nroc_auc = auc(fpr, tpr)\nC=confusion_matrix(y_test,y_pred)\nsns.heatmap(C \/ C.astype(np.float).sum(axis=1))\nplt.title(\"Confusion Matrix Normalized\")","a29aeeb5":"def print_classfiction_metrics(testy,yhat_classes):\n    # accuracy: (tp + tn) \/ (p + n)\n    accuracy = accuracy_score(testy, yhat_classes)\n    print('Accuracy: %f' % accuracy)\n    # precision tp \/ (tp + fp)\n    precision = precision_score(testy, yhat_classes)\n    print('Precision: %f' % precision)\n    # recall: tp \/ (tp + fn)\n    recall = recall_score(testy, yhat_classes)\n    print('Recall: %f' % recall)\n    # f1: 2 tp \/ (2 tp + fp + fn)\n    f1 = f1_score(testy, yhat_classes)\n    print('F1 score: %f' % f1)\n    \n    ","1a379b33":"print_classfiction_metrics(y_test,y_pred)\nprint(\"Area Under ROC Curve:\", roc_auc)","b5f3bb00":"result=base_clf.predict(X_valid)","aebf4497":"#Submission Score is 0.888300 ","4c8f5992":"y_train=train_data[\"y\"]\nX_train=train_data.drop(\"y\",axis=1)\ncutoff=X_train.shape[0]\nX_train.drop([\"id\",\"log_age\",\"test_campaign\",\"pdays_category\"],axis=1,inplace=True)","bb3810b3":"try:\n    test_data_new=test_data.drop([\"id\",\"duration\"],axis=1)\nexcept: \n    test_data_new=test_data.drop([\"id\"],axis=1)\nX=X_train.append(test_data_new,sort=False)\nX_transform=X.copy()","5b8223e9":"X_transform[\"age\"]=np.log(X_transform[\"age\"])\nX_transform[\"campaign\"]=X_transform['campaign'].apply(lambda x: np.floor(x\/3)+1)\n\n#Transform pdays to a category: \nbins=[0,7,14,21,28,1000]\nX_transform[\"pdays\"]=pd.cut(X_transform[\"pdays\"],bins, labels=[\"OneWeek\", \"TwoWeek\", \"ThreeWeek\",\"FourWeek\",\"NoPreviousCampaign\"],right=False)\n","d1533893":"X_dummies=pd.get_dummies(X_transform,drop_first=True)","bc1ddcee":"X_dummies.head(1)","123b6594":"X=X_dummies.iloc[:cutoff]\nX_valid=X_dummies.iloc[cutoff:]\ny=y_train","9bbc75a1":"numerics=[\"age\",\"campaign\",\"previous\",\"emp.var.rate\",\"cons.price.idx\",\"cons.conf.idx\",\"euribor3m\",\"nr.employed\"]\nX[numerics] = preprocessing.scale(X[numerics])\nX_valid[numerics] = preprocessing.scale(X_valid[numerics])\n","d3aa5246":"X.head(1)","06dd62d6":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","a92d0e67":"base_clf = LogisticRegression(random_state=0).fit(X_train, y_train)\ny_pred=base_clf.predict(X_test)\ny_score = base_clf.fit(X_train, y_train).decision_function(X_test)\n# Compute ROC curve and ROC area for each class\nfpr, tpr, _ = roc_curve(y_test, y_score)\nroc_auc = auc(fpr, tpr)\nC=confusion_matrix(y_test,y_pred)\nsns.heatmap(C \/ C.astype(np.float).sum(axis=1))\nplt.title(\"Confusion Matrix Normalized\")","bdbf26a7":"print_classfiction_metrics(y_test,y_pred)\nprint(\"Area Under ROC Curve:\", roc_auc)","7e6572c7":"## Changing Thresholds of the logistic model \n####################################\n# The optimal cut off would be where tpr is high and fpr is low\n# tpr - (1-fpr) is zero or near to zero is the optimal cut off point\n####################################\ni = np.arange(len(tpr)) # index for df\nroc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i), '1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i), 'thresholds' : pd.Series(_, index = i)})\nroc.ix[(roc.tf-0).abs().argsort()[:1]]\n\n# Plot tpr vs 1-fpr\nfig, ax = plt.subplots()\nplt.plot(roc['tpr'])\nplt.plot(roc['1-fpr'], color = 'red')\nplt.xlabel('1-False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nax.set_xticklabels([])\n\n\n","ee635340":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    \"\"\"\n    Modified from:\n    Hands-On Machine learning with Scikit-Learn\n    and TensorFlow; p.89\n    \"\"\"\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')","70ed0121":"p, r, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall_vs_threshold(p,r,thresholds)","69265f08":"import random\ndef find_best_accuracy(clf,X,y):\n    result={}\n    rands=[]\n    for x in range(5):\n        rands.append(random.randint(1,100001))\n    rands.append(42)\n    \n    for i in rands: \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=i)\n        clf.fit(X_train,y_train)\n        y_pred=clf.predict(X_test)\n        y_score=clf.decision_function(X_test)\n        thresholds= np.linspace(y_score.min(), y_score.max(),1000)\n        accr={}\n        for thre in thresholds:\n            new_y_pred=[0 if i<thre else 1 for i in y_score]\n            accuracy = accuracy_score(y_test, new_y_pred)\n            accr[thre]=accuracy*100    \n        result[i]=sorted(accr, key=(lambda key:accr[key]), reverse=True)[:3]\n    return result","363790ec":"find_best_accuracy(clf=LogisticRegression(random_state=0),X=X,y=y)","15971d34":"new_y_pred=[0 if i<0.03582943178391851 else 1 for i in y_score]","f5e6467d":"print_classfiction_metrics(y_test,new_y_pred)","b9ce3bfb":"params = {'penalty':['l1','l2'],'C':[0.01,0.1,1,10],'solver':['liblinear','saga']}\n# Create grid search using 5-fold cross validation\nbest_lg = GridSearchCV(LogisticRegression(max_iter=100), params, cv=5, verbose=0,scoring='accuracy',return_train_score=True)\nbest_lg.fit(X_train,y_train)","f6beb797":"best_lg.best_params_","93815a2f":"y_pred=best_lg.predict(X_test)","dda6d0a3":"y_score = best_lg.decision_function(X_test)\ny_pred=best_lg.predict(X_test)\nfpr, tpr, _ = roc_curve(y_test, y_score)\nroc_auc = auc(fpr, tpr)\nprint_classfiction_metrics(y_test,y_pred)\nprint(\"Area Under ROC Curve\",roc_auc)","1dedcd64":"train_data=pd.read_csv(\"bank-train.csv\")\ntest_data=pd.read_csv(\"bank-test.csv\")\ny_train=train_data[\"y\"]\nX_train=train_data.drop(\"y\",axis=1)\ncutoff=X_train.shape[0]\nX=X_train.append(test_data)\nX.drop([\"duration\",\"id\"],axis=1,inplace=True)\nX_transform=X.copy()\nX_transform=pd.get_dummies(X_transform,drop_first=True)\nX=X_transform.iloc[:cutoff]\ny=y_train\nX_valid=X_transform.iloc[cutoff:]\n\n#Scaling\nnumerics=[\"age\",\"campaign\",\"previous\",\"emp.var.rate\",\"cons.price.idx\",\"cons.conf.idx\",\"euribor3m\",\"nr.employed\"]\nX[numerics] = preprocessing.scale(X[numerics])\nX_valid[numerics] = preprocessing.scale(X_valid[numerics])\n","249facf6":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","d3378dd1":"names = [\"Nearest Neighbors\", \n         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n         \"Naive Bayes\", \"QDA\"]\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()]\n\n\n    # iterate over classifiers\nfor name, clf in zip(names, classifiers):\n    clf.fit(X_train, y_train.ravel())\n    if hasattr(clf, \"decision_function\"):\n        y_score = clf.decision_function(X_test)\n    else:\n        y_score = clf.predict_proba(X_test)[:, 1]\n    y_pred=clf.predict(X_test)\n    fpr, tpr, _ = roc_curve(y_test.ravel(), y_score)\n    roc_auc = auc(fpr, tpr)\n    print_classfiction_metrics(y_test.ravel(),y_pred)\n    plt.plot(fpr, tpr,lw=2, label='ROC curve (area = {}) for {}'.format(roc_auc,name) )\n  \nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic Curve')\nplt.legend(loc=\"upper left\", bbox_to_anchor=(1.05,1.05))\nplt.show()\n","b930acd4":"train_data=pd.read_csv(r\"..\/input\/bank-train.csv\")\ntest_data=pd.read_csv(r\"..\/input\/bank-test.csv\")\n#Delete Outliers \ntrain_data=train_data[train_data[\"default\"]!=\"yes\"]\ntrain_data=train_data[train_data[\"previous\"]<=5]\ntrain_data[\"campaign\"]=train_data['campaign'].apply(lambda x: np.floor(x\/3)+1)\ntest_data[\"campaign\"]=test_data['campaign'].apply(lambda x: np.floor(x\/3)+1)\n\ntrain_data=train_data[train_data[\"campaign\"]<=10]\ny_train=train_data[\"y\"]\nX_train=train_data.drop(\"y\",axis=1)\ncutoff=X_train.shape[0]\nX=X_train.append(test_data)\nX.drop([\"duration\",\"id\"],axis=1,inplace=True)\nX_transform=X.copy()\n\nX_transform[\"age\"]=np.log(X_transform[\"age\"])\n#Transform pdays to a category: \nbins=[0,7,14,21,28,1500]\nX_transform[\"pdays\"]=pd.cut(X_transform[\"pdays\"],bins, labels=[\"OneWeek\", \"TwoWeek\", \"ThreeWeek\",\"FourWeek\",\"NoPreviousCampaign\"],right=False)\nX_transform=pd.get_dummies(X_transform,drop_first=True)\nX=X_transform.iloc[:cutoff]\ny=y_train\nX_valid=X_transform.iloc[cutoff:]\n\n#Scaling\n\nnumerics=[\"age\",\"campaign\",\"previous\",\"emp.var.rate\",\"cons.price.idx\",\"cons.conf.idx\",\"euribor3m\",\"nr.employed\"]\nX[numerics] = preprocessing.scale(X[numerics])\nX_valid[numerics] = preprocessing.scale(X_valid[numerics])\n\n","26686e28":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","da529bd7":"names = [\"Nearest Neighbors\", \n         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n         \"Naive Bayes\", \"QDA\"]\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()]\n\n\n    # iterate over classifiers\nfor name, clf in zip(names, classifiers):\n    clf.fit(X_train, y_train.ravel())\n    if hasattr(clf, \"decision_function\"):\n        y_score = clf.decision_function(X_test)\n    else:\n        y_score = clf.predict_proba(X_test)[:, 1]\n    y_pred=clf.predict(X_test)\n    fpr, tpr, _ = roc_curve(y_test.ravel(), y_score)\n    roc_auc = auc(fpr, tpr)\n    print_classfiction_metrics(y_test.ravel(),y_pred)\n    plt.plot(fpr, tpr,lw=2, label='ROC curve (area = {}) for {}'.format(roc_auc,name) )\n  \nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic Curve')\nplt.legend(loc=\"upper left\", bbox_to_anchor=(1.05,1.05))\nplt.show()\n","8ed449fc":"train_data=pd.read_csv(r\"..\/input\/bank-train.csv\")\ntest_data=pd.read_csv(r\"..\/input\/bank-test.csv\")\ny_train=train_data[\"y\"]\nX_train=train_data.drop(\"y\",axis=1)\ncutoff=X_train.shape[0]\nX=X_train.append(test_data)\nX.drop([\"duration\",\"id\"],axis=1,inplace=True)","63913e45":"#Transform pdays to a category: \nbins=[0,7,14,21,28,1500]\nX[\"pdays\"]=pd.cut(X[\"pdays\"],bins, labels=[\"OneWeek\", \"TwoWeek\", \"ThreeWeek\",\"FourWeek\",\"NoPreviousCampaign\"],right=False)","597b63b4":"X.head(1)","1971d927":"nums=[\"age\",\"campaign\",\"previous\",\"emp.var.rate\",\"cons.price.idx\",\"cons.conf.idx\",\"euribor3m\",\"nr.employed\"]\ncats=[i for i in X.columns.tolist() if i not in nums]","80e1d455":"for i in cats:\n    X[i]=X[i].astype('category')","3b8c1bc6":"X.head(1)","5f9d0b26":"# Desired label orders for categorical columns.\n\neduc_order = ['unknown', 'illiterate', 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'professional.course', 'university.degree']\nday_order = ['mon', 'tue', 'wed', 'thu', 'fri']\n\n\ndef ordered_labels(df, col, order):\n    df[col] = df[col].astype('category')\n    df[col] = df[col].cat.reorder_categories(order, ordered=True)\n    df[col] = df[col].cat.codes.astype(int)\n    return df\n\nX=ordered_labels(X,\"education\",educ_order)\nX=ordered_labels(X,\"day_of_week\",day_order)","f2ac6681":"X=pd.get_dummies(X,drop_first=True)","6cdb9c02":"X.head(1)","c3c7751f":"X_transform=X.copy()\nX=X_transform.iloc[:cutoff]\ny=y_train\nX_valid=X_transform.iloc[cutoff:]","7e6ca65e":"#Scaling\nnumerics=[\"age\",\"campaign\",\"previous\",\"emp.var.rate\",\"cons.price.idx\",\"cons.conf.idx\",\"euribor3m\",\"nr.employed\"]\nX[numerics] = preprocessing.scale(X[numerics])\nX_valid[numerics] = preprocessing.scale(X_valid[numerics])\n\n","3d575a32":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","f04e83a6":"clf=AdaBoostClassifier()","887cd16b":"clf.fit(X_train,y_train)","7af9cb1a":"y_score = clf.decision_function(X_test)\ny_pred= clf.predict(X_test)\nfpr, tpr, _ = roc_curve(y_test, y_score)\nroc_auc = auc(fpr, tpr)\nprint_classfiction_metrics(y_test,y_pred)\nprint(\"Area Under ROC Curve\",roc_auc)","4eaa4eb7":"####################################\n# The optimal cut off would be where tpr is high and fpr is low\n# tpr - (1-fpr) is zero or near to zero is the optimal cut off point\n####################################\ni = np.arange(len(tpr)) # index for df\nroc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i), '1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i), 'thresholds' : pd.Series(_, index = i)})\nroc.ix[(roc.tf-0).abs().argsort()[:1]]\n\n# Plot tpr vs 1-fpr\nfig, ax = plt.subplots()\nplt.plot(roc['tpr'])\nplt.plot(roc['1-fpr'], color = 'red')\nplt.xlabel('1-False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nax.set_xticklabels([])","ad8bc062":"p, r, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall_vs_threshold(p,r,thresholds)","66f55465":"find_best_accuracy(clf,X,y)","5ff95c0c":"result_scores=clf.decision_function(X_valid)","a8ed86c6":"result=[0 if i<-0.0013974661683098244 else 1 for i in result_scores]","06de901f":"submission = pd.concat([test_data[\"id\"], pd.Series(result)], axis = 1)\nsubmission.columns = ['id', 'Predicted']\nsubmission.to_csv('submission.csv', index=False)","3b548c15":"Submission reulst on this model is 0.88911","5c9ff601":"Tuning submission result is 0.88830.","38f7cf77":"### EDA","d89289a8":"#### Changing Threshold  of the Ada Boost Model","ee3244f7":"### Feature Transformation ","cda01ab4":"### Baseline","27e3bc30":"### Try Other Models on Transformed Datasets ","4bdd8430":"### Try Other Models Using Original Dataset","28caee47":"### AdaBoost On Entire DataSet","0053852d":"### Tune the Logistic Model "}}