{"cell_type":{"97c09df1":"code","80de2432":"code","af8df81a":"code","332c420b":"code","8750bfaa":"code","65dcc285":"code","073889b0":"code","7651f02a":"code","64b3a2b1":"code","08c1913b":"code","eb217403":"code","c12d7ceb":"code","844930db":"markdown","2a1226fd":"markdown","fc1e9d33":"markdown"},"source":{"97c09df1":"!git clone https:\/\/github.com\/NVlabs\/stylegan2-ada-pytorch.git\n!pip install ninja","80de2432":"\n!ls .\/stylegan2-ada-pytorch","af8df81a":"\n!python .\/stylegan2-ada-pytorch\/generate.py \\\n    --network=https:\/\/nvlabs-fi-cdn.nvidia.com\/stylegan2-ada\/pretrained\/ffhq.pkl \\\n  --outdir=.\/results --seeds=6600-6625 ","332c420b":"!ls .\/results","8750bfaa":"import sys\nsys.path.insert(0, \".\/stylegan2-ada-pytorch\")\nimport pickle\nimport os\nimport numpy as np\nimport PIL.Image\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport IPython.display\nimport torch\nimport dnnlib\nimport legacy","65dcc285":"\ndef seed2vec(G, seed):\n  return np.random.RandomState(seed).randn(1, G.z_dim)\n\ndef display_image(image):\n  plt.axis('off')\n  plt.imshow(image)\n  plt.show()\n\ndef generate_image(G,z,truncation_psi):\n  Gs_kwargs = {\n      'output_transform': dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True),\n      'randomize_noise':False\n  }\n  if truncation_psi is not None:\n    Gs_kwargs['truncation_psi'] = truncation_psi\n\n  label = np.zeros([1] + G.input_shape[1][1:])\n  images = G.run(z, label, **G_kwargs)\n  return images[0]\n\ndef get_label(G, device, class_idx):\n  label = torch.zeros([1, G.c_dim], device=device)\n\n  if G.c_dim != 0:\n    if class_idx is None:\n      ctx.fail('Must specify class label with --class when using a conditional network')\n    label[:, class_idx] = 1\n  else :\n    if class_idx is not None:\n      print('warn: --class-lbl ignoredwhen running on an unconditional network')\n  return label\n\ndef generate_image(device, G, z, truncation_psi=1.0, noise_mode='const', class_idx=None):\n  z = torch.from_numpy(z).to(device)\n  label = get_label(G, device, class_idx)\n  img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n  img = (img.permute(0,2,3,1)* 127.5 + 128).clamp(0,255).to(torch.uint8)\n  return PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB')","073889b0":"\nURL = \"https:\/\/github.com\/jeffheaton\/pretrained-gan-fish\/releases\/download\/1.0.0\/fish-gan-2020-12-09.pkl\"\n\nprint(f'Loading networks from \"{URL}\"...')\ndevice = torch.device('cuda')\nwith dnnlib.util.open_url(URL) as f:\n    G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n","7651f02a":"# Choose your own starting and ending seed.\nSEED_FROM = 99\nSEED_TO = 111\n\n# Generate the images for the seeds.\nfor i in range(SEED_FROM, SEED_TO):\n  print(f\"Seed {i}\")\n  z = seed2vec(G, i)\n  img = generate_image(device, G, z)\n  display_image(img)","64b3a2b1":"def expand_seed(seeds, vector_size):\n  result = []\n\n  for seed in seeds:\n    rnd = np.random.RandomState(seed)\n    result.append( rnd.randn(1, vector_size) ) \n  return result\n\n#URL = \"https:\/\/github.com\/jeffheaton\/pretrained-gan-fish\/releases\/download\/1.0.0\/fish-gan-2020-12-09.pkl\"\n#URL = \"https:\/\/github.com\/jeffheaton\/pretrained-merry-gan-mas\/releases\/download\/v1\/christmas-gan-2020-12-03.pkl\"\nURL = \"https:\/\/nvlabs-fi-cdn.nvidia.com\/stylegan2-ada\/pretrained\/ffhq.pkl\"\n\nprint(f'Loading networks from \"{URL}\"...')\ndevice = torch.device('cuda')\nwith dnnlib.util.open_url(URL) as f:\n    G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n\nvector_size = G.z_dim\n# range(8192,8300)\nseeds = expand_seed( [8192+1,8192+9], vector_size)\n#generate_images(Gs, seeds,truncation_psi=0.5)\nprint(seeds[0].shape)","08c1913b":"# Choose your seeds to morph through and the number of steps to take to get to each.\n\nSEEDS = [111,113,111]\nSTEPS = 100\n\n# Remove any prior results\n!rm .\/results\/* \n\nfrom tqdm.notebook import tqdm\n\nos.makedirs(\".\/results\/\", exist_ok=True)\n\n# Generate the images for the video.\nidx = 0\nfor i in range(len(SEEDS)-1):\n  v1 = seed2vec(G, SEEDS[i])\n  v2 = seed2vec(G, SEEDS[i+1])\n\n  diff = v2 - v1\n  step = diff \/ STEPS\n  current = v1.copy()\n\n  for j in tqdm(range(STEPS), desc=f\"Seed {SEEDS[i]}\"):\n    current = current + step\n    img = generate_image(device, G, current)\n    img.save(f'.\/results\/frame-{idx}.png')\n    idx+=1\n \n","eb217403":"# Link the images into a video.\n!ffmpeg -r 30 -i .\/results\/frame-%d.png -vcodec mpeg4 -y movie.mp4","c12d7ceb":"from google.colab import files\nfiles.download('movie.mp4')","844930db":"## Latent gan","2a1226fd":"Notebook authored by Jeff heaton. Just trying in kaggle. https:\/\/colab.research.google.com\/github\/jeffheaton\/t81_558_deep_learning\/blob\/master\/t81_558_class_07_3_style_gan.ipynb#scrollTo=RCQba6qmVwF0","fc1e9d33":"lookat the result in form of a video !"}}