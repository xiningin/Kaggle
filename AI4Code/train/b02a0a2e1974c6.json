{"cell_type":{"8372e86f":"code","53cf13b1":"code","a2eefcf4":"code","4da75bc7":"code","e1a4993c":"code","2460552f":"code","59602f5d":"code","562cb77f":"code","2b6bd32e":"code","41a4ce8a":"code","cd6585a7":"code","c25a6b8f":"code","2971500c":"code","ba1ff553":"code","b4ecf813":"code","8c8ea2b6":"code","e574b888":"code","093c4ad1":"code","5c727d74":"code","9e455f59":"code","e793d9aa":"code","f2eab06a":"code","090b0375":"code","df2f86fb":"markdown","57059311":"markdown","2ddcb8c6":"markdown","335c0f9a":"markdown","58c3773e":"markdown","e0b7cc00":"markdown","8d24dce4":"markdown"},"source":{"8372e86f":"# Let us import the required packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns # visualize the co-relation using this library\n%matplotlib inline\nfrom sklearn.preprocessing import StandardScaler","53cf13b1":"# Let us read the data from the file and see the first five rows of the data\ndata = pd.read_csv(\"..\/input\/DS_BEZDEKIRIS_STD.data\", header = None)\ndata.head()","a2eefcf4":"data.corr()","4da75bc7":"correlation = data.corr()\nplt.figure(figsize=(4,4))\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='cubehelix')\n\nplt.title('Correlation between different fearures')","e1a4993c":"X = data.iloc[:,0:4].values\ny = data.iloc[:,-1].values\nX","2460552f":"y","59602f5d":"np.shape(X)","562cb77f":"np.shape(y)","2b6bd32e":"X_std = StandardScaler().fit_transform(X)","41a4ce8a":"X_std.shape","cd6585a7":"mean_vec = np.mean(X_std, axis=0)\ncov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) \/ (X_std.shape[0]-1)\nprint('Covariance matrix \\n%s' %cov_mat)","c25a6b8f":"print('NumPy covariance matrix: \\n%s' %np.cov(X_std.T))","2971500c":"plt.figure(figsize=(4,4))\nsns.heatmap(cov_mat, vmax=1, square=True,annot=True,cmap='cubehelix')\n\nplt.title('Correlation between different features')","ba1ff553":"eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)","b4ecf813":"# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])","8c8ea2b6":"tot = sum(eig_vals)\nvar_exp = [(i \/ tot)*100 for i in sorted(eig_vals, reverse=True)]","e574b888":"with plt.style.context('dark_background'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(4), var_exp, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","093c4ad1":"feature_vector = np.hstack((eig_pairs[0][1].reshape(4,1), \n                      eig_pairs[1][1].reshape(4,1)\n                    ))\nprint('Matrix W:\\n', feature_vector)","5c727d74":"final_data = X_std.dot(feature_vector)","9e455f59":"final_data","e793d9aa":"from sklearn.decomposition import PCA\npca = PCA().fit(X_std)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlim(0,4,1)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')","f2eab06a":"from sklearn.decomposition import PCA \nsklearn_pca = PCA(n_components=2)\nY_sklearn = sklearn_pca.fit_transform(X_std)","090b0375":"Y_sklearn","df2f86fb":"## PCA in scikit-learn","57059311":"Seperating the features and classes from the dataset.","2ddcb8c6":"Standardize features by removing the mean and scaling to unit variance\n\nCentering and scaling happen independently on each feature by computing\nthe relevant statistics on the samples in the training set. Mean and\nstandard deviation are then stored to be used on later data using the\n`transform` method.","335c0f9a":"#### df.corr()\nwill display the co-relation between features.\n+ve value denotes the positive relation ( if we increase value of one feature, other feature value also increases).\n-ve value denotes the negative relation ( if we increase value of one feature, other feature value also decreases).","58c3773e":"The first two components are giving 90% of information. Hence choosing the top two vectors. i.e. `Feature Vector`","e0b7cc00":"`Final data = Row feature vector * Row data adjust`\n\n`Row feature vector` is the matrix with the eigenvectors in the columns transposed so that the eigenvectors are now in the rows, with the most significant eigenvector at the top. i.e. `Feature Vector`\n\n`Row data adjust` is the mean adjusted data. i.e. `X_std`.","8d24dce4":"References: \n    http:\/\/www.cs.otago.ac.nz\/cosc453\/student_tutorials\/principal_components.pdf\n    https:\/\/www.kaggle.com\/nirajvermafcb\/principal-component-analysis-explained"}}