{"cell_type":{"fc9d1b21":"code","3f449887":"code","72eecb84":"code","73e86b1a":"code","75aff841":"code","c2fbeca1":"code","98d38588":"code","b48ddbbb":"code","2d2eb6ae":"code","62f2d3e4":"code","677090ed":"code","8ade4fbf":"code","1e91d88e":"code","8213ed10":"code","d6244b01":"code","ebd17404":"code","4b030d1a":"code","4b8037b4":"code","05752e71":"code","9f7db6bf":"code","1dc877ee":"code","7e30b8a1":"code","3f9f90f6":"code","25942f85":"code","172923a7":"code","cdb18cb3":"code","3fae60de":"code","88a4ea79":"code","5ba7e388":"code","6cd932ff":"code","9ef6e76b":"code","c08122c0":"code","78f4719e":"code","69215d44":"code","6b055f9d":"code","d238727e":"code","7f603f6b":"code","30fcd3da":"code","66d8f7f5":"code","cf53a74c":"code","a8a38e1b":"code","4cb43322":"code","a3a76c93":"code","762bbb80":"code","8b06721c":"code","283e4101":"code","5d99c2d0":"code","3829bd8a":"code","1cf3f019":"code","97651e57":"code","b084cfcc":"code","267b6544":"code","229f8837":"code","ea34ed15":"code","fccba373":"code","580ba7a3":"code","891d461b":"code","0c1ef04a":"code","a4a4b5b2":"code","7e63b667":"code","ffd313fe":"code","75546ab9":"code","226c9fe6":"markdown","8d7ba9fa":"markdown","87f66c87":"markdown","56ecf9d5":"markdown","b78e3ee5":"markdown","44911c3b":"markdown","1c69b186":"markdown","ae8e4256":"markdown","004ec1ad":"markdown","ea22120f":"markdown","10218939":"markdown","58f96e0e":"markdown","a24c64bb":"markdown","f6badf85":"markdown","0214b9e7":"markdown","f8e53184":"markdown","269c47a3":"markdown","5df20d30":"markdown","8c7d6fd4":"markdown","d815bbe2":"markdown","07aca843":"markdown","a05635a2":"markdown","9305236f":"markdown","644c6603":"markdown","5c6a76c0":"markdown","b14e2e90":"markdown","7770fdda":"markdown","b16bb04e":"markdown","cc53d5ce":"markdown","b93f1d05":"markdown","cfbc8c3d":"markdown","b54708ad":"markdown","70ff5df4":"markdown","0395e11b":"markdown","065430bb":"markdown","c84ee23a":"markdown","33c11bb9":"markdown","6f7c9a5d":"markdown","36f81f67":"markdown","2b6fa03d":"markdown","071206d0":"markdown","df10a255":"markdown","ce7cc881":"markdown","d49e5c31":"markdown","06073b85":"markdown","5ec1a5ed":"markdown","946510ac":"markdown","7b435629":"markdown","be923917":"markdown","5ec19a6f":"markdown"},"source":{"fc9d1b21":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom sklearn import metrics\n\nimport numpy as np\n\n# allow plots to appear directly in the notebook\n%matplotlib inline","3f449887":"data = pd.read_csv('..\/input\/advertising.csv\/Advertising.csv', index_col=0)\ndata.head(100)","72eecb84":"data.shape","73e86b1a":"data.info()","75aff841":"data.describe()","c2fbeca1":"f, axes = plt.subplots(2, 2, figsize=(15, 7), sharex=False)            # Set up the matplotlib figure\nsns.despine(left=True)\n\nsns.distplot(data.sales, color=\"b\", ax=axes[0, 0])\n\nsns.distplot(data.TV, color=\"r\", ax=axes[0, 1])\n\nsns.distplot(data.radio, color=\"g\", ax=axes[1, 0])\n\nsns.distplot(data.newspaper, color=\"m\", ax=axes[1, 1])","98d38588":"JG1 = sns.jointplot(\"newspaper\", \"sales\", data=data, kind='reg')","b48ddbbb":"JG2 = sns.jointplot(\"radio\", \"sales\", data=data, kind='reg')","2d2eb6ae":"JG3 = sns.jointplot(\"TV\", \"sales\", data=data, kind='reg')","62f2d3e4":"sns.pairplot(data, height = 2, aspect = 1.5)","677090ed":"sns.pairplot(data, x_vars=['TV', 'radio', 'newspaper'], y_vars='sales', size=5, aspect=1, kind='reg')","8ade4fbf":"data.corr()","1e91d88e":"plt.figure(figsize=(7,5))\nsns.heatmap(round(data.corr(),2),annot=True)\nplt.show()","8213ed10":"features = ['TV', 'radio', 'newspaper']                # create a Python list of feature names\ntarget = ['sales']                                     # Define the target variable","d6244b01":"data.head()","ebd17404":"data[features]","4b030d1a":"data[target]","4b8037b4":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.05, random_state=5000)","05752e71":"print('Train cases as below')\nprint('X_train shape: ',X_train.shape)\nprint('y_train shape: ',y_train.shape)\nprint('\\nTest cases as below')\nprint('X_test shape: ',X_test.shape)\nprint('y_test shape: ',y_test.shape)","9f7db6bf":"X_train.head()","1dc877ee":"y_train.head()","7e30b8a1":"X_test.head()","3f9f90f6":"y_test.head()","25942f85":"#Instantiating the model\nfrom sklearn.linear_model import LinearRegression\nlr_model = LinearRegression(fit_intercept=True)","172923a7":"lr_model.fit(X_train, y_train)","cdb18cb3":"print('Intercept:',lr_model.intercept_)          # print the intercept \nprint('Coefficients:',lr_model.coef_)  ","3fae60de":"X_train.columns","88a4ea79":"(lr_model.coef_).T","5ba7e388":"pd.DataFrame((lr_model.coef_).T,index=X_train.columns,\\\n             columns=['Co-efficients']).sort_values('Co-efficients',ascending=False)","6cd932ff":"y_pred_train = lr_model.predict(X_train)  ","9ef6e76b":"y_pred_train                                                         # make predictions on the training set","c08122c0":"y_pred_test = lr_model.predict(X_test)                                  # make predictions on the testing set","78f4719e":"y_pred_test","69215d44":"MAE_train = metrics.mean_absolute_error(y_train, y_pred_train)\nMAE_test = metrics.mean_absolute_error(y_test, y_pred_test)","6b055f9d":"print('MAE for training set is {}'.format(MAE_train))\nprint('MAE for test set is {}'.format(MAE_test))","d238727e":"MSE_train = metrics.mean_squared_error(y_train, y_pred_train)\nMSE_test = metrics.mean_squared_error(y_test, y_pred_test)","7f603f6b":"print('MSE for training set is {}'.format(MSE_train))\nprint('MSE for test set is {}'.format(MSE_test))","30fcd3da":"RMSE_train = np.sqrt( metrics.mean_squared_error(y_train, y_pred_train))\nRMSE_test = np.sqrt(metrics.mean_squared_error(y_test, y_pred_test))","66d8f7f5":"print('RMSE for training set is {}'.format(RMSE_train))\nprint('RMSE for test set is {}'.format(RMSE_test))","cf53a74c":"data['sales'].mean()","a8a38e1b":"RMSE_test\/data['sales'].mean()","4cb43322":"np.random.seed(123456)                                                # set a seed for reproducibility\nnums = np.random.rand(len(data))\nmask_suburban = (nums > 0.33) & (nums < 0.66)                         # assign roughly one third of observations to each group\nmask_urban = nums > 0.66\ndata['Area'] = 'rural'\ndata.loc[mask_suburban, 'Area'] = 'suburban'\ndata.loc[mask_urban, 'Area'] = 'urban'\n\ndata.head(50)","a3a76c93":"data.groupby(['Area'])['sales'].mean().sort_values(ascending=False).plot(kind = 'bar')","762bbb80":"a = sns.scatterplot(x=\"TV\", y=\"sales\", data=data, hue='Area')","8b06721c":"a = sns.scatterplot(x=\"Area\", y=\"sales\", data=data)","283e4101":"#data.to_csv(\"data_with_area.csv\")","5d99c2d0":"features = ['TV', 'radio', 'newspaper', 'Area']\ncat_cols = ['Area']                                           # Define the categorical variables","3829bd8a":"data_with_dummies = pd.get_dummies(data, columns=cat_cols, drop_first=True)\ndata_with_dummies.head()","1cf3f019":"feature_cols = ['TV', 'radio', 'newspaper', 'Area_suburban', 'Area_urban']             # create a Python list of feature names\nX = data_with_dummies[feature_cols]  \ny = data_with_dummies.sales\nlr_model_cat = LinearRegression()","97651e57":"lr_model_cat.fit(X,y)","b084cfcc":"y_pred_cat = lr_model_cat.predict(X)  ","267b6544":"pd.DataFrame((lr_model_cat.coef_).T,index=X.columns,\\\n             columns=['Co-efficients']).sort_values('Co-efficients',ascending=False)","229f8837":"print('Intercept:',lr_model_cat.intercept_)","ea34ed15":"data_with_dummies['predictions'] = y_pred_cat","fccba373":"data_with_dummies","580ba7a3":"data_with_dummies['error'] = data_with_dummies['sales'] - data_with_dummies['predictions']","891d461b":"data_with_dummies['error'].describe()","0c1ef04a":"data_with_dummies.plot.scatter(x='sales', y='predictions',\\\n                      figsize=(8,5), grid=True, title='Actual vs Predicted')","a4a4b5b2":"sns.distplot(data_with_dummies['error'])","7e63b667":"data_with_dummies[data_with_dummies['error']<-4]","ffd313fe":"data_with_dummies.plot.scatter(x='sales', y='error',\\\n                      figsize=(8,5), grid=True, title='Actual vs Predicted')","75546ab9":"data_with_dummies.to_csv('data_with_predictions.csv')","226c9fe6":"### 5.1 Model Evaluation using __metrics.__","8d7ba9fa":"There are 200 **observations**, and thus 200 markets in the dataset.","87f66c87":"What are the **features**?\n- TV: advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n- Radio: advertising dollars spent on Radio\n- Newspaper: advertising dollars spent on Newspaper\n\nWhat is the **response**?\n- Sales: sales of a single product in a given market (in thousands of widgets)","56ecf9d5":"## 3. Exploratory Data Analysis","b78e3ee5":"# Some EDA of categorical variable","44911c3b":"__y = 2.9 + 0.0468 `*` TV + 0.1785 `*` radio + 0.00258 `*` newspaper__","1c69b186":"__Importing Packages__","ae8e4256":"<a id=section8><\/a>","004ec1ad":"### 4.1 Splitting data into training and test datasets. \"WHY?\"","ea22120f":"__Observation__\n\n- The diagonal of the above matirx shows the auto-correlation of the variables. It is always 1. You can observe that the correlation between __TV and Sales is highest i.e. 0.78__ and then between __sales and radio i.e. 0.576__.\n\n- correlations can vary from -1 to +1. Closer to +1 means strong positive correlation and close -1 means strong negative correlation. Closer to 0 means not very strongly correlated. variables with __strong correlations__ are mostly probably candidates for __model builing__.\n","10218939":"## Table of Content\n\n1. [Problem Statement](#section1)<br>\n2. [Data Loading and Description](#section2)<br>\n3. [Exploratory Data Analysis](#section3)<br>\n4. [Training and testing the Model](#section4)<br>\n    - 4.1 [Splitting data into training and test datasets](#section401)<br>\n    - 4.2 [Linear regression in scikit-learn](#section402)<br>\n    - 4.3 [Interpreting Model Coefficients](#section403)<br>\n    - 4.4 [Using the Model for Prediction](#section404)<br>\n    \n5. [Model evaluation](#section5)<br>\n    - 5.1 [Model evaluation using metrics](#section501)<br>\n6. [Handling Categorical Features](#section6)<br>","58f96e0e":"<a id=section3><\/a>","a24c64bb":"Here is how we interpret the coding:\n- **rural** is coded as  Area_suburban = 0  and  Area_urban = 0\n- **suburban** is coded as  Area_suburban = 1  and  Area_urban = 0\n- **urban** is coded as  Area_suburban = 0  and  Area_urban = 1\n\nIf this sounds confusing, think in general terms that why we need only __k-1 dummy variables__ if we have a categorical feature with __k \"levels\"__.\n\nAnyway, let's add these two new dummy variables onto the original DataFrame, and then include them in the linear regression model.","f6badf85":"- We need an evaluation metric in order to compare our predictions with the actual values.","0214b9e7":"<a id=section2><\/a>","f8e53184":"<a id=section5><\/a>","269c47a3":"__Root Mean Squared Error__ (RMSE) is the square root of the mean of the squared errors:\n\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n\nComputing the RMSE for our Sales predictions","5df20d30":"__Observation__<br\/>\n_Sales and spend on newpaper_ is __not__ highly correlaed where are _sales and spend on tv_ is __highly correlated__.","8c7d6fd4":"<a id=section501><\/a>","d815bbe2":"### 4.4 Using the Model for Prediction","07aca843":"__Observation__\n\n- Strong relationship between TV ads and sales\n- Weak relationship between Radio ads and sales\n- Very weak to no relationship between Newspaper ads and sales\n\n","a05635a2":"## 2. Data Loading and Description\n\nThe adverstising dataset captures sales revenue generated with respect to advertisement spends across multiple channels like radio, tv and newspaper.\n- TV        - Spend on TV Advertisements\n- Radio     - Spend on radio Advertisements\n- Newspaper - Spend on newspaper Advertisements\n- Sales     - Sales revenue generated","9305236f":"## 5. Model evaluation ","644c6603":"__Mean Squared Error__ (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n\nComputing the MSE for our Sales predictions","5c6a76c0":"### 4.2 Linear regression in scikit-learn","b14e2e90":"We want to represent Area numerically, but we can't simply code it as:<br\/>\n- 0 = rural,<br\/>\n- 1 = suburban,<br\/>\n- 2 = urban<br\/>\nBecause that would imply an **ordered relationship** between suburban and urban, and thus urban is somehow \"twice\" the suburban category.<br\/> Note that if you do have ordered categories (i.e., strongly disagree, disagree, neutral, agree, strongly agree), you can use a single dummy variable to represent the categories numerically (such as 1, 2, 3, 4, 5).<br\/>\n\nAnyway, our Area feature is unordered, so we have to create **additional dummy variables**. Let's explore how to do this using pandas:","7770fdda":"#### Importing the Dataset","b16bb04e":"<a id=section401><\/a>","cc53d5ce":"### Is there a relationship between sales and spend various advertising channels?","b93f1d05":"__Mean Absolute Error__ (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\nComputing the MAE for our Sales predictions","cfbc8c3d":"## 1. Problem Statement\n\n__Sales__ (in thousands of units) for a particular product as a __function__ of __advertising budgets__ (in thousands of dollars) for _TV, radio, and newspaper media_. Suppose that in our role as __Data Scientist__ we are asked to suggest.\n\n- We want to find a function that given input budgets for TV, radio and newspaper __predicts the output sales__.\n\n- Which media __contribute__ to sales?\n\n- Visualize the __relationship__ between the _features_ and the _response_ using scatter plots.","b54708ad":"### Calculating and plotting heatmap correlation","70ff5df4":"__Error__ is the _deviation_ of the values _predicted_ by the model with the _true_ values.<br\/>\nFor example, if a model predicts that the price of apple is Rs75\/kg, but the actual price of apple is Rs100\/kg, then the error in prediction will be Rs25\/kg.<br\/>\nBelow are the types of error we will be calculating for our _linear regression model_:\n- Mean Absolute Error\n- Mean Squared Error\n- Root Mean Squared Error","0395e11b":"__y =  2.92 + 0.045802 `*` TV + 0.1876 `*` radio - 0.001018 `*` newspaper - 0.117890 `*` Area_suburban + 0.2535 `*` Area_urban__<br\/>\nHow do we interpret the coefficients?<br\/>\n- Holding all other variables fixed, being a **suburban** area is associated with an average **decrease** in Sales of 0.1178 widgets (as compared to the baseline level, which is rural).\n- Being an **urban** area is associated with an average **increase** in Sales of 0.2535 widgets (as compared to rural).","065430bb":"<a id=section403><\/a>","c84ee23a":"__Distribution of Features__","33c11bb9":"<a id=section402><\/a>","6f7c9a5d":"To apply any machine learning algorithm on your dataset, basically there are 4 steps:\n1. Load the algorithm\n2. Instantiate and Fit the model to the training dataset\n3. Prediction on the test set\n4. Evaluate - Calculate RMSE and R square\n\nThe code block given below shows how these steps are carried out:<br\/>\n\n``` from sklearn.linear_model import LinearRegression\n    lr_model = LinearRegression()\n    ll_model.fit(X_train, y_train) \n    RMSE_test = np.sqrt(metrics.mean_squared_error(y_test, y_pred_test))\n    r2_test = metrics.r2_score(y_test, y_pred_test)```","36f81f67":"<a id=section6><\/a>","2b6fa03d":"### Visualising Pairwise correlation","071206d0":"## 4. Training and Testing the Model","df10a255":"<a id=section1><\/a>","ce7cc881":"### 4.3 Interpreting Model Coefficients","d49e5c31":"How do we interpret the TV coefficient (0.0468)\n- A \"unit\" increase in TV ad spending is **associated with** a _\"0.0468_ unit\" increase in Sales.\n- Or more clearly: An additional $1,000 spent on TV ads is **associated with** an increase in sales of ~0.0468 * 1000 = 47 widgets.\n\nImportant Notes:\n- This is a statement of __association__, not __causation__.\n- If an increase in TV ad spending was associated with a __decrease__ in sales,  \u03b21  would be __negative.__","06073b85":"Comparing these metrics:\n- __RMSE__ is more popular than MSE, because RMSE is _interpretable_ in the \"y\" units.\n    - Easier to put in context as it's the same units as our response variable.","5ec1a5ed":"## 6.  Handling Categorical Features\n\nLet's create a new feature called **Area**, and randomly assign observations to be **rural, suburban, or urban** :","946510ac":"<a id=section4><\/a>","7b435629":"<a id=section404><\/a>","be923917":"ORIGINAL EQUATION\n\n__y = 2.9 + 0.0468 `*` TV + 0.1785 `*` radio + 0.00258 `*` newspaper__","5ec19a6f":"# LINEAR REGRESSION"}}