{"cell_type":{"7ef40d8b":"code","67bdec32":"code","69717d5c":"code","cce8262d":"code","2b96dda9":"code","c05f3c87":"markdown","783030f9":"markdown","a028d9a8":"markdown","aa82b8c0":"markdown","06b7319d":"markdown","9acce10c":"markdown","b800a40d":"markdown"},"source":{"7ef40d8b":"!pip freeze > kaggle_image_requirements.txt","67bdec32":"# Pipeline uses `gpt2` by default, but we specify it explicitly to be fully transparent\nfrom transformers import pipeline\ngpt = pipeline('text-generation',model='gpt2')","69717d5c":"gpt(\"Transfer learning is a field of study\", max_length=100)","cce8262d":"from transformers import AutoModelWithLMHead, AutoTokenizer # you can use these utility classes that automatically load the right classes\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer # or these more specific classes directly\nimport torch\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"microsoft\/DialoGPT-medium\")\nmodel = GPT2LMHeadModel.from_pretrained(\"microsoft\/DialoGPT-medium\")","2b96dda9":"# Chat for 5 Lines\nconversation_length = 5\nfor step in range(conversation_length):\n    # encode new user input, add end-of-sentence token, return tensor\n    new_user_inputs_ids = tokenizer.encode(input(\"User: \") + tokenizer.eos_token, return_tensors='pt')\n    \n    # add new input to chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_inputs_ids], dim=1) if step > 0 else new_user_inputs_ids\n    \n    # generate a response of up to max_length tokens\n    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n    \n    # display response\n    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))","c05f3c87":"Now, let's generate some text with GPT-2","783030f9":"# Open Ended Text Generation with GPT-2","a028d9a8":"# Conversational Text Generation with DialoGPT\n\nDialoGPT is an extension of GPT to conversational response generation","aa82b8c0":"# WARNING\n**Please make sure to \"COPY AND EDIT NOTEBOOK\" to use compatible library dependencies! DO NOT CREATE A NEW NOTEBOOK AND COPY+PASTE THE CODE - this will use latest Kaggle dependencies at the time you do that, and the code will need to be modified to make it work. Also make sure internet connectivity is enabled on your notebook**","06b7319d":"# Preliminaries\nWrite requirements to file, anytime you run it, in case you have to go back and recover dependencies. **MOST OF THESE REQUIREMENTS WOULD NOT BE NECESSARY FOR LOCAL INSTALLATION**\n\nRequirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https:\/\/github.com\/azunre\/transfer-learning-for-nlp","9acce10c":"A nonexhaustive list of other model choices suitable for text generation within the transformers library include \"ctrl\" (CTRL - huge! too big for Kaggle), \"xlnet-base-cased\" (XLNet), \"transfo-xl-wt103\" (Transformer XL)... These often need to be padded very carefully to work well, GPT-2 is the safest choice for open-ended text generation. See https:\/\/huggingface.co\/transformers\/usage.html#text-generation for more.","b800a40d":"**Because the notebook is not saving input text, here is the entire conversation so you know how we prompted the model**\n\nUser:  hi robot\n\nDialoGPT: Hello, human.\n\nUser:  huh?\n\nDialoGPT: I'm a bot.\n\nUser:  ok, what is your name?\n\nDialoGPT: Robot. I'm a robot.\n\nUser:  ok! Can you say something else?\n\nDialoGPT: Robot. I'm a robot.\n\nUser:  Do you have children?\n\nDialoGPT: Robot. I'm a robot."}}