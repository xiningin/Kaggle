{"cell_type":{"c4ddd83a":"code","705a4b9c":"code","9ac05ab4":"code","fb59184b":"code","4ea15081":"code","85ed6ff6":"code","80ebb5a3":"code","ae462fa3":"code","3a913aa3":"code","913789d3":"code","aac5b024":"code","f532bf68":"code","01918550":"code","a63a3bab":"code","d512bb25":"code","c3d8b3b9":"code","67edb6f8":"code","4e3e3c71":"code","96eae8eb":"code","3c14c5b7":"code","4ab5e8fa":"code","64aeeeea":"code","96fc2560":"code","dc945a1e":"code","c5ad3af2":"code","2c6a4078":"code","566a7ac7":"code","108f8840":"code","e9cd2c04":"code","16aa2b41":"code","d9e10c21":"code","4a89c014":"code","cbb7a183":"code","e18e045e":"code","8b5311f5":"code","78ab63a7":"code","993e4fb6":"code","99d81ec5":"code","bbf3d660":"code","dc0aef6c":"code","0640b34d":"code","ed0397d7":"code","b7e33ccd":"code","9c2d4feb":"code","0bfa5064":"code","5c648bb7":"code","2afe5b51":"code","c9fd4700":"code","2f7294b4":"code","412e0aac":"code","575f665f":"code","2935132a":"code","76b7c2b4":"code","fcf4e3d3":"markdown","232f1b0d":"markdown","9b9edaff":"markdown","396f27ed":"markdown","7d05771b":"markdown","18f10fc7":"markdown","c68182bc":"markdown","bf1606ae":"markdown","d8b47bf7":"markdown"},"source":{"c4ddd83a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nfrom matplotlib.ticker import MaxNLocator           \n%matplotlib inline \nimport seaborn as sns   \n\nfrom scipy import stats \nfrom scipy.stats import skew, boxcox_normmax, norm\nfrom scipy.special import boxcox1p \n\nimport warnings\nwarnings.filterwarnings('ignore')","705a4b9c":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","9ac05ab4":"train_data.head()","fb59184b":"train_data.dtypes","4ea15081":"# Segregating numerical data\nnumeric_columns = train_data.dtypes[train_data.dtypes != 'object'].index\nlen(numeric_columns)","85ed6ff6":"# Segregating Catagorical Data\ncategorical_columns = train_data.dtypes[train_data.dtypes == 'object'].index\nlen(categorical_columns)","80ebb5a3":"train_data.describe()","ae462fa3":"train_data.shape","3a913aa3":"# Segregating important independent variables for sales price\nimp_columns = ['SalePrice','GrLivArea', 'TotalBsmtSF', 'OverallQual', 'YearBuilt']","913789d3":"# SalePrice\nfor col in imp_columns:\n  print(\"Skewness of \", col,\": \" , train_data[col].skew());\n  print(\"Kurtosis of \",col,\": \" , train_data[col].kurtosis());\n  print(\"---------------------------\")\n  sns.set_style('white');\n  plt.figure();\n  sns.distplot(train_data[col], fit = norm);  \n\n## SalePrice and GrLivArea are positively skewed and not normally distributed, can be transformed using log","aac5b024":"# Histogram to display skewness of every numeric variable\nf = pd.melt(train_data, value_vars = numeric_columns)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=4, sharex=False, sharey=False)\ng.map(lambda _x, **kwargs: sns.distplot(_x, fit = norm), 'value');\n\n## No variable is normally distributed, but LotFrontage, 1stFlrSF, GrLivArea, LotArea can be transformed using log.","f532bf68":"# Null Values and their Percentages\nnull_value_train = pd.DataFrame(train_data.isnull().sum()).reset_index()\nnull_value_train = null_value_train.rename(columns = {'index': 'Column Name', 0: 'Number of Null Values'}, inplace = False)\nnull_value_train['Percentage of Null Values'] = (null_value_train['Number of Null Values']\/len(train_data)*100) \nnull_value_train.sort_values(by = 'Percentage of Null Values', ascending = False).head(20)","01918550":"# Replacing NaN with None in columns\n## NaN in these columns represents No Pool Quality, No Garage Type, etc. \ncolumns_fillnone = ['PoolQC', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n                    'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\n\nfor col in columns_fillnone:\n    train_data[col].fillna('None',inplace=True)","a63a3bab":"# Updated Null Values and their Percentages\nnull_value_train = pd.DataFrame(train_data.isnull().sum()).reset_index()\nnull_value_train = null_value_train.rename(columns = {'index': 'Column Name', 0: 'Number of Null Values'}, inplace = False)\nnull_value_train['Percentage of Null Values'] = (null_value_train['Number of Null Values']\/len(train_data)*100) \nnull_value_train.sort_values(by = 'Percentage of Null Values', ascending = False).head(10)","d512bb25":"# BoxPlot for LotFrontage  \nplt.subplots(figsize=(15,10))\nsns.boxplot( x = 'LotFrontage', y = 'Neighborhood', data = train_data);","c3d8b3b9":"# Houses in similar area will have same Lot Frontage, Masonry veneer area and Masonry type\n## Replacing them with median \n\ntrain_data['LotFrontage'] = train_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\ntrain_data['MasVnrType'] = train_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\ntrain_data['MasVnrArea'] = train_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","67edb6f8":"# Updated Null Values and their Percentages\nnull_value_train = pd.DataFrame(train_data.isnull().sum()).reset_index()\nnull_value_train = null_value_train.rename(columns = {'index': 'Column Name', 0: 'Number of Null Values'}, inplace = False)\nnull_value_train['Percentage of Null Values'] = (null_value_train['Number of Null Values']\/len(train_data)*100) \nnull_value_train.sort_values(by = 'Percentage of Null Values', ascending = False).head(7)","4e3e3c71":"# Using Mode to replace missing values for catagorical data such as Electrical System\ntrain_data['Electrical'] = train_data['Electrical'].fillna(train_data['Electrical'].mode()[0])","96eae8eb":"# Replcing the rest with None\ncolumns_fillnone = ['MiscFeature', 'Alley', 'Fence', 'FireplaceQu']\n\nfor col in columns_fillnone:\n    train_data[col].fillna('None',inplace=True)","3c14c5b7":"# Updated Null Values and their Percentages\nnull_value_train = pd.DataFrame(train_data.isnull().sum()).reset_index()\nnull_value_train = null_value_train.rename(columns = {'index': 'Column Name', 0: 'Number of Null Values'}, inplace = False)\nnull_value_train['Percentage of Null Values'] = (null_value_train['Number of Null Values']\/len(train_data)*100) \nnull_value_train.sort_values(by = 'Percentage of Null Values', ascending = False).head(3)","4ab5e8fa":"# Calculating Skewness and Kurtosis for numerical columns\nfor col in numeric_columns:\n    print('{:15}'.format(col), \n          'Skewness: {:05.2f}'.format(train_data[col].skew()) , \n          '   ' ,\n          'Kurtosis: {:06.2f}'.format(train_data[col].kurt())  \n         )","64aeeeea":"# Performing log transformation on some positively skewed features\nfor df in [train_data]:\n  df['SalePrice_Log'] = np.log(df['SalePrice'])\n  df.drop('SalePrice', inplace= True, axis = 1)\n  df['GrLivArea_Log'] = np.log(df['GrLivArea'])\n  df.drop('GrLivArea', inplace= True, axis = 1)\n  df['LotArea_Log'] = np.log(df['LotArea'])\n  df.drop('LotArea', inplace= True, axis = 1)","96fc2560":"log_trans_columns = ['SalePrice_Log', 'GrLivArea_Log', 'LotArea_Log']\nfor col in log_trans_columns:\n  print(\"Skewness of \", col,\": \" , train_data[col].skew());\n  print(\"Kurtosis of \",col,\": \" , train_data[col].kurtosis());\n  print(\"---------------------------\")\n  sns.set_style('white');\n  plt.figure();\n  sns.distplot(train_data[col], fit = norm); ","dc945a1e":"# Correlation Matrix 1\ncorrmat = train_data.corr()\nk = 10 \ncols = corrmat.nlargest(k, 'SalePrice_Log')['SalePrice_Log'].index\ncm = np.corrcoef(train_data[cols].values.T)\nf, ax = plt.subplots(figsize=(10, 10))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', yticklabels=cols.values, xticklabels=cols.values, cmap = 'Blues')\nplt.show()","c5ad3af2":"imp_columns_corrmat1 = ['SalePrice_Log', 'OverallQual', 'GrLivArea_Log', 'GarageCars', 'TotalBsmtSF','FullBath', 'YearBuilt', 'YearRemodAdd']\n\n# Converting YearBuilt and YearRemodAdd to str \ntrain_data['YearBuilt'] = train_data['YearBuilt'].astype('str')\ntrain_data['YearBuilt'] = train_data['YearRemodAdd'].astype('str') ","2c6a4078":"# Updating numeric_columns\nnumeric_columns = train_data.dtypes[train_data.dtypes != 'object'].index\nnumeric_columns","566a7ac7":"# Regression Plots for Numeric Features\ndef srt_reg(y, df):\n    fig, axes = plt.subplots(12, 3, figsize=(25, 80))\n    axes = axes.flatten()\n    for i, j in zip(train_data[[col for col in numeric_columns]], axes):\n\n        sns.regplot(x=i, y=y, data=df, ax=j, order=3, ci=None, color='#e74c3c', \n                    line_kws={'color': 'black'}, scatter_kws={'alpha':0.4})\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=10))\n        plt.tight_layout()\n\nsrt_reg('SalePrice_Log', train_data)","108f8840":"# Correlational Analysis for numerical features \ntrain_data_num = pd.DataFrame(train_data[[col for col in numeric_columns]])\ncorrmat = train_data_num.corr()\nf, ax = plt.subplots(figsize=(30, 25))\nsns.heatmap(corrmat, vmax=.8, square=True, annot = True, cmap = 'Blues');","e9cd2c04":"## Some numerical features such as OverAllQual, GrLivArea_Log, TotalRmsAbvGrd, etc. are highly correlated to SalePrice_Log.\n## Other numerical features such as Id, LotArea, OverallCond, etc. are weakly correlated to the target variable, therefore, can be dropped.\n\n## Observations for numerical features:  \n# OverallQual: sale price of the house increases with overall quality.\n# OverallCondition: Most of the houses are in 5\/10 condition. Does not have much effect on SalePrice\n# YearBuilt: Again new buildings are generally expensive than the old ones.\n# Basement: Bigger basements are increasing the price. \n# GrLivArea: This feature is linear but two outliers can be spotted.\n# SaleDates: They seem to have no effect on sale prices.","16aa2b41":"# Box Plot Analysis for Categorical Variables\ndef srt_box(y, df):\n    fig, axes = plt.subplots(14, 3, figsize=(25, 80))\n    axes = axes.flatten()\n\n    for i, j in zip(train_data[[col for col in categorical_columns]], axes):\n\n        sortd = df.groupby([i])[y].median().sort_values(ascending=False)\n        sns.boxplot(x=i,\n                    y=y,\n                    data=df,\n                    palette='plasma',\n                    order=sortd.index,\n                    ax=j)\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=18))\n\n        plt.tight_layout()\n\nsrt_box('SalePrice_Log', train_data)","d9e10c21":"## Observations for categorical features: \n\n# MSZoning: #Floating village houses, has the highest median value.\n            #Residental low density houses comes second with the some outliers. \n            #Residental high and low seems similar meanwhile commercial is the lowest.\n\n# LandContour: Hillside houses seems a little bit higher expensive than the rest meanwhile banked houses are the lowest.\n\n# Neighborhood: #Northridge Heights, Northridge and Timberland are top 3 expensive places for houses.\n                #Somerset, Veenker, Crawford, Clear Creek, College Creek and Bloomington Heights seems above average.\n                #Sawyer West has wide range for prices related to similar priced regions.\n                #Old Town and Edwards has some outlier prices but they generally below average.\n                #Briardale, Iowa DOT and Rail Road, Meadow Village are the cheapest places for houses it seems\n\n# Conditions: #Meanwhile having wide range of values being close to North-South Railroad seems having positive effect on the price.\n              #Being near or adjacent to positive off-site feature (park, greenbelt, etc.) increases the price.\n\n# MasVnrType: Having stone masonry veneer seems better priced than having brick.\n\n# CentralAir: Having central air system has decent positive effect on sale prices.\n\n# GarageType: #Built-In garage typed houses are the most expensive ones.\n              #Attached garage types follow the built-in ones in prices.\n              #Car ports are the lowest\n","4a89c014":"# Converting some important Categorical Features to Numerical Features\n\nneigh_map = {\n    'MeadowV': 1,\n    'IDOTRR': 1,\n    'BrDale': 1,\n    'BrkSide': 2,\n    'OldTown': 2,\n    'Edwards': 2,\n    'Sawyer': 3,\n    'Blueste': 3,\n    'SWISU': 3,\n    'NPkVill': 3,\n    'NAmes': 3,\n    'Mitchel': 4,\n    'SawyerW': 5,\n    'NWAmes': 5,\n    'Gilbert': 5,\n    'Blmngtn': 5,\n    'CollgCr': 5,\n    'ClearCr': 6,\n    'Crawfor': 6,\n    'Veenker': 7,\n    'Somerst': 7,\n    'Timber': 8,\n    'StoneBr': 9,\n    'NridgHt': 10,\n    'NoRidge': 10\n}\ntrain_data['Neighborhood'] = train_data['Neighborhood'].map(neigh_map).astype('int')\next_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\ntrain_data['ExterQual'] = train_data['ExterQual'].map(ext_map).astype('int')\ntrain_data['ExterCond'] = train_data['ExterCond'].map(ext_map).astype('int')\nbsm_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\ntrain_data['BsmtQual'] = train_data['BsmtQual'].map(bsm_map).astype('int')\ntrain_data['BsmtCond'] = train_data['BsmtCond'].map(bsm_map).astype('int')\nbsmf_map = {\n    'None': 0,\n    'Unf': 1,\n    'LwQ': 2,\n    'Rec': 3,\n    'BLQ': 4,\n    'ALQ': 5,\n    'GLQ': 6\n}\ntrain_data['BsmtFinType1'] = train_data['BsmtFinType1'].map(bsmf_map).astype('int')\ntrain_data['BsmtFinType2'] = train_data['BsmtFinType2'].map(bsmf_map).astype('int')\nheat_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\ntrain_data['HeatingQC'] = train_data['HeatingQC'].map(heat_map).astype('int')\ntrain_data['KitchenQual'] = train_data['KitchenQual'].map(heat_map).astype('int')\ntrain_data['FireplaceQu'] = train_data['FireplaceQu'].map(bsm_map).astype('int')\ntrain_data['GarageCond'] = train_data['GarageCond'].map(bsm_map).astype('int')\ntrain_data['GarageQual'] = train_data['GarageQual'].map(bsm_map).astype('int')","cbb7a183":"# Updating numerical and categorical feature columns\nnumeric_columns = train_data.dtypes[train_data.dtypes != 'object'].index\ncategorical_columns = train_data.dtypes[train_data.dtypes == 'object'].index","e18e045e":"# Correlation Matrix 2\ncorrmat = train_data.corr()\nk = 10 \ncols = corrmat.nlargest(k, 'SalePrice_Log')['SalePrice_Log'].index\ncm = np.corrcoef(train_data[cols].values.T)\nf, ax = plt.subplots(figsize=(10, 10))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', yticklabels=cols.values, xticklabels=cols.values, cmap = 'Blues')\nplt.show()","8b5311f5":"# Updating the list of highly correlated features\nimp_columns_corrmat2 = ['SalePrice_Log', 'OverallQual', 'Neighborhood', 'GrLivArea_Log', 'GarageCars', 'ExterQual','KitchenQual','BsmtQual','TotalBsmtSF']","78ab63a7":"# Detecting outliers and percentages using Extreme Value Analysis\ndef quantile_info(qu_dataset, qu_field):\n  \n    iqr = qu_dataset[qu_field].quantile(0.75) - qu_dataset[qu_field].quantile(0.25)\n    print(\"Inter-Quartile Range:\", iqr)\n    \n    upper_boundary = qu_dataset[qu_field].quantile(0.75) + (iqr * 1.5)\n    lower_boundary = qu_dataset[qu_field].quantile(0.25) - (iqr * 1.5)\n    print(\"Upper Boundary:\", upper_boundary)\n    print(\"Lower Boundary:\", lower_boundary)\n    \n    upper_boundary_extreme = qu_dataset[qu_field].quantile(0.75) + (iqr * 3)\n    lower_boundary_extreme = qu_dataset[qu_field].quantile(0.25) - (iqr * 3)\n    print(\"Upper Extreme Boundary:\", upper_boundary_extreme)\n    print(\"Lower Extreme Boundary:\", lower_boundary_extreme)\n\n    count_over_upper = len(qu_dataset[qu_dataset[qu_field] > upper_boundary])\n    count_under_lower = len(qu_dataset[qu_dataset[qu_field] < lower_boundary])\n    percentage = 100 * (count_under_lower + count_over_upper) \/ len(qu_dataset[qu_field])\n    print(\"Percentage of records out of Upper and Lower Boundaries: %.2f\"% (percentage))\n    \n    count_over_upper = len(qu_dataset[qu_dataset[qu_field]>upper_boundary_extreme])\n    count_under_lower = len(qu_dataset[qu_dataset[qu_field]<lower_boundary_extreme])\n    percentage = 100 * (count_under_lower + count_over_upper) \/ len(qu_dataset[qu_field])\n    print(\"Percentage of records out of Upper and Lower Extreme Boundaries: %.2f\"% (percentage))\n\nfor col in imp_columns_corrmat2:\n  print(\"Outlier Detection for \", col, \":\")\n  quantile_info(train_data, col);\n  print(\"---\"*10)","993e4fb6":"# Removing Outliers that lie outside Upper and Lower Boundaries\n\ndef remove_outliers_quantiles(qu_dataset, qu_field, qu_fence):\n  iqr = qu_dataset[qu_field].quantile(0.75) - qu_dataset[qu_field].quantile(0.25)\n  upper_boundary = qu_dataset[qu_field].quantile(0.75) + (iqr * 1.5)\n  lower_boundary = qu_dataset[qu_field].quantile(0.25) - (iqr * 1.5)\n  upper_boundary_extreme = qu_dataset[qu_field].quantile(0.75) + (iqr * 3)\n  lower_boundary_extreme = qu_dataset[qu_field].quantile(0.25) - (iqr * 3)\n\n  if qu_fence == \"inner\":\n        output_dataset = qu_dataset[qu_dataset[qu_field] <= upper_boundary]\n        output_dataset = output_dataset[output_dataset[qu_field] >= lower_boundary]\n      \n  elif qu_fence == \"extreme\":\n        output_dataset = qu_dataset[qu_dataset[qu_field]<=upper_boundary_extreme]\n        output_dataset = output_dataset[output_dataset[qu_field] >= lower_boundary_extreme]\n\n  else:\n        output_dataset = qu_dataset\n  \n  return output_dataset\n\ntrain_data_new = remove_outliers_quantiles(train_data, 'SalePrice_Log', 'inner')\ntrain_data_new = remove_outliers_quantiles(train_data_new, 'OverallQual', 'inner')\ntrain_data_new = remove_outliers_quantiles(train_data_new, 'Neighborhood', 'inner')\ntrain_data_new = remove_outliers_quantiles(train_data_new, 'TotalBsmtSF', 'extreme')","99d81ec5":"train_data_new.shape","bbf3d660":"# Updating Numerical and Categorical Features\nnumeric_columns = train_data.dtypes[train_data.dtypes != 'object'].index\ncategorical_columns = train_data.dtypes[train_data.dtypes == 'object'].index","dc0aef6c":"imp_columns_corrmat3 = list(cols)\nimp_columns_corrmat3","0640b34d":"# Creating new features by combining some features\n\ntrain_data['TotalSF'] = (train_data['BsmtFinSF1'] + train_data['BsmtFinSF2'] + \n                       train_data['1stFlrSF'] + train_data['2ndFlrSF'])\n\ntrain_data['TotalBathrooms'] = (train_data['FullBath'] +\n                              (0.5 * train_data['HalfBath']) +\n                              train_data['BsmtFullBath'] +\n                              (0.5 * train_data['BsmtHalfBath']))\n\ntrain_data['TotalPorchSF'] = (train_data['OpenPorchSF'] + train_data['3SsnPorch'] +\n                            train_data['EnclosedPorch'] +\n                            train_data['ScreenPorch'] + train_data['WoodDeckSF'])","ed0397d7":"# Merging Quality and Condition\ntrain_data['TotalExtQual'] = (train_data['ExterQual'] + train_data['ExterCond'])\n\ntrain_data['TotalBsmQual'] = (train_data['BsmtQual'] + train_data['BsmtCond'] +\n                            train_data['BsmtFinType1'] + train_data['BsmtFinType2'])\n\ntrain_data['TotalGrgQual'] = (train_data['GarageQual'] + train_data['GarageCond'])","b7e33ccd":"train_data['HasPool'] = train_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\ntrain_data['Has2ndFloor'] = train_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n\ntrain_data['HasFireplace'] = train_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","9c2d4feb":"# Dropping Features \n\ndrop_columns = ['BsmtFinSF1', 'BsmtFinSF2', '1stFlrSF', '2ndFlrSF', 'FullBath', \n                'HalfBath', 'BsmtFullBath','BsmtHalfBath', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n                'ScreenPorch', 'WoodDeckSF', 'ExterQual', 'ExterCond', 'BsmtQual',\n                'BsmtFinType1', 'BsmtFinType2', 'BsmtCond', 'GarageQual', \n                'GarageCond','GarageArea', 'PoolArea','Fireplaces']","0bfa5064":"train_data.drop(columns = drop_columns, inplace=True)","5c648bb7":"# Correlation Matrix 3\ncorrmat = train_data.corr()\nk = 10\ncols = corrmat.nlargest(k, 'SalePrice_Log')['SalePrice_Log'].index\ncm = np.corrcoef(train_data[cols].values.T)\nf, ax = plt.subplots(figsize=(10, 10))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', yticklabels=cols.values, xticklabels=cols.values, cmap = 'Blues')\nplt.show()","2afe5b51":"# Updating numerical and categorical feature columns\nnumeric_columns = train_data.dtypes[train_data.dtypes != 'object'].index\ncategorical_columns = train_data.dtypes[train_data.dtypes == 'object'].index","c9fd4700":"# Sorting data according to skewness\nskew_data = np.abs(train_data[numeric_columns].apply(lambda x: skew(x)).sort_values(ascending=False))","2f7294b4":"# Segregating highly positively skewed data \nhigh_skew = skew_data[skew_data > 0.4]\nskew_index = high_skew.index","412e0aac":"# Applying BoxCox Transformation for highly skewed features\nfor i in skew_index:\n    train_data[i] = boxcox1p(train_data[i], boxcox_normmax(train_data[i] + 1))","575f665f":"# Correlation Matrix 4\ncorrmat = train_data.corr()\nk = 10\ncols = corrmat.nlargest(k, 'SalePrice_Log')['SalePrice_Log'].index\ncm = np.corrcoef(train_data[cols].values.T)\nf, ax = plt.subplots(figsize=(10, 10))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', yticklabels=cols.values, xticklabels=cols.values, cmap = 'Blues')\nplt.show()","2935132a":"train_data.shape","76b7c2b4":"correlation = train_data.corrwith(train_data['SalePrice_Log'])\ncorrelation ['Abs Corr'] = correlation.abs()\nsorted_correlations = correlation['Abs Corr'].sort_values(ascending=False)\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(sorted_correlations.to_frame()[sorted_correlations>=.5], cmap='coolwarm', annot=True, vmin=-1, vmax=1, ax=ax);","fcf4e3d3":"# Handling Outliers","232f1b0d":"# Feature Transformation","9b9edaff":"# Feature Creation\n","396f27ed":"# Bi-Variate Analysis\n","7d05771b":"# Variable Identification and Descriptive Statistics\n","18f10fc7":"# Handling Missing Values\n\n","c68182bc":"# Log Transformation\n\n","bf1606ae":"# Univariate Analysis\n","d8b47bf7":"# Categorical to Numerical Conversion"}}