{"cell_type":{"65671d11":"code","ead4a944":"code","e7ba40cf":"code","d27b3d2e":"code","21ecef5c":"code","2da5cc13":"code","f90059e8":"code","e292eb65":"code","bfa7cfe5":"code","a1cb7351":"code","b0a58453":"code","94f84453":"code","6cd7d6e7":"code","e5110c04":"code","7d401f04":"code","f76aedd4":"code","de41f1b5":"code","94f9b64c":"code","c3e7bbbe":"code","23a16b95":"code","27999a6f":"code","cc856b12":"code","f5299fea":"code","05445d93":"code","5d4fa855":"code","0f57f9ea":"code","62f355f6":"code","e981bff8":"code","1653c881":"code","d34425b6":"code","0c088853":"code","781bfbd8":"code","defbf600":"code","2fbde40e":"code","0a0f3477":"code","feacd94f":"code","eb6a1ef8":"code","4e5cbe0e":"code","9a9c2c51":"code","2146fa3a":"code","982ee9e8":"code","a39afb83":"code","a6f00511":"code","6632040f":"code","41ced0fe":"code","e6ab099d":"code","dcca9459":"code","b58b4c67":"code","3092ba22":"code","cdfce7d6":"code","d22f6beb":"code","806279f2":"code","bfe2a29a":"code","b4e57cfd":"code","3cc90975":"code","89734313":"code","164eef73":"code","a8355f34":"code","4acfcb2c":"markdown","3f6b82c3":"markdown","d7920742":"markdown","02b0f03a":"markdown","dfb9042e":"markdown","99af5bd3":"markdown","f40f2af0":"markdown","da1c5ba2":"markdown","00fd1dd7":"markdown","0a8f920f":"markdown","eff7ebb7":"markdown","38216567":"markdown","d01df9c4":"markdown","64587447":"markdown","a756f5fb":"markdown","ef102603":"markdown","6fa1e4eb":"markdown","cf4626dc":"markdown","64ff1f7e":"markdown","9e0536d1":"markdown","963491b1":"markdown","bc8930d5":"markdown","2b45e4fd":"markdown","ad08f6e2":"markdown","176b0a1b":"markdown"},"source":{"65671d11":"# import all the tools we need\n\n# regular EDA and plotting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# modles of sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n#  model evaluation\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV , GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve \n","ead4a944":"df = pd.read_csv('heart-disease.csv')\ndf.head()","e7ba40cf":"# lets calc. how many of each classes there are\ndf['target'].value_counts()","d27b3d2e":"df['target'].value_counts().plot(kind = 'bar', color = ['salmon', 'lightblue']);","21ecef5c":"df.info()","2da5cc13":"df.isna().sum()","f90059e8":"df.sex.value_counts()","e292eb65":"# compare target column with sex column\npd.crosstab(df.target, df.sex)","bfa7cfe5":"# create a plot of crosstab\npd.crosstab(df.target, df.sex).plot(kind = 'bar', \n                                    figsize = (10, 6), \n                                    color =['salmon', 'lightblue'])\nplt.title('heart disease frequency for sex')\nplt.xlabel('0 = no disease , 1 = disease')\nplt.ylabel('amount')\nplt.legend(['feamale', 'male'])\nplt.xticks(rotation =0)","a1cb7351":"# creating an another figure\nplt.figure(figsize =(10, 6))\n\n# scatter with positive cases\nplt.scatter(df.age[df.target ==1],\n            df.thalach[df.target == 1],\n            c = 'salmon')\n\n# scatter for negative cases\nplt.scatter(df.age[df.target ==0],\n            df.thalach[df.target == 0],\n            c = 'blue')\n\n\nplt.title('age vs. max heart rate for heart disease')\nplt.xlabel('age')\nplt.ylabel('max heart rate')\nplt.legend(['disease', 'not disease'])","b0a58453":"# check distribution of age with histagram\ndf.age.plot.hist()","94f84453":"df.cp.value_counts()","6cd7d6e7":"pd.crosstab(df.target, df.cp)","e5110c04":"pd.crosstab(df.cp, df.target).plot(kind = 'bar',\n                                   figsize=(10,6),\n                                   color = ['salmon', 'pink'])\n\nplt.title(\"heart disease frequency per chest pain type\")\nplt.xlabel('chest pain')\nplt.ylabel('amount')\nplt.legend(['not disease', 'disease'])","7d401f04":"df.head()","f76aedd4":"df.corr()","de41f1b5":"corr_matrix = df.corr()\nfig, ax = plt.subplots(figsize=(15, 10))\nax = sns.heatmap(corr_matrix,\n                 annot = True,\n                 linewidths= 0.5,\n                 fmt = '.2f',\n                 cmap = 'YlGnBu')","94f9b64c":"df.head()","c3e7bbbe":"# split data into x and y\nx = df.drop('target', axis =1)\ny = df.target\n\nx.head()","23a16b95":"y.head()","27999a6f":"# split data into train and test\nnp.random.seed(42)\n\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.2)\n\n","cc856b12":"# put models in a dictionary\nclf = { 'logistic regressor' : LogisticRegression(),\n         'k-n neighbour' : KNeighborsClassifier(),\n         'random forest' : RandomForestClassifier()}\n\n# create a function to fit and score models\ndef fit_and_score(clf, xtrain, xtest, ytrain, ytest):\n    \"\"\"\n    fits and evaluates machine learning models\n    \"\"\"\n    # set random seed\n    \n    np.random.seed(42)\n    # make dictionary to store model scores\n    model_scores = {}\n    # loop through models\n    for name, model in clf.items():\n        # fit the model to the data\n        model.fit(xtrain, ytrain)\n        #evaluate the model and append it's score into model_scores\n        model_scores[name] = model.score(xtest, ytest)\n    return model_scores\n","f5299fea":"model_scores = fit_and_score(clf = clf,\n                             xtrain = xtrain,\n                             xtest = xtest, \n                             ytrain = ytrain, ytest=ytest)\nmodel_scores","05445d93":"model_compare = pd.DataFrame(model_scores, index =['accuracy'])\nmodel_compare.T.plot.bar();","5d4fa855":"# let's tune KNN\n\ntrain_scores = []\ntest_scores = []\n\n# create a list of different values of n_neighbours\nneighbours = range(1, 21)\n\n# set up KNN instance\nKNN = KNeighborsClassifier()\n\n# loop through different n_neighbours\nfor i in neighbours:\n    KNN.set_params(n_neighbors = i)\n\n    # fit the algorithom\n    KNN.fit(xtrain, ytrain)\n    \n    # update the training score list\n    train_scores.append(KNN.score(xtrain, ytrain))\n    \n    # update the test score list\n    test_scores.append(KNN.score(xtest, ytest))","0f57f9ea":"train_scores","62f355f6":"test_scores","e981bff8":"plt.plot(neighbours, train_scores, test_scores)\nplt.xlabel('no. of neighbours')\nplt.ylabel('model score')\nplt.xticks(np.arange(1, 21, 1))\nplt.legend(['train scores', 'test scores'])\n\nprint(f'max knn score on the test data : {max(test_scores) * 100 :.2f}%')","1653c881":"# create a hyperperameter grid for logistic regression\nlr_grid = {'C' : np.logspace(-4, 4, 20),\n            'solver': ['liblinear']}\n\n# create a hyperperameter grid for random forest\nrf_grid = {'n_estimators' : np.arange(10, 1000, 50),\n           'max_depth': [None, 3, 5, 10],\n           'min_samples_split' : np.arange(2, 20, 2),\n           'min_samples_leaf' : np.arange(1, 20, 2)}","d34425b6":"# Tune LogisticRegression\n\nnp.random.seed(42)\n\n#set up random hyperperameter search for LogisticResgression\nrs_lr = RandomizedSearchCV(LogisticRegression(),\n                           param_distributions= lr_grid,\n                           cv = 5, \n                            n_iter= 20,\n                            verbose=True)\n\n#fit random hyperperameter search for LogisticRegression\nrs_lr.fit(xtrain, ytrain)\n","0c088853":"rs_lr.best_params_","781bfbd8":"rs_lr.score(xtest, ytest)","defbf600":"# tune random forest classifier\nnp.random.seed(42)\n\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv = 5, \n                           n_iter = 20, \n                           verbose=True)\n\nrs_rf.fit(xtrain, ytrain)","2fbde40e":"rs_rf.score(xtest, ytest)","0a0f3477":"# creating grid for LogisticResgession Model\nlr_grid= {'C' : np.logspace(-4, 4, 30),\n          'solver' :['liblinear']}\n\n# setup grid hyperperameter search for logistic regressor\nlr_gs = GridSearchCV(LogisticRegression(),\n                     param_grid= lr_grid,\n                     cv = 5, \n                     verbose=True)\n\nlr_gs.fit(xtrain, ytrain)","feacd94f":"lr_gs.best_params_","eb6a1ef8":"lr_gs.score(xtest, ytest)","4e5cbe0e":"# let's make predictions with tuned model\ny_preds = lr_gs.predict(xtest)","9a9c2c51":"y_preds","2146fa3a":"ytest","982ee9e8":"# plot rOC curve and calculate AUC metrics\nplot_roc_curve(lr_gs, xtest, ytest)","a39afb83":"# confusion metrics\nprint(confusion_matrix(y_preds, ytest))","a6f00511":"sns.set(font_scale = 1.5)\n\ndef plot_conf_metrics(ytest, y_preds):\n    \"\"\"\n    this function plots confusion matrics using seaborn's heatmap()\n    \"\"\"\n    fig, ax = plt.subplots(figsize = (3,3))\n    ax = sns.heatmap(confusion_matrix(ytest, y_preds),\n                     annot = True,\n                     cbar = False)\n    plt.xlabel('true labels')\n    plt.ylabel('predicted labels')\n    \nplot_conf_metrics(ytest, y_preds)\n    \n    ","6632040f":"print(classification_report(ytest, y_preds))","41ced0fe":"# check best parameters\nlr_gs.best_params_","e6ab099d":"# create new classifier with best params\nclf = LogisticRegression(C = 0.20433597178569418, solver = 'liblinear')","dcca9459":"# cross validated accuracy\ncv_acc = cross_val_score(clf, x, y, cv = 5, scoring ='accuracy')\ncv_acc = np.mean(cv_acc)\ncv_acc","b58b4c67":"# cross validated precision\ncv_pre = cross_val_score(clf, x, y, scoring = 'precision')\ncv_pre = np.mean(cv_pre)\ncv_pre","3092ba22":"# cross validated recall\ncv_rec = cross_val_score(clf, x, y, scoring = 'recall')\ncv_rec = np.mean(cv_rec)\ncv_rec","cdfce7d6":"# cross validated f1 score\ncv_f1 = cross_val_score(clf, x, y, scoring = 'f1')\ncv_f1 = np.mean(cv_f1)\ncv_f1","d22f6beb":"# visualise cross validated matrics\ncv_matrics = pd.DataFrame({'accuracy' : cv_acc,\n              'precision': cv_pre,\n              'recall': cv_rec,\n               'f1 score': cv_f1},\n                         index = [0])\n\ncv_matrics.T.plot.bar(title ='cross validated matrics',\n                      legend = False);","806279f2":"# fitting\nclf.fit(xtrain, ytrain)","bfe2a29a":"# check coef_\nclf.coef_","b4e57cfd":"# match coef's of features to columns\nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","3cc90975":"# visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index =[0])\nfeature_df.T.plot.bar(title = 'feature importance', legend = False)\n","89734313":"pd.crosstab(df.sex, df.target) ","164eef73":"pd.crosstab(df.slope, df.target)","a8355f34":"import pickle\n# save model to disk\nfilename = 'heart-disease-final-model.sav'\npickle.dump(clf, open(filename, 'wb'))\n\n# load the model from disk\nloaded_model = pickle.load(open(filename, 'rb'))\nresult = loaded_model.score(xtest, ytest)\nprint(result)","4acfcb2c":"# 5. modelling","3f6b82c3":"since logistic regression model works best we will try and improve it with gridSearchCV","d7920742":"# Predicting heart disease using machine learning\n\nThis notebook looks into various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting weather or not someone has heart disease based on there medical attributes.","02b0f03a":"# make a corelation matrix","dfb9042e":"it have a positive corelation as slope value increases target value also increases","99af5bd3":"# Hyper tuning with GridSearchCV","f40f2af0":"now we have got hyperperameters grid setup let's tune our models by using randomizedsearchcv","da1c5ba2":" it have a negative coprelation therefore as sex value increases the target vaue decreases","00fd1dd7":"# Heart disease frequency according to sex","0a8f920f":"# Preparing the tools\n\nwe are going to use pandas, numpy, matplotlib for data analysis and manipulation","eff7ebb7":"we're going to calc accuracy,  precision, recall, f1 using cross-validation and to do this we are going to use cross_val_score","38216567":"here if value is +ve i.e having positive corellation : value of 1 variable increases then other variable's value also insrease\nfor ex cp vs target as value of cp increases for target = 1 (consider previous graph)\nnd if value is -ve i.e negative corellation : value of 1 variable increases then value of other variable dicreses\nfor ex exang vs target as value of exand inreases i.e is 1 then target value dicreases i.e is 0 ","d01df9c4":" let's get a classification report as well as cross-validated precission, recall and f1 score","64587447":"# 6. Experimation\n\nIf you haven't hit ur evaluation metrics yet... ask yourself..\n\n* could u collect more data?\n* couls u try a better model? \n* could u improve the current model(beyond what we've done so far)\n* if ur model is gud enough        (you have hit ur evaluation metrics) how would you export and shear with other?\n\n","a756f5fb":"# age vs. max heart rate for heart disease","ef102603":"## Feature importance\nfeature importance is another as asking , which features 'contibuted most to the outcome of the model and how did they congtibute?'\n\nfinding feature importance is different for each machine learning model","6fa1e4eb":"# load data","cf4626dc":"we are going to take following approch:\n1. problem defination\n2. data \n3. evaluation\n4. features\n5. modelling\n6. exparimantation\n\n# problem\nin a statement, \n8> given clinical parameters about a patient , can we predict weather he have heart disease or not\n\n# data\n> originol data came from the cleaveland data from UCI machine learning repository\n\n# evaluation\n> if we can reach 95% accuracy at predicting weather or not a patient has heart disease during a proof of concept , we'll persue the project\n\n# features\nto get info abpout each feature of ur data\n\n\n**Create data dictionary**\n\n* age: age in years\n* sex: sex (1 = male; 0 = female)\n* cp: chest pain type\n   1: typical angina\n   2: atypical angina\n   3: non-anginal pain\n   4: asymptomatic\n* trestbps: resting blood pressure (in mm Hg on admission to the hospital) above 130 - 140 is a typical cause of concern\n* chol: serum cholestoral in mg\/dl\n   1. serum = LDL + HDL + .2* triglycerides\n   2. above 200 is cause for concern\n*  fbs: (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n   1. 126 mg\/dL signals diabetes\n* restecg: resting electrocardiographic results\n   0: normal\n   1: having ST-T wave abnormality \n     * can range from mild symptoms to severe problems\n     * signals non-normal heart beat\n    \n   2: showing probable or definite left ventricular hypertrophy \n*  thalach: maximum heart rate achieved\n*  exang: exercise induced angina (1 = yes; 0 = no)\n* oldpeak = ST depression induced by exercise relative to rest\n* slope: the slope of the peak exercise ST segment\n    1: upsloping : better heart rate exercise ST segment\n    2: flat : minimal change (typical healthy heart)\n    3: downsloping : signs of unhealthy heart \n* ca: number of major vessels (0-3) colored by flourosopy\n   1. colored vessels means the dr. can see the blood passing through\n   2. the more blood movement the better \n*  thal: thalium stress rate\n   1. 3 = normal; \n   2. 6 = fixed defect; \n   3. 7 = reversable defect : no proper blood movement when exercising\n*  target - have heart disease or not (1= yea, 0= no )","64ff1f7e":"now we have got our data split ito train and test data, now it's time to build a machine learning model\n\nwe'll train it (find patterns) on train set\n\nwe'll test it (using patterns) on test set\n\nWe are going to try 3 different machine learning models:\n1. logistic regression\n2. k-nearest neighbour classifier\n3. random forest classifier","9e0536d1":"## data exploration (exploratory data analysis or EDA)\n\nthe goal here is to find more about the data and become a subject matter export on the\n\n1. what questions r u trying to solve\n2. what kind of data do we have and how do we treat different types\n3. whats missing data and do u deal with it\n4. where are the outliers and why sould u care about them\n5. how can uh add , change or remove to get more out of ur data","963491b1":"# heart disease frequency per chest pain type\n\ncp: chest pain type \n1. typical angina : cp due to decrease blood supply to the heart\n2. atypical angina : cp not related to heart\n3. non-anginal pain : typically esthophageal spasms\n4. asymptomatic : cp not showing signs of disease","bc8930d5":"Now we have got a baseline model.... and we know a model's first predictions aren't always what we should base our next steps off. what should we do?\n\nnow let's look at the following\n* Hyperperameter tuning\n* feature importance\n* confusion matrix\n* cross-validation\n* precision\n* recall\n* f1 score\n* classification report\n* ROC curve\n* area under the curve (AUC)\n\n## hyperperameter tuning (by hands)","2b45e4fd":"**Save model using Pickel**","ad08f6e2":"# hyperperameter tuning with RandomizedSearchCV\n\nwe're going to tune:\n* LogisticRegression\n* randomforest","176b0a1b":"## Evaluating Our tuned machine learning model , beyond accuracy\n\n* roc curve and area under the curve \n* confusion matrix \n* classification report\n* precision\n* recall\n* f1 score\n\n... and it would be great if cross - validation is used where possible\n\nto make comparisions and evaluate our trained model, first we need to make predictions"}}