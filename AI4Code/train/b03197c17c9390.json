{"cell_type":{"c34a8f93":"code","01747797":"code","08aa99fb":"code","a79115da":"code","7c4a0125":"code","43c3a3c6":"code","b621c1a0":"code","81a6e737":"code","a0ec0b0a":"code","ba52985c":"code","bcfe856b":"code","61b5ed2f":"code","405b8277":"code","d12baa71":"code","c27b719a":"code","3c345c69":"code","bbca8c28":"code","70e29181":"code","503dcab3":"code","be32ff51":"code","5bdf16be":"code","f8b00c9a":"code","23e693cc":"code","017262db":"code","486588b7":"code","f7193fca":"code","35cdb9b4":"code","cc26a569":"code","b86c9edc":"code","1c9d6e57":"code","2bf9fbdc":"code","39ab3f8b":"code","9f90793a":"code","3797262f":"code","13c7d9c3":"code","e187b09c":"code","a711ebc3":"markdown","baf99f35":"markdown","11fad74d":"markdown","849c6004":"markdown","fe5b3f63":"markdown","d2f7c7a6":"markdown","af5047fc":"markdown","fe48a3e2":"markdown"},"source":{"c34a8f93":"%config Completer.use_jedi = False","01747797":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport featuretools as ft\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        pass","08aa99fb":"all_train_data = pd.read_csv('\/kaggle\/input\/danthon2021\/data_train.csv')\nall_train_data","a79115da":"all_test_data = pd.read_csv('\/kaggle\/input\/danthon2021\/data_test.csv')\nall_test_data","7c4a0125":"from sklearn.preprocessing import LabelEncoder\nencoder=LabelEncoder()\nall_train_data['Labels']=encoder.fit_transform(all_train_data['Labels'])","43c3a3c6":"all_train_data[['s2token_15','pub_date','hour']] = all_train_data['Ids'].str.split('_',expand=True)","b621c1a0":"all_test_data[['s2token_15','pub_date','hour']] = all_test_data['Ids'].str.split('_',expand=True)","81a6e737":"all_train_data","a0ec0b0a":"all_test_data","ba52985c":"all_train_data['pub_date'] = pd.to_datetime(all_train_data['pub_date'], errors='coerce')\nall_train_data['day'] = all_train_data['pub_date'].dt.day_name() ","bcfe856b":"all_test_data['pub_date'] = pd.to_datetime(all_test_data['pub_date'], errors='coerce')\nall_test_data['day'] = all_test_data['pub_date'].dt.day_name() ","61b5ed2f":"!pip install s2cell\nimport s2cell\nall_train_data['latitude']=all_train_data['s2token_15'].apply (lambda row: s2cell.token_to_lat_lon(row)[0])\nall_train_data['longitude']=all_train_data['s2token_15'].apply (lambda row: s2cell.token_to_lat_lon(row)[1])\n\nall_test_data['latitude']=all_test_data['s2token_15'].apply (lambda row: s2cell.token_to_lat_lon(row)[0])\nall_test_data['longitude']=all_test_data['s2token_15'].apply (lambda row: s2cell.token_to_lat_lon(row)[1])\n","405b8277":"all_train_data['Labels']=pd.to_numeric(all_train_data['Labels'])\nall_train_data['hour']=pd.to_numeric(all_train_data['hour'])\nall_test_data['hour']=pd.to_numeric(all_test_data['hour'])\n","d12baa71":"y_train = all_train_data['Labels']","c27b719a":"all_train_data['day']=encoder.fit_transform(all_train_data['day'])","3c345c69":"all_test_data['day']=encoder.fit_transform(all_test_data['day'])","bbca8c28":"dropped_columns = ['Labels','Ids','pub_date','s2token_15']\nfor col in dropped_columns:\n    all_train_data = all_train_data.drop(col,axis=1)","70e29181":"dropped_columns_test = ['Ids','pub_date','s2token_15']\n\nfor col in dropped_columns_test:\n    all_test_data = all_test_data.drop(col,axis=1)","503dcab3":"all_train_data.info()","be32ff51":"all_test_data.info()","5bdf16be":"y_train.value_counts()","f8b00c9a":"from sklearn.feature_selection import VarianceThreshold\nX = all_train_data\nselector = VarianceThreshold()\nprint(\"Original feature shape:\", X.shape)\nnew_X = selector.fit_transform(X)\nprint(\"Transformed feature shape:\", new_X.shape)\n\n#The output shows that \n#the transformed features are the same shape so all features have at least some variance.","23e693cc":"\nfrom catboost import Pool, CatBoostClassifier, cv","017262db":"X_train_, X_test_, y_train_, y_test_ = train_test_split(all_train_data,y_train, test_size=0.2, random_state=38)\n","486588b7":"model = CatBoostClassifier(iterations=10000,learning_rate=0.01, l2_leaf_reg=3.5,\n                           colsample_bylevel=    0.0962895297660657,depth= 11, boosting_type='Plain',  \n                           eval_metric=\"CrossEntropy\",use_best_model=True\n                           ,random_seed=22,bootstrap_type= \"Bernoulli\",subsample=0.6927844340277456)","f7193fca":"cate_features_index = np.where(X_train_.dtypes != float)[0] \n","35cdb9b4":"model.fit(X_train_, y_train_, cat_features=cate_features_index,eval_set=(X_test_, y_test_))\n","cc26a569":"pred = model.predict_proba(all_test_data)\npreds= pred[:,1]","b86c9edc":"preds","1c9d6e57":"preds=[\"TRUE\" if x>0.60 else 'FALSE' for x in preds]\n","2bf9fbdc":"preds","39ab3f8b":"submission = pd.read_csv('\/kaggle\/input\/danthon2021\/data_test.csv')\nsubmission['Labels']=preds\nprint(\"submission\")\nsubmission.to_csv('test-catboost-scv.csv',index=False)","9f90793a":"import optuna\n\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score","3797262f":"X = all_train_data.iloc[:,:].to_numpy()\n# X = all_train_data\ny = y_train.copy()\ndef objective(trial):\n#     train_x, valid_x, train_y, valid_y = train_test_split(all_train_data,y_train, test_size=0.3)\n\n    param = {\n        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\n            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n        ),\n        \"used_ram_limit\": \"3gb\",\n    }\n\n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n#     gbm = CatBoostClassifier(**param)\n\n#     gbm.fit(train_x, train_y, cat_features=cate_features_index, eval_set=[(valid_x, valid_y)], verbose=0, early_stopping_rounds=100)\n\n#     preds = gbm.predict(valid_x)\n#     pred_labels = np.rint(preds)\n#     accuracy = accuracy_score(valid_y, pred_labels)\n    '''cross-validation'''\n    kf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n    CV_score_array    =[]\n    for train_index, test_index in kf.split(X,y):\n        X_train, X_valid = X[train_index], X[test_index]\n        X_train = pd.DataFrame(X_train)\n        X_train[[0,1]]=X_train[[0,1]].astype('int') \n        X_valid = pd.DataFrame(X_valid)\n        X_valid[[0,1]]=X_valid[[0,1]].astype('int') \n        y_train, y_valid = y[train_index], y[test_index]\n        clf = CatBoostClassifier(**param)\n        clf.fit(X_train, y_train, cat_features=[0,1],eval_set=[(X_valid, y_valid)], verbose=0)\n        y_pred_test = clf.predict(X_valid)\n        y_pred_test=y_pred_test.round(0).astype(int)\n        score = f1_score(y_pred_test,y_valid)\n        CV_score_array.append(score)\n    avg = np.mean(CV_score_array)\n#     return accuracy\n    return avg","13c7d9c3":"\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=100, timeout=600)","e187b09c":"\n# print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n# print(\"Best trial:\")\n# trial = study.best_trial\n\n# print(\"  Value: {}\".format(trial.value))\n\n# print(\"  Params: \")\n# for key, value in trial.params.items():\n#     print(\"    {}: {}\".format(key, value))","a711ebc3":"# Conclusions","baf99f35":"# CatBoost","11fad74d":"1. CATBOOST has the highest accuracy  among all the models.\n2. CATBOOST has bene\ufb01t on the robustness of using categorical data  features.\n3. Only using 4 features, our technique\/model is able to achieved  a fast yet high and comparable  accuracy on leaderboard.\n4. Not much features could be used rawly. To get higher accuracy In the  future, Well feature engineering is  required to apply on dataset.\n","849c6004":"# Read Train Data","fe5b3f63":"# Features Encoder","d2f7c7a6":"![image.png](attachment:image.png)","af5047fc":"![image.png](attachment:image.png)\n","fe48a3e2":"# Hyper Param"}}