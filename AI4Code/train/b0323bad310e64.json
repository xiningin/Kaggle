{"cell_type":{"c5556990":"code","883e0e5d":"code","2072bb8f":"code","b074fb61":"code","f2882339":"markdown","6634b6cd":"markdown","1620466d":"markdown","a1e32cfd":"markdown","552d080e":"markdown","0b542d52":"markdown","f82b58cf":"markdown","c733369a":"markdown","bfb368c0":"markdown","e015c221":"markdown","ced486f8":"markdown","945c534e":"markdown","c28ffe8d":"markdown"},"source":{"c5556990":"!pip install mglearn\nimport mglearn\nmglearn.plots.plot_animal_tree()","883e0e5d":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import datasets\niris = datasets.load_iris()\nX=iris.data\ny=iris.target\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.5,random_state=42)\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))","2072bb8f":"tree = DecisionTreeClassifier(max_depth=3, random_state=0)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))","b074fb61":"print(\"Feature importances:\\n{}\".format(tree.feature_importances_))","f2882339":" **Strengths, weaknesses, and parameters**\n \n The parameters that control model complexity in decision trees\nare the pre-pruning parameters that stop the building of the tree before it is fully\ndeveloped. Usually, picking one of the pre-pruning strategies\u2014setting either max_depth , max_leaf_nodes , or min_samples_leaf \u2014is sufficient to prevent overfit\u2010\nting.\n\nThe resulting model can easily be visualized and understood by nonexperts, and the algorithms are completely invariant to scaling of the\ndata. As each feature is processed separately, and the possible splits of the data don\u2019t\ndepend on scaling, no preprocessing like normalization or standardization of features\nis needed for decision tree algorithms. In particular, decision trees work well when\nyou have features that are on completely different scales, or a mix of binary and con\u2010\ntinuous features.","6634b6cd":"As expected, the accuracy on the training set is 100%\u2014because the leaves are pure,\nthe tree was grown deep enough that it could perfectly memorize all the labels on the\ntraining data. The test set accuracy is slightly worse which is 96 percent.\n\nIf we don\u2019t restrict the depth of a decision tree, the tree can become very deep\nand complex. Unpruned trees are therefore prone to overfitting and not generalizing\nwell to new data.","1620466d":"Now let\u2019s apply pre-pruning to the tree, which will stop developing\nthe tree before we perfectly fit to the training data. One option is to stop building the\ntree after a certain depth has been reached.\nLimiting the\ndepth of the tree decreases overfitting. This leads to a lower accuracy on the training\nset, but an improvement on the test set","a1e32cfd":"To build a tree, the algorithm searches over all possible tests and finds the one that is\nmost informative about the target variable.\nThis recursive process yields a binary tree of decisions, with each node containing a\ntest. Alternatively, you can think of each test as splitting the part of the data that is\ncurrently being considered along one axis. This yields a view of the algorithm as\nbuilding a hierarchical partition.\nThe recursive partitioning of the data is repeated until each region in the partition\n(each leaf in the decision tree) only contains a single target value (a single class or a\nsingle regression value).","552d080e":"Look at the effect of pre-pruning in more detail on the Iris dataset. As\nalways, we import the dataset and split it into a training and a test part.","0b542d52":"**IF YOU HAVE READ THIS NOTEBOOK ON DECISION TREE AND LIKED IT, PLEASE KINDLY UPVOTE.**","f82b58cf":"Usually data does not come in the form of binary\nyes\/no features as in the animal example, but is instead represented as continuous\nfeatures such as in the 2D dataset.","c733369a":"Decision trees are widely used models for classification and regression tasks.\nImagine you want to distinguish between the following four animals: bears, hawks,\npenguins, and dolphins. Your goal is to get to the correct answer by asking as few if\/else\nquestions as possible. You might start off by asking whether the animal has feathers, a\nquestion that narrows down your possible animals to just two. If the answer is \u201cyes,\u201d\nyou can ask another question that could help you distinguish between hawks and\npenguins. For example, you could ask whether the animal can fly. If the animal\ndoesn\u2019t have feathers, your possible animal choices are dolphins and bears, and you\nwill need to ask a question to distinguish between these two animals\u2014for example,\nasking whether the animal has fins.\n\nHere is the decision tree to distinguish among animals","bfb368c0":"**Overfitting**\n\nThere are two common strategies to prevent overfitting: stopping the creation of the\ntree early (pre-pruning), or building the tree but then removing or collaps\u2010\ning nodes that contain little information (post-pruning or pruning).\nPossible criteria for pre-pruning include limiting the maximum depth of the tree,\nlimiting the maximum number of leaves, or requiring a minimum number of points\nin a node to keep splitting it.","e015c221":"As here we have only four features so a graph would not make sense. But we can make feature importance graph on some other data like Breast cancer data","ced486f8":"**Feature importance in trees**\n\nInstead of looking at the whole tree,  there are some useful prop\u2010\nerties that we can use for the workings of the tree. The most commonly\nused summary is feature importance, which rates how important each feature is for\nthe decision a tree makes. It is a number between 0 and 1 for each feature, where 0\nmeans \u201cnot used at all\u201d and 1 means \u201cperfectly predicts the target.\u201d The feature\nimportances always sum to 1","945c534e":"**Controlling complexity of decision trees**\n\nTypically, building a tree as described here and continuing until all leaves are pure\nleads to models that are very complex and highly overfit to the training data. The\npresence of pure leaves mean that a tree is 100% accurate on the training set; each\ndata point in the training set is in a leaf that has the correct majority class.\nWhich can lead to overfitting.","c28ffe8d":"  **DECISION TREES**"}}