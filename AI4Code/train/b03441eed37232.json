{"cell_type":{"637001a6":"code","3fab3546":"code","bb12aa9a":"code","6edc3139":"code","4122586f":"code","5cff5c14":"code","08f2c79c":"code","d332113d":"code","84822548":"markdown"},"source":{"637001a6":"# On the string of experiments I have been doing, this one is inspired from @anfro18 \n# so please let me know your thoughts \n# I am trying to use different classification techniques \n#Done - 1. Logistic Regression  2. Random Forest  \n#To be Added -  3. Neural Net 4. Keras\/Tener Flow  \nimport numpy as np  \nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\ndt = pd.read_csv(\"..\/input\/Dataset_spine.csv\")\n#see few top rows \ndt.head()","3fab3546":"#drop last column\ndt.drop('Unnamed: 13', axis=1, inplace=True)\n# Convert class attriutes of abnormal and normal to integer values of 0 and 1,using get_dummies\n# please note abnormal is 1 here  \ndt = pd.concat([dt, pd.get_dummies(dt['Class_att'])], axis=1)\n# Drop unnecessary label column in place. \ndt.drop(['Class_att','Normal'], axis=1, inplace=True)\n# Rename columns\ndt.columns= ['pelvic_incidence','pelvic_tilt','lumbar_lordosis_angle','sacral_slope','pelvic_radius','degree_spondylolisthesis','pelvic_slope','Direct_tilt',\n'thoracic_slope','cervical_tilt','sacrum_angle','scoliosis_slope','Class_cat']\n#see top 10 rows\ndt.head(10)","bb12aa9a":"#see all variables\ndt.describe()","6edc3139":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np \nimport pickle \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.formula.api as sm\nfrom sklearn.model_selection import train_test_split\n#class distribution of dependent variable \nsns.countplot(x = \"Class_cat\", data = dt)\nplt.title('Class Variable Distribution')\nplt.show()","4122586f":"#List of all columns [['pelvic_incidence','pelvic_tilt','lumbar_lordosis_angle','sacral_slope','pelvic_radius','degree_spondylolisthesis','pelvic_slope','Direct_tilt',\n#'thoracic_slope','cervical_tilt','sacrum_angle','scoliosis_slope']]\nX = dt[['pelvic_tilt', 'cervical_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius','Direct_tilt',\n'thoracic_slope','sacrum_angle','scoliosis_slope', 'degree_spondylolisthesis']]\n# Removed three variables pelvic_incidence, pelvic_tilt and sacral_slope as their p-value >.05\n# this led to the reduction in overall fit by ~2%. \ny = dt[\"Class_cat\"]\nX['intercept'] = 1.0  # so we don't need to use sm.add_constant every time\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n#train the model on Training dataset\nmodel = sm.Logit(y_train, X_train)\nresult = model.fit()","5cff5c14":"# model results \nresult.summary2()","08f2c79c":"#see the performance \ndef logPredict(modelParams, X):  \n    probabilities = modelParams.predict(X)\n    return [1 if x >= 0.5 else 0 for x in probabilities]\npredictions = logPredict(result, X_test)\naccuracy = np.mean(predictions == y_test)\n#print ('Variable List \\n')\n#print(X_test.columns.values)\nprint ('accuracy = {0}%'.format(accuracy*100))\n#['lumbar_lordosis_angle' 'pelvic_radius' 'Direct_tilt' 'thoracic_slope', 'cervical_tilt' 'sacrum_angle' 'scoliosis_slope']\n#accuracy = 77.06422018348624%\n#['lumbar_lordosis_angle' 'pelvic_radius' 'Direct_tilt' 'thoracic_slope', 'sacrum_angle' 'scoliosis_slope']\n#accuracy = 72.47706422018348%\n#['pelvic_radius' 'Direct_tilt' 'thoracic_slope' 'sacrum_angle', 'scoliosis_slope']\n#accuracy = 72.47706422018348%, lumbar_lordosis_angle looks insignificant since we have same accuracy on test set \n#['pelvic_incidence' 'pelvic_tilt' 'sacral_slope' 'pelvic_radius', 'Direct_tilt' 'thoracic_slope' 'sacrum_angle' 'scoliosis_slope']\n#accuracy = 73.39449541284404%, but failed to converge \n#['cervical_tilt' 'sacral_slope' 'pelvic_radius' 'Direct_tilt', 'thoracic_slope' 'sacrum_angle' 'scoliosis_slope']\n#accuracy = 70.64220183486239%, adding cervical tilt helped in first iteration but it looks as if this varaible has significant when used with other vars \n#['pelvic_tilt' 'cervical_tilt' 'sacral_slope' 'pelvic_radius', 'Direct_tilt' 'thoracic_slope' 'sacrum_angle' 'scoliosis_slope']\n#accuracy = 74.31192660550458%, pelvic incidence and pelvic tiltseems to be corelated, removed pelvic incidence \n#['pelvic_tilt' 'cervical_tilt' 'lumbar_lordosis_angle' 'sacral_slope',  'pelvic_radius' 'Direct_tilt' 'thoracic_slope' 'sacrum_angle'\n# 'scoliosis_slope']\n#accuracy = 76.14678899082568%, whoa, added lumbar_lordosis_angle and we reached at accuracy of 76.14678899082568% without convergence failed error \n#['pelvic_tilt' 'cervical_tilt' 'lumbar_lordosis_angle' 'sacral_slope',  'pelvic_radius' 'Direct_tilt' 'thoracic_slope' 'sacrum_angle'\n# 'scoliosis_slope' 'degree_spondylolisthesis']\n#accuracy = 83.4862385321101%, added degree_spondylolisthesis, the variable shows as insiignificant\n# but has improved the accuracy drastically ,Error : Current function value: inf, Psudo R squared - inf  \n#['pelvic_tilt' 'cervical_tilt' 'lumbar_lordosis_angle' 'sacral_slope', 'pelvic_radius' 'Direct_tilt' 'thoracic_slope' 'sacrum_angle'\n# 'scoliosis_slope' 'degree_spondylolisthesis']\n#accuracy = 81.65137614678899%","d332113d":"#Adding Random Forest technique\nfrom sklearn.ensemble import RandomForestClassifier\n#Techniquely, the more trees we use, the precise outcome we would get\nx_train2,x_test2,y_train2,y_test2 = train_test_split(X,y,test_size=0.3,random_state=42)\n\n#Generate forests containing 10(default), 50, 100 trees\nn_trees = [10, 30,  50, 100]\nfor i in n_trees:\n    mdl = RandomForestClassifier(n_estimators=i)\n    mdl.fit(x_train2,y_train2)\n    pred = mdl.predict(x_test2)\n    \n    print('number of trees: {}'.format(i))\n    #Each time of prediction,the accuracy is measured\n    correct_pred = 0\n    for j,k in zip(y_test2,pred):\n        if j == k:\n            correct_pred += 1\n    print('accuracy: {}'.format(correct_pred\/len(y_test2) *100))","84822548":"**Observations So Far : **\n1. Logistic regression results are not as effective as SVM or Tensor flow model \n2. There are few variables that are causing divide by zero error in results and with all variables the model fail to converge. Need to understand whyand how these situations happening. \nError : RuntimeWarning: divide by zero encountered in log\n  return np.sum(np.log(self.cdf(q*np.dot(X,params))))\n\n**Next steps: **\n1. Try feature enginnering and remove outliers to see if it results in improvement. \n2. See correlation between variables like pelvic_tilt and pelvic_incidence, there seem to be a correlation but the magnitude needs to be understood.\n3. Explore other techniques for classifcation like Random forest and NN etc. \n\n"}}