{"cell_type":{"1f439930":"code","b0a1bd02":"code","c7c91033":"code","032b4515":"code","ea59cf81":"code","1ee198a8":"code","fe1484b5":"code","59fefbd4":"code","1ae14d2e":"code","49a77358":"code","5390d565":"code","344f9af2":"code","b1495ab9":"code","c8243b0f":"code","17a7a966":"code","96083ac9":"code","cb1b00d1":"code","292ad029":"code","39c45dfd":"code","9be8bf2d":"code","9ff38449":"code","8956570c":"code","c0e28e3c":"code","83679568":"markdown","40a6c002":"markdown","258f613b":"markdown","72b6e030":"markdown"},"source":{"1f439930":"%reset -f","b0a1bd02":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import PoissonRegressor\n\nimport joblib\n\nimport tensorflow as tf\nmae = tf.keras.losses.MeanAbsoluteError()    ## The main loss for this task\n\nimport os\n\nINPUT_DIR = '\/kaggle\/input\/morebikes2021'\n\nFEATURE_SET = 'short_full_temp'   ## Change here !\nSHUFFLE_SAMPLES = True\nTRAIN_SIZE = 0.95","c7c91033":"def get_peters_features():\n    short = ['bikes_3h_ago', 'short_profile_3h_diff_bikes', 'short_profile_bikes']\n    short_temp = short + ['temperature.C']\n    full = ['bikes_3h_ago', 'full_profile_3h_diff_bikes', 'full_profile_bikes']\n    full_temp = full + ['temperature.C']\n    short_full = ['bikes_3h_ago', 'short_profile_3h_diff_bikes', 'short_profile_bikes', 'full_profile_3h_diff_bikes', 'full_profile_bikes']\n    short_full_temp = short_full + ['temperature.C']\n    \n    if FEATURE_SET=='short':\n        return short\n    elif FEATURE_SET=='short_temp':\n        return short_temp\n    elif FEATURE_SET=='full':\n        return full\n    elif FEATURE_SET=='full_temp':\n        return full_temp\n    elif FEATURE_SET=='short_full':\n        return short_full\n    elif FEATURE_SET=='short_full_temp':\n        return short_full_temp\n    else:\n        return 'all'\n\ndef get_train_path(station_id):\n    return INPUT_DIR+'\/Train\/Train\/station_'+str(station_id)+'_deploy.csv'\n\ndef preprocess(df):\n    df = df.drop(['weekday', 'latitude', 'longitude','year', 'month'], axis=1)\n#     df = df.drop(['numDocks'], axis=1)\n    df = df.drop(['station'], axis=1)\n    df['timestamp'] = (df['timestamp'] - 1412114400) \/ 3600\n    return df\n\ndef select_peters_features(df):\n    features_to_use = get_peters_features()\n        \n    if features_to_use == 'all':\n        return df.dropna()\n    else:\n#         features_to_use += ['numDocks']         #### Remember to remove this !\n        if 'bikes' in list(df.columns):\n            features_to_use += ['bikes']\n        return df[features_to_use].dropna()\n    \ndef make_learning_data(station_id):\n    df = pd.read_csv(get_train_path(station_id))\n    \n    df = preprocess(df)\n#     df = df.sample(frac=1, random_state=12)\n\n    df = select_peters_features(df)\n\n    df_y = df['bikes']\n    df_X = df.drop(['bikes'], axis=1)\n\n    return np.array(df_X), np.array(df_y)\n\n# df = make_learning_data(201)\n# df","032b4515":"MAX_DOCKS = 27\ndef plot_loss_evol(history):\n    fig, (ax2, ax1) = plt.subplots(1, 2, figsize=(18, 5))\n\n    # summarize history for accuracy\n    ax1.plot(history.history['sparse_categorical_accuracy'])\n    ax1.plot(history.history['val_sparse_categorical_accuracy'])\n    ax1.set_title('KerasNet accuracy')\n    ax1.set_ylabel('accuracy')\n    ax1.set_xlabel('epochs')\n    ax1.legend(['train', 'val'], loc='upper left')\n\n    # summarize history for loss\n    ax2.plot(history.history['loss'])\n    ax2.plot(history.history['val_loss'])\n    ax2.set_title('KerasNet loss')\n    ax2.set_ylabel('loss')\n    ax2.set_xlabel('epochs')\n    ax2.legend(['train', 'val'], loc='upper right')\n    \n    plt.show();\n    \n\ndef train_bikes_models(X_train, X_val, y_train, y_val, station_id=0):\n    \"\"\"\n    @station_id: id of the station we want to train\n    return the best model for this station\n    \"\"\"\n\n    \"\"\" All models that will be trained and their scores \"\"\"\n    potential_models = []\n\n    \"\"\"### Linear Regression\"\"\"\n\n#     degrees = [1, 2, 3]\n#     degrees = [1]\n#     for i in range(len(degrees)):\n#         scaled_features = StandardScaler()\n#         polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n# #         linear_regression = LinearRegression()\n# #         linear_regression = Lasso()\n#         linear_regression = Ridge()\n#         pipeline = Pipeline(\n#             [\n#                 (\"scaled_features\", scaled_features),\n#                 (\"polynomial_features\", polynomial_features),\n#                 (\"linear_regression\", linear_regression),\n#             ]\n#         )\n#         pipeline.fit(X_train, y_train)\n#         y_pred = np.rint(pipeline.predict(X_val))\n#         potential_models.append(('linear_reg_'+str(degrees[i]), pipeline, mae(y_val, y_pred).numpy()))\n\n    \"\"\"### K-Nearest Neighbors\"\"\"\n\n#     KNN = KNeighborsClassifier(n_neighbors=2)\n#     KNN.fit(X_train, y_train)\n#     y_pred = KNN.predict(X_val)\n#     potential_models.append(('knn', KNN, mae(y_val, y_pred).numpy()))\n\n    \"\"\"### Logistic regression\"\"\"\n\n    \"\"\" From here on, scale the data between 0 and 1\"\"\"\n#     scaler = MinMaxScaler()\n#     X = scaler.fit_transform(X)\n#     X_train, X_val, y_train, y_val = train_test_split(X, y, shuffle=None, train_size=0.75)\n#     LogReg = LogisticRegressionCV(multi_class='multinomial')\n#     LogReg = pipeline = Pipeline([(\"minmax_scaler\", MinMaxScaler()), (\"logistic_reg\", LogisticRegressionCV(multi_class='multinomial'))])\n#     LogReg.fit(X_train, y_train)\n#     y_pred = LogReg.predict(X_val)\n#     potential_models.append(('logistic_reg', DcTree, mae(y_val, y_pred).numpy()))\n\n    \"\"\"### Decision Tree\"\"\"\n\n#     DcTree = Pipeline([(\"minmax_scaler\", MinMaxScaler()), (\"decision_tree\", DecisionTreeClassifier(random_state=12))])\n#     DcTree.fit(X_train, y_train)\n#     y_pred = DcTree.predict(X_val)\n#     potential_models.append(('decision_tree', DcTree, mae(y_val, y_pred).numpy()))\n\n    \"\"\"### Random Forest\"\"\"\n\n#     RdFor = Pipeline([(\"minmax_scaler\", MinMaxScaler()), (\"random_forest\", RandomForestClassifier(n_estimators=100, max_leaf_nodes=None, n_jobs=-1, random_state=12))])\n#     RdFor.fit(X_train, y_train)\n#     y_pred = RdFor.predict(X_val)\n#     potential_models.append(('random_forest', RdFor, mae(y_val, y_pred).numpy()))\n\n    \"\"\"### SVM - One vs. Rest\"\"\"\n\n#     SVM1 = Pipeline([(\"minmax_scaler\", MinMaxScaler()), (\"svm_ovr\", OneVsRestClassifier(LinearSVC(random_state=12)))])\n#     SVM1.fit(X_train, y_train)\n#     y_pred = SVM1.predict(X_val)\n#     potential_models.append(('svm_ovr', SVM1, mae(y_val, y_pred).numpy()))\n\n    \"\"\"### SVM - One vs. One\"\"\"\n\n#     SVM2 = Pipeline([(\"minmax_scaler\", MinMaxScaler()), (\"svm_ovo\", OneVsOneClassifier(SVC(random_state=12)))])\n#     SVM2.fit(X_train, y_train)\n#     y_pred = SVM2.predict(X_val)\n#     potential_models.append(('svm_ovo', SVM2, mae(y_val, y_pred).numpy()))\n    \n    \"\"\"### Poisson Regression \"\"\"\n\n#     Poisreg = Pipeline([(\"minmax_scaler\", MinMaxScaler()), (\"pois_reg\", PoissonRegressor())])\n#     Poisreg.fit(X_train, y_train)\n#     y_pred = Poisreg.predict(X_val)\n#     potential_models.append(('pois_reg', Poisreg, mae(y_val, y_pred).numpy()))\n\n    \"\"\"### Neural Network\"\"\"\n\n    \"\"\" Define the model  \"\"\"\n    KerasNet = tf.keras.Sequential([\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(4096, activation='relu'),\n            tf.keras.layers.Dense(1024, activation='relu'),\n            tf.keras.layers.Dense(MAX_DOCKS+1, activation='softmax')        \n        ])\n    KerasNet(X_train)\n    \"\"\" Compile the model  \"\"\"\n    KerasNet.compile(\n        loss=tf.losses.sparse_categorical_crossentropy,\n        optimizer=tf.optimizers.Adam(5e-5),\n        metrics=[tf.metrics.sparse_categorical_accuracy])\n    \"\"\" Train the mode \"\"\"\n    history = KerasNet.fit(\n        X_train,\n        y_train,\n        batch_size=300,\n        epochs=150,\n        verbose=0,\n        # steps_per_epoch=100,\n        validation_data=(X_val, y_val),\n    )\n    plot_loss_evol(history)\n    \"\"\" Make a few predictions \"\"\"\n    # Generate predictions on new data\n    y_hat_pred_proba = KerasNet.predict(X_val)\n    y_pred = tf.cast(tf.argmax(y_hat_pred_proba, axis=1), tf.int32)\n    ## Add to potential models\n    potential_models.append(('neural_net', KerasNet, mae(y_val, y_pred).numpy()))\n\n\n    \"\"\"## Sort the models and pick the best\"\"\"\n    potential_models = sorted(potential_models, key=lambda el:el[2])\n\n    ## Save the best model\n#     if potential_models[0][0] != 'neural_net':\n#         save_name = str(station_id)+'_'+potential_models[0][0]+'.pkl'\n#         joblib.dump(potential_models[0][1], save_name)\n#     else:\n#         save_name = str(station_id)+'_'+potential_models[0][0]+'.h5'\n#         potential_models[0][1].save(save_name)\n\n    ## The best model is\n#     return (save_name, potential_models[0][1], potential_models[0][2])    \n    return potential_models[0]","ea59cf81":"# %%capture\n\n# per_station_models = {}\n\n# for station_id in range(201, 276):\n#     X, y = make_learning_data(station_id)\n#     MAX_DOCKS = np.max(y)\n#     X_train, X_val, y_train, y_val = train_test_split(X, y, shuffle=SHUFFLE_SAMPLES, train_size=TRAIN_SIZE)\n#     per_station_models[station_id] = train_bikes_models(X_train, X_val, y_train, y_val)[1]","1ee198a8":"# per_station_models","fe1484b5":"### Remove this afterwards\n# per_station_models[201].named_steps['linear_regression'].coef_","59fefbd4":"X, y = make_learning_data(201) \nX_train, X_val, y_train, y_val = train_test_split(X, y, shuffle=SHUFFLE_SAMPLES, train_size=TRAIN_SIZE)\nstations = [201]*len(y_val)\n\nfor station_id in range(202, 276):\n    X_, y_ = make_learning_data(station_id) \n    X_train_, X_val_, y_train_, y_val_ = train_test_split(X_, y_, shuffle=SHUFFLE_SAMPLES, train_size=TRAIN_SIZE)\n    \n    X_train = np.vstack([X_train, X_train_])\n    X_val = np.vstack([X_val, X_val_])    \n    y_train = np.concatenate([y_train, y_train_])    \n    y_val = np.concatenate([y_val, y_val_])\n    \n    stations += [station_id]*len(y_val_)\n\nMAX_DOCKS = max([np.max(y_train), np.max(y_val)])","1ae14d2e":"## Save this data for later testing in Phase 2\nnp.save('X_val.npy', X_val)\nnp.save('y_val.npy', y_val)\nnp.save('stations_val.npy', np.array(stations))","49a77358":"## Shapes\n\nprint(\"All stations together:\")\nprint(\"train:\", X_train.shape, y_train.shape)\nprint(\"val:\", X_val.shape, y_val.shape)\n\nprint(\"stations for each sample:\", len(stations))","5390d565":"model_name, all_stations_model = train_bikes_models(X_train, X_val, y_train, y_val)[:2]","344f9af2":"### Remove this afterwards\n# all_stations_model.named_steps['linear_regression'].coef_","b1495ab9":"all_stations_model","c8243b0f":"y_pred = all_stations_model.predict(X_val)\ny_pred = np.rint(tf.argmax(y_pred, axis=1).numpy())\n\nfig, ax = plt.subplots(1, 1, figsize=(18, 5))\n\nplt.plot(y_val[:600], \"o\", label=\"True values\")\nax.plot(y_pred[:600], \".\", label=\"Predictions\")\nax.set_xlabel(\"instances\")\nax.set_ylabel(\"# bikes\")\nax.legend(loc=\"best\")\nax.set_title(\n    model_name+\" for all-stations\\nMAE = {:.2f}\".format(\n        mae(y_val, y_pred).numpy()\n    )\n)","17a7a966":"## Save the model\n# joblib.dump(all_stations_model, 'all_stations_model.pkl')","96083ac9":"# y_pred = np.zeros_like(y_val)\n\n# for i in range(len(y_val)):\n#     station_id = stations[i]\n#     tmp = per_station_models[station_id].predict(np.array([X_val[i]]))\n#     y_pred[i] = np.rint(tf.argmax(tmp, axis=1).numpy())","cb1b00d1":"# fig, ax = plt.subplots(1, 1, figsize=(18, 5))\n\n# plt.plot(y_val[:600], \"o\", label=\"True values\")\n# ax.plot(y_pred[:600], \".\", label=\"Predictions\")\n# ax.set_xlabel(\"instances\")\n# ax.set_ylabel(\"# bikes\")\n# ax.legend(loc=\"best\")\n# ax.set_title(\n#     \"Mixed models per-station\\nMAE = {:.2f}\".format(\n#         mae(y_val, y_pred).numpy()\n#     )\n# )","292ad029":"## Save the models\n# joblib.dump(per_station_models, 'per_station_models.pkl')","39c45dfd":"def make_test_data():\n    df = pd.read_csv(INPUT_DIR+'\/test.csv')\n    \n    de = preprocess(df)\n    \n    ids = df['Id']\n    df = df.drop(['Id'], axis=1)\n    \n    station_ids = df['station']\n    df = df.drop(['station'], axis=1)\n    \n    df = select_peters_features(df)\n    \n    return np.array(df), np.array(ids), np.array(station_ids)\n    \nX_test, ids, station_ids = make_test_data()\nX_test.shape","9be8bf2d":"## Save this data for later testing in Phase 3\nnp.save('X_test.npy', X_test)\nnp.save('ids_test.npy', ids)\nnp.save('stations_test.npy', station_ids)","9ff38449":"\"\"\" If we chose to all stations at once \"\"\"\ny_pred = all_stations_model.predict(X_test)\ny_pred = np.rint(tf.argmax(y_pred, axis=1).numpy())\n\n\"\"\" If we chose one model per-station \"\"\"\n# y_pred = np.zeros((len(X_test)))\n# for i in range(len(X_test)):\n#     station_id = station_ids[i]\n#     tmp = per_station_models[station_id].predict(np.array([X_test[i]]))\n#     y_pred[i] = np.rint(tf.argmax(tmp, axis=1).numpy())\n# np.unique(y_pred)\n\ny_pred = np.where(y_pred<0, np.zeros_like(y_pred), y_pred)","8956570c":"results = pd.DataFrame({'Id':ids, 'bikes':np.rint(y_pred)})\nresults","c0e28e3c":"results.to_csv('\/kaggle\/working\/roussel_subin_submission.csv', index=False)\nos.listdir('\/kaggle\/working\/')","83679568":"## Train and predict for all stations at once","40a6c002":"## Build all per-station models","258f613b":"## Predictions for the competition","72b6e030":"## Predictions with per-station models"}}