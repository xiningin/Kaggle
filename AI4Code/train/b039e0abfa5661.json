{"cell_type":{"0c3d7c30":"code","1029864d":"code","f164f535":"code","baa00c18":"code","89aa632a":"code","a6443ec2":"code","7a69d1bc":"code","8da2a2e6":"code","95b0d4b3":"code","5d90c762":"code","a74ab9cb":"code","128598a4":"code","ef60c905":"code","e9680bdf":"code","e4358cb8":"code","cbaceead":"code","b197e2ed":"code","d0e9cf4c":"code","d8126aa3":"code","3c3c68d7":"code","1e753e87":"code","543be45a":"code","9b8ae296":"markdown","5f9ec5e0":"markdown","1fece586":"markdown","3460bc45":"markdown"},"source":{"0c3d7c30":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nfrom torchtext import data\nfrom torchtext import datasets\nfrom torchtext.data import Field\nfrom torchtext.data import Iterator, BucketIterator\nimport torch.optim as optim\nimport os\nprint(os.listdir(\"..\/input\"))","1029864d":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nBATCH_SIZE = 64\nMAX_VOCAB_SIZE = 25_000","f164f535":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsample = pd.read_csv('..\/input\/sample_submission.csv')","baa00c18":"small = train[:-10000]\nvalid = train[-10000:]\nsmall.to_csv('small.csv', index=False)\nvalid.to_csv('valid.csv', index=False)","89aa632a":"SEED = 1234\n\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\ntokenize = lambda x: x.split()\nTEXT = Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True)\nLABEL = Field(sequential=False, use_vocab=False, dtype=torch.float)","a6443ec2":"from torchtext.data import TabularDataset\n \ntrain_datafields = [(col, TEXT) if col == 'comment_text' else \n                              (col, LABEL) if col == 'target' else \n                              (col, None) \n                              for col in train.columns]\ntrain_data, valid_data = TabularDataset.splits(\n            path='',\n            train='small.csv',\n            validation='valid.csv',\n            format='csv',\n            skip_header=True,\n            fields=train_datafields)\n\ntest_datafields = [('id', None), ('comment_text', TEXT)]\ntest_data = TabularDataset(\n            path=\"..\/input\/test.csv\",\n            format='csv',\n            skip_header=True,\n            fields=test_datafields)","7a69d1bc":"print(f'Number of training examples: {len(train_data)}')\nprint(f'Number of validation examples: {len(valid_data)}')\nprint(f'Number of test examples: {len(test_data)}')","8da2a2e6":"TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\nLABEL.build_vocab(train_data)","95b0d4b3":"train_iter, val_iter = BucketIterator.splits(\n    (train_data, valid_data),\n    batch_sizes=(BATCH_SIZE, BATCH_SIZE),\n    device=device,\n    sort_key=lambda x: len(x.comment_text),\n    sort_within_batch=True,\n    repeat=False\n)\n\ntest_iter = Iterator(test_data, batch_size=1, device=device, sort=False, sort_within_batch=False, repeat=False)","5d90c762":"# Kudos to http:\/\/mlexplained.com\/2018\/02\/08\/a-comprehensive-tutorial-to-torchtext\/\n\nclass BatchWrapper:\n    def __init__(self, iterator, x_var, y_vars):\n        self.iterator, self.x_var, self.y_vars = iterator, x_var, y_vars\n  \n    def __iter__(self):\n        for batch in self.iterator:\n            x = getattr(batch, self.x_var)\n            if self.y_vars is not None:\n                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n            else:\n                y = torch.zeros((1))\n            yield (x, y)\n    def __len__(self):\n        return len(self.iterator)\n\ntrain_loader = BatchWrapper(train_iter, \"comment_text\", [\"target\"])\nvalid_loader = BatchWrapper(val_iter, \"comment_text\", [\"target\"])\ntest_loader = BatchWrapper(test_iter, \"comment_text\", None)","a74ab9cb":"class RNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n                 bidirectional, dropout, pad_idx):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n        self.rnn = nn.LSTM(embedding_dim, \n                           hidden_dim, \n                           num_layers=n_layers, \n                           bidirectional=bidirectional, \n                           dropout=dropout)\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n \n\n    def forward(self, text, text_lengths):\n        embedded = self.dropout(self.embedding(text))\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n        return self.fc(hidden.squeeze(0))","128598a4":"INPUT_DIM = len(TEXT.vocab)\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = 1\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.5\nPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]","ef60c905":"model = RNN(INPUT_DIM, \n            EMBEDDING_DIM, \n            HIDDEN_DIM, \n            OUTPUT_DIM, \n            N_LAYERS, \n            BIDIRECTIONAL, \n            DROPOUT, \n            PAD_IDX)","e9680bdf":"optimizer = optim.Adam(model.parameters())\nloss_func = nn.BCEWithLogitsLoss()","e4358cb8":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","cbaceead":"model = model.to(device)\nloss_func = loss_func.to(device)","b197e2ed":"def train_model(model, data_loader, optimizer, loss_func):\n    epoch_loss = 0\n    \n    model.train()\n    \n    for x, y in data_loader:\n        optimizer.zero_grad()\n        text, text_lengths = x\n        preds = model(text, text_lengths)\n        loss = loss_func(preds, y)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    return epoch_loss \/ len(data_loader)","d0e9cf4c":"def validate_model(model, data_loader, loss_func):\n    val_loss = 0.0\n    model.eval()\n    for x, y in data_loader:\n        text, text_lengths = x\n        preds = model(text, text_lengths)\n        loss = loss_func(preds, y)\n        val_loss += loss.item()\n    return val_loss \/ len(data_loader)","d8126aa3":"epochs = 1\n\nbest_valid_loss = float('inf')\nbest_epoch = 0\n        \nfor epoch in range(1, epochs + 1):\n    epoch_loss = train_model(model, train_loader, optimizer, loss_func)\n    val_loss = validate_model(model, valid_loader, loss_func)\n    if val_loss < best_valid_loss:\n        best_valid_loss = val_loss\n        best_epoch = epoch\n        print(f'Best validation loss!! {best_valid_loss}')\n        torch.save(model.state_dict(), 'toxic_model.pt')\n    print(f'Epoch: {epoch}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')","3c3c68d7":"print(f'Best validation loss at epoch = {best_epoch}')\nmodel.load_state_dict(torch.load('toxic_model.pt'))\ntest_preds = []\nfor i, tup in enumerate(test_loader):\n    if i % 1000 == 0:\n        print(f'Progress = {i\/len(test_loader):.2%}')\n    x, y = tup\n    text, text_lengths = x\n    preds = model(text, text_lengths)\n    preds = preds.view(x[0].shape[1])\n    preds = preds.data.cpu().numpy()\n    preds = 1 \/ (1 + np.exp(-preds))\n    test_preds.append(preds)\ntest_preds = np.hstack(test_preds)","1e753e87":"submission = pd.read_csv('..\/input\/test.csv')\nsubmission.loc[:, 'prediction'] = test_preds\nsubmission.drop('comment_text', axis=1).to_csv('submission.csv', index=False)","543be45a":"submission","9b8ae296":"### Config","5f9ec5e0":"### Load data","1fece586":"### Model","3460bc45":"### Train"}}