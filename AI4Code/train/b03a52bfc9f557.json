{"cell_type":{"79a88a59":"code","2552f47b":"code","4779959a":"code","cb4faf59":"code","079c6d51":"code","518e3efa":"code","a21c865a":"code","2b604988":"code","d8a30490":"code","6dc1dbf1":"code","b183c906":"code","84dc5937":"code","2868c66c":"code","a25a4ffd":"code","1f821909":"code","7a6de22f":"code","cd19a685":"code","2317cdbd":"code","beb950c9":"markdown","605477c5":"markdown","0fbd7eee":"markdown","c546b679":"markdown","4445c0af":"markdown","14ad34f8":"markdown","7d600445":"markdown","683696cf":"markdown","00651370":"markdown","12193e9e":"markdown","159a73c7":"markdown","33da2ec2":"markdown","63b63a6f":"markdown","ab52bd05":"markdown","2847a341":"markdown","85c38599":"markdown","e81db4d4":"markdown","0a7d4f6b":"markdown","9ccc0330":"markdown","6b2ba8af":"markdown"},"source":{"79a88a59":"import os\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nimport cv2\nimport torch\nfrom torch.utils import data\nfrom PIL import Image\nimport torchvision\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt","2552f47b":"txt = np.genfromtxt('train\/gt.txt',delimiter =';', dtype= None,encoding=None)\n\n#Creating a dictionary with image names as key and annotations as value\ndic ={}\nfor i in range (0,len(txt)):\n    #Image name is first element of annotation file\n    img_name = txt[i][0]\n    # 4 Coordinates\n    target = [txt[i][1],txt[i][2],txt[i][3],txt[i][4],txt[i][5]]\n    #Last element is the class number\n    clas = txt[i][-1]\n    #If multiple objects, store coordinates and classes as list of lists\n    if(img_name in dic):\n        dic[img_name].append(target)\n    else:\n        dic[img_name] = [target]\nprint(dic['00001.ppm'])\nprint(\"Number of Images: \" + str(len(dic)))","4779959a":"dic['00001.ppm']","cb4faf59":"#Data Distribution\n\ncls_lst = {}\n\nfor i in dic:\n    for j in dic[i][:]:\n        #print(len(dic[i]))\n        for k in range(len(dic[i])):\n            clss = dic[i][:][k][-1]\n            if clss in cls_lst:\n                cls_lst[clss] += 1\n            else:\n                cls_lst[clss] = 1\n                \nprint(cls_lst)\n\nxx = []\nyy = []\nfor i in cls_lst:\n    xx.append(str(i))\n    yy.append(cls_lst[i])\n\nx_pos = [i for i, _ in enumerate(xx)]\n\nplt.bar(x_pos, yy, color='green')\nplt.xlabel(\"Class Number\")\nplt.ylabel(\"Number of Examples\")\nplt.title(\"GTSDB\")\n#plt.figure(figsize=(30,30))\nplt.xticks(x_pos, xx)\n\nplt.show()","079c6d51":"#Copy only files that are annotated in the gt.txt to imagesf\nimport shutil\n\npt = glob('.\/train\/images\/*.ppm')\n\nlen(pt)\n#Copying into new directory\nfor i in range(len(dic)):\n    ofile = r'.\/train\/images\/{}'.format(list(dic)[i])\n    target = r'.\/train\/imagesf\/{}'.format(list(dic)[i])\n    shutil.copyfile(ofile, target)\n#Check if len(dic) == number of images in folder\nprint(len(glob('.\/train\/imagesf\/*.ppm')))\nlen(dic)","518e3efa":"class myDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"imagesf\"))))\n \n    def __getitem__(self, idx):\n        # Load image path\n        img_path = os.path.join(self.root, \"imagesf\", self.imgs[idx])\n        #Load image as PIL\n        img = Image.open(img_path).convert(\"RGB\")        \n        # Get objects in the image\n        objects = dic[self.imgs[idx]]\n        # Get bounding box coordinates for each object in image\n        boxes = []\n        labels = []\n        for obj in objects:\n            #print(idx, obj[-1], self.imgs)\n            name = obj[-1]\n            labels.append(np.int(name))\n            #Get bounding box coordinates\n            xmin = np.float(obj[0])\n            ymin = np.float(obj[1])\n            xmax = np.float(obj[2])\n            ymax = np.float(obj[3])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)        \n \n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((len(objects),), dtype=torch.int64)\n \n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n \n        if self.transforms is not None:\n            # Note that target (including bbox) is also transformed\\enhanced here, which is different from transforms from torchvision import\n            # Https:\/\/github.com\/pytorch\/vision\/tree\/master\/references\/detectionOfTransforms.pyThere are examples of target transformations when RandomHorizontalFlip\n            img, target = self.transforms(img, target)\n \n        return img, target\n \n    def __len__(self):\n        return len(self.imgs)","a21c865a":"import utilss\nimport transforms as T\nfrom engine import train_one_epoch, evaluate\n# utils, transforms, engine were just downloadedUtils.py,transforms.py,engine.py\n \ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        # 50% chance of flipping horizontally\n        transforms.append(T.RandomHorizontalFlip(0.5))\n \n    return T.Compose(transforms)","2b604988":"from engine import train_one_epoch, evaluate\nimport utilss\nimport torch.nn as nn\nos.environ['TORCH_HOME'] = '.\/'\n\nroot = r'.\/train'\n\n# Train on the GPU if available else CPU.\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# 44 classes = 43 + background\nnum_classes = 44\n#Send the data to the myDataset class (Apply transformations, Get bbox, labels, objects)\ndataset = myDataset(root, get_transform(train=True))\ndataset_test = myDataset(root, get_transform(train=False))\n\n# split the dataset in train and test set\n# My dataset has 506 images, almost training validation 4:1\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-100])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-100:])\n\n# define training and validation data loaders\n#collate_fn returns tuples of images and image annotations for every iteration.\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=2, shuffle=True, # num_workers=4,\n    collate_fn=utilss.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=2, shuffle=False, # num_workers=4,\n    collate_fn=utilss.collate_fn)\n\n# Define model\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=num_classes, pretrained_backbone=True)\n# OR model = get_object_detection_model(num_classes)\n#model = torch.load('.\/train150.pkl')\n\n#Use specific GPUs:\nmodel = nn.DataParallel(model, device_ids=[0,1,2,3]) #Remove this line if not necessary.\n\n# Move the model to device\nmodel.to(device)\n\nprint(\"Model loaded\")","d8a30490":"from engine import train_one_epoch, evaluate\nimport utilss\nfrom IPython.display import clear_output\nimport pickle\n\n\n# Constructing the optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\n\n# SGD\noptimizer = torch.optim.SGD(params, lr=0.0005,\n                            momentum=0.9, weight_decay=0.0005)\n\n# Learning Rate Scheduler\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n\n# Training for no. of Epochs\nnum_epochs = 1000\n\n\nlosses = []\nloss_box_reg = []\nloss_rpn_box_reg = []\nloss_classifier = []\nloss_objectness = []\n\nstat0 = []\nstat1 = []\nstat2 = []\nstat3 = []\nstat4 = []\nstat5 = []\nstat6 = []\nstat7 = []\nstat8 = []\nstat9 = []\nstat10 = []\nstat11 = []\n\n\nfor epoch in range(num_epochs):\n    # Engine.py's train_one_epoch function takes both images and targets. to(device)\n    # Metrics (metric_logger) was returned by train_one_epoch() in engine.py to get losses\n    metrics = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)\n    losses.append(float(str(metrics.meters['loss']).split(\" \")[0]))\n    loss_box_reg.append(float(str(metrics.meters['loss_box_reg']).split(\" \")[0]))\n    loss_rpn_box_reg.append(float(str(metrics.meters['loss_rpn_box_reg']).split(\" \")[0]))\n    loss_classifier.append(float(str(metrics.meters['loss_classifier']).split(\" \")[0]))\n    loss_objectness.append(float(str(metrics.meters['loss_objectness']).split(\" \")[0]))\n    \n    # Update the learning rate\n    lr_scheduler.step()\n\n    # Evaluate on the test dataset\n    # _ gives coco_evaL obj from coco_eval.py from CocoEvaluator()\n    _, metric_logger = evaluate(model, data_loader_test, device=device)\n    #Stat object is from pycocotools' self.stats in summarize()\n    #https:\/\/github.com\/cocodataset\/cocoapi\/blob\/master\/PythonAPI\/pycocotools\/cocoeval.py\n    stat = _.coco_eval['bbox'].stats\n    \n    #Append all stats\n    stat0.append(stat[0])\n    stat1.append(stat[1])\n    stat2.append(stat[2])\n    stat3.append(stat[3])\n    stat4.append(stat[4])\n    stat5.append(stat[5])\n    stat6.append(stat[6])\n    stat7.append(stat[7])\n    stat8.append(stat[8])\n    stat9.append(stat[9])\n    stat10.append(stat[10])\n    stat11.append(stat[11])\n    \n    \n    print('')\n    print('==================================================')\n    print('')\n\nprint(\"Done!\")","6dc1dbf1":"r,c = 9,2\nfig, ax = plt.subplots(nrows=r, ncols=c)\nfig.set_figheight(40)\nfig.set_figwidth(10)\nfig.subplots_adjust(left=14,right=15, top=6, bottom=5, hspace=1, wspace=1)\n\n\nax1 = plt.subplot(r, c, 1)\nax1.set_title(\"Losses\")\nax2 = plt.subplot(r, c, 2)\nax2.set_title(\"Loss Box Reg\")\nax3 = plt.subplot(r, c, 3)\nax3.set_title(\"Loss RPN Box Reg\")\nax4 = plt.subplot(r, c, 4)\nax4.set_title(\"Loss Classifier\")\nax5 = plt.subplot(r, c, 5)\nax5.set_title(\"Loss Objectness\")\nax6 = plt.subplot(r, c, 6)\nax6.set_title(\"(AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100\")\nax7 = plt.subplot(r, c, 7)\nax7.set_title(\"(AP) @[ IoU=0.50      | area=   all | maxDets=100\")\nax8 = plt.subplot(r, c, 8)\nax8.set_title(\"(AP) @[ IoU=0.75      | area=   all | maxDets=100\")\nax9 = plt.subplot(r, c, 9)\nax9.set_title(\"(AP) @[ IoU=0.50:0.95 | area= small | maxDets=100\")\nax10 = plt.subplot(r, c, 10)\nax10.set_title(\"(AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100\")\nax11 = plt.subplot(r, c, 11)\nax11.set_title(\"(AP) @[ IoU=0.50:0.95 | area= large | maxDets=100\")\nax12 = plt.subplot(r, c, 12)\nax12.set_title(\"(AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1\")\nax13 = plt.subplot(r, c, 13)\nax13.set_title(\"(AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10\")\nax14 = plt.subplot(r, c, 14)\nax14.set_title(\"(AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100\")\nax15 = plt.subplot(r, c, 15)\nax15.set_title(\"(AR) @[ IoU=0.50:0.95 | area= small | maxDets=100\")\nax16 = plt.subplot(r, c, 16)\nax16.set_title(\"(AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100\")\nax17 = plt.subplot(r, c, 17)\nax17.set_title(\"(AR) @[ IoU=0.50:0.95 | area= large | maxDets=100\")\n\nax1.plot(losses, 'b')\nax2.plot(loss_box_reg, 'b')\nax3.plot(loss_rpn_box_reg, 'b')\nax4.plot(loss_classifier, 'b')\nax5.plot(loss_objectness, 'b')\nax1.plot(losses, 'b')\nax2.plot(loss_box_reg, 'b')\nax3.plot(loss_rpn_box_reg, 'b')\nax4.plot(loss_classifier, 'b')\nax5.plot(loss_objectness, 'b')\nax6.plot(stat0, 'b')\nax7.plot(stat1, 'b')\nax8.plot(stat2, 'b')\nax9.plot(stat3, 'b')\nax10.plot(stat4, 'b')\nax11.plot(stat5, 'b')\nax12.plot(stat6, 'b')\nax13.plot(stat7, 'b')\nax14.plot(stat8, 'b')\nax15.plot(stat9, 'b')\nax16.plot(stat10, 'b')\nax17.plot(stat11, 'b')\nplt.show()","b183c906":"#Save the model\ntorch.save(model, r'.\/train1000.pkl')\n\ntorch.save(model.state_dict(), 'train1000.pth')\ntorch.save({\n    'epoch' : epoch,\n    \"model_state_dict\" : model.state_dict(),\n    'optimizer_state_dict' : optimizer.state_dict(),\n}, 'ckpt1000.pth')","84dc5937":"#Storing losses and stats in pickle format\nimport pickle\n\nwith open('vars1000.pickle', 'wb') as f:\n    pickle.dump([losses, loss_box_reg, loss_rpn_box_reg, loss_classifier, loss_objectness, stat0, stat1, stat2, stat3,\n stat4, stat5, stat6, stat7, stat8, stat9, stat10, stat11], f)","2868c66c":"#Uncomment to :\n\n#Load vars pickle file to calc mAP and other statistics\n\"\"\"\nwith open('vars400.pickle', 'rb') as f:\n    losses, loss_box_reg, loss_rpn_box_reg, loss_classifier, loss_objectness, stat0, stat1, stat2, stat3,\n    stat4, stat5, stat6, stat7, stat8, stat9, stat10, stat11 = pickle.load(f)\n\"\"\"","a25a4ffd":"def showbbox(model, img):\n    # The img entered is a tensor in the 0-1 range        \n    model.eval()\n    with torch.no_grad():\n        '''\n        prediction Like:\n        [{'boxes': tensor([[1221.7869,  523.7036, 1272.7373,  575.1018],\n        [ 192.8189,  527.5751,  240.7135,  589.8405],\n        [ 197.3745,  538.7914,  235.9153,  572.1550],\n        [ 195.1216,  533.9565,  238.6585,  578.0548],\n        [ 194.0861,  517.0943,  238.0777,  582.4178]], device='cuda:0'), \n        'labels': tensor([7, 7, 7, 8, 5], device='cuda:0'), \n        'scores': tensor([0.9792, 0.9036, 0.2619, 0.2407, 0.0575], device='cuda:0')}]\n        '''\n        prediction = model([img.to(device)])\n\n    print(prediction)\n    b = prediction[0]['boxes']\n    #print(b)\n    s = prediction[0]['scores']\n    #print(s)\n    \n    #Apply Non-maximum suppression:\n    keep = torchvision.ops.nms(b,s,0.1)\n    #print(keep)\n        \n    img = img.permute(1,2,0)  # C,H,W_H,W,C, for drawing\n    img = (img * 255).byte().data.cpu()  # * 255, float to 0-255\n    img = np.array(img)  # tensor \u2192 ndarray\n    #Convert np array img to right format.\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    #Class number coressponding to Classes\n    classes = { 0:' Speed limit (20km\/h)' ,\n    1:' Speed limit (30km\/h)' ,\n    2:' Speed limit (50km\/h)' ,\n    3:' Speed limit (60km\/h)' ,\n    4:' Speed limit (70km\/h)' ,\n    5:' Speed limit (80km\/h)' ,\n    6:' End of speed limit (80km\/h)' ,\n    7:' Speed limit (100km\/h)' ,\n    8:' Speed limit (120km\/h)' ,\n    9:' No passing' ,\n    10:' No passing veh over 3.5 tons' ,\n    11:' Right-of-way at intersection' ,\n    12:' Priority road' ,\n    13:' Yield' ,\n    14:' Stop' ,\n    15:' No vehicles' ,\n    16:' Veh > 3.5 tons prohibited' ,\n    17:' No entry' ,\n    18:' General caution' ,\n    19:' Dangerous curve left' ,\n    20:' Dangerous curve right' ,\n    21:' Double curve' ,\n    22:' Bumpy road' ,\n    23:' Slippery road' ,\n    24:' Road narrows on the right' ,\n    25:' Road work' ,\n    26:' Traffic signals' ,\n    27:' Pedestrians' ,\n    28:' Children crossing' ,\n    29:' Bicycles crossing' ,\n    30:' Beware of ice\/snow' ,\n    31:' Wild animals crossing' ,\n    32:' End speed + passing limits' ,\n    33:' Turn right ahead' ,\n    34:' Turn left ahead' ,\n    35:' Ahead only' ,\n    36:' Go straight or right' ,\n    37:' Go straight or left' ,\n    38:' Keep right' ,\n    39:' Keep left' ,\n    40:' Roundabout mandatory' ,\n    41:' End of no passing' ,\n    42:' End no passing veh > 3.5 tons'  }\n    \n    \n    for k in range(len(keep)):\n        xmin = round(prediction[0]['boxes'][k][0].item())\n        ymin = round(prediction[0]['boxes'][k][1].item())\n        xmax = round(prediction[0]['boxes'][k][2].item())\n        ymax = round(prediction[0]['boxes'][k][3].item())\n        \n        label = prediction[0]['labels'][k].item()\n        print(\"Label is: {}\\n===\\n(Xmin, Ymin, Xmax, Ymax) = ({}, {}, {}, {}) \\n===\".format(label, xmin, ymin, xmax, ymax))\n        \n        #color = list(np.random.random(size=3)*256)\n        colors = np.random.uniform(0, 255, size=(43, 3))\n        \n        if label in classes:\n            pt1 = (xmin, ymin)\n            pt2 = (xmax, ymax)\n            print(\"Class Label: \"+ classes[label])\n            score = prediction[0]['scores'][k].item()\n            print(\"Score: \"+ str(score))\n            print(\"\\n===============\\n\")\n            color = list(colors[label])\n            cv2.rectangle(img, pt1, pt2, color, thickness=2)\n            cv2.putText(img, classes[label]+\"-\"+str(round(score,2)), (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color,\n                        thickness=2)\n\n    plt.figure(figsize=(40,35))\n    plt.imshow(img)\n\nprint(\"Function Loaded\")","1f821909":"#Load saved model\nmodel1 = torch.load(r'.\/train400.pkl')\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel1 = nn.DataParallel(model, device_ids=[0,1,2,3])\nmodel1.to(device)\nprint(\"Model loaded!\")","7a6de22f":"#Test the model:\nimg, _ = dataset_test[99]\nshowbbox(model, img)","cd19a685":"#Show original image:\nimg = img.permute(1,2,0)  # C,H,W_H,W,C, for drawing\nimg = (img * 255).byte().data.cpu()  # * 255, float to 0-255\nimg = np.array(img)  # tensor \u2192 ndarray\nplt.figure(figsize=(30,30))\nplt.imshow(img)","2317cdbd":"#Check manually for a specific class of image\n\nfig, ax = plt.subplots(1,2)\n\nax1 = plt.subplot(1, 2, 1)\nax1.set_title(\"First class\")\n\nax2 = plt.subplot(1, 2, 2)\nax2.set_title(\"Second class\")\n\ni1 = cv2.cvtColor(cv2.imread('.\/train\/17\/00001.ppm'), cv2.COLOR_BGR2RGB)\n\ni2 = cv2.cvtColor(cv2.imread('.\/train\/38\/00001.ppm'), cv2.COLOR_BGR2RGB)\n\nax1.imshow(i1)\nax2.imshow(i2)","beb950c9":"#### Dictionary representation:\n\nHere we see that the Image with name (key) '00001.ppm' is stored as a list of lists containing the bounding box coordinates as the first 4 elements and the class as the last element.","605477c5":"#### Saving the losses and stats\n\nThe loss history and statistics obtained from the metric_logger and coco_eval objects so that we can use them for future evaluation.","0fbd7eee":"<a id='plots'><\/a>\n\n### 3) Evaluation\n\n#### Plotting the Stats\n\nAll the metrics from COCO Evaluation are recorded and plotted below.\n<br>\nTo understand them, refer to [COCO's Detection Evaluation](https:\/\/cocodataset.org\/#detection-eval). <\/br>\n\nTrue Positive (TP): When the IoU over predicted bounding box and ground truth is greater than or equal to the threshold.\nFalse Positive (FP): When the IoU over predicted bounding box and ground truth is less than threshold.\n\n\nAverage Precision ($AP$) is the number of true positives in the resulting bounding boxes.\nAverage Recall ($AR$) is the proportion of true positives out of possible positives.\n\n\nCOCO Evaluation mentions that they make no distinction between AP and mAP, AR and mAP. The AP and AR are averaged over multiple IoU values. They have used 10 IoU thresholds of .50:.05:.95 (start from 0.5 to 0.95 with a step size of 0.05) instead of computing over a single IoU of .50. Averaging ensures better localization.\n\nThe size of objects (area = small, medium, large) is mesasured in number of pixels.\n\n\n\nThe following can be inferred from the stats of the last iteration:\n1. The AP @ IoU=0.5:0.95 for area = large is 0.800 which means that when the model detects an object with large area, 80% of the time it matches the ground truth objects.\n\n2. The AR @IoU=0.5:0.95 for area = large is 0.800 which means that the model detects 80% of objects with large area, correctly.\n\n3. For area = medium and small, the model does not do well. This was probably caused by the small size of dataset and the insufficient number of examples for small and medium sized objects.\n\nThe following can be inferred from the loss plots:\n\n1. $Loss Box Reg$ is the measure of how tightly the model predicted the bounding box around the true object. It can be observed that the model works well to fit the bbox tightly to the object.\n\n2. $Loss RPN Box Reg$ measures the performance of network for retrieving the region proposals. The plot shows that further training or tweaking the hyperparameters may be required to decrease the loss. This may require more data to improve the results significantly.\n\n3. $Loss Classifier$ measures the performance of the object classification for detected bounding boxes. The plot shows that the model performs well in classifying the objects in the detected bounding boxes.\n\n4. $Loss Objectness$ measures the performance of network for retrieving bounding boxes which contain an object. We can infer that the model is detecting the object very well.","c546b679":"#### Data Distribution\n\nFrom the plot below, it is evident that the distribution of data is not equal. Some classes have very few examples.","4445c0af":"#### Loading the Model from file","14ad34f8":"### 1) Data Preparation","7d600445":"#### Drawing the bounding box for a prediction\n\nThe code below obtains the predictions made by the model in the format of a dictionary of boxes, labels and scores.\n\n##### Non maximum suppression:\nA number of proposals can be made by the model for the same object. We can filter the unwanted boxes by using Non-maximum suppression. Torchvision's library was used to perform NMS.\n\nNext, the Image is converted to the a numpy array from tensor format and then to the RGB format.\nA dictionary of class ID as keys and values as their string name is declared to display on the image.\n\nFinally, for every bounding box, a rectangle box and class text is displayed in the predicted image. ","683696cf":"# Traffic Sign Detection with Faster RCNN","00651370":"#### Handling Image data and annotations\n\n[Information about Dataset](http:\/\/benchmark.ini.rub.de\/?section=gtsdb&subsection=dataset) <\/br>\n[Download the Dataset](https:\/\/sid.erda.dk\/public\/archives\/ff17dc924eba88d5d01a807357d6614c\/published-archive.html)\n\nThe Train set and gt.txt files were used here.\n\n\nWe create a dictionary with the image names as key and annotations + class ID as value. If an image contains multiple objects then the coordinates along with the class is stored as list of lists in the same dictionary's key.","12193e9e":"#### Starting the Training\n\n1. Defining all the parameters required for training. (Using SGD as optimizer, Cosine Annealing\/Decreasing Warm Restarts as learning rate scheduler which decreases the initial learning rate set in a cosine manner until a restart; the lr is set back to the initial lr and the cycle repeats, number of epochs = 1000)\n2. Declaring all the variable to be retrieved from the COCO Evaluation metrics.\n3. We start train them model and evaluate the performance on test set.\n\nHere, train_one_epoch function in engine.py is used to do the training. The train_one_epoch function returns metric_logger object which we store in 'metrics'. We use the metric_logger's attributes (losses) to append into their respective variables.\n<\/br>\n\nThen, the evaluate method in CocoEvaluator() in coco_eval.py returns a coco_eval object which is stored in ' _ '. We use this coco_eval object to retrieve the stats attribute from pycocotools' library's summarize(). We append them to all the stat variables to later plot them after training.\n\nThere is a lot of gibberish of every iteration. Here's the link to jump to next cell: [Next Cell](#plots)","159a73c7":"#### Saving the model\n\nThe model is saved as pickle file and as .pth file with state dictionary so that it can be used later.","33da2ec2":"#### Importing libraries\n\nInstall pycocotools along with the other libraries imported.\n\npip install pycocotools\n\n\n##### Files needed:\n\nCOCO Evaluation metrics and some transform functions were used in this code. The [Torchvision Git repo](https:\/\/github.com\/pytorch\/vision\/tree\/master\/references\/detection) provides an API for COCO evaluation, transforms and other useful stuff used in object detection. These files are required to be placed in the root directory of the project.\n\n.\/coco_eval.py\n.\/coco_utils.py\n.\/engine.py\n.\/transforms.py\n.\/utils.py\n\n\nNote: I had some conflicts while importing these files. For example, utils.py. They probably won't work out of the box. I've also made a few simple changes to return evaluation metrics while training, you could download the files I used from my [Git repo](https:\/\/github.com\/haxothermic\/Traffic-Sign-Detection-with-Faster-R-CNN-using-PyTorch).","63b63a6f":"#### Testing the Model","ab52bd05":"#### Manual check for German Traffic Sign classes:\n\nTo check for the correctness of class, the below code was written to verify manually by changing the train subfolders' with class_id outputted by the model. This is done for 2 object that were detected above.","2847a341":"#### Data Augmentation using PyTorch's Transforms\n\nThe images are enhanced before being passed to the network. The images are transformed using the functions defined in the \"transforms.py\" file in [pytorch\/vision](https:\/\/github.com\/pytorch\/vision\/blob\/master\/references\/detection\/transforms.py).\n\nThe difference between original and transformed images are shown in the results.","85c38599":"#### Displaying the original image\n\nTo check the difference between processed image and original image.\n\nIt can be observed that using ImageNet's mean and standard deviation values, normalization was performed by default in Faster RCNN.","e81db4d4":"#### Defining Custom Dataset Class\n\nThe below code is to get all the required data from the dataset while reading. Here's what happens in the myDataset (torch.utils.data.Dataset) class:\n\n1. Initialize all the required variables: Root directory of images (path), transforms (boolean), imgs (images dir in root).\n2. According to PyTorch's documentation, the Dataset class should implement __getitem__ and __len__ methods. So we declare them.\n3. In the __getitem__ method, for each image we take the annotations and labels as input from the dictionary we created before. We store them in 'objects' variable.\n4. A 'targets' dictionary is then initialized to pass all the data to the model while training.\n5. 'area' declared for the evaluation metrics of COCO API. It separates the metric scores between small, medium and large boxes.<\/br> 'iscrowd=True' will ignore all instances with numerous objects in one image. <\/br> 'image_id' is an image identifier. It is unique between all images in the dataset and is used during evaluation.\n6. 'transforms=True' will call the transforms function to apply transformations.\n7. __len__ method returns the size of the Dataset.\n\nRefer to the [PyTorch's Object Detection Tutorial](https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html)","0a7d4f6b":"#### Loading the saved variables","9ccc0330":"#### Organizing the GTSDB Dataset:\n\nUnzipping the dataset zip, there are class folders with respective images and .ppm files. There is also the annotation file named as gt.txt\n\nI had copied all the .ppm files to a directory named \"train\/images\". \n\nIn the code below, the images that had annotations were only copied to a new folder.","6b2ba8af":"## 2) Training the Model\n\n#### Defining the Model and Loading Data\n\n1. The number of classes is 44 since there are 43 classes + background\n2. Then declare train and test dataset by calling the myDataset class which was defined earlier. \n3. Split the dataset into two 4:1 Train to Test approximately.\n4. Use PyTorch's DataLoader to load data.\n5. Define the model. Faster RCNN with a pretrained ResNet50 backbone network is used to finetune according to our dataset.\n6. Since I had 4 RTX 2080 GPU's available (thanks to my university, Asian Institute of Technology, Thailand and my Machine Learning Professor, Dr. Matthew N. Dailey), I was able to train on all GPUs parallely. "}}