{"cell_type":{"1924e424":"code","233cb0db":"code","3050bbf1":"code","2645fbf9":"code","37c79fb4":"code","0be790c0":"code","29382e5d":"code","90e67f20":"code","801dbed9":"code","f6451467":"code","68a01048":"code","9f532d27":"code","27810f47":"code","f7537efd":"code","3355f875":"code","192e2bbd":"code","a4391687":"code","a5db9d39":"code","11c47787":"code","b3ae8b62":"code","719b1313":"code","25ac9637":"code","b6fce43e":"code","73532e91":"code","a9e14412":"code","15f9f59d":"code","192426d2":"code","e5447e27":"code","907aa935":"code","7838413c":"code","a4817a25":"code","73e9f6b1":"code","9db333e8":"code","03dc9da0":"code","9345c839":"code","15dffc89":"code","33f8e568":"code","c53e0e52":"code","693c9ae3":"code","4155a9db":"code","98037013":"code","65753b54":"code","525b7282":"code","ab2f997a":"code","26cb1a2b":"code","3078e389":"code","a7f3510b":"code","489755e2":"code","5e5e5a42":"code","fde94c0b":"code","0d7022d5":"code","6d66a50f":"code","e21e4380":"code","4be2dc01":"code","eaf3a429":"code","8d6298bb":"code","3e0aa9e1":"code","69021fdd":"code","90893597":"code","3eca9ee8":"code","4392e349":"code","d5b7c6f1":"code","dba7c378":"code","22065b51":"code","7b287978":"code","4efcd2db":"code","9d351d4a":"code","84454f27":"code","100b5aad":"code","e7ea4702":"code","815e089c":"code","2f124fa1":"code","19a753a7":"code","4539c775":"code","07fc6c48":"code","fa58b28a":"code","026aa505":"code","05e7652d":"code","974b46da":"code","d9a3dd34":"code","c0e0299e":"code","c395b912":"code","c11ddd8e":"code","2c0edbbb":"code","af92910d":"code","427787a1":"code","2ff6e8b9":"code","0d6b9a61":"code","4c7187f7":"code","ffd0aba0":"code","73af800b":"code","1eee97fe":"code","96aca5ad":"code","9da193aa":"code","4f1705e5":"code","0a7d682d":"code","ed0954f9":"code","4b2de37b":"code","1c767d1f":"code","cd854a33":"code","84677075":"code","9d4008f5":"code","03fc8209":"code","8234f2ea":"code","b0a35e6d":"code","516a7ca8":"code","f9456d01":"code","5a54c275":"code","ddedd7a2":"code","f2dd0a29":"code","279eac02":"code","5d6d0c52":"code","5e408e61":"code","387cb2ca":"code","0af08d31":"code","957dab74":"code","bf49cbee":"code","11f54c78":"code","a3a2baa2":"code","9e587043":"markdown","4491d5ae":"markdown","5c0da51b":"markdown","d2fbc7b4":"markdown","51911519":"markdown","082308b9":"markdown","78f0d2a7":"markdown","0af26353":"markdown","19f876c1":"markdown","42c71cfd":"markdown","817df597":"markdown","2bb0ac82":"markdown","b3f562c5":"markdown","fb7f9aaf":"markdown","def22006":"markdown","5034bb59":"markdown","cc2a53cb":"markdown","0dc9f249":"markdown","e370c390":"markdown","8f2ebad1":"markdown","e1c06783":"markdown"},"source":{"1924e424":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n","233cb0db":"# essential libraries\n\n# for data visulization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#for data processing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import  OneHotEncoder as ohe\nfrom sklearn.preprocessing import StandardScaler as ss\nfrom sklearn.compose import ColumnTransformer as ct\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\n\n# for modeling estimators\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier as gbm\nfrom xgboost.sklearn import XGBClassifier\nimport lightgbm as lgb\n\n# for measuring performance\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import average_precision_score\nimport sklearn.metrics as metrics\nfrom xgboost import plot_importance\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\n\n#for tuning parameters\nfrom bayes_opt import BayesianOptimization\nfrom skopt import BayesSearchCV\nfrom eli5.sklearn import PermutationImportance\n\n# Misc.\nimport os\nimport time\nimport gc\nimport random\nfrom scipy.stats import uniform\nimport warnings\n","3050bbf1":"pd.options.display.max_columns = 150\n\n# Read in data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","2645fbf9":"train.head()\n","37c79fb4":"ids=test['Id']\ntrain.shape, test.shape","0be790c0":"train.info()   ","29382e5d":"test.head(10)","90e67f20":"sns.countplot(\"Target\", data=train)","801dbed9":"sns.countplot(x=\"r4t3\",hue=\"Target\",data=train)","f6451467":"sns.countplot(x=\"v18q\",hue=\"Target\",data=train)","68a01048":"sns.countplot(x=\"v18q1\",hue=\"Target\",data=train)\n","9f532d27":"sns.countplot(x=\"tamhog\",hue=\"Target\",data=train)\n","27810f47":"sns.countplot(x=\"hhsize\",hue=\"Target\",data=train)","f7537efd":"sns.countplot(x=\"abastaguano\",hue=\"Target\",data=train)","3355f875":"sns.countplot(x=\"noelec\",hue=\"Target\",data=train)","192e2bbd":"train.select_dtypes('object').head()","a4391687":"yes_no_map = {'no':0,'yes':1}\ntrain['dependency'] = train['dependency'].replace(yes_no_map).astype(np.float32)\ntrain['edjefe'] = train['edjefe'].replace(yes_no_map).astype(np.float32)\ntrain['edjefa'] = train['edjefa'].replace(yes_no_map).astype(np.float32)\n    ","a5db9d39":"yes_no_map = {'no':0,'yes':1}\ntest['dependency'] = test['dependency'].replace(yes_no_map).astype(np.float32)\ntest['edjefe'] = test['edjefe'].replace(yes_no_map).astype(np.float32)\ntest['edjefa'] = test['edjefa'].replace(yes_no_map).astype(np.float32)","11c47787":"train[[\"dependency\",\"edjefe\",\"edjefa\"]].describe()","b3ae8b62":" # Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","719b1313":"train['v18q1']     = train['v18q1'].fillna(0)\ntest['v18q1']      = test['v18q1'].fillna(0)\ntrain['v2a1']      = train['v2a1'].fillna(0)\ntest['v2a1']       = test['v2a1'].fillna(0)\n\ntrain['rez_esc']   = train['rez_esc'].fillna(0)\ntest['rez_esc']    = test['rez_esc'].fillna(0)\ntrain['SQBmeaned'] = train['SQBmeaned'].fillna(0)\ntest['SQBmeaned']  = test['SQBmeaned'].fillna(0)\ntrain['meaneduc']  = train['meaneduc'].fillna(0)\ntest['meaneduc']   = test['meaneduc'].fillna(0)","25ac9637":"#Checking for missing values again to confirm that no missing values present\n# Number of missing in each column\nmissing = pd.DataFrame(train.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","b6fce43e":"#Checking for missing values again to confirm that no missing values present\n# Number of missing in each column\nmissing = pd.DataFrame(test.isnull().sum()).rename(columns = {0: 'total'})\n\n# Create a percentage missing\nmissing['percent'] = missing['total'] \/ len(train)\n\nmissing.sort_values('percent', ascending = False).head(10)","73532e91":"train.drop(['Id','idhogar'], inplace = True, axis =1)\n\ntest.drop(['Id','idhogar'], inplace = True, axis =1)","a9e14412":"train.shape","15f9f59d":"test.shape","192426d2":"y = train.iloc[:,140]\nX = train.iloc[:,1:141]\nX.shape, y.shape","e5447e27":"my_imputer = SimpleImputer()\nX = my_imputer.fit_transform(X)\nscale = ss()\nX = scale.fit_transform(X)\n","907aa935":"#subjecting the same to test data\nmy_imputer = SimpleImputer()\ntest = my_imputer.fit_transform(test)\nscale = ss()\ntest = scale.fit_transform(test)\n","7838413c":"X.shape, y.shape,test.shape","a4817a25":"X_train, X_test, y_train, y_test = train_test_split(\n                                                    X,\n                                                    y,\n                                                    test_size = 0.2)","73e9f6b1":"from sklearn.ensemble import RandomForestClassifier as rf\nmodelrf = rf()\n\nstart = time.time()\nmodelrf = modelrf.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","9db333e8":"out_class = modelrf.predict(X_test)\nout_class","03dc9da0":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, out_class)\ncm\n\n","9345c839":"\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, out_class)\naccuracy","15dffc89":"bayes_cv_tuner = BayesSearchCV(\n    rf(\n       n_jobs = 2         \n      ),\n    \n    {\n        'n_estimators': (100, 500),         \n        'criterion': ['gini', 'entropy'],     \n        'max_depth': (4, 100),                \n        'max_features' : (10,64),            \n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   \n    },\n\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","33f8e568":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","c53e0e52":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","693c9ae3":"#using the best params \nmodelrfTuned=rf(criterion=\"entropy\",\n               max_depth=100,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=173)","4155a9db":"#fit the data in the model\nmodelrfTuned = modelrfTuned.fit(X_train, y_train)\n\n#Predict\ny_rf=modelrfTuned.predict(X_test)\ny_rf","98037013":"#predict for the test data\ny_rf_test=modelrfTuned.predict(test)\ny_rf_test","65753b54":"#  Get what average accuracy was acheived during cross-validation\naccuracy = bayes_cv_tuner.best_score_\naccuracy","525b7282":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)\n","ab2f997a":"#  what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","26cb1a2b":"modelknn = KNeighborsClassifier(n_neighbors=4)","3078e389":"start = time.time()\nmodelknn = modelknn.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","a7f3510b":"out_class = modelrf.predict(X_test)\nout_class","489755e2":"(out_class == y_test).sum()\/y_test.size ","5e5e5a42":"#from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, out_class)\ncm","fde94c0b":"#from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, out_class)\naccuracy","0d7022d5":"bayes_cv_tuner = BayesSearchCV(\n    \n    KNeighborsClassifier(\n       n_neighbors=4        \n      ),\n    {\"metric\": [\"euclidean\", \"cityblock\"]},\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n   )","6d66a50f":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","e21e4380":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","4be2dc01":"modelneighTuned = KNeighborsClassifier(n_neighbors=4,metric=\"cityblock\")","eaf3a429":"#Fit to the model\nmodelneighTuned = modelneighTuned.fit(X_train, y_train)\n\n# Predict \ny_neigh=modelneighTuned.predict(X_test)\n","8d6298bb":"# predict for the test data\n\ny_neigh_test=modelneighTuned.predict(test)","3e0aa9e1":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","69021fdd":"modelgbm=gbm()\nstart = time.time()\nmodelgbm = modelgbm.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","90893597":"out_class = modelgbm.predict(X_test)\n\nout_class","3eca9ee8":"(out_class == y_test).sum()\/y_test.size ","4392e349":"#from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, out_class)\ncm","d5b7c6f1":"#from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, out_class)\naccuracy","dba7c378":"bayes_cv_tuner = BayesSearchCV(\n\n    gbm(\n      ),\n    {\n        'n_estimators': (100, 500),           \n        \n        'max_depth': (4, 100),               \n        'max_features' : (10,64),            \n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   \n    },\n    n_iter=32,            # How many points to sample\n    cv = 2                # Number of cross-validation folds\n)","22065b51":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","7b287978":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","4efcd2db":"modelgbmTuned=gbm(\n               max_depth=100,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=500)","9d351d4a":"#Re fitting the model with optimized parameters\nmodelgbmTuned = modelgbmTuned.fit(X_train, y_train)\n\n# Predict\ny_gbm=modelgbmTuned.predict(X_test)\n","84454f27":"# Predicting for the test data\ny_gbm_test=modelgbmTuned.predict(test)","100b5aad":"#  Get what average accuracy was acheived during cross-validation\naccuracy = bayes_cv_tuner.best_score_\naccuracy","e7ea4702":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","815e089c":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","2f124fa1":"#### accracy is high 100% before and after optimization","19a753a7":"model_etf = ExtraTreesClassifier()","4539c775":"start = time.time()\nmodel_etf = model_etf.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","07fc6c48":"out_class = model_etf.predict(X_test)\n\nout_class","fa58b28a":"(out_class == y_test).sum()\/y_test.size","026aa505":"#from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, out_class)\ncm","05e7652d":"#from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, out_class)\naccuracy\n","974b46da":"bayes_cv_tuner = BayesSearchCV(\n\n    ExtraTreesClassifier( ),   \n    {   'n_estimators': (100, 500),           \n        'criterion': ['gini', 'entropy'],     \n        'max_depth': (4, 100),                \n        'max_features' : (10,64),             \n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   \n    },\n\n    n_iter=32,            # How many points to sample\n    cv = 2            # Number of cross-validation folds\n)\n","d9a3dd34":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","c0e0299e":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","c395b912":"#using the best parameters\nmodeletfTuned=ExtraTreesClassifier(criterion=\"entropy\",\n               max_depth=4,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=100)","c11ddd8e":"#Fit the model\nmodeletfTuned = modeletfTuned.fit(X_train, y_train)\n\n# predict\ny_etf=modeletfTuned.predict(X_test)","2c0edbbb":"#predict with the test data\ny_etftest=modeletfTuned.predict(test)","af92910d":"#  Get what average accuracy was acheived during cross-validation\naccuracy = bayes_cv_tuner.best_score_\naccuracy","427787a1":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","2ff6e8b9":"#### accracy increased from 98.27% to 100%","0d6b9a61":"model_xgb=XGBClassifier()","4c7187f7":"start = time.time()\nmodel_xgb = model_xgb.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","ffd0aba0":"out_class = model_xgb.predict(X_test)\n\nout_class","73af800b":"(out_class == y_test).sum()\/y_test.size ","1eee97fe":"#from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, out_class)\ncm","96aca5ad":"\n#from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, out_class)\naccuracy","9da193aa":"bayes_cv_tuner = BayesSearchCV(\n\n    XGBClassifier(\n       n_jobs = 2         \n      ),\n    {\n        'n_estimators': (100, 500),           \n        'criterion': ['gini', 'entropy'],     \n        'max_depth': (4, 100),                \n        'max_features' : (10,64),             \n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   \n    },\n\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","4f1705e5":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","0a7d682d":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","ed0954f9":"modelxgbTuned=XGBClassifier(criterion=\"entropy\",\n               max_depth=51,\n               max_features=16,\n               min_weight_fraction_leaf=0.2150244429465713,\n               n_estimators=355)","4b2de37b":"# refit the model with optimized data\nmodelxgbTuned = modelxgbTuned.fit(X_train, y_train)\n\n#predict\ny_xgb=modelxgbTuned.predict(X_test)","1c767d1f":"y_xgbtest=modelxgbTuned.predict(test)","cd854a33":"#  Get what average accuracy was acheived during cross-validation\naccuracy = bayes_cv_tuner.best_score_\naccuracy","84677075":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","9d4008f5":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","03fc8209":"model_lgb = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=None, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 14, subsample = 0.96)\n","8234f2ea":"start = time.time()\nmodel_lgb = model_lgb.fit(X_train, y_train)\nend = time.time()\n(end-start)\/60","b0a35e6d":"out_class = model_lgb.predict(X_test)\n\nout_class","516a7ca8":"(out_class == y_test).sum()\/y_test.size ","f9456d01":"#from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, out_class)\ncm\n","5a54c275":"#from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, out_class)\naccuracy","ddedd7a2":"bayes_cv_tuner = BayesSearchCV(\n\n    lgb.LGBMClassifier(\n       n_jobs = 2         \n      ),\n\n    {\n        'n_estimators': (100, 500),           \n        'criterion': ['gini', 'entropy'],     \n        'max_depth': (4, 100),                \n        'max_features' : (10,64),             \n        'min_weight_fraction_leaf' : (0,0.5, 'uniform')   \n    },\n    n_iter=32,            # How many points to sample\n    cv = 3                # Number of cross-validation folds\n)","f2dd0a29":"# Start optimization\nbayes_cv_tuner.fit(X_train, y_train)","279eac02":"#  Get list of best-parameters\nbayes_cv_tuner.best_params_","5d6d0c52":"modellgbTuned = lgb.LGBMClassifier(criterion=\"gini\",\n               max_depth=23,\n               max_features=48,\n               min_weight_fraction_leaf=0.4939249242565817,\n               n_estimators=437)","5e408e61":"# Re fit the model\nmodellgbTuned = modellgbTuned.fit(X_train, y_train)\n\n# Predict\ny_lgb=modellgbTuned.predict(X_test)","387cb2ca":"# predict for the test data\ny_lgb_test=modellgbTuned.predict(test)\n","0af08d31":"#  Get what average accuracy was acheived during cross-validation\naccuracy = bayes_cv_tuner.best_score_\naccuracy","957dab74":"#  What accuracy is available on test-data\nbayes_cv_tuner.score(X_test, y_test)","bf49cbee":"#  And what all sets of parameters were tried?\nbayes_cv_tuner.cv_results_['params']","11f54c78":"## looking at the accuracy before and after tuning all the models I am choosing Random Forest with following params\n## as it was giving the optimum accuracy\nmodelrfTuned=rf(criterion=\"entropy\",\n               max_depth=100,\n               max_features=64,\n               min_weight_fraction_leaf=0.0,\n               n_estimators=173)\n\n#fit the data in the model\nmodelrfTuned = modelrfTuned.fit(X_train, y_train)\n\n#predict for the test data\ny_rf_test=modelrfTuned.predict(test)\ny_rf_test","a3a2baa2":"test_final = pd.read_csv('..\/input\/test.csv')\n\ntest_final['Pred_Out'] = y_rf_test.tolist()\n\ntest_final.head(10)\n","9e587043":"### Modelling with KNeighborsClassifier\n","4491d5ae":"#### accracy is the same 100%","5c0da51b":"### final selected features fr modelling","d2fbc7b4":"   ## Modelling with ExtraTreeClassifier","51911519":"### Dividing the data into predictors and target","082308b9":"### Reading the Test file again and apending the predicted out .put column \n#### Mahine learning algorithm used Random Forest \n#### Tuning done using Bayesian Optimization technique","78f0d2a7":"### Modelling with GradientBoostingClassifier\n","0af26353":"#### Accuray reduced from 96.44% to 89.43%","19f876c1":"#### Accuracy is same 100%","42c71cfd":"### Performing tuning using Bayesian Optimization.","817df597":"### Performing tuning using Bayesian Optimization.\n","2bb0ac82":"### Performing tuning using Bayesian Optimization.\u00b6\n","b3f562c5":"## Modlelling with Random Forest","fb7f9aaf":"### Modelling with XGBClassifier","def22006":"#### Accuracy improved from ____96.44%____ to __100%____\n","5034bb59":"### Modelling with Light Gradient Booster","cc2a53cb":"### splitting the data into train and test ","0dc9f249":"### Performing tuning using Bayesian Optimization.\n","e370c390":"### Performing tuning using Bayesian Optimization.\n","8f2ebad1":"### Performing tuning using Bayesian Optimization.","e1c06783":"### Scaling numeric features & applying PCA to reduce features"}}