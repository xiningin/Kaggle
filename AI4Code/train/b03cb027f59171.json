{"cell_type":{"5950cd57":"code","d77fdd9e":"code","6edbfb2a":"code","2446965b":"code","215b467a":"code","57410cae":"code","51595858":"code","30aac1bf":"code","697cdd02":"code","16b3ffdc":"code","462459b0":"code","ee41f631":"code","09c7d9e0":"code","1010853e":"code","cf10017b":"code","8a48b92d":"code","a483ec17":"code","02e9c410":"code","e9a4cec9":"code","c937ecc4":"code","b9deb6ca":"code","e389804a":"code","76ed1b1c":"code","0188b181":"code","17108186":"code","5d18f2e5":"code","5c04aa8a":"code","b49b1825":"code","cc662e97":"code","d09b853b":"code","2a95a177":"code","8023bd28":"code","bd6b0121":"code","5e289ea4":"code","2f4eef4c":"code","446679db":"code","70916f52":"code","8d6bdd80":"code","86da2cd2":"code","aa7bb636":"code","f462c587":"code","3f5e7f2b":"code","5f84fc5e":"code","4a1bda1e":"code","fe4a2d6e":"code","b1385e8c":"code","bd43ab2a":"code","1c698ef6":"code","85af01a9":"code","86ff5d91":"code","75ce01db":"code","2462e66c":"code","d89352c2":"code","2e2c2872":"code","ec3bf562":"code","f6ab7888":"code","ef2f81be":"code","a50dbd89":"code","6d3a0fb1":"code","e625bd74":"code","60d34966":"code","1cce6064":"code","b6e72a80":"code","4ca0cf4e":"code","2dc71efa":"code","a12b16e4":"code","d4c2ee16":"code","61703ab1":"code","083c89e1":"code","6e3884ce":"code","0d3f4fd6":"code","02832f73":"code","51998a0a":"code","de95dfcd":"code","30b50db6":"code","f3d792b5":"code","f6e932b3":"code","397607e6":"code","bc4111e1":"code","55955f51":"code","a3b2678f":"code","ca92798f":"code","4419b9ac":"code","98468d90":"code","8e8abd6f":"code","da2b30c7":"code","29994a8c":"code","bbec9687":"code","96c83013":"code","62f00949":"code","6bbad59e":"code","62b03dce":"code","9c29ec2d":"code","7dc343ac":"code","2a0ea69f":"code","4eeeeee4":"markdown","e3499345":"markdown","7ae5d784":"markdown","c1fa2012":"markdown","86d15de7":"markdown","305cd28c":"markdown","10db9868":"markdown","7a525b4c":"markdown","f4638205":"markdown","ef04375c":"markdown","f47147d7":"markdown","fd473ed9":"markdown","ae1f0cd8":"markdown","eb3a5b60":"markdown","732bc14a":"markdown","c6d5f046":"markdown","af90be9f":"markdown","357675c5":"markdown","b06ee123":"markdown","12121b23":"markdown","6d65acfb":"markdown","8bbb6c0d":"markdown","b39ba6ef":"markdown","6fd6328c":"markdown","5461be8f":"markdown","ab14f686":"markdown","9cb76f68":"markdown","643d27b1":"markdown","c9df1df4":"markdown","6cc1e98a":"markdown","2fcbaf30":"markdown","d34be38b":"markdown","19306b72":"markdown","15018198":"markdown","4e714085":"markdown","1c3a014d":"markdown","db406f68":"markdown","29985f1b":"markdown","6b9c7405":"markdown","a8044c63":"markdown"},"source":{"5950cd57":"# Libraries\nimport numpy as np\nimport pandas as pd\nimport os\nfrom datetime import datetime\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn import preprocessing\nimport string\n#import itertools\n#from itertools import product","d77fdd9e":"#setting working directory\n#os.chdir(\"\/home\/srishti\/Srishti Saha- backup\/misc\/personal\/kickstarter_projects\")","6edbfb2a":"# read in data\nkickstarters_2017 = pd.read_csv(\"..\/input\/ks-projects-201801.csv\")\nkickstarters_2017.head()","2446965b":"#printing all summary of the kickstarter data\n#this will give the dimensions of data set : (rows, columns)\nprint(kickstarters_2017.shape)\n#columns and data types\nprint(kickstarters_2017.info())\n#basic stats of columns\nprint(kickstarters_2017.describe())\n#number of unique values in all columns\nprint(kickstarters_2017.nunique())","215b467a":"#Distribution of data across state\npercent_success = round(kickstarters_2017[\"state\"].value_counts() \/ len(kickstarters_2017[\"state\"]) * 100,2)\n\nprint(\"State Percent: \")\nprint(percent_success)","57410cae":"#renaming column usd_pledged as there is no '_' in the actual dataset variable name\ncol_names_prev=list(kickstarters_2017)\ncol_names_new= ['ID',\n 'name',\n 'category',\n 'main_category',\n 'currency',\n 'deadline',\n 'goal',\n 'launched',\n 'pledged',\n 'state',\n 'backers',\n 'country',\n 'usd_pledged',\n 'usd_pledged_real',\n 'usd_goal_real']\nkickstarters_2017.columns= col_names_new","51595858":"#segregating the variables as categorical and constinuous\ncat_vars=[ 'category', 'main_category', 'currency','country']\ncont_vars=['goal', 'pledged', 'backers','usd_pledged','usd_pledged_real','usd_goal_real']","30aac1bf":"#correlation of continuous variables\nkickstarters_2017[cont_vars].corr()","697cdd02":"#setting unique ID as index of the table\n#this is because the ID column will not be used in the algorithm. yet it is needed to identify the project\ndf_kick= kickstarters_2017.set_index('ID')","16b3ffdc":"# Filtering only for successful and failed projects\nkick_projects = df_kick[(df_kick['state'] == 'failed') | (df_kick['state'] == 'successful')]\n#converting 'successful' state to 1 and failed to 0\nkick_projects['state'] = (kick_projects['state'] =='successful').astype(int)\nprint(kick_projects.shape)","462459b0":"#checking distribution of projects across various main categories\nkick_projects.groupby(['main_category','state']).size()\n#kick_projects.groupby(['category','state']).size()","ee41f631":"#correlation of continuous variables with the dependent variable\nkick_projects[['goal', 'pledged', 'backers','usd_pledged','usd_pledged_real','usd_goal_real','state']].corr()","09c7d9e0":"#creating derived metrics\/ features\n\n#converting the date columns from string to date format\n#will use it to derive the duration of the project\nkick_projects['launched_date'] = pd.to_datetime(kick_projects['launched'], format='%Y-%m-%d %H:%M:%S')\nkick_projects['deadline_date'] = pd.to_datetime(kick_projects['deadline'], format='%Y-%m-%d %H:%M:%S')","1010853e":"kick_projects= kick_projects.sort_values('launched_date',ascending=True)","cf10017b":"kick_projects.head()","8a48b92d":"#creating features from the project name\n\n#length of name\nkick_projects['name_len'] = kick_projects.name.str.len()\n\n# presence of !\nkick_projects['name_exclaim'] = (kick_projects.name.str[-1] == '!').astype(int)\n\n# presence of !\nkick_projects['name_question'] = (kick_projects.name.str[-1] == '?').astype(int)\n\n# number of words in the name\nkick_projects['name_words'] = kick_projects.name.apply(lambda x: len(str(x).split(' ')))\n\n# if name is uppercase\nkick_projects['name_is_upper'] = kick_projects.name.str.isupper().astype(float)","a483ec17":"# normalizing goal by applying log\nkick_projects['goal_log'] = np.log1p(kick_projects.goal)\n#creating goal features to check what range goal lies in\nkick_projects['Goal_10'] = kick_projects.goal.apply(lambda x: x \/\/ 10)\nkick_projects['Goal_1000'] = kick_projects.goal.apply(lambda x: x \/\/ 1000)\nkick_projects['Goal_100'] = kick_projects.goal.apply(lambda x: x \/\/ 100)\nkick_projects['Goal_500'] = kick_projects.goal.apply(lambda x: x \/\/ 500)","02e9c410":"#features from date column\nkick_projects['duration']=(kick_projects['deadline_date']-kick_projects['launched_date']).dt.days\n#the idea for deriving launched quarter month year is that perhaps projects launched in a particular year\/ quarter\/ month might have a low success rate\nkick_projects['launched_quarter']= kick_projects['launched_date'].dt.quarter\nkick_projects['launched_month']= kick_projects['launched_date'].dt.month\nkick_projects['launched_year']= kick_projects['launched_date'].dt.year\nkick_projects['launched_week']= kick_projects['launched_date'].dt.week","e9a4cec9":"#additional features from goal, pledge and backers columns\nkick_projects.loc[:,'goal_reached'] = kick_projects['pledged'] \/ kick_projects['goal'] # Pledged amount as a percentage of goal.\n#The above field will be used to compute another metric\n# In backers column, impute 0 with 1 to prevent undefined division.\nkick_projects.loc[kick_projects['backers'] == 0, 'backers'] = 1 \nkick_projects.loc[:,'pledge_per_backer'] = kick_projects['pledged'] \/ kick_projects['backers'] # Pledged amount per backer.","c937ecc4":"#will create percentile buckets for the goal amount in a category\nkick_projects['goal_cat_perc'] =  kick_projects.groupby(['category'])['goal'].transform(\n                     lambda x: pd.qcut(x, [0, .35, .70, 1.0], labels =[1,2,3]))\n\n#will create percentile buckets for the duration in a category\nkick_projects['duration_cat_perc'] =  kick_projects.groupby(['category'])['duration'].transform(\n                     lambda x: pd.qcut(x, [0, .35, .70, 1.0], labels =False, duplicates='drop'))","b9deb6ca":"#creating a metric to see number of competitors for a given project in a given quarter\n#number of participants in a given category, that launched in the same year and quarter and in the same goal bucket\nks_particpants_qtr=kick_projects.groupby(['category','launched_year','launched_quarter','goal_cat_perc']).count()\nks_particpants_qtr=ks_particpants_qtr[['name']]\n#since the above table has all group by columns created as index, converting them into columns\nks_particpants_qtr.reset_index(inplace=True)\n\n#creating a metric to see number of competitors for a given project in a given month\n#number of participants in a given category, that launched in the same year and month and in the same goal bucket\nks_particpants_mth=kick_projects.groupby(['category','launched_year','launched_month','goal_cat_perc']).count()\nks_particpants_mth=ks_particpants_mth[['name']]\n#since the above table has all group by columns created as index, converting them into columns\nks_particpants_mth.reset_index(inplace=True)\n\n#creating a metric to see number of competitors for a given project in a given week\n#number of participants in a given category, that launched in the same year and week and in the same goal bucket\nks_particpants_wk=kick_projects.groupby(['category','launched_year','launched_week','goal_cat_perc']).count()\nks_particpants_wk=ks_particpants_wk[['name']]\n#since the above table has all group by columns created as index, converting them into columns\nks_particpants_wk.reset_index(inplace=True)","e389804a":"#renaming columns of the derived table\ncolmns_qtr=['category', 'launched_year', 'launched_quarter', 'goal_cat_perc', 'participants_qtr']\nks_particpants_qtr.columns=colmns_qtr\n\ncolmns_mth=['category', 'launched_year', 'launched_month', 'goal_cat_perc', 'participants_mth']\nks_particpants_mth.columns=colmns_mth\n\ncolmns_wk=['category', 'launched_year', 'launched_week', 'goal_cat_perc', 'participants_wk']\nks_particpants_wk.columns=colmns_wk","76ed1b1c":"#merging the particpants column into the base table\nkick_projects = pd.merge(kick_projects, ks_particpants_qtr, on = ['category', 'launched_year', 'launched_quarter','goal_cat_perc'], how = 'left')\nkick_projects = pd.merge(kick_projects, ks_particpants_mth, on = ['category', 'launched_year', 'launched_month','goal_cat_perc'], how = 'left')\nkick_projects = pd.merge(kick_projects, ks_particpants_wk, on = ['category', 'launched_year', 'launched_week','goal_cat_perc'], how = 'left')","0188b181":"#creating 2 metrics to get average pledge per backer for a category in a year according to the goal bucket it lies in and the success rate ie average pledged to goal ratio for the category and goal bucket in this year\n#using pledge_per_backer (computed earlier) and averaging it by category in a launch year\nks_ppb_goal=pd.DataFrame(kick_projects.groupby(['category','launched_year','goal_cat_perc'])['pledge_per_backer','goal_reached'].mean())\n#since the above table has all group by columns created as index, converting them into columns\nks_ppb_goal.reset_index(inplace=True)\n#renaming column\nks_ppb_goal.columns= ['category','launched_year','goal_cat_perc','avg_ppb_goal','avg_success_rate_goal']\n\n#creating a metric: the success rate ie average pledged to goal ratio for the category in this year\nks_ppb_duration=pd.DataFrame(kick_projects.groupby(['category','launched_year','duration_cat_perc'])['goal_reached'].mean())\n#since the above table has all group by columns created as index, converting them into columns\nks_ppb_duration.reset_index(inplace=True)\n#renaming column\nks_ppb_duration.columns= ['category','launched_year','duration_cat_perc','avg_success_rate_duration']","17108186":"#merging the particpants column into the base table\nkick_projects = pd.merge(kick_projects, ks_ppb_goal, on = ['category', 'launched_year','goal_cat_perc'], how = 'left')\nkick_projects = pd.merge(kick_projects, ks_ppb_duration, on = ['category', 'launched_year','duration_cat_perc'], how = 'left')","5d18f2e5":"#creating 2 metrics: mean and median goal amount\nmedian_goal_cat=pd.DataFrame(kick_projects.groupby(['category','launched_year','duration_cat_perc'])['goal'].median())\n#since the above table has all group by columns created as index, converting them into columns\nmedian_goal_cat.reset_index(inplace=True)\n#renaming column\nmedian_goal_cat.columns= ['category','launched_year','duration_cat_perc','median_goal_year']\n\nmean_goal_cat=pd.DataFrame(kick_projects.groupby(['category','launched_year','duration_cat_perc'])['goal'].mean())\n#since the above table has all group by columns created as index, converting them into columns\nmean_goal_cat.reset_index(inplace=True)\n#renaming column\nmean_goal_cat.columns= ['category','launched_year','duration_cat_perc','mean_goal_year']","5c04aa8a":"#merging the particpants column into the base table\nkick_projects = pd.merge(kick_projects, median_goal_cat, on = ['category', 'launched_year','duration_cat_perc'], how = 'left')\nkick_projects = pd.merge(kick_projects, mean_goal_cat, on = ['category', 'launched_year','duration_cat_perc'], how = 'left')","b49b1825":"print(kick_projects.shape)\nkick_projects[:3]","cc662e97":"# replacing all 'N,0\"' values in the country column with 'NZERO' to avoid discrepancies while one hot encoding\nkick_projects = kick_projects.replace({'country': 'N,0\"'}, {'country': 'NZERO'}, regex=True)","d09b853b":"list(kick_projects)","2a95a177":"#selecting the needed fields only\n#this will lead to the final features list\n\n#creating a list of columns to be dropped\ndrop_columns= ['name','launched','deadline','launched_date','deadline_date','pledged','backers','usd_pledged','usd_pledged_real','pledge_per_backer','goal_reached']\n#dropping columns above\nkick_projects.drop(drop_columns, axis=1, inplace=True)","8023bd28":"#these functions will be used on the textual column entries to remove '&','-' or white spaces\ndef replace_ampersand(val):\n    if isinstance(val, str):\n        return(val.replace('&', 'and'))\n    else:\n        return(val)\n\ndef replace_hyphen(val):\n    if isinstance(val, str):\n        return(val.replace('-', '_'))\n    else:\n        return(val)    \n    \ndef remove_extraspace(val):\n        if isinstance(val, str):\n            return(val.strip())\n        else:\n            return(val) \n\ndef replace_space(val):\n        if isinstance(val, str):\n            return(val.replace(' ', '_'))\n        else:\n            return(val)         ","bd6b0121":"#apply those functions to all cat columns\n#this will remove special characters from the character columns.\n#Since these fields will be one-hot encoded, the column names so derived should be compatible with the requied format\nkick_projects['category'] = kick_projects['category'].apply(remove_extraspace)\nkick_projects['category'] = kick_projects['category'].apply(replace_ampersand)\nkick_projects['category'] = kick_projects['category'].apply(replace_hyphen)\nkick_projects['category'] = kick_projects['category'].apply(replace_space)\n\nkick_projects['main_category'] = kick_projects['main_category'].apply(remove_extraspace)\nkick_projects['main_category'] = kick_projects['main_category'].apply(replace_ampersand)\nkick_projects['main_category'] = kick_projects['main_category'].apply(replace_hyphen)\nkick_projects['main_category'] = kick_projects['main_category'].apply(replace_space)","5e289ea4":"#missing value treatment\n# Check for nulls.\nkick_projects.isnull().sum()","2f4eef4c":"#dropping all rows that have any nulls\nkick_projects=kick_projects.dropna() ","446679db":"# Check for nulls again.\nkick_projects.isnull().sum()","70916f52":"#creating a backup copy of the dataset\nkick_projects_copy= kick_projects.copy()\n\nkick_projects_copy[:5]","8d6bdd80":"for c in kick_projects.columns:\n    #this gives us the list of columns and the respective data types\n    col_type = kick_projects[c].dtype\n    #looking through all categorical columns in the list above\n    if col_type == 'object' :\n        a=kick_projects[c].unique()\n        keys= range(a.shape[0])\n        #initiating a dictionary\n        diction={}\n        for idx,val in enumerate(a):\n        #looping through to create the dictionary with mappings\n            diction[idx] = a[idx]\n        #the above step maps integers to the values in the column\n        # hence inverting the key-value pairs\n        diction = {v: k for k, v in diction.items()}\n        print(diction)\n        # creating a dictionary for mapping the values to integers\n        kick_projects_copy[c] = [diction[item] for item in kick_projects_copy[c]] \n        # converting data type to 'category'\n        kick_projects_copy[c] = kick_projects_copy[c].astype('category')","86da2cd2":"# One-Hot encoding to convert categorical columns to numeric\nprint('start one-hot encoding')\n\nkick_projects_ip = pd.get_dummies(kick_projects, prefix = [ 'category', 'main_category', 'currency','country'],\n                             columns = [ 'category', 'main_category', 'currency','country'])\n    \n#this will have created 1-0 flag columns (like a sparse matrix)    \nprint('ADS dummy columns made')","aa7bb636":"#creating 2 arrays: features and response\n\n#features will have all independent variables\nfeatures=list(kick_projects_ip)\nfeatures.remove('state')\n#response has the target variable\nresponse= ['state']","f462c587":"#creating a backup copy of the input dataset\nkick_projects_ip_copy= kick_projects_ip.copy()","3f5e7f2b":"kick_projects_ip[features].shape","5f84fc5e":"# normalize the data attributes\nkick_projects_ip_scaled_ftrs = pd.DataFrame(preprocessing.normalize(kick_projects_ip[features]))\nkick_projects_ip_scaled_ftrs.columns=list(kick_projects_ip[features])","4a1bda1e":"kick_projects_ip_scaled_ftrs[:3]\n#kick_projects_ip[features].shape","fe4a2d6e":"#creating test and train dependent and independent variables\n#Split the data into test and train (30-70: random sampling)\n#will be using the scaled dataset to split \ntrain_ind, test_ind, train_dep, test_dep = train_test_split(kick_projects_ip_scaled_ftrs, kick_projects_ip[response], test_size=0.3, random_state=0)","b1385e8c":"from xgboost import XGBClassifier\nfrom sklearn import model_selection","bd43ab2a":"#def timer(start_time=None):\n#    if not start_time:\n#        start_time = datetime.now()\n#        return start_time\n#    elif start_time:\n#        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n#        tmin, tsec = divmod(temp_sec, 60)\n#        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","1c698ef6":"# defining the XGBoost model\nxgb_model = XGBClassifier(\n n_estimators= 1200,\n learning_rate= 0.08,\n max_depth= 5,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic'\n )\n\n\n# Tried doing a grid search but commented it out for the amount of time it takes\n## Defining parameters\n#n_estimators = [500,1000, 1200]\n#learning_rate = [0.0001, 0.01,0.1, 0.3]\n#param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)\n\n## Starting stratified Kfold\n#kfold = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=100)\n#random_search = model_selection.RandomizedSearchCV(xgb_model, param_grid, scoring=\"neg_log_loss\", n_jobs=4, cv=kfold.split(train_ind[features], train_dep[response]), n_iter=12)\n\n## fitting the random search\n#start_time = timer(None)\n#random_result = random_search.fit(train_ind[features], train_dep[response])\n#timer(start_time) # timing ends here for \"start_time\" variable","85af01a9":"# model fitting\nxgb_model=xgb_model.fit(train_ind[features], train_dep[response])","86ff5d91":"# Predict the on the train_data\ntest_ind[\"Pred_state_XGB_2\"] = xgb_model.predict(test_ind[features])\n\n# Predict the on the train_data\ntrain_ind[\"Pred_state_XGB_2\"] = xgb_model.predict(train_ind[features])\n\n# Predict the on the train_data\nkick_projects_ip[\"Pred_state_XGB_2\"] = xgb_model.predict(kick_projects_ip_scaled_ftrs)","75ce01db":"print (\"Test Accuracy :: \",accuracy_score(test_dep[response], xgb_model.predict(test_ind[features])))\nprint (\"Train Accuracy :: \",accuracy_score(train_dep[response], xgb_model.predict(train_ind[features])))\nprint (\"Complete Accuracy  :: \",accuracy_score(kick_projects_ip[response], xgb_model.predict(kick_projects_ip_scaled_ftrs)))\nprint (\" Confusion matrix of complete data is\", confusion_matrix(kick_projects_ip[response],kick_projects_ip[\"Pred_state_XGB_2\"]))","2462e66c":"## Feature importances\nftr_imp=zip(features,xgb_model.feature_importances_)","d89352c2":"for values in ftr_imp:\n    print(values)","2e2c2872":"# creating a dataframe\nfeature_imp=pd.DataFrame(list(zip(features,xgb_model.feature_importances_)))\ncolumn_names= ['features','XGB_imp']\nfeature_imp.columns= column_names","ec3bf562":"\n# sort in descending order of importances\nfeature_imp= feature_imp.sort_values('XGB_imp',ascending=False)\nfeature_imp[:15]","f6ab7888":"from sklearn.ensemble import RandomForestClassifier\nimport math","ef2f81be":"features_count = train_ind.shape[1]\n\nparameters_rf = {'n_estimators':[50], 'max_depth':[20], 'max_features': \n                     [math.floor(np.sqrt(features_count)), math.floor(features_count\/3)]}\n\ndef random_forest_classifier(features, target):\n    \"\"\"\n    To train the random forest classifier with features and target data\n    :param features:\n    :param target:\n    :return: trained random forest classifier\n    \"\"\"\n    clf = RandomForestClassifier(n_estimators=50,criterion='gini' ,max_depth=20, max_features=2)\n    clf.fit(features, target)\n    return clf","a50dbd89":"trained_model_RF= random_forest_classifier(train_ind[features], train_dep[response])","6d3a0fb1":"# Predict the on the train_data\ntest_ind[\"Pred_state_RF\"] = trained_model_RF.predict(test_ind[features])\n\n# Predict the on the train_data\ntrain_ind[\"Pred_state_RF\"] = trained_model_RF.predict(train_ind[features])\n\n# Predict the on the train_data\nkick_projects_ip[\"Pred_state_RF\"] = trained_model_RF.predict(kick_projects_ip_scaled_ftrs)","e625bd74":"# Train and Test Accuracy\nprint (\"Train Accuracy :: \", accuracy_score(train_dep[response], trained_model_RF.predict(train_ind[features])))\nprint (\"Test Accuracy  :: \", accuracy_score(test_dep[response], trained_model_RF.predict(test_ind[features])))\nprint (\"Complete Accuracy  :: \", accuracy_score(kick_projects_ip[response], trained_model_RF.predict(kick_projects_ip_scaled_ftrs)))\nprint (\" Confusion matrix of complete data is\", confusion_matrix(kick_projects_ip[response],kick_projects_ip[\"Pred_state_RF\"]))","60d34966":"## Feature importances\nftr_imp_rf=zip(features,trained_model_RF.feature_importances_)\nfor values in ftr_imp_rf:\n    print(values)","1cce6064":"feature_imp_RF=pd.DataFrame(list(zip(features,trained_model_RF.feature_importances_)))\ncolumn_names_RF= ['features','RF_imp']\nfeature_imp_RF.columns= column_names_RF","b6e72a80":"feature_imp_RF= feature_imp_RF.sort_values('RF_imp',ascending=False)\nfeature_imp_RF[:15]","4ca0cf4e":"import lightgbm as lgb","2dc71efa":"#create LGBM classifier model\ngbm_model = lgb.LGBMClassifier(\n        boosting_type= \"dart\",\n        n_estimators=1300,\n        learning_rate=0.08,\n        num_leaves=35,\n        colsample_bytree=.8,\n        subsample=.9,\n        max_depth=9,\n        reg_alpha=.1,\n        reg_lambda=.1,\n        min_split_gain=.01\n)\n\n# LGBM with one-hot encoded features\n#fit the model on training data\ngbm_model=gbm_model.fit(train_ind[features], \n            train_dep[response], \n              verbose=0)","a12b16e4":"# Predict the on the train_data\ntest_ind[\"Pred_state_LGB\"] = gbm_model.predict(test_ind[features])\n\n# Predict the on the train_data\ntrain_ind[\"Pred_state_LGB\"] = gbm_model.predict(train_ind[features])\n\n# Predict the on the train_data\nkick_projects_ip[\"Pred_state_LGB\"] = gbm_model.predict(kick_projects_ip_scaled_ftrs)","d4c2ee16":"# Train and Test Accuracy\nprint (\"Train Accuracy :: \", accuracy_score(train_dep[response], gbm_model.predict(train_ind[features])))\nprint (\"Test Accuracy  :: \", accuracy_score(test_dep[response], gbm_model.predict(test_ind[features])))\nprint (\"Complete Accuracy  :: \", accuracy_score(kick_projects_ip[response], gbm_model.predict(kick_projects_ip_scaled_ftrs)))\nprint (\" Confusion matrix of complete data is\", confusion_matrix(kick_projects_ip[response],kick_projects_ip[\"Pred_state_LGB\"]))","61703ab1":"# classification matrix\nprint('\\nClassification metrics')\nprint(classification_report(y_true=test_dep[response], y_pred=test_ind[\"Pred_state_LGB\"]))","083c89e1":"## Feature importances\nftr_imp_lgb=zip(features,gbm_model.feature_importances_)\n\nfor values in ftr_imp_lgb:\n    print(values)","6e3884ce":"feature_imp_lgb=pd.DataFrame(list(zip(features,gbm_model.feature_importances_)))\ncolumn_names_lgb= ['features','LGB_imp']\nfeature_imp_lgb.columns= column_names_lgb\n\nfeature_imp_lgb= feature_imp_lgb.sort_values('LGB_imp',ascending=False)\nfeature_imp_lgb","0d3f4fd6":"#creating features and response list\nfeatures_2=list(kick_projects_copy)\nfeatures_2.remove('state')\nfeatures_2_numerical = [e for e in features_2 if e not in ('category','main_category','country','currency')]\nfeatures_2_categorical = ['category','main_category','country','currency']\nresponse = ['state']","02832f73":"# Assuming same lines from your example\ncols_to_norm = features_2_numerical\nkick_projects_copy[cols_to_norm] = kick_projects_copy[cols_to_norm].apply(lambda x: (x - x.min()) \/ (x.max() - x.min()))","51998a0a":"#creating test and train dependent and independent variables\n#Split the data into test and train (30-70: random sampling)\n#will be using the scaled dataset to split \ntrain_ind_2, test_ind_2, train_dep_2, test_dep_2 = train_test_split(kick_projects_copy[features_2],kick_projects_copy[response], test_size=0.3, random_state=0)","de95dfcd":"#create LGBM classifier model\ngbm_model_2 = lgb.LGBMClassifier(\n        boosting_type= \"dart\",\n        n_estimators=1500,\n        learning_rate=0.05,\n        num_leaves=38,\n        colsample_bytree=.8,\n        subsample=.9,\n        max_depth=9,\n        reg_alpha=.1,\n        reg_lambda=.1,\n        min_split_gain=.01\n)\n\n# LGBM with one-hot encoded features\n#fit the model on training data\ngbm_model_2=gbm_model_2.fit(train_ind_2[features_2], \n            train_dep_2[response], \n            feature_name=features_2,\n            categorical_feature= features_2_categorical,                \n              verbose=0)","30b50db6":"# Predict the on the train_data\ntest_ind_2[\"Pred_state_LGB\"] = gbm_model_2.predict(test_ind_2[features_2])\n\n# Predict the on the train_data\ntrain_ind_2[\"Pred_state_LGB\"] = gbm_model_2.predict(train_ind_2[features_2])\n\n# Predict the on the train_data\nkick_projects_copy[\"Pred_state_LGB\"] = gbm_model_2.predict(kick_projects_copy[features_2])","f3d792b5":"# Train and Test Accuracy\nprint (\"Train Accuracy :: \", accuracy_score(train_dep_2[response], gbm_model_2.predict(train_ind_2[features_2])))\nprint (\"Test Accuracy  :: \", accuracy_score(test_dep_2[response], gbm_model_2.predict(test_ind_2[features_2])))\nprint (\"Complete Accuracy  :: \", accuracy_score(kick_projects_copy[response], gbm_model_2.predict(kick_projects_copy[features_2])))\nprint (\" Confusion matrix of complete data is\", confusion_matrix(kick_projects_copy[response],kick_projects_copy[\"Pred_state_LGB\"]))","f6e932b3":"# classification matrix\nprint('\\nClassification metrics')\nprint(classification_report(y_true=test_dep_2[response], y_pred=gbm_model_2.predict(test_ind_2[features_2])))","397607e6":"## Feature importances\nftr_imp_lgb_2=zip(features_2,gbm_model_2.feature_importances_)\n\nfor values in ftr_imp_lgb_2:\n    print(values)","bc4111e1":"# creating a dataframe to get top features\nfeature_imp_lgb_2=pd.DataFrame(list(zip(features_2,gbm_model_2.feature_importances_)))\ncolumn_names_lgb_2= ['features','LGB_imp_2']\nfeature_imp_lgb_2.columns= column_names_lgb_2\n\nfeature_imp_lgb_2= feature_imp_lgb_2.sort_values('LGB_imp_2',ascending=False)\nfeature_imp_lgb_2","55955f51":"class LGBMClassifier_GainFE(lgb.LGBMClassifier):\n    @property\n    def feature_importances_(self):\n        if self._n_features is None:\n            raise LGBMNotFittedError('No feature_importances found. Need to call fit beforehand.')\n        return self.booster_.feature_importance(importance_type='gain')","a3b2678f":"# defining parameters\nlgb_gain = LGBMClassifier_GainFE(boosting_type= \"dart\",\n        n_estimators=1500,\n        learning_rate=0.05,\n        num_leaves=38,\n        colsample_bytree=.8,\n        subsample=.9,\n        max_depth=9,\n        reg_alpha=.1,\n        reg_lambda=.1,\n        min_split_gain=.01)","ca92798f":"#fitting the model\nlgb_gain.fit(train_ind_2[features_2], \n            train_dep_2[response], \n            feature_name=features_2,\n            categorical_feature= features_2_categorical,                \n              verbose=0)","4419b9ac":"# Predict the on the train_data\ntest_ind_2[\"Pred_state_LGB_Gain\"] = lgb_gain.predict(test_ind_2[features_2])\n\n# Predict the on the train_data\ntrain_ind_2[\"Pred_state_LGB_Gain\"] = lgb_gain.predict(train_ind_2[features_2])\n\n# Predict the on the train_data\nkick_projects_copy[\"Pred_state_LGB_Gain\"] = lgb_gain.predict(kick_projects_copy[features_2])","98468d90":"# Train and Test Accuracy\nprint (\"Train Accuracy :: \", accuracy_score(train_dep_2[response], lgb_gain.predict(train_ind_2[features_2])))\nprint (\"Test Accuracy  :: \", accuracy_score(test_dep_2[response], lgb_gain.predict(test_ind_2[features_2])))\nprint (\"Complete Accuracy  :: \", accuracy_score(kick_projects_copy[response], lgb_gain.predict(kick_projects_copy[features_2])))\nprint (\" Confusion matrix of complete data is\", confusion_matrix(kick_projects_copy[response],kick_projects_copy[\"Pred_state_LGB_Gain\"]))","8e8abd6f":"## Feature importances\nftr_imp_lgb_gain=zip(features_2,lgb_gain.feature_importances_)\n\nfor values in ftr_imp_lgb_gain:\n    print(values)","da2b30c7":"# creating a dataframe to get top 15 features\nftr_imp_lgb_gain=pd.DataFrame(list(zip(features_2,lgb_gain.feature_importances_)))\ncolumn_names_lgb_gain= ['features','LGB_gain_imp']\nftr_imp_lgb_gain.columns= column_names_lgb_gain\n\nftr_imp_lgb_gain= ftr_imp_lgb_gain.sort_values('LGB_gain_imp',ascending=False)\nftr_imp_lgb_gain[:15]","29994a8c":"from sklearn import tree\nfrom sklearn import neighbors\nimport math","bbec9687":"#creating 4 models for ensembling: Decision Tree (using gini and entropy), knn and Logistic Regression\nmodel_dtc_g = tree.DecisionTreeClassifier()\nmodel_dtc_e = tree.DecisionTreeClassifier(criterion=\"entropy\")\nmodel_knn = neighbors.KNeighborsClassifier()\nmodel_lr= LogisticRegression(penalty='l1',solver='saga')","96c83013":"#fitting each of the model above\nmodel_dtc_g.fit(train_ind[features], train_dep[response])\nmodel_dtc_e.fit(train_ind[features], train_dep[response])\nmodel_knn.fit(train_ind[features], train_dep[response])\nmodel_lr.fit(train_ind[features], train_dep[response])\n\n#predicting the probabilities\npred_dtc_g=model_dtc_g.predict_proba(test_ind[features])\npred_dtc_e=model_dtc_e.predict_proba(test_ind[features])\npred_knn=model_knn.predict_proba(test_ind[features])\npred_lr=model_lr.predict_proba(test_ind[features])\n\n#averaging the 4 predictions above\nfinalpred=(pred_dtc_g+pred_dtc_e+pred_knn+pred_lr)\/4","62f00949":"#creating the dataframe with predicted probabilities (for 0 and 1)\npred_proba_avg=pd.DataFrame(finalpred)\n#the results have 2 probabilities: prob of the state being 0 and state being 1 in that order: hence the 2 columns\ncol_names=['prob_0','prob_1']\npred_proba_avg.columns=col_names","6bbad59e":"# if the probability of 0> probability of 1: state is 0 and vice versa\ndef final_state(c):\n    if c['prob_0'] >c['prob_1']:\n        return 0\n    else:\n        return 1\n#creating the final predicted state column using the averaging method    \npred_proba_avg['final_state_avg'] = pred_proba_avg.apply(final_state, axis=1)","62b03dce":"#appending to base dataframe\ntest_ind = test_ind.reset_index(drop=True)\npred_proba_avg = pred_proba_avg.reset_index(drop=True)\ntest_ind=pd.concat([test_ind,pred_proba_avg],axis=1)","9c29ec2d":"print (\"Test Accuracy  :: \", accuracy_score(test_dep[response],test_ind['final_state_avg']))","7dc343ac":"from sklearn.ensemble import AdaBoostClassifier","2a0ea69f":"#creating the ADA Boost classifier using XGBoost\nmodel_ada = AdaBoostClassifier(random_state=1)\nmodel_ada.fit(train_ind[features], train_dep[response])\n\n#accuracy score\nmodel_ada.score(test_ind[features],test_dep[response])","4eeeeee4":"#### Prediction XGB","e3499345":"Since we see category is coming out to be abnormally high in importance, treating it...","7ae5d784":"# Problem Statement\nTo predict if a kickstarter project will be successful or will fail before its actual deadline. Also identify the factors that determine the success rate of a project.\n\n\n# Solution Notebook\nThis notebook basically has 4 steps\/ modules:\n    1. Data Understanding (EDA) and Preprocessing\n    2. Feature Engineering and heuristic feature selection\n    3. Model Building\n        3A. XGBoost\n        3B. Random Forest\n        3C. LGBM (2 versions: with one-hot encoded features and with categorical features at integer-category columns)\n        3D. Ensemble Models- ormal Averaging and AdaBoosting\n    4. Feature importance\n    \nThe best accuracy obtained was 70.3% accuracy on Test Data from LGBM (version 2)","c1fa2012":"#### Feature Importances","86d15de7":"#### Predictions using LGBM","305cd28c":"#### Model Evaluation","10db9868":"LGBM or LightGBM is yet another gradient boosting framework that uses tree-based learning algorithm. What sets it apart from conventional tree-based algorithms like XGBoost is that it grows trees vertically instead of horizontally splitting them. In other words, it means that LightGBM grows trees leaf-wise while other algorithms grows level-wise or depth-wise. \n\nThe LGBM model chooses the leaf with maximum delta loss to grow. Thus, in the process of growing the same leaf, a leaf-wise algorithm can reduce more loss than any other level-wise algorithm. This results in better accuracy than most other tree-based learning algorithms. Additionally, as the name suggests, LightGBM is computationally less taxing and has faster execution speeds. ","7a525b4c":"### LGBM with categorical level as category columns; no normalization of numerical columns","f4638205":"This still gives a similar feature list. Category is still the most important feature.","ef04375c":"## Model Building","f47147d7":"XGBoost is an implementation of gradient boosted decision trees designed which promises speed and performance. The two reasons XGBoost is a good choice of model are: 1.Execution Speed (as compared to decision trees)  2.Model Performance. This algorithm is also known as gradient boosting, multiple additive regression trees, stochastic gradient boosting or gradient boosting machines.\n\nThe basic concept of Boosting is that it is an ensemble technique where new models are added to correct the errors made by existing models. Models are added sequentially until no further improvements can be made. In Gradient boosting,  new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. It is called so as it uses a gradient descent algorithm to minimize the loss when adding new models. The parameters used to optimize the XGBoost model at every tree is: Logarithmic Loss, or simply Log Loss. It is a classification loss function.\n\nLog Loss quantifies the accuracy of a classifier by penalising false classifications based on the probability of assigning that class. Minimising the Log Loss is basically equivalent to maximising the accuracy of the classifier.","fd473ed9":"### Boosting","ae1f0cd8":"## Ensemble Classifiers","eb3a5b60":"## Feature Engineering","732bc14a":"#### Predictions using LGBM (version 2)","c6d5f046":"Let us call this one LGB2 model for easy reference later","af90be9f":"## Basic Tests and EDA on input data","357675c5":"#### Model Evaluation","b06ee123":"### LGBM","12121b23":"#### Evaluating model performance","6d65acfb":"#### Evaluating XGB classifier","8bbb6c0d":"No nulls, we are good to go","b39ba6ef":"There are only 3 rows with nulls, and the rows with nulls have no names. These rows can be removed.","6fd6328c":"### XGBoost classifier","5461be8f":"#### Understanding Variables in the Dataset\n\nThe dataset has 15 variablesincluding ID. SInce ID is the level of the dataset, we can set it as the index of the ata later. Variables like name, currency, deadline, launched date and country as self explanatory. Explanations of some key variables are as follows:\n\nMain_Category: There are 15 main categories for the project. These main categories broadly classify projects based on topic and genre they belong to.\n\nCategory: Main Categories are further sub divided in categories to give more general idea of the project. For example, Main Category \u201cTechnology\u201d has 15 categories like Gadgets, Web, Apps, Software etc. There are 159 total categories.\n\nGoal: This is the goal amount which the company need to raise to start its project. The goal amount is important variable for company as if it is too high, the project may fail to raise that amount of money and be unsuccessful. If it is too low, then it may reach its goal soon and backers may not be interested to pledge more.\n\nPledged: This is amount raised by the company through its backers. On Kickstarter, if total amount pledged is lower than goal, then the project is unsuccessful and the start-up company doesn\u2019t receive any fund. If pledged amount is more than the goal, the company is considered successful. The variable \u201cusd pledged\u201d is amount of money raised in US dollars.\n\nNumber of Backers: These are number of people who have supported the project by pledging some amount.","ab14f686":"#### Feature Importances","9cb76f68":"The above stats help us reaching the following conclusions:\n1. the data is at ID level (unique of ID=number of rows)\n2. The numerical data fields are: goal, pledged, backers, usd_pledged, usd_pledged_real,usd_goal_real","643d27b1":"#### Feature Importance ","c9df1df4":"Algorithms like XGBoost have a tendency to overfit  odels as the models are added sequentially to minimize the log loss. Random Forest makes a good choice to compensate for this overfitting. Random Forest is a supervised learning algorithm that is an ensemble of Decision Trees, often trained with the \u201cbagging\u201d method. In simple words, the bagging method combines learning models to improve the overall performance. The Random forest algorithm builds multiple decision trees and merges them together to get a more accurate and stable prediction.\n\nRandom Forest gets its name from the fact that it lends an additional randomness to the model, while growing the trees. The algorithm searches for the best feature among a random subset of features, instead of using the most important feature directly to split a node. This results in random splits leading to a better model and also prevents overfitting.\n\nIt is important to note that the RF algorithm uses only a random subset of the features for splitting a node. The randomness of the trees can be additionally increased using random thresholds for each feature instead of searching for the most optimal thresholds.","6cc1e98a":"## Setting up the requires libraries and packages","2fcbaf30":"Doing this exercise with LGBM again, but without one-hot encoding the categorical features. Instead, I have assigned an integer value to each of the 'category', 'main_category', 'currency' and 'country' values. This will then be passed as category columns to LGBM using the 'categorical_feature' argument of the LGBMClassifier.fit function","d34be38b":"## Importing a dataset","19306b72":"### Simple Ensemble: Average Probabailities","15018198":"#### Deriving important features for predicting state of kickstarter projects","4e714085":"#### Predictions using RF","1c3a014d":"### Random Forest Classifier","db406f68":"#### Model Evaluation","29985f1b":"#### Accuracies of RF","6b9c7405":"#### Predictions","a8044c63":"#### Key drivers from Random Forest"}}