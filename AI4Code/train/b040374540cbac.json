{"cell_type":{"23ed65fd":"code","0bc75857":"code","159b7734":"code","710f926e":"code","13a5e9ce":"code","fe8d117b":"code","3d7cf316":"code","c2853fd4":"code","f2040324":"code","ff5574df":"code","4c55304d":"code","bb94bc91":"code","abf04ab8":"code","353b6dae":"code","1dce9284":"code","064eb1c1":"code","01617929":"code","018e4601":"code","5cc2253d":"code","29503547":"code","d192b607":"code","fb7a17f3":"code","dec7f9e4":"code","76dd1ff6":"code","00d4721c":"code","c597fe95":"code","79e65883":"code","16214169":"code","30dbb0de":"code","fc7b5cea":"code","9770efa3":"markdown","8b34214d":"markdown","20e46687":"markdown","4b635df1":"markdown","fdb4debc":"markdown","24362eac":"markdown","852875a8":"markdown","162a7a10":"markdown","84f8617b":"markdown","a82762e0":"markdown","cf583fac":"markdown","1e3723d1":"markdown","30a1754c":"markdown","60f8b085":"markdown","14337538":"markdown","04afe11c":"markdown","9b4b6c32":"markdown","834256ae":"markdown","432c3415":"markdown","416ef18b":"markdown","85a73be7":"markdown"},"source":{"23ed65fd":"# Import libraries\nimport os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Import Warnings \nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n#from sklearn.cross_validation import train_test_split\n# Import tensorflow as the backend for Keras\nfrom keras import backend as K\nK.set_image_dim_ordering('tf')\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\nfrom keras.optimizers import SGD,RMSprop,adam\nfrom keras.callbacks import TensorBoard\n# Import required libraries for cnfusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport itertools","0bc75857":"PATH = os.getcwd()\n# Define data path\ndata_path = '..\/input\/data\/data'\ndata_dir_list = os.listdir(data_path)\ndata_dir_list","159b7734":"img_rows=128\nimg_cols=128\nnum_channel=1\nnum_epoch=100\n# Define the number of classes\nnum_classes = 7\nimg_data_list=[]\nfor dataset in data_dir_list:\n\timg_list=os.listdir(data_path+'\/'+ dataset)\n\tprint ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n\tfor img in img_list:\n\t\tinput_img=cv2.imread(data_path + '\/'+ dataset + '\/'+ img )\n\t\tinput_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n\t\tinput_img_resize=cv2.resize(input_img,(128,128))\n\t\timg_data_list.append(input_img_resize)\n\nimg_data = np.array(img_data_list)\nimg_data = img_data.astype('float32')\nimg_data \/= 255\nprint (img_data.shape)","710f926e":"if num_channel==1:\n\tif K.image_dim_ordering()=='th':\n\t\timg_data= np.expand_dims(img_data, axis=1) \n\t\tprint (img_data.shape)\n\telse:\n\t\timg_data= np.expand_dims(img_data, axis=4) \n\t\tprint (img_data.shape)\n\t\t\nelse:\n\tif K.image_dim_ordering()=='th':\n\t\timg_data=np.rollaxis(img_data,3,1)\n\t\tprint (img_data.shape)","13a5e9ce":"num_classes = 7\nnum_of_samples = img_data.shape[0]\nlabels = np.ones((num_of_samples,),dtype='int64')\nlabels[0:365]=0\nlabels[365:567]=1\nlabels[567:987]=2\nlabels[987:1189]=3\nlabels[1189:1399]=4\nlabels[1399:1601]=5\nlabels[1601:1803]=6\nnames = ['bike', 'cars', 'cats', 'dogs', 'flowers', 'horses', 'human']","fe8d117b":"Y = np_utils.to_categorical(labels, num_classes)","3d7cf316":"x,y = shuffle(img_data,Y, random_state=2)\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)","c2853fd4":"print(\"X_train shape = {}\".format(X_train.shape))\nprint(\"X_test shape = {}\".format(X_test.shape))","f2040324":"image = X_train[1203,:].reshape((128,128))\nplt.imshow(image)\nplt.show()","ff5574df":"#Initialising the input shape\ninput_shape=img_data[0].shape\n# Design the CNN Sequential model\ncnn_model = Sequential([\n    Convolution2D(32,3,3,border_mode ='same',activation='relu',input_shape = input_shape),\n    Convolution2D(32,3,3,activation='relu'),\n    MaxPooling2D(pool_size=(2,2)) ,\n    Dropout(0.5),\n    Flatten(), \n    Dense(128,activation='relu'),\n    Dropout(0.5),\n    Dense(num_classes,activation = 'softmax')\n])","4c55304d":"cnn_model.compile(loss='categorical_crossentropy', optimizer='adadelta',metrics=[\"accuracy\"])","bb94bc91":"cnn_model.summary()","abf04ab8":"hist = cnn_model.fit(X_train, y_train, batch_size=16, nb_epoch=num_epoch, verbose=1, validation_data=(X_test, y_test))\ntrain_loss=hist.history['loss']\nval_loss=hist.history['val_loss']\ntrain_acc=hist.history['acc']\nval_acc=hist.history['val_acc']\nxc=range(num_epoch)","353b6dae":"plt.figure(1,figsize=(10,5))\nplt.plot(xc,train_loss)\nplt.plot(xc,val_loss)\nplt.xlabel('Number of Epochs')\nplt.ylabel('Loss')\nplt.title('Train Loss vs Validation Loss')\nplt.grid(True)\nplt.legend(['Train Loss','Validation Loss'])\nplt.style.use(['classic'])","1dce9284":"plt.figure(2,figsize=(10,5))\nplt.plot(xc,train_acc)\nplt.plot(xc,val_acc)\nplt.xlabel('Number of Epochs')\nplt.ylabel('Accuracy')\nplt.title('Train Accuracy vs Validation Accuracy')\nplt.grid(True)\nplt.legend(['Train Accuracy','Validation Accuracy'],loc=4)\nplt.style.use(['classic'])","064eb1c1":"score = cnn_model.evaluate(X_test, y_test, verbose=0)\nprint('Test Loss:', score[0])\nprint('Test Accuracy:', score[1])","01617929":"test_image = X_test[0:1]\nprint (test_image.shape)\nprint(cnn_model.predict(test_image))\nprint(cnn_model.predict_classes(test_image))\nprint(y_test[0:1])","018e4601":"image = test_image.reshape((128,128))\nplt.imshow(image)\nplt.show()","5cc2253d":"test_img = cv2.imread('..\/input\/data\/data\/human\/rider-104.jpg')\ntest_img = cv2.cvtColor(test_img,cv2.COLOR_BGR2GRAY)\ntest_img = cv2.resize(test_img,(128,128))\ntest_img = np.array(test_img)\ntest_img = test_img.astype('float32')\ntest_img \/= 255\nprint(test_img.shape)","29503547":"image = test_img.reshape((128,128))\nplt.imshow(image)\nplt.show()","d192b607":"if num_channel==1:\n\tif K.image_dim_ordering()=='th':\n\t\ttest_img= np.expand_dims(test_img, axis=0)\n\t\ttest_img= np.expand_dims(test_img, axis=0)\n\t\tprint (test_img.shape)\n\telse:\n\t\ttest_img= np.expand_dims(test_img, axis=3) \n\t\ttest_img= np.expand_dims(test_img, axis=0)\n\t\tprint (test_img.shape)\n\t\t\nelse:\n\tif K.image_dim_ordering()=='th':\n\t\ttest_img=np.rollaxis(test_img,2,0)\n\t\ttest_img= np.expand_dims(test_img, axis=0)\n\t\tprint (test_img.shape)\n\telse:\n\t\ttest_img= np.expand_dims(test_img, axis=0)\n\t\tprint (test_img.shape)\n\t\t\n# Predicting the test image\nprint((cnn_model.predict(test_img)))\nprint(cnn_model.predict_classes(test_img))","fb7a17f3":"def get_featuremaps(cnn_model, layer_idx, X_batch):\n\tget_activations = K.function([cnn_model.layers[0].input, K.learning_phase()],[cnn_model.layers[layer_idx].output,])\n\tactivations = get_activations([X_batch,0])\n\treturn activations\nlayer_num=3\nfilter_num=0\nactivations = get_featuremaps(cnn_model, int(layer_num),test_img)\nprint (np.shape(activations))","dec7f9e4":"feature_maps = activations[0][0]      \nprint (np.shape(feature_maps))","76dd1ff6":"if K.image_dim_ordering()=='th':\n\tfeature_maps=np.rollaxis((np.rollaxis(feature_maps,2,0)),2,0)\nprint (feature_maps.shape)","00d4721c":"fig=plt.figure(figsize=(16,16))\nplt.imshow(feature_maps[:,:,filter_num],cmap='gray')\nplt.savefig(\"featuremaps-layer-{}\".format(layer_num) + \"-filternum-{}\".format(filter_num)+'.jpg')\nnum_of_featuremaps=feature_maps.shape[2]\nfig=plt.figure(figsize=(16,16))\t\nplt.title(\"featuremaps-layer-{}\".format(layer_num))\nsubplot_num=int(np.ceil(np.sqrt(num_of_featuremaps)))\nfor i in range(int(num_of_featuremaps)):\n\tax = fig.add_subplot(subplot_num, subplot_num, i+1)\n\t#ax.imshow(output_image[0,:,:,i],interpolation='nearest' ) #to see the first filter\n\tax.imshow(feature_maps[:,:,i],cmap='gray')\n\tplt.xticks([])\n\tplt.yticks([])\n\tplt.tight_layout()\nplt.show()","c597fe95":"# Print the confusion matrix\nY_pred = cnn_model.predict(X_test)\nprint(Y_pred)\ny_pred = np.argmax(Y_pred,axis=1)\nprint(y_pred)\ntarget_names=['Class 0 (flowers)', 'Class 1 (cars)', 'Class 2 (cats)', 'Class 3 (horses)',\n              'Class 4 (human)', 'Class 5 (bike)', 'Class 6 (dogs)']\nprint(classification_report(np.argmax(y_test,axis=1),y_pred,target_names=target_names))","79e65883":"print('Confusion Matrix \\n')\nprint(confusion_matrix(np.argmax(y_test,axis=1), y_pred))","16214169":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float32') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Confusion matrix with Normalization\")\n    else:\n        print('Confusion matrix without normalization')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n# Compute confusion matrix\ncnf_matrix = (confusion_matrix(np.argmax(y_test,axis=1), y_pred))\nnp.set_printoptions(precision=2)\nplt.figure()","30dbb0de":"plot_confusion_matrix(cnf_matrix, classes=target_names,\n                      title='Confusion Matrix without Normalisation')\nplt.show()","fc7b5cea":"plot_confusion_matrix(cnf_matrix, classes=target_names, normalize=True,\n                      title='Normalized Confusion Matrix')\nplt.figure()\nplt.show()","9770efa3":"Convert class labels to on-hot encoding","8b34214d":"### 3. Plotting the Loss and Accuracy curve\n#### Visualizing Training Loss & Validation Loss","20e46687":"From the above confusion matrix each row represents a class .For example let us look at the first row where it shows 37 images are classified for class 0 (flowers) and rest across other classes. To visualise this in a more understandable format let us plot it using matplotlib library as below \n\n**Plotting the confusion matrix**","4b635df1":"So from the above prediction it is evident that the model predict the class as human for the test image that we have picked up .\n\n### 5. Visualizing the intermediate layer output of CNN","fdb4debc":"Assigning Labels & define the number of classes","24362eac":"Now let us plot confusion matrix with normalization which means all values will lie between 0 and 1.\n**Plot normalized confusion matrix**","852875a8":"# Simple Tutorial on Object Recognition 4 Beginner's!\n![](https:\/\/i0.wp.com\/deepomatic.com\/wp-content\/uploads\/2019\/05\/85384-1z4kp_opo_wiju9srtlv_1a.png?resize=1000%2C297&ssl=1)\nThere is no real difference between object recognition and image recognition. In fact, they both refer to technologies that can recognize certain targeted subjects through specific algorithms like deep learning. They are strictly related to computer vision, which we define as the art and science of making computers understand images.\n\nIn this kernel, we will go thru some basic concepts in object recognition: \n\n#### What is Object\/Image recognition ?\n\nObject recognition consists of recognizing, identifying, and locating objects within a picture with a given degree of confidence.\n\nIn this process, the four main tasks are:\n\n* Classification.\n* Tagging.\n* Detection.\n* Segmentation.\n\n#### Classification and tagging\n\nAn important task in Object recognition is to identify what is in the image and with what level of confidence. \n![](https:\/\/i0.wp.com\/deepomatic.com\/wp-content\/uploads\/2019\/05\/85384-1z4kp_opo_wiju9srtlv_1a.png?resize=1000%2C297&ssl=1)\n\nThe mechanism of this task is straightforward. It starts with the definition of the ontology, i.e. the class of objects to detect. Then, both classification and tagging identify what is in the image and the associated level of confidence. \n\nWhile **classification** recognizes only one class of objects, **tagging** can recognize multiple ones for a given image.\n\nIn **classification** the algorithm will only remember that there is a dog, ignoring all other classes. \n\nIn **tagging**, it will try to return all the best classes corresponding to the image. \n\n#### Detection and segmentation\n\nOnce identified what is in the image, we want to locate the objects. There are two ways to do so: detection and segmentation.\n![](https:\/\/i0.wp.com\/deepomatic.com\/wp-content\/uploads\/2019\/05\/c74a7-1he3sybzm-ihofw4r39zgra.png?resize=1000%2C297&ssl=1)\nDetection outputs a rectangle, also called bounding box, where the objects are. It is a very robust technology, prone to minor errors and imprecisions. \n\nSegmentation identifies the objects for each pixel\u200a in the image, resulting in a very precise map. However, the accuracy of segmentation depends on an extensive and often time-consuming training of the neural network.\n\nThis kernel covers the Object Recognition using CNN, Keras ,Tensorflowworks implementation using image dataset . The following topics will be covered.\n\n### Table Of Contents\n\n#### 1. Loading and preprocess Object dataset\n#### 2. Designing and training a CNN model in Keras\n#### 3. Plotting the Loss and Accuracy  curve\n#### 4. Evaluating the model & Predicting the output class of a test Object\n#### 5. Visualizing the intermediate layer output of CNN\n#### 6. Plotting the confusion matrix for your result\n\n### 1. Loading and preprocessing own dataset\n\nThe dataset that I am using for this kernel is my own accumulated dataset of 7 types of classes namely \n'flowers', 'cars', 'cats', 'horses', 'human', 'bike', 'dogs' with total of 1803 image samples.","162a7a10":"Now let use a test image and predict the probability of this image belonging to which class.Let us find out ","84f8617b":"### Test with a new image ","a82762e0":"## I hope by now you are able to completely understand how to do Object Recognition using deep learning CNN model.\n\n# Please do leave your comments\/suggestions and you like this kernel greatly appreciate to UPVOTE.\n","cf583fac":"### Plot non-normalized confusion matrix","1e3723d1":"Shuffle and Split the dataset","30a1754c":"### 4. Evaluating the model & Predicting the output class of a test image","60f8b085":"**Compiling the model**","14337538":"### 2. Designing and training a CNN model in Keras","04afe11c":"### Train the model","9b4b6c32":"#### Visualizing Training Accuracy & Validation Accuracy","834256ae":"So based on the above results it is evident that predict class is 1 .In this case it is a car. Let us visualise it .","432c3415":"As shown above in the classification report the recall shows the individual class accuracy.In this case horse and dogs have a very low class accuracy of 0.38 and 0.32 respectively.","416ef18b":"### 6. Plotting the confusion matrix to observe the result","85a73be7":"### View Model Configuration"}}