{"cell_type":{"e0736b23":"code","f49ba891":"code","2340af09":"code","6a57a6bb":"code","8a71b391":"code","3ddf1956":"code","cd966b0e":"code","d15274d3":"code","7a8c15c0":"code","d0a9092a":"code","60424837":"code","3edfaa66":"markdown","b77c7ec7":"markdown","6f3e1d65":"markdown","b8e84c6e":"markdown","02e7e0b6":"markdown","28fec494":"markdown","789cca91":"markdown","9778c6fb":"markdown","e9e88d33":"markdown","8b27aa31":"markdown","6e3601b1":"markdown","5d83bdca":"markdown","8034f5d3":"markdown","ac31634e":"markdown","c3da86b5":"markdown","442440d6":"markdown","2aa7ea8b":"markdown","79dbe445":"markdown","b945a986":"markdown","a6addf10":"markdown","926e82f4":"markdown","34dc6ef8":"markdown"},"source":{"e0736b23":"%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics, tree\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f49ba891":"#The values used in this example are random and used just to exemplify the matrix\n\nbin_class_matrix = np.array([[15,6],[1,21]])\nmatrix_flat = bin_class_matrix.flatten()\nnotes = ['True Positive', 'False Negative', 'False Positive', 'True Negative']\nto_show = np.array([[49,70],[70,49]])\n\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(matrix_flat, notes)]\nlabels = np.asarray(labels).reshape(2,2)\n\nfake_category = ['Class 1','Class 2']\nsns.heatmap(to_show, annot=labels, cmap='Paired',\n            xticklabels=fake_category, yticklabels=fake_category,\n            fmt='',vmin = 0, vmax = 150, cbar = False, linecolor='white', linewidths=0.5)\nplt.title(\"Confusion matrix for binary classification\")\nplt.ylabel('True labels (Groundtruth)');\nplt.xlabel('Predicted labels');","2340af09":"#The values used in this example are random and used just to exemplify the matrix\n\nter_class_matrix = np.array([[21,6,4],[15,1,11],[1,5,13]])\nto_show = np.array([[49,70,70],[70,49,70],[70,70,49]])\n\nmatrix_flat = ter_class_matrix.flatten()\nlabels = [f\"{v1}\" for v1 in matrix_flat]\nlabels = np.asarray(labels).reshape(3,3)\n\nfake_category = ['Apples','Pineapples','Bananas']\n\nsns.heatmap(to_show, annot=labels,cmap='Paired',\n            xticklabels=fake_category,yticklabels= fake_category,\n            fmt='',vmin = 0,vmax = 150,cbar = False,\n            linecolor = 'white',linewidths=0.5)\n\nplt.title(\"Confusion matrix for three class classification\")\nplt.ylabel('True labels (Groundtruth)');\nplt.xlabel('Predicted labels');","6a57a6bb":"#The values used in this example are random and used just to exemplify the curve\n\nequal_prob = [0 for _ in range(20)]\nmodel_prob = np.random.rand(1,20).flatten()\nmodel_prob[:10] = 1 \ny_values = np.random.randint(0,2,(1,20)).flatten()\ny_values[:10] = 1\ny_values[10:15] = 0\n\nmodel_fpr, model_tpr, _ = roc_curve(y_values, model_prob)\nequal_fpr, equal_tpr, _ = roc_curve(y_values, equal_prob)\n\nplt.plot(equal_fpr, equal_tpr, linestyle='--', label='Equal TP and FP');\nplt.plot(model_fpr, model_tpr, marker='.', label='Model');\n\nplt.xlabel('False Positive Rate');\nplt.ylabel('True Positive Rate');\nplt.legend();","8a71b391":"iris_dataset = pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")\niris_dataset = iris_dataset.drop(labels = ['Id'], axis=1)\niris_dataset.head()","3ddf1956":"iris_dataset['Species'].value_counts()","cd966b0e":"iris_dataset.describe()","d15274d3":"# Separate training and test\n\niris_values = iris_dataset.values\nX,y = iris_values[:,:-1], iris_values[:,-1]\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.5, random_state = 42)\n\n#Create model\nclassifier = tree.DecisionTreeClassifier(random_state=42)\nclassifier.fit(X_train, y_train)\n\n#Make predictions\npredictions = classifier.predict(X_test)","7a8c15c0":"conf_matrix = metrics.confusion_matrix(y_test, predictions)\n\ncategories = ['Iris-setosa','Iris-versicolor','Iris-virginica']\nsns.heatmap(conf_matrix,\n            annot=True,cmap='YlOrRd',\n            xticklabels=categories, cbar=False)\n\nplt.yticks(np.arange(3),categories)\nplt.ylabel('True labels');\nplt.xlabel('Predicted labels');\nplt.title('Confusion matrix of Iris species classification');","d0a9092a":"print(metrics.classification_report(y_test, predictions, digits=3))","60424837":"metrics.classification_report(y_test, predictions, digits=3, output_dict=True)","3edfaa66":"### 9.1 - Read and explore the dataset","b77c7ec7":"So, if we have a dataset with 77 samples, where 31 of them are apples, 27 are pineapples and 19 are bananas, and the model predicts that 37 of them are apples, we can see in the confusion matrix that 16 predictions are wrong, where 15 of the them are from the pineapple class and 1 is from the bananas class. That is, 15 pineapples and 1 banana are classified as apples. Also, if we consider apples as the positive class, we say that we had 21 TP and 16 FP, because the model said that pineapples and bananas were apples when, in fact, they were not. \n\nIn the same way, if the model predicts that only 21 out of the 31 apples samples are really apples, that 6 are pineapples and that 4 are bananas, we see that we obtained 21 TP and 10 FN, because the model said that these samples were not apples, when, in fact, they were.\n\nAn important detail, which will help us to understand the importance of each of the metrics that we will discuss, is that, in the real world, positive and negative predictions are not always equally important. In other words, depending on the model's application scenario, it may be more important to measure and decrease the amount of False Positive (FP) than False Negative (FN) predictions, and vice versa.","6f3e1d65":"## 1 - Confusion matrix\n\nConfusion matrix is one of the tools used for the evaluation of a classification model and, from it, we can derive the evaluation metrics that we will discuss. It allows us to both measure the performance of the model, as well as observe which classes are being predicted correctly or incorrectly.\n\nTo build a confusion matrix, we first determine the number of rows and columns. A confusion matrix is a square matrix, that is, the number of rows and columns are equal. In addition, they are equal to the number of classes present in the data set.\n\nNext, we determine the meaning of the rows and columns. To the columns we assign the classes predicted by the model and to the rows we assign the true classes of the data (groundtruth) -- It is important to note that the column and row definition can be exchanged. In this way, we can use the confusion matrix to compare the predictions made by the classification model and the expected values for each sample in the data set. As a result of these comparisons, each position in the matrix receives a value that is increased when one of four situations occurs:\n\n* **True Positive (TP)**: when the class predicted by the model is equal to the true class of the sample. For example, if the class of a sample is an apple and the model predicts that it is an apple.\n\n* **False Positive (FP)**: when the model predicts one class for a sample, but its true class is another. For example, the model determines that the sample class is apple, but in fact, it is pineapple.\n\n* **True Negative (TN)**: again the class predicted by the model is the same as the true class of the sample - we will see below why one of them is called TP and the other is TN. For example, when the real class of a sample is pineapple and the model predicts that it is pineapple.\n\n* **False Negative (FN)**: it happens when the model predicts one class, but the real class of the sample is another - we will also see how FN differs from FP. For example, when the model says that the class of a sample is pineapple when, in fact, it is apple.\n\nIn the situations explained above, we saw that the definitions of the terms TP and TN, as well as those of the terms FP and FN, are confused. To understand them better, we must think about a binary classification problem, that is, in which we consider only two classes. In this type of problem, one class is considered positive, while the other is called negative. In this way, we say that we have a case of True Positive when the model predicts the positive class and the sample class is really positive. Likewise, we say that we have a situation of True Negative when the model predicts the negative class and the sample class is really negative. On the other hand, we say that we have a situation of False Positive when the model predicts the class as being positive, but it is negative. And we have a False Negative situation when the model says that the class is negative, but it is positive. \n\nIn the figure below, we can see an example of a confusion matrix for a binary classification model. The green cells indicate how many times the model got the prediction right - for both the positive and negative classes - and the red cells indicate how many times the model was wrong.","b8e84c6e":"# <center> Evaluation metrics for classification problems <\/center>\n\nMachine learning algorithms can be used for several purposes, such as classification, regression, clustering and many others, depending on the application in which we want to use them. Regardless of the purpose of the learning model, we must have tools that allow us to assess how good it is. For that, we use **Evaluation Metrics**, which are chosen according to the characteristics of the data set -- for example, if it is balanced --, with the application domain for which the model will be built -- medicine, agriculture, among others -- and with the type of task that it must perform -- classification, regression, etc. In addition, in order to get better assessments and interpretations, we usually need to employ more than one metric.\n\nIn this notebook, we will see some evaluation techniques and metrics used when we want to analyze the results of machine learning models built for data classification. In particular, we will discuss **Confusion Matrices**, **Accuracy**, **Precision**, **Recall**, **F-score**, **ROC curves** and **AUC**.","02e7e0b6":"False Positive errors, which occur when the model says that the class is positive when, in fact, it is negative, are also called Type I errors. In turn, False Negative errors, which occur when the model says the class of a sample is negative when, in reality, it is positive, are called Type II Errors.\n\nWhen dealing with classification problems that involve more than one class, we follow the same method of construction of the confusion matrix: the number of rows and columns will be equal to the number of classes, the columns will represent the predicted classes and the rows represents the true classes. However, to fill the confusion matrix cells, we will look at one class at a time, with the observed class being considered positive and the others being seen as negative.\n\nIn the figure below, we can see an example of a confusion matrix for a classification problem with three classes. The green cells indicate the number of times the model has made a correct prediction, for each class, and the red cells indicate the number of times it was wrong. We can observe that, now, given a class as positive, the number of errors is distributed among the remaining classes.","28fec494":"## 9 - Example of applying classification evaluation metrics\n\nTo exemplify the topics discussed in this notebook, we will build the confusion matrix and calculate the classification metrics for the [Iris Species Dataset](https:\/\/www.kaggle.com\/uciml\/iris). This data set provides information about the lengths and widths of the flower sepals and petals and asks us to classify the samples into one of three classes (flower species): Iris Setosa, Iris Virginica and Iris Versicolor. Altogether, 150 samples are provided and, to build a confusion matrix that facilitates the discussion, we will separate half of them for training and the other half for testing. Also, for the classification task, we will use a decision tree.","789cca91":"## 8 - Classification metrics for multiclass problems\n\nWhen dealing with problems that have more than two classes, the metrics discussed above can be calculated in different ways. One of them is called **micro**, where we calculate the metric value for the entire confusion matrix, in general. That is, when we try to know the number of true positives, we add the TP of all the classes. When we want to know the amount of false positives, we add the FP of each class, and so on.\n\nAnother way to calculate metrics is to consider one class at a time, considering it positive, and the others as negative. Thus, we calculate the model's metrics as the result of a simple average of the metrics calculated for each class. We say that we used the **macro** method, as in the example below:\n\n$$\nmacro\\_precision = \\\\\n\\frac{precision\\_apple + precision\\_pineapple + precision\\_banana}{3}\n$$\n\nWhen the metrics in the model are the result of a weighted average of the metrics calculated for each class, where the weights are the number of instances of each class, we say that we used the **weighted** method. For example:\n\n$$\nweighted\\_recall =\\\\\n\\frac{n\\_apple * recall\\_apple + n\\_pineapple * recall\\_pineapple + n\\_banana*recal\\_banana}{n\\_apple + n\\_pineapple + n\\_banana}\n$$\n\n\n","9778c6fb":"The Area Under the Curve (AUC), in turn, calculates the area under the ROC curve. The higher its value, the better the curve. In this way, the ROC and AUC allow us not only to choose the best decision threshold, but also to observe which is the best model, since different models can generate different curves that can be compared.\n\nAn interesting detail is that, in some applications, when the data is unbalanced, some people use precision instead of the false positive rate, because precision suffers less from this imbalance.","e9e88d33":"### 9.2 - Training the model and making predictions","8b27aa31":"## 4 - Recall metric\n\nAnother evaluation metric is recall, also called **sensitivity**. It is defined as the ratio between the number of correct positive predictions and the total of truly positive samples, as can be seen in the equation below:\n\n$$\nRecall = \\frac{TP}{TP + FN}\n$$\n\nThe recall is usually used when the worst case scenario is caused by false negatives (FN). Thus, the greater the number of false negatives, the lower the model recall. We can think of the recall as a measure that tells us how many of the positive samples we predicted as positive. Thus, it answers the question: \"How many of the positive samples were classified as positive during the predictions?\"\n\nAs explained, both for precision and recall, we are interested in metrics that allow us to better analyze and interpret the worst case scenario. Thus, one way to choose which metric to use, between precision and recall, is to answer the question: \"Which scenario will have the highest cost, the greatest amount of false positives or false negatives?\".","6e3601b1":"## Import Libraries\n\nIn this notebook, we are going to use libraries to deal with the example dataset and to plot the graphics used in the discussion. They are imported in the code below.","5d83bdca":"## 5 - F1 score\n\nThe F1 score is a metric that allows us to combine precision and recall in a single value, being defined as the harmonic mean of these two metrics, as can be seen in the following equation:\n\n$$\nF1 score = \\frac{2}{\\frac{1}{recall} + \\frac{1}{precision}} = \\frac{2*recall*precision}{recall + precision}\n$$ \n\nThe higher the F1 score, the better the model. In other words, we say that we have a good F1 score when we have a small number of false positives and false negatives. We can think that a good F1 score is achieved when a large part of the predictions that we classified as positive are really positive and when a large part of the positive samples are correctly predicted.","8034f5d3":"## Notes\n\nThank you guys for reading this notebook. This is my first attempt to write a notebook in Kaggle about some topic that I'm learning. I hope it can help you in some way. Remember:\n> \"A journey of a thousand miles starts with one simple step\" - Japonese proverb","ac31634e":"## 7 - ROC curve and AUC\n\nROC (*Receiver Operating Characteristic*) is a methodology for evaluating classification models that allows us to choose characteristics - such as the probability thresholds - that lead us to the best results.\n\nIt is constructed as a curve in the Cartesian plane, where the vertical axis indicates the rate of true positives (TPR) - which we saw earlier as recall or sensitivity - and the horizontal axis indicates the rate of false positives (FPR) -- defined as $ 1 - specificity $.\n\nThe rate of true positives is given by:\n\n$$\nTPR = Recall = \\frac{TP}{TP + FN}\n$$\n\nSpecificity is given by:\n\n$$\nSpecificity = \\frac{TN}{TN + FP}\n$$\n\nThus, the rate of false positives can be defined as:\n\n$$\nFPR = 1 - {Specificity} = \\frac{FP}{TN + FP}\n$$\n\nThe ROC curve allows us to carry out, in a more practical way, the same analysis that we would carry out to build infinite confusion matrices, one for each decision threshold to be evaluated. For this, we look for points on the curve that maximize TPR and minimize FPR. Furthermore, the choice of the best point depends on the amount of false positives that we are willing to accept. \n\nIn the figure below, we can see an example of a ROC curve, where the **Model** curve is the ROC curve achieved when using a given classification model, and the **Equal TP and FP** curve is a reference curve that represents the cases where the proportion of true positives is equal to the proportion of false positives.\n\n","c3da86b5":"If we want to get this values and make operations with them, we just need to set the **output_dict** parameter of the **classification_report** method to True, as shown below, making the method to return a dictionary with all those information.","442440d6":"## 3 - Precision metric\n\nAnother important measure used in the evaluation of classification models is precision. It is defined by the ratio between the number of correct (true) positive predictions and the total number of positive predictions, as indicated in the equation below:\n\n$$\nPrecision = \\frac{TP}{TP + FP}\n$$ \n\nPrecision is generally used when the worst case scenario is caused by false positives (FP). We say that a model has a low accuracy if it has a large amount of false positives, that is, the higher the FP number, the lower the precision.\n\nWe can think of precision as a measure that tells us how many of our predictions we get right. In other words, it answers the question: \"How many of the predictions classified as positive are really positive?\"","2aa7ea8b":"## 2 - Accuracy metric\n\nThe first evaluation metric that we will discuss, and one of the most used, is accuracy. It is defined as the ratio between the number of correct predictions and the total number of predictions, as shown in the equation below:\n\n$$\nAccuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n$$\n\nAccuracy, however, fails to express the model's performance well when the data set used is unbalanced - the number of samples in some classes is different from the number of others - and, in the real world, the data is often unbalanced. When this happens, even high accuracy may not mean a good model. Thus, in addition to the accuracy of the model, other information needs to be measured.\n","79dbe445":"### 9.3 - Evaluate the model\n\n#### 9.3.1 - Confusion matrix","b945a986":"#### 9.3.2 - Classification metrics\n\nIn the code below, the first three lines show the precision, recall, f1 score and the number of samples (support) for each class. In the last three lines, we can see the *micro* accuracy -- once the micro accuracy, micro precision, micro recall and micro f1 score are always equal to each other, only the accuracy is displayed --, and the macro and weighted values for precision, recall and f1 score.","a6addf10":"1. More Performance Evaluation Metrics for Classification Problems You Should Know: https:\/\/www.kdnuggets.com\/2020\/04\/performance-evaluation-metrics-classification.html\n\n2. Multi-Class Metrics Made Simple, Part I: Precision and Recall: https:\/\/towardsdatascience.com\/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2\n\n3. Multi-Class Metrics Made Simple, Part II: the F1-score: https:\/\/towardsdatascience.com\/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1\n\n4. ROC and AUC, Clearly Explained!: https:\/\/www.youtube.com\/watch?v=4jRBRDbJemM\n\n5. Confusion Matrix Visualization: https:\/\/medium.com\/@dtuk81\/confusion-matrix-visualization-fc31e3f30fea\n\n6. How to Use ROC Curves and Precision-Recall Curves for Classification in Python: https:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/\n ","926e82f4":"## 6 - $F_{\\beta}$ score\n\nDepending on the problem we want to address and, consequently, the importance of false positives (FP) and false negatives (FN), the F1 score may not be a good measure of the model's performance. To deal with this problem, we can use the $F_{\\beta} score$, which is a weighted harmonic mean of precision and recall, as shown in the following equation:\n\n$$\nF_{\\beta} score = (1 + \\beta^2) * \\frac{(precision * recall)}{\\beta^2*precision + recall}\n$$\n\nThe $ \\beta $ parameter allows us to control which of the two metrics we give the most importance to: precision or recall. When $0 \\leq \\beta <1$, we place more importance on precision. When $ \\beta = 1 $, we have the F1 score. Finally, when $ \\beta> 1 $, we give more importance to the recall.","34dc6ef8":"## References\n\n"}}