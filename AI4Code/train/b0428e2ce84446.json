{"cell_type":{"70bf72ae":"code","8f95d591":"code","33f505ef":"code","3ef1216d":"code","1f9c1516":"code","35cf30e2":"code","d538e35f":"code","0ca76085":"code","2e5ec5b8":"code","0bc8e32e":"code","7fa78c92":"code","7b610ab6":"code","29def621":"code","070ff0b9":"code","c9b5e7f9":"code","5b224767":"code","0a1eaefd":"code","9fc1c8f6":"code","76d3d51d":"code","b84fc171":"code","c60fb399":"code","b1aaa302":"code","da3a868f":"code","56dcd476":"code","fba6e0f3":"code","eb5a808c":"code","85a1991e":"code","412f6a62":"code","1d8431cb":"markdown","a8e4452b":"markdown","6760b177":"markdown","d32a3835":"markdown","8e0fffdb":"markdown","2c68db81":"markdown","ecf548b3":"markdown","3e919976":"markdown","49c784c2":"markdown","6ac32e5d":"markdown","49fa685c":"markdown","8e576dae":"markdown","93b993ae":"markdown","73bb18fd":"markdown","5a16001a":"markdown","64e13dc1":"markdown","2497f101":"markdown","9d9663f9":"markdown"},"source":{"70bf72ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8f95d591":"import pandas as pd\nimport numpy as np\nimport random as rnd3\nfrom sklearn.impute import SimpleImputer\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split, LeaveOneOut, KFold, cross_val_score, cross_validate, ShuffleSplit, GridSearchCV\nfrom sklearn.utils import resample\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import AdaBoostClassifier","33f505ef":"df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf.info()\ndf.describe()\ndf.head()","3ef1216d":"\n# Title - retreving it from the name\n\ndf['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.')\npd.crosstab(df['Title'], df['Sex']) #some titles are rare - so bucket them\ndf['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndf['Title'] = df['Title'].replace('Mlle', 'Miss')\ndf['Title'] = df['Title'].replace('Ms', 'Miss')\ndf['Title'] = df['Title'].replace('Mme', 'Mrs')\ndf[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()  #survival rate by the title\n\n\n\n# Age - impute the nulls (263 nulls), bucket\n\nguess_age = df.groupby(['Pclass','Sex']).median()\ndf[\"Age\"].fillna(df.groupby(['Pclass','Sex'])[\"Age\"].transform(\"median\"), inplace=True)\ndf[\"Age\"].value_counts()\nbins = [0,16,32,48,64,80] #to get the limits pd.cut(df['Age'], 5)\nlabels = ['0-16','17-32','33-48','49-64','64-80']\ndf['AgeBand'] =pd.cut(df['Age'], bins = bins, labels= labels) ## Age into band of 5\ndf[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean()  #survival rate by the title\n\n\n# Fare  - impute the nulls (1 null), bucket\ndf[\"Fare\"].fillna(df.groupby(['Pclass','Sex'])[\"Fare\"].transform(\"median\"), inplace=True)\nbins_fare = [-1,8,14,31,513] #to get the limits pd.qcut(df['Fare'], 4)\nlabels_fare = ['0-8','9-14','15-31','32-513']\ndf['FareBand'] =pd.cut(df['Fare'], bins = bins_fare, labels= labels_fare) \ndf[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean()  #survival rate by the title\n\n\n# Embarked  - impute the nulls (2 null), categorical so most frequent\n\ndf['Embarked'].value_counts()\nimp = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\ndf[\"Embarked\"] = imp.fit_transform(df[[\"Embarked\"]]).ravel()\ndf['Embarked'].value_counts()\n\n\n# New features\n\n#Family Size\ndf['FamilySize'] = df['SibSp'] + df['Parch'] + 1\ndf[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean()  #survival rate by the title\n\n#is alone?\n\n\ndf['IsAlone'] = df['FamilySize'].apply(lambda x: 1 if x==1 else 0 )\ndf[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()  #survival rate by the title\n\n\ndf.info()\n","1f9c1516":"df.head()","35cf30e2":"## ENCODING\n## Transforming categorical values to ordinal values\n## Sex, AgeBand, Title, Embarked\n\n\nencoding = {\n        \n            \"Sex\":     {\"female\": 0, \"male\": 1},\n            \"AgeBand\":     {\"0-16\": 0, \"17-32\": 1,\"33-48\": 2,\"49-64\": 3, \"64-80\": 4},\n            \"FareBand\":     {\"0-8\": 0, \"9-14\": 1,\"15-31\": 2,\"32-513\": 3},\n            \"Embarked\":     {\"S\": 0, \"C\": 1,\"Q\": 2},\n            \"Title\":     {\"Mr\": 0, \"Miss\": 1,\"Mrs\": 2,\"Master\": 4, \"Rare\":5}\n        }\n\n\ndf.replace(encoding, inplace=True)","d538e35f":"df.head()","0ca76085":"df_train = df[(df['Type']=='Train')]\nX = df_train[['Pclass','Sex','AgeBand','FareBand','Embarked','Title','IsAlone']]\nnp.any(np.isnan(X))## To check if anything is a NULL - if it is a null model will throw an error!\n\nY = df_train['Survived']\nnp.any(np.isnan(Y))## To check if anything is a NULL - if it is a null model will throw an error!\n\n\ndf_sumbission = df[(df['Type']=='Test')]\nX_sumbission = df_sumbission[['Pclass','Sex','AgeBand','FareBand','Embarked','Title','IsAlone']]\nnp.any(np.isnan(X_sumbission))## To check if anything is a NULL - if it is a null model will throw an error!","2e5ec5b8":"\n\n## Logistic Regression\n\nlogreg = LogisticRegression()\n'''\npenalty = 'none', solver = 'newton-cg',C=1 \n\n'''\n\n\nlogreg.fit(X, Y)\nY_pred = logreg.predict(X)\nlogreg.coef_\n'''\narray([[-0.98213819, -2.199304  , -0.0257264 ,  0.00286028,  0.27445946,\n         0.3443201 ,  0.36350466]])\n'''\nlogreg.intercept_\n'''\narray([ 3.07642252])\n'''\n\n\n    #Classification Report\nprint(classification_report(Y,Y_pred))\n    #Accuracy\nprint(round(logreg.score(X, Y) * 100, 2)) #Get accuracy by placing training X and Y. It will predict the Y and then compare it with the training Y \n    #Confusion Matrix\nprint(confusion_matrix(Y, Y_pred))\n    #Submission\n    \nlogreg.get_params()    #all the parameters used for the LogisticRegression()\nlogreg.set_params()    #all the parameters used for the LogisticRegression()\nlogreg.sparsify()\n    \nY_sumbission = logreg.predict(X_sumbission) ## This is what we are tested with, Y_pred needs to be uploaded to Kaggle ","0bc8e32e":"\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    Y, test_size=0.30, \n                                                    random_state=101)\nlogreg = LogisticRegression(penalty = 'none', solver = 'newton-cg' )\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_test)\n\n    #Classification Report\nprint(classification_report(y_test,Y_pred))\n    #Accuracy\nprint(round(logreg.score(X_test, y_test) * 100, 2)) #Get accuracy by placing X and Y. It will predict the Y and then compare it with the training Y \n    #Confusion Matrix\nprint(confusion_matrix(y_test, Y_pred))\n","7fa78c92":"\ndf_majority = df_train[df_train.Survived==0]\ndf_minority = df_train[df_train.Survived==1]\n\ndf_majority.describe()  #549 - not survived\ndf_minority.describe()  #342 - survived\n\n\n#can do this using Scikit Learn as well...\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=549,    # to match majority class\n                                 random_state=123) # reproducible results\n\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\n\ndf_upsampled.Survived.value_counts()\n\nX = df_upsampled[['Pclass','Sex','AgeBand','FareBand','Embarked','Title','IsAlone']]\nY = df_upsampled['Survived']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    Y, test_size=0.30, \n                                                    random_state=1011)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_test)\n\n    #Classification Report\nprint(classification_report(y_test,Y_pred))\n    #Accuracy\nprint(round(logreg.score(X_test, y_test) * 100, 2)) #Get accuracy by placing X and Y. It will predict the Y and then compare it with the training Y \n    #Confusion Matrix\nprint(confusion_matrix(y_test, Y_pred))\n","7b610ab6":"# primer on k-fold split - how the splitting happens\n\nscores = []\nlogreg = LogisticRegression()\n\nkf = KFold(n_splits=10)\nfor train_index, test_index in kf.split(X):\n    print(\"%s %s\" % (train_index, test_index))\n    X_train, X_test, y_train, y_test = X.loc[train_index], X.loc[test_index], Y.loc[train_index], Y.loc[test_index]\n    logreg.fit(X_train, y_train)\n    scores.append(logreg.score(X_test, y_test))\n\ndf_score= pd.DataFrame(scores) \n","29def621":"scores","070ff0b9":"\npoly = PolynomialFeatures(2) #Modify this to change the order of the polynomial\n\n'''\nGenerate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the \nspecified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features \nare [1, a, b, a^2, ab, b^2].\n'''\nX = poly.fit_transform(X)\n\nscores = []\nlogreg = LogisticRegression()\nkf = KFold(n_splits=10)\nfor train_index, test_index in kf.split(X):\n    print(\"%s %s\" % (train_index, test_index))\n    X_train, X_test, y_train, y_test = X[train_index], X[test_index], Y[train_index], Y[test_index]\n    logreg.fit(X_train, y_train)\n    scores.append(logreg.score(X_test, y_test))\n\n\n\n#Another way to get the cross validation score without doing the split manually (cv = integer - means it is k-fold)\nprint(cross_val_score(logreg, X, Y, cv=10)) #accuracy by deault\nprint(cross_val_score(logreg, X, Y, cv=10, scoring = 'f1')) #accuracy by deault\n#Shuffle splitting - closest to Bootstrapping\ncv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\ncross_val_score(logreg, X, Y, cv=cv)  \n","c9b5e7f9":"import statsmodels.api as sm\nfrom scipy import stats\nstats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) #can't recollect why i did this\nlogit = sm.Logit(Y,X)\nresult = logit.fit()\nresult.summary()","5b224767":"'''\n**For Pclass**\n\n\nodds(survival\/class+1)\n______________________    =    e^-0.4110 = 0.662987   = odds ratio - see the table below\nodds(survival\/class)\n\n'''","0a1eaefd":"np.exp(result.params)","9fc1c8f6":"'''\nPclass      0.662988 - increasing a class by 1 (can be from 1 to 2 or 2 to 3), decreases the odds of survival by 24%\nSex         0.118074 - being a male (when compared to being a female), decreases the odd of survival by 89%\nAgeBand     0.834307 - the more older you are the lesser odds of survival\nFareBand    1.693142 - the more you paid, the higher odds of survival\nEmbarked    1.482920\nTitle       1.491672\nIsAlone     2.627665 - \n\n'''","76d3d51d":"svc = SVC()\nsvc.fit(X, Y)\nY_pred = svc.predict(X)\nacc_svc = round(svc.score(X, Y) * 100, 2)\nprint(classification_report(Y,Y_pred))\nacc_svc","b84fc171":"'''Random Forest - model 1'''\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=101)\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train,y_train)\nY_pred = random_forest.predict(X_test)\nprint(classification_report(y_test,Y_pred))\nprint(confusion_matrix(y_test, Y_pred))\n\n\n\n# Feature importance\nimportances = list(random_forest.feature_importances_)\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(['Pclass','Sex','AgeBand','FareBand','Embarked','Title','IsAlone'], importances)]\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n\n","c60fb399":"'''Random Forest - model 2'''\nrandom_forest = RandomForestClassifier(n_estimators=200, oob_score = True , n_jobs = -1,random_state =50,max_features = \"auto\", min_samples_leaf = 50)    \nrandom_forest.fit(X_train,y_train)\nY_pred = random_forest.predict(X_test)\nprint(classification_report(y_test,Y_pred))\nprint(confusion_matrix(y_test, Y_pred))","b1aaa302":"'''Random Forest - model 3'''\n\nrandom_forest = RandomForestClassifier(n_estimators=10)\nparam_grid = { \n    'n_estimators': [200, 700],\n    'max_features': ['auto', 'sqrt', 'log2']\n}\n7865\nCV_rfc = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv= 5, n_jobs = -1)\nCV_rfc.fit(X_train,y_train)\nY_pred = CV_rfc.predict(X_test)\nprint(classification_report(y_test,Y_pred))\nprint(confusion_matrix(y_test, Y_pred))\nCV_rfc.best_params_","da3a868f":"abc = AdaBoostClassifier(n_estimators=50,learning_rate=1)\nmodel = abc.fit(X_train, y_train)\nY_pred = abc.predict(X_test)\nprint(classification_report(y_test,Y_pred))\nprint(confusion_matrix(y_test, Y_pred))","56dcd476":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X, Y)\nY_pred = knn.predict(X)\n\nprint(classification_report(Y,Y_pred))\n","fba6e0f3":"gaussian = GaussianNB()\ngaussian.fit(X, Y)\nY_pred = gaussian.predict(X)\nprint(classification_report(Y,Y_pred))","eb5a808c":"perceptron = Perceptron()\nperceptron.fit(X, Y)\nY_pred = perceptron.predict(X)\n    \nprint(classification_report(Y,Y_pred))","85a1991e":"sgd = SGDClassifier()\nsgd.fit(X,Y)\nY_pred = sgd.predict(X)\nprint(classification_report(Y,Y_pred))","412f6a62":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X, Y)\nY_pred = decision_tree.predict(X)\nprint(classification_report(Y,Y_pred))","1d8431cb":"# Model 2: Logistic Regression - with training and validation sets","a8e4452b":"## Training and Test Dataset","6760b177":"# Model 8: Random Forest","d32a3835":"# Model 6: Logistic Regression - Inferential Model","8e0fffdb":"# Model 9 - KNN","2c68db81":"# Model 7: Support Vector Machine","ecf548b3":"## Feature Extraction","3e919976":"# Model 12 - SGD Classifier\n\nLinear classifiers (SVM, logistic regression, a.o.) with SGD training.\n\nThis estimator implements regularized linear models with stochastic gradient descent (SGD) learning:","49c784c2":"## A note on confusion matrix\n\n**How it is displayed in Scikit Learn**\n![](https:\/\/static.packt-cdn.com\/products\/9781838555078\/graphics\/C13314_06_05.jpg)\n\n**How it is displayed in [Wikipedia](http:\/\/https:\/\/en.wikipedia.org\/wiki\/Confusion_matrix)**\n\n![](https:\/\/i.stack.imgur.com\/3eGlc.png)\n\n        ","6ac32e5d":"# Model 13 - Decision Trees","49fa685c":"# Model 1: Logistic Regression","8e576dae":"# Model 4: Logistic Regression - with train and validation, with CV error analysis - K-fold CV","93b993ae":"## Encoding","73bb18fd":"# Model 8: Adaptive Boost","5a16001a":"# Model 10 - Gaussian","64e13dc1":"# Model 3: Logistic Regression - with train and validation & also trying to make the dataset balanced","2497f101":"# Model 5: Logistic Regression - with train and validation, with CV error analysis, and higher order polynomials!","9d9663f9":"# Model 11 - Perceptron"}}