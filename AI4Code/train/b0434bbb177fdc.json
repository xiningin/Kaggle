{"cell_type":{"dffdbee9":"code","152ed28b":"code","160084c8":"code","c3d0be71":"code","9c63b0cc":"code","4cafeb77":"code","f7549679":"code","e6d15125":"code","93b05905":"code","b2b971ca":"code","56e3ee70":"code","50a69123":"code","e399c1d9":"code","4cd1340a":"code","16428b2b":"code","b66baa37":"code","7dc6deab":"code","79ca082f":"code","9b963acf":"code","1ba54a98":"code","7678098d":"code","e51e8b32":"code","4c81db38":"code","7b395abb":"code","5376eac6":"code","379d8472":"code","a867177f":"code","6c15b625":"code","cfd7ce4f":"code","dafd5004":"code","ba7af40e":"code","22d0007b":"code","fe1c0dad":"code","cbc74d68":"code","e8fe36a1":"code","d48074a2":"code","b7cf3eed":"code","d80f9e8f":"code","60b00bc7":"code","786c30c1":"code","fb86ee78":"code","dc0324d1":"code","16c536d3":"code","c6390cf5":"code","f9385d7d":"code","0375621d":"code","2feb073d":"code","683e4f9d":"code","7d973440":"code","6f87615d":"code","b943ca0e":"code","dc2beaf5":"code","81023e57":"code","c814d825":"code","96e70fe0":"markdown","4eb7b628":"markdown","18fa7b20":"markdown","a8b6599c":"markdown","e2894351":"markdown","afec9c2b":"markdown"},"source":{"dffdbee9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport random\n\nfrom tqdm.notebook import tqdm\n\nfrom functools import partial\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt","152ed28b":"def seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nseed_everything(seed=0)","160084c8":"train = False","c3d0be71":"weights_lambda = 0.90 # reflect the weight decay for distant days","9c63b0cc":"print([weights_lambda ** i for i in range(100)])","4cafeb77":"train_df = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-5\/train.csv')\ntrain_df = train_df.fillna('')\ntrain_df.tail()","f7549679":"test_df = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-5\/test.csv')\ntest_df = test_df.fillna('')\ntest_df.tail()","e6d15125":"num_dates_total = len(np.unique(list(train_df['Date']) + list(test_df['Date'])))\nprint(num_dates_total)\nnum_dates_test = len(np.unique(list(test_df['Date'])))\nprint(num_dates_test)\nnum_dates_train = len(np.unique(list(train_df['Date'])))\nprint(num_dates_train)","93b05905":"sample_submission = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-5\/submission.csv')\nsample_submission.head()","b2b971ca":"len(sample_submission) \/ 3","56e3ee70":"cases = train_df[\"TargetValue\"][train_df[\"Target\"] == 'ConfirmedCases'].values.reshape((-1, num_dates_train))\ncases","50a69123":"fatalities = train_df[\"TargetValue\"][train_df[\"Target\"] == 'Fatalities'].values.reshape((-1, num_dates_train))\nfatalities","e399c1d9":"#making the weights equal\nfatalities = fatalities * 10","4cd1340a":"population = train_df[\"Population\"].values[::num_dates_train * 2]\npopulation\nlen(population)","16428b2b":"weignts_cases = train_df[\"Weight\"][train_df[\"Target\"] == 'ConfirmedCases'].values[::num_dates_train]\nweignts_cases\nlen(weignts_cases)","b66baa37":"import lightgbm as lgbm\nparams = {\n    \"metric\":\"mse\",\n}\nf = lgbm.LGBMRegressor(**params)","7dc6deab":"if train:\n    days_to_predict = 45\nelse:\n    days_to_predict = 31\ntrain_days_used = num_dates_total - days_to_predict\nassert train_days_used <= num_dates_train\n\n\nfeatures_days = 40\ntest_days_predict = 40\ntrain_data = []\ntarget_data = []\ntest_data = []\nweights = []\n\n\nfat_index_add = features_days\nfat_index_add_test = test_days_predict ","79ca082f":"for i in range(len(population)):\n    for j in range(train_days_used - 1 - features_days):\n        full_train = True\n        \n        data = []\n        data = data + [population[i]]\n        max_cases = cases[i][:j+features_days].max()\n        data = data + [max_cases]\n        max_fatalities = fatalities[i][:j+features_days].max()\n        data = data + [max_fatalities]\n        data = data + list(cases[i][j:j+features_days]\/(max_cases+1))\n        data = data + list(fatalities[i][j:j+features_days]\/(max_fatalities+1))\n        train_data.append(data)\n\n        target = []\n        target_list = list(cases[i][j+features_days:j+features_days + test_days_predict]\/(max_cases+1))\n        target_list = target_list + [None]*(test_days_predict - len(target_list))\n        target = target + target_list\n\n        target_list = list(fatalities[i][j+features_days:j+features_days + test_days_predict]\/(max_fatalities+1))\n        target_list = target_list + [None]*(test_days_predict - len(target_list))\n        target = target + target_list           \n\n        target_data.append(target)\n        weights.append(weignts_cases[i]*weights_lambda**(train_days_used - test_days_predict - features_days - j - 1))\n    \n    test = []\n    test = test + [population[i]]\n    max_cases = cases[i][:train_days_used].max()\n    test = test + [max_cases]\n    max_fatalities = fatalities[i][:train_days_used].max()\n    test = test + [max_fatalities]\n    test = test + list(cases[i][train_days_used - features_days:train_days_used]\/(max_cases+1))\n    test = test + list(fatalities[i][train_days_used - features_days:train_days_used]\/(max_fatalities+1))\n    test_data.append(test)        \n\ninitial_train_data = np.stack(train_data)\ntarget_data = np.stack(target_data)\nmax_features_days = features_days\nmax_test_days_predict = test_days_predict\ninitial_test_data = np.stack(test_data)\nweights = np.array(weights)    ","9b963acf":"i = 0\ntest = []\ntest = test + [population[i]]\nmax_cases = cases[i][:train_days_used].max()\ntest = test + [max_cases]\nmax_fatalities = fatalities[i][:train_days_used].max()\ntest = test + [max_fatalities]\ntest = test + list(cases[i][train_days_used - features_days:train_days_used]\/(max_cases+1))\ntest = test + list(fatalities[i][train_days_used - features_days:train_days_used]\/(max_fatalities+1))\ntest_data.append(test)   ","1ba54a98":"predictions_cases_global = []\npredictions_cases_max_global = np.zeros((len(population), days_to_predict))\npredictions_cases_min_global = np.zeros((len(population), days_to_predict)) + 1000000\n\npredictions_fatalities_global = []\npredictions_fatalities_max_global = np.zeros((len(population), days_to_predict))\npredictions_fatalities_min_global = np.zeros((len(population), days_to_predict)) + 1000000","7678098d":"for features_days in tqdm(range(5, 40, 10)):\n    fat_index_add = features_days\n    train_data = np.concatenate([initial_train_data[:,:3], \n                                 initial_train_data[:,3+max_features_days-features_days:3+max_features_days],\n                                 initial_train_data[:,3+2*max_features_days-features_days:3+2*max_features_days],\n                                ], axis = 1).copy()\n    \n    case_predictors = []\n    for i in range(max_test_days_predict):\n        f = lgbm.LGBMRegressor(**params)\n        mask = np.logical_not(pd.isnull(target_data[:,i]))\n        f.fit(train_data[mask], target_data[:,i][mask],sample_weight = weights[mask], verbose=False)\n        case_predictors.append(f)\n\n    fatalities_predictors = []\n    for i in range(max_test_days_predict):\n        f = lgbm.LGBMRegressor(**params)\n        mask = np.logical_not(pd.isnull(target_data[:,max_test_days_predict + i]))\n        f.fit(train_data[mask], target_data[:,max_test_days_predict + i][mask],sample_weight = weights[mask], verbose=False)\n        fatalities_predictors.append(f)\n        \n    \n    for test_days_predict in range(1,32, 5):\n        \n        test_data = np.concatenate([initial_test_data[:,:3], \n                             initial_test_data[:,3+max_features_days-features_days:3+max_features_days],\n                             initial_test_data[:,3+2*max_features_days-features_days:3+2*max_features_days],\n                            ], axis = 1).copy()\n        \n        predictions_cases_sum = np.zeros((len(population), days_to_predict))\n        predictions_cases_max = np.zeros((len(population), days_to_predict))\n        predictions_cases_min= np.zeros((len(population), days_to_predict))+ 1000000\n        predictions_cases_counts = np.zeros(days_to_predict)\n\n\n        predictions_fatalities_sum = np.zeros((len(population), days_to_predict))\n        predictions_fatalities_max = np.zeros((len(population), days_to_predict))\n        predictions_fatalities_min= np.zeros((len(population), days_to_predict))+ 1000000\n        predictions_fatalities_counts = np.zeros(days_to_predict)\n\n        for step in range(days_to_predict - test_days_predict + 1):\n            predictions_cases_local = np.zeros((len(population), test_days_predict))\n            predictions_fatalities_local = np.zeros((len(population), test_days_predict))\n\n            for i in range(test_days_predict):\n                predictions_cases_local[:,i] = case_predictors[i].predict(test_data) * (test_data[:,1] + 1)\n\n            for i in range(test_days_predict):\n                predictions_fatalities_local[:,i] = fatalities_predictors[i].predict(test_data) * (test_data[:,2] + 1)\n\n\n            predictions_cases_sum[:,step:step+test_days_predict] += predictions_cases_local\n            predictions_cases_max[:,step:step+test_days_predict] = np.maximum(predictions_cases_max[:,step:step+test_days_predict], predictions_cases_local)\n            predictions_cases_min[:,step:step+test_days_predict] = np.minimum(predictions_cases_min[:,step:step+test_days_predict], predictions_cases_local)\n            predictions_cases_counts[step:step+test_days_predict] += 1\n\n            current_predictions_cases = predictions_cases_sum[:,step] \/ predictions_cases_counts[step]\n\n\n            predictions_fatalities_sum[:,step:step+test_days_predict] += predictions_fatalities_local\n            predictions_fatalities_max[:,step:step+test_days_predict] = np.maximum(predictions_fatalities_max[:,step:step+test_days_predict], predictions_fatalities_local)\n            predictions_fatalities_min[:,step:step+test_days_predict] = np.minimum(predictions_fatalities_min[:,step:step+test_days_predict], predictions_fatalities_local)\n            predictions_fatalities_counts[step:step+test_days_predict] += 1\n\n            current_predictions_fatalities = predictions_fatalities_sum[:,step] \/ predictions_fatalities_counts[step]\n\n            test_data[:,3:3+features_days-1] = test_data[:,4:3+features_days]\n            new_max = np.maximum(test_data[:,1], current_predictions_cases)\n            test_data[:,3:3+features_days -1] *= ((test_data[:,1] + 1) \/ (new_max + 1)).reshape((-1,1))\n            test_data[:,2+features_days] = current_predictions_cases \/ (new_max+1)\n            test_data[:,1] = new_max\n\n\n\n            test_data[:,3+features_days:3+features_days-1+features_days] = test_data[:,4+features_days:3+features_days+features_days]\n            new_max = np.maximum(test_data[:,2], current_predictions_fatalities)\n            test_data[:,3+features_days:3+features_days+features_days-1] *= ((test_data[:,2] + 1) \/(new_max + 1) ).reshape((-1,1))\n            test_data[:,2+features_days+features_days] = current_predictions_fatalities \/ (new_max+1)\n            test_data[:,2] = new_max\n\n\n        predictions_cases_global.append(predictions_cases_sum \/ predictions_cases_counts)\n        predictions_cases_min_global = np.minimum(predictions_cases_min_global, predictions_cases_min)\n        predictions_cases_max_global = np.maximum(predictions_cases_max_global, predictions_cases_max)\n\n        predictions_fatalities_global.append(predictions_fatalities_sum \/ predictions_fatalities_counts)\n        predictions_fatalities_min_global = np.minimum(predictions_fatalities_min_global, predictions_fatalities_min)\n        predictions_fatalities_max_global = np.maximum(predictions_fatalities_max_global, predictions_fatalities_max)\n","e51e8b32":"final_predictions_cases = np.stack(predictions_cases_global).mean(0)\nfinal_predictions_fatalities = np.stack(predictions_fatalities_global).mean(0)","4c81db38":"final_predictions_cases_std = np.stack(predictions_cases_global).std(0)\nfinal_predictions_fatalities_std = np.stack(predictions_fatalities_global).std(0)","7b395abb":"valid_len = num_dates_train + days_to_predict - num_dates_total\nvalid_len","5376eac6":"valid_true_cases = cases[:,-valid_len:]\nvalid_true_fatalities = fatalities[:, -valid_len:]","379d8472":"predict_mean_cases = final_predictions_cases[:,:valid_len].copy()\npredict_min_cases = predictions_cases_min_global[:,:valid_len].copy()\npredict_max_cases = predictions_cases_max_global[:,:valid_len].copy()\n\npredict_mean_fatalities = final_predictions_fatalities[:,:valid_len].copy()\npredict_min_fatalities = predictions_fatalities_min_global[:,:valid_len].copy()\npredict_max_fatalities = predictions_fatalities_max_global[:,:valid_len].copy()","a867177f":"def compute_loss_L(true_array, predicted_array, tau, weight):\n    array = predicted_array * (predicted_array > 0)\n    abs_diff = np.absolute(true_array - array)\n    result = abs_diff * (1 -tau) * (array > true_array) + abs_diff * (tau) * (array <= true_array)\n    result = (result.mean(1)) * weight\n#     print(result.mean())\n    return result.mean()","6c15b625":"def compute_loss(true_array, mean_array, min_array, max_array, weights):\n    result = (compute_loss_L(true_array, max_array, 0.95, weights) + \n              compute_loss_L(true_array, min_array, 0.05, weights) + \n              compute_loss_L(true_array, mean_array, 0.5, weights))\n    return result \/ 3","cfd7ce4f":"x0 = [1,0,0,0]\n\ndef normalize(x, mean_array, min_array, max_array, base = None):\n    if base is None:\n        base = np.zeros_like(mean_array)\n        lamb = 0\n    else:\n        lamb = x[3]\n    deviation = np.array([lamb * n for n in range(base.shape[1])])\n    new_array = base + (x[0] * mean_array + x[1] * min_array + x[2]*max_array) * (deviation + 1).reshape((1,-1))\n#     print(deviation)\n    return new_array","dafd5004":"def fun(x, mean_array, min_array, max_array, true_array, weights, tau, base = None):\n    new_array = normalize(x, mean_array, min_array, max_array, base = base)\n    return compute_loss_L(true_array, new_array, tau, weights)","ba7af40e":"from scipy.optimize import minimize\nfrom functools import partial","22d0007b":"x = [1.13952572e+00,  5.49225865e-02, -2.41205637e-01,  0]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_cases, min_array = predict_min_cases, max_array = predict_max_cases, weights = weignts_cases, tau = 0.5, true_array = valid_true_cases)\n    res = minimize(part_func, x0 = [1,0,0,0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_final_predictions_cases = normalize(x, final_predictions_cases, predictions_cases_min_global, predictions_cases_max_global)\n\n\nx = [ 0.15951811, -0.16219168, -0.45721537, -0.01818924]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_cases, min_array = predict_min_cases, max_array = predict_max_cases, weights = weignts_cases, tau = 0.05, true_array = valid_true_cases, \n                        base = new_final_predictions_cases[:,:valid_len])\n    res = minimize(part_func, x0 = [1,0,0, 0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_predictions_cases_min_global = normalize(x, final_predictions_cases, predictions_cases_min_global, predictions_cases_max_global, base = new_final_predictions_cases)\n\n\nx = [0.69243738, -0.25191857,  0.14696969,  0.00829207]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_cases, min_array = predict_min_cases, max_array = predict_max_cases, weights = weignts_cases, tau = 0.95, true_array = valid_true_cases,\n                       base = new_final_predictions_cases[:,:valid_len])\n    res = minimize(part_func, x0 = [1,0,0, 0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_predictions_cases_max_global = normalize(x, final_predictions_cases, predictions_cases_min_global, predictions_cases_max_global, base = new_final_predictions_cases)\n\nfinal_predictions_cases = new_final_predictions_cases\npredictions_cases_min_global = new_predictions_cases_min_global\npredictions_cases_max_global = new_predictions_cases_max_global","fe1c0dad":"x = [ 8.14860611e-01,  8.80132452e-02, -1.60911727e-01,  0]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_fatalities, min_array = predict_min_fatalities, max_array = predict_max_fatalities, weights = weignts_cases, \n                        tau = 0.5, true_array = valid_true_fatalities)\n    res = minimize(part_func, x0 = [1,0,0, 0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_final_predictions_fatalities = normalize(x, final_predictions_fatalities, predictions_fatalities_min_global, predictions_fatalities_max_global)\n\n\nx = [-0.92953006,  0.998952,   -0.40161728, -0.04630638]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_fatalities, min_array = predict_min_fatalities, max_array = predict_max_fatalities, weights = weignts_cases, \n                        tau = 0.05,\n                        true_array = valid_true_fatalities, base = new_final_predictions_fatalities[:,:valid_len])\n    res = minimize(part_func, x0 = [1,0,0, 0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_predictions_fatalities_min_global = normalize(x, final_predictions_fatalities, predictions_fatalities_min_global, predictions_fatalities_max_global,\n                                                  base = new_final_predictions_fatalities)\n\n\nx = [0.8965989,   0.02662769, -0.01948303,  0.00444532]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_fatalities, min_array = predict_min_fatalities, max_array = predict_max_fatalities, weights = weignts_cases, \n                        tau = 0.95, \n                        true_array = valid_true_fatalities, base = new_final_predictions_fatalities[:,:valid_len])\n    res = minimize(part_func, x0 = [1,0,0, 0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_predictions_fatalities_max_global = normalize(x, final_predictions_fatalities, predictions_fatalities_min_global, predictions_fatalities_max_global,\n                                                  base = new_final_predictions_fatalities)\n\nfinal_predictions_fatalities = new_final_predictions_fatalities\npredictions_fatalities_min_global = new_predictions_fatalities_min_global\npredictions_fatalities_max_global = new_predictions_fatalities_max_global","cbc74d68":"predict_mean_cases = final_predictions_cases[:,:valid_len].copy()\npredict_min_cases = predictions_cases_min_global[:,:valid_len].copy()\npredict_max_cases = predictions_cases_max_global[:,:valid_len].copy()\n\npredict_mean_fatalities = final_predictions_fatalities[:,:valid_len].copy()\npredict_min_fatalities = predictions_fatalities_min_global[:,:valid_len].copy()\npredict_max_fatalities = predictions_fatalities_max_global[:,:valid_len].copy()","e8fe36a1":"if train:\n    total_loss = (compute_loss(valid_true_cases, predict_mean_cases, predict_min_cases, \n                                   predict_max_cases , weignts_cases) + \n                     compute_loss(valid_true_fatalities, predict_mean_fatalities, predict_min_fatalities, \n                                  predict_max_fatalities, weignts_cases)) \/ 2\n\n    print(total_loss)","d48074a2":"submission_mean_cases = np.zeros((len(population), 45))\nsubmission_min_cases = np.zeros((len(population), 45))\nsubmission_max_cases = np.zeros((len(population), 45))\n\nsubmission_mean_fatalities = np.zeros((len(population), 45))\nsubmission_min_fatalities = np.zeros((len(population), 45))\nsubmission_max_fatalities = np.zeros((len(population), 45))","b7cf3eed":"submission_mean_cases[:, -days_to_predict:] = final_predictions_cases\nsubmission_min_cases[:, -days_to_predict:] = predictions_cases_min_global\nsubmission_max_cases[:, -days_to_predict:] = predictions_cases_max_global\n\n\nsubmission_mean_fatalities[:, -days_to_predict:] = final_predictions_fatalities \/ 10\nsubmission_min_fatalities[:, -days_to_predict:] = predictions_fatalities_min_global \/ 10\nsubmission_max_fatalities[:, -days_to_predict:] = predictions_fatalities_max_global \/ 10","d80f9e8f":"submission_mean_cases[:, :-days_to_predict] = cases[:, num_dates_total-45:num_dates_total-days_to_predict]\nsubmission_min_cases[:, :-days_to_predict] = cases[:, num_dates_total-45:num_dates_total-days_to_predict]\nsubmission_max_cases[:, :-days_to_predict] = cases[:, num_dates_total-45:num_dates_total-days_to_predict]\n\n\nsubmission_mean_fatalities[:, :-days_to_predict] = fatalities[:, num_dates_total-45:num_dates_total-days_to_predict] \/ 10\nsubmission_min_fatalities[:, :-days_to_predict] = fatalities[:, num_dates_total-45:num_dates_total-days_to_predict] \/ 10\nsubmission_max_fatalities[:, :-days_to_predict] = fatalities[:, num_dates_total-45:num_dates_total-days_to_predict] \/ 10 ","60b00bc7":"loss_1 = compute_loss(fatalities[:,-14:] \/ 10, submission_mean_fatalities[:,:14] , submission_min_fatalities[:,:14], \n                                   submission_max_fatalities[:,:14],weignts_cases * 10)","786c30c1":"loss_2 = compute_loss(cases[:,-14:], submission_mean_cases[:,:14] , submission_min_cases[:,:14], \n                                   submission_max_cases[:,:14],weignts_cases)","fb86ee78":"loss_1","dc0324d1":"loss_2","16c536d3":"(loss_1 + loss_2) \/ 2","c6390cf5":"#0.25734741800417726","f9385d7d":"submission_file = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-5\/submission.csv')","0375621d":"submission_file","2feb073d":"submission = []\nfor i in range(len(submission_mean_cases)):\n    for j in range(len(submission_mean_cases[0])):\n        submission.append(submission_min_cases[i][j])\n        submission.append(submission_mean_cases[i][j])\n        submission.append(submission_max_cases[i][j])\n        \n        submission.append(submission_min_fatalities[i][j])\n        submission.append(submission_mean_fatalities[i][j])\n        submission.append(submission_max_fatalities[i][j])","683e4f9d":"submission = [max(0,x) for x in submission]","7d973440":"submission_file['TargetValue'] = submission","6f87615d":"submission_file.to_csv('submission.csv', index = False)","b943ca0e":"submission_file","dc2beaf5":"import matplotlib.pyplot as plt\ndef plot_results(true_array, mean_array, max_array, min_array):\n    nans = np.array([None]*len(true_array))\n    plt.plot(true_array)\n    plt.plot(np.concatenate([nans,mean_array[-days_to_predict:]]))\n    plt.plot(np.concatenate([nans,min_array[-days_to_predict:]]))\n    plt.plot(np.concatenate([nans,max_array[-days_to_predict:]]))\n    \n    plt.show()","81023e57":"for i in np.random.randint(0, len(cases),30):\n    plot_results(cases[i], submission_mean_cases[i], submission_max_cases[i], submission_min_cases[i])","c814d825":"for i in np.random.randint(0, len(cases),10):\n    plot_results(fatalities[i], submission_mean_fatalities[i], submission_max_fatalities[i], submission_min_fatalities[i])","96e70fe0":"Quite messy notebook using mix of several lightgmb models predictions.","4eb7b628":"# EDA","18fa7b20":"# Making prediction file","a8b6599c":"# Visualisation of predictions","e2894351":"# Preparing data","afec9c2b":"# Making simple submission and valiadtion"}}