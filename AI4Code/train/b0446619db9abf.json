{"cell_type":{"049c7143":"code","b19176ee":"code","28e6c882":"code","d9b8dcc6":"code","05a2d23a":"code","75071f0b":"code","6b965391":"code","d4d7e4e2":"code","e8692266":"code","6e8dba8e":"code","d6db8c6c":"code","1cc6c6a7":"code","5f95b485":"code","b28cfa59":"code","b5983893":"code","f72d7aaa":"code","57e545a9":"code","7af08b3b":"code","5d85785e":"code","c01b01c0":"code","5ec81aaf":"code","3916cf2c":"code","dbd13b6c":"code","f58788bc":"markdown","fff6851e":"markdown","c5246387":"markdown","5fe21b98":"markdown","671fe1ca":"markdown","0f4b4d28":"markdown","7ad53178":"markdown","3e18bea1":"markdown","c9e9ca73":"markdown","3a4f50b4":"markdown","b71f565d":"markdown","0219c9d7":"markdown","5bd1bdfd":"markdown","dcf227a1":"markdown","bc42d0e5":"markdown","6d014b48":"markdown","f221c2b4":"markdown","fbecf2fd":"markdown","961d2bec":"markdown"},"source":{"049c7143":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b19176ee":"!pip install tensorflow==1.14.0","28e6c882":"!pip install pyspellchecker","d9b8dcc6":"!pip install pandas-profiling --ignore-installed","05a2d23a":"# to hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# basic data processing\nimport os\nimport datetime\nimport pandas as pd\nimport numpy as np\n\n# for EDA\nfrom pandas_profiling import ProfileReport\n\n# for text preprocessing\nimport re\nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom spellchecker import SpellChecker\n\n# progress bar\nfrom tqdm.auto import tqdm\nfrom tqdm import tqdm_notebook\n\n# instantiate\ntqdm.pandas(tqdm_notebook)\n\n# for wordcloud\nfrom PIL import Image\nfrom wordcloud import WordCloud\n\n# for aesthetics and plots\nfrom IPython.display import display, Markdown, clear_output\nfrom termcolor import colored\n\nimport matplotlib.pyplot as plt\n\nimport plotly.graph_objects as go\nfrom plotly.offline import plot, iplot\nfrom plotly.subplots import make_subplots\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\n\n# for model\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport keras.layers as layers\nfrom keras.models import Model\nfrom keras import backend as K\nimport keras\nfrom keras.models import load_model\n\ndisplay(Markdown('_All libraries are imported successfully!_'))","75071f0b":"col_names =  ['target', 'id', 'date', 'flag','user','text']\n\ndf = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding = \"ISO-8859-1\", names=col_names)\n\nprint(colored('DATA','blue',attrs=['bold']))\ndisplay(df.head())","6b965391":"from pandas_profiling import ProfileReport\n\nprofile = ProfileReport(df, title='Pandas Profiling Report')\nprofile","d4d7e4e2":"# dropping irrelevant columns\ndf.drop(['id', 'date', 'flag', 'user'], axis=1, inplace=True)\n\n# replacing positive sentiment 4 with 1\ndf.target = df.target.replace(4,1)\n\ntarget_count = df.target.value_counts()\n\ncategory_counts = len(target_count)\ndisplay(Markdown('__Number of categories__: {}'.format(category_counts)))","e8692266":"# set of stop words declared\nstop_words = stopwords.words('english')\n\ndisplay(Markdown('__List of stop words__:'))\ndisplay(Markdown(str(stop_words)))","6e8dba8e":"updated_stop_words = stop_words.copy()\nfor word in stop_words:\n    if \"n't\" in word or \"no\" in word or word.endswith('dn') or word.endswith('sn') or word.endswith('tn'):\n        updated_stop_words.remove(word)\n\n# custom select words you don't want to eliminate\nwords_to_remove = ['for','by','with','against','shan','don','aren','haven','weren','until','ain','but','off','out']\nfor word in words_to_remove:\n    updated_stop_words.remove(word)\n\ndisplay(Markdown('__Updated list of stop words__:'))\ndisplay(Markdown(str(updated_stop_words)))","d6db8c6c":"# Defining dictionary containing all emojis with their meanings.\nemojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n\n# Defining regex patterns.\nurlPattern        = r\"((http:\/\/)[^ ]*|(https:\/\/)[^ ]*|( www\\.)[^ ]*)\"\nuserPattern       = '@[^\\s]+'\nalphaPattern      = \"[^a-zA-Z0-9]\"\nsequencePattern   = r\"(.)\\1\\1+\"\nseqReplacePattern = r\"\\1\\1\"\n\n# creating instance of spellchecker\nspell = SpellChecker()\n\n# creating instance of lemmatizer\nlemm = WordNetLemmatizer()\n\n\ndef preprocess(tweet):\n    # lowercase the tweets\n    tweet = tweet.lower().strip()\n    \n    # REMOVE all URls\n    tweet = re.sub(urlPattern,'',tweet)\n    \n    # Replace all emojis.\n    for emoji in emojis.keys():\n        tweet = tweet.replace(emoji, \"emoji\" + emojis[emoji])        \n    \n    # Remove @USERNAME\n    tweet = re.sub(userPattern,'', tweet)        \n    \n    # Replace all non alphabets.\n    tweet = re.sub(alphaPattern, \" \", tweet)\n    \n    # Replace 3 or more consecutive letters by 2 letter.\n    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n\n    splitted_tweet = tweet.split()\n    # spell checks\n#     misspelled = spell.unknown(splitted_tweet)\n#     if misspelled == set():\n#         pass\n#     else:\n#         for i,word in enumerate(misspelled):\n#             splitted_tweet[i] = spell.correction(word)\n\n    tweetwords = ''\n    for word in splitted_tweet:\n        # Checking if the word is a stopword.\n        if word not in updated_stop_words:\n            if len(word)>1:\n                # Lemmatizing the word.\n                lem_word = lemm.lemmatize(word)\n                tweetwords += (lem_word+' ')\n    \n    return tweetwords","1cc6c6a7":"df['text'] = df['text'].progress_apply(lambda x: preprocess(x))\nprint(colored('DATA','blue',attrs=['bold']))\ndisplay(df.head())","5f95b485":"def plot_wordcloud(text, mask, title = None):\n    wordcloud = WordCloud(background_color='black', max_words = 200,\n                          max_font_size = 200, random_state = 42, mask = mask)\n    wordcloud.generate(text)\n    \n    plt.figure(figsize=(25,25))\n    \n    plt.imshow(wordcloud)\n    plt.title(title, fontdict={'size': 40, 'verticalalignment': 'bottom'})\n    plt.axis('off')\n    plt.tight_layout()","b28cfa59":"pos_text = \" \".join(df[df['target'] == 1]['text'])\npos_mask = np.array(Image.open('\/kaggle\/input\/sentiment140-vis-img\/source\/upvote.png'))\n\nplot_wordcloud(pos_text, pos_mask, title = 'Most common 200 words in positive tweets')","b5983893":"neg_text = \" \".join(df[df['target'] == 0].text)\nneg_mask = np.array(Image.open('\/kaggle\/input\/sentiment140-vis-img\/source\/downvote.png'))\n\nplot_wordcloud(neg_text, neg_mask, title = 'Most common 200 words in negative tweets')","f72d7aaa":"def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=369):\n    np.random.seed(seed)\n    perm = np.random.permutation(df.index)\n    m = len(df.index)\n    train_end = int(train_percent * m)\n    validate_end = int(validate_percent * m) + train_end\n    train = df.iloc[perm[:train_end]]\n    validate = df.iloc[perm[train_end:validate_end]]\n    test = df.iloc[perm[validate_end:]]\n    return train, validate, test\n\ntrain_df, val_df, test_df = train_validate_test_split(df)\n\nprint('Train: {}, Validation: {}, Test: {}'.format(train_df.shape, val_df.shape, test_df.shape))\n\nprint(colored('\\nTRAIN DATA','magenta',attrs=['bold']))\ndisplay(train_df.head())\n\ntrain_text = train_df['text'].tolist()\ntrain_text = np.array(train_text, dtype=object)[:, np.newaxis]\ntrain_label = np.asarray(pd.get_dummies(train_df['target']), dtype = np.int8)\n\nval_text = val_df['text'].tolist()\nval_text = np.array(val_text, dtype=object)[:, np.newaxis]\nval_label = np.asarray(pd.get_dummies(val_df['target']), dtype = np.int8)\n\ntest_text = test_df['text'].tolist()\ntest_text = np.array(test_text, dtype=object)[:, np.newaxis]\ntest_label = np.asarray(pd.get_dummies(test_df['target']), dtype = np.int8)","57e545a9":"# we can change this model. check the url 'https:\/\/tfhub.dev\/google\/' for more\nembed = hub.Module(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/2\")\n\nembed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\nprint(\"Embedding size: \",embed_size)","7af08b3b":"# Compute a representation for each message, showing various lengths supported.\nword = \"Elephant\"\nsentence = \"I am a sentence for which I would like to get its embedding.\"\nparagraph = (\"Universal Sentence Encoder embeddings also support short paragraphs. \"\n             \"There is no hard limit on how long the paragraph is. Roughly, the longer the more 'diluted' the embedding will be.\")\nmessages = [word, sentence, paragraph]\n\n# Reduce logging output.\ntf.logging.set_verbosity(tf.logging.ERROR)\n\nwith tf.Session() as session:\n    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n    message_embeddings = session.run(embed(messages))\n\n    for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n        print(\"Message: {}\".format(messages[i]))\n        print(\"Embedding size: {}\".format(len(message_embedding)))\n        message_embedding_snippet = \", \".join((str(x) for x in message_embedding[:3]))\n        print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))","5d85785e":"def UniversalEmbedding(x):\n    return embed(tf.squeeze(tf.cast(x, tf.string)), \n                 signature=\"default\", as_dict=True)[\"default\"]\n\ninput_text = layers.Input(shape=(1,), dtype=\"string\")\nembedding = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,))(input_text)\n\n# experiment on the custom FC layer here\n#------------------------------------------------------#\nx = layers.Dense(256, activation='relu')(embedding)\nx = layers.Dropout(0.25)(x)\nx = layers.Dense(64, activation='relu')(x)\nx = layers.Dropout(0.125)(x)\nx = layers.Dense(category_counts, activation='sigmoid')(x)\n#------------------------------------------------------#\n\nmodel_sa = Model(inputs=[input_text], outputs=x)\n\n# we are selecting Adam optimizer - one of the best optimizer in this field\nopt = keras.optimizers.Adam(learning_rate=0.001)\n\n# setting `binary_crossentropy` as loss function for the classifier\nmodel_sa.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\nmodel_sa.summary()","c01b01c0":"with tf.Session() as session:\n    K.set_session(session)\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n    history = model_sa.fit(train_text, train_label,\n                            validation_data=(val_text, val_label),\n                            epochs=10,\n                            batch_size=64,\n                            shuffle=True)\n    model_sa.save_weights('best_model.h5')","5ec81aaf":"# load the saved model\nwith tf.Session() as session:\n    K.set_session(session)\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n    model_sa.load_weights('best_model.h5')\n    _, train_acc = model_sa.evaluate(train_text, train_label)\n    _, test_acc = model_sa.evaluate(test_text, test_label)\n\nclear_output()\ndisplay(Markdown('__Train Accuracy__: {}, __Test Accuracy__: {}'.format(round(train_acc,4), round(test_acc,4))))","3916cf2c":"fig = make_subplots(rows=1, cols=2)\n\nfig.add_trace(go.Scatter(x=list(range(50)), y=history.history['accuracy'], name='train'),\n              row=1, col=1)\nfig.add_trace(go.Scatter(x=list(range(50)), y=history.history['val_accuracy'], name='validation'),\n              row=1, col=1)\n\nfig.add_trace(go.Scatter(x=list(range(50)), y=history.history['loss'], name='train'),\n              row=1, col=2)\nfig.add_trace(go.Scatter(x=list(range(50)), y=history.history['val_loss'], name='validation'),\n              row=1, col=2)\n\nfig.update_layout(height=600, width=900, showlegend=False,hovermode=\"x\",\n                  title_text=\"Train and Validation Accuracy and Loss\")\nfig.show()","dbd13b6c":"with tf.Session() as session:\n    K.set_session(session)\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n    model_sa.load_weights('best_model.h5')\n    predicts = model_sa.predict(test_text, batch_size=32)\n\ncategories = train_df['target'].unique().tolist()\n\npredict_logits = predicts.argmax(axis=1)\ntest_df['predicted'] = [categories[i] for i in predict_logits]\n\ndef highlight_rows(x):\n    if x['target'] != x['predicted']:\n        return ['background-color: #d65f5f']*3\n    else:\n        return ['background-color: lightgreen']*3\n\nclear_output()\ndisplay(test_df.head(20).style.apply(highlight_rows, axis=1))","f58788bc":"# Model creation\n\nWe have loaded the Universal Sentence Encoder as variable `embed`. To have it work with Keras, it is necessary to wrap it in a Keras Lambda layer and explicitly cast its input as a string. Then we build the Keras model in its standard Functional API. We can view the model summary and realize that __only the Keras layers are trainable, that is how the transfer learning task works by assuring the Universal Sentence Encoder weights untouched__.\n\nNow, the let's eliminate the confusion between the terms that is used in deep learning aspect - __loss function__ and __optimizer__.\n\nThe __loss function__ is a mathematical way of measuring how wrong the predictions are.\n\nDuring the training process, we tweak and change the parameters (weights) of the model to __try and minimize that loss function__, and make the predictions as correct and optimized as possible. But how exactly is it done, by how much, and when?\n\n_This is where optimizers come in_. They tie together the loss function and model parameters by updating the model in response to the output of the loss function.","fff6851e":"# Pre-trained Embedding model\n\n## Understanding the difference\n\nThere are two types of embedding in NLP domain:\n\n__WORD EMBEDDING__\n\n* Baseline\n    1. __Word2Vec__\n    2. __GloVe__\n    3. __FastText__\n* State-of-the-art\n    1. __ELMo__ (`E`mbeddeding from `L`anguage `Mo`del)\n    2. __BERT__ (`B`idirectional `E`ncoder `R`epresentations from `T`ransformers)\n    3. __OpenAI GPT__ (`G`enerative `P`re-Training `T`ransformer)\n    4. __ULMFiT__ (`U`niversal `L`anguage `M`odel `Fi`ne-`T`uning) - This is more of a process that includes word embedding along with NN architecture.\n\n__SENTENCE EMBEDDING__\n\n* Basline\n    1. __Bag of Words__\n    2. __Doc2Vec__\n* State-of-the-art\n    1. __Sentence BERT__\n    2. __Skip-Thoughts and Quick-Thoughts__\n    2. __InferSent__\n    3. __Universal Sentence Encoder__\n\nSo, the fundamental difference is that __Word Embedding__ turns a word to N-dimensional vector, but the __Sentence Embedding__ is much more powerful because it is able to embed not only words but phrases and sentences as well.\n\n__ULMFiT__ is considered to be the best choice for Transfer Learning in NLP but it is built using __fast.ai__ library in which the code implementation is different from that of __Keras__ or __Tensorflow__, hence for this notebook, we will be using __Universal Sentence Encoder__.\n\n## Universal Sentence Encoder\n\nIt can be used for text classification, semantic similarity, clustering and other natural language tasks. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs.\n\nIt takes __variable length English text as input__ and __outputs a 512-dimensional vector__. Handling variable length text input sounds great, but the problem is that as sentence keeps getting longer counted by words, the more diluted embedding results could be.\n\nHence, there are 2 Universal Sentence Encoders to choose from with different encoder architectures to achieve distinct design goals:\n* __Transformer__ architecture that targets high accuracy at the cost of greater model complexity and resource consumption\n* __Deep Averaging Network(DAN)__ that targets efficient inference with slightly reduced accuracy using simple architecture\n\nBoth models were trained with the __Stanford Natural Language Inference (SNLI)__ corpus. The [SNLI](https:\/\/nlp.stanford.edu\/pubs\/snli_paper.pdf) corpus is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as __recognizing textual entailment (RTE)__. Essentially, the models were trained to learn the semantic similarity between the sentence pairs.\n\n__This model is trained using DAN.__ DAN works in three simple steps:\n1. take the vector average of the embeddings associated with an input sequence of tokens\n2. pass that average through one or more feedforward layers\n3. perform (linear) classification on the final layer\u2019s representation\n\nThe primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence.\n\nThis module is about 1GB. Depending on your network speed, it might take a while to load the first time you run inference with it. After that, loading the model should be faster as modules are cached by default.","c5246387":"Now, we will apply the function _`preprocess`_ on each value of the column `text` where tweets are located.","5fe21b98":"The Learning Curve of loss and accuracy of the model on each epoch are shown as below:","671fe1ca":"# Training\n\nNow, we train the model with the training dataset and validate its performance at the end of each training epoch with validation dataset.","0f4b4d28":"Let us explore the data for better understanding.","7ad53178":"Let's take a quick look at the words that are frequently used for positive and negative tweets.","3e18bea1":"We will be requiring only __target__ and __text__ columns. As observed from the above report, we have only __positive (4)__ and __negative (0)__ sentiment. We will replace 4 as 1 for convenience.\n\n![](https:\/\/media1.tenor.com\/images\/53278eb207d72b741c783acf9421bb77\/tenor.gif?itemid=16273067)\n\nAlso, it's a __perfectly balanced__ dataset without any skewness - equal distribution of positive and negative sentiment.","c9e9ca73":"# Data Split\n\nWe will shuffle the dataset and split it to gives __train__, __validation__ and __test__ dataset. It's important to shuffle our dataset before training. The split is in the ratio of __6:2:2__ respectively.","3a4f50b4":"# Text Preprocessing\n\nAt first glance, it's evident that the data is not clean. Tweet texts often consists of other user mentions, hyperlink texts, emoticons and special characters which no value as feature to the model we are training. So we need to get rid of them. To do this, we need to perform 4 crucial process step-by-step:\n\n1. __Hyperlinks and Mentions__: In Twitter, people can tag\/mention other people's ID and share URLs\/hyperlinks. We need to eliminate this as well.\n\n2. __Stopwords__ : These are commonly used words (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) which have no contextual meaning in a sentence and hence we ignore them when indexing entries for searching and when retrieving them as the result of a search query.\n\n3. __Spelling Correction__: We can definitely expect incorrect spellings in the tweets\/data, and we need to fix as many as possible, because without doing this, the following step will not work properly.\n\n4. __Stemming\/Lemmatization__: The goal of both stemming and lemmatization is to reduce inflectional and derivationally related forms of a word to a common base form. However, there is a difference which you can understand from the image below.\n\n![](https:\/\/miro.medium.com\/max\/2050\/1*ES5bt7IoInIq2YioQp2zcQ.png)\n\nLemmatization is similar to stemming with one difference - the final form is also a __meaningful word__. Thus, stemming operation does not need a dictionary like lemmatization. Hence, here we will be going ahead with lemmetization.\n\n1, 2 and 4 can be done using the library __`NLTK`__, and spell-checking using __`pyspellchecker`__.","b71f565d":"# Data Preprocessing\n\nIn this notebook, I am using __[Sentiment140](http:\/\/help.sentiment140.com\/for-students)__. It contains two labeled data:\n* data of __1.6 Million Tweets__ to be used as __train,validation,test split data__\n* data of __498 Tweets__ to be used as another fresh __test data__\n\nData dictionary are as follows:\n\n* __target__: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n* __ids__: The id of the tweet (2087)\n* __date__: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n* __flag__: The query (lyx). If there is no query, then this value is NO_QUERY.\n* __user__: the user that tweeted (robotickilldozr)\n* __text__: the text of the tweet (Lyx is cool)\n\n__NOTE__: The training data isn't perfectly categorised as it has been created by tagging the text according to the emoji present. So, any model built using this dataset may have lower than expected accuracy, since the dataset isn't perfectly categorised.","0219c9d7":"# Importing libraries\n\nHere, we are using TF 1.14.0 to run the code snippets.","5bd1bdfd":"# Evaluation\n\nNow that we have trained the model, we can evaluate its performance. We will some evaluation metrics and techniques to test the model.","dcf227a1":"Some words like `not`, `haven't`, `don't` are included in stopwords and ignoring them will make sentences like `this was not good` and `this was good` or `He is a nice guy... not!` and `He is a nice guy... !` have same predictions. So we need to eliminate the words that expresses negation, denial, refusal or prohibition.","bc42d0e5":"# Prediction\n\nFinally, lets perform some predictions to see where and why are we getting false positives.","6d014b48":"# Executive Summary\n\nThe objective of this notebook is to analyze and classify the sentiment of Tweets obtained from Twitter as positive or negative.\n\nAfter identifying the relevant columns required, we performed an intensive text preprocessing that can be provided as an input to the model, without the need to even tokenization - a step that is required in traditional deep learning approach.\n\nUsing Universal Sentence Encoder, which is a state-of-the-art pre-trained sentence embedding module, we contextualized the tweets and created a model that holds the information as to which tweets are referring to a positive sentiment, and which ones are negative.\n\nAn interesting observation is that despite the architecture on which this Encoder works yields less accuracy, the dataset not properly tagged, and such a small number of NN layers - we are obtaining a pretty decent accuracy.\n\nPLEASE UPVOTE THIS KERNEL IF YOU LIKE IT!\n\n__NEXT KERNEL: Using a different model for the same objective!! STAY TUNED.__","f221c2b4":"We have loaded the Universal Sentence Encoder and computing the embeddings for some text can be as easy as shown below.","fbecf2fd":"<div style=\"display:block\">\n    <div style=\"width: 69%; display: inline-block\">\n        <h5  style=\"color:blue; text-align: center; font-size:25px;\">Sentiment Analysis using Transfer Learning<\/h5>\n        <div style=\"width: 90%; text-align: center; display: inline-block;\"><strong>Author: <\/strong>TAMOGHNA SAHA<\/div>\n    <\/div>\n<\/div>\n\n![](https:\/\/lionbridge.ai\/wp-content\/uploads\/2018\/10\/ai-terms_sentiment-analysis.jpg)\n\nSentiment Classification is a perfect problem in Natural language Processing (NLP) for getting started in it. As the name suggests, it is classification of peoples opinion or expressions into different sentiments, such as __Positive__, __Neutral__, and __Negative__.\n\nNLP is a powerful tool, but in real-world we often come across tasks which suffer from data deficit and poor model generalisation. __Transfer learning__ solved this problem by allowing us to take a pre-trained model of a task and use it for others.\n\nBefore starting, I would like to give credit to the following kernel that inspired me to write this. Make sure you visit there kernels as well. :)\n\n* [NLP Beginner - Text Classification using LSTM](https:\/\/www.kaggle.com\/arunrk7\/nlp-beginner-text-classification-using-lstm)\n* [Twitter Sentiment Analysis](https:\/\/www.kaggle.com\/paoloripamonti\/twitter-sentiment-analysis)\n* [Twitter Sentiment Analysis for Beginners](https:\/\/www.kaggle.com\/stoicstatic\/twitter-sentiment-analysis-for-beginners)\n\n# Loading the data paths","961d2bec":"Now, let us define the function to perform the necessary preprocessing.\n\n__NOTE__: I have skipped the spell check section since it was taking too much time to process. You can perform it if you want."}}