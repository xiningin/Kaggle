{"cell_type":{"0c2e8c64":"code","a95e10fc":"code","9c53c9de":"code","f6e52683":"code","c3de7965":"code","68083034":"code","77fcc00c":"code","2b7c6408":"code","45bdb3fe":"code","ce76b157":"code","3991fff0":"code","4c6bcd45":"code","9abdc628":"code","2abff074":"code","463db213":"code","d1c1a0e9":"code","8f3ba7ec":"code","9c4f9257":"code","3504ca3b":"code","1fcaa77b":"code","c71221eb":"markdown"},"source":{"0c2e8c64":"import numpy as np # linear algebra\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","a95e10fc":"from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler","9c53c9de":"train_file = \"..\/input\/train.csv\"\ntest_file = \"..\/input\/test.csv\"\noutput_file = \"submission.csv\"","f6e52683":"raw_data = np.loadtxt(train_file, skiprows=1, dtype='int', delimiter=',')\nx_train, x_val, y_train, y_val = train_test_split(\n    raw_data[:,1:], raw_data[:,0], test_size=0.1)","c3de7965":"fig, ax = plt.subplots(2, 1, figsize=(12,6))\nax[0].plot(x_train[0])\nax[0].set_title('784x1 data')\nax[1].imshow(x_train[0].reshape(28,28), cmap='gray')\nax[1].set_title('28x28 data')","68083034":"x_train = x_train.reshape(-1, 28, 28, 1)\nx_val = x_val.reshape(-1, 28, 28, 1)","77fcc00c":"x_train = x_train.astype(\"float32\")\/255.\nx_val = x_val.astype(\"float32\")\/255.","2b7c6408":"y_train = to_categorical(y_train)\ny_val = to_categorical(y_val)\n#example:\nprint(y_train[0])","45bdb3fe":"model = Sequential()\nmodel.add(Conv2D(filters = 128, kernel_size = (3, 3), activation='tanh', input_shape = (28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(strides=(2,2)))\nmodel.add(Conv2D(filters = 128, kernel_size = (3, 3), activation='tanh', input_shape = (28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(strides=(2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.summary()","ce76b157":"datagen = ImageDataGenerator(zoom_range = 0.1,\n                            height_shift_range = 0.1,\n                            width_shift_range = 0.1,\n                            rotation_range = 10)","3991fff0":"model.compile(loss='categorical_crossentropy', optimizer = Adam(lr=1e-4), metrics=[\"accuracy\"])","4c6bcd45":"annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)","9abdc628":"hist = model.fit_generator(datagen.flow(x_train, y_train, batch_size=16),\n                           steps_per_epoch=500,\n                           epochs=10, #Increase this when not on Kaggle kernel\n                           verbose=2,  #1 for ETA, 0 for silent\n                           validation_data=(x_val[:400,:], y_val[:400,:]), #For speed\n                           callbacks=[annealer])","2abff074":"final_loss, final_acc = model.evaluate(x_val, y_val, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))","463db213":"plt.plot(hist.history['loss'], color='b')\nplt.plot(hist.history['val_loss'], color='r')\nplt.show()\nplt.plot(hist.history['accuracy'], color='b')\nplt.plot(hist.history['val_accuracy'], color='r')\nplt.show()","d1c1a0e9":"y_hat = model.predict(x_val)\ny_pred = np.argmax(y_hat, axis=1)\ny_true = np.argmax(y_val, axis=1)\ncm = confusion_matrix(y_true, y_pred)\nprint(cm)","8f3ba7ec":"mnist_testset = np.loadtxt(test_file, skiprows=1, dtype='int', delimiter=',')\nx_test = mnist_testset.astype(\"float32\")\nx_test = x_test.reshape(-1, 28, 28, 1)\/255.","9c4f9257":"y_hat = model.predict(x_test, batch_size=64)","3504ca3b":"y_pred = np.argmax(y_hat,axis=1)","1fcaa77b":"with open(output_file, 'w') as f :\n    f.write('ImageId,Label\\n')\n    for i in range(len(y_pred)) :\n        f.write(\"\".join([str(i+1),',',str(y_pred[i]),'\\n']))","c71221eb":"## CNN Activation\nIn this book, I design and test all the experimental CNN models. To design my models, I use the python programming language. To perform the experimental review I use three different datasets, those are- Sign Language MNIST, Digit-Recognizer MNIST, and Skin Cancer MNIST. \nI use 10 epochs for the experimental fold. For the experimental purpose, I use one fold for each model. In this experiment, I used a total of twenty-seven similar types of CNN models. \nIn this book, I analyze the performances of the CNN models for the different activation functions. After analyzing the results, I recommend the activation functions based on the performances for the convolutional layer in neural networks. \nThe goal of this experiment is to help the developer to get a clear concept of the activation functions for the convolutional layers of the networks. "}}