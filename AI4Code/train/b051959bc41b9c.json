{"cell_type":{"3ed2f8f3":"code","8c22b97b":"code","f41982bd":"code","ee147975":"code","ffd0666d":"code","2913e89d":"code","6cffe69e":"code","56990224":"code","b56abf72":"code","9c683488":"code","0e1fd210":"code","673e48e7":"code","442cf9c3":"code","176b2cc2":"code","c8317b65":"code","d98157da":"code","af7bfc50":"code","9239c382":"code","18552a41":"code","94a32997":"code","243b38fd":"code","3f80c7b1":"code","d70678df":"code","22364f6e":"code","2fd5ccca":"code","fef99e14":"code","ef0c40d8":"code","11f1e6dd":"code","a95f4ec0":"code","1384ea1a":"code","bbc4d1af":"code","68660b45":"code","567524dc":"code","0de3d79f":"code","0be10d8a":"code","5b84907d":"code","a84eddb3":"code","633c567b":"code","e2dc8ab6":"code","567f4bff":"code","ddeb0f07":"code","86f3f33b":"code","2e5b5452":"code","eca07710":"code","900c20f3":"markdown","e09ff979":"markdown","bfd58a78":"markdown","2d196f84":"markdown","1e5ea82c":"markdown","845938c2":"markdown","d573c310":"markdown","1e0d9441":"markdown","2ad0bc0e":"markdown"},"source":{"3ed2f8f3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy.stats as stats\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nimport pyarrow.parquet as pq\nimport math\nimport os\nimport gc\nfrom tqdm import tqdm_notebook as tqdm, tnrange\n\nprint(os.listdir(\"..\/input\"))\n","8c22b97b":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","f41982bd":"train = pq.read_pandas('..\/input\/train.parquet').to_pandas()","ee147975":"mdtrain = pd.read_csv('..\/input\/metadata_train.csv')","ffd0666d":"train.info()","2913e89d":"mdtrain.head()\n","6cffe69e":"def feature_extractor(x, n_part=1000):\n    length = len(x)\n    n_feat = 7\n    pool = np.int32(np.ceil(length\/n_part))\n    output = np.zeros((n_part, n_feat))\n    for j, i in enumerate(range(0,length, pool)):\n        if i+pool < length:\n            k = x[i:i+pool]\n        else:\n            k = x[i:]\n        output[j, 0] = np.mean(k, axis=0) #mean\n        output[j, 1] = np.min(k, axis=0) #min\n        output[j, 2] = np.max(k, axis=0) #max\n        output[j, 3] = np.std(k, axis=0) #std\n        output[j, 4] = np.median(k, axis=0) #median\n        output[j, 5] = stats.skew(k, axis=0) #skew\n        output[j, 6] = stats.kurtosis(k, axis=0) # kurtosis\n    return output","56990224":"X = []\ny = []\nfor i in tqdm(mdtrain.signal_id):\n    idx = mdtrain.loc[mdtrain.signal_id==i, 'signal_id'].values.tolist()\n    y.append(mdtrain.loc[mdtrain.signal_id==i, 'target'].values)\n    X.append(feature_extractor(train.iloc[:, idx].values, n_part=400))","b56abf72":"X = np.array(X).reshape(-1, X[0].shape[0], X[0].shape[1])\nX = np.transpose(X, [0,2,1]) # Make X shape (batch size, channels, time steps) \n# because channels\/time-steps are different in Keras\n\ny = np.array(y).reshape(-1,1)","9c683488":"X.shape, y.shape","0e1fd210":"del train; gc.collect()\n","673e48e7":"sss = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=0)\n(train_idx, val_idx) = next(sss.split(X, y))\n\nX_train, X_val = X[train_idx], X[val_idx]\ny_train, y_val = y[train_idx], y[val_idx]","442cf9c3":"X_train.shape, X_val.shape, y_train.shape, y_val.shape","176b2cc2":"scalers = {} # This code actually looked wrong for Keras but right for Pytorch already!\nfor i in range(X_train.shape[1]):\n    scalers[i] = MinMaxScaler(feature_range=(-1, 1))\n    X_train[:, i, :] = scalers[i].fit_transform(X_train[:, i, :]) \n\nfor i in range(X_val.shape[1]):\n    X_val[:, i, :] = scalers[i].transform(X_val[:, i, :]) \n","c8317b65":"def confusion(prediction, truth):\n    \"\"\" Returns the confusion matrix for the values in the `prediction` and `truth`\n    tensors, i.e. the amount of positions where the values of `prediction`\n    and `truth` are\n    - 1 and 1 (True Positive)\n    - 1 and 0 (False Positive)\n    - 0 and 0 (True Negative)\n    - 0 and 1 (False Negative)\n    \"\"\"\n\n    confusion_vector = torch.as_tensor(prediction, dtype=torch.float32) \/ torch.as_tensor(truth, dtype=torch.float32)\n    # Element-wise division of the 2 tensors returns a new tensor which holds a\n    # unique value for each case:\n    #   1     where prediction and truth are 1 (True Positive)\n    #   inf   where prediction is 1 and truth is 0 (False Positive)\n    #   nan   where prediction and truth are 0 (True Negative)\n    #   0     where prediction is 0 and truth is 1 (False Negative)\n\n    true_positives = torch.sum(confusion_vector == 1).item()\n    false_positives = torch.sum(confusion_vector == float('inf')).item()\n    true_negatives = torch.sum(torch.isnan(confusion_vector)).item()\n    false_negatives = torch.sum(confusion_vector == 0).item()\n\n    return true_positives, false_positives, true_negatives, false_negatives","d98157da":"def matthews(TP, FP, TN, FN):\n    nom = TP*TN - FP*FN\n    denom = math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n    return nom\/denom\n    ","af7bfc50":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset","9239c382":"class VSBNet1(nn.Module):\n    \n    def __init__(self):\n        super(VSBNet1, self).__init__()\n        \n        self.conv1 = nn.Conv1d(in_channels=7, out_channels=64, kernel_size=4)\n        self.conv2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=4)\n        \n        self.mp1 = nn.MaxPool1d(2, padding=1)\n        \n        self.conv3 = nn.Conv1d(in_channels=64, out_channels=20, kernel_size=4)\n        self.conv4 = nn.Conv1d(in_channels=20, out_channels=20, kernel_size=4)\n        \n        #self.gap1 = nn.AdaptiveMaxPool1d(20)\n        self.gap1 = nn.AvgPool1d(192)\n        \n        self.do1 = nn.Dropout(0.2)\n        \n        # Flatten\n        \n        self.lin1 = nn.Linear(20,32)\n        self.do2 = nn.Dropout(0.5)\n        \n        self.lin2 = nn.Linear(32,8)\n        self.do3 = nn.Dropout(0.5)\n\n        self.lin3 = nn.Linear(8,1)\n        \n        def init_weights(m):\n            # Conv1d defaults to kaiming just like Keras does\n            if type(m) == nn.Linear:\n                # Keras defaults Dense to glorot_uniform (which is also called xavier uniform)\n                # Whereas Pytorch default for Linear is kaiming\n                torch.nn.init.xavier_uniform_(m.weight)\n                \n        self.apply(init_weights)\n        \n        \n    def forward(self,x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        \n        x = self.mp1(x)\n        \n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n\n        x = self.gap1(x)\n        \n        x = self.do1(x)\n        \n        x = x.view(x.shape[0],-1)\n        \n        x = torch.tanh(self.lin1(x))\n        x = self.do2(x)\n        \n        x = torch.tanh(self.lin2(x))\n        x = self.do3(x)\n        \n        x = self.lin3(x) # Leave sigmoid for the loss function\n            \n        return x\n    ","18552a41":"net = VSBNet1()","94a32997":"print(net)","243b38fd":"NUM_EPOCHS = 30\nBS = 16","3f80c7b1":"X_train.shape, y_train.shape","d70678df":"trainloader = DataLoader(list(zip(X_train,y_train)), batch_size=BS, shuffle=False, num_workers=2)","22364f6e":"criterion = torch.nn.BCEWithLogitsLoss() #pos_weight=torch.Tensor([1.0,1.2])) - pos_weight seems to work differently on latest pytorch\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-3)","2fd5ccca":"net.train()\n    \nfor t in tnrange(NUM_EPOCHS, desc='Epochs'):\n    \n    #running_loss = 0.0\n    for i, (X_batch, y_target_batch) in tqdm(enumerate(trainloader), total=len(trainloader)):\n              \n        # Forward pass: Compute predicted y by passing x to the model\n        y_pred = net(X_batch.float())\n        # y_pred = net(X.view(X.shape[0], 1, X.shape[1]))\n\n        # Compute and print loss\n        loss = criterion(y_pred, y_target_batch.float())\n\n\n        # Zero gradients, perform a backward pass, and update the weights.\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n","fef99e14":"net.eval()\ny_preds = net(torch.Tensor(X_train).float()).detach()\nloss = criterion(y_preds, torch.Tensor(y_train).float()).detach(); loss","ef0c40d8":"# Number of target==0, no of target==1\n(y_preds <= 0).sum().item(), (y_preds > 0).sum().item()","11f1e6dd":"net.eval()\ny_val_preds = net(torch.Tensor(X_val).float()).detach()\nloss_val = criterion(y_val_preds, torch.Tensor(y_val).float()).detach(); loss_val","a95f4ec0":"(y_val_preds <= 0).sum().item(), (y_val_preds > 0).sum().item()","1384ea1a":"c_train = confusion(y_preds > 0, y_train); c_train","bbc4d1af":"matthews(*c_train)","68660b45":"c_val = confusion(y_val_preds > 0, y_val); c_val","567524dc":"matthews(*c_val)","0de3d79f":"mdtest = pd.read_csv('..\/input\/metadata_test.csv')","0be10d8a":"mdtest.head()","5b84907d":"start_test = mdtest.signal_id.min()\nend_test = mdtest.signal_id.max()","a84eddb3":"X_test = []\n\npool_test = 2000\n\nfor start_col in tqdm(range(start_test, end_test + 1, pool_test)):\n    end_col = min(start_col + pool_test, end_test + 1)\n    test = pq.read_pandas('..\/input\/test.parquet',\n                          columns=[str(c) for c in range(start_col, end_col)]).to_pandas()\n\n    for i in tqdm(test.columns, desc=str(start_test)):\n        X_test.append(feature_extractor(test[i].values, n_part=400))\n        #test.drop([i], axis=1, inplace=True); gc.collect()\n    del test; gc.collect()","633c567b":"X_test = np.array(X_test).reshape(-1, X_test[0].shape[0], X_test[0].shape[1])\nX_test = np.transpose(X_test, [0,2,1]) # Make X shape (batch size, channels, time steps) \n# because channels\/time-steps are different in Keras","e2dc8ab6":"for i in range(X_test.shape[1]):\n    X_test[:, i, :] = scalers[i].transform(X_test[:, i, :]) ","567f4bff":"testloader = DataLoader(X_test, batch_size=100, shuffle=False, num_workers=2)","ddeb0f07":"net.eval()\n\ny_test_preds = None\nfor i, X_test_batch in tqdm(enumerate(testloader), total=len(testloader)):\n    y_test_preds_batch = net(torch.as_tensor(X_test_batch, dtype=torch.float32)).detach()\n    if y_test_preds is None:\n        y_test_preds = y_test_preds_batch\n    else:\n        y_test_preds = torch.cat([y_test_preds, y_test_preds_batch])\n        ","86f3f33b":"y_test_classes = y_test_preds > 0","2e5b5452":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['signal_id'] = mdtest.signal_id.values\nsubmission['target'] = y_test_classes.data.numpy().astype(int)\nsubmission.to_csv('submission.csv', index=False)","eca07710":"(y_test_classes <= 0).sum().item(), (y_test_classes > 0).sum().item()","900c20f3":"## Extract features","e09ff979":"### Confusion and Matthews","bfd58a78":"## Now apply to test dataset","2d196f84":"## Neural Network in Pytorch","1e5ea82c":"## Evaluation Metrics","845938c2":"## Convert an existing kernel from Keras to Pytorch\nhttps:\/\/www.kaggle.com\/fernandoramacciotti\/cnn-with-class-weights ","d573c310":"## Train the model","1e0d9441":"## Evaluate","2ad0bc0e":"## Split into test\/val sets and normalize"}}