{"cell_type":{"616bfa78":"code","236bd299":"code","e27d8af2":"code","49402909":"code","2d93f7f7":"code","3d783dea":"code","45846201":"code","562bb20d":"code","c50dd252":"code","2328502d":"code","425050ac":"code","e591f269":"code","4b5f69e9":"code","2da27af0":"code","82bbdecf":"markdown","c2912f64":"markdown","c98bf997":"markdown","46d99013":"markdown","2437c095":"markdown","b43be410":"markdown","7149ca9d":"markdown","1ad00ac6":"markdown","7c639218":"markdown","48ae4b7a":"markdown","87787066":"markdown","daacb045":"markdown"},"source":{"616bfa78":"# Core\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom itertools import combinations\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler \n\n# Tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","236bd299":"train_data=pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv', index_col='id')\ntest_data=pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv', index_col='id')\n\ntest_index=test_data.index # save for submission\n\nprint(train_data.shape)\ntrain_data.head()","e27d8af2":"print('Number of null values in training set:',train_data.isnull().sum().sum())\nprint('')\nprint('Number of null values in test set:',test_data.isnull().sum().sum())","49402909":"# Labels\ny=train_data.target\n\n# Features\nX=train_data.drop('target', axis=1)","2d93f7f7":"sns.heatmap(X.corr())","3d783dea":"fig, axes = plt.subplots(len(X.columns)\/\/4, 4, figsize=(14, 52))\n\ni = 0\nfor triaxis in axes:\n    for axis in triaxis:\n        X.hist(column = X.columns[i], bins = 100, ax=axis)\n        plt.title(X.columns[i]+'\\n')\n        i = i+1","45846201":"# This doesn't help\n'''\n# Remove 'spiky' columns\ncolumns_to_drop=['f0','f2','f4','f9','f12','f16','f19','f20','f23','f24','f27','f28',\n                'f30','f31','f32','f33','f35','f36','f39','f42','f44','f46','f48','f49',\n                'f51','f52','f53','f56','f58','f59','f60','f61','f62','f63','f64','f68','f69',\n                'f72','f73','f75','f76','f78','f79','f81','f83','f84','f86','f88','f89',\n                'f90','f92','f93','f94','f95','f98']\n\n# Loop over bad columns\nfor col in columns_to_drop:\n    X.drop(col, axis=1, inplace=True)\n    test_data.drop(col, axis=1, inplace=True)\n\n# Shape and preview\nprint(X.shape)\nX.head()\n'''","562bb20d":"scaler = StandardScaler()\nX=scaler.fit_transform(X)\ntest_data = scaler.transform(test_data)","c50dd252":"X_train, X_valid, y_train, y_valid = train_test_split(X,y,train_size=0.9,\n                                                             test_size=0.1,random_state=0)","2328502d":"# Define model\nmodel = keras.Sequential([\n    \n    # hidden layer 1\n    layers.Dense(units=256, activation='relu', input_shape=[X.shape[1]]),\n    layers.Dropout(rate=0.3),\n    \n    # hidden layer 2\n    layers.Dense(units=256, activation='relu'),\n    layers.Dropout(rate=0.3),\n    \n    # hidden layer 3\n    layers.Dense(units=128, activation='relu'),\n    layers.Dropout(rate=0.2),\n    \n    # output layer\n    layers.Dense(units=1, activation='sigmoid')\n])\n\n# Define loss, optimizer and metric\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['binary_accuracy'])\n\n# Define early stopping callback\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=10,\n    min_delta=0.0001,\n    restore_best_weights=True,\n)","425050ac":"history = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=500,\n    epochs=150,\n    callbacks=[early_stopping],\n    verbose=True\n)","e591f269":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\")\n\nprint('Final accuracy on validation set:', \n      history_df.loc[len(history_df)-1,'val_binary_accuracy'])","4b5f69e9":"preds=model.predict(test_data)\npred_classes = np.round(np.squeeze(preds),0).astype(int)\n\n# Save predictions to file\noutput = pd.DataFrame({'id': test_index,\n                       'target': pred_classes})\n\n# Check format\noutput.head()","2da27af0":"output.to_csv('submission.csv', index=False)\n","82bbdecf":"**Remove low variance columns**","c2912f64":"# Data","c98bf997":"**Learning curves**","46d99013":"**Feature correlations**","2437c095":"**Scale data**","b43be410":"**Check for null values**","7149ca9d":"**Labels and features**","1ad00ac6":"# Model","7c639218":"# Make predictions","48ae4b7a":"# Libraries","87787066":"**Break off validation set**","daacb045":"**Train model**"}}