{"cell_type":{"f5b4eac4":"code","8fda1b53":"code","45a16148":"code","c163aa72":"code","6c0434a1":"code","9a4d95b9":"code","e8d9a38f":"code","c579ca74":"code","e27d1604":"code","02249497":"code","00c9a863":"code","a66f05ae":"code","5b5b6425":"code","f9e51b3d":"code","72164b15":"code","d7bd9e4d":"code","803b87d6":"code","a36312b9":"code","c3ca7ed9":"code","ae24e214":"code","4cc68a4b":"code","75a79a92":"markdown","b9fd6712":"markdown","8ac3417c":"markdown"},"source":{"f5b4eac4":"# Set environment variables\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom math import ceil\n\nVERSION = 1\nINPUT_PATH = f\"\/kaggle\/input\/m5-forecasting-accuracy\"\nBASE_PATH = f\"\/kaggle\/working\/m5-forecasting-accuracy-ver{VERSION}\"","8fda1b53":"# Turn off warnings\n\nwarnings.filterwarnings(\"ignore\")","45a16148":"# Change directory\n\nos.chdir(INPUT_PATH)\nprint(f\"Change to directory: {os.getcwd()}\")","c163aa72":"# Memory usage function and merge by concat function (not to lose data type)\n\ndef format_memory_usage(total_bytes):\n    unit_list = [\"\", \"Ki\", \"Mi\", \"Gi\"]\n    for unit in unit_list:\n        if total_bytes < 1024:\n            return f\"{total_bytes:.2f}{unit}B\"\n        total_bytes \/= 1024\n    return f\"{total_bytes:.2f}{unit}B\"\n\ndef merge_by_concat(df1, df2, columns):\n    df_temp = df1[columns]\n    df_temp = df_temp.merge(df2, on = columns, how = \"left\")\n    new_columns = [column for column in list(df_temp) if column not in columns]\n    df1 = pd.concat([df1, df_temp[new_columns]], axis = 1)\n    return df1","6c0434a1":"# Load and check dataset\n\ndf_calendar = pd.read_csv(\"calendar.csv\")\ndf_calendar.head(10)","9a4d95b9":"# Select the necessary information\n# For example, we can extract day, month or year information from \"date\" column\n\ncalendar_selected_columns = [\n    \"date\"\n    , \"d\"\n    , \"event_name_1\"\n    , \"event_type_1\"\n    , \"event_name_2\"\n    , \"event_type_2\"\n    , \"snap_CA\"\n    , \"snap_TX\"\n    , \"snap_WI\"\n]\ndf_calendar_features = df_calendar[calendar_selected_columns]","e8d9a38f":"# Memory usage control\n\nmemory_usage_string = format_memory_usage(df_calendar_features.memory_usage().sum())\nprint(f\"Original memory usage: {memory_usage_string}\")\n\n# Technics: converting strings to categorical variables\ncalendar_category_columns = [\n    \"event_name_1\"\n    , \"event_type_1\"\n    , \"event_name_2\"\n    , \"event_type_2\"\n    , \"snap_CA\"\n    , \"snap_TX\"\n    , \"snap_WI\"\n]\nfor column in calendar_category_columns:\n    df_calendar_features[column] = df_calendar_features[column].astype(\"category\")\n\nmemory_usage_string = format_memory_usage(df_calendar_features.memory_usage().sum())\nprint(f\"Reduced memory usage: {memory_usage_string}\")","c579ca74":"# Create features\n# Convert date to datetime variables and store the derivative information in int8\n\nmemory_usage_string = format_memory_usage(df_calendar_features.memory_usage().sum())\nprint(f\"Original memory usage: {memory_usage_string}\")\n\ndf_calendar_features[\"date\"] = pd.to_datetime(df_calendar_features[\"date\"])\ndf_calendar_features[\"day\"] = df_calendar_features[\"date\"].dt.day.astype(np.int8)\ndf_calendar_features[\"weekday\"] = df_calendar_features[\"date\"].dt.dayofweek.astype(np.int8)\ndf_calendar_features[\"week\"] = df_calendar_features[\"date\"].dt.week.astype(np.int8)\ndf_calendar_features[\"month\"] = df_calendar_features[\"date\"].dt.month.astype(np.int8)\ndf_calendar_features[\"year\"] = (df_calendar_features[\"date\"].dt.year - df_calendar_features[\"date\"].dt.year.min()).astype(np.int8)\ndf_calendar_features[\"week_of_month\"] = df_calendar_features[\"date\"].dt.day.apply(lambda x: ceil(x \/ 7)).astype(np.int8)\ndf_calendar_features[\"is_weekend\"] = (df_calendar_features[\"weekday\"] >= 5).astype(np.int8)\n\n# Technics: for column \"d\", we would like to store it with int16 format\ndf_calendar_features[\"d\"] = df_calendar_features[\"d\"].apply(lambda x: int(x[2:])).astype(np.int16)\n\nmemory_usage_string = format_memory_usage(df_calendar_features.memory_usage().sum())\nprint(f\"Memory usage after columns added: {memory_usage_string}\")","e27d1604":"# Check dataset\n\ndf_calendar_features.head(10)","02249497":"# Check data type\n\ndf_calendar_features.info()","00c9a863":"# Load and check dataset\n\ndf_sell_prices = pd.read_csv(\"sell_prices.csv\")\ndf_sell_prices.head(10)","a66f05ae":"# Create features\n# Selling prices are not as fluctuating as we expect,\n# so we only need several characteristics to capture their distribution\n\ndf_sell_prices_grouped = df_sell_prices.groupby([\"store_id\", \"item_id\"])\n\nmemory_usage_string = format_memory_usage(df_sell_prices.memory_usage().sum())\nprint(f\"Original memory usage: {memory_usage_string}\")\n\ndf_sell_prices[\"price_max\"] = df_sell_prices_grouped[\"sell_price\"].transform(\"max\").astype(np.float16)\ndf_sell_prices[\"price_min\"] = df_sell_prices_grouped[\"sell_price\"].transform(\"min\").astype(np.float16)\ndf_sell_prices[\"price_mean\"] = df_sell_prices_grouped[\"sell_price\"].transform(\"mean\").astype(np.float16)\ndf_sell_prices[\"price_std\"] = df_sell_prices_grouped[\"sell_price\"].transform(\"std\").astype(np.float16)\ndf_sell_prices[\"price_scaled\"] = (\n    (df_sell_prices[\"sell_price\"] - df_sell_prices[\"price_min\"])\n    \/ (df_sell_prices[\"price_max\"] - df_sell_prices[\"price_min\"])\n).astype(np.float16)\ndf_sell_prices[\"price_nunique\"] = df_sell_prices_grouped[\"sell_price\"].transform(\"nunique\").astype(np.int16)\ndf_sell_prices[\"item_nunique\"] = df_sell_prices.groupby([\"store_id\", \"sell_price\"])[\"item_id\"].transform(\"nunique\").astype(np.int16)\n\nmemory_usage_string = format_memory_usage(df_sell_prices.memory_usage().sum())\nprint(f\"Memory usage after columns added: {memory_usage_string}\")","5b5b6425":"# Check dataset\n\ndf_sell_prices.head(10)","f9e51b3d":"# Join df_sell_prices and raw df_calendar\n\ndf_price_features = merge_by_concat(df_sell_prices, df_calendar[[\"wm_yr_wk\", \"month\", \"year\", \"d\"]], [\"wm_yr_wk\"])\ndf_price_features.head(10)","72164b15":"# Create features\n# Evaluate how do prices change periodically\n\nmemory_usage_string = format_memory_usage(df_price_features.memory_usage().sum())\nprint(f\"Original memory usage: {memory_usage_string}\")\n\ndf_price_features[\"price_mean_change_week\"] = (\n    df_price_features[\"sell_price\"] \/ df_price_features.groupby([\"store_id\", \"item_id\", \"wm_yr_wk\"])[\"sell_price\"].transform(\"mean\")\n).astype(np.float16)\ndf_price_features[\"price_mean_change_month\"] = (\n    df_price_features[\"sell_price\"] \/ df_price_features.groupby([\"store_id\", \"item_id\", \"month\"])[\"sell_price\"].transform(\"mean\")\n).astype(np.float16)\ndf_price_features[\"price_mean_change_year\"] = (\n    df_price_features[\"sell_price\"] \/ df_price_features.groupby([\"store_id\", \"item_id\", \"year\"])[\"sell_price\"].transform(\"mean\")\n).astype(np.float16)\n\nmemory_usage_string = format_memory_usage(df_price_features.memory_usage().sum())\nprint(f\"Memory usage after columns added: {memory_usage_string}\")","d7bd9e4d":"# Check dataset\n\nprice_selected_columns = [\n    \"store_id\"\n    , \"item_id\"\n    , \"d\"\n    , \"sell_price\"\n    , \"price_max\"\n    , \"price_min\"\n    , \"price_mean\"\n    , \"price_std\"\n    , \"price_scaled\"\n    , \"price_nunique\"\n    , \"item_nunique\"\n    , \"price_mean_change_week\"\n    , \"price_mean_change_month\"\n    , \"price_mean_change_year\"\n]\ndf_price_features = df_price_features[price_selected_columns]\ndf_price_features.head(10)","803b87d6":"# Memory usage control\n\nmemory_usage_string = format_memory_usage(df_price_features.memory_usage().sum())\nprint(f\"Original memory usage: {memory_usage_string}\")\n\n# Technics: converting strings to categorical variables\nprice_category_columns = [\"store_id\", \"item_id\"]\nfor column in price_category_columns:\n    df_price_features[column] = df_price_features[column].astype(\"category\")\n\n# Technics: for column \"sell_price\", we would like to store it with float16 format\ndf_price_features[\"sell_price\"] = df_price_features[\"sell_price\"].astype(np.float16)\n\n# Technics: for column \"d\", we would like to store it with int16 format\ndf_price_features[\"d\"] = df_price_features[\"d\"].apply(lambda x: int(x[2:])).astype(np.int16)\n\nmemory_usage_string = format_memory_usage(df_price_features.memory_usage().sum())\nprint(f\"Reduced memory usage: {memory_usage_string}\")","a36312b9":"# Check dataset\n\ndf_price_features.head(10)","c3ca7ed9":"# Check data type\n\ndf_price_features.info()","ae24e214":"# Change to output path\n\ntry:\n    os.chdir(BASE_PATH)\n    print(f\"Change to directory: {os.getcwd()}\")\nexcept:\n    os.mkdir(BASE_PATH)\n    os.chdir(BASE_PATH)\n    print(f\"Create and change to directory: {os.getcwd()}\")","4cc68a4b":"# Save pickle file\n\ndf_calendar_features.to_pickle(\"calendar_features.pkl\")\ndf_price_features.to_pickle(\"price_features.pkl\")","75a79a92":"# Feature Engineering - Price with Calendar\n- Joining DataFrames in Pandas is memory consuming, so we do the join work after creating basic features.\n- We want to evaluate how do prices change over weeks, months or years,\n- so we need to join price and calendar datasets to generate these features.","b9fd6712":"# Feature Engineering - Calendar\n- For each sales record, we want to add further information from the raw calendar dataset.","8ac3417c":"# Feature Engineering - Price\n- For each sales record, we want to add further information from the raw price dataset."}}