{"cell_type":{"8cb7ce78":"code","5f5dd953":"code","0fc23343":"code","6a60a417":"code","cbeaf677":"code","2d7174e7":"code","321c3300":"code","ee48ff61":"code","5107a16b":"code","88c773c9":"code","cd3e1904":"code","18f3dbd4":"code","512cab5a":"code","bad7e605":"code","88033f71":"code","11af14f2":"code","ac4eac18":"code","7a76e17f":"code","4050a154":"code","ce85ded6":"code","4b1c5677":"code","00752d37":"code","70a52a5c":"code","f7443fb9":"code","4fe14d38":"code","a20c025e":"code","0f93958f":"code","ac915093":"code","146ed5b6":"code","0cdda557":"code","f8213587":"code","c43c84c2":"code","6b793fe8":"code","94480248":"code","e4b1c667":"code","2fb83749":"code","babf47ff":"code","19d3ca21":"code","76ca8de4":"code","249639a2":"code","b0cd562d":"markdown","65cdcb16":"markdown","946b71fd":"markdown","f0613084":"markdown","90c9e4d7":"markdown","ed2b3e96":"markdown","c4b172e2":"markdown","0b161651":"markdown","d7e534c0":"markdown","85f26c1c":"markdown","3ccbbbdd":"markdown","c650fc33":"markdown","39c35493":"markdown","dd6f79b8":"markdown","3891ca8e":"markdown","f7423712":"markdown","926635b6":"markdown","7f6bec09":"markdown","aedaeb12":"markdown","ebfd6705":"markdown","01c6de2b":"markdown","706fbd49":"markdown","3e644c61":"markdown","e4e9faf3":"markdown","b61862bf":"markdown","3a617ded":"markdown","8da7bffd":"markdown","06d8c0ca":"markdown","c9047474":"markdown","b6867056":"markdown","fc56558a":"markdown","12cd46f0":"markdown","ca8d1959":"markdown","014850c5":"markdown","b4e1ea1a":"markdown"},"source":{"8cb7ce78":"! pip install tensorflow==2.2.0 -q","5f5dd953":"import os\nimport PIL\nimport math\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\n\nSEED = 1337\nprint('Tensorflow version : {}'.format(tf.__version__))\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    \nprint('Number of replicas:', strategy.num_replicas_in_sync)","0fc23343":"MAIN_DIR = '..\/input\/pandatilesagg'\nTRAIN_IMG_DIR = os.path.join(MAIN_DIR, 'all_images')\ntrain_csv = pd.read_csv(os.path.join(MAIN_DIR, 'train.csv'))","6a60a417":"radboud_csv = train_csv[train_csv['data_provider'] == 'radboud']\nkarolinska_csv = train_csv[train_csv['data_provider'] != 'radboud']\nimg_ids = train_csv['image_id']","cbeaf677":"r_train, r_test = train_test_split(\n    radboud_csv,\n    test_size=0.2, random_state=SEED\n)\n\nk_train, k_test = train_test_split(\n    karolinska_csv,\n    test_size=0.2, random_state=SEED\n)","2d7174e7":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nIMG_DIM = (1536, 128)\nCLASSES_NUM = 6\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nEPOCHS = 100\nN=12","321c3300":"def decode_img(img):\n  # convert the compressed string to a 3D uint8 tensor\n  img = tf.image.decode_jpeg(img, channels=3)\n  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n  img = tf.image.convert_image_dtype(img, tf.float32)\n  # resize the image to the desired size.\n  return tf.image.resize(img, IMG_DIM)","ee48ff61":"def get_item(file_path):    \n    image = tf.io.read_file(file_path)\n    image = decode_img(image)\n    label = tf.strings.split(file_path, '_')\n    label = tf.strings.to_number(label[-2])\n    label = tf.cast(label, tf.int32)\n    \n    return image, tf.one_hot(label, CLASSES_NUM)","5107a16b":"r_train['isup_grade'] = r_train['isup_grade'].apply(str)\nr_train['file'] = TRAIN_IMG_DIR +'\/_' + r_train['isup_grade'] + '_' + r_train['image_id'] + '.jpg'\n\nr_test['isup_grade'] = r_test['isup_grade'].apply(str)\nr_test['file'] = TRAIN_IMG_DIR +'\/_' + r_test['isup_grade'] + '_' + r_test['image_id'] + '.jpg'\n\nk_train['isup_grade'] = k_train['isup_grade'].apply(str)\nk_train['file'] = TRAIN_IMG_DIR +'\/_' + k_train['isup_grade'] + '_' + k_train['image_id'] + '.jpg'\n\nk_test['isup_grade'] = k_test['isup_grade'].apply(str)\nk_test['file'] = TRAIN_IMG_DIR +'\/_' + k_test['isup_grade'] + '_' + k_test['image_id'] + '.jpg'","88c773c9":"def get_dataset(df):\n    ds = tf.data.Dataset.from_tensor_slices(df['file'].values)\n    ds = ds.map(get_item, num_parallel_calls=AUTOTUNE)\n    ds = ds.shuffle(buffer_size=1000)\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds","cd3e1904":"r_train_ds = get_dataset(r_train)\nk_train_ds = get_dataset(k_train)","18f3dbd4":"def show_batch(image_batch, label_batch):\n    plt.figure(figsize=(10,10))\n    for n in range(10):\n        ax = plt.subplot(1,10,n+1)\n        plt.imshow(image_batch[n])\n        plt.axis(\"off\")","512cab5a":"image_batch, label_batch = next(iter(r_train_ds))\nshow_batch(image_batch, label_batch)","bad7e605":"image_batch, label_batch = next(iter(k_train_ds))\nshow_batch(image_batch, label_batch)","88033f71":"def make_model():\n    base_model = tf.keras.applications.VGG16(input_shape=(*IMG_DIM, 3),\n                                             include_top=False,\n                                             weights='imagenet')\n    \n    base_model.trainable = True\n    \n    model = tf.keras.Sequential([\n        base_model,\n        \n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(16, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(CLASSES_NUM, activation='softmax'),\n    ])\n    \n    model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n                  loss='categorical_crossentropy',\n                  metrics=[tf.keras.metrics.AUC(name='auc')])\n    \n    return model","11af14f2":"def exponential_decay(lr0, s):\n    def exponential_decay_fn(epoch):\n        return lr0 * 0.1 **(epoch \/ s)\n    return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(0.01, 20)\n\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n                                                     restore_best_weights=True)","ac4eac18":"with strategy.scope():\n    model = make_model()\n\nhistory = model.fit(\n    r_train_ds, epochs=20,\n    callbacks=[early_stopping_cb, lr_scheduler]\n)","7a76e17f":"with strategy.scope():\n    model = make_model()\n    \nhistory = model.fit(\n    k_train_ds, epochs=20,\n    callbacks=[early_stopping_cb, lr_scheduler]\n)","4050a154":"image_batch, _ = next(iter(r_train_ds))\nr_image = image_batch[4].numpy()\nplt.figure(figsize=(10,10))\nax = plt.subplot(1,2,1)\nplt.title(\"radboud\")\nplt.imshow(r_image)\nplt.axis(\"off\")\n\nimage_batch, _ = next(iter(k_train_ds))\nk_image = image_batch[4].numpy()\nax = plt.subplot(1,2,2)\nplt.imshow(k_image)\nplt.title(\"karolinska\")\nplt.axis(\"off\")","ce85ded6":"content_image = tf.expand_dims(r_image, axis = 0)\nstyle_image = tf.expand_dims(k_image, axis = 0)","4b1c5677":"content_layers = ['block5_conv2'] \n\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1']\n\nnum_content_layers = len(content_layers)\nnum_style_layers = len(style_layers)","00752d37":"def vgg_layers(layer_names):\n  \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n  # Load our model. Load pretrained VGG, trained on imagenet data\n  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n  vgg.trainable = False\n  \n  outputs = [vgg.get_layer(name).output for name in layer_names]\n\n  model = tf.keras.Model([vgg.input], outputs)\n  return model","70a52a5c":"style_extractor = vgg_layers(style_layers)\nstyle_outputs = style_extractor(style_image*255)","f7443fb9":"def gram_matrix(input_tensor):\n  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n  input_shape = tf.shape(input_tensor)\n  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n  return result\/(num_locations)","4fe14d38":"class StyleContentModel(tf.keras.models.Model):\n  def __init__(self, style_layers, content_layers):\n    super(StyleContentModel, self).__init__()\n    self.vgg =  vgg_layers(style_layers + content_layers)\n    self.style_layers = style_layers\n    self.content_layers = content_layers\n    self.num_style_layers = len(style_layers)\n    self.vgg.trainable = False\n\n  def call(self, inputs):\n    \"Expects float input in [0,1]\"\n    inputs = inputs*255.0\n    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n    outputs = self.vgg(preprocessed_input)\n    style_outputs, content_outputs = (outputs[:self.num_style_layers], \n                                      outputs[self.num_style_layers:])\n\n    style_outputs = [gram_matrix(style_output)\n                     for style_output in style_outputs]\n\n    content_dict = {content_name:value \n                    for content_name, value \n                    in zip(self.content_layers, content_outputs)}\n\n    style_dict = {style_name:value\n                  for style_name, value\n                  in zip(self.style_layers, style_outputs)}\n    \n    return {'content':content_dict, 'style':style_dict}","a20c025e":"extractor = StyleContentModel(style_layers, content_layers)\n\nresults = extractor(tf.constant(content_image))","0f93958f":"style_targets = extractor(style_image)['style']\ncontent_targets = extractor(content_image)['content']","ac915093":"def clip_0_1(image):\n  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n\nopt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)","146ed5b6":"style_weight=1e-1\ncontent_weight=1e4","0cdda557":"def style_content_loss(outputs):\n    style_outputs = outputs['style']\n    content_outputs = outputs['content']\n    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n                           for name in style_outputs.keys()])\n    style_loss *= style_weight \/ num_style_layers\n\n    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n                             for name in content_outputs.keys()])\n    content_loss *= content_weight \/ num_content_layers\n    loss = style_loss + content_loss\n    return loss","f8213587":"def train_step(image):\n  with tf.GradientTape() as tape:\n    outputs = extractor(image)\n    loss = style_content_loss(outputs)\n\n  grad = tape.gradient(loss, image)\n  opt.apply_gradients([(grad, image)])\n  image.assign(clip_0_1(image))","c43c84c2":"steps_per_epoch = 3\n\ndef style_transfer(image):\n    image = tf.expand_dims(image, axis = 0)\n    image = tf.Variable(lambda : image)\n    step = 0\n    for m in range(steps_per_epoch):\n        step += 1\n        train_step(image)\n        print(\".\", end='')\n        \n    return image","6b793fe8":"plt.figure(figsize=(10,10))\nax = plt.subplot(1,2,1)\nplt.title(\"radboud\")\nplt.imshow(r_image)\nplt.axis(\"off\")\n\nr_aug_image = style_transfer(r_image)\nax = plt.subplot(1,2,2)\nplt.imshow(r_aug_image[0])\nplt.title(\"radboud augmented\")\nplt.axis(\"off\")","94480248":"# ! mkdir images","e4b1c667":"# for file in r_train['file'].values:\n#     img = np.array(PIL.Image.open(file)) \/255.0\n#     img = style_transfer(img)[0] * 255.0\n#     img = img.numpy()\n#     im = img.astype('uint8')\n#     im = PIL.Image.fromarray(im)\n#     im.save(\"images\/\" + file.split('\/')[-1])","2fb83749":"# import shutil\n# shutil.make_archive(\"images\", 'zip', \"\/kaggle\/working\/images\")","babf47ff":"def get_dataset(ds):\n    ds = ds.map(get_item, num_parallel_calls=AUTOTUNE)\n    ds = ds.shuffle(buffer_size=1000)\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds","19d3ca21":"rad_aug = tf.data.Dataset.list_files(\"..\/input\/pandatilesagg\/radboud_aug\/*\")\nrad_aug = get_dataset(rad_aug)","76ca8de4":"image_batch, label_batch = next(iter(rad_aug))\nshow_batch(image_batch, label_batch)","249639a2":"with strategy.scope():\n    model = make_model()\n    \nhistory = model.fit(\n    rad_aug, epochs=20,\n    callbacks=[early_stopping_cb, lr_scheduler]\n)","b0cd562d":"# Load augmented images and train the model\n\nLet's load in our augmented radboud images.","65cdcb16":"Visualize the augmented radboud images.","946b71fd":"The first step is to create a function that will decode our image into a tensor. We can use the `tf.image` API.","f0613084":"The StyleContentModel will return the gram matrix of the style layers and content layers.","90c9e4d7":"# Visualize the neural style transfer\n\nBefore we augment all of the images in our dataset, let's fist visualize how our augmented radboud image compared to our original one.","ed2b3e96":"We can extract our style features, defined by the style layers we chose earlier, and convert them to style outputs. Since our inputs are values between 0 and 255, we have to multiple the style features by 255.","c4b172e2":"**Radboud Scans**","0b161651":"# Build the neural style transfer model\n\nOur next step is to define a model that returns a list of intermediate layer outputs for our VGG19 model.","d7e534c0":"# Data loading\n\nThe first step is to load in our data. We will be working with the PANDA data that was used for the Prostate Cancer Grade Assessment Challenge. The tiling for this dataset is explained in my first [PANDA notebook](https:\/\/www.kaggle.com\/amyjang\/tensorflow-cnn-data-augmentation-prostate-cancer). The tiles have been aggregated into singular images for your convenience in a new [dataset](https:\/\/www.kaggle.com\/amyjang\/pandatilesagg).\n\nThe PANDA data contains prostate cancer microscopy scans from two different institutions - Radboud University Medical Center and Karolinska Institute. We'll run our ML classifying model on the two sets of images separately to see which institution has better performing data.","85f26c1c":"We see from these two visualizations that the Radboud and Karolinska images look very different. Even from a quick initial glance, we se that the hues and colors are different. Will the variations affect how well the images do in our model?","3ccbbbdd":"The ROC AUC score for the augmented images score a lot higher than the original radboud score.\n\nFrom 0.54 --> 0.67!! (The exact values may be different because we did not specify a random seed)\n\nThe other model does not shown signs of improvement after each epoch, but using augmented images shows visible improvements. This notebook highlights neural style transfer's potential to augment images in the medical space!!\n\nThis idea has many important implications. First, it provides a solution to the variation in processing images. It may be difficult to universalize the processing of a microscopy scan, MRI scan, or any other type of medical image, but it is a lot easier to export a software that many institutions and labs can utilize. Additionally, different scans may introduce different biases. Neural style transfer may help eliminate certain biases. And most importantly, neural style transfer can lead to higher scoring of medical images. This means better diagnoses and less error when it comes to using ML for medical images.","c650fc33":"The ROC AUC scores are way higher for the karolinska images. This can be a problem because the discrepancy shows that the images from the different institutions do in fact have noticable variances by the machine. It can also cause the model to be biased towards karolinska images, and this can hurt patients with radboud images.\n\nThe idea is that we will use neural style transfer to change radboud images to look more like karolinska images.","39c35493":"This second function maps the filepath to the the image and label. The files are labeled so that the label - the ISUP grade - is the first part of the file name and the image id is the second part.","dd6f79b8":"Define the weights. Higher the weight, the more important the loss will be calculated.","3891ca8e":"We see that there a style transfer did in fact occur. Running the neural style transfer may change the original image too much, and changing the higher-level features may hurt classifiction. Therefore, we want to keep the number of epochs relatively low.","f7423712":"# Run neural style transfer on Radboud images\n\nNow that we tested to see that neural style transfer works on a single radboud image, we want to run neural style on all the radboud images. Because of resource limitations, the code is commented out below. However, the output for the data augmentation is under the radboud_aug directory in the dataset.\n\nAs a note, not all the radboud images have been augmented for efficiency purposes.","926635b6":"# Introduction + Set-up\n\nTensorFlow is a powerful tool to develop any machine learning pipeline. This model explores the concept of using neural style transfer to augment medical images.\n\nMedical images are rarely ever processed in the same way. Different institutions, different machines, different technicians, and many other factors lead to variation within medical scans. These variations can impact how well an ML model can learn from given data. If images processed by institution #1 work better with the model than images processed by institution #2, we would want our institution #2 images to look more like institution #1 images.\n\nNeural style transfer is outlined in [\"A Neural Algorithm of Artistic Style\"](https:\/\/arxiv.org\/abs\/1508.06576) (Gatys et al.). On a very high level, neural style transfer is used to compose on image in the style of another using deep learning. This notebook shows that neural style transfer does indeed improve scores for images processed by lower-scoring methods.","7f6bec09":"# Calculate style\n\nStyle is a rather arbitrary concept, and we want to convert it into a value that can be understood by the model. Style can be calculated by a gram matrix below.","aedaeb12":"The ROC AUC score for the radboud images are quite low, near 0.5. This means that the the model finds it difficult to distinguish between the images of the different class.","ebfd6705":"We'll define a clipping method to keep the values between 0 and 1.","01c6de2b":"# Compare scores between institutions\n\nLet's run our model on the Radboud images first.","706fbd49":"**Karolinska Scans**","3e644c61":"# Define style and content layers\n\nIn a deep convolution neural net, the lower layers capture lower-level features like texture and higher layers capture higher-level features such as shapes. We define the content layers and style layers below.","e4e9faf3":"# Visualize our input data\n\nRun the following cell to define the method to visualize our input data. This method displays the new images and their corresponding ISUP grade.","b61862bf":"Define the training step.","3a617ded":"The differences in the processing techniques is apparent.\n\nWe will be using our karolinska image as our style image because we want to change our radboud images to look more like our karolinska images. Our model takes in 4D tensors, so let's first reformat our style and content images. Before we restyle all of our radboud images, let's first visualize how the neural style transfer works on a single image.","8da7bffd":"Generally, it is better practice to specify constant variables than it is to hard-code numbers. This way, changing parameters is more efficient and complete. Specfiy some constants below.","06d8c0ca":"# Build our classifying model\n\nWe will be utilizing the VGG16 pre-trained model to classify our data. Since the base model has already been trained with imagenet weights, we do not want the weights to change, so the base mode must not be trainable. However, the number of classes that our model has differs from the original model. Therefore, we do not want to include the top layers because we will add our own Dense layer that has the same number of nodes as our output class.","c9047474":"Let's run the same model on the Karolinska images.","b6867056":"# Neural style transfer intro\n\nLet's first visualize a radboud image and a karolinska image next to each other.\n\nA more in-depth tutorial of neural style transfer for artistic painting transformations can be found [here](https:\/\/www.tensorflow.org\/tutorials\/generative\/style_transfer).","fc56558a":"The way that loss is calculated is by calculating mean square error of the model outputs to the targets, defined below. The weight of the losses (style vs. content) are defined below.","12cd46f0":"Learning rate is a very important hyperparameter, and it can be difficult to choose the \"right\" one. A learning rate that it too high will prevent the model from converging, but one that is too low will be far too slow. We will utilize multiple callbacks, using the `tf.keras` API to make sure that we are using an ideal learning rate and to prevent the model from overfitting. We can also save our model so that we do not have to retrain it next time.","ca8d1959":"# Train the neural style model\n\nBecause we aren't using a typical Keras model, we cannot run model.fit. Instead, we'll define our own training loop.","014850c5":"We're going to define two different datasets: one with radboud images and one with the karolinska images.","b4e1ea1a":"Build a model that extracts style and content."}}