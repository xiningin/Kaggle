{"cell_type":{"e5a2c6bb":"code","eb2338d2":"code","1ca1f7aa":"code","bbfa6c11":"code","00936524":"code","820b4821":"code","bc7d9d05":"code","a7070f89":"code","d9c00166":"code","ea3e323e":"code","62fc49e8":"code","c146a9d5":"code","b09b6274":"code","fb6509ac":"code","c18e65a8":"code","ecd11331":"code","fe49c34b":"code","e5bb3c1b":"code","68e5d0ea":"code","5417b948":"code","18ce3ebe":"code","1133597f":"code","72f2298b":"code","30ab79c5":"code","c4ccc027":"code","16da4029":"code","6e53717d":"code","c39baf2a":"code","837a2f86":"code","865fe7ab":"code","011474c2":"code","d6385b55":"code","840dd2d6":"code","604b9911":"code","718aaabd":"code","75da2c6e":"code","12da2447":"code","4728e521":"code","71250911":"code","b65a2f8c":"code","913acaa2":"code","52ef6ca0":"code","90007ac0":"code","27526b49":"code","69c4a9bf":"code","a72b8c63":"code","d1e76334":"code","5eb08ee8":"code","7aadc00d":"code","e7c21d4f":"code","1dcaba43":"code","0d3f7609":"code","8e7a94ec":"code","a5d40221":"code","748a6562":"code","77a9b17f":"code","95cb1e5b":"code","dc09d8ed":"code","985f018b":"code","7f39fe6f":"code","9c2ca2ff":"code","61bd3d1b":"code","0fe1da2f":"code","557d0b8c":"code","04661363":"code","58b181c4":"code","3b3b18a5":"code","a65cab1a":"code","ffe5dc76":"code","3332a0c4":"code","81347539":"code","458dee4b":"code","b8c6558c":"code","ee8be242":"code","e46daf0c":"code","d4758913":"code","75585d3d":"code","8b2dce50":"code","9eef8fda":"code","e36e0624":"code","781adf53":"code","88e9416d":"code","e1320ebe":"code","eadedf3e":"code","2d97a138":"code","dbda87e3":"code","0bbef68e":"code","87047bef":"code","bd87dc5d":"code","e2f3dbfd":"code","3afa667d":"code","ecc0448b":"code","0d61632a":"code","58acf09d":"code","cbc4a447":"code","6c7dbead":"code","e2e1ffc7":"code","43c33bef":"code","0d724cdc":"code","188f708f":"code","9083288c":"code","dffcfa49":"code","43b17c97":"code","6e34ddb4":"code","cd84aad8":"code","63babf8d":"code","c163e081":"code","83a2ef45":"code","c024142f":"code","096e24ee":"code","b688ef68":"code","60b0e404":"code","2c3dc467":"code","7e55e820":"code","89d9891a":"code","d066b48f":"code","d128ec0f":"code","1bf8c70c":"code","c3f101a7":"code","04522f8e":"code","aeb958fb":"code","4cc19163":"code","a9d2e6ee":"code","f4015a7f":"code","413af8fc":"markdown","e68e8192":"markdown","0169df7f":"markdown","2323d2e3":"markdown","386eb619":"markdown","c762cf2e":"markdown","40a1a452":"markdown","8ddebeac":"markdown","dc313045":"markdown","93b2f67b":"markdown","480901b2":"markdown","a1c8be91":"markdown","5b4c088f":"markdown","e7b07d21":"markdown","e87c5ab4":"markdown","78a0e710":"markdown","192451c5":"markdown","dd20bc10":"markdown","97cc96b5":"markdown","0398fc4c":"markdown","d3ccdb7c":"markdown","5505062f":"markdown","4e6c71bb":"markdown","814ca701":"markdown","181c16aa":"markdown","c55d8978":"markdown","caa41dd8":"markdown","6da17e71":"markdown","6c691353":"markdown","f519f3e4":"markdown","c00d3af6":"markdown","9481a439":"markdown","2cff05e3":"markdown","c4696bf4":"markdown","74426f8e":"markdown","f282f9fe":"markdown","4227eeb6":"markdown","88f0914e":"markdown","be207e6c":"markdown","ce768d6b":"markdown","8851a877":"markdown","7a704b66":"markdown","100d85a6":"markdown","bbc3c6bc":"markdown","79b35f26":"markdown","4e90953d":"markdown","fc6e2d0b":"markdown","561eb1d2":"markdown","7037be11":"markdown","2ac0a234":"markdown","da2849a0":"markdown","2966a28b":"markdown","1467a299":"markdown","df83bc96":"markdown","9ef00629":"markdown","95bbf931":"markdown","83ed389f":"markdown","10627afa":"markdown","2abe893a":"markdown","fd3e3ca6":"markdown","3595afd2":"markdown"},"source":{"e5a2c6bb":"import time\nfrom datetime import datetime\n\n#measure notebook running time\nstart_time = time.time()\n\n%matplotlib inline\n\nimport os, warnings\nimport numpy as np \nfrom numpy.random import seed\nimport pandas as pd \nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input, Dropout, BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import metrics\nimport tensorflow as tf\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom category_encoders import MEstimateEncoder\n\nsns.set(style='white', context='notebook', palette='deep', rc={'figure.figsize':(10,8)})\nprint(\"loaded ...\")","eb2338d2":"# Reproducibility\ndef set_seed(sd=13):\n    seed(sd)\n    np.random.seed(sd)\n    tf.random.set_seed(sd)\n    os.environ['PYTHONHASHSEED'] = str(sd)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()","1ca1f7aa":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_data['Survived'] = -1\ntrain_data['Set'] = \"Train\"\ntest_data['Set'] = \"Test\"\nDATA = train_data.append(test_data)\nDATA.reset_index(inplace=True)\nDATA.dtypes","bbfa6c11":"print(\"Missing data:\\n\", DATA.isna().sum())","00936524":"index_NaN_age = list(DATA[\"Age\"][DATA[\"Age\"].isnull()].index)\nmed_age = DATA.Age.median()\nfor i in index_NaN_age:\n    pred_age = DATA[\"Age\"][((DATA.SibSp == DATA.iloc[i]['SibSp']) & (DATA.Parch == DATA.iloc[i]['Parch']))].median()\n    if np.isnan(pred_age):\n        DATA.loc[DATA.index[i],'Age'] = med_age\n    else:\n        DATA.loc[DATA.index[i],'Age'] = pred_age","820b4821":"g = sns.FacetGrid(train_data,col=\"Survived\")\ng = g.map(sns.histplot, \"Age\", kde=True)\ng.fig.subplots_adjust(top=0.8)\ng.fig.suptitle(\"Age distribution over Survived\");","bc7d9d05":"g = sns.catplot(x=\"Sex\",y=\"Survived\",data=DATA[DATA.Set == 'Train'], kind=\"bar\", height = 6, palette = \"muted\")\ng = g.set_ylabels(\"survival probability - Sex\")","a7070f89":"DATA['SexN'] = DATA.Sex.apply(lambda row: 1 if row == \"female\" else 0)","d9c00166":"def extract(sex, age):\n    if sex == 'male' and age > 15: return 1\n    return 0\n    \nDATA['AdultMan'] = DATA[['Sex','Age']].apply(lambda row: extract(*row), axis=1)","ea3e323e":"g = sns.catplot(x=\"AdultMan\",y=\"Survived\",data=DATA[DATA.Set == 'Train'], kind=\"bar\", height = 6, palette = \"muted\")\ng = g.set_ylabels(\"survival probability - Sex\")","62fc49e8":"def cut_age(age):\n    if age <= 15:\n        return 'child'\n    if age >= 60:\n        return 'senior'\n    return 'adult'\n\nDATA['Age_Cat'] = DATA.Age.apply(cut_age)","c146a9d5":"g = sns.catplot(x=\"Age_Cat\",y=\"Survived\",data=DATA[DATA.Set == 'Train'], kind=\"bar\", height = 6, palette = \"muted\")\ng = g.set_ylabels(\"survival probability - Age_Cat\")","b09b6274":"def get_title(sex,name):\n    split_name = name.split(\",\")\n    surname = split_name[0].strip(\" \")\n    title = split_name[1].split('.')[0].strip(' ')\n    common_titles = ['Mr', 'Miss', 'Mrs', 'Master']\n    if title not in common_titles:\n        title = sex\n    return title,surname\n    \n    \nDATA[['Title', 'Surname']] = DATA[['Sex','Name']].apply(lambda row: get_title(*row), axis=1, result_type= 'expand')\nDATA.head(10)\n\n#DATA[DATA.Surname == 'Peacock'] #poor people, they all die\n#DATA[DATA.Surname == 'Elias'] #family of men, doomed\n","fb6509ac":"g = sns.catplot(x=\"Title\",y=\"Survived\", data=DATA[DATA.Set == 'Train'], kind=\"bar\", height = 6, palette = \"muted\")\ng = g.set_ylabels(\"survival probability - Title\")","c18e65a8":"DATA['FamilySize'] = DATA.SibSp + DATA.Parch + 1\ng = sns.catplot(x=\"FamilySize\",y=\"Survived\",data=DATA[DATA.Set == 'Train'], kind=\"bar\", height = 6, palette = \"muted\")\ng = g.set_ylabels(\"survival probability - FamilySize\")","ecd11331":"def familySize_to_cat(size):\n    if size == 1: return \"alone\"\n    if size >= 2 and size <= 4: return 'small'\n    if size >= 5 and size <= 7: return 'medium'\n    if size > 7 : return \"large\"\n\nDATA['FamilySizeCategory'] = DATA.FamilySize.apply(familySize_to_cat)","fe49c34b":"g = sns.catplot(x=\"FamilySizeCategory\",y=\"Survived\",data=DATA[DATA.Set == 'Train'], kind=\"bar\", height = 6, palette = \"muted\")\ng = g.set_ylabels(\"survival probability - FamilySizeCategory\")","e5bb3c1b":"g = sns.catplot(x=\"SibSp\",y=\"Survived\",data=DATA[DATA.Set == 'Train'], kind=\"bar\", height = 6, palette = \"muted\")\ng = g.set_ylabels(\"survival probability - SibSp\")","68e5d0ea":"g = sns.catplot(x=\"Parch\",y=\"Survived\",data=DATA[DATA.Set == 'Train'], kind=\"bar\", height = 6, palette = \"muted\")\ng = g.set_ylabels(\"survival probability - Parch\")","5417b948":"DATA.Embarked.fillna(\"S\",inplace=True)","18ce3ebe":"ax = sns.countplot(data = DATA[DATA.Set == 'Train'], x = 'Embarked', hue = \"Survived\");\nax.set_title(\"Embarked\");","1133597f":"for i in DATA.index:\n    \n    # adult male\n    if DATA.iloc[i]['Sex'] == 'male' and DATA.iloc[i]['Age_Cat'] == 'adult':\n        DATA.loc[DATA.index[i],'SG'] = \"adult_male\"\n        # based on actual ratio\n        DATA.loc[DATA.index[i],'FSR'] = 0.15\n        #DATA.loc[DATA.index[i],'FSR'] = 0\n        continue\n        \n    family = DATA[(DATA.Surname == DATA.iloc[i][\"Surname\"]) & (DATA.Ticket == DATA.iloc[i][\"Ticket\"])]\n    kids = family[family.Age_Cat == 'child']\n    N_kids = len(kids)\n    \n    # no_children\n    if N_kids == 0:\n        DATA.loc[DATA.index[i],'SG'] = \"no_children\"\n        #DATA.loc[DATA.index[i],'FSR'] = 1\n        # based on actual ratio\n        DATA.loc[DATA.index[i],'FSR'] = 0.75\n        continue\n        \n    if N_kids == 1 and len(family) == 1:\n        DATA.loc[DATA.index[i],'SG'] = \"solo_kid\"\n         # based on actual ratio\n        DATA.loc[DATA.index[i],'FSR'] = 0.62\n        continue\n    \n    wc_group = family[family.Title != 'Mr']\n    survived = wc_group[wc_group.Survived != -1]['Survived'].to_list()\n    \n    if len(survived) == 0:\n        # in memoriam 'peacock family', is this cheating?\n        DATA.loc[DATA.index[i],'SG'] = \"group_died\"\n        DATA.loc[DATA.index[i],'FSR'] = 0\n        continue\n    \n    if np.mean(survived) >= 0.5:\n        DATA.loc[DATA.index[i],'SG'] = \"group_survived\"\n    else:\n        DATA.loc[DATA.index[i],'SG'] = \"group_died\"\n        \n    DATA.loc[DATA.index[i],'FSR'] = np.mean(survived)","72f2298b":"ax = sns.countplot(data = DATA[DATA.Set == 'Train'], x = 'SG', hue = \"Survived\");\nax.set_title(\"SG\");","30ab79c5":"DATA[(DATA.Set == 'Train') & (DATA.SG == 'no_children')]['Survived'].value_counts()\/len(DATA[(DATA.Set == 'Train') & (DATA.SG == 'no_children')]['Survived'])","c4ccc027":"DATA[(DATA.Set == 'Train') & (DATA.SG == 'solo_kid')]['Survived'].value_counts()\/len(DATA[(DATA.Set == 'Train') & (DATA.SG == 'solo_kid')]['Survived'])","16da4029":"DATA[(DATA.Set == 'Train') & (DATA.SG == 'adult_male')]['Survived'].value_counts()\/len(DATA[(DATA.Set == 'Train') & (DATA.SG == 'adult_male')]['Survived'])","6e53717d":"g = sns.catplot(x=\"Pclass\",y=\"Survived\",data=DATA[DATA.Set == 'Train'], kind=\"bar\", height = 6, palette = \"muted\")\ng = g.set_ylabels(\"survival probability\")","c39baf2a":"DATA['RealFare'] = DATA.Fare \/ DATA.FamilySize\n\nindex_zero = list(DATA['RealFare'][(DATA['RealFare'] == 0) | (DATA['Fare'].isna())].index)\nfor i in index_zero:\n    med_fare = DATA['RealFare'][(DATA.Pclass == DATA.iloc[i]['Pclass'])].median()\n    DATA.loc[DATA.index[i],'RealFare'] = med_fare","837a2f86":"sns.histplot(data = DATA[DATA.Set == 'Train'], x='RealFare', stat='percent', hue='Pclass', kde=True, log_scale=True);","865fe7ab":"DATA['RealFare'].describe()\nDATA['FareBins'] = pd.cut(DATA['RealFare'], [0, 5,10, 25, 50, 100, np.inf], labels = ['0-5', '5-10', '10-25', '25-50', '50-100', '>100'], include_lowest = True)\nDATA['FareBins'].value_counts()","011474c2":"sns.countplot(data = DATA[DATA.Set == 'Train'], x='FareBins',  hue='Survived');","d6385b55":"sns.histplot(data = DATA[DATA.Set == 'Train'], x='RealFare', stat='percent', hue='Survived', kde=True, log_scale=True);","840dd2d6":"DATA['RealFare'] = DATA['RealFare'].apply(lambda row: np.log(row) if row > 0 else 0)","604b9911":"def extractData(sex,fare,sg):\n    if sex != \"female\": return 0\n    if sg != 'no_children': return 0\n    if fare > 10: return 0\n    return 1\n\nDATA['FemaleClass3Nochildren'] = DATA[['Sex','RealFare','SG']].apply(lambda row: extractData(*row), axis=1)\n#DATA['FemaleClass3Nochildren'].value_counts()","718aaabd":"g = sns.catplot(x=\"FemaleClass3Nochildren\",y=\"Survived\",data=DATA[DATA.Set == 'Train'], kind=\"bar\", height = 6, palette = \"muted\")\ng = g.set_ylabels(\"survival probability - Female, no children pclass 3\")","75da2c6e":"def get_deck(cabin):\n    if cabin is np.nan or cabin.startswith('T'):\n        return 'X'\n    else:\n        return cabin[0]\n    \nDATA.Cabin = DATA.Cabin.apply(get_deck)","12da2447":"g = sns.countplot(x=\"Cabin\",data=DATA[DATA.Set == 'Train'],palette = \"muted\");","4728e521":"g = sns.catplot(x=\"Cabin\",y=\"Survived\",data=DATA[DATA.Set == 'Train'], kind=\"bar\", height = 6, palette = \"muted\")\ng = g.set_ylabels(\"survival probability - Cabin\")","71250911":"fig, ax = plt.subplots(figsize=(10,10)) \nax = sns.heatmap(DATA[DATA.Set == 'Train'][['Survived',\"SibSp\",\"Parch\", \"FSR\",\"Age\",\"RealFare\",\"FamilySize\",\"SexN\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\");\nax.set_title(\"Survival correlation to numeric features\");","b65a2f8c":"#mm = MinMaxScaler()\nmm = StandardScaler()\nscale_features = [\"SibSp\",\"Parch\", \"Age\",\"RealFare\",\"FamilySize\"]\nDATA[scale_features] = mm.fit_transform(DATA[scale_features])","913acaa2":"pca_features = ['Age',\"RealFare\",\"FamilySize\", \"FSR\", \"SexN\", 'AdultMan']\npca = PCA(3)\nX_PCA = pca.fit_transform(DATA.loc[:, pca_features])\ncomponent_names = [f\"PC{i+1}\" for i in range(X_PCA.shape[1])]\nX_PCA = pd.DataFrame(X_PCA, columns=component_names)\nX_PCA.head()","52ef6ca0":"DATA_PCA = X_PCA.copy()\nDATA_PCA['Survived'] = DATA['Survived']\nDATA_PCA=DATA_PCA[DATA_PCA['Survived'] != -1]\npca.explained_variance_ratio_","90007ac0":"%%time\ndef scatterplot(x,y,**kwargs):\n    sns.regplot(x=x,y=y)\n    _=plt.xticks(rotation=90)\n\nf = pd.melt(DATA_PCA, id_vars=['Survived'], value_vars=component_names)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=4, sharex=False, sharey=True, height=5)\ng = g.map(scatterplot, \"value\", \"Survived\")","27526b49":"fig, axs = plt.subplots(1, 2)\nn = pca.n_components_\ngrid = np.arange(1, n + 1)\n# Explained variance\nevr = pca.explained_variance_ratio_\naxs[0].bar(grid, evr)\naxs[0].set(xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0))\n# Cumulative Variance\ncv = np.cumsum(evr)\naxs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\naxs[1].set(xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0))\n# Set up figure\nfig.set(figwidth=8, dpi=100);","69c4a9bf":"loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=DATA.loc[:, pca_features].columns,  # and the rows are the original features\n    )\nloadings","a72b8c63":"%%time\nclustering_features = ['Age',\"RealFare\",\"FamilySize\", \"FSR\", \"SexN\", 'AdultMan']\n\nkmeans = KMeans(n_clusters = 2, random_state=13)\nclust_data = DATA[DATA.Set == 'Train'].loc[:, clustering_features]\nclust_data['cluster'] = kmeans.fit_predict(clust_data)\nclust_data['cluster'] = clust_data['cluster'].astype('category')\nclust_data['Survived'] = DATA[DATA.Set == 'Train']['Survived']","d1e76334":"%%time\nsns.relplot(data = clust_data.melt(value_vars=clustering_features, id_vars = [\"Survived\", \"cluster\"]), x=\"value\", y=\"Survived\", hue='cluster', col= \"variable\", col_wrap=8, height=3);","5eb08ee8":"clust_data = DATA.loc[:, clustering_features]\nX_CD = kmeans.fit_transform(clust_data)\nX_CD = mm.fit_transform(X_CD)\nX_CD = pd.DataFrame(X_CD, columns=[f\"Centroid_{i}\" for i in range(X_CD.shape[1])])","7aadc00d":"encode_features = ['Sex','AdultMan','FemaleClass3Nochildren',\"Cabin\",'FareBins','Pclass','FamilySizeCategory','Age_Cat','Embarked',\"SG\"]\nX_encode = DATA[DATA.Set == 'Train'].sample(frac=0.25, random_state=13)\ny_encode = X_encode.pop(\"Survived\")\nX_encode[encode_features].head()","e7c21d4f":"encoder = MEstimateEncoder(cols=encode_features,m=1.5)\nenc_cols = [\"TE_\"+ f for f in encode_features]\nencoder.fit(X_encode, y_encode)\nENC = encoder.transform(DATA.drop(\"Survived\", axis=1))\nX_ENC = ENC[encode_features]\nX_ENC.columns = enc_cols;\nX_ENC.head()","1dcaba43":"_Embarked = DATA[DATA.Set == 'Train'].Embarked\n_PClass = DATA[DATA.Set == 'Train'].Pclass\n_Cabin = DATA[DATA.Set == 'Train'].Cabin\n_Title = DATA[DATA.Set == 'Train'].Title\n_Family = DATA[DATA.Set == 'Train'].FamilySizeCategory\n_Sex = DATA[DATA.Set == 'Train'].Sex\n_Age = DATA[DATA.Set == 'Train'].Age\n_Fare = DATA[DATA.Set == 'Train'].FareBins\n_SG = DATA[DATA.Set == 'Train'].SG\n_FSR = DATA[DATA.Set == 'Train'].FSR\n\n#train\nT_Embarked = DATA[DATA.Set == 'Test'].Embarked\nT_PClass = DATA[DATA.Set == 'Test'].Pclass\nT_Cabin = DATA[DATA.Set == 'Test'].Cabin\nT_Title = DATA[DATA.Set == 'Test'].Title\nT_Family = DATA[DATA.Set == 'Test'].FamilySizeCategory\nT_Sex = DATA[DATA.Set == 'Test'].Sex\nT_Age = DATA[DATA.Set == 'Test'].Age\nT_Fare = DATA[DATA.Set == 'Test'].FareBins\nT_SG = DATA[DATA.Set == 'Test'].SG\nT_FSR = DATA[DATA.Set == 'Test'].FSR","0d3f7609":"drop_this_run = [\"SibSp\",\"Parch\",\"FamilySize\"] #does not contribute","8e7a94ec":"redundant_features = ['Ticket', \"Name\",'Surname','Fare', \"Sex\"]\n\nDATA.drop([*redundant_features,*drop_this_run], inplace = True, axis = 1)\n#DATA.dtypes","a5d40221":"categorical = ['Cabin','Pclass','FamilySizeCategory','Title','Embarked', \"SG\", \"Age_Cat\", 'FareBins']\ncategorical = [cat for cat in categorical if cat not in drop_this_run]\nDATA = pd.get_dummies(DATA,columns=categorical)","748a6562":"DATA = DATA.join([X_CD,X_PCA, X_ENC])","77a9b17f":"fig, ax = plt.subplots(figsize=(20,20))     \ng = sns.heatmap(DATA[DATA.Set == 'Train'].corr(),annot=False, fmt = \".2f\", cmap = \"coolwarm\")","95cb1e5b":"TRAIN = DATA[DATA.Set == 'Train']\nTEST = DATA[DATA.Set == 'Test']\nPassengerIds = TEST.PassengerId.to_list()\nTEST = TEST.drop(['PassengerId','Set',\"Survived\",'index'], axis = 1)\ny = TRAIN.Survived\nX = TRAIN.drop(['Survived','PassengerId','Set','index'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 13, stratify=y)","dc09d8ed":"X.head()","985f018b":"TEST.head()","7f39fe6f":"def plot_CM(model, clf):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6,6))\n    fig.suptitle(clf)\n    cm_train=confusion_matrix(y_train, model.predict(X_train), normalize = 'pred', labels = model.classes_)\n    disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=model.classes_)\n    disp_train.plot(ax=ax1)\n    disp_train.ax_.set_title('Train')\n    cm_test=confusion_matrix(y_test, model.predict(X_test), normalize = 'pred', labels = model.classes_)\n    disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=model.classes_)\n    disp_test.plot(ax=ax2)\n    disp_test.ax_.set_title('Test')\n    plt.show()","9c2ca2ff":"def plot_loss(loss,val_loss):\n    plt.figure()\n    plt.plot(loss)\n    plt.plot(val_loss)\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper right')\n    plt.show()\n\ndef plot_accuracy(acc,val_acc):\n    plt.figure()\n    plt.plot(acc)\n    plt.plot(val_acc)\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show() ","61bd3d1b":"dnn_model = Sequential()\nn_cols = X.shape[1]\ndnn_model.add(Input(shape = (n_cols,), name = 'input'))\ndnn_model.add(Dense(1024, activation=\"relu\"))\ndnn_model.add(Dropout(0.25))\ndnn_model.add(Dense(512, activation=\"relu\"))\ndnn_model.add(Dropout(0.25))\ndnn_model.add(Dense(256, activation=\"relu\"))\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(128, activation=\"relu\"))\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(64, activation=\"relu\"))\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(32, activation=\"relu\"))\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(16, activation=\"relu\"))\n#dnn_model.add(BatchNormalization())\ndnn_model.add(Dropout(0.1))\ndnn_model.add(Dense(2, activation=\"relu\"))\ndnn_model.add(BatchNormalization())\ndnn_model.add(Dense(1, activation=\"sigmoid\", name='out'))            \ndnn_model.summary()","0fe1da2f":"tf.keras.utils.plot_model(dnn_model, show_shapes=True)","557d0b8c":"%%time\n#dnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\ndnn_model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, epsilon=1e-03), metrics=['binary_accuracy'])\nearly_stopping_monitor = EarlyStopping(patience=25, monitor='val_binary_accuracy')\ncheckpoint = ModelCheckpoint(\"weights.hdf5\", monitor = 'val_binary_accuracy', save_best_only = True)\n#dnn_model.fit(X_train,y_train, validation_data=(X_test,y_test), callbacks=[checkpoint, early_stopping_monitor], epochs=300, batch_size=64, verbose=0, validation_split=0.25)\ndnn_model.fit(X_train,y_train, validation_data=(X_test,y_test), callbacks=[checkpoint, early_stopping_monitor], epochs=300, batch_size=96, verbose=0, validation_split=0.25)\ndnn_model.load_weights(\"weights.hdf5\")\n\nplot_loss(dnn_model.history.history['loss'], dnn_model.history.history['val_loss'])\nplot_accuracy(dnn_model.history.history['binary_accuracy'], dnn_model.history.history['val_binary_accuracy'])\n\n_, train_dnn_accuracy = dnn_model.evaluate(X_train, y_train)\n_, dnn_accuracy = dnn_model.evaluate(X_test, y_test)\nprint('Train accuracy: {:.2f} %'.format(train_dnn_accuracy*100))\nprint('Accuracy: {:.2f} %'.format(dnn_accuracy*100))\nprint('Overfit: {:.2f} % '.format((train_dnn_accuracy - dnn_accuracy)*100))","04661363":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6,6))\nfig.suptitle('DNN CM')\ncm_train=confusion_matrix(y_train, np.rint(dnn_model.predict(X_train)), normalize = 'pred', labels =[0,1])\ndisp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=[0,1])\ndisp_train.plot(ax=ax1)\ndisp_train.ax_.set_title('Train')\ncm_test=confusion_matrix(y_test, np.rint(dnn_model.predict(X_test)), normalize = 'pred', labels = [0,1])\ndisp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=[0,1])\ndisp_test.plot(ax=ax2)\ndisp_test.ax_.set_title('Test')\nplt.show()","58b181c4":"# %%time\n# clf = RandomForestClassifier(random_state = 13, n_jobs=-1)\n# param_grid = {'n_estimators': [25, 50,75,150,300, 500],'max_depth': [*range(3,13), None], 'max_features': [*np.arange(0.5,1.0,0.1),'auto','sqrt',\"log2\"],\n#              'bootstrap': [True]}\n# rf_grid_clf = GridSearchCV(clf, param_grid, cv=4, scoring= \"accuracy\")\n# rf_grid_clf.fit(X, y)\n# print(rf_grid_clf.best_estimator_)\n# print(rf_grid_clf.best_params_)\n\n# rf_accuracy = rf_grid_clf.best_score_\n# print(rf_accuracy)","3b3b18a5":"rf_model = RandomForestClassifier(max_depth=11, n_estimators=50, n_jobs=-1, random_state=13)\nrf_model.fit(X_train, y_train)\nrf_train_score = rf_model.score(X_train, y_train)\nrf_accuracy = rf_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(rf_train_score * 100))\nprint(\"Test: {:.2f} %\".format(rf_accuracy*100))\nprint('Overfit: {:.2f} %'.format((rf_train_score-rf_accuracy)*100))","a65cab1a":"plot_CM(rf_model, \"Random Forest\")","ffe5dc76":"features = {}\nfor feature, importance in zip(X_train.columns, rf_model.feature_importances_):\n    features[feature] = importance\n\nimportances = pd.DataFrame({\"RF\":features})\nimportances.sort_values(\"RF\", ascending = False, inplace=True)\nRF_best_features = list(importances[importances.RF > 0.03].index)\nimportances.plot.bar()\nprint(\"RF_best_features:\",RF_best_features, len(RF_best_features))","3332a0c4":"# %%time\n# xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state = 13)\n# xgb_param_grid = {'max_depth': [*range(4, 10), None],\n#                   'learning_rate': [0.001, 0.01, 0.1, 0.05],\n#                   'subsample': np.arange(0.6,1.0,0.1),\n#                   'colsample_bytree': np.arange(0.2,1.0,0.1),\n#                   'reg_alpha':[0.001, 0.01, 0.1],\n#                   'reg_lambda': [0.01, 1.0, 10.0, 100.0, 1000, 10000],\n#                   'n_estimators': [50, 100, 250, 500, 1000]\n#                  }\n\n# #xgb_grid = GridSearchCV(estimator=xgb_clf, param_grid = xgb_param_grid, cv=4, scoring= \"accuracy\")\n# xgb_grid = RandomizedSearchCV(estimator=xgb_clf, param_distributions = xgb_param_grid, cv=4, scoring= \"accuracy\", random_state = 13)\n# xgb_grid.fit(X,y)\n\n# print(xgb_grid.best_params_)\n# xgb_accuracy = xgb_grid.best_score_\n# print(xgb_accuracy)","81347539":"xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', learning_rate=0.1, colsample_bytree = 0.4, subsample= 0.9, reg_lambda= 1000, reg_alpha = 0.01, n_estimators= 100,\n                             max_depth = None)\n\nxgb_model.fit(X_train, y_train)\nxgb_train_score = xgb_model.score(X_train, y_train)\nxgb_accuracy = xgb_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(xgb_train_score*100))\nprint(\"Test: {:.2f} %\".format(xgb_accuracy*100))\nprint('Overfit: {:.2f} %'.format((xgb_train_score-xgb_accuracy)*100))","458dee4b":"plot_CM(xgb_model, \"XGB\")","b8c6558c":"weights = xgb_model.get_booster().get_score(importance_type=\"gain\")\nweights = [(weights[w],w) for w in sorted(weights, key=weights.get, reverse=True)]\nXGB_features = [w[1] for w in weights]\nprint(\"XGB best features\", XGB_features)","ee8be242":"# %%time\n# param_grid = {'C': [1, 100, 1000, 10000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001],'kernel': ['rbf']}\n# SVM_grid = GridSearchCV(estimator = SVC(),param_grid=param_grid, cv=5, scoring= \"accuracy\")\n# SVM_grid.fit(X,y)\n# print(SVM_grid.best_params_)\n# SVM_accuracy = SVM_grid.best_score_\n# print(SVM_accuracy)","e46daf0c":"SVM_model = SVC(C = 100, gamma= 0.001, kernel='rbf', probability=True, random_state = 13)\nSVM_model.fit(X_train, y_train)\nsvm_train_score = SVM_model.score(X_train, y_train)\nSVM_accuracy = SVM_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(svm_train_score*100))\nprint(\"Test: {:.2f} %\".format(SVM_accuracy*100))\nprint('Overfit: {:.2f} %'.format((svm_train_score - SVM_accuracy)*100))","d4758913":"plot_CM(SVM_model, \"SVM\")","75585d3d":"# %%time\n# param_grid = {'C': np.logspace(-4, 4, 10), 'penalty': ['l2','l1'], 'solver': ['liblinear']}\n# #LR_grid = GridSearchCV(estimator = LogisticRegression(solver='liblinear'), param_grid=param_grid, cv=4, scoring= \"accuracy\", random_state = 13)\n# LR_grid = GridSearchCV(estimator = LogisticRegression(random_state = 13), param_grid=param_grid, cv=4, scoring= \"accuracy\", )\n# LR_grid.fit(X,y)\n# print(LR_grid.best_params_)\n# LR_accuracy = LR_grid.best_score_\n# print(LR_accuracy)","8b2dce50":"LR_model = LogisticRegression(solver='liblinear', C=0.005994842503189409, penalty='l2', random_state = 13)\n\nLR_model.fit(X_train, y_train)\nLR_train_score = LR_model.score(X_train, y_train)\nLR_accuracy = LR_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(LR_train_score*100))\nprint(\"Test: {:.2f} %\".format(LR_accuracy*100))\nprint('Overfit: {:.2f} %'.format((LR_train_score-LR_accuracy)*100))","9eef8fda":"plot_CM(LR_model, \"Logistic Regression\")","e36e0624":"# grid_params = {'n_neighbors': range(2,20), 'weights': ['uniform', 'distance'], 'metric': ['euclidean', 'manhattan', 'minkowski']}\n# KNN_grid = GridSearchCV(estimator = KNeighborsClassifier(), param_grid = grid_params, cv=5, scoring= \"accuracy\")\n# KNN_grid.fit(X,y)\n# print(KNN_grid.best_params_)\n# KNN_accuracy = KNN_grid.best_score_\n# print(KNN_accuracy)","781adf53":"KNN_model = KNeighborsClassifier(n_neighbors=12,metric='manhattan',weights='uniform')\nKNN_model.fit(X_train, y_train)\nKNN_train_score = KNN_model.score(X_train, y_train)\nKNN_accuracy = KNN_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(KNN_train_score*100))\nprint(\"Test: {:.2f} %\".format(KNN_accuracy*100))\nprint('Overfit: {:.2f} %'.format((KNN_train_score-KNN_accuracy)*100))","88e9416d":"plot_CM(KNN_model, \"KNN\")","e1320ebe":"# %%time\n# abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n# parameters = {'base_estimator__max_depth':[3, 4, 10, None],\n#               'base_estimator__min_samples_split':range(3,10),\n#               'base_estimator__min_samples_leaf':range(3,10),\n#               'base_estimator__max_features':['auto','sqrt',0.75, 0.9],\n#               'n_estimators':[50, 100,250,500],\n#               'learning_rate':[0.001, 0.01, 0.1, 0.5]}\n\n# ADA_grid = RandomizedSearchCV(estimator=abc, param_distributions = parameters, cv=4, scoring= \"accuracy\", random_state = 13, verbose=0)\n# ADA_grid.fit(X,y)\n# print(ADA_grid.best_params_)\n# ADA_accuracy = ADA_grid.best_score_\n# print(ADA_accuracy)","eadedf3e":"ADA_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3, min_samples_leaf=6, max_features='auto', min_samples_split= 3), \n                               n_estimators=250, learning_rate=0.001, random_state=13) \nADA_model.fit(X_train,y_train)\nADA_train_score = ADA_model.score(X_train, y_train)\nADA_accuracy = ADA_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(ADA_train_score*100))\nprint(\"Test: {:.2f} %\".format(ADA_accuracy*100))\nprint('Overfit: {:.2f} %'.format((ADA_train_score - ADA_accuracy)*100))","2d97a138":"plot_CM(ADA_model, \"ADA Boost\")","dbda87e3":"features = {}\nfor feature, importance in zip(X_train.columns, ADA_model.feature_importances_):\n    features[feature] = importance\n\nimportances = pd.DataFrame({\"ADA\":features})\nimportances.sort_values(\"ADA\", ascending = False, inplace=True)\nimportances\nADA_best_features = list(importances[importances.ADA > 0.03].index)\nimportances.plot.bar()\nprint(\"ADA_best_features:\",ADA_best_features, len(ADA_best_features))","0bbef68e":"# %%time\n# ExtC = ExtraTreesClassifier(random_state = 13)\n# ex_param_grid = {\"max_depth\": [*range(3,10),None],\n#               'max_features':['auto',0.9, 0.8, 0.75,'sqrt', 'log2'],\n#               \"n_estimators\" :[50,100,300, 500],\n#                 'bootstrap': [False, True],\n#                 \"min_samples_split\": range(2,10),\n#                  \"min_samples_leaf\": range(1,10),\n#                  'criterion': ['gini', 'entropy']\n#                 }\n\n# gsExtC = RandomizedSearchCV(estimator=ExtC, param_distributions = ex_param_grid, cv=4, scoring= \"accuracy\", random_state = 13, verbose=0)\n# gsExtC.fit(X,y)\n# print(gsExtC.best_estimator_)\n# ExtC_accuracy = gsExtC.best_score_\n# print(ExtC_accuracy)","87047bef":"ETC_model = ExtraTreesClassifier(bootstrap=True, max_depth=8, max_features=0.9,\n                     min_samples_leaf=7, min_samples_split=8, n_estimators=300,\n                     random_state=13)\nETC_model.fit(X_train, y_train)\nETC_train_score = ETC_model.score(X_train, y_train)\nETC_accuracy = ETC_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(ETC_train_score*100))\nprint(\"Test: {:.2f} %\".format(ETC_accuracy*100))\nprint('Overfit: {:.2f} %'.format((ETC_train_score-ETC_accuracy)*100))","bd87dc5d":"plot_CM(ETC_model, \"Extra Trees\")","e2f3dbfd":"features = {}\nfor feature, importance in zip(X_train.columns, ETC_model.feature_importances_):\n    features[feature] = importance\n\nimportances = pd.DataFrame({\"ETC\":features})\nimportances.sort_values(\"ETC\", ascending = False, inplace=True)\nimportances\nETC_best_features = list(importances[importances.ETC > 0.03].index)\nimportances.plot.bar()\nprint(\"ETC_best_features:\",ETC_best_features, len(ETC_best_features))","3afa667d":"# %%time\n# GBC = GradientBoostingClassifier(random_state = 13)\n# gbc_param_grid = {\n#               'n_estimators' : [25, 50, 100,250, 500, 1000],\n#               'learning_rate': [0.5, 0.1, 0.05, 0.01, 0.001],\n#               'max_depth': [*range(3,10,1), None],\n#               'max_features': [*np.arange(0.3,1.0,0.1),'auto','sqrt',\"log2\"]\n#               }\n\n# gsGBC = RandomizedSearchCV(estimator=GBC, param_distributions = gbc_param_grid, cv=4, scoring= \"accuracy\", random_state = 13, verbose=0)\n# gsGBC.fit(X,y)\n# print(gsGBC.best_estimator_)\n# gbc_accuracy = gsGBC.best_score_\n# print(gbc_accuracy)","ecc0448b":"GBC_model = GradientBoostingClassifier(learning_rate=0.001, max_depth=8, max_features=0.3, n_estimators=500, random_state=13)\nGBC_model.fit(X_train, y_train)\nGBC_train_score = GBC_model.score(X_train, y_train)\nGBC_accuracy = GBC_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(GBC_train_score*100))\nprint(\"Test: {:.2f} %\".format(GBC_accuracy*100))\nprint('Overfit: {:.2f} %'.format((GBC_train_score - GBC_accuracy)*100))","0d61632a":"plot_CM(GBC_model, \"GradientBoosting\")","58acf09d":"features = {}\nfor feature, importance in zip(X_train.columns, GBC_model.feature_importances_):\n    features[feature] = importance\n\nimportances = pd.DataFrame({\"GBC\":features})\nimportances.sort_values(\"GBC\", ascending = False, inplace=True)\nGBC_best_features = list(importances[importances.GBC > 0.03].index)\nimportances.plot.bar()\nprint(\"GBC_best_features:\",GBC_best_features, len(GBC_best_features))","cbc4a447":"# %%time\n# params = {\n#     \"loss\" : [\"modified_huber\"],\n#     \"alpha\" : [0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n#     \"penalty\": ['l2', 'l1', 'elasticnet'],\n#     \"l1_ratio\": np.arange(0.0, 1.0, 0.05),\n#     \"epsilon\": [0.1, 0.01, 1]\n# }\n\n# sgd_grid = GridSearchCV(estimator =  SGDClassifier(max_iter=1000, random_state=13, n_jobs=-1), param_grid=params, cv=4, scoring=\"accuracy\")\n# sgd_grid.fit(X,y)\n\n# print(sgd_grid.best_estimator_)\n# sgd_accuracy = sgd_grid.best_score_\n# print(sgd_accuracy)","6c7dbead":"SGD_model = SGDClassifier(alpha=1, l1_ratio=0.0, loss='modified_huber', n_jobs=-1,random_state=13)\n\nSGD_model.fit(X_train, y_train)\nSGD_train_score = SGD_model.score(X_train, y_train)\nSGD_accuracy = SGD_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(SGD_train_score*100))\nprint(\"Test: {:.2f} %\".format(SGD_accuracy*100))\nprint('Overfit: {:.2f} %'.format((SGD_train_score-SGD_accuracy)*100))","e2e1ffc7":"plot_CM(SGD_model, \"StochasticGradientDescent\")","43c33bef":"# %%time\n# params = {\n#     \"criterion\": ['gini', 'entropy'],\n#     \"splitter\":['best', 'random'],\n#     \"max_depth\": [*range(3,12), None],\n#     'max_features': [*np.arange(0.3,1.0,0.1),'auto','sqrt',\"log2\"]\n# }\n\n# dt_grid = RandomizedSearchCV(estimator =  DecisionTreeClassifier(random_state=13), param_distributions=params, cv=4, scoring=\"accuracy\")\n# dt_grid.fit(X,y)\n\n# print(dt_grid.best_estimator_)\n# sgd_accuracy = dt_grid.best_score_\n# print(sgd_accuracy)","0d724cdc":"DT_model = DecisionTreeClassifier(criterion='entropy', max_depth=10, max_features=0.5,\n                       random_state=13)\n\nDT_model.fit(X_train, y_train)\nDT_train_score = DT_model.score(X_train, y_train)\nDT_accuracy = DT_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(DT_train_score*100))\nprint(\"Test: {:.2f} %\".format(DT_accuracy*100))\nprint('Overfit: {:.2f} %'.format((DT_train_score - DT_accuracy)*100))","188f708f":"plot_CM(DT_model, \"DecisionTreeClassifier\")","9083288c":"features = {}\nfor feature, importance in zip(X_train.columns, DT_model.feature_importances_):\n    features[feature] = importance\n\nimportances = pd.DataFrame({\"DT\":features})\nimportances.sort_values(\"DT\", ascending = False, inplace=True)\nDT_best_features = list(importances[importances.DT > 0.03].index)\nimportances.plot.bar()\nprint(\"DT_best_features:\",DT_best_features, len(DT_best_features))","dffcfa49":"# %%time\n# lgbm_clf = lgb.LGBMClassifier(random_state=13)\n# lgbm_param_grid = {\n#                 'max_depth': [4, 10, None],\n#                   'learning_rate': [0.001, 0.01, 0.1, 0.05],\n#                   'subsample': np.arange(0.7, 0.95, 0.05),\n#                   'colsample_bytree': np.arange(0.5, 1.0, 0.1),\n#                   'reg_alpha':[0.001, 0.01, 0.1],\n#                   'reg_lambda': [0.01, 1.0, 10.0, 100.0, 1000, 10000],\n#                   'n_estimators': [50, 100, 250, 500, 1000]\n#                  }\n\n# lgbm_grid = RandomizedSearchCV(estimator=lgbm_clf, param_distributions = lgbm_param_grid, cv=4, scoring= \"accuracy\", random_state = 13)\n# lgbm_grid.fit(X,y)\n\n# print(lgbm_grid.best_params_)\n# lgbm_accuracy = lgbm_grid.best_score_\n# print(lgbm_accuracy)","43b17c97":"%%time\nLGBM_model = lgb.LGBMClassifier(random_state=13, subsample=0.9, reg_lambda=100, reg_alpha=0.01, n_estimators=250, max_depth=None, learning_rate=0.05, colsample_bytree=0.6)\nLGBM_model.fit(X_train, y_train)\nLGBM_train_score = LGBM_model.score(X_train, y_train)\nLGBM_accuracy = LGBM_model.score(X_test, y_test)\nprint(\"Train: {:.2f} %\".format(LGBM_train_score*100))\nprint(\"Test: {:.2f} %\".format(LGBM_accuracy*100))\nprint('Overfit: {:.2f} %'.format((LGBM_train_score - LGBM_accuracy)*100))","6e34ddb4":"plot_CM(LGBM_model, \"LGBM Classifier\")","cd84aad8":"features = {}\nfor feature, importance in zip(X_train.columns, LGBM_model.feature_importances_):\n    features[feature] = importance\n\nimportances = pd.DataFrame({\"LGBM\":features})\nimportances.sort_values(\"LGBM\", ascending = False, inplace=True)\nLGBM_best_features = list(importances[importances.LGBM > 0.03].index)\nimportances.plot.bar()\nprint(\"LGBM_best_features:\",LGBM_best_features, len(LGBM_best_features))","63babf8d":"L = min(len(RF_best_features), len(XGB_features), len(ADA_best_features),  \n        len(ETC_best_features), len(GBC_best_features),  len(DT_best_features), len(LGBM_best_features))\n\nTF = pd.DataFrame({\"ADA\":ADA_best_features[:L], \n                   \"XGB\":XGB_features[:L], \"RF\":RF_best_features[:L],\n                  \"ETC\":ETC_best_features[:L], \"GBC\":GBC_best_features[:L], \n                  \"DT\": DT_best_features[:L], \"LGBM\": LGBM_best_features[:L]} )\nTF","c163e081":"print(\"Accuracy Scores:\")\nprint(\"==========================================================\")\nprint(\"DNN: {:.3f}\".format(dnn_accuracy))\nprint(\"RandomForest: {:.3f}\".format(rf_accuracy))\nprint(\"XGBoost classifier: {:.3f}\".format(xgb_accuracy))\nprint(\"SVM classifier: {:.3f}\".format(SVM_accuracy))\nprint(\"LR classifier: {:.3f}\".format(LR_accuracy))\nprint(\"KNN classifier: {:.3f}\".format(KNN_accuracy))\nprint(\"ADA Boost classifier: {:.3f}\".format(ADA_accuracy))\nprint(\"Extra Tree classifier: {:.3f}\".format(ETC_accuracy))\nprint(\"Gradient Boosting classifier: {:.3f}\".format(GBC_accuracy))\nprint(\"Stochastic Gradient descent: {:.3f}\".format(SGD_accuracy))\nprint(\"Decision Tree classifier: {:.3f}\".format(DT_accuracy))\nprint(\"LGBM classifier: {:.3f}\".format(LGBM_accuracy))\nprint(\"==========================================================\")","83a2ef45":"class DNN_wrapper:\n    def __init__(self, model):\n        self.model = model\n    def predict(self, df):\n        return np.rint(self.model.predict(df))[:,0]\n    def predict_proba(self, df):\n        probs = self.model.predict(df)\n        probs2 = np.ones_like(probs) - probs\n        packed = np.concatenate((probs2, probs), axis=1)        \n        return packed\n    \nDNN_model = DNN_wrapper(dnn_model)","c024142f":"models = [xgb_model, rf_model, SVM_model, LR_model, KNN_model, ADA_model, ETC_model, GBC_model, SGD_model, DT_model, LGBM_model, DNN_model]\nmodel_names = [\"XGB\", \"RF\", \"SVM\", \"LR\", \"KNN\", \"ADA\", \"ETC\", \"GBC\", \"SGD\", \"DT\", \"LGBM\", \"DNN\"]\nprint(\"using\", len(models), \"classifiers\")","096e24ee":"#SVC\n#TRESHOLD = 0.50001 #sub score 0.79425\n#TRESHOLD = 0.549 #sub score 0.79904\n#TRESHOLD = 0.6 #sub score 0.79904\n#TRESHOLD = 0.6 #with standard scaler, sub score 0.79665\nTRESHOLD = 0.65 #sub score 0.80861\n\nHVC_TRESHOLD = 0.500001","b688ef68":"%%time\nALL_PREDICTIONS = pd.DataFrame({'PassengerId': PassengerIds})\nfor i, m in enumerate(models):\n    ALL_PREDICTIONS[model_names[i]] = m.predict(TEST)\nALL_PREDICTIONS['Vote'] = ALL_PREDICTIONS[model_names].mean(axis=1)\nALL_PREDICTIONS['Predict'] = ALL_PREDICTIONS.Vote.apply(lambda row: 1 if row > HVC_TRESHOLD else 0)\nvc_predictions = ALL_PREDICTIONS.Predict\nALL_PREDICTIONS.head(10)","60b0e404":"fig, ax = plt.subplots(figsize=(12,12))  \ng= sns.heatmap(ALL_PREDICTIONS[model_names].corr(),annot=True)","2c3dc467":"%%time\n\n\nSVC_ALL_PREDICTIONS = pd.DataFrame({'PassengerId': PassengerIds})\nfor i, m in enumerate(models):\n    SVC_ALL_PREDICTIONS[model_names[i]] = m.predict_proba(TEST)[:,1]\nSVC_ALL_PREDICTIONS['MedianVote'] = SVC_ALL_PREDICTIONS[model_names].median(axis=1)\nSVC_ALL_PREDICTIONS['SoftVote'] = SVC_ALL_PREDICTIONS[model_names].mean(axis=1)\nSVC_ALL_PREDICTIONS['Predict'] = SVC_ALL_PREDICTIONS.SoftVote.apply(lambda row: 1 if row > TRESHOLD else 0)\nsvc_predictions = SVC_ALL_PREDICTIONS.Predict\nSVC_ALL_PREDICTIONS.head(10)","7e55e820":"fig, ax = plt.subplots(figsize=(12,12))  \ng= sns.heatmap(SVC_ALL_PREDICTIONS[model_names].corr(),annot=True)","89d9891a":"COMP_PREDICTIONS = pd.DataFrame({'PassengerId': PassengerIds})\nCOMP_PREDICTIONS['HVC'] = vc_predictions\nCOMP_PREDICTIONS['SVC'] = svc_predictions\nCOMP_PREDICTIONS.head(20)","d066b48f":"TRAIN_PREDICTIONS = pd.DataFrame({'Survived':train_data.Survived, 'Fare':_Fare, \"Title\": _Title,\"PClass\": _PClass, })\nfor i, m in enumerate(models):\n    TRAIN_PREDICTIONS[model_names[i]] = m.predict(X)\nTRAIN_PREDICTIONS['Vote'] = TRAIN_PREDICTIONS[model_names].mean(axis=1)\nTRAIN_PREDICTIONS['VC'] = TRAIN_PREDICTIONS.Vote.apply(lambda row: 1 if row > HVC_TRESHOLD else 0)\nwrong = TRAIN_PREDICTIONS[TRAIN_PREDICTIONS.Survived != TRAIN_PREDICTIONS.VC]\nwrong[(wrong.Vote >= 0.4) & (wrong.Vote <= 0.6)].head(10)","d128ec0f":"SVC_TRAIN_PREDICTIONS = pd.DataFrame({'Survived':train_data.Survived, 'Fare':_Fare, \"Title\": _Title,\"PClass\": _PClass, \"Sex\":_Sex})\nfor i, m in enumerate(models):\n    SVC_TRAIN_PREDICTIONS[model_names[i]] = m.predict_proba(X)[:,1]\n    \nSVC_TRAIN_PREDICTIONS['MedianVote'] = SVC_TRAIN_PREDICTIONS[model_names].median(axis=1)\nSVC_TRAIN_PREDICTIONS['SoftVote'] = SVC_TRAIN_PREDICTIONS[model_names].mean(axis=1)\nSVC_TRAIN_PREDICTIONS['SVC'] = SVC_TRAIN_PREDICTIONS.SoftVote.apply(lambda row: 1 if row > TRESHOLD else 0)\n\nwrong = SVC_TRAIN_PREDICTIONS[SVC_TRAIN_PREDICTIONS.Survived != SVC_TRAIN_PREDICTIONS.SVC]\nWS = wrong[(wrong.SoftVote >= 0.35) & (wrong.SoftVote <= 0.65) & (wrong.Survived == 1)].sort_values(\"SoftVote\", ascending=False)\nWD = wrong[(wrong.SoftVote >= 0.35) & (wrong.SoftVote <= 0.65) & (wrong.Survived == 0)].sort_values(\"SoftVote\", ascending=True)","1bf8c70c":"WS.head(10)","c3f101a7":"WD.head(20)","04522f8e":"train_scores = dict()\nfor clf in [*model_names, 'VC']:\n    train_scores[clf] = [len(TRAIN_PREDICTIONS[TRAIN_PREDICTIONS.Survived == TRAIN_PREDICTIONS[clf]]) \/ TRAIN_PREDICTIONS.shape[0]]\n\nTRAIN_SCORES = pd.DataFrame(train_scores)\nTRAIN_SCORES","aeb958fb":"TRAIN_SCORES.plot.bar();","4cc19163":"# don't forget to enter best predictions ...\n#output = pd.DataFrame({'PassengerId': PassengerIds, 'Survived': xgb_predictions})\n#output = pd.DataFrame({'PassengerId': PassengerIds, 'Survived': ada_predictions})\n# output = pd.DataFrame({'PassengerId': PassengerIds, 'Survived': dnn_predictions})\n#output = pd.DataFrame({'PassengerId': PassengerIds, 'Survived': vc_predictions})\n#output = pd.DataFrame({'PassengerId': PassengerIds, 'Survived': rf_predictions})\noutput = pd.DataFrame({'PassengerId': PassengerIds, 'Survived': svc_predictions})\n#svc_predictions\n\noutput.head(10)","a9d2e6ee":"#output\noutput.to_csv('submission.csv', index=False)\nprint(\"Submission was successfully saved!\")","f4015a7f":"end_time = time.time()\nprint(\"Notebook run time: {:.1f} seconds. Finished at {}\".format(end_time - start_time, datetime.now()) )","413af8fc":"## Cabin -> Deck\n* some cabin data can be aquired from othe family members, but only in two cases, it is not worth it\n* so X if person does not have cabin","e68e8192":"# Titanic, take #2\nLearning from my previous try https:\/\/www.kaggle.com\/lovroselic\/titanic-ls <br>\nGood points: <br>\n* Joining train and test for EDA, and split again for modeling\n* Survival groups, Family survival rate\n* using soft voting classifier, with treshold of 0.65\n* classifiers that are used: XGB, RF, SVM, ADA, GBC, SGD, ETC, DT,DNN, KNN, LR, LGBM","0169df7f":"# Feature selection & engineering","2323d2e3":"# Imports & config","386eb619":"### Save features for review","c762cf2e":"## Missing data\n* 20% of age values missing, we don't care: using title!\n    * Age was later used, and filled\n* 77% of cabin values missing (that is itself a value of feature)","40a1a452":"----------------","8ddebeac":"## ExtraTrees","dc313045":"## Target Encoding","93b2f67b":"## SGD Classifier","480901b2":"## Logistic Regression","a1c8be91":"# Split the data","5b4c088f":"## ADA Boost","e7b07d21":"---","e87c5ab4":"### Top X features","78a0e710":"## LGBM","192451c5":"## Decision Tree","dd20bc10":"# Models","97cc96b5":"# Predictions","0398fc4c":"## Family size\n* I will bin it to categorical","d3ccdb7c":"## XGBoost","5505062f":"## Survival groups (SG) and FamilySurvivalRate (FSR)\n* SG instead of FSR, works better:\n* adult_male: prepare to die\n* no_chilren: male doomed, female probably survives\n* solo_kid: not neccesarily alone (eg: father + daughter, daughter has chance, father not so ...)\n* group_survived: majority of the woman+children family group survived\n* group_died: majority of the woman+children family group died\n* added FSR back again, it's like duplicate , but numeric feature","4e6c71bb":"### Treshold","814ca701":"## Female, no children, pclass=3","181c16aa":"* equal weights","c55d8978":"## Clustering","caa41dd8":"### Categorical to dummies","6da17e71":"## Information from name\n* Title indicates both sex and age ","6c691353":"## Join","f519f3e4":"## Random Forest","c00d3af6":"## Gradient Boost (GBC)","9481a439":"## DNN","2cff05e3":"### Scale numeric features","c4696bf4":"## SibSp, Parch","74426f8e":"## Adult man","f282f9fe":"## Soft Voting - SVC","4227eeb6":"## Features","88f0914e":"## Scores","be207e6c":"## Prediction comparison","ce768d6b":"### Probability of Survival based on title\n* all rare titles grouped in male or female","8851a877":"## KNN","7a704b66":"### Fare to Bins","100d85a6":"## Dropping\n* drop selected in this run and see how the models behave\n* some features were explored but ultimately not used,as they are duplicates of other feature","bbc3c6bc":"## Age to categorical","79b35f26":"---","4e90953d":"## Sex","fc6e2d0b":"---","561eb1d2":"### Age","7037be11":"# Dropping & Encoding features","2ac0a234":"## Fare\n* fare needs to be divided with family size\n* some values are zero, one is missing, i will fill this with median of the corresponding PClass\n* distribution is skewed, I will apply log","da2849a0":"## PCA","2966a28b":"## Pclass","1467a299":"## SVM","df83bc96":"---","9ef00629":"## Remaining numeric features","95bbf931":"## Embarked, missing values\n* S is majority, fill with this","83ed389f":"## Checking Train Scores - SVC","10627afa":"# Hard Voting Classifier - VC","2abe893a":"---","fd3e3ca6":"# Load and check data","3595afd2":"## Checking Train Scores - HVC"}}