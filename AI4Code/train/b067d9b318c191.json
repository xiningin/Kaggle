{"cell_type":{"592b3c7e":"code","847a6260":"code","267811ea":"code","f1fd38f1":"code","bc92b1f9":"code","970f2132":"code","bd1149ce":"code","958d9831":"code","6cba9dff":"code","4bced379":"code","76cc9079":"code","a11ab213":"code","10c47679":"code","6600831c":"code","83b92caf":"code","f32fbe4b":"code","c0de0944":"markdown","0ab77833":"markdown","ce0ac2b0":"markdown","304791d8":"markdown","b26f3921":"markdown","b31abc9f":"markdown","876abfd0":"markdown","d50fff79":"markdown","7d39c7ff":"markdown","10f0bdb1":"markdown","3171134c":"markdown","12986c47":"markdown","06d8a15f":"markdown"},"source":{"592b3c7e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# importing sklearn packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import accuracy_score, roc_curve\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","847a6260":"all_data =  pd.read_csv(os.path.join(dirname, 'train.csv'))\nprint(\"Shape of data\", all_data.shape)\nall_data.head()","267811ea":"all_data.Survived.value_counts().plot(kind='bar')","f1fd38f1":"Y_full = all_data.Survived\nX_full = all_data.drop(columns='Survived', axis=1)\nx_train, x_valid, y_train, y_valid = train_test_split(X_full, Y_full, random_state=0, test_size =0.2, stratify=Y_full)","bc92b1f9":"x_train.dtypes","970f2132":"x_train.describe()","bd1149ce":"total_entries, _ =  x_train.shape\nmissing_columns = x_train.isnull().sum(axis=0)\npercentage_missing_counts = missing_columns\/total_entries\nfig, ax = plt.subplots(1, 2)\nmissing_columns.plot(kind='bar', ax=ax[0])\npercentage_missing_counts.plot(kind='bar', ax=ax[1])\nax[0].set_ylabel('number of missing values')\nax[0].set_yscale('log')\nax[1].set_ylabel('% of missing values')\nplt.tight_layout()\nprint(x_train.isnull().sum(axis=0))","958d9831":"numeric_features = ['Age', 'Fare']\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\ncategorical_features = ['Embarked', 'Sex', 'Pclass', 'SibSp']\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])","6cba9dff":"clfs = [LogisticRegression(), KNeighborsClassifier(), SVC(probability=True, random_state=0), RandomForestClassifier(random_state=0)]\nclf_names = [\"logsitic_regression\", \"knn\", 'SVM', 'random_forest']\nerrors = {'traning_accuracy':[], 'testing_accuracy':[]}\nclassifiers = []\nfor clf, name in zip(clfs, clf_names):\n    my_pipeline = Pipeline([('preprocessing', preprocessor), ('clf', clf)])\n    my_pipeline.fit(x_train, y_train)\n    errors['traning_accuracy'].append(accuracy_score(y_train, my_pipeline.predict(x_train)))\n    errors['testing_accuracy'].append(accuracy_score(y_valid, my_pipeline.predict(x_valid)))\n    classifiers.append(my_pipeline)\ndf_error = pd.DataFrame(errors, index=clf_names)\ndf_error.plot(kind='bar')\nprint(df_error)","4bced379":"# Plotting roc curve for all the baseline methods\ndef plot_roc_curves(clf_list, name_list, x_test, y_test):\n    fig, ax = plt.subplots()\n    for clf, name in zip(clf_list, name_list):\n        fpr, tpr, _ = roc_curve(y_test, clf.predict(x_test))\n        ax.plot(fpr, tpr, label=name)\n        ax.set_xlabel('False Positive Rate')\n        ax.set_ylabel('True Positive Rate')\n    plt.legend()\n    plt.show()\nplot_roc_curves(classifiers, clf_names, x_valid, y_valid)    ","76cc9079":"clfs_gcv = [LogisticRegression(solver='lbfgs', max_iter=1000), KNeighborsClassifier(), SVC(probability=True, random_state=0), RandomForestClassifier(random_state=0)]\nrf_params = {'estimate__n_estimators': [110, 120, 130], 'estimate__max_depth':[7, 8, 10,12, 15]}\nsvc_params = {'estimate__C': [15, 18, 20, 25], 'estimate__gamma': [0.5, 0.7, 1, 1.2, 1.5]}\nlinear_params = {'estimate__C':[2, 3, 4, 5, 6]}\nknn_params = {'estimate__n_neighbors': [10, 20, 30]}\nlist_params = [linear_params,knn_params, svc_params, rf_params]","a11ab213":"from sklearn.model_selection import GridSearchCV\ngcv = []\nerrors = {'traning_accuracy':[], 'testing_accuracy':[]}\nfor clf, params, name in zip(clfs_gcv, list_params, clf_names):\n    my_pipeline = Pipeline([('Preprocessing', preprocessor), ('estimate', clf)])\n    g = GridSearchCV(estimator=my_pipeline, param_grid=params, scoring='accuracy', cv=5, n_jobs=-1)\n    g.fit(x_train, y_train)\n    print('Best parameters using %s is %s' %(name, g.best_params_))\n    print('Best score using %s is %s' %(name, accuracy_score(y_valid, g.best_estimator_.predict(x_valid))))\n    gcv.append(g.best_estimator_)\n    errors['traning_accuracy'].append(accuracy_score(y_train, g.best_estimator_.predict(x_train)))\n    errors['testing_accuracy'].append(accuracy_score(y_valid, g.best_estimator_.predict(x_valid)))\nplot_roc_curves(gcv, clf_names, x_valid, y_valid)","10c47679":"from sklearn.ensemble import VotingClassifier\nvote_clf = VotingClassifier(estimators=[('rf', gcv[3]),\n                                        ('svc', gcv[2]),\n                                        ('log_reg', gcv[0]),\n                                        ('knn', gcv[1])],\n                           n_jobs=-1, voting='hard')\nvote_clf.fit(x_train, y_train)\nclf_names.append(\"Voting_clf\")\nerrors['traning_accuracy'].append(accuracy_score(y_train, vote_clf.predict(x_train)))\nerrors['testing_accuracy'].append(accuracy_score(y_valid, vote_clf.predict(x_valid)))\nprint('Accuracy socre for hard voting on validation data %s' %(accuracy_score(y_valid, vote_clf.predict(x_valid))))","6600831c":"from xgboost import XGBClassifier\nmy_model = XGBClassifier(n_estimators=100, max_depth=6, learning_rate = 1e-2)\nx_train_mod = preprocessor.fit_transform(x_train)\nx_valid_mod = preprocessor.transform(x_valid)\nmy_model.fit(x_train_mod, y_train, eval_metric='error', early_stopping_rounds=10, eval_set=[(x_valid_mod, y_valid)],verbose=False)\n","83b92caf":"clf_names.append(\"XGBoost\")\nerrors['traning_accuracy'].append(accuracy_score(y_train, my_model.predict(x_train_mod)))\nerrors['testing_accuracy'].append(accuracy_score(y_valid, my_model.predict(x_valid_mod)))\nprint('Model Accuracy on validation data', accuracy_score(y_valid, my_model.predict(x_valid_mod)))","f32fbe4b":"fig, ax =  plt.subplots(figsize = (8, 5))\ndf_error = pd.DataFrame(errors, index=clf_names)\ndf_error = df_error.sort_values(by = ['testing_accuracy'])\ndf_error.plot(kind='bar', ax=ax)\nax.grid()\nprint(df_error)","c0de0944":"### Setting up out of box classifiers to have a baseline accuracy","0ab77833":"### Now lets do a grid search for best parameters for the classifiers","ce0ac2b0":"### Now using XG boost to classifiy ","304791d8":"### The numerical columns had differnet scales, we will need to scale it, we will do so while data preprocessing","b26f3921":"Now creating a Ensmable by Hard Voting, hoping it would perform better on the test set\n","b31abc9f":"### Doing some EDA","876abfd0":"### data has categorical as well as numerical data","d50fff79":"Trasforming the data\n\n**Numerical Columns**\n\n* Imputing by Median Value\n* Standardization\n\n**Categorical Columns**\n\n* Impute by Most Frequent\n* OneHot Encoding\n\nRest of the colums like 'Name', 'Pasenegr ID', etc will be dropped as it's highly unlikey that they will be linked to survival.","7d39c7ff":"### looking at distribution of Survived person, To check if data set is balanced.\n### this will be factored in while spliting the data in to test and validation","10f0bdb1":"## loading the data","3171134c":"### Data is imbalaced, we will factor this in for train test split","12986c47":"### Accuracy scores across all the model","06d8a15f":"### There are missing values in 'Age', Cabin, and Embarked. the fraction of missing values in Cabin is too high, so we will drop this column and for rest of them we will do imputation."}}