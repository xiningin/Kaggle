{"cell_type":{"fe18bd5c":"code","19caa8e4":"code","da980461":"code","86173d67":"code","8048005d":"code","9b3a06d0":"code","8bc4e4fc":"code","8363e584":"code","be426e81":"code","3672674a":"code","e4bd4082":"code","17d2120b":"code","66b22643":"code","9bc2d301":"markdown","4141d9c9":"markdown","7b29cc5f":"markdown","e6b6194a":"markdown","cea4df30":"markdown","141ff71f":"markdown","8ab238dc":"markdown","d4eb5a75":"markdown","70427d3f":"markdown","26f26b5d":"markdown"},"source":{"fe18bd5c":"## General Libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\n\n## Deep Learning Libraries\n\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\nfrom keras.layers import Conv2D,MaxPooling2D\nfrom keras.preprocessing.image import load_img, img_to_array","19caa8e4":"df = pd.read_csv('..\/input\/facialexpressionrecognition\/fer2013.csv')","da980461":"df.head()","86173d67":"## shape of the dataset\n\ndf.shape","8048005d":"## checking for null values\n\ndf.isnull().sum()","9b3a06d0":"df['emotion'].value_counts()","8bc4e4fc":"train_data_dir = '..\/input\/emotion-detection-fer\/train'\nvalidation_data_dir = '..\/input\/emotion-detection-fer\/test'","8363e584":"# size of the image: 48*48 pixels\npicture_size = 48\n\n# input path for the images\nfolder_path = \"..\/input\/emotion-detection-fer\/train\"\n\nexpression = 'sad'\n\nplt.figure(figsize= (12,12))\nfor i in range(1, 10, 1):\n    plt.subplot(3,3,i)\n    img = load_img(folder_path+ '\/' +expression+\"\/\"+\n                  os.listdir(folder_path+ '\/' + expression)[i], target_size=(picture_size, picture_size))\n    plt.imshow(img)   \nplt.show()","be426e81":"## Defining different classes of emotion\nnum_classes = 7\n\n## Define image size\nimg_rows,img_cols = 48,48\n\n## Deifne the batch\nbatch_size = 64","3672674a":"train_datagen = ImageDataGenerator(\n\t\t\t\t\trescale=1.\/255,\n\t\t\t\t\trotation_range=30,\n\t\t\t\t\tshear_range=0.3,\n\t\t\t\t\tzoom_range=0.3,\n\t\t\t\t\twidth_shift_range=0.4,\n\t\t\t\t\theight_shift_range=0.4,\n\t\t\t\t\thorizontal_flip=True,\n\t\t\t\t\tfill_mode='nearest')\n\nvalidation_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n\t\t\t\t\ttrain_data_dir,\n\t\t\t\t\tcolor_mode='grayscale',\n\t\t\t\t\ttarget_size=(img_rows,img_cols),\n\t\t\t\t\tbatch_size=batch_size,\n\t\t\t\t\tclass_mode='categorical',\n\t\t\t\t\tshuffle=True)\n\nvalidation_generator = validation_datagen.flow_from_directory(\n\t\t\t\t\t\t\tvalidation_data_dir,\n\t\t\t\t\t\t\tcolor_mode='grayscale',\n\t\t\t\t\t\t\ttarget_size=(img_rows,img_cols),\n\t\t\t\t\t\t\tbatch_size=batch_size,\n\t\t\t\t\t\t\tclass_mode='categorical',\n\t\t\t\t\t\t\tshuffle=True)","e4bd4082":"\nmodel = Sequential()\n\n# Block-1\n\nmodel.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\n# Block-2 \n\nmodel.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\n# Block-3\n\nmodel.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\n# Block-4 \n\nmodel.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\n# Block-5\n\nmodel.add(Flatten())\nmodel.add(Dense(64,kernel_initializer='he_normal'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\n# Block-6\n\nmodel.add(Dense(64,kernel_initializer='he_normal'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\n# Block-7\n\nmodel.add(Dense(num_classes,kernel_initializer='he_normal'))\nmodel.add(Activation('softmax'))\n\nprint(model.summary())\n\nfrom keras.optimizers import RMSprop,SGD,Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\ncheckpoint = ModelCheckpoint('Emotion_little_vgg.h5',\n                             monitor='val_loss',\n                             mode='min',\n                             save_best_only=True,\n                             verbose=1)\n\nearlystop = EarlyStopping(monitor='val_loss',\n                          min_delta=0,\n                          patience=3,\n                          verbose=1,\n                          restore_best_weights=True\n                          )\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                              factor=0.2,\n                              patience=3,\n                              verbose=1,\n                              min_delta=0.0001)\n\ncallbacks = [earlystop,checkpoint,reduce_lr]\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer = Adam(lr=0.001),\n              metrics=['accuracy'])\n\nnb_train_samples = 24320\nnb_validation_samples = 3072\nepochs=40\n","17d2120b":"history=model.fit_generator(\n                train_generator,\n                steps_per_epoch=nb_train_samples\/\/batch_size,\n                epochs=epochs,\n                callbacks=callbacks,\n                validation_data=validation_generator,\n                validation_steps=nb_validation_samples\/\/batch_size)","66b22643":"plt.style.use('dark_background')\n\nplt.figure(figsize=(20,10))\nplt.subplot(1, 2, 1)\nplt.suptitle('Optimizer : Adam', fontsize=10)\nplt.ylabel('Loss', fontsize=16)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend(loc='upper right')\n\nplt.subplot(1, 2, 2)\nplt.ylabel('Accuracy', fontsize=16)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.show()","9bc2d301":"# Libraries","4141d9c9":"# Model Building","7b29cc5f":"# Data","e6b6194a":"# Path of input Data","cea4df30":"So, in the dataset there is not any missing values.","141ff71f":"## Facial expression for emotion detection has always been an easy task for humans, but achieving the same task with a computer algorithm is quite challenging. With the recent advancement in computer vision and machine learning, it is possible to detect emotions from images.\n\n### In this project we use a technique called facial emotion recognition using convolutional neural networks **(FERC)**\n\n### The FERC is based on two-part :-\n### 1. convolutional neural network (CNN): The first-part removes the background from the picture, \n### 2. The second part concentrates on the facial feature vector extraction","8ab238dc":"# Displaying Images","d4eb5a75":"# Model Training and Validation Data","70427d3f":"# Accuracy and Performance","26f26b5d":"# count of each classes"}}