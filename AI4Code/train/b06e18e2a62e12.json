{"cell_type":{"b541bc84":"code","9573bcee":"code","03d93a90":"code","083b3b82":"code","6bbb9819":"code","024a42a3":"code","dbbd4fab":"code","8e69575d":"code","374fb451":"code","28a9ec05":"code","32a18bc8":"code","02e9f8b3":"code","174ca2db":"code","3ed1f7b9":"code","989e4f0c":"code","ae5625ac":"code","3cb87898":"code","b740a0d7":"code","6797f6ae":"code","6ed2507f":"code","390df121":"code","b4e5f3e8":"code","4da6781a":"code","31cd5b00":"code","593e42fb":"code","70ecd5bd":"code","18889040":"code","9784686b":"code","6fa43b39":"code","22ef1f47":"code","5b8ae9a1":"code","03ef325f":"code","c60ac7e7":"code","4f62176e":"code","ab68fead":"code","df601674":"code","9df6513f":"code","784d39ab":"code","ca9c634c":"code","de29713d":"code","55d4fc76":"code","19eb632f":"code","d6c40306":"code","0a429119":"code","10822e8b":"code","f5881e54":"code","1f1aad07":"code","9d80568a":"code","ad01bc6a":"code","d5bb10e8":"code","f52feb02":"markdown","9069528f":"markdown","7c6f4cef":"markdown","eeaacf07":"markdown","081705cd":"markdown","f0c601dd":"markdown","02f0135e":"markdown","7a047da4":"markdown","40930f76":"markdown","4959453b":"markdown","b0b57435":"markdown","7f373f86":"markdown"},"source":{"b541bc84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9573bcee":"train = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col=0)\ntest = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col=0)\n\n\nprint(\"train: \", train.shape)\nprint(\"test: \", test.shape)\ntrain.head()","03d93a90":"X = pd.concat([train.drop(\"SalePrice\", axis=1),test], axis=0)\ny = train[['SalePrice']]","083b3b82":"X.info()","6bbb9819":"numeric_ = X.select_dtypes(exclude=['object']).drop(['MSSubClass'],axis = 1).copy()\nnumeric_","024a42a3":"disc_num_var = ['OverallQual','OverallCond','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath',\n                'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'MoSold', 'YrSold']\ncont_num_var = []\nfor i in numeric_.columns:\n    if i not in disc_num_var:\n        cont_num_var.append(i)","dbbd4fab":"categor_train = X.select_dtypes(['object']).copy()\ncategor_train","8e69575d":"fig = plt.figure(figsize=(18,16))\nfor index,col in enumerate(cont_num_var):\n    plt.subplot(6,4,index+1)\n    sns.distplot(numeric_.loc[:,col].dropna(), kde = False)\nfig.tight_layout(pad=1.0)","374fb451":"fig = plt.figure(figsize=(14,15))\nfor index,col in enumerate(cont_num_var):\n    plt.subplot(6,4,index+1)\n    sns.boxplot(y=col, data=numeric_.dropna())\nfig.tight_layout(pad=1.0)","28a9ec05":"fig = plt.figure(figsize=(25,25))\nfor index, cols in enumerate(disc_num_var):\n    plt.subplot(5,3,index+1)\n    sns.countplot(x=col,data = numeric_.dropna())\nfig.tight_layout(pad=1.0)","32a18bc8":"fig = plt.figure(figsize=(20,20))\nfor index in range(len(categor_train.columns)):\n    plt.subplot(9,5,index+1)\n    sns.countplot(x=categor_train.iloc[:,index], data=categor_train.dropna())\n    plt.xticks(rotation=90)\nfig.tight_layout(pad=1.0)","02e9f8b3":"plt.figure(figsize=(14,12))\n\nsns.heatmap(numeric_.corr(),annot=True, mask = numeric_.corr() < 0.8 ,linewidth=0.7,cmap='Blues')","174ca2db":"numeric_train = train.select_dtypes(exclude=['object'])\ncorrelation = numeric_train.corr()\ncorrelation[['SalePrice']].sort_values(['SalePrice'],ascending = False)","3ed1f7b9":"X.drop(['GarageYrBlt','TotRmsAbvGrd','1stFlrSF','GarageCars'], axis=1, inplace=True)","989e4f0c":"plt.figure(figsize=(25,8))\n\nplt.title('Number of missing rows')\n\n\nmissing_count = pd.DataFrame(X.isnull().sum(), columns = ['sum']).sort_values(by=['sum'],ascending = False).head(15).reset_index()\n\nmissing_count.columns = ['features','sum']\n\nsns.barplot(x = 'features',y = 'sum',data = missing_count)","ae5625ac":"X.drop(['PoolQC','MiscFeature','Alley'], axis=1, inplace=True)","3cb87898":"print(correlation[['SalePrice']].sort_values(['SalePrice'], ascending=False).tail(10))\n\nX.drop(['MoSold','YrSold'], axis=1, inplace=True)","b740a0d7":"cat_col = X.select_dtypes(include=['object']).columns\noverfit_cat1 = []\nfor i in cat_col:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 96:\n        overfit_cat1.append(i)\noverfit_cat1 = list(overfit_cat1)\nX.drop(overfit_cat1, axis=1,inplace = True)","6797f6ae":"num_col = X.select_dtypes(exclude = ['object']).columns\noverfit_num = []\nfor i in num_col:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros\/len(X)*100 > 96:\n        overfit_num.append(i)\noverfit_num = list(overfit_num)\nX.drop(overfit_num,axis=1,inplace = True)","6ed2507f":"X.shape","390df121":"print(\"Categorical Features with >96% of the same value: \",overfit_cat1)\nprint(\"Numerical Features with >96% of the same value: \",overfit_num)","b4e5f3e8":"cat = ['GarageType','GarageFinish','BsmtFinType2','BsmtExposure','BsmtFinType1', \n       'GarageCond','GarageQual','BsmtCond','BsmtQual','FireplaceQu','Fence',\"KitchenQual\",\n       \"HeatingQC\",'ExterQual','ExterCond']\n\nX[cat] = X[cat].fillna(\"NA\")\n","4da6781a":"cols = [\"MasVnrType\", \"MSZoning\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\", \"Electrical\", \"Functional\"]\nX[cols] = X.groupby(\"Neighborhood\")[cols].transform(lambda x: x.fillna(x.mode()[0]))\n","31cd5b00":"X['LotFrontage'] = X.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))\nX['GarageArea'] = X.groupby('Neighborhood')['GarageArea'].transform(lambda x: x.fillna(x.mean()))\nX['MSZoning'] = X.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n#numerical\ncont = [\"BsmtHalfBath\", \"BsmtFullBath\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"MasVnrArea\"]\nX[cont] = X[cont] = X[cont].fillna(X[cont].mean())","593e42fb":"X['MSSubClass'] = X['MSSubClass'].apply(str)\n","70ecd5bd":"X['BsmtFinType1'].value_counts()","18889040":"ordinal_map = {'Ex': 5,'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA':0}\nfintype_map = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'NA': 0}\nexpose_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\nfence_map = {'GdPrv': 4,'MnPrv': 3,'GdWo': 2, 'MnWw': 1,'NA': 0}","9784686b":"ord_col = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond', 'FireplaceQu']\nfor col in ord_col:\n    X[col] = X[col].map(ordinal_map)\n    \nfin_col = ['BsmtFinType1','BsmtFinType2']\nfor col in fin_col:\n    X[col] = X[col].map(fintype_map)\n\nX['BsmtExposure'] = X['BsmtExposure'].map(expose_map)\nX['Fence'] = X['Fence'].map(fence_map)\n","6fa43b39":"train.shape","22ef1f47":"test.shape","5b8ae9a1":"X.shape\n","03ef325f":"X.drop('BsmtFinSF2', axis=1,inplace = True)","c60ac7e7":"X.shape","4f62176e":"X = pd.get_dummies(X)\n","ab68fead":"X.shape","df601674":"plt.figure(figsize=(10,6))\nplt.title(\"Before transformation of SalePrice\")\ndist = sns.distplot(train['SalePrice'],norm_hist=False)","9df6513f":"y[\"SalePrice\"] = np.log(y['SalePrice'])","784d39ab":"x = X.loc[train.index]\ny = y.loc[train.index]","ca9c634c":"X.shape","de29713d":"test = X.loc[test.index]","55d4fc76":"test.shape","19eb632f":"from sklearn.preprocessing import RobustScaler\n\ncols = x.select_dtypes(np.number).columns\ntransformer = RobustScaler().fit(x[cols])\nx[cols] = transformer.transform(x[cols])\ntest[cols] = transformer.transform(test[cols])","d6c40306":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=2020)","0a429119":"y.shape","10822e8b":"from sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn import ensemble\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom catboost import CatBoostRegressor","f5881e54":"xgb = XGBRegressor(booster='gbtree', objective='reg:squarederror',n_estimators = 1000,\n                  learning_rate =0.05) \nxgb.fit(X_train,y_train)","1f1aad07":"lgbm = LGBMRegressor(boosting_type='gbdt',objective='regression', max_depth=8,\n                    lambda_l1=0.0001, lambda_l2=0, learning_rate=0.1,\n                    n_estimators=1000, max_bin=200, min_child_samples=20, \n                    bagging_fraction=0.75, bagging_freq=5,\n                    bagging_seed=7, feature_fraction=0.8,\n                    feature_fraction_seed=7, verbose=-1)\nlgbm.fit(X_train,y_train)","9d80568a":"cb = CatBoostRegressor(loss_function='RMSE', logging_level='Silent',\n                       n_estimators = 1000,learning_rate=0.05)\ncb.fit(X_train,y_train)","ad01bc6a":"def blend_models_predict(X, b, c, d):\n    return ((b* xgb.predict(X)) + (c * lgbm.predict(X)) + (d * cb.predict(X)))","d5bb10e8":"subm = np.exp(blend_models_predict(test, 0.4, 0.2, 0.4))\nsubmission = pd.DataFrame({'Id': test.index,\n                           'SalePrice': subm})\n\nsubmission.to_csv(\"..\/..\/kaggle\/working\/submission.csv\", index=False)","f52feb02":"* ### Features with multicollinearity","9069528f":"## Scaling the data\nRobustScaler is a transformation technique that removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). It is also robust to outliers, which makes it ideal for data where there are too many outliers that will drastically reduce the number of training data.","7c6f4cef":"This section outlines the steps for Data Processing:\n1. Removing Redundant Features\n2. Dealing with Outliers\n3. Filling in missing values","eeaacf07":"- We see a lot of outliers, will remove them during our preprocessing period.","081705cd":"- Values with only 0s don't add value to our data, hence we will remove them from our data in the preprocessing time.","f0c601dd":"# Data Analysis","02f0135e":"## Univariate Analysis","7a047da4":"## Dealing with Outliers","40930f76":"## 2.1        Removing Redundant Features","4959453b":"**** **Highly Correlated variables**:\n* GarageYrBlt and YearBuilt\n* TotRmsAbvGrd and GrLivArea\n* 1stFlrSF and TotalBsmtSF\n* GarageArea and GarageCars\n\nFrom the correlation matrix we have identified the above variables which are highly correlated with each other. This finding will guide us in our preprocessing steps later on as we aim to remove highly correlated features to avoid performance loss in our model","b0b57435":"Since the distribution is right skewed, we take the log transformation and convert it too Normal Distribution","7f373f86":"# 2. Data Preprocessing"}}