{"cell_type":{"2b42cdb7":"code","c7a40ae3":"code","76babd70":"code","dce48f3d":"code","4116c535":"code","85cde0a4":"code","66143356":"code","7c0d235e":"code","9463c7fa":"code","de903339":"code","3f892b7e":"code","80d4080b":"code","7cf0c3c7":"code","5fc6c549":"code","7dcfbab4":"code","50b7eddd":"code","15aaf4ad":"code","f52eaa38":"code","0e0c9878":"code","749f98c7":"code","d1902d99":"code","fc827c7d":"code","83700762":"code","74a26ded":"code","ce3b7827":"code","93b360ae":"code","fa2c8cb3":"code","c725b7f4":"code","c81f2174":"code","3a93e00c":"code","e8078808":"code","fa1b456c":"code","cceec96f":"code","67ab75f3":"code","12cd93e5":"code","2f8e6e4c":"code","37bd00de":"code","9850f9a7":"code","69f049dd":"code","bf715aa8":"code","9ca18cd0":"code","101536e6":"code","26cfc98c":"code","5b41ec46":"markdown","93e1223a":"markdown","e0a4b312":"markdown","ebaf3721":"markdown","8ace6ee3":"markdown","a7ea5e5b":"markdown","4a645a6b":"markdown","33f9f340":"markdown","1fcefef8":"markdown","af5c346d":"markdown","c3131bca":"markdown","295b4515":"markdown","bb97e996":"markdown","66829f3f":"markdown","da58a2ce":"markdown","7fd25e97":"markdown","5e7e6dc5":"markdown","8178f6f4":"markdown","48c670f2":"markdown","6388b107":"markdown","394f6a9a":"markdown","ee84ef49":"markdown","0fe88cbb":"markdown","e6804f0b":"markdown","0eb03f7d":"markdown","46ddc450":"markdown","e71bc05f":"markdown","2af7702b":"markdown","2be84f2a":"markdown","b536037e":"markdown","ceedd3b9":"markdown","91176cd1":"markdown","52e27ae9":"markdown","7161c936":"markdown","2b3f7f42":"markdown","40e0cc0f":"markdown","b2ea2993":"markdown","200af264":"markdown","dde66571":"markdown","1bc5d6ed":"markdown","8cb89535":"markdown","edf17aa5":"markdown","d6b5090e":"markdown","b487e176":"markdown","6ce8b236":"markdown","c3b99e1e":"markdown","a2d0c628":"markdown","22f0f796":"markdown","6977ce08":"markdown"},"source":{"2b42cdb7":"import seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nfrom scipy.stats import skew, kurtosis, norm\nfrom sklearn.ensemble import RandomForestRegressor","c7a40ae3":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","76babd70":"test.head()","dce48f3d":"train.head()","4116c535":"y_train, train = train['SalePrice'], train.drop(['SalePrice'], axis=1)","85cde0a4":"y_train.head()","66143356":"train.info()","7c0d235e":"train.drop(['Id'], axis=1, inplace=True)\nid_test, test = test['Id'], test.drop(['Id'], axis=1)","9463c7fa":"Total = pd.concat([train,test], axis=0).isnull().sum().sort_values(ascending=False)\npercent = (pd.concat([train,test], axis=0).isnull().sum() \/ pd.concat([train,test], axis=0).isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([Total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(35)","de903339":"data = [train, test]\n\nfor dataset in data:\n    #fill NA cells of column 'MSZoning' with 'RL' (Mode)\n    dataset['MSZoning'].fillna('RL', inplace=True)\n\n    #fill NA cells of column 'LotFrontage' with '0' (Don't have any connection with streets)\n    dataset['LotFrontage'].fillna(0, inplace=True)\n\n    #fill NA cells of column 'Utilities' with 'AllPub' (Mode)\n    dataset['Utilities'].fillna('AllPub', inplace=True)\n\n    #fill NA cells of columns 'Exterior1st' and 'Exterior2nd' with 'VinylSd' (Mode)\n    dataset['Exterior1st'].fillna('VinylSd', inplace=True)\n    dataset['Exterior2nd'].fillna('VinylSd', inplace=True)\n\n    #fill NA cells of column 'Utilities' with 'AllPub' (Mode)\n    dataset['Utilities'].fillna('AllPub', inplace=True)\n\n    #fill NA cells of column 'MasVnrType' with 'None' (Mode)\n    dataset['MasVnrType'].fillna('None', inplace=True)\n\n    #fill NA cells of column 'MasVnrArea' with 0.0\n    dataset['MasVnrArea'].fillna(0.0, inplace=True)\n\n    #fill NA cells of column 'BsmtQual' with 'NB'\n    dataset['BsmtQual'].fillna('NB', inplace=True)\n\n    #fill NA cells of column 'BsmtCond' with 'NB'\n    dataset['BsmtCond'].fillna('NB', inplace=True)\n\n    #fill NA cells of column 'BsmtCond' with 'NB'\n    dataset['BsmtCond'].fillna('NB', inplace=True)\n\n    #fill NA cells of column 'BsmtExposure' with 'NB'\n    dataset['BsmtExposure'].fillna('NB', inplace=True)\n\n    #fill NA cells of column 'BsmtFinType1' with 'NB'\n    dataset['BsmtFinType1'].fillna('NB', inplace=True)\n\n    #fill NA cells of column 'BsmtFinSF1' with 0.0\n    dataset['BsmtFinSF1'].fillna(0.0, inplace=True)\n\n    #fill NA cells of column 'BsmtFinType2' with 'NB'\n    dataset['BsmtFinType2'].fillna('NB', inplace=True)\n\n    #fill NA cells of column 'BsmtFinSF2' with 0.0\n    dataset['BsmtFinSF2'].fillna(0.0, inplace=True)\n\n    #fill NA cells of column 'BsmtUnfSF' with 0.0\n    dataset['BsmtUnfSF'].fillna(0.0, inplace=True)\n\n    #fill NA cells of column 'TotalBsmtSF' with 0.0\n    dataset['TotalBsmtSF'].fillna(0.0, inplace=True)\n\n    #fill NA cells of column 'Electrical' with 'SBrkr' (Mode)\n    dataset['Electrical'].fillna('SBrkr', inplace=True)\n\n    #fill NA cells of column 'BsmtFullBath' with 0.0 (Mode)\n    dataset['BsmtFullBath'].fillna(0.0, inplace=True)\n\n    #fill NA cells of column 'BsmtHalfBath' with 0.0 (Mode)\n    dataset['BsmtHalfBath'].fillna(0.0, inplace=True)\n\n    #fill NA cells of column 'KitchenQual' with 'TA' (Mode)\n    dataset['KitchenQual'].fillna('TA', inplace=True)\n\n    #fill NA cells of column 'Functional' with 'Typ' (Mode)\n    dataset['Functional'].fillna('Typ', inplace=True)\n\n    #fill NA cells of column 'FireplaceQu' with 'NB'\n    dataset['FireplaceQu'].fillna('NB', inplace=True)\n\n    #fill NA cells of column 'GarageType' with 'NB'\n    dataset['GarageType'].fillna('NB', inplace=True)\n\n    #fill NA cells of column 'GarageYrBlt' with 0\n    dataset['GarageYrBlt'].fillna(0, inplace=True)\n\n    #fill NA cells of column 'GarageFinish' with 'NB'\n    dataset['GarageFinish'].fillna('NB', inplace=True)\n\n    #fill NA cells of column 'GarageCars' with 0.0\n    dataset['GarageCars'].fillna(0.0, inplace=True)\n\n    #fill NA cells of column 'GarageArea' with 0.0\n    dataset['GarageArea'].fillna(0.0, inplace=True)\n\n    #fill NA cells of column 'GarageQual' with 'NB'\n    dataset['GarageQual'].fillna('NB', inplace=True)\n\n    #fill NA cells of column 'GarageCond' with 'NB'\n    dataset['GarageCond'].fillna('NB', inplace=True)\n\n    #fill NA cells of column 'SaleType' with 'WD'\n    dataset['SaleType'].fillna('WD', inplace=True)\n\n    #fill NA cells of column 'Fence' with 'NB'\n    dataset['Fence'].fillna('NB', inplace=True)\n\n    #fill NA cells of column 'Alley' with 'NB'\n    dataset['Alley'].fillna('NB', inplace=True)\n\n    #fill NA cells of column 'PoolQC' with 'NB'\n    dataset['PoolQC'].fillna('NB', inplace=True)\n\n    #fill NA cells of column 'MiscFeature' with 0\n    dataset['MiscFeature'].fillna(0, inplace=True)","3f892b7e":"features = list(train.columns.values)","80d4080b":"cat_features = ['MSSubClass', 'MSZoning', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope',\n 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n'Exterior1st', 'Exterior2nd', 'MasVnrType',  'Foundation', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'Electrical',\n 'GarageType', 'MiscVal', 'YrSold', 'SaleType', 'SaleCondition', 'MoSold']","7cf0c3c7":"for j in range(0, 28, 9):\n    fig, axes = plt.subplots(3, 3, constrained_layout=True, figsize=(20,10))\n    for i, col in enumerate(cat_features[j:j + 9]):\n        sns.countplot(x=col, data=train, ax=axes[int(i \/ 3), int(i % 3)])","5fc6c549":"ord_features = ['Alley', 'Street', 'Utilities', 'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual','BsmtCond',\n 'BsmtExposure', 'HeatingQC', 'CentralAir', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n                'KitchenAbvGr', 'KitchenQual','TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu',\n                'GarageFinish', 'GarageCars', 'GarageQual', 'GarageCond',  'PavedDrive', 'PoolQC', 'Fence' , 'MiscFeature']","7dcfbab4":"train[ord_features] = train[ord_features].replace({'Grvl': 1, 'Pave': 2,\n                         'AllPub': 3,\t'NoSewr': 2, 'NoSeWa': 1, 'ELO': 0,\n                         'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NB': 0,\n                         'Av': 3, 'Mn': 2, 'No': 1, 'Y': 1, 'N': 0,\n                         'Typ': 3.5, 'Min1': 3, 'Min2': 2.5, 'Mod': 2, 'Maj1': 1.5,\n                         'Maj2': 1, 'Sev': 0.5, 'Sal': 0, 'Fin': 3, 'RFn': 2, 'Unf': 1,\n                         'P': 0.5, 'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1,\n                        'Elev': 1, 'Gar2':1, 'Othr': 1, 'Shed': 1, 'TenC': 1})\n\ntest[ord_features] = test[ord_features].replace({'Grvl': 1, 'Pave': 2,\n                         'AllPub': 3,\t'NoSewr': 2, 'NoSeWa': 1, 'ELO': 0,\n                         'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NB': 0,\n                         'Av': 3, 'Mn': 2, 'No': 1, 'Y': 1, 'N': 0,\n                         'Typ': 3.5, 'Min1': 3, 'Min2': 2.5, 'Mod': 2, 'Maj1': 1.5,\n                         'Maj2': 1, 'Sev': 0.5, 'Sal': 0, 'Fin': 3, 'RFn': 2, 'Unf': 1,\n                         'P': 0.5, 'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1,\n                        'Elev': 1, 'Gar2':1, 'Othr': 1, 'Shed': 1, 'TenC': 1})","50b7eddd":"for j in range(0, 28, 9):\n    fig, axes = plt.subplots(3, 3, constrained_layout=True, figsize=(20,10))\n    for i, col in enumerate(ord_features[j:j + 9]):\n        sns.countplot(x=col, data=train, ax=axes[int(i \/ 3), int(i % 3)])","15aaf4ad":"num_features = list(filter(lambda x: x not in cat_features + ord_features, features))","f52eaa38":"for j in range(0, 10, 9):\n    fig, axes = plt.subplots(3, 3, constrained_layout=True, figsize=(20,10))\n    for i, col in enumerate(num_features[j:j + 9]):\n        sns.distplot(train[col], ax=axes[int(i \/ 3), int(i % 3)])","0e0c9878":"corrmat = train.corr()\nf, ax = plt.subplots(figsize=(15, 12))\nsns.heatmap(corrmat, vmax=.8, square=True)","749f98c7":"corr = pd.concat([train,y_train], axis=1).corr()\nhighest_corr_features = list(corr.index[abs(corr[\"SalePrice\"])>0.4])\nplt.figure(figsize=(15,15))\nsns.heatmap(pd.concat([train,y_train], axis=1)[highest_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","d1902d99":"corr[\"SalePrice\"].sort_values(ascending=False)","fc827c7d":"sns.distplot(list(y_train))\nprint(\"Skewness: %f\" % skew(list(y_train)))\nprint(\"Kurtosis: %f\" % kurtosis(list(y_train)))","83700762":"y_train = np.log1p(list(y_train))\nsns.distplot(y_train, fit=norm)","74a26ded":"# compare 'MiscFeature' and 'MiscVal'\ntrain.drop(['MiscFeature'], axis=1, inplace=True)\ntest.drop(['MiscFeature'], axis=1, inplace=True)\n\n# compare 'PoolArea' and 'PoolQC'\ntrain.drop(['PoolArea'], axis=1, inplace=True)\ntest.drop(['PoolArea'], axis=1, inplace=True)\n\n# compare 'GarageCond' and 'GarageQual'\ntrain.drop(['GarageCond'], axis=1, inplace=True)\ntest.drop(['GarageCond'], axis=1, inplace=True)\n\n# compare 'GarageCars' and 'GarageArea'\ntrain.drop(['GarageArea'], axis=1, inplace=True)\ntest.drop(['GarageArea'], axis=1, inplace=True)\n\n# compare 'Fireplaces' and 'FireplaceQu'\ntrain.drop(['Fireplaces'], axis=1, inplace=True)\ntest.drop(['Fireplaces'], axis=1, inplace=True)\n\n# compare 'GarageYrBlt' and 'GarageQual'\ntrain.drop(['GarageYrBlt'], axis=1, inplace=True)\ntest.drop(['GarageYrBlt'], axis=1, inplace=True)\n\n# compare 'BsmtCond' and 'BsmtQual'\ntrain.drop(['BsmtCond'], axis=1, inplace=True)\ntest.drop(['BsmtCond'], axis=1, inplace=True)\n\n# these features have low correlation with 'SalePrice' and include in 'FullBath' and 'TotRmsAbvGrd'\ntrain.drop(['BsmtFullBath'], axis=1, inplace=True)\ntrain.drop(['BsmtHalfBath'], axis=1, inplace=True)\ntrain.drop(['HalfBath'], axis=1, inplace=True)\ntrain.drop(['KitchenAbvGr'], axis=1, inplace=True)\ntrain.drop(['BedroomAbvGr'], axis=1, inplace=True)\n\ntest.drop(['BsmtFullBath'], axis=1, inplace=True)\ntest.drop(['BsmtHalfBath'], axis=1, inplace=True)\ntest.drop(['HalfBath'], axis=1, inplace=True)\ntest.drop(['KitchenAbvGr'], axis=1, inplace=True)\ntest.drop(['BedroomAbvGr'], axis=1, inplace=True)","ce3b7827":"features = list(train.columns.values)\n\ncat_features = ['MSSubClass', 'MSZoning', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope',\n 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n'Exterior1st', 'Exterior2nd', 'MasVnrType',  'Foundation', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'Electrical',\n 'GarageType', 'YrSold', 'SaleType', 'SaleCondition', 'MoSold']\n\nord_features = ['Alley', 'Street', 'Utilities', 'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual',\n                'BsmtExposure', 'HeatingQC', 'CentralAir', 'FullBath', 'KitchenQual','TotRmsAbvGrd',\n                'Functional', 'FireplaceQu', 'GarageFinish', 'GarageCars', 'GarageQual',  'PavedDrive', 'PoolQC',\n                'Fence', 'MiscVal']\n\nnum_features = list(filter(lambda x: x not in cat_features + ord_features, features))","93b360ae":"for feature in num_features:\n    train[feature] = np.log1p(train[feature])\n    test[feature] = np.log1p(test[feature])","fa2c8cb3":"train = pd.DataFrame(train)\ny_train = pd.DataFrame(y_train, columns=['SalePrice'])\ntest = pd.DataFrame(test)","c725b7f4":"corr = pd.concat([train,y_train], axis=1).corr()\nhighest_corr_features = list(corr.index[abs(corr[\"SalePrice\"])>0.00])\nhighest_corr_features = highest_corr_features[:-1]\n\n#%%\n\ncorr[\"SalePrice\"].sort_values(ascending=False)","c81f2174":"cat_train = train[cat_features]\ncat_test = test[cat_features]","3a93e00c":"cat_train['MSSubClass'] = cat_train['MSSubClass'].apply(str)\ncat_train['YrSold'] = cat_train['YrSold'].apply(str)\ncat_train['MoSold'] = cat_train['MoSold'].apply(str)\n\ncat_test['MSSubClass'] = cat_test['MSSubClass'].apply(str)\ncat_test['YrSold'] = cat_test['YrSold'].apply(str)\ncat_test['MoSold'] = cat_test['MoSold'].apply(str)","e8078808":"def encode_and_bind(original_dataframe, feature_to_encode):\n    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n    res = pd.concat([original_dataframe, dummies], axis=1)\n    res = res.drop([feature_to_encode], axis=1)\n    return(res)","fa1b456c":"for feature in cat_features:\n    cat_train = encode_and_bind(cat_train, feature)\n    cat_test = encode_and_bind(cat_test, feature)","cceec96f":"new_common_cat_features = []\nfor feature in list(cat_test.columns):\n    if feature in list(cat_train.columns):\n        new_common_cat_features.append(feature)","67ab75f3":"cat_test = cat_test[new_common_cat_features]\ncat_train = cat_train[new_common_cat_features]","12cd93e5":"cat_train","2f8e6e4c":"train = train[highest_corr_features]\ntrain[new_common_cat_features] = cat_train.values\n\ntest = test[highest_corr_features]\ntest[new_common_cat_features] = cat_test.values","37bd00de":"import xgboost as XGB\n\nxgb = XGB.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468,\n                             learning_rate=0.05, max_depth=3,\n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, random_state =42, nthread = -1)\nxgb.fit(train, y_train)","9850f9a7":"xgb_pred = xgb.predict(train)\nnp.sqrt(metrics.mean_squared_error(y_train, xgb_pred))","69f049dd":"import lightgbm as lgb\n\nlgbm = lgb.LGBMRegressor(objective='regression',n_estimators=720, )\nlgbm.fit(train, y_train)","bf715aa8":"lgb_pred = lgbm.predict(train)\nnp.sqrt(metrics.mean_squared_error(y_train, lgb_pred))","9ca18cd0":"ensemble = xgb_pred * 0.80 + lgb_pred * 0.20\nnp.sqrt(metrics.mean_squared_error(y_train, ensemble))","101536e6":"submission = pd.DataFrame(id_test)","26cfc98c":"xgb_pred = xgb.predict(test)\nlgb_pred = lgbm.predict(test)\nanswer = xgb_pred * 0.80 + lgb_pred * 0.20\nsubmission['SalePrice'] = np.expm1(answer)\nsubmission.to_csv('submission.csv', index=False)","5b41ec46":"I sorted through these weights. They give max result","93e1223a":"**Fix long right tales**","e0a4b312":"Testing shows that getting rid of 21 features does not lead to a significant deterioration in quality.\n* RMSE without considering categorical features on test data set:\nbefore - 0.14861, after - 0.15127\n* Model: Default RandomForestRegressor","ebaf3721":"## NA Filling","8ace6ee3":"**Look up our train data**","a7ea5e5b":"Probably we could mix some of these features, but my english skill is not perfect and it was difficult to understand features meaning\n\nyou could experiment with them :)","4a645a6b":"# Let's started! Yoo-hoo ","33f9f340":"do u see long right tale?  It is not ok for our future model, so let's improve it.\n\nI recommend using log function in these situations","1fcefef8":"**First thousands of participants used lifehack to get 0.0000 rmse on submission**\n\nThere is an AmesHousing. csv on Kaggle, in which the same dataset is split differently. As a result, AmesHousing. csv contains some of the answers for our test. These kagglers compare the table of our test and AmesHousing.csv data line by line, and get the result. \nHow this dataset was formed, whether it is a prediction of a well-constructed model, or whether it is the source data , is a mystery.\n\n**Please do not do this, it's not fair, and you won't get the knowledge**","af5c346d":"these parameters I found by GridSearchCV()","c3131bca":"**Look up our test data**","295b4515":"### Look up catecorical features","bb97e996":"Now we can visual our data to understand what we should do next","66829f3f":"### Look up ordinal and binary data","da58a2ce":"## Correlation (Only Int or Float data)","7fd25e97":"I did not use complex ensembles. You can find this in any notebooks. My goal was to use the simplest tools to achieve high result","5e7e6dc5":"## Transform categorical features","8178f6f4":"# Preprocessing data we have","48c670f2":"**Separate target and train data**","6388b107":"**Look up NA cells percent**","394f6a9a":"## Ensemble XGB and LGB","ee84ef49":"I noticed that test data does not have some columns, so need to remove the extra ones in train data","0fe88cbb":"## Feature Engineering","e6804f0b":"### Look up numerical features","0eb03f7d":"Oh, this data have so much nan values and a lot of categorical features.\nI guess, this analysis will pretty interesting","46ddc450":"## Union one encoding categorical features and highest correlation features","e71bc05f":"# Submission","2af7702b":"## Impotance note","2be84f2a":"**Good result but it is not enough to reach 10%**","b536037e":"## LGB","ceedd3b9":"## Impotance Note","91176cd1":"**Let's know more about our data**","52e27ae9":"encode_and_bind() work as \"One Hot Encoding\" for all the data","7161c936":"I am new, and i don't use anything difficult, so i think this notebook will useful, and u rate my work","2b3f7f42":"You can continue work with **highest_corr_features** and check what happens :)\n\nSpoiler:\nYour data set become a little (about 25 features) but its score will not be much different from full dataset","40e0cc0f":"Here you can again check correlation and delete low correlate features\n\nI made the threshold equal to 0.00","b2ea2993":"# Creating model","200af264":"and visual after this","dde66571":"## Data Visualization","1bc5d6ed":"## XGBoost","8cb89535":"But every 0.000001 is impotant on kaggle's competitions so i continue to work with full data set","edf17aa5":"**\u0421onverting categorical features with numerical values to strings**","d6b5090e":"These features exist in naturally occurring ordered categories.\nLet's represent it with numbers","b487e176":"### Delete one of features that have the same meaning (Priority is more correlation with target feature)","6ce8b236":"**Create feature's list**","c3b99e1e":"Drop 'Id' because this feature don't give us any information","a2d0c628":"**Fill NA cells using documentation and mode**","22f0f796":"Look up target feature","6977ce08":"**Almost all features have long right tail.\nI am going to fix it later.**"}}