{"cell_type":{"cc0efdfb":"code","aedc8038":"code","e482326a":"code","1f2c3c3d":"code","7a9c80c8":"code","bb7244e7":"code","d164fa7e":"code","28dcf6b5":"code","b866fe33":"code","1ba85a99":"code","61e7f88c":"code","3b09b891":"code","6a0f7892":"code","4f6505db":"code","cf244e07":"code","8424a27f":"code","5a244999":"code","1c4eaf31":"code","6c525118":"code","a0222dfe":"code","9801b454":"code","aa0efae3":"code","68596a5a":"code","fbae0666":"code","056a02a4":"code","41dbb454":"code","d56bd0e8":"code","b895a4c7":"code","c93feca2":"code","39e6e62a":"code","f132b394":"code","ee4ea8aa":"code","5faa389b":"code","3302253d":"code","3925740b":"code","47afcf16":"code","1ccbf4e1":"code","c2227c49":"code","d4f718a6":"code","24bfd089":"code","6e55b156":"code","514b1032":"code","5a3972f9":"code","9f43b21c":"code","88d7d0ce":"code","27747c87":"code","9eacf5f8":"code","1729ccac":"code","80739eeb":"code","eabb607a":"code","5085cd8f":"code","2b703cfb":"code","071cbbf3":"code","017b77e2":"code","98eac5e4":"markdown","e8bafaac":"markdown","9aafe999":"markdown","3b941308":"markdown","3eb1f647":"markdown","ecde0c72":"markdown","fb9225eb":"markdown","1267fcf4":"markdown","40492431":"markdown","b1d28615":"markdown","e0440172":"markdown","4b8a756b":"markdown","1454e867":"markdown","f750e5ec":"markdown","9510134f":"markdown","a1923a4d":"markdown","4be39a6f":"markdown","8ea4e790":"markdown","323918d9":"markdown"},"source":{"cc0efdfb":"import os\nfrom os.path import join\n\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nimport tensorflow as tf\nimport keras\nfrom keras import layers, Input, models\nfrom keras.utils import to_categorical\nfrom keras.wrappers.scikit_learn import KerasClassifier \nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n\ndatapath = join('data', 'wafer')\n\nprint(os.listdir(\"..\/input\"))\nimport warnings\nwarnings.filterwarnings(\"ignore\")","aedc8038":"df=pd.read_pickle(\"..\/input\/LSWMD.pkl\")\ndf.info()","e482326a":"df.head()","1f2c3c3d":"df.tail()","7a9c80c8":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n\nuni_Index=np.unique(df.waferIndex, return_counts=True)\nplt.bar(uni_Index[0],uni_Index[1], color='gold', align='center', alpha=0.5)\nplt.title(\" wafer Index distribution\")\nplt.xlabel(\"index #\")\nplt.ylabel(\"frequency\")\nplt.xlim(0,26)\nplt.ylim(30000,34000)\nplt.show()","bb7244e7":"df = df.drop(['waferIndex'], axis = 1)","d164fa7e":"def find_dim(x):\n    dim0=np.size(x,axis=0)\n    dim1=np.size(x,axis=1)\n    return dim0,dim1\ndf['waferMapDim']=df.waferMap.apply(find_dim)\ndf.sample(5)","28dcf6b5":"sub_df = df.loc[df['waferMapDim'] == (26, 26)]\nsub_wafer = sub_df['waferMap'].values\n\nsw = np.ones((1, 26, 26))\nlabel = list()\n\nfor i in range(len(sub_df)):\n    # skip null label\n    if len(sub_df.iloc[i,:]['failureType']) == 0:\n        continue\n    sw = np.concatenate((sw, sub_df.iloc[i,:]['waferMap'].reshape(1, 26, 26)))\n    label.append(sub_df.iloc[i,:]['failureType'][0][0])","b866fe33":"x = sw[1:]\ny = np.array(label).reshape((-1,1))","1ba85a99":"mask_x = np.zeros((24, 24))\ndummy_x = cv2.resize(x[0], (24,24))\nmask_x[dummy_x == 1] = 1 \nmask_x[dummy_x == 2] = 1 \nmask_x = mask_x.reshape((1, 24,24))","61e7f88c":"# check dimension\nprint('x shape : {}, y shape : {}'.format(x.shape, y.shape))","3b09b891":"# plot 1st data\nplt.imshow(x[0])\nplt.show()\n\n# check faulty case\nprint('Faulty case : {} '.format(y[0]))","6a0f7892":"#add channel\nx = x.reshape((-1, 26, 26, 1))","4f6505db":"faulty_case = np.unique(y)\nprint('Faulty case list : {}'.format(faulty_case))","cf244e07":"faulty_case_dict =dict()","8424a27f":"for i, f in enumerate(faulty_case) :\n    print('{} : {}'.format(f, len(y[y==f])))\n    faulty_case_dict[i] = f","5a244999":"# One-hot-Encoding faulty categorical variable as channel\nnew_x = np.zeros((len(x), 26, 26, 3))\n\nfor w in range(len(x)):\n    for i in range(26):\n        for j in range(26):\n            new_x[w, i, j, int(x[w, i, j])] = 1","1c4eaf31":"#check new x dimension\nnew_x.shape","6c525118":"# parameter\nepoch=15\nbatch_size=1024","a0222dfe":"# Encoder\ninput_shape = (26, 26, 3)\ninput_tensor = Input(input_shape)\nencode = layers.Conv2D(64, (3,3), padding='same', activation='relu')(input_tensor)\n\nlatent_vector = layers.MaxPool2D()(encode)\n\n# Decoder\ndecode_layer_1 = layers.Conv2DTranspose(64, (3,3), padding='same', activation='relu')\ndecode_layer_2 = layers.UpSampling2D()\noutput_tensor = layers.Conv2DTranspose(3, (3,3), padding='same', activation='sigmoid')\n\n# connect decoder layers\ndecode = decode_layer_1(latent_vector)\ndecode = decode_layer_2(decode)\n\nae = models.Model(input_tensor, output_tensor(decode))\nae.compile(optimizer = 'Adam',\n              loss = 'mse',\n             )","9801b454":"ae.summary()","aa0efae3":"# start train\nae.fit(new_x, new_x,\n       batch_size=batch_size,\n       epochs=epoch,\n       verbose=2)","68596a5a":"# Make encoder model with part of autoencoder model layers\nencoder = models.Model(input_tensor, latent_vector)","fbae0666":"# Make decoder model with part of autoencoder model layers\ndecoder_input = Input((13, 13, 64))\ndecode = decode_layer_1(decoder_input)\ndecode = decode_layer_2(decode)\n\ndecoder = models.Model(decoder_input, output_tensor(decode))","056a02a4":"# Encode original faulty wafer\nencoded_x = encoder.predict(new_x)","41dbb454":"# Add noise to encoded latent faulty wafers vector.\nnoised_encoded_x = encoded_x + np.random.normal(loc=0, scale=0.1, size = (len(encoded_x), 13, 13, 64))","d56bd0e8":"# check original faulty wafer data\nplt.imshow(np.argmax(new_x[3], axis=2))","b895a4c7":"# check new noised faulty wafer data\nnoised_gen_x = np.argmax(decoder.predict(noised_encoded_x), axis=3)\nplt.imshow(noised_gen_x[3])","c93feca2":"# check reconstructed original faulty wafer data\ngen_x = np.argmax(ae.predict(new_x), axis=3)\nplt.imshow(gen_x[3])","39e6e62a":"# augment function define\ndef gen_data(wafer, label):\n    # Encode input wafer\n    encoded_x = encoder.predict(wafer)\n    \n    # dummy array for collecting noised wafer\n    gen_x = np.zeros((1, 26, 26, 3))\n    \n    # Make wafer until total # of wafer to 2000\n    for i in range((2000\/\/len(wafer)) + 1):\n        noised_encoded_x = encoded_x + np.random.normal(loc=0, scale=0.1, size = (len(encoded_x), 13, 13, 64)) \n        noised_gen_x = decoder.predict(noised_encoded_x)\n        gen_x = np.concatenate((gen_x, noised_gen_x), axis=0)\n    # also make label vector with same length\n    gen_y = np.full((len(gen_x), 1), label)\n    \n    # return date without 1st dummy data.\n    return gen_x[1:], gen_y[1:]","f132b394":"# Augmentation for all faulty case.\nfor f in faulty_case : \n    # skip none case\n    if f == 'none' : \n        continue\n    \n    gen_x, gen_y = gen_data(new_x[np.where(y==f)[0]], f)\n    new_x = np.concatenate((new_x, gen_x), axis=0)\n    y = np.concatenate((y, gen_y))","ee4ea8aa":"print('After Generate new_x shape : {}, new_y shape : {}'.format(new_x.shape, y.shape))","5faa389b":"for f in faulty_case :\n    print('{} : {}'.format(f, len(y[y==f])))","3302253d":"# choice index without replace.\nnone_idx = np.where(y=='none')[0][np.random.choice(len(np.where(y=='none')[0]), size=11000, replace=False)]","3925740b":"# delete choiced index data.\nnew_x = np.delete(new_x, none_idx, axis=0)\nnew_y = np.delete(y, none_idx, axis=0)","47afcf16":"print('After Delete \"none\" class new_x shape : {}, new_y shape : {}'.format(new_x.shape, new_y.shape))","1ccbf4e1":"for f in faulty_case :\n    print('{} : {}'.format(f, len(new_y[new_y==f])))","c2227c49":"# make string label data to numerical data\nfor i, l in enumerate(faulty_case):\n    new_y[new_y==l] = i\n    \n# one-hot-encoding\nnew_y = to_categorical(new_y)","d4f718a6":"# split data train, test\nx_train, x_test, y_train, y_test = train_test_split(new_x, new_y,\n                                                    test_size=0.33,\n                                                    random_state=2019)","24bfd089":"print('Train x : {}, y : {}'.format(x_train.shape, y_train.shape))\nprint('Test x: {}, y : {}'.format(x_test.shape, y_test.shape))","6e55b156":"input_shape = (26, 26, 3)\ninput_tensor = Input(input_shape)\n\ndef create_model():\n    global input_tensor\n    \n    conv = layers.Conv2D(32, (3,3), activation='relu', name='conv1')(input_tensor)\n    padding = layers.ZeroPadding2D(padding=(1, 1), name='padding1')(conv)\n    conv = layers.Conv2D(64, (3,3), activation='relu', name='conv2')(padding)\n    padding = layers.ZeroPadding2D(padding=(1, 1), name='padding2')(conv)\n    conv = layers.Conv2D(128, (3,3), activation='relu', name='conv3')(padding)\n    padding = layers.ZeroPadding2D(padding=(1, 1), name='padding3')(conv)\n    conv_out = layers.Conv2D(256, (3,3), activation='relu', name='conv4')(padding)\n\n    gap_layer = layers.GlobalAveragePooling2D(name='GAP')\n    output_layer = layers.Dense(9, activation='softmax', name='output')\n\n    aver_pool = gap_layer(conv_out)\n    output_tensor = output_layer(aver_pool)\n\n    model = models.Model(input_tensor, output_tensor)\n\n    model.compile(optimizer='Adam',\n             loss='categorical_crossentropy',\n             metrics=['accuracy'])\n\n    return model","514b1032":"# Make keras model to sklearn classifier.\nmodel = KerasClassifier(build_fn=create_model, epochs=10, batch_size=1024, verbose=2) \n# 3-Fold Crossvalidation\nkfold = KFold(n_splits=3, shuffle=True, random_state=2019) \nresults = cross_val_score(model, x_train, y_train, cv=kfold)\n# Check 3-fold model's mean accuracy\nprint('Class Activation Map Cross validation score : {:.4f}'.format(np.mean(results)))","5a3972f9":"history = model.fit(x_train, y_train,\n         validation_data=[x_test, y_test],\n         epochs=50,\n         batch_size=batch_size,\n         )","9f43b21c":"# accuracy plot \nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# loss plot\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","88d7d0ce":"#set target wafer number\ntarget_wafer_num = 11\n# predict \nprob = model.model.predict(x_test[target_wafer_num].reshape(1,26,26,3))","27747c87":"aver_output = model.model.layers[7]\naver_model = models.Model(input_tensor, aver_output.output)\ncam_result = aver_model.predict(x_test[target_wafer_num].reshape(1, 26, 26, 3))","9eacf5f8":"weight_result = model.model.layers[-1].get_weights()[0]","1729ccac":"def make_cam(cam_result, weight_result): \n    cam_arr = np.zeros((1,24, 24))\n    for row in range(0,9):\n        cam = np.zeros((1, 24, 24))\n        for i, w in enumerate(weight_result[:, row]):\n            cam += (w*cam_result[0,:,:,i]).reshape(24,24)\n        cam = (cam - np.min(cam)) \/ (np.max(cam) - np.min(cam))\n        cam[mask_x == 0] = 0\n        cam_arr = np.concatenate((cam_arr, cam))\n    return cam_arr[1:]\n\ndef display_activation(cam_arr, prob, wafer): \n    fig, ax = plt.subplots(9, 1, figsize=(50, 50))\n    count = 0\n    cam_arr[np.percentile(cam_arr, 0.8) > cam_arr] = 0\n    for row in range(0,9):\n        ax[row].imshow(np.argmax(wafer, axis=2))\n        ax[row].imshow(cam_arr[row],cmap='Reds', alpha=0.7)\n        ax[row].set_title('class : ' + faulty_case_dict[count]+', prob : {:.4f}'.format(prob[:, count][0]*100) + '%')\n        count += 1","80739eeb":"faulty_case_dict[np.argmax(y_test[11])]","eabb607a":"plt.imshow(np.argmax(x_test[target_wafer_num], axis=2))\nprint('faulty case : {}'.format(faulty_case_dict[np.argmax(y_test[target_wafer_num])]))","5085cd8f":"cam_result.shape","2b703cfb":"cam_arr = make_cam(cam_result, weight_result)","071cbbf3":"prob.shape","017b77e2":"display_activation(cam_arr, prob, x_test[target_wafer_num])","98eac5e4":"plot 1st data for check.","e8bafaac":"## Additional changes\nI have used CNN as classifier, but this kernel use CAM(Class Activation Map) as classifier and Activation feature map. \nMost of them are same, Little different at Network. and than we can see how CAM activate.\n\n\n## Introduction\nHi, It's my first dataset kernel. <br>\nThis kernel forked from WM-811k Wafermap[https:\/\/www.kaggle.com\/ashishpatel26\/wm-811k-wafermap].<br>\nThis dataset has various wafer resolution with class imbalanced. so I just consider specific subset wafer that has 26x26 resolution.<br> \nand solve imbalance problem using 2D convolutional autoencoder. then, classfy faulty case labels.","9aafe999":"## Convolutional Autoencoder for augmentation.\nAs solving class imbalanced problem, we need for data augmentation. <br>\nThe wafer data is image data. so we use convolutional autoencoder.","3b941308":"## Get sub wafer with specific resolution.\nget wafers have (26, 26) resolution. rarrange wafer nd-array with fautly case label.<br>\nsome wafer has null label, skip it.","3eb1f647":"We will use 2D Convolutional Autoencoder, extend dimension for channel.","ecde0c72":">Target distribution","fb9225eb":"## Data augmentation\nWe made convolutional autoencoder model for data augmentation.<br>\nI just want data has 2000 samples for each case. Let's augment data for all faulty case.","1267fcf4":"Wafer data's each pixels have a categorical variable that express 0 : not wafer, 1 : normal, 2 : faulty. <br>\nExtend extra dimension with one-hot-encoded categorical data as channel. <br>\n**that idea from Data Science & Business Analytics Lab, School of Industrial Management Engineering, College of Engineering, Korea University**[http:\/\/dsba.korea.ac.kr\/main]","40492431":"Make faulty case list, and check how classes imbalanced.","b1d28615":"Check summary","e0440172":"### Cross validate model\nUsing sklearn KFold Cross validation, we validate our simple cnn.","4b8a756b":"* The dataset comprises **811,457 wafer maps**, along with additional information such as **wafer die size**, **lot name** and **wafer index**. \n\n* The training \/ test set were already split by domain experts, but in this kernel we ignore this info and we re-divided the dataset into training set and test set by hold-out mehtod which will be introduced in later section.","1454e867":"Our model seems quite a good model.","f750e5ec":"### Visualize How CAM activates","9510134f":"* The dataset were collected from **47,543 lots** in real-world fab. However, **47,543 lots x 25 wafer\/lot =1,157,325 wafer maps ** is larger than **811,457 wafer maps**. \n\n* Let's see what happened. ","a1923a4d":"## Class Activation Map Model\nThe data is ready. As wafer data is image. simply use cnn for classification.<br>\n\n### Make model\ndefine create model function, because we will validate model with sklearn kfold cross validation.","4be39a6f":"* The figure shows that not all lots have perfect 25 wafer maps and it may caused by **sensor failure** or other unknown problems.\n\n* Fortunately, we do not need wafer index feature in our classification so we can just drop the variable. ","8ea4e790":"* We can not get much information from the wafer map column but we can see the die size for each instance is different. \n\n* We create a new variable **'waferMapDim'** for wafer map dim checking.\n","323918d9":"### Read data"}}