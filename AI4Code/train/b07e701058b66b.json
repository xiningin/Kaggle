{"cell_type":{"ef884ebc":"code","13971d8a":"code","388a313b":"code","2d237f4b":"code","5b96b855":"code","606a403b":"code","dab1e04d":"code","53e977e5":"code","b93bda40":"code","d16472bc":"code","167d778b":"code","22f30407":"code","c19bbda5":"code","a5f2fb29":"markdown","4cc267e9":"markdown","63cbffcc":"markdown","9ce8258b":"markdown","5919d91e":"markdown","c5ea07c1":"markdown","4e85b76f":"markdown","96e593de":"markdown","0ab666ee":"markdown","0f6ba669":"markdown","9c49e2f5":"markdown","74aaa571":"markdown","b3d16115":"markdown","270c92e1":"markdown","da699d56":"markdown","7d4f2a3a":"markdown","3dfe68ba":"markdown","e8a14601":"markdown","f566cf82":"markdown","f8b014d8":"markdown","d5077e14":"markdown"},"source":{"ef884ebc":"import torch\nimport torch.nn as nn            \nimport torch.nn.functional as F          \nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport time\ntorch.manual_seed(101)  ","13971d8a":"Transform = transforms.ToTensor()\ntrain = datasets.MNIST(root='..\/DATA', train=True, download=True, transform=Transform)\ntrain","388a313b":"test = datasets.MNIST(root='..\/DATA', train=False, download=True, transform=Transform)\ntest","2d237f4b":"image, label = train[0]\nprint('Shape:', image.shape, '\\nLabel:', label)","5b96b855":"plt.imshow(image.reshape((28,28)), cmap=\"gray\")","606a403b":"train_loader = DataLoader(train, batch_size=100, shuffle=True)\n\ntest_loader = DataLoader(test, batch_size=500, shuffle=False)","dab1e04d":"class MultilayerPerceptron(nn.Module):\n    def __init__(self, input_size=784, output_size=10, layers=[120,84]):\n        super().__init__()\n        self.d1 = nn.Linear(input_size,layers[0])  #hidden layer 1\n        self.d2 = nn.Linear(layers[0],layers[1])   # hidden layer 2\n        self.d3 = nn.Linear(layers[1],output_size)  # output layer\n    \n    def forward(self,X):\n        X = F.relu(self.d1(X))\n        X = F.relu(self.d2(X))\n        X = self.d3(X)\n        return F.log_softmax(X, dim=1)","53e977e5":"model = MultilayerPerceptron()\nprint(model)","b93bda40":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","d16472bc":"for images, labels in train_loader:\n    print('Initial Batch shape:', images.size())\n    break\nprint('Batch shape after flattening',images.view(100,-1).size())","167d778b":"epochs = 10\ntrain_losses = []\ntest_losses = []\ntrain_correct = []\ntest_correct = []\n\nfor i in range(epochs):\n    trn_corr = 0\n    tst_corr = 0\n    \n    # Run the training batches\n    for b, (X_train, y_train) in enumerate(train_loader):\n        b+=1\n        \n        # Apply the model\n        y_pred = model(X_train.view(100, -1))  # Here we flatten X_train\n        loss = criterion(y_pred, y_train)\n \n        # Calculate the number of correct predictions\n        predicted = torch.max(y_pred.data, 1)[1] # the prediction that has the maximum probability\n        batch_corr = (predicted == y_train).sum()\n        trn_corr += batch_corr\n        \n        # Update parameters\n        optimizer.zero_grad() # reset the gradients after each training step\n        loss.backward() #to trigger backprop\n        optimizer.step() #perform parameter update \n        \n        # Print interim results\n        if b%600 == 0:\n            print(f'epoch: {i:2}  batch: {b:4} [{100*b:6}\/60000] Train loss: {loss.item():10.8f}  Train accuracy: {trn_corr.item()*100\/(100*b):7.3f}%')\n    \n    # Update train loss & accuracy for the epoch\n    train_losses.append(loss)\n    train_correct.append(trn_corr)\n        \n    # Run the testing batches\n    with torch.no_grad():   # don't calculate gradients during testing\n        for b, (X_test, y_test) in enumerate(test_loader):\n\n            # Apply the model\n            y_val = model(X_test.view(500, -1))  # Here we flatten X_test\n\n            # Tally the number of correct predictions\n            predicted = torch.max(y_val.data, 1)[1] \n            tst_corr += (predicted == y_test).sum()\n    \n    # Update test loss & accuracy for the epoch\n    loss = criterion(y_val, y_test)\n    test_losses.append(loss)\n    test_correct.append(tst_corr)","22f30407":"print(f'Test accuracy: {test_correct[-1].item()*100\/10000:.3f}%') # test accuracy for the last epoch","c19bbda5":"plt.subplot(3, 1, 1)\nplt.plot(train_losses, label='training loss')\nplt.plot(test_losses, label='validation loss')\nplt.title('Loss at the end of each epoch')\n\nplt.subplot(3, 1, 3)\nplt.plot([t\/600 for t in train_correct], label='training accuracy')\nplt.plot([t\/100 for t in test_correct], label='validation accuracy')\nplt.title('Accuracy at the end of each epoch')\n\nplt.legend()","a5f2fb29":"## It's time to train the model!\n\nBefore I write a plethora of code for training, let me expalin a few concepts that'll be used. \n* **Epoch** - An epoch is a single pass through our full training data(60,000 images). An epoch consists of training steps, \nwhich is nothing but the number of batches passed to the model until all the traiing data is covered. \n\nIt could be expressed as : `number of training steps = number of training records\/batch_size` , which is 600(60000\/100) in our case. We'll train the model for 10 epochs- the model will see the full training data exactly 10 times.\n\n* **Flattening the image** - Instead of sending the image as a 2D tensor, we flatten it in one-dimension.   ","4cc267e9":"The training loss keeps on decreasing throughout the epochs and we can conclude that our model is definitely learning.\n\nBut to actually guage the performance of our model we'll have to see how well it does on unseen(test) data.","63cbffcc":"Now, we know the basics of the architecture. To understand the working better let's take the example of our use case- image classfication with MNIST.\n\n\n![](https:\/\/i.imgur.com\/7mErE7j.png)","9ce8258b":"* There are a lot of loss functions out there like Binary Cross Entropy, Mean Squared Error, Hinged loss etc. The choice of the loss function depends on the problem at hand and the number of classes. Since we are dealing with a Multi-class classification problem, Pytorch's [CrossEntropyLoss](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.CrossEntropyLoss.html) is our go-to loss function.\n\n* Let us talk about the elephant in the room -- ***the optimizer***\n\n     > Remember, I mentioned that during Backpropogation, we update the weights according to the loss throughout the iterations. We basically try to minimize loss as we move ahead throgh or training. This process is called optimization. Optimizers are algorithms that try to find the optimal way to minimize the loss by nagivating the surface of our loss function. To understand more about this, please read this [article](https:\/\/www.kdnuggets.com\/2020\/12\/optimization-algorithms-neural-networks.html#:~:text=Optimizers%20are%20algorithms%20or%20methods,problems%20by%20minimizing%20the%20function.)\n     \n     We use [Adam](https:\/\/pytorch.org\/docs\/stable\/optim.html) because it's the best optimizer out there, as proven by different experiments in the scientific community. ","5919d91e":"The code for training is a few-lines in Keras. As you can see, in Pytorch it's way more because there are wrappers only for very essential stuff and the rest is left to the user to play with. In Pytorch, the user gets a better control over training and it also clears the fundamentals behind model training which is necessary for beginners.","c5ea07c1":"Each image is made up of 28X28 pixels. The 1 in `torch.Size` stands for the number of channels, since it's a grayscale image there's only one channel. \n\nBefore we go any further, the neural network we will be using is the most basic one. So,let's have a quick introduction.","4e85b76f":"###  Ending Notes\n---\n> We are at the end of this notebook and have successfully trained an image recognition model on MNIST. \n\n> Handwriting recognition from images isn't only limited to MNIST or understanding the basics of Deep Learning - there is a whole field based around it called OCR or Optical Character Recognition.\n\nOCR is very useful in digitalizing handwritten documents and is also used by Google Lens to extract text from images.","96e593de":"* The code is pretty straightforward. In Pytorch there isn't any implementation for the input layer, the input is passed directly into the first hidden layer. However, you'll find the InputLayer in the Keras implementation.\n* The number of neurons in the hidden layers and the number of hidden layers is a parameter that can be played with, to get a better result.\n\n### Defining loss function and the optimizer","0ab666ee":"Pytorch has a very convinent way to load the MNIST data using `datasets.MNIST`. Rather than Data Structures such as Numpy arrays and lists, Deep learning models use a very similar DS called a [Tensor](https:\/\/pytorch.org\/tutorials\/beginner\/examples_tensor\/two_layer_net_tensor.html#:~:text=PyTorch%3A%20Tensors,-A%20fully%2Dconnected&text=A%20PyTorch%20Tensor%20is%20basically,used%20for%20arbitrary%20numeric%20computation.&text=To%20run%20operations%20on%20the,Tensor%20to%20a%20cuda%20datatype.). When compared to arrays tensors are more computationally efficient and can run on GPUs too.\n\nWe will convert our MNIST images into tensors when loading them. There are lots of other transformations that you can do using `torchvision.transforms`like Reshaping, Normalizing etc on your images but we won't need that since MNIST is a very primitive dataset.","0f6ba669":"Now, that we know most of the things, let's dive right into the code.\n\n### Loading data into batches","9c49e2f5":"Image Classification is a subfield of Computer Vision which is in turn a subfield of AI. It deals with uncovering hidden pattern in an image by a Neural Network and classifying it into one of the predefined categories. Image Classification had its Eureka moment back in 2012 when Alexnet won the ImageNet Challenge and since then there has been an exponential growth in the field.\n\nWhile we humans take our ability to easily classify objects surrounding us for granted, the problem is not that easy after all. Several factors like view-point variation, size variation, occlusion(blending of objects with other objects in the image), differences in the direction and source of light make it difficult for machines to classify images correctly. Nonetheless, it is an exciting and growing field and there can't be a better way to learn the basics of image classification than to classify images in MNIST. \nBefore we go any further let's see what is MNIST.\n\n\n![](https:\/\/camo.githubusercontent.com\/01c057a753e92a9bc70b8c45d62b295431851c09cffadf53106fc0aea7e2843f\/687474703a2f2f692e7974696d672e636f6d2f76692f3051493378675875422d512f687164656661756c742e6a7067)\n\n\n>MNIST is a database of handwritten digits. It is the most commonly used dataset for learning Image Recognition. It is labelled in the sense that each image of handwritten digit has the corresponding numeral value attached to it. This helps our Algorithm\/Neural Network to learn which image stands for which number(0-9) and to learn hidden patterns in human writing.","74aaa571":"### Types of MNIST\n---\n\nWhile the handwritten MNIST is the most popular one, there are 6 different extended variations of MNIST:\n\n* [Fashion MNIST](https:\/\/github.com\/zalandoresearch\/fashion-mnist) : This dataset from Zalando Research contains images of 10 classes consisting of clothing apparels and accessories like ankle boot, bag, coat, dress, pullover, sandal, shirt, sneaker, etc. instead of handwritten digits. The images are grayscale just like the original MNIST.\n\n\n* [3D MNIST](https:\/\/www.kaggle.com\/daavoo\/3d-mnist): While the original MNIST has 28X28 grayscale(one channel) images, 3D MNIST has images with 3 channels(vis. Red, Green, Blue) like any other color-image out there.It provides a good way to start with 3D Computer Vision Problems.\n\n\n* [EMNIST](https:\/\/www.nist.gov\/itl\/products-and-services\/emnist-dataset): EMNIST is a set of handwritten letters contrary to MNIST which only has handwritten digits. The structure is pretty much the same as MNIST containing  grayscale 28X28 images.\n\n\n* [ Sign Language MNIST](https:\/\/www.kaggle.com\/datamunge\/sign-language-mnist): It is like EMNIST, in the sense that it has images of sign langauge interpretations of the English alphabets(A-Z).It poses a little more challenging problem of hand gesture recognition and therefore has more useful real-world applications.\n\n\n\n* [Colorectal Histology MNIST](https:\/\/www.kaggle.com\/kmader\/colorectal-histology-mnist): The dataset serves a much more interesting MNIST problem for biologists by focusing on histology tiles from patients with colorectal cancer - affecting colon or rectum in the human body. In particular, the data has 8 different classes of cancerous tissue.\n\n\n\n* [Skin Cancer MNIST](https:\/\/www.kaggle.com\/kmader\/skin-cancer-mnist-ham10000): It is a medical dataset containig images of skin lessions\/cancers along with their corresponding labels. This dataset was made for 2018 Skin Lesion Detection Challenge. It can be used as a primary dataset for anyone trying to tackle a medical classification problem using deep learning.\n\n","b3d16115":"Predictions are made on our test data after training completes in every epoch. Since our model continually keeps getting better, the test accuracy of the last epoch is the best.\n\nIt will be intuitive and fun to see the progression of loss and accuracy through the epochs.\n","270c92e1":"* From the 60,000 training records, our images would be sent in batches of 100 through 600 iterations. \n* For training, setting a smaller batch size will enable the model to update the weights more often and learn better, but there's a [caveat](https:\/\/datascience.stackexchange.com\/questions\/72922\/does-small-batch-size-improve-the-model#:~:text=It%20could%20lead%20us%20to,the%20model%20to%20convergence%20anywhere.) here with smaller batch sizes. This is a hyperparameter that could be tuned, I would suggest you to try smaller and larger batch sizes than 100 and see the results.\n* During testing, no learning or flow of gradients take place. So you can keep the batch size as big as can fit in your RAM.\n* Setting shuffle to True means that the dataset will be shuffled after each epoch.\n","da699d56":"## Image Classification with MNIST\n---","7d4f2a3a":"### Time to define our model !\n","3dfe68ba":"## Multilayer Perceptron\n---\n* A multilayer perceptron has several Dense layers of neurons in it, hence the name multi-layer. \n* These artificial neurons\/perceptrons are the fundamental unit in a neural network, quite analogous to the biological neurons in the human brain. The computation happening in a single neuron can be denoted by the equation. **`N = Wx + b`** , where x denotes the input to that neuron and W,b stand for weight and bias respectively. These two values are set at random initially and then keep on updating as the network learns.\n                                                                   \n* Each neuron in a layer is connected to every other neuron in it's next layer. In MLPs, data only flows forwards hence they are also sometimes called Feed Forward Networks.\n\nThere are 3 basic components:\n1. **Input Layer**- The input layer would take in the input signal to be processed. In our case it's a tensor of image pixels.\n2. **Output Layer**- The output layer does the required task of classification\/regression. In our case it outputs one of the 10 classes for digits 0-9 for a given input image.\n3. **Hidden Layers**- There are an arbitary number of hidden layers in between the input and output layer that do all the computations in a Multilayer Perceptron. The number of hidden layers and the number of neurons can be decided keeping in mind the fact that one layer's output is next layer's input. \n","e8a14601":"I'll try to break down the process in different steps:\n1. The pixels in the 28X28 handwritten digit image are flattened to form an array of 784 pixel values. Nothing heavy going on here, just decompressing a 2D array into one dimension.\n2. The function of the input layer is just to pass-on the input (array of 784 pixels) into the first hidden layer.\n3. The first hidden layer is where the computations start. It has 120 neurons that are each fed the input array. After calculating the result from the formula stated above, each neuron generates an output that is fed into each neuron of the next layer. Except, there is a little twist here. Instead of just simply passing on the result of `Wx+b`, an activation is calculated on this result. \n > The activation function is used to clip the output in a definite range like 0-1 or -1 to 1, these ranges can be achieved by _Sigmoid_ and _Tanh_ respectively. The activation function we have used here is ReLu. It has several advantages over other functions that can be read in depth [here](https:\/\/www.analyticsvidhya.com\/blog\/2020\/01\/fundamentals-deep-learning-activation-functions-when-to-use-them\/#:~:text=The%20main%20advantage%20of%20using,neurons%20at%20the%20same%20time.&text=Due%20to%20this%20reason%2C%20during,neurons%20which%20never%20get%20activated.). \n \n![](https:\/\/miro.medium.com\/max\/357\/1*oePAhrm74RNnNEolprmTaQ.png)\n\n\n > In short, Relu clips all the negative values and keeps the positive values just the same.\n \n4. The same thing happens in the second hidden layer. It has 84 neurons and takes 120 inputs from the previous layer. The output of this layer is fed into the last layer which is the Output Layer.\n5. The Output Layer has only 10 neurons for the 10 classes that we have(digits between 0-9). There isn't any acctivation function in the output layer because we'll apply another function later.\n6. The Softmax takes the output of the last layer(called logits) which could be any 10 real values and converts it into another 10 real values that sum to 1. Softmax transform the values between 0 and 1 , such that they can be interpreted as probabilities. The maximum value pertains to the class predicted by the classifier. In our case the value is 0.17 and the class is 5.\n\n![](https:\/\/jamesmccaffrey.files.wordpress.com\/2016\/03\/softmaxequation.jpg)\n\nThe process described above is a single forward pass through the network and instead of just sending one image as input in a pass, a batch of images is fed in a single pass. \n\nBut how does the network learn? \n\nAfter a single pass through the network, the prediction of the model for that batch of images is compared with the actual labels of those images and a loss is calculated. Based on the value of this loss, a gradient flows backwards through the neural network to update weights(W and b) in each layer. This process is called [Backpropogation](https:\/\/medium.com\/@karpathy\/yes-you-should-understand-backprop-e2f06eab496b)\n\nIn the next iteration the neural network would do a slightly better job while predicting. This process of forward-pass and back propogation keeps on repeating as we try to minimize our loss and we the end of our training.","f566cf82":"Please don't forget to upvote the kernel, if you found it useful!","f8b014d8":"### Let's get our hands dirty\n\nWhile MNIST is also availabe in the [CSV](https:\/\/www.kaggle.com\/oddrationale\/mnist-in-csv?select=mnist_train.csv) format, for the purpose of this notebook we'll use the original MNIST in ubyte.\n\nThere are a lot of Deep Learning Frameworks out there that you can use like Tensorflow, Keras, Mxnet, Pytorch. \n\nWe'll be using Pytorch because the code is more Python-like \ud83d\udc0d and the implementation of the Neural Network is not hidden behind layers of abstraction. So basically, coding ends up being more intuitive.\n\nOkay, time to load some libraries we will be needing.","d5077e14":"The train data has 60,000 images and test has 10,000. Let's take a look at one."}}