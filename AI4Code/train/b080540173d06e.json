{"cell_type":{"4066332f":"code","a90aa0b6":"code","df1b9cf7":"code","209d5242":"code","a44bad0e":"code","2bdc2321":"code","e850c996":"code","84c080b6":"code","2e00e474":"code","38088089":"code","eb6a1f9c":"code","b62a8514":"code","703702c2":"code","0dd20fa1":"code","c79833d5":"code","5a643c70":"code","814e4443":"code","cc9cbfce":"code","c2048a41":"code","da3cadf6":"markdown","f4d63bc4":"markdown","ed8cceec":"markdown"},"source":{"4066332f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Import the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a90aa0b6":"# Import the dataset\ndataset = pd.read_json('..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json', lines= True)","df1b9cf7":"# Lets see how our data looks\ndataset.head()","209d5242":"# Drop article_link column\ndataset = dataset.drop(axis= 1, columns= 'article_link')","a44bad0e":"# Lets verify if the column was dropped\ndataset.head()","2bdc2321":"# Cleaning the texts\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","e850c996":"# Lets first get the string in each headline and then remove stop words using PorterStemmer\ncorpus = []\nfor i in range(0,26709):\n    headline = re.sub('[^a-zA-Z]', ' ', dataset['headline'][i]) \n    headline = headline.lower()\n    headline = headline.split()\n    porterStemmer = PorterStemmer()\n    headline = [porterStemmer.stem(word) for word in headline if not word in set(stopwords.words('english'))]\n    headline = ' '.join(headline)\n    corpus.append(headline)","84c080b6":"corpus[0]","2e00e474":"# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500)\nX = cv.fit_transform(corpus).toarray()\ny = dataset.iloc[:, 1].values","38088089":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","eb6a1f9c":"# Fit Logistic Regression Model to our training dataset\nfrom sklearn.linear_model import LogisticRegression\nregressor = LogisticRegression()\nregressor.fit(X_train, y_train)","b62a8514":"# Make predictions\ny_pred = regressor.predict(X_test)","703702c2":"# Analyze the results\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, y_pred)","0dd20fa1":"print(f'Accuracy with Logistic Regression model is {accuracy*100} %')","c79833d5":"# Lets analyze the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)","5a643c70":"cm","814e4443":"# Lets check the precision\nprecision = cm[0][0]\/sum(cm[0])\nprint(f'Precision for the model is {precision*100}')","cc9cbfce":"# Now lets check the recall\nrecall = cm[0][0]\/(cm[0][0]+cm[1][0])\nprint(f'Recall for the model is {recall*100}')","c2048a41":"# Finally!, lets see the F1-Score\nf1_score = 2 * ((precision*recall)\/(precision+recall))\nprint(f'F1-Score :- {f1_score}')","da3cadf6":"**NOW Lets apply some NLP Techniques **","f4d63bc4":"Hurray!!, we have actually removed stop words from the text","ed8cceec":"**Further Improvements** :- Getting optimum values of parameters for LogisticRegression Model to enhance the performance or try out different model with the dataset and analyze the results.!!"}}