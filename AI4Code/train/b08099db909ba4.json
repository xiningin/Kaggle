{"cell_type":{"27f8d7c5":"code","67a00229":"code","6dfdc068":"code","99a91ea6":"code","f064dad3":"code","7e25d785":"code","6192d8c8":"code","d692fd68":"code","8783693c":"code","72253909":"code","df87dd02":"code","d5bedf94":"code","3d9244f6":"code","f0082bde":"code","11dcbbbc":"code","49cb4812":"code","26c59098":"code","47a39d43":"code","b8eae63a":"code","f015137d":"code","64aa85e2":"code","407ecd8a":"markdown","cf78a4c4":"markdown","635df724":"markdown","8c873a58":"markdown","fed07548":"markdown","6de8567b":"markdown","5c41ea81":"markdown","af0cd7a6":"markdown","7ab32774":"markdown","68b5898c":"markdown","f6164a4e":"markdown","8bebae98":"markdown","a69df718":"markdown","9d551d62":"markdown","be91b45f":"markdown","dadf1247":"markdown","6f7743d9":"markdown","5e15750d":"markdown","4a69e44d":"markdown","2bad4076":"markdown","a51bef5e":"markdown","168bd252":"markdown"},"source":{"27f8d7c5":"!pip install 'kaggle-environments==0.1.6' > \/dev\/null 2>&1","67a00229":"import numpy as np\nimport gym\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom kaggle_environments import evaluate, make","6dfdc068":"# \u57fa\u672c\u7684\u306aConnectX\u306e\u74b0\u5883\u4f5c\u6210\u30af\u30e9\u30b9\n\nclass ConnectX(gym.Env):\n    def __init__(self, switch_prob=0.5):\n        self.env = make('connectx', debug=False)\n        self.pair = [None, 'random']\n        self.trainer = self.env.train(self.pair)\n        self.switch_prob = switch_prob\n\n        # Define required gym fields (examples):\n        config = self.env.configuration\n        self.action_space = gym.spaces.Discrete(config.columns) #\u30a2\u30af\u30b7\u30e7\u30f3\u3092\u884c\u3046\u7bc4\u56f2\uff08\u96e2\u6563\u5024\uff1a7\uff09\n        self.observation_space = gym.spaces.Discrete(config.columns * config.rows) #\u53d6\u308a\u5f97\u308b\u72b6\u614b\uff086\u884c7\u5217=42\uff09\n\n    def switch_trainer(self):\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)\n\n    def step(self, action):\n        return self.trainer.step(action)\n    \n    def reset(self):\n        if np.random.random() < self.switch_prob:\n            self.switch_trainer()\n        return self.trainer.reset()\n    \n    def render(self, **kwargs):\n        return self.env.render(**kwargs)\n    ","99a91ea6":"# \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u69cb\u7bc9\u30af\u30e9\u30b9\n# Input\u306f\uff08\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\uff08\u884c\uff09 \u00d7 \u72b6\u614b\uff0842\u5217\uff09\uff09\u3001output\u306f\uff08\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\uff08\u884c\uff09 \u00d7 \u884c\u52d5\uff087\u5217\uff09\uff09\n\nclass DeepModel(tf.keras.Model):\n    def __init__(self, num_states, hidden_units, num_actions):\n        super(DeepModel, self).__init__()\n        self.input_layer = tf.keras.layers.InputLayer(input_shape=(num_states,))\n        self.hidden_layers = []\n        for i in hidden_units:\n            self.hidden_layers.append(tf.keras.layers.Dense(\n                i, activation='sigmoid', kernel_initializer='RandomNormal'))\n        self.output_layer = tf.keras.layers.Dense(\n            num_actions, activation='linear', kernel_initializer='RandomNormal')\n\n#     @tf.function\n    def call(self, inputs):\n        z = self.input_layer(inputs)\n        for layer in self.hidden_layers:\n            z = layer(z)\n        output = self.output_layer(z)\n        return output\n","f064dad3":"class DQN:\n    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n        self.num_actions = num_actions\n        self.batch_size = batch_size\n        self.optimizer = tf.optimizers.Adam(lr)\n        self.gamma = gamma\n        self.model = DeepModel(num_states, hidden_units, num_actions)\n        self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []} # The buffer\n        self.max_experiences = max_experiences\n        self.min_experiences = min_experiences\n\n    def predict(self, inputs):\n        # state\u304b\u3089\u884c\u52d5\u3092\u4e88\u6e2c\u3002batch size\u306estate\u3082single state\u306b\u3082\u5bfe\u5fdc\u3059\u308b\u305f\u3081\u306bnp.atleast_2d\u3092\u5229\u7528\n        # np.atleast_2d:[1, -1]\u306breshape\u3059\u308b\u3084\u3064\n        # arr = np.arange(0, 5) print(np.atleast_2d(arr)) [[0 1 2 3 4]]\n        return self.model(np.atleast_2d(inputs.astype('float32')))\n\n#     @tf.function\n    def train(self, TargetNet):\n        if len(self.experience['s']) < self.min_experiences:\n            # Only start the training process when we have enough experiences in the buffer\n            return 0\n\n        # Randomly select n experience in the buffer, n is batch-size\n        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n        states = np.asarray([self.preprocess(self.experience['s'][i]) for i in ids])\n        actions = np.asarray([self.experience['a'][i] for i in ids])\n        rewards = np.asarray([self.experience['r'][i] for i in ids])\n\n        # Prepare labels for training process\n        states_next = np.asarray([self.preprocess(self.experience['s2'][i]) for i in ids])\n        dones = np.asarray([self.experience['done'][i] for i in ids])\n        value_next = np.max(TargetNet.predict(states_next), axis=1)\n        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n        \n        # \u640d\u5931\u95a2\u6570\u306e\u5b9a\u7fa9\n        with tf.GradientTape() as tape:\n            selected_action_values = tf.math.reduce_sum(\n                self.predict(states) * tf.one_hot(actions, self.num_actions), axis=1)\n            loss = tf.math.reduce_sum(tf.square(actual_values - selected_action_values))\n        variables = self.model.trainable_variables\n        # \u52fe\u914d\u3092\u5b9a\u7fa9\n        gradients = tape.gradient(loss, variables)\n        # \u52fe\u914d\u3092\u6700\u5c0f\u5316\n        self.optimizer.apply_gradients(zip(gradients, variables))\n    \n\n    # Get an action by using epsilon-greedy\n    def get_action(self, state, epsilon):\n        if np.random.random() < epsilon:\n            return int(np.random.choice([c for c in range(self.num_actions) if state.board[c] == 0]))\n        else:\n            prediction = self.predict(np.atleast_2d(self.preprocess(state)))[0].numpy()\n            for i in range(self.num_actions):\n                if state.board[i] != 0:\n                    prediction[i] = -1e7\n            return int(np.argmax(prediction))\n\n    # Method used to manage the buffer\n    def add_experience(self, exp):\n        if len(self.experience['s']) >= self.max_experiences:\n            for key in self.experience.keys():\n                self.experience[key].pop(0)\n        for key, value in exp.items():\n            self.experience[key].append(value)\n\n    def copy_weights(self, TrainNet):\n        variables1 = self.model.trainable_variables\n        variables2 = TrainNet.model.trainable_variables\n        for v1, v2 in zip(variables1, variables2):\n            v1.assign(v2.numpy())\n\n    def save_weights(self, path):\n        self.model.save_weights(path)\n\n    def load_weights(self, path):\n        ref_model = tf.keras.Sequential()\n\n        ref_model.add(self.model.input_layer)\n        for layer in self.model.hidden_layers:\n            ref_model.add(layer)\n        ref_model.add(self.model.output_layer)\n\n        ref_model.load_weights(path)\n    \n    # Each state will consist of the board and the mark\n    # in the observations\n    def preprocess(self, state):\n        result = state.board[:]\n        result.append(state.mark)\n\n        return result","7e25d785":"def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n    rewards = 0\n    iter = 0\n    done = False\n    observations = env.reset()\n    while not done:\n        # epsilon-greedy\u3067\u884c\u52d5\u3092\u9078\u629e\n        action = TrainNet.get_action(observations, epsilon)\n\n        # \u524d\u306e\u72b6\u614b\n        prev_observations = observations\n\n        # env.step\u3067\u5831\u916c\u30fb\u6b21\u306e\u72b6\u614b\u30fb\u30b2\u30fc\u30e0\u7d42\u4e86\u30d5\u30e9\u30b0\u304c\u5f97\u3089\u308c\u308b\n        observations, reward, done, _ = env.step(action)\n\n        # Apply new rules\n        if done:\n            if reward == 1: # Won\n                reward = 20\n            elif reward == 0: # Lost\n                reward = -20\n            else: # Draw\n                reward = 10\n        else:\n            reward = -0.05 # \u9577\u304f\u30d7\u30ec\u30a4\u3057\u3059\u304e\u306a\u3044\u3088\u3046\u306b\n\n        rewards += reward\n\n        # \u524d\u306e\u72b6\u614b\u30fb\u884c\u52d5\u30fb\u5831\u916c\u30fb\u4eca\u306e\u72b6\u614b\u30fb\u30b2\u30fc\u30e0\u7d42\u4e86\u30d5\u30e9\u30b0\u3092\u60c5\u5831\u3068\u3057\u3066\u8ffd\u52a0\n        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n        TrainNet.add_experience(exp)\n\n        # \u8ffd\u52a0\u3057\u305f\u60c5\u5831\u3067\u5b66\u7fd2\uff08TrainNet\u3068\u306f\u5225\u306b\u3001\u6b21\u306eQ\u5024\u3092\u4e88\u6e2c\u3059\u308b\u305f\u3081\u306eTargetNet\u3082\u7528\u3044\u308b\uff09\n        TrainNet.train(TargetNet)\n        iter += 1\n        if iter % copy_step == 0:\n            # \u4e00\u5b9a\u56de\u6570\u5b66\u7fd2\u3057\u305f\u3089\u3001TargetNet\u306e\u91cd\u307f\u3092\u66f4\u65b0\u3059\u308b\n            TargetNet.copy_weights(TrainNet)\n    return rewards","6192d8c8":"env = ConnectX()","d692fd68":"gamma = 0.99\ncopy_step = 25\nhidden_units = [100, 200, 200, 100]\nmax_experiences = 10000\nmin_experiences = 100\nbatch_size = 32\nlr = 1e-2\nepsilon = 0.99\ndecay = 0.99999\nmin_epsilon = 0.1\nepisodes = 10000\n\nprecision = 7\n\n# log_dir = 'logs\/'\n# summary_writer = tf.summary.create_file_writer(log_dir)","8783693c":"num_states = env.observation_space.n + 1\nnum_actions = env.action_space.n\n\nall_total_rewards = np.empty(episodes)\nall_avg_rewards = np.empty(episodes) # Last 100 steps\nall_epsilons = np.empty(episodes)\n\n# Initialize models\nTrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\nTargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)","72253909":"pbar = tqdm(range(episodes))\nfor n in pbar:\n    epsilon = max(min_epsilon, epsilon * decay)\n    total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n    all_total_rewards[n] = total_reward\n    avg_reward = all_total_rewards[max(0, n - 100):(n + 1)].mean()\n    all_avg_rewards[n] = avg_reward\n    all_epsilons[n] = epsilon\n\n    pbar.set_postfix({\n        'episode reward': total_reward,\n        'avg (100 last) reward': avg_reward,\n        'epsilon': epsilon\n    })\n\n#     with summary_writer.as_default():\n#         tf.summary.scalar('episode reward', total_reward, step=n)\n#         tf.summary.scalar('running avg reward (100)', avg_reward, step=n)\n#         tf.summary.scalar('epsilon', epsilon, step=n)","df87dd02":"# plt.plot(all_total_rewards)\n# plt.xlabel('Episode')\n# plt.ylabel('Total rewards')\n# plt.show()","d5bedf94":"plt.plot(all_avg_rewards)\nplt.xlabel('Episode')\nplt.ylabel('Avg rewards (100)')\nplt.show()","3d9244f6":"plt.plot(all_epsilons)\nplt.xlabel('Episode')\nplt.ylabel('Epsilon')\nplt.show()","f0082bde":"TrainNet.save_weights('.\/weights.h5')","11dcbbbc":"fc_layers = []\n\n# Get all hidden layers' weights\nfor i in range(len(hidden_units)):\n    fc_layers.extend([\n        TrainNet.model.hidden_layers[i].weights[0].numpy().tolist(), # weights\n        TrainNet.model.hidden_layers[i].weights[1].numpy().tolist() # bias\n    ])\n\n# Get output layer's weights\nfc_layers.extend([\n    TrainNet.model.output_layer.weights[0].numpy().tolist(), # weights\n    TrainNet.model.output_layer.weights[1].numpy().tolist() # bias\n])\n\n# Convert all layers into usable form before integrating to final agent\nfc_layers = list(map(\n    lambda x: str(list(np.round(x, precision))) \\\n        .replace('array(', '').replace(')', '') \\\n        .replace(' ', '') \\\n        .replace('\\n', ''),\n    fc_layers\n))\nfc_layers = np.reshape(fc_layers, (-1, 2))","49cb4812":"fc_layers.shape","26c59098":"# \u4f8b\uff1a\u6700\u521d\u306e\u96a0\u308c\u5c64\u306e\u91cd\u307f\nfc_layers[0][0]","47a39d43":"# Create the agent\nmy_agent = '''def my_agent(observation, configuration):\n    import numpy as np\n\n'''\n\n# Write hidden layers\nfor i, (w, b) in enumerate(fc_layers[:-1]):\n    my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n    my_agent += '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n# Write output layer\nmy_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\nmy_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\n\nmy_agent += '''\n    state = observation.board[:]\n    state.append(observation.mark)\n    out = np.array(state, dtype=np.float32)\n\n'''\n\n# Calculate hidden layers\nfor i in range(len(fc_layers[:-1])):\n    my_agent += '    out = np.matmul(out, hl{0}_w) + hl{0}_b\\n'.format(i+1)\n    my_agent += '    out = 1\/(1 + np.exp(-out))\\n' # Sigmoid function\n# Calculate output layer\nmy_agent += '    out = np.matmul(out, ol_w) + ol_b\\n'\n\nmy_agent += '''\n    for i in range(configuration.columns):\n        if observation.board[i] != 0:\n            out[i] = -1e7\n\n    return int(np.argmax(out))\n    '''","b8eae63a":"with open('submission.py', 'w') as f:\n    f.write(my_agent)","f015137d":"from submission import my_agent","64aa85e2":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) \/ sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate agent's performance.\nprint(\"My Agent vs. Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs. Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))\nprint(\"Random Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", my_agent], num_episodes=10)))\nprint(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=10)))","407ecd8a":"<a class=\"anchor\" id=\"evaluate_the_agent\"><\/a>\n# Evaluate the agent\n[Back to Table of Contents](#ToC)","cf78a4c4":"<a class=\"anchor\" id=\"define_useful_classes\"><\/a>\n# Define useful classes\n\n\u3053\u306enotebook\u3067\u306f\u3001[*Siwei Xu's \u3055\u3093\u306etutorial*](https:\/\/towardsdatascience.com\/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998)\u306e\u5185\u5bb9\u3092\u30d9\u30fc\u30b9\u306bNN\u3092\u7d44\u3093\u3067\u3044\u307e\u3059\n\n> NOTE: I use the neural network in [*Siwei Xu's tutorial\n> *](https:\/\/towardsdatascience.com\/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998) with some proper modifications to adapt to the problem in ConnectX competition and be able to save trained and load pre-trained models.\n> \n\n\n[Back to Table of Contents](#ToC)","635df724":"<a class=\"anchor\" id=\"train_the_agent\"><\/a>\n# Train the agent\n[Back to Table of Contents](#ToC)","8c873a58":"<a class=\"anchor\" id=\"install_libraries\"><\/a>\n# Install libraries\n[Back to Table of Contents](#ToC)","fed07548":"<a class=\"anchor\" id=\"save_weights\"><\/a>\n# Save weights\n[Back to Table of Contents](#ToC)","6de8567b":"- \u5f97\u3089\u308c\u305f\u91cd\u307f\u3092\u6587\u5b57\u5217\u3067\u4fdd\u5b58","5c41ea81":"- evaluate\u95a2\u6570\u3067\u4ed6\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u3068\u6226\u308f\u305b\u305f\u6642\u306e\u8a55\u4fa1\u304c\u78ba\u8a8d\u53ef\u80fd","af0cd7a6":"- \u3053\u306enotebook\u306f\u3001\u5143\u306e\u5049\u5927\u306anotebook\u3092\u65e5\u672c\u8a9e\u306b\u7ffb\u8a33\u3057\u3066\u5b66\u3073\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u306b\u4f5c\u6210\u3057\u307e\u3057\u305f\n(This notebook was created to translate the original great notebook into Japanese to make it easier to learn!)\n- \u305f\u307e\u306b\u3001\u4e0a\u8a18\u30ab\u30fc\u30cd\u30eb\u4ee5\u5916\u306b\u3082\u65e5\u672c\u8a9e\u3067\u9069\u5b9c\u88dc\u8db3\u89e3\u8aac\u3092\u52a0\u3048\u3066\u3044\u307e\u3059\n(Occasionally, supplementary explanations are added in Japanese.)\n- upvote\u306f\u3053\u306enotebook\u3067\u306f\u306a\u304f[\u5143\u306enotebook](https:\/\/www.kaggle.com\/phunghieu\/connectx-with-deep-q-learning)\u306b\u304a\u9858\u3044\u3057\u307e\u3059\n(Please upvote to [original great notebook](https:\/\/www.kaggle.com\/phunghieu\/connectx-with-deep-q-learning), not this notebook. Thanks!)","7ab32774":"- \u3053\u306e\u30ab\u30fc\u30cd\u30eb\u306e\u809d\u306b\u306a\u308bDQN\u30af\u30e9\u30b9","68b5898c":"<a class=\"anchor\" id=\"define_helper_functions\"><\/a>\n# Define helper-functions\n[Back to Table of Contents](#ToC)","f6164a4e":"<a class=\"anchor\" id=\"create_connectx_environment\"><\/a>\n# Create ConnectX environment\n[Back to Table of Contents](#ToC)","8bebae98":"- kaggle\u304c\u7528\u610f\u3057\u3066\u304f\u308c\u3066\u308b\u5f37\u5316\u5b66\u7fd2\u7528\u306e\u74b0\u5883\n- \u516c\u5f0f(https:\/\/github.com\/Kaggle\/kaggle-environments) \u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u66f8\u3044\u3066\u3042\u308a\u307e\u3059\n\n> Kaggle Environments was created to evaluate episodes. While other libraries have set interface precedents (such as Open.ai Gym), the emphasis of this library focuses on:\n> Episode evaluation (compared to training agents).\n> Configurable environment\/agent lifecycles.\n> Simplified agent and environment creation.\n> Cross language compatible\/transpilable syntax\/interfaces.\n\n\u7279\u306b\u300c\u30a8\u30d4\u30bd\u30fc\u30c9\u306e\u8a55\u4fa1\u300d\u3084\u300c\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u3084\u74b0\u5883\u306e\u4f5c\u6210\u3092\u30b7\u30f3\u30d7\u30eb\u306b\u300d\u3059\u308b\u3053\u3068\u3092\u610f\u56f3\u3057\u3066\u4f5c\u3089\u308c\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002<BR>\u307e\u305f\u3001\u4ed6\u306e\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\uff08openai.gym\u306a\u3069\uff09\u3068\u306e\u4e92\u63db\u6027\u3082\u610f\u8b58\u3057\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002","a69df718":"- weight\u3092\u4fdd\u5b58","9d551d62":"# About Reinforcement Learning and Deep Q-Learning\n\n\u5f37\u5316\u5b66\u7fd2\u3068\u306f\u3001\u5831\u916c\u3092\u6700\u5927\u5316\u3059\u308b\u305f\u3081\u306b\u3001\u74b0\u5883\u306e\u4e2d\u304b\u3089\u7279\u5b9a\u306e\u72b6\u614b\u3067\u7279\u5b9a\u306e\u884c\u52d5\u3092\u53d6\u308b\u3088\u3046\u306b\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u3092\u8a13\u7df4\u3059\u308b\u3053\u3068\u306b\u91cd\u70b9\u3092\u7f6e\u3044\u305f\u6a5f\u68b0\u5b66\u7fd2\u306e\u4e00\u5206\u91ce\u3067\u3042\u308b\u3002<BR>\nDQN(Deep Q-Net)\u306f\u3001\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u304c\u5404\u72b6\u614b\u3067\u53d6\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u884c\u52d5\u3092\u898b\u3064\u3051\u308b\u305f\u3081\u306b\u6df1\u5c64\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3059\u308b\u5f37\u5316\u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u3059\u3002\n> \"Reinforcement learning is an area of machine learning that is focused on training agents to take certain actions at certain states from within an environment to maximize rewards. DQN (Deep Q-Net) is a reinforcement learning algorithm where a deep learning model is built to find the actions an agent can take at each state.\" [*Siwei Xu's tutorial\n*](https:\/\/towardsdatascience.com\/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998)","be91b45f":"<a class=\"anchor\" id=\"import_libraries\"><\/a>\n# Import libraries\n[Back to Table of Contents](#ToC)","dadf1247":"<a class=\"anchor\" id=\"ToC\"><\/a>\n# Table of Contents\n1. [Install libraries](#install_libraries)\n1. [Import libraries](#import_libraries)\n1. [Define useful classes](#define_useful_classes)\n1. [Define helper-functions](#define_helper_functions)\n1. [Create ConnectX environment](#create_connectx_environment)\n1. [Configure hyper-parameters](#configure_hyper_parameters)\n1. [Train the agent](#train_the_agent)\n1. [Save weights](#save_weights)\n1. [Create an agent](#create_an_agent)\n1. [Evaluate the agent](#evaluate_the_agent)","6f7743d9":"<a class=\"anchor\" id=\"configure_hyper_parameters\"><\/a>\n# Configure hyper-parameters\n[Back to Table of Contents](#ToC)","5e15750d":"<a class=\"anchor\" id=\"create_an_agent\"><\/a>\n# Create an agent\n[Back to Table of Contents](#ToC)","4a69e44d":" - \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u3092\u4f5c\u308b\n - submit\u306f\u3001\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306e\u52d5\u304d\u3092\u5b9a\u7fa9\u3059\u308b\u95a2\u6570(my_agent)\u3092\u6587\u5b57\u5217\u306b\u3057\u305f.py\u306e\u5f62\u3067\u884c\u3046\n - my_agent\uff1a\u72b6\u614b\u306b\u5408\u308f\u305b\u3066\u3001\u65e2\u306b\u5b66\u7fd2\u3057\u305f\u5c64\u306e\u91cd\u307f\u3092\u639b\u3051\u5408\u308f\u305b\u3066Q\u5024\u3092\u7b97\u51fa\u3057\u3001Q\u5024\u304c\u6700\u5927\u306b\u306a\u308b\u884c\u52d5\u3092\u51fa\u529b","2bad4076":"- game_play\u95a2\u6570\u3092\u7e70\u308a\u8fd4\u3057\u3001Q\u5024\u3092\u5b66\u7fd2\n- \u5f97\u3089\u308c\u305f\u5831\u916c\u3092\u30b0\u30e9\u30d5\u5316","a51bef5e":"- play_game\u95a2\u6570\n    - game\u3092play\u3057\u305f\u5831\u916c\u3092\u8fd4\u3059\n    - \uff08\u3069\u3046play\u3059\u308b\uff1f\uff09\u03b5-greedy\u3068\u3044\u3046\u300c\u30e9\u30f3\u30c0\u30e0\u63a2\u7d22\u3068\u6700\u9069\u884c\u52d5\u3092\u9069\u5b9c\u7e70\u308a\u8fd4\u3059\uff08\u30e9\u30f3\u30c0\u30e0\u884c\u52d5\u306e\u78ba\u7387\u306f\u03b5\u3067\u5b9a\u7fa9\u3057\u3001\u5f90\u3005\u306b\u6e1b\u3089\u3057\u3066\u3044\u304f\uff09\u300d\u884c\u52d5\u3092\u884c\u3044\u305d\u308c\u305e\u308c\u306e\u5831\u916c\u3092\u8fd4\u3059\n    - \uff08\u6700\u9069\u884c\u52d5\u3063\u3066\uff1f\uff09\u6700\u9069\u884c\u52d5\u306fNN(TrainNet\uff09\u3067Q\u5024\u3092\u4e88\u6e2c\u3057\u3066\u9078\u629e\u3055\u308c\u308b\n    - \uff08NN\u306e\u5b66\u7fd2\u306f\u3069\u3046\u3057\u3066\u308b\u306e\uff1f\uff09NN\u306e\u5b66\u7fd2\u306f\u884c\u52d5\u3092\u4e00\u5b9a\u4ee5\u4e0a\u884c\u3063\u305f\u5f8c\u3001\u5b9f\u969b\u306eQ\u5024\u3068NN\u3067\u4e88\u6e2c\u3057\u305fQ\u5024\u306e\u5dee\u3092\u5c0f\u3055\u304f\u3059\u308b\u3088\u3046\u306b\u5c64\u306e\u91cd\u307f\u3092\u5b66\u7fd2\u3059\u308b\n    - \uff08\u300c\u5b9f\u969b\u306eQ\u5024\u300d\u306f\u3069\u3046\u7b97\u51fa\u3057\u3066\u3044\u308b\uff1f\uff09max(\u5b9f\u969b\u306b\u5f97\u3089\u308c\u305f\u5831\u916c\uff0b\u6b21\u306e\u72b6\u614b\u306eQ\u5024)\n    - \uff08\u300c\u6b21\u306e\u72b6\u614b\u306eQ\u5024\u300d\u306f\u3069\u3046\u7b97\u51fa\u3057\u3066\u3044\u308b\uff1f\uff09\u5225\u306eNN(TargetNet\uff09\u3067\u4e88\u6e2c\u3057\u3066\u3044\u308b\n    - \uff08\u306a\u305c\u5225\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u5fc5\u8981\uff1f\uff09Q\u5024\u3092NN\u3067\u6c42\u3081\u308b\u969b\u306b\u91cd\u307f\u3092\u56fa\u5b9a\u3057\u3066\u304a\u3044\u305f\u65b9\u304c\u51fa\u3066\u304f\u308bQ\u5024\u304c\u5b89\u5b9a\u3059\u308b\uff1d\u5b66\u7fd2\u3082\u5b89\u5b9a\u3059\u308b\u304b\u3089\uff08\u305f\u3060\u3057\u3001\u3042\u308b\u7a0b\u5ea6\u5b66\u7fd2\u3092\u9032\u3081\u305f\u6bce\u306bTargetNet\u306e\u91cd\u307f\u3082\u66f4\u65b0\u3055\u308c\u308b\uff09","168bd252":"ConnectX\u306e\u74b0\u5883\u4f5c\u308a\u3002\n\u57fa\u672c\u7684\u306a\u30e1\u30bd\u30c3\u30c9\u306b\u3064\u3044\u3066\u306f\u3001\u3053\u3061\u3089\u306e[Qiita\u8a18\u4e8b](https:\/\/qiita.com\/ishizakiiii\/items\/75bc2176a1e0b65bdd16)\u304c\u308f\u304b\u308a\u3084\u3059\u3044\u3068\u601d\u3044\u307e\u3059"}}