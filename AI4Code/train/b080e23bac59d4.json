{"cell_type":{"17f1c7ef":"code","9adbb816":"code","dbcbd8cd":"code","12e55dac":"code","54f3c900":"code","d6fcea75":"code","594bd9b8":"code","d0d49da3":"code","c5065f9f":"code","7e777b4e":"code","975b9b21":"code","784fc7a0":"code","f4fdae65":"code","26db47bf":"code","08a52974":"code","ee938421":"code","559b9e41":"code","14817a8d":"code","1e3e6f22":"code","911e61b5":"code","62f8f3f2":"code","0798941a":"code","9481b0f3":"code","411322f1":"code","59755d87":"code","0cbaa451":"code","a1aaf3f6":"code","ccd4d0ff":"code","69c44874":"code","903b9285":"code","96043364":"code","022d088a":"markdown","bcbd66cd":"markdown","6c751dc7":"markdown","ae72638c":"markdown","91ecbfe5":"markdown","1b1b5398":"markdown","e3af10d2":"markdown","cd208c97":"markdown","c2f9b237":"markdown","c75d9d91":"markdown","dc8fd1c8":"markdown","7fed318f":"markdown","52540e22":"markdown","1dc11a0f":"markdown","f2e508d4":"markdown"},"source":{"17f1c7ef":"import numpy as np \nimport pandas as pd \ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\ntrain.head(5)","9adbb816":"## combine the datasets \ntrain['is_train'] = 1\ntest['is_train'] = 0\ntest['SalePrice'] = 0.0\n\ncombined = pd.concat([train, test])\nprint (train.shape)\nprint (test.shape)\nprint (combined.shape )","dbcbd8cd":"miss = combined.isna().sum(axis=0).to_frame().rename(columns={0:\"count\"})\nmiss[miss['count'] > 0].sort_values(\"count\", ascending = False)","12e55dac":"## Maynot drop any missing columns, just impute with something  \ndrop_columns = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu']\ncombined[drop_columns] = combined[drop_columns].fillna(\"NA\")","54f3c900":"miss = combined.isna().sum(axis=0).to_frame().rename(columns={0:\"count\"})\nmiss[miss['count'] > 0].sort_values(\"count\", ascending = False)","d6fcea75":"## Fill the LogFrontage Values using following table \nlookup = combined.groupby(['LotConfig', 'LotShape']).agg({\"LotFrontage\" : \"mean\"}).reset_index()\ndef fill_lf(r):\n    if str(r['LotFrontage']).lower() != \"nan\":\n        return r['LotFrontage']\n    else:\n        return lookup[(lookup[\"LotConfig\"] == r[\"LotConfig\"]) & (lookup[\"LotShape\"] == r[\"LotShape\"])][\"LotFrontage\"].iloc(0)[0]\ncombined['LotFrontage'] = combined.apply(lambda r : fill_lf(r), axis = 1)\n\ncombined['GaragePresent'] = combined[\"GarageArea\"].apply(lambda x : 1 if x>0.0 else 0 )\ncombined['BasementPresent'] = combined[\"TotalBsmtSF\"].apply(lambda x : 1 if x>0.0 else 0 )","594bd9b8":"cols_to_fix = [c for c in combined.columns if \"Garage\" in c]\ncols_to_fix += [c for c in combined.columns if \"Bsmt\" in c]\ncols_to_fix += [c for c in combined.columns if \"Mas\" in c]\n\nfor c in cols_to_fix:\n    if combined[c].dtype == \"object\":\n        combined[c] = combined[c].fillna(\"NA\")\n    else:\n        combined[c] = combined[c].fillna(0.0)","d0d49da3":"miss = combined.isna().sum(axis=0).to_frame().rename(columns={0:\"count\"})\nmiss = miss[miss['count'] > 0].sort_values(\"count\", ascending = False)\nmiss","c5065f9f":"for c in miss.index:\n    combined[c] = combined[c].fillna(combined[c].mode().iloc(0)[0])","7e777b4e":"miss = combined.isna().sum(axis=0).to_frame().rename(columns={0:\"count\"})\nmiss = miss[miss['count'] > 0].sort_values(\"count\", ascending = False)\nmiss","975b9b21":"ignore_cols = ['SalePrice','is_train', 'target', 'Id']\nnum_cols = combined._get_numeric_data().columns\nnum_cols = [f for f in num_cols if f not in ignore_cols]\ncat_cols = [f for f in combined.columns if f not in num_cols]\ncat_cols = [f for f in cat_cols if f not in ignore_cols]\nfeatures = num_cols + cat_cols","784fc7a0":"check =[ ]\nfor c in combined._get_numeric_data().columns:\n    if combined[c].var() < 1.0:\n        check.append (c)\n        \ncheck = []\nfor c in cat_cols:\n    if len(combined[c].value_counts()) < 3:\n        check.append (c)\n        \ncombined = combined.drop(['Street', \"Utilities\"], axis=1)\nfeatures.remove(\"Street\")\nfeatures.remove(\"Utilities\")\ncombined.head()","f4fdae65":"import seaborn as sns \nimport matplotlib.pyplot as plt \n\nplt.figure(figsize=(12,12))\nsns.heatmap(combined[features].corr())","26db47bf":"from scipy.stats import norm, skew #for some statistics\nfrom scipy.special import boxcox1p\n\nskewed_feats = combined[num_cols].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\n\nskewness = skewness[abs(skewness) > 0.75]\nskewed_features = skewness.index\n\nlam = 0.15\nfor c in skewed_features:\n    combined[c] = boxcox1p(combined[c], lam)\ncombined.head()","08a52974":"## Numerical Features : Standard Scaler \nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nscl = RobustScaler()\ncombined[num_cols] = scl.fit_transform(combined[num_cols].values)","ee938421":"updated_combined = pd.get_dummies(combined[features + [\"is_train\", \"SalePrice\"]])\nupdated_features = [f for f in updated_combined.columns if f not in [\"is_train\", \"SalePrice\"]]\nupdated_combined[updated_features].head()","559b9e41":"# ## Categorical Features - Label Encoding \/ OneHot Encoding \/ MeanEncoding \n# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n# le = LabelEncoder()\n# for c in cat_cols:\n#     combined[c] = le.fit_transform(combined[c].values)","14817a8d":"def duplicate_columns(frame):\n    groups = frame.columns.to_series().groupby(frame.dtypes).groups\n    dups = []\n    for t, v in groups.items():\n        dcols = frame[v].to_dict()\n\n        vs = list(dcols.values())\n        ks = list(dcols.keys())\n        lvs = len(vs)\n\n        for i in range(lvs):\n            for j in range(i+1,lvs):\n                if vs[i] == vs[j]: \n                    dups.append(ks[i])\n                    break\n\n    return dups     \n\ndupcols = duplicate_columns(updated_combined[updated_features])\nfor c in dupcols:\n    updated_features.remove(c)","1e3e6f22":"##### Not working will -- will use only K fold CV\n\n# from sklearn.linear_model import LogisticRegression\n# model = LogisticRegression()\n\n# feats = [c for c in combined.columns if c not in ['is_train', 'Id', 'SalePrice']]\n# model.fit(combined[feats], combined['is_train'])\n\n# preds = model.predict_proba(combined[feats])\n# model.classes_\n\n# combined['adval'] = preds[:,0]\n\n# val_x = combined[combined['is_train'] == 1].sort_values(\"adval\").head(int(len(train) * 0.20))\n# train_x = combined[combined['is_train'] == 1].sort_values(\"adval\").tail(len(train) - int(len(train) * 0.20))\n# test_x = combined[combined['is_train'] == 0]\n\n# drops = ['is_train', 'adval', 'Id']\n# val_x = val_x.drop(drops, axis = 1)\n# train_x = train_x.drop(drops, axis = 1)\n# test_x = test_x.drop(drops, axis = 1)\n\n# print (train_x.shape, val_x.shape, test_x.shape)","911e61b5":"## Training a Simple LGB \n# feats = [f for f in combined.columns if f not in ['Id', 'SalePrice', 'is_train']]\n\n## LightGBM with K-Fold Cross Validation \nimport lightgbm as lgb \nimport numpy as np \n\ndef rmsle(y_true, y_pred):\n    return np.sqrt(np.mean(np.power(y_true - y_pred, 2)))\n\ndef run_lgb(train_X, train_y, val_X, val_y):\n    params = {\n        'objective': 'regression',\n        'boosting_type': 'gbdt',\n        'metric': 'rmse',\n        'verbose': -1,\n        'seed': 3,\n        'learning_rate': 0.06,\n        'bagging_seed' : 3,\n        \n        'subsample': 0.9691,\n        'colsample_bytree':  0.4415,\n        'max_depth': 3,\n        'num_leaves': 6,\n        'reg_alpha': 0.05,\n        'reg_lambda': 1.05,\n         }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, 400, valid_sets=[lgval, lgtrain], early_stopping_rounds=50, verbose_eval=100)\n    return model\n\ntrain_x = updated_combined[updated_combined['is_train'] == 1][updated_features]\ntest_x = updated_combined[updated_combined['is_train'] == 0][updated_features]\ntrain_y = np.log1p( updated_combined[updated_combined['is_train'] == 1][\"SalePrice\"] )\n\n\n## Single Run \n\n# model = run_lgb(train_x, train_y), val_x, val_x)\n# lgb_val1 = model.predict(val_x[feats], num_iteration=model.best_iteration)\n\n# import matplotlib.pyplot as plt \n# fig, ax = plt.subplots(figsize=(12,18))\n# lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n# ax.grid(False)\n# plt.title(\"LightGBM - Feature Importance\", fontsize=15)\n# plt.show()","62f8f3f2":"from sklearn.model_selection import KFold\nn_folds = 5\nkf = KFold(n_splits=n_folds, shuffle=True)\n\n# tst_pred_lgb = pd.DataFrame()\n# split = 0\n# for train_index, test_index in kf.split(train_x):\n#     split += 1 \n#     print (\"Split : \", split)\n#     X_train, X_dev = train_x.iloc[train_index], train_x.iloc[test_index]\n#     y_train, y_dev = train_y.iloc[train_index], train_y.iloc[test_index]\n    \n#     ## LightGBM\n#     model_lgb = run_lgb(X_train, y_train, X_dev, y_dev)\n","0798941a":"from sklearn.model_selection import cross_val_score\n\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_x.values)\n    rmse = np.sqrt(-cross_val_score(model, train_x.values, train_y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return (rmse)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=6,\n                              learning_rate=0.06, n_estimators=600,\n                              max_bin = 55, bagging_fraction = 0.9691,\n                              bagging_freq = 5, feature_fraction = 0.4415,\n                              max_depth = 3, reg_alpha=0.05, reg_lambda=1.05,\n                              feature_fraction_seed=9, bagging_seed=9)\n\nscore = rmsle_cv(model_lgb)\nscore.mean()","9481b0f3":"def fp(mod, tr, y, ts):\n    mod.fit(tr, y)\n    train_pred = mod.predict(tr)\n    test_pred = np.expm1(mod.predict(ts))\n    print (rmsle(y, train_pred))\n    return test_pred\n\nlgb_preds = fp(model_lgb, train_x, train_y, test_x)","411322f1":"from bayes_opt import BayesianOptimization\nfrom sklearn.model_selection import cross_val_score\n\ndef lgb_eval(num_leaves=8, \n             max_depth=15,\n             bagging_fraction=0.5, \n             feature_fraction=0.5,\n             lambda_l1=0.0,\n             lambda_l2=0.0):\n    \n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\", \n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"num_threads\" : 4,\n\n        \"num_leaves\" : int(num_leaves),\n        \"max_depth\" : int(max_depth),\n        \"bagging_fraction\" : bagging_fraction,\n        \"feature_fraction\" : feature_fraction,\n        \"lambda_l2\" : lambda_l2,\n        \"lambda_l1\" : lambda_l1,\n        # \"min_child_samples\" : int(min_child_samples),\n\n        \"learning_rate\" : 0.08,\n        \"subsample_freq\" : 5,\n    }\n\n    lgtrain = lgb.Dataset(train_x, label = train_y) \n    cv_result = lgb.cv(params, lgtrain, 1000, early_stopping_rounds=50, stratified=False, nfold=5, metrics=['rmse'])\n    return -cv_result['rmse-mean'][-1]\n\n\ndef param_tuning(init_points, num_iter, **args):\n    lgbBO = BayesianOptimization(lgb_eval, {\n                                            'num_leaves' : (6, 28),\n                                            'max_depth': (3, 16),\n                                            'bagging_fraction': (0.1, 1.0),\n                                            'feature_fraction': (0.1, 1.0),\n                                            'lambda_l1' : (0.0, 5.0),\n                                            'lambda_l2' : (0.0, 5.0)\n                                            })\n\n    lgbBO.maximize(init_points=init_points, n_iter=num_iter, **args)\n    return lgbBO\n\nresult = param_tuning(5, 40)\n# result.res['max']#['max_params']\n## Can Fine Tune More -- Later","59755d87":"import xgboost as xgb\n\ndef run_xgb(train_X, train_y, val_X, val_y):\n    params = {\n              'objective': 'reg:linear', \n              'eval_metric': 'rmse',\n              'random_state': 42, \n              'silent': True,\n        \n              'eta': 0.08,\n              'max_depth': 3, \n              'subsample': 0.8634, \n              'colsample_bytree': 0.3623,\n              'alpha' : 0.1017,\n              'lambda' : 0.2852\n            }\n    \n    tr_data = xgb.DMatrix(train_X, train_y)\n    val_data = xgb.DMatrix(val_X, val_y)\n    watchlist = [(tr_data, 'train'), (val_data, 'valid')]\n    model_xgb = xgb.train(params, tr_data, 1000, watchlist, maximize=False, early_stopping_rounds = 50, verbose_eval=50)\n    return model_xgb\n\n# tst_pred_xgb = pd.DataFrame()\n# split = 0\n# for train_index, test_index in kf.split(train_x):\n#     split += 1 \n#     print (\"Split : \", split)\n#     X_train, X_dev = train_x.iloc[train_index], train_x.iloc[test_index]\n#     y_train, y_dev = train_y.iloc[train_index], train_y.iloc[test_index]\n    \n#     model_xgb = run_xgb(X_train, y_train, X_dev, y_dev)\n#     tst_pred_xgb[split] = model_xgb.predict(xgb.DMatrix(test_x), ntree_limit=model_xgb.best_ntree_limit)  ","0cbaa451":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nscore = rmsle_cv(model_xgb)\nscore.mean()","a1aaf3f6":"xgb_preds = fp(model_xgb, train_x, train_y, test_x)","ccd4d0ff":"def xgb_eval(max_depth, subsample, colsample_bytree, alpha, lambda_):\n    params = {\n              'objective': 'reg:linear', \n              'eval_metric': 'rmse',\n              'random_state': 42, \n              'silent': True,\n        \n              'eta': 0.08,\n              'max_depth': int(max_depth), \n              'subsample': max(min(subsample, 1), 0), \n              'colsample_bytree': max(min(colsample_bytree, 1), 0),                \n              'alpha' : alpha,\n              'lambda' : lambda_\n            }\n    dtrain = xgb.DMatrix(train_x, train_y)\n    cv_result = xgb.cv(params, dtrain, num_boost_round=1000, nfold=5)\n    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]\n\n\ndef param_tuning_xgb(init_points, num_iter):\n    xgbBO = BayesianOptimization(xgb_eval, {\n                                            'max_depth': (3, 16),\n                                            'subsample': (0.3, 1.0),\n                                            'colsample_bytree': (0.3, 1.0),\n                                            'alpha' : (0.0, 5.0),\n                                            'lambda_' : (0.0, 5.0)\n                                            })\n    \n    xgbBO.maximize(init_points=init_points, n_iter=num_iter, acq='ei', xi=0.0)\n    return xgbBO\n\n# result = param_tuning_xgb(5, 30)","69c44874":"from sklearn.linear_model import Lasso, Ridge, ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge","903b9285":"predDF = pd.DataFrame()\npredDF['lgb'] = lgb_preds\npredDF['xgb'] = xgb_preds\n\nsub = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub['SalePrice'] = predDF.mean(axis=1)\nsub.to_csv(\"submission.csv\", index = False)\nsub.head()","96043364":"# - pass categorical features directle \n\n# ## ideas to try : \n# - Dataset PreProcessing : V1\n# - Feature Engineering : V1\n    # - Check Outliers\n\n# - Model Tuninig : V1\n# - Stacking \/ Blending","022d088a":"## xgb tuning ","bcbd66cd":"## 5. Ensembling : Blending","6c751dc7":"## 2.2 Remove Some Columns","ae72638c":"## 4. Modelling \n\n## 4.1 Lightgbm","91ecbfe5":"## Model Tuning : lightgbm","1b1b5398":"## 2. Preprocessing ","e3af10d2":"## Model 2 : xgboost","cd208c97":"## Training more models for stacking ","c2f9b237":"## 2.3.2 Categoricals - Dummy Encoding ","c75d9d91":"## 2.1 Missing Values Treatment ","dc8fd1c8":"## 3. Validation Strategy \n\n1. Advarsarial Validation - Bad Performance\n2. KFold cross Validation","7fed318f":"## Checking Duplicate Columns now ","52540e22":"## 1. Load Datasets ","1dc11a0f":"## 2.3.1 Numerical - BoxCox Transformation","f2e508d4":"## Check for High Correlated Variables"}}