{"cell_type":{"dcf98ee9":"code","7362e01c":"code","750c1b5e":"code","7613b062":"code","123add34":"code","4b37f297":"code","eac0e5d1":"code","7b8e55c7":"code","0601494f":"code","3ed17e1e":"code","d8bce164":"code","baf32958":"code","8f445fb1":"code","bbb6dca4":"code","5cc7f96a":"code","1eee478a":"code","accf3b41":"code","9e34a329":"code","49aa88b3":"code","9fe67039":"code","9e3fd9f8":"code","22b3cd03":"code","491fb725":"code","046a3353":"code","f0ed6bba":"code","36cb4505":"code","44306128":"code","92b3e362":"code","638fbd09":"code","e403c346":"code","f3850898":"code","5cd5c1ce":"code","b6f85ba1":"code","fb836643":"code","7f98c75c":"code","40cf18d1":"code","9ae7738c":"code","6a53b467":"code","9cd5f901":"code","8c024b3b":"code","b40b03ca":"code","0b07397a":"code","ca272695":"code","2c5ecdce":"code","a2e69597":"code","f52e699d":"code","c8c49581":"code","503233ba":"code","4d83c8e5":"code","e5a82f9a":"code","d4c549fd":"code","7c06b20f":"code","d0476127":"code","602202d0":"code","2a3179f8":"code","56b12fb2":"code","25e4b55e":"code","d140a5e6":"code","32298873":"code","73835b84":"code","a68a529d":"code","956ec2d5":"code","229823dc":"code","1d2fef46":"code","38480673":"code","973867f4":"code","606a70a9":"code","f510de17":"code","33ac0a2d":"code","1d93a9b4":"code","8d95f5f4":"markdown","0aac92f5":"markdown","abc737c2":"markdown","5459a201":"markdown","73140811":"markdown","86028e64":"markdown","0590b685":"markdown","707b53af":"markdown","695a1897":"markdown"},"source":{"dcf98ee9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport math\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport shapely.geometry as geom\nimport geopandas as gpd\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    #for filename in filenames:\n    #    print(os.path.join(dirname, filename))\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","7362e01c":"!pip install git+https:\/\/github.com\/jonbarron\/robust_loss_pytorch\n!pip install --user torchcontrib\n!cp -r \/kaggle\/input\/optimizers\/lookahead.pytorch-master\/* \/kaggle\/working\/lookahead\n!cp -r \/kaggle\/input\/optimizers\/RAdam-master\/* \/kaggle\/working\/RAdam","750c1b5e":"df_train = pd.read_csv('\/kaggle\/input\/bigquery-geotab-intersection-congestion\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/bigquery-geotab-intersection-congestion\/test.csv')\ndf_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","7613b062":"list(df_train.columns.values)","123add34":"df_train.fillna(\"Unknown\",inplace=True)\ndf_test.fillna(\"Unknown\",inplace=True)","4b37f297":"df_train[\"IntersectionUID\"] = df_train[\"City\"] + df_train[\"IntersectionId\"].astype(str)\ndf_test[\"IntersectionUID\"] = df_test[\"City\"] + df_test[\"IntersectionId\"].astype(str)","eac0e5d1":"df_train.sample(5)","7b8e55c7":"df_test.sample(5)","0601494f":"#df_train[\"IntersectionUID\"] = df_train[[\"City\", \"IntersectionId\"]].apply(lambda row: row.City+str(row.IntersectionId), axis=1)\n#df_test[\"IntersectionUID\"] = df_test[[\"City\", \"IntersectionId\"]].apply(lambda row: row.City+str(row.IntersectionId), axis=1)\nuid_dict=pd.DataFrame(data=range(len(df_train[\"IntersectionUID\"].unique())), index=df_train[\"IntersectionUID\"].unique()).to_dict()[0]\ndf_train[\"IntersectionUID\"] = df_train[\"IntersectionUID\"].map(uid_dict)\ndf_test[\"IntersectionUID\"] = df_test[\"IntersectionUID\"].map(uid_dict)\ndf_test[\"IntersectionUID\"].fillna(int(4796), inplace=True)\ndf_test[\"IntersectionUID\"] = df_test[\"IntersectionUID\"].astype(\"int32\")","3ed17e1e":"print(len(df_train[\"IntersectionUID\"].unique()))\nprint(df_train[\"IntersectionUID\"].unique())\nprint(len(df_test[\"IntersectionUID\"].unique()))\nprint(df_test[\"IntersectionUID\"].unique())","d8bce164":"street_uid_dict=pd.DataFrame(data=range(len(df_train[\"EntryStreetName\"].append(df_train[\"ExitStreetName\"]).unique())), index=df_train[\"EntryStreetName\"].append(df_train[\"ExitStreetName\"]).unique()).to_dict()[0]\ndf_train[\"EntryStreetID\"] = df_train[\"EntryStreetName\"].map(street_uid_dict)\ndf_test[\"EntryStreetID\"] = df_test[\"EntryStreetName\"].map(street_uid_dict)\ndf_train[\"ExitStreetID\"] = df_train[\"ExitStreetName\"].map(street_uid_dict)\ndf_test[\"ExitStreetID\"] = df_test[\"ExitStreetName\"].map(street_uid_dict)\ndf_test[\"EntryStreetID\"].fillna(int(1), inplace=True)\ndf_test[\"ExitStreetID\"].fillna(int(1), inplace=True)\ndf_train[\"ExitStreetID\"] = df_train[\"ExitStreetID\"].astype(np.int32)\ndf_test[\"ExitStreetID\"] = df_test[\"ExitStreetID\"].astype(np.int32)\ndf_train[\"EntryStreetID\"] = df_train[\"EntryStreetID\"].astype(np.int32)\ndf_test[\"EntryStreetID\"] = df_test[\"EntryStreetID\"].astype(np.int32)","baf32958":"print(len(df_train[\"EntryStreetName\"]))\n#print(df_train[~df_train[\"ExitStreetName\"].str.contains(\"Street|St|Avenue|Ave|Boulevard|Road|Drive|Lane|Tunnel|Highway|Way|Parkway|Parking|Oval|Unknown|Square|Place|Bridge\", regex=True)][\"ExitStreetName\"])\n#print(df_train[~df_train[\"EntryStreetName\"].str.contains(\"Street|St|Avenue|Ave|Boulevard|Road|Drive|Lane|Tunnel|Highway|Way|Parkway|Parking|Oval|Unknown|Square|Place|Bridge\", regex=True)][\"EntryStreetName\"])\n#print(df_train[df_train[\"EntryStreetName\"].str.contains(\"Place\", regex=True)][\"EntryStreetName\"])\n\nstreet_type = [\"Street\", \"St\", \"Avenue\", \"Ave\", \"Boulevard\", \"Road\", \"Drive\", \"Lane\",\n              \"Tunnel\", \"Highway\", \"Way\", \"Parkway\", \"Parking\", \"Oval\", \"Square\",\n              \"Place\", \"Bridge\", \"Unknown\"]\n\nstreet_type_num = [0, 0, 1, 1, 2, 3, 4, 5,\n                     6, 7, 8, 9, 9, 10, 11,\n                     12, 13, 14]\n\ndef get_street_type(row, column):\n    \n    for i, s in enumerate(street_type):\n        if s in row[column]:\n            return street_type_num[i]\n    return 15\n\ndef extract_street_type(df, column):\n    df[column.replace(\"StreetName\", \"Type\")] = df.apply(lambda row: get_street_type(row, column), axis=1)","8f445fb1":"extract_street_type(df_train, \"EntryStreetName\")\nextract_street_type(df_train, \"ExitStreetName\")\nextract_street_type(df_test, \"EntryStreetName\")\nextract_street_type(df_test, \"ExitStreetName\")\ndf_train.head()","bbb6dca4":"directions={\"E\":0, \"SE\":1, \"S\":2, \"SW\":3, \"W\":4, \"NW\":5, \"N\":6, \"NE\":7,\n           0:0, 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7}\n\ndf_train[\"EntryHeading\"] = df_train[\"EntryHeading\"].map(directions).astype(np.int32)\ndf_train[\"ExitHeading\"] = df_train[\"ExitHeading\"].map(directions).astype(np.int32)\n\ndf_test[\"EntryHeading\"] = df_test[\"EntryHeading\"].map(directions).astype(np.int32)\ndf_test[\"ExitHeading\"] = df_test[\"ExitHeading\"].map(directions).astype(np.int32)\n\ndef calc_turn(dataframe):\n    dataframe[\"RightTurn\"] = ((dataframe[\"EntryHeading\"] - dataframe[\"ExitHeading\"]) % 8 == 2).astype(np.int32)\n    dataframe[\"LeftTurn\"] = ((dataframe[\"EntryHeading\"] - dataframe[\"ExitHeading\"]) % 8 == 6).astype(np.int32)\n    dataframe[\"PassThru\"] = ((dataframe[\"EntryHeading\"] - dataframe[\"ExitHeading\"]) % 8 == 4).astype(np.int32)\n    dataframe[\"UTurn\"] = (dataframe[\"EntryHeading\"] - dataframe[\"ExitHeading\"] == 0).astype(np.int32)\n    \n    dataframe[\"RightSide\"] = ((dataframe[\"ExitHeading\"] - dataframe[\"EntryHeading\"]) % 8 > 5).astype(np.int32)\n    dataframe[\"LeftSide\"] = ((dataframe[\"ExitHeading\"] - (dataframe[\"EntryHeading\"] + 1)) % 8 < 3).astype(np.int32)\n    dataframe[\"Direction\"] = ((dataframe[\"ExitHeading\"] - (dataframe[\"EntryHeading\"])) % 8).astype(np.int32)\n\ncalc_turn(df_train)\ncalc_turn(df_test)","5cc7f96a":"df_train[[\"EntryHeading\", \"ExitHeading\", \"RightTurn\", \"LeftTurn\", \"PassThru\", \"UTurn\", \"RightSide\", \"LeftSide\", \"Direction\"]].sample(10)","1eee478a":"df_train[\"RushHour1\"] = 8 - df_train[\"Hour\"]\ndf_train[\"RushHour2\"] = 17 - df_train[\"Hour\"]\ndf_test[\"RushHour1\"] = 8 - df_test[\"Hour\"]\ndf_test[\"RushHour2\"] = 17 - df_test[\"Hour\"]","accf3b41":"df_train.sample(10)","9e34a329":"df_high_temp = pd.DataFrame({\"Atlanta\":[52.3, 56.6, 64.6, 72.5, 79.9, 86.4, 89.1, 88.1, 82.2, 72.7, 63.6, 54],\n                              \"Boston\":[35.8, 38.7, 45.4, 55.6, 66.0, 75.9, 81.4, 79.6, 72.4, 61.4, 51.5, 41.2],\n                              \"Chicago\": [31.5, 35.8, 46.8, 59.2, 70.2, 79.9, 84.2, 82.1, 75.3, 62.8, 48.6, 35.3],\n                              \"Philadelphia\":[40.3, 43.8, 52.7, 63.9, 73.8, 82.7, 87.1, 85.3, 78.0, 66.6, 56.0, 44.8]})\n\ndf_low_temp = pd.DataFrame({\"Atlanta\":[34.3, 37.7, 44.1, 51.5, 60.3, 68.2, 71.3, 70.7, 64.8, 54.0, 44.5, 36.5],\n                           \"Boston\":[22.2, 24.7, 31.1, 40.6, 49.9, 59.5, 65.4, 64.6, 57.4, 46.5, 38.0, 28.2],\n                           \"Chicago\":[18.1, 21.7, 30.9, 41.7, 51.6, 62.1, 67.5, 66.2, 57.5, 45.7, 34.5, 22.7],\n                           \"Philadelphia\":[25.6, 27.7, 33.4, 44.1, 54.0, 62.8, 69.2, 67.9, 60.3, 48.4, 39.2, 30.1]})\n\ndf_snowfall = pd.DataFrame({\"Atlanta\":[1.3, 0.4, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4],\n                              \"Boston\":[12.9, 10.9, 7.8, 1.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 1.3, 9.0],\n                              \"Chicago\":[11.5, 9.1, 5.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 1.3, 8.7],\n                              \"Philadelphia\":[6.5, 8.8, 2.9, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 3.4]})\n                                                                                   \ndf_rainfall = pd.DataFrame({\"Atlanta\":[4.2, 4.7, 4.8, 3.4, 3.7, 4.0, 5.3, 3.9, 4.5, 3.4, 4.1, 3.9],\n                              \"Boston\":[3.4, 3.3, 4.3, 3.7, 3.5, 3.7, 3.4, 3.4, 3.4, 3.9,4.0, 3.8],\n                              \"Chicago\":[2.1, 1.9, 2.7, 3.6, 4.1, 4.1, 4.0, 4.0, 3.3, 3.2, 3.4, 2.6],\n                              \"Philadelphia\":[3.0, 2.7, 3.8, 3.6, 3.7, 3.4, 4.4, 3.5, 3.8, 3.2, 3.0, 3.6]})\n                                                                                   \ndf_daylight = pd.DataFrame({\"Atlanta\":[10, 11, 12, 13, 14, 14, 14, 13, 12, 11, 10, 10],\n                             \"Boston\":[9, 11, 12, 13, 15, 15, 15, 14, 12, 11, 10, 9],\n                             \"Chicago\":[10, 11, 12, 13, 15, 15, 15, 14, 12, 11, 10, 9],\n                             \"Philadelphia\":[10, 11, 12, 13, 14, 15, 15, 14, 12, 11, 10, 9]})         \n                                                                                  \ndf_sunshine = pd.DataFrame({\"Atlanta\":[5.3, 6.1, 7.1, 8.7, 9.3, 9.5, 8.8, 8.3, 7.6, 7.7, 6.2, 5.3], \n                             \"Boston\":[5.3, 6.0, 6.9, 7.6, 8.6, 9.6, 9.7, 8.9, 7.9, 6.7, 4.8, 4.6],\n                             \"Chicago\":[4.4, 4.9, 6.0, 7.2, 9.1, 10.4, 10.3, 9.1, 7.6, 6.2, 3.6, 3.4],  \n                             \"Philadelphia\":[5.0, 5.5, 6.5, 7.2, 7.9, 9.0, 8.9, 8.4, 7.3, 6.6, 5.2, 4.4]})","49aa88b3":"def get_climate_dict(df):\n    df[\"Index\"] = range(1, 13)\n    melt_df = pd.melt(df, value_vars=[\"Atlanta\", \"Boston\", \"Chicago\", \"Philadelphia\"], id_vars=\"Index\")\n    melt_df[\"Index\"] = melt_df[\"variable\"] + melt_df[\"Index\"].astype(str)\n    melt_df = melt_df.drop(columns=[\"variable\"])\n    melt_df = melt_df.set_index(\"Index\")\n    return melt_df.to_dict()[\"value\"]","9fe67039":"high_temp_dict = get_climate_dict(df_high_temp)\nlow_temp_dict = get_climate_dict(df_low_temp)\nsnowfall_dict = get_climate_dict(df_snowfall)\nrainfall_dict = get_climate_dict(df_rainfall)\ndaylight_dict = get_climate_dict(df_daylight)\nsunshine_dict = get_climate_dict(df_sunshine)","9e3fd9f8":"def get_climate(df, climate_dict, column):\n    df[column] = df[\"City\"] + df[\"Month\"].astype(str)\n    df[column] = df[column].map(climate_dict)","22b3cd03":"get_climate(df_train, high_temp_dict, \"HighTemp\")\nget_climate(df_train, low_temp_dict, \"LowTemp\")\nget_climate(df_train, snowfall_dict, \"SnowFall\")\nget_climate(df_train, rainfall_dict, \"RainFall\")\nget_climate(df_train, daylight_dict, \"DayLight\")\nget_climate(df_train, sunshine_dict, \"SunShine\")\n\nget_climate(df_test, high_temp_dict, \"HighTemp\")\nget_climate(df_test, low_temp_dict, \"LowTemp\")\nget_climate(df_test, snowfall_dict, \"SnowFall\")\nget_climate(df_test, rainfall_dict, \"RainFall\")\nget_climate(df_test, daylight_dict, \"DayLight\")\nget_climate(df_test, sunshine_dict, \"SunShine\")","491fb725":"df_train[\"SummerBreak\"] = ((df_train[\"Month\"] >= 6) & (df_train[\"Month\"] <= 8)).astype(int)\ndf_test[\"SummerBreak\"] = ((df_test[\"Month\"] >= 6) & (df_test[\"Month\"] <= 8)).astype(int)\ndf_train[\"WinterBreak\"] = (df_train[\"Month\"] == 12).astype(int)\ndf_test[\"WinterBreak\"] = (df_test[\"Month\"] == 12).astype(int)","046a3353":"cities={\"Atlanta\":0, \"Boston\":1, \"Chicago\":2, \"Philadelphia\":3, 0:0, 1:1, 2:2, 3:3}\n\ndf_train[\"City\"] = df_train[\"City\"].map(cities).astype(np.int32)\ndf_test[\"City\"] = df_test[\"City\"].map(cities).astype(np.int32)","f0ed6bba":"def get_geo_dist(df_train, df_test):\n    \n    MAX_DIR = 0.002\n    \n    def get_dist_dict(atlanta_df, boston_df, chicago_df, philadelphia_df, column):\n    \n        atlanta_dict = atlanta_df[[\"IntersectionUID\", column]].set_index(\"IntersectionUID\").to_dict()[column]\n        boston_dict = boston_df[[\"IntersectionUID\", column]].set_index(\"IntersectionUID\").to_dict()[column]\n        chicago_dict = chicago_df[[\"IntersectionUID\", column]].set_index(\"IntersectionUID\").to_dict()[column]\n        philadelphia_dict = philadelphia_df[[\"IntersectionUID\", column]].set_index(\"IntersectionUID\").to_dict()[column]\n\n        dist_dict = {**atlanta_dict, **boston_dict}\n        dist_dict = {**dist_dict, **chicago_dict}\n        dist_dict = {**dist_dict, **philadelphia_dict}\n        \n        return dist_dict\n    \n    def min_dist(point1, point2):\n        \n        dist = point2.distance(point1)\n        min_dist = (dist.sort_values().reset_index()).iloc[1,1]        \n        return min_dist\n    \n    def min_dir(point1, point2, dir):\n\n        if dir == 0:\n            #dir_vector = Point(1, 0)\n            dir_point2 = point2[point2.x > point1.x]\n        elif dir == 1:\n            #dir_vector = Point(-1, 0)\n            dir_point2 = point2[point2.x < point1.x]\n        elif dir == 2: \n            #dir_vector = Point(0, 1)\n            dir_point2 = point2[point2.y > point1.y]\n        elif dir == 3:\n            #dir_vector = Point(0, -1)\n            dir_point2 = point2[point2.y < point1.y]\n        elif dir == 4:\n            #dir_vector = Point(1, 1)\n            dir_point2 = point2[point2.y < point1.y]\n        elif dir == 5:\n            #dir_vector = Point(1, -1)\n            dir_point2 = point2[point2.y < point1.y]\n        elif dir == 6:\n            #dir_vector = Point(-1, -1)\n            dir_point2 = point2[point2.y < point1.y]\n        elif dir == 7:\n            #dir_vector = Point(-1, 1)\n            dir_point2 = point2[point2.y < point1.y]\n        else:\n            exit(1)\n        \n        #vector=(point2 - point1)\n        #cos_similarity = (vector * dir_vector) \/ (vector * vector + dir_vector * dir_vector)\n        #dir_point2 = point2[cos_similarity > 0.8]\n        \n        if len(dir_point2) == 0:\n            dir_dist = MAX_DIR\n        else:\n            dir_dist = dir_point2.distance(point1)\n            dir_dist = (dir_dist.sort_values().reset_index()).iloc[0, 1]\n            dir_dist = min(MAX_DIR, dir_dist)\n            \n        return dir_dist\n    \n    def min_inter(point1, point2, dir):\n        if dir == 0:\n            #dir_vector = Point(1, 0)\n            dir_point2 = point2[point2.x > point1.x]\n        elif dir == 1:\n            #dir_vector = Point(-1, 0)\n            dir_point2 = point2[point2.x < point1.x]\n        elif dir == 2: \n            #dir_vector = Point(0, 1)\n            dir_point2 = point2[point2.y > point1.y]\n        elif dir == 3:\n            #dir_vector = Point(0, -1)\n            dir_point2 = point2[point2.y < point1.y]\n        elif dir == 4:\n            #dir_vector = Point(1, 1)\n            dir_point2 = point2[point2.y < point1.y]\n        elif dir == 5:\n            #dir_vector = Point(1, -1)\n            dir_point2 = point2[point2.y < point1.y]\n        elif dir == 6:\n            #dir_vector = Point(-1, -1)\n            dir_point2 = point2[point2.y < point1.y]\n        elif dir == 7:\n            #dir_vector = Point(-1, 1)\n            dir_point2 = point2[point2.y < point1.y]\n        else:\n            exit(1)\n\n        #vector=(point2 - point1)\n        #cos_similarity = (vector * dir_vector) \/ (vector * vector + dir_vector * dir_vector)\n        #dir_point2 = point2[cos_similarity > 0.8]\n\n        if len(dir_point2) == 0:\n            dir_inter = 4096\n        else:\n            dir_dist = dir_point2.distance(point1)\n            dir_dist = dir_dist.sort_values().reset_index().iloc[0, 1]\n\n            if dir_dist > MAX_DIR:\n                dir_inter = 4096\n            else:\n                dir_iter = 0\n\n        return dir_inter\n\n    def apply_map(df, dictionary, column):\n        \n        df[column] = df[\"IntersectionUID\"]\n        df[column] = df[column].map(dictionary)\n    \n    df = pd.concat((df_train[[\"IntersectionUID\", \"Latitude\", \"Longitude\", \"City\"]], \\\n                    df_test[[\"IntersectionUID\", \"Latitude\", \"Longitude\", \"City\"]]), axis=0)\n\n    df_geo = df.drop_duplicates(subset=\"IntersectionUID\")\n    df_geo = gpd.GeoDataFrame(df_geo, geometry=gpd.points_from_xy(df_geo.Latitude, df_geo.Longitude))\n\n    atlanta_df = df_geo.loc[df_geo[\"City\"] == 0]\n    boston_df = df_geo.loc[df_geo[\"City\"] == 1]\n    chicago_df = df_geo.loc[df_geo[\"City\"] == 2]\n    philadelphia_df = df_geo.loc[df_geo[\"City\"] == 3]\n    \n    atlanta_df[\"CenterDist\"] = atlanta_df.geometry.distance(geom.Point(33.753746, -84.386330))\n    boston_df[\"CenterDist\"] = boston_df.geometry.distance(geom.Point(42.361145, -71.057083))\n    chicago_df[\"CenterDist\"] = chicago_df.geometry.distance(geom.Point(41.881832, -87.623177))\n    philadelphia_df[\"CenterDist\"] = philadelphia_df.geometry.distance(geom.Point(39.952583, -75.165222))\n\n    center_dist_dict = get_dist_dict(atlanta_df, boston_df, chicago_df, philadelphia_df, \"CenterDist\")\n    apply_map(df_train, center_dist_dict, \"CenterDist\")\n    apply_map(df_test, center_dist_dict, \"CenterDist\")\n    \n    atlanta_df[\"MinDist\"] = atlanta_df.geometry.apply(min_dist, args=(atlanta_df.geometry, ))\n    boston_df[\"MinDist\"] = boston_df.geometry.apply(min_dist, args=(boston_df.geometry, ))\n    chicago_df[\"MinDist\"] = chicago_df.geometry.apply(min_dist, args=(chicago_df.geometry, ))\n    philadelphia_df[\"MinDist\"] = philadelphia_df.geometry.apply(min_dist, args=(philadelphia_df.geometry, ))\n\n    for i, column in enumerate([\"NDist\", \"SDist\", \"WDist\", \"EDist\"]):\n        atlanta_df[column] = atlanta_df.geometry.apply(min_dir, args=(atlanta_df.geometry, i, ))\n        boston_df[column] = boston_df.geometry.apply(min_dir, args=(boston_df.geometry, i, ))\n        chicago_df[column] = chicago_df.geometry.apply(min_dir, args=(chicago_df.geometry, i, ))\n        philadelphia_df[column] = philadelphia_df.geometry.apply(min_dir, args=(philadelphia_df.geometry, i, ))\n\n    min_dist_dict = get_dist_dict(atlanta_df, boston_df, chicago_df, philadelphia_df, \"MinDist\")\n    n_dist_dict = get_dist_dict(atlanta_df, boston_df, chicago_df, philadelphia_df, \"NDist\")\n    s_dist_dict = get_dist_dict(atlanta_df, boston_df, chicago_df, philadelphia_df, \"SDist\")\n    w_dist_dict = get_dist_dict(atlanta_df, boston_df, chicago_df, philadelphia_df, \"WDist\")\n    e_dist_dict = get_dist_dict(atlanta_df, boston_df, chicago_df, philadelphia_df, \"EDist\")\n    \n    apply_map(df_train, min_dist_dict, \"MinDist\")\n    apply_map(df_test, min_dist_dict, \"MinDist\")\n    \n    apply_map(df_train, n_dist_dict, \"NDist\")\n    apply_map(df_test, n_dist_dict, \"NDist\")\n    \n    apply_map(df_train, s_dist_dict, \"SDist\")\n    apply_map(df_test, s_dist_dict, \"SDist\")\n    \n    apply_map(df_train, w_dist_dict, \"WDist\")\n    apply_map(df_test, w_dist_dict, \"WDist\")\n    \n    apply_map(df_train, e_dist_dict, \"EDist\")\n    apply_map(df_test, e_dist_dict, \"EDist\")","36cb4505":"%time get_geo_dist(df_train, df_test)\ndf_train.head()","44306128":"fig, ax = plt.subplots(figsize=(12,12)) \nsns.heatmap(data=(df_train).corr(), square=True, ax=ax)","92b3e362":"def count_plot(df, value, ax):\n    sns.countplot(x=value, data=df, ax=ax)","638fbd09":"fig = plt.figure(figsize = (20, 12)) # width x height\nax1 = fig.add_subplot(3, 3, 1) # row, column, position\nax2 = fig.add_subplot(3, 3, 2)\nax3 = fig.add_subplot(3, 3, 3)\nax4 = fig.add_subplot(3, 3, 4)\nax5 = fig.add_subplot(3, 3, 5)\nax6 = fig.add_subplot(3, 3, 6)\nax7 = fig.add_subplot(3, 3, 7)\nax8 = fig.add_subplot(3, 3, 8)\nax9 = fig.add_subplot(3, 3, 9)\n\nsns.countplot(data=df_train, x=\"City\", ax=ax1)\nsns.countplot(data=df_train, x=\"Month\", ax=ax2)\nsns.countplot(data=df_train, x=\"Hour\", ax=ax3)\nsns.countplot(data=df_train, x=\"Weekend\", ax=ax4)\nsns.countplot(data=df_train, x=\"EntryHeading\", ax=ax5)\nsns.countplot(data=df_train, x=\"ExitHeading\", ax=ax6)\nsns.countplot(data=df_train, x=\"Direction\", ax=ax7)\nsns.countplot(data=df_train, x=\"EntryType\", ax=ax8)\nsns.countplot(data=df_train, x=\"ExitType\", ax=ax9)","e403c346":"fig = plt.figure(figsize = (20, 12)) # width x height\nax1 = fig.add_subplot(3, 3, 1) # row, column, position\nax2 = fig.add_subplot(3, 3, 2)\nax3 = fig.add_subplot(3, 3, 3)\nax4 = fig.add_subplot(3, 3, 4)\nax5 = fig.add_subplot(3, 3, 5)\nax6 = fig.add_subplot(3, 3, 6)\nax7 = fig.add_subplot(3, 3, 7)\nax8 = fig.add_subplot(3, 3, 8)\nax9 = fig.add_subplot(3, 3, 9)\n\nsns.countplot(data=df_test, x=\"City\", ax=ax1)\nsns.countplot(data=df_test, x=\"Month\", ax=ax2)\nsns.countplot(data=df_test, x=\"Hour\", ax=ax3)\nsns.countplot(data=df_test, x=\"Weekend\", ax=ax4)\nsns.countplot(data=df_test, x=\"EntryHeading\", ax=ax5)\nsns.countplot(data=df_test, x=\"ExitHeading\", ax=ax6)\nsns.countplot(data=df_test, x=\"Direction\", ax=ax7)\nsns.countplot(data=df_test, x=\"EntryType\", ax=ax8)\nsns.countplot(data=df_test, x=\"ExitType\", ax=ax9)","f3850898":"def plot_6(df, plot):\n    fig = plt.figure(figsize = (20,8)) # width x height\n    ax1 = fig.add_subplot(2, 3, 1) # row, column, position\n    ax2 = fig.add_subplot(2, 3, 2)\n    ax3 = fig.add_subplot(2, 3, 3)\n    ax4 = fig.add_subplot(2, 3, 4)\n    ax5 = fig.add_subplot(2, 3, 5)\n    ax6 = fig.add_subplot(2, 3, 6)\n    \n    plot(df, \"TotalTimeStopped_p20\", ax1)\n    plot(df, \"TotalTimeStopped_p50\", ax2)\n    plot(df, \"TotalTimeStopped_p80\", ax3)\n    plot(df, \"DistanceToFirstStop_p20\", ax4)\n    plot(df, \"DistanceToFirstStop_p50\", ax5)\n    plot(df, \"DistanceToFirstStop_p80\", ax6)","5cd5c1ce":"def plot_week_hour(df, value, ax):\n    data = df.groupby([\"Weekend\", \"Hour\"]).mean()[value].unstack(level=0).reset_index().rename(columns={0:\"Weekday\", 1:\"Weekend\"}).set_index(\"Hour\")\n    sns.lineplot(data=data, ax=ax, legend=\"brief\")\n    ax.set(ylabel=value)\n    ax.legend(loc=2)\n    \nplot_6(df_train, plot_week_hour)","b6f85ba1":"def plot_month_hour(df, value, ax):\n    data = df.groupby([\"Month\", \"Hour\"]).mean()[value].unstack(level=0).reset_index()\n    data.fillna(0, inplace=True)\n    data = data.melt(id_vars=\"Hour\", value_vars=[1,5,6,7,8,9,10,11,12], value_name=value)\n    #sns.catplot(x=\"Hour\", y=value, hue=\"Month\", data=data, ax=ax, kind=\"strip\")\n    sns.factorplot(x=\"Hour\", y=value, hue=\"Month\", data=data, ax=ax)\n    ax.set(ylabel=value)\n    ax.legend(loc=2)\n    plt.close(2)\n    \nplot_6(df_train, plot_month_hour)","fb836643":"def plot_city_hour(df, value, ax):\n    data = df.groupby([\"City\", \"Hour\"]).mean()[value].unstack(level=0).reset_index()\n    data.fillna(0, inplace=True)\n    data = data.melt(id_vars=\"Hour\", value_vars=[0,1,2,3], value_name=value)\n    #sns.catplot(x=\"Hour\", y=value, hue=\"Month\", data=data, ax=ax, kind=\"strip\")\n    sns.factorplot(x=\"Hour\", y=value, hue=\"City\", data=data, ax=ax)\n    ax.set(ylabel=value)\n    ax.legend(loc=2)\n    plt.close(2)\n\nplot_6(df_train, plot_city_hour)","7f98c75c":"def plot_direction_city(df, value, ax):\n    data = df.groupby([\"Direction\", \"City\"]).mean()[value].unstack(level=0).reset_index()\n    data.fillna(0, inplace=True)\n    data = data.melt(id_vars=\"City\", value_vars=[0,1,2,3,4,5,6,7], value_name=value)\n    #sns.catplot(x=\"Hour\", y=value, hue=\"Month\", data=data, ax=ax, kind=\"strip\")\n    sns.factorplot(x=\"City\", y=value, hue=\"Direction\", data=data, ax=ax, kind=\"bar\")\n    ax.set(ylabel=value)\n    ax.legend(loc=2)\n    plt.close(2)\n\nplot_6(df_train, plot_direction_city)","40cf18d1":"def plot_map_count(df, city):\n    count=df.groupby(['City','Latitude','Longitude'])[\"IntersectionUID\"].count().reset_index()\n    \n    fig = px.scatter_mapbox(count[count[\"City\"]==city], \n                            lat=\"Latitude\", lon=\"Longitude\",size=\"IntersectionUID\",size_max=10,\n                            color=\"IntersectionUID\", color_continuous_scale=px.colors.sequential.Inferno, zoom=11)\n    fig.update_layout(mapbox_style=\"stamen-terrain\")\n    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n    fig.show()","9ae7738c":"#plot_map_count(df_train, 0)","6a53b467":"#plot_map_count(df_test, 0)","9cd5f901":"#plot_map_count(df_train, 3)","8c024b3b":"#plot_map_count(df_test, 3)","b40b03ca":"def plot_map(city, feature, hour=17):\n    TotalTimeStopped=df_train.groupby(['City','Latitude','Longitude', 'Hour'])[feature].mean().reset_index()\n    TotalTimeStopped = TotalTimeStopped[TotalTimeStopped[\"Hour\"]== hour]\n    \n    fig = px.scatter_mapbox(TotalTimeStopped[TotalTimeStopped[\"City\"]==city], \n                            lat=\"Latitude\", lon=\"Longitude\",size=feature,size_max=10,\n                            color=feature, color_continuous_scale=px.colors.sequential.Inferno, zoom=11)\n    fig.update_layout(mapbox_style=\"stamen-terrain\")\n    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n    fig.show()","0b07397a":"#plot_map(0, \"DistanceToFirstStop_p80\")","ca272695":"#plot_map(3, \"DistanceToFirstStop_p80\")","2c5ecdce":"drop_features = ['IntersectionId', 'EntryStreetName', 'ExitStreetName', 'Path']\ndrop_unused_targets = ['TotalTimeStopped_p40', 'TotalTimeStopped_p60',\n                        'TimeFromFirstStop_p20', 'TimeFromFirstStop_p40', 'TimeFromFirstStop_p50',\n                        'TimeFromFirstStop_p60', 'TimeFromFirstStop_p80', 'DistanceToFirstStop_p40',\n                         'DistanceToFirstStop_p60']\n\ninput_train = df_train.drop(drop_features + drop_unused_targets, axis=1)\ninput_test = df_test.drop(drop_features, axis=1)\nprint(\"Total      - {}\".format(len(input_train)))\ninput_val = input_train.sample(frac=0.2, random_state=0)\ninput_train = input_train.drop(input_val.index)\nprint(\"Training   - {}\".format(len(input_train)))\nprint(\"Validation - {}\".format(len(input_val)))\nprint(\"Test       - {}\".format(len(input_test)))","a2e69597":"def min_max(df, min, max):\n    #min = base_df.min()\n    #max = base_df.max()\n\n    return (df - min) \/ (max - min)\n\ndef z_score(df, base_df):\n    mean = base_df.mean()\n    std = base_df.std()\n    return (df - mean) \/ std\n\ndef preprocess_data(input_train, input_val, input_test):\n\n    #input_test[\"Latitude\"] = min_max(input_test[\"Latitude\"], input_train[\"Latitude\"])\n    #input_test[\"Longitude\"] = min_max(input_test[\"Longitude\"], input_train[\"Latitude\"])\n    #input_test[\"CenterDistance\"] = min_max(input_test[\"CenterDistance\"], input_train[\"Latitude\"])\n\n    #input_val[\"Latitude\"] = min_max(input_val[\"Latitude\"], input_train[\"Latitude\"])\n    #input_val[\"Longitude\"] = min_max(input_val[\"Longitude\"], input_train[\"Longitude\"])\n    #input_val[\"CenterDistance\"] = min_max(input_val[\"CenterDistance\"],input_train[\"CenterDistance\"])\n\n    #input_train[\"Latitude\"] = min_max(input_train[\"Latitude\"], input_train[\"Latitude\"])\n    #input_train[\"Longitude\"] = min_max(input_train[\"Longitude\"], input_train[\"Longitude\"])\n    #input_train[\"CenterDistance\"] = min_max(input_train[\"CenterDistance\"], input_train[\"CenterDistance\"])\n    \n    input_test[\"Latitude\"] = z_score(input_test[\"Latitude\"], input_train[\"Latitude\"])\n    input_test[\"Longitude\"] = z_score(input_test[\"Longitude\"], input_train[\"Longitude\"])\n    input_test[\"CenterDist\"] = z_score(input_test[\"CenterDist\"], input_train[\"CenterDist\"])\n    input_test[\"MinDist\"] = z_score(input_test[\"MinDist\"], input_train[\"MinDist\"])\n    input_test[\"NDist\"] = z_score(input_test[\"NDist\"], input_train[\"NDist\"])\n    input_test[\"SDist\"] = z_score(input_test[\"SDist\"], input_train[\"SDist\"])\n    input_test[\"WDist\"] = z_score(input_test[\"WDist\"], input_train[\"WDist\"])\n    input_test[\"EDist\"] = z_score(input_test[\"EDist\"], input_train[\"EDist\"])\n    input_test[\"HighTemp\"] = z_score(input_test[\"HighTemp\"], input_train[\"HighTemp\"])\n    input_test[\"LowTemp\"] = z_score(input_test[\"LowTemp\"], input_train[\"LowTemp\"])\n    input_test[\"SnowFall\"] = z_score(input_test[\"SnowFall\"], input_train[\"SnowFall\"])\n    input_test[\"RainFall\"] = z_score(input_test[\"RainFall\"], input_train[\"RainFall\"])\n    input_test[\"DayLight\"] = z_score(input_test[\"DayLight\"], input_train[\"DayLight\"])\n    input_test[\"SunShine\"] = z_score(input_test[\"SunShine\"], input_train[\"SunShine\"])\n\n    input_val[\"Latitude\"] = z_score(input_val[\"Latitude\"], input_train[\"Latitude\"])\n    input_val[\"Longitude\"] = z_score(input_val[\"Longitude\"], input_train[\"Longitude\"])\n    input_val[\"CenterDist\"] = z_score(input_val[\"CenterDist\"], input_train[\"CenterDist\"])\n    input_val[\"MinDist\"] = z_score(input_val[\"MinDist\"], input_train[\"MinDist\"])\n    input_val[\"NDist\"] = z_score(input_val[\"NDist\"], input_train[\"NDist\"])\n    input_val[\"SDist\"] = z_score(input_val[\"SDist\"], input_train[\"SDist\"])\n    input_val[\"WDist\"] = z_score(input_val[\"WDist\"], input_train[\"WDist\"])\n    input_val[\"EDist\"] = z_score(input_val[\"EDist\"], input_train[\"EDist\"])\n    input_val[\"HighTemp\"] = z_score(input_val[\"HighTemp\"], input_train[\"HighTemp\"])\n    input_val[\"LowTemp\"] = z_score(input_val[\"LowTemp\"], input_train[\"LowTemp\"])\n    input_val[\"SnowFall\"] = z_score(input_val[\"SnowFall\"], input_train[\"SnowFall\"])\n    input_val[\"RainFall\"] = z_score(input_val[\"RainFall\"], input_train[\"RainFall\"])\n    input_val[\"DayLight\"] = z_score(input_val[\"DayLight\"], input_train[\"DayLight\"])\n    input_val[\"SunShine\"] = z_score(input_val[\"SunShine\"], input_train[\"SunShine\"])\n    \n    input_train[\"Latitude\"] = z_score(input_train[\"Latitude\"], input_train[\"Latitude\"])\n    input_train[\"Longitude\"] = z_score(input_train[\"Longitude\"], input_train[\"Longitude\"])\n    input_train[\"CenterDist\"] = z_score(input_train[\"CenterDist\"], input_train[\"CenterDist\"])\n    input_train[\"MinDist\"] = z_score(input_train[\"MinDist\"], input_train[\"MinDist\"])\n    input_train[\"NDist\"] = z_score(input_train[\"NDist\"], input_train[\"NDist\"])\n    input_train[\"SDist\"] = z_score(input_train[\"SDist\"], input_train[\"SDist\"])\n    input_train[\"WDist\"] = z_score(input_train[\"WDist\"], input_train[\"WDist\"])\n    input_train[\"EDist\"] = z_score(input_train[\"EDist\"], input_train[\"EDist\"])\n    input_train[\"HighTemp\"] = z_score(input_train[\"HighTemp\"], input_train[\"HighTemp\"])\n    input_train[\"LowTemp\"] = z_score(input_train[\"LowTemp\"], input_train[\"LowTemp\"])\n    input_train[\"SnowFall\"] = z_score(input_train[\"SnowFall\"], input_train[\"SnowFall\"])\n    input_train[\"RainFall\"] = z_score(input_train[\"RainFall\"], input_train[\"RainFall\"])\n    input_train[\"DayLight\"] = z_score(input_train[\"DayLight\"], input_train[\"DayLight\"])\n    input_train[\"SunShine\"] = z_score(input_train[\"SunShine\"], input_train[\"SunShine\"])\n    \n    input_train[\"Hour\"] = min_max(input_train[\"Hour\"], 0, 23)\n    input_train[\"RushHour1\"] = min_max(input_train[\"RushHour1\"], -15, 8)\n    input_train[\"RushHour2\"] = min_max(input_train[\"RushHour2\"], -6, 17)\n    input_train[\"Month\"] = min_max(input_train[\"Month\"], 1, 12)\n    \n    input_val[\"Hour\"] = min_max(input_val[\"Hour\"], 0, 23)\n    input_val[\"RushHour1\"] = min_max(input_val[\"RushHour1\"], -15, 8)\n    input_val[\"RushHour2\"] = min_max(input_val[\"RushHour2\"], -6, 17)\n    input_val[\"Month\"] = min_max(input_val[\"Month\"], 1, 12)\n    \n    input_test[\"Hour\"] = min_max(input_test[\"Hour\"], 0, 23)\n    input_test[\"RushHour1\"] = min_max(input_test[\"RushHour1\"], -15, 8)\n    input_test[\"RushHour2\"] = min_max(input_test[\"RushHour2\"], -6, 17)\n    input_test[\"Month\"] = min_max(input_test[\"Month\"], 1, 12)\n\npreprocess_data(input_train, input_val, input_test)","f52e699d":"from random import randint\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom RAdam.radam import RAdam\nfrom lookahead.lookahead import Lookahead\nfrom torchcontrib.optim import SWA\nfrom robust_loss_pytorch.adaptive import AdaptiveLossFunction","c8c49581":"BATCH_SIZE = 512","503233ba":"input_train.head()","4d83c8e5":"INTERSEC_IN = 4797\nSTREET_IN = 1831\nDIR_IN = 8\nCITY_IN = 4\nSTREET_TYPE_IN = 16","e5a82f9a":"def to_one_hot(targets, num_classes):\n    return torch.from_numpy(np.eye(num_classes)[targets]).float()\n\ndef prepare_batch(batch, training=True):\n    batch_x = {}\n\n    batch_x[\"InDir\"] = to_one_hot(batch[\"EntryHeading\"].values, DIR_IN)\n    batch_x[\"OutDir\"] = to_one_hot(batch[\"ExitHeading\"].values, DIR_IN)\n    batch_x[\"City\"] = to_one_hot(batch[\"City\"].values, CITY_IN)\n    batch_x[\"InType\"] = to_one_hot(batch[\"EntryType\"].values, STREET_TYPE_IN)\n    batch_x[\"OutType\"] = to_one_hot(batch[\"ExitType\"].values, STREET_TYPE_IN)\n    \n    batch_x[\"InStreet\"] = to_one_hot(batch[\"EntryStreetID\"].values, STREET_IN)\n    batch_x[\"OutStreet\"] = to_one_hot(batch[\"ExitStreetID\"].values, STREET_IN)\n    \n    if randint(0, 7) == 0 and training:\n        uid = batch[\"IntersectionUID\"].values\n        uid[0] = INTERSEC_IN - 1\n        batch_x[\"UID\"] = to_one_hot(uid, INTERSEC_IN)\n    else:\n        batch_x[\"UID\"] = to_one_hot(batch[\"IntersectionUID\"].values, INTERSEC_IN)\n        \n    batch_x[\"Hour\"] = torch.from_numpy(batch[\"Hour\"].values).float().unsqueeze(1)\n    batch_x[\"Weekend\"] = torch.from_numpy(batch[\"Weekend\"].values).float().unsqueeze(1)\n    batch_x[\"Month\"] = torch.from_numpy(batch[\"Month\"].values).float().unsqueeze(1)\n    batch_x[\"Latitude\"] = torch.from_numpy(batch[\"Latitude\"].values).float().unsqueeze(1)\n    batch_x[\"Longitude\"] = torch.from_numpy(batch[\"Longitude\"].values).float().unsqueeze(1)\n    \n    batch_x[\"RightTurn\"] = torch.from_numpy(batch[\"RightTurn\"].values).float().unsqueeze(1)\n    batch_x[\"LeftTurn\"] = torch.from_numpy(batch[\"LeftTurn\"].values).float().unsqueeze(1)\n    batch_x[\"PassThru\"] = torch.from_numpy(batch[\"PassThru\"].values).float().unsqueeze(1)\n    batch_x[\"UTurn\"] = torch.from_numpy(batch[\"UTurn\"].values).float().unsqueeze(1)\n    batch_x[\"Direction\"] = torch.from_numpy(batch[\"Direction\"].values).float().unsqueeze(1)\n    batch_x[\"LeftSide\"] = torch.from_numpy(batch[\"LeftSide\"].values).float().unsqueeze(1)\n    batch_x[\"RightSide\"] = torch.from_numpy(batch[\"RightSide\"].values).float().unsqueeze(1)\n    \n    batch_x[\"HighTemp\"] = torch.from_numpy(batch[\"HighTemp\"].values).float().unsqueeze(1)\n    batch_x[\"LowTemp\"] = torch.from_numpy(batch[\"LowTemp\"].values).float().unsqueeze(1)\n    batch_x[\"SnowFall\"] = torch.from_numpy(batch[\"SnowFall\"].values).float().unsqueeze(1)\n    batch_x[\"RainFall\"] = torch.from_numpy(batch[\"RainFall\"].values).float().unsqueeze(1)\n    batch_x[\"DayLight\"] = torch.from_numpy(batch[\"DayLight\"].values).float().unsqueeze(1)\n    batch_x[\"SunShine\"] = torch.from_numpy(batch[\"SunShine\"].values).float().unsqueeze(1)\n    batch_x[\"RushHour1\"] = torch.from_numpy(batch[\"RushHour1\"].values).float().unsqueeze(1)\n    batch_x[\"RushHour2\"] = torch.from_numpy(batch[\"RushHour2\"].values).float().unsqueeze(1)\n    \n    batch_x[\"CenterDist\"] = torch.from_numpy(batch[\"CenterDist\"].values).float().unsqueeze(1)\n    batch_x[\"MinDist\"] = torch.from_numpy(batch[\"MinDist\"].values).float().unsqueeze(1)\n    batch_x[\"NDist\"] = torch.from_numpy(batch[\"NDist\"].values).float().unsqueeze(1)\n    batch_x[\"SDist\"] = torch.from_numpy(batch[\"SDist\"].values).float().unsqueeze(1)\n    batch_x[\"EDist\"] = torch.from_numpy(batch[\"EDist\"].values).float().unsqueeze(1)\n    batch_x[\"WDist\"] = torch.from_numpy(batch[\"WDist\"].values).float().unsqueeze(1)\n\n    \n    if \"TotalTimeStopped_p20\" in batch:\n        batch_y = torch.from_numpy(pd.concat([batch[\"TotalTimeStopped_p20\"], batch[\"TotalTimeStopped_p50\"],\n                                                 batch[\"TotalTimeStopped_p80\"], batch[\"DistanceToFirstStop_p20\"],\n                                                 batch[\"DistanceToFirstStop_p50\"], batch[\"DistanceToFirstStop_p80\"]],\n                                                 axis=1).values).float()\n        return batch_x, batch_y\n    else:\n        return batch_x, None","d4c549fd":"batch = input_train.sample(5)\nbatch_x, batch_y = prepare_batch(batch)\nprint(batch_x.keys())","7c06b20f":"class TraficPred(nn.Module):\n    def __init__(self, dropout_rate=0.5):\n        super(TraficPred, self).__init__()\n         \n        INTERSEC_EMB_CH = 128\n        STREET_EMB_CH = 64\n        DIR_EMB_CH = 8\n        CITY_EMB_CH = 4\n        STREET_TYPE_CH = 16\n\n        NUM_OUT = 6\n        \n        self.uid_emb = nn.Linear(INTERSEC_IN, INTERSEC_EMB_CH, bias=True)\n        self.city_emb = nn.Linear(CITY_IN, CITY_EMB_CH, bias=True)\n\n        self.street_emb = nn.Linear(STREET_IN, STREET_EMB_CH, bias=True)\n        self.s_type_emb = nn.Linear(STREET_TYPE_IN, STREET_TYPE_CH, bias=True)\n\n        self.dir_emb = nn.Linear(DIR_IN, DIR_EMB_CH, bias=True)\n        \n        EMB_CH = 2 * DIR_EMB_CH + CITY_EMB_CH + 2 * STREET_TYPE_CH + INTERSEC_EMB_CH + 2 * STREET_EMB_CH\n        self.linear_block = nn.Sequential(\n                                nn.Linear(EMB_CH + 21, 256, bias=True),\n                                nn.PReLU(),\n                                nn.Linear(256, 512, bias=True),\n                                nn.PReLU(),\n                                nn.Linear(512, 256, bias=True),\n                                nn.PReLU(),\n                                nn.Dropout(dropout_rate),\n                                nn.Linear(256, NUM_OUT, bias=True),\n                                nn.ReLU())\n        \n    def forward(self, input):\n        \n        uid_emb = self.uid_emb(input[\"UID\"])\n        city_emb = self.city_emb(input[\"City\"])\n        \n        in_street_emb = self.street_emb(input[\"InStreet\"])\n        out_street_emb = self.street_emb(input[\"OutStreet\"])\n        \n        in_type_emb = self.s_type_emb(input[\"InType\"])\n        out_type_emb = self.s_type_emb(input[\"OutType\"])\n        \n        in_dir_emb = self.dir_emb(input[\"InDir\"])\n        out_dir_emb = self.dir_emb(input[\"OutDir\"])\n        \n        emb = torch.cat((uid_emb, in_dir_emb, out_dir_emb, city_emb, in_type_emb, out_type_emb,\n                                                         in_street_emb, out_street_emb), dim=1)\n        \n        x = torch.cat((emb, input[\"Latitude\"], input[\"Longitude\"], input[\"Hour\"],\n                            input[\"Weekend\"], input[\"CenterDist\"],# input[\"Month\"],\n                            #input[\"NDist\"], input[\"SDist\"], input[\"WDist\"], input[\"EDist\"],\n                            input[\"HighTemp\"], input[\"LowTemp\"], input[\"SnowFall\"],\n                            input[\"RainFall\"], input[\"DayLight\"], input[\"SunShine\"],\n                            input[\"RightTurn\"], input[\"LeftTurn\"], input[\"PassThru\"],\n                            input[\"UTurn\"], input[\"Direction\"], input[\"LeftSide\"],\n                            input[\"RightSide\"], input[\"RushHour1\"], input[\"RushHour2\"],\n                            input[\"MinDist\"]), dim=1)\n        \n        return self.linear_block(x)\n\n    \nclass MSELoss(nn.Module): #\n    def __init__(self):\n        super(MSELoss, self).__init__()\n        \n        self.mse = nn.MSELoss()\n    \n    def forward(self, input, target):\n        \n        return self.mse(input, target)\n\n    \nclass L1Loss(nn.Module): #\n    def __init__(self):\n        super(L1Loss, self).__init__()\n        \n        self.l1 = nn.L1Loss()\n    \n    def forward(self, input, target):\n        \n        return self.l1(input, target)\n\nclass BarronLoss(nn.Module): #\n    def __init__(self):\n        super(BarronLoss, self).__init__()\n            \n        self.barron = AdaptiveLossFunction(6, torch.float32, \"cpu\")\n    \n    def forward(self, input, target):\n        \n            return torch.mean(self.barron.lossfun(input - target))","d0476127":"def train(model, loss_fn, optimizer, input, num_epoch=30):\n    \n    best_loss = float(\"INF\")\n    \n    for epoch in range(num_epoch):\n        print(\"{}\/{}: LR-{}\".format(epoch+1, num_epoch,\n                        optimizer.param_groups[0][\"lr\"]))\n        \n        sum_loss = 0\n        model.train()\n\n        with tqdm(total=1000, iterable=range(1000),\n                 dynamic_ncols=False, unit=\"it\", unit_scale=True,\n                 desc=\"Training\", postfix={\"Loss\":0.0}, file=sys.stdout) as tqdm_bar:\n            \n            for it in tqdm_bar:\n                batch = input.sample(BATCH_SIZE)\n                batch_x, batch_y = prepare_batch(batch)\n\n                output = model(batch_x)\n                loss = loss_fn(output, batch_y)\n                sum_loss += loss.item()\n                \n                del batch_x, batch_y\n                \n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            \n                tqdm_bar.set_postfix(oredered_dict={\"Loss\":\"{0:.4f}\"\\\n                            .format(sum_loss \/ (it + 1))}, refres=True)\n                del output, loss\n                \n        sum_loss = 0        \n        model.eval()\n        with tqdm(total=400, iterable=range(400),\n                 dynamic_ncols=False, unit=\"it\", unit_scale=True,\n                 desc=\"Validation\", postfix={\"Loss\":0.0}, file=sys.stdout) as tqdm_bar:\n            \n            for it in tqdm_bar:\n                batch = input.sample(BATCH_SIZE)\n                batch_x, batch_y = prepare_batch(batch)\n\n                output = model(batch_x)\n                loss = loss_fn(output, batch_y)\n                sum_loss += loss.item()\n                \n                del batch_x, batch_y\n            \n                tqdm_bar.set_postfix(oredered_dict={\"Loss\":\"{0:.4f}\"\\\n                            .format(sum_loss \/ (it + 1))}, refres=True)\n                del output, loss\n\n        if sum_loss \/ 400 < best_loss:\n            model_state = {\"weights\": model.state_dict(),\n                         \"loss\": sum_loss \/ 400,\n                         \"epoch\": epoch + 1}\n            best_loss = sum_loss \/ 400\n            \n            torch.save(model_state, \"weights.pth.tar\")\n            ","602202d0":"model = TraficPred()\noptimizer = Lookahead(RAdam(model.parameters(), lr=0.001), k=5, alpha=0.5)\n#optimizer = Adam(model.parameters(), lr=0.01)\ntrain(model, MSELoss(), optimizer, input_train)","2a3179f8":"model_state = torch.load(\"weights.pth.tar\")\nprint(\"Loading weights from epoch:{}\".format(model_state[\"epoch\"]))\nmodel.load_state_dict(model_state[\"weights\"])","56b12fb2":"def pred(model, input):\n    model.eval()\n    preds = np.zeros([input.shape[0]] + [6])\n    num_samples = len(input.index)\n    with tqdm(total=num_samples\/\/BATCH_SIZE, iterable=range(num_samples\/\/BATCH_SIZE),\n             dynamic_ncols=False, unit=\"it\", unit_scale=True,\n             file=sys.stdout) as tqdm_bar:\n\n        for it in tqdm_bar:\n            batch = input[it * BATCH_SIZE:(it + 1) * BATCH_SIZE]\n            batch_x, _ = prepare_batch(batch, training=False)\n\n            output = model(batch_x)\n            out_np = output.detach().numpy()\n            preds[it * BATCH_SIZE:it * BATCH_SIZE + out_np.shape[0]] = out_np\n            del batch_x, output\n    \n    return preds","25e4b55e":"def test(pred, target):\n    dif = pred - target\n    return math.sqrt(np.mean(dif ** 2, axis=(0,1)))","d140a5e6":"columns=[\"pred1\", \"pred2\", \"pred3\", \"pred4\", \"pred5\", \"pred6\"]\n\ntrains = pd.DataFrame(data=pred(model, input_train), columns=columns, index=input_train[\"RowId\"])\nvals = pd.DataFrame(data=pred(model, input_val), columns=columns, index=input_val[\"RowId\"])\nprint(\"Training  : {}\".format(test(trains.values, input_train[[\"TotalTimeStopped_p20\", \"TotalTimeStopped_p50\",\n                                                             \"TotalTimeStopped_p80\", \"DistanceToFirstStop_p20\",\n                                                             \"DistanceToFirstStop_p50\", \"DistanceToFirstStop_p80\"]].values)))\n\nprint(\"Validation: {}\".format(test(vals.values, input_val[[\"TotalTimeStopped_p20\", \"TotalTimeStopped_p50\",\n                                                             \"TotalTimeStopped_p80\", \"DistanceToFirstStop_p20\",\n                                                             \"DistanceToFirstStop_p50\", \"DistanceToFirstStop_p80\"]].values)))","32298873":"input_train.head()","73835b84":"input_test.head()","a68a529d":"preds = pd.DataFrame(data=pred(model, input_test), columns=columns, index=input_test[\"RowId\"])","956ec2d5":"preds.head()","229823dc":"preds.mean(axis=0)","1d2fef46":"means=pd.DataFrame({})\nmeans[\"Training\"] = trains.mean(axis=0)\nmeans[\"Validation\"] = vals.mean(axis=0)\nmeans[\"Testing\"] = preds.mean(axis=0)\nmeans[\"Index\"] = means.index\nmeans = means.melt(id_vars=\"Index\", value_vars=[\"Training\", \"Validation\", \"Testing\"], )\nsns.factorplot(x=\"Index\", y=\"value\", hue=\"variable\", data=means, kind=\"bar\")","38480673":"def rmse(pred, target):\n    dif = pred - target\n    return np.sqrt(np.mean(dif ** 2, axis=(1)))\n\ninput_train[\"Error\"] = rmse(trains.values, input_train[[\"TotalTimeStopped_p20\", \"TotalTimeStopped_p50\",\n                                                             \"TotalTimeStopped_p80\", \"DistanceToFirstStop_p20\",\n                                                             \"DistanceToFirstStop_p50\", \"DistanceToFirstStop_p80\"]].values)\n\ninput_train[\"ErrorDistance\"] = rmse(trains[[\"pred4\", \"pred5\", \"pred6\"]].values, input_train[[\"DistanceToFirstStop_p20\",\n                                                             \"DistanceToFirstStop_p50\", \"DistanceToFirstStop_p80\"]].values)\n\ninput_train[\"ErrorDistance80\"] = rmse(trains[[\"pred6\"]].values, input_train[[\"DistanceToFirstStop_p80\"]].values)\n\ninput_train[\"ErrorTime\"] = rmse(trains[[\"pred1\", \"pred2\", \"pred3\"]].values, input_train[[\"TotalTimeStopped_p20\",\n                                                        \"TotalTimeStopped_p50\", \"TotalTimeStopped_p80\"]].values)","973867f4":"def plot_map_error(df, city, error):\n    count=df.groupby(['City','Latitude','Longitude'])[error].mean().reset_index()\n    \n    fig = px.scatter_mapbox(count[count[\"City\"]==city], \n                            lat=\"Latitude\", lon=\"Longitude\",size=error,size_max=10,\n                            color=error, color_continuous_scale=px.colors.sequential.Inferno, zoom=11)\n    fig.update_layout(mapbox_style=\"stamen-terrain\")\n    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n    fig.show()","606a70a9":"plot_map_error(input_train, 0, \"Error\")","f510de17":"plot_map_error(input_train, 0, \"ErrorDistance80\")","33ac0a2d":"plot_map_error(input_train, 0, \"ErrorTime\")","1d93a9b4":"preds = preds.values.flatten()\n\nsub  = pd.read_csv(\"..\/input\/bigquery-geotab-intersection-congestion\/sample_submission.csv\")\nsub[\"Target\"] = preds.tolist()\nsub.to_csv(\"pred.csv\",index = False)","8d95f5f4":"## Add Turn direction","0aac92f5":"## Create UID Intersection","abc737c2":"## Map","5459a201":"## Rush Hours","73140811":"## Add Street Type","86028e64":"## Create Street ID","0590b685":"## Distance to City Center","707b53af":"## Add Climate Data [1](https:\/\/www.weather-us.com\/en\/)","695a1897":"## School break"}}