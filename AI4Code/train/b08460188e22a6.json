{"cell_type":{"9f48b852":"code","56966428":"code","872ab349":"code","8f317972":"code","01253339":"code","a602ad08":"code","a7cc32ee":"code","c1ead9cd":"code","b95d953c":"code","017fc28d":"code","0bc7b859":"code","970063a9":"code","bc86419a":"code","582f5bb0":"code","2b75cc3f":"code","63e3fcab":"code","2d67c659":"code","e937615b":"code","0e097b3b":"code","faac8fb9":"code","374fabc1":"code","771dc329":"code","c5a19492":"code","e66c358a":"code","9189aa19":"code","8d20239b":"code","e19b0f00":"code","db1739e2":"code","5e9369ba":"code","4f5a62e5":"code","175bee6a":"code","18815e9b":"code","de9ad93a":"code","c1155d6f":"code","770c2d5c":"code","6c3082dc":"code","e3d6bc24":"code","158f1e13":"code","ab11c2eb":"code","3fe3bde5":"code","059bbba7":"code","60c4fe0d":"code","aef6f6eb":"code","c87bac59":"code","9b86edce":"code","a17c850b":"code","789cf068":"code","64b0960a":"markdown","fd02958d":"markdown","9d99cef8":"markdown","b2bed704":"markdown","3e6a7eb4":"markdown","19887376":"markdown","9e2f9c88":"markdown","06442243":"markdown","35f440b6":"markdown","c2b11775":"markdown","72d3ab97":"markdown","ea0fbf46":"markdown","f34b6399":"markdown","28ca8ef3":"markdown","5cb208e0":"markdown","cc5b7133":"markdown","9a46bd13":"markdown","f2c424ec":"markdown","951d8f03":"markdown","04f85034":"markdown","8b0463e0":"markdown","56399fe4":"markdown","2a01afd0":"markdown","4cef83d0":"markdown","d866acb7":"markdown"},"source":{"9f48b852":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n","56966428":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.tree import DecisionTreeRegressor \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.metrics import mean_squared_error as mse","872ab349":"import seaborn as sns","8f317972":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","01253339":"nRowsRead = 1259 # specify 'None' if want to read whole file\n# MagicBricks.csv may have more rows in reality, but we are only loading\/previewing the first 1000 rows\ndf1 = pd.read_csv('\/kaggle\/input\/MagicBricks.csv', delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'MagicBricks.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","a602ad08":"df1.head(5)","a7cc32ee":"df1.describe()","c1ead9cd":"sns.countplot(df1['BHK'])","b95d953c":"df1['BHK'].value_counts()","017fc28d":"df1[df1['BHK']==6]","0bc7b859":"df1.drop([721,345,163,164,261,352,353,585],inplace=True)  ##### these are indexes of rows, being removed from dataset","970063a9":"sns.countplot(df1['Bathroom'])","bc86419a":"df1[df1['Bathroom']==6]","582f5bb0":"df1[df1['Bathroom']==7]","2b75cc3f":"df1.drop([225,495,527,659,676,681,1211,248,1029],inplace=True)","63e3fcab":"df1.Furnishing.value_counts().plot.bar()","2d67c659":"plt.figure(figsize=(14,7))\nsns.boxplot(x=df1.Furnishing,y=df1.Price)","e937615b":"df1.isnull().sum()","0e097b3b":"df1.Parking.fillna(0,inplace=True)","faac8fb9":"sns.countplot(df1.Parking)","374fabc1":"df1[df1.Parking==39]","771dc329":"df1['Parking'].replace([39,114],1,inplace=True)\ndf1['Parking'].replace([5,9,10],4,inplace=True)","c5a19492":"sns.countplot(df1.Status)","e66c358a":"sns.countplot(df1['Type'])","9189aa19":"sns.boxplot(x=df1.Transaction,y=df1.Price)","8d20239b":"plt.figure(figsize=(14,7))\nsns.scatterplot(x=df1.Area,y=df1.Price)","e19b0f00":"df1.drop('Per_Sqft',axis=1,inplace=True)","db1739e2":"df1.isnull().sum()","5e9369ba":"df1.Bathroom.fillna(df1.Bathroom.median(),inplace=True)\ndf1.Type.fillna('Apartment',inplace=True)\ndf1.Furnishing.fillna('Semi-Furnished',inplace=True)","4f5a62e5":"df1.Locality.unique()","175bee6a":"df1.drop('Locality',axis=1,inplace=True)","18815e9b":"df1 = pd.get_dummies(df1)\n","de9ad93a":"Y = df1.Price\nX = df1.drop('Price',axis=1)","c1155d6f":"x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.2,random_state = 42)","770c2d5c":"x_train","6c3082dc":"lr = LinearRegression()\nlr.fit(x_train,y_train)   ########### traing model\npred = lr.predict(x_test) ########### Getting predictions","e3d6bc24":"pred","158f1e13":"from math import sqrt","ab11c2eb":"print(sqrt(mse(y_test,pred)))  ######## Root Mean Square Error ","3fe3bde5":"lr = DecisionTreeRegressor()\nlr.fit(x_train,y_train)\npred = lr.predict(x_test)\nprint(sqrt(mse(y_test,pred)))","059bbba7":"sns.lineplot(x=df1.Area,y=df1.Price)","60c4fe0d":"plt.figure(figsize=(14,7))\nsns.scatterplot(x=df1.Area,y=df1.Price)","aef6f6eb":"p = np.array(df1[df1.Area>5000].index)","c87bac59":"df1.drop(p,inplace=True)  ##### these are indexes of rows, being removed from dataset","9b86edce":"plt.figure(figsize=(14,7))\nsns.regplot(x=\"Area\", y=\"Price\", data=df1)","a17c850b":"Y = df1.Price\nX = df1.drop('Price',axis=1)\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.2,random_state = 42)\nlr = DecisionTreeRegressor()\nlr.fit(x_train,y_train)\npred = lr.predict(x_test)\nprint(sqrt(mse(y_test,pred)))","789cf068":"Y = df1.Price\nX = df1.drop('Price',axis=1)\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.2,random_state = 42)\nlr = LinearRegression()\nlr.fit(x_train,y_train)\npred = lr.predict(x_test)\nprint(sqrt(mse(y_test,pred)))","64b0960a":"**So far, all good. \nLocalities are essential for any property, infact after 'Area', 'locality' is the characteristic which matter most in property sale\/purchase. for example:- if you are buying a house of area 200 sqft in locality like 'Seelampur' it may cost you about 2 Lac (say) 1000 Rs per square feet but on the contrary in 'Rohini' it may cost you 20 Lac, 10000 Rs per square feet.   **","fd02958d":"As, I anticipated there is a positive correlation between 'Area' and 'Price. \n","9d99cef8":"And there is a definitely a relation between 'Area','Price' and 'Per_Sqft', which is\n'Per_Sqft' = 'Price'\/'Area'. \n\n**Multicolinearity : if two or more features(except target variable) are highly correlated on each other,this phenomena is known as multicolinearity. we don't want that to happen as it just create redundancy in dataset.**\n\nso, we can eliminate one of the variable, as to make model less complex and that variable would be 'Per_Sqft' because it contains 240 NAN values.","b2bed704":"As, we can see,in the countplot and count_values there is only 1 property where 'BHK' is 7 and 10. And 6 rows have 6 'BHK'. These are some rare values that i don't want my model to train on. so the best option is to remove them from dataset.","3e6a7eb4":"I'd like to get rid of all the 'Areas' greater than 5000. ","19887376":"These are DDA flats of Narela and price of each apartment is very low, and 39 must be the sum of all parkings available so we can safely assume every flat has one parking alloted. we are repalcing 39 as 1. similary 114 is the sum of all the parkings available,so we will replace 114 by 1.","9e2f9c88":"There is 1 csv file in the current version of the dataset:\n","06442243":"Let's take a quick look at what the data looks like:","35f440b6":"Before moving on, we need to convert all categorical(where data type is object) features into numerical features,because machine learning algorithm doesn't work on strings or other objects.\nHere i am using one hot encoding , since we don't have too many category in a feature for example :- in 'Type' we have 'Apartment' and 'Builder-Floor'. so one-hot-encoding would be right choice.","c2b11775":"### Let's check 1st file: \/kaggle\/input\/MagicBricks.csv","72d3ab97":"Here, I want to remove some outliers, we'll see how does it affect RMSE","ea0fbf46":"## Conclusion\nWe are still getting 8 digit no. but we have minimized it a lot.\nI have few more things to try, but i don't want to make it too large for a starter code.\nI hope you like it and please share your feedbacks\/suggestion to improve it. Thanks ","f34b6399":"This is pretty obvious, right?","28ca8ef3":"8 digit no. but less than the previous, we are making progress. i guess ","5cb208e0":"wow, we've got 39 and 114 parking for some apartments, which is definitely an error. Now the problem is that we can not just remove all the rows we have , 'cause we already facing some serious shortage of data, removing them won't do any good. \n","cc5b7133":"33 Null values in Parking, which means, No Parking available in the apartment. so we can impute NAN by 0\n","9a46bd13":"similarly, we are removing Bathrooms, where Bathrooms are 6 and 7","f2c424ec":"Using Linear Regression, my first choice for this project, Intiutive right?","951d8f03":"Dividing data in 80% training and 20% testing parts.usally i keep it 70 & 30.","04f85034":"**Regression Plot**","8b0463e0":"## Exploratory Analysis\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using `matplotlib`. Depending on the data, not all plots will be made.","56399fe4":"Too many null values, we've got to impute them by mean, meadian or mode","2a01afd0":"Let's try Decision tree regressor","4cef83d0":"This is too messy. For the time being,I would remove this from dataset. I can't do anything with this feature right now. Always open to suggestions\/feedback.","d866acb7":"This is scary(8 digit no.) as an error"}}