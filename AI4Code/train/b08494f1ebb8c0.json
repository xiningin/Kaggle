{"cell_type":{"75455600":"code","4d3b6f4a":"code","fff2f397":"code","4c976e85":"code","36d26f22":"code","a740cd56":"code","0cafbc8e":"code","19066856":"code","41e32457":"code","215f15e9":"code","726cb027":"code","9ba182d5":"code","e59f8293":"code","f9d3a0a6":"code","477669fe":"code","841e439e":"code","ba531ce9":"code","773dc668":"code","0a9e64ab":"code","69ad9f28":"code","474ac295":"markdown","ee95f565":"markdown","15633cdb":"markdown","3d51148c":"markdown","3809ab41":"markdown","55197c10":"markdown","27b5fb6a":"markdown","18041511":"markdown","e0b77843":"markdown","01e12ecb":"markdown","b4c8c68b":"markdown","97bf9e26":"markdown","71ad1741":"markdown"},"source":{"75455600":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as mp\nfrom sklearn.model_selection import cross_val_score\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\npd.set_option('max_columns', 1000)\npd.set_option('max_rows', 10)","4d3b6f4a":"data= pd.read_csv('..\/input\/clustering_data.csv',encoding='latin1')","fff2f397":"y = data.IS_BOUND\nx = data.drop('IS_BOUND', axis=1)\n\nfrom sklearn.preprocessing import LabelEncoder\nL = LabelEncoder()\nfor i in x.columns:\n    if x[i].dtypes == 'object':\n        x[i] = L.fit_transform(x[i].astype(str))\n        \nfrom scipy.stats import pearsonr\nfor I in x.columns:\n   print(I,pearsonr(x[i],y))\n\nfrom sklearn.feature_selection import chi2, SelectKBest\na = SelectKBest(score_func=chi2,k=10)\nx1 = a.fit_transform(x, y)\nc=['VEHICLEMAKE','VEHICLEMODEL','ANNUAL_KM','COMMUTE_DISTANCE','Value','POSTAL_CODE','AREA_CODE','MULTI_PRODUCT','age','province']\nx1=pd.DataFrame(x1,columns=c)\n\nfrom sklearn.preprocessing import StandardScaler\na = StandardScaler ()\nx2= a.fit_transform(x1)\nx2=pd.DataFrame(x2,columns=x1.columns)\n\nfrom sklearn.preprocessing import MinMaxScaler\na = MinMaxScaler ()\nx3= a.fit_transform(x2)\nx3=pd.DataFrame(x3,columns=x2.columns)\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x2, y, test_size=0.2, shuffle=True)\n","4c976e85":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(class_weight='balanced')\nparameters = {'max_depth':range(2,6) , 'min_samples_leaf':range(20,50,10),'max_leaf_nodes':range(10,15)}\nfrom sklearn.model_selection import GridSearchCV\nmodel = GridSearchCV(tree, parameters, scoring='accuracy', return_train_score=True)\nmodel.fit(x_train, y_train)\nfrom sklearn import metrics\ny_pred = model.predict(x_test)\nprint(model.score(x_test, y_test))\nprint(metrics.confusion_matrix(y_test,y_pred))\nprint(metrics.mean_squared_error(y_test,y_pred))","36d26f22":"from sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import make_blobs\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier","a740cd56":"model = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nprint(model.score(x_test, y_test))\nprint(metrics.confusion_matrix(y_test,y_pred))\nprint(metrics.mean_squared_error(y_test,y_pred))","0cafbc8e":"importance = model.feature_importances_\nheaders = x2.columns\n\nimport matplotlib.pyplot as mp\nmp.figure('Feature Importance', facecolor='lightgray',figsize=(25,7))\nmp.title('Car Insurance')\nmp.ylabel('Feature Importance')\nmp.grid(linestyle=\":\")\nsorted_indexes = importance.argsort()[::-1]\nx = np.arange(headers.size)\nmp.bar(x, importance[sorted_indexes], 0.5, color='dodgerblue')\nmp.xticks(x, headers[sorted_indexes])\nmp.legend()","19066856":"model= RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=2, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n            oob_score=False, random_state=0, verbose=0, warm_start=False)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nprint(model.score(x_test, y_test)) \nprint(metrics.confusion_matrix(y_test,y_pred))\nprint(metrics.mean_squared_error(y_test,y_pred))","41e32457":"importance = model.feature_importances_\nheaders = x2.columns\n\nimport matplotlib.pyplot as mp\nmp.figure('Feature Importance', facecolor='lightgray',figsize=(25,7))\nmp.title('Car Insurance')\nmp.ylabel('Feature Importance')\nmp.grid(linestyle=\":\")\nsorted_indexes = importance.argsort()[::-1]\nx = np.arange(headers.size)\nmp.bar(x, importance[sorted_indexes], 0.5, color='g')\nmp.xticks(x, headers[sorted_indexes])\nmp.legend()\n","215f15e9":"from sklearn.ensemble import AdaBoostClassifier\nmodel = AdaBoostClassifier(n_estimators=100)\nscores = cross_val_score(model,x_train, y_train)\nprint('accuracy\uff1a',scores.mean())","726cb027":"from sklearn import datasets\nfrom sklearn.ensemble import GradientBoostingClassifier\n \nclf = GradientBoostingClassifier(n_estimators=500, learning_rate=1.0,max_depth=1, random_state=0)\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.utils import shuffle\n\nmodel = GradientBoostingRegressor(n_estimators=500, learning_rate=0.01,max_depth=4,min_samples_split=2,loss='ls')\nmodel.fit(x_train, y_train)\nprint('MSE\uff1a',mean_squared_error(y_test, model.predict(x_test)))\n\nmp.plot(np.arange(500), model.train_score_, 'b-') \nmp.show()","9ba182d5":"importance = model.feature_importances_\nheaders = x2.columns\n\nimport matplotlib.pyplot as mp\nmp.figure('Feature Importance', facecolor='lightgray',figsize=(25,7))\nmp.title('Car Insurance')\nmp.ylabel('Feature Importance')\nmp.grid(linestyle=\":\")\nsorted_indexes = importance.argsort()[::-1]\nx = np.arange(headers.size)\nmp.bar(x, importance[sorted_indexes], 0.5, color='k')\nmp.xticks(x, headers[sorted_indexes])\nmp.legend()","e59f8293":"model = ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nprint(model.score(x_test, y_test)) \nprint(metrics.confusion_matrix(y_test,y_pred))\nprint(metrics.mean_squared_error(y_test,y_pred))","f9d3a0a6":"importance = model.feature_importances_\nheaders = x2.columns\n\nimport matplotlib.pyplot as mp\nmp.figure('Feature Importance', facecolor='lightgray',figsize=(25,7))\nmp.title('Car Insurance')\nmp.ylabel('Feature Importance')\nmp.grid(linestyle=\":\")\nsorted_indexes = importance.argsort()[::-1]\nx = np.arange(headers.size)\nmp.bar(x, importance[sorted_indexes], 0.5, color='r')\nmp.xticks(x, headers[sorted_indexes])\nmp.legend()","477669fe":"from sklearn.linear_model import LogisticRegression  \nmodel= LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, \n    fit_intercept=True, intercept_scaling=1, class_weight=None, \n    random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', \n    verbose=0, warm_start=False, n_jobs=1)  \nmodel.fit(x_train, y_train)  \ny_pred = model.predict(x_test)\nprint(model.score(x_test, y_test)) \nprint(metrics.confusion_matrix(y_test,y_pred))\nprint(metrics.mean_squared_error(y_test,y_pred))","841e439e":"from sklearn.neural_network import MLPClassifier\nmodel=MLPClassifier(solver='lbfgs',alpha=1e-5,hidden_layer_sizes=(13,13,13),max_iter=500,random_state=1)\nmodel.fit(x_train,y_train)\ny_pred = model.predict(x_test)\nprint(model.score(x_test, y_test)) \nprint(metrics.confusion_matrix(y_test,y_pred))\nprint(metrics.mean_squared_error(y_test,y_pred))","ba531ce9":"y = data.IS_BOUND\nfrom sklearn.preprocessing import MinMaxScaler\na = MinMaxScaler()\nx3= a.fit_transform(x2)\nx3=pd.DataFrame(x3,columns=x2.columns)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x3, y, test_size=0.2, shuffle=True)\n\nfrom sklearn.naive_bayes import GaussianNB \nmodel = GaussianNB(priors=None)\nmodel.fit(x_train,y_train)\ny_pred = model.predict(x_test)\nprint(model.score(x_test, y_test)) \nprint(metrics.confusion_matrix(y_test,y_pred))\nprint(metrics.mean_squared_error(y_test,y_pred))","773dc668":"y = data.IS_BOUND\nfrom sklearn.preprocessing import MinMaxScaler\na = MinMaxScaler()\nx3= a.fit_transform(x2)\nx3=pd.DataFrame(x3,columns=x2.columns)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x3, y, test_size=0.2, shuffle=True)\n\nfrom sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB(alpha=1.0,fit_prior=True,class_prior=None)\nmodel.fit(x_train,y_train)\ny_pred = model.predict(x_test)\nprint(model.score(x_test, y_test)) \nprint(metrics.confusion_matrix(y_test,y_pred))\nprint(metrics.mean_squared_error(y_test,y_pred))","0a9e64ab":"def main():\n    from sklearn.neighbors import KNeighborsClassifier\n    best_k=-1\n    best_score=0\n    for i in range(1,30):\n        knn_clf=KNeighborsClassifier(n_neighbors=i)\n        knn_clf.fit(x_train,y_train)\n        scores=knn_clf.score(x_test,y_test)\n        if scores>best_score:\n            best_score=scores\n            best_k=i\n    print('bset k:%d,best score:%.4f'%(best_k,best_score))\nif __name__ == '__main__':\n    main()","69ad9f28":"from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors = 22)\nmodel.fit(x_train, y_train)\nprint(model.score(x_test, y_test)) \nprint(metrics.mean_squared_error(y_test,y_pred))","474ac295":"1. **identify input variables and target variable**\n1. **Feature preprocessing-label encoding**\n1. **feature selection - Correlation and Chi-Square**\n1. **feature scalling(mean=0,std=1)**\n1. **feature scalling(mean=0,std=1)**\n1. **data split(train and test data)**\n\n","ee95f565":"**Nerual Network(MLPClassifier)**","15633cdb":"**KNN(k-nearest neighbors algorithm)**","3d51148c":"**Decision tree**","3809ab41":"**Navie Bayes(MultinomialNB)**","55197c10":"![image.png](attachment:image.png)","27b5fb6a":"**Navie Bayes(GaussianNB)**","18041511":"**Gradient Tree Boosting**","e0b77843":"**Random forest**","01e12ecb":"** Extra tree classification**","b4c8c68b":"**Adaboosting**","97bf9e26":"**Logistic Regression**","71ad1741":"**Decision tree and automatically optimize the parameters(GridSearchCV)**"}}