{"cell_type":{"9cb4d1ce":"code","b0f708f1":"code","1860f860":"code","8f898871":"code","f94cf020":"code","51b62cad":"code","8120ad4c":"code","ce1078e6":"code","fc5a74d9":"code","99f449f5":"code","6625312c":"code","cb207063":"code","1d300432":"code","86d3f52a":"code","8becf2a8":"code","786b55dd":"code","04d0509b":"code","235a1b36":"code","f63a9a97":"code","1a37a100":"code","f9b4b930":"code","be6e5b0e":"code","0130974e":"code","2d9fb0c8":"code","85e1dee7":"code","d3797956":"code","8130b1ef":"code","0856cec2":"markdown","62c69603":"markdown","81d74341":"markdown","3b603dd7":"markdown","e6db8eb6":"markdown","a87afdea":"markdown","25e3bd80":"markdown"},"source":{"9cb4d1ce":"import sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\nsys.path.append('..\/input\/pytorch-optimizers\/')","b0f708f1":"import os\nimport gc\nimport cv2\nimport copy\nimport time\nimport yaml\nimport random\nimport shutil\nimport pickle\nimport warnings\nimport subprocess\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom glob import glob\nfrom tqdm import tqdm\nfrom PIL import Image, ImageDraw\nfrom shutil import copyfile\nfrom IPython.core.display import Video, display\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, KFold, GroupKFold, StratifiedKFold\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport timm\nimport torch\nfrom torch import nn\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom fastai.vision.all import *\n\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import Resize as albResize\nfrom albumentations import (\n    Compose, OneOf, Transpose, RandomResizedCrop, CenterCrop, \n    Rotate, RandomRotate90, ShiftScaleRotate, Flip, HorizontalFlip, VerticalFlip,\n    HueSaturationValue, RandomBrightnessContrast, RGBShift, ChannelShuffle,\n    Normalize, Blur, MotionBlur, Cutout, CoarseDropout)\n\nwarnings.simplefilter('ignore')\npd.set_option(\"max_columns\", 150)\npd.set_option('display.max_rows', 150)","1860f860":"CFG = {\n    \"submit\"      : True,\n    \"stacking\"    : False,\n    \"seed\"        : 42,\n    'device'      : \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n    \"input_trimg\" : '..\/input\/petfinder-pawpularity-score\/train\/',\n    \"input_trpath\": '..\/input\/petfinder-smogn-dataset\/train_drop_duplicated.csv',\n    \"input_teimg\" : '..\/input\/petfinder-pawpularity-score\/test\/',\n    \"input_tepath\": '..\/input\/petfinder-pawpularity-score\/test.csv',\n    \"output_path\" : '.\/',\n    \"db_model\"    : \"swin_large_patch4_window7_224_in22k\",\n    \"db_size\"     : 224,\n    \"models\"      : [[False, 224, 999, 1,  \"swin_large_patch4_window7_224_in22k\", \"..\/input\/petfinder-cnn-models-with-pretrained-edata\/petfinder_swin_binary_ss2_meta_0.pt\"],\n                     [False, 224, 999, 1,  \"swin_large_patch4_window7_224_in22k\", \"..\/input\/petfinder-cnn-models-with-pretrained-edata\/petfinder_swin_binary_ss2_meta_1.pt\"],\n                     [False, 224, 999, 1,  \"swin_large_patch4_window7_224_in22k\", \"..\/input\/petfinder-cnn-models-with-pretrained-edata\/petfinder_swin_binary_ss2_meta_2.pt\"],\n                     [False, 224, 999, 1,  \"swin_large_patch4_window7_224_in22k\", \"..\/input\/petfinder-cnn-models-with-pretrained-edata\/petfinder_swin_binary_ss2_meta_3.pt\"],\n                     [True,  512,   0, 1,                \"tf_efficientnet_b5_ns\", \"..\/input\/petfinder-fastai-models-pseudolabel-1\/petfinder_effnet_binary_fastai_ss_0.pth\"],\n                     [True,  512,   0, 1,                \"tf_efficientnet_b5_ns\", \"..\/input\/petfinder-fastai-models-pseudolabel-1\/petfinder_effnet_binary_fastai_ss_1.pth\"],\n                     [True,  512,   0, 1,                \"tf_efficientnet_b5_ns\", \"..\/input\/petfinder-fastai-models-pseudolabel-1\/petfinder_effnet_binary_fastai_ss_2.pth\"],\n                     [True,  512,   0, 1,                \"tf_efficientnet_b5_ns\", \"..\/input\/petfinder-fastai-models-pseudolabel-1\/petfinder_effnet_binary_fastai_ss_3.pth\"],\n                     [True,  384,   0, 1,                \"beit_base_patch16_384\", \"..\/input\/petfinder-fastai-models-pseudolabel-3\/petfinder_beit_binary_fastai_0.pth\"],\n                     [True,  384,   0, 1,                \"beit_base_patch16_384\", \"..\/input\/petfinder-fastai-models-pseudolabel-3\/petfinder_beit_binary_fastai_1.pth\"],\n                     [True,  384,   0, 1,                \"beit_base_patch16_384\", \"..\/input\/petfinder-fastai-models-pseudolabel-3\/petfinder_beit_binary_fastai_2.pth\"],\n                     [True,  384,   0, 1,                \"beit_base_patch16_384\", \"..\/input\/petfinder-fastai-models-pseudolabel-3\/petfinder_beit_binary_fastai_3.pth\"],\n                     [True,  224,   0, 1,                \"vit_large_patch16_224\", \"..\/input\/petfinder-fastai-models-pseudolabel-4\/petfinder_vit_binary_fastai_mixup_0.pth\"],\n                     [True,  224,   0, 1,                \"vit_large_patch16_224\", \"..\/input\/petfinder-fastai-models-pseudolabel-4\/petfinder_vit_binary_fastai_mixup_1.pth\"],\n                     [True,  224,   0, 1,                \"vit_large_patch16_224\", \"..\/input\/petfinder-fastai-models-pseudolabel-4\/petfinder_vit_binary_fastai_mixup_2.pth\"],\n                     [True,  224,   0, 1,                \"vit_large_patch16_224\", \"..\/input\/petfinder-fastai-models-pseudolabel-4\/petfinder_vit_binary_fastai_mixup_3.pth\"],\n                     [True,  384,   0, 1,                         \"cait_s24_384\", \"..\/input\/petfinder-fastai-models-pseudolabel-4\/petfinder_cait_binary_fastai_mixup_0.pth\"],\n                     [True,  384,   0, 1,                         \"cait_s24_384\", \"..\/input\/petfinder-fastai-models-pseudolabel-4\/petfinder_cait_binary_fastai_mixup_1.pth\"],\n                     [True,  384,   0, 1,                         \"cait_s24_384\", \"..\/input\/petfinder-fastai-models-pseudolabel-4\/petfinder_cait_binary_fastai_mixup_2.pth\"],\n                     [True,  384,   0, 1,                         \"cait_s24_384\", \"..\/input\/petfinder-fastai-models-pseudolabel-4\/petfinder_cait_binary_fastai_mixup_3.pth\"],\n                     [True,  224,   0, 1,  \"swin_large_patch4_window7_224_in22k\", \"..\/input\/petfinder-fastai-models-pseudolabel-5\/petfinder_swin_binary_fastai_highlr_0.pth\"],\n                     [True,  224,   0, 1,  \"swin_large_patch4_window7_224_in22k\", \"..\/input\/petfinder-fastai-models-pseudolabel-5\/petfinder_swin_binary_fastai_highlr_1.pth\"],\n                     [True,  224,   0, 1,  \"swin_large_patch4_window7_224_in22k\", \"..\/input\/petfinder-fastai-models-pseudolabel-5\/petfinder_swin_binary_fastai_highlr_2.pth\"],\n                     [True,  224,   0, 1,  \"swin_large_patch4_window7_224_in22k\", \"..\/input\/petfinder-fastai-models-pseudolabel-5\/petfinder_swin_binary_fastai_highlr_3.pth\"],\n                     [True,  224,   0, 1,  \"swin_large_patch4_window7_224_in22k\", \"..\/input\/petfinder-fastai-models-pseudolabel-2\/petfinder_swin_binary_fastai_ss_0.pth\"],\n                     [True,  224,   0, 1,  \"swin_large_patch4_window7_224_in22k\", \"..\/input\/petfinder-fastai-models-pseudolabel-2\/petfinder_swin_binary_fastai_ss_1.pth\"],\n                     [True,  224,   0, 1,  \"swin_large_patch4_window7_224_in22k\", \"..\/input\/petfinder-fastai-models-pseudolabel-2\/petfinder_swin_binary_fastai_ss_2.pth\"],\n                     [True,  224,   0, 1,  \"swin_large_patch4_window7_224_in22k\", \"..\/input\/petfinder-fastai-models-pseudolabel-2\/petfinder_swin_binary_fastai_ss_3.pth\"]],\n    \"tta\"         : 2,  # If set over 2, tta will run\n    \"batch_size\"  : 1,\n    \"num_workers\" : 4\n}\n\nprint(f\"# of models is {int(len(CFG['models'])\/4)} with fold 4 and tta {CFG['tta']}.\")\nprint(f\"Total of inference will be {len(CFG['models']) * CFG['tta']}.\")\nCFG","8f898871":"def get_img(path):\n    im_bgr = cv2.imread(path)\n    im_rgb = im_bgr[:, :, ::-1]\n    return im_rgb\n\ndef my_sigmoid(a):\n    return 1 \/ (1 + np.exp(-a))\n\ndef softmax(x):\n    max = np.max(x,axis=1,keepdims=True)\n    e_x = np.exp(x - max)\n    sum = np.sum(e_x,axis=1,keepdims=True)\n    return e_x \/ sum \n\ndef seed_everything(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nseed_everything(CFG[\"seed\"])","f94cf020":"df_train = pd.read_csv(CFG['input_trpath'])\ndf_train[\"path\"] = [f\"{CFG['input_trimg']}{i}.jpg\" for i in df_train.Id]\ndf_train[\"Pawclass\"] = df_train.Pawpularity \/ 100\n\nif CFG[\"submit\"]:\n    df_test = pd.read_csv(CFG['input_tepath'])\n    df_test[\"path\"] = [f\"{CFG['input_teimg']}{i}.jpg\" for i in df_test.Id]\nelse:\n    df_test = pd.read_csv(CFG['input_trpath'])\n    df_test[\"path\"] = [f\"{CFG['input_trimg']}{i}.jpg\" for i in df_test.Id]\n\nmeta_features = [c for c in df_test.columns if c not in [\"Id\",\"path\", \"Pawpularity\"]]\n\nprint(meta_features)\nprint(df_test.shape)\ndf_test.head(2)","51b62cad":"df_test.describe()","8120ad4c":"def load_model(path):\n    try:\n        checkpoint = torch.load(path, map_location='cpu')\n    except Exception as err:\n        print(err)\n        return None\n    model = models.densenet121(pretrained=False)\n    model.classifier = nn.Sequential(\n        nn.Linear(1024, 512),\n        nn.ReLU(),\n        nn.Dropout(0.2),\n        nn.Linear(512, 256),\n        nn.ReLU(),\n        nn.Dropout(0.1),\n        nn.Linear(256, 2)\n    )\n    model.parameters = checkpoint['parameters']\n    model.load_state_dict(checkpoint['state_dict'])\n    return model","ce1078e6":"def image_transform(imagepath):\n    test_transforms = transforms.Compose([transforms.Resize(255),\n                                          transforms.CenterCrop(224),\n                                          transforms.ToTensor(),\n                                          transforms.Normalize([0.485, 0.456, 0.406],\n                                                               [0.229, 0.224, 0.225])])\n    image = Image.open(imagepath)\n    imagetensor = test_transforms(image)\n    return imagetensor\n\nclass PetFinderDataset(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index: int):\n        img = image_transform(self.df.loc[index].path)\n        return img","fc5a74d9":"def inference_one_epoch(model, data_loader, device):\n    model.eval()\n\n    img_preds_all = []\n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (imgs) in pbar:\n        imgs = imgs.to(device).float()\n        img_preds = model(imgs)\n        img_preds_all += [img_preds.detach().cpu().numpy()]\n        \n    img_preds_all = np.concatenate(img_preds_all, axis=0)\n    return img_preds_all","99f449f5":"inference_ds = PetFinderDataset(df_test)\ndata_loader  = torch.utils.data.DataLoader(inference_ds,\n                                           batch_size=CFG['batch_size'],\n                                           drop_last=False,\n                                           pin_memory=False,\n                                           shuffle=False,\n                                           num_workers=CFG['num_workers'])\nmodel = load_model(\"..\/input\/cat-vs-dog-model\/cat-v-dog-classifier-pytorch-master\/models\/catvdog.pth\")\nmodel.to(CFG[\"device\"])\nwith torch.no_grad():\n    res_cat_dog = inference_one_epoch(model, data_loader, CFG[\"device\"])\n\ndel model, inference_ds, data_loader\ntorch.cuda.empty_cache()","6625312c":"df_test[[\"cat\",\"dog\"]] = softmax(res_cat_dog)\nmeta_features += [\"cat\",\"dog\"]\n\nprint(df_test.shape)\ndf_test.head(2)","cb207063":"class SwinModel(nn.Module):\n    def __init__(self, model_name, pretrained=True):\n        super().__init__()\n        self.model   = timm.create_model(model_name, pretrained=pretrained, num_classes=0, in_chans=3)\n        num_features = self.model.num_features\n        self.linear  = nn.Linear(num_features, 120)\n\n    def forward(self, x):\n        x = self.model(x)\n        output = self.linear(x)\n        return output","1d300432":"class PetFinderDataset(Dataset):\n    def __init__(self, df, transforms=None):\n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        self.transforms  = transforms\n        \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index: int):\n        img  = get_img(self.df.loc[index].path).copy()\n        if self.transforms:\n            img = self.transforms(image=img)['image']\n        return img\n    \ndef get_inference_transforms():\n    return Compose([\n        albResize(CFG[\"db_size\"], CFG[\"db_size\"], p=1.0),\n        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n        ToTensorV2(p=1.0)], p=1.0)","86d3f52a":"def inference_one_epoch(model, data_loader, device):\n    model.eval()\n\n    image_preds_all = []\n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (imgs) in pbar:\n        imgs = imgs.to(device).float()\n        image_preds = model(imgs)\n        image_preds_all += [image_preds.detach().cpu().numpy()]\n        \n    image_preds_all = np.concatenate(image_preds_all, axis=0)\n    return image_preds_all","8becf2a8":"inference_ds = PetFinderDataset(df_test, transforms=get_inference_transforms())\ndata_loader  = torch.utils.data.DataLoader(inference_ds,\n                                           batch_size=CFG['batch_size'],\n                                           drop_last=False,\n                                           pin_memory=False,\n                                           shuffle=False,\n                                           num_workers=CFG['num_workers'])\nmodel = SwinModel(CFG['db_model'], pretrained=False)\nmodel.load_state_dict(torch.load(\"..\/input\/petfinder-dogbreed-cnn-models\/dogbreed_swin_ce.pt\"))\nmodel.to(CFG[\"device\"])\nwith torch.no_grad():\n    res_dogbreed = inference_one_epoch(model, data_loader, CFG[\"device\"])\n\ndel model, inference_ds, data_loader\ntorch.cuda.empty_cache()","786b55dd":"df_dogbreed = pd.DataFrame(softmax(res_dogbreed))\ndf_dogbreed[\"dog\"] = softmax(res_cat_dog)[:,1] > 0.5\ndf_dogbreed.loc[df_dogbreed.dog==False, :120] = 0\ndf_dogbreed = df_dogbreed.drop(\"dog\", axis=1)\ndf_dogbreed.columns = [f\"db{i}\" for i in df_dogbreed.columns]\n\nprint(df_dogbreed.shape)\ndf_dogbreed.head(2)","04d0509b":"df_meta = df_test[meta_features].join(df_dogbreed)\n\nprint(df_meta.shape)\ndf_meta.head(2)","235a1b36":"class Model(nn.Module):\n    def __init__(self, model_name, pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=3)\n        self.n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Linear(self.n_features, 1)\n\n    def forward(self, x):\n        output = self.model(x)\n        return output\n    \nclass ModelwithMetadata(nn.Module):\n    def __init__(self, model_name, size=512, pretrained=True):\n        super().__init__()\n        self.size  = size\n        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=4)\n        self.n_features = self.model.classifier.in_features\n        # Exclude the top layer\n        self.model.reset_classifier(0)\n        self.linear1 = nn.Linear(134, size*size)        \n        self.linear2 = nn.Linear(134, self.n_features)\n        self.linear3 = nn.Linear(self.n_features, 256)\n        self.linear4 = nn.Linear(256, 1)\n        self.relu    = nn.ReLU()\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, x1, x2):\n        # Metadata first\n        x = self.linear1(x2)\n        x = torch.reshape(x, (x2.shape[0], 1, self.size, self.size))\n        x = torch.cat((x1, x), dim=1)\n        x = self.model(x)\n        # Metadata Last\n        x2 = self.linear2(x2)\n        x  = torch.add(x, x2)\n        x  = self.relu(self.linear3(x))\n        x  = self.dropout(x)\n        output = self.linear4(x)\n        return output\n\nclass TransformerModel(nn.Module):\n    def __init__(self, model_name, pretrained=True):\n        super().__init__()\n        self.model   = timm.create_model(model_name, pretrained=pretrained, num_classes=0, in_chans=3)\n        num_features = self.model.num_features\n        self.linear  = nn.Linear(num_features, 1)\n        \n    def forward(self, x):\n        x = self.model(x)\n        output = self.linear(x)\n        return output\n    \nclass TransformerModelwithMetadataLast(nn.Module):\n    def __init__(self, model_name, size=224, pretrained=True):\n        super().__init__()\n        self.size  = size\n        self.backbone = TransformerModel(model_name, False)\n        num_features  = self.backbone.model.num_features\n        self.backbone.linear = nn.Linear(num_features, 256)\n        self.linear1 = nn.Linear(134, 256)\n        self.linear2 = nn.Linear(256, 128)\n        self.linear3 = nn.Linear(128, 1)\n        self.relu    = nn.ReLU()\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, x1, x2):\n        x  = self.backbone(x1)\n        # Metadata Last\n        x2 = self.linear1(x2)\n        x  = torch.add(x, x2)\n        x  = self.relu(self.linear2(x))\n        x  = self.dropout(x)\n        output = self.linear3(x)\n        return output","f63a9a97":"class PetFinderDataset(Dataset):\n    def __init__(self, df_img, df_meta, size, transforms=None, output_meta=True):\n        super().__init__()\n        self.df_img  = df_img.reset_index(drop=True).copy()\n        self.df_meta = df_meta.reset_index(drop=True).copy()\n        self.size    = size\n        self.transforms  = transforms\n        self.output_meta = output_meta\n        \n    def __len__(self):\n        return self.df_img.shape[0]\n    \n    def __getitem__(self, index: int):\n        img  = get_img(self.df_img.loc[index].path)\n        meta = torch.from_numpy(np.array(self.df_meta.loc[index], dtype=float))\n        if self.transforms:\n            h, w, _ = img.shape\n            trans = self.transforms(self.size, h, w)\n            img   = trans(image=img)['image']\n        if self.output_meta:\n            return img, meta\n        return img","1a37a100":"def get_tta_inference_transforms_1(size, h, w):\n    return Compose([\n        RandomResizedCrop(size, size, scale=(0.7, 1.0), p=1.0),\n        HorizontalFlip(p=0.5),\n        MotionBlur(p=0.5),\n        HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n        RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n        ToTensorV2(p=1.0)], p=1.0)\n\ndef get_tta_inference_transforms_2(size, h, w):\n    return Compose([\n        albResize(size, size, p=1.0),\n        HorizontalFlip(p=0.5),\n        HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n        RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n        ToTensorV2(p=1.0)], p=1.0)\n\ndef get_inference_transforms_1(size, h, w):\n    h = int(size*1.2) if int(size*1.2) < h else h\n    w = int(size*1.2) if int(size*1.2) < w else w\n    return Compose([\n        CenterCrop(h, w, p=1.0),\n        albResize(size, size, p=1.0),\n        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n        ToTensorV2(p=1.0)], p=1.0)\n\ndef get_inference_transforms_2(size, h, w):\n    return Compose([\n        albResize(size, size, p=1.0),\n        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n        ToTensorV2(p=1.0)], p=1.0)","f9b4b930":"def inference_one_epoch(model, use_meta, data_loader, device):\n    model.eval()\n\n    img_preds_all = []\n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (imgs, metas) in pbar:\n        imgs  = imgs.to(device).float()\n        if use_meta != 0:\n            metas = metas[:,:use_meta].to(device).float()\n            img_preds = model(imgs, metas)\n        else:\n            img_preds = model(imgs)\n        img_preds_all += [img_preds.detach().cpu().numpy()]\n        \n    img_preds_all = np.concatenate(img_preds_all, axis=0)\n    return img_preds_all","be6e5b0e":"def prepare_dataloader(df, size):\n    df = df.copy()\n    label_col  = \"Pawclass\"\n    dataloader = ImageDataLoaders.from_df(\n        df,\n        valid_pct=0.2,  # Dummy\n        seed=CFG[\"seed\"],\n        fn_col='path',\n        label_col=label_col,\n        y_block=RegressionBlock,\n        bs=CFG['batch_size'],\n        num_workers=CFG['num_workers'],\n        item_tfms=Resize(size),\n        batch_tfms=setup_aug_tfms([Brightness(), Contrast(), Hue(), Saturation()])\n    )\n    return dataloader\n\ndef petfinder_rmse(input,target):\n    return 100*torch.sqrt(F.mse_loss(F.sigmoid(input.flatten()), target))","0130974e":"def get_learner(df, size, model, model_path):\n    dataloader = prepare_dataloader(df, size)\n    if -1 < max(model.find(\"swin\"), model.find(\"beit\"), model.find(\"vit\"), model.find(\"cait\")):\n        model = TransformerModel(model, pretrained=False)\n    else:\n        model = Model(model, pretrained=False)\n    model.load_state_dict(torch.load(model_path))\n    learner = Learner(\n        dataloader,\n        model,\n        loss_func=BCEWithLogitsLossFlat(),\n        metrics=petfinder_rmse).to_fp16()\n    return learner, dataloader","2d9fb0c8":"all_res = []\nfor fastai, size, use_meta, dataset_type, model, model_path in CFG['models']:\n    \n    if fastai:\n        learn, data_loader = get_learner(df_train, size, model, model_path)\n        data_loader = data_loader.test_dl(df_test)\n        res, _ = learn.tta(dl=data_loader, n=CFG[\"tta\"], beta=0)\n        res    = res.detach().numpy()\n        if -1 < model_path.find(\"binary\"):\n            res = res*100\n        all_res.append(res)\n        \n        del learn, data_loader\n    else:\n        if 1 < CFG[\"tta\"]:\n            if dataset_type == 1:\n                inference_ds = PetFinderDataset(df_test, df_meta, size, transforms=get_tta_inference_transforms_1)\n            else:\n                inference_ds = PetFinderDataset(df_test, df_meta, size, transforms=get_tta_inference_transforms_2)\n        else:\n            if dataset_type == 1:\n                inference_ds = PetFinderDataset(df_test, df_meta, size, transforms=get_inference_transforms_1)\n            else:\n                inference_ds = PetFinderDataset(df_test, df_meta, size, transforms=get_inference_transforms_2)\n        data_loader = torch.utils.data.DataLoader(inference_ds,\n                                                  batch_size=CFG['batch_size'],\n                                                  drop_last=False,\n                                                  pin_memory=False,\n                                                  shuffle=False,\n                                                  num_workers=CFG['num_workers'])\n        if -1 < model_path.find(\"swin\"):\n            if use_meta == 0:\n                model = TransformerModel(model, pretrained=False)\n            else:\n                model = TransformerModelwithMetadataLast(model, size, pretrained=False)\n        else:\n            if use_meta == 0:\n                model = Model(model, pretrained=False)\n            else:\n                model = ModelwithMetadata(model, size, pretrained=False)\n        model.load_state_dict(torch.load(model_path))\n        model.to(CFG[\"device\"])\n\n        tta_res = []\n        for i in range(CFG[\"tta\"]):\n            with torch.no_grad():\n                res = inference_one_epoch(model, use_meta, data_loader, CFG[\"device\"])\n                if -1 < model_path.find(\"binary\"):\n                    res = my_sigmoid(res)*100\n                tta_res.append(res)\n        all_res.append(np.mean(tta_res, 0))\n\n        del model, inference_ds, data_loader\n    torch.cuda.empty_cache()","85e1dee7":"df_pred = pd.DataFrame(np.array(all_res).reshape(len(CFG[\"models\"]),-1)).T\nif not CFG[\"submit\"]:\n    df_pred[\"target\"] = df_test.Pawpularity\n    df_pred.to_csv(\".\/inference_result_train_data.csv\", index=False)","d3797956":"if CFG[\"stacking\"]:\n    loaded_model = pickle.load(open(\"..\/input\/petfinder-stacking-model\/stacking_model.pickle\", 'rb'))\n    # Calculate mean every model type\n    sta = 0\n    mean_every_model = []\n    for i in range(4, len(CFG[\"models\"])+1, 4):  # 4 is fold num\n        mean_every_model.append(df_pred.iloc[:, sta:i].mean(1))\n        sta = i\n    df_mean_every_model = pd.concat(mean_every_model, axis=1)\n    # Predict with stacking model\n    ensembled = loaded_model.predict(df_mean_every_model)\nelse:\n    ensembled = df_pred.mean(1)\nprint(ensembled)","8130b1ef":"if CFG[\"submit\"]:\n    ss = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/sample_submission.csv\")\n    ss[\"Pawpularity\"] = ensembled\n    ss.to_csv(\".\/submission.csv\", index=False)","0856cec2":"# Make dog breed for dog label","62c69603":"# Configuration","81d74341":"# Load data","3b603dd7":"# Make dog and cat labels","e6db8eb6":"### First of all, congrats to all winners and thanks to organizers. I have learned a lot from this competition, so I would like to share my solution here. I hope some people find new ideas from my solution.\n\nKey solutions were to train a model with pseudo labeled data(semi-supervised learning) and to ensemble many models.  \n\n- First, I trained two models, which are SwinTransformer(swin_large_patch4_window7_224_in22k) and EfficientNetB5(tf_efficientnet_b5_ns) with this competition data. After that, predicted all of previous competition data with them(2 TTA).  \n\n- Secondly, I trained some models with pseudo labeled data. After that, I trained this competition data and evaluated RMSE every 15 steps. I always used 4 KFold and BCE Loss. Some models like BEiT didn't work with this process, so I experimented many cases. In my experiment, SwinTransformer and EfficientNet worked well and other models such as BEiT, ViT and CaiT worked with the normal training process.\n\n- Finary, I ensembled all of models I made above process. FastAI's  TTA performance depends on batch size. I experimented some batch sizes like 1, 2 and 16. Batch size 1 brought me better LB score, so I needed to select 7 models(4 Fold and 2 TTA, total was 56 models) due to the submitting time limitation. Eventually, I selected the models below and ensembled them with simple blending. Stacking didn't work at this time.\n  1. swin_large_patch4_window7_224_in22k (Semi-Supervised -> Training with this competition images and meta data)\n  1. tf_efficientnet_b5_ns (Semi-Supervised -> Training with images)\n  1. beit_base_patch16_384 (Training with images)\n  1. vit_large_patch16_224 (Training with images)\n  1. cait_s24_384 (Training with images)\n  1. swin_large_patch4_window7_224 (Training with images)\n  1. swin_large_patch4_window7_224_in22k (Semi-Supervised -> Training with images)\n  \nNotes:  \nNo 1 model was trained with own pytorch training code not FastAI. Other cases were tranied with FastAI.  \nNo 6 model used high learning rate(4e-5). Other Swin models used 2e-5.\n  \nAppendix.  \nHere are some tips from my experiments.\n* Used own pytorch training code when I joined this competition(two months ago). After a month, I changed from that to FastAI training method. I think many Kagglers used FastAI. The fit one cycle brought us better score.\n* Used drop duplicated images to train models. This idea was from this discussion, https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/278497.\n* Made meta data with additional datasets, which were Cat VS Dog and Dog breed from past Kaggle competitions. \n  - Used Cat VS Dog model from https:\/\/github.com\/amitrajitbose\/cat-v-dog-classifier-pytorch.\n  - Made a model with Dog Breed Identification data from https:\/\/www.kaggle.com\/c\/dog-breed-identification.\n* Made SMOGN dataset with https:\/\/github.com\/nickkunz\/smogn, which is for imbalanced data. I used a model that trained with generated data from SMOGN, however, this didn't work.\n* Used stacking with Ridge regression, however, this didn't work.","a87afdea":"# Define model","25e3bd80":"# Run inference"}}