{"cell_type":{"616680ae":"code","422b5db9":"code","3124f649":"code","4328f2b1":"code","a238ea50":"code","8771ba95":"code","5160436f":"code","0d19bf6e":"code","94c8a30e":"code","a3ef0b39":"code","b34f4baf":"code","07d3eee0":"code","e9c20c26":"code","7a9ab3ca":"code","4f5d8980":"code","5e90319b":"code","45c7b764":"code","f8eec2b2":"code","2f59756b":"code","877064a4":"code","13685227":"code","f736c2a5":"code","45ec9009":"code","7e8fffd2":"code","a03ad0cc":"code","c5ff59cf":"code","ccbf4c07":"code","f6884a10":"code","426fab92":"code","f44107ed":"code","9d3b8d31":"code","0e0fe9f7":"code","92df4261":"code","6ab8117e":"code","4cd4621f":"code","d9cfc46a":"code","2f9c4b82":"code","584131f4":"code","49900a7f":"markdown","2f858ce4":"markdown","560bc72d":"markdown","a2b10279":"markdown","25909453":"markdown","8b5cf62a":"markdown","2adaaae6":"markdown","3ecfff85":"markdown","d889bb58":"markdown","ed38ceb8":"markdown","d52bff93":"markdown","18e2ccf0":"markdown","64f69ae9":"markdown","90efd8b8":"markdown","456fdb59":"markdown","5aa29947":"markdown","fd099ec7":"markdown","fc91d51c":"markdown","ee5ee5a2":"markdown","ce470af7":"markdown","edfa8301":"markdown","3885594c":"markdown","fa3d9a94":"markdown"},"source":{"616680ae":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","422b5db9":"# pytorch_pretained_bert already available in kaggle conda env.\n# !pip install pytorch-nlp","3124f649":"import sys\nimport numpy as np\nimport random as rn\nimport pandas as pd\nimport torch\nfrom pytorch_pretrained_bert import BertModel\nfrom torch import nn\n# from torchnlp.datasets import imdb_dataset      # --> We are using our own uploaded dataset.\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.optim import Adam\nfrom torch.nn.utils import clip_grad_norm_\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\n%matplotlib inline","4328f2b1":"rn.seed(321)\nnp.random.seed(321)\ntorch.manual_seed(321)\ntorch.cuda.manual_seed(321)","a238ea50":"path = '..\/input\/imdb-50k-movie-reviews-test-your-bert\/'\n\ntrain_data = pd.read_csv(path + 'train.csv')\ntest_data = pd.read_csv(path + 'test.csv')","8771ba95":"# experimenting here with a sample of dataset, to avoid memory overflow.\ntrain_data = train_data[:2000]\ntest_data = test_data[:500]\n\ntrain_data = train_data.to_dict(orient='records')\ntest_data = test_data.to_dict(orient='records')\ntype(train_data)","5160436f":"train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), train_data)))\ntest_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), test_data)))\n\nlen(train_texts), len(train_labels), len(test_texts), len(test_labels)","0d19bf6e":"train_texts[0]","94c8a30e":"sentences = [len(sent) for sent in train_texts]\n\nplt.rcParams.update({'figure.figsize':(7,5), 'figure.dpi':100})\nplt.bar(range(1,2001), sentences, color = ['red'])\nplt.gca().set(title='No. of characters in each sentence', xlabel='Number of sentence', ylabel='Number of Characters in each sentence');","a3ef0b39":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","b34f4baf":"tokenizer.tokenize('Hi my name is Atul')","07d3eee0":"train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\ntest_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))\n\nlen(train_tokens), len(test_tokens)","e9c20c26":"train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\ntest_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n\ntrain_tokens_ids.shape, test_tokens_ids.shape","7a9ab3ca":"train_y = np.array(train_labels) == 'pos'\ntest_y = np.array(test_labels) == 'pos'\ntrain_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y)","4f5d8980":"train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\ntest_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]","5e90319b":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report","45c7b764":"baseline_model = make_pipeline(CountVectorizer(ngram_range=(1,3)), LogisticRegression()).fit(train_texts, train_labels)","f8eec2b2":"baseline_predicted = baseline_model.predict(test_texts)","2f59756b":"print(classification_report(test_labels, baseline_predicted))","877064a4":"class BertBinaryClassifier(nn.Module):\n    def __init__(self, dropout=0.1):\n        super(BertBinaryClassifier, self).__init__()\n\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(768, 1)\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, tokens, masks=None):\n        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n        dropout_output = self.dropout(pooled_output)\n        linear_output = self.linear(dropout_output)\n        proba = self.sigmoid(linear_output)\n        return proba","13685227":"# ensuring that the model runs on GPU, not on CPU\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","f736c2a5":"str(torch.cuda.memory_allocated(device)\/1000000 ) + 'M'","45ec9009":"bert_clf = BertBinaryClassifier()\nbert_clf = bert_clf.cuda()     # running BERT on CUDA_GPU","7e8fffd2":"str(torch.cuda.memory_allocated(device)\/1000000 ) + 'M'","a03ad0cc":"x = torch.tensor(train_tokens_ids[:3]).to(device)\ny, pooled = bert_clf.bert(x, output_all_encoded_layers=False)\nx.shape, y.shape, pooled.shape","c5ff59cf":"y = bert_clf(x)\ny.cpu().detach().numpy()        # kinda Garbage Collector to free up used and cache space","ccbf4c07":"# Cross- checking CUDA GPU Memory to ensure GPU memory is not overflowing.\nstr(torch.cuda.memory_allocated(device)\/1000000 ) + 'M'","f6884a10":"y, x, pooled = None, None, None\ntorch.cuda.empty_cache()     # Clearing Cache space for fresh Model run\nstr(torch.cuda.memory_allocated(device)\/1000000 ) + 'M'","426fab92":"# Setting hyper-parameters\n\nBATCH_SIZE = 4\nEPOCHS = 10","f44107ed":"train_tokens_tensor = torch.tensor(train_tokens_ids)\ntrain_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n\ntest_tokens_tensor = torch.tensor(test_tokens_ids)\ntest_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n\ntrain_masks_tensor = torch.tensor(train_masks)\ntest_masks_tensor = torch.tensor(test_masks)\n\nstr(torch.cuda.memory_allocated(device)\/1000000 ) + 'M'","9d3b8d31":"train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\ntrain_sampler = RandomSampler(train_dataset)\ntrain_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n\ntest_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)","0e0fe9f7":"param_optimizer = list(bert_clf.sigmoid.named_parameters()) \noptimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]","92df4261":"optimizer = Adam(bert_clf.parameters(), lr=3e-6)","6ab8117e":"torch.cuda.empty_cache()   # Clearing Cache space for a fresh Model run","4cd4621f":"for epoch_num in range(EPOCHS):\n    bert_clf.train()\n    train_loss = 0\n    for step_num, batch_data in enumerate(train_dataloader):\n        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n        print(str(torch.cuda.memory_allocated(device)\/1000000 ) + 'M')\n        logits = bert_clf(token_ids, masks)\n        \n        loss_func = nn.BCELoss()\n\n        batch_loss = loss_func(logits, labels)\n        train_loss += batch_loss.item()\n        \n        \n        bert_clf.zero_grad()\n        batch_loss.backward()\n        \n\n        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        clear_output(wait=True)\n        print('Epoch: ', epoch_num + 1)\n        print(\"\\r\" + \"{0}\/{1} loss: {2} \".format(step_num, len(train_data) \/ BATCH_SIZE, train_loss \/ (step_num + 1)))","d9cfc46a":"bert_clf.eval()\nbert_predicted = []\nall_logits = []\nwith torch.no_grad():\n    for step_num, batch_data in enumerate(test_dataloader):\n\n        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n\n        logits = bert_clf(token_ids, masks)\n        loss_func = nn.BCELoss()\n        loss = loss_func(logits, labels)\n        numpy_logits = logits.cpu().detach().numpy()\n        \n        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n        all_logits += list(numpy_logits[:, 0])\n","2f9c4b82":"np.mean(bert_predicted)","584131f4":"print(classification_report(test_y, bert_predicted))","49900a7f":"# BERT Model\n\n\n### Bidirectional Encoder Representations from Transformers. Each word here has a meaning to it and we will encounter that one by one in this article. For now, the key takeaway from this line is \u2013 **BERT is based on the Transformer architecture**.","2f858ce4":"![Begin](https:\/\/pbs.twimg.com\/tweet_video_thumb\/ECvtpBRXoAIpyUv.jpg)\n\nImg source : https:\/\/pbs.twimg.com\/tweet_video_thumb\/ECvtpBRXoAIpyUv.jpg","560bc72d":"## Prepare the data","a2b10279":"### Sample of how BERT Tokenizer works and Embeddings prepared to be fed into BERT Model.\n\n![BERT TOKENS](https:\/\/miro.medium.com\/max\/619\/1*iJqlhZz-g6ZQJ53-rE9VvA.png)","25909453":"**Disclaimer :** Before, we go any further, Let me clear that You would need **GPU** and **Internet**- toggle turned **ON** (install external libraries) to succesfully run this kernel.","8b5cf62a":"# Fine Tune BERT","2adaaae6":"### Many a times your Kernel will Freeze but this is just OK. Let it be. This is a heavy computing task; So,it is just a common thing to happen. I have also put Monitoring code snippets to monitor your CPU\/GPU usage and also Garbage Collector to free up space.\n\n\nIt is quite common to see your CPU floating above 100% and\/or GPU over 100% like these screens below:\n![SNAP-1](https:\/\/i.ibb.co\/3cFD5Hs\/cut-1.png)\n![SNAP-2](https:\/\/i.ibb.co\/G5qFRxj\/cut-2.png)","3ecfff85":"## Preparing Token embeddings...","d889bb58":"#### visualizing one of the sentences from train set","ed38ceb8":"### Mapping sentences with their Labels...","d52bff93":"#### Our baseline model is working just fine and yeilding a fair enough score. Now, its time to play Dirty with the \"BERT\".","18e2ccf0":"# Thank You! Please UPVOTE to show your support.","64f69ae9":"## visualizing sentences lengths","90efd8b8":"**Note :** uncomment the code line in above cell; you are running this notebook locally, and would need pytorch-nlp library.Here, it is pre-installed.","456fdb59":"#### We can see that most of the sentences are around 700 - 1000 characters long, which is pretty obvious. HOwever, few sentences are shorter and few even long as 6000 characters. So, this is a good, very versatile Review Dataset.","5aa29947":"## Preparing Token Ids...\n\n\n![token ids](https:\/\/jalammar.github.io\/images\/distilBERT\/sst2-text-to-tokenized-ids-bert-example.png)","fd099ec7":"#### A much better score with BERT.\n\n**P.S.** - Since, you have come this far, I hope you Liked and got to learn something new. Its time to show your **LOVE** with an upvote. **All kind of Feedbacks are welcomed**! ","fc91d51c":"### Now Masking few random IDs from each sentences to remove Biasness from model.","ee5ee5a2":"### Initializing seed values to stabilize the outcomes.","ce470af7":"![BYE BYE](https:\/\/bloximages.newyork1.vip.townnews.com\/mdjonline.com\/content\/tncms\/assets\/v3\/editorial\/6\/7c\/67cfcc90-ec48-11e9-a76f-bffb4b3ee322\/5da0b4cd79df1.image.jpg?resize=400%2C266)","edfa8301":"**NOte :** This will take anything around 20 to 30 minutes on Kaggle GPU. I literally felt asleep. But,that was a **POWER NAP** for me ;-) .\n\n### You can go, and Grab some Coffee for yourself, or call someone you Love. They need your time, especially your MOM :-).\n\n\n![Coffee Break](https:\/\/media.harpersbazaar.com.sg\/2019\/04\/coffee-feat1-700x350.jpg)","3885594c":"# Baseline","fa3d9a94":"### importing necessaries libraries..."}}