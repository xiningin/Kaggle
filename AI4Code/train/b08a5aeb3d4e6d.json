{"cell_type":{"2d210bc4":"code","196950eb":"code","7b506b70":"code","e78c9a3e":"code","2a99f1d2":"code","8cc836f7":"code","6d03f58c":"code","2188218b":"code","34641170":"code","52ea3827":"code","9cc11c74":"code","54f09516":"code","f548f7bd":"code","549b6329":"code","54069b08":"code","e1df849a":"code","5900c7e6":"code","6f66f106":"code","adc86627":"code","ad2d0d76":"code","15a6bf68":"code","a50935be":"code","2ccb3d79":"code","eeb4812d":"code","d208bd2e":"code","958fde4f":"code","9bad82fe":"code","da3796a5":"code","cd1b37b9":"markdown","f61ee51b":"markdown","2845a1e8":"markdown","a1124c93":"markdown","87bb2d9d":"markdown","0f3cba5c":"markdown","2c4b994f":"markdown","b33140f9":"markdown","d1ea8889":"markdown","4fe07fee":"markdown","b2bee320":"markdown","e9246de0":"markdown","062eba7b":"markdown","468b2bc7":"markdown","7e77eb9a":"markdown","2c639c4a":"markdown","ffd5d1c3":"markdown","f12af0b1":"markdown","3015a206":"markdown","e8cfb0c5":"markdown","9af77a56":"markdown","6483e937":"markdown","3aef2af6":"markdown","2e7f4dea":"markdown","27adcc6f":"markdown","5fec5d45":"markdown","60c4bc82":"markdown","eef78bd1":"markdown","cd2b7998":"markdown","bbd64853":"markdown","f800ae46":"markdown","53ac2792":"markdown","a21dcbd7":"markdown","64e0335e":"markdown","0e896960":"markdown","58a0220d":"markdown"},"source":{"2d210bc4":"# Import statements\nimport copy\nimport time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.svm import SVC, SVR\nfrom IPython.display import display\nfrom scipy.stats.stats import pearsonr\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import Imputer, StandardScaler\nfrom sklearn.neural_network import MLPRegressor, MLPClassifier\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.feature_selection import VarianceThreshold, chi2, RFE, SelectKBest, f_classif\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict, GridSearchCV\n","196950eb":"# Import the datasets (responses and columns).\nresponsesData = pd.read_csv('..\/input\/responses.csv')\ncolumnsData = pd.read_csv('..\/input\/columns.csv')\n\nprint(\"Datasets loaded!\")\nprint(\"Shape of the data : {0} and {1}\".format(responsesData.shape, columnsData.shape))","7b506b70":"# Some styling..\npd.set_option('display.max_columns',150)\npd.set_option('display.max_rows',1010)\nplt.style.use('bmh')","e78c9a3e":"print(\"Responses Data\")\nresponsesData.head(n=3)","2a99f1d2":"print(\"Columns Data\")\ncolumnsData.head(n=3)","8cc836f7":"# Pre processing the data set provided\ndef preprocessingDataset(dataset):\n\n    # Define imp from Imputer class for missing values\n    imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n\n    #### Preprocessing the Dataset\n    music = dataset.iloc[:, 0:19]\n    movies = dataset.iloc[:, 19:31]\n    phobias = dataset.iloc[:, 63:73]\n    interests = dataset.iloc[:, 31:63]\n    health = dataset.iloc[:, 73:76]\n    personal = dataset.iloc[:, 76:133]\n    information = dataset.iloc[:, 140:150]\n    expenditure = dataset.iloc[:, 133:140]\n\n    \"\"\"\n    print(music)\n    print(movies)\n    print(phobias)\n    print(interests)\n    print(health)\n    print(personal)\n    print(information)\n    print(spendings)\n    \"\"\"\n    # Processing the personal\n    for x in personal[\"Lying\"]:\n        if x == \"never\":\n            personal.replace(x, 1.0, inplace=True)\n        elif x == \"only to avoid hurting someone\":\n            personal.replace(x, 2.0, inplace=True)\n        elif x == \"sometimes\":\n            personal.replace(x, 3.0, inplace=True)\n        elif x == \"everytime it suits me\":\n            personal.replace(x, 4.0, inplace=True)\n        elif x == \"Nan\":\n            personal.replace(x, np.nan, inplace=True)\n        elif x == \"nan\":\n            personal.replace(x, np.nan, inplace=True)\n\n    for x in personal[\"Punctuality\"]:\n        if x == \"i am often early\":\n            personal.replace(x, 3.0, inplace=True)\n        elif x == \"i am always on time\":\n            personal.replace(x, 2.0, inplace=True)\n        elif x == \"i am often running late\":\n            personal.replace(x, 1.0, inplace=True)\n        elif x == \"Nan\":\n            personal.replace(x, np.nan, inplace=True)\n        elif x == \"nan\":\n            personal.replace(x, np.nan, inplace=True)\n\n    for x in personal[\"Internet usage\"]:\n        if x == \"most of the day\":\n            personal.replace(x, 4.0, inplace=True)\n        elif x == \"few hours a day\":\n            personal.replace(x, 3.0, inplace=True)\n        elif x == \"less than an hour a day\":\n            personal.replace(x, 2.0, inplace=True)\n        elif x == \"no time at all\":\n            personal.replace(x, 1.0, inplace=True)\n        elif x == \"Nan\":\n            personal.replace(x, np.nan, inplace=True)\n        elif x == \"nan\":\n            personal.replace(x, np.nan, inplace=True)\n\n    # Replace strings with numpy NaNs\n    personal = personal.replace(\"NaN\", np.nan)\n    personal = personal.replace(\"nan\", np.nan)\n\n    # Replace missing values with most frequent values\n    imp.fit(personal)\n    personal_data = imp.transform(personal)\n\n    d = personal_data[:, :]\n    ind = []\n    for x in range(len(personal_data)):\n        ind.append(x)\n    c = personal.columns.tolist()\n    personal = pd.DataFrame(data=d, index=ind, columns=c)\n\n    # Processing the health\n    for x in health[\"Smoking\"]:\n        if x == \"current smoker\":\n            health.replace(x, 1.0, inplace=True)\n        elif x == \"former smoker\":\n            health.replace(x, 2.0, inplace=True)\n        elif x == \"tried smoking\":\n            health.replace(x, 3.0, inplace=True)\n        elif x == \"never smoked\":\n            health.replace(x, 4.0, inplace=True)\n        elif x == \"Nan\":\n            health.replace(x, np.nan, inplace=True)\n        elif x == \"nan\":\n            health.replace(x, np.nan, inplace=True)\n\n    for x in health[\"Alcohol\"]:\n        if x == \"drink a lot\":\n            health.replace(x, 1.0, inplace=True)\n        elif x == \"social drinker\":\n            health.replace(x, 2.0, inplace=True)\n        elif x == \"never\":\n            health.replace(x, 3.0, inplace=True)\n        elif x == \"Nan\":\n            health.replace(x, np.nan, inplace=True)\n        elif x == \"nan\":\n            health.replace(x, np.nan, inplace=True)\n\n    # Replace strings with numpy NaNs\n    health = health.replace(\"NaN\", np.nan)\n    health = health.replace(\"nan\", np.nan)\n\n    # Replace missing values with most frequent values\n    imp.fit(health)\n    healthData = imp.transform(health)\n    d = healthData[:, :]\n    ind = []\n    for x in range(len(healthData)):\n        ind.append(x)\n    c = health.columns.tolist()\n    health = pd.DataFrame(data=d, index=ind, columns=c)\n\n    # Processing the information\n    for x in information[\"Gender\"]:\n        if x == \"female\":\n            information.replace(x, 2.0, inplace=True)\n        elif x == \"male\":\n            information.replace(x, 1.0, inplace=True)\n        elif x == \"Nan\":\n            information.replace(x, np.nan, inplace=True)\n        elif x == \"nan\":\n            information.replace(x, np.nan, inplace=True)\n\n    for x in information[\"Left - right handed\"]:\n        if x == \"right handed\":\n            information.replace(x, 1.0, inplace=True)\n        elif x == \"left handed\":\n            information.replace(x, 2.0, inplace=True)\n        elif x == \"Nan\":\n            information.replace(x, np.nan, inplace=True)\n        elif x == \"nan\":\n            information.replace(x, np.nan, inplace=True)\n\n    for x in information[\"Education\"]:\n        if x == \"doctorate degree\":\n            information.replace(x, 6.0, inplace=True)\n        elif x == \"masters degree\":\n            information.replace(x, 5.0, inplace=True)\n        elif x == \"college\/bachelor degree\":\n            information.replace(x, 4.0, inplace=True)\n        elif x == \"secondary school\":\n            information.replace(x, 3.0, inplace=True)\n        elif x == \"primary school\":\n            information.replace(x, 2.0, inplace=True)\n        elif x == \"currently a primary school pupil\":\n            information.replace(x, 1.0, inplace=True)\n        elif x == \"Nan\":\n            information.replace(x, np.nan, inplace=True)\n        elif x == \"nan\":\n            information.replace(x, np.nan, inplace=True)\n\n    for x in information[\"Only child\"]:\n        if x == \"yes\":\n            information.replace(x, 1.0, inplace=True)\n        elif x == \"no\":\n            information.replace(x, 2.0, inplace=True)\n        elif x == \"Nan\":\n            information.replace(x, np.nan, inplace=True)\n        elif x == \"nan\":\n            information.replace(x, np.nan, inplace=True)\n\n    for x in information[\"Village - town\"]:\n        if x == \"village\":\n            information[\"Village - town\"].replace(x, 1.0, inplace=True)\n        elif x == \"city\":\n            information[\"Village - town\"].replace(x, 2.0, inplace=True)\n        elif x == \"Nan\":\n            information.replace(x, np.nan, inplace=True)\n        elif x == \"nan\":\n            information.replace(x, np.nan, inplace=True)\n\n    for x in information[\"House - block of flats\"]:\n        if x == \"block of flats\":\n            information[\"House - block of flats\"].replace(x, 1, inplace=True)\n        elif x == \"house\/bungalow\":\n            information[\"House - block of flats\"].replace(x, 2, inplace=True)\n        elif x == \"Nan\":\n            information.replace(x, np.nan, inplace=True)\n        elif x == \"nan\":\n            information.replace(x, np.nan, inplace=True)\n\n    information = information.replace(\"nan\", np.nan)\n    information = information.replace(\"NaN\", np.nan)\n\n    imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n    imp.fit(information)\n    informationData = imp.transform(information)\n    d = informationData[:, :]\n    ind = []\n    for x in range(len(informationData)):\n        ind.append(x)\n    c = information.columns.tolist()\n    information = pd.DataFrame(data=d, index=ind, columns=c)\n\n    # Processing music\n    music = music.replace(\"nan\", np.nan)\n    music = music.replace(\"NaN\", np.nan)\n\n    imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n    imp.fit(music)\n    musicData = imp.transform(music)\n    d = musicData[:, :]\n    ind = []\n    for x in range(len(musicData)):\n        ind.append(x)\n    c = music.columns.tolist()\n    music = pd.DataFrame(data=d, index=ind, columns=c)\n\n    # Processing movies\n    movies = movies.replace(\"nan\", np.nan)\n    movies = movies.replace(\"NaN\", np.nan)\n\n    imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n    imp.fit(movies)\n    moviesData = imp.transform(movies)\n    d = moviesData[:, :]\n    ind = []\n    for x in range(len(moviesData)):\n        ind.append(x)\n    c = movies.columns.tolist()\n    movies = pd.DataFrame(data=d, index=ind, columns=c)\n\n    # Processing phobias\n    phobias = phobias.replace(\"nan\", np.nan)\n    phobias = phobias.replace(\"NaN\", np.nan)\n\n    imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n    imp.fit(phobias)\n    phobiasData = imp.transform(phobias)\n    d = phobiasData[:, :]\n    ind = []\n    for x in range(len(phobiasData)):\n        ind.append(x)\n    c = phobias.columns.tolist()\n    phobias = pd.DataFrame(data=d, index=ind, columns=c)\n\n    # Processing interests\n    interests = interests.replace(\"nan\", np.nan)\n    interests = interests.replace(\"NaN\", np.nan)\n\n    imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n    imp.fit(interests)\n    interestsData = imp.transform(interests)\n    d = interestsData[:, :]\n    ind = []\n    for x in range(len(interestsData)):\n        ind.append(x)\n    c = interests.columns.tolist()\n    interests = pd.DataFrame(data=d, index=ind, columns=c)\n\n    # Processing spendings\n    expenditure = expenditure.replace(\"nan\", np.nan)\n    expenditure = expenditure.replace(\"NaN\", np.nan)\n\n    imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n    imp.fit(expenditure)\n    expenditureData = imp.transform(expenditure)\n    d = expenditureData[:, :]\n    ind = []\n    for x in range(len(expenditureData)):\n        ind.append(x)\n    c = expenditure.columns.tolist()\n    expenditure = pd.DataFrame(data=d, index=ind, columns=c)\n\n    # Joining all the processed sections\n    joinedDatasets = music.join(movies.join(phobias.join(interests.join(health.join(personal.join(information.join(expenditure)))))))\n\n    return joinedDatasets","6d03f58c":"# Collect the dataset with missing values filled with the most frequent entry in the column. \n\nprint(\"Preprocessing data might take some time..\")\nprint(\"1) Missing values are being handled!\")\nprint(\"2) Categorical entries are getting converted to numeric values!\")\nprint(\"3) Dummy variables are being handled!\")\nfilledData = preprocessingDataset(responsesData)\nprint(\"Done preprocessing data!\")","2188218b":"filledData.head(n=3)","34641170":"# Scale the dataset.\ndef scalingDataset(dataset):\n    # Scaling the dataset\n    scaler = StandardScaler()\n    scaledDataarray = scaler.fit_transform(dataset)\n    if type(dataset) is np.ndarray:\n        return scaledDataarray\n    else:\n        d = scaledDataarray[:, :]\n        ind = []\n        for x in range(len(dataset)):\n            ind.append(x)\n        c = dataset.columns.tolist()\n        scaledData = pd.DataFrame(data=d, index=ind, columns=c)\n        return scaledData\n\nscaledData = scalingDataset(filledData)\nscaledData.head(n=3)","52ea3827":"relations = filledData.corr()\n# The line of code below is used from a question posted in StackOverflow\n# Link : https:\/\/stackoverflow.com\/questions\/17778394\/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\nmulticollinearity = (relations.where(np.triu(np.ones(relations.shape), k=1).astype(np.bool)).stack().sort_values(ascending=False))","9cc11c74":"# Top positive correlations are:\nmulticollinearity.head(n=15)","54f09516":"# Top negative correlations are:\nmulticollinearity.tail(n=15)","f548f7bd":"filledData.describe()","549b6329":"def correlationFigure(featureVariablesMain, targetVariable):\n    # Calculate correlation\n    #print(featureVariablesMain.columns)\n    #print(targetVariable.values)\n    def correlationCalculation(targetVariable, featureVariables, features):\n        columns = [] # For maintaining the feature names\n        values = [] # For maintaining the corr values of features with \"Empathy\"\n\n        # Traverse through all the input features\n        for x in features:\n            if x is not None:\n                columns.append(x) # Append the column name\n                # Calculate the correlation\n                c = np.corrcoef(featureVariables[x], featureVariables[targetVariable])\n                absC = abs(c) # Absolute value because important values might miss\n                values.append(absC[0,1])\n\n        corrValues = pd.DataFrame()\n        dataDict = {'features': columns, 'correlation_values': values}\n        corrValues = pd.DataFrame(dataDict)\n        # Sort the value by correlation values\n        sortedCorrValues = corrValues.sort_values(by=\"correlation_values\")\n\n        # Plot the graph to show the features with their correlation values\n        figure, ax = plt.subplots(figsize=(15, 45), squeeze=True)\n        ax.set_title(\"Correlation Coefficients of Features\")\n        sns.barplot(x=sortedCorrValues.correlation_values, y=sortedCorrValues['features'], ax=ax)\n        ax.set_ylabel(\"-----------Corr Coefficients--------->\")\n\n\n        plt.show()\n\n        return sortedCorrValues\n\n    # Make a list of columns\n    columns = []\n    for x in featureVariablesMain.columns:\n        columns.append(x)\n    # Remove \"Empathy\" from df\n    columns.remove(targetVariable)\n\n    # Compute correlations\n    correlations = correlationCalculation(targetVariable, featureVariablesMain, columns)\n    return correlations","54069b08":"# Plotting the correlations with respect to \"Empathy\" variable\ntarget = \"Empathy\"\ntargetVariable = filledData['Empathy'].to_frame()\ncorrData = correlationFigure(scaledData, target)\nimportantFeatures = corrData.sort_values(by=\"correlation_values\", ascending=True).tail(20)","e1df849a":"# 20 most important feature with their correlation values\nimportantFeatures","5900c7e6":"finalColumnsList = []\nfor x in importantFeatures['features']:\n    finalColumnsList.append(x)\n\ndf = pd.DataFrame() # Final prepared dataset for modelling\ndf = filledData[finalColumnsList[0]].to_frame()\nfor x in range(1, len(finalColumnsList)):\n    df = df.join(filledData[finalColumnsList[x]].to_frame())","6f66f106":"xTrain, xTest, yTrain, yTest = train_test_split(df, targetVariable, test_size=0.2, random_state=0)\nxTrain = xTrain.sort_index()\nxTest = xTest.sort_index()\nyTrain = yTrain.sort_index()\nyTest = yTest.sort_index()","adc86627":"# Decision Tree Modelling\ndef dt(xTrain, yTrain, xTest, yTest):\n    print(\"Hyperparameter Tuning!\")\n    gridClassifier = DecisionTreeClassifier()\n    depthList = [1,3,5,10]\n    parameters = {'max_depth':depthList}\n    gridSearch = GridSearchCV(estimator=gridClassifier,\n                              param_grid=parameters,\n                              scoring=\"accuracy\",\n                              cv=10,\n                              n_jobs=5)\n    gridSearch.fit(xTrain, yTrain.values.ravel())\n    bestAccuracyMLP = gridSearch.best_score_\n    bestParametersMLP = gridSearch.best_params_\n\n    print(\"The best parameters for Decision Tree model are :\\n{}\\n\".format(bestParametersMLP))\n    dtclassifier = DecisionTreeClassifier(max_depth=3)\n    dtclassifier.fit(xTrain, yTrain.values.ravel())\n    yPredictiondtTest = dtclassifier.predict(xTest)\n    yPredictiondtTrain = dtclassifier.predict(xTrain)\n    print(\"Decision Tree Evaluations :\\n\")\n    print(\"Training Accuracy => {}\".format(accuracy_score(yTrain, yPredictiondtTrain) * 100))\n    print(\"Testing Accuracy => {}\\n\".format(accuracy_score(yTest, yPredictiondtTest) * 100))\n    print(\"Confusion Matrix => \\n{}\\n\".format(confusion_matrix(yTest, yPredictiondtTest)))\n    print(\"Classification Summary => \\n{}\\n\".format(classification_report(yTest, yPredictiondtTest)))\n    plt.scatter(yTest, yPredictiondtTest)\n\ndt(xTrain, yTrain, xTest, yTest)","ad2d0d76":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\n# Below function is taken from the official documentation of \"sklearn\" \n# Link : http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n\ntitle = \"Learning Curves\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\nestimator = DecisionTreeClassifier(max_depth=3)\nplot_learning_curve(estimator, title, xTrain, yTrain, cv=cv, n_jobs=4)","15a6bf68":"# KNN Modelling\ndef knn(xTrain, yTrain, xTest, yTest):\n    print(\"Hyperparameter Tuning!\")\n    gridClassifier = KNeighborsClassifier()\n    nearestNeighbors = [1, 3, 5, 10]\n    parameters = {'n_neighbors': nearestNeighbors}\n    gridSearch = GridSearchCV(estimator=gridClassifier,\n                              param_grid=parameters,\n                              scoring=\"accuracy\",\n                              cv=10,\n                              n_jobs=5)\n    gridSearch.fit(xTrain, yTrain.values.ravel())\n    bestAccuracyMLP = gridSearch.best_score_\n    bestParametersMLP = gridSearch.best_params_\n\n    print(\"The best parameters for KNN model are :\\n{}\\n\".format(bestParametersMLP))\n    knnClassifier = KNeighborsClassifier(n_neighbors=1)\n    knnClassifier.fit(xTrain, yTrain.values.ravel())\n    yPredKNNTest = knnClassifier.predict(xTest)\n    yPredKNNTrain = knnClassifier.predict(xTrain)\n    print(\"KNN Evaluation :\\n\")\n    print(\"Training Accuracy => {}\".format(accuracy_score(yTrain, yPredKNNTrain) * 100))\n    print(\"Testing Accuracy => {}\\n\".format(accuracy_score(yTest, yPredKNNTest) * 100))\n    print(\"Confusion Matrix => \\n{}\\n\".format(confusion_matrix(yTest, yPredKNNTest)))\n    print(\"Classification Summary => \\n{}\\n\".format(classification_report(yTest, yPredKNNTest)))\n    plt.scatter(yTest, yPredKNNTest)\n\nknn(xTrain, yTrain, xTest, yTest)","a50935be":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\n# Below function is taken from the official documentation of \"sklearn\" \n# Link : http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n\ntitle = \"Learning Curves\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\nestimator = KNeighborsClassifier(n_neighbors=1)\nplot_learning_curve(estimator, title, xTrain, yTrain, cv=cv, n_jobs=4)","2ccb3d79":"# Logistics Regression Modelling\ndef logisticRegression(xTrain, yTrain, xTest, yTest):\n    logRegClassifier = LogisticRegression(multi_class='ovr', random_state=0, C=3)\n    logRegClassifier.fit(xTrain, yTrain.values.ravel())\n    yPredLogTest = logRegClassifier.predict(xTest)\n    yPredLogTrain = logRegClassifier.predict(xTrain)\n    print(\"Logistic Regression Evaluation :\\n\")\n    print(\"Testing Accuracy => {}\".format(accuracy_score(yTest, yPredLogTest) * 100))\n    print(\"Confusion Matrix => \\n{}\\n\".format(confusion_matrix(yTest, yPredLogTest)))\n    print(\"Classification Summary => \\n{}\\n\".format(classification_report(yTest, yPredLogTest)))\n    plt.scatter(yTest, yPredLogTest)\n\n\nlogisticRegression(xTrain, yTrain, xTest, yTest)","eeb4812d":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\n# Below function is taken from the official documentation of \"sklearn\" \n# Link : http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n\ntitle = \"Learning Curves\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\nestimator = LogisticRegression(multi_class='ovr', random_state=0, C=3)\nplot_learning_curve(estimator, title, xTrain, yTrain, cv=cv, n_jobs=4)","d208bd2e":"def kBestLogReg(filledData):\n    bestDF = filledData.drop(columns=['Empathy'], axis=1)\n    targetVariable = filledData['Empathy'].to_frame()\n    selected = SelectKBest(score_func=f_classif, k=20)\n    selectedFit = selected.fit(bestDF, targetVariable.values.ravel())\n    selectedFitTransform = selectedFit.transform(bestDF)\n\n    xTrainBestK, xTestBestK, yTrainBestK, yTestBestK = train_test_split(\n        scalingDataset(selectedFitTransform),\n        targetVariable,\n        test_size=0.2,\n        random_state=0)\n    print(\"Cross Validating for best parameters..\")\n    print(\"This might take some time..\\n\")\n    lr = LogisticRegression(multi_class='ovr')\n    cList = [10, 100, 1000, 10000]\n    solverList = ['lbfgs', 'sag', 'saga', 'newton-cg']\n    maxIterList = [100, 1000, 10000]\n    parameters = {'C': cList, 'solver': solverList, 'max_iter': maxIterList}\n    gridSearch = GridSearchCV(estimator=lr,\n                              param_grid=parameters,\n                              scoring=\"accuracy\",\n                              cv=10,\n                              n_jobs=4)\n    gridSearch.fit(xTrainBestK, yTrainBestK.values.ravel())\n    bestAccuracyLogBestK = gridSearch.best_score_\n    bestParametersLogBestK = gridSearch.best_params_\n    print(\"The best parameters for Logistic Regression model are :\\n{}\\n\".format(bestParametersLogBestK))\n    # Best parameters : C:10, maxiter:100, solver:sag\n    lr = LogisticRegression(C=10, max_iter=100, solver='lbfgs', multi_class='ovr', random_state=1)\n    lr.fit(xTrainBestK, yTrainBestK.values.ravel())\n    yPredLogBestTest = lr.predict(xTestBestK)\n    bestKLogAcc = accuracy_score(yTestBestK, yPredLogBestTest)\n    print(\"Logistic Regression using SelectKBest Method Evaluations :\\n\")\n    print(\"Training Accuracy : {}\".format(bestAccuracyLogBestK*100))\n    print(\"Testing Accuracy  : {}\\n\".format(bestKLogAcc*100))\n    print(\"Confusion Matrix => \\n{}\\n\".format(confusion_matrix(yTestBestK, yPredLogBestTest)))\n    print(\"Classification Summary => \\n{}\\n\".format(classification_report(yTestBestK, yPredLogBestTest)))\n    plt.scatter(yTestBestK, yPredLogBestTest)\n\nkBestLogReg(filledData)","958fde4f":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\n# Below function is taken from the official documentation of \"sklearn\" \n# Link : http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n\ntitle = \"Learning Curves\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\nestimator = LogisticRegression(C=10, max_iter=100, solver='lbfgs', multi_class='ovr', random_state=1)\nplot_learning_curve(estimator, title, xTrain, yTrain, cv=cv, n_jobs=4)","9bad82fe":"def mlp(df, scaledData, targetVariable):\n\n    # For MLP, we have to use scaled data\n    scaledDF = scalingDataset(df)\n    xTrain, xTest, yTrain, yTest = train_test_split(scaledData, targetVariable, test_size=0.2, random_state=0, shuffle=False)\n    xTrain = xTrain.sort_index()\n    xTest = xTest.sort_index()\n    yTrain = yTrain.sort_index()\n    yTest = yTest.sort_index()\n    \n    # You can run the below commented code to cross validate for the best parameters\n    # print(\"SIT BACK AND RELAX! CROSS VALIDATION WOULD TAKE SOME TIME...\")\n    # mlpClass = MLPClassifier(random_state=0)\n    # iterList = [100, 500]\n    # hiddenLayerList = [(100, 100), (100, 200)]\n    # parameters = {'alpha': 10.0 ** -np.arange(1, 7), 'max_iter': iterList, 'hidden_layer_sizes': hiddenLayerList}\n    # gridSearch = GridSearchCV(estimator=mlpClass,\n    #                           param_grid=parameters,\n    #                           scoring=\"accuracy\",\n    #                           cv=10,\n    #                           n_jobs=10)\n    # gridSearch.fit(xTrain, yTrain.values.ravel())\n    # bestAccuracyMLP = gridSearch.best_score_\n    # bestParametersMLP = gridSearch.best_params_\n    # print(\"The best parameters for MLP model are :\\n{}\\n\".format(bestParametersMLP))\n\n    mlpClass = MLPClassifier(hidden_layer_sizes=(100, 200), alpha=0.1, max_iter=500, random_state=0)\n    mlpClass.fit(xTrain, yTrain.values.ravel())\n    yPredMLP = mlpClass.predict(xTest)\n    print(\"Multilayer Perceptron Evaluation :\\n\")\n    # print(\"Training Accuracy => {}\".format(bestAccuracyMLP*100))\n    print(\"Testing Accuracy => {}\\n\".format(accuracy_score(yTest, yPredMLP)*100))\n    print(\"Confusion Matrix => \\n{}\\n\".format(confusion_matrix(yTest, yPredMLP)))\n    print(\"Classification Summary => \\n{}\\n\".format(classification_report(yTest, yPredMLP)))\n    plt.scatter(yTest, yPredMLP)\n\nmlp(df, scaledData, targetVariable)","da3796a5":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\n# Below function is taken from the official documentation of \"sklearn\" \n# Link : http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=10,\n                        n_jobs=10, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n\ntitle = \"Learning Curves\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\nxTrain, xTest, yTrain, yTest = train_test_split(scaledData, targetVariable, test_size=0.2, random_state=0, shuffle=False)\nxTrain = xTrain.sort_index()\nxTest = xTest.sort_index()\nyTrain = yTrain.sort_index()\nyTest = yTest.sort_index()\n\nestimator = MLPClassifier(hidden_layer_sizes=(100, 200), alpha=0.1, max_iter=500)\nplot_learning_curve(estimator, title, xTrain, yTrain, cv=10, n_jobs=10)","cd1b37b9":"Now, let's try to understand the data in even more detail.","f61ee51b":"Let's try to predict the empathy using Decision Trees. The accuracy comes around 40%.","2845a1e8":"Now, after importing datasets, preprocessing the imported datasets is very important. The datasets include some missing values. After having look at the dataset, there are not much missing values. There could be numerous ways to handle data with missing values. <br>\n1) Dropping the entire row. <br>\n2) Filling the missing values with some data (mean, median, most frequent).<br><br>\nDropping the entire row from the dataset might not be the most intelligent way because the row might contain some important information needed for building our model. Also, since not many data entries are missing, we can also replace the empty cells with the most frequent entries of the columns.<br><br>\nAlso, while preprocessing, we need to take care of the categorical features. The dataset contains 11 categorical features. I have converted the categorical features into numeric. The approach used here is to get the unique values of the columns and assign each of the unique values with a number (1 - N) with N unique values.","a1124c93":"Note that some models require the data to be normalized. So, let's also scale the preprocessed dataset ahead.","87bb2d9d":"### Logistic Regression","0f3cba5c":"### Logistic Regression (SelectKBest Feature Method)","2c4b994f":"## Data Loading","b33140f9":"Let's try to predict using K-Nearest Neighbours model. Clearly, KNN fails miserably and the accuracy is 30.7%.","d1ea8889":"Let's continue our visualization process. Following information would help us identify the columns which are very biased could add unnecesary bias in our model.","4fe07fee":"Let's try to use Logistic Regression by using SelectKBest method from sklearn library.","b2bee320":"Let's look at the learning curve of Logistic Regression.","e9246de0":"Let's try to plot the learning curve for the Decision Tree model.","062eba7b":"### KNN Model","468b2bc7":"The various important features like preferences, interests, habits, opinions, and fears of young people. After exploring such features, a machine learning model is build and predictions are made for the young people's empathy. This project mainly consists of four parts: <br>\n**1) Data Loading and Preprocessing.**<br>\n**2) Feature Engineering.**<br>\n**3) Model Training and Testing.**<br>\n**4) Model Comparisons and Conclusion.**<br>\n**4) Interesting Insights drawn from Dataset.**<br>","7e77eb9a":"Let's try to plot the learning curves for KNN model.","2c639c4a":"### Multilayer Perceptron","ffd5d1c3":"## Feature Engineering","f12af0b1":"Another important thing to note about such huge datasets is that it is interesting to explore the multicollinearity between features and try to draw some interesting conclusions from the observations.","3015a206":"## Model Building ","e8cfb0c5":"The most important step in our project is to select the most important features out of 150 features in order to reduce the computation time and complexity. This also helps us in know what features are totally irrelevant to our target variable \"Empathy\". I have used correlations between several features and the most correlated features are given as the most important features.","9af77a56":"Below is the preprocessed dataset! Have a look at it and see that the categorical feature entries are converted to numeric values.","6483e937":"First of all, import the datasets using pandas. The file 'importDataset' contains the code for importing.","3aef2af6":"The final prepared data should be split into training and testing sets. We train our classifiers on the training set.","2e7f4dea":"Now, let's join all the important features to make a completely prepared dataset.","27adcc6f":"From all the models that I have tried like Decision Trees, KNN, Logistic Regression and Multilayer Perceptron, we can clearly say that Multilayer Perceptron acts as the best model for the dataset provided. ","5fec5d45":"### Some Interesting Insights drawn from the Dataset and Models<br>\n**Why 'Gender' is related to 'Life Struggles'?**\nTo answer the question of why 'Life Struggles' is related to 'Gender', I would first point to the strong correlation in 'Gender' with 'height' and 'weight'. Thus, we can say that 'Life Struggles' is strongly correlated with 'height' and 'weight'. Women are shorter in height and lighter in weight than men. The original phrase for 'Life Struggles' is 'I cry when I feel down or things don't go the right way'. Since, womenn used this phrase a lot in the dataset, 'Life Struggles' and 'Gender' are said to be correlated strongly.<br><br>\n**Why 'Gender' is related to 'Western'?**\nUnlike the previous argument, in case of correlation between 'Western' and 'Gender', from in-depth analysis of data, we get to know that 75% of the people have voted more than 3 when asked whether they like 'Western Movies' and majority of the people have voted 5 for the question. These statistics show that most of the people like 'Western Movies' irrespective of the 'Gender'.<br><br>\n**Is the Multilayer Perceptron really learning or just memorizing the dataset?**\nThis question is really an interesting one. As you increase the hidden layers and the neurons in those hidden layers, you clearly find that the training and testing accuracy shoot up. I tried to understand why this phenonmenon is happening and got to know of the memorization capabilities of the deep neural networks from a Technical paper published by several Deep Networks scholars from MIT and Google. The link to the paper is <a href=\"https:\/\/arxiv.org\/pdf\/1706.05394.pdf\">this<\/a>. By understanding how the deep networks are capable of memorizing the datasets with relatively smaller size like the one used in this project. In order to get the best accuracy, I would say Deep Neural Networks are the best for such datasets. But I also tend to doubt my model to a certain extent because it is a sheer memorization process and not an actual training that is happening. Because the scope of the project was not very huge, I would not dig much deeper into it and conclude that using Deep Neural Networks like a Multilayer Perceptron is the best strategy to gain maximum advantage. ","60c4bc82":"**Problem Statement** : You are working for a non-profit that is recruiting student volunteers to help with Alzheimer's patients. You have been tasked with predicting how suitable a person is for this task by predicting how empathetic he or she is on the scale of 1 to 5. ","eef78bd1":"**Dataset Information** : The dataset used is called Young People Survey Dataset from Kaggle (*https:\/\/www.kaggle.com\/miroslavsabo\/young-people-survey\/*). It contains 1010 rows and 150 columns.","cd2b7998":"As you can see, there are many features related to gender. Most correlated features are height, weight, PC, Cars, Action and War. Most of them are very clear to understant like men are more into war, action and cars. The person who likes Biology would also most likely, like Maths and Chemistry.\n\nBut there are some features which left me puzzled like how can 'gender be related to western movies' and 'life struggles be related to height and weight'. I did some research about it which I would like to disclose it at the end. To know the mystery behind what's going on, keep reading.......","bbd64853":"Let's try to predict using Logistic Regression model. ","f800ae46":"### Decision Tree Model","53ac2792":"## Data Preprocessing","a21dcbd7":"# Neural Networks Approach for Predictions of Empathy","64e0335e":"## Data Visualization","0e896960":" Let's predict the empathy using Multilayer Perceptron. <br>\n *Note: While using Multilayer Perceptron, it is needed to have normalized dataset.*","58a0220d":"*\"Most people think empathy is something that you reserve for your life, your family or your friends but the reality is that it is an existential priority for a business. Our business is to meet unmet, unarticulated needs of our customers. There is no way that innovation is gonna come out if we don\u2019t listen to the words and understand the true meaning behind them by being empathetic.\" - Satya Nadella, CEO of Microsoft.*"}}