{"cell_type":{"ceacbc11":"code","f01a27f5":"code","219b4b37":"code","2c0dae5e":"code","95dfade6":"code","0dcbb15c":"code","c4812b22":"code","4cb045ac":"code","936a5566":"code","197dcbcd":"code","91fceb2d":"code","bb465db2":"code","a3ebe421":"code","58ecdfff":"code","0843ea81":"code","c152fab7":"code","ba15f20d":"code","c35b3592":"code","eac85a6c":"code","f6716481":"code","5fd58bf6":"code","3d63dec4":"code","f499fd33":"code","d45044e5":"code","e519b110":"code","ba9b8fc0":"markdown","2cc25805":"markdown","ce3e6b6e":"markdown","a5761933":"markdown","db52642b":"markdown","26acc31a":"markdown","389771bf":"markdown","be184f79":"markdown","ce6c96b5":"markdown","bbae3436":"markdown","0f65d2ae":"markdown","49a4440a":"markdown","0f1ad753":"markdown","82a50124":"markdown","56f13d92":"markdown","bb018c29":"markdown","ff7816dd":"markdown","2c1463cb":"markdown","c7509015":"markdown","307ce20e":"markdown","02cb8593":"markdown"},"source":{"ceacbc11":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport lightgbm as lgb\n#import dask_xgboost as xgb\n#import dask.dataframe as dd\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport os\nfrom tqdm import tqdm, tqdm_notebook\nfrom scipy.sparse import csr_matrix\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f01a27f5":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns: #columns\u6bce\u306b\u51e6\u7406\n        col_type = df[col].dtypes\n        if col_type in numerics: #numerics\u306e\u30c7\u30fc\u30bf\u578b\u306e\u7bc4\u56f2\u5185\u306e\u3068\u304d\u306b\u51e6\u7406\u3092\u5b9f\u884c. \u30c7\u30fc\u30bf\u306e\u6700\u5927\u6700\u5c0f\u5024\u3092\u5143\u306b\u30c7\u30fc\u30bf\u578b\u3092\u52b9\u7387\u7684\u306a\u3082\u306e\u306b\u5909\u66f4\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n","219b4b37":"def read_data():\n    print('Reading files...')\n    calendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\n    calendar = reduce_mem_usage(calendar)\n    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n    \n    sell_prices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\n    sell_prices = reduce_mem_usage(sell_prices)\n    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n    \n    sales_train_val = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\n    print('Sales train validation has {} rows and {} columns'.format(sales_train_val.shape[0], sales_train_val.shape[1]))\n    \n    submission = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\n    \n    return calendar, sell_prices, sales_train_val, submission","2c0dae5e":"import IPython\n\ndef display(*dfs, head=True):\n    for df in dfs:\n        IPython.display.display(df.head() if head else df)","95dfade6":"calendar, sell_prices, sales_train_val, submission = read_data()","0dcbb15c":"# \u4e88\u6e2c\u671f\u9593\u3068item\u6570\u306e\u5b9a\u7fa9\nNUM_ITEMS = sales_train_val.shape[0]  # 30490\nDAYS_PRED = submission.shape[1] - 1  # 28","c4812b22":"def encode_categorical(df, cols):\n    \n    for col in cols:\n        # Leave NaN as it is.\n        le = LabelEncoder()\n        not_null = df[col][df[col].notnull()]\n        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n\n    return df\n\n\ncalendar = encode_categorical(\n    calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n).pipe(reduce_mem_usage)\n\nsales_train_val = encode_categorical(\n    sales_train_val, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n).pipe(reduce_mem_usage)\n\nsell_prices = encode_categorical(sell_prices, [\"item_id\", \"store_id\"]).pipe(\n    reduce_mem_usage\n)\n","4cb045ac":"nrows = 365 * 2 * NUM_ITEMS","936a5566":"#\u52a0\u5de5\u524d  \ndisplay(sales_train_val.head(5))","197dcbcd":"# date columns\ncols_date = [c for c in sales_train_val.columns if 'd_' in c]\n\n# add pre-sales flag\nsales_train_val['pre_sale_flag'] = True\n\n# replace pre-sales demand as -1\nfor c in tqdm_notebook(cols_date):\n    sales_train_val.loc[(sales_train_val[c]>0)&(sales_train_val['pre_sale_flag']),'pre_sale_flag'] = False\n    sales_train_val.loc[sales_train_val['pre_sale_flag'],c] = -1\n\n# drop pre-sales flag\nsales_train_val.drop('pre_sale_flag',axis=1,inplace=True)","91fceb2d":"sales_train_val = pd.melt(sales_train_val,\n                                     id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                                     var_name = 'day', value_name = 'demand')","bb465db2":"# drop demand=-1 rows\nsales_train_val = sales_train_val[sales_train_val['demand']>=0]","a3ebe421":"#\u52a0\u5de5\u5f8c  \ndisplay(sales_train_val.head(5))\nprint('Melted sales train validation has {} rows and {} columns'.format(sales_train_val.shape[0],\n                                                                            sales_train_val.shape[1]))","58ecdfff":"sales_train_val = sales_train_val.iloc[-nrows:,:]","0843ea81":"# seperate test dataframes\n\n# submission file\u306eid\u306evalidation\u90e8\u5206\u3068, ealuation\u90e8\u5206\u306e\u540d\u524d\u3092\u53d6\u5f97\ntest1_rows = [row for row in submission['id'] if 'validation' in row]\ntest2_rows = [row for row in submission['id'] if 'evaluation' in row]\n\n# submission file\u306evalidation\u90e8\u5206\u3092test1, ealuation\u90e8\u5206\u3092test2\u3068\u3057\u3066\u53d6\u5f97\ntest1 = submission[submission['id'].isin(test1_rows)]\ntest2 = submission[submission['id'].isin(test2_rows)]\n\n# test1, test2\u306e\u5217\u540d\u306e\"F_X\"\u306e\u7b87\u6240\u3092d_XXX\"\u306e\u5f62\u5f0f\u306b\u5909\u66f4\ntest1.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + DAYS_PRED)]\ntest2.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n\n# test2\u306eid\u306e'_evaluation'\u3092\u7f6e\u63db\n#test1['id'] = test1['id'].str.replace('_validation','')\ntest2['id'] = test2['id'].str.replace('_evaluation','_validation')\n\n# sales_train_val\u304b\u3089id\u306e\u8a73\u7d30\u90e8\u5206(item\u3084department\u306a\u3069\u306eid)\u3092\u91cd\u8907\u306a\u304f\u4e00\u610f\u306b\u53d6\u5f97\u3002\nproduct = sales_train_val[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n\n# id\u3092\u30ad\u30fc\u306b\u3057\u3066, id\u306e\u8a73\u7d30\u90e8\u5206\u3092test1, test2\u306b\u7d50\u5408\u3059\u308b.\ntest1 = test1.merge(product, how = 'left', on = 'id')\ntest2 = test2.merge(product, how = 'left', on = 'id')\n\n# test1, test2\u3092\u3068\u3082\u306bmelt\u51e6\u7406\u3059\u308b.\uff08\u58f2\u4e0a\u6570\u91cf:demand\u306f0\uff09\ntest1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                var_name = 'day', value_name = 'demand')\n\ntest2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                var_name = 'day', value_name = 'demand')\n\n# validation\u90e8\u5206\u3068, evaluation\u90e8\u5206\u304c\u308f\u304b\u308b\u3088\u3046\u306bpart\u3068\u3044\u3046\u5217\u3092\u4f5c\u308a\u3001 test1,test2\u306e\u30e9\u30d9\u30eb\u3092\u4ed8\u3051\u308b\u3002\nsales_train_val['part'] = 'train'\ntest1['part'] = 'test1'\ntest2['part'] = 'test2'\n\n# sales_train_val\u3068test1, test2\u306e\u7e26\u7d50\u5408.\ndata = pd.concat([sales_train_val, test1, test2], axis = 0)\n\n# memory\u306e\u958b\u653e\ndel sales_train_val, test1, test2\n\n# delete test2 for now(6\/1\u4ee5\u524d\u306f, validation\u90e8\u5206\u306e\u307f\u63d0\u51fa\u306e\u305f\u3081.)\ndata = data[data['part'] != 'test2']\n\ngc.collect()","c152fab7":"#calendar\u306e\u7d50\u5408\n# drop some calendar features(\u4e0d\u8981\u306a\u5909\u6570\u306e\u524a\u9664:weekday\u3084wday\u306a\u3069\u306fdatetime\u5909\u6570\u304b\u3089\u5f8c\u307b\u3069\u4f5c\u6210\u3067\u304d\u308b\u3002)\ncalendar.drop(['weekday', 'wday', 'month', 'year'], \n              inplace = True, axis = 1)\n\n# notebook crash with the entire dataset (maybee use tensorflow, dask, pyspark xD)(day\u3068d\u3092\u30ad\u30fc\u306bdata\u306b\u7d50\u5408)\ndata = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\ndata.drop(['d', 'day'], inplace = True, axis = 1)\n\n# memory\u306e\u958b\u653e\ndel  calendar\ngc.collect()\n\n#sell price\u306e\u7d50\u5408\n# get the sell price data (this feature should be very important)\ndata = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\nprint('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n\n# memory\u306e\u958b\u653e\ndel  sell_prices\ngc.collect()","ba15f20d":"data.head(3)","c35b3592":"def simple_fe(data):\n    \n    # demand features(\u904e\u53bb\u306e\u6570\u91cf\u304b\u3089\u5909\u6570\u751f\u6210)\n    \n    for diff in [0, 1, 2]:\n        shift = DAYS_PRED + diff\n        data[f\"shift_t{shift}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(shift)\n        )\n\n    for size in [7, 30, 60, 90, 180]:\n        data[f\"rolling_std_t{size}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(size).std()\n        )\n\n    for size in [7, 30, 60, 90, 180]:\n        data[f\"rolling_mean_t{size}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(size).mean()\n        )\n\n    data[\"rolling_skew_t30\"] = data.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(30).skew()\n    )\n    data[\"rolling_kurt_t30\"] = data.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(30).kurt()\n    )\n    \n    # price features\n    # price\u306e\u52d5\u304d\u3068\u7279\u5fb4\u91cf\u5316\uff08\u4fa1\u683c\u306e\u5909\u5316\u7387\u3001\u904e\u53bb1\u5e74\u9593\u306e\u6700\u5927\u4fa1\u683c\u3068\u306e\u6bd4\u306a\u3069\uff09\n    \n    data[\"shift_price_t1\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.shift(1)\n    )\n    data[\"price_change_t1\"] = (data[\"shift_price_t1\"] - data[\"sell_price\"]) \/ (\n        data[\"shift_price_t1\"]\n    )\n    data[\"rolling_price_max_t365\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.shift(1).rolling(365).max()\n    )\n    data[\"price_change_t365\"] = (data[\"rolling_price_max_t365\"] - data[\"sell_price\"]) \/ (\n        data[\"rolling_price_max_t365\"]\n    )\n\n    data[\"rolling_price_std_t7\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(7).std()\n    )\n    data[\"rolling_price_std_t30\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(30).std()\n    )\n    \n    # time features\n    # \u65e5\u4ed8\u306b\u95a2\u3059\u308b\u30c7\u30fc\u30bf\n    dt_col = \"date\"\n    data[dt_col] = pd.to_datetime(data[dt_col])\n    \n    attrs = [\n        \"year\",\n        \"quarter\",\n        \"month\",\n        \"week\",\n        \"day\",\n        \"dayofweek\",\n        \"is_year_end\",\n        \"is_year_start\",\n        \"is_quarter_end\",\n        \"is_quarter_start\",\n        \"is_month_end\",\n        \"is_month_start\",\n    ]\n\n    for attr in attrs:\n        dtype = np.int16 if attr == \"year\" else np.int8\n        data[attr] = getattr(data[dt_col].dt, attr).astype(dtype)\n\n    data[\"is_weekend\"] = data[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n    \n    return data\n\ndata = simple_fe(data)\ndata = reduce_mem_usage(data)","eac85a6c":"display(data.head())","f6716481":"# going to evaluate with the last 28 days\nx_train = data[data['date'] <= '2016-03-27']\ny_train = x_train['demand']\nx_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\ny_val = x_val['demand']\ntest = data[(data['date'] > '2016-04-24')]\n\n#data\u306e\u524a\u9664\uff08\u30e1\u30e2\u30ea\u306e\u524a\u9664\uff09\n#del data\n#gc.collect()","5fd58bf6":"# define random hyperparammeters for LGBM\nfeatures = [\n    \"item_id\",\n    \"dept_id\",\n    \"cat_id\",\n    \"store_id\",\n    \"state_id\",\n    \"event_name_1\",\n    \"event_type_1\",\n    \"event_name_2\",\n    \"event_type_2\",\n    \"snap_CA\",\n    \"snap_TX\",\n    \"snap_WI\",\n    \"sell_price\",\n    # demand features.\n    \"shift_t28\",\n    \"shift_t29\",\n    \"shift_t30\",\n    \"rolling_std_t7\",\n    \"rolling_std_t30\",\n    \"rolling_std_t60\",\n    \"rolling_std_t90\",\n    \"rolling_std_t180\",\n    \"rolling_mean_t7\",\n    \"rolling_mean_t30\",\n    \"rolling_mean_t60\",\n    \"rolling_mean_t90\",\n    \"rolling_mean_t180\",\n    \"rolling_skew_t30\",\n    \"rolling_kurt_t30\",\n    # price features\n    \"price_change_t1\",\n    \"price_change_t365\",\n    \"rolling_price_std_t7\",\n    \"rolling_price_std_t30\",\n    # time features.\n    \"year\",\n    \"month\",\n    \"week\",\n    \"day\",\n    \"dayofweek\",\n    \"is_year_end\",\n    \"is_year_start\",\n    \"is_quarter_end\",\n    \"is_quarter_start\",\n    \"is_month_end\",\n    \"is_month_start\",\n    \"is_weekend\",\n]\n\nparams = {\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'objective': 'regression',\n    'n_jobs': -1,\n    'seed': 236,\n    'learning_rate': 0.1,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 10, \n    'colsample_bytree': 0.75}\n\ntrain_set = lgb.Dataset(x_train[features], y_train)\nval_set = lgb.Dataset(x_val[features], y_val)\n\ndel x_train, y_train\n\n# model estimation\nmodel = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, valid_sets = [train_set, val_set], verbose_eval = 100)\nval_pred = model.predict(x_val[features])\nval_score = np.sqrt(metrics.mean_squared_error(val_pred, y_val))\nprint(f'Our val rmse score is {val_score}')\ny_pred = model.predict(test[features])\ntest['demand'] = y_pred","3d63dec4":"predictions = test[['id', 'date', 'demand']]\npredictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\npredictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\nevaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \nevaluation = submission[submission['id'].isin(evaluation_rows)]\n\nvalidation = submission[['id']].merge(predictions, on = 'id')\nfinal = pd.concat([validation, evaluation])\nfinal.to_csv('submission.csv', index = False)","f499fd33":"weight_mat = np.c_[np.identity(NUM_ITEMS).astype(np.int8), #item :level 12\n                   np.ones([NUM_ITEMS,1]).astype(np.int8), # level 1\n                   pd.get_dummies(product.state_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.store_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.store_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.store_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.item_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.item_id.astype(str),drop_first=False).astype('int8').values\n                   ].T\n\nweight_mat_csr = csr_matrix(weight_mat)\ndel weight_mat; gc.collect()\n\ndef weight_calc(data,product):\n\n    # calculate the denominator of RMSSE, and calculate the weight base on sales amount\n    \n    sales_train_val = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\n    \n    d_name = ['d_' + str(i+1) for i in range(1913)]\n    \n    sales_train_val = weight_mat_csr * sales_train_val[d_name].values\n    \n    # calculate the start position(first non-zero demand observed date) for each item \/ \u5546\u54c1\u306e\u6700\u521d\u306e\u58f2\u4e0a\u65e5\n    # 1-1914\u306eday\u306e\u6570\u5217\u306e\u3046\u3061, \u58f2\u4e0a\u304c\u5b58\u5728\u3057\u306a\u3044\u65e5\u3092\u4e00\u65e60\u306b\u3057\u30010\u30929999\u306b\u7f6e\u63db\u3002\u305d\u306e\u3046\u3048\u3067minimum number\u3092\u8a08\u7b97\n    df_tmp = ((sales_train_val>0) * np.tile(np.arange(1,1914),(weight_mat_csr.shape[0],1)))\n    \n    start_no = np.min(np.where(df_tmp==0,9999,df_tmp),axis=1)-1\n    \n    \n    # denominator of RMSSE \/ RMSSE\u306e\u5206\u6bcd\n    weight1 = np.sum((np.diff(sales_train_val,axis=1)**2),axis=1)\/(1913-start_no)\n    \n    # calculate the sales amount for each item\/level\n    df_tmp = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n    df_tmp['amount'] = df_tmp['demand'] * df_tmp['sell_price']\n    df_tmp =df_tmp.groupby(['id'])['amount'].apply(np.sum).values\n    \n    weight2 = weight_mat_csr * df_tmp \n\n    weight2 = weight2\/np.sum(weight2)\n    \n    del sales_train_val\n    gc.collect()\n    \n    return weight1, weight2\n\n\nweight1, weight2 = weight_calc(data,product)\n\ndef wrmsse(preds, data):\n    \n    # actual obserbed values \/ \u6b63\u89e3\u30e9\u30d9\u30eb\n    y_true = data.get_label()\n    \n    # number of columns\n    num_col = len(y_true)\/\/NUM_ITEMS\n    \n    # reshape data to original array((NUM_ITEMS*num_col,1)->(NUM_ITEMS, num_col) ) \/ \u63a8\u8ad6\u306e\u7d50\u679c\u304c 1 \u6b21\u5143\u306e\u914d\u5217\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u76f4\u3059\n    reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n    reshaped_true = y_true.reshape(num_col, NUM_ITEMS).T\n    \n    x_name = ['pred_' + str(i) for i in range(num_col)]\n    x_name2 = [\"act_\" + str(i) for i in range(num_col)]\n          \n    train = np.array(weight_mat_csr*np.c_[reshaped_preds, reshaped_true])\n    \n    score = np.sum(\n                np.sqrt(\n                    np.mean(\n                        np.square(\n                            train[:,:num_col] - train[:,num_col:])\n                        ,axis=1) \/ weight1) * weight2)\n    \n    return 'wrmsse', score, False\n\ndef wrmsse_simple(preds, data):\n    \n    # actual obserbed values \/ \u6b63\u89e3\u30e9\u30d9\u30eb\n    y_true = data.get_label()\n    \n    # number of columns\n    num_col = len(y_true)\/\/NUM_ITEMS\n    \n    # reshape data to original array((NUM_ITEMS*num_col,1)->(NUM_ITEMS, num_col) ) \/ \u63a8\u8ad6\u306e\u7d50\u679c\u304c 1 \u6b21\u5143\u306e\u914d\u5217\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u76f4\u3059\n    reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n    reshaped_true = y_true.reshape(num_col, NUM_ITEMS).T\n          \n    train = np.c_[reshaped_preds, reshaped_true]\n    \n    weight2_2 = weight2[:NUM_ITEMS]\n    weight2_2 = weight2_2\/np.sum(weight2_2)\n    \n    score = np.sum(\n                np.sqrt(\n                    np.mean(\n                        np.square(\n                            train[:,:num_col] - train[:,num_col:])\n                        ,axis=1) \/  weight1[:NUM_ITEMS])*weight2_2)\n    \n    return 'wrmsse', score, False","d45044e5":"params = {\n    'boosting_type': 'gbdt',\n    'metric': 'custom',\n    'objective': 'regression',\n    'n_jobs': -1,\n    'seed': 236,\n    'learning_rate': 0.1,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 10, \n    'colsample_bytree': 0.75}\n\n# model estimation\nmodel = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, \n                  valid_sets = [train_set, val_set], verbose_eval = 100, feval= wrmsse)\nval_pred = model.predict(x_val[features])\nval_score = np.sqrt(metrics.mean_squared_error(val_pred, y_val))\nprint(f'Our val wrmsse score is {val_score}')\ny_pred = model.predict(test[features])\ntest['demand'] = y_pred","e519b110":"predictions = test[['id', 'date', 'demand']]\npredictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\npredictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\nevaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \nevaluation = submission[submission['id'].isin(evaluation_rows)]\n\nvalidation = submission[['id']].merge(predictions, on = 'id')\nfinal = pd.concat([validation, evaluation])\nfinal.to_csv('submission2.csv', index = False)","ba9b8fc0":"### 3.1\u3068\u540c\u69d8\u306b\u4e88\u6e2c\u90e8\u5206(validation\/evaluation\u90e8\u5206)\u306emelt\u51e6\u7406\u3057, \u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u7d50\u5408\u3059\u308b. \u51fa\u529b\u306fdata\u3068\u3044\u3046\u5909\u6570.","2cc25805":"## data\u52a0\u5de5 ","ce3e6b6e":"model\u306eLGBM\u3067\u306e\u63a8\u5b9a\u3000\u3000\n* early stopping\u306emetric\u306b\u5168\u4f53\u306eRMSE\u3092\u4f7f\u3063\u3066\u3044\u308b\u305f\u3081, \u30b3\u30f3\u30da\u306e\u6307\u6a19\u306eWRMSSE\u3068\u306f\u7570\u306a\u308b.","a5761933":"## WRMSSE calculation","db52642b":"reduce_mem_usage\u306f\u3001\u30c7\u30fc\u30bf\u306e\u30e1\u30e2\u30ea\u3092\u6e1b\u3089\u3059\u305f\u3081\u306b\u30c7\u30fc\u30bf\u578b\u3092\u5909\u66f4\u3059\u308b\u95a2\u6570\u3067\u3059\u3002  https:\/\/qiita.com\/hiroyuki_kageyama\/items\/02865616811022f79754\u3000\u3092\u53c2\u7167\u304f\u3060\u3055\u3044\u3002","26acc31a":"\u4e88\u6e2c\u90e8\u5206\u306esubmission file\u3092\u540c\u3058\u304fmelt\u51e6\u7406\u3057\u3001sales_train_val\u3068\u3064\u306a\u3052\u308b\u3002  \n\u51e6\u7406\u306e\u6ce8\u610f\u70b9:  \n* submission file\u306e\u5217\u540d\u3092\"d_xx\"\u5f62\u5f0f\u306b\u5909\u66f4\u3059\u308b. submission file\u3067\u7e26\u306b\u7d50\u5408\u3055\u308c\u305fvalidation\u3068evaluation\u3092\u4e00\u5ea6\u5206\u5272\u3057\u3001\u305d\u308c\u305e\u308c\u3053\u3068\u306a\u308b28\u65e5\u9593\u306e\u5217\u540d\"d_xx\"\u3092\u305d\u308c\u305e\u308c\u4ed8\u4e0e\u3002\n* submission file\u306b\u306f, id\u306e\u8a73\u7d30\uff08item, department, state\u7b49\uff09\u304c\u7121\u3044\u305f\u3081id\u3092\u30ad\u30fc\u306b, sales validation\u304b\u3089\u53d6\u5f97\u3057\u305fproduct\u3092\u7d50\u5408\n* test2\u306f\u30016\/1\u307e\u3067\u4e0d\u8981\u306a\u305f\u3081\u524a\u9664","389771bf":"## function\u306e\u5b9a\u7fa9","be184f79":"LightGBM\u306eMetric\u3068\u3057\u3066, WRMSSE\u306e\u52b9\u7387\u7684\u306a\u8a08\u7b97\u3092\u884c\u3046\u3002\u3042\u304f\u307e\u3067, 28day-lag\u30671\u3064\u306e\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3059\u308b\u3068\u304d\u306bLGBM\u3067\u52b9\u7387\u7684\u306aWRMSSE\u306e\u8a08\u7b97\u3092\u884c\u3046\u5834\u5408\u3067\u3042\u308b\u3002\n* weight_mat\u3068\u3044\u30460 or 1\u306e\u758e\u884c\u5217\u3067\u3001\u52b9\u7387\u7684\u306baggregation level\u3092\u884c\u5217\u7a4d\u3067\u8a08\u7b97\u51fa\u6765\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u308b","ce6c96b5":"### 2.sales_train_validation\u306emelt\u51e6\u7406  \n\uff08\u6642\u7cfb\u5217\u306e\u7279\u5fb4\u91cf\u304c\u4f5c\u308a\u3084\u3059\u3044\u3088\u3046\u306b, id\u6bce\u306b\u6a2a\u306b\u4e26\u3093\u3060\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u3001\uff08id , \u6642\u7cfb\u5217\uff09\u3067\u7e26\u306b\u5909\u63db\uff09","bbae3436":"As [@kaushal2896](https:\/\/www.kaggle.com\/kaushal2896) suggested in [this comment](https:\/\/www.kaggle.com\/harupy\/m5-baseline#770558), encode the categorical columns before merging to prevent the notebook from crashing even with the full dataset. [@harupy](https:\/\/www.kaggle.com\/harupy) also use this encoding suggested in [m5-baseline](https:\/\/www.kaggle.com\/harupy\/m5-baseline).  \n\u30e1\u30e2\u30ea\u306e\u52b9\u7387\u5229\u7528\u306e\u305f\u3081, \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092\u3042\u3089\u304b\u3058\u3081Label encoding.","0f65d2ae":"### 4.data\u306bcalendar\/sell_prices\u3092\u7d50\u5408","49a4440a":"## module import","0f1ad753":"Pandas\u306edataFrame\u3092\u304d\u308c\u3044\u306b\u8868\u793a\u3059\u308b\u95a2\u6570","82a50124":"pandas\u306emelt\u3092\u4f7f\u3044demand(\u58f2\u4e0a\u6570\u91cf)\u3092\u7e26\u306b\u4e26\u3079\u308b.  \n* pandas\u306emelt\u306f https:\/\/qiita.com\/ishida330\/items\/922caa7acb73c1540e28\u3000\u3092\u53c2\u7167\u304f\u3060\u3055\u3044\u3002\n* data\u306e\u884c\u6570\u304c\u83ab\u5927\u306b\u306a\u308b\u306e\u3067, Kaggle Notebook\u306ememory\u5236\u9650\u3092\u8003\u616e\u3057\u3001nrows\u3067\u76f4\u8fd1365*2\u65e5\u5206\uff082\u5e74\u5206\uff09\u306e\u30c7\u30fc\u30bf\u306b\u9650\u5b9a\uff08TODO:\u74b0\u5883\u306b\u5fdc\u3058\u3066\u671f\u9593\u3092\u5909\u66f4\uff09\n* \u5546\u54c1\u767a\u58f2\u524d\u306e\u671f\u9593\u306e\u30c7\u30fc\u30bf\u3092\u524a\u9664","56f13d92":"2016\/3\/27\u3088\u308a\u524d\u3092\u5b66\u7fd2\u7528\u30012016\/3\/27~2016\/4\/24\uff0828day\uff09\u3092\u691c\u8a3c\u7528\u3068\u3057\u3066\u5206\u5272  \n\uff08LightGBM\u306eEarly stopping\u306e\u5bfe\u8c61\uff09\n* \u4ea4\u5dee\u691c\u8a3c\u306e\u65b9\u6cd5\u306f\u3044\u308d\u3044\u308d\u3068\u691c\u8a0e\u4f59\u5730\u3042\u308a\u3002","bb018c29":"### 5 data\u304b\u3089\u7279\u5fb4\u91cf\u751f\u6210\n* groupby & transofrm\u306e\u5909\u63db\u65b9\u6cd5\u306f\u3053\u3061\u3089\u3092\u53c2\u7167:https:\/\/qiita.com\/greenteabiscuit\/items\/132e0f9b1479926e07e0\n* shift\/rolling\u306a\u3069\u306e\u5f79\u5272\u306f\u3053\u3061\u3089\u3092\u53c2\u7167:https:\/\/note.nkmk.me\/python-pandas-rolling\/ (\u3053\u3053\u3067melt\u304c\u3046\u307e\u304f\u52b9\u304d\u307e\u3059\u3002)\n\u3000\u30e9\u30b0\u5909\u6570\u3084\u904e\u53bb\u306e\u5e73\u5747\u5024\u306a\u3069\u306e\u7279\u5fb4\u91cf\u304c\u751f\u6210\u3067\u304d\u308b\u3002\n* \u5909\u6570\u306f, \u3059\u3079\u3066lag\u309228\u4ee5\u4e0a\u306b\u3057\u3066, F1~F28\u306e\u4e88\u6e2c\u30921\u3064\u306e\u30e2\u30c7\u30eb\u3067\u8868\u73fe\u3059\u308b\u306e\u304c\u76ee\u7684\u3002\n* TODO\uff1a\u7279\u5fb4\u91cf\u306e\u751f\u6210\u65b9\u6cd5\u306f\u8272\u3005\u5909\u66f4\u53ef\u80fd. Shift\u3084Rolling\u306e\u5024\u306e\u5909\u66f4\u306a\u3069\u306a\u3069","ff7816dd":"read_data\u306f\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u3068, reduce_mem_usage\u306e\u9069\u7528\u3092\u884c\u3046\u95a2\u6570","2c1463cb":"## train\/test\u306e\u5206\u5272\u3068model\u306e\u63a8\u5b9a","c7509015":"This kernel is:  \n- Based on [Very fst Model](https:\/\/www.kaggle.com\/ragnar123\/very-fst-model). Thanks [@ragnar123](https:\/\/www.kaggle.com\/ragnar123).  \n- Based on [m5-baseline](https:\/\/www.kaggle.com\/harupy\/m5-baseline). Thank [@harupy](https:\/\/www.kaggle.com\/harupy).  \nto explain the detail of these great notebook by Japanese especially for beginner.  \n\nAdditionaly, I have added an efficient evaluation of WRSSE for LGBM metric to these kernel.\n\n- Add preprocess for dropping pre-sales demands","307ce20e":"### 1.\u6700\u521d\u306b\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306e\u51e6\u7406","02cb8593":"## submission file\u306e\u51fa\u529b"}}