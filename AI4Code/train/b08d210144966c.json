{"cell_type":{"f31231fa":"code","ddab5d02":"code","7efd047b":"code","3d8d3286":"code","a8516966":"code","569c8c75":"code","14c77bc2":"code","59d837fc":"code","b1c3cc6a":"code","b98e502d":"code","abd796d7":"code","cdbeeb2a":"code","cd4f76f1":"code","ff615383":"code","e5f1f719":"code","3f53420e":"code","dbdf98ff":"code","52ffd20a":"code","8e33fd5c":"code","ea288e04":"code","d7f39578":"code","f1c82b08":"code","2bbb3f12":"code","9cf06f98":"code","773a9e3c":"code","3c2a9454":"markdown","6754a67d":"markdown","8c9f65d6":"markdown","94b2f1bf":"markdown","50c5acb9":"markdown","cbef5b8d":"markdown","7eebd589":"markdown","91ea30ec":"markdown","66424b37":"markdown","bc42ff04":"markdown","0bf95dc2":"markdown","9d68cfad":"markdown","379d9141":"markdown","ba5f0165":"markdown","da8ff4cc":"markdown","14afb982":"markdown","a424a2a1":"markdown","65f40650":"markdown","e0219112":"markdown","c870f89e":"markdown","d8dde15b":"markdown"},"source":{"f31231fa":"import re\nimport os\nimport nltk\nimport string\nimport random\nimport warnings\nimport numpy                               as np\nimport pandas                              as pd \nimport matplotlib.pyplot                   as plt\nimport seaborn                             as sns\nimport plotly.express                      as ex\nimport plotly.graph_objs                   as go\nimport plotly.offline                      as pyo\nimport pymc3                               as pm\nimport theano.tensor                       as T\nfrom plotly.subplots                       import make_subplots\nfrom sklearn.decomposition                 import TruncatedSVD,PCA\nfrom sklearn.feature_extraction.text       import CountVectorizer\nnltk.download('vader_lexicon')\nfrom sklearn.cluster                       import KMeans\nfrom nltk.sentiment.vader                  import SentimentIntensityAnalyzer as SIA\nfrom wordcloud                             import WordCloud,STOPWORDS\nfrom pandas.plotting                       import autocorrelation_plot\nfrom statsmodels.graphics.tsaplots         import plot_acf\nfrom statsmodels.graphics.tsaplots         import plot_pacf\nfrom statsmodels.tsa.seasonal              import seasonal_decompose\nfrom statsmodels.tsa.ar_model              import AR,AutoReg\nfrom statsmodels.tsa.statespace.sarimax    import SARIMAX\nfrom nltk.util                             import ngrams\nfrom nltk                                  import word_tokenize\nfrom nltk.stem                             import PorterStemmer\nfrom nltk.stem                             import WordNetLemmatizer\nfrom wordcloud                             import STOPWORDS\nfrom tqdm.notebook                         import tqdm\n\n%matplotlib inline\n\npyo.init_notebook_mode()\ntqdm.pandas()\n\nwarnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',\n                        FutureWarning)\nwarnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARIMA',\n                        FutureWarning)\n\n\nplt.style.use('ggplot')\npyo.init_notebook_mode()\nplt.rc('figure',figsize=(18,11))\nsns.set_context('paper',font_scale=2)\n\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    \nset_seed()","ddab5d02":"n_data = pd.read_csv('\/kaggle\/input\/million-headlines\/abcnews-date-text.csv',parse_dates=['publish_date'])\nn_data = n_data.rename(columns={'publish_date':'date','headline_text':'text'})\nn_data = n_data.groupby(['date'], as_index = False).agg({'text': ' '.join})\nn_data = n_data.drop_duplicates()\nn_data.head(3)","7efd047b":"#Sorting And Feature Engineering\nn_data['year']         = pd.DatetimeIndex(n_data['date']).year\nn_data['month']        = pd.DatetimeIndex(n_data['date']).month\nn_data['day']          = pd.DatetimeIndex(n_data['date']).day\nn_data['day_of_year']  = pd.DatetimeIndex(n_data['date']).dayofyear\nn_data['quarter']      = pd.DatetimeIndex(n_data['date']).quarter\nn_data['season']       = n_data.month%12 \/\/ 3 + 1","3d8d3286":"yearly = n_data.groupby(['year'], as_index = False).agg({'text': ' '.join})\nmonthly = n_data.groupby(['month'], as_index = False).agg({'text': ' '.join})\n","a8516966":"#Vader Setiment Analysis\nsid = SIA()\nn_data['sentiments']           = n_data['text'].progress_apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\nn_data['Positive Sentiment']   = n_data['sentiments'].progress_apply(lambda x: x['pos']+1*(10**-6)) \nn_data['Neutral Sentiment']    = n_data['sentiments'].progress_apply(lambda x: x['neu']+1*(10**-6))\nn_data['Negative Sentiment']   = n_data['sentiments'].progress_apply(lambda x: x['neg']+1*(10**-6))\nn_data.drop(columns=['sentiments'],inplace=True)\n","569c8c75":"#Number of Words\nn_data['Number_Of_Words'] = n_data.text.apply(lambda x:len(x.split(' ')))\n#Average Word Length\nn_data['Mean_Word_Length'] = n_data.text.apply(lambda x:np.round(np.mean([len(w) for w in x.split(' ')]),2) )\n","14c77bc2":"yearly_average = n_data.groupby(by='year', as_index = False).mean()\nmonthly_average = n_data.groupby(by='month', as_index = False).mean()\n\nyearly['Number_Of_Words'] =  yearly_average['Number_Of_Words'] \nmonthly['Number_Of_Words'] =  monthly_average['Number_Of_Words'] \n\nyearly['Mean_Word_Length'] =  yearly_average['Mean_Word_Length'] \nmonthly['Mean_Word_Length'] =  monthly_average['Mean_Word_Length']\n\nfor i in ['Positive Sentiment','Neutral Sentiment','Negative Sentiment']:\n    yearly[i] =  yearly_average[i] \n    monthly[i] =  monthly_average[i] ","59d837fc":"yearly_d = n_data.year.value_counts()\nfig = go.Figure()\nfig.add_trace(go.Bar(x=yearly_d.index,y=yearly_d.values,name='Article Count'))\n\n\nfig.update_layout(title='Amount of New Headlines per Year')\n\n\nfig.show()","b1c3cc6a":"yearly_n1_word =[]\nyearly_n2_word =[]\nyearly_n3_word =[]\n\nyearly_word_freqs = []\nfor year in tqdm(yearly.year):\n    F = nltk.FreqDist([i for i in ' '.join(yearly.query(f'year=={year}').text).split() if i not in STOPWORDS])\n    yearly_word_freqs.append(F)\n    result = [i for i in F.most_common(3)]\n    yearly_n1_word.append(result[0])\n    yearly_n2_word.append(result[1])\n    yearly_n3_word.append(result[2])\n    ","b98e502d":"fig = make_subplots(rows=3, cols=1,shared_xaxes=True)#,subplot_titles=(f'{scope}ly Deviation in Positive Sentiment',  f'{scope}ly Deviation in Negative Sentiment'))\n\nfor i,dt in zip(range(3),[yearly_n1_word,yearly_n2_word,yearly_n3_word]):\n    words  = [i[0] for i in dt]\n    counts = [i[1] for i in dt]\n    fig.add_trace(go.Scatter(x=yearly.year,y=counts,text=words,mode=\"lines+markers+text\",textposition=\"top center\"\n    ,name=f'#{i+1} Most Used Word'),row=i+1,col=1)\n\n\n\n#fig['layout']['xaxis2']['title'] = scope\nfig.update_layout(height=700, width=900, title_text=\"Yearly Top 3 Most Used Word\")\n\n\nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n))\n\nfig.show()","abd796d7":"disaster_tags = ['earthquake','death','killed','tornado','hurricane','flood','fire','dead','epidemic','wildfire',\n                 'drought','terrorism','landslide','flu','virus']\nyearly_disaster =[]\nfor y in yearly_word_freqs:\n    aux = 0\n    for w in disaster_tags:\n        aux+=y[w]\n    yearly_disaster.append(aux)\n    \nfig = go.Figure()\nfig.add_trace(go.Scatter(x=yearly.year,y=yearly_disaster,name='Number of Disaster Tags'))\nfig.update_layout(title='Yearly Number of Disaster Related Words Used')\nfig.show()","cdbeeb2a":"\nplt.subplot(2,1,1)\nplt.title('Distriubtion Of Sentiments Across Our Tweets',fontsize=19,fontweight='bold')\nsns.kdeplot(n_data['Negative Sentiment'],label='Negative Sentiment',lw=2.5)\nsns.kdeplot(n_data['Positive Sentiment'],label='Positive Sentiment',lw=2.5)\nsns.kdeplot(n_data['Neutral Sentiment'], label='Neutral Sentiment',lw=2.5 )\nplt.legend()\nplt.subplot(2,1,2)\nplt.title('CDF Of Sentiments Across Our Tweets',fontsize=19,fontweight='bold')\nsns.kdeplot(n_data['Negative Sentiment'],cumulative=True ,label='Negative Sentiment',lw=2.5)\nsns.kdeplot(n_data['Positive Sentiment'],cumulative=True ,label='Positive Sentiment',lw=2.5)\nsns.kdeplot(n_data['Neutral Sentiment'],cumulative=True  ,label='Neutral Sentiment' ,lw=2.5)\nplt.xlabel('Sentiment Value',fontsize=19)\nplt.legend()\nplt.tight_layout()\nplt.show()","cd4f76f1":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=n_data.date,y=n_data['Positive Sentiment'],name='Positive Sentiment'))\nfig.add_trace(go.Scatter(x=yearly.year,y=yearly['Positive Sentiment'],name='Yearly Mean Positive Sentiment'))\n\nfig.add_trace(go.Scatter(x=n_data.date,y=n_data['Negative Sentiment'],name='Negative Sentiment'))\nfig.add_trace(go.Scatter(x=yearly.year,y=yearly['Negative Sentiment'],name='Yearly Mean Negative Sentiment'))\n\nfig.update_layout(title='Daily Sentiments Throughout or Time Line')\nfig.show()","ff615383":"f_data = n_data\npartitions = []\npartitions.append(f_data.loc[44:np.round(len(f_data)\/3,0)-1,:])\npartitions.append(f_data.loc[np.round(len(f_data)\/3,0):2*int(len(f_data)\/3)-1,:])\npartitions.append(f_data.loc[2*np.round(len(f_data)\/3,0):3*int(len(f_data)\/3)-1,:])\n\n\n\nneg_part_means =[]\nneg_part_std   =[]\npos_part_means =[]\npos_part_std   =[]\nfor part in partitions:\n    neg_part_means.append(part['Negative Sentiment'].mean())\n    neg_part_std.append(part['Negative Sentiment'].std())\n    pos_part_means.append(part['Positive Sentiment'].mean())\n    pos_part_std.append(part['Positive Sentiment'].std())\n    \nres_df = pd.DataFrame({'Positive Sentiment Mean':pos_part_means,'Negative Sentiment Mean':neg_part_means,'Positive Sentiment SD':pos_part_std,'Negative Sentiment SD':neg_part_std},\n                     index = [f'Partition_{i}' for i in range(1,4)])\n\n\ndef highlight_greater(x):\n    temp = x.copy()\n    temp = temp.round(0).astype(int)\n    m1 = (temp['Partition_1_Mean'] == temp['Partition_2_Mean'])\n    m2 = (temp['Partition_1_SD'] == temp['Partition_2_SD'])\n    m3 = (temp['Partition_1_Mean'] < temp['Partition_2_Mean']+3) & (temp['Partition_1_Mean'] > temp['Partition_2_Mean']-3)\n    m4 = (temp['Partition_1_SD'] < temp['Partition_2_SD']+3) & (temp['Partition_1_SD'] > temp['Partition_2_SD']-3)\n\n    df1 = pd.DataFrame('background-color: ', index=x.index, columns=x.columns)\n    #rewrite values by boolean masks\n    df1['Partition_1_Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'),        df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'),        df1['Partition_2_Mean'])\n    df1['Partition_1_Mean'] = np.where(m3, 'background-color: {}'.format('gold'),           df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(m3, 'background-color: {}'.format('gold'),           df1['Partition_2_Mean'])\n    df1['Partition_1_Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition_2_Mean'])\n\n    df1['Partition_1_SD'] = np.where(~m2, 'background-color: {}'.format('salmon'),        df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(~m2, 'background-color: {}'.format('salmon'),        df1['Partition_2_SD'])\n    df1['Partition_1_SD'] = np.where(m4, 'background-color: {}'.format('gold'),           df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(m4, 'background-color: {}'.format('gold'),           df1['Partition_2_SD'])\n    df1['Partition_1_SD'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition_2_SD'])\n\n    return df1\n\n\n\n#res_df.style.apply(highlight_greater,axis=None)\nres_df = res_df.T\nres_df = pd.DataFrame(res_df.values,columns=res_df.columns,index=['Positive Sentiment','Negative Sentiment','Positive Sentiment','Negative Sentiment'])\nres_df = pd.concat([res_df.iloc[:2,:],res_df.iloc[2:,:]],axis=1)\nres_df.columns = ['Partition_1_Mean','Partition_2_Mean','Partition_3_Mean','Partition_1_SD','Partition_2_SD','Partition_3_SD']\nres_df.style.apply(highlight_greater,axis=None)","e5f1f719":"fig = make_subplots(rows=3, cols=2)\n\nfor idx,prt in enumerate(partitions):\n    fig.add_trace(\n    go.Scatter(x=prt['date'], y=prt['Positive Sentiment'],name=f'Positive Part {idx+1}'),\n    row=idx+1, col=1)\n    fig.add_trace(\n    go.Scatter(x=prt['date'], y=prt['Negative Sentiment'],name=f'Negative Part {idx+1}'),\n    row=idx+1, col=2)\n\nfig.update_layout(height=600, width=900, title_text=\"Distibution Of Daily Sentiments Over Our Time Line For Each Partition\")\nfig.show()","3f53420e":"fig = make_subplots(rows=4, cols=2, subplot_titles=('Observed Pos', 'Observed Neg', 'Trend Pos','Trend Neg','Seasonal Pos','Seasonal Neg','Residual Pos','Residual Neg'))\nscope = 'year'\nb_date_mean = yearly\n\n\nlbl = ['Positive','Negative']\n\nfor idx,column in enumerate(['Positive Sentiment','Negative Sentiment']):\n    \n    res = seasonal_decompose(b_date_mean[column], period=5, model='additive', extrapolate_trend='freq')\n    \n    fig.add_trace(\n    go.Scatter(x=b_date_mean[scope], y=res.observed,name='{} Observed'.format(lbl[idx])),\n    row=1, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=b_date_mean[scope], y=res.trend,name='{} Trend'.format(lbl[idx])),\n    row=2, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=b_date_mean[scope], y=res.seasonal,name='{} Seasonal'.format(lbl[idx])),\n    row=3, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=b_date_mean[scope], y=res.resid,name='{} Residual'.format(lbl[idx])),\n    row=4, col=idx+1)\n            \nfig.update_layout(height=600, width=900, title_text=\"Decomposition Of Our Yearly Average Sentiments into Trend,Level,Seasonality and Residuals\")\nfig.show()","dbdf98ff":"f, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 10))\n\nax[0].set_title('Positive Autocorrelation Analysis ',fontsize=18,fontweight='bold')\nautocorrelation_plot(b_date_mean['Positive Sentiment'],ax=ax[0],lw=3)\nax[1].set_title('Negative Autocorrelation Analysis ',fontsize=18,fontweight='bold')\nautocorrelation_plot(b_date_mean['Negative Sentiment'],ax=ax[1],color='tab:red',lw=3)\n\nplt.tight_layout()\nplt.show()","52ffd20a":"f, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 10))\nax[0].set_ylim(-1.1,1.1)\nax[1].set_ylim(-1.1,1.1)\n\nplot_pacf(b_date_mean['Negative Sentiment'],lags=5, ax=ax[0],title='Partial Autocorrelation Negative Sentiment')\nplot_pacf(b_date_mean['Positive Sentiment'],lags=5, ax=ax[1],color='tab:blue',title='Partial Autocorrelation Positive Sentiment')\nplt.show()","8e33fd5c":"arma_5 = SARIMAX(endog=b_date_mean['Positive Sentiment'],order=(1,0,1)).fit()\n","ea288e04":"fig = plt.figure(figsize=(16,9))\nfig = arma_5.plot_diagnostics(fig=fig, lags=2)","d7f39578":"predicted_AR_1 = arma_5.predict(1)\n\noutput = pd.DataFrame({'Prediction':predicted_AR_1,'Actual':b_date_mean['Positive Sentiment']})\n\nfig = make_subplots(\n    rows=3, cols=2,subplot_titles=('','Actual','Predictions','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"table\",\"rowspan\": 3}     ,{\"type\": \"scatter\"}] ,\n           [None                               ,{\"type\": \"scatter\"}]            ,           \n           [None                               ,{\"type\": \"scatter\"}]                           \n          ]\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=b_date_mean[scope],\n        y=output[\"Actual\"],\n        mode=\"lines+markers\",\n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=b_date_mean[scope],\n        y=output[\"Prediction\"],\n        mode=\"lines+markers\",\n    ),\n    row=2, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Prediction\"])),\n        y=output[\"Prediction\"]-output[\"Actual\"],\n        mode=\"lines+markers\",\n    ),\n    row=3, col=2\n)\n\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['Prediction','Actual'],\n            font=dict(size=10),\n            align=\"left\"\n        ),\n        cells=dict(\n            values=[output[k].tolist() for k in output.columns],\n            align = \"left\")\n    ),\n    row=1, col=1\n)\n\n\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=(output[\"Prediction\"]-output[\"Actual\"]).mean(), x1=len(output[\"Prediction\"]), y1=(output[\"Prediction\"]-output[\"Actual\"]).mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x3', \n        yref='y3'\n)\n\nfig.update_layout(\n    height=800,\n    showlegend=False,\n    title_text=\"Prediction Evaluation\",\n)\n\nfig.show()","f1c82b08":"w1_dict = dict()\njune_text = ' '.join(yearly.text)\nl_t=' '.join(i for i in june_text.split(' ') if i not in STOPWORDS and i.isalpha())\n\nfor word in l_t.split():\n    w= word.strip()\n    if w in STOPWORDS:\n        continue\n    else:\n        w1_dict[w] = w1_dict.get(w,0)+1\nw1_dict = {k: v for k, v in sorted(w1_dict.items(), key=lambda item: item[1],reverse=True)}\n\nw2_dict = dict()\n\ntop_10_w1 = list(w1_dict.keys())[:10]\ntoken=nltk.word_tokenize(l_t)\ntrigram =ngrams(token,3)\ntrigram = [k for k in trigram if k[0] in top_10_w1]\n\ntoken=nltk.word_tokenize(l_t)\nbigram=ngrams(token,2)\nbigram_dict = dict()\nfor i in bigram:\n    bigram_dict[i] = bigram_dict.get(i,0)+1\n    \ntrigram_dict = dict()\nfor i in trigram:\n    trigram_dict[i] = trigram_dict.get(i,0)+1\n    \ntri_gram =pd.DataFrame(list(trigram_dict.keys())[:15],columns=['One Of Top 10 Words','Second Word','Third Word'])\n\ndef get_prob(sir):\n    key = (sir['One Of Top 10 Words'],sir['Second Word'],sir['Third Word'])\n    w3 = trigram_dict[key]\n    w2 = bigram_dict[(sir['One Of Top 10 Words'],sir['Second Word'])]\n    return w3\/w2\n\ntri_gram['Probabilty Of Sentence'] = tri_gram.apply(get_prob,axis=1)\n\ntri_gram.style.background_gradient(cmap='coolwarm')    ","2bbb3f12":"NUMBER_OF_COMPONENTS=400\n\nCV = CountVectorizer()\nsvd = TruncatedSVD(NUMBER_OF_COMPONENTS)\n\nc_matrix = CV.fit_transform(n_data.text)\n\ndec_matrix = svd.fit_transform(c_matrix)\ndec_df=pd.DataFrame(dec_matrix,columns=['PC_{}'.format(i) for i in range(1,NUMBER_OF_COMPONENTS+1)])\n\nex_var = svd.explained_variance_ratio_\nvariance_cum = np.cumsum(ex_var)\ndata = [go.Scatter(x=np.arange(0,len(variance_cum)),y=variance_cum,name='Cumulative Explained Variance',mode='lines+markers'),\n        go.Scatter(x=np.arange(0,len(variance_cum)),y=ex_var,name='Explained Variance',mode='lines+markers')]\nlayout = dict(title='Explained Variance Ratio Using {} Words'.format(NUMBER_OF_COMPONENTS),\n             xaxis_title='# Componenets',yaxis_title='Explained Variance',height=650,width=900)\nfig = go.Figure(data=data,layout=layout)\nfig.update_layout(template='seaborn')\nfig.show()","9cf06f98":"dec_DF = dec_df[['PC_1','PC_2']].copy()\ndec_DF['year'] = n_data.year\ndec_DF['Number_Of_Words'] = n_data.Number_Of_Words\ndec_DF['Positive Sentiment'] = n_data['Positive Sentiment']\ndec_DF['Negative Sentiment'] = n_data['Negative Sentiment']\n\nex.scatter(dec_DF,x='PC_1',y='PC_2',color='year',title='R^2 Representation of Headlines Colored by Year')","773a9e3c":"ex.scatter(dec_DF,x='PC_1',y='PC_2',color='Number_Of_Words',title='R^2 Representation of Headlines Colored by Number of Words')","3c2a9454":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:x;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Time Series Based Features<\/h1>\n","6754a67d":"**Observation**: Decomposing our yearly sentiments into trend, seasonality, and residual components, we see that there is a weak trend in the positive sentiment, which indicates a decrease in overall positivity as we progress in time, which is a bit sad as an insight, but it may be not as bad as we think, at the same time there is a drastic decrease in negative sentiment and because of the properties of sentiment percentage we know that all those proportions in term go to the neutral sentiment which in my opinion is the default for new headlines.","8c9f65d6":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Text Decomposition and Analysis<\/h1>\n\n","94b2f1bf":"**Observation**: Looking at the distributions of sentiments across our data, we see very distinct differences between the means and standard deviations of the distribution.\nWe see that most of the headlines have a high neutral sentiment percentage, and at most, it is accompanied with negative sentiment percentages, which makes sense considering they really tell us positive things on the new, such claim can be strengthened by looking at the tight distribution around zero of our positive sentiments percentage.","50c5acb9":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:x;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Sentiment Analysis via the Vader Lexicon<\/h1>\n","cbef5b8d":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Libraries And Utilities<\/h1>\n","7eebd589":"**Observation**: We see that using a simple moving average model with integrated components that are based on the first lag of our data, we get fairly good results!\n","91ea30ec":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Feature Engineering<\/h1>\n","66424b37":"**Observation**: Interestingly, we see a significant correlation between a given year's sentiment and the lagged one sentiment; such insight sparks a question: Can we use a relatively simple model to model such a connection? Well, we will have to investigate!","bc42ff04":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Modeling Future Posetive Sentiment<\/h1>\n\n","0bf95dc2":"**Observation**: After representing our data in a bag-of-words representation, we see that we need a hefty amount of words in order to explain a meer 70% of the variance in the original domain; such an indicator teaches us how extreme is the variation in the text is in the original domain.\n","9d68cfad":"**Explanation**: The above data was derived by counting at each year the number of \"disaster\" related terms appearing in the news headlines; the list of terms used includes the following words: 'earthquake,' 'death,' 'killed,' 'tornado,' 'hurricane,' 'flood,' 'fire,' 'dead,' 'epidemic,' 'wildfire,' 'drought,' 'terrorism,' 'landslide,' 'flu,' 'virus.'\n\n\n**Observation**: from the above graph, we learn something very fascinating considering the situation the world is in right now with the coronavirus.\nApparently, there were much worse years in terms of \"disasters\" as an insight it is fascinating and gives us some perspective when we joke about \"2020 being the worst year\".","379d9141":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Time Based Analysis<\/h1>\n","ba5f0165":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:x;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Naive Text Features<\/h1>\n","da8ff4cc":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Loading and Preprocessing<\/h1>\n","14afb982":"**Observation**: Looking at the distribution of yearly new headlines, we see that we have a uniform distribution, i.e., each year we have the same amount of headlines; knowing this fact allows us to compare and analyze quantitative features derived from the text between the years without considering sample size inequality. ","a424a2a1":"**Observation**: From a brief look at the average progression of our sentiments through time, we see that overall the sentiment are stationary, excluding a brief period where there is a change in dominance between the negative and positive sentiments - apparently, 2014 is conceded a bit more positive in comparison to following years.","65f40650":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis<\/h1>\n","e0219112":"**Observation**: One potential reason for the distinct clustering we observed in the previous year may be partially due to the average yearly headline length, which, as can be seen in the current plot, also shows a distinct pattern.","c870f89e":"**Observation**: Looking at the yearly top three words, we can observe the most recurrent \"topics\" , also while going down in the word rank, we see that we get a more \"distilled\" understanding of some main events discussed at a given year.","d8dde15b":"**Observation**: In the above plot, we see that there are distinct clusters in the original domain of our data which is clearly visible when projected to R^2, and what unites those clusters is the year of the headline publication."}}