{"cell_type":{"77322597":"code","1405aa5c":"code","4a74a0af":"code","afa12559":"code","848fa750":"code","c2a8d5bf":"code","a091f7d0":"code","b41019af":"code","46051ab3":"code","5e7852d2":"code","f99f1058":"code","cf5b683a":"code","837ecc96":"code","c2982ce6":"code","5bd7c5ac":"code","7afc9b15":"code","79b1a0a2":"code","52d62315":"code","4e7c43b2":"code","5c933d92":"code","08268957":"code","42132c8f":"code","df0b3a05":"code","89b6fd6f":"code","34b01e92":"code","08cb27e1":"code","626275de":"code","80c61a38":"code","9c1366c4":"code","33286805":"code","83afb1fc":"code","f05f7cd4":"code","1d83bd67":"code","28bfdd68":"code","74569113":"code","49bc4556":"code","0b4db998":"code","d37e4c1a":"code","7e11042d":"markdown","dd9b3503":"markdown","7493db2b":"markdown","8a138f02":"markdown","f9dc98d3":"markdown","38d23de0":"markdown","15f1be1f":"markdown","2a37a792":"markdown","ba53ece2":"markdown","0a44d551":"markdown","39fa8132":"markdown","7dfb30c3":"markdown","07ff2ea2":"markdown","10fe809c":"markdown","7d2f2646":"markdown","1911f0dd":"markdown","2116e25a":"markdown","3ab4aa77":"markdown","cf93d053":"markdown","2d69f3dc":"markdown","90b106f5":"markdown","ed9fdd7f":"markdown","e3f7aa40":"markdown","1e4e2c77":"markdown","1cf0b43f":"markdown","97e1cce0":"markdown","e929e0ea":"markdown","c5b63724":"markdown","7a31f917":"markdown","ff980df0":"markdown","63bb1699":"markdown","ebd24da3":"markdown","dbc592f0":"markdown","69b578f7":"markdown","4200c570":"markdown","e2ad7aed":"markdown","1ad58daa":"markdown","3af5a5d0":"markdown","1d75a2f6":"markdown","a0e89e2b":"markdown","368cae40":"markdown","d925d03c":"markdown","2b298cf2":"markdown","6899c104":"markdown","d75b85a4":"markdown","38b1e84f":"markdown","8a6c1a8e":"markdown","b98d83cd":"markdown","6c658ba6":"markdown","97705e00":"markdown","6d0a3c50":"markdown","c14f8905":"markdown"},"source":{"77322597":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcdefaults()\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1405aa5c":"df = pd.read_csv('\/kaggle\/input\/banckingmarket\/bank.csv')\nX1 = df[['job','balance']]","4a74a0af":"df.head()","afa12559":"# NOT CONSIDERING -1 VALUES IN pdays COLUMN\nvalues = []\nfor i,row in df.iterrows():\n    if row[\"pdays\"] > -1:\n        values.append(row[\"pdays\"])","848fa750":"quartile = [0.25,0.50,0.75]\nquartiles = []\nindex = ['count','mean','std','min','25%','50%','75%','max']\nfor i in quartile:\n    quartiles.append(np.quantile(values,i))\nsummary = [len(values),\n           np.mean(values),\n           np.std(values),\n           np.min(values),\n           quartiles[0],\n           quartiles[1],\n           quartiles[2],\n           np.max(values)\n]\nfor i,j in zip(summary,index):\n    print(f'{j}   {i}')","c2a8d5bf":"pdays_mean = np.mean(values)\npdays_median = np.median(values)","a091f7d0":"# Replacing unkown by the minimum level of education in the education column\ndf.loc[(df['education']=='unknown'),'education'] = 'primary'\nprint(df['education'].value_counts())","b41019af":"df1 = df[df['education'] == 'secondary']\ndf2 = df[df['education'] == 'tertiary']\ndf3 = df[df['education'] == 'primary']","46051ab3":"med1 = np.median(df1['balance'])  # 392.0\nmed2 = np.median(df2['balance'])  # 577.0\nmed3 = np.median(df3['balance'])  # 432.0\ncompare = [med1,med2,med3]\nedu_list = ['primary','secondary','tertiary']\nprint(f'Tertiary education has the highest median that is {med2}')","5e7852d2":"plt.figure(figsize = (5,3))\nplt.barh(edu_list, compare, align='center', alpha=0.5)\nfor index, value in enumerate(compare):\n    plt.text(value, index, str(value))\nprint('Horizontal bar graph displaying the median values of column \"balance\" for the different levels of education')","f99f1058":"fig, ax = plt.subplots()\n\nmy_data = [df['pdays'],values]\nax.boxplot(my_data)\nplt.show()\nprint('Described pdays column using boxplot. \\n 1. Considering all the values of pdays column including \"-1\"\\n 2. Considering only the non-negative values of pdays column')","cf5b683a":"for i in ['marital','education','targeted','default','housing','loan','month','poutcome']:\n    df[i].unique()\n    fig, ax = plt.subplots()\n    fig.set_size_inches(6,3)\n    sns.countplot(x = i, data = df)\n    ax.set_xlabel(i)\n    ax.set_ylabel('Count')\n    ax.set_xticklabels(ax.get_xticklabels(),rotation= 45)\n    sns.despine()","837ecc96":"plt.figure(figsize=(13,8))\nsns.countplot(X1['job'])\nplt.show()\nplt.figure(figsize=(13,8))\nsns.violinplot(\n    x='job',\n    y='balance',\n    data=X1\n)\nplt.show()\nprint('In the graphs above, we have displayed the distribution of jobs among the customers through a countplot graph and the balance the customers of different professions have using violin plot')","c2982ce6":"plt.figure(figsize=(16,10))\nsns.heatmap(df.corr(), annot = True)","5bd7c5ac":"# Encoding the 'response' variable with 1 and 0\ndf1.loc[(df1['response']=='yes'),'response'] = 1\ndf1.loc[(df1['response']=='no'),'response'] = 0\ndf1['response']=df1['response'].astype('int64')","7afc9b15":"# Replacing unknown by mode of the job column\ndf.loc[(df['job']=='unknown'),'job'] = df['job'].mode().get(0)\n\n# Replacing unkown by the minimum level of education in the education column\ndf.loc[(df['education']=='unknown'),'education'] = 'primary'\n\nsns.pairplot(data=df,x_vars = ['job','marital','education','targeted','default','housing','loan','day','previous'],\n             y_vars = ['age','salary','balance','month','duration','campaign','pdays','previous'])","79b1a0a2":"plt.subplot(2,1,1)\ndf['previous'].value_counts().nlargest(5).plot(kind='barh')\nplt.xlabel('Count')\nplt.ylabel('Previous Contact')\n\nplt.subplot(2,1,2)\ndf['poutcome'].value_counts().nlargest(5).plot(kind='barh')\nplt.xlabel('Count')\nplt.ylabel('Previous Outcome')","52d62315":"df1=df[df['poutcome']==1]\ndf2=df[df['poutcome']==0]\n\nprint('Response of people who is marked success in previous Campaign\\n',df1['response'].value_counts(),'\\n')\nprint('Response of people who is marked failure in previous Campaign\\n',df2['response'].value_counts())\n","4e7c43b2":"df['pdays'].value_counts().nlargest(5).plot(kind='barh')\nplt.xlabel('Count')\nplt.ylabel('Previous Contct Days')","5c933d92":"for i in df:\n    if df[i].dtypes == object and i != 'contact':\n        df[i] = df[i].astype('category').cat.codes","08268957":"per = df['contact'].value_counts()['unknown']\ntotal = df['contact'].count()\nprint(f'Null values percentage = {per\/total*100}')\ndf.drop(['contact'],axis=1,inplace = True)","42132c8f":"from sklearn.model_selection import train_test_split\nX = df.iloc[:,:-1].values\ny = df.iloc[:,-1].values\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)","df0b3a05":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","89b6fd6f":"# Splitting df into two dataframes X and y\nX = df.iloc[:,:-1]\ny = df.iloc[:,-1]\n\n# Extracting the columns of X and storing them in 'cols' list\ncols = list(X.columns)\n\n# Importing the necessary libraries\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nrfe = RFE(estimator = LogisticRegression())\nrfe.fit(X,y)\nX = rfe.transform(X)\n\ntemp = pd.Series(rfe.support_,index = cols)\nselected_features_rfe = temp[temp==True].index\n\nX=df[selected_features_rfe]\nX.head()","34b01e92":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\ndef calc_VIF(X):\n    vif['variables'] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    return(vif)\n\ncalc_VIF(df)\nimportant_features=[]\nfor i,row in vif.iterrows():\n    if row[\"VIF\"] < 2.5 and row[\"variables\"] != \"response\":\n        print(f'{row[\"variables\"]} ----> {row[\"VIF\"]}')\n        important_features.append(row[\"variables\"])","08cb27e1":"type(X)\nimport statsmodels.api as sm\nfrom scipy import stats\nX2 = sm.add_constant(df)\nest = sm.OLS(y,X2)\nest2 = est.fit()\nprint(est2.summary())","626275de":"X = df[important_features]\nfeatures = X.columns\nX.head()","80c61a38":"X=X.values\ny = df['response'].values","9c1366c4":"from sklearn.metrics import confusion_matrix, accuracy_score\n# Training the model\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train,y_train)\n\n# Testing the model\ny_pred=classifier.predict(X_test)\n\n# Printing the accuracy score\nprint(accuracy_score(y_test,y_pred))\n\n# Printing the confusion matrix\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)","33286805":"from sklearn.model_selection import KFold\nimport numpy\nfrom sklearn.model_selection import cross_val_score\ncv = KFold(n_splits = 10, random_state = 1, shuffle = True)\nscores = cross_val_score(classifier,X,y,scoring = 'accuracy', cv = cv, n_jobs = -1)\n# report performance\nprint('Accuracy: %.3f (%.3f)' % (numpy.mean(scores), numpy.std(scores)))","83afb1fc":"recall = cm[0][0]\/(cm[0][0] + cm[1][0])\nprecision = cm[0][0]\/(cm[0][0] + cm[0][1])\nnumpy.mean(scores)\nprint(f'Recall is -> {recall}\\nPrecision is -> {precision}\\nAccuracy is -> {numpy.mean(scores)}')","f05f7cd4":"features","1d83bd67":"df = pd.read_csv('\/kaggle\/input\/banckingmarket\/bank.csv')\ndf.drop(['contact'],axis = 1, inplace = True)\n\nfor i in df:\n    if df[i].dtypes == object:\n        df[i] = df[i].astype('category').cat.codes\n\nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","28bfdd68":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\naccuracy=[]\nestimators_count=[]\nfor i in range(1,50,2):\n    rf = RandomForestClassifier(n_estimators = i, criterion = 'entropy', random_state = 0)\n    rf.fit(X_train, y_train)\n    y_pred = rf.predict(X_test)\n    accuracy.append(accuracy_score(y_test,y_pred))\n    estimators_count.append(i)\n    print(f'{i} {accuracy_score(y_test,y_pred).round(4)}')\n    \nplt.plot(estimators_count,accuracy)\nplt.xlabel('Number of Estimators')\nplt.ylabel('Accuracy')\nplt.title('No_of_Estimators VS Accuracy')\nplt.grid(b=None)\nplt.show()","74569113":"classifier = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\n \ny_pred = classifier.predict(X_test)","49bc4556":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\ncv = KFold(n_splits = 10, random_state = 1, shuffle = True)\nscores = cross_val_score(classifier,X,y,scoring = 'accuracy', cv = cv, n_jobs = -1)\n# report performance\nprint('Accuracy: %.4f (%.4f)' % (numpy.mean(scores), numpy.std(scores)))","0b4db998":"cm = confusion_matrix(y_test, y_pred)\naccuracy_score(y_test, y_pred)\n\nrecall = cm[0][0]\/(cm[0][0] + cm[1][0])\nprecision = cm[0][0]\/(cm[0][0] + cm[0][1])\nnp.mean(scores)\nprint(f'Recall is -> {recall.round(4)}\\nPrecision is -> {precision.round(4)}\\nAccuracy is -> {numpy.mean(scores).round(4)}')","d37e4c1a":"importances = classifier.feature_importances_\nfeature_names = df.iloc[:,:-1].columns\nindices = np.argsort(importances)[::-1]\n\nprint(\"Feature ranking\\n\")\nfor i in range(X_train.shape[1]):\n    print(\"%d.   %s   = %f\" % (i + 1, df.columns[indices[i]], importances[indices[i]]))","7e11042d":"From the graph of 'previous' and 'poutcome', its clear that people who is not contacted before in previous campagins are marked '-1'","dd9b3503":"#### Converting X, y dataframe into arrays","7493db2b":"### Plotting the categorical variables","8a138f02":"The evaluation is done above just after the prediction. The results for RF and Logistic Regression using k-Fold Cross Validation are .9002 and .888 respectively which clearly shows that the RF is working better than the Logistic Regresion but, on the other hand, RF is taking more time to training.\n","f9dc98d3":"# Before the predictive modeling part, make sure to perform \u2013\n","38d23de0":"Here we have converted categorical columns into numerical columns","15f1be1f":"\n# RANDOM FOREST CLASSIFICATION","2a37a792":"### Displaying the feature ranking","ba53ece2":"From these graphs, it is clear that whoever is not previously contacted is marked outcome as Unknown.","0a44d551":"### RFE TO REMOVE UNNECESSARY FEATURES","39fa8132":"### Importing Dataset","7dfb30c3":"# CLEANING THE DATASET","07ff2ea2":"### Calculating mean of column 'balance' for each education level","10fe809c":"The smaller the p-value shows that that feature is not suitable for the model as it violates the null-hypothesis which is the feature is good for the model. \n\nThe larger the p-value the better the feature is for the model. From the above table  previous, housing, education and marital are some of the best features","7d2f2646":"# Make a box plot for pdays. Do you see any outliers?","1911f0dd":"### Scaling the features","2116e25a":"# First, perform bi-variate analysis to identify the features that are directly associated with the target variable","3ab4aa77":"### k-Fold Cross Validation","cf93d053":" Here, we have plotted the categorical variables against numerical variables.","2d69f3dc":"Here, we have drawn a heatmap to establish correlation between the features and the 'response' variable","90b106f5":"# Convert the response variable to a convenient form\n","ed9fdd7f":"### ViolinPlot for jobs and balance column","e3f7aa40":"## Train test split","1e4e2c77":"The features shown above are the best features according to VIF, where Variance inflation factor is a measure of the amount of multicollinearity in a set of multiple regression variables. A high VIF indicates that the associated independent variable is highly collinear with the other variables in the model","1cf0b43f":"# In this model, we will follow the features provided by VIF ","97e1cce0":" # Make suitable plots for associations with numerical features and categorical features\u2019","e929e0ea":"## Importing Libraries","c5b63724":"There is only one feature which is doubtfull which is pdays because of -1 value, as 'no previous contact person is marked with 999' but while checking the values, there is no record of '999'\n\nWhile according to the 'previous' and 'poutcome' coulumn, the records which should marked to be '999' re marked as '-1'. There is no need to change the value of -1 to 999 as no other value is olliding with '-1'. So -1 can be considered as it is.   ","7a31f917":"# Are the features about the previous campaign data useful?\n","ff980df0":"##  The necessary transformations for the categorical variables and the numeric variables","63bb1699":"RF has better performance than the Logisitc model as confirmed by k-Fold Cross Validation result.","ebd24da3":"The difference in mean is 184.38","dbc592f0":"### Calculating p-value","69b578f7":"### Important Features","4200c570":"# Plot a horizontal bar graph with the median values of balance for each education level value. Which group has the highest median?","e2ad7aed":"# LOGISTIC REGRESSION","1ad58daa":"### Accuracy using  k-fold Cross Validation","3af5a5d0":"Here we have chosen k-Fold Cross validation as the response variable have large amount of 'no' as compared to 'yes', which shows that the dataset is not balanced. IFf we use accuracy as our metric, any random model can give a very good accuracy, but at the end it would be a random model. TO conquer this problem, we are using k_Fold Cross Validation. ","1d75a2f6":"# Describe the pdays column, make note of the mean, median and minimum values. Anything fishy in the values?\n","a0e89e2b":"# Compare the performance of the Random Forest and the logistic model ","368cae40":"### Creating the confusion matrix and printing the recall, precision and accuracy of the model","d925d03c":"The null-percentage of contact column is almost 30% of the total values of the column therefore, we drop 'contacts' column","2b298cf2":"As we can see, that those who responded positively in previous campaign sill have high response in current campaign, while those who responded negatively still have same opinion.\n\nThis shows that, the results of previous campaign is still affecting the current campaign.","6899c104":"### Calculating VIF","d75b85a4":"# Are pdays and poutcome associated with the target?","38b1e84f":"![](https:\/\/csbcorrespondent.com\/sites\/default\/files\/styles\/blog_feature_full\/public\/blog\/BANK%20MARKETING%20ANALYTICS.jpg?itok=SwPf4x34)","8a6c1a8e":"### Introduction\nThe data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.\n\n### Input variables:\n\n### Bank client data\n1. **age** (numeric)\n2. **job** : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n3. **salary** : amount of salary (numeric)\n4. **marital** : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n5. **education** (categorical: 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate', 'professional.course', 'university.degree', 'unknown')\n6. **targeted** : has been targeted for subscription of term deposit? (categorical: 'no','yes')\n7. **default** : has credit in default? (categorical: 'no','yes','unknown')\n8. **balance** : balance in the account (numeric)\n9. **housing** : has housing loan? (categorical: 'no','yes','unknown')\n10. **loan** : has personal loan? (categorical: 'no','yes','unknown')\n\n### Related with the last contact of the current campaign\n11. **contact** : contact communication type (categorical: 'cellular','telephone')\n12. **day** : last contact day of the week (categorical: '1:mon','2:tue','3:wed','4:thu','5:fri')\n13. **month** : last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n14. **duration**: last contact duration, in seconds (numeric)\n\nImportant note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n### Other attributes\n15. **campaign**: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n16. **pdays**: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n17. **previous**: number of contacts performed before this campaign and for this client (numeric)\n18. **poutcome**: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\n\n### Output variable (desired target)\n19. **response** - has the client subscribed a term deposit? (binary: 'yes','no')","b98d83cd":"In Logistic regression, for selection of features, we have followed VIF and for RF, we have used its inbuilt feature_importance_ attribute to check the features which both the models are using for training and prediction.\n\nIn VIF, we have default, balance, loan, duration, campaign and previous as important features while in RF we have duration, balance, age, day, month, pdays as important features. Therefore, we can say that only 2 features as common from the two models.\n\nAlso, according to the EDA, previous and pdays are having same values but on different scales. So, this can also be considered as common.","6c658ba6":"### Creating different datasets for each education level","97705e00":"Through the countplot we can see that most of the customers are in blue-collar profession.\nThrough the violin plot, we can infer that a few customers working in management have the highest balance as compared to other jobs","6d0a3c50":"##  Handle variables corresponding to the previous campaign\n","c14f8905":"### Precision, Accuracy and Recall of our model"}}