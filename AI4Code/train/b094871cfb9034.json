{"cell_type":{"a7e2aa10":"code","4ab465a2":"code","8dce3b5b":"code","be0a77fa":"code","e0f2e9b9":"code","f98df1a4":"code","78c65a38":"code","b9a561ef":"code","3126710b":"code","2c108f76":"code","60787936":"code","5982cd67":"code","331f9f39":"code","ea3a97c3":"code","40f5e82b":"code","6cf0554c":"code","08833215":"code","fd434503":"code","d158577c":"code","5f11209d":"code","afe85608":"code","db786cd7":"code","dd75141d":"code","8a06e7d6":"code","a6ff40b1":"code","c540ff09":"code","1c7c5915":"code","064c83c2":"code","51f5c0bc":"code","07b2a913":"markdown","706b3317":"markdown","28c4a6e1":"markdown","45aacb35":"markdown","66f0f96f":"markdown","8a646659":"markdown","610b0bd7":"markdown","ef7d9d1c":"markdown","00a0552d":"markdown","7517be2c":"markdown","2f1a3cb7":"markdown","3b4ac2b3":"markdown","24001ed2":"markdown"},"source":{"a7e2aa10":"# Data wrangling \nimport pandas as pd \n\n# Fastetext embeddings\nimport fasttext\n\n# Fasttext utilities\nimport fasttext.util\n\n# Importing regex \nimport re \n\n# Array math \nimport numpy as np\n\n# OS traversal\nimport os \n\n# Zip files\nimport zipfile\n\n# Plotting \nimport matplotlib.pyplot as plt\n\n# Itteration tracking\nfrom tqdm import tqdm\n\n# Machine learning \nimport xgboost as xgb\n\n# Time tracking \nimport time\n\n# Data scalers\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Vectorization of text \nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Hp parameter search \nfrom sklearn.model_selection import ParameterGrid","4ab465a2":"def clean_text(text: str) -> str:\n    \"\"\"\n    Function to clean the text\n    \"\"\"\n    # Lowering \n    text = text.lower()\n    \n    # Leaving only the english letters and numerics\n    text = text.replace('\\n', ' ')\n\n    # Removing the punctuations\n    text = re.sub(r'[^\\w\\s]', ' ', text)\n\n    # Removing the special characters\n    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n\n    # Removing more than 1 whitespaces\n    text = re.sub('\\s+', ' ', text)\n\n    return text","8dce3b5b":"# Defining the input directory\n_input_dir = '\/kaggle\/input\/jigsaw-toxic-severity-rating\/'\n\n# Defining the path to the input file \n_input_file = os.path.join(_input_dir, 'comments_to_score.csv')\n_val_file = os.path.join(_input_dir, 'validation_data.csv')\n\n# Reading the data file\nd = pd.read_csv(_input_file)\n\n# Reading the validation data \ndval = pd.read_csv(_val_file)","be0a77fa":"print(f\"Number of comments:\\n{d.shape[0]}\\nColumns:\\n{d.columns.tolist()}\")","e0f2e9b9":"# Eyeballing some data \nprint(d.sample(1)['text'].tolist())","f98df1a4":"print(f\"Number of observations in validation set:\\n{dval.shape[0]}\")\nprint(f\"Sample of data:\\n{dval.sample(10)}\")","78c65a38":"# Cleaning the main data \nd['clean_text'] = [clean_text(x) for x in tqdm(d['text'], desc='Cleaning the main submission file text', total=len(d))]\ndval['clean_less_toxic'] = [clean_text(x) for x in tqdm(dval['less_toxic'], desc='Cleaning the less toxic text in validation', total=len(dval))]\ndval['clean_more_toxic'] = [clean_text(x) for x in tqdm(dval['more_toxic'], desc='Cleaning the more toxic text in validation', total=len(dval))]","b9a561ef":"# Path to data \naux_path = '\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/'\naux_file_path = os.path.join(aux_path, 'train.csv')\n\n# Reading the data \ndaux = pd.read_csv(aux_file_path)","3126710b":"print(f\"Shape of data:\\n{daux.shape}\\nColumns:\\n{daux.columns.tolist()}\")","2c108f76":"print(daux.head(10))","60787936":"# Cleaning the text \ndaux['comment_text_clean'] = [clean_text(x) for x in tqdm(daux['comment_text'], desc='Preprocesing the comments', total=len(daux))]","5982cd67":"# Defining the toxicity level columns\ntoxicity_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\n# Summing all the columns\ndaux['Y'] = daux.apply(lambda x: x[toxicity_columns].sum(), axis=1)\n\n# Inspecting the distribution of the Y \nagg = daux.groupby(\"Y\", as_index=False).size()\nagg['share_in_data'] = agg['size'] \/ agg['size'].sum()\n\nplt.bar(x=agg['Y'], height=agg['share_in_data'])\nplt.title(f\"Total data points: {agg['size'].sum()}\")\nplt.show()","331f9f39":"# Scaling to 0 - 1 range the Y variable\nscaler = MinMaxScaler()\ndaux['Y'] = scaler.fit_transform(daux['Y'].values.reshape(-1, 1))\n\n# Leaving only the needed columns \ndaux = daux[['comment_text_clean', \"Y\"]].copy()","ea3a97c3":"# Path to data \naux_path = '\/kaggle\/input\/jigsaw-unintended-bias-in-toxicity-classification\/'\naux_file_path = os.path.join(aux_path, 'train.csv')\n\n# Reading the data \ndaux2 = pd.read_csv(aux_file_path, usecols=['comment_text', 'target'])","40f5e82b":"print(f\"Shape of data:\\n{daux2.shape}\\nColumns:\\n{daux2.columns.tolist()}\")","6cf0554c":"print(daux2.head(10))","08833215":"daux2['comment_text_clean'] = [clean_text(x) for x in tqdm(daux2['comment_text'], desc='Cleaning additional data source text', total=len(daux2))]","fd434503":"# Renaming the target to the Y variable \ndaux2.rename(columns={'target': 'Y'}, inplace=True)\n\n# Rearanging\ndaux2 = daux2[['comment_text_clean', 'Y']]","d158577c":"# Concatenating the dataframes\n#train = pd.concat([daux, daux2]).copy()\ntrain = daux.copy()\n\n# Deleting the big objects from memory \ndel daux, daux2","5f11209d":"print(f\"Number of training observations: {train.shape[0]}\")\nprint(f\"Share of 'neutral' comments in the dataset: {round(np.sum(train['Y'] == 0) * 100 \/ train.shape[0], 3)}%\")","afe85608":"# Defining the hyper parameters \nvect_dict = {\n    'max_features': 40000,\n    'ngram_range': (1, 4),\n    'binary': False,\n    'stop_words': 'english',\n}\n\n# Initiating the count vectorizer \nvectorizer = CountVectorizer(**vect_dict)\n\n# Fitting on text \nvectorizer.fit(train['comment_text_clean'])","db786cd7":"# Creating the sparse X matrix \nX = vectorizer.transform(train['comment_text_clean'])\n\n# Extracting the Y variable\nY = train['Y'].values","dd75141d":"# Creating the BOW matrices of the validation data \nbow_less_toxic = vectorizer.transform(dval['clean_less_toxic'])\nbow_more_toxic = vectorizer.transform(dval['clean_more_toxic'])","8a06e7d6":"# Creating the BOW matrix for the final submission \nbow_submission = vectorizer.transform(d['clean_text'])","a6ff40b1":"# Defining a list of hyperparameters \nhp_dict = {\n    \"objective\": ['reg:squarederror'],\n    \"tree_method\": ['gpu_hist'],\n    \"max_depth\": [4, 6, 8],\n    'n_estimators': [200, 400, 600, 800]\n}\n\n# Creating the hp grid \nhp_grid = ParameterGrid(hp_dict)\n\n# Max score tracker  \nmax_score = 0\n\n# Best hp dictionary \nbest_hp = {}\n\nfor hp in hp_grid: \n    # Initiating the empty model\n    reg = xgb.XGBRegressor(**hp)\n\n    # Fitting on data \n    reg.fit(X, Y)\n\n    # Predicting \n    less_toxic_hat = reg.predict(bow_less_toxic)\n    more_toxic_hat = reg.predict(bow_more_toxic)\n\n    # Calculating how many entries are larger in more toxic set\n    # than in less toxic set \n    current_score = np.sum([less_toxic_hat[i] < more_toxic_hat[i] for i in range(len(less_toxic_hat))])\n    current_score = current_score \/ len(less_toxic_hat)\n\n    # Checking if this is the highest auc \n    if current_score > max_score:\n        max_score = current_score \n        best_hp = hp \n        \n        print(f\"New best hp parameters found:\\n{best_hp}\\nBest score: {round(max_score, 3)}\")\n        \n        # Applying the best found model\n        score = reg.predict(bow_submission)","c540ff09":"# Saving the score\nd['score'] = score\n\n# Sorting by distance \nd.sort_values(by='score', inplace=True)","1c7c5915":"# Most \"light\" comments\nd.head(10)","064c83c2":"# Most \"severe\" comments\nd.tail(10)","51f5c0bc":"# File for submission\nd[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","07b2a913":"## Training the xgboost regression model \n\nWe will treat this as a regression problem. \n\nThe ML algorithm of choice is **xgboost.** \n\nTo search for the optimal parameters, we will use a simple grid search. \n\nTo evaluate which parameters are the best, we will use the added validation set. \n\n* Get the score for less toxic \n* Get the score for more toxic \n* Calculate the number of correct predictions","706b3317":"### Creating model digestable input \n\nThis target in this dataset fits perfectly with the target from the previous auxilary data set. The only thing needed is to do some text cleaning. ","28c4a6e1":"## Inspecting the validation data set ","45aacb35":"## Creating input for model \n\nIn this competition, it is important to measure the scale of toxicity. It is importnat to distinguish between low toxicity and high toxicity. \n\nTo encompass that logic, from the collumns \n* toxic\n* severe_toxic\n* obscene\n* threat\n* insult\n* identity_hate\n\nI will create a column $Y$ that is the sum of all of the above columns. The higher the sum - the higher the toxicity. \n\nAt the end, we will scale the Y variable to be in the range of [0, 1]. ","66f0f96f":"## Jigsaw unintended bias competition ","8a646659":"# Text cleaning function \n\nThe preprocesing of text is key in many NLP objectives. The function which will clean data is defined bellow. ","610b0bd7":"# Data reading ","ef7d9d1c":"In order to create a good classifier to evaluate the toxicity of a comment, we need to gather as much prior labeled observations as possible. Luckaly, there are numerous data sources that provide a label for toxicity in one way or another. In this notebook, I shall use data from: \n\n* https:\/\/www.kaggle.com\/julian3833\/jigsaw-toxic-comment-classification-challenge\n* https:\/\/www.kaggle.com\/julian3833\/jigsaw-unintended-bias-in-toxicity-classification\n* This competition's data\n\nThe goal is to create a classifier $f$:\n\n$$f: \\mathbb{X} \\rightarrow \\mathbb{Y}$$\n\nWhere \n\n$\\mathbb{X}$ - comment (text)\n\n$\\mathbb{Y}$ - toxicity score ($\\in$ $\\mathbb{R}$)\n\nThe output score is real number thus we need to convert data in all the datasets to be applicable for a **regression** ML algorithm.\n\n## Main data","00a0552d":"## Merging all the datasources data ","7517be2c":"# Creating sparse matrix for training \n\nIn order to convert text to numbers for any ML algorithm to work with, we will use the bag-of-words representation of the texts. \n\nLets say we have $n$ texts.\n\nFor that representation, we must define the maximum number of words $k$ that we shall use. \n\nThe input matrix $\\mathbb{X}$ will then have the following shape: \n\n$$\\mathbb{X}_{nxk}$$\n\nEach entry in the matrix is either 1 or 0 - corresponding an appereance of a word in text. ","2f1a3cb7":"# Creating the final submission file ","3b4ac2b3":"## Additional data \n\n### Jigsaw toxicity classification challenge","24001ed2":"# Package imports "}}