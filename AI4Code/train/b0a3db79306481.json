{"cell_type":{"ebc98ba6":"code","0c2cc4a1":"code","186f66fd":"code","2ee2213b":"code","1105860c":"code","e529c19d":"code","8375e425":"code","a7731cbf":"code","958bef0c":"code","5360512f":"code","a14e20b7":"code","6fb20df2":"code","5e646df6":"code","0535a21c":"code","8d4e5f28":"code","77135772":"code","ba7d38c0":"code","67f1a775":"code","cbc3b2f3":"code","ce5134e3":"code","16a4e5f0":"code","3b812870":"code","8ad42c92":"code","511e14d6":"code","d207e282":"code","c0f777a2":"code","4fe152f7":"code","c396e7bf":"code","430886fa":"code","0eade188":"code","994bdc64":"code","6fc14faa":"code","1cbbf4bf":"code","207e1fe2":"code","5c45cc24":"code","548259f8":"code","068687bb":"code","bd271e47":"markdown","21e1c52a":"markdown","0a136173":"markdown","b4aba757":"markdown","1eae3759":"markdown","79bb0e6d":"markdown","011306e1":"markdown","7b33dba0":"markdown","825e32b8":"markdown","1cea5f81":"markdown","e5be7ae8":"markdown","8c7fbe84":"markdown","a90b8042":"markdown","25853ce2":"markdown","9d4850ee":"markdown","d801cae0":"markdown","50ac9fd2":"markdown","0bd4d4f4":"markdown","ace69ee1":"markdown","0fb05ed2":"markdown","c28bd16f":"markdown","ce3d987d":"markdown","e99a87c2":"markdown","a92f969f":"markdown","0b562c9b":"markdown","9287c44a":"markdown","45ebcae0":"markdown","2bbe3950":"markdown","923f95c3":"markdown"},"source":{"ebc98ba6":"# General libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\n# Feature extraction\nfrom tsfresh import extract_features, extract_relevant_features, select_features, feature_extraction\nfrom tsfresh.utilities.dataframe_functions import impute\nfrom tsfresh.feature_extraction.feature_calculators import set_property\nfrom tsfresh.feature_extraction import feature_calculators\n\n# Model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Feature importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance","0c2cc4a1":"# Import data set\ndata = pd.read_csv('..\/input\/swarm-behaviour-classification\/Swarm_Behaviour.csv')\ndata.head()","186f66fd":"data.shape","2ee2213b":"# Create a unique id based on the index\ndata = data.reset_index()\ndata = data.rename(columns={\"index\": \"id\"})\n\ndata.head()","1105860c":"# Create target DataFrame\ntarget = data[[\"id\",\"Swarm_Behaviour\"]]\n\ndata = data.drop(columns = \"Swarm_Behaviour\")","e529c19d":"data_columns = [\"time\", \"id\", 'x', 'y', 'xVel', 'yVel', 'xA', 'yA', 'xS', 'yS', 'xC', 'yC', 'nAC', 'nS']","8375e425":"data_rows = pd.DataFrame(columns = data_columns)\n\ndata_rows.head()","a7731cbf":"x = [\"x\" + str(i) for i in range(1,201)]","958bef0c":"data[[\"id\"] + x].head()","5360512f":"temporal = data[[\"id\"] + x].melt(id_vars=[\"id\"], \n            var_name = \"time\",               \n            value_vars = x,\n            value_name = \"x\")\n\ntemporal.head()","a14e20b7":"temporal[\"time\"] = [i[1:] for i in temporal[\"time\"]]\n\ntemporal.head()","6fb20df2":"temporal.tail()","5e646df6":"data_rows = data_rows.append(temporal)\n\ndata_rows.head()","0535a21c":"for column in data_columns[3:]:\n    \n    column_string = [column + str(i) for i in range(1,201)]\n\n    temporal = data[[\"id\"] + column_string].melt(id_vars=[\"id\"], \n                var_name = \"time\",               \n                value_vars= column_string,\n                value_name = column)\n\n    temporal = temporal.drop(columns=[\"time\",\"id\"])\n    \n    data_rows[column] = temporal\n\ndata_rows","8d4e5f28":"target[\"Swarm_Behaviour\"].value_counts().plot(kind=\"bar\")\nplt.show()","77135772":"not_flock = np.array(target[target[\"Swarm_Behaviour\"] == 0][\"id\"])\nflock = np.array(target[target[\"Swarm_Behaviour\"] == 1][\"id\"])\n\n\n# Randomly select which swarm ids we will keep\nnot_flock_random = np.random.choice(not_flock, int(1.2*len(flock)),replace=False)\n\n# Save the rows of the selected swarm ids and all of the flock ids\nrandom_target = target.loc[target[\"id\"].isin(not_flock_random) | target[\"id\"].isin(flock)][\"id\"]\nrandom_data = data_rows[data_rows[\"id\"].isin(not_flock_random) | data_rows[\"id\"].isin(flock)][\"id\"]\n\n# Create the balanced dataset\ndata_rows_balanced = data_rows.loc[data_rows.id.isin(random_data)]\ntarget_balanced = target.loc[target.id.isin(random_target)]\n\nprint(f\"We keep {len(data_rows_balanced)\/(len(data_rows))*100:.2f}% of the original data.\")","ba7d38c0":"data_rows_balanced.head()","67f1a775":"target_balanced[\"Swarm_Behaviour\"].value_counts().plot(kind=\"bar\")\nplt.show()","cbc3b2f3":"target_balanced = target_balanced.set_index(\"id\")","ce5134e3":"target_balanced.head()","16a4e5f0":"target_balanced = pd.Series(data=target_balanced[\"Swarm_Behaviour\"], index=target_balanced.index)","3b812870":"@set_property(\"fctype\", \"simple\")\ndef distance(x):\n    \"\"\"\n    Distance traveled in an axis\n\n    :param x: the time series to calculate the feature of\n    :type x: pandas.Series\n    :return: the value of this feature\n    :return type: bool, int or float\n    \"\"\"\n    # Distance traveled in one axis\n    result = x[-1] - x[0]\n    return result","8ad42c92":"fc_parameters = {\n                 'abs_energy':None, # Returns the absolute energy of the time series which is the sum over the squared values\n                 'absolute_sum_of_changes':None, # Returns the sum over the absolute value of consecutive changes in the series x\n                 'fft_aggregated':[ {'aggtype': 'centroid'}, # Returns the spectral centroid (mean), \n                                    {'aggtype': 'variance'}, # variance, \n                                    {'aggtype': 'skew'},     # skew, \n                                    {'aggtype': 'kurtosis'}], # and kurtosis of the absolute fourier transform spectrum.\n                 'c3': [{'lag': 1}, {'lag': 2}, {'lag': 3}], # Uses c3 statistics to measure non linearity in the time series\n                 'standard_deviation': None, # Returns the standard deviation of x\n                 'variance': None, # Returns the variance of x\n                 'skewness': None, # Returns the sample skewness of x (calculated with the adjusted Fisher-Pearson standardized moment coefficient G1).\n                 'kurtosis': None, # Returns the kurtosis of x (calculated with the adjusted Fisher-Pearson standardized moment coefficient G2).\n                 'maximum': None, # Calculates the highest value of the time series x.\n                 'minimum': None, # Calculates the lowest value of the time series x.\n                 'sample_entropy':None, # Calculate and return sample entropy of x.\n                 'mean_abs_change':None, # Average over first differences.\n                 'sum_values':None, # Calculates the sum over the time series values\n                 'distance':None # Calculates the distance traveled in an axis\n                }","511e14d6":"feature_calculators.__dict__[\"distance\"] = distance","d207e282":"X = extract_features(data_rows_balanced, \n                     column_id=\"id\", \n                     column_sort=\"time\", \n                     default_fc_parameters=fc_parameters, \n                     impute_function=impute)","c0f777a2":"X.head()","4fe152f7":"X_train, X_test, y_train, y_test = train_test_split(X, target_balanced, test_size=0.2, random_state=1)","c396e7bf":"X_train = X_train.sort_index()\nX_train.head()","430886fa":"y_train = y_train.sort_index()\n\ny_train.head()","0eade188":"features_filtered_direct = select_features(X_train, \n                                           y_train,\n                                           ml_task=\"classification\", # Classification o regression, by default set to auto\n                                           fdr_level = 0 # Respected percentage of features to be discarded as irrelevant, by default set to 0.05\n                                          )","994bdc64":"features_filtered_direct.head()","6fc14faa":"DTC = DecisionTreeClassifier()\nDTC.fit(features_filtered_direct, y_train)","1cbbf4bf":"KNN = KNeighborsClassifier(n_neighbors=5)\n\nKNN.fit(features_filtered_direct,y_train)","207e1fe2":"print(classification_report(y_test, DTC.predict(X_test[features_filtered_direct.columns])))","5c45cc24":"print(classification_report(y_test, KNN.predict(X_test[features_filtered_direct.columns])))","548259f8":"perm = PermutationImportance(DTC, random_state=1).fit(X_test[features_filtered_direct.columns], y_test)\neli5.show_weights(perm, feature_names = X_test[features_filtered_direct.columns].columns.tolist())","068687bb":"perm = PermutationImportance(KNN, random_state=1).fit(X_test[features_filtered_direct.columns], y_test)\neli5.show_weights(perm, feature_names = X_test[features_filtered_direct.columns].columns.tolist())","bd271e47":"To create our time variable, we simply streap the x char from each string to end up with the numbering of the observations.","21e1c52a":"And we start building our final DataFrame.","0a136173":"We end up with a row of features for each timeseries.","b4aba757":"We continue preparing our target data by indexing the DataFrame with the id column.","1eae3759":"# Model Training","79bb0e6d":"And we test the models using our test set.","011306e1":"We train a Decision Tree and a Random Forest Classifier.","7b33dba0":"We have a dataset containing 23309 timeseries with 200 obervations of 12 variables.","825e32b8":"We confirm we know have a semi-balanced data set.","1cea5f81":"Now we select the relevant features only using our training data. Feature significance is determined in tsfresh by making univariate feature significance tests. In a binary classification problem, the feature significance with respect to the target value is calculated using either the Fisher exact test, if the feature is binary, or the Mann-Whitney U test, if the feature is real valued. In both cases, lower p-values indicate a higher feature significance. The generated p-values are then evaluated by the Banjamini Hochberg procedure, which determines relevant features to be kept and the non-relevant features to be deleted. The tests being <br\/>\n$H_0 = $ Target variable and feature are independent (Feature should be deleted) <\/br>\n$H_1 = $ Target variable and feature are associated (Feature should be kept)","e5be7ae8":"Now, that we have an identifier, we create a second DataFrame to store our target variable, which is the Swarm_Behaviour column.","8c7fbe84":"We create two lists, one containing the ids of the not flock observations and another of the flock observations. Then we undersample our data by choosing around the same amount of not flock observations as we have of flock observations.","a90b8042":"So we can split our data into our training and test sets using the train_test_split function.","25853ce2":"And we create said DataFrame.","9d4850ee":"We generalize these steps below to end up with the transofmration we previously stated.","d801cae0":"Now we can extract the features.","50ac9fd2":"Given that our variables data is stored in 200 columns for each variable, we need to transform our data into a dataset containing 12 columns, one for each variable, and the corresponding rows of observations in order to use tsfresh. \n\nThere are several ways we can provide the data in tsfresh. Here we make use of pandas DataFrame containing our variables in columns and the different observations in rows, with an extra column to specify the ordering of the data and another to specify to which time-series the set of observations in a certain row belongs to. Meaning we need to transform our dataset so that instead of having one complete time-series per row, we end up having a single set of observations (one for each of the 12 variables) in each row belonging to a time-series and a time-point.\n\nFor this, we first create a unique id column using the index of each row.","0bd4d4f4":"## Relevant Features","ace69ee1":"Finally, we have to transform our target DataFrame into a Series to use it as input in tsfresh.","0fb05ed2":"And we add the new feature to the tsfresh feature calculators dictionary.","c28bd16f":"We can see that we can now select only the columns containing observations of the x variable of all of our time series.","ce3d987d":"Now we focus on our target data. We see we have an unbalanced data set.","e99a87c2":"# Feature Extraction\n\nWe will make use of tsfresh to automatize the feature extraction and selection process. For this, we define a set of features to calculate. Tsfresh has a preset of predefined features, so one can simply call extract_features with the dataset, specifying the name of the id and time columns. Nonetheless, we can also provide a dictionary of features to calculate, with the key being the string of feature calculator and the value being the parameters of the feature calculator. Thus, with a little intuition of the interesting features that my encapsulate important information of the time series, we can save computation time. Otherwise, we can calculate a subset of features provided by the tsfresh.feature_extraction.settings.MinimalFCParameters() dictionary, which contains features that do not require input parameters, or the tsfresh.feature_extraction.settings.EfficientFCParameters() dictionary, which contains features that are not tagged with the \"high_comp_cost\" and benefits runtime performance.","a92f969f":"# Feature Generation and Selection\n\nFeature generation is the process of finding a quantitative way of encapsulating the most important traits of time series data into few numeric values and categorical labels. The purpose of feature generation is to compress as much  information as possible about the time series into as few metrics as possible, or to use those metrics to identify the most relevant information and discard the rest. Once useful features have been generated, we must verify that they are indeed useful.\n\nGenerally, feature generation is used to apply machine learning techniques to time series data, by generating a set of relevant features to use as input in a machine learning technique.\n\n## A Time Series Classification Problem\n\nIn this notebook we make use of the tsfresh library to classify a set of time series with a binary label. Tsfresh is a python library that enables the automatization of the extraction of relevant features from time series data.\n\nWe will be working with flocking behaviour data of groups of birds, insects, fish, and other animals. From the data context, we know that flocking behaviour refers to the way that groups of animals, move close to each other. They are able to move as a group with the same velocity, yet without running into each other.\n\nThe attributes are $x_m$, $y_m$ as the $(x,y)$ position of each boid, $x_{vel_m}$, $y_{vel_m}$ as the velocity vector, $x_{A_m}$, $y_{A_m}$ as the alignment vector, $x_{S_m}$, $y_{S_m}$ as the separation vector, $x_{C_m}$, $y_{C_m}$ as the cohesion vector, $n_{AC_m}$ as the number of boids the radius of Alignment\/Cohesion, and nSm as the number of boids in the radius of Separation. These attributes are repeated for all m boids, where $m=1,2,3,\\dots,200$. Also, class labels are binary, which 1 refers to flocking, grouped, and aligned, and 0 refers to not flocking, not grouped, and not aligned.\n\nSo we will classify the time series data into two groups, 1 as flocking, grouped, and aligned, or 0 as not flocking, not grouped, and not aligned.\n\n","0b562c9b":"To transform our columns of observations to rows of observations, we need to parse every column of each variable. We start with the x variable by creating an array containing the strings from x1 to x200 (all the columns of observations for the x variable).","9287c44a":"To continue the transformation of the data, we create a list containing the final columns we want in our new DataFrame.","45ebcae0":"Also, we can define a new feature calculator to create a feature regarding distance traveled as follows.","2bbe3950":"We melt our data in such a way that we end up with each observation in a different row with a column containing the string of the observation x_i and the id to identify to which instance it corresponds to.","923f95c3":"# Feature Importance\n\nLastly, we will determine the features with the highest importance. By permutating the set of features one by one, we will understand the importance the model gave to each feature to predict the target variable and validate if it a reasonable approach."}}