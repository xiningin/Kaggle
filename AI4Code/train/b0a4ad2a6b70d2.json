{"cell_type":{"7311e6bb":"code","38499d84":"code","297a5f31":"code","24274be2":"code","cdc7b200":"code","d07ff6c6":"code","57b2b99e":"code","1cd87077":"code","0761d763":"code","ed85489b":"code","4e808a15":"code","0049ab1b":"code","bb14a216":"code","3c5b0702":"code","9ae8ffcf":"code","1d8e7de2":"code","435e01c5":"code","e824a0bf":"markdown","7645c072":"markdown","54391716":"markdown","9910c5af":"markdown","0b024fc3":"markdown","4dcfab4a":"markdown"},"source":{"7311e6bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\nfrom keras import backend as K\nfrom keras.preprocessing.image import load_img, save_img, img_to_array\nfrom keras.applications import vgg19\nfrom keras.models import Model\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport time\nfrom scipy.optimize import fmin_l_bfgs_b\n","38499d84":"style_path = \"..\/input\/best-artworks-of-all-time\/images\/images\/\"\ncontent_path = \"..\/input\/image-classification\/validation\/validation\/travel and adventure\/\"","297a5f31":"content_image_name = \"13.jpg\"\nbase_image_path = content_path + content_image_name\nstyle_image_name = \"Pablo_Picasso\/Pablo_Picasso_92.jpg\"\n# style_image_name = \"Vincent_van_Gogh\/Vincent_van_Gogh_875.jpg\"\nstyle_image_path = style_path + style_image_name","24274be2":"plt.figure()\nplt.title(\"Base Image\", fontsize=20)\n# print(base_image_path)\nimg_base = load_img(base_image_path)\nplt.imshow(img_base)\n\nplt.figure()\nplt.title(\"Style Image\", fontsize=20)\n# print(style_image_path)\nimg_style = load_img(style_image_path)\nplt.imshow(img_style)\n\nwidth, height = load_img(base_image_path).size\nimg_nrows = 400\nimg_ncols = int(width * img_nrows \/ height)","cdc7b200":"def preprocess_img(image_path):\n    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = vgg19.preprocess_input(img)\n    return img","d07ff6c6":"# create base & style image tf variable\nbase_image = K.variable(preprocess_img(base_image_path))\nstyle_image = K.variable(preprocess_img(style_image_path))\nK.image_data_format()\n\n# a placeholder to contain generated image\nif K.image_data_format() == 'channels_first':\n    combination_image = K.placeholder((1,3,img_nrows, img_ncols))\nelse:\n    combination_image = K.placeholder((1, img_nrows, img_ncols, 3))\n\n# combine the 3 images into a single Keras tensor which is suitable for processing by vgg19 model\ninput_tensor = K.concatenate([base_image, style_image, combination_image], axis=0)","57b2b99e":"# building VGG19 model\nvgg19_weights = \"..\/input\/vgg19\/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\"\nmodel = vgg19.VGG19(input_tensor=input_tensor, include_top=False, weights=vgg19_weights)\nprint(\"Model loaded.\")","1cd87077":"layers = dict([(layer.name, layer.output) for layer in model.layers])\nlayers","0761d763":"# the relative importance of content loss, style loss and total variation\ncontent_weight = 0.025\nstyle_weight = 1.0\ntotal_variation_weight = 1.0","ed85489b":"# compute content loss\ndef get_content_loss(base_content, combination):\n    return K.sum(K.square(combination - base_content))\n","4e808a15":"# compute style loss\ndef gram_matrix(input_tensor):\n    features = K.batch_flatten(K.permute_dimensions(input_tensor, (2, 0, 1)))\n    gram = K.dot(features, K.transpose(features))\n    return gram\n\n    \ndef get_style_loss(style, combination):\n    style_gram = gram_matrix(style)\n    combine_gram = gram_matrix(combination)\n    channels = 3\n    size = img_nrows * img_ncols\n    return K.sum(K.square(style_gram - combine_gram)) \/ (4. * (channels ** 2) * (size ** 2))","0049ab1b":"# compute total variation loss:\ndef total_variation_loss(x):\n    a = K.square(x[:, :img_nrows-1, :img_ncols-1, :] - x[:, 1:, :img_ncols-1, :])\n    b = K.square(x[:, :img_nrows-1, :img_ncols-1, :] - x[:, :img_nrows-1, 1:, :])\n    return K.sum(K.pow(a + b, 1.25))","bb14a216":"# initialise the total loss to 0 and adding to it in stages\nloss = K.variable(0.)\n\ncontent_features = layers['block4_conv2']\ncontent_image_features = content_features[0, :, :, :]\ncontent_combination_features = content_features[2, :, :, :]\nloss += content_weight * get_content_loss(content_image_features, content_combination_features)\n\nstyle_layers = ['block1_conv1', \n                'block2_conv1',\n                'block3_conv1',\n                'block4_conv1',\n                'block5_conv1']\nfor layer_name in style_layers:\n    style_features = layers[layer_name]\n    style_image_features = style_features[1, :, :, :]\n    style_combination_features = style_features[2, :, :, :]\n    style_loss = get_style_loss(style_image_features, style_combination_features)\n    loss += (style_weight \/ len(style_layers)) * style_loss\n    \nloss += total_variation_weight * total_variation_loss(combination_image)","3c5b0702":"# define gradients\ngrads = K.gradients(loss, combination_image)\n\noutputs = [loss]\noutputs += grads\nf_outputs = K.function([combination_image], outputs)\n\ndef eval_loss_and_grads(x):\n    x = x.reshape((1, img_nrows, img_ncols, 3))\n    outs = f_outputs([x])\n    loss_value = outs[0]\n    grad_values = outs[1].flatten().astype('float64')\n    return loss_value, grad_values\n\nclass Evaluator(object):\n    \n    def __init__(self):\n        self.loss_value = None\n        self.grad_values = None\n    \n    def loss(self, x):\n        assert self.loss_value is None\n        loss_value, grad_values = eval_loss_and_grads(x)\n        self.loss_value = loss_value\n        self.grad_values = grad_values\n        return self.loss_value\n    \n    def grads(self, x):\n        assert self.loss_value is not None\n        grad_values = np.copy(self.grad_values)\n        self.loss_value = None\n        self.grad_values = None\n        return grad_values\n\nevaluator = Evaluator()","9ae8ffcf":"# 10 iterations of L-BFGS\n# start with random collections of pixels\nx = np.random.uniform(0, 255, (1, img_nrows, img_ncols, 3)) - 128\n\niterations = 20\n\nfor i in range(iterations):\n    print('Start iteration', i)\n    start_time = time.time()\n    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n                                     fprime=evaluator.grads, maxfun=20)\n    print('Current loss value:', min_val)\n    end_time = time.time()\n    print('Iteration %d completed in %ds' % (i, end_time-start_time))\n    ","1d8e7de2":"# get result image\nx = x.reshape((img_nrows, img_ncols, 3))\nx = x[:, :, ::-1]\nx[:, :, 0] += 103.939\nx[:, :, 1] += 116.779\nx[:, :, 2] += 123.68\nimg_result = np.clip(x, 0, 255).astype('uint8')","435e01c5":"# show content image\nplt.figure()\nplt.title(\"Base Image\", fontsize=20)\n# print(base_image_path)\nimg_base = load_img(base_image_path)\nplt.imshow(img_base)\n\n# show style image\nplt.figure()\nplt.title(\"Style Image\", fontsize=20)\n# print(style_image_path)\nimg_style = load_img(style_image_path)\nplt.imshow(img_style)\n\n# show results\nplt.figure()\nplt.title(\"Combined Image\", fontsize=20)\nplt.imshow(img_result)","e824a0bf":"# Neural style transfer","7645c072":"## Preprocessing Image\n* we convert these images into a form suitable for numercial processing by adding another dimension (beyond the classic height x width x 3 dimensions) so that we can later concatenate the representations of these two images into a common data structure.\n* we need to match imput data into vgg network model","54391716":"## Load Images","9910c5af":"## Define Gradients and Solve the Optimisation Problem\n* Define gradients of the total loss relative to the combination image, and use these gradients to iteratively improve upon our combination image to minise the loss.\n* Evaluator: computes loss and gradients in one pass while retrieving them via two separate functions, loss and grads. Because scipy.optimize requires separate functions for loss and gradients, but computing them separately would be inefficient.\n* Use L-BFGS algorithm (a quasi-Newton algorithm that's significantly quicker to converge than standard gradient descent) to iteratively imporve upon it.","0b024fc3":"## The Pre-trained VGG Model\n* The idea introduced by the paper is that convolutional neural networks (CNNs) pre-trained for image classification already know how to encode perceptual and semantic information about images.\n* Since we are not interested in the classification problem, we do not need the fully connected layers or the final softmax classifier.\n* Setting include_top=False in the code below means that we do not include any of the fully connected layers.","4dcfab4a":"## The Optimisation Problem\n* The loss function we want to minimise can be decomposed into three distinct parts: the content loss, the style loss, and the total variation loss.\n## The content loss\n* Draw the content features from block4_conv2\n* Content loss is defined as the Mean Squared Error between the feature map F of out content image C and the feature map P of our generated image Y, in other words, (scaled, squared) Eucildean distance between feature representations of the content and combination images.\n## The style loss\n* Define Gram matrix: The terms of this matrix are proportional to the covariances of corresponding sets of features, and thus captures information about which features tend to activate together. By only capturing these aggregate statistics across the image, they are blind to the specific arrangement of objects inside the image. This is what allows them to capture information about style independent of content.\n* Compute Gram matrix: The Gram matrix can be computed efficiently by reshaping the feature spaces suitably and taking an outer product.\n* The style loss: is (scaled, squared) Frobenius norm of the difference between the Gram matices of the style and combination images.\n## The total variation loss\n* If you were to solve the optimisation problem with only the two loss terms we've introduced so far (style and content), you'll find that the output is quite noisy. We thus add another term, called the total variation loss (a regularisation term) that encourages spatial smoothness."}}