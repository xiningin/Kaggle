{"cell_type":{"d4d063a3":"code","35711981":"code","6cbde38e":"code","4183f87a":"code","5382b0b0":"code","a5881ef8":"code","f187deac":"code","7bec0beb":"code","2fd98a1b":"code","e2076753":"code","d81da508":"code","dd7ee5fd":"code","f68e747b":"code","12069d4f":"code","56151ca1":"code","865b2943":"code","f0ab84f0":"code","c4c41d30":"code","d5725824":"code","5c0249ad":"markdown","9e707d58":"markdown","1f49ec99":"markdown","01786ff2":"markdown","4e2acf05":"markdown"},"source":{"d4d063a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport cv2\nfrom tqdm import tqdm\nimport multiprocessing as mp\ntqdm.pandas(desc=\"my bar!\")\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","35711981":"train_dir = os.listdir('..\/input\/omniglot\/images_background\/')\ndatax = np.array([])","6cbde38e":"def image_rotate(img, angle):\n    \"\"\"\n    Image rotation at certain angle. It is used for data augmentation \n    \"\"\"\n    rows,cols, _ = img.shape\n    M = cv2.getRotationMatrix2D((cols\/2 ,rows\/2),angle,1)\n    dst = cv2.warpAffine(img,M,(cols,rows))\n    return np.expand_dims(dst, 0)\n\ndef read_alphabets(alphabet_directory, directory):\n    \"\"\"\n    Reads all the characters from alphabet_directory and augment each image with 90, 180, 270 degrees of rotation.\n    \"\"\"\n    datax = None\n    datay = []\n    characters = os.listdir(alphabet_directory)\n    for character in characters:\n        images = os.listdir(alphabet_directory + character + '\/')\n        for img in images:\n            image = cv2.resize(cv2.imread(alphabet_directory + character + '\/' + img), (28,28))\n            image90 = image_rotate(image, 90)\n            image180 = image_rotate(image, 180)\n            image270 = image_rotate(image, 270)\n            image = np.expand_dims(image, 0)\n            if datax is None:\n                datax = np.vstack([image, image90, image180, image270])\n            else:\n                datax = np.vstack([datax, image, image90, image180, image270])\n            datay.append(directory + '_' + character + '_0')\n            datay.append(directory + '_' + character + '_90')\n            datay.append(directory + '_' + character + '_180')\n            datay.append(directory + '_' + character + '_270')\n    return datax, np.array(datay)\n\ndef read_images(base_directory):\n    \"\"\"\n    Used multithreading for data reading to decrease the reading time drastically\n    \"\"\"\n    datax = None\n    datay = []\n    pool = mp.Pool(mp.cpu_count())\n    results = [pool.apply(read_alphabets, args=(base_directory + '\/' + directory + '\/', directory, )) for directory in os.listdir(base_directory)]\n    pool.close()\n    for result in results:\n        if datax is None:\n            datax = result[0]\n            datay = result[1]\n        else:\n            datax = np.vstack([datax, result[0]])\n            datay = np.concatenate([datay, result[1]])\n    return datax, datay","4183f87a":"%time trainx, trainy = read_images('..\/input\/omniglot\/images_background\/')","5382b0b0":"%time testx, testy = read_images('..\/input\/omniglot\/images_evaluation\/')","a5881ef8":"trainx.shape, trainy.shape, testx.shape, testy.shape","f187deac":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom tqdm import trange\nfrom time import sleep\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nuse_gpu = torch.cuda.is_available()","7bec0beb":"trainx = torch.from_numpy(trainx).float()\n#trainy = torch.from_numpy(trainy)\ntestx = torch.from_numpy(testx).float()\n#testy = torch.from_numpy(testy)\nif use_gpu:\n    trainx = trainx.cuda()\n    testx = testx.cuda()\ntrainx.size(), testx.size()","2fd98a1b":"trainx = trainx.permute(0,3,1,2)\ntestx = testx.permute(0,3,1,2)","e2076753":"class Net(nn.Module):\n    \"\"\"\n    Image2Vector CNN which takes image of dimension (28x28x3) and return column vector length 64\n    \"\"\"\n    def sub_block(self, in_channels, out_channels=64, kernel_size=3):\n        block = torch.nn.Sequential(\n                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=in_channels, out_channels=out_channels, padding=1),\n                    torch.nn.BatchNorm2d(out_channels),\n                    torch.nn.ReLU(),\n                    torch.nn.MaxPool2d(kernel_size=2)\n                )\n        return block\n    \n    def __init__(self):\n        super(Net, self).__init__()\n        self.convnet1 = self.sub_block(3)\n        self.convnet2 = self.sub_block(64)\n        self.convnet3 = self.sub_block(64)\n        self.convnet4 = self.sub_block(64)\n\n    def forward(self, x):\n        x = self.convnet1(x)\n        x = self.convnet2(x)\n        x = self.convnet3(x)\n        x = self.convnet4(x)\n        x = torch.flatten(x, start_dim=1)\n        return x","d81da508":"class PrototypicalNet(nn.Module):\n    def __init__(self, use_gpu=False):\n        super(PrototypicalNet, self).__init__()\n        self.f = Net()\n        self.gpu = use_gpu\n        if self.gpu:\n            self.f = self.f.cuda()\n    \n    def forward(self, datax, datay, Ns,Nc, Nq, total_classes):\n        \"\"\"\n        Implementation of one episode in Prototypical Net\n        datax: Training images\n        datay: Corresponding labels of datax\n        Nc: Number  of classes per episode\n        Ns: Number of support data per class\n        Nq:  Number of query data per class\n        total_classes: Total classes in training set\n        \"\"\"\n        k = total_classes.shape[0]\n        K = np.random.choice(total_classes, Nc, replace=False)\n        Query_x = torch.Tensor()\n        if(self.gpu):\n            Query_x = Query_x.cuda()\n        Query_y = []\n        Query_y_count = []\n        centroid_per_class  = {}\n        class_label = {}\n        label_encoding = 0\n        for cls in K:\n            S_cls, Q_cls = self.random_sample_cls(datax, datay, Ns, Nq, cls)\n            centroid_per_class[cls] = self.get_centroid(S_cls, Ns)\n            class_label[cls] = label_encoding\n            label_encoding += 1\n            Query_x = torch.cat((Query_x, Q_cls), 0) # Joining all the query set together\n            Query_y += [cls]\n            Query_y_count += [Q_cls.shape[0]]\n        Query_y, Query_y_labels = self.get_query_y(Query_y, Query_y_count, class_label)\n        Query_x = self.get_query_x(Query_x, centroid_per_class, Query_y_labels)\n        return Query_x, Query_y\n    \n    def random_sample_cls(self, datax, datay, Ns, Nq, cls):\n        \"\"\"\n        Randomly samples Ns examples as support set and Nq as Query set\n        \"\"\"\n        data = datax[(datay == cls).nonzero()]\n        perm = torch.randperm(data.shape[0])\n        idx = perm[:Ns]\n        S_cls = data[idx]\n        idx = perm[Ns : Ns+Nq]\n        Q_cls = data[idx]\n        if self.gpu:\n            S_cls = S_cls.cuda()\n            Q_cls = Q_cls.cuda()\n        return S_cls, Q_cls\n    \n    def get_centroid(self, S_cls, Nc):\n        \"\"\"\n        Returns a centroid vector of support set for a class\n        \"\"\"\n        return torch.sum(self.f(S_cls), 0).unsqueeze(1).transpose(0,1) \/ Nc\n    \n    def get_query_y(self, Qy, Qyc, class_label):\n        \"\"\"\n        Returns labeled representation of classes of Query set and a list of labels.\n        \"\"\"\n        labels = []\n        m = len(Qy)\n        for i in range(m):\n            labels += [Qy[i]] * Qyc[i]\n        labels = np.array(labels).reshape(len(labels), 1)\n        label_encoder = LabelEncoder()\n        Query_y = torch.Tensor(label_encoder.fit_transform(labels).astype(int)).long()\n        if self.gpu:\n            Query_y = Query_y.cuda()\n        Query_y_labels = np.unique(labels)\n        return Query_y, Query_y_labels\n    \n    def get_centroid_matrix(self, centroid_per_class, Query_y_labels):\n        \"\"\"\n        Returns the centroid matrix where each column is a centroid of a class.\n        \"\"\"\n        centroid_matrix = torch.Tensor()\n        if(self.gpu):\n            centroid_matrix = centroid_matrix.cuda()\n        for label in Query_y_labels:\n            centroid_matrix = torch.cat((centroid_matrix, centroid_per_class[label]))\n        if self.gpu:\n            centroid_matrix = centroid_matrix.cuda()\n        return centroid_matrix\n    \n    def get_query_x(self, Query_x, centroid_per_class, Query_y_labels):\n        \"\"\"\n        Returns distance matrix from each Query image to each centroid.\n        \"\"\"\n        centroid_matrix = self.get_centroid_matrix(centroid_per_class, Query_y_labels)\n        Query_x = self.f(Query_x)\n        m = Query_x.size(0)\n        n = centroid_matrix.size(0)\n        # The below expressions expand both the matrices such that they become compatible to each other in order to caclulate L2 distance.\n        centroid_matrix = centroid_matrix.expand(m, centroid_matrix.size(0), centroid_matrix.size(1)) # Expanding centroid matrix to \"m\".\n        Query_matrix = Query_x.expand(n, Query_x.size(0), Query_x.size(1)).transpose(0,1) # Expanding Query matrix \"n\" times\n        Qx = torch.pairwise_distance(centroid_matrix.transpose(1,2), Query_matrix.transpose(1,2))\n        return Qx","dd7ee5fd":"protonet = PrototypicalNet(use_gpu=use_gpu)\noptimizer = optim.SGD(protonet.parameters(), lr = 0.01, momentum=0.99)","f68e747b":"def train_step(datax, datay, Ns,Nc, Nq):\n    optimizer.zero_grad()\n    Qx, Qy= protonet(datax, datay, Ns, Nc, Nq, np.unique(datay))\n    pred = torch.log_softmax(Qx, dim=-1)\n    loss = F.nll_loss(pred, Qy)\n    loss.backward()\n    optimizer.step()\n    acc = torch.mean((torch.argmax(pred, 1) == Qy).float())\n    return loss, acc","12069d4f":"num_episode = 20000\nframe_size = 1000","56151ca1":"frame_loss = 0\nframe_acc = 0\nfor i in range(num_episode):\n    loss, acc = train_step(trainx, trainy, 5, 60, 5)\n    frame_loss += loss.data\n    frame_acc += acc.data\n    if( (i+1) % frame_size == 0):\n        print(\"Frame Number:\", ((i+1) \/\/ frame_size), 'Frame Loss: ', frame_loss.data.cpu().numpy().tolist()\/ frame_size, 'Frame Accuracy:', (frame_acc.data.cpu().numpy().tolist() * 100) \/ frame_size)\n        frame_loss = 0\n        frame_acc = 0","865b2943":"def test_step(datax, datay, Ns,Nc, Nq):\n    Qx, Qy= protonet(datax, datay, Ns, Nc, Nq, np.unique(datay))\n    pred = torch.log_softmax(Qx, dim=-1)\n    loss = F.nll_loss(pred, Qy)\n    acc = torch.mean((torch.argmax(pred, 1) == Qy).float())\n    return loss, acc","f0ab84f0":"num_test_episode = 2000","c4c41d30":"avg_loss = 0\navg_acc = 0\nfor _ in range(num_test_episode):\n    loss, acc = test_step(testx, testy, 5, 60, 15)\n    avg_loss += loss.data\n    avg_acc += acc.data\nprint('Avg Loss: ', avg_loss.data.cpu().numpy().tolist() \/ num_test_episode , 'Avg Accuracy:', (avg_acc.data.cpu().numpy().tolist() * 100) \/ num_test_episode)","d5725824":"torch.save(protonet.state_dict(), 'protonet.pt')","5c0249ad":"## Model","9e707d58":"# Few-Shot Learning With Prototypical Networks\n\n> Artificial Intelligence is the new electricity - Andrew NG\n\nThe change occurred in our life after the expeditious growth in AI and deep learning, in particular, is a solid example of this. The research is booming at unprecedented speed and lives of thousands of people have been improved. If AI is the new electricity then definitely data is the new coal. But recently we\u2019ve seen hazardous depletion in the amount of coal in our environment. This resulted in the development of new technologies which needed a fraction of coal or even no coal at all. Similarly, there are many applications in artificial intelligence where we only have meager data or even no data at all. Hence, we need new techniques to perform well in them. Such problems resulted in the growth of a very popular field, the field of N-shot learning.\n\nTo know about it more, go to my blog at [N-Shot Learning: Learning More with Less Data](https:\/\/blog.floydhub.com\/n-shot-learning\/). If you want to run it fork this kernel and or \n\n<a href=\"https:\/\/www.floydhub.com\/run?template=https:\/\/github.com\/Hsankesara\/Prototypical-Networks\">\n    <img src=\"https:\/\/static.floydhub.com\/button\/button-small.svg\" alt=\"Run\">\n<\/a>\n\nTo use it for your own, go to my repository at [GitHub](https:\/\/github.com\/Hsankesara\/DeepResearch)\n\n<table>\n    <tr>\n        <td><img src=\"https:\/\/github.com\/Hsankesara\/DeepResearch\/raw\/master\/Prototypical_Nets\/img1.png\" alt=\"Basic Idea behind prototypical Network\"><\/td>\n        <td><img src=\"https:\/\/github.com\/Hsankesara\/DeepResearch\/raw\/master\/Prototypical_Nets\/img2.png\" alt=\"How prototypical Network works\"><\/td>\n    <\/tr>\n<\/table>\n","1f49ec99":"## Training","01786ff2":"## Testing","4e2acf05":"## Data Reading and Augmentation\nThe Omniglot data set is designed for developing more human-like learning algorithms. It contains 1623 different handwritten characters from 50 different alphabets. Then to increase the number of classes, all the images are rotated by 90, 180 and 270 degrees and each rotation resulted in one more class. Hence the total count of classes reached to 6492(1623 * 4) classes. We split images of 4200 classes to training data and the rest went to test set."}}