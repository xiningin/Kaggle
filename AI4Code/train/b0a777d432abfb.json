{"cell_type":{"5183bc05":"code","e392ed74":"code","49a3def5":"code","5c76c563":"code","bb58b3f7":"code","b05d5c70":"code","a004c3af":"code","2e0bc96f":"code","1253d44c":"code","c151d676":"code","b2726a82":"code","2b49c302":"code","f2041fc1":"code","d540bae4":"code","9763828d":"code","262524dc":"code","1626b053":"code","f17b9f28":"code","a7664ec9":"markdown"},"source":{"5183bc05":"!pip install --quiet ipyplot\n!pip install --quiet ftfy regex\n!pip install --quiet git+https:\/\/github.com\/openai\/CLIP.git","e392ed74":"from pathlib import Path\n\nimport cv2\nimport ipyplot\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport torch\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\n\nimport clip","49a3def5":"DATA_DIR = Path(\"\/kaggle\/input\/where-is-the-toilet-the-right-one\/\")","5c76c563":"test = pd.read_csv(DATA_DIR \/ \"test.csv\")\ntest.shape","bb58b3f7":"test.head()","b05d5c70":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B\/32\", device=DEVICE, jit=False)","a004c3af":"def infer(img_paths):\n    embeddings = []\n    for path in tqdm(img_paths):\n        img_path = DATA_DIR \/ path\n        img = Image.open(img_path)\n        x = preprocess(img).unsqueeze(0).to(DEVICE)\n\n        with torch.no_grad():\n            emb = model.visual.forward(x)\n\n        embeddings.append(emb)\n    \n    embeddings = torch.cat(embeddings)\n    embeddings = F.normalize(embeddings, p=2, dim=1)\n    \n    return embeddings","2e0bc96f":"embeddings = infer(test[\"path\"])","1253d44c":"embeddings.shape","c151d676":"distances = torch.mm(embeddings, embeddings.t())\ndistances = (distances + 1.) \/ 2.\ndistances.shape","b2726a82":"query_idxs = test[test[\"is_query\"] == 1].index\nnon_query_idxs = test[test[\"is_query\"] == 0].index\ndistances = distances[query_idxs, :]\ndistances = distances[:, non_query_idxs]","2b49c302":"distances.shape","f2041fc1":"topk_dists, topk_idxs = torch.topk(distances, 11, dim=1)","d540bae4":"for i in range(0, 5):\n    query_idx = query_idxs[i].item()\n    query_path = DATA_DIR \/ test.loc[query_idx][\"path\"]\n    query_img = cv2.imread(str(query_path))[..., ::-1]\n    rel_paths = list(map(lambda idx: DATA_DIR \/ test.loc[idx.item()][\"path\"], non_query_idxs[topk_idxs[i]]))\n    rel_images = list(map(lambda p: cv2.imread(str(p))[..., ::-1], rel_paths))\n\n    ipyplot.plot_images([query_img])\n    ipyplot.plot_images(rel_images)","9763828d":"# Take second best to skip the same gender\nmatches_dists, matches_idxs = topk_dists[:, 1], topk_idxs[:, 1]\nmatches_dists = matches_dists.cpu().numpy()\nmatches_idxs = matches_idxs.cpu().numpy()\nmatches_idxs = non_query_idxs[matches_idxs]","262524dc":"submission = {\n    \"QueryId\": [],\n    \"Iteration\": [],\n    \"DocumentNumber\": [],\n    \"Rank\": [],\n    \"Similarity\": [],\n    \"RunId\": []\n}\n\nfor q_idx, m_idx, m_dist in zip(query_idxs, matches_idxs, matches_dists):\n    submission[\"QueryId\"].append(q_idx)\n    submission[\"Iteration\"].append('')\n    submission[\"DocumentNumber\"].append(m_idx)\n    submission[\"Rank\"].append(0)\n    submission[\"Similarity\"].append(m_dist)\n    submission[\"RunId\"].append('')\n    \nsubmission = pd.DataFrame(submission)","1626b053":"submission.head()","f17b9f28":"submission.to_csv(\"submission.csv\", index=False)","a7664ec9":"## CLIP"}}