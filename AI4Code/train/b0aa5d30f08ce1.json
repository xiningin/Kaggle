{"cell_type":{"67299d99":"code","ea121e2b":"code","358e1d6e":"markdown","f3704b3d":"markdown","be3b996b":"markdown","835910ba":"markdown","9cde497e":"markdown","09c77f12":"markdown","db106510":"markdown"},"source":{"67299d99":"!pip install --upgrade transformers simpletransformers\nimport pandas as pd, torch, warnings; warnings.simplefilter('ignore'); from simpletransformers.classification import ClassificationModel\ntrain_data = pd.read_csv('..\/input\/nlp-with-disaster-tweets-cleaning-data\/train_data_cleaning.csv')[['text', 'target']]\ntest_data = pd.read_csv('..\/input\/nlp-with-disaster-tweets-cleaning-data\/test_data_cleaning.csv')[['id','text']]\nmodel = ClassificationModel('distilbert', 'distilbert-base-uncased', args={'fp16': False,'train_batch_size': 4, 'gradient_accumulation_steps': 2,\n        'learning_rate': 1e-05, 'do_lower_case': True, 'overwrite_output_dir': True, 'manual_seed': 100, 'num_train_epochs': 2}, weight = [0.44, 0.56])\nmodel.train_model(train_data)\ntest_data[\"target\"], _ = model.predict(test_data['text'])\ntest_data.drop(columns=['text']).to_csv(\"submission.csv\", index=False)","ea121e2b":"# In addition - accuracy evaluation\nimport sklearn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Accuracy\nresult, model_outputs, wrong_predictions = model.eval_model(train_data, acc=sklearn.metrics.accuracy_score)\nprint('Accuracy = ',round(result['acc'],2),'%', sep = \"\")\n\n# Showing Confusion Matrix\ndef plot_cm(y_true, y_pred, title, figsize=(5,5)):\n    # From https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n    \npredictions, _ = model.predict(train_data['text'])\nplot_cm(predictions, train_data['target'], 'Confusion matrix for model', figsize=(7,7))\n\n# Classification report\nreport = classification_report(train_data['target'],predictions)\nprint('Classification report:',report)","358e1d6e":"## This notebook is based on:\n* my notebook [Supershort NLP classification notebook](https:\/\/www.kaggle.com\/vbmokin\/supershort-nlp-classification-notebook) - source of code\n* my notebook [NLP with DT: Simple Transformers Research](https:\/\/www.kaggle.com\/vbmokin\/nlp-with-dt-simple-transformers-research) - source of model parameters\n* my notebook [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert) - source of the function for the confusion_matrix visualization\n* my dataset [NLP with Disaster Tweets - cleaning data](https:\/\/www.kaggle.com\/vbmokin\/nlp-with-disaster-tweets-cleaning-data) - source of data\n* library [simpletransformers](https:\/\/github.com\/ThilinaRajapakse\/simpletransformers)\n* library [transformers](https:\/\/huggingface.co\/transformers)\n","f3704b3d":"## The main section","be3b996b":"I hope you find this kernel useful and enjoyable.","835910ba":"# Supershort NLP classification notebook for prize competition \"[Real or Not? NLP with Disaster Tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started)\"","9cde497e":"## Additional (optional) section","09c77f12":"Your comments and feedback are most welcome.","db106510":"### The notebook is universal and can be used in other Kaggle competitions or for forecasting data from one dataset after a little adaptation to another data structure."}}