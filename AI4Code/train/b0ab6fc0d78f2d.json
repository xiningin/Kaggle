{"cell_type":{"67c3531d":"code","e5e1413a":"code","2d7767bc":"code","8e5ab5d3":"code","c7edc407":"code","de3b3f43":"code","7a4bc599":"code","0037b3fe":"code","976941f0":"code","ea4a9855":"code","6a8bdfb1":"code","c3ef3a36":"code","a11df64f":"markdown","52724809":"markdown","a2d30290":"markdown","4d50c3e0":"markdown","4db73b37":"markdown"},"source":{"67c3531d":"import pandas as pd\nimport numpy as np\n\n# for ploting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# for linear algebra\nfrom scipy import linalg\n\n# for PCA and scaling\nfrom sklearn import decomposition\nfrom sklearn import preprocessing","e5e1413a":"data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ndata.head()","2d7767bc":"label = data.label\ndata = data.drop('label' , axis = 1)","8e5ab5d3":"one_image = data.loc[3,:]\nplt.figure(figsize = (7,7))\none = np.array(one_image).reshape(28,28)\nplt.imshow(one , cmap = 'gray')","c7edc407":"# Let's do code\n# Preprocessing\n# we can do standardization using scikit-learn as well\n\nscaler = preprocessing.StandardScaler()\nscaler.fit(data)\ns_data = scaler.transform(data)","de3b3f43":"# standardization\nfor i in range(784):\n    s = f\"pixel{i}\"\n    if np.std(data[s]) != 0:\n        data[s] = (data[s] - np.mean(data[s])) \/ np.std(data[s])","7a4bc599":"data = s_data\n\n# create covariance matrix\nS = np.matmul(data.T , data)\n\n# compute eigen vector\n# this function return eigen values and vectors in ascending order\n# through eigvals we're selecting 2 eigen vector with heighest eigen values\nvalues , vectors = linalg.eigh(S , eigvals = (782 , 783))\n\nprint(vectors.shape)\n\n# so we need to convert vector to transpose\nvectors = vectors.T","0037b3fe":"# now let's project original data point on vectors\nnew_data = np.matmul(vectors , data.T)\nprint(new_data.shape)","976941f0":"new_data = np.vstack((new_data , label)).T\nreduced_data = pd.DataFrame(\n    data = new_data,\n    columns = ['1st_principle' , '2nd_principle' , 'label']\n)\nreduced_data.head()","ea4a9855":"# now let's plot data\nsns.FacetGrid(reduced_data , hue = 'label' , height = 7).map(plt.scatter , '1st_principle' , '2nd_principle').add_legend()","6a8bdfb1":"# Now let's use scikit learn for PCA\npca = decomposition.PCA()\n\npca.n_components = 2\npca_data = pca.fit_transform(s_data)\n\npca_data = np.vstack((pca_data.T , label)).T\n\npca_df = pd.DataFrame(\n    data = pca_data,\n    columns = ['1st_principle' , '2nd_principle' , 'label']\n)\n\npca_df.head()","c3ef3a36":"sns.FacetGrid(pca_df , hue = 'label' , height = 7).map(plt.scatter , '1st_principle' , '2nd_principle').add_legend()","a11df64f":"**Dimensionality Reduction :**\n\n* Dimensionality reduction is a technique to visualize data that has more dimension (typically more than 3 dimension)\n\n* There are two methods that widely used for reduction one is PCA(principal component analysis) and other is t-SNE (t-distributed stochastic neighbourhood embedding)\n","52724809":"Geometric intuition of PCA :\n\n* Let's say we have 2 dimensional graph and we want to convert it into 1 dimensional graph.\n\n* e.g. if we have a graph like below, then we'll use f2 as a main feature and get rid of f1 because f2 has a more spread so we'll loose very small amount of information. \n\n![](https:\/\/i.ibb.co\/6wsvx8D\/dr.png)\n\n* Let's look at another complicated example, In below graph, spread in f1 and f2 direction are same, so In such case we'll rotate f1 and f2 and select f1' as a pricipal direction\n\n![](https:\/\/i.ibb.co\/4mYY9B2\/dr2.png)\n\n* To conclude , we want to find direction f1' such that the variance of $x_i$'s (data point) projected onto f1' is maximized\n\nMathematical Objective function of PCA (variance maximization):\n\n* Let's call $u_1$ = f1' and ||$u_1$|| = 1 (cause $u_1$ is just a direction)\n\n* $x_i$' = $proj_{u_1} x_i$ now we know that,\n\n  $x_i$' = $u_1 \\cdot x_i$ \/ ||$u_1$|| = $u_1^T * x_i$\n  \n  $\\bar{x_i}' = u_1^T * \\bar{x_i}$   where $\\bar{x_i} =$ mean of $x_i$\n  \n* So our objective is to find $u_1$ such that $variance(Proj_{u_1} x_i)_{i=1}^{n}$ is maximized.\n\n  $var(u_1^T * x_i)_{i=1}^n$ = $(1\/n) * \\sum_{i=1}^n (u_1^T * x_i - u_1^T * \\bar{x_i})^2$\n  \n  But we know that after standardization $\\bar{x_i} = 0$ so,\n  \n  $var(u_1^T * x_i)_{i=1}^n$ = $(1\/n) * \\sum_{i=1}^n (u_1^T * x_i)^2$\n  \n* Let's rewrite our objective more formally,\n\n  Objective = $\\underset{u_1}{max}$ $(1\/n) * \\sum_{i=1}^n (u_1^T * x_i)^2$ such that ||$u_1$|| = 1 ......................(1)\n  \n  Objective = $\\underset{u_1}{max}$ $(1\/n) * \\sum_{i=1}^n (u_1^T * x_i) * (x_i^T * u_1)$\n  \n  Objective = $\\underset{u_1}{max}$ $ u_1^T * [ (1\/n) * \\sum_{i=1}^n (x_i * x_i^T) ] * u_1$\n  \n  Objective = $\\underset{u_1}{max}$ $ u_1^T * [ (1\/n) * (X^T \\cdot X) ] * u_1$\n  \n  Objective = $\\underset{u_1}{max}$ $ u_1^T * S * u_1$ ........................(2)\n  \n   where $S$ = Covariance matrix\n   \n  we can solve eq. (2) using lagrangian multiplier and get the $u_1$.\n   \nAnother formulation for PCA (distance minimization) :\n\n![](https:\/\/i.ibb.co\/12NnCMk\/dr3.png)\n\n* In above image red line is a distance of data point ($x_i$) to line $u_1$,\n\n* so in this case our aim is to find $u_1$ such that sum of distance of data point is minimized. In more formally,\n\n  $ Aim = \\underset{u_1}{min}$ $d_i^2 $ \n  \n  where $d_i$ = distance of $x_i$ to $u_1$\n  \n![](https:\/\/i.ibb.co\/pQ85sFs\/dr4.png)\n\n  By looking at above picture we can say that,\n\n  $d_i^2 = (||x_i||)^2 - (proj_{u_1}x_i)^2$\n  \n  $d_i^2 = x_i^T \\cdot x_i - (u_1^T \\cdot x_i)^2 $\n  \n  $ Aim = \\underset{u_1}{min}$ $ (x_i^T \\cdot x_i - (u_1^T \\cdot x_i)^2)$ ........................(3)\n  \n  By looking at both equation (1) and (3) we can say both are same after doing some maths.\n  \n* Solution of our main objective(aim) is,\n\n  Let's first define some of the terminology, \n  \n  Eigen values of covariance matrix ($S$) are : $\\lambda_1 , \\lambda_2 , ... \\lambda_d$\n\n  Eigen vector correspond to eigen values are : $v_1 , v_2 , ... v_d$\n  \n  and also assume that : $\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_d $\n  \n  So Our $u_1$ = $v_1$ = eigen vector correspond to maximum eigen values\n  \n  We can get this by solving eq. (1) or (3). You can check out this [video](https:\/\/www.youtube.com\/watch?v=MUENAuYkgmI) for proof.\n\n  By using ($\\lambda_1 , \\lambda_2 ... \\lambda_i $) we can preserve = ($ \\lambda_1 + \\lambda_2 + ... + \\lambda_i $)*100 \/($\\sum_{j=1}^d \\lambda_j$)\n  percent information.\n  \n  \nNow Let's Use all this info. to do dimensionality reduction :\n\n* Let's say our data is $X$ of $(n*d)$ dimension and we want to convert this data set into $(n*2)$ dimension,\n\n  First we'll calculate covariance matrix $(S)$ that is equal to $X^T \\cdot X$\n  \n  Second step is to calculate eigen values of $S$ and also find corresponding eigen vector $(v_1,v_2 ...v_d)$\n  \n  Then our transformed data $X$' will be $(n*2)$ dimensional and first column of $X$ will be $X \\cdot v_1$ and second column of $X$ will be $X \\cdot v_2$.","a2d30290":"Limitations of PCA :\n\n![](https:\/\/i.ibb.co\/w6DvrzS\/dr5.png)\n\n* In Graph 1 we can see that cluster 1 and cluster 2 will be projected on the same place so we'll not able distinguish data specifically.\n\n* In Graph 2 we can see that data is spreaded in every direction equally in this case if we do dimensionality reduction then we'll loose lots of data.\n\n* there are many cases in which PCA won't work or work very badly.","4d50c3e0":"Preprocessing :\n\n* Before doing dimensionalty reduction , It is important to normalize data\n\n* Normalization is done by below formula , \n\n  $A_i = (A_i - min(A_i))\\hspace{0.2cm} \/ \\hspace{0.2cm} (max(A_i) - min(A_i)) $\n\n  $      where$ $A_i = i^{th} column$\n\n* By doing normalization we're putting every data point on the same scale. More formally After normalization all data points relies between [0,1]\n\n* There is also another technique known as Standardization (It is more used then normalization)\n\n* Standardization is done by below formula ,\n\n  $A_i = ( A_i - \\mu_i ) \\hspace{0.2cm} \/ \\hspace{0.2cm} \\sigma_i $\n  \n  $      where$ $A_i = i^{th} column$\n  \n  $\\hspace{1cm}  \\mu_i = mean$ $of$ $i^{th}$ $column$\n  \n  $\\hspace{1cm} \\sigma_i = standard$ $deviation $ $of$ $i^{th}$ $column$\n  \n* After Standardization, mean of data becomes 0 and standard deviation of data becomes 1, More Formally $\\mu_i = 0$ and $\\sigma_i = 1$.\n\n* Another thing that we have to know before starting Dimensionality reduction is co-variance matrix.\n\n* Co-variance matrix $(S)$ of data is $d * d$ dimensional matrix , where $d =$ number of feature(column)\n\n  $ S_{ij} = Co-variance(A_{i} , A_{j}) $\n  \n  where $S_{ij} =$ $element$ $of$ $i^{th}$ $row$ $and$ $j^{th}$ $column$ \n  \n  $ \\hspace{1cm} A_{i} = i^{th} column $\n  \n  $ \\hspace{1cm} A_{j} = j^{th} column $\n  \n  $ \\hspace{1cm} Co-variance(X , Y) = (1\/n) * \\sum_{i=0}^{i=n} (x_{i} - \\mu_{x}) * (y_{i} - \\mu_{y})$\n  \n  $ \\hspace{1cm} Co-variance(X , X) = variance(X)$\n  \n  So diagonal of co-variance matrix(S) will be variance and S will be symmetric as well.\n  \n  After Standardization mean $(\\mu)$ of column becomes $0$ so,\n  \n  $ \\hspace{1cm} Co-variance(X , Y) = (1\/n) * \\sum_{i=0}^{i=n} ( x_{i} * y_{i} )$ and this can be written as,\n  \n  $ \\hspace{1cm} Co-variance(X , Y) = (1\/n) * (X^{T} \\cdot Y)$ \n  \n  and we can proove that, After Standardization $S = 1\/(n-1) * (X^T \\cdot X)$\n  \n  In above equation we divide $S$ by $(n-1)$ cause we do not have whole data(so mean of whole data is unknown)\n  \n  here's [link](https:\/\/en.wikipedia.org\/wiki\/Covariance#Calculating_the_sample_covariance) for when to use $n$ and when to use $n-1$\n","4db73b37":"# **1. Principal Component Analysis(PCA)**"}}