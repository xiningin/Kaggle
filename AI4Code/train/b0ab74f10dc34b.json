{"cell_type":{"c39d0290":"code","939e3f9b":"code","7fc574b6":"code","a5a4bdcd":"code","eaba5a9a":"code","bb1af53a":"code","28a9cc11":"code","4c9e70b7":"code","bc9781b6":"code","d8e68093":"code","df025d65":"code","5a6a2bbc":"code","fbcae1aa":"code","81726583":"markdown","2fa19566":"markdown","fbd74071":"markdown","dc697aa1":"markdown","8fbf8c6f":"markdown","5e86e29f":"markdown","a249ab80":"markdown"},"source":{"c39d0290":"# These resources must be created by hand, either by the gcloud cli or via the GCP console\nPROJECT_ID = \"cord-19-271603\"\nBUCKET_NAME = \"acmiyaguchi-cord-19-data\"\nDATASET_ID = \"kaggle\"","939e3f9b":"! pip install pyspark","7fc574b6":"from pyspark.sql.functions import lit\nfrom pyspark.sql.types import (\n    ArrayType,\n    IntegerType,\n    MapType,\n    StringType,\n    StructField,\n    StructType,\n)\n\n\ndef generate_cord19_schema():\n    \"\"\"Generate a Spark schema based on the semi-textual description of CORD-19 Dataset.\n\n    This captures most of the structure from the crawled documents, and has been\n    tested with the 2020-03-13 dump provided by the CORD-19 Kaggle competition.\n    The schema is available at [1], and is also provided in a copy of the\n    challenge dataset.\n\n    One improvement that could be made to the original schema is to write it as\n    JSON schema, which could be used to validate the structure of the dumps. I\n    also noticed that the schema incorrectly nests fields that appear after the\n    `metadata` section e.g. `abstract`.\n    \n    [1] https:\/\/ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com\/2020-03-13\/json_schema.txt\n    \"\"\"\n\n    # shared by `metadata.authors` and `bib_entries.[].authors`\n    author_fields = [\n        StructField(\"first\", StringType()),\n        StructField(\"middle\", ArrayType(StringType())),\n        StructField(\"last\", StringType()),\n        StructField(\"suffix\", StringType()),\n    ]\n\n    authors_schema = ArrayType(\n        StructType(\n            author_fields\n            + [\n                StructField(\n                    \"affiliation\",\n                    StructType(\n                        [\n                            StructField(\"laboratory\", StringType()),\n                            StructField(\"institution\", StringType()),\n                            StructField(\n                                \"location\",\n                                StructType(\n                                    [\n                                        StructField(\"settlement\", StringType()),\n                                        StructField(\"country\", StringType()),\n                                    ]\n                                ),\n                            ),\n                        ]\n                    ),\n                ),\n                StructField(\"email\", StringType()),\n            ]\n        )\n    )\n\n    # used in `section_schema` for citations, references, and equations\n    spans_schema = ArrayType(\n        StructType(\n            [\n                # character indices of inline citations\n                StructField(\"start\", IntegerType()),\n                StructField(\"end\", IntegerType()),\n                StructField(\"text\", StringType()),\n                StructField(\"ref_id\", StringType()),\n            ]\n        )\n    )\n\n    # A section of the paper, which includes the abstract, body, and back matter.\n    section_schema = ArrayType(\n        StructType(\n            [\n                StructField(\"text\", StringType()),\n                StructField(\"cite_spans\", spans_schema),\n                StructField(\"ref_spans\", spans_schema),\n                # Equations don't appear in the abstract, but appear here\n                # for consistency\n                StructField(\"eq_spans\", spans_schema),\n                StructField(\"section\", StringType()),\n            ]\n        )\n    )\n\n    bib_schema = MapType(\n        StringType(),\n        StructType(\n            [\n                StructField(\"ref_id\", StringType()),\n                StructField(\"title\", StringType()),\n                StructField(\"authors\", ArrayType(StructType(author_fields))),\n                StructField(\"year\", IntegerType()),\n                StructField(\"venue\", StringType()),\n                StructField(\"volume\", StringType()),\n                StructField(\"issn\", StringType()),\n                StructField(\"pages\", StringType()),\n                StructField(\n                    \"other_ids\",\n                    StructType([StructField(\"DOI\", ArrayType(StringType()))]),\n                ),\n            ]\n        ),\n        True,\n    )\n\n    # Can be one of table or figure captions\n    ref_schema = MapType(\n        StringType(),\n        StructType(\n            [\n                StructField(\"text\", StringType()),\n                # Likely equation spans, not included in source schema, but\n                # appears in JSON\n                StructField(\"latex\", StringType()),\n                StructField(\"type\", StringType()),\n            ]\n        ),\n    )\n\n    return StructType(\n        [\n            StructField(\"paper_id\", StringType()),\n            StructField(\n                \"metadata\",\n                StructType(\n                    [\n                        StructField(\"title\", StringType()),\n                        StructField(\"authors\", authors_schema),\n                    ]\n                ),\n                True,\n            ),\n            StructField(\"abstract\", section_schema),\n            StructField(\"body_text\", section_schema),\n            StructField(\"bib_entries\", bib_schema),\n            StructField(\"ref_entries\", ref_schema),\n            StructField(\"back_matter\", section_schema),\n        ]\n    )\n\n\ndef extract_dataframe_kaggle(spark):\n    \"\"\"Extract a structured DataFrame from the semi-structured document dump.\n\n    It should be fairly straightforward to modify this once there are new\n    documents available. The date of availability (`crawl_date`) and `source`\n    are available as metadata.\n    \"\"\"\n    base = \"\/kaggle\/input\/CORD-19-research-challenge\"\n    crawled_date = \"2020-03-20\"\n    sources = [\n        \"noncomm_use_subset\",\n        \"comm_use_subset\",\n        \"biorxiv_medrxiv\",\n        \"custom_license\",\n    ]\n\n    dataframe = None\n    for source in sources:\n        path = f\"{base}\/{source}\/{source}\"\n        df = (\n            spark.read.json(path, schema=generate_cord19_schema(), multiLine=True)\n            .withColumn(\"crawled_date\", lit(crawled_date))\n            .withColumn(\"source\", lit(source))\n        )\n        if not dataframe:\n            dataframe = df\n        else:\n            dataframe = dataframe.union(df)\n    return dataframe\n","a5a4bdcd":"! ls \/kaggle\/input\/CORD-19-research-challenge","eaba5a9a":"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\ndf = extract_dataframe_kaggle(spark)\n\noutput_path = \"\/kaggle\/working\/cord19\"\n%time df.coalesce(1).write.parquet(output_path, mode=\"overwrite\")\n\n! ls -alh \/kaggle\/working\/cord19\n! cp \/kaggle\/working\/cord19\/*.parquet .\/cord19.parquet","bb1af53a":"import pandas as pd\nimport re\n\ndef snake_case(name):\n    return \"_\".join([word.lower() for word in re.split(r\"\\W\", name) if word])\n\ninput_path = \"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\"\noutput_path = \"metadata.parquet\"\n\ndf = pd.read_csv(input_path)\ndf.columns = [snake_case(name) for name in df.columns]\n%time df.to_parquet(output_path)","28a9cc11":"! ls -alh .","4c9e70b7":"# https:\/\/cloud.google.com\/storage\/docs\/uploading-objects#storage-upload-object-python\n# https:\/\/cloud.google.com\/bigquery\/docs\/loading-data-cloud-storage-parquet\nimport glob\nfrom google.cloud import storage\nfrom google.cloud import bigquery\n\ndef upload(\n    project,\n    bucket_name,\n    source_file_name,\n    destination_blob_name,\n    dataset_id,\n    table_id,\n    source_format=bigquery.SourceFormat.PARQUET\n):\n    storage_client = storage.Client(project=project)\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_filename(source_file_name)\n    uri = f\"gs:\/\/{bucket_name}\/{destination_blob_name}\"\n    print(f\"copied {source_file_name} to {uri}\")\n\n    bigquery_client = bigquery.Client(project=PROJECT_ID)\n    dataset_ref = bigquery_client.dataset(dataset_id)\n\n    job_config = bigquery.LoadJobConfig()\n    job_config.source_format = source_format\n    job_config.write_disposition = bigquery.job.WriteDisposition.WRITE_TRUNCATE\n\n    print(f\"loading {uri} into {project}:{dataset_id}.{table_id}\")\n    load_job = bigquery_client.load_table_from_uri(\n        uri, dataset_ref.table(table_id), job_config=job_config\n    )\n    load_job.result()","bc9781b6":"upload(\n    PROJECT_ID, \n    BUCKET_NAME, \n    \"cord19.parquet\", \n    \"2020-03-20_cord19.parquet\", \n    DATASET_ID,\n    \"cord19\",\n)","d8e68093":"upload(\n    PROJECT_ID, \n    BUCKET_NAME, \n    \"metadata.parquet\", \n    \"2020-03-20_metadata.parquet\", \n    DATASET_ID,\n    \"metadata\",\n)","df025d65":"client = bigquery.Client(project=PROJECT_ID)","5a6a2bbc":"query = \"\"\"\nWITH\n  cited_publications AS (\n  SELECT\n    metadata.title AS dst_title,\n    bib_entry.value.title AS src_title\n  FROM\n    `cord-19-271603.kaggle.cord19`,\n    UNNEST(bib_entries.key_value) AS bib_entry )\nSELECT\n  src_title AS title,\n  COUNT(DISTINCT dst_title) AS num_referenced\nFROM\n  cited_publications\nGROUP BY\n  title\nORDER BY\n  num_referenced DESC\nLIMIT 20\n\"\"\"\n\n# Query complete (4.8 sec elapsed, 63.3 MB processed)\nfor row in client.query(query).result():\n    print(f\"{row.num_referenced}: {row.title[:80]}\")","fbcae1aa":"query = \"\"\"\nSELECT\n  source_x,\n  SUM(CAST(has_full_text AS INT64)) AS num_full_text,\n  COUNT(*) AS total_documents\nFROM\n  `cord-19-271603.kaggle.metadata`\nGROUP BY\n  1\n\"\"\"\n\n# Query complete (0.4 sec elapsed, 164.7 KB processed)\nfor row in client.query(query).result():\n    print(f\"{row.source_x}: {row.num_full_text}\/{row.total_documents} have text\")","81726583":"## all sources metadata","2fa19566":"# BigQuery Dataset for CORD-19\n\nSee https:\/\/www.kaggle.com\/acmiyaguchi\/pyspark-dataframe-preprocessing-for-cord-19 for working with Spark.","fbd74071":"# Code","dc697aa1":"# Extract, Tranform, Load\n\n## cord19","8fbf8c6f":"# Querying the table","5e86e29f":"## Upload to BigQuery","a249ab80":"## Overview of output"}}