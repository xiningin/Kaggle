{"cell_type":{"4199d087":"code","4a56c27f":"code","34da203e":"code","0701d364":"code","9e6954da":"code","bd44c43b":"code","744c842c":"code","4ce19921":"code","449b858f":"code","5485cb8d":"code","569c5a4c":"code","ba11e96a":"code","0a7e016e":"code","2e4f292d":"code","025b6aa8":"code","2aacc647":"code","33ab2c1a":"code","a3a8c9ca":"code","d698a7b6":"code","b3c32d61":"code","79bb50a1":"code","0d4d5791":"code","fd88c09a":"code","e55eddac":"code","0206c1ad":"code","77ac414c":"code","edba9f85":"code","7fb264fb":"code","935413dc":"code","f1f4304f":"code","1bbbb502":"code","5cc08bf3":"code","4a38a1c2":"code","1f0335b4":"code","d78866ef":"code","606b030e":"code","bfd4cbd8":"code","485aad1a":"code","7ac99308":"code","e73b4d2c":"code","25cd8416":"code","6348b088":"code","acb0220c":"code","521b2c0d":"code","aa221794":"code","287aa4f6":"code","a4b69321":"code","ef9bf315":"code","0d4a68c2":"code","9936c712":"code","0698d94b":"code","712d3ea8":"markdown","da0bdd0f":"markdown","e4f7b0d7":"markdown","6f910cfb":"markdown","628490c5":"markdown","b6bceada":"markdown","8d2ef53e":"markdown","1a826e4b":"markdown","dc63c614":"markdown","4363b546":"markdown","5f99437e":"markdown","cfbfd658":"markdown","0297f056":"markdown","1afbfbcb":"markdown","9bd119f9":"markdown","99f275f7":"markdown","7d9923df":"markdown","0b4a2d92":"markdown","c0bfdb13":"markdown","035d99db":"markdown","85818a03":"markdown"},"source":{"4199d087":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\nimport os\nimport logging\nimport datetime\n\nimport lightgbm as lgb\n\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')","4a56c27f":"IS_LOCAL = False\nif(IS_LOCAL):\n    PATH=\"..\/input\/Santander\/\"\nelse:\n    PATH=\"..\/input\/\"\nos.listdir(PATH)","34da203e":"# import Dataset to play with it\ntrain= pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/train.csv\")\ntest = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/test.csv')\nsample_submission = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/sample_submission.csv')\nsample_submission.head()","0701d364":"train.shape, test.shape, sample_submission.shape","9e6954da":"train.head()","bd44c43b":"train.info()","744c842c":"def reduce_mem_usage(df):\n    start_mem_usg = df.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in df.columns:\n        if df[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            print(\"******************************\")\n            print(\"Column: \",col)\n            print(\"dtype before: \",df[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = df[col].max()\n            mn = df[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(df[col]).all(): \n                NAlist.append(col)\n                df[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = df[col].fillna(0).astype(np.int64)\n            result = (df[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < 65535:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        df[col] = df[col].astype(np.uint32)\n                    else:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                df[col] = df[col].astype(np.float32)\n            \n            # Print new column type\n            print(\"dtype after: \",df[col].dtype)\n            print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = df.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return df, NAlist","4ce19921":"train, NAlist = reduce_mem_usage(train)\nprint(\"_________________\")\nprint(\"\")\nprint(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\nprint(\"_________________\")\nprint(\"\")\nprint(NAlist)","449b858f":"test, NAlist = reduce_mem_usage(test)\nprint(\"_________________\")\nprint(\"\")\nprint(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\nprint(\"_________________\")\nprint(\"\")\nprint(NAlist)","5485cb8d":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","569c5a4c":"missing_data(train)","ba11e96a":"missing_data(test)","0a7e016e":"train.describe().T","2e4f292d":"test.describe().T","025b6aa8":"sns.countplot(train['target'], palette='Set3')","2aacc647":"print(\"There are {}% target values with 1 on the train data\".format(100 * train[\"target\"].value_counts()[1]\/train.shape[0]))","33ab2c1a":"f,ax=plt.subplots(1,2,figsize=(20,10))\ntrain[train['target']==0].var_0.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')\nax[0].set_title('target= 0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ntrain[train['target']==1].var_0.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')\nax[1].set_title('target= 1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()\n","a3a8c9ca":"train[\"var_10\"].hist();","d698a7b6":"train[\"var_52\"].hist();","b3c32d61":"train[\"var_181\"].hist();","79bb50a1":"sns.set(rc={'figure.figsize':(9,7)})\nsns.distplot(train['target']);","0d4d5791":"sns.violinplot(data=train,x=\"target\", y=\"var_10\")","fd88c09a":"sns.violinplot(data=train,x=\"target\", y=\"var_110\")","e55eddac":"sns.violinplot(data=train,x=\"target\", y=\"var_81\")","0206c1ad":"train[train.columns[2:]].mean().plot(kind='hist', color='#ffa600');plt.title('Mean Frequency');","77ac414c":"features = train.columns.values[2:202]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(train[features].mean(axis=0),color=\"magenta\",kde=True,bins=120, label='train')\nsns.distplot(test[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","edba9f85":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(train[features].mean(axis=1),color=\"green\", kde=True,bins=120, label='train')\nsns.distplot(test[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","7fb264fb":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per row in the train and test set\")\nsns.distplot(train[features].std(axis=1),color=\"black\", kde=True,bins=120, label='train')\nsns.distplot(test[features].std(axis=1),color=\"red\", kde=True,bins=120, label='test')\nplt.legend();plt.show()","935413dc":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per column in the train and test set\")\nsns.distplot(train[features].std(axis=0),color=\"blue\",kde=True,bins=120, label='train')\nsns.distplot(test[features].std(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend(); plt.show()","f1f4304f":"train['target'].unique()","1bbbb502":"train['target'].value_counts()","5cc08bf3":"def check_balance(df,target):\n    check=[]\n    # written by MJ Bahmani for binary target\n    print('size of data is:',df.shape[0] )\n    for i in [0,1]:\n        print('for target  {} ='.format(i))\n        print(df[target].value_counts()[i]\/df.shape[0]*100,'%')","4a38a1c2":"check_balance(train,'target')","1f0335b4":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train['target'].skew())\nprint(\"Kurtosis: %f\" % train['target'].kurt())","d78866ef":"%%time\ncorrelations = train[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.head(10)","606b030e":"correlations.tail(10)","bfd4cbd8":"%%time\nfeatures = train.columns.values[2:202]\nunique_max_train = []\nunique_max_test = []\nfor feature in features:\n    values = train[feature].value_counts()\n    unique_max_train.append([feature, values.max(), values.idxmax()])\n    values = test[feature].value_counts()\n    unique_max_test.append([feature, values.max(), values.idxmax()])","485aad1a":"np.transpose((pd.DataFrame(unique_max_train, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head())\n#TRAIN SET","7ac99308":"np.transpose((pd.DataFrame(unique_max_test, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head())\n#TEST SET","e73b4d2c":"%%time\nidx = features = train.columns.values[2:202]\nfor df in [test, train]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)","25cd8416":"train[train.columns[202:]].head()","6348b088":"test[test.columns[201:]].head()","acb0220c":"def plot_new_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,4,figsize=(18,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,4,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=11)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","521b2c0d":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\nfeatures = train.columns.values[202:]\nplot_new_feature_distribution(t0, t1, 'target: 0', 'target: 1', features)","aa221794":"features = train.columns.values[202:]\nplot_new_feature_distribution(train, test, 'train', 'test', features)","287aa4f6":"features = [c for c in train.columns if c not in ['ID_code', 'target']]\nfor feature in features:\n    train['r2_'+feature] = np.round(train[feature], 2)\n    test['r2_'+feature] = np.round(test[feature], 2)\n    train['r1_'+feature] = np.round(train[feature], 1)\n    test['r1_'+feature] = np.round(test[feature], 1)","a4b69321":"print('Train and test columns: {} {}'.format(len(train.columns), len(test.columns)))","ef9bf315":"features = [c for c in train.columns if c not in ['ID_code', 'target']]\ntarget = train['target']","0d4a68c2":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}","9936c712":"folds = StratifiedKFold(n_splits=10, shuffle=False, random_state=44000)\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 1000000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","0698d94b":"sub_enes = pd.DataFrame({\"ID_code\":test[\"ID_code\"].values})\nsub_enes[\"target\"] = predictions\nsub_enes.to_csv(\"submission.csv\", index=False)","712d3ea8":"# Data Visualization","da0bdd0f":"# Santander Customer Transaction Prediction\n![image.png](attachment:63868198-337e-4338-80d2-880182801764.png)","e4f7b0d7":"# Features correlation","6f910cfb":"> Let's check the distribution of these new, engineered features.\n> \n> We plot first the distribution of new features, grouped by value of corresponding target values.","628490c5":"> # Introduction\n> \n> In this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem.","b6bceada":"# Imports","8d2ef53e":"> A large part of the data is unbalanced, but how can we solve it?","1a826e4b":"> **Before the beginning I want to say that I hope you find this notebook helpful and some  UPVOTES would be very much appreciated. Thanks a lot.**","dc63c614":"# Duplicate values","4363b546":"# Load data","5f99437e":"# Submission","cfbfd658":"# Feature engineering","0297f056":"# **References & credits **","1afbfbcb":"> **Hyperparameters for the model.**","9bd119f9":"> **Reducing memory size about %50**\n> Because we make a lot of calculations in this kernel, we'd better reduce the size of the data.","99f275f7":"# Data exploration","7d9923df":"# Modelling","0b4a2d92":"> **The target in dataset is imbalance**","c0bfdb13":"1. https:\/\/www.kaggle.com\/gpreda\/elo-world-high-score-without-blending\n2. https:\/\/www.kaggle.com\/chocozzz\/santander-lightgbm-baseline-lb-0-897\n3. https:\/\/www.kaggle.com\/brandenkmurray\/nothing-works\n4. https:\/\/www.kaggle.com\/dansbecker\/permutation-importance\n5. https:\/\/www.kaggle.com\/dansbecker\/partial-plots\n6. https:\/\/www.kaggle.com\/miklgr500\/catboost-with-gridsearch-cv\n7. https:\/\/www.kaggle.com\/dromosys\/sctp-working-lgb\n8. https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction\n9. https:\/\/www.kaggle.com\/dansbecker\/permutation-importance\n10. https:\/\/www.kaggle.com\/dansbecker\/partial-plots\n11. https:\/\/www.kaggle.com\/dansbecker\/shap-values\n12. https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/algorithm-choice\n13. https:\/\/www.kaggle.com\/arjanso\/reducing-dataframe-memory-size-by-65\n14. https:\/\/www.kaggle.com\/brandenkmurray\/nothing-works\n15. https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction\n16. https:\/\/www.kaggle.com\/mjbahmani\/santander-ml-explainability#--Santander-ML-Explainability--","035d99db":"# Binary Classification","85818a03":"> **Numerical values Describe**\n> *  There are no missing data in train and test datasets. Let's check the numerical values in train and test dataset."}}