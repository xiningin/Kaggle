{"cell_type":{"cfdfa0ec":"code","afad71e9":"code","a41c335f":"code","36df69e7":"code","f0350bea":"code","7abd2176":"markdown","66d786b8":"markdown","7e5ed851":"markdown"},"source":{"cfdfa0ec":"%matplotlib inline\nimport gc, os, sys, time\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nfrom IPython.display import HTML, display\n\nIN_DIR = '..\/input'\nAUC = 'Area Under Receiver Operating Characteristic Curve'\n\ndef read_csv_filtered(csv, col, values, **kwargs):\n    dfs = [df.loc[df[col].isin(values)]\n           for df in pd.read_csv(f'{IN_DIR}\/{csv}',\n                                 chunksize=100000, **kwargs)]\n    return pd.concat(dfs, axis=0)\n\nTYPES = { 'Featured', 'Research', 'Recruitment' }\n\n# Read Competitions\ncomps = read_csv_filtered('Competitions.csv', 'HostSegmentTitle', TYPES).set_index('Id')\nidx = comps.EvaluationAlgorithmName.isnull()\ncomps.loc[idx, 'EvaluationAlgorithmName'] = comps.loc[idx, 'EvaluationAlgorithmAbbreviation']\ncomps['Year'] = pd.to_datetime(comps.DeadlineDate).dt.year\ncomps['RewardQuantity'].fillna('', inplace=True)\n\ncomps = comps.query('EvaluationAlgorithmName==@AUC', engine='python')\nprint(\"Competitions\", comps.shape)\n\n# Read Teams for those Competitions\nasints = ['PublicLeaderboardSubmissionId',\n           'PrivateLeaderboardSubmissionId']\nDT = {c:'object' for c in asints}\nteams = read_csv_filtered('Teams.csv', 'CompetitionId', comps.index, dtype=DT).set_index('Id')\nteams[asints] = teams[asints].fillna(-1).astype(int)\nprint(\"Teams\", teams.shape)\n\nasfloats = ['PublicScoreLeaderboardDisplay',\n            'PublicScoreFullPrecision',\n            'PrivateScoreLeaderboardDisplay',\n            'PrivateScoreFullPrecision',]\n\n# Read Submissions for those Teams\nsubs = read_csv_filtered('Submissions.csv', 'TeamId', teams.index)\nsubs = subs.query('not IsAfterDeadline', engine='python')\nsubs[asfloats] = subs[asfloats].astype(float)\nprint(\"Submissions\", subs.shape)\n\nsubs['CompetitionId'] = subs.TeamId.map(teams.CompetitionId)\nsubs['UsedPublic'] = subs.Id.isin(teams.PublicLeaderboardSubmissionId)\nsubs['UsedPrivate'] = subs.Id.isin(teams.PrivateLeaderboardSubmissionId)\nsubs['Used'] = subs.eval('UsedPublic or UsedPrivate')\n\nsubs['dist1'] = ((subs.PublicScoreFullPrecision-0.25)**2 + (subs.PrivateScoreFullPrecision-0.75)**2) ** 0.5\nsubs['dist2'] = ((subs.PublicScoreFullPrecision-0.75)**2 + (subs.PrivateScoreFullPrecision-0.25)**2) ** 0.5\n\nCOLORS = np.asarray([\n    '#c0c0c080', # gray: not used for either leaderboard\n    '#0000ffff', # blue: used as public lb entry for team\n    '#ff0000ff', # red: used as private lb entry for team\n    '#ff00ffff'  # purple: used as public and private lb entry for team\n])\nRADIUS = 0.12\n\nsubs['Color'] = (subs.UsedPublic*1) + (subs.UsedPrivate*2)\n# subs['Color'].value_counts()\n\n# Plot each Competition\nfor comp, df in subs.groupby('CompetitionId'):\n    if df.PublicScoreFullPrecision.var() == 0 or df.PrivateScoreFullPrecision.var() == 0:\n        continue\n    c = comps.loc[comp]\n    if c.Year < 2013:  # bug not there 2010..2012\n        continue\n    a = (df.dist1 <= RADIUS).sum()\n    b = (df.dist2 <= RADIUS).sum()\n    used_flippable = df.eval('UsedPrivate and PrivateScoreFullPrecision<0.5').sum()\n    top_flipzone = df.eval('UsedPrivate and PrivateScoreFullPrecision<0.75').sum()\n    teams = c.TotalTeams # should == df.UsedPrivate.sum()\n    display(HTML(\n        f'<h1 id=\"{c.Slug}\">{c.Title}<\/h1><h3>{c.Subtitle}<\/h3>'\n        f'<p>{teams} teams &mdash; {c.TotalSubmissions} submissions.<br\/>'\n        f'public low ~0.25 &rarr; private high ~0.75 = {a}<br\/>'\n        f'public high ~0.75 &rarr; private low ~0.25 = {b}<br\/>'\n        f'private LB teams under 0.5 = {used_flippable}<br\/>'\n        f'private LB teams under 0.75 = {top_flipzone}<br\/>'\n    ))\n    \n    title = f'{c.Title} \u2014 {teams} teams \u2014 {c.TotalSubmissions} submissions \u2014 {c.Year}'\n    df1 = df.sort_values('Used')\n    ax = df1.plot.scatter('PublicScoreFullPrecision', 'PrivateScoreFullPrecision', c=COLORS[df1.Color], title=title, figsize=(14, 14))\n    \n    # https:\/\/stackoverflow.com\/questions\/4143502\/how-to-do-a-scatter-plot-with-empty-circles-in-python\n    for centre in [ (0.25, 0.75), (0.75, 0.25) ]:\n        e = Circle(xy=centre, radius=RADIUS)\n        ax.add_artist(e)\n        e.set_clip_box(ax.bbox)\n        e.set_edgecolor('black')\n        e.set_facecolor('none')  # \"none\" not None\n        e.set_alpha(0.5)\n\n    plt.axis('equal')\n    plt.show()","afad71e9":"subs.dist1.plot.hist(bins=100)","a41c335f":"subs.dist2.plot.hist(bins=100)","36df69e7":"subs.dist1[subs.dist1<0.3].plot.hist(bins=100)","f0350bea":"subs.dist2[subs.dist2<0.3].plot.hist(bins=100)","7abd2176":"# AUC Scoring Quirk\n\nWith Kaggle's AUC metric it is [impossible to score less than 0.25][2] - predictions with lower AUC than that are inverted with 1-score, so 0.2 becomes 0.8 and 0.1 shows as 0.9 etc.\n\nThis kernel selects all AUC competitions and plots every submission for each as Public LB vs Private LB score - these scores should be very correlated.\n\nHowever - the plots have outliers: it seems the inversion is applied **separately** to public and private scores.\n\nThis means it is possible to score ~0.25 on one leaderboard and ~0.75 on the other.\n\nSome of the competitions have a random public\/private test set split - so it should not be possible to have such divergent scores. From a users' perspective, we submit one set of predictions, they should not be treated differently for each of the public\/private subsets.\n\nThis has so far had a fairly small impact; competitions with top scores under 0.75 cannot be affected, but those with higher scores may be, and occasionally this has lead to large shake-up for individual teams (albeit in the bottom half of the LB).\n\nIf a competition with a score ceiling of 0.76 is launched it could cause a lot more confusion :)\n\nIt's most noticeable on the most recent AUC competition [IEEE-CIS Fraud Detection][1].\n\nKudos to Kaggle for making the raw data available to see this in such detail :)\n\n### Color Scheme\n - gray: not used for either leaderboard\n - blue: used as public LB entry for team\n - red: used as private LB entry for team\n - purple: used as public and private LB entry for team\n \n### Suggestions to Kaggle:\n - Invert both scores or neither.\n - Always inform the user whether the score is true or inverted.\n - Invert at 0.5 (some libraries do this).\n\nChanging the threshold to 0.5 would potentially propel some teams from just over 0.25 to just under 0.75, slightly affecting ranking & points. Some stats are listed per competition to see the effect.\n\n### Alternatively:\n - Do not invert at all, reject submissions with scores under 0.5\n - Do not invert at all, but warn participants to look at the bottom of the LB.\n - Do not invert scores, but add 'ghost' LB entries for submissions with low AUC, showing where they appear if inverted.\n \nAre there other possibilities? Leave your comments below :)\n\n [1]: https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\n [2]: https:\/\/www.kaggle.com\/c\/santander-customer-satisfaction\/discussion\/19323\n ","66d786b8":"# Histogram of Small Distances to (0.25,0.75) and (0.75,0.25)","7e5ed851":"# Histogram of Distances to (0.25,0.75) and (0.75,0.25)"}}