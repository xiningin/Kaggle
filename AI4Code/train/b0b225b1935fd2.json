{"cell_type":{"972d2189":"code","88ddbddd":"code","6137c478":"code","b3ef853c":"code","d80c83bb":"code","67dd19bd":"code","eaafd65c":"code","2f393b9e":"code","72efe1af":"code","aa1f44a3":"code","1c675e19":"code","51328bcc":"code","27f55660":"code","2bb4a531":"code","beaea376":"code","2814dce4":"code","9fac9999":"code","4780b801":"code","0faf1a77":"code","83ad6b71":"code","ec323196":"code","98d70b0b":"code","ee4c1332":"code","96dd2ef6":"code","bbe9d32b":"code","7224bfc6":"code","923a4b10":"code","3f0981af":"code","5b9175ad":"code","81be7a11":"code","a7c9c596":"code","eceefe36":"code","a24d792b":"code","93cd1b50":"code","cbe50bba":"code","a3f68124":"code","7ccf295e":"code","3d7a7e95":"code","b0af0c3a":"code","c5f2560e":"code","36eb2630":"code","681906ca":"code","8bb04fee":"code","17b6cb58":"code","c03ccf72":"code","85a86167":"code","bd119c93":"code","0f0d9a93":"code","61db395d":"code","f7c0a06d":"code","123fdd7a":"code","4108ec64":"code","976b11c1":"code","8c3e7b5a":"code","c043338f":"code","c686f845":"code","31f7ace7":"code","9c84604d":"code","f64d6474":"code","de4fe091":"markdown","4f97cb7c":"markdown","8c4e346f":"markdown","5dd30a33":"markdown"},"source":{"972d2189":"import torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport csv\nimport random\nimport re\nimport os\nimport unicodedata\nimport codecs\nimport itertools","88ddbddd":"USE_CUDA = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if USE_CUDA else \"cpu\")","6137c478":"USE_CUDA","b3ef853c":"device","d80c83bb":"lines_filepath=os.path.join(\"..\/input\/movie_lines.txt\")\nconv_filepath=os.path.join(\"..\/input\/movie_conversations.txt\")","67dd19bd":"#visuilize some lines\nwith open (lines_filepath,'rb') as file:\n    lines = file.readlines()\nfor line in lines[:8]:\n    print(line)\n    print(line.strip())\n    ","eaafd65c":"# Splits each line of the file into a dictionary of fields [lineID,chartcterID,movi ID,charcter, text]\nline_fields=[\"lineID\",\"chartcterID\",\"movieID\",\"charcter\",\"text\"]\nlines = {}\nwith open(lines_filepath, 'r', encoding='iso-8859-1') as f:\n    for line in f:\n        values = line.split(\" +++$+++ \")\n            # Extract fields\n        lineObj = {}\n        for i, field in enumerate(line_fields):\n            lineObj[field] = values[i]\n        lines[lineObj['lineID']] = lineObj\n            \n        ","2f393b9e":"lines","72efe1af":"list(lines.items())[0]","aa1f44a3":"lines['L194']","1c675e19":"# Groups fields of lines from `loadLines` into conversations based on *movie_conversations.txt*\nconv_fields=[\"charcter1ID\",\"charcter2ID\",\"movieID\",\"utteranceIDs\"]\nconversations = []\n   \nwith open(conv_filepath, 'r', encoding='iso-8859-1') as f:\n    for line in f:\n        values = line.split(\" +++$+++ \")\n        # Extract fields\n        convObj = {}\n        for i, field in enumerate(conv_fields):\n            \n            convObj[field] = values[i]\n        # Convert string to list (convObj[\"utteranceIDs\"] == \"['L598485', 'L598486', ...]\")\n        lineIds = eval(convObj[\"utteranceIDs\"])\n        # Reassemble lines\n        convObj[\"lines\"] = []\n        for lineId in lineIds:\n            convObj[\"lines\"].append(lines[lineId])\n        conversations.append(convObj)","51328bcc":"conversations[0]","27f55660":"len(conversations[0][\"lines\"])","2bb4a531":"conversations[0][\"lines\"]","beaea376":"conversations[0][\"lines\"][0]","2814dce4":"conversations[0][\"lines\"][0][\"text\"].strip()","9fac9999":"# Extracts pairs of sentences from conversations\n\nqa_pairs = []\nfor conversation in conversations:\n    # Iterate over all the lines of the conversation\n    for i in range(len(conversation[\"lines\"]) - 1):  # We ignore the last line (no answer for it)\n        inputLine = conversation[\"lines\"][i][\"text\"].strip()\n        targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n        # Filter wrong samples (if one of the lists is empty)\n        if inputLine and targetLine:\n            qa_pairs.append([inputLine, targetLine])","4780b801":"qa_pairs","0faf1a77":"len(qa_pairs)","83ad6b71":"# Define path to new file\ndatafile = os.path.join(\"formatted_movie_lines.txt\")\ndelimiter = '\\t'\n# Unescape the delimiter\ndelimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n\n# Write new csv file\nprint(\"\\nWriting newly formatted file...\")\nwith open(datafile, 'w', encoding='utf-8') as outputfile:\n    writer = csv.writer(outputfile, delimiter=delimiter)\n    for pair in qa_pairs :\n        writer.writerow(pair)\nprint(\"Done writing to file\")","ec323196":"#visulize some lines\ndatafile = os.path.join(\"formatted_movie_lines.txt\")\nwith open(datafile,'rb') as file:\n    lines=file.readlines()\nfor line in lines[:8]:\n    print(line)","98d70b0b":"# Default word tokens\nPAD_token = 0  # Used for padding short sentences\nSOS_token = 1  # Start-of-sentence token\nEOS_token = 2  # End-of-sentence token\n\nclass Vocabulary:\n    def __init__(self, name):\n        self.name = name\n        self.trimmed = False\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n        self.num_words = 3  # Count SOS, EOS, PAD\n\n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.num_words\n            self.word2count[word] = 1\n            self.index2word[self.num_words] = word\n            self.num_words += 1\n        else:\n            self.word2count[word] += 1\n\n    # Remove words below a certain count threshold\n    def trim(self, min_count):\n        if self.trimmed:\n            return\n        self.trimmed = True\n\n        keep_words = []\n\n        for k, v in self.word2count.items():\n            if v >= min_count:\n                keep_words.append(k)\n\n        print('keep_words {} \/ {} = {:.4f}'.format(\n            len(keep_words), len(self.word2index), len(keep_words) \/ len(self.word2index)\n        ))\n\n        # Reinitialize dictionaries\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n        self.num_words = 3 # Count default tokens\n\n        for word in keep_words:\n            self.addWord(word)","ee4c1332":"#turn aunicode string to plain AscII\ndef unicodeToAscii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s)if unicodedata.category(c) != 'Mn')","96dd2ef6":"#Test the function \nunicodeToAscii(\"Montr\u00e9al\")","bbe9d32b":"#Processing the text\n# Lowercase, trim, and remove non-letter characters\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    #replace any !? \n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    #remove\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    s = re.sub(r\"\\s+\", r\" \", s).strip()\n    return s","7224bfc6":"#test normlizing\nnormalizeString(\"aya!!@ 346\")","923a4b10":"# Read query\/response pairs and return a voc object\ndatafile = os.path.join(\"formatted_movie_lines.txt\")\nprint(\"Reading the process lines .. please wait\")\n# Read the file and split into lines\nlines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n# Split every line into pairs and normalize\npairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\nprint(\"Done Reading\")\nvoc =Vocabulary(\"CornellMovie-Dialog corpus\")\n","3f0981af":"lines[0]","5b9175ad":"len(pairs[0][1].split())<10","81be7a11":"len(pairs)","a7c9c596":"len(pairs[1])","eceefe36":"MAX_LENGTH = 10  # Maximum sentence length to consider\n\n# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\ndef filterPair(p):\n    # Input sequences need to preserve the last word for EOS token\n    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n\n# Filter pairs using filterPair condition\ndef filterPairs(pairs):\n    return [pair for pair in pairs if filterPair(pair)]\n","a24d792b":"pairs= [pair for pair in pairs if len(pair)>1]\nprint(\"there are {} pairs\/conversations in the dataset\".format(len(pairs)))\npairs= filterPairs(pairs)\nprint(\"After filtering there are {} pairs\/conversations=\".format(len(pairs)))","93cd1b50":"#Load throth each pair of and add the question and reply sentence to vocabulary\nfor pair in pairs:\n    voc.addSentence(pair[0])\n    voc.addSentence(pair[1])\nprint(\"Counted words:\", voc.num_words)\nfor pair in pairs[:10]:\n    print(pair)\n","cbe50bba":"MIN_COUNT = 3    # Minimum word count threshold for trimming\n\ndef trimRareWords(voc, pairs, MIN_COUNT):\n    # Trim words used under the MIN_COUNT from the voc\n    voc.trim(MIN_COUNT)\n    # Filter out pairs with trimmed words\n    keep_pairs = []\n    for pair in pairs:\n        input_sentence = pair[0]\n        output_sentence = pair[1]\n        keep_input = True\n        keep_output = True\n        # Check input sentence\n        for word in input_sentence.split(' '):\n            if word not in voc.word2index:\n                keep_input = False\n                break\n        # Check output sentence\n        for word in output_sentence.split(' '):\n            if word not in voc.word2index:\n                keep_output = False\n                break\n\n        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n        if keep_input and keep_output:\n            keep_pairs.append(pair)\n\n    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) \/ len(pairs)))\n    return keep_pairs\n\n\n# Trim voc and pairs\npairs = trimRareWords(voc, pairs, MIN_COUNT)","a3f68124":"def indexesFromSentence(voc, sentence):\n    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]","7ccf295e":"pairs[1][0]","3d7a7e95":"#Test the function\nindexesFromSentence(voc, pairs[1][0])","b0af0c3a":"#Define some samples for testing\ninp=[]\nout=[]\nfor pair in pairs[:10]:\n    inp.append(pair[0])\n    out.append(pair[1])\nprint(inp)\nprint(len(inp))\nindexes=[indexesFromSentence(voc, sentence) for sentence in inp]\nindexes","c5f2560e":"def zeroPadding(l, fillvalue=PAD_token):\n    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n","36eb2630":"leng=[len(inp) for inp in indexes]\nmax(leng)","681906ca":"#Test the function\ntest_result=zeroPadding(indexes)\nprint(len(test_result))\ntest_result","8bb04fee":"#convert to binary\ndef binaryMatrix(l, value=PAD_token):\n    m = []\n    for i, seq in enumerate(l):\n        m.append([])\n        for token in seq:\n            if token == PAD_token:\n                m[i].append(0)\n            else:\n                m[i].append(1)\n    return m","17b6cb58":"#test\nbinary_result=binaryMatrix(test_result)\nbinary_result","c03ccf72":"# Returns padded input sequence tensor and lengths\ndef inputVar(l, voc):\n    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n    padList = zeroPadding(indexes_batch)\n    padVar = torch.LongTensor(padList)\n    return padVar, lengths\n","85a86167":"# Returns padded target sequence tensor, padding mask, and max target length\ndef outputVar(l, voc):\n    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n    max_target_len = max([len(indexes) for indexes in indexes_batch])\n    padList = zeroPadding(indexes_batch)\n    mask = binaryMatrix(padList)\n    mask = torch.ByteTensor(mask)\n    padVar = torch.LongTensor(padList)\n    return padVar, mask, max_target_len","bd119c93":"# Returns all items for a given batch of pairs\ndef batch2TrainData(voc, pair_batch):\n    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n    input_batch, output_batch = [], []\n    for pair in pair_batch:\n        input_batch.append(pair[0])\n        output_batch.append(pair[1])\n    inp, lengths = inputVar(input_batch, voc)\n    output, mask, max_target_len = outputVar(output_batch, voc)\n    return inp, lengths, output, mask, max_target_len\n\n","0f0d9a93":"# Example for validation\nsmall_batch_size = 5\nbatches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\ninput_variable, lengths, target_variable, mask, max_target_len = batches\n\nprint(\"input_variable:\", input_variable)\nprint(\"lengths:\", lengths)\nprint(\"target_variable:\", target_variable)\nprint(\"mask:\", mask)\nprint(\"max_target_len:\", max_target_len)","61db395d":"class EncoderRNN(nn.Module):\n    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n        super(EncoderRNN, self).__init__()\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n        self.embedding = embedding\n\n        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n        #   because our input size is a word embedding with number of features == hidden_size\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n\n    def forward(self, input_seq, input_lengths, hidden=None):\n        # Convert word indexes to embeddings\n        embedded = self.embedding(input_seq)\n        # Pack padded batch of sequences for RNN module\n        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n        # Forward pass through GRU\n        outputs, hidden = self.gru(packed, hidden)\n        # Unpack padding\n        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n        # Sum bidirectional GRU outputs\n        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n        # Return output and final hidden state\n        return outputs, hidden","f7c0a06d":"# Luong attention layer\nclass Attn(torch.nn.Module):\n    def __init__(self, method, hidden_size):\n        super(Attn, self).__init__()\n        self.method = method\n        if self.method not in ['dot', 'general', 'concat']:\n            raise ValueError(self.method, \"is not an appropriate attention method.\")\n        self.hidden_size = hidden_size\n        if self.method == 'general':\n            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n        elif self.method == 'concat':\n            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n\n    def dot_score(self, hidden, encoder_output):\n        return torch.sum(hidden * encoder_output, dim=2)\n\n    def general_score(self, hidden, encoder_output):\n        energy = self.attn(encoder_output)\n        return torch.sum(hidden * energy, dim=2)\n\n    def concat_score(self, hidden, encoder_output):\n        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n        return torch.sum(self.v * energy, dim=2)\n\n    def forward(self, hidden, encoder_outputs):\n        # Calculate the attention weights (energies) based on the given method\n        if self.method == 'general':\n            attn_energies = self.general_score(hidden, encoder_outputs)\n        elif self.method == 'concat':\n            attn_energies = self.concat_score(hidden, encoder_outputs)\n        elif self.method == 'dot':\n            attn_energies = self.dot_score(hidden, encoder_outputs)\n\n        # Transpose max_length and batch_size dimensions\n        attn_energies = attn_energies.t()\n\n        # Return the softmax normalized probability scores (with added dimension)\n        return F.softmax(attn_energies, dim=1).unsqueeze(1)","123fdd7a":"import torch.nn.functional as F\na=torch.rand(5,7)\na","4108ec64":"b=F.softmax(a,dim=1)\nb","976b11c1":"b[0].sum()","8c3e7b5a":"class LuongAttnDecoderRNN(nn.Module):\n    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n        super(LuongAttnDecoderRNN, self).__init__()\n\n        # Keep for reference\n        self.attn_model = attn_model\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n\n        # Define layers\n        self.embedding = embedding\n        self.embedding_dropout = nn.Dropout(dropout)\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n\n        self.attn = Attn(attn_model, hidden_size)\n\n    def forward(self, input_step, last_hidden, encoder_outputs):\n        # Note: we run this one step (word) at a time\n        # Get embedding of current input word\n        embedded = self.embedding(input_step)\n        embedded = self.embedding_dropout(embedded)\n        # Forward through unidirectional GRU\n        rnn_output, hidden = self.gru(embedded, last_hidden)\n        # Calculate attention weights from the current GRU output\n        attn_weights = self.attn(rnn_output, encoder_outputs)\n        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n        # Concatenate weighted context vector and GRU output using Luong eq. 5\n        rnn_output = rnn_output.squeeze(0)\n        context = context.squeeze(1)\n        concat_input = torch.cat((rnn_output, context), 1)\n        concat_output = torch.tanh(self.concat(concat_input))\n        # Predict next word using Luong eq. 6\n        output = self.out(concat_output)\n        output = F.softmax(output, dim=1)\n        # Return output and final hidden state\n        return output, hidden","c043338f":"def maskNLLLoss(decoder_out, target, mask):\n    nTotal = mask.sum() #how many element should be consider\n    target= target.view(-1, 1)\n    #decode_out shape: (batch_size,vocab_size),target_size=(batch_size,1)\n    gathered_tensor=torch.gather(decoder_out, 1, target)\n    # calculate the negative  log likelihood loss\n    crossEntropy = -torch.log(gathered_tensor)\n    #select the non_zero elements\n    loss = crossEntropy.masked_select(mask)\n    #calculate the mean of loss\n    loss=loss.mean()\n    loss = loss.to(device)\n    return loss, nTotal.item()","c686f845":"#visulizeing what's happenning in one iteration , only run this for visulizing\nsmall_batch_size = 5\nbatches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\ninput_variable, lengths, target_variable, mask, max_target_len = batches\n\nprint(\"input_variable:\", input_variable.shape)\nprint(\"lengths:\", lengths.shape)\nprint(\"target_variable:\", target_variable.shape)\nprint(\"mask:\", mask.shape)\nprint(\"max_target_len:\", max_target_len)\n\n# define the parameters\nattn_model = 'dot'\nhidden_size = 500\nencoder_n_layers = 2\ndecoder_n_layers = 2\ndropout = 0.1\nembedding = nn.Embedding(voc.num_words, hidden_size)\n\nprint('Building encoder and decoder ...')\n#Define the encoder and decoder \nencoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\ndecoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n# Use appropriate device\nencoder = encoder.to(device)\ndecoder = decoder.to(device)\nprint('Models built and ready to go!')\n# Ensure dropout layers are in train mode\nencoder.train()\ndecoder.train()\n\n# Initialize optimizers\nprint('Building optimizers ...')\nencoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0001)\ndecoder_optimizer = optim.Adam(decoder.parameters(), lr=0.0001)\nencoder_optimizer.zero_grad()\ndecoder_optimizer.zero_grad()\n\n# Set device options\ninput_variable = input_variable.to(device)\nlengths = lengths.to(device)\ntarget_variable = target_variable.to(device)\nmask = mask.to(device)\n\n# Initialize variables\nloss = 0\nprint_losses = []\nn_totals = 0\n\n# Forward pass through encoder\nencoder_outputs, encoder_hidden = encoder(input_variable, lengths)\nprint(\"encoder_outputs shape\",encoder_outputs.shape)\nprint(\"last encoder_hidden shape\",encoder_hidden.shape)\n\n # Create initial decoder input (start with SOS tokens for each sentence)\ndecoder_input = torch.LongTensor([[SOS_token for _ in range(small_batch_size)]])\ndecoder_input = decoder_input.to(device)\nprint(\"initilize decoder_input shape\",decoder_input.shape)\nprint(decoder_input)\n\n# Set initial decoder hidden state to the encoder's final hidden state\ndecoder_hidden = encoder_hidden[:decoder.n_layers]\nprint(\"initial decoder hidden state shape\",decoder_hidden.shape)\nprint(\"\\n\")\nprint(\"---------------------------------------------------------\")\nprint(\"Now let's look what's happinig in every timestep of the GPU!\")\nprint(\"---------------------------------------------------------\")\nprint(\"\\n\")\n\n\nfor t in range(max_target_len):\n        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n        print(\"decoder_output shape\",decoder_output.shape)\n        print(\"encoder_hidden shape\",encoder_hidden.shape)\n        # Teacher forcing: next input is current target\n        decoder_input = target_variable[t].view(1, -1)\n        print(\"The target variable at the current timestep before reshaping\",target_variable[t])\n        print(\"The target variable at the current timestep shape before reshaping\",target_variable[t].shape)\n        print(\"The decoder_input shape(reshape target_variable)\",decoder_input.shape)\n        # Calculate and accumulate loss\n        print(\"The mask at the current timestep : \",mask[t] )\n        print(\"The mask at the current timestep shape: \",mask[t].shape )\n        mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n        print(\"Mask loss \",mask_loss)\n        print(\"Total \",nTotal)\n        loss += mask_loss\n        print_losses.append(mask_loss.item() * nTotal)\n        print(print_losses)\n        n_totals += nTotal\n        print(n_totals)\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n        returned_loss=sum(print_losses)\/ n_totals\n        print(\"returned_loss : \",returned_loss)\n        print(\"\\n\")\n        print(\"------------------- Done one timestep ----------------------------\")\n        print(\"\\n\")","31f7ace7":"random.random()","9c84604d":"def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n\n    # Zero gradients\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    # Set device options\n    input_variable = input_variable.to(device)\n    lengths = lengths.to(device)\n    target_variable = target_variable.to(device)\n    mask = mask.to(device)\n\n    # Initialize variables\n    loss = 0\n    print_losses = []\n    n_totals = 0\n\n    # Forward pass through encoder\n    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n\n    # Create initial decoder input (start with SOS tokens for each sentence)\n    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n    decoder_input = decoder_input.to(device)\n\n    # Set initial decoder hidden state to the encoder's final hidden state\n    decoder_hidden = encoder_hidden[:decoder.n_layers]\n\n    # Determine if we are using teacher forcing this iteration\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    # Forward batch of sequences through decoder one time step at a time\n    if use_teacher_forcing:\n        for t in range(max_target_len):\n            decoder_output, decoder_hidden = decoder(\n                decoder_input, decoder_hidden, encoder_outputs\n            )\n            # Teacher forcing: next input is current target\n            decoder_input = target_variable[t].view(1, -1)\n            # Calculate and accumulate loss\n            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n            loss += mask_loss\n            print_losses.append(mask_loss.item() * nTotal)\n            n_totals += nTotal\n    else:\n        for t in range(max_target_len):\n            decoder_output, decoder_hidden = decoder(\n                decoder_input, decoder_hidden, encoder_outputs\n            )\n            # No teacher forcing: next input is decoder's own current output\n            _, topi = decoder_output.topk(1)\n            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n            decoder_input = decoder_input.to(device)\n            # Calculate and accumulate loss\n            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n            loss += mask_loss\n            print_losses.append(mask_loss.item() * nTotal)\n            n_totals += nTotal\n\n    # Perform backpropatation\n    loss.backward()\n\n    # Clip gradients: gradients are modified in place\n    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n\n    # Adjust model weights\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return sum(print_losses) \/ n_totals","f64d6474":"def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n\n    # Load batches for each iteration\n    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n                      for _ in range(n_iteration)]\n\n    # Initializations\n    print('Initializing ...')\n    start_iteration = 1\n    print_loss = 0\n    if loadFilename:\n        start_iteration = checkpoint['iteration'] + 1\n\n    # Training loop\n    print(\"Training...\")\n    for iteration in range(start_iteration, n_iteration + 1):\n        training_batch = training_batches[iteration - 1]\n        # Extract fields from batch\n        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n\n        # Run a training iteration with batch\n        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n        print_loss += loss\n\n        # Print progress\n        if iteration % print_every == 0:\n            print_loss_avg = print_loss \/ print_every\n            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration \/ n_iteration * 100, print_loss_avg))\n            print_loss = 0\n\n        # Save checkpoint\n        if (iteration % save_every == 0):\n            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n            if not os.path.exists(directory):\n                os.makedirs(directory)\n            torch.save({\n                'iteration': iteration,\n                'en': encoder.state_dict(),\n                'de': decoder.state_dict(),\n                'en_opt': encoder_optimizer.state_dict(),\n                'de_opt': decoder_optimizer.state_dict(),\n                'loss': loss,\n                'voc_dict': voc.__dict__,\n                'embedding': embedding.state_dict()\n            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))","de4fe091":"We are done building the archticer let's trainig it","4f97cb7c":"**2- Prepare Data for Models\n**","8c4e346f":"# 1- Data Preprocessing ","5dd30a33":"**Define Models\n**"}}