{"cell_type":{"afcd97ae":"code","22477b54":"code","9dd2e891":"code","351fdfa3":"code","5f73e949":"code","16fa675a":"code","8e935395":"code","cf1d798a":"code","62375263":"code","f4668efc":"code","14646463":"code","17360e68":"code","0b93e89d":"code","146092c3":"code","7243fb3c":"code","35638549":"code","aea7847e":"code","99321f8c":"code","60ecfbf9":"code","9b1f2e32":"code","6c8146c3":"markdown","ea4b4d47":"markdown","321ab50a":"markdown","8f56b80e":"markdown","c59cc61b":"markdown","c85541ab":"markdown","a7ee8b0f":"markdown","38480d9b":"markdown","a289fab5":"markdown","b2350417":"markdown","63c20997":"markdown","8e4f2bd1":"markdown","447a9efc":"markdown","4f95ac1a":"markdown","28076027":"markdown","9ae2f7fa":"markdown","0b3a0448":"markdown","82931c57":"markdown","3e7fc979":"markdown","c345c103":"markdown","d2eefcdd":"markdown"},"source":{"afcd97ae":"!pip install seaborn --upgrade","22477b54":"from scipy import stats","9dd2e891":"import numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nsample_sizes = [10, 50, 200, 1000]\nf,((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2, figsize=(12,12))\naxs = [ax1,ax2,ax3,ax4]\n\nprint('Population theoric mean=0.0 std=1')\n\nfor sample_size,ax in zip(sample_sizes, axs):\n    x = [np.random.normal() for _ in range(sample_size)]\n    sns.histplot(x, kde=True, ax=ax)\n    mean = np.mean(x)\n    std = np.std(x)\n    ax.axvline(mean, color='blue', label='mean')\n    ax.axvline(std, color='red', label='\u00b1std')\n    ax.axvline(-std, color='red')\n    ax.set_title(f'sample_size={sample_size}')\n    ax.legend()\n    print(f'Sample_size={sample_size} sample_mean={mean} sample_std={std}')\n","351fdfa3":"from scipy.stats import norm\nimport math\nfrom matplotlib import pyplot as plt\n\nmu = 0\nsigma_sq = 1\nsigma = math.sqrt(sigma_sq)\n\nx = np.arange(-4, +4, 0.001)\nplt.subplots(figsize=(14,6))\ny = norm.pdf(x)\nsns.lineplot(x=x, y=y)\n\nppf95 = norm.ppf(.954)\nprint(f'95.4% region with mean plus minus 2*sigma [{mu-2*sigma}, {mu+2*sigma}]')\n\nleft_tail_prob = (1-0.954)\/2\nright_tail_prob = (1-0.954)\/2 # same as left_tail_prob (both tails hold the same total probability under them)\nright_p = 1- right_tail_prob\nprint(f'95.4% region with ppfs [{norm.ppf(left_tail_prob)},{norm.ppf(right_p)}]')\n\nprint('\\nUsing ppf and isf to obtain the right side of the interval')\nprint(f'{norm.ppf(right_p)} vs vs {norm.isf(right_tail_prob)}')\n\nprint(f'probability of lower or equal to 1.645 {norm.cdf(1.645)}%, probability of a greater value {norm.sf(1.645)}%')\n\nplt.axvline(mu-sigma, color=\"red\", label=r'$\\mu \\pm \\sigma$ (68.2%)')\nplt.axvline(mu+sigma, color=\"red\")\nplt.axvline(mu-2*sigma, color=\"orange\", label=r'$\\mu \\pm 2\\sigma$ (95.4%)')\nplt.axvline(mu+2*sigma, color=\"orange\")\nplt.axvline(mu-3*sigma, color=\"yellow\", label=r'$\\mu \\pm 3\\sigma$ (99.7%)')\nplt.axvline(mu+3*sigma, color=\"yellow\")\nplt.ylim(0,0.5)\nplt.legend()","5f73e949":"from scipy.stats import t\n\nplt.subplots(figsize=(14,6))\n\n#pdf is defined in the \u201cstandardized\u201d form to shift\/scale the distribution use the loc\/scale parameters\nsns.lineplot(x=x, y=t.pdf(x, df=1), label=r'$\\nu=1$')\nsns.lineplot(x=x, y=t.pdf(x, df=10), label=r'$\\nu=10$')\nsns.lineplot(x=x, y=t.pdf(x, df=100), label=r'$\\nu=100$')\nplt.ylim(0,0.5)","16fa675a":"from scipy.stats import bernoulli\n\np=.2\nx = [0,1]\nsns.barplot(x=x, y=bernoulli.pmf(x, p), label=f'p={p}')","8e935395":"from scipy.stats import beta\n\nparameters=[[(1,3), (1,2), (1,1),(2,2), (2,1),(3,1)],\n            [(2,10), (5,10), (3,3),(10,10), (10,2),(10,5)]]\nx = np.arange(0, 1, 0.001)\nf, rows = plt.subplots(2,6, figsize=(20,6))\nfor ps, row in zip(parameters,rows):\n    for  p, ax in zip(ps, row):\n        sns.lineplot(x=x, y=beta.pdf(x, p[0], p[1]), ax=ax, label=rf'$\\alpha={p[0]}, \\beta={p[1]}$')","cf1d798a":"means = []\nstd_devs = []\nn_samples = 1000\nsample_size = 100\n\nfor _ in range(n_samples):\n    x = [np.random.normal() for _ in range(sample_size)]\n    means.append(np.mean(x))\n    std_devs.append(np.std(x))\n\nf, (ax1, ax2) = plt.subplots(1,2)\nsns.histplot(means, ax=ax1)\nax1.axvline(np.mean(means), color='blue')\nax1.set_title('Mean of sample means')\n\nsns.histplot(std_devs, ax=ax2)\nax2.axvline(np.mean(std_devs), color='red')\nax2.set_title('Mean of sample std.devs')","62375263":"from numpy.random import multivariate_normal\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nmean = [0,0]\ncorr0 = [[1, 0],[0, 1]]\ncorr_pos = [[1, 1],[1, 1]]\ncorr_pos07 = [[1, .7],[.7, 1]]\ncorr_neg = [[1, -1],[-1, 1]]\ncorr_neg07 = [[1, -.7],[-.7, 1]]\n\nx_0, y_0 = multivariate_normal(mean, corr0, 100).T\nx_pos, y_pos  = multivariate_normal(mean, corr_pos, 100).T\nx_pos07, y_pos07  = multivariate_normal(mean, corr_pos07, 100).T\nx_neg, y_neg = multivariate_normal(mean, corr_neg, 100).T\nx_neg07, y_neg07 = multivariate_normal(mean, corr_neg07, 100).T\n\nf, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5, figsize=(20,6))\nsns.scatterplot(x=x_neg, y=y_neg, ax=ax1)\nsns.scatterplot(x=x_neg07, y=y_neg07, ax=ax2)\nsns.scatterplot(x=x_0, y=y_0, ax=ax3)\nsns.scatterplot(x=x_pos07, y=y_pos07, ax=ax4)\nsns.scatterplot(x=x_pos, y=y_pos, ax=ax5)\nax1.set_title(\"Correlation -1\")\nax2.set_title(\"Correlation -.7\")\nax3.set_title(\"Correlation 0\")\nax4.set_title(\"Correlation .7\")\nax5.set_title(\"Correlation 1\")","f4668efc":"#TODO!\n#....","14646463":"import random\nx_random = [random.random() for _ in range(0,100)]\nx_norm = [random.normalvariate(0,1) for _ in range(0,100)]\n\nprint(\"*** Results for random data ***\")\n#(statisticfloat, p-value)\nshapiro = stats.shapiro(x_random)\n\nks = stats.kstest(x_random, cdf='norm')\nanderson = stats.anderson(x_random, dist='norm')\nprint(f'Shapiro: {shapiro}\\nKolmogorov-Smirnov: {ks}\\nAnderson: {anderson}\\n')\nprint(f'Shapiro p-value: {shapiro[1]} (>.05 {shapiro[1]>.05}!)\\nKolmogorov-Smirnov p-value: {ks.pvalue} (>.05 {ks.pvalue>.05}!)\\nAnderson result for 5%: {anderson.statistic<anderson.critical_values[2]}')\n\nprint(\"\\n*** Results for normal data ***\")\nshapiro = stats.shapiro(x_norm)\nks = stats.kstest(x_norm, cdf='norm')\nanderson = stats.anderson(x_norm, dist='norm')\nprint(f'Shapiro: {shapiro}\\nKolmogorov-Smirnov: {ks}\\nAnderson: {anderson}\\n')\nprint(f'Shapiro p-value: {shapiro[1]} (>.05 {shapiro[1]>.05}!)\\nKolmogorov-Smirnov p-value: {ks.pvalue} (>.05 {ks.pvalue>.05}!)\\nAnderson result for 5%: {anderson.statistic<anderson.critical_values[2]}')\n","17360e68":"x_norm0 = [random.normalvariate(0,1) for _ in range(0,100)]\nx_norm1 = [random.normalvariate(1,1) for _ in range(0,100)]\n\nshapiro0 = stats.shapiro(x_norm0)\nks0 = stats.kstest(x_norm0, cdf='norm')\nanderson0 = stats.anderson(x_norm0, dist='norm')\n\nshapiro1 = stats.shapiro(x_norm1)\nks1 = stats.kstest(x_norm1, cdf='norm')\nanderson1 = stats.anderson(x_norm1, dist='norm')\n\nprint('Results at 5% for data sampled from N(0,1) vs N(1,1)\\n*******************************************************')\nprint(f'Shapiro: {shapiro0[1]>.05} vs {shapiro1[1]>.05}')\nprint(f'Kolmogorov-Smirnov: {ks0.pvalue>.05} vs {ks1.pvalue>.05}')\nprint(f'Anderson: {anderson0.statistic<anderson.critical_values[2]} vs {anderson1.statistic<anderson.critical_values[2]}')","0b93e89d":"from matplotlib import pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\n\nsm.qqplot(np.array(x_norm0), line='45')\nplt.show()\nsm.qqplot(np.array(x_norm1), line='45')\nplt.show()","146092c3":"x_norm0 = [random.normalvariate(0,1) for _ in range(0,100)]\nx_norm1 = [random.normalvariate(1,1) for _ in range(0,100)]\nx_norm2 = [random.normalvariate(2,1) for _ in range(0,100)]\n\n#different variance\nx_norm3 = [random.normalvariate(0,6) for _ in range(0,100)]\n\nlevene = stats.levene(x_norm0, x_norm1, x_norm2)\nbartlett = stats.bartlett(x_norm0, x_norm1, x_norm2)\nfligner = stats.fligner(x_norm0, x_norm1, x_norm2)\n\nlevene2 = stats.levene(x_norm0, x_norm3)\nbartlett2 = stats.bartlett(x_norm0, x_norm3)\nfligner2 = stats.fligner(x_norm0, x_norm3)\n\n\nprint(f'Results for equal variance samples\\n{levene}\\n{bartlett}\\n{fligner}\\n')\nprint(f'Results for different variance samples\\n{levene2}\\n{bartlett2}\\n{fligner2}')","7243fb3c":"from scipy.stats import f_oneway\n\nx_norm0 = [random.normalvariate(0,1) for _ in range(0,200)]\nx_norm1 = [random.normalvariate(1,1) for _ in range(0,200)]\nx_norm2 = [random.normalvariate(2,1) for _ in range(0,200)]\n\nshapiro0 = stats.shapiro(x_norm0)\nshapiro1 = stats.shapiro(x_norm1)\nshapiro2 = stats.shapiro(x_norm2)\nlevene = stats.levene(x_norm0, x_norm1, x_norm2)\n\nprint(\"\\n\".join(map(str,[shapiro0, shapiro1, shapiro2, levene])))\n\nanova = f_oneway(x_norm0, x_norm1, x_norm2)\n\nprint(anova)\nprint(f'Different Means? {anova.pvalue<0.05}')","35638549":"from statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\n\nfrom sklearn import datasets\nimport pandas as pd\nfrom pandas import DataFrame\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\niris = datasets.load_iris()\ncolumn_names = ['sepal_l', 'sepal_w', 'petal_l', 'petal_w']\ndf = DataFrame(data=iris.data, columns=column_names)\ndf['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\naux = df.groupby([df['species']])\nfor c in column_names:\n    print(\"\\n****** Shapiro test\",c)\n    aux_df = aux[c].apply(lambda x: pd.Series(stats.shapiro(x), index=[\"statistic\",\"p-value\"]))\n    print(aux_df)\n    print(\"\\n***** \",\n          stats.levene(\n              df[df['species']=='setosa'][c],\n              df[df['species']=='versicolor'][c],\n              df[df['species']=='virginica'][c]))\n","aea7847e":"import seaborn as sns\n\nformula = \"sepal_l ~ C(species)\"\nmodel = ols(formula, df).fit()\nprint(\"***** Some OLS internals *****\\n\")\nprint(\"OLF summary\\n\", model.summary())\nprint(\"OLF influente summary\\n\", model.get_influence().summary_frame())\nprint(\"OLF residuals\\n\", model.resid)\n\nplt.scatter(df['species'], model.resid.array)\nsns.boxplot(x=df['species'], y=model.resid)\nplt.title(\"Residuals for OLS [\"+formula+\"]\")\nplt.show()","99321f8c":"x = DataFrame(data=[['setosa'],['versicolor'],['virginica']], columns=['species'])\ny = model.predict(x)\nf, ax = plt.subplots()\nplt.scatter(x['species'],y, color=\"g\", marker=\"s\")\nplt.plot(y, 'b--')\nplt.title(\"OLS [\"+formula+\"]\")\n#ax.set_xticks([0,1,2])\n#ax.set_xticklabels(['setosa','versicolor','virginica'])\nplt.show()\n\nprint(\"OLS predictions for data [\"+formula+\"]\")\nprint(model.predict())","60ecfbf9":"aov_table = anova_lm(model, typ=1)\n\nprint(f'\\n\\nAOV for [{formula}]:')\nprint(aov_table)","9b1f2e32":"from statsmodels.stats.multicomp import pairwise_tukeyhsd\n\nformula1 = \"sepal_l ~ C(species)\"\nformula2 = \"sepal_w ~ C(species)\"\n\nprint(\"**** ANOVA ****\")\nfor f in [formula1, formula2]:\n    model = ols(f, df).fit()\n    aov_table = anova_lm(model, typ=1)\n    print(f'\\nAOV for [{f}]:')\n    print(aov_table)\n\nprint(\"\\n**** Tukey ****\\n\")\ntukey1 = pairwise_tukeyhsd(endog=df['sepal_l'], groups=df['species'])\ntukey2 = pairwise_tukeyhsd(endog=df['sepal_w'], groups=df['species'])\n\nprint(\"sepal_length\\n\", tukey1,\"\\n\")\nprint(\"sepal_width\\n\", tukey2,\"\\n\")","6c8146c3":"Test results show:\n* petal_l doesn't pass the Levene test\n* petal_w fails both Shapiro and Levene tests","ea4b4d47":"## Example: Distribution of sample means and sample variances","321ab50a":"## Example 2: ANOVA\n\n\nPerform preliminary tests.","8f56b80e":"One difference between them:","c59cc61b":"We can see how as $\\nu$ gets bigger the  distribution has less spread. See Central Theorem Limit.\n\n## Bernoulli\n\nDiscrete random variable with two possible outcomes, one with probability p and the other (1-p):\n* probability mass function f(x) equal to p if x=1 and (1-p) if x=0\n","c85541ab":"# ANOVA (ANalysis Of Variance)\n\nANOVA assumes:\n* Samples are from normal distributions\n * Here can be checked also with \"residuals are normally distributed\"\n* Populations with same variance\n* Independent samples\n\nWe have:\n* The null hypothesis for the test is that the means of all samples are equal.\n * Alternative is at least one mean is different.\n* Independent variables are categorical and also known as factors\n * Groups or levels identify multiple samples with the same value for the factor.\n* ANOVA is an omnibus test statistic and cannot tell you which specific groups were statistically significantly different from each other.\n * You can do this using a post hoc test like Tukey\u2019s Honest Significant Difference (Tukey's HSD)\n   * compares all possible pairs of group means\n\nThere are two kinds:\n* One-way: 1 independent variable.\n* Two-way: two independent variables.\n * With replication or without replication.\n * Takes into account variation at row level (if we see one-way as column variation)\n\nTest results will include:\n * $F = \\frac{between\\ group\\ variation}{within\\ group\\ variation}$\n   * Larger F ratios indicate higher probability than the groups have different means\n   * E.g. F-statistic of 10 means the between-groups variance is 10 times the within-group variance\n * p-value: if ANOVA's p-value is less than let's say 0.05 (we should choose this value, though 0.05 is typical) we have sufficient\n evidence to say that the means across each group are not equal (reject the null hypothesis that the means are the same).\n\nRegarding the F-stat $F$, the F-distribution and the rejection region:\n* F-distribution dfn (degrees of freedom numerator) is equal to $k-1$ ($k$ is the number of groups being compared)\n* F-distribution dfd (degrees of freedom denominator) is equal to $N-k$ ($N$ is the total number of samples)\n* To check the rejection region of the F-distribution for $H_0$ (groups means are equal) in python:\n  * rejection_reg = scipy.stats.f.ppf(q=.95, dfn=(k-1)), dfd=(N-k))\n\nIn python:\n* ANOVA:\n * one-way ANOVA: scipy.stats.f_oneway\n * statsmodels.stats.anova.anova_lm : \"Anova table for one or more fitted linear models\"\n  * statsmodels.formula.api.ols: Ordinary Least Squares.\n    * model = ols(formula,df).fit()\n    * model.summary()\n  * ANOVA type:\n    * I:\n    * II:\n    * III:\n* Tukey's HSD: statsmodels.stats.multicomp.pairwise_tukeyhsd(endog, groups, alpha=0.05)\n * endog: response variable\n * groups: array with groups\n * alpha: significance level for the test\n * Assumptions:\n   * observations are independent\n   * groups are normally distributed\n   * homogeneity of within-group variance across the groups\n\nThe resulting ANOVA table for a single model has columns:\n* sum_sq : Sum of squares for model terms.\n* df : Degrees of freedom for model terms.\n* F : F statistic value for significance of adding model terms.\n* PR(>F) : P-value for significance of adding model terms.\n\nThe calculation of the F-stat uses (see: https:\/\/medium.com\/@rrfd\/f-tests-and-anovas-examples-with-the-iris-dataset-fe7caa3e21d0 ):\n* F = (Mean Squared Between) \/ (Mean Squared Error)\n* Mean Squared Between = MSB \/ (k-1)\n * is the variance between groups\n * k is df of independent variable\n * MSB = sum of the squared error (sum_sq) between each group mean and the overall mean\n* Mean Squared Error = MSE \/ (n-k)\n * is the variance within groups\n * N is df of Residual\n * MSE = sum of the squared error (sum_sq of Residual) between each observation and its own group mean\n\n\n## R-like formulas, general form:\n* dependent-variable ~ independent_variables_formula\n\nFormula operators:\n* \"C(variable)\": treat variable as categorical.\n* \"+\": adds columns\n* \":\": adds new column with the product of two\n* \"*\": adds new column with the product of two and includes individual columns\n* \"-\": removes columns (\"-1\" to remove incercept)\n* You can apply vectorized functions to the variables in your model \"func(column)\"\n\n## Example 1: One-way ANOVA with scipy.stats.f_oneway\n","a7ee8b0f":"# Baye's Rule\n\n$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$\n\n\nWe call here:\n* Posterior P(A|B)\n* Likelihood P(B|A)\n* Priors P(A),P(B)\n\nWe can find the same rule written in many different ways:\n* $P(\\theta|X)=\\frac{P(X|\\theta)P(\\theta)}{P(X)}$\n\nHere $\\theta$ represents the parameters we're looking for and X our data.\n* $P(\\theta)$ is the prior, the distribution of $\\theta$ when we haven\u2019t collected any data\n* $P(X|\\theta)$ is the likelihood probability of the data X given (how likely is to find $X$ if $\\theta$ is true)\n\nIn Machine Learning context we're usually trying to find:\n* $\\underset{\\theta}{argmax}\\ P(\\theta|X)$\n\n## The Bayesian approach:\n* we make assumptions on the underlying generating process (e.g. gaussian data...)\n* we have a prior belief\n    * choose a distribution or family of distributions maybe depending on the range of $\\theta$. e.g.\n        * $\\theta \\in [0,1]$ : $\\theta \\sim $ Beta(a,b). This distribution can have many different shapes depending on the value of the parameters (see Beta distribution section)\n* we want to update that belief using the data and transform it into a posterior belief\n    * the initial prior belief will have a less and less important role as we collect data\n\nUsing:\n* $P(\\theta|X) \\propto P(X|\\theta)P(\\theta)$\n* [1] \"We are weighting our likelihood based on our prior belief of $\\theta$\"\n* We can try to sample from the posterior distribution without computing the denominator P(X), by using the proportional relation above or even using ratios of posterior probabilities.\n\n## Conjugate priors\n\nConjugate priors are special:\n* if we pick the right likelihood and prior, the posterior will be the same kind of distribution as the prior\n* Only works for certain distributions. E.g.\n    * Gaussian $\\propto$ Gaussian x Gaussian\n    * Beta distribution is the conjugate prior for Bernoulli likelihood\n    * ...\n\n## Non-informative priors\n* In case of no prior knowledge about the distribution of $\\theta$\n* Use uniform prior (assigns equal probability to all values of $\\theta$) (e.g. Beta(1,1))\n\n\n## Other typical nomenclatures\n\n* $D$ instead of $X$ to refer to our data\n* $h$ (hypothesis) instead of $\\theta$\n\n## Sources to check\n\n* [1] \"MIT 18.650 Statistics for Applications, Fall 2016\" https:\/\/www.youtube.com\/watch?v=bFZ-0FH5hfs\n* \"Artificial Intelligence: Reinforcement Learning in Python\" https:\/\/www.udemy.com\/course\/artificial-intelligence-reinforcement-learning-in-python\/\n\n\n# Some Probability Distributions\n\n## Methods of random variables on scipy.stats\n\nCheck:\n* https:\/\/docs.scipy.org\/doc\/scipy\/reference\/tutorial\/stats.html\n* https:\/\/docs.scipy.org\/doc\/scipy\/reference\/stats.html#module-scipy.stats\n\n\"All continuous distributions take loc and scale as keyword parameters to adjust the location and scale of the distribution\"\n\n\"Discrete distributions have mostly the same basic methods as the continuous distributions. However pdf is replaced by\nthe probability mass function pmf, no estimation methods, such as fit, are available, and scale is not a valid keyword parameter\"\n\n\n### Common methods of Random variables:\n* rvs: Random Variates (argument size indicates how many we want)\n* pdf: Probability Density Function (probability that the variate has the value x)\n* cdf: Cumulative Distribution Function (probability that the variable takes a value less than or equal to x)\n* sf: Survival Function (1-CDF) (probability that the variable takes a value greater than x)\n* ppf: Percent Point Function (Inverse of CDF, given a probability calculates the corresponding x for the cumulative distribution)\n * ppf(0.95) value for which the random variable has a 95% probability of being less or equal to\n* isf: Inverse Survival Function (Inverse of SF, given a probability calculates the corresponding x for the SF)\n * sf(0.10) value for which the random variable has 10% probability of being greater than\n* stats: Return mean, variance, (Fisher\u2019s) skew, or (Fisher\u2019s) kurtosis\n* moment: non-central moments of the distribution\n\n\n## Normal or Gaussian\n\nAbbreviated as $X \\sim N(\\mu,\\sigma^2)$\n* $E[X]=\\mu$ and $Var(X)=\\sigma^2$\n* Standard normal distribution N(0,1) $\\mu=0$ and $\\sigma^2=1$\n* Standard normal random variables are often labeled z\n\nIts units can be thought as standard deviation units\nPercentiles:\n* 10, 5,  2.5 and 1 are -1.28, -1.645, -1.96, -2.33\n* 90, 95, 97.5 and 99 are 1.28, 1.645, 1.96, 2.33\n  * 1.96 contains 95% of the probability mass\n\nProbability masses:\n* $(\\mu \\pm \\sigma)$ 68.2%=(34.1% x2)\n* $(\\mu \\pm 2\\sigma)$ 95.4%=(47.7% x2)\n* $(\\mu \\pm 3\\sigma)$ 99.7%=(49.85% x2)\n\nChanging between standard and non-standard normals:\n* Change units to standard deviations from the mean with $Z = \\frac{(X - \\mu)}{\\sigma}  \\sim N(0,1)$\n* $X=\\mu + \\sigma Z \\sim N(\\mu,\\sigma^2)$\n\nPython's:\n* scipy.stats.norm(loc=0, scale=1)","38480d9b":"## Student's t-distribution\n\nWhen we have samples of small size $n$ from a normally-distributed population with unknown variance.\n\n\"If we take a sample of $n$ observations from a normal distribution, the t-distribution with \u03bd = n \u2212 1 degrees\nof freedom can be defined as the distribution of the location of the sample mean relative to the true mean, divided by\nthe sample standard deviation, after multiplying by the standardizing term $\\sqrt {n}$. In this way, the t-distribution\ncan be used to construct a confidence interval for the true mean\"\nSource: https:\/\/en.wikipedia.org\/wiki\/Student%27s_t-distribution","a289fab5":"## Examples of Q-Q Plot","b2350417":"The AOV results for \"sepal_l ~ C(species)\" are significant, see value of PR(>F):\n* We reject the null hypothesis that the sepal_l ofr each species have the same mean.","63c20997":"# Covariance and Correlation\n* The units of measurement of the covariance $Cov(X , Y)$ are those of $X$ times those of $Y$.\n* By contrast, correlation coefficients, are a dimensionless measure of linear dependence. They measure how much of the\nchange of one variable is explained by its relation with the other.\n* Covariance is in $(-\\infty, \\infty)$, Correlation is in $[-1, 1]$\n\nCovariance is $Cov(X,Y) = E[(X-E[Y])(Y-E[Y])]$\n* Positive covariance: means two variables change in the same direction\n* Negative covariance: means two variables change in opposite direction\n* Covariance zero: means there doesn't appear to be any relation in their change\n\nPopulation covariance\n* $\\sigma_{xy} = \\frac{(\\sum{x_i-\\mu_x})(\\sum{y_i-\\mu_y})}{N}$\n\nSample covariance:\n* $S_{xy} = \\frac{(\\sum{x_i-\\bar{x}})(\\sum{y_i-\\bar{y}})}{n-1}$\n\nPearson's Correlation:\n* From Wikipedia:\n * \"is invariant under separate changes in location and scale in the two variables\"\n * \"A value of 1 implies that a linear equation describes the relationship between X and Y perfectly, with all data\n points lying on a line for which Y increases as X increases. A value of \u22121 implies that all data points lie on a line for which Y decreases as X increases. \"\n* Population: $Correlation = \\rho_{xy} = \\frac{Cov(X,Y)}{\\sigma_x \\sigma_y}$\n* Sample: $Correlation = r_{xy} = \\frac{Cov(X,Y)}{S_x S_y} = \\frac{1}{n-1} \\sum_i (\\frac{x-\\bar{x}}{S_x})(\\frac{y-\\bar{y}}{S_y})$\n\nCorrelation and covariance matrices:\n* The correlation matrix is a symmetrical matrix that contains the correlation coefficients between variables\n(each cell between two variables, diagonal is a variable with itself).\n* The covariance matrix ($K_{XX}$ or $\\Sigma$) has the same structure but contains covariance coefficients.\n\nPython's:\n* numpy.cov(m, ...)\n* numpy.corrcoef(x, y=None, ...)\n* scipy.stats.pearsonr(x, y)","8e4f2bd1":"From AI-Bootcamp: https:\/\/github.com\/v-fuentec\/AI-bootcamp\n\n# Basic Concepts\n\n<b>Statistic Inference:<\/b>\n* Is generating conclusions about a population from a noisy sample.\n* Extending our knowledge beyond our data.\n* Many ways of doing it...\n\n<b>Process of statistical inference:<\/b>\n* Probability: model connects the data to the population using assumptions.\n* Estimator: rule for calculating and estimate of a given quantity based on observed data.\n* Estimand: Parameter in the population which is to be estimated.\n\n\n<b>Probability:<\/b>\n* We will try to estimate the population properties from samples (collected data)\n\n<b>Some rules of probability:<\/b>\n* Prob. that nothing occurs is 0\n* Prob. something occurs is 1\n* Prob_of_something = 1 - probability _opposite_thing\n* Prob one or more of mutually exclusive events occur is $\\sum_i{prob(event_i)}$\n* Probability that one or more occurs:  P(A\u222aB)=P(A)+P(B)\u2212P(A\u2229B)\n* If statistically independent:  P(A\u222aB)=P(A)+P(B)\n* If A and B are two independent events then the probability of them both occurring is the product of the probabilities.\n * P(A&B) = P(A) * P(B)\n\n<b>Random variable:<\/b> numerical outcome of an experiment (observation, ...).\n* Continuous (we assign probabilities to ranges they can take)\n* Categorical (we assign probabilities to values they can take)\n\n<b>Independence:<\/b>\n* Independent: statistically unrelated to one another\n* Independent events if prob. of their intersection is product of their independent probabilities\n* IID: Independent and Identically Distributed random variables\n* Identically distributed: all having been drawn from the same probability distribution.\n\n<b>Probability Mass function (pmf):<\/b>\n* Prob. that random variable takes a value\n* Always >=0 and sum of all possible values must be 1\n\n<b>Probability Density Function (pdf):<\/b>\n* Associated to random continuous variables.\n* Value >=0 and area under functions equals 1.\n* Use areas under the curve to calculate probabilities of ranges\n\n<b>Cumulative density function (cdf):<\/b>\n* F(x) = P(X<=x)\n* Works for continuous and categorical random variables.\n* Survival function (is 1-CDF): S(x) = P(X>x)\n* Inherently when you say probability you are talking about a population quality.\n\nWhen the random variable is continuous, the PDF is the derivative of the CDF, so integrating the PDF yields the CDF.\n\n\n<b>Quantiles:<\/b>\n\n$\\alpha^{th}$ quantile of a distribution with cumulative density distribution function F is the point $X_\\alpha$ so that\n$F(X_\\alpha)=\\alpha$\n* $\\alpha$ proportion is below X\n\n<b>Percentiles:<\/b>\nA percentile is a quantile with  expressed as a percent.\n* 90 percentile -> 90% of the observations lie below\n* Median is the 50th percentile\n\n\n# Expected values\n\n## Mean\n\nThe mean is a characterization of centrality.\n\nPopulation mean:\n* The expected value or mean of a random variable is the center of its distribution.\n* $E[X] = \\mu= \\sum_i{x_i p(x_i)}$\n* $E[X]$ represents the center of mass of a collection of locations and weights, $(x_i,p(x_i))$\n\nSample mean:\n* The center of mass of the data is the empirical mean\n* $\\bar{x} =\\sum_i{x_i p(x_i)}$ where $p(x_i)=1\/n$\n* The expected value of the sample mean is exactly the population mean that it's trying to estimate.\n* The distribution of the sample mean, the population distribution of the sample mean, is centered in the same place\nas the original population that the data is drawn from.\n* The sample mean is unbiased (unbiased estimator) because its distribution is centered at what it\u2019s trying to estimate.\n* The more data goes into the sample mean, the more concentrated its density\/mass function is around the population mean.\n\n\n## Variance & Standard Deviation\n\nVariance & Standard Deviation are measures of variation or spread.\n\n### Variance:\n* Is the expected value of the squared deviation of a random variable from its mean.\n* Population: $\\sigma^2=Var(X)=E[(X-\\mu)^2]=\\frac{1}{N}(X-\\mu)^2$\n* Sample: $S^2=\\frac{1}{n-1}(X-\\bar{x})^2$\n * It is also a random variable with an associated population distribution.\n * Its expected value is the population variance.\n * From wikipedia: \"unbiased estimator for the variance is given by applying Bessel's correction, using N \u2212 1 instead of N to yield the unbiased sample variance, denoted $S^2$\"\n\n### Standard deviation:\n* Square root of the variance\n * Notation: $\\sigma$ (population std. deviation) and S (sample std. deviation)\n* This measure has the same units as the population\n* Standard deviation of a statistic is called a standard error\n\n## Z-score (a.k.a. Standard Score)\nA standard score (a.k.a. Z-score) indicates how many standard deviations an element is from the mean.\n* $Z = \\frac{(X - \\mu)}{\\sigma}$\n\n\n### IMPORTANT NOTES\n\n* Our sample expected values for the sample mean and variance will estimate the population mean and variance respectively.\n* The average of a random variable is itself a random variable and its associated distribution has  an expected value.\n* The center of mass of this distribution is the same as that of the original distribution.\n\n## Relation between sample and population statistics\n\nRelation between sample and population means:\n* $E[\\hat{x}]=\\mu$\n* Sample means (means of random samples) estimate the population mean (their distribution is centered around it).\n* Gets more concentrated around its center with larger sample sizes\n* Variance of the sample mean is $\\sigma^2\/n$\n * Estimate of the variance of sample mean is $S^2\/n$\n * Estimate of the standard error is $S\/n$\n\nSample variance $S^2$ estimates the population variance $\\sigma^2$:\n* has a distribution centered around it\n* gets more concentrated around its center with larger sample sizes\n\n## Example: distribution, mean and variance for different sample sizes","447a9efc":"## Binomial\n\n...\n\n## Poisson\n\n...\n\n## Chi-squared ($\\chi^2)$\n\n...\n\n\n\n## F-distribution\n\n...\n\n# Law of Large Numbers (LLN)\n\nThe average limits to what it's estimating, the population mean.\nAverages of iid samples converge to the population means that they are estimating.\n\n# Central Limit Theorem (CLT)\n\nAsymptotics: refers to the behavior of estimators as the sample size\n(or other relevant quantity) goes to infinity (or some other relevant number).\n\n\nThe Central Limit Theorem (CLT) states that the distribution of averages of IID variables (properly normalized) becomes\nthat of a standard normal as the sample size increases.\n* $\\frac{\\bar{X_n}-\\mu}{n}$ is (estimate - mean_of_estimate) \/ std_error_of_estimate\n* Has a distribution like that of a standard normal for large n\n * $\\bar{X}_n$ is approximately $N(\\mu,\\sigma\/n)$\n  * Centered around the mean\n  * Sd = SE of the mean ($\\sigma^2\/n$)\n\nIMPORTANT!!\n* We can assume normality of a sample mean no matter what kind of population we have, as long as our sample\nsize is large enough and our samples are independent.","4f95ac1a":"Here we see ANOVA and Tukey HSD results for:\n* sepal_l ~ C(species)\n* sepal_w ~ C(species)\n\nIn both cases results are significant and we can see the confidence intervals for the mean differences between groups.","28076027":"# Equal Variance Tests\n* All assume \"null hypothesis that all input samples are from populations with equal variances.\"\n* Levene: scipy.stats.levene(...)\n * \"is an alternative to Bartlett\u2019s test bartlett in the case where there are significant deviations from normality\"\n* Bartlett: scipy.stats.bartlett(...)\n* Fligner-Killeen: scipy.stats.fligner(...)\n\n## Example Equal Variance Tests\nLet's assume that if the p-value > 0.05, then we fail to reject the null hypothesis and we assume the samples have the same variance.","9ae2f7fa":"## Example 3: ANOVA and Tukey HSD","0b3a0448":"# Normality tests\n\n* Shapiro-Wilk normality test: shapiro\n * Null hypothesis that the data was drawn from a normal distribution\n* Kolmogorov-Smirnov test for goodness of fit: kstest(rvs, cdf)\n * Null hypothesis, the two distributions are identical.\n * cdf: If a string, it should be the name of a distribution in `scipy.stats`, which will be used as the cdf function.\n * Use cdf='norm'\n* Anderson-Darling test for data coming from a particular distribution: anderson(x, dist='norm')\n * \"Critical values provided are for the following significance levels:\n    normal\/exponenential\n      15%, 10%, 5%, 2.5%, 1%\"\n * \"If the returned statistic is larger than these critical values then for the corresponding significance level, the null hypothesis that the data come from the chosen distribution can be rejected.\"\n* Other:\n * Jarque-Bera (scipy.stats.jarque_bera)\n   * tests whether the sample data has the skewness and kurtosis matching a normal distribution.\n   * only use with large number of samples (>2000)\n * Quantile-Quantile Plot (Q-Q Plot)\n   * Relation between theoretical (if normally distributed) and actual quantiles\n   * If data comes from a normal distribution should approximately present as straight line\n\nLet's assume that if the p-value > 0.05, then we fail to reject the null hypothesis and we assume the distribution of our variable is normal\/gaussian.\n\n## Examples of normality tests","82931c57":"# Confidence Intervals\n\n...\n\n# Hypothesis testing\n\nHypothesis testing is concerned with making decisions using data:\n* A null hypothesis ($H_0$) is specified that represents the status quo\n* The null hypothesis is assumed true and statistical evidence is required to reject in favor of a research or\nalternative hypothesis (H_a).\n* Alternative hypotheses are typically expressed as a condition ('<', '>' or '!=')\n* In court law, the null hypothesis is that the defendant is innocent.\n\n## Possible outcomes:\n\n|Truth|Decide|Result|Example|\n|---|---|---|---|\n|$H_0$ |$H_0$ |Correctly accept $H_0$||\n|$H_0$ |$H_a$ |Type I error |False positive! Innocent people convicted|\n|$H_a$ |$H_a$ |Correctly reject $H_0$||\n|$H_a$ |$H_0$ |Type II error |False negative! Guilty people let free|\n\n\n## Choosing a rejection region:\nThe rejection region is the area of values of $\\bar{X}$ (sometimes called $\\mu_a$) were we can reject $H_0$ with a\nType I Error of up to $\\alpha$ (we choose this value):\n* For example, we could reject the null hypothesis if $\\bar{X}$ (our sample or experimental mean) was larger than some constant C\n* Typically C is chosen so that the probability of a Type I error, is .05 (or some other relevant constant)\n* $\\alpha$ = Type I error rate = Probability of rejecting the null hypothesis when it\u2019s correct.\n\nExample:\n* $H_0: \\mu = \\mu_0$ y $H_a : \\mu \\gt \\mu_0$\n* Choose C so that $P(\\bar{X}>C;H_0)$ is 5%\n* Reject $H_0$ when $\\bar{X} \\ge C$ (probability of rejection is 5% when $H_0$ is true)\n* When can convert to Z-score (to get how many standard errors the sample mean is above the hypothesized mean).\n  * If it\u2019s greater than 1.645 (the 0.95 quantile) we reject $H_0$ .\n* Or when $(\\bar{X}-\\mu_0)\/(s\/\\sqrt{n}) > Z_{1-\\alpha}$ (using the appropriate upper quantile)\n  * Our standarized value for $H_a$ if greater than the $Z_{1-\\alpha}$ quantile.\n\n## One side vs two sided tests\n* One side tests have the forms $H_a : \\mu_0 > \\mu_a$ or $H_a : \\mu_0 < \\mu_a$\n* Two sided test: $H_a : \\mu_0 \\neq \\mu_a$ or $H_a : \\mu_{a1} < \\mu_0\\ or\\ \\mu_0 < \\mu_{a2}  $\n * With confidence $1-\\alpha=.05$ we reject if $\\mu_a$ lower than the lower quantile ($Z_{\\alpha\/2}$) or higher than the upper quantile ($Z_{1-\\alpha\/2}$)\n\n## p-value\n\nP-values are the most common measure of statistical significance.\n* Formally, the p-value is the probability under the null hypothesis of obtaining evidence as or more extreme than that\nobtained.\n* p-values are a convenient way to communicate the results of a hypothesis test.\n  * When communicating a p-value, the reader can perform the test at whatever Type I error rate that they would like.\n  * Just compare the p-value to the desired Type I error rate and if the p-value is smaller, reject the null hypothesis.\n* In other words if we get a very large T statistic the P-value answers the question:\n  * \"How likely would it be to get a statistic this large or larger if the null was actually true?\".\n  * If the answer to that question is \"very unlikely\", the p-value is very small, then it sheds doubt on the null being\n  true, since you observed a statistic that extreme.\n\nSo:\n* Start under the null hypothesis (asume is correct)  and then calculate the probability of obtaining evidence as or\nmore extreme that we actually obtain.\n* How unusual is our result if the null hypothesis is true?\n* If p-value is less than $\\alpha$ you reject the null hypothesis.\n\nFor two sided hypothesis tests, double the smaller of the two sided hypothesis p-values.\n\nApproach to calculate the p-value:\n* Define hypothetical distribution of a data summary (statistic) when \u201cnothing is going on\u201d (null hypothesis).\n* Calculate the summary\/statistic with the data we have (test statistic).\n* Compare what we calculated to our hypothetical distribution and see if the value is \u201cextreme\u201d (p-value is low).\n* If p-value is small either $H_0$ is true and we have observed a rare event or it is false.\n\nThe attained significance level:\nP-value can be seen as the smallest value of $\\alpha$ for which you will still reject $H_0$.\n\n## Connection with confidence intervals\n\nConsider testing:\n* $H_0: \\mu = \\mu_0$ y $H_a : \\mu \\neq \\mu_0$\n\nConsidering all possible values for with you fail to reject $H_0$:\n* This set is $(1-\\alpha) 100$% C.I. for $\\mu$\n\nIn reverse if a $(1-\\alpha) 100$% interval contains $\\mu_0$ we fail to reject $H_0$\n\n\nHypothesis testing in Python:","3e7fc979":"We can see also OLS model's predicted values for sepal_l for each class.\n\nNow for the ANOVA results.","c345c103":"Here we see the normaly distributed residuals for sepal_l.","d2eefcdd":"## Beta\n\n$Beta(\\alpha, \\beta)$ is defined on the interval [0, 1].\n\"In Bayesian inference, the beta distribution is the conjugate prior probability distribution for the Bernoulli, binomial, negative binomial and geometric distributions. The beta distribution is a suitable model for the random behavior of percentages and proportions. \"\nSource: https:\/\/en.wikipedia.org\/wiki\/Beta_distribution\n\nShapes:\n* Beta(1,1): uniform distribution\n* $\\alpha=\\beta$ and greater than one: bell shaped, the higher the parameters value the narrower it gets\n* $\\alpha>\\beta$: $\\beta=1$ exponential growth\n* $\\alpha>\\beta$: with  $\\beta>1$ left skewed (less skewed the less difference there is between $\\alpha$ and $\\beta$)\n* $\\beta>\\alpha$: $\\alpha=1$ exponential decay\n* $\\beta>\\alpha$: with  $\\alpha>1$ right skewed (less skewed the less difference there is between $\\alpha$ and $\\beta$)\n"}}