{"cell_type":{"a6f2dfe6":"code","734e9e24":"code","920977f3":"code","ddc713a3":"code","6e276123":"code","44285be0":"code","37f97ebd":"code","d2bc0d4c":"code","a3045f59":"code","a9c15adf":"code","a625d32e":"code","1dc5784c":"code","589e86dd":"code","cd40eb40":"code","04754570":"code","cc51e401":"code","8e6610c7":"code","baa64f84":"code","02a11544":"code","d94a2e47":"code","f823c2db":"code","58ea6b85":"code","cf312755":"code","6a2aa834":"code","f0cbbc3c":"code","6716cd6b":"code","23801a59":"code","09a2c91e":"code","2c646089":"code","d23ce289":"code","25635e49":"code","d8cc7bd3":"code","4d298e83":"code","64c65fee":"code","8192cbe9":"code","01ea7ce8":"code","f31c17e5":"code","c9cbdda4":"code","951178a4":"code","50021dcb":"code","8030fa52":"code","94b5235a":"code","5fff0609":"code","8b48bf28":"code","5b40a89e":"code","2ee37d63":"code","637d10bb":"code","516982d4":"code","50e27822":"code","c61e6519":"code","09481625":"code","edf6f091":"code","ea6cea20":"code","05678c71":"code","41594fbc":"code","99eb9b2f":"code","8bd413e7":"code","bc49de3e":"code","3cb819f0":"code","6cbbffb2":"code","5fae8241":"code","e5ea3520":"code","b0b6b63a":"code","87cf8ec9":"code","ca99628b":"code","9cad9715":"code","16260d67":"code","35e56813":"code","fa38f192":"code","091537bc":"code","ddefce88":"code","7e0b0dc6":"code","516372b6":"code","3c5df783":"code","9bdfb41c":"code","2723c14e":"code","dd61a97e":"code","4663a951":"code","bcfe8ead":"code","eef973ce":"code","861277a3":"code","45da82ab":"code","1bc4b97d":"code","933aab5a":"code","8613fb68":"code","3c8743a6":"code","4f1193a1":"code","d42d70ac":"markdown","1c6ef7de":"markdown","6bb2b453":"markdown","081813d6":"markdown","06582b72":"markdown","8cc5ca2b":"markdown","b09dcbb5":"markdown","71aef2c5":"markdown","26ae7088":"markdown","e70c898e":"markdown","ee1b94dc":"markdown","286c7e83":"markdown","7cb31fa4":"markdown","c6748b57":"markdown","3cbea17e":"markdown","8a065bbd":"markdown","826dd64c":"markdown","4b0d73d8":"markdown","ceac37cb":"markdown","8556b8a4":"markdown","35a6d441":"markdown","9d36468c":"markdown","a7ff210a":"markdown","8af6c819":"markdown","1a1d02eb":"markdown","0f62f01a":"markdown","77cad4ec":"markdown","f77ff25c":"markdown","3b7e8b15":"markdown","4268d82f":"markdown","ce58f959":"markdown","97149857":"markdown","f065c43a":"markdown","2c5b7629":"markdown","83334a48":"markdown","76acb697":"markdown","f4a22121":"markdown","93364e81":"markdown","a2697b52":"markdown","4aec91c1":"markdown","dea49491":"markdown","d9688ad6":"markdown","89f562c5":"markdown","b5d37511":"markdown","be954910":"markdown","2b7ef175":"markdown","5588d0af":"markdown","10ca4bf1":"markdown","0a777d19":"markdown","c8496167":"markdown","ff91869d":"markdown","809ed41f":"markdown","9b3c9c15":"markdown","6e794ad7":"markdown","6e21786a":"markdown"},"source":{"a6f2dfe6":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","734e9e24":"import numpy as np\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport cv2\n\n%matplotlib inline","920977f3":"# Read in the image\nimage = mpimg.imread('\/kaggle\/input\/introcv\/images\/oranges.jpg')\n\n# Print out the image dimensions\nprint('Image dimensions:', image.shape)\nplt.imshow(image)","ddc713a3":"# Change from color to grayscale\ngray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\nplt.imshow(gray_image, cmap='gray')","6e276123":"# Specific grayscale pixel values\n# Pixel value at x = 400 and y = 300 \n\nx = 200\ny = 100\n\nprint(gray_image[y,x])","44285be0":"# 5x5 image using just grayscale, numerical values\ntiny_image = np.array([[0, 20, 30, 150, 120],\n                      [200, 200, 250, 70, 3],\n                      [50, 180, 85, 40, 90],\n                      [240, 100, 50, 255, 10],\n                      [30, 0, 75, 190, 220]])\n\n# To show the pixel grid, use matshow\nplt.matshow(tiny_image, cmap='gray')","37f97ebd":"# Read in the image\nimage = mpimg.imread('\/kaggle\/input\/introcv\/images\/rainbow_flag.jpg')\n\nplt.imshow(image)","d2bc0d4c":"# Isolate RGB channels\nr = image[:,:,0]\ng = image[:,:,1]\nb = image[:,:,2]\n\n# The individual color channels\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,10))\nax1.set_title('R channel')\nax1.imshow(r, cmap='gray')\nax2.set_title('G channel')\nax2.imshow(g, cmap='gray')\nax3.set_title('B channel')\nax3.imshow(b, cmap='gray')\n","a3045f59":"IMG_PATH='\/kaggle\/input\/introcv\/'","a9c15adf":"image = cv2.imread(IMG_PATH+'images\/pizza_bluescreen.jpg')\n\nprint('This image is:', type(image), \n      ' with dimensions:', image.shape)","a625d32e":"image_copy = np.copy(image)\n\n# RGB (from BGR)\nimage_copy = cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB)\n\n# Display the image copy\nplt.imshow(image_copy)","1dc5784c":"# Color Threshold\nlower_blue = np.array([0,0,200]) \nupper_blue = np.array([50,50,255])","589e86dd":"# Define the masked area\nmask = cv2.inRange(image_copy, lower_blue, upper_blue)\n\n# Vizualize the mask\nplt.imshow(mask, cmap='gray')","cd40eb40":"# Masking the image to let the pizza show through\nmasked_image = np.copy(image_copy)\n\nmasked_image[mask != 0] = [0, 0, 0]\n\nplt.imshow(masked_image)","04754570":"# Loading in a background image, and converting it to RGB \nbackground_image = cv2.imread(IMG_PATH+'images\/space_background.jpg')\nbackground_image = cv2.cvtColor(background_image, cv2.COLOR_BGR2RGB)\n\n# Cropping it to the right size (514x816)\ncrop_background = background_image[0:514, 0:816]\n\n# Masking the cropped background so that the pizza area is blocked\ncrop_background[mask == 0] = [0, 0, 0]\n\n# Displaying the background\nplt.imshow(crop_background)","cc51e401":"# Adding the two images together to create a complete image!\ncomplete_image = masked_image + crop_background\n\n# Displaying the result\nplt.imshow(complete_image)","8e6610c7":"image = cv2.imread(IMG_PATH+'images\/water_balloons.jpg')\n\nimage_copy = np.copy(image)\n\nimage = cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB)\n\nplt.imshow(image)","baa64f84":"# Converting from RGB to HSV\nhsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n\n# HSV channels\nh = hsv[:,:,0]\ns = hsv[:,:,1]\nv = hsv[:,:,2]\n\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,10))\n\nax1.set_title('Hue')\nax1.imshow(h, cmap='gray')\n\nax2.set_title('Saturation')\nax2.imshow(s, cmap='gray')\n\nax3.set_title('Value')\nax3.imshow(v, cmap='gray')","02a11544":"# Color selection criteria in HSV values for getting only Pink balloons\nlower_hue = np.array([160,0,0]) \nupper_hue = np.array([180,255,255])\n","d94a2e47":"# Defining the masked area in HSV space\nmask_hsv = cv2.inRange(hsv, lower_hue, upper_hue)\n\n# masking the image\nmasked_image = np.copy(image)\nmasked_image[mask_hsv==0] = [0,0,0]\n\n# Vizualizing the mask\nplt.imshow(masked_image)","f823c2db":"# Helper functions\nimport glob # library for loading images from a directory\n\n# This function loads in images and their labels and places them in a list\n# The list contains all images and their associated labels\n# For example, after data is loaded, im_list[0][:] will be the first image-label pair in the list\ndef load_dataset(image_dir):\n    \n    # Populate this empty image list\n    im_list = []\n    image_types = [\"day\", \"night\"]\n    \n    # Iterate through each color folder\n    for im_type in image_types:\n        \n        # Iterate through each image file in each image_type folder\n        # glob reads in any image with the extension \"image_dir\/im_type\/*\"\n        for file in glob.glob(os.path.join(image_dir, im_type, \"*\")):\n            \n            # Read in the image\n            im = mpimg.imread(file)\n            \n            # Check if the image exists\/if it's been correctly read-in\n            if not im is None:\n                # Append the image, and it's type (red, green, yellow) to the image list\n                im_list.append((im, im_type))\n    \n    return im_list\n\n\n\n## Standardizing the input images\n# Resizing each image to the desired input size: 600x1100px (hxw).\n\n## Standardizing the output\n# With each loaded image, we also specify the expected output.\n# For this, we use binary numerical values 0\/1 = night\/day.\n\n\n# This function should take in an RGB image and return a new, standardized version\n# 600 height x 1100 width image size (px x px)\ndef standardize_input(image):\n    \n    # Resize image and pre-process so that all \"standard\" images are the same size\n    standard_im = cv2.resize(image, (1100, 600))\n    \n    return standard_im\n\n\n# Examples:\n# encode(\"day\") should return: 1\n# encode(\"night\") should return: 0\ndef encode(label):\n    \n    numerical_val = 0\n    if(label == 'day'):\n        numerical_val = 1\n    # else it is night and can stay 0\n    \n    return numerical_val\n\n# using both functions above, standardize the input images and output labels\ndef standardize(image_list):\n    \n    # Empty image data array\n    standard_list = []\n    \n    # Iterate through all the image-label pairs\n    for item in image_list:\n        image = item[0]\n        label = item[1]\n        \n        # Standardize the image\n        standardized_im = standardize_input(image)\n        \n        # Create a numerical label\n        binary_label = encode(label)\n        \n        # Append the image, and it's one hot encoded label to the full, processed list of image data\n        standard_list.append((standardized_im, binary_label))\n    \n    return standard_list\n","58ea6b85":"# Image data directories\nimage_dir_training = \"\/kaggle\/input\/introcv\/day_night_images\/training\/\"\nimage_dir_test = \"\/kaggle\/input\/introcv\/day_night_images\/test\/\"\n\n# Load training data\nIMAGE_LIST = load_dataset(image_dir_training)","cf312755":"image_index = 20\nselected_image = IMAGE_LIST[image_index][0]\nselected_label = IMAGE_LIST[image_index][1]\n\nprint(len(IMAGE_LIST))\nprint(selected_image.shape)\n\nplt.imshow(selected_image)","6a2aa834":"STANDARDIZED_LIST = standardize(IMAGE_LIST)","f0cbbc3c":"image_num = 0\nselected_image = STANDARDIZED_LIST[image_num][0]\nselected_label = STANDARDIZED_LIST[image_num][1]\n\n# Displaying image and data about it\nplt.imshow(selected_image)\nprint(\"Shape: \"+str(selected_image.shape))\nprint(\"Label [1 = day, 0 = night]: \" + str(selected_label))","6716cd6b":"# Converting and image to HSV colorspace\n# Visualizing the individual color channels\n\nimage_num = 0\ntest_im = STANDARDIZED_LIST[image_num][0]\ntest_label = STANDARDIZED_LIST[image_num][1]\n\n# Converting to HSV\nhsv = cv2.cvtColor(test_im, cv2.COLOR_RGB2HSV)\n\n# Printing image label\nprint('Label: ' + str(test_label))\n\n# HSV channels\nh = hsv[:,:,0]\ns = hsv[:,:,1]\nv = hsv[:,:,2]\n\n# Plotting the original image and the three channels\nf, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20,10))\nax1.set_title('Standardized image')\nax1.imshow(test_im)\nax2.set_title('H channel')\nax2.imshow(h, cmap='gray')\nax3.set_title('S channel')\nax3.imshow(s, cmap='gray')\nax4.set_title('V channel')\nax4.imshow(v, cmap='gray')","23801a59":"def avg_brightness(rgb_image):\n    # Converting image to HSV\n    hsv = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV)\n\n    # Adding up all the pixel values in the V channel\n    sum_brightness = np.sum(hsv[:,:,2])\n    area = 600*1100.0  # pixels\n    \n    avg = sum_brightness\/area\n    \n    return avg","09a2c91e":"# Testing average brightness levels\n# As an example, a \"night\" image is loaded in and its avg brightness is displayed\nimage_num = 190\ntest_im = STANDARDIZED_LIST[image_num][0]\n\navg = avg_brightness(test_im)\nprint('Avg brightness: ' + str(avg))\nplt.imshow(test_im)","2c646089":"# This function should take in RGB image input\ndef estimate_label(rgb_image):\n    \n    # Extracting average brightness feature from an RGB image \n    avg = avg_brightness(rgb_image)\n        \n    # Using the avg brightness feature to predict a label (0, 1)\n    predicted_label = 0\n    threshold = 98\n    if(avg > threshold):\n        # if the average brightness is above the threshold value, we classify it as \"day\"\n        predicted_label = 1\n    # else, the pred-cted_label can stay 0 (it is predicted to be \"night\")\n    \n    return predicted_label    \n    ","d23ce289":"import random\n\n# Load test data\nTEST_IMAGE_LIST = load_dataset(image_dir_test)\n\n# Standardize the test data\nSTANDARDIZED_TEST_LIST = standardize(TEST_IMAGE_LIST)\n\n# Shuffle the standardized test data\nrandom.shuffle(STANDARDIZED_TEST_LIST)","25635e49":"def get_misclassified_images(test_images):\n    # Tracking misclassified images by placing them into a list\n    misclassified_images_labels = []\n\n    # Iterating through all the test images\n    for image in test_images:\n\n        im = image[0]\n        true_label = image[1]\n\n        predicted_label = estimate_label(im)\n\n        # Comparing true and predicted labels \n        if(predicted_label != true_label):\n            # If these labels are not equal, the image has been misclassified\n            misclassified_images_labels.append((im, predicted_label, true_label))\n            \n    return misclassified_images_labels\n","d8cc7bd3":"MISCLASSIFIED = get_misclassified_images(STANDARDIZED_TEST_LIST)\n\n# Accuracy calculations\ntotal = len(STANDARDIZED_TEST_LIST)\nnum_correct = total - len(MISCLASSIFIED)\naccuracy = num_correct\/total\n\nprint('Accuracy: ' + str(accuracy))\nprint(\"Number of misclassified images = \" + str(len(MISCLASSIFIED)) +' out of '+ str(total))","4d298e83":"image_stripes = cv2.imread(IMG_PATH+'images\/stripes.jpg')\nimage_stripes = cv2.cvtColor(image_stripes, cv2.COLOR_BGR2RGB)\n\nimage_solid = cv2.imread(IMG_PATH+'images\/pink_solid.jpg')\nimage_solid = cv2.cvtColor(image_solid, cv2.COLOR_BGR2RGB)\n\n\n# Displaying the images\nf, (ax1,ax2) = plt.subplots(1, 2, figsize=(10,5))\n\nax1.imshow(image_stripes)\nax2.imshow(image_solid)","64c65fee":"# converting to grayscale to focus on the intensity patterns in the image\ngray_stripes = cv2.cvtColor(image_stripes, cv2.COLOR_RGB2GRAY)\ngray_solid = cv2.cvtColor(image_solid, cv2.COLOR_RGB2GRAY)\n\n# normalizing the image color values from a range of [0,255] to [0,1] for further processing\nnorm_stripes = gray_stripes\/255.0\nnorm_solid = gray_solid\/255.0\n\n# performing a fast fourier transform and create a scaled, frequency transform image\ndef ft_image(norm_image):\n    '''This function takes in a normalized, grayscale image\n       and returns a frequency spectrum transform of that image. '''\n    f = np.fft.fft2(norm_image)\n    fshift = np.fft.fftshift(f)\n    frequency_tx = 20*np.log(np.abs(fshift))\n    \n    return frequency_tx\n","8192cbe9":"f_stripes = ft_image(norm_stripes)\nf_solid = ft_image(norm_solid)\n\n# displaying the images\n# original images to the left of their frequency transform\nf, (ax1,ax2,ax3,ax4) = plt.subplots(1, 4, figsize=(20,10))\n\nax1.set_title('original image')\nax1.imshow(image_stripes)\nax2.set_title('frequency transform image')\nax2.imshow(f_stripes, cmap='gray')\n\nax3.set_title('original image')\nax3.imshow(image_solid)\nax4.set_title('frequency transform image')\nax4.imshow(f_solid, cmap='gray')\n","01ea7ce8":"image = mpimg.imread(IMG_PATH+'images\/curved_lane.jpg')\n\nplt.imshow(image)","f31c17e5":"#Converting to gray scale\ngray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\nplt.imshow(gray, cmap='gray')","c9cbdda4":"# 3x3 array for edge detection\nsobel_y = np.array([[ -1, -2, -1], \n                   [ 0, 0, 0], \n                   [ 1, 2, 1]])\n\nsobel_x = np.array([[ -1, 0, 1], \n                   [ -2, 0, 2], \n                   [ -1, 0, 1]])\n\n# Filtering the image using filter2D \nfiltered_image = cv2.filter2D(gray, -1, sobel_y)\n\nplt.imshow(filtered_image, cmap='gray')","951178a4":"# Displaying sobel x filtered image\nfiltered_image = cv2.filter2D(gray, -1, sobel_x)\n\nplt.imshow(filtered_image, cmap='gray')","50021dcb":"image = cv2.imread(IMG_PATH+'images\/brain_MR.jpg')\n\nimage_copy = np.copy(image)\n\nimage_copy = cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB)\n\nplt.imshow(image_copy)","8030fa52":"# Converting to grayscale for filtering\ngray = cv2.cvtColor(image_copy, cv2.COLOR_RGB2GRAY)\n\n# Creating a Gaussian blurred image\ngray_blur = cv2.GaussianBlur(gray, (9, 9), 0)\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n\nax1.set_title('original gray')\nax1.imshow(gray, cmap='gray')\n\nax2.set_title('blurred image')\nax2.imshow(gray_blur, cmap='gray')","94b5235a":"# High-pass filter \n\n# 3x3 sobel filters for edge detection\nsobel_x = np.array([[ -1, 0, 1], \n                   [ -2, 0, 2], \n                   [ -1, 0, 1]])\n\n\nsobel_y = np.array([[ -1, -2, -1], \n                   [ 0, 0, 0], \n                   [ 1, 2, 1]])\n\n\n# Filters the orginal and blurred grayscale images using filter2D\nfiltered = cv2.filter2D(gray, -1, sobel_x)\n\nfiltered_blurred = cv2.filter2D(gray_blur, -1, sobel_y)\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n\nax1.set_title('original gray')\nax1.imshow(filtered, cmap='gray')\n\nax2.set_title('blurred image')\nax2.imshow(filtered_blurred, cmap='gray')","5fff0609":"retval, binary_image = cv2.threshold(filtered_blurred, 30, 255, cv2.THRESH_BINARY)\n\nplt.imshow(binary_image, cmap='gray')","8b48bf28":"# gaussian, sobel, and laplacian (edge) filters\n\ngaussian = (1\/9)*np.array([[1, 1, 1],\n                           [1, 1, 1],\n                           [1, 1, 1]])\n\nsobel_x= np.array([[-1, 0, 1],\n                   [-2, 0, 2],\n                   [-1, 0, 1]])\n\nsobel_y= np.array([[-1,-2,-1],\n                   [0, 0, 0],\n                   [1, 2, 1]])\n\n# laplacian, edge filter\nlaplacian=np.array([[0, 1, 0],\n                    [1,-4, 1],\n                    [0, 1, 0]])\n\nfilters = [gaussian, sobel_x, sobel_y, laplacian]\nfilter_name = ['gaussian','sobel_x', \\\n                'sobel_y', 'laplacian']\n\n\n# performing a fast fourier transform on each filter\n# and creating a scaled, frequency transform image\nf_filters = [np.fft.fft2(x) for x in filters]\nfshift = [np.fft.fftshift(y) for y in f_filters]\nfrequency_tx = [np.log(np.abs(z)+1) for z in fshift]\n\n# displaying 4 filters\nfor i in range(len(filters)):\n    plt.subplot(2,2,i+1),plt.imshow(frequency_tx[i],cmap = 'gray')\n    plt.title(filter_name[i]), plt.xticks([]), plt.yticks([])\n\nplt.show()","5b40a89e":"image = cv2.imread(IMG_PATH+'images\/brain_MR.jpg')\n\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Converting the image to grayscale for processing\ngray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\nplt.imshow(gray, cmap='gray')","2ee37d63":"# Canny using \"wide\" and \"tight\" thresholds\n\nwide = cv2.Canny(gray, 30, 100)\ntight = cv2.Canny(gray, 200, 240)\n     \n# Displaying the images\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n\nax1.set_title('wide')\nax1.imshow(wide, cmap='gray')\n\nax2.set_title('tight')\nax2.imshow(tight, cmap='gray')","637d10bb":"image = cv2.imread(IMG_PATH+'images\/phone.jpg')\n\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.imshow(image)","516982d4":"gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n# Parameters for Canny\nlow_threshold = 50\nhigh_threshold = 100\nedges = cv2.Canny(gray, low_threshold, high_threshold)\n\nplt.imshow(edges, cmap='gray')","50e27822":"# Hough transform parameters\nrho = 1\ntheta = np.pi\/180\nthreshold = 30\nmin_line_length = 100\nmax_line_gap = 5\n\nline_image = np.copy(image) #creating an image copy to draw lines on\n\n# Hough on the edge-detected image\nlines = cv2.HoughLinesP(edges, rho, theta, threshold, np.array([]),\n                        min_line_length, max_line_gap)\n\n\n# Iterating over the output \"lines\" and drawing lines on the image copy\nfor line in lines:\n    for x1,y1,x2,y2 in line:\n        cv2.line(line_image,(x1,y1),(x2,y2),(255,0,0),3)\n        \nplt.imshow(line_image)","c61e6519":"image = cv2.imread(IMG_PATH+'images\/round_farms.jpg')\n\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Gray and blur\ngray = cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n\ngray_blur = cv2.GaussianBlur(gray, (3, 3), 0)\n\nplt.imshow(gray_blur, cmap='gray')","09481625":"# for drawing circles on\ncircles_im = np.copy(image)\n\ncircles = cv2.HoughCircles(gray_blur, cv2.HOUGH_GRADIENT, 1, \n                           minDist=45,\n                           param1=70,\n                           param2=11,\n                           minRadius=30,\n                           maxRadius=100)\n\n# converting circles into expected type\ncircles = np.uint16(np.around(circles))\n\n# drawing each one\nfor i in circles[0,:]:\n    # the outer circle\n    cv2.circle(circles_im,(i[0],i[1]),i[2],(0,255,0),2)\n    # the center of the circle\n    cv2.circle(circles_im,(i[0],i[1]),2,(0,0,255),3)\n    \nplt.imshow(circles_im)\n\nprint('Circles shape: ', circles.shape)","edf6f091":"# loading in color image for face detection\nimage = cv2.imread(IMG_PATH+'images\/multi_faces.jpg')\n\n# converting to RBG\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(20,10))\nplt.imshow(image)","ea6cea20":"# converting to grayscale\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  \n\nplt.figure(figsize=(20,10))\nplt.imshow(gray, cmap='gray')","05678c71":"# loading in cascade classifier\nface_cascade = cv2.CascadeClassifier('\/kaggle\/input\/introcv\/detector_architectures\/haarcascade_frontalface_default.xml')\n\n# running the detector on the grayscale image\nfaces = face_cascade.detectMultiScale(gray, 4, 6)","41594fbc":"# printing out the detections found\nprint ('We found ' + str(len(faces)) + ' faces in this image')\nprint (\"Their coordinates and lengths\/widths are as follows\")\nprint ('=============================')\nprint (faces)","99eb9b2f":"img_with_detections = np.copy(image)   #a copy of the original image to plot rectangle detections ontop of\n\n# looping over our detections and draw their corresponding boxes on top of our original image\nfor (x,y,w,h) in faces:  \n    # Note: the fourth element (255,0,0) determines the color of the rectangle, \n    # and the final argument (here set to 5) determines the width of the drawn rectangle\n    cv2.rectangle(img_with_detections,(x,y),(x+w,y+h),(255,0,0),5)  \n\nplt.figure(figsize=(20,10))\nplt.imshow(img_with_detections)","8bd413e7":"# Read in the image\nimage = cv2.imread(IMG_PATH+'images\/waffle.jpg')\n\n# Make a copy of the image\nimage_copy = np.copy(image)\n\n# Change color to RGB (from BGR)\nimage_copy = cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB)\n\nplt.imshow(image_copy)","bc49de3e":"gray = cv2.cvtColor(image_copy, cv2.COLOR_RGB2GRAY)\ngray = np.float32(gray)\n\n# Detecting corners \ndst = cv2.cornerHarris(gray, 2, 3, 0.04)\n\n# Dilating corner image to enhance corner points\ndst = cv2.dilate(dst,None)\n\nplt.imshow(dst, cmap='gray')\n","3cb819f0":"thresh = 0.1*dst.max()\n\n# Creating an image copy to draw corners on\ncorner_image = np.copy(image_copy)\n\n# Iterating through all the corners and draw them on the image (if they pass the threshold)\nfor j in range(0, dst.shape[0]):\n    for i in range(0, dst.shape[1]):\n        if(dst[j,i] > thresh):\n            # image, center pt, radius, color, thickness\n            cv2.circle( corner_image, (i, j), 1, (0,255,0), 1)\n\nplt.imshow(corner_image)","6cbbffb2":"image = cv2.imread(IMG_PATH+'images\/thumbs_up_down.jpg')\n\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.imshow(image)","5fae8241":"gray = cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n\n# Binary thresholded image\nretval, binary = cv2.threshold(gray, 225, 255, cv2.THRESH_BINARY_INV)\n\nplt.imshow(binary, cmap='gray')","e5ea3520":"# Finding contours from thresholded, binary image\ncontours, hierarchy = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n# Drawing all contours on a copy of the original image\ncontours_image = np.copy(image)\ncontours_image = cv2.drawContours(contours_image, contours, -1, (0,255,0), 3)\n\nplt.imshow(contours_image)","b0b6b63a":"image = cv2.imread(IMG_PATH+'images\/monarch.jpg')\n\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.imshow(image)","87cf8ec9":"# Reshaping image into a 2D array of pixels and 3 color values (RGB)\npixel_vals = image.reshape((-1,3))\n\n# Converting to float type\npixel_vals = np.float32(pixel_vals)","ca99628b":"# you can change the number of max iterations for faster convergence!\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 0.2)\n\nk = 3 # k\n\nretval, labels, centers = cv2.kmeans(pixel_vals, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n\n# converting data into 8-bit values\ncenters = np.uint8(centers)\nsegmented_data = centers[labels.flatten()]\n\n# reshaping data into the original image dimensions\nsegmented_image = segmented_data.reshape((image.shape))\nlabels_reshape = labels.reshape(image.shape[0], image.shape[1])\n\nplt.imshow(segmented_image)","9cad9715":"image = cv2.imread(IMG_PATH+'images\/rainbow_flag.jpg')\n\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.imshow(image)","16260d67":"level_1 = cv2.pyrDown(image)\nlevel_2 = cv2.pyrDown(level_1)\nlevel_3 = cv2.pyrDown(level_2)\n\n# Displaying the images\nf, (ax1,ax2,ax3,ax4) = plt.subplots(1, 4, figsize=(20,10))\n\nax1.set_title('original')\nax1.imshow(image)\n\nax2.imshow(level_1)\nax2.set_xlim([0, image.shape[1]])\nax2.set_ylim([0, image.shape[0]])\n\nax3.imshow(level_2)\nax3.set_xlim([0, image.shape[1]])\nax3.set_ylim([0, image.shape[0]])\n\nax4.imshow(level_3)\nax4.set_xlim([0, image.shape[1]])\nax4.set_ylim([0, image.shape[0]])","35e56813":"img_path = IMG_PATH+'images\/car.png'\n\nbgr_img = cv2.imread(img_path)\ngray_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY)\n\ngray_img = gray_img.astype(\"float32\")\/255\n\nplt.imshow(gray_img, cmap='gray')\nplt.show()","fa38f192":"filter_vals = np.array([[-1, -1, 1, 1], [-1, -1, 1, 1], [-1, -1, 1, 1], [-1, -1, 1, 1]])\n# defining four filters\nfilter_1 = filter_vals\nfilter_2 = -filter_1\nfilter_3 = filter_1.T\nfilter_4 = -filter_3\nfilters = np.array([filter_1, filter_2, filter_3, filter_4])","091537bc":"# visualizing filters\nfig = plt.figure(figsize=(10, 5))\nfor i in range(4):\n    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n    ax.imshow(filters[i], cmap='gray')\n    ax.set_title('Filter %s' % str(i+1))\n    width, height = filters[i].shape\n    for x in range(width):\n        for y in range(height):\n            ax.annotate(str(filters[i][x][y]), xy=(y,x),\n                        horizontalalignment='center',\n                        verticalalignment='center',\n                        color='white' if filters[i][x][y]<0 else 'black')","ddefce88":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# data loading and transforming\nfrom torchvision.datasets import FashionMNIST\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms","7e0b0dc6":"# neural network with a single convolutional layer with four filters\nclass Net(nn.Module):\n    \n    def __init__(self, weight):\n        super(Net, self).__init__()\n        # initializing the weights of the convolutional layer to be the weights of the 4 defined filters\n        k_height, k_width = weight.shape[2:]\n        self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False)\n        self.conv.weight = torch.nn.Parameter(weight)\n\n    def forward(self, x):\n        # calculates the output of a convolutional layer\n        # pre- and post-activation\n        conv_x = self.conv(x)\n        activated_x = F.relu(conv_x)\n        \n        # returns both layers\n        return conv_x, activated_x\n    \n# instantiating the model and setting the weights\nweight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor)\nmodel = Net(weight)\n\nprint(model)","516372b6":"# helper function for visualizing the output of a given layer\n# default number of filters is 4\ndef viz_layer(layer, n_filters= 4):\n    fig = plt.figure(figsize=(20, 20))\n    \n    for i in range(n_filters):\n        ax = fig.add_subplot(1, n_filters, i+1, xticks=[], yticks=[])\n        # grab layer outputs\n        ax.imshow(np.squeeze(layer[0,i].data.numpy()), cmap='gray')\n        ax.set_title('Output %s' % str(i+1))","3c5df783":"# plotting original image\nplt.imshow(gray_img, cmap='gray')\n\n# visualizing all filters\nfig = plt.figure(figsize=(12, 6))\nfig.subplots_adjust(left=0, right=1.5, bottom=0.8, top=1, hspace=0.05, wspace=0.05)\nfor i in range(4):\n    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n    ax.imshow(filters[i], cmap='gray')\n    ax.set_title('Filter %s' % str(i+1))\n\n    \n# converting the image into an input Tensor\ngray_img_tensor = torch.from_numpy(gray_img).unsqueeze(0).unsqueeze(1)\n\n# getting the convolutional layer (pre and post activation)\nconv_layer, activated_layer = model(gray_img_tensor)\n\n# visualizing the output of a conv layer\nviz_layer(conv_layer)","9bdfb41c":"# Adding a pooling layer of size (4, 4)\nclass Net(nn.Module):\n    \n    def __init__(self, weight):\n        super(Net, self).__init__()\n        k_height, k_width = weight.shape[2:]\n        self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False)\n        self.conv.weight = torch.nn.Parameter(weight)\n        # defining a pooling layer\n        self.pool = nn.MaxPool2d(4, 4)\n\n    def forward(self, x):\n        # calculates the output of a convolutional layer\n        # pre- and post-activation\n        conv_x = self.conv(x)\n        activated_x = F.relu(conv_x)\n        \n        # applies pooling layer\n        pooled_x = self.pool(activated_x)\n        \n        # returns all layers\n        return conv_x, activated_x, pooled_x\n    \nweight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor)\nmodel = Net(weight)\n\nprint(model)","2723c14e":"plt.imshow(gray_img, cmap='gray')\n\n# visualizing all filters\nfig = plt.figure(figsize=(12, 6))\nfig.subplots_adjust(left=0, right=1.5, bottom=0.8, top=1, hspace=0.05, wspace=0.05)\nfor i in range(4):\n    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n    ax.imshow(filters[i], cmap='gray')\n    ax.set_title('Filter %s' % str(i+1))\n\n    \n# converting the image into an input Tensor\ngray_img_tensor = torch.from_numpy(gray_img).unsqueeze(0).unsqueeze(1)\n\nconv_layer, activated_layer, pooled_layer = model(gray_img_tensor)\n\n# visualizing the output of the activated conv layer\nviz_layer(activated_layer)\n\n# visualizing the output of the pooling layer\nviz_layer(pooled_layer)","dd61a97e":"# The output of torchvision datasets are PILImage images of range [0, 1]. \n# We transform them to Tensors for input into a CNN\n\ndata_transform = transforms.ToTensor()\n\ntrain_data = FashionMNIST(root='.\/data', train=True,\n                                   download=True, transform=data_transform)\n\ntest_data = FashionMNIST(root='.\/data', train=False,\n                                  download=True, transform=data_transform)\n\n\n# some stats about the training and test data\nprint('Train data, number of images: ', len(train_data))\nprint('Test data, number of images: ', len(test_data))","4663a951":"# preparing data loaders, set the batch_size\nbatch_size = 16\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n\n# specifying the image classes\nclasses = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']","bcfe8ead":"# one batch of training images\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimages = images.numpy()\n\n# plotting the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(25, 4))\nfor idx in np.arange(batch_size):\n    ax = fig.add_subplot(2, batch_size\/2, idx+1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n    ax.set_title(classes[labels[idx]])","eef973ce":"class Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        # 1 input image channel (grayscale), 10 output channels\/feature maps\n        # 3x3 square convolution kernel\n        ## output size = (W-F)\/S +1 = (28-3)\/1 +1 = 26\n        # the output Tensor for one image, will have the dimensions: (10, 26, 26)\n        # after one pool layer, this becomes (10, 13, 13)\n        self.conv1 = nn.Conv2d(1, 10, 3)\n        \n        # maxpool layer\n        # pool with kernel_size=2, stride=2\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        # second conv layer: 10 inputs, 20 outputs, 3x3 conv\n        ## output size = (W-F)\/S +1 = (13-3)\/1 +1 = 11\n        # the output tensor will have dimensions: (20, 11, 11)\n        # after another pool layer this becomes (20, 5, 5); 5.5 is rounded down\n        self.conv2 = nn.Conv2d(10, 20, 3)\n        \n        # 20 outputs * the 5*5 filtered\/pooled map size\n        self.fc1 = nn.Linear(20*5*5, 50)\n        \n        # dropout with p=0.4\n        self.fc1_drop = nn.Dropout(p=0.4)\n        \n        # finally, 10 output channels (for the 10 classes)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        # two conv\/relu + pool layers\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n\n        # this line of code is the equivalent of Flatten in Keras\n        x = x.view(x.size(0), -1)\n        \n        # two linear layers with dropout in between\n        x = F.relu(self.fc1(x))\n        x = self.fc1_drop(x)\n        x = self.fc2(x)\n        \n        # final output\n        return x\n\nnet = Net()\nprint(net)","861277a3":"import torch.optim as optim\n\n# using cross entropy whcih combines softmax and NLL loss\ncriterion = nn.CrossEntropyLoss()\n\n# stochastic gradient descent with a small learning rate AND some momentum\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)","45da82ab":"# Calculating accuracy before training\ncorrect = 0\ntotal = 0\n\n# Iterating through test dataset\nfor images, labels in test_loader:\n\n    # forward pass to get outputs\n    # the outputs are a series of class scores\n    outputs = net(images)\n\n    # the predicted class from the maximum value in the output-list of class scores\n    _, predicted = torch.max(outputs.data, 1)\n\n    # counting up total number of correct labels\n    # for which the predicted and true labels are equal\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\n# the accuracy\n# to convert `correct` from a Tensor into a scalar, .item() is used.\naccuracy = 100.0 * correct.item() \/ total\n\nprint('Accuracy before training: ', accuracy)","1bc4b97d":"def train(n_epochs):\n    \n    loss_over_time = [] # to track the loss as the network trains\n    \n    for epoch in range(n_epochs):  # loop over the dataset multiple times\n        \n        running_loss = 0.0\n        \n        for batch_i, data in enumerate(train_loader):\n\n            inputs, labels = data\n\n            # zero the parameter (weight) gradients\n            optimizer.zero_grad()\n\n            # forward pass to get outputs\n            outputs = net(inputs)\n\n            # calculating the loss\n            loss = criterion(outputs, labels)\n\n            # backward pass to calculate the parameter gradients\n            loss.backward()\n\n            # updating the parameters\n            optimizer.step()\n\n            # printing loss statistics\n            # to convert loss into a scalar and add it to running_loss, we use .item()\n            running_loss += loss.item()\n            \n            if batch_i % 1000 == 999:    # printing every 1000 batches\n                avg_loss = running_loss\/1000\n                # printing the avg loss over the 1000 batches\n                loss_over_time.append(avg_loss)\n                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, avg_loss))\n                running_loss = 0.0\n\n    print('Finished Training')\n    return loss_over_time\n","933aab5a":"n_epochs = 25 \n\ntraining_loss = train(n_epochs)","8613fb68":"# visualizing the loss as the network trained\nplt.plot(training_loss)\nplt.xlabel('1000\\'s of batches')\nplt.ylabel('loss')\nplt.ylim(0, 2.5) # consistent scale\nplt.show()","3c8743a6":"# initializing tensor and lists to monitor test loss and accuracy\ntest_loss = torch.zeros(1)\nclass_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\n\n# setting the module to evaluation mode\nnet.eval()\n\nfor batch_i, data in enumerate(test_loader):\n    \n    inputs, labels = data\n    \n    # forward pass to get outputs\n    outputs = net(inputs)\n\n    # calculating the loss\n    loss = criterion(outputs, labels)\n            \n    # updating average test loss \n    test_loss = test_loss + ((torch.ones(1) \/ (batch_i + 1)) * (loss.data - test_loss))\n    \n    # getting the predicted class from the maximum value in the output-list of class scores\n    _, predicted = torch.max(outputs.data, 1)\n    \n    # comparing predictions to true label\n    # this creates a `correct` Tensor that holds the number of correctly classified images in a batch\n    correct = np.squeeze(predicted.eq(labels.data.view_as(predicted)))\n    \n    # calculating test accuracy for *each* object class\n    # we get the scalar value of correct items for a class, by calling `correct[i].item()`\n    for i in range(batch_size):\n        label = labels.data[i]\n        class_correct[label] += correct[i].item()\n        class_total[label] += 1\n\nprint('Test Loss: {:.6f}\\n'.format(test_loss.numpy()[0]))\n\nfor i in range(10):\n    if class_total[i] > 0:\n        print('Test Accuracy of %5s: %2d%% (%2d\/%2d)' % (\n            classes[i], 100 * class_correct[i] \/ class_total[i],\n            np.sum(class_correct[i]), np.sum(class_total[i])))\n    else:\n        print('Test Accuracy of %5s: N\/A (no training examples)' % (classes[i]))\n\n        \nprint('\\nTest Accuracy (Overall): %2d%% (%2d\/%2d)' % (\n    100. * np.sum(class_correct) \/ np.sum(class_total),\n    np.sum(class_correct), np.sum(class_total)))","4f1193a1":"# obtaining one batch of test images\ndataiter = iter(test_loader)\nimages, labels = dataiter.next()\n# getting predictions\npreds = np.squeeze(net(images).data.max(1, keepdim=True)[1].numpy())\nimages = images.numpy()\n\n# plotting the images in the batch, along with predicted and true labels\nfig = plt.figure(figsize=(25, 4))\nfor idx in np.arange(batch_size):\n    ax = fig.add_subplot(2, batch_size\/2, idx+1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))","d42d70ac":"### High and Low pass Filters","1c6ef7de":"### Convolutional layer \n\nSingle convolutional layer contains all created filters. Note that we are not training this network; we are initializing the weights in a convolutional layer so that we can visualize what happens after a forward pass through this network!","6bb2b453":"## Feature Extraction\n\nA feature that represents the brightness in an image. We'll be extracting the **average brightness** using HSV colorspace. Specifically, we'll use the V channel (a measure of brightness), add up the pixel values in the V channel, then divide that sum by the area of the image to get the average Value of the image.","081813d6":"### Determining Accuracy","06582b72":"### Training and Testing Data\nThe 200 day\/night images are separated into training and testing datasets. \n\n* 60% of these images are training images\n* 40% are test images, which will be used to test the accuracy of classifier.","8cc5ca2b":"### RGB Channels","b09dcbb5":"### Mask","71aef2c5":"### Harris Corner Detection","26ae7088":"### Visualizing some training data\n\nThis cell iterates over the training dataset, loading a random batch of image\/label data, using `dataiter.next()`. It then plots the batch of images and labels in a `2 x batch_size\/2` grid.","e70c898e":"### Testing Classifier","ee1b94dc":"**Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do.**\n\n**Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.**\n\n[Source : Wiki](https:\/\/en.wikipedia.org\/wiki\/Computer_vision)","286c7e83":"# Images and Colors","7cb31fa4":"### Canny Edge Detection","c6748b57":"### A note on accuracy\n\nIt's interesting to look at the accuracy of network **before and after** training. This way we can really see that your network has learned something. In the next cell, let's see what the accuracy of an untrained network is (we expect it to be around 10% which is the same accuracy as just guessing for all 10 classes).","3cbea17e":"### K-Means Clustering","8a065bbd":"### Preprocessed images with labels","826dd64c":"### Hough Lines","4b0d73d8":"## Visualizing the loss\n\nA good indication of how much network is learning as it trains is the loss over time. In this example, we printed and recorded the average loss for each 1000 batches and for each epoch. Let's plot it and see how the loss decreases (or doesn't) over time. \n\nIn this case, we should see that the loss has an initially large decrease and even looks like it would decrease more (by some small, linear amount) if we let it train for more epochs.","ceac37cb":"## Day and Night Image Classifier\n\nThe day\/night image dataset consists of 200 RGB color images in two categories: day and night. There are equal numbers of each example: 100 day images and 100 night images.\n\nWe'd like to build a classifier that can accurately label these images as day or night, and that relies on finding distinguishing features between the two types of images!\n\n*Note: All images come from the [AMOS dataset](http:\/\/cs.uky.edu\/~jacobs\/datasets\/amos\/) (Archive of Many Outdoor Scenes).*\n","8556b8a4":"### HSV","35a6d441":"### The Network Architecture\n\nThe various layers that make up any neural network are documented, [here](http:\/\/pytorch.org\/docs\/master\/nn.html). For a convolutional neural network, we'll use a simple series of layers:\n* Convolutional layers\n* Maxpooling layers\n* Fully-connected (linear) layers\n\nYou are also encouraged to look at adding [dropout layers](http:\/\/pytorch.org\/docs\/stable\/nn.html#dropout) to avoid overfitting this data.\n\n---\n\nTo define a neural network in PyTorch, We define the layers of a model in the function `__init__` and define the feedforward behavior of a network that employs those initialized layers in the function `forward`, which takes in an input image tensor, `x`. The structure of this Net class is shown below.\n\nNote: During training, PyTorch will be able to perform backpropagation by keeping track of the network's feedforward behavior and using autograd to calculate the update to the weights in the network.\n\n#### Define the Layers in ` __init__`\n\n```\n# 1 input image channel (for grayscale images), 32 output channels\/feature maps, 3x3 square convolution kernel\nself.conv1 = nn.Conv2d(1, 32, 3)\n\n# maxpool that uses a square window of kernel_size=2, stride=2\nself.pool = nn.MaxPool2d(2, 2)      \n```\n\n#### Layers in `forward`\n\nconv1 layer has a ReLu activation applied to it before maxpooling is applied:\n```\nx = self.pool(F.relu(self.conv1(x)))\n```\n\nYou must place any layers with trainable weights, such as convolutional layers, in the `__init__` function and refer to them in the `forward` function; any layers or functions that always behave in the same way, such as a pre-defined activation function, may appear *only* in the `forward` function. In practice, we'll often see conv\/pool layers defined in `__init__` and activations defined in `forward`.\n\n#### Convolutional layer\nThe first convolution layer has been defined and it takes in a 1 channel (grayscale) image and outputs 10 feature maps as output, after convolving the image with 3x3 filters.\n\n#### Flattening\n\nTo move from the output of a convolutional\/pooling layer to a linear layer, we must first flatten our extracted features into a vector. If we've used the deep learning library, Keras, we may have seen this done by `Flatten()`, and in PyTorch we can flatten an input `x` with `x = x.view(x.size(0), -1)`.\n","9d36468c":"Finally, Classifying Images Using CNN","a7ff210a":"### Contour Detection\nEvery contour has a number of features that can be calculated, including the area of the contour, it's orientation (the direction that most of the contour is pointing in), it's perimeter, and many other properties outlined in [OpenCV documentation, here](http:\/\/opencv-python-tutroals.readthedocs.io\/en\/latest\/py_tutorials\/py_imgproc\/py_contours\/py_contour_properties\/py_contour_properties.html).\n","8af6c819":"Low frequencies are at the center of the frequency transform image. \n\nThe transform images for these example show that the solid image has most low-frequency components (as seen by the center bright spot). \n\nThe stripes tranform image contains low-frequencies for the areas of white and black color and high frequencies for the edges in between those colors. The stripes transform image also tells us that there is one dominating direction for these frequencies; vertical stripes are represented by a horizontal line passing through the center of the frequency transform image.","1a1d02eb":"# Applications of Computer Vision","0f62f01a":"### Sobel Filter","77cad4ec":"# Image Filters","f77ff25c":"### Training the Network\n\nBelow, we've defined a `train` function that takes in a number of epochs to train for. \n* The number of epochs is how many times a network will cycle through the entire training dataset. \n* Inside the epoch loop, we loop over the training dataset in batches; recording the loss every 1000 batches.\n\nHere are the steps that this training function performs as it iterates over the training dataset:\n\n1. Zero's the gradients to prepare for a forward pass\n2. Passes the input through the network (forward pass)\n3. Computes the loss (how far is the predicted classes are from the correct labels)\n4. Propagates gradients back into the network\u2019s parameters (backward pass)\n5. Updates the weights (parameter update)\n6. Prints out the calculated loss","3b7e8b15":"### Fourier Transforms\n\nThe frequency components of an image can be displayed after doing a Fourier Transform (FT). An FT looks at the components of an image (edges that are high-frequency, and areas of smooth color as low-frequency), and plots the frequencies that occur as points in spectrum.\n\nIn fact, an FT treats patterns of intensity in an image as sine waves with a particular frequency, and you can look at an interesting visualization of these sine wave components [on this page](https:\/\/plus.maths.org\/content\/fourier-transforms-images).\n","4268d82f":"**A note on parameters** \n\nHow many faces are detected is determined by the function, `detectMultiScale` which aims to detect faces of varying sizes. The inputs to this function are: `(image, scaleFactor, minNeighbors)`; you will often detect more faces with a smaller scaleFactor, and lower value for minNeighbors, but raising these values often produces better matches. Modify these values depending on your input image.","ce58f959":"### Visualizing sample test results","97149857":"**Applications of computer vision range from automation in self-driving vehicles to developing accurate facial recognition software to inspecting bottles in a manufacturing production line to controlling robots, to organizing information for example indexing databases of images.**\n\n**Computer vision, as well as AI and machine learning concepts, are key to realising complete, or Level 5, automation in self-driving vehicles. Here CV software analyses data from cameras placed around the car. This allows the car to identify other vehicles and pedestrians as well as reading road signs.**\n\n**Computer vision is also key to developing accurate facial recognition software. This is regularly being implemented by law enforcement agencies as well as helping to authenticate consumer device ownership.**\n\n**Augmented and mixed reality technologies are increasingly being implemented in smartphones and tablets.**\n\n**Additionally, smart glasses are also becoming more widely available.**\n\n**All of this requires computer vision to help determine the location, detect objects and establish the depth or dimensions of the virtual world.**\n\n[Source](https:\/\/algorithmxlab.com\/blog\/computer-vision\/#What_are_Computer_Vision_Applications)","f065c43a":"# What is Computer Vision?","2c5b7629":"### Pooling Layer","83334a48":"### Testing the Trained Network\n\nOnce we are satisfied with how the loss of your model has decreased, there is one last step: test!\n\nWe must test your trained model on a previously unseen dataset to see if it generalizes well and can accurately classify this new dataset. For FashionMNIST, which contains many pre-processed training images, a good model should reach **greater than 85% accuracy** on this test dataset. If we are not reaching this value, then we must try training for a larger number of epochs, tweaking hyperparameters, or adding\/subtracting layers from CNN.","76acb697":"# Classifying Images based on Features","f4a22121":"# Understanding Images","93364e81":"### Classifier","a2697b52":"### We will use Pytorch for DL","4aec91c1":"### The loss function and optimizer\n\nLearn more about [loss functions](http:\/\/pytorch.org\/docs\/master\/nn.html#loss-functions) and [optimizers](http:\/\/pytorch.org\/docs\/master\/optim.html) in the online documentation.\n\nNote that for a classification problem like this, one typically uses cross entropy loss, which can be defined in code like: `criterion = nn.CrossEntropyLoss()`. PyTorch also includes some standard stochastic optimizers like stochastic gradient descent and Adam. ","dea49491":"**OpenCV is a very popular library for computer vision applications. You can check the docs** [HERE](https:\/\/docs.opencv.org\/3.4\/) ","d9688ad6":"### This notebook is for beginners who want to learn Computer Vision and this notebook is a perfect introduction to start with. I have dedicated my time in creating this content from my  learning of Computer Vision from Udacity and I want to dedicate my learning to all.\n<h3 align=center style=\"color:red\">If you like my effort then please upvote this notebook.<\/h3>","89f562c5":"## Color Threshold","b5d37511":"# Convolutional Neural Networks","be954910":"#### Please be Patient it will take a couple of minutes as we are not using GPU here.","2b7ef175":"# Table of Content\n1. What is Computer Vision?\n2. Computer Vision and Applications\n3. Understanding Images\n4. Images and Colors\n5. Classifying Images based on Features\n6. Image Filters\n7. Face Detection\n8. Image Features\n9. Convolutional Neural Networks\n10. Conquering Fashion MNIST with CNN","5588d0af":"# Image Features","10ca4bf1":"### Image Pyramids","0a777d19":"<big><h1 align=\"center\"> Introduction to Deep Learning for Computer Vision <\/h1><\/big>","c8496167":"### Filters","ff91869d":"### Hough Circles","809ed41f":"### Gaussian Blur","9b3c9c15":"# Face Detection\n\nOne older, but still popular scheme for face detection is a Haar cascade classifier; these classifiers in the OpenCV library and use feature-based classification cascades that learn to isolate and detect faces in an image.[the original paper proposing this approach here](https:\/\/www.cs.cmu.edu\/~efros\/courses\/LBMV07\/Papers\/viola-cvpr-01.pdf).\n\nLet's see how face detection works on an exampe.","6e794ad7":"# Fashion MNIST Classification using CNN","6e21786a":"### Visualizing an Image"}}