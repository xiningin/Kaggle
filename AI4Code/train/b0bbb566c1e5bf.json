{"cell_type":{"ab447f5d":"code","f30fb3dc":"code","8437c12b":"code","d2ac308d":"code","f69fe9fb":"code","26c47e94":"code","f15701b5":"code","cb45cd0b":"code","7ddcd79a":"code","aa1cef3d":"code","14d5f3d5":"code","8dffc758":"code","3c550137":"code","8a53a38d":"code","51ce9300":"code","fd8a26b0":"code","da121a07":"code","2fb0d589":"code","bd04a596":"code","3a04c9a4":"code","0a548fbf":"code","56df695a":"code","e651c41e":"code","24b459d8":"code","958fcc49":"code","eba708ae":"code","4ad4b9a5":"code","948f0b22":"code","2cabed4c":"code","cbcbf7ed":"code","4084705f":"code","28dde544":"code","709e7b6a":"code","f8eb095b":"code","130ffd06":"code","7c383e56":"code","9eb9c0d8":"code","1101d09f":"code","7d5ed999":"code","8dd71f78":"code","7248daba":"code","c437220e":"code","25ece2a8":"code","c70f06a0":"code","8cbf1719":"code","c67cdb1a":"code","8cde7e18":"code","6594a8ae":"code","2d904133":"code","7c170adb":"code","f7a04632":"code","f10dc986":"code","c06bfb0e":"code","7e93306d":"code","22a205d9":"code","54d0e40b":"code","3f963987":"code","eb678f0d":"code","29d4f8e7":"code","a9018094":"code","bc88a1b1":"code","5cd52680":"code","e09d7bbb":"code","67d2e2b7":"code","de6a71bf":"markdown","c9acf05c":"markdown","6d8b356c":"markdown","186167a4":"markdown","e3ac0029":"markdown","661e8c49":"markdown","ac97a89b":"markdown","e4ae7bb2":"markdown","b7f6f603":"markdown","b905bb2f":"markdown","1d0e221d":"markdown","bffa7ee6":"markdown","420e9389":"markdown","2619ae83":"markdown","757a5942":"markdown","61b9b5b1":"markdown","3a8bba17":"markdown","24a4f0de":"markdown","c6200431":"markdown","53bc7eda":"markdown","e6de7da6":"markdown","b94d4a9c":"markdown","60d2bb74":"markdown","ccc5b762":"markdown","114ef1e7":"markdown","fe9175d7":"markdown","dcb43091":"markdown","b24f062e":"markdown","ef4cdf08":"markdown","73983dbc":"markdown","e0daf3ea":"markdown","253e3fff":"markdown","2bdb9cd9":"markdown","087e3b7a":"markdown","23d47a37":"markdown","990782a9":"markdown","b6acb914":"markdown","2e6bc79d":"markdown","cc7686b0":"markdown","42d5ac1f":"markdown","252fc177":"markdown","c500e67a":"markdown","8863b859":"markdown","78e364b2":"markdown"},"source":{"ab447f5d":"import pandas as pd # for data processing and analysis modeled\nimport matplotlib   # for scientific and visualization\nimport numpy as np  # for scientific computing\nimport scipy as sp  # for scientific computing and mathematics Functions\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport IPython \nfrom IPython import display #  printing of dataframes in Jupyter notebook\nimport sklearn      # for machine learning algorithms\n\nimport seaborn as sns\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\n\n#misc libraries\nimport random\nimport time\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","f30fb3dc":"from subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n!pwd\n!ls ","8437c12b":"data_raw = pd.read_csv('..\/input\/titanic\/train.csv') # this is the data for training and our Evaluation\n\ndata_val  = pd.read_csv('..\/input\/titanic\/test.csv') # this is provided by Kaggle and to be used to Submit the final Predictions\n\n# make a copy for future usage to check on data.\ndata_train = data_raw.copy(deep = True)\ndata_test = data_val.copy(deep = True)\n\nprint (data_train.info())\nprint (\"#\"*50)\nprint (data_test.info())\nprint (\"#\"*50)\n\n\"\"\"\nCombine both Test and Train Datasets for doing analysis on Categorical values (Classes) that may be present \nonly in Test but not in Training Dataset\n\"\"\"\ndata_combine = [data_train, data_test]\n","d2ac308d":"data_train.head(15)","f69fe9fb":"data_train.describe(include=['O'])","26c47e94":"data_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","f15701b5":"data_train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","cb45cd0b":"data_train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","7ddcd79a":"data_train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","aa1cef3d":"data_train[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","14d5f3d5":"plt.hist(x = [data_train[data_train['Survived']==1]['Age'], data_train[data_train['Survived']==0]['Age']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Age Histogram by Survival')\nplt.xlabel('Age (Years)')\nplt.ylabel('# of Passengers')\nplt.legend()","8dffc758":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(data_train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","3c550137":"# grid = sns.FacetGrid(train_df, col='Embarked')\ngrid = sns.FacetGrid(data_train, row='Embarked', height=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","8a53a38d":"grid = sns.FacetGrid(data_train, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","51ce9300":"print(\"Before\", data_train.shape, data_test.shape, data_combine[0].shape, data_combine[1].shape)\n\ndata_train = data_train.drop(['Ticket', 'Cabin'], axis=1)\ndata_test = data_test.drop(['Ticket', 'Cabin'], axis=1)\ndata_combine = [data_train, data_test]\n\nprint(\"After\", data_train.shape, data_test.shape, data_combine[0].shape, data_combine[1].shape)","fd8a26b0":"for dataset in data_combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(data_train['Title'], data_train['Sex'])","da121a07":"for dataset in data_combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ndata_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","2fb0d589":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in data_combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ndata_train.head()","bd04a596":"data_train = data_train.drop(['Name', 'PassengerId'], axis=1)\ndata_test = data_test.drop(['Name'], axis=1)\ndata_combine = [data_train, data_test]\ndata_train.shape, data_test.shape","3a04c9a4":"for dataset in data_combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ndata_train.head()","0a548fbf":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\ngrid = sns.FacetGrid(data_train, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","56df695a":"guess_ages = np.zeros((2,3))\nguess_ages","e651c41e":"for dataset in data_combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n            age_guess = guess_df.median()\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ndata_train.head()","24b459d8":"data_train['AgeBand'] = pd.cut(data_train['Age'], 5)\ndata_train[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","958fcc49":"for dataset in data_combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ndata_train.head()","eba708ae":"#AgeBand feature can be removed\ndata_train = data_train.drop(['AgeBand'], axis=1)\ndata_combine = [data_train, data_test]\ndata_train.head()","4ad4b9a5":"for dataset in data_combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ndata_train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","948f0b22":"# another new feature called IsAlone. \n\nfor dataset in data_combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ndata_train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","2cabed4c":"# with IsAlone field with good correlation with Survived Feature, can drop Parch, Sibsp and Familysize Features\ndata_train = data_train.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ndata_test = data_test.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ndata_combine = [data_train, data_test]\n\ndata_train.head()","cbcbf7ed":"# Another New feature combining Pclass and Age.\nfor dataset in data_combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ndata_train.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","4084705f":"freq_port = data_train.Embarked.dropna().mode()[0]\n\nfor dataset in data_combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ndata_train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","28dde544":"for dataset in data_combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ndata_train.head()","709e7b6a":"data_test['Fare'].fillna(data_test['Fare'].dropna().median(), inplace=True)\ndata_test.head()","f8eb095b":"# Fare has continous numeric data and hence to be converted to category range to make it categorical\ndata_train['FareBand'] = pd.qcut(data_train['Fare'], 4)\ndata_train[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","130ffd06":"# Convert the Fare feature to ordinal values based on the FareBand\nfor dataset in data_combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ndata_train = data_train.drop(['FareBand'], axis=1)\ndata_combine = [data_train, data_test]","7c383e56":"print (data_train.head(10))\nprint (\"#\"*75)\nprint (data_test.head(10))","9eb9c0d8":"import scipy.stats as stats\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(8, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        data_train.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.6 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=12)\n\ncorrelation_heatmap(data_train)","1101d09f":"#set correlation above 0.75 and see true\/false values\nabs(data_train.corr())> 0.50","7d5ed999":"sns.heatmap(data_train.corr(), center=0);","8dd71f78":"X_train = data_train.drop(\"Survived\", axis=1)\nY_train = data_train[\"Survived\"]\nX_test  = data_test.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","7248daba":"print (X_train.columns)\nprint (\"#\"*50)\nprint (X_test.columns)\nprint (\"#\"*50)\nX_train.drop(\"Embarked\", axis=1)\nX_test.drop(\"Embarked\", axis=1)","c437220e":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics","25ece2a8":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log\nprint ('Test ACC Logistic Regression -- > ', acc_log)\n\n# generating ROC and RMSC for training just to get hang of if RMSE is going down or up with each model\nX_pred = logreg.predict(X_train)\nX_predprob = logreg.predict_proba(X_train)[:,1]\nt_lr_score = metrics.accuracy_score(Y_train, X_pred)\nprint ('Score -- >', t_lr_score)\nt_lr_roc = metrics.roc_auc_score(Y_train, X_predprob)\nprint ('ROC -- > ',  t_lr_roc)\nt_lr_rmse = metrics.mean_squared_error(Y_train, X_predprob)\nprint ('RMSC -- > ',  t_lr_rmse)","c70f06a0":"coeff_df = pd.DataFrame(data_train.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","8cbf1719":"# Support Vector Machines\n\nsvc = SVC(probability=True) \nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc\nprint ('Test ACC SVM -- > ', acc_svc)\n# generating ROC and RMSC for training just to get hang of if RMSE is going down or up with each model\nX_pred = svc.predict(X_train)\nX_predprob = svc.predict_proba(X_train)[:,1]\nt_svm_score = metrics.accuracy_score(Y_train, X_pred)\nprint ('Score -- >', t_svm_score)\nt_svm_roc = metrics.roc_auc_score(Y_train, X_predprob)\nprint ('ROC -- > ',  t_svm_roc)\nt_svm_rmse = metrics.mean_squared_error(Y_train, X_predprob)\nprint ('RMSC -- > ',  t_svm_rmse)","c67cdb1a":"from sklearn.model_selection import GridSearchCV \n  \n# defining parameter range \nparam_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['linear', 'rbf']}  \n  \n#grid = GridSearchCV(LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr'), param_grid, refit = True, verbose = 2) \ngrid = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr')\ngrid.fit(X_train, Y_train) \nY_pred = grid.predict(X_test)\nacc_svc1 = round(grid.score(X_train, Y_train) * 100, 2)\nprint ('Test ACC LinearSVC -- > ', acc_svc)","8cde7e18":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn\nprint ('Test ACC KNN ', acc_knn)\n\nX_pred = knn.predict(X_train)\nX_predprob = knn.predict_proba(X_train)[:,1]\nt_knn_score = metrics.accuracy_score(Y_train, X_pred)\nprint ('Score -- >', t_knn_score)\nt_knn_roc = metrics.roc_auc_score(Y_train, X_predprob)\nprint ('ROC -- > ',  t_knn_roc)\nt_knn_rmse = metrics.mean_squared_error(Y_train, X_predprob)\nprint ('RMSC -- > ',  t_knn_rmse)","6594a8ae":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian\nprint ('Test ACC Naive ', acc_gaussian)\n\nX_pred = gaussian.predict(X_train)\nX_predprob = gaussian.predict_proba(X_train)[:,1]\nt_nb_score = metrics.accuracy_score(Y_train, X_pred)\nprint ('Score -- >', t_nb_score)\nt_nb_roc = metrics.roc_auc_score(Y_train, X_predprob)\nprint ('ROC -- > ',  t_nb_roc)\nt_nb_rmse = metrics.mean_squared_error(Y_train, X_predprob)\nprint ('RMSC -- > ',  t_nb_rmse)","2d904133":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","7c170adb":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","f7a04632":"# Gradient Descent\n\ngd = GradientBoostingClassifier()\ngd.fit(X_train, Y_train)\nY_pred = gd.predict(X_test)\nacc_gd = round(gd.score(X_train, Y_train) * 100, 2)\nacc_gd\nprint ('Test ACC Gradient Descent', acc_gd)\n\nX_pred = gd.predict(X_train)\nX_predprob = gd.predict_proba(X_train)[:,1]\nt_gd_score = metrics.accuracy_score(Y_train, X_pred)\nprint ('Score -- >', t_gd_score)\nt_gd_roc = metrics.roc_auc_score(Y_train, X_predprob)\nprint ('ROC -- > ',  t_gd_roc)\nt_gd_rmse = metrics.mean_squared_error(Y_train, X_predprob)\nprint ('RMSC -- > ',  t_gd_rmse)","f10dc986":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier(loss='log')\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd\nprint ('Test ACC Stochastic Gradient Descent', acc_sgd)\nX_pred = sgd.predict(X_train)\nX_predprob = sgd.predict_proba(X_train)[:,1]\nt_sgd_score = metrics.accuracy_score(Y_train, X_pred)\nprint ('Score -- >', t_sgd_score)\nt_sgd_roc = metrics.roc_auc_score(Y_train, X_predprob)\nprint ('ROC -- > ',  t_sgd_roc)\nt_sgd_rmse = metrics.mean_squared_error(Y_train, X_predprob)\nprint ('RMSC -- > ',  t_sgd_rmse)","c06bfb0e":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree\nprint ('Test ACC Decision Tree', acc_decision_tree)\nX_pred = decision_tree.predict(X_train)\nX_predprob = decision_tree.predict_proba(X_train)[:,1]\nt_dt_score = metrics.accuracy_score(Y_train, X_pred)\nprint ('Score -- >', t_dt_score)\nt_dt_roc = metrics.roc_auc_score(Y_train, X_predprob)\nprint ('ROC -- > ',  t_dt_roc)\nt_dt_rmse = metrics.mean_squared_error(Y_train, X_predprob)\nprint ('RMSC -- > ',  t_dt_rmse)","7e93306d":"from sklearn.ensemble import BaggingClassifier\n\nbag_cla = BaggingClassifier()\nbag_cla.fit(X_train, Y_train)\n\ny_pred=bag_cla.predict(X_test)\nacc_bag_cla = round(bag_cla.score(X_train, Y_train) * 100, 2)\n\nprint ('Test ACC Bagging Classifier', acc_bag_cla)\nX_pred = bag_cla.predict(X_train)\nX_predprob = bag_cla.predict_proba(X_train)[:,1]\nt_bc_score = round(metrics.accuracy_score(Y_train, X_pred) *100,2)\nprint ('Score -- >', t_bc_score)\nt_bc_roc = metrics.roc_auc_score(Y_train, X_predprob)\nprint ('ROC -- > ',  t_bc_roc)\nt_bc_rmse = metrics.mean_squared_error(Y_train, X_predprob)\nprint ('RMSC -- > ',  t_bc_rmse)\n# Summary of the predictions made by the classifier\n#print(classification_report(test1_y_dummy,y_pred))\n#print(confusion_matrix(y_pred,test1_y_dummy))\n\n#Accuracy Score\n#print('accuracy is ',accuracy_score(y_pred,test1_y_dummy))\n\n#BCC = accuracy_score(y_pred,test1_y_dummy)","22a205d9":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest\nprint ('Test ACC Random Forest', acc_random_forest)\nX_pred = random_forest.predict(X_train)\nX_predprob = random_forest.predict_proba(X_train)[:,1]\nt_rm_score = metrics.accuracy_score(Y_train, X_pred)\nprint ('Score -- >', t_rm_score)\nt_rm_roc = metrics.roc_auc_score(Y_train, X_predprob)\nprint ('ROC -- > ',  t_rm_roc)\nt_rm_rmse = metrics.mean_squared_error(Y_train, X_predprob)\nprint ('RMSC -- > ',  t_rm_rmse)","54d0e40b":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(n_estimators=100)\nxgb.fit(X_train, Y_train)\nY_pred_xgb=xgb.predict(X_test)\nxgb.score(X_train, Y_train)\nacc_xgb = round(xgb.score(X_train, Y_train) * 100, 2)\nacc_xgb\nprint ('Test ACC XGBoost', acc_xgb)\nX_pred = xgb.predict(X_train)\nX_predprob = xgb.predict_proba(X_train)[:,1]\nt_xgb_score = metrics.accuracy_score(Y_train, X_pred)\nprint ('Score -- >', t_xgb_score)\nt_xgb_roc = metrics.roc_auc_score(Y_train, X_predprob)\nprint ('ROC -- > ',  t_xgb_roc)\nt_xgb_rmse = metrics.mean_squared_error(Y_train, X_predprob)\nprint ('RMSC -- > ',  t_xgb_rmse)\n\n#Parameters list can be found here as well: https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n","3f963987":"## have tunned parameters using GridSearch and randomly to come up with better score\n## refer to this Notebook \nxgb_tuned = XGBClassifier(\n learning_rate =0.1,\n n_estimators=143,\n max_depth=5,\n min_child_weight=1,\n gamma=0.0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27\n)\nxgb_tuned.fit(X_train, Y_train)\nY_pred_tuned=xgb_tuned.predict(X_test)\nxgb_tuned.score(X_train, Y_train)\nacc_xgb_tuned = round(xgb_tuned.score(X_train, Y_train) * 100, 2)\nacc_xgb_tuned\nprint ('Test ACC XGBoost Tunned', acc_xgb_tuned)\nX_pred = xgb_tuned.predict(X_train)\nX_predprob = xgb_tuned.predict_proba(X_train)[:,1]\nt_xgbt_score = metrics.accuracy_score(Y_train, X_pred)\nprint ('Score -- >', t_xgbt_score)\nt_xgbt_roc = metrics.roc_auc_score(Y_train, X_predprob)\nprint ('ROC -- > ',  t_xgbt_roc)\nt_xgbt_rmse = metrics.mean_squared_error(Y_train, X_predprob)\nprint ('RMSC -- > ',  t_xgbt_rmse)","eb678f0d":"#https:\/\/catboost.ai\/docs\/concepts\/python-reference_parameters-list.html\n\nfrom catboost import CatBoostClassifier\n\ncatb=CatBoostClassifier(iterations=2500, depth=5, learning_rate=0.3, verbose=0, \n                        allow_writing_files=False, loss_function='CrossEntropy', random_strength=0.1, leaf_estimation_method='Gradient') \ncatb.fit(X_train, Y_train)\n\ny_pred=catb.predict(X_test)\nacc_catb = round(catb.score(X_train, Y_train) * 100, 2)\n\nprint ('Test ACC CatBoost', acc_catb)\nX_pred = catb.predict(X_train)\nX_predprob = catb.predict_proba(X_train)[:,1]\nt_catb_score = metrics.accuracy_score(Y_train, X_pred)\nprint ('Score -- >', t_catb_score)\nt_catb_roc = metrics.roc_auc_score(Y_train, X_predprob)\nprint ('ROC -- > ',  t_catb_roc)\nt_catb_rmse = metrics.mean_squared_error(Y_train, X_predprob)\nprint ('RMSC -- > ',  t_catb_rmse)","29d4f8e7":"from sklearn.ensemble import VotingClassifier\n\nclr = LogisticRegression()\ncsvc = SVC(probability=True) \ncknn = KNeighborsClassifier(n_neighbors = 3)\ncgau = GaussianNB()\ncgb = GradientBoostingClassifier()\ncsgb = SGDClassifier (loss='log')\ncrf = RandomForestClassifier(n_estimators=100)\ncxgbt = XGBClassifier(learning_rate =0.1, n_estimators=143, max_depth=5, min_child_weight=1, gamma=0.0, subsample=0.8,\n                      colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\ncdt = DecisionTreeClassifier()\ncbc = BaggingClassifier()\nccb = CatBoostClassifier(iterations=2500, depth=5, learning_rate=0.3, verbose=0, \n                        allow_writing_files=False #, train_dir='\/gdrive\/My Drive\/MyLearning\/MLDLAIPython\/Data\/TextData\/'\n                        , loss_function='CrossEntropy', random_strength=0.1, leaf_estimation_method='Gradient') \n\"\"\"\neclf1 = VotingClassifier(estimators=[('LogReg', clr), ('SVC', csvc), ('KNN', cknn), ('GradientBoost', cgb), \n                                    ('StochaisticGB', csgb), ('RandomForest', crf), ('XGB', cxgb), \n                                    ('DecisionTree', cdt), ('BaggClassifier', cbc), ('CatBoost', ccb)], voting='soft')\n\"\"\"\neclf1 = VotingClassifier(estimators=[('RandomForest', crf), ('DecisionTree', cdt), ('BaggClassifier', cbc), ('CatBoost', ccb)], voting='soft')\n\neclf1.fit(X_train, Y_train)\ny_pred=eclf1.predict(X_test)\nacc_eclf1 = round(eclf1.score(X_train.astype(float), Y_train.astype(float)).astype(float) * 100, 2)\n#acc_eclf1 = eclf1.score(X_train.astype('float64'), Y_train.astype('float64'))\n\nprint ('Test ACC Voting Classifier ', acc_eclf1)\nX_pred = eclf1.predict(X_train.astype(float))\nX_predprob = eclf1.predict_proba(X_train)[:,1]\nt_eclf1_score = metrics.accuracy_score(Y_train, X_pred)\nprint ('Score -- >', t_eclf1_score)\nt_eclf1_roc = metrics.roc_auc_score(Y_train, X_predprob)\nprint ('ROC -- > ',  t_eclf1_roc)\nt_eclf1_rmse = metrics.mean_squared_error(Y_train, X_predprob)\nprint ('RMSC -- > ',  t_eclf1_rmse)\neclf1.fit(X_train, Y_train)\ny_pred=catb.predict(X_test)\nacc_eclf1 = round(eclf1.score(X_train.astype(float), Y_train.astype(float)).astype(float) * 100, 2)\n","a9018094":"from sklearn.model_selection import cross_val_score\n\"\"\"\nfor clf, label in zip([clr, csvc, cknn, cgb, csgb, crf, cxgb, cdt, cbc, ccb, eclf1], ['Logistic Regression', \n'KNN', 'GradientBoosting', 'StochasticGB', 'RandomForest', 'XGB', 'Decision Tree', 'Bagging Classifier', \n'CatBoost', 'Voting Classifier']) :\n\"\"\"\nfor clf, label in zip([crf, cdt, cbc, ccb, eclf1], ['RandomForest',  'Decision Tree', 'Bagging Classifier', \n                                                    'CatBoost', 'Voting Classifier']):  \n  scores = cross_val_score(clf, X_train, Y_train, cv=5, scoring='accuracy')\n  print (\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (round(scores.mean()*100,2), scores.std(), label))","bc88a1b1":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Gradient Descent',\n              'Stochastic Gradient Descent', 'Linear SVC', \n              'Decision Tree', 'XgBoost', 'XgBoost_Tuned', 'Bagging Classifer', 'Cat Boost', 'Voting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_gd,acc_sgd, acc_linear_svc, acc_decision_tree, acc_xgb,acc_xgb_tuned, acc_bag_cla, acc_catb, \n              acc_eclf1],\n    'ROC': [t_svm_roc, t_knn_roc, t_lr_roc, \n              t_rm_roc, t_nb_roc, -100, \n              t_gd_roc,t_sgd_roc, -100, t_dt_roc, t_xgb_roc,t_xgbt_roc, t_bc_roc, t_catb_roc, t_eclf1_roc],\n    'RMSE': [t_svm_rmse, t_knn_rmse, t_lr_rmse, \n              t_rm_rmse, t_nb_rmse, 100, \n              t_gd_rmse,t_sgd_rmse, 100, t_dt_rmse, t_xgb_rmse,t_xgbt_rmse, t_bc_rmse, t_catb_rmse, t_eclf1_rmse]\n    \n})\nmodels.sort_values(by='RMSE', ascending=True)","5cd52680":"X_pred = decision_tree.predict(X_train)\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": data_test[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\n\nsubmission.to_csv(\"..\/working\/submission_TitanicSurvived_pred_26Feb20_1757hr.csv\", index=False)\n\nprint('Validation Data Distribution: \\n', submission['Survived'].value_counts(normalize = True))\nsubmission.sample(10)\n\nprint (\"Not Normalized Counts\")\nsubmission.Survived.value_counts()","e09d7bbb":"X_train.drop['Embarked']\nX_train.columns","67d2e2b7":"X_test.columns","de6a71bf":"We can convert the categorical titles to ordinal.","c9acf05c":"Let us replace Age with ordinals based on these bands.","6d8b356c":"<font color=brown size=4>\nObservations <br>\n<font color=black size=3>\nFemale passengers had much better survival rate than males. Confirms classifying assumption.  <br>\nException in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived. <br>\nMales had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports.  <br>\nPorts of embarkation have varying survival rates for Pclass=3 and among male passengers.  <br>\n\n<font color=brown size=4>\nDecisions <br>\n<font color=black size=3>\nAdd Sex feature to model training.  <br>\nComplete and add Embarked feature to model training.  <br>","186167a4":"<font color=brown size=4> \nCreating new feature extracting from existing <br>\n    <font color=black size=3> \nWe want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.\n\nIn the following code we extract Title feature using regular expressions. The RegEx pattern (\\w+\\.) matches the first word which ends with a dot character within Name feature. The expand=False flag returns a DataFrame.<br>\n<font color=brown size=4> \nObservations<br>\n<font color=black size=3> \nWhen we plot Title, Age, and Survived, we note the following observations.\n\nMost titles band Age groups accurately. For example: Master title has Age mean of 5 years.\nSurvival among Title Age bands varies slightly.\nCertain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).<br>\n\n<font color=brown size=4> \nDecision <br>\n<font color=black size=3> \nWe decide to retain the new Title feature for model training.","e3ac0029":"Let us create Age bands and determine correlations with Survived.","661e8c49":"<font color=brown size=4>\n4.1 Load Data Modelling Libraries <br>\n<font color=black size=3>\nThere are many Predictive Modelling Algorithms. However, below are narrowed down give the given problem of Supervised Learning (as dataset is being used for Training the Model) and the Classification Prediction (if a passenger is survived or not).<br>\n1. Logistic Regression\n2. KNN or k-Nearest Neighbors\n3. Support Vector Machines\n4. Naive Bayes classifier\n5. Decision Tree\n6. Random Forrest and Gradient Descents\n7. Perceptron\n8. XGB\n9. CatBoost\n10. Voting Classifier","ac97a89b":"<font color=brown size=4>\n    Converting categorical feature to numeric <br>\n<font color=black size=3>\nWe can now convert the Embarked feature by creating a new numeric Port feature","e4ae7bb2":"This is my first on Kaggle and hence, in this kernel, I use Kaggle's Getting Started Competition, Titanic: Machine Learning from Disaster, to use my learning of different Aspects of DataScience and ML to beat the odds in coming up with a better Accuracy and low RMSE score.\n\n<font color=brown size=5>\nData Science Frame Work: \n<font color=black size=3>\n1. Define the Problem: Besides Algorithm, Model and Technology is defined, we need to get the Business Problem defined. Typically this happens with various Stakeholders getting together to articulate it for the Technology to provide sutle requirements like  trade-off between False Negatives and True Negative. These are not available in thie competition. However, we will stick to the Kaggle's evaluation criteria that is to predict the Survival Classifier (1 for Survived and 0 for Not-Survived). Will use Python to Build the Model. \n2. Data Collection and Gathering: This would in reality be the task that require lots of efforts and resources. In this, case Kaggle has provided the for downloading (https:\/\/www.kaggle.com\/c\/titanic\/data). But, Data Analysis and Correction w.r.t its integrity, meaning, abberations (Outliers and Missing data) is still to be done and that will be done during Data Clearning activity and thereafter Data Tranformation to make it ready for Machine's consumption. This is usually referred to as Data Wrangling. \n3. Exploratory Data Analysis: To understand data in Satistical terms that is Correlations and Linearity between and among Features. Identifying Univariate and Multivariate variables. This can be done either in Pivot and\/or Graphical representation. This is where the sutle requirement of Hypothesis and rejection of it with Significance and Confidence will have to be done. \n4. Build Model with Data: Preparing the Model to arrive at the rules based on the Data and the Outcome. Data and Expected Outcome will determine the Algorithm to be used. Its not that selecting an Algorithm will be produce the desired output and thruput as it requires the Techiniques and Tricks that are at the Craftman's (call him\/her DataScient) disposal. Typically, this forms the Activity of Building and Training the Model. \n5. Validate the Model: Validation is the critical step and again Craftmanship comes into play in selecting the Data for Validation(s). This step is significant as it eludicidates if the Model is fit to Predict for Known Data (in ML terminology Overfit) or can work equally good with unseen Data. The opposite of Overfitting is Underfitting and that tells us that the Model is not designed for it to grasp the completeness of the Dataset to understand various possibilities. This is also called Generalized Model. In either of the cases, will have to go back to Previous steps to inculcate the required changes for the Model to have Best fit (Training). \n6. Optimize and Strategize: This is task where certain Technical Or Repetative tasks can be given to the Data Engineer and to concerntrate on Optimizing the Model Performance. This is an ongoing tasks as it is expected in real-world that new data keeps coming and required Model to be retrained to maintain the Performance and Prediction Accuracy. <br>\n<font color=brown size=4>\nThis kernal starts with Point 2 <font color=black size=3> as first one is already taken care by Kaggle and First part of point 2, of having Raw Data making available for this Competition is also done by Kaggle as well. <br>\n<font size=3>\nWe can make use of popular Libraries Python3.x for Data Wrangling. <br>\n2.1 Importing Libraries","b7f6f603":"<font color=brown size=4>\n3.1 Load Data Exploratory Libraries","b905bb2f":"<font color=brown size=5> \n    KNN (K-Nearest Neighbours) <br>\n<font color=black size=3> \nKNN is a non-parametric method used for Classification (and Regression). Classification happens with the majority of votes it gets from its neighbours, more votes and the Prediction is assigned to that Classifier Class. K is a positive integer, typically small (K=1). When K is 1, then the object is assigned to the class that of that single nearest neighbour. ","1d0e221d":"<font color=brown size=4>\nConverting a categorical feature <br>\n<font color=black size=3>\nNow we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\n\nLet us start by converting Sex feature to a new feature called Gender where female=1 and male=0.","bffa7ee6":"<font color=brown size=5>\n    Decision Trees\n<font color=black size=3>\n    Decision Tree is a predictive model that maps features (Tree Branches) to conclusions about the target value (Tree Leaves). In this model, target variable takes a finite set of values called Classification trees; in these tree structures, leaves represent class labels and braches represent conjuctions of features that lead to those class labels. Decision Trees where target variable can take continous values, typically real numbers, are called Regression Trees. \n    \nDrawback of this Learning Model is that they tend to Overfit and are rigid due to the defined Tree Structured formed during Training. ","420e9389":"<font color=brown size=4>\nCompleting a numerical continuous feature <br>\n    <font color=black size=3>\nNow we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\n\nWe can consider three methods to complete a numerical continuous feature.\n\n1. A simple way is to generate random numbers between mean and standard deviation.\n\n2. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using median values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\n\n3. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2.","2619ae83":"<font color=brown size=6>\nModel Evaluation\n<font color=black size=3>\nLets rank the evaluation of all the models to choose the best. ","757a5942":"<font color=brown size=4>\nObservations <br>\n<font color=black size=3>\nInfants (Age <=4) had high survival rate.<br>\nOldest passengers (Age = 80) survived.<br>\nLarge number of 15-25 year olds did not survive.<br>\nMost passengers are in 15-35 age range. <br>\n<font color=brown size=4>\nDecisions<br>\n<font color=black size=3>\nThis simple analysis confirms our assumptions<br>\nWe should consider Age (confirms our assumption) in our model training.<br>\nComplete the Age feature for null values. <br>\nWe should create band age groups as New Feature. <br>","61b9b5b1":"<font color=brown size=4> \nQuick completing and converting a numeric feature <br>\n<font color=black size=3>\nWe can now complete the Fare feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature. We do this in a single line of code.","3a8bba17":"<font color=brown size=5>\n   XGBoost (Extreme Gradient Boosting)\n<font color=black size=3>\n    XGBoost is a decision tree-based Ensemble machine learning algorithm that uses Gradient Boosting framework. In prediction problems involving unstructured data (images, text etc) artificial neural networks tend to outperform other algorithm frameworks. However, when it comes to small-to-medium structure\/tabular data, decision tree based algorithms are considered best-in class now. \n    \nEvaluation of Ensemble models started with Decision trees, a graphical representation of possible solutions to a decision based on certain conditions.<br> \n    > <font color=brown size=4> Bootstraping aggregating or Bagging <font color=black size=3> ensemble meta-algorithms combining predictions from multiple decision trees through a voting mechaism, gave rise to Boosting Algorithms. <br>\n    > Baggin-based algorithms where only <font color=brown size=4> sub-set of features are selected at random to build a forest <font color=black size=3> or collection of decision trees, gave rise to Random Forest Algorithm <br>\n    > Models are <font color=brown size=4> build sequentially by minimizing errors <font color=black size=3> from previous models while increasing the influence of high-performing models, gave rise to Boosting Algorithms. <br>\n    > On these Boosting Algorithms, when additionally <font color=brown size=4> employed Gradient Descent algorithm <font color=black size=3> to minimize errors, gave rise to Gradient Descent Algorithms. <br>\n    > <font color=brown size=4> Optimized Gradient Boosting Algorithm <font color=black size=3> by employing parallel processing , tree purning, handling missing values and regularization to avoid overfitting (or Bias), gave rise to this new Queen of Machine Learning Algorithms. \n","24a4f0de":"<font color=brown size=4>\n2.3.3 Overall Training Sample set Categorical Data Distribution is: <br>\n    <font color=black size=3>\n> Names are unique across the dataset (count=unique=891). <br>\n> Sex variable as two possible values with 65% male (top=male, freq=577\/count=891). <br>\n> Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin. <br>\n> Embarked takes three possible values. S port used by most passengers (top=S). <br>\n> Ticket feature has high ratio (22%) of duplicate values (unique=681). <br>","c6200431":"<font color=brown size=4>\n    Data Exploration using Pivots and\/or Visualization","53bc7eda":"In this model, \n> Sex has highest Positive Correlation\/Coefficient implying as the value of Sex increases (from 0=Male to 1=Female) the probability of Survived=1 increases. So, is the Title has the second highest Postive Correlation. \n\n> Same with Pclass but it has inverse relationship, that is, Pclass=1 to 3 increases, Surival=1 decreases. This way Age*Class Artificial feature is the second best negative correlation with Survived. ","e6de7da6":"<font color=brown size=4> \nObservations <br>\n<font color=black size=3>\nHigher fare paying passengers had better survival. Confirms our assumption for creating fare ranges.<br>\nPort of embarkation correlates with survival rates. Confirms correlating and completing assumptions. <br>\n\n<font color=brown size=4> \nDecisions <br>\n<font color=black size=3>\nConsider banding Fare feature.","b94d4a9c":"<font color=brown size=4>\n    2.2 Get to know the Data and go a step further to look at the Individual Charactertics and few more steps further towards gaining knowledge of Dependencies among data parts (Features) of the Data Point (Row)","60d2bb74":"We can replace many titles with a more common name or classify them as Rare.","ccc5b762":"<font color=brown size=5>\n    Stochastic Gradient Descent <br>\n<font color=black size=3>  \nIn Gradient Descent Algorithm, gradients on each observation is done one by one. It becomes resource intensive when the Dataset is Large. To overcome this, Observations are Randomly picked up. This random probabilistic selection of Observations makes this Stochastic. This Algorithm offers wide variety of parameters to minimize the lost, increase the scope and pace of descent (learning). We will explore these options later to improve on the performance. Let us for now use the basic algorithm with defaults. ","114ef1e7":"Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations","fe9175d7":"<font color=brown size=4>\nAs can see, Random Forest and Decision Trees have the same scores but Decision Tree Beats Random Forest in RMSE. The evaluation is based on lowest RMSE and hence, will be using it for submission on 02Oct19. \nEarlier Submissions:\n    -- A week ago, Random forest but now, when i checked RMSE score, choosing DT. ","dcb43091":"<font color=brown size=5> \n    Naive Bayes <br>\n<font color=black size=3> \n    Naive Bayes classifiers are the family of simple probabilistic classifiers based on applying Bayes Theorm with strong assumption of Independence assumption between the Features. This Algorithm is highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. \n    Drawback of this is that its sheldom is the case in real-time to have such Independent Features. With such correlation between the feature in our case, it most probably will have the low confidence levels. ","b24f062e":"Pclass We observe significant correlation (>0.6) among Pclass=1 and Survived (classifying #3). Retain this feature in our model.","ef4cdf08":"Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.","73983dbc":"<font color=brown size=5> \n    SVM (Support Vector Machines) <br>\n<font color=black size=3> \nSVM is a non-probabilistic Binary Classifier. This  is a Supervised Learning model with associated Learning Algorithms that analyze data for Classification and Regression. Given training samples, each sample will be marks\/assigns to one of the two categories and thus makes it Non-Probabilistic. ","e0daf3ea":"<font color=brown size=5>\n    Random Forest\n<font color=black size=3>\n    Random Forest are Ensemble learning method for classification and regression. This model Operates by constructing multitude of Decision Trees at training time and the output is the mode of classes (Classification) or mean prediction (regression) of individual Trees. ","253e3fff":"<font color=brown size=4> \nWrangle data <br>\n<font color=black size=3> \n\nWith confirmed Assumptions and taken decisions, now time to work on Data to Create New Features, Dropping unncessisary Features and Converting Types of features and lastly imputing. \n\n<font color=brown size=4> \nCorrecting by dropping features\n<font color=black size=4> \n\nThis is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\n\nBased on our assumptions and decisions we want to drop the Cabin (correcting #2) and Ticket (correcting #1) features.\n\nNote that where applicable we perform operations on both training and testing datasets together to stay consistent.","2bdb9cd9":"Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset.","087e3b7a":"<font color=brown size = 4>\n2.2.4 Assumption based on the Data Analysis done so far <br> \n<font color=brown size = 3> Correlation:<br> <font color=black> We will have know how each Feature (or the features that would be included in the model) with Survied Feature. Importantly, these are to be done so as to compare with the Modelled Correlation later. <br> \n<font color=brown size = 3> Completing: <font color=black> <br>\n    Age Feature data is to be completed as it seem to have strong correlation with Survived Feature. <br>\n    Embarked feature also have correlation with the Survived Feature and needs to be completed (imputing)\n<font color=brown size = 3> Correcting: <font color=black> <br>\n    Ticket Feature has 22% duplicate values and does not contribute much to the Prediction. This field will be dropped. <br><br> \n    Cabin Feature as it is incomplete and has high number of missing values in both Training and Validation Datasets. <br> <br>\n    PassengerId will be dropped as this is unique value and certainly have no impact on Survived Feature. <br> <br>\n    Name Feature also may not have much contribution and will be dropped. \n<font color=brown size = 3> Creating: <font color=black> <br>\n    SibSp and Parch are two Features that more or less convey the Familysize. A new feature FamilySize will be created by combinig these two. This new feature gives out number of Family members onboarded. <br> <br>\n    Name Feature has Title in it which can be used alogn with Sex to establish some kind of correlation. <br><br>\n    Age and Fare Features are a continous numeric value and will have to create a new Ordinal Categorical Field to bucket them in different ranges. <br> <br>\n<font color=brown size = 3> Classifying: <font color=black> <br>\n    Sex Feature to be classified. Women more likely to have survived <br> <br>\n    Age Feature: Children were more likely to have survived <br> <br>\n    Pclass Feature: Upper Class (Pclass=1) were more likely to have survived. ","23d47a37":"<font color=brown size=4>\n2.2.3 Individual Feature Analysis <br>\n<font size=4>\n2.2.3 Categorical values: Going by the above\n<font color=black size=3>\n    > Survived Feature is already numeric and its Categorical, 1 Denotes Survived and 0 denotes otherwise.  <br>\n    > Pclass -- is Ordinal and denotes 1=Upper, 2=Middle 3=Lower and already in numeric format  <br>\n    > Name is a Nominal type and as it is is of no use in the Predictor Model.<br>\n    > Sex is categorical and in nominal format. Hence, this should be convered to numeric type.  <br>\n    > SibSp (Sibling & Spouse) and Parch (Parent & Children) are numeric fields and are Ordinal in nature. <br>\n    > Ticket value is Alpha-numerica and a unique value like PassengerId. These can be dropped.<br>\n    > Overall Training Sample Numeric Data Distribution after high level analysis: <br>\n            >> Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).<br>\n            >> Survived is a categorical feature with 0 or 1 values. <br>\n            >> Around 38% samples survived representative of the actual survival rate at 32%. <br>\n            >> Most passengers (> 75%) did not travel with parents or children. <br>\n            >> Nearly 30% of the passengers had siblings and\/or spouse aboard. <br>\n            >> Fares varied significantly with few passengers (<1%) paying as high as $512. <br>\n            >> Few elderly passengers (<1%) within age range 65-80. <br>","990782a9":"Assumption is confirmed that Sex=female had very high survival rate at 74% (classifying #1).","b6acb914":"SibSp and Parch features have low or zero correlation as the counts go up. It may be best to derive a feature or a set of features from these individual features (creating #1).","2e6bc79d":"<font color=brown size=4> \nObservations. <br>\n<font color=black size=3> \nPclass=3 had most passengers, however most did not survive. Confirms assumption.<br>\nInfant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies assumption. <br>\nMost passengers in Pclass=1 survived. Confirms assumption. <br>\nPclass varies in terms of Age distribution of passengers. <br>\n\n<font color=brown size=4> \nDecisions. <br>\n<font color=black size=3> \nConsider Pclass for model training.","cc7686b0":"With Logistic Regression we can validate Assumption and Decisions made for Creating and Completing Feature Goals. Internally, Algorithm calculates the coefficients of the features in decision function. \nPositve Coefficients increase the Odds of probability of right Prediction and Negative Coefs decrease the Odds. ","42d5ac1f":"<font color=brown size=4>\n    Creating a new Feature: <br>\n    <font color=black size=3>\n    New FamilySize by addining up Sibsp and Parch","252fc177":"<font color=brown size=5>\n2.2 Data Analysis <nr>\n<font color=brown size=4>\n2.2.1 DataTypes: <br>\n<font color=black size=3>\n1. There are two continuous quantitative variable namely, Age and Fare. This is in case of both Train and Test datasets.\n2. There are 5 variable with Object Datatype, meaning, these could be free-flowing Nominal Datatype or Categorical\n3. There are 5 and 4 Numerical values in Train and Test Datasets respectively. These again could be Ordinal or Nominal. However, the difference between Train and Test is that Survived Variable is not in Test Dataset. This is the Dependent varilable and the rest are potential Independent variables that could be included in the Model for it to come up with the Predictions. \n    \n<font color=brown size=4>\n2.2.2 MissingData: <br>\n<font color=black size=3>\n1. In Training Dataset, Age and Cabin Features have missing values. 20% of Age values are missing, where as 80% of Cabin are missing. Will have to retain Age as missing values are less than the Standard prescription that is 40%, moreover, we will have to check Relevance of Age on Survial Chances. Eventhough Cabin has more missing values, relevance (or inference) is to be extracted so as to make a decision. Same with Test Dataset However in Test Dataset, additionally one Fare value missing. This value is to be imputed and probably with relevant Mean value.","c500e67a":"<font color=brown size=5>\n    Gradient Descent <br>\n<font color=black size=3>\nEvery Machine Learning Engineer is looking to improve their model performance. Gradient Descent one of the most popular optimization algorithm that helps machine learning models converge at a minimum value through repeated steps. Essentially, gradient descent is used to minimize a function by finding the value that gives the lowest output of that function. Often times, this function is usually a loss function. Loss functions measure how bad our model performs compared to actual occurrences. Hence, it only makes sense that we should reduce this loss. One way to do this is via Gradient Descent. This works on the same principle of Linear Relationships between Independent and Dependent variables. ","8863b859":"<font color=brown size=5> \n    Perceptron <br>\n<font color=black size=3> \n    Perceptron is a supervised learning of Binary classifiers with functions that decide whether an input, represented by a vector of numbers,belongs to a specific class. This is typically Linear classifier, that is prediction happens based on a linear predictor function with addition of having weights assigned to feature vector. ","78e364b2":"<font color=brown size=4>\n    Completing a categorical feature <br>\n    <font color=black size=3>\nEmbarked feature takes S, Q, C values based on port of embarcation. Our training dataset has two missing values. We simply fill these with the most common occurance"}}