{"cell_type":{"52881dc9":"code","fae8bf53":"code","715b9e13":"code","0bd58920":"code","fe83f7c2":"code","b099fd06":"code","cca7f4b9":"code","59252f18":"code","a71820a6":"code","c409ff23":"code","66aa47b0":"code","3339792b":"code","38485728":"code","39639e13":"code","3b1edafa":"code","da3a78f9":"code","869bef86":"code","7c2ad4ba":"code","cf644aba":"code","8babc1ef":"code","013695b7":"code","e40d9930":"markdown","8d5362f0":"markdown","f968e285":"markdown","6bfb9980":"markdown","d92172b3":"markdown","60531a18":"markdown","516a3551":"markdown","8385b4a5":"markdown","e4732f63":"markdown","f56ef550":"markdown","e2467921":"markdown","0dfca410":"markdown","2e0e4f22":"markdown","fb9fdaf0":"markdown","c06687dc":"markdown","37290d9c":"markdown","53def21d":"markdown","c3f0ddee":"markdown","5afc2256":"markdown","3709e80e":"markdown","32d5f319":"markdown","17f94bd2":"markdown","13250973":"markdown","9c7f4522":"markdown"},"source":{"52881dc9":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision.transforms import transforms\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\n\nfrom PIL import Image\nfrom typing import cast, Any, Union, Dict, List\nimport time\nimport random\nimport copy \n \nimport matplotlib.pyplot as plt\n\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","fae8bf53":"train_df = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ntrain_df.head(5)","715b9e13":"train, val = train_test_split(train_df, test_size=0.1, shuffle=True, random_state=9) #test_size=0.1 gives a good result","0bd58920":"aug1_transformers = transforms.Compose([\n                           transforms.ToPILImage(),\n                           transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n                           transforms.RandomRotation(25),\n                           transforms.RandomHorizontalFlip(),\n                           transforms.RandomAffine(degrees=15, translate=(0.1,0.1), scale=(0.8,0.8)),\n                           transforms.ToTensor(),\n                           #after calculating the mean and standard deviation.\n                           transforms.Normalize((0.2861), (0.3528)) \n])\n\naug2_transformers = transforms.Compose([\n                           transforms.ToPILImage(),\n                           transforms.RandomVerticalFlip(p=0.6),\n                           transforms.RandomRotation(40),\n                           transforms.RandomHorizontalFlip(p=1),\n                           transforms.RandomAffine(degrees=30, translate=(0.1,0.1), scale=(0.8,0.8)),\n                           transforms.ToTensor(),\n                           #after calculating the mean and standard deviation.\n                           transforms.Normalize((0.2861), (0.3528)) \n])\n\naug3_transformers = transforms.Compose([\n                           transforms.ToPILImage(),\n                           transforms.RandomRotation(degrees=(-90, 90)),\n                           transforms.RandomVerticalFlip(p=0.6),\n                           transforms.RandomRotation(40),\n                           transforms.RandomHorizontalFlip(p=0.5),\n                           transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.8,0.8)),\n                           transforms.ToTensor(),\n                           #after calculating the mean and standard deviation.\n                           transforms.Normalize((0.2861), (0.3528)) \n])\n\ntrain_transformers = transforms.Compose([\n                           transforms.ToPILImage(),\n                           transforms.ToTensor(),\n                           #after calculating the mean and standard deviation.\n                           transforms.Normalize((0.2861), (0.3528))    \n])\n\nval_transformers = transforms.Compose([\n                           transforms.ToPILImage(),\n                           transforms.ToTensor(),\n                           #after calculating the mean and standard deviation.\n                           transforms.Normalize((0.2861), (0.3528))\n])","fe83f7c2":"class MyDataSet(Dataset):\n  def __init__(self, features, labels, Transform):\n    self.x = features\n    self.y = labels\n    self.transform = Transform\n\n  def __len__(self):\n    return len(self.x)\n\n  def __getitem__(self, index):\n    return self.transform(self.x[index]), self.y[index]\n","b099fd06":"\ndef GetDf(df, Transform):\n  x_train = df.iloc[:, 1:].values\n  y_train = df.label.values\n  x_train = x_train.reshape(-1, 1, 28, 28)\n  x_train = np.uint8(x_train)\n  y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n  x_train = torch.from_numpy(x_train)\n  return MyDataSet(x_train, y_train, Transform)","cca7f4b9":"datasets = {\n    'aug1':  GetDf(train_df, aug1_transformers),\n    'aug2':  GetDf(train_df, aug2_transformers),\n    'aug3':  GetDf(train_df, aug3_transformers),\n    'train': GetDf(train, train_transformers),\n    'val': GetDf(val, val_transformers)\n}\n\n# new_train = ConcatDataset([datasets['train'], datasets['aug1'], datasets['aug2'],datasets['aug3']])\nnew_train = ConcatDataset([datasets['train'], datasets['aug1']])\nloaders = {\n    'train': DataLoader(new_train, batch_size=64, shuffle = True, num_workers=4),\n    'val': DataLoader(datasets['val'], batch_size=64, shuffle = True, num_workers=4)\n}\n","59252f18":"#for metrics\ndataset_sizes = { 'train': len(new_train), 'val': len(datasets['val'])}\n\n#for normalization\ndataset_sizes_for_norm = {'train': len(datasets['train']), 'val': len(datasets['val'])}\nloaders_for_norm = {\n    'train': DataLoader(datasets['train'], batch_size=64, shuffle = True, num_workers=4),\n    'val': DataLoader(datasets['val'], batch_size=64, shuffle = True, num_workers=4)\n}\n\n#show\ndataset_sizes, dataset_sizes_for_norm","a71820a6":"for x in ['train', 'val']:\n   \n  #number of pixels in the dataset = number of all pixels in one object * number of all objects in the dataset\n  num_pxl = dataset_sizes_for_norm[x]*28*28\n    \n  #we go through the butches and sum up the pixels of the objects, \n  #which then divide the sum by the number of all pixels to calculate the average\n  total_sum = 0\n  for batch in loaders_for_norm[x]: total_sum += batch[0].sum()\n  mean = total_sum \/ num_pxl\n\n    #we calculate the standard deviation using the formula that I indicated above\n  sum_sqrt = 0\n  for batch in loaders_for_norm[x]: sum_sqrt += ((batch[0] - mean).pow(2)).sum()\n  std = torch.sqrt(sum_sqrt \/ num_pxl)\n\n#   print(f'{x} - mean: {mean}, std: {std}')\n","c409ff23":"i = np.random.randint(50)\nx, y = next(iter(loaders['train']))\nplt.imshow(x[i].view(28,28),), x.mean(), x.std()","66aa47b0":"class VGG(nn.Module):\n  def __init__(self, features:nn.Module, num_classes:int=1000, init_weights:bool=True):\n      super(VGG, self).__init__()\n      self.features = features #features = Feature extraction\n      self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n      self.classifier = nn.Sequential(nn.Linear(256, num_classes))\n\n      if init_weights:\n        self.initialize_weights()\n  \n  def forward(self, x):\n    x = self.features(x) #features = Feature extraction\n    x = self.avgpool(x)\n    # print(x.shape)\n    x = torch.flatten(x, 1)\n    x = self.classifier(x)\n    return x\n\n  def initialize_weights(self):\n    for m in self.modules():\n      if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n        if m.bias is not None:\n          nn.init.constant_(m.bias, 0) \n      if isinstance(m, nn.BatchNorm2d):\n        nn.init.constant_(m.weight, 1)\n        nn.init.constant_(m.bias, 0)\n      if isinstance(m, nn.Linear):\n        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n        if m.bias is not None:\n          nn.init.constant_(m.bias, 0) \n\ncfgs: Dict[str, List[Union[int, str]]] = {\n    'A':[64, 'M', 128, 128, 'M', 256, 256, 256],\n    'B':[16, 'M', 256, 'M'],\n    'firstPadding':2\n}\n# cfgs: Dict[str, List[Union[str, int]]] = {\n#     'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n#     'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n#     'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n#     'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n# }\n\n\n\ndef make_layers(cfg:List[Union[int,str]], batch_norm:bool=False) -> nn.Sequential:\n  layers:List[nn.Moduel] = []\n  in_channels = 1\n  in_padding = 5\n  i = 0\n  for v in cfg:\n    if v == 'M':\n      layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n    else:\n      v = cast(int, v)\n      in_padding = 1\n      if i == 5:\n        in_padding = 2\n      conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=in_padding)\n      if batch_norm:\n        layers += [conv2d, nn.BatchNorm2d(v), nn.PReLU(num_parameters=1)]\n      else:\n        layers += [conv2d, nn.PReLU(num_parameters=1)] #nn.PReLU(num_parameters=1) nn.ReLU(inplace=True)\n      in_channels = v\n    i += 1\n  return nn.Sequential(*layers)\n\n\ndef selfDefineVgg(arch, cfg, batch_norm,  num_classes:int, **kwargs: Any) -> VGG:\n    model = VGG(make_layers(arch[cfg], batch_norm=batch_norm), num_classes, **kwargs)\n    return model\nmodel = selfDefineVgg(cfgs, 'A', True, 10)\nmodel = model.to(device)","3339792b":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.001, betas=(0.9,0.999), eps=1e-9)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.1, verbose=False)###Best accuracy 0.9349, tensor(0.9385)","38485728":"#save the losses for further visualization\nlosses = {'train':[], 'val':[]}\naccuracies = {'train':[], 'val':[]}\n\ndef train(model, criterion, optimizer, scheduler, epochs):\n  since = time.time()\n  best_model = copy.deepcopy(model.state_dict())\n  best_acc = 0.0\n  for epoch in range(epochs):\n    for phase in ['train', 'val']:\n      if phase == 'train':\n        model.train()\n      else:\n        model.eval()\n      \n      running_loss = 0.0\n      running_corrects = 0.0\n\n      for inputs, labels in loaders[phase]:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        with torch.set_grad_enabled(phase=='train'):\n          outp = model(inputs)\n          _, pred = torch.max(outp, 1)\n          loss = criterion(outp, labels)\n        \n          if phase == 'train':\n            loss.backward()\n            optimizer.step()\n        \n        running_loss += loss.item()*inputs.size(0)\n        running_corrects += torch.sum(pred == labels.data)\n\n\n      epoch_loss = running_loss \/ dataset_sizes[phase]\n      epoch_acc = running_corrects.double()\/dataset_sizes[phase]\n      losses[phase].append(epoch_loss)\n      accuracies[phase].append(epoch_acc)\n      if phase == 'train':\n        print('Epoch: {}\/{}'.format(epoch+1, epochs))\n      print('{} - loss:{}, accuracy{}'.format(phase, epoch_loss, epoch_acc))\n    \n      if phase == 'val':\n        print('Time: {}m {}s'.format((time.time()- since)\/\/60, (time.time()- since)%60))\n            \n      if phase == 'val' and epoch_acc > best_acc:\n        best_acc = epoch_acc\n        best_model = copy.deepcopy(model.state_dict())\n    scheduler.step()  \n  time_elapsed = time.time() - since\n  print('Training Time {}m {}s'.format(time_elapsed\/\/60, time_elapsed%60)) \n  print('Best accuracy {}'.format(best_acc))\n\n  model.load_state_dict(best_model)\n  return model   ","39639e13":"epochs = 10\nmodel = train(model, criterion, optimizer, scheduler, epochs)","3b1edafa":"from pylab import rcParams\nrcParams['figure.figsize'] = 7, 7\n\nfig, axs = plt.subplots(2)\n\naxs[0].plot([x for x in range(epochs)], losses['train'], label ='train')\naxs[0].plot([x for x in range(epochs)], losses['val'], label='val')\naxs[0].set_title('loss')\naxs[0].legend()\n\naxs[1].plot([x for x in range(epochs)], accuracies['train'], label ='train')\naxs[1].plot([x for x in range(epochs)], accuracies['val'], label='val')\naxs[1].set_title('accuracy')\naxs[1].legend()","da3a78f9":"test_df = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\ntest_df.head(5)","869bef86":"test_test_transformers = transforms.Compose([\n                         transforms.ToPILImage(),\n                         transforms.ToTensor(),\n                          transforms.Normalize((0.2869), (0.3540)) \n                         #transforms.Normalize((0.5), (0.5))     \n])\n","7c2ad4ba":"datasetsq = GetDf(test_df, test_test_transformers)\n\nloadersq=  DataLoader(datasetsq, batch_size=32, shuffle = True, num_workers=4)","cf644aba":"dataset_sizesq = len(datasetsq)\ndataset_sizesq","8babc1ef":"num_px = dataset_sizesq*28*28\n\n\ntotal_sum = 0\nfor batch in loadersq: total_sum += batch[0].sum()\nmean = total_sum \/ num_px\n\nsum_of_squared_error = 0\nfor batch in loadersq: sum_of_squared_error += ((batch[0] - mean).pow(2)).sum()\nstd = torch.sqrt(sum_of_squared_error \/ num_px)\n\nmean, std","013695b7":"corr = 0\n \nwith torch.no_grad(): \n    for data in loadersq:\n        images, labels = data\n        images, labels = images.to(device), labels.to(device)\n        y_pred = model(images) \n        _, predicted = torch.max(y_pred, 1) \n        corr += torch.sum(predicted == labels.data)\n \nprint(f\"Test accuracy: {corr\/dataset_sizesq}\")","e40d9930":"**Let's see what the images look like**","8d5362f0":"**Uploading the data**","f968e285":"**Loss function, Optimizer, Scheduler**","6bfb9980":"**Visualization**","d92172b3":"**Launching training**","60531a18":"# Model","516a3551":"# Loading data","8385b4a5":"**Predictions**","e4732f63":"![fg_18.png](attachment:603cffa3-5d67-4128-b929-c92f76a089a5.png)","f56ef550":"***I am always happy to receive any feedback. What do you think can be changed and what can be removed?***","e2467921":"# Test","0dfca410":"**Creating a dictionary with the size of the test dataset.**","2e0e4f22":"**This function converts the pandas table to a matrix, changes the shape of the feature matrix, and changes the data type. At the end, the function converts both matrices to the torch tensor and passes it to the class**","fb9fdaf0":"**Find the mean and standard deviation**","c06687dc":"**We transform the test part only into a tensor and normalize it according to the same principle as before.**","37290d9c":"**\u0421onverting and wrapping in DataLoader**","53def21d":"**For the FashionMNIST classification, I decided to use VGG, but I changed the source code a bit (source code: https:\/\/pytorch.org\/vision\/stable\/_modules\/torchvision\/models\/vgg.html). I would like to note that I initialized the weights according to Kaiming initialization and PReLU instead of ReLU. This really makes the result better. You can read more about Kaiming initialization here: https:\/\/arxiv.org\/abs\/1502.01852v1, https:\/\/towardsdatascience.com\/understand-kaiming-initialization-and-implementation-detail-in-pytorch-f7aa967e9138. I advise you to read both articles, since one of them is a theory, and the other is an implementation.**","c3f0ddee":"**Similarly, we calculate the average and standard deviation for test**","5afc2256":"**Data transformation, including data for expanding training data through argumentation. \nBelow I will write why I chose such values for normalization**","3709e80e":"**The sizes of the datasets will be needed further to calculate the average, standard deviation, and calculation of metrics for training. Questions may arise, why calculate the mean and standard deviation ourselves, if pandas does it for us? I was confused by the fact that the calculations are different manually and via pandas, so I decided that it would be better to calculate manually. First, I create datasets and loaders *WITHOUT* normalization and augmentation to calculate the mean and standard deviation. Next, I normalize the data according to the received values. I normalize augmentations using the training dataset value. It is not necessary to calculate the mean and standard deviation of the concatenated dataset, because this will lead to a sharp drop in accuracy on the validation part**","32d5f319":"**Datasets and Loaders. I tried using all the augmentations, but I didn't notice a significant increase in accuracy, and the training time became too long, so I used only one augmentation**","17f94bd2":"# Train","13250973":"**\u0421lass for loading and transformation.**","9c7f4522":"**Let's see what the data in the pandas table looks like**"}}