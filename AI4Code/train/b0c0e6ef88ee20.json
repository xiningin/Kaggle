{"cell_type":{"c346d87a":"code","a0261407":"code","687db5dc":"code","2ef4ca87":"code","602a7ae2":"code","b41c84e8":"code","85523def":"code","86abfc64":"code","970e659e":"code","de5f72b2":"code","7edb9399":"code","db2c75f9":"code","fb38b6c2":"code","340e3314":"code","d80d137e":"code","155d4256":"code","deb2d28b":"code","a3255bda":"code","4896716f":"code","0b3be473":"code","f6a97487":"code","ef5e8d25":"code","ade1287f":"code","bfebd7e9":"code","1c06e859":"code","6fec3bdc":"code","d10e2499":"code","1b20235d":"markdown","edb9351c":"markdown","28c0a031":"markdown","3f659222":"markdown","b2e4c6c5":"markdown","497113d3":"markdown","309a9f0d":"markdown","bf036b8e":"markdown","ff15d73c":"markdown","6674179e":"markdown","292e341a":"markdown","e1cda3f7":"markdown","33fe004c":"markdown","2570f6bb":"markdown","8e0f038c":"markdown","c3f923ba":"markdown","58609818":"markdown","a965ab47":"markdown","6c3ce1f4":"markdown","cd15c116":"markdown","7dfa5438":"markdown","f016f24b":"markdown","d46cf480":"markdown","cb5f1cd9":"markdown","ab8f0949":"markdown","bbd14219":"markdown","079ecdc6":"markdown","59af8f62":"markdown","99a765fa":"markdown","223d7c1b":"markdown","643f5908":"markdown","6de07c7e":"markdown","7df3396f":"markdown","e54a131c":"markdown","cfc1d1f4":"markdown","ff8883ea":"markdown","edb00d91":"markdown","a8e4ef55":"markdown","e688266a":"markdown","5bb9c011":"markdown","4cf71278":"markdown","a9abd971":"markdown","1dc39954":"markdown"},"source":{"c346d87a":"from IPython.display import HTML\nfrom IPython.lib.display import YouTubeVideo\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport warnings\nimport plotly.express as px\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.linear_model import LinearRegression","a0261407":"HTML('<center><iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/pd-0G0MigUA?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>')","687db5dc":"HTML('<center><iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/YWFqtmGK0fk?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>')","2ef4ca87":"data = pd.read_csv(\"..\/input\/titanic\/train.csv\")","602a7ae2":"data.head(2)","b41c84e8":"data.isna().sum()","85523def":"data.drop(columns = 'PassengerId', inplace=True)\n# I will only use numerical values to make work short but you should do it using all options available\nfiltered_data = data[['Pclass', 'Fare', 'SibSp', 'Parch', 'Age']]","86abfc64":"train = filtered_data[filtered_data['Age'].notnull()]\ntest = filtered_data[filtered_data['Age'].isna()]\n\nfiltered_data.shape ,train.shape, test.shape","970e659e":"linear_regressor = LinearRegression()\n\nlinear_regressor.fit(train.drop(columns = 'Age'), train['Age'])\n\ntest['Age'] = linear_regressor.predict(test.drop(columns = 'Age'))","de5f72b2":"test.isna().sum()","7edb9399":"data['Cabin'].notnull().sum()","db2c75f9":"data['Cabin'][data['Cabin'].notnull()][:5]","fb38b6c2":"floor = []\nfor i in data['Cabin']:\n    \n    try:\n        value = list(i)[0]\n        floor.append(value)\n    except:\n        floor.append(i)\n        \ndata['floor'] = floor\nprint(data['floor'][data['floor'].notnull()][:5])","340e3314":"print(\"Unique values in floor column: \", len(data['floor'].unique()))\nprint('Unique values in Cabin column: ', len(data['Cabin'].unique()))","d80d137e":"data.head(2)","155d4256":"titles = []\nfor i in data['Name']:\n    value = (i.split(',')[1]).split(' ')[1]\n    titles.append(value)","deb2d28b":"data['titles'] = titles\npx.bar(data, x = 'titles', color='titles')","a3255bda":"for i in range(len(titles)):\n    if titles[i] not in ['Mr.', 'Mrs.', 'Miss.', 'Master.']:\n        titles[i] = 'other'\n\ndata['titles'] = titles","4896716f":"px.bar(data, x = 'titles', color='titles', facet_col='Survived')","0b3be473":"data = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')","f6a97487":"data.sample(2)","ef5e8d25":"# Which values you would have to predict\ncols_target = ['obscene','insult','toxic','severe_toxic','identity_hate','threat']\n\nvalues = list(data[cols_target].sum())\ntarget = cols_target\n\nother_rows = np.array(values).sum()\n\nneutral_comment = data.shape[0] - other_rows\n\nvalues.append(neutral_comment)\ntarget.append('Total_rows')\n\nax = sns.barplot(target, values)\nax.set_xticklabels(target, rotation=45)\nax.set_title('Number of rows per type', fontsize=17)\nax.set_ylabel('Number of rows', fontsize=14)\nax.set_xlabel('Comment type', fontsize=14);","ade1287f":"from sklearn.manifold import TSNE\nimport imblearn","bfebd7e9":"from sklearn.model_selection import StratifiedKFold\nk_fold = StratifiedKFold(n_splits=10)","1c06e859":"for fold, (train, validation) in enumerate(k_fold.split(X=data, y=data.toxic.values)):\n    data.loc[validation, 'kfold'] = fold","6fec3bdc":"from skmultilearn.problem_transform import ClassifierChain, BinaryRelevance\nclf_chain = ClassifierChain()\nbin_rel = BinaryRelevance()","d10e2499":"def chain_classifier(model, x_train, y_train, x_validation=None, y_validation=None, \n                     x_test=None, y_test=None, validate=False, test=False, display=False, tune=False, param=None):\n\n    print('Model = ', model, '\\n')\n    train_scores = []\n    validation_scores = []\n    test_scores = []\n    model_details = []\n    \n    from sklearn.model_selection import RandomizedSearchCV\n    from sklearn.metrics import accuracy_score, f1_score\n    \n    for label in cols_target:\n\n        print('... Processing {} \\n'.format(label))\n        \n        y_train_label = y_train[label]\n        y_validation_label = y_validation[label]\n        y_test_label = y_test[label]\n        \n        # To tune the model\n        if tune:\n            model = RandomizedSearchCV(model, param, n_jobs=-1, cv=10)\n        \n        # train the model using x_train & y_train\n        model.fit(x_train,y_train_label)\n        \n        # compute the training results\n        y_train_pred = model.predict(x_train)\n        \n        if display:\n            print('Training Accuracy is {}'.format(accuracy_score(y_train_label, y_train_pred)))\n            print('Training F1Score is {} \\n'.format(f1_score(y_train_label, y_train_pred)))\n        \n        # Append scores\n        to_append = (accuracy_score(y_train_label, y_train_pred), f1_score(y_train_label, y_train_pred))\n        train_scores.append(to_append)\n        \n        # Adding predictions as features\n        x_train = add_feature(x_train, y_train_pred)\n        \n        if validate:\n            # compute validation results\n            y_validation_pred = model.predict(x_validation)\n            \n            if display:\n                print('Validation Accuracy is {}'.format(accuracy_score(y_validation_label, y_validation_pred)))\n                print('Validation F1Score is {} \\n'.format(f1_score(y_validation_label, y_validation_pred)))\n            \n            # Adding prediction as feature\n            x_validation = add_feature(x_validation, y_validation_pred)\n            \n            \n            # Append scores\n            to_append = (accuracy_score(y_validation_label, y_validation_pred), f1_score(y_validation_label, y_validation_pred))\n            validation_scores.append(to_append)\n\n        if test:\n            # compute test results\n            y_test_pred = model.predict(x_test)\n            \n            if display:\n                print('Test Accuracy is {}'.format(accuracy_score(y_test_label, y_test_pred)))\n                print('Test F1Score is {} \\n'.format(f1_score(y_test_label, y_test_pred)))\n            \n            # append Scores\n            to_append = (accuracy_score(y_test_label, y_test_pred), f1_score(y_test_label, y_test_pred))\n            test_scores.append(to_append)\n            \n            # Adding prediction as feature\n            x_test = add_feature(x_test, y_test_pred)\n        \n        model_details.append(model)\n        \n        \n    scores = (train_scores, validation_scores, test_scores)    \n        \n    return scores, model_details","1b20235d":"You will get imbalaced data a lot of time and you need to know the tricks to handle imbalanced data efficiently and I will try to show you just that but first let's see what actually is imbalanced data.","edb9351c":"Well this is something very easy and we have already partly done it\n * like we did in Cabin column instead of encoding something with lot's of unique values directly try to look for something that makes sense\n * If you have continuous values then try to divide them in bins\n * view your data and look for things to convert (in titanic dataset the columns \"Name\" has a 2 interesting things - Familyname and title so try to make new columns of them alone and see the results)\n \nencoding doesn't only mean to convert to numeric data, it means to map some data to some other kind of data so think and use that.","28c0a031":"![image.png](attachment:image.png)","3f659222":"## 5) Encoding","b2e4c6c5":"For this I would recommend these notebooks  and you should explore similar title without deep learning<br>\n * https:\/\/www.kaggle.com\/faressayah\/natural-language-processing-nlp-for-beginners\n * https:\/\/www.kaggle.com\/sudalairajkumar\/a-look-at-different-embeddings\n \nalso study some regular expression and if possible do the **Applied Data Science with python specialisation course-4 on Coursera.**","497113d3":"###### You can use better algorithms but u get the basic idea on how to do it, now to tell u more let me show you one more thing.","309a9f0d":"After reading the above article you will see that it's not possible to optimize your model using some RandomSearchCV or something else if you use inbuilt modules for chainclassifier and etc. from skmultilearn.","bf036b8e":"# Some tips\n\n * While using large files after feature engineering you might run out of RAM so always export your data at different levels and import data with dtype as np.float16 as always your data is smallet than than.\n * Try to get other open sourse data for competitions\n * To get better at writing notebooks this might come in handy - https:\/\/github.com\/Abhishek-Prajapat\/Cheat-Sheets-collected-\/blob\/master\/markdown-cheatsheet-online.pdf\n * If you have a GPU VRAM >= 6GB install Ubuntu(18.04) and Rapids (https:\/\/rapids.ai\/start.html) and start using gpu as it really really help in iterating model cycle faster.\n * Use automl and such libraries (Pycaret) if possible to reduce time.","ff15d73c":"Well as you can see we have very few training samples for comments types other than neutral and if we don't take this into consideration we will get a bad model. Also you could get good accuracy if you predict if \"Neutral\" for all comments but that model isn't doing anything and is just garbage. So now let's head towards how to solve this.","6674179e":"You already know and use - mean imputing, mode imputing <br>\nwhat you should\/could do is a very simple trick - Create a model to predict the me missing value by using other columns so let's do that and I hope you will try to understand the usefulness of this method. To make thing real simple I will use the all known Titanic data.","292e341a":"**let's try to get the title from name**","e1cda3f7":"Well the problem is that the basic Kfold could use any segmented data section for training and validation and hence in case of a skewed\/imbalanced data the distribustion of data for training and validation may not remain constant and we could get biased\/fake results and to overcome that we use StratifiedKfold which divided each fold in same distribution as that of the original training data.","33fe004c":"### I would request you to not go over the simplicity and neglect this, it's truly very useful, think about its practical implementation.","2570f6bb":"## 3) Sqlite and SQlAlchemy","8e0f038c":"<font color=\"red\" size=3>Please upvote this kernel if you like it. It motivates me to produce more quality content :)<\/font>","c3f923ba":"## 4) Imputing","58609818":"# Intermediate Machine Learning","a965ab47":"**Let's see the tistribution of out target columns**","6c3ce1f4":"## 6) Performance metric","cd15c116":"There is yet another very useful method while traing the model which is StratifiedKfold. <br>\nWe used other methods for corss validation in basic machine learning and saw that Kfold is a very important aspect of our evaluation of model","7dfa5438":"A must read - https:\/\/towardsdatascience.com\/journey-to-the-center-of-multi-label-classification-384c40229bff <br>\nfocus on part-5 Multi-Label-Classification_techniques.","f016f24b":"well you can see we reduced our unique value to just 9 from 148 unique values and we get an option encode these and could also get relatively better results if we predict the missing values using these floor values as target. <br>","d46cf480":"I recently had to deal with multilabel data and had the same problem and to overcome this I made a fairly simple funtion for chain classification and you should also be able to create your own functions and classes.","cb5f1cd9":"## 8) Multilabel Classification","ab8f0949":"This is necessary to know what should u change in either your dataset or in your model to get better results in the competition.\n * Study all common metrics\n * To improve yourself in this field study the data and the target and then try to come up with a performance metric by yourself and then compare it with the competition metric and now u know why it is this\/that metric.","bbd14219":"##### Similarly we can handle the family name and well. So you get the basic idea of what to do. Encodinng is not bound to always changing categorical to numerical it could also be categorical to categorical and so on. I request once again don't go over its simplicit.","079ecdc6":"----------------------------------------","59af8f62":"TSNE is an algorithm that maps highdimensional data to lower dimesions (2D and 3D) and after doing that we could we could plot the 2D or 3D data to see if there is a structure in the data and if there is some structure we can use the imblearn library to generate\/remove samples and much more.","99a765fa":"And now we will use this new data instead of the previous normal k-fold. I would recomment that you always use stratified-k-fold in compare to normal k-fold.","223d7c1b":"A lot of time it happens that you have to predict say top x possible outcome from a set of outcomes of length y. This is a kind of multilabel problem also in a case u might have to predicte the presence of various labels at the smae time and that is again a multilabel classification problem. <br>\nexample - https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge <br>\nalso - https:\/\/www.kaggle.com\/c\/youtube8m","643f5908":"## 2) Seaborn\n\nCheck this - https:\/\/www.kaggle.com\/kanncaa1\/seaborn-tutorial-for-beginners","6de07c7e":"------------","7df3396f":"## 7) Imbalaned Data","e54a131c":"## Must Must Must do relevant competitions","cfc1d1f4":"We no longer have any mssing value in test data","ff8883ea":"TSNE vs PCA notebook - https:\/\/www.kaggle.com\/vimary\/tsne-vs-pca <br>\nHow to use TSNE efficiently - https:\/\/distill.pub\/2016\/misread-tsne\/ <br>\nHow to use imblearn - https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/user_guide.html","edb00d91":"we Have 204 values of Cabin and let's see some values","a8e4ef55":"What we did was extracting the floor from the Cabin columns, so what happens from doing something so trivial ?","e688266a":"you could also do oversampling and then check the results of training on newdataset distribusion and testing on test data distribution. <br>\nAlso Machine Learning is a iterative processes you will need to see what works for you for yourself by iterating over different settinngs.","5bb9c011":"##  1) Plotly\n\nyou should check out this tutorial by Pranav Anand - https:\/\/www.kaggle.com\/pranavanand24\/interactive-visualization-with-plotly-express <br>\nAs I said Original docs are gold - https:\/\/plotly.com\/python\/plotly-express\/","4cf71278":"You see some thing interesting ? <br>\nno worries you will see it now","a9abd971":"First of all let me make it clear what I consider as Basic, Intermediate and Advanced Machine Learning and just to be clear I am not considering Deep learning as a part of machine learning in this case so advanced machine learning doesn't mean DEEP Learning.\n<br>\nThis is very necessary to be able to diffrentiate the different levels of machine learning by content and by doing so you know what specifically you should know right now.\n\n-----------------------------------\n\n<font color=\"red\" size=3>Please upvote this kernel if you like it. It motivates me to produce more quality content :)<\/font>\n\n-----------------------------------\n\n * **Basic Machine Learning**\n     * You know about the Pandas, Matplotlib, Numpy and could work with \"*.csv\" files\n     * You know some basic imputing methods - SimpleImputer, KNNImputer\n     * Use some common encodings such as LabelEncoding, OneHotEncoding\n     * Know basic performance measure - Accuracy, confusion metric\n     * You can use common algorithms to make models (no need to know inner workings)\n         * Regression - Linear, Ridge, Lasso, SVM, Decision tree, RandomForest\n         * Classification - Logistic, KNN, N.B., SVM, Decision tree, RandomForest\n         * Unsupervised - Kmeans\n         * Dimensionality Reduction - PCA\n     * Could do basic Hyper-parameter tuning using - GridSearchCV and RandomSearchCV\n     * As baseline you could get 70% accuracy on Titanic Kaggle Challenge\n\n#### Best Method to complete Basic Machine Learning is to do a course. My recommendation - Applied Data Science in python specialisation (Coursera) first 3 courses.\n\n-----------------------------\n\n * **Intermediate Machine Learning**\n     * You know Pandas, Numpy, Matplotlib and some plotting libraries such as Plotly, Seaborn to make work faster\n     * You could use SQLite, SQLAlchemy and such. and could work with other file types specially databases, not bounded by \"*.csv\"\n     * You could use your own methods to impute the missing data\n     * Encode data to the best of your understanding\n     * Know about different Performace metrics - AUC, ROC, f1score (micro, macro), HammingLoss, precision, recall\n     * Could handle imbalanced datasets\n     * Could do multilabel predictive analysis\n     * Start exploring new algorithms - xgboost, catboost, tsne etc. (still not must to know inner workings)\n     * Could use other k-fold-techniques such as StratifiedKfold, hold-out-Kfold\n     * **And could work upon text data using basic (no neural networks) nlp using pretrained embeddings at most**\n     * You get to know about multiple online solutions and options such as Kaggle, Colab, AWS, towardsDataScience, Medium, etc.\n     * You start Using Github for others approaches to different problems and use\/contribute to open Source.\n     * You start using original documentation for libraries and their workings ( its gold )\n     \n#### Best Method to complete Intermediate Machine Learning is by doing old Kaggle competitions and reading others notebooks and approaches. You can get a course but this time it won't be free for that quality.\n  \n------------------------------\n     \n * **Advanced Machine Learning**\n     * You are simple not bound in regards to Data\n         * You strt collecting more data from various sources and mold them accoring to needs\n     * You can use AutoML to speed up your work <br> <br>\n     \" When people asks why u need to know the inner working of algorithms the answer comes here well when making a model by ensambling multiple algorithms it becomes hard to know how to optimize things and how to make new\/custom function for loss also by nowing which algorithm is working as well as why you will know what kind of chnages does the data need\"<br> <br>\n     * You can make ensambles of various algorithms and could make variations in algorithms for better results\n     * You can make new features for ensambled models<br> <br>\n     \" By this time you have experience and know one the most important things that is how to map a real world problem to a Machine Learning Problem \"<br> <br>\n     * You can create a real world machine learning challenge\n#### Best Method - **I don't know, I am not here yet**\n     \n-------------------------\n\n### With all those things out of the way let's start the real work","1dc39954":"## 9) Basic NLP "}}