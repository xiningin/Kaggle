{"cell_type":{"b0371fc0":"code","aa962d44":"code","6b719aeb":"code","435286f1":"code","4b9102af":"code","ba871c8a":"code","e43c56a5":"code","41aabb3d":"code","11203739":"code","1fabe9d8":"code","289641f5":"code","87172efb":"code","c51cd14a":"code","04069c8c":"code","0a674275":"code","40070db2":"code","051b28f8":"code","09f30b11":"code","3ee97df7":"code","fc32c49b":"code","841689c3":"code","c887fdef":"code","65eab1d8":"code","a36f6b4b":"code","9762afb1":"code","246ad5e8":"code","bd148d4b":"code","7854998b":"code","9a07f83e":"code","4acc1ccb":"code","11547e4f":"code","e8e360e9":"code","f5ea8518":"code","9d8e3825":"code","eeed9b8c":"code","2368c6ea":"code","8644f4ed":"code","3298dffa":"code","15135218":"code","f1cc54c5":"code","5502e224":"code","658e7b23":"code","03b7e730":"code","f25678da":"code","541013c2":"code","4266fb48":"code","478cbf5e":"code","90faebf8":"code","16424f83":"code","79ee746f":"code","31c486a0":"code","71eecd5c":"code","41e136f6":"code","4e7e8a74":"code","40ef11f3":"code","48507589":"code","4b388db2":"code","5464bb4e":"code","a2066f49":"markdown","4262e74c":"markdown","c0400210":"markdown","04b3d361":"markdown","3b35e86f":"markdown","0a5a59a9":"markdown","a54d3c8d":"markdown","78be05aa":"markdown","5bb973a6":"markdown","1da5657e":"markdown","f52cbf76":"markdown","84eb6fed":"markdown","40371a8c":"markdown","cf7fdb80":"markdown","788329b6":"markdown","6d230827":"markdown","962432de":"markdown","98e35248":"markdown","1a19430e":"markdown","f56b287a":"markdown","117774e8":"markdown","dd580173":"markdown","9bacd091":"markdown","ede2e160":"markdown","ccdc960f":"markdown","a91f3370":"markdown","a93b89fa":"markdown","c805b3b1":"markdown","a353f7a3":"markdown","1f9b1c0a":"markdown","0c43b2bc":"markdown","1212a34d":"markdown","aeb7511a":"markdown","3f842ad8":"markdown","9a7d33c3":"markdown","185f51c6":"markdown"},"source":{"b0371fc0":"# importing all the necessary library and tools\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Logistic Regression Classifier from sklearn\nfrom sklearn.linear_model import LogisticRegression\n\n# Evaluation metrics\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import plot_roc_curve\n\n# Model Evaluation\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","aa962d44":"H_D = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","6b719aeb":"H_D.head(4)","435286f1":"H_D.isna().sum()","4b9102af":"H_D.info()","ba871c8a":"H_D.describe()","e43c56a5":"H_D[\"target\"].value_counts()","41aabb3d":"CountNoDisease = len(H_D[H_D.target == 0])\nCountHaveDisease = len(H_D[H_D.target == 1])\nprint(\"Percentage of Patient With H_D --> {:.2f}%\".format((CountNoDisease \/ (len(H_D.target))*100)))\nprint(\"Percentage of Patients Without H_D --> {:.2f}%\".format((CountHaveDisease \/ (len(H_D.target))*100)))","11203739":"# Visualizing\nH_D['target'].value_counts().plot(kind = 'bar', color = [\"#990000\",\"lightblue\"]);","1fabe9d8":"H_D.count()","289641f5":"H_D[\"sex\"].value_counts()","87172efb":"# Comparing sex with target\npd.crosstab(H_D.target, H_D.sex)","c51cd14a":"# Visualizing above crosstab\npd.crosstab(H_D.target, H_D.sex).plot(kind = \"bar\", figsize = (10,6), \n                                     color = [\"#900000\", \"#000070\"])\nplt.title(\"Heart_Disease according to Sex\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Number\")\nplt.legend([\"Female\",\"Male\"]);","04069c8c":"pd.crosstab(H_D.target, H_D.age[H_D.target == 1]) # Age Vs Heart_D","0a674275":"pd.crosstab(H_D.age,H_D.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Age and No of Patient with heart disease')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","40070db2":"pd.crosstab(H_D.target, H_D.thalach)  # Thalach = maximun heart rate","051b28f8":"# Scatter plot\nplt.scatter(H_D.age[H_D.target ==1], H_D.thalach[H_D.target == 1], c = 'black')\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max_Heart_Rate_Acheived\");","09f30b11":"# Checking Age Distribution with boxplot\nplt.boxplot(H_D.age);","3ee97df7":"H_D['sex'].value_counts().plot(kind = \"bar\", color = [\"#000050\", \"#500000\"])\nplt.title(\"Heart Disease according to Sex\")\nplt.xlabel(\" Male                            Female \") \nplt.legend();","fc32c49b":"H_D.corr()","841689c3":"# Visualizing Correlation in heatmap\ncor_mat = H_D.corr()\nfig, ax = plt.subplots(figsize = (16,10))\nax = sns.heatmap(cor_mat,\n                annot = True,\n                linewidth = 0.5,\n                fmt = \".2f\",\n                cmap = \"YlGnBu\")","c887fdef":"def split_data(data, test_ratio):\n    np.random.seed(42)\n    shuffling_indices = np.random.permutation(len(data))\n    test_size = int(len(data)*test_ratio)\n    test_indices = shuffling_indices[:test_size] # Setting test index from start to upto test_size\n    train_indices = shuffling_indices[test_size:]\n    \n    return data.iloc[train_indices], data.iloc[test_indices]","65eab1d8":"train_set,test_set = split_data(H_D, 0.2) # Passing 20% test data ratio to function","a36f6b4b":"train_set.shape, test_set.shape","9762afb1":"X_train = train_set.drop([\"target\"], axis = 1)\ny_train = train_set[\"target\"]\nX_test = test_set.drop(\"target\", axis = 1)\ny_test = test_set[\"target\"]","246ad5e8":"# No of train and test sets \nlen(X_train), len(y_train), len(X_test), len(y_test)","bd148d4b":"# Instiantiate the model\nlogistic_model = LogisticRegression()\n\n# Fitting the model\nlogistic_model.fit(X_train, y_train)\n\n# Evaluating Model\nlogistic_model.score(X_test, y_test)","7854998b":"logistic_model = LogisticRegression(max_iter=1000)\nlogistic_model.fit(X_train, y_train)\nlogistic_model.score(X_test, y_test)","9a07f83e":"LogisticRegression().get_params().keys()","4acc1ccb":"# Creating a dictionary to pass some of the parameters that LogReg takes\nlog_reg_grid = {\"C\": np.logspace(-4,4,20),\n               \"solver\" : [\"liblinear\"],} #The logspace() function return numbers spaced evenly on a log scale.","11547e4f":"# Setup random hyperparameter search for Logistic Regression\nrandomized_search_log_reg = RandomizedSearchCV(LogisticRegression(),\n                               param_distributions = log_reg_grid,\n                                              cv = 5,\n                                              n_iter = 20,\n                                              verbose = True)","e8e360e9":"# Fitting the hyperparmaeter search model for LogisticRegression\nrandomized_search_log_reg.fit(X_train, y_train)","f5ea8518":"# Checking the score\nrandomized_search_log_reg.score(X_test,y_test), randomized_search_log_reg.best_params_","9d8e3825":"# Setup hyperparameter search \n#grid_search_logistic_regression(gs_lr)\n# Setup logistic regression grid(lrg)\nlrg = {\"C\": np.logspace(-4,4,30),\n                \"solver\": [\"liblinear\"],\n               }\n\ngs_lr = GridSearchCV(LogisticRegression(),\n                    param_grid = lrg,\n                    cv =5,\n                    verbose = True)\ngs_lr.fit(X_train, y_train)","eeed9b8c":"gs_lr.score(X_test, y_test), gs_lr.best_params_","2368c6ea":"y_preds = gs_lr.predict(X_test)\ny_preds","8644f4ed":"# ROC curve\nplot_roc_curve(gs_lr, X_test, y_test)","3298dffa":"# Confusion Matrix\nprint(confusion_matrix(y_test, y_preds))","15135218":"# Visualizing confusion matrix\n\ndef plot_confusion_mat(y_test, y_preds):\n    fig,ax = plt.subplots()\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                    annot = True,\n                    cbar = False)\n    plt.xlabel(\"True Label\")\n    plt.ylabel(\"False Label\")\nplot_confusion_mat(y_test, y_preds)","f1cc54c5":"H_D.tail(4) # Let's look our dataset before feature encoding","5502e224":"dummies = pd.get_dummies(H_D[[\"cp\", \"slope\", \"thal\"]]) # Passing list of categories\nprint(dummies)","658e7b23":"H_D.tail(4) # This is after feature encoding","03b7e730":"X_orig = H_D.drop(\"target\", axis = 1)\ny = H_D.iloc[:, -1].values\nX_orig, y","f25678da":"# Normalizing data , hereX = x normalized\nx = ( X_orig - np.min(X_orig) ) \/ ( np.max(X_orig) -  np.min(X_orig) ).values","541013c2":"from sklearn.model_selection import train_test_split","4266fb48":"# I have used iloc and pandas df method here\nX_orig = H_D.drop(\"target\", axis = 1)\ny = H_D.iloc[:, -1].values\nX_orig, y","478cbf5e":"x = (X_orig - np.min(X_orig)) \/ (np.max(X_orig) - np.min(X_orig)).values","90faebf8":"X_train, X_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)","16424f83":"#Transposing \nX_train = X_train.T\ny_train = y_train.T\nX_test = X_test.T\ny_test = y_test.T","79ee746f":"#initialize\ndef initialize(dimension):\n\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b","31c486a0":"def sigmoid(z):\n    \n    A = 1\/(1+ np.exp(-z))\n#     return (1\/(1+ np.exp(-z)))\n    return A\n\n","71eecd5c":"import numpy as np\nsigmoid(np.array([0,2]))","41e136f6":"m = X_train.shape[1]\nm","4e7e8a74":"def forwardBackward(w,b,X_train,y_train):\n    # Forward\n    \n    Z = np.dot(w.T,X_train) + b\n    A = sigmoid(Z)\n    loss = -(y_train*np.log(A) + (1-y_train)*np.log(1- A))\n    cost = np.sum(loss) \/ m\n    \n    # Backward\n    dw =  np.dot(X_train,((A-y_train).T))\/ m\n    db =  np.sum(A - y_train)\/ m\n    gradients = {\"dw\" : dw, \"db\" : db}\n    \n    return cost,gradients\n","40ef11f3":"def update(w,b,X_train,y_train,learningRate,iteration) :\n    costList = []\n    index = []\n    \n    #for each iteration, update weight and bias values\n    for i in range(iteration):\n        cost,gradients = forwardBackward(w,b, X_train,y_train)\n        w = w - learningRate * gradients[\"dw\"]\n        b = b - learningRate * gradients[\"db\"]\n        \n        costList.append(cost)\n        index.append(i)\n    \n    parameters = {\"weight\": w,\"bias\": b}\n    \n    print(\"iteration:\",iteration)\n    print(\"cost:\",cost)\n\n    plt.plot(index,costList)\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n\n    return parameters, gradients","48507589":"def predict(w,b, X_test):\n    # After update again calculating Z and A(with updated w & b)\n    Z = np.dot(w.T,X_test) + b\n    A = sigmoid(Z)\n\n    y_prediction = np.zeros((1,X_test.shape[1]))\n    \n    for i in range(A.shape[1]):\n        if A[0,i] >= 0.5:\n            y_prediction[0,i] = 1\n        else:\n            y_prediction[0,i] = 0\n    return y_prediction\n","4b388db2":"def logistic_regression(X_train,y_train,X_test,y_test,learningRate,iteration):\n    \n    dimension = X_train.shape[0]\n    w,b = initialize(dimension)\n    \n    parameters, gradients = update(w,b,X_train,y_train,learningRate,iteration)\n\n    y_prediction =  predict(parameters[\"weight\"],parameters[\"bias\"],X_test)\n    \n    print(\"Test Accuracy: {:.2f}%\".format((100 - np.mean(np.abs(y_prediction - y_test))*100)))","5464bb4e":"logistic_regression(X_train,y_train,X_test,y_test,0.05,1000)","a2066f49":"Note\n<font color =\"red\">\n>\n    Male is more 2x than Female.\n","4262e74c":"Insight\n>\n As we can see","c0400210":"Insight\n>\nSince all data are numeric but...\nas I've mentioned before 3 columns are categoric. I will do fearure encoding later..","04b3d361":"<font color = \"Blue\">\n    If AUC = 1, it means there is perfect prediction by the model. If AUC = 0.5 it would mean the model is unable to discriminate between classes.","3b35e86f":"### Loading Data As Pandas DataFrame","0a5a59a9":"Accuracy is lower than the RandomizedSearchCV one way to improve is using stratifiedKfold or for larger dataset GridSearchCV 2.0 = `TuneGridSearchCV` can be used","a54d3c8d":"As we can see the accuracy.\nTo remove warning we can\n1. Increase max iter to higher value like max_iter=1000\n2. Import warning and ignore ","78be05aa":"### Defining a function to split data into Train_Test","5bb973a6":"# Applying LogisticRegression Classifier","1da5657e":"## **Data Scouce**:\n\n[Kaggle](http:\/\/https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci)\n[UCI Repo](http:\/\/https:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease)\n\n\n## **Features\/Attribute Information**::\n\n>\n**Create data dictionary**\n\n1. age - age in years\n2. sex - (1 = male; 0 = female)\n3. cp - chest pain type\n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n5. chol - serum cholestoral in mg\/dl\n    * serum = LDL + HDL + .2 * triglycerides\n    * above 200 is cause for concern\n6. fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n    * '>126' mg\/dL signals diabetes\n7. restecg - resting electrocardiographic results\n    * 0: Nothing to note\n    * 1: ST-T Wave abnormality\n        * can range from mild symptoms to severe problems\n        * signals non-normal heart beat\n    * 2: Possible or definite left ventricular hypertrophy\n        * Enlarged heart's main pumping chamber\n8. thalach - maximum heart rate achieved\n9. exang - exercise induced angina (1 = yes; 0 = no)\n10. oldpeak - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more\n11. slope - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n12. ca - number of major vessels (0-3) colored by flourosopy\n    * colored vessel means the doctor can see the blood passing through, * more blood movement- better  \n13. thal - thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising\n14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)\n","f52cbf76":"Insight\n>\nLess age = High thalach\n","84eb6fed":"<font color = \"#111140\"> \n    Note:Here Cp, Thal and Slope has more category associated with them. \n","40371a8c":"### Sex and Heart Disease","cf7fdb80":"# Modeling","788329b6":"Insight\n>\n1. (1 = Has Heart Disease), as we can see no. of heart disease in female is around 78 and in male is around 98.\n2. Since data of male sample is more than2x than female so its not the perfectly balanced dataset\n3. As we can see female are more likely to have heart disease than male.(Note: data is not equal)","6d230827":"### Exploring Data","962432de":"### Visualizing Age Vs. Thalach","98e35248":"### Converting 3 categorical columns to numeric using pandas","1a19430e":"# Hyperparameter tuning using GridSearchCV","f56b287a":"No. of postive(with) and negative(without) examples are balanced.","117774e8":"Insight\n>\n ","dd580173":"These are the parameters that we can tune|","9bacd091":"Insight\n>\nlooks like age group above 40 are more prone to have heart_disease ...","ede2e160":"### Correlation Matrix","ccdc960f":"<font color = \"blue\">\n1. As we can see the performance has incresed from 0.866 to 0.883 . \n    \n    \n    2. And on using randomsearch for 20 iter best value for C is 0.23..., solver = 'liblinear'","a91f3370":"# Using Logistic regression with 1 Hidden Layer NN","a93b89fa":"Here are the two formulas you will be using: \n\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$","c805b3b1":"Insight\n>\nMedian age is 55+ so it more aapropriate to test t\n![image.png](attachment:image.png)","a353f7a3":"# Hyperparameter Tuning with RandomizedSearchCV","1f9b1c0a":"$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})$$","0c43b2bc":"The steps are as follows:\n1. Converting categorical to Numeric\n2. Normalizing\n3. Splitting data int train and test set\n4. Transpose our data set in order to make ready for dimension match\n\n**Since this is binary classification problem so we will use sigmoid activation function in output layer.**\n4. Define sigmoid function\n5. Initialize weight(random) and bias(0)\n6. Define function to calculate forward( compute activation and cost) and backward propagation(compute gradients from cost)\n7. Define function to update weight and bias and compute new activation function\n8. Use that activation function to predict\/classify","1212a34d":"### Age and heart disease","aeb7511a":"### Heart_Disease Frequency according to Sex.","3f842ad8":"<font color = \"#000050\">\nNo Missing Data.","9a7d33c3":"We can use train_test_split from sklearn which is go to way But for the purpose of learning I am creating function to split data.","185f51c6":"# *This Notebook Covers LogisticRegression Classifier Used To Correctly Classify The Patients With Or Without Heart_Disease*\n\n **Task:**\n> Given Information About various health test of patient, can we predict if someone has heart disease  ?\n\n\n**Algorithm Used**\n>\n`logisticregression()`"}}