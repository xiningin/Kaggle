{"cell_type":{"f2cc414b":"code","19ce042d":"code","515a9143":"code","bba6c786":"code","780a97d3":"code","00598b09":"code","661fc95d":"code","66b866ea":"code","a7dd97d9":"code","663b054e":"code","d2a69be4":"code","7ff22fd4":"code","e718b399":"code","b4e921ad":"code","fb44a37e":"code","97f27db2":"code","24fcd2ed":"code","1a70e70d":"code","adfdd753":"code","0cb53b67":"code","7e04fddb":"code","02d01e22":"code","ce884f9c":"code","35f58224":"code","a0a4b6b5":"code","2d6fe12a":"code","cce60df4":"code","f8497ad5":"code","b1ae690d":"code","3691e86e":"code","d78eb05f":"code","0f96b52e":"code","496169a9":"code","fbe2af62":"code","c7411252":"code","89dca823":"markdown","6eaa71db":"markdown","11794c20":"markdown","8c858913":"markdown","6e8eacdb":"markdown","85295aaa":"markdown","8c2060f8":"markdown","8638ba20":"markdown","7fdc1908":"markdown","cc775cb0":"markdown","e75632fb":"markdown","453f6b45":"markdown","4274cf1a":"markdown","46560d6f":"markdown","8698c556":"markdown","2952251b":"markdown","b78b5424":"markdown","9832d73c":"markdown","64d32723":"markdown","c686aa7e":"markdown","de1b08c5":"markdown","50363f9a":"markdown","e6468926":"markdown","a2f9e6ca":"markdown","fbeaf449":"markdown","8f20b1f0":"markdown","6bde1ea7":"markdown","631ab6f7":"markdown","ef2b031a":"markdown","89bb0d0c":"markdown","743c2ef4":"markdown","58907815":"markdown","802b6a33":"markdown","6ad86327":"markdown","f79659d5":"markdown"},"source":{"f2cc414b":"# Importing the relevant packages\nfrom IPython.display import Image\nimport os\nimport numpy as np\nimport pandas as pd\nimport ast\nimport time \nimport random\nfrom datetime import datetime\nfrom sklearn import ensemble, metrics, linear_model\nfrom sklearn.preprocessing import LabelEncoder, LabelBinarizer, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVR","19ce042d":"Image(\"..\/input\/svdimage\/SVD.png\",width=500)","515a9143":"Image(\"..\/input\/svrimage\/SVR.png\",width=450)","bba6c786":"Metadata = pd.read_csv('\/kaggle\/input\/the-movies-dataset\/movies_metadata.csv')\nNmovies = Metadata['id'].shape[0]\nprint(Nmovies)\nMetadata.head(5)","780a97d3":"# treat or drop mal-formatted data rows\nMetadata['id'][35587] = '22'\nMetadata['id'][29503] = '12'\nMetadata['id'][19730] = '1'\nMetadata['budget'][35587] = '0'\nMetadata['budget'][29503] = '0'\nMetadata['budget'][19730] = '0'\nMetadata['popularity'][35587] = '2.185485'\nMetadata['popularity'][29503] = '1.931659'\nMetadata['popularity'][19730] = '0.065736'\nMetadata['revenue'][35587] = '0'\nMetadata['revenue'][29503] = '0'\nMetadata['revenue'][19730] = '0'\nMetadata['runtime'][35587] = '0'\nMetadata['runtime'][29503] = '0'\nMetadata['runtime'][19730] = '0'\nMetadata['adult'][35587] = 'False'\nMetadata['adult'][29503] = 'False'\nMetadata['adult'][19730] = 'False'\nMetadata['original_language'][35587] = 'en'\nMetadata['original_language'][29503] = 'ja'\nMetadata['original_language'][19730] = 'en'\nMetadata['genres'][35587] = float('nan')\nMetadata['genres'][29503] = float('nan')\nMetadata['genres'][19730] = float('nan')\nMetadata['production_companies'][35587] = \"[{'name': 'Odyssey Media', 'id': 17161}, {'name': 'Pulser Productions', 'id': 18012}, {'name': 'Rogue State', 'id': 18013}, {'name': 'The Cartel', 'id': 23822}]\"\nMetadata['production_companies'][29503] = \"[{'name': 'Aniplex', 'id': 2883}, {'name': 'GoHands', 'id': 7759}, {'name': 'BROSTA TV', 'id': 7760}, {'name': 'Mardock Scramble Production Committee', 'id': 7761}, {'name': 'Sentai Filmworks', 'id': 33751}]\"\nMetadata['production_companies'][19730] = \"[{'name': 'Carousel Productions', 'id': 11176}, {'name': 'Vision View Entertainment', 'id': 11602}, {'name': 'Telescene Film Group Productions', 'id': 29812}]\"\nMetadata['production_countries'][35587] = \"[{'iso_3166_1': 'CA', 'name': 'Canada'}]\"\nMetadata['production_countries'][29503] = \"[{'iso_3166_1': 'US', 'name': 'United States of America'}, {'iso_3166_1': 'JP', 'name': 'Japan'}]\"\nMetadata['production_countries'][19730] = \"[{'iso_3166_1': 'CA', 'name': 'Canada'}, {'iso_3166_1': 'LU', 'name': 'Luxembourg'}, {'iso_3166_1': 'GB', 'name': 'United Kingdom'}, {'iso_3166_1': 'US', 'name': 'United States of America'}]\"\nMetadata['spoken_languages'][35587] = \"[{'iso_639_1': 'en', 'name': 'English'}]\"\nMetadata['spoken_languages'][29503] = \"[{'iso_639_1': 'ja', 'name': '\u65e5\u672c\u8a9e'}]\"\nMetadata['spoken_languages'][19730] = \"[{'iso_639_1': 'en', 'name': 'English'}]\"\nMetadata['release_date'][35587] = '2014-01-01'\nMetadata['release_date'][29503] = '2012-09-29'\nMetadata['release_date'][19730] = '1997-08-20'\nMetadata['original_title'][35587] = 'Avalanche Sharks'\nMetadata['original_title'][29503] = 'Mardock Scramble: The Third Exhaust'\nMetadata['original_title'][19730] = 'Midnight Man'\nMetadata['overview'][35587] = ' Avalanche Sharks tells the story of a bikini contest that turns into a horrifying affair when it is hit by a shark avalanche.'\nMetadata['overview'][29503] = ' Rune Balot goes to a casino connected to the October corporation to try to wrap up her case once and for all.'\nMetadata['overview'][19730] = ' - Written by \u00d8rn\u00e5s'\nMetadata['tagline'][35587] = 'Beware Of Frost Bites'\nMetadata['tagline'][29503] = float('nan')\nMetadata['tagline'][19730] = float('nan')","00598b09":"RatingsDataFrame = pd.read_csv('\/kaggle\/input\/the-movies-dataset\/ratings.csv')\nNusers = RatingsDataFrame['userId'].nunique()\nprint(Nusers)\nRatingsDataFrame.head(10)","661fc95d":"MovieFeatures = pd.DataFrame()\n# keep the numerical columns untouched for now\nMovieFeatures[0] = Metadata['id']\nMovieFeatures[1] = Metadata['budget']\nMovieFeatures[2] = Metadata['popularity']\nMovieFeatures[3] = Metadata['revenue']\nMovieFeatures[4] = Metadata['runtime']\nMovieFeatures[5] = Metadata['vote_average']\nMovieFeatures[6] = Metadata['vote_count']","66b866ea":"# extracting the year from the release date feature\nrelease_date = np.array(Metadata['release_date'])\nfor i in range(0,Nmovies):\n    if (type(release_date[i])==str and \\\n        len(release_date[i]) == 10):\n        year = datetime.strptime(release_date[i], '%Y-%m-%d')\n        year = year.year\n        release_date[i] = year\n    else:\n        release_date[i] = float('nan')\n\nfor i in range(1,31): \n    if (type(release_date[-i])==str and \\\n        len(release_date[-i]) == 10): \n        year = datetime.strptime(release_date[-i], '%Y-%m-%d') \n        year = year.year \n        release_date[-i] = year \n    else: \n        release_date[-i] = float('nan') \n\nMovieFeatures[7] = release_date","a7dd97d9":"Metadata['genres'][0]","663b054e":"# this function extracts words out of dictionaries and formats them\ndef word_extractor(row_of_words):\n    words_joined = []\n    if (type(row_of_words)!=str or type(ast.literal_eval(row_of_words))!=list):\n        words_joined = ['']\n    else:\n    # extract words from the dictionaries\n        word_list = ast.literal_eval(row_of_words)\n        for w in range(0,len(word_list)):\n            word_list[w] = word_list[w]['name']\n            word_list[w] = word_list[w].replace(\" \",\"\")\n        words_joined.append(' '.join(word_list))    \n    return words_joined\n\n# applying the word_extractor function to dict. features\ngenres = []\nfor m in range(0,Nmovies):\n    genres.append(word_extractor(Metadata['genres'][m]))\n\ngenres = np.array(genres)\ngenres = genres.ravel()\nprint(genres[0])","d2a69be4":"# one-hot encoding binary features\nle = LabelEncoder()\nlb = LabelBinarizer()\noriginal_language = le.fit_transform(Metadata['original_language'].fillna('0'))\noriginal_language = lb.fit_transform(original_language)\nfor i in range(8,8+original_language.shape[1]):\n    MovieFeatures[i] = original_language[:,i-8]\n    \n# finally, vectorizing the features.\ncount_vectorizer = CountVectorizer()\ntfid = TfidfVectorizer(stop_words={'english','french','spanish','german'},\\\n                  max_features=200)\n\ngenres = count_vectorizer.fit_transform(genres)\noriginal_title = tfid.fit_transform(Metadata['original_title'])\noverview = tfid.fit_transform(Metadata['overview'].values.astype('U'))\ntagline = tfid.fit_transform(Metadata['tagline'].values.astype('U'))","7ff22fd4":"genres[0].todense()","e718b399":"print(\"finally, vectorizing the features... \\n\")\ncount_vectorizer = CountVectorizer()\ntfid = TfidfVectorizer(stop_words={'english','french','spanish','german'},\\\n                  max_features=200)\n\noriginal_title = tfid.fit_transform(Metadata['original_title'])\noverview = tfid.fit_transform(Metadata['overview'].values.astype('U'))\ntagline = tfid.fit_transform(Metadata['tagline'].values.astype('U'))\n\n# this function records the processed data in the MovieFeatures DataFrame\ndef record_new_data(new_data):\n  size = MovieFeatures.shape[1]\n  for i in range(size,size+new_data.shape[1]):\n    MovieFeatures[i] = new_data.toarray()[:,i-size]\n\nrecord_new_data(genres)\nrecord_new_data(original_title)\nrecord_new_data(overview)\nrecord_new_data(tagline)\n\ngenres = genres.toarray()\nprint(\"Getting rid of NaN values... \\n\")\nMetadata_mean = Metadata.mean(skipna=True,numeric_only=True)\nMovieFeatures[3] = MovieFeatures[3].fillna(Metadata_mean['revenue'])\nMovieFeatures[4] = MovieFeatures[4].fillna(Metadata_mean['runtime'])\nMovieFeatures[5] = MovieFeatures[5].fillna(Metadata_mean['vote_average'])\nMovieFeatures[6] = MovieFeatures[6].fillna(Metadata_mean['vote_count'])\nMovieFeatures = MovieFeatures.fillna('0')","b4e921ad":"print(\"Running PCA on Movie Features... \\n\")\nfeatures = np.array(MovieFeatures)\nscaler = MinMaxScaler(feature_range=[0, 1])\nfeatures[:,3:7] = scaler.fit_transform(features[:, 3:7])\nncomp = 2\npca = PCA(n_components=ncomp)\npca_features = pca.fit_transform(features[:,1:-1]) \nPCAfeatures = np.zeros((pca_features.shape[0],pca_features.shape[1]+1))\nPCAfeatures[:,1:pca_features.shape[1]+1] = pca_features\nPCAfeatures[:,0] = MovieFeatures[0]\nPCA_df = pd.DataFrame(PCAfeatures)\npca_variance = pca.explained_variance_ratio_.sum()","fb44a37e":"print(pca_variance)","97f27db2":"MovieFeatures.head(5)","24fcd2ed":"PCA_df.head(5)","1a70e70d":"def user_dataframe(active_user):\n  user_df = RatingsDataFrame.groupby('userId').get_group(active_user)\n  user_df = PCA_df.merge(user_df,left_on=0,right_on='movieId')\n  return user_df","adfdd753":"user_dataframe(11)","0cb53b67":"# Step 1: this function splits a user dataframe into training and test data\ndef test_split(active_user):\n  percentage = 0.85\n  user_df = user_dataframe(active_user)\n  X = user_df.iloc[:,1:3]\n  y = user_df.iloc[:,5]\n  x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.15)\n  return x_train, x_test, y_train, y_test      ","7e04fddb":"# this function finds the best hyperparameters for an svr user model\ndef svr_tuning(active_user):\n  parameters = {'C':[0.1, 1, 10],'epsilon':[0.1,0.2,0.5],'gamma':['auto','scale']}\n  x_train, x_test, y_train, y_test = test_split(active_user)\n  svr = SVR(gamma='scale')\n  svr = GridSearchCV(svr, parameters, random_state=0)\n  search = svr.fit(x_train, y_train)\n  return search.best_params_\n\n# this function finds the best hyperparameters for an gbr user model\ndef GBR_tuning(active_user):\n  parameters = {'n_estimators':[100,300,500],'learning_rate':[0.001,0.01,0.1,1],\\\n    'loss':['ls','lad','huber','quantile']}\n  x_train, x_test, y_train, y_test = test_split(active_user)\n  GBR = ensemble.GradientBoostingRegressor()\n  clf = GridSearchCV(GBR, parameters)\n  search = clf.fit(x_train, y_train)\n  return search.best_params_","02d01e22":"# this function creates an svr model for an active user\ndef training(active_user):\n  x_train, x_test, y_train, y_test = test_split(active_user)\n  svr = SVR(gamma='auto', epsilon=0.2, C=0.1)\n  LR = linear_model.LinearRegression()\n  GBR = ensemble.GradientBoostingRegressor(learning_rate=0.001,loss='ls',\\\n    n_estimators=100)\n  svr.fit(x_train, y_train)\n  predicted = svr.predict(x_test)\n  model_rmse = np.sqrt(metrics.mean_squared_error(y_test,predicted))\n  return svr, [x_test, y_test], model_rmse","ce884f9c":"# this function returns the N largest elements of a list\ndef Nmaxelements(list1, N): \n    final_list = []  \n    for i in range(0, N):  \n        max1 = 0          \n        for j in range(len(list1)):      \n            if list1[j] > max1: \n                max1 = list1[j];                  \n        list1.remove(max1); \n        final_list.append(max1)          \n    return final_list \n\n# this function creates an svr model for an active user\ndef recommendations(active_user,n_recom):\n  svr, testdata, model_rmse = training(active_user)  \n  recommend = svr.predict(pca_features)\n  recommend_max = Nmaxelements(recommend.tolist(),n_recom)\n  suggestions = []\n  genres_array = np.zeros(20)\n  for i in range(0,len(recommend)): \n     if recommend[i] in recommend_max:\n       suggestions.append(Metadata['original_title'][i])\n       suggestions.append(word_extractor(Metadata['genres'][i]))\n       genres_array = genres_array + genres[i]\n  return genres_array, suggestions, model_rmse","35f58224":"# this function computes a (N movies watched)-weighted accuracy avg for N users\ndef accuracy(N):\n  counter = 0\n  R2 = 0\n  error_rsme = 0\n  for i in range(1,N):\n    active_user = random.randint(1,Nusers)\n    user_df = user_dataframe(active_user)\n    Nmovies_rated = user_df.shape[0]\n    if (Nmovies_rated > 10):\n      svr, testdata, model_rmse = training(active_user)\n      predicted = svr.predict(testdata[0])\n      rmse = np.sqrt(metrics.mean_squared_error(testdata[1],predicted))\n      error_rsme = error_rsme + Nmovies_rated*rmse\n      test_score = svr.score(testdata[0], testdata[1])\n      R2 = R2 + Nmovies_rated*test_score\n      counter = counter + Nmovies_rated\n  error_rsme = error_rsme\/counter\n  R2 = R2\/counter\n  return error_rsme, R2","a0a4b6b5":"# this function computes a diversity index for an active user\ndef diversity(active_user,n_recom):\n  genres_counter = 0\n  Ngenres = 20\n  user_df = user_dataframe(active_user)\n  Nmovies_rated = user_df.shape[0]\n  if (Nmovies_rated > 1):\n    genres_array, suggestions, model_rmse = recommendations(active_user,n_recom)\n    for i in range(0,genres_array.size):\n      if (genres_array[i] > 0):\n        genres_counter = genres_counter + 1\n  genres_counter = genres_counter\/Ngenres\n  return genres_counter\n\n# this function computes a diversity avg index for N users\ndef diversity_avg(N,n_recom):\n  diversity_total = 0\n  for user in range(1,N):\n    diversity_total = diversity_total + diversity(user,n_recom)\n  diversity_total = diversity_total\/N\n  return diversity_total","2d6fe12a":"# this function measures the algorithmic runtime and accuracy for N user models\ndef model_performance(N):\n  t1 = time.clock()\n  error_rsme, R2 = accuracy(N)\n  t2 = time.clock()\n  t  = t2-t1\n  return error_rsme, R2, t","cce60df4":"recommendations(2,5)","f8497ad5":"recommendations(3,5)","b1ae690d":"recommendations(11,5)","3691e86e":"recommendations(13,5)","d78eb05f":"performance = model_performance(100)","0f96b52e":"performance[0]","496169a9":"performance[1]","fbe2af62":"performance[2]","c7411252":"Image(\"..\/input\/noveltyhist\/novelty_hist2.png\",width=750)","89dca823":"<h1>Implementation<\/h1>\n\nSeparating the data into user-specific tables is a necessary step. Each user is treated independently of the others, and for each user, the rating history of others is irelevant for making predictions. Therefore, the system creates a separate model for each user. They learn user-specific preferences and make user-specific recommendations.\n\nThe following sequence of steps describes the methodology applied to each user dataframe:\n\nStep 1: 15% of the data is held-out for testing purposes. the remaining 85% is the training data, and will be used for the learning stage. \n\nStep 2: an SVR model is fit to the preprocessed training data.\n\nStep 3: the model is then applied for making predictions on the test data \n\nStep 4: the predicted ratings are compared with the real ratings to measure the model accuracy. \n\nStep 5: finally, The model is applied to the task of generating custom movie suggestions.","6eaa71db":"with an average R2 score of:","11794c20":"<h1> References <\/h1>\n[1] Corless RM, Fillion N. A graduate introduction to numerical methods. AMC. 2013;10:12.  \n[2] Hastie T, Tibshirani R, Friedman J. The elements of statistical learning: data mining, inference, and prediction. Springer Science & Business Media; 2009 Aug 26.  \n[3] Huang, Hua-juan & Ding, Shifei & Shi, Zhongzhi. (2013). Primal least squares twin support vector regression. Journal of Zhejiang University SCIENCE C. 14. 722-732. 10.1631\/jzus.CIIP1301.  \n[4] https:\/\/www.kaggle.com\/netflix-inc\/netflix-prize-data\/activity  \n[5] CC. Recommender systems. Cham: Springer International Publishing; 2016.  \n[6] M. Pazzani and D. Billsus. Learning and revising user profiles: The identification of interesting Web sites. Machine learning, 27(3), pp. 313?331, 1997.","8c858913":"<h3> Diversity Analysis <\/h3>\n\nLet's see if the models are being realistic in terms of diversity\/novelty. If we measure the diversity index for various user models, and we predict the ratings correctly, intuition indicates that we should obtain a bell-shaped distribution. \n\nDifferent people have different tastes and preferences, some people are versatile and enjoy a fair variety of movie genres, while others enjoy just a few. It is equally improbable to find people who like all kinds of genres, and people who like virtually none. Most people would be located near the centre of the distribution.\n\nAs we increase the number of suggestions $N_s$, the number of recommended genres tends to increase. However, the expected distribution shape would still be a gaussian-like curve, for the effect of this parameter tuning would be to shift the location of the distribution mean. Therefore, we can set the number of suggestions to any number we like, and look for the distribution pattern to see if it checks out.\n\nBy applying the diversity_avg function for 1000 randomly picked user models, and plotting the result into a histogram, we can have an idea of the recommended genres' distribution (fig ?). The distribution has a bell shape! With most occurrences around the centre. These results indicate a great correspondence between the model and reality patterns.\n","6e8eacdb":"<h1>Data Preparation<\/h1>\n\nWe need to get the data in the right shape to be fed into the regression model. The *Metadata* table has a great variety of data types. Regression models can only understand language expressed in numbers, and not natural language, e.g., titles, names in genres, etc. So, we'll need to convert all non-numerical values such as keywords and dictionaries to numerical values such as vectors. The *MovieFeatures* table will store the cleansed data:","85295aaa":"After extracting the genre names of all movies and writing them to a list, we can use *one-hot-encoding* to transform those lists into vectors. The *original languages* are also stored in dictionaries, so they can be vectorized in the same way.","8c2060f8":"A vectorization method was applied to the non-numerical columns, which includes dictionaries and strings. Dictionaries were vectorized using different strategies from strings. An example of a dictionary column is the *genres* feature. Each movie has a set of descriptive genres, and they are stored in a list of dictionaries. E.g.: for the movie \u201dToy Story\u201d, the list of genres is stored as:","8638ba20":"The function below measures the accuracy and computational cost of the recommendation system at the same time by calling the *weighted_accuracy* function and measuring its runtime. It can be used to access the general performance of the system for N random users:","7fdc1908":"The *movieID* key is a common key between the *metadata* table and the *user* table. So, we can join both tables on the *movieID* key and produce a new table with the metadata and user rating for every movie. \n\nOf course, each user will only have watched a subset of movies (no one can watch and rate 45466 movies). And the missing ratings are precisely what we want to predict. If we can accurately estimate the missing ratings, we'll be able to tell if a user will enjoy a movie they haven't seen yet, and we'll be able to make recommendations.\n\nIn this notebook, we'll use regression models to make predictions. Regression is well suited for problems where the labels are ordered numerical quantities. Since the ratings in the *Movies Dataset* range from 1.0 to 5.0, it is appropriate to use regression-based models. Furthermore, once the model is trained, it can be effectively and costlessly applied to new instances.","cc775cb0":"<h3> Performance Analysis <\/h3>\n\nThe performance results above shows that the average RMSE score of 1000 randomly picked user models is:","e75632fb":"Now let's take a look at the ratings table. For each user (we have 270896 users in total), this table has the rating for every movie they have seen and rated:","453f6b45":"The function above creates a merged dataframe for an active user. Let's take a look at the user_dataframe for user 11. It contains the PCA transformed movie features for each movie that user 11 watched, and the rate they provided. We need to be able to predict the rate that user 11 will give to the movies that don't have a rate yet.","4274cf1a":"<h1>Intoduction<\/h1>\n\nThis kernel shows the development, implementation, and results of a movie recommendation engine based on *The Movies Dataset*. Present-day media service providers benefit from using recommender systems on their platforms to increase the number of sales by improving the quality of their services. In particular, movie recommender systems attempts to successfully recommend relevant movies to those who watch them (let's call them users).\n\nThis project uses Support Vector Regression to build a *user-specific* recommendation model. The model is trained on movie attributes such as title, keywords, etc, and learns how to predict user ratings. ","46560d6f":"GridSearchCV lets us search for the best combination of hyperparameters for the model by looking at a grid of values. For SVR, we need to tune the cost, epsilon, and gamma. by giving a range of options for each hyperparameter to GridSearchCV, we can find the best ones.\n\nIt is a good idea to train other regression models and compare their results to the predictions obtained with SVR. Gradient Boosting Regression (GBR) and Linear Regression (LR) were applied to the same problem and compared to SVR. The 3 models are considerably different from one another in terms of the underlying mechanisms. \n\nWhile LR works by applying the standard approach of Least Squares, GBR implements a group of decision trees (random forest) for carrying out the regression task, and converges by iteratively computing a loss parameter of the training predictions, and minimizing it by using the gradient descent of the loss function. The loss function, learning rate and number of trees in the random forest are free hyper parameters, and therefore must be tuned.","8698c556":"If we wish to assess the accuracy of the engine as a whole, we can average the RMSE and R2 over all user models, or we can randomly select a large number of user models to make that assessment, which is what the *model_performance* function does:","2952251b":"Another way to evaluate the model's accuracy is to compare its predicting patterns with reality's behavior. Which is why an additional indicator is being used for validation purposes: the *diversity* measure. \n\nThe diversity of a user model can be computed by counting the number of *different genres* being suggested in the $N_s$ most recommended movies for a user, and dividing through by 20, which is the total number of unique genres in the corpus:","b78b5424":"<h1>Results<\/h1>\n\nWe are now ready to use the system to make costom recommendations (step 5). Let's recommend 5 movies to user number 2:","9832d73c":"Where the 3rd, 4th, and 8th positions of the vector indicate the presence of unique genres. In the entire Movies Dataset, there are 20 unique genre names.\n\nThe vectorization of string features (title, overview, tagline), can be achieved in a similar fashion, though for a corpus of unstructured text, there is a large number of unique words available. According to the experimental results obtained by [6] and described by [5], the number of words used for developing a vectorial space should be somewhere between 50 and 300 for a large data corpus.\n\nThe features in our dataset were better represented when 200 components were used. Noisy words often result in overfitting and should be removed. This includes stop words, i.e., words that are very frequent in the data corpus, but with no significant meaning, such as the, a, an, of, this, in, at, etc. For this reason, stop words from the most common languages found in movies (English, French, Spanish, and German) were removed.","64d32723":"Now that we have the features dataframe ready, we are ready to merge it with the users dataframe. We can write a function using *pandas* commands to do just that:","c686aa7e":"We can write a function to extract the names encoded in a dictionary and write them to a list: ","de1b08c5":"After tuning, we are ready for step 2: training. The function below trains and fit the tuned SVR model to the training data. If you wish to train an LR or GBR model instead, just change the svr.fit( ) call to LR.fit( ) or GBR.fit( ).","50363f9a":"Statistically speaking, words that occur too often are less discriminative. Although it is worth removing the stop words, other frequent words carry some meaning, and we would be throwing out information by simply removing them. Instead, we can down-weight them using the inverse document frequency ($id_i$) index:\n\n\\begin{equation}\nid_i = \\log(n\/n_i)\n\\end{equation}\n\nWhere $n$ is the total number of items (movies) in the collection, $n_i$ is the number of documents for which the ith word occurs \n\nand thus id_i is a decreasing function of n_i. So, instead of using the absolute number of occurrences, in this project, the idi is used for encoding the vector component of each word.\n\nThe vectorization of dictionaries and strings increased the number of features in the user dataset from 14 to 718, with every element of every vector being a new feature. However, those vectors are sparse, i.e., contain a great number of zero elements. Therefore, the 718 columns can be projected into a lower dimensional space without losing a lot of information. \n\nWhen applying SVD for dimensionality reduction, the goal is to transform the original dataset to obtain the fewest number of components without throwing away too much information. In order to evaluate the amount of lost information, the *explained variance* sum is analysed. The explained variance of one component is the ratio between the the variance from that component, and the total variance of the data in the original space. Ideally, the total sum over all components should be 1.\n\nFor this project, it was possible to reduce the 718 features to only 2 features without barely losing any information, as the explained variance of those 2 features summed to 0.9999999999998916.","e6468926":"Now, we can make the recommendations (step 3). To do that, we need to apply the trained model to the remaining movies in the dataframe and predict what will be the scores that a user will give to the movies they haven't watched yet. Remember that each user will have their own model.\n\nWith the predictions made, we can find the movies with the highest predicted scores for that user, and those movies can be recommended to that user. The *recommendations* function below takes the *userID* and the number of recommendations to be generated as arguments, and returns a list of suggested titles, as well as their genres.","a2f9e6ca":"For the *release date*, which is a *datetime* type, only the *year* of release was selected. The dataset contains movies released from 1874 to 2020. But we know that the style of movie production does not change much throughout a year, so only the year information is assumed to be necessary for this feature:","fbeaf449":"For the movie \u201dToy Story\u201d, a vector representation for the genres would be:\n","8f20b1f0":"Now, the predicted ratings are compared with the real ratings to measure the model accuracy (step 4). The *accuracy* function below computes the RMSE and R2 averages for N users, for all the movies in their test sets. ","6bde1ea7":"The number on the bottom of the list is the rmse error of the SVR model for user 2, when 5 movies where selected. the rmse score can be interpreted as a standard deviation, so for the predicted ratings, we would have:\n\n$$pred\\_rating = real\\_rating \\pm rmse_score$$\n\nConsidering that the rating range is (1.0,5.0), if we predict that a user will rate a movie 4.0, and we get a rmse score of 0.6, the user could actually rate it 3.4 or 4.6 on the worst scenarios.\n\nIf an user hasn't rated many movies, their model will not have a lot of supporting data for the learning stage, and consequentially, will be less accurate. The accuracy of a user model also tends to drop as we increase the number of movies being recommended, since we are moving down the list of highest predicted ratings.\n\nLet's take a look at the recommendations generated for other users too, and their model rmse scores:","631ab6f7":"<h1>Background<\/h1>\n\nIn this section, we go over two of the core algorithms applied in this project, as well as the selected evaluation criteria. Feel free to skip this section if you are already familiar with the chosen methods.\n\n<h3>1. Singular Value Decomposition (SVD)<\/h3>\n\nSVD is a matrix factorization algorithm that can be applied for dimensionality reduction problems by finding the largest singular values $\u03c3_1,\u03c3_2,...,\u03c3_r$ of an m\u00d7n matrix $A$, such that:\n\n\\begin{equation}\nA = U\u03a3V^H\n\\end{equation}\n\nwhere U and V are unitary matrices and \u03a3 is an m\u00d7n non-negative diagonal matrix such that \u03a3 = diag($\u03c3_1,\u03c3_2,...,\u03c3_p$) with p = min(m, n)[1].\n\nFurthermore, we can represent the principal components of A by the columns of AV = U\u03a3. The largest principal component of a matrix is the direction that maximizes the variance of the projected data. The figure below shows a geometrical example of a principal component decomposition of 2D data components into 1D components [2]. The new coordinates are the orthogonal distances between the data points and the largest principal component axis.","ef2b031a":"<h3>2. Support Vector Regression (SVR)<\/h3>\n\n\nSVR is a regression algorithm, and therefore it models the relationship between a dependent variable (labels) and independent variables (attributes) of a dataset. What characterizes SVR is that it converges by applying the same principles of a Support Vector Machine (SVM).\n\nSVM is a classification method that attempts to find the fattest hyperplanes that separate the data into groups, or classes. Polynomial or radial basis kernels can be used when the segregation boundaries are not linear. Alternatively, SVR is a method that tries to find the thinnest hyperplane that contains the data points, sequentially fitting a curve through the points by finding the mean location of the hyperplane (see figure below).\n\nThe SVR model can be specified by a set of hyper parameters: epsilon, cost, and gamma. epsilon is the allowed distance between predicted points and actual values (if the predictions are made within the epsilon boundary, no penalties are established). The cost determines how large is the penalty when predictions are made outside of the epsilon distance, and gamma is a parameter that defines the kernel relationship. \n\nThe figure below shows a geometrical example of SVR with a polynomial kernel applied to a dataset where nonlinear relationships being modelled. [3]","89bb0d0c":"![](http:\/\/)","743c2ef4":"Feel free to check out the full paper with the complete discription and analysis of the project, including a residual analysis of the engine as a whole, and of particular user models:\n\nhttps:\/\/github.com\/anacamargos11\/MovieRecommendationSystem\/blob\/master\/RecommendationSystem.pdf\n\nThis is a project that can be extended way beyond the scope of this problem, and it can be applied in a wide range of contexts in addition to the movie industry.","58907815":"<h1>Methodology<\/h1>\n\nOne of the most common strategies for developing recommender systems is applying the collaborative-based method. It works by grouping people into neighbourhoods of users with common interests, with the objective of recommending to users items that were ranked high by their neighbours.\n\nCollaborative-based recommenders need only the rating history of a group of users in order to be applied. Because it doesn\u2019t need many resources, it has been widely used in the literature, including by top competitors of the Netflix Prize data challenge [4].\n\nThe weakness of this approach is the *cold start* problem, i.e., the challenges with recommending movies that are new on the platform, as they don\u2019t have ratings, and therefore lack data to support predictions as to which users they should be recommended to.\n\nThe strategy used in this kernel is content-based. It uses the descriptive attributes of movies (such as title, overview, release date, etc.) and the rating history of each user in order to recommend movies that are similar to the ones they rated high. Content-based methods work well with cold start problems, since a new movie will always have its attributes available to be explored.\n\nLet's explore the shape of the data avaliable for this problem before coming up with a suitable technique. The movie contents are stored in the *movies_metadata* table, which has information about 45466 movies. It contains atributes like genre, title, overview, vote_average, and others that give us a richness of information associated with each movie:","802b6a33":"To get an idea of what the PCA transformation did, let's take a look at the features dataframe before and after the transformation. The *MovieID* key is present on col 0 in both cases, but the remaining values are very different. For the transformed dataframe, we only have 2 columns of values representing the movie features.","6ad86327":"seconds to run everything. \n\nThe RMSE index is a standard deviation of the residuals, and it can be interpreted as an error interval within which the real measurement can lie. The R2 index, on the other hand, measures how well the model is predicting y (in the ideal scenario, R2 = 1.0), and it also measures if the model is *constant*, which would be a model that always predicts the expected value of y (in those cases, R2 = 0.0).\n\nOur recommendation engine has a satisfactory RMSE avg. score, which means that the predictions are being generated within a small error range. The R2 score, however, tells us that the model is a little more *constant* than we would have expected, which may be an indication of a slight underfit.\n\nA *hybrid* approach can be explored in the future. Hybrid recommenders combine the power of collaborative- and content-based systems to produce a more dynamic engine. While content-based systems can effectively tackle the cold start problem, collaborative-based systems are able to provide less predictable recommendations.","f79659d5":"and it took us"}}