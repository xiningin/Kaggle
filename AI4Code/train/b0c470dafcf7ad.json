{"cell_type":{"6fc973a3":"code","e01cf25e":"code","3e1b36f2":"code","a325cb1a":"code","4c43666a":"code","f830bafd":"code","9bd36e19":"code","8427b02c":"code","7b68f709":"code","5717ad4a":"code","c70658f6":"code","57558f50":"code","838f888e":"code","65b88d26":"code","3402cbc1":"code","ef8d738f":"code","6d4cf150":"code","e659403d":"code","ac203e60":"code","a881b5f6":"markdown","62dc68d0":"markdown","ba669a04":"markdown","26ae7681":"markdown","d4ef6c4d":"markdown","b9dbb6dd":"markdown","c818cb48":"markdown","9b355b9c":"markdown"},"source":{"6fc973a3":"# importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","e01cf25e":"data = pd.read_csv(\"..\/input\/rtb\/biddings.csv\")\ndf = pd.DataFrame(data)\ndf.shape","3e1b36f2":"df.head()","a325cb1a":"# showing column wise %ge of NaN values they contains \nnull_col = []\n\nfor i in df.columns:\n  print(i,\"\\t-\\t\", df[i].isna().mean()*100)\n  if df[i].isna().mean()*100 > 0:\n    null_col.append(i)","4c43666a":"df.info()","f830bafd":"plt.figure(figsize=(5,10))\nax = sns.countplot(x='convert', data=df)\n\nfor p in ax.patches:\n        ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))\n","9bd36e19":"class_1 = df[df['convert'] == 1]\nclass_0 = df[df['convert'] == 0].sample(n = len(class_1))","8427b02c":"new_df = pd.concat([class_1, class_0]).sample(frac=1)","7b68f709":"new_df.head()","5717ad4a":"plt.figure(figsize=(5,5))\nax = sns.countplot(x='convert', data=new_df)\n\nfor p in ax.patches:\n        ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))\n","c70658f6":"X = new_df.drop(['convert'], axis=1)\ny = new_df['convert']","57558f50":"from sklearn.decomposition import PCA \npca = PCA(n_components=10) \nX = pd.DataFrame(pca.fit_transform(X))\nX.head()","838f888e":"pca.explained_variance_ratio_ ","65b88d26":"#now lets split data in test train pairs\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)","3402cbc1":"from sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier()\nclf.get_params()","ef8d738f":"from sklearn.model_selection import GridSearchCV\n\n# Number of features to consider at every split \nmax_features = ['auto', 'sqrt', 'log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Criterion used while gormin tree\ncriterion = ['gini', 'entropy']\n# The strategy used to choose the split at each node\nsplitter = ['best', 'random']\n\n# Create the random grid\ngrid = {'max_features': max_features,\n        'max_depth': max_depth,\n        'min_samples_split': min_samples_split,\n        'min_samples_leaf': min_samples_leaf,\n        'criterion': criterion,\n        'splitter': splitter}","6d4cf150":"clf = GridSearchCV( estimator = DecisionTreeClassifier(),  param_grid = grid, cv = 5)\nclf = clf.fit(X_train, y_train)","e659403d":"y_pred = clf.predict(X_test)\n\npred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\npred_df.head()\n","ac203e60":"\nfrom sklearn import metrics\n\n# Generate the roc curve using scikit-learn.\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\nplt.plot(fpr, tpr)\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.show()\n\n# Measure the area under the curve.  The closer to 1, the \"better\" the predictions.\nprint(\"AUC of the predictions: {0}\".format(metrics.auc(fpr, tpr)))\n\n# Measure the Accuracy Score\nprint(\"Accuracy score of the predictions: {0}\".format(metrics.accuracy_score(y_pred, y_test)))\n","a881b5f6":"> In order to get better results lets tune the some specific Hyperparameters using GridSearch","62dc68d0":"> Since data does'nt contain any null values, we can move further","ba669a04":"> Here we ca see that no both the classes are balanced.","26ae7681":"> Here all the features are numerical and contains decimal values. Only last featur has integer values so it's appeared it's our target class with value either 0 or 1.","d4ef6c4d":"> Here this bar graph easily shows how data is imbalanced. Less than 1% data is in class __1__. So, first, we have to balance th data in to get more precise predictions.\n>\n> For Balancing the data I'm using undersampling in which we will reduce the rows of class __0__ to the number equal to that of class __1__ (ratio 1:1).\n","b9dbb6dd":"> Here we got AUC score 0.64 we is not an excellent score but still acceptable.\n> So we can say that our model is 65% acuurate ( Accuracy score = 0.649 ).","c818cb48":"> Since we have a very huge dataset with a large no of features, so in order to precise th results there is need of dimentionality reduction. For that we are using here PCA.","9b355b9c":"## Real Time Bidding\n#### Decision Tree Classification"}}