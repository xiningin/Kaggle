{"cell_type":{"6283d159":"code","659a9f95":"code","5f35d8f5":"code","138f2bec":"code","9b2a510e":"code","691ecea7":"code","c3b76668":"code","d185fb40":"code","ad39e84c":"code","a7144f24":"code","ff0c89a6":"code","44438685":"code","5da82fe8":"code","3f366bb8":"code","2dd1c5ea":"code","0c4c2b30":"code","95d50cea":"code","cae7506a":"code","9695d881":"code","f19be1db":"code","25798867":"code","bf937988":"code","5ff76d48":"code","a0dc40a1":"code","ac027eb7":"markdown","fc497372":"markdown","64d9d354":"markdown","06656787":"markdown"},"source":{"6283d159":"import os\nimport gc\nimport warnings\n\nimport pandas as pd\nfrom pandas.plotting import register_matplotlib_converters\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nimport IPython","659a9f95":"warnings.filterwarnings(\"ignore\")\npd.set_option(\"display.max_columns\", 500)\npd.set_option(\"display.max_rows\", 500)\nregister_matplotlib_converters()\nsns.set()","5f35d8f5":"def display(*dfs, head=True):\n    for df in dfs:\n        IPython.display.display(df.head() if head else df)\n\n\ndef on_kaggle():\n    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ","138f2bec":"if on_kaggle():\n    os.system(\"pip install --quiet mlflow_extend\")","9b2a510e":"def reduce_mem_usage(df, verbose=False):\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    int_columns = df.select_dtypes(include=[\"int\"]).columns\n    float_columns = df.select_dtypes(include=[\"float\"]).columns\n\n    for col in int_columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n\n    for col in float_columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","691ecea7":"def read_data():\n    INPUT_DIR = \"\/kaggle\/input\" if on_kaggle() else \"input\"\n    INPUT_DIR = f\"{INPUT_DIR}\/m5-forecasting-accuracy\"\n\n    print(\"Reading files...\")\n\n    calendar = pd.read_csv(f\"{INPUT_DIR}\/calendar.csv\").pipe(reduce_mem_usage)\n    prices = pd.read_csv(f\"{INPUT_DIR}\/sell_prices.csv\").pipe(reduce_mem_usage)\n\n    sales = pd.read_csv(f\"{INPUT_DIR}\/sales_train_validation.csv\",).pipe(\n        reduce_mem_usage\n    )\n    submission = pd.read_csv(f\"{INPUT_DIR}\/sample_submission.csv\").pipe(\n        reduce_mem_usage\n    )\n\n    print(\"sales shape:\", sales.shape)\n    print(\"prices shape:\", prices.shape)\n    print(\"calendar shape:\", calendar.shape)\n    print(\"submission shape:\", submission.shape)\n\n    # calendar shape: (1969, 14)\n    # sell_prices shape: (6841121, 4)\n    # sales_train_val shape: (30490, 1919)\n    # submission shape: (60980, 29)\n\n    return sales, prices, calendar, submission","c3b76668":"sales, prices, calendar, submission = read_data()\n\nNUM_ITEMS = sales.shape[0]  # 30490\nDAYS_PRED = submission.shape[1] - 1  # 28","d185fb40":"def encode_categorical(df, cols):\n    for col in cols:\n        # Leave NaN as it is.\n        le = LabelEncoder()\n        not_null = df[col][df[col].notnull()]\n        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n\n    return df\n\n\ncalendar = encode_categorical(\n    calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n).pipe(reduce_mem_usage)\n\nsales = encode_categorical(\n    sales, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n).pipe(reduce_mem_usage)\n\nprices = encode_categorical(prices, [\"item_id\", \"store_id\"]).pipe(reduce_mem_usage)","ad39e84c":"def extract_num(ser):\n    return ser.str.extract(r\"(\\d+)\").astype(np.int16)\n\n\ndef reshape_sales(sales, submission, d_thresh=0, verbose=True):\n    # melt sales data, get it ready for training\n    id_columns = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n\n    # get product table.\n    product = sales[id_columns]\n\n    sales = sales.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\",)\n    sales = reduce_mem_usage(sales)\n\n    # separate test dataframes.\n    vals = submission[submission[\"id\"].str.endswith(\"validation\")]\n    evals = submission[submission[\"id\"].str.endswith(\"evaluation\")]\n\n    # change column names.\n    vals.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + DAYS_PRED)]\n    evals.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n\n    # merge with product table\n    evals[\"id\"] = evals[\"id\"].str.replace(\"_evaluation\", \"_validation\")\n    vals = vals.merge(product, how=\"left\", on=\"id\")\n    evals = evals.merge(product, how=\"left\", on=\"id\")\n    evals[\"id\"] = evals[\"id\"].str.replace(\"_validation\", \"_evaluation\")\n\n    if verbose:\n        print(\"validation\")\n        display(vals)\n\n        print(\"evaluation\")\n        display(evals)\n\n    vals = vals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n    evals = evals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n\n    sales[\"part\"] = \"train\"\n    vals[\"part\"] = \"validation\"\n    evals[\"part\"] = \"evaluation\"\n\n    data = pd.concat([sales, vals, evals], axis=0)\n\n    del sales, vals, evals\n\n    data[\"d\"] = extract_num(data[\"d\"])\n    data = data[data[\"d\"] >= d_thresh]\n\n    # delete evaluation for now.\n    data = data[data[\"part\"] != \"evaluation\"]\n\n    gc.collect()\n\n    if verbose:\n        print(\"data\")\n        display(data)\n\n    return data\n\n\ndef merge_calendar(data, calendar):\n    calendar = calendar.drop([\"weekday\", \"wday\", \"month\", \"year\"], axis=1)\n    return data.merge(calendar, how=\"left\", on=\"d\")\n\n\ndef merge_prices(data, prices):\n    return data.merge(prices, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])","a7144f24":"data = reshape_sales(sales, submission, d_thresh=1000)\ndel sales\ngc.collect()\n\ncalendar[\"d\"] = extract_num(calendar[\"d\"])\ndata = merge_calendar(data, calendar)\ndel calendar\ngc.collect()\n\ndata = merge_prices(data, prices)\ndel prices\ngc.collect()\n\ndata = reduce_mem_usage(data)","ff0c89a6":"def add_demand_features(df):\n    for diff in [0, 1, 2]:\n        shift = DAYS_PRED + diff\n        df[f\"shift_t{shift}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(shift)\n        )\n\n    for window in [7, 30, 60, 90, 180]:\n        df[f\"rolling_std_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(window).std()\n        )\n\n    for window in [7, 30, 60, 90, 180]:\n        df[f\"rolling_mean_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(window).mean()\n        )\n\n    for window in [7, 30, 60]:\n        df[f\"rolling_min_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(window).min()\n        )\n\n    for window in [7, 30, 60]:\n        df[f\"rolling_max_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(window).max()\n        )\n\n    df[\"rolling_skew_t30\"] = df.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(30).skew()\n    )\n    df[\"rolling_kurt_t30\"] = df.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(30).kurt()\n    )\n    return df\n\n\ndef add_price_features(df):\n    df[\"shift_price_t1\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.shift(1)\n    )\n    df[\"price_change_t1\"] = (df[\"shift_price_t1\"] - df[\"sell_price\"]) \/ (\n        df[\"shift_price_t1\"]\n    )\n    df[\"rolling_price_max_t365\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.shift(1).rolling(365).max()\n    )\n    df[\"price_change_t365\"] = (df[\"rolling_price_max_t365\"] - df[\"sell_price\"]) \/ (\n        df[\"rolling_price_max_t365\"]\n    )\n\n    df[\"rolling_price_std_t7\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(7).std()\n    )\n    df[\"rolling_price_std_t30\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(30).std()\n    )\n    return df.drop([\"rolling_price_max_t365\", \"shift_price_t1\"], axis=1)\n\n\ndef add_time_features(df, dt_col):\n    df[dt_col] = pd.to_datetime(df[dt_col])\n    attrs = [\n        \"year\",\n        \"quarter\",\n        \"month\",\n        \"week\",\n        \"day\",\n        \"dayofweek\",\n    ]\n\n    for attr in attrs:\n        dtype = np.int16 if attr == \"year\" else np.int8\n        df[attr] = getattr(df[dt_col].dt, attr).astype(dtype)\n\n    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n    return df","44438685":"data = add_demand_features(data).pipe(reduce_mem_usage)\ndata = add_price_features(data).pipe(reduce_mem_usage)\ndt_col = \"date\"\ndata = add_time_features(data, dt_col).pipe(reduce_mem_usage)\ndata = data.sort_values(\"date\")\n\nprint(\"start date:\", data[dt_col].min())\nprint(\"end date:\", data[dt_col].max())\nprint(\"data shape:\", data.shape)","5da82fe8":"class CustomTimeSeriesSplitter:\n    def __init__(self, n_splits=5, train_days=80, test_days=20, day_col=\"d\"):\n        self.n_splits = n_splits\n        self.train_days = train_days\n        self.test_days = test_days\n        self.day_col = day_col\n\n    def split(self, X, y=None, groups=None):\n        SEC_IN_DAY = 3600 * 24\n        sec = (X[self.day_col] - X[self.day_col].iloc[0]) * SEC_IN_DAY\n        duration = sec.max()\n\n        train_sec = self.train_days * SEC_IN_DAY\n        test_sec = self.test_days * SEC_IN_DAY\n        total_sec = test_sec + train_sec\n\n        if self.n_splits == 1:\n            train_start = duration - total_sec\n            train_end = train_start + train_sec\n\n            train_mask = (sec >= train_start) & (sec < train_end)\n            test_mask = sec >= train_end\n\n            yield sec[train_mask].index.values, sec[test_mask].index.values\n\n        else:\n            # step = (duration - total_sec) \/ (self.n_splits - 1)\n            step = DAYS_PRED * SEC_IN_DAY\n\n            for idx in range(self.n_splits):\n                # train_start = idx * step\n                shift = (self.n_splits - (idx + 1)) * step\n                train_start = duration - total_sec - shift\n                train_end = train_start + train_sec\n                test_end = train_end + test_sec\n\n                train_mask = (sec > train_start) & (sec <= train_end)\n\n                if idx == self.n_splits - 1:\n                    test_mask = sec > train_end\n                else:\n                    test_mask = (sec > train_end) & (sec <= test_end)\n\n                yield sec[train_mask].index.values, sec[test_mask].index.values\n\n    def get_n_splits(self):\n        return self.n_splits","3f366bb8":"day_col = \"d\"\ncv_params = {\n    \"n_splits\": 3,\n    \"train_days\": int(365 * 3),\n    \"test_days\": DAYS_PRED,\n    \"day_col\": day_col,\n}\ncv = CustomTimeSeriesSplitter(**cv_params)","2dd1c5ea":"def show_cv_days(cv, X, dt_col, day_col):\n    for ii, (tr, tt) in enumerate(cv.split(X)):\n        print(f\"----- Fold: ({ii + 1} \/ {cv.n_splits}) -----\")\n        tr_start = X.iloc[tr][dt_col].min()\n        tr_end = X.iloc[tr][dt_col].max()\n        tr_days = X.iloc[tr][day_col].max() - X.iloc[tr][day_col].min() + 1\n\n        tt_start = X.iloc[tt][dt_col].min()\n        tt_end = X.iloc[tt][dt_col].max()\n        tt_days = X.iloc[tt][day_col].max() - X.iloc[tt][day_col].min() + 1\n\n        df = pd.DataFrame(\n            {\n                \"start\": [tr_start, tt_start],\n                \"end\": [tr_end, tt_end],\n                \"days\": [tr_days, tt_days],\n            },\n            index=[\"train\", \"test\"],\n        )\n\n        display(df)\n\n\ndef plot_cv_indices(cv, X, dt_col, lw=10):\n    n_splits = cv.get_n_splits()\n    _, ax = plt.subplots(figsize=(20, n_splits))\n\n    # Generate the training\/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X)):\n        # Fill in indices with the training\/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(\n            X[dt_col],\n            [ii + 0.5] * len(indices),\n            c=indices,\n            marker=\"_\",\n            lw=lw,\n            cmap=plt.cm.coolwarm,\n            vmin=-0.2,\n            vmax=1.2,\n        )\n\n    # Formatting\n    MIDDLE = 15\n    LARGE = 20\n    ax.set_xlabel(\"Datetime\", fontsize=LARGE)\n    ax.set_xlim([X[dt_col].min(), X[dt_col].max()])\n    ax.set_ylabel(\"CV iteration\", fontsize=LARGE)\n    ax.set_yticks(np.arange(n_splits) + 0.5)\n    ax.set_yticklabels(list(range(n_splits)))\n    ax.invert_yaxis()\n    ax.tick_params(axis=\"both\", which=\"major\", labelsize=MIDDLE)\n    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=LARGE)\n    return ax","0c4c2b30":"sample = data.iloc[::1000][[day_col, dt_col]].reset_index(drop=True)\nshow_cv_days(cv, sample, dt_col, day_col)\nplot_cv_indices(cv, sample, dt_col)\n\ndel sample\ngc.collect()","95d50cea":"features = [\n    \"item_id\",\n    \"dept_id\",\n    \"cat_id\",\n    \"store_id\",\n    \"state_id\",\n    \"event_name_1\",\n    \"event_type_1\",\n    \"event_name_2\",\n    \"event_type_2\",\n    \"snap_CA\",\n    \"snap_TX\",\n    \"snap_WI\",\n    \"sell_price\",\n    # demand features\n    \"shift_t28\",\n    \"shift_t29\",\n    \"shift_t30\",\n    # std\n    \"rolling_std_t7\",\n    \"rolling_std_t30\",\n    \"rolling_std_t60\",\n    \"rolling_std_t90\",\n    \"rolling_std_t180\",\n    # mean\n    \"rolling_mean_t7\",\n    \"rolling_mean_t30\",\n    \"rolling_mean_t60\",\n    \"rolling_mean_t90\",\n    \"rolling_mean_t180\",\n    # min\n    \"rolling_min_t7\",\n    \"rolling_min_t30\",\n    \"rolling_min_t60\",\n    # max\n    \"rolling_max_t7\",\n    \"rolling_max_t30\",\n    \"rolling_max_t60\",\n    # others\n    \"rolling_skew_t30\",\n    \"rolling_kurt_t30\",\n    # price features\n    \"price_change_t1\",\n    \"price_change_t365\",\n    \"rolling_price_std_t7\",\n    \"rolling_price_std_t30\",\n    # time features\n    \"year\",\n    \"quarter\",\n    \"month\",\n    \"week\",\n    \"day\",\n    \"dayofweek\",\n    \"is_weekend\",\n]\n\n# prepare training and test data.\n# 2011-01-29 ~ 2016-04-24 : d_1    ~ d_1913\n# 2016-04-25 ~ 2016-05-22 : d_1914 ~ d_1941 (public)\n# 2016-05-23 ~ 2016-06-19 : d_1942 ~ d_1969 (private)\n\nis_train = data[\"d\"] < 1914\n\n# Attach \"d\" to X_train for cross validation.\nX_train = data[is_train][[day_col] + features].reset_index(drop=True)\ny_train = data[is_train][\"demand\"].reset_index(drop=True)\nX_test = data[~is_train][features].reset_index(drop=True)\n\n# keep these two columns to use later.\nid_date = data[~is_train][[\"id\", \"date\"]].reset_index(drop=True)\n\ndel data\ngc.collect()\n\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)","cae7506a":"def train_lgb(bst_params, fit_params, X, y, cv, drop_when_train=None):\n    models = []\n\n    if drop_when_train is None:\n        drop_when_train = []\n\n    for idx_fold, (idx_trn, idx_val) in enumerate(cv.split(X, y)):\n        print(f\"\\n----- Fold: ({idx_fold + 1} \/ {cv.get_n_splits()}) -----\\n\")\n\n        X_trn, X_val = X.iloc[idx_trn], X.iloc[idx_val]\n        y_trn, y_val = y.iloc[idx_trn], y.iloc[idx_val]\n        train_set = lgb.Dataset(\n            X_trn.drop(drop_when_train, axis=1),\n            label=y_trn,\n            categorical_feature=[\"item_id\"],\n        )\n        val_set = lgb.Dataset(\n            X_val.drop(drop_when_train, axis=1),\n            label=y_val,\n            categorical_feature=[\"item_id\"],\n        )\n\n        model = lgb.train(\n            bst_params,\n            train_set,\n            valid_sets=[train_set, val_set],\n            valid_names=[\"train\", \"valid\"],\n            **fit_params,\n        )\n        models.append(model)\n\n        del idx_trn, idx_val, X_trn, X_val, y_trn, y_val\n        gc.collect()\n\n    return models","9695d881":"bst_params = {\n    \"boosting_type\": \"gbdt\",\n    \"metric\": \"rmse\",\n    \"objective\": \"poisson\",\n    \"n_jobs\": -1,\n    \"seed\": 42,\n    \"alpha\": 0.1,\n    \"lambda\": 0.1,\n    \"learning_rate\": 0.1,\n    \"bagging_fraction\": 0.75,\n    \"bagging_freq\": 10,\n    \"colsample_bytree\": 0.75,\n}\n\nfit_params = {\n    \"num_boost_round\": 100_000,\n    \"early_stopping_rounds\": 50,\n    \"verbose_eval\": 100,\n}\n\nmodels = train_lgb(\n    bst_params, fit_params, X_train, y_train, cv, drop_when_train=[day_col]\n)\n\ndel X_train, y_train\ngc.collect()","f19be1db":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","25798867":"imp_type = \"gain\"\nimportances = np.zeros(X_test.shape[1])\npreds = np.zeros(X_test.shape[0])\n\nfor model in models:\n    preds += model.predict(X_test)\n    importances += model.feature_importance(imp_type)\n\npreds = preds \/ cv.get_n_splits()\nimportances = importances \/ cv.get_n_splits()","bf937988":"from mlflow_extend import mlflow, plotting as mplt\n\nwith mlflow.start_run():\n    mlflow.log_params_flatten({\"bst\": bst_params, \"fit\": fit_params, \"cv\": cv_params})\n\n\nfeatures = models[0].feature_name()\n_ = mplt.feature_importance(features, importances, imp_type, limit=30)","5ff76d48":"def make_submission(test, submission):\n    preds = test[[\"id\", \"date\", \"demand\"]]\n    preds = preds.pivot(index=\"id\", columns=\"date\", values=\"demand\").reset_index()\n    preds.columns = [\"id\"] + [\"F\" + str(d + 1) for d in range(DAYS_PRED)]\n\n    vals = submission[[\"id\"]].merge(preds, how=\"inner\", on=\"id\")\n    evals = submission[submission[\"id\"].str.endswith(\"evaluation\")]\n    final = pd.concat([vals, evals])\n\n    assert final.drop(\"id\", axis=1).isnull().sum().sum() == 0\n    assert final[\"id\"].equals(submission[\"id\"])\n\n    final.to_csv(\"submission.csv\", index=False)","a0dc40a1":"make_submission(id_date.assign(demand=preds), submission)","ac027eb7":"# https:\/\/github.com\/harupy\/mlflow-extend","fc497372":"As [@kaushal2896](https:\/\/www.kaggle.com\/kaushal2896) suggested in [this comment](https:\/\/www.kaggle.com\/harupy\/m5-baseline#770558), encode the categorical columns before merging to prevent the notebook from crashing even with the full dataset.","64d9d354":"# This kernel is:\n## - Based on [Very fst Model](https:\/\/www.kaggle.com\/ragnar123\/very-fst-model). Thanks [@ragnar123](https:\/\/www.kaggle.com\/ragnar123).\n## - Automatically uploaded by [push-kaggle-kernel](https:\/\/github.com\/harupy\/push-kaggle-kernel).\n## - Formatted by [Black](https:\/\/github.com\/psf\/black).","06656787":"# Objective\n\n* Make a baseline model that predict the validation (28 days).\n* This competition has 2 stages, so the main objective is to make a model that can predict the demand for the next 28 days."}}