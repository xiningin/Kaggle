{"cell_type":{"ad486cc3":"code","b1e1b9a7":"code","002da7e3":"code","b3efd7ae":"code","67695bb9":"code","922352ed":"code","97d5e844":"code","3a573d78":"code","f07498ff":"code","b84c911b":"code","bf154640":"code","f7328cca":"code","e7339e61":"code","6346e76a":"code","a31e6826":"code","e475f13e":"code","fbe2a840":"code","36658d23":"code","5840e7a1":"code","e9f191c5":"code","efacc6f8":"code","f06f416a":"code","003d1a70":"code","f98f2554":"code","6fb67bc4":"code","3e6f2b4e":"code","4b577b92":"code","3fa58e5d":"code","19c3ca69":"markdown","0f203753":"markdown","e24b97fb":"markdown","a4b2aae8":"markdown","433a7590":"markdown","30eae1d5":"markdown","0794e895":"markdown","40dd6e75":"markdown","35f7c2e3":"markdown","cedc9bf2":"markdown","2a6be06f":"markdown","8628ff28":"markdown","60cbb66c":"markdown","858d0b40":"markdown","2c4d7b84":"markdown","addaf576":"markdown"},"source":{"ad486cc3":"import re\nimport numpy as np\nfrom PIL import Image\n\nfrom sklearn.model_selection import train_test_split\nfrom keras import backend as K\nfrom keras.layers import Activation\nfrom keras.layers import Input, Lambda, Dense, Dropout, Convolution2D, MaxPooling2D, Flatten\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import RMSprop\nfrom keras import optimizers\n\nimport matplotlib.image as mpimg \nimport matplotlib.pyplot as plt \n\nfrom keras import callbacks\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\nimport os\nfrom keras.models import Model,load_model\nimport json\nfrom keras.models import model_from_json, load_model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nimport warnings\nwarnings.filterwarnings('ignore')","b1e1b9a7":"selected_image_size = 224\nresize = True\ntotal_sample_size = 10000 # 5k-50k\n\nchannel = 1\nsize = 2\n\nfolder_count = 38\nimage_count = 20 #0-50\n\nif resize == True:\n    batch_size=256\nelse:\n    batch_size=64\n\npath =  os.path.join('..\/input\/plantvillage\/plantvillage_resize_224\/PlantVillage_resize_224\/')\nprint(path)","002da7e3":"def read_image(filename, byteorder='>'):\n    \n    #first we read the image, as a raw file to the buffer\n    with open(filename, 'rb') as f:\n        buffer = f.read()\n    \n    #using regex, we extract the header, width, height and maxval of the image\n    header, width, height, maxval = re.search(\n        b\"(^P5\\s(?:\\s*#.*[\\r\\n])*\"\n        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n]\\s)*)\", buffer).groups()\n    \n    #then we convert the image to numpy array using np.frombuffer which interprets buffer as one dimensional array\n    return np.frombuffer(buffer,\n                            dtype='u1' if int(maxval) < 256 else byteorder+'u2',\n                            count=int(width)*int(height),\n                            offset=len(header)\n                            ).reshape((int(height), int(width)))\n\n\ndef euclidean_distance(vects):\n    x, y = vects\n    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n\n\ndef eucl_dist_output_shape(shapes):\n    shape1, shape2 = shapes\n    return (shape1[0], 1)\n\ndef contrastive_loss(y_true, y_pred):\n    margin = 1\n    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n\ndef compute_accuracy(predictions, labels):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    return labels[predictions.ravel() < 0.5].mean()\n\ndef accuracy(y_true, y_pred):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))","b3efd7ae":"Image.open(path+'s1\/1.jpg')","67695bb9":"image = mpimg.imread(path+'s1\/1.jpg')\ndim1 = image.shape[0]\nprint('dim1',dim1)\ndim2 = image.shape[1]\nprint('dim2',dim2)","922352ed":"image.shape","97d5e844":"def get_data(size, total_sample_size):\n    #read the image\n    image = mpimg.imread(path+'s' + str(1) + '\/' + str(1) + '.jpg', 'rw+')\n    #reduce the size\n    if resize == True:\n        image = image[::size, ::size]\n    #get the new size\n    dim1 = image.shape[0]\n    dim2 = image.shape[1]\n\n    count = 0\n\n    #initialize the numpy array with the shape of [total_sample, no_of_pairs, dim1, dim2]\n    x_geuine_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2])\n\n    y_genuine = np.zeros([total_sample_size,1])\n\n    for i in range(folder_count):\n        for j in range(int(total_sample_size\/folder_count)):\n            ind1 = 0\n            ind2 = 0\n\n            #read images from same directory (genuine pair)\n            while ind1 == ind2:\n                ind1 = np.random.randint(image_count)\n                ind2 = np.random.randint(image_count)\n\n            # read the two images\n            img1 = mpimg.imread(path+'s' + str(i+1) + '\/' + str(ind1 + 1) + '.jpg', 'rw+')\n            img2 = mpimg.imread(path+'s' + str(i+1) + '\/' + str(ind2 + 1) + '.jpg', 'rw+')\n\n            #reduce the size\n            if resize == True:\n                img1 = img1[::size, ::size]\n                img2 = img2[::size, ::size]\n\n            #store the images to the initialized numpy array\n            print\n            x_geuine_pair[count, 0, 0, :, :] = img1\n            x_geuine_pair[count, 1, 0, :, :] = img2\n\n            #as we are drawing images from the same directory we assign label as 1. (genuine pair)\n            y_genuine[count] = 1\n            count += 1\n\n    count = 0\n    x_imposite_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2])\n    y_imposite = np.zeros([total_sample_size, 1])\n\n    for i in range(int(total_sample_size\/image_count)):\n        for j in range(image_count):\n\n            #read images from different directory (imposite pair)\n            while True:\n                ind1 = np.random.randint(folder_count)\n                ind2 = np.random.randint(folder_count)\n                if ind1 != ind2:\n                    break\n\n            img1 = mpimg.imread(path+'s' + str(ind1+1) + '\/' + str(j + 1) + '.jpg', 'rw+')\n            img2 = mpimg.imread(path+'s' + str(ind2+1) + '\/' + str(j + 1) + '.jpg', 'rw+')\n\n            if resize == True:\n                img1 = img1[::size, ::size]\n                img2 = img2[::size, ::size]\n\n            x_imposite_pair[count, 0, 0, :, :] = img1\n            x_imposite_pair[count, 1, 0, :, :] = img2\n            #as we are drawing images from the different directory we assign label as 0. (imposite pair)\n            y_imposite[count] = 0\n            count += 1\n\n    #now, concatenate, genuine pairs and imposite pair to get the whole data\n    #print(x_geuine_pair.shape)\n    #print(x_imposite_pair.shape)\n    X = np.concatenate([x_geuine_pair, x_imposite_pair], axis=0)\/255\n    Y = np.concatenate([y_genuine, y_imposite], axis=0)\n\n    return X, Y\nX, Y = get_data(size, total_sample_size)","3a573d78":"X.shape","f07498ff":"Y.shape","b84c911b":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,5))\nsns.countplot(Y[:,0])\nplt.show()","bf154640":"x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.15)","f7328cca":"print('x_train',x_train.shape)\nprint('x_test',x_test.shape)\nprint('y_train',y_train.shape)\nprint('y_test',y_test.shape)","e7339e61":"def build_base_network(input_shape):\n    \n    seq = Sequential()\n    \n    nb_filter = [16, 32, 16]\n    kernel_size = 3\n    \n    \n    #convolutional layer 1\n    seq.add(Convolution2D(nb_filter[0], kernel_size, kernel_size, input_shape=input_shape,border_mode='valid', dim_ordering='th'))\n    seq.add(Activation('relu'))\n    seq.add(MaxPooling2D(pool_size=(2, 2)))  \n    seq.add(Dropout(.25))\n    \n    #convolutional layer 2\n    seq.add(Convolution2D(nb_filter[1], kernel_size, kernel_size, border_mode='valid', dim_ordering='th'))\n    seq.add(Activation('relu'))\n    seq.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='th')) \n    seq.add(Dropout(.25))\n    \n    #convolutional layer 2\n    seq.add(Convolution2D(nb_filter[2], kernel_size, kernel_size, border_mode='valid', dim_ordering='th'))\n    seq.add(Activation('relu'))\n    seq.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='th')) \n    seq.add(Dropout(.25))\n\n    #flatten \n    seq.add(Flatten())\n    seq.add(Dense(128, activation='relu'))\n    seq.add(Dropout(0.1))\n    seq.add(Dense(50, activation='relu'))\n    return seq","6346e76a":"input_dim = x_train.shape[2:]\nimg_a = Input(shape=input_dim)\nimg_b = Input(shape=input_dim)\nprint('input_dim',input_dim)","a31e6826":"base_network = build_base_network(input_dim)\nfeat_vecs_a = base_network(img_a)\nfeat_vecs_b = base_network(img_b)","e475f13e":"distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([feat_vecs_a, feat_vecs_b])","fbe2a840":"epochs = 20\nrms = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)#RMSprop()\nrms = RMSprop()\n\nearlyStopping = EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=3,\n                              verbose=1,\n                              restore_best_weights=True)\ncallback_early_stop_reduceLROnPlateau=[earlyStopping]","36658d23":"model = Model(input=[img_a, img_b], output=distance)\nmodel.compile(loss=contrastive_loss, optimizer=rms,metrics=[accuracy])\nmodel.summary()","5840e7a1":"img_1 = x_train[:, 0]\nimg2 = x_train[:, 1]\nimg_1.shape\nhistory = model.fit([img_1, img2], y_train, validation_split=.20,\n      batch_size= batch_size, verbose=1, nb_epoch=epochs, callbacks=callback_early_stop_reduceLROnPlateau)\n\n# Option 1: Save Weights + Architecture\nmodel.save_weights('model_weights.h5')\nwith open('model_architecture.json', 'w') as f:\n    f.write(model.to_json())\nprint('saved')","e9f191c5":"pred = model.predict([x_test[:, 0], x_test[:, 1]])\n\nprint('Accuracy on test set: %0.2f%%' % (100 * compute_accuracy(pred, y_test)))","efacc6f8":"pred = model.predict([x_train[:, 0], x_train[:, 1]])\n\nprint('* Accuracy on training set: %0.2f%%' % (100  * compute_accuracy(pred, y_train)))","f06f416a":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n#Train and validation accuracy\nplt.plot(epochs, acc, 'b', label='Training accurarcy')\nplt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\nplt.title('Training and Validation accurarcy')\nplt.legend()\n\nplt.figure()\n#Train and validation loss\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.legend()\nplt.show()","003d1a70":"if resize==True:\n    selected_image_size = int(selected_image_size\/2)\n    print('selected_image_size',selected_image_size)","f98f2554":"target_label = 1\nvalues = np.array(y_test[:,0])\n\ntarget_index = values.tolist().index(target_label)\nprint(target_index)\nprint('target_index value : ',y_test[target_index])","6fb67bc4":"img1 = (x_test[target_index, 0] * 255).astype(np.uint8)\nimg1 = img1.reshape(selected_image_size,selected_image_size)\nprint(img1.shape)\nimg1\nplt.imshow(img1)\nplt.show()","3e6f2b4e":"img2 = (x_test[target_index, 1] * 255).astype(np.uint8)\nimg2 = img2.reshape(selected_image_size,selected_image_size)\nprint(img2.shape)\nimg2\nplt.imshow(img2)\nplt.show()","4b577b92":"x_test[target_index:target_index+1, 0].shape","3fa58e5d":"pred = model.predict([x_test[target_index:target_index+1, 0], x_test[target_index:target_index+1, 1]])\npred = pred < 0.5\nprint('y_test[target_index]:',y_test[target_index,0]==True,' pred :',pred)","19c3ca69":"Now, we set the epoch length to 20 and we use RMS prop for optimization and define our model.","0f203753":"# Test","e24b97fb":"Now, we define a function for reading our input image. The function read_image takes input as an image and returns the numpy array.\nThese feat_vecs_a and feat_vecs_b are the feature vectors of our image pair. Next, we feed this feature vectors to the energy function to compute the distance between them, we use Euclidean distance as our energy function. Next, we define our loss function as contrastive_loss function and compile the model.","a4b2aae8":"Now, we make predictions with test data. Finally, we check our model accuracy.","433a7590":"We will understand the siamese network by building the plant disease model. The objective of our network is to understand whether two plants are similar or dissimilar.\n\nOnce we have our data as pairs along with their labels, we train our siamese network. From the image pair, we feed one image to the network A and another image to the network B. The role of these two networks is only to extract the feature vectors. So, we use two convolution layers with relu activations for extracting the features. Once we have learned the feature, we feed the resultant feature vector from both of the networks to the energy function which measures the similarity, we use Euclidean distance as our energy function. So, we train our network by feeding the image pair to learn the semantic similarity between them.\n\n**What is Siamese Neural Network?**\n\nSiamese Neural Network is a special type of neural network in first we train an image with a sequence of convolutional layers, pooling layers and fully connected layers we end up with a feature vector f(x1).\nThen we train another image in the same sequence to get another feature vector f(x2). Now we compute d which will be the distance between each of the points in feature vector f(x1) with the feature vector f(x2).\nIf d is small we can tell both images are same else if d is large it\u2019s the other way round.\n\n![Fig 1: A Siamese Neural Network for Image Recognition](https:\/\/miro.medium.com\/max\/1068\/1*V27gq7s7elBc8G52T8t1LQ.png)\n\n\n**One-shot Image Recognition**\n\nPeople may ask why have they used One-shot image recognition method though there are other state of art models like CNN and Hierarchical Bayesian Program Learning. The main reason for people using this method is the lack of data. The state of art Machine Learning Algorithms work very well when there is a huge amount of data but can fail miserably if there is a data scarcity.\n\nIn this method the model must make the correct prediction given only one example in each class in the training set. In this paper however the author has used more than one example for each class but it is very less compared to what the state of art algorithm requires.\n","30eae1d5":"# Preparing the Network","0794e895":"Now that, we have successfully generated our data, we build our siamese network. First, we define the base network which is basically a convolutional network used for feature extraction. We build two convolutional layers with rectified linear unit (ReLU) activations and max pooling followed by flat layer.","40dd6e75":"# To prepare the data","35f7c2e3":"For an example, Let us open one image,","cedc9bf2":"\nNow, we generate our data and check our data size. As you can see we have 20,000 data points, out of these 10,000 are genuine pairs and 10,000 are imposite pairs.","2a6be06f":"Next, we split our data for training and testing with 85% training and 15% testing proportions:","8628ff28":"# Predictions","60cbb66c":"# Plant Disease Using Siamese Network - Keras","858d0b40":"References :\n* https:\/\/github.com\/sudharsan13296\/Hands-On-Meta-Learning-With-Python\/blob\/master\/02.%20Face%20and%20Audio%20Recognition%20using%20Siamese%20Networks\/2.4%20Face%20Recognition%20Using%20Siamese%20Network.ipynb\n* https:\/\/keras.io\/examples\/mnist_siamese\/\n* https:\/\/msiam.github.io\/Few-Shot-Learning\/\n* https:\/\/towardsdatascience.com\/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d\n* https:\/\/medium.com\/@subham.tiwari186\/siamese-neural-network-for-one-shot-image-recognition-paper-analysis-44cf7f0c66cb\n* https:\/\/www.katnoria.com\/siamese-one-shot\/\n* https:\/\/sorenbouma.github.io\/blog\/oneshot\/","2c4d7b84":"Next, we feed the image pair, to the base network, which will return the embeddings that is, feature vectors:","addaf576":"Now, we define another function get_data for generating our data. As we know, for the Siamese network, data should be in the form of pairs (genuine and imposite) with a binary label.\n\nFirst, we read the images (img1, img2) from the same directory and store them in the x_genuine_pair array and assign y_genuine to 1. Next, we read the images (img1, img2) from the different directory and store them in the x_imposite pair and assign y_imposite to 0.\n\nFinally, we concatenate both x_genuine_pair, x_imposite to X and y_genuine, y_imposite to Y:"}}