{"cell_type":{"ca4df350":"code","c3baa403":"code","63509faa":"code","111871f5":"code","a84d5ede":"code","c2d037a4":"code","0d22332c":"code","d98c9cc9":"code","45ac833f":"code","a74f6a58":"code","81146d1f":"code","c2b957bb":"markdown","f6031beb":"markdown","09c2c0c7":"markdown","39f855d7":"markdown"},"source":{"ca4df350":"from kaggle.competitions import twosigmanews\nimport numpy as np\nfrom tqdm import tqdm\n\n# Get data\nenv = twosigmanews.make_env()\nmarket_train_df, news_train_df = env.get_training_data()","c3baa403":"market_train_df.head()","63509faa":"news_train_df.head()","111871f5":"# # Get set of subjects\n# subjectSet = set()\n# for row in tqdm(news_train_df['subjects']):\n#     myset = eval(row)\n#     subjectSet = subjectSet.union(myset)","a84d5ede":"# # Convert subjects to columns\n# for subject in tqdm(subjectSet):\n#     news_train_df[subject] = news_train_df[\"subjects\"].str.contains(subject)","c2d037a4":"news_train_df['date'] = news_train_df['time'].dt.date\nmarket_train_df['date'] = market_train_df['time'].dt.date","0d22332c":"%%time\n\n# Make news_train_df smaller\nnews_small = news_train_df.drop([\"time\", \"sourceTimestamp\", \"firstCreated\",\n                               \"sourceId\", \"headline\", \"takeSequence\",\n                               \"provider\", \"subjects\", \"audiences\",\n                               \"companyCount\", \"marketCommentary\", \"assetCodes\"], axis=1)","d98c9cc9":"news_small.head()","45ac833f":"%%time\n# multiply columns by relevance\nweighted_cols = ['urgency', 'bodySize', 'sentenceCount',\n                 'wordCount', 'firstMentionSentence', 'sentimentClass',\n                 'sentimentNegative', 'sentimentNeutral', 'sentimentPositive', \n                 'sentimentWordCount', \"noveltyCount12H\", \"noveltyCount24H\",\n                 \"noveltyCount3D\", \"noveltyCount5D\", \"noveltyCount7D\",\n                 \"volumeCounts12H\", \"volumeCounts24H\", \"volumeCounts3D\",\n                 \"volumeCounts5D\", \"volumeCounts7D\"]\n\n# memory error if we use all columns at once\nfor col in weighted_cols:\n    news_small[col] = news_small[col] * news_small['relevance']","a74f6a58":"%%time\n# sum all columns by group, now relevance becomes total relevance\nsumFunctions = {\"relevance\": np.sum,\n                \"urgency\": np.sum,\n                \"bodySize\": np.sum,\n                \"sentenceCount\": np.sum,\n                \"wordCount\": np.sum,\n                \"firstMentionSentence\": np.sum,\n                \"sentimentClass\": np.sum,\n                \"sentimentNegative\": np.sum,\n                \"sentimentNeutral\": np.sum,\n                \"sentimentPositive\": np.sum,\n                \"sentimentWordCount\": np.sum,\n                \"noveltyCount12H\": np.sum,\n                \"noveltyCount24H\": np.sum,\n                \"noveltyCount3D\": np.sum,\n                \"noveltyCount5D\": np.sum,\n                \"noveltyCount7D\": np.sum,\n                \"volumeCounts12H\": np.sum,\n                \"volumeCounts24H\": np.sum,\n                \"volumeCounts3D\": np.sum,\n                \"volumeCounts5D\": np.sum,\n                \"volumeCounts7D\": np.sum}\nnews_small = news_small.groupby([\"date\",\"assetName\"]).agg(sumFunctions)\n\n# divide everything by total relevance to get weighted averages\nfor col in weighted_cols:\n    news_small[col] = news_small[col] \/ news_small['relevance']\n","81146d1f":"import pandas as pd\n# now we merge market and news, also drop relevance since it is already used\nnews_small = news_small.drop('relevance', axis=1)\ndf = pd.merge(market_train_df, news_small, how='left', on=['date', 'assetName'])\ndf.head()","c2b957bb":"I would like to join the news and market data together into one table for easy training. First, I want to create the date column for both market and news data for me to join them together by. ","f6031beb":"I attempted to encode subjects as catergorical data but it took too long. It takes around 1hr in total just to encode everything. Hence, I have decided that it is not worth it. ","09c2c0c7":"Let's get a view on how the data looks","39f855d7":"After studying the news and market data, I notice that there could be multiple news articles for the same asset per day. This results in multiple rows for the same asset, hence we need to aggregate somehow. I can think of two ways to do it for now. \n\n1) Just take a mean of all the numerical data, combine the non-numerical ones and keep a column to count how many news articles we have. \n2) We aggregate only the rows with news articles we deem significant\n3) Use weighted means of numerical data where the weights are the relevance value\n\nI have decided to go with weighted means for now. And I think I should aggregate before merging the two dataframes together. "}}