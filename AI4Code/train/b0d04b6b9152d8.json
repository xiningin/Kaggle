{"cell_type":{"c370ca5e":"code","36be989c":"code","a7485a57":"code","14395e04":"code","32f1cefc":"code","47bb5b28":"code","39728cb8":"code","8aacaaf2":"code","abd7bdb7":"code","6e6f816b":"code","8039cdef":"code","3ded84d0":"code","e5281577":"code","157f532f":"code","694b80b7":"code","47b6ea65":"code","d48ceb87":"code","5e204805":"code","7a0f2385":"code","ab0dd64d":"code","4185f81c":"code","a994f766":"code","5215d04e":"code","549cc5d9":"code","0a3ea805":"code","ef65d266":"code","ac2ce392":"code","9799fc3e":"code","c1e426b0":"code","2b388360":"code","bd7ef77c":"code","67e69581":"code","b6b4d894":"code","1894abd8":"code","05f67fed":"code","97b9d7c0":"code","9789d47a":"code","eee37f45":"code","11877f41":"code","fdf83dae":"code","c84fbd10":"markdown","cf1de944":"markdown","ef7b366a":"markdown","31f8d884":"markdown","8082738c":"markdown","9602ca19":"markdown","6dd436a1":"markdown","8aa65759":"markdown"},"source":{"c370ca5e":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nfrom numpy import nan\nfrom numpy import absolute\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.graphics.regressionplots import *\nfrom yellowbrick.regressor import CooksDistance\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom xgboost.sklearn import XGBRegressor\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","36be989c":"df = pd.read_csv(\"\/kaggle\/input\/gccsv\/compresive_strength_concrete.csv\")\nprint('Dataset Shape:{}'.format(df.shape))\ndf.head()","a7485a57":"df.columns = [\"Cement\", \"BlastFurnaceSlag\", \"FlyAsh\", \"Water\", \"Superplasticizer\",\n              \"CoarseAggregate\", \"FineAggregare\", \"Age\", \"CC_Strength\"]","14395e04":"print(df.info(), '\\n')\nprint(df.isnull().sum())","32f1cefc":"df.describe()","47bb5b28":"sns.pairplot(df)","39728cb8":"fig, ax = plt.subplots(3,3, figsize=(17,12), constrained_layout=True)\nax=ax.flatten()\nsns.set_style(\"darkgrid\")\nfor num, col in enumerate(df.columns):\n    sns.distplot(df[col], ax=ax[num])\nplt.show()","8aacaaf2":"plt.figure(figsize=(12,9))\nsns.heatmap(df.corr(), annot=True, fmt='.2f')","abd7bdb7":"correlation_unstakced = df.corr().unstack().sort_values(ascending = False)\ncorrelation_unstakced = correlation_unstakced[correlation_unstakced != 1]\ncorrelation_unstakced = np.round(correlation_unstakced, 2)\ncorrelation_unstakced.head(10)","6e6f816b":"fig, ax = plt.subplots(figsize = (12, 10))\nsns.scatterplot(data = df, y = 'CC_Strength', x = 'Cement', hue = 'Age', size = 'Water', \n                ax = ax, sizes = (30, 250))   ","8039cdef":"X = df.iloc[:,:-1]\ny = df.iloc[:,-1]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","3ded84d0":"model = sm.OLS(y_train, sm.add_constant(X_train)).fit()   # To create a model \nmodel_summary = model.summary()\nprint(model_summary)","e5281577":"influence = model.get_influence()\ninfluence_summary = influence.summary_frame()\nprint(influence_summary)\n\n# To studentized the residuals\nstudentized_residuals = influence.resid_studentized_external   \n# Apply Cooks' Distance\n(cooks, p) = influence.cooks_distance\n# Apply DFFITS\n(dffits, p) = influence.dffits \n# The diagonals of the hat matrix indicate the amount of leverage (influence) that observations have in a least squares regression\nleverage = influence.hat_matrix_diag  \n\nprint('\\n')\nprint('Leverage vs. Studentized Residuals')\n\nsns.regplot(leverage, model.resid_pearson, fit_reg=False) \n\nplt.title('Leverage vs. Studentized Residuals')\nplt.xlabel('Leverage')\nplt.ylabel('Studentized Residuals')\nplt.show()","157f532f":"influence_summary","694b80b7":"y_df = pd.DataFrame(y_train, columns = ['CC_Strength'])\nconcat_y_df = pd.concat([y_df, influence_summary], axis = 1)\nconcat_y_df","47b6ea65":"# Identity Outliers Manually\nstudentized_residual = concat_y_df.student_resid\nconcat_y_df.CC_Strength[abs(studentized_residual) >3]","d48ceb87":"features = df.columns[:-1].to_list()\n\n#Number of observations\nn = df.shape[0]\n\n#Predictors\nk = df[features].shape[1]\n\n#Leverage\ncutoff_leverage = ((2*k)+2)\/n","5e204805":"# High leverage data points\nleverage = concat_y_df.hat_diag\nprint(concat_y_df.CC_Strength[abs(leverage) > cutoff_leverage])","7a0f2385":"# student_resid & Get outliers and high leverage data points\noutliers_student_resid = concat_y_df[abs(concat_y_df['student_resid'])>3]\nhigh_leverage = concat_y_df[abs(leverage) > cutoff_leverage]\noutliers_student_resid.shape, high_leverage.shape","ab0dd64d":"# Observations with high leverage, or large residuals are labeled in the plot to show potential influence points.\nfig, ax = plt.subplots(1, 1, figsize = (15, 8))\nfig = sm.graphics.influence_plot(model, ax=ax, criterion = \"cooks\", alpha = 0.5)","4185f81c":"# Visualize outliers using cook's distance\n# Data points that are higher than the red dotted line are considered to be outliers\noutliers_cooks = CooksDistance()\noutliers_cooks.fit(X, y)\noutliers_cooks.show()","a994f766":"# set cutoff and outliers using Cooks Distance\ncutoff_cooks =concat_y_df.loc[:,\"cooks_d\"].mean()*3\noutliers_cooks = concat_y_df.cooks_d[abs(concat_y_df.cooks_d) > cutoff_cooks]","5215d04e":"# set cutoff and outliers using DFFITS\ncutoff_dffits = 2* np.sqrt(k\/n)\noutliers_dffits = concat_y_df[abs(concat_y_df.dffits) > cutoff_dffits]","549cc5d9":"index_student_resid = outliers_student_resid.index.to_list()\nindex_cooks = outliers_cooks.index.to_list()\nindex_dffits = outliers_dffits.index.to_list()","0a3ea805":"index_list = [index_student_resid, index_cooks, index_dffits]\nmodel_names = ['Studentized Residuals', 'Cooks Distance', 'DFFITS']\nfeatures = df.columns[:-1].to_list()\nAIC = []\nBIC = []\nfor index, name in zip(index_list, model_names):\n    X = df[features].drop(index).values\n    y = df['CC_Strength'].drop(index).values\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n    model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n    AIC.append(model.aic)\n    BIC.append(model.bic)","ef65d266":"compare_model = pd.DataFrame(model_names, columns = ['Model Name'])\ncompare_model['AIC'] = AIC\ncompare_model['BIC'] = BIC\ncompare_model.sort_values(by = ['AIC', 'BIC'], ascending = True)\n# A low value of AIC and BIC means less information is lost, thus a better model\n# The lower the values of AIC and BIC, the better the model","ac2ce392":"# DFFITS has the lower value of AIC and BIC\n# Therefore, we are going to use DFFITS to remove outliers\ndf_dffits_removed = df.drop(index_dffits)\ndf_dffits_removed.shape","9799fc3e":"df_dffits_removed.boxplot()","c1e426b0":"# Number of outliers from the original dataset\noutlier_num_list = []\nfor col in df.columns:\n    outlier_num = df[((df[col] - df[col].mean())\/ df[col].std())> 3][col].count()\n    outlier_num_list.append(outlier_num)\noutliers_df = pd.DataFrame(df.columns, columns = ['Features'])\noutliers_df['outliers_num'] = outlier_num_list\noutliers_df.sort_values(by = 'outliers_num', ascending = False)","2b388360":"# Number of outliers after using dffits\noutlier_num_list = []\nfor col in df_dffits_removed.columns:\n    outlier_num = df_dffits_removed[((df_dffits_removed[col] - df_dffits_removed[col].mean())\/ df_dffits_removed[col].std()) > 3][col].count()                         \n    outlier_num_list.append(outlier_num)\noutliers_df = pd.DataFrame(df_dffits_removed.columns, columns = ['Features'])\noutliers_df['outliers_num'] = outlier_num_list\noutliers_df.sort_values(by = 'outliers_num', ascending = False)","bd7ef77c":"# There are still some outliers. I am going to replace those outliers with median\n\ndf_median = df_dffits_removed.copy()\n\nfor col in df_median.columns:\n    Q1 = df_median[col].quantile(0.25)\n    Q3 = df_median[col].quantile(0.75)\n    IQR = Q3 - Q1\n    low = Q1 - 1.5*IQR\n    high = Q3 + 1.5*IQR\n    df_median.loc[(df_median[col] < low) | (df_median[col] > high), col] = df_median[col].median() \n  ","67e69581":"outlier_num_list = []\nfor col in df_median.columns:\n    outlier_num = df_median[((df_median[col] - df_median[col].mean())\/ df_median[col].std()) > 3][col].count()                         \n    outlier_num_list.append(outlier_num)\noutliers_df = pd.DataFrame(df_median.columns, columns = ['Features'])\noutliers_df['outliers_num'] = outlier_num_list\noutliers_df = outliers_df.sort_values(by = 'outliers_num', ascending = False)\noutliers_df    ","b6b4d894":"fig , ax = plt.subplots(3, 3, squeeze=True, figsize=(15, 15))\nax = ax.flatten()\nfor num, col in enumerate(df_median.columns):\n    sns.boxplot(x=col, data = df_median, ax = ax[num])\n    ","1894abd8":"X = df_dffits_removed.iloc[:, :-1]\ny = df_dffits_removed.iloc[:, -1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle=True, random_state=42)\n\nsc = StandardScaler()\nminmax = MinMaxScaler()\nX_train = minmax.fit_transform(X_train)\nX_test = minmax.transform(X_test)\n\nlin_reg = LinearRegression()\nlasso = Lasso()\nridge = Ridge()\nelastic_net = ElasticNet()\nsgd_reg = SGDRegressor()\nrand_reg = RandomForestRegressor()\ntree_reg = DecisionTreeRegressor()\ngb_boost = GradientBoostingRegressor()\nada_boost = AdaBoostRegressor()\nknn_reg = KNeighborsRegressor()\nsvm = SVR(kernel='linear')\nxgb_reg = XGBRegressor()\n\nregressor_list = [lin_reg, lasso, ridge, elastic_net, sgd_reg, rand_reg, \n                  tree_reg, gb_boost, ada_boost, knn_reg, svm, xgb_reg]                             ","05f67fed":"rmse = []\nmse = []\nmae = []\nr2 = []\ny_predicted = []\nfor reg in regressor_list:\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    y_predicted.append(y_pred)\n    rmse.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n    mse.append(mean_squared_error(y_test, y_pred))\n    mae.append(mean_absolute_error(y_test, y_pred))\n    r2.append(r2_score(y_test, y_pred))","97b9d7c0":"fig, ax = plt.subplots(4, 3, sharex = True, sharey = True, figsize = (15,13))\nmodels = ['Linear Regression', 'Lasso Regression', 'Ridge Regression', 'Elastic Net', 'SGD Regressor',\n         'RandomForest Regressor', 'DecisionTree Regressor', 'GradientBoost Regression', \n          'AdaBoost Regressor', 'KNN Regressor', 'SVM', 'XGBoost Regressor']\ny_pred_models = y_predicted\nax = ax.flatten()\nfor num, (pred, model) in enumerate(zip(y_pred_models, models)):\n    ax[num].scatter(pred, y_test, s=20)\n    ax[num].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n    ax[num].set_title(model, fontsize = 14)\n    \nfig.supxlabel('Predicted Values', fontsize = 14)\nfig.supylabel('True Values', fontsize = 14)\nplt.suptitle(\"True Values vs Predicted Values\", fontsize = 14)\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])","9789d47a":"compare_regressor = pd.DataFrame(regressor_list, columns = ['Model'])\ncompare_regressor['rmse'] = rmse\ncompare_regressor['mse'] = mse\ncompare_regressor['mae'] = mae\ncompare_regressor['r2'] = r2\ncompare_regressor.sort_values(by = 'rmse', ascending = True)","eee37f45":"xgb_reg = XGBRegressor()\n\nparams = {'max_depth': [3, 5, 6, 10, 15, 20],\n         'learning_rate': [0.01, 0.1, 0.2, 0.3],\n         'subsample': np.arange(0.5, 1.0, 0.1),\n        'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n         'n_estimators': [100, 200, 300, 400, 500]}\nrand_search = RandomizedSearchCV(estimator = xgb_reg, \n                                param_distributions = params,\n                                scoring='neg_mean_squared_error',\n                                n_iter = 25,\n                                verbose = 1,\n                                return_train_score = True)\nsearch = rand_search.fit(X_train, y_train)\nsearch","11877f41":"search.best_estimator_","fdf83dae":"search.best_estimator_.fit(X_train, y_train)\ny_pred_xgb = search.best_estimator_.predict(X_test)\n\nprint('XGBoost Regressor')\nprint('rmse: {:.2f}, mse: {:.2f}, mae: {:.2f}, R2 score: {:.2f}'.format(np.sqrt(mean_squared_error(y_test, y_pred_xgb)),\n                                                        mean_squared_error(y_test, y_pred_xgb),\n                                                        mean_absolute_error(y_test, y_pred_xgb),\n                                                        r2_score(y_test, y_pred_xgb)))","c84fbd10":"# Outlier Detection","cf1de944":"# Start building a model","ef7b366a":"Concrete Strength has high positive correlation with Cement (0.50), Superplasticizer (0.37), and Age (0.33)\n\nConcrete Strength has high positive correlation with Water (-0.29)","31f8d884":"# Import Libraries Here","8082738c":"### Compare different models according to RMSE, MSE, MAE, and R2 Score","9602ca19":"# Compare the number of outliters before and after using DFFITS","6dd436a1":"### Determine influential points which have high residuals and high leverage\n","8aa65759":"#### It shows that XGBoost Regessor has the lowest RMSE score, the best model\n#### Use Randomized Search Cv to tune the hyperparameters"}}