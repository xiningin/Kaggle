{"cell_type":{"dd0596a8":"code","5827004e":"code","8d6d2892":"code","e9c75c1c":"code","c4b52e29":"code","82c0f50b":"code","36e59300":"code","bae623f2":"code","2494396c":"code","d5cc8fb1":"code","f686c2f3":"code","2cccd1e6":"code","92d437f5":"code","13d21fd7":"code","567b45ed":"code","a4250c80":"code","0b953352":"code","61276907":"code","90e6c319":"code","dc6c79a9":"code","03b34d5a":"code","57ce1dd9":"code","390cdc06":"code","b7b4da34":"code","1edab31c":"code","beb4e4f1":"code","eb2fee06":"code","c8488380":"markdown","8d99bfb6":"markdown","79364f71":"markdown","c745e0e9":"markdown","424e4a55":"markdown","e517fca7":"markdown","ae71f18c":"markdown","d2ee62e4":"markdown","0f14e475":"markdown"},"source":{"dd0596a8":"import os\nimport re\nimport time\nimport string\nimport pickle\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport random\n\nimport torch\nimport torch.nn as nn\nimport seaborn as sns\n\nfrom spacy.lang.en import English\nfrom collections import defaultdict\nfrom nltk.stem import WordNetLemmatizer\n\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nfrom torch.nn.functional import softmax\nfrom torch.optim.swa_utils import AveragedModel, SWALR\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nfrom scipy.stats import skew","5827004e":"def seed_everything(s):\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed(s)\n    torch.cuda.manual_seed_all(s)\n    \nseed_everything(10)","8d6d2892":"class CONFIG:\n    glove_path='..\/input\/glove6b100dtxt\/glove.6B.100d.txt'\n    glove_dim=100\n    gru_hdim=128\n    dmodel=512\n    \n    folds=5\n    batch_size=128\n    max_seq_len=150\n    eval_every=10\n    \n    learning_rate=1e-4\n    weight_decay=1e-4\n    optimizer='AdamW'\n    epochs=30\n    clip_gradient_norm=1.0\n    \n    device=torch.device( 'cuda' if torch.cuda.is_available() else 'cpu')","e9c75c1c":"train_df=pd.read_csv('..\/input\/commonlit-kfold-dataset\/fold_train.csv')\n\ntarget_mean=train_df.target.mean()\ntarget_std=train_df.target.std()\n\nprint(\"Taget Mean:\", target_mean)\nprint(\"Taget Std:\", target_std)\n\ntrain_df['normalized_target']=(train_df.target - target_mean)\/target_std\nsns.histplot(data=train_df, x='normalized_target')","c4b52e29":"qs=[]\nfor i in np.arange(0.1, 1.1, 0.1):\n    q=train_df.normalized_target.quantile(i)\n    qs.append(q)\n\ndef get_quantile(target):\n    for i,q in enumerate(qs):\n        if target<=q:\n            return i\n\ntrain_df['q']=train_df.normalized_target.apply(get_quantile)\ntrain_df.head()","82c0f50b":"%%time\nglove_embeddings={}\nwith open(CONFIG.glove_path) as file:\n    for line in file:\n        line=line.split()\n        word=line[0]\n        v=np.array(line[1:]).astype(np.float)\n        glove_embeddings[word]=v\nprint(len(glove_embeddings))","36e59300":"class Tokenizer:\n    def __init__(self):\n        self.lemmatizer=WordNetLemmatizer()\n        self.nlp=English()\n    def __call__(self, doc):\n        tokens=[]\n        for token in self.nlp(doc):\n            if token.like_num or token.text=='' or (not token.is_ascii):\n                continue\n            token=token.lower_.strip()\n            for p in string.punctuation:\n                token=token.replace(p, ' ')\n            token=token.split(' ')\n            token=[w for w in token if w!='']\n            tokens+=token\n        return tokens","bae623f2":"tokenizer=Tokenizer()\ntrain_df['doc']=train_df.excerpt.apply(tokenizer)\ntrain_df.head()","2494396c":"class TrainDataSampler:\n    def __init__(self, batch_size, df):\n        self.qmap={}\n        self.batch_size=batch_size\n        self.batch_fraction=1.0\n        self.df=df.copy()\n        \n        for i in range(10):\n            ids=self.df[self.df.q==i].id.values\n            np.random.shuffle(ids)\n            self.qmap[i]=ids\n        \n    def convert_sentences(self, num_samples, sentences):\n        X=torch.zeros((num_samples, CONFIG.max_seq_len, CONFIG.glove_dim), dtype=torch.float32)\n        for i, doc in enumerate(sentences):\n            for j, word in enumerate(doc[:CONFIG.max_seq_len]):\n                if word in glove_embeddings:\n                    X[i][j]=torch.tensor(glove_embeddings[word])\n        return X\n    \n    def get_mbs(self):\n        y=[]\n        sentences=[]\n        for i in range(10):\n            if i not in self.qmap:\n                continue\n            yids=self.qmap[i][-12:]\n            y+=list(self.df[self.df.id.isin(yids)].normalized_target.values)\n            sentences+=list(self.df[self.df.id.isin(yids)].doc.values)\n            \n            self.qmap[i]=self.qmap[i][:-12]\n            if len(self.qmap[i]) == 0:\n                self.qmap.pop(i)\n        \n        num_samples=len(y)\n        self.batch_fraction=len(y)\/self.batch_size\n        \n        X=self.convert_sentences(num_samples, sentences)\n        y=torch.tensor(y, dtype=torch.float32)\n        \n        X=X.to(CONFIG.device)\n        y=y.to(CONFIG.device)\n        return X, y\n    \n    def __iter__(self):\n        while len(self.qmap)>0:\n            X, y=self.get_mbs()\n            if self.batch_fraction < 0.5:\n                break\n            yield X, y\n    def __next__(self):\n        for i in range(10):\n            yield i","d5cc8fb1":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, phase):\n        self.df=df\n        self.phase=phase\n    def __getitem__(self, idx):\n        row=self.df.iloc[idx]\n        doc=row.doc\n        \n        X=torch.zeros((CONFIG.max_seq_len, CONFIG.glove_dim), dtype=torch.float32)\n        for i, word in enumerate(doc[:CONFIG.max_seq_len]):\n            if word in glove_embeddings:\n                X[i]=torch.tensor(glove_embeddings[word])\n        \n        if self.phase in ['train', 'val']:\n            y=torch.tensor(row.normalized_target, dtype=torch.float32)\n            return (X.to(CONFIG.device), y.to(CONFIG.device))\n        return X.to(CONFIG.device)\n    def __len__(self):\n        return len(self.df)","f686c2f3":"class ProjectionHead(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bn=nn.BatchNorm1d(CONFIG.dmodel)\n        self.dropout=nn.Dropout()\n        self.relu=nn.ReLU()\n        self.out_layer=nn.Linear(CONFIG.dmodel, 1)\n    def forward(self, x):\n        x=self.bn(x)\n        x=self.dropout(x)\n        x=self.relu(x)\n        x=self.out_layer(x)\n        return x\n\nclass AttentionAggregation(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear=nn.Linear(CONFIG.dmodel, 1)\n    def forward(self, x):\n        w=self.linear(x)\n        w=softmax(w, dim=1)\n        x=x.permute(0, 2, 1)\n        x=torch.matmul(x, w).squeeze(-1)\n        return x\n\nclass FFN(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.linear=nn.Linear(in_dim, out_dim)\n        self.relu=nn.ReLU()\n        self.dropout=nn.Dropout(0.2)\n        self.layer_norm=nn.LayerNorm( (CONFIG.max_seq_len,out_dim) )\n    def forward(self, x):\n        x=self.linear(x)\n        x=self.layer_norm(x)\n        x=self.dropout(x)\n        x=self.relu(x)\n        return x\n    \nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.gru=nn.GRU(CONFIG.glove_dim, CONFIG.gru_hdim, num_layers=2,\n                         bidirectional=True,batch_first=True, dropout=0.25)\n        self.layernorm=nn.LayerNorm((CONFIG.max_seq_len, 2*CONFIG.gru_hdim))\n        self.relu=nn.ReLU()\n        self.ffn=FFN(2*CONFIG.gru_hdim, CONFIG.dmodel)\n        self.attn_agg=AttentionAggregation()\n        self.projection_head=ProjectionHead()\n        \n    def forward(self, x):\n        batch_size=x.shape[0]\n        x,_ = self.gru(x)\n        x=self.layernorm(x)\n        x=self.relu(x)\n        x=self.ffn(x)\n        x=self.attn_agg(x)\n        x=self.projection_head(x)\n        return x","2cccd1e6":"class Trainer:\n    def __init__(self, model, fold_train_df, train_dataloader, val_dataloader):\n        self.model=model\n        self.max_iter_count=500\n        #self.swa_model=AveragedModel(model, device=CONFIG.device)\n        \n        self.criterion=nn.MSELoss(reduction='mean')\n        self.fold_train_df=fold_train_df\n        \n        self.train_dataloader=train_dataloader\n        self.val_dataloader=val_dataloader\n        self.optimizer=torch.optim.AdamW(model.parameters(), lr=CONFIG.learning_rate, weight_decay=CONFIG.weight_decay)\n        self.schedular=torch.optim.lr_scheduler.OneCycleLR(self.optimizer,\n                                                           max_lr=CONFIG.learning_rate,\n                                                           total_steps=self.max_iter_count)\n                                                           #epochs=CONFIG.epochs,\n                                                           #steps_per_epoch=len(self.train_dataloader))\n        self.iter_count=0\n        self.best_loss=None\n        self.best_iteration=None\n        self.train_loss_=[]\n        self.train_batch_stdv_=[]\n        self.batch_skew=[]\n        self.val_loss_=[]\n        #self.swa_start=500\n        \n    def evaluate(self):\n        self.model.eval()\n        ytrue=[]; ypred=[];\n        for X, y in self.val_dataloader:\n            X=X.to(CONFIG.device)\n            ytrue+=y.view(-1).tolist()\n            with torch.no_grad():\n                yhat=self.model(X).view(-1).detach().cpu()\n                ypred+=yhat.tolist()\n        ytrue=torch.tensor(ytrue, dtype=torch.float32)\n        ypred=torch.tensor(ypred, dtype=torch.float32)\n        return self.criterion(ypred, ytrue).item()\n    \n    def checkpoint(self, val_loss):\n        if self.best_loss is None or self.best_loss > val_loss:\n            torch.save(self.model, 'best_model.pt')\n            #torch.save(model, 'model_{}'.format(self.iter_count))\n            self.best_loss=val_loss\n            self.best_iteration=self.iter_count\n    \n    def train_ops(self, X, y):\n        self.model.train()\n        \n        self.optimizer.zero_grad()\n        y_hat=self.model(X).view(-1)\n        \n        loss=self.criterion(y_hat, y)\n        loss.backward()\n        \n        self.optimizer.step()\n        self.schedular.step()\n        \n        return loss.item()\n    \n    def train(self):\n        \n        t1=time.time()\n        for _ in range(CONFIG.epochs):\n            if self.iter_count > self.max_iter_count:\n                break\n            for mbs in TrainDataSampler(120, self.fold_train_df):\n                X, y=mbs\n                self.iter_count+=1\n                if self.iter_count > self.max_iter_count:\n                    break\n                X=X.to(CONFIG.device)\n                y=y.to(CONFIG.device)\n                \n                self.train_batch_stdv_.append(np.std(y.view(-1).tolist()))\n                self.batch_skew.append(skew(y.view(-1).tolist()))\n                \n                train_loss=self.train_ops(X, y)\n                self.train_loss_.append(train_loss)\n                \n                if self.iter_count%CONFIG.eval_every==0:\n                    val_loss=self.evaluate()\n                    self.val_loss_.append(val_loss)\n                    self.checkpoint(val_loss)\n                    t2=time.time()\n                    print(\"======\"*10)\n                    print()\n                    print(\"Iteration:{} | Time Taken: {:.2f} | Train Loss:{:.3f}\".format(self.iter_count, (t2-t1)\/60, self.train_loss_[-1]))\n                    print(\"Val Loss:{:.4f} | Best Loss:{:.4f} | Best Iteration:{}\".format(val_loss,self.best_loss,self.best_iteration))\n                    \n                    #print()\n                    #print(\"======\"*10)\n                    #print(\"Batch Stats:\")\n                    #print(\"Last 3 batch std: \", self.train_batch_stdv_[-3:])\n                    #print(\"Last 3 batch skew: \", self.batch_skew[-3:])\n                    \n                    #print(\"Last 6 Avg std:\", np.mean(self.train_batch_stdv_[-6:]))\n                    #print(\"Last 6 Avg skew:\", np.mean(self.batch_skew[-6:]))\n                    \n                    #print(\"Val Loss:{:.3f}\".format(val_loss))\n                    t1=time.time()\n        #torch.optim.swa_utils.update_bn(self.train_dataloader, self.swa_model)\n        #torch.save(self.swa_model, 'swa_model.pt')\n                    \n    def lr_range_test(self):\n        self.model.train()\n        lrs=[];losses=[]\n        min_lr=5e-5;max_lr=1e-3;\n        optimizer=torch.optim.AdamW(model.parameters(), lr=min_lr, weight_decay=CONFIG.weight_decay)\n        mse_loss=nn.MSELoss(reduction='mean')\n        schedular=torch.optim.lr_scheduler.StepLR(optimizer, 1, 1.05)\n\n        lrs=[]\n        losses=[]\n\n        for i in range(10):\n            print('Epoch:', i+1, schedular.get_last_lr())\n            #for j, (X, y) in enumerate(self.train_dataloader):\n            for X, y in TrainDataSampler(120, self.fold_train_df):\n                y_hat=self.model(X).view(-1)\n                loss=mse_loss(y_hat, y)\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                schedular.step()\n\n                lrs+=schedular.get_last_lr()\n                losses.append(loss.item())\n                if lrs[-1]>max_lr:\n                    break\n            if lrs[-1]>max_lr:\n                break\n        return lrs, losses","92d437f5":"#model=Model()\n#model=model.to(CONFIG.device)\n#trainer=Trainer(model, fold_train_df, train_dataloader, val_dataloader)\n\n#lrs, losses=trainer.lr_range_test()\n#plt.plot(lrs, losses)","13d21fd7":"models=[]\nfor i in range(5):\n    print()\n    print()\n    print(\"===\"*10)\n    print(\"Fold:{}\".format(i))\n    print()\n    fold_train_df=train_df[train_df.kfold!=i].copy()\n    fold_val_df=train_df[train_df.kfold==i].copy()\n\n    train_dataset=Dataset(fold_train_df, 'train')\n    train_dataloader=torch.utils.data.DataLoader(train_dataset, \n                                                 #batch_size=CONFIG.batch_size,\n                                                 batch_size=120,\n                                                 shuffle=True)\n\n    val_dataset=Dataset(fold_val_df, 'val')\n    val_dataloader=torch.utils.data.DataLoader(val_dataset, batch_size=CONFIG.batch_size, shuffle=False)\n\n\n    #Model Instance \n    model=Model()\n    model=model.to(CONFIG.device)\n    \n    #Trainer Instance\n    trainer=Trainer(model, fold_train_df, train_dataloader, val_dataloader)\n    trainer.train()\n    \n    model=torch.load('.\/best_model.pt')\n    models.append(model)\n    \n    plt.title(\"Train Loss- Fold:{}\".format(i))\n    plt.plot(trainer.train_loss_)\n    plt.show()\n    \n    plt.title(\"Val Loss- Fold:{}\".format(i))\n    plt.plot(trainer.val_loss_)\n    plt.show()","567b45ed":"for i, model in enumerate(models):\n    torch.save(model, \"model_{}.pt\".format(i))","a4250c80":"ytrue=[]; ypred=[];\n\nfor X, y in val_dataloader:\n    X=X.to(CONFIG.device)\n    ytrue+=y.view(-1).tolist()\n    \n    yhat=np.zeros(y.size(0))\n    for model in models:\n        model.eval()\n        with torch.no_grad():\n            yhat+=model(X).view(-1).detach().cpu().numpy()\n    \n    yhat\/=len(models)\n    ypred+=list(yhat)\n\nytrue=torch.tensor(ytrue, dtype=torch.float32)\nypred=torch.tensor(ypred, dtype=torch.float32)\nprint(nn.MSELoss(reduction='mean')(ypred, ytrue).item())","0b953352":"def infer(models, dataloader):\n    preds=[]\n    for X in dataloader:\n        X=X.to(CONFIG.device)\n        y_hat=torch.zeros(X.shape[0])\n        for model in models:\n            model.eval()\n            with torch.no_grad():\n                y=model(X).view(-1).detach().cpu()\n                y_hat+=(target_std*y) + target_mean\n        preds+=list(y_hat.numpy()\/len(models))\n    return preds","61276907":"infer_train_dataset=Dataset(train_df, 'test')\ninfer_train_dataloader=torch.utils.data.DataLoader(infer_train_dataset, batch_size=200, shuffle=False)\ntrain_df['preds'] = infer(models, infer_train_dataloader)\ntrain_df[['id', 'target', 'normalized_target', 'preds']].head()","90e6c319":"(np.sqrt((train_df.preds-train_df.target)**2)).mean()","dc6c79a9":"_, ax=plt.subplots(2, 1)\nsns.boxplot(data=train_df, x='target', ax=ax[0])\nsns.boxplot(data=train_df, x='preds', ax=ax[1])","03b34d5a":"sns.histplot(train_df, x='target', bins=100, color='red')\nsns.histplot(train_df, x='preds', bins=100,)\n","57ce1dd9":"train_df.to_csv('train_with_preds.csv', index=False)","390cdc06":"test_df=pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntest_df['doc']=test_df.excerpt.apply(tokenizer)\ntest_df.head()","b7b4da34":"infer_test_dataset=Dataset(test_df, 'test')\ninfer_test_dataloader=torch.utils.data.DataLoader(infer_test_dataset, batch_size=200, shuffle=False)\ntest_df['target'] = infer(models, infer_test_dataloader)\n","1edab31c":"submission_df=test_df[['id', 'target']].copy()\nsubmission_df.head()","beb4e4f1":"submission_df.tail()","eb2fee06":"submission_df.to_csv('submission.csv', index=False)","c8488380":"# Load Glove 100-d vectors","8d99bfb6":"# Config","79364f71":"# Submission","c745e0e9":"# Model","424e4a55":"# Target Normalization","e517fca7":"# Dataset and DataSamplers","ae71f18c":"# Inference","d2ee62e4":"# Train","0f14e475":"# Tokenizer"}}