{"cell_type":{"fa70eec1":"code","5b317e50":"code","d31119a0":"code","b703ba25":"code","d59c8d47":"code","9a55abd0":"code","702ed24d":"code","8b013454":"code","9eedf012":"code","9034eaa7":"code","8a35e3ff":"code","30dedaf1":"code","30cb010e":"code","7a1e3904":"markdown","10659d6f":"markdown","bf79c950":"markdown","ab26ffc7":"markdown"},"source":{"fa70eec1":"import pandas as pd\ndf = pd.read_csv(\"\/kaggle\/input\/brent-oil-prices\/BrentOilPrices.csv\")\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt","5b317e50":"day = []\nmonth = []\nyear = []\nmonth_mapped = []\nindex = []","d31119a0":"#Map text date to number\ndef monthToNum(shortMonth):\n    return{\n            'Jan' : 1,\n            'Feb' : 2,\n            'Mar' : 3,\n            'Apr' : 4,\n            'May' : 5,\n            'Jun' : 6,\n            'Jul' : 7,\n            'Aug' : 8,\n            'Sep' : 9, \n            'Oct' : 10,\n            'Nov' : 11,\n            'Dec' : 12\n    }[shortMonth]","b703ba25":"#Seperate date to month, date, year so we can work with month\nfor i in df['Date']:\n    month.append(i[:3])\n    day.append(i[3:6])\n    year.append(i[8:12])","d59c8d47":"#Mapping month to number\nfor i in month:\n    month_mapped.append(monthToNum(i))\n#Taking difference of dates\nfrom datetime import date\nfor i in range(0, len(day)):\n    d0 = date(int(year[0]), int(month_mapped[0]), int(day[0]))\n    d1 = date(int(year[i]), int(month_mapped[i]), int(day[i]))\n    delta = d1 - d0\n    index.append(delta.days + 1)","9a55abd0":"d = {'Index' : index, \n      'Price' : df['Price']} \nfeatures = 'Index'\ntarget = 'Price'\nnew_df = pd.DataFrame(d)","702ed24d":"df_train = new_df[0:int((len(new_df)*0.98))]\ndf_test = new_df[int((len(new_df)*0.98)):len(new_df)]\ndf_train","8b013454":"plt.figure(figsize=(16,8))\nplt.plot(df_train[features], df_train[target], 'magenta')","9eedf012":"#Data is non-linear, we use polynomial regression\nimport operator\n\nimport numpy as np\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\n\nnp.random.seed(0)\n\n# transforming the data to include another axis\nx = df_train[features]\ny = df_train[target]\nx = x[:, np.newaxis]\ny = y[:, np.newaxis]\n\npolynomial_features= PolynomialFeatures(degree=14)\nx_poly = polynomial_features.fit_transform(x)\n\nmodel = LinearRegression()\nmodel.fit(x_poly, y)\ny_poly_pred = model.predict(x_poly)\n\nrmse = np.sqrt(mean_squared_error(y,y_poly_pred))\nr2 = r2_score(y,y_poly_pred)\nprint(\"RMSE for polynomial regression is\", rmse)\nprint(r2)\n\nfig, ax = plt.subplots(figsize = (16,8))\nax.plot(x, y, '-b', label = 'Data')\n\n# sort the values of x before line plot\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)\nx, y_poly_pred = zip(*sorted_zip)\nax.plot(x, y_poly_pred, '-m', label = 'Fit')\nax.legend()\nplt.show()\n","9034eaa7":"#Using exponential smoothing\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfit = SimpleExpSmoothing(df_train[target]).fit(optimized = True, use_brute = True)\nplt.figure(figsize=(16,8))\nplt.plot(df_train[features], fit.fittedvalues, 'blue', label = r'$\\alpha=%s$'%fit.model.params['smoothing_level'])\nplt.plot(x, y_poly_pred, '-m', label = 'Fit')\nplt.legend()","8a35e3ff":"N = len(df_test['Index'])\nfcast = fit.forecast(N)\nplt.figure(figsize=(16,8))\nplt.plot(df_test['Index'], fcast, 'green', label = r'Forecast, $\\alpha=%s$'%fit.model.params['smoothing_level'])\nplt.plot(df_test[features], df_test[target], 'black', label = 'Data')\nplt.legend()\nrmse = np.sqrt(mean_squared_error(df_test[target],fcast))\nprint(\"RMSE for SES is\", rmse)","30dedaf1":"fit1 = ExponentialSmoothing(df_train[target], seasonal_periods = 8, trend = 'mul', seasonal = 'None').fit()\nfcast1 = fit1.forecast(N)\nplt.figure(figsize=(16,8))\nplt.plot(df_test[features], df_test[target], 'black', label = 'Data')\nplt.plot(df_test[features], fcast1, 'green', label = r'Forecast, $\\alpha=%s$'%fit1.model.params['smoothing_level'])\nrmse = np.sqrt(mean_squared_error(df_test[target],fcast1))\nprint(\"RMSE for Holt is\", rmse)","30cb010e":"from statsmodels.tsa.statespace.sarimax import SARIMAX as sarimax\nfit2 = sarimax(df_train[target], order = (2,1,6), seasonal_order = (0,1,1,8)).fit()\nfcast2 = fit2.forecast(N)\nplt.figure(figsize=(16,8))\nplt.plot(df_test[features], df_test[target], 'black', label = 'Data')\nplt.plot(df_test[features], fcast2, 'green', label = 'Sarimax')\nrmse = np.sqrt(mean_squared_error(df_test[target],fcast2))\nprint(\"RMSE for ARIMA Is\", rmse)","7a1e3904":"RMSE best for Holt's method.","10659d6f":"Regression gave us a fit(a good one), we still cannot forecast future values.","bf79c950":"We can't use date as a feature since it's a combination of text and numbers\nUsing May 20, 1987 as 0, then edit rest of the entries as number of days ahead of base. So May 21,1987 is 1. May 24, 1987 is 4 etc","ab26ffc7":"Data has slight trend, seasonality or cyclicity. We first attempt using regression then SES then Holt's winter mthod then ARIMA.\n"}}