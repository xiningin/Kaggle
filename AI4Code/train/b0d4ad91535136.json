{"cell_type":{"d90472c2":"code","99a3d2f8":"code","fc1b5316":"code","87619b88":"code","f2aaef6d":"code","80ebeaa1":"code","26c46926":"code","6f570a46":"code","f35b70e2":"code","432017a0":"code","40e178a4":"code","27411199":"markdown","1ac9090f":"markdown","c31371b1":"markdown","807d871a":"markdown","ae9d5fb3":"markdown","c8c81f83":"markdown","822d5754":"markdown","d6b1e41e":"markdown"},"source":{"d90472c2":"import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nimport scipy\nfrom tqdm import tqdm_notebook as tqdm","99a3d2f8":"dd0=pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\")\nddtest0=pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\")\nddall=dd0.append(ddtest0, sort=False)\nnum_train=len(dd0)\nddall.head()","fc1b5316":"drop_cols=[\"bin_0\"]\n\n# Split 2 Letters; This is the only part which is not generic and would actually require data inspection\nddall[\"ord_5a\"]=ddall[\"ord_5\"].str[0]\nddall[\"ord_5b\"]=ddall[\"ord_5\"].str[1]\ndrop_cols.append(\"ord_5\")","87619b88":"for col in [\"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\"]:\n    train_vals = set(dd0[col].unique())\n    test_vals = set(ddtest0[col].unique())\n   \n    xor_cat_vals=train_vals ^ test_vals\n    if xor_cat_vals:\n        ddall.loc[ddall[col].isin(xor_cat_vals), col]=\"xor\"","f2aaef6d":"X=ddall[ddall.columns.difference([\"id\", \"target\"] + drop_cols)]","80ebeaa1":"X_oh=X[X.columns.difference([\"ord_1\", \"ord_4\", \"ord_5a\", \"ord_5b\", \"day\", \"month\"])]\noh1=pd.get_dummies(X_oh, columns=X_oh.columns, drop_first=True, sparse=True)\nohc1=oh1.sparse.to_coo()","26c46926":"from sklearn.base import TransformerMixin\nfrom itertools import repeat\nimport scipy\n\n\nclass ThermometerEncoder(TransformerMixin):\n    \"\"\"\n    Assumes all values are known at fit\n    \"\"\"\n    def __init__(self, sort_key=None):\n        self.sort_key = sort_key\n        self.value_map_ = None\n    \n    def fit(self, X, y=None):\n        self.value_map_ = {val: i for i, val in enumerate(sorted(X.unique(), key=self.sort_key))}\n        return self\n    \n    def transform(self, X, y=None):\n        values = X.map(self.value_map_)\n        \n        possible_values = sorted(self.value_map_.values())\n        \n        idx1 = []\n        idx2 = []\n        \n        all_indices = np.arange(len(X))\n        \n        for idx, val in enumerate(possible_values[:-1]):\n            new_idxs = all_indices[values > val]\n            idx1.extend(new_idxs)\n            idx2.extend(repeat(idx, len(new_idxs)))\n            \n        result = scipy.sparse.coo_matrix(([1] * len(idx1), (idx1, idx2)), shape=(len(X), len(possible_values)), dtype=\"int8\")\n            \n        return result","6f570a46":"thermos=[]\nfor col in [\"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5a\", \"day\", \"month\"]:\n    if col==\"ord_1\":\n        sort_key=['Novice', 'Contributor', 'Expert', 'Master', 'Grandmaster'].index\n    elif col==\"ord_2\":\n        sort_key=['Freezing', 'Cold', 'Warm', 'Hot', 'Boiling Hot', 'Lava Hot'].index\n    elif col in [\"ord_3\", \"ord_4\", \"ord_5a\"]:\n        sort_key=str\n    elif col in [\"day\", \"month\"]:\n        sort_key=int\n    else:\n        raise ValueError(col)\n    \n    enc=ThermometerEncoder(sort_key=sort_key)\n    thermos.append(enc.fit_transform(X[col]))","f35b70e2":"ohc=scipy.sparse.hstack([ohc1] + thermos).tocsr()\ndisplay(ohc)\n\nX_train = ohc[:num_train]\nX_test = ohc[num_train:]\ny_train = dd0[\"target\"].values","432017a0":"clf=LogisticRegression(C=0.123456789, solver=\"lbfgs\", max_iter=5000)  # MODEL\n\nclf.fit(X_train, y_train)\n\npred=clf.predict_proba(X_test)[:,1]\n\npd.DataFrame({\"id\": ddtest0[\"id\"], \"target\": pred}).to_csv(\"submission.csv\", index=False)","40e178a4":"from sklearn.model_selection import cross_validate\n\nscore=cross_validate(clf, X_train, y_train, cv=3, scoring=\"roc_auc\")[\"test_score\"].mean()\nprint(f\"{score:.6f}\")","27411199":"# Make prediction","1ac9090f":"The recipe for today is just doing One-Hot most columns and Thermometer encoding of some ordinal columns. Finally, we apply plain Logistic Regression.\n\nDid anyone have success with more complex methods? Please let us know!","c31371b1":"## Thermometer encode some ordinal columns","807d871a":"## Combine sparse matrices","ae9d5fb3":"# Crazy feature engineering","c8c81f83":"## One-Hot Encode all","822d5754":"## Map cat vals which are not in both sets to single values","d6b1e41e":"# Evaluate"}}