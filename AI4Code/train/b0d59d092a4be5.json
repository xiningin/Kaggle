{"cell_type":{"462ba1bd":"code","c592c379":"code","e10c732f":"code","76614ed3":"code","f751108a":"code","e9d59d5e":"code","5e586201":"code","310eb6e2":"code","84993faf":"code","6781a8dc":"code","aa7e4dca":"code","89d55e59":"code","d63fe75a":"code","5823c3bf":"code","6f4954e9":"code","daf95613":"code","71e95df8":"code","adc6f18d":"code","c44597e4":"code","d00f3016":"code","7059f9d2":"code","ba4b055d":"code","d75ac82a":"code","fe15e369":"code","b8da2eeb":"code","24283038":"code","f67bb674":"code","5bd97bd4":"code","3e3ed973":"markdown","9a1953cf":"markdown","60dbe57b":"markdown","c07bc8e1":"markdown","37512a62":"markdown","9be23d6e":"markdown","aa3e8d7c":"markdown","79fb9c37":"markdown","978a807d":"markdown","f4c5a995":"markdown","54627342":"markdown","bb16062e":"markdown","bc5347a4":"markdown","e704b232":"markdown","f4f36f41":"markdown","6ca305bf":"markdown","f99e1d54":"markdown","64480214":"markdown","96558f30":"markdown","e67aa237":"markdown","5d9f35ba":"markdown","d6fce9bc":"markdown","68164afb":"markdown","ef01556d":"markdown","46d13a74":"markdown","04566fbe":"markdown","ad6de1e3":"markdown","4128ee0f":"markdown","7862d266":"markdown","d14ddc2d":"markdown","b22c9e5f":"markdown","d475823f":"markdown","860313a0":"markdown","e03e7a5a":"markdown"},"source":{"462ba1bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c592c379":"df = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndf.head(2)","e10c732f":"df = df.drop(['id','diagnosis','Unnamed: 32'],axis=1) # Dropping Multiple Columns\ndf.head(2)","76614ed3":"X = df.iloc[:,:-1]\ny = df.iloc[:,-1]","f751108a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state= 10)","e9d59d5e":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train,y_train)\ny_predict = lr.predict(X_test)","5e586201":"from sklearn.metrics import mean_squared_error\nimport math\nrsme = math.sqrt(mean_squared_error(y_test,y_predict))\nrsme","310eb6e2":"from sklearn.feature_selection import f_regression as fr\nresult = fr(X,y)\nf_score = result[0]\np_values = result[1]\n\n# Getting column names \ncolumns = list(X.columns)\nprint(\" \")\nprint(\" \")\nprint(\" \")\n\nprint(\"     Features                     \",\"F-Score  \",\"P-Values\")\nprint(\"     ------------                   --------  ---------\")\n\nfor i in range(0,len(columns)):\n    f1 = \"%4.2f\" % f_score[i]\n    p1 = \"%2.6f\" % p_values[i]\n    print(\"    \",columns[i].ljust(25),f1.rjust(12),\"\",p1.rjust(8))","84993faf":"X_train_n = X_train[['texture_mean','smoothness_mean','compactness_mean','concavity_mean','concave points_mean','symmetry_mean','fractal_dimension_mean','perimeter_se','smoothness_se','compactness_se','concavity_se','concave points_se','fractal_dimension_se','radius_worst','texture_worst','perimeter_worst','smoothness_worst','compactness_worst','concavity_worst','concave points_worst','symmetry_worst']]\nX_test_n = X_test[['texture_mean','smoothness_mean','compactness_mean','concavity_mean','concave points_mean','symmetry_mean','fractal_dimension_mean','perimeter_se','smoothness_se','compactness_se','concavity_se','concave points_se','fractal_dimension_se','radius_worst','texture_worst','perimeter_worst','smoothness_worst','compactness_worst','concavity_worst','concave points_worst','symmetry_worst']]","6781a8dc":"from sklearn.linear_model import LinearRegression\nlr1 = LinearRegression()\nlr1.fit(X_train_n,y_train)\ny_predict1 = lr1.predict(X_test_n)","aa7e4dca":"from sklearn.metrics import mean_squared_error\nimport math\nrsme1 = math.sqrt(mean_squared_error(y_test,y_predict1))\nrsme1","89d55e59":"from sklearn.feature_selection import f_regression as fr\nfrom sklearn.feature_selection import SelectKBest\nselectorK = SelectKBest(score_func=fr,k=5)\nX_k = selectorK.fit_transform(X,y)\n#X_k","d63fe75a":"# Getting f_score and p values for the selected features\n#f_score = X_k[0]\n#p_values = X_k[1]\n\nf_score = selectorK.scores_\np_values = selectorK.pvalues_\n\n# Getting column names \ncolumns = list(X.columns)\nprint(\" \")\nprint(\" \")\nprint(\" \")\n\nprint(\"     Features                     \",\"F-Score  \",\"P-Values\")\nprint(\"     ------------                   --------  ---------\")\n\nfor i in range(0,len(columns)):\n    f1 = \"%4.2f\" % f_score[i]\n    p1 = \"%2.6f\" % p_values[i]\n    print(\"    \",columns[i].ljust(25),f1.rjust(12),\"\",p1.rjust(8))","5823c3bf":"# get the column names \ncols = selectorK.get_support(indices = True)\nselectedCols = X.columns[cols].tolist()\nprint(selectedCols)","6f4954e9":"from sklearn.feature_selection import f_regression as fr\nfrom sklearn.feature_selection import SelectPercentile\nselectorP = SelectPercentile(score_func=fr,percentile=20)\nX_p = selectorP.fit_transform(X,y)","daf95613":"# get the column names \ncols_p = selectorP.get_support(indices = True)\nselectedCols_p = X.columns[cols_p].tolist()\nprint(selectedCols_p)","71e95df8":"from sklearn.feature_selection import f_regression as fr\nfrom sklearn.feature_selection import GenericUnivariateSelect\nselectorG1 = GenericUnivariateSelect(score_func=fr,mode='k_best',param=3)\nX_g1 = selectorG1.fit_transform(X,y)","adc6f18d":"# get the column names \ncols_g1 = selectorG1.get_support(indices = True)\nselectedCols_g1 = X.columns[cols_g1].tolist()\nprint(selectedCols_g1)","c44597e4":"from sklearn.feature_selection import f_regression as fr\nfrom sklearn.feature_selection import GenericUnivariateSelect\nselectorG2 = GenericUnivariateSelect(score_func=fr,mode='percentile',param=20)\nX_g2 = selectorG2.fit_transform(X,y)","d00f3016":"# get the column names \ncols_g2 = selectorG2.get_support(indices = True)\nselectedCols_g2 = X.columns[cols_g2].tolist()\nprint(selectedCols_g2)","7059f9d2":"from sklearn.ensemble import RandomForestRegressor\nnp.random.seed()\nforest = RandomForestRegressor(n_estimators=1000)\nfit = forest.fit(X_train,y_train)\naccuracy = fit.score(X_test,y_test)\npredict = fit.predict(X_test)\n#cmatrix = confusion_matrix(y_test,predict)\n\n#-------------------------------------------------------------------------------------------------#\n# Perform k Fold cross- validation \n\nprint('Accuracy of Random Forest: %s'% \"{0:.2%}\".format(accuracy))","ba4b055d":"# Feature importance \nimportances = forest.feature_importances_\nindices = np.argsort(importances)[::-1]","d75ac82a":"import matplotlib.pyplot as plt\nimport seaborn as sns\nprint(\"Feature ranking\")\nfor f in range(X.shape[1]):\n    print(\"Feature %s (%f)\" % (list(X)[f],importances[indices[f]]))\n\nfeat_imp = pd.DataFrame({'Feature':list(X),\n                        'Gini importance':importances[indices]})\nplt.rcParams['figure.figsize']=(12,12)\nsns.set_style('whitegrid')\nax = sns.barplot(x='Gini importance',y='Feature',data=feat_imp)\nax.set(xlabel='Gini Importance')\npass","fe15e369":"from sklearn.linear_model import Lasso \nfrom sklearn.feature_selection import SelectFromModel\nsel_ = SelectFromModel(Lasso(alpha=0.005,random_state=0))\nsel_.fit(X_train,y_train)","b8da2eeb":"sel_.get_support()","24283038":"selected_feat=X_train.columns[(sel_.get_support())]\n\nprint('total features: {}'.format((X_train.shape[1])))\nprint('Selected features: {}'.format(len(selected_feat)))\nprint('Features with coefficients shrank to zero: {}'.format(np.sum(sel_.estimator_.coef_==0)))","f67bb674":"selected_feat","5bd97bd4":"selected_feat = X_train.columns[(sel_.estimator_.coef_!=0).ravel().tolist()]\nselected_feat","3e3ed973":"# 3.Tree Based","9a1953cf":"### Dropping Columns","60dbe57b":"#### Based on K Best","c07bc8e1":"We have dropped the columns which will not be useful for us during demonstration of feature selection techniques.","37512a62":"### You can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code","9be23d6e":"### Feature Importance ","aa3e8d7c":"# 5.Conclusion \n\nIn the Kernel we have covered Univariate,Tree based and Lasso feature selection techniques for Regression Problems,I hope this kernel will be useful for you in future to carry out activity of feature selection on your dataset.","79fb9c37":"### b)Select Percentile","978a807d":"### c)Generic Univariate Select","f4c5a995":"### Recently I published a self help book titled Inspiration: Thoughts on Spirituality, Technology, Wealth, Leadership and Motivation. The preview of the book can be read from the Amazon link https:\/\/lnkd.in\/gj7bMQA\n\n### You can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code","54627342":"# TO BE CONTINUED","bb16062e":"### Splitting data into Independent and Dependent Features","bc5347a4":"### Updated Matrix of features","e704b232":"Doing a feature selection based on p value of 0.05 has increased rsme value.So it tells us that some features having p value more than 0.05 have statistical significance on the outcome of the prediction.So we can either change the p value or include all the features while predicting the outcome.","f4f36f41":"#### Based on Percentile","6ca305bf":"### a)Select K Best\n\nIn this case we will be specifying the value of K which represents the number of best features we want to select.","f99e1d54":"### You can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code","64480214":"### Updated Linear Model","96558f30":"## Model Performance With Selected Features","e67aa237":"Here we are taking all the feature except fractal_dimension_worst as independent variables and fractal_dimension_worst is considered as dependent variables.","5d9f35ba":"### Mean Square Error","d6fce9bc":"We have see how we are able to do a feature slection using Lasso Regression.By changing the value of alpha we will be ablw to get desired number of featured for our ML Model.","68164afb":"In an machine learning problem one of the most import steps is feature selection.Feature selection helps in simplification of model,improves accuracy,reduces training time,reduces overfitting and helps in avoiding curse of dimentionality.Here we will cover different techniques like\n\n1.Univariate feature Selection using p value and f Distribution\n\n2.Types of Univarate Feature Selection \n\na)Select K Best\n\nb)Select Percentile\n\nc)Generic Univariate Select\n\n3.Tree Based using Random Forest\n\n4.Regularization using Lasso\n\n5.Conclusion ","ef01556d":"Using the Random Forest Regressor we have done feature selection based on their importance.","46d13a74":"So if we were to selected 5 best features for our prediction then they will be the five features desplayed above.They are based on five lowest p values which is less than 0.05","04566fbe":"# 2.Types of Univariate Feature Selection","ad6de1e3":"# 1.Univariate Feature Selection using f Distribution","4128ee0f":"So we have selected 20 % of the important features in our dataset.So we get a list of 6 most important features.","7862d266":"### Importing Data Set ","d14ddc2d":"### Test Train Split","b22c9e5f":"So we have covered different Univariate Feature selection techiniques in the above sections.","d475823f":"# 4.Lasso Regression","860313a0":"### Linear Regression","e03e7a5a":"So now we have the F_score and p_values for all the features.For a feature to be statistically significant the p values should be less than 5% ie 0.05.If for any feature the p values is more than 0.05 we can discard the feature as it wont be statistically significant for predicting the dependent feature."}}