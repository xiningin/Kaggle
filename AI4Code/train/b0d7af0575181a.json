{"cell_type":{"601c0e2f":"code","06d2ee4f":"code","8cdb844a":"code","4e4497cd":"code","5320ab1a":"code","224e79c8":"code","9c0ce411":"code","2b0799a8":"code","3b0da799":"markdown","d6d7644a":"markdown","b3bce6c4":"markdown","5a38d3bd":"markdown","33c787de":"markdown","e957604d":"markdown","f1610989":"markdown","f059226d":"markdown","0aa03728":"markdown","4f5b10ce":"markdown","df2a729f":"markdown","1c3042c0":"markdown","56d272b4":"markdown"},"source":{"601c0e2f":"!apt-get update\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n!pip3 install tensorflow==2.2\n!pip3 install tensorflow_probability==0.9.0\n!pip3 install kaggle-environments -U\n\nfrom IPython.display import Image, clear_output\n\nclear_output()","06d2ee4f":"!cp -r \/kaggle\/input\/gfootball-baseline\/* .\n\n!mkdir -p football\/third_party\/gfootball_engine\/lib\n!wget https:\/\/storage.googleapis.com\/gfootball\/prebuilt_gameplayfootball_v2.3.so -O football\/third_party\/gfootball_engine\/lib\/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .\n\n!mkdir -p football\/third_party\/gfootball_engine\n!cd seed_rl && git checkout 34fb2874d41241eb4d5a03344619fb4e34dd9be6\n\n!mkdir \/kaggle_simulations\/agent\n\nclear_output()","8cdb844a":"%%writefile train.sh\n# Training launcher script.\n\n# Make SEED RL visible to Python.\nexport PYTHONPATH=$PYTHONPATH:$(pwd)\n#export PYTHONPATH=$PYTHONPATH:\n\nENVIRONMENT=$1\nAGENT=$2\nNUM_ACTORS=$3\nshift 3\n\n# Start actor tasks which run environment loop.\nactor=0\nwhile [ \"$actor\" -lt ${NUM_ACTORS} ]; do\n  python3 seed_rl\/${ENVIRONMENT}\/${AGENT}_main.py --run_mode=actor --logtostderr $@ --num_actors=${NUM_ACTORS} --task=${actor} 2>\/dev\/null >\/dev\/null &\n  actor=$(( actor + 1 ))\ndone\n# Start learner task which performs training of the agent.\npython3 seed_rl\/${ENVIRONMENT}\/${AGENT}_main.py --run_mode=learner --logtostderr $@ --num_actors=\"${NUM_ACTORS}\"","4e4497cd":"!bash train.sh football vtrace 8 '--total_environment_frames=1000000 --game=11_vs_11_kaggle --reward_experiment=scoring,checkpoints --logdir=\/kaggle_simulations\/agent\/'\n\n#clear_output()","5320ab1a":"!ls -la \/kaggle_simulations\/agent\/saved_model","224e79c8":"%%writefile \/kaggle_simulations\/agent\/main.py\n\nimport collections\nimport gym\nimport numpy as np\nimport os\nimport sys\nimport tensorflow as tf\n\nfrom gfootball.env import observation_preprocessing\nfrom gfootball.env import wrappers\n\nEnvOutput = collections.namedtuple(\n    'EnvOutput', 'reward done observation abandoned episode_step')\n\ndef prepare_agent_input(observation, prev_action, state):\n    # SEED RL agent accepts input in a form of EnvOutput. When not training\n    # only observation is used for generating action, so we use a dummy values\n    # for the rest.\n    env_output = EnvOutput(reward=tf.zeros(shape=[], dtype=tf.float32),\n        done=tf.zeros(shape=[], dtype=tf.bool),\n        observation=observation, abandoned=False,\n        episode_step=tf.zeros(shape=[], dtype=tf.int32))\n    # add batch dimension\n    prev_action, env_output = tf.nest.map_structure(\n        lambda t: tf.expand_dims(t, 0), (prev_action, env_output))\n\n    return (prev_action, env_output, state)\n\n# Previously executed action\nprevious_action = tf.constant(0, dtype=tf.int64)\n# Queue of recent observations (SEED agent we trained uses frame stacking).\nobservations = collections.deque([], maxlen=4)\n# Current state of the agent (used by recurrent agents).\nstate = ()\n\n# Load previously trained Tensorflow model.\npolicy = tf.compat.v2.saved_model.load('\/kaggle_simulations\/agent\/saved_model')\n\ndef agent(obs):\n    global step\n    global previous_action\n    global observations\n    global state\n    global policy\n    # Get observations for the first (and only one) player we control.\n    obs = obs['players_raw'][0]\n    # Agent we trained uses Super Mini Map (SMM) representation.\n    # See https:\/\/github.com\/google-research\/seed_rl\/blob\/master\/football\/env.py for details.\n    obs = observation_preprocessing.generate_smm([obs])[0]\n    if not observations:\n        observations.extend([obs] * 4)\n    else:\n        observations.append(obs)\n    \n    # SEED packs observations to reduce transfer times.\n    # See PackedBitsObservation in https:\/\/github.com\/google-research\/seed_rl\/blob\/master\/football\/observation.py\n    obs = np.packbits(obs, axis=-1)\n    if obs.shape[-1] % 2 == 1:\n        obs = np.pad(obs, [(0, 0)] * (obs.ndim - 1) + [(0, 1)], 'constant')\n    obs = obs.view(np.uint16)\n\n    # Execute our agent to obtain action to take.\n    agent_output, state = policy.get_action(*prepare_agent_input(obs, previous_action, state))\n    previous_action = agent_output.action[0]\n    return [int(previous_action)]","9c0ce411":"from kaggle_environments import make\nenv = make(\"football\", configuration={\"save_video\": True, \"scenario_name\": \"11_vs_11_kaggle\", \"running_in_notebook\": True})\nenv.run([\"\/kaggle_simulations\/agent\/main.py\", \"run_right\"])\nenv.render(mode=\"human\", width=800, height=600)","2b0799a8":"!cd \/kaggle_simulations\/agent && tar -czvf \/kaggle\/working\/submit.tar.gz main.py saved_model","3b0da799":"# 2. Write train bash file","d6d7644a":"# 1. Install","b3bce6c4":"Now we can run the training for the Kaggle competition scenario (11_vs_11_kaggle). As this example is meant to be interactive, we train for 10000 steps, which doesn't provide a good quality agent, but training should take only a few minutes.\n\n","5a38d3bd":"# 4. Simulate","33c787de":"# 3. Train","e957604d":"SEED RL provides scripts for running training on local machine inside Docker and distributed training at scale using AI Platform. To make it run inside a notebook we need to create a launcher script (based on SEED's docker launcher script):","f1610989":"# 5. Submission","f059226d":"Lets first try to visualize a game played by our trained agent. For that we need to implement a wrapper which loads Tensorflow model and converts observations provided by Kaggle environment to observations accepted by the SEED agent:","0aa03728":"Now we can easily visualize behavior of our agent in action:","4f5b10ce":"### Submit to Competition\n1. \"Save & Run All\" (commit) this Notebook\n1. Go to the notebook viewer\n1. Go to \"Data\" section and find submit.tar.gz file.\n1. Click \"Submit to Competition\"\n1. Go to My Submissions to view your score and episodes being played.","df2a729f":"At the end of the training Tensorflow model is saved for later use.","1c3042c0":"After looking at the baseline code, I thought about what to fix and how to develop it. This notebook tells you which files to focus on, and introduces how to add modified files to the dataset, rather than simply git clone them. Additionally, I removed the unnecessarily long exposed output for visibility.\n\nreference: https:\/\/www.kaggle.com\/piotrstanczyk\/gfootball-train-seed-rl-agent\n\n![](https:\/\/github.com\/seriousran\/img_link\/blob\/master\/kg\/img1.JPG?raw=true)\n\n## This is what you want to modify\n\n1. networks for Reinforcement Learning\n    - `gfootball-baseline\/seed_rl\/football\/networks.py` & `gfootball-baseline\/seed_rl\/agents\/vtrace\/networks.py`\n1. v-tracer\n    - `gfootball-baseline\/seed_rl\/football\/vtrace_main.py` & `gfootball-baseline\/seed_rl\/agents\/vtrace\/learner.py`\n1. train arguments\n    - `!bash train.sh football vtrace 4 '--total_environment_frames=10000 --game=11_vs_11_kaggle --reward_experiment=scoring,checkpoints --logdir=\/kaggle_simulations\/agent\/'`\n        - `AGENT = vtrace` V-trace is an off-policy actor-critic reinforcement learning algorithm\n        - `NUM_ACTORS = 4`\n        - `total_environment_frames = 10000` Original author said it's very small\n        \n## Contents\n1. Install\n2. Write train bash file\n3. Train\n4. Simulate\n5. Submission","56d272b4":"Prepare a submision package containing trained model and the main execution logic."}}