{"cell_type":{"d50bf270":"code","7e2ef44b":"code","ac367d09":"code","32911e6a":"code","9a378ec0":"code","2c439392":"code","67124750":"code","3aa088eb":"code","498ad191":"code","5b17bb00":"code","5ed0646f":"code","b5517b1a":"code","5aba098b":"code","65a30f29":"code","9c4a0532":"code","b0f7abc7":"code","9fbd4b24":"code","94a1c160":"code","9a2e67f5":"code","18cf8781":"code","d151b73c":"code","04ef1eed":"code","5d6e8b6a":"code","fb089857":"code","e11d9a77":"code","88240f7e":"code","62b960d7":"code","ff85ae08":"code","6e122268":"code","100bed4d":"code","d93f60cc":"code","cd97a456":"code","ed7de697":"code","2803af36":"code","33016c47":"code","693f5f2f":"code","5caaef39":"code","703c3389":"code","08ce9353":"code","12caccc4":"code","eb1242af":"code","0a786ebf":"code","c6e7f5fd":"code","b3beb905":"code","83b4cb58":"code","aaf5d645":"code","47c741df":"code","89ec0114":"code","9026f85c":"code","f1b14171":"code","feef9e3f":"code","9a85b12a":"code","b7facf2a":"code","b5e5d941":"code","daacacdb":"code","754c5bc8":"code","67c479d8":"code","3eb0126d":"code","de8a31c4":"code","48b12897":"code","0a0d5c75":"code","4ee423e5":"code","51b24f46":"code","c076b63b":"code","226eacb8":"code","f017f65a":"code","c97e8af7":"code","1274be17":"code","d1969adf":"code","fb098d6a":"code","3eb9cce7":"code","498415bc":"code","b0af218d":"code","d706875b":"code","d9e947eb":"code","2e0b3459":"code","5f84f4d3":"code","7ae57530":"code","c36ec908":"code","b437b145":"code","c7d59ff3":"code","3093c830":"code","857ccd44":"code","c318e242":"code","3a115c3e":"code","d5584b8a":"code","feb4e02b":"code","64ff77af":"code","8b54fa29":"code","5bbe5413":"code","f7509a2a":"code","82cf92d7":"code","f6b13e08":"code","db4a50ed":"code","1d347945":"code","8048714b":"code","2c71f345":"code","7700e09f":"code","cc71766e":"code","e9b9bc0a":"code","4f1dc096":"code","6a3d374b":"code","011e3c4b":"code","a739ea8e":"code","001c6a7c":"code","984953eb":"code","358b123c":"code","bccabc80":"code","01647eb4":"code","83c3697b":"markdown","314a548e":"markdown","1246ee32":"markdown","ec392423":"markdown","11a3bf51":"markdown","011a24ff":"markdown","7d3fe417":"markdown","213385bf":"markdown","81d8f746":"markdown","bd56678b":"markdown","837c188f":"markdown","980994cb":"markdown","3c0140bd":"markdown","e4a80dcd":"markdown","ade54ea3":"markdown","b5f7a8dc":"markdown","2a9a428e":"markdown","c4ddd427":"markdown","6954c2bf":"markdown","4ffa35ff":"markdown","66dcce53":"markdown","66fc882c":"markdown","9de51dff":"markdown","7b374092":"markdown","10cd6f9f":"markdown","48f93ccf":"markdown","5d4fee79":"markdown","4da13518":"markdown","9325518f":"markdown","0c5148d1":"markdown","4169a925":"markdown","4ed1a503":"markdown"},"source":{"d50bf270":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","7e2ef44b":"import time\nimport lightgbm as lgb\nimport xgboost as xgb\nimport seaborn as sns\n\nfrom fastai.imports import *\nfrom fastai.structured import *\nfrom fbprophet import Prophet\n\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\n\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\nfrom sklearn.model_selection import KFold\nfrom scipy import stats\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import graph_objs as go\n\nimport statsmodels.api as sm\n# Initialize plotly\ninit_notebook_mode(connected=True)\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\npd.option_context(\"display.max_rows\", 1000);\npd.option_context(\"display.max_columns\", 1000);","ac367d09":"os.getcwd()","32911e6a":"PATH = '..\/input\/'","9a378ec0":"print(os.listdir(PATH))","2c439392":"df_raw = pd.read_csv(f'{PATH}\/train.csv', low_memory=False, parse_dates=['date'], index_col=['date'])\ndf_test = pd.read_csv(f'{PATH}\/test.csv', low_memory=False, parse_dates=['date'], index_col=['date'])\nsubs = pd.read_csv(f'{PATH}\/sample_submission.csv')","67124750":"df_raw.head()","3aa088eb":"print(\"Train and Test shape are {} and {} respectively\".format(df_raw.shape, df_test.shape))","498ad191":"df_test.head()","5b17bb00":"#### Seasonality Check\n# preparation: input should be float type\ndf_raw['sales'] = df_raw['sales'] * 1.0\n\n# store types\nsales_a = df_raw[df_raw.store == 2]['sales'].sort_index(ascending = True)\nsales_b = df_raw[df_raw.store == 3]['sales'].sort_index(ascending = True) # solve the reverse order\nsales_c = df_raw[df_raw.store == 1]['sales'].sort_index(ascending = True)\nsales_d = df_raw[df_raw.store == 4]['sales'].sort_index(ascending = True)\n\nf, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize = (12, 13))\nc = '#386B7F'\n\n# store types\nsales_a.resample('W').sum().plot(color = c, ax = ax1)\nsales_b.resample('W').sum().plot(color = c, ax = ax2)\nsales_c.resample('W').sum().plot(color = c, ax = ax3)\nsales_d.resample('W').sum().plot(color = c, ax = ax4)\n\n#All Stores have same trend... Weird Seems like the dataset is A Synthetic One..;","5ed0646f":"f, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize = (12, 13))\n\n# Yearly\ndecomposition_a = sm.tsa.seasonal_decompose(sales_a, model = 'additive', freq = 365)\ndecomposition_a.trend.plot(color = c, ax = ax1)\n\ndecomposition_b = sm.tsa.seasonal_decompose(sales_b, model = 'additive', freq = 365)\ndecomposition_b.trend.plot(color = c, ax = ax2)\n\ndecomposition_c = sm.tsa.seasonal_decompose(sales_c, model = 'additive', freq = 365)\ndecomposition_c.trend.plot(color = c, ax = ax3)\n\ndecomposition_d = sm.tsa.seasonal_decompose(sales_d, model = 'additive', freq = 365)\ndecomposition_d.trend.plot(color = c, ax = ax4)","b5517b1a":"date_sales = df_raw.drop(['store','item'], axis=1).copy() #it's a temporary DataFrame.. Original is Still intact..","5aba098b":"date_sales.get_ftype_counts()","65a30f29":"y = date_sales['sales'].resample('MS').mean() \ny['2017':] #sneak peak","9c4a0532":"y.plot(figsize=(15, 6),);\n#The time-series has seasonality pattern, such as sales are always low at the beginning of the year and high at the middle(festive season maybe) of the year\n# and again low at the end of the year...\n#There is always an upward trend within any single year with a couple of low months in the mid of the year...","b0f7abc7":"#We can also visualize our data using a method called time-series decomposition that allows us to decompose our time series into three distinct components: \n#trend, seasonality, and noise.\ndecomposition = sm.tsa.seasonal_decompose(y, model='additive')\ndecomposition.plot();\n#The plot clearly shows that the sales is unstable, along with its obvious seasonality.;","9fbd4b24":"#We can also visualize our data using a method called time-series decomposition that allows us to decompose our time series into three distinct components: \n#trend, seasonality, and noise.\ndecomposition = sm.tsa.seasonal_decompose(y, model='multiplicative')\ndecomposition.plot();\n#The plot above clearly shows that the sales is unstable, along with its obvious seasonality.;","94a1c160":"def moving_average(series, n):\n    \"\"\"\n        Calculate average of last n observations\n    \"\"\"\n    return np.average(series[-n:])\n\nmoving_average(date_sales, 24) # prediction for the last observed day (past 24 hours)","9a2e67f5":"def plotMovingAverage(series, window, plot_intervals=False, scale=2, plot_anomalies=False):\n\n    \"\"\"\n        series - dataframe with timeseries\n        window - rolling window size \n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n\n    \"\"\"\n    rolling_mean = series.rolling(window=window).mean()\n\n    plt.figure(figsize=(15,5))\n    plt.title(\"Moving average\\n window size = {}\".format(window))\n    plt.plot(rolling_mean, color='Black', label=\"Rolling mean trend\", alpha=0.5)\n\n    # Plot confidence intervals for smoothed values\n    if plot_intervals:\n        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n        deviation = np.std(series[window:] - rolling_mean[window:])\n        lower_bond = rolling_mean - (mae + scale * deviation)\n        upper_bond = rolling_mean + (mae + scale * deviation)\n        plt.plot(upper_bond, color='Black', label=\"Upper Bond \/ Lower Bond\", alpha=.3)\n        plt.plot(lower_bond, color='Black', alpha=.3)\n        \n        # Having the intervals, find abnormal values\n        if plot_anomalies:\n            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n            anomalies[series<lower_bond] = series[series<lower_bond]\n            anomalies[series>upper_bond] = series[series>upper_bond]\n            plt.plot(anomalies, \"ro\", markersize=10)\n        \n    plt.plot(series[window:],color='Red', label=\"Actual values\", alpha=.3)\n    plt.legend(loc=\"upper left\")\n    plt.grid(True)","18cf8781":"def exponential_smoothing(series, alpha):\n    \"\"\"\n        series - dataset with timestamps\n        alpha - float [0.0, 1.0], smoothing parameter\n    \"\"\"\n    result = [series[0]] # first value is same as series\n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n    return result","d151b73c":"def plotExponentialSmoothing(series, alphas):\n    \"\"\"\n        Plots exponential smoothing with different alphas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters\n        \n    \"\"\"\n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(15, 7))\n        for alpha in alphas:\n            plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n        plt.plot(series.values, \"c\", label = \"Actual\", alpha = 0.4)\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Exponential Smoothing\")\n        plt.grid(True);","04ef1eed":"plotExponentialSmoothing(date_sales.sales[:30000], [0.3, 0.05])","5d6e8b6a":"def double_exponential_smoothing(series, alpha, beta):\n    \"\"\"\n        series - dataset with timeseries\n        alpha - float [0.0, 1.0], smoothing parameter for level\n        beta - float [0.0, 1.0], smoothing parameter for trend\n    \"\"\"\n    # first value is same as series\n    result = [series[0]]\n    for n in range(1, len(series)+1):\n        if n == 1:\n            level, trend = series[0], series[1] - series[0]\n        if n >= len(series): # forecasting\n            value = result[-1]\n        else:\n            value = series[n]\n        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n        trend = beta*(level-last_level) + (1-beta)*trend\n        result.append(level+trend)\n    return result\n\ndef plotDoubleExponentialSmoothing(series, alphas, betas):\n    \"\"\"\n        Plots double exponential smoothing with different alphas and betas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters for level\n        betas - list of floats, smoothing parameters for trend\n    \"\"\"\n    \n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(20, 8))\n        for alpha in alphas:\n            for beta in betas:\n                plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n        plt.plot(series.values, label = \"Actual\", alpha = 0.1)\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Double Exponential Smoothing\")\n        plt.grid(True)","fb089857":"plotDoubleExponentialSmoothing(date_sales.sales[:30000], alphas=[0.9, 0.02], betas=[0.9, 0.02])","e11d9a77":"ts_diff = date_sales - date_sales.shift(7)\nplt.figure(figsize=(22,10))\nplt.plot(ts_diff[:20000])\nplt.title(\"Differencing method\") \nplt.xlabel(\"Date\")\nplt.ylabel(\"Differencing Sales\");","88240f7e":"df_raw = df_raw.reset_index()\ndf_test = df_test.reset_index()","62b960d7":"import re\ndef add_datepart(df, fldname, drop=True):\n\n    \"\"\"\n    Parameters:\n    -----------\n    df: A pandas data frame. df gain several new columns.\n    fldname: A string that is the name of the date column you wish to expand.\n        If it is not a datetime64 series, it will be converted to one with pd.to_datetime.\n    drop: If true then the original date column will be removed.\n    \"\"\"\n    \n    fld = df[fldname]\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n        \n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    \n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear','weekofyear',\n            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    \n    for n in attr: \n        df[targ_pre + n] = getattr(fld.dt, n.lower())\n        \n    if drop: \n        df.drop(fldname, axis=1, inplace=True)\n\nadd_datepart(df_raw,'date',False)\nadd_datepart(df_test,'date',False)","ff85ae08":"pivoted = pd.pivot_table(df_raw, values='sales', columns='Year', index='Month')\npivoted.plot(figsize=(12,12));","6e122268":"pivoted = pd.pivot_table(df_raw, values='sales' , columns='Year', index='Week')\npivoted.plot(figsize=(12,12));","100bed4d":"pivoted = pd.pivot_table(df_raw, values='sales' , columns='Month', index='Day')\npivoted.plot(figsize=(12,12));","d93f60cc":"temp_1 = df_raw.groupby(['Year','Month','item'])['sales'].mean().reset_index()\nplt.figure(figsize=(12,8))\nsns.swarmplot('item', 'sales', data=temp_1, hue = 'Month');\n# Place legend to the right\nplt.legend(bbox_to_anchor=(1, 1), loc=2);","cd97a456":"#In case the above plot is clutterd(which it is), try this, (Will create a grid for Year vs Month)\n#sns.factorplot('item', 'sales', data=temp_1, hue = 'Month', col='Year',row='Month', kind='swarm', size = 5);","ed7de697":"temp_1 = df_raw.groupby(['Year','Month'])['sales'].mean().reset_index()\nplt.figure(figsize=(12,8));\nsns.lmplot('Month','sales',data = temp_1, hue='Year', fit_reg= False);","2803af36":"temp_1 = df_raw.groupby(['Year'])['sales'].mean().reset_index()\nplt.figure(figsize=(12,8));\nsns.factorplot('Year','sales',data = temp_1, hue='Year', kind='point');","33016c47":"def inverse_boxcox(y, lambda_):\n    return np.exp(y) if lambda_ == 0 else np.exp(np.log(lambda_ * y + 1) \/ lambda_)","693f5f2f":"original_target = df_raw.sales.values\ntarget, lambda_prophet = stats.boxcox(df_raw['sales'] + 1)\nlen_train=target.shape[0]\nmerged_df = pd.concat([df_raw, df_test])","5caaef39":"%%time\nmerged_df[\"median-store_item\"] = merged_df.groupby([\"item\", \"store\"])[\"sales\"].transform(\"median\")\nmerged_df[\"mean-store_item\"] = merged_df.groupby([\"item\", \"store\"])[\"sales\"].transform(\"mean\")\nmerged_df[\"mean-Month_item\"] = merged_df.groupby([\"Month\", \"item\"])[\"sales\"].transform(\"mean\")\nmerged_df[\"median-Month_item\"] = merged_df.groupby([\"Month\", \"item\"])[\"sales\"].transform(\"median\")\nmerged_df[\"median-Month_store\"] = merged_df.groupby([\"Month\", \"store\"])[\"sales\"].transform(\"median\")\nmerged_df[\"median-item\"] = merged_df.groupby([\"item\"])[\"sales\"].transform(\"median\")\nmerged_df[\"median-store\"] = merged_df.groupby([\"store\"])[\"sales\"].transform(\"median\")\nmerged_df[\"mean-item\"] = merged_df.groupby([\"item\"])[\"sales\"].transform(\"mean\")\nmerged_df[\"mean-store\"] = merged_df.groupby([\"store\"])[\"sales\"].transform(\"mean\")\n\nmerged_df[\"median-store_item-Month\"] = merged_df.groupby(['Month', \"item\", \"store\"])[\"sales\"].transform(\"median\")\nmerged_df[\"mean-store_item-week\"] = merged_df.groupby([\"item\", \"store\",'weekofyear'])[\"sales\"].transform(\"mean\")\nmerged_df[\"item-Month-mean\"] = merged_df.groupby(['Month', \"item\"])[\"sales\"].transform(\"mean\")# mean sales of that item  for all stores scaled\nmerged_df[\"store-Month-mean\"] = merged_df.groupby(['Month', \"store\"])[\"sales\"].transform(\"mean\")# mean sales of that store  for all items scaled\n\n# adding more lags (Check the rationale behind this in the links attached)\nlags = [90,91,98,105,112,119,126,182,189,364]\nfor i in lags:\n#     print(\"Done For Lag {}\".format(i))\n    merged_df['_'.join(['item-week_shifted-', str(i)])] = merged_df.groupby(['weekofyear',\"item\"])[\"sales\"].transform(lambda x:x.shift(i).sum()) \n    merged_df['_'.join(['item-week_shifted-', str(i)])] = merged_df.groupby(['weekofyear',\"item\"])[\"sales\"].transform(lambda x:x.shift(i).mean()) \n    merged_df['_'.join(['item-week_shifted-', str(i)])].fillna(merged_df['_'.join(['item-week_shifted-', str(i)])].mode()[0], inplace=True)\n    ##### sales for that item i days in the past\n    merged_df['_'.join(['store-week_shifted-', str(i)])] = merged_df.groupby(['weekofyear',\"store\"])[\"sales\"].transform(lambda x:x.shift(i).sum())\n    merged_df['_'.join(['store-week_shifted-', str(i)])] = merged_df.groupby(['weekofyear',\"store\"])[\"sales\"].transform(lambda x:x.shift(i).mean()) \n    merged_df['_'.join(['store-week_shifted-', str(i)])].fillna(merged_df['_'.join(['store-week_shifted-', str(i)])].mode()[0], inplace=True)","703c3389":"df_raw.drop('sales', axis=1, inplace=True)\nmerged_df.drop(['id','date','sales'], axis=1, inplace=True)","08ce9353":"merged_df.head(1)","12caccc4":"# comes from the public kernel\nmerged_df = merged_df * 1\nparams = {\n    'nthread': 4,\n    'categorical_feature' : [0,1,9,10,12,13,14], # Day, DayOfWeek, Month, Week, Item, Store, WeekOfYear\n    'max_depth': 8,\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression_l1',\n    'metric': 'mape', # this is abs(a-e)\/max(1,a)\n    'num_leaves': 127,\n    'learning_rate': 0.25,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 30,\n    'lambda_l1': 0.06,\n    'lambda_l2': 0.1,\n    'verbose': -1\n}","eb1242af":"# do the training\nnum_folds = 3\ntest_x = merged_df[len_train:].values\nall_x = merged_df[:len_train].values\nall_y = target # removing what we did earlier\n\noof_preds = np.zeros([all_y.shape[0]])\nsub_preds = np.zeros([test_x.shape[0]])\n\nfeature_importance_df = pd.DataFrame()\nfolds = KFold(n_splits=num_folds, shuffle=True, random_state=345665)\n\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(all_x)):\n    \n    train_x, train_y = all_x[train_idx], all_y[train_idx]\n    valid_x, valid_y = all_x[valid_idx], all_y[valid_idx]\n    lgb_train = lgb.Dataset(train_x,train_y)\n    lgb_valid = lgb.Dataset(valid_x,valid_y)\n        \n    # train\n    gbm = lgb.train(params, lgb_train, 1000, \n        valid_sets=[lgb_train, lgb_valid],\n        early_stopping_rounds=100, verbose_eval=100)\n    \n    oof_preds[valid_idx] = gbm.predict(valid_x, num_iteration=gbm.best_iteration)\n    sub_preds[:] += gbm.predict(test_x, num_iteration=gbm.best_iteration) \/ folds.n_splits\n    valid_idx += 1\n    importance_df = pd.DataFrame()\n    importance_df['feature'] = merged_df.columns\n    importance_df['importance'] = gbm.feature_importance()\n    importance_df['fold'] = n_fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, importance_df], axis=0)\n    \ne = 2 * abs(all_y - oof_preds) \/ ( abs(all_y)+abs(oof_preds) )\ne = e.mean()\nprint('Full validation score With Box Cox %.4f' %e)\nprint('Inverting Box Cox Transformation')\nprint('Done!!')\n\nsub_preds = inverse_boxcox(sub_preds , lambda_prophet) - 1\noof_preds = inverse_boxcox(oof_preds , lambda_prophet) - 1\ne = 2 * abs(all_y - oof_preds) \/ ( abs(all_y)+abs(oof_preds) )\ne = e.mean()\nprint('Full validation score Re-Box Cox Transformation is %.4f' %e)\n#Don't Forget to apply inverse box-cox","0a786ebf":"feature_importance_df.head()","c6e7f5fd":"importance_df.sort_values(['importance'], ascending=False, inplace=True);","b3beb905":"def plot_fi(fi): \n    return fi.plot('feature', 'importance', 'barh', figsize=(12,12), legend=False)","83b4cb58":"plot_fi(importance_df[:]);","aaf5d645":"merged_df.get_ftype_counts()","47c741df":"# OHE FOR 0,1,9,10,12,13,14  Cols - Day, Dayofweek, Month, Week, item, store, weekofyear\nprint(\"Before OHE\", merged_df.shape)\nmerged_df = pd.get_dummies(merged_df, columns=['Day', 'Dayofweek', 'Month', 'Week', 'item', 'store', 'weekofyear'])\nprint(\"After OHE\", merged_df.shape)\ntest_x = merged_df[len_train:].values\nall_x = merged_df[:len_train].values\nall_y = target;","89ec0114":"def XGB_regressor(train_X, train_y, test_X, test_y= None, feature_names=None, seed_val=2018, num_rounds=500):\n\n    param = {}\n    param['objective'] = 'reg:linear'\n    param['eta'] = 0.1\n    param['max_depth'] = 5\n    param['silent'] = 1\n    param['eval_metric'] = 'mae'\n    param['min_child_weight'] = 4\n    param['subsample'] = 0.8\n    param['colsample_bytree'] = 0.8\n    param['seed'] = seed_val\n    num_rounds = num_rounds\n\n    plst = list(param.items())\n\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n    if test_y is not None:\n        xgtest = xgb.DMatrix(test_X, label=test_y)\n        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n    else:\n        xgtest = xgb.DMatrix(test_X)\n        model = xgb.train(plst, xgtrain, num_rounds)\n        \n    return model    ","9026f85c":"model = XGB_regressor(train_X = all_x, train_y = all_y, test_X = test_x)\ny_test = model.predict(xgb.DMatrix(test_x), ntree_limit = model.best_ntree_limit)","f1b14171":"print('Inverting Box Cox Transformation')\ny_test = inverse_boxcox(y_test, lambda_prophet) - 1","feef9e3f":"df = date_sales.reset_index()\ndf.columns = ['ds', 'y']","9a85b12a":"df.head()","b7facf2a":"df['store'] = df_raw['store'].copy()\ndf['Week'] = df_raw['Week'].copy()\ndf['item'] = df_raw['item'].copy()","b5e5d941":"df = df.query('item == 1 & store == 1')","daacacdb":"df.groupby(['Week','store','item'])['y'].mean().reset_index().head(10)","754c5bc8":"prediction_size = 31 \ntrain_df = df[:-prediction_size]\ntrain_df.tail(n=3)","67c479d8":"%%time\nm = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)\nm.fit(train_df[['ds','y']]);","3eb0126d":"future = m.make_future_dataframe(periods=prediction_size)\nfuture.tail(n=3)","de8a31c4":"%%time\nforecast = m.predict(future)\nforecast.tail(n=3)","48b12897":"m.plot(forecast)\nm.plot_components(forecast)","0a0d5c75":"#Such a bad baseline forecasting on train data.Isn't it!! ","4ee423e5":"playoffs = pd.DataFrame({\n  'holiday' : 'playoff',\n  'ds' : pd.to_datetime(['2013-01-12','2013-07-12','2013-12-24','2014-01-12', '2014-07-12', '2014-07-19',\n                 '2014-07-02','2014-12-24', '2015-07-11','2015-12-24', '2016-07-17',\n                 '2016-07-24', '2016-07-07','2016-07-24','2016-12-24','2017-07-17','2017-07-24','2017-07-07','2017-12-24']),\n  'lower_window' : 0,\n  'upper_window' : 2}\n)\nsuperbowls = pd.DataFrame({\n  'holiday': 'superbowl',\n  'ds': pd.to_datetime(['2013-01-01','2013-01-21','2013-02-14','2013-02-18',\n'2013-05-27','2013-07-04','2013-09-02','2013-10-14','2013-11-11','2013-11-28','2013-12-25','2014-01-01','2014-01-20','2014-02-14','2014-02-17',\n'2014-05-26','2014-07-04','2014-09-01','2014-10-13','2014-11-11','2014-11-27','2014-12-25','2015-01-01','2015-01-19','2015-02-14','2015-02-16',\n'2015-05-25','2015-07-03','2015-09-07','2015-10-12','2015-11-11','2015-11-26','2015-12-25','2016-01-01','2016-01-18','2016-02-14','2016-02-15',\n'2016-05-30','2016-07-04','2016-09-05','2016-10-10','2016-11-11','2016-11-24','2016-12-25','2017-01-02','2017-01-16','2017-02-14','2017-02-20',\n'2017-05-29','2017-07-04','2017-09-04','2017-10-09','2017-11-10','2017-11-23','2017-12-25','2018-01-01','2018-01-15','2018-02-14','2018-02-19'\n                       ]),\n  'lower_window': 0,\n  'upper_window': 3,\n})\n\nholidays = pd.concat((playoffs, superbowls))","51b24f46":"%%time\nm_holi = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True, holidays=holidays)\nm_holi.fit(train_df[['ds','y']]);","c076b63b":"future_holi = m_holi.make_future_dataframe(periods=prediction_size)\nfuture_holi.tail(n=3)","226eacb8":"%%time\nforecast_holi = m_holi.predict(future_holi)\nforecast_holi.tail(n=3)","f017f65a":"#from the docs..\nforecast_holi[(forecast_holi['playoff'] + forecast_holi['superbowl']).abs() > 0][\n        ['ds', 'playoff', 'superbowl']][-10:]","c97e8af7":"m_holi.plot(forecast_holi)\nm_holi.plot_components(forecast_holi)","1274be17":"#remember that we will evaluate the forcast later..","d1969adf":"print(', '.join(forecast.columns))","fb098d6a":"def make_comparison_dataframe(historical, forecast):\n    \"\"\"Join the history with the forecast\n       The resulting dataset will contain columns 'yhat', 'yhat_lower', 'yhat_upper' and 'y'.\n    \"\"\"\n    return forecast.set_index('ds')[['yhat', 'yhat_lower', 'yhat_upper']].join(historical.set_index('ds'))","3eb9cce7":"cmp_df = make_comparison_dataframe(df, forecast)\ncmp_df.tail(3)","498415bc":"cmp_df_holi = make_comparison_dataframe(df, forecast_holi)\ncmp_df_holi.tail(3)","b0af218d":"def calculate_forecast_errors(df, prediction_size):\n    \"\"\"Calculate MAPE and MAE of the forecast.\n    \n       Args:\n           df: joined dataset with 'y' and 'yhat' columns.\n           prediction_size: number of days at the end to predict.\n    \"\"\"\n    \n    # Make a copy\n    df = df.copy()\n    \n    # Now we calculate the values of e_i and p_i according to the formulas given in the article above.\n    df['e'] = df['y'] - df['yhat']\n    df['p'] = 100 * df['e'] \/ df['y']\n    \n    # Recall that we held out the values of the last `prediction_size` days\n    # in order to predict them and measure the quality of the model. \n    \n    # Now cut out the part of the data which we made our prediction for.\n    predicted_part = df[-prediction_size:]\n    \n    # Define the function that averages absolute error values over the predicted part.\n    error_mean = lambda error_name: np.mean(np.abs(predicted_part[error_name]))\n    \n    # Now we can calculate MAPE and MAE and return the resulting dictionary of errors.\n    return {'MAPE': error_mean('p'), 'MAE': error_mean('e')}","d706875b":"for err_name, err_value in calculate_forecast_errors(cmp_df, prediction_size).items():\n    print('Non Holiday', err_name, err_value)","d9e947eb":"for err_name, err_value in calculate_forecast_errors(cmp_df_holi, prediction_size).items():\n    print('Including Holiday', err_name, err_value)","2e0b3459":"from plotly.offline import init_notebook_mode, iplot\nfrom plotly import graph_objs as go\n\n# Initialize plotly\ninit_notebook_mode(connected=True)\n\ndef show_forecast(cmp_df, num_predictions, num_values, title):\n    \"\"\"Visualize the forecast.\"\"\"\n    \n    def create_go(name, column, num, **kwargs):\n        points = cmp_df.tail(num)\n        args = dict(name=name, x=points.index, y=points[column], mode='lines')\n        args.update(kwargs)\n        return go.Scatter(**args)\n    \n    lower_bound = create_go('Lower Bound', 'yhat_lower', num_predictions,\n                            line=dict(width=0),\n                            marker=dict(color=\"aqua\"))\n    upper_bound = create_go('Upper Bound', 'yhat_upper', num_predictions,\n                            line=dict(width=0),\n                            marker=dict(color=\"aqua\"),\n                            fillcolor='rgba(68, 68, 68, 0.3)', \n                            fill='tonexty')\n    forecast = create_go('Forecast', 'yhat', num_predictions,\n                         line=dict(color='rgb(31, 119, 180)'))\n    actual = create_go('Actual', 'y', num_values,\n                       marker=dict(color=\"red\"))\n    \n    # In this case the order of the series is important because of the filling\n    data = [lower_bound, upper_bound, forecast, actual]\n\n    layout = go.Layout(yaxis=dict(title='sales'), title=title, showlegend = False)\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig, show_link=False)\n\nshow_forecast(cmp_df, prediction_size, 100, 'Sales on Store $1$ for Item $1$')","5f84f4d3":"show_forecast(cmp_df_holi, prediction_size, 100, 'Sales on Store $1$ for Item $1$ Holidays Version')","7ae57530":"#train_df2 = df.copy().set_index('ds')\ntrain_df2 = df[:-prediction_size]","c36ec908":"train_df2['y'], lambda_prophet = stats.boxcox(train_df2['y'])\ntrain_df2.reset_index(inplace=True)","b437b145":"%%time\nm2 = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)\nm2.fit(train_df2[['ds','y']]);","c7d59ff3":"future2 = m2.make_future_dataframe(periods=prediction_size)\nforecast2 = m2.predict(future2)","3093c830":"for column in ['yhat', 'yhat_lower', 'yhat_upper']:\n    forecast2[column] = inverse_boxcox(forecast2[column], lambda_prophet)","857ccd44":"cmp_df2 = make_comparison_dataframe(df, forecast2)\ncmp_df2.tail()","c318e242":"cmp_df2 = make_comparison_dataframe(df, forecast2)\nfor err_name, err_value in calculate_forecast_errors(cmp_df2, prediction_size).items():\n    print('Including Holidays', err_name, err_value)\n# We Get Slight Improvement Over No Transformations !!!","3a115c3e":"show_forecast(cmp_df, prediction_size, 100, 'No transformations')\nshow_forecast(cmp_df2, prediction_size, 100, 'With Box\u2013Cox transformation')","d5584b8a":"import itertools\np = d = q = range(0, 3)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))","feb4e02b":"# figure for subplots\nplt.figure(figsize = (12, 8))\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# acf and pacf for A\nplt.subplot(421); plot_acf(sales_a, lags = 50, ax = plt.gca(), color = c)\nplt.subplot(422); plot_pacf(sales_a, lags = 50, ax = plt.gca(), color = c)\n\n# acf and pacf for B\nplt.subplot(423); plot_acf(sales_b, lags = 50, ax = plt.gca(), color = c)\nplt.subplot(424); plot_pacf(sales_b, lags = 50, ax = plt.gca(), color = c)\n\n# acf and pacf for C\nplt.subplot(425); plot_acf(sales_c, lags = 50, ax = plt.gca(), color = c)\nplt.subplot(426); plot_pacf(sales_c, lags = 50, ax = plt.gca(), color = c)\n\n# acf and pacf for D\nplt.subplot(427); plot_acf(sales_d, lags = 50, ax = plt.gca(), color = c)\nplt.subplot(428); plot_pacf(sales_d, lags = 50, ax = plt.gca(), color = c)\n#these plots are showing the correlation of the series with itself, lagged by x time units correlation of the series with itself, lagged by x time units.","64ff77af":"cnt = 0\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(y,\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n            results = mod.fit()\n            cnt += 1\n            if cnt % 50 :\n                print('Current Iter - {}, ARIMA{}x{} 12 - AIC:{}'.format(cnt, param, param_seasonal, results.aic))\n        except:\n            continue","8b54fa29":"mod = sm.tsa.statespace.SARIMAX(y,\n                                order=(2, 0, 1),\n                                seasonal_order=(2, 2, 0, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\nresults = mod.fit()\nprint(results.summary().tables[1])","5bbe5413":"## Validating Forecast\npred = results.get_prediction(start=pd.to_datetime('2017-01-01'), dynamic=False)\npred_ci = pred.conf_int()\nax = y['2014':].plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nplt.legend()","f7509a2a":"y_forecasted = pred.predicted_mean\ny_truth = y['2017-01-01':]\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))\n#The MSE is a measure of the quality of an estimator\u200a\u2014\u200ait is always non-negative, \n#and the smaller the MSE, the closer we are to finding the line of best fit.","82cf92d7":"pred_uc = results.get_forecast(steps=100)\npred_ci = pred_uc.conf_int()\nax = y.plot(label='observed', figsize=(14, 7))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nplt.legend()","f6b13e08":"subs.head()","db4a50ed":"out_df_lgb = pd.DataFrame({'id': subs.id.astype(np.int32), 'sales': sub_preds.astype(np.int32)})\nout_df_xgb = pd.DataFrame({'id': subs.id.astype(np.int32), 'sales': y_test.astype(np.int32)})\nout_df_combined = pd.DataFrame({'id': subs.id.astype(np.int32), 'sales': ((sub_preds + y_test + 4)\/2.).astype(np.int32)})\nout_df_combined_25_75 = pd.DataFrame({'id': subs.id.astype(np.int32), 'sales': ((sub_preds *.25 + y_test *.75 + 4)).astype(np.int32)})\n\nout_df_lgb.to_csv('submission_lgbm.csv', index=False)\nout_df_xgb.to_csv('submission_xgb.csv', index=False)\nout_df_combined.to_csv('submission_combined.csv', index=False)\nout_df_combined_25_75.to_csv('submission_combined_25_75.csv', index=False)","1d347945":"out_df_combined.head(10)","8048714b":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Reshape, merge, Embedding, Input, Concatenate\nfrom keras.models import Model as KerasModel\nimport keras.backend as K","2c71f345":"cat_cols = ['Day', 'Dayofweek', 'Month', 'Week', 'item', 'store', 'weekofyear','Year']\nfor cols in cat_cols:\n    df_raw[cols] = df_raw[cols].astype('category')\n    df_test[cols] = df_test[cols].astype('category')","7700e09f":"df_raw_cats = df_raw[cat_cols].copy()\ndf_test_cats = df_test[cat_cols].copy()","cc71766e":"cat_cols","e9b9bc0a":"df_raw_cats.head()","4f1dc096":"def split_features(X):\n    \n    X_list = []\n    \n    day = X[..., [0]]\n    X_list.append(day)\n\n    day_of_week = X[..., [1]]\n    X_list.append(day_of_week)\n\n    month = X[..., [2]]\n    X_list.append(month)\n\n    week_of_year = X[..., [6]]\n    X_list.append(week_of_year)\n    \n    item = X[..., [4]]\n    X_list.append(item)\n    \n    store = X[..., [5]]\n    X_list.append(store)\n    \n    year = X[..., [7]]\n    X_list.append(year)\n\n    return X_list","6a3d374b":"def custom_smape(x, x_): # From the Public Kernel https:\/\/www.kaggle.com\/rezas26\/simple-keras-starter        \n    return K.mean(2*K.abs(x-x_)\/(K.abs(x)+K.abs(x_)))\n    \nclass NN_with_EntityEmbedding():\n\n    def __init__(self, X_train, y_train):\n        \n        super().__init__()\n        \n        self.epochs = 3\n        self.__build_keras_model()\n        self.fit(X_train, y_train)\n    \n    def preprocessing(self, X):\n        \n        X_list = split_features(X)\n        return X_list\n\n    def __build_keras_model(self):\n        \n        model_day = Input(shape=(1,))\n        output_day = Embedding(31, 16, name='day_embedding')(model_day)\n        output_day = Reshape(target_shape=(16,))(output_day)\n\n        model_dow = Input(shape=(1,))\n        output_dow = Embedding(7, 5, name='dow_embedding')(model_dow)\n        output_dow = Reshape(target_shape=(5,))(output_dow)\n        \n        input_month = Input(shape=(1,))\n        output_month = Embedding(12, 6, name='month_embedding')(input_month)\n        output_month = Reshape(target_shape=(6,))(output_month)\n        \n        model_woy = Input(shape=(1,))\n        output_woy = Embedding(52, 26, name='week_embedding')(model_woy)\n        output_woy = Reshape(target_shape=(26,))(output_woy)\n        \n        model_item = Input(shape=(1,))\n        output_item = Embedding(50, 26, name='item_embedding')(model_item)\n        output_item = Reshape(target_shape=(26,))(output_item)\n        \n        model_store = Input(shape=(1,))\n        output_store = Embedding(10, 6, name='store_embedding')(model_store)\n        output_store = Reshape(target_shape=(6,))(output_store)\n        \n        model_year = Input(shape=(1,))\n        output_year = Embedding(5, 3, name='year_embedding')(model_year)\n        output_year = Reshape(target_shape=(3,))(output_year)\n\n        input_model = [model_day, model_dow, input_month,\n                       model_woy, model_item, model_store, model_year]\n\n        output_embeddings = [output_day, output_dow, output_month,\n                             output_woy, output_item, output_store, output_year]\n\n        output_model = Concatenate()(output_embeddings)\n        output_model = Dense(128, kernel_initializer=\"glorot_uniform\")(output_model)\n        output_model = Activation('relu')(output_model)\n        output_model = Dense(32, kernel_initializer=\"glorot_uniform\")(output_model)\n        output_model = Activation('relu')(output_model)\n        output_model = Dense(1)(output_model)\n\n        self.model = KerasModel(inputs=input_model, outputs=output_model)\n        self.model.compile(loss= custom_smape, optimizer='sgd')\n\n\n    def fit(self, X_train, y_train):\n        self.model.fit(self.preprocessing(X_train), y_train, epochs=self.epochs, batch_size=64)\n\n    def guess(self, features):\n        features = self.preprocessing(features)\n        result = self.model.predict(features).flatten()\n        return result","011e3c4b":"nn = NN_with_EntityEmbedding(df_raw_cats.values, original_target)","a739ea8e":"nn.model.summary()","001c6a7c":"emd_layers = []\nfor idx,layer in enumerate(nn.model.layers):\n    if 'embedding' in str(layer):\n        emd_layers.append(idx)\n        print(idx,layer)","984953eb":"emd_layers","358b123c":"embd_weights = []\nfor i in emd_layers:\n    embd_weights.append(nn.model.layers[i].get_weights()[0])\n    #print(nn.model.layers[i].get_weights()[0])\n    #print('#'*60, i)","bccabc80":"nn_preds = nn.guess(df_test_cats.values)\nmin(nn_preds), max(nn_preds)","01647eb4":"out_df_nn = pd.DataFrame({'id': subs.id.astype(np.int32), 'sales': nn_preds.astype(np.int32)})\nout_df_nn.to_csv('submission_nn.csv', index=False)\n\nout_df_combined = pd.DataFrame({'id': subs.id.astype(np.int32), 'sales': ((sub_preds + nn_preds + 4)\/2.).astype(np.int32)})\nout_df_combined.to_csv('submission_combined_nn_lgbm.csv', index=False)","83c3697b":"We can see that this dataframe contains all the information we need except for the historical values. We need to join the forecast object with the actual values y from the original dataset","314a548e":"Now we need to create a new Prophet object. Here we can pass the parameters of the model into the constructor. But currently we will use the defaults as it is.. Then we train our model by invoking its fit method on our training dataset:","1246ee32":"As a result, the relative error of our forecast (MAPE) is about 27.5%, and on average our model is wrong by 3.54 predicts (MAE).","ec392423":"## Basic ARIMA","11a3bf51":"In the resulting dataframe you can see many columns characterizing the prediction, including trend and seasonality components as well as their confidence intervals. The forecast itself is stored in the yhat column.\n\nThe Prophet library has its own built-in tools for visualization that enable us to quickly evaluate the result.\n\n- First, there is a method called Prophet.plot that plots all the points from the forecast:\n- The Second function Prophet.plot_components might be much more useful in our case. It allows us to observe different components of the model separately: trend, yearly and weekly seasonality. In addition, if you supply information about holidays and events to your model, they will also be shown in this plot.\n\nLet's try it out:","011a24ff":"## Forecast quality evaluation","7d3fe417":"##  Few Pivoted Plots","213385bf":"The holiday effect can be seen in the forecast dataframe:","81d8f746":"## Exponential smoothing\n\nNow, let's see what happens if, instead of weighting the last $k$ values of the time series, we start weighting all available observations while exponentially decreasing the weights as we move further back in time. There exists a formula for exponential smoothing that will help us with this:\n\n$$\\hat{y}_{t} = \\alpha \\cdot y_t + (1-\\alpha) \\cdot \\hat y_{t-1} $$\n\nHere the model value is a weighted average between the current true value and the previous model values. The $\\alpha$ weight is called a smoothing factor. It defines how quickly we will \"forget\" the last available true observation. The smaller $\\alpha$ is, the more influence the previous observations have and the smoother the series is.\n\nExponentiality is hidden in the recursiveness of the function -- we multiply by $(1-\\alpha)$ each time, which already contains a multiplication by $(1-\\alpha)$ of previous model values.","bd56678b":"We begin with a **simple definition of time series**:\n\n- Time series is a series of data points indexed (or listed or graphed) in time order. Therefore, the data is organized by relatively deterministic timestamps, and may, compared to random sample data, contain additional information that we can extract.","837c188f":"Let's start with a naive hypothesis: \"tomorrow will be the same as today\". However, instead of a model like $\\hat{y}_{t} = y_{t-1}$ (which is actually a great baseline for any time series prediction problems and sometimes is impossible to beat), we will assume that the future value of our variable depends on the average of its $k$ previous values. Therefore, we will use the moving average.\n\n$\\hat{y}_{t} = \\frac{1}{k} \\displaystyle\\sum^{k}_{n=1} y_{t-n}$","980994cb":"- The authors of the library generally advise to make predictions based on at least several months, ideally, more than a year of historical data. Luckily, in our case we have more than a couple of years of data to fit the model.\n\n- To measure the quality of our forecast, we need to split our dataset into the historical part and the prediction part... (We should have done this)","3c0140bd":"## Prophet\n\n*(From the Docs Itself)*\n\nlet's take a closer look at how Prophet works. In its essence, this library utilizes the additive regression model $y(t)$ comprising the following components:\n\n$$y(t) = g(t) + s(t) + h(t) + \\epsilon_{t},$$ where:\n\n- Trend $g(t)$ models non-periodic changes.\n- Seasonality $s(t)$ represents periodic changes.\n- Holidays component $h(t)$ contributes information about holidays and events.\n\n\n> ### Trend     $g(t)$\n\nThe Prophet library implements two possible trend models for $g(t)$.\n\nThe first one is called Nonlinear, Saturating Growth. It is represented in the form of the logistic growth model:\n\n$$g(t) = \\frac{C}{1+e^{-k(t - m)}},$$ where:\n\n- $C$ is the carrying capacity (that is the curve's maximum value).\n- $k$ is the growth rate (which represents \"the steepness\" of the curve).\n- $m$ is an offset parameter.\n\n> ### Seasonality    $s(t)$\n\nThe seasonal component $s(t)$ provides a flexible model of periodic changes due to weekly and yearly seasonality. Yearly seasonality model in Prophet relies on Fourier series.\n\n> ### Holidays and Events      $h(t)$\n\nThe component $h(t)$ represents predictable abnormal days of the year including those on irregular schedules, e.g., Black Fridays.\n\nTo utilize this feature, the analyst needs to provide a custom list of events.\n\n> ### Error    $\\epsilon(t)$\n\nThe error term $\\epsilon(t)$ represents information that was not reflected in the model. Usually it is modeled as normally distributed noise.\n\nIn describing these time series, we have used words such as $\u201ctrend\u201d$ and $\u201cseasonal\u201d$ which need to be defined more carefully.\n\n>Trend\n\n A trend exists when there is a long-term increase or decrease in the data. It does not have to be linear. Sometimes we will refer to a trend as \u201cchanging direction\u201d, when it might go from an increasing trend to a decreasing trend.\n \n> Seasonal\n\n A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known frequency.\n\n> Cycle\n\nA cycle occurs when the data exhibit rises and falls that are not of a fixed frequency. These fluctuations are usually due to economic conditions, and are often related to the \u201cbusiness cycle\u201d. The duration of these fluctuations is usually at least 2 years.","e4a80dcd":"## Necessary Imports","ade54ea3":"## Keras Embeddings ","b5f7a8dc":"The last Weekly Plot Says it All.\n\n- **Seems like People go to Shopping Mostly in July**[](http:\/\/)\n- **Have a look at the peak at Sundays and Saturdays** **(3rd Plot)**\n- **So we should add the holidays effect to make Prohet perform better.**","2a9a428e":"We predict values with Prophet by passing in the dates for which we want to create a forecast. If we also supply the historical dates (as in our case), then in addition to the prediction we will get an in-sample fit for the history. Let's call the model's predict method with our future dataframe as an input:","c4ddd427":"The above output suggests that SARIMA(2, 0, 1)x(2, 2, 0, 12) yields the lowest AIC value of 17.435499462373613. Therefore we should consider this to be optimal option.\n- ARIMA(0, 0, 0)x(2, 2, 0, 12)12 - AIC:28.152584128715233\n- ARIMA(0, 0, 1)x(2, 2, 0, 12)12 - AIC:21.20352160942468\n- ARIMA(0, 0, 2)x(2, 2, 0, 12)12 - AIC:18.308712222027623\n- ARIMA(1, 0, 1)x(2, 2, 0, 12)12 - AIC:18.039431593093965\n- ARIMA(1, 0, 2)x(2, 2, 0, 12)12 - AIC:17.583895110587616\n- ARIMA(2, 0, 1)x(2, 2, 0, 12)12 - AIC:17.435499462373613\n- ARIMA(2, 0, 2)x(2, 2, 0, 12)12 - AIC:17.473412955915293","6954c2bf":"**We are going to apply one of the most commonly used method for time-series forecasting, known as ARIMA, which stands for Autoregressive Integrated Moving Average.**\n\n**ARIMA models are denoted with the notation ARIMA(p, d, q). These three parameters account for seasonality, trend, and noise in data..**\n\n- AR: Auto-Regressive (p): AR terms are just lags of dependent variable. For example lets say p is 3, we will use x(t-1), x(t-2) and x(t-3) to predict x(t)\n- I: Integrated (d): These are the number of nonseasonal differences. For example, in our case we take the first order difference. So we pass that variable and put d=0\n- MA: Moving Averages (q): MA terms are lagged forecast errors in prediction equation.","4ffa35ff":"**This Code Section rather (Features) comes from this Publicly Shared Kernel and added a bit more to it on basis of this discussion**\n\n - https:\/\/www.kaggle.com\/abhilashawasthi\/feature-engineering-lgb-model\/comments#362974 (discussion)\n\n - https:\/\/www.kaggle.com\/CVxTz\/keras-starter (Kernel)\n\n - https:\/\/www.kaggle.com\/abhilashawasthi\/feature-engineering-lgb-model (Kernel)\n\n**Thanks a lot for the same !!!**\n\n**Aditya.**","66dcce53":"## FE","66fc882c":"We are also going to define a helper function that we will use to gauge the quality of our forecasting with MAPE and MAE error measures:","9de51dff":"## Adding Holidays (Finally)","7b374092":"## Double exponential Smoothing\n\nUp to now, the methods that we've have seen are for a single future point prediction (with some nice smoothing). That is cool, but it is also not enough. Let's extend exponential smoothing so that we can predict two future points (of course, we will also include more smoothing).\n\nSeries decomposition will help us -- we obtain two components: intercept (i.e. level) $\\ell$ and slope (i.e. trend) $b$. We have learnt to predict intercept (or expected series value) with our previous methods; now, we will apply the same exponential smoothing to the trend by assuming that the future direction of the time series changes depends on the previous weighted changes. As a result, we get the following set of functions:\n\n$$\\ell_x = \\alpha y_x + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n\n$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n\n$$\\hat{y}_{x+1} = \\ell_x + b_x$$\n\n- The first one describes the intercept, which, as before, depends on the current value of the series.\n- The second term is now split into previous values of the level and of the trend. \n    -  The second function describes the trend, which depends on the level changes at the current step and on the previous value of the trend. In this case, the $\\beta$ coefficient is a weight for exponential smoothing. \n- The final prediction is the sum of the model values of the intercept and trend.","10cd6f9f":"### Prophet With Box Cox Transformation","48f93ccf":"The holiday effects will also show up in the components plot, where we see that there is a spike on the days around playoff appearances, with an especially large spike for the superbowl:","5d4fee79":"In this notebook, I have tried to depict the following\n- Little Bit TS Theory from Wiki , Different Blogposts and Online Books\n- EDA\n- Seasonality Demonstration\n- Moving Average, Exponential Average and Smoothen\n- Few Pivot Plots\n- Keras Embedding(Re-added for Testing)\n- Prophet (With and Without Transformations)\n- ARIMA\n- Validating The Forecast via Plots and Different Metrics\n- Modelling\n- **LSTM Modelling( Next Update and Probably the last by me...)**\n\nHere's the collection of Resources (including all the publicly shared kernels)\n- https:\/\/www.kaggle.com\/c\/demand-forecasting-kernels-only\/discussion\/63568\n\nAnd in particular this discussion (Thanks a Lot, I learnt a lot from it locally)\n- https:\/\/www.kaggle.com\/c\/demand-forecasting-kernels-only\/discussion\/62592","4da13518":"**From the above we cn see that the lags till 50 are having weightage wrt the ACF Plots, but according to the PACF plots they valley out after the 10th lag...**","9325518f":"## Thanks For Making It To The End !!!","0c5148d1":"Using the helper method ```Prophet.make_future_dataframe```, we create a dataframe which will contain all dates from the history and also extend into the future for those 92 days that we left out before.","4169a925":"## Moving Average","4ed1a503":"Kindly Refer To the [docs](https:\/\/facebook.github.io\/prophet\/docs\/seasonality,_holiday_effects,_and_regressors.html) for the nomenclature...\n\n(In my understanding, It's just denoting that one is more dominant than the other..)"}}