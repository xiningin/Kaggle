{"cell_type":{"ef1000a5":"code","7bf3d283":"code","e4637ab6":"code","6451aea6":"code","c1492e41":"code","454b543f":"code","b8dd494e":"code","971544d2":"code","89fcf035":"code","b17b33e7":"code","8e2ba290":"code","3992c08a":"markdown","d5dfee14":"markdown","3699e281":"markdown","f129e400":"markdown","33aa77f0":"markdown","e970bdb1":"markdown","b8c35f0d":"markdown","704639b9":"markdown","6dcadd72":"markdown","b4c4f8c3":"markdown","83eb0b00":"markdown"},"source":{"ef1000a5":"import time\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom sklearn.preprocessing import LabelEncoder","7bf3d283":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nmerchants = pd.read_csv(\"..\/input\/merchants.csv\")\nhistorical_transactions = pd.read_csv(\"..\/input\/historical_transactions.csv\")\nnew_merchant_transactions = pd.read_csv(\"..\/input\/new_merchant_transactions.csv\")","e4637ab6":"def mem_usage_mb(dataframes):\n    return int(sum([df.memory_usage().sum() for df in dataframes]) \/ 1024**2)\n \nmem_usg_0 = mem_usage_mb([train, test, merchants, historical_transactions, new_merchant_transactions])\nprint(\"Memory usage: {} MB\".format(mem_usg_0))","6451aea6":"card_ids = np.hstack((train[\"card_id\"].values, test[\"card_id\"].values))\nencoder = LabelEncoder().fit(card_ids)\n\nfor df in (test, train, historical_transactions, new_merchant_transactions):\n    df[\"card_id\"] = encoder.transform(df[\"card_id\"])\n    \nencoder = LabelEncoder().fit(merchants[\"merchant_id\"])\nfor df in (merchants, historical_transactions, new_merchant_transactions):\n    # rows with non-null merchant_id\n    df.loc[~df[\"merchant_id\"].isnull(), \"merchant_id\"] \\\n        = encoder.transform(df.loc[~df[\"merchant_id\"].isnull(), \"merchant_id\"])\n    # rows with null merchant_id\n    df.loc[df[\"merchant_id\"].isnull(), \"merchant_id\"] = -1","c1492e41":"historical_transactions[\"historical\"] = True\nnew_merchant_transactions[\"historical\"] = False\ntransactions = pd.concat((historical_transactions, new_merchant_transactions), axis=0).reset_index(drop=True)","454b543f":"transactions[\"authorized_flag\"] = transactions[\"authorized_flag\"] == \"Y\"\n  \ntransactions[\"category_1\"] = transactions[\"category_1\"] == \"Y\"\n\ntransactions[\"category_3\"] = transactions[\"category_3\"].fillna(\"\")\ntransactions[\"category_3\"] = LabelEncoder().fit(transactions[\"category_3\"]).transform(transactions[\"category_3\"])\n\ntransactions[\"purchase_date\"] = pd.to_datetime(transactions[\"purchase_date\"])","b8dd494e":"def reduce_mem_usage(dataframe, skip=[]):\n    \n    for col in dataframe.columns:\n        \n        if col in skip:\n            continue\n        \n        col_type = str(dataframe[col].dtype)\n        \n        if col_type.startswith(\"int\"):\n            \n            mx = dataframe[col].max()\n            mn = dataframe[col].min()\n            \n            if mn >= 0:\n                if mx < 255:\n                    dataframe[col] = dataframe[col].astype(np.uint8)\n                elif mx < 65535:\n                    dataframe[col] = dataframe[col].astype(np.uint16)\n                elif mx < 4294967295:\n                    dataframe[col] = dataframe[col].astype(np.uint32)\n                else:\n                    dataframe[col] = dataframe[col].astype(np.uint64)\n            else:\n                if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                    dataframe[col] = dataframe[col].astype(np.int8)\n                elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                    dataframe[col] = dataframe[col].astype(np.int16)\n                elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                    dataframe[col] = dataframe[col].astype(np.int32)\n                elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                    dataframe[col] = dataframe[col].astype(np.int64)\n                    \n        elif col_type.startswith(\"float\"):\n            \n            mx = dataframe[col].max()\n            mn = dataframe[col].min()\n    \n            if mn > np.finfo(np.float32).min and mx < np.finfo(np.float32).max:\n                dataframe[col] = dataframe[col].astype(np.float32)","971544d2":"    reduce_mem_usage(transactions)\n    reduce_mem_usage(train)\n    reduce_mem_usage(test)\n    \n    reduce_mem_usage(merchants, skip=[\"avg_purchases_lag3\", \"avg_purchases_lag6\", \"avg_purchases_lag12\"])","89fcf035":"mem_usg_1 = mem_usage_mb([train, test, merchants, transactions])\nprint(\"Memory usage: {} MB\".format(mem_usg_1))\nprint(mem_usg_1 \/ mem_usg_0 * 100, \"% of initial size\")\n","b17b33e7":"def save_as_parquet(df, path):\n    table = pa.Table.from_pandas(df)\n    pq.write_table(table, path, use_dictionary=True, compression='snappy')\n\nsave_as_parquet(train, \"train.parquet\")\nsave_as_parquet(test, \"test.parquet\")\nsave_as_parquet(merchants, \"merchants.parquet\")\nsave_as_parquet(historical_transactions, \"historical_transactions.parquet\")\nsave_as_parquet(new_merchant_transactions, \"new_merchant_transactions.parquet\")\nsave_as_parquet(transactions, \"transactions.parquet\")\n","8e2ba290":"def load_parquet(path):\n    table = pq.read_table(path, nthreads=4)\n    return table.to_pandas()\n\np0 = time.time()\nload_parquet(\"train.parquet\")\nload_parquet(\"test.parquet\")\nload_parquet(\"merchants.parquet\")\nload_parquet(\"transactions.parquet\")\np1 = time.time()\np1 - p0\n    ","3992c08a":"Let's see how much memory we've saved:","d5dfee14":"Encode rest of the text columns:","3699e281":"Save to parquet format:","f129e400":"Simplified version of reduce_mem_usage:","33aa77f0":"Compress the old and new numeric fields is possible:","e970bdb1":"Merge old and new transactions in one table:","b8c35f0d":"Let's see how much memory it occupies in the beginning:","704639b9":"First load all the data:","6dcadd72":"Label encoding for Card ID and Merchant ID:","b4c4f8c3":"How fast is the loading now:","83eb0b00":"Loading this data from csv is pain when you have slow machine with limited memory (like I do).\n\nSo, by several steps we gonna reduce both memory size and loading time:\n\n1. Label encode Card ID and Merchant ID labels in transaction histories and train\/test sets\n2. Merge historical and \"new\" transaction logs into one (also add distinguishing column)\n3. Encode date field\n4. Encode text categorical fields as numeric\n5. Compress all numeric fields if possible with reduce_mem_usage*\n6. At last: save all the data in parquete format \n\n\\* reduce_mem_usage is a slightly simplified version of [this kernel](https:\/\/www.kaggle.com\/arjanso\/reducing-dataframe-memory-size-by-65) by [arjanso](https:\/\/www.kaggle.com\/arjanso)\n\n\\** Saving data in parquete format (by using [Pyarrow](https:\/\/arrow.apache.org\/docs\/python\/parquet.html)) gets us great loading speed due to compression, muilthreaded loading, etc.)\n\nOn my home machine (cpu: 4 cores, mem: 16GB) loading time is reduced down to **2.7 sec.**\n\nIn this notebook it's around **6 sec.**\n"}}