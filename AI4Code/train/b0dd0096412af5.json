{"cell_type":{"d1d1f343":"code","f3330f06":"code","62760330":"code","82dac538":"code","94d68162":"code","420089c3":"code","9e9e8198":"code","11e2bdb8":"code","61d3b52f":"code","134c925f":"code","4a5b6bf3":"code","8c5575f8":"code","2e5674c7":"markdown","cdd5a8aa":"markdown","b64cc328":"markdown","fedfcad7":"markdown","a27c1df9":"markdown","afa420b1":"markdown","e21bc337":"markdown","b617996f":"markdown","beb3e67b":"markdown","eda7222c":"markdown","9a9e3b59":"markdown","f69ce9b5":"markdown","7b781224":"markdown"},"source":{"d1d1f343":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\ntrain_filename = \"\/kaggle\/input\/titanic\/train.csv\"\ntest_filename = \"\/kaggle\/input\/titanic\/test.csv\"\nresult_filename = \"result.csv\"\n\ntrain_data = pd.read_csv(train_filename)\ntest_data = pd.read_csv(test_filename)\n\ntrain_data.describe(include='all')","f3330f06":"test_data.describe(include='all')","62760330":"combined_data = pd.concat([train_data, test_data])\nassumed_values = np.zeros((3, 2))\nfor pclass in range(1, 4):\n    for sex in ['male', 'female']:\n        guess = round(\n            combined_data[(combined_data['Sex'] == sex) & (combined_data[\"Pclass\"] == pclass)]['Age'].dropna().mean(),\n            0)\n        train_data.loc[\n            (train_data['Age'].isnull()) & (train_data['Sex'] == sex) & (train_data['Pclass'] == pclass), 'Age'] = guess\n        \n        test_data.loc[\n            (test_data['Age'].isnull()) & (test_data['Sex'] == sex) & (test_data['Pclass'] == pclass), 'Age'] = guess\n\nfor dataset in [train_data, test_data]:    \n    dataset.loc[dataset['Age'] <= 8, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 8) & (dataset['Age'] <= 16), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 4\n    dataset.loc[dataset['Age'] > 64, 'Age'] = 5","82dac538":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in [train_data, test_data]:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', \\\n                                                   'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)","94d68162":"for dataset in [train_data, test_data]:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1","420089c3":"for dataset in [train_data, test_data]:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n    dataset['Embarked'] = dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)","9e9e8198":"for dataset in [train_data, test_data]:\n    median_fare = dataset['Fare'].dropna().median()\n    dataset['Fare'] = dataset['Fare'].fillna(median_fare)\n    dataset['FarePerPerson'] = dataset['Fare'] \/ dataset['FamilySize']","11e2bdb8":"for dataset in [train_data, test_data]:\n    dataset['CabinCategory'] = dataset['Cabin'].astype(str).str[0]\n    dataset['CabinCategory'] = dataset['CabinCategory'].map(\n        {'A': 1, 'B': 2, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'T': 7})\n    dataset['CabinCategory'] = dataset['CabinCategory'].fillna(0)\n    dataset['CabinCategory'] = dataset['CabinCategory'].astype(int)\n    # Cabin Grouping\n    dataset['HasCabin'] = dataset['Cabin'].apply(lambda x: 0 if x is np.nan else 1)","61d3b52f":"for dataset in [train_data, test_data]:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","134c925f":"y = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"Title\", \"FamilySize\", \"IsAlone\", \"Embarked\", \"FarePerPerson\", \"CabinCategory\", \"HasCabin\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nX.head()","4a5b6bf3":"random_forest_estimator = RandomForestClassifier(random_state=1337, max_depth=3)\nrandom_forest_params = {'n_estimators': range(70, 140, 5)}\n\ngsearch = GridSearchCV(estimator = random_forest_estimator,\n                       param_grid = random_forest_params,\n                       n_jobs=4, cv=2)\ngsearch.fit(X, y)\n\nprint(gsearch.best_params_)\nprint(gsearch.best_score_)","8c5575f8":"predictions = gsearch.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv(result_filename, index=False)\nprint(\"Finished :D\")","2e5674c7":"Now, let's talk about how much the passengers paid to be onboard of the Titanic: the **Fare** feature. Since there is on passenger that we don't know the fare, we will assume it's the median fare of the other passengers. We also know, by looking at the tickets, that people from the same family have the same ticket and the same fare value. Because of that, we will create a new feature called **FarePerPerson** that is just the Fare value divided by the size of the family","cdd5a8aa":"We have the name of the passenger. Besides the fact that the passager name would not be a good feature to predict her survival, we have the information of the **title** of the passenger. Since the title could represent a \"social status\" this info could be interesting to be used as a feature in our model.\n\nOther aspect is that there are a lot of \"rare\" titles. We will group those rare titles in one title because it will not be a good information to our model, since we have few passengers with those titles.","b64cc328":"Now we will fit a Random Forest Classifier with our train data.\n\nIn order to find the best **n_estimators** hyperparameter, we will use a GridSearch with two folds.\n\nAfter the fit phase we will print the best params and our score with the training_data (This code could take a few seconds to run)","fedfcad7":"We will start to work with the **Age** feature. It's natural to think that the difference in the age of the passangers would be a good feature to predict the likelihood of survival.\n\nSince we have some passengers without their ages, we have to \"guess\". I will assume, for any passenger without age, that her age will be the mean age of other passenger of the same **sex** and that are on the same **pclass**\n\nTo do that, first of all we will combine train_data and test_date to have more datapoints. After that we will calculate the mean age for every combination of sex and pclass, and we will fill the null values with the guessed age.\n\nIn the end we will group the data into six ages groups. *I really don't know why, but I saw that the model performs better this way. If anyone could add more information about that, please leave it the comments :D*","a27c1df9":"And that is it! We could get a **score of 0.78947**","afa420b1":"Ok! So we are done with the features. Good work so far!\n\nLet's see how our data is looking...","e21bc337":"Now, we will create two new features. **IsAlone** and **FamilySize**\n\nThe first one will map if the passenger is alone in the boat. This appears to help to understand if the passenger survives or not. And the second will be the size of you family that is onboard. Try to imagine how was the struggle to survive and to help your family... Would this give you more energy or would this be more difficult?","b617996f":"Last but not least we will map the **Sex** of the passenger as a integer feature","beb3e67b":"By describing the features of both *train_data* and *test_data* we can note: \n- We have a few null values for the **Age** and **Cabin** in both datasets \n- We discovered that in the *test_data* we have one null **Fare**. \n- And, in the *train_data* we have two null values in **Embarked**\n\nWe will deal with those null values in a moment.","eda7222c":"Not so bad! :D\n\nLet's predict the test_data and create our output file. **Hope we will get a 0.78+ score!**","9a9e3b59":"# Titanic Survivors!\n## What's my goal here?\nSo, this was my first competition here at Kaggle. My goal was to perform better than the two simple examples that exist in the tutorial.\n\nWe already know that if we say that only women survived, we end up with a score of **0.76555**,\nand we already know that the [tutorial made by Alexis Cook](https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial) have a score of **0.77511**\n\nSo, started aiming at a score of **0.78+**. Let's try!\n\nAnd, of course: **All comments and improvement tips will be very appreciated!** Thanks ;)\n\nLet's start by looking a little bit to our data.","f69ce9b5":"We saw that there are two passenger that we don't know where she boarded. We will simply say that she boarded at Southampton. Thus, the \"S\" value. After that we will cast the letters to numbers.","7b781224":"Now, let's deal with the **Cabin** feature.\n\nWe know that the first letter of the Cabin represent its category, and we will create a new feature called **CabinCategory** that will map this. For the passengers that we don't know the Cabin, we will simply use the number zero.\n\nAfter that we will create another feature to map if we know of if we don't know the passenger's cabin. It will be the **HasCabin** feature."}}