{"cell_type":{"a6827aae":"code","5ff8cb42":"code","2106add0":"code","9c759122":"code","41868274":"code","24757c6c":"code","ab813430":"code","a22f4a78":"code","4ab55d2e":"code","a4306124":"code","71bd4a78":"code","c04e514d":"code","3791aea0":"code","cd2b306f":"code","cb87b678":"code","d21e8ac1":"code","96e956c2":"code","08c7edd4":"code","a1cade2d":"code","05b68b47":"code","f5253630":"code","80d74eb7":"markdown","5048521f":"markdown","377932f3":"markdown","77cd1e75":"markdown","1ec07ea6":"markdown","a696e821":"markdown","6122b107":"markdown","04b4417c":"markdown","65960009":"markdown","64c03324":"markdown","50152191":"markdown","ff5c669f":"markdown","2edc76b5":"markdown","db3b3ae4":"markdown","17def156":"markdown","23ca080d":"markdown","6175d077":"markdown","e646c16d":"markdown","179abc72":"markdown","ac743f3c":"markdown","d480a65c":"markdown","197496ba":"markdown","df276e5d":"markdown","2822a609":"markdown","77bfa4d9":"markdown","49dc284b":"markdown","11f92600":"markdown","88a086d7":"markdown","af6fa03e":"markdown","d835b368":"markdown","5739df3a":"markdown","216827e3":"markdown","9340d26a":"markdown","dceb9a2b":"markdown"},"source":{"a6827aae":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, random_split, DataLoader\nfrom PIL import Image\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nfrom sklearn.metrics import f1_score\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchvision.utils import make_grid\n%matplotlib inline","5ff8cb42":"DATA_DIR = '..\/input\/jovian-pytorch-z2g\/Human protein atlas'\n\nTRAIN_DIR = DATA_DIR + '\/train'                           # Contains training images\nTEST_DIR = DATA_DIR + '\/test'                             # Contains test images\n\nTRAIN_CSV = DATA_DIR + '\/train.csv'                       # Contains real labels for training images\nTEST_CSV = '..\/input\/jovian-pytorch-z2g\/submission.csv'   # Contains dummy labels for test image","2106add0":"train_df = pd.read_csv(TRAIN_CSV)\nlabels = {\n    0: 'Mitochondria',\n    1: 'Nuclear bodies',\n    2: 'Nucleoli',\n    3: 'Golgi apparatus',\n    4: 'Nucleoplasm',\n    5: 'Nucleoli fibrillar center',\n    6: 'Cytosol',\n    7: 'Plasma membrane',\n    8: 'Centrosome',\n    9: 'Nuclear speckles'\n}","9c759122":"def encode_label(label):\n    #create an initial target which is all zeros\n    target = torch.zeros(10)\n    for l in str(label).split(' '):\n        target[int(l)] = 1.\n    return target\n\ndef decode_target(target, text_labels=False, threshold=0.5):\n    result = []\n    for i, x in enumerate(target):\n        if (x >= threshold):\n            if text_labels:\n                result.append(labels[i] + \"(\" + str(i) + \")\")\n            else:\n                result.append(str(i))\n    return ' '.join(result)\n\ndef show_sample(img, target, invert=True):\n    if invert:\n        plt.imshow(1 - img.permute((1, 2, 0)))\n    else:\n        plt.imshow(img.permute(1, 2, 0))\n    print('Labels:', decode_target(target, text_labels=True))\n    \ndef show_batch(dl, invert=True):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(16, 8))\n        ax.set_xticks([]); ax.set_yticks([])\n        data = 1-images if invert else images\n        ax.imshow(make_grid(data, nrow=16).permute(1, 2, 0))\n        break\n        \ndef get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)","41868274":"class HumanProteinDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        self.df = pd.read_csv(csv_file)\n        self.transform = transform\n        self.root_dir = root_dir\n        \n    def __len__(self):\n        return len(self.df)    \n    \n    def __getitem__(self, idx):\n        row = self.df.loc[idx]\n        img_id, img_label = row['Image'], row['Label']\n        img_fname = self.root_dir + \"\/\" + str(img_id) + \".png\"\n        img = Image.open(img_fname)\n        if self.transform:\n            img = self.transform(img)\n        return img, encode_label(img_label)","24757c6c":"transform = transforms.Compose([transforms.Resize(256), transforms.ToTensor()])\ndataset = HumanProteinDataset(TRAIN_CSV, TRAIN_DIR, transform=transform)","ab813430":"torch.manual_seed(10)\nval_pct = 0.1\nval_size = int(val_pct * len(dataset))\ntrain_size = len(dataset) - val_size\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)","a22f4a78":"batch_size = 64\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size*2, num_workers=2, pin_memory=True)","4ab55d2e":"show_batch(train_dl)","a4306124":"def F_score(output, label, threshold=0.5, beta=1):\n    prob = output > threshold\n    label = label > threshold\n\n    TP = (prob & label).sum(1).float()\n    TN = ((~prob) & (~label)).sum(1).float()\n    FP = (prob & (~label)).sum(1).float()\n    FN = ((~prob) & label).sum(1).float()\n\n    precision = torch.mean(TP \/ (TP + FP + 1e-12))\n    recall = torch.mean(TP \/ (TP + FN + 1e-12))\n    F2 = (1 + beta**2) * precision * recall \/ (beta**2 * precision + recall + 1e-12)\n    return F2.mean(0)","71bd4a78":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvgg16 = models.vgg16(pretrained=True)\nvgg16 ","c04e514d":"for param in vgg16.parameters():\n  param.require_grad = False","3791aea0":"fc = nn.Sequential(\n    nn.Linear(25088, 5000),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    \n    nn.Linear(5000, 1000),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    \n    nn.Linear(1000, 460),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    \n    nn.Linear(460,10),\n    \n)\n\nvgg16.classifier = fc\nvgg16","cd2b306f":"class MultilabelImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, targets = batch \n        out = self(images)                      \n        loss = F.binary_cross_entropy(out, targets)      \n        return loss\n    \n    def validation_step(self, batch):\n        images, targets = batch \n        out = self(images)                           # Generate predictions\n        loss = F.binary_cross_entropy(out, targets)  # Calculate loss\n        score = F_score(out, targets)\n        return {'val_loss': loss.detach(), 'val_score': score.detach() }\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_scores = [x['val_score'] for x in outputs]\n        epoch_score = torch.stack(batch_scores).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_score': epoch_score.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_score: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_score']))","cb87b678":"class DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","d21e8ac1":"class TransferModel(MultilabelImageClassificationBase):\n    def __init__(self, model):\n        super().__init__()\n        # Use a pretrained model\n        self.network = model\n    \n    def forward(self, xb):\n        return torch.sigmoid(self.network(xb))","96e956c2":"device = get_default_device()\ndevice","08c7edd4":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)","a1cade2d":"from tqdm.notebook import tqdm\n@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.Adam):\n    torch.cuda.empty_cache()\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","05b68b47":"model = to_device(TransferModel(vgg16), device)","f5253630":"num_epochs = 1\nopt_func = torch.optim.Adam\nlr = .0001\nfit(num_epochs, lr, model, train_dl, val_dl, opt_func)","80d74eb7":"Lets see how out model looks after adding the new layers. You can see that the input and the output of linear layers got changed.","5048521f":"![image.png](attachment:image.png)","377932f3":"<a id=\"utility\"><\/a>\n## 4. Utility functions","77cd1e75":"Lets create our own layer. For the purpose of this tutorial, I am creating some random layers.","1ec07ea6":"We can see the last layer of this network, it contain three linear layers and with relu activation and three dropout layers. We remove this layer and put our own classifier layer.","a696e821":"<a id=\"transfer\"><\/a>\n## 8. Transfer Learning\n<a id=\"download\"><\/a>\n### 8.1 Downloading model","6122b107":"<a id=\"what\"><\/a>\n## 2. What is transfer learning ?\n\nTransfer learning is a machine learning technique where knowledge gained during training in one type of problem is used to train in other, similar types of problem.<p\/>\nThus, instead of building your own deep neural networks, which can be a cumbersome task to say the least, you can find an existing neural network that accomplishes the same task you\u2019re trying to solve and reuse the layers that are essential for pattern detection, while also making changes to the fully connected layer to suit your problem.","04b4417c":"<a id=\"reference\"><\/a>\n## 11 Reference\n\n* https:\/\/www.analyticsvidhya.com\/blog\/2019\/10\/how-to-master-transfer-learning-using-pytorch\/\n* https:\/\/www.quora.com\/What-is-the-VGG-neural-network\n* https:\/\/www.kaggle.com\/carloalbertobarbano\/vgg16-transfer-learning-pytorch\n* https:\/\/heartbeat.fritz.ai\/transfer-learning-with-pytorch-cfcb69016c72","65960009":"<a id=\"introduction\"><\/a>\n## 1. Introduction","64c03324":"Please dont mind the result. This was just an experiment to show how we can do transfer learning. You can create better classifiers using the b[eefier pretrained models.](https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html) ","50152191":"<a id=\"data_loader\"><\/a>\n## 6. Creating Dataloaders","ff5c669f":"<a id=\"dataset\"><\/a>\n## 5. Create dataset","2edc76b5":"Below are some utility functions that we use in this notebook","db3b3ae4":"<a id=\"train\"><\/a>\n## 9. Training","17def156":"![image.png](attachment:image.png)","23ca080d":"Lets download this pretrained model. pretrained=True will download a pretrained network for us.","6175d077":"We are using the F2 score metric for calculating the validation score. ","e646c16d":"<a id=\"base\"><\/a>\n### 9.1 Baseclass","179abc72":"<a id=\"import\"><\/a>\n## 3. Importing libraries and data","ac743f3c":"For us humans if we learn something, similar things become easy. This is because we can apply our existing knowledge to the new tasks. One example of this is learning to ride a bycycle. If we know know to ride a bycycle, we can easly learn how to ride a motor bike. \n\nSimilar to this there are many many CNN models availabe to us, which are pretrained in a tens of thousands of images. Instead of building everything from scratch, we can leverage the knowledge of these neural networks and modfy them according to our needs. This is called transfer learning. ","d480a65c":"Lets create the dataloaders","197496ba":"![image.png](attachment:image.png)","df276e5d":"<a id=\"metric\"><\/a>\n## 7. Creating metric","2822a609":"<a id=\"Tmodel\"><\/a>\n### 9.3 Transfer learning model","77bfa4d9":"<a id=\"freeze\"><\/a>\n### 8.2 Freezing the model","49dc284b":"Lets create the dataset","11f92600":"<a id=\"1epoch\"><\/a>\n### 9.4 Training","88a086d7":"<a id=\"conclusion\"><\/a>\n## 10 Conclusion\n\nTransfer learning has several benefits, but the main advantages are  saving training time, better performance of neural networks (in most cases), and not needing a lot of data. \n\nUsually, a lot of data is needed to train a neural network from scratch but access to that data isn't always available \u2014 this is where transfer learning comes in handy. With transfer learning a solid machine learning model can be built with comparatively little training data because the model is already pre-trained.\n\nExperiment with [beefier models](https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html) availabe with pytorch for better results.","af6fa03e":"## Table of contents\n* [1. Introduction](#introduction)\n* [2. What is transfer learning](#what)\n* [3. Importing libraries and data](#import)\n* [4. Utility functions](#utility)\n* [5. Create dataset](#dataset)\n* [6. Creating Dataloaders](#data_loader)\n* [7. Creating metric](#metric)\n* [8. Transfer Learning](#transfer)\n    * [8.1 Downloading model](#download)\n    * [8.2 Freezing the model](#freeze)\n    * [8.3 Creating final layer](#create)\n* [9. Training](#train)\n    * [9.1 Baseclass](#base)\n    * [9.2 DataLoader](#dloader)\n    * [9.3 Transfer learning model](#Tmodel)\n    * [9.4 Training](#1epoch)\n* [10. Conclusion](#conclusion)\n* [11. Reference](#reference)","d835b368":"We are going use the vgg16 pretrained model for transfer learning.","5739df3a":"Lets create a base class for the Image classification.","216827e3":"The purpose of using a pretrained model is to avoid long training time. Here we dont need to train this entire model. Instead we are training only the layers that we add. So lets freeze the existing layers. We can freeze the model using the require_grad as false.","9340d26a":"<a id=\"create\"><\/a>\n### 8.3 Creating final layer","dceb9a2b":"<a id=\"dloader\"><\/a>\n### 9.2 DataLoader"}}