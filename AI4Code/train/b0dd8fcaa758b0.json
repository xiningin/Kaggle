{"cell_type":{"bda1f927":"code","5611db3c":"code","cb4fccf7":"code","d5eda601":"code","32857057":"code","2a416d2a":"code","c49adb4e":"code","7b019051":"code","09513e5c":"code","067c3109":"code","fa21bb0e":"code","bee9e032":"code","afc9b685":"code","311bdb6e":"code","21e6439a":"code","39a62497":"code","ef57d5f8":"code","7e0c5f56":"code","6b60b798":"code","dd1b68c9":"code","745bcedb":"code","af60b36b":"code","26efb51c":"code","59ad7e30":"code","70f7edc7":"code","dd63e301":"code","a04cdc4d":"code","c24ee9f3":"code","e331d12d":"code","3c746080":"code","8f539f89":"code","1817bcbd":"code","fef90057":"markdown","3ddac3b8":"markdown","820e599d":"markdown","d499495f":"markdown","230b23bb":"markdown","2894b756":"markdown","baaf63cf":"markdown","9889f05d":"markdown","dd20f681":"markdown"},"source":{"bda1f927":"# utilities\nimport re\nimport numpy as np\nimport pandas as pd\n# plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# sklearn\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve\n","5611db3c":"# LOADING THE DATASET\ndf = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding = 'Latin-1', names=('target','id','date','flag','username','tweet'))\ndf.head()","cb4fccf7":"# Exploring the Dataset\ndf.info()","d5eda601":"# countplot for the occurence of each target value\nsns.countplot(x = 'target',data = df)","32857057":"# dropped the irrelevant columns that will not be used in sentiment analysis\ndf.drop(['date','flag','username'], axis=1, inplace=True)\ndf.drop('id', axis=1, inplace=True)","2a416d2a":"df.head()","c49adb4e":"# FOR STOP WORDS\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\n# FOR LEMMATIZATION\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# FOR STEMMING\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()","7b019051":"# functionn for nlp pipeline\ndef data_preprocessing(raw_text):\n\n    #Data Cleansing\n    sentence = re.sub(r'[^\\w\\s]', ' ',raw_text )\n\n\n    #Removing numbers\n    sentence = re.sub(r'[0-9]', '', sentence)\n    \n    #Tokenization\n    words = nltk.word_tokenize(sentence)\n\n    #Lowercase\n    for word in words:\n            word.lower()\n    \n    #Stop words removal\n    words = [w for w in words if not w in stop_words]\n    \n    #stemming\n    words = [stemmer.stem(w) for w in words]\n    \n    #Lemmatization\n    final_words = [lemmatizer.lemmatize(w) for w in words]\n    \n    return  final_words ","09513e5c":"df.tweet = df.tweet.apply(data_preprocessing)","067c3109":"df.tweet","fa21bb0e":"def listToString(s): \n    \n    # initialize an empty string\n    str1 = \" \" \n    \n    # return string  \n    return (str1.join(s))\n\nstring_list = []\nfor i in df.tweet :\n    string = listToString(i)\n    string_list.append(string)\n    \n# storing the string list created into the dataframe\ndf.tweet = string_list","bee9e032":"df.tweet","afc9b685":"# converting the 4 in target column to 1 to denote the value 'positive'\ndf.target = df.target.apply(lambda x: 1 if x==4 else x)","311bdb6e":"X = df.tweet\ny = df.target","21e6439a":"# splitting the dataset into test and train\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=42)","39a62497":"# tranforming the tweet data into vectors matrix\nvectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000, lowercase= False)\nvectoriser.fit(X_train)\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))","ef57d5f8":"X_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)","7e0c5f56":"naive_model = ComplementNB().fit(X_train,y_train)","6b60b798":"y_pred1 = naive_model.predict(X_test)\nprint(confusion_matrix(y_pred1,y_test))","dd1b68c9":"print(classification_report(y_pred1,y_test))","745bcedb":"fpr_dt_1, tpr_dt_1,_ = roc_curve(y_test,y_pred1)\nplt.plot(fpr_dt_1,tpr_dt_1,label = \"ROC curve\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.gcf().set_size_inches(5, 5)\nplt.show()","af60b36b":"from sklearn.linear_model import LogisticRegression\nlogistic = LogisticRegression(solver='liblinear').fit(X_train,y_train)\ny_pred2= logistic.predict(X_test)\nprint(confusion_matrix(y_pred2,y_test))","26efb51c":"print(classification_report(y_pred2,y_test))","59ad7e30":"fpr_dt_1, tpr_dt_1,_ = roc_curve(y_test,y_pred2)\nplt.plot(fpr_dt_1,tpr_dt_1,label = \"ROC curve\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.gcf().set_size_inches(5, 5)\nplt.show()","70f7edc7":"from sklearn.svm import LinearSVC\nSVC = LinearSVC().fit(X_train,y_train)\ny_pred3 = SVC.predict(X_test)\nprint(confusion_matrix(y_pred3,y_test))","dd63e301":"print(classification_report(y_pred3,y_test))","a04cdc4d":"fpr_dt_1, tpr_dt_1,_ = roc_curve(y_test,y_pred3)\nplt.plot(fpr_dt_1,tpr_dt_1,label = \"ROC curve\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.gcf().set_size_inches(5, 5)\nplt.show()","c24ee9f3":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n\ny_pred4 = model.predict(X_test)\nprint(confusion_matrix(y_pred4,y_test))","e331d12d":"print(classification_report(y_pred4 ,y_test))","3c746080":"fpr_dt_1, tpr_dt_1,_ = roc_curve(y_test,y_pred4)\nplt.plot(fpr_dt_1,tpr_dt_1,label = \"ROC curve\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.gcf().set_size_inches(5, 5)\nplt.show()","8f539f89":"# dataframe for test and predicted sentiments (from logistic regression model)\ntest = pd.DataFrame(list(zip((vectoriser.inverse_transform(X_test)) ,y_pred2, y_test)),\n                    columns = ['TEST_TWEETS', 'PREDICTED' , 'ORIGINAL'])","1817bcbd":"test","fef90057":"### XGB CLASSIFIER","3ddac3b8":"### SVC MODEL","820e599d":"# DATA PREPROCESSING","d499495f":"As it can be observed that the Logistic Regression model is giving the best results.","230b23bb":"# IMPORTING LIBRARIES","2894b756":"# EDA","baaf63cf":"### LOGISTIC REGRESSION","9889f05d":"### NAIVE BAYES MODEL","dd20f681":"# IMPLOYING MODELS"}}