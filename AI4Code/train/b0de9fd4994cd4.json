{"cell_type":{"92188bba":"code","b28cabc8":"code","b80b98ac":"code","ce80c4a1":"code","1e980d57":"code","0a5dade0":"code","a14473bd":"code","c584eab0":"code","9f13f324":"code","1408054d":"code","d2226ea9":"code","3dc09f56":"code","f2507b14":"code","d68b0e25":"code","2b949b0d":"code","a4af195d":"code","2b0d7bac":"code","ebfcce95":"markdown","83968d0d":"markdown","f8558fed":"markdown","64a144f6":"markdown","e52b27d9":"markdown","d54de805":"markdown","4424047c":"markdown"},"source":{"92188bba":"import numpy as np\nimport pandas as pd\nfrom numpy import concatenate\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.semi_supervised import LabelSpreading\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.semi_supervised import LabelPropagation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import VotingClassifier","b28cabc8":"path = '..\/input\/heart-disease-uci\/heart.csv'\ndf = pd.read_csv(path)\ndf = shuffle(df)\ndf.head()","b80b98ac":"labels = df['target']\nfeatures = df.drop(columns=['target'])\n\n#normalize\nfeatures = (features - features.mean()) \/ features.std()","ce80c4a1":"X_train, X_test, y_train, y_test = \\\n        train_test_split(features, labels, test_size=0.50, \n                         random_state=1, stratify=labels)\n    \nX_train_lab, X_test_unlab, y_train_lab, y_test_unlab = \\\n        train_test_split(X_train, y_train, test_size=0.50, \n                         random_state=1, stratify=y_train)","1e980d57":"# create the training dataset input\nX_train_mixed = concatenate((X_train_lab, X_test_unlab))\n\n# create \"no label\" for unlabeled data\nnolabel = [-1 for _ in range(len(y_test_unlab))]\n\n# recombine training dataset labels\ny_train_mixed = concatenate((y_train_lab, nolabel))","0a5dade0":"# define model\nmodel = LabelPropagation()\n# fit model on training dataset\nmodel.fit(X_train_mixed, y_train_mixed)","a14473bd":"# get labels for entire training dataset data\ntrain_labels = model.transduction_\n\n# define supervised learning model\nmodel2 = LogisticRegression()\n# fit supervised learning model on entire training dataset\nmodel2.fit(X_train_mixed, train_labels)","c584eab0":"# make predictions on hold out test set\nyhat = model2.predict(X_test)\n\n# calculate score for test set\nscore = accuracy_score(y_test, yhat)\n# summarize score\nprint('Accuracy: {:1.3f}%'.format(score*100))","9f13f324":"# create the training dataset input\nX_train_mixed = concatenate((X_train_lab, X_test_unlab))\n\n# create \"no label\" for unlabeled data\nnolabel = [-1 for _ in range(len(y_test_unlab))]\n\n# recombine training dataset labels\ny_train_mixed = concatenate((y_train_lab, nolabel))","1408054d":"# define model\nmodel3 = LabelSpreading()\n# fit model on training dataset\nmodel3.fit(X_train_mixed, y_train_mixed)","d2226ea9":"# get labels for entire training dataset data\ntrain_labels = model3.transduction_\n\n# define supervised learning model\nmodel4 = LogisticRegression()\n# fit supervised learning model on entire training dataset\nmodel4.fit(X_train_mixed, train_labels)","3dc09f56":"# make predictions on hold out test set\nyhat = model4.predict(X_test)\n\n# calculate score for test set\nscore = accuracy_score(y_test, yhat)\n# summarize score\nprint('Accuracy: {:1.3f}%'.format(score*100))","f2507b14":"def stack_preds(X):\n    s_X = np.vstack((model2.predict(X), model4.predict(X)))\n    return np.swapaxes(s_X, 0, 1)","d68b0e25":"def get_models():\n    models = list()\n    models.append(('lr', LogisticRegression()))\n    models.append(('knn', KNeighborsClassifier()))\n    models.append(('cart', DecisionTreeClassifier()))\n    models.append(('svm', SVC(probability=True)))\n    models.append(('bayes', GaussianNB()))\n    return models","2b949b0d":"models = get_models()\nfor name, model in models:\n    model.fit(stack_preds(X_train), y_train)\n    yhat = model.predict(stack_preds(X_test))\n    score = accuracy_score(y_test.values, yhat)\n    print('Model:{:6s} -- Accuracy: {:1.3f}%'\n              .format(name, score*100))","a4af195d":"ensemble = VotingClassifier(estimators=models, voting='hard')","2b0d7bac":"cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(ensemble, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\nprint('Voting Ensemble - Mean Accuracy: {:1.3f}% - Standard Deviation: {:1.3f}'\n          .format((n_scores).mean() * 100, (n_scores).std()))","ebfcce95":"<h1 id=\"definition\" style=\"color:#151515; background:#c4d7de; border:0.5px dotted;\"> \n    <center>Definition\n        <a class=\"anchor-link\" href=\"#Definition\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","83968d0d":"<h1 id=\"spreading\" style=\"color:#151515; background:#c4d7de; border:0.5px dotted;\"> \n    <center>Label Spreading\n        <a class=\"anchor-link\" href=\"#spreading\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","f8558fed":"![sphx_glr_plot_label_propagation_structure_0011.png](attachment:sphx_glr_plot_label_propagation_structure_0011.png)\n\n*An illustration of label-propagation: the structure of unlabeled observations is consistent with the class structure, and thus the class label can be propagated to the unlabeled observations of the training set.*\n\n\n**Label Propagation** and **Label Spreading** differ in modifications to the similarity matrix that graph and the clamping effect on the label distributions. Clamping allows the algorithm to change the weight of the true ground labeled data to some degree. The Label Propagation algorithm performs hard clamping of input labels, which means alpha = 0. This clamping factor can be relaxed, to say alpha = 0.2, which means that we will always retain 80 percent of our original label distribution, but the algorithm gets to change its confidence of the distribution within 20 percent.\n\n**Label Propagation** uses the raw similarity matrix constructed from the data with no modifications. In contrast, **Label Spreading** minimizes a loss function that has regularization properties, as such it is often more robust to noise. The algorithm iterates on a modified version of the original graph and normalizes the edge weights by computing the normalized graph Laplacian matrix. This procedure is also used in Spectral clustering.","64a144f6":"<h1 id=\"dataset\" style=\"color:#151515; background:#c4d7de; border:0.5px dotted;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","e52b27d9":"<div>\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/33180\/43520\/718447d8c4f7e29f129c25b44b324efa\/dataset-cover.jpg\"\/>\n<\/div>","d54de805":"<h1 id=\"blending\" style=\"color:#151515; background:#c4d7de; border:0.5px dotted;\"> \n    <center>Blending\n        <a class=\"anchor-link\" href=\"#blending\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","4424047c":"<h1 id=\"propagation\" style=\"color:#151515; background:#c4d7de; border:0.5px dotted;\"> \n    <center>Label Propagation\n        <a class=\"anchor-link\" href=\"#propagation\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}