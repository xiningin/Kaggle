{"cell_type":{"eddb6bc5":"code","b4fa31aa":"code","acc0da1e":"code","0bc60aeb":"code","e2133739":"code","b8ff70d0":"code","8af70438":"code","82f03e81":"code","dcbf69ee":"code","8b0fda90":"code","0e5c2bb0":"code","c1a4f471":"code","cc4635b7":"code","7c9b9f41":"code","8cb26dc0":"code","5d01d3a8":"code","31154999":"code","389c1115":"code","4b5f8fc8":"code","e343aa51":"code","90c0d7e1":"markdown","ab93c490":"markdown","1b47bfea":"markdown","7f69f29b":"markdown","4de3e202":"markdown","56aff027":"markdown","80360fa1":"markdown","f5387235":"markdown","fe0caf05":"markdown","d64f6ea6":"markdown","a1ff80d3":"markdown","fe5e9fae":"markdown","4610cc84":"markdown"},"source":{"eddb6bc5":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import fetch_lfw_people","b4fa31aa":"# Loads photos of people with a minimum number of 70 images\nlfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)","acc0da1e":"plt.imshow(lfw_people.images[6], cmap='gray')\nplt.show()","0bc60aeb":"lfw_people.images.shape","e2133739":"# Set the data format of the photos\nn_samples, h, w, = lfw_people.images.shape\nprint(n_samples)\nprint(h)\nprint(w)","b8ff70d0":"lfw_people.data.shape","8af70438":"# target lables\nlfw_people.target","82f03e81":"# target names\ntarget_names = lfw_people.target_names\ntarget_names","dcbf69ee":"n_classes = lfw_people.target_names.shape[0]","8b0fda90":"# split datas to trian and test\nx_train, x_test, y_train, y_test = train_test_split(lfw_people.data, lfw_people.target)","0e5c2bb0":"# import SVM method\nmodel = SVC(kernel='rbf', class_weight='balanced')\nmodel.fit(x_train, y_train)","c1a4f471":"predictions = model.predict(x_test)\nprint(classification_report(y_test, predictions))","cc4635b7":"lfw_people.data.shape","7c9b9f41":"# Let's go from 1850 dimensions down to 100\nn_components = 100\npca = PCA(n_components=n_components, whiten=True).fit(lfw_people.data)\n\n# transform data to pca data\nx_train_pca = pca.transform(x_train)\nx_test_pca = pca.transform(x_test)","8cb26dc0":"model = SVC(kernel='rbf', class_weight='balanced')\nmodel.fit(x_train_pca, y_train)","5d01d3a8":"predictions = model.predict(x_test_pca)\nprint(classification_report(y_test, predictions, target_names=target_names))","31154999":"param_grid = {'C': [0.1, 1, 5, 10, 100],\n              'gamma': [0.0005, 0.001, 0.005, 0.01], }\nmodel = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'),param_grid)\nmodel.fit(x_train_pca, y_train)\nprint(model.best_estimator_)","389c1115":"predictions = model.predict(x_test_pca)\nprint(classification_report(y_test, predictions, target_names=target_names))","4b5f8fc8":"def plot_gallery(images, titles, h, w, n_row=3, n_col=5):\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(titles[i], size=12)\n        plt.xticks(())\n        plt.yticks(())\n\ndef title(predictions, y_test, target_names, i):\n    pred_name = target_names[predictions[i]].split(' ')[-1]\n    true_name = target_names[y_test[i]].split(' ')[-1]\n    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name) ","e343aa51":"predictions_titles = [title(predictions, y_test, target_names, i)for i in range(len(predictions))]\nplot_gallery(x_test, predictions_titles, h, w)\nplt.show()","90c0d7e1":"## SVM(Support Vector Machines)\nIt was first published by Vladimir N. Vapnik and Alexey Ya Chervonenkis In 1963\n\nThe current version (Soft Margin) was developed by Corinna Cortes and Vapnik in 1993\nIt was introduced in 1995 and published in 1995\n\nBefore the emergence of Deep Learning (2012), SVM was considered in machine learning for nearly a decade\n\nThe most successful, the best performing algorithms","ab93c490":"## The accuracy is improved\uff0871%\u219290%\uff09","1b47bfea":"![image.png](attachment:image.png)","7f69f29b":"#### Parameter interpretation\n - Imagine that your computer is an idiot, The number of samples is 100, with 99 A categories and 1 B categories. All models are classified as A categories after learning.After training, the accuracy rate is 99%, but once the test set is passed in, the accuracy rate drops dramatically\n \n - \u2018balanced\u2019When the number of samples is 150, the computer will try its best to make the number of A and B categories approximately equal to ensure the accuracy after passing in the test set","4de3e202":"# Load data","56aff027":"#### The accuracy rate is only about 70%\n#### Let us try to improve the accuracy rate","80360fa1":"#### It seems that C=5, gamma=0.005 is an accurate parameter","f5387235":"## Adjust the parameters (GridSearchCV)\nWe need to go a step further and try to improve the accuracy\n![image.png](attachment:image.png)","fe0caf05":"#### There is a slight lift in the Recall score\n","d64f6ea6":"## Of the 15 photos that had to be predicted, only one was wrong","a1ff80d3":"- Some TIPS when you installing LWF face databese\n1. It takes a long time to your first installing \n2. An error will be reported if the installation is interrupted\n![image.png](attachment:image.png)\n\n- The Solution\n- Manually download the following content into your (scikit_learn_data\\lfw_home) folder\n\nhttps:\/\/ndownloader.figshare.com\/files\/5976018 #lfw.tgz\nhttps:\/\/ndownloader.figshare.com\/files\/5976015 #lfw-funneled.tgz\nhttps:\/\/ndownloader.figshare.com\/files\/5976012 #pairsDevTrain.txt\nhttps:\/\/ndownloader.figshare.com\/files\/5976009 #pairsDevTest.txt\nhttps:\/\/ndownloader.figshare.com\/files\/5976006 #pairs.txt","fe5e9fae":"# Drawing","4610cc84":"### PCA\uff08principal component analysis\uff09\n![image.png](attachment:image.png)\n\n### In short:\nPCA is a tool to reduce dimensionality and filter out the noise of the data to some extent"}}