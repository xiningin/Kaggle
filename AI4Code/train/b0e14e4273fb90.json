{"cell_type":{"1dadd405":"code","4bdd3524":"code","5f605055":"code","10b4c5c0":"code","7e4e6c4e":"code","9e170bc7":"code","c05afd6d":"code","7c756003":"code","d0de091e":"code","8ecbf3d3":"code","d1ca95ef":"code","a7679bd4":"code","c38daebe":"code","6afc5838":"code","e9cc16cf":"code","d29f7fd0":"code","eda2d198":"code","d4126425":"code","5e91d356":"code","7f9a2748":"code","13424921":"code","284043e6":"code","98389085":"code","b5df0da5":"code","2d6c4b0d":"code","3bdd187b":"code","268621b4":"code","020e16e8":"code","cdf5c5bd":"code","cccf494f":"code","e86b81ac":"code","fde78f45":"code","f00bd8e9":"code","cfd76118":"markdown","f5d6c01e":"markdown","d0b8966a":"markdown","d4b5e814":"markdown"},"source":{"1dadd405":"import os\nimport zipfile\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom pathlib import Path\nfrom subprocess import check_output\nimport warnings \nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\nwarnings.filterwarnings('ignore')\nprint('Setup Complet!')","4bdd3524":"plt.style.use(\n    'seaborn-whitegrid'\n)\nplt.rc(\n    'figure', \n    autolayout = True\n)\nplt.rc(\n    'axes', \n    labelweight = 'bold', \n    labelsize = 'large', \n    titleweight = 'bold', \n    titlesize = 14, \n    titlepad = 10\n)\n\nprint('Setup Complet!')","5f605055":"print(check_output([\"ls\", \"..\/input\/sberbank-russian-housing-market\"]).decode(\"utf8\"))","10b4c5c0":"dataset = 'train'\nwith zipfile.ZipFile('..\/input\/sberbank-russian-housing-market\/'+dataset+'.csv.zip','r') as z:\n    z.extractall('.')","7e4e6c4e":"print(check_output([\"ls\", \"train.csv\"]).decode(\"utf8\"))","9e170bc7":"df = pd.read_csv('.\/train.csv')\ndf.head()","c05afd6d":"df.tail()","7c756003":"df.info()","d0de091e":"df.describe()","8ecbf3d3":"print(df.shape)\nprint(df.isnull().sum().sum())","d1ca95ef":"#columns with null values;\ncol_with_nan = [col for col in df.columns if df[col].isnull().sum() > 4000]\ndf[col_with_nan].isnull().sum().sort_values(ascending=False)","a7679bd4":"corr_features = df.corr()['price_doc'].sort_values(ascending = False).head(50)\ncorr_features","c38daebe":"plt.figure(figsize = (18, 10))\nsns.heatmap(df[col_with_nan + ['price_doc']].corr());","6afc5838":"droped_col = [col for col in col_with_nan if col != 'num_room']\ndf.drop(\n    columns  = droped_col, inplace = True\n)","e9cc16cf":"print(len(droped_col))","d29f7fd0":"# we've droped 33 features, we hope so, that will be fine and usefull\n# we will rpeate the same with other null features to see some to grasp a new consept of these features;\n#columns with null values;\ncol_with_nan = [col for col in df.columns if df[col].isnull().sum() > 0]\ndf[col_with_nan].isnull().sum().sort_values(ascending=False)","eda2d198":"plt.figure(figsize = (20, 12))\nsns.heatmap(df[col_with_nan + ['price_doc']].corr(), annot = True);","d4126425":"droped_col = [col for col in col_with_nan if col != 'num_room']\ndf.drop(\n    columns  = droped_col, inplace = True\n)","5e91d356":"df.head()","7f9a2748":"df.info()","13424921":"# fifty-four columns have to dropped, that great!\n# now let's figure out non null features\n# create a list with numerical & categorical variables;\ncat_vars = [var for var in df.columns if df[var].dtypes == 'O']\nprint(cat_vars)","284043e6":"df[cat_vars]","98389085":"## we don't need the id and times\ndf['id'].nunique() == df.shape[0]\ndf.drop(\n    columns = ['id', 'timestamp'], \n    inplace = True\n)","b5df0da5":"df.sub_area.unique()","2d6c4b0d":"df['sub_area'] = df['sub_area'].str.strip()\ndf['sub_area'] = df['sub_area'].str.lower()\ndf['sub_area'].unique()","3bdd187b":"df.product_type.unique()","268621b4":"df.ecology.unique()","020e16e8":"cat_vars = [var for var in df.columns if df[var].dtypes == 'O']","cdf5c5bd":"def check_unique_values(var):\n    if var not in ['ecology', 'product_type', 'sub_area']:\n        uniques =  df[var].unique()\n        print(uniques)\nfor var in cat_vars:\n    check_unique_values(var)","cccf494f":"#that's cool for inconsisten data entry;\n#now let's go into deep;\n# for numerical variables\nnum_vars = [var for var in df.columns if df[var].dtypes != 'O']\nlen(num_vars)\n#that's huge amount of numerical data \ud83e\udd76;","e86b81ac":"# Nominal variables;\nnom_fea = ['sub_area']\ndf[nom_fea] = df[nom_fea].astype('category')\n# Ordical variables;\norderd_levels = {\n    'ecology':['not data', 'poor', 'satisfactory', 'good', 'excellent'], \n    'product_type': ['Investment', 'OwnerOccupier'], \n    'culture_objects_top_25':['no' 'yes'], \n    'thermal_power_plant_raion':['no' 'yes'], \n    'incineration_raion':['no' 'yes'], \n    'oil_chemistry_raion':['no' 'yes'], \n    'radiation_raion':['no', 'yes'], \n    'railroad_terminal_raion':['no' 'yes'], \n    'big_market_raion':['no' 'yes'], \n    'nuclear_reactor_raion':['no' 'yes'], \n    'detention_facility_raion':['no' 'yes'], \n    'water_1line':['no' 'yes'], \n    'big_road1_1line':['no' 'yes'], \n    'railroad_1line':['no' 'yes']\n}\norderd_levels = {key: ['None'] + value for key, value in orderd_levels.items()}\n# encoding processing\ndef encode(df):\n    for name, levels in orderd_levels.items():\n        \n        df[name] = df[name].astype(CategoricalDtype(levels, ordered = True))\n    return df\ndf = encode(df)","fde78f45":"for name in df.select_dtypes('number'):\n    df[name] = df[name].fillna(0)","f00bd8e9":"# start;\nX = df.copy()\ny = X.pop('price_doc')\nmodel = XGBRegressor()\n#hot encoding;\nfor col in X.select_dtypes('category'):\n    X[col] = X[col].cat.codes\nlog_y = np.log(y)\nlog_y\nscore = cross_val_score(\n    model, X, log_y, cv = 5, scoring = 'neg_mean_squared_error'\n)\nscore = -1 * score.mean()\nbaseline_score = np.sqrt(score)\nprint(f'Baseline score: {baseline_score:.5f} RMSLE')","cfd76118":"# <center>Comming Soon..........<\/center>","f5d6c01e":"# <center>Data Cleaning<\/center>","d0b8966a":"# <center>Import libraries<\/center>","d4b5e814":"# <center>Evaluate Dataset, RMSLE<\/center>"}}