{"cell_type":{"25460195":"code","7ef81f1c":"code","690c5086":"code","66ff02cc":"code","a5889cab":"code","a9bb5c58":"code","9e4f1f85":"code","8198d1be":"code","28ac9c79":"code","8abdba19":"code","76f55d45":"code","b241f014":"code","dee097ff":"code","c62ca7a8":"code","f9b44c67":"code","2effa073":"code","e9d1fa84":"code","c5a955a9":"code","9bb595d2":"code","42dfaa21":"code","5d65fa3f":"code","2a1a52f5":"code","5e424342":"code","79f882c8":"code","66b9cdd4":"code","9dade7c7":"code","67b2bee9":"code","387201ea":"code","eac14a08":"code","1d31dc3c":"code","e7de7ce8":"code","9b5f1f00":"code","be3475e9":"code","64e3dfca":"code","ea53ab4f":"code","7d4965c3":"code","e07ea9b5":"code","54b0af82":"code","00d6f131":"code","1a13d199":"code","c925fabc":"code","a7e6694c":"code","ab352061":"code","66e10a98":"code","15f5e575":"markdown","6220878d":"markdown","b30d1a5b":"markdown","acc7e771":"markdown","96b63a6b":"markdown","d72a025c":"markdown","746e8d25":"markdown","094ce3c7":"markdown","cb81cd28":"markdown","c658cb36":"markdown","4f2d9e42":"markdown","f4b71184":"markdown","05e282cb":"markdown","8fa1c658":"markdown","31af9c64":"markdown","08093dff":"markdown","922dc473":"markdown","1b1fb18d":"markdown","fbfc2ef8":"markdown","13edaafc":"markdown","f07a2613":"markdown","fe2f5cfd":"markdown","a3375837":"markdown","95ec5844":"markdown","f29a3918":"markdown","3dc00542":"markdown","b2ddf9ab":"markdown"},"source":{"25460195":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7ef81f1c":"# Install the additional packages required to run the notebook\n\n!pip install mahotas\n!pip install pypng","690c5086":"# Import all the necessary library and pacgakes\n\nimport os,png,array\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nfrom sklearn import svm\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\n\nimport gc\nimport warnings\n\nfrom PIL import Image\nfrom skimage import feature\n\nimport cv2\nimport mahotas\nimport _pickle as cPickle","66ff02cc":"# Initialize notebook parameters\n\n# To display all the columns\npd.options.display.max_columns = None\n\n# To display all the rows\npd.options.display.max_rows = None\n\n# To map Empty Strings or numpy.inf as Na Values\npd.options.mode.use_inf_as_na = True\n\n# Set Precision to 8 for better readability\npd.set_option('precision', 8)\npd.options.display.float_format = '{:.4f}'.format\n\npd.options.display.expand_frame_repr =  False\n\n%matplotlib inline\n\n# Set Style\nsns.set(style = \"whitegrid\")\n\n# Ignore Warnings\nwarnings.filterwarnings('ignore')","a5889cab":"# Variable declaration\nHOG_feature_data = []\ntest_data = False","a9bb5c58":"# import time\n\n# os.chdir('C:\/Hackathons\/Digits_Identification-AnalyticsVidya\/Train_UQcUa52\/Images\/train')\n\n# Creating a list of columns to represent the pixel format for a particular image.\n\n# columnNames = list()\n# for i in range(784):\n#     pixel = 'pixel'\n#     pixel += str(i)\n#     columnNames.append(pixel)\n\n# pixel_data = pd.DataFrame(columns = columnNames)\n# start_time = time.time()\n# for i in range(1,49000):\n#     img_name = str(i)+'.png'\n#     img = Image.open(img_name)\n#     rawData = img.load()\n#     data = []\n#     for y in range(28):\n#         for x in range(28):\n#             data.append(rawData[x,y][0])\n#     k = 0\n#     pixel_data.loc[i] = [data[k] for k in range(784)]\n\n# The below 6 lines of code does not need to be executed for test image set as we do not want the label column.\n# **************************************************************************************************************\n# label_data = pd.read_csv(\"train.csv\")\n# train_labels = label_data['label']\n# pixel_data = pd.concat([label_data, pixel_data],axis = 1)\n# pixel_data = pixel_data.drop('filename',1)\n# pixel_data = pixel_data.dropna()\n# **************************************************************************************************************\n\n# Convert the pixel dataset into a csv file.\n# pixel_data.to_csv(\"image_pixel_data.csv\",index = False)\n\n# Change the directory to the original directory. This could change depending on the directory level.\n# os.chdir('..\/..\/..\/')\n# print(\"Conversion completed in  - \", time.time()-start_time)","9e4f1f85":"def getdigits(dfPath):\n    \n    if test_data == True:\n        # build the dataset. We do not have the label column in case of test data. \n        # Hence we would not get the target\/labels and return 0 for the same.\n        data = np.genfromtxt(dfPath, delimiter = \",\", dtype = \"uint8\")\n        data = data[:, 0:].reshape(data.shape[0], 28, 28)\n        # return a tuple of the data and targets\n        return (data, 0)\n    else:\n        # build the dataset and then split it into data and labels (target)\n        data = np.genfromtxt(dfPath, delimiter = \",\", dtype = \"uint8\")\n        target = data[:, 0]\n        data = data[:, 1:].reshape(data.shape[0], 28, 28)\n        # return a tuple of the data and targets\n        return (data, target)","8198d1be":"test_data = False\n(imgdata,label) = getdigits('\/kaggle\/input\/digit-recognizer\/train.csv')\n\n%matplotlib inline\n\n\nfor i in range(1, 21):\n  plt.subplot(4,5,i)\n  plt.tight_layout()\n  plt.imshow(imgdata[i], cmap='gray', interpolation='none')\n  plt.title(\"Digit: {}\".format(label[i]))\n  plt.xticks([])\n  plt.yticks([])\nplt.show()","28ac9c79":"digits_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ndigits_df.head()","8abdba19":"digit_one = digits_df.iloc[0, 1:]\ndigit_one.shape","76f55d45":"digit_one = digit_one.values.reshape(28, 28)\nplt.imshow(digit_one, cmap='gray')","b241f014":"print(digit_one[5:-5, 5:-5])","dee097ff":"def deskewImage(image, width):\n    # grab the width and height of the image and compute\n    # moments for the image\n    (h, w) = image.shape[:2]\n    moments = cv2.moments(image)\n    \n    # deskew the image by applying an affine transformation\n    skew = moments[\"mu11\"] \/ moments[\"mu02\"]\n    M = np.float32([\n        [1, skew, -0.5 * w * skew],\n        [0, 1, 0]])\n    image = cv2.warpAffine(image, M, (w, h),\n        flags = cv2.WARP_INVERSE_MAP | cv2.INTER_LINEAR)\n\n    # resize the image to have a constant width\n    image = cv2.resize(image, (28,28))\n    \n    # return the deskewed image\n    return image","c62ca7a8":"deSkewedImage = deskewImage(imgdata[8],28)\n\nplt.subplot(1,2,1)\nplt.tight_layout()\nplt.imshow(imgdata[8], cmap='gray', interpolation='none')\nplt.title(\"Skewed Image\")\n\nplt.subplot(1,2,2)\nplt.tight_layout()\nplt.imshow(deSkewedImage, cmap='gray', interpolation='none')\nplt.title(\"De-Skewed Image\")\n\nplt.show()","f9b44c67":"def centre_align_image(image, size):\n    # grab the extent width and height\n    (eW, eH) = size\n    \n    #Image Shape is\n    (h, w) = image.shape[:2]\n    \n    #New dimension according to image aspect ratio\n    dim = None\n    \n    # handle when the width is greater than the height\n    if image.shape[1] > image.shape[0]:\n        #image = resize(image, width = eW)\n        r = eW \/ float(w)\n        dim = (eW, int(h * r))\n        image = cv2.resize(image,dim,cv2.INTER_AREA)\n\n    # otherwise, the height is greater than the width\n    else:\n        #image = resize(image, height = eH)\n        r = eH \/ float(h)\n        dim = (int(w * r), eH)\n        image = cv2.resize(image,dim,cv2.INTER_AREA)\n\n    # allocate memory for the extent of the image and grab it\n    extent = np.zeros((eH, eW), dtype = \"uint8\")\n    offsetX = (eW - image.shape[1]) \/ 2\n    offsetY = (eH - image.shape[0]) \/ 2\n    offsetX = int(offsetX)\n    offsetY = int(offsetY)\n    extent[offsetY:offsetY + image.shape[0], offsetX:offsetX + image.shape[1]] = image\n\n    # compute the center of mass of the image and then\n    # move the center of mass to the center of the image\n    (cY, cX) = np.round(mahotas.center_of_mass(extent)).astype(\"int32\")\n    (dX, dY) = ((size[0] \/ 2) - cX, (size[1] \/ 2) - cY)\n    M = np.float32([[1, 0, dX], [0, 1, dY]])\n    extent = cv2.warpAffine(extent, M, size)\n\n    # return the extent of the image\n    return extent","2effa073":"# An example transformation on a de-centered image.\n\nimg = cv2.imread('\/kaggle\/input\/digit-recognizer-images\/Decenter.png')\nimg = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\naligned_image = centre_align_image(img,(28,28))\n\nplt.subplot(1,2,1)\nplt.tight_layout()\nplt.imshow(img, cmap='gray', interpolation='none')\nplt.title(\"De-Centered Image\")\n\nplt.subplot(1,2,2)\nplt.tight_layout()\nplt.imshow(aligned_image, cmap='gray', interpolation='none')\nplt.title(\"Centered Image\")\n\nplt.show()","e9d1fa84":"def extract_HOG_features(image):\n    hist = feature.hog(image, orientations = 9,\n        pixels_per_cell = (8, 8),\n        cells_per_block = (3, 3)\n        )\n    return hist","c5a955a9":"hist = extract_HOG_features(imgdata[0])\n\nprint(np.shape(hist))","9bb595d2":"# This method transforms an image to a proper format so as to increase the performance of the classifier.\ndef processImages(filePath):\n    image_size = 28\n    \n    (digits, target) = getdigits(filePath)\n\n    # loop over the images\n    for image in digits:\n        # deskew the image, center it\n        image = deskewImage(image, image_size)\n        image = centre_align_image(image, (image_size, image_size))\n    \n        # describe the image and update the data matrix\n        hist = extract_HOG_features(image)\n        HOG_feature_data.append(hist)","42dfaa21":"#call the above function tp apply the transformations.\ntest_data = False\nprocessImages('\/kaggle\/input\/digit-recognizer\/train.csv')","5d65fa3f":"print(\"Total images processed - \", len(HOG_feature_data))\nprint(\"HOG features extracted per image - \", len(HOG_feature_data[0]))","2a1a52f5":"# Let's check the total number of observations present for each digit.\n# Our label field represents numbers (int) from 1 to 9. But the label column is actually a categorical\n# column here. Let's convert 'label' field as categorical for a smoother and correct analysis.\n\ndigits_df.label = digits_df.label.astype('category')\n\ndigits_df.label.value_counts()","5e424342":"# Summarise count in terms of percentage \n(round(digits_df.label.value_counts()\/len(digits_df.index), 4))*100","79f882c8":"# average values\/distributions of features\ndescription = digits_df.describe()\ndescription","66b9cdd4":"# Creating training and test sets\n# Splitting the data into train and test\nX = digits_df.iloc[:, 1:]\ny = digits_df.iloc[:, 0]\n\n# Rescaling the features\nX = scale(X)\n\n# train test split with train_size=10% and test size=90%\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size=0.2, random_state=101)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n","9dade7c7":"linear_SVM = svm.SVC(kernel='linear')\n\n# fit\nlinear_SVM.fit(X_train, y_train)","67b2bee9":"# predict\npredictions = linear_SVM.predict(X_test)\npredictions[:10]","387201ea":"confusion_matrix = metrics.confusion_matrix(y_true = y_test, y_pred = predictions)\nconfusion_matrix","eac14a08":"metrics.accuracy_score(y_true=y_test, y_pred=predictions)","1d31dc3c":"# class-wise accuracy\nclass_wise = metrics.classification_report(y_true=y_test, y_pred=predictions)\nprint(class_wise)","e7de7ce8":"# run gc.collect() (garbage collect) to free up memory\n# else, since the dataset is large and SVM is computationally heavy,\n# it'll throw a memory error while training\ngc.collect()","9b5f1f00":"# rbf kernel with other hyperparameters kept to default \nsvm_rbf = svm.SVC(kernel='rbf')\nsvm_rbf.fit(X_train, y_train)","be3475e9":"# predict\npredictions = svm_rbf.predict(X_test)\npredictions[:10]","64e3dfca":"# accuracy \nprint(metrics.accuracy_score(y_true=y_test, y_pred=predictions))","ea53ab4f":"# conduct (grid search) cross-validation to find the optimal values \n# of cost C and the choice of kernel\n\n# parameters = {'C':[1, 10, 100], \n#              'gamma': [1e-2, 1e-3, 1e-4]}\n\n# # instantiate a model \n# svc_grid_search = svm.SVC(kernel=\"rbf\")\n\n# # create a classifier to perform grid search\n# gs_cv = GridSearchCV(svc_grid_search, param_grid=parameters, scoring='accuracy')\n\n# # fit\n# gs_cv.fit(X_train, y_train)","7d4965c3":"# results\n# gs_cv_results = pd.DataFrame(gs_cv.cv_results_)\n# gs_cv_results","e07ea9b5":"# converting C to numeric type for plotting on x-axis\n# gs_cv_results['param_C'] = gs_cv_results['param_C'].astype('int')\n\n# # # plotting\n# plt.figure(figsize=(16,6))\n\n# # subplot 1\/3\n# plt.subplot(1,3,1)\n# gamma_01 = gs_cv_results[gs_cv_results['param_gamma'] == 0.01]\n\n# plt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_test_score\"])\n# plt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_train_score\"])\n# plt.xlabel('C')\n# plt.ylabel('Accuracy')\n# plt.title(\"Gamma=0.01\")\n# plt.ylim([0.60, 1])\n# plt.legend(['test accuracy', 'train accuracy'], loc='lower right')\n# plt.xscale('log')\n\n# # subplot 2\/3\n# plt.subplot(1,3,2)\n# gamma_001 = gs_cv_results[gs_cv_results['param_gamma'] == 0.001]\n\n# plt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_test_score\"])\n# plt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_train_score\"])\n# plt.xlabel('C')\n# plt.ylabel('Accuracy')\n# plt.title(\"Gamma=0.001\")\n# plt.ylim([0.60, 1])\n# plt.legend(['test accuracy', 'train accuracy'], loc='lower right')\n# plt.xscale('log')\n\n\n# # subplot 3\/3\n# plt.subplot(1,3,3)\n# gamma_0001 = gs_cv_results[gs_cv_results['param_gamma'] == 0.0001]\n\n# plt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_test_score\"])\n# plt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_train_score\"])\n# plt.xlabel('C')\n# plt.ylabel('Accuracy')\n# plt.title(\"Gamma=0.0001\")\n# plt.ylim([0.60, 1])\n# plt.legend(['test accuracy', 'train accuracy'], loc='lower right')\n# plt.xscale('log')\n\n# plt.show()","54b0af82":"svm_poly = svm.SVC(kernel='poly', C=100, gamma='auto', degree=3, coef0=1, decision_function_shape='ovo')\n\nsvm_poly.fit(X_train, y_train)","00d6f131":"poly_predictions = svm_poly.predict(X_test)\npoly_predictions[:10]","1a13d199":"# evaluation: CM \npoly_confusion = metrics.confusion_matrix(y_true = y_test, y_pred = poly_predictions)\n\n# measure accuracy\npoly_test_accuracy = metrics.accuracy_score(y_true = y_test, y_pred = poly_predictions)\n\nprint(\"Accuracy - \", poly_test_accuracy, \"\\n\")\nprint(poly_confusion)","c925fabc":"test_data = True\nprocessImages('\/kaggle\/input\/digit-recognizer\/test.csv')","a7e6694c":"processed_test_set_df_kaggle = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\n# Rescaling the features\nprocessed_test_set_df_kaggle = scale(processed_test_set_df_kaggle)","ab352061":"# final predict Kaggle\nfinal_predictions_kaggle = svm_poly.predict(processed_test_set_df_kaggle)\nfinal_predictions_kaggle[:10]","66e10a98":"image_ids = []\nfor i in range(1, 28001):\n    image_ids.append(i)\n\nsubmission_file_df = pd.DataFrame({'ImageId': image_ids, 'Label': final_predictions_kaggle})\n\nfilename = 'submission_Kaggle.csv'\n\nsubmission_file_df.to_csv(filename, index = False, sep = \",\")","15f5e575":"## Non-Linear SVM\n\n## RBF Kernel","6220878d":"#### Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).","b30d1a5b":"![image.png](attachment:image.png)","acc7e771":"### We built models with LinearSVC, rbf kernel and polynomial kernel. We tuned GridSearchCV with hyperparameters {'C':[1, 10, 100], 'gamma': [1e-2, 1e-3, 1e-4]}.\n\n### From the predictions and accuracy score of all the above models, the model with polynomial kernel turns out to be the best one.\n\n\n### Hence the model with Polynomial Kernel will be our Final Model","96b63a6b":"# Histogram of Oriented Gradients (HOG) features -\n\n1. Histogram\n2. Oriented\n3. Gradients\n\n**Histogram** is nothing but a frequncy map which shows how many times a random variable appears in the context.\n**Orientation** is directly associated with angles.\n**Gradients** signifies transitions of a random variable.\n\n**HOG shows us a frequency map of edges (Gradients) in different orientations of an Image.\n\n#### Lets define a method for extracting the HOG features. ","d72a025c":"# Part 3 - Exploratory Data Analysis","746e8d25":"The accuracy achieved with a non-linear kernel is slightly higher than a linear one. Let's now do a grid search CV to tune the hyperparameters C and gamma.\n\n## Grid Search Cross-Validation\n\n#### We will not be running the GridSearchCV for hypertuning the parameters. Kaggle kernel throws an error due to over consumption of memory and the execution stops with error - \"Failed. Exited with code 137\" (https:\/\/www.kaggle.com\/c\/talkingdata-adtracking-fraud-detection\/discussion\/53972)\n\n#### It is a memory issue as the train dataset is on the large side. The Kaggle kernel limits us to 16Gb and I do not think that it sufficient to process the entire data set for the GridSearchCV.\n\n**I ran the GridSearchCV on my system and the results are summarized in the markdown cell below the 3 commented out cells.**","094ce3c7":"# Part 4 - Data Preparation for Model Building","cb81cd28":"# Part 1 - Data Import\n\n#### The below script will help converting the \".png\" images into pixel format and write them into a csv file. \n#### The generated csv file will be fed into for our model building.\n\n#### The script used methods from the image module in python's PIL package. We will be using the below methods in the script.\n\n#### PIL.Image.open(fp, mode='r') : \n    Opens and identifies the given image file.\n    This is a lazy operation; this function identifies the file, but the file remains open and the actual image data is \n    not read from the file until you try to process the data (or call the load() method).\n    \n#### Image.load() : \n    Allocates storage for the image and loads the pixel data.\n\n#### We will repeat the above 2 methods for the entire set of 49000 images and append the pixel data for each image to an array. This  rawData will then be concateneted with the respective image label provided in the \"train.csv\" file. The resultant dataset will be written to a csv file which will be out input\/train data for the model.\n\n#### ***Note that the script will take time to run.\n","c658cb36":"# We will be using a SVM model for this digit recognition analysis.\n\n#### Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n\n#### We need to convert these images into actual pixels format so that they can be fed into the model. We will leverage python's PIL (Python Image Library) package to help convert these images.\n\n#### The converted image dataset would consist of pair, \u201chandwritten digit image\u201d and \u201clabel\u201d. Digit ranges from 0 to 9, meaning 10 patterns in total.\n\n    1. handwritten digit image: This is gray scale image with size 28 x 28 pixel.\n    2. label : This is actual digit number this handwritten digit image represents. It is either  0 to 9.","4f2d9e42":"# Part 5 - Model Building\n\n## Linear SVM","f4b71184":"#### Now lets try to re-centre an image.\n\n#### Parameters user -\n\n- INTER_AREA - resampling using pixel area relation. It may be a preferred method for image decimation, as it gives moire\u2019-free results.\n\n- **Mahotas** - A computer vision and image processing library for Python.\n\n- **Affine transformation** - A linear mapping method that preserves points, straight lines, and planes.","05e282cb":"# Polynomial Kernel","8fa1c658":"As we see ach digit has around 9%-11% share in the dataset and we can say that the **dataset is balanced**. \nThis is an important factor in considering the choices of models to be used, especially SVM, since **SVMs usually do not perform well on imbalanced data**.","31af9c64":"#### CV2\/OpenCV -\n    OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. \n    OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of \n    machine perception in the commercial products.\n    \n#### Image Moments - \n    Image moment is a certain particular weighted average (moment) of the image pixels' intensities.  \n    Image moments are useful to describe objects.  Simple properties of the image which are found via \n    image moments include area (or total intensity), its centroid, and information about its orientation.\n    \n#### Affine Transformations - \n\n    Affine transformation is a linear mapping method that preserves points, straight lines, and planes. Sets of parallel\n    lines remain parallel after an affine transformation. The affine transformation technique is typically used to \n    correct for geometric distortions or deformations that occur with non-ideal camera angles.\n    \n**More information on OpenCV methods can be found at** - https:\/\/www.analyticsvidhya.com\/blog\/2019\/03\/opencv-functions-computer-vision-python\/\n    \n#### Lets have a look at how the above function de-skews an image.","08093dff":"#### As we have the pixels represented in the column format, inorder to visualize a particular image, we need to represent these pixels in a matrix form. All our images are of size (28 x 28). So the entire 784 columns need to be reshaped to a (28 x 28) matrix form.","922dc473":"#### This is how our images looked like in the image collection. So the conversion is correct. Now lets do some more analysis on the pixel format for an image.","1b1fb18d":"# Part 6 - Final predictions & writing results into the submission file","fbfc2ef8":"#### Lets get the HOG features for a particular image","13edaafc":"## An image in pixel format looks as below -\n\n![image.png](attachment:image.png)\n\n\n#### Our final data set will consist of such pixel format entries spread across 784 columns for all the images.","f07a2613":"#### Lets also visualize the pixel (28 x 28) array.","fe2f5cfd":"#### So a total of 81 features were extracted from the image. It is roughly 9 histograms from an image with 9 bins\n\n#### Lets now apply the above transformation & feature extraction methods to our entire data set","a3375837":"#### Now with this we are done with the image or data preparation phase.","95ec5844":"#### Before starting to implement a classifier we need to do some pre-processing with the images so that we can make a efficient model. We will follow the below 2 procedures as part of image pre-processing. \n\n1. **Deskew Image** \n\n    - The process of straightening an image that has been scanned or photographed crookedly \u2014 that is an image that is \n     slanting too far in one direction, or one that is misaligned. \n\n    - Aligning digits before building a classifier similarly produces superior results. In the case of faces, aligment is \n    rather obvious \u2014 you can apply a similarity transformation to an image of a face to align the two corners of the eyes\n    to the two corners of a reference face.\n\n    - In the case of handwritten digits, we do not have obvious features like the corners of the eyes we can use for alignment. However, an obvious variation in writing among people is the slant of their writing. Some writers have a right or forward slant where the digits are slanted forward, some have a backward or left slant, and some have no slant at all. We can help the algorithm quite a bit by fixing this vertical slant so it does not have to learn this variation of the digits.\n\n    \n2. **Re-Center Image** - Cemtre aling the image (or the image object) \n\n\n**Ref** - \n\n1. [handwritten-digits-classification-an-opencv-c-python-tutorial](https:\/\/www.learnopencv.com\/handwritten-digits-classification-an-opencv-c-python-tutorial\/)\n    \n2. [image-processing-for-text-recognition](http:\/\/blog.mathocr.com\/2017\/06\/25\/image-processing-for-text-recognition.html)\n","f29a3918":"# Part 2 - Image Preprocessing","3dc00542":"#### You can see that the max value of the mean and maximum values of some features (pixels) is 139, 255 etc., while most features lie in much lower ranges.\n\n#### Thus, rescale of the features would be required for building a better model.","b2ddf9ab":"* From the plot above (Generated from the GridSearchCV execution from my system), we observe \n\n- At very high gamma (0.01), the model is achieving 100% accuracy on the training data, though the test score is quite low (<75%). Thus, the model is overfitting.\n\n- At gamma=0.001, the training and test scores are comparable at around C=1, though the model starts to overfit at higher values of C\n\n- At gamma=0.0001, the model does not overfit till C=10 but starts showing signs at C=100. Also, the training and test scores are slightly lower than at gamma=0.001.\n\nThus, it seems that the best combination is gamma=0.001 and C=1 (the plot in the middle), which gives the highest test accuracy while avoiding overfitting.\n\nSo based on GridSearchCV, our best choice of hyperparameters would be - \n- best_C = 1\n- best_gamma = 0.001"}}