{"cell_type":{"2a44eaaa":"code","fd931f40":"code","bc61f13d":"code","a4736f6c":"code","9e475f1f":"code","74dbc113":"code","e1849191":"code","6a085e6e":"code","4aeb777e":"code","90cd8aae":"code","55cfcd28":"code","0b73c148":"code","2b0173a1":"code","e5f13884":"code","4ac34aff":"code","d9c9aac0":"code","80f35e90":"code","d70162fc":"code","c6602e14":"code","c9fed3c0":"code","f625ee24":"code","ccddbb04":"code","b2533ff8":"code","f36044a7":"code","b2ca89c0":"code","fac8868b":"code","7e5a5b88":"code","9d610c09":"code","b64e4f76":"code","d66ac5ad":"code","1475f77d":"code","4b76cfc5":"code","5a7d5885":"code","08b07b83":"code","8da8f1a5":"code","557eaf66":"code","0ddde190":"code","1736a09a":"code","eb3d87c4":"code","c8b1898a":"code","4fabc7c4":"code","6a49f7e8":"code","c5a05d80":"code","76d762cf":"code","82a8574b":"code","c11697f2":"code","57d3c6a7":"code","73f84b8e":"code","4ca0c99a":"code","b4c8c0ab":"code","2be1f27c":"code","503ae063":"markdown","736c1b1a":"markdown","8c955098":"markdown","52723222":"markdown"},"source":{"2a44eaaa":"import copy\nimport os\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm.notebook import tqdm","fd931f40":"os.listdir(\"..\/input\")","bc61f13d":"main_data_dir = \"..\/input\/covid19-global-forecasting-week-4\"\nmetadata_dir = \"..\/input\/covid19-countrywise-metadata\"","a4736f6c":"data_train = pd.read_csv(os.path.join(main_data_dir, \"train.csv\"))\ndata_test = pd.read_csv(os.path.join(main_data_dir, \"test.csv\"))\nsample_sub = pd.read_csv(os.path.join(main_data_dir, \"submission.csv\"))\nmetadata = pd.read_csv(os.path.join(metadata_dir, \"covid19_countrywise_metadata.csv\"))","9e475f1f":"def generate_loc(x):\n    if isinstance(x[\"Province_State\"], float):\n        return x[\"Country_Region\"]\n    else:\n        return \"_\".join([x[\"Country_Region\"], x[\"Province_State\"]])","74dbc113":"def generate_loc_date(x):\n    return \"_\".join([x[\"location\"], x[\"Date\"]])","e1849191":"data_train[\"location\"] = data_train.apply(generate_loc, axis=1)\ndata_test[\"location\"] = data_test.apply(generate_loc, axis=1)","6a085e6e":"data_train[\"log_cfm\"] = data_train[\"ConfirmedCases\"].map(np.log1p)\ndata_train[\"log_ftl\"] = data_train[\"Fatalities\"].map(np.log1p)","4aeb777e":"data_train[\"loc_date\"] = data_train.apply(generate_loc_date, axis=1)\ndata_test[\"loc_date\"] = data_test.apply(generate_loc_date, axis=1)","90cd8aae":"data_train_by_loc = data_train.groupby(\"location\")","55cfcd28":"data_train[\"Date\"] = pd.to_datetime(data_train[\"Date\"])\ndata_test[\"Date\"] = pd.to_datetime(data_test[\"Date\"])","0b73c148":"data_train[\"rel_date\"] = data_train.apply(lambda x: (x[\"Date\"] - data_train[\"Date\"].min()), axis=1).dt.days\ndata_test[\"rel_date\"] = data_test.apply(lambda x: (x[\"Date\"] - data_train[\"Date\"].min()), axis=1).dt.days","2b0173a1":"data_train[\"rel_date_pct\"] = data_train.rel_date \/ data_test.rel_date.max()\ndata_test[\"rel_date_pct\"] = data_test.rel_date \/ data_test.rel_date.max()","e5f13884":"def get_day_zero(df):\n    progress = pd.pivot_table(data_train[[\"location\", \"ConfirmedCases\", \"rel_date\"]],\n        values=\"ConfirmedCases\", index=\"location\", columns=\"rel_date\")\n    day_zero = np.argmax((progress.values > 0).cumsum(axis=1) == 1, axis=1)\n    day_zero_location = {progress.index[i]: day_zero[i] for i in range(len(progress))}\n    \n    return day_zero_location","4ac34aff":"def get_since_day_zero(row, day_zero):\n    cur_date = row[\"rel_date\"]\n    cur_loc = row[\"location\"]\n    cur_day_zero = day_zero[cur_loc]\n    return max(cur_date - cur_day_zero, -1)","d9c9aac0":"day_zero_location = get_day_zero(data_train)\ndata_train[\"since_day_zero\"] = data_train.apply(get_since_day_zero, axis=1, day_zero=day_zero_location)\ndata_test[\"since_day_zero\"] = data_test.apply(get_since_day_zero, axis=1, day_zero=day_zero_location)","80f35e90":"data_train[\"since_day_zero\"] = data_train[\"since_day_zero\"].map(lambda x: -0.1 if x < 0 else x \/ data_test.since_day_zero.max())\ndata_test[\"since_day_zero\"] = data_test[\"since_day_zero\"].map(lambda x: -0.1 if x < 0 else x \/ data_test.since_day_zero.max())","d70162fc":"lockdown_cols = [\"quarantine\", \"close_school\", \"close_public_place\", \"limit_gathering\", \"stay_home\"]\nmetadata[lockdown_cols] = metadata[lockdown_cols].fillna(\"2099-12-31\")\nfor col in lockdown_cols:\n    metadata[col] = pd.to_datetime(metadata[col], format=\"%Y-%m-%d\")","c6602e14":"def add_lockdown_data(df, metadata):\n    lockdown_cols = [\"quarantine\", \"close_school\", \"close_public_place\", \"limit_gathering\", \"stay_home\"]\n    lockdown_df = pd.DataFrame(np.zeros((len(df), len(lockdown_cols))), columns=lockdown_cols)\n    for i in range(len(df)):\n        cur_date = df.Date[i]\n        cur_loc = df.location[i]\n        idx = metadata.loc[metadata.location == cur_loc, :].index.values[0]\n        lockdown_df.loc[i, :] = (metadata.loc[idx, lockdown_cols] <= cur_date).astype(int)\n    \n    return pd.concat([df, lockdown_df], axis=1)","c9fed3c0":"data_train = add_lockdown_data(data_train, metadata)\ndata_test = add_lockdown_data(data_test, metadata)","f625ee24":"metadata.drop(lockdown_cols, axis=1, inplace=True)","ccddbb04":"def min_max_normalize(data):\n    scaler = MinMaxScaler()\n    return scaler.fit_transform(data)","b2533ff8":"num_cols = metadata.columns[3:]\nmetadata[num_cols] = min_max_normalize(metadata[num_cols])","f36044a7":"metadata.drop([\"region\", \"country\"], axis=1, inplace=True)","b2ca89c0":"metadata.set_index(\"location\", inplace=True)","fac8868b":"data_test[\"log_cfm\"] = 0\ndata_test[\"log_ftl\"] = 0","7e5a5b88":"data_test_known = data_test.loc[data_test.rel_date <= data_train.rel_date.max(), :].sort_values([\"rel_date\", \"location\"]).reset_index(drop=True)\ndata_test_unknown = data_test.loc[data_test.rel_date > data_train.rel_date.max(), :].sort_values([\"rel_date\", \"location\"]).reset_index(drop=True)\ndata_train.sort_values([\"rel_date\", \"location\"], inplace=True)","9d610c09":"data_test_known[[\"log_cfm\", \"log_ftl\"]] = data_train.loc[data_train.rel_date >= data_test.rel_date.min(), [\"log_cfm\", \"log_ftl\"]].values","b64e4f76":"data_test_input = pd.concat([data_test_known, data_test_unknown], axis=0)\ninput_len = 30\ndata_test_input = pd.concat([data_train.loc[(data_train.rel_date >= data_test_unknown.rel_date.min() - input_len)\\\n    & (data_train.rel_date < data_test_known.rel_date.min()), data_test_input.columns],\n    data_test_input], axis=0).sort_values([\"rel_date\", \"location\"]).reset_index(drop=True)","d66ac5ad":"class TrainDataset(Dataset):\n    \n    def __init__(self, df, metadata, x_cols, y_cols, input_len=30):\n        self.df = df\n        self.metadata = metadata\n        self.x_cols = x_cols\n        self.y_cols = y_cols\n        self.input_len = input_len\n        \n        self.loc_list = list(self.df.location.unique())\n        self.num_pos_train_period = self.df.rel_date.max() - self.input_len + 1\n    \n    def __len__(self):\n        return len(self.loc_list) * self.num_pos_train_period\n    \n    def __getitem__(self, idx):\n        cur_loc = self.loc_list[idx \/\/ self.num_pos_train_period]\n        input_beg = idx % self.num_pos_train_period\n        input_end = input_beg + self.input_len\n        in_input_period = (self.df.rel_date >= input_beg) & (self.df.rel_date < input_end)\n        is_cur_loc = self.df.location == cur_loc\n        inputs = self.df.loc[(is_cur_loc) & (in_input_period), self.x_cols].values\n        meta_inputs = self.metadata.loc[cur_loc, :].values\n        targets = self.df.loc[(is_cur_loc) & (self.df.rel_date == input_end), self.y_cols].values.reshape((-1))\n        \n        return torch.tensor(inputs, dtype=torch.float32),\\\n            torch.tensor(meta_inputs, dtype=torch.float32),\\\n            torch.tensor(targets, dtype=torch.float32)\n    \n    @staticmethod\n    def get_dataloader(dataset, batch_size, shuffle=True):\n        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)","1475f77d":"rand_seed = 42\nloc_list = data_train.location.unique()\nloc_train, loc_val = train_test_split(loc_list, test_size=0.2, random_state=rand_seed)\ndata_train.set_index(\"location\", inplace=True, drop=True)\ndata_val = data_train.loc[loc_val, :]\ndata_train = data_train.loc[loc_train, :]\ndata_train.reset_index(drop=False, inplace=True)\ndata_val.reset_index(drop=False, inplace=True)","4b76cfc5":"x_cols = [\"log_cfm\", \"log_ftl\", \"rel_date_pct\", \"since_day_zero\"] + lockdown_cols\ny_cols = [\"log_cfm\", \"log_ftl\"]\nbatch_size = 512\nd_in = len(x_cols)\nd_meta = len(metadata.columns)\nds_train = TrainDataset(data_train, metadata, x_cols, y_cols, input_len)\ndl_train = TrainDataset.get_dataloader(ds_train, batch_size)\nds_val = TrainDataset(data_val, metadata, x_cols, y_cols, input_len)\ndl_val = TrainDataset.get_dataloader(ds_val, batch_size, shuffle=False)","5a7d5885":"for _, sample in enumerate(dl_train):\n    for x in sample:\n        print(x.size())\n    break","08b07b83":"class TestDataset(Dataset):\n    \n    def __init__(self, df, metadata, x_cols, test_period_len, input_len=30):\n        self.df = df\n        self.metadata = metadata\n        self.x_cols = x_cols\n        self.test_period_len = test_period_len\n        self.input_len = input_len\n        \n        self.loc_list = list(self.df.location.unique())\n        self.test_beg = self.df.rel_date.max() - test_period_len + 1\n    \n    def __len__(self):\n        return len(self.df) - self.input_len * len(self.loc_list)\n    \n    def __getitem__(self, idx):\n        df_idx = idx + self.input_len * len(self.loc_list)\n        cur_loc = self.df.location[df_idx]\n        cur_date = self.df.rel_date[df_idx]\n        input_beg = cur_date - self.input_len\n        input_end = cur_date\n        in_input_period = (self.df.rel_date >= input_beg) & (self.df.rel_date < input_end)\n        inputs = self.df.loc[(in_input_period) & (self.df.location == cur_loc), self.x_cols].values\n        inputs = np.expand_dims(inputs, axis=0)\n        meta_inputs = self.metadata.loc[cur_loc, :].values\n        meta_inputs = np.expand_dims(meta_inputs, axis=0)\n        \n        return torch.tensor(inputs, dtype=torch.float32),\\\n            torch.tensor(meta_inputs, dtype=torch.float32)\n    \n    @staticmethod\n    def get_dataloader(dataset, shuffle=False):\n        return DataLoader(dataset, batch_size=None, shuffle=shuffle)","8da8f1a5":"test_period_len = data_test.rel_date.max() - data_test.rel_date.min() + 1\nds_test = TestDataset(data_test_input, metadata, x_cols, test_period_len)\ndl_test = TestDataset.get_dataloader(ds_test, shuffle=False)","557eaf66":"for i, sample in enumerate(dl_test):\n    for x in sample:\n        print(x.size())\n    if i == 10:\n        break","0ddde190":"class CovidTransformer(nn.Module):\n    \n    def __init__(self, d_in, d_out, d_meta, d_model=256, d_fwd=512, n_head=4,\n            num_layers=6, dropout=0.1):\n        super().__init__()\n        self.linear = nn.Linear(d_in, d_model)\n        encoder_layer = nn.TransformerEncoderLayer(d_model, n_head, d_fwd, dropout)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n        self.meta_linear = nn.Linear(d_meta, d_model)\n        self.regressor = nn.Linear(2 * d_model, d_out)\n    \n    def forward(self, inputs, meta_inputs):\n        x = self.linear(inputs)\n        features = self.transformer(x)\n        features = features[-1, :, :]\n        meta_features = self.meta_linear(meta_inputs)\n        features = torch.cat([features, meta_features], dim=-1)\n        output = self.regressor(features)\n        \n        return output","1736a09a":"class RMSLE(nn.Module):\n    \n    def forward(self, output, target):\n        return torch.sqrt(F.mse_loss(output, target))","eb3d87c4":"class Engine(object):\n    \n    def compile(self, model, criterion, optimizer, scheduler=None):\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n    \n    def _init_history(self):\n        self.history = {}\n        self.history[\"loss_train\"] = []\n        self.history[\"loss_val\"] = []\n    \n    def _update_history(self, train_loss, val_loss):\n        self.history[\"loss_train\"].append(train_loss)\n        self.history[\"loss_val\"].append(val_loss)\n    \n    def plot_loss(self):\n        fig, ax = plt.subplots()\n        ax.plot(self.history[\"loss_train\"], label=\"train\")\n        ax.plot(self.history[\"loss_val\"], label=\"val\")\n        plt.legend()\n        plt.show()\n    \n    def _fit_epoch(self, dl_train):\n        train_loss = 0.0\n        for _, sample in enumerate(dl_train):\n            inputs, meta_inputs, target = sample\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n                meta_inputs = meta_inputs.cuda()\n                target = target.cuda()\n            inputs = torch.transpose(inputs, 1, 0)\n\n            # zero the parameter gradients\n            self.optimizer.zero_grad()\n\n            # forward + backward + optimize\n            output = self.model(inputs, meta_inputs)\n\n            loss = self.criterion(output, target)\n            loss.backward()\n            self.optimizer.step()\n\n            train_loss += loss.detach().cpu().item()\n        \n        train_loss \/= len(dl_train)\n        return train_loss\n    \n    def evaluate(self, dl_val):\n        val_loss = 0.0\n        self.model.eval()\n        \n        with torch.no_grad():\n            for _, sample in enumerate(dl_val):\n                inputs, meta_inputs, target = sample\n                if torch.cuda.is_available():\n                    inputs = inputs.cuda()\n                    meta_inputs = meta_inputs.cuda()\n                    target = target.cuda()\n                inputs = torch.transpose(inputs, 1, 0)\n\n                output = self.model(inputs, meta_inputs)\n\n                loss = self.criterion(output, target)\n                val_loss += loss.detach().cpu().item()\n        \n        val_loss \/= len(dl_val)\n        return val_loss\n    \n    def fit(self, epochs, dl_train, dl_val=None):\n        self._init_history()\n        progress = tqdm(total=epochs)\n        \n        for i in range(epochs):\n            progress.set_description_str(\"Epoch {}\/{}\"\n                .format(i + 1, epochs))\n            \n            train_loss = self._fit_epoch(dl_train)\n            val_loss = self.evaluate(dl_val)\n            if self.scheduler is not None:\n                if isinstance(self.scheduler, ReduceLROnPlateau):\n                    self.scheduler.step(val_loss)\n                else:\n                    self.scheduler.step()\n            \n            self._update_history(train_loss, val_loss)\n            \n            postfix = \"RMSLE - train: {:.4f}, val: {:.4f}\"\\\n                .format(train_loss, val_loss)\n            progress.set_postfix_str(postfix)\n            progress.update(1)\n    \n    def predict_batch(self, sample):\n        self.model.eval()\n        \n        with torch.no_grad():\n            inputs, meta_inputs = sample\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n                meta_inputs = meta_inputs.cuda()\n            inputs = torch.transpose(inputs, 1, 0)\n            output = self.model(inputs, meta_inputs)\n        \n        return output\n    \n    def predict(self, dl):\n        self.model.eval()\n        output = []\n        \n        with torch.no_grad():\n            for _, sample in enumerate(dl):\n                inputs, meta_inputs, target = sample\n                if torch.cuda.is_available():\n                    inputs = inputs.cuda()\n                    meta_inputs = meta_inputs.cuda()\n                    target = target.cuda()\n                inputs = torch.transpose(inputs, 1, 0)\n\n                output_batch = self.model(inputs, meta_inputs)\n                output.append(output_batch)\n        \n        output = torch.cat(output, dim=0).expm1()\n        \n        return output","c8b1898a":"model = CovidTransformer(d_in, 2, d_meta)\nif torch.cuda.is_available():\n    model = model.cuda()\ncriterion = RMSLE()\noptimizer = AdamW(model.parameters())\nepochs = 20\nscheduler = ExponentialLR(optimizer, 0.707)","4fabc7c4":"engine = Engine()\nengine.compile(model, criterion, optimizer, scheduler)","6a49f7e8":"engine.fit(epochs, dl_train, dl_val)","c5a05d80":"engine.plot_loss()","76d762cf":"def make_predictions(dl_test, engine, y_cols):\n    progress = tqdm(total=len(dl_test))\n    for i, sample in enumerate(dl_test):\n        idx = i + dl_test.dataset.input_len * len(dl_test.dataset.loc_list)\n        cur_loc = dl_test.dataset.df.location[idx]\n        cur_date = dl_test.dataset.df.rel_date[idx]\n        output = engine.predict_batch(sample).detach().cpu().numpy()\n        prev_date = cur_date - 1\n        prev_output = dl_test.dataset.df.loc[(dl_test.dataset.df.rel_date == prev_date)\n            & (dl_test.dataset.df.location == cur_loc), y_cols].values\n        output = np.maximum(output, prev_output)\n        dl_test.dataset.df.loc[(dl_test.dataset.df.rel_date == cur_date)\n            & (dl_test.dataset.df.location == cur_loc), y_cols] = output\n        progress.update(1)\n        \n    return dl_test.dataset.df","82a8574b":"predictions = make_predictions(dl_test, engine, y_cols)","c11697f2":"predictions[\"ConfirmedCases\"] = predictions[\"log_cfm\"].map(np.expm1)\npredictions[\"Fatalities\"] = predictions[\"log_ftl\"].map(np.expm1)","57d3c6a7":"predictions = predictions.loc[~predictions.ForecastId.isna(), :]","73f84b8e":"predictions = predictions.sort_values([\"ForecastId\"]).reset_index(drop=True)","4ca0c99a":"predictions","b4c8c0ab":"submission = predictions[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]\nsubmission[\"ForecastId\"] = submission[\"ForecastId\"].astype(int)","2be1f27c":"submission.to_csv(\"submission.csv\", index=False)","503ae063":"# Preprocess original data","736c1b1a":"# Metadata preprocessing","8c955098":"# Prepare test data","52723222":"# Add lockdown data"}}