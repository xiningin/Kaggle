{"cell_type":{"5e5af80c":"code","6dde6b3e":"code","932651c5":"code","c2c490f6":"code","701d5e53":"code","ff867fa4":"code","73981466":"code","ef0a5dd9":"code","94013e7d":"code","1284c202":"code","de636bd1":"code","768d5d74":"code","dc33f8f2":"code","4696d0fb":"code","42f780f9":"code","7b5025e2":"code","6474083f":"code","faab7220":"code","47db3455":"code","e3cc1f58":"code","bedba3d5":"code","4827dbbe":"code","fd935e4c":"code","03e47fd5":"code","298ef6ef":"code","7a5f9976":"code","42e497f8":"code","fb9f8699":"code","41b5ca0b":"code","79d41e3d":"code","1209c032":"code","df56704c":"code","3252a365":"code","a2780118":"code","a909cb3a":"code","41019d68":"code","54d2a235":"code","9ad88216":"code","15d13c6f":"code","72d85dff":"code","a2333ca2":"code","17979d2a":"code","53b4ae0a":"code","28d9aa62":"code","14156920":"code","bd5e9d9b":"code","8747bd3c":"code","3945416e":"code","b7ee283c":"code","a80ee448":"code","09640d2a":"code","3827d327":"code","0b2109c4":"code","3ee5818c":"code","a49186bc":"code","75918f42":"code","a069fbd4":"code","db6085a1":"code","75e3f912":"code","67698bad":"code","ece4633a":"code","7352715b":"code","6b766ef4":"code","99f5d081":"code","8473ac3c":"code","14e7f58f":"code","0bb8a8e2":"code","3624e389":"code","e48f398e":"markdown","7719eb7e":"markdown","58da96ae":"markdown","ee5a5947":"markdown","3341a0eb":"markdown","5d2467df":"markdown","c4033efd":"markdown","4f6dc0ea":"markdown","8b52185e":"markdown","d2d2f5c0":"markdown","3368df40":"markdown","fe0a2992":"markdown","c0ff1786":"markdown","1888bd3b":"markdown","ea889a8c":"markdown","62288107":"markdown","50ade84b":"markdown","304c3c78":"markdown","2b9d36c3":"markdown","1a8fffd0":"markdown","9c088d73":"markdown","c034e308":"markdown","8c440cb3":"markdown","d5a42769":"markdown","6893edfe":"markdown","37e592d1":"markdown","8afb2457":"markdown","b5f0ce61":"markdown","40c13d86":"markdown","b83a5ddf":"markdown","90d7d7d3":"markdown","05ae531f":"markdown","1d510ea5":"markdown","ca332bc4":"markdown","ad91c480":"markdown","ff9e4c06":"markdown","a77cf7f4":"markdown","86d2d967":"markdown","cce80dfb":"markdown","490ad672":"markdown","8e625616":"markdown","654c23b7":"markdown","2d173bf2":"markdown","ed9a137e":"markdown","b326a6b4":"markdown","c222329f":"markdown","e8ef3d10":"markdown","57365460":"markdown","54a19c53":"markdown","d04b41e0":"markdown","efaf9013":"markdown","d229f74f":"markdown","31eb0041":"markdown","dd069d83":"markdown"},"source":{"5e5af80c":"import wave\n\n# Create audio file wave object\ngood_morning = wave.open('..\/input\/audio-files\/audio_files\/good_morning.wav', 'r')\n\n# Read all frames from wave object \nsignal_gm = good_morning.readframes(-1)\n\n# View first 20\ndisplay(signal_gm[:20], type(signal_gm), len(signal_gm))","6dde6b3e":"import numpy as np\n# Convert good morning audio bytes to integers\nsoundwave_gm = np.frombuffer(signal_gm, dtype='int16')\n# View the first 10 sound wave values\ndisplay(soundwave_gm[:10], len(soundwave_gm))","932651c5":"# Get the sound wave frame rate\nframerate_gm = good_morning.getframerate()\ndisplay(framerate_gm)\n\n# Find the sound wave timestamps\ntime_gm = np.linspace(start=0,\n                      stop=len(soundwave_gm)\/framerate_gm, num=len(soundwave_gm))\n\n# Print the first 10 timestamps\ndisplay(time_gm[:10], len(time_gm))","c2c490f6":"# Load file\ngood_afternoon = wave.open('..\/input\/audio-files\/audio_files\/good_afternoon.wav', 'r')\n# read the frames of the file in bytes form\nsignal_ga = good_afternoon.readframes(-1)\ndisplay(signal_ga[:20], type(signal_ga), len(signal_ga))\n# Convert into integers\nsoundwave_ga = np.frombuffer(signal_ga, dtype='int16')\ndisplay(soundwave_ga[:10], len(soundwave_ga))","701d5e53":"# check teh framerate\nframerate_ga = good_afternoon.getframerate()\ndisplay(framerate_ga)\n# Find the sound wave timestamps\ntime_ga = np.linspace(start=0,\n                      stop=len(soundwave_ga)\/framerate_ga, num=len(soundwave_ga))\n\n# Print the first 10 timestamps\ndisplay(time_ga[:10], len(time_ga))","ff867fa4":"# comapre the statistics of both the good morning and good afternoon audios\nimport pandas as pd\ndisplay(pd.Series(soundwave_gm).describe(), pd.Series(soundwave_ga).describe())","73981466":"import matplotlib.pyplot as plt\n\n# Setup the title and axis titles\nplt.title('Good Afternoon vs. Good Morning')\nplt.ylabel('Amplitude')\nplt.xlabel('Time (seconds)')\n\n# Add the Good Afternoon data to the plot\nplt.plot(time_ga, soundwave_ga, label='Good Afternoon')\n\n# Add the Good Morning data to the plot\nplt.plot(time_gm, soundwave_gm, label='Good Morning',\n   # Set the alpha variable to 0.5\n   alpha=0.5)\n\nplt.legend()\nplt.show()","ef0a5dd9":"!pip install speechrecognition\n# Importing the speech_recognition library\nimport speech_recognition as sr\n\n# Create an instance of the Recognizer class\nrecognizer = sr.Recognizer()\n\n# Set the energy threshold\nrecognizer.set_threshold = 300\ntype(recognizer)","94013e7d":"# Instantiate Recognizer\nrecognizer = sr.Recognizer()\n\n# Convert audio to AudioFile\nclean_support_call = sr.AudioFile('..\/input\/audio-files\/audio_files\/clean-support-call.wav')\n\n# Convert AudioFile to AudioData\nwith clean_support_call as source:\n    clean_support_call_audio = recognizer.record(clean_support_call)\n\n# Transcribe AudioData to text\ntext = recognizer.recognize_google(clean_support_call_audio,\n                                   language=\"en-US\")\nprint(text)","1284c202":"nothing_at_end = sr.AudioFile('..\/input\/audio-files\/audio_files\/30-seconds-of-nothing-16k.wav')\n\n# Convert AudioFile to AudioData\nwith nothing_at_end as source:\n    nothing_at_end_audio = recognizer.record(source,duration=10,offset=None)\n\n# Transcribe AudioData to text\ntext_5 = recognizer.recognize_google(nothing_at_end_audio,language=\"en-US\")\n\nprint(text_5)","de636bd1":"unwanted_bgng = sr.AudioFile('..\/input\/audio-files\/audio_files\/static-out-of-warranty.wav')\n\nwith unwanted_bgng as source:\n    unwanted_bgng_audio = recognizer.record(source,duration=10,offset=4)\n\n# Transcribe AudioData to text\ntext_vs_1 = recognizer.recognize_google(unwanted_bgng_audio,language=\"en-US\")\n\nprint(text_vs_1)","768d5d74":"# Create a recognizer class\nrecognizer = sr.Recognizer()\n\njapanese = sr.AudioFile('..\/input\/audio-files\/audio_files\/good-morning-japanense.wav')\nwith japanese as source:\n    japanese_audio = recognizer.record(source)\n\n# Pass the Japanese audio to recognize_google\ntext = recognizer.recognize_google(japanese_audio, language='en-US')\n\n# Print the text\nprint(text)","dc33f8f2":"# Create a recognizer class\nrecognizer = sr.Recognizer()\n\n# Pass the Japanese audio to recognize_google\ntext = recognizer.recognize_google(japanese_audio, language='ja')\n\n# Print the text\nprint(text)","4696d0fb":"# Create a recognizer class\nrecognizer = sr.Recognizer()\n\nleapard = sr.AudioFile('..\/input\/audio-files\/audio_files\/leopard.wav')\nwith leapard as source:\n    leopard_audio = recognizer.record(source)\n    \n# Pass the leopard roar audio to recognize_google\ntext = recognizer.recognize_google(leopard_audio, language=\"en-US\",show_all=True)\n\n# Print the text\nprint(text)","42f780f9":"# Create a recognizer class\nrecognizer = sr.Recognizer()\n\ncharlie = sr.AudioFile('..\/input\/audio-files\/audio_files\/charlie-bit-me-5.wav')\nwith charlie as source:\n    charlie_audio = recognizer.record(source)\n    \n# Pass charlie_audio to recognize_google\ntext = recognizer.recognize_google(charlie_audio,language=\"en-US\")\n\n# Print the text\nprint(text)","7b5025e2":"recognizer = sr.Recognizer()\n\nmulti = sr.AudioFile('..\/input\/audio-files\/audio_files\/multiple-speakers-16k.wav')\nwith multi as source:\n    multi_audio = recognizer.record(source, offset=0)\ntext = recognizer.recognize_google(multi_audio, language = 'en-US')\ntext","6474083f":"# recognizer = sr.Recognizer()\n\n# # Multiple speakers on different files\n# speakers = [sr.AudioFile(\"speaker_0.wav\"), \n#             sr.AudioFile(\"speaker_1.wav\"), \n#             sr.AudioFile(\"speaker_2.wav\")]\n\n# # Transcribe each speaker individually\n# for i, speaker in enumerate(speakers):\n#     with speaker as source:\n#         speaker_audio = recognizer.record(source)\n#     print(f\"Text from speaker {i}:\")\n#     print(recognizer.recognize_google(speaker_audio, language=\"en-US\"))","faab7220":"recognizer = sr.Recognizer()\n\nclean_support_call = sr.AudioFile('..\/input\/audio-files\/audio_files\/clean-support-call.wav')\n\n# Record the audio from the clean support call\nwith clean_support_call as source:\n  clean_support_call_audio = recognizer.record(source)\n\n# Transcribe the speech from the clean support call\ntext = recognizer.recognize_google(clean_support_call_audio,language=\"en-US\")\n\nprint(text)","47db3455":"recognizer = sr.Recognizer()\n\nnoisy_support_call = sr.AudioFile('..\/input\/audio-files\/audio_files\/2-noisy-support-call.wav')\n\n# Record the audio from the noisy support call\nwith noisy_support_call as source:\n      noisy_support_call_audio = recognizer.record(source)\n\n# Transcribe the speech from the noisy support call\ntext = recognizer.recognize_google(noisy_support_call_audio,language=\"en-US\",show_all=True)\n\nprint(text)","e3cc1f58":"recognizer = sr.Recognizer()\n\n# Record the audio from the noisy support call\nwith noisy_support_call as source:\n# Adjust the recognizer energy threshold for ambient noise\n    recognizer.adjust_for_ambient_noise(source, duration=1)\n    noisy_support_call_audio = recognizer.record(noisy_support_call)\n\n# Transcribe the speech from the noisy support call\ntext = recognizer.recognize_google(noisy_support_call_audio,language=\"en-US\")\n\nprint(text)","bedba3d5":"recognizer = sr.Recognizer()\n\n# Record the audio from the noisy support call\nwith noisy_support_call as source:\n# Adjust the recognizer energy threshold for ambient noise\n    recognizer.adjust_for_ambient_noise(source, duration=.5)\n    noisy_support_call_audio = recognizer.record(noisy_support_call)\n\n# Transcribe the speech from the noisy support call\ntext = recognizer.recognize_google(noisy_support_call_audio,language=\"en-US\")\n\nprint(text)","4827dbbe":"# Import AudioSegment from Pydub\nfrom pydub import AudioSegment\n\n# Create an AudioSegment instance\nwav_file = AudioSegment.from_file(file='..\/input\/audio-files\/audio_files\/wav_file.wav',format=\"wav\")\n\n# Check the type\nprint(type(wav_file))","fd935e4c":"# !pip install pyaudio\n# Import pyaudio and play\nfrom pydub.playback import play\n# import pyaudio\n\n# Play the audio file\nplay(wav_file)","03e47fd5":"# Find the frame rate\nprint(wav_file.frame_rate)\n\n# Find the number of channels\nprint(wav_file.channels)\n\n# Find the max amplitude\nprint(wav_file.max)\n\n# Find the length\nprint(len(wav_file))","298ef6ef":"# Create a new wav file with adjusted frame rate\nwav_file_16k = wav_file.set_frame_rate(16000)\n\n# Check the frame rate of the new wav file\nprint(wav_file_16k.frame_rate)\nplay(wav_file_16k)","7a5f9976":"# Set number of channels to 1\nwav_file_1_ch = wav_file.set_channels(1)\n\n# Check the number of channels\nprint(wav_file_1_ch.channels)","42e497f8":"# Print sample_width\nprint(f\"Old sample width: {wav_file.sample_width}\")\n\n# Set sample_width to 1\nwav_file_sw_1 = wav_file.set_sample_width(1)\n\n# Check new sample_width\nprint(f\"New sample width: {wav_file_sw_1.sample_width}\")","fb9f8699":"# Import audio file\nvolume_adjusted = AudioSegment.from_file('..\/input\/audio-files\/audio_files\/volume_adjusted.wav')\nplay(volume_adjusted)","41b5ca0b":"# Loour the volume by 60 dB\nquiet_volume_adjusted = volume_adjusted - 60\nplay(quiet_volume_adjusted)","79d41e3d":"# Increase the volume by 15 dB\nlouder_volume_adjusted = volume_adjusted + 15\nplay(louder_volume_adjusted)","1209c032":"# Import normalize\nfrom pydub.effects import normalize\n\n# Import target audio file\nloud_then_quiet = AudioSegment.from_file('..\/input\/audio-files\/audio_files\/ex3_loud_then_quiet.wav')\nplay(loud_then_quiet)\n\n# Normalize target audio file\nnormalized_loud_then_quiet = normalize(loud_then_quiet)\nplay(normalized_loud_then_quiet)","df56704c":"# Import part 1 and part 2 audio files\npart_1 = AudioSegment.from_file('..\/input\/audio-files\/audio_files\/ex3_slicing_part_1.wav')\nplay(part_1)\n\npart_2 = AudioSegment.from_file('..\/input\/audio-files\/audio_files\/ex3_slicing_part_2.wav')\nplay(part_2)\n\n# Remove the first four seconds of part 1\npart_1_removed = part_1[4*1000:]\nplay(part_1_removed)\n\n# Add the remainder of part 1 and part 2 together\npart_3 = part_1_removed + part_2\nplay(part_3)","3252a365":"# Import stereo audio file and check channels\nstereo_phone_call = AudioSegment.from_file('..\/input\/audio-files\/audio_files\/ex3_stereo_call.wav')\nprint(f\"Stereo number channels: {stereo_phone_call.channels}\")\nplay(stereo_phone_call)\n\n# Split stereo phone call and check channels\nchannels = stereo_phone_call.split_to_mono()\nprint(f\"Split number channels: {channels[0].channels}, {channels[1].channels}\")\n\n# Save new channels separately\nphone_call_channel_1 = channels[0]\nphone_call_channel_2 = channels[1]","a2780118":"play(phone_call_channel_1)\nplay(phone_call_channel_2)","a909cb3a":"# Import the .mp3 file\nmp3_file = AudioSegment.from_file('..\/input\/audio-files\/audio_files\/non_wav_files\/mp3_file.mp3')\n\n# Export the .mp3 file as wav\nmp3_file.export(out_f='mp3_file.wav',format='wav')","41019d68":"# # Loop through the files in the folder\n# for audio_file in folder:\n    \n#     # Create the new .wav filename\n#     wav_filename = os.path.splitext(os.path.basename(audio_file))[0] + \".wav\"\n        \n#     # Read audio_file and export it in wav format\n#     AudioSegment.from_file(audio_file).export(out_f=wav_filename,format='wav')\n        \n#     print(f\"Creating {wav_filename}...\")","54d2a235":"file_with_static = AudioSegment.from_file('..\/input\/audio-files\/audio_files\/non_wav_files\/ex3-static-help-with-account.mp3')\nplay(file_with_static)\n\n# Cut the first 3-seconds of static off\nfile_without_static = file_with_static[3000:]\nplay(file_without_static)","9ad88216":"# Increase the volume by 10dB\nlouder_file_without_static = file_without_static + 10\nplay(louder_file_without_static)","15d13c6f":"folder = ['..\/input\/audio-files\/audio_files\/non_wav_files\/mp3_file.mp3',\n         '..\/input\/audio-files\/audio_files\/non_wav_files\/ex3-static-help-with-account.mp3']","72d85dff":"import os\nfor audio_file in folder:\n    file_with_static = AudioSegment.from_file(audio_file)\n\n    # Cut the 3-seconds of static off\n    file_without_static = file_with_static[3000:]\n\n    # Increase the volume by 10dB\n    louder_file_without_static = file_without_static + 10\n    \n    # Create the .wav filename for export\n    wav_filename = os.path.splitext(os.path.basename(audio_file))[0] + \".wav\"\n    \n    # Export the louder file without static as .wav\n    louder_file_without_static.export(wav_filename, format='wav')\n    print(f\"Creating {wav_filename}...\")","a2333ca2":"# Create function to convert audio file to wav\ndef convert_to_wav(filename):\n  \"\"\"Takes an audio file of non .wav format and converts to .wav\"\"\"\n  # Import audio file\n  audio = AudioSegment.from_file(filename)\n  \n  # Create new filename\n  new_filename = filename.split(\".\")[0] + \".wav\"\n  \n  # Export file as .wav\n  audio.export(new_filename, format='wav')\n  print(f\"Converting {filename} to {new_filename}...\")\n \n# Test the function\nconvert_to_wav('..\/input\/audio-files\/audio_files\/non_wav_files\/ex4_call_1_stereo_mp3.mp3')","17979d2a":"def show_pydub_stats(filename):\n  \"\"\"Returns different audio attributes related to an audio file.\"\"\"\n  # Create AudioSegment instance\n  audio_segment = AudioSegment.from_file(filename)\n  \n  # Print audio attributes and return AudioSegment instance\n  print(f\"Channels: {audio_segment.channels}\")\n  print(f\"Sample width: {audio_segment.sample_width}\")\n  print(f\"Frame rate (sample rate): {audio_segment.frame_rate}\")\n  print(f\"Frame width: {audio_segment.frame_width}\")\n  print(f\"Length (ms): {len(audio_segment)}\")\n  return audio_segment\n\n# Try the function\ncall_1_audio_segment = show_pydub_stats('..\/input\/audio-files\/audio_files\/ex4_call_1_stereo_formatted.wav')","53b4ae0a":"def transcribe_audio(filename):\n  \"\"\"Takes a .wav format audio file and transcribes it to text.\"\"\"\n  # Setup a recognizer instance\n  recognizer = sr.Recognizer()\n  \n  # Import the audio file and convert to audio data\n  audio_file = sr.AudioFile(filename)\n\n  with audio_file as source:\n    audio_data = recognizer.record(source)\n\n  # Return the transcribed text\n  return recognizer.recognize_google(audio_data)\n\n# Test the function\nfile_name = '..\/input\/audio-files\/audio_files\/ex4_call_1_stereo_formatted.wav'\nprint(transcribe_audio(filename=file_name))","28d9aa62":"# # Convert mp3 file to wav\nconvert_to_wav('..\/input\/audio-files\/audio_files\/non_wav_files\/ex4_call_1_stereo_mp3.mp3')\n\n# Check the stats of new file\ncall_1 = show_pydub_stats(\"..\/input\/audio-files\/audio_files\/ex4_call_1_stereo_formatted.wav\")\nplay(call_1)\n\n# Split call_1 to mono\ncall_1_split = call_1.split_to_mono()\nprint(call_1_split)\n\n# Export channel 2 (the customer channel)\ncall_1_split[1].export(\"call_1_channel_2.wav\",format=\"wav\")\n\n# Transcribe the single channel\nprint(transcribe_audio(\"call_1_channel_2.wav\"))","14156920":"import nltk\nnltk.download('punkt')\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer","bd5e9d9b":"# Create SentimentIntensityAnalyzer instance\nsid = SentimentIntensityAnalyzer()\nplay(AudioSegment.from_file('..\/input\/audio-files\/audio_files\/ex4_call_2_stereo_native.wav'))\n# Let's try it on one of our phone calls\ncall_2_text = transcribe_audio('..\/input\/audio-files\/audio_files\/ex4_call_2_stereo_native.wav')\n\n# Display text and sentiment polarity scores\nprint(call_2_text)\nprint(sid.polarity_scores(call_2_text))","8747bd3c":"# Create SentimentIntensityAnalyzer instance\nsid = SentimentIntensityAnalyzer()\n\n# Transcribe customer channel of call 2\ncall_2_channel_2_text = transcribe_audio('..\/input\/audio-files\/audio_files\/ex4_call_2_channel_2_formatted.wav')\n\n# Display text and sentiment polarity scores\nprint(call_2_channel_2_text)\nprint(sid.polarity_scores(call_2_channel_2_text))","3945416e":"# Import sent_tokenize from nltk\nfrom nltk import sent_tokenize\n\n# Split call 2 channel 2 into sentences and score each\nfor sentence in sent_tokenize(call_2_channel_2_text):\n    print(sentence)\n    print(sid.polarity_scores(sentence))","b7ee283c":"call_2_channel_2_paid_api_text = transcribe_audio('..\/input\/audio-files\/audio_files\/ex4_call_2_channel_2_formatted.wav')\n\n# Split channel 2 paid text into sentences and score each\nfor sentence in sent_tokenize(call_2_channel_2_paid_api_text):\n    print(sentence)\n    print(sid.polarity_scores(sentence))","a80ee448":"import spacy\nimport en_core_web_sm\n\n# Transcribe call 4 channel 2\ncall_4_channel_2_text = transcribe_audio(\"..\/input\/audio-files\/audio_files\/ex4_call_4_channel_2_formatted.wav\")\n\n# Create a spaCy language model instance\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Create a spaCy doc with call 4 channel 2 text\ndoc = nlp(call_4_channel_2_text)\n\n# Check the type of doc\nprint(type(doc))","09640d2a":"# Show tokens in doc\nfor token in doc:\n    print(token.text, token.idx)","3827d327":"# Show sentences in doc\nfor sentence in doc.sents:\n    print(sentence)","0b2109c4":"# Show named entities and their labels\nfor entity in doc.ents:\n    print(entity.text, entity.label_)","3ee5818c":"nlp.pipeline[2]","a49186bc":"# Instantiate nlp and check pipeline\nnlp = spacy.load(\"en_core_web_sm\")\ndisplay(nlp.pipeline)\n\n# Import EntityRuler class\nfrom spacy.pipeline import EntityRuler\n\n# Create EntityRuler instance\n# ruler = nlp.add_pipe(\"entity_ruler\",before=\"ner\")\nruler = EntityRuler(nlp)\nruler.add_patterns([{\"label\": \"PRODUCT\", \"pattern\": 'smartphone'}, \n                    {\"label\": \"EMOTION\", \"pattern\": 'happy'},\n                   {'label': \"STATUS\", \"pattern\": 'delivered'}])\n\n# Update existing pipeline\nnlp.add_pipe(nlp.create_pipe('entity_ruler'), before=\"ner\")\ndisplay(nlp.pipeline)\n\n# Define pattern for new entity\n\ndisplay(ruler.patterns)\n\n# Create a spaCy doc with call 4 channel 2 text\ndoc = nlp(call_4_channel_2_text)\n\n# Test new entity\nfor entity in doc.ents:\n  print(entity.text, entity.label_)","75918f42":"# Test new entity\nfor entity in doc.ents:\n  print(entity.sent, entity.label_)","a069fbd4":"df = pd.read_csv('..\/input\/transcribed-text-labelled-data\/customer_call_transcriptions.csv')\ndf.head()","db6085a1":"train_df = df.sample(int(.7*df.shape[0]))\ntest_df = df[~df.index.isin(train_df.index)]","75e3f912":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Build the text_classifier as an sklearn pipeline\ntext_classifier = Pipeline([\n    ('vectorizer', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('classifier', MultinomialNB()),\n])\n\n# Fit the classifier pipeline on the training data\ntext_classifier.fit(train_df.text, train_df.label)","67698bad":"# Evaluate the MultinomialNB model\npredicted = text_classifier.predict(test_df.text)\naccuracy = 100 * np.mean(predicted == test_df.label)\nprint(f'The model is {accuracy}% accurate')","ece4633a":"path = '..\/input\/audio-files\/audio_files\/'","7352715b":"# Instantiate recognizer\nrecognizer = sr.Recognizer()\n\n# Setup an empty listfor Trump's transcribed speech text\ntrump_text =[]\n\n# transcribe all the audio and combine\nfor audio_file in [path+'1_audio_dt.wav',path+'2_audio_dt.wav',path+'3_audio_dt.wav',path+'4_audio_dt.wav',path+'5_audio_dt.wav']:\n    aud = sr.AudioFile(audio_file)\n    with aud as source:\n        aud_data = recognizer.record(source, offset=0)\n    text = recognizer.recognize_google(aud_data, language = 'en-US')\n    trump_text.append(text)\ntrump_text = ' '.join(trump_text)\ntrump_text","6b766ef4":"# Import the stop word packages\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n# Setup teh stop words\nstop_words_1 = set(stopwords.words('english'))\nstop_words_2 = set(STOPWORDS)\nstop_words = stop_words_1.union(stop_words_2)","99f5d081":"# Create wordcloud object\nwordcloud = WordCloud(stopwords=stop_words, background_color=\"white\", max_words=50).generate(trump_text)\n\n# Plot the wordloud\nplt.figure(figsize=[15,10])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"Key words in Donald Trump's election victory speech\", size =25, color='red')\nplt.axis(\"off\")\nplt.show()  ","8473ac3c":"# Instantiate recognizer\nrecognizer = sr.Recognizer()\n\n# Setup an empty listfor Swami Vivekananda's transcribed speech text\nswami_text=[]\n\npath = '..\/input\/swami-vivekananda-files-1\/'\n\n# Define file names\nfnames = ['{}_sv_audio_file.wav'.format(i) for i in range(1,6)]\n\n# transcribe all the audio and combine\nfor fname in fnames:\n    sv_partial = sr.AudioFile(path+fname)\n    with sv_partial as source:\n        recognizer.adjust_for_ambient_noise(source, duration=.1)\n        audio_data = recognizer.record(source)\n        track_text = recognizer.recognize_google(audio_data, language='en-US')\n        swami_text.append(track_text)","14e7f58f":"path = '..\/input\/swamivivekanandafiles2\/'\n\n# Define file names\nfnames = ['{}_sv_audio_file.wav'.format(i) for i in range(6,11)]\n\n# transcribe all the audio and combine\nfor fname in fnames:\n    sv_partial = sr.AudioFile(path+fname)\n    with sv_partial as source:\n        recognizer.adjust_for_ambient_noise(source, duration=.1)\n        audio_data = recognizer.record(source)\n        track_text = recognizer.recognize_google(audio_data, language='en-US')\n        swami_text.append(track_text)","0bb8a8e2":"path = '..\/input\/swamivivekanandafiles3\/'\n\n# Define file names\nfnames = ['{}_sv_audio_file.wav'.format(i) for i in range(11,16)]\n\n# transcribe all the audio and combine\nfor fname in fnames:\n    sv_partial = sr.AudioFile(path+fname)\n    with sv_partial as source:\n        recognizer.adjust_for_ambient_noise(source, duration=.1)\n        audio_data = recognizer.record(source)\n        track_text = recognizer.recognize_google(audio_data, language='en-US')\n        swami_text.append(track_text)\nswami_text = ' '.join(swami_text)","3624e389":"# Plot the wordloud\nwordcloud = WordCloud(stopwords=stop_words, background_color=\"white\", max_words=100, mode='RGBA').generate(swami_text)\nplt.figure(figsize=[15,10])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"Key words in Swamy Vivekananda's Chicago speech\", size =25, color='red')\nplt.axis(\"off\")\nplt.show()   ","e48f398e":"### Preparing audio files for text classification\n\nWe have a .csv file where each transcribed speech text is labelled as either 'post purchase' or 'pre purchase'.\n\nWe will first build a datframe from teh .csv file andprint the .head() of each of these to the console.\n\nThen we split the data into train and test sets randomly.\n\nWe'll build an sklearn pipeline using CountVectorizer() and TfidfTransformer() to convert our text samples to numbers and then use a MultinomialNB() classifier to learn what category each sample belongs to.\n\nThis model will work  we ll on our small example here but for larger amounts of text, we may want to consider something more sophisticated.\n\nSteps to perform:\n\n    1.Create the data frame\n    2. SPlit the data frame into train and test sets\n    3.Create text_classifier using CountVectorizer(), TfidfTransformer(), and MultinomialNB().\n    4.Fit text_classifier on train_df.text and train_df.label.\n","7719eb7e":"### Audio parameters with PyDub\n\nEvery audio file we work with will have a number of characteristics associated with them, such as, channels, frame rate (or sample rate), sample width and more.\n\nKnowing these parameters is useful to ensure our audio files are compatible with various API requirements for speech transcription.\n\nFor example, many APIs recommend a minimum frame rate (wav_file.frame_rate) of 16,000 Hz.\n\nWhen we create an instance of AudioSegment, PyDub automatically infers these parameters from our audio files and saves them as attributes.\n\nSteps to perform:\n\n    1.Find the frame_rate of wav_file.\n    2.Find the number of channels of wav_file.\n    3.Find the max amplitude of wav_file.\n    4.Find the length of wav_file in milliseconds.","58da96ae":"### Multiple Speakers 2\n\nDeciphering between multiple speakers in one audio file is called speaker diarization. However,  we've seen the free function we've been using, recognize_google() doesn't have the ability to transcribe different speakers.\n\nOne way around this, without using one of the paid speech to text services, is to ensure our audio files are single speaker.\n\nThis means if we are working with phone call data, we would make sure the caller and receiver are recorded separately. Then we could transcribe each file individually.\n\nIn this exercise,  we'll transcribe each of the speakers in our multiple speakers audio file individually.\n\nSteps to perform:\n\n    1.Pass speakers to the enumerate() function to loop through the different speakers.\n    2.Call record() on recognizer to convert the AudioFiles into AudioData.\n    3.Use recognize_google() to transcribe each of the speaker_audio objects.","ee5a5947":"### From AudioFile to AudioData\n\nAs  we  saw earlier, there are some transformation steps  we  have to take to make our audio data useful. The same goes for SpeechRecognition.\n\nIn this exercise,  we'll import the clean_support_call.wav audio file and get it ready to be recognized.\n\nWe  first read our audio file using the AudioFile class. But the recognize_google() method requires an input of type AudioData.\n\nTo convert our AudioFile to AudioData,  we'll use the Recognizer class's method record() along with a context manager. The record() method takes an AudioFile as input and converts it to AudioData, ready to be used with recognize_google().\n\nSpeechRecognition has already been imported as sr.\n\nSteps to perform:\n\n    1.Pass the AudioFile class clean_support_call.wav.\n    2.Use the context manager to open and read clean_support_call as source.\n    3.Record source and run the code.","3341a0eb":"Having audio files with only one speaker usually results in better quality transcriptions. Now  we've done all this audio processing, how do save our altered audio files to use later? Let's find out.","5d2467df":"### Using the helper functions  we've built\n\nNow  we've got some helper functions ready to go, it's time to put them to use!\n\nWe'll first use convert_to_wav() to convert call_1.mp3 to .wav format and save it as call_1.wav\n\nUsing show_pydub_stats() we find call_1.wav has 2 channels so we decide to split them using PyDub's split_to_mono(). We know the customer channel is likely channel 2. So we export channel 2 using PyDub's .export().\n\nFinally,  we'll use transcribe_audio() to transcribe channel 2 only.\n\nSteps to perform:\n\n    1.Convert the .mp3 version of call_1 to .wav and then check the stats of the .wav version.\n    2.Split call_1 to mono and then export the second channel in .wav format.\n    3.Transcribe the audio of call 1's channel 2.","c4033efd":"### Finding the time stamps\n\n we  know the frequency of our sound wave is 48 kHz, but what if we didn't? we could find it by dividing the length of our sound wave array by the duration of our sound wave. Ho we ver, Python's wave module has a better way. Calling getframerate() on a wave object returns the frame rate of that wave object.\n\n we  can then use NumPy's linspace() method to find the time stamp of each integer in our sound wave array. This will help us visualize our sound wave in the future.\n\nThe linspace() method takes start, stop and num parameters and returns num evenly spaced values bet we en start and stop.\n\nIn our case, start will be zero, stop will be the length of our sound wave array over the frame rate (or the duration of our audio file) and num will be the length of our sound wave array.\n\nSteps to perform:\n\n    - Convert the sound wave bytes to integers.\n    - Get the frame rate of the good morning audio file using getframerate().\n    - Set stop to be the length of soundwave_gm over the frame rate.\n    - Set num to be the length of soundwave_gm.","4f6dc0ea":"### Working with noisy audio\n\nIn this exercise,  we'll start by transcribing a clean speech sample to text and then see what happens when we add some background noise.\n\nA clean audio sample has been imported as clean_support_call.\n\nWe'll then do the same with the noisy audio file saved as noisy_support_call. It has the same speech as clean_support_call but with additional background noise.\n\nTo try and negate the background noise,  we'll take advantage of Recognizer's adjust_for_ambient_noise() function.\n\nSteps to perform:\n\n    1. Let's transcribe some clean audio. Read in clean_support_call as the source and call recognize_google() on the file.\n    2. Let's do the same as before but with a noisy audio file saved as noisy_support_call and show_all parameter as True.\n    3. Set the duration parameter of adjust_for_ambient_noise() to 1 (second) so recognizer adjusts for background noise.\n    4. A duration of 1 was too long and it cut off some of the audio. Try setting duration to 0.5.","8b52185e":"# Spoken Language Processing, Exploratory Analysis and Classification in Python\n\nIn this notebook we will see how to process differnt audio formatted files, understand their characteristics and analyse them. We will convert the file formats, do pre-processing, transcribe and some speech recognition of the audio files. We will also perform named entity serach, sentiment analysis, classification model building and develop word clouds using a transcribed audio.","d2d2f5c0":"We notice the recognizer didn't transcribe the words 'fast as' adequately on the last line, starring them out as a potential expletive, this is a reminder speech recognition still isn't perfect. But now  we've now got a function which can transcribe the audio of a .wav file with one line of code. They're a bit of effort to setup but once  we've got them, helper functions like transcribe_audio() save time and prevent errors later on.","3368df40":"Notice the two sound waves are very similar in the beginning. Because the first word is good in both audio files, they almost completely overlap. A  we ll-built speech recognition system would recognize this and return the same first word for each wave. Let's build one to do just that.","fe0a2992":"### Importing an audio file with Python\n\nThere are different kinds of audio files and streaming music and spoken language have different sampling rates. Let us start working with these files.\n\nTo begin,  we're going to import the good_morning.wav audio file using Python's in-built wave library. Then  we'll see what it looks like in byte form using the built-in readframes() method.\n\nGood_morning.wav is only a few seconds long but at 48 kHz, that means it contains 48,000 pieces of information per second.\n\nSteps to perform:\n\n    1.Import the Python wave library.\n    2.Read in the good_morning.wav audio file and save it to good_morning.\n    3.Create signal_gm by reading all the frames from good_morning using readframes().\n    4.See what the first 20 frames of audio look like by slicing signal_gm.","c0ff1786":"Now for multiple files. Use from_file() to import each audio_file and export the louder files without static with the \"wav\" format.","1888bd3b":"### Recording the audio we need\n\nSometimes we may not want the entire audio file  we're working with. The duration and offset parameters of the record() method can help with this.\n\nAfter exploring our dataset, we find there's one file, imported as nothing_at_end which has 30-seconds of silence at the end and a support call file, imported as out_of_warranty has 3-seconds of static at the front.\n\nSetting duration and offset means the record() method will record up to duration audio starting at offset. They're both measured in seconds.\n\nSteps to perform:\n\n    1. Let's get the first 10-seconds of nothing_at_end_audio. To do this, we can set duration to 10.\n\n    2. Let's remove the first 3-seconds of static of static_at_start by setting offset to 3","ea889a8c":"### Creating a custom named entity in spaCy\n\nIf spaCy's built-in named entities aren't enough, we can make our own using spaCy's add_pipe() class.\n\nWe  can then call add_patterns() on the instance and pass it a dictionary of the text pattern  we'd like to label with an entity.\n\nOnce  we've setup a pattern we can add it the nlp pipeline using add_pipe().\n\nSince Acme is a technology company, we decide to tag the pattern \"smartphone\" with the \"PRODUCT\" entity tag.\n\nspaCy has been imported and a doc already exists containing the transcribed text from call_4_channel_2.wav.\n\nSteps to perform:\n\n    1.Add \"smartphone\", \"happ\" and \"delivered\" as the values for the \"pattern\" key.\n    2.Add \"PRODUCT\", \"EMOTION\", and \"STATUS\" as the values \"label\" key\n    3.Print the entity attributes contained in doc.","62288107":"Reading back the transcribed text and listening to the phone call, a compound score of close to 1 (more positive) makes sense since the customer states they're very happy and enjoying their device. Let's keep going!","50ade84b":"### Analyzing sentiment of a phone call\n\nOnce  we've transcribed the text from an audio file, it's possible to perform natural language processing on the text.\n\nIn this exercise,  we'll use NLTK's VADER (Valence Aware Dictionary and sEntiment Reasoner) to analyze the sentiment of the transcribed text of call_2.wav.\n\nTo transcribe the text,  we'll use the transcribe_audio() function we created earlier.\n\nOnce we have the text,  we'll use NLTK's SentimentIntensityAnalyzer() class to obtain a sentiment polarity score.\n\n.polarity_scores(text) returns a value for pos (positive), neu (neutral), neg (negative) and compound. Compound is a mixture of the other three values. The higher it is, the more positive the text. Loour means more negative.\n\nSteps to perform:\n\n    1.Instantiate an instance of SentimentIntensityAnalyzer() and save it to the variable sid.\n    2.Transcribe the target call and save it to call_2_text.\n    3.Print the polarity_scores() of call_2_text.","304c3c78":"Adjusting the volume with operators can be useful but doesn't help when we only want to increase the loudness of only quiet sections. Let's take a look at a function which can help!","2b9d36c3":"### Sentiment analysis on formatted text\n\nIn this exercise,  we'll calculate the sentiment on the customer channel of call_2.wav.\n\nWe've split the customer channel and saved it to call_2_channel_2.wav.\n\nBut from our experience with sentiment analysis, we know it can change sentence to sentence.\n\nTo calculate it sentence to sentence, we split the split using NLTK's sent_tokenize() module.\n\nBut transcribe_audio() doesn't return sentences. To try sentiment anaylsis with sentences,  we've tried a paid API service to get call_2_channel_2_paid_api_text which has sentences.\n\nSteps to perform:\n\n    1.Transcribe the audio of call_2_channel_2.wav and find the sentiment scores.\n    2.Split call_2_channel_2_text into sentences and find the sentiment score of each sentence.\n    3.Split call_2_channel_2_paid_api_text into sentences and score the sentiment of each.","1a8fffd0":"### Manipulating multiple audio files with PyDub\n\nWe've seen how to convert a single file using PyDub but what if we had a folder with multiple different file types?\n\nFor this exercise,  we need a folder which has different versions of audio file such as .mp3, .m4a and .aac. You can execute the below code if you have such setup with files and appropriate code modification.\n\nWe'll use PyDub to open each of the files and export them as .wav format so they're compatible with speech recognition APIs.\n\nSteps to perform:\n\n    1.Pass audio_file to the from_file() function.\n    2.Use export() to export wav_filename with the format \".wav\".","9c088d73":"### Bytes to integers\n\n we've seen how to import and read an audio file using Python's wave module and the readframes() method. But doing that results in an array of bytes.\n\nTo convert the bytes into something more useful,  we'll use NumPy's frombuffer() method.\n\nBy passing to frombuffer() our sound waves bytes and indicating a dtype of 'int16', we can convert our bytes to integers. Integers are much easier to work with than bytes.\n\nSteps to perform:\n\n    1.Import the numpy package with its common alias np.\n    2.Open and read the good morning audio file.\n    3.Convert the signal_gm bytes to int16 integers.\n    4.View the first 10 sound wave values.","c034e308":"Speech recognition works best on clear speech files, so the more we can do to improve the quality of our audio files, including their volume, the better.","8c440cb3":"Looks like installation of pyaudio (alternatively simpleaudio) has issues in Kaggle kernel. Hence they were commented out in the above code block. Hence audios cannot be played here in this notebook. If the installation and import happend successfuly, audios can be played.","d5a42769":"It seems call_1.wav has two channels, potentially they could be split using PyDubs's split_to_mono() and transcribed separately.","6893edfe":"### Import an audio file with PyDub\n\nPyDub's AudioSegment class makes it easy to import and manipulate audio files with Python.\n\nIn this exercise,  we'll import an audio file of interest by creating an instance of AudioSegment.\n\nTo import an audio file, we can use the from_file() function on AudioSegment and pass it our target audio file's pathname as a string. The format parameter gives we an option to specify the format of our audio file, ho we ver, this is optional as PyDub will automatically infer it.\n\nPyDub works with .wav files without any extra dependencies but for other file types like .mp3,  we'll need to install ffmpeg.\n\nA sample audio file has been setup as wav_file.wav, we can listen to it here.\n\nSteps to perform:\n\n    1.Import AudioSegment from pydub.\n    2. Call the from_file method and pass it the audio file pathname.","37e592d1":"With custom entities like this, we can start to get even more information out of our transcribed text. Depending on the problem  we're working with, we may want to combine a few different patterns together.","8afb2457":"### Multiple Speakers 1\n\nIf our goal is to transcribe conversations, there will be more than one speaker. However, as  we'll see, the recognize_google() function will only transcribe speech into a single block of text.\n\nWe  can hear in this audio file there are three different speakers.\n\nBut if we transcribe it on its own, recognize_google() returns a single block of text. Which is still useful but it doesn't let we know which speaker said what.\n\nWe'll see an alternative to this in the next exercise.\n\nThe multiple speakers audio file has been imported and converted to AudioData as multiple_speakers.\n\nSteps to perform:\n\n    1.Create an instance of Recognizer.\n    2.Recognize the multiple_speakers variable using the recognize_google() function.\n    3.Set the language to US English (\"en-US\").","b5f0ce61":"### Splitting stereo audio to mono with PyDub\n\nIf  we're trying to transcribe phone calls, there's a chance they've been recorded in stereo format, with one speaker on each channel.\n\nAs  we've seen, it's hard to transcribe an audio file with more than one speaker. One solution is to split the audio file with multiple speakers into single files with individual speakers.\n\nPyDub's split_to_mono() function can help with this. When called on an AudioSegment recorded in stereo, it returns a list of two separate AudioSegment's in mono format, one for each channel.\n\nWe will split this stereo phone call (stereo_phone_call.wav) recording into channel 1 and channel 2. This separates the two speakers, allowing for easier transcription.\n\nSteps to perform:\n\n    1.Create an AudioSegment instance stereo_phone_call with stereo_phone_call.wav.\n    2.Split stereo_phone_call into channels using split_to_mono() and check the channels of the resulting output.\n    3.Save each channel to new variables, phone_call_channel_1 and phone_call_channel_2.","40c13d86":"### Processing audio data with Python\n\n we've seen how a sound waves can be turned into numbers but what does all that conversion look like?\n\nAnd how about another similar sound wave? One slightly different?\n\n we're going to use MatPlotLib to plot the sound wave of good_morning against good_afternoon.\n\nTo have the good_morning and good_afternoon sound waves on the same plot and distinguishable from each other,  we'll use MatPlotLib's alpha parameter.\n\nSteps to perform:\n\n    1.Perform same steps on gooafternoon audio file to obtain timestamps and values\n    2.Set the title to reflect the plot we are making.\n    3.Add the good_afternoon time variable (time_ga) and amplitude variable (soundwave_ga) to the plot.\n    4.Do the same with the good_morning time variable (time_gm) and amplitude variable (soundwave_gm) to the plot.\n    5.Set the alpha variable to 0.5.","b83a5ddf":"Well, the results still aren't perfect. This should be expected with some audio files though, sometimes the background noise is too much. If our audio files have a large amount of background noise, we may need to preprocess them with an audio tool such as Audacity before using them with speech_recognition.","90d7d7d3":"### Chopping and changing audio files\n\nSome of our audio files may have sections of redundancy. For example, we might find at the beginning of each file, there's a few seconds of static.\n\nInstead of wasting compute trying to transcribe static, we can remove it.\n\nSince an AudioSegment is iterable, and measured in milliseconds, we can use slicing to alter the length.\n\nTo get the first 3-seconds of wav_file,  we'd use wav_file[:3000].\n\nWe  can also add two AudioSegment's together using the addition operator. This is helpful if we need to combine several audio files.\n\nTo practice both of these,  we're going to remove the first four seconds of part1.wav, and add the remainder to part2.wav. Leaving the end result sounding like part_3.wav.\n\nSteps to perform:\n\n    1.Import part_1.wav and part_2.wav and save them to part_1 and part_2 respectively.\n    2.Remove the first 4-seconds of part_1 using slicing and save the new audio to part_1_removed.\n    3.Add part_1_removed to part_2 and save it to part_3.","05ae531f":"### Finding PyDub stats\n\nWe  decide it'll be helpful to know the audio attributes of any given file easily. This will be especially helpful for finding out how many channels an audio file has or if the frame rate is adequate for transcription.\n\nWe'll create show_pydub_stats() which takes a filename of an audio file as input. It then imports the audio as a PyDub AudioSegment instance and prints attributes such as number of channels, length and more.\n\nIt then returns the AudioSegment instance so it can be used later on.\n\nWe'll use our function on the newly converted .wav file, call_1.wav\n\nSteps to perform:\n\n    1.Create an AudioSegment instance called audio_segment by importing the filename parameter.\n    2.Print the number of channels using the channels attribute.\n    3.Return the audio_segment variable.\n    4.Test the function on \"call_1.wav\".","1d510ea5":"### Converting audio to the right format\n\nAs  we'll be interacting with many audio files, we decide to begin by creating some helper functions.\n\nThe first one, convert_to_wav(filename) takes a file path and uses PyDub to convert it from a non-wav format to .wav format.\n\nOnce it's built,  we'll use the function to convert call_1.mp3, from .mp3 format to .wav.\n\nSteps to perform:\n\n    1.Import the filename parameter using AudioSegment's from_file().\n    2.Set the export format to \"wav\".\n    3.Pass the target audio file, call_1.mp3, to the function.","ca332bc4":"### Create a spoken language text classifier\n\nNow  we've transcribed some customer call audio data,  we'll build a model to classify whether the text from the customer call is pre_purchase or post_purchase.\n\nWe've got 45 examples of pre_purchase calls and 57 examples of post_purchase calls.\n\nThe data the model will train on is stored in train_df and the data the model will predict on is stored in test_df.\n\n","ad91c480":"### Play an audio file with PyDub\n\nIf  we're working with audio files, chances are we want to listen to them.\n\nPyDub's playback module provides a function called play() which can be passed an AudioSegment. Running the play() function with an AudioSegment passed in will play the AudioSegment out loud.\n\nThis can be helpful to check the quality of our audio files and assess any changes we need to make.\n\nRemember: to use the play() function,  we'll need simpleaudio or pyaudio installed for .wav files and ffmpeg for other kinds of files.\n\nSteps to perform:\n\n    1.Import play from the pydub.playback module.\n    2.Call play() whilst passing it the wav_file AudioSegment.","ff9e4c06":"Now  we're ready to start accessing the speech_recognition library and use the Recognizer class!","a77cf7f4":"### Named entity recognition in spaCy\n\nNamed entities are real-world objects which have names, such as, cities, people, dates or times. we can use spaCy to find named entities in our transcribed text.\n\nWe'll transcribe call_4_channel_2.wav using transcribe_audio() and then use spaCy's language model, en_core_ we b_sm to convert the transcribed text to a spaCy doc.\n\nTransforming text to a spaCy doc allows us to leverage spaCy's built-in features for analyzing text, such as, .text for tokens (single words), .sents for sentences and .ents for named entities.\n\nSteps to perform:\n\n    1.Create a spaCy doc by passing the transcribed call 4 channel 2 text to nlp() and then check its type.\n    2.Create a spaCy doc with call_4_channel_2_text then print all the token text in it using the .text attribute.\n    3.Load the \"en_core_ we b_sm\" language model and then print the sentences in the doc using the .sents attribute.\n    4.Access the entities in the doc using .ents and then print the text of each.","86d2d967":"There are many more characteristics we can find out about our audio files once  we've imported them as an AudioSegment.","cce80dfb":"Now the static has been removed, increase the volume by 10dB.","490ad672":"### Normalizing an audio file with PyDub\n\nSometimes  we'll have audio files where the speech is loud in some portions and quiet in others. Having this variance in volume can hinder transcription.\n\nLuckily, PyDub's effects module has a function called normalize() which finds the maximum volume of an AudioSegment, then adjusts the rest of the AudioSegment to be in proportion. This means the quiet parts will get a volume boost.\n\nWe'll use normalize() to normalize the volume of our file, making it sound more like this.\n\nSteps to perform:\n\n    Import AudioSegment from PyDub and normalize from the PyDub's effects module.\n    Import the target audio file, loud_then_quiet.wav and save it to loud_then_quiet.\n    Normalize the imported audio file using the normalize() function and save it to normalized_loud_then_quiet.","8e625616":"### Transcribing audio with one line\n\nAlright, now  we've got functions to convert audio files and find out their attributes, it's time to build one to transcribe them.\n\nIn this exercise,  we'll build transcribe_audio() which takes a filename as input, imports the filename using speech_recognition's AudioFile class and then transcribes it using recognize_google().\n\nWe've seen these functions before but now  we'll put them together so they're accessible in a function.\n\nTo test it out,  we'll transcribe \"call_1.wav\".\n\nSteps to perform:\n\n    1.Define a function called transcribe_audio which takes filename as an input parameter.\n    2.Setup a Recognizer() instance as recognizer.\n    3.Use recognize_google() to transcribe the audio data.\n    4.Pass the target call to the function.","654c23b7":"The word cloud does indicate that the speech is about politics, America, President, Trump and Clinton","2d173bf2":"There are other methods we can call on our AudioSegment instances to adjust their attributes as further practice, we should try and find some more. But remember, lowering the values generally leads to lower audio qaulity and worse transcriptions but increasing them may increase the file size and but not the quality of the transcription. Best to explore with different values and find out the ideal tradeoff.","ed9a137e":"### Exporting and reformatting audio files\n\nIf  we've made some changes to our audio files, or if they've got the wrong file extension, we can use PyDub to export and save them as new audio files.\n\nWe  can do this by using the .export() function on any instance of an AudioSegment  we've created. The export() function takes two parameters, out_f, or the destination file path of our audio file and format, the format  we'd like our new audio file to be. Both of these are strings.\n\nWe'll import a .mp3 file (mp3_file.mp3) and then export it with the .wav extension using .export().\n\nTo work with files other than .wav,  we'll need ffmpeg.\n\nSteps to perform:\n\n    1.Import mp3_file.mp3 and save it to mp3_file.\n    2.Export mp3_file with the file name mp3_file.wav with \"wav\" format.","b326a6b4":"The model was able to classify our test examples with a high level of accuracy. For larger datasets, our pipeline is a good baseline but we might want to look into something like a language model. Now we can start capturing speech, converting it to text and classifying it into different categories.","c222329f":"### Adjusting audio parameters\n\nDuring our exploratory data analysis, we may find some of the parameters of our audio files differ or are incompatible with speech recognition APIs.\n\nPyDub has built-in functionality which allows we to change various attributes.\n\nFor example, we can set the frame rate of our audio file calling set_frame_rate() on our AudioSegment instance and passing it an integer of the desired frame rate measured in Hertz.\n\nSteps to perform:\n\n    1.Create a new wav_file with a frame rate of 16,000 Hz and then check its frame rate.\n    2.Set the wav_file number of channels to 1 and then check the number of channels.\n    3.Print the sample width of wav_file and then set it to 1 and print it again.","e8ef3d10":"    1.Create predicted by calling predict() on text_classifier and passing it the text column of test_df.\n    2.Evaluate the model by seeing how predicted compares to the test_df.label.","57365460":"The word cloud indicates the speech is about Hindu religion and spirituality in teh context of life and the world.","54a19c53":"Prepare good afternoon sound data similarly.","d04b41e0":"### Turning it down... then up\n\nSpeech recognition works best on clean, audible speech. If our audio files are too quiet or too loud, it can hinder transcription.\n\nSteps to perform:\n\n    1.Import volume_adjusted.wav and loour its volume by 60 dB and save it to a new variable quiet_volume_adjusted.\n    2.Import the target audio file, increase its volume by 15 dB and save it to the variable louder_volume_adjusted.","efaf9013":"### Wordclouds\nNow, we take Donald Trumpt's victory speech, transcribe it and visualize the key words of the speech using WOrdCloud. We will do the same exercise with the speech given by Swami Vivekananda in Chicago way back in 1893.\n\nSteps to perform:\n        \n        1.Extract transcribed text from all the video files\n        2.Setup stop words\n        3.Produce wordcloud graphs","d229f74f":"### An audio processing workflow\n\nWe'll use PyDub to format a folder of files to be ready to use with speech_recognition.\n\nWe've found our customer call files all have 3-seconds of static at the start and are quieter than they could be.\n\nTo fix this,  we'll use PyDub to cut the static, increase the sound level and convert them to the .wav extension.\n\nLet's start with one file. Import account_help.mp3 and cut off the first 3-seconds (3000 milliseconds) of static","31eb0041":"### Using the SpeechRecognition library\n\nTo save typing speech_recognition every time,  we'll import it as sr.\n\n we'll also setup an instance of the Recognizer class to use later.\n\nThe energy_threshold is a number bet we en 0 and 4000 for how much the Recognizer class should listen to an audio file.\n\nenergy_threshold will dynamically adjust whilst the recognizer class listens to audio.\n\nSteps to perform:\n\n    1.Import the speech_recognition library as sr.\n    2.Setup an instance of the Recognizer class and save it to recognizer.\n    3.Set the recognizer.energy_threshold to 300.","dd069d83":"Different kinds of audio\n\nNow  we've seen an example of how the Recognizer class works. Let's try a few more. How about speech from a different language?\n\nWhat do we think will happen when we call the recognize_google() function on a Japanese version of good_morning.wav (japanese_audio)?\n\nThe default language is \"en-US\", are the results the same with the \"ja\" tag?\n\nHow about non-speech audio? Like leopard roaring.\n\nOr speech where the sounds may not be real words, such as a baby talking?\n\nTo familiarize more with the Recognizer class,  we'll look at an example of each of these.\n\nSteps to perform:\n\n    1. Pass the Japanese version of good morning (japanese_audio) to recognize_google() using \"en-US\" as the language.\n    2. Pass the same Japanese audio (japanese_audio) using \"ja\" as the language parameter. Do we see a difference?\n    3. What about about non-speech audio? Pass leopard_audio to recognize_google() with show_all as True.\n    4. What if our speech files have non-audible human sounds? Pass charlie_audio to recognize_google() to find out."}}