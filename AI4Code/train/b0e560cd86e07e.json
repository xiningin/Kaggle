{"cell_type":{"68baf180":"code","ec8f2d04":"code","3abd7f10":"code","da1b96c0":"code","2412a86f":"code","1e839414":"code","36e8bb4f":"code","1e3f5719":"code","5177de3d":"code","c610876b":"code","ce919b14":"code","29a93628":"code","98077ab1":"code","43a08241":"code","02bffb07":"code","41feecc6":"code","37692176":"code","305f0cd0":"code","066eff03":"code","38abc994":"code","012c1bf0":"code","d097ad0d":"code","b354eb65":"code","9ae8d936":"code","951507c5":"code","11d8c919":"code","c6ee6702":"code","c24410f5":"code","519bbef3":"code","a0b0bd6e":"code","90a91037":"code","cecd3b72":"code","58ae8c54":"code","11347d69":"code","c6852b60":"code","a651d8d7":"code","81958e70":"code","75309d56":"code","6fedd50d":"code","5c668c1f":"code","4de47cf4":"code","bb1af5ce":"code","922a278e":"code","08386b1b":"code","904d9dc8":"code","ea4fd593":"code","de154a39":"code","d060b615":"markdown","51d4d453":"markdown","77120823":"markdown","f5f07f16":"markdown","5a3619b8":"markdown","646c9a12":"markdown","e84952ac":"markdown","c7f5145d":"markdown","39a2162f":"markdown","b212b6e2":"markdown","b20e2c00":"markdown","0413d95b":"markdown","b5aa1deb":"markdown","86a1acbf":"markdown"},"source":{"68baf180":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec8f2d04":"!pip3 install hvplot[pandas]","3abd7f10":"import json\nimport spacy","da1b96c0":"from functools import partial\nfrom pathlib import Path\nfrom tqdm import tqdm","2412a86f":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_validate","1e839414":"import matplotlib.pyplot as plt\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport hvplot.pandas \n\nimport yellowbrick\nfrom yellowbrick.text import FreqDistVisualizer, UMAPVisualizer","36e8bb4f":"# reading csv files and train & test file paths\npath = Path(\"\/kaggle\/input\/coleridgeinitiative-show-us-the-data\")\ntrain_df = pd.read_csv(path \/ 'train.csv')\nsample_sub = pd.read_csv(path \/ 'sample_submission.csv')\ntrain_files_path = path \/ 'train'\ntest_files_path = path \/ 'test'","1e3f5719":"train_df.head(3)","5177de3d":"def retrieve_first_section(filename, train_files_path=train_files_path):\n    \"\"\"Return the first section of the article (most probably abstract or introduction).\"\"\"\n    \n    json_path = (train_files_path \/ filename).with_suffix('.json')\n    first_section = []\n    \n    with json_path.open() as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            if data.get('text').strip() != '' and not first_section:\n                first_section.append(data.get('text'))\n\n    return first_section[0]","c610876b":"tqdm.pandas()   #tqdm is used to show any code running with a progress bar. \ntrain_df['text'] = train_df['Id'].progress_apply(retrieve_first_section)","ce919b14":"tqdm.pandas()\nsample_sub['text'] = sample_sub['Id'].progress_apply(partial(retrieve_first_section, train_files_path=test_files_path))","29a93628":"train_df.head(3)","98077ab1":"# Drop every first_section with more than 100000 caracters\ntrain_df = train_df[train_df.text.str.len() < 100000]","43a08241":"train_df.index.size","02bffb07":"# Remove badly represented labels\/articles\ntrain_df = train_df[train_df['cleaned_label'].isin(train_df['cleaned_label'].value_counts()[train_df['cleaned_label'].value_counts()>5].index)]","41feecc6":"train_df.index.size","37692176":"import spacy\n\nis_using_gpu = spacy.prefer_gpu()\nnlp = spacy.load(\"en_core_web_lg\")","305f0cd0":"corpus = train_df.text.str.lower()","066eff03":"def lemmatize(doc):\n    return [token.lemma_ for token in doc \n            if token.is_alpha \n            and not token.is_stop \n            and (len(token.text) > 2)] ","38abc994":"def preprocess(docs, batch_size=200):\n    preprocessed = []\n    for doc in tqdm(nlp.pipe(docs, batch_size=batch_size)):\n        preprocessed.append(' '.join(lemmatize(doc)))\n    return preprocessed","012c1bf0":"train_df['preprocessed'] = preprocess(corpus)","d097ad0d":"train_df.preprocessed.str.len().hvplot.hist()","b354eb65":"# Drop every first_section with more than 2000 preprocessed words\ntrain_df = train_df[train_df.preprocessed.str.len() < 20000]","9ae8d936":"train_df.index.size","951507c5":"train_df.info()","11d8c919":"labels = train_df.cleaned_label.unique()","c6ee6702":"labels.size","c24410f5":"pd.Series(train_df.cleaned_label).value_counts().head(50).hvplot.bar(width=900, height=1000, rot=70)","519bbef3":"vectorizer = CountVectorizer()\ndocs = vectorizer.fit_transform(train_df.preprocessed)\nfeatures = vectorizer.get_feature_names()","a0b0bd6e":"visualizer = FreqDistVisualizer(features=features, orient=\"v\", size=(1920, 1080))\nvisualizer.fit(docs)\nvisualizer.ax.set_xlabel(\"Tokens\", fontsize=22)\nvisualizer.ax.set_ylabel(\"Frequency [#]\", fontsize=22)\nvisualizer.ax.tick_params(axis='x', labelsize=16, rotation=60)\nvisualizer.ax.tick_params(axis='y', labelsize=16)\nvisualizer.set_title(\"Frequency Distribution of Top 50 tokens\")\nyellowbrick.style.rcmod.set_style(rc={\"legend.fontsize\": 32})","90a91037":"!pip install wordcloud","cecd3b72":"from wordcloud import WordCloud\n\ntext = \" \".join(article for article in train_df.preprocessed)\nwordcloud = WordCloud(\n    max_font_size=50, max_words=100, background_color=\"white\", width=250, height=180\n).generate(text)\nplt.figure(figsize=(16, 10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","58ae8c54":"le = LabelEncoder()\nlabels = le.fit_transform(train_df.cleaned_label)","11347d69":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit","c6852b60":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","a651d8d7":"scorings = {\n    \"jaccard\": \"jaccard_weighted\",\n    \"accuracy\": \"accuracy\",\n    \"balanced_accuracy\": \"balanced_accuracy\",\n    \"precision\": \"precision_weighted\",\n    \"recall\": \"recall_weighted\",\n    \"f1\": \"f1_weighted\",\n    \"roc_auc\": \"roc_auc_ovr_weighted\",\n}","81958e70":"tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2))\nlr = LogisticRegression(class_weight='balanced', solver='liblinear')\npipe = make_pipeline(tfidf, lr)","75309d56":"res = cross_validate(pipe, train_df.preprocessed, train_df.cleaned_label, cv=cv, scoring=scorings, n_jobs=-1)","6fedd50d":"for scoring, scores in res.items():\n    if not scoring.endswith(\"time\"):\n        print(\n            f\"{' '.join(scoring.split('_')[1:])}: {scores.mean():0.2f} (+\/- {scores.std() * 2:0.2f})\"\n        )","5c668c1f":"# Actual training\npipe.fit(train_df.preprocessed, y=train_df.cleaned_label)","4de47cf4":"import joblib","bb1af5ce":"filename = \".\/baseline_lr_model.pkl\"","922a278e":"joblib.dump(pipe, filename)","08386b1b":"baseline_model = joblib.load(filename)","904d9dc8":"sample_sub_preprocessed = preprocess(sample_sub.text.str.lower())","ea4fd593":"sample_sub['PredictionString'] = baseline_model.predict(sample_sub_preprocessed)","de154a39":"sample_sub.drop(columns=['text']).to_csv('submission.csv', index=False)","d060b615":"## Text cleaning","51d4d453":"## Retrieve introduction \/ abstract","77120823":"# Libraries installation and import","f5f07f16":"# Exploratory Data Analysis","5a3619b8":"## Save model","646c9a12":"## Wordcloud representation","e84952ac":"# Baseline","c7f5145d":"# Dataset","39a2162f":"## Labels distribution\n\nCheck labels balance and unicity","b212b6e2":"##  Load model","b20e2c00":"## Token frequency distribution","0413d95b":"## Predictions","b5aa1deb":"## Logistic regression","86a1acbf":"## Sample  size"}}