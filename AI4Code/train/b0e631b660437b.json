{"cell_type":{"f27e700b":"code","5c61191a":"code","172c59f7":"code","49b6a910":"code","c4c54dbe":"code","fee8cc6c":"code","34934d35":"code","b6ce8be9":"code","5753eff9":"code","549e9b19":"code","ddababd5":"code","d17b7289":"code","337ea197":"code","ad40081e":"code","1a9ac5a0":"code","4cacf115":"code","6e11a439":"code","3d57300e":"code","cce696a8":"markdown","31fd1ddc":"markdown"},"source":{"f27e700b":"# hide\n!pip install deepspeed\n!pip install --upgrade wandb\nimport copy\nfrom functools import partial\nimport multiprocessing as mp\nfrom pathlib import Path\nfrom typing import Any, Callable, List, Tuple\n\nfrom deepspeed.ops.adam import FusedAdam\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch.distributions.bernoulli import Bernoulli\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import io, transforms\n# from torchvision.utils import Image, ImageDraw\nfrom torchvision.transforms.functional import to_pil_image\nfrom tqdm.auto import tqdm","5c61191a":"# Wandb login:\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\nuser_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"wandb_api_key\")\nwandb.login(key=secret_value)","172c59f7":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n%matplotlib inline\nprint(torch.__version__, pl.__version__, wandb.__version__)","49b6a910":"# Image parameters\nTRAIN_FILES = \"..\/input\/portrait-paintings\/Images\"\nIMAGE_SIZE = 256\nPATCH_SIZE = 16\nZERO_PCT = 0.1\nPATCHES_PER_ROW = (IMAGE_SIZE \/\/ PATCH_SIZE)\nNUM_PATCHES = PATCHES_PER_ROW ** 2\nRGB_CHANNELS = 3\nNUM_PIXELS = PATCH_SIZE ** 2 * RGB_CHANNELS\nVALID_IMAGES = 5\nTOPK = 10      \n\n# Training parameters\nBATCH_SIZE = 16\nEPOCHS = 10\nLR = 1e-4\n\n# Transformer parameters\nN_HEADS = 8\nN_LAYERS = 6\n\n# Update constants\nTEMPERATURE_S = 0.1\nTEMPERATURE_T = 0.05\nCENTER_MOMENTUM = 0.9\nTEACHER_MOMENTUM = 0.995","c4c54dbe":"class ImageData(Dataset):\n    def __init__(self, files: List[str]):\n        self.files = files\n        self.randcrop_big = transforms.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE), scale=(0.5, 1.0))\n        self.randcrop_small = transforms.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE))\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, i):\n        img = io.read_image(self.files[i])\n        img1 = self.randcrop_big(img)\n        img2 = self.randcrop_small(img)\n        if img.shape[0] == 1:\n            img1 = torch.cat([img1]*3)\n            img2 = torch.cat([img2]*3)\n\n        return img1, img2\n\n\nclass CollateFn:\n    def reshape(self, batch):\n        patches = torch.stack(batch)\\\n                    .unfold(2, PATCH_SIZE, PATCH_SIZE)\\\n                    .unfold(3, PATCH_SIZE, PATCH_SIZE)\n\n        num_images = len(patches)\n        patches = patches.reshape(\n            num_images,\n            RGB_CHANNELS, \n            NUM_PATCHES, \n            PATCH_SIZE, \n            PATCH_SIZE\n        )\n        patches.transpose_(1, 2)\n        \n        return patches.reshape(num_images, NUM_PATCHES, -1) \/ 255.0 - 0.5\n        \n    def __call__(\n        self, batch: List[Tuple[torch.Tensor, torch.Tensor]]\n    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n        x1, x2 = zip(*batch)\n\n        return self.reshape(x1), self.reshape(x2)","fee8cc6c":"# hide\nclass ImageOriginalData(Dataset):\n    def __init__(self, files: List[str]):\n        self.files = files\n        self.resize = transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, i):\n        img = io.read_image(self.files[i])\n        if img.shape[0] == 1:\n            img = torch.cat([img]*3)\n\n        return self.resize(img)\n    \nclass CollateSingleImage(CollateFn):    \n    def __call__(\n        self, batch: List[torch.Tensor]\n    ) -> torch.FloatTensor:\n        return self.reshape(batch)\n    \nfiles = [str(file) for file in Path(TRAIN_FILES).glob(\"*.jpg\")]\ntrain_files, valid_files = train_test_split(files, test_size=0.15, random_state=42)\n\ntrain_data = ImageData(train_files)\ntrain_dl = DataLoader(\n    train_data, \n    BATCH_SIZE, \n    shuffle=True, \n    drop_last=True, \n    num_workers=4,\n    pin_memory=True,\n    collate_fn=CollateFn(),\n)\n\nvalid_data = ImageOriginalData(valid_files)\nvalid_dl = DataLoader(\n    valid_data, \n    BATCH_SIZE*2, \n    shuffle=False, \n    drop_last=False, \n    num_workers=4,\n    pin_memory=True,\n    collate_fn=CollateSingleImage(),\n)","34934d35":"x, y = next(iter(train_dl))\nx2 = next(iter(valid_dl))\n(x.shape, y.shape), (x2.shape)","b6ce8be9":"class Model(nn.Module):\n    def __init__(self, d_model, n_head, n_layers):\n        super().__init__()\n        # transformer\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head)\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n\n        # positional embedding\n        w_pos = torch.randn(NUM_PATCHES, d_model) \/ d_model ** 0.5\n        cls_token = torch.randn(1, d_model) \/ d_model ** 0.5\n        self.register_parameter(\"pos_embed\", nn.Parameter(w_pos))\n        self.register_parameter(\"cls_token\", nn.Parameter(cls_token))\n\n        # pixel projection\n        self.linear = nn.Linear(2 * d_model, d_model)\n        self.norm1 = nn.LayerNorm(2 * d_model, elementwise_affine=False)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        batch_size = len(x)\n        position = torch.stack([self.pos_embed] * batch_size)\n        x = torch.cat([x, position], dim=-1)\n        pixel_proj = self.norm2(F.relu(self.linear(self.norm1(x))))\n        batched_cls_token = torch.stack([self.cls_token]*batch_size)\n        cls_x = torch.cat([batched_cls_token, pixel_proj], dim=1)\n        \n        cls_x.transpose_(0, 1)\n        return F.normalize(self.encoder(cls_x)[0, ...], dim=-1)","5753eff9":"class HLoss:\n    def __init__(self, temperature_t: float, temperature_s: float):\n        self.temperature_t = temperature_t\n        self.temperature_s = temperature_s\n        \n    def __call__(\n        self, \n        t: torch.FloatTensor, \n        s: torch.FloatTensor, \n        center: torch.FloatTensor\n    ) -> torch.FloatTensor:\n        t = F.softmax((t.detach() - center) \/ self.temperature_t, dim=1)\n        log_s = F.log_softmax(s \/ self.temperature_s, dim=1)\n\n        return -(t * log_s).sum(dim=1).mean()","549e9b19":"# hide\nresize = transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))\n\ndef get_closest(embedding: torch.FloatTensor, i: int):\n    similarity = embedding @ embedding[i,:].T\n    scores, idx = similarity.topk(TOPK)\n    return scores.cpu().numpy(), idx.cpu().numpy()\n\ndef get_closest_wandb_images(embedding: torch.FloatTensor, i: int, files: List[str]):\n    main_img = to_pil_image(resize(io.read_image(files[i])))\n    closest_imgs = [wandb.Image(main_img)]\n    \n    scores, idx = get_closest(embedding, i)\n    \n    for i, score in zip(idx, scores):\n        img = to_pil_image(resize(io.read_image(files[i])))\n        closest_imgs.append(\n            wandb.Image(img, caption=f\"{score:.4f}\")\n        )\n        \n    return closest_imgs","ddababd5":"class LightningModel(pl.LightningModule):\n    def __init__(\n        self,\n        teacher: nn.Module,\n        lr: float,\n        loss_fn: Callable,\n        valid_files: List[str],\n        dim: int,\n        center_momentum: float,\n        param_momentum: float,\n    ):\n        super().__init__()\n        self.teacher = teacher\n        self.student = copy.deepcopy(teacher)\n        self.lr = lr\n        self.loss_fn = loss_fn\n        self.c_mom = center_momentum\n        self.p_mom = param_momentum\n        self.register_buffer(\"center\", torch.zeros((1, dim)).float())\n        self.valid_files = valid_files\n        \n        for p in self.teacher.parameters():\n            p.requires_grad = False\n\n    def loss_calculation(\n        self,\n        batch: Tuple[torch.FloatTensor, torch.FloatTensor],\n    ) -> torch.FloatTensor:\n        x1, x2 = batch\n        \n        s1, s2 = self.student(x1), self.student(x2)\n        t1, t2 = self.teacher(x1), self.teacher(x2)\n        \n        loss = self.loss_fn(t1, s2, self.center) + self.loss_fn(t2, s1, self.center)\n        \n        emperical_center = F.normalize(\n            torch.cat([t1, t2]).mean(dim=0, keepdims=True),\n            dim=-1,\n        )\n        return loss, emperical_center\n\n    def training_step(\n        self, batch: Tuple[torch.FloatTensor, torch.FloatTensor], *args: List[Any]\n    ) -> torch.Tensor:\n        loss, emperical_center = self.loss_calculation(batch)\n        self.log(name=\"Training loss\", value=loss, on_step=True, on_epoch=True)\n        \n        self.center = F.normalize(\n            self.c_mom * self.center + (1 - self.c_mom) * emperical_center,\n            dim=-1,\n        )\n        for s_p, t_p in zip(self.student.parameters(), self.teacher.parameters()):\n            t_p.data = self.p_mom * t_p.data + (1 - self.p_mom) * s_p.data\n            \n        return loss\n    \n    def validation_step(self, images: torch.FloatTensor, *args: List[Any]) -> None:\n        return self.teacher(images)\n        \n    def validation_epoch_end(self, validation_step_outputs):\n        valid_embeds = torch.cat([pred for pred in validation_step_outputs])\n        columns = [\"image\"] + [f\"closest_{i+1}\" for i in range(TOPK)]\n        indices = np.random.choice(len(self.valid_files), VALID_IMAGES, replace=False)\n        rows = [get_closest_wandb_images(valid_embeds, i, self.valid_files) for i in indices]\n        table = wandb.Table(data=rows, columns=columns)\n        self.logger.experiment.log({f\"epoch {self.current_epoch} results\": table})\n        \n    def on_after_backward(self):\n        if self.trainer.global_step % 50 == 0:  # don't make the tf file huge\n            global_step = self.trainer.global_step\n            for name, param in self.student.named_parameters():\n                if \"weight\" in name and not \"norm\" in name and param.requires_grad:\n                    self.logger.experiment.log(\n                        {f\"{name}_grad\": wandb.Histogram(param.grad.cpu())}\n                    )\n\n    def configure_optimizers(self) -> torch.optim.Optimizer:\n        return FusedAdam(self.student.parameters(), lr=self.lr)","d17b7289":"!mkdir \/kaggle\/working\/logs\nteacher = Model(NUM_PIXELS, N_HEADS, N_LAYERS)\nh_loss = HLoss(TEMPERATURE_T, TEMPERATURE_S)\nlightning_model = LightningModel(\n    teacher, \n    LR,\n    h_loss,\n    valid_files,\n    NUM_PIXELS,\n    CENTER_MOMENTUM, \n    TEACHER_MOMENTUM,\n)\n\nlogger = WandbLogger(\"DINO\", \"\/kaggle\/working\/logs\/\", project=\"DINO\")\ntrainer = pl.Trainer(\n    max_epochs=EPOCHS,\n    gpus=torch.cuda.device_count(),\n    gradient_clip_val=1.0,\n    logger=logger,\n    precision=16,\n#     limit_train_batches=10,\n    num_sanity_val_steps=0,\n)\ntrainer.fit(lightning_model, train_dl, valid_dl)","337ea197":"# hide\nimage_orig_data = ImageOriginalData(files)\nimage_orig_dl = DataLoader(\n    image_orig_data, \n    BATCH_SIZE*2, \n    shuffle=False, \n    drop_last=False, \n    num_workers=4,\n    pin_memory=True,\n    collate_fn=CollateSingleImage(),\n)","ad40081e":"teacher = teacher.eval().to(device)\nembedding = []\nwith torch.no_grad():\n    for x in tqdm(image_orig_dl):\n        out = teacher(x.to(device))\n        embedding.append(out.cpu())\n        \n    embedding = torch.cat(embedding, dim=0)","1a9ac5a0":"def plot_closest_pairs(embedding, i, files):\n    img = to_pil_image(resize(io.read_image(files[i])))\n    plt.imshow(img)\n    scores, idx = get_closest(embedding, i)\n    print(idx)\n    print(scores)\n    fig, axs = plt.subplots(2,5, figsize=(12,5))\n    for j in range(10):\n        i=idx[j]\n        score=scores[j]\n        r=j\/\/5\n        c=j%5\n        img = to_pil_image(resize(io.read_image(files[i])))\n        axs[r][c].imshow(img)\n        axs[r][c].axis(\"off\")\n        axs[r][c].set_title(f\"{score:.4f}\")\n\n    plt.show()","4cacf115":"i = 10\nplot_closest_pairs(embedding, i, files)","6e11a439":"i = 20\nplot_closest_pairs(embedding, i, files)","3d57300e":"i = 30\nplot_closest_pairs(embedding, i, files)","cce696a8":"##### This notebook referred to the greatest sachin's notebook \"DINO Self Supervised Vision Transformers\".\nhttps:\/\/www.kaggle.com\/sachin\/dino-self-supervised-vision-transformers\n\n# What is DINO?\n##### PAPER: \"Emerging Properties in Self-Supervised Vision Transformers\", released in 2021\nhttps:\/\/arxiv.org\/pdf\/2104.14294.pdf<br\/>\n- Self-supervised ViT features contain explicit information about the semantic segmentation of an image\n- These features are also excellent k-NN classifiers\n\n##### GitHub: Self-Supervised Vision Transformers with DINO\nhttps:\/\/github.com\/facebookresearch\/dino\n\n##### YouTube: DINO: Emerging Properties in Self-Supervised Vision Transformers (Facebook AI Research Explained)\nhttps:\/\/www.youtube.com\/watch?v=h3ij3F3cPIk<br\/>","31fd1ddc":"# Portrait Recognition by DINO Vision Transformer"}}