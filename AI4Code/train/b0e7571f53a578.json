{"cell_type":{"69591d0c":"code","41c89d30":"code","0704fe50":"code","22620150":"code","a8577b9e":"code","3fc300ea":"code","cf9f0b46":"code","3c3014af":"code","c1116bbb":"code","ea6fb177":"code","8272b7ec":"code","8cca2d0a":"code","8bb5a71c":"code","b5bba454":"code","b2a21531":"code","05379a3e":"code","258fcde9":"code","580ff650":"code","2c778054":"code","0a5b5081":"code","f5f701b6":"code","8885811d":"code","ef16d919":"code","4e342d22":"code","e497f65f":"code","7fdad7e4":"code","f60f7e10":"code","4eab56ad":"code","fabb3cfb":"code","5287ead1":"code","b65a3d32":"code","5f867cd8":"code","5500c720":"code","83360691":"code","1442aea4":"code","a61de973":"code","8052e6d8":"code","804f098e":"code","8534b725":"code","cb19feb3":"code","d5a6f978":"code","4c8f54b3":"code","a5348042":"code","8c93c874":"code","65568db1":"code","9e3a80f8":"code","6b7a8890":"code","f1575848":"code","f418374e":"code","fcaabd1d":"code","f001757f":"markdown","e404378f":"markdown","41867f46":"markdown","37b07341":"markdown"},"source":{"69591d0c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","41c89d30":"df = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\",encoding='ISO-8859-1')","0704fe50":"df.head()","22620150":"#droping the unwanted columns\ndf.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'], axis='columns', inplace=True)","a8577b9e":"df.head()","3fc300ea":"df.shape","cf9f0b46":"df['v1'].unique()","3c3014af":"df['v1'].value_counts()","c1116bbb":"#changing the v1 coulmn to 0 and 1\n#ham is represented sa 0\n#spam is represented as 1\n\n\ndf['v1']=df['v1'].replace('ham', 0)\ndf['v1']=df['v1'].replace('spam', 1)","ea6fb177":"df['v1'].unique()","8272b7ec":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nsns.countplot(df['v1'])","8cca2d0a":"from nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nfrom nltk.corpus import stopwords","8bb5a71c":"import re\nps=PorterStemmer()\n\ncorpus_stem=[]\nfor i in range(len(df)):\n    review=re.sub('[^a-zA_Z]',' ',df['v2'][i])\n    review=review.lower()\n    review=review.split()\n    review=[ps.stem(word) for word in review if word not in set(stopwords.words('english'))]\n    review=' '.join(review)\n    corpus_stem.append(review)\n    \ncorpus_stem","b5bba454":"lem=WordNetLemmatizer()\n\ncorpus_lem=[]\nfor i in range(len(df)):\n    review=re.sub('[^a-zA_Z]',' ',df['v2'][i])\n    review=review.lower()\n    review=review.split()\n    review=[lem.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n    review=' '.join(review)\n    corpus_lem.append(review)\n    \ncorpus_lem","b2a21531":"from sklearn.feature_extraction.text import CountVectorizer\nCV=CountVectorizer(max_features=5000)\nx_lem=CV.fit_transform(corpus_lem).toarray()\nx_lem","05379a3e":"x_lem.shape","258fcde9":"x_stem=CV.fit_transform(corpus_stem).toarray()\nx_stem","580ff650":"x_stem.shape","2c778054":"#TF-IDF for stemming\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntf_stem=TfidfVectorizer()\nx_tf_stem=tf_stem.fit_transform(corpus_stem)\nprint(x_tf_stem.shape)\n","0a5b5081":"#TF-IDF for lemmetizing\ntf_lem=TfidfVectorizer()\nx_tf_lem=tf_stem.fit_transform(corpus_lem)\nprint(x_tf_lem.shape)","f5f701b6":"y=df['v1']\ny[:5]","8885811d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(x_stem,y,test_size=0.2)","ef16d919":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","4e342d22":"from sklearn.naive_bayes import MultinomialNB\nmodel=MultinomialNB().fit(X_train,y_train)","e497f65f":"model.score(X_test,y_test)","7fdad7e4":"from sklearn.metrics import confusion_matrix\ny_pred=model.predict(X_test)\ncm=confusion_matrix(y_test,y_pred)\ncm","f60f7e10":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","4eab56ad":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted value')\nplt.ylabel('True value')","fabb3cfb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(x_lem,y,test_size=0.2)","5287ead1":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","b65a3d32":"from sklearn.naive_bayes import MultinomialNB\nmodel=MultinomialNB().fit(X_train,y_train)","5f867cd8":"model.score(X_test,y_test)","5500c720":"model.score(X_test,y_test)","83360691":"from sklearn.metrics import confusion_matrix\ny_pred=model.predict(X_test)\ncm=confusion_matrix(y_test,y_pred)\ncm","1442aea4":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","a61de973":"sns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted value')\nplt.ylabel('True value')","8052e6d8":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(x_tf_stem,y,test_size=0.2)","804f098e":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","8534b725":"from sklearn.naive_bayes import MultinomialNB\nmodel=MultinomialNB().fit(X_train,y_train)","cb19feb3":"model.score(X_test,y_test)","d5a6f978":"from sklearn.metrics import confusion_matrix\ny_pred=model.predict(X_test)\ncm=confusion_matrix(y_test,y_pred)\ncm","4c8f54b3":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","a5348042":"sns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted value')\nplt.ylabel('True value')","8c93c874":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(x_tf_lem,y,test_size=0.2)","65568db1":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","9e3a80f8":"from sklearn.naive_bayes import MultinomialNB\nmodel=MultinomialNB().fit(X_train,y_train)","6b7a8890":"model.score(X_test,y_test)","f1575848":"from sklearn.metrics import confusion_matrix\ny_pred=model.predict(X_test)\ncm=confusion_matrix(y_test,y_pred)\ncm","f418374e":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","fcaabd1d":"sns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted value')\nplt.ylabel('True value')","f001757f":"### **making the model by using countvectorizer and stemming**","e404378f":"### making the model by using TF-IDF and stemming","41867f46":"### making the model by using lemmetizing and countvectorizer","37b07341":"### making the model by using TF-IDF and lemmetizing"}}