{"cell_type":{"e2308b0a":"code","60e79349":"code","b51aa3de":"code","85739caa":"code","e0d4c260":"code","afb3ace1":"code","237fc38e":"code","5ae0b5d0":"code","3a7c5354":"code","df982e09":"code","a79cfe61":"code","32a351dd":"code","510c848e":"code","beb5f51b":"code","5d933788":"code","b37d9350":"code","0fe3e9d1":"code","c0beb2ac":"code","025a2f4d":"code","21a02ebf":"code","7bd5f393":"code","4f69e478":"markdown","46b7a3d9":"markdown"},"source":{"e2308b0a":"import torch\n\n\ndef ppo_step(policy_net, value_net, optimizer_policy, optimizer_value, optim_value_iternum, states, actions,\n             returns, advantages, fixed_log_probs, clip_epsilon, l2_reg):\n\n    \"\"\"update critic\"\"\"\n    for _ in range(optim_value_iternum):\n        values_pred = value_net(states)\n        value_loss = (values_pred - returns).pow(2).mean()\n        # weight decay\n        for param in value_net.parameters():\n            value_loss += param.pow(2).sum() * l2_reg\n        optimizer_value.zero_grad()\n        value_loss.backward()\n        optimizer_value.step()\n\n    \"\"\"update policy\"\"\"\n    log_probs = policy_net.get_log_prob(states, actions)\n    ratio = torch.exp(log_probs - fixed_log_probs)\n    surr1 = ratio * advantages\n    surr2 = torch.clamp(ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * advantages\n    policy_surr = -torch.min(surr1, surr2).mean()\n    optimizer_policy.zero_grad()\n    policy_surr.backward()\n    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 40)\n    optimizer_policy.step()","60e79349":"import numpy as np\nimport random\nfrom collections import namedtuple\n\ntensor = torch.tensor\nDoubleTensor = torch.DoubleTensor\nFloatTensor = torch.FloatTensor\nLongTensor = torch.LongTensor\nByteTensor = torch.ByteTensor\nones = torch.ones\nzeros = torch.zeros\n\n\ndef to_device(device, *args):\n    return [x.to(device) for x in args]\n\n\ndef get_flat_params_from(model):\n    params = []\n    for param in model.parameters():\n        params.append(param.view(-1))\n\n    flat_params = torch.cat(params)\n    return flat_params\n\n\ndef set_flat_params_to(model, flat_params):\n    prev_ind = 0\n    for param in model.parameters():\n        flat_size = int(np.prod(list(param.size())))\n        param.data.copy_(\n            flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n        prev_ind += flat_size\n\n\ndef get_flat_grad_from(inputs, grad_grad=False):\n    grads = []\n    for param in inputs:\n        if grad_grad:\n            grads.append(param.grad.grad.view(-1))\n        else:\n            if param.grad is None:\n                grads.append(zeros(param.view(-1).shape))\n            else:\n                grads.append(param.grad.view(-1))\n\n    flat_grad = torch.cat(grads)\n    return flat_grad\n\n\ndef compute_flat_grad(output, inputs, filter_input_ids=set(), retain_graph=False, create_graph=False):\n    if create_graph:\n        retain_graph = True\n\n    inputs = list(inputs)\n    params = []\n    for i, param in enumerate(inputs):\n        if i not in filter_input_ids:\n            params.append(param)\n\n    grads = torch.autograd.grad(output, params, retain_graph=retain_graph, create_graph=create_graph)\n\n    j = 0\n    out_grads = []\n    for i, param in enumerate(inputs):\n        if i in filter_input_ids:\n            out_grads.append(zeros(param.view(-1).shape, device=param.device, dtype=param.dtype))\n        else:\n            out_grads.append(grads[j].view(-1))\n            j += 1\n    grads = torch.cat(out_grads)\n\n    for param in params:\n        param.grad = None\n    return grads\n\n\ndef estimate_advantages(rewards, masks, values, gamma, tau):\n    rewards, masks, values = to_device(torch.device('cpu'), rewards, masks, values)\n    tensor_type = type(rewards)\n    deltas = tensor_type(rewards.size(0), 1)\n    advantages = tensor_type(rewards.size(0), 1)\n\n    prev_value = 0\n    prev_advantage = 0\n    for i in reversed(range(rewards.size(0))):\n        deltas[i] = rewards[i] + gamma * prev_value * masks[i] - values[i]\n        advantages[i] = deltas[i] + gamma * tau * prev_advantage * masks[i]\n\n        prev_value = values[i, 0]\n        prev_advantage = advantages[i, 0]\n\n    returns = values + advantages\n    advantages = (advantages - advantages.mean()) \/ advantages.std()\n\n    advantages, returns = to_device(torch.device(\"cpu\"), advantages, returns)\n    return advantages, returns\n\ndef normal_entropy(std):\n    var = std.pow(2)\n    entropy = 0.5 + 0.5 * torch.log(2 * var * math.pi)\n    return entropy.sum(1, keepdim=True)\n\n\ndef normal_log_density(x, mean, log_std, std):\n    var = std.pow(2)\n    log_density = -(x - mean).pow(2) \/ (2 * var) - 0.5 * math.log(2 * math.pi) - log_std\n    return log_density.sum(1, keepdim=True)\n\nTransition = namedtuple('Transition', ('state', 'action', 'mask', 'next_state',\n                                       'reward'))\n\n\nclass Memory(object):\n    def __init__(self):\n        self.memory = []\n\n    def push(self, *args):\n        \"\"\"Saves a transition.\"\"\"\n        self.memory.append(Transition(*args))\n\n    def sample(self, batch_size=None):\n        if batch_size is None:\n            return Transition(*zip(*self.memory))\n        else:\n            random_batch = random.sample(self.memory, batch_size)\n            return Transition(*zip(*random_batch))\n\n    def append(self, new_memory):\n        self.memory += new_memory.memory\n\n    def __len__(self):\n        return len(self.memory)\n    \nclass RunningStat(object):\n    def __init__(self, shape):\n        self._n = 0\n        self._M = np.zeros(shape)\n        self._S = np.zeros(shape)\n\n    def push(self, x):\n        x = np.asarray(x)\n        assert x.shape == self._M.shape\n        self._n += 1\n        if self._n == 1:\n            self._M[...] = x\n        else:\n            oldM = self._M.copy()\n            self._M[...] = oldM + (x - oldM) \/ self._n\n            self._S[...] = self._S + (x - oldM) * (x - self._M)\n\n    @property\n    def n(self):\n        return self._n\n\n    @property\n    def mean(self):\n        return self._M\n\n    @property\n    def var(self):\n        return self._S \/ (self._n - 1) if self._n > 1 else np.square(self._M)\n\n    @property\n    def std(self):\n        return np.sqrt(self.var)\n\n    @property\n    def shape(self):\n        return self._M.shape\n\n\nclass ZFilter:\n    \"\"\"\n    y = (x-mean)\/std\n    using running estimates of mean,std\n    \"\"\"\n\n    def __init__(self, shape, demean=True, destd=True, clip=10.0):\n        self.demean = demean\n        self.destd = destd\n        self.clip = clip\n\n        self.rs = RunningStat(shape)\n        self.fix = False\n\n    def __call__(self, x, update=True):\n        if update and not self.fix:\n            self.rs.push(x)\n        if self.demean:\n            x = x - self.rs.mean\n        if self.destd:\n            x = x \/ (self.rs.std + 1e-8)\n        if self.clip:\n            x = np.clip(x, -self.clip, self.clip)\n        return x","b51aa3de":"import multiprocessing\nimport math\nimport time\n\n\ndef collect_samples(pid, queue, env, policy, custom_reward,\n                    mean_action, render, running_state, min_batch_size):\n    torch.randn(pid)\n    log = dict()\n    memory = Memory()\n    num_steps = 0\n    total_reward = 0\n    min_reward = 1e6\n    max_reward = -1e6\n    total_c_reward = 0\n    min_c_reward = 1e6\n    max_c_reward = -1e6\n    num_episodes = 0\n\n    while num_steps < min_batch_size:\n        state = reset(env)\n        if running_state is not None:\n            state = running_state(state)\n        reward_episode = 0\n\n        for t in range(10000):\n            state_var = DoubleTensor(state).unsqueeze(0)\n            with torch.no_grad():\n                if mean_action:\n                    action = policy(state_var)[0][0].numpy()\n                else:\n                    action = policy.select_action(state_var)[0].numpy()\n            action = int(action) if policy.is_disc_action else action.astype(np.float64)\n            next_state, reward, done, _ = step(action, env)\n            reward_episode += reward\n            if running_state is not None:\n                next_state = running_state(next_state)\n\n            if custom_reward is not None:\n                reward = custom_reward(state, to_categorical(action, 19))\n                total_c_reward += reward\n                min_c_reward = min(min_c_reward, reward)\n                max_c_reward = max(max_c_reward, reward)\n\n            mask = 0 if done else 1\n\n            memory.push(state, action, mask, next_state, reward)\n\n            if render:\n                env.render()\n            if done:\n                break\n\n            state = next_state\n\n        # log stats\n        num_steps += (t + 1)\n        num_episodes += 1\n        total_reward += reward_episode\n        min_reward = min(min_reward, reward_episode)\n        max_reward = max(max_reward, reward_episode)\n\n    log['num_steps'] = num_steps\n    log['num_episodes'] = num_episodes\n    log['total_reward'] = total_reward\n    log['avg_reward'] = total_reward \/ num_episodes\n    log['max_reward'] = max_reward\n    log['min_reward'] = min_reward\n    if custom_reward is not None:\n        log['total_c_reward'] = total_c_reward\n        log['avg_c_reward'] = total_c_reward \/ num_steps\n        log['max_c_reward'] = max_c_reward\n        log['min_c_reward'] = min_c_reward\n\n    if queue is not None:\n        queue.put([pid, memory, log])\n    else:\n        return memory, log\n\n\ndef merge_log(log_list):\n    log = dict()\n    log['total_reward'] = sum([x['total_reward'] for x in log_list])\n    log['num_episodes'] = sum([x['num_episodes'] for x in log_list])\n    log['num_steps'] = sum([x['num_steps'] for x in log_list])\n    log['avg_reward'] = log['total_reward'] \/ log['num_episodes']\n    log['max_reward'] = max([x['max_reward'] for x in log_list])\n    log['min_reward'] = min([x['min_reward'] for x in log_list])\n    if 'total_c_reward' in log_list[0]:\n        log['total_c_reward'] = sum([x['total_c_reward'] for x in log_list])\n        log['avg_c_reward'] = log['total_c_reward'] \/ log['num_steps']\n        log['max_c_reward'] = max([x['max_c_reward'] for x in log_list])\n        log['min_c_reward'] = min([x['min_c_reward'] for x in log_list])\n\n    return log\n\n\nclass Agent:\n\n    def __init__(self, env, policy, custom_reward=None,\n                 mean_action=False, render=False, running_state=None, num_threads=1):\n        self.env = env\n        self.policy = policy\n        self.custom_reward = custom_reward\n        self.mean_action = mean_action\n        self.running_state = running_state\n        self.render = render\n        self.num_threads = num_threads\n\n    def collect_samples(self, min_batch_size):\n        t_start = time.time()\n        to_device(torch.device('cpu'), self.policy)\n        thread_batch_size = int(math.floor(min_batch_size \/ self.num_threads))\n        queue = multiprocessing.Queue()\n        workers = []\n\n        for i in range(self.num_threads-1):\n            worker_args = (i+1, queue, self.env, self.policy, self.custom_reward, self.mean_action,\n                           False, self.running_state, thread_batch_size)\n            workers.append(multiprocessing.Process(target=collect_samples, args=worker_args))\n        for worker in workers:\n            worker.start()\n\n        memory, log = collect_samples(0, None, self.env, self.policy, self.custom_reward, self.mean_action,\n                                      self.render, self.running_state, thread_batch_size)\n\n        worker_logs = [None] * len(workers)\n        worker_memories = [None] * len(workers)\n        for _ in workers:\n            pid, worker_memory, worker_log = queue.get()\n            worker_memories[pid - 1] = worker_memory\n            worker_logs[pid - 1] = worker_log\n        for worker_memory in worker_memories:\n            memory.append(worker_memory)\n        batch = memory.sample()\n        if self.num_threads > 1:\n            log_list = [log] + worker_logs\n            log = merge_log(log_list)\n        to_device(torch.device('cpu'), self.policy)\n        t_end = time.time()\n        log['sample_time'] = t_end - t_start\n        log['action_mean'] = np.mean(np.vstack(batch.action), axis=0)\n        log['action_min'] = np.min(np.vstack(batch.action), axis=0)\n        log['action_max'] = np.max(np.vstack(batch.action), axis=0)\n        return batch, log","85739caa":"# Update kaggle-environments to the newest version.\n!pip3 install kaggle-environments -U\n\n# GFootball environment.\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n\n# Make sure that the Branch in git clone and in wget call matches !!\n!git clone -b v2.7 https:\/\/github.com\/google-research\/football.git\n!mkdir -p football\/third_party\/gfootball_engine\/lib\n\n!wget https:\/\/storage.googleapis.com\/gfootball\/prebuilt_gameplayfootball_v2.7.so -O football\/third_party\/gfootball_engine\/lib\/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .","e0d4c260":"import torch.nn as nn\nimport torch\n\n\nclass Value(nn.Module):\n    def __init__(self, state_dim, hidden_size=(128, 128), activation='tanh'):\n        super().__init__()\n        if activation == 'tanh':\n            self.activation = torch.tanh\n        elif activation == 'relu':\n            self.activation = torch.relu\n        elif activation == 'sigmoid':\n            self.activation = torch.sigmoid\n\n        self.affine_layers = nn.ModuleList()\n        last_dim = state_dim\n        for nh in hidden_size:\n            self.affine_layers.append(nn.Linear(last_dim, nh))\n            last_dim = nh\n\n        self.value_head = nn.Linear(last_dim, 1)\n        self.value_head.weight.data.mul_(0.1)\n        self.value_head.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        for affine in self.affine_layers:\n            x = self.activation(affine(x))\n\n        value = self.value_head(x)\n        return value\nclass Discriminator(nn.Module):\n    def __init__(self, num_inputs, hidden_size=(128, 128), activation='tanh'):\n        super().__init__()\n        if activation == 'tanh':\n            self.activation = torch.tanh\n        elif activation == 'relu':\n            self.activation = torch.relu\n        elif activation == 'sigmoid':\n            self.activation = torch.sigmoid\n\n        self.affine_layers = nn.ModuleList()\n        last_dim = num_inputs\n        for nh in hidden_size:\n            self.affine_layers.append(nn.Linear(last_dim, nh))\n            last_dim = nh\n\n        self.logic = nn.Linear(last_dim, 1)\n        self.logic.weight.data.mul_(0.1)\n        self.logic.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        for affine in self.affine_layers:\n            x = self.activation(affine(x))\n\n        prob = torch.sigmoid(self.logic(x))\n        return prob\nclass Policy(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_size=(128, 128), activation='tanh', log_std=0):\n        super().__init__()\n        self.is_disc_action = False\n        if activation == 'tanh':\n            self.activation = torch.tanh\n        elif activation == 'relu':\n            self.activation = torch.relu\n        elif activation == 'sigmoid':\n            self.activation = torch.sigmoid\n\n        self.affine_layers = nn.ModuleList()\n        last_dim = state_dim\n        for nh in hidden_size:\n            self.affine_layers.append(nn.Linear(last_dim, nh))\n            last_dim = nh\n\n        self.action_mean = nn.Linear(last_dim, action_dim)\n        self.action_mean.weight.data.mul_(0.1)\n        self.action_mean.bias.data.mul_(0.0)\n\n        self.action_log_std = nn.Parameter(torch.ones(1, action_dim) * log_std)\n\n    def forward(self, x):\n        for affine in self.affine_layers:\n            x = self.activation(affine(x))\n\n        action_mean = self.action_mean(x)\n        action_log_std = self.action_log_std.expand_as(action_mean)\n        action_std = torch.exp(action_log_std)\n\n        return action_mean, action_log_std, action_std\n\n    def select_action(self, x):\n        action_mean, _, action_std = self.forward(x)\n        action = torch.normal(action_mean, action_std)\n        return action\n\n    def get_kl(self, x):\n        mean1, log_std1, std1 = self.forward(x)\n\n        mean0 = mean1.detach()\n        log_std0 = log_std1.detach()\n        std0 = std1.detach()\n        kl = log_std1 - log_std0 + (std0.pow(2) + (mean0 - mean1).pow(2)) \/ (2.0 * std1.pow(2)) - 0.5\n        return kl.sum(1, keepdim=True)\n\n    def get_log_prob(self, x, actions):\n        action_mean, action_log_std, action_std = self.forward(x)\n        return normal_log_density(actions, action_mean, action_log_std, action_std)\n\n    def get_fim(self, x):\n        mean, _, _ = self.forward(x)\n        cov_inv = self.action_log_std.exp().pow(-2).squeeze(0).repeat(x.size(0))\n        param_count = 0\n        std_index = 0\n        id = 0\n        for name, param in self.named_parameters():\n            if name == \"action_log_std\":\n                std_id = id\n                std_index = param_count\n            param_count += param.view(-1).shape[0]\n            id += 1\n        return cov_inv.detach(), mean, {'std_id': std_id, 'std_index': std_index}\n    \nclass DiscretePolicy(nn.Module):\n    def __init__(self, state_dim, action_num, hidden_size=(128, 128), activation='tanh'):\n        super().__init__()\n        self.is_disc_action = True\n        if activation == 'tanh':\n            self.activation = torch.tanh\n        elif activation == 'relu':\n            self.activation = torch.relu\n        elif activation == 'sigmoid':\n            self.activation = torch.sigmoid\n\n        self.affine_layers = nn.ModuleList()\n        last_dim = state_dim\n        for nh in hidden_size:\n            self.affine_layers.append(nn.Linear(last_dim, nh))\n            last_dim = nh\n\n        self.action_head = nn.Linear(last_dim, action_num)\n        self.action_head.weight.data.mul_(0.1)\n        self.action_head.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        for affine in self.affine_layers:\n            x = self.activation(affine(x))\n\n        action_prob = torch.softmax(self.action_head(x), dim=1)\n        return action_prob\n\n    def select_action(self, x):\n        action_prob = self.forward(x)\n        action = action_prob.multinomial(1)\n        return action\n\n    def get_kl(self, x):\n        action_prob1 = self.forward(x)\n        action_prob0 = action_prob1.detach()\n        kl = action_prob0 * (torch.log(action_prob0) - torch.log(action_prob1))\n        return kl.sum(1, keepdim=True)\n\n    def get_log_prob(self, x, actions):\n        action_prob = self.forward(x)\n        return torch.log(action_prob.gather(1, actions.long().unsqueeze(1)))\n\n    def get_fim(self, x):\n        action_prob = self.forward(x)\n        M = action_prob.pow(-1).view(-1).detach()\n        return M, action_prob, {}","afb3ace1":"seed = 0\ngamma = 0.999 #0.95\ntau = 0.95\nl2_reg = 0.95\nlearning_rate = 1e-6 #3e-4\nclip_epsilon = 0.2\nnum_threads = 4\nmin_batch_size = 50000\nmax_iter_num = 1000\nlog_interval = 1\nrender = \"False\"\nlog_std = 0.0\n\n\n\ndtype = torch.float64\ntorch.set_default_dtype(dtype)\ntorch.device('cpu')\n","237fc38e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5ae0b5d0":"def convert_observation(s):\n    s[0]['relative_position'] = np.array(s[0]['left_team'][s[0]['active']]) - np.array(s[0]['ball'][:2])\n    s[0]['absolute_position'] = s[0]['left_team'][s[0]['active']]\n    s[0]['player_direction'] = s[0]['left_team_direction'][s[0]['active']]\n    s[0]['sticky_is_sprint'] = int(s[0]['sticky_actions'][8]==1)\n    closer_enemy = np.argmin(np.sum((np.array(s[0]['absolute_position'])-np.array(s[0]['right_team']))**2, axis=1))\n    s[0]['closer_enemy_relative_position'] = np.array(s[0]['absolute_position'])-np.array(s[0]['right_team'][closer_enemy])\n    s[0]['closer_enemy_direction'] = s[0]['right_team_direction'][closer_enemy]\n    new_s = np.concatenate([np.array(v).astype(np.float32).reshape(-1) if type(v)==list or type(v)==np.ndarray else [float(v)]\n                            for k,v in s[0].items()])\n    return new_s.astype(np.float32)\n\n\"\"\"environment\"\"\"\nfrom kaggle_environments import make\nimport gym\nimport gfootball\nimport gfootball.env as football_env\nenv = football_env.create_environment(env_name=\"11_vs_11_easy_stochastic\", representation =\"raw\", stacked=False, logdir='\/tmp\/football', write_goal_dumps=False, write_full_episode_dumps=False, render=False)\nenv.reset()\nstate_dim = 214\nis_disc_action = True\naction_dim = 1 if is_disc_action else env.action_space.shape[0]\nrunning_state = ZFilter((state_dim,), clip=5)\n# running_reward = ZFilter((1,), demean=False, clip=10)\n","3a7c5354":"def step(action,env):\n    obs, reward,done,info = env.step(action)\n    return convert_observation(obs),reward,done,info\n\ndef reset(env):\n    obs = env.reset()\n    return convert_observation(obs)","df982e09":"import pickle\n\"\"\"seeding\"\"\"\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nenv.seed(seed)\n\n\"\"\"define actor and critic\"\"\"\nif is_disc_action:\n    policy_net = DiscretePolicy(214, 19)\nelse:\n    policy_net = Policy(state_dim, env.action_space.shape[0], log_std=log_std)\nvalue_net = Value(state_dim)\ndiscrim_net = Discriminator(state_dim + env.action_space.n)\ndiscrim_criterion = nn.BCELossWithLogits()\n\noptimizer_policy = torch.optim.Adam(policy_net.parameters(), lr=learning_rate)\noptimizer_value = torch.optim.Adam(value_net.parameters(), lr=learning_rate)\noptimizer_discrim = torch.optim.Adam(discrim_net.parameters(), lr=learning_rate)\n\n# optimization epoch number and batch size for PPO\noptim_epochs = 50\noptim_batch_size = 16\n\n# load trajectory\n\nwith open(\"\/kaggle\/input\/dataexpert\/data_expert.pickle\",\"rb\") as handle:\n    expert_traj_dataset = pickle.load(handle)","a79cfe61":"expert_states = expert_traj_dataset[\"states\"]\nexpert_actions = expert_traj_dataset[\"actions\"]","32a351dd":"expert_states.shape","510c848e":"def to_categorical(y, num_classes):\n    \"\"\" 1-hot encodes a tensor \"\"\"\n    return np.eye(num_classes, dtype='uint8')[y]\n\nexpert_actions = to_categorical(expert_actions, 19)","beb5f51b":"expert_actions","5d933788":"expert_states = torch.from_numpy(expert_states).to(dtype).to(torch.device(\"cpu\"))\nexpert_actions = torch.from_numpy(expert_actions).to(dtype).to(torch.device(\"cpu\"))\n\n","b37d9350":"expert_actions.shape, expert_states.shape","0fe3e9d1":"torch.cat([expert_states, expert_actions], 1)","c0beb2ac":"\ndef expert_reward(state, action):\n    state_action = tensor(np.hstack([state, action]), dtype=dtype)\n    with torch.no_grad():\n        return -math.log(discrim_net(state_action)[0].item()+ 1e-8) + math.log(1 - discrim_net(state_action)[0].item() + 1e-8)\n\n\n\"\"\"create agent\"\"\"\nagent = Agent(env, policy_net,custom_reward=expert_reward)","025a2f4d":"\ndef update_params(batch, i_iter):\n    states = torch.from_numpy(np.stack(batch.state)).to(dtype).to(torch.device(\"cpu\"))\n    actions_one_hot = to_categorical(np.stack(batch.action),19)\n    actions_one_hot = torch.from_numpy(actions_one_hot).to(dtype).to(torch.device(\"cpu\"))\n    actions = torch.from_numpy(np.stack(batch.action)).to(dtype).to(torch.device(\"cpu\"))\n    rewards = torch.from_numpy(np.stack(batch.reward)).to(dtype).to(torch.device(\"cpu\"))\n    masks = torch.from_numpy(np.stack(batch.mask)).to(dtype).to(torch.device(\"cpu\"))\n    with torch.no_grad():\n        values = value_net(states)\n        print(actions.shape)\n        fixed_log_probs = policy_net.get_log_prob(states, actions)\n\n    \"\"\"get advantage estimation from the trajectories\"\"\"\n    advantages, returns = estimate_advantages(rewards, masks, values, gamma, tau)\n\n    \"\"\"update discriminator\"\"\"\n    for _ in range(1):\n        \n        expert_state_actions = torch.cat([expert_states, expert_actions], 1)\n        g_o = discrim_net(torch.cat([states, actions_one_hot], 1))\n        e_o = discrim_net(expert_state_actions)\n        optimizer_discrim.zero_grad()\n        discrim_loss = discrim_criterion(g_o, ones((states.shape[0], 1), device=torch.device(\"cpu\"))) + \\\n            discrim_criterion(e_o, zeros((expert_states.shape[0], 1), device=torch.device(\"cpu\")))\n        print(discrim_loss.item(), \"discrim loss\")\n        discrim_loss.backward()\n        optimizer_discrim.step()\n\n    \"\"\"perform mini-batch PPO update\"\"\"\n    optim_iter_num = int(math.ceil(states.shape[0] \/ optim_batch_size))\n    for _ in range(optim_epochs):\n        perm = np.arange(states.shape[0])\n        np.random.shuffle(perm)\n        perm = LongTensor(perm).to(torch.device(\"cpu\"))\n\n        states, actions, returns, advantages, fixed_log_probs = \\\n            states[perm].clone(), actions[perm].clone(), returns[perm].clone(), advantages[perm].clone(), fixed_log_probs[perm].clone()\n\n        for i in range(optim_iter_num):\n            ind = slice(i * optim_batch_size, min((i + 1) * optim_batch_size, states.shape[0]))\n            states_b, actions_b, advantages_b, returns_b, fixed_log_probs_b = \\\n                states[ind], actions[ind], advantages[ind], returns[ind], fixed_log_probs[ind]\n\n            ppo_step(policy_net, value_net, optimizer_policy, optimizer_value, 1, states_b, actions_b, returns_b,\n                     advantages_b, fixed_log_probs_b, clip_epsilon, l2_reg)\n\n\ndef main_loop():\n    for i_iter in range(max_iter_num):\n        \"\"\"generate multiple trajectories that reach the minimum batch_size\"\"\"\n        discrim_net.to(torch.device('cpu'))\n        batch, log = agent.collect_samples(min_batch_size)\n\n        t0 = time.time()\n        update_params(batch, i_iter)\n        t1 = time.time()\n\n        if i_iter % log_interval == 0:\n            print('{}\\tT_sample {:.4f}\\tT_update {:.4f}\\texpert_R_avg {:.2f}\\tR_avg {:.2f}'.format(\n                i_iter, log['sample_time'], t1-t0, log['avg_c_reward'], log['avg_reward']))\n\nmain_loop()","21a02ebf":"actor_jit = torch.jit.script(agent.policy)\ntorch.jit.save(actor_jit, \"actor.pt\")","7bd5f393":"%%writefile main.py\nimport numpy as np\n\nimport torch\nfrom torch import nn\n\ndef convert_observation(s):\n    s[0]['relative_position'] = np.array(s[0]['left_team'][s[0]['active']]) - np.array(s[0]['ball'][:2])\n    s[0]['absolute_position'] = s[0]['left_team'][s[0]['active']]\n    s[0]['player_direction'] = s[0]['left_team_direction'][s[0]['active']]\n    s[0]['sticky_is_sprint'] = int(s[0]['sticky_actions'][8]==1)\n    closer_enemy = np.argmin(np.sum((np.array(s[0]['absolute_position'])-np.array(s[0]['right_team']))**2, axis=1))\n    s[0]['closer_enemy_relative_position'] = np.array(s[0]['absolute_position'])-np.array(s[0]['right_team'][closer_enemy])\n    s[0]['closer_enemy_direction'] = s[0]['right_team_direction'][closer_enemy]\n    new_s = np.concatenate([np.array(v).astype(np.float32).reshape(-1) if type(v)==list or type(v)==np.ndarray else [float(v)]\n                            for k,v in s[0].items()])\n    return new_s.astype(np.float32)\n\nac = torch.jit.load(\"\/kaggle_simulations\/agent\/actor.pt\")\n\ndef agent(obs):\n    \n    obs = obs['players_raw']\n    obs = convert_observation(obs)\n    obs = torch.DoubleTensor(obs).unsqueeze(0)\n    actions = ac(obs)\n    action = actions.argmax().item() # modified\n    return [action]\n","4f69e478":"Attempt of high impact work from ( Ho & Ermon, 2016 ) known as Generative Adversarial Imitation Learning ( see https:\/\/arxiv.org\/abs\/1606.03476 ) for football. The implementation is adapted from this useful repository https:\/\/github.com\/Khrylx\/PyTorch-RL.\n\nIt can be useful also to see how to submit PyTorch agent with a compressed repository .tar.gz\n\nJust run all this notebook and then compress the model \"actor.pt\" and \"main.py\" in a folder \"submission.tar.gz\". This is a valid format for the Kaggle competition.\n\nThe result won't be great in term of performance, at least it was not for me. Each suggestion is warmly welcomed! :)","46b7a3d9":"**Generative Adversarial Imitation Learning from the episodes of the Team Leader**"}}