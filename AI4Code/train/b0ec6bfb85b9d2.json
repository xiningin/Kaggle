{"cell_type":{"9ef613cc":"code","5aa98c6b":"code","7d27a0f3":"code","ee50bdc2":"code","42d9a4c1":"code","6748e91a":"code","9ddae457":"code","ab1ccfbb":"code","4c08a068":"code","270a1c8c":"code","e5d2ce9c":"code","973cfca0":"code","c9adc6f0":"code","e6a68576":"code","5328e675":"code","c3ab23ce":"code","840650f9":"code","55d50eb5":"code","1db7836f":"code","49b4b27f":"code","55bdb838":"code","e0c0d3d9":"code","38341e47":"code","729f4333":"code","2c84587c":"code","0b883d2b":"code","80407b37":"code","2cefa41d":"markdown","b44a6cb1":"markdown","8e82d398":"markdown","d124839d":"markdown","ceb1a2cc":"markdown","8402e78e":"markdown","8456d1fe":"markdown","7a31f53e":"markdown","27b352b0":"markdown","be31e52c":"markdown"},"source":{"9ef613cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5aa98c6b":"df = pd.read_csv('..\/input\/fish-market\/Fish.csv')\ndf.head()","7d27a0f3":"df.Species.value_counts()","ee50bdc2":"import seaborn as sns\nimport matplotlib.pyplot as plt","42d9a4c1":"sns.pairplot(data=df)","6748e91a":"sns.countplot(x= df.Species);","9ddae457":"sns.displot(data=df, x='Weight', hue='Species', kind='hist')","ab1ccfbb":"df.isna().sum().sum() # Check for NaNs","4c08a068":"from sklearn.preprocessing import OneHotEncoder, StandardScaler","270a1c8c":"ohe = OneHotEncoder()\nfeat_array = ohe.fit_transform(df[['Species']]).toarray()\ndf_ohe = pd.DataFrame(feat_array, columns=ohe.categories_)\ndf = df.drop('Species', axis=1)","e5d2ce9c":"df = pd.concat([df, df_ohe], axis=1)","973cfca0":"y = df.Weight\nX = df.drop('Weight', axis=1)","c9adc6f0":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import train_test_split","e6a68576":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 1)","5328e675":"scaler = StandardScaler()\n\ns_X_train = scaler.fit_transform(X_train)\ns_X_test = scaler.transform(X_test)","c3ab23ce":"print(s_X_train.shape, s_X_test.shape, y_train.shape, y_test.shape)","840650f9":"LR = LinearRegression()\nLR.fit(s_X_train, y_train)","55d50eb5":"y_preds = LR.predict(s_X_test)\ny_preds","1db7836f":"import math\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.metrics import r2_score as r2","49b4b27f":"rmse_LR = math.sqrt(mse(y_preds, y_test))\nrmse_LR","55bdb838":"mae_LR = mae(y_preds, y_test)\nmae_LR\n\nmse_LR = mse(y_preds, y_test)\nmse_LR","e0c0d3d9":"r2_LR = r2(y_preds, y_test)\nr2_LR","38341e47":"print(f'The rmse_LR is: {rmse_LR}, the mse_LR is: {mse_LR}. the mae_LR is {mae_LR}, the r2_LR is {r2_LR}')","729f4333":"#XGBoost Regressor\nxgbr = XGBRegressor(verbosity=0)\nxgbr.fit(X_train, y_train)\n\n#Predictions\ny_predX = xgbr.predict(X_test)\n\n#Metrics\nrmse_X = math.sqrt(mse(y_predX, y_test))\nmse_X = rmse_X ** 2\nmae_X = mae(y_predX, y_test)\nr2_X = r2(y_predX, y_test)\n\nprint(f'The rmse_X is: {rmse_X}, the mse_X is: {mse_X}. the mae_X is {mae_X}, the r2_X is {r2_X}')","2c84587c":"lasso_reg = Lasso(alpha=0.1,tol=0.03)\nlasso_reg.fit(s_X_train, y_train)\ny_preds_lasso = lasso_reg.predict(s_X_test)\n\n#Metrics\nrmse_Ls = math.sqrt(mse(y_preds_lasso, y_test))\nmse_Ls = rmse_Ls ** 2\nmae_Ls = mae(y_preds_lasso, y_test)\nr2_Ls = r2(y_preds_lasso, y_test)\n\nprint(f'The rmse for Lasso is: {round(rmse_Ls, 2)}, the mse for Lasso is: {round(mse_Ls, 2)}. the mae for Lasso is {round(mae_Ls, 2)}, the r2 score for Lasso is {round(r2_Ls, 2)}')","0b883d2b":"ridge = Ridge(alpha=0.1)\nridge.fit(s_X_train, y_train)\ny_preds_ridge= ridge.predict(s_X_test)\n\n#Metrics\nrmse_R = math.sqrt(mse(y_preds_ridge, y_test))\nmse_R = rmse_R ** 2\nmae_R = mae(y_preds_ridge, y_test)\nr2_R = r2(y_preds_ridge, y_test)\n\nprint(f'The rmse for Ridge is: {round(rmse_R, 3)}, the mse for Ridge is: {round(mse_R, 3)}, the mae for Ridge is {round(mae_R, 3)}, the r2 score for Ridge is {round(r2_R, 3)}')","80407b37":"models = ['Linear Regression', 'Lasso Regression', 'Ridge Regression', 'XGBoost']\nRMSE_scores = [rmse_LR, rmse_Ls, rmse_R, rmse_X]\nMSE_scores = [mse_LR, mse_Ls, mse_R, mse_X]\nMAE_scores = [mae_LR, mae_Ls, mae_R, mae_X]\nR2_scores = [r2_LR, r2_Ls, r2_R, r2_X]\n\ndf = pd.DataFrame(list(zip(models,RMSE_scores, MSE_scores, MAE_scores, R2_scores)), columns=['Model Name', 'RMSE Score','MSE Score', 'MAE Score', 'R^2 coefficient'])\ndf","2cefa41d":"### Lasso Regression","b44a6cb1":"### XGBoost","8e82d398":"Splitting then Scaling the data using StandardScaler()","d124839d":"# Output results","ceb1a2cc":"# Pre-processing","8402e78e":"Conclusion: XGBoost outperformed by a long shot! (As expected) yay for tree based algorthims","8456d1fe":"### Linear Regression","7a31f53e":"### Ridge Regression","27b352b0":"# Model training \/ evaluation","be31e52c":"OneHotEncoding the 'Species' column"}}