{"cell_type":{"d20752c4":"code","786bc8af":"code","181bc3e4":"code","2f19444b":"code","f5f7030d":"code","35fa0056":"code","ce633b77":"code","785dfde7":"code","af070262":"code","c7b8f118":"code","dd3ac605":"code","3f5919ec":"code","eb7d20c3":"code","12e6df7b":"code","1963805f":"code","cc90d76a":"code","1a72e256":"code","6fbf1b00":"code","c9eeea42":"code","60c320a5":"code","2ac5c67e":"code","ed599402":"code","ff9c2b9a":"code","a9de8815":"code","d85ff6ff":"code","86880c18":"code","df6b2cab":"code","0f122bd7":"code","0777a9a9":"code","25d28a0f":"code","ed49052f":"code","cc8ef34e":"code","ddad77cb":"code","662e25c9":"code","1f3ba444":"code","de683f73":"code","d134bfb2":"code","7bc69391":"code","e67866c2":"code","0ea66041":"code","a3c1f4bd":"code","26c4cf84":"code","40706c51":"code","c2b701ba":"code","ec3b56ec":"code","9014cf3d":"code","78fe2e3b":"code","956b0c8e":"code","0564126f":"code","6b6c6b51":"code","697290df":"code","d25eed95":"code","d3754f78":"code","31544569":"code","c830c909":"code","e1aaae7c":"code","628258a5":"code","aa486e9e":"markdown","654dfe62":"markdown","ec271dde":"markdown","3a75c802":"markdown","2a913444":"markdown","1b74cb6a":"markdown","55bfb5d4":"markdown","c0a6b137":"markdown","7043fd8e":"markdown","d393efee":"markdown","6bd774c7":"markdown","75af9731":"markdown","7348e3db":"markdown","3442501f":"markdown","4ccce8f0":"markdown","e1fb235b":"markdown"},"source":{"d20752c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","786bc8af":"#Basic imports\nimport numpy as np\nimport pandas as pd\n\n#sklearn imports\nfrom sklearn.decomposition import PCA #Principal Component Analysis\nfrom sklearn.manifold import TSNE #T-Distributed Stochastic Neighbor Embedding\nfrom sklearn.cluster import KMeans #K-Means Clustering\nfrom sklearn.preprocessing import StandardScaler #used for 'Feature Scaling'\n\n#plotly imports\nimport plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\ninit_notebook_mode(connected=True)\nfrom IPython.display import HTML, Image","181bc3e4":"df_raw = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')\ndf_raw.head()","2f19444b":"import pandas_profiling as pdpf\n\nprofile = pdpf.ProfileReport(df_raw);","f5f7030d":"profile.to_widgets()","35fa0056":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf = df_raw.apply(LabelEncoder().fit_transform)   \ndf.head()","ce633b77":"df.describe()","785dfde7":"f, ax = plt.subplots(1, 2, figsize = (15, 7))\ndf['class'].value_counts().plot.bar(ax=ax[0])\ndf['class'].value_counts().plot.pie(ax=ax[1], autopct = \"%.2f%%\");","af070262":"df.hist(figsize=(15,15));","c7b8f118":"sns.set(style=\"white\")\ndfx = df.loc[:,['gill-color','ring-type','gill-size', 'habitat']]\ng = sns.PairGrid(dfx, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot, lw=3);","dd3ac605":"corr=df.corr()\n\nsns.set(font_scale=0.75)\nplt.figure(figsize=(35, 15))\n\nsns.heatmap(corr, vmax=.8, linewidths=0.01, square=True,annot=True,cmap='YlGnBu',linecolor=\"black\")\nplt.title('Correlation between features');","3f5919ec":"x = df.drop(['class'] , axis = 1)\ny = df['class']","eb7d20c3":"# using sklearn variancethreshold to find constant features\n\nfrom sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(threshold=0)\nsel.fit(x)  # fit finds the features with zero variance\n# print the constant features\nconst_columns = [x_ for x_ in x.columns if x_ not in x.columns[sel.get_support()]]\nprint(const_columns)","12e6df7b":"x = x.drop(const_columns, axis=1)","1963805f":"# Import SelectKBest, chi2(score function for classification), f_regression (score function for regression)\nfrom sklearn.feature_selection import SelectKBest, chi2, f_regression\n# Create the object for SelectKBest and fit and transform the classification data\n# k is the number of features you want to select [here it's 2]\nX_clf_new = SelectKBest(score_func=chi2, k=2).fit(x,y)\n# Get the indices sorted by most important to least important\nindices = np.argsort(X_clf_new.scores_)[::-1]\n\n# To get your feature names\nfeatures = []\nfor i in range(x.columns.size):\n    features.append(x.columns[indices[i]])\n\n# Now plot\nplt.figure()\nplt.bar(features, X_clf_new.scores_[indices[range(x.columns.size)]], color='r', align='center')\nplt.xticks(rotation=90)\nplt.show()","cc90d76a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)","1a72e256":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import plot_confusion_matrix\n\n#Model\nDTC = DecisionTreeClassifier(random_state = 10)\n\n#fiting the model\nDTC.fit(X_train, y_train);\n\nplot_confusion_matrix(DTC, X_test, y_test, cmap=plt.cm.Blues);","6fbf1b00":"print(DTC.score(X_test , y_test));","c9eeea42":"from sklearn.tree import export_graphviz\nimport pydot\nfeature_list = x.columns.values\n\n# Save the tree as a png image\nexport_graphviz(DTC, out_file = 'mushrooms_DTC.dot', feature_names = feature_list, rounded = True, precision = 1, filled = True, class_names=['edible','poisonous'])\n(graph, ) = pydot.graph_from_dot_file('mushrooms_DTC.dot')\ngraph.write_png('mushrooms_DTC.png');\nImage('mushrooms_DTC.png')","60c320a5":"feature_import = pd.DataFrame(data=DTC.feature_importances_, index=feature_list, columns=['values'])\nfeature_import.sort_values(['values'], ascending=False, inplace=True)\nfeature_import.transpose()","2ac5c67e":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import plot_confusion_matrix\n\n#Model\nRFC = RandomForestClassifier(n_estimators=10, bootstrap=True, random_state = 0)\n\n#fiting the model\nRFC.fit(X_train, y_train);\n\nplot_confusion_matrix(RFC, X_test, y_test, cmap=plt.cm.Blues);","ed599402":"print(RFC.score(X_test , y_test))","ff9c2b9a":"# Save the tree as a png image\nexport_graphviz(RFC.estimators_[0], out_file = 'mushrooms_RFC.dot', feature_names = feature_list, rounded = True, precision = 1, filled = True, class_names=['edible','poisonous'])\n(graph, ) = pydot.graph_from_dot_file('mushrooms_RFC.dot')\ngraph.write_png('mushrooms_RFC.png');\nImage('mushrooms_RFC.png')","a9de8815":"feature_import = pd.DataFrame(data=DTC.feature_importances_, index=feature_list, columns=['values'])\nfeature_import.sort_values(['values'], ascending=False, inplace=True)\nfeature_import.transpose()","d85ff6ff":"features[0:7]","86880c18":"#Model\nDTC7 = DecisionTreeClassifier(random_state = 10)\n\nX_train_7 = X_train[features[0:7]]\n\nX_test_7 = X_test[features[0:7]]\n\n#fiting the model\nDTC7.fit(X_train_7, y_train);\n\nplot_confusion_matrix(DTC7, X_test_7, y_test, cmap=plt.cm.Blues);","df6b2cab":"feature_import = pd.DataFrame(data=DTC7.feature_importances_, index=features[0:7], columns=['values'])\nfeature_import.sort_values(['values'], ascending=False, inplace=True)\nfeature_import.transpose()","0f122bd7":"import tensorflow as tf\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\n\ndef CreateModel(dropout = 0.1): \n    model = tf.keras.Sequential()\n    model.add(layers.Dense(128, activation='relu', input_shape=(X_train.values.shape[1],)))\n    model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(32, activation='relu'))\n    model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model","0777a9a9":"model = CreateModel()\nhistory = model.fit(X_train.values, y_train.values, epochs=10, batch_size=16, validation_data=(X_test.values,y_test.values))","25d28a0f":"plt.plot(history.history['accuracy'],'bo', label='Trainning acc')\nplt.plot(history.history['val_accuracy'],'b', label='Validation acc')\nplt.legend();","ed49052f":"import shap\nbackground = X_train[0:10].values\nexplainer = shap.DeepExplainer(model,  background)\nshap_values = explainer.shap_values(background)","cc8ef34e":"# Print the feature attributions for the first example in our test set\nshap_values[0][0]","ddad77cb":"Xx,yx = shap.datasets.adult()","662e25c9":"X_train","1f3ba444":"# This is the baseline value shap is using\nexplainer.expected_value.numpy()","de683f73":"shap.initjs()\nshap.force_plot(explainer.expected_value[0].numpy(), shap_values[0][0,:], X_train.iloc[0,:])\n","d134bfb2":"shap.force_plot(explainer.expected_value[0].numpy(), shap_values[0][1,:], X_train.iloc[0,:])","7bc69391":"plt.xticks(rotation='vertical')\nplt.bar(list(X_train.columns), list(shap_values[0][1,:]))","e67866c2":"shap.summary_plot(shap_values[0], X_train.columns)","0ea66041":"# In the original dataset \"4\" indicates the pet was not adopted.\ndf_raw['target'] = np.where(df_raw['class']=='e', 0, 1)","a3c1f4bd":"train, test = train_test_split(df_raw, test_size=0.2)\ntrain, val = train_test_split(train, test_size=0.2)\nprint(len(train), 'train examples')\nprint(len(val), 'validation examples')\nprint(len(test), 'test examples')","26c4cf84":"# A utility method to create a tf.data dataset from a Pandas Dataframe\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\n  dataframe = dataframe.copy()\n  labels = dataframe.pop('target')\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n  if shuffle:\n    ds = ds.shuffle(buffer_size=len(dataframe))\n  ds = ds.batch(batch_size)\n  return ds","40706c51":"batch_size = 5 # A small batch sized is used for demonstration purposes\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)","c2b701ba":"for feature_batch, label_batch in train_ds.take(1):\n  print('Every feature:', list(feature_batch.keys()))\n  print('A batch of targets:', label_batch )","ec3b56ec":"from tensorflow import feature_column","9014cf3d":"features_columns = []\n\nfor i in range(0,7):\n    print(features[i])\n    feature_col = feature_column.categorical_column_with_vocabulary_list(features[i], df_raw[features[i]].unique())\n    feature_col_one_hot = feature_column.indicator_column(feature_col)\n    features_columns.append(feature_col_one_hot)","78fe2e3b":"feature_layer = tf.keras.layers.DenseFeatures(features_columns)","956b0c8e":"batch_size = 32\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)","0564126f":"model = tf.keras.Sequential([\n  feature_layer,\n  layers.Dense(128, activation='relu'),\n  layers.Dense(128, activation='relu'),\n  layers.Dropout(.1),\n  layers.Dense(1)\n])\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nmodel.fit(train_ds,\n          validation_data=val_ds,\n          epochs=5);","6b6c6b51":"loss, accuracy = model.evaluate(test_ds)\nprint(\"Accuracy\", accuracy)","697290df":"y_pred = model.predict_classes(test_ds)\ny_true = tf.concat([y for x, y in test_ds], axis=0)\ncon_mat = tf.math.confusion_matrix(labels=y_true, predictions=y_pred).numpy()","d25eed95":"figure = plt.figure(figsize=(8, 8))\nsns.heatmap(con_mat, annot=True, cmap=plt.cm.Blues, fmt=\"d\")\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","d3754f78":"from sklearn.preprocessing import StandardScaler\nX_std= StandardScaler().fit_transform(X_train_7)","31544569":"from sklearn.manifold import TSNE\nmodel = TSNE(n_components = 3, random_state = 0)\ntsne_model = model.fit_transform(X_std)","c830c909":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\n\nsns.set(style = \"darkgrid\")\n\nfig = plt.figure()\nax = Axes3D(fig)\n\nx = tsne_model[:,0]\ny = tsne_model[:,1]\nz = tsne_model[:,2]\n\nax.set_xlabel(\"Dimension 1\")\nax.set_ylabel(\"Dimension 2\")\nax.set_zlabel(\"Dimension 3\")\n\nax.scatter(x, y, z, c=y_train, cmap='magma')\n\nax.view_init(60, 60)","e1aaae7c":"model = TSNE(n_components = 2, random_state = 0, learning_rate=100)\ntsne_model_test = model.fit_transform(X_std)","628258a5":"x = tsne_model_test[:,0]\ny = tsne_model_test[:,1]\n\nplt.xlabel(\"Dimension 1\")\nplt.ylabel(\"Dimension 2\")\n\nplt.scatter(x, y, c=y_train, cmap='magma');","aa486e9e":"# Remove constant features","654dfe62":"<div class=\"alert alert-block alert-info\">\n<b>Tip:<\/b> Use blue boxes (alert-info) for tips and notes. \n<\/div>","ec271dde":"# Split data into training and test","3a75c802":"# Data Visualization","2a913444":"# Tensorflow Feature Columns","1b74cb6a":"# Select best features","55bfb5d4":"# t-SNE","c0a6b137":"# Numerical encoding of data","7043fd8e":"# Decisision Tree with 7 best features","d393efee":"Attribute Information: (classes: edible=e, poisonous=p)\n\ncap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n\ncap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n\ncap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n\nbruises: bruises=t,no=f\n\nodor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n\ngill-attachment: attached=a,descending=d,free=f,notched=n\n\ngill-spacing: close=c,crowded=w,distant=d\n\ngill-size: broad=b,narrow=n\n\ngill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n\nstalk-shape: enlarging=e,tapering=t\n\nstalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n\nstalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n\nstalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n\nstalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n\nstalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n\nveil-type: partial=p,universal=u\n\nveil-color: brown=n,orange=o,white=w,yellow=y\n\nring-number: none=n,one=o,two=t\n\nring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n\nspore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n\npopulation: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n\nhabitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d","6bd774c7":"# **Data Analysis and Visualisation**","75af9731":"# Decision Tree","7348e3db":"# Get x, y data","3442501f":"# Random Forest","4ccce8f0":"# Deep neural network","e1fb235b":"?# Load mushroom data"}}