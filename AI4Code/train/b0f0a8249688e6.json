{"cell_type":{"446d5302":"code","3d6d4eda":"code","9fc5e24f":"code","1f545a3b":"code","ac5220c3":"code","fe98807f":"code","fcfbeed2":"code","9a42bd3e":"code","39ad0428":"code","dab8b572":"code","a649cec1":"code","1c6797a0":"code","881fbf4a":"code","b05668a3":"code","c3c08ae1":"code","ec1fb6b7":"code","83768c28":"code","d1134644":"code","278a6574":"code","9b64028a":"code","858d1142":"code","29e6a0d9":"code","23328e93":"code","36724ac3":"code","ca71434e":"code","46880264":"code","03325265":"code","6d7ee16c":"code","da0180f5":"code","7371503e":"code","76328d49":"code","57b92f81":"code","0d7be392":"code","a5d10fbe":"code","4aa77af5":"code","de35b1dc":"code","72f20966":"code","29655c0f":"code","8292e072":"code","94b1175d":"code","e55a5fe3":"markdown","a351ec25":"markdown","ad88ed9a":"markdown","9f561449":"markdown","53c1d7f0":"markdown","be27531c":"markdown","915a7a29":"markdown","1e76d790":"markdown","250b68a2":"markdown","01b484f4":"markdown","bfd9d82e":"markdown"},"source":{"446d5302":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder #Encode Categorical Features\nimport lightgbm as lgb #Gradient Boosting Machine\nimport matplotlib.pyplot as plt #Visualization\nimport seaborn as sns #Visualization\nfrom sklearn.model_selection import KFold #N-Fold Validation\nfrom sklearn.metrics import mean_squared_error #Evaluation Metric\nimport optuna #hyperparams Tuning\nimport scipy\nimport random\nimport keras\nfrom keras import layers\nimport tensorflow as tf","3d6d4eda":"trainSet = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')","9fc5e24f":"trainSet.head()","1f545a3b":"#plot the Target Distribution\nsns.displot(data=trainSet, x=\"target\", kde=True)","ac5220c3":"len(trainSet[trainSet.target < 5])\/len(trainSet)","fe98807f":"len(trainSet[trainSet.target > 10])\/len(trainSet)","fcfbeed2":"#From the distribution graph, I would like to get rid of rows which has target < 5 and > 10 to minimize outlier.\ntrainSet = trainSet[(trainSet.target > 5) & (trainSet.target < 10)]","9a42bd3e":"#encode categorical feats\ncat_feat = [f\"cat{val}\" for val in range(0,10)]\n\nlabelEnc = [LabelEncoder() for _ in range(len(cat_feat))]\n\nfor i in range(len(cat_feat)):\n    trainSet[cat_feat[i]] = labelEnc[i].fit_transform(trainSet[cat_feat[i]])","39ad0428":"#Lets see the Correlation of each features and target\n\ncorr = trainSet.drop(['id'], axis=1).corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 9))\nsns.heatmap(corr, mask=mask, cmap='BrBG', vmin=-1, vmax=1, annot=True)","dab8b572":"cont_var = [f\"cont{val}\" for val in range(14)]","a649cec1":"X_input = trainSet.drop(['id', 'target'], axis=1)\nX_input = X_input.loc[:, X_input.columns.isin(cont_var)].values ","1c6797a0":"from sklearn.preprocessing import OneHotEncoder\n\noheEnc = [OneHotEncoder() for _ in range(len(cat_feat))]\nX_ohe = []\nfor i in range(len(cat_feat)):\n    if X_input[cat_feat[i]].nunique() <= 2:\n        X_ohe.append(X_input[cat_feat[i]])\n    else:\n        X_ohe.append(oheEnc[i].fit_transform(X_input[cat_feat[i]].values.reshape([-1,1])))","881fbf4a":"for i in range(len(cat_feat)):\n    if type(X_ohe[i]) == scipy.sparse.csr.csr_matrix:\n        X_ohe[i] = X_ohe[i].toarray()\\\n\nX_ohe_df = pd.DataFrame()\n        \nfor i in range(len(cat_feat)):\n    X_ohe_df = pd.concat([X_ohe_df, pd.DataFrame(X_ohe[i])], axis=1)\n    \nX_ohe_df = X_ohe_df.values","b05668a3":"def add_random_noise(X, randomize_rate=0.4, row_random_rate=0.4):\n    row_size = int(X.shape[0])\n    col_size = int(X.shape[1])\n    \n    all_feat_size = int(row_size*col_size)\n    randomize_size = int(all_feat_size*randomize_rate)\n    row_random_size = int(row_size*row_random_rate)\n    \n    col_random_size = int(np.floor(randomize_size\/row_random_size))\n    \n    idx_randomize = [random.randint(0, row_size-1) for _ in range(row_random_size)]\n    \n    for i in idx_randomize:\n        col_feat = [random.randint(0, col_size-1) for _ in range(col_random_size)]\n        \n        for k in col_feat:\n            X[i, k] = random.random()\n            \n    return X","c3c08ae1":"def add_feat_noise(X, randomize_rate=0.4, row_random_rate=0.4):\n    row_size = int(X.shape[0])\n    col_size = int(X.shape[1])\n    \n    all_feat_size = int(row_size*col_size)\n    randomize_size = int(all_feat_size*randomize_rate)\n    row_random_size = int(row_size*row_random_rate)\n    \n    col_random_size = int(np.floor(randomize_size\/row_random_size))\n    \n    idx_randomize = [random.randint(0, row_size-1) for _ in range(row_random_size)]\n    \n    for i in idx_randomize:\n        col_feat = [random.randint(0, col_size-1) for _ in range(col_random_size)]\n        \n        for k in col_feat:\n            idx = random.randint(0, row_size-1)\n            X[i, k] = X[idx, k]\n            \n    return X","ec1fb6b7":"def create_batch_set(X, batch_size):\n    X_sample = []\n    row_size = X.shape[0]\n    idx = [random.randint(0, row_size-1) for _ in range(batch_size)]\n\n    return X[idx]","83768c28":"# This is the size of our encoded representations\nencoding_dim = 1000  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\nbottleneck_dim = 500\ninput_shape = X_input.shape[1]\n\n# This is our input image\ninput_set = keras.Input(shape=(input_shape,))\ninput_encode1 = layers.Dense(encoding_dim, activation='relu')(input_set)\ninput_encode2 = layers.Dense(encoding_dim, activation='relu')(input_encode1)\nencoded = layers.Dense(bottleneck_dim, activation='relu')(input_encode2)\ninput_decode1 = layers.Dense(encoding_dim, activation='relu')(encoded)\ninput_decode2 = layers.Dense(encoding_dim, activation='relu')(input_decode1)\ndecoded = layers.Dense(input_shape, activation='relu')(input_decode2)\n\n# This model maps an input to its reconstruction\nautoencoder = keras.Model(input_set, decoded)\nencoder = keras.Model(input_set, encoded)","d1134644":"epochs = 500\n\n# Instantiate an optimizer to train the model.\noptimizer = keras.optimizers.Adam(learning_rate=1e-4)\n# Instantiate a loss function.\nloss_fn = keras.losses.MeanSquaredError()\n\nfor epoch in range(epochs):\n    print(\"\\nStart of epoch %d\" % (epoch,))\n    \n    X_sample = create_batch_set(X_input, 1024)\n    X_noise = add_feat_noise(X_sample, 0.3, 0.6)\n\n    # Open a GradientTape to record the operations run\n    # during the forward pass, which enables auto-differentiation.\n    with tf.GradientTape() as tape:\n\n        # Run the forward pass of the layer.\n        # The operations that the layer applies\n        # to its inputs are going to be recorded\n        # on the GradientTape.\n        logits = autoencoder(X_noise, training=True)  # Logits for this minibatch\n\n        # Compute the loss value for this minibatch.\n        loss_value = loss_fn(X_sample, logits)\n\n    # Use the gradient tape to automatically retrieve\n    # the gradients of the trainable variables with respect to the loss.\n    grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n\n    # Run one step of gradient descent by updating\n    # the value of the variables to minimize the loss.\n    optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n\n\n    print(\n        \"Training loss (for one batch) at step %d: %.4f\"\n        % (epoch, float(loss_value))\n    )","278a6574":"X_encode = pd.DataFrame(encoder.predict(X_input))","9b64028a":"cont_var = [f\"cont_{val}\" for val in range(bottleneck_dim)]\nX_encode.columns = cont_var","858d1142":"cat_feat = [f\"cat{val}\" for val in range(0,10)]\ntrainSet = pd.concat([trainSet.loc[:, trainSet.columns.isin(cat_feat)], X_encode, trainSet.target], axis=1)","29e6a0d9":"trainSet.head()","23328e93":"#Seperate features and its target\ny = trainSet.target\nX = trainSet.drop(['target'], axis=1)","36724ac3":"def objective(trial):\n    # Define the search spaces, for your guidance, visit the optuna official sample codes https:\/\/optuna.org\/#code_examples\n    params = {\n        'num_iterations' : trial.suggest_int('num_iterations', 100, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.05),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 256),\n        'num_leaves': trial.suggest_int('num_leaves', 15, 256),\n        'lambda_l1': trial.suggest_float('lambda_l1', 0, 25.0),\n        'lambda_l2': trial.suggest_float('lambda_l2', 0, 25.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 0, 25.0),\n        'random_state': 47,\n        'boosting_type': 'gbdt', \n        'verbose': -1,\n        'device' : 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0\n\n    }\n\n    # Use 5 folds cross-validation\n    N_FOLDS = 5\n    rmse_score = 0\n    lgbm_models = []\n\n    kf = KFold(n_splits = N_FOLDS)\n    \n    for folds, (train_idx,val_idx) in enumerate(kf.split(X, y)):\n        print(f\"folds: {folds}\")\n        trainSet = lgb.Dataset(X.iloc[train_idx], y.iloc[train_idx])\n        valSet = lgb.Dataset(X.iloc[val_idx], y.iloc[val_idx])\n\n        model = lgb.train(params, trainSet)\n        lgbm_models.append(model)\n        y_pred = model.predict(X.iloc[val_idx])\n\n        rmse_score += mean_squared_error(y.iloc[val_idx], y_pred, squared=False)\/N_FOLDS\n\n        print(mean_squared_error(y.iloc[val_idx], y_pred, squared=False))\n        \n    return rmse_score","ca71434e":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Start the hyperparams tunning and suppress any warnings\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, timeout=5*60)","46880264":"best_params = study.best_params\nprint(study.best_params)","03325265":"study.best_value","6d7ee16c":"N_FOLDS = 5\nrmse_score = 0\nlgbm_models = []\neval_results = [{} for _ in range (N_FOLDS)]\n\nkf = KFold(n_splits = N_FOLDS)","da0180f5":"#Train our LGBM using the best parameter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfor folds, (train_idx,val_idx) in enumerate(kf.split(X, y)):\n    print(f\"folds: {folds}\")\n    trainSet = lgb.Dataset(X.iloc[train_idx], y.iloc[train_idx])\n    valSet = lgb.Dataset(X.iloc[val_idx], y.iloc[val_idx])\n    \n    model = lgb.train(best_params, trainSet, valid_sets=[trainSet, valSet], evals_result=eval_results[folds])\n    lgbm_models.append(model)\n    y_pred = model.predict(X.iloc[val_idx])\n    \n    rmse_score += mean_squared_error(y.iloc[val_idx], y_pred, squared=False)\/N_FOLDS\n    \n    print(mean_squared_error(y.iloc[val_idx], y_pred, squared=False))","7371503e":"print(rmse_score)","76328d49":"#plot the rmse score for each iteration in 5th fold model\nlgb.plot_metric(eval_results[4])","57b92f81":"lgb.plot_importance(lgbm_models[4])","0d7be392":"testSet = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')\n\nfor i in range(len(cat_feat)):\n    testSet[cat_feat[i]] = labelEnc[i].transform(testSet[cat_feat[i]])","a5d10fbe":"cont_var = [f\"cont{val}\" for val in range(14)]\nfor i in cont_var:\n    testSet[i] = np.log(testSet[i])","4aa77af5":"id = testSet.id\ntestSet.drop('id', axis=1, inplace=True)","de35b1dc":"y_pred = np.zeros(len(testSet))","72f20966":"for model in lgbm_models:\n    y_pred += model.predict(testSet)","29655c0f":"y_pred = pd.DataFrame(y_pred\/N_FOLDS)","8292e072":"submFile = pd.concat([id, y_pred],axis=1)\nsubmFile.columns = ['id', 'target']","94b1175d":"submFile.to_csv('submFile.csv', index=False)","e55a5fe3":"# Predict the Test Set","a351ec25":"# Data Preprocessing","ad88ed9a":"# Implement Denoising AutoEncoder","9f561449":"From the correlation matrix, I could say that there is no single feature that is highly correlated to the target. So for this notebook, I will use all those features.","53c1d7f0":"# End DAE Training","be27531c":"I am going to tune the DAE hyperparams hope to get a better result.","915a7a29":"In this try, I use bottleneck DAE. The noise is taken from the same feature but taken from other rows. For deeper explanation check out this great notebook https:\/\/www.kaggle.com\/springmanndaniel\/1st-place-turn-your-data-into-daeta#denoising-autoencoders .","1e76d790":"# End Hyperparam Tuning","250b68a2":"For this first try, I will try to use DAE on continuous features, and label encoding the categorical features","01b484f4":"# Create Submission File as in sample_submission.csv","bfd9d82e":"# Optuna Hyperparams Tuning on Light GBM Model"}}