{"cell_type":{"968c897a":"code","c2d1b66b":"code","2bb36612":"code","f8f4f56f":"code","dc033b54":"code","563e27cb":"code","50fa6b10":"code","2e09e343":"code","e3b54df0":"code","683d03f7":"code","3d9534c3":"code","92b0cd1b":"code","cf6ce1e9":"code","06c91d6f":"code","961825af":"code","715f20cd":"code","c75462e6":"code","ee3042a8":"code","219f4862":"code","ce1b7d7a":"code","27f973fb":"code","82b6c340":"code","fb11e86e":"code","d10b6b12":"code","d776c53f":"code","7ea5ad7e":"code","514c05cf":"code","390a50a2":"code","380a1f0f":"code","f27d622f":"code","ff20b0f0":"code","71b146af":"code","fc38c1ea":"code","aee50d4b":"code","140623f1":"code","1f753f04":"code","34b21ada":"code","f223ffb5":"code","5415530e":"code","cc4a3816":"code","277425f9":"code","deed7dac":"code","419fcf48":"code","57dbd0fd":"code","3154dd73":"code","04b0ac30":"code","eee08c18":"code","01552cf2":"code","9a28221b":"code","5b79bb38":"code","0b43fdf7":"code","9ac5c97e":"markdown","27dcd892":"markdown","eaab06af":"markdown","cf77895d":"markdown","dfb60f82":"markdown"},"source":{"968c897a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c2d1b66b":"import numpy as np\nimport pandas as pd\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats \nfrom sklearn import linear_model\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom tabulate import tabulate\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","2bb36612":"def outliers(df, dt):\n    sorted(df[dt])\n    Q1 = df[dt].quantile(0.25)\n    Q3 = df[dt].quantile(0.75)\n    IQR = Q3 - Q1\n    print(\"Column:\", dt)\n    print(\"Old Shape\", df.shape)\n    upper_val = (Q3 + (1.5 * IQR))\n    lower_val = Q1 - (1.5 * IQR)\n    count = len(df[(df[dt] > upper_val) | (df[dt] < lower_val)])\n    df.drop(df[(df[dt] > upper_val) | (df[dt] < lower_val)].index, inplace=True)\n    print(\"New Shape\", df.shape)\n    print(\"Count of Item Removed:\", count)\n    print(\"Outliers ratio:\", count \/ len(df[dt]))\n","f8f4f56f":"\ndef null_values(df):\n    null_value = df.isnull().sum().sort_values(ascending=False)\n    percent_1 = df.isnull().sum() \/ df.isnull().count() * 100\n    percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n    missing_data = pd.concat([null_value, percent_2], axis=1, keys=['Total', '%'])\n    print(missing_data)\n","dc033b54":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n","563e27cb":"# display first 5 row in train data\ntrain_data.head()","50fa6b10":"# display description of train data\ntrain_data.describe()","2e09e343":"# display information about train data\ntrain_data.info()","e3b54df0":"# display first 5 row in test data\ntest_data.head()","683d03f7":"# display description of test data\ntest_data.describe()","3d9534c3":"# display information about test data\ntest_data.info()","92b0cd1b":"# check for the null train_data\nnull_values(train_data)","cf6ce1e9":"# check for the null test data\nnull_values(test_data)","06c91d6f":"# Correlation for train data\ncorr = train_data.corr()\ncorr[\"Survived\"].sort_values(ascending=False)","961825af":"plt.style.use(\"ggplot\")\nsns.set_style(\"darkgrid\")","715f20cd":"# Shimazaki H. and Shinomoto S. rule to calculate optimal bins in histogram\ndef optimal_bins(df, dt):\n    data_max = max(df[dt])  # lower end of data\n    data_min = min(df[dt])  # upper end of data\n    n_min = 2  # Minimum number of bins Ideal value = 2\n    n_max = 200  # Maximum number of bins  Ideal value =200\n    n_shift = 30  # number of shifts Ideal value = 30\n    N = np.array(range(n_min, n_max))\n    D = float(data_max - data_min) \/ N  # Bin width vector\n    Cs = np.zeros((len(D), n_shift))  # Cost function vector\n    for i in range(np.size(N)):\n        shift = np.linspace(0, D[i], n_shift)\n        for j in range(n_shift):\n            edges = np.linspace(data_min + shift[j] - D[i] \/ 2, data_max + shift[j] - D[i] \/ 2,\n                                N[i] + 1)  # shift the Bin edges\n            binindex = np.digitize(df[dt], edges)  # Find binindex of each data point\n            ki = np.bincount(binindex)[1:N[i] + 1]  # Find number of points in each bin\n            k = np.mean(ki)  # Mean of event count\n            v = sum((ki - k) ** 2) \/ N[i]  # Variance of event count\n            Cs[i, j] += (2 * k - v) \/ ((D[i]) ** 2)  # The cost Function\n    C = Cs.mean(1)\n    # Optimal Bin Size Selection\n    loc = np.argwhere(Cs == Cs.min())[0]\n    cmin = C.min()\n    idx = np.where(C == cmin)\n    idx = idx[0][0]\n    optD = D[idx]\n    return N[idx]","c75462e6":"# histogram for Age Series \nbin_count = optimal_bins(train_data,\"Age\")\nbin_count\n# calculate optimal bins numbers and display it\nplt.hist(train_data[\"Age\"], edgecolor='black', bins=bin_count, color=\"steelblue\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")","ee3042a8":"sns.countplot(x=\"Survived\", data=train_data, hue=\"Sex\")","219f4862":"sns.countplot(x=\"Survived\", data=train_data, hue=\"Pclass\")","ce1b7d7a":"# show boxplot\n# Create a figure and a subplots, with size of figure 15X4\n# A box plot shows the distribution of quantitative data in a way that facilitates comparisons between variables\n# or across levels of a categorical variable\nfig, ax = plt.subplots(1, 4, figsize=(15, 4))\nsns.boxplot(data=train_data, x=\"Age\", color=\"#ce181f\", ax=ax[0], showmeans=True)\nsns.boxplot(data=train_data, x=\"SibSp\", color=\"#232f51\", ax=ax[1], showmeans=True)\nsns.boxplot(data=train_data, x=\"Parch\", color=\"#3b3742\", ax=ax[2], showmeans=True)\nsns.boxplot(data=train_data, x=\"Fare\", color=\"#70dc88\", ax=ax[3], showmeans=True)","27f973fb":"\nr, p = stats.pearsonr(train_data[\"Fare\"], train_data[\"Survived\"])\nj = sns.jointplot(x=\"Fare\",y=\"Survived\",data= train_data, color='steelblue')\nphantom, = j.ax_joint.plot([], [], linestyle=\"\", alpha=0)\nj.ax_joint.legend([phantom],[\"p={:f},r={:f}\".format(r,p)])","82b6c340":"# Correlation\ncorr = train_data.corr()\nprint(corr[\"Survived\"].sort_values(ascending=False))","fb11e86e":"# PassengerId values\n# we don't need this feature, so we go to drop this\ntrain_data = train_data.drop(\"PassengerId\", axis=1)\npassengerid = test_data[\"PassengerId\"]\ntest_data = test_data.drop(\"PassengerId\", axis=1)","d10b6b12":"# Name values\n# we don't need this feature, so we go to drop this\ntrain_data = train_data.drop(\"Name\", axis=1)\ntest_data = test_data.drop(\"Name\", axis=1)","d776c53f":"# Sex values\n# Convert from categorical to numerical\nenc = OrdinalEncoder()\ntrain_data[[\"Gender\"]] = enc.fit_transform(train_data[[\"Sex\"]])\ntest_data[[\"Gender\"]] = enc.fit_transform(test_data[[\"Sex\"]])\ntrain_data = train_data.drop(\"Sex\", axis=1)\ntest_data = test_data.drop(\"Sex\", axis=1)","7ea5ad7e":"# Age values\n# Fill null value\ntrain_data[\"Age\"] = train_data[\"Age\"].fillna(method=\"ffill\")\n# Cast data from type float to int\ntrain_data[\"Age\"] = train_data[\"Age\"].astype(int)\n# Drop outliers data\noutliers(train_data, \"Age\")\ntest_data[\"Age\"] = test_data[\"Age\"].fillna(method=\"ffill\")\ntest_data[\"Age\"] = test_data[\"Age\"].astype(int)\noutliers(test_data, \"Age\")","514c05cf":"# Jointplot\nr, p = stats.pearsonr(train_data[\"Age\"], train_data[\"Survived\"])\nj = sns.jointplot(x=\"Age\", y=\"Survived\", kind=\"kde\", fill=True,\n             thresh=0, data=train_data, color=\"blue\")\nphantom, = j.ax_joint.plot([], [], linestyle=\"\", alpha=0)\nj.ax_joint.legend([phantom],[\"p={:f},r={:f}\".format(r,p)])","390a50a2":"# Categorizing every age into a groups\ndata_set = [train_data, test_data]\nfor data in data_set:\n    data.loc[data[\"Age\"] <= 12, \"Age\"] = 0  # child passengers\n    data.loc[(data[\"Age\"] > 12) & (data[\"Age\"] <= 20), \"Age\"] = 1  # teenage passengers\n    data.loc[(data[\"Age\"] > 20) & (data[\"Age\"] <= 30), \"Age\"] = 2  # young passengers\n    data.loc[(data[\"Age\"] > 30) & (data[\"Age\"] <= 40), \"Age\"] = 3  # man passengers\n    data.loc[(data[\"Age\"] > 40) & (data[\"Age\"] <= 50), \"Age\"] = 4  # adult passengers\n    data.loc[(data[\"Age\"] > 50) & (data[\"Age\"] <= 60), \"Age\"] = 5  # old passengers\n    data.loc[(data[\"Age\"] > 60), \"Age\"] = 6  # very old passengers","380a1f0f":"# check Relative between passengers and Relatives\ntrain_data[\"Relatives\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\ntrain_data.loc[train_data[\"Relatives\"] > 0, \"not_singular\"] = 0\ntrain_data.loc[train_data[\"Relatives\"] == 0, \"not_singular\"] = 1\ntrain_data[\"not_singular\"] = train_data[\"not_singular\"].astype(int)\n\ntest_data[\"Relatives\"] = test_data[\"SibSp\"] + test_data[\"Parch\"]\ntest_data.loc[test_data[\"Relatives\"] > 0, \"not_singular\"] = 0\ntest_data.loc[test_data[\"Relatives\"] == 0, \"not_singular\"] = 1\ntest_data[\"not_singular\"] = test_data[\"not_singular\"].astype(int)","f27d622f":"# Ticket values\n# drop this because we have many unique value\ntrain_data = train_data.drop(\"Ticket\", axis=1)\ntest_data = test_data.drop(\"Ticket\", axis=1)","ff20b0f0":"# Fare values\n# Fill null value\ntrain_data[\"Fare\"] = train_data[\"Fare\"].fillna(method=\"ffill\")\ntest_data[\"Fare\"] = test_data[\"Fare\"].fillna(method=\"ffill\")\n# Drop outliers data\noutliers(train_data, \"Fare\")\noutliers(test_data, \"Fare\")","71b146af":"# Cabin values\n# drop this becuse we have many null value\ntrain_data = train_data.drop(\"Cabin\", axis=1)\ntest_data = test_data.drop(\"Cabin\", axis=1)","fc38c1ea":"# Embarked values\n# Fill null values\ntrain_data[\"Embarked\"] = train_data[\"Embarked\"].fillna(method=\"ffill\")\ntest_data[\"Embarked\"] = test_data[\"Embarked\"].fillna(method=\"ffill\")\n# Convert from categorical to numerical\nenc = OrdinalEncoder()\ntrain_data[[\"Embarked_encode\"]] = enc.fit_transform(train_data[[\"Embarked\"]])\ntest_data[[\"Embarked_encode\"]] = enc.fit_transform(test_data[[\"Embarked\"]])\ntrain_data = train_data.drop(\"Embarked\", axis=1)\ntest_data = test_data.drop(\"Embarked\", axis=1)","aee50d4b":"train_data.head()","140623f1":"test_data.head()","1f753f04":"# Standardization\nnumeric_coloumns = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\",\n                    \"Fare\", \"Gender\", \"Relatives\", \"not_singular\", \"Embarked_encode\"]\npipeline = ColumnTransformer([\n    (\"num\", StandardScaler(), numeric_coloumns,)\n])\ntrain_data_scaled = pd.DataFrame(pipeline.fit_transform(train_data), columns=[\"Pclass\", \"Age\", \"SibSp\", \"Parch\",\n                                                                              \"Fare\", \"Gender\", \"Relatives\",\n                                                                              \"not_singular\", \"Embarked_encode\"])\ntest_data_scaled = pd.DataFrame(pipeline.fit_transform(test_data), columns=[\"Pclass\", \"Age\", \"SibSp\", \"Parch\",\n                                                                            \"Fare\", \"Gender\", \"Relatives\",\n                                                                            \"not_singular\", \"Embarked_encode\"])","34b21ada":"train_y = train_data[\"Survived\"]\ntrain_x = train_data_scaled\ntest_x = test_data_scaled","f223ffb5":"# Logistic Regression\nlogistic_regression = LogisticRegression()\nlogistic_regression.fit(train_x, train_y)\nlogistic_regression_accuracy = round(logistic_regression.score(train_x, train_y) * 100, 3)","5415530e":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(train_x, train_y)\nrandom_forest_accuracy = round(random_forest.score(train_x, train_y) * 100, 3)","cc4a3816":"# KNN\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(train_x, train_y)\nknn_accuracy = round(knn.score(train_x, train_y) * 100, 3)","277425f9":"# Gaussian Naive Bayes\ngaussian_naive_bayes = GaussianNB()\ngaussian_naive_bayes.fit(train_x, train_y)\ngaussian_naive_bayes_accuracy = round(gaussian_naive_bayes.score(train_x, train_y) * 100, 3)","deed7dac":"# Perceptron\nperceptron = Perceptron(max_iter=5)\nperceptron.fit(train_x, train_y)\nperceptron_accuracy = round(perceptron.score(train_x, train_y) * 100, 3)","419fcf48":"# Linear Support Vector Machine:\nlinear_svc = LinearSVC()\nlinear_svc.fit(train_x, train_y)\nlinear_svc_accuracy = round(linear_svc.score(train_x, train_y) * 100, 3)","57dbd0fd":"# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(train_x, train_y)\ndecision_tree_accuracy = round(decision_tree.score(train_x, train_y) * 100, 3)","3154dd73":"# Stochastic Gradient Descent (SGD)\nsgd = linear_model.SGDClassifier(max_iter=5, tol=None)\nsgd.fit(train_x, train_y)\nsgd_accuracy = round(sgd.score(train_x, train_y) * 100, 3)","04b0ac30":"\n# Display accuracy of Algorithms\nAlgorithms = [\"Linear Support Vector Machines\", \"KNN\", \"Logistic Regression\",\n              \"Random Forest Classifier\", \"Gaussian Naive Bayes\", \"Perceptron\",\n              \"Stochastic Gradient Decent\",\n              \"Decision Tree\"]\nScores = [linear_svc_accuracy, knn_accuracy, logistic_regression_accuracy, random_forest_accuracy,\n          gaussian_naive_bayes_accuracy, perceptron_accuracy, sgd_accuracy, decision_tree_accuracy]\nAccuracy = [Algorithms, Scores]\nprint(tabulate(Accuracy))","eee08c18":"# Cross Validations Random Forest Classifier\nrf = RandomForestClassifier()\nscores = cross_val_score(rf, train_x, train_y, cv=10, scoring=\"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","01552cf2":"# Cross Validations Decision Tree\ndt = DecisionTreeClassifier()\nscores = cross_val_score(dt, train_x, train_y, cv=10, scoring=\"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","9a28221b":"# Grid Search\nparam_grid = {\"criterion\": [\"gini\", \"entropy\"], \"min_samples_leaf\": [1, 5, 10, 25, 40],\n              \"min_samples_split\": [2, 4, 10, 12, 16, 18, 24, 30],\n              \"n_estimators\": [100, 400, 700]}\n\nrf_g = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=42)\ngrid_rf = GridSearchCV(estimator=rf_g, param_grid=param_grid, n_jobs=5, cv=8)\ngrid_rf.fit(train_x, train_y)\nprint(\" Results from Grid Search \")\nprint(\"\\n The best estimator across ALL searched params:\\n\", grid_rf.best_estimator_)\nprint(\"\\n The best score across ALL searched params:\\n\", grid_rf.best_score_)\nprint(\"\\n The best parameters across ALL searched params:\\n\", grid_rf.best_params_)","5b79bb38":"# RFC with new parm\nrandom_forest = RandomForestClassifier(min_samples_split=16, n_estimators=100, oob_score=True,\n                                       random_state=42, criterion=\"gini\", min_samples_leaf=1,\n                                       max_features=\"auto\", n_jobs=5)\nrandom_forest.fit(train_x,train_y)\nrandom_forest_accuracy = round(random_forest.score(train_x, train_y) * 100, 3)\nrandom_forest_accuracy","0b43fdf7":"# Classification Report\n# Confusion Matrix\npredictions = cross_val_predict(random_forest, train_x, train_y, cv=8)\nprint(\"Confusion Matrix:\\n\", confusion_matrix(train_y, predictions))\nprint(\"Classification Report:\\n\", classification_report(train_y, predictions))","9ac5c97e":"# Building Machine Learning Models","27dcd892":"  ## * target train_data\n## Survived is our target train_data *","eaab06af":" # Data Visualization ","cf77895d":"# Analyze Data and Preparing  Data","dfb60f82":"### histogram"}}