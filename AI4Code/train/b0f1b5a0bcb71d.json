{"cell_type":{"27d2981d":"code","876f1216":"code","7eb1bf3f":"code","8e91fc61":"code","acaae831":"code","27dc6da6":"code","253a5b39":"code","f9c75441":"code","ebc75b77":"code","a7c816d3":"code","f9feccf4":"code","67d14007":"code","8bcb50f1":"code","56b269c1":"code","62febe32":"code","f84b9b3a":"markdown","8541f0c2":"markdown","aa086645":"markdown","32375b10":"markdown","4a26ff06":"markdown","d1bce2c6":"markdown"},"source":{"27d2981d":"import pandas as pd\nimport numpy as np\nimport datetime\nimport time\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n%matplotlib inline","876f1216":"#Import data\nprint('Importing data...')\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_history = pd.read_csv(\"..\/input\/historical_transactions.csv\")","7eb1bf3f":"#Preprocess transactions\nprint('Preprocessing historical transactions...')\ndf_history['authorized_flag'] = df_history['authorized_flag'].map({'Y':1, 'N':0})\ndf_history['category_1'] = df_history['category_1'].map({'Y':1, 'N':0})\ndf_history['purchase_date'] = pd.to_datetime(df_history['purchase_date'])\nlast_date_hist = datetime.datetime(2018, 2, 28)\ndf_history['time_since_purchase_date'] = ((last_date_hist - df_history['purchase_date']).dt.days)\ndf_history.loc[:, 'purchase_date'] = pd.DatetimeIndex(df_history['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n\ndf_history['installments'] = df_history['installments'].replace(999,-1)\ncols_with_nulls = ['city_id', 'state_id', 'subsector_id', 'installments']\nfor col in cols_with_nulls:\n    df_history[col] = df_history[col].replace(-1, np.nan)\n","8e91fc61":"#Perform aggregations by card ID\nprint('Aggregating historical transactions...')\n\nagg_func = {\n        'authorized_flag': ['mean'],\n        'city_id': ['nunique'], \n        'category_1': ['sum', 'mean'],\n        'installments': ['median', 'max'],\n        'category_3': ['nunique'],\n        'merchant_category_id': ['nunique'], \n        'merchant_id': ['nunique'],\n        'month_lag': ['min', 'max'],\n        'purchase_amount': ['sum', 'median', 'max', 'min'],\n        'purchase_date': ['min', 'max'],\n        'time_since_purchase_date': ['min', 'max', 'mean'],\n        'category_2': ['nunique'], \n        'state_id': ['nunique'], \n        'subsector_id': ['nunique']\n        }\n\n\nagg_history = df_history.groupby(['card_id']).agg(agg_func)\nagg_history.columns = ['hist_' + '_'.join(col).strip() for col in agg_history.columns.values]\nagg_history.reset_index(inplace=True)","acaae831":"#Merge with train and test\nprint('Merging all data...')\ndf_train_all = pd.merge(df_train, agg_history, on='card_id', how='left')","27dc6da6":"#Split initial train set into new train and test sets\ny_label_regr = df_train_all['target']\n\ndf_train_all = df_train_all.drop(['target',\n                                    'first_active_month', \n                                    'card_id'\n                                    ],\n                                     axis = 1)\n\ntrain_x, test_x, train_y, test_y = train_test_split(df_train_all, y_label_regr, test_size=0.7, random_state=42)\n\ntrain_x.reset_index(inplace=True, drop = True)\ntest_x.reset_index(inplace=True, drop = True)\ntrain_y.reset_index(inplace=True, drop = True)\ntest_y.reset_index(inplace=True, drop = True)","253a5b39":"#Train LightGBM model on original data\nparam = {'num_leaves': 111,\n         'min_data_in_leaf': 149,\n         'objective':'regression',\n         'max_depth': 9,\n         'learning_rate': 0.005,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.7522,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.7083 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.3134,\n         \"random_state\": 133,\n         \"verbosity\": -1}\n\nfeatures = train_x.columns\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof_normal = np.zeros(len(df_train_all))\npredictions_normal = np.zeros(len(test_x))\nstart = time.time()\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_x.values, train_y)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(train_x.iloc[trn_idx][features],\n                           label=train_y[trn_idx],\n                           #categorical_feature=cat_feats\n                           )\n    val_data = lgb.Dataset(train_x.iloc[val_idx][features],\n                           label=train_y[val_idx],\n                           #categorical_feature=cat_feats\n                           )\n\n    num_round = 10000\n    clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets=[trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds=200)\n\n    oof_normal[val_idx] = clf.predict(train_x.iloc[val_idx][features], num_iteration=clf.best_iteration)\n\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions_normal += clf.predict(test_x[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n","f9c75441":"#Perform Principal Component Analysis  \npca = PCA()\npca.fit(train_x)\npca.transform(train_x)","ebc75b77":"#Visualize Scree plot\nfig,ax=plt.subplots(1,2,figsize=(12,6))\npc_total=np.arange(1,pca.n_components_+1)\nax[0].plot(pc_total,np.cumsum(pca.explained_variance_ratio_))\nax[0].set_xticks(pc_total)\nax[0].set_xlabel('Principal Components')\nax[0].set_ylabel('Cumulative explained variance')\n###############################################################\nax[1].plot(pc_total,pca.explained_variance_)\nax[1].set_xticks(pc_total)\nax[1].set_xlabel('Principal Components')\nax[1].set_ylabel('Explained Variance Ratio')\nfig.suptitle('SCREE PLOT')\nplt.show()","a7c816d3":"var_exp_3 = sum(pca.explained_variance_ratio_[:3])\nprint('Variance explained by the first 3 PCA components:', var_exp_3)","f9feccf4":"#Visualize Biplot\n#Unfortunately this throws a size error here but not in my local notebook. I am just leaving the \n#code here commented out so you can use it on your own\n\n# y=pca.fit_transform(train_x)\n\n# plt.figure(figsize = (12,10))\n\n# xvector = pca.components_[0] \n# yvector = pca.components_[1]\n\n# xs = y[:,0]\n# ys = y[:,1]\n\n# ## visualize projections\n# for i in range(len(xvector)):\n#     plt.arrow(0, 0, xvector[i]*max(xs), yvector[i]*max(ys),\n#               color='darkred', width=0.2, head_width=0.5)\n#     plt.text(xvector[i]*max(xs)*1.2, yvector[i]*max(ys)*1.2,\n#              list(train_x.columns)[i], color='darkred', fontsize=6)\n\n# plt.scatter(xs,ys,c='b',alpha=0.02)\n# plt.axhline(0, color='black',alpha=0.8)\n# plt.axvline(0, color='black',alpha=0.8)\n# plt.xlim(-300,300)\n# plt.ylim(-300,300)\n# plt.show()","67d14007":"#Create train and test sets from the first 3 PCA components\npca_train_x= pca.transform(train_x)\npca_train_x = pca_train_x[:,:3]\npca_train_x = pd.DataFrame(pca_train_x, columns=['comp1', 'comp2', 'comp3'])\n\npca_test_x= pca.transform(test_x)\npca_test_x = pca_test_x[:,:3]\npca_test_x = pd.DataFrame(pca_test_x, columns=['comp1', 'comp2', 'comp3'])","8bcb50f1":"#Train LightGBM model on these 3 PCA components only\nparam = {#'num_leaves': 21,\n         #'min_data_in_leaf': 49,\n         'objective':'regression',\n         'max_depth': 8,\n         'learning_rate': 0.001,\n         #\"boosting\": \"gbdt\",\n         #\"feature_fraction\": 0.5,\n         #\"bagging_freq\": 1,\n         #\"bagging_fraction\": 0.5 ,\n         #\"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         #\"lambda_l1\": 0.3134,\n         \"random_state\": 133,\n         #\"is_unbalance\": True,\n         \"verbosity\": -1}\n\nfeatures = pca_train_x.columns\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof_pca = np.zeros(len(pca_train_x))\npredictions_pca = np.zeros(len(pca_test_x))\nstart = time.time()\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(pca_train_x.values, train_y)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(pca_train_x.iloc[trn_idx][features],\n                           label=train_y[trn_idx],\n                           #categorical_feature=cat_feats\n                           )\n    val_data = lgb.Dataset(pca_train_x.iloc[val_idx][features],\n                           label=train_y[val_idx],\n                           #categorical_feature=cat_feats\n                           )\n\n    num_round = 10000\n    clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets=[trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds=200)\n\n    oof_pca[val_idx] = clf.predict(pca_train_x.iloc[val_idx][features], num_iteration=clf.best_iteration)\n\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions_pca += clf.predict(pca_test_x[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n","56b269c1":"#Compare performance\nprint(\"RMSE test normal: {:<8.5f}\".format(mean_squared_error(predictions_normal, test_y) ** 0.5))\nprint(\"RMSE test PCA: {:<8.5f}\".format(mean_squared_error(predictions_pca, test_y) ** 0.5))","62febe32":"#Add these 3 PCA components as features on original data and re-run model\ntrain_x_2 = pd.concat([train_x, pca_train_x], axis = 1)\ntest_x_2 = pd.concat([test_x, pca_test_x], axis = 1)\n\ndel train_x\ndel test_x\ndel pca_train_x\ndel pca_test_x\n\n#Train LightGBM model\nparam = {'num_leaves': 111,\n         'min_data_in_leaf': 149,\n         'objective':'regression',\n         'max_depth': 9,\n         'learning_rate': 0.005,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.7522,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.7083 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.3134,\n         \"random_state\": 133,\n         \"verbosity\": -1}\n\nfeatures = train_x_2.columns\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof_pca = np.zeros(len(train_x_2))\npredictions_all = np.zeros(len(test_x_2))\nstart = time.time()\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_x_2.values, train_y)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(train_x_2.iloc[trn_idx][features],\n                           label=train_y[trn_idx],\n                           #categorical_feature=cat_feats\n                           )\n    val_data = lgb.Dataset(train_x_2.iloc[val_idx][features],\n                           label=train_y[val_idx],\n                           #categorical_feature=cat_feats\n                           )\n\n    num_round = 10000\n    clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets=[trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds=200)\n\n    oof_pca[val_idx] = clf.predict(train_x_2.iloc[val_idx][features], num_iteration=clf.best_iteration)\n\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions_all += clf.predict(test_x_2[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"RMSE test PCA: {:<8.5f}\".format(mean_squared_error(predictions_all, test_y) ** 0.5))","f84b9b3a":"We see that using the three components as new features produces around the same score with our initial model. Maybe more or less components can actually improve our model. I leave that for your experimentation...Hope you enjoyed it!","8541f0c2":"Let's now re-run our initial model after adding the three PCA components as features and see what we get.","aa086645":"Now let's train a LightGBM model using only these 3 components.","32375b10":"## Goal of the Notebook\nAfter looking at the various kernels by fellow Kagglers I came to realize that incresign complexity was only marginally improving RMSE (at least from available kernels). So I thought of running a PCA to perform a dimensionality check. I soon came to realize that most of the available data is just noise: the three first components of PCA account for 99.99% of the variance as we will see below. I also used these three components as added features and re-run my model but to no avail. I hope you find this analysis useful and you come to new ideas of why this might be happening. ","4a26ff06":"We see that the loss is minimal. Maybe not minimal for a Kaggle competition but a) in real life situations we would indeed opt for the three new features vs original 29 and b) the 3-feature model is not fine-tuned thus the difference may well be even smaller than that.","d1bce2c6":"First lets train a model to check its performance using the original data."}}