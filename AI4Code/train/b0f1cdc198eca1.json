{"cell_type":{"d4c19c90":"code","e902fdb2":"code","41083a8a":"code","ac5f4c98":"code","dca19fd0":"code","718c9ab0":"code","62f1505a":"code","91eaf7fd":"code","3a8cbe75":"code","8f332bf5":"code","7950dc6c":"code","89a2da18":"code","8b152702":"code","229e4677":"markdown"},"source":{"d4c19c90":"# import libraries\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom optuna.samplers import TPESampler\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\nfrom xgboost import cv\nfrom sklearn.ensemble import VotingRegressor\nimport xgboost as xgb\nimport lightgbm as lgbm\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n","e902fdb2":"# import data\ntrain_df = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\n\n# separate data\nX = train_df.drop(['loss', 'id'], axis=1)\ny = train_df['loss']\nX_test = test_df.drop(['id'], axis=1)\n\nXGB_OPTUNA = False\nCAT_OPTUNA = True\nLGBM_OPTUNA = False\n\nEARLY_OPTUNA = 30\nEARLY_FIT = 100","41083a8a":"# scale data\nscaler = StandardScaler()\nscaler.fit(pd.concat([X, X_test]))\nX = scaler.transform(X)\nX_test = scaler.transform(X_test)","ac5f4c98":"def xgb_objective(trial,data=X,target=y):\n    X_train, X_valid, y_train, y_valid = train_test_split(data, target, test_size=0.4,random_state=42)\n    param_grid = {'max_depth': trial.suggest_int('max_depth', 4, 10),\n                  'n_estimators': trial.suggest_int('n_estimators', 1200, 4800, 400), \n                  'eta': trial.suggest_float('eta', 0.006, 0.05),\n                  'subsample': trial.suggest_discrete_uniform('subsample', 0.3, 0.9, 0.05),\n                  'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.6, 1.0, 0.1),\n                  'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.3, 0.7, 0.1),\n                  'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-3, 1e2),\n                  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 1e4),\n                  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 1e4),\n                  'gamma': trial.suggest_loguniform('gamma', 1e-6, 1e3)} \n    \n    model = xgb.XGBRegressor(tree_method='gpu_hist',\n                             predictor='gpu_predictor',\n                             n_jobs=4,\n                             **param_grid)\n    \n    model.fit(X_train, y_train,\n              eval_set=[(X_valid, y_valid)],\n              eval_metric='rmse',\n              early_stopping_rounds=EARLY_OPTUNA,\n              verbose=False)\n\n    return mean_squared_error(y_valid, model.predict(X_valid), squared=False)","dca19fd0":"def cat_objective(trial,data=X,target=y):\n    X_train, X_valid, y_train, y_valid = train_test_split(data, target, test_size=0.4,random_state=42)\n    params = {'iterations':trial.suggest_int(\"iterations\", 1000, 10000),\n              'od_wait':trial.suggest_int('od_wait', 500, 2000),\n              'learning_rate' : trial.suggest_uniform('learning_rate',0.02,0.5),\n              'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 1e2),\n              'subsample': trial.suggest_discrete_uniform('subsample', 0.3, 1.0, 0.05),\n              'random_strength': trial.suggest_uniform('random_strength',10,50),\n              'depth': trial.suggest_int('depth',5,15),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',5,35),\n              'max_bin': trial.suggest_int('max_bin', 1, 300),\n              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15)}\n    \n    model = CatBoostRegressor(loss_function='RMSE',\n                              task_type=\"GPU\",\n                              eval_metric='RMSE',\n                              leaf_estimation_method='Newton',\n                              bootstrap_type= 'Bernoulli',\n                              **params)  \n    \n    model.fit(X_train,y_train,\n              eval_set=[(X_valid,y_valid)],\n              early_stopping_rounds=EARLY_OPTUNA,\n              verbose=False)\n        \n    return mean_squared_error(y_valid, model.predict(X_valid), squared=False)","718c9ab0":"def lgbm_objective(trial,data=X,target=y):\n    X_train, X_valid, y_train, y_valid = train_test_split(data, target, test_size=0.4,random_state=42)\n    params = {\n        'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 1e-3, 1e3),\n        'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 1e-3, 1e3),\n        'num_leaves': trial.suggest_int(\"num_leaves\", 100, 500),\n        'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n        'subsample': trial.suggest_float(\"subsample\", 0.3, 0.6),\n        'subsample_freq': trial.suggest_int(\"subsample_freq\", 0, 5),\n        'min_child_samples': trial.suggest_int(\"min_child_samples\", 5, 80),\n        'max_depth': trial.suggest_int('max_depth', 4, 10),\n        'n_estimators': trial.suggest_int('n_estimators', 1200, 10000, 400),\n        'learning_rate' : trial.suggest_uniform('learning_rate',0.01,0.5)}\n    \n    model = lgbm.LGBMRegressor(device = 'gpu',\n                               boosting_type = 'gbdt',\n                               random_state=42,\n                               metric= \"RMSE\",\n                               verbosity= -1,\n                               n_jobs=-1,\n                               **params)\n    \n    model.fit(X_train,y_train,\n              eval_set=[(X_valid,y_valid)],\n              early_stopping_rounds=EARLY_OPTUNA,\n              verbose = False)\n    \n    return mean_squared_error(y_valid, model.predict(X_valid), squared=False)","62f1505a":"def create_optuna_study(objective, study_name, train_time):\n    study = optuna.create_study(direction='minimize', \n                                sampler=TPESampler(), \n                                study_name=study_name)\n    study.optimize(objective, \n                   timeout=train_time)\n    trial = study.best_trial\n    \n    print('Number of finished trials: ', len(study.trials))\n    print('Best trial:')\n    print('\\tValue: {}'.format(trial.value))\n    print('\\tParams: ')\n    for key, value in trial.params.items():\n        print('\\t\\t{}: {}'.format(key, value))\n    \n    return trial, study","91eaf7fd":"# Optimize for 30 minutes\ntrain_time = 1 * 60 * 10\n\n# XGB Optimize\nif XGB_OPTUNA:\n    xgb_trial, xgb_study = create_optuna_study(xgb_objective, 'XGBRegressor', train_time)\n    xgb_params = xgb_trial.params\n\nelse:\n    # \tValue: 7.845515259148512\n    xgb_params = {'max_depth': 10,\n                'n_estimators': 4800,\n                'eta': 0.006004080448420365,\n                'subsample': 0.7,\n                'colsample_bytree': 0.8,\n                'colsample_bylevel': 0.5,\n                'min_child_weight': 26.692885575205427,\n                'reg_lambda': 30.107616169341984,\n                'reg_alpha': 0.055043751121261995,\n                'gamma': 1.7568597718693732e-05}\nxgb_params['tree_method'] = 'gpu_hist'\nxgb_params['predictor'] = 'gpu_predictor'\nxgb_params['n_jobs'] = 4\n\n# Catboost Optimize\nif CAT_OPTUNA:\n    cat_trial, cat_study = create_optuna_study(cat_objective, 'CatRegressor', train_time)\n    cat_params = cat_trial.params\nelse:\n    # \tValue: 7.8525427867211395\n    cat_params = {'iterations': 5249,\n                    'od_wait': 1455,\n                    'learning_rate': 0.027166443415647373,\n                    'reg_lambda': 0.034058831747453534,\n                    'subsample': 0.8500000000000001,\n                    'random_strength': 18.348082769035948,\n                    'depth': 5,\n                    'min_data_in_leaf': 13,\n                    'leaf_estimation_iterations': 7}\ncat_params['loss_function'] = 'RMSE'\ncat_params['eval_metric'] = 'RMSE'\ncat_params['bootstrap_type']= 'Bernoulli'\ncat_params['leaf_estimation_method'] = 'Newton'\ncat_params['random_state'] = 0\ncat_params['task_type']='GPU'\n\n\n# LGBM Optimize\nif LGBM_OPTUNA:\n    lgbm_trial, lgbm_study = create_optuna_study(lgbm_objective, 'LGBMRegressor', train_time)\n    lgbm_params = lgbm_trial.params\nelse:\n    # \tValue: 7.848009541262737\n    lgbm_params = {'reg_alpha': 196.9980774975926,\n                'reg_lambda': 1.0086558093083937,\n                'num_leaves': 482,\n                'colsample_bytree': 0.49585597524837915,\n                'subsample': 0.5964181419864539,\n                'subsample_freq': 3,\n                'min_child_samples': 78,\n                'max_depth': 5,\n                'n_estimators': 6000,\n                'learning_rate': 0.01373955979023822}\n            \nlgbm_params['metric'] = 'RMSE'\nlgbm_params['random_state'] = 0\nlgbm_params['device'] = 'gpu'\nlgbm_params['n_jobs'] = -1\n\n","3a8cbe75":"#optuna.visualization.plot_param_importances(xgb_study)","8f332bf5":"optuna.visualization.plot_param_importances(cat_study)","7950dc6c":"#optuna.visualization.plot_param_importances(lgbm_study)","89a2da18":"'''optuna.visualization.plot_optimization_history(study)\noptuna.visualization.plot_param_importances(study)\noptuna.visualization.plot_parallel_coordinate(study)\noptuna.visualization.plot_slice(study)'''","8b152702":"test_preds = np.zeros(len(X_test))\nn_splits = 10\n\nkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X, y), 1):\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_valid, y_valid = X[valid_idx], y[valid_idx]\n    \n    xgb_model = xgb.XGBRegressor(**xgb_params)\n    xgb_model.fit(X_train, y_train,\n                  eval_set=[(X_valid, y_valid)],\n                  early_stopping_rounds=EARLY_FIT,\n                  verbose=False)\n    \n    lgbm_model = LGBMRegressor(**lgbm_params)\n    lgbm_model.fit(X_train, y_train,\n                   eval_set=[(X_valid, y_valid)],\n                   early_stopping_rounds=EARLY_FIT,\n                   verbose=False)\n    \n    cat_model = CatBoostRegressor(**cat_params)\n    cat_model.fit(X_train, y_train,\n                  eval_set=[(X_valid, y_valid)],\n                  early_stopping_rounds=EARLY_FIT,\n                  verbose=False)\n    \n    test_preds += xgb_model.predict(X_test) * 0.7\n    test_preds += lgbm_model.predict(X_test) * 0.15\n    test_preds += cat_model.predict(X_test) * 0.15\n    \n    xgb_rmse = mean_squared_error(y_valid, xgb_model.predict(X_valid), squared=False)\n    lgbm_rmse = mean_squared_error(y_valid, lgbm_model.predict(X_valid), squared=False)\n    cat_rmse = mean_squared_error(y_valid, cat_model.predict(X_valid), squared=False)\n    \n    print(f'Fold {fold}\/{n_splits}\\n\\txgb: {xgb_rmse}\\n\\tlbgm: {lgbm_rmse}\\n\\tcat: {cat_rmse}\\n')\n\ntest_preds \/= n_splits \n\nsubmission['loss'] = test_preds\nsubmission.to_csv('submission.csv', index=False)","229e4677":" A lot of what I have implemented here is creditted to \n 1. https:\/\/www.kaggle.com\/michael127001\/xgbregressor-with-optuna-tuning\n 2. https:\/\/www.kaggle.com\/pranjalverma08\/tps-08-cb-lgbm-xgb-starter\n 3. https:\/\/www.kaggle.com\/dmitryuarov\/falling-below-7-87-voting-cb-xgb-lgbm"}}