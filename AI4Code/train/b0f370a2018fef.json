{"cell_type":{"5011d07b":"code","506e088f":"code","50f2ffb6":"code","e4443769":"code","56149f2f":"code","20573923":"code","a785a445":"code","19672cd5":"code","25e93e00":"code","4c7c6160":"code","f2e62b1b":"code","8224b005":"code","1eb16433":"code","7c16ae44":"code","68880000":"code","dee0a893":"code","11ff5a2a":"code","5ca75edb":"code","151c5641":"code","2d365bf0":"code","4b2f2b18":"code","666d788f":"code","7f1574c8":"code","041d19ec":"code","7b182f5e":"code","38834131":"code","2b9492fd":"code","e25990b0":"markdown","bb10c7fb":"markdown","19a9b025":"markdown","43b47136":"markdown","6d2304eb":"markdown","7ef3a052":"markdown","633840bb":"markdown","1785f5dd":"markdown","0467f155":"markdown","5274b35d":"markdown","bd2989cf":"markdown","11caeef6":"markdown","e17a27b9":"markdown","e31aed9a":"markdown","21755925":"markdown","1f7c83ca":"markdown","1685ace6":"markdown","09e49245":"markdown"},"source":{"5011d07b":"# libraries we will use\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, roc_curve,confusion_matrix,f1_score,precision_recall_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score,StratifiedShuffleSplit\n","506e088f":"data = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndata_copy = data.copy()\ndata.head()","50f2ffb6":"outcome_values = data['Outcome'].value_counts()\noutcome_values.sort_index()\nplt.pie(outcome_values.values, labels=['Negative', 'Positive'], autopct='%1.1f%%')\nplt.title('Diabete result')\nplt.show()","e4443769":"data.info()","56149f2f":"data.describe()","20573923":"plt.figure(figsize = (17,14))\ncorr = data.corr()\nsns.heatmap(corr, annot=True)\nplt.show()","a785a445":"fig, axes = plt.subplots(3,3,figsize = (18,16))\n\nfor i,feature in enumerate(data.columns):\n    sns.histplot(data[feature], ax=axes[i\/\/3, i%3])","19672cd5":"plt.figure(figsize=(10,8))\nsns.scatterplot(data=data, x = 'Glucose', y='Insulin', hue = 'Outcome')\nplt.show()","25e93e00":"missing_cols = ['Glucose', 'Insulin', 'SkinThickness', 'BloodPressure', 'BMI']\nmissing_counts = {}\ntotal_rows = data.shape[0]\nfor col in missing_cols:\n    count = (data[col] == 0).sum()\n    missing_counts[col] = count\n\nplt.figure(figsize=(13,10))\nax = sns.barplot(x=list(missing_counts.keys()),y=list(missing_counts.values()))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2,\n            height + 3,\n            '{:1.2f}%'.format(height*100\/total_rows),\n            ha=\"center\") \nplt.title('Distribution of missing values')\nplt.show()","4c7c6160":"for col in missing_cols:\n    data.loc[data[col] == 0.0, [col]] = None","f2e62b1b":"fig,ax = plt.subplots(2,3,figsize = (15,10))\nfor i, col in enumerate(missing_cols):\n    sns.boxplot(y = data[col], x=[\"\"]*(data.shape[0]),hue = data['Outcome'], ax = ax[i\/\/3][i%3])\nax[1][2].set_visible(False)","8224b005":"\nfor col in missing_cols:\n    \n    positive_median = data[data['Outcome'] == 1][col].median()\n    negative_median = data[data['Outcome'] == 0][col].median()\n    \n    data.loc[(data['Outcome']==0)&(data[col].isna()),col] = negative_median\n    data.loc[(data['Outcome']==1)&(data[col].isna()),col] = positive_median","1eb16433":"fig, ax = plt.subplots(3,1,figsize=(15,15))\n\nsns.histplot(x=data['BMI'],hue=data['Outcome'],multiple='stack',ax=ax[0])\nsns.countplot(x=data['Age'],hue=data['Outcome'],ax=ax[1])\nsns.countplot(x=data['Pregnancies'],hue=data['Outcome'],ax=ax[2])\n\n\nplt.show()","7c16ae44":"data['CategoricalAge'] = pd.qcut(data['Age'], q = 5)\ndata['CategoricalBMI'] = pd.qcut(data['BMI'], q=5)\ndata['CategoricalPregnancies'] = pd.qcut(data['Pregnancies'], q=5)\n\n\nfig, ax = plt.subplots(3,1,figsize=(15,15))\n\nsns.countplot(x=data['CategoricalBMI'],hue=data['Outcome'],ax=ax[0])\nsns.countplot(x=data['CategoricalAge'],hue=data['Outcome'],ax=ax[1])\nsns.countplot(x=data['CategoricalPregnancies'],hue=data['Outcome'],ax=ax[2])\n\nplt.show()","68880000":"data = pd.get_dummies(data)\ndata.drop(['Age', 'Pregnancies', 'BMI'], axis = 1, inplace=True)","dee0a893":"continuous_variables = ['Glucose','BloodPressure','SkinThickness','Insulin','DiabetesPedigreeFunction']\nplt.figure(figsize = (17,14))\nsns.pairplot(data[continuous_variables+['Outcome']],hue='Outcome')","11ff5a2a":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata[continuous_variables] = pd.DataFrame(scaler.fit_transform(data[continuous_variables]))","5ca75edb":"data_x = data.drop(['Outcome'], axis = 1)\ndata_y = data['Outcome']\n\n\nsss = StratifiedShuffleSplit(test_size=0.3,n_splits=1,random_state=4321)\ntrain_val_index, test_index = next(sss.split(data_x, data_y))\nX_train, X_test = data_x.iloc[train_val_index, :], data_x.iloc[test_index]\ny_train, y_test = data_y[train_val_index], data_y[test_index]\n\nX_train.reset_index(drop = True, inplace=True)\ny_train.reset_index(drop=True,inplace = True)","151c5641":"scores = {}\nmodels = [LogisticRegression(max_iter=10000),KNeighborsClassifier(),RandomForestClassifier(random_state=42),GradientBoostingClassifier(random_state=42)]\nfor model in models:\n    cv_scores = cross_val_score(model, X_train,y_train)\n    estimator = model.__class__.__name__\n    scores[estimator] = np.mean(cv_scores)*100\nax = sns.barplot(y=list(scores.keys()),x=list(scores.values()),orient='h')\nfor p in ax.patches:\n    width = p.get_width()\n    ax.text( width\/2,\n            p.get_y()+0.5,\n            '{:1.2f}%'.format(width))","2d365bf0":"params = {'n_estimators':np.arange(100,1001,100),\n         'max_depth':np.arange(2,41,2)}\n    \nrfc_cv = GridSearchCV(RandomForestClassifier(),param_grid=params, cv = 5, verbose = 2,n_jobs=5,scoring = 'f1').fit(X_train, y_train)","4b2f2b18":"rfc_model = rfc_cv.best_estimator_\nrfc_cv.best_score_,rfc_cv.best_params_","666d788f":"def scoring(model, X_test, y_test):\n    y_pred = model.predict(X_test)\n    rfc_mat = confusion_matrix(y_test, y_pred)\n    print('Recall: ', recall_score(y_test, y_pred))\n    print('Precision: ', precision_score(y_test, y_pred))\n    print('Roc-auc score: ',roc_auc_score(y_test,y_pred))\n    print('F1 score: ', f1_score(y_test, y_pred))\n    sns.heatmap(rfc_mat,annot=True,fmt='1')\n    plt.xlabel('True classes')\n    plt.ylabel('Predictions')\n    plt.show()\n    \n    ","7f1574c8":"scoring(rfc_model, X_test, y_test)","041d19ec":"predict_proba = rfc_model.predict_proba(X_test)\nprecision, recall, thresholds = precision_recall_curve(y_test, predict_proba[:,1])\nthresholds=np.concatenate([thresholds, [1.0]])\n","7b182f5e":"fig, ax =   plt.subplots(1,2,figsize = (15,5))\nax[0].grid()\nax[0].plot(recall, precision)\nax[0].set_xlabel('Recall')\nax[0].set_ylabel('Precision')\nax[0].plot([0.9, 0.9], [min(precision),1],'g--')\nax[1].grid()\nax[1].plot(thresholds, precision,'r',label = 'Precision')\nax[1].plot(thresholds, recall, label = 'Recall')\nax[1].set_xlabel('Thresholds')\nax[1].legend()\nax[1].plot([min(thresholds), 1], [0.9,0.9],'g--')\n\nplt.show()","38834131":"ind = np.argmin(recall >= 0.90)-1\nthreshold = thresholds[ind]\nprint('Threshold is {}'.format(threshold))\ny_pred = (predict_proba[:,1]>threshold).astype(np.int32)\nsns.heatmap(confusion_matrix(y_test, y_pred), annot = True,fmt = '1')\nplt.show()","2b9492fd":"print('AUC:',roc_auc_score(y_test, y_pred))","e25990b0":"We can get 90% without handling outliers, so I will leave them as they are. This is my first time sharing notebook here and I have mistakes of course. Please do tell me what can I improve. If you like my work, consider upvoting ","bb10c7fb":"It now looks much better, let's one-hot encode these categorical columns and drop previous features","19a9b025":"Looks like we can get about 0.85 precision with 0.9 recall, We can find the threshold where recall is greater than 0.9, but also precision isn't too low.","43b47136":"Some columns have minimum value of 0, which means they can be missing values, even though there are no NaN value in dataset","6d2304eb":"Let's mark 0 values as NaN to make things easier","7ef3a052":"It looks like there are no missing values and there are no categorical variables , that makes it easier to work with datasets.","633840bb":"From correlation matrix, we can see that there are no highly correlated features with outcome, but we can note glucose whose corelation with outcome is 0.47. There is also moderate correlation between pregnancies and age, which is understandable. It can also be seen that BloodPressure and SkinThicknes aren't correlated with Outcome.\nLet's look at the distribution of features:","1785f5dd":"There are several columns that can't be 0. If your blood pressure or glucose level is 0, then you are dead. BMI also can't be 0 as it is weight\/(height\\*height). Skin thickness can't be 0 too. Although insulin can be 0, it isn't likely that about half of women in this dataset to have 0 insulin. Let's compare insulin, glucose level and outcome:","0467f155":"Reading data and taking a quick look at it","5274b35d":"Older people's chances of having diabete is slightly higher than younger people's, but it's not that clear from barplot (look at ages 37-38-39). Let's divide these features to ranges and treat them as categoric variables","bd2989cf":"Now we clearly see that, we can't just impute median value to missing columns. We should take Outcome into account","11caeef6":"Let's look at age, pregnancy and BMI with regard to outcome. Even though they are continuous variables, it doesn't make sense to use them as it is.","e17a27b9":"It is sensitive data and we must increase recall to predict as much diabetic patients as possible, let's look at precision and recall and see if we can choose .","e31aed9a":"It is clear that there are enough examples of both classes and we will easily split data to train-test splits","21755925":"Since there are not same amount of classes, we should use StratifiedShuffleSplit to keep ratio in test\/train set","1f7c83ca":"RandomForestClassifier look okay, let's fine tune it","1685ace6":"I am no medical expert, but I know that 0 insulin leads to type-I diabete and in this scatterplot we see that there are several patients with insulin level of 0, but they are not diagnosed with diabete, which means there is missing data. To be honest, I can't be sure that all those 0 level insulins are missing, nevertheless I will fill them. In the scatterplot we can also see that Outcome classes doesn't have the same distribution at least in Glucose-Insulin relation, so when we are imputing values, we have to be careful of class. Also let's look at how many zeros are in columns.","09e49245":"There are outlier in data and we will deal with them after checking accuracies"}}