{"cell_type":{"4b3c400a":"code","a5636050":"code","5afd7882":"code","925fd7da":"code","9d0b8d01":"code","06d86ebc":"code","c2870ab9":"code","b0d42e89":"code","cc393968":"code","ded71dbf":"code","a9b7f38c":"code","cb3eff22":"code","5aaee768":"code","d7c0932e":"code","2d1385cb":"code","54c29ce7":"code","7f2f9791":"code","fac04f4d":"code","a4bdde2b":"code","dbbfb225":"code","2d1a0284":"code","3dfd51b9":"code","bd6ba28e":"code","c04aef68":"code","aed9a1ea":"code","492e40bf":"code","224b860d":"code","55d4b410":"code","ab2ea329":"code","d3c20532":"code","0b04ddc4":"code","87a1514a":"code","6d3f4876":"code","48742fdc":"code","b752627a":"code","fa4d1c39":"code","70a32731":"code","96aa9ea1":"code","ee4d57fd":"code","36bcb550":"code","c4de48d7":"code","2ec6583e":"markdown","13af8f0e":"markdown","c77fdf92":"markdown","aecfabc8":"markdown","4f310ceb":"markdown","961b05f1":"markdown","424f80b6":"markdown","496165dd":"markdown","916bbe44":"markdown","d35b0327":"markdown","3f6efd53":"markdown","a4cc8947":"markdown","0374098f":"markdown","46f400aa":"markdown","f79979eb":"markdown","223617a8":"markdown","2e4929fb":"markdown","577c02de":"markdown","595a3245":"markdown","973a3113":"markdown","203baaee":"markdown"},"source":{"4b3c400a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.options.display.float_format = '{:.2f}'.format\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import precision_recall_curve","a5636050":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","5afd7882":"data.shape","925fd7da":"data.columns","9d0b8d01":"data.info()","06d86ebc":"data.describe()","c2870ab9":"sns.heatmap(data.isnull(),cmap = 'magma',cbar = False)","b0d42e89":"f = data[data['Class'] == 1]\nnf = data[data['Class'] == 0]","cc393968":"f.describe()","ded71dbf":"nf.describe()","a9b7f38c":"fraud = len(data[data['Class'] == 1])\/len(data)*100\nno_fraud = len(data[data['Class'] == 0])\/len(data)*100\nfraud_percentage = [fraud,no_fraud]","cb3eff22":"fig,ax = plt.subplots(nrows = 1,ncols = 3,figsize = (20,5))\nplt.subplot(1,3,1)\nplt.pie(fraud_percentage,labels = ['Fraud','No Fraud'],autopct='%1.1f%%',startangle = 90,)\nplt.title('FRAUD PERCENTAGE')\n\nplt.subplot(1,3,2)\nsns.countplot('Class',data = data,)\nplt.title('DISTRIBUTION OF FRAUD CASES')\n\nplt.subplot(1,3,3)\nsns.scatterplot('Time','Amount',data = data,hue = 'Class')\nplt.title('TIME vs AMOUNT w.r.t CLASS')\nplt.show()","5aaee768":"sns.heatmap(data.corr(),cmap = 'RdBu',cbar = True)","d7c0932e":"corr = data.corrwith(data['Class']).sort_values(ascending = False).to_frame()\ncorr.columns = ['Correlations']\nplt.subplots(figsize = (5,25))\nsns.heatmap(corr,annot = True,cmap = 'RdBu',linewidths = 0.4,linecolor = 'black')\nplt.title('CORRELATION w.r.t CLASS')","2d1385cb":"df = data[['V4','V11','V7','V3','V16','V10','V12','V14','V17','Class']]\ndf.head()","54c29ce7":"import imblearn\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\n\nimblearn.__version__","7f2f9791":"def model(classifier):\n    \n    classifier.fit(x_train,y_train)\n    prediction = classifier.predict(x_test)\n    cv = RepeatedStratifiedKFold(n_splits = 10,n_repeats = 3,random_state = 1)\n    print(\"CROSS VALIDATION SCORE : \",'{0:.2%}'.format(cross_val_score(classifier,x_train,y_train,cv = cv,scoring = 'roc_auc').mean()))\n    print(\"ROC_AUC SCORE : \",'{0:.2%}'.format(roc_auc_score(y_test,prediction)))\n    plot_roc_curve(classifier, x_test,y_test)\n    plt.title('ROC_AUC_PLOT')\n    plt.show()","fac04f4d":"def model_evaluation(classifier):\n    \n    # CONFUSION MATRIX\n    cm = confusion_matrix(y_test,classifier.predict(x_test))\n    names = ['True Neg','False Pos','False Neg','True Pos']\n    counts = [value for value in cm.flatten()]\n    percentages = ['{0:.2%}'.format(value) for value in cm.flatten()\/np.sum(cm)]\n    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(names,counts,percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(cm,annot = labels,cmap = 'Blues',fmt ='')\n    \n    # CLASSIFICATION REPORT\n    print(classification_report(y_test,classifier.predict(x_test)))","a4bdde2b":"over = SMOTE(sampling_strategy= 0.5)\nunder = RandomUnderSampler(sampling_strategy = 0.1)\nfeatures = df.iloc[:,:9].values\ntarget = df.iloc[:,9].values","dbbfb225":"steps = [('under', under),('over', over)]\npipeline = Pipeline(steps=steps)\nfeatures, target = pipeline.fit_resample(features, target)\nCounter(target)","2d1a0284":"x_train, x_test, y_train, y_test = train_test_split(features, target, test_size = 0.20, random_state = 2)","3dfd51b9":"from sklearn.linear_model import LogisticRegression","bd6ba28e":"classifier_lr = LogisticRegression(random_state = 0,C=10,penalty= 'l2') ","c04aef68":"model(classifier_lr)","aed9a1ea":"model_evaluation(classifier_lr)","492e40bf":"from sklearn.svm import SVC","224b860d":"classifier_svc = SVC(kernel = 'linear',C = 0.1)","55d4b410":"model(classifier_svc)","ab2ea329":"model_evaluation(classifier_svc)","d3c20532":"from sklearn.tree import DecisionTreeClassifier","0b04ddc4":"classifier_dt = DecisionTreeClassifier(random_state = 1000,max_depth = 4,min_samples_leaf = 1)","87a1514a":"model(classifier_dt)","6d3f4876":"model_evaluation(classifier_dt)","48742fdc":"from sklearn.ensemble import RandomForestClassifier","b752627a":"classifier_rf = RandomForestClassifier(max_depth = 4,random_state = 0)","fa4d1c39":"model(classifier_rf)","70a32731":"model_evaluation(classifier_rf)","96aa9ea1":"from sklearn.neighbors import KNeighborsClassifier","ee4d57fd":"classifier_knn = KNeighborsClassifier(leaf_size = 1, n_neighbors = 3,p = 1)","36bcb550":"model(classifier_knn)","c4de48d7":"model_evaluation(classifier_knn)","2ec6583e":"### 4] RANDOM FOREST CLASSIFIER:- ","13af8f0e":"# FEATURE SELECTION","c77fdf92":"### 1] LOGISTIC REGRESSION:-","aecfabc8":"### CLEARLY THE DATA IS UNBALANCED \n- THIS WILL BIAS OUR CLASSIFICATION RESULTS TOWARDS THE MAJORITY [NO FRAUDS]\n- FOR BALACING THE DATA, WE HAVE 2 OPTIONS \n\n### 1] UNDERSAMPLING\n- DECREASE THE MAJORITY DATA VALUES [NO FRAUDS]\n\n### 2] OVERSAMPLING\n- INCREASE THE MINORITY DATA VALUES [FRAUDS]","4f310ceb":"### 5] K-NEAREST NEIGHBORS:-","961b05f1":"# DATA INSIGHTS","424f80b6":"# PLEASE UPVOTE IF YOU LIKE IT!\n# STAY SAFE!","496165dd":"### SUMMARY OF FRAUD CASES","916bbe44":"### IMPORT THE NECESSARY LIBRARIES","d35b0327":"# DATA BALANCING\n\n### WE WILL USE THE COMBINATION OF UNDERSAMPLING & OVERSAMPLING\n- WE WILL USE UNDERSAMPLING FIRST TO TRIM DOWN MAJORITY DATA VALUES \n- THEN WE WILL USE OVERSAMPLING MINORITY DATA VALUES TO BALANCE THE DATA\n\n### IMBLEARN FOR OVERSAMPLING & UNDERSAMPLING\n- PIP STATEMENT FOR INSTALLATION:- pip install imbalanced-learn","3f6efd53":"### POINTS TO REMEMBER:- \n- SAMPLING STRATEGY:- DESIRED RATIO OF NUMBER OF SAMPLES IN MINORITY CLASS OVER NUMBER OF SAMPLES IN MAJORITY CLASS AFTER RESAMPLING.\n- UNDERSAMPLE:- DECREASES MAJORITY CLASS SAMPLES\n- OVERSAMPLE:- INCREASES MINORITY CLASS SAMPLES","a4cc8947":"# EVALUATION METRICS\n\n- FOR IMBALANCED DATASETS, WE CANNOT USE THE TRADITIONAL METRIC LIKE ACCURACY.\n- THIS IS BECAUSE THE DATA IS SKEWED TOWARDS THE MAJORITY CLASS AND THE MODEL WILL FAVOUR ITS PREDICTIONS TOWARDS MAJORITY CLASS.\n- NOW AFTER RESAMPLING, WE HAVE DUPLICATED THE DATA AND ADDED NEW DATA POINTS.THUS USING ACCURACY WOULD BE MISLEADING TO EVALUATE THE MODEL.\n- WE WILL USE THE CONFUSION MATRIX,ROC-AUC GRAPH AND ROC-AUC SCORE TO EVALUATE THE MODEL.\n- ROC-AUC GIVES US THE RELATION ABOUT TRUE POSITIVE & FALSE POSITIVE RATE.\n- WE WILL ALSO USE THE F1 SCORE,RECALL AND PRECISION.","0374098f":"- HEATMAP CANNOT BE USED IN THIS CASE BECAUSE OF THE NUMBER OF FEATURES\n- WE USE A DIFFERENT KIND OF VISUALIZATION FOR PLOTTING THE CORRELATION PLOT\n- A LOT OF FEATURES ARE NOT USEFUL FOR CLASSIFICATION FROM THE CORRELATION PLOT\n- WE WILL EXCLUDE THE FEATURES HAVING VALUES IN THE RANGE [-0.1,0.1]\n- 'V4','V11' ARE POSITIVELY CORRELATED & 'V7','V3','V16','V10','V12','V14','V17' ARE NEGATIVELY CORRELATED\n- WE WILL SELECT THE ABOVE FEATURES FOR CLASSIFICATION PURPOSES","46f400aa":"### SUMMARY OF NON-FRAUD CASES","f79979eb":"# CONCLUSION:-\n- WE CAN HANDLE THE IMBALANCED DATASETS USING THE RESAMPLING TECHNIQUES.\n- COMBINATION OF UNDERSAMPLING & OVERSAMPLING SHOULD BE PREFERRED. \n- WE CANNOT USE THE METRIC ACCURACY FOR IMBALANCED DATASETS.\n- PRECISION,RECALL,F1-SCORE HELP US IN DETERMINING THE MODEL PERFORMANCE.","223617a8":"### FRAUDS vs NO FRAUDS","2e4929fb":"### 3] DECISION TREE CLASSIFIER:- ","577c02de":"# CREDIT CARD FRAUD DETECTION","595a3245":"### DATA INFORMATION","973a3113":"# MODELS","203baaee":"### 2] SUPPORT VECTOR CLASSIFIER:-"}}