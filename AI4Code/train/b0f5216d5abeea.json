{"cell_type":{"25bca8c2":"code","868c3f6b":"code","d8c68f36":"code","c9e83bce":"code","b9dfce87":"code","9d7aebc5":"code","fb150dff":"code","bb1f5ab0":"code","43284148":"code","214b1d20":"code","63e5af5c":"code","9755d6bb":"code","b23467e6":"code","813ac5db":"code","f03596f9":"code","8177a5b1":"code","ec418805":"code","ba60e8bb":"code","cdddc73a":"code","180aa453":"code","c2585263":"code","a6bd77a8":"code","db310863":"code","2eba97b7":"code","b6e87f22":"code","ef89ca32":"code","ed105203":"code","8d8379da":"code","b3c6d7cb":"code","1f1eaaef":"code","fbd83a50":"code","5ff97bff":"code","da1c3eba":"code","ebd0e2ce":"code","54bee2d6":"code","03a217d3":"code","c5404051":"code","5f52bba8":"code","f0712b77":"code","5efd0a03":"code","7ec7097a":"code","0212235e":"code","93bd76ba":"code","39472bfe":"code","8116a2f0":"code","9319d9e1":"markdown","360cce6c":"markdown","8a259cd3":"markdown","518444f2":"markdown","8f31efbb":"markdown","02c77b07":"markdown","9f20f92c":"markdown","c67f17e1":"markdown","54ef2558":"markdown","4dfba66d":"markdown","43d64192":"markdown","d04d84f8":"markdown","f44b2841":"markdown","ba86b325":"markdown","81775284":"markdown","f710a840":"markdown","0c671ec0":"markdown","3df5a75a":"markdown","a602da9e":"markdown","8f5154b2":"markdown"},"source":{"25bca8c2":"import warnings\nwarnings.filterwarnings('ignore')\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import regularizers\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score,confusion_matrix\nfrom sklearn.manifold import TSNE\nfrom sklearn import preprocessing \nimport matplotlib.pyplot as plt\nimport pandas as pd \nimport numpy as np\nimport seaborn as sns\nfrom sklearn.metrics import roc_curve\nfrom matplotlib import pyplot\nfrom sklearn.metrics import precision_recall_curve,roc_auc_score,fbeta_score,recall_score\nfrom sklearn.metrics import plot_precision_recall_curve,average_precision_score,f1_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn import metrics\nfrom sklearn.svm import SVC\n\nsns.set(style=\"whitegrid\")\nnp.random.seed(203)\n","868c3f6b":"raw_features=pd.read_csv(\"https:\/\/media.githubusercontent.com\/media\/GuyenSoto\/BTC\/master\/elliptic_txs_features.csv\")\nraw_classes=pd.read_csv(\"https:\/\/media.githubusercontent.com\/media\/GuyenSoto\/BTC\/master\/elliptic_txs_classes.csv\")\nraw_edgelist=pd.read_csv(\"https:\/\/media.githubusercontent.com\/media\/GuyenSoto\/BTC\/master\/elliptic_txs_edgelist.csv\")","d8c68f36":"raw_classes['class'].value_counts()","c9e83bce":"# renaming columns\nraw_features.columns = ['id', 'time'] + [f'trans_feat_{i}' for i in range(93)] + [f'agg_feat_{i}' for i in range(72)]\nraw_features.head()","b9dfce87":"raw_features['time'].value_counts().sort_index().plot();\nplt.title('Number of transactions in each time step');","9d7aebc5":"# merge with classes\nraw_features = pd.merge(raw_features, raw_classes, left_on='id', right_on='txId', how='left')","fb150dff":"plt.figure(figsize=(12, 8))\ngrouped = raw_features.groupby(['time', 'class'])['id'].count().reset_index().rename(columns={'id': 'count'})\nsns.lineplot(x='time', y='count', hue='class', data=grouped);\nplt.legend(loc=(1.0, 0.8));\nplt.title('Number of transactions in each time step by class');","bb1f5ab0":"raw_features.head()","43284148":"raw_features=raw_features.rename(columns={\"class\":\"Class\"})","214b1d20":"cleaned_df = raw_features.copy()\n# You don't want the `Time` column.\ncleaned_df.pop('time')\n# You don't want the `txId` column.\ncleaned_df.pop('txId')\n# You don't want the `id` column.\ncleaned_df.pop('id')","63e5af5c":"cleaned_df['Class'].replace({\"unknown\": \"-1\"}, inplace=True)\ncleaned_df['Class'].replace({\"1\": \"1\"}, inplace=True)\ncleaned_df['Class'].replace({\"2\": \"0\"}, inplace=True)\ncleaned_df['Class'] = pd.to_numeric(cleaned_df['Class'])","9755d6bb":"#Create array of unknown Class randomly with the same proportion 1\/10 between \"zeros\" and \"ones\"\n#df[\"new_column\"] = np.random.choice([1, 0], len(df), p=[0.7, 0.3])   #  0) First step\ndef rand_bin_array(K, N):\n    arr = np.zeros(N,int)\n\n    arr[:K]  = int( 1)\n    np.random.shuffle(arr)\n    return arr","b23467e6":"# Put togheter all Classes\nprueba_0=cleaned_df[cleaned_df['Class']==0] # 1) Split \"0\" Class array \nprueba_0","813ac5db":"prueba_1=cleaned_df[cleaned_df['Class']==1] # 1) Split \"1\" Class array \nprueba_1","f03596f9":"prueba=cleaned_df[cleaned_df['Class']==-1]  # 2) Split Class array \"-1\"\nprueba","8177a5b1":"prueba['Class']=rand_bin_array(15720,157204)  # 3) Change Class array -1  target with a relation 1\/10  Illicit-Licit","ec418805":"vertical = pd.concat([prueba, prueba_0,prueba_1], axis=0) #4) Put together all Classes","ba60e8bb":"vertical=vertical.sort_index()   # 5) Order again \nvertical","cdddc73a":"y=vertical['Class']","180aa453":"data= vertical.copy()","c2585263":"vc = data['Class'].value_counts().to_frame().reset_index()\nvc['percent'] = vc[\"Class\"].apply(lambda x : round(100*float(x) \/ len(data), 2))\nvc = vc.rename(columns = {\"index\" : \"Target\", \"Class\" : \"Count\"})\nvc","a6bd77a8":"# We consider Fraud Like \"1\" and Not Fraud like \"0\"\nnon_fraud = data[data['Class'] == 0].sample(1000)\nfraud = data[data['Class'] == 1]\n\ndf = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)\nX = df.drop(['Class'], axis = 1).values\nY = df[\"Class\"].values","db310863":"def tsne_plot(x1, y1, name=\"graph.png\"):\n    tsne = TSNE(n_components=2, random_state=0)\n    X_t = tsne.fit_transform(x1)\n\n    plt.figure(figsize=(12, 8))\n    plt.scatter(X_t[np.where(y1 == 1), 0], X_t[np.where(y1 == 1), 1], marker='o', color='r', linewidth=1, alpha=0.8, label='Fraud')\n    plt.scatter(X_t[np.where(y1 == 0), 0], X_t[np.where(y1 == 0), 1], marker='o', color='g', linewidth=1, alpha=0.8, label='Non Fraud')\n\n    plt.legend(loc='best');\n    plt.savefig(name);\n    plt.show();\n    \ntsne_plot(X, Y, \"original.png\")","2eba97b7":"## input layer \ninput_layer = Input(shape=(X.shape[1],))\n\n## encoding part\nencoded = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)\nencoded = Dense(50, activation='relu')(encoded)\n\n## decoding part\ndecoded = Dense(50, activation='tanh')(encoded)\ndecoded = Dense(100, activation='tanh')(decoded)\n\n## output layer\noutput_layer = Dense(X.shape[1], activation='relu')(decoded)","b6e87f22":"autoencoder = Model(input_layer, output_layer)\nautoencoder.compile(optimizer=\"adadelta\", loss=\"mse\")","ef89ca32":"x = data.drop([\"Class\"], axis=1)\ny = data[\"Class\"].values\n\nx_scale = preprocessing.MinMaxScaler().fit_transform(x.values)\nx_norm, x_fraud = x_scale[y == 0], x_scale[y == 1]","ed105203":"autoencoder.fit(x_norm[0:2000], x_norm[0:2000], \n                batch_size = 256, epochs = 10, \n                shuffle = True, validation_split = 0.20);","8d8379da":"hidden_representation = Sequential()\nhidden_representation.add(autoencoder.layers[0])\nhidden_representation.add(autoencoder.layers[1])\nhidden_representation.add(autoencoder.layers[2])","b3c6d7cb":"norm_hid_rep = hidden_representation.predict(x_norm[:4000])\nfraud_hid_rep = hidden_representation.predict(x_fraud)","1f1eaaef":"rep_x = np.append(norm_hid_rep, fraud_hid_rep, axis = 0)\ny_n = np.zeros(norm_hid_rep.shape[0])\ny_f = np.ones(fraud_hid_rep.shape[0])\nrep_y = np.append(y_n, y_f)\ntsne_plot(rep_x, rep_y, \"latent_representation.png\")","fbd83a50":"train_x, val_x, train_y, val_y = train_test_split(rep_x, rep_y, test_size=0.25)\nclf = LogisticRegression(solver=\"lbfgs\",max_iter=4000).fit(train_x, train_y)\npred_y = clf.predict(val_x)\n\nprint (\"\")\nprint (\"Classification Report: \")\nprint (classification_report(val_y, pred_y))\n\nprint (\"\")\nprint (\"Accuracy Score: \", accuracy_score(val_y, pred_y))","5ff97bff":"#Model Evaluation using Confusion Matrix\ncnf_matrix = metrics.confusion_matrix(val_y, pred_y)\ncnf_matrix","da1c3eba":"total1=sum(sum(cnf_matrix))\n#print('F1_score: {0:0.2f}'.format(F1[1]))","ebd0e2ce":"#####from confusion matrix calculate accuracy\naccuracy1=(cnf_matrix[0,0]+cnf_matrix[1,1])\/total1\nprint ('Accuracy : {0:0.2f}'.format(accuracy1))","54bee2d6":"sensitivity1 = cnf_matrix[0,0]\/(cnf_matrix[0,0]+cnf_matrix[0,1])\nprint('Sensitivity : {0:0.2f}'.format(sensitivity1 ))","03a217d3":"specificity1 = cnf_matrix[1,1]\/(cnf_matrix[1,0]+cnf_matrix[1,1])\nprint('Specificity : {0:0.2f}'.format(specificity1))","c5404051":"#Visualizing Confusion Matrix using Heatmap\n%matplotlib inline\n\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","5f52bba8":"# predict probabilities and create ROC Curve\nyhat = clf.predict_proba(val_x)\n# retrieve just the probabilities for the positive class\npos_probs = yhat[:, 1]\n# plot no skill roc curve\npyplot.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n# calculate roc curve for model\nfpr, tpr, _ = roc_curve(val_y, pos_probs)\n# plot model roc curve\npyplot.plot(fpr, tpr, marker='.', label='Logistic')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","f0712b77":"svc = SVC(random_state=42)\nsvc.fit(train_x, train_y)\nsvc=svc_disp = plot_roc_curve(svc, val_x, val_y)\nplt.show()","5efd0a03":"ROC_AUC=roc_auc_score(val_y, clf.predict_proba(val_x)[:, 1])\nprint('ROC_AUC: {0:0.2f}'.format(ROC_AUC))","7ec7097a":"F1=fbeta_score(val_y,yhat[:, 1].round(),beta=1)\nF2=fbeta_score(val_y,yhat[:, 1].round(),beta=2)\nF_0_5=fbeta_score(val_y,yhat[:, 1].round(),beta=0.5)\n\nprint('F1-score: {0:0.2f}'.format(F1),'F2-score:  {0:0.2f}'.format(F2), 'F_0.5 score:  {0:0.2f}'.format(F_0_5 ))","0212235e":"F1=f1_score(val_y,yhat[:, 1].round(), average=None)\nprint('F1_score: {0:0.2f}'.format(F1[1]))","93bd76ba":" AS=accuracy_score(val_y,yhat[:, 1].round())\n print('Accuracy Score: {0:0.2f}'.format(AS))","39472bfe":"average_precision = average_precision_score(val_y,yhat[:, 1].round())\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","8116a2f0":"disp = plot_precision_recall_curve(clf, val_x, val_y)\ndisp.ax_.set_title('2-class Precision-Recall curve: '\n                   'AP={0:0.2f}'.format(average_precision))","9319d9e1":"##FB-Score ","360cce6c":"## ROC AUC","8a259cd3":"Generate the hidden representations of two classes : non-fraud and fraud by predicting the raw inputs using the above model.","518444f2":"From the above graph we can observe that there are many non_fraud transactions which are very close to fraud transactions, thus are difficult to accurately classify from a model. \n\n## 3. AutoEncoders to the rescue \n\n<br>\n**What are Autoencoders?** - Autoencoders are a special type of neural network architectures in which the output is same as the input. Autoencoders are trained in an unsupervised manner in order to learn the exteremely low level repersentations of the input data. These low level features are then deformed back to project the actual data. An autoencoder is a regression task where the network is asked to predict its input (in other words, model the identity function). These networks has a tight bottleneck of a few neurons in the middle, forcing them to create effective representations that compress the input into a low-dimensional code that can be used by the decoder to reproduce the original input. \n\n**More about Autoencoders** - If you want to gain more understanding about autoencoders, you can refer to the following kernel : https:\/\/www.kaggle.com\/shivamb\/how-autoencoders-work-intro-and-usecases\n\n![](https:\/\/i.imgur.com\/Rrmaise.png)\n\nWe will create an autoencoder model in which we only show the model non-fraud cases. The model will try to learn the best representation of non-fraud cases. The same model will be used to generate the representations of fraud cases and we expect them to be different from non-fraud ones. \n\nCreate a network with one input layer and one output layer having identical dimentions ie. the shape of non-fraud cases. We will use keras package. ","8f31efbb":"## 5. Visualize the latent representations : Fraud Vs Non Fraud\n\nNow we will create a training dataset using the latent representations obtained and let's visualize the nature of fraud vs non-fraud cases. ","02c77b07":"Create the model architecture by compiling input layer and output layers. Also add the optimizer and loss function, I am using \"adadelta\" as the optimizer and \"mse\" as the loss function.","9f20f92c":"The dataset consists of 28 anonymized variables, 1 \"amount\" variable, 1 \"time\" variable and 1 target variable - Class. Let's look at the distribution of target. ","c67f17e1":"## 4. Obtain the Latent Representations \n\nNow, the model is trained. We are intereseted in obtaining **latent representation of the input** learned by the model. This can be accessed by the weights of the trained model. We will create another network containing sequential layers, and we will only add the trained weights till the third layer where latent representation exists. ","54ef2558":"Before training, let's perform min max scaling. ","4dfba66d":"##Can we see ROC an PRC have a good relation of Recall, Precision and Accuracy","43d64192":"## Let's use simple method to Calculate Accurracy,Sensitivity and Specificity\n  \n   I'm more interested in Specificity, because is the one related with Illegal transactions","d04d84f8":"##F1_score","f44b2841":"One of the biggest challenge of this problem is that the **target is imbalanced** as only ** %** cases are fraud transactions. But the advantage of the representation learning approach is that it is still able to handle such imbalance nature of the problems. We will look how.  For our use-case let's take only about 1000 rows of non-fraud transactions. \n\n## Consider only 1000 rows of non fraud cases","ba86b325":"Now, we can just train a simple linear classifier on the dataset. \n\n## 6. Simple Linear Classifier and Metrics\n","81775284":"## 2. Visualize Fraud and NonFraud Transactions \n\nLet's visualize the nature of fraud and non-fraud transactions using T-SNE. T-SNE (t-Distributed Stochastic Neighbor Embedding) is a dataset decomposition technique which reduced the dimentions of data and produces only top n components with maximum information.  \n\nEvery dot in the following represents a transaction. Non Fraud transactions are represented as Green while Fraud transactions are represented as Red. The two axis are the components extracted by tsne. ","f710a840":"## Average Precision Recall Score","0c671ec0":"<h1 align=\"center\">Semi Supervised Classification using AutoEncoders<\/h1>\n\n## Introduction\n\nBy definition, machine learning can be defined as a complex process of learning the best possible and most relevant patterns, relationships, or associations from a dataset which can be used to predict the outcomes on unseen data. Broadly, their exists three different machine learning processes: \n\n**1. Supervised Learning**\u00a0is a process of training a machine learning model on a labelled dataset ie. a dataset in which the target variable is known. In this technique, the model aims to find the relationships among the independent and dependent variable. Examples of supervised learning are classification, regression and forecasting.\u00a0\n\n**2. Unsupervised Learning** is a process of training a machine learning model on a dataset in which target variable is not known. In this technique, the model aims to find the most relevant patterns in the data or the segments of data. Examples of unsupervised learning are clustering, segmentations, dimensionality reduction etc.\u00a0\n\n**3. Semi-Supervised Learning** is combination of supervised and unsupervised learning processes in which the unlabelled data is used for training a model as well. In this approach, the properties of unspervised learning are used to learn the best possible representation of data and the properties of supervised learning are used to learn the relationships in the representations which are then used to make predictions. \n\n<p align=\"center\">In this kernel, I have explained how to perform classification task using semi supervised learning approach. This approach makes use of autoencoders to learn the representation of the data then a simple linear classifier is trained to classify the dataset into respective classes.  \n \n<br>\n<h1 align=\"center\">Fraud Detection using Semi Supervised Learning<\/h1>  \n\nI am using the dataset of [BTC](https:\/\/media.githubusercontent.com\/media\/GuyenSoto\/BTC\/master\/elliptic_txs_features.csv) by NoDerivatives 4.0 International (CC BY-NC-ND 4.0). (https:\/\/www.kaggle.com\/ellipticco\/elliptic-data-set.) A number of kagglers have shared different approaches such as dataset balancing, anomaly detection, boosting models, deep learning etc but this approach is different. \n\n### Contents \n\n1. Dataset Preparation  \n2. Visualize Fraud Vs Non Fraud Transactions  \n3. AutoEncoders : Latent Representation Extraction  \n4. Obtain the Latent Representations  \n5. Visualize Latent Representations : Fraud vs Non Fraud  \n6. Simple Linear Classifier  \n\n \n## 1. Dataset Preparation\n\nFirst, we will load all the required libraries and load the dataset using pandas dataframe. \n \n\n","3df5a75a":"## Accuracy Score","a602da9e":"## Precision- Recall Curve","8f5154b2":"The beauty of this approach is that we do not need too many samples of data for learning the good representations. We will use **only 2000 rows** of non fraud cases to train the autoencoder. Additionally, We do not need to run this model for a large number of epochs. \n\n**Explanation:** The choice of small samples from the original dataset is based on the intuition that one class characteristics (non fraud) will differ from that of the other (fraud). To distinguish these characteristics we need to show the autoencoders only one class of data. This is because the autoencoder will try to learn only one class and automaticlly distinuish the other class. "}}