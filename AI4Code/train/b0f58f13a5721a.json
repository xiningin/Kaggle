{"cell_type":{"1f6c9d15":"code","ee1432f7":"code","233140ff":"code","657a5b5e":"code","8e198b6e":"code","edece255":"code","3e7ca6d3":"code","4940f81b":"code","7646d9d0":"code","17d81935":"code","836b218b":"code","48201798":"code","52c62653":"code","2a9c2fc1":"code","560d86d7":"code","390fa18d":"code","cc3a079e":"code","f9b250de":"code","e11778b2":"code","a7a6c62e":"code","f69b5016":"code","93f8ccee":"code","696fb288":"code","668f2b84":"code","d2e82047":"code","21e5d1df":"code","706e5109":"code","35e7c4d4":"code","4631df03":"code","479bef21":"code","1f65f766":"code","b5f4c98f":"code","49a3ac5e":"code","f228a815":"code","08fca918":"code","b970cf35":"code","e82c4bb7":"code","ef46dcf7":"code","44db1552":"code","94b87d1a":"code","24516f31":"code","24e5485f":"code","4a06e147":"code","1a97448a":"code","46ca350e":"markdown","c4e4e3cf":"markdown","ba38e027":"markdown","2b7d863a":"markdown","09369e32":"markdown","f5948aff":"markdown","41d35913":"markdown","7236cdf8":"markdown","5a39eb14":"markdown","945cbe5c":"markdown","45e11a2a":"markdown","d3290a25":"markdown","188b94f0":"markdown","a5ec06b2":"markdown","4791c90d":"markdown","2312fbdf":"markdown","adc76a7f":"markdown","2cede3e4":"markdown","832a65a3":"markdown","a2559f07":"markdown","e3187315":"markdown","95e9ef29":"markdown","6883d88c":"markdown","4b9f2932":"markdown","2922c781":"markdown","cf812a7d":"markdown","e32c8fff":"markdown","73a60fa6":"markdown"},"source":{"1f6c9d15":"# General libraries\nimport cv2\nimport random\nimport pandas as pd\nimport numpy as np\nimport time\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom os import getenv\nimport itertools\n\n# Deep Learning using Keras\nfrom keras.layers import Conv2D, Dense, Dropout, Flatten\nfrom keras.models import Sequential, load_model\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# Visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.utils import print_summary\n%matplotlib inline\n\n# Sci-kit learn libraries\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Image Preprocessing\nfrom skimage.transform import resize\nfrom skimage.color import rgb2gray\n\n# Set random seeds so results are the same each time\nnp.random.seed(0)\nfrom tensorflow import set_random_seed\nset_random_seed(0)","ee1432f7":"# Define parameters and functions\n\n# Change this to point to your data directory (if the default relative path \n# below does not work, please provide complete path: \n# eg:'C:\/Users\/subsh\/Documents\/207\/final_project\/asl-alphabet\/asl_alphabet_train\/asl_alphabet_train\/)\nTRAIN_DIR = '..\/input\/asl-alphabet\/asl_alphabet_train\/asl_alphabet_train\/'\nFILE_PATTERN = TRAIN_DIR + '{letter}\/{letter}{index}.jpg'\n\n# Define train size - this is the number of training samples for each letter\nFULL_TRAIN_SIZE = 3000\nMINI_TRAIN_SIZE = 100\n\n# Define the classes (letters of the ASL alphabet) that are being analyzed.\nLETTERS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P',\n           'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n\n# Define a hash table (dict) to store labels\nLABEL_DICT = dict(zip(LETTERS, np.arange(1, 30)))\n\n# Image sizing\nIMAGE_SIZE = 50","233140ff":"# Some visualization helpers\n\ndef plot_confusion_matrix(cm, classes,\n                      normalize=False,\n                      title='Confusion matrix',\n                      cmap=plt.cm.Blues):\n    '''\n    Plot a confusion matrix heatmap using matplotlib. This code was obtained from\n    the scikit-learn documentation:\n\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    '''\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    return\n\n\ndef plot_confusion_matrix_with_default_options(y_pred, y_true, classes):\n    '''Plot a confusion matrix heatmap with a default size and default options.'''\n    cm = confusion_matrix(y_true, y_pred)\n    with sns.axes_style('ticks'):\n        plt.figure(figsize=(16, 16))\n        plot_confusion_matrix(cm, classes)\n        plt.show()\n    return","657a5b5e":"def get_data(indices):\n    '''Read data into two numpy arrays.'''\n    xdata = []\n    ydata = []\n\n    for letter in LETTERS:\n        for index in indices:\n            # Create label\n            ydata.append(LABEL_DICT[letter])\n\n            # Read original image\n            image_path = FILE_PATTERN.format(letter=letter, index=index)\n            img_file = cv2.imread(image_path)\n\n            if img_file is None:\n                continue\n            img_file = resize(img_file, (IMAGE_SIZE, IMAGE_SIZE, 3))\n\n            # Append to data array\n            xdata.append(np.asarray(img_file))\n\n    xdata = np.asarray(xdata)\n    ydata = np.asarray(ydata)\n    return xdata, ydata\n\n\n# Read in train and dev data\nINDEX_LIST = np.arange(1, 3001)\nTRAIN_INDICES, DEV_INDICES = train_test_split(\n    INDEX_LIST,\n    train_size=MINI_TRAIN_SIZE,\n    test_size=MINI_TRAIN_SIZE,\n    random_state=42)\nX_MINI_TRAIN_ORIG, Y_MINI_TRAIN = get_data(TRAIN_INDICES)\nX_DEV_ORIG, Y_DEV = get_data(DEV_INDICES)\n\nprint('Initial shape of training data:', X_MINI_TRAIN_ORIG.shape)\nprint('Initial shape of dev data:', X_DEV_ORIG.shape)\n\n# Reshape the images from 3d to 2d arrays. The 3d images will be used for color\n# based pre-processing and plotting while the 2d arrays will be used for other\n# analyses.\n\nNSAMPLES, NX, NY, NZ = X_MINI_TRAIN_ORIG.shape\nX_MINI_TRAIN = X_MINI_TRAIN_ORIG[:NSAMPLES, :, :, :].reshape(NSAMPLES, 7500)\nY_MINI_TRAIN = Y_MINI_TRAIN[:NSAMPLES]\n\nNSAMPLES_DEV, NX, NY, NZ = X_DEV_ORIG.shape\nX_DEV = X_DEV_ORIG[:NSAMPLES_DEV, :, :, :].reshape(NSAMPLES_DEV, 7500)\nY_DEV = Y_DEV[:NSAMPLES_DEV]\n\nprint('Current shape of training data:', X_MINI_TRAIN.shape)\nprint('Current shape of dev data:', X_DEV.shape)","8e198b6e":"def show_class_counts():\n    '''Look at counts of each label in the mini training data to\n    ensure the data is spread across the various labels.'''\n    df = pd.DataFrame(Y_MINI_TRAIN, columns=['labels'])\n    df['count'] = 1\n    pd.pivot_table(df, columns='labels', values='count',\n                   aggfunc=np.count_nonzero)\n\n    plt.subplots(figsize=(10, 5))\n    sns.countplot(df['labels'])\n    return\n\n\nshow_class_counts()","edece255":"def plot_examples(num_examples):\n    '''Define counter variable to iterate through unique Y label values.'''\n    plt.figure(figsize=(16, len(LETTERS) * 2))\n\n    plot_index = 0\n    for counter in np.unique(Y_MINI_TRAIN):\n        # Subset data for each letter\n        x2 = X_MINI_TRAIN_ORIG[np.where(Y_MINI_TRAIN == counter)[0]]\n        y2 = Y_MINI_TRAIN[np.where(Y_MINI_TRAIN == counter)[0]]\n\n        for _, (image, label) in enumerate(zip(x2[0:num_examples], y2[0:num_examples])):\n            plot_index += 1\n            plt.subplot(len(LETTERS), num_examples, plot_index)\n            plt.imshow(image, cmap=plt.cm.gray)\n            plt.title(LETTERS[label - 1])\n            plt.axis('off')\n\n    return\n\n\nplot_examples(10)","3e7ca6d3":"def plot_symbol(symbol, examples):\n    '''Function to plot a symbol a certain number of times (examples variable).'''\n    # Pulls the index for the symbol. This is also the label in our y data.\n    label = LETTERS.index(symbol)\n\n    # Filter data so it only has the symbol.\n    data = X_MINI_TRAIN_ORIG[np.where(Y_MINI_TRAIN == label)]\n\n    # Random sampling of filtered data with the number of examples specified.\n    img_arr = data[random.sample(range(0, data.shape[0]-1), examples)]\n\n    # Create subplots and titles.\n    fig, ax = plt.subplots(nrows=examples, ncols=4,\n                           sharex=True, sharey=True, figsize=(10, 20))\n    title = \"Subplots of \" + LETTERS[label]\n    fig.suptitle(title, fontsize=30, y=0.93)\n\n    # For each example, grab the Red, Green, Blue, and Complete representation of the data\n    # and show it in the subplot.\n    for i in range(examples):\n        data_all = img_arr[i, :, :, :]\n        data_red = img_arr[i, :, :, 0]\n        data_green = img_arr[i, :, :, 1]\n        data_blue = img_arr[i, :, :, 2]\n\n        ax[i][0].imshow(data_all)\n        ax[i][0].set_title(\"Original\")\n        ax[i][0].axis('off')\n\n        ax[i][1].imshow(data_red)\n        ax[i][1].set_title(\"Red\")\n        ax[i][1].axis('off')\n\n        ax[i][2].imshow(data_green)\n        ax[i][2].set_title(\"Green\")\n        ax[i][2].axis('off')\n\n        ax[i][3].imshow(data_blue)\n        ax[i][3].set_title(\"Blue\")\n        ax[i][3].axis('off')\n    return","4940f81b":"plot_symbol('B', 5)","7646d9d0":"# Break the data into red, green and blue mini training sets to use in our later algorithms.\n# For this, first we need to break down the third dimension and reshape to a 2d array\n\n# Reshape train data\nNSAMPLES, NX, NY, _ = X_MINI_TRAIN_ORIG.shape\nX_RED_MINI_TRAIN = X_MINI_TRAIN_ORIG[:NSAMPLES, :, :, 0].reshape(\n    NSAMPLES, 2500)\nX_GREEN_MINI_TRAIN = X_MINI_TRAIN_ORIG[:NSAMPLES, :, :, 1].reshape(\n    NSAMPLES, 2500)\nX_BLUE_MINI_TRAIN = X_MINI_TRAIN_ORIG[:NSAMPLES, :, :, 2].reshape(\n    NSAMPLES, 2500)\n\nprint(\"X_RED_MINI_TRAIN Shape:\", X_RED_MINI_TRAIN.shape)\nprint(\"X_GREEN_MINI_TRAIN Shape:\", X_GREEN_MINI_TRAIN.shape)\nprint(\"X_BLUE_MINI_TRAIN Shape:\", X_BLUE_MINI_TRAIN.shape)\n\n# Reshape dev data\nNSAMPLES_DEV, _, _, _ = X_DEV_ORIG.shape\nX_RED_DEV = X_DEV_ORIG[:NSAMPLES_DEV, :, :, 0].reshape(NSAMPLES_DEV, 2500)\nX_GREEN_DEV = X_DEV_ORIG[:NSAMPLES_DEV, :, :, 1].reshape(NSAMPLES_DEV, 2500)\nX_BLUE_DEV = X_DEV_ORIG[:NSAMPLES_DEV, :, :, 2].reshape(NSAMPLES_DEV, 2500)","17d81935":"# This function will take a data set as an input and perform a gaussian blur to\n# the data. It will then return that data set with a blur, meaning a pixel will\n# take on the average of its 8 closest neighbors (plus itself).\ndef blur(data):\n\n    # Initialize a blank data set that is the same size as the 'data' set\n    # parameter passed to our function We then iterate through the rows in our\n    # data set, transforming them into a 50X50 matrix and performing the\n    # gaussian blur on them\n    blurred_data_set = np.zeros((data.shape))\n    for i in range(data.shape[0]):\n        blur_matrix = np.zeros((50, 50))\n        test_matrix = np.reshape(data[i], (50, 50))\n\n        # for each 50X50 matrix we have made, we iterate through each pixel\n        # (excluding the edges) and take the average of that respective pixel\n        # and its 8 closest neighbors and make that the pixel's new value. After\n        # the loop we then return that matrix back to its original shape and add\n        # it to a blurred_data_set that will end up being the transformed\n        # version of the original 'data' parameter. This blurred_data_set will\n        # then be returned.\n        for j in range(1, 49):\n            for k in range(1, 49):\n                blur_matrix[j][k] = (test_matrix[j-1][k-1] + test_matrix[j-1][k]\n                                     + test_matrix[j][k] + test_matrix[j-1][k+1] + test_matrix[j][k-1]\n                                     + test_matrix[j][k+1] + test_matrix[j+1][k-1] + test_matrix[j+1][k]\n                                     + test_matrix[j+1][k+1]) \/ 9\n        blur_matrix = np.reshape(blur_matrix, (2500))\n        blurred_data_set[i] = blur_matrix\n    return blurred_data_set\n\n\n# Blur our mini train data and dev data for use in our models\nBLUR_RED_MINI_TRAIN = blur(X_RED_MINI_TRAIN)\nBLUR_RED_DEV = blur(X_RED_DEV)\n\nBLUR_GREEN_MINI_TRAIN = blur(X_GREEN_MINI_TRAIN)\nBLUR_GREEN_DEV = blur(X_GREEN_DEV)\n\nBLUR_BLUE_MINI_TRAIN = blur(X_BLUE_MINI_TRAIN)\nBLUR_BLUE_DEV = blur(X_BLUE_DEV)","836b218b":"# Take the blurred red, green and blue data pixels and stack them back together\n# into an array of the original size and shape of the mini training data. This\n# leads to a blurring of the original image.\nX_BLUR = np.stack(\n    [BLUR_RED_MINI_TRAIN, BLUR_GREEN_MINI_TRAIN, BLUR_BLUE_MINI_TRAIN], axis=2)\nX_BLUR_DEV = np.stack([BLUR_RED_DEV, BLUR_GREEN_DEV, BLUR_BLUE_DEV], axis=2)\n\nX_BLUR_MINI_TRAIN = X_BLUR.reshape(NSAMPLES, 7500)\nX_BLUR_DEV = X_BLUR_DEV.reshape(NSAMPLES_DEV, 7500)","48201798":"def auto_canny(image, sigma=0.33):\n    '''Define automatic canny detection\n\n    See: https:\/\/www.pyimagesearch.com\/2015\/04\/06\/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv\/\n    '''\n    # compute the median of the single channel pixel intensities\n    v = np.median(image)\n\n    # apply automatic Canny edge detection using the computed median\n    lower = int(max(0, (1.0 - sigma) * v))\n    upper = int(min(255, (1.0 + sigma) * v))\n    edged = cv2.Canny(image, lower, upper)\n\n    # return the edged image\n    return edged","52c62653":"def get_data2(indices):\n    nrows = 1\n    ncols = 8\n\n    xdata = []\n    ydata = []\n\n    for letter in LETTERS:\n        for index in indices:\n            # Create label\n            ydata.append(LABEL_DICT[letter])\n\n            # Read original image\n            image_path = FILE_PATTERN.format(letter=letter, index=index)\n            image = cv2.imread(image_path)\n\n            # Process image\n\n            # Apply Guassian blur\n            blurred = cv2.GaussianBlur(image, (5, 5), 0)\n\n            # Convert to HSV\n            imgray = cv2.cvtColor(blurred, cv2.COLOR_RGB2HSV)\n\n            # Apply median blur - need to pick one of Guassian or median\n            imgray2 = cv2.medianBlur(imgray, 5)\n\n            # Resize\n#             resized = resize(imgray2, (IMAGE_SIZE, IMAGE_SIZE, 3))\n            resized = imgray2\n\n            # Convert to gray - THIS IS NOT WORKING WITH CANNY\n            resized2 = rgb2gray(resized)\n\n            # Get the edges - tight\n            # Try changing this for better edge detection\n            edge_t = cv2.Canny(resized, 100, 250)\n\n            # Get the edges - auto\n            edge_a = auto_canny(resized)\n\n            # Convert to gray - THIS IS NOT WORKING WITH CANNY\n            resized2 = rgb2gray(edge_t)\n            # Binarize to be done\n#             https:\/\/sourcedexter.com\/manipulating-image-pixels-with-python-scikit-image-color-schemes\/\n\n            # Pick which image to use for training\n            xdata.append(np.asarray(resized2))\n\n        # THIS DOES NOT SEEM TO BE WORKING  - FIX SIZE\n        plt.figure(figsize=(16, 5))\n\n        plots = {\n            'Original {}'.format(letter): image,\n            'Gaussian Blur': blurred,\n            'HSV': imgray,\n            'Median blur': imgray2,\n            'Resized': resized,\n            'Tight edged': edge_t,\n            'Auto edged': edge_a,\n            'Edged grayscale': edge_a,\n        }\n\n        plot_index = 0\n        for name, image in plots.items():\n            plot_index += 1\n            plt.subplot(nrows, ncols, plot_index)\n            plt.imshow(image)\n            plt.title(\"Original \" + letter)\n            plt.axis('off')\n\n        plt.show()\n\n    return xdata, ydata\n\n\nX_IMAGE_MINI_TRAIN_ORIG, Y_TRAIN = get_data2(TRAIN_INDICES)\nX_IMAGE_DEV_ORIG, Y_DEV = get_data2(DEV_INDICES)\n\nX_IMAGE_MINI_TRAIN_ORIG = np.asarray(X_IMAGE_MINI_TRAIN_ORIG)\nX_IMAGE_DEV_ORIG = np.asarray(X_IMAGE_DEV_ORIG)\n\n# Reshape train data\nNSAMPLES, NX, NY = X_IMAGE_MINI_TRAIN_ORIG.shape\nX_IMAGE_MINI_TRAIN = X_IMAGE_MINI_TRAIN_ORIG[:NSAMPLES, :, :].reshape(NSAMPLES, NX * NY)\n\n# Reshape dev data\nNSAMPLES_DEV, NX, NY = X_IMAGE_DEV_ORIG.shape\nX_IMAGE_DEV = X_IMAGE_DEV_ORIG[:NSAMPLES_DEV, :, :].reshape(NSAMPLES_DEV, NX * NY)","2a9c2fc1":"# Set the number of components to be used below.\nN_COMPONENTS = 110\n\ndef reduce_dimensionality(model, data):\n    '''Reduce dimensionality with PCA.'''\n    n_components = 110\n    model.fit(data)\n\n    # # look at cumulative explained variance in increments of 10 components\n    i = [1] + list(range(10, n_components + 1, 10))\n    df = pd.DataFrame(\n        index=i, columns=[\"num_components\", \"cum_pct_variance\"])\n    for k in i:\n        df.loc[k] = [k, sum(model.explained_variance_ratio_[:k])]\n\n    return df\n\n\nPCA_MODEL = PCA(n_components=N_COMPONENTS, random_state=0)\nDF_PCA = reduce_dimensionality(PCA_MODEL, X_MINI_TRAIN)","560d86d7":"def plot_pca(df):\n    '''Plot number of principal components vs cumulative explained variance.'''\n    plt.plot(df['num_components'], df['cum_pct_variance'], marker='.')\n    plt.xlabel('Number of Principal Components')\n    plt.ylabel('Cumulative Variance Percentage')\n    plt.show()\n    return\n\n\nplot_pca(DF_PCA)","390fa18d":"# Transform mini training data and development data using PCA fitted with mini\n# training data.\nX_MINI_TRAIN_PCA = PCA_MODEL.transform(X_MINI_TRAIN)\nX_DEV_PCA = PCA_MODEL.transform(X_DEV)\n\n\ndef pca_transform(model, data, data_pca):\n    model.fit(data)\n    return model.transform(data_pca)\n\n\nX_BLUR_MINI_TRAIN_PCA = pca_transform(PCA_MODEL, X_BLUR_MINI_TRAIN, X_BLUR_MINI_TRAIN)\nX_BLUR_DEV_PCA = pca_transform(PCA_MODEL, X_BLUR_MINI_TRAIN, X_BLUR_DEV)\n\nX_RED_MINI_TRAIN_PCA = pca_transform(PCA_MODEL, X_RED_MINI_TRAIN, X_RED_MINI_TRAIN)\nX_RED_DEV_PCA = pca_transform(PCA_MODEL, X_RED_MINI_TRAIN, X_RED_DEV)\nBLUR_RED_MINI_TRAIN_PCA = pca_transform(PCA_MODEL, BLUR_RED_MINI_TRAIN, BLUR_RED_MINI_TRAIN)\nBLUR_RED_DEV_PCA = pca_transform(PCA_MODEL, BLUR_RED_MINI_TRAIN, BLUR_RED_DEV)\n\nX_BLUE_MINI_TRAIN_PCA = pca_transform(PCA_MODEL, X_BLUE_MINI_TRAIN, X_BLUE_MINI_TRAIN)\nX_BLUE_DEV_PCA = pca_transform(PCA_MODEL, X_BLUE_MINI_TRAIN, X_BLUE_DEV)\nBLUR_BLUE_MINI_TRAIN_PCA = pca_transform(PCA_MODEL, BLUR_BLUE_MINI_TRAIN, BLUR_BLUE_MINI_TRAIN)\nBLUR_BLUE_DEV_PCA = pca_transform(PCA_MODEL, BLUR_BLUE_MINI_TRAIN, BLUR_BLUE_DEV)\n\nX_GREEN_MINI_TRAIN_PCA = pca_transform(PCA_MODEL, X_GREEN_MINI_TRAIN, X_GREEN_MINI_TRAIN)\nX_GREEN_DEV_PCA = pca_transform(PCA_MODEL, X_GREEN_MINI_TRAIN, X_GREEN_DEV)\nBLUR_GREEN_MINI_TRAIN_PCA = pca_transform(PCA_MODEL, BLUR_GREEN_MINI_TRAIN, BLUR_GREEN_MINI_TRAIN)\nBLUR_GREEN_DEV_PCA = pca_transform(PCA_MODEL, BLUR_GREEN_MINI_TRAIN, BLUR_GREEN_DEV)","cc3a079e":"def fit_and_tune(traindata, devdata, clf, params_list, clf_name, message, print_cr, print_cm, **kwargs):\n    '''For our baseline KNN classifier, we look for optimal k using gridsearch,\n    both with and wihtout PCA. We willram use these best parameters to fit train\n    data as well as evaluate accuracy on dev data.'''\n    gs = GridSearchCV(estimator=clf, param_grid=params_list, refit=True)\n    gs.fit(traindata, Y_MINI_TRAIN)\n    print('Tuning parameters for {} for classifier {}'.format(message, clf_name))\n    print('Best paramaters:', gs.best_params_)\n    print('Best grid score: {:.3f}'.format(gs.best_score_))\n\n    # Fit and score with the best parameter found\n#     pred = gs.fit_transform(traindata)\n    clf.set_params(**gs.best_params_)\n    clf.fit(traindata, Y_MINI_TRAIN)\n\n    dev_pred = gs.predict(devdata)\n    print(\"Dev accuracy: \", 100*np.sum(dev_pred == Y_DEV)\/devdata.shape[0])\n\n    if print_cr == 1:\n        print(\"Classification Report\\n\")\n        print(classification_report(Y_DEV, dev_pred))\n\n    if print_cm == 1:\n        plot_confusion_matrix_with_default_options(\n            y_pred=dev_pred,\n            y_true=Y_DEV,\n            classes=LETTERS)\n\n    return\n\n\n# Define gridsearch parameters\ndef fit_and_tune_knn():\n    params_list = {'n_neighbors': [1, 5, 10, 15]}\n    clf = KNeighborsClassifier()\n    clf_name = 'KNN'\n\n    # Call function once on data without PCA and one without:\n    fit_and_tune(X_MINI_TRAIN, X_DEV, clf, params_list,\n                 clf_name, 'without PCA', 0, 0)\n    fit_and_tune(X_MINI_TRAIN_PCA, X_DEV_PCA, clf,\n                 params_list, clf_name, 'with PCA', 0, 0)\n    return\n\n\nfit_and_tune_knn()","f9b250de":"# Place data into lists to make it iterable for use in loops when running algorithms.\nTRAINING_SETS = [X_MINI_TRAIN, X_MINI_TRAIN_PCA, X_BLUR_MINI_TRAIN_PCA, X_RED_MINI_TRAIN_PCA, \n                 BLUR_RED_MINI_TRAIN_PCA, X_BLUE_MINI_TRAIN_PCA, BLUR_BLUE_MINI_TRAIN_PCA,\n                 X_GREEN_MINI_TRAIN_PCA, BLUR_GREEN_MINI_TRAIN_PCA, X_IMAGE_MINI_TRAIN]\nDEVELOPMENT_SETS = [X_DEV, X_DEV_PCA, X_BLUR_DEV_PCA, X_RED_DEV_PCA, BLUR_RED_DEV_PCA, X_BLUE_DEV_PCA,\n                    BLUR_BLUE_DEV_PCA, X_GREEN_DEV_PCA, BLUR_GREEN_DEV_PCA, X_IMAGE_DEV]\nSET_DESCRIPTIONS = ['No_PCA', 'PCA', 'Blur_PCA', 'Red_PCA', 'Blur_Red_PCA', 'Blue_PCA', 'Blur_Blue_PCA',\n                    'Green_PCA', 'Blur_Green_PCA', 'Image_Contour']","e11778b2":"KNN_MODEL = KNeighborsClassifier(n_neighbors=1)\nprint(\"K Nearest Neighbors Classifier:\")\n\n\ndef run_knn(model, training, development, description):\n    scores = []\n    for train, dev in zip(training, development):\n        model.fit(train, Y_MINI_TRAIN)\n        scores.append(\"{:.1%}\".format(model.score(dev, Y_DEV)))\n\n    df_knn = pd.DataFrame(columns=['Data Description', 'Accuracy'])\n    i = 0\n    for desc, score in zip(description, scores):\n        df_knn.loc[i] = [desc, score]\n        i += 1\n    return df_knn\n\n\nrun_knn(KNN_MODEL, TRAINING_SETS, DEVELOPMENT_SETS, SET_DESCRIPTIONS)","a7a6c62e":"# Classification report and error analysis on blurred data\n\nKNN_MODEL.fit(X_BLUR_MINI_TRAIN_PCA, Y_MINI_TRAIN)\nDEV_PREDICTED_LABELS = KNN_MODEL.predict(X_BLUR_DEV_PCA)\nprint(classification_report(Y_DEV, DEV_PREDICTED_LABELS))","f69b5016":"def run_bagged_trees():\n    # base bagged trees classifier with dimensionality reduction\n    bg = BaggingClassifier(random_state=0)\n    bg.fit(X_MINI_TRAIN_PCA, Y_MINI_TRAIN)\n    print(\"Bagged Trees Classifier:\\n\")\n    print(\"Accuracy with PCA dimensionality reduction:\", \"{:.1%}\".format(bg.score(X_DEV_PCA, Y_DEV)))\n\n    # base bagged trees classifier without dimensionality reduction\n    bg = BaggingClassifier(random_state=0)\n    bg.fit(X_MINI_TRAIN, Y_MINI_TRAIN)\n    print(\"Accuracy without PCA dimensionality reduction:\", \"{:.1%}\".format(bg.score(X_DEV, Y_DEV)))\n    return\n    \n\nrun_bagged_trees()","93f8ccee":"# Bagged trees tuning by varying size of underlying decision tree (leaf_nodes)\n# and number of estimators for bagging.\ndef make_decision_tree():\n    leaf_nodes = [100, 500, 800]\n    estimators = [10, 20, 30, 50]\n    num_leaf_nodes = len(leaf_nodes)\n    num_estimators = len(estimators)\n\n    df1 = pd.DataFrame(index=range(num_leaf_nodes*num_estimators),\n                       columns=[\"leaf_nodes\", \"estimators\", \"accuracy\"])\n    for i in range(num_leaf_nodes):\n        dt = DecisionTreeClassifier(max_leaf_nodes=leaf_nodes[i])\n        for j in range(num_estimators):\n            bg = BaggingClassifier(\n                base_estimator=dt, n_estimators=estimators[j], random_state=0)\n            bg.fit(X_MINI_TRAIN, Y_MINI_TRAIN)\n            accuracy = bg.score(X_DEV, Y_DEV)\n            df1.loc[(i*num_estimators)+j] = [leaf_nodes[i],\n                                             estimators[j], accuracy]\n\n    return df1\n\n\nBAGGING_DF = make_decision_tree()","696fb288":"def plot_bagging(df):\n    '''Plot number of estimators vs accuracy with different number of leaf nodes\n    for the underlying decision tree note that the lines for 500 leaf nodes and\n    800 leaf nodes completely overlap each other.'''\n    plt.plot(df.loc[0:3][\"estimators\"], df.loc[0:3][\"accuracy\"], color='steelblue',\n             marker='.', linestyle='dotted', label=\"Bagging w\/100 Leaf Nodes\")\n    plt.plot(df.loc[4:7][\"estimators\"], df.loc[4:7][\"accuracy\"], color='green',\n             marker='.', linestyle='dotted', label=\"Bagging w\/500 Leaf Nodes\")\n    plt.plot(df.loc[8:11][\"estimators\"], df.loc[8:11][\"accuracy\"], color='red',\n             marker='.', linestyle='dotted', label=\"Bagging w\/800 Leaf Nodes\")\n    plt.xlabel(\"Number of Estimators\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.show()\n    return\n\n\nplot_bagging(BAGGING_DF)","668f2b84":"def run_bagged_trees2():\n    '''As seen above varying size of leaf_nodes does not seem to impact the accruracy much\n    in contrast number of estimators for bagging seems to have a direct impact on accuracy\n    here we try a few more parameters for number of estimators.'''\n    estimators = [50, 70, 80, 100]\n    num_estimators = len(estimators)\n\n    df = pd.DataFrame(index=range(num_estimators),\n                       columns=[\"estimators\", \"accuracy\"])\n    for j in range(num_estimators):\n        bg = BaggingClassifier(n_estimators=estimators[j], random_state=0)\n        bg.fit(X_MINI_TRAIN, Y_MINI_TRAIN)\n        accuracy = bg.score(X_DEV, Y_DEV)\n        df.loc[j] = [estimators[j], accuracy]\n\n    return df\n\n\nBAGGING_DF2 = run_bagged_trees2()","d2e82047":"def plot_bagging2(df):\n    # plot number of estimators vs accuracy without no limites on size of underlying decision tree\n    plt.plot(df[\"estimators\"], df[\"accuracy\"], color='steelblue', marker='.', linestyle='dotted', markersize=8)\n    plt.xlabel(\"Number of Estimators\")\n    plt.ylabel(\"Accuracy\")\n    plt.show()\n    return\n    \n    \nplot_bagging2(BAGGING_DF2)","21e5d1df":"def bagging_cm():\n    # output confusion matrix for bagged tree classifier with n_estimators = 50\n    bg = BaggingClassifier(n_estimators=50, random_state=0)\n    bg.fit(X_MINI_TRAIN, Y_MINI_TRAIN)\n    predicted_labels = bg.predict(X_DEV)\n    cm = confusion_matrix(Y_DEV, predicted_labels)\n    plot_confusion_matrix_with_default_options(\n        y_pred=predicted_labels,\n        y_true=Y_DEV,\n        classes=LETTERS)\n\n    # Print total number of incorrect predictions for each digit.\n    print(\"Number of incorrect predictions for each hand-sign:\",\n          cm.sum(axis=1) - cm.diagonal(), \"\\n\")\n\n    # Print maximum number of incorrect predictions for each digit.\n    print(\"Maximum number of incorrect predictions for each hand-sign:\",\n          cm[np.where(cm != cm.diagonal())].reshape(29, 28).max(axis=1), \"\\n\")\n    \n    print(classification_report(Y_DEV, predicted_labels))\n    return\n\n\nbagging_cm()","706e5109":"SVC_MODEL = SVC(random_state=0)\nprint(\"Support Vector Machine:\")\n\n\ndef run_svc(training, development, description):\n    scores = []\n    for train, dev in zip(training, development):\n        SVC_MODEL.fit(train, Y_MINI_TRAIN)\n        scores.append(\"{:.1%}\".format(SVC_MODEL.score(dev, Y_DEV)))\n\n    df_svc = pd.DataFrame(columns=['Data Description', 'Accuracy'])\n    i = 0\n    for desc, score in zip(description, scores):\n        df_svc.loc[i] = [desc, score]\n        i += 1\n    return df_svc\n\n\nrun_svc(TRAINING_SETS, DEVELOPMENT_SETS, SET_DESCRIPTIONS)","35e7c4d4":"def fit_and_tune_svc():\n    c = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n    gamma = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n    params_list = {'C': c, 'gamma': gamma}\n    clf = SVC(kernel='rbf')\n    clf_name = \"SVM\"\n\n    return fit_and_tune(\n        X_MINI_TRAIN_PCA,\n        X_DEV_PCA,\n        clf,\n        params_list,\n        clf_name,\n        \"with PCA\",\n        1,\n        1)\n\n\nfit_and_tune_svc()","4631df03":"CUSTOM_TEST_DIR = '..\/input\/asl-alphabet-test\/asl-alphabet-test'\n\nTARGET_SIZE = (64, 64)\nTARGET_DIMS = (64, 64, 3)  # add channel for RGB\nN_LETTERS = len(LETTERS)\nVALIDATION_SPLIT = 0.1\nBATCH_SIZE = 64\n\n# Model saving for easier local iterations\nMODEL_DIR = '.'\nMODEL_PATH = MODEL_DIR + '\/cnn-model.h5'\nMODEL_WEIGHTS_PATH = MODEL_DIR + '\/cnn-model.weights.h5'\nMODEL_SAVE_TO_DISK = getenv('KAGGLE_WORKING_DIR') != '\/kaggle\/working'\n\nprint('Save model to disk? {}'.format('Yes' if MODEL_SAVE_TO_DISK else 'No'))","479bef21":"def preprocess_image(image):\n    '''Function that will be implied on each input. The function\n    will run after the image is resized and augmented.\n    The function should take one argument: one image (Numpy tensor\n    with rank 3), and should output a Numpy tensor with the same\n    shape. This preprocessor tries to simplify the image by finding\n    its edges.'''\n    sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\n    return sobely\n\ndef make_generator(options):\n    '''Creates two generators for dividing and preprocessing data.'''\n    validation_split = options.get('validation_split', 0.0)\n    preprocessor = options.get('preprocessor', None)\n    data_dir = options.get('data_dir', TRAIN_DIR)\n\n    augmentor_options = {\n        'samplewise_center': True,\n        'samplewise_std_normalization': True,\n    }\n    if validation_split is not None:\n        augmentor_options['validation_split'] = validation_split\n\n    if preprocessor is not None:\n        augmentor_options['preprocessing_function'] = preprocessor\n\n    flow_options = {\n        'target_size': TARGET_SIZE,\n        'batch_size': BATCH_SIZE,\n        'shuffle': options.get('shuffle', None),\n        'subset': options.get('subset', None),\n    }\n\n    data_augmentor = ImageDataGenerator(**augmentor_options)\n    return data_augmentor.flow_from_directory(data_dir, **flow_options)","1f65f766":"def load_model_from_disk():\n    '''A convenience method for re-running certain parts of the\n    analysis locally without refitting all the data.'''\n    model_file = Path(MODEL_PATH)\n    model_weights_file = Path(MODEL_WEIGHTS_PATH)\n\n    if model_file.is_file() and model_weights_file.is_file():\n        print('Retrieving model from disk...')\n        model = load_model(model_file.__str__())\n\n        print('Loading CNN model weights from disk...')\n        model.load_weights(model_weights_file)\n        return model\n\n    return None\n\n\nCNN_MODEL = load_model_from_disk()\nREPROCESS_MODEL = (CNN_MODEL is None)\n\nprint('Need to reprocess? {}'.format(REPROCESS_MODEL))","b5f4c98f":"def build_model(save=False):\n    print('Building model afresh...')\n\n    model = Sequential()\n    model.add(Conv2D(64, kernel_size=5, strides=1,\n                     activation='relu', input_shape=TARGET_DIMS))\n    model.add(Conv2D(64, kernel_size=5, strides=2, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Conv2D(128, kernel_size=4, strides=1, activation='relu'))\n    model.add(Conv2D(128, kernel_size=4, strides=2, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Conv2D(256, kernel_size=4, strides=1, activation='relu'))\n    model.add(Conv2D(256, kernel_size=4, strides=2, activation='relu'))\n    model.add(Flatten())\n    model.add(Dropout(0.5))\n    model.add(Dense(512, activation='relu'))\n    model.add(Dense(N_LETTERS, activation='softmax'))\n\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy', metrics=['accuracy'])\n    if save:\n        model.save(MODEL_PATH)\n\n    return model\n\n\nif REPROCESS_MODEL:\n    CNN_MODEL = build_model(save=MODEL_SAVE_TO_DISK)\n\nprint_summary(CNN_MODEL)","49a3ac5e":"def make_generator_for(subset):\n    '''Create a generator for the training or validation set.'''\n    generator_options = dict(\n        validation_split=VALIDATION_SPLIT,\n        shuffle=True,\n        subset=subset,\n        preprocessor=preprocess_image,\n    )\n    return make_generator(generator_options)\n\n\ndef fit_model(model, train_generator, val_generator, save=False):\n    '''Fit the model with the training and validation generators.'''\n    history = model.fit_generator(\n        train_generator, epochs=5, validation_data=val_generator)\n\n    if save:\n        model.save_weights(MODEL_WEIGHTS_PATH)\n\n    return history\n\n\nCNN_TRAIN_GENERATOR = make_generator_for('training')\nCNN_VAL_GENERATOR = make_generator_for('validation')\n\ndef reprocess_model():\n    start_time = time.time()\n    history = fit_model(CNN_MODEL, CNN_TRAIN_GENERATOR,\n                        CNN_VAL_GENERATOR, save=MODEL_SAVE_TO_DISK)\n    print('Fitting the model took ~{:.0f} second(s).'.format(\n        time.time() - start_time))\n    return history\n\n\ndef show_cnn_model_weights():\n    columns = ['Dimension 1', 'Dimension 2', 'Dimension 3', 'Dimension 4']\n    return pd.DataFrame(data=[x.shape for x in CNN_MODEL.weights], columns=columns)\n\nHISTORY = None\nif REPROCESS_MODEL:\n    HISTORY = reprocess_model()\nshow_cnn_model_weights()","f228a815":"# HISTORY is only available if the model was run this time around,\n# which won't be the case when the model is read in from a file, so\n# we just skip it in that case.\nif HISTORY:\n    print('Final Accuracy: {:.2f}%'.format(HISTORY.history['acc'][4] * 100))\n    print('Validation set accuracy: {:.2f}%'.format(\n        HISTORY.history['val_acc'][4] * 100))","08fca918":"%%HTML\n<div align=\"middle\">\n    <video width=\"80%\" controls>\n        <source src=\"https:\/\/s3-us-west-2.amazonaws.com\/danrasband-w207\/A.mp4\" type=\"video\/mp4\">\n    <\/video>\n<\/div>","b970cf35":"def evaluate_model(generator):\n    start_time = time.time()\n    evaluations = CNN_MODEL.evaluate_generator(generator)\n    for i in range(len(CNN_MODEL.metrics_names)):\n        print(\"{}: {:.2f}%\".format(\n            CNN_MODEL.metrics_names[i], evaluations[i] * 100))\n    print('Took {:.0f} seconds to evaluate this set.'.format(\n        time.time() - start_time))\n\n    start_time = time.time()\n    predictions = CNN_MODEL.predict_generator(generator)\n    print('Took {:.0f} seconds to get predictions on this set.'.format(\n        time.time() - start_time))\n\n    y_pred = np.argmax(predictions, axis=1)\n    y_true = generator.classes\n    return dict(y_pred=y_pred, y_true=y_true)\n\n\ndef evaluate_validation_dataset():\n    gen_options = dict(\n        validation_split=0.1,\n        data_dir=TRAIN_DIR,\n        shuffle=False,\n        subset='validation',\n        preprocessor=preprocess_image,\n    )\n    val_gen = make_generator(gen_options)\n    return evaluate_model(val_gen)\n\n\ndef evaluate_test_dataset():\n    gen_options = dict(\n        validation_split=0.0,\n        data_dir=CUSTOM_TEST_DIR,\n        shuffle=False,\n        preprocessor=preprocess_image,\n    )\n    test_gen = make_generator(gen_options)\n    return evaluate_model(test_gen)","e82c4bb7":"CNN_VALIDATION_SET_EVAL = evaluate_validation_dataset()","ef46dcf7":"print(classification_report(**CNN_VALIDATION_SET_EVAL, target_names=LETTERS))","44db1552":"with sns.axes_style('ticks'):\n    plot_confusion_matrix_with_default_options(\n        **CNN_VALIDATION_SET_EVAL, classes=LETTERS)","94b87d1a":"CNN_TEST_SET_EVAL = evaluate_test_dataset()","24516f31":"print(classification_report(**CNN_TEST_SET_EVAL, target_names=LETTERS))","24e5485f":"with sns.axes_style('ticks'):\n    plot_confusion_matrix_with_default_options(\n        **CNN_TEST_SET_EVAL, classes=LETTERS)","4a06e147":"def fit_and_tune_random_forest():\n    clf = RandomForestClassifier()\n    clf_name = \"Random_Forest\"\n    params_list = {\n        'n_estimators': [10, 20, 50, 100, 200, 700],\n        'max_features': ['auto', 'sqrt', 'log2']\n    }\n\n    return fit_and_tune(\n        X_MINI_TRAIN_PCA,\n        X_DEV_PCA,\n        clf,\n        params_list,\n        clf_name,\n        \"with PCA\",\n        0,\n        0)\n\n\nfit_and_tune_random_forest()","1a97448a":"def run_naive_bayes(train, dev):\n    clf = BernoulliNB()\n    params_list = {'alpha': [0.0001, 0.001, 0.01, 1, 10, 100]}\n    clf_name = \"Bernoulli NB\"\n    fit_and_tune(train, dev, clf, params_list, clf_name, \"Contoured data\", 0, 0)\n    return\n\n\nrun_naive_bayes(X_IMAGE_MINI_TRAIN, X_IMAGE_DEV)","46ca350e":"### 3. Images Pre-processing\n\n#### 3.1 Extracting R, G and B\n\nSince each image has the color dimension that we saw above, one of our pre-processing steps is to extract each of these to create separate blue, red and green training examples. ","c4e4e3cf":"### 6. Convolutional Neural Network\n\nThe following section shows another methodology we tried out. We were able to use Kaggle's GPU-enabled kernels to train a model in less than 15 minutes.","ba38e027":"# W207 Applied Machine Learning\n\n## Summer 2018 Final Project\n\n**Team Members**: Rachel Ho, Dan Rasband, Subha Vadakkumkoor, Matt Vay\n\n**Topic**: ASL Alphabet\n\n#### Kaggle Datasets\n\n* [ASL Alphabet](https:\/\/www.kaggle.com\/grassknoted\/asl-alphabet) - this data set is the basis for all models.\n* [ASL Alphabet Test](https:\/\/www.kaggle.com\/danrasband\/asl-alphabet-test) - this data set is used to test against more realistic images.\n\n#### Running this Notebook Locally\n\n1. Download the datasets from the links above and update the directory paths accordingly in the relevant cell blocks.\n\n```\nTRAIN_DIR = \"directory path where training dataset is saved\"\n```\n\nNote that the current structure is set to how things would look if this notebook were to be run on Kaggle.\n\n2. Install libraries below.\n\n```\npip install --upgrade opencv-python tqdm scikit-image pandas \\\n    numpy matplotlib keras tensorflow scikit-learn seaborn\n```\n\n#### Credits\n\n1. Code for loading images is credited to Paul Mooney: https:\/\/www.kaggle.com\/paultimothymooney\/interpret-sign-language-with-deep-learning\n\n2. Image processing credited to Adrian Rosebrock:  https:\/\/www.pyimagesearch.com\/2015\/04\/06\/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv\/\n\n3. Convolutional Neural Network: https:\/\/www.kaggle.com\/grassknoted\/asl-alphabet\n\n#### Libraries to Install\n\nTo run this notebook, you may have to run the following commands to install some prerequisites:\n\n```bash\npip install --upgrade opencv-python tqdm scikit-image pandas \\\n    numpy matplotlib keras tensorflow scikit-learn seaborn\n```","2b7d863a":"In this part, we build out the convolutional neural network. Again, the model building process is skipped if the model has already been saved to disk.","09369e32":"In this section we perform error analysis for the bagged tree classifier with number of estimators set to 50.   Since accuracy hovers around 85% for estimators equal to or above 50, we picked this over a larger number of estimators to run things more efficiently.   From the confusion matrix, we can see that for this classifier the most often mis-labeled pairs of hand-signs include \"B\" with \"A\" and \"D\" with \"C\".  Consistent with the results of the confusion matrix, we see from the classification report that the hand-sign \"B\" has the lowest F1 score among all the symbols.","f5948aff":"Running predictions on another set of data, however, shows that the feature engineering is insufficient to have a reasonable success rate, though the predictions are better than pure chance. In fact, it's apparent that there are a few letters that do extremely well, while others struggle. This may be an artifact of where those particular images were taken, though further analysis would have to be made to determine the exact cause.","41d35913":"#### 5.3 Support Vector Machines\n\nNext we will take a look at the support vector machine algorithm to predict our data. We will run the RGB and blurred data through it along with transformations through PCA to determine which data set has the highest accuracy. From there we will tune the parameters C and gamma. Support vector machines work by placing hyperplanes through the data in N-dimensional space when a simple linear line can not separate the data.","7236cdf8":"### 6.1 Data Processing Set-Up\n\nIn the next snippet, I make a generator for use by Keras. The `make_generator` function is versatile enough to be used for setting up a generator for training, validation, prediction, and testing.","5a39eb14":"### 8. Conclusion\n\nUsing 29 different classes, we looked to take an image of a hand and predict the letter or symbol it was trying to sign. When plotting out all the images, we realized the images varied greatly ranging from what was in the background, the brightness to the angles of the hand. To combat some of this noise, we used filtering techniques such as gaussian blurs, cropping, edge detection and even breaking the images down by their red, green and blue pixels. After this initial processing, we used PCA to reduce the dimensionality of our images down to 110 components which explained 95% of the variance.\n\nThe models we used first for our predictions were K Nearest Neighbors, Bagged Trees, and Support Vector Machines. Out of these 3 models, Bagged Trees performed the best with an accuracy of 87% with the number of estimators set to 80.   K Nearest Neighbors performed the second with an accuracy of around 70%. We found that applying PCA to reduce the dimensionality and also performing a gaussian blur increased the accuracy over just using our original unfiltered data. The least accurate model was SVM at around 60%. The SVM's accuracy was also greatly helped by transformation through PCA. For future iterations, we would like to explore random forests, naive bayes and multilayer perceptrons more in depth.\n\nWe next tried a convolutional neural network. It had good results with some minor preprocessing (converting images using the Sobel method). When this model was applied to similar images from a less contrived environment, it still performed better than chance, but had much lower accuracy than when run on the validation set. In fact, it was only about half as good.\n\nLastly, we spent time looking at classifying the images with Naive Bayes and Multi-layer Perceptron classifiers, but eventually pulled the Multi-layer Perceptron classifier because of how long it was taking to run.","945cbe5c":"In this next section, the model is fitted against the data, splitting the data into training and validation sets.","45e11a2a":"#### Naive Bayes\n\nHere we briefly look at performance of Naive Bayes on the contoured data. We used a Bernoulli NB as the data is contoured and has only 2 levels of pixels. NaiveBayes seems to work better on the contoured processed images better. Yet the accuracy at <45% is low ","d3290a25":"After determinig the optimal value of k is 1 with the mini training data and PCA lead to a slightly higher accuracy, we ran the k nearest neighbors algorithm against 8 different variations of our mini train data with our optimal k value. The data sets we used included the original and the original with PCA applied. Along with those, we took our Red, Green, and Blue data sets and applied PCA and\/or a gaussian blur. This lead to some interesting results such as the data set with the highest score was taking only the blue pixels from our image, applying a PCA to it and then a gaussian blur lead to a 3.6% accuracy increase over our original mini training data. This could mean a few things such as maybe the blue pixels did a better job at hiding images in the background and brining out the hand and shadows in the foreground. Overall, each time we applied a blur we saw an increase in the accuracy over the original data. It is worth noting that while the \"Blur_Blue_PCA\" dataset had the highest score, our \"Blur_PCA\" (original mini training data with a blur applied) came in second place with only an accuracy decrease of 0.2% which is likely insignificant. ","188b94f0":"For our classification task it seems we should focus on precision and f1-score as a few false positives would not cause a great deal of harm unlike if we were predicting bank fraud or sick patient detection. We can see that a few of our letters stand out as being easier to predict than others. These include C, F, G and M to name a few that have an f1 score falling between 0.80 and 0.85. The outright winner would be the 'nothing' sign with an f1 score of 0.93. This is a fairly easy one to classify though as it means no hand is found in the image. As far as letters where our model struggled, A, U, V and W all had low f1 scores which is interesting because the images for V and W seem fairly easy to distinguish compared to others. As we move towards signing full sentences instead of just predicting images, we can likely use our image classification along with some kind of 'bag of letters' model to determine which letters are likely to come after each other. ","a5ec06b2":"### 7. Other Classifiers\n\nThis section includes other classifiers we explored but did not tune in depth for this project.\n\n#### Random forest","4791c90d":"### 5. Model Development\n\n#### 5.1 K Nearest Neighbors\n\nWe use KNN as a baseline model to compare against more sophisticated models including bagged trees, SVM, and neural networks. Below we fit the KNN classifiers with both the dataset with original dimensions and the dataset with reduced dimensions. We use GridSearchCV to find the optimal value of k for both datasets. As seen from the results, in both cases, the optimal value of k is 1. This is expected as the feature space is very sparse even for the dataset with reduced dimensions. The training exmmples are spread out far from each other. In both cases, the accuray rates are similar at around 61-62%.","2312fbdf":"### 6.1 Model Specification\n\nThe model used here is taken from a [Kaggle kernel called Running Kaggle Kernels with a GPU](https:\/\/www.kaggle.com\/grassknoted\/asl-alphabet), and is an example of a convolutional neural network. It is made up of 12 layers, as is diagrammed below. I've provided a helper function below to load the model from disk so that this part of the notebook can be run without rebuilding the model each time.","adc76a7f":"We can see the the variations in gestures in sample plots below of 10 random images of each letter. The length of the hands, extent of background images, color intensity and brightness, all vary from image to image.","2cede3e4":"### 6.2 Validation Against Real-World Data\n\nThe data provided in the ASL Alphabet data set is very much contrived. It's obvious that the images are made with one person's hand, in basically one environment. Because if this, it seemed like a good idea to validate that the models were not overfitting to images in this controlled environment. Below you can see a video compilation of all the \"A\" images.","832a65a3":"### 1. Introduction\n\nOur dataset contains 29 classes: one for each letter from A to Z, space, delete, and nothing.  Each example in the dataset is a three-dimensional array of 200 X 200 X 3 pixels.  For the purpose of this project, in the step where the images are loaded from their corresponding sub-folder, we re-size the images to 50 x 50 x 3 (and 64 x 64 x 3 in the section on convolutional neural networks) to make the run-times of training more managable and reduce the number of pixels from 120,000 to 7,500.\n\nWe set the mini training dataset to 2900 examples as there are 29 labels so there are 100 examples for each label.  When splitting up the dataset, we collapse the three dimensional feature arrays (50X50X3) into one dimensional feature arrays (7,500) so they can be used for training.  \n\nFirst, we read and load the data into arrays. As the data size is huge, instead of reading all the images, we read in only a sample. As we have 3000 images of each letter named in the format of A1, A2, ..., A3000, B1, B2, ...., B3000, we create a random sample of indices and then read in only the sampled indiced. We use the `train_test_split` functionality to split the indices into two disjoint train and test indices.","a2559f07":"After performing a grid search we have found the optimal parameters for C and gamma. C, also known as regularization, will choose a smaller-margin hyperplane to make sure all the data is classified correctly for large values of C. For small values of C, it will look for a larger-margin hyperplane even if that means misclassifying points. Our model found a C value somewhere in the middle to be optimal for our case. Next we looked at the gamma parameter. A low gamma means points far from the likely separation line are considered in the calculation while a high gamma means points close to the separation line are considered in the calculation. Our optimal gamma value was fairly low which could mean our data was sparse and far from the decision boundary. After performing the gridsearch on the these two parameters, we were able to increase our accuracy from 60.9% up to 67.0%.\n\nBased off the confusion matrix shown above, two of the most confused signs are C & D, the model incorrectly predicts them a total of 7 times. A & E also have a high count of incorrect predictions at 9 due to the signs being very similar. They both make fists with the only difference being E tucking in the thumb and A not tucking. Based off the signs, I would expect R & U to be confused frequently as the only differnce between the two signs is R crosses the index and middle finger while U does not. However, based off the confusion matrix the signs are only confused for each other a total of 3 times. I think much of the confusion boiled down to darker photos and not being able to distinguish the lines of the fingers. Maybe increasing the brightness would be a further option for us to consider in future iterations of this model.","e3187315":"#### 3.2 Gaussian blur\n\nWe create a Gaussian blur as a pre-processing step to see if it can help improve training accuracy. The Gaussian blur takes the average of a pixel's 8 nearest neighbors and returns the average as its new pixel value. This technique was used on a few different algorithms such as k nearest neighbors and support vector machines with varying degrees of success which will be discussed further on in our report.","95e9ef29":"Here we show the \"history\" of the fitting process, which shows the loss and the accuracy of the fit.","6883d88c":"#### 5.2 Bagged Trees\n\nThis section covers the bagged trees classifier which is the bagging classifier with a decision tree as the underlying estimator. There are two main hyperparameters to tune for a bagged trees classifier: 1) the size of the underlying decision tree, and 2) the number of estimators used for bagging. \n\nIn the first code block, we train the bagged trees classifer with both the dataset with original dimensions and the dataset with reduced dimensions without tuning any parameters aside from setting the random state. As can be seen, the bagged trees classifer does much worse with dimensionality reduction. Therefore, we proceed to tune the bagged trees classifier without dimensionality reduction.\n\nIn the second code block, we tune the bagged trees classifier by varying: 1) the size of the underlying decision tree, and 2) the number of estimators used for bagging. The parameters we try for tree size include (100, 500, and 800) and number of estimators include (10, 20, 30, 50). From the output result, we conclude that limiting the size of the underlying tree i.e. pruning does not improve accuracy. On the other hand, as the number of estimators increases the accuracy rate increases accordingly. We proceed to focus on tuning the number of estimators without limiting the size of the underlying decision tree.\n\nIn the final code block, we tune the bagged trees classifier by trying a few more parameters for the number of estimators, including (50, 70, 80, 100). As evident from the results, accuracy increases steadily as the number of estimators increases until it starts to decrease again after the number of estimators reaches 80. The best accuracy of 87% is achieved with 80 estimators.","4b9f2932":"Below, we evaluate the CNN model against a set of images made with various backgrounds and with a different person's hand to see how well the model works on a related, but different, set of images.","2922c781":"The model performs quite well on the original validation set, as seen below:","cf812a7d":"#### 3.3 Edge Detection\n\nIn this segment we try various image processing functionalities available in Python to blur in the data ways other than Gaussian, binarize and to extract the contour\/edges of images. Edge detection can be tricky because there can be edges in the background as well and thus how tightly\/loosely to identify the edges can affect the quality of the output image and the prediction. In addition to a 'manual' edging using parameters provided, we also use a function to detect the image edges automatically. This function has been adapted from https:\/\/www.pyimagesearch.com\/2015\/04\/06\/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv\/","e32c8fff":"### 4. Feature Engineering - Dimensionality Reduction with PCA\n\nAs the dimensions (7,500) of our training data are still quite large after re-sizing the images, we try to reduce the dimensions using PCA in order to see if we can make the algorithms run more efficiently and if we can improve the accuracy for some of the classifiers. Our goal is to retain 95% of the variance. Originally we wanted to retain 99% of the variance but discovered that this requires the number of componenets to go up to thousands, essentially defeating the purpose of PCA. As can be seen from the results below, if we set the number of principal components to 110, we can reach 95% explained variance. The plot below shows the number of principal components vs. cumulative explained variance as we increase the number of components. We reduce the dimensions of the training dataset and development dataset by transforming them with the fitted PCA and store them separately to be used for training.\nInterestingly, we find that the accuracy worsens for the tree classifiers including random forests and bagged tress but improves for other classifers including KNN and SVM.","73a60fa6":"### 2. Exploration Data Analysis\n\nWe will explore the data to ensure that all labels are read in and also to visually examine the variations in the images."}}