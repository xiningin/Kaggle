{"cell_type":{"70ff0bf0":"code","781541ae":"code","1c5c18a6":"code","64e15d9a":"code","1138641e":"code","10f39ef9":"code","005b7760":"code","b04e389a":"code","b1419c07":"code","c1046559":"code","a1ad09d5":"code","1e43b16c":"code","e73e04ef":"code","b58ac8ea":"code","cacc5f5a":"code","1b667f31":"code","77b090b8":"code","e8c4feda":"code","4fdf0bc6":"code","2cdda829":"code","5ef19bc0":"code","4f6e98e1":"code","d0a9908e":"code","2df54c4d":"code","77b3f98b":"code","b5305993":"code","a5dfff9b":"code","110328a0":"code","2205266d":"code","4092eec9":"code","1d9aa4d7":"code","0a439c7e":"code","fb105a78":"code","ffd82840":"code","c99cdbcf":"code","b8a085b8":"code","f9213a29":"code","dc3e7199":"code","dcca113b":"code","c10e3f6c":"code","a392fd0a":"code","ca905dc0":"code","61b51b63":"code","7dba80e8":"code","2fb43448":"code","fd1d38c6":"code","3dfe9910":"code","ffac2669":"markdown","8c109a14":"markdown","1c6594a5":"markdown","9d7af3f8":"markdown","6f6ff939":"markdown","2812e8bb":"markdown","b44827bb":"markdown","b17a9ae9":"markdown","3eea3a40":"markdown","23ac9608":"markdown","0b1d8c71":"markdown","79ce8e71":"markdown","0a9e6cb5":"markdown","973d4921":"markdown","89dacf69":"markdown","57d40953":"markdown","516a4bc2":"markdown","1bc91da0":"markdown","415760c9":"markdown","6eb05a94":"markdown","a98b2d31":"markdown","c428af0a":"markdown","21bc9d3f":"markdown","9c204d7f":"markdown"},"source":{"70ff0bf0":"!pip install simple-colors","781541ae":"import numpy as np\nimport pandas as pd\nfrom simple_colors import *\nfrom termcolor import colored\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\nfrom scipy.stats import normaltest\nfrom scipy import stats\nfrom scipy.stats import iqr\n\n# Supress Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1c5c18a6":"#Setting up options\n\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)\npd.options.display.float_format = \"{:,.3f}\".format","64e15d9a":"# Load the data\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')","1138641e":"def data_desc(df):\n    \n    \"\"\"\n    This function helps us with simple data analysis.\n    We may explore the common information about the dataset, missing values, features distribution and duplicated rows\n    \"\"\"\n    \n    # applying info() method\n    print('*******************')\n    print(cyan('General information of this dataset', 'bold'))\n    print('*******************\\n')\n    print(df.info())\n    \n    print('\\n*******************')\n    print(cyan('Number of rows and columns', 'bold'))\n    print('*******************\\n')\n    print(\"Number of rows:\", colored(df.shape[0], 'green', attrs=['bold']))\n    print(\"Number of columns:\", colored(df.shape[1], 'green', attrs=['bold']))\n    \n    # missing values\n    print('\\n*******************')\n    print(cyan('Missing value checking', 'bold'))\n    print('*******************\\n')\n    if df.isna().sum().sum() == 0:\n        print(colored('There are no missing values', 'green'))\n        print('*******************')\n    else:\n        print(colored('Missing value detected!', 'green', attrs=['bold']))\n        print(\"\\nTotal number of missing values:\", colored(sum(df.isna().sum()), 'green', attrs=['bold']))\n        \n        print('\\n*******************')\n        print(cyan('Missing values of features', 'bold'))\n        print('*******************\\n')\n        display(df.isna().sum().sort_values(ascending = False).to_frame().rename({0:'Counts'}, axis = 1).T.style.background_gradient('Purples', axis = None))\n        print('\\n*******************')\n        print(cyan('Percentage of missing values of features', 'bold'))\n        print('*******************\\n')\n        display(round((df.isnull().sum() \/ (len(df.index)) * 100) , 3).sort_values(ascending = False).to_frame().rename({0:'%'}, axis = 1).T.style.background_gradient('PuBuGn', axis = None))\n\n        \n    # applying describe() method for categorical features\n    cat_feats = [col for col in df.columns if df[col].nunique() < 3]\n    print('\\n*******************')\n    print(cyan('Categorical columns', 'bold'))\n    print('*******************\\n')\n    print(\"Total categorical (binary) features:\", colored(len(cat_feats), 'green', attrs=['bold']))\n    display(df[cat_feats].describe())\n        \n        \n    # describe() for numerical features\n    cont_feats = [col for col in df.columns if df[col].nunique() >= 3 and col not in ('Id', 'Cover_Type')]\n    print('\\n*******************')\n    print(cyan('Numerical columns', 'bold'))\n    print('*******************\\n')\n    print(\"Total numerical features:\", colored(len(cont_feats), 'green', attrs=['bold']))\n    df = df[df.columns.difference(['Id', 'Cover_Type'], sort = False)]\n    display(df[cont_feats].describe())\n    \n    # Checking for duplicated rows -if any-\n    if df.duplicated().sum() == 0:\n        print('\\n*******************')\n        print(colored('There are no duplicates!', 'green', attrs=['bold']))\n        print('*******************')\n    else:\n        print('\\n*******************')\n        print(colored('Duplicates found!', 'green', attrs=['bold']))\n        print('*******************')\n        display(df[df.duplicated()])\n\n    print('\\n*******************')\n    print(cyan('Preview of the data - Top 10 rows', 'bold'))\n    print('*******************\\n')\n    display(df.head(10))\n    print('*******************\\n')\n    \n    print('\\n*******************')\n    print(cyan('End of the report', 'bold'))","10f39ef9":"data_desc(train)","005b7760":"data_desc(test)","b04e389a":"plt.figure(figsize=(10, 7))\nax = sns.countplot(y=train[\"Cover_Type\"], palette='muted', zorder=3, linewidth=5, orient='h', saturation=1, alpha=1)\nax.set_title('Distribution of Cover Type', fontname = 'Times New Roman', fontsize = 30, color = '#8c49e7', x = 0.5, y = 1.05)\nbackground_color = \"#8c49e7\"\nsns.set_palette(['#ffd514']*120)\n\nfor a in ax.patches:\n    value = f'Amount and percentage of values: {a.get_width():,.0f} | {(a.get_width()\/train.shape[0]):,.3%}'\n    x = a.get_x() + (a.get_width() \/ 16) \n    y = a.get_y() + a.get_height() \/ 2  \n    ax.text(x, y, value, ha='left', va='center', fontsize=11, \n    bbox=dict(facecolor='none', edgecolor='black', boxstyle='round4', linewidth=0.7))\n\n# ax.margins(-0.12, -0.12)\nax.grid(axis=\"x\")\n\nsns.despine(right=True)\nsns.despine(offset=15, trim=True)","b1419c07":"categorical_features =[]\nnumerical_features =[]\n\nfor col in train.columns:\n    if train[col].nunique() < 3:\n        categorical_features.append(col)\n    elif train[col].nunique() >= 3 and col not in ('Id', 'Cover_Type'):\n        numerical_features.append(col)\nprint('Catagoric features: ', categorical_features)\nprint()\nprint('Numerical features: ', numerical_features)","c1046559":"# Cardinality check\n\nprint(colored(\"In Train Dataset\", 'cyan', attrs=['bold', 'underline']))\nfor col in categorical_features:\n    print('{} unique values in {}'.format(train[col].nunique(), col))\n\nprint()\nprint(colored(\"In Test Dataset\", 'cyan', attrs=['bold', 'underline']))\nfor col in categorical_features:\n    print('{} unique values in {}'.format(test[col].nunique(), col))","a1ad09d5":"def cardinality(data):\n    for k in categorical_features:\n        print(f'{k}\\n{(np.round((data[k].value_counts() \/ len(data[k]))*100,3))}\\n')","1e43b16c":"cardinality(train)","e73e04ef":"cardinality(test)","b58ac8ea":"fig = go.Figure([go.Bar(x = train[categorical_features].nunique().index, y = train[categorical_features].nunique().values, marker_color='rgb(100, 14, 175)')])\n#fig.show()\n\nfig.update_traces(marker_line_color='rgb(120, 15, 155)', marker_line_width=1, opacity=0.7)\n\nfig.update_layout(\n    title=\"<b>Number of unique values of categorical features<b>\",\n    width=2000,\n    height=1250,\n    \n    xaxis = dict(showline=True,\n    title = '<b>Categorical Variables<b>',\n    tickangle = -30,\n    tickfont = dict(family='Times New Roman', color='black', size=16),\n    titlefont_size = 16,\n    ),\n\n    yaxis = dict(showline=True,\n    ticks = \"outside\", tickwidth=2, tickcolor='red', ticklen=7.5,\n    title = '<b># of unique values<b>',\n    tickfont = dict(family = 'Times New Roman', color='black', size=16),\n    titlefont_size = 16,\n    title_standoff = 5,\n    ),\n    bargap = 0.50, # gap between bars of adjacent location coordinates.   \n)","cacc5f5a":"def count_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 5\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 80))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.97)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.01, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n\n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.countplot(x = feature, palette='rocket_r', data=data, hue=None)\n        plt.xlabel(feature, fontsize=14, fontweight = 'bold')\n        plt.ylabel('#', fontsize=14, fontweight = 'bold')\n        for p in ax.patches:\n            height = p.get_height()\n            value = f'{p.get_height():,.0f} | {(p.get_height()\/data[feature].shape[0]):,.3%}'\n            ax.text(p.get_x()+p.get_width()\/2., height+15000, value, ha=\"center\", fontsize = 11, fontweight = 'bold')     \n        i += 1\n    \n    plt.suptitle(titleText, fontsize = 28, fontweight = 'bold', color = 'darkorange')\n    plt.show()    ","1b667f31":"count_plot(train, categorical_features, 'Categorical features of train dataset', hue=None)","77b090b8":"count_plot(test, categorical_features, 'Categorical features of test dataset', hue=None)","e8c4feda":"def count_plot_testtrain(data1, data2, features, titleText):\n  \n    L = len(features)\n    nrow= int(np.ceil(L\/4))\n    ncol= 5\n    remove_last= (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 80))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last = remove_last - 1\n\n    fig.subplots_adjust(top = 0.97)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.01, \n                    right=0.9,  \n                    wspace=0.4, \n                    hspace=0.4)\n    \n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.countplot(x=feature, color='#61057c', data=data1, label='train')         \n        ax = sns.countplot(x=feature, color='#b7f035', data=data2, label='test')\n        plt.xlabel(feature, fontsize=14, fontweight = 'bold')\n        plt.ylabel('#', fontsize=14, fontweight = 'bold')\n        ax = ax.legend(loc = \"best\", fontsize = 12)\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 28, fontweight = 'bold', color = 'indigo')\n    plt.show()","4fdf0bc6":"count_plot_testtrain(train, test, categorical_features, titleText = 'Categorical features of train & test datasets')","2cdda829":"def correlation_matrix(data, features):\n    \n    fig, ax = plt.subplots(1, 1, figsize = (20, 20))\n    plt.title('Pearson Correlation Matrix', fontweight='bold', fontsize=25)\n    fig.set_facecolor('#d0d0d0') \n    corr = data[features].corr()\n\n    # Mask to hide upper-right part of plot as it is a duplicate\n    mask = np.triu(np.ones_like(corr, dtype = bool))\n    sns.heatmap(corr, annot = False, center = 0, cmap = 'jet', mask = mask, linewidths = .5, square = True, cbar_kws = {\"shrink\": .70})\n    ax.set_xticklabels(ax.get_xticklabels(), fontfamily = 'sans', rotation = 90, fontsize = 12)\n    ax.set_yticklabels(ax.get_yticklabels(), fontfamily = 'sans', rotation = 0, fontsize = 12)\n    plt.tight_layout()\n    plt.show()","5ef19bc0":"correlation_matrix(train, categorical_features)","4f6e98e1":"correlation_matrix(test, categorical_features)","d0a9908e":"def box_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 5\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 20))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last = remove_last - 1\n\n    fig.subplots_adjust(top = 0.94)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.01, \n                    right=0.9,  \n                    wspace=0.1, \n                    hspace=0.7)\n    \n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        v0 = sns.color_palette(palette = \"pastel\").as_hex()[2]\n        ax = sns.boxplot(x = data[feature], color=v0, saturation=.75)  \n        ax = ax.legend(loc = \"best\")    \n        plt.xlabel(feature, fontsize=14, fontweight = 'bold')\n        plt.ylabel('Values', fontsize=14, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 28, fontweight = 'bold', color = 'navy')\n    plt.show()","2df54c4d":"box_plot(train, numerical_features, 'Box Plot of Numerical Columns of Train Dataset')","77b3f98b":"box_plot(test, numerical_features, 'Box Plot of Numerical Columns of Train Dataset')","b5305993":"def kde_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 5\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 20))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.94)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.01, \n                    right=0.9,  \n                    wspace=0.1, \n                    hspace=0.7)\n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.kdeplot(data[feature], color=\"m\", shade=True, label=\"%.3f\"%(data[feature].skew()))  \n        ax = ax.legend(loc = \"best\")    \n        plt.xlabel(feature, fontsize=14, fontweight = 'bold')\n        plt.ylabel('Density', fontsize=14, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 28, fontweight = 'bold', color = 'navy')\n    \n    plt.show()","a5dfff9b":"kde_plot(train, numerical_features, titleText = 'KDE Plot of Numerical Features of Train Dataset', hue = None)","110328a0":"kde_plot(test, numerical_features, titleText = 'KDE Plot of Numerical Features of Test Dataset', hue = None)","2205266d":"def correlation_matrix(data, features):\n    \n    fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n    plt.title('Pearson Correlation Matrix', fontweight='bold', fontsize=25)\n    fig.set_facecolor('#d0d0d0') \n    corr = data[features].corr()\n\n    # Mask to hide upper-right part of plot as it is a duplicate\n    mask = np.triu(np.ones_like(corr, dtype = bool))\n    sns.heatmap(corr, annot = False, center = 0, cmap = 'jet', mask = mask, linewidths = .5, square = True, cbar_kws = {\"shrink\": .70})\n    ax.set_xticklabels(ax.get_xticklabels(), fontfamily = 'sans', rotation = 90, fontsize = 12)\n    ax.set_yticklabels(ax.get_yticklabels(), fontfamily = 'sans', rotation = 0, fontsize = 12)\n    plt.tight_layout()\n    plt.show()","4092eec9":"correlation_matrix(train, numerical_features)","1d9aa4d7":"correlation_matrix(test, numerical_features)","0a439c7e":"def hist_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 5\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 20))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.94)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.01, \n                    right=0.9,  \n                    wspace=0.1, \n                    hspace=0.7)\n    \n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.histplot(data[feature], edgecolor=\"black\", color=\"darkseagreen\", alpha=0.7)  \n        ax = ax.legend(loc = \"best\")    \n        plt.xlabel(feature, fontsize=18, fontweight = 'bold')\n        plt.ylabel('Frequency', fontsize=18, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 32, fontweight = 'bold', color = 'navy')\n    plt.show()","fb105a78":"train_frac = train.sample(frac = 0.25).reset_index(drop = True)\nhist_plot(train, numerical_features, titleText = 'Histogram of Numerical Features of Train Dataset', hue = None)","ffd82840":"test_frac = test.sample(frac = 0.25).reset_index(drop = True)\nhist_plot(test, numerical_features, titleText = 'Histogram of Numerical Features of Test Dataset', hue = None)","c99cdbcf":"def qqplot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 5\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(30, 20))\n    fig.tight_layout()\n    fig.set_facecolor('#e4e4e4')\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.94)\n    plt.subplots_adjust(left=0.1,\n                    bottom=0.01, \n                    right=0.9,  \n                    wspace=0.1, \n                    hspace=0.7)\n        \n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)   \n        stats.probplot(data[feature],plot=plt)\n        plt.title('\\nQ-Q Plot')\n        plt.xlabel(feature, fontsize=18, fontweight = 'bold')\n        plt.ylabel('Sample Quantile', fontsize=18, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 32, fontweight = 'bold', color = 'navy')\n    plt.show()","b8a085b8":"qqplot(train, numerical_features, 'Q-Q Plot of Numerical Features of Train Dataset', hue=None)","f9213a29":"qqplot(test, numerical_features, 'Q-Q Plot of Numerical Features of Train Dataset', hue=None)","dc3e7199":"# D'Agostino and Pearson's Test\n\ndef normality_check(data):\n  for i in numerical_features:\n    # normality test\n    stat, p = normaltest(data[[i]])\n    print('Statistics=%.3f, p=%.3f' % (stat, p))\n    # interpret results\n    alpha = 1e-2\n    if p > alpha:\n        print(f'{i} looks Gaussian (fail to reject H0)\\n')\n    else:\n        print(f'{i} does not look Gaussian (reject H0)\\n')","dcca113b":"normality_check(train)","c10e3f6c":"normality_check(test)","a392fd0a":"def detect_outliers(x, c = 1.5):\n    \"\"\"\n    Function to detect outliers.\n    \"\"\"\n    q1, q3 = np.percentile(x, [25,75])\n    iqr = (q3 - q1)\n    lob = q1 - (iqr * c)\n    uob = q3 + (iqr * c)\n\n    # Generate outliers\n\n    indicies = np.where((x > uob) | (x < lob))\n\n    return indicies","ca905dc0":"# Detect all Outliers \noutliers = detect_outliers(train['Cover_Type'])\nprint(\"Total Outliers count for Cover Type : \", len(outliers[0]))\n\nprint(\"\\nShape before removing outliers : \",train.shape)\n\n# Remove outliers\n#train.drop(outliers[0],inplace=True, errors = 'ignore')\nprint(\"Shape after removing outliers : \",train.shape)","61b51b63":"train_iqr = pd.DataFrame()\ntrain_iqr.reindex(columns=[*train_iqr.columns.tolist(), \"-3 IQR\", \"-1.5 IQR\", \"1.5 IQR\", \"3 IQR\"], fill_value = 0)","7dba80e8":"data = []\n\nk = 0\ncolumns = [\"-3 IQR\", \"-1.5 IQR\", \"1.5 IQR\", \"3 IQR\"]\n\nfor i in numerical_features:\n\n    q1 = train[i].quantile(0.25)\n    q3 = train[i].quantile(0.75)\n    \n    iqr = (q3 - q1)\n    lob_1 = q1 - (iqr * 1.5)\n    uob_1 = q3 + (iqr * 1.5)\n    lob_3 = q1 - (iqr * 3)\n    uob_3 = q3 + (iqr * 3)\n    \n    number_uob_1 = f'{round(sum(train[numerical_features[k]] > uob_1) \/ len(train[numerical_features[k]]), 5):,.3%}'\n    number_lob_1 = f'{round(sum(train[numerical_features[k]] < lob_1) \/ len(train[numerical_features[k]]), 5):,.3%}'\n    number_uob_3 = f'{round(sum(train[numerical_features[k]] > uob_3) \/ len(train[numerical_features[k]]), 5):,.3%}'\n    number_lob_3 = f'{round(sum(train[numerical_features[k]] < lob_3) \/ len(train[numerical_features[k]]), 5):,.3%}'\n\n    values = [number_lob_3, number_lob_1, number_uob_1, number_uob_3]\n    zipped = zip(columns, values)\n    a_dictionary = dict(zipped)\n    print(a_dictionary)\n    data.append(a_dictionary)\n    \n    k = k + 1","2fb43448":"train_iqr = train_iqr.append(data, True)\ntrain_iqr.set_axis([numerical_features], axis='index')","fd1d38c6":"def colour(value):\n\n    if float(value.strip('%')) > 10:\n      color = 'red'\n    elif float(value.strip('%')) > 5:\n        color = 'darkorange'   \n    else:\n      color = 'green'\n\n    return 'color: %s' % color\n\n# train_iqr = train_iqr.set_axis([numerical_features], axis='index')\ntrain_iqr = train_iqr.style.applymap(colour)","3dfe9910":"train_iqr","ffac2669":"* **It is very obvious that some features contain significant amount of outlier value in both data sets. This has to be handled.**","8c109a14":"<a id=\"corr_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.2.3. Correlation Matrix of Numerical Variables<\/p>","1c6594a5":"* **The Q-Q plot with clues to the normal distribution also shows tremendously that the data is not normally distributed.**","9d7af3f8":"* **Supporting the box chart, it can be seen from this chart that there are various outliers.**","6f6ff939":"<a id=\"categorical_variables\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">2.1. Categorical Variables<\/p>","2812e8bb":"<a id=\"corr_categorical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.1.2. Correlation Matrix of Categorical Variables<\/p>","b44827bb":"<a id=\"kde_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.2.2. KDE Plot of Numerical Variables<\/p>","b17a9ae9":"* **The logic in the KDE plots is also executed in the histogram plots.**","3eea3a40":"[back to top](#table-of-contents)\n<a id=\"preperation\"><\/a>\n# <p style=\"background-color:#3a2c57; font-family:newtimeroman; font-size:150%; text-align:center\">1. Preperation<\/p>\n\n\n<a id=\"load_packages_import_libraries\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">1.1. Loading Packages and Importing Libraries<\/p>\n\n* **Loading packages and importing some helpful libraries.**","23ac9608":"* **Soil_type7 and Soil_type15 seem to only take 0.**","0b1d8c71":"<a id=\"numerical_variables\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">2.2. Numerical Variables<\/p>","79ce8e71":"<a id=\"no_cat_features\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.1.1. Number of Categorical Variables<\/p>","0a9e6cb5":"<a id=\"norm_check_outlier_detect\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">2.3. Normality Check and Outlier Detection<\/p>","973d4921":"<a id=\"data_description\"><\/a>\n## <p style=\"background-color:#664e99; font-family:newtimeroman; font-size:120%; text-align:center\">1.2. Data Description<\/p>\n\n* **First of all, some setting up options were made. It is aimed to show all rows and columns in order to improve the general view of data sets. Next, I will load the train and test data sets and display train and test data sets as well.**","89dacf69":"* **There is no significant correlation between numerical variables in both train and test dataset.**","57d40953":"* **Except between 'Wilderness_Area1' and 'Wilderness_Area3', there is no significant correlation between categorical variables in both train and test dataset. No correlation between variables is even greater than 0.01. Additionally, the relationships between the variables are similar in both data sets.**","516a4bc2":"[back to top](#table-of-contents)\n<a id=\"eda\"><\/a>\n# <p style=\"background-color:#3a2c57; font-family:newtimeroman; font-size:150%; text-align:center\">2. Exploratory Data Analysis (EDA)<\/p>\n\n* **All numerical and categorical variables will be explored in this section.**","1bc91da0":"<a id=\"hist_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.2.4. Histogram Plot of Numerical Variables<\/p>","415760c9":"## <p style=\"background-color:#3a2c57; font-family:newtimeroman; margin-bottom:2px; font-size:32px; color: white; text-align:center\">Table of Content<\/p>  \n\n<a id=\"table-of-contents\"><\/a>\n1. [Preperation](#preperation)\n    * 1.1. [Loading Packages and Importing Libraries](#load_packages_import_libraries)\n    * 1.2. [Data Description](#data_description)\n2. [Exploratory Data Analysis (EDA)](#eda)\n    * 2.1. [Categorical Variables](#categorical_variables)\n        * 2.1.1. [Number of Categorical Variables](#no_cat_features)\n        * 2.1.2. [Correlation Matrix of Categorical Variables](#corr_categorical_variables)\n    * 2.2. [Numerical Variables](#numerical_variables)\n        * 2.2.1. [Box Plot of Numerical Variables](#box_numerical_variables)\n        * 2.2.2. [KDE Plot of Numerical Variables](#kde_numerical_variables)\n        * 2.2.3. [Correlation Matrix of Numerical Variables](#corr_numerical_variables)\n        * 2.2.4. [Histogram Plot of Numerical Variables](#hist_numerical_variables)\n        * 2.2.5. [Q-Q Plot of Numerical Variables](#qq_numerical_variables)\n    * 2.3. [Normality Check and Outlier Detection](#norm_check_outlier_detect)\n       * 2.3.1. [Mild and Extreme Outlier Detection](#mild_extreme_outlier)","6eb05a94":"* **This dataframe about how to manage outlier values during the feature engineering section while developing the model will be very helpful.** ","a98b2d31":"<a id=\"qq_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.2.5. Q-Q Plot of Numerical Variables<\/p>","c428af0a":"* **Obviously, since the Cover Type variable is categorical, there is no outlier value for this variable. There are many outliers for other features, but no direct data dropping is done in order not to lose an enormous number of rows.** ","21bc9d3f":"<a id=\"box_numerical_variables\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.2.1. Box Plot of Numerical Variables<\/p>","9c204d7f":"<a id=\"mild_extreme_outlier\"><\/a>\n## <p style=\"background-color:#9370db; font-family:newtimeroman; font-size:100%; text-align:center\">2.3.1. Mild and Extreme Outlier Detection<\/p>"}}