{"cell_type":{"101d368f":"code","5eed8084":"code","4422016b":"code","b07d01e0":"code","4b06d078":"code","615b24c0":"code","72324150":"code","c8296c2a":"code","77f3009a":"code","6a7de281":"code","bb65d5c0":"code","6d1bdf9a":"code","280c03ac":"code","5ec7f0c0":"code","a9219e74":"code","9ae43302":"markdown","e0796384":"markdown","cb6b4d49":"markdown","df1b0c66":"markdown","e9ef6f9d":"markdown","d90fbde2":"markdown","63130d76":"markdown","2942c14e":"markdown","6bd3230d":"markdown","e9356a24":"markdown"},"source":{"101d368f":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\n\ndf = pd.read_csv(\"..\/input\/company-bankruptcy-prediction\/data.csv\")","5eed8084":"# calculate the correlation matrix\ncols = []\ncols_done = []\nfor col_one in df.iloc[:,:].columns:\n    if (df[col_one].corr(df['Bankrupt?']) > 0.05):\n        cols.append(col_one)\n    cols_done.append(col_one)\ncorrdf = df.copy()\ncorrdf = corrdf[cols].corr()\n\n    \n# plot the heatmap\nsns.heatmap(corrdf, \n        xticklabels=corrdf.columns,\n        yticklabels=corrdf.columns)","4422016b":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\npltdf = df.copy()\nrename = [cname[0:10] for cname in df.columns]\npltdf.columns = rename\npltdf.iloc[:100, :30].plot(subplots=True, layout=(20,5), figsize=(20,15))\n\nplt.show()","b07d01e0":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\ny = df.iloc[:,0].copy()\nX = df.iloc[:,1:].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) \nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","4b06d078":"from sklearn.model_selection import cross_val_score\n\ndef rmse_cv(model, X, y, cv):\n    rmse= np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = cv))\n    return(rmse)","615b24c0":"from sklearn import linear_model\n\nclf = linear_model.LogisticRegression(solver='liblinear')\nclf.fit(X_train,y_train)\nprint(rmse_cv(clf, X, y, 5))\nprint(clf.score(X_test,y_test))","72324150":"import lightgbm as lgb\nclf = lgb.LGBMClassifier()\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)","c8296c2a":"from sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(random_state=0)\nclf.fit(X_train, y_train)\nscore = clf.score(X_test, y_test)\nprint(str(score))","77f3009a":"!pip install pycaret","6a7de281":"from pycaret.classification import *\n\ny = df.iloc[:,0].copy()\nX = df.iloc[:,1:].copy()\nX_train_pycaret, X_test_pycaret, y_train_pycaret, y_test_pycaret = train_test_split(X, y, test_size=0.1, random_state=42) \nsetup(data = pd.concat([X_train_pycaret, y_train_pycaret], axis=1), \n             target = 'Bankrupt?',\n             numeric_imputation = 'mean',\n             silent = True, normalize = True)\nprint(\"done\")","bb65d5c0":"top3 = compare_models(exclude = ['catboost','xgboost','lightgbm'], n_select=3)","6d1bdf9a":"top3[0]","280c03ac":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=100,\n                       n_jobs=-1, oob_score=False, random_state=874, verbose=0,\n                       warm_start=False)\nclf.fit(X_train, y_train)\nscore = clf.score(X_test, y_test)\nprint(str(score))","5ec7f0c0":"top3[2]","a9219e74":"clf = GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n                           learning_rate=0.1, loss='deviance', max_depth=3,\n                           max_features=None, max_leaf_nodes=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=1, min_samples_split=2,\n                           min_weight_fraction_leaf=0.0, n_estimators=100,\n                           n_iter_no_change=None, presort='deprecated',\n                           random_state=0, subsample=1.0, tol=0.0001,\n                           validation_fraction=0.1, verbose=0,\n                           warm_start=False)\nclf.fit(X_train, y_train)\nscore = clf.score(X_test, y_test)\nprint(str(score))","9ae43302":"# Look at the Data","e0796384":"# Scale Data and Split X\u00b6","cb6b4d49":"# Check Correlations","df1b0c66":"95 is much better than expected considering the lack of correlation before. Cross Validation looks consistent too.","e9ef6f9d":"# Apply Logistic Regression","d90fbde2":"Unfortunately Pycaret produces worse results than what we had. Though they are very similar. I'm not sure why it would be worse though.","63130d76":"# Try Pycaret","2942c14e":"# Load the Data","6bd3230d":"# Setup Cross Validation","e9356a24":"# Try other approaches"}}