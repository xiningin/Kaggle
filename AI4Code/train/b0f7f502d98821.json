{"cell_type":{"ceb99946":"code","750052cf":"code","7563f0a0":"code","3ab28961":"code","7fae514e":"code","4e5e56cc":"code","791f21ff":"code","dfe6b63c":"code","2ad79486":"code","bc86ad39":"code","530a7320":"code","4a94391e":"code","09992266":"code","c0a8d922":"code","6c598096":"markdown","11ab4c38":"markdown","0e4af319":"markdown","127953ec":"markdown","cbc240af":"markdown","a6973912":"markdown","5092d7eb":"markdown","a1280589":"markdown","e948f587":"markdown"},"source":{"ceb99946":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_datasets as tfds\nfrom matplotlib import pyplot as plt","750052cf":"DATA_DIR = '\/kaggle\/input\/cars196'\n\n[train_ds, test_ds], ds_info = tfds.load(\n    \"cars196\",\n    # Reserve 10% for validation and 50% for test\n    split=[\"train\", \"test\"],\n    as_supervised=True,  # Include labels\n    with_info=True,\n    download=False,\n    data_dir=DATA_DIR,\n)","7563f0a0":"tfds.visualization.show_examples(train_ds, ds_info)","3ab28961":"train_ds","7fae514e":"train_ds","4e5e56cc":"height, width = 150, 150\n#size = (150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (tf.image.resize(x, (height, width)), y))\n#validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, size), y))\ntest_ds = test_ds.map(lambda x, y: (tf.image.resize(x, (height, width)), y))\ntrain_ds","791f21ff":"# # use a different plotting function\n# # tfds.visualization.show_examples(train_ds, ds_info)\n\n# # for i, (first_image, label) in enumerate(train_ds.take(18)):\n# #     plt.figure(figsize=(10, 20))\n# #     ax = plt.subplot(6, 3, i + 1)\n# #     plt.imshow(first_image.numpy().astype(\"int32\"))\n# plt.figure(figsize=(10, 20))\n# for i, (first_image, label) in enumerate(train_ds.take(18)):        \n#         ax = plt.subplot(6, 3, i + 1)\n#         plt.imshow(first_image.numpy().astype(\"int32\"))\n#         plt.title(ds_info.features[\"label\"].names[int(label)])\n#         plt.axis(\"off\")","dfe6b63c":"batch_size = 32 # sdantard batch size for images\n\n\n# train_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n# #validation_ds = validation_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n# test_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n# from tensorflow import keras\n# from tensorflow.keras import layers\n\n# data_augmentation = keras.Sequential(\n#     [\n#         layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n#         layers.experimental.preprocessing.RandomRotation(0.1),\n#     ]\n# )\n\n\ndef augment_func(image,label):\n    image = tf.image.resize_with_crop_or_pad(image,height+6,width+6)\n    #image = tf.clip_by_value(image,0,255) #make sure you have no color value higher than 225 or lower than 0.\n    image = tf.image.random_crop(image,size=[height,width,3])\n    image = tf.image.random_flip_left_right(image) #different aspect of vehicles\n    image = tf.image.random_hue(image,0.2) #random color, change a red cat into a blue car\n    image = tf.image.random_contrast(image,0.5,2)# random contrast\n    image = tf.image.random_saturation(image,0,2)# random sturations\n    return image, label\n\n\ntrain_ds = train_ds.cache().map(augment_func).shuffle(100).batch(batch_size).prefetch(buffer_size=10) # cache makes the images ready before running\ntest_ds = test_ds.cache().map(augment_func).batch(batch_size).prefetch(buffer_size=10)","2ad79486":"plt.figure(figsize=(10, 20))\nfor i, (image_batch, label) in enumerate(train_ds.take(18)): # we did batch(batch_size) before, if we didn't, \"take\" will take individual image.\n        ax = plt.subplot(6, 3, i + 1)\n        plt.imshow(image_batch[3].numpy().astype(\"int32\")) #tensor flow treats things as floating numbers, but images need integer.\n        plt.title(ds_info.features[\"label\"].names[int(label[3])])\n        plt.axis(\"off\")\n","bc86ad39":"train_ds","530a7320":"base_model = tf.keras.applications.Xception(\n    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n    input_shape=(height, width, 3), ### Change here in my own project\n    include_top=False, # Do not include the final ImageNet classifier layer at the top.\n)  \n\nbase_model.trainable = True # We want to update all the model weights, so set this to true.\n# we have different calssification from before, because we classify the lable of car, not whether it's a car or not.\n\n# Create new model on surrounding our pretrained base model.\ninputs = tf.keras.Input(shape=(height, width, 3))\n\n# Pre-trained Xception weights requires that input be normalized\n# from (0, 255) to a range (-1., +1.), the normalization layer\n# does the following, outputs = (inputs - mean) \/ sqrt(var)\nnorm_layer = keras.layers.experimental.preprocessing.Normalization()\nmean = np.array([127.5] * 3)\nvar = mean ** 2\n# Scale inputs to [-1, +1]\nx = norm_layer(inputs)\nnorm_layer.set_weights([mean, var])\n\n# The base model contains batchnorm layers. We want to keep them in inference mode\n# when we unfreeze the base model for fine-tuning, so we make sure that the\n# base_model is running in inference mode here.\nx = base_model(x, training=False)\nx = keras.layers.GlobalAveragePooling2D()(x) # this is a neural network operation to help adapt the features learned by the pretrained model to our specific task.\nx = keras.layers.Dropout(0.5)(x)  # Regularize with dropout ### Change here in my own project\nnum_outputs = ds_info.features['label'].num_classes # This is the number of output variables we want, 196 in this case.\noutputs = keras.layers.Dense(num_outputs, activation=\"softmax\")(x) # Change num_outputs to 1, and delete activation in our own project.\n# Use activation=softmax for classification, and activation=None for regression.\nmodel = keras.Model(inputs, outputs)\n\nmodel.summary()","4a94391e":"# model.compile(optimizer='adam',\n#                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n#                 metrics=['sparse_categorical_accuracy'])\n\n# epochs = 100\n# model.fit(train_ds, epochs=epochs,validation_data = test_ds)","09992266":"# Unfreeze the base_model. Note that it keeps running in inference mode\n# since we passed `training=False` when calling it. This means that\n# the batchnorm layers will not update their batch statistics.\n# This prevents the batchnorm layers from undoing all the training\n# we've done so far.\nlearning_rate = 5.0e-5\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate = learning_rate),\n                loss=tf.keras.losses.SparseCategoricalCrossentropy(), # from_logits=True\n                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n\nepochs = 200\nmodel.fit(train_ds, epochs = epochs, validation_data = test_ds)","c0a8d922":"model.save(\"model.h5\", save_format=\"h5\")","6c598096":"Let's visualize what the first 18 images of the first batch looks like after various random transformations.\n\nNote that because the augmentations in the previous cell are applied randomly, these images will look different everytime they are run through the model during training.","11ab4c38":"First, let's get the data. If you're doing this on a non-Kaggle machine, change `download` to `True` and change `DATA_DIR` to the name of the local directory you want to download the data to.","0e4af319":"Finally, we can save the model for later use. If you are doing this on Kaggle, there is an option to download the saved file in the panel on the right side of the screen.","127953ec":"## Build a model\n\nNow let's built a model.\n\n1. We add a Normalization layer to scale input values (initially in the [0, 255] range) to the [-1, 1] range, because this is the format that is expected by the pre-trained model that comes next.\n1. We start with a pre-trained model that's trained on the [ImageNet](http:\/\/image-net.org\/about-overview) dataset, which includes a large number of images with a large number of different labels, but doesn't not include as much specificity regarding vehicle types as the cars196 dataset does. Training these models from scratch is tricky; it is much easier to start with a pre-trained model and fine tune it for use for a different task.\n3. We add our own classification layer at the end of the model, with 96 outputs representing our 96 vehicle classes, and \"softmax\" activation which forces the output values to all be between 0 and 1, and to all sum to 1.\n4. We add a Dropout layer before the above classification layer, for regularization.\n\n\nWe need the number of outputs in the final layer to equal the number of variables or classes we want to predict: in this case, 196 vehicle types. \nWe use a softmax activation on the on the final layer for classification problems, but if we want to use this model for regression we would only have to change the number of desired outputs and set `activation=None`.","cbc240af":"## Preprocessing: Resizing and random data augmentation\n\nWhen you don't have a large image dataset, it's a good practice to artificially introduce sample diversity by applying random yet realistic transformations to the training images, such as random horizontal flipping or small random rotations. This helps expose the model to different aspects of the training data while slowing down overfitting.\n\nAdditionally, let's the data and use caching and prefetching to optimize load speed:","a6973912":"## Fine-tune the model\n\nWe use a relatively low learning rate to prevent the model from unlearning what it learned when being trained on the larger imagenet dataset.","5092d7eb":"Now, let's use the built-in visualization function to show some example images:","a1280589":"## Standardizing the data\nOur raw images have a variety of sizes. In addition, each pixel consists of 3 integer values between 0 and 255 (RGB level values). This isn't a great fit for feeding a neural network. We need to do 2 things:\n\n* Standardize to a fixed image size. We pick 150x150.\n* Normalize pixel values between -1 and 1. We'll do this using a Normalization layer as part of the model itself.\n\nIn general, it's a good practice to develop models that take raw data as input, as opposed to models that take already-preprocessed data. The reason being that, if your model expects preprocessed data, any time you export your model to use it elsewhere (in a web browser, in a mobile app), you'll need to reimplement the exact same preprocessing pipeline. This gets very tricky very quickly. So we should do the least possible amount of preprocessing before hitting the model.\n\nHere, we'll do image resizing in the data pipeline (because a deep neural network can only process contiguous batches of data), and we'll do the input value scaling as part of the model, when we create it.\n\nLet's resize images to 150x150:","e948f587":"# Computer vision: Classifying car make, model and year\n\nComputer vision could potentially be used to automate traffic censuses and other tasks that require identification of vehicles.\nThe <a href=\"https:\/\/www.tensorflow.org\/datasets\/catalog\/cars196\">cars196<\/a> dataset contains 16,185 images of 196 different types of cars, which\ncan be used to train a supervised learning system to determine the make and model of a vehicle in a photograph."}}