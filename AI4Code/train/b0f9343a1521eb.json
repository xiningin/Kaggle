{"cell_type":{"fea16e85":"code","ec03f95a":"code","1c8b1c1a":"code","ab89529c":"code","ac643e3a":"code","696ccfd3":"code","c3390150":"code","82c671b4":"code","557fa81c":"code","c0c705c7":"code","7b0c5cc5":"code","22941349":"code","d22e383a":"code","ad739af0":"code","c19bfe9d":"code","8031ad5f":"code","8ed62cd3":"code","590fdd1b":"code","fa2cc898":"code","25fdc824":"code","a0df6069":"code","7e46044e":"code","ac7ad2f6":"code","f33fd7f6":"code","13e906d8":"code","3c102bda":"code","c4974e3b":"code","0364fa05":"code","ce62ec58":"code","b39b8073":"code","7bab78ee":"code","c5770649":"code","6da91a84":"code","9f3a6f00":"code","9f2b3546":"code","345e2f54":"code","6ec14009":"code","c1e4a04a":"code","b511ce24":"code","7dcc58f1":"code","934f106a":"code","091a2f26":"code","c4e4d639":"code","7e8f7078":"code","e686c2dc":"code","545e4f82":"code","6a22a527":"code","44942ef1":"code","5f32a361":"code","86348312":"code","424769c0":"code","ae370389":"code","0aed0daf":"code","c941b961":"code","d4fca896":"code","5b506360":"code","bcb2a702":"code","c76f951c":"code","83679724":"code","ed008fb5":"code","c57da936":"code","cc0c7577":"code","3d22cc24":"code","a88bfa1e":"markdown","9125b8fd":"markdown","4d950eeb":"markdown","8170bd5f":"markdown","60f2cb9e":"markdown","072cbffe":"markdown","072e4a9b":"markdown","d9e79935":"markdown","5a2ff030":"markdown","db904928":"markdown","1f671cae":"markdown","e9ecda78":"markdown","04e7736a":"markdown","912b3fb1":"markdown","03ff4d9a":"markdown","32e47a79":"markdown","c48b38f4":"markdown","0f6caae1":"markdown","361d429d":"markdown"},"source":{"fea16e85":"import pandas as pd\nimport seaborn as sns\nimport plotly.express as xp\nimport plotly.graph_objects as go\nimport numpy as np\nfrom datetime import datetime\nimport missingno\nimport yaml\nfrom collections import Counter\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV,ShuffleSplit\nfrom sklearn.manifold import TSNE\n!pip install deep-forest\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\npalette = ['blue',\"red\",\"yellow\"]\nsns.palplot(palette)","ec03f95a":"def split_to_onehot(df, col):\n    \"\"\"\n    This method converts features separated by '|' into one-hot vectors.\n    Additionally it drops unnecessary values, which present only in \n    test set \/ train set or have only one value.\n    \"\"\"\n    # Getting all unique ganres values.\n    unique = []\n    for i in df.index:\n        unique.extend(df.loc[i,col].split(\"|\"))\n    if \"\" in unique:\n        unique.remove(\"\")\n    unique = list(set(unique))\n    \n    # Putting values into binary form \n    onehot = df.loc[:,[\"Category\"]]\n    onehot[unique] = np.zeros((len(unique),), dtype = np.int8)\n    for i in df.index:\n        g = set(df.loc[i,col].split(\"|\"))\n        for j in g:\n            if j!=\"\":\n                onehot.loc[i,j] = 1\n                \n    # Dropping unnecessary values            \n    _a = onehot.groupby(\"Category\").sum()\n    only_one = list(_a.sum()[_a.sum()==1].index)\n    only_train = list(_a.loc[\"none\"][_a.loc[\"none\"]==0].index)\n    only_test = list(_a.loc[[\"like\",'dislike']].sum()[_a.loc[[\"like\",'dislike']].sum()==0].index)\n    _a = set(only_one + only_train + only_test)\n    onehot = onehot.drop(_a, axis=1)\n    \n    return onehot\n\ndef onehot_to_tsne2(df, title):\n    \"\"\"\n    This method converts one-hot representation into two tsne values.\n    Such operation is needed to shrink the dimentionality of the dataset\n    \"\"\"\n    onehot = df.drop(\"Category\",axis=1)\n    embedding = TSNE(n_components=2, init=\"pca\")\n    embedded = embedding.fit_transform(onehot)\n    embedded = pd.DataFrame(embedded,columns=[f\"{title}_tsne1\",f\"{title}_tsne2\"])\n    return embedded\n\ndef plot_commulative_onehot(onehot):\n    \"\"\"\n    Method of plotting commulative values of the one hot feature representation\n    \"\"\"\n    _df = onehot.groupby(\"Category\").sum()\n    fig = go.Figure()\n    for i in range(len(_df.index)):\n        k = _df.index[i]\n        x,y=[],[]\n        for g in _df.columns:\n            if _df.loc[k,g]!=0:\n                x.append(g)\n                y.append(_df.loc[k,g])\n        fig.add_trace(go.Bar(x=x, y=y,name=k,marker=dict(color=palette[i])))\n    fig.show()","1c8b1c1a":"PATH = \"..\/input\/mymusicalprefrences\/\" \ntrain = pd.read_csv(f\"{PATH}train.csv\")\ntest = pd.read_csv(f\"{PATH}test.csv\")\ndescription = yaml.load(open(f\"{PATH}Description.yaml\",'r'),Loader=yaml.FullLoader)\ndf = pd.concat([train,test]).reset_index(drop=True)\ntr_mask = ~df.Category.isna()","ab89529c":"df.columns = [i.strip() for i in df.columns]\nprint(set(df.columns))","ac643e3a":"df.describe()","696ccfd3":"\nmissingno.bar(df, color=palette, figsize=(30,2))","c3390150":"cat_features = {\"Artists\",\"Track\",\"Version\",\"Artists_Genres\",\"Album\",\"Album_type\",\"Labels\",\"Vocal\",\"Country\",\"Key\"}\ncon_features = {\"Duration\",\"Release_year\",\"BPM\",\"Energy\",\"Dancebility\",\"Happiness\"}\ndisplay(df[cat_features].head())\ndisplay(df[con_features].head())","82c671b4":"sns.pairplot(df[list(con_features)+[\"Category\"]],palette=palette[:2], hue=\"Category\")","557fa81c":"\ndf[\"Category\"] = df[\"Category\"].fillna(\"none\").replace({0:\"dislike\",1:\"like\"})\ndf","c0c705c7":"description[\"Key\"]","7b0c5cc5":"xp.scatter(df, x=\"Key\", y=\"Track\",color=\"Category\", height=500, color_discrete_sequence=palette)","22941349":"df[\"isMajor\"], df[\"Key\"] = df[\"Key\"].apply(lambda x: x.split(\" \")[1]), df[\"Key\"].apply(lambda x: x.split(\" \")[0])\ndf.loc[:,\"Key\"] = df[\"Key\"].replace({\"D\u266d\": \"C#\", \"E\u266d\": \"D#\", \"G\u266d\": \"F#\", \"A\u266d\": \"G#\",\"B\u266d\":\"A#\"})\nxp.scatter(df, x=\"Key\", y=\"Track\",color=\"Category\", height=500, color_discrete_sequence=palette)","d22e383a":"df.loc[:,\"isMajor\"] = (df[\"isMajor\"]==\"Major\").astype(int)\n_df = df.groupby([\"isMajor\",\"Category\"], as_index=False).count()\nxp.bar(_df,x=\"isMajor\", y=\"Track\",color=\"Category\", height=400, color_discrete_sequence=palette)","ad739af0":"_df = df.copy(deep=True)\n_df[\"Key_percise\"] = _df[\"Key\"] +\"_major:\"+ _df[\"isMajor\"].astype(str)\n_df = _df.groupby([\"Key_percise\",\"Category\"], as_index=False).count()\nxp.bar(_df, x=\"Key_percise\", y=\"Track\", color=\"Category\", height=500, color_discrete_sequence=palette)","c19bfe9d":"df[list(set(df[\"Key\"].values))] = OneHotEncoder().fit_transform(df[[\"Key\"]]).toarray()\ndf = df.drop(\"Key\", axis=1)","8031ad5f":"description[\"Release year\"]","8ed62cd3":"xp.scatter(df, x=\"Release_year\", y=\"Track\",color=\"Category\", height=500, color_discrete_sequence=palette)","590fdd1b":"# Lets create the decade feature, to detect some music of 80th, 90th etc. as a specific janre\ndf.loc[:,\"Release_decade\"] = (df.loc[:,\"Release_year\"]\/\/10 * 10)\n# Cause of the small number of values, we will put all <90th toone value, called 80th\ndf.loc[df.loc[:,\"Release_decade\"]<1990,\"Release_decade\"] = 1980 \n_df = df.groupby([\"Release_decade\",\"Category\"], as_index=False).count()\nxp.bar(_df,x=\"Release_decade\", y=\"Track\",color=\"Category\",height=500, color_discrete_sequence=palette)","fa2cc898":"description[\"Artists Genres\"]","25fdc824":"ganres_onehot = split_to_onehot(df, \"Artists_Genres\")\nplot_commulative_onehot(ganres_onehot)","a0df6069":"genres_embedded = onehot_to_tsne2(ganres_onehot, \"Genres\")\n_df = genres_embedded.copy(deep=True)\n_df[[\"Category\",\"Artists_Genres\"]] = df[[\"Category\",\"Artists_Genres\"]]\nxp.scatter(_df,x=\"Genres_tsne1\",y=\"Genres_tsne2\",color=\"Category\", hover_data=[\"Artists_Genres\"], height=500, color_discrete_sequence=palette)","7e46044e":"df = pd.concat([df,genres_embedded], axis=1)\ndf = df.drop(\"Artists_Genres\", axis=1)","ac7ad2f6":"for k in [\"Energy\",\"Happiness\",\"Dancebility\",\"BPM\"]:\n    print(f\"{k}:{description[k]}\")","f33fd7f6":"df[\"BPM\"] = df[\"BPM\"].apply(lambda x: str(x)[1:] if str(x)[0]=='`' else x)\ndf[['Energy', 'Happiness', 'Dancebility','BPM']] = df[['Energy', 'Happiness', 'Dancebility','BPM']].fillna(0)\ndf[['Energy%', 'Happiness%', 'Dancebility%']] = df[['Energy', 'Happiness', 'Dancebility']].apply(lambda x: x\/sum(x), axis=1)\ndf[['Energy%', 'Happiness%', 'Dancebility%']] = df[['Energy%', 'Happiness%', 'Dancebility%']].fillna(0)","13e906d8":"print(description[\"Labels\"])","3c102bda":"df.Labels = df.Labels.fillna('NA')\nlabels_onehot = split_to_onehot(df, \"Labels\")\nplot_commulative_onehot(labels_onehot)","c4974e3b":"labels_embedded = onehot_to_tsne2(labels_onehot, \"Labels\")\n_df = labels_embedded.copy(deep=True)\n_df[[\"Category\",\"Labels\"]] = df[[\"Category\",\"Labels\"]]\nxp.scatter(_df,x=\"Labels_tsne1\",y=\"Labels_tsne2\",color=\"Category\", hover_data=[\"Labels\"], height=500, color_discrete_sequence=palette)","0364fa05":"df = pd.concat([df,labels_embedded[[\"Labels_tsne1\",\"Labels_tsne2\"]]], axis=1)\ndf = df.drop(\"Labels\", axis=1)","ce62ec58":"print(description[\"Artists\"])","b39b8073":"df.Artists = df.Artists.fillna(\"NA\")\nallstars = []\nfor i in df.index:\n    allstars.extend(df.loc[i, \"Artists\"].split(\"|\"))\nlen(set(allstars))","7bab78ee":"threshold = 3\nothers = Counter(allstars)\nothers = [k for k in others if others[k]<=threshold]\nlen(others)","c5770649":"in_train, in_test = [], []\nfor i in df.loc[tr_mask].index:\n    in_train.extend(df.loc[i, \"Artists\"].split(\"|\"))\nfor i in df.loc[~tr_mask].index:\n    in_test.extend(df.loc[i, \"Artists\"].split(\"|\"))\n    \nonly_test = set(in_test) - set(in_train)\nonly_train = set(in_train) - set(in_test)\ndisplay(len(only_test))\ndisplay(len(only_train))","6da91a84":"allstars = list(set(allstars) - set(others) - only_test - only_train)\nprint(len(allstars))\nothers = set(others) | only_test | only_train\nprint(len(others))","9f3a6f00":"res = []\ndef prune(x):\n    vector = np.zeros(len(allstars)+1) #for others\n    x = [i for i in x.split(\"|\")]\n    for i in range(len(allstars)):\n        vector[i]=1 if allstars[i] in x else 0\n    if len(x)>sum(vector):\n        vector[-1]=1\n    res.append(vector)\n\ndf[\"Artists\"].apply(prune)\nonehot_artists = pd.DataFrame(res, columns = allstars+[\"Others\"], index=df.index)","9f2b3546":"onehot_artists","345e2f54":"df[\"Other_Artists\"] = onehot_artists[\"Others\"]\nonehot_artists = onehot_artists.drop(\"Others\", axis=1)\nonehot_artists[\"Category\"] = df[\"Category\"]","6ec14009":"plot_commulative_onehot(onehot_artists)","c1e4a04a":"artists_embedded = onehot_to_tsne2(onehot_artists, \"Artists\")\n_df = artists_embedded.copy(deep=True)\n_df[[\"Category\",\"Artists\"]] = df[[\"Category\",\"Artists\"]]\nxp.scatter(_df,x=\"Artists_tsne1\",y=\"Artists_tsne2\",color=\"Category\", hover_data=[\"Artists\"], height=500, color_discrete_sequence=palette)","b511ce24":"df = pd.concat([df,artists_embedded[[\"Artists_tsne1\",\"Artists_tsne2\"]]], axis=1)\ndf = df.drop(\"Artists\", axis=1)","7dcc58f1":"for i in [\"Track\", \"Version\", \"Album_type\"]:\n    print(description[i])","934f106a":"artists_encoder = LabelEncoder()\ndf[\"Track\"] = artists_encoder.fit_transform(df[\"Track\"])","091a2f26":"_df = df.groupby([\"Version\",\"Category\"], as_index=False).count()\nxp.bar(_df,x=\"Version\",y=\"Id\",color=\"Category\", color_discrete_sequence=palette)","c4e4d639":"df[\"Version\"] = df[\"Version\"].fillna(\"NA\")\nversions = set(df[\"Version\"])\ndf[list(versions)] = OneHotEncoder().fit_transform(df[[\"Version\"]]).toarray()\ndf = df.drop([\"Version\",\"NA\"], axis=1)","7e8f7078":"_df = df.groupby([\"Album_type\",\"Category\"], as_index=False).count()\nxp.bar(_df,x=\"Album_type\",y=\"Id\",color=\"Category\", color_discrete_sequence=palette)","e686c2dc":"df[\"Album_type\"] = df[\"Album_type\"].fillna(\"NA\")\nversions = set(df[\"Album_type\"])\ndf[list(versions)] = OneHotEncoder().fit_transform(df[[\"Album_type\"]]).toarray()\ndf = df.drop([\"Album_type\",\"NA\"], axis=1)","545e4f82":"print(description[\"Album\"])","6a22a527":"df[\"Album\"] = df[\"Album\"].fillna(\"NA\")\nganres_onehot = split_to_onehot(df, \"Album\")\nplot_commulative_onehot(ganres_onehot)","44942ef1":"album_embedded = onehot_to_tsne2(onehot_artists, \"Album\")\n_df = album_embedded.copy(deep=True)\n_df[[\"Category\",\"Album\"]] = df[[\"Category\",\"Album\"]]\nxp.scatter(_df,x=\"Album_tsne1\",y=\"Album_tsne2\",color=\"Category\", hover_data=[\"Album\"], height=500, color_discrete_sequence=palette)","5f32a361":"df = pd.concat([df,album_embedded[[\"Album_tsne1\",\"Album_tsne2\"]]], axis=1)\ndf = df.drop(\"Album\", axis=1)","86348312":"print(description[\"Vocal\"])","424769c0":"df[\"Vocal\"] = df[\"Vocal\"].fillna('N')\nonehot = np.zeros((len(df),2))\nfor i in range(len(df)):\n    v = df.iloc[i][\"Vocal\"]\n    if v == 'F':\n        onehot[i] = [1,0]\n    elif v == 'M':\n        onehot[i] = [0,1]\n    elif v == 'F|M':\n        onehot[i] = [1,1]\ndf[[\"Fem_voc\",\"Mal_voc\"]] = onehot\ndf = df.drop(\"Vocal\",axis=1)","ae370389":"print(description[\"Country\"])","0aed0daf":"df[\"Country\"] = df[\"Country\"].fillna(\"NA\")\ncountry_onehot = split_to_onehot(df, \"Country\")\nplot_commulative_onehot(country_onehot)","c941b961":"country_onehot = country_onehot.drop(\"Category\", axis=1)\ndf = pd.concat([df,country_onehot], axis=1)\ndf = df.drop(\"Country\", axis=1)","d4fca896":"df2=df.replace({\"dislike\":0,\"like\":1}).drop(\"Id\", axis=1)\ndf2","5b506360":"from sklearn.model_selection import train_test_split\ny=df2.iloc[0:665,0]\nx=df2.iloc[0:665,1:]\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15)\n\ny_train=y_train.astype(np.uint8)\n\ny_test=y_test.astype(np.uint8)\n\nX_train=X_train.astype(np.uint8)\n\nX_test=X_test.astype(np.uint8)\n\ndeploy=df2[lambda df2: df2.columns[1:]].tail(300)\ndeploy=deploy.astype(float).astype(np.uint8)\n","bcb2a702":"y_train.values","c76f951c":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nscaler.fit(X_train)\nX_train = pd.DataFrame(data=scaler.transform(X_train),columns = X_train.columns,index=X_train.index)\ndeploy = pd.DataFrame(data=scaler.transform(deploy),columns = deploy.columns,index=deploy.index)\nX_test = pd.DataFrame(data=scaler.transform(X_test),columns = X_test.columns,index=X_test.index)\n\n# scaler.fit(X)\n# X_train = pd.DataFrame(data=scaler.transform(X))\n# deploy = pd.DataFrame(data=scaler.transform(x_test))\n# xtest = pd.DataFrame(data=scaler.transform(Xt))\n","83679724":"X_train.values","ed008fb5":"from deepforest import CascadeForestClassifier\n\nmodel=CascadeForestClassifier(random_state=2,n_trees=57)\nmodel.fit(X_train.values,y_train.values)","c57da936":"y_pred=model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy with CascadeForestClassifier: ', accuracy)","cc0c7577":"sample = pd.read_csv(f\"{PATH}sample_submition.csv\")\nsample[\"Category\"] = model.predict(deploy)\n","3d22cc24":"sample.to_csv(\"deploy.csv\", index=False)","a88bfa1e":"# 5. Model Selection  ","9125b8fd":"# 2.Data and Feature Processing","4d950eeb":"## 2.3 Genres features","8170bd5f":"## 2.5 Labels","60f2cb9e":"### 2.7.1 Tracks","072cbffe":"## 2.8 Vocal","072e4a9b":"### 2.7.2 Version","d9e79935":"# 4. Standardization","5a2ff030":"We selected the CascadeForestClassifier which is provided by Nanjing University.","db904928":"# 3. Train_Test Split","1f671cae":"## 2.4 Energy,Happiness,Dancebility, BPM","e9ecda78":"## 2.1 Key feature","04e7736a":"## 2.8 Album","912b3fb1":"# 1.Explorational Data Analysis","03ff4d9a":"## 2.9 Country","32e47a79":"### 2.7.3 Album_type","c48b38f4":"## 2.6 Artists","0f6caae1":"## 2.2 Release year feature","361d429d":"## 2.7 Tracks, Version, Album_type"}}