{"cell_type":{"c6606376":"code","f2032e14":"code","6cfc51b5":"code","f40d0513":"code","96e579e3":"code","bb764c0f":"code","139f8e3e":"code","14f5ebc8":"code","b6e00483":"code","6c1a79ef":"code","ab3e701f":"code","9952ff3c":"code","0d200089":"code","ed00debd":"code","fdbc2b0c":"code","77d5b605":"code","372c7a56":"code","1c51861b":"code","a09e961e":"code","a1bf40d9":"markdown","2a2c5c81":"markdown","afeba07d":"markdown","27b7cd0b":"markdown","0d0d7b14":"markdown","b1c6315e":"markdown","ac065989":"markdown","72018163":"markdown","75542dd5":"markdown","3318f814":"markdown","0905935a":"markdown","335cf35a":"markdown","71c8b1c5":"markdown","62841a0c":"markdown"},"source":{"c6606376":"# if we have internet connection\n#!pip install lightautoml -q\n\n# else\n!tar xvfz ..\/input\/lightautoml-tar\/lightautoml.tgz > \/dev\/null","f2032e14":"#Install the offline packages into our kernel\n!pip install lightautoml --no-index --find-links=file:.\/lightautoml\/  -q","6cfc51b5":"# Standard python libraries\nimport os\nimport time\nimport re\n\n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Imports from LightAutoML package\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\nfrom pandas_profiling import ProfileReport\n\nimport cv2\nimport datetime\nimport gc\nimport glob\nimport imagehash\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport sys\nimport tqdm\nimport PIL\nfrom keras.preprocessing import image","f40d0513":"DATA_DIR = '..\/input\/petfinder-pawpularity-score\/'\ntrain_data = pd.read_csv(DATA_DIR + 'train.csv')\ntest_data = pd.read_csv(DATA_DIR + 'test.csv')\nsample_submission = pd.read_csv(DATA_DIR + 'sample_submission.csv')\nsubmission = pd.read_csv(DATA_DIR+'sample_submission.csv')\n\nX_test = test_data.values","96e579e3":"def images_find_duplicates(image_files, threshold=0.9):\n    \"\"\"\n    Function to find duplicates in images.\n    References: https:\/\/www.kaggle.com\/appian\/let-s-find-out-duplicate-images-with-imagehash\n    Args:\n        image_files:\n        threshold:\n\n    Returns:\n\n    \"\"\"\n    funcs = [imagehash.average_hash, imagehash.phash, imagehash.dhash, imagehash.whash]\n    image_ids = image_files\n    hashes = []\n    for file in tqdm.tqdm(image_files):\n        image = PIL.Image.open(file)\n        hashes.append(np.array([f(image).hash for f in funcs]).reshape(256))\n    hashes_all = np.array(hashes)\n\n    # Comparisons without Pytorch\n    sim_list = []\n    for i in tqdm.tqdm(range(hashes_all.shape[0])):\n        sim_list.append(np.sum(hashes_all[i] == hashes_all, axis=1)\/256)\n\n    # nxn-matrix of similarities (n = # of images), upper triangular matrix\n    similarities = np.triu(np.array(sim_list), 1)\n\n    idx_pair = np.where(similarities > threshold)\n    df_pairs = pd.DataFrame({'image1': [image_ids[i] for i in list(idx_pair[0])],\n                             'image2': [image_ids[i] for i in list(idx_pair[1])],\n                             'similarity': [similarities[i1, i2] for i1, i2 in zip(idx_pair[0], idx_pair[1])]})\n\n    idx_group = np.zeros(len(image_files))\n    group_id = 1\n    for i1, i2 in zip(idx_pair[0], idx_pair[1]):\n        if idx_group[i1] == 0 and idx_group[i2] == 0:\n            idx_group[i1] = group_id\n            idx_group[i2] = group_id\n            group_id += 1\n        elif idx_group[i1] != 0 and idx_group[i2] == 0:\n            idx_group[i2] = idx_group[i1]\n        elif idx_group[i1] == 0 and idx_group[i2] != 0:\n            idx_group[i1] = idx_group[i2]\n        elif idx_group[i1] != 0 and idx_group[i2] != 0 and idx_group[i1] != idx_group[i2]:\n            common_id = min(idx_group[i1], idx_group[i2])\n            idx_group[idx_group == idx_group[i1]] = common_id\n            idx_group[idx_group == idx_group[i2]] = common_id\n\n    group_list = []\n    for i in range(1, group_id + 1):\n        group_ids = list(np.where(idx_group == i)[0])\n        if len(group_ids) > 0:\n            group_list.append([image_ids[j] for j in group_ids])\n\n    return df_pairs, group_list","bb764c0f":"train_files = []\nfor image in train_data['Id']:\n    image_path = f'{DATA_DIR}\/train\/{image}.jpg'\n    train_files.append(image_path)\nprint(f'Number of Petfinder training files: {len(train_files)}')\n\ntotal_files = []\ntotal_files.extend(train_files)\n\ndf_pairs, group_list = images_find_duplicates(total_files, threshold=0.90)\n\nprint(f'\\nNumber of duplicate pairs: {len(df_pairs)}')\n\nids_to_delete = []\nfor path1,path2 in zip(df_pairs['image1'],df_pairs['image2']):\n    image_id1 = path1.split('\/')[-1].split('.')[0]\n    \n    #print(train_data[train_data.Id == image_id1])\n    ids_to_delete.append(image_id1)\nprint(\"Size of DF before deleting duplicates\",len(train_data))\n\nids_to_delete = list(set(ids_to_delete))\nprint(\"Duplicates count = \", len(ids_to_delete))\n\nfor ids in ids_to_delete:\n    train_data = train_data[train_data.Id != ids]\nprint(\"Size of DF after removing Duplicates\",len(train_data))","139f8e3e":"%%time\nprofile = ProfileReport(train_data, title=\"Pandas Profiling Report\")\nprofile","14f5ebc8":"profile.to_file(\"PetFinder Meta features.html\")","b6e00483":"train_data.sample(5)","6c1a79ef":"y = train_data.Pawpularity.values\nX = train_data.drop(['Pawpularity'], axis=1).values","ab3e701f":"%%time\n\ntr_data, valid_data = train_test_split(train_data, test_size=0.2,random_state=42)","9952ff3c":"task = Task('reg', metric='mse',greater_is_better=False, loss='mse')","0d200089":"%%time\n\nroles = {'target': 'Pawpularity','drop': 'id','category':'format'}","ed00debd":"%%time\nautoml = TabularUtilizedAutoML(task = task, \n                       timeout = 10800, # 3 hours\n                       cpu_limit = 4, # Optimal for Kaggle kernels\n                       general_params = {'use_algos': [['linear_l2', 'lgb_tuned','cb_tuned']]})","fdbc2b0c":"%%time\noof_pred = automl.fit_predict(tr_data, roles = roles)","77d5b605":"valid_pred = automl.predict(valid_data)","372c7a56":"print('OOF RMSE: {}'.format(mean_squared_error(tr_data['Pawpularity'].values, oof_pred.data[:, 0],squared=False)))\nprint('VAL RMSE: {}'.format(mean_squared_error(valid_data['Pawpularity'].values, valid_pred.data[:, 0],squared=False)))","1c51861b":"test_pred = automl.predict(test_data)","a09e961e":"submission['Pawpularity'] = (test_pred.data[:, 0]).astype(int)\nsubmission.to_csv('submission.csv', index = False)","a1bf40d9":"# EOF","2a2c5c81":"# Quick EDA","afeba07d":"# Spilit","27b7cd0b":"# Check Valid","0d0d7b14":"# ChangeLog\n* v1 - install, import, quick EDA, train, predict, submit, Error with offline lib\n* v2 - changed offline lib \n* v3 - forget to pip install offline packages\n* v4 - look for duplicates in image and pawpularity\n* v5 - added meta features from [here](https:\/\/www.kaggle.com\/nexus6roy\/extract-extra-data-from-image) and remove duplicates taken from [here](https:\/\/www.kaggle.com\/yingpengchen\/find-duplicate-images)\n* v6 - hide large output, remove meta features, change the print from previous notebook","b1c6315e":"# Submission","ac065989":"# Test prediction","72018163":"## Train of dataset\n> Now we know what model to use to receive good results on the dataset","75542dd5":"# Create Task object\n> Below this line we are ready to build the model for Price target variable prediction. First of all, we setup the type of model we need using LightAutoML Task class object, there the valid values can be:\n\n* \u2018binary\u2019 for binary classification\n* \u2018reg\u2019 for regression and\n* \u2018multiclass\u2019 for multiclass classification","3318f814":"# Standard libs","0905935a":"# Let's use SBER AutoML lib \n> Only for this you can give me an upvote \ud83c\udf1f \n\n> And feel free to comment your opinion and anything you want :) \n\n> This will motivate me to do more experiments :)","335cf35a":"# OOF score","71c8b1c5":"# Data","62841a0c":"# Look for Duplicates and remove\n> [Thanks to](https:\/\/www.kaggle.com\/yingpengchen\/find-duplicate-images)"}}