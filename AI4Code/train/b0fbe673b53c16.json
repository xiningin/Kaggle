{"cell_type":{"b358f5c0":"code","8dec6a60":"code","bd5ba1c0":"code","0c56f017":"code","f0b99a6c":"code","e752d117":"code","4d6c2378":"code","214a9b1e":"code","0e4138f8":"code","1dfd00b2":"code","100a71d2":"code","34622854":"code","0099b094":"code","20cc1a41":"code","89e7fb1d":"code","a9c4fe6f":"markdown","ce6a8990":"markdown","96304be6":"markdown","7cfd1a40":"markdown","49dda019":"markdown","b8916e61":"markdown","0700c697":"markdown"},"source":{"b358f5c0":"import sys\nimport pathlib\nimport itertools\nimport collections\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import gaussian_kde\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model","8dec6a60":"data_folder = pathlib.Path(\"..\/input\/tabular-playground-series-jan-2021\")\ntrain_file = data_folder \/ \"train.csv\"\ntest_file = data_folder \/ \"test.csv\"\nsample_file = data_folder \/ \"sample_submission.csv\"","bd5ba1c0":"train_df = pd.read_csv(train_file)\ntest_df = pd.read_csv(test_file)","0c56f017":"print(f\"total data points in train set: {train_df.shape[0]}\")\nprint(f\"total data points in test set: {test_df.shape[0]}\")","f0b99a6c":"train_df = train_df.drop(\"id\", axis=1)\ntest_df = test_df.drop(\"id\", axis=1)","e752d117":"train_df.describe()","4d6c2378":"df_corr = np.abs(train_df.drop(\"target\", axis=1).corr())","214a9b1e":"for x_idx, y_idx in itertools.product(range(len(df_corr.index)), range(len(df_corr.columns))):\n    if y_idx>=x_idx:\n        df_corr.loc[df_corr.index[x_idx], df_corr.columns[y_idx]] = 0","0e4138f8":"fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111)\n\nim = ax.imshow(df_corr.values, cmap=\"plasma\")\nfig.colorbar(im, ax=ax)\n\nax.set_title(\"feature correlations!!\")\nax.set_xticks(range(len(df_corr.index)))\nax.set_xticklabels(df_corr.index, rotation=90)\nax.set_yticks(range(len(df_corr.columns)))\nax.set_yticklabels(df_corr.columns)\nfig.show()","1dfd00b2":"fig = plt.figure(figsize=(20,15))\naxes = fig.subplots(5,3).ravel()\n\nfor i, ax in enumerate(axes):\n    if not i<len(train_df.columns):\n        break\n    # Get data and columns\n    column = train_df.columns[i]\n    distribution = train_df.loc[:, column].values\n    x = np.linspace(0, 1, 100)\n    \n    # Get KDE\n    kde = gaussian_kde(distribution)(x)\n    ax.fill_between(x, kde, alpha=0.3, color=\"b\",linestyle=\"--\")\n    ax.set_title(f\"distribution for columns: {column}\")\n    ax.set_ylabel(\"kde\")\n\nfig.suptitle(\"Feature distributions\")\nfig.tight_layout()\nfig.show()","100a71d2":"fig = plt.figure(figsize=(10,10))\nax = fig.subplots(1)\n\nx = np.linspace(0, 1, 100)\ndistribution = train_df.loc[:, \"target\"]\nkde = gaussian_kde(distribution)(x)\n\nax.fill_between(x, kde, alpha=0.5, color=\"b\")\n\nax.set_title(\"Target distribution\")\nax.set_ylabel(\"kde\")\nax.set_xlabel(\"x\")\n\nfig.tight_layout()\nfig.show()","34622854":"X,y = train_df.drop(\"target\", axis=1).values, train_df.loc[:, \"target\"]","0099b094":"reg = linear_model.LinearRegression()\nreg.fit(X,y)","20cc1a41":"sample_submission = pd.read_csv(sample_file, index_col='id')\npreds = reg.predict(test_df.values)\nsample_submission.loc[:, \"target\"] = preds","89e7fb1d":"sample_file_stem = sample_file.name\nsample_submission.to_csv(out_filename,index=False)","a9c4fe6f":"From the visual above, we can safely infer that the feature cont8 onwards are highly correlated with other features. These features are expendible and can be discarded. But, we will keep them for our primitive model and drop them when necessary.","ce6a8990":"## Target distribution\nNow we do the same thing for our target","96304be6":"First we will see the correlation between features. i.e the redundancy in features","7cfd1a40":"## Train a model","49dda019":"## Make predictions","b8916e61":"## Feature distributions\n\nVisualising feature distribtions","0700c697":"### Visualise the data"}}