{"cell_type":{"62c671c2":"code","d7d03866":"code","8ccb7a9c":"code","f24ba0c1":"code","2627aba9":"code","f4ab19bb":"code","3f881d4b":"code","ef14b594":"code","1a52bbcf":"code","20eeb7fe":"code","71b9cd65":"code","adb6920e":"code","a3a6f9e0":"code","f3001047":"code","ae1ca49e":"code","a4c4bc70":"code","f4ebb190":"code","f6317f3a":"code","d248ee18":"code","a3c26bdd":"code","dde4f522":"code","1f64e8e5":"code","6057cb1e":"code","7e3fc432":"code","71cfe9d6":"code","6e0ca8c8":"code","523b44a4":"code","e766d98d":"code","9e2d1040":"code","049737d0":"code","e08cad85":"code","2d0eef2e":"code","3f54cf1f":"code","96b49f77":"code","88cfffaf":"code","65dc646b":"code","e4739f63":"code","947101f1":"code","35ce6632":"code","b7528e08":"code","050efb32":"code","a5886067":"code","4e275e6b":"code","19cb7949":"code","44673dec":"code","5e0d32b7":"code","411d3396":"code","cc130a5b":"code","acdf94fe":"code","f62fa548":"code","fe77bdd4":"code","be654918":"code","84326a8c":"code","a2947a39":"code","854d95aa":"code","5f918bde":"code","f6a76e97":"code","fdaddd31":"code","b9e3d66b":"code","04410a94":"code","58e996b5":"code","c4a34d5f":"code","6610f5c9":"code","38ef3772":"code","3910a32b":"code","9cc84225":"code","c2df5620":"code","5e48681b":"code","354eb75a":"code","92c0af38":"code","67769034":"code","4a5f38e3":"code","e8d724b4":"code","2ba55733":"code","0f960bcc":"code","6602a6ab":"code","acfc98f4":"code","693d898d":"markdown","ef8d0ddb":"markdown","308fff7a":"markdown","f263a3c1":"markdown","c1159358":"markdown","79751565":"markdown","3380bb69":"markdown","66f09009":"markdown","3da49ace":"markdown","64d783a5":"markdown","30dde4f0":"markdown","09546e77":"markdown","31bfd626":"markdown","48272bba":"markdown","221641f5":"markdown","2843f6ff":"markdown","b31f6239":"markdown","0282f7da":"markdown","a9482703":"markdown","56768fc8":"markdown","ff7eea6d":"markdown","c668c175":"markdown","71214dd1":"markdown","6471019e":"markdown","7b3693f8":"markdown","c76f1873":"markdown","6234fb0b":"markdown","75d12949":"markdown","9f0a80b0":"markdown","bed7d2ae":"markdown","b9c21d28":"markdown","3a2391ad":"markdown","75a922f2":"markdown","5a9492bb":"markdown","f42ee54f":"markdown","e2311688":"markdown","b02c57bc":"markdown","5b147365":"markdown","781d1533":"markdown","37c4f039":"markdown","b0de8cd7":"markdown","6e4afc06":"markdown","26999516":"markdown","2f0dbba9":"markdown","b761309b":"markdown","0e1571d4":"markdown"},"source":{"62c671c2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nplt.style.use('ggplot')","d7d03866":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom scipy.stats import skew\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.preprocessing import Imputer","8ccb7a9c":"from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom xgboost import XGBRegressor","f24ba0c1":"pd.set_option('max_colwidth',200)\npd.set_option('display.width',200)\npd.set_option('display.max_columns',500)\npd.set_option('display.max_rows',1000)","2627aba9":"#train=pd.read_csv('E:\/Workspace\/HousePrices\/train.csv')\n#test=pd.read_csv('E:\/Workspace\/HousePrices\/test.csv')","f4ab19bb":" train = pd.read_csv('..\/input\/train.csv')\n test = pd.read_csv('..\/input\/test.csv')","3f881d4b":"plt.figure(figsize=(15,8))\nsns.boxplot(train.YearBuilt, train.SalePrice)","ef14b594":"plt.figure(figsize=(12,6))\nplt.scatter(x=train.GrLivArea, y=train.SalePrice)\nplt.xlabel(\"GrLivArea\", fontsize=13)\nplt.ylabel(\"SalePrice\", fontsize=13)\nplt.ylim(0,800000)","1a52bbcf":"train.drop(train[(train[\"GrLivArea\"]>4000)&(train[\"SalePrice\"]<300000)].index,inplace=True)","20eeb7fe":"full=pd.concat([train,test], ignore_index=True)","71b9cd65":"full.drop(['Id'],axis=1, inplace=True)\nfull.shape","adb6920e":"aa = full.isnull().sum()\naa[aa>0].sort_values(ascending=False)","a3a6f9e0":"full.groupby(['Neighborhood'])[['LotFrontage']].agg(['mean','median','count'])","f3001047":"full[\"LotAreaCut\"] = pd.qcut(full.LotArea,10)","ae1ca49e":"full.groupby(['LotAreaCut'])[['LotFrontage']].agg(['mean','median','count'])","a4c4bc70":"full['LotFrontage']=full.groupby(['LotAreaCut','Neighborhood'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))","f4ebb190":"# Since some combinations of LotArea and Neighborhood are not available, so we just LotAreaCut alone.\nfull['LotFrontage']=full.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))","f6317f3a":"cols=[\"MasVnrArea\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"GarageCars\", \"BsmtFinSF2\", \"BsmtFinSF1\", \"GarageArea\"]\nfor col in cols:\n    full[col].fillna(0, inplace=True)","d248ee18":"cols1 = [\"PoolQC\" , \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"GarageFinish\", \"GarageYrBlt\", \"GarageType\", \"BsmtExposure\", \"BsmtCond\", \"BsmtQual\", \"BsmtFinType2\", \"BsmtFinType1\", \"MasVnrType\"]\nfor col in cols1:\n    full[col].fillna(\"None\", inplace=True)","a3c26bdd":"# fill in with mode\ncols2 = [\"MSZoning\", \"BsmtFullBath\", \"BsmtHalfBath\", \"Utilities\", \"Functional\", \"Electrical\", \"KitchenQual\", \"SaleType\",\"Exterior1st\", \"Exterior2nd\"]\nfor col in cols2:\n    full[col].fillna(full[col].mode()[0], inplace=True)","dde4f522":"full.isnull().sum()[full.isnull().sum()>0]","1f64e8e5":"NumStr = [\"MSSubClass\",\"BsmtFullBath\",\"BsmtHalfBath\",\"HalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"MoSold\",\"YrSold\",\"YearBuilt\",\"YearRemodAdd\",\"LowQualFinSF\",\"GarageYrBlt\"]\nfor col in NumStr:\n    full[col]=full[col].astype(str)","6057cb1e":"full.groupby(['MSSubClass'])[['SalePrice']].agg(['mean','median','count'])","7e3fc432":"def map_values():\n    full[\"oMSSubClass\"] = full.MSSubClass.map({'180':1, \n                                        '30':2, '45':2, \n                                        '190':3, '50':3, '90':3, \n                                        '85':4, '40':4, '160':4, \n                                        '70':5, '20':5, '75':5, '80':5, '150':5,\n                                        '120': 6, '60':6})\n    \n    full[\"oMSZoning\"] = full.MSZoning.map({'C (all)':1, 'RH':2, 'RM':2, 'RL':3, 'FV':4})\n    \n    full[\"oNeighborhood\"] = full.Neighborhood.map({'MeadowV':1,\n                                               'IDOTRR':2, 'BrDale':2,\n                                               'OldTown':3, 'Edwards':3, 'BrkSide':3,\n                                               'Sawyer':4, 'Blueste':4, 'SWISU':4, 'NAmes':4,\n                                               'NPkVill':5, 'Mitchel':5,\n                                               'SawyerW':6, 'Gilbert':6, 'NWAmes':6,\n                                               'Blmngtn':7, 'CollgCr':7, 'ClearCr':7, 'Crawfor':7,\n                                               'Veenker':8, 'Somerst':8, 'Timber':8,\n                                               'StoneBr':9,\n                                               'NoRidge':10, 'NridgHt':10})\n    \n    full[\"oCondition1\"] = full.Condition1.map({'Artery':1,\n                                           'Feedr':2, 'RRAe':2,\n                                           'Norm':3, 'RRAn':3,\n                                           'PosN':4, 'RRNe':4,\n                                           'PosA':5 ,'RRNn':5})\n    \n    full[\"oBldgType\"] = full.BldgType.map({'2fmCon':1, 'Duplex':1, 'Twnhs':1, '1Fam':2, 'TwnhsE':2})\n    \n    full[\"oHouseStyle\"] = full.HouseStyle.map({'1.5Unf':1, \n                                           '1.5Fin':2, '2.5Unf':2, 'SFoyer':2, \n                                           '1Story':3, 'SLvl':3,\n                                           '2Story':4, '2.5Fin':4})\n    \n    full[\"oExterior1st\"] = full.Exterior1st.map({'BrkComm':1,\n                                             'AsphShn':2, 'CBlock':2, 'AsbShng':2,\n                                             'WdShing':3, 'Wd Sdng':3, 'MetalSd':3, 'Stucco':3, 'HdBoard':3,\n                                             'BrkFace':4, 'Plywood':4,\n                                             'VinylSd':5,\n                                             'CemntBd':6,\n                                             'Stone':7, 'ImStucc':7})\n    \n    full[\"oMasVnrType\"] = full.MasVnrType.map({'BrkCmn':1, 'None':1, 'BrkFace':2, 'Stone':3})\n    \n    full[\"oExterQual\"] = full.ExterQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n    \n    full[\"oFoundation\"] = full.Foundation.map({'Slab':1, \n                                           'BrkTil':2, 'CBlock':2, 'Stone':2,\n                                           'Wood':3, 'PConc':4})\n    \n    full[\"oBsmtQual\"] = full.BsmtQual.map({'Fa':2, 'None':1, 'TA':3, 'Gd':4, 'Ex':5})\n    \n    full[\"oBsmtExposure\"] = full.BsmtExposure.map({'None':1, 'No':2, 'Av':3, 'Mn':3, 'Gd':4})\n    \n    full[\"oHeating\"] = full.Heating.map({'Floor':1, 'Grav':1, 'Wall':2, 'OthW':3, 'GasW':4, 'GasA':5})\n    \n    full[\"oHeatingQC\"] = full.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n    \n    full[\"oKitchenQual\"] = full.KitchenQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n    \n    full[\"oFunctional\"] = full.Functional.map({'Maj2':1, 'Maj1':2, 'Min1':2, 'Min2':2, 'Mod':2, 'Sev':2, 'Typ':3})\n    \n    full[\"oFireplaceQu\"] = full.FireplaceQu.map({'None':1, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n    \n    full[\"oGarageType\"] = full.GarageType.map({'CarPort':1, 'None':1,\n                                           'Detchd':2,\n                                           '2Types':3, 'Basment':3,\n                                           'Attchd':4, 'BuiltIn':5})\n    \n    full[\"oGarageFinish\"] = full.GarageFinish.map({'None':1, 'Unf':2, 'RFn':3, 'Fin':4})\n    \n    full[\"oPavedDrive\"] = full.PavedDrive.map({'N':1, 'P':2, 'Y':3})\n    \n    full[\"oSaleType\"] = full.SaleType.map({'COD':1, 'ConLD':1, 'ConLI':1, 'ConLw':1, 'Oth':1, 'WD':1,\n                                       'CWD':2, 'Con':3, 'New':3})\n    \n    full[\"oSaleCondition\"] = full.SaleCondition.map({'AdjLand':1, 'Abnorml':2, 'Alloca':2, 'Family':2, 'Normal':3, 'Partial':4})            \n                \n                        \n                        \n    \n    return \"Done!\"","71cfe9d6":"map_values()","6e0ca8c8":"# drop two unwanted columns\nfull.drop(\"LotAreaCut\",axis=1,inplace=True)\nfull.drop(['SalePrice'],axis=1,inplace=True)","523b44a4":"class labelenc(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        lab=LabelEncoder()\n        X[\"YearBuilt\"] = lab.fit_transform(X[\"YearBuilt\"])\n        X[\"YearRemodAdd\"] = lab.fit_transform(X[\"YearRemodAdd\"])\n        X[\"GarageYrBlt\"] = lab.fit_transform(X[\"GarageYrBlt\"])\n        return X","e766d98d":"class skew_dummies(BaseEstimator, TransformerMixin):\n    def __init__(self,skew=0.5):\n        self.skew = skew\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        X_numeric=X.select_dtypes(exclude=[\"object\"])\n        skewness = X_numeric.apply(lambda x: skew(x))\n        skewness_features = skewness[abs(skewness) >= self.skew].index\n        X[skewness_features] = np.log1p(X[skewness_features])\n        X = pd.get_dummies(X)\n        return X","9e2d1040":"# build pipeline\npipe = Pipeline([\n    ('labenc', labelenc()),\n    ('skew_dummies', skew_dummies(skew=1)),\n    ])","049737d0":"# save the original data for later use\nfull2 = full.copy()","e08cad85":"data_pipe = pipe.fit_transform(full2)","2d0eef2e":"data_pipe.shape","3f54cf1f":"data_pipe.head()","96b49f77":"scaler = RobustScaler()","88cfffaf":"n_train=train.shape[0]\n\nX = data_pipe[:n_train]\ntest_X = data_pipe[n_train:]\ny= train.SalePrice\n\nX_scaled = scaler.fit(X).transform(X)\ny_log = np.log(train.SalePrice)\ntest_X_scaled = scaler.transform(test_X)","65dc646b":"lasso=Lasso(alpha=0.001)\nlasso.fit(X_scaled,y_log)","e4739f63":"FI_lasso = pd.DataFrame({\"Feature Importance\":lasso.coef_}, index=data_pipe.columns)","947101f1":"FI_lasso.sort_values(\"Feature Importance\",ascending=False)","35ce6632":"FI_lasso[FI_lasso[\"Feature Importance\"]!=0].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(15,25))\nplt.xticks(rotation=90)\nplt.show()","b7528e08":"class add_feature(BaseEstimator, TransformerMixin):\n    def __init__(self,additional=1):\n        self.additional = additional\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        if self.additional==1:\n            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n            \n        else:\n            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n            \n            X[\"+_TotalHouse_OverallQual\"] = X[\"TotalHouse\"] * X[\"OverallQual\"]\n            X[\"+_GrLivArea_OverallQual\"] = X[\"GrLivArea\"] * X[\"OverallQual\"]\n            X[\"+_oMSZoning_TotalHouse\"] = X[\"oMSZoning\"] * X[\"TotalHouse\"]\n            X[\"+_oMSZoning_OverallQual\"] = X[\"oMSZoning\"] + X[\"OverallQual\"]\n            X[\"+_oMSZoning_YearBuilt\"] = X[\"oMSZoning\"] + X[\"YearBuilt\"]\n            X[\"+_oNeighborhood_TotalHouse\"] = X[\"oNeighborhood\"] * X[\"TotalHouse\"]\n            X[\"+_oNeighborhood_OverallQual\"] = X[\"oNeighborhood\"] + X[\"OverallQual\"]\n            X[\"+_oNeighborhood_YearBuilt\"] = X[\"oNeighborhood\"] + X[\"YearBuilt\"]\n            X[\"+_BsmtFinSF1_OverallQual\"] = X[\"BsmtFinSF1\"] * X[\"OverallQual\"]\n            \n            X[\"-_oFunctional_TotalHouse\"] = X[\"oFunctional\"] * X[\"TotalHouse\"]\n            X[\"-_oFunctional_OverallQual\"] = X[\"oFunctional\"] + X[\"OverallQual\"]\n            X[\"-_LotArea_OverallQual\"] = X[\"LotArea\"] * X[\"OverallQual\"]\n            X[\"-_TotalHouse_LotArea\"] = X[\"TotalHouse\"] + X[\"LotArea\"]\n            X[\"-_oCondition1_TotalHouse\"] = X[\"oCondition1\"] * X[\"TotalHouse\"]\n            X[\"-_oCondition1_OverallQual\"] = X[\"oCondition1\"] + X[\"OverallQual\"]\n            \n           \n            X[\"Bsmt\"] = X[\"BsmtFinSF1\"] + X[\"BsmtFinSF2\"] + X[\"BsmtUnfSF\"]\n            X[\"Rooms\"] = X[\"FullBath\"]+X[\"TotRmsAbvGrd\"]\n            X[\"PorchArea\"] = X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n            X[\"TotalPlace\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"] + X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n\n    \n            return X","050efb32":"pipe = Pipeline([\n    ('labenc', labelenc()),\n    ('add_feature', add_feature(additional=2)),\n    ('skew_dummies', skew_dummies(skew=1)),\n    ])","a5886067":"full_pipe = pipe.fit_transform(full)","4e275e6b":"full_pipe.shape","19cb7949":"n_train=train.shape[0]\nX = full_pipe[:n_train]\ntest_X = full_pipe[n_train:]\ny= train.SalePrice\n\nX_scaled = scaler.fit(X).transform(X)\ny_log = np.log(train.SalePrice)\ntest_X_scaled = scaler.transform(test_X)","44673dec":"pca = PCA(n_components=410)","5e0d32b7":"X_scaled=pca.fit_transform(X_scaled)\ntest_X_scaled = pca.transform(test_X_scaled)","411d3396":"X_scaled.shape, test_X_scaled.shape","cc130a5b":"# define cross validation strategy\ndef rmse_cv(model,X,y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n    return rmse","acdf94fe":"models = [LinearRegression(),Ridge(),Lasso(alpha=0.01,max_iter=10000),RandomForestRegressor(),GradientBoostingRegressor(),SVR(),LinearSVR(),\n          ElasticNet(alpha=0.001,max_iter=10000),SGDRegressor(max_iter=1000,tol=1e-3),BayesianRidge(),KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5),\n          ExtraTreesRegressor(),XGBRegressor()]","f62fa548":"names = [\"LR\", \"Ridge\", \"Lasso\", \"RF\", \"GBR\", \"SVR\", \"LinSVR\", \"Ela\",\"SGD\",\"Bay\",\"Ker\",\"Extra\",\"Xgb\"]\nfor name, model in zip(names, models):\n    score = rmse_cv(model, X_scaled, y_log)\n    print(\"{}: {:.6f}, {:.4f}\".format(name,score.mean(),score.std()))","fe77bdd4":"class grid():\n    def __init__(self,model):\n        self.model = model\n    \n    def grid_get(self,X,y,param_grid):\n        grid_search = GridSearchCV(self.model,param_grid,cv=5, scoring=\"neg_mean_squared_error\")\n        grid_search.fit(X,y)\n        print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])","be654918":"grid(Lasso()).grid_get(X_scaled,y_log,{'alpha': [0.0004,0.0005,0.0007,0.0006,0.0009,0.0008],'max_iter':[10000]})","84326a8c":"grid(Ridge()).grid_get(X_scaled,y_log,{'alpha':[35,40,45,50,55,60,65,70,80,90]})","a2947a39":"grid(SVR()).grid_get(X_scaled,y_log,{'C':[11,12,13,14,15],'kernel':[\"rbf\"],\"gamma\":[0.0003,0.0004],\"epsilon\":[0.008,0.009]})","854d95aa":"param_grid={'alpha':[0.2,0.3,0.4,0.5], 'kernel':[\"polynomial\"], 'degree':[3],'coef0':[0.8,1,1.2]}\ngrid(KernelRidge()).grid_get(X_scaled,y_log,param_grid)","5f918bde":"grid(ElasticNet()).grid_get(X_scaled,y_log,{'alpha':[0.0005,0.0008,0.004,0.005],'l1_ratio':[0.08,0.1,0.3,0.5,0.7],'max_iter':[10000]})","f6a76e97":"class AverageWeight(BaseEstimator, RegressorMixin):\n    def __init__(self,mod,weight):\n        self.mod = mod\n        self.weight = weight\n        \n    def fit(self,X,y):\n        self.models_ = [clone(x) for x in self.mod]\n        for model in self.models_:\n            model.fit(X,y)\n        return self\n    \n    def predict(self,X):\n        w = list()\n        pred = np.array([model.predict(X) for model in self.models_])\n        # for every data point, single model prediction times weight, then add them together\n        for data in range(pred.shape[1]):\n            single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]\n            w.append(np.sum(single))\n        return w","fdaddd31":"lasso = Lasso(alpha=0.0005,max_iter=10000)\nridge = Ridge(alpha=60)\nsvr = SVR(gamma= 0.0004,kernel='rbf',C=13,epsilon=0.009)\nker = KernelRidge(alpha=0.2 ,kernel='polynomial',degree=3 , coef0=0.8)\nela = ElasticNet(alpha=0.005,l1_ratio=0.08,max_iter=10000)\nbay = BayesianRidge()","b9e3d66b":"# assign weights based on their gridsearch score\nw1 = 0.02\nw2 = 0.2\nw3 = 0.25\nw4 = 0.3\nw5 = 0.03\nw6 = 0.2","04410a94":"weight_avg = AverageWeight(mod = [lasso,ridge,svr,ker,ela,bay],weight=[w1,w2,w3,w4,w5,w6])","58e996b5":"rmse_cv(weight_avg,X_scaled,y_log),  rmse_cv(weight_avg,X_scaled,y_log).mean()","c4a34d5f":"weight_avg = AverageWeight(mod = [svr,ker],weight=[0.5,0.5])","6610f5c9":"rmse_cv(weight_avg,X_scaled,y_log),  rmse_cv(weight_avg,X_scaled,y_log).mean()","38ef3772":"class stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self,mod,meta_model):\n        self.mod = mod\n        self.meta_model = meta_model\n        self.kf = KFold(n_splits=5, random_state=42, shuffle=True)\n        \n    def fit(self,X,y):\n        self.saved_model = [list() for i in self.mod]\n        oof_train = np.zeros((X.shape[0], len(self.mod)))\n        \n        for i,model in enumerate(self.mod):\n            for train_index, val_index in self.kf.split(X,y):\n                renew_model = clone(model)\n                renew_model.fit(X[train_index], y[train_index])\n                self.saved_model[i].append(renew_model)\n                oof_train[val_index,i] = renew_model.predict(X[val_index])\n        \n        self.meta_model.fit(oof_train,y)\n        return self\n    \n    def predict(self,X):\n        whole_test = np.column_stack([np.column_stack(model.predict(X) for model in single_model).mean(axis=1) \n                                      for single_model in self.saved_model]) \n        return self.meta_model.predict(whole_test)\n    \n    def get_oof(self,X,y,test_X):\n        oof = np.zeros((X.shape[0],len(self.mod)))\n        test_single = np.zeros((test_X.shape[0],5))\n        test_mean = np.zeros((test_X.shape[0],len(self.mod)))\n        for i,model in enumerate(self.mod):\n            for j, (train_index,val_index) in enumerate(self.kf.split(X,y)):\n                clone_model = clone(model)\n                clone_model.fit(X[train_index],y[train_index])\n                oof[val_index,i] = clone_model.predict(X[val_index])\n                test_single[:,j] = clone_model.predict(test_X)\n            test_mean[:,i] = test_single.mean(axis=1)\n        return oof, test_mean","3910a32b":"# must do imputer first, otherwise stacking won't work, and i don't know why.\na = Imputer().fit_transform(X_scaled)\nb = Imputer().fit_transform(y_log.values.reshape(-1,1)).ravel()","9cc84225":"stack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)","c2df5620":"print(rmse_cv(stack_model,a,b))\nprint(rmse_cv(stack_model,a,b).mean())","5e48681b":"X_train_stack, X_test_stack = stack_model.get_oof(a,b,test_X_scaled)","354eb75a":"X_train_stack.shape, a.shape","92c0af38":"X_train_add = np.hstack((a,X_train_stack))","67769034":"X_test_add = np.hstack((test_X_scaled,X_test_stack))","4a5f38e3":"X_train_add.shape, X_test_add.shape","e8d724b4":"print(rmse_cv(stack_model,X_train_add,b))\nprint(rmse_cv(stack_model,X_train_add,b).mean())","2ba55733":"# This is the final model I use\nstack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)","0f960bcc":"stack_model.fit(a,b)","6602a6ab":"pred = np.exp(stack_model.predict(test_X_scaled))","acfc98f4":"result=pd.DataFrame({'Id':test.Id, 'SalePrice':pred})\nresult.to_csv(\"submission.csv\",index=False)","693d898d":"+ __Based on the \"Feature Importance\" plot and other try-and-error, I decided to add some features to the pipeline.__","ef8d0ddb":"### Ridge","308fff7a":"+ __Next we can build a pipeline. It's convenient to experiment different feature combinations once you've got a pipeline.__","f263a3c1":"+ __It seems that the price of recent-built houses are higher. So later I 'll use labelencoder for three \"Year\" feature.__","c1159358":"+ __Im my case, doing PCA is very important. It lets me gain a relatively big boost on leaderboard. At first I don't believe PCA can help me, but \nin retrospect, maybe the reason is that the features I built are highly correlated, and it leads to multicollinearity. PCA can decorrelate these features.__","79751565":"__1. Exploratory Visualization__  \n__2. Data Cleaning__  \n__3. Feature Engineering__  \n__4. Modeling & Evaluation__  \n__5. Ensemble Methods__  ","3380bb69":"+ __Let's first try it out ! It's a bit slow to run this method, since the process is quite compliated. __","66f09009":"## Stacking","3da49ace":"### Weight Average","64d783a5":"+ __Then we filling in other missing values according to data_description.__","30dde4f0":"# Content","09546e77":"# Modeling & Evaluation","31bfd626":"+ __Different people may have different views on how to map these values, so just follow your instinct =^_^=__  \n__Below I also add a small \"o\" in front of the features so as to keep the original features to use get_dummies in a moment.__","48272bba":"## Pipeline","221641f5":"### Submission","2843f6ff":"+ __Next we do some hyperparameters tuning. First define a gridsearch method.__","b31f6239":"### Kernel Ridge","0282f7da":"### ElasticNet","a9482703":"# Data Cleaning","56768fc8":"+ __So basically I'll do__  \n                '180' : 1\n                '30' : 2   '45' : 2\n                '190' : 3, '50' : 3, '90' : 3,\n                '85' : 4, '40' : 4, '160' : 4\n                '70' : 5, '20' : 5, '75' : 5, '80' : 5, '150' : 5\n                '120': 6, '60' : 6","ff7eea6d":"# Ensemble Methods ","c668c175":"### SVR","71214dd1":"+ __But if we average only two best models, we gain better cross-validation score.__","6471019e":"+ __I have to confess, the feature engineering above is not enough, so we need more.__   \n+ __Combining different features is usually a good way, but we have no idea what features should we choose. Luckily there are some models that can provide feature selection, here I use Lasso, but you are free to choose Ridge, RandomForest or GradientBoostingTree.__","7b3693f8":"+ __You can even do parameter tuning for your meta model after you get \"X_train_stack\", or do it after combining with the original features. but that's a lot of work too !__","c76f1873":"+ __Convert some numerical features into categorical features. It's better to use LabelEncoder and get_dummies for these features.__","6234fb0b":"+ __As is discussed in other kernels, the bottom right two two points with extremely large GrLivArea are likely to be outliers. So we delete them.__","75d12949":"### Missing Data","9f0a80b0":"+ __Next we extract the features generated from stacking, then combine them with original features.__","bed7d2ae":"# Feature Engineering","b9c21d28":"## PCA","3a2391ad":"# Exploratory Visualization","75a922f2":"+ __Average base models according to their weights.__","5a9492bb":"+ __So I'll use approximately the same dimension in PCA as  in the original data. Since the aim here is not deminsion reduction.__","f42ee54f":"+ __use robustscaler since maybe there are other outliers.__","e2311688":"+ __We choose 13 models and use 5-folds cross-calidation to evaluate these models.__","b02c57bc":"## Feature Selection","5b147365":"+ __And there is no missing data except for the value we want to predict !__","781d1533":"### Lasso","37c4f039":"+ __Aside from normal stacking, I also add the \"get_oof\" method, because later I'll combine features generated from stacking and original features.__","b0de8cd7":"+ __Apply log1p to the skewed features, then get_dummies.__","6e4afc06":"+ __Let's first imput the missing values of LotFrontage based on the median of LotArea and Neighborhood. Since LotArea is a continuous feature, We use qcut to divide it into 10 parts.__","26999516":"+ __Now I want to do a long list of value-mapping. __\n+ __I was influenced by the insight that we should build as many features as possible and trust the model to choose the right features. So I decided to groupby SalePrice according to one feature and sort it based on mean and median. Here is an example:__","2f0dbba9":"Models include:\n\n+ LinearRegression\n+ Ridge\n+ Lasso\n+ Random Forrest\n+ Gradient Boosting Tree\n+ Support Vector Regression\n+ Linear Support Vector Regression\n+ ElasticNet\n+ Stochastic Gradient Descent\n+ BayesianRidge\n+ KernelRidge\n+ ExtraTreesRegressor\n+ XgBoost","b761309b":"+ __By using a pipeline, you can quickily experiment different feature combinations.__","0e1571d4":"+ __Label Encoding three \"Year\" features.__"}}