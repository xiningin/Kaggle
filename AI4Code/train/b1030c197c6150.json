{"cell_type":{"b560c51a":"code","c88ef492":"code","8f6a54f9":"code","62d99cfb":"code","142da159":"code","c97f7768":"code","86fc8f37":"code","61152f40":"code","e59d18d9":"code","ae828022":"code","c643ac37":"code","9bfcfbb0":"code","d7aedf2b":"code","3db82f19":"code","2481c680":"code","bc00b689":"code","bb752c94":"markdown","9b395645":"markdown","947edad1":"markdown","eb91fcaa":"markdown","85368006":"markdown","ea3f44fc":"markdown","b122a4a0":"markdown","eefc5db8":"markdown","427d80ba":"markdown","3eb87b97":"markdown","476ddaf0":"markdown","166526cc":"markdown","b8b695b4":"markdown","baf7e3c3":"markdown","b4162707":"markdown","804b289c":"markdown","9e2ef532":"markdown","a80c67e3":"markdown","c02963ba":"markdown","4f655960":"markdown","32d5e182":"markdown","e04ccfa6":"markdown","209e1e14":"markdown"},"source":{"b560c51a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c88ef492":"from sklearn.preprocessing import StandardScaler\n\n## numpy\n## pandas \n## matplotlib (pyplot)\n## seaborn\n\n%matplotlib inline \n\nimport os\nimport warnings\n\nwarnings.filterwarnings('ignore')","8f6a54f9":"df.rename(index=str, columns=____________, inplace=True)\ndf.head()","62d99cfb":"sns.____________(df.drop('CustomerID', axis=1), hue='____________', aspect=1.5)\nplt.show()","142da159":"X = df.drop(['____________', '____________'], axis=1)","c97f7768":"## Kmeans \u00e0 importer\n\nclusters = []\n\nfor i in range(1, ____________):\n    km = KMeans(____________).fit(X)\n    clusters.append(km.inertia_)\n    \nfig, ax = plt.subplots(figsize=(12, 8))\nsns.lineplot(x=list(range(1, 11)), y=clusters, ax=ax)\nax.set_title('Searching for Elbow')\nax.set_xlabel('Clusters')\nax.set_ylabel('Inertia')\n\n# Annotate arrow\nax.annotate('Possible Elbow Point', xy=(3, 140000), xytext=(3, 50000), xycoords='data',          \n             arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n\nax.annotate('Possible Elbow Point', xy=(5, 80000), xytext=(5, 150000), xycoords='data',          \n             arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n\nplt.show()","86fc8f37":"# 3 cluster\nkm3 = ____________\n\nX['Labels'] = ____________\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['____________'], X['____________'], hue=X['____________'], \n                palette=sns.color_palette('hls', 3))\nplt.title('KMeans with 3 Clusters')\nplt.show()","61152f40":"# Let's see with 5 Clusters\nkm5 = ____________\n\nX['Labels'] = ____________\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['____________'], X['____________'], hue=X['____________'], \n                palette=sns.color_palette('hls', 5))\nplt.title('KMeans with 5 Clusters')\nplt.show()","e59d18d9":"fig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(121)\nsns.____________(x='____________', y='____________', data=X, ax=ax)\nax.set_title('Labels According to Annual Income')\n\nax = fig.add_subplot(122)\nsns.____________(x='____________', y='____________', data=X, ax=ax)\nax.set_title('Labels According to Scoring History')\n\nplt.show()","ae828022":"## Importer AgglomerativeClustering \n\nagglom = ____________\n\nX['Labels'] = ____________\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['____________'], X['____________'], hue=X['____________'], \n                palette=sns.color_palette('hls', 5))\nplt.title('Agglomerative with 5 Clusters')\nplt.show()","c643ac37":"from scipy.cluster import hierarchy \nfrom scipy.spatial import distance_matrix \n\ndist = ____________\nprint(dist)","9bfcfbb0":"Z = hierarchy.linkage(____________)","d7aedf2b":"plt.figure(figsize=(18, 50))\ndendro = hierarchy.dendrogram(____________, leaf_rotation=0, leaf_font_size=12, orientation='right')","3db82f19":"# Importer DBSCAN \n\ndb = ____________\n\nX['Labels'] = ____________\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['____________'], X['____________'], hue=X['____________'], \n                palette=sns.color_palette('hls', np.unique(db.labels_).shape[0]))\nplt.title('DBSCAN with epsilon 11, min samples 6')\nplt.show()\n","2481c680":"fig = plt.figure(figsize=(20,15))\n\n##### KMeans #####\nax = fig.add_subplot(221)\n\nkm5 = ____________\nX['Labels'] = ____________\nsns.scatterplot(X['Income'], X['____________'], hue=X['____________'], style=X['____________'],\n                palette=sns.color_palette('hls', 5), s=60, ax=ax)\nax.set_title('KMeans with 5 Clusters')\n\n\n##### Agglomerative Clustering #####\nax = fig.add_subplot(222)\n\nagglom = ____________\nX['Labels'] = ____________\nsns.scatterplot(X['Income'], X['____________'], hue=X['____________'], style=X['____________'],\n                palette=sns.color_palette('hls', 5), s=60, ax=ax)\nax.set_title('Agglomerative with 5 Clusters')\n\n\n##### DBSCAN #####\nax = fig.add_subplot(223)\n\ndb = ____________\nX['Labels'] = ____________\nsns.scatterplot(X['____________'], X['____________'], hue=X['____________'], style=X['____________'], s=60,\n                palette=sns.color_palette('hls', np.unique(db.labels_).shape[0]), ax=ax)\nax.set_title('DBSCAN with epsilon 11, min samples 6')\n\nplt.tight_layout()\nplt.show()","bc00b689":"from sklearn import metrics\n\nprint(\"Silhouette Coefficient Kmeans: %0.3f\" % ____________(X, ____________, metric='sqeuclidean'))\nprint(\"Silhouette Coefficient Agglomerative: %0.3f\" % ____________(X, ____________, metric='sqeuclidean'))\nprint(\"Silhouette Coefficient DBSCAN: %0.3f\" % ____________(X, ____________, metric='sqeuclidean'))","bb752c94":"Un regroupement hi\u00e9rarchique est g\u00e9n\u00e9ralement visualis\u00e9 sous la forme d'un dendrogramme, comme le montre la cellule suivante. Chaque fusion est repr\u00e9sent\u00e9e par une ligne horizontale. La coordonn\u00e9e y de la ligne horizontale est la similitude des deux groupes qui ont \u00e9t\u00e9 fusionn\u00e9s. En remontant de la couche inf\u00e9rieure vers le n\u0153ud sup\u00e9rieur, un dendrogramme nous permet de reconstruire l'histoire des fusions qui ont abouti \u00e0 la mise en cluster repr\u00e9sent\u00e9e. ","9b395645":"# Question 10:\n- Afficherle dendogram de Z","947edad1":"# Question 11:\n- Importer DBSCAN\n- Utiliser les param\u00e8tres eps = 11 et min_samples = 6 puis fitt\u00e9 l'algorithme\n- Affihcer le scatterplot pour le croisement \"Income\", \"Score\" avec les labels des clusters\n- Analyse les r\u00e9sultats du DBSCAN","eb91fcaa":"R\u00e9ponse: \nNous pouvons maintenant analyser en d\u00e9tail nos 5 groupes :\n- L'\u00e9tiquette 0 correspond aux revenus \u00e9lev\u00e9s et d\u00e9penses faibles\n- L'\u00e9tiquette 1 correspond aux revenus moyens et d\u00e9penses moyennes\n- L'\u00e9tiquette 2 correspond aux revenu faibles et d\u00e9penses \u00e9lev\u00e9s\n- L'\u00e9tiquette 3 correspond aux revenus \u00e9lev\u00e9s et d\u00e9penses \u00e9lev\u00e9s\n- L'\u00e9tiquette 4 correspond aux revenu faibles et d\u00e9penses faibles","85368006":"# Question 7:\n- Importer AgglomerativeClustering de sklearn. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.AgglomerativeClustering.html\n- Instancier la m\u00e9thode avec les param\u00e8tres n_clusters = 5\n- Afficher avec le scatterplot la relation \"Income\" et \"Score\" avec les labels associ\u00e9s ('Labels' - hue)","ea3f44fc":"# Question 8:\n- Importer hierarchy de scipy - https:\/\/docs.scipy.org\/doc\/scipy\/reference\/cluster.hierarchy.html\n- Importer distance_matrix de scipy - https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.spatial.distance_matrix.html\n- Utiliser distance_matrix pour calculer la matrice des distances en les individus de X\n- Affihcer la matrice et v\u00e9rifier que vous avez des 0 sur la diagonale","b122a4a0":"\u00c0 en juger par les graphiques, on pourrait dire que 5 grappes semblent meilleures que les 3. Comme il s'agit d'un probl\u00e8me non supervis\u00e9, nous ne pouvons pas vraiment savoir avec certitude lequel est le meilleur dans la vie r\u00e9elle, mais en examinant les donn\u00e9es, on peut dire sans risque de se tromper que 5 est un bon choix. ","eefc5db8":"## Hierarchical Clustering\n\n## Agglomerative\nNous nous pencherons sur une technique de clustering, \u00e0 savoir la mise en cluster hi\u00e9rarchique agglom\u00e9rative. L'agglom\u00e9ration est l'approche ascendante qui est plus populaire que le regroupement par division. Nous utiliserons \u00e9galement le lien complet comme crit\u00e8re de liaison. \n\nLa classe de regroupement agglom\u00e9ratif n\u00e9cessitera deux entr\u00e9es :\n- n_clusters : Le nombre de grappes \u00e0 former ainsi que le nombre de centro\u00efdes \u00e0 g\u00e9n\u00e9rer. \n- lien : Quel crit\u00e8re de liaison \u00e0 utiliser. Le crit\u00e8re de lien d\u00e9termine la distance \u00e0 utiliser entre les ensembles d'observation. L'algorithme fusionnera les paires de grappes qui minimisent ce crit\u00e8re.","427d80ba":"# Question 1:\n- Importer numpy, pandas, matplotlib.pyplot, seaborn\n- Importer le fichier Mall_customers.csv et le stocker dans df\n- Afficher le nombre de lignes et de colonnes\n- Afficher les statistiques des variables\n- Afficher le typage des variables\n- Afficher les 10 premi\u00e8re lignes de df","3eb87b97":"### Dendrogram pour Agglomerative Hierarchical Clustering\n\n![](https:\/\/miro.medium.com\/max\/2556\/1*r1YriAFjwJokgcdtaodQAQ.png)\n\nN'oubliez pas qu'une matrice de distance contient la distance entre chaque point et chaque autre point d'un ensemble de donn\u00e9es. Nous pouvons utiliser la fonction distance_matrix, qui n\u00e9cessite deux entr\u00e9es. N'oubliez pas que les valeurs de distance sont sym\u00e9triques, avec une diagonale de 0. C'est une fa\u00e7on de s'assurer que votre matrice est correcte.","476ddaf0":"# Algorithmes de clustering - Non supervis\u00e9\n\nDans l'apprentissage automatique, les types d'apprentissage peuvent \u00eatre class\u00e9s en trois grandes cat\u00e9gories : \n\n1. L'apprentissage supervis\u00e9 \n2. L'apprentissage non supervis\u00e9 \n3. L'apprentissage semi-supervis\u00e9  \n\nLes algorithmes appartenant \u00e0 la famille de l'apprentissage non supervis\u00e9 n'ont aucune variable \u00e0 pr\u00e9dire li\u00e9e aux donn\u00e9es. Au lieu d'avoir une sortie, les donn\u00e9es n'ont qu'une entr\u00e9e qui serait constitu\u00e9e de plusieurs variables qui d\u00e9crivent les donn\u00e9es. C'est l\u00e0 qu'intervient le regroupement.\n\nLa mise en cluster consiste \u00e0 regrouper un ensemble d'objets de telle sorte que les objets d'un m\u00eame cluster se ressemblent davantage les uns les autres qu'avec les objets d'autres clusters. La similarit\u00e9 est une mesure qui refl\u00e8te la force de la relation entre deux objets de donn\u00e9es. Le regroupement est principalement utilis\u00e9 pour l'exploration de donn\u00e9es. Elle est utilis\u00e9e dans de nombreux domaines tels que le Machine Learning, la reconnaissance de formes, l'analyse d'images, la recherche d'informations, la bio-informatique, la compression de donn\u00e9es et l'infographie.","166526cc":"# Question 4:\n- Instancier la m\u00e9thode avec les param\u00e8tres n_clusters = 3\n- Lancer un Kmeans avec K = 3\n- Afficher le scatter plot croisant \"Income\" et \"Score\" avec pour label (hue) \"Labels\"","b8b695b4":"# Question 13:\n\nLe coefficient de silhouette est une m\u00e9trique usuelle pour \u00e9valuer la performance du clustering lorsqu'on ne conna\u00eet pas les vrais clusters.\nLe coefficient de silhouette est calcul\u00e9 pour chaque \u00e9chantillon : \n\n$$SC = \\frac{b-a} {max(a, b)}$$\n\n- a est la distance moyenne \u00e0 tous les points du m\u00eame cluster\n- b est la distance moyenne \u00e0 tous les autres points du cluster le plus proche.\n\nLa silhouette prend des valeurs de -1 (pire performance) \u00e0 +1 (meilleure performance). Le score global est la moyenne de la silhouette.\n\nle coefficient de silhouette est une mesure de qualit\u00e9 d'une partition d'un ensemble de donn\u00e9es en clustering. Pour chaque point, son coefficient de silhouette est la diff\u00e9rence entre la distance moyenne avec les points du m\u00eame groupe que lui (coh\u00e9sion) et la distance moyenne avec le points des autres groupes voisins (s\u00e9paration). Si cette diff\u00e9rence est n\u00e9gative, le point est en moyenne plus proche du groupe voisin que du sien : il est donc mal class\u00e9. A l'inverse, si cette diff\u00e9rence est positive, le point est en moyenne plus proche de son groupe que du groupe voisin : il est donc bien class\u00e9.\n\nLe coefficient de silhouette proprement dit est la moyenne du coefficient de silhouette pour tous les points.\n\n- Importer metrics de sklearn\n- Calculer la silhouette pour chaque algorithmes\n- Lequel choisirez vous?","baf7e3c3":"## Density Based Clustering (DBSCAN)\n\n![](https:\/\/miro.medium.com\/max\/1128\/1*ejlV2WryiH4zGFP_KohEeA.png)\n\nLa plupart des techniques de regroupement traditionnelles, telles que les k-means, le regroupement hi\u00e9rarchique, peuvent \u00eatre utilis\u00e9es pour regrouper des donn\u00e9es sans supervision. \n\nCependant, lorsqu'elles sont appliqu\u00e9es \u00e0 des t\u00e2ches avec des clusters de forme arbitraire, ou des clusters \u00e0 l'int\u00e9rieur d'un cluster, les techniques traditionnelles peuvent ne pas permettre d'obtenir de bons r\u00e9sultats. En d'autres termes, les \u00e9l\u00e9ments d'un m\u00eame groupe peuvent ne pas \u00eatre suffisamment similaires ou les performances peuvent \u00eatre m\u00e9diocres. En outre, le clustering bas\u00e9 sur la densit\u00e9 localise les r\u00e9gions de haute densit\u00e9 qui sont s\u00e9par\u00e9es les unes des autres par des r\u00e9gions de faible densit\u00e9. La densit\u00e9, dans ce contexte, est d\u00e9finie comme le nombre de points dans un rayon donn\u00e9.\n\nDans cette partie, l'accent sera mis sur la manipulation des donn\u00e9es et des propri\u00e9t\u00e9s de DBSCAN et sur l'observation du clustering qui en r\u00e9sulte. De plus Kmeans force la cr\u00e9ation d'un nombre K de clusters. Ainsi tout les individus seront assign\u00e9s \u00e0 un regroupement, m\u00eame les outliers... DBSCAN prend en compte ce type d'indivdus et va les isoler dans un autre cluster.\n\n### Modeling\nDBSCAN est l'acronyme de Density-Based Spatial Clustering of Applications with Noise. Cette technique est l'un des algorithmes de mise en cluster les plus courants qui fonctionne sur la base de la densit\u00e9 de l'objet. L'id\u00e9e g\u00e9n\u00e9rale est que si un point particulier appartient \u00e0 une cluster, il doit \u00eatre proche de nombreux autres points de ce cluster.\n\nIl fonctionne sur la base de deux param\u00e8tres : Epsilon et points minimums  \n- Epsilon : D\u00e9termine un rayon pr\u00e9cis qui, s'il comprend un nombre suffisant de points, est appel\u00e9 zone dense  \n- minimumSamples : D\u00e9termine le nombre minimum de points de donn\u00e9es que nous voulons dans un quartier pour d\u00e9finir un cluster.","b4162707":"# Question 9:\n- Utiliser hierarchy.linkage sur la matrice des distances calcul\u00e9es pr\u00e9c\u00e9demment\n- Stocker dans Z","804b289c":"# Question 5:\n- Instancier la m\u00e9thode avec les param\u00e8tres n_clusters = 5\n- Lancer un Kmeans avec K = 5\n- Afficher le scatter plot croisant \"Income\" et \"Score\" avec pour label (hue) \"Labels\"\n- Utiliser random_state = 42 afin d'avoir tout le temps les m\u00eames r\u00e9sultats","9e2ef532":"# Question 6:\nNous avons identifier des groupes cependant il faut analyser les diff\u00e9rences entre ces groupes afin de pouvoir donner un nom plus explicite que \"Cluster N\". Pour cela nous allons r\u00e9aliser des graphiques. Il est possible de comparer nos populations avec des tests statistiques afin de juger de la significativit\u00e9 des diff\u00e9rences entre clusters.\n\n- Afficher le swarmplot entre \"Labels\" et \"Income\"\n- Afficher le swarmplot entre \"Labels\" et \"Score\"\n- Analyser les graphiques afin de trouver des noms \u00e0 nos clusters","a80c67e3":"## K-Means \n\nIl existe de nombreux mod\u00e8les de **clustering**. Nous allons passer en revue les plus populaires. Malgr\u00e9 sa simplicit\u00e9, le **K-means** est largement utilis\u00e9 pour le clustering dans de nombreuses applications de Dara Science, particuli\u00e8rement utile si vous avez besoin de d\u00e9couvrir rapidement des aper\u00e7us \u00e0 partir de donn\u00e9es **non \u00e9tiquet\u00e9es**. Dans ce notebook, nous voyons comment utiliser le k-Means pour la segmentation de la client\u00e8le.","c02963ba":"# Question 2:\n- Renommer les variables \"Annual Income (k$)\" en 'Income' et 'Spending Score (1-100)' en 'Score'. Utiliser .rename() de pandas pour cela. https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.rename.html\n- Afficher un pairplit selon le genre (Homme - Femme) https:\/\/seaborn.pydata.org\/generated\/seaborn.pairplot.html\n- Que peut on dire sur la variable gender? Semble t'elle pertinente pour notre segmentation?\n- Supprmer les deux variables \u00e0 ne pas utiliser","4f655960":"# Question 12:\n\n- Lancer les 3 algorithmes avec les param\u00e8tres d\u00e9fini pr\u00e9c\u00e9demment\n- Pour chaque algorithmes afficher le graphique croisant \"Income\", \"Score\" avec les labels associ\u00e9s","32d5e182":"La m\u00e9thode du coude nous dit de s\u00e9lectionner le cluster lorsqu'il y a un changement significatif de l'inertie. Comme on peut le voir sur le graphique, on peut dire que cela peut \u00eatre 3 ou 5. Voyons les deux r\u00e9sultats dans le graphique et d\u00e9cidons.\n\nPrincipe :Une strat\u00e9gie simple pour identifier le nombre de classes consiste \u00e0 faire varier K et surveiller l\u2019\u00e9volution de l\u2019inertie intra-classes W. L\u2019id\u00e9e est de visualiser le \u00abcoude\u00bb o\u00f9 l\u2019adjonction d\u2019une classe ne correspond \u00e0 rien dans la structuration des donn\u00e9es.\n\nPour la suite nous choisirons K = 3.","e04ccfa6":"R\u00e9ponse:","209e1e14":"# Question 3:\n![](https:\/\/dendroid.sk\/wp-content\/uploads\/2013\/01\/kmeansimg-scaled1000.jpg)\n\n- Importer Kmeans de sklearn. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.KMeans.html\n- Nous voulons r\u00e9aliser plusieurs Kmeans afin d'utiliser la r\u00e8gle du coude afin de d\u00e9finir le nombre de cluster. R\u00e9aliser 10 kmeans."}}