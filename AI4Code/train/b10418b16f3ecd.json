{"cell_type":{"a8b2a942":"code","4ea30acb":"code","f728f9a2":"code","7003f6c0":"code","bc9cff44":"code","f005225f":"code","d41f731a":"code","76e0be7a":"code","0a00a2a0":"code","e12fc44c":"code","8b298e74":"code","67739d1b":"code","74b58a8a":"code","7e2a9398":"code","bb82fbe3":"code","fe58433e":"code","e3490ff1":"code","607cfff4":"code","ceb9752d":"code","2b4cbe87":"code","4e4831fa":"code","eafd5f0b":"code","96ccef8c":"code","0ae42668":"code","fb672716":"code","6843c26e":"code","eebafd7e":"code","2486233c":"code","d9e2edb5":"code","64788670":"code","53c4da4e":"code","c2f6c1cb":"code","9c22b14f":"code","bcc095a3":"code","3b831314":"code","b15c5977":"code","ac7c8d26":"code","d0c06575":"code","f7e61390":"code","8a9dc96b":"markdown","fadfb024":"markdown","adfc69f9":"markdown","e0cc9c63":"markdown","a9a16641":"markdown","cb70f1bd":"markdown","82fac1f6":"markdown","56696d96":"markdown","ba26394d":"markdown","cb8d56d9":"markdown","f3f75337":"markdown","b392af85":"markdown","94750a53":"markdown","1f6b9791":"markdown","a0f869a1":"markdown","e045315f":"markdown","8a942e4d":"markdown","a21a0e3a":"markdown","d2879614":"markdown","2911721c":"markdown","d682ed49":"markdown","e825dfa6":"markdown","03d9fc98":"markdown","24e6df47":"markdown"},"source":{"a8b2a942":"# import necessary modules\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport os\nimport warnings\n#from datetime import datetime\nfrom scipy import stats\nfrom scipy.stats import norm, skew, probplot \n\nwarnings.filterwarnings('ignore')","4ea30acb":"dftrain = pd.read_csv('..\/input\/covid19-global-forecasting-week-2\/train.csv', parse_dates=['Date']).sort_values(by=['Country_Region', 'Date'])\ndftest = pd.read_csv('..\/input\/covid19-global-forecasting-week-2\/test.csv', parse_dates=['Date']).sort_values(by=['Country_Region', 'Date'])\ndftrain.head()","f728f9a2":"confirmed = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_confirmed_global.csv').sort_values(by='Country\/Region')#.set_index('Country\/Region')\ndeaths = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_deaths_global.csv')#.set_index('Country\/Region')\nrecovered = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_recovered_global.csv')#.set_index('Country\/Region')","7003f6c0":"confirmed['Country_Region'] = confirmed['Country\/Region']\nconfirmed['Province_State'] = confirmed['Province\/State']\nconfirmed.head()","bc9cff44":"def transpose_df(df):\n    df = df.drop(['Lat','Long'],axis=1).groupby('Country\/Region').sum().T\n    df.index = pd.to_datetime(df.index)#.date\n    return df","f005225f":"confirmedT = transpose_df(confirmed)\ndeathsT = transpose_df(deaths)\nrecoveredT = transpose_df(recovered)\nmortalityT = deathsT\/confirmedT","d41f731a":"dftrain = dftrain.join(confirmed[['Country_Region', 'Province_State', 'Lat', 'Long']].set_index(['Province_State', 'Country_Region']), on=['Province_State', 'Country_Region'])#, how='outer')#.set_index(['Province_State', 'Country_Region']))","76e0be7a":"def add_day(df):\n    df['Date'] = df.index\n    df['Dayofyear'] = df['Date'].dt.dayofyear\n    return df","0a00a2a0":"dftrain['Dayofyear'] = dftrain['Date'].dt.dayofyear\ndftest['Dayofyear'] = dftest['Date'].dt.dayofyear","e12fc44c":"allcountries = dftrain['Country_Region'].unique().tolist()","8b298e74":"from math import radians, cos, sin, asin, sqrt\n\ndef haversine(lat1, lon1, lat2, lon2):\n    \"\"\"\n    Calculate the great circle distance between two points \n    on the earth (specified in decimal degrees)\n    \"\"\"\n    # convert decimal degrees to radians \n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n\n    # haversine formula \n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    a = sin(dlat\/2)**2 + cos(lat1) * cos(lat2) * sin(dlon\/2)**2\n    c = 2 * asin(sqrt(a)) \n    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n    return c * r","67739d1b":"list_countries = dftrain[dftrain['Date'] == '2020-01-22']['Country_Region'].tolist()\nlist_states = dftrain[dftrain['Date'] == '2020-01-22']['Province_State'].tolist()\n#\ndf_distance = pd.DataFrame(index=[list_countries, list_states],columns=[list_countries, list_states])\ndf_distance.index.names = ['Country_Region', 'Province_State']\ndf_distance.columns.names = ['Country_Region', 'Province_State']\ndf_distance['Lat'] = dftrain[dftrain['Date'] == '2020-01-22']['Lat'].tolist()\ndf_distance['Long']  = dftrain[dftrain['Date'] == '2020-01-22']['Long'].tolist()\n#\nfor country, state in list(zip(list_countries, list_states)):\n    lat, lon = df_distance.loc[(country, state),['Lat', 'Long']]\n    df_distance[(country, state)] = df_distance[['Lat', 'Long']].apply(lambda x: haversine(lat, lon, x[0], x[1]), axis=1)\n#\ndf_distance.head()","74b58a8a":"from datetime import datetime\nlockdown = confirmedT.copy()\nlockdown.loc[:,:] = 0\ncountry_lockdown = pd.DataFrame({\n                    'Argentina' : datetime(2020,3,19), # 2020-03-19\n                    'Australia' : datetime(2020,3,23), # 2020-03-23\n                    'Austria' : datetime(2020,3,16), # 2020-03-16\n                    'Belgium' : datetime(2020,3,18), # 2020-03-18\n                    'Colombia' : datetime(2020,3,25), # 2020-03-25\n                    'Czechia' : datetime(2020,3,16), # 2020-03-16\n                    'Denmark' : datetime(2020,3,11), # 2020-03-11\n                    'El Salvador' : datetime(2020,3,12), # 2020-03-12\n                    'Fiji' : datetime(2020,3,9), # 2020-03-20\n                    'France' : datetime(2020,3,17), # 2020-03-17\n                    'Greece' : datetime(2020,3,23), # 2020-03-23\n                    'Honduras' : datetime(2020,3,17), # 2020-03-17\n                    'Ireland' : datetime(2020,3,12), # 2020-03-12\n                    'Italy' : datetime(2020,3,9), # 2020-03-09\n                    'Lebanon' : datetime(2020,3,15), # 2020-03-15\n                    'Lithuania' : datetime(2020,3,16), # 2020-03-16\n                    'Malaysia' : datetime(2020,3,18), # 2020-03-18\n                    'Morocco' : datetime(2020,3,19), # 2020-03-19\n                    'Philippines' : datetime(2020,3,15), # 2020-03-15\n                    'Poland' : datetime(2020,3,13), # 2020-03-13\n                    'Romania' : datetime(2020,3,25), # 2020-03-25\n                    'South Africa' : datetime(2020,3,26), # 2020-03-26\n                    'Spain' : datetime(2020,3,14), # 2020-03-14\n                    'Tunisia' : datetime(2020,3,22), # 2020-03-22\n                    'United Kingdom' : datetime(2020,3,23), # 2020-03-23\n                    'Venezuela' : datetime(2020,3,17), # 2020-03-17} \n                    },index=['Start Lockdown Country']).T\nstate_lockdown = pd.DataFrame({\n                    ('US', 'California') : datetime(2020,3,19), # 2020-03-19\n                    ('US', 'Nevada') : datetime(2020,3,20), # 2020-03-20\n                    ('US', 'Connecticut') : datetime(2020,3,23), # 2020-03-23\n                    ('US', 'Illinois') : datetime(2020,3,21), # 2020-03-21\n                    ('US', 'Massachusetts') : datetime(2020,3,24), # 2020-03-24\n                    ('US', 'Michigan') : datetime(2020,3,24), # 2020-03-24\n                    ('US', 'New York') : datetime(2020,3,20), # 2020-03-20\n                    ('US', 'Oregon') : datetime(2020,3,24), # 2020-03-24\n                   },index=['Start Lockdown State']).T\nstate_lockdown.head()","7e2a9398":"dftrainlockdown = dftrain.join(country_lockdown, on='Country_Region').join(state_lockdown, on=['Country_Region', 'Province_State'])\ndftrainlockdown['Lockdown'] = 0\ndftrainlockdown.loc[dftrainlockdown['Date'] > dftrainlockdown['Start Lockdown Country'],'Lockdown'] = 1\ndftrainlockdown.loc[dftrainlockdown['Date'] > dftrainlockdown['Start Lockdown State'],'Lockdown'] = 1","bb82fbe3":"pop = pd.read_csv('..\/input\/population-by-country-2020\/population_by_country_2020.csv').set_index('Country (or dependency)')\npop.sort_values(by='Country (or dependency)').head()","fe58433e":"flights = pd.read_csv('..\/input\/datacountries\/API_IS.AIR.PSGR_DS2_en_csv_v2_887266.csv').set_index('Country Name')['2018']\nflights = flights.rename('FlightPassengers_2018')","e3490ff1":"dftrainall = dftrain.join(pop, on='Country_Region')\ndftrainall['Lockdown'] = dftrainlockdown['Lockdown']\ndftrainall = dftrainall.join(flights, on='Country_Region')\n#\n#\ndftrainall['Mortality'] = dftrainall['Fatalities']\/dftrainall['ConfirmedCases']\ndftrainall['ConfirmedCases_by_pop'] = dftrainall['ConfirmedCases']\/dftrainall['Population (2020)']\ndftrainall['ConfirmedCases_by_Km\u00b2'] = dftrainall['ConfirmedCases']\/dftrainall['Land Area (Km\u00b2)']\n#\ndftrainall.tail()","607cfff4":"allcountries_ordered = confirmed.set_index(['Country_Region']).iloc[:,-2].sort_values(ascending=False).index.tolist()","ceb9752d":"confirmedT, deathsT, recoveredT, mortalityT = add_day(confirmedT), add_day(deathsT), add_day(recoveredT), add_day(mortalityT)","2b4cbe87":"def df_day1(df, confirmed):\n    df_day1 = pd.DataFrame({'Days since 100 cases' : np.arange(1000)}).set_index('Days since 100 cases')\n    countries_df = df.columns.tolist()[:-2]\n    countries_conf = confirmed.columns.tolist()[:-2]\n    #print(len(countries_df), len(confirmed.columns.tolist()[:-2]))\n    for ic, country in enumerate(countries_df):\n        for ic2, country2 in enumerate(countries_conf):\n            if country == country2:\n                dfsub = df[confirmed[country] > 100.][country]\n                df_day1[country] = np.nan\n                df_day1.loc[:len(dfsub)-1,country] = (dfsub).tolist()\n        #try:\n        #except:\n        #    pass\n    df_day1 = df_day1.dropna(how='all')\n    #df_day1 = df_day1.fillna(0.)\n    return df_day1\n","4e4831fa":"confirmed_day1 = df_day1(confirmedT, confirmedT)\ndeaths_day1 = df_day1(deathsT, confirmedT)\nrecovered_day1 = df_day1(recoveredT, confirmedT)\nmortality_day1 = df_day1(mortalityT, confirmedT)\nconfirmednorm_day1 = confirmed_day1\/confirmed_day1.loc[0,:]\nmaxday = confirmed_day1.shape[0]","eafd5f0b":"date_day1 = confirmedT.copy()\nfor column in date_day1:\n    date_day1[column] = confirmedT.index.tolist()\ndate_day1 = df_day1(date_day1, confirmedT)\ndate_day1.head()","96ccef8c":"plt.figure(figsize=(15,10))\nplt.subplots_adjust(wspace=0.2, hspace=0.2)\n#\nylabels = ['Cumulative confirmed cases', 'Number of deceased', 'New cases per day',' Deceased per day']\nys = [[confirmedT.sum(axis=1),confirmedT.drop('China',axis=1).sum(axis=1)], \n    [deathsT.sum(axis=1),deathsT.drop('China',axis=1).sum(axis=1)],\n    [confirmedT.sum(axis=1).diff().rolling(2).mean(),confirmedT.drop('China',axis=1).sum(axis=1).diff().rolling(2).mean()],\n    [deathsT.sum(axis=1).diff().rolling(2).mean(),deathsT.drop('China',axis=1).sum(axis=1).diff().rolling(2).mean()],]\nloglin = ['log', 'log', 'linear', 'linear']\nfor iy, y in enumerate(ys):\n    plt.subplot(2,2,1+iy)\n    plt.xticks(rotation=30)\n    plt.xlabel('Date')\n    plt.ylabel(ylabels[iy])\n    plt.yscale(loglin[iy])\n    for y2 in y:\n        plt.plot(y2)\n    plt.legend(['All countries', 'All except China'])\n#\nplt.show()","0ae42668":"plt.figure(figsize=(15,10))\nplt.subplots_adjust(wspace=0.2, hspace=0.2)\n#\nylabels = ['Cumulative confirmed cases', 'Number of deceased', 'New cases per day',' Deceased per day']\nys = [confirmedT[allcountries_ordered[:11]], deathsT[allcountries_ordered[:11]],\n    confirmedT[allcountries_ordered[:11]].diff().rolling(2).mean(), deathsT[allcountries_ordered[:11]].diff().rolling(2).mean()]\nloglin = ['log', 'log', 'linear', 'linear']\nfor iy, y in enumerate(ys):\n    plt.subplot(2,2,1+iy)\n    plt.xticks(rotation=30)\n    plt.xlabel('Date')\n    plt.ylabel(ylabels[iy])\n    plt.yscale(loglin[iy])\n    plt.plot(y)\n    plt.legend(allcountries_ordered[:11])\n#\nplt.show()","fb672716":"plt.figure(0,figsize=[20,10])\nplt.subplots_adjust(wspace=0.17, hspace=0.15)\nmaxday = 35\n#\nylabels = ['Normalised number of cases','Number of deceased','Mortality rate','Percent increase of new cases','New cases per day','Deceased per day']\nys = [confirmednorm_day1, deaths_day1, mortality_day1, confirmed_day1.diff().pct_change().rolling(1).mean()*100., confirmed_day1.diff(), deaths_day1.diff()]\nloglin = ['log', 'log', 'linear', 'linear', 'linear', 'linear']\nfor iy, y in enumerate(ys):\n    plt.subplot(2,3,iy+1)\n    plt.xlabel('Number of days since 100 confirmed cases')\n    plt.ylabel(ylabels[iy])\n    plt.xlim(0,maxday+1)\n    plt.yscale(loglin[iy])\n    plt.plot(y[allcountries_ordered[:7]].rolling(2).mean())\n    plt.legend(allcountries_ordered[:7])\n#plt.show()\nplt.savefig('cases_vs_day1.png',bbox_inches='tight',transparent=False)","6843c26e":"#italy = pd.read_csv('csse_covid_19_data\/csse_covid_19_time_series\/covid19_italy_region.csv').set_index('Date')\n#italy.index = pd.to_datetime(italy.index).date\nlist_regions = ['Total', 'Lombardia', 'Veneto', 'Piemonte', 'Emilia Romagna', 'Toscana', 'Campania', 'Sicilia']\nurlitaly = 'https:\/\/raw.githubusercontent.com\/pcm-dpc\/COVID-19\/master\/dati-regioni\/dpc-covid19-ita-regioni-'\ndateini = datetime(2020,2,24)\ndatenow = datetime.now()\nlist_dates = pd.date_range(dateini, datenow).tolist()\ndateitaly = ['%4s%2s%2s' % (str(date.year), str(date.month).zfill(2), str(date.day).zfill(2)) for date in list_dates[:-1]]\n#\nlist_italy = []\nfor date in dateitaly:\n    italy2 = pd.read_csv(urlitaly+date+'.csv').set_index('data')\n    italy2.head()\n    list_italy.append(italy2)\nitaly = pd.concat(list_italy)\nitaly.index = pd.to_datetime(italy.index).date\nitaly.head(5) ","eebafd7e":"italy_tot = italy.groupby([italy.index]).sum()\nitaly_tot['denominazione_regione'] = 'Total'\nitaly = italy.append(italy_tot)","2486233c":"totalcases = [] ; totaldeaths = [] ; mortality = [] ; percentincrease = [] ; casesperday = [] ; deathsperday = [] \nfor region in list_regions:\n    italy2 = italy[italy['denominazione_regione'] == region]\n    totalcases.append(italy2['totale_casi'])\n    totaldeaths.append(italy2['deceduti'])\n    mortality.append(italy2['deceduti']\/italy2['totale_casi'])\n    percentincrease.append(italy2['totale_casi'].pct_change().rolling(3).mean())\n    casesperday.append(italy2['totale_casi'].diff())\n    deathsperday.append(italy2['deceduti'].diff())","d9e2edb5":"plt.figure(0,figsize=[20,10])\nplt.subplots_adjust(wspace=0.17, hspace=0.2)\n#\nylabels = ['Number of cases','Number of deceased','Mortality rate','Percent increase of new cases','New cases per day','Deceased per day']\nys = [totalcases, totaldeaths, mortality, percentincrease, casesperday, deathsperday]\nloglin = ['log', 'log', 'linear', 'linear', 'log', 'log']\nfor iy, y in enumerate(ys):\n    plt.subplot(2,3,iy+1)\n    plt.xlabel('Date')\n    plt.ylabel(ylabels[iy])\n    if iy == 3:\n        plt.ylim(0,0.8)\n    plt.xticks(rotation=30)\n    #plt.xlim(0,maxday)\n    plt.yscale(loglin[iy])\n    for region in y:\n        plt.plot(region.rolling(2).mean()) #y.rolling(2).mean())\n    plt.legend(list_regions)\n#plt.show()\nplt.savefig('cases_vs_time_italy.png',bbox_inches='tight',transparent=False)","64788670":"def create_lags(df, maxshift):\n    # Shifts\n    shifts = np.arange(1,maxshift+1)\n    # Create a dictionary of time-shifted data\n    many_shifts = {'lag_{}'.format(ii): df.shift(ii) for ii in shifts}\n    # Convert them into a dataframe\n    many_shifts = pd.DataFrame(many_shifts).fillna(0.)\n    return many_shifts","53c4da4e":"# Accuracy metrics\nfrom statsmodels.tsa.stattools import acf\ndef forecast_accuracy(forecast, actual):\n    rmsle = np.sqrt(np.mean((np.log(forecast+1)-np.log(actual+1))**2))\n    mape = np.mean(np.abs(forecast - actual)\/np.abs(actual))  # Mean Absolute Percentage Error\n    me = np.mean(forecast - actual)             # ME\n    mae = np.mean(np.abs(forecast - actual))    # MAE\n    mpe = np.mean((forecast - actual)\/actual)   # MPE\n    rmse = np.mean((forecast - actual)**2)**.5  # RMSE\n    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n    mins = np.amin(np.hstack([forecast[:,None], \n                              actual[:,None]]), axis=1)\n    maxs = np.amax(np.hstack([forecast[:,None], \n                              actual[:,None]]), axis=1)\n    minmax = 1 - np.mean(mins\/maxs)             # minmax\n    #acf1 = acf(fc-test)[1]                      # ACF1\n    return({'mape':mape, 'me':me, 'mae': mae, 'rmsle' : rmsle,\n            'mpe': mpe, 'rmse':rmse, #'acf1':acf1, \n            'corr':corr, 'minmax':minmax})","c2f6c1cb":"import xgboost as xgb\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import ElasticNet\n#\n# define list of models and parameters\nlist_models = [('ElasticNet', ElasticNet()),\n                ('ElasticNet_wdiff', ElasticNet()),\n                ('ElasticNet_wdiffwrolling', ElasticNet()),\n              ]\n#\nlist_params = [{'alpha' : np.logspace(-4,2,10),\n                'l1_ratio' : np.array([0.6,0.7,0.8,0.9,1.])},\n                {'alpha' : np.logspace(-4,2,10),\n                'l1_ratio' : np.array([0.6,0.7,0.8,0.9,1.])},\n                {'alpha' : np.logspace(-4,2,10),\n                'l1_ratio' : np.array([0.6,0.7,0.8,0.9,1.])},\n              ]","9c22b14f":"order_countries = date_day1.loc[0,:].sort_values().index.tolist()#[:11]\norder_countries.remove('Diamond Princess')\norder_countries = order_countries[:11]","bcc095a3":"import xgboost as xgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n#\nNlags = 5\nlist_inpdate = ['Dayofyear']\nlist_rescv = ['params', 'mean_test_score', 'std_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score']\n#\nlist_train_fc = [] ; list_valid_fc = [] ; list_train = [] ; list_valid = []\nlist_confirmed_train_fc = [] ; list_confirmed_valid_fc = [] ; list_confirmed_train = [] ; list_confirmed_valid = []\nlist_bestparams = [] ; list_bestest = [] ; list_cvres = [] ; list_inpfeat = [] ; list_error = []\ndf_error = pd.DataFrame(index=order_countries)\n#\nfor im, model in enumerate(list_models): \n    list_bestparams2 = [] ; list_bestest2 = [] ; list_cvres2 = [] ; list_inpfeat2 = []\n    list_train_fc2 = [] ; list_valid_fc2 = [] ; list_train2 = [] ; list_valid2 = []\n    list_confirmed_train_fc2 = [] ; list_confirmed_valid_fc2 = [] ; list_confirmed_train2 = [] ; list_confirmed_valid2 = []\n    list_error2 = [] ; list_index = []\n    for ic, country in enumerate(order_countries): \n        print('Analyzing '+country+' with '+model[0])\n        #\n        # here the daily number of confirmed cases is used to make the TS more stationary, \n        # a rolling mean of 2 is also used to smooth the curves since some data are not updated on a daily basis\n        inpfeature = list_inpdate + ['lag_'+str(i+1) for i in range(Nlags)] # + list_inpfeat \n        #\n        if im == 0: # model directly the total number of confirmed cases\n            dfcountry = confirmedT[[country, 'Dayofyear']].replace([np.inf, -np.inf], np.nan).dropna() \n        elif im == 1: # model the daily number of new confirmed cases\n            dfcountry = confirmedT[[country, 'Dayofyear']].diff().rolling(1).mean().replace([np.inf, -np.inf], np.nan).dropna() \n        elif im == 2: # model the daily number of new confirmed cases averaged over a week (to smooth the data)\n            dfcountry = confirmedT[[country, 'Dayofyear']].diff().rolling(7).mean().replace([np.inf, -np.inf], np.nan).dropna() \n        #\n        dflag = create_lags(dfcountry[country], Nlags)\n        masktrain =  dfcountry.index <= datetime(2020,3,15)\n        datetrain, datevalid = dfcountry[masktrain].index[0], dfcountry[masktrain].index[-1] \n        dfconfirmedcountry = confirmedT[[country, 'Dayofyear']].replace([np.inf, -np.inf], np.nan).dropna() \n        confirmedstart = dfconfirmedcountry.loc[datetrain,country]\n        confirmedend = dfconfirmedcountry.loc[datevalid,country]\n        #\n        # join all features\n        dftrain2 = dflag[masktrain].join(dfcountry[masktrain])\n        dfvalid2 = dflag[~masktrain].join(dfcountry[~masktrain])\n        if ic > 0: # and im == 1\n            inpfeature += order_countries[:ic]\n            for ic2 in range(ic):\n                dftrain2 = dftrain2.join(list_train2[ic2])\n                dfvalid2 = dfvalid2.join(list_valid2[ic2])\n        #\n        # define training and validation sets\n        X = dftrain2[inpfeature]#.drop(country, axis=1)\n        y = dftrain2[country] \n        X_valid = dfvalid2[inpfeature]#.drop(country, axis=1)\n        y_valid = dfvalid2[country]\n        # \n        # run grid search\n        param_search = list_params[im]\n        tscv = TimeSeriesSplit(n_splits=5)\n        gsearch = GridSearchCV(estimator=model[1],        # choice of model\n                               cv=tscv,                   # choice of splitting\n                               param_grid=param_search,   # grid of parameters\n                               verbose=1,                 # print messages\n                               return_train_score=True,   # return train score in CV grid result\n                               n_jobs=-1,                 # number of CPUs to be used\n                               scoring='neg_mean_squared_error' # metrics to be used\n                              )\n        gsearch.fit(X, y)\n        #\n        # save results\n        list_bestparams2.append(gsearch.best_params_)\n        list_bestest2.append(gsearch.best_estimator_)\n        cvres = pd.DataFrame(gsearch.cv_results_)[list_rescv].sort_values(by='rank_test_score',ascending=True)\n        list_cvres2.append(cvres)\n        #print(cvres.head())\n        print('Best-fit parameters: ',gsearch.best_params_,'\\n \\n')\n        #\n        # get the forecast TS\n        #n_periods = len(valid.index)\n        valid_fc = pd.Series(gsearch.predict(X_valid), index=X_valid.index) #gsearch.predict(X_valid) \n        train_fc = pd.Series(gsearch.predict(X), index=X.index) #gsearch.predict(X)\n        #\n        # save the forecast \n        list_train2.append(y) ; list_valid2.append(y_valid)\n        list_train_fc2.append(train_fc) ; list_valid_fc2.append(valid_fc)\n        list_inpfeat2.append(inpfeature)\n        #\n        # re-compute number of confirmed cases\n        if im == 0: \n            list_confirmed_train2.append(y) ; list_confirmed_valid2.append(y_valid)\n            list_confirmed_train_fc2.append(train_fc) ; list_confirmed_valid_fc2.append(valid_fc)\n        elif im == 1: # \n            list_confirmed_train2.append(y.cumsum()+confirmedstart) ; list_confirmed_valid2.append(y_valid.cumsum()+confirmedend)\n            list_confirmed_train_fc2.append(train_fc.cumsum()+confirmedstart) ; list_confirmed_valid_fc2.append(valid_fc.cumsum()+confirmedend)\n        elif im == 2: \n            #ytrain = (confirmedT[[country, 'Dayofyear']].diff().dropna()).loc[:datetime.datetime(2020,3,15),country]\n            #yvalid = (confirmedT[[country, 'Dayofyear']].diff().dropna()).loc[datetime.datetime(2020,3,15):,country]\n            list_confirmed_train2.append(y.cumsum()+confirmedstart) ; list_confirmed_valid2.append(y_valid.cumsum()+confirmedend)\n            list_confirmed_train_fc2.append(train_fc.cumsum()+confirmedstart) ; list_confirmed_valid_fc2.append(valid_fc.cumsum()+confirmedend)\n        #\n        # evaluate the error\n        list_error2.append(forecast_accuracy(list_confirmed_valid_fc2[-1], list_confirmed_valid2[-1])['rmsle'])\n        #list_index.append(storedept)\n\n    #\n    list_train.append(list_train2) ; list_valid.append(list_valid2)\n    list_train_fc.append(list_train_fc2) ; list_valid_fc.append(list_valid_fc2)\n    list_confirmed_train.append(list_confirmed_train2) ; list_confirmed_valid.append(list_confirmed_valid2)\n    list_confirmed_train_fc.append(list_confirmed_train_fc2) ; list_confirmed_valid_fc.append(list_confirmed_valid_fc2)\n    list_bestparams.append(list_bestparams2) ; list_bestest.append(list_bestest2) ; list_cvres.append(list_cvres2)\n    list_inpfeat.append(list_inpfeat2)\n    #\n    df_error['RMSLE_'+model[0]] = list_error2 ","3b831314":"df_error.head(10)","b15c5977":"# Compare predicted and actual test TS\ndef plot_forecast(country, train, trainfc, valid, validfc, validfclower, validfcupper, plottitle):\n    #fig, axes = plt.subplots(1, 1, figsize=(10,3), dpi=100, sharex=True)\n    plt.xticks(rotation=15)\n    plt.plot(train, label='Training set', color='C0')\n    plt.plot(trainfc, label='Training set forecast', color='C0', ls='--')\n    plt.plot(valid, label='Validation set', color='C1', ls='-')\n    plt.ylabel('Confirmed cases')\n    try:\n        plt.fill_between(validfclower.index, \n                         validfclower, \n                         validfcupper, \n                         color='k', alpha=.15)\n    except:\n        pass\n    plt.plot(validfc, label='Validation set forecast', color='C1', ls='--')\n    plt.legend()\n    plt.title(\"Forecast of \"+str(country)+\" with \"+plottitle)\n    #plt.show()","ac7c8d26":"order_countries2 = order_countries[:11]","d0c06575":"plt.figure(0,figsize=[20,30])\nplt.subplots_adjust(wspace=0.2, hspace=0.45)\niy = 0\nfor ic, country in enumerate(order_countries2): #allcountries_ordered:\n    for im, model in enumerate(list_models): \n        plt.subplot(len(order_countries2),len(list_models), iy+1)\n        train = list_confirmed_train[im][ic] ; valid = list_confirmed_valid[im][ic]\n        train_fc = list_confirmed_train_fc[im][ic] ; valid_fc = list_confirmed_valid_fc[im][ic]\n        plot_forecast(country, train, train_fc, valid, valid_fc, 0., 0., model[0])\n        iy += 1\nplt.show()","f7e61390":"# plot the coefficients derived by the best estimators of each model\nlist_models2 = list(zip([model[0] for model in list_models], list_bestest)) ; model_score = []\nlist_usedfeat = X.columns.tolist()\n#\niy = 0\nfig = plt.figure(0,figsize=[20,30])\nfig.subplots_adjust(wspace=0.20, hspace=0.45)\nfor ic, country in enumerate(order_countries2):\n    for im, model in enumerate(list_models2): \n        ax = fig.add_subplot(len(order_countries2),len(list_models), iy+1)\n        bestest = list_bestest[im][ic]\n        #plt.subplot(len(list_bestest),1,im+1)\n        importances = pd.DataFrame(bestest.coef_, index=list_inpfeat[im][ic], columns=['Importance']).sort_values(by='Importance',ascending=False)\n        ax = sns.barplot(x='Importance', y=importances.index, data=importances)\n        plt.title(\"Coefficients of \"+str(country)+\" with model \"+model[0])\n        iy += 1\nplt.show()\n#plt.savefig('feature_importance.png',bbox_inches='tight',transparent=True)\n#plt.close(0)g('feature_importance.png',bbox_inches='tight',transparent=True)\n#plt.close(0)","8a9dc96b":"#### Create variables of interest.","fadfb024":"## 2.2 COVID-19 evolution in the most affected countries in the world","adfc69f9":"#### Compute the distance between all countries from the Haversine formula using their latitude and longitude.","e0cc9c63":"#### Let's now plot several variables as function of \"Day 1\" of the epidemic in each country, here defined as the day when 100 cases have been confirmed so we can directly compare the trend of all countries of interest. ","a9a16641":"#### Read csv file giving some useful information for all countries.","cb70f1bd":"#### Define the italian regions of interest and read the csv files from the Italian Dipartimento della Protezione Civile on github.","82fac1f6":"#### Create dataframes as function of days from \"Day 1\" of the epidemic, defined here as day when 100 cases have been confirmed in each country","56696d96":"#### Read the csv files on the Johns Hopkins CSSE database on github.","ba26394d":"# 3. ML modelling","cb8d56d9":"#### List the RMSLE error for our different models.","f3f75337":"This notebook is essentially divided in three parts. The first part is devoted to the importation of various sets of data in order to include exogeneous variables for predicting the temporal evolution of the COVID-19 pandemic. The second one consists in EDA by showing how the pandemic evolves in the most affected countries, with an emphasis of Italy, the most affected country. In the last part, I started to perform an auto-regressive linear regression model. Obviously, this work is in progress and will be updated regularly.","b392af85":"## 2.3. COVID-19 in Italy","94750a53":"#### Let's first plot the absolute number of new cases per day and deaths per day. Here, a rolling mean is used in order to smooth the curves as some data are updated every day.","1f6b9791":"#### Create dataframe listing the starting date of lockdowns.","a0f869a1":"# 1. Data Importation","e045315f":"#### Read csv file listing yearly number of airline passengers for each country.","8a942e4d":"## 2.1 Data Importation of the daily-updated John Hopkins dataset","a21a0e3a":"#### Transpose the dataframes","d2879614":"#### Compare our forecast with actual evolution both for training and validation sets.","2911721c":"#### Read the csv files from kaggle.","d682ed49":"#### Let's now plot several variables as function of time. A rolling mean is used in order to smooth the data as some data are not updated every day.","e825dfa6":"Let's now model the temporal evolution of daily new cases for all countries. The approach is the following: \n- we first create an order of countries where the epidemic appears (defined as when 10 days are discovered); \n- for the first country (China), a simple auto-regressive model is used; \n- for other countries, we model the evolution of daily new cases by also taking into account the impact of total cases in countries where the epidemic appears first\n- the impact of the epidemic of other countries is weighted by their distance between countries and the yearly number of flight passengers to include the \"interaction\" between two countries\n- other variables, like lockdown starting day, will be used.\n\nFor now, a simple linear regression model is used to model 1) directly the total number of confirmed cases, 2) the number daily new cases, i.e. confirmed.diff() since this variable should be more stationary and therefore more easily modelled, 3) the number of daily new cases averaged within a year (with rolling(7).mean()) to smooth the data since some data are not updated every day.","03d9fc98":"# 2. EDA ","24e6df47":"#### Join all dataframes."}}