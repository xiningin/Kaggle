{"cell_type":{"3c2dfaed":"code","6b65c9e4":"code","11efdf2b":"code","ea8aa1c1":"code","004a075a":"code","e2687197":"code","68d75128":"code","e8d541f6":"code","3a3f38d1":"code","d8b3321c":"code","1ce260e7":"code","725adce4":"code","b9dd502b":"code","a4260299":"code","5fdac693":"code","92e56ebc":"code","1f0bbf9d":"code","297a20aa":"code","b2e82627":"code","8d5f2f36":"code","1048ce7d":"code","f921e1ff":"code","44612451":"markdown","17fcf646":"markdown","484c8471":"markdown","c0771d10":"markdown","3946b4d8":"markdown","d489aef7":"markdown","5685bf97":"markdown","c5510a8d":"markdown","558568f1":"markdown","6ebba1f8":"markdown","ed459f34":"markdown","dd868f7d":"markdown","e42f0c7c":"markdown","c53e39e1":"markdown","7aac77c6":"markdown","0c580e3d":"markdown","efa54d37":"markdown","1e01e5a9":"markdown"},"source":{"3c2dfaed":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings; warnings.simplefilter('ignore')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","6b65c9e4":"df_train= pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","11efdf2b":"print('The shape of train dataset: ', df_train.shape)\nprint('The shape of test dataset: ', df_test.shape)","ea8aa1c1":"df_train.head()","004a075a":"df_test.head()","e2687197":"df_train.sample(n=10, replace=False, random_state=1) # We will look at 10 random samples","68d75128":"df_train.isnull().sum()","e8d541f6":"df_test.isnull().sum()","3a3f38d1":"sns.set_style('darkgrid')\nsns.countplot(x='target', data= df_train)\nLabels= ('No Disaster', 'Real Disaster')\nplt.xticks(range(2), Labels)","d8b3321c":"df_train[df_train['target']== 1]['keyword'].value_counts().head()","1ce260e7":"df_train['text_length']= df_train['text'].apply(len)\ndf_train.sample(n=5, replace=False, random_state=1)","725adce4":"sns.boxplot(x= 'target', y= 'text_length', data= df_train, palette= 'rainbow')","b9dd502b":"correlation= df_train['target'].corr(df_train['text_length'])\ncorrelation","a4260299":"missing_cols = ['keyword', 'location']\n\nfig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)\n\nsns.barplot(x=df_train[missing_cols].isnull().sum().index, y=df_train[missing_cols].isnull().sum().values, ax=axes[0])\nsns.barplot(x=df_test[missing_cols].isnull().sum().index, y=df_test[missing_cols].isnull().sum().values, ax=axes[1])\n\naxes[0].set_ylabel('Missing Value Count', size=15, labelpad=20)\naxes[0].tick_params(axis='x', labelsize=15)\naxes[0].tick_params(axis='y', labelsize=15)\naxes[1].tick_params(axis='x', labelsize=15)\naxes[1].tick_params(axis='y', labelsize=15)\n\naxes[0].set_title('Training Set', fontsize=13)\naxes[1].set_title('Test Set', fontsize=13)","5fdac693":"def cleaned_tweet(text):\n    import re\n    from nltk.corpus import stopwords\n    from nltk.stem.porter import PorterStemmer\n    tweets = re.sub(\"[^a-zA-Z]\", ' ', text)\n    tweets = tweets.lower()\n    tweets = tweets.split()\n    ps = PorterStemmer()\n    tweets = [ps.stem(word) for word in tweets if not word in set(stopwords.words('english'))]\n    tweets = ' '.join(tweets)\n    return tweets\n","92e56ebc":"df_train['clean_tweet'] = df_train['text'].apply(cleaned_tweet)\ndf_test['clean_tweet'] = df_test['text'].apply(cleaned_tweet)","1f0bbf9d":"from sklearn.feature_extraction.text import CountVectorizer\ncv= CountVectorizer(max_features = 1500)\nX_train_vector= cv.fit_transform(df_train['clean_tweet'])\ny_train= df_train.iloc[:, 4].values\n\n'''note that I'm NOT using .fit_transform() here. Using just .transform() makes sure\n that the tokens in the train vectors are the only ones mapped to the test vectors -\n i.e. that the train and test vectors use the same set of tokens.'''\n\nX_test_vector= cv.transform(df_test['clean_tweet'])","297a20aa":"# Fitting Naive Bayes to the Training set\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()","b2e82627":"from sklearn import model_selection\nscores= model_selection.cross_val_score(classifier, X_train_vector, df_train['target'], cv=5, scoring=\"f1\")\nscores","8d5f2f36":"# Fitting to the train set\nclassifier.fit(X_train_vector, y_train)","1048ce7d":"y_pred = classifier.predict(X_test_vector)","f921e1ff":"submit_df = pd.DataFrame()\nsubmit_df['id'] = df_test['id']\nsubmit_df['target'] = y_pred\n\nsubmit_df.to_csv('submission_final.csv', index=False)","44612451":"Let's see if the length of the tweets have any relation with the disaster. People tend to express their thoughts in the limited space of a tweet. But disasters affect people in maybe shocking ways, when they may want to avoid giving extra informations or want to be careful with the tweet length. So they might use lesser words to express their thoughts! ","17fcf646":"***Predicting the Test set results***","484c8471":"***Fitting The data***","c0771d10":"***Creating the Bag of Words model***","3946b4d8":"Let's do some exploration and visualization of the data. This will help us to gain some valuable insights about the dataset.","d489aef7":"***Importing Usual Libraries***","5685bf97":"***Checking Missing Values***","c5510a8d":"## Data Exploration & Visualization","558568f1":"The competition datasets conatin about 10,000 tweets from twitter. Twitter has always been an important communication portal for people all over the world. People use twitter to talk about real-world incidents\/accidents they\u2019re observing in real-time. We, humans may can easily decide whether a person is tweeting about real disaster or not. But a machine needs some sophisticated model and cleaned data to do that. In this notebook I will try to build a model which will analyze the datasets provided and give verdict if the tweet is depicting a real disaster (1) or not (0). So let's get started.","6ebba1f8":"Yes, it is balanced. And we can also understand that tweets about disaster are less than those about no disaster.Let's see the top 5 frequent disastrous keywords:","ed459f34":"Let's define a function to clean the text data. I will remove all the stopwords that does not contribute in the model to make the classification. I will also take only the stems of the words.  ","dd868f7d":"## Cleaning The Dataset","e42f0c7c":"From above visualization we can say, there is 18% correlation between them, it should have a little effect in predicting. And 0.8% of keyword is missing in both training and test set 33% of location is missing in both training and test set.","c53e39e1":"Well, almost got what I expected. Tweets not related to disasters are more dense around 70 to 130 while disastrous tweets have density from 90 and spanned to almost 140 words. But in general the text length range is shorter in the later case. Let's check the correlation between them.","7aac77c6":"***Checking if the dataset is well balanced***","0c580e3d":"***First Look At Data***","efa54d37":"***Loading Datasets***","1e01e5a9":"Let's test the model and see how well it performs on the train data. For this I need to use cross-validation; where the model will be trained on a portion of the known data and validated with the rest. If this is done several times with different portions (in this case 5 times), it is possible to get a good idea for how a particular model or method performs.\n\nThe metric for this competition is F1, so let's use that here:"}}