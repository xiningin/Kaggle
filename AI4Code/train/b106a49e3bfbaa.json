{"cell_type":{"3a3b2e47":"code","20bd24f5":"code","cba314f8":"code","98f41866":"code","d75169f0":"code","f7bee1d3":"code","cd005bfd":"code","9e69a859":"code","f6af3271":"code","11a06258":"code","81f7a0e2":"code","5747c1ce":"code","b31d7eec":"code","ca87e439":"code","08b83d3f":"code","768a7b6c":"code","092f354d":"code","5ab2da69":"code","e4e9df9b":"code","159942ab":"code","9ee3b8f4":"code","fc044a2e":"code","f92ee5e1":"code","d4999d1c":"code","470c95be":"code","1a0b910e":"code","fae50600":"code","2912af66":"code","963cc694":"code","658e2692":"code","aae34b87":"code","cff4a0fd":"code","4dea7f00":"code","61e0dc64":"code","60cb6082":"code","0fb0f69d":"code","d3f90410":"code","60d14dec":"code","5bb03a4e":"code","6deb0c5e":"code","0bad5e40":"code","ac4a8be0":"code","493ad903":"code","4b43db83":"code","4101689d":"code","c454abb2":"code","a42a78cc":"code","6fcbb947":"code","c4957049":"code","f9d5770c":"markdown","d0fa09c4":"markdown","148f5081":"markdown","8e01481e":"markdown","60825e61":"markdown","19c9274c":"markdown","9306b9ea":"markdown","c5e9fb91":"markdown","6a922319":"markdown","3e99b5bf":"markdown","82687d29":"markdown","89c0ea4b":"markdown","c10e866b":"markdown","0c60c6fb":"markdown","9499dd78":"markdown","cd456079":"markdown","41c37bb2":"markdown","66c89cdc":"markdown","2b63c40b":"markdown","94d5fd04":"markdown","f5ae890d":"markdown"},"source":{"3a3b2e47":"import pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npd.set_option('display.max_colwidth', -1)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom gensim.models import Word2Vec\nfrom nltk.cluster import KMeansClusterer\nimport nltk\nfrom sklearn import cluster\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, roc_curve, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.metrics import confusion_matrix\nimport math","20bd24f5":"load_data = pd.read_csv('..\/input\/lending-club-loan-data\/loan.csv', low_memory=False)","cba314f8":"load_data.head()","98f41866":"load_data.shape","d75169f0":"load_data.isnull().sum()[load_data.columns[load_data.isnull().mean()<0.8]]","f7bee1d3":"load_data = load_data[load_data.columns[load_data.isnull().mean()<0.8]]\nload_data = load_data[~load_data.earliest_cr_line.isnull()]\nload_data = load_data.fillna(0)\n# Remove outliers fro annual_income\n","cd005bfd":"date_issued = pd.to_datetime(load_data['issue_d'])\ndate_last_payment = pd.to_datetime(load_data['last_pymnt_d'])\nload_data['issue_year']=date_issued.dt.year\nload_data['issue_month']=date_issued.dt.month\n\n\n\n","9e69a859":"bad_loan = [\"Charged Off\", \"Default\", \"Does not meet the credit policy. Status:Charged Off\", \"In Grace Period\", \n            \"Late (16-30 days)\", \"Late (31-120 days)\"]\n\n\nload_data['loan_condition'] = np.nan\n\ndef loan_condition(status):\n    if status in bad_loan:\n        return 'Bad Loan'\n    elif status == \"Current\":\n        return 'Current'\n    else:\n        return 'Good Loan'\n    \n    \nloan_status = load_data['loan_status'].apply(loan_condition)\nload_data['loan_condition']=loan_status","f6af3271":"f, ax = plt.subplots(1,2, figsize=(16,8))\n\ncolors = [\"#0B6623\",\"#E1AD01\", \"#D72626\" ]\nlabels =\"Good Loans\",\"Current\", \"Bad Loans\"\n\nplt.suptitle('Information on Loan Conditions', fontsize=20)\n\nload_data[\"loan_condition\"].value_counts().plot.pie(explode=[0,0.1, 0.2], autopct='%1.2f%%', ax=ax[0], shadow=True, colors=colors, \n                                             labels=labels, fontsize=12, startangle=70)\n\n\n# ax[0].set_title('State of Loan', fontsize=16)\nax[0].set_ylabel('% of Condition of Loans', fontsize=14)\n\n\npalette = [\"#E1AD01\",\"#0B6623\", \"#D72626\" ]\n\nsns.countplot(x=\"issue_year\", hue=\"loan_condition\", data=load_data, palette=palette)","11a06258":"def countplot_category_against_loan_condition(column):\n    palette = [\"#E1AD01\",\"#0B6623\", \"#D72626\" ]\n    order = sorted(load_data[column].unique())\n    sns.countplot(x=column, hue=\"loan_condition\", data=load_data, palette=palette, order=order)\n    \ncountplot_category_against_loan_condition('grade')    \n    ","81f7a0e2":"color = [\"#D72626\" ,\"#E1AD01\",\"#0B6623\" ]\nax = load_data.groupby(['issue_year','loan_condition']).int_rate.mean().unstack().plot(title=\"Interest Rate by Load Status\", color=color)\nax.set_ylabel('Interest Rate (%)', fontsize=12)\nax.set_xlabel('Year', fontsize=12)\n","5747c1ce":"house_hold_income = pd.read_csv(\"..\/input\/purchase-power-index\/HouseHold_Income_by_State_by_year.csv\")\nhouse_hold_income= house_hold_income.drop([\"Unnamed: 0\",\"State\"], axis=1)\nhouse_hold_income.head()","b31d7eec":"house_hold_income = house_hold_income.set_index('State_abbr').stack().reset_index().rename(columns ={'level_1':'year',0:'median_household_income'})\nhouse_hold_income['year'] = house_hold_income['year'].astype(int)\nhouse_hold_income.head()","ca87e439":"load_data = pd.merge(house_hold_income,load_data,right_on=['addr_state','issue_year'],left_on=['State_abbr','year'],how='right')\nload_data = load_data.drop([\"State_abbr\",\"year\"], axis=1)","08b83d3f":"ppi = pd.read_csv(\"..\/input\/purchase-power-index\/ppi.csv\")\nppi.head()","768a7b6c":"\nppi = ppi.drop(\"GeoName\",axis=1)\nppi = ppi.set_index('State_abbr').stack().reset_index().rename(columns = {'level_1':'year',0:'ppi'})\nppi['year'] = ppi['year'].astype(int)\nppi.head()","092f354d":"\nload_data = pd.merge(ppi,load_data,right_on=['addr_state','issue_year'],left_on=['State_abbr','year'],how='right')\nload_data = load_data.drop([\"State_abbr\",\"year\"], axis=1)\nload_data['relative_income_index'] = (load_data['annual_inc']\/load_data['ppi']*100) - load_data['median_household_income']\nload_data = load_data.drop([\"ppi\",\"median_household_income\"], axis=1)\n\n","5ab2da69":"def sigmoid(x):\n    if x < -709:\n        return 0.0\n    else:\n        return 1.0 \/ (1.0 + math.exp(-x))\n\n\nmonths_into_loan = 12*(date_last_payment.dt.year-load_data['issue_year']) + (date_last_payment.dt.month-load_data['issue_month'])\npayment_index = (months_into_loan \/ pd.to_numeric(load_data['term'].str.extract('(\\d+)')[0]))\/(load_data['out_prncp']\/load_data['funded_amnt'])\nload_data['payment_index'] = payment_index.fillna(0)\nload_data['payment_index'] = load_data['payment_index'].apply(lambda x: sigmoid(x))\n\n","e4e9df9b":"load_data['emp_length_integer'] = load_data.emp_length.replace({'< 1 year':0,     \n                                                      '1 year':1,\n                                                      '2 years':2,\n                                                      '3 years':3,\n                                                      '4 years':4,\n                                                      '5 years':5,\n                                                      '6 years':6,\n                                                      '7 years':7,\n                                                      '8 years':8,\n                                                      '9 years':9,\n                                                      '10+ years':10,\n                                                      None:-1})\npalette = [\"#E1AD01\",\"#0B6623\", \"#D72626\" ]\ncountplot_category_against_loan_condition('emp_length_integer')\n\"\"\"\nNUM_CLUSTERS = range(3, 20)\nmodel = [cluster.KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1) for i in NUM_CLUSTERS]\nscore = [model[i].fit(X).score(X) for i in range(len(model))]\nplt.plot(NUM_CLUSTERS,score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show()\n\"\"\"\n\n","159942ab":"vectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(load_data.emp_title.replace({0:\"Missing\"}))\nNUM_CLUSTERS = 10\nmodel = KMeans(n_clusters=NUM_CLUSTERS, init='k-means++', max_iter=100, n_init=1)\nmodel.fit(X)\nload_data['emp_title_tfidf'] = model.predict(X)\nsns.countplot(x=\"emp_title_tfidf\",data=load_data)","9ee3b8f4":"load_data['emp_title_tfidf'] = load_data['emp_title_tfidf'].astype('object')","fc044a2e":"def distplot_numberical_feature_across_loan_condition(column):\n    plt.figure()\n    remove_outliers = load_data[~((load_data[column]-load_data[column].mean()).abs() > 3*load_data[column].std())]\n    palette = [\"#0B6623\", \"#D72626\" , \"#E1AD01\"]\n    loan_condition = [\"Good Loan\",\"Bad Loan\",\"Current\"]\n    for i in range(len(loan_condition)):\n        # Subset to the airline\n\n        subset = remove_outliers[remove_outliers['loan_condition'] == loan_condition[i]]\n        # Draw the density plot\n        sns.distplot(subset[column], hist = False, kde = True,\n                     kde_kws = {'linewidth': 1},\n                     label = loan_condition[i],\n                     color = palette[i])\n\n    # Plot formatting\n    plt.legend(prop={'size': 16}, title = 'Loan Condition')\n    plt.title('Density Plot of {}'.format(column))\n    plt.ylabel('Density')\n\n    \n","f92ee5e1":"for numeric_features in load_data.columns[load_data.dtypes!='object'].tolist():\n    distplot_numberical_feature_across_loan_condition(numeric_features)\n","d4999d1c":"cat_columns = [\"term\",\n                \"grade\",\n                \"home_ownership\",\n                \"verification_status\",\n                \"purpose\",\n                \"application_type\",\n                \"hardship_flag\",\n                \"debt_settlement_flag\",\n                \"pymnt_plan\",\n                \"disbursement_method\"]\nfor categorical_features in cat_columns:\n    plt.figure()\n    palette = [\"#0B6623\", \"#D72626\",\"#E1AD01\" ]\n    sns.countplot(x=categorical_features, hue=\"loan_condition\", data=load_data, palette=palette)\n","470c95be":"date_columns = [\"issue_d\",\n                \"issue_year\",\n                \"issue_month\",\n               \"earliest_cr_line\",\n               \"last_pymnt_d\",\n               \"next_pymnt_d\",\n               \"last_credit_pull_d\"]\nonehot_columns = [\"term\",\n                    \"grade\",\n                    \"home_ownership\",\n                    \"verification_status\",\n                    \"purpose\",\n                    \"application_type\"]\nstring_columns =[\"emp_title\",\n                    \"emp_length\",\n                    \"title\",\n                    \"zip_code\",\n                    \"addr_state\",\n                    \"loan_condition\"]\n\nengineered_features =[\"emp_length_integer\",\n                     \"emp_title_tfidf\"]\n\nremove_columns = [\"title\",\n                  \"zip_code\",\n                  \"loan_status\",\n                 \"policy_code\",\n                 \"acc_now_delinq\"]\nremove_columns_time_skewed = [\n                        \"total_rec_prncp\",\n                        \"total_pymnt\",\n                        \"total_rec_int\",\n                        \"total_pymnt_inv\",\n                        \"max_bal_bc\",\n                        \"last_pymnt_amnt\",\n                        \"out_prncp\",\n                        \"out_prncp_inv\",\n                        \"recoveries\",\n                        \"collection_recovery_fee\",\n                        \"total_rec_late_fee\",\n                        \"inq_last_6mths\",\n                        \"mths_since_rcnt_il\",\n                        \"mths_since_recent_bc\",\n                        \"mths_since_recent_bc_dlq\",\n                        \"mths_since_recent_inq\",\n                        \"mths_since_recent_revol_delinq\"\n                 ]\n\n\nremove_one_hot_column = [\"hardship_flag\",\n                        \"debt_settlement_flag\",\n                        \"pymnt_plan\",\n                        \"disbursement_method\"]\n\n\n\n","1a0b910e":"onehot_loan_df = pd.get_dummies(load_data[list(set(onehot_columns)-set(remove_one_hot_column))])\nprint(onehot_loan_df.shape)\nfeatures_df = onehot_loan_df.join(load_data.drop((date_columns+\n                                                  onehot_columns+\n                                                  string_columns+\n                                                  remove_columns+\n                                                 remove_columns_time_skewed),axis=1))\nfeatures_df = features_df.drop(features_df.columns[features_df.dtypes=='object'].tolist(), axis=1)","fae50600":"features_df['loan_status']=load_data['loan_condition']\nfeatures_df.head()","2912af66":"current_loans = features_df[features_df['loan_status']==\"Current\"]\ncurrent_loan_status = features_df[features_df['loan_status']==\"Current\"]['loan_status']","963cc694":"past_loans = features_df[features_df['loan_status']!=\"Current\"]\npast_loan_status = features_df[features_df['loan_status']!=\"Current\"]['loan_status']","658e2692":"past_loan_status = past_loan_status.replace({'Good Loan':1,\n                         'Bad Loan':0})\npast_loans = past_loans.drop(\"loan_status\", axis=1)\ncurrent_loans = current_loans.drop(\"loan_status\", axis=1)","aae34b87":"X_train, X_test, y_train, y_test = train_test_split(past_loans, \n                                                    past_loan_status, \n                                                    test_size=0.20, \n                                                    random_state=42,\n                                                    stratify=past_loan_status)","cff4a0fd":"\"\"\"\nclf = ExtraTreesClassifier(n_estimators=50)\nclf = clf.fit(X_train, y_train)\nclf.feature_importances_  \nmodel = SelectFromModel(clf, prefit=True)\nX_train = model.transform(X_train)\nX_test = model.transform(X_test)\n\"\"\"","4dea7f00":"gb = GradientBoostingClassifier(n_estimators=50,learning_rate=0.1)\ngb.fit(X_train,y_train)\ny_gb_pred = gb.predict(X_test)\nprint(classification_report(y_test,y_gb_pred, target_names=[\"Bad Loan\",\"Good Loan\"]))","61e0dc64":"\"\"\"\ngb_params = {\"n_estimators\":np.arange(40,80,10),\"learning_rate\":np.arange(0.05,0.3,0.05)}\ngrid_gb = GridSearchCV(GradientBoostingClassifier(),gb_params)\ngrid_gb.fit(X_train,y_train)\nprint(classification_report(y_test, grid_gb.predict(X_test), target_names=[\"Lost Sales\",\"Funded Loans\"]))\nprint(grid_gb.best_params_)\n\n\"\"\"","60cb6082":"labels = [\"Good Loan\",\"Bad Loan\"]\ncm = confusion_matrix(y_true = np.array(y_test),\n                      y_pred = pd.DataFrame(gb.predict(X_test)))\nprint(cm)","0fb0f69d":"scaler = StandardScaler()\nX_train_preprocessed_std = scaler.fit_transform(X_train)\nX_test_preprocessed_std = scaler.fit_transform(X_test)\nlr_std = LogisticRegression(class_weight=\"balanced\")\nlr_std.fit(X_train_preprocessed_std,y_train)\ny_lr_pred = lr_std.predict(X_test_preprocessed_std)\nprint(classification_report(y_test,y_lr_pred, target_names=[\"Bad Loan\",\"Good Loan\"]))","d3f90410":"lr = LogisticRegression(max_iter=400, class_weight=\"balanced\")\nlr.fit(X_train,y_train)\ny_lr_pred = lr.predict(X_test)\nprint(classification_report(y_test,y_lr_pred, target_names=[\"Bad Loan\",\"Good Loan\"]))","60d14dec":"labels = [\"Good Loan\",\"Bad Loan\"]\ncm = confusion_matrix(y_true = np.array(y_test),\n                      y_pred = pd.DataFrame(lr.predict(X_test)))\nprint(cm)","5bb03a4e":"XGB_model = XGBClassifier()\nXGB_model.fit(X_train,y_train)\n","6deb0c5e":"\ny_xgb_pred = XGB_model.predict(X_test)\nprint(classification_report(y_test,y_xgb_pred, target_names=[\"Bad Loan\", \"Good Loan\"]))","0bad5e40":"plot_importance(XGB_model, max_num_features= 15)","ac4a8be0":"\ndef add_roc_curve(y_pred, model_name, y_test=y_test, plt=plt):\n    fpr, tpr, thres = roc_curve(y_test, y_pred)\n    auc = round(roc_auc_score(y_test, y_pred),2)\n    plt.plot(1-fpr,tpr,label=\"{model_name}, auc={auc}\".format(model_name=model_name,auc=auc))\n    plt.legend(loc=0)\n    return(plt)","493ad903":"gb_plt = add_roc_curve(pd.DataFrame(gb.predict_proba(X_test))[1], y_test= y_test, model_name=\"Gradient Boost\")\nxgb_plt = add_roc_curve(pd.DataFrame(XGB_model.predict_proba(X_test))[1], y_test= y_test, model_name=\"XGBoost\")\nlr_plt = add_roc_curve(pd.DataFrame(lr.predict_proba(X_test))[1], y_test= y_test, model_name=\"Logistic\")\nlr_std_plt = add_roc_curve(pd.DataFrame(lr_std.predict_proba(X_test))[1], y_test= y_test, model_name=\"Logistic_std\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.show()","4b43db83":"labels = [\"Good Loan\",\"Bad Loan\"]\ncm = confusion_matrix(y_true = np.array(y_test),\n                      y_pred = pd.DataFrame(XGB_model.predict(X_test)))\nprint(cm)","4101689d":"cm = confusion_matrix(y_true = np.array(y_train),\n                      y_pred = pd.DataFrame(XGB_model.predict(X_train)))\nprint(cm)","c454abb2":"y_train.value_counts()","a42a78cc":"pd.Series(XGB_model.predict(current_loans)).value_counts()","6fcbb947":"palette = [\"#0B6623\", \"#D72626\" ]\n\nsns.countplot(x=\"issue_year\", hue=\"loan_condition\", data=load_data[load_data['loan_condition']!=\"Current\"], palette=palette)","c4957049":"palette = [\"#D72626\" , \"#0B6623\"]\ncurrent_loans_raw_data = load_data[load_data['loan_condition']==\"Current\"]\n#keeping threshold low; as model needs more tweeks\ncurrent_loans_raw_data['predicted_loan_condition'] = (pd.DataFrame(XGB_model.predict_proba(current_loans))[1]>0.12)\nsns.countplot(x=\"issue_year\", hue=\"predicted_loan_condition\", data=current_loans_raw_data, palette=palette)","f9d5770c":"# Lending Club\n## Lending Club dataset is a database of about 2.2 Million Loans with 145 features in them.\n## Each Loan has it's Status which can be classified as \"Bad\" Loan or \"Good\" Loan; by grouping \"Charged Off\", \"Default\", \"Does not meet the credit policy. Status:Charged Off\", \"In Grace Period\", \"Late (16-30 days)\", \"Late (31-120 days)\" as \"BAD LOAN\" and considering \"Fully Paid\" as \"GOOD LOAN\"\n## There are a significant recent loans which aren't classified as either \"GOOD LOAN\" or \"BAD LOAN\", in this notebook, I am trying to train a model on the classified \"GOOD\" and \"BAD\" Loans to try and predict which one of the \"Current\" Loans will become \"GOOD\" Loans and which would become  \"BAD\"\n\n<a id='table_of_contents'><\/a>\n\n\n\n## Table of Contents\n\n### <a href=#loan_condition>Loan Condition Analysis<\/a>\n### <a href=#numerical_features>Numerical Features Analysis across Loan Condition<\/a>\n### <a href=#categorical_features>Catogorical Features Analysis across Loan Condition<\/a>\n### <a href=#feature_engineering>Feature Engineering<\/a>\n### <a href=#predictive_model>Predictive Model<\/a>\n### <a href=#results>Results<\/a>\n### <a href=#future_scope_for_improvement>Future scope for Improvement<\/a>\n","d0fa09c4":"<a id='future_scope_for_improvement'><\/a>\n\n\n\n# Future Scope for Improvement\n## More Meaningful Feature Engineering\n### Need Domain knowledge to interpret the meaning of features like revolving trades and open trades and how the opening of such accounts would factor in the prediction of the applicant's loan\n### Hyperparameter Tuning, For now, I have gone with a Vanila XGBoost Model without paying attention to the hyperparameter tuning. I would be very interested to find out if the performance would vary with parameters like \"scale_pos_weight\" to balance the classes; max_depth, learning rate and even try out a custom loss function\n\n\n### <a href=#table_of_contents>Back to Table of Contents<\/a>","148f5081":"### Removing all columns with more than 80% missing Values\n### Removing a few rows with a lot of missing columns\n### Fill Missing Values with 0\n### Remove Outlier for annual income further than 3 std dev\n#### Iterate back on these with more time","8e01481e":"# Get Issue Year and Issue Month\n","60825e61":"<a id='categorical_features'><\/a>\n\n\n\n\n# Categorical Features\n\n## What to read from these plots?\n### Ideally, I would like to pick Categorical Features that bring out a variation of Good Loans\/Bad Loan ratio across the different categorical values. I would also remove those features which do not have adequate number of representation of all 3 loan conditions.\n\n\n## pymnt_plan, hardship_flag, debt_settlement_flag, disbursement_method\n### Although these variables seems really important, they would be adding little value or probably harmful to the model since they don't have enough representation of the 3 loan_conditions across their classes\n\n\n\n### <a href=#table_of_contents>Back to Table of Contents<\/a>","19c9274c":"<a id='feature_engineering'><\/a>\n\n\n\n\n\n\n# Feature Engineering\n\n### <a href=#relative_income_index>Relative Income Index<\/a>\n### <a href=#payment_index>Payment Index<\/a>\n### <a href=#employment_length>Employment Length<\/a>\n### <a href=#employment_title>Employment Title<\/a>\n\n\n\n\n<a id='relative_income_index'><\/a>\n\n\n## Relative Income Index\n### I wanted to try stitching some data Using the applicants Income, State, Loan Issue Date, Purchasing Power Index per State per Year, Median Household Income per year per state to get a single value that can represent the person relative income index normalized across time and state\n\n\n### The reason behind this thinking was that the US economy could be affected by various factors through time. For Example, the economic crisis in 2008 could've hindered the economy which could've reduced a person's income reported at that time of loan application. When I am modelling the classification of loans, it made sense to normalize his income across time and geography using different data sources\n\n### I found 2 datasets on which gave me break down of Purchasing Power Per State per year and median :\nhttps:\/\/en.wikipedia.org\/wiki\/List_of_U.S._states_and_territories_by_income\n### I have attached hte CSV files in the email for reference\n\n\n### <a href=#table_of_contents>Back to Table of Contents<\/a>","9306b9ea":"<a id='employment_length'><\/a>\n\n\n\n## Employment Lenght\n\n\n### <a href=#feature_engineering>Back to Feature Engineering<\/a>\n","c5e9fb91":"\n\n\n<a id='loan_condition'><\/a>\n\n\n# Loan Condition\n## Let's Group the Different Loan Status into \"Bad Loan\", \"Good Loan\" and \"Current\"\n\n\n### <a href=#table_of_contents>Back to Table of Contents<\/a>","6a922319":"<a id='logistic_regression'><\/a>\n\n\n\n## Logistic Regression\n\n\n\n## <a href=#predictive_model>Back to Predictive Model<\/a>","3e99b5bf":"<a id='numerical_features'><\/a>\n\n\n# Numerical Features\n\n## What to read from these plots?\n### Ideally, I would like to pick Numerical Features that bring out a variation in the distribution of the features across Good Loans and Bad Loans. Since most of the current loans are recent loans, as compared to the older loans which already have a result of \"Bad Loan\" or \"Good Loan\",  some of the features might be time bound. For example, \"tot_cur_bal\" is a feature which represents the Current Balance in the Loan. When training my model on just the \"Good Loan\" and \"Bad Loan\" Category, it is very easy for my model to learn things like \"having a low current balance\" implies \"Good Loan\", and \"having a high current balance\" implies \"Bad Loan\". But this is a bad feature to learn since all of my recent loans would also have a \"low current balance\", hence these features can't be used in their raw format.  \n\n\n## loan_amnt, funded_amnt, funded_amnt_inv\n### These seem like they are similar features and might have a very high correlation to each other\n\n## int_rate\n### Interest Rate would be a feature which could potential indicate a good loan from a bad loan, but I would like to exclude this feature from the model as I would like to use int_rate as a multiplying factor with the predicted probability of the loan being good to determine if the loan is worth investing in. An example of how this would add value is :\n\n### Let's try to rank these loans in the order of preference to invest in\n### LOAN A -  Predicted Probablity of being a good loan = 0.4 , interest rate = 15%\n### LOAN B -  Predicted Probablity of being a good loan = 0.45 , interest rate = 12%\n### Without using \"int_rate\" as a multiplying factor, we would choose Loan B over Loan A ; But if we use \"int_rate\" as a multiplying factor, we would be choosing Loan A(0.4 x 15\/100 = 0.06) over Loan B(0.45 x 12\/100 = 0.054)\n\n\n### <a href=#table_of_contents>Back to Table of Contents<\/a>","82687d29":"<a id='feature_importance'><\/a>\n\n\n\n## Feature Importance\n\n## <a href=#predictive_model>Back to Predictive Model<\/a>\n","89c0ea4b":"<a id='results'><\/a>\n\n\n\n\n# Results\n\n## <a href=#auc_plot>AUC PLOT<\/a>\n## <a href=#Confusion_Matrix>Confusion Matrix<\/a>\n## <a href=#predict_current_loans>Predict Current Loans<\/a>\n\n\n### <a href=#table_of_contents>Back to Table of Contents<\/a>","c10e866b":"<a id='Confusion_Matrix'><\/a>\n\n\n\n\n# Confusion_Matrix\n\n### <a href=#results>Back to results<\/a>","0c60c6fb":"<a id='xgboost'><\/a>\n\n\n\n\n\n\n## XGBoost\n\n## <a href=#predictive_model>Back to Predictive Model<\/a>","9499dd78":"<a id='auc_plot'><\/a>\n\n\n\n\n# AUC PLOT\n\n### <a href=#results>Back to results<\/a>","cd456079":"<a id='payment_index'><\/a>\n\n\n\n\n## Payment Index\n### Using features like \"total amount of principle paid\" might make your model learn something like, \"low amount of principle paid\" is a Bad Loan and \"high amount of principle paid\" is a Good Loan. This could be dangerous since most of the \"Current\" Loans are recent loans, which might have \"low amount of principle paid\" as they haven't gotten the time to repay them yet. Hence I am trying to come up with a \"Payment Index\" which would be a ration of months_into_loan \/ ratio_of_principle_paid\n\n### This will ensure newer loans are not penalised and even might reward loans which are ahead of it's target\n\n\n### <a href=#feature_engineering>Back to Feature Engineering<\/a>","41c37bb2":"<a id='feature_selection'><\/a>\n\n\n\n\n## Feature Selection\n### Nice to have, but it would in the current setup encode the variable names hence losing the interpretability of the feature importance in the XGBoost Step. Commenting this for now\n## <a href=#predictive_model>Back to Predictive Model<\/a>","66c89cdc":"<a id='predictive_model'><\/a>\n\n\n# Predictive Model\n\n## <a href=#prepare_test_train>Pick Features and Prepare Train Test Dataset<\/a>\n## <a href=#feature_selection>Feature Selection<\/a>\n## <a href=#gradient_boost>Gradient Boost<\/a>\n## <a href=#logistic_regression>Logistic Regression<\/a>\n## <a href=#xgboost>XGBoost Classification<\/a>\n## <a href=#feature_importance>Feature Importance<\/a>\n\n<a id='prepare_test_train'><\/a>\n\n\n\n\n\n### <a href=#table_of_contents>Back to Table of Contents<\/a>","2b63c40b":"<a id='predict_current_loans'><\/a>\n\n\n\n\n# Predict Current Loans\n\n### <a href=#results>Back to results<\/a>","94d5fd04":"<a id='employment_title'><\/a>\n\n\n## Employment Title\n### There are a lot of titles provided by the applicants. I used TFIDF Vectorizer followed by K Means to try and cluster the titles together. I would have also like to get a word2vec embedding and validate the cluster formation with more time. I also would have like to determine the ideal number of clusters using Elbow Method. Didn't get into the details for this feature, as it needs a more work\n\n\n### <a href=#feature_engineering>Back to Feature Engineering<\/a>","f5ae890d":"<a id='gradient_boost'><\/a>\n\n\n\n## Gradient Boost\n\n\n\n## <a href=#predictive_model>Back to Predictive Model<\/a>"}}