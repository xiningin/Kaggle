{"cell_type":{"f81b9f37":"code","68cf194f":"code","8b505a8d":"code","5636a94b":"code","b0c8e134":"code","5db194c3":"code","a3b7b607":"code","77ab5866":"code","d2a75d8b":"code","0b8b7d07":"code","339aaeed":"code","a3610ed3":"code","01ad4976":"code","7551b846":"code","6ea20951":"code","09e434e4":"code","13402ab1":"code","1b49fffd":"code","75cda992":"code","9645060c":"code","5db36e8e":"code","fc4fe0fa":"code","32dc7688":"code","f48fbe79":"code","c60a678f":"code","58a1a173":"code","a685e5bc":"code","b1664f5f":"code","a12af4ff":"code","2568e122":"code","64c0cbac":"code","fe8d9632":"code","085a37eb":"code","3b66290c":"markdown","10b36004":"markdown","42dfa5d5":"markdown","e4845a12":"markdown","98ccfc8c":"markdown","c40c0470":"markdown","7e77cf58":"markdown","a188f757":"markdown","c1595b60":"markdown","acf055ef":"markdown","b950642e":"markdown","c94ef151":"markdown","46053b81":"markdown","b10e850e":"markdown","179f1623":"markdown","ee4cad5f":"markdown","93cc6f88":"markdown","78a72b7d":"markdown","1a2daeef":"markdown","8aeb5f84":"markdown","f9126f65":"markdown","e62deb98":"markdown","b03ef4d4":"markdown","c8a7340a":"markdown","fe822653":"markdown","6c62ab61":"markdown","66498a0b":"markdown","01abc830":"markdown","58a16c2a":"markdown","7d1482f5":"markdown","b8a6ef23":"markdown","388001d5":"markdown"},"source":{"f81b9f37":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as ss\nfrom scipy.stats import f_oneway, norm\nfrom collections import Counter\nimport math\nfrom itertools import product\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","68cf194f":"df = pd.read_csv('..\/input\/telecom-customer-churn-dataset\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","8b505a8d":"df.head()","5636a94b":"df.info()","b0c8e134":"df.isnull().sum().max()","5db194c3":"# We exclude data customer with tenure = 0\ndf = df[~(df['tenure']==0)]\ndf = df.reset_index(drop=True)","a3b7b607":"def barPerc_without_hue(ax, feature):\n    total = len(feature)\n    for p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n        x = p.get_x() + p.get_width() \/ 2 - 0.05\n        y = p.get_y() + p.get_height()\n        ax.annotate(percentage, (x, y), size = 12)\n\n\ndef barPerc_with_hue(df,xVar,ax):\n    '''\n    barPerc(): Add percentage for hues to bar plots\n    args:\n        df: pandas dataframe\n        xVar: (string) X variable \n        ax: Axes object (for Seaborn Countplot\/Bar plot or\n                         pandas bar plot)\n    '''\n    # 1. how many X categories\n    ##   check for NaN and remove\n    numX=len([x for x in df[xVar].unique() if x==x])\n\n    # 2. The bars are created in hue order, organize them\n    bars = ax.patches\n    ## 2a. For each X variable\n    for ind in range(numX):\n        ## 2b. Get every hue bar\n        ##     ex. 8 X categories, 4 hues =>\n        ##    [0, 8, 16, 24] are hue bars for 1st X category\n        hueBars=bars[ind:][::numX]\n        ## 2c. Get the total height (for percentages)\n        total = sum([x.get_height() for x in hueBars])\n\n        # 3. Print the percentage on the bars\n        for bar in hueBars:\n            ax.text(bar.get_x() + bar.get_width()\/2.,\n                    bar.get_height(),\n                    f'{bar.get_height()\/total:.0%}',\n                    ha=\"center\",va=\"bottom\")","77ab5866":"ax = sns.countplot(data=df, x='Churn')\nbarPerc_without_hue(ax, df['Churn'])\nplt.title('Churn Vs No Churn')\nplt.show()","d2a75d8b":"# I only choose some columns that i think important, but only based on my instinct, so it can be wrong\ncol_check = ['gender','SeniorCitizen','Partner','InternetService',\n             'OnlineSecurity','OnlineBackup','DeviceProtection',\n             'TechSupport','Contract']\nplt.figure(figsize=(15,20))\nfor i,column in enumerate(col_check):\n    plt.subplot(len(col_check), 3, i+1)\n    plt.suptitle(\"Churn\", fontsize=20, x=0.5, y=1)\n    ax = sns.countplot(data=df, x=column, hue='Churn')\n    barPerc_with_hue(df, column, ax)\n    plt.title(f\"{column}\")\n    plt.legend(loc=8)\n    plt.tight_layout()","0b8b7d07":"sns.displot(data = df[df['Churn']=='Yes'], x='tenure')\nplt.title('Tenure Distribution For Churn Customer')\nplt.show()","339aaeed":"df_check = df[df['tenure']==2]\ndf_check[['tenure','MonthlyCharges','TotalCharges']].head()","a3610ed3":"def check(tenure, monthly, total):\n    tenure = int(tenure)\n    monthly = float(monthly)\n    total = float(total)\n    if tenure*monthly == total:\n        result = 0 # Match\n        value = (total - (tenure*monthly)) \/ (tenure*monthly)\n    elif tenure*monthly > total:\n        result = 1 # Decrease\n        value = (total - (tenure*monthly)) \/ (tenure*monthly)\n    else:\n        result = 2 # Increase\n        value = (total - (tenure*monthly)) \/ (tenure*monthly)\n    return result, round(value,4)\n\nresults, values = [], []\nfor i in range(len(df)):\n    try:\n        tenure = int(df.loc[i].tenure)\n        monthly = float(df.loc[i].MonthlyCharges)\n        total = float(df.loc[i].TotalCharges)\n        result, value = check(tenure, monthly, total)\n        results.append(result)\n        values.append(value)\n    except:\n        results.append(0)\n        values.append(float(df.loc[i].MonthlyCharges))\n\n# Create new DF\ndf_cp = df.copy()\ndf_cp['TotalChargesInfo'] = results\ndf_cp['TotalChargesPercent'] = values\ndf_cp['TotalCharges'] = df_cp['TotalCharges'].astype('float64')\n\n# Visualize\nax = sns.countplot(data=df_cp, x='TotalChargesInfo', hue='Churn')\nbarPerc_with_hue(df_cp, column, ax)\nplt.title('Effect Of Different Total Charge On Churn')\nplt.show()","01ad4976":"df_cp = df_cp.sample(frac=1)\n\n# amount of Churn classes 1869 rows.\nchurn_df = df_cp.loc[df['Churn'] == 'Yes']\nnot_churn_df = df_cp.loc[df['Churn'] == 'No'][:1869]\n\nnormal_distributed_df = pd.concat([churn_df, not_churn_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\nnew_df.drop(columns='customerID', inplace=True)","7551b846":"sns.countplot(data=new_df, x='Churn')\nplt.show()","6ea20951":"ob=[]\nfor data in new_df.columns:\n    if data == 'SeniorCitizen':\n        ob.append(data)\n    if new_df[data].dtype=='object':\n        ob.append(data)","09e434e4":"def cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))\n\ncramers_df = pd.DataFrame(index=ob)\n\nfor x in ob:\n    a = []\n    for y in ob:\n        a.append(cramers_v(new_df[y], new_df[x]))\n    cramers_df[x] = a\n\n\nplt.figure(figsize=(15,10))\nsns.heatmap(cramers_df, annot=True, fmt='.2f')\nplt.title('Cramers Corr')\nplt.show()","13402ab1":"def conditional_entropy(x,y, log_base: float = math.e):\n    y_counter = Counter(y)\n    xy_counter = Counter(list(zip(x, y)))\n    total_occurrences = sum(y_counter.values())\n    entropy = 0.0\n    for xy in xy_counter.keys():\n        p_xy = xy_counter[xy] \/ total_occurrences\n        p_y = y_counter[xy[1]] \/ total_occurrences\n        entropy += p_xy * math.log(p_y \/ p_xy, log_base)\n    return entropy\n\ndef theils_u(x,y):\n    s_xy = conditional_entropy(x, y)\n    x_counter = Counter(x)\n    total_occurrences = sum(x_counter.values())\n    p_x = list(map(lambda n: n \/ total_occurrences, x_counter.values()))\n    s_x = ss.entropy(p_x)\n    if s_x == 0:\n        return 1.\n    else:\n        s_diff = s_x - s_xy\n        if -1e-13 <= s_diff < 0.:\n            warnings.warn(f'Rounded U = {s_diff} to zero. This is probably due to computation errors as a result of almost insignificant differences between x and y.', RuntimeWarning)\n            return 0.\n        else:\n            return (s_diff) \/ s_x\n        \ntheil_df = pd.DataFrame(index=ob)\nfor x in ob:\n    a = []\n    for y in ob:\n        a.append(theils_u(new_df[y], new_df[x]))\n    theil_df[x] = a\n\nplt.figure(figsize=(15,10))\nsns.heatmap(theil_df, annot=True, fmt='.2f')\nplt.title('Theils Corr')\nplt.show()","1b49fffd":"## Creating a DataFrame with all categorical variables\ndf_cat = new_df[ob]\n\n## Let us split this list into two parts\ncat_var1 = ('gender','SeniorCitizen','Partner','Dependents','PhoneService','MultipleLines','InternetService','OnlineSecurity',\n            'OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies','Contract',\n            'PaperlessBilling','PaymentMethod','Churn')\ncat_var2 = ('gender','SeniorCitizen','Partner','Dependents','PhoneService','MultipleLines','InternetService','OnlineSecurity',\n            'OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies','Contract',\n            'PaperlessBilling','PaymentMethod','Churn')\n\n## Creating all possible combinations between the above two variables list\ncat_var_prod = list(product(cat_var1,cat_var2, repeat = 1))\n\n## Creating an empty variable and picking only the p value from the output of Chi-Square test\nresult = []\nfor i in cat_var_prod:\n    if i[0] != i[1]:\n        result.append((i[0],i[1],list(ss.chi2_contingency(pd.crosstab(\n                                df_cat[i[0]], df_cat[i[1]])))[1]))\n\nchi_test_output = pd.DataFrame(result, columns = ['var1', 'var2', 'coeff'])\n## Using pivot function to convert the above DataFrame into a crosstab\nchi_test_output.pivot(index='var1', columns='var2', values='coeff')","75cda992":"con = []\nfor data in new_df.columns:\n    if data == 'SeniorCitizen':\n        pass\n    elif data == 'Churn':\n        con.append(data)\n    elif new_df[data].dtype!='object':\n        con.append(data)","9645060c":"anova_df = pd.DataFrame(index=['Churn'])\nfor i in con:\n    try:\n        CategoryGroupLists=new_df.groupby('Churn')[i].apply(list)\n        AnovaResults = f_oneway(*CategoryGroupLists)\n        anova_df[i] = AnovaResults.pvalue\n    except:\n        pass\nanova_df","5db36e8e":"from sklearn import preprocessing\n\nnew_df2 = new_df.copy()\nnew_df2 = new_df2[con]\nnew_df2['Churn'] = preprocessing.LabelEncoder().fit_transform(new_df2[['Churn']])\n\nsns.heatmap(new_df2.corr(method='spearman'), annot=True, fmt='.2f')\nplt.title('Spearman Correlation')","fc4fe0fa":"df_model = new_df.copy()\ndf_model = df_model[['tenure', 'InternetService', 'OnlineSecurity',\n       'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n       'Contract', 'PaymentMethod', 'MonthlyCharges', 'Churn']]\ndf_model['Churn'] = LabelEncoder().fit_transform(df_model[['Churn']])","32dc7688":"# Undersampling before cross validating (prone to overfit)\nX = df_model.drop(columns='Churn')\ny = df_model['Churn']","f48fbe79":"# This is explicitly used for undersampling.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","c60a678f":"# Do LabelEncoder to encode categorical from string to int\nfor col in df_model.columns[1:9]: # from InternetService to PaymentMethod\n    LE = LabelEncoder()\n    X_train[col] = LE.fit_transform(X_train[[col]])\n    X_test[col] = LE.transform(X_test[[col]])","58a1a173":"# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","a685e5bc":"# Let's implement simple classifiers\n\nclassifiers = {\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"RandomForrestClassifier\": RandomForestClassifier()\n}","b1664f5f":"for key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","a12af4ff":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n\n# tree best estimator\ntree_clf = grid_tree.best_estimator_\n\n# RandomForest Classifier\nforest_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_forest = GridSearchCV(RandomForestClassifier(), forest_params)\ngrid_forest.fit(X_train, y_train)\n\n# tree best estimator\nforest_clf = grid_forest.best_estimator_","2568e122":"# Overfitting Case\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')\n\nforest_score = cross_val_score(forest_clf, X_train, y_train, cv=5)\nprint('RandomForest Classifier Cross Validation Score', round(forest_score.mean() * 100, 2).astype(str) + '%')","64c0cbac":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n# Create a DataFrame with all the scores and the classifiers names.\n\nforest_pred = cross_val_predict(forest_clf, X_train, y_train, cv=5,)\n\nknears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\ntree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)","fe8d9632":"from sklearn.metrics import roc_auc_score\n\nprint('Random Forest Classifier: ', roc_auc_score(y_train, forest_pred))\nprint('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))","085a37eb":"forest_fpr, forest_tpr, forest_thresold = roc_curve(y_train, forest_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n\n\ndef graph_roc_curve_multiple(forest_fpr, forest_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(forest_fpr, forest_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, forest_pred)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(forest_fpr, forest_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\nplt.show()","3b66290c":"# Preprocessing Data Before Train\nIn this section we will do some preprocessing before we train the data.\n1. We will create df with only column that get high correlation on previous section\n2. Split the data to X(all columns exclude Target(Churn)) and y(Only target column)\n3. Split data to train and test\n4. Encode all string categorical column with LabelEncoder ","10b36004":"### Correlation Between Categorical And Continous","42dfa5d5":"#### Cramers V","e4845a12":"For point No.3, i think it happen because customer with Month-to-month contract more easy dan flexible to decide whether they will churn or not, because they pay the service per month, for customer with 1 or 2 year contract they will loss if they churn before the contract end.","98ccfc8c":"In statistics, Spearman's rank correlation coefficient or Spearman's \u03c1, named after Charles Spearman, is a nonparametric measure of rank correlation (statistical dependence between the rankings of two variables). It assesses how well the relationship between two variables can be described using a monotonic function.\n\nThe Spearman correlation between two variables is equal to the Pearson correlation between the rank values of those two variables; while Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not). If there are no repeated data values, a perfect Spearman correlation of +1 or \u22121 occurs when each of the variables is a perfect monotone function of the other.\n\nIntuitively, the Spearman correlation between two variables will be high when observations have a similar (or identical for a correlation of 1) rank (i.e. relative position label of the observations within the variable: 1st, 2nd, 3rd, etc.) between the two variables, and low when observations have a dissimilar (or fully opposed for a correlation of \u22121) rank between the two variables.\n\nSpearman's coefficient is appropriate for both continuous and discrete ordinal variables.","c40c0470":"## Conclusion For Correlation\nConclusion from this part is same like the first part, but there some different<br>\n1. Age (Partner and Senior Citizen) doesn't really affect the Churn\n2. Payment Method and tenure have an affect on churn","7e77cf58":"## Conclusion:\n- SVC is more accurate than the other three classifiers in most cases.\n- GridSearchCV is used to determine the paremeters that gives the best predictive score for the classifiers.\n- Logistic Regression has the best Receiving Operating Characteristic score (ROC), meaning that LogisticRegression pretty accurately separates fraud and non-fraud transactions.","a188f757":"#### ANOVA","c1595b60":"ANOVA stands for Analysis Of Variance. So, basically this test measures if there are any significant differences between the means of the values of the numeric variable for each categorical value. This is something that you can visualize using a box-plot as well.<br>\n\nBelow items must be remembered about ANOVA hypothesis test\n\n- Null hypothesis(H0): The variables are not correlated with each other\n- P-value: The probability of Null hypothesis being true\n- Accept Null hypothesis if P-value>0.05. Means variables are NOT correlated\n- Reject Null hypothesis if P-value<0.05. Means variables are correlated","acf055ef":"#### Spearman Correlation","b950642e":"The result from ANOVA and spearman are same. Tenure has the highest correlation with Churn, and TotalChargePercent doesn't have. For this we will use tenure and MonthlyCharges as our feature to model, why not TotalCharges, it has higher corr tham MonthlyCharges. We don't use TotalCharges because tenure and TotalCharges has correlation, if customer use the service for long time, he will get high TotalCharges, for that reason we will only use tenure","c94ef151":"#### Chi-Square","46053b81":"For Theils U Correlation, only Contract who have high correlation with churn","b10e850e":"Many of customer that use the service for less than 1 year do churn, maybe this happen because of the service they get","179f1623":"From the heatmap we know that service(Support, Security, Backup, Protection) and contract have high correlation to Churn that other, so its same like the first way.<br>\nThinking over the output of Cramer\u2019s V, I realized I\u2019m losing valuable information due to the symmetry of it. Because of symmetry This valuable information is lost when using Cramer\u2019s V , so to preserve it we need an asymmetric measure of association between categorical features. And this is exactly what Theil\u2019s U is.\n<br>\n<br>\nTheil\u2019s U, also referred to as the Uncertainty Coefficient, is based on the conditional entropy between x and y \u2014 or in human language, given the value of x, how many possible states does y have, and how often do they occur. Just like Cramer\u2019s V, the output value is on the range of [0,1], with the same interpretations as before \u2014 but unlike Cramer\u2019s V, it is asymmetric, meaning U(x,y)\u2260U(y,x) (while V(x,y)=V(y,x), where V is Cramer\u2019s V). Using Theil\u2019s U in the simple case above will let us find out that knowing y means we know x, but not vice-versa.","ee4cad5f":"After read the data more deep, i just find out there something strange with TotalCharges column, \nwhat i know is TotalCharges column is from MonthlyCharges multiply by tenure, but TotalCharges has different number from count result, this is for example","93cc6f88":"After we do some way to find the good feature for modeling, we have got some features that will be used to create the model, the features are tenure, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, Contract, PaymentMethod.<br>\nFinally we got feature for modelling, the next step is we will do some feature engineering like encode the value with One-Hot or LabelEncoder, and then we will find the best ML model","78a72b7d":"# Result For Feature Selection","1a2daeef":"* More customer churn when they don't get the service such as Support, Security, Backup, Protection etc.\n* Junior Citizen and Single Customer, more likely do churn.\n* Customer that have Month-to-Month contract more likely do churn.\n* For internet service i don't know why many customer FiberOptic service do churn, what i know is Fiber Optic is faster than DSL, So i assume in this telcom company the FiberOptic service maybe has more interference than DSL, like indigo(IYKWIM), so many customer do churn.","8aeb5f84":"# Classifiers (UnderSampling):\nIn this section we will train four types of classifiers and decide which classifier will be more effective in detecting churn. Before we have to split our data into training and testing sets and separate the features from the labels.","f9126f65":"## 1. Comparison Data With Visualization\nFor this way, we just use simple analysis from data visualization to find feature which is about influential","e62deb98":"# Finding The Good Feature For Model\n\nIn this section we will try to find the feature that we will use to train. There are 2 ways to find the feature which will be used in this notebook.\n1. Comparison Data With Visualization\n2. Correlation To Target","b03ef4d4":"Before we can discuss about what correlation is not, let\u2019s talk about what it is. In human language, correlation is the measure of how two features are, well, correlated; just like the month-of-the-year is correlated with the average daily temperature, and the hour-of-the-day is correlated with the amount of light outdoors. Formalizing this mathematically, the definition of correlation usually used is Pearson\u2019s R for a data sample (which results in a value in the range (-1,1), but Pearson\u2019s R isn\u2019t defined when the data is categorical.\n<br>\nBecause of that, we will create 2 correlations, first is for categorical column with categorical column, and second for categorical column with continous column. For first correlation we will use Cramers V, Theils U, and Chi Square method, and for the second correlation we will use ANOVA hypothesis\n<br><br>\nBut before that, I once saw a notebook that explained that imbalanced data could affect the correlation results, therefore, we made a subsample whose target was balance by using under sampling. [This Notebook](https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets)","c8a7340a":"## 2. Correlation","fe822653":"# REFERENCES:\n- [The Search for Categorical Correlation](https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9) \n- [How to measure the correlation between a numeric and a categorical variable in Python](https:\/\/thinkingneuron.com\/how-to-measure-the-correlation-between-a-numeric-and-a-categorical-variable-in-python\/)\n- [Correlation between Categorical Variables](https:\/\/medium.com\/@ritesh.110587\/correlation-between-categorical-variables-63f6bd9bf2f7)\n- [Credit Fraud || Dealing with Imbalanced Datasets](https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets#Test-Data-with-Logistic-Regression:)\n- [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets#Test-Data-with-Logistic-Regression:)","6c62ab61":"The findings prove that different TotalCharges should have no effect on churn, because Customer who get decrease TotalCharge has same churn percentage with Customer who get increase TotalCharge, for Customer with match TotalCharges there are many who do churn, this because most match TotalCharge is Customer who use the service less than 2 months, from data we know that most of Customer whos use the service less than 2 months does churn.\n<br>\n<br>\n## Conclusion For Comparison Data With Visualization \nFor the first way(Comparison with visualization), The conclussion we get is service (Support, Security, Backup, Protection) very influential on whether the customer will churn or not. Beside that, age, kind of internet and contract also influential. After this we will try correlation to find out if there any other column that influental.","66498a0b":"What we need is something that will look like correlation, but will work with categorical values \u2014 or more formally, we\u2019re looking for a measure of association between two categorical features. Introducing: [Cram\u00e9r\u2019s V](https:\/\/en.wikipedia.org\/wiki\/Cram\u00e9r%27s_V). It is based on a nominal variation of [Pearson\u2019s Chi-Square Test](https:\/\/en.wikipedia.org\/wiki\/Pearson%27s_chi-squared_test), and comes built-in with some great benefits:<br>\n1. Similarly to correlation, the output is in the range of [0,1], where 0 means no association and 1 is full association. (Unlike correlation, there are no negative values, as there\u2019s no such thing as a negative association. Either there is, or there isn\u2019t)\n2. Like correlation, Cramer\u2019s V is symmetrical \u2014 it is insensitive to swapping x and y","01abc830":"#### Theils U","58a16c2a":"at first row the TotalCharges should be 107.7 but in data 108.15, so there's an increase of 0.45. If we think that maybe tax, okay, let's look at the last row, 80.65 multiply by 2 is 161.3, why decrease to 144.15. Because of this discovery, i have an idea to make a column to see does TotalCharges match, increase or decrease","7d1482f5":"After that we have hypothesis that<br>\nH0 - There is no relationship between that column and churn<br>\nHa - There is relationship between that column and churn<br><br>\n\nAnd we will do analysis <br>\nif p-value(value from chi-square) <= 0.05 then we will reject H0 but if p-value > 0.05 we will accept H0<br><br>\nFrom table above we see that all columns has corelation with Churn except gender and phone service, and same like another correlation we have tried, some service(Support, Security, Backup, Protection) and contract has the highest score.","b8a6ef23":"Our dataset is imbalance, although not extreme imbalance, but this can affect the model. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most Customers will not churn.","388001d5":"### Correlation Between Categorical and Categorical"}}