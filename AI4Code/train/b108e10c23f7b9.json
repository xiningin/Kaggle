{"cell_type":{"d64080b2":"code","0bab04d0":"code","416a1ecc":"code","447e7b03":"code","59b3cdfd":"code","388761b2":"code","26c1e0ca":"code","e5391dbd":"code","5d143359":"code","b32e13cf":"code","8146849f":"code","2c960114":"code","139921dd":"code","23940209":"code","2baa4335":"code","20122da2":"code","14cc7312":"code","23d8a3b3":"code","e115a694":"code","852198c1":"code","440e9bc1":"code","a0c7f6e5":"code","7829c42c":"code","745ccc6b":"code","8b4ae8bd":"code","f8bd1f89":"code","a655501d":"code","9ca1782f":"code","35fa807a":"code","8ae76efb":"code","9327cc96":"code","71ecd2d4":"code","1e885e08":"code","94b11927":"code","64808a9b":"code","e3c640ea":"code","6f78207d":"code","90b1432c":"code","cc02bde2":"code","e13c0b83":"code","24208b1c":"code","98a9e5f5":"code","754a0ff3":"code","a5caa305":"code","765fdcd1":"code","595a1245":"code","819f1a02":"code","943d4197":"code","dbcd2915":"code","c2b98c38":"code","b104bf51":"code","52ce548b":"code","d87b8296":"code","aa98409e":"code","e1145c92":"code","38d4b66a":"code","011ddd01":"code","124df0b6":"markdown","d75906a4":"markdown","0ea1db29":"markdown","7d456f1c":"markdown","6fe87190":"markdown","81c832b4":"markdown","08a7e900":"markdown","743a70c0":"markdown","a80f6c08":"markdown"},"source":{"d64080b2":"\n# import dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\n\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n","0bab04d0":"train = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/train.csv',\n                   usecols=[1, 2, 3, 4, 5, 7, 8, 9],\n                   dtype={'timestamp': 'int64',\n                          'user_id': 'int32',\n                          'content_id': 'int16',\n                          'content_type_id': 'int8',\n                          'task_container_id': 'int16',\n                          'answered_correctly':'int8',\n                          'prior_question_elapsed_time': 'float32',\n                          'prior_question_had_explanation': 'boolean'}\n                   )","416a1ecc":"#reading in question df\nquestions_df = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/questions.csv',                         \n                            usecols=[0, 3],\n                            dtype={'question_id': 'int16',\n                              'part': 'int8'}\n                          )","447e7b03":"#removing True or 1 for content_type_id\n\ntrain = train[train.content_type_id == False].sort_values('timestamp').reset_index(drop = True)","59b3cdfd":"train[(train.task_container_id == 9999)].tail()","388761b2":"train[(train.content_type_id == False)].task_container_id.nunique()","26c1e0ca":"#saving value to fillna\nelapsed_mean = train.prior_question_elapsed_time.mean()\n","e5391dbd":"group1 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['count'])\ngroup1.columns = ['avg_questions']\ngroup2 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['nunique'])\ngroup2.columns = ['avg_questions']\ngroup3 = group1 \/ group2","5d143359":"group3['avg_questions_seen'] = group3.avg_questions.cumsum()","b32e13cf":"group3.iloc[0].avg_questions_seen","8146849f":"results_u_final = train.loc[train.content_type_id == False, ['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_final.columns = ['answered_correctly_user']\n\nresults_u2_final = train.loc[train.content_type_id == False, ['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_final.columns = ['explanation_mean_user']","2c960114":"results_u2_final.explanation_mean_user.describe()","139921dd":"train = pd.merge(train, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')","23940209":"results_q_final = train.loc[train.content_type_id == False, ['question_id','answered_correctly']].groupby(['question_id']).agg(['mean'])\nresults_q_final.columns = ['quest_pct']","2baa4335":"results_q2_final = train.loc[train.content_type_id == False, ['question_id','part']].groupby(['question_id']).agg(['count'])\nresults_q2_final.columns = ['count']","20122da2":"question2 = pd.merge(questions_df, results_q_final, left_on = 'question_id', right_on = 'question_id', how = 'left')","14cc7312":"question2 = pd.merge(question2, results_q2_final, left_on = 'question_id', right_on = 'question_id', how = 'left')","23d8a3b3":"question2.quest_pct = round(question2.quest_pct,5)","e115a694":"display(question2.head(), question2.tail())","852198c1":"train.head()","440e9bc1":"len(train)","a0c7f6e5":"len(train)","7829c42c":"train.answered_correctly.mean()","745ccc6b":"prior_mean_user = results_u2_final.explanation_mean_user.mean()","8b4ae8bd":"train.loc[(train.timestamp == 0)].answered_correctly.mean()","f8bd1f89":"train.loc[(train.timestamp != 0)].answered_correctly.mean()","a655501d":"train.drop(['timestamp', 'content_type_id', 'question_id', 'part'], axis=1, inplace=True)","9ca1782f":"len(train)","35fa807a":"validation = train.groupby('user_id').tail(5)\ntrain = train[~train.index.isin(validation.index)]\nlen(train) + len(validation)","8ae76efb":"validation.answered_correctly.mean()","9327cc96":"train.answered_correctly.mean()","71ecd2d4":"results_u_val = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_val.columns = ['answered_correctly_user']\n\nresults_u2_val = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_val.columns = ['explanation_mean_user']","1e885e08":"X = train.groupby('user_id').tail(18)\ntrain = train[~train.index.isin(X.index)]\nlen(X) + len(train) + len(validation)","94b11927":"X.answered_correctly.mean()","64808a9b":"train.answered_correctly.mean()","e3c640ea":"results_u_X = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_X.columns = ['answered_correctly_user']\n\nresults_u2_X = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_X.columns = ['explanation_mean_user']","6f78207d":"#clearing memory\ndel(train)","90b1432c":"X = pd.merge(X, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\nX = pd.merge(X, results_u_X, on=['user_id'], how=\"left\")\nX = pd.merge(X, results_u2_X, on=['user_id'], how=\"left\")","cc02bde2":"validation = pd.merge(validation, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\nvalidation = pd.merge(validation, results_u_val, on=['user_id'], how=\"left\")\nvalidation = pd.merge(validation, results_u2_val, on=['user_id'], how=\"left\")","e13c0b83":"#from sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\n\nX.prior_question_had_explanation.fillna(False, inplace = True)\nvalidation.prior_question_had_explanation.fillna(False, inplace = True)\n\nvalidation[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(validation[\"prior_question_had_explanation\"])\nX[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(X[\"prior_question_had_explanation\"])","24208b1c":"#reading in question df\n#question2 = pd.read_csv('\/kaggle\/input\/question2\/question2.csv)","98a9e5f5":"content_mean = question2.quest_pct.mean()\n\nquestion2.quest_pct.mean()\n#there are a lot of high percentage questions, should use median instead?","754a0ff3":"#filling questions with no info with a new value\nquestion2.quest_pct = question2.quest_pct.mask((question2['count'] < 3), .65)\n\n\n#filling very hard new questions with a more reasonable value\nquestion2.quest_pct = question2.quest_pct.mask((question2.quest_pct < .2) & (question2['count'] < 21), .2)\n\n#filling very easy new questions with a more reasonable value\nquestion2.quest_pct = question2.quest_pct.mask((question2.quest_pct > .95) & (question2['count'] < 21), .95)","a5caa305":"X = pd.merge(X, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\nvalidation = pd.merge(validation, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\nX.part = X.part - 1\nvalidation.part = validation.part - 1","765fdcd1":"X.head()","595a1245":"y = X['answered_correctly']\nX = X.drop(['answered_correctly'], axis=1)\nX.head()\n\ny_val = validation['answered_correctly']\nX_val = validation.drop(['answered_correctly'], axis=1)","819f1a02":"X = X[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n       'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part']]\nX_val = X_val[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n       'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part']]","943d4197":"\n# Filling with 0.5 for simplicity; there could likely be a better value\nX['answered_correctly_user'].fillna(0.65,  inplace=True)\nX['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\nX['quest_pct'].fillna(content_mean, inplace=True)\n\nX['part'].fillna(4, inplace = True)\nX['avg_questions_seen'].fillna(1, inplace = True)\nX['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\nX['prior_question_had_explanation_enc'].fillna(0, inplace = True)\n","dbcd2915":"X_val['answered_correctly_user'].fillna(0.65,  inplace=True)\nX_val['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\nX_val['quest_pct'].fillna(content_mean,  inplace=True)\n\nX_val['part'].fillna(4, inplace = True)\nX['avg_questions_seen'].fillna(1, inplace = True)\nX_val['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\nX_val['prior_question_had_explanation_enc'].fillna(0, inplace = True)","c2b98c38":"scaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_val = scaler.transform(X_val)","b104bf51":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import BatchNormalization,Dropout,Dense,Flatten,Conv1D\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.metrics import BinaryAccuracy\nfrom keras import backend as K","52ce548b":"X_train = X.reshape(X.shape[0], X.shape[1], 1)\nX_test = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n    \nmodel=Sequential()\nmodel.add(Conv1D(64, 2, activation='relu', input_shape=X_train[0].shape))\nmodel.add(Conv1D(128, 2, activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])","d87b8296":"from keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_auc', mode='max',patience=5)","aa98409e":"history = model.fit(X_train, y, epochs=500, verbose=1, validation_split=0.1,batch_size=65536,callbacks=[es])","e1145c92":"y_pred = model.predict(X_test)\ny_true = np.array(y_val)\nroc_auc_score(y_true, y_pred)","38d4b66a":"import riiideducation\nenv = riiideducation.make_env()\niter_test = env.iter_test()","011ddd01":"for (test_df, sample_prediction_df) in iter_test:\n    test_df['task_container_id'] = test_df.task_container_id.mask(test_df.task_container_id > 9999, 9999)\n    test_df = pd.merge(test_df, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\n    test_df = pd.merge(test_df, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    test_df = pd.merge(test_df, results_u_final, on=['user_id'],  how=\"left\")\n    test_df = pd.merge(test_df, results_u2_final, on=['user_id'],  how=\"left\")\n    test_df['answered_correctly_user'].fillna(0.65,  inplace=True)\n    test_df['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\n    test_df['quest_pct'].fillna(content_mean,  inplace=True)\n    test_df['part'] = test_df.part - 1\n\n    test_df['part'].fillna(4, inplace = True)\n    test_df['avg_questions_seen'].fillna(1, inplace = True)\n    test_df['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n    test_df[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(test_df[\"prior_question_had_explanation\"])\n    X = scaler.transform(test_df[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n                                                            'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part']])\n    test_df['answered_correctly'] =  model.predict(X.reshape(X.shape[0], X.shape[1], 1))\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","124df0b6":"## Data Exploration ##","d75906a4":"## Making Predictions for New Data ##","0ea1db29":"## Creating Validation Set (Most Recent Answers by User) ##","7d456f1c":"## Merging Data ##","6fe87190":"## Modeling ##","81c832b4":"## Extracting Training Data ##","08a7e900":"Affirmatives (True) for content_type_id are only for those with a different type of content (lectures). These are not real questions.","743a70c0":"Does it make sense to use last questions as validation? Why is the rate of correct answers so low?\nI am convinced there is a better way to match the test data.","a80f6c08":"## Reading Data and Importing Libraries ##"}}