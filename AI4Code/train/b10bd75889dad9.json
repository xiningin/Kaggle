{"cell_type":{"ae6fdc07":"code","e3bbf7d8":"code","975e78df":"code","5cb1addf":"code","0daa0cd2":"code","d281bfb6":"code","cd13ae2b":"code","e856212d":"code","c4a830da":"code","e460cc2b":"code","136f2c1e":"code","f3715155":"code","5e14bf8d":"code","0e0671a6":"code","9fb316bb":"code","03a210f0":"code","3dc05c30":"code","07192735":"code","cc369404":"code","44ae957f":"code","821e7a1a":"code","f86aaf5e":"code","20421bc5":"code","b47fd680":"code","76c88af3":"code","0344ac9d":"code","59e4468d":"code","ea197153":"code","477d3110":"code","0e085341":"code","c21c5b80":"code","5963cb60":"code","c33812db":"code","3f284765":"code","48ef034b":"code","27359a5e":"code","20ed1098":"code","ac986437":"code","7a0ce2dd":"code","27e66586":"code","4e5cb33d":"code","407591a9":"code","fc9f9e37":"code","95252360":"code","107c3df4":"code","f1fc7d8c":"code","00282d4c":"code","b2ea9c8a":"code","5bf4ad59":"code","ccde428d":"code","cf830427":"code","121cab6c":"code","620a9cd3":"code","d212409a":"code","cce24827":"code","ce5ef98a":"code","613cd4a0":"code","e1067287":"code","ee09e9bf":"code","1aae684c":"code","3fde8089":"code","02937393":"code","7764235c":"code","6953a8c8":"code","6901a604":"code","7fe52d3d":"code","14cd3ae7":"code","49d0cca6":"code","531ba4ef":"code","752d4c91":"code","495e5162":"code","cbd6ef5b":"code","6d509ac5":"code","48278c48":"code","008a38cd":"code","f94bad37":"code","896982cc":"code","39c84ee0":"code","4006b004":"code","bac7bc28":"code","23d17829":"code","d1b52454":"code","197d0766":"code","8df7c68d":"code","3b799ada":"code","de33ae05":"code","f96d6b0e":"code","b6e4f4e2":"code","8abd53ed":"code","663d2470":"code","0c2e780b":"code","f23505c0":"code","461b34fa":"code","bd8c199d":"code","0c8f202a":"code","92aabeca":"code","c99ae6be":"code","c3716413":"code","516d2914":"code","722542bb":"code","7a963736":"code","1af826cf":"code","6bd3329b":"code","eb62ff7d":"code","56b2f2c3":"code","d3c45bbb":"code","0ea281ee":"code","ebbda787":"code","457aa0c0":"code","1cd6908d":"code","ec6a4ecc":"code","dcfc56b4":"code","4d735e5a":"code","06a12482":"code","b7831133":"code","3a23cb25":"code","99cde921":"code","35fb8979":"code","983aa9e9":"code","07f6158a":"code","1e4ebd0e":"code","36b9a9d7":"code","32026aee":"code","db7659ac":"code","c1e97d54":"code","dee4f7a2":"code","62bbd6ed":"code","781fa4cb":"code","fcd96ad3":"code","589e15c5":"code","8205c15f":"code","73cea4d7":"code","03734c88":"code","cf7db68e":"code","41fcbdc5":"code","0fb84cd8":"code","d0aeb846":"code","8ff42014":"code","e0bcbf0c":"code","2ab8029d":"code","3b73ef80":"code","4bfbac5f":"code","c8957b68":"code","d09e6b0c":"code","0f1ab27a":"code","9e2fdc3c":"code","850bf8bb":"code","c742ab10":"code","4f452e7b":"code","f5f88ee6":"code","8282dd3a":"code","3bb980fb":"code","9e0424a6":"code","e1d7a290":"code","9e9eab40":"code","00224b25":"code","f3b50c6d":"code","29e07641":"code","d32f146b":"code","9c5108cf":"code","3bf649df":"code","0d97d93b":"code","88f0ee08":"code","ea718b9e":"code","4b6ad81b":"code","0d135bf6":"code","e6313f05":"code","a3f79248":"code","a144aaaa":"code","06f49df2":"code","d6149b2c":"code","744bdfca":"code","d4c847a3":"code","7af58536":"code","ee0a6615":"code","b23e4ad5":"code","f34d23bd":"code","6b578ba6":"code","8f0bf7c7":"code","1604488e":"code","0258c7f5":"code","c0ec087d":"code","91dcb155":"code","cea2621a":"code","03ff8f35":"code","ea2fac38":"code","caf4b77c":"code","bb035813":"code","fe0c6ec4":"code","6c23a5cb":"code","c2b68665":"code","3cce10b7":"code","06d1c33d":"code","59544281":"code","6b5f5862":"code","b1652e39":"code","de32518c":"code","1aa95f73":"code","0a99f272":"code","2eb340ce":"code","9a7264cb":"code","d8f2f1bb":"code","48266c4f":"code","2512f902":"code","2a0c53a3":"code","305d3aec":"code","fc04d3c1":"code","a9a4429a":"code","d7120f2c":"code","1d6d3ab3":"code","f8d2b04b":"code","4aea3b51":"code","2b570ccb":"code","d05656c5":"code","d278b247":"code","8c132a11":"code","10752a18":"code","5f1e2383":"code","7c301ad7":"code","71d293d8":"code","fa24d7bd":"code","75e51749":"code","558aa037":"code","f5b3b562":"code","b2acfca5":"code","d92960dd":"code","20959697":"code","e75bd5f2":"code","28a2d372":"markdown","cc38ec8e":"markdown","2fd40167":"markdown","f908aeb9":"markdown","bc444ac5":"markdown","d283f1d6":"markdown","15e39819":"markdown","6e36451d":"markdown","56fba1fa":"markdown","c0c8cb02":"markdown","7e4b6607":"markdown","04044659":"markdown","ce2296e3":"markdown","b2741ee5":"markdown","7711b75d":"markdown","281563b5":"markdown","e0d8e098":"markdown","9363d309":"markdown","7bd01182":"markdown","05fdb427":"markdown","dffb7908":"markdown","4a30a329":"markdown","613b1e41":"markdown","eacfc34a":"markdown","f086923f":"markdown","53d897d1":"markdown","1740ca1e":"markdown","4e0dd7ad":"markdown","c28f7065":"markdown","c3dd3e4c":"markdown","53f3e0ed":"markdown","fd289ca1":"markdown","1f809678":"markdown","98ff24c6":"markdown","29b8c6e7":"markdown","4d387778":"markdown","fb5d24e6":"markdown","251a7f45":"markdown","d78518ca":"markdown","df54cbf2":"markdown","ebc871f9":"markdown","9572fcef":"markdown","349a2ce1":"markdown","a58adb9a":"markdown","f2a59d5a":"markdown","b0f3529f":"markdown","5422fc64":"markdown","66b91ea6":"markdown","1caa099d":"markdown","2d182326":"markdown","46f1db2e":"markdown","fe0e0e26":"markdown","3141df9f":"markdown","33c649ca":"markdown","91840406":"markdown","28cc9992":"markdown","b720b713":"markdown","5ddbb56e":"markdown","5141c47b":"markdown","5d618115":"markdown","01abd713":"markdown","839e8b3a":"markdown","ad28936b":"markdown","4b0f1f44":"markdown","b3a591f6":"markdown","8daec212":"markdown","a0150ccf":"markdown","d7161601":"markdown","4c72bf6b":"markdown","01e90bab":"markdown","9ee3ecb2":"markdown","b3da0e7f":"markdown"},"source":{"ae6fdc07":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e3bbf7d8":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# import sklearn lib\nimport sklearn\nfrom sklearn import linear_model, metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import precision_score, recall_score\n\n# import statsmodel lib\nimport statsmodels.api as sm  \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# set options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nnp.seterr(divide='ignore', invalid='ignore')","975e78df":"# read data\ndf = pd.read_csv(\"\/kaggle\/input\/telecomchurndata\/telecom_churn_data.csv\")","5cb1addf":"# check shape of the dataset\ndf.shape","0daa0cd2":"df.head()","d281bfb6":"# check the dtypes of the columns\ndf.info()","cd13ae2b":"df.columns","e856212d":"# get the all the ninth month columns\ncols_9 = [col for col in df.columns if '_9' in col]\nprint(cols_9)","c4a830da":"# to find the number if customer churned as per 'usage based churn'\nlen(df[(df['total_ic_mou_9']==0.0) & (df['total_og_mou_9']==0.0) & (df['vol_2g_mb_9']==0.0) & (df['vol_3g_mb_9']==0.0)])","e460cc2b":"df['target'] = ((df['total_ic_mou_9']==0.0) & (df['total_og_mou_9']==0.0) & (df['vol_2g_mb_9']==0.0) & (df['vol_3g_mb_9']==0.0)).astype(int)","136f2c1e":"# Drop all the columns of 9 th month after deriving the target variable\n\ndf = df.drop(cols_9, axis = 1)","f3715155":"df.shape","5e14bf8d":"# We now have 173 columns\n# Lets do the missing values treatment\n# Find out if there are any missing values\nnull_data = round(100*(df.isnull().sum()\/len(df.index)), 2)\nnull_data[null_data > 0]","0e0671a6":"# Lets drop the columns having more than 70% of missing values\n# Number of columns having more than 70% of missing values\nlen(null_data[null_data > 70])","9fb316bb":"df.shape","03a210f0":"# lets us separate categorical and numerical columns for further analysis\ndf.info()","3dc05c30":"# Lets visualize the numeric data:\n\ndf_numeric = df.select_dtypes(include=['float64','int64','int32'])\ndf_numeric.columns","07192735":"# Lets visualize the categorical data:\n\ndf_categorical = df.select_dtypes(include=['object'])\ndf_categorical.columns","cc369404":"# Lets check the numeric data:\ndf_numeric.describe()","44ae957f":"# lets drop mobile_number as it is the unique_id column not useful in model \ndf = df.drop('mobile_number',axis=1)","821e7a1a":"# Lets drop sep_vbc_3g as it is 9th month data we need to drop\ndf = df.drop('sep_vbc_3g',axis=1)","f86aaf5e":"# Lets check and drop columns which have only 1 unique value\none_unique = df.loc[:, df.apply(lambda x: x.nunique()) == 1]\none_unique.columns","20421bc5":"# lets drop the above columns\ndf = df.drop(one_unique.columns, axis = 1)","b47fd680":"# get numeric columns\ndf_numeric = df.select_dtypes(include=['float64','int64','int32'])\ndf_numeric.columns","76c88af3":"df_numeric.describe()","0344ac9d":"# lets analyse the monthly and satchet data, looks like there are lots of zero values\n\n# monthly_2g_6, monthly_2g_7, monthly_2g_8\n\n# sachet_2g_6, sachet_2g_7, sachet_2g_8\n\n# monthly_3g_6, monthly_3g_7, monthly_3g_8\n\n# sachet_3g_6, sachet_3g_7, sachet_3g_8\n\n# Lets drop all these columns as they also do not provide much variance in the data","59e4468d":"df.monthly_2g_6.value_counts().head()","ea197153":"df.monthly_2g_7.value_counts().head()","477d3110":"df.monthly_2g_8.value_counts().head()","0e085341":"# Almost 92% of customers are not using this service, we can drop the 3 columns column\ndf = df.drop(['monthly_2g_6','monthly_2g_7','monthly_2g_8'],axis = 1)","c21c5b80":"# lets check the satchet 2g  service\ndf.sachet_2g_6.value_counts().head()","5963cb60":"df.sachet_2g_7.value_counts().head()","c33812db":"df.sachet_2g_8.value_counts().head()","3f284765":"# Almost 92% of customers are not using this service, we can drop the 3 columns column\ndf = df.drop(['sachet_2g_6','sachet_2g_7','sachet_2g_8'],axis = 1)","48ef034b":"# lets check the monthly 3g  service\ndf.monthly_3g_6.value_counts().head()","27359a5e":"df.monthly_3g_7.value_counts().head()","20ed1098":"df.monthly_3g_8.value_counts().head()","ac986437":"# Almost >93% data is 0 , service not being used, lets drop the columns \n# monthly_3g_6, monthly_3g_7, monthly_3g_8\ndf = df.drop(['monthly_3g_6', 'monthly_3g_7', 'monthly_3g_8'], axis = 1)","7a0ce2dd":"df.sachet_3g_6.value_counts().head()","27e66586":"df.sachet_3g_7.value_counts().head()","4e5cb33d":"df.sachet_3g_8.value_counts().head()","407591a9":"# Almost >95% data is 0 , service not being used, lets drop the columns \n# sachet_3g_6, sachet_3g_7, sachet_3g_8\ndf = df.drop(['sachet_3g_6', 'sachet_3g_7', 'sachet_3g_8'], axis = 1)","fc9f9e37":"# get numeric columns\ndf_numeric = df.select_dtypes(include=['float64','int64','int32'])\ndf_numeric.columns","95252360":"df_numeric.describe()","107c3df4":"df_numeric.shape","f1fc7d8c":"#Lets check column wise null value percentage for all numeric columns\n\nnull_data = round(100*(df_numeric.isnull().sum()\/len(df_numeric.index)), 2)\nnull_data[null_data > 0]","00282d4c":"# The colums:\n# night_pck_user_6,night_pck_user_7,night_pck_user_8\n# fb_user_6,fb_user_7,fb_user_8\n# Are categorical columns , they have only 2 distinct values\n\ncat_cols = ['night_pck_user_6','night_pck_user_7','night_pck_user_8','fb_user_6','fb_user_7','fb_user_8']","b2ea9c8a":"df[cat_cols] = df[cat_cols].astype('object')","5bf4ad59":"# get numeric columns\ndf_numeric = df.select_dtypes(include=['float64','int64','int32'])\ndf_numeric.columns","ccde428d":"#Lets check column wise null value percentage for numeric columns\n\nnull_data = round(100*(df_numeric.isnull().sum()\/len(df_numeric.index)), 2)\nnull_data[null_data > 0]","cf830427":"# We will impute missing values with 0 for columns:\n# av_rech_amt_data_6 , av_rech_amt_data_7, av_rech_amt_data_8  \n# As customers do not recharge every month, hence those values are missing\navg_rech = ['av_rech_amt_data_6' , 'av_rech_amt_data_7', 'av_rech_amt_data_8']\ndf[avg_rech] = df[avg_rech].fillna(0)","121cab6c":"# lets drop other columns where more than 70% data is missing\n# total_rech_data_6, total_rech_data_7,total_rech_data_8,max_rech_data_6,max_rech_data_7,max_rech_data_8,count_rech_2g_6,\n# count_rech_2g_7,count_rech_2g_8,count_rech_3g_6,count_rech_3g_7,count_rech_3g_8,arpu_3g_6,arpu_3g_7,arpu_3g_8,arpu_2g_6,\n# arpu_2g_7,arpu_2g_8\ndrop_cols = ['total_rech_data_6', 'total_rech_data_7','total_rech_data_8','max_rech_data_6','max_rech_data_7','max_rech_data_8','count_rech_2g_6',\n'count_rech_2g_7','count_rech_2g_8','count_rech_3g_6','count_rech_3g_7','count_rech_3g_8','arpu_3g_6','arpu_3g_7','arpu_3g_8','arpu_2g_6',\n'arpu_2g_7','arpu_2g_8','og_others_6','og_others_7','og_others_8']\n    \ndf = df.drop(drop_cols, axis = 1)","620a9cd3":"# #Lets check column wise null value percentage for numeric columns \ndf_numeric = df.select_dtypes(include=['float64','int64','int32'])\nnull_data = round(100*(df_numeric.isnull().sum()\/len(df_numeric.index)), 2)\nnull_data[null_data > 0]","d212409a":"df_numeric.describe()","cce24827":"# get columns to impute null data\nimpute_cols = list(null_data[null_data > 0].index)","ce5ef98a":"len(impute_cols)","613cd4a0":"# impute missing values\ndf[impute_cols] = df[impute_cols].fillna(df[impute_cols].median())","e1067287":"# numeric columns\ndf_numeric = df.select_dtypes(include=['float64','int64','int32'])\ndf_numeric.columns","ee09e9bf":"# lets check if there is any missing values in numeric columns\nnull_data = round(100*(df_numeric.isnull().sum()\/len(df_numeric.index)), 2)\nnull_data[null_data > 0]","1aae684c":"# No missing values in numeric columns","3fde8089":"# get numeric columns\nnumeric_cols = df_numeric.columns\nnumeric_cols","02937393":"# Find columns with more than 75% zeroes in the numeric columns and drop them\n#df_numeric2 = df_numeric.loc[:, (df_numeric==0).mean() < .75]\n#df_numeric2.columns","7764235c":"# get categorical columns\ndf_categorical = df.select_dtypes(include=['object'])\ndf_categorical.columns","6953a8c8":"# lets concatenate all columns numeric, categorical, and target\n#df = pd.concat([df_numeric2,df_categorical,df['target']], axis=1)","6901a604":"df.shape","7fe52d3d":"df_numeric = df.select_dtypes(include=['float64','int64','int32'])","14cd3ae7":"# get numeric columns\nnumeric_cols = list(df_numeric.columns)","49d0cca6":"len(numeric_cols)","531ba4ef":"# We have 113 numerical columns","752d4c91":"# lets analyse the categorical columns now","495e5162":"df_categorical = df.select_dtypes(include=['object'])","cbd6ef5b":"#Lets check column wise missing value percentage in categorical columns\n\nnull_data = round(100*(df_categorical.isnull().sum()\/len(df_categorical.index)), 2)\nnull_data[null_data > 0]","6d509ac5":"# drop columns : 'date_of_last_rech_data_6','date_of_last_rech_data_7', 'date_of_last_rech_data_8'\n# having more than 70% missing values\ndrop_cols = ['date_of_last_rech_data_6','date_of_last_rech_data_7', 'date_of_last_rech_data_8']\ndf  = df.drop(drop_cols, axis= 1)","48278c48":"# lets replace the missing values with mode\nimpute_cols = ['date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8']\ndf[impute_cols] = df[impute_cols].fillna(df[impute_cols].mode().iloc[0])","008a38cd":"# these are monthly data hence derive new columns and as week of the month\n# date_of_last_rech_6,date_of_last_rech_7,date_of_last_rech_8\n# formatting data and converting the date into week of the month\ndf.date_of_last_rech_6 = pd.to_datetime(df.date_of_last_rech_6, format='%m\/%d\/%Y')\ndf.date_of_last_rech_6 = df.date_of_last_rech_6.apply(lambda d: 'week_'+str((d.day-1) \/\/ 7 + 1)).astype(object)","f94bad37":"df.date_of_last_rech_6.value_counts()","896982cc":"# formatting data and converting the date into week of the month\ndf.date_of_last_rech_7 = pd.to_datetime(df.date_of_last_rech_7, format='%m\/%d\/%Y')\ndf.date_of_last_rech_7 = df.date_of_last_rech_7.apply(lambda d: 'week_'+str((d.day-1) \/\/ 7 + 1)).astype(object)","39c84ee0":"df.date_of_last_rech_7.value_counts()","4006b004":"# formatting data and converting the date into week of the month\ndf.date_of_last_rech_8 = pd.to_datetime(df.date_of_last_rech_8, format='%m\/%d\/%Y')\ndf.date_of_last_rech_8 = df.date_of_last_rech_8.apply(lambda d: 'week_'+str((d.day-1) \/\/ 7 + 1)).astype(object)","bac7bc28":"df.date_of_last_rech_8.value_counts()","23d17829":"df.night_pck_user_6.value_counts()","d1b52454":"df.fb_user_6.value_counts()","197d0766":"# For columns 'night_pck_user_6','night_pck_user_7','night_pck_user_8','fb_user_6','fb_user_7','fb_user_8'\n\n# fill missing values with -1 out of sample imputation\nimpute_cols = ['night_pck_user_6','night_pck_user_7','night_pck_user_8','fb_user_6','fb_user_7','fb_user_8'] \ndf[impute_cols] = df[impute_cols].fillna(-1).astype(object)","8df7c68d":"# Lets check overall missing data\nnull_data = round(100*(df.isnull().sum()\/len(df.index)), 2)\nnull_data[null_data > 0]\n","3b799ada":"# No null values","de33ae05":"#Lets filter the data set to retain only the high value customers","f96d6b0e":"df.shape","b6e4f4e2":"df.describe()","8abd53ed":"df['total_amt'] = df.total_rech_amt_6 + df.total_rech_amt_7 + (df.av_rech_amt_data_6 * df.total_rech_num_6) + (df.av_rech_amt_data_7 * df.total_rech_num_7)","663d2470":"# To filter the high value customers , retain only those rows where df['total_amt'] percentile >= 70%\npercentile_70th = df.total_amt.quantile(.70)\ndf = df[df.total_amt >= percentile_70th]","0c2e780b":"df.shape","f23505c0":"# After filtering the high value customers we are left with 30010 rows and 123 columns","461b34fa":"numeric_cols.append('total_amt')\nlen(numeric_cols)","bd8c199d":"# Lets visulaize the data by doing some Univariate and Bivariate Analysis","0c8f202a":"# lets plot some scatter plot with some numeric columns and the total_amt amount column","92aabeca":"# lets define some utility methods to plot graphs\n\ndef pair_plot(data, x_vars):\n    plt.figure(figsize=(8,6))\n    sns.pairplot(df, x_vars=x_vars, \n                 y_vars='total_amt',size=3, aspect=1, kind='scatter')\n    plt.title('Pair plot with total_amt vs '+ ','.join(x_vars[1:]),fontsize=15)\n    plt.show()","c99ae6be":"# Lets plot scatter plots for few numeric columns\nx_vars=['total_amt','arpu_7', 'onnet_mou_7', 'offnet_mou_7', 'loc_og_t2t_mou_7', 'loc_og_t2m_mou_7']\npair_plot(df, x_vars)\n","c3716413":"# Lets plot scatter plots for few numeric columns\nx_vars=['total_amt','loc_og_t2f_mou_7', 'loc_og_mou_7', 'std_og_t2t_mou_7', 'std_og_t2m_mou_7', 'std_og_mou_7']\npair_plot(df, x_vars)","516d2914":"# Lets plot scatter plots for few numeric columns\nx_vars=['total_amt','spl_og_mou_7', 'total_og_mou_7', 'loc_ic_t2t_mou_7', 'loc_ic_t2m_mou_7', 'loc_ic_t2f_mou_7']\npair_plot(df, x_vars)","722542bb":"# Lets plot scatter plots for few numeric columns\nx_vars=['total_amt','loc_ic_mou_7', 'std_ic_t2t_mou_7', 'std_ic_t2m_mou_7', 'std_ic_mou_7', 'total_ic_mou_7']\npair_plot(df, x_vars)","7a963736":"# Lets plot scatter plots for few numeric columns\nx_vars=['total_amt','total_rech_num_7', 'total_rech_amt_7', 'max_rech_amt_7', 'last_day_rch_amt_7', 'av_rech_amt_data_7']\npair_plot(df, x_vars)","1af826cf":" # Lets write a utility function to plot box_plots\ndef box_plots(data, values):\n    # Set the width and height of the figure\n    plt.figure(figsize=(25, 20))\n    for i,val in enumerate(values):      \n\n        plt.subplot(4,3,i+1)\n        sns.boxplot(y = val, data = data)\n        # Add labels\n        plt.xlabel(val)\n        # Add plot title\n        plt.title('Boxplot for '+val, fontsize=14)\n\n    plt.show()\n   ","6bd3329b":"# Box plots for 'arpu_7', 'onnet_mou_7', 'offnet_mou_7', 'loc_og_t2t_mou_7', 'loc_og_t2m_mou_7', \n#'loc_og_t2f_mou_7', 'loc_og_mou_7', 'std_og_t2t_mou_7', 'std_og_t2m_mou_7', 'std_og_mou_7', \n#'spl_og_mou_7', 'total_og_mou_7'\n    \ncols = ['arpu_7', 'onnet_mou_7', 'offnet_mou_7', 'loc_og_t2t_mou_7', 'loc_og_t2m_mou_7', \n'loc_og_t2f_mou_7', 'loc_og_mou_7', 'std_og_t2t_mou_7', 'std_og_t2m_mou_7', 'std_og_mou_7', \n'spl_og_mou_7', 'total_og_mou_7']\nbox_plots(df,cols)","eb62ff7d":"# Box plots for 'loc_ic_t2t_mou_7', 'loc_ic_t2m_mou_7', 'loc_ic_t2f_mou_7', \n#'loc_ic_mou_7', 'std_ic_t2t_mou_7', 'std_ic_t2m_mou_7', 'std_ic_mou_7', 'total_ic_mou_7', \n#'total_rech_num_7', 'total_rech_amt_7', 'max_rech_amt_7', 'last_day_rch_amt_7'\n    \ncols = ['loc_ic_t2t_mou_7', 'loc_ic_t2m_mou_7', 'loc_ic_t2f_mou_7', \n'loc_ic_mou_7', 'std_ic_t2t_mou_7', 'std_ic_t2m_mou_7', 'std_ic_mou_7', 'total_ic_mou_7', \n'total_rech_num_7', 'total_rech_amt_7', 'max_rech_amt_7', 'last_day_rch_amt_7']\nbox_plots(df,cols)","56b2f2c3":"# Box plots for 'av_rech_amt_data_7','aon','total_amt'  \ncols = ['av_rech_amt_data_7','aon','total_amt']\nbox_plots(df,cols)","d3c45bbb":"# Lets do some outlier analysis using percentiles:\ndf[numeric_cols].describe(percentiles=[.25, .5, .75, .90, .95, .99])","0ea281ee":"# Outlier treatment\n# Lets cap columns where the ratio max\/99th value is greater than 2\n# Lets write a utility to get the column names\ndef cap_outliers(df,cols):\n    \n    for col in cols:\n        # get max value\n        col_max = df[col].max()\n        # get 99th percentile\n        col_99th = df[col].quantile(.99)\n        # calculate ratio\n        ratio = col_max\/col_99th\n        # compare and cap\n        if ratio > 2:\n            print(col, round(ratio,2))\n            df[col] = df[col].apply(lambda x : col_99th if x > col_99th else x)","ebbda787":"cap_outliers(df,numeric_cols)","457aa0c0":"df.shape","1cd6908d":"# Since there are too many numeric columns we will plot heatmap in 3 parts\n# Lets plot a heatmap for all the numeric columns\n# plotting correlations on a heatmap","ec6a4ecc":"# First half\nsize = len(numeric_cols)\ncols_1 = numeric_cols[0:size\/\/3]\ncols_2 = numeric_cols[(size\/\/3):2*(size\/\/3)]\ncols_3 = numeric_cols[2*(size\/\/3):-1]","dcfc56b4":"\n# figure size\nplt.figure(figsize=(16,8))\n\n# heatmap\nsns.heatmap(df[cols_1].corr(), cmap=\"Greens\", annot=True)\nplt.show()","4d735e5a":"\n# figure size\nplt.figure(figsize=(16,8))\n\n# heatmap\nsns.heatmap(df[cols_2].corr(), cmap=\"Greens\", annot=True)\nplt.show()","06a12482":"\n# figure size\nplt.figure(figsize=(16,8))\n\n# heatmap\nsns.heatmap(df[cols_3].corr(), cmap=\"Greens\", annot=True)\nplt.show()","b7831133":"# Lets plot some count plots for the categorical data","3a23cb25":" # Lets write a utility function to plot dist_plots\ndef count_plots(data, values):\n    # Set the width and height of the figure\n    plt.figure(figsize=(25, 20))\n    for i,val in enumerate(values):      \n\n        plt.subplot(4,3,i+1)\n        sns.countplot(x = val, data = data)\n        # Add labels\n        plt.xlabel(val)\n        # Add plot title\n        plt.title('Countplot for '+val, fontsize=14)\n\n    plt.show()","99cde921":"# Count plots for 'date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8','night_pck_user_6', 'night_pck_user_7',\n       #'night_pck_user_8', 'fb_user_6', 'fb_user_7', 'fb_user_8'\ncat_cols = ['date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8','night_pck_user_6', 'night_pck_user_7',\n       'night_pck_user_8', 'fb_user_6', 'fb_user_7', 'fb_user_8']\ncount_plots(df,cat_cols)","35fb8979":"# get categorical columns\ndf_categorical = df.select_dtypes(include=['object'])\ndf_categorical.columns","983aa9e9":"# getting dummy variables for all the categorical columns\ndf_dummies_cat = pd.get_dummies(df_categorical,drop_first=True)\ndf_dummies_cat.head()","07f6158a":"# drop categorical variables \ndf = df.drop(list(df_categorical.columns), axis=1)","1e4ebd0e":"# concatenate the dummies with the main df\ndf = pd.concat([df, df_dummies_cat], axis=1)","36b9a9d7":"df.shape","32026aee":"df.info()","db7659ac":"# 138 columnns after dummy variable creation","c1e97d54":"numeric_cols","dee4f7a2":"# split into X and y\ny = df.pop('target') # predictors in variable y\n\nX = df # response variable in X\n","62bbd6ed":"# lets do a test train split\nnp.random.seed(0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=100)","781fa4cb":"# Lets scale the numeric columns except the target variable\nnumeric_cols.remove('target')\n\nX_train_numeric = X_train[numeric_cols]","fcd96ad3":"X_train_numeric.columns","589e15c5":"# Scaling the numeric variables using StandardScaler\nscaler = StandardScaler()\nX_train[X_train_numeric.columns] = scaler.fit_transform(X_train[X_train_numeric.columns])\nX_test[X_train_numeric.columns] = scaler.transform(X_test[X_train_numeric.columns])","8205c15f":"X_train.shape","73cea4d7":"X_test.shape","03734c88":"X_train.head()","cf7db68e":"y_train.head()","41fcbdc5":"y_test.head()","0fb84cd8":"# to get 90% of variance\npca = PCA(random_state=100, n_components=0.90)\nX_train_reduced = pca.fit_transform(X_train)\npca.explained_variance_ratio_","d0aeb846":"# cummulative sum \nvar_cumu = np.cumsum(pca.explained_variance_ratio_)","8ff42014":"# Lets plot a graph for Explained Cummulative variance\nfig = plt.figure(figsize=[12,8])\nplt.vlines(x=15, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=0.95, xmax=30, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.plot(var_cumu)\nplt.ylabel(\"Cumulative variance explained\")\nplt.show()","e0bcbf0c":"X_train_reduced.shape","2ab8029d":"# Lets transfrom the test data using pca model\nX_test_reduced = pca.transform(X_test)\nX_test_reduced.shape","3b73ef80":"# Random Forest Classifier\nrf = RandomForestClassifier(random_state=100, oob_score=True,class_weight='balanced', n_jobs=-1)","4bfbac5f":"# hyper parameter tuning\nparams = {\n    'max_depth': [2,3,5,10,20],\n    'min_samples_leaf': [5,10,20,50,100,200],\n    'n_estimators': [10, 25, 50, 100]\n}","c8957b68":"# Using GridSearchCV to get best score\ngrid_search = GridSearchCV(estimator=rf,\n                           param_grid=params,\n                           cv = 5,\n                           n_jobs=-1, verbose=1, scoring=\"accuracy\")","d09e6b0c":"%%time\ngrid_search.fit(X_train_reduced, y_train)","0f1ab27a":"# Best score \ngrid_search.best_score_","9e2fdc3c":"# Getting the classifier with the best score\nrf_best = grid_search.best_estimator_\nrf_best","850bf8bb":"# Lets plot ROC_AUC curve to the check the model goodness\nplot_roc_curve(rf_best, X_train_reduced, y_train)\nplt.show()","c742ab10":"## Lets write a utility function to evaluate the model performance","4f452e7b":"def evaluate_model(classifier,X_train,y_train,X_test,y_test):\n    print(\"Train Accuracy :\", accuracy_score(y_train, classifier.predict(X_train)))\n    print(\"Train Confusion Matrix:\")\n    print(confusion_matrix(y_train, classifier.predict(X_train)))\n    print(\"-\"*50)\n    print(\"Test Accuracy :\", accuracy_score(y_test, classifier.predict(X_test)))\n    print(\"Test Confusion Matrix:\")\n    print(confusion_matrix(y_test, classifier.predict(X_test)))","f5f88ee6":"evaluate_model(rf_best,X_train_reduced,y_train,X_test_reduced,y_test)","8282dd3a":"X_train.shape","3bb980fb":"# Using RFE to get the top 50 features","9e0424a6":"logreg = LogisticRegression(solver='liblinear')\nrfe = RFE(logreg, 50)             # running RFE with 50 variables as output\nrfe = rfe.fit(X_train, y_train)","e1d7a290":"rfe.support_","9e9eab40":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","00224b25":"# top 50 columns suggested by RFE\nrfe_cols = X_train.columns[rfe.support_]","f3b50c6d":"X_train_sm = sm.add_constant(X_train[rfe_cols])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","29e07641":"# Lets check VIF","d32f146b":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nX = X_train[rfe_cols]\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","9c5108cf":"# Random forest classifier\nrf2 = RandomForestClassifier(random_state=100, oob_score=True,class_weight='balanced', n_jobs=-1)","3bf649df":"# hyper parameter tuning\nparams = {\n    'max_depth': [2,3,5,10,20],\n    'min_samples_leaf': [5,10,20,50,100,200],\n    'n_estimators': [10, 25, 50, 100]\n}","0d97d93b":"# GridSearchCV to find the best classifier\ngrid_search = GridSearchCV(estimator=rf2,\n                           param_grid=params,\n                           cv = 5,\n                           n_jobs=-1, verbose=1, scoring=\"roc_auc\")","88f0ee08":"# We will train the  RFC classifier with the columns suggested by RFE\nX_train_new = X_train[rfe_cols]","ea718b9e":"X_train_new.shape","4b6ad81b":"# RFE suggested columns for test data\nX_test_new = X_test[rfe_cols]","0d135bf6":"X_test_new.shape","e6313f05":"%%time\ngrid_search.fit(X_train_new, y_train)","a3f79248":"# best score for RFC model\ngrid_search.best_score_","a144aaaa":"# Best classifier\nrf_best = grid_search.best_estimator_\nrf_best","06f49df2":"# Lets plot the ROC AUC curve\nplot_roc_curve(rf_best, X_train_new, y_train)\nplt.show()","d6149b2c":"# lets evaluate the model performance\nevaluate_model(rf_best,X_train_new,y_train,X_test_new,y_test)","744bdfca":"# lets get the features based on their feature importances\nrf_best.feature_importances_","d4c847a3":"# Lets put the features and its importances in a Dataframe , to make it easier to sort the top 10 columns\nimp_df = pd.DataFrame(index=X_train_new.columns)\nimp_df.rows = X_train_new.columns\nimp_df['Imp'] = rf_best.feature_importances_","7af58536":"imp_df.sort_values(by=\"Imp\", ascending=False).head(10)","ee0a6615":"top_10_cols = list(imp_df.sort_values(by=\"Imp\", ascending=False).index[0:10])","b23e4ad5":"top_10_cols","f34d23bd":"# Lets build Logistic Regression Model using the top 10 cols","6b578ba6":"X_train_new2 = X_train[top_10_cols] ","8f0bf7c7":"X_train_sm = sm.add_constant(X_train_new2)\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","1604488e":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nX = X_train_new2\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","0258c7f5":"# Lets drop column 'loc_og_t2m_mou_8' since it has high p-value\ntop_10_cols.remove('loc_og_t2m_mou_8')\ntop_10_cols","c0ec087d":"X_train_new2 = X_train[top_10_cols] ","91dcb155":"X_train_sm = sm.add_constant(X_train_new2)\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","cea2621a":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nX = X_train_new2\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","03ff8f35":"# Lets drop column 'loc_ic_mou_8' because the VIF > 5\ntop_10_cols.remove('loc_ic_mou_8')\ntop_10_cols","ea2fac38":"X_train_new2 = X_train[top_10_cols] ","caf4b77c":"X_train_sm = sm.add_constant(X_train_new2)\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","bb035813":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nX = X_train_new2\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","fe0c6ec4":"# Lets predict on train set\ny_train_pred = res.predict(X_train_sm).values.reshape(-1)","6c23a5cb":"y_train_pred[:10]","c2b68665":"# Lets make the dataframe with predicted values for better evaluation\ny_train_pred_final = pd.DataFrame({'Churn':y_train.values, 'Churn_Prob':y_train_pred})\ny_train_pred_final['CustID'] = y_train.index\ny_train_pred_final.head()","3cce10b7":"# Lets make predictions based on random cut-off of 0.5\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","06d1c33d":"# Lets check the Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nprint(confusion)","59544281":"# Predicted     not_churn    churn\n# Actual\n# not_churn     19158       109\n# churn         1215        525  ","6b5f5862":"# Let's check the overall accuracy.\nprint(\"Train Accuracy score:\",round(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted),2))","b1652e39":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","de32518c":"# Let's see the sensitivity of our logistic regression model\nprint('Train Sensitivity Score:',round(TP \/ float(TP+FN),2))","1aa95f73":"# Let us calculate specificity\nprint('Train Specificity Score:',round(TN \/ float(TN+FP),2))","0a99f272":"# Lets write a utitlity function to plot the ROC AUC curve\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False)\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","2eb340ce":"# Lets calculate FPR , TPR and Thresholds\nfpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Churn, y_train_pred_final.Churn_Prob, drop_intermediate = False )","9a7264cb":"# Lets plot the ROC AUC score\ndraw_roc(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","d8f2f1bb":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","48266c4f":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","2512f902":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","2a0c53a3":"# Looking at the above plot we decide the cut-off as 0.1\n\ny_train_pred_final['final_predicted'] = y_train_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.1 else 0)\n\ny_train_pred_final.head()","305d3aec":"# Let's check the overall accuracy.\nprint(\"Train Accuracy score:\",round(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted),2))","fc04d3c1":"# Lets get the confusion matrix\nconfusion2 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.final_predicted )\nconfusion2","a9a4429a":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","d7120f2c":"# Let's see the sensitivity of our logistic regression model\nprint('Train Sensitivity Score:',round(TP \/ float(TP+FN),2))","1d6d3ab3":"# Let us calculate specificity\nprint('Train Specificity Score:',round(TN \/ float(TN+FP),2))","f8d2b04b":"print('Train Precision Score:',round(precision_score(y_train_pred_final.Churn, y_train_pred_final.predicted),2))","4aea3b51":"print('Train Recall Score:',round(recall_score(y_train_pred_final.Churn, y_train_pred_final.predicted),2))","2b570ccb":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","d05656c5":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","d278b247":"# Lets make prediction on the test data\nX_test_new = X_test[top_10_cols]\nX_test_new.head()","8c132a11":"X_test_sm = sm.add_constant(X_test_new)","10752a18":"# Predicting on test data\ny_test_pred = res.predict(X_test_sm)","5f1e2383":"# getting the predicted data on test in a data frame for better visibilty\ny_test_pred_final = pd.DataFrame({'Churn':y_test.values, 'Churn_Prob':y_test_pred})\ny_test_pred_final['CustID'] = y_test.index\ny_test_pred_final.head()","7c301ad7":"# Usig the same cut-off as used for the train set\ny_test_pred_final['final_predicted'] = y_test_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.35 else 0)","71d293d8":"y_test_pred_final.head()","fa24d7bd":"# Let's check the overall accuracy.\nprint(\"Test Accuracy score:\",round(metrics.accuracy_score(y_test_pred_final.Churn, y_test_pred_final.final_predicted),2))","75e51749":"# Lets check the confusion matrix\nconfusion2 = metrics.confusion_matrix(y_test_pred_final.Churn, y_test_pred_final.final_predicted )\nconfusion2","558aa037":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","f5b3b562":"# Let's see the sensitivity of our logistic regression model\nprint('Test Sensitivity Score:',round(TP \/ float(TP+FN),2))","b2acfca5":"# Let us calculate specificity\nprint('Test Specificity Score:',round(TN \/ float(TN+FP),2))","d92960dd":"print('Test Precision Score:',round(precision_score(y_test_pred_final.Churn, y_test_pred_final.final_predicted),2))","20959697":"print('Test Recall Score:',round(recall_score(y_test_pred_final.Churn, y_test_pred_final.final_predicted),2))","e75bd5f2":"res.summary()","28a2d372":"#### Filter high value customers","cc38ec8e":"## Lets build a Predictive Model Using PCA and Random Forest Classifier","2fd40167":"#### loc_og_t2m_mou_8 has very high p-value","f908aeb9":"#### Here also we get the very good test and train accuracy score","bc444ac5":"#### Looking at the above graph we see that the optimal cutoff would be 0.35","d283f1d6":"#### Splitting data into X and y","15e39819":"#### preserving the numeric columns in a list","6e36451d":"## Data Preparation for Modelling","56fba1fa":"#### Lets apply Random forest Clasifier to get the top 10 columns","c0c8cb02":"##### Recall\nTP \/ TP + FN","7e4b6607":"#### Understanding the Data : Data Visualization for Categorical data","04044659":"#### Some useful insights from above heatmap:\n\n- The numeric columns having strong correlation among each other\n\n- Columns highly correlated with each other :\n\n    - Postive correlation\n       + 'onnet_mou_6','onnet_mou_7','onnet_mou_8','std_og_t2t_mou_7','std_og_t2t_mou_7','std_og_t2t_mou_8'\n       + 'std_og_t2m_mou_6','std_og_t2m_mou_7','offnet_mou_6','offnet_mou_7','offnet_mou_8'\n       + 'loc_og_mou_6','loc_og_mou_7','loc_og_mou_8','loc_og_t2t_mou_7','loc_og_t2t_mou_8',\n            'loc_og_t2m_mou_6','loc_og_t2m_mou_7','loc_og_t2m_mou_8\n       + 'roam_ic_mou_6','roam_ic_mou_7','roam_ic_mou_8',\n","ce2296e3":"### Logistic Regression Model","b2741ee5":"#### We will plot scatter plots for all numeric columns in batches for better visulization","7711b75d":"#### binnig the date values to week1, week2, week3, week4 etc","281563b5":"#### Lets use PCA and find high variance components","e0d8e098":"### Lets Make Predictions","9363d309":"### Dummy variables for the categorical data","7bd01182":"### Missing value treatment","05fdb427":"### Predictions with Optimal Cut-off","dffb7908":"### Predictions with Random Cut-Off","4a30a329":"##### Precision\nTP \/ TP + FP","613b1e41":"### Making predictions on the test set","eacfc34a":"#### Lets check the VIF ","f086923f":"#### Impute missing data in numeric columns with median","53d897d1":"#### Lets build a Random Forest Model with Hyper-parameter tuning with the above reduced train data","1740ca1e":"#### looks like there is lot of multicollinearity","4e0dd7ad":"#### The Top 8 predictor variables are :\n\n+ total_ic_mou_8     ->         -3.3163\n+ last_day_rch_amt_8 ->         -0.4209\n+ total_og_mou_8     ->         -0.7451\n+ av_rech_amt_data_8 ->         -0.3218\n+ loc_ic_t2m_mou_8   ->          1.3164\n+ arpu_8             ->          0.2832\n+ loc_og_mou_8       ->         -0.1834\n+ fb_user_8_1.0      ->         -1.7700","c28f7065":"### Feature Engineering","c3dd3e4c":"#### Lets check VIF","53f3e0ed":"#### Build another Logistic Model","fd289ca1":"#### drop date columns with >=70% mising values","1f809678":"#### p-values are all fine","98ff24c6":"## Explanatory Model","29b8c6e7":"## Data Exploration","4d387778":"### Data Visualization","fb5d24e6":"### Finding Optimal Cutoff Point","251a7f45":"An ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","d78518ca":"#### Lets plot some boxplots to check outliers","df54cbf2":"## Precision and Recall","ebc871f9":"#### dropping columns where > 70% data is missing","9572fcef":"#### drop all the 9th month columns, since the target variable has been derived","349a2ce1":"#### All VIF values are also fine < 5, there is no more multicollinearity","a58adb9a":"#### get total amount spend by customers in the 6th and 7th month","f2a59d5a":"#### Lets build another Explanatory Model using RFE then Random Forest Classifier and finally get variable significance using Logistic Regression","b0f3529f":"#### Out of sample imputation for columns 'night_pck_user_6','night_pck_user_7','night_pck_user_8','fb_user_6','fb_user_7','fb_user_8'","5422fc64":"#### Categorical data looks fine, lets move ahead with Data Preparation","66b91ea6":"#### Some useful insights from above heatmap:\n\n- The numeric columns having strong correlation among each other\n\n- Columns highly correlated with each other :\n\n    - Postive correlation\n       + 'total_og_mou_6','total_og_mou_7','total_og_mou_8','std_og_mou_6','std_og_mou_7','std_og_mou_8'","1caa099d":"## Data Cleaning","2d182326":"#### Outlier Treatment","46f1db2e":"#### Some useful insights from above heatmap:\n\n- The numeric columns having strong correlation among each other\n\n- Columns highly correlated with each other :\n\n    - Postive correlation\n       + 'std_ic_mou_6','std_ic_mou_7','std_ic_mou_8','std_ic_t2m_mou_6','std_ic_t2m_mou_7','std_ic_t2m_mou_8'","fe0e0e26":"#### Lets check the VIF for multicollinearity","3141df9f":"### Lets Analyse Numeric and Categorical variables separately","33c649ca":"#### The test and train accuracy score is quite high","91840406":"### Plotting the ROC Curve","28cc9992":"#### Lets drop columns having only 1 unique value, as it does not provide any data variance","b720b713":"#### get all the 9th month columns to derive the target variable","5ddbb56e":"### Precision and recall tradeoff","5141c47b":"#### Lets plot the Heatmap to check correlations","5d618115":"#### Lets plot countplots for the categorical data","01abd713":"#### Looking at the scatter plots above we see that the numerical data is not that much skewed","839e8b3a":"#### All p-values are within .05 hence all the variable are significant","ad28936b":"# Telecom Churn Logistic Regression","4b0f1f44":"#### Understanding the Data : Data Visualization for Numeric data","b3a591f6":"#### number of customers churned","8daec212":"#### Metrics beyond simply accuracy","a0150ccf":"#### Lets build Logistic Regression model again","d7161601":"#### Deriving the target variable","4c72bf6b":"#### Test Train split","01e90bab":"#### impute missing dates with mode","9ee3ecb2":"### Lets get the top 10 columns","b3da0e7f":"#### Assessing the model with StatsModels"}}