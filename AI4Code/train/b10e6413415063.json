{"cell_type":{"41508390":"code","ebe5116f":"code","ebd8b0d3":"code","a5d3a1ce":"code","1b11a667":"code","63487da9":"code","e1ea58ce":"code","c7e6c026":"code","b43374a7":"code","29a31329":"code","2773c182":"code","bb0ca877":"code","5be220bc":"code","6be9307f":"code","511c009f":"code","cc4599c7":"code","b86bb5f0":"code","a8c84044":"code","6636aa5e":"code","90ce6c99":"code","98cad613":"code","298bebc1":"code","df8246cc":"code","3dc3262c":"code","141a870c":"code","cc2f9f1e":"code","56cd7eab":"code","d10dcc6d":"code","d2261bc3":"code","e7c7223c":"code","1b79a82d":"code","6adf477f":"code","647ea69e":"code","a928e3a2":"markdown","29516e03":"markdown","b4ade6b7":"markdown","909fc7a4":"markdown","bcd60e2d":"markdown","57cfdd02":"markdown","570abbe5":"markdown","d2aefc23":"markdown","e1e62784":"markdown","1674e4fc":"markdown","5c112161":"markdown","3e0b13b8":"markdown","a6bc761c":"markdown","a35e0741":"markdown","28804c45":"markdown","479a7fab":"markdown","fa18a8cc":"markdown","cdfeb5d6":"markdown","40e7c806":"markdown","1aa1fcf8":"markdown","2c554717":"markdown","380951ab":"markdown","39f9ff4f":"markdown","d76b7d17":"markdown","6f2b8c61":"markdown","919879bb":"markdown","5d11fa65":"markdown","8f26f326":"markdown","a8acecdb":"markdown","9b977d6d":"markdown","974004b3":"markdown"},"source":{"41508390":"# Install ray\n! pip install tune-sklearn ray[tune]","ebe5116f":"import numpy as np\nimport csv\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\n\nimport sys\nnp.set_printoptions(threshold=sys.maxsize)","ebd8b0d3":"# Import our data set from directory\ndirectory = \"\/kaggle\/input\/airline-passenger-satisfaction\/\"\nfeature_tables = ['train.csv', 'test.csv']\n\ndf_train = directory + feature_tables[0]\ndf_test = directory + feature_tables[1]\n\ndf = pd.read_csv(df_train, index_col=0)\ndf2 = pd.read_csv(df_test, index_col=0)","a5d3a1ce":"df.head()","1b11a667":"df.drop('id', inplace=True, axis=1)\ndf2.drop('id', inplace=True, axis=1)\n\n# Show one of them\ndf.head()","63487da9":"df.isnull().sum()","e1ea58ce":"df2.isnull().sum()","c7e6c026":"categorical = ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'satisfaction']\n\nmaxDistinct = 0\nfor column in categorical:\n    distinctRows1 = df[column].nunique()\n    distinctRows2 = df2[column].nunique()\n    if distinctRows1 > maxDistinct:\n        maxDistinct = distinctRows1\n    if distinctRows2 > maxDistinct:\n        maxDistinct = distinctRows2\n\nprint('The maximum number of distinct values of a categorical feature is', maxDistinct)","b43374a7":"# Categorical to numerical in train set\nfor column in categorical:\n    numerical = pd.get_dummies(df[column], drop_first=True)\n    df = pd.concat([df, numerical], axis=1)\n    df.drop(column, inplace=True, axis=1)\n\ndf.head()","29a31329":"# Categorical to numerical in test set\nfor column in categorical:\n    numerical = pd.get_dummies(df2[column], drop_first=True)\n    df2 = pd.concat([df2, numerical], axis=1)\n    df2.drop([column], inplace=True, axis=1)\n\ndf2.head()","2773c182":"# Convert pandas to numpy array for processing\ntrain = df.values\ntest = df2.values","bb0ca877":"# Use imputer to replace our missing values with the mean of the corresponding column of train dataset\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\ntrain = imputer.fit_transform(train)\ntest = imputer.transform(test)\n\ncount = 0 \n\nsamples = train.shape[0]\nfor i in range(samples):\n    if np.isnan(train[i,17]):\n        count += 1\n        \nprint(count, \"missing values\")","5be220bc":"# Seperate labels from features. Label is our last column\nidx_of_label = train.shape[1] - 1\n\nX_train =  train[:, :idx_of_label]\ny_train = train[:, idx_of_label]\n\nX_test =  test[:, :idx_of_label]\ny_test = test[:, idx_of_label]\n\n# Final data shapes\nprint(X_train.shape)\nprint(y_train.shape)","6be9307f":"from sklearn.metrics import accuracy_score","511c009f":"import time\nimport math\n\n# Save the results here\nout_of_the_box_results = np.empty(shape=(3, ))","cc4599c7":"from sklearn.dummy import DummyClassifier\n\n# Ignore warnings\nimport warnings \nwarnings.filterwarnings('ignore')\n\nclf = DummyClassifier()\n\nfit_start = time.time()\nclf.fit(X_train, y_train)\nfit_end = time.time() \n\npredict_start = time.time()\npreds = clf.predict(X_test)\npredict_end = time.time()\n\nacc = accuracy_score(y_test, preds) * 100\nout_of_the_box_results[0] = acc\n\nfit_time = math.ceil(fit_end - fit_start)\npredict_time = math.ceil(predict_end - predict_start)\n\nprint('Out-of-the-box dummy classifier')\nprint('Fit time: {} min {} sec'.format(fit_time \/\/ 60, fit_time % 60))\nprint('Predict time: {} min {} sec'.format(predict_time \/\/ 60, predict_time % 60))\nprint('Total time: {} min {} sec'.format( (fit_time + predict_time) \/\/ 60, (fit_time + predict_time) % 60 ))\nprint('Accuracy: {:.2f} %'.format(acc))","b86bb5f0":"from sklearn.neural_network import MLPClassifier\n\nclf = MLPClassifier()\n\nfit_start = time.time()\nclf.fit(X_train, y_train)\nfit_end = time.time() \n\npredict_start = time.time()\npreds = clf.predict(X_test)\npredict_end = time.time()\n\nacc = accuracy_score(y_test, preds) * 100\nout_of_the_box_results[1] = acc\n\nfit_time = math.ceil(fit_end - fit_start)\npredict_time = math.ceil(predict_end - predict_start)\n                    \nprint('Out-of-the-box MLP classifier')\nprint('Fit time: {} min {} sec'.format(fit_time \/\/ 60, fit_time % 60))\nprint('Predict time: {} min {} sec'.format(predict_time \/\/ 60, predict_time % 60))\nprint('Total time: {} min {} sec'.format( (fit_time + predict_time) \/\/ 60, (fit_time + predict_time) % 60 ))\nprint('Accuracy: {:.2f} %'.format(acc))","a8c84044":"from sklearn.svm import SVC # \"Support vector classifier\"\n\nclf = SVC()\n\nfit_start = time.time()\nclf.fit(X_train, y_train)\nfit_end = time.time() \n\npredict_start = time.time()\npreds = clf.predict(X_test)\npredict_end = time.time()\n\nacc = accuracy_score(y_test, preds) * 100\nout_of_the_box_results[2] = acc\n\nfit_time = math.ceil(fit_end - fit_start)\npredict_time = math.ceil(predict_end - predict_start)\n\nprint('Out-of-the-box SVM classifier')\nprint('Fit time: {} min {} sec'.format(fit_time \/\/ 60, fit_time % 60))\nprint('Predict time: {} min {} sec'.format(predict_time \/\/ 60, predict_time % 60))\nprint('Total time: {} min {} sec'.format( (fit_time + predict_time) \/\/ 60, (fit_time + predict_time) % 60 ))\nprint('Accuracy: {:.2f} %'.format(acc))","6636aa5e":"out_of_the_box_df = pd.DataFrame(out_of_the_box_results,\n                                 ['Dummy','MLP', 'SVM'],\n                                 ['Accuracy'])\nout_of_the_box_df","90ce6c99":"import matplotlib.pyplot as plt\n\nx = ['Dummy','MLP', 'SVM']\nx_axis = np.arange(len(x))\nplt.figure(figsize=(10, 5))\n\nplt.bar(x_axis, out_of_the_box_results, 0.2)\nplt.xticks(x_axis, x)\nplt.xlabel('Classifiers')\nplt.ylabel('Accuracy')\nplt.title('Out-of-the-box metrics')\nplt.show()","98cad613":"train_variance = X_train.var(axis=0)\nprint('The variance of our features is:', train_variance)","298bebc1":"no_of_satisfied = np.sum(y_train, axis=0)\nsamples = y_train.shape[0]\nprint('The percentage of satisfied customers is: {:.2f} %'.format((no_of_satisfied \/ samples) * 100))\nprint('The percentage of dissatisfied customers is: {:.2f} %'.format((1 - (no_of_satisfied \/ samples)) * 100))","df8246cc":"from imblearn.pipeline import Pipeline\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\nfrom tune_sklearn import TuneGridSearchCV\nimport logging\n\nselector = VarianceThreshold()\nscaler = StandardScaler()\npca = PCA()\n\n# These are the pipeline architectures I want to test out and optimize\n# Afterwards, I will find the best of all of the optimized ones\nsteps = [\n    [],\n    [('selector', selector)],\n    [('scaler', scaler)],\n    [('pca', pca)],\n    [('selector', selector), ('scaler', scaler)],\n    [('selector', selector), ('pca', pca)],\n    [('scaler', scaler), ('pca', pca)],\n    [('selector', selector), ('scaler', scaler), ('pca', pca)]\n]\n\n# Creating the grid of values that \u0399 will use to optimize the above architectures\nvthreshold = [0, 0.1, 1]\nn_components = [5, 10, 15]\ntf = [True, False]\n\nlogger = logging.getLogger('ray.tune')\nlogger.setLevel('CRITICAL')","3dc3262c":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=1, train_size=0.05, random_state=0)\nsss.get_n_splits(X_train, y_train)\n\nX = X_train\ny = y_train\n\nfor train_index, test_index in sss.split(X, y):\n    X_new_train = X[train_index]\n    y_new_train = y[train_index]\n    print('My new array shapes are:')\n    print(X_new_train.shape)\n    print(y_new_train.shape)","141a870c":"no_of_satisfied = np.sum(y_new_train, axis=0)\nsamples = y_new_train.shape[0]\nprint('The percentage of satisfied customers is: {:.2f} %'.format((no_of_satisfied \/ samples) * 100))\nprint('The percentage of dissatisfied customers is: {:.2f} %'.format((1 - (no_of_satisfied \/ samples)) * 100))","cc2f9f1e":"mlp = MLPClassifier(max_iter=5000)\nmlp_opt_score = 0\n\nfor i in range(len(steps)):\n    # Construct the pipe with the specific architecture for this classifier\n    mySteps = steps[i][:]\n    mySteps.append(('mlp', mlp))\n    pipe = Pipeline(steps=mySteps, memory='tmp')\n    \n    # Construct a temporary dictionary with only the values needed for this pipe\n    myDictionary = {\n        'mlp__activation': ['logistic', 'tanh', 'relu'],\n        'mlp__alpha': [0.00001, 0.0001, 0.001],\n        'mlp__learning_rate_init': [0.0001, 0.001, 0.01]\n    }\n    for element in mySteps:   \n        name = element[0]\n        if name == 'selector':\n            myDictionary['selector__threshold'] = vthreshold\n        elif name == 'pca':\n            myDictionary['pca__n_components'] = n_components\n        elif name == 'scaler':\n            myDictionary['scaler__with_mean'] = tf\n            myDictionary['scaler__with_std'] = tf\n            \n    estimator = TuneGridSearchCV(\n        pipe, \n        myDictionary, \n        early_stopping=True,\n        max_iters=20,\n        scoring='accuracy', \n        cv=5 \n    )\n    \n    # Find the optimal model for this pipeline architecture\n    start_time = time.time()\n    estimator.fit(X_new_train, y_new_train)\n    end_time = time.time()\n    \n    duration = math.ceil(end_time - start_time)\n    print('Pipeline architecture {} finished after {} min {} sec'.format(i+1, duration\/\/60, duration%60))\n    \n    if estimator.best_score_ > mlp_opt_score:\n        mlp_opt_estimator = estimator.best_estimator_\n        mlp_opt_score = estimator.best_score_\n        \nprint('Optimal MLP estimator is:\\n{}'.format(mlp_opt_estimator))\nprint('with score: {:.2f} %'.format(mlp_opt_score * 100))","56cd7eab":"svm = SVC()\nsvm_opt_score = 0\n\nfor i in range(len(steps)):\n    # Construct the pipe with the specific architecture for this classifier\n    mySteps = steps[i][:]\n    mySteps.append(('svm', svm))\n    pipe = Pipeline(steps=mySteps, memory='tmp')\n    \n    # Construct a temporary dictionary with only the values needed for this pipe\n    myDictionary = {\n        'svm__C': [0.1, 1, 10],\n#         'svm__kernel': ['poly', 'rbf', 'sigmoid'],\n        'svm__kernel': ['rbf', 'sigmoid'],\n        'svm__gamma': ['scale', 'auto']\n    }\n    for element in mySteps:   \n        name = element[0]\n        if name == 'selector':\n            myDictionary['selector__threshold'] = vthreshold\n        elif name == 'pca':\n            myDictionary['pca__n_components'] = n_components\n        elif name == 'scaler':\n            myDictionary['scaler__with_mean'] = tf\n            myDictionary['scaler__with_std'] = tf\n            \n    estimator = TuneGridSearchCV(\n        pipe, \n        myDictionary, \n        max_iters=20,\n        scoring='accuracy',\n        cv=5\n    )\n    \n    # Find the optimal model for this pipeline architecture\n    start_time = time.time()\n    estimator.fit(X_new_train, y_new_train)\n    end_time = time.time()\n    \n    duration = math.ceil(end_time - start_time)\n    print('Pipeline architecture {} finished after {} min {} sec'.format(i+1, duration\/\/60, duration%60))\n    \n    if estimator.best_score_ > svm_opt_score:\n        svm_opt_estimator = estimator.best_estimator_\n        svm_opt_score = estimator.best_score_\n        \nprint('Optimal SVM estimator is:\\n{}'.format(svm_opt_estimator))\nprint('with score: {:.2f} %'.format(svm_opt_score * 100))","d10dcc6d":"# Optimal MLP estimator\nfit_start = time.time()\nmlp_opt_estimator.fit(X_train, y_train)\nfit_end = time.time()\n\npredict_start = time.time()\nmlp_preds = mlp_opt_estimator.predict(X_test)\npredict_end = time.time()\n\nmlp_fit_time = math.ceil(fit_end - fit_start)\nmlp_predict_time = math.ceil(predict_end - predict_start)\nmlp_score = accuracy_score(y_test, mlp_preds) * 100\n\nprint('Optimal MLP estimator')\nprint('Fit time: {} min {} sec'.format(mlp_fit_time \/\/ 60, mlp_fit_time % 60))\nprint('Predict time: {} min {} sec'.format(mlp_predict_time \/\/ 60, mlp_predict_time % 60))\nprint('Total time: {} min {} sec'.format((mlp_fit_time + mlp_predict_time) \/\/ 60, (mlp_fit_time + mlp_predict_time) % 60))\nprint('Accuracy: {:.2f} %\\n'.format(mlp_score))","d2261bc3":"# Optimal SVM estimator\nfit_start = time.time()\nsvm_opt_estimator.fit(X_train, y_train)\nfit_end = time.time()\n\npredict_start = time.time()\nsvm_preds = svm_opt_estimator.predict(X_test)\npredict_end = time.time()\n\nsvm_fit_time = math.ceil(fit_end - fit_start)\nsvm_predict_time = math.ceil(predict_end - predict_start)\nsvm_score = accuracy_score(y_test, svm_preds) * 100\n\nprint('Optimal SVM estimator')\nprint('Fit time: {} min {} sec'.format(svm_fit_time \/\/ 60, svm_fit_time % 60))\nprint('Predict time: {} min {} sec'.format(svm_predict_time \/\/ 60, svm_predict_time % 60))\nprint('Total time: {} min {} sec'.format((svm_fit_time + svm_predict_time) \/\/ 60, (svm_fit_time + svm_predict_time) % 60))\nprint('Accuracy: {:.2f} %'.format(svm_score))","e7c7223c":"x = ['Dummy', 'MLP', 'SVM']\nx_axis = np.arange(len(x))\nplt.figure(figsize=(10, 5))\n# plt.figure()\n\nplt.bar(x_axis+0.15, [0, mlp_score, svm_score], 0.3, label='Optimized')\nplt.bar(x_axis-0.15, out_of_the_box_results, 0.3, label='Out-of-the-box')\nplt.xticks(x_axis, x)\nplt.xlabel('Classifiers')\nplt.ylabel('Accuracy')\nplt.title('Final Results')\nplt.show()","1b79a82d":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Heatmap Visualization Parameters\ndef visualize_confusion_matrix(conf):\n    group_names = ['Correctly satisfied', 'Predicted satisfied while not', 'Predicted dissatisfied while not', 'Correctly dissatisfied']\n    group_counts = [value for value in conf.flatten()]\n    group_percentages = ['{0:.2%}'.format(value) for value in conf.flatten() \/ np.sum(conf)]\n\n    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n            zip(group_names, group_counts, group_percentages)]\n    labels = np.asarray(labels).reshape(2, 2)\n\n    sns.heatmap(conf, annot=labels, fmt='', cmap='Blues')","6adf477f":"mlp_opt_conf = confusion_matrix(y_test, mlp_preds)\nvisualize_confusion_matrix(mlp_opt_conf)","647ea69e":"svm_opt_conf = confusion_matrix(y_test, svm_preds)\nvisualize_confusion_matrix(svm_opt_conf)","a928e3a2":"We can see that our out-of-the-box MLP classifier is by far the best classifier of the 3 with a very good accuracy score of 92.51%. It even needs much less time compared to the SVM model.","29516e03":"### MLP","b4ade6b7":"# Data Overview","909fc7a4":"MLPs include at least 3 layers: an input layer, one or more hidden layers and an output layer. All the layers besides the input layer are made up of neurons with a non-linear activation function (eg RELU or sigmoid). MLP networks use backpropagation and stochastic gradient descent to find the local minima and train the model.","bcd60e2d":"Due to the very large volume of data we have to make some decisions based on the dataset that will minimize the number of pipeline architectures that we have to test (Kaggle sessions expire after a certain amount as well).\n","57cfdd02":"### Multi-Layer Perceptron","570abbe5":"## Out-of-the-box performance","d2aefc23":"### SVM","e1e62784":"We have some very small variances so we will experiment with a selector to ignore those features.\n","1674e4fc":"As part of our preprocessing we want to check if our dataset has missing values.","5c112161":"We will fill our missing values for \"Arrival Delay in Minutes\" using the mean value of our training dataset for that feature. The same value will be used to fill the missing values of the test set as well.","3e0b13b8":"Dummy classifier is included as baseline for the rest of our classifiers.","a6bc761c":"Obviously the id does not contain useful information and we can discard it.","a35e0741":"No missing values belong to categorical features. Therefore we will handle them after handling the categorical features, so as not to switch our data back and forth from pandas to numpy arrays.\n\nReading the dataset informaton we can deduce that we have some categorical features that are not compatible with the sklearn classifiers, so we use get_dummies to make them numerical. We also drop one of the columns that would be created since we have few distinct values (maximum 3) and the resulting columns would be highly correlated.","28804c45":"SVMs are models that \"draw\" lines or hyperplanes (based on the training set) to determine which class any new test sample belongs to. The lines or hyperplanes are created so as to maximize the distance between them and the training samples. Linear SVMs are used to solve linearly separable problems whereas for more complex problems SVMs that utilize rbf kernels and augment the dimensionality of the problem are used instead.","479a7fab":"We choose accuracy as a metric, because we consider a wrong prediction equally important whether it is a false negative or a false positive. A false positive means we classified a customer as satisfied although he wasn't, which means we will not take steps towards improving our services. A false negative means we classified a customer as dissatisfied (or neutral) when in fact he was satisfied, which means we might proceed to upgrade our services more than we need to and waste money.\nEither way, a misclassification of either category isn't particularly harmful and therefore we decide accuracy is a reasonable metric for our problem.","fa18a8cc":"We first import our data from the kaggle library.","cdfeb5d6":"### Support Vector Machines","40e7c806":"# Classifiers","1aa1fcf8":"We gain a sample view of our dataset.","2c554717":"This dataset contains an airline passenger satisfaction survey. It was created by observing the passengers from past flights of an airline company. It consists of factors that are supposed to affect the passengers' satisfaction during the journey. Most of them are personal information (eg. gender, age), flight information (eg. gate location, arrival delay) and ratings appointed by the passengers for flights services (eg. food, wi-fi).\n\nOur purpose is to use the labeled training data to train our models and accurately predict, given a test dataset, which passengers were left satisfied.","380951ab":"Because our sample volume is very large and SVM classifier needs quadratic time over the number of samples we decided to sample our dataset to greatly reduce the time needed to train our SVM model. We used only a fraction of ~5,000 samples to find out our best estimator for the problem. We will use the same samples to train our MLP model too, to earn a more fair comparison.","39f9ff4f":"The above confusion matrices allow us to see that the difference in accurate predictions is pretty small either of the optimized classifiers we choose. The optimized MLP classifier predicts 10 more false positives, while the optimized SVM classifier 33 more false negatives. The difference in accuracy percentage however is extremely small.","d76b7d17":"## Optimization","6f2b8c61":"We confirm that we still have the same balance of classes in our new training dataset.","919879bb":"Finally, we try our final 2 best estimators (one for each classifier type) at our test set (after training on the entire training dataset), measure the time needed to fit and predict, and see how they do.","5d11fa65":"We use a bar-plot to better visualize our results.","8f26f326":"We can see that both our optimized MLP and SVM models have about the same accuracy score ~96%. The total time that the optimized MLP model needed was 1 min and 7 sec vs 5 min and 37 sec of the optimized SVM model. Also, the out-of-the-box MLP model gave an accuracy of 93.36%, which is also very good and only took 41 seconds.\n\nHowever, all the above times are very small and would not make a difference for us. Therefore, we can choose whichever of the 2 optimized classifiers we prefer. Let's say:\n\nPipeline(memory='tmp',\n         steps=[('selector', VarianceThreshold(threshold=0)),\n                ('scaler', StandardScaler()),\n                ('mlp',\n                 MLPClassifier(alpha=1e-05, learning_rate_init=0.01,\n                               max_iter=5000))])","a8acecdb":"# Introduction to Dataset","9b977d6d":"We can conclude that the data is quite balanced and do not require any special treatment or resampling.\n\nWe will also normalize our dataset using a Standard Scaler, which we know pairs well with MLPs and handles outliers decently too.\n\nFinally, the number of samples we have is much greater than the number of features, so our 23 features shouldn't pose a problem. Nevertheless, we will experiment with using a VarianceThreshold selector as well as a PCA dimensionality reduction method.","974004b3":"### Dummy"}}