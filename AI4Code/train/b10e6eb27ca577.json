{"cell_type":{"f3beaa90":"code","4ea9602d":"code","743cdc1c":"code","b0a19a89":"code","f010c8e1":"code","4b2d0e56":"code","7f5e9cd5":"code","a7df4716":"code","f624ab95":"code","02f1fb8b":"code","3c509d53":"code","9c291d2f":"code","b6b8d70c":"code","8bc10b2e":"code","b64477a2":"code","2e2811f7":"code","6f4106ab":"code","724decc9":"code","1de96360":"code","833cff1a":"code","921c5657":"code","a6a14e59":"code","197c5e2c":"code","364d36e5":"code","9f30305e":"markdown","a6e6930d":"markdown","ce9cb978":"markdown","8913f877":"markdown","266b1c15":"markdown","f59c913b":"markdown","ed954688":"markdown","710b7f9e":"markdown","2378ac72":"markdown","5016a6db":"markdown","3e3b832a":"markdown","5a221569":"markdown","4ddde328":"markdown","d2e9cf8a":"markdown","0b43b6ae":"markdown","e6f733d9":"markdown","36618324":"markdown","c0cab011":"markdown","32e3caa4":"markdown","bf968f67":"markdown","abc2adf4":"markdown","fb1d9827":"markdown","d7accac7":"markdown"},"source":{"f3beaa90":"import os","4ea9602d":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","743cdc1c":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint","b0a19a89":"import transformers\n\nfrom tokenizers import BertWordPieceTokenizer\n\nfrom tqdm.notebook import tqdm\n\nfrom kaggle_datasets import KaggleDatasets","f010c8e1":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    Tokenize text\n    Source: https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","4b2d0e56":"def build_model(transformer, max_len=512):\n    \"\"\"\n    Model initalization\n    Source: https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    dense_layer = Dense(224, activation='relu')(cls_token)\n    out = Dense(1, activation='sigmoid')(dense_layer)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    # model = InceptionV3(input_tensor=input_word_ids, weights='imagenet', include_top=True)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model\n","7f5e9cd5":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","a7df4716":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Data access\n# GCS_DS_PATH = KaggleDatasets().get_gcs_path()","f624ab95":"# Configuration\nEPOCHS = 3\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync\nMAX_LEN = 192","02f1fb8b":"# First load the real tokenizer\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=True)\nfast_tokenizer","3c509d53":"DATA_PATH = \"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/\"","9c291d2f":"train1 = pd.read_csv(os.path.join(DATA_PATH, \"jigsaw-toxic-comment-train.csv\"))\ntrain2 = pd.read_csv(os.path.join(DATA_PATH, \"jigsaw-unintended-bias-train.csv\"))\ntrain2.toxic = train2.toxic.round().astype(int)\n\nvalid = pd.read_csv(os.path.join(DATA_PATH, 'validation.csv'))\ntest = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\nsub = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))","b6b8d70c":"# Combine train1 with a subset of train2\ntrain = pd.concat([\n    train1[['comment_text', 'toxic']],\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=150000, random_state=3982)\n])\n\n# Note: changed random_state from 0 to 3982","8bc10b2e":"x_train = fast_encode(train.comment_text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nx_valid = fast_encode(valid.comment_text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nx_test = fast_encode(test.content.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\ny_train = train.toxic.values\ny_valid = valid.toxic.values","b64477a2":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(BATCH_SIZE)\n)","2e2811f7":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-multilingual-cased')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","6f4106ab":"n_steps = x_train.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","724decc9":"# Plot training & validation accuracy values\nplt.plot(train_history.history['accuracy'])\nplt.plot(train_history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","1de96360":"n_steps = x_valid.shape[0] \/\/ BATCH_SIZE\ntrain_history_2 = model.fit(\n    valid_dataset.repeat(),\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS*2\n)","833cff1a":"# Plot training & validation accuracy values\nplt.plot(train_history_2.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper left')\nplt.show()","921c5657":"sub['toxic'] = model.predict(test_dataset, verbose=1)\nsub.to_csv('submission.csv', index=False)","a6a14e59":"EXTERNAL_DATA_PATH = '..\/input\/external\/'\nos.listdir(EXTERNAL_DATA_PATH)","197c5e2c":"submission1 = pd.read_csv(os.path.join(EXTERNAL_DATA_PATH, 'submission-1.csv'))\nsubmission2 = pd.read_csv(os.path.join(EXTERNAL_DATA_PATH, 'submission-2.csv'))\nsubmission3 = pd.read_csv(os.path.join(EXTERNAL_DATA_PATH, 'submission-3.csv'))","364d36e5":"submission1['toxic'] = submission1['toxic']*0.05 + submission2['toxic']*0.15 + submission3['toxic']*0.8\nsubmission1.to_csv('submission-1.csv', index=False)","9f30305e":"### Bland 1","a6e6930d":"# Jigsaw Multilingual Toxic Comment Classification\n\nUse TPUs to identify toxicity comments across multiple languages\n\n[*Link on the Competition*](https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification)\n\n\n### Let's go!!!","ce9cb978":"First, we train on the subset of the training set, which is completely in English.","8913f877":"# Submission","266b1c15":"# Competition task description\n\nAs our computing resources and modeling capabilities grow, so does our potential to support healthy conversations across the globe. **Develop strategies to build effective multilingual models** and you'll help Conversation AI and the entire industry realize that potential. [More...](https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification)\n\n![task gif](https:\/\/preen.ph\/files\/2018\/05\/FunnyIGAccounts.gif)","f59c913b":"# What is Tensor Processing Units?","ed954688":"# Create fast tokenizer","710b7f9e":"# Build datasets objects","2378ac72":"![head image](https:\/\/i.imgur.com\/P9G09Ck.jpg)","5016a6db":"# Bland different submissions\n\n- [Ensemble](https:\/\/www.kaggle.com\/hamditarek\/ensemble)\n- [Super Fast XLMRoberta](https:\/\/www.kaggle.com\/shonenkov\/tpu-inference-super-fast-xlmroberta)\n- [Jigsaw TPU: BERT with Huggingface and Keras](https:\/\/www.kaggle.com\/miklgr500\/jigsaw-tpu-bert-with-huggingface-and-keras)\n- [inference of bert tpu model ml w\/ validation](https:\/\/www.kaggle.com\/abhishek\/inference-of-bert-tpu-model-ml-w-validation)\n\nP.S. I want to test, how can I bland different submissions. After that I will build ensemble of the different models.","3e3b832a":"# Load model into the TPU","5a221569":"# Model Tuning","4ddde328":"| Version | Changes   |  Score |\n|---------|-----------|--------|\n|    1    | default   | 0.8697 |\n|    2    | (EPOCHS = 3+1; BATCH_SIZE = 16x2)   | 0.8587 |\n|    3    | Blanding | 0.9406  |\n|.   4.   | lowercase=True; add Dense layer | ? |","d2e9cf8a":"# Content\n\n1. [Competition task description](#Competition-task-description)\n2. [What is Tensor Processing Units?](#What-is-Tensor-Processing-Units?)\n3. [Import](#Import)\n4. [Functions](#Functions)\n5. [TPU Configs](#TPU-Configs)\n6. [Create fast tokenizer](#Create-fast-tokenizer)\n7. [Load text data](#Load-text-data)\n8. [Build datasets objects](#Build-datasets-objects)\n9. [Load model into the TPU](#Load-model-into-the-TPU)\n10. [Train Model](#Train-Model)\n11. [Submission](#Submission)\n12. [Model Tuning](#Model-Tuning)\n13. [Bland different submissions](#Bland-different-submissions)","0b43b6ae":"# Import","e6f733d9":"### References\n* Original Author: [@xhlulu](https:\/\/www.kaggle.com\/xhlulu\/)\n* Original notebook: [Link](https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras)","36618324":"### Read data","c0cab011":"A **tensor processing unit (TPU)** is an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural network machine learning, particularly using Google's own TensorFlow software. Google began using TPUs internally in 2015, and in 2018 made them available for third party use, both as part of its cloud infrastructure and by offering a smaller version of the chip for sale.\n\n- **Wikipedia** - [Tensor processing unit (TPU)](https:\/\/en.wikipedia.org\/wiki\/Tensor_processing_unit)\n- **Kaggle** - [Tensor Processing Units (TPUs)](https:\/\/www.kaggle.com\/docs\/tpu)\n\n![tpu image](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/be\/Tensor_Processing_Unit_3.0.jpg\/1280px-Tensor_Processing_Unit_3.0.jpg)","32e3caa4":"# Functions","bf968f67":"# Load text data","abc2adf4":"# TPU Configs","fb1d9827":"Now that we have pretty much saturated the learning potential of the model on english only data, we train it for one more epoch on the `validation` set, which is significantly smaller but contains a mixture of different languages.","d7accac7":"# Train Model"}}