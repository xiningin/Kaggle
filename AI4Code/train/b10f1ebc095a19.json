{"cell_type":{"9d98893a":"code","2a1c1fad":"code","245992ae":"code","69b7286c":"code","c8b915af":"code","65e3fab3":"code","2aefe276":"code","a46d5d99":"code","1968e212":"code","914576e5":"code","3e29291f":"code","65065bc6":"code","e2b419e5":"code","93c9a523":"code","880a4107":"code","e15df2d3":"code","ac519099":"markdown","54c32227":"markdown","2240837c":"markdown","2df8255b":"markdown","fec40c8c":"markdown","2c46d56e":"markdown","8fac8667":"markdown","04819992":"markdown","03cd9439":"markdown","2f857f60":"markdown","8919f081":"markdown","1fb5179f":"markdown","a77c7c31":"markdown","0f9cf6ff":"markdown","42a0fd80":"markdown","af57fafe":"markdown","4db423f6":"markdown"},"source":{"9d98893a":"#Easy data manipulation\nimport pandas as pd\nimport numpy as np\n\n#Plotting\nimport seaborn as sns\nsns.set(style='white', context='notebook', palette='deep')\nimport matplotlib.pyplot as plt\n\n#Who likes warnings anyway?\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Sklearn stuffs\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import StratifiedKFold\n\n#Keras stuffs\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation, Conv2D, MaxPooling2D, AveragePooling2D\nfrom keras.optimizers import Adadelta\n\n#Math\nfrom math import ceil\n\n#Time to measure runtime\nfrom time import time","2a1c1fad":"#Tells us which class is associated with which modern hiragana\nclassmap = pd.read_csv('..\/input\/kmnist_classmap.csv')\n\n#The set is already split in a training and a testing set, and has separated labels\ntrain_images = np.load('..\/input\/kmnist-train-imgs.npz')['arr_0']\ntest_images = np.load('..\/input\/kmnist-test-imgs.npz')['arr_0']\ntrain_labels = np.load('..\/input\/kmnist-train-labels.npz')['arr_0']\ntest_labels = np.load('..\/input\/kmnist-test-labels.npz')['arr_0']","245992ae":"print(\"KMNIST train shape:\", train_images.shape)\nprint(\"KMNIST test shape:\", test_images.shape)\nprint(\"KMNIST train shape:\", train_labels.shape)\nprint(\"KMNIST test shape:\", test_labels.shape)\n\n#Adding hiragana's romaji to the class map for convenient display\nromaji = [\"o\", \"ki\", \"su\", \"tsu\", \"na\", \"ha\", \"ma\", \"ya\", \"re\", \"wo\"]\nclassmap['romaji'] = romaji\nprint(\"\\nKMNIST class map shape:\", classmap.shape)\nprint('\\nClass map:\\n', classmap)","69b7286c":"Classes_proportion_train = [len(train_images[np.where(train_labels == i)])\/len(train_images)*100 for i in range(len(classmap))]\nClasses_proportion_test = [len(test_images[np.where(test_labels == i)])\/len(test_images)*100 for i in range(len(classmap))]\n\nprint(\"----- PROPORTION OF CLASSES IN TRAINING SET -----\\n\")\nfor i in range(len(classmap)):\n    print(\"Proportion of class {0}: {1}%\". format(i, Classes_proportion_train[i]))\n\nprint(\"\\n----- PROPORTION OF CLASSES IN TEST SET -----\\n\")\nfor i in range(len(classmap)):\n    print(\"Proportion of class {0}: {1}%\". format(i, Classes_proportion_test[i]))\n","c8b915af":"figure = plt.figure(figsize=(15,5))\nfigure.suptitle('Labeled hiragana examples from the data set', fontsize=16)\nfor lab in range(len(classmap)):\n    images = train_images[np.where(train_labels == lab)]\n    labels = train_labels[np.where(train_labels == lab)]\n    for inst in range(3):\n        \n        #Make a grid of 10x3. Each line will receive the 3 first example of one of the 10 classes\n        plt.subplot(3,10,1 + lab + (inst * 10)) #Be careful with the subplot index, it begins at 1, not 0\n        \n        #Plot image with label as title\n        plt.imshow(images[inst], cmap=plt.cm.Greys) #We use grayscale for readability\n        plt.title(labels[inst]) \n        #We can't display the computer-version if the modern hiragana as the plots' title\n        #matplotlib doesn't seem to support these characters\n        \n        #Formatting: no grid, no ticks\n        plt.grid(False)\n        plt.xticks(ticks=[])\n        plt.yticks(ticks=[])\n            \nplt.show()","65e3fab3":"flat_image_train = np.reshape(train_images, (60000, -1))\nflat_image_test = np.reshape(test_images, (10000, -1))","2aefe276":"ss = StandardScaler()\nflat_image_train = ss.fit_transform(flat_image_train)\nflat_image_test = ss.transform(flat_image_test)","a46d5d99":"start = time()\nknn = KNeighborsClassifier(n_neighbors=4, n_jobs=-1)\nknn.fit(flat_image_train, train_labels)\ny_predicted = knn.predict(flat_image_test)\nend = time()\n\nprint(classification_report(test_labels, y_predicted))\nprint(\"kNN accuracy: {0:.2f}%\".format(knn.score(flat_image_test, test_labels)))\nprint(\"kNN took {0:.2f} seconds to perform fit and predict.\".format(end-start))","1968e212":"x_train = np.reshape(train_images, (60000, 28, 28,1))  \ny_train = keras.utils.to_categorical(train_labels, num_classes=len(classmap))\nx_test = np.reshape(test_images, (10000, 28, 28,1))\ny_test = keras.utils.to_categorical(test_labels, num_classes=len(classmap))","914576e5":"def GetMyConvNet():\n    model = Sequential()\n\n    model.add(Conv2D(32, (3, 3), strides=(1,1), padding=\"same\", input_shape=(28, 28,1)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n\n    model.add(Conv2D(32, (3, 3), strides=(1,1), padding=\"same\"))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n\n    model.add(MaxPooling2D(pool_size=(2, 2))) \n    #model.add(Dropout(0.25))\n\n    model.add(Conv2D(62, (3, 3), strides=(1,1), padding=\"same\"))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Conv2D(62, (3, 3), strides=(1,1), padding=\"same\"))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    #model.add(Dropout(0.25))\n\n    model.add(Conv2D(96, (3, 3), strides=(1,1), padding=\"same\"))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n\n    model.add(Conv2D(96, (3, 3), strides=(1,1), padding=\"same\"))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n\n    model.add(AveragePooling2D(pool_size=(2, 2)))\n    #model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(Dense(384, activation='relu'))\n    model.add(Dense(192, activation='relu'))\n    #model.add(Dropout(0.5))\n    model.add(Dense(10))\n    model.add(BatchNormalization())\n    model.add(Activation('softmax'))\n\n    adadelta = Adadelta(lr=1, rho=0.95, epsilon=None, decay=0.0)\n    model.compile(loss='categorical_crossentropy', optimizer=adadelta, metrics=['accuracy'])\n\n    return model","3e29291f":"GetMyConvNet().summary()","65065bc6":"model = GetMyConvNet()\n\nstart=time()\n\nhistory = model.fit(x_train, y_train, batch_size=256, epochs=100)\nscore = model.evaluate(x_test, y_test)\n\nelapsed_time = time() - start","e2b419e5":"print(\"Accuracy on test set: {0:.2f}%\\nLoss: {1:.2f}\\nTime elapsed: {2:.2f} seconds\".format(score[1]*100, score[0], elapsed_time))","93c9a523":"figure = plt.figure(figsize=(12,3))\nplt.subplot(1,2,1)\n\n#Accuracy\nplt.plot(history.history['acc'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Accuracy over time\\n')\nplt.legend(['Train Accuracy','Test Accuracy'])\n#plt.show()\n\nplt.subplot(1,2,2)\n\n#Loss\nplt.plot(history.history['loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss over time\\n')\nplt.legend(['Train Loss','Test Loss'])\n\nplt.show()","880a4107":"#Find missclassified from the test set and put their index in a list\npredicted = model.predict(x_test)\npreds = [np.argmax(predicted[x]) for x in range(len(predicted))]\nmissclassified= [i for i in range(len(predicted)) if preds[i]!=test_labels[i]]\n\n#Images, Labels and Predictions for missclassified images\nimages = test_images[[i for i in missclassified]]\nlabels = test_labels[[i for i in missclassified]]\nmissed_pred = [preds[i] for i in missclassified]\n\nfigure = plt.figure(figsize=(20, 6*ceil(len(images)\/10)))\n\nfor inst in range(len(images)):\n       \n    #Make a grid \n    plt.subplot(ceil(len(images)\/5), 10, 2*inst+1)\n       \n    #Plot image with predicted and actual labels as title\n    plt.imshow(images[inst], cmap=plt.cm.Greys)\n    plt.title(\"Predicted: {0} ({1})\\nActual: {2} ({3})\".format(missed_pred[inst],\\\n                                                               classmap[classmap['index']==missed_pred[inst]]['romaji'].values[0],\\\n                                                               labels[inst],\\\n                                                               classmap[classmap['index']==labels[inst]]['romaji'].values[0])) \n    \n    #Formatting: no grid, no tick\n    plt.grid(False)\n    plt.xticks(ticks=[])\n    plt.yticks(ticks=[])\n            \n\n    plt.subplot(ceil(len(images)\/5), 10, 2*inst+2)\n    plt.bar(range(10), predicted[[i for i in missclassified]][inst])\n    #Formatting: no grid, no tick\n    plt.grid(False)\n    plt.xticks(range(10), range(10))\n    plt.yticks(ticks=[])\n\nprint(\"----- Mislabeled hiraganas from the test set -----\")\nplt.show()\n","e15df2d3":"#Concatenate training and test set and labels.\nx_full = np.reshape(np.concatenate((x_train, x_test), axis=0), (70000, -1)) #Flatten images to allow direct use of StratifiedKFold\ny_full = np.concatenate((train_labels, test_labels), axis=0)\n\nkfold = StratifiedKFold(n_splits=7, shuffle=True, random_state=12)\ncvscores = []\ncvtimes = []\nfold=0\n\nfor train, test in kfold.split(x_full, y_full):\n    \n    #Training and Test sets and labels for the fold\n    x_full_train = np.reshape(x_full[train], (len(x_full[train]), 28, 28, 1)) #Restore the images as 28x28x1\n    y_full_train = keras.utils.to_categorical(y_full[train], num_classes=len(classmap)) #One-hot encoding of labels\n    x_full_test = np.reshape(x_full[test], (len(x_full[test]), 28, 28, 1)) #Restore the images as 28x28x1\n    y_full_test = keras.utils.to_categorical(y_full[test], num_classes=len(classmap))#One-hot encoding of labels\n    \n    start=time()\n    fold+=1\n    \n    model = GetMyConvNet()\n\n    model.fit(x_full_train, y_full_train, batch_size=256, epochs=100, verbose=0)\n    scores = model.evaluate(x_full_test, y_full_test, verbose=0)\n    \n    elapsed_time = time() - start\n    print(\"Accuracy for fold n\u00b0{0}: {1:.2f}% ({2:.2f} seconds)\".format(fold,scores[1]*100, elapsed_time))\n    \n    cvscores.append(scores[1] * 100)\n    cvtimes.append(elapsed_time)\n    \nprint(\"\\n\\nMean accuracy on {0} folds: {1:.2f}% (+\/- {2:.2f})\\nTotal elapsed time for {0}-fold validation: {3:.2f} seconds\".format(fold, np.mean(cvscores), np.std(cvscores), np.sum(cvtimes)))","ac519099":"A lot of the mislabeled hiragana would be very hard to recognize. Let's see if the training set contains noticeably harder hiraganas than the rest of the set by performing a 7-fold validation of our model on the whole data set with random split.","54c32227":"# Ancient hiragana recognition with a Convolutional Neural Network\n*Lo\u00efc Gauthier*\n\nIn this kernel, we'll take a look at the K-MNIST data set. It is composed of 70,000 28x28 pictures of ancient hiraganas in greyscale. Our goal is to associate each image to one of 10 modern hiraganas.\n\nIn order to do so, we will use a Convolutional Neural Network.","2240837c":"# 3. A first simple classification: kNN","2df8255b":"Then we know that kNN classifier tend to give better results on normalized data. So let's fit a scaler on the training data, and transform both the training and the test set with that scaler.","fec40c8c":"# 5. 7-fold validation of our ConvNet on the whole data set","2c46d56e":"For both the training and the test set, the proportion of each class is perfectly balanced.","8fac8667":"# 1. Loading the images and class map","04819992":"In order to use the kNN classifier, we first have to flatten the 28x28 pictures into a single array with 784 elements.","03cd9439":"Here our ConvNet results in substantially better result with a mean accuracy consistently over 99%, and with a training over 100 epochs (it is already consistently over 99% with training over only 10 epochs! ). This shows that hiraganas from the test set are particularly hard to predict. Our better results here are explained by the repartition of these hard to predict hiraganas in different folds.\n\nPossible ameliorations could include image augmentation, or maybe the use of pre-trained models. Although, given our ConvNet performance, I'm not sure it would be worth it.","2f857f60":"We implement the architecture Input -> (Conv2d -> ReLU -> Conv2d -> ReLU -> Pooling)x3 -> (Fully Connected -> ReLU)x2 -> Fully Connected","8919f081":"# 0. Imports","1fb5179f":"The training set contains 60,000 28x28 pictures, associated with 10 possible modern hiraganas. The test set contains 10,000 instances.\n\nLet's see if the classes repartition is balanced in the train set as well as in the data set.","a77c7c31":"The kNN classifier gives us a good baseline before using better methods. Here we can see that kNN (with k=4) took roughly 10 minutes to fit  and predict the classes of our test set. It gives rather good results given the simplicity of the method, with a weighed average f1-score of 0.89.","0f9cf6ff":"# 4. A better approach: Convolutional Neural Networks\n\nConvolutional neural networks are particularly used for image recognition as they respect the spatial repartition of the pixels of input pictures and to not require them to be flattened.\n\nThe most common architectures for ConvNet used for simple classifications like CIFAR-10 are the following:\n* Input -> Conv2d -> ReLU -> Fully Connected;\n* Input -> (Conv2d -> ReLU -> Pooling)x2 -> Fully Connected -> ReLU -> Fully Connected;\n* Input -> (Conv2d -> ReLU -> Conv2d -> ReLU -> Pooling)x3 -> (Fully Connected -> ReLU)x2 -> Fully Connected\n\nIt is also good practise to use drop out steps to prevent the activation of a certain proportion of neurons (regardless of their input) in order to avoid overfitting. Although, in this case, as we will use Batch normalization, we will introduce some noise in the network that will have the side effect to help with regularization. Batch normalization will be mainly used for faster training, eliminate covariate shift, and to have less sensibility to possibly bad randomly initialized weights. Another thing to consider is 0 padding to ensure that all regions of the pictures get sampled the same number of time through our convolution layers.\n\nLet's get into it and reshape our pictures so that they are 28x28x1 arrays, and define our labels to keras.","42a0fd80":"We can already see in that small sample that there is apparently a lot of variance in the shapes inside each class of hiragana. For instance, from a very naive point of view, it seems that the second and third member of the 9th class are very different from each other.","af57fafe":"We will use 7-fold validation so that for each fold, the ratio between the training and validation set are the same as the ratio between the predefined training and test set.\n\nThis will give us a more robust measure of the ConvNet's accuracy.","4db423f6":"# 2. A first look at the test set"}}