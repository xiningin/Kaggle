{"cell_type":{"2096cfb8":"code","0e295704":"code","5da0bc25":"code","74a8c5f2":"code","51010ef9":"code","b828bad0":"code","f2d31ed0":"code","4f5b9dd7":"code","5e0e0e6d":"code","64f53cfb":"code","c8da3937":"code","e17bd6a6":"code","26a58cb7":"code","92278247":"code","4d88dfe7":"code","2bcefbfc":"code","08b942e8":"code","02d99210":"code","de8024f3":"code","d4fb920b":"code","ef4ac4ae":"code","c6e3e034":"markdown","e4e1f3b5":"markdown","f29c02a0":"markdown","04b70de1":"markdown","973ed405":"markdown","10ee1c32":"markdown","b90beb12":"markdown"},"source":{"2096cfb8":"import pandas as pd \nimport numpy as np\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport optuna\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score","0e295704":"train = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')","5da0bc25":"train.isnull().sum().values.sum()","74a8c5f2":"test.isnull().sum().values.sum()","51010ef9":"train.head()","b828bad0":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor i in range(19):\n    le.fit(list(train['cat'+str(i)])+list(test['cat'+str(i)]))\n    train['cat'+str(i)] = le.transform(train['cat'+str(i)])\n    test['cat'+str(i)] = le.transform(test['cat'+str(i)])","f2d31ed0":"X = train.iloc[:,1:-1].values\ny = train.iloc[:,-1].values\nX_test = test.iloc[:,1:]","4f5b9dd7":"X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.15,random_state=42)","5e0e0e6d":"lg = LGBMClassifier()\nlg.fit(X_train,y_train)\ny_pred_l = lg.predict_proba(X_dev)[:,1]\nroc_auc_score(y_dev,y_pred_l)","64f53cfb":"def fun(trial,data=X,target=y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2,random_state=42)\n    param = {\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_uniform('subsample', 0,1),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0, 0.1 ),\n        'max_depth': trial.suggest_categorical('max_depth', [10,20,100]),\n        'num_leaves' : trial.suggest_int('num_leaves', 1, 1000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight' , 1e-5 , 1),\n        'cat_smooth' : trial.suggest_int('cat_smooth', 1, 100),\n        'cat_l2': trial.suggest_int('cat_l2',1,20),\n        'metric': 'auc', \n        'random_state': 13,\n        'n_estimators': 10000,\n        \n    }\n    model = LGBMClassifier(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=200,verbose=False)\n    \n    preds = model.predict_proba(test_x)[:,1]\n    \n    auc = roc_auc_score(test_y, preds)\n    \n    return auc","c8da3937":"study = optuna.create_study(direction='maximize')\nstudy.optimize(fun, n_trials=30)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","e17bd6a6":"#plot_optimization_histor: shows the scores from all trials as well as the best score so far at each point.\noptuna.visualization.plot_optimization_history(study)","26a58cb7":"#plot_parallel_coordinate: interactively visualizes the hyperparameters and scores\noptuna.visualization.plot_parallel_coordinate(study)","92278247":"# plot_slice: shows the evolution of the search. You can see where in the hyperparameter space your search\n# went and which parts of the space were explored more.\noptuna.visualization.plot_slice(study)","4d88dfe7":"#Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)","2bcefbfc":"best_params = study.best_params\nbest_params['n_estimators'] = 10000\nbest_params['cat_feature'] = [i for i in range(19)]\nbest_params['random_state'] = 13\nbest_params['metric'] = 'auc'","08b942e8":"columns = [col for col in train.columns if col not in ['id','target'] ]","02d99210":"preds = np.zeros(X_test.shape[0])\nkf = StratifiedKFold(n_splits = 10 , random_state = 13 , shuffle = True)\nauc =[]\nn=0\n\nfor tr_idx, test_idx in kf.split(train[columns], train['target']):\n    \n    X_tr, X_val = train[columns].iloc[tr_idx], train[columns].iloc[test_idx]\n    y_tr, y_val = train['target'].iloc[tr_idx], train['target'].iloc[test_idx]\n    \n    model = LGBMClassifier(**best_params)\n    \n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=200,verbose=False)\n    \n    preds+=model.predict_proba(X_test)[:,1]\/kf.n_splits\n    auc.append(roc_auc_score(y_val, model.predict_proba(X_val)[:,1]))\n    print(n+1,auc[n])\n    n+=1","de8024f3":"np.mean(auc)","d4fb920b":"submission = pd.DataFrame({'id':test['id'],'target':preds})\nsubmission.to_csv('submit.csv',index=False)","ef4ac4ae":"submission.head()","c6e3e034":"# Please upvote the notebook if it helped in any way! \n\n## Have a nice day :)","e4e1f3b5":"# Hyperparameter tuning using Optuna","f29c02a0":"## Best Parameters found by Optuna","04b70de1":"# Baseline Model","973ed405":"## Make Predictions","10ee1c32":"No null values!!","b90beb12":"* Binary Classification problem based on real life data\n* We have to predict probabilities and the metric is ROC-AUC\n* This is a sample notebook which gives beginner approach to the data and hyperparameter tuning using Optuna\n\n\nI'm a beginner in this field too, Please give suggestions to improove the score!!\n"}}