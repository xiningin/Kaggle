{"cell_type":{"c51c56f6":"code","33e508bf":"code","820a90fc":"code","49c837d5":"markdown","9633d6f9":"markdown","ace46f46":"markdown","dea8186a":"markdown"},"source":{"c51c56f6":"#Google News Scraping\n!pip install goose3\nimport requests\nimport pandas as pd\nimport re\nfrom urllib.parse import urljoin\nfrom goose3 import Goose\nfrom bs4 import BeautifulSoup\nfrom bs4.dammit import EncodingDetector\n\n\n#Enter the URL to check for news\nurl = 'https:\/\/news.google.com\/topics\/CAAqIQgKIhtDQkFTRGdvSUwyMHZNRE55YXpBU0FtVnVLQUFQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen'\n#Append this url for all the articles\nurl1 = 'https:\/\/news.google.com'\n#Match the pattern in all the url's\npattern = 'https:\/\/news.google.com\/articles'\n#Get Response of url\nresponse = requests.get(url)\nhttp_encoding = response.encoding if 'charset' in response.headers.get('content-type', '').lower() else None\nhtml_encoding = EncodingDetector.find_declared_encoding(response.content, is_html=True)\nencoding = html_encoding or http_encoding\nsoup = BeautifulSoup(response.content, from_encoding=encoding)\n\nfinal_urls = []\n#Get the list of hyperlink on the page\nfor link in soup.find_all('a'):\n    href = link.attrs.get(\"href\")\n    href = urljoin(url1, href)\n    if pattern in href:\n    # final_urls = [href]\n        final_urls.append(href)\n\n#Remove duplicate URL's        \nunique_urls = set(final_urls)\n#Convert it back to lists\nunique_urls_list = list(unique_urls)\n\n#Get the Title, Author and Text of Each URL\n#Create the lists of all variables\nfinal_title_list = []\nfinal_text_list = []\nfinal_source_list = []\nfor data in unique_urls_list:\n    request = requests.get(data)\n    g = Goose()\n    article = g.extract(url=request.url)\n    title = article.title\n    text = article.cleaned_text\n    domain = article.domain\n    source=re.findall(r'(?<=\\.)([^.]+)(?:\\.(?:co\\.uk|ac\\.us|[^.]+(?:$|\\n)))',domain)\n    final_title_list.append(title)\n    final_text_list.append(text)\n    final_source_list.extend(source)\n    dict_1 = {'title': final_title_list, 'text': final_text_list, 'author':final_source_list}\n    df = pd.DataFrame({key: pd.Series(value) for key, value in dict_1.items()})\n    df.to_csv('Googlenews.csv', encoding='utf-8', index=False)\n#Download the file in your local\n# files.download(\"test.csv\")\n","33e508bf":"#The Hindu Scraping\n# !pip install goose3\nimport requests\nimport pandas as pd\nimport re\nfrom urllib.parse import urljoin\nfrom goose3 import Goose\nfrom bs4 import BeautifulSoup\nfrom bs4.dammit import EncodingDetector\n\nurl = 'https:\/\/www.thehindu.com\/'\n\nresponse = requests.get(url)\nhttp_encoding = response.encoding if 'charset' in response.headers.get('content-type', '').lower() else None\nhtml_encoding = EncodingDetector.find_declared_encoding(response.content, is_html=True)\nencoding = html_encoding or http_encoding\nsoup = BeautifulSoup(response.content, from_encoding=encoding)\n\nfinal_urls = []\nfor link in soup.select(\"a[href$='.ece']\"):\n    href = link.attrs.get(\"href\")\n    final_urls.append(href)\n#   href = urljoin(url, href)\n#   if pattern in href:\n#     # final_urls = [href]\n        \nunique_urls = set(final_urls)\nunique_urls_list = list(unique_urls)\n\n\n#Get the Title, Author and Text of Each URL\n#Create the lists of all variables\nfinal_title_list = []\nfinal_text_list = []\nfinal_source_list = []\nfor data in unique_urls_list:\n  # request = requests.get(data)\n    g = Goose()\n    article = g.extract(url=data)\n    title = article.title\n    text = article.cleaned_text\n    domain = article.domain\n    source=re.findall(r'(?<=\\.)([^.]+)(?:\\.(?:co\\.uk|ac\\.us|[^.]+(?:$|\\n)))',domain)\n    final_title_list.append(title)\n    final_text_list.append(text)\n    final_source_list.extend(source)\n    dict_1 = {'title': final_title_list, 'text': final_text_list, 'author':final_source_list}\n    df = pd.DataFrame({key: pd.Series(value) for key, value in dict_1.items()})\n    df.to_csv('TheHindu_DataSet.csv', encoding='utf-8', index=False)\n# #Download the file in your local\n# files.download(\"TheHindu_DataSet.csv\")\n\n","820a90fc":"#Fake News \n#OPTimess scraping\n# !pip install goose3\nimport requests\nimport datetime\nimport pandas as pd\nimport re\nfrom urllib.parse import urljoin\nfrom goose3 import Goose\nfrom bs4 import BeautifulSoup\nfrom bs4.dammit import EncodingDetector\n\n#Get the Current Year\nnow = datetime.date.today()\nyear = now.year\ncurrentyear = year.__str__()\n#Get the Curent Month\nmonth = (now.month)\nif(month == 1 or month == 2 or month == 3 or month == 4 or month == 5 or month == 6 or month == 7 or month == 8 or month == 9):\n    currentmonth = '{:02d}'.format(month)\n    currentmonth = currentmonth.__str__()\nelse:\n    currentmonth = month.__str__()\n\n#Enter the URL to check for news\nurl = 'https:\/\/www.opindia.com\/'\n#Append this url for all the articles\nurl1 = 'https:\/\/www.opindia.com\/'\n#Match the pattern in all the url's\npattern = 'https:\/\/www.opindia.com\/'+currentyear+'\/'+currentmonth+'\/'\n#Get Response of url\nresponse = requests.get(url)\nhttp_encoding = response.encoding if 'charset' in response.headers.get('content-type', '').lower() else None\nhtml_encoding = EncodingDetector.find_declared_encoding(response.content, is_html=True)\nencoding = html_encoding or http_encoding\nsoup = BeautifulSoup(response.content, from_encoding=encoding)\n\nfinal_urls = []\n#Get the list of hyperlink on the page\nfor link in soup.find_all('a'):\n    href = link.attrs.get(\"href\")\n    href = urljoin(url1, href)\n    if pattern in href:\n        final_urls.append(href)\n\n# #Remove duplicate URL's        \nunique_urls = set(final_urls)\n# #Convert it back to lists\nunique_urls_list = list(unique_urls)\n\n#Get the Title, Author and Text of Each URL\n#Create the lists of all variables\nfinal_title_list = []\nfinal_text_list = []\nfinal_source_list = []\nfinal_image_list = []\nfor data in unique_urls_list:\n    g = Goose()\n    article = g.extract(url=data)\n    title = article.title\n    text = article.cleaned_text\n    domain = article.domain\n    image = article.infos\n    print(image)\n    source=re.findall(r'(?<=\\.)([^.]+)(?:\\.(?:co\\.uk|ac\\.us|[^.]+(?:$|\\n)))',domain)\n    final_title_list.append(title)\n    final_text_list.append(text)\n    final_source_list.extend(source)\n    dict_1 = {'title': final_title_list, 'text': final_text_list, 'author':final_source_list}\n    df = pd.DataFrame({key: pd.Series(value) for key, value in dict_1.items()})\n    df.to_csv('OPTimes_DataSet.csv', encoding='utf-8', index=False)\n# #Download the file in your local\n# files.download(\"OPTimes_DataSet.csv\")\n","49c837d5":"Use following functions to scrape the news from various sources which can be used further used for NLP.","9633d6f9":"# Google News Scraping","ace46f46":"# The Hindu News Scraping","dea8186a":"# OPTimess Scraping"}}