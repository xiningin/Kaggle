{"cell_type":{"1e9dbc86":"code","e7c440ed":"code","3246bf04":"code","6450ccc2":"code","58e966bc":"code","1831dc62":"code","718f9d74":"code","343f3a07":"code","91433518":"code","48e682fb":"code","44881e6a":"code","56c2ac25":"code","e5ebde8d":"code","805f3192":"code","76d4a611":"code","8246b396":"code","7598fa68":"code","7b4e7651":"code","d34796ea":"code","4378bf18":"code","c3eb145a":"code","7501fd8d":"code","0b6801eb":"code","a86d731a":"code","9c6b21a6":"code","9c7cc3ef":"code","77f0b02e":"code","ea8b8736":"code","70769e98":"code","fc5bb7da":"code","f1b913db":"code","dee252a3":"code","0e916e6e":"code","e9fe7177":"code","f8357d61":"code","280abd97":"code","fcb71d20":"code","21df2dfd":"code","5ebae956":"code","df302643":"code","40166d7f":"code","ba471da1":"code","c29c8321":"code","439ca42d":"code","778e94bf":"markdown","b3fee885":"markdown","26c86fcd":"markdown","73e80146":"markdown","7aacb7d8":"markdown","325836f5":"markdown","210b8616":"markdown"},"source":{"1e9dbc86":"import numpy as np\nimport pandas as pd\nimport random\nrandom.seed(28)\nnp.random.seed(28)\nimport itertools\nimport scipy\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score,\n                             precision_recall_fscore_support)\nimport os\nimport copy\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\npd.options.display.precision = 15\nfrom collections import defaultdict\nimport time\nfrom collections import Counter\nimport datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\nfrom sklearn import metrics\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import HTML\nimport json\n\nfrom category_encoders.ordinal import OrdinalEncoder\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom typing import List\nimport datetime\nfrom functools import partial\nimport scipy as sp\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nfrom sklearn import metrics\npd.set_option('max_rows', 500)\nimport re\nfrom collections import Counter\npd.set_option('display.max_rows', 500)\npd.set_option('display.width', 1000)\npd.set_option('display.float_format', '{:20,.2f}'.format)\npd.set_option('display.max_colwidth', -1)","e7c440ed":"from sklearn import metrics, preprocessing\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import utils","3246bf04":"import numpy as np\nfrom collections import Counter, defaultdict\nfrom sklearn.utils import check_random_state\n\nclass RepeatedStratifiedGroupKFold():\n\n    def __init__(self, n_splits=5, n_repeats=1, random_state=None):\n        self.n_splits = n_splits\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n        \n    # Implementation based on this kaggle kernel:\n    #    https:\/\/www.kaggle.com\/jakubwasikowski\/stratified-group-k-fold-cross-validation\n    def split(self, X, y=None, groups=None):\n        k = self.n_splits\n        def eval_y_counts_per_fold(y_counts, fold):\n            y_counts_per_fold[fold] += y_counts\n            std_per_label = []\n            for label in range(labels_num):\n                label_std = np.std(\n                    [y_counts_per_fold[i][label] \/ y_distr[label] for i in range(k)]\n                )\n                std_per_label.append(label_std)\n            y_counts_per_fold[fold] -= y_counts\n            return np.mean(std_per_label)\n            \n        rnd = check_random_state(self.random_state)\n        for repeat in range(self.n_repeats):\n            labels_num = np.max(y) + 1\n            y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n            y_distr = Counter()\n            for label, g in zip(y, groups):\n                y_counts_per_group[g][label] += 1\n                y_distr[label] += 1\n\n            y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n            groups_per_fold = defaultdict(set)\n        \n            groups_and_y_counts = list(y_counts_per_group.items())\n            rnd.shuffle(groups_and_y_counts)\n\n            for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n                best_fold = None\n                min_eval = None\n                for i in range(k):\n                    fold_eval = eval_y_counts_per_fold(y_counts, i)\n                    if min_eval is None or fold_eval < min_eval:\n                        min_eval = fold_eval\n                        best_fold = i\n                y_counts_per_fold[best_fold] += y_counts\n                groups_per_fold[best_fold].add(g)\n\n            all_groups = set(groups)\n            for i in range(k):\n                train_groups = all_groups - groups_per_fold[i]\n                test_groups = groups_per_fold[i]\n\n                train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n                test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n                yield train_indices, test_indices","6450ccc2":"train = pd.read_csv(\"\/kaggle\/input\/widsdatathon2021\/TrainingWiDS2021.csv\")\nsamplesubmission = pd.read_csv(\"..\/input\/widsdatathon2021\/SampleSubmissionWiDS2021.csv\")\ntest = pd.read_csv(\"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\")\ndictionary = pd.read_csv(\"..\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv\")\nsolution_template = pd.read_csv(\"..\/input\/widsdatathon2021\/SolutionTemplateWiDS2021.csv\")\n\nprint('train ' , train.shape)\nprint('test ' , test.shape)\nprint('samplesubmission ' , samplesubmission.shape)\nprint('solution_template ' , solution_template.shape)\nprint('dictionary ' , dictionary.shape)","58e966bc":"# some data cleaning\nif 1 :\n    #d1_mbp_min\n    train[ 'd1_mbp_min'] = np.where(train[ 'd1_mbp_min'].isna(), train[ 'd1_mbp_noninvasive_min']  ,train[ 'd1_mbp_min'] )\n    test[  'd1_mbp_min'] = np.where(test[ 'd1_mbp_min'].isna(), test[ 'd1_mbp_noninvasive_min']  ,test[ 'd1_mbp_min'] )\n    \n    train[ 'd1_mbp_noninvasive_min']  = np.where(train[ 'd1_mbp_noninvasive_min'].isna(), train[ 'd1_mbp_min']  ,train[ 'd1_mbp_noninvasive_min'] )\n    test[  'd1_mbp_noninvasive_min']  = np.where(test[ 'd1_mbp_noninvasive_min'].isna() , test[ 'd1_mbp_min']   ,test[ 'd1_mbp_noninvasive_min'] )\n    #d1_mbp_max\n    train[ 'd1_mbp_max'] = np.where(train[ 'd1_mbp_max'].isna(), train[ 'd1_mbp_noninvasive_max']  ,train[ 'd1_mbp_max'] )\n    test[ 'd1_mbp_max'] = np.where(test[ 'd1_mbp_max'].isna(), test[ 'd1_mbp_noninvasive_max']  ,test[ 'd1_mbp_max'] )\n    \n    train[ 'd1_mbp_noninvasive_max']  = np.where(train[ 'd1_mbp_noninvasive_max'].isna(), train[ 'd1_mbp_max']  ,train[ 'd1_mbp_noninvasive_max'] )\n    test[  'd1_mbp_noninvasive_max']  = np.where(test[ 'd1_mbp_noninvasive_max'].isna() , test[ 'd1_mbp_max']   ,test[ 'd1_mbp_noninvasive_max'] )\n    \n\n    #d1_diasbp_min\n    train[ 'd1_diasbp_min'] = np.where(train[ 'd1_diasbp_min'].isna(), train[ 'd1_diasbp_noninvasive_min']  ,train[ 'd1_diasbp_min'] )\n    test[  'd1_diasbp_min'] = np.where(test[  'd1_diasbp_min'].isna(), test [ 'd1_diasbp_noninvasive_min']  ,test[  'd1_diasbp_min'] )\n\n    train[ 'd1_diasbp_noninvasive_min']  = np.where(train[ 'd1_diasbp_noninvasive_min'].isna(), train[ 'd1_diasbp_min']  ,train[ 'd1_diasbp_noninvasive_min'] )\n    test[  'd1_diasbp_noninvasive_min']  = np.where(test[  'd1_diasbp_noninvasive_min'].isna() , test[ 'd1_diasbp_min']   ,test[ 'd1_diasbp_noninvasive_min'] )\n\n    train[ 'd1_diasbp_max'] = np.where(train[ 'd1_diasbp_max'].isna(), train[ 'd1_diasbp_noninvasive_max']  ,train[ 'd1_diasbp_max'] )\n    test[  'd1_diasbp_max']  = np.where(test[ 'd1_diasbp_max'].isna(), test[  'd1_diasbp_noninvasive_max']  ,test[  'd1_diasbp_max'] )\n\n    train[ 'd1_diasbp_noninvasive_max']  = np.where(train[ 'd1_diasbp_noninvasive_max'].isna(), train[ 'd1_diasbp_max']  ,train[ 'd1_diasbp_noninvasive_max'] )\n    test[  'd1_diasbp_noninvasive_max']  = np.where(test[  'd1_diasbp_noninvasive_max'].isna() , test[ 'd1_diasbp_max']   ,test[ 'd1_diasbp_noninvasive_max'] )\n","1831dc62":"# verif et correction des quelques valeures incoherentes genre max < min \nif 1 :\n    min_col= [col for col in train.columns if 'd1' in col and col.endswith('_min')]\n    for col in min_col :\n        d1_col = col.replace('_min','_max')\n        train[d1_col] = np.where(train[col] > train[d1_col], np.nan ,  train[d1_col])\n        test[d1_col]  = np.where(test[col]   > test[d1_col], np.nan ,test[d1_col] )\n        \n    min_col= [col for col in train.columns if 'h1' in col and col.endswith('_min')]\n    for col in min_col :\n        d1_col = col.replace('_min','_max')\n        train[d1_col ] = np.where(train[col] > train[d1_col], np.nan ,  train[d1_col])\n        test[d1_col]  = np.where(test[col] > test[d1_col]   , np.nan ,test[d1_col] )\n\n\n       ","718f9d74":"# split the apache 3 diag in parts\ntrain['apache_3j_diagnosis_split0'] = np.where(train['apache_3j_diagnosis'].isna() , np.nan , train['apache_3j_diagnosis'].astype('str').str.split('.',n=1,expand=True)[0]  )\ntest['apache_3j_diagnosis_split0']   = np.where(test['apache_3j_diagnosis'].isna() , np.nan , test['apache_3j_diagnosis'].astype('str').str.split('.',n=1,expand=True)[0]  )\n\n\ntrain['apache_3j'] = np.where(train['apache_3j_diagnosis_split0'].isna() , np.nan ,\n                            np.where(train['apache_3j_diagnosis_split0'].fillna(9999).astype('int') < 200, 'Cardiovascular' , \n                            np.where(train['apache_3j_diagnosis_split0'].fillna(9999).astype('int') < 400, 'Respiratory' , \n                            np.where(train['apache_3j_diagnosis_split0'].fillna(9999).astype('int') < 500, 'Neurological' , \n                            np.where(train['apache_3j_diagnosis_split0'].fillna(9999).astype('int') < 600, 'Sepsis' , \n                            np.where(train['apache_3j_diagnosis_split0'].fillna(9999).astype('int') < 800, 'Trauma' ,  \n                            np.where(train['apache_3j_diagnosis_split0'].fillna(9999).astype('int') < 900, 'Haematological' ,         \n                            np.where(train['apache_3j_diagnosis_split0'].fillna(9999).astype('int') < 1000, 'Renal\/Genitourinary' ,         \n                            np.where(train['apache_3j_diagnosis_split0'].fillna(9999).astype('int') < 1200, 'Musculoskeletal\/Skin disease' , 'Operative Sub-Diagnosis Codes' ))))))))\n                                    )\ntest['apache_3j'] = np.where(test['apache_3j_diagnosis_split0'].isna() , np.nan ,\n                            np.where(test['apache_3j_diagnosis_split0'].fillna(9999).astype('int') < 200, 'Cardiovascular' , \n                            np.where(test['apache_3j_diagnosis_split0'].fillna(9999).astype('int') < 400, 'Respiratory' , \n                            np.where(test['apache_3j_diagnosis_split0'].fillna(9999).astype('int') < 500, 'Neurological' , \n                            np.where(test['apache_3j_diagnosis_split0'].fillna(9999).astype('int') < 600, 'Sepsis' , \n                            np.where(test['apache_3j_diagnosis_split0'].fillna(9999).astype('int') < 800, 'Trauma' ,  \n                            np.where(test['apache_3j_diagnosis_split0'].fillna(9999).astype('int') < 900, 'Haematological' ,         \n                            np.where(test['apache_3j_diagnosis_split0'].fillna(9999).astype('int') < 1000, 'Renal\/Genitourinary' ,         \n                            np.where(test['apache_3j_diagnosis_split0'].fillna(9999).astype('int') < 1200, 'Musculoskeletal\/Skin disease' , 'Operative Sub-Diagnosis Codes' ))))))))\n                                    )\n\ntrain['apache_3j_diagnosis_split1'] = np.where(train['apache_3j_diagnosis'].isna() , np.nan , train['apache_3j_diagnosis'].astype('str').str.split('.',n=1,expand=True)[1]  )\ntest['apache_3j_diagnosis_split1']  = np.where(test['apache_3j_diagnosis'].isna() , np.nan , test['apache_3j_diagnosis'].astype('str').str.split('.',n=1,expand=True)[1]  )\n\n","343f3a07":"train['pre_icu_los_days'] = train['pre_icu_los_days'].apply(lambda x:scipy.special.expit(x) )\ntest['pre_icu_los_days']  = test['pre_icu_los_days'].apply(lambda x:scipy.special.expit(x) )","91433518":"column='pre_icu_los_days'\nfig = plt.figure(figsize=(20,4))\ndf1      = train.loc[train['diabetes_mellitus']==0]\ndf2      = train.loc[train['diabetes_mellitus']==1]\nsns.distplot(df1[column].fillna(-5),  color='green', label='diabetes_mellitus 0', kde=True); \nsns.distplot(df2[column].fillna(-5),  color='red'  , label='diabetes_mellitus 1', kde=True); \nfig=plt.legend(loc='best')\nplt.xlabel(column, fontsize=12);\nplt.show()","48e682fb":"train.columns","44881e6a":"if 1 :\n    train['comorbidity_score'] = train['aids'].values * 23 + train['cirrhosis'] *4      + train['hepatic_failure'] *16 + train['immunosuppression'] *10     + train['leukemia'] * 10     + train['lymphoma'] * 13     + train['solid_tumor_with_metastasis'] * 11\n    test['comorbidity_score'] = test['aids'].values * 23    + test['cirrhosis'] *4    + test['hepatic_failure'] *16 + test['immunosuppression'] *10     + test['leukemia'] * 10     + test['lymphoma'] * 13     + test['solid_tumor_with_metastasis'] * 11\n    train['comorbidity_score'] = train['comorbidity_score'].fillna(0)\n    test['comorbidity_score'] = test['comorbidity_score'].fillna(0)\n    ","56c2ac25":"# Drop columns based on threshold limit\nif 1:\n    print(train.shape)\n    threshold = len(train) * 0.20\n    train=train.dropna(axis=1, thresh=threshold)\n    print(train.shape)","e5ebde8d":"train.shape , test.shape","805f3192":"## frequency encoding for icu_id \/ hospital \nall_df = pd.concat([train,test],axis=0)\nfrequence_encode = ['icu_id','hospital_id']\nfor col in frequence_encode :\n    hosp_asource = pd.DataFrame(all_df[col])\n    hosp_asource = hosp_asource.dropna()\n    hosp_asource.columns =[col]\n    fe = 100*(hosp_asource.groupby(col).size()\/len(hosp_asource))\n    train.loc[:,col+'_fe'] = train[col].map(fe)\n    test.loc[:,col+'_fe'] = test[col].map(fe)\n\n","76d4a611":"for bin_col in ['elective_surgery','apache_post_operative','arf_apache','intubated_apache','ventilated_apache','aids','cirrhosis','gcs_eyes_apache','gcs_verbal_apache',   'gcs_motor_apache' , 'gcs_unable_apache','hepatic_failure','immunosuppression','leukemia','lymphoma','solid_tumor_with_metastasis'] :\n    print(bin_col)\n    train[bin_col].fillna(10)\n    test[bin_col].fillna(10)","8246b396":"categoricals_features = set(['apache_3j_diagnosis_split0','apache_3j','apache_3j_diagnosis_split1','hospital_admit_source','icu_admit_source','icu_stay_type','icu_type','elective_surgery',\n                        'apache_post_operative','arf_apache','intubated_apache','ventilated_apache','aids','cirrhosis',\n                        'diabetes_mellitus','hepatic_failure','immunosuppression','leukemia','lymphoma','solid_tumor_with_metastasis',\n                         'apache_2_diagnosis'])       \n   ","7598fa68":"# this is the list of all input feature we would like our model to use (we remove the target and the ids.)\nto_remove=['hospital_id','icu_id','ethnicity','gender','patient_id','encounter_id','diabetes_mellitus','apache_4a_diabetes_mellitus_prob','apache_4a_icu_death_prob' ]\n\nfeatures = [col for col in train.columns if col not in to_remove]\n\n# this is a list of features that look like to be categorical\ncategoricals_features = [col for col in categoricals_features if col not in to_remove]\n","7b4e7651":"# categorical feature need to be transform to numeric for mathematical purpose.\n# different technics of categorical encoding exists here we will rely on our model API to deal with categorical\n# still we need to encode each categorical value to an id , for this purpose we use LabelEncoder\n\nprint('Transform all String features to category.\\n')\nfor usecol in categoricals_features:\n    colcount = train[usecol].value_counts().index[0]\n    train[usecol] = train[usecol].fillna(colcount)\n    test[usecol]  = test[usecol].fillna(colcount)\n    \n    train[usecol] = train[usecol].astype('str')\n    test[usecol] = test[usecol].astype('str')\n    \n    #Fit LabelEncoder\n    le = LabelEncoder().fit(\n            np.unique(train[usecol].unique().tolist()+\n                      test[usecol].unique().tolist()))\n\n    #At the end 0 will be used for dropped values\n    train[usecol] = le.transform(train[usecol])+1\n    test[usecol]  = le.transform(test[usecol])+1\n    \n    train[usecol] = train[usecol].replace(np.nan, -1).astype('int')\n    test[usecol]  = test[usecol].replace(np.nan , -1).astype('int')\n","d34796ea":"num_feature = [col for col in features if col not in categoricals_features]\ndrop_columns=[]\nlowvar_columns=[]\nif 1 :\n    threshold = 0.01\n    lowvar_columns = train[num_feature].std()[train[num_feature].std() < threshold].index.values\n    print(len(lowvar_columns),lowvar_columns)","4378bf18":"num_feature = [col for col in features if col not in categoricals_features and train[col].dtype != 'object']\ndrop_columns=[]\ncorr = train[num_feature].corr()\n# Drop highly correlated features \ncolumns = np.full((corr.shape[0],), True, dtype=bool)\n\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >=0.99 :\n            if columns[j]:\n                columns[j] = False\n                print('FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(train[num_feature].columns[i] , train[num_feature].columns[j], corr.iloc[i,j]))\n        elif corr.iloc[i,j] <= -0.995:\n            if columns[j]:\n                columns[j] = False\n\ndrop_columns = train[num_feature].columns[columns == False].values\nprint('drop_columns',len(drop_columns),drop_columns)","c3eb145a":"features = [col for col in features if col not in drop_columns]\nfeatures = [col for col in features if col not in lowvar_columns]","7501fd8d":"print('number of features ' , len(features))\nprint('shape of train \/ test ', train.shape , test.shape)","0b6801eb":"non_categorical = [f for f in features if f not in categoricals_features]","a86d731a":"# create a flag for null field\nfor i in non_categorical:\n    train[str(i)+\"_Na\"]=pd.get_dummies(train[i].isnull(),prefix=i).iloc[:,0]\n    test[str(i)+\"_Na\"]=pd.get_dummies(test[i].isnull(),prefix=i).iloc[:,0]\n    \ntrain['age']=train['age'].fillna(99)\ntest['age']=test['age'].fillna(99)\ntrain=train.fillna(train.median())\ntest=test.fillna(test.median())","9c6b21a6":"features = [col for col in train.columns if col not in to_remove]\nfeatures = [col for col in features if col not in drop_columns]\nfeatures = [col for col in features if col not in lowvar_columns]","9c7cc3ef":"num_feature = [col for col in features if col not in categoricals_features]\ndrop_columns=[]\nlowvar_columns=[]\nif 1 :\n    threshold = 0.01\n    lowvar_columns = train[num_feature].std()[train[num_feature].std() < threshold].index.values\n    print(len(lowvar_columns),lowvar_columns)","77f0b02e":"num_feature = [col for col in features if col not in categoricals_features and train[col].dtype != 'object']\ndrop_columns=[]\ncorr = train[num_feature].corr()\n# Drop highly correlated features \ncolumns = np.full((corr.shape[0],), True, dtype=bool)\n\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >=0.999 :\n            if columns[j]:\n                columns[j] = False\n                print('FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(train[num_feature].columns[i] , train[num_feature].columns[j], corr.iloc[i,j]))\n        elif corr.iloc[i,j] <= -0.995:\n            if columns[j]:\n                columns[j] = False\n\ndrop_columns = train[num_feature].columns[columns == False].values\nprint('drop_columns',len(drop_columns),drop_columns)","ea8b8736":"features = [col for col in features if col not in drop_columns]\nfeatures = [col for col in features if col not in lowvar_columns]","70769e98":"train.head()","fc5bb7da":"def auc(y_true, y_pred):\n    def fallback_auc(y_true, y_pred):\n        try:\n            return metrics.roc_auc_score(y_true, y_pred)\n        except:\n            return 0.5\n    return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)","f1b913db":"num_features = [f for f in features if f not in categoricals_features]","dee252a3":"data = pd.concat([train[features], test[features]]).reset_index(drop=True)","0e916e6e":"data.shape","e9fe7177":"print(len(categoricals_features))\nprint(len(num_features))","f8357d61":"list(categoricals_features)","280abd97":"list(num_features)","fcb71d20":" test_data = [np.absolute(test[i]) for i in categoricals_features] + [test[num_features]]","21df2dfd":"print(train.shape,test.shape)#, test_data.shape)","5ebae956":"numeric_min = [f for f in num_features if '_min' in f]\nnumeric_max = [f for f in num_features if '_max' in f]\nnumeric_apache = [f for f in num_features if 'apache' in f]\nnumeric_other = [f for f in num_features if f not in numeric_min and f not in numeric_max and f not in numeric_apache]\n\nprint('_____________ numeric_min ',len(numeric_min) , numeric_min)\nprint('_____________ numeric_max ',len(numeric_max) , numeric_max )                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \nprint('_____________ numeric_apache ',len(numeric_apache ) , numeric_apache )                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \nprint('_____________ numeric_other ',len(numeric_other) , numeric_other)\n","df302643":"import tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom sklearn import metrics\nfrom keras import backend as K\n\nMETRICS = [\n      tf.keras.metrics.AUC(name='auc'),\n]\n\ndef model_wids2020():    \n    inputs = []\n    embeddings  = []\n    for c in categoricals_features:\n        num_unique_values = int(data[c].nunique())\n        embed_dim = int(min(np.ceil((num_unique_values)\/2), 50))               \n        inp = layers.Input(shape=(1,))\n        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n        out = layers.SpatialDropout1D(0.5)(out)\n        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n        inputs.append(inp)\n        embeddings.append(out)\n\n    input_numeric1     = layers.Input(shape=(len(numeric_min),))\n    embedding_numeric1 = layers.Dense(250, activation='relu')(input_numeric1)\n    inputs.append(input_numeric1)\n    embeddings.append(embedding_numeric1)\n    input_numeric2     = layers.Input(shape=(len(numeric_max),))\n    embedding_numeric2 = layers.Dense(250, activation='relu')(input_numeric2)\n    inputs.append(input_numeric2)\n    embeddings.append(embedding_numeric2)\n    input_numeric3     = layers.Input(shape=(len(numeric_apache),))\n    embedding_numeric3 = layers.Dense(250, activation='relu')(input_numeric3)\n    inputs.append(input_numeric3)\n    embeddings.append(embedding_numeric3)\n    input_numeric4     = layers.Input(shape=(len(numeric_other),))\n    embedding_numeric4 = layers.Dense(50, activation='relu')(input_numeric4)\n    inputs.append(input_numeric4)\n    embeddings.append(embedding_numeric4)\n    \n    x = layers.Concatenate()(embeddings)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(128)(x)\n\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(32)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.Dropout(0.3)(x)\n    \n    y = layers.Dense(2, activation=\"softmax\")(x)\n    model = Model(inputs=inputs, outputs=y)\n    return model\n\nmodel = model_wids2020()\ntf.keras.utils.plot_model(model, show_shapes=True, to_file='embeddings.png')","40166d7f":"oof_preds  = np.zeros((len(train)))\ntest_preds = np.zeros((len(test)))\ny = train['diabetes_mellitus']\nFOLD= 7\nkf = RepeatedStratifiedGroupKFold(n_splits=FOLD, n_repeats=5, random_state=42)\nkf = StratifiedKFold(n_splits=10, shuffle=True, random_state=2020)\n\nfold = 0\nfor tdx, vdx in kf.split(train, train['diabetes_mellitus'].values):\n#for tdx, vdx in   kf.split(train, train['diabetes_mellitus'], train['hospital_id']):\n    X_train, X_val, y_train, y_val = train.iloc[tdx], train.iloc[vdx], y[tdx], y[vdx]\n    \n    scalar = StandardScaler()\n\n    X_train[num_features] = scalar.fit_transform(X_train[num_features])\n    X_val[num_features]   = scalar.transform(X_val[num_features])\n    test_data = test.copy()\n    test_data[num_features]  = scalar.transform(test_data[num_features])\n    \n    test_data = [np.absolute(test_data[i]) for i in categoricals_features] + [test_data[numeric_min]]+ [test_data[numeric_max]]+ [test_data[numeric_apache]]+ [test_data[numeric_other]]\n   \n    X_train = [np.absolute(X_train[i]) for i in categoricals_features] + [X_train[numeric_min]]+ [X_train[numeric_max]]+ [X_train[numeric_apache]]+ [X_train[numeric_other]]\n    X_val   = [np.absolute(X_val[i])   for i in categoricals_features] + [X_val[numeric_min]]  + [X_val[numeric_max]]  + [X_val[numeric_apache]]  + [X_val[numeric_other]]\n    \n    \n    model = model_wids2020()\n    \n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n    es = callbacks.EarlyStopping(monitor='val_auc', min_delta=0.001, patience=30,\n                                 verbose=1, mode='max', baseline=None, restore_best_weights=True)\n    rlr        = callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5,\n                                     patience=5, min_lr=1e-6, mode='max', verbose=1)\n   \n    model.fit(X_train,\n              utils.to_categorical(y_train),\n              validation_data=(X_val, utils.to_categorical(y_val)),\n              verbose=1,\n              batch_size=5024,\n              callbacks=[es, rlr],\n              epochs=100\n             )\n    valid_fold_preds = model.predict(X_val)[:, 1]\n    test_fold_preds  = model.predict(test_data)[:, 1]\n    oof_preds[vdx] = valid_fold_preds.ravel()\n    test_preds += test_fold_preds.ravel()\n    print('FOLD ',str(fold) , ' AUC ...............' ,metrics.roc_auc_score(y_val, valid_fold_preds))\n    fold=fold+1\n    K.clear_session()","ba471da1":"AUC_FINAL=metrics.roc_auc_score(train['diabetes_mellitus'].values, oof_preds)\nimport joblib\njoblib.dump(oof_preds , 'NN1_oof_preds_'+str(AUC_FINAL)+'.pkl')\njoblib.dump(test_preds, 'NN1_test_preds_'+str(AUC_FINAL)+'.pkl')\nprint(\"Overall AUC={}\".format(AUC_FINAL))","c29c8321":"test_preds \/= FOLD","439ca42d":"test[\"diabetes_mellitus\"] = test_preds\ntest[[\"encounter_id\",\"diabetes_mellitus\"]].to_csv(\"submission_nn_\"+ str(AUC_FINAL) +\".csv\",index=False)","778e94bf":"Courtesy: https:\/\/www.kaggle.com\/jayjay75\/3rd-place-nn-wids2020","b3fee885":"re remove new useless col","26c86fcd":"remove col","73e80146":"# feature eng .","7aacb7d8":"> Encode Null value","325836f5":"# categorical","210b8616":"read the data"}}