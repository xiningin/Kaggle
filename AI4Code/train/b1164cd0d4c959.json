{"cell_type":{"a88f585b":"code","f8de345b":"code","8445909f":"code","99a0053d":"code","16a76528":"code","8ce7bccd":"code","dddb9624":"code","20fc2d8c":"code","5b7defe2":"code","9fb95860":"code","621b3e6c":"code","2e9d3480":"code","5c7eccbb":"code","88ad95fe":"code","f49cc912":"code","38974bde":"code","bf1b5373":"code","898ac089":"code","d9d78752":"code","ad93617f":"code","57b5be0e":"code","3a44fc93":"code","ac601826":"code","f3b8f559":"code","617aba09":"code","04c2f484":"code","d5109016":"code","b7dc1a93":"code","b90ff74b":"code","7221344a":"code","57b54e22":"code","0f99ebfe":"code","c8c08248":"code","58b4fec1":"code","e7386cb9":"code","4b555f2a":"code","faa9e789":"code","b78df14e":"code","f9cbe4f0":"code","968b3d76":"code","6cdadac8":"code","4a2df16b":"code","6b3123ba":"code","7ba73c65":"code","991cdecd":"markdown","0c1389a0":"markdown","4412272f":"markdown","2fbbf47f":"markdown","c29ea09c":"markdown","ff327cc8":"markdown","82f3f110":"markdown","db2a904c":"markdown"},"source":{"a88f585b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# importing ploting libraries\nimport matplotlib.pyplot as plt   \n\n#importing seaborn for statistical plots\nimport seaborn as sns\n\n#Let us break the X and y dataframes into training set and test set. For this we will use\n#Sklearn package's data splitting function which is based on random function\n\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\n\n\n# calculate accuracy measures and confusion matrix\nfrom sklearn import metrics\n\n\nfrom sklearn.metrics import recall_score\n\nfrom imblearn.over_sampling import SMOTE\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f8de345b":"#Reading the dataset\npima_df = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")","8445909f":"pima_df.head()","99a0053d":"# Let us check whether any of the columns has any value other than numeric i.e. data is not corrupted such as a \"?\" instead of \n# a number.\n\n# we use np.isreal a numpy function which checks each column for each row and returns a bool array, \n# where True if input element is real.\n# applymap is pandas dataframe function that applies the np.isreal function columnwise\n# Following line selects those rows which have some non-numeric value in any of the columns hence the  ~ symbol\n\npima_df[~pima_df.applymap(np.isreal).all(1)]","16a76528":"# replace the missing values in pima_df with median value :Note, we do not need to specify the column names\n# every column's missing value is replaced with that column's median respectively\npima_df = pima_df.fillna(pima_df.median())\npima_df","8ce7bccd":"#Lets analysze the distribution of the various attributes\npima_df.describe().transpose()","dddb9624":"# Let us look at the target column which is 'Outcome' to understand how the data is distributed amongst the various values\npima_df.groupby([\"Outcome\"]).count()\n\n# Most are not diabetic. The ratio is almost 1:2 in favor or class 0.  The model's ability to predict class 0 will \n# be better than predicting class 1. ","20fc2d8c":"# Pairplot using sns\n\nsns.pairplot(pima_df , hue='Outcome' , diag_kind = 'kde')","5b7defe2":"#data for all the attributes are skewed, especially for the variable \"Insulin\"\n\n#The mean for test is 80(rounded) while the median is 30.5 which clearly indicates an extreme long tail on the right","9fb95860":"# Attributes which look normally distributed (glucose, blood pressure, skin thickness, and BMI).\n# Some of the attributes look like they may have an exponential distribution (pregnancy, insulin, DiabetesPedigreeFunction, age).\n# Age should probably have a normal distribution, the constraints on the data collection may have skewed the distribution.\n\n# There is no obvious relationship between age and onset of diabetes.\n# There is no obvious relationship between DiabetesPedigreeFunction function and onset of diabetes.","621b3e6c":"array = pima_df.values\nX = array[:,0:7] # select all rows and first 8 columns which are the attributes\nY = array[:,8]   # select all rows and the 8th column which is the classification \"Yes\", \"No\" for diabeties\ntest_size = 0.30 # taking 70:30 training and test set\nseed = 7  # Random numbmer seeding for reapeatability of the code\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\ntype(X_train)","2e9d3480":"print(\"Before UpSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before UpSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(sampling_strategy = 1 ,k_neighbors = 5, random_state=1)   #Synthetic Minority Over Sampling Technique\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n\n\nprint(\"After UpSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After UpSampling, counts of label '0': {} \\n\".format(sum(y_train_res==0)))\n\n\n\nprint('After UpSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After UpSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))","5c7eccbb":"# Fit the model on original data i.e. before upsampling\n\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\nmodel_score = model.score(X_test, y_test)\nprint(model_score)","88ad95fe":"test_pred = model.predict(X_test)\n\nprint(metrics.classification_report(y_test, test_pred))\nprint(metrics.confusion_matrix(y_test, test_pred))","f49cc912":"# fit model on upsampled data \n\nmodel.fit(X_train_res, y_train_res)\ny_predict = model.predict(X_test)\nmodel_score = model.score(X_test, y_test)\nprint(model_score)\nprint(metrics.confusion_matrix(y_test, y_predict))\nprint(metrics.classification_report(y_test, y_predict))","38974bde":"non_diab_indices = pima_df[pima_df['Outcome'] == 0].index   # Get the record numbers of non-diab cases\nno_diab = len(pima_df[pima_df['Outcome'] == 0])             # how many non-diab cases\nprint(no_diab)\n\ndiab_indices = pima_df[pima_df['Outcome'] == 1].index       # record number of the diabeteics cases\ndiab = len(pima_df[pima_df['Outcome'] == 1])                # how many diabetic cases\nprint(diab)","bf1b5373":"random_indices = np.random.choice( non_diab_indices, no_diab - 200 , replace=False)    #Randomly pick up 200 non-diab indices","898ac089":"down_sample_indices = np.concatenate([diab_indices,random_indices])  # combine the 200 non-diab indices with diab indices","d9d78752":"pima_df_down_sample = pima_df.loc[down_sample_indices]  # Extract all those records for diab and non-diab to create new set\npima_df_down_sample.shape\npima_df_down_sample.groupby([\"Outcome\"]).count()  # look at the class distribution after downsample","ad93617f":"array = pima_df_down_sample.values\nX = array[:,0:7] # select all rows and first 8 columns which are the attributes\nY = array[:,8]   # select all rows and the 8th column which is the classification \"Yes\", \"No\" for diabeties\ntest_size = 0.30 # taking 70:30 training and test set\nseed = 7  # Random numbmer seeding for reapeatability of the code\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\ntype(X_train)","57b5be0e":"print('After DownSampling, the shape of X_train: {}'.format(X_train.shape))\nprint('After DownSampling, the shape of X_test: {} \\n'.format(X_test.shape))","3a44fc93":"# Fit the model on 30%\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\nmodel_score = model.score(X_test, y_test)\nprint(model_score)\nprint(metrics.confusion_matrix(y_test, y_predict))\nprint(metrics.classification_report(y_test, y_predict))","ac601826":"from imblearn.under_sampling import RandomUnderSampler","f3b8f559":"rus = RandomUnderSampler(return_indices=True)","617aba09":"X_rus, y_rus, id_rus = rus.fit_sample(X_train, y_train)","04c2f484":"y_rus","d5109016":"y_rus.shape","b7dc1a93":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler()\n\nX_ros, y_ros = ros.fit_sample(X_train, y_train)","b90ff74b":"y_ros","7221344a":"y_ros.shape","57b54e22":"X_ros.shape","0f99ebfe":"from imblearn.under_sampling import TomekLinkstl = TomekLinks(return_indices=True, ratio='majority')\n","c8c08248":"tl = TomekLinks(return_indices=True, ratio='majority')","58b4fec1":"X_tl, y_tl, id_tl = tl.fit_sample(X_train, y_train)   # id_tl is removed instances of majority class","e7386cb9":"y_tl.shape","4b555f2a":"X_tl.shape","faa9e789":"from imblearn.combine import SMOTETomek","b78df14e":"smt = SMOTETomek(ratio='auto')","f9cbe4f0":"X_smt, y_smt = smt.fit_sample(X_train, y_train)","968b3d76":"X_smt.shape","6cdadac8":"from imblearn.under_sampling import ClusterCentroids","4a2df16b":"cc = ClusterCentroids()  \nX_cc, y_cc = cc.fit_sample(X_train, y_train)","6b3123ba":"X_cc.shape","7ba73c65":"y_cc","991cdecd":"## Upsampling followed by downsampling\n","0c1389a0":"## IMBLearn Random Under Sampling","4412272f":"## Cluster based undersampling\n","2fbbf47f":"##  Deleting nearest majority neighbors  TomekLinks\n","c29ea09c":"# SMOTE to upsample smaller class","ff327cc8":"## IMBLearn Random Over Sampling\n","82f3f110":"# Down Sampling the larger class","db2a904c":"# UpSample smaller class"}}