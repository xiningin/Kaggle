{"cell_type":{"e8c8f35d":"code","8e1dd383":"code","413c8263":"code","d6064de4":"code","d098b731":"code","d6cd00eb":"code","a0886887":"code","6ecbd392":"code","ee999389":"code","e5e4e50f":"code","7faee6dd":"code","2d71e819":"code","1d12ba1e":"code","0964141f":"markdown","a5e96a63":"markdown","9cadbbed":"markdown"},"source":{"e8c8f35d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8e1dd383":"!wget http:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/spa-eng.zip\n!unzip -q spa-eng.zip","413c8263":"# The text contains english and spanish words, arranged correspondingly \n\ntext_file = 'spa-eng\/spa.txt'\n\nwith open(text_file) as f:\n    lines = f.read().split('\\n')\n    \ntext_pairs = []\nfor line in lines:\n    english = line.split(\"\\t\")[0]\n    # since we're going to translating from English to Spanish,\n    # The translation part is all about predicting the next word or character given an English word\n    # Therefore, we need a way to signal to the model when translation is starting and when it's ending\n    spanish = \"[start] \" + line.split(\"\\t\")[-1] + \" [end]\"\n    text_pairs.append((english, spanish))","d6064de4":"import string\nimport random\nimport re\n\n# string module in python provides default punctuactions, and you can see them by printing like this\n# print(string.punctuation)\n\nchars = string.punctuation\n# Some special punctuation marks are not included, so, you can add to the existing ones\nchars = chars + \"\u00bf\"\n# since [ and ] will be used as part of the translation \"[start]\" and \"[end]\". You can remove them from the punctuation\nchars = chars.replace(\"[\", \"\")\nchars = chars.replace(\"]\", \"\")\n\nrandom.seed(1334)\n# checking some random text\nrandom.choice(text_pairs)","d098b731":"# Once the punctuations have been dealt with, the next stage is about the dataset\n# split the dataset into Training, Validation and Test sets\n\n# size of the validation set\nvalid_samples = int(0.15*len(text_pairs))\n# size of the training set\ntrain_samples = len(text_pairs) - 2*valid_samples\n\n# shuffle the data to avoid creating unbalanced dataset\nrandom.seed(1334)\nrandom.shuffle(text_pairs)\n\n# now split the data into the respective sections\ntrain_pairs = text_pairs[:train_samples]\nvalid_pairs = text_pairs[train_samples:train_samples+valid_samples]\ntest_pairs = text_pairs[train_samples+valid_samples:]\n\n# check the sizes of each of the splits\nprint(\"# training examples: \", len(train_pairs))\nprint(\"# validation examples: \", len(valid_pairs))\nprint(\"# test examples: \", len(test_pairs))","d6cd00eb":"# We're almost there, but first we need to import the necessary libraries to be used.\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np","a0886887":"# Custom standardization.\n# Remember the punctuations we made at the top, its time to standardize them so that they can be used in the texts\n# First stage in standardize is to convert the texts to lowercase, \n# then tokenize by splitting the sentences into individual words called tokens\n# using the tokens, either their indices or lengths will be used to vectorize them\n# But don't worry, steps are all handled by tensorflow.\n\ndef custom_standardization(text):\n    # first\n    lowercase = tf.strings.lower(text)\n    # regex, is a regular expression, it definitely deserves a topic of its own. \n    # for our case, we need to replace all the punctuations in the text with just space\n    return tf.strings.regex_replace(\n        lowercase, f\"[{re.escape(chars)}]\", \"\"\n    )","6ecbd392":"# Neural networks deal with only integers\n# as such need to convert to integers\nvocab_size = 15000     # this represents the number of most commonly used words, somtimes its referred to as max_tokens\nsequence_length=20     # the length of each paragraph, limit it to the first 20 words\n\n# source vectorization, this represents the English translation part\n# automatically deals with the punctuations\nsource_vectorization = layers.TextVectorization(\n    max_tokens=vocab_size,\n    output_mode='int',\n    output_sequence_length=sequence_length\n)\n\n# target vectorization, this represents the Spanish translation part\n# and now we can apply the custom standardization to the texts\ntarget_vectorization = layers.TextVectorization(\n    max_tokens=vocab_size,\n    output_mode='int',\n    output_sequence_length=sequence_length+1,  # remember for translation, we're always trying to predict the next character.\n    standardize=custom_standardization,\n)\n\n# remember from the part on file operation\n# we created a list containing, (english, spanish) texts,\n# now we can split these into their respective components.\ntrain_english_texts = [pair[0] for pair in train_pairs]\ntrain_spanish_texts = [pair[1] for pair in train_pairs]\n\n# since the training set is bigger than the validation and test set, we use it to create \n# the vectors that will represent the texts\nsource_vectorization.adapt(train_english_texts)\ntarget_vectorization.adapt(train_spanish_texts)\n\n# DON'T WORRY ABOUT THE TENSORFLOW MESSAGE BELOW!!!!","ee999389":"# As a good machine learning practice, its always recommended to batch your dataset\nbatch_size = 64\n# the format_dataset, will be used to now convert each of specific languages into \n# vectors\ndef format_dataset(eng, spa):\n    eng = source_vectorization(eng)\n    spa = target_vectorization(spa)\n    return ({\n        \"english\":eng,\n        \"spanish\":spa[:, :-1]   # both the spanish and english texts need to be of the same length.\n    }, spa[:, 1:])\n\n# using tensorflow's tf.data.Dataset to create datasets\ndef make_dataset(pairs):\n    # the input pairs, represents, (english,spanish) texts from the training, validation and test sets\n    # we can explore, how zip, dict, map and generators, work. They are very essential,\n    # for now, just follow (:\n    eng_texts, spa_texts = zip(*pairs)\n    eng_texts = list(eng_texts)\n    spa_texts = list(spa_texts)\n    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n    # before applying the format_dataset function, its highly recommended that you batch the files.\n    dataset = dataset.batch(batch_size=batch_size)\n    # number of parallel calls refers to distributing the workload among the available cpus or gpus\n    # its recommended to use for better performance\n    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n    # always cache your dataset at the end. This means, the data will always be easily accessed rather than \n    # storing it memory, which makes it harder to access\n    # prefetch usually takes a default value provided by tf.data.AUTOTUNE, \n    return dataset.shuffle(2048).cache().prefetch(16)\n\ntry:\n    # since tensorflow always keeps things in memory, remember to always delete \n    # otherwise, they will be change in the structure of the dataset, when you run this section multiple times\n    del train_ds, valid_ds, test_ds\nexcept:\n    pass\n\n# now, its time to create the datasets\ntrain_ds = make_dataset(train_pairs)\nvalid_ds = make_dataset(valid_pairs)\ntest_ds = make_dataset(test_pairs)","e5e4e50f":"train_ds","7faee6dd":"# Creating an end to end model Recurrent Neural Network encoder and decoder model\n# First the encoder will only take the english words as inputs, \n# creating an encoder\nembed_dim=512     # embed dim is used to create a matrix that generates semantic or geometric relationship among words\nlatent_dim=64     # this is the size of the RNN or LSTM or GRU, this can be any value, 32, 128, 256. But there is memory cost. \n\nsource = keras.Input(shape=(None,), dtype='int64', name='english')\n# mask zero is used to reduce the number of zeros, this enables faster calculations\nx = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(source)  \nencoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode='sum')(x)\n\n# creating a decoder;\n# a decoder takes the spanish words as inputs\ntarget_input = keras.Input(shape=(None,), dtype='int64', name='spanish')\n\nx = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(target_input)\n# return sequences, ensures that another GRU or LSTM layer can be used \ndecoder_gru = layers.GRU(latent_dim, return_sequences=True)\n\n# the initial state can be used as a variable for the encoded source\nx = decoder_gru(x, initial_state=encoded_source)\nx = layers.Dropout(0.5)(x)\n\n# we are doing translation, therefore, for every english word, we need a spanish corresponces\n# softmax produces probabilities of the outputs\noutputs = layers.Dense(vocab_size, activation='softmax')(x)\nmodel = keras.Model([source, target_input], outputs)\n\nmodel.summary()","2d71e819":"# This probably the easiest part in the whole notebook\nmodel.compile(optimizer='rmsprop',\n             loss='sparse_categorical_crossentropy',\n             metrics=['accuracy'])\n\n# remember this can be take a lot of time.\n# When learning, I will usually like to use very small epochs like 3, or 5.\n# for this demonstration, i'll use 10\nmodel.fit(train_ds, validation_data=valid_ds, epochs=10)","1d12ba1e":"# Time to check how our model performs.\n# Target vectorization contains the most commonly used spanish words\nspa_vocab = target_vectorization.get_vocabulary()\n# we need to create a dictionary to hold. This will hold, the position of each word eg {0: 'y', 1:'mes', 2:'bone', ...}\nspa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n# the size of the words to predicted\nmax_decoded_sentence_length = 20\n\n# decode sequence will be used to take in raw English inputs \ndef decode_sequence(input_sentence):\n    # since we're taking raw English inputs, we need to vectorize them\n    tokenized_input_sentence = source_vectorization([input_sentence])\n    # In the beginning, I said, that translation is all about predicting the next word\n    # for that we need a way to tell the model where to begin and end. \n    # As such, we'll be using \"[start]\" to signal the model to predict the next spanish word, given a english word.\n    # when the model predicts \"[end]\", then, we'll break out of the loop.\n    decoded_sentence = \"[start]\"\n    for i in range(max_decoded_sentence_length):\n        # now we need to vectorize the predicted spanish words that is the decoded sentence.\n        # this will be used as input for the decoder model.\n        tokenized_target_sentence = target_vectorization([decoded_sentence])\n        # to predict the next word, the model will need an english and a previous spanish word\n        next_token_prediction = model.predict([tokenized_input_sentence, tokenized_target_sentence])\n        # since our outputs are probabilities, we need to find the location of the highest probability.\n        # argmax is used to find position of the word with the largest probability.\n        sampled_token_index = np.argmax(next_token_prediction[0, i, :])\n        # after finding the location, we can use the dictionary we created, to find the word at that exact location.\n        sample_token = spa_index_lookup[sampled_token_index]\n        # then add this word to the already decoded sentences or predicted sentences or translated sentences\n        decoded_sentence += \" \" + sample_token\n        # if the position predicted correspongs to the \"[end]\", then break out of the loop.\n        if sample_token == '[end]':\n            break\n    # finally return the decoded sentence or translated sentence or predicted sentence\n    return decoded_sentence\n\n# we need to translate english words\n# the test pair contains (english, spanish) pairs. we need to translate the english words to spanish\ntest_eng_texts = [pair[0] for pair in test_pairs]\n# let's try and translate 10 english sentence\nimport time\n# just to see the translation, one step at a time.\nfor _ in range(10):\n    # since test pairs, contains, many sentences, we can randomly choose some sentences to be translated\n    input_sentence = random.choice(test_eng_texts)\n    print(\"-\")\n    print(input_sentence)\n    print(decode_sequence(input_sentence))\n    time.sleep(3)","0964141f":"# **DOWNLOAD THE DATA**\n\nAll language translation datasets can be found at manythings.anki.org\n\nFor this tutorial, we'll be using the ones provided by googleapis.com\n\nDownloading of the dataset can be done using !wget or !curl -O, which are common ways of downloading on Mac OS or Linux","a5e96a63":"**FOR MORE**\n\nCheck Deep learning with Python second edition\n\nAlso check the keras and tensorflow documentaion. They are gemstones.","9cadbbed":"# **FILE OPERATIONS**\n\nWorking with files is very interesting and scary initially, but its totally worth it.\n\nFor the steps, I highly suggest, that you unpack the lines piece by piece, like this for example\n\nwith open('file.txt') as f:\n\n    line = f.read()\n    \nprint(line)\n\nThis will enable you to see how the files are arranged and that way, you could make the necessary \nadjustments"}}