{"cell_type":{"dcfeee85":"code","70bdc1ca":"code","f463e905":"code","d0418559":"code","1fa6fe20":"code","ea085f23":"code","86ab8c16":"code","a7eeac87":"code","0c716416":"code","e9f51193":"code","37a9b4ec":"code","68f5f107":"markdown","4cd8be5e":"markdown","8d3c2e0c":"markdown","e0575d85":"markdown","179be571":"markdown","765295bf":"markdown","f0ca4be9":"markdown","42eedfd2":"markdown","61bbb3f2":"markdown","b0a2e8bf":"markdown"},"source":{"dcfeee85":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","70bdc1ca":"# read in all our data\nX_data = pd.read_json(\"..\/input\/covid-patient-datasets\/covid.json\")\nX_data['result_test'] = 'yes'\n#X_data.info()\nto_drop=['#','age']\nX_data.drop(to_drop,inplace=True,axis=1)\n#print(X_data.shape)\nX_data.head()\nnum_iter=X_data.duplicated().sum()\nprint(f'{num_iter} of data are iterated')\nX_data=X_data.drop_duplicates()\ndisplay(X_data)","f463e905":"from sklearn import preprocessing\n\nX_data.replace(('yes','es', 'Yes', 'no', 'No' , ''), (1, 1, 1, 0, 0 , 0), inplace=True)\nY_data = X_data.copy()\nY_data.replace((1, 0), (0, 1), inplace=True)\n\ndata = X_data.append(Y_data, ignore_index=True)\n# Randomize the dataset\ndata = data.sample(frac=1, random_state=1)\nfeature_vector = X_data.columns\ndata_normalized = preprocessing.normalize(data)\ndisplay(data)\ndisplay(X_data)\ndisplay(Y_data)\ndisplay(data_normalized)","d0418559":"import numpy as np\n\nd = np.array(data_normalized)[:,:-1]\ntarget = np.array(data_normalized)[:,-1]\n\n# traing function to implement Find-s algorithm\ndef find_s(c,t):\n    for i, val in enumerate(t):\n        if val == 1:\n            specific_hypothesis = c[i].copy()\n            break\n             \n    for i, val in enumerate(c):\n        if t[i] == 1:\n            for x in range(len(specific_hypothesis)):\n                if val[x] != specific_hypothesis[x]:\n                    specific_hypothesis[x] = -1\n                else:\n                    pass\n                 \n    return specific_hypothesis\n\n\nprint(\"The final hypothesis is:\",find_s(d,target))","1fa6fe20":"d = np.array(data_normalized)[:,:-1]\ntarget = np.array(data_normalized)[:,-1]\n\n#Candidate Elimination algorithm\ndef candidate_elimination(concepts, target):\n    specific_h = concepts[0].copy()\n    print(\"Initialization of specific_h and general_h\")\n    print(\"specific_h: \",specific_h)\n    general_h = [[-1 for i in range(len(specific_h))] for i in range(len(specific_h))]\n    print(\"general_h: \",general_h)\n    print(\"concepts: \",concepts)\n    for i, h in enumerate(concepts):\n        if target[i] == 1:\n            for x in range(len(specific_h)):\n                if h[x] != specific_h[x]:\n                    specific_h[x] = -1\n                    general_h[x][x] = -1\n        if target[i] == 0:\n            for x in range(len(specific_h)):\n                if h[x] != specific_h[x]:\n                    general_h[x][x] = specific_h[x]\n                else:\n                    general_h[x][x] = -1\n    print(\"\\nSteps of Candidate Elimination Algorithm: \",i+1)\n    print(\"Specific_h: \",i+1)\n    print(specific_h,\"\\n\")\n    print(\"general_h :\", i+1)\n    print(general_h)\n    indices = [i for i, val in enumerate(general_h) if val == [-1, -1, -1, -1, -1, -1]]\n    print(\"\\nIndices\",indices)\n    for i in indices:\n        general_h.remove([-1, -1, -1, -1, -1, -1])\n    return specific_h, general_h\n\ns_final,g_final = candidate_elimination(d, target)\nprint(\"\\nFinal Specific_h:\", s_final, sep=\"\\n\")\nprint(\"Final General_h:\", g_final, sep=\"\\n\")","ea085f23":"d = np.array(data)[:,:-1]\ntarget = np.array(data)[:,-1]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(d, target, test_size = 0.30, random_state = 0)","86ab8c16":"# Naive Bayes Algorithm\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_temp = sc.fit_transform(X_train)\nX_test_temp = sc.transform(X_test)\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train_temp, Y_train)\n\ny_pred  =  classifier.predict(X_test_temp)\n\nfrom sklearn.metrics import confusion_matrix,accuracy_score\ncm = confusion_matrix(Y_test, y_pred)\nac = accuracy_score(Y_test,y_pred)\n\nprint(ac)","a7eeac87":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=5)\nclassifier.fit(X_train, Y_train)\ny_pred1 = classifier.predict(X_test)\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(Y_test, y_pred1))\nprint(classification_report(Y_test, y_pred1))","0c716416":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,Y_train)\n\n#Predict the response for test dataset\ny_pred3 = clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, y_pred3))","e9f51193":"import math\nfrom collections import deque\n\nclass Node:\n    \"\"\"Contains the information of the node and another nodes of the Decision Tree.\"\"\"\n    def __init__(self):\n        self.value = None\n        self.next = None\n        self.childs = None\n\n\nclass DecisionTreeClassifier:\n    \"\"\"Decision Tree Classifier using ID3 algorithm.\"\"\"\n\n    def __init__(self, X, feature_names, labels):\n        self.X = X\n        self.feature_names = feature_names\n        self.labels = labels\n        self.labelCategories = list(set(labels))\n        self.labelCategoriesCount = [list(labels).count(x) for x in self.labelCategories]\n        self.node = None\n        self.entropy = self._get_entropy([x for x in range(len(self.labels))])  # calculates the initial entropy\n\n    def _get_entropy(self, x_ids):\n        \"\"\" Calculates the entropy.\n        Parameters\n        __________\n        :param x_ids: list, List containing the instances ID's\n        __________\n        :return: entropy: float, Entropy.\n        \"\"\"\n        # sorted labels by instance id\n        labels = [self.labels[i] for i in x_ids]\n        # count number of instances of each category\n        label_count = [labels.count(x) for x in self.labelCategories]\n        # calculate the entropy for each category and sum them\n        entropy = sum([-count \/ len(x_ids) * math.log(count \/ len(x_ids), 2) if count else 0 for count in label_count])\n        return entropy\n\n    def _get_information_gain(self, x_ids, feature_id):\n        \"\"\"Calculates the information gain for a given feature based on its entropy and the total entropy of the system.\n        Parameters\n        __________\n        :param x_ids: list, List containing the instances ID's\n        :param feature_id: int, feature ID\n        __________\n        :return: info_gain: float, the information gain for a given feature.\n        \"\"\"\n        # calculate total entropy\n        info_gain = self._get_entropy(x_ids)\n        # store in a list all the values of the chosen feature\n        x_features = [self.X[x][feature_id] for x in x_ids]\n        # get unique values\n        feature_vals = list(set(x_features))\n        # get frequency of each value\n        feature_vals_count = [x_features.count(x) for x in feature_vals]\n        # get the feature values ids\n        feature_vals_id = [\n            [x_ids[i]\n            for i, x in enumerate(x_features)\n            if x == y]\n            for y in feature_vals\n        ]\n\n        # compute the information gain with the chosen feature\n        info_gain = info_gain - sum([val_counts \/ len(x_ids) * self._get_entropy(val_ids)\n                                     for val_counts, val_ids in zip(feature_vals_count, feature_vals_id)])\n\n        return info_gain\n\n    def _get_feature_max_information_gain(self, x_ids, feature_ids):\n        \"\"\"Finds the attribute\/feature that maximizes the information gain.\n        Parameters\n        __________\n        :param x_ids: list, List containing the samples ID's\n        :param feature_ids: list, List containing the feature ID's\n        __________\n        :returns: string and int, feature and feature id of the feature that maximizes the information gain\n        \"\"\"\n        # get the entropy for each feature\n        features_entropy = [self._get_information_gain(x_ids, feature_id) for feature_id in feature_ids]\n        # find the feature that maximises the information gain\n        max_id = feature_ids[features_entropy.index(max(features_entropy))]\n\n        return self.feature_names[max_id], max_id\n\n    def id3(self):\n        \"\"\"Initializes ID3 algorithm to build a Decision Tree Classifier.\n        :return: None\n        \"\"\"\n        x_ids = [x for x in range(len(self.X))]\n        feature_ids = [x for x in range(len(self.feature_names))]\n        self.node = self._id3_recv(x_ids, feature_ids, self.node)\n        print('')\n\n    def _id3_recv(self, x_ids, feature_ids, node):\n        \"\"\"ID3 algorithm. It is called recursively until some criteria is met.\n        Parameters\n        __________\n        :param x_ids: list, list containing the samples ID's\n        :param feature_ids: list, List containing the feature ID's\n        :param node: object, An instance of the class Nodes\n        __________\n        :returns: An instance of the class Node containing all the information of the nodes in the Decision Tree\n        \"\"\"\n        if not node:\n            node = Node()  # initialize nodes\n        # sorted labels by instance id\n        labels_in_features = [self.labels[x] for x in x_ids]\n        # if all the example have the same class (pure node), return node\n        if len(set(labels_in_features)) == 1:\n            node.value = self.labels[x_ids[0]]\n            return node\n        # if there are not more feature to compute, return node with the most probable class\n        if len(feature_ids) == 0:\n            node.value = max(set(labels_in_features), key=labels_in_features.count)  # compute mode\n            return node\n        # else...\n        # choose the feature that maximizes the information gain\n        best_feature_name, best_feature_id = self._get_feature_max_information_gain(x_ids, feature_ids)\n        node.value = best_feature_name\n        node.childs = []\n        # value of the chosen feature for each instance\n        feature_values = list(set([self.X[x][best_feature_id] for x in x_ids]))\n        # loop through all the values\n        for value in feature_values:\n            child = Node()\n            child.value = value  # add a branch from the node to each feature value in our feature\n            node.childs.append(child)  # append new child node to current node\n            child_x_ids = [x for x in x_ids if self.X[x][best_feature_id] == value]\n            if not child_x_ids:\n                child.next = max(set(labels_in_features), key=labels_in_features.count)\n                print('')\n            else:\n                if feature_ids and best_feature_id in feature_ids:\n                    to_remove = feature_ids.index(best_feature_id)\n                    feature_ids.pop(to_remove)\n                # recursively call the algorithm\n                child.next = self._id3_recv(child_x_ids, feature_ids, child.next)\n        return node\n\n    def printTree(self):\n        if not self.node:\n            return\n        nodes = deque()\n        nodes.append(self.node)\n        while len(nodes) > 0:\n            node = nodes.popleft()\n            print(node.value)\n            if node.childs:\n                for child in node.childs:\n                    print('({})'.format(child.value))\n                    nodes.append(child.next)\n            elif node.next:\n                print(node.next)\n\ntree_clf = DecisionTreeClassifier(X=X_train, feature_names=feature_vector[:-1], labels=Y_train)\nprint(\"System entropy {:.4f}\".format(tree_clf.entropy))","37a9b4ec":"resultDicDT = {}\nresultDicNB = {}\n\nfor i in range(len(X_train[0])):\n    X_train_removed_feature = X_train\n    X_test_removed_feature = X_test\n    for row in X_train_removed_feature:\n        np.delete(row,i)\n    for row in X_test_removed_feature:\n        np.delete(row,i)\n  \n    # Decision Tree\n    # Create Decision Tree classifer object\n    clf = DecisionTreeClassifier()\n    # Train Decision Tree Classifer\n    clf = clf.fit(X_train_removed_feature ,Y_train)\n    #Predict the response for test dataset\n    y_pred3 = clf.predict(X_test_removed_feature)\n    # Model Accuracy, how often is the classifier correct?\n    ac = metrics.accuracy_score(Y_test, y_pred3)\n    resultDicDT[i] = ac\n\n    # Naive Bayes Algorithm\n    from sklearn.preprocessing import StandardScaler\n    sc = StandardScaler()\n    X_train_temp = sc.fit_transform(X_train_removed_feature)\n    X_test_temp = sc.transform(X_test_removed_feature)\n    from sklearn.naive_bayes import GaussianNB\n    classifier = GaussianNB()\n    classifier.fit(X_train_temp, Y_train)\n    y_pred  =  classifier.predict(X_test_temp)\n    from sklearn.metrics import confusion_matrix,accuracy_score\n    ac = accuracy_score(Y_test,y_pred)\n    resultDicNB[i] = ac\n\ndef translate(i):\n    for f, b in zip(data.columns,range(len(data.columns))):\n        if i == b:\n          return f\n\nprint(\"\\nDT:\")  \nsort_orders_DT = sorted(resultDicDT.items(), key=lambda x: x[1], reverse=True)\nfor i in sort_orders_DT:\n    print(\"After removing feature\", translate(i[0]) ,\"Accuracy is:\", i[1])\n\nprint(\"\\nNB:\")\nsort_orders_NB = sorted(resultDicNB.items(), key=lambda x: x[1], reverse=True)\nfor i in sort_orders_NB:\n    print(\"After removing feature\", translate(i[0]) ,\"Accuracy is:\", i[1])","68f5f107":"<div dir=\"rtl\">\n    \u062f\u0631 \u0627\u06cc\u0646 \u0642\u0633\u0645\u062a \u06a9\u0627\u0631\u0647\u0627\u06cc \u0632\u06cc\u0631 \u0631\u0627 \u0627\u0646\u062c\u0627\u0645  \u0645\u06cc \u062f\u0647\u06cc\u0645\n    \n    * \u062e\u0648\u0627\u0646\u062f\u0646 \u0641\u0627\u06cc\u0644 \u0627\u0632 \u067e\u0627\u06cc\u06af\u0627\u0647 \u062f\u0627\u062f\u0647\n    * \u0627\u0636\u0627\u0641\u0647 \u06a9\u0631\u062f\u0646 \u0628\u0631\u0686\u0633\u0628 \u0645\u062b\u0628\u062a \u0628\u0648\u062f\u0646 \u0646\u062a\u06cc\u062c\u0647 \u062a\u0633\u062a \u0628\u0647 \u062f\u0627\u062f\u0647 \u0647\u0627\n    * \u062d\u0630\u0641 \u0633\u062a\u0648\u0646 \u0647\u0627\u06cc id \u0648 \u0633\u0646 \u0628\u0647 \u062f\u0644\u06cc\u0644 \u0646\u06cc\u0627\u0632 \u0646\u0628\u0648\u062f\u0646 \u0648 \u06a9\u0627\u0645\u0644 \u0646\u0628\u0648\u062f\u0646\n    * \u062d\u0630\u0641 \u0633\u0637\u0631\u0647\u0627\u06cc \u062a\u06a9\u0631\u0627\u0631\u06cc \u0627\u0632 \u067e\u0627\u06cc\u06af\u0627\u0647 \u062f\u0627\u062f\u0647\n<\/div>\n","4cd8be5e":"**Candidate Elimination algorithm**","8d3c2e0c":"**KNN**","e0575d85":"**Split Train and Test data**","179be571":"**Find 5 worst features**","765295bf":"**Naive Bayes Algorithm**","f0ca4be9":"**Decision Tree ID3 Algorithm**","42eedfd2":"<div dir=\"rtl\">\n    \u062f\u0631 \u0627\u062f\u0627\u0645\u0647 \u06a9\u0627\u0631\u0647\u0627\u06cc \u0632\u06cc\u0631 \u0631\u0627 \u0628\u0631\u0627\u06cc \u0645\u0647\u06cc\u0627 \u06a9\u0631\u062f\u0646 \u062f\u0627\u062f\u0647\u200c\u0647\u0627 \u0627\u0646\u062c\u0627\u0645  \u0645\u06cc\u200c\u062f\u0647\u06cc\u0645\n    \n    * \u0639\u062f\u062f\u06cc \u06a9\u0631\u062f\u0646 \u067e\u0627\u06cc\u06af\u0627\u0647 \u062f\u0627\u062f\u0647 \u0628\u0627 \u062a\u0628\u062f\u06cc\u0644 \u0645\u0642\u0627\u062f\u06cc\u0631 yes \u0628\u0647 1 \u0648 no \u0628\u0647 \u0635\u0641\u0631. \u0644\u0627\u0632\u0645 \u0628\u0647 \u0630\u06a9\u0631 \u0627\u0633\u062a \u0628\u0647 \u062f\u0644\u06cc\u0644 \u0627\u06cc\u0646\u06a9\u0647 \u062f\u0631 \u062f\u0627\u062f\u0647\u200c\u0647\u0627\u06cc \u062b\u0628\u062a \u0634\u062f\u0647 \u0627\u0634\u062a\u0628\u0627\u0647\u0627\u062a\u06cc \u0648\u062c\u0648\u062f \u062f\u0627\u0634\u062a\u0647 \u0648 \u0645\u0648\u0627\u0631\u062f\u06cc \u0646\u06cc\u0632 \u0646\u0627\u0647\u0645\u06af\u0648\u0646\u06cc \u062f\u06cc\u062f\u0647 \u0645\u06cc\u0634\u062f \u0645\u062c\u0628\u0648\u0631 \u0628\u0647 \u062a\u063a\u06cc\u06cc\u0631 \u0645\u0648\u0627\u0631\u062f \u062f\u06cc\u06af\u0631\u06cc \u0646\u06cc\u0632 \u0634\u062f\u06cc\u0645.\n    * \u0627\u0636\u0627\u0641\u0647 \u06a9\u0631\u062f\u0646 \u0645\u062c\u0645\u0648\u0639\u0647 \u062f\u0627\u062f\u0647\u200c\u0647\u0627\u06cc \u062f\u06cc\u06af\u0631 \u0628\u0647 \u0635\u0648\u0631\u062a\u06cc \u06a9\u0647 \u0634\u0631\u0627\u06cc\u0637 \u0639\u06a9\u0633 \u062f\u0627\u062f\u0647\u200c\u0647\u0627\u06cc \u0641\u0639\u0644\u06cc \u0631\u0627 \u062f\u0627\u0634\u062a\u0647 \u0648 \u0628\u0631\u0686\u0633\u0628 \u0646\u062a\u06cc\u062c\u0647 \u062a\u0633\u062a \u0622\u0646 \u0646\u06cc\u0632 \u0645\u0646\u0641\u06cc \u0628\u0627\u0634\u062f.\n    * \u0627\u062f\u063a\u0627\u0645 \u062f\u0648 \u0645\u062c\u0645\u0648\u0639\u0647 \u0648 \u0627\u06cc\u062c\u0627\u062f \u062f\u06cc\u062a\u0627\u0633\u062a \u0646\u0647\u0627\u06cc\u06cc\n    * \u0646\u0631\u0645\u0627\u0644 \u06a9\u0631\u062f\u0646 \u062f\u0627\u062f\u0647 \u0628\u0627 \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u0627\u0632 \u06a9\u062a\u0627\u0628\u062e\u0627\u0646\u0647 sklearn\n<\/div>","61bbb3f2":"**FIND-S Algorithm**","b0a2e8bf":"**Decision Tree**"}}