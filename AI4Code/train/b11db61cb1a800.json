{"cell_type":{"d37763c9":"code","81dafef9":"code","adc6a12f":"code","fdaf6972":"code","8ab3a09e":"code","3d2f3a8c":"code","c7c714b4":"code","b88f87d6":"code","91590ef4":"code","68962d28":"code","a087170d":"code","dec6a43c":"code","f9853f18":"code","c80349a8":"code","31f6bb2e":"code","a73f728c":"code","4dc470f5":"code","1b29939f":"code","c94cd2b8":"code","4c92d167":"code","babcb77e":"code","88e1b468":"code","10e0f211":"code","4653ebd5":"code","e40adf47":"code","43bf7d7c":"code","14758204":"code","7370fa5a":"code","711db292":"code","e3e4e635":"code","32c9e064":"code","a457d87c":"code","e5cd1920":"code","61df656c":"code","050b1889":"code","3bfe1312":"code","49231e61":"code","26b1ac47":"code","21ec4b01":"code","6675d60e":"code","b3ee4821":"code","57863acf":"code","73e96872":"code","d63ca6ec":"code","5de9b9e7":"code","7a188624":"code","3182e7d5":"code","6dfcaab0":"code","6085770c":"code","a4f9ca74":"code","16e1f9c2":"code","42ae637d":"code","7ff6df88":"code","f9a7b09c":"code","6196b3ab":"code","b5afaeb5":"code","9dacb516":"code","532fda6a":"code","1e8d7047":"code","fbc05b4e":"code","fc4699d2":"code","0929f601":"code","ab9b86f0":"code","6e41ff3f":"code","ae22b8c5":"code","d6b5f01e":"code","024baca5":"code","6ac148bf":"code","ffef7789":"code","9ccbcdc3":"code","66260c0b":"code","6ac97dce":"code","852fe6b7":"code","ca11e336":"code","0c8ce78a":"code","eacf50ad":"code","1ea28f13":"code","2873f32f":"code","bfddbc74":"code","e06c0e07":"code","c02c2ddb":"code","156c0069":"code","d800bb41":"code","fbf6e786":"code","9756e990":"code","409eedb4":"code","300aae10":"code","e8094833":"code","5efeeef9":"code","7375bdbb":"code","740ac088":"code","e5380d7d":"code","c6bfb2e9":"code","20782a31":"markdown","41f4f0c9":"markdown","b01e72fd":"markdown","a2e98996":"markdown","150a0c0c":"markdown","99f1b05d":"markdown","1dc0a025":"markdown","6c0a6592":"markdown","a56074a3":"markdown","054f7269":"markdown","bcc6fb71":"markdown","5436f2ad":"markdown","8401c0e5":"markdown","a63243ed":"markdown","f7dd7144":"markdown","f0d7ff9b":"markdown","bcc71b9f":"markdown","11204ac2":"markdown","fc465516":"markdown","e729ca16":"markdown","ecc5c03d":"markdown","79920814":"markdown","7854495d":"markdown","c8bb0c77":"markdown","6f3eb1a5":"markdown","f04f3769":"markdown","474d511e":"markdown","05f81869":"markdown","613af633":"markdown","e8425945":"markdown","7fe9678d":"markdown","92b95de8":"markdown","f6106a4a":"markdown","baad97cc":"markdown","39684a4a":"markdown","9555c466":"markdown","21161948":"markdown","d0b66155":"markdown","73cc782e":"markdown","ad369654":"markdown","ea7e3ff4":"markdown","590051b4":"markdown","65b93d3d":"markdown","7ca906f4":"markdown","392a112d":"markdown","364f1444":"markdown","74e58cc4":"markdown","b0d224a2":"markdown","e5ba43b9":"markdown","dbb8d28f":"markdown","69f463b1":"markdown","0398ca91":"markdown","b8282140":"markdown","a2a48c4b":"markdown","1c049272":"markdown","7ecb9cce":"markdown","5e7767bd":"markdown","64596ee1":"markdown"},"source":{"d37763c9":"#Standard Python libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#Natural language processing libraries\nimport nltk\nimport re\n\nimport time\n\n#Interactive computing\nfrom IPython.core.magics.execution import _format_time\nfrom IPython.display import display as d\nfrom IPython.display import Audio\nfrom IPython.core.display import HTML\n\n#Accountability\nimport logging as log\n\nimport os\nprint(os.listdir(\"..\/input\"))","81dafef9":"train_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.head()","adc6a12f":"test = pd.read_csv('..\/input\/test.csv')\ntest.head()","fdaf6972":"#Checking if we have all sixteen personality types represented\nPersonalities = train_df.groupby(\"type\").count()\nPersonalities.sort_values(\"posts\", ascending=False, inplace=True)\nPersonalities.index.values","8ab3a09e":"#Visualizing the distribution of the personality types\ncount_types = train_df['type'].value_counts()\nplt.figure(figsize=(10,5))\nsns.barplot(count_types.index, count_types.values, alpha=1, palette=\"winter\")\nplt.ylabel('No of persons', fontsize=12)\nplt.xlabel('personality Types', fontsize=12)\nplt.title('Distribution of personality types')\nplt.show()","3d2f3a8c":"def alert():\n    \"\"\" makes sound on client using javascript\"\"\"  \n    \n    framerate = 44100\n    duration=0.5\n    freq=340\n    t = np.linspace(0,duration,framerate*duration)\n    data = np.sin(2*np.pi*freq*t)\n    d(Audio(data,rate=framerate, autoplay=True))","c7c714b4":"def link_transformer(df, column, reports=True):\n    \"\"\"Search over a column in a pandas dataframe for urls.\n    \n    extract the title related to the url then replace the url with the title.\n    \n    \n    df : pandas Dataframe object\n    \n    column: string type object equal to the exact name of the colum you want to replace the urls\n    \n    reports: Boolean Value (default=True)\n        If true give active report updates on the last index completed and the ammount of reported fail title extractions\n   \n    \"\"\"\n    \n    total_errors = 0\n    count = 0\n    from mechanize import Browser\n    br = Browser()\n    \n    while count != len(df):\n        errors = 0\n        \n        url = re.findall(r'http[s]?:\/\/(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+', df.loc[count, column])\n        \n        for link in url:\n            try: \n                br.open(link)\n                df.loc[count, column] = df.loc[count, column].replace(link, br.title())\n            except:\n                \n                if reports == True:\n                    print(f'failed--- {link}')\n                elif reports == False:\n                    pass\n                else:\n                    raise ValueError('reports expected a boolean value')\n                \n                total_errors += 1\n                errors += 1\n                \n                continue\n                \n        if reports == True:\n            if errors == 0:\n                report = 'no errors'\n                errors = ''\n            elif errors == 1:\n                report = 'error'\n            else:\n                report = 'errors'\n            print(f'\\nIndex {count + 1} completed. {errors} {report} reported\\n______________________\\n\\n')\n    \n        elif reports == False:\n            pass\n        \n        else:\n            raise ValueError('reports expected a boolean value')\n                \n        \n        count += 1\n    print(f'{total_errors} total errors throughout full runtime')\n    \n#example\n\n#sample = pd.read_csv('train.csv').sample(3, random_state=20).reset_index(drop=True)\n#sample\n\n#link_transformer(sample, 'posts')\n\n#sample","b88f87d6":"def remove_links(df, column):\n    \"\"\"Replace urls by searching for the characters normally found in urls \n    and replace the string found with the string web-link\n    \n    df : pandas Dataframe object\n    \n    column: string type object equal to the exact name of the colum you want to replace the urls\n    \"\"\"\n    \n    return df[column].replace(r'http[s]?:\/\/(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+', \n                           r'web-link', regex=True, inplace=True)","91590ef4":"def no_punc_num(post):\n    \"\"\"The function imports punctuation and define numbers then removes them from a column in a dataframe \n    using a for loop.\n    \n    to use, use pd.DataFrame.apply() on a Dataframe object\"\"\"\n    \n    from string import punctuation\n    pun_num = punctuation + '1234567890'\n    return ''.join([letter for letter in post if letter not in pun_num])","68962d28":"tokens = nltk.tokenize.TreebankWordTokenizer()","a087170d":"lem = nltk.stem.WordNetLemmatizer()\ndef lemmatized(words, lemmatizer):\n    \"\"\"Transform a list of words into base forms \n    \n    example: hippopotami = hippopotamus\n\n\n    Required imports  \n   ------------------\n    nltk.stem.WordNetLemmatizer()\n    \n    \n    Parameters \n   ------------\n   lemmatizer: nltk.stem.WordNetLemmatizer() object\n   \n   \n   to use, use pd.DataFrame.apply() on a Dataframe object \n    \"\"\"\n    return [lemmatizer.lemmatize(word) for word in words]","dec6a43c":"def remove_stop_words(tokens):\n    \"\"\"Removes a list of words from a Dataframe\n    \n    Required imports  \n   ------------------\n    A list of stopwords to remove\n    \n    \n    to use, use pd.DataFrame.apply() on a Dataframe object  \n    \"\"\"\n    return [t for t in tokens if t not in stopwords]","f9853f18":"#Splitting sentences\ntrain = []\nfor types, posts in train_df.iterrows():\n    for split_at in posts['posts'].split('|||'):\n        train.append([posts['type'], split_at])\ntrain = pd.DataFrame(train, columns=['type', 'post'])","c80349a8":"train.head()","31f6bb2e":"#making all the words lowwer case\ntrain.post = train.post.str.lower()","a73f728c":"#removing punctuation and numbers\ntrain.post = train.post.apply(no_punc_num)","4dc470f5":"#tokenizing words\ntrain['tokenized'] = train.post.apply(tokens.tokenize)","1b29939f":"#lemmatizing words\ntrain['lemmatized'] = train.tokenized.apply(lemmatized, args=(lem,))","c94cd2b8":"def bag_count(word, bag={}):\n    '''text vectorize by representing every word as a integer and counting the frequency of appearence'''\n    for w in word:\n        if w not in bag.keys():\n            bag[w] = 1\n        else:\n            bag[w] += 1\n    return bag\n\nper_type = {}\nfor pt in list(train.type.unique()):\n    df = train.groupby('type')\n    per_type[pt] = {}\n    for row in df.get_group(pt)['lemmatized']:\n        per_type[pt] = bag_count(row, per_type[pt])\n\nlen(per_type.keys())","4c92d167":"#creating a list of unique words\nunique_words = set()\nfor pt in list(train.type.unique()):\n    for word in per_type[pt]:\n        unique_words.add(word)","babcb77e":"unique_words","88e1b468":"personality_stop_words = list(per_type.keys())","10e0f211":"#finding the frequency of words\nper_type['all'] = {}\nfor tp in list(train.type.unique()):\n    for word in unique_words:\n        if word in per_type[tp].keys():\n            if word not in per_type['all']:\n                per_type['all'][word] = per_type[tp][word]\n            else:\n                per_type['all'][word] += per_type[tp][word] ","4653ebd5":"per_type['all']","e40adf47":"print(len(per_type['all']))","43bf7d7c":"#Appearence of a word longer that 2 standard deviations in percentage\n(sum([v for v in per_type['all'].values() if v >= 43]))\/sum([v for v in per_type['all'].values()])","14758204":"#Checking the words\nword_index = [k for k, v in per_type['all'].items() if v > 43]","7370fa5a":"#using for loop to find word usage per type\nper_type_words = []\nfor pt, p_word in per_type.items():\n    word_useage = pd.DataFrame([(k, v) for k, v in p_word.items() if k in word_index], columns=['Word', pt])\n    word_useage.set_index('Word', inplace=True)\n    per_type_words.append(word_useage)","711db292":"word_useage = pd.concat(per_type_words, axis=1)\nword_useage.fillna(0, inplace=True)","e3e4e635":"word_useage.sample(10)","32c9e064":"personality_stop_words","a457d87c":"#Finding sum of the word usage and identifying them to each variable\n\nI = [x for x in personality_stop_words if x[0] == 'I']\nE = [x for x in personality_stop_words if x[0] == 'E']\nword_useage['I'] = word_useage[I].sum(axis=1)\nword_useage['E'] = word_useage[E].sum(axis=1)\n\nS = [x for x in personality_stop_words if x[1] == 'S']\nN = [x for x in personality_stop_words if x[1] == 'N']\nword_useage['S'] = word_useage[S].sum(axis=1)\nword_useage['N'] = word_useage[N].sum(axis=1)\n\nF = [x for x in personality_stop_words if x[2] == 'F']\nT = [x for x in personality_stop_words if x[2] == 'T']\nword_useage['F'] = word_useage[F].sum(axis=1)\nword_useage['T'] = word_useage[T].sum(axis=1)\n\nP = [x for x in personality_stop_words if x[3] == 'P']\nJ = [x for x in personality_stop_words if x[3] == 'J']\nword_useage['P'] = word_useage[P].sum(axis=1)\nword_useage['J'] = word_useage[J].sum(axis=1)","e5cd1920":"word_useage.sample(10)","61df656c":"#Word usage in percentage form\nfor col in ['I', 'all']:\n    word_useage[col+'_perc'] = word_useage[col] \/ word_useage[col].sum()\nfor col in ['E', 'all']:\n    word_useage[col+'_perc'] = word_useage[col] \/ word_useage[col].sum()\n\nfor col in ['S', 'all']:\n    word_useage[col+'_perc'] = word_useage[col] \/ word_useage[col].sum()\nfor col in ['N', 'all']:\n    word_useage[col+'_perc'] = word_useage[col] \/ word_useage[col].sum()\n\nfor col in ['F', 'all']:\n    word_useage[col+'_perc'] = word_useage[col] \/ word_useage[col].sum()\nfor col in ['T', 'all']:\n    word_useage[col+'_perc'] = word_useage[col] \/ word_useage[col].sum()\n\nfor col in ['P', 'all']:\n    word_useage[col+'_perc'] = word_useage[col] \/ word_useage[col].sum()\nfor col in ['J', 'all']:\n    word_useage[col+'_perc'] = word_useage[col] \/ word_useage[col].sum()","050b1889":"word_useage.sample(1)","3bfe1312":"stopwords = nltk.corpus.stopwords.words('english')","49231e61":"#Word usage in percentage form for each variable \nword_useage['I chi2'] = np.power((word_useage['I_perc'] - word_useage['all_perc']), 2) \/ word_useage['all_perc'].astype(np.float64)\nword_useage['E chi2'] = np.power((word_useage['E_perc'] - word_useage['all_perc']), 2) \/ word_useage['all_perc'].astype(np.float64)\n\nword_useage['S chi2'] = np.power((word_useage['S_perc'] - word_useage['all_perc']), 2) \/ word_useage['all_perc'].astype(np.float64)\nword_useage['N chi2'] = np.power((word_useage['N_perc'] - word_useage['all_perc']), 2) \/ word_useage['all_perc'].astype(np.float64)\n\nword_useage['F chi2'] = np.power((word_useage['F_perc'] - word_useage['all_perc']), 2) \/ word_useage['all_perc'].astype(np.float64)\nword_useage['T chi2'] = np.power((word_useage['T_perc'] - word_useage['all_perc']), 2) \/ word_useage['all_perc'].astype(np.float64)\n\nword_useage['P chi2'] = np.power((word_useage['P_perc'] - word_useage['all_perc']), 2) \/ word_useage['all_perc'].astype(np.float64)\nword_useage['J chi2'] = np.power((word_useage['J_perc'] - word_useage['all_perc']), 2) \/ word_useage['all_perc'].astype(np.float64)","26b1ac47":"I_words = word_useage[['I_perc', 'all_perc', 'I chi2']][(word_useage['I_perc'] > word_useage['all_perc'])].sort_values(by='I chi2', ascending=False)\nE_words = word_useage[['E_perc', 'all_perc', 'E chi2']][word_useage['E_perc'] > word_useage['all_perc']].sort_values(by='E chi2', ascending=False)\n\nS_words = word_useage[['S_perc', 'all_perc', 'S chi2']][(word_useage['S_perc'] > word_useage['all_perc'])].sort_values(by='S chi2', ascending=False)\nN_words = word_useage[['N_perc', 'all_perc', 'N chi2']][word_useage['N_perc'] > word_useage['all_perc']].sort_values(by='N chi2', ascending=False)\n\nF_words = word_useage[['F_perc', 'all_perc', 'F chi2']][(word_useage['F_perc'] > word_useage['all_perc'])].sort_values(by='F chi2', ascending=False)\nT_words = word_useage[['T_perc', 'all_perc', 'T chi2']][word_useage['T_perc'] > word_useage['all_perc']].sort_values(by='T chi2', ascending=False)\n\nP_words = word_useage[['P_perc', 'all_perc', 'P chi2']][(word_useage['P_perc'] > word_useage['all_perc'])].sort_values(by='P chi2', ascending=False)\nJ_words = word_useage[['J_perc', 'all_perc', 'J chi2']][word_useage['J_perc'] > word_useage['all_perc']].sort_values(by='J chi2', ascending=False)","21ec4b01":"I_keep = I_words[I_words.index.isin(list(stopwords))].head(5)\nE_keep = E_words[E_words.index.isin(list(stopwords))].head(5)\n\nS_keep = S_words[S_words.index.isin(list(stopwords))].head(5)\nN_keep = N_words[N_words.index.isin(list(stopwords))].head(5)\n\nF_keep = F_words[F_words.index.isin(list(stopwords))].head(5)\nT_keep = T_words[T_words.index.isin(list(stopwords))].head(5)\n\nP_keep = P_words[P_words.index.isin(list(stopwords))].head(5)\nJ_keep = J_words[J_words.index.isin(list(stopwords))].head(5)","6675d60e":"I_keep = list(I_keep.index)\nE_keep = list(E_keep.index)\n\nS_keep = list(S_keep.index)\nN_keep = list(N_keep.index)\n\nF_keep = list(F_keep.index)\nT_keep = list(T_keep.index)\n\nP_keep = list(P_keep.index)\nJ_keep = list(J_keep.index)","b3ee4821":"keep = I_keep+E_keep+S_keep+N_keep+F_keep+T_keep+P_keep+J_keep\n\nkeep = set(keep)","57863acf":"len(keep)","73e96872":"stop = nltk.corpus.stopwords.words('english')","d63ca6ec":"len(stop)","5de9b9e7":"stopwords = []\nfor i in stop:\n    if i in keep:\n        pass\n    else:\n        stopwords.append(i)","7a188624":"len(stopwords)","3182e7d5":"train = train_df\nsample = train.sample(3).reset_index(drop=True)\ntrain.shape","6dfcaab0":"#Removes links\nremove_links(train, 'posts')\ntrain.head(1)","6085770c":"train['posts'].replace(r'\\|\\|\\|', r' ', regex=True, inplace=True)\ntrain['posts'].head(1)","a4f9ca74":"#Removes punchuations and set text to lowercase\ntrain['posts'] = train['posts'].str.lower()\ntrain['posts'] = train['posts'].apply(no_punc_num)\ntrain['posts'].head(1)","16e1f9c2":"#Tokenize the posts\ntrain['posts'] = train['posts'].apply(tokens.tokenize)\ntrain['posts'].head(1)","42ae637d":"#Lemmatize the posts\ntrain['posts'] = train['posts'].apply(lemmatized, args=(lem,))\ntrain.head(1)","7ff6df88":"#Removes stopwords from the posts\ntrain['posts'] = train['posts'].apply(remove_stop_words)\ntrain.head(1)","f9a7b09c":"train['posts'] = [' '.join(map(str, l)) for l in train['posts']]\ntrain.head(1)","6196b3ab":"train['Mind']   = train['type'].apply(lambda x: x[0] == 'E').astype('int')\ntrain['Energy'] = train['type'].apply(lambda x: x[1] == 'N').astype('int')\ntrain['Nature'] = train['type'].apply(lambda x: x[2] == 'T').astype('int')\ntrain['Tactics']= train['type'].apply(lambda x: x[3] == 'J').astype('int')\ntrain = train[['Mind','Energy','Nature','Tactics','posts', 'type']]","b5afaeb5":"train.head(1)","9dacb516":"#Removes links\nremove_links(test, 'posts')\ntest.head(1)","532fda6a":"test['posts'].replace(r'\\|\\|\\|', r' ', regex=True, inplace=True)\ntest['posts'].head(1)","1e8d7047":"#Removes punchuations and set text to lowercase\ntest['posts'] = test['posts'].str.lower()\ntest['posts'] = test['posts'].apply(no_punc_num)\ntest['posts'].head(1)","fbc05b4e":"#Tokenize the posts\ntest['posts'] = test['posts'].apply(tokens.tokenize)\ntest['posts'].head(1)","fc4699d2":"#Lemmatize the posts\ntest['posts'] = test['posts'].apply(lemmatized, args=(lem,))\ntest.head(1)","0929f601":"#Removes the stopwords from the posts\ntest['posts'] = test['posts'].apply(remove_stop_words)\ntest.head(1)","ab9b86f0":"test['posts'] = [' '.join(map(str, l)) for l in test['posts']]\ntest.head(1)","6e41ff3f":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","ae22b8c5":"#Vectorising using CountVectorizer\nCount_vect = CountVectorizer(max_df=0.8, min_df=43,  lowercase=False)\nCount_train = Count_vect.fit_transform(train['posts'])\nCount_test = Count_vect.transform(test['posts'])","d6b5f01e":"#Vectorising using TfidfVectorizer\nTfidf_vect =TfidfVectorizer(max_df=0.8, min_df=43, lowercase=False)\nTfidf_train = Tfidf_vect.fit_transform(train['posts'])\nTfidf_test = Tfidf_vect.transform(test['posts'])","024baca5":"#It seems they have exactly the same result in this case according to the results printed out.\nprint(f'count: {Count_train.shape}\\nCount_test: {Count_test.shape}\\n\\nTfidf: {Tfidf_train.shape}\\nTfidf_test: {Tfidf_test.shape}')","6ac148bf":"#Import libraries\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.metrics import log_loss","ffef7789":"#We saved our id for submission purposes\nsubm = {}\nsubm['id'] = test['id'].values","9ccbcdc3":"np.mean(train['Mind'] == 1)","66260c0b":"mind = LogisticRegression(C=1, solver='lbfgs')\nmind.fit(Tfidf_train, train['Mind'].values)\nalert()","6ac97dce":"y_probs = mind.predict_proba(Tfidf_train)\nmind_pred = mind.predict(Tfidf_train)\n\nfor thresh in np.arange(0.1, 1, 0.1).round(1):\n    fiddled_y = np.where(y_probs[:,1] > thresh, 1, 0)\n    print(f'the loss for {thresh} is:  {log_loss(fiddled_y, train[\"Mind\"])}')","852fe6b7":"true_mind = np.where(mind.predict_proba(Tfidf_test)[:,1] > 0.3, 1, 0)","ca11e336":"true_mind","0c8ce78a":"subm['mind'] = true_mind","eacf50ad":"np.mean(train['Energy'] == 1)\nEnergy  = LogisticRegression(C=1, solver='lbfgs')\nEnergy.fit(Tfidf_train,  train['Energy'])\nalert()","1ea28f13":"y_probs = Energy.predict_proba(Tfidf_train)\nEnergy_pred = Energy.predict(Tfidf_train)\n\nfor thresh in np.arange(0.1, 1, 0.1).round(1):\n    fiddled_y = np.where(y_probs[:,1] > thresh, 1, 0)\n    print(f'the loss for {thresh} is:  {log_loss(fiddled_y, train[\"Energy\"])}')","2873f32f":"true_energy = np.where(Energy.predict_proba(Tfidf_test)[:,1] > 0.7, 1, 0)","bfddbc74":"true_energy","e06c0e07":"subm['energy'] = true_energy","c02c2ddb":"np.mean(train['Nature'] == 1)\nNature  = LogisticRegression(C=1, solver='lbfgs')\nNature.fit(Tfidf_train,  train['Nature'])\nalert()","156c0069":"y_probs = Nature.predict_proba(Tfidf_train)\nNature_pred = Nature.predict(Tfidf_train)\n\nfor thresh in np.arange(0.1, 1, 0.1).round(1):\n    fiddled_y = np.where(y_probs[:,1] > thresh, 1, 0)\n    print(f'the loss for {thresh} is:  {log_loss(fiddled_y, train[\"Nature\"])}')","d800bb41":"true_nature = np.where(Nature.predict_proba(Tfidf_test)[:,1] > 0.5, 1, 0)","fbf6e786":"true_nature","9756e990":"subm['nature'] = true_nature","409eedb4":"np.mean(train['Tactics'] == 1)\nTactics = LogisticRegression(C=1, solver='lbfgs')\nTactics.fit(Tfidf_train, train['Tactics'])\nalert()","300aae10":"y_probs = Tactics.predict_proba(Tfidf_train)\nTactics_pred = Tactics.predict(Tfidf_train)\n\nfor thresh in np.arange(0.1, 1, 0.1).round(1):\n    fiddled_y = np.where(y_probs[:,1] > thresh, 1, 0)\n    print(f'the loss for {thresh} is:  {log_loss(fiddled_y, train[\"Tactics\"])}')","e8094833":"true_tactics = np.where(Tactics.predict_proba(Tfidf_test)[:,1] > 0.4, 1, 0)","5efeeef9":"true_tactics","7375bdbb":"subm['tactics'] = true_tactics","740ac088":"submit = pd.DataFrame(subm)","e5380d7d":"submit.sample(10)","c6bfb2e9":"submit.to_csv('kaggle submit.csv', index=False)","20782a31":"[Return to index](#index)\n___________________________________________________________________________________________________________________________________________________\n<a id='6'><\/a>","41f4f0c9":"* Attempt upscaling to help with the skewness of the data\n* Weighting the response of personality traits based on other traits\n* Apply GridsearchCV to my models","b01e72fd":"In cleaning and exploring our data we built some functions that will help remove and transform our features that make machine learning algorithms work. We created the 'alert' function for the long running time cells, this will alert us when they are done running. The 'link transformer' function opens the link address in the data frame to find title names of the url. It then replaces the url with the title. The 'remove urls' function removes urls and replace it with web-link. The function 'no_punc_num' removes numbers and puntuation. The 'lemmatized' function lemmatizes our words using WordNetLemmatizer. Lastly, we created 'remove_stop_words' function which removed stop words that we will decide not to use.","a2e98996":"Above output shows the 'type' column contains 16 unique codes, representing the 16 different personality types.","150a0c0c":"# test","99f1b05d":"[Return to index](#index)\n___________________________________________________________________________________________________________________________________________________\n<a id='12'><\/a>","1dc0a025":"The bar chart above shows that INFP (Introversion - Intuition - Feeling - Perceiving) is the most frequently appearing type in the dataset, followed by INFJ (Introversion - Intuition - Feeling - Judging). Overall, the dataset contains many more Intuitive-Intuition (IN-) groupings than any other type. Conversely, the dataset contains very few Extroversion-Sensing (ES-) types.","6c0a6592":"We created a bag of words above but we will also be using CountVectorizer and TfidfVectorizer below. These methods work differently to do the same work. Although CountVectorizer is traditionally the main vectorizer these methods are found on the same module and not superior to each other. But since they work differently and have different parameters, we will test them both.","a56074a3":"We did the same for our test data. We removed links, punctuation, numbers and stop words. We lammatized our words. We did all this by using the functions we built. We will call our functions and check everytime to see if the function is working by using .head()","054f7269":"# Prepared submission","bcc6fb71":"TfidfVectorizer convert a collection of raw documents to a matrix of TF-IDF features.","5436f2ad":"# Tactics","8401c0e5":"We loaded our data (train.csv and test.csv) and inspected. This helped to see where we can start with feature engineering. ","a63243ed":"[Return to index](#index)\n___________________________________________________________________________________________________________________________________________________\n<a id='5'><\/a>","f7dd7144":"[Return to index](#index)\n___________________________________________________________________________________________________________________________________________________\n<a id='7'><\/a>","f0d7ff9b":"# Importing data","bcc71b9f":"Column names for the new columns and their binary codes(1s and 0s):\n\n- Mind: Introversion(I = 0) - Extroversion(E = 1)<br\/>\n- Energy: Sensing(S = 0) - Intuition(N = 1)<br\/>\n- Nature: Feeling(F = 0) - Thinking(T = 1)<br\/>\n- Tactics: Perceiving(P = 0) - Judging(J = 1)","11204ac2":"# Energy","fc465516":"___________________________________________________________________________________________________________________________________________________","e729ca16":"Last one is Tactics which compares Perceiving (P) and Judging (J). We fitted the model with out 'y' as train['Tactics'] and our 'x' as Tfidf_train. We then we predicted our X_test that we also vectorised using TfidfVectorizer.","ecc5c03d":"# Introduction","79920814":"# Vectorization","7854495d":"# Table of contents","c8bb0c77":"# Mind","6f3eb1a5":"[Return to index](#index)\n___________________________________________________________________________________________________________________________________________________\n<a id='14'><\/a>","f04f3769":"1. A large part of the code and idea to fiddle with the logistic regression thresholds was inspired by the **advanced logistic regression** train and **Nicholas Meyers** \n2. EDA\/Feature engineering (to keep certain stopwords) was largely inspired by the **How do machines understand language** train\n3. Most preprocessing functions were largely inspired by the **How do machines understand language** train\n4. EDSA supervisors **Bryan Davies, Tristan Naidoo**\n5. Kaggle dataset from [Personality Cafe website forums](https:\/\/www.personalitycafe.com\/forum\/)","474d511e":"[Return to index](#index)\n___________________________________________________________________________________________________________________________________________________","05f81869":"# Acknowledgements","613af633":"[Return to index](#index)\n___________________________________________________________________________________________________________________________________________________\n<a id='15'><\/a>","e8425945":"# Importing libraries","7fe9678d":"Second is Energy which compares  Sensing (S) and Intuitive (N). We fitted the model with out 'y' as train['Energy'] and our 'x' as Tfidf_train. We then we predicted our X_test that we also vectorised using TfidfVectorizer.","92b95de8":"\n<a id='1'><\/a>\n___________________________________________________________________________________________________________________________________________________","f6106a4a":"# Function creation","baad97cc":"# Nature","39684a4a":"[Return to index](#index)\n___________________________________________________________________________________________________________________________________________________\n<a id='10'><\/a>","9555c466":"[Return to index](#index)\n___________________________________________________________________________________________________________________________________________________\n<a id='8'><\/a>","21161948":"[Return to index](#index)\n___________________________________________________________________________________________________________________________________________________\n<a id='13'><\/a>","d0b66155":"[Return to index](#index)\n___________________________________________________________________________________________________________________________________________________\n<a id='3'><\/a>","73cc782e":"### Distribution of Myers-Briggs Personality Types in the Dataset","ad369654":"# Feature engineering","ea7e3ff4":"<div style=\"text-align: justify\">The Myers-Briggs Type Indicator (mbti) categories individuals into 16 different personality types using four opposite pairs of variables represented by a letter or word. These letters each represent a characteristic that groups interests, needs and values together. The MBTI personality type binary variables are: Mind: Introverted (I) or Extraverted (E) Energy: Sensing (S) or Intuitive (N) Nature: Feeling (F) or Thinking (T) Tactics: Perceiving (P) or Judging (J) An individual's final type is made up of one of the variables combined. For example an individual with INFP type would have a combination of Introverted(I), Intuitive (N), Feeling (F) and Perceiving (P). <\/div>\n\n<div style=\"text-align: justify\"> The common way of finding out your persoanlity type is to take a personality type test on a websites, where they would determine your personality type from the different questions you have to answer about yourself.\nIn this notebook, I will build a model that will predict the personality of a person from their twitter post. We will predict four separate labels for each person which, when combined, results in that person's personality type just like the example. The data is available on kaggle competition.<\/div>\n\nFor more info about the MBTI personality types click [here](https:\/\/www.16personalities.com\/personality-types) OR the test click [here to take the test](https:\/\/www.16personalities.com\/free-personality-test)","590051b4":"# <center> Personality Profile Predictions <\/center>","65b93d3d":"[Return to index](#index)\n___________________________________________________________________________________________________________________________________________________\n<a id='9'><\/a>","7ca906f4":"To check if we still have any unwatned characters, we look at our training data again and remove any unwanted characters. We also created our four columns for each variable using the type column. ","392a112d":"We are ready to fit our model. First we import our models. There are different machine learning models which we first tried (Logistic Regression, Naive Bayes(the three known), Extra-trees Classifier and Random Forest) among which the code for logistic regression is shown below. We decided to use logistic regression, because it seemed to fit our data and predict better from our kaggle submissions.","364f1444":"### Model fitting and predicting for each class (Mind, Energy, Nature, and Tactics) using Logistic Regression","74e58cc4":"[Return to index](#index)\n___________________________________________________________________________________________________________________________________________________\n<a id='11'><\/a>","b0d224a2":"1. [Introduction](#1)\n2. [Importing libraries](#2)\n3. [Importing data](#3)\n3. [Creating required functions](#4)\n4. [EDA and feature engineering](#5)\n5. [Train preprocessing](#6)\n6. [Test preprocessing](#7)\n7. [Vectorization](#8)\n8. [Model fitting and predicting](#9)\n    * [Mind](#10)\n    * [Energy](#11)\n    * [Nature](#12)\n    * [Tactics](#13)\n    \n9. [Prepared submission](#14)\n10. [Still to do](#15) \n11. [Acknowledgements](#16)\n","e5ba43b9":"# Conclusion\nText data comes in many different forms depending on the source. The data we used are twitter posts which do not come with translation of any videos or images shared. We had to clean our data in a way that saves some of those messages we can't see just from the text. We saw that in our model improved a lot after we have replaced the urls with title of their videos from youtube.\n\nThis model present an alternative way of getting mbti personality type. Also with the method described in this notebook, one does not need to wait for somebody to take the test but need their social media posts or words they normally say to people. It makes it easier to get someone else's personality type. This can be very useful for companies that would like to pick people according to their personality type, people looking to interact with certain personality type and more.\n\nThere are different machine learning models which we first tried (Logistic Regression, Naive Bayes, Extra-trees Classifier and Random Forest) among which the code for logistic regression is shown above. We decided to use logistic regression, because it seemed to fit our data and predict better score from our kaggle submissions.The binary classification exercise to predict each of the classes in the four axes (mind, energy, nature, and tactics) was somewhat more successful. A Logistic Regression was used in each of the classes.\n\nWe still have challenges of reading people's tone and level of english and their writting skills. This factors can play a big role in our model predicting the exact personality type. The fact that another forum or platform can have a different way of writting can also mean that the model will have to be trained and fitted again for that specific platform. ","dbb8d28f":"[Return to index](#index)\n<a id='2'><\/a>\n___________________________________________________________________________________________________________________________________________________","69f463b1":"# train","0398ca91":"Third is Nature which compares Feeling (F) and Thinking (T). We fitted the model with out 'y' as train['Nature'] and our 'x' as Tfidf_train. We then we predicted our X_test that we also vectorised using TfidfVectorizer.","b8282140":"CountVectorizer encodes text by splitting a set of words into one column per word, with (by default) the count of the word for that row in that column.","a2a48c4b":"[Return to index](#index)\n___________________________________________________________________________________________________________________________________________________\n<a id='4'><\/a>","1c049272":"# Still to do","7ecb9cce":"First is mind which compares Introverted (I) and Extraverted (E). We fitted the model with out 'y' as train['Mind'] and our 'x' as Tfidf_train. We then we predicted our X_test that we also vectorised using TfidfVectorizer.","5e7767bd":"To start with our feature engineering we started by spitting the sentences. (|||) indicated where a sentece started or ended so we split the sentence were we find (|||). We tokenized and lammatized our sentences. Then we created a bag of words. We grouped the lemmatized words that were used per type, this we can see which words were mostly used by certain personality types and create stopwords for each variable.   ","64596ee1":"# Model fitting and predicting"}}