{"cell_type":{"55c34d23":"code","8f678df6":"code","426e4f49":"code","b7fb2962":"code","913cc5b9":"code","803788bb":"code","8387f77a":"code","e76404f2":"code","83325409":"markdown","0610057f":"markdown","83e7ba43":"markdown","0ecd2ea5":"markdown","27445515":"markdown","1c9b87f1":"markdown","3d89bb0a":"markdown","f415c8cc":"markdown"},"source":{"55c34d23":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport optuna\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.preprocessing import LabelEncoder","8f678df6":"train=pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\ntest=pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')","426e4f49":"cat_var=[f'cat{i}' for i in range(10)]\ncont_var=[f'cont{i}' for i in range(14)]\ncolumns=[ col for col in train.columns.tolist() if col not in ['id','target']]\n\nfor cat in cat_var:\n    le = LabelEncoder()\n    train[cat]=le.fit_transform(train[cat])\n    test[cat]=le.transform(test[cat])","b7fb2962":"X=train[columns]\ny=train.target","913cc5b9":"# base lgbm models\n\n\nlgb_params={'random_state': 2021,\n          'metric': 'rmse',\n          'n_estimators': 30000,\n          'n_jobs': -1,\n          'cat_feature': [x for x in range(len(cat_var))],\n          'bagging_seed': 2021,\n          'feature_fraction_seed': 2021,\n          'learning_rate': 0.003899156646724397,\n          'max_depth': 99,\n          'num_leaves': 63,\n          'reg_alpha': 9.562925363678952,\n          'reg_lambda': 9.355810045480153,\n          'colsample_bytree': 0.2256038826485174,\n          'min_child_samples': 290,\n          'subsample_freq': 1,\n          'subsample': 0.8805303688019942,\n          'max_bin': 882,\n          'min_data_per_group': 127,\n          'cat_smooth': 96,\n          'cat_l2': 19\n          }","803788bb":"# factor by which we will reduce lambda\nf1= 0.6547870667136243\n# we will increase alpha by 2.6 each time we retrain the model\nf2= 2.6711351556035487\n# increase number of leaves by 20 each time we retrain the model\nf3= 20\n# decrease min child samples by 49 each time we retrain the model\nf4= 49\n\nf5= 2","8387f77a":"%%time\n\nkf=KFold(n_splits=5,random_state=48,shuffle=True)\n\n# we will store our final predictions in preds\npreds = np.zeros(test.shape[0])\n#store rmse of each iterations\nrmse=[]\ni=0\n\n# --------------------------------------------------------------------------------\n# Phase 1: create the pretrained model\nfor idx_train,idx_test in kf.split(X,y):\n    \n    X_train,X_test=X.iloc[idx_train],X.iloc[idx_test]\n    y_train,y_test=y.iloc[idx_train],y.iloc[idx_test]\n\n    \n    model=LGBMRegressor(**lgb_params)\n    \n    model.fit(X_train,y_train,eval_set=(X_test,y_test),early_stopping_rounds=300,verbose=False,eval_metric='rmse')\n    \n    predictions=model.predict(X_test,num_iteration=model.best_iteration_)\n    \n    rmse.append(mean_squared_error(y_test,predictions,squared=False))\n    \n    print('First Round:')\n    \n    print(f'RMSE {rmse[i]}')\n    \n    rmse_tuned=[]\n    params = lgb_params.copy()\n    \n    # -----------------------------------------------------------------------------\n    # Phase 2: iterations where we decrease the learning rate and regularization params    \n    for t in range(1,17):\n        \n        \n        if t >2:    \n                    \n            params['reg_lambda'] *=  f1\n            params['reg_alpha'] += f2\n            params['num_leaves'] += f3\n            params['min_child_samples'] -= f4\n            params['cat_smooth'] -= f5\n        \n            \n        params['learning_rate']=0.003\n        \n        # min_child_samples can not be lower than 0\n        if params['min_child_samples']<1:\n            params['min_child_samples']=1\n        \n        # we decrease the learning rate even more after 11 rounds of retraining\n        if t>11:\n            params['learning_rate']=0.001\n              \n        \n        model=LGBMRegressor(**params).fit(X_train,y_train,eval_set=(X_test,y_test),eval_metric='rmse',early_stopping_rounds=200,verbose=False,init_model=model)\n        \n        predictions=model.predict(X_test, num_iteration= model.best_iteration_)\n        \n        rmse_tuned.append(mean_squared_error(y_test,predictions,squared=False))\n        \n        print(f'RMSE tuned {t}: {rmse_tuned[t-1]}')\n        \n    print(f'Improvement of {rmse[i]-rmse_tuned[t-1]}')\n    \n    # ---------------------------------------------------------------------------\n    # Inference time: calculate predictions for test set\n    \n    preds+=model.predict(test[columns],num_iteration=model.best_iteration_)\/kf.n_splits\n        \n    i+=1","e76404f2":"# Create submission file\ntest['target']=preds\ntest=test[['id','target']]\ntest.to_csv('submission.csv',index=False)","83325409":"## Goal of the notebook\n\nI decided to participate to this competition in order to increase my understanding of gradient boosting algorithms. I found the notebook of **Awwal Malhi** (https:\/\/www.kaggle.com\/awwalmalhi\/extreme-fine-tuning-lgbm-using-7-step-training) about Extreme Gradient Boosting very interesting (special thanks to him) and I decided to take a deep dive into it. My notebook is just an implementation of the strategy that he used with some explanations and some improvements.\n\n\n## Pretrained LGBM strategy:\n\nThis strategy enables me to go from **0.84198** to **0.84184** with a single lgbm model. **Awwal Malhi** already gave some explanations of his strategy, but I will try to add mine.\n\n### How it works ?\n\n* train your best model\n* decrease learning rate and train the model again\n* decrease regularization params and retrain the model\n\n### Explanations\n\nThis strategy is mostly based on **transfer learning** (mostly used in neural networks). In transfer learning,we use a pretrained model and add a head to it. Moreover, we usually freeze lower layers (the ones of the pretrained model) and train higher layers (those that we add to the pretrained model). This is exactly the case here:\n\nWe create a normal lgbm model and fit it on our data. Once it starts overfitting we stop the training. We will consider this part of the lgbm model as the pretrained model (to make an analogy to neural networks). \n\nAfter that, and in order to fight against overfitting, we decrease learning rate and starts fitting again the pretrained model on our data, in other words we add more weak learners to our pretrained model (that can be compared to higher layers in a neural network). We can also make an analogy to neural networks in this case. Indeed, when we train neural networks, it is good practice to decrease the learning rate during training process.\n\nOnce reducing the learning rate is not adding a significant improvement to our model, we should increase the complexity of our weak learners. Indeed, increasing weak learners complexity might increase their performance while also increasing their chance of overfitting. At inference time, we will have weak learners with high bias and low variance (weak learners from the pretrained model) and some which are slightly overfit (low bias- high variance). This is why we reduce the learning rate before adding overfitted weak learning (when we reduce learning rate, we basically reduce the contribution of these overfitted trees to final prediction).\n\nI tried many things in order to increase model compelxity and decrease regularization params. I found that the best thing to do is to increase number of leaves and decrease minimum child samples. \n\n**This explanation was simply my understanding of how this strategy works. If you think anything is wrong or you want to add something, feel free to add a comment, I will be glad to read it and find what others think of how it works.**","0610057f":"I wanted to say many thanks to **Hamza** (https:\/\/www.kaggle.com\/hamzaghanmi\/lgbm-hyperparameter-tuning-using-optuna) who creates an amazing notebook for tuning hyperparameters using Optuna, I learnt a lot from it. Using Optuna and a 5 fold cross validation strategy,the best result I could get is 0.84201 on public lb. So for my experiments I decided to use the hyperparameters of **Bizen** (https:\/\/www.kaggle.com\/hiro5299834\/tps-feb-2021-with-single-lgbm-tuned) which gave him a slightly better score of 0.84198 on public lb. ","83e7ba43":"# Imports","0ecd2ea5":"## Hyperparameter tuning with Optuna","27445515":"### Thanks for reading\n\nIf you like this work and find it useful, upvote please.","1c9b87f1":"def objective(trial,train=train,target=y):\n    \n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n    \n    param={\n        'num_leaves':trial.suggest_int('num_leaves',100,1000),\n        'max_depth':trial.suggest_categorical('max_depth',[7,10,20,50]),\n        'min_child_samples':trial.suggest_int('min_child_samples',100,500),\n        'max_bin':trial.suggest_categorical('max_bin',[255,350,512,1024]),\n        'learning_rate':trial.suggest_categorical('learning_rate',[0.006,0.008,0.01,0.014,0.017,0.02]),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'metric': 'rmse', \n        'random_state': 48,\n        'n_estimators': 30000\n         \n    }\n    \n    model=LGBMRegressor(**param)\n    \n    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100,verbose=False)\n    \n    predictions=model.predict(X_test)\n    \n    rmse=mean_squared_error(y_test,predictions,squared=False)\n    \n    return rmse\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=200)\n\nstudy.best_params","3d89bb0a":"# Preprocessing","f415c8cc":"# Pretrained lgbm model strategy"}}