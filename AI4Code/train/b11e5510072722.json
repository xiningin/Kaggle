{"cell_type":{"84ad9f23":"code","b3fa5b77":"code","4386ced0":"code","85bebdbc":"code","07bf099e":"code","cf585e4e":"code","f4703514":"code","65e7a342":"code","99948868":"code","1fae9098":"code","3d4d207c":"code","5cf95774":"code","061ee375":"code","ce9ea604":"code","ec9f84d7":"code","47ec1844":"code","a041342b":"code","0c259012":"code","cb979d5e":"code","2d594f08":"code","47c66877":"code","9043105c":"code","0b5695c5":"code","b49216d3":"code","8ed92f09":"code","2eaa5a04":"code","81a96688":"code","f794db80":"code","74748209":"code","5e358664":"code","0be6f47f":"code","e79b9335":"code","c83369ae":"code","d10d4bdf":"code","77d64971":"code","5667ffbf":"code","98bdcfeb":"code","56cbb885":"code","16c317dc":"code","57b55c98":"code","8b7c9fe4":"code","4dfc2a9e":"code","5be56885":"code","ba5de1b1":"code","4be385c9":"code","a11a63bf":"code","a957a25c":"code","1726d010":"code","5f204a8c":"code","78fad020":"code","0d241c5b":"code","82c94b58":"code","f5f8909a":"code","c98ab239":"code","10c6a7ea":"code","0193551f":"code","18a5f198":"code","610380dc":"code","b104ceb4":"code","4bb0cb09":"code","f4302edf":"code","aa9bed12":"code","1b283e69":"code","e048e701":"code","fd6e013f":"code","7af43bf8":"code","c1376a13":"code","1cdcf082":"code","2052e5ff":"code","0e921ffe":"code","472b0529":"code","d13b72d1":"code","0a8a1401":"code","fae93ce7":"code","fa02e8bb":"code","51759563":"code","459bb4a6":"code","07d8b12c":"code","682bf580":"code","a1ba37cb":"code","3dc0b81a":"code","1d5140e4":"code","058a16dc":"code","46b3dd7f":"code","715b362f":"code","e2b03276":"code","95fe9001":"code","c13ebe4e":"code","10d0eed3":"code","cfe82988":"code","224f570d":"code","86b16fdf":"code","192d96c2":"code","8c5b1e26":"code","4b37903b":"code","5435d5d8":"code","03ec642d":"code","914e2829":"code","8bc22a32":"code","9cce1011":"code","290b39f8":"code","468d93c6":"code","e20b376b":"code","304fef9d":"code","f663d2a6":"code","509f1993":"code","795784b6":"code","f63140db":"code","10404d1b":"code","17851192":"code","8450271c":"code","b15f7957":"code","c570d6b8":"code","0474c376":"code","1d0d153b":"code","70b2fd6a":"code","716c360d":"code","b015eb16":"code","a7247e59":"code","1e52dbb2":"code","131cf922":"code","0615abc0":"code","83dec423":"code","edfe67aa":"code","19c87f65":"code","b9865b1f":"code","8615b83e":"code","70b2beb8":"code","776621de":"code","3587a70a":"code","1e09c95b":"code","ae800396":"code","dbd2bc8f":"code","154dd9e6":"code","24273d25":"code","b3be6365":"code","0430bc75":"code","805f0e9f":"code","01d0394b":"code","e8e8be21":"code","1e61401f":"code","722503ab":"code","f0f7ba60":"code","7441a102":"code","d21f6a7d":"code","3993f9c5":"code","789d2c38":"code","d9110d0a":"code","c504ffeb":"code","d1dff802":"code","7dc86fdd":"code","e881ff09":"code","4dc3a21e":"code","468df8f2":"code","5cf7692a":"code","96e005b7":"code","9417a961":"code","6afd41a4":"code","f090dc8d":"code","2583b0f0":"code","9d7f13b7":"code","dad95362":"code","bc05659d":"code","5b7ff0fc":"code","a3f7a7f9":"code","d28fd719":"code","d25d088e":"code","735a40eb":"code","da4e519d":"code","9396e061":"code","e769886c":"code","3e2c21e2":"code","9f7b5c4e":"code","8a5a37e0":"code","6346fa9a":"code","00bea854":"code","48c89fca":"code","f6014c2a":"code","c91bf6a2":"code","9a4d8397":"code","9cf1234b":"code","660cdf43":"code","31819b48":"code","ee2d9099":"code","cdc36ca8":"code","3aa0b02e":"code","b19cdc53":"code","89833379":"code","af2a8f75":"code","6e99c17b":"code","7eb6d8d8":"code","89b08c81":"code","82d71076":"code","7063a5fd":"code","6b755774":"code","570d3128":"markdown","bea97809":"markdown","23b00723":"markdown","328d1347":"markdown","ff836907":"markdown","04447609":"markdown","05f0f015":"markdown","b1c86109":"markdown","c0b90fec":"markdown","0a69731b":"markdown","1e83d7e2":"markdown","05b7c8cc":"markdown","dffa0ca7":"markdown","47703a98":"markdown","770297d5":"markdown","7f8a2e9e":"markdown","a002f2fb":"markdown","530ed610":"markdown","1d7e770e":"markdown","626490f2":"markdown","9a661d47":"markdown","feeb5c4d":"markdown","01c69a55":"markdown","b2de42b5":"markdown","20a64fc4":"markdown","939af001":"markdown","b65f67c5":"markdown","8d87f85d":"markdown","db27ff2e":"markdown","7ea71554":"markdown","c495ea46":"markdown","2e75257d":"markdown","44e6451b":"markdown","4e87b171":"markdown","a1a95f76":"markdown","22031d08":"markdown","7c5821b6":"markdown","e0db643b":"markdown","689d7e75":"markdown","f90bacec":"markdown","3e2ed49b":"markdown","4b607557":"markdown","4147418c":"markdown","d65d1b29":"markdown","556c1814":"markdown","29cd039c":"markdown","b6dca726":"markdown","c13ae716":"markdown","146e05a8":"markdown","a5f6d11f":"markdown","e1bfe300":"markdown","14d6ad99":"markdown","e44b984b":"markdown","a0dcdc9a":"markdown","1b96c18a":"markdown","6d6b4719":"markdown","7d4885d6":"markdown","c01f6a55":"markdown","56565bc3":"markdown","bcf0324d":"markdown","7db2e7cb":"markdown","faf42201":"markdown","da9a294e":"markdown","c069057d":"markdown","a49bc519":"markdown","1efd50e7":"markdown","4b19a12c":"markdown","4b3ea23f":"markdown","c8244e24":"markdown","4c8a9e43":"markdown","d6f86cd5":"markdown","dc67c8e4":"markdown","c2ede664":"markdown","df04a0cf":"markdown","12fac570":"markdown","933e2f7d":"markdown","e4346639":"markdown","79806b16":"markdown","af0aa549":"markdown","011956ef":"markdown","74d93f19":"markdown","e5648f32":"markdown","7f0e6d8b":"markdown","21479767":"markdown","d0ab6266":"markdown","f49d3aca":"markdown","7d33db69":"markdown","2459854a":"markdown","c003757c":"markdown","c5445506":"markdown","79e1f128":"markdown","e5bd3355":"markdown","ec221358":"markdown","07560483":"markdown","8b4e5d4a":"markdown","9467a12c":"markdown","243d22b4":"markdown","e9a20bba":"markdown","4d880bf2":"markdown","3f3d54bb":"markdown","62febad6":"markdown","598fd921":"markdown","aeb3e81d":"markdown","6509c7bb":"markdown","ad25e260":"markdown","76fe4a64":"markdown","4ea51136":"markdown","726964ac":"markdown","46effca2":"markdown","f2df78d7":"markdown","13cb7244":"markdown","786e1824":"markdown","1729f792":"markdown"},"source":{"84ad9f23":"!pip install --upgrade pip\n!pip uninstall -y numpy\n!pip install numpy==1.18.2\n!pip uninstall -y pandas\n!pip install pandas==1.0.3\n!pip uninstall -y matplotlib\n!pip install matplotlib==3.2.1\n!pip uninstall -y wordcloud\n!pip install wordcloud==1.6.0\n!pip uninstall -y swifter\n!pip install swifter==0.301\n!pip uninstall -y seaborn\n!pip install seaborn==0.10.0\n!pip uninstall -y plotly\n!pip install plotly==4.5.4\n!pip uninstall -y tensorflow\n!pip install tensorflow==2.0.0","b3fa5b77":"# Hiding all warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import numpy, pandas and other necessary libraries\nimport re\nimport numpy as np\nimport pandas as pd\nimport swifter\nfrom wordcloud import STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer","4386ced0":"# Optimizing settings and configuraturation\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.options.display.max_columns = 50","85bebdbc":"# Reading the dataset\n\ndata = pd.read_csv('..\/input\/us-accidents\/US_Accidents_June20.csv')","07bf099e":"# Displaying initial data\n\ndata.head(5)","cf585e4e":"# Understanding the dataset | Meta Data\n\ndata.info()","f4703514":"# Understanding the dataset | Data Content\n\ndata.describe(include='all')","65e7a342":"# Displaying the summary of the dataset\n\nprint('Rows     :',data.shape[0])\nprint('Columns  :',data.shape[1])","99948868":"# Displaying the summary of the dataset\n\nprint('Rows     :',data.shape[0])\nprint('Columns  :',data.shape[1])","1fae9098":"# Displaying the attributes in the dataset\n\nprint('\\nAttributes:\\n',data.columns.tolist())","3d4d207c":"# Displaying the number of missing values per attribute\n\nprint('Percentage of Missing values:\\n\\n',(100*data.isnull().sum()\/data.shape[0]).round(2))","5cf95774":"#Displaying the number of unique values per attribute\n\nprint('Unique values per column :',data.nunique())","061ee375":"# Displaying the names of the attributes which consists of numerical values\n\ndata.select_dtypes(include=['int','float']).columns","ce9ea604":"# Displaying the names of the attributes which consists of non-numerical values\n\ndata.select_dtypes(exclude=['int','float']).columns","ec9f84d7":"# Displaying the list of attributes with high amount of missing values (>20%)\n\nprint('Attributes with > 20% missing values: ', data.columns[(100*data.isnull().sum()\/data.shape[0]).round(2)>20].tolist())","47ec1844":"# Imputing the missing values in TMC to 201, since that means generic 'Accident' and we and not sure of further details.\n\ndata['TMC'].fillna(value=201, inplace=True)","a041342b":"# Removing the attributes from the dataset\n\ndata.drop(columns=['End_Lat', 'End_Lng', 'Number', 'Wind_Chill(F)', 'Precipitation(in)'], inplace=True)","0c259012":"# Checking if we still have missing values per attribute in our dataset\n# Displaying the list of attributes with any amount of missing values (>0%)\n\nprint('Attributes with missing values: ', data.columns[(100*data.isnull().sum()\/data.shape[0])>0].tolist())","cb979d5e":"# Setting 'Wind_Speed(mph)' as 0 for rows where 'Wind_Direction' is 'Calm', meaning almost no wind.\n\ndata.loc[data['Wind_Direction'] == 'Calm', 'Wind_Speed(mph)'] = 0","2d594f08":"# Checking if we still have missing values in the attribute 'Wind_Speed(mph)'\n\nprint('Percentage of Missing values in attribute Wind_Speed(mph) is: ', (100*data['Wind_Speed(mph)'].isnull().sum()\/data.shape[0]).round(2))","47c66877":"# Checking what percentage of rows will remain after dropping rows with missing values\n\nprint(f'Percentage of rows remaining after removal of rows containing missing values: {(100*data.dropna().shape[0]\/data.shape[0]):.4}')\nprint(f'Percentage of rows deleted in order to remove missing values: {100-(100*data.dropna().shape[0]\/data.shape[0]):.4}')","9043105c":"# Dropping all rows with missing values since the less than 30% records gets deleted, and we have huge dataset\n\ndata.dropna(inplace=True)","0b5695c5":"# Checking if we still have missing values per attribute in our dataset\n# Displaying the list of attributes with any amount of missing values (>0%)\n\nprint('Attributes with Missing values: ', data.columns[(100*data.isnull().sum()\/data.shape[0])>0].tolist())","b49216d3":"# Checking for datatypes of the attributes\n\ndata.dtypes","8ed92f09":"# Converting datatype for attributes related to datetime.\n\ndata[\"Start_Time\"]= pd.to_datetime(data[\"Start_Time\"]) \ndata[\"End_Time\"]= pd.to_datetime(data[\"End_Time\"])\ndata[\"Weather_Timestamp\"]= pd.to_datetime(data[\"Weather_Timestamp\"])","2eaa5a04":"# Checking the percentage of Duplicate records in the dataset\n\nprint(f'Percentage of duplicate records: {100-(100*data.drop_duplicates().shape[0]\/data.shape[0])}')","81a96688":"# Dropping the duplicate records, if any\n\ndata.drop_duplicates(inplace=True)","f794db80":"# Adding consistency to the various wind speed direction values\n\ndata['Wind_Direction'].replace({'North': 'N'}, inplace=True)\ndata['Wind_Direction'].replace({'East': 'E'}, inplace=True)\ndata['Wind_Direction'].replace({'West': 'W'}, inplace=True)\ndata['Wind_Direction'].replace({'South': 'S'}, inplace=True)\ndata['Wind_Direction'].replace({'VAR': 'Variable'}, inplace=True)\ndata['Wind_Direction'].replace({'CALM': 'Calm'}, inplace=True)\n\ndata['Wind_Direction'].unique()","74748209":"# 'End_Time' should always be greater than 'Start_Time'\n\ndata.drop(data[data['End_Time']<data['Start_Time']].index, inplace=True)","5e358664":"# Since the entire dataset is of one country, we can remove the attribute 'Country' as it contains only one value\n\ndata.drop(columns=['Country'], inplace=True)","0be6f47f":"# Since the entire dataset contains single value for 'Turning_Loop' we can remove the attribute 'Turning_Loop'\n\ndata.drop(columns=['Turning_Loop'], inplace=True)","e79b9335":"# Since the ID column is an identifier, we can remove the attribute 'ID'\n\ndata.drop(columns=['ID'], inplace=True)","c83369ae":"# Deriving the attribute 'Time_Duration(min)'\n\ndata.insert(4,'Time_Duration(min)',(data['End_Time']-data['Start_Time'])\/\/np.timedelta64(1,'m'))","d10d4bdf":"data['Time_Duration(min)'].describe()","77d64971":"# Dropping the attribute 'End_Time' since it is redundant now\n\ndata.drop(columns=['End_Time'], inplace=True)","5667ffbf":"# Breaking 'Start_Time' into 'Year', 'Month', 'Day', 'Hour' and 'Weekend'\n\ndata['Year']=data['Start_Time'].dt.year\ndata['Month']=data['Start_Time'].dt.month\ndata['Day']=data['Start_Time'].dt.day\ndata['Hour']=data['Start_Time'].dt.hour\ndata['Minute']=data['Start_Time'].dt.minute\ndata['Weekday']=data['Start_Time'].dt.weekday\n\ndef weekday_text(w):\n    d = {0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}\n    return d[w]\ndata['Weekday']=data['Weekday'].apply(lambda x:weekday_text(x))\n","98bdcfeb":"# Dropping the attribute 'Start_Time' since it is redundant now\n\ndata.drop(columns=['Start_Time'], inplace=True)","56cbb885":"# Extrating keywords from the attribute 'Description' using NLP and suggestion from article\n# https:\/\/medium.com\/analytics-vidhya\/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n\ndef clean(text):\n    # lowercase\n    text=text.lower()\n    #remove tags\n    text=re.sub(\"<\/?.*?>\",\" <> \",text)\n    # remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    return text\n\ndata['Description'] = data['Description'].swifter.apply(lambda x:clean(x))\n\n# removing stopwords\ndata['Description'] = data['Description'].swifter.apply(lambda x: ' '.join([item for item in x.split(' ') if item not in STOPWORDS]))\n\n#show the starting few 'Descriptions'\ndata['Description'][:10]","16c317dc":"# Getting the Description (text) column \ndocs=data['Description'].tolist()\n\n# Creating a vocabulary of words, Ignoring words that appear in 85% of documents, Eliminating stop words\ncv=CountVectorizer(max_df=0.85,stop_words=STOPWORDS)\nword_count_vector=cv.fit_transform(docs)\n\n# Displaying Shape\nword_count_vector.shape","57b55c98":"# Generating TFIDF Transformer\n\ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(word_count_vector)\ntfidf_transformer.idf_","8b7c9fe4":"# Sorting the feature name based on score\n\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)[:3]\n\n# Extracting \ndef extract_top5_from_vector(feature_names, sorted_items):\n    keyword = []\n\n    for idx, score in sorted_items:\n        keyword.append(feature_names[idx])\n\n    return ';'.join(keyword)","4dfc2a9e":"# Getting actual feature names\n\nfeature_names=cv.get_feature_names()","5be56885":"# Extracting the 'Keywords' from 'Description' attribute of dataset\n\ndef extract_description_keywords(doc):\n    #generate tf-idf for the given document\n    tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n    #sort the tf-idf vectors by descending order of scores and get features\n    sorted_items=sort_coo(tf_idf_vector.tocoo(copy=False))\n    return extract_top5_from_vector(feature_names,sorted_items)\n\ndata['Keywords'] = data['Description'].swifter.apply(lambda x:extract_description_keywords(x))","ba5de1b1":"# Adding an attribute per keyword to be used for modelling\n\ndata['Keyword_1'] = data['Keywords'].swifter.apply(lambda x:str(x.split(';')[0]) if len(x.split(';'))>0 else None)\ndata['Keyword_2'] = data['Keywords'].swifter.apply(lambda x:str(x.split(';')[1]) if len(x.split(';'))>1 else None)\ndata['Keyword_3'] = data['Keywords'].swifter.apply(lambda x:str(x.split(';')[2]) if len(x.split(';'))>2 else None)","4be385c9":"# Displaying the number of missing values per attribute\n\nprint('Percentage of Missing values:\\n\\n',(100*data.isnull().sum()\/data.shape[0]).round(2))","a11a63bf":"# Removing records with missing values, i.e. missing 'Keywords' since they are only around 2.5%\n\ndata.dropna(inplace=True)","a957a25c":"# Removing Redundant Column; 'Description' and 'Keywords'\n\ndata.drop(columns=['Description', 'Keywords'], inplace=True)","1726d010":"# Removing outliers | Time_Duration\n# Removing records stating duration more than 12 days (since higher than 12 days is not recorded yet)\n\ndata.drop(data[data['Time_Duration(min)'] > (12*1440)].index, inplace=True)","5f204a8c":"# Removing outliers | Wind_Speed(mph)\n# Removing records wind speed more than 260 mph (since higher than ~253mph is not recorded yet)\n\ndata.drop(data[data['Wind_Speed(mph)'] > 260].index, inplace=True)","78fad020":"# Removing outliers | Distance(mi)\n# Removing records distance(mi) more than 109 miles (since higher than ~109 miles is not recorded yet)\n\ndata.drop(data[data['Distance(mi)'] > 109].index, inplace=True)","0d241c5b":"# Removing outliers | Temperature(F)\n# Removing records temperature(f) more than 131.4 mph (since higher than ~134.1 F is not recorded yet)\n\ndata.drop(data[data['Temperature(F)'] > 134.1].index, inplace=True)","82c94b58":"# Removing outliers | Pressure(in)\n# Removing records pressure(in) less than 25.69 (since lesser than ~25.69 is not recorded yet)\n# Removing records pressure(in) more than 32.03 (since higher than ~32.03 is not recorded yet)\n\ndata.drop(data[data['Pressure(in)'] < 25.69].index, inplace=True)\ndata.drop(data[data['Pressure(in)'] > 32.03].index, inplace=True)","f5f8909a":"# Removing outliers | Wind_Speed(mph)\n# Removing records visibility(mi) more than 150 miles (since higher than ~150 miles is not recorded yet)\n\ndata.drop(data[data['Visibility(mi)'] > 150].index, inplace=True)","c98ab239":"# Checking if none of the value is NaN and all of the values are finite and saving to file\n\nif data.notnull().values.all() and not data.isnull().values.any():\n    data.to_csv(\"\/kaggle\/working\/data.csv\", index=False)\n    print('Data Saved')\nelse:\n    print('Data Not Saved')","10c6a7ea":"%reset -f\n\n# Hiding all warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import numpy and pandas\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n# import for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# do an inline so that plt.show() is not required everytime\n%matplotlib inline","0193551f":"# Reading the dataset\n\ndata = pd.read_csv('\/kaggle\/working\/data.csv').dropna()","18a5f198":"# Displaying the summary of the dataset\n\nprint('Rows     :',data.shape[0])\nprint('Columns  :',data.shape[1])","610380dc":"# Displaying all attributes of the dataset\n\ndata.columns","b104ceb4":"data.dtypes","4bb0cb09":"data.describe(exclude=[np.object]).T","f4302edf":"data.describe(include=[np.object]).T","aa9bed12":"# Analysing the 'Severity' attribute\n\n#Textual Representation\nprint(data.groupby(by='Severity').size())","1b283e69":"# Analysing the 'TMC' attribute\n\n#Pictorial Representation\nfig = plt.figure(figsize = (12, 4))\nsns.countplot(y=\"TMC\", data=data, order=data['TMC'].value_counts().index[:10], palette='Blues_d')\nplt.show()","e048e701":"# Set up the matplotlib figure\nf, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Analysing the 'Amenity' attribute\nsns.distplot(data['Year'], ax=axes[0, 0])\n\n# Analysing the 'Amenity' attribute\nsns.distplot(data['Month'], ax=axes[0, 1])\n\n# Analysing the 'Amenity' attribute\nsns.distplot(data['Day'], ax=axes[1, 0])\n\n# Analysing the 'Amenity' attribute\nsns.countplot(data['Weekday'], palette='Blues_d',ax=axes[1, 1])","fd6e013f":"# Set up the matplotlib figure\nf, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Analysing the 'Start_Time' Hour, Minute attribute\nsns.distplot(data['Hour'], bins=24, ax=axes[0])\nsns.distplot(data['Minute'], bins=60, ax=axes[1])","7af43bf8":"# Analysing the 'Timestamp' (Year, Month) attribute\n\nfig = plt.figure(figsize = (16, 4))\ndata.groupby(by=['Year', 'Month']).size().plot()","c1376a13":"# Set up the matplotlib figure\nf, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Analysing the 'Time_Duration(min)' attribute\nsns.distplot(data['Time_Duration(min)']\/60, ax=axes[0])\nsns.boxplot(data['Time_Duration(min)'], ax=axes[1])","1cdcf082":"# Set up the matplotlib figure\nf, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Analysing the 'Distance(mi)' attribute\nsns.distplot(data['Distance(mi)'], kde=False, ax=axes[0])\nsns.boxplot(data['Distance(mi)'], ax=axes[1])","2052e5ff":"# Analysing the 'Description' attribute\n\ntext = ' '.join(data['Keyword_1'].to_list())\nwordcloud = WordCloud(width = 400, height = 400, background_color = 'white', stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(figsize = (5, 5))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","0e921ffe":"# Analysing the 'Description' attribute\n\ntext = ' '.join(data['Keyword_2'].to_list())\nwordcloud = WordCloud(width = 400, height = 400, background_color = 'white', stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(figsize = (5, 5))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","472b0529":"# Analysing the 'Description' attribute\n\ntext = ' '.join(data['Keyword_3'].to_list())\nwordcloud = WordCloud(width = 400, height = 400, background_color = 'white', stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(figsize = (5, 5))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","d13b72d1":"# Analysing the 'State' attribute\n\nfig = plt.figure(figsize = (16, 6))\nsns.countplot(x='State', data=data, order=data['State'].value_counts().index, palette='Blues_d')\nplt.show()","0a8a1401":"data.groupby(by='State').size().sort_values().plot.pie(autopct='%1.1f%%', shadow=True, figsize=(16, 16))","fae93ce7":"# Set up the matplotlib figure\nf, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Analysing the 'Temperature(F)' attribute\nsns.distplot(data['Temperature(F)'], ax=axes[0, 0])\nsns.distplot(data['Humidity(%)'], ax=axes[0, 1])\nsns.distplot(data['Pressure(in)'], ax=axes[1, 0])\nsns.distplot(data['Wind_Speed(mph)'], ax=axes[1, 1])","fa02e8bb":"# Analysing the 'Visibility(mi)' attribute\n\nfig = plt.figure(figsize = (16, 6))\nsns.distplot(data['Visibility(mi)'])\nplt.show()","51759563":"# Analysing the 'Wind_Direction' attribute\n\nfig = plt.figure(figsize = (12, 6))\nsns.countplot(y='Wind_Direction', data=data, order=data['Wind_Direction'].value_counts().index, palette='Blues_d')\nplt.show()","459bb4a6":"# Analysing the 'Weather_Condition' attribute\n\nfig = plt.figure(figsize = (12, 6))\nsns.countplot(y='Weather_Condition', data=data, order=data['Weather_Condition'].value_counts()[:20].index, palette='Blues_d')\nplt.show()","07d8b12c":"# Set up the matplotlib figure\nf, axes = plt.subplots(3, 4, figsize=(20, 16))\n\n# Analysing the 'Amenity' attribute\nsns.countplot(x='Amenity', data=data, ax=axes[0, 0], palette='Blues_d')\n\n# Analysing the 'Bump' attribute\nsns.countplot(x='Bump', data=data, ax=axes[0, 1], palette='Blues_d')\n\n# Analysing the 'Crossing' attribute\nsns.countplot(x='Crossing', data=data, ax=axes[0, 2], palette='Blues_d')\n\n# Analysing the 'Give_Way' attribute\nsns.countplot(x='Give_Way', data=data, ax=axes[0, 3], palette='Blues_d')\n\n# Analysing the 'Junction' attribute\nsns.countplot(x='Junction', data=data, ax=axes[1, 0], palette='Blues_d')\n\n# Analysing the 'No_Exit' attribute\nsns.countplot(x='No_Exit', data=data, ax=axes[1, 1], palette='Blues_d')\n\n# Analysing the 'Railway' attribute\nsns.countplot(x='Railway', data=data, ax=axes[1, 2], palette='Blues_d')\n\n# Analysing the 'Roundabout' attribute\nsns.countplot(x='Roundabout', data=data, ax=axes[1, 3], palette='Blues_d')\n\n# Analysing the 'Station' attribute\nsns.countplot(x='Station', data=data, ax=axes[2, 0], palette='Blues_d')\n\n# Analysing the 'Stop' attribute\nsns.countplot(x='Stop', data=data, ax=axes[2, 1], palette='Blues_d')\n\n# Analysing the 'Traffic_Calming' attribute\nsns.countplot(x='Traffic_Calming', data=data, ax=axes[2, 2], palette='Blues_d')\n\n# Analysing the 'Traffic_Signal' attribute\nsns.countplot(x='Traffic_Signal', data=data, ax=axes[2, 3], palette='Blues_d')","682bf580":"# Set up the matplotlib figure\nf, axes = plt.subplots(2, 2, figsize=(8, 8))\n\n# Analysing the 'Sunrise_Sunset' attribute\nsns.countplot(x='Sunrise_Sunset', data=data, ax=axes[0, 0], palette='Blues_d')\n\n# Analysing the 'Civil_Twilight' attribute\nsns.countplot(x='Civil_Twilight', data=data, ax=axes[0, 1], palette='Blues_d')\n\n# Analysing the 'Nautical_Twilight' attribute\nsns.countplot(x='Nautical_Twilight', data=data, ax=axes[1, 0], palette='Blues_d')\n\n# Analysing the 'Astronomical_Twilight' attribute\nsns.countplot(x='Astronomical_Twilight', data=data, ax=axes[1, 1], palette='Blues_d')","a1ba37cb":"# Analysing the 'TMC' & 'Severity' attribute\n\ntemp = pd.DataFrame(data.groupby(by='TMC').size())\ntemp = temp.sort_values(by=0, ascending=False).index[:5]\n\n#Pictorial Representation\n\nfig = plt.figure(figsize = (16, 6))\nsns.countplot(y=\"TMC\", data=data, order=temp, hue='Severity', palette='Blues_d')\nplt.legend(loc='lower right')\nplt.show()","3dc0b81a":"# Analysing the 'Year' & 'Severity' attribute\n\nfig = plt.figure(figsize = (16, 4))\nsns.countplot(y=\"Year\", data=data, hue='Severity', palette='Blues_d')\nplt.legend(loc='lower right')\nplt.show()","1d5140e4":"# Analysing the 'Month' & 'Severity' attribute\n\nfig = plt.figure(figsize = (16, 6))\nsns.countplot(y=\"Month\", data=data, hue='Severity', palette='Blues_d')\nplt.legend(loc='lower right')\nplt.show()","058a16dc":"# Analysing the 'Weekday' & 'Severity' attribute\n\nfig = plt.figure(figsize = (16, 6))\nsns.countplot(y=\"Weekday\", data=data, hue='Severity', palette='Blues_d')\nplt.legend(loc='lower right')\nplt.show()","46b3dd7f":"# Set up the matplotlib figure\nf, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Analysing the impact of 'Time_Duration(min)' attribute on 'Severity' attribute | Scatter Plot\ndata.plot.scatter(x='Time_Duration(min)', y='Severity', ax=axes[0])\n\n# Analysing the impact of 'Distance(mi)' attribute on 'Severity' attribute | Scatter Plot\ndata.plot.scatter(x='Distance(mi)', y='Severity', ax=axes[1])","715b362f":"# Analysing the 'State' & 'Severity' attribute\n\nfig = plt.figure(figsize = (16, 6))\nsns.countplot(x=\"State\", data=data, order=data['State'].value_counts().index, hue='Severity', palette='Blues_d')\nplt.legend(loc='lower right')\nplt.show()","e2b03276":"# Set up the matplotlib figure\nf, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Analysing the impact of 'Temperature(F)' attribute on 'Severity' attribute | Scatter Plot\ndata.plot.scatter(x='Temperature(F)', y='Severity', ax=axes[0, 0])\n\n# Analysing the impact of 'Humidity(%)' attribute on 'Severity' attribute | Scatter Plot\ndata.plot.scatter(x='Humidity(%)', y='Severity', ax=axes[0, 1])\n\n# Analysing the impact of 'Pressure(in)' attribute on 'Severity' attribute | Scatter Plot\ndata.plot.scatter(x='Pressure(in)', y='Severity', ax=axes[1, 0])\n\n# Analysing the impact of 'Wind_Speed(mph)' attribute on 'Severity' attribute | Scatter Plot\ndata.plot.scatter(x='Wind_Speed(mph)', y='Severity', ax=axes[1, 1])","95fe9001":"# Analysing the impact of 'Visibility(mi)' attribute on 'Severity' attribute | Scatter Plot\n\nfig = plt.figure(figsize = (8, 6))\ndata.plot.scatter(x='Visibility(mi)', y='Severity')\nplt.show()","c13ebe4e":"# Analysing the 'Side' & 'Severity' attribute\n\nfig = plt.figure(figsize = (16, 6))\nsns.countplot(x=\"Side\", data=data, order=data['Side'].value_counts().index, hue='Severity', palette='Blues_d')\nplt.legend(loc='lower right')\nplt.show()","10d0eed3":"# Analysing the 'Side' & 'Severity' attribute\n\nfig = plt.figure(figsize = (16, 6))\nsns.countplot(x=\"Wind_Direction\", data=data, order=data['Wind_Direction'].value_counts().index, hue='Severity', palette='Blues_d')\nplt.legend(loc='lower right')\nplt.show()","cfe82988":"# Analysing the 'Weather_Condition' & 'Severity' attribute\n\nfig = plt.figure(figsize = (16, 6))\nsns.countplot(x=\"Weather_Condition\", data=data, order=data['Weather_Condition'].value_counts()[:12].index, hue='Severity', palette='Blues_d')\nplt.legend(loc='lower right')\nplt.show()","224f570d":"# Set up the matplotlib figure\nf, axes = plt.subplots(2, 2, figsize=(16, 6))\n\n# Analysing the impact of 'Sunrise_Sunset' attribute on 'Severity' attribute | Box Plot\nsns.boxplot(x='Sunrise_Sunset', y='Severity', data=data, ax=axes[0, 0], order=data['Sunrise_Sunset'].value_counts()[:10].index)\n\n# Analysing the impact of 'Civil_Twilight' attribute on 'Severity' attribute | Box Plot\nsns.boxplot(x='Civil_Twilight', y='Severity', data=data, ax=axes[0, 1], order=data['Civil_Twilight'].value_counts()[:10].index)\n\n# Analysing the impact of 'Nautical_Twilight' attribute on 'Severity' attribute | Box Plot\nsns.boxplot(x='Nautical_Twilight', y='Severity', data=data, ax=axes[1, 0], order=data['Nautical_Twilight'].value_counts()[:10].index)\n\n# Analysing the impact of 'Astronomical_Twilight' attribute on 'Severity' attribute | Box Plot\nsns.boxplot(x='Astronomical_Twilight', y='Severity', data=data, ax=axes[1, 1], order=data['Astronomical_Twilight'].value_counts()[:10].index)","86b16fdf":"# plotting correlations on a heatmap\n\nplt.figure(figsize=(16,8))\nsns.heatmap(data.corr(), cmap=\"YlGnBu\", annot=False)\nplt.show()","192d96c2":"BBox = ((data.Start_Lng.min(), data.Start_Lng.max(), data.Start_Lat.min(), data.Start_Lat.max()))\nBBox","8c5b1e26":"map_pic = plt.imread('map\/map_pic.png')","4b37903b":"fig, ax = plt.subplots(figsize = (26,14))\nax.scatter(data[data['Severity']==1].Start_Lng+0.3, data[data['Severity']==1].Start_Lat-0.8, zorder=1, alpha= 0.7, c='blue', s=4)\nax.scatter(data[data['Severity']==2].Start_Lng+0.3, data[data['Severity']==2].Start_Lat-0.8, zorder=1, alpha= 0.7, c='green', s=3)\nax.scatter(data[data['Severity']==3].Start_Lng+0.3, data[data['Severity']==3].Start_Lat-0.8, zorder=1, alpha= 0.7, c='orange', s=2)\nax.scatter(data[data['Severity']==4].Start_Lng+0.3, data[data['Severity']==4].Start_Lat-0.8, zorder=1, alpha= 0.7, c='red', s=1)\n\nax.set_xlim(BBox[0],BBox[1])\nax.set_ylim(BBox[2],BBox[3])\nax.imshow(map_pic, zorder=0, extent = BBox, aspect= 'auto', interpolation='none')\nax.imshow(map_pic, zorder=2, alpha= 0.5, extent = BBox, aspect= 'auto')","5435d5d8":"# Deleting all data\n%reset -f\n\n# Reloading necessary libraries\n# import numpy and pandas\nimport pandas as pd\nimport numpy as np","03ec642d":"# Selecting the features which are likely to be available initially upon the accident, plus target variable 'Severity' for model building.\n\n# Reading the above processed data from disk\ndata= pd.read_csv(\"\/kaggle\/working\/data.csv\").dropna()\n\n# List of all available attributes\ndata.columns","914e2829":"# List of selected attributes based upon fast availability of attributes, considering:\n# 1. the objectibe of the research\n# 2. the non repetition of information (e.g. Complete Address + Zipcode)\n\ncols = [\n        'Source', 'TMC', 'Severity', 'Start_Lat', 'Start_Lng', 'Temperature(F)',\n        'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Direction', 'Wind_Speed(mph)', 'Weather_Condition',\n        'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway','Roundabout', 'Station', \n        'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Sunrise_Sunset', 'Civil_Twilight', \n        'Year', 'Month', 'Day', 'Hour', 'Minute','Weekday', \n        'Street', 'Side', 'City', 'County', 'Keyword_1', 'Keyword_2', 'Keyword_3'\n]","8bc22a32":"# Generating the new dataset with selected columns and dummies for categorical data\n# Checking if none of the value is NaN and all of the values are finite\n# Saving the top 5 state specific dataset to disk and freeing up RAM before building model\n\n# State CA\ndata[data['State']=='CA'][cols].to_csv(\"\/kaggle\/working\/data_CA.csv\", index=False)\n\n# State TX\ndata[data['State']=='TX'][cols].to_csv(\"\/kaggle\/working\/data_TX.csv\", index=False)\n\n# State FL\ndata[data['State']=='FL'][cols].to_csv(\"\/kaggle\/working\/data_FL.csv\", index=False)\n\n# State SC\ndata[data['State']=='SC'][cols].to_csv(\"\/kaggle\/working\/data_SC.csv\", index=False)\n\n# State NC\ndata[data['State']=='NC'][cols].to_csv(\"\/kaggle\/working\/data_NC.csv\", index=False)","9cce1011":"# Deleting all data\n%reset -f\n\n# Reloading necessary libraries\n# import numpy and pandas\nimport pandas as pd\nimport numpy as np\n\n# import for pre-processing\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\n\n# import for visualization\nimport matplotlib.pyplot as plt\n\n# import for model building\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# import for Neural Network based model building\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","290b39f8":"# Building State specific model | State 'CA'\n\nresult = {}\nstate='CA'\nresult['State']=state\nprocessed_data = pd.read_csv(f'\/kaggle\/working\/data_{state}.csv').dropna()\ncols = processed_data.select_dtypes(include='object').columns","468d93c6":"# Class Balancing | Using Up Sampling\n\n# Separate majority and minority classes\ndf_s1 = processed_data[processed_data['Severity']==1]\ndf_s2 = processed_data[processed_data['Severity']==2]\ndf_s3 = processed_data[processed_data['Severity']==3]\ndf_s4 = processed_data[processed_data['Severity']==4]\n\ncount = max(df_s1.count()[0], df_s2.count()[0], df_s3.count()[0], df_s4.count()[0])\n\n# Upsample minority class\ndf_s1 = resample(df_s1, replace=df_s1.count()[0]<count, n_samples=count, random_state=42)\ndf_s2 = resample(df_s2, replace=df_s2.count()[0]<count, n_samples=count, random_state=42)\ndf_s3 = resample(df_s3, replace=df_s3.count()[0]<count, n_samples=count, random_state=42)\ndf_s4 = resample(df_s4, replace=df_s4.count()[0]<count, n_samples=count, random_state=42)\n \n# Combine majority class with upsampled minority class\nprocessed_data = pd.concat([df_s1, df_s2, df_s3, df_s4])\n \n# Display new class counts\nprocessed_data.groupby(by='Severity')['Severity'].count()","e20b376b":"# Set the target for the prediction\ntarget='Severity' \n\n# set X and y\ny = processed_data[target]\nX = processed_data.drop(target, axis=1)\n\n# Create the encoder.\nencoder = OrdinalEncoder()\nX[cols] = encoder.fit_transform(X[cols])\n\n# Split the data set into training and testing data sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n\n# Split the data set into training and validation data sets\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, stratify=y_train, random_state=42)\n\n# Scalling the features of Train Dataset, Validation Dataset and Test Dataset\nscaler = StandardScaler()\n\n# Scaling Train Dataset\nscaler = scaler.fit(X_train)\nX_train = scaler.transform(X_train)\n\n# Scaling Validation Dataset\nscaler = scaler.fit(X_val)\nX_val = scaler.transform(X_val)\n\n# Scaling Test Dataset\nscaler = scaler.fit(X_test)\nX_test = scaler.transform(X_test)","304fef9d":"BBox = ((processed_data.Start_Lng.min(), processed_data.Start_Lng.max(), processed_data.Start_Lat.min(), processed_data.Start_Lat.max()))\nBBox","f663d2a6":"map_pic = plt.imread('\/kaggle\/working\/map\/map_pic_ca.png')","509f1993":"fig, ax = plt.subplots(figsize = (27,33))\nax.scatter(processed_data[processed_data['Severity']==1].Start_Lng, processed_data[processed_data['Severity']==1].Start_Lat-.1, zorder=1, c='b', s=4)\nax.scatter(processed_data[processed_data['Severity']==2].Start_Lng, processed_data[processed_data['Severity']==2].Start_Lat-.1, zorder=1, c='g', s=6)\nax.scatter(processed_data[processed_data['Severity']==3].Start_Lng, processed_data[processed_data['Severity']==3].Start_Lat-.1, zorder=1, c='y', s=8)\nax.scatter(processed_data[processed_data['Severity']==4].Start_Lng, processed_data[processed_data['Severity']==4].Start_Lat-.1, zorder=1, c='r', s=10)\n\nax.set_xlim(BBox[0],BBox[1])\nax.set_ylim(BBox[2],BBox[3])\nax.imshow(map_pic, zorder=0, extent = BBox, aspect= 'auto', interpolation='none')\nax.imshow(map_pic, zorder=2, alpha= 0.5, extent = BBox, aspect= 'auto', interpolation='lanczos')","795784b6":"# Support Vector Machine | First Iteration\n\n# Instantiate an object of class SVC()\nclf = SVC(gamma='auto', kernel='rbf', random_state=42)\n\n# Train & Test (limiting rows since SVM takes much time)\nclf.fit(X_train[:10000], y_train[:10000])\ny_pred = clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Support Vector Machine accuracy_score: {:.3f}.\".format(accuracy_score(y_test, y_pred)))","f63140db":"# Support Vector Machine | Optimization\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'C': [0.1, 0.5, 1],\n    'gamma': ['auto', 'scale']\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(cv=5, estimator = clf, param_grid = param_grid, scoring='balanced_accuracy', n_jobs = -1,verbose = 5)\n\n# Fit the grid search to the Validation Dataset\ngrid_search.fit(X_val[:5000], y_val[:5000])\n\n# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","10404d1b":"# Support Vector Machine | Final Evaluation\n\n# Create a SVM Classifier\nclf=SVC(**grid_search.best_params_, kernel='rbf', random_state=42)\n\n# Train & Test\nclf.fit(X_train[:20000], y_train[:20000])\ny_train_pred= clf.predict(X_train[:20000])\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train[:20000], y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# stroring the accuracy score\nresult['Support Vector Machine'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","17851192":"# Decision Tree Algorithm | First Iteration\n\n# Instantiate a Decision Tree Classifier\nclf = DecisionTreeClassifier(random_state=42)\n\n# Train & Test\nclf.fit(X_train, y_train)\ny_pred= clf.predict(X_test)\n\n# Print accuracy_entropy\nprint('Decision Tree accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_pred)))","8450271c":"# Decision Tree Algorithm | Optimization\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [12, 14, 16],\n    'min_samples_split': [1000, 2000, 3000],\n    'min_samples_leaf': [500, 1000, 1500]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(cv=3, estimator = clf, param_grid = param_grid, scoring='balanced_accuracy', n_jobs = -1,verbose = 5)\n\n# Fit the grid search to the Validation Dataset\ngrid_search.fit(X_val, y_val)\n\n# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","b15f7957":"# Decision Tree Algorithm | Final Evaluation\n\n# Instantiate a Decision Tree Classifier with Best Parameters\nclf = DecisionTreeClassifier(**grid_search.best_params_, random_state=42)\n\n# Train & Test\nclf.fit(X_train, y_train)\ny_train_pred= clf.predict(X_train)\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# Highlighting the significance of each of the factors in the model\nfeature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\nprint(\"\\nImportant features:\\n\", feature_imp.sort_values(ascending=False)[:10])\n\n# stroring the accuracy score\nresult['Decision Tree'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","c570d6b8":"# Random Forest Algorithm | First Iteration\n\n# Create a Random Forest Classifier\nclf=RandomForestClassifier(n_estimators=100, bootstrap=False, min_samples_split=400, min_samples_leaf=100, n_jobs=-1, random_state=42)\n\n# Train & Test\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Randon forest algorithm accuracy_score: {:.3f}.\".format(accuracy_score(y_test, y_pred)))","0474c376":"# Random Forest Algorithm | Optimization\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'n_estimators': [100, 150, 200],\n    'max_depth': [12, 14, 16],\n    'min_samples_split': [200, 300],\n    'min_samples_leaf': [50, 75],\n    'bootstrap': [False]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(cv=3, estimator = clf, param_grid = param_grid, scoring='balanced_accuracy', n_jobs = -1,verbose = 5)\n\n# Fit the grid search to the Validation Dataset\ngrid_search.fit(X_val[:15000], y_val[:15000])\n\n# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","1d0d153b":"# Random Forest Algorithm | Final Evaluation\n\n# Create a Random Forest Classifier\nclf=RandomForestClassifier(**grid_search.best_params_, n_jobs=-1, random_state=42)\n\n# Train & Test\nclf.fit(X_train, y_train)\ny_train_pred= clf.predict(X_train)\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# Highlighting the significance of each of the factors in the model\nfeature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\nprint(\"\\nImportant features:\\n\", feature_imp.sort_values(ascending=False)[:10])\n\n# stroring the accuracy score\nresult['Random Forest'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100, 1))]","70b2fd6a":"# K-Nearest Neighbor | First Iteration\n\n# Create a k-NN classifier\nclf = KNeighborsClassifier(n_jobs=-1)\n\n# Train & Test\nclf.fit(X_train[:20000], y_train[:20000])\ny_train_pred= clf.predict(X_train[:20000])\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train[:20000], y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# stroring the accuracy score\nresult['K-Nearest Neighbors'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","716c360d":"# Neural Network | First Iteration\n\nmodel = Sequential()\nmodel.add(Dense(128, input_dim=np.size(X_train,1), activation='relu'))\nmodel.add(Dense(64, input_dim=np.size(X_train,1), activation='relu'))\nmodel.add(Dense(5, activation='softmax'))\n\n# compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# build the model\nhistory = model.fit(X_train, to_categorical(y_train.to_numpy()), \n                    epochs=5, validation_data=(X_val, to_categorical(y_val.to_numpy())), \n                    validation_steps=30, verbose=0)\n\n\nloss, train_accuracy = model.evaluate(X_train, to_categorical(y_train.to_numpy()), verbose=0)\nprint(f\"\\nFor Training Dataset: Loss: {loss} and Accuracy: {train_accuracy}\")\n\nloss, test_accuracy = model.evaluate(X_test, to_categorical(y_test.to_numpy()), verbose=0)\nprint(f\"\\nFor Testing Dataset: Loss: {loss} and Accuracy: {test_accuracy}\")\n\n# stroring the accuracy score\nresult['Neural Network'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","b015eb16":"# Saving the results in file\n\ndf = pd.DataFrame.from_dict(result)\ndf.set_index(['State'])\ndf.to_csv(f'\/kaggle\/working\/result_{state}.csv', index=False)","a7247e59":"# Deleting all data\n%reset -f\n\n# Reloading necessary libraries\n# import numpy and pandas\nimport pandas as pd\nimport numpy as np\n\n# import for pre-processing\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\n\n# import for visualization\nimport matplotlib.pyplot as plt\n\n# import for model building\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# import for Neural Network based model building\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","1e52dbb2":"# Building State specific model | State 'TX'\n\nresult = {}\nstate='TX'\nresult['State']=state\nprocessed_data = pd.read_csv(f'\/kaggle\/working\/data_{state}.csv').dropna()\ncols = processed_data.select_dtypes(include='object').columns","131cf922":"# Class Balancing | Using Up Sampling\n\n# Separate majority and minority classes\ndf_s1 = processed_data[processed_data['Severity']==1]\ndf_s2 = processed_data[processed_data['Severity']==2]\ndf_s3 = processed_data[processed_data['Severity']==3]\ndf_s4 = processed_data[processed_data['Severity']==4]\n\ncount = max(df_s1.count()[0], df_s2.count()[0], df_s3.count()[0], df_s4.count()[0])\n\n# Upsample minority class\ndf_s1 = resample(df_s1, replace=df_s1.count()[0]<count, n_samples=count, random_state=42)\ndf_s2 = resample(df_s2, replace=df_s2.count()[0]<count, n_samples=count, random_state=42)\ndf_s3 = resample(df_s3, replace=df_s3.count()[0]<count, n_samples=count, random_state=42)\ndf_s4 = resample(df_s4, replace=df_s4.count()[0]<count, n_samples=count, random_state=42)\n \n# Combine majority class with upsampled minority class\nprocessed_data = pd.concat([df_s1, df_s2, df_s3, df_s4])\n \n# Display new class counts\nprocessed_data.groupby(by='Severity')['Severity'].count()","0615abc0":"# Set the target for the prediction\ntarget='Severity' \n\n# set X and y\ny = processed_data[target]\nX = processed_data.drop(target, axis=1)\n\n# Create the encoder.\nencoder = OrdinalEncoder()\nX[cols] = encoder.fit_transform(X[cols])\n\n# Split the data set into training and testing data sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n\n# Split the data set into training and validation data sets\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, stratify=y_train, random_state=42)\n\n# Scalling the features of Train Dataset, Validation Dataset and Test Dataset\nscaler = StandardScaler()\n\n# Scaling Train Dataset\nscaler = scaler.fit(X_train)\nX_train = scaler.transform(X_train)\n\n# Scaling Validation Dataset\nscaler = scaler.fit(X_val)\nX_val = scaler.transform(X_val)\n\n# Scaling Test Dataset\nscaler = scaler.fit(X_test)\nX_test = scaler.transform(X_test)","83dec423":"BBox = ((processed_data.Start_Lng.min(), processed_data.Start_Lng.max(), processed_data.Start_Lat.min(), processed_data.Start_Lat.max()))\nBBox","edfe67aa":"map_pic = plt.imread('\/kaggle\/working\/map\/map_pic_tx.png')","19c87f65":"fig, ax = plt.subplots(figsize = (30,17))\nax.scatter(processed_data[processed_data['Severity']==1].Start_Lng, processed_data[processed_data['Severity']==1].Start_Lat, zorder=1, c='b', s=4)\nax.scatter(processed_data[processed_data['Severity']==2].Start_Lng, processed_data[processed_data['Severity']==2].Start_Lat, zorder=1, c='g', s=6)\nax.scatter(processed_data[processed_data['Severity']==3].Start_Lng, processed_data[processed_data['Severity']==3].Start_Lat, zorder=1, c='y', s=8)\nax.scatter(processed_data[processed_data['Severity']==4].Start_Lng, processed_data[processed_data['Severity']==4].Start_Lat, zorder=1, c='r', s=10)\n\nax.set_xlim(BBox[0],BBox[1])\nax.set_ylim(BBox[2],BBox[3])\nax.imshow(map_pic, zorder=0, extent = BBox, aspect= 'auto', interpolation='none')\nax.imshow(map_pic, zorder=2, alpha= 0.5, extent = BBox, aspect= 'auto', interpolation='lanczos')","b9865b1f":"# Support Vector Machine | First Iteration\n\n# Instantiate an object of class SVC()\nclf = SVC(gamma='auto', kernel='rbf', random_state=42)\n\n# Train & Test (limiting rows since SVM takes much time)\nclf.fit(X_train[:10000], y_train[:10000])\ny_pred = clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Support Vector Machine accuracy_score: {:.3f}.\".format(accuracy_score(y_test, y_pred)))","8615b83e":"# Support Vector Machine | Optimization\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'C': [0.1, 0.5, 1],\n    'gamma': ['auto', 'scale']\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(cv=5, estimator = clf, param_grid = param_grid, scoring='balanced_accuracy', n_jobs = -1,verbose = 5)\n\n# Fit the grid search to the Validation Dataset\ngrid_search.fit(X_val[:5000], y_val[:5000])\n\n# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","70b2beb8":"# Support Vector Machine | Final Evaluation\n\n# Create a SVM Classifier\nclf=SVC(**grid_search.best_params_, random_state=42)\n\n# Train & Test\nclf.fit(X_train[:20000], y_train[:20000])\ny_train_pred= clf.predict(X_train[:20000])\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train[:20000], y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# stroring the accuracy score\nresult['Support Vector Machine'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","776621de":"# Decision Tree Algorithm | First Iteration\n\n# Instantiate a Decision Tree Classifier\nclf = DecisionTreeClassifier(random_state=42)\n\n# Train & Test\nclf.fit(X_train, y_train)\ny_pred= clf.predict(X_test)\n\n# Print accuracy_entropy\nprint('Decision Tree accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_pred)))","3587a70a":"# Decision Tree Algorithm | Optimization\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [12, 14, 16],\n    'min_samples_split': [600, 1000],\n    'min_samples_leaf': [300, 500]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(cv=3, estimator = clf, param_grid = param_grid, scoring='balanced_accuracy', n_jobs = -1,verbose = 5)\n\n# Fit the grid search to the Validation Dataset\ngrid_search.fit(X_val, y_val)\n\n# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","1e09c95b":"# Decision Tree Algorithm | Final Evaluation\n\n# Instantiate a Decision Tree Classifier with Best Parameters\nclf = DecisionTreeClassifier(**grid_search.best_params_, random_state=42)\n\n# Train & Test\nclf.fit(X_train, y_train)\ny_train_pred= clf.predict(X_train)\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# Highlighting the significance of each of the factors in the model\nfeature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\nprint(\"\\nImportant features:\\n\", feature_imp.sort_values(ascending=False)[:10])\n\n# stroring the accuracy score\nresult['Decision Tree'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","ae800396":"# Random Forest Algorithm | First Iteration\n\n# Create a Random Forest Classifier\nclf=RandomForestClassifier(n_estimators=100, bootstrap=False, min_samples_split=400, min_samples_leaf=100, n_jobs=-1, random_state=42)\n\n# Train & Test\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Randon forest algorithm accuracy_score: {:.3f}.\".format(accuracy_score(y_test, y_pred)))","dbd2bc8f":"# Random Forest Algorithm | Optimization\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'n_estimators': [100, 150, 200],\n    'max_depth': [14, 16, 18],\n    'min_samples_split': [100, 200],\n    'min_samples_leaf': [25, 50],\n    'bootstrap': [False]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(cv=5, estimator = clf, param_grid = param_grid, scoring='balanced_accuracy', n_jobs = -1,verbose = 5)\n\n# Fit the grid search to the Validation Dataset\ngrid_search.fit(X_val[:20000], y_val[:20000])\n\n# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","154dd9e6":"# Random Forest Algorithm | Final Evaluation\n\n# Create a Random Forest Classifier\nclf=RandomForestClassifier(**grid_search.best_params_, n_jobs=-1, random_state=42)\n\n# Train & Test\nclf.fit(X_train, y_train)\ny_train_pred= clf.predict(X_train)\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# Highlighting the significance of each of the factors in the model\nfeature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\nprint(\"\\nImportant features:\\n\", feature_imp.sort_values(ascending=False)[:10])\n\n# stroring the accuracy score\nresult['Random Forest'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","24273d25":"# K-Nearest Neighbor | First Iteration\n\n# Create a k-NN classifier\nclf = KNeighborsClassifier(n_jobs=-1)\n\n# Train & Test\nclf.fit(X_train[:20000], y_train[:20000])\ny_train_pred= clf.predict(X_train[:20000])\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train[:20000], y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# stroring the accuracy score\nresult['K-Nearest Neighbors'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","b3be6365":"# Neural Network | First Iteration\n\nmodel = Sequential()\nmodel.add(Dense(128, input_dim=np.size(X_train,1), activation='relu'))\nmodel.add(Dense(64, input_dim=np.size(X_train,1), activation='relu'))\nmodel.add(Dense(5, activation='softmax'))\n\n# compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# build the model\nhistory = model.fit(X_train, to_categorical(y_train.to_numpy()), \n                    epochs=5, validation_data=(X_val, to_categorical(y_val.to_numpy())), \n                    validation_steps=30, verbose=0)\n\n\nloss, train_accuracy = model.evaluate(X_train, to_categorical(y_train.to_numpy()), verbose=0)\nprint(f\"\\nFor Training Dataset: Loss: {loss} and Accuracy: {train_accuracy}\")\n\nloss, test_accuracy = model.evaluate(X_test, to_categorical(y_test.to_numpy()), verbose=0)\nprint(f\"\\nFor Testing Dataset: Loss: {loss} and Accuracy: {test_accuracy}\")\n\n# stroring the accuracy score\nresult['Neural Network'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","0430bc75":"# Saving the results in file\n\ndf = pd.DataFrame.from_dict(result)\ndf.set_index(['State'])\ndf.to_csv(f'\/kaggle\/working\/result_{state}.csv', index=False)","805f0e9f":"# Deleting all data\n%reset -f\n\n# Reloading necessary libraries\n# import numpy and pandas\nimport pandas as pd\nimport numpy as np\n\n# import for pre-processing\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\n\n# import for visualization\nimport matplotlib.pyplot as plt\n\n# import for model building\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# import for Neural Network based model building\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","01d0394b":"# Building State specific model | State 'FL'\n\nresult = {}\nstate='FL'\nresult['State']=state\nprocessed_data = pd.read_csv(f'\/kaggle\/working\/data_{state}.csv').dropna()\ncols = processed_data.select_dtypes(include='object').columns","e8e8be21":"# Class Balancing | Using Up Sampling\n\n# Separate majority and minority classes\ndf_s1 = processed_data[processed_data['Severity']==1]\ndf_s2 = processed_data[processed_data['Severity']==2]\ndf_s3 = processed_data[processed_data['Severity']==3]\ndf_s4 = processed_data[processed_data['Severity']==4]\n\ncount = max(df_s1.count()[0], df_s2.count()[0], df_s3.count()[0], df_s4.count()[0])\n\n# Upsample minority class\ndf_s1 = resample(df_s1, replace=df_s1.count()[0]<count, n_samples=count, random_state=42)\ndf_s2 = resample(df_s2, replace=df_s2.count()[0]<count, n_samples=count, random_state=42)\ndf_s3 = resample(df_s3, replace=df_s3.count()[0]<count, n_samples=count, random_state=42)\ndf_s4 = resample(df_s4, replace=df_s4.count()[0]<count, n_samples=count, random_state=42)\n \n# Combine majority class with upsampled minority class\nprocessed_data = pd.concat([df_s1, df_s2, df_s3, df_s4])\n \n# Display new class counts\nprocessed_data.groupby(by='Severity')['Severity'].count()","1e61401f":"# Set the target for the prediction\ntarget='Severity' \n\n# set X and y\ny = processed_data[target]\nX = processed_data.drop(target, axis=1)\n\n# Create the encoder.\nencoder = OrdinalEncoder()\nX[cols] = encoder.fit_transform(X[cols])\n\n# Split the data set into training and testing data sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n\n# Split the data set into training and validation data sets\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, stratify=y_train, random_state=42)\n\n# Scalling the features of Train Dataset, Validation Dataset and Test Dataset\nscaler = StandardScaler()\n\n# Scaling Train Dataset\nscaler = scaler.fit(X_train)\nX_train = scaler.transform(X_train)\n\n# Scaling Validation Dataset\nscaler = scaler.fit(X_val)\nX_val = scaler.transform(X_val)\n\n# Scaling Test Dataset\nscaler = scaler.fit(X_test)\nX_test = scaler.transform(X_test)","722503ab":"BBox = ((processed_data.Start_Lng.min(), processed_data.Start_Lng.max(), processed_data.Start_Lat.min(), processed_data.Start_Lat.max()))\nBBox","f0f7ba60":"map_pic = plt.imread('\/kaggle\/working\/map\/map_pic_fl.png')","7441a102":"fig, ax = plt.subplots(figsize = (30,30))\nax.scatter(processed_data[processed_data['Severity']==1].Start_Lng, processed_data[processed_data['Severity']==1].Start_Lat, zorder=1, c='b', s=4)\nax.scatter(processed_data[processed_data['Severity']==2].Start_Lng, processed_data[processed_data['Severity']==2].Start_Lat, zorder=1, c='g', s=6)\nax.scatter(processed_data[processed_data['Severity']==3].Start_Lng, processed_data[processed_data['Severity']==3].Start_Lat, zorder=1, c='y', s=8)\nax.scatter(processed_data[processed_data['Severity']==4].Start_Lng, processed_data[processed_data['Severity']==4].Start_Lat, zorder=1, c='r', s=10)\n\nax.set_xlim(BBox[0],BBox[1])\nax.set_ylim(BBox[2],BBox[3])\nax.imshow(map_pic, zorder=0, extent = BBox, aspect= 'auto', interpolation='none')\nax.imshow(map_pic, zorder=2, alpha= 0.5, extent = BBox, aspect= 'auto', interpolation='lanczos')","d21f6a7d":"# Support Vector Machine | First Iteration\n\n# Instantiate an object of class SVC()\nclf = SVC(gamma='auto', kernel='rbf', random_state=42)\n\n# Train & Test (limiting rows since SVM takes much time)\nclf.fit(X_train[:10000], y_train[:10000])\ny_pred = clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Support Vector Machine accuracy_score: {:.3f}.\".format(accuracy_score(y_test, y_pred)))","3993f9c5":"# Support Vector Machine | Optimization\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'C': [0.1, 0.5, 1],\n    'gamma': ['auto', 'scale']\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(cv=5, estimator = clf, param_grid = param_grid, scoring='balanced_accuracy', n_jobs = -1,verbose = 5)\n\n# Fit the grid search to the Validation Dataset\ngrid_search.fit(X_val[:5000], y_val[:5000])\n\n# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","789d2c38":"# Support Vector Machine | Final Evaluation\n\n# Create a SVM Classifier\nclf=SVC(**grid_search.best_params_, random_state=42)\n\n# Train & Test\nclf.fit(X_train[:20000], y_train[:20000])\ny_train_pred= clf.predict(X_train[:20000])\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train[:20000], y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# stroring the accuracy score\nresult['Support Vector Machine'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","d9110d0a":"# Decision Tree Algorithm | First Iteration\n\n# Instantiate a Decision Tree Classifier\nclf = DecisionTreeClassifier(random_state=42)\n\n# Train & Test\nclf.fit(X_train, y_train)\ny_pred= clf.predict(X_test)\n\n# Print accuracy_entropy\nprint('Decision Tree accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_pred)))","c504ffeb":"# Decision Tree Algorithm | Optimization\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [12, 14, 16],\n    'min_samples_split': [2000, 4000],\n    'min_samples_leaf': [1000, 2000]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(cv=3, estimator = clf, param_grid = param_grid, scoring='balanced_accuracy', n_jobs = -1,verbose = 5)\n\n# Fit the grid search to the Validation Dataset\ngrid_search.fit(X_val, y_val)\n\n# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","d1dff802":"# Decision Tree Algorithm | Final Evaluation\n\n# Instantiate a Decision Tree Classifier with Best Parameters\nclf = DecisionTreeClassifier(**grid_search.best_params_, random_state=42)\n\n# Train & Test\nclf.fit(X_train, y_train)\ny_train_pred= clf.predict(X_train)\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# Highlighting the significance of each of the factors in the model\nfeature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\nprint(\"\\nImportant features:\\n\", feature_imp.sort_values(ascending=False)[:10])\n\n# stroring the accuracy score\nresult['Decision Tree'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","7dc86fdd":"# Random Forest Algorithm | First Iteration\n\n# Create a Random Forest Classifier\nclf=RandomForestClassifier(n_estimators=100, min_samples_split=400, min_samples_leaf=100, n_jobs=-1, random_state=42)\n\n# Train & Test\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Randon forest algorithm accuracy_score: {:.3f}.\".format(accuracy_score(y_test, y_pred)))","e881ff09":"# Random Forest Algorithm | Optimization\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'n_estimators': [100, 150, 200],\n    'max_depth': [14, 16, 18],\n    'min_samples_split': [100, 200],\n    'min_samples_leaf': [25, 50],\n    'bootstrap': [False]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(cv=5, estimator = clf, param_grid = param_grid, scoring='balanced_accuracy', n_jobs = -1,verbose = 5)\n\n# Fit the grid search to the Validation Dataset\ngrid_search.fit(X_val[:20000], y_val[:20000])\n\n# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","4dc3a21e":"# Random Forest Algorithm | Final Evaluation\n\n# Create a Random Forest Classifier\nclf=RandomForestClassifier(**grid_search.best_params_, n_jobs=-1, random_state=42)\n\n# Train & Test\nclf.fit(X_train, y_train)\ny_train_pred= clf.predict(X_train)\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# Highlighting the significance of each of the factors in the model\nfeature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\nprint(\"\\nImportant features:\\n\", feature_imp.sort_values(ascending=False)[:10])\n\n# stroring the accuracy score\nresult['Random Forest'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","468df8f2":"# K-Nearest Neighbor | First Iteration\n\n# Create a k-NN classifier\nclf = KNeighborsClassifier(n_jobs=-1)\n\n# Train & Test\nclf.fit(X_train[:20000], y_train[:20000])\ny_train_pred= clf.predict(X_train[:20000])\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train[:20000], y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# stroring the accuracy score\nresult['K-Nearest Neighbors'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","5cf7692a":"# Neural Network | First Iteration\n\nmodel = Sequential()\nmodel.add(Dense(128, input_dim=np.size(X_train,1), activation='relu'))\nmodel.add(Dense(64, input_dim=np.size(X_train,1), activation='relu'))\nmodel.add(Dense(5, activation='softmax'))\n\n# compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# build the model\nhistory = model.fit(X_train, to_categorical(y_train.to_numpy()), \n                    epochs=5, validation_data=(X_val, to_categorical(y_val.to_numpy())), \n                    validation_steps=30, verbose=0)\n\n\nloss, train_accuracy = model.evaluate(X_train, to_categorical(y_train.to_numpy()), verbose=0)\nprint(f\"\\nFor Training Dataset: Loss: {loss} and Accuracy: {train_accuracy}\")\n\nloss, test_accuracy = model.evaluate(X_test, to_categorical(y_test.to_numpy()), verbose=0)\nprint(f\"\\nFor Testing Dataset: Loss: {loss} and Accuracy: {test_accuracy}\")\n\n# stroring the accuracy score\nresult['Neural Network'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","96e005b7":"# Saving the results in file\n\ndf = pd.DataFrame.from_dict(result)\ndf.set_index(['State'])\ndf.to_csv(f'\/kaggle\/working\/result_{state}.csv', index=False)","9417a961":"# Deleting all data\n%reset -f\n\n# Reloading necessary libraries\n# import numpy and pandas\nimport pandas as pd\nimport numpy as np\n\n# import for pre-processing\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\n\n# import for visualization\nimport matplotlib.pyplot as plt\n\n# import for model building\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# import for Neural Network based model building\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","6afd41a4":"# Building State specific model | State 'SC'\n\nresult = {}\nstate='SC'\nresult['State']=state\nprocessed_data = pd.read_csv(f'\/kaggle\/working\/data_{state}.csv').dropna()\ncols = processed_data.select_dtypes(include='object').columns","f090dc8d":"# Class Balancing | Using Up Sampling\n\n# Separate majority and minority classes\ndf_s1 = processed_data[processed_data['Severity']==1]\ndf_s2 = processed_data[processed_data['Severity']==2]\ndf_s3 = processed_data[processed_data['Severity']==3]\ndf_s4 = processed_data[processed_data['Severity']==4]\n\ncount = max(df_s1.count()[0], df_s2.count()[0], df_s3.count()[0], df_s4.count()[0])\n\n# Upsample minority class\ndf_s1 = resample(df_s1, replace=df_s1.count()[0]<count, n_samples=count, random_state=42)\ndf_s2 = resample(df_s2, replace=df_s2.count()[0]<count, n_samples=count, random_state=42)\ndf_s3 = resample(df_s3, replace=df_s3.count()[0]<count, n_samples=count, random_state=42)\ndf_s4 = resample(df_s4, replace=df_s4.count()[0]<count, n_samples=count, random_state=42)\n \n# Combine majority class with upsampled minority class\nprocessed_data = pd.concat([df_s1, df_s2, df_s3, df_s4])\n \n# Display new class counts\nprocessed_data.groupby(by='Severity')['Severity'].count()","2583b0f0":"# Set the target for the prediction\ntarget='Severity' \n\n# set X and y\ny = processed_data[target]\nX = processed_data.drop(target, axis=1)\n\n# Create the encoder.\nencoder = OrdinalEncoder()\nX[cols] = encoder.fit_transform(X[cols])\n\n# Split the data set into training and testing data sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n\n# Split the data set into training and validation data sets\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, stratify=y_train, random_state=42)\n\n# Scalling the features of Train Dataset, Validation Dataset and Test Dataset\nscaler = StandardScaler()\n\n# Scaling Train Dataset\nscaler = scaler.fit(X_train)\nX_train = scaler.transform(X_train)\n\n# Scaling Validation Dataset\nscaler = scaler.fit(X_val)\nX_val = scaler.transform(X_val)\n\n# Scaling Test Dataset\nscaler = scaler.fit(X_test)\nX_test = scaler.transform(X_test)","9d7f13b7":"BBox = ((processed_data.Start_Lng.min(), processed_data.Start_Lng.max(), processed_data.Start_Lat.min(), processed_data.Start_Lat.max()))\nBBox","dad95362":"map_pic = plt.imread('\/kaggle\/working\/map\/map_pic_sc.png')","bc05659d":"fig, ax = plt.subplots(figsize = (34,26))\nax.scatter(processed_data[processed_data['Severity']==1].Start_Lng, processed_data[processed_data['Severity']==1].Start_Lat-0.01, zorder=1, c='b', s=4)\nax.scatter(processed_data[processed_data['Severity']==2].Start_Lng, processed_data[processed_data['Severity']==2].Start_Lat-0.01, zorder=1, c='g', s=6)\nax.scatter(processed_data[processed_data['Severity']==3].Start_Lng, processed_data[processed_data['Severity']==3].Start_Lat-0.01, zorder=1, c='y', s=8)\nax.scatter(processed_data[processed_data['Severity']==4].Start_Lng, processed_data[processed_data['Severity']==4].Start_Lat-0.01, zorder=1, c='r', s=10)\n\nax.set_xlim(BBox[0],BBox[1])\nax.set_ylim(BBox[2],BBox[3])\nax.imshow(map_pic, zorder=0, extent = BBox, aspect= 'auto', interpolation='none')\nax.imshow(map_pic, zorder=2, alpha= 0.5, extent = BBox, aspect= 'auto', interpolation='lanczos')","5b7ff0fc":"# Support Vector Machine | First Iteration\n\n# Instantiate an object of class SVC()\nclf = SVC(gamma='auto', kernel='rbf', random_state=42)\n\n# Train & Test (limiting rows since SVM takes much time)\nclf.fit(X_train[:10000], y_train[:10000])\ny_pred = clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Support Vector Machine accuracy_score: {:.3f}.\".format(accuracy_score(y_test, y_pred)))","a3f7a7f9":"# Support Vector Machine | Optimization\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'C': [0.1, 0.5, 1],\n    'gamma': ['auto', 'scale']\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(cv=5, estimator = clf, param_grid = param_grid, scoring='balanced_accuracy', n_jobs = -1,verbose = 5)\n\n# Fit the grid search to the Validation Dataset\ngrid_search.fit(X_val[:5000], y_val[:5000])\n\n# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","d28fd719":"# Support Vector Machine | Final Evaluation\n\n# Create a SVM Classifier\nclf=SVC(**grid_search.best_params_, random_state=42)\n\n# Train & Test\nclf.fit(X_train[:20000], y_train[:20000])\ny_train_pred= clf.predict(X_train[:20000])\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train[:20000], y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# stroring the accuracy score\nresult['Support Vector Machine'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","d25d088e":"# Decision Tree Algorithm | First Iteration\n\n# Instantiate a Decision Tree Classifier\nclf = DecisionTreeClassifier(random_state=42)\n\n# Train & Test\nclf.fit(X_train, y_train)\ny_pred= clf.predict(X_test)\n\n# Print accuracy_entropy\nprint('Decision Tree accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_pred)))","735a40eb":"# Decision Tree Algorithm | Optimization\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [12, 14, 16],\n    'min_samples_split': [500, 1000],\n    'min_samples_leaf': [250, 500]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(cv=3, estimator = clf, param_grid = param_grid, scoring='balanced_accuracy', n_jobs = -1,verbose = 5)\n\n# Fit the grid search to the Validation Dataset\ngrid_search.fit(X_val, y_val)\n\n# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","da4e519d":"# Decision Tree Algorithm | Final Evaluation\n\n# Instantiate a Decision Tree Classifier with Best Parameters\nclf = DecisionTreeClassifier(**grid_search.best_params_, random_state=42)\n\n# Train & Test\nclf.fit(X_train, y_train)\ny_train_pred= clf.predict(X_train)\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# Highlighting the significance of each of the factors in the model\nfeature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\nprint(\"\\nImportant features:\\n\", feature_imp.sort_values(ascending=False)[:10])\n\n# stroring the accuracy score\nresult['Decision Tree'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","9396e061":"# Random Forest Algorithm | First Iteration\n\n# Create a Random Forest Classifier\nclf=RandomForestClassifier(n_estimators=100, min_samples_split=400, min_samples_leaf=100, n_jobs=-1, random_state=42)\n\n# Train & Test\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Randon forest algorithm accuracy_score: {:.3f}.\".format(accuracy_score(y_test, y_pred)))","e769886c":"# Random Forest Algorithm | Optimization\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'n_estimators': [100, 150, 200],\n    'max_depth': [12, 14, 16],\n    'min_samples_split': [100, 200],\n    'min_samples_leaf': [25, 50],\n    'bootstrap': [False]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(cv=5, estimator = clf, param_grid = param_grid, scoring='balanced_accuracy', n_jobs = -1,verbose = 5)\n\n# Fit the grid search to the Validation Dataset\ngrid_search.fit(X_val[:20000], y_val[:20000])\n\n# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","3e2c21e2":"# Random Forest Algorithm | Final Evaluation\n\n# Create a Random Forest Classifier\nclf=RandomForestClassifier(**grid_search.best_params_, n_jobs=-1, random_state=42)\n\n# Train & Test\nclf.fit(X_train, y_train)\ny_train_pred= clf.predict(X_train)\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# Highlighting the significance of each of the factors in the model\nfeature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\nprint(\"\\nImportant features:\\n\", feature_imp.sort_values(ascending=False)[:10])\n\n# stroring the accuracy score\nresult['Random Forest'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","9f7b5c4e":"# K-Nearest Neighbor | First Iteration\n\n# Create a k-NN classifier\nclf = KNeighborsClassifier(n_jobs=-1)\n\n# Train & Test\nclf.fit(X_train[:20000], y_train[:20000])\ny_train_pred= clf.predict(X_train[:20000])\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train[:20000], y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# stroring the accuracy score\nresult['K-Nearest Neighbors'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","8a5a37e0":"# Neural Network | First Iteration\n\nmodel = Sequential()\nmodel.add(Dense(128, input_dim=np.size(X_train,1), activation='relu'))\nmodel.add(Dense(64, input_dim=np.size(X_train,1), activation='relu'))\nmodel.add(Dense(5, activation='softmax'))\n\n# compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# build the model\nhistory = model.fit(X_train, to_categorical(y_train.to_numpy()), \n                    epochs=5, validation_data=(X_val, to_categorical(y_val.to_numpy())), \n                    validation_steps=30, verbose=0)\n\n\nloss, train_accuracy = model.evaluate(X_train, to_categorical(y_train.to_numpy()), verbose=0)\nprint(f\"\\nFor Training Dataset: Loss: {loss} and Accuracy: {train_accuracy}\")\n\nloss, test_accuracy = model.evaluate(X_test, to_categorical(y_test.to_numpy()), verbose=0)\nprint(f\"\\nFor Testing Dataset: Loss: {loss} and Accuracy: {test_accuracy}\")\n\n# stroring the accuracy score\nresult['Neural Network'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","6346fa9a":"# Saving the results in file\n\ndf = pd.DataFrame.from_dict(result)\ndf.set_index(['State'])\ndf.to_csv(f'\/kaggle\/working\/result_{state}.csv', index=False)","00bea854":"# Deleting all data\n%reset -f\n\n# Reloading necessary libraries\n# import numpy and pandas\nimport pandas as pd\nimport numpy as np\n\n# import for pre-processing\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\n\n# import for visualization\nimport matplotlib.pyplot as plt\n\n# import for model building\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# import for Neural Network based model building\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","48c89fca":"# Building State specific model | State 'NC'\n\nresult = {}\nstate='NC'\nresult['State']=state\nprocessed_data = pd.read_csv(f'\/kaggle\/working\/data_{state}.csv').dropna()\ncols = processed_data.select_dtypes(include='object').columns","f6014c2a":"# Class Balancing | Using Up Sampling\n\n# Separate majority and minority classes\ndf_s1 = processed_data[processed_data['Severity']==1]\ndf_s2 = processed_data[processed_data['Severity']==2]\ndf_s3 = processed_data[processed_data['Severity']==3]\ndf_s4 = processed_data[processed_data['Severity']==4]\n\ncount = max(df_s1.count()[0], df_s2.count()[0], df_s3.count()[0], df_s4.count()[0])\n\n# Upsample minority class\ndf_s1 = resample(df_s1, replace=df_s1.count()[0]<count, n_samples=count, random_state=42)\ndf_s2 = resample(df_s2, replace=df_s2.count()[0]<count, n_samples=count, random_state=42)\ndf_s3 = resample(df_s3, replace=df_s3.count()[0]<count, n_samples=count, random_state=42)\ndf_s4 = resample(df_s4, replace=df_s4.count()[0]<count, n_samples=count, random_state=42)\n \n# Combine majority class with upsampled minority class\nprocessed_data = pd.concat([df_s1, df_s2, df_s3, df_s4])\n \n# Display new class counts\nprocessed_data.groupby(by='Severity')['Severity'].count()","c91bf6a2":"# Set the target for the prediction\ntarget='Severity' \n\n# set X and y\ny = processed_data[target]\nX = processed_data.drop(target, axis=1)\n\n# Create the encoder.\nencoder = OrdinalEncoder()\nX[cols] = encoder.fit_transform(X[cols])\n\n# Split the data set into training and testing data sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n\n# Split the data set into training and validation data sets\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, stratify=y_train, random_state=42)\n\n# Scalling the features of Train Dataset, Validation Dataset and Test Dataset\nscaler = StandardScaler()\n\n# Scaling Train Dataset\nscaler = scaler.fit(X_train)\nX_train = scaler.transform(X_train)\n\n# Scaling Validation Dataset\nscaler = scaler.fit(X_val)\nX_val = scaler.transform(X_val)\n\n# Scaling Test Dataset\nscaler = scaler.fit(X_test)\nX_test = scaler.transform(X_test)","9a4d8397":"BBox = ((processed_data.Start_Lng.min(), processed_data.Start_Lng.max(), processed_data.Start_Lat.min(), processed_data.Start_Lat.max()))\nBBox","9cf1234b":"map_pic = plt.imread('\/kaggle\/working\/map\/map_pic_nc.png')","660cdf43":"fig, ax = plt.subplots(figsize = (31,12))\nax.scatter(processed_data[processed_data['Severity']==1].Start_Lng, processed_data[processed_data['Severity']==1].Start_Lat, zorder=1, c='b', s=4)\nax.scatter(processed_data[processed_data['Severity']==2].Start_Lng, processed_data[processed_data['Severity']==2].Start_Lat, zorder=1, c='g', s=6)\nax.scatter(processed_data[processed_data['Severity']==3].Start_Lng, processed_data[processed_data['Severity']==3].Start_Lat, zorder=1, c='y', s=8)\nax.scatter(processed_data[processed_data['Severity']==4].Start_Lng, processed_data[processed_data['Severity']==4].Start_Lat, zorder=1, c='r', s=10)\n\nax.set_xlim(BBox[0],BBox[1])\nax.set_ylim(BBox[2],BBox[3])\nax.imshow(map_pic, zorder=0, extent = BBox, aspect= 'auto', interpolation='none')\nax.imshow(map_pic, zorder=2, alpha= 0.5, extent = BBox, aspect= 'auto', interpolation='lanczos')","31819b48":"# Support Vector Machine | First Iteration\n\n# Instantiate an object of class SVC()\nclf = SVC(gamma='auto', kernel='rbf', random_state=42)\n\n# Train & Test (limiting rows since SVM takes much time)\nclf.fit(X_train[:10000], y_train[:10000])\ny_pred = clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Support Vector Machine accuracy_score: {:.3f}.\".format(accuracy_score(y_test, y_pred)))","ee2d9099":"# Support Vector Machine | Optimization\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'C': [0.1, 0.5, 1],\n    'gamma': ['auto', 'scale']\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(cv=5, estimator = clf, param_grid = param_grid, scoring='balanced_accuracy', n_jobs = -1,verbose = 5)\n\n# Fit the grid search to the Validation Dataset\ngrid_search.fit(X_val[:5000], y_val[:5000])\n\n# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","cdc36ca8":"# Support Vector Machine | Final Evaluation\n\n# Create a SVM Classifier\nclf=SVC(**grid_search.best_params_, random_state=42)\n\n# Train & Test\nclf.fit(X_train[:20000], y_train[:20000])\ny_train_pred= clf.predict(X_train[:20000])\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train[:20000], y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# stroring the accuracy score\nresult['Support Vector Machine'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","3aa0b02e":"# Decision Tree Algorithm | First Iteration\n\n# Instantiate a Decision Tree Classifier\nclf = DecisionTreeClassifier(random_state=42)\n\n# Train & Test\nclf.fit(X_train, y_train)\ny_pred= clf.predict(X_test)\n\n# Print accuracy_entropy\nprint('Decision Tree accuracy_score: {:.3f}.'.format(accuracy_score(y_test, y_pred)))","b19cdc53":"# Decision Tree Algorithm | Optimization\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [12, 14, 16],\n    'min_samples_split': [1000, 2000],\n    'min_samples_leaf': [500, 1000]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(cv=3, estimator = clf, param_grid = param_grid, scoring='balanced_accuracy', n_jobs = -1,verbose = 5)\n\n# Fit the grid search to the Validation Dataset\ngrid_search.fit(X_val, y_val)\n\n# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","89833379":"# Decision Tree Algorithm | Final Evaluation\n\n# Instantiate a Decision Tree Classifier with Best Parameters\nclf = DecisionTreeClassifier(**grid_search.best_params_, random_state=42)\n\n# Train & Test\nclf.fit(X_train, y_train)\ny_train_pred= clf.predict(X_train)\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# Highlighting the significance of each of the factors in the model\nfeature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\nprint(\"\\nImportant features:\\n\", feature_imp.sort_values(ascending=False)[:10])\n\n# stroring the accuracy score\nresult['Decision Tree'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","af2a8f75":"# Random Forest Algorithm | First Iteration\n\n# Create a Random Forest Classifier\nclf=RandomForestClassifier(n_estimators=100, min_samples_split=400, min_samples_leaf=100, n_jobs=-1, random_state=42)\n\n# Train & Test\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Randon forest algorithm accuracy_score: {:.3f}.\".format(accuracy_score(y_test, y_pred)))","6e99c17b":"# Random Forest Algorithm | Optimization\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'n_estimators': [100, 150, 200],\n    'max_depth': [12, 14, 16],\n    'min_samples_split': [100, 200],\n    'min_samples_leaf': [25, 50],\n    'bootstrap': [False]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(cv=5, estimator = clf, param_grid = param_grid, scoring='balanced_accuracy', n_jobs = -1,verbose = 5)\n\n# Fit the grid search to the Validation Dataset\ngrid_search.fit(X_val[:20000], y_val[:20000])\n\n# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","7eb6d8d8":"# Random Forest Algorithm | Final Evaluation\n\n# Create a Random Forest Classifier\nclf=RandomForestClassifier(**grid_search.best_params_, n_jobs=-1, random_state=42)\n\n# Train & Test\nclf.fit(X_train, y_train)\ny_train_pred= clf.predict(X_train)\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# Highlighting the significance of each of the factors in the model\nfeature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\nprint(\"\\nImportant features:\\n\", feature_imp.sort_values(ascending=False)[:10])\n\n# stroring the accuracy score\nresult['Random Forest'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","89b08c81":"# K-Nearest Neighbor | First Iteration\n\n# Create a k-NN classifier\nclf = KNeighborsClassifier(n_jobs=-1)\n\n# Train & Test\nclf.fit(X_train[:20000], y_train[:20000])\ny_train_pred= clf.predict(X_train[:20000])\ny_test_pred= clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\n# Detailed report of classification done by model\n\ntrain_accuracy, test_accuracy = accuracy_score(y_train[:20000], y_train_pred), accuracy_score(y_test, y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(f'Accuracy for the train dataset {train_accuracy:.1%}')\nprint(f'Accuracy for the test dataset {test_accuracy:.1%}')\n\n# stroring the accuracy score\nresult['K-Nearest Neighbors'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","82d71076":"# Neural Network | First Iteration\n\nmodel = Sequential()\nmodel.add(Dense(128, input_dim=np.size(X_train,1), activation='relu'))\nmodel.add(Dense(64, input_dim=np.size(X_train,1), activation='relu'))\nmodel.add(Dense(5, activation='softmax'))\n\n# compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# build the model\nhistory = model.fit(X_train, to_categorical(y_train.to_numpy()), \n                    epochs=5, validation_data=(X_val, to_categorical(y_val.to_numpy())), \n                    validation_steps=30, verbose=0)\n\n\nloss, train_accuracy = model.evaluate(X_train, to_categorical(y_train.to_numpy()), verbose=0)\nprint(f\"\\nFor Training Dataset: Loss: {loss} and Accuracy: {train_accuracy}\")\n\nloss, test_accuracy = model.evaluate(X_test, to_categorical(y_test.to_numpy()), verbose=0)\nprint(f\"\\nFor Testing Dataset: Loss: {loss} and Accuracy: {test_accuracy}\")\n\n# stroring the accuracy score\nresult['Neural Network'] = ['Train: '+str(round(train_accuracy*100, 1))+', Test: '+str(round(test_accuracy*100,1))]","7063a5fd":"# Saving the results in file\n\ndf = pd.DataFrame.from_dict(result)\ndf.set_index(['State'])\ndf.to_csv(f'\/kaggle\/working\/result_{state}.csv', index=False)","6b755774":"df = pd.concat([\n    pd.read_csv('\/kaggle\/working\/result_CA.csv'),pd.read_csv('\/kaggle\/working\/result_TX.csv'),\n    pd.read_csv('\/kaggle\/working\/result_FL.csv'),pd.read_csv('\/kaggle\/working\/result_SC.csv'),\n    pd.read_csv('\/kaggle\/working\/result_NC.csv')]).set_index(['State'])\ndf","570d3128":"#### BUILDING MODEL USING DECISION TREE","bea97809":"#### Extracting 'Keyword' from 'Description' attribute","23b00723":"Analysis and Prediction of Severity of Traffic Accident\n`US Accident Traffic Severity Assessment`","328d1347":"**Summary**: We are getting decent accuracy with SVM, but, the computation time is very high, even with limited dataset.","ff836907":"### Miscellaneous Plots","04447609":"___","05f0f015":"#### Deriving the attribute 'Time_Duration(min)'","b1c86109":"#### BUILDING MODEL USING SUPPORT VECTOR MACHINE","c0b90fec":"#### BUILDING MODEL USING NEURAL NETWORK","0a69731b":"## Table of Content","1e83d7e2":"**Summary**: We are getting decent accuracy with SVM, but, the computation time is very high, even with limited dataset.","05b7c8cc":"#### BUILDING MODEL USING K-NEAREST NEIGHBOR (KNN)","dffa0ca7":"#### BUILDING MODEL USING K-NEAREST NEIGHBOR (KNN)","47703a98":"### Building ML Model for State 'FL'","770297d5":"### Building ML Model for State 'SC'","7f8a2e9e":"___","a002f2fb":"### Feature Engineering","530ed610":"### Importing Libraries","1d7e770e":"#### BUILDING MODEL USING NEURAL NETWORK","626490f2":"#### BUILDING MODEL USING RANDOM FOREST","9a661d47":"### Importing Libraries for Exploratory Data Analysis","feeb5c4d":"**Summary**: We are getting decent accuracy with Neural Network and computation time is also comparatively less.","01c69a55":"### Importing Libraries for Data Pre-processing","b2de42b5":"#### EVALUATING ADDITIONAL ALGORITHM'S PERFORMANCE","20a64fc4":"___","939af001":"#### EVALUATING ADDITIONAL ALGORITHM'S PERFORMANCE","b65f67c5":"**Summary**: We are getting decent accuracy with Decision Tree algorithm and computation time is also comparatively less.","8d87f85d":"**Summary**: We are getting decent accuracy with Neural Network and computation time is also comparatively less.","db27ff2e":"**Summary**: We are getting good accuracy with Random Forest algorithm and computation time is also comparatively less.","7ea71554":"**Summary**: We are getting decent accuracy with Neural Network and computation time is also comparatively less.","c495ea46":"It is important to note that observation of these attributes revealed the following:\n1. 'Wind_Speed(mph)' is missing when the 'Wind_Direction' is calm, thus considering it 0 will be fair.\n2. Other attributes except 'Wind_Speed(mph)' are missing randomly and such records are less than 5% of entire data, thus removing them.","2e75257d":"#### VISUALIZING THE DATA ON MAP","44e6451b":"**Summary**: We are getting decent accuracy with Decision Tree algorithm and computation time is also comparatively less.","4e87b171":"___","a1a95f76":"#### BUILDING MODEL USING RANDOM FOREST","22031d08":"It is important to note that observation of these attributes revealed the following:\n1. 'TMC' is one of the important attribute which is communicated by the authorities, hence we would not delete it.\n2. 'End_Lat' and 'End_Lng' are missing when the distance of road affected by accident is very small, thus, Start and End location would be almost same and End could be removed.\n3. 'Number', 'Wind_Chill(F)' and 'Precipitation(in)' can be removed due to high percentage of missing values.","7c5821b6":"### Building ML Model for State 'NC'","e0db643b":"### Importing Libraries","689d7e75":"#### VISUALIZING THE DATA ON MAP","f90bacec":"**Summary**: We are getting decent accuracy with SVM, but, the computation time is very high, even with limited dataset.","3e2ed49b":"#### EVALUATING ADDITIONAL ALGORITHM'S PERFORMANCE","4b607557":"#### Addressing Dataset Validity","4147418c":"#### Addressing Dataset Consistency","d65d1b29":"#### BUILDING MODEL USING SUPPORT VECTOR MACHINE","556c1814":"**Summary**: We are getting decent accuracy with Decision Tree algorithm and computation time is also comparatively less.","29cd039c":"## Model Building and Evaluation","b6dca726":"#### BUILDING MODEL USING NEURAL NETWORK","c13ae716":"**Summary**: We are getting good accuracy with Random Forest algorithm and computation time is also comparatively less.","146e05a8":"**Summary**: We are getting poor accuracy with K-Nearest Neighbor algorithm and the computation time is very high, even with limited dataset.","a5f6d11f":"___","e1bfe300":"#### BUILDING MODEL USING SUPPORT VECTOR MACHINE","14d6ad99":"___","e44b984b":"#### BUILDING MODEL USING DECISION TREE","a0dcdc9a":"#### BUILDING MODEL USING SUPPORT VECTOR MACHINE","1b96c18a":"### Data Summarization","6d6b4719":"### Multivariate Analysis","7d4885d6":"#### BUILDING MODEL USING K-NEAREST NEIGHBOR (KNN)","c01f6a55":"### Selecting Columns","56565bc3":"**Summary**: We are getting decent accuracy with Decision Tree algorithm and computation time is also comparatively less.","bcf0324d":"### Importing Libraries","7db2e7cb":"___","faf42201":"#### Addressing Dataset Accuracy","da9a294e":"#### BUILDING MODEL USING SUPPORT VECTOR MACHINE","c069057d":"#### EVALUATING ADDITIONAL ALGORITHM'S PERFORMANCE","a49bc519":"## Data Preparation","1efd50e7":"#### EVALUATING ADDITIONAL ALGORITHM'S PERFORMANCE","4b19a12c":"#### Splitting the 'Start_Time' timestamp to 'Year', 'Month', 'Day', 'Hour' and 'Weekend'","4b3ea23f":"#### BUILDING MODEL USING RANDOM FOREST","c8244e24":"#### Dropping unnecessary attributes","4c8a9e43":"#### BUILDING MODEL USING NEURAL NETWORK","d6f86cd5":"### Building ML Model for State 'TX'","dc67c8e4":"**Summary**: We are getting poor accuracy with K-Nearest Neighbor algorithm and the computation time is very high, even with limited dataset.","c2ede664":"#### BUILDING MODEL USING RANDOM FOREST","df04a0cf":"#### BUILDING MODEL USING DECISION TREE","12fac570":"----","933e2f7d":"**Summary**: We are getting poor accuracy with K-Nearest Neighbor algorithm and the computation time is very high, even with limited dataset.","e4346639":"## Combined Results of Models on Datasets of States","79806b16":"#### BUILDING MODEL USING K-NEAREST NEIGHBOR (KNN)","af0aa549":"### Outlier Treatment","011956ef":"**Summary**: We are getting decent accuracy with Neural Network and computation time is also comparatively less.","74d93f19":"### Importing Libraries","e5648f32":"### Data Cleaning","7f0e6d8b":"### Importing Libraries","21479767":"### Summary Statistics","d0ab6266":"### Bivariate Analysis","f49d3aca":"**Summary**: We are getting decent accuracy with Neural Network and computation time is also comparatively less.","7d33db69":"### Data Loading","2459854a":"### Building ML Model for State 'CA'","c003757c":"#### BUILDING MODEL USING RANDOM FOREST","c5445506":"1. [Data Preparation](#Data-Preparation \"Goto Data Preparation Section\")\n   - [Importing Libraries](#Importing-Libraries \"Goto Importing Libraries Sub-Section\")\n   - [Data Loading](#Data-Loading \"Goto Data Loading Sub-Section\")\n   - [Data Summarization](#Data-Summarization \"Goto Data Summarization Sub-Section\")\n   - [Data Cleaning](#Data-Cleaning \"Goto Data Cleaning Sub-Section\")\n   - [Feature Engineering](#Feature-Engineering \"Goto Feature Engineering Sub-Section\")\n   - [Outlier Treatment](#Outlier-Treatment \"Goto Outlier Treatment Sub-Section\")\n   \n   \n2. [Exploratory Data Analysis](#Exploratory-Data-Analysis \"Goto Exploratory Data Analysis Section\")\n   - [Importing Libraries for Exploratory Data Analysis](#Importing-Libraries-for-Exploratory-Data-Analysis \"Goto Importing Libraries for Exploratory Data Analysis Sub-Section\")\n   - [Summary Statistics](#Summary-Statistics \"Goto Summary Statistics Sub-Section\")\n   - [Univariate Analysis](#Univariate-Analysis \"Goto Univariate Analysis Sub-Section\")\n   - [Bivariate Analysis](#Bivariate-Analysis \"Goto Bivariate Analysis Sub-Section\")\n   - [Multivariate Analysis](#Multivariate-Analysis \"Goto Multivariate Analysis Sub-Section\")\n   - [Miscellaneous Plots](#Miscellaneous-Plots \"Goto Miscellaneous Plots Sub-Section\")\n   \n   \n3. [Data Pre-processing](#Data-Pre-processing \"Goto Data Pre-processing Section\")\n   - [Importing Libraries for Data Pre-processing](#Importing-Libraries-for-Data-Pre-processing \"Goto Importing Libraries Sub-Section\")\n   - [Selecting Columns](#Selecting-Columns \"Goto Selecting Columns Sub-Section\")\n   - [Splitting State Specific Data](#Splitting-State-Specific-Data \"Goto Splitting State Specific Data Sub-Section\")\n   \n   \n4. [Model Building and Evaluation](#Model-Building-and-Evaluation \"Goto Model Building and Evaluation Section\")\n   - [Building ML Model for State 'CA'](#Building-ML-Model-for-State-'CA' \"Goto Building ML Model for State 'CA' Sub-Section\")\n   - [Building ML Model for State 'TX'](#Building-ML-Model-for-State-'TX' \"Goto Building ML Model for State 'TX' Sub-Section\")\n   - [Building ML Model for State 'FL'](#Building-ML-Model-for-State-'FL' \"Goto Building ML Model for State 'FL' Sub-Section\")\n   - [Building ML Model for State 'SC'](#Building-ML-Model-for-State-'SC' \"Goto Building ML Model for State 'SC' Sub-Section\")\n   - [Building ML Model for State 'NC'](#Building-ML-Model-for-State-'NC' \"Goto Building ML Model for State 'NC' Sub-Section\")\n   \n   \n5. [Combined Results of Models on Datasets of States](#Combined-Results-of-Models-on-Datasets-of-States \"Goto Combined Results of Models on Datasets of States Section\")","79e1f128":"**Summary**: We are getting decent accuracy with Decision Tree algorithm and computation time is also comparatively less.","e5bd3355":"___","ec221358":"#### BUILDING MODEL USING DECISION TREE","07560483":"#### VISUALIZING THE DATA ON MAP","8b4e5d4a":"**Summary**: We are getting poor accuracy with K-Nearest Neighbor algorithm and the computation time is very high, even with limited dataset.","9467a12c":"**Summary**: We are getting poor accuracy with K-Nearest Neighbor algorithm and the computation time is very high, even with limited dataset.","243d22b4":"### Univariate Analysis","e9a20bba":"___","4d880bf2":"#### BUILDING MODEL USING NEURAL NETWORK","3f3d54bb":"**Summary**: We are getting good accuracy with Random Forest algorithm and computation time is also comparatively less.","62febad6":"**Summary**: We are getting good accuracy with Random Forest algorithm and computation time is also comparatively less.","598fd921":"## Data Pre-Processing","aeb3e81d":"**Summary**: We are getting good accuracy with Random Forest algorithm and computation time is also comparatively less.","6509c7bb":"## Exploratory Data Analysis","ad25e260":"#### Addressing Dataset Completeness","76fe4a64":"### Importing Libraries","4ea51136":"**Summary**: We are getting decent accuracy with SVM, but, the computation time is very high, even with limited dataset.","726964ac":"#### VISUALIZING THE DATA ON MAP","46effca2":"#### VISUALIZING THE DATA ON MAP","f2df78d7":"**Summary**: We are getting decent accuracy with SVM, but, the computation time is very high, even with limited dataset.","13cb7244":"#### BUILDING MODEL USING K-NEAREST NEIGHBOR (KNN)","786e1824":"#### BUILDING MODEL USING DECISION TREE","1729f792":"### Splitting State Specific Data"}}