{"cell_type":{"9bfcbeab":"code","54af584d":"code","af64f0f0":"code","4c05075d":"code","35a9dd6e":"code","5b37c9ee":"code","4a581022":"code","10019541":"code","e449e981":"code","a49bc55a":"code","ba964cee":"code","1bee7ead":"code","9e82f1c8":"code","dbd1d73a":"code","c2d85a46":"code","b642e93a":"code","f12fd94d":"code","26776529":"code","a1489bb7":"markdown","98d8af20":"markdown","d0ff1b62":"markdown","93709932":"markdown"},"source":{"9bfcbeab":"import os\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport cv2\nimport math\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw, ImageEnhance","54af584d":"tf.__version__","af64f0f0":"gpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n  except RuntimeError as e:\n    print(e)","4c05075d":"sample_path = \"\/kaggle\/input\/state-farm-distracted-driver-detection\/sample_submission.csv\"\nimgs_list_path = \"\/kaggle\/input\/state-farm-distracted-driver-detection\/driver_imgs_list.csv\"\ntrain_path = \"\/kaggle\/input\/state-farm-distracted-driver-detection\/imgs\/train\"","35a9dd6e":"driver_imgs_list = pd.read_csv(imgs_list_path)\ndriver_imgs_list.head()","5b37c9ee":"os.listdir(train_path)","4a581022":"def pair_sort(className,values):\n    for j in range(0,len(className)-1):\n        for i in range(0,len(className)-1):\n            if values[i] > values[i+1]:\n                temp =  values[i+1]\n                values[i+1] = values[i]\n                values[i] = temp\n\n                N_temp =  className[i+1]\n                className[i+1] = className[i]\n                className[i] = N_temp\n    \n    return className,values","10019541":"from matplotlib.pyplot import figure\nfigure(num=None, figsize=(15, 5), dpi=80, facecolor='w', edgecolor='k')\n\nclass_names = np.unique(driver_imgs_list['classname'])\nclass_image_list = [len(driver_imgs_list[driver_imgs_list['classname'] == current_class]) for current_class in class_names]\n\nclass_names,class_image_list=  pair_sort(class_names,class_image_list)\n\n#plt.figure()\nplt.suptitle('Number of images per Class')\nplt.bar(class_names,class_image_list,color=(0.2, 0.3, 0.6, 0.6))\nplt.show()","e449e981":"from matplotlib.pyplot import figure\nsub_names = np.unique(driver_imgs_list['subject'])\nsub_image_list = [len(driver_imgs_list[driver_imgs_list['subject'] == current_sub]) for current_sub in sub_names]\nsub_names,sub_image_list=  pair_sort(sub_names,sub_image_list)\n\nfigure(num=None, figsize=(15, 10), dpi=80, facecolor='w', edgecolor='k')\n\ny_pos = np.arange(len(sub_names))\n# Create horizontal bars\nplt.barh(y_pos, sub_image_list,color=(0.2, 0.4, 0.6, 0.6))\n \n# Create names on the y-axis\nplt.yticks(y_pos,sub_names )\nplt.suptitle('Number of images per subject')\n\n# Show graphic\nplt.show()","a49bc55a":"def make_square_test(im):\n    \"\"\"\n    Adds black pixel padding to an image to make it a square.\n    \n    Args:\n        im: A PIL Image\n    \"\"\"\n    x, y = im.size\n    size = max(x, y)\n    new_im = Image.new('RGB', (size, size))\n    new_im.paste(im, (int((size - x) \/ 2), int((size - y) \/ 2)))\n    return new_im","ba964cee":"module = hub.load(\"https:\/\/tfhub.dev\/google\/movenet\/singlepose\/thunder\/3\")\ninput_size = 256\n    \ndef movenet(input_image):\n    \"\"\"Runs detection on an input image.\n\n    Args:\n      input_image: A [1, height, width, 3] tensor represents the input image\n        pixels. Note that the height\/width should already be resized and match the\n        expected input resolution of the model before passing into this function.\n\n    Returns:\n      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n      coordinates and scores.\n    \"\"\"\n    model = module.signatures['serving_default']\n\n    # SavedModel format expects tensor type of int32.\n    input_image = tf.cast(input_image, dtype=tf.int32)\n    # Run model inference.\n    outputs = model(input_image)\n    # Output is a [1, 1, 17, 3] tensor.\n    keypoint_with_scores = outputs['output_0'].numpy()\n    return keypoint_with_scores","1bee7ead":"# The movenet model can be used to take input images of 256x256 pixels\nimg_width,img_height = (256,256)\nmodel_input_shape = (img_width,img_height,3)\nbatch_size = 16\ninput_image = (img_width, img_height)\n\n\ndef load_image(path):\n    read_path = train_path+\"\/\"+path\n    \n    # getting movenet coordinates\n    movenet_image = tf.io.read_file(read_path)\n    movenet_image = tf.image.decode_jpeg(movenet_image)\n    movenet_image = tf.expand_dims(movenet_image, axis=0)\n    movenet_image = tf.image.resize_with_pad(movenet_image, 256, 256)\n    \n    movenet_coordinates = movenet(movenet_image)\n    movenet_coordinates = tf.reshape(movenet_coordinates, [17, 3]).numpy()\n    \n    # Finding the least and most x, y coordinates in which a limb is detected\n    min_x = 1\n    min_y = 1\n    max_x = 0\n    max_y = 0\n    \n    for coord in movenet_coordinates[:11]:\n        if coord[1] < min_x:\n            min_x = coord[1]\n        if coord[0] < min_y:\n            min_y = coord[0]\n        if coord[1] > max_x: \n            max_x = coord[1]\n        if coord[0] > max_y: \n            max_y = coord[0]\n    \n    # loading the image again\n    image = Image.open(read_path)\n    image = make_square_test(image)  \n    \n    width, height = image.size\n    \n    # Uses the extreme end coordinates found earlier to crop the images\n    min_x = min_x - (min_x)*0.2\n    max_x = 1\n    min_y = min_y - (min_y)*0.5\n    max_y = max_y + (1-max_y)*0.1\n    \n    # cropping dimensions\n    left = math.floor(min_x*width)\n    right = math.ceil(max_x*width)\n    top = math.floor(min_y*height)\n    bottom = math.ceil(max_y*height)\n    \n    image = image.crop((left, top, right, bottom))\n    \n    image = make_square_test(image)\n    image = image.resize(input_image)\n    \n    return image","9e82f1c8":"def show_images(image_ids,class_names):\n    pixels = [load_image(path) for path in image_ids]\n    \n    num_of_images = len(image_ids)\n    \n    fig, axes = plt.subplots(\n        1, \n        num_of_images, \n        figsize=(5 * num_of_images, 5 * num_of_images),\n        \n    )\n   \n    \n    for i, image_pixels in enumerate(pixels):\n        axes[i].imshow(image_pixels)\n        axes[i].axis(\"off\")\n        axes[i].set_title(class_names[i])","dbd1d73a":"sub_names_imgs = [ current_class+\"\/\"+driver_imgs_list[driver_imgs_list['classname'] == current_class]['img'].values[0] for current_class in class_names]\n\nshow_images(sub_names_imgs[:5],class_names[:5])\nshow_images(sub_names_imgs[5:],class_names[5:])","c2d85a46":"train_path = \"\/kaggle\/input\/state-farm-distracted-driver-detection\/imgs\/train\"\ntest_path = \"\/kaggle\/input\/state-farm-distracted-driver-detection\/imgs\/test\"\noutput_path = \"\/kaggle\/working\/imgs\/train\"","b642e93a":"for current_class in class_names:\n    select_df = driver_imgs_list[driver_imgs_list['classname'] == current_class ]\n    image_list = select_df['img'].values\n    if not os.path.exists(output_path+\"\/\"+current_class):\n        os.makedirs(output_path+\"\/\"+current_class)\n    for filename in image_list:\n        # load_image(current_class+\"\/\"+filename)\n        im = load_image(current_class+\"\/\"+filename)\n        im.save(output_path+\"\/\"+current_class+\"\/\"+filename)\n\n","f12fd94d":"!zip -r \/kaggle\/working\/output.zip \/kaggle\/working\/imgs","26776529":"!rm -rf \/kaggle\/working\/imgs\/*\n!rm -r \/kaggle\/working\/imgs","a1489bb7":"## 1.Check data distribution","98d8af20":"## 2.Plot class (images after augmentation)","d0ff1b62":"### Changing load function to augment images with movenet","93709932":" ## 3. Loads the labelled images, crops, and saves them as outputs"}}