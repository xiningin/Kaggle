{"cell_type":{"059c5db5":"code","69ca6707":"code","e29f155d":"code","4fca91d3":"code","e5d073ba":"code","b679d97d":"code","33b860e7":"code","cdd52936":"code","33ce8168":"code","bbc61c50":"code","5ad2aa0b":"code","0991e1f5":"code","d8c116a5":"code","8a51a52a":"code","8deae7b5":"code","38ee5328":"code","45d9edc0":"code","cf9e02e3":"code","c1c3df76":"code","e2c0059b":"code","e04c10e2":"code","e933a5ef":"code","e649fe18":"code","c07e5dc2":"code","a78d4631":"code","16b59146":"code","86ae68fb":"code","3a4eccef":"code","0f7ab842":"code","294ab4b9":"markdown","f6479a72":"markdown","58a7969d":"markdown","d809a750":"markdown","4e615c87":"markdown","54cbb498":"markdown","24d42067":"markdown","6d495a17":"markdown","713129a7":"markdown","b1d7c340":"markdown","f67d1028":"markdown","2380d05f":"markdown","1bab2801":"markdown","d5a6ce58":"markdown","2826d206":"markdown","4a4282ef":"markdown","5b334875":"markdown","1db84bba":"markdown"},"source":{"059c5db5":"# import libraries and custom Postgres connector\nfrom e2eml.full_processing import postprocessing\nfrom e2eml.classification import classification_blueprints as cb\nfrom e2eml.timetravel import timetravel\n\nimport pandas as pd\nfrom sklearn import model_selection\nfrom sklearn.metrics import matthews_corrcoef\nfrom sklearn.metrics import classification_report\nimport re\nimport numpy as np\npd.set_option('display.max_colwidth', None)\n\nimport pickle\nimport gc","69ca6707":"target = 'Class'","e29f155d":"creditcard = pd.read_csv('creditcard.csv')\ncreditcard","4fca91d3":"creditcard[target].value_counts()","e5d073ba":"creditcard[target].isna().sum()","b679d97d":"len(creditcard.index)","33b860e7":"creditcard = creditcard.sort_values(by=[\"Time\"], ascending=[True])","cdd52936":"train_df = creditcard.head(230000)","33ce8168":"train_df = train_df.sample(100000, random_state=1000)\ntrain_df_target = train_df[target]","bbc61c50":"train_df_target.value_counts()","5ad2aa0b":"# actual holdout\nholdout_df = creditcard.tail(50000)\nhold_df_target = holdout_df[target]\ndel holdout_df[target]","0991e1f5":"hold_df_target.value_counts()","d8c116a5":"del creditcard\n_ = gc.collect()","8a51a52a":"automl_pipeline = cb.ClassificationBluePrint(datasource=train_df,\n                                       target_variable=target,\n                                       preferred_training_mode='cpu',\n                                       tune_mode='accurate',\n                                       ml_task='binary',\n                                       rapids_acceleration=False\n                                        )","8deae7b5":"results = timetravel.timewalk_auto_exploration(class_instance=automl_pipeline,\n                                   holdout_df=holdout_df,\n                                   holdout_target=hold_df_target,\n                                   is_imbalanced=True,\n                                   algs_to_test=None,\n                                   experiment_name=\"timewalk_creditcard_fraud.pkl\")","38ee5328":"results = pd.read_pickle(\"timewalk_creditcard_fraud.pkl\")","45d9edc0":"results.sort_values(by=[\"Matthews\"], ascending=[False]).head(30).drop_duplicates(subset=['Trial number', 'Algorithm', 'Matthews'], keep='last')","cf9e02e3":"results = timetravel.timewalk_auto_exploration(class_instance=automl_pipeline,\n                                   holdout_df=holdout_df,\n                                   holdout_target=hold_df_target,\n                                   is_imbalanced=True,\n                                    preprocess_checkpoints=[\"default\", \n                                                            \"delete_high_null_cols\",\n                                                            \"early_numeric_only_feature_selection\",\n                                                            \"fill_infinite_values\"],\n                                   algs_to_test=[\"lgbm\", \"lgbm_focal\", \"ridge\", \"logistic_regression\", \"vowpal_wabbit\"],\n                                   name_of_exist_experiment=\"timewalk_creditcard_fraud.pkl\",\n                                   experiment_name=\"timewalk_creditcard_fraud.pkl\")","c1c3df76":"results = pd.read_pickle(\"timewalk_creditcard_fraud.pkl\")","e2c0059b":"results.sort_values(by=[\"Matthews\"], ascending=[False]).head(30).drop_duplicates(subset=['Trial number', 'Algorithm', 'Matthews'], keep='last')","e04c10e2":"import plotly.express as px\nfig = px.line(\n            results,\n            x=\"Total runtime\",\n            y=\"Matthews\",\n            color=\"Algorithm\",\n            text=\"Trial number\",\n            title='Performance vs runtime comparison of algorithms'\n        )\nfig.update_traces(textposition=\"bottom right\")\nfig.show()","e933a5ef":"target = 'Class'\ncreditcard = pd.read_csv('creditcard.csv')\ncreditcard = creditcard.sort_values(by=[\"Time\"], ascending=[True])\ntrain_df = creditcard.head(230000)\ntrain_df = train_df.sample(230000, random_state=1000)\ntrain_df_target = train_df[target]\n# actual holdout\nholdout_df = creditcard.tail(50000)\nhold_df_target = holdout_df[target]\ndel holdout_df[target]\ndel creditcard\n_ = gc.collect()","e649fe18":"results = pd.read_pickle(\"timewalk_creditcard_fraud.pkl\")\nresults.sort_values(by=[\"Matthews\"], ascending=[False]).head(30).drop_duplicates(subset=['Trial number', 'Algorithm', 'Matthews'], keep='last')","c07e5dc2":"winner_set = results[(results[\"Accuracy\"].isna() == False)].sort_values(by=[\"Matthews\"], ascending=[False]).head(1)\nwinner_set","a78d4631":"automl_pipeline = cb.ClassificationBluePrint(datasource=train_df,\n                                       target_variable=target,\n                                       preferred_training_mode='cpu',\n                                       tune_mode='accurate',\n                                       ml_task='binary',\n                                       rapids_acceleration=False\n                                        )","16b59146":"# showing the preprocessing steps of our best iteration and model\nbest_params = winner_set[\"Preprocessing applied\"].values.tolist()[0]\n\n# use best prarams from timewalk\nfor key, value in best_params.items():\n    automl_pipeline.blueprint_step_selection_non_nlp[key] = value","86ae68fb":"automl_pipeline.ml_bp14_multiclass_full_processing_lgbm_focal()","3a4eccef":"automl_pipeline.ml_bp14_multiclass_full_processing_lgbm_focal(holdout_df)","0f7ab842":"algorithms = [\"lgbm_focal\"]\n\ndef get_matthews(algorithm):\n    # Assess prediction quality on holdout data\n    print(classification_report(hold_df_target, automl_pipeline.predicted_classes[algorithm]))\n    try:\n        matthews = matthews_corrcoef(hold_df_target, automl_pipeline.predicted_classes[algorithm])\n    except Exception:\n        print(\"Matthew failed.\")\n        matthews = 0\n    print(matthews)\n    \nfor i in algorithms:\n    print(f\"---------Start evaluating {i}----------\")\n    get_matthews(i)","294ab4b9":"# Continue the trial\nWe continue our trial, but change the parameters (for this step we restartet Jupyter Kernel and rerun the code until ClassificationBlueprint instantiation. This is for demonstation purposes only).. We explicitly call certain algorithms and also reduce checkpoints (\"default\" is mandatory here). We also provide the name of an existing experiment file, so Timewalk can append new to previous results. This is kind of a mini MLOps utility.\n\nWhat exactly happens here?\nTimetravel will run the \"default\" step. During this step it creates different save points of our preprocessed data and stores them locally. In it's current implementation it does this every time we run Timnetravel. Please be aware, that this might consume much disk space (depending on dataset size).\nThen Timewalk will load a previous preprocessing checkpoint from disk, adjust some preprocessing decisions (i.e. different feature selection) and run the models again. In our case this should run a lot faster than in the attempt above, because we do not make use of all steps and algorithms anymore. Especially TabNet has been very slow. ","f6479a72":"Using e2eml we first instantiate a class, here a ClassificationBluePrint.","58a7969d":"# Summary\n\nIt seems like we created a good model here. However some notes:\n- Accuracy is not a good metric for imbalanced data. Matthews correlation is a lot better as a general metric\n- Our model could still fail on production! The training data might be from a wrong season or just too old and would suffer a lot seeing new patterns in production. So monitoring ongoing performance of your model is extremely important.\n- Does automl replace data scientists? No, it empowers them. Instead of building stuff from scratch, data scientists can focus on finding better features. Also it needs expertise to properly evaluate a model. Automl cannot replace domain knowledge at all.\n\nI hope you enjoyed the notebook. Upvotes are very welcome :-) ","d809a750":"# Importing libraries","4e615c87":"# Import needed data","54cbb498":"Here we go crazy actually. Timewalk should be used with smaller samples, but we want to illustrate what might happen, if the sample is very big.","24d42067":"# Creating the final pipeline\nWe:\n- restart the notebook\n- reimport libraries\n- reload the data, but this time use 230k rows for training instead of 100k\n- load in the results from Timnewalk\n- instantiate a ClassificationBlueprint class\n- take the winning parameters from results and overwrite the class default preprocessing steps\n- run the lgbm_focal_loss pipeline\n- run the function agsin (from here it is in prediction mode and will predict on new data)\n- do a final evaluation\n\nLet's see, if more data can help improving the score. It could also hurt performance, if we sudeenly have more bad data points in the set. ","6d495a17":"# Test train split","713129a7":"# Results overview\nTimewalk returns a result dataframe. Here we can sort the results by Matthews correlation to see our best results.\n\nDuring our trial we ran out of memory. As Timewalk is still in an early stage we still try to aim for optimization. Timewalk tries to clean memory, but sometimes it does not clean properly., However we had a different reason here. For some reason memory consumption exploded after transforming the data.\n\nHowever it is not a big problem here. We already saw very good results. If we would like to continue Timewalk we could pass the parameter \"preprocess_checkpoints\", which expects a list. On default this includes:\n- \"default\"\n- \"delete_low_variance_features\"\n- \"automated_feature_selection\"\n- \"autotuned_clustering\"\n- \"cardinality_remover\"\n- \"delete_high_null_cols\"\n- \"early_numeric_only_feature_selection\"\n- \"fill_infinite_values\"\n\nWe failed during checkpoint \"cardinality_remover\". So we could provide the list \n[\"default\", \n \"delete_high_null_cols\",\n \"early_numeric_only_feature_selection\",\n \"fill_infinite_values\"] instead. We exclude the failing step as well as we would trigger the same issue again.\n \n We can also chose to run less algorithms. SGD & Quadratic Discriminent Analysis did not look promising at all. TabNet has been very good, but extremely slow.","b1d7c340":"# Introduction\n\nHi,\n\nI am Thomas, creator of the automl library e2eml. In this notebook I give you a little walkthrough and example run using a feature called Timewalk.\n\nI created this library for personal development, but also to give something back to the data science community. You can install the library using !pip install e2eml.\n\n\n# What does e2eml offer?\ne2eeml has two major goals: You can either speed up your prototyping and exploration (with Timewalk) or create a full pipeline with just a few lines of code. e2eml will take care of:\n- data preprocessing\n- model training\n- model fine-tuning\n- model evaluation\n- logging file\n\nIt can handle datetime, categorical and numerical data and allows you to build classification & regression models. For NLP tasks you can even let it create a full BERT model for you.\n\n# Why e2eml and not any other automl framework?\nThis decision is fully up to you. They all have their ins & outs. Chose what suits your needs best. e2eml is just an option for you.\n\n\n# What is the spirit of e2eml?\nThis library tries to maximize the model performance. It shall help to see how far you can get given your data.\nAs a characteristic it creates huge notebooks, but we really wanted to print out a lot for. We want you to be able to actually see what is happening under the hood. That comes at a cost here. If you prefer very elegant and silent implementations, check out the fantastic Pycaret library.\nAdditionally being able to fine-tune BERT models for you can be a life saver. We also provide some GPU acceleration with RAPIDS. Currently this is implemented in a few spots only however.","f67d1028":"After the delete_high_null_cols step we would run into the same issues as every following step would use feature transformation as well.\nHere we could cose to run this npart with a smaller sample size, but LGBM focal seems to be the winner anyway.","2380d05f":"# General information about this notebook\nThis notebook has empty output cells. It ran outside of Kaggle for computation reasons:\n- Kaggle has many libraries, so dependencies\n- long notebook runtime\n\nHowever you can find the full notebook with full output on Github:\nhttps:\/\/github.com\/ThomasMeissnerDS\/e2e_ml\/blob\/develop\/Example%20notebooks\/Auto%20exploration%20using%20Timewalk\/Kaggle%20credit%20card%20fraud%20with%20Timewalk.ipynb","1bab2801":"First we sort the dataset by timestamp and then hold back the newest data points as unseen holdout data for validation.\nThis has two reasons:\n- Without any unseen data we cannot see overfitting\n- Fraud does not consist of static behaviour. In real world applications fraudsters change their behaviour to adapt for anti-fraud mechanisms. For model prototyping (not final training!) it makes sense to keep newest data as holdout, so we can see, if our model can adapt to changing patterns (given they even change in our training data)","d5a6ce58":"This notebook did not run on Kaggle, but a local system.\nWe are running on e2eml version 2.10.1","2826d206":"More data did not improve our model.","4a4282ef":"LGBM focal is has been fast and superior. This makes sense as focal loss is most suitable for imbalanced data (binary and multiclass).","5b334875":"# Automl using e2eml\n\nWe use e2eml. There are plenty of fantastic frameworks. Chose whatever you like.","1db84bba":"From here we have two general options. We can:\n- run a certain blueprint straight away (this is great to create a ready-to-use pipeline for prediction)\n- run Timewalk to fully explore many algorithms and preprocessing combinations\n\nHere we assume that we don't know much about what works for this dataset. So we chose Timewalk. Timewalk takes a long time to run (this can be controlled by manually chosing algorithms and preprocessing steps to use). As we have plenty of data Timewalk will automatically switch off some algorithms. I.e. Xgboost will not run here (this has been designed due to problems of releasing the memory again and also due to high consumption of system resources).\n\nAs our data is imbalanced, we add this flag as a parameter. This will add an additional step for imbalanced data only."}}