{"cell_type":{"130a19ca":"code","e4f9bfb3":"code","bf221c19":"code","36e05bff":"code","292b9128":"code","d9b6c8c9":"code","2af614dd":"markdown","29a60550":"markdown","2e7d913b":"markdown","c05490f8":"markdown","1ad7569b":"markdown","838f3d7e":"markdown","67b2536c":"markdown"},"source":{"130a19ca":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport librosa\nfrom librosa import display\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore', category=UserWarning)","e4f9bfb3":"visualize_digits = [1, 3, 5, 8]\n\ndescription = pd.read_csv('..\/input\/freespokendigitsdataset\/train.csv')\nfig, axs = plt.subplots(nrows=2, ncols=(len(visualize_digits)))\naxs = axs.flatten()\nfig.suptitle('Data Exploration', fontsize=16)\nfig.set_size_inches(24, 8)\nfor i, digit in enumerate(visualize_digits):\n    # Select random file for selected digit\n    rand_file = np.random.choice(description[description['label'] == digit]['file_name'])\n    # Load audio and plot waveform and melspectrogram\n    audio, sr = librosa.load(os.path.join('..\/input\/freespokendigitsdataset\/recordings', rand_file), sr=None)\n    librosa.display.waveplot(y=audio, sr=sr, x_axis='time', ax=axs[i])\n    axs[i].set(title=f'Waveplot of {rand_file}')\n    mels_db = librosa.power_to_db(S=librosa.feature.melspectrogram(y=audio, sr=sr), ref=1.0)\n    librosa.display.specshow(data=mels_db, sr=sr, x_axis='time', y_axis='mel', ax=axs[i+len(visualize_digits)])\n    axs[i+len(visualize_digits)].set(title=f'Melspectrogram of {rand_file}')\nplt.tight_layout()\nplt.show()","bf221c19":"feature_list = []\nlabel_list = []\n# Iterate over all files in given source path\nprint('Preparing feature dataset and labels.')\nfor file in tqdm(os.listdir('..\/input\/freespokendigitsdataset\/recordings')):\n    # Skip if it's not a wav file\n    if not file.endswith('.wav'):\n        continue\n    # Load audio and stretch it to length 1s\n    audio_path = os.path.join('..\/input\/freespokendigitsdataset\/recordings', file)\n    audio, sr = librosa.load(path=audio_path, sr=None)\n    audio = librosa.effects.time_stretch(y=audio, rate=len(audio)\/sr)\n    # Calculate features and get the label from the filename\n    mels = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=2048, hop_length=512)\n    mels_db = librosa.power_to_db(S=mels, ref=1.0)\n    feature_list.append(mels_db.reshape((128, 16, 1)))\n    label_list.append(file[0])\nfeatures = np.array(feature_list)\nlabels = np.array(label_list)","36e05bff":"model = keras.Sequential(layers=[\n        keras.layers.InputLayer(input_shape=features[0].shape),\n        keras.layers.Conv2D(16, 3, padding='same', activation=keras.activations.relu),\n        keras.layers.MaxPooling2D(),\n        keras.layers.Conv2D(32, 3, padding='same', activation=keras.activations.relu),\n        keras.layers.MaxPooling2D(),\n        keras.layers.Flatten(),\n        keras.layers.Dropout(0.3),\n        keras.layers.Dense(64, activation=keras.activations.relu),\n        keras.layers.Dense(10, activation=keras.activations.softmax)\n    ])\nmodel.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\nprint(model.summary())","292b9128":"# Set parameters for data splitting and training\nTEST_SIZE = 0.3\nBATCH_SIZE = 64\nEPOCHS = 50\n\n# Encode Labels\nencoded_labels = tf.one_hot(indices=labels, depth=10)\n# Split dataset to train and test data\nX_train, X_test, y_train, y_test = train_test_split(features, encoded_labels.numpy(), test_size=TEST_SIZE)\n\n# Train the model\nearly_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nhistory = model.fit(x=X_train, y=y_train, validation_split=TEST_SIZE, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping])\n\n# Plot the training history\nfig, axs = plt.subplots(2)\nfig.set_size_inches(12, 8)\nfig.suptitle('Training History', fontsize=16)\naxs[0].plot(history.epoch, history.history['loss'], history.history['val_loss'])\naxs[0].set(title='Loss', xlabel='Epoch', ylabel='Loss')\naxs[0].legend(['loss', 'val_loss'])\naxs[1].plot(history.epoch, history.history['accuracy'], history.history['val_accuracy'])\naxs[1].set(title='Accuracy', xlabel='Epoch', ylabel='Accuracy')\naxs[1].legend(['accuracy', 'val_accuracy'])\nplt.show()","d9b6c8c9":"y_predicted = np.argmax(model.predict(x=X_test), axis=1)\ny_true = np.argmax(y_test, axis=1)\nlabel_names = np.unique(labels)\nconfusion_matrix = tf.math.confusion_matrix(labels=y_true, predictions=y_predicted)\nfig = plt.figure()\nfig.set_size_inches(12, 8)\nsns.heatmap(confusion_matrix, xticklabels=label_names, yticklabels=label_names, annot=True, fmt='g')\nplt.xlabel('Prediction')\nplt.ylabel('Label')\nplt.show()","2af614dd":"## Evaluate the model\nAfter training, the accuracy on new data can be determined using the test set that the model has not seen before.\nThis indicates whether overfitting or underfitting is present, or the training has achieved a good result.\n\nThe results of the prediction can be displayed in a confusion matrix. This shows which class a recording really has and\nwhich class the model predicted. In the main diagonal the correct predictions can be found.","29a60550":"## Data Exploration\nIn the first step, the so-called Data Exploration, individual audio recordings are visualized and examined for first\nvisible differences. From the available 3,000 recordings, a random recording is chosen for different digits and\nvisualized as waveform and mel spectrogram.","2e7d913b":"## Import of the required packages","c05490f8":"# Digit Recognition from Audio Recordings\nThis notebook deals with the classification or recognition of spoken digits using Convolutional Neural Networks (CNN for\nshort). The dataset used is freely available and can be downloaded from the platform Kaggle platform. It contains a\ntotal of 3,000 audio recordings of different speakers who have spoken each digit 50 times. The code processes the audio\nrecordings, extracts features, and trains a CNN.\n\nDataset available at: https:\/\/www.kaggle.com\/jackvial\/freespokendigitsdataset\n\nThe _train.csv_ file, and the _recordings_ directory should be in the same directory as this notebook.","1ad7569b":"## Model Architecture\nAfter the data is prepared, the next step is to define the architecture or the structure of the neural network.\nUsing the Keras Sequential API, the individual layers can be defined one after the other.\n1. Input layer: In the first layer it is only defined which form the incoming data has. This does not necessarily be\nentered as a separate layer, but can also be defined in the subsequent layer.\n2. Conv2D Layers & MaxPooling: The next part is the core of CNNs. Within the Convolutional Layers possible relevant\ncorrelations are determined from the data by kernels. The first number indicates in each case, how many filters are to\nbe generated. The MaxPooling layer reduces the output of the previous layers by again running a kernel over the data\nand selecting the largest value from a 2\u00d72 field.\n3. Flatten & Dropout: The Flatten layer generates a 1-dimensional tensor\/vector from an n-dimensional input. The\nDropout layer randomly deactivates the given fraction of neurons per epoch. By this methodology the so-called\noverfitting (overfitting to the training data) can be reduced.\n4 Dense Layers: Finally, the CNN contains a Dense Layer with 64 neurons as well as the actual classifier\n(layer with 10 neurons, one for each class). Through the Softmax activation function, values between 0 and 1 are\nemitted which can be seen as the probability for each class.\n\nThe network is then compiled with an optimizer, loss function as well as metrics.","838f3d7e":"## Train the model\nBefore the model can be trained, the labels have to be encoded, and the data has to be partitioned.\n1. Label encoding: As defined in the last step, the last layer of the network contains 10 neurons, so the output is a\nvector of length 10. The labels are currently in pure text format, which the net cannot process. With the help of the\nso-called One-Hot-Encoding, the labels can be converted into vectors of length 10 with a 1 at one position (hence\nOne-Hot).\n2. Splitting the data: Basically, in machine learning, the data set is divided into at least two parts.\nThe largest part is the training data, which is used to train the model. The second part is the test data, which can be\nused to evaluate the model after training. In this example, when training the model, it is determined that a part of the\ntraining data should be retained. With this data it is already possible during the training to determine the score the\nmodel can achieve on unseen data.\n\nAfter training, the results of the history can be visualized over the individual epochs.","67b2536c":"## Feature Engineering\nThe next step is to extract features from the audio recordings, in this case a Melspectrogram. In order later train the\nneural network, all features must have the same shape. However, since the recordings are of different lengths, they are\nbrought to a length of 1s by time-stretching. The features and labels of the recordings are stored in numpy arrays to be\nable to work with them easily later."}}