{"cell_type":{"9f11cf67":"code","47cf78e2":"code","ed778ffc":"code","25587958":"code","fe56abf8":"code","68863cac":"code","d8cc01f3":"code","ebf50974":"code","a77fef44":"code","df9d192e":"code","fda18bd4":"code","bb8ee0d1":"code","6427e3ca":"code","0906d5e4":"code","7f99c24a":"code","3168abfd":"code","ab8207ef":"code","3d85ed92":"code","65bfefc6":"code","5ab12dce":"code","e49f802e":"code","837bfc06":"code","eaf04f6e":"code","674c3517":"code","cac6931f":"code","7f573e15":"code","046b35b1":"code","47e14a57":"code","98303e3d":"code","8f3eb71b":"code","140754b3":"code","0846b80b":"code","28dc2a5f":"code","21a788a4":"code","4024f75c":"markdown","906dba2f":"markdown","f4352cef":"markdown","ea858cbe":"markdown","76cfe8c8":"markdown","1d17d298":"markdown","4c6f587f":"markdown","a908ed06":"markdown"},"source":{"9f11cf67":"import pandas as pd","47cf78e2":"df = pd.DataFrame({\n    'student_id': [1,2,3,4,5,6,7],\n    'country': ['China', 'USA', 'UK', 'Japan', 'Korea', 'China', 'USA'],\n    'education': ['Master', 'Bachelor', 'Bachelor', 'Master', 'PHD', 'PHD', 'Bachelor'],\n    'target': [1, 0, 1, 0, 1, 0, 1]\n})\ndf.head(10)","ed778ffc":"pd.get_dummies(df, columns=['education'])","25587958":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder()\nohe.fit_transform(df[['country']]).toarray()","fe56abf8":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['country_LabelEncoder'] = le.fit_transform(df['country'])\ndf.head(10)","68863cac":"df['country_LabelEncoder'] = pd.factorize(df['country'])[0]\ndf.head(10)","d8cc01f3":"pd.factorize(df['country'])","ebf50974":"df['education'] = df['education'].map(\n                    {'Bachelor': 1, \n                    'Master': 2, \n                    'PHD': 3})\ndf.head(10)","a77fef44":"import category_encoders as ce\nencoder = ce.BinaryEncoder(cols= ['country'])\n\npd.concat([df, encoder.fit_transform(df['country']).iloc[:, 1:]], axis=1)","df9d192e":"df['country_count'] = df['country'].map(df['country'].value_counts()) \/ len(df)\ndf.head(10)","fda18bd4":"df['country_count'] = df['country'].map(df['country'].value_counts())\ndf.head(10)","bb8ee0d1":"df.groupby(['country'])['target'].mean()","6427e3ca":"df['country_target'] = df['country'].map(df.groupby(['country'])['target'].mean())\ndf.head(10)","0906d5e4":"df = pd.DataFrame({\n    'student_id': [1,2,3,4,5,6,7],\n    'country': ['China', 'USA', 'UK', 'Japan', 'Korea', 'China', 'USA'],\n    'education': ['Master', 'Bachelor', 'Bachelor', 'Master', 'PHD', 'PHD', 'Bachelor'],\n    'age': [34.5, 28.9, 19.5, 23.6, 19.8, 29.8, 31.7],\n    'target': [1, 0, 1, 0, 1, 0, 1]\n})\ndf.head(10)","7f99c24a":"df['age_round1'] = df['age'].round()\ndf['age_round2'] = (df['age'] \/ 10).astype(int)\ndf.head(10)","3168abfd":"df['age_<20'] = (df['age'] <= 20).astype(int)\ndf['age_20-25'] = ((df['age'] > 20) & (df['age'] <=25)).astype(int)\ndf['age_20-25'] = ((df['age'] > 25) & (df['age'] <= 30)).astype(int)\ndf['age_>30'] = (df['age'] > 30).astype(int)\ndf.head(10)","ab8207ef":"df","3d85ed92":"%pylab inline\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.datasets import load_boston\ndata = load_boston()\n\nrf = RandomForestRegressor()\nrf.fit(data.data, data.target);\nprint(rf.feature_importances_)\n\nplt.figure(figsize=(12, 6))\nplt.bar(range(1, 14), rf.feature_importances_)\n_ = plt.xticks(range(1, 14), data.feature_names)","65bfefc6":"import numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom lightgbm import LGBMRegressor\ndata = load_boston()\n\nclf = LGBMRegressor()\nclf.fit(data.data, data.target)\n\nplt.figure(figsize=(12, 6))\nplt.bar(range(1, 14), clf.feature_importances_)\n_ = plt.xticks(range(1, 14), data.feature_names)","5ab12dce":"import numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom xgboost import XGBRegressor\ndata = load_boston()\n\nclf = XGBRegressor()\nclf.fit(data.data, data.target)\n\nplt.figure(figsize=(12, 6))\nplt.bar(range(1, 14), clf.feature_importances_)\n_ = plt.xticks(range(1, 14), data.feature_names)","e49f802e":"import os\nimport sys\nimport operator\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nimport random\nimport xgboost as xgb\nfrom sklearn import model_selection, preprocessing, ensemble\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom collections import defaultdict, Counter","837bfc06":"def runXGB(train_X, train_y, val_X, val_y=None, test_X=None, feature_names=None, seed_val=0, num_rounds=1000):\n    param = {}\n    param['objective'] = 'multi:softprob'\n    param['eta'] = 0.03\n    param['max_depth'] = 6\n    param['silent'] = 0\n    param['num_class'] = 3\n    param['eval_metric'] = \"mlogloss\"\n    param['min_child_weight'] = 1\n    param['subsample'] = 0.7\n    param['colsample_bytree'] = 0.7\n    param['seed'] = seed_val\n    param['nthread'] = 12\n    num_rounds = num_rounds\n\n    plst = list(param.items())\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n    if val_y is not None:\n        xgval = xgb.DMatrix(val_X, label=val_y)\n        watchlist = [ (xgtrain,'train'), (xgval, 'val') ]\n        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50)\n    else:\n        model = xgb.train(plst, xgtrain, num_rounds)\n    xgtest = xgb.DMatrix(test_X)\n    \n    pred_test_y = model.predict(xgtest)\n    return pred_test_y, model","eaf04f6e":"train_df = pd.read_json('..\/input\/two-sigma-connect-rental-listing-inquiries\/train.json.zip', compression='zip')\ntest_df = pd.read_json('..\/input\/two-sigma-connect-rental-listing-inquiries\/test.json.zip', compression='zip')\n\nfeatures_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]\n\nmean_price = int(train_df['price'].mean())\ntest_df.loc[test_df['price']<200,'price'] = mean_price\ntrain_df.loc[train_df['price']<200,'price'] = mean_price","674c3517":"train_test = pd.concat([train_df, test_df], 0,sort=False)\n\nfeatures = train_test[[\"features\"]].apply(\n    lambda _: [list(map(str.strip, map(str.lower, x))) for x in _])\n\n\nn = 5\n\nfeature_counts = Counter()\nfor feature in features.features:\n    feature_counts.update(feature)\nfeature = sorted([k for (k,v) in feature_counts.items() if v > n])\nfeature[:10]\n\n\ndef clean(s):\n    x = s.replace(\"-\", \"\")\n    x = x.replace(\" \", \"\")\n    x = x.replace(\"24\/7\", \"24\")\n    x = x.replace(\"24hr\", \"24\")\n    x = x.replace(\"24-hour\", \"24\")\n    x = x.replace(\"24hour\", \"24\")\n    x = x.replace(\"24 hour\", \"24\")\n    x = x.replace(\"common\", \"cm\")\n    x = x.replace(\"concierge\", \"doorman\")\n    x = x.replace(\"bicycle\", \"bike\")\n    x = x.replace(\"pets:cats\", \"cats\")\n    x = x.replace(\"allpetsok\", \"pets\")\n    x = x.replace(\"dogs\", \"pets\")\n    x = x.replace(\"private\", \"pv\")\n    x = x.replace(\"deco\", \"dc\")\n    x = x.replace(\"decorative\", \"dc\")\n    x = x.replace(\"onsite\", \"os\")\n    x = x.replace(\"outdoor\", \"od\")\n    x = x.replace(\"ss appliances\", \"stainless\")\n    return x\n\ndef feature_hash(x):\n    cleaned = clean(x, uniq)\n    key = cleaned[:4].strip()\n    return key\n\n\nkey2original = defaultdict(list)\nk = 4\nfor f in feature:\n    cleaned = clean(f)\n    key = cleaned[:k].strip()\n\n    key2original[key].append(f)\n\n    \ndef to_tuples():\n    for f in feature:\n        key = clean(f)[:k].strip()\n        yield (f, key2original[key][0])\n        \ndeduped = list(to_tuples())\ndf = pd.DataFrame(deduped, columns=[\"original_feature\", \"unique_feature\"])\n\ndict_rep_features = pd.Series(df['unique_feature'].values, df['original_feature'].values)","cac6931f":"test_df['features'] = test_df['features'].apply(lambda x: list(map(str.strip, map(str.lower, x))))\\\n                    .apply(lambda x: [dict_rep_features[i] for i in x if i in dict_rep_features.index])\\\n                    .apply(lambda x: list(set(x)))\n\ntrain_df['features'] = train_df['features'].apply(lambda x: list(map(str.strip, map(str.lower, x))))\\\n                    .apply(lambda x: [dict_rep_features[i] for i in x if i in dict_rep_features.index])\\\n                    .apply(lambda x: list(set(x)))\n","7f573e15":"import math\ndef cart2rho(x, y):\n    rho = np.sqrt(x**2 + y**2)\n    return rho\n\n\ndef cart2phi(x, y):\n    phi = np.arctan2(y, x)\n    return phi\n\n\ndef rotation_x(row, alpha):\n    x = row['latitude']\n    y = row['longitude']\n    return x*math.cos(alpha) + y*math.sin(alpha)\n\n\ndef rotation_y(row, alpha):\n    x = row['latitude']\n    y = row['longitude']\n    return y*math.cos(alpha) - x*math.sin(alpha)\n\n\ndef add_rotation(degrees, df):\n    namex = \"rot\" + str(degrees) + \"_X\"\n    namey = \"rot\" + str(degrees) + \"_Y\"\n\n    df['num_' + namex] = df.apply(lambda row: rotation_x(row, math.pi\/(180\/degrees)), axis=1)\n    df['num_' + namey] = df.apply(lambda row: rotation_y(row, math.pi\/(180\/degrees)), axis=1)\n\n    return df\n\ndef operate_on_coordinates(tr_df, te_df):\n    for df in [tr_df, te_df]:\n        #polar coordinates system\n        df[\"num_rho\"] = df.apply(lambda x: cart2rho(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n        df[\"num_phi\"] = df.apply(lambda x: cart2phi(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n        #rotations\n        for angle in [15,30,45,60]:\n            df = add_rotation(angle, df)\n\n    return tr_df, te_df\n\ntrain_df, test_df = operate_on_coordinates(train_df, test_df)\n\nfeatures_to_use.extend(['num_rho', 'num_phi', 'num_rot15_X', 'num_rot15_Y', 'num_rot30_X',\n       'num_rot30_Y', 'num_rot45_X', 'num_rot45_Y', 'num_rot60_X',\n       'num_rot60_Y'])","046b35b1":"import re\n\ndef cap_share(x):\n    return sum(1 for c in x if c.isupper())\/float(len(x)+1)\n\nfor df in [train_df, test_df]:\n    # do you think that users might feel annoyed BY A DESCRIPTION THAT IS SHOUTING AT THEM?\n    df['num_cap_share'] = df['description'].apply(cap_share)\n    \n    # how long in lines the desc is?\n    df['num_nr_of_lines'] = df['description'].apply(lambda x: x.count('<br \/><br \/>'))\n   \n    # is the description redacted by the website?        \n    df['num_redacted'] = 0\n    df['num_redacted'].loc[df['description'].str.contains('website_redacted')] = 1\n\n    \n    # can we contact someone via e-mail to ask for the details?\n    df['num_email'] = 0\n    df['num_email'].loc[df['description'].str.contains('@')] = 1\n    \n    #and... can we call them?\n    \n    reg = re.compile(\".*?(\\(?\\d{3}\\D{0,3}\\d{3}\\D{0,3}\\d{4}).*?\", re.S)\n    def try_and_find_nr(description):\n        if reg.match(description) is None:\n            return 0\n        return 1\n\n    df['num_phone_nr'] = df['description'].apply(try_and_find_nr)\n\n    \n\n\nfeatures_to_use.extend(['num_cap_share', 'num_nr_of_lines', 'num_redacted',\n       'num_email', 'num_phone_nr'])","47e14a57":"# count of photos #\ntrain_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\ntest_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n\n# count of \"features\" #\ntrain_df[\"num_features\"] = train_df[\"features\"].apply(len)\ntest_df[\"num_features\"] = test_df[\"features\"].apply(len)\n\n# count of words present in description column #\ntrain_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\ntest_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n\n# convert the created column to datetime object so as to extract more features \ntrain_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\ntest_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n\n# Let us extract some features like year, month, day, hour from date columns #\ntrain_df[\"created_year\"] = train_df[\"created\"].dt.year\ntest_df[\"created_year\"] = test_df[\"created\"].dt.year\ntrain_df[\"created_month\"] = train_df[\"created\"].dt.month\ntest_df[\"created_month\"] = test_df[\"created\"].dt.month\ntrain_df[\"created_day\"] = train_df[\"created\"].dt.day\ntest_df[\"created_day\"] = test_df[\"created\"].dt.day\ntrain_df[\"created_hour\"] = train_df[\"created\"].dt.hour\ntest_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n\n# adding all these new features to use list #\nfeatures_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\",\"created_year\", \"created_month\", \"created_day\", \"listing_id\", \"created_hour\"])\n\ntrain_df[\"price_t\"] =train_df[\"price\"]\/train_df[\"bedrooms\"]\ntest_df[\"price_t\"] = test_df[\"price\"]\/test_df[\"bedrooms\"] \n\ntrain_df[\"room_sum\"] = train_df[\"bedrooms\"]+train_df[\"bathrooms\"] \ntest_df[\"room_sum\"] = test_df[\"bedrooms\"]+test_df[\"bathrooms\"] \n\nfeatures_to_use.extend([\"price_t\", \"room_sum\", \"num_description_words\"])","98303e3d":"start_values = [0,0,0]\n\nindex=list(range(train_df.shape[0]))\nrandom.shuffle(index)\na=[np.nan]*len(train_df)\nb=[np.nan]*len(train_df)\nc=[np.nan]*len(train_df)\n\nfor i in range(5):\n    building_level={}\n    for j in train_df['manager_id'].values:\n        building_level[j]= start_values.copy()\n    test_index=index[int((i*train_df.shape[0])\/5):int(((i+1)*train_df.shape[0])\/5)]\n    train_index=list(set(index).difference(test_index))\n    for j in train_index:\n        temp=train_df.iloc[j]\n        if temp['interest_level']=='low':\n            building_level[temp['manager_id']][0]+=1\n        if temp['interest_level']=='medium':\n            building_level[temp['manager_id']][1]+=1\n        if temp['interest_level']=='high':\n            building_level[temp['manager_id']][2]+=1\n    for j in test_index:\n        temp=train_df.iloc[j]\n        if sum(building_level[temp['manager_id']])!=0:\n            a[j]=building_level[temp['manager_id']][0]*1.0\/sum(building_level[temp['manager_id']])\n            b[j]=building_level[temp['manager_id']][1]*1.0\/sum(building_level[temp['manager_id']])\n            c[j]=building_level[temp['manager_id']][2]*1.0\/sum(building_level[temp['manager_id']])\ntrain_df['manager_level_low']=a\ntrain_df['manager_level_medium']=b\ntrain_df['manager_level_high']=c\n\n\na=[]\nb=[]\nc=[]\nbuilding_level={}\nfor j in train_df['manager_id'].values:\n    building_level[j]= start_values.copy()\nfor j in range(train_df.shape[0]):\n    temp=train_df.iloc[j]\n    if temp['interest_level']=='low':\n        building_level[temp['manager_id']][0]+=1\n    if temp['interest_level']=='medium':\n        building_level[temp['manager_id']][1]+=1\n    if temp['interest_level']=='high':\n        building_level[temp['manager_id']][2]+=1\n\nfor i in test_df['manager_id'].values:\n    if i not in building_level.keys():\n        a.append(np.nan)\n        b.append(np.nan)\n        c.append(np.nan)\n    else:\n        a.append(building_level[i][0]*1.0\/sum(building_level[i]))\n        b.append(building_level[i][1]*1.0\/sum(building_level[i]))\n        c.append(building_level[i][2]*1.0\/sum(building_level[i]))\ntest_df['manager_level_low']=a\ntest_df['manager_level_medium']=b\ntest_df['manager_level_high']=c\n\nfeatures_to_use.append('manager_level_low') \nfeatures_to_use.append('manager_level_medium') \nfeatures_to_use.append('manager_level_high')","8f3eb71b":"train_df[\"listing_id1\"] = train_df[\"listing_id\"] - 68119576.0\ntest_df[\"listing_id1\"] =  test_df[\"listing_id\"] - 68119576.0\n\ntrain_df[\"num_price_by_furniture\"] = (train_df[\"price\"])\/ (train_df[\"bathrooms\"] + train_df[\"bedrooms\"] + 1.0)\ntest_df[\"num_price_by_furniture\"] =  (test_df[\"price\"])\/ (test_df[\"bathrooms\"] + test_df[\"bedrooms\"] +  1.0)\n\ntrain_df[\"price_latitue\"] = (train_df[\"price\"])\/ (train_df[\"latitude\"]+1.0)\ntest_df[\"price_latitue\"] =  (test_df[\"price\"])\/ (test_df[\"latitude\"]+1.0)\n\ntrain_df[\"price_longtitude\"] = (train_df[\"price\"])\/ (train_df[\"longitude\"]-1.0)\ntest_df[\"price_longtitude\"] =  (test_df[\"price\"])\/ (test_df[\"longitude\"]-1.0)  \n\ntrain_df[\"num_furniture\"] =  train_df[\"bathrooms\"] + train_df[\"bedrooms\"] \ntest_df[\"num_furniture\"] =   test_df[\"bathrooms\"] + test_df[\"bedrooms\"] \n\ntrain_df[\"total_days\"] =   (train_df[\"created_month\"] -4.0)*30 + train_df[\"created_day\"] +  train_df[\"created_hour\"] \/25.0\ntest_df[\"total_days\"] =(test_df[\"created_month\"] -4.0)*30 + test_df[\"created_day\"] +  test_df[\"created_hour\"] \/25.0        \ntrain_df[\"diff_rank\"]= train_df[\"total_days\"]\/train_df[\"listing_id1\"]\ntest_df[\"diff_rank\"]= test_df[\"total_days\"]\/test_df[\"listing_id1\"]\n\n\nfeatures_to_use.extend([ \"total_days\",\"diff_rank\",\n\"num_price_by_furniture\",\"price_latitue\",\"price_longtitude\",'num_furniture'])","140754b3":"categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\nfor f in categorical:\n        if train_df[f].dtype=='object':\n            #print(f)\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f] = lbl.transform(list(test_df[f].values))\n            features_to_use.append(f)","0846b80b":"train_df[\"price0\"] = (train_df[\"price\"]%10==0).astype(int)\ntest_df[\"price0\"] = (test_df[\"price\"]%10==0).astype(int)\n\ntrain_df[\"manager_count\"] = train_df[\"manager_id\"].replace(train_df[\"manager_id\"].value_counts())\ntest_df[\"manager_count\"] = test_df[\"manager_id\"].replace(train_df[\"manager_id\"].value_counts())\n\nfeatures_to_use.extend([\"price0\",'manager_count'])\n\ntrain_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\ntest_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\nprint(train_df[\"features\"].head())\ntfidf = CountVectorizer(stop_words='english', max_features=70)\nte_sparse = tfidf.fit_transform(test_df[\"features\"])\ntr_sparse = tfidf.transform(train_df[\"features\"])\n\ntfidfdesc=TfidfVectorizer(min_df=20, max_features=50, strip_accents='unicode',lowercase =True,\n                    analyzer='word', token_pattern=r'\\w{16,}', ngram_range=(1, 2), use_idf=False,smooth_idf=False, \n                    sublinear_tf=True, stop_words = 'english')  \n\ntrain_df['description'] =  train_df['description'].apply(lambda x: str(x).encode('utf-8') if len(x)>2 else \"nulldesc\") \ntest_df['description'] =   test_df['description'].apply(lambda x: str(x).encode('utf-8') if len(x)>2 else \"nulldesc\") \nte_sparsed = tfidfdesc. fit_transform (test_df[\"description\"])  \ntr_sparsed = tfidfdesc.transform(train_df[\"description\"])\n\ntrain_X = sparse.hstack([train_df[features_to_use], tr_sparse,tr_sparsed]).tocsr()#\ntest_X = sparse.hstack([test_df[features_to_use], te_sparse,te_sparsed]).tocsr()#\n\ntarget_num_map = {'high':0, 'medium':1, 'low':2}\ntrain_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n\nprint(train_X.shape, test_X.shape)","28dc2a5f":"preds, model = runXGB(train_X, train_y, val_X=None, val_y=None, test_X=test_X, num_rounds=100)\nout_df = pd.DataFrame(preds)\nout_df.columns = [\"high\", \"medium\", \"low\"]\nout_df[\"listing_id\"] = test_df.listing_id.values\nout_df.to_csv(\"xgb_baseline3.csv\", index=False)","21a788a4":"cv_scores = []\ntest_pred = None\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\nfor dev_index, val_index in kf.split(range(train_X.shape[0])):\n        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n        dev_y, val_y = train_y[dev_index], train_y[val_index]\n        preds, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, num_rounds=2000)\n        \n        if test_pred is None:\n            test_pred = preds\n        else:\n            test_pred += preds\ntest_pred \/= 5\nout_df = pd.DataFrame(test_pred)\nout_df.columns = [\"high\", \"medium\", \"low\"]\nout_df[\"listing_id\"] = test_df.listing_id.values\nout_df.to_csv(\"xgb_baseline3.csv\", index=False)","4024f75c":"# Two-Simg\u4ee3\u7801","906dba2f":"## Ordinal Encoding","f4352cef":"## Mean\/Target Encoding","ea858cbe":"## Frequency Encoding\u3001Count Encoding","76cfe8c8":"## LabelEncoder","1d17d298":"# \u7279\u5f81\u7f16\u7801","4c6f587f":"## BinaryEncoder","a908ed06":"## Onehot"}}