{"cell_type":{"2bfa66d6":"code","95a29bbd":"code","19377e58":"code","a4f6ecbc":"code","37954ada":"code","3ab35eac":"code","a1766654":"code","871f338e":"code","f035b29e":"code","59ab681a":"code","1ab6deff":"code","647e2bcd":"code","9ac5f960":"code","dc9643ac":"code","31b4ea93":"code","d3c74f96":"code","9b814a79":"code","88592420":"code","5a7a0840":"code","8cf7cef8":"code","f552a9dd":"code","cb72d3f3":"code","01fbbcc5":"code","0dad7ebd":"code","0c479705":"code","e4701fd8":"code","ed830f63":"code","584816ff":"code","ec52b967":"code","d4486c5f":"code","eb22064c":"code","10f5c659":"code","f5f01ff1":"markdown","6f02a926":"markdown","1fc3b9bd":"markdown","ba0df339":"markdown","7776b82a":"markdown","234e7d88":"markdown","e1e636de":"markdown","cf1179c0":"markdown","09719570":"markdown","27172542":"markdown","05bcae0e":"markdown","5fc45a84":"markdown","3122aa77":"markdown","4ce89962":"markdown","6343b3e5":"markdown","54e3c93e":"markdown","e01cec7f":"markdown","2639a104":"markdown","b933b67f":"markdown","8815d7b4":"markdown","5de1fbc4":"markdown","cb9d57ad":"markdown"},"source":{"2bfa66d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\npd.options.display.max_colwidth = 150","95a29bbd":"# graphics imports\nimport plotly\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud,STOPWORDS\n\n# Natural language tool kits\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n# download stopwords\nnltk.download('stopwords')\n\n# string operations\nimport string \nimport re\n\n# general imports\nimport math","19377e58":"# load data\ndf = pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndf.head()","a4f6ecbc":"\nlens = df['review'].str.len()\n\nfig = go.Figure()\nfig.add_trace(\n    go.Histogram(x=lens, xbins=dict(size=200))\n    )\nfig.update_layout(title='Length of reviews', \n                    xaxis_title=\"Length\",\n                    yaxis_title=\"# of reviews\")\nplotly.offline.iplot(fig)","37954ada":"poslens = df[df['sentiment']=='positive']['review'].str.len()\nneglens = df[df['sentiment']=='negative']['review'].str.len()\nfig = go.Figure()\nfig.add_trace(\n    go.Histogram(x=poslens, xbins=dict(size=200), name='positive'),\n    )\nfig.add_trace(\n    go.Histogram(x=neglens, xbins=dict(size=200), name='negative'),\n    )\nfig.update_layout(title='Length of reviews', \n                    xaxis_title=\"Length\",\n                    yaxis_title=\"# of reviews\",)\nplotly.offline.iplot(fig)","3ab35eac":"df_pos = df[df['sentiment']=='positive']['review']\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_pos))\n\nplt.figure(1,figsize=(15, 15))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.show()","a1766654":"df_neg = df[df['sentiment']=='negative']['review']\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_neg))\n\nplt.figure(1,figsize=(15, 15))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.show()","871f338e":"# the text mode is enough...\ndf['sentiment'].value_counts()","f035b29e":"# show the reviews again... \ndf[['review']].head(20)","59ab681a":"df['review_lw'] = df['review'].str.lower()\ndf[['review','review_lw']].head(10)","1ab6deff":"sw = stopwords.words('english')\n\nprint(f'Stopwords sample: {sw[0:10]}')\nprint(f'Number of stopwords: {len(sw)}')","647e2bcd":"print(f'Punctuation {string.punctuation}')","9ac5f960":"def transform_text(s):\n    \n    # remove html\n    html=re.compile(r'<.*?>')\n    s = html.sub(r'',s)\n    \n    # remove numbers\n    s = re.sub(r'\\d+', '', s)\n    \n    # remove punctuation\n    # remove stopwords\n    tokens = nltk.word_tokenize(s)\n    \n    new_string = []\n    for w in tokens:\n        # remove words with len = 2 AND stopwords\n        if len(w) > 2 and w not in sw:\n            new_string.append(w)\n    \n    \n    \n    s = ' '.join(new_string)\n    s = s.strip()\n\n    exclude = set(string.punctuation)\n    s = ''.join(ch for ch in s if ch not in exclude)\n    \n    return s.strip()","dc9643ac":"transform_text('there is a tree near <br\/> the river 123! see')","31b4ea93":"df['review_sw'] = df['review_lw'].apply(transform_text)\ndf[['review','review_lw', 'review_sw']].head(20)","d3c74f96":"lemmatizer = WordNetLemmatizer() \n\nprint(lemmatizer.lemmatize(\"rocks\", pos=\"v\"))\nprint(lemmatizer.lemmatize(\"gone\", pos=\"v\"))","9b814a79":"def lemmatizer_text(s):\n    tokens = nltk.word_tokenize(s)\n    \n    new_string = []\n    for w in tokens:\n        lem = lemmatizer.lemmatize(w, pos=\"v\")\n        # exclude if lenght of lemma is smaller than 2\n        if len(lem) > 2:\n            new_string.append(lem)\n    \n    s = ' '.join(new_string)\n    return s.strip()","88592420":"df['review_lm'] = df['review_sw'].apply(lemmatizer_text)\ndf[['review','review_lw', 'review_sw', 'review_lm']].head(20)","5a7a0840":"df_pos = df[df['sentiment']=='positive']['review_lm']\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_pos))\n\nplt.figure(1,figsize=(15, 15))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.show()","8cf7cef8":"df_neg = df[df['sentiment']=='negative']['review_lm']\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_neg))\n\nplt.figure(1,figsize=(15, 15))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.show()","f552a9dd":"# There are 25.000 reviews for each outcome, so we can use the first 17.500 (70%) for training and 7.500 (30%) remaining for testing\n\n# Train dataset (first 17.500 rows)\npos_train = df[df['sentiment']=='positive'][['review_lm', 'sentiment']].head(17500)\nneg_train = df[df['sentiment']=='negative'][['review_lm', 'sentiment']].head(17500)\n\n\n# Test dataset (last 7.500 rows)\npos_test = df[df['sentiment']=='positive'][['review_lm', 'sentiment']].tail(7500)\nneg_test = df[df['sentiment']=='negative'][['review_lm', 'sentiment']].tail(7500)\n\n# put all toghether again...\ntrain_df = pd.concat([pos_train, neg_train]).sample(frac = 1).reset_index(drop=True)\ntest_df = pd.concat([pos_test, neg_test]).sample(frac = 1).reset_index(drop=True)\n","cb72d3f3":"train_df.head()","01fbbcc5":"test_df.head()","0dad7ebd":"def get_word_counts(words):\n    word_counts = {}\n    for word in words:\n        word_counts[word] = word_counts.get(word, 0.0) + 1.0\n    return word_counts\n\ndef fit(df_fit):\n    num_messages = {}\n    log_class_priors = {}\n    word_counts = {}\n    vocab = set()\n \n    n = df_fit.shape[0]\n    num_messages['positive'] = df_fit[df_fit['sentiment']=='positive'].shape[0]\n    num_messages['negative'] = df_fit[df_fit['sentiment']=='negative'].shape[0]\n    log_class_priors['positive'] = math.log(num_messages['positive'] \/ n)\n    log_class_priors['negative'] = math.log(num_messages['negative'] \/ n)\n    word_counts['positive'] = {}\n    word_counts['negative'] = {}\n \n    for x, y in zip(df_fit['review_lm'], df_fit['sentiment']):\n        \n        counts = get_word_counts(nltk.word_tokenize(x))\n        for word, count in counts.items():\n            if word not in vocab:\n                vocab.add(word)\n            if word not in word_counts[y]:\n                word_counts[y][word] = 0.0\n \n            word_counts[y][word] += count\n    \n    return word_counts, log_class_priors, vocab, num_messages","0c479705":"word_counts, log_class_priors, vocab, num_messages = fit(train_df)","e4701fd8":"word_count_df = pd.DataFrame(word_counts).fillna(0).sort_values(by='positive', ascending=False).reset_index()\nword_count_df","ed830f63":"# Let's see how some words are distributed\nword_count_sample_df = word_count_df.head(5000)\nfig = go.Figure(go.Scatter(\n    x = word_count_sample_df['positive'],\n    y = word_count_sample_df['negative'],\n    text = word_count_sample_df['index'],\n    mode='markers'\n))\nfig.update_layout(title='Word distribution sample', \n                xaxis_title=\"Positive word count\",\n                yaxis_title=\"Negative word count\",)\n\nplotly.offline.iplot(fig)         ","584816ff":"def predict(df_predict, vocab, word_counts, num_messages, log_class_priors):\n    result = []\n    for x in df_predict:\n        counts = get_word_counts(nltk.word_tokenize(x))\n        positive_score = 0\n        negative_score = 0\n        for word, _ in counts.items():\n            if word not in vocab: continue\n            \n            # add Laplace smoothing\n            log_w_given_positive = math.log((word_counts['positive'].get(word, 0.0) + 1) \/ (num_messages['positive'] + len(vocab)) )\n            log_w_given_negative= math.log((word_counts['negative'].get(word, 0.0) + 1) \/ (num_messages['negative'] + len(vocab)) )\n \n            positive_score += log_w_given_positive\n            negative_score += log_w_given_negative\n \n        positive_score += log_class_priors['positive']\n        negative_score += log_class_priors['negative']\n \n        if positive_score > negative_score:\n            result.append('positive')\n        else:\n            result.append('negative')\n    return result","ec52b967":"result = predict(test_df['review_lm'], vocab, word_counts, num_messages, log_class_priors)\nresult[0:10] # result sample...","d4486c5f":"y_true = test_df['sentiment'].tolist()\n\nacc = sum(1 for i in range(len(y_true)) if result[i] == y_true[i]) \/ float(len(y_true))\nprint(\"{0:.4f}\".format(acc))","eb22064c":"y_actu = pd.Series(y_true, name='Real')\ny_pred = pd.Series(result, name='Predicted')\ndf_confusion = pd.crosstab(y_actu, y_pred)\ndf_confusion = df_confusion \/ df_confusion.sum(axis=1) * 100\ndf_confusion.round(2)","10f5c659":"def plot_confusion_matrix(df_confusion, title='Confusion matrix'):\n    plt.matshow(df_confusion) # imshow\n    #plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(df_confusion.columns))\n    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n    plt.yticks(tick_marks, df_confusion.index)\n    #plt.tight_layout()\n    plt.ylabel(df_confusion.index.name)\n    plt.xlabel(df_confusion.columns.name)    \n\nplot_confusion_matrix(df_confusion)","f5f01ff1":"## Now I wanna see again the word cloud with treated text","6f02a926":"# Sentiment analysis of IMDB comments using a Bayesian classifier (from scratch)\n\n![IMDB Logo](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/6\/69\/IMDB_Logo_2016.svg\/250px-IMDB_Logo_2016.svg.png)\n\n**What's IMDB?**\n\n### From wikipedia (https:\/\/en.wikipedia.org\/wiki\/IMDb):\n\n> \"IMDb (also known as the Internet Movie Database) is an online database, owned by Amazon, of information related to films, television programs, home videos, video games, and streaming content online \u2013 including cast, production crew and personal biographies, plot summaries, trivia, ratings, and fan and critical reviews. An additional fan feature, message boards, was abandoned in February 2017. Originally a fan-operated website, the database is owned and operated by IMDb.com, Inc., a subsidiary of Amazon. As of January 2020, IMDb has approximately 6.5 million titles (including episodes) and 10.4 million personalities in its database, as well as 83 million registered users.\"\n\n## The task:\n\n### Given a set of text and its classification (positive or negative), build a Bayesian classifier without any type of machine learing framework or libraries (sklearn, keras etc)... ok ok, I'll use pandas... but it's basic, ok?\n\n## Attention: This work has a purely didactic purpose\n\n## 1st ask:\n### Please: vote positively if you liked this work\n\n![upvote](https:\/\/i.imgflip.com\/39skgl.jpg)\n\n\n## 2nd ask:\n### Please: comment critically (doubts or suggestions) if you found something that can be improved in this work\n\n\n## Table of Contents\n\n### 1. [Probability *super quick* review](#quickreview)\n### 2. [Exploratory Data Analysis](#eda)\n### 3. [Data preparation](#datapreparation)\n### 4. [Naive bayes method](#bayes)\n","1fc3b9bd":"## 4th question: How is the database balancing?","ba0df339":"# Naive bayes method <a name=\"bayes\"><\/a>\n\n\n![](https:\/\/timtyler.org\/bayesianism\/graphics\/bayes_theorem.jpg)","7776b82a":"## The predict method\n\nNow let's create the method to predict whether the reviews are positive or negative","234e7d88":"## Now let's finally measure the accuracy of the model ...\n\n### Accuracy","e1e636de":"# Probability *super quick* review  <a name=\"quickreview\"><\/a>\n\n**Q1: What is the simplest definition of probability?**\n\n**A**: It is a number that varies between 0 and 1 that indicates the ratio between the **number of elements in the desired outcome** AND the **total number of outcomes**\ne.g.:\n\n* Given a 6-sided \"fair die\": what is the probability that in 1 throw it will result in number 3?\n\n* Possible results: {1, 2, 3, 4, 5, 6} (6 elements)\n* Desired outcome: {3} (1 element)\n* Answer: 1 outcome in 6 possible outcomes or 1\/6 or ~ 0.167\n\n**Q2 What is conditional probability?**\n\n**A** It is the calculation that takes place when we want to find the probability of an outcome GIVEN that a previous related event occurred\n\ne.g.: What is the probability of the outcome being 3 GIVEN you know that the result is odd?\n\n* Possible results: {1, 2, 3, 4, 5, 6} (6 elements)\n* Possible odd results: {1, 3, 5}\n* Desired outcome: {3} (1 element)\n* Answer: 1 outcome in 3 odd results or 1\/3 or ~ 0.333\n\n**Formally:**\n\n#### $ p(A | B) = p(A \\cap B)\/p(B)$\nor \n#### $ p(B | A) = p(A \\cap B)\/p(A)$\n\n\n**Q3 How can we derive from the definition of conditional probability to the bayes probability?**\n\n**A** Like this:\n\n#### $ p(A | B) = p(A \\cap B)\/p(B)$ => $ p(A | B).p(B) = p(A \\cap B)$\n\nand\n\n#### $ p(B | A) = p(A \\cap B)\/p(A)$ => $ p(B | A).p(A) = p(A \\cap B)$\n\nso...\n\n####  p(A | B).p(B) = p(B | A).p(A)\n\nor...\n\n###  $p(A | B)= \\frac{p(B | A).p(A)}{p(B)}$\n\n\nAnd that's enough to get the job started ...","cf1179c0":"# Thank you!!!\n![](https:\/\/i.pinimg.com\/originals\/2c\/2e\/ef\/2c2eef8da1285d958914eef079f9b70c.jpg)","09719570":"## 3rd question: what will we see in the word cloud? (review content)\n","27172542":"## 2nd question: what is the length of the reviews per outcome?","05bcae0e":"## Train and test dataset\n\nNow...we will separate the dataset in 2 parts: train and test. As we know, the base is balanced (50% for each outcome), then we will take the first 70% of each outcome for training and the remaining 30% for testing.","5fc45a84":"### Step 2: remove stopwords 'n punctuation","3122aa77":"### Step 1: Transform to lowercase","4ce89962":"### The classifier below was inspired by the video below ... watch! it is very cool!!! (Thanks statquest!)\n\n\n[![](http:\/\/img.youtube.com\/vi\/O2L2Uv9pdDA\/0.jpg)](http:\/\/www.youtube.com\/watch?v=O2L2Uv9pdDA \"Naive Bayes...clearly explained!!!\")","6343b3e5":"We will now calculate the probability of each revision to be positive or negative using the naive bayes method. According to the formula in section 1 we have:\n\n\n## $p(A | B)= \\frac{p(B | A).p(A)}{p(B)}$\n\nWe can transform the above formula into the format ...\n\n## $p(positive | w_1, w_2, ...w_n)= \\frac{p(w_1 | positive).p(w_2 | positive)....p(w_n | positive).p(positive)}{p(w_1, w_2, ...w_n)}$\n\n... in the same way\n\n## $p(negative | w_1, w_2, ...w_n)= \\frac{p(w_1 | negative).p(w_2 | negative)....p(w_n | positive).p(negative)}{p(w_1, w_2, ...w_n)}$\n\nWhere $w_1, w_2, ...w_n$ are the words of the review.\n\nNote that the denominator of both probabilities are the same ... so we can remove it to preserve processing. Thus, instead of calculating the probabilities, we will calculate a score that will be proportional to the probability:\n\n\n### $p(positive | w_1, w_2, ...w_n) \\: \\alpha \\: p(w_1 | positive).p(w_2 | positive)....p(w_n | positive).p(positive)$\n\n### $p(negative | w_1, w_2, ...w_n) \\: \\alpha \\: p(w_1 | negative).p(w_2 | negative)....p(w_n | negative).p(negative)$\n\n\nAs we are going to work with really really small numbers, we may be interested in working with its logarithm, thus transforming multiplication into sum:\n\n### $log(p(positive | w_1, w_2, ...w_n)) \\: \\alpha \\: log(p(w_1 | positive)) + log (p(w_2 | positive)) .... log(p(w_n | positive)) + log(p(positive))$\n\n### $log(p(negative | w_1, w_2, ...w_n)) \\: \\alpha \\: log(p(w_1 | negative)) + log (p(w_2 | negative)) .... log(p(w_n | negative)) + log(p(negative))$\n\n\n**Question: How do we know if a comment is positive or negative?**\n\n**Answer:** So simples...\n\nif $log(p(positive | w_1, w_2, ...w_n)) > log(p(negative | w_1, w_2, ...w_n))$ it is a positive review, otherwise... it is a negative review\n\n\nFollows the reference to the classifier code (it has been simplified for teaching purposes):\n\nref.: https:\/\/pythonmachinelearning.pro\/text-classification-tutorial-with-naive-bayes\/","54e3c93e":"# Data preparation (cleaning, transformation...)<a name=\"datapreparation\"><\/a>\n\n## Data cleaning: removing unnecessary items from the text\n\n\n**Q: What are the unnecessary items?**\n\n**A:** Stowords, punctuation, numbers, html tags","e01cec7f":"# Exploratory Data Analysis  <a name=\"eda\"><\/a>","2639a104":"### Confusion matrix","b933b67f":"## The fit method\n\nNow it's time to implement the method to \"train\" the model.\nThanks again https:\/\/pythonmachinelearning.pro\/text-classification-tutorial-with-naive-bayes\/","8815d7b4":"### Let's see some results of fit method ...","5de1fbc4":"### Step 3: lemmatizer","cb9d57ad":"## 1st question: what is the length of the reviews?"}}