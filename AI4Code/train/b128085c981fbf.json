{"cell_type":{"4ab0e2e0":"code","117d6d9b":"code","161f46d0":"code","365bd558":"code","33365c73":"markdown","f10e7d97":"markdown","abf2d4a0":"markdown","3b6432a2":"markdown","9752070e":"markdown","c36bec79":"markdown"},"source":{"4ab0e2e0":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\nfrom keras.utils import to_categorical","117d6d9b":"model = Sequential()\n\nn_cols = data.shape[1]\nmodel.add(Dense(5, activation='relu', input_shape=n_cols))  # input shape has to be the same as number of columns\nmodel.add(Dense(5, activation='relu'))\nmodel.add(Dense(1))  # output layer\n\nmodel.compile(optimizer='adam', loss='mean_square_error') \n# adam is more effcicient than gradient descent.\n# it adapts the learning rate automatically\n\nmodel.fit(predictors, target)\npredictions = model.predict(test_data)","161f46d0":"model = Sequential()\n\nn_cols = data.shape[1]\ntarget = to_categorical(target)\n\nmodel.add(Dense(5, activation='relu', input_shape=n_cols)) \nmodel.add(Dropout(0.2))  # dropout is a regularization technique to prevent overfitting. Normally ~0.2-0.4\nmodel.add(Dense(5, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation='softmax'))  \n# for classification the last layer has an activation function which ussually is softmax\n# in addition, the output dimension has to be the same as the calsses in the target\n\nmodel.compile(optimizer='adam', \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])  # to measure accuracy in classification\n\nmodel.fit(predictors, target, \n          epochs=20,  # number of iterations\n          batch_size=50,  # size of the splitted bachs (this only works with SGD?)\n          validation_split=0.2, )\npredictions = model.predict(test_data)","365bd558":"model = Sequential()\n\ninput_shape = (N, N, 3)  # 3 for RGB images and 1 for gray scale images\n\nmodel.add(Conv2D(16, kernel_size=(2, 2),  # size of the filter to use\n                 strides=(1, 1),  # steps the filter is moved\n                 activation='relu', \n                 input_shape=input_shape)) \nmodel.add(MaxPool2D(pool_size(2, 2),  strides=(1, 1))\nmodel.add(Conv2D(16, kernel_size=(2, 2), strides=(1, 1), activation='relu') \nmodel.add(MaxPool2D(pool_size(2, 2))  \nmodel.add(Flatten())  # so the data can proceed to the fully-connected layer\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(num_classes, activation='sotmax'))\n\nmodel.compile(optimizer='adam', \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])  # to measure accuracy in classification\n\nmodel.fit(predictors, target)\npredictions = model.predict(test_data)","33365c73":"## Recurrent Neural Networks (RNN)  - supervised\n\nThis are networks with loops that take into account dependency of data like images in a movie.","f10e7d97":"## Classification","abf2d4a0":"## Convolutional Neural Networks (CNN) - supervised\n\nThis are mainly use for images as they reduce dimensionality. Check this [link](https:\/\/courses.edx.org\/courses\/course-v1:IBM+DL0101EN+3T2019\/courseware\/89227024130b43f684d95376901b65c8\/052a444d45914712a597f0c58cbc4391\/?child=first)","3b6432a2":"# **Neural Networks Summary**","9752070e":"## Autoencoders  - unsupervised\n\nThese commpress and decompress functions learned from data. For this reason they are data-specific.\n\nThese are used in data de-noising and dimensionality reduction for data visualisation.","c36bec79":"## Regression"}}