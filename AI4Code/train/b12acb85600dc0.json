{"cell_type":{"d76e1504":"code","0bd2c620":"code","8cebfbe4":"code","7193a4ca":"code","8b5782b2":"code","d821c5bc":"code","9060f345":"code","e6dbe4f0":"code","d4ddac31":"code","2224c305":"code","cd3fcf58":"code","7a20d7c7":"code","d875b143":"code","01081c0e":"code","a7697634":"code","c7a6cafb":"code","0bdff02d":"code","75ea2c9f":"code","80918a0c":"code","52376bc0":"code","1d37aa17":"code","d4f345de":"code","505a5366":"code","5087d5fe":"code","278bb6c3":"code","3d77bd65":"markdown","0b660479":"markdown","d11c16fc":"markdown","04c1e954":"markdown","70764a49":"markdown","998e3e24":"markdown","268e6d72":"markdown","a95d9a1d":"markdown","c28992ba":"markdown","2a00f464":"markdown","72652d61":"markdown","50abe84d":"markdown","fec58a61":"markdown"},"source":{"d76e1504":"import numpy as np\nimport pickle\nimport cv2\nfrom os import listdir\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Activation, Flatten, Dropout, Dense\nfrom keras import backend as K\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import img_to_array\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","0bd2c620":"import os\nos.listdir(\"..\/input\/plantdisease\/plantvillage\/PlantVillage\")","8cebfbe4":"EPOCHS = 5\nINIT_LR = 1e-3\nBS = 32\ndefault_image_size = tuple((128,128))\nimage_size = 0\ndirectory_root = '..\/input\/plantdisease\/plantvillage'\nwidth=128\nheight=128\ndepth=3","7193a4ca":"def convert_image_to_array(image_dir):\n    try:\n        image = cv2.imread(image_dir)\n        if image is not None :\n            image = cv2.resize(image, default_image_size)   \n            return img_to_array(image)\n        else :\n            return np.array([])\n    except Exception as e:\n        print(f\"Error : {e}\")\n        return None\nimage_list, label_list = [], []\ntry:\n    print(\"[INFO] Loading images ...\")\n    root_dir = listdir(directory_root)\n    for directory in root_dir :\n        # remove .DS_Store from list\n        if directory == \".DS_Store\" :\n            root_dir.remove(directory)\n\n    for plant_folder in root_dir :\n        plant_disease_folder_list = listdir(f\"{directory_root}\/{plant_folder}\")\n        \n        for disease_folder in plant_disease_folder_list :\n            # remove .DS_Store from list\n            if disease_folder == \".DS_Store\" :\n                plant_disease_folder_list.remove(disease_folder)\n\n        for plant_disease_folder in plant_disease_folder_list:\n            print(f\"[INFO] Processing {plant_disease_folder} ...\")\n            plant_disease_image_list = listdir(f\"{directory_root}\/{plant_folder}\/{plant_disease_folder}\/\")\n                \n            for single_plant_disease_image in plant_disease_image_list :\n                if single_plant_disease_image == \".DS_Store\" :\n                    plant_disease_image_list.remove(single_plant_disease_image)\n\n            for image in plant_disease_image_list[:200]:\n                image_directory = f\"{directory_root}\/{plant_folder}\/{plant_disease_folder}\/{image}\"\n                if image_directory.endswith(\".jpg\") == True or image_directory.endswith(\".JPG\") == True:\n                    image_list.append(convert_image_to_array(image_directory))\n                    label_list.append(plant_disease_folder)\n    print(\"[INFO] Image loading completed\")  \nexcept Exception as e:\n    print(f\"Error --- : {e}\")","8b5782b2":"image_size = len(image_list)\nimage_size","d821c5bc":"label_binarizer = LabelBinarizer()\nimage_labels = label_binarizer.fit_transform(label_list)\npickle.dump(label_binarizer,open('label_transform.pkl', 'wb'))\nn_classes = len(label_binarizer.classes_)\nprint(label_binarizer.classes_)","9060f345":"np_image_list = np.array(image_list, dtype=np.float16) \/ 225.0\nprint(\"[INFO] Spliting data to train, test\")\nx_train, x_test, y_train, y_test = train_test_split(np_image_list, image_labels, test_size=0.2, random_state = 42) ","e6dbe4f0":"aug = ImageDataGenerator(\n    rotation_range=25, width_shift_range=0.1,\n    height_shift_range=0.1, shear_range=0.2, \n    zoom_range=0.2,horizontal_flip=True, \n    fill_mode=\"nearest\")","d4ddac31":"model = Sequential()\ninputShape = (height, width, depth)\nchanDim = -1\nif K.image_data_format() == \"channels_first\":\n    inputShape = (depth, height, width)\n    chanDim = 1\nmodel.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(1024))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(n_classes))\nmodel.add(Activation(\"softmax\"))\nmodel.summary()","2224c305":"opt = Adam(lr=INIT_LR, decay=INIT_LR \/ EPOCHS)\n# distribution\nmodel.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n# train the network\nprint(\"[INFO] training network...\")","cd3fcf58":"history = model.fit_generator(\n    aug.flow(x_train, y_train, batch_size=BS),\n    validation_data=(x_test, y_test),\n    steps_per_epoch=len(x_train) \/\/ BS,\n    epochs=5, verbose=1\n    )","7a20d7c7":"def Res_Plot_Test_Save(name):\n    print(\"[INFO] Plotting model accuracy and Loss\")\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(acc) + 1)\n    #Train and validation accuracy\n    plt.plot(epochs, acc, 'b', label='Training accurarcy')\n    plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\n    plt.title('Training and Validation accurarcy')\n    plt.legend()\n\n    plt.figure()\n    #Train and validation loss\n    plt.plot(epochs, loss, 'b', label='Training loss')\n    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n    plt.title('Training and Validation loss')\n    plt.legend()\n    plt.show()\n    \n    print(\"[INFO] Calculating model accuracy\")\n    scores = model.evaluate(x_test, y_test)\n    print(f\"Test Accuracy: {scores[1]*100}\")\n    \n    #print(\"[INFO] Saving model\")\n    #model.save(name)\n    ","d875b143":"Res_Plot_Test_Save('penta_CNN_Algorithm.h5')","01081c0e":"from keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import Dense\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.utils.np_utils import to_categorical\n\n# Get the InceptionV3 model so we can do transfer learning\nbase_inception = InceptionV3(weights='imagenet', include_top=False, \n                             input_shape=(128, 128, 3))\n                             \n# Add a global spatial average pooling layer\nout = base_inception.output\nout = GlobalAveragePooling2D()(out)\nout = Dense(512, activation='relu')(out)\nout = Dense(512, activation='relu')(out)\npredictions = Dense(n_classes, activation='softmax')(out)\nmodel = Model(inputs=base_inception.input, outputs=predictions)\n\n# only if we want to freeze layers\nfor layer in base_inception.layers:\n    layer.trainable = False\n    \n# Compile \nopt = Adam(lr=INIT_LR, decay=INIT_LR \/ EPOCHS)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"]) \nmodel.summary()","a7697634":"history = model.fit_generator(\n    aug.flow(x_train, y_train, batch_size=BS),\n    validation_data=(x_test, y_test),\n    steps_per_epoch=len(x_train) \/\/ BS,\n    epochs=EPOCHS, verbose=1\n    )","c7a6cafb":"Res_Plot_Test_Save('Inception_ModelV3_(Imagenet_Weights).h5')","0bdff02d":"from keras.models import Model\nfrom keras import applications\nfrom keras.optimizers import Adam\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import Dense\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.utils.np_utils import to_categorical\n\n# Get the Resnet model so we can do transfer learning\nbase_model = applications.resnet50.ResNet50(weights= None, include_top=False, input_shape= (128,128,3))\n                             \n# Add a global spatial average pooling layer\nout = base_model.output\nout = GlobalAveragePooling2D()(out)\nout = Dense(512, activation='relu')(out)\nout = Dense(512, activation='relu')(out)\npredictions = Dense(n_classes, activation='softmax')(out)\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# only if we want to freeze layers\nfor layer in base_inception.layers:\n    layer.trainable = False\n    \n# Compile \nopt = Adam(lr=INIT_LR, decay=INIT_LR \/ EPOCHS)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"]) \nmodel.summary()","75ea2c9f":"history = model.fit_generator(\n    aug.flow(x_train, y_train, batch_size=BS),\n    validation_data=(x_test, y_test),\n    steps_per_epoch=len(x_train) \/\/ BS,\n    epochs=5, verbose=1\n    )","80918a0c":"Res_Plot_Test_Save('Resnet_With_Imagenet_Weingts.h5')","52376bc0":"from keras.models import Model\nfrom keras.applications import vgg16\nfrom keras.optimizers import Adam\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import Dense\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.utils.np_utils import to_categorical\n\n# Get the Resnet model so we can do transfer learning\nvgg = vgg16.VGG16(weights= None, include_top=False, input_shape= (128,128,3))\n                             \n# Add a global spatial average pooling layer\nout = base_model.output\nout = GlobalAveragePooling2D()(out)\nout = Dense(512, activation='relu')(out)\nout = Dense(512, activation='relu')(out)\npredictions = Dense(n_classes, activation='softmax')(out)\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# only if we want to freeze layers\nfor layer in base_inception.layers:\n    layer.trainable = False\n    \n# Compile \nopt = Adam(lr=INIT_LR, decay=INIT_LR \/ EPOCHS)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"]) \nmodel.summary()","1d37aa17":"history = model.fit_generator(\n    aug.flow(x_train, y_train, batch_size=BS),\n    validation_data=(x_test, y_test),\n    steps_per_epoch=len(x_train) \/\/ BS,\n    epochs=5, verbose=1\n    )","d4f345de":"Res_Plot_Test_Save('VGG16_With_Imagenet_Weingts.h5')","505a5366":"from keras.models import Model\nfrom keras.applications import vgg19\nfrom keras.optimizers import Adam\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import Dense\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.utils.np_utils import to_categorical\n\n# Get the Resnet model so we can do transfer learning\nvgg = vgg19.VGG19(weights= None, include_top=False, input_shape= (128,128,3))\n                             \n# Add a global spatial average pooling layer\nout = base_model.output\nout = GlobalAveragePooling2D()(out)\nout = Dense(512, activation='relu')(out)\nout = Dense(512, activation='relu')(out)\npredictions = Dense(n_classes, activation='softmax')(out)\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# only if we want to freeze layers\nfor layer in base_inception.layers:\n    layer.trainable = False\n    \n# Compile \nopt = Adam(lr=INIT_LR, decay=INIT_LR \/ EPOCHS)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"]) \nmodel.summary()","5087d5fe":"history = model.fit_generator(\n    aug.flow(x_train, y_train, batch_size=BS),\n    validation_data=(x_test, y_test),\n    steps_per_epoch=len(x_train) \/\/ BS,\n    epochs=15, verbose=1\n    )","278bb6c3":"Res_Plot_Test_Save('VGG19_Imagenet.h5')","3d77bd65":"# ***5 Layer CNN Algorithm ***","0b660479":"# **Optimizer**","d11c16fc":"### **Resnet Model**\n\n##### Weights : Imagenet","04c1e954":"**For References Check out**\n[https:\/\/towardsdatascience.com\/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a]","70764a49":"# **Results and Plots**","998e3e24":"# **Augmentation Of data**","268e6d72":"### **VGG16 Model**\n\n##### Weights : Imagenet","a95d9a1d":"# Transfer Learning Challenges\nTransfer learning has immense potential and is a commonly required enhancement for existing learning algorithms. Yet, there are certain pertinent issues related to transfer learning that need more research and exploration. Apart from the difficulty of answering the questions of what, when, and how to transfer, negative transfer and transfer bounds present major challenges.\n* Negative Transfer: The cases we have discussed so far talk about improvements in target tasks based on knowledge transfer from the source task. There are cases when transfer learning can lead to a drop in performance. Negative transfer refers to scenarios where the transfer of knowledge from the source to the target does not lead to any improvement, but rather causes a drop in the overall performance of the target task. There can be various reasons for negative transfer, such as cases when the source task is not sufficiently related to the target task, or if the transfer method could not leverage the relationship between the source and target tasks very well. Avoiding negative transfer is very important and requires careful investigation. In their work, Rosenstien and their co-authors present empirically how brute-force transfer degrades performance in target tasks when the source and target are too dissimilar. Bayesian approaches by Bakker and their co-authors, along with other techniques exploring clustering-based solutions to identify relatedness, are being researched to avoid negative transfers.\n* Transfer Bounds: Quantifying the transfer in transfer learning is also very important, that affects the quality of the transfer and its viability. To gauge the amount for the transfer, Hassan Mahmud and their co-authors used Kolmogorov complexity to prove certain theoretical bounds to analyze transfer learning and measure relatedness between tasks. Eaton and their co-authors presented a novel graph-based approach to measure knowledge transfer. Detailed discussions of these techniques are outside the scope of this article. Readers are encouraged to explore more on these topics using the publications outlined in this section!","c28992ba":"# **Normalization Of images**","2a00f464":"### **VGG19 Model**\n\n##### Weights : Imagenet","72652d61":"### **Inception ModelV3**\n\n##### Weights : Imagenet","50abe84d":"# **Hi there** ,\n\nThis Notebook will deal with an implementation of \n5 Layered CNN architecture\n* **Resnet**\n* **VGG16**\n* **VGG19**\n* **Imagnet**\n* **Inception Model**","fec58a61":"# Transfer Learning Implementation\n\n### Transfer Learning Advantages\nTypically transfer learning enables us to build more robust models which can perform a wide variety of tasks.\n* Helps solve complex real-world problems with several constraints\n* Tackle problems like having little or almost no labeled data availability\n* Ease of transfering knowledge from one model to another based on domains and tasks\n* Provides a path towards achieving Artificial General Intelligence some day in the future!"}}