{"cell_type":{"e4d94354":"code","b9021ba9":"code","819256c1":"code","fe700a68":"code","f3c56bb2":"code","3b27b54f":"code","4be99984":"code","9d9f5381":"code","b47c46b8":"code","f520c960":"code","40cea2ec":"code","d1b9c528":"code","9df848c4":"code","da1f8c45":"code","98b2bf45":"code","b741a8a8":"code","2758d089":"markdown","cb369a71":"markdown","5e890a0c":"markdown","ea4be2cc":"markdown","495577c5":"markdown","ebf9d316":"markdown","f9ede817":"markdown","d865de0b":"markdown","b41312cd":"markdown","6f71169f":"markdown","e1da21b3":"markdown","35f70521":"markdown","196ac68f":"markdown","76cb42a3":"markdown","4fd99a6f":"markdown","43d2f826":"markdown","386df76b":"markdown","5f1db7dc":"markdown","2a1489cc":"markdown","2862b395":"markdown","28acdcad":"markdown"},"source":{"e4d94354":"# we don't like warnings\n# you can comment the following 2 lines if you'd like to\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV","b9021ba9":"# loading data\ndata = pd.read_csv('..\/input\/microchip_tests.txt',\n                   header=None, names = ('test1','test2','released'))\n# getting some info about dataframe\ndata.info()","819256c1":"data.head(5)","fe700a68":"data.tail(5)","f3c56bb2":"X = data.iloc[:,:2].values\ny = data.iloc[:,2].values","3b27b54f":"plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', label='Released')\nplt.scatter(X[y == 0, 0], X[y == 0, 1], c='orange', label='Faulty')\nplt.xlabel(\"Test 1\")\nplt.ylabel(\"Test 2\")\nplt.title('2 tests of microchips. Logit with C=1')\nplt.legend();","4be99984":"def plot_boundary(clf, X, y, grid_step=.01, poly_featurizer=None):\n    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, grid_step),\n                         np.arange(y_min, y_max, grid_step))\n\n\n    # to every point from [x_min, m_max]x[y_min, y_max]\n    # we put in correspondence its own color\n    Z = clf.predict(poly_featurizer.transform(np.c_[xx.ravel(), yy.ravel()]))\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)","9d9f5381":"poly = PolynomialFeatures(degree=7)\nX_poly = poly.fit_transform(X)","b47c46b8":"X_poly.shape","f520c960":"C = 1e-2\nlogit = LogisticRegression(C=C, random_state=17)\nlogit.fit(X_poly, y)\n\nplot_boundary(logit, X, y, grid_step=.01, poly_featurizer=poly)\n\nplt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', label='Released')\nplt.scatter(X[y == 0, 0], X[y == 0, 1], c='orange', label='Faulty')\nplt.xlabel(\"Test 1\")\nplt.ylabel(\"Test 2\")\nplt.title('2 tests of microchips. Logit with C=%s' % C)\nplt.legend();\n\nprint(\"Accuracy on training set:\", \n      round(logit.score(X_poly, y), 3))","40cea2ec":"C = 1\nlogit = LogisticRegression(C=C, random_state=17)\nlogit.fit(X_poly, y)\n\nplot_boundary(logit, X, y, grid_step=.005, poly_featurizer=poly)\n\nplt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', label='Released')\nplt.scatter(X[y == 0, 0], X[y == 0, 1], c='orange', label='Faulty')\nplt.xlabel(\"Test 1\")\nplt.ylabel(\"Test 2\")\nplt.title('2 tests of microchips. Logit with C=%s' % C)\nplt.legend();\n\nprint(\"Accuracy on training set:\", \n      round(logit.score(X_poly, y), 3))","d1b9c528":"C = 1e4\nlogit = LogisticRegression(C=C, random_state=17)\nlogit.fit(X_poly, y)\n\nplot_boundary(logit, X, y, grid_step=.005, poly_featurizer=poly)\n\nplt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', label='Released')\nplt.scatter(X[y == 0, 0], X[y == 0, 1], c='orange', label='Faulty')\nplt.xlabel(\"Test 1\")\nplt.ylabel(\"Test 2\")\nplt.title('2 tests of microchips. Logit with C=%s' % C)\nplt.legend();\n\nprint(\"Accuracy on training set:\", \n      round(logit.score(X_poly, y), 3))","9df848c4":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\n\nc_values = np.logspace(-2, 3, 500)\n\nlogit_searcher = LogisticRegressionCV(Cs=c_values, cv=skf, verbose=1, n_jobs=-1)\nlogit_searcher.fit(X_poly, y)","da1f8c45":"logit_searcher.C_","98b2bf45":"plt.plot(c_values, np.mean(logit_searcher.scores_[1], axis=0))\nplt.xlabel('C')\nplt.ylabel('Mean CV-accuracy');","b741a8a8":"plt.plot(c_values, np.mean(logit_searcher.scores_[1], axis=0))\nplt.xlabel('C')\nplt.ylabel('Mean CV-accuracy');\nplt.xlim((0,10));","2758d089":"Let's inspect at the first and last 5 lines.","cb369a71":"**Regularization parameter tuning**","5e890a0c":"To discuss the results, let's rewrite the function that is optimized in logistic regression with the form:\n\n$$\\large J(X,y,w) = \\mathcal{L} + \\frac{1}{C}||w||^2,$$\n\nwhere\n\n- $\\mathcal{L}$ is the logistic loss function summed over the entire dataset\n- $C$ is the reverse regularization coefficient (the very same $C$ from `sklearn`'s implementation of `LogisticRegression`)","ea4be2cc":"Recall that these curves are called validation curves. Previously, we built them manually, but sklearn has special methods to construct these that we will use going forward.","495577c5":"We define the following polynomial features of degree $d$ for two variables $x_1$ and $x_2$:\n\n$$\\large \\{x_1^d, x_1^{d-1}x_2, \\ldots x_2^d\\} =  \\{x_1^ix_2^j\\}_{i+j=d, i,j \\in \\mathbb{N}}$$\n\nFor example, for $d=3$, this will be the following features:\n\n$$\\large 1, x_1, x_2,  x_1^2, x_1x_2, x_2^2, x_1^3, x_1^2x_2, x_1x_2^2, x_2^3$$\n\nDrawing a Pythagorean Triangle would show how many of these features there will be for $d=4,5...$ and so on.\nThe number of such features is exponentially large, and it can be costly to build polynomial features of large degree (e.g $d=10$) for 100 variables. More importantly, it's not needed. \n","ebf9d316":"Now we should save the training set and the target class labels in separate NumPy arrays.","f9ede817":"### Useful resources\n- Medium [\"story\"](https:\/\/medium.com\/open-machine-learning-course\/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220) based on this notebook\n- If you read Russian: an [article](https:\/\/habrahabr.ru\/company\/ods\/blog\/323890\/) on Habrahabr with ~ the same material. And a [lecture](https:\/\/youtu.be\/oTXGQ-_oqvI) on YouTube\n- A nice and concise overview of linear models is given in the book [\u201cDeep Learning\u201d](http:\/\/www.deeplearningbook.org) (I. Goodfellow, Y. Bengio, and A. Courville).\n- Linear models are covered practically in every ML book. We recommend \u201cPattern Recognition and Machine Learning\u201d (C. Bishop) and \u201cMachine Learning: A Probabilistic Perspective\u201d (K. Murphy).\n- If you prefer a thorough overview of linear model from a statistician\u2019s viewpoint, then look at \u201cThe elements of statistical learning\u201d (T. Hastie, R. Tibshirani, and J. Friedman).\n- The book \u201cMachine Learning in Action\u201d (P. Harrington) will walk you through implementations of classic ML algorithms in pure Python.\n- [Scikit-learn](http:\/\/scikit-learn.org\/stable\/documentation.html) library. These guys work hard on writing really clear documentation.\n- Scipy 2017 [scikit-learn tutorial](https:\/\/github.com\/amueller\/scipy-2017-sklearn) by Alex Gramfort and Andreas Mueller.\n- One more [ML course](https:\/\/github.com\/diefimov\/MTH594_MachineLearning) with very good materials.\n- [Implementations](https:\/\/github.com\/rushter\/MLAlgorithms) of many ML algorithms. Search for linear regression and logistic regression.","d865de0b":"To see how the quality of the model (percentage of correct responses on the training and validation sets) varies with the hyperparameter $C$, we can plot the graph. ","b41312cd":"Let's train logistic regression with regularization parameter $C = 10^{-2}$.","6f71169f":"Using this example, let's identify the optimal value of the regularization parameter $C$. This can be done using `LogisticRegressionCV` - a grid search of parameters followed by cross-validation. This class is designed specifically for logistic regression (effective algorithms with well-known search parameters). For an arbitrary model, use `GridSearchCV`, `RandomizedSearchCV`, or special algorithms for hyperparameter optimization such as the one implemented in `hyperopt`.","e1da21b3":"We will use `sklearn`'s implementation of logistic regression. So, we create an object that will add polynomial features up to degree 7 to matrix $X$.","35f70521":"As an intermediate step, we can plot the data. Orange points correspond to defective chips, blue to normal ones.","196ac68f":"Finally, select the area with the \"best\" values of $C$.","76cb42a3":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\">\n## Open Machine Learning Course\n<center>Author: [Yury Kashnitsky](https:\/\/www.linkedin.com\/in\/festline). Translated and edited by [Christina Butsko](https:\/\/www.linkedin.com\/in\/christinabutsko\/), [Nerses Bagiyan](https:\/\/www.linkedin.com\/in\/nersesbagiyan\/), [Yulia Klimushina](https:\/\/www.linkedin.com\/in\/yuliya-klimushina-7168a9139), and [Yuanyuan Pao](https:\/\/www.linkedin.com\/in\/yuanyuanpao\/).\n\nThis material is subject to the terms and conditions of the license [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/). Free use is permitted for any non-comercial purpose with an obligatory indication of the names of the authors and of the source.\n\n<center>Original kernel of Yury Kashitskiy is [here](https:\/\/www.kaggle.com\/kashnitsky\/topic-4-linear-models-part-3-regularization)","4fd99a6f":"**Subtotals**:\n- the larger the parameter $C$, the more complex the relationships in the data that the model can recover (intuitively $C$ corresponds to the \"complexity\" of the model - model capacity)\n- if regularization is too strong i.e. the values of $C$ are small, the solution to the problem of minimizing the logistic loss function may be the one where many of the weights are too small or zeroed. The model is also not sufficiently \"penalized\" for errors (i.e. in the function $J$, the sum of the squares of the weights \"outweighs\", and the error $\\mathcal{L}$ can be relatively large). In this case, the model will underfit as we saw in our first case.\n- on the contrary, if regularization is too weak i.e. the values of $C$ are large, a vector $w$ with high absolute value components can become the solution to the optimization problem. In this case, $\\mathcal{L}$ has a greater contribution to the optimized functional $J$. Loosely speaking, the model is too \"afraid\" to be mistaken on the objects from the training set and will therefore overfit as we saw in the third case.\n- logistic regression will not \"understand\" (or \"learn\") what value of $C$ to choose as it does with the weights $w$. That is to say, it can not be determined by solving the optimization problem in logistic regression. We have seen a similar situation before -- a decision tree can not \"learn\" what depth limit to choose during the training process. Therefore, $C$ is the a model hyperparameter that is tuned on cross-validation; so is the max_depth in a tree.","43d2f826":"In the first article, we demonstrated how polynomial features allow linear models to build nonlinear separating surfaces. Let's now show this visually.\n\nLet's see how regularization affects the quality of classification on a dataset on microchip testing from Andrew Ng's course on machine learning. We will use logistic regression with polynomial features and vary the regularization parameter $C$. First, we will see how regularization affects the separating border of the classifier and intuitively recognize under- and overfitting. Then, we will choose the regularization parameter to be numerically close to the optimal value via (`cross-validation`) and (`GridSearch`).","386df76b":"Then, why don't we increase $C$ even more - up to 10,000? Now, regularization is clearly not strong enough, and we see overfitting. Note that, with $C$=1 and a \"smooth\" boundary, the share of correct answers on the training set is not much lower than here. But one can easily imagine how our second model will work much better on new data.","5f1db7dc":"We could now try increasing $C$ to 1. In doing this, we weaken regularization, and the solution can now have greater values (in absolute value) of model weights than previously. Now the accuracy of the classifier on the training set improves to 0.831.","2a1489cc":"Let's define a function to display the separating curve of the classifier.","2862b395":"# <center>Topic 4. Linear Classification and Regression\n## <center> Part 3. An Illustrative Example of Logistic Regression Regularization","28acdcad":"Let's load the data using `read_csv` from the `pandas` library. In this dataset on 118 microchips (objects), there are results for two tests of quality control (two numerical variables) and information whether the microchip went into production. Variables are already centered, meaning that the column values have had their own mean values subtracted. Thus, the \"average\" microchip corresponds to a zero value in the test results.  "}}