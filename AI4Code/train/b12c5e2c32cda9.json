{"cell_type":{"ed01bc19":"code","6c029392":"code","2098ae82":"code","8e434571":"code","a7f25afb":"code","0df2da5f":"code","5f243397":"code","1dbd53ad":"code","8285c910":"code","8b9359dc":"code","cea1447d":"code","4cd1ef24":"code","bffdc5d4":"code","bb559ba8":"code","5c071379":"code","08335e90":"code","4f6afeff":"code","e0d06e95":"code","3317157b":"code","206bbd95":"code","ec525ce0":"code","b2070ec2":"code","6e62b9ae":"code","36b49652":"code","5f6abc26":"code","9624376d":"code","cf8fef0e":"code","a1e48f84":"code","a735513b":"code","87cb61a9":"markdown","b5c0d3ba":"markdown","165f3631":"markdown","92be2656":"markdown","3f17a80c":"markdown","b0ca57bc":"markdown","64ada335":"markdown","b8b02e4e":"markdown","1c9fd88e":"markdown","ada5ed17":"markdown","028afa00":"markdown","67bdcffa":"markdown","78deb044":"markdown","29c30bc9":"markdown","fa9cc374":"markdown"},"source":{"ed01bc19":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score","6c029392":"df = pd.read_csv('..\/input\/prostate-cancer\/Prostate_Cancer.csv')","2098ae82":"df","8e434571":"df.diagnosis_result.unique()","a7f25afb":"def encodage(df):\n    code = { 'M': 0,'B': 1}\n    for col in df.select_dtypes('object'):\n        df[col] = df[col].map(code)\n    \n    return df","0df2da5f":"encodage(df)","5f243397":"#df.shape","1dbd53ad":"df = df.drop(['id'], axis=1)","8285c910":"# Cluster map avec colin\u00e9arit\u00e9 \n\nsns.clustermap(df.corr(),annot=True)","8b9359dc":"y = df['diagnosis_result']\n# je garde toute les colonnes sauf Purchased (target)\nX = df.drop(['diagnosis_result'], axis=1)","cea1447d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nprint('Train set:', X_train.shape)\nprint('Test set:', X_test.shape)","4cd1ef24":"scaler = MinMaxScaler()  \nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","bffdc5d4":"parameters = {'algorithm':('auto', 'ball_tree', 'kd_tree', 'brute'), \n              'weights':('uniform', 'distance'), \n              'metric': ('minkowski', 'euclidean', 'manhattan' )}\n#param_grid = {'n_neighbors': np.arange(1, 25)}\n\ngrid = GridSearchCV(KNeighborsClassifier(), parameters, cv=5)\ngrid.fit(X_train,y_train)","bb559ba8":"grid.best_estimator_","5c071379":"model = KNeighborsClassifier( algorithm= 'auto',\n                             n_neighbors=5,leaf_size=30, \n                             metric='minkowski', \n                             metric_params=None, \n                             n_jobs=None, \n                             p=2, \n                             weights='uniform')","08335e90":"model.fit(X_train, y_train)\nprint('train score:', model.score(X_train, y_train))\nprint('test score:', model.score(X_test, y_test))","4f6afeff":"y_pred = model.predict(X_test)\n\nprint(classification_report(y_test, y_pred))","e0d06e95":"cv_scores = cross_val_score(model, X, y, cv=5)\n#print each cv score (accuracy) and average them\ncv_scores\nprint('cv_scores mean:{}'.format(np.mean(cv_scores)))","3317157b":"import statsmodels.api as sm ","206bbd95":"# trouver les valeurs des param\u00e8tres qui maximisent la fonction de vraisemblance\nimport statsmodels.formula.api as smf\nresult = smf.logit(\"diagnosis_result ~ perimeter + area + compactness\", data = df).fit()\nresult.summary()","ec525ce0":"from statsmodels.stats.outliers_influence import variance_inflation_factor    \n\ndef calculate_vif_(X, thresh=100):\n    cols = X.columns\n    variables = np.arange(X.shape[1])\n    dropped=True\n    while dropped:\n        dropped=False\n        c = X[cols[variables]].values\n        vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n\n        maxloc = vif.index(max(vif))\n        if max(vif) > thresh:\n            print('Supprime cette feature \\'' + X[cols[variables]].columns[maxloc] + '\\' at index: ' + str(maxloc))\n            variables = np.delete(variables, maxloc)\n            dropped=True\n\n    print('Garde ces variables:')\n    print(X.columns[variables])\n    return X[cols[variables]]","b2070ec2":"df_final = calculate_vif_(X, thresh=100)\n#df_final = df_final.drop(['id'], axis=1)","6e62b9ae":"y = df['diagnosis_result']\n# je garde toute les colonnes sauf Purchased (target)\nX_final = df_final","36b49652":"X_train_final, X_test_final, y_train, y_test = train_test_split(X_final, y, test_size=0.3, random_state=0)\n\nprint('Train set:', X_train_final.shape)\nprint('Test set:', X_test_final.shape)","5f6abc26":"scaler = MinMaxScaler()  \nX_train_final = scaler.fit_transform(X_train_final)\nX_test_final = scaler.transform(X_test_final)","9624376d":"model_final = KNeighborsClassifier(algorithm= 'auto',\n                             n_neighbors=5,leaf_size=30, \n                             metric='minkowski', \n                             metric_params=None, \n                             n_jobs=None, \n                             p=2, \n                             weights='uniform')","cf8fef0e":"model_final.fit(X_train_final, y_train)\nprint('train score:', model_final.score(X_train_final, y_train))\nprint('test score:', model_final.score(X_test_final, y_test))","a1e48f84":"y_pred_final = model_final.predict(X_test_final)\n\nprint(classification_report(y_test, y_pred_final))","a735513b":"plot_confusion_matrix(model_final, X_test_final, y_test)","87cb61a9":"### On red\u00e9fini notre X avec les features s\u00e9lectionn\u00e9es","b5c0d3ba":"### On drop la colonne id qui n'est pas n\u00e9cessaire","165f3631":"### KNN classificateur model avec optimisation des hyperparam\u00e8tres","92be2656":"### On fait un grid search","3f17a80c":"## S\u00e9lection de features","b0ca57bc":"### On d\u00e9finis les features et la cible","64ada335":"## Encodage de la target","b8b02e4e":"### On standardise nos donn\u00e9es","1c9fd88e":"### On check la colin\u00e9arit\u00e9","ada5ed17":"### On check la colin\u00e9arit\u00e9 avec statmodels","028afa00":"## Heatmap corr\u00e9lation","67bdcffa":"\n## Sans s\u00e9lection de feature","78deb044":"### On s\u00e9pare les donn\u00e9es d'entrainement et de test","29c30bc9":"# Model KNN","fa9cc374":"# Data"}}