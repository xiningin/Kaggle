{"cell_type":{"6dd332a7":"code","6168dc28":"code","8f63675a":"code","8d682697":"code","8b478e4b":"code","22812fed":"code","ade65ad0":"markdown","9358e169":"markdown"},"source":{"6dd332a7":"import pandas as pd\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver","6168dc28":"categories = {\n    100: 'Politics',\n    101: 'Economy',\n    102: 'Society',\n    103: 'Life and Culture',\n    104: 'World',\n    105: 'IT\/Science',  \n}","8f63675a":"def get_articles(category, date, page):\n    url = 'https:\/\/news.naver.com\/main\/list.nhn?\\\n    mode=LSD&mid=sec&sid1={}&\\\n    date={}&page={}'.format(category, date, page)\n    options = webdriver.ChromeOptions()\n    options.add_argument('headless')\n    driver = webdriver.Chrome(options=options)\n    driver.get(url)\n    articles = driver.find_elements_by_css_selector('.list_body li')\n    datas = []\n    for article in articles:\n\n        title = article.find_element_by_css_selector('dt:not(.photo)').text,\n        link = article.find_element_by_css_selector('a').get_attribute('href')\n        response = requests.get(link, headers=headers)\n        dom = BeautifulSoup(response.content, 'html.parser')\n        try:\n            content = dom.select_one('#articleBodyContents').text\n            content = content.replace('\\n', '').replace('\\t', '').replace(\"\\'\", \"'\").strip()\n        except Exception as e:\n            print('Content Error')\n            \n        datas.append({\n            'title': title,\n            'link': link,\n            'content': content,\n            'category': category,\n        })\n    driver.quit()\n    return pd.DataFrame(datas)","8d682697":"dfs = []\ndate = 20210316\n\nfor category in categories:\n    print(categories[category], end=' ')\n    for page in range(1, 6):\n        print(page, end=' ')\n        df = get_articles(category, date, page)\n        dfs.append(df)\n    print()\n    \narticles_df = pd.concat(dfs, ignore_index=True)\narticles_df.tail()","8b478e4b":"import pygsheets\ngs = pygsheets.authorize(client_secret='client_secret.json')\nf = gs.open('naver_article')\nws = f.add_worksheet('2021-03-16')\nws.set_dataframe(articles_df, 'A1')","22812fed":"articles_df.to_csv('article_20210316.csv', index=False, encoding='utf-8-sig')","ade65ad0":"### Requirements\n1. chrome browser & driver\n2. selenium, beautifulsoup, requests\nmove you chrome driver to your python env path, or your working dir!","9358e169":"# Korean news crawler\n\n\n\nI hope this source code can help you guys! and...\n###          Do not abuse!!"}}