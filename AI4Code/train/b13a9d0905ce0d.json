{"cell_type":{"f94db6c1":"code","cfb0d109":"code","00384bb1":"code","d672ee32":"code","b6151622":"code","f44fd382":"code","5517d359":"code","8ec90377":"code","a7ce1433":"code","0fdf7148":"code","1948e23c":"code","ee8d9add":"code","0c69c6a8":"code","833a0a03":"code","562f44a2":"code","0262acce":"code","e592dec5":"code","4f7ffb3a":"code","f328666c":"code","79e10925":"code","ec2fbd50":"code","642605fc":"code","fb9939ae":"code","fd22928e":"code","cb081a3f":"code","a2b6e120":"code","1fa17474":"code","123a0163":"code","e5f64203":"code","12e020db":"code","eb89ffa4":"code","a25dce72":"code","b8627f5c":"code","d2e24dc4":"code","21e81698":"code","87dde5ce":"code","d76e15a5":"code","797b8e5e":"code","355a387f":"code","93722901":"markdown","bef3d802":"markdown","1027150d":"markdown","0bf9b27c":"markdown","557ebaed":"markdown","bcb30209":"markdown","12a8f073":"markdown","3b55467f":"markdown","e8ba730f":"markdown","166f02e4":"markdown","1b6842f9":"markdown","28748eda":"markdown","44a832d1":"markdown","0714925b":"markdown","1b5c8db3":"markdown","1004a082":"markdown","45e91413":"markdown","b674abff":"markdown","4eeba19a":"markdown","f3544d7a":"markdown","ee162d14":"markdown"},"source":{"f94db6c1":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport random as rnd\nimport re\nimport string\nimport operator\nimport numpy as np\nimport pandas as pd\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer, word_tokenize\n\n\nfrom keras.models import Sequential\nfrom keras.initializers import Constant\nfrom keras.layers import (LSTM, \n                          Embedding, \n                          BatchNormalization,\n                          SpatialDropout1D,\n                          Dense, \n                          Dropout)\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.optimizers import Adam\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    precision_score, \n    recall_score, \n    f1_score, \n    classification_report,\n    accuracy_score)\n\nimport tensorflow as tf\nfrom clr_callback import *","cfb0d109":"rnd.seed(42) \nnp.random.seed(42)\ntf.random.set_seed(42)","00384bb1":"df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\ndf.shape, df_test.shape","d672ee32":"df.loc[df['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target'] = 0\ndf.loc[df['text'] == 'Hellfire is surrounded by desires so be careful and don\u0089\u00db\u00aat let your desires control you! #Afterlife', 'target'] = 0\ndf.loc[df['text'] == 'To fight bioterrorism sir.', 'target'] = 0\ndf.loc[df['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https:\/\/t.co\/rqWuoy1fm4', 'target'] = 1\ndf.loc[df['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97\/Georgia Ave Silver Spring', 'target'] = 1\ndf.loc[df['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target'] = 0\ndf.loc[df['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target'] = 0\ndf.loc[df['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target'] = 1\ndf.loc[df['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http:\/\/t.co\/JlzK2HdeTG', 'target'] = 1\ndf.loc[df['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target'] = 0\ndf.loc[df['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target'] = 0\ndf.loc[df['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target'] = 0\ndf.loc[df['text'] == \"Hellfire! We don\u0089\u00db\u00aat even want to think about it or mention it so let\u0089\u00db\u00aas not do anything that leads to it #islam!\", 'target'] = 0\ndf.loc[df['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target'] = 0\ndf.loc[df['text'] == \"Caution: breathing may be hazardous to your health.\", 'target'] = 1\ndf.loc[df['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target'] = 0\ndf.loc[df['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target'] = 0\ndf.loc[df['text'] == \"that horrible sinking feeling when you\u0089\u00db\u00aave been at home on your phone for a while and you realise its been on 3G this whole time\", 'target'] = 0","b6151622":"def clean(tweet):             \n    # Special characters\n    tweet = re.sub(r\"\\x89\u00db_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d3\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cf\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00f7\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00aa\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c8\", \"\", tweet)\n    tweet = re.sub(r\"Jap\u00cc_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"\u00cc\u00a9\", \"e\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a8\", \"\", tweet)\n    tweet = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c7\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a33million\", \"3 million\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c0\", \"\", tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"don\u00e5\u00abt\", \"do not\", tweet)   \n            \n    # Character entity references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # Typos, slang and informal abbreviations\n    tweet = re.sub(r\"w\/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w\/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8\/5\/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8\/6\/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    \n    # Acronyms\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n    tweet = re.sub(r\"m\u00cc\u00bcsica\", \"music\", tweet)\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \n    \n    # Grouping same words without embeddings\n    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n    \n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # remove HTML tags\n    tweet = re.sub(r'<.*?>', '', tweet)\n    # remove words with digits (and emojis)\n    tweet = re.sub('\\w*\\d\\w*', '', tweet)\n    # remove punctuations\n    table = str.maketrans('', '', string.punctuation)\n    tweet =  tweet.translate(table)\n    \n    return tweet","f44fd382":"%%time\ndf['text'] = df['text'].apply(clean)\ndf_test['text'] = df_test['text'].apply(clean)","5517d359":"y_train = np.array(df['target']).tolist()","8ec90377":"%%time\nembedding_dim = 100\nembedding_dict = {}\n\nwith open(f'..\/input\/glove-global-vectors-for-word-representation\/glove.twitter.27B.{embedding_dim}d.txt','r') as f:\n    \n    for line in f:\n        \n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:],'float32')\n        embedding_dict[word] = vectors\n        \nf.close()","a7ce1433":"print(f\"There are {len(embedding_dict.keys())} tokens in this embedding representation\")","0fdf7148":"stopwords = set(stopwords.words('english'))","1948e23c":"%%time\ndef tokenize_corpus(df,  stopwords):\n    \n    corpus = []\n    \n    for tweet in df['text']:\n        words = [word.lower() for word in word_tokenize(tweet) if ((word.isalpha() == 1) & (word not in stopwords))]\n        corpus.append(words)\n        \n    return corpus","ee8d9add":"train_corpus = tokenize_corpus(df, stopwords)\nlen(train_corpus)","0c69c6a8":"test_corpus = tokenize_corpus(df_test, stopwords)\nlen(test_corpus)","833a0a03":"def build_vocab(tweets):\n       \n    vocab = {}\n    \n    for tweet in tweets:\n        for word in tweet:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1                \n    return vocab\n\ndef check_embeddings_coverage(X, embeddings):\n    \n    vocab = build_vocab(X)    \n    \n    covered = {}\n    oov = {}    \n    n_covered = 0\n    n_oov = 0\n    \n    for word in vocab:\n        try:\n            covered[word] = embeddings[word]\n            n_covered += vocab[word]\n        except:\n            oov[word] = vocab[word]\n            n_oov += vocab[word]\n            \n    vocab_coverage = len(covered) \/ len(vocab)\n    text_coverage = (n_covered \/ (n_covered + n_oov))\n    \n    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n    return sorted_oov, vocab_coverage, text_coverage","562f44a2":"train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(train_corpus, embedding_dict)\ntest_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(test_corpus, embedding_dict)\n\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))","0262acce":"MAX_LEN = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(train_corpus)\nsequences = tokenizer_obj.texts_to_sequences(train_corpus)\n\ntweet_pad = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')","e592dec5":"test_sequences = tokenizer_obj.texts_to_sequences(test_corpus)\ntest_pads = pad_sequences(test_sequences, maxlen=MAX_LEN, truncating='post', padding='post')","4f7ffb3a":"word_index = tokenizer_obj.word_index\nprint('Number of unique tokens:',len(word_index))","f328666c":"num_words = len(word_index) + 1 # +1 refer to pad token\nembedding_matrix = np.zeros((num_words, embedding_dim))\n\nfor word,i in word_index.items():\n    emb_vec = embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec","79e10925":"len(embedding_matrix), num_words","ec2fbd50":"len(tweet_pad), len(y_train)","642605fc":"X, X_val, y, y_val = train_test_split(tweet_pad, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\nprint('Length of train:', len(X))\nprint(\"Length of val:\",   len(X_val))","fb9939ae":"X = np.asarray(X)\ny = np.asarray(y)\n\nX_val = np.asarray(X_val)\ny_val = np.asarray(y_val)","fd22928e":"np.unique(y_val, return_counts=True)","cb081a3f":"model=Sequential()\n\nembedding=Embedding(num_words,\n                    embedding_dim,\n                    embeddings_initializer=Constant(embedding_matrix),\n                    input_length=MAX_LEN,\n                    trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimzer, metrics=['accuracy'])\n","a2b6e120":"model.summary()","1fa17474":"clr = CyclicLR(mode      = \"triangular\",\n               base_lr   = 1e-5,\n               max_lr    = 1e-4,\n               step_size = 25)","123a0163":"early_stopping_callback = EarlyStopping(monitor='loss', mode='min', patience=5)","e5f64203":"model_checkpoint_callback = ModelCheckpoint(\n    filepath='best_model.h5',\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)","12e020db":"history = model.fit(X, y, \n                    batch_size=64, \n                    epochs=100, \n                    validation_data=(X_val, y_val), \n                    callbacks=[clr, early_stopping_callback, model_checkpoint_callback],\n                    shuffle=True)","eb89ffa4":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","a25dce72":"#model.load_weights('best_model.h5')","b8627f5c":"def metrics(pred_tag, y_test):\n    print(\"F1-score: \", f1_score(pred_tag, y_test))\n    print(\"Precision: \", precision_score(pred_tag, y_test))\n    print(\"Recall: \", recall_score(pred_tag, y_test))\n    print(\"Acuracy: \", accuracy_score(pred_tag, y_test))\n    print(\"-\"*50)\n    print(classification_report(pred_tag, y_test))","d2e24dc4":"preds = model.predict_classes(X_val)\n\nmetrics(preds, y_val)","21e81698":"sample_sub = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nperfect_sub = pd.read_csv('..\/input\/perfect-submissiondisastertweetsnlp\/perfect_submission.csv')","87dde5ce":"preds = model.predict_classes(test_pads)\npreds = preds.astype(int).reshape(3263)\nlen(preds)","d76e15a5":"metrics(preds, perfect_sub['target'])","797b8e5e":"sub = pd.DataFrame({'id': sample_sub['id'].values.tolist(),'target': preds})\nsub.head()","355a387f":"sub.to_csv('submission.csv',index=False)","93722901":"## Create embedding matrix","bef3d802":"# Compare with perfect submission","1027150d":" # Acknowledgements\n \n This kernel was based in multiple great kernels that i found at this competition,\n \n Thank you all.\n \n [Cheatsheet - Text Helper Functions](https:\/\/www.kaggle.com\/raenish\/cheatsheet-text-helper-functions)\n \n [NLP with Disaster Tweets - EDA, Cleaning and BERT](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n \n [NLP (Disaster Tweets) with Glove and LSTM](https:\/\/www.kaggle.com\/mariapushkareva\/nlp-disaster-tweets-with-glove-and-lstm)\n \n [Glove biLSTM](https:\/\/www.kaggle.com\/xwalker\/glove-bilstm)","0bf9b27c":"## Editing Mislabeled Samples","557ebaed":"# Building the model","bcb30209":"# Building vocab","12a8f073":"# What's in this kernel?\n\n- Data Cleaning\n- A simple LSTM model using keras and [GloVe](https:\/\/nlp.stanford.edu\/projects\/glove\/) pre trained embeddings ","3b55467f":"## Training ","e8ba730f":"# Importing libraries","166f02e4":"## Checking embedding coverage","1b6842f9":"# Creating embedding dict from glove","28748eda":"# Submission","44a832d1":"# Intro\n\n","0714925b":"## Tweet Cleaning","1b5c8db3":"In this competition we have a challenge to predict if a tweet is about a real desaster or not. \n\nThere are several approachs for solve this challenge on Kaggle. I made this kernel to test some insights and start my contribution \nat Kaggle comunnity.\n\nIf you like, please do a upvote! ","1004a082":"## Creating embedding matrix","45e91413":"# Data Preparation","b674abff":"# Metrics","4eeba19a":"# Loading data","f3544d7a":"## Ensuring reproducibility\n\n`42 is the answer to everything`","ee162d14":"## Checking if we have embeddings for all tokens"}}