{"cell_type":{"71eba0cb":"code","763511b9":"code","51a0a313":"code","16eac78d":"code","a66ba5c6":"code","150ca4cb":"code","6ad6162f":"code","c95c78b5":"code","61bd0626":"code","f3f1351f":"code","a46779aa":"code","d11793b3":"code","b3fc6d63":"code","11590b48":"code","e64937cf":"code","41925252":"code","44fa5792":"code","4bc785b1":"code","1fa4199e":"code","aec8700d":"code","eb593b49":"code","48499219":"code","9e8b155f":"code","e477605f":"code","b2afce49":"code","a25d0688":"code","c03e5f53":"code","f69a6eb3":"code","fa4feb33":"code","1607d885":"code","da0f7611":"code","96e090d1":"code","1cde1cfc":"code","10a23a73":"code","8a80e5a3":"code","11431f79":"code","c2465831":"code","68e546e9":"code","f5995b41":"markdown","d14d61f7":"markdown","ae07f7f8":"markdown","701bf0f0":"markdown","e4e01147":"markdown","6c2c3269":"markdown","8d3704e6":"markdown","b25fa701":"markdown","fc84be7f":"markdown","7a4b3911":"markdown","18923227":"markdown","81b95f47":"markdown","f34e665d":"markdown","0c3c0a35":"markdown"},"source":{"71eba0cb":"import numpy as np \nimport pandas as pd \n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\n\nimport math\nimport matplotlib.pyplot as plt","763511b9":"train = pd.read_csv('..\/input\/sales_train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nitems = pd.read_csv('..\/input\/items.csv')\nitem_cats = pd.read_csv('..\/input\/item_categories.csv')\nshops = pd.read_csv('..\/input\/shops.csv')","51a0a313":"train.head()","16eac78d":"test.head()","a66ba5c6":"train_agg = train.drop(['date', 'item_price'], axis=1)\ntrain_agg.describe()","150ca4cb":"# Sum up item_cnt_day grouped by shop_id and item_id and date_block_num to get unique rows of items sold per month. \n\ndf = train_agg.groupby([\"shop_id\", \"item_id\", \"date_block_num\"])\n\nmonthly = df.aggregate({\"item_cnt_day\":np.sum}).fillna(0)\nmonthly.reset_index(level=[\"shop_id\", \"item_id\", \"date_block_num\"], inplace=True)\nmonthly = monthly.rename(columns={ monthly.columns[3]: \"item_cnt_month\" })","6ad6162f":"monthly.describe()","c95c78b5":"monthly['item_id'].value_counts()\/34","61bd0626":"test['item_id'].value_counts()","f3f1351f":"monthly['shop_id'].loc[monthly['item_id'] == 5822].value_counts().sort_index()","a46779aa":"test['shop_id'].loc[test['item_id'] == 5822].value_counts().sort_index()","d11793b3":"monthly['shop_id'].value_counts()","b3fc6d63":"test['shop_id'].value_counts()","11590b48":"monthly.describe()","e64937cf":"test.describe()","41925252":"train_simple = monthly.drop('date_block_num', axis=1)\n#shuffle rows\ntrain_simple = train_simple.sample(frac=1).reset_index(drop=True)\n\nX_simple = train_simple[['shop_id', 'item_id']]\ny_simple = train_simple['item_cnt_month']","44fa5792":"def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n\nn_valid = 214200\nn_trn = len(train_simple) - n_valid\nX_train, X_valid = split_vals(X_simple, n_trn)\ny_train, y_valid = split_vals(y_simple, n_trn)","4bc785b1":"plt.scatter(X_valid.iloc[:100,1], y_valid[:100], color='black')","1fa4199e":"m = RandomForestRegressor(n_estimators=1, n_jobs=-1)\n%time m.fit(X_train, y_train)","aec8700d":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)\n\nprint_score(m)","eb593b49":"plt.scatter(X_valid.iloc[:100,1], m.predict(X_valid)[:100], color='black')","48499219":"m_2 = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n%time m_2.fit(X_train, y_train)\n%time print_score(m_2)","9e8b155f":"preds = np.stack([t.predict(X_valid) for t in m_2.estimators_])","e477605f":"plt.plot([metrics.mean_squared_error(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(100)]);","b2afce49":"X_simple = train_simple[['shop_id', 'item_id']]\ny_simple = train_simple['item_cnt_month'].clip(0,20)","a25d0688":"n_valid = 214200\nn_trn = len(train_simple) - n_valid\nX_train, X_valid = split_vals(X_simple, n_trn)\ny_train, y_valid = split_vals(y_simple, n_trn)","c03e5f53":"m = RandomForestRegressor(n_estimators=1, n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_score(m)","f69a6eb3":"m_2 = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n%time m_2.fit(X_train, y_train)\nprint_score(m_2)","fa4feb33":"preds = np.stack([t.predict(X_valid) for t in m_2.estimators_])\nplt.plot([metrics.mean_squared_error(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(100)]);","1607d885":"pd.DataFrame(m_2.predict(X_valid)).describe()","da0f7611":"train_td = monthly.sort_values(by=[\"date_block_num\"])\nvalid_td = monthly[monthly[\"date_block_num\"] == 33]\n\nX_train = train_td[['shop_id', 'item_id']]\ny_train = train_td['item_cnt_month'].clip(0,20)\nX_valid = valid_td[['shop_id', 'item_id']]\ny_valid = valid_td['item_cnt_month'].clip(0,20)","96e090d1":"m_3 = RandomForestRegressor(n_estimators=60, n_jobs=-1)\n%time m_3.fit(X_train, y_train)\nprint_score(m_3)","1cde1cfc":"preds = np.stack([t.predict(X_valid) for t in m_3.estimators_])\nplt.plot([metrics.mean_squared_error(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(60)]);","10a23a73":"import xgboost as xgb\nparam = {'max_depth':12,  # originally 10\n         'subsample':1,  # 1\n         'min_child_weight':0.5,  # 0.5\n         'eta':0.3,\n         'num_round':1000, \n         'seed':0,  # 1\n         'silent':0,\n         'eval_metric':'rmse',\n         'early_stopping_rounds':100\n        }\n\nprogress = dict()\nxgbtrain = xgb.DMatrix(X_train, y_train)\nwatchlist  = [(xgbtrain,'train-rmse')]\nm_4 = xgb.train(param, xgbtrain)","8a80e5a3":"preds = m_4.predict(xgb.DMatrix(X_valid))\n\nrmse = np.sqrt(mean_squared_error(preds, y_valid))\nprint(rmse)","11431f79":"new_submission = pd.merge(month_sum, test, how='right', left_on=['shop_id','item_id'], right_on = ['shop_id','item_id']).fillna(0)\nnew_submission.drop(['shop_id', 'item_id'], axis=1)\nnew_submission = new_submission[['ID','item_cnt_month']]","c2465831":"new_submission['item_cnt_month'] = new_submission['item_cnt_month'].clip(0,20)\nnew_submission.describe()","68e546e9":"new_submission.to_csv('previous_value_benchmark.csv', index=False)","f5995b41":"# Things to try\n* using daily values to predict, then aggregate the predictions and use the extra data for modeling\n* add a column to indicate number of days since last sale for each item\n* mapping categories of items (using the other csv files)\n* onehotencoding date_block_num for items sold to reflect the months where nothing is sold\n* adding column indicating the year\n* leave in item_price, map it to test set","d14d61f7":"Looking good so far. Quick sanity check - how many shops does item_id=5822 appear in?","ae07f7f8":"# Introduction\n\nBuilding on my first simple benchmark kernel EDA where I used October 2015 sales to predict November 2015 sales (found here: https:\/\/www.kaggle.com\/alexyau\/previous-value-benchmark-simple-eda ), I will now try building some simple models.","701bf0f0":"The model is significantly better when the target values are clipped within [0,20], with RMSE of 2.17 from 100-tree random forest. Compared to the benchmark kernel using October 2015 as prediction for November 2015, our model does not score as well. This is definitely because we have built a model with data that is not dependent on time.\n\nLet us now see what happens when time dependency is put back into the data. We can start by sorting the dataset by the date_block_num column, and splitting the October 2015 dataset for validation.","e4e01147":"The results seem poor - RMSE of 6.8 and R^2 of 0.38 on the validation set. But this is a good baseline to improve from. A random forest with many trees should definitely perform better.","6c2c3269":"It appears that the test set has structured its data so that it has a total of 42 shops, with each shop containing 5100 of the same items. \n\nFrom the historical training data, we are not sure how many items each shop contains, but records show that some shops have sold far less than 5100 unique items - perhaps some items in stock never get sold. Also there may be a few shops that sell much more than the unique item count in the test set - this can be explained by some products being discontinued from the store over time with new ones added recently. \n\n","8d3704e6":"Quite a bit better. RMSE of 1.71 with random forest of 60 trees. \n\n## Improved model? - XGBoost\n\nLet's now try to build an XGBoost model.","b25fa701":"Just a slight improvement here using a random forest regressor with 100 trees - RMSE of 6.7 and R^2 of 0.4. \n\n### Improving the model\n\nLet's try to clip the item_cnt_month values into [0,20] for both the training and validation set to see how much the models improve.","fc84be7f":"Now, we have the same problem as before, where the training set and testing set consist of different columns. \n\nThe test set contains only the shop_id and item_id from the training set, but the training set also includes a daily date, the date_block_num indicating consecutive month since January 2013, as well as item_price.\n\nFor simplicity's sake, and to probe the feature relationships, we will try to build a model with only shop_id and item_id. \nThe dependent variable will be the aggregated item_cnt_day counts (i.e. item_cnt_month) for each value of date_block_num.","7a4b3911":"## Train\/validation split\n\nNow that we have some more real-world insight on the differences between the train and test set and cleaned the train set a bit, we can make the train\/validation split. We will want to structure the validation set roughly the same as the test set for our model. \n\nNormally for time-series data we will want to sort the data by date and make some latter portion of the data the validation set. Since we will try to work without the dates first, we can arbitrarily portion 214200 of the 1.61 million rows for validation, the same 'size' as the test set. Working without dates, we can drop the date_block_num column and shuffle the rows in the train set as we like.","18923227":"## Basic model - decision tree regression\n\nJudging by the discrete nature of the data in the dependent feature that we are predicting, the number of sales per item\/shop, a linear or polynomial regression model would not work since there is no direct collinearity between the item_id\/shop_id and the number of sales. Therefore we will build decision tree regression models that can predict the sales based on learning past behavior of sales for certain items in certain shops. \n\nWe'll start with a single tree and see how it performs.","81b95f47":"There seems to be more shops in the training set than in the test set for some items. \n\nThis makes sense if you figure out that some shops may have closed over time. \n","f34e665d":"## EDA\nWe'll start here by splitting the data and arranging it in order to fit the model. ","0c3c0a35":"Column item_cnt_day in the training set has a min of -22 and max of 2169. This might be a realistic number where 22 items were returned or 2169 items were sold in one day, but may be outliers for the prediction model. Let's keep note of this - later we will clip the item_cnt_day numbers like before between something like [0,20] and see if the results improve."}}