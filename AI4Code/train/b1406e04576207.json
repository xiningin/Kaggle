{"cell_type":{"8d1164e8":"code","bcb2f475":"code","10672563":"code","cd33e0e5":"code","66258b75":"code","bdd63dda":"code","c22c4815":"code","157ed5df":"code","4c4a48d7":"code","e2d32b99":"code","fd557ded":"code","24278077":"code","87677e06":"code","8d8e6de6":"code","ae5ebb74":"code","d4c33e9f":"code","0c241eb1":"code","b029f3f5":"code","264a47d6":"code","7b42c5fc":"code","c11ccbfe":"code","db30b259":"code","34bb6f04":"code","3f858d86":"code","2eaeeb01":"code","5293e1da":"code","31efe560":"code","83a9691f":"code","10bb3011":"code","31b8a92d":"markdown","bfec45fe":"markdown","95ebbfa7":"markdown","42ce335e":"markdown","be03b11c":"markdown","0f73dbc8":"markdown","83c06a64":"markdown","ccf3b0d4":"markdown","50d756f0":"markdown","93f28128":"markdown","5a2fd58b":"markdown","ec23c402":"markdown","9ff9afc0":"markdown","5d181a48":"markdown","a8aaf1e1":"markdown","c17af62e":"markdown","d2a4b3af":"markdown","43fd175c":"markdown","86ea6484":"markdown"},"source":{"8d1164e8":"#PACKAGES WILL BE NEED FOR UTILITY\n\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_validate\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        path = os.path.join(dirname, filename)\n        print(os.path.join(dirname, filename))\n\n#READ THE DATA\ndata = pd.read_csv(path)","bcb2f475":"data=data.iloc[:,1:]\ndata.head(4)","10672563":"data.describe()\ndata.isnull().sum()\n#data = data.dropna()\n#data = data.reset_index(drop=True)","cd33e0e5":"BDI = data.iloc[:,5:]\n#SUM ALL BDI SCORES FOR BDI TOTAL SCORE\nBDI[\"BDI TOTAL\"] = BDI.sum(axis=1)    ","66258b75":"#DROP BDI 1 - BDI 21 FROM DATASET AND CONCAT BDI TOTAL\n\ndataset = data.iloc[:,:5]\nBDI_SUM = BDI.iloc[:,-1:]\ndf=pd.concat([dataset, BDI_SUM], axis=1)\ndf.head(4)","bdd63dda":"df.isnull().sum()","c22c4815":"from sklearn import preprocessing\nohe= preprocessing.OneHotEncoder()\n\ngender = df.iloc[:,:1]\ngender_hot = ohe.fit_transform(gender).toarray()\n\neducation = df.iloc[:,1:2]\neducation_hot = ohe.fit_transform(education).toarray()\n\nworking = df.iloc[:,2:3]\nworking_hot = ohe.fit_transform(working).toarray()\n\nmarriage = df.iloc[:,3:4]\nmarriage_hot = ohe.fit_transform(marriage).toarray()\n\nsiblings = df.iloc[:,4:5]\nsiblings_hot = ohe.fit_transform(siblings).toarray()\n","157ed5df":"dfgender =pd.DataFrame(data=gender_hot, index=range(gender.shape[0]), columns= [\"female\",\"male\"])\ndfeducation = pd.DataFrame(data=education_hot, index=range(gender.shape[0]), columns= [\"primary\",\"high scool\",\"Bachelor\",\"Msc or PhD\"])\ndfworking =pd.DataFrame(data=working_hot, index=range(working.shape[0]), columns= [\"employed\",\"unemployed\"])\ndfmarriage =pd.DataFrame(data=marriage_hot, index=range(marriage.shape[0]), columns= [\"arranged\",\"flirt\"])\ndfchild =pd.DataFrame(data=siblings_hot, index=range(siblings.shape[0]), columns= [\"Have kids\",\"Not Have kids\"])\n\ndf=pd.concat([dfgender,dfeducation,dfworking,dfmarriage,dfchild],axis=1)","4c4a48d7":"x=pd.concat([dfgender,dfeducation,dfworking,dfmarriage,dfchild],axis=1)\ny= BDI.iloc[:,-1:]\ndata = pd.concat([x,y],axis=1)","e2d32b99":"korelasyon=data.corr()\nfigure, axis=plt.subplots(figsize=(10,10))\nsns.heatmap(korelasyon, annot=True)","fd557ded":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=4)","24278077":"from sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg.fit(x_train,y_train)\ny_pred = reg.predict(x_test)\nr2_score(y_test, y_pred)","87677e06":"from sklearn.tree import DecisionTreeRegressor\nRTD = DecisionTreeRegressor(random_state = 0)\nRTD.fit(x_train, y_train)\ny_pred = RTD.predict(x_test)\nr2_score(y_test, y_pred)","8d8e6de6":"from sklearn.ensemble import RandomForestRegressor\n\nRFR = RandomForestRegressor(n_estimators=10,random_state = 0 )\nRFR.fit(x_train, y_train)\ny_pred=RFR.predict(x_test)\nr2_score(y_test, y_pred)","ae5ebb74":"from sklearn.svm import SVR\n\nSVR_Reg = SVR(kernel = \"rbf\",degree=3, C=40)\nSVR_Reg.fit(x_train, y_train)\ny_pred = SVR_Reg.predict(x_test)\nr2_score(y_test, y_pred)","d4c33e9f":"cv_sonuc= cross_validate(RFR, x, y, cv=5 , scoring=('r2', 'neg_mean_squared_error'))\nres=cv_sonuc['test_r2'].mean()\nprint(\"R2 Score is: \", res*100, \"%\")","0c241eb1":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score as ass\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\n\n#Linear Discriminant Analysis k\u00fct\u00fcphaneleri\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.covariance import LedoitWolf\nfrom sklearn.covariance import MinCovDet\nfrom sklearn.covariance import OAS\nfrom sklearn.covariance import GraphicalLasso","b029f3f5":"import warnings\nwarnings.filterwarnings(\"ignore\")\nbins = [-np.inf, 10, 16, 20, 30, 40, np.inf]\nlabels = [\"normal\", \"mid-mood\",\"borderline\",\"moderate-depression\",\"severe-depression\",\"extreme-depression\"]\n\ny['binned BDI'] = pd.cut(y['BDI TOTAL'], bins = bins, labels = labels)\ny.head(81)","264a47d6":"y=y.iloc[:,-1:]\ny.isnull().sum()","7b42c5fc":"k_nn=KNeighborsClassifier(n_neighbors=3, metric=\"chebyshev\")\nlogi = LogisticRegression(random_state=5)\nDT = DecisionTreeClassifier(max_features=\"sqrt\")\nSDF = SGDClassifier(penalty=\"l2\", random_state=10)\nS_VC= SVC(degree=3,C=8, kernel=\"rbf\")\nRF= RandomForestClassifier(n_estimators=78, criterion= \"gini\") # criterion = \"gini\" or \"entropy\"\nBayes=  GaussianNB()\nMBayes = MultinomialNB()\nBBayes = BernoulliNB()\nLDA = LinearDiscriminantAnalysis(solver=\"eigen\")    #solver= \u2018svd\u2019, \u2018lsqr\u2019, \u2018eigen\u2019\nResult =[]","c11ccbfe":"cv_sonuc= cross_validate(k_nn, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of KNN: \", res*100, \"%\")\nResult.append( \"KNN :\")\nResult.append( res)","db30b259":"cv_sonuc= cross_validate(SDF, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of SDG: \", res*100, \"%\")\n\nResult = []\nResult.append( \"SDG :\")\nResult.append( res)","34bb6f04":"cv_sonuc= cross_validate(logi, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of Logistic Regression: \", res*100, \"%\")\nResult.append( \"LR :\")\nResult.append( res)","3f858d86":"cv_sonuc= cross_validate(DT, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\n\nprint(\"Accuracy of Decision Tree: \", res*100, \"%\")\nResult.append( \"DT :\")\nResult.append( res)","2eaeeb01":"cv_sonuc= cross_validate(S_VC, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of Support Vector Classifier: \", res*100, \"%\")\nResult.append( \"SVC :\")\nResult.append( res)","5293e1da":"cv_sonuc= cross_validate(RF, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of Random Forest: \", res*100, \"%\")\nResult.append( \"RF :\")\nResult.append( res)","31efe560":"cv_sonuc= cross_validate(Bayes, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of Naive Bayes: \", res*100, \"%\")\nResult.append( \"NB :\")\nResult.append( res)","83a9691f":"cv_sonuc= cross_validate(MBayes, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of Multinomial Naive Bayes: \", res*100, \"%\")\nResult.append( \"MNB :\")\nResult.append( res)","10bb3011":"Result","31b8a92d":"Checking the data for cleaning is essential, hopefully there is no null data in the dataframe.","bfec45fe":"**Any ideas why the score is low? Garbage Data?** Next time i will try to turn the problem into a classification task, may be it helps!","95ebbfa7":"CrossValidation\n\nSame, R square is minus, which means output and input are not related. Other words garbage in garbage out.","42ce335e":"Dicritize by bining","be03b11c":"Discritize by Bining of the target value.","0f73dbc8":"**Random Forest**\nSame, R square is minus, which means output and input are not related. Other words garbage in garbage out.","83c06a64":"# ML Models Solve as Regression Task\n\nWe will use linear regression, Decision Tree, Random Forest and Support Vector Regressor to build a model. ","ccf3b0d4":"As be seen in below, we concat the data set to gather a meaningful dataframe. Now we have 5 input features which are Gender, Education, Working Status, Marriage Style, Having kid and an output as BDI score. All inputs are categoric and the output is seem numeric. Thus, the problem turns into a **regression task**. \n\n\nHowever, we will see!. I have doubts about that may be we can turn it into sub-groups which leads the problem into a classification task. hmm interesting...","50d756f0":"**Support Vector Machines: Regressor**","93f28128":"**Linear Regression**\n\nR square is minus, which means output and input are not related. Other words garbage in garbage out.","5a2fd58b":"We have to add all colums of the points which means sum all values in the B1 to B21 to get the final BDI score of depression.","ec23c402":"# Beck's Depression Inventory \n\nThe Beck Depression Inventory (BDI) is a 21-item, self-rated scale that evaluates key symptoms of depression including mood, pessimism, sense of failure, self-dissatisfaction, guilt, punishment, self-dislike, self-accusation, suicidal ideas, crying, irritability, social withdrawal, indecisiveness, body image change, work difficulty, insomnia, fatigability, loss of appetite, weight loss, somatic preoccupation, and loss of libido (Beck & Steer, 1993; Beck, Steer & Garbing, 1988).\n\nYou can reach the test in below ( as long as the link is alive ^^ )\n\n[Beck's Depression Inventory Test](https:\/\/www.ismanet.org\/doctoryourspirit\/pdfs\/Beck-Depression-Inventory-BDI.pdf)\n\n\nCalculation is based on summing the scores to the questions. The sum of the 21 BDI score, gives the intensity of the depression. Hence, it will vary from 0 to 63.","9ff9afc0":"**Decision Tree**\n\nSame, R square is minus, which means output and input are not related. Other words garbage in garbage out.","5d181a48":"# ML Models Solve as Classification Task\n","a8aaf1e1":"\n1-10____________________These ups and downs are considered normal\n\n11-16___________________ Mild mood disturbance\n\n17-20___________________Borderline clinical depression\n\n21-30___________________Moderate depression\n\n31-40___________________Severe depression\n\nover 40__________________Extreme depression","c17af62e":"Models to compare:\n\n* K Nearest Neighbors\n* Logistic Regression\n* Decision Tree\n* Stochastic Gradient Descent\n* Support Vector Classifier\n* Random Forest\n* Gaussian Naive Bayes\n* Multinomial Naive Bayes\n* Linear Discriminant Analysis\n\nAlthough we do not examine each model in detail, it should not be forgotten that there are parameters that must be determined by cross-validation within the models. We call these parameters hyperparameters. We need to do these parameters manually or with the structures we call grid-search.","d2a4b3af":"As you see, our data is consist of 26 inputs. We drop the first column since it is just a index of the test. ","43fd175c":"Here is important, categorical values can be encoded. In our case they are already encoded but we will make it as one hot encoding. Why, Because it may help the succession of the regression model. Still not sure, it should be experienced.","86ea6484":"# Results: \n\nHighest Accuracy is 56%\n\nSDG: 0.51\n\nKNN: 0.36\n\nLogistic Regression: 0.56\n\nDecision Tree: 0.43\n\nSupport Vector Classifier: 0.51\n\nRandom Forest:: 0.49\n\nNaive Bayes - Gauissian: 0.13\n\nNaive Bayes - Multinomial: 0.56\n\nLinear Discriminant Analysis: NaN"}}