{"cell_type":{"067dbe6f":"code","33ac8e58":"code","516fa9a3":"code","3a745d74":"code","17134573":"code","7378972a":"code","df0789bb":"code","146b6c17":"code","3b620bdf":"code","6547f361":"code","eb536571":"code","df777b3c":"code","f7761329":"code","e33ac71e":"code","be9771b2":"code","a462f21a":"code","166cbe32":"code","d911b8a8":"code","e7cf3d00":"code","f9432b2e":"code","2acbcdf0":"code","3567c3a8":"code","2c27e513":"code","4657f946":"code","013c5445":"markdown","075b9b8c":"markdown","3c3cac42":"markdown","623a50f1":"markdown","3d1ab6bc":"markdown","466a2352":"markdown","de594430":"markdown","ecb6e0f9":"markdown","27889aaf":"markdown","e3f8e57d":"markdown","8fd722cd":"markdown","90072fae":"markdown","af7ec24f":"markdown","464c8ba2":"markdown","30dca80c":"markdown","0c59ca55":"markdown","4d774a0d":"markdown","3e8354f6":"markdown","94763026":"markdown","8e5f46fc":"markdown","735c1276":"markdown","081cfd9b":"markdown","3585a703":"markdown","09761389":"markdown","dd0f6bed":"markdown","a1be7543":"markdown"},"source":{"067dbe6f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","33ac8e58":"# The following code creates and train a voting classifier in sklearn, composed of 3 divers classfier on the moon dataset","516fa9a3":"import sklearn","3a745d74":"from sklearn.datasets import make_moons\nx, y = make_moons()","17134573":"x.shape, y.shape","7378972a":"from sklearn.cross_validation import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, train_size = 0.8, random_state = 12)","df0789bb":"from sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\n\nlog_clf = LogisticRegression()\nrnd_clf = RandomForestClassifier()\nsvm_clf = SVC()\n\nvoting_clf = VotingClassifier(estimators = [('lr',log_clf), ('rf',rnd_clf),('svc',svm_clf)], voting = 'hard')\n\nvoting_clf.fit(x_train, y_train)","146b6c17":"from sklearn.metrics import accuracy_score\nfor clf in (log_clf,rnd_clf,svm_clf, voting_clf):\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n    print (clf.__class__.__name__, accuracy_score(y_test, y_pred))","3b620bdf":"from sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\n\nlog_clf = LogisticRegression()\nrnd_clf = RandomForestClassifier()\nsvm_clf = SVC(probability = True)\n\nvoting_clf = VotingClassifier(estimators = [('lr',log_clf), ('rf',rnd_clf),('svc',svm_clf)], voting = 'soft')\n\nvoting_clf.fit(x_train, y_train)","6547f361":"from sklearn.metrics import accuracy_score\nfor clf in (log_clf,rnd_clf,svm_clf, voting_clf):\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n    print (clf.__class__.__name__, accuracy_score(y_test, y_pred))","eb536571":"# the following code trains an ensemble of 500 decision tree classifier, each\n# training instances randomly sampled from the training set with replacement\n\n# to use pasting set bootstrap =false\n\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nbag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators = 500,\n                          max_samples = 70, bootstrap = True, n_jobs = -1)\nbag_clf.fit(x_train, y_train)\ny_pred = bag_clf.predict(x_test)","df777b3c":"from sklearn.metrics import accuracy_score\nfor clf in (log_clf,rnd_clf,svm_clf, bag_clf):\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n    print (clf.__class__.__name__, accuracy_score(y_test, y_pred))","f7761329":"# Setting oob_score = True while creating a bagging classifier to request an \n# automatic oob evaluation after training.\n\nbag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators = 500,\n                           bootstrap = True, n_jobs = -1, oob_score = True)\nbag_clf.fit(x_train, y_train)\nbag_clf.oob_score_","e33ac71e":"from sklearn.metrics import accuracy_score\ny_pred = bag_clf.predict(x_test)\naccuracy_score(y_test, y_pred)","be9771b2":"bag_clf.oob_decision_function_","a462f21a":"from sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1)\nrnd_clf.fit(x_train, y_train)\n\ny_pred_rf = rnd_clf.predict(x_test)","166cbe32":"# the following bagging classifier is roughly equivalent to the previous randomfores classifier\n\nbag_clf = BaggingClassifier(DecisionTreeClassifier(splitter = 'random', max_leaf_nodes = 16),\n                           n_estimators = 500, max_samples = 1.0, bootstrap = True ,n_jobs = -1)","d911b8a8":"bag_clf.fit(x_train, y_train)\nrand_bag_clf = bag_clf.predict(x_test)\naccuracy_score(y_test, y_pred)","e7cf3d00":"from sklearn.ensemble import ExtraTreesClassifier\n\nextr_cls = ExtraTreesClassifier()","f9432b2e":"from sklearn.datasets import load_iris\niris = load_iris()\nrnd_clf = RandomForestClassifier(n_estimators = 500, n_jobs = -1)\nrnd_clf.fit(iris['data'], iris['target'])\nfor name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n    print (name, score)","2acbcdf0":"# weighted error rate of the Jth predictor\n\ndef weighted(x):\n    return x\n\n\ndef weighted_error_rate(data,target, predictor):\n    no_of_samples = data.shape[0]\n    predict_value = predictor.predict(data)\n    weighted_value = [weighted(i) if i != j else int(1) for i,j in zip(predict_value, target)]\n    normal_weight = [weighted(i) for i in predict_value]\n    solution = [a\/b for a,b in zip(weighted_value, normal_weight)]\n    return solution\n    ","3567c3a8":"def predicators_weight(data,target,predictor,learning_param = 0.3):\n    compute_1 = weighted_error_rate(data,target,predictor)\n    compute_2 = np.log((1-compute_1)\/compute_1)\n    return learning_param * compute_2","2c27e513":"def alpha(val):\n    return val\n\ndef updated_weights(predicted_value, value):\n    if predicted_value == value:\n        val = weighted(predicted_value)\n    else:\n        val = weighted(predicted_value) * np.exp(alpha(value))\n    return val","4657f946":"from sklearn.ensemble import AdaBoostClassifier\nada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1), n_estimators = 200, \n                            algorithm = 'SAMME.R', learning_rate = 0.5)\nada_clf.fit(x_train, y_train)","013c5445":"# out of bag evaluation","075b9b8c":"The bagging classifier class support sampling the features as well. This is controlled by two hyperparameters: max_features and bootstrap_features. They work the same way as max_samples and bootstrap, but for feature sampling instead of instance sampling.","3c3cac42":"A forest of extremely random trees is simple called an extremely randomized tree ensemble (or extra trees for short).","623a50f1":"with bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. The precentage of unsampled\ndata are called out-of-bag(oob).\n\nSince a predictor never sees the oobs instances during training, it can be evaluated on these instances, without the need for a separate validation set or cross-validation.","3d1ab6bc":" it is also possible to implement early stopping training early (instead of training\n a large number of trees first and then looking back to find the optimal number).\n we can do by setting warm_start = True\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngbrt = GradientBoostingRegressor(max_depth = 2, warm_start = True)\n\nmin_val_error = float('inf')\n\nerror_going_up = 0\n\nfor n_estimators in range(1,120):\n\n    gbrt.n_estimators = n_estimators\n    \n    gbrt.fit(x_train, y_train)\n    \n    y_pred = gbrt.predict(x_val)\n    \n    val_error = mean_squared_error(y_val, y_pred)\n    \n    if val_error < min_val_error:\n    \n        min_val_error = val_error\n        \n        error_going_up = 0\n        \n    else:\n        error_going_up += 1\n        \n        if error_going_up == 5:\n        \n            break # early stopping","466a2352":"It works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictors.","de594430":"if all classifiers are able to estimate class probablities (i.e, they have a predict_proba method), they you can tell sklearn to predict the class with the \nhighest class probability, averaged over all the individual classifiers.\n\nwe have soft voting by replacing voting = 'hard' with 'soft'","ecb6e0f9":"# Adaboost","27889aaf":"# Boosting","e3f8e57d":"# Bagging and pasting","8fd722cd":" random forest are optimized for decisiontrees\n with few exceptions, a randomforestclassifier has all the hyperparameters of a decisiontreeclassifier(to control how trees are grown), plus all the hyperparameters of a bagging classifier to control the ensemble itself.\n \n Random forest algorithm introduces extra randomness when growing trees; instead of searching for the best feature when spliting a node. It searches for the best feature among a random subset of features.","90072fae":"# Gradient boosting","af7ec24f":"# random forest classifier","464c8ba2":"# feature importance with randomforest","30dca80c":"# This kernel is a basic introduction to Ensemble learning using sklearn for newbies\n# importing neccessary libaries","0c59ca55":"# Voting classifier","4d774a0d":"It involves using the same training algorithms for every predictor, but to train\nthem on different random subsets of the training set.\n\nBagging involves sampling with replacement\npasting involves sampling without replacement","3e8354f6":"from sklearn.tree import DecisionTreeRegressor\nx, y = None, None\n\n\n\ntree_reg1 = DecisionTreeRegressor(max_depth = 2)\n\ntree_reg1.fit(x,y)\n\ny2 = y - tree_reg1.predict(x)\n\ntree_reg2 = DecisionTreeRegressor(max_depth = 2)\n\ntree_reg2.fit(x, y2)\n\ny3 = y2 - tree_reg2.predict(x)\n\ntree_reg3 = DecisionTreeRegressor(max_depth = 2)\n\ntree_reg3.fit(x,y3)\n\n\ny_pred = sum(tree.predict(x_new) for tree in (tree_reg1, tree_reg2, tree_reg3))","94763026":"suppose you have trained a few classifier, each one achieving about 80% accuracy.\nYou may have a logistic regression classifier, an SVM classifier, a random forest classifier, a K-nearest neighbor\nclassifier and perhaps a few more.\n\nA very simple way to create a even better classifier is to aggregate the predictions of each classifier\nand predict the class that gets the most votes.\n\nSomewhat, the voting classifier often achieves a higher accuracy than the best classifier in the ensemble.\n\nIn fact, even if each classifier is a weak learner (meaning it does only slightly better than random guessing), \nthe ensemble can still be a strong learner (achieving a higher accuracy), provided there are a sufficient number\nof weak learner and they sufficiently diverse.","8e5f46fc":"# bagging and pasting in sklearn","735c1276":"# the following doe trains a GBRT ensemble with 120 trees, then measures the validation\n# error at each stage of training to find the optimal number of trees, and finally trains\n# another gbrt ensemble using the optimal number of trees.\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nx_train, x_val, y_train, y_val = train_test_split(X,y)\n\ngbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 120)\ngbrt.fit(x_train, y_train)\n\nerrors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(x_val)]\nbst_n_estimators = np.argmin(errors)\n\ngbrt_best = GradientBoostRegressor(max_depth = 2, n_estimators =bst_n_estimators)\ngbrt_best.fit(x_train, y_train)","081cfd9b":"One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases. this is the technique used by Adaboost.","3585a703":"# soft voting","09761389":"# EXtra Tree classifier","dd0f6bed":"from sklearn.ensemble import GradientBoostRegressor\n\ngbrt = GradientBoostRegressor(max_depth = 2, n_estimators = 3, learning_rate = 1.0)\ngbrt.fit(x,y)","a1be7543":"Boosting (originally called hypothesis boosting) refers to any ensemble method that can combine several weak learners into a strong learner.\nThe general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor."}}