{"cell_type":{"3d025723":"code","d66f465d":"code","a133cfd8":"code","66d6aa79":"code","bd2e1a11":"code","957b0105":"code","150728ee":"code","c3261fa4":"code","5b5db988":"code","f65e7855":"code","389b801b":"code","6303e739":"code","06faa368":"code","7c848452":"code","a0c88544":"code","a37fd90f":"code","379ee6d7":"code","2a9e0f0e":"code","2755f693":"code","5eba94fa":"code","c4902d54":"code","b80ba977":"code","1d83e146":"code","183c14c6":"code","2d030c28":"code","010079b2":"code","1ef812d6":"code","9600f9b3":"code","c96f029f":"code","177cf72c":"code","3d582aa3":"code","6dc2728d":"code","a0e41b98":"code","81d837df":"code","529b3f66":"code","cd8cd37f":"code","e3b49dc6":"code","090214ab":"code","bcfbbd27":"code","995073e9":"markdown","7de2624e":"markdown","923f0c45":"markdown","a7bf9b2c":"markdown","4e725dbb":"markdown","da0c48ef":"markdown","58e063c9":"markdown","020c2d6c":"markdown","0679c6e8":"markdown","120b9e64":"markdown","7296f72a":"markdown","316b07a3":"markdown","b9bb7508":"markdown","e6eb9349":"markdown","16fcc0de":"markdown","14090e4e":"markdown","30044602":"markdown","9bcd265e":"markdown","335e8b7f":"markdown"},"source":{"3d025723":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport missingno as msno\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings('ignore')","d66f465d":"df_train = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")\nsample = pd.read_csv('\/kaggle\/input\/song-popularity-prediction\/sample_submission.csv')","a133cfd8":"df_train.describe()","66d6aa79":"df_train.info()","bd2e1a11":"df_train.isnull().sum().sort_values(ascending = False)","957b0105":"df_train.count(axis = 0).sum()","150728ee":"df_train.isnull().sum().sum()","c3261fa4":"df_test.info()","5b5db988":"df_test.describe()","f65e7855":"df_test.isnull().sum().sort_values(ascending = False)","389b801b":"df_test.count(axis = 0).sum()","6303e739":"df_test.isnull().sum().sum()","06faa368":"train_null = pd.DataFrame(df_train.isnull().sum())\ntest_null = pd.DataFrame(df_test.isnull().sum())\ntest_null = test_null.sort_values(by = 0 , ascending = False)[:8]\ntrain_null = train_null.sort_values(by = 0 , ascending = False)[:8]\n\nfig, ax = plt.subplots(1,2, figsize = (18,7))\n#fig, axes = plt.subplots(1,2, figsize=(18,10))\nsns.barplot(x = train_null[0], y = train_null.index, ax = ax[0])\nplt.title(\"Training data missing values\");\n\nsns.barplot(x = test_null[0], y = test_null.index, ax = ax[1])\nplt.title(\"Test data missing values\");","7c848452":"msno.bar(df_train)","a0c88544":"msno.bar(df_test)","a37fd90f":"msno.matrix(df_train)","379ee6d7":"msno.matrix(df_test)","2a9e0f0e":"msno.dendrogram(df_train)","2755f693":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(strategy = 'mean')","5eba94fa":"cols_with_missing_vals = [cols for cols in df_train if df_train[cols].isna().any()]\ncols_with_missing_vals","c4902d54":"imp_train = pd.DataFrame(imp.fit_transform(df_train[cols_with_missing_vals]),columns = cols_with_missing_vals)\nimp_test = pd.DataFrame(imp.transform(df_test[cols_with_missing_vals]),columns = cols_with_missing_vals)\n","b80ba977":"remaining_columns = [cols for cols in df_test.columns if cols not in cols_with_missing_vals and cols != 'id']\nremaining_columns","1d83e146":"train = pd.concat([imp_train, df_train[remaining_columns], df_train['song_popularity']], axis = 1)\ntest = pd.concat([imp_test, df_test[remaining_columns]], axis = 1)\n","183c14c6":"train.isnull().sum().sum(), test.isnull().sum().sum()","2d030c28":"plt.figure(figsize = (10,7))\nsns.countplot(train['song_popularity']);\nplt.title(\"No of popular Vs non-popular songs\");","010079b2":"train['song_popularity'].value_counts()","1ef812d6":"cat_cols = [cols for cols in train.columns if train[cols].nunique() < 20]\ncat_cols","9600f9b3":"for i in cat_cols:\n    train[i] = train[i].astype('int32')","c96f029f":"px.bar(train['audio_mode'].value_counts(), width= 1000, height=500,title = \"No of unique values of audio mode\")\n","177cf72c":"px.pie(data_frame = train, values = train['key'].value_counts(),names = train['key'].unique(), hole=0.55, title='Unique Values of key')","3d582aa3":"px.bar(train['time_signature'].value_counts(), width= 1000, height=500,title = \"No of unique values of time_signature\")\n","6dc2728d":"remaining_cols = [cols for cols in train.columns if cols not in cat_cols]\nremaining_cols","a0e41b98":"for cols in remaining_cols:\n    fig,ax = plt.subplots(1, 2,figsize = (15,6))\n    \n    sns.distplot(train[cols],ax = ax[0],color = 'b')\n    \n    sns.boxplot(train[cols],ax = ax[1], palette = 'rocket')\n    \n    plt.suptitle(f'Distribution and outlier detection of {cols}',fontsize = 20)\n    plt.show()\n    print(\"-\"*190)\n","81d837df":"kfold = KFold(n_splits = 5, shuffle = True)\ntrain['kfold'] =-1\n\nfor i, (train_index, valid_index) in enumerate(kfold.split(train)):\n    train.loc[valid_index, \"kfold\"] = i\n\ntrain.head()","529b3f66":"features = [c for c in train.columns if c not in ('kfold', 'song_popularity')]\nfeatures","cd8cd37f":"from sklearn.metrics import roc_auc_score","e3b49dc6":"predictions= []\nscore = []\n\nxgb_params = {\n    \"objective\":\"binary:logistic\",\n    \"eval_metric\": \"auc\",\n}\n\nfor i in range(5):\n    \n    ## selecting the train and validation set for each fold\n    x_train = train[train.kfold != i]\n    x_valid = train[train.kfold == i]\n    x_test = test\n    \n    ## Selecting the target variable\n    y_train = x_train.song_popularity\n    y_valid = x_valid.song_popularity\n    \n    \n    ## Selecting all features except kfold and target\n    x_train = x_train[features]\n    x_valid = x_valid[features]\n    \n    ## Scaling the values\n    \n    scale = StandardScaler()\n    \n    x_train[features] = scale.fit_transform(x_train[features])\n    x_valid[features] = scale.transform(x_valid[features])\n    x_test[features] = scale.transform(x_test[features])\n    \n    model = XGBClassifier(**xgb_params)\n    model.fit(x_train, y_train,early_stopping_rounds=300,eval_set=[(x_valid,y_valid)],verbose=False)\n    \n    y_val_preds = model.predict(x_valid)\n    y_preds = model.predict(x_test)\n\n    sc = roc_auc_score(y_val_preds, y_valid)\n    \n    score.append(sc)\n    predictions.append(y_preds)\n    \n    print(f\"Fold {i} score is {sc}\")\n    \n   \nprint(f\"The mean and standard deviation of the score  is {np.mean(score)}, {np.std(score)}\")","090214ab":"preds  = np.mean(np.column_stack(predictions),axis = 1)\npreds","bcfbbd27":"xgb_sample = sample.copy()\nxgb_sample.song_popularity = preds\nxgb_sample.to_csv('submission_xgb.csv',index = False)\nxgb_sample.head()","995073e9":"This helps us to see how the missing values(i.e. the white blank spaces in between the dark region of the column shows how the missing values is distributed, the major use of such visualization is that we can see whether any columns have similar missing values in the same range. To put it bluntly we are trying to see is there any pattern between missing value columns. But unfortunately there is no clear similarity.( I am pretty new to this library so I could have missed out some important observation if you find any which i have missed please do let me).","7de2624e":"Some features are termed as int but even though the no of unique variable in those columns are less.","923f0c45":"We can see that what took us nearly 8 -10 lines of code to plot some graphs which showed us the missing values, missingo did it in single line, finally found an learnt something new a","a7bf9b2c":"After googling some effective techniques to see visualization of missing values per column I came across missingo library, it is new library to me, So I am trying to get familiar with it.","4e725dbb":"### Exploring the test data set","da0c48ef":"### K-Fold Cross Validation","58e063c9":"Here we are see that there are 567813 observations, are there but we will subtract 40k from it because we will remove the count of the target variable and since it has no missing values also. so now we will account for 5,27,813 observations.","020c2d6c":"Now we got the columns of the missing values, lets first go with simple imputer and later try out various techniques to handel missing values.","0679c6e8":"Here we are having quite a lot of missing values, lets explore test data too and fill in the missing values with appropriate methods.","120b9e64":"Lets have a look into remaining columsn are numerical features","7296f72a":"### Handeling the missing values","316b07a3":"### Model Training","b9bb7508":"Now that we have cleaned data lets explore the data with help of visualizations.","e6eb9349":"### Exploring Training data set","16fcc0de":"### EDA","14090e4e":"In test data too there are quite a bit of missing values, lets handel them, before that lets see whether the columns in test and train which contains the missing values are same or not. ","30044602":"This shows which columns are closely correlated, we can see that for somg popularity audio valence has the highest correlation, so lets explore further to see how this turns out.","9bcd265e":"Data is slightly imbalanced towards non popular side.","335e8b7f":"#### XGB Classifier"}}