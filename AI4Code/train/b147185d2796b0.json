{"cell_type":{"c21edf7d":"code","480e2b73":"code","59c45d4e":"code","55ae15a1":"code","243b486d":"code","498b60f7":"code","1c6fe312":"code","ce8cf66d":"code","2c9332a0":"code","af334c7c":"code","bc366960":"code","ddfe02c9":"code","91b30a7b":"code","59acfb2b":"code","645ef28b":"code","131b78b6":"code","e92b16bf":"code","ccb4ed0d":"code","d537bd02":"code","4b6dfcfa":"code","90453576":"code","3de0f09e":"code","dae47dd2":"code","12cb9293":"code","2fe9f691":"code","e6a6fbd3":"code","c4e7f406":"code","1a6f9f76":"code","0d830118":"code","62a7ba54":"code","4f17e26e":"code","efc995b6":"code","f162095b":"code","c9d17ed4":"code","75c28f0a":"code","94128acb":"code","bf2c0a64":"code","fa2d1a76":"code","1923b447":"code","c0bbdcc8":"code","b7bb168e":"code","e2fea09a":"code","023ea88d":"code","d58f887c":"code","069a8562":"code","c380db88":"code","46129013":"code","9c2e0deb":"code","59cb58af":"code","0a6582f3":"code","623ad5dd":"code","0310ff33":"code","3394a738":"code","1f7980b5":"code","5000f4a7":"code","5fdf403a":"code","4d6aedc2":"code","d72e434f":"code","7332349a":"code","31862eff":"code","26550a64":"code","3d1eadfd":"code","e11887e5":"code","a424bd5d":"code","4b712987":"code","e61984f3":"code","508c7399":"code","a26b6b58":"code","b5598adf":"code","3ad320c0":"markdown","2cc25b8a":"markdown","a7d9dfad":"markdown","3f40ca66":"markdown","fbc18f7a":"markdown","619cdea7":"markdown","8686e8a5":"markdown","a7dc1933":"markdown","ad6c5ded":"markdown","f4a21466":"markdown","613871bd":"markdown","76d7af48":"markdown","4119dce2":"markdown","2220d16e":"markdown","f0082032":"markdown","e110d997":"markdown","dbd811d8":"markdown","df64b8ae":"markdown","a64693f7":"markdown","e633bd9a":"markdown","b66b25e4":"markdown","5e8e6195":"markdown","b7f6147f":"markdown","dbb356bd":"markdown","73543935":"markdown","a0afc292":"markdown"},"source":{"c21edf7d":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt","480e2b73":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.linear_model import LogisticRegression","59c45d4e":"data = 'https:\/\/raw.githubusercontent.com\/alexeygrigorev\/mlbookcamp-code\/master\/chapter-03-churn-prediction\/WA_Fn-UseC_-Telco-Customer-Churn.csv'","55ae15a1":"!wget $data","243b486d":"df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\n\ncategorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n\nfor c in categorical_columns:\n    df[c] = df[c].str.lower().str.replace(' ', '_')\n\ndf.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\ndf.totalcharges = df.totalcharges.fillna(0)\n\ndf.churn = (df.churn == 'yes').astype(int)","498b60f7":"df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\ny_train = df_train.churn.values\ny_val = df_val.churn.values\ny_test = df_test.churn.values\n\ndel df_train['churn']\ndel df_val['churn']\ndel df_test['churn']","1c6fe312":"numerical = ['tenure', 'monthlycharges', 'totalcharges']\n\ncategorical = [\n    'gender',\n    'seniorcitizen',\n    'partner',\n    'dependents',\n    'phoneservice',\n    'multiplelines',\n    'internetservice',\n    'onlinesecurity',\n    'onlinebackup',\n    'deviceprotection',\n    'techsupport',\n    'streamingtv',\n    'streamingmovies',\n    'contract',\n    'paperlessbilling',\n    'paymentmethod',\n]","ce8cf66d":"dv = DictVectorizer(sparse=False)\n\ntrain_dict = df_train[categorical + numerical].to_dict(orient='records')\nX_train = dv.fit_transform(train_dict)\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)","2c9332a0":"val_dict = df_val[categorical + numerical].to_dict(orient='records')\nX_val = dv.transform(val_dict)\n\ny_pred = model.predict_proba(X_val)[:, 1]\nchurn_decision = (y_pred >= 0.5)\n(y_val == churn_decision).mean()","af334c7c":"# Total predictions\nlen(y_val)","bc366960":"# Accuracy of churning\n(y_val == churn_decision).mean()","ddfe02c9":"# Correct predictions \/ total predictions \n1132\/ 1409","91b30a7b":"# scikit-learn has a module for this\nfrom sklearn.metrics import accuracy_score","59acfb2b":"accuracy_score(y_val, y_pred >= 0.5)","645ef28b":"# Recall linspace produces an nparray with equal 'spacings'\nthresholds = np.linspace(0, 1, 21)\n\nscores = []\n\n# Printing threshold, printing accuracy score with the threshold \nfor t in thresholds:\n    score = accuracy_score(y_val, y_pred >= t)\n    print('%.2f %.3f' % (t, score))\n    scores.append(score)","131b78b6":"plt.plot(thresholds, scores)","e92b16bf":"from collections import Counter","ccb4ed0d":"# counting stuff\nCounter(y_pred >= 1.0)","d537bd02":"# There is a class imbalance - there are significantly more number of people that are not expected to churn.\nCounter(y_val)","4b6dfcfa":"# Percentage of people that are not expected to churn in the first place \n1 - y_val.mean()","90453576":"# Actual positives and negatives \nactual_positive = (y_val == 1)\nactual_negative = (y_val == 0)","3de0f09e":"# Predictions with threshold \nt = 0.5\npredict_positive = (y_pred >= t)\npredict_negative = (y_pred < t)","dae47dd2":"# Four components of the confusion matrix\n\ntp = (predict_positive & actual_positive).sum()\ntn = (predict_negative & actual_negative).sum()\n\nfp = (predict_positive & actual_negative).sum()\nfn = (predict_negative & actual_positive).sum()","12cb9293":"# Creating the confusion matrix itself \n\nconfusion_matrix = np.array([\n    [tn, fp],\n    [fn, tp]\n])\nconfusion_matrix","2fe9f691":"# Percentage, rounded off to 2 dp \n(confusion_matrix \/ confusion_matrix.sum()).round(2)","e6a6fbd3":"# Precision \np = tp \/ (tp + fp)\np","c4e7f406":"r = tp \/ (tp + fn)\nr","1a6f9f76":"# True Positive Rate aka true positives divided by total number of positives\n# total number of positives = predicted positive, is positive (TP) + predicted negative, is positive (FN)\ntpr = tp \/ (tp + fn)\ntpr","0d830118":"# False Positive Rate aka false positives divided total number of negatives\n# total number of negatives = predicted negative, is negative (TN) + predicted positives, is negative (FP)\nfpr = fp \/ (fp + tn)\nfpr","62a7ba54":"scores = []\n\n# Creating an array of evenly-spaced probabilities from 0 to 1 (aka 0, 0.1, 0.2, ..., 0.99, 1)\nthresholds = np.linspace(0, 1, 101)\n\n# Printing true positives & negatives, false positives & negatives for each rate \nfor t in thresholds:\n    actual_positive = (y_val == 1)\n    actual_negative = (y_val == 0)\n    \n    predict_positive = (y_pred >= t)\n    predict_negative = (y_pred < t)\n\n    tp = (predict_positive & actual_positive).sum()\n    tn = (predict_negative & actual_negative).sum()\n\n    fp = (predict_positive & actual_negative).sum()\n    fn = (predict_negative & actual_positive).sum()\n    \n    # List of tuples is created per threshold\n    scores.append((t, tp, fp, fn, tn))","4f17e26e":"# Creating a dataframe for these scores so that we are able model the TPRs and FPRs for each threshold\ncolumns = ['threshold', 'tp', 'fp', 'fn', 'tn']\ndf_scores = pd.DataFrame(scores, columns=columns)\n\ndf_scores['tpr'] = df_scores.tp \/ (df_scores.tp + df_scores.fn)\ndf_scores['fpr'] = df_scores.fp \/ (df_scores.fp + df_scores.tn)","efc995b6":"plt.plot(df_scores.threshold, df_scores['tpr'], label='TPR')\nplt.plot(df_scores.threshold, df_scores['fpr'], label='FPR')\nplt.legend()","f162095b":"# Plucking numbers from a straight line distribution, size should be the same as validation data \nnp.random.seed(1)\ny_rand = np.random.uniform(0, 1, size=len(y_val))","c9d17ed4":"# Default threshold of 0.5 \n((y_rand >= 0.5) == y_val).mean()","75c28f0a":"# Using the previous function to create \ndef tpr_fpr_dataframe(y_val, y_pred):\n    scores = []\n\n    thresholds = np.linspace(0, 1, 101)\n\n    for t in thresholds:\n        actual_positive = (y_val == 1)\n        actual_negative = (y_val == 0)\n\n        predict_positive = (y_pred >= t)\n        predict_negative = (y_pred < t)\n\n        tp = (predict_positive & actual_positive).sum()\n        tn = (predict_negative & actual_negative).sum()\n\n        fp = (predict_positive & actual_negative).sum()\n        fn = (predict_negative & actual_positive).sum()\n\n        scores.append((t, tp, fp, fn, tn))\n\n    columns = ['threshold', 'tp', 'fp', 'fn', 'tn']\n    df_scores = pd.DataFrame(scores, columns=columns)\n\n    df_scores['tpr'] = df_scores.tp \/ (df_scores.tp + df_scores.fn)\n    df_scores['fpr'] = df_scores.fp \/ (df_scores.fp + df_scores.tn)\n    \n    return df_scores","94128acb":"# We use y_random ffor the prediction values in this case\ndf_rand = tpr_fpr_dataframe(y_val, y_rand)","bf2c0a64":"plt.plot(df_rand.threshold, df_rand['tpr'], label='TPR')\nplt.plot(df_rand.threshold, df_rand['fpr'], label='FPR')\nplt.legend()","fa2d1a76":"num_neg = (y_val == 0).sum()\nnum_pos = (y_val == 1).sum()\nnum_neg, num_pos","1923b447":"# Repeating the 0 and 1 values according to the num_neg and num_pos \ny_ideal = np.repeat([0, 1], [num_neg, num_pos])\ny_ideal\n\n# We create a ideal prediction dataset with values corresponding to 0 or 1 with regards to the threshold\ny_ideal_pred = np.linspace(0, 1, len(y_val))","c0bbdcc8":"# Rate of non-churning, aka the threshold which we will set this model with \n1 - y_val.mean()","b7bb168e":"accuracy_score(y_ideal, y_ideal_pred >= 0.726)","e2fea09a":"df_ideal = tpr_fpr_dataframe(y_ideal, y_ideal_pred)\ndf_ideal[::10]","023ea88d":"plt.plot(df_ideal.threshold, df_ideal['tpr'], label='TPR')\nplt.plot(df_ideal.threshold, df_ideal['fpr'], label='FPR')\nplt.legend()\n","d58f887c":"plt.plot(df_scores.threshold, df_scores['tpr'], label='TPR', color='black')\nplt.plot(df_scores.threshold, df_scores['fpr'], label='FPR', color='blue')\n\nplt.plot(df_ideal.threshold, df_ideal['tpr'], label='TPR ideal')\nplt.plot(df_ideal.threshold, df_ideal['fpr'], label='FPR ideal')\n\n# plt.plot(df_rand.threshold, df_rand['tpr'], label='TPR random', color='grey')\n# plt.plot(df_rand.threshold, df_rand['fpr'], label='FPR random', color='grey')\n\nplt.legend()","069a8562":"plt.figure(figsize=(5, 5))\n\nplt.plot(df_scores.fpr, df_scores.tpr, label='Model')\n\n#Uncomment if you want to see the random plot\n#plt.plot(df_rand.fpr, df_rand.tpr, label = 'random')\n\n# The random plot, simplified as ultiamtely it's still quite linear despite some fluctuations\nplt.plot([0, 1], [0, 1], label='Random', linestyle='--')\n\n# Uncomment this if you want to see the ideal plot\n# We ultimately want our model plot to be as close to the ideal plot as possible\n#plt.plot(df_ideal.fpr, df_ideal.tpr, label = 'ideal')\n\nplt.xlabel('FPR')\nplt.ylabel('TPR')\n\nplt.legend()","c380db88":"from sklearn.metrics import roc_curve","46129013":"fpr, tpr, thresholds = roc_curve(y_val, y_pred)","9c2e0deb":"plt.figure(figsize=(5, 5))\n\nplt.plot(fpr, tpr, label='Model')\nplt.plot([0, 1], [0, 1], label='Random', linestyle='--')\n\nplt.xlabel('FPR')\nplt.ylabel('TPR')\n\nplt.legend()","59cb58af":"# Area Under Curve function\n# Do note that we can use any curve for this, not just the ROC curve \nfrom sklearn.metrics import auc","0a6582f3":"# AUC with our manually calculated values\nauc(fpr, tpr)","623ad5dd":"# Via sklearn, this is slightly more accurate.\nauc(df_scores.fpr, df_scores.tpr)","0310ff33":"# AUC under ideal ROC curve\n# The AUC is not 1 since we manually calculated it ourselves.\nauc(df_ideal.fpr, df_ideal.tpr)","3394a738":"# Explanation for the roc_auc_score function itself! \nfpr, tpr, thresholds = roc_curve(y_val, y_pred)\nauc(fpr, tpr)","1f7980b5":"from sklearn.metrics import roc_auc_score","5000f4a7":"roc_auc_score(y_val, y_pred)","5fdf403a":"neg = y_pred[y_val == 0]\npos = y_pred[y_val == 1]","4d6aedc2":"import random","d72e434f":"n = 100000\nsuccess = 0 \n\n# random.randint returns random integer in range, including both the end points \nfor i in range(n):\n    # Calling the index of a random positive decision & random negative decision\n    pos_ind = random.randint(0, len(pos) - 1)\n    neg_ind = random.randint(0, len(neg) - 1)\n    \n    # If randomly selected positive example has a higher score than a randomly selected negative example, +1\n    if pos[pos_ind] > neg[neg_ind]:\n        success = success + 1\n\n# Number of successes \/ total number of times. We can tell that it's rather close to our AUC metric\nsuccess \/ n","7332349a":"# Using numpy instead\nn = 50000\n\nnp.random.seed(1)\npos_ind = np.random.randint(0, len(pos), size=n)\nneg_ind = np.random.randint(0, len(neg), size=n)\n\n(pos[pos_ind] > neg[neg_ind]).mean()\n\n# Do note that the size of n will influence how close the AUC score is to the actual AUC score.","31862eff":"# Preprocessing + training of model\ndef train(df_train, y_train, C=1.0):\n    dicts = df_train[categorical + numerical].to_dict(orient='records')\n\n    dv = DictVectorizer(sparse=False)\n    X_train = dv.fit_transform(dicts)\n\n    model = LogisticRegression(C=C, max_iter=1000)\n    model.fit(X_train, y_train)\n    \n    return dv, model","26550a64":"dv, model = train(df_train, y_train, C=0.001)","3d1eadfd":"# Prediction of y values\ndef predict(df, dv, model):\n    dicts = df[categorical + numerical].to_dict(orient='records')\n\n    X = dv.transform(dicts)\n    y_pred = model.predict_proba(X)[:, 1]\n\n    return y_pred","e11887e5":"y_pred = predict(df_val, dv, model)","a424bd5d":"from sklearn.model_selection import KFold","4b712987":"!pip install tqdm","e61984f3":"# tqdm allows you to see the process for each iteration \nfrom tqdm.auto import tqdm","508c7399":"n_splits = 5\n\nfor C in tqdm([0.001, 0.01, 0.1, 0.5, 1, 5, 10]):\n    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n\n    scores = []\n\n    for train_idx, val_idx in kfold.split(df_full_train):\n        # the k-fold split uses index to shuffle the data\n        df_train = df_full_train.iloc[train_idx]\n        df_val = df_full_train.iloc[val_idx]\n        \n        # y values come from dataset\n        y_train = df_train.churn.values\n        y_val = df_val.churn.values\n        \n        # training and predicting\n        dv, model = train(df_train, y_train, C=C)\n        y_pred = predict(df_val, dv, model)\n        \n        # AUC\n        auc = roc_auc_score(y_val, y_pred)\n        scores.append(auc)\n\n    print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))","a26b6b58":"scores","b5598adf":"dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\ny_pred = predict(df_test, dv, model)\n\nauc = roc_auc_score(y_test, y_pred)\nauc","3ad320c0":"### Putting everything together","2cc25b8a":"## Precision vs Recall - when do we use it? \nAnswer: It depends on the situation at hand. \n\nSometimes, we would need to focus on precision a bit more than that of recall, and sometimes vice versa. \n\nFor example, we would need to focus more on recall if ","a7d9dfad":"Afterthoughts of accuracy:\n\nAccuracy does not tell us how good a model is if there is a class imbalance. Only with balanced data (such as data that is 50\/50, or a bit more varied than that) would accuracy be a good metric to use. We have to look at other evaluation metrics","3f40ca66":"# 4. Evaluation Metrics for Classification\n\nIn the previous session we trained a model for predicting churn. How do we know if it's good?\n\n\n## 4.1 Evaluation metrics: session overview \n\n* Dataset: https:\/\/www.kaggle.com\/blastchar\/telco-customer-churn\n* https:\/\/raw.githubusercontent.com\/alexeygrigorev\/mlbookcamp-code\/master\/chapter-03-churn-prediction\/WA_Fn-UseC_-Telco-Customer-Churn.csv\n\n\n*Metric* - function that compares the predictions with the actual values and outputs a single number that tells how good the predictions are","fbc18f7a":"## 4.2 Accuracy and dummy model\n\n* Evaluate the model on different thresholds\n* Check the accuracy of dummy baselines","619cdea7":"NOTE: For 4.3 and 4.4, it would be good if you watched the videos for better visualisations of the terms below. You can draw it out on your own as well, but I personally believe that visualisations are a must to understand these concepts a little bit better.","8686e8a5":"We use the full train dataset for cross-validation, leaving away the test dataset.\n\nWe split it into k parts and use different parts such that we're able to use different parts for training and validate in on another part. This is known as a fold.\n\nAfter that, we record the metric used for the model itself. Once we have finished all our folds, we will be able to calculate things like the average metric and the standard deviation of the metrics.\n\nFor example: k = 3,\n- We use parts 1 and 2 for training, and validating for part 3, recording the metric.\n- We then use parts 1 and 3 for training, and validating for part 2, recording the metric.\n- Finally we use parts 2 and 3 for training, and validating for part 1, recording the metric. ","a7dc1933":"We can see that both our TPR and FPR are rather far from the ideal rates themselves.\nPlotting against the threshold isn't always very intuitive, but what we can do is to plot FPR against TPR, also known as a ROC curve.","ad6c5ded":"## 4.7 Cross-Validation\n\n* Evaluating the same model on different subsets of data\n* Getting the average prediction and the spread within predictions","f4a21466":"1. At threshold 0, we do predict everyone that churns as positive, so the TPR = 1. But at the same time, we do not predict anyone as negative. Therefore, the true negative = 0, and FPR = 1 since FP \/ FP + TN is effectively FP\/FP which equals 1.\n2. Both TPR and FPR goes down when we increase the threshold, but the FPR decreases at a faster rate. We do want the FPR to decrease faster! ","613871bd":"## 4.8 Summary\n\n* Metric - a single number that describes the performance of a model\n* Accuracy - fraction of correct answers; sometimes misleading \n* Precision and recall are less misleading when we have class inbalance\n* ROC Curve - a way to evaluate the performance at all thresholds; okay to use with imbalance\n* K-Fold CV - more reliable estimate for performance (mean + std)","76d7af48":"The area under the curve is a very useful for the metric itself. To calculate the AUC, we will usually have to use calculus for this - but sklearn has a AUC function that doesn't need us to calculate the AUC. \n\nWe should, however, consider what the value of the AUC should be: \n1. It should definitely be more than 0.5 - the random model* that we did has a area of 0.5. If it is less than 0.5, there is definitely a problem with the curve itself.\n2. It should be as close to 1 as possible. The AUC of the ideal model is 1, and while we can't always achieve the ideal model, we should bring it as close to the ideal model as possible. \n\n*= Reason why the random model has an area of 0.5 is because we consider it as a straight line. This means that we can calculate the area under the line itself as a 'triangle'. At FPR = 1, TPR also = 1, therefore the 'triangle' random model has a length of 1 and height of 1. Area of triangle = 0.5 * length * height, therefore AUC = 0.5.","4119dce2":"Accuracy can be based on here, by looking at the probability of the true negatives and positives.\nThis totals up to 0.8, which is the accuracy of our previous model.","2220d16e":"## 4.4 Precision and Recall\n\nSome definitions here:\n\nPrecision: Fraction of positive predictions that are correct\n\nRecall: Fraction of correctly identified positive predictions ","f0082032":"## 4.3 Confusion table\n\n* Different types of errors and correct decisions\n* Arranging them in a table","e110d997":"For a guideline:\n- The closer a model is to the ideal model, the better it is\n- If it's closer to the random model but still above the model, it is not very good.\n- Your model should NOT go below the random model itself, that means that there is something wrong.","dbd811d8":"## 4.6 ROC AUC\n\n* Area under the ROC curve - useful metric\n* Interpretation of AUC","df64b8ae":"True positive always remains at 1 until the threshold of 0.726 whereby it starts to drop.\n\nThe model still makes mistakes at anything before its threshold, since the FPR is still going down despite the TPR still remaining 1.","a64693f7":"## 4.5 ROC Curves\n\n### TPR and FPR","e633bd9a":"## What are thresholds? \nRecall that in our model, we made it such that if y_pred >= 0.5, it would be counted as 1. \nHowever, that probability can be adjusted such that we can make stricter predictions or more lenient predictions. \n\nFor example, instead of 0.5, we can make it such that anything from 0.3 onwards would be counted as 1, or anything from 0.74 would be counted as 1. ","b66b25e4":"Basics of a confusion table\/matrix (as some people would call it): \n\nThere will be 2 possible scenarios when there is a prediction of no churn:\n1. The customer indeed didn't churn - also known as a true negative\n2. The customer actually churned - also knows as a false negative\n\nThe same thing will happen when there is a prediction of churn:\n1. The customer indeed churned - also known as a true positive\n2. The customer actually didn't churn - also known as a false positive \n\nThese four scenarios form a confusion table\/matrix.","5e8e6195":"## What is accuracy?\nAccuracy is basically **the number of correct predictions\/number of total predictions.**\n\ne.g. 4 people are predicted not to churn, while 2 people are predicted to churn.\n\nHowever, in reality, 2 people that are predicted not to churn did churn, while 1 person that is predicted to churn did not churn. That would mean that out of 6, only 3 predictions were correct.\n\nTherefore, the accuracy of the model is 50%.","b7f6147f":"### Random model\nWe will be using this model as a baseline.","dbb356bd":"## 4.9 Explore more\n\n* Check the precision and recall of the dummy classifier that always predict \"FALSE\"\n* F1 score = 2 * P * R \/ (P + R)\n* Evaluate precision and recall at different thresholds, plot P vs R - this way you'll get the precision\/recall curve (similar to ROC curve)\n* Area under the PR curve is also a useful metric\n\nOther projects:\n\n* Calculate the metrics for datasets from the previous week","73543935":"### Ideal model","a0afc292":"Ok, but what does AUC actually tell us? \n\nAUC tells us the probability that a randomly selected positive example has a higher score than a randomly selected negative exmaple."}}