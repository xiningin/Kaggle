{"cell_type":{"cef2b52e":"code","35c4fef1":"code","7e4c64e7":"code","0ecaae39":"code","c5f6aa27":"code","e7697f55":"code","c6ef85b1":"code","c818bda7":"code","2dd78f63":"code","ea203a2b":"code","04ee5366":"code","a731cd16":"code","cf153061":"code","a5d8d876":"code","10a2f487":"code","0aeae008":"code","f727151f":"code","03310469":"code","85567e1d":"code","1f4cfed7":"code","63b7b0de":"code","f67b1080":"code","7b5700fd":"code","73a26f63":"code","66f3bf5d":"code","3e2df7bf":"code","1b77c613":"code","3a3dfb2a":"code","71c7114b":"code","603aa62a":"code","8ee5c14f":"code","9be3c6a3":"code","6ad6dd79":"code","c53d307b":"code","29f1b91d":"code","bd270c72":"code","2b4ad66f":"code","c86e9fac":"code","118f0e95":"code","92f1eede":"code","a9179d10":"code","c9a321fd":"code","6b55d467":"code","c1325355":"code","54914d3a":"code","a5ea10d2":"code","19635c31":"code","45e69d50":"code","3cb1de20":"code","ccc21e03":"code","d61aa48b":"code","67d853ac":"code","74015e34":"code","5edb7457":"code","d15d56fc":"code","49fe793d":"code","413a51d9":"code","d2c0a6ea":"code","42a72eff":"code","644ef037":"code","2f88501f":"code","776b815b":"code","4e5aeeab":"code","c9302d0d":"code","282ea16e":"code","59ea475b":"code","692ae0a5":"code","85eaf961":"code","075121dd":"code","aa2994fa":"code","88b2013f":"code","734f495a":"code","15e2ccea":"code","fe63da51":"code","42eac4d7":"code","b9460cdb":"code","61b9c11b":"code","447813bf":"code","c4d9ef3e":"code","23913d37":"code","af4e7ed4":"code","af9a6f22":"code","457b833e":"code","5d085cec":"code","c936a022":"code","4b303854":"code","12281252":"code","0371aee1":"code","fb6ab042":"code","0fa6f5de":"code","8bf32653":"code","b4eed888":"code","f33a38d9":"code","79eef3ad":"code","c6ada891":"code","27426e72":"code","452c1702":"code","f915c4bd":"code","1336917f":"code","b9a04b2f":"code","7dde6d5a":"code","1b35dc18":"code","95e0d46f":"code","1cb2e7f0":"code","df589489":"code","22064cd5":"code","b6c9bc51":"markdown","90b92819":"markdown","18cee8a4":"markdown","e431bd74":"markdown","f48f7c47":"markdown","d5674ce2":"markdown","58bd6737":"markdown","386ffc5c":"markdown","3f034654":"markdown","01e528f4":"markdown","d621bbfd":"markdown","2cba2ced":"markdown","69848350":"markdown","89ccbd7f":"markdown","dcb8601e":"markdown","680b110f":"markdown","3b2fdef5":"markdown","e64b2162":"markdown","58ce90a7":"markdown","c6dc6901":"markdown","19b93c4e":"markdown","5467f815":"markdown","facc95cd":"markdown","761ec1d2":"markdown","675072cd":"markdown","dfa8128c":"markdown","24ad9e94":"markdown","e81259b4":"markdown","80f2eaa0":"markdown","0235ac3a":"markdown","9aee01ec":"markdown","52d86467":"markdown","6ef3afb6":"markdown","dc6af1ef":"markdown","14ceb3a4":"markdown","0614a270":"markdown","7b13a745":"markdown","57d798a2":"markdown","8665c154":"markdown","d6a75084":"markdown"},"source":{"cef2b52e":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)","35c4fef1":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col='PassengerId')","7e4c64e7":"df.head()","0ecaae39":"df.info()","c5f6aa27":"df.describe()","e7697f55":"df.describe(include=['O'])","c6ef85b1":"# Understanding the target class is very important\n\nsns.countplot('Survived', data=df)","c818bda7":"100.0*df['Survived'].value_counts() \/ len(df)","2dd78f63":"df.corr()['Survived']","ea203a2b":"df[['Pclass','Survived']].groupby('Pclass', as_index=False).mean()","04ee5366":"df[['Sex', 'Survived']].groupby('Sex', as_index=False).mean()","a731cd16":"df[['SibSp', 'Survived']].groupby('SibSp', as_index=False).mean().sort_values('Survived', ascending=False)","cf153061":"df[['Parch','Survived']].groupby('Parch', as_index=False).mean().sort_values('Survived', ascending=False)","a5d8d876":"g = sns.FacetGrid(df, col='Survived')\ng.map(plt.hist, 'Age', bins=15)","10a2f487":"g = sns.FacetGrid(df, col='Survived', row='Pclass')\ng.map(plt.hist, 'Age')","0aeae008":"df[['Embarked','Survived']].groupby('Embarked', as_index=False).mean().sort_values('Survived', ascending=False)","f727151f":"100.0*df['Embarked'].value_counts() \/ len(df)","03310469":"df['Title'] = df.Name.str.extract('([A-Za-z]+)\\.')\ntest_df['Title'] = test_df.Name.str.extract('([A-Za-z]+)\\.')","85567e1d":"df.head()","1f4cfed7":"df['Title'].value_counts()","63b7b0de":"pd.crosstab(df['Title'], df['Sex'])","f67b1080":"replace_titles = ['Capt','Col','Countess','Don','Jonkheer','Lady','Major','Dr','Rev','Sir']","7b5700fd":"df['Title'] = df['Title'].replace(replace_titles, 'other')\ndf['Title'] = df['Title'].replace('Mlle', 'Miss')\ndf['Title'] = df['Title'].replace('Ms', 'Miss')\ndf['Title'] = df['Title'].replace('Mme', 'Mrs')","73a26f63":"df[['Title','Survived']].groupby('Title').mean().sort_values('Survived', ascending=False)","66f3bf5d":"test_df['Title'] = test_df['Title'].replace(replace_titles, 'other')\ntest_df['Title'] = test_df['Title'].replace('Mlle', 'Miss')\ntest_df['Title'] = test_df['Title'].replace('Ms', 'Miss')\ntest_df['Title'] = test_df['Title'].replace('Mme', 'Mrs')","3e2df7bf":"pd.crosstab(test_df['Title'], test_df['Sex'])","1b77c613":"test_df['Title'] = test_df['Title'].replace('Dona', 'other')","3a3dfb2a":"df.isnull().sum().sort_values(ascending=False)","71c7114b":"test_df.isnull().sum().sort_values(ascending=False)","603aa62a":"print('Number of age entries missing for title Miss:', df[df['Title'] == 'Miss']['Age'].isnull().sum())\nprint('Number of age entries missing for title Mr:', df[df['Title'] == 'Mr']['Age'].isnull().sum())\nprint('Number of age entries missing for title Mrs:', df[df['Title'] == 'Mrs']['Age'].isnull().sum())\nprint('Number of age entries missing for title other:', df[df['Title'] == 'other']['Age'].isnull().sum())\nprint('Number of age entries missing for title Master:', df[df['Title'] == 'Master']['Age'].isnull().sum())","8ee5c14f":"print('Mean age for title Miss:', df[df['Title'] == 'Miss']['Age'].mean())\nprint('Mean age for title Mr:', df[df['Title'] == 'Mr']['Age'].mean())\nprint('Mean age for title Mrs:', df[df['Title'] == 'Mrs']['Age'].mean())\nprint('Mean age for title other:', df[df['Title'] == 'other']['Age'].mean())\nprint('Mean age for title Master:', df[df['Title'] == 'Master']['Age'].mean())","9be3c6a3":"df.loc[(df['Title']== 'Miss') & (df['Age'].isnull()), 'Age'] = 22\ndf.loc[(df['Title']== 'Mr') & (df['Age'].isnull()), 'Age'] = 32\ndf.loc[(df['Title']== 'Mrs') & (df['Age'].isnull()), 'Age'] = 36\ndf.loc[(df['Title']== 'other') & (df['Age'].isnull()), 'Age'] = 46\ndf.loc[(df['Title']== 'Master') & (df['Age'].isnull()), 'Age'] = 5","6ad6dd79":"df.isnull().sum().sort_values(ascending=False)","c53d307b":"# Repeating the steps for test set\n\nprint('Number of age entries missing for title Miss:', test_df[test_df['Title'] == 'Miss']['Age'].isnull().sum())\nprint('Number of age entries missing for title Mr:', test_df[test_df['Title'] == 'Mr']['Age'].isnull().sum())\nprint('Number of age entries missing for title Mrs:', test_df[test_df['Title'] == 'Mrs']['Age'].isnull().sum())\nprint('Number of age entries missing for title other:', test_df[test_df['Title'] == 'other']['Age'].isnull().sum())\nprint('Number of age entries missing for title Master:', test_df[test_df['Title'] == 'Master']['Age'].isnull().sum())","29f1b91d":"print('Mean age for title Miss:', test_df[test_df['Title'] == 'Miss']['Age'].mean())\nprint('Mean age for title Mr:', test_df[test_df['Title'] == 'Mr']['Age'].mean())\nprint('Mean age for title Mrs:', test_df[test_df['Title'] == 'Mrs']['Age'].mean())\nprint('Mean age for title other:', test_df[test_df['Title'] == 'other']['Age'].mean())\nprint('Mean age for title Master:', test_df[test_df['Title'] == 'Master']['Age'].mean())","bd270c72":"test_df.loc[(test_df['Title']== 'Miss') & (test_df['Age'].isnull()), 'Age'] = 22\ntest_df.loc[(test_df['Title']== 'Mr') & (test_df['Age'].isnull()), 'Age'] = 32\ntest_df.loc[(test_df['Title']== 'Mrs') & (test_df['Age'].isnull()), 'Age'] = 39\ntest_df.loc[(test_df['Title']== 'other') & (test_df['Age'].isnull()), 'Age'] = 44\ntest_df.loc[(test_df['Title']== 'Master') & (test_df['Age'].isnull()), 'Age'] = 7","2b4ad66f":"test_df.isnull().sum().sort_values(ascending=False)","c86e9fac":"df['Embarked'] = df['Embarked'].fillna('S')","118f0e95":"test_df.loc[test_df['Fare'].isnull()]","92f1eede":"# Finding out the mean Fare for Pclass=3\n\ntest_df[test_df['Pclass']==3]['Fare'].mean()","a9179d10":"test_df['Fare'] = test_df['Fare'].fillna(12.46)","c9a321fd":"print('Percentage of cabin values missing in train set:', 100.0*df['Cabin'].isnull().sum() \/ len(df))\nprint('Percentage of cabin values missing in test set:', 100.0*test_df['Cabin'].isnull().sum() \/ len(df))","6b55d467":"print('Missing values for train set')\nprint(df.isnull().sum().sort_values(ascending=False))\nprint('----------------')\nprint('Missing values for test set')\nprint(test_df.isnull().sum().sort_values(ascending=False))","c1325355":"df.head()","54914d3a":"# Creating a new column for age groups\n\ndf['AgeGroup'] = pd.cut(df['Age'],5)","a5ea10d2":"df[['AgeGroup', 'Survived']].groupby('AgeGroup', as_index=False).mean().sort_values('Survived', ascending=False)","19635c31":"df.loc[df['Age'] <= 16, 'Age'] = 4\ndf.loc[(df['Age'] > 32) & (df['Age'] <= 48), 'Age'] = 3\ndf.loc[(df['Age'] >48) & (df['Age'] <= 64), 'Age'] = 2\ndf.loc[(df['Age'] > 16) & (df['Age'] <= 32), 'Age'] = 1\ndf.loc[(df['Age'] > 64), 'Age'] = 0","45e69d50":"df = df.drop('AgeGroup', axis=1)","3cb1de20":"test_df.loc[test_df['Age'] <= 16, 'Age'] = 4\ntest_df.loc[(test_df['Age'] > 32) & (test_df['Age'] <= 48), 'Age'] = 3\ntest_df.loc[(test_df['Age'] >48) & (test_df['Age'] <= 64), 'Age'] = 2\ntest_df.loc[(test_df['Age'] > 16) & (test_df['Age'] <= 32), 'Age'] = 1\ntest_df.loc[(test_df['Age'] > 64), 'Age'] = 0","ccc21e03":"df.head()","d61aa48b":"df[['Fare','Pclass']].groupby('Pclass', as_index=False).mean()","67d853ac":"df['Fare'].min()","74015e34":"df['Fare'].max()","5edb7457":"df['fareband'] = pd.cut(df['Fare'], 4)","d15d56fc":"df[['fareband', 'Survived']].groupby('fareband', as_index=False).mean().sort_values('Survived', ascending=False)","49fe793d":"df.loc[(df['Fare'] >= 384), 'Fare'] = 3\ndf.loc[(df['Fare'] >= 256) & (df['Fare'] < 384), 'Fare'] = 2\ndf.loc[(df['Fare'] >=128) & (df['Fare'] < 256), 'Fare'] = 1\ndf.loc[df['Fare'] < 128, 'Fare'] = 0","413a51d9":"df = df.drop('fareband', axis=1)","d2c0a6ea":"# Repeating the steps for the test set\n\ntest_df.loc[(test_df['Fare'] >= 384), 'Fare'] = 3\ntest_df.loc[(test_df['Fare'] >= 256) & (test_df['Fare'] < 384), 'Fare'] = 2\ntest_df.loc[(test_df['Fare'] >=128) & (test_df['Fare'] < 256), 'Fare'] = 1\ntest_df.loc[test_df['Fare'] < 128, 'Fare'] = 0","42a72eff":"df.head()","644ef037":"df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\ntest_df['FamilySize'] = test_df['SibSp'] + test_df['Parch'] + 1","2f88501f":"# lets take a final look at our dataframe before processing it further\n\ndf.head()","776b815b":"df.info()","4e5aeeab":"drop_cols = ['Name', 'SibSp', 'Parch', 'Ticket', 'Cabin']","c9302d0d":"df = df.drop(drop_cols, axis=1)\ntest_df = test_df.drop(drop_cols, axis=1)","282ea16e":"df.head()","59ea475b":"dummy_cols = ['Pclass','Sex', 'Age',  'Fare', 'Embarked', 'Title', 'FamilySize']\nprefix_cats = ['pcl', 'sex', 'age', 'fare', 'emb', 'title', 'fsize']\n\ndf = pd.get_dummies(df, columns=dummy_cols, prefix=prefix_cats, drop_first=True)\ntest_df = pd.get_dummies(test_df, columns=dummy_cols, prefix=prefix_cats, drop_first=True)","692ae0a5":"df.head()","85eaf961":"df.shape","075121dd":"test_df.shape","aa2994fa":"X = df.drop('Survived', axis=1)\ny = df['Survived']","88b2013f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score","734f495a":"# Creating an empty dataframe to add model predictions for comparison\n\npred_df = pd.DataFrame()","15e2ccea":"# initialize all the predictors and fit the training data\n\nlog_clf = LogisticRegression(random_state=42)\nlog_clf.fit(X, y)\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X, y)\n\nsvc_clf = SVC(random_state=42)\nsvc_clf.fit(X, y)\n\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_clf.fit(X, y)\n\nforest_clf = RandomForestClassifier(random_state=42)\nforest_clf.fit(X, y)\n\nextra_clf = ExtraTreesClassifier(random_state=42)\nextra_clf.fit(X, y)\n\ngb_clf = GradientBoostingClassifier(random_state=42)\ngb_clf.fit(X, y)","fe63da51":"# cross_val_predict and generate accuracy scores for all the predictors\n\nlog_preds = cross_val_predict(log_clf, X, y, cv=10)\nlog_acc = accuracy_score(y, log_preds)\n\nsgd_preds = cross_val_predict(sgd_clf, X, y, cv=10)\nsgd_acc = accuracy_score(y, sgd_preds)\n\nsvc_preds = cross_val_predict(svc_clf, X, y, cv=10)\nsvc_acc = accuracy_score(y, svc_preds)\n\ntree_preds = cross_val_predict(tree_clf, X, y, cv=10)\ntree_acc = accuracy_score(y, tree_preds)\n\nforest_preds = cross_val_predict(forest_clf, X, y, cv=10)\nforest_acc = accuracy_score(y, forest_preds)\n\nextra_preds = cross_val_predict(extra_clf, X, y, cv=10)\nextra_acc = accuracy_score(y, extra_preds)\n\ngb_preds = cross_val_predict(gb_clf, X, y, cv=10)\ngb_acc = accuracy_score(y, gb_preds)","42eac4d7":"print('log_clf', log_acc)\nprint('sgd_clf', sgd_acc)\nprint('svc_clf', svc_acc)\nprint('tree_clf', tree_acc)\nprint('forest_clf', forest_acc)\nprint('extra_clf', extra_acc)\nprint('gb_clf', gb_acc)","b9460cdb":"# Generating paramater grids for predictors\n\nlog_param = [\n    {#'penalty':['l1', 'l2', 'elasticnet'],\n    'C':[0.001, 0.01, 0.1, 1.0, 10.0]\n    }\n]\n\nsgd_param = [\n    {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n    #'penalty':['l1', 'l2', 'elasticnet']\n    }\n]\n\nsvc_param = [\n    {'C':[0.001, 0.01, 0.1, 1.0, 10.0],\n    'gamma':[0.001, 0.01, 0.1, 1.0],\n    'kernel':['rbf', 'sigmoid']}\n]\n\ntree_param = [\n    {'max_depth':[2,4,8,12,16,20,30],\n    'min_samples_split':[2,4,6,8,10],\n    'min_samples_leaf':[2,4,6,8,10]\n    }\n]\n\nforest_param = [\n    {'max_depth':[2,4,8,12,16,20],\n    'min_samples_split':[2,4,6,8,10],\n    'min_samples_leaf':[2,4,6,8,10],\n    'n_estimators':[100,200,300]}\n]\n\nextra_param = [\n    {'max_depth':[2,4,8,12,16,20,30],\n    'min_samples_split':[2,4,6,8,10],\n    'min_samples_leaf':[2,4,6,8,10]}\n]\n\ngb_param = [\n    {'max_depth':[2,8,16,20],\n    'min_samples_split':[2,4,6,10],\n    'min_samples_leaf':[2,4,6,10],\n    'learning_rate':[0.01, 0.05, 0.1],\n    'n_estimators':[100,200,300],\n    'subsample':[0.5, 0.8, 1.0]}\n]","61b9c11b":"log_grid = GridSearchCV(log_clf, log_param, cv=5)\nlog_grid.fit(X, y)","447813bf":"log_best = log_grid.best_estimator_","c4d9ef3e":"sgd_grid = GridSearchCV(sgd_clf, sgd_param, cv=5)\nsgd_grid.fit(X, y)","23913d37":"sgd_best = sgd_grid.best_estimator_","af4e7ed4":"svc_grid = GridSearchCV(svc_clf, svc_param, cv=5)\nsvc_grid.fit(X, y)","af9a6f22":"svc_best = svc_grid.best_estimator_","457b833e":"tree_grid = GridSearchCV(tree_clf, tree_param, cv=5)\ntree_grid.fit(X, y)","5d085cec":"tree_best = tree_grid.best_estimator_","c936a022":"forest_grid = GridSearchCV(forest_clf, forest_param, cv=5, verbose=1, n_jobs=-1)\nforest_grid.fit(X, y)","4b303854":"forest_best = forest_grid.best_estimator_","12281252":"extra_grid = GridSearchCV(extra_clf, extra_param, cv=5, verbose=1, n_jobs=-1)\nextra_grid.fit(X, y)","0371aee1":"extra_best = extra_grid.best_estimator_","fb6ab042":"gb_grid = GridSearchCV(gb_clf, gb_param, cv=5, verbose=1, n_jobs=-1)\ngb_grid.fit(X, y)","0fa6f5de":"gb_best = gb_grid.best_estimator_","8bf32653":"log_best.fit(X, y)","b4eed888":"sgd_best.fit(X, y)","f33a38d9":"svc_best.fit(X, y)","79eef3ad":"tree_best.fit(X, y)","c6ada891":"forest_best.fit(X, y)","27426e72":"extra_best.fit(X, y)","452c1702":"gb_best.fit(X, y)","f915c4bd":"log_best_preds = cross_val_predict(log_best, X, y, cv=10)\nlog_best_acc = accuracy_score(y, log_best_preds)\n\nsgd_best_preds = cross_val_predict(sgd_best, X, y, cv=10)\nsgd_best_acc = accuracy_score(y, sgd_best_preds)\n\nsvc_best_preds = cross_val_predict(svc_best, X, y, cv=10)\nsvc_best_acc = accuracy_score(y, svc_best_preds)\n\ntree_best_preds = cross_val_predict(tree_best, X, y, cv=10)\ntree_best_acc = accuracy_score(y, tree_best_preds)\n\nforest_best_preds = cross_val_predict(forest_best, X, y, cv=10)\nforest_best_acc = accuracy_score(y, forest_best_preds)\n\nextra_best_preds = cross_val_predict(extra_best, X, y, cv=10)\nextra_best_acc = accuracy_score(y, extra_best_preds)\n\ngb_best_preds = cross_val_predict(gb_best, X, y, cv=10)\ngb_best_acc = accuracy_score(y, gb_best_preds)","1336917f":"pred_df = pred_df.append({'b.Best Estimtor Accuracy': log_best_acc, 'b.Accuracy': log_acc, 'a.Model':'log_clf'}, ignore_index=True)\npred_df = pred_df.append({'b.Best Estimtor Accuracy': sgd_best_acc, 'b.Accuracy': sgd_acc, 'a.Model':'sgd_clf'}, ignore_index=True)\npred_df = pred_df.append({'b.Best Estimtor Accuracy': svc_best_acc, 'b.Accuracy': svc_acc, 'a.Model':'svc_clf'}, ignore_index=True)\npred_df = pred_df.append({'b.Best Estimtor Accuracy': tree_best_acc, 'b.Accuracy': tree_acc, 'a.Model':'tree_clf'}, ignore_index=True)\npred_df = pred_df.append({'b.Best Estimtor Accuracy': forest_best_acc, 'b.Accuracy': forest_acc, 'a.Model':'forest_clf'}, ignore_index=True)\npred_df = pred_df.append({'b.Best Estimtor Accuracy': extra_best_acc, 'b.Accuracy': extra_acc, 'a.Model':'extra_clf'}, ignore_index=True)\npred_df = pred_df.append({'b.Best Estimtor Accuracy': gb_best_acc, 'b.Accuracy': gb_acc, 'a.Model':'gb_clf'}, ignore_index=True)","b9a04b2f":"pred_df","7dde6d5a":"svc_test_preds = svc_best.predict(test_df)","1b35dc18":"gb_test_preds = gb_best.predict(test_df)","95e0d46f":"forest_test_preds = forest_best.predict(test_df)","1cb2e7f0":"submission = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv', index_col='PassengerId')\nsubmission['Survived'] = svc_test_preds\nsubmission.to_csv('svc_final_submission.csv')","df589489":"submission = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv', index_col='PassengerId')\nsubmission['Survived'] = gb_test_preds\nsubmission.to_csv('gb_final_submission.csv')","22064cd5":"submission = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv', index_col='PassengerId')\nsubmission['Survived'] = forest_test_preds\nsubmission.to_csv('forest_final_submission.csv')","b6c9bc51":"### Feature Engineering continued...","90b92819":"for the test set, use the age group results from the train set itself as the test set does not have the target class to be able to do the same analysis","18cee8a4":"The basic approach for predictive modeling is as follows:\n- Initialize all the predictors and fit the training data\n- Use cross_val_predict instead of splitting the data as it's quite a small dataset\n- Generate accuracy score for every model\n- Use GridSearachCV for hyperparameter tuning\n- Create parameter grids for grid search\n- Run grid search and find the best estimator\n- Fit the training data this time on the best estimator\n- Use grid search on the best estimator and generate accuracy score\n- Append the model name, accuracy score, and accuracy score on best estimator to a dataframe for comparison\n- Select the model with the best score","e431bd74":"- Training data contains 891 samples (40%) compared to 2205 total passengers on board\n- 61.6% of the people did not survive\n- 38.38% of the people survived comapred to the 32% survival rate of the complete dataset","f48f7c47":"- Infants (age<5) have a high survival rate\n- Oldest peole (age=80) all survived\n- Most passangers in 15-25 age range, highest mortality rate in that range","d5674ce2":"Looks good. ","58bd6737":"This notebook is a simple yet an effective approach to Machine Learning for the Titanic dataset. After having gone through multiple notebooks and approaches, I have come to realize that the simpler the approach, the better the results for this dataset. The more complexity in feature engineering I applied, the more the model overfit, giving poor results on the test dataset. \n\nThis notebook does not explain ML concepts, but concludes learnings and findings wherever necessary. It follows a very straight forward step by step ML approach with simple python commands.\n\nIt follows the following steps:\n* Basic exploratory data analysis\n* Analyzing features with the target column\n* Feature engineering\n* Treating missing values\n* Feature engineering continued\n* Dropping redundant features\n* Creating dummies for all features\n* Separating target and features\n* Predictive modeling\n* Hyperparameter tuning using grid search\n* Comparing accuracy scores\n\nIf you have any questions or feedback, please let me know in the comment section. ","386ffc5c":"- 69% of Pclss=1 passangers survived\n- 74% of females survived\n- Passangers with lesser SibSp have a higher survival rate\n- SibSp and Parch have zero correlation for certain values","3f034654":"Assigning values based on the above age groups. The groups with higher survival rate will be assigned a higher number. So <=16 will be 4, >32 but <=48 will be 3 and so on and so forth","01e528f4":"### Analysing features with the target column","d621bbfd":"#### Pclass, Sex, SibSp, Parch","2cba2ced":"#### Comparing accuracy scores","69848350":"#### Hyperparameter tuning","89ccbd7f":"#### Grouping ages in Age feature and assigning values based on their survival rate","dcb8601e":"- 72% of the passengers on board embarked from port S\n- Port S also has the highest number of survivors, 55%","680b110f":"### Treating Mising Values before performing further Feature Engineering","3b2fdef5":"### Separating features and target","e64b2162":"#### Merging Titles \nComparing Titles with the Sex feature to figure out how rarely used titles can be merged with other titles","58ce90a7":"#### Filling missing values for age based on mean age per Title","c6dc6901":"#### Embarked","19b93c4e":"#### Filling in missing values in train set for Embarked \nFilling in missing values with most_frequent i.e. S","5467f815":"### Dropping redundant features","facc95cd":"Looks like svc_clf and gb_clf are performing best with sgd_clf, forest_clf, and extra_clf close behind it.\nOutputting predictions from svc_clf and gb_clf to a csv document for submission. When the scores for multiple models are similar, due to overfitting either model may perform better on the unseen test dataset. So experimenting with both.\n\n*I'm also going to submit predictions from the Random Forest model as in my experience random forests tend perform very well on unseen test datasets","761ec1d2":"### Creating dummies for all features","675072cd":"All missing values have been treated apart from Cabin which will be dropped later","dfa8128c":"#### Grouping fares in Fare feature and assigning values based on their survival rate","24ad9e94":"#### Extracting Title from the Name feature","e81259b4":"The logic adding adding 1 is for solo passengers that don't have any siblings\/spouses or parents\/children. This feature takes care of all the information in the features SibSp and Parch. We can effectively drop those two features now.","80f2eaa0":"### Feature Engineering","0235ac3a":"#### Pclass, Survived Vs. Age","9aee01ec":"#### Filling in missing values in test set for Fare","52d86467":"# Predictive Modeling","6ef3afb6":"#### Cabin","dc6af1ef":"#### Combining SibSp and Parch to create FamilySize feature","14ceb3a4":"Oh, test set has another unique title called 'dona', merging that with other","0614a270":"#### Age","7b13a745":"Make sure to perform actions simultaneously on both, the train and test dataset","57d798a2":"Too many values for the Cabin feature are missing. Also, it isn't a very useful feature. So drop this feature.","8665c154":"### Basic exploratory data analysis","d6a75084":"- Infants with Pclass 1 and 2 mostly survived\n- Most adults in Pclass 3 did not surivive\n- Most passangers in Pclass 1 survived"}}