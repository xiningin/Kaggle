{"cell_type":{"89c151e2":"code","2515b079":"code","8fa8469d":"code","14ba88c9":"code","7fab0a30":"code","295af3ef":"code","e5ce1c58":"code","ae984b2c":"code","29d2e852":"code","1e4c6ac4":"code","c0cc71df":"code","f231fd6a":"code","51a5859a":"code","142f2464":"code","4f6b60e3":"code","03d06c69":"code","2ee429b8":"code","5221a203":"code","83e6bb29":"code","93199e36":"code","4b5c7616":"code","a3a4b5bf":"code","5adc541e":"code","c8d9f371":"code","0da89307":"code","4d79d2b0":"code","6a635124":"code","1234462d":"code","ae2a20ce":"code","81752fe8":"code","c381ace4":"code","6c8239c4":"code","08caccd4":"code","a64d2cd5":"code","f7b449f5":"code","dcd872bd":"code","cb9af990":"code","46b806d3":"code","d5f0e522":"code","c97f0fd0":"markdown","a12f270d":"markdown","13af7177":"markdown","7c5fb515":"markdown","b56c3c6f":"markdown","0ddb217d":"markdown","8633d821":"markdown","8758711d":"markdown","027b8652":"markdown","8c3231fc":"markdown","876d88f8":"markdown","065f8e30":"markdown","3452a685":"markdown","6b983bfd":"markdown","95e38caf":"markdown","6202aa20":"markdown","b07d6a90":"markdown","2a72749f":"markdown","db721992":"markdown","f4ed629e":"markdown","cc2f3ee9":"markdown","e749e35f":"markdown","a9328763":"markdown","06de5bdb":"markdown","66730001":"markdown","980a480c":"markdown","3c391273":"markdown","667d91a9":"markdown","cc3213bf":"markdown"},"source":{"89c151e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2515b079":"from IPython.display import clear_output\n!pip3 install -U lazypredict\n\nclear_output()","8fa8469d":"import os , glob \nimport pandas as pd \nimport numpy as np \nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nsns.set(style=\"whitegrid\")\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n#machine learning\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, Normalizer, MaxAbsScaler\nfrom sklearn.preprocessing import (StandardScaler, PowerTransformer, QuantileTransformer ,LabelEncoder,OneHotEncoder, OrdinalEncoder)\n#models \nfrom xgboost import XGBRegressor\nprint(\"set up complete\")","14ba88c9":"!pip3 install -U pandas==1.2.3\nclear_output()","7fab0a30":"train_data= pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\") # train_data \ntest_data = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\") # test_data \nsample = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\",index_col=\"id\") # sample_submission ","295af3ef":"train_data.head(3)\n","e5ce1c58":"print(f'Number of rows: {train_data.shape[0]};  Number of columns: {train_data.shape[1]}; No of missing values: {sum(train_data.isna().sum())}')","ae984b2c":"print('Info about train data: ')\ntrain_data.info()","29d2e852":"cat_features = [feature for feature in train_data.columns if 'cat' in feature]\ncont_features = [feature for feature in train_data.columns if 'cont' in feature]\nprint(f'categorical features are : {cat_features};  numerical features are  : {cont_features}')","1e4c6ac4":"train_data.describe().T.style.bar().background_gradient(cmap='viridis')","c0cc71df":"plt.figure(figsize=(20,10))\nax= sns.violinplot(data=train_data[cont_features],inner=None, palette=\"viridis\")\nplt.title('Continuous features distribution');","f231fd6a":"train_data.var()","51a5859a":"train_data.std()","142f2464":"cat= train_data.select_dtypes(include='object').columns.tolist()\nidx = 0\nf, axes = plt.subplots(5, 2, sharex=True, figsize=(12,14))\nplt.suptitle('Categorical features distribution', size=16, y=(0.94))\nfor row in range(5):\n    for col in range(2):\n        data = train_data[cat[idx]].value_counts()\n        sns.barplot(x = data.index, y = data.values, palette='viridis', ax=axes[row, col])\n        axes[row,col].set_title(cat[idx])\n        idx += 1","4f6b60e3":"corrMatrix =train_data.corr(method='pearson', min_periods=1)\ncorrMatrix ","03d06c69":"#heatmap \nplt.figure(figsize=(25,20))\nax = sns.heatmap(corrMatrix, cmap=\"viridis\", annot=True)","2ee429b8":"print('target column basic statistics:')\ntrain_data['target'].describe()","5221a203":"plt.figure(figsize=(12,5))\nsns.distplot(train_data['target'],color=\"maroon\", kde=True,bins=120, label='target')\nplt.title(\"target values Distribution \")","83e6bb29":"plt.figure(figsize=(12,5))\nax = sns.boxplot(train_data[\"target\"] , orient='h')\nax.set_title('Target variable boxplot')","93199e36":"target =train_data[\"target\"] \nfig,ax = plt.subplots(5,3,figsize=(20,30),sharey=False)\nrow = col = 0\nfor n,i in enumerate(cont_features):\n    if (n % 3 == 0) & (n > 0):\n        row += 1\n        col = 0\n    sns.scatterplot(x=i,y=target,data=train_data,color=\"maroon\",ax=ax[row,col])\n    ax[row,col].set_title(f\"Target vs {i}\")\n    col += 1\n    \nplt.show();","4b5c7616":"test_data.head(3)\n","a3a4b5bf":"test_data.tail(3)","5adc541e":"print(f'Number of rows: {test_data.shape[0]};  Number of columns: {test_data.shape[1]}; No of missing values: {sum(test_data.isna().sum())}')","c8d9f371":"test_data.describe().T.style.bar().background_gradient(cmap='viridis')","0da89307":"cont_test=test_data.select_dtypes('float64').columns.tolist()\nplt.figure(figsize=(20,10))\nax= sns.violinplot(data=test_data[cont_features],inner=None, palette=\"viridis\")\nplt.title('Continuous test features distribution')","4d79d2b0":"cat_test= test_data.select_dtypes(include='object').columns.tolist()\nidx = 0\nf, axes = plt.subplots(5, 2, sharex=True, figsize=(12,14))\nplt.suptitle('Categorical features distribution', size=16, y=(0.94))\nfor row in range(5):\n    for col in range(2):\n        data = test_data[cat[idx]].value_counts()\n        sns.barplot(x = data.index, y = data.values, palette='viridis', ax=axes[row, col])\n        axes[row,col].set_title(cat[idx])\n        idx += 1","6a635124":"# Correlation matrix\ncorrMatrix =train_data.corr(method='pearson', min_periods=1)\nplt.figure(figsize=(25,20))\nax = sns.heatmap(corrMatrix, cmap=\"viridis\", annot=True)\n","1234462d":"features = train_data.iloc[:,11:25]","ae2a20ce":"# we will look into train_test features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(5, 3,figsize=(14, 24))\nfor feature in features:\n    plt.subplot(5, 3,i)\n    sns.distplot(train_data[feature],color=\"maroon\", kde=True,bins=120, label='train')\n    sns.distplot(test_data[feature],color=\"darkblue\", kde=True,bins=120, label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","81752fe8":"import lazypredict\nfrom lazypredict import Supervised\nfrom lazypredict.Supervised import LazyRegressor\n\n\nplt.style.use('fivethirtyeight')\nplt.rcParams[\"figure.figsize\"] = (20,5)\nnum_models = 36   #Number of Models\nclear_output()","c381ace4":"for col in cat_features:\n    encoder = OrdinalEncoder()\n    train_data[col] = encoder.fit_transform(np.array(train_data[col]).reshape(-1, 1))\n    test_data[col] = encoder.transform(np.array(test_data[col]).reshape(-1, 1))\n    \nX = train_data.drop([\"id\", \"target\"], axis=1)\nX_test = test_data.drop([\"id\"], axis=1)\ny = train_data[\"target\"]","6c8239c4":"#Spliting into training and validation set\noffset = int(X.shape[0] * 0.67)\nX_train, y_train = X[:offset], y[:offset]\nX_valid, y_valid = X[offset:], y[offset:]","08caccd4":"reg_idx = [i for i in range(num_models)]\nnoregs_idx = [10,15,23,24,29,32] # Removing 6 models from 42 models. Some of these models are time consuming whereas other require lot of ram.\nregs_name =[]\nregs = []\nfor i in range(42):\n    regs_name.append(lazypredict.Supervised.REGRESSORS[i][0])\n    regs.append(lazypredict.Supervised.REGRESSORS[i][1])\n\nfor i in noregs_idx:\n    del regs_name[i]\n    del regs[i]","a64d2cd5":"print(\"ALL 36 AVAILABLE REGRESSION MODELS:\")\nfor i in range(num_models):\n    print(i+1 , regs_name[i])","f7b449f5":"results = pd.DataFrame()\nfor i in range(num_models):\n    reg = LazyRegressor(verbose=1000, \n                    ignore_warnings=False,\n                    custom_metric=None,\n                    regressors = [regs[i]])\n    models, predictions = reg.fit(X_train, X_valid, y_train, y_valid)\n    models.index = [regs_name[i]]\n    results = results.append(models)\nclear_output()","dcd872bd":"results = results.sort_values(by = \"RMSE\")\nresults","cb9af990":"results.head()\n","46b806d3":"plt.plot(reg_idx , results[\"RMSE\"],label = \"RMSE\" ,marker='o')\nplt.xlabel(\"Model ID\")\nplt.ylabel(\"RMSE\")\nplt.title(\"RMSE Comparison of 36 Different Models\")\nplt.legend()\nplt.show()","d5f0e522":"plt.plot(reg_idx , results[\"Time Taken\"],label = \"Time Taken\" ,marker='*' , color = 'r')\nplt.xlabel(\"Model ID\")\nplt.ylabel(\"Time Taken\")\nplt.title(\"TIME TAKEN of 36 Different Models\")\nplt.legend()\nplt.show()","c97f0fd0":"**Categorical features**","a12f270d":"# **Training on 36 different models**","13af7177":"# **Comparison Plots**","7c5fb515":"**Test dataset**","b56c3c6f":"# **MODEL SELECTION**","0ddb217d":"# **Feature correlations**","8633d821":"**We can see that train and test features have almost identical distribution.**","8758711d":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTlYfXwnNJQ2jAooTlVpmXsjXw11mXD7Su92g&usqp=CAU)\n# **If you find this useful Please Upvote the notebook. Cheers!!! **","027b8652":"**Target variable**","8c3231fc":"# **Exploratory Data Analysis**","876d88f8":"**The test data has 200000 rows and 25 columns with 0 missing values**","065f8e30":"# **Basic Import**","3452a685":"# **RESULTS**","6b983bfd":"**Printing distribution of categorical features**","95e38caf":"**Categorical features**","6202aa20":"# **Building the Model**","b07d6a90":"# **Feature correlations**","2a72749f":"**Numerical features variation and standard deviation**","db721992":"**Basic summary statistic**","f4ed629e":"# **Train\/Test features distribution**","cc2f3ee9":"**It is one of the best python libraries that helps you to semi-automate your Machine Learning Task. It builds a lot of basic models without much code and helps understand which models work better without any parameter tuning.**\n\n**Suppose we have a problem statement and we really need to apply all the models on that particular dataset and we have to analyze that how our basic model is performing. Here basic model means \u201cModel without parameters\u201d. So we can do this task directly using Lazy Predict. After getting all accuracy we can choose the top 5 models and then apply hyperparameter tuning to them. It provides a Lazy Classifier to solve the classification problem and Lazy Regressor to solve the regression problem.**","e749e35f":"# **Loading Data**","a9328763":"Distribution of categorical features","06de5bdb":"# **Basic summary statistic**","66730001":"# **TOP 5 Performing Models (BY RMSE)**","980a480c":"**We can see that columns [cat0, cat2, cat4, cat6, cat7]. are dominated by one category which make them non-informative but still we need to go deep.**","3c391273":"[Read more about Lazy Predict](http:\/\/https:\/\/lazypredict.readthedocs.io\/en\/latest\/readme.html)","667d91a9":"# **What is Lazy Predict ?**","cc3213bf":"![](https:\/\/us.123rf.com\/450wm\/hancik80\/hancik801805\/hancik80180500013\/102165173-lazy-teenage-boy-sitting-in-the-armchair-watching-tv-drinking-cola-and-eating-popcorn-mess-is-all-ar.jpg?ver=6)"}}