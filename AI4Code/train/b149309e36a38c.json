{"cell_type":{"40a26458":"code","4d723dad":"code","17d0e339":"code","96bd2776":"code","4dbeafb9":"code","74510438":"code","5d5d03b8":"code","7bae1198":"code","9c0cf15c":"code","4057f561":"code","81b1ad42":"code","0a580a96":"code","d99bcb08":"code","aead12a3":"code","cbb1c242":"code","76bffdf2":"code","831cccf6":"code","82ab534e":"code","02604b6d":"code","1b36a109":"code","1261648c":"code","c74feb4d":"code","27a44a78":"markdown","a88bfff8":"markdown"},"source":{"40a26458":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d723dad":"pip install xfeat","17d0e339":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#from matplotlib_venn import venn2\nimport category_encoders as ce\n%matplotlib inline\n\nfrom xfeat import (SelectCategorical, LabelEncoder, Pipeline, ConcatCombination, SelectNumerical, \n                   ArithmeticCombinations, TargetEncoder, aggregation, GBDTFeatureSelector, GBDTFeatureExplorer)\n\nfrom catboost import CatBoost\nfrom catboost import CatBoostClassifier\nfrom catboost import CatBoostRegressor\nfrom catboost import Pool\nfrom catboost import cv\nfrom sklearn.metrics import mean_squared_log_error\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom tqdm import tqdm\n\nimport os\nfrom glob import glob\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\n\nimport shap\nimport gc\n\nfrom optuna.integration import _lightgbm_tuner as lgb_tuner\nimport optuna\nfrom collections import Counter\npd.set_option('display.max_columns', 100)\n\nimport warnings\nwarnings.filterwarnings('ignore')","96bd2776":"from contextlib import contextmanager\nfrom time import time\n\n@contextmanager\ndef timer(logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None):\n    if prefix: format_str = str(prefix) + format_str\n    if suffix: format_str = format_str + str(suffix)\n    start = time()\n    yield\n    d = time() - start\n    out_str = format_str.format(d)\n    if logger:\n        logger.info(out_str)\n    else:\n        print(out_str)\n        \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","4dbeafb9":"train_df = pd.read_csv(\"..\/input\/tabular-playground-series-apr-2021\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-apr-2021\/test.csv\")\nsub_df = pd.read_csv(\"..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv\")","74510438":"train_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)","5d5d03b8":"train_df.head()","7bae1198":"train_df.info()","9c0cf15c":"# change category->object\n\nchange_cols = [\"Pclass\",\"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"]\n\nfor col in change_cols:\n    train_df[col] = train_df[col].astype(str)\n    test_df[col] = test_df[col].astype(str)","4057f561":"# baseBlock \n\nclass BaseBlock(object):\n    def fit(self, input_df, y=None):\n        return self.transform(input_df)\n    \n    def transform(self, input_df):\n        raise NotImplementedError()\n        \n# CountEncoding\nclass CountEncodingBlock(BaseBlock):\n    def __init__(self, cols):\n        self.cols = cols\n        self.encoder = None\n        \n    def fit(self, input_df, y=None):\n        return self.transform(input_df[self.cols])\n    \n    def transform(self, input_df):\n        self.encoder = ce.CountEncoder()\n        self.encoder.fit(input_df[self.cols])\n        return self.encoder.transform(input_df[self.cols]).add_prefix(\"CE_\")\n    \n# OneHotEncoding\nclass OneHotEncodingBlock(BaseBlock):\n    def __init__(self, cols):\n        self.cols = cols\n        self.encoder = None\n        \n    def fit(self, input_df, y=None):\n        self.encoder = ce.OneHotEncoder(use_cat_names=True)\n        self.encoder.fit(input_df[self.cols])\n        return self.transform(input_df[self.cols])\n    \n    def transform(self, input_df):\n        return self.encoder.transform(input_df[self.cols]).add_prefix(\"OHE_\")\n\n# OrdinalEncoding\nclass OrdinalEncodingBlock(BaseBlock):\n    def __init__(self, cols):\n        self.cols = cols\n        self.encoder = None\n        \n    def fit(self, input_df, y=None):\n        self.encoder = ce.OrdinalEncoder()\n        self.encoder.fit(input_df[self.cols])\n        return self.transform(input_df[self.cols])\n    \n    def transform(self, input_df):\n        return self.encoder.transform(input_df[self.cols]).add_prefix(\"OE_\")","81b1ad42":"def get_ce_features(input_df):\n    _input_df = pd.concat([input_df], axis=1)\n    \n    cols = [\"Pclass\",\"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"]\n    \n    encoder = CountEncodingBlock(cols = cols)\n    output_df = encoder.fit(_input_df.astype(str))\n    return output_df\n\ndef get_oe_features(input_df):\n    _input_df = pd.concat([input_df], axis=1)\n    cols = [\"Pclass\",\"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"]\n    \n    encoder = OrdinalEncodingBlock(cols = cols)\n    output_df = encoder.fit(_input_df)\n    return output_df\n\ndef get_ohe_features(input_df):\n    _input_df = pd.concat([input_df], axis=1)\n    cols = [\"Pclass\",\"Sex\", \"Embarked\"]\n    encoder = OneHotEncodingBlock(cols=cols)\n    output_df = encoder.fit(_input_df)\n    return output_df\n\n# numeric_feature\ndef create_numeric_feature(input_df):\n    use_columns = [\n        \"Age\",\n        \"SibSp\",\n        \"Parch\",\n        \"Fare\",\n    ]\n\n    return input_df[use_columns].copy()","0a580a96":"def to_features(train, test):\n    input_df = pd.concat([train, test]).reset_index(drop=True)\n\n    processes = [\n        get_oe_features,\n        get_ce_features,\n        get_ohe_features,\n        create_numeric_feature,\n    ]\n\n    output_df = pd.DataFrame()\n    for func in tqdm(processes):\n        _df = func(input_df)\n        assert len(_df) == len(input_df), func.__name__\n        output_df = pd.concat([output_df, _df], axis=1)\n\n    train_x = output_df.iloc[:len(train)] \n    test_x = output_df.iloc[len(train):].reset_index(drop=True)\n    return train_x, test_x","d99bcb08":"target_data = \"Survived\" \n\ntrain_x, test_x = to_features(train_df, test_df)\ntrain_ys = train_df[target_data]","aead12a3":"# ligthGBM\ndef fit_lgbm(X, y, cv, params: dict=None, verbose: int=50):\n    metric_func = accuracy_score\n    if params is None:\n        params = {}\n\n    models = []\n    # training data \u306e target \u3068\u540c\u3058\u3060\u3051\u306e\u30bc\u30ed\u914d\u5217\u3092\u7528\u610f\n    # float \u306b\u3057\u306a\u3044\u3068\u60b2\u3057\u3044\u4e8b\u4ef6\u304c\u8d77\u3053\u308b\u306e\u3067\u305d\u3053\u3060\u3051\u6ce8\u610f\n    oof_pred = np.zeros_like(y, dtype=np.float)\n\n    for i, (idx_train, idx_valid) in enumerate(cv): \n        # \u3053\u306e\u90e8\u5206\u304c\u4ea4\u5dee\u691c\u8a3c\u306e\u3068\u3053\u308d\u3067\u3059\u3002\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092 cv instance \u306b\u3088\u3063\u3066\u5206\u5272\u3057\u307e\u3059\n        # training data \u3092 trian\/valid \u306b\u5206\u5272\n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n\n        clf = lgb.LGBMClassifier(**params)\n\n        with timer(prefix='fit fold={} '.format(i + 1)):\n            clf.fit(x_train, y_train, \n                    eval_set=[(x_valid, y_valid)],  \n                    early_stopping_rounds=verbose,\n                    verbose=verbose)\n\n        pred_i = clf.predict(x_valid)\n\n        oof_pred[idx_valid] = pred_i\n        models.append(clf)\n\n        print(f'Fold {i} Accuracy: {metric_func(y_valid, pred_i) :.4f}')\n\n    score = metric_func(y, oof_pred) \n    print('FINISHED | Whole Accuracy: {:.4f}'.format(score))\n    return oof_pred, models\n\nparams = {\n    'learning_rate': 0.01,\n     'objective': 'binary',\n    'metric':'binary_error',\n    'n_estimators': 10000,\n    'num_leaves': 20067,\n    'max_depth': 27,\n    'reg_alpha': 9.630576598001266,\n    'reg_lambda': 2.346945113164939,\n    'colsample_bytree': 0.29858836720777177,\n    'subsample': 0.6267448547447422,\n}","cbb1c242":"fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=71)\ncv = list(fold.split(train_x, train_ys))\n\noof, models = fit_lgbm(train_x.values, train_ys, cv, params=params)","76bffdf2":"np.unique(oof)","831cccf6":"def visualize_importance(models, feat_train_df):\n    feature_importance_df = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df['feature_importance'] = model.feature_importances_\n        _df['column'] = feat_train_df.columns\n        _df['fold'] = i + 1\n        feature_importance_df = pd.concat([feature_importance_df, _df], axis=0, ignore_index=True)\n\n    order = feature_importance_df.groupby('column')\\\n        .sum()[['feature_importance']]\\\n        .sort_values('feature_importance', ascending=False).index[:50]\n\n    fig, ax = plt.subplots(figsize=(max(6, len(order) * .4), 7))\n    sns.boxenplot(data=feature_importance_df, x='column', y='feature_importance', order=order, ax=ax, palette='viridis')\n    ax.tick_params(axis='x', rotation=90)\n    ax.grid()\n    fig.tight_layout()\n    return fig, ax","82ab534e":"fig, ax = visualize_importance(models, train_x)","02604b6d":"pred1 = np.array([model.predict(test_x.values) for model in models])\npred1 = np.mean(pred1, axis=0)\npred1 = np.where(pred1<0.5, 0, pred1)\npred1 = np.where(pred1>0.5, 1, pred1)","1b36a109":"np.unique(pred1)","1261648c":"sub_df[\"Survived\"] = pred1.astype(int)\nsub_df.to_csv('submission.csv', index=False)","c74feb4d":"sub_df","27a44a78":"# model","a88bfff8":"# Load Data"}}