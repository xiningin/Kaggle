{"cell_type":{"cdb461cd":"code","ef53a88c":"code","2084dc25":"code","ab553034":"code","6c4573c8":"code","d310de90":"code","0efc3613":"code","dc7acbdd":"code","13365fc3":"code","b3846329":"code","a6e89b52":"code","d80ccb4a":"code","0f5e604f":"code","98de7014":"code","b9e5759f":"code","96d374b2":"code","f22c90e3":"code","7c543313":"code","1a25e062":"code","e515d033":"code","1c41557b":"code","10089e53":"code","4d65c8cf":"code","b43d20b0":"code","0ae83fce":"code","e3815879":"code","ef84de99":"code","0cc83133":"code","7d313285":"code","414f5e27":"code","5e67bd40":"code","7e6f96d9":"code","433688d1":"code","bd6e9d3b":"code","81d684fa":"code","f7d2fc97":"markdown","05410e4d":"markdown","0621adfb":"markdown","a5055168":"markdown","da420598":"markdown","19d42595":"markdown","2c786237":"markdown","5fe54267":"markdown","d99719d2":"markdown","3701d1b3":"markdown"},"source":{"cdb461cd":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","ef53a88c":"# Data loading\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2084dc25":"heart = pd.read_csv(\"\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")\nheart.head()","ab553034":"heart.shape","6c4573c8":"heart[\"DEATH_EVENT\"].value_counts()\n\n# deaths = 13\n# alive = 299","d310de90":"heart[\"DEATH_EVENT\"].isnull().sum()","0efc3613":"sns.barplot(x = \"DEATH_EVENT\" , y = \"age\" , data = heart)\nsns.set(style = \"whitegrid\")","dc7acbdd":"sns.barplot(x = \"DEATH_EVENT\" , y = \"serum_creatinine\" , data = heart)\nsns.set(style = \"whitegrid\")","13365fc3":"sns.barplot(x = \"DEATH_EVENT\" , y = \"creatinine_phosphokinase\" , data = heart)\nsns.set(style = \"whitegrid\")","b3846329":"g = sns.FacetGrid(heart,hue = \"DEATH_EVENT\",height = 5)\ng.map(sns.distplot,\"anaemia\")\ng.add_legend()","a6e89b52":"g = sns.FacetGrid(heart,hue = \"DEATH_EVENT\",height = 5)\ng.map(sns.distplot,\"platelets\")\ng.add_legend()","d80ccb4a":"g = sns.FacetGrid(heart,hue = \"DEATH_EVENT\",height = 5)\ng.map(sns.distplot,\"high_blood_pressure\")\ng.add_legend()","0f5e604f":"g = sns.FacetGrid(heart,hue = \"DEATH_EVENT\",height = 5)\ng.map(sns.distplot,\"serum_sodium\")\ng.add_legend()","98de7014":"'''by above analysis we can say that serum_creatinine,platelets,high_blood_pressure,creatinine_phosphokinase,anaemia\ncause changes to death event'''","b9e5759f":"heart.corr()  # correlation between any two features","96d374b2":"plt.subplots(figsize = (10,10))\nsns.heatmap(heart.corr() , annot = True , linewidths = 1 )","f22c90e3":"correlation = heart.corr()\ncorrelation_target = abs(correlation)\ncorrelation_target['DEATH_EVENT']","7c543313":"heart.head()","1a25e062":"# importing libraries\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import confusion_matrix \n\n","e515d033":"# making new datasets with important features\n\nx = heart.loc[:,{\"high_blood_pressure\",\"anaemia\",\"age\",\"ejection_fraction\",\"serum_creatinine\",\"time\"}]\ny = np.array(heart[\"DEATH_EVENT\"])\nheart['DEATH_EVENT'].value_counts()","1c41557b":"# train and test\n# train = 70% of total data\n# test = 30% of total data\n\nx_train,x_test,y_train,y_test = model_selection.train_test_split(x,y,test_size = 0.3,random_state = 0)\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","10089e53":"model = []\nf1_score = []\naccuracy = []","4d65c8cf":"# now for cross validation we will take data from train data and split it equally\n\n# x_tr = 70% of total x_train\n# x_cv = 30% of total x_train\n# y_tr = 70% of total y_train\n# y_cv = 30% of total y_train\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nx_tr,x_cv,y_tr,y_cv = model_selection.train_test_split(x_train,y_train,test_size = 0.3,random_state = 0)\n\nprint(x_tr.shape)\nprint(x_cv.shape)\nprint(y_tr.shape)\nprint(y_cv.shape)","b43d20b0":"# now main thing i.e, fitting and predicting\n\nfor i in range(1,30,2):\n    knn = KNeighborsClassifier(n_neighbors = i )\n    knn.fit(x_tr,y_tr)\n    pred = knn.predict(x_cv)\n    acc = accuracy_score(y_cv , pred ,normalize = True)*float(100)\n    print(' cv accuracy for k = {0} is {1}' .format (i,acc))","0ae83fce":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_tr,y_tr)\npredict = knn.predict(x_test)\nprint('test accuracy',accuracy_score(y_test , predict ,normalize = True)*float(100))\n\nknn_normal_accuracy = accuracy_score(y_test , predict ,normalize = True)*float(100)\n\n# to plot confusion matrix\n\ncm = confusion_matrix(y_test,predict)\nsns.heatmap(cm , annot = True )\n\n# calculation of F1 score\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[0,1]\nFP = cm[1,0]\n\nRecall = TP\/(TP+FN)\nPrecision = TP\/(TP+FP)\n\nknn_normal_f1_score = ((2 * Recall * Precision)\/(Recall + Precision))\n\nprint('f1_score of the model is {}'.format(knn_normal_f1_score))\n\n\n# putting datas in list\n\nmodel.append('knn_normal')\nf1_score.append(knn_normal_f1_score)\naccuracy.append(knn_normal_accuracy)\n\n","e3815879":"\nknn = KNeighborsClassifier()\n\nparam_grid = {'n_neighbors': np.arange(1, 15)}\n\n\n\nknn_gcv = GridSearchCV(knn, param_grid, cv=4)\n\nknn_gcv.fit(x_train, y_train)\n\nprint(\"Best K Value is \",knn_gcv.best_params_)\n\nprint(\"test accuracy \",(knn_gcv.score(x_test,y_test))*float(100))\n\nknn_grid_accuracy = knn_gcv.score(x_test,y_test)*float(100)\n\n# to plot confusion matrix\n\ncm = confusion_matrix(y_test,knn_gcv.predict(x_test))\nsns.heatmap(cm , annot = True )\n\n# calculation of F1 score\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[0,1]\nFP = cm[1,0]\n\nRecall = TP\/(TP+FN)\nPrecision = TP\/(TP+FP)\n\nknn_grid_f1_score = ((2 * Recall * Precision)\/(Recall + Precision))\n\nprint('f1_score of the model is {}'.format(knn_grid_f1_score))\n\n# putting datas in list\n\nmodel.append('knn_grid')\nf1_score.append(knn_grid_f1_score)\naccuracy.append(knn_grid_accuracy)","ef84de99":"from sklearn.linear_model import LogisticRegression\n\nlor = LogisticRegression(max_iter=1000)\n\nparams_lor = {'C':[0.00001,0.0001,0.001,0.1,1,10,100]}\n\nlor_gcv = GridSearchCV(lor , param_grid = params_lor)\n\nlor_gcv.fit(x_train, y_train)\n\nprint(\"Best C Value is \",lor_gcv.best_params_)\n\nprint(\"test accuracy \",(lor_gcv.score(x_test,y_test))*float(100))\n\nlogistic_regression_accuracy = lor_gcv.score(x_test,y_test)*float(100)\n\n# to plot confusion matrix\n\ncm = confusion_matrix(y_test,lor_gcv.predict(x_test))\nsns.heatmap(cm , annot = True )\n\n\n# calculation of F1 score\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[0,1]\nFP = cm[1,0]\n\nRecall = TP\/(TP+FN)\nPrecision = TP\/(TP+FP)\n\nlogistic_regression_f1_score = ((2 * Recall * Precision)\/(Recall + Precision))\n\nprint('f1_score of the model is {}'.format(logistic_regression_f1_score))\n\n# putting datas in list\n\nmodel.append('logistic_regression')\nf1_score.append(logistic_regression_f1_score)\naccuracy.append(logistic_regression_accuracy)","0cc83133":"from sklearn.naive_bayes import GaussianNB\n\nnaive = GaussianNB()\n\nparams_naive = {'var_smoothing':[0.00001,0.0001,0.001,0.1,1,10,100]}\n\nnaive_gcv = GridSearchCV(naive , param_grid = params_naive )\n\nnaive_gcv.fit(x_train, y_train)\n\nprint(\"Best var_smoothing Value is \",naive_gcv.best_params_)\n\nprint(\"test accuracy \",(naive_gcv.score(x_test,y_test))*float(100))\n\nnaive_bayes_accuracy = naive_gcv.score(x_test,y_test)*float(100)\n\n# to plot confusion matrix\n\ncm = confusion_matrix(y_test,naive_gcv.predict(x_test))\nsns.heatmap(cm , annot = True )\n\n\n# calculation of F1 score\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[0,1]\nFP = cm[1,0]\n\nRecall = TP\/(TP+FN)\nPrecision = TP\/(TP+FP)\n\nnaive_bayes_f1_score = ((2 * Recall * Precision)\/(Recall + Precision))\n\nprint('f1_score of the model is {}'.format(naive_bayes_f1_score))\n\n\n\n# putting datas in list\n\nmodel.append('naive_bayes')\nf1_score.append(naive_bayes_f1_score)\naccuracy.append(naive_bayes_accuracy)","7d313285":"from sklearn.svm import SVC\n\nsvm = SVC()\n\nparams_svm = {'C':[0.00001,0.0001,0.001,0.1,1,10,100]}\n\nsvm_gcv = GridSearchCV(svm , param_grid = params_svm )\n\nsvm_gcv.fit(x_train,y_train)\n\nprint(\"Best C Value is \",svm_gcv.best_params_)\n\nprint(\"test accuracy \",(svm_gcv.score(x_test,y_test))*float(100))\n\nsvm_accuracy = svm_gcv.score(x_test,y_test)*float(100)\n\n# to plot confusion matrix\n\ncm = confusion_matrix(y_test,svm_gcv.predict(x_test))\nsns.heatmap(cm , annot = True )\n\n\n# calculation of F1 score\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[0,1]\nFP = cm[1,0]\n\nRecall = TP\/(TP+FN)\nPrecision = TP\/(TP+FP)\n\nsvm_f1_score = ((2 * Recall * Precision)\/(Recall + Precision))\n\nprint('f1_score of the model is {}'.format(svm_f1_score))\n\n\n# putting datas in list\n\nmodel.append('svm')\nf1_score.append(svm_f1_score)\naccuracy.append(svm_accuracy)","414f5e27":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\n\nparams_dt = {'max_depth':np.arange(1,10)}\n\ndt_gcv = GridSearchCV(dt , param_grid = params_dt)\n\ndt_gcv.fit(x_train , y_train)\n\nprint(\"Best C Value is \",dt_gcv.best_params_)\n\nprint(\"test accuracy \",(dt_gcv.score(x_test,y_test))*float(100))\n\ndecision_tree_accuracy = dt_gcv.score(x_test,y_test)*float(100)\n\n# to plot confusion matrix\n\ncm = confusion_matrix(y_test,dt_gcv.predict(x_test))\nsns.heatmap(cm , annot = True )\n\n# calculation of F1 score\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[0,1]\nFP = cm[1,0]\n\nRecall = TP\/(TP+FN)\nPrecision = TP\/(TP+FP)\n\ndecision_tree_f1_score = ((2 * Recall * Precision)\/(Recall + Precision))\n\nprint('f1_score of the model is {}'.format(decision_tree_f1_score))\n\n\n# putting datas in list\n\nmodel.append('decision_tree')\nf1_score.append(decision_tree_f1_score)\naccuracy.append(decision_tree_accuracy)","5e67bd40":"from sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrf = RandomForestClassifier()\n\nparams_rf = {'n_estimators' : np.arange(1,100,10) }\n\nrf_gcv = RandomizedSearchCV(rf , param_distributions = params_rf)\n\nrf_gcv.fit(x_train,y_train)\n\nprint(\"Best n_estimators Value is \",rf_gcv.best_params_)\n\n\nprint(\"test accuracy \",(rf_gcv.score(x_test,y_test))*float(100))\n\nrandom_forest_accuracy = rf_gcv.score(x_test,y_test)*float(100)\n\n# to plot confusion matrix\n\ncm = confusion_matrix(y_test,rf_gcv.predict(x_test))\nsns.heatmap(cm , annot = True )\n\n\n# calculation of F1 score\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[0,1]\nFP = cm[1,0]\n\nRecall = TP\/(TP+FN)\nPrecision = TP\/(TP+FP)\n\nrandom_forest_f1_score = ((2 * Recall * Precision)\/(Recall + Precision))\n\nprint('f1_score of the model is {}'.format(random_forest_f1_score))\n\n\n# putting datas in list\n\nmodel.append('random_forest')\nf1_score.append(random_forest_f1_score)\naccuracy.append(random_forest_accuracy)","7e6f96d9":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier()\n\nparams_xgb = {'learning_rate':[0.00001,0.0001,0.001,0.01,0.1,1],'n_estimators':np.arange(1,50,10),'max_depth':np.arange(1,10)}\n\n\nxgb_gcv = RandomizedSearchCV(xgb , param_distributions = params_xgb)\n\nxgb_gcv.fit(x_train,y_train)\n\nprint(\"Best parameters values are \",xgb_gcv.best_params_)\n\n\nprint(\"test accuracy \",(xgb_gcv.score(x_test,y_test))*float(100))\n\nxg_boost_accuracy = xgb_gcv.score(x_test,y_test)*float(100)\n\n# to plot confusion matrix\n\ncm = confusion_matrix(y_test,xgb_gcv.predict(x_test))\nsns.heatmap(cm , annot = True )\n\n\n# calculation of F1 score\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[0,1]\nFP = cm[1,0]\n\nRecall = TP\/(TP+FN)\nPrecision = TP\/(TP+FP)\n\nxg_boost_f1_score = ((2 * Recall * Precision)\/(Recall + Precision))\n\nprint('f1_score of the model is {}'.format(xg_boost_f1_score))\n\n\n# putting datas in list\n\nmodel.append('xg_boost')\nf1_score.append(xg_boost_f1_score)\naccuracy.append(xg_boost_accuracy)\n","433688d1":"# table for heart_failure_models\n\ndict = {'model': model, 'accuracy': accuracy, 'f1_score': f1_score}   \n\nheart_failure_prediction = pd.DataFrame(dict) \n\nheart_failure_prediction","bd6e9d3b":"# plotting to compare each models on accuracy\n\nfig_dims = (13, 4)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.barplot(x = model, y = accuracy, ax=ax)\n","81d684fa":"# plotting to compare each models on f1_score\n\nfig_dims = (13, 4)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.barplot(x = model, y = f1_score, ax=ax)","f7d2fc97":"# 3. NAIVE BAYES WITH GRID SEARCH CV","05410e4d":"# 7. X-GRADIENT BOOSTING WITH GRID SEARCH CV","0621adfb":"# 1.a) K-NN WITH SIMPLE CV","a5055168":"1. All codes are easy to understand\n2. XG boost,knn,naive have maximum accuracy and maximum f1_score for the given data.\n3. upvote if you found this notebook usefull\n4. Most important thing i observed while making these models is choosing correct features.","da420598":"# 6. RANDOM FOREST WITH RANDOMIZED SEARCH CV","19d42595":"# 2. LOGISTIC REGRESSION WITH GRID SEARCH CV","2c786237":"# 4. RBF SVM WITH GRID SEARCH CV","5fe54267":"# 1.b) KNN WITH GRID SEARCH CV","d99719d2":"# 5. Decision Tree WITH GRID SEARCH CV","3701d1b3":"here we can say that age,ejection_fraction,serum_creatinine,serum_sodium,time has impacted death event more\nbut after comparing with above seaborn analysis i analysed below features as important"}}