{"cell_type":{"8b4d6ada":"code","7ff177ce":"code","809d122d":"code","1c7da343":"code","7a2cfb83":"code","a788c667":"code","cf186e7c":"code","ab23c5d8":"code","ede22fb7":"code","23c2e3d5":"code","50555c46":"code","3933e154":"code","288a805d":"code","737070a3":"code","acb4b830":"code","0f64d6f7":"code","82fd90cd":"code","a18e210d":"code","e770fe25":"code","27155efb":"code","2ae8a4ab":"code","f0c5ad93":"markdown","e7ef7589":"markdown"},"source":{"8b4d6ada":"!pip install -q imageio\n!pip install -q opencv-python\n!pip install -q git+https:\/\/github.com\/tensorflow\/docs","7ff177ce":"import numpy as np\nimport pandas as pd\nimport cv2\nimport os\nfrom tqdm import tqdm\nimport random\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\nimport matplotlib.patches as patches\nimport imageio\nfrom IPython.display import HTML, display\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow_hub as hub\nfrom tensorflow_docs.vis import embed\nimport keras\nfrom keras.models import Sequential\n\nfrom sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom sklearn.model_selection import train_test_split","809d122d":"KEYPOINT_DICT = {\n    'nose': 0,'left_eye': 1,'right_eye': 2,'left_ear': 3,'right_ear': 4,'left_shoulder': 5,\n    'right_shoulder': 6,'left_elbow': 7,'right_elbow': 8,'left_wrist': 9,'right_wrist': 10,\n    'left_hip': 11,'right_hip': 12,'left_knee': 13,'right_knee': 14,'left_ankle': 15,\n    'right_ankle': 16\n}\n\nKEYPOINT_EDGE_INDS_TO_COLOR = {\n    (0, 1): 'm',(0, 2): 'c',(1, 3): 'm',(2, 4): 'c',(0, 5): 'm',(0, 6): 'c',\n    (5, 7): 'm',(7, 9): 'm',(6, 8): 'c',(8, 10): 'c',(5, 6): 'y',(5, 11): 'm',\n    (6, 12): 'c',(11, 12): 'y',(11, 13): 'm',(13, 15): 'm',(12, 14): 'c',(14, 16): 'c'\n}","1c7da343":"def _keypoints_and_edges_for_display(keypoints_with_scores,\n         height,width,keypoint_threshold=0.11):\n\n  keypoints_all = []\n  keypoint_edges_all = []\n  edge_colors = []\n  num_instances, _, _, _ = keypoints_with_scores.shape\n  for idx in range(num_instances):\n    kpts_x = keypoints_with_scores[0, idx, :, 1]\n    kpts_y = keypoints_with_scores[0, idx, :, 0]\n    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n    kpts_absolute_xy = np.stack(\n        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n    kpts_above_thresh_absolute = kpts_absolute_xy[\n        kpts_scores > keypoint_threshold, :]\n    keypoints_all.append(kpts_above_thresh_absolute)\n\n    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n          kpts_scores[edge_pair[1]] > keypoint_threshold):\n        x_start = kpts_absolute_xy[edge_pair[0], 0]\n        y_start = kpts_absolute_xy[edge_pair[0], 1]\n        x_end = kpts_absolute_xy[edge_pair[1], 0]\n        y_end = kpts_absolute_xy[edge_pair[1], 1]\n        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n        keypoint_edges_all.append(line_seg)\n        edge_colors.append(color)\n  if keypoints_all:\n    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n  else:\n    keypoints_xy = np.zeros((0, 17, 2))\n\n  if keypoint_edges_all:\n    edges_xy = np.stack(keypoint_edges_all, axis=0)\n  else:\n    edges_xy = np.zeros((0, 2, 2))\n  return keypoints_xy, edges_xy, edge_colors\n\n\ndef draw_prediction_on_image(\n    image, keypoints_with_scores, crop_region=None, close_figure=False,\n    output_image_height=None):\n\n  height, width, channel = image.shape\n  aspect_ratio = float(width) \/ height\n  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n  # To remove the huge white borders\n  fig.tight_layout(pad=0)\n  ax.margins(0)\n  ax.set_yticklabels([])\n  ax.set_xticklabels([])\n  plt.axis('off')\n\n  im = ax.imshow(image)\n  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n  ax.add_collection(line_segments)\n  # Turn off tick labels\n  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n\n  (keypoint_locs, keypoint_edges,\n   edge_colors) = _keypoints_and_edges_for_display(\n       keypoints_with_scores, height, width)\n\n  line_segments.set_segments(keypoint_edges)\n  line_segments.set_color(edge_colors)\n  if keypoint_edges.shape[0]:\n    line_segments.set_segments(keypoint_edges)\n    line_segments.set_color(edge_colors)\n  if keypoint_locs.shape[0]:\n    scat.set_offsets(keypoint_locs)\n\n  if crop_region is not None:\n    xmin = max(crop_region['x_min'] * width, 0.0)\n    ymin = max(crop_region['y_min'] * height, 0.0)\n    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n    rect = patches.Rectangle(\n        (xmin,ymin),rec_width,rec_height,\n        linewidth=1,edgecolor='b',facecolor='none')\n    ax.add_patch(rect)\n\n  fig.canvas.draw()\n  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n  image_from_plot = image_from_plot.reshape(\n      fig.canvas.get_width_height()[::-1] + (3,))\n  plt.close(fig)\n  if output_image_height is not None:\n    output_image_width = int(output_image_height \/ height * width)\n    image_from_plot = cv2.resize(\n        image_from_plot, dsize=(output_image_width, output_image_height),\n         interpolation=cv2.INTER_CUBIC)\n  return image_from_plot\n\n\ndef to_gif(images, fps):\n  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n  imageio.mimsave('.\/animation.gif', images, fps=fps)\n  return embed.embed_file('.\/animation.gif')\n\n\ndef progress(value, max=100):\n  return HTML(\"\"\"\n      <progress\n          value='{value}'\n          max='{max}',\n          style='width: 100%'\n      >\n          {value}\n      <\/progress>\n  \"\"\".format(value=value, max=max))","7a2cfb83":"model_name = \"movenet_lightning\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n\nif \"tflite\" in model_name:\n  if \"movenet_lightning_f16\" in model_name:\n    !wget -q -O model.tflite https:\/\/tfhub.dev\/google\/lite-model\/movenet\/singlepose\/lightning\/tflite\/float16\/4?lite-format=tflite\n    input_size = 192\n  elif \"movenet_thunder_f16\" in model_name:\n    !wget -q -O model.tflite https:\/\/tfhub.dev\/google\/lite-model\/movenet\/singlepose\/thunder\/tflite\/float16\/4?lite-format=tflite\n    input_size = 256\n  elif \"movenet_lightning_int8\" in model_name:\n    !wget -q -O model.tflite https:\/\/tfhub.dev\/google\/lite-model\/movenet\/singlepose\/lightning\/tflite\/int8\/4?lite-format=tflite\n    input_size = 192\n  elif \"movenet_thunder_int8\" in model_name:\n    !wget -q -O model.tflite https:\/\/tfhub.dev\/google\/lite-model\/movenet\/singlepose\/thunder\/tflite\/int8\/4?lite-format=tflite\n    input_size = 256\n  else:\n    raise ValueError(\"Unsupported model name: %s\" % model_name)\n\n  # Initialize the TFLite interpreter\n  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n  interpreter.allocate_tensors()\n\n  def movenet(input_image):\n\n    # TF Lite format expects tensor type of uint8.\n    input_image = tf.cast(input_image, dtype=tf.uint8)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n    # Invoke inference.\n    interpreter.invoke()\n    # Get the model prediction.\n    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n    return keypoints_with_scores\n\nelse:\n  if \"movenet_lightning\" in model_name:\n    module = hub.load(\"https:\/\/tfhub.dev\/google\/movenet\/singlepose\/lightning\/4\")\n    input_size = 192\n  elif \"movenet_thunder\" in model_name:\n    module = hub.load(\"https:\/\/tfhub.dev\/google\/movenet\/singlepose\/thunder\/4\")\n    input_size = 256\n  else:\n    raise ValueError(\"Unsupported model name: %s\" % model_name)\n\n  def movenet(input_image):\n\n    model = module.signatures['serving_default']\n\n    # SavedModel format expects tensor type of int32.\n    input_image = tf.cast(input_image, dtype=tf.int32)\n    # Run model inference.\n    outputs = model(input_image)\n    # Output is a [1, 1, 17, 3] tensor.\n    keypoint_with_scores = outputs['output_0'].numpy()\n    return keypoint_with_scores","a788c667":"data_dir='..\/input\/sports-classification\/images to predict'\nfiles=os.listdir(data_dir)\nprint(files[0:5])","cf186e7c":"paths=[]\nfor item in files:\n    paths+=[os.path.join(data_dir,item)]","ab23c5d8":"plt.figure(figsize=(5,5))\nimageT0 = tf.io.read_file(paths[1])\nimageT = tf.image.decode_jpeg(imageT0)\nprint(type(imageT))\nplt.imshow(imageT)\n_ = plt.axis('off')","ede22fb7":"def overlay(image):\n    input_image = tf.expand_dims(image, axis=0)\n    input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n\n    # Run model inference.\n    keypoint_with_scores = movenet(input_image)\n\n    # Visualize the predictions with image.\n    display_image = tf.expand_dims(image, axis=0)\n    display_image = tf.cast(tf.image.resize_with_pad(\n        display_image,128,128), dtype=tf.int32)\n    output_overlay = draw_prediction_on_image(\n        np.squeeze(display_image.numpy(), axis=0), keypoint_with_scores)\n\n    return output_overlay","23c2e3d5":"plt.figure(figsize=(5,5))\n#imageT0 = tf.io.read_file(image_path)\n#imageT = tf.image.decode_jpeg(imageT0)\noutput_overlay=overlay(imageT)\nplt.imshow(output_overlay)\n_ = plt.axis('off')","50555c46":"def overlay2(image):\n    input_image = tf.expand_dims(image, axis=0)\n    input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n\n    # Run model inference.\n    keypoint_with_scores = movenet(input_image)\n\n    # Visualize the predictions with image.\n    display_image = tf.expand_dims(image, axis=0)\n    display_image = tf.cast(tf.image.resize_with_pad(\n        display_image, 128, 128), dtype=tf.int32)\n    output_overlay = draw_prediction_on_image(\n        np.squeeze(display_image.numpy()*0, axis=0), keypoint_with_scores)\n\n    return output_overlay","3933e154":"plt.figure(figsize=(5,5))\n#imageT0 = tf.io.read_file(image_path)\n#imageT = tf.image.decode_jpeg(imageT0)\noutput_overlay=overlay2(imageT)\nplt.imshow(output_overlay)\n_ = plt.axis('off')","288a805d":"train_dir = '..\/input\/sports-classification\/train'","737070a3":"Name0=[]\nfor file in os.listdir(train_dir):\n    Name0+=[file]\nName=sorted(Name0)\nprint(Name)\nprint(len(Name))","acb4b830":"N=list(range(len(Name)))\nnormal_mapping=dict(zip(Name,N)) \nreverse_mapping=dict(zip(N,Name)) ","0f64d6f7":"trainset=list()\ntrainset2=list()\ntrainname=list()\nfor name in tqdm(Name):\n    path=os.path.join(train_dir,name)\n    for i in range(10):\n        im=os.listdir(path)[i]\n        img1=cv2.imread(os.path.join(path,im))\n        img2=cv2.resize(img1,dsize=(100,100),interpolation=cv2.INTER_CUBIC)\n        img3=overlay2(img2)\n        trainset.append(img3)\n        trainset2.append(img2)\n        trainname.append(name)","82fd90cd":"trainX=np.array(trainset)\ntrainX2=np.array(trainset2)","a18e210d":"trainY0=pd.Series(trainname).map(normal_mapping)","e770fe25":"m=len(trainX)\nM=list(range(m))\nrandom.seed(2021)\nrandom.shuffle(M)","27155efb":"fig, axs = plt.subplots(4,4,figsize=(16,16))\nfor i in range(16):\n    r=i\/\/4\n    c=i%4\n    ax=axs[r][c].axis(\"off\")\n    ax=axs[r][c].set_title(trainname[M[i]])\n    ax=axs[r][c].imshow(trainX[M[i]])\nplt.show()","2ae8a4ab":"fig, axs = plt.subplots(4,4,figsize=(16,16))\nfor i in range(16):\n    r=i\/\/4\n    c=i%4\n    ax=axs[r][c].axis(\"off\")\n    ax=axs[r][c].set_title(trainname[M[i]])\n    ax=axs[r][c].imshow(cv2.cvtColor(trainX2[M[i]],cv2.COLOR_BGR2RGB))\nplt.show()","f0c5ad93":"## MoveNet: Pose Detection Model\nhttps:\/\/tfhub.dev\/s?q=movenet","e7ef7589":"# Sports ImageMoveNet"}}