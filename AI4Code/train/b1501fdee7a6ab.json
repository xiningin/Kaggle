{"cell_type":{"ec97ff2f":"code","ff5a589d":"code","bce6e92d":"code","aa12c3e1":"code","89d67e91":"code","32308511":"code","fd8cc7a7":"code","afed48e4":"code","07183b07":"code","4a3e199b":"code","afc78f46":"code","aa8a443c":"code","1a354360":"code","f01f0c5f":"code","7a2217d9":"code","5ffb70e0":"code","f36cf558":"code","0af7c6ab":"code","0716a696":"markdown","33e5e188":"markdown","06ed8858":"markdown","bb4568d9":"markdown","41d10bfe":"markdown","33e59f67":"markdown","a76a6c39":"markdown","7c2129af":"markdown","d464084a":"markdown","b0020ae7":"markdown","25a2c5bd":"markdown","1e846a60":"markdown","68981a0c":"markdown","18644644":"markdown","87b447a0":"markdown","da2251db":"markdown","0b0c7263":"markdown","86acee16":"markdown","62c07b94":"markdown","398fa8b5":"markdown"},"source":{"ec97ff2f":"import shap\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nfrom tqdm import tqdm\nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ff5a589d":"train = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/train.csv\", index_col = 'id')\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/test.csv\", index_col = 'id')\ntrain = train[~train.drop('target', axis = 1).duplicated()]\n\nX = pd.DataFrame(train.drop(\"target\", axis = 1))\n\nlencoder = LabelEncoder()\ny = pd.DataFrame(lencoder.fit_transform(train['target']), columns=['target'])","bce6e92d":" params = { \n        'objective': 'multiclass', \n        'num_class' : 4, \n        'metric': 'multi_logloss' \n    } ","aa12c3e1":"    test_preds = None\n    train_rmse = 0\n    val_rmse = 0\n    n_splits = 10\n    \n    model =  LGBMClassifier(**params)\n    \n    skf = StratifiedKFold(n_splits = n_splits, shuffle = True,  random_state = 0)\n    \n    for tr_index , val_index in tqdm(skf.split(X.values , y.values), total=skf.get_n_splits(), desc=\"k-fold\"):\n\n        x_train_o, x_val_o = X.iloc[tr_index] , X.iloc[val_index]\n        y_train_o, y_val_o = y.iloc[tr_index] , y.iloc[val_index]\n        \n        eval_set = [(x_val_o, y_val_o)]\n        \n        model.fit(x_train_o, y_train_o, eval_set = eval_set, early_stopping_rounds=100, verbose=False)\n\n        train_preds = model.predict(x_train_o)\n        train_rmse += mean_squared_error(y_train_o ,train_preds , squared = False)\n\n        val_preds = model.predict(x_val_o)\n        val_rmse += mean_squared_error(y_val_o , val_preds , squared = False)\n\n        if test_preds is None:\n            test_preds = model.predict_proba(test.values)\n        else:\n            test_preds += model.predict_proba(test.values)\n\n    print(f\"\\nAverage Training RMSE : {train_rmse \/ n_splits}\")\n    print(f\"Average Validation RMSE : {val_rmse \/ n_splits}\\n\")\n\n    test_preds \/= n_splits","89d67e91":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(test)","32308511":"shap.summary_plot(shap_values, X)","fd8cc7a7":"selected_features = [\"feature_25\", \"feature_14\", \"feature_15\", \"feature_31\"]\n\nplt.figure(figsize=(20,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","afed48e4":"test[selected_features].describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","07183b07":"selected_features = [\"feature_14\", \"feature_15\", \"feature_6\", \"feature_16\", \"feature_31\", \"feature_37\"]\n\nplt.figure(figsize=(15,10))\nc = 1\nfor feat in selected_features:\n    plt.subplot(2, 3, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","4a3e199b":"test[selected_features].describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","afc78f46":" shap.summary_plot(shap_values[0], test)","aa8a443c":"selected_features = [\"feature_25\", \"feature_37\", \"feature_23\", \"feature_38\"]\n\nplt.figure(figsize=(30,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","1a354360":" shap.summary_plot(shap_values[1], test)","f01f0c5f":"selected_features = [\"feature_14\", \"feature_6\", \"feature_28\", \"feature_37\"]\n\nplt.figure(figsize=(30,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","7a2217d9":" shap.summary_plot(shap_values[2], test)","5ffb70e0":"selected_features = [\"feature_15\", \"feature_14\", \"feature_2\", \"feature_11\"]\n\nplt.figure(figsize=(30,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","f36cf558":" shap.summary_plot(shap_values[3], test)","0af7c6ab":"selected_features = [\"feature_31\", \"feature_24\", \"feature_14\", \"feature_16\"]\n\nplt.figure(figsize=(30,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","0716a696":"### HOW FEATURES IMPACT MODEL","33e5e188":"- Looks like most of values is ZERO(!) This is why you can observe some patterns later during class prediction analysis.\n- This could be a potential problem with the model. Which will divide classes with respect to 0. And if there is a majority of them then ...\n- In my opinion they in this competition is feature modeling - not dataset imbalance, not super model building ...","06ed8858":"### LOAD TPS-04 DATA AND PREPROCESS (QUICK WAY)","bb4568d9":"### LGBM CROSS VALIDATED TRAINING LOOP\nLGBM is one of the most popular algorith used in TPS-05. Let's look how it predict. Let's look how it sees the TPS-05 data.","41d10bfe":"## CLASS 3 - FEATURES IMPORTANCE","33e59f67":"Conclusion:\n- We see some interesting patters here - blue blobs\n- The most important feature is 15 and 14. Here we can see obvious pattens. In feature_15 small values (probalby 0) drives model to Class 2. Feature_14 - high values drive to Class 2.\n- Feature_2 there is seen good separation - all positive values (blue blob is 0) highly influence model in Class_2 direction.\n- Almost in all features from list we can see that \"high\" values drives model to Class_2","a76a6c39":"# Reference - [\"Interpretable Machine Learning, A Guide for Making Black Box Models Explainable.\"](https:\/\/christophm.github.io\/interpretable-ml-book\/) Christoph Molnar (updated 09.05.2021)\n","7c2129af":"## CLASS 0 - FEATURES IMPORTANCE","d464084a":"# My path to finding for best features ....\n\n**This is attempt for finding TPS-05 solution and best feature for model building. I decided to create simple model using LGBM and then analize what features drive model for each particular class.**\n\nInteresting in my TPS-05 notebooks?\n- [Pytorch NN for tabular - step by step](https:\/\/www.kaggle.com\/remekkinas\/tps-5-pytorch-nn-for-tabular-step-by-step)\n- [CNN (2D Convolution) for solving TPS-05](https:\/\/www.kaggle.com\/remekkinas\/cnn-2d-convolution-for-solving-tps-05)\n- [Weighted training - XGB, RF, LR, ... SMOTE](https:\/\/www.kaggle.com\/remekkinas\/tps-5-weighted-training-xgb-rf-lr-smote)\n- [HydraNet!! ... Keras Stacked Ensemble ..](https:\/\/www.kaggle.com\/remekkinas\/tps-5-hydranet-keras-stacked-ensemble)\n","b0020ae7":"## CLASS 1 - FEATURES IMPORTANCE","25a2c5bd":"Conclusion:\n- there is no obvious patters here, this is why model has problem with class 0 prediction - it works almost randomly (I think but I could be wrong)\n- as you can see reds are on the left and right - but we can see that most \"high\" values are on right side of chart \n- almost all features are 0 balanced ... so 0 can drive model to Class 0 and .... rest of Class -> ZERO plays main role in this competition. \n- Feature 25 - we can see that \"medium\" feature value (purple) drives model to predict class 0 \n- Feature 37 - the highest value (red) then class 0 is predicted,\n- Feature 23 - high values drive model to predict class 0\n- there is almost no uniform distribution","1e846a60":"As we can see:\n- Feature_14 and feature_15 affect the model the most, but you can also see the balance between classes (feature_15 plays big role in class_2 prediction) - they are definitely important variables\n- The most important feature for class_0 is feature_25, class_1 -> feature_14, class_2 -> feature_15, class_3 -> feature_31 \n","68981a0c":"Conclusion:\n- We see some interesting patters here - blue blobs\n- if most of features (till 20) are \"low\" they drives model to other class (not class 1, probably for class 3)\n- feature 14 and \"low\" values says that probaly there is no Class_1\n- feature 14->37 are uniform - \"higher\" values drive model to predict Class_1\n- feature_34 - \"higher\" values drive model to Class_0","18644644":"### Lets look inside TOP6 features .... ","87b447a0":"Conclusion:\n- We see some interesting patters here - blue blobs\n- From feature 31 to 23 ... we can see that small values drive model to Class_3. This is probably 0 (we shold see data in these feature).\n- The bigger number in features the less chance that Class 3 will be predicted.","da2251db":"- The same situation we can see here - ZERO rulez! ZERO vs ........ REST!!!","0b0c7263":"# CONCLUTIONS\n\nIn my opinion ....  focus points:\n- not model .... (low priority) -  same results you can achive using XGBoost, LightGBM, CatBoost (slight differences) ... and even poorly designed NN :) \n- not imbalance ..... (low priority) - weighted training is not a solution (see this notebook: https:\/\/www.kaggle.com\/remekkinas\/tps-5-weighted-training-xgb-rf-lr-smote ), sampling (uder\/over) is not a solution \n\nFeature \n- feature engineering (HIGH priority) - dealing with 0 which drives models to biased prediction\n- finding better ML model to deal with sparsity ...\n\nOf course, you may have a different opinion on this. Conversations below are welcome.","86acee16":"# SHAP model explainer","62c07b94":"## CLASS 2 - FEATURES IMPORTANCE","398fa8b5":"#### Shap in my opinion is absolutely one of the best tool you can use for model understanding and hacking (improving). In this competition I made many analysis using Shap which lead me to interesting solutions.  "}}