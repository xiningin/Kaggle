{"cell_type":{"0876c7e3":"code","26ed6d75":"code","150ef330":"code","dc049f4a":"code","927b6ad3":"code","b9a38c00":"code","3e32772c":"code","02162b67":"code","9ee7ef69":"code","12c1f1d9":"code","ac898a2f":"markdown","1e8c2b3f":"markdown","c4c90ff3":"markdown","fb2a2bd6":"markdown","52078af7":"markdown","d26d14e0":"markdown","4d1577a3":"markdown","1d6a68d1":"markdown","13e577c9":"markdown"},"source":{"0876c7e3":"!pip install kaggle-environments --upgrade","26ed6d75":"%%writefile agent-epsilon_greedy.py\n\nimport random\nrandom.seed(72)\n\nclass Agent:\n    \n    def __init__(self,eps=7e-1,eps_decay=0.99999):\n        self.best_machine = None\n        self.total_reward = 0\n        self.rewards = None \n\n        self.bandit_chosen = None\n        self.rewards = None\n        self.selections = None\n        self.epsilon = eps\n        self.decay = eps_decay\n        \n    def decay_eps(self):\n        self.epsilon = self.epsilon * self.decay\n        \n\nmyagent = Agent(eps=9e-1)\n\ndef agent(observation, configuration):\n    '''\n    '''\n    # Need to preserve this information across runs\n#     global best_machine, total_reward, rewards, bandit_chosen, rewards, selections, possible_machines\n#     global DEFAULT_EPS\n    \n    # Determine the reward-score of selection\n    def get_reward_score(bandit):\n        '''\n        '''\n        # No reward for no bandit\n        if bandit is None:\n            score = 0\n        # Average of rewards\n        else:\n            score = myagent.rewards[bandit]\/myagent.selections[bandit] \n        return score\n\n    \n    # Record reward from last round\n    if myagent.bandit_chosen is not None:\n        # Difference from last reward and ttoal reward\n        last_reward = observation.reward - myagent.total_reward\n        myagent.rewards[myagent.bandit_chosen] += last_reward\n        myagent.total_reward += last_reward\n        # Check if this is now the best solution\n        # On the first two runs, the solution is undefined or the last chosen\n        if (myagent.best_machine is None): \n            myagent.best_machine = myagent.bandit_chosen\n        # Use the average rewards to determine the \"best\"\n        elif get_reward_score(myagent.bandit_chosen) >= get_reward_score(myagent.best_machine):\n            myagent.best_machine = myagent.bandit_chosen\n            \n    \n    \n    # Number of vending machines to select & rewards so far from selections\n    if observation.step == 0:\n        myagent.selections = [0] * configuration.banditCount\n        myagent.possible_machines = list(range(configuration.banditCount))\n        myagent.rewards = [0] * configuration.banditCount\n\n\n    # Determine if explore or exploit\n    eps = myagent.epsilon\n    beta = random.random()\n    # Explore only eps portion of the time\n    explore = (beta < eps) or (myagent.best_machine is None)\n        \n    # Pick randomly if exploring\n    if explore:\n        myagent.bandit_chosen = random.choice(myagent.possible_machines)\n    # Find the best vending machine if exploiting\n    else:\n        myagent.bandit_chosen = myagent.best_machine\n    \n    # Record selection\n    myagent.selections[myagent.bandit_chosen] += 1\n    \n    # Decay (if any)\n    myagent.decay_eps()\n\n    return myagent.bandit_chosen","150ef330":"from kaggle_environments import make","dc049f4a":"env = make(\"mab\", debug=True)\n\nenv.run([\"agent-epsilon_greedy.py\", \"agent-epsilon_greedy.py\"])\nenv.render(mode=\"ipython\", width=800, height=300)","927b6ad3":"# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src=\"..\/input\/visualizing-reward-outcomes\/SimulationExplorer.py\", \n         dst= \"..\/working\/SimulationExplorer.py\")\n\n# import all our functions\nimport SimulationExplorer as Explorer","b9a38c00":"import random\nrandom.seed(27)\n\nclass Agent:\n    \n    def __init__(self,eps=5e-1, eps_decay=1.0):\n        self.best_machine = None\n        self.total_reward = 0\n        self.rewards = None \n\n        self.bandit_chosen = None\n        self.rewards = None\n        self.selections = None\n        self.epsilon = eps\n        self.decay = eps_decay\n        \n    def decay_eps(self):\n        self.epsilon = self.epsilon * self.decay","3e32772c":"def agent(observation, configuration):\n    '''\n    '''\n    \n    # Determine the reward-score of selection\n    def get_reward_score(bandit):\n        '''\n        '''\n        # No reward for no bandit\n        if bandit is None:\n            score = 0\n        # Average of rewards\n        else:\n            score = myagent.rewards[bandit]\/myagent.selections[bandit] \n        return score\n\n    \n    # Record reward from last round\n    if myagent.bandit_chosen is not None:\n        # Difference from last reward and ttoal reward\n        last_reward = observation.reward - myagent.total_reward\n        myagent.rewards[myagent.bandit_chosen] += last_reward\n        myagent.total_reward += last_reward\n        # Check if this is now the best solution\n        # On the first two runs, the solution is undefined or the last chosen\n        if (myagent.best_machine is None): \n            myagent.best_machine = myagent.bandit_chosen\n        # Use the average rewards to determine the \"best\"\n        elif get_reward_score(myagent.bandit_chosen) >= get_reward_score(myagent.best_machine):\n            myagent.best_machine = myagent.bandit_chosen\n            \n    \n    \n    # Number of vending machines to select & rewards so far from selections\n    if observation.step == 0:\n        print(f'Eps: {myagent.epsilon}')\n        myagent.selections = [0] * configuration.banditCount\n        myagent.possible_machines = list(range(configuration.banditCount))\n        myagent.rewards = [0] * configuration.banditCount\n\n\n    # Determine if explore or exploit\n    eps = myagent.epsilon\n    beta = random.random()\n    # Explore only eps portion of the time\n    explore = (beta < eps) or (myagent.best_machine is None)\n        \n    # Pick randomly if exploring\n    if explore:\n        myagent.bandit_chosen = random.choice(myagent.possible_machines)\n    # Find the best vending machine if exploiting\n    else:\n        myagent.bandit_chosen = myagent.best_machine\n    \n    # Record selection\n    myagent.selections[myagent.bandit_chosen] += 1\n    \n    # Decay (if any)\n    myagent.decay_eps()\n\n    return myagent.bandit_chosen","02162b67":"def rand_agent(obs,conf):\n    import numpy as np\n    return int(np.random.choice(np.arange(conf.banditCount)))","9ee7ef69":"all_eps = [1e-1, 5e-1, 7e-1, 9e-1, 9.5e-1]","12c1f1d9":"import time\n\n\nsims = {}\nfor e in all_eps[1:]:\n\n    for trial in range(2):\n        start_time = time.time()\n        decay = 0.99999 if trial % 2 else 1.0\n        myagent = Agent(e,decay)\n        env = make(\"mab\", debug=True)\n        env.run([agent, rand_agent])\n\n        #\n        name = f'egreed_{e}d{decay} v rand: #{trial}'\n        sims[name] = env\n        print(f'\\t{time.time()-start_time}')\n    \ntest = Explorer.SimViz(sims)\ntest.plot_total_reward()","ac898a2f":"# [Santa 2020 - The Candy Cane Contest:](https:\/\/www.kaggle.com\/c\/santa-2020\/) Epsilon-Greedy Solution","1e8c2b3f":"## Setup","c4c90ff3":"[This competion](https:\/\/www.kaggle.com\/c\/santa-2020\/) is a twist on the [multi-armed bandit problem](https:\/\/en.wikipedia.org\/wiki\/Multi-armed_bandit) where we compete with another agent to get the most candy canes from 100 vending machines that produce a candy cane by some unknown distiribution. To add icing to the cake, the vending machine reduces the likelihood of producing a candy cane every time an elf (agent) tests that machine.","fb2a2bd6":"## Evaluation","52078af7":"# Epsilon-Greedy ($\\epsilon$-greedy)","d26d14e0":"Here we're including the visualizer I made (see this notebook: https:\/\/www.kaggle.com\/mrgeislinger\/visualizing-reward-outcomes) ","4d1577a3":"## Agent","1d6a68d1":"There are some intesting strategies to this problem but here we'll try the simple epsilon-greedy solution where we randomly try out different vending machines (explore) $\\epsilon$ portion of the times and the rest of the time pick the most optimal machine tried so far.","13e577c9":"# Try Different Exploring Options"}}