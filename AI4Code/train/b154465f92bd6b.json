{"cell_type":{"a535357d":"code","2fddcff1":"code","85bfbae0":"code","9292adc2":"code","fbda8af0":"code","55fb8149":"code","f75719d1":"code","2ec9af8f":"code","8a73809a":"code","f907794e":"code","dda45531":"code","3af9cca8":"code","a01fe0b2":"code","4c7a8e9d":"code","1eefceb8":"code","5028d3b8":"code","e18b7363":"code","6071067b":"code","04b168dc":"code","46003996":"code","505a3461":"markdown","9918bdb8":"markdown","260263dd":"markdown","cc641b61":"markdown","060bfa76":"markdown","b40d42c3":"markdown","a9a7d199":"markdown","4c62f551":"markdown","0bd5a6e5":"markdown"},"source":{"a535357d":"# The models and stuff\nfrom sklearn.feature_extraction import text\nfrom sklearn.model_selection import (\n    train_test_split,\n    cross_val_score,\n    cross_val_predict,\n    RandomizedSearchCV,\n)\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\n\n# serialization of model\nimport joblib\n\n# for preprocessing of text\nimport nltk\n\n# representation of data\nimport numpy as np\nimport pandas as pd\n\n# path handling\nimport os\nimport glob\n\n# for parsing purposes\nimport mailbox\nimport codecs\nfrom io import StringIO\nfrom html.parser import HTMLParser\n#\n# These do not work in all cases. But mailbox can open all the files!!\n# import email\n# import email.policy\n\n# for text visualization\nimport textwrap\nimport pprint\n\n# type information, not relevant to actual task\nimport typing","2fddcff1":"DATADIR = \"datasets\/\"\nRUNNING_KAGGLE = os.path.isdir(\"\/kaggle\")\n\nif RUNNING_KAGGLE:\n    DATADIR = os.path.join(\n        DATADIR,\n        os.path.normpath(\"\/kaggle\/input\/spamassassin-public-corpus\/\")\n    )\n\nSPAM_2 = os.path.join(DATADIR, \"spam_2\/spam_2\" if RUNNING_KAGGLE else \"spam_2\")\nHARD_HAM = os.path.join(DATADIR, \"hard_ham\/hard_ham\" if RUNNING_KAGGLE else \"hard_ham\")\nEASY_HAM = os.path.join(DATADIR, \"easy_ham\/easy_ham\" if RUNNING_KAGGLE else \"easy_ham\")\n\n# not available in kaggle's dataset\nSPAM = os.path.join(DATADIR, \"spam\")","85bfbae0":"# a default function\n# Caveat: we can lose data if the tags are just innocent placeholders meant by\n# the user.\n# Eg: \"I'm sure you'll say <whatever>\"\n# Gotta lose something to gain another.\nstrip_tags = text.strip_tags\n\n\ndef _exists_encoding(enc: str) -> bool:\n    \"\"\"\n    Internal function; check if an encoding exists. Used by ``_get_charsets``.\n    \"\"\"\n    try:\n        codecs.lookup(enc)\n    except LookupError:\n        return False\n    return True\n\n\ndef _get_charsets(msg):\n    \"\"\"\n    Internal function; find the correct charset for the ``msg``. If no charset\n    can be found, use ``utf-8``.\n    \"\"\"\n    charset = \"\"\n    for c in msg.get_charsets():\n        if c is not None and _exists_encoding(c):\n            charset = c\n    return charset or \"utf-8\"\n\n\n# ============================================================================\n#                        The data extractor functions\n# ============================================================================\ndef get_body(data: str) -> str:\n    \"\"\"\n    Get the body from mailbox files.\n\n    This is the raw data and its output will be passed through ``strip_tags``.\n    \"\"\"\n    msg = mailbox.mboxMessage(data)\n    subject = msg[\"Subject\"] or \"\"\n    while msg.is_multipart():\n        msg = msg.get_payload()[0]\n    t = msg.get_payload(decode=True)\n\n    # let's use the subject line too. They seem interesting\n    return \"\\n\".join((subject, t.decode(_get_charsets(msg), errors=\"ignore\")))\n\n\ndef strip_whitespace(data: str, n: typing.Optional[int] = None) -> str:\n    \"\"\"\n    Return a string void of all whitespaces.\n\n    This will be used mostly for getting an insight into the data.\n    \"\"\"\n    return \" \".join(data.split()[:n])\n\n\ndef strip_all_lint(\n    raw: str,\n    strip_func: typing.Callable[[str], str] = text.strip_tags,\n) -> str:\n    \"\"\"\n    Helper shortcut function to perform data extraction in one go.\n\n    Uses ``sklearn.feature_extraction.text.strip_tags`` as ``strip_func`` by\n    default.\n    \"\"\"\n    return strip_func(get_body(raw))\n\n\n# ============================================================================\n#             Filepaths and data retrieval related functions\n# ============================================================================\ndef get_filepath(kind: str, num: int) -> str:\n    \"\"\"\n    Get the filepath, provided the prefix ``num``.\n\n    The datafiles in this dataset has a [num]-[hash] format, and only the\n    ``num`` will be enough for exploration.\n    \"\"\"\n    # the zeroth corpus is BS.\n    assert not (kind == SPAM and num == 0), \"Use num=1 to get from spam set.\"\n\n    filepaths = glob.glob(os.path.join(kind, f\"*{num}.*\"))\n    return filepaths[0]\n\n\ndef get_cleaned_corpus(file: str) -> str:\n    \"\"\"\n    Returns the body of the email, which is preprocessed by other functions.\n    \"\"\"\n    with open(file, errors=\"backslashreplace\") as f:\n        return strip_all_lint(f.read())","9292adc2":"# Helper function for getting insight\ndef peek_into_data(\n    data: str,\n    *,\n    l: int = 0,\n    h: typing.Optional[int] = 200,\n    strip_func: typing.Callable[[str], str] = text.strip_tags,\n) -> None:\n    \"\"\"\n    Simple function to look into the data, both as raw and processed form.\n    \"\"\"\n    print(\"=\"*10, \"The raw data:\")\n    print(data[l:h])\n    print(\"=\"*10, \"\\n\")\n\n    body = get_body(data)\n    print(\"=\"*10, \"The body of email:\")\n    print(body[l:h])\n    print(\"=\"*10, \"\\n\")\n\n    cleaned = strip_func(body)\n    print(\"=\"*10, \"The body, void of any HTML tags:\")\n    print(cleaned[l:h])\n    print(\"=\"*10, \"\\n\")\n\n    print(\"=\"*10, \"The cleaned data, void of any whitespace or newline (wrapped):\")\n    print(textwrap.fill(strip_whitespace(cleaned[l:h]), 79))\n    print(\"=\"*10)","fbda8af0":"# we have to use 'backslashreplace'; not everything on the internet is UTF-8,\n# esp when the data is from a long gone past.\n# TODO: Can we use binary?\n# 247\nwith open(get_filepath(SPAM_2, 201), errors=\"backslashreplace\") as file:\n    raw = file.read()","55fb8149":"peek_into_data(raw)","f75719d1":"def get_list_of_paths(\n    seed: typing.Optional[int] = None,\n    *,\n    max_num: typing.Optional[int] = None,\n) -> typing.Tuple[typing.List[str], typing.List[str]]:\n    \"\"\"\n    Get all the data paths of ham and spam.\n    Setting ``seed`` to ``None`` means the random number generator will not be\n    deterministic.\n    \n    Returns tuple of (ham paths, spam paths).\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    fetch = lambda kind: glob.glob(os.path.join(kind, \"*.*\"))\n\n    all_ham = fetch(EASY_HAM) + fetch(HARD_HAM)\n    all_spam = fetch(SPAM) + fetch(SPAM_2)\n\n    try:\n        all_spam.remove(\n            os.path.join(\n                SPAM,\n                # this file is useless.\n                \"0000.7b1b73cf36cf9dbc3d64e3f2ee2b91f1\",\n            )\n        )\n    except ValueError:\n        # probably running in kaggle\n        pass\n\n    rng.shuffle(all_ham)\n    rng.shuffle(all_spam)\n\n    if max_num is not None:\n        return all_ham[:max_num], all_spam[:max_num]\n\n    # the ``n`` becomes whichever list's length is less\n    if len(all_ham) > len(all_spam):\n        return all_ham[:len(all_spam)], all_spam\n    return all_ham, all_spam[:len(all_ham)]","2ec9af8f":"# Step 1: Get the shuffled list of paths.\nham, spam = get_list_of_paths(42)\n\nprint(f\"No. of hams: {len(ham)}\\nNo. of spams: {len(spam)}\")","8a73809a":"# Step 2 and 3: Load data into memory and apply a label (ham -> 1, spam -> 0)\nham_df = pd.DataFrame(\n    [(get_cleaned_corpus(x), 0) for x in ham],\n    columns=[\"text\", \"label\"],\n)\n\nspam_df = pd.DataFrame(\n    [(get_cleaned_corpus(x), 1) for x in spam],\n    columns=[\"text\", \"label\"],\n)\n\ndf = pd.concat([spam_df, ham_df], ignore_index=True)\ndf.tail()","f907794e":"# Step 4: Remove all sorts of whitespaces and stem the sentences\n# This step is important and much more can be done.\nstem = nltk.SnowballStemmer(\"english\")\n\nstemmer = stem.stem\ndf[\"text\"] = (\n    df[\"text\"]\n    .apply(nltk.word_tokenize)\n    .apply(lambda x: [stemmer(i) for i in x])\n    .str.join(\" \")\n)\ndf.tail()","dda45531":"# Step 5: Splitting the dataset into train and test set.\ntrain, test = train_test_split(df, stratify=df[\"label\"])\n\nprint(\"Train dataset:\", train[\"label\"].value_counts(), sep=\"\\n\")\nprint(\"Test dataset:\", test[\"label\"].value_counts(), sep=\"\\n\")","3af9cca8":"# Step 1: The pipeline\npipe = Pipeline(\n    [\n        (\"vec\", text.TfidfVectorizer()),\n        (\"model\", RandomForestClassifier())\n    ]\n)","a01fe0b2":"# Step 2: The randomized search\n# Add as many hyperparameters as you like.\nparams = {\n    # vectorizer params\n    \"vec__sublinear_tf\": [True, False],\n    \"vec__max_features\": [None, *range(100, 1000, 100)],\n    \"vec__ngram_range\": [(1, 1), (1, 2), (2, 2), (2, 3), (3, 3)],\n    \"vec__strip_accents\": [\"unicode\", \"ascii\", None],\n    \"vec__norm\": [\"l1\", \"l2\"],\n    \"vec__stop_words\": [\"english\", None],\n    \n    # model params\n    \"model__criterion\": [\"gini\", \"entropy\"],\n    \"model__n_estimators\": [*range(100, 800, 100)],\n    \"model__n_jobs\": [-1],\n}\n\ngrid = RandomizedSearchCV(pipe, params, cv=5, n_jobs=-1)\ngrid.fit(train[\"text\"], train[\"label\"].values)","4c7a8e9d":"# Check the best parameter values.\nprint(\"Best params:\")\npprint.pprint(grid.best_params_)\n\nprint(\"Best Score:\", grid.best_score_)","1eefceb8":"# best estimator is our model.\nmodel = grid.best_estimator_","5028d3b8":"# Step 3: Additional scores like f1 score and cross val predict\ny_train_pred = cross_val_predict(model, train[\"text\"], train[\"label\"], cv=3, n_jobs=-1)\n\nprint(\"f1 score:\", f1_score(train[\"label\"], y_train_pred))\nprint(\"Confusion matrix:\", confusion_matrix(train[\"label\"], y_train_pred), sep=\"\\n\")","e18b7363":"model.score(test[\"text\"], test[\"label\"])","6071067b":"def predict_message(msg: str) -> str:\n    \"\"\"\n    Predicts whether the message is spam or ham and returns a probability.\n    \"\"\"\n    preds = model.predict_proba(\n            [strip_all_lint(msg)]\n        )[0]\n    return {\"HAM\": preds[0], \"SPAM\": preds[1]}","04b168dc":"predict_message(\"\"\"\\\n In computer programming, glob patterns specify sets of filenames with\nwildcard characters. For example, the Unix Bash shell command mv *.txt\ntextfiles\/ moves (mv) all files with names ending in .txt from the\ncurrent directory to the directory textfiles. Here, * is a wildcard\nstanding for \"any string of characters\" and *.txt is a glob pattern.\nThe other common wildcard is the question mark (?), which stands for\none character. For example, mv ?.txt shorttextfiles\/ will move all\nfiles named with a single character followed by .txt from the current\ndirectory to directory shorttextfiles, while ??.txt would match all\nfiles whose name consists of 2 characters followed by .txt\n\"\"\")","46003996":"joblib.dump(model, \"pipeline-tfidf-random_forest.pkl\")","505a3461":"# Classification Problem: Email spam-or-ham classifier\n\nCh3-Q4 of Hands-on ML with Scikit Learn and tensorflow 1e","9918bdb8":"# Arranging the data\n\n## The plan\n\n1.  Combine and shuffle the paths of ``easy_ham`` and ``hard_ham``, and keep\n    only ``n`` nos. of paths, Where, ``n`` equals the number of files in\n    ``spam_2``.\n\n    Note: ___non-Kaggle only___: We might also combine and shuffle the paths of\n    ``spam`` and ``spam_2``, and ``n`` becomes whichever has less paths.\n    \n2.  Since we are __not__ doing any online learning (and it wouldn't make sense\n    for ``tfidf`` and ``RandomForestClassifier``), we will load data directly\n    into memory.\n\n3.  After loading into memory, we will associate ``spam`` as __1__ and ``ham``\n    as __0__.\n    \n4.  We will perform some data cleaning as we saw above in the [experiment](#Experiment-with-data-cleaners.).\n\n    We are going to keep this step simple (ie, some lemmatization, stemming etc.),\n    although you can add more steps as you like.\n    \n5.  Stratified sampling is very important.\n\n    We want to make sure that the number of hams and spams are equally\n    represented in the train and test dataset.","260263dd":"# Preparing the model\n\n1.  We will create a pipeline. This allows us to achieve greater flexibility.\n\n2.  Next, we will perform a randomized search (as grid search will be extremely slow).\n\n    Using the randomized search, we will select the best model.\n    \n3.  We will calculate some additional scores.","cc641b61":"# Testing our model","060bfa76":"# The parsers, data cleaners and filepath handlers.","b40d42c3":"# Serialization of model","a9a7d199":"And later restore it with\n\n```python\npipe = joblib.dump(\"pipeline-tfidf-random_forest.pkl\")\n```","4c62f551":"# Basic Imports\n\n> Add your imports here, and optionally describe its usage as a comment","0bd5a6e5":"## Experiment with data cleaners.\n\nBasic steps:\n\n1.  Open the file and read contents\n2.  Fetch the body of the message\n3.  Clean the body by stripping the HTML tags.\n4.  Split the cleaned text?"}}