{"cell_type":{"6e2b292a":"code","cb7fbe0b":"code","ab95457f":"code","12402265":"code","01964219":"code","3392b903":"code","a02e8494":"code","a50be288":"code","49680f2b":"code","4c6a05e8":"code","f5ed22d5":"code","976d7647":"code","6addd601":"code","cefe2d38":"code","34fb4a7a":"code","30a811e7":"markdown","761abf2e":"markdown","c7e93d0e":"markdown","e1a01812":"markdown","7798df44":"markdown"},"source":{"6e2b292a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","cb7fbe0b":"data=pd.read_csv('..\/input\/data.csv')\ndata.head()","ab95457f":"X_ori=data\n# y is the labels\ny_ori=X_ori.diagnosis\n# Drop the unrelated colomns\n# X is the instances\ndropping_col = ['id','diagnosis','Unnamed: 32']\nX_ori=X_ori.drop(dropping_col,axis=1)\nprint(X_ori.shape)\nX_ori.head()","12402265":"# Normalize data\n# Normalization (z-score)\nX=(X_ori - X_ori.mean()) \/ (X_ori.std())\ny=[1 if label=='M' else 0 for label in y_ori]\n#data_nm = (data_ins-data_ins.min())\/(data_ins.max()-data_ins.min())\nX.head()","01964219":"# DataFrame to np\nX=X.values\ny=np.array(y)\nattributes=['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']","3392b903":"from sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nimport time\n# SVM_linear_kernel\ndef SVM(X_train,y_train,X_test,y_test,C=0.1):       # X,y are numpy\n    clf = SVC(kernel='linear',C=C)\n    clf.fit(X_train, y_train)  \n    # train accuracy\n    pred_train=clf.predict(X_train)\n    res_train=[pred_train==y_train]\n    acc_train=np.mean(np.array(res_train).astype(np.int))\n    # test accuracy\n    pred_test=clf.predict(X_test)\n    res_test= [pred_test==y_test]\n    acc_test=np.mean(np.array(res_test).astype(np.int))\n    return acc_train,acc_test\n\ndef SVM_iter(X,y,k=100,C=0.1):\n    SVM_train_acc=[]\n    SVM_test_acc=[]\n    for _ in range(k):\n        # SVM\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n        acc_train,acc_test=SVM(X_train,y_train, X_test,y_test,C)\n        SVM_train_acc.append(acc_train)\n        SVM_test_acc.append(acc_test)\n    return np.array(SVM_train_acc).mean(),np.array(SVM_test_acc).mean()","a02e8494":"# SVM\n# start=time.time()\n# train_acc=[]\n# test_acc=[]\n# for i in range(100):\n#     X_train, X_test, y_train, y_test = train_test_split(X_ori, y_ori, test_size=0.3)\n#     acc_train,acc_test=SVM(X_train,y_train, X_test,y_test)\n#     train_acc.append(acc_train)\n#     test_acc.append(acc_test)\n# print(np.array(train_acc).mean())\n# print(np.array(test_acc).mean())\n# print('Original data operation time: %f'%(time.time()-start))\n\nprint()\nstart=time.time()\ntrain_acc=[]\ntest_acc=[]\nfor i in range(100):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n    acc_train,acc_test=SVM(X_train,y_train, X_test,y_test)\n    train_acc.append(acc_train)\n    test_acc.append(acc_test)\nprint(np.array(train_acc).mean())\nprint(np.array(test_acc).mean())\nprint('Normalized data operation time: %f'%(time.time()-start))","a50be288":"from sklearn.feature_selection import RFE\nselector_SVM = RFE(SVC(kernel='linear',C=0.1),1, step=1)\nselector_SVM = selector_SVM.fit(X, y)\nprint(selector_SVM.ranking_)\nprint([attributes[i] for i in np.argsort(selector_SVM.ranking_)])","49680f2b":"def SVM_Select(X,y):\n    train_acc=[]\n    test_acc=[]\n    for k in range(X.shape[1]):\n        selector_SVM = RFE(SVC(kernel='linear',C=0.1),k+1, step=1)\n        X_trans = selector_SVM.fit(X, y).transform(X)\n        acc_train,acc_test=SVM_iter(X_trans,y)\n        train_acc.append(acc_train)\n        test_acc.append(acc_test)\n    return train_acc,test_acc","4c6a05e8":"import matplotlib.pyplot as plt\nSVM_train_acc,SVM_test_acc=SVM_Select(X,y)\nplt.plot(SVM_test_acc)\nplt.ylim([0.8,1])","f5ed22d5":"print(SVM_test_acc)\nplt.plot([i+1 for i in range(30)],SVM_train_acc,c='red',label='training accuracy')\nplt.plot([i+1 for i in range(30)],SVM_test_acc,c='green',label='testing accuracy')\nplt.scatter([i+1 for i in range(30)],SVM_train_acc,c='red',s=10)\nplt.scatter([i+1 for i in range(30)],SVM_test_acc,c='green',s=10)\nplt.title('Training & Testing Accuracy for SVM')\nplt.xlabel('features')\nplt.ylabel('accuracy')\nplt.ylim([0.8,1])\nplt.legend(loc='best')\nplt.show()","976d7647":"from sklearn.metrics import confusion_matrix\n# SVM\ndef SVM_CM(X,y):       # X,y are numpy\n    matrix=np.array([[0,0],[0,0]])\n    for i in range(100):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n        clf = SVC(kernel='linear',C=0.1)\n        clf.fit(X_train, y_train)  \n        pred_test=clf.predict(X_test)\n        matrix+=confusion_matrix(y_test, pred_test)\n    \n    tn, fp, fn, tp=matrix.ravel()\n    print(tn, fp, fn, tp)\n    accuracy=(tn+tp)\/(tn+fp+fn+tp)\n    precision=(tp)\/(tp+fp)\n    recall=tp\/(tp+fn)\n    specificity=tn\/(tn+fp)\n    f1=(2*precision*recall)\/(precision+recall)\n    matrix=(matrix\/100+0.5).astype(int)\n    return matrix,np.array([[accuracy,precision,recall,specificity,f1]])","6addd601":"import seaborn as sns\n# SVM selection -> 14 features\nselector_SVM = RFE(SVC(kernel='linear',C=0.1),14, step=1)\nX_SVM = selector_SVM.fit(X, y).transform(X)\nres,metrics_SVM_trans=SVM_CM(X_SVM,y)\nsns.heatmap(res,annot=True,fmt=\"d\")\nplt.show()\nprint(metrics_SVM_trans)","cefe2d38":"from sklearn.decomposition import PCA\n# feature selection -> PCA\npca = PCA(n_components=0.99)\npca.fit(X_SVM)\nX_pca_SVM=pca.transform(X_SVM)\nprint(X_SVM.shape)\nprint(X_pca_SVM.shape)","34fb4a7a":"rain_acc=[]\ntest_acc=[]\nfor i in range(100):\n    X_train, X_test, y_train, y_test = train_test_split(X_pca_SVM, y, test_size=0.3)\n    acc_train,acc_test=SVM(X_train,y_train, X_test,y_test)\n    train_acc.append(acc_train)\n    test_acc.append(acc_test)\nprint(np.array(train_acc).mean())\nprint(np.array(test_acc).mean())","30a811e7":"DataFrame to numpy.","761abf2e":"Finally, SVM with linear kernels reaches 98.05% testing accuracy with only 9 features.","c7e93d0e":"Drop id, diagnosis and Unnamed: 32 to get our X_ori, which is the original X without data pre-processing.","e1a01812":"Z-score normalization. Set label 'M' to be 1, label 'B' to be 0.","7798df44":"Read and have a look at the data."}}