{"cell_type":{"aeafd33d":"code","fa1c5351":"code","232cb1ad":"code","bf8a2ef0":"code","b7a66f2c":"code","9f969f7a":"code","9786d5db":"code","5ec280ed":"code","7eb3d079":"code","65c54951":"code","37b45306":"code","93d3acc9":"code","34f613ae":"code","9e9e19ed":"code","6570b9b0":"code","178c45fc":"code","80f4f2c2":"code","8e5201b4":"markdown","43734636":"markdown","bd36996f":"markdown","eaa2287f":"markdown","157b31bc":"markdown","7ed5b2af":"markdown","9240426d":"markdown","fff90e0a":"markdown"},"source":{"aeafd33d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fa1c5351":"from fastai.text.all import *","232cb1ad":"df_lm = pd.read_csv('..\/input\/avatar-the-last-air-bender\/avatar.csv',encoding='latin1') ","bf8a2ef0":"df_lm.info()","b7a66f2c":"df_lm.head()","9f969f7a":"df_lm['chapter'].unique()","9786d5db":"def get_scripts(df):\n    scripts = []\n    for episode in df['chapter'].unique():\n        script = ''\n        for i, obj in df.iterrows():\n            if obj['chapter'] == episode:\n                script += obj['character'] + \": \" + obj['full_text'] + \" \"\n        \n        scripts.append(script)\n    return pd.DataFrame(scripts)\n\nscripts_ = get_scripts(df_lm)\nscripts_ = scripts_.apply(lambda x: x.replace(\"\\n\", \" \"))","5ec280ed":"scripts_","7eb3d079":"dls = TextDataLoaders.from_df(scripts_, is_lm=True)\ndls.show_batch(max_n=3)","65c54951":"learn = language_model_learner(dls, AWD_LSTM, pretrained=True, drop_mult=0.5, metrics=[accuracy, perplexity])","37b45306":"learn.lr_find()","93d3acc9":"learn.fit_one_cycle(5, 2e-2, \n                    #cbs=EarlyStoppingCallback(min_delta=0.005, monitor='valid_loss', patience=2)\n                   )","34f613ae":"learn.unfreeze()\nlearn.lr_find()","9e9e19ed":"learn.fit_one_cycle(8, slice(1e-5, 2e-3))","6570b9b0":"learn.recorder.plot_loss()","178c45fc":"start = \"The scene opens\"\ndef write_episode(start):\n\n    N_WORDS = 400\n    N_SENTENCES = 1\n    preds = [learn.predict(start, N_WORDS) \n             for _ in range(N_SENTENCES)]\n\n    return (\" \".join(preds)) + \". THE END.\"\n\n","80f4f2c2":"ep = write_episode(start)\nprint(ep)","8e5201b4":"![](http:\/\/imgix.bustle.com\/uploads\/image\/2020\/6\/1\/65220bb4-d554-403f-a682-d704bd217007-atla-iroh-tea.jpg?w=1200&h=630&fit=crop&crop=faces&fm=jpg)","43734636":"THE END","bd36996f":"Lets have a little fun with fastai. Im going to use a language model from fastai to generate new episodes of Avatar the last airbender. Cause we all wanted more episodes.","eaa2287f":"# Time to get serious","157b31bc":"The TextDataLoaders object just created a dataset where the independent variable is a sequence of antecedent from the avatar corpus and the dependent variable is a sequence of words index offset by one. It also takes care of the tokenization process for us.","7ed5b2af":"![](http:\/\/cdn.vox-cdn.com\/thumbor\/mXo5ObKpTbHYi9YslBy6YhfedT4=\/95x601:1280x1460\/1200x800\/filters:focal(538x858:742x1062)\/cdn.vox-cdn.com\/uploads\/chorus_image\/image\/66699060\/mgidarccontentnick.comc008fa9d_d.0.png)","9240426d":"The data object rows are broken up by scene. Each full_text entry is the full text for a scene. So lets make a function that gets the full script for each episode. ","fff90e0a":"More tweeks obviously need to be made. Let me know in the comments if any of you have suggestions. "}}