{"cell_type":{"d54eae5b":"code","910c7737":"code","4800a8c2":"code","f0b0962f":"code","d9125246":"code","d32f7569":"code","3fe8f3cb":"code","f1b832bc":"code","e5a41024":"code","676981b9":"code","01e59e2a":"code","1493bfe7":"code","5def22b3":"code","c4e8f543":"code","243bd23c":"code","a3b0cd60":"code","0b0c7687":"code","75253c67":"code","3f1deb90":"code","41142554":"code","0190f567":"code","f482f5b4":"code","8f2e940c":"code","dbe8abe4":"code","f51e95f6":"code","b52a6d7e":"code","2e97235b":"code","900bafe3":"markdown","77cf1092":"markdown","402d937e":"markdown","2eec25d3":"markdown","1bb254ee":"markdown","9e082d96":"markdown","5e25ff23":"markdown"},"source":{"d54eae5b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport re\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom nltk.tokenize import word_tokenize \nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english')) \nfrom nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer() \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report\nimport xgboost as xgb\nfrom xgboost import XGBClassifier, DMatrix","910c7737":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4800a8c2":"df_train = pd.read_excel('..\/input\/Data_Train.xlsx')\ndf_test = pd.read_excel('..\/input\/Data_Test.xlsx')","f0b0962f":"df_train.sample(5)","d9125246":"# Detecting the possible missing values\ndf_train.isna().sum()\n# None found","d32f7569":"#plot\nplt.figure(figsize=(8,5))\nlabels = list(set(df_train['SECTION']))\ncounts = []\nfor label in labels:\n    counts.append(np.count_nonzero(df_train['SECTION'] == label))\nplt.pie(counts, labels=labels, autopct='%1.1f%%')\nplt.show()","3fe8f3cb":"def nlp_processing(text):\n    \"\"\" Function to clean the text of misleading characters and even out it's representation\n        in the feature space.\n    \"\"\"\n    # Convert all text to lower case\n    text = text.lower()\n    # remove special characters, numbers, punctuations\n    text = re.sub(r'[^\\w\\s]','',text)\n    text = re.sub(r'[0-9]+', '', text)\n    # Remove extra whitespaces\n    text = re.sub('[\\s]+', ' ', text)\n    # converting the text to tokens\n    text_tokens = word_tokenize(text)\n    # Lemmatization the tokens\n    text_tokens = [lemmatizer.lemmatize(i) for i in text_tokens] # stemming\n    text_tokens = [w for w in text_tokens if w not in stop_words and not w.isdigit()]\n    text = ' '.join(text_tokens)\n    return text","f1b832bc":"# Cleaning the training data\ndf_train['STORY'] = df_train['STORY'].apply(nlp_processing)\n# Cleaning the test data\ndf_test['STORY'] = df_test['STORY'].apply(nlp_processing)","e5a41024":"X = df_train['STORY']\ny = df_train['SECTION']","676981b9":"tfidf_vec = tfidf = TfidfVectorizer(ngram_range = (1,3),stop_words = stop_words)\ntfidf_vec.fit(X)\nx_vec = tfidf_vec.transform(X)","01e59e2a":"smt = SMOTE(random_state=777, k_neighbors=1)\nX_SMOTE, y_SMOTE = smt.fit_sample(x_vec, y)","1493bfe7":"#plot\nplt.figure(figsize=(8,5))\nlabels = list(set(y_SMOTE))\ncounts = []\nfor label in labels:\n    counts.append(np.count_nonzero(y_SMOTE == label))\nplt.pie(counts, labels=labels, autopct='%1.1f%%')\nplt.show()","5def22b3":"x_train_vec, x_eval_vec, y_train, y_eval = train_test_split(X_SMOTE,y_SMOTE, test_size = 0.2, shuffle = True)","c4e8f543":"#Importing necessary libraries\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import mean_squared_error","243bd23c":"#Converting the dataframe into XGBoost\u2019s Dmatrix object\ndtrain = DMatrix(x_train_vec, label=y_train)","a3b0cd60":"#Bayesian Optimization function for xgboost\n#specify the parameters you want to tune as keyword arguments\ndef bo_tune_xgb(max_depth, gamma, n_estimators ,learning_rate):\n    params = {'max_depth': int(max_depth),\n              'gamma': gamma,\n              'n_estimators': int(n_estimators),\n              'learning_rate':learning_rate,\n              'subsample': 0.8,\n              'eta': 0.1,\n              'eval_metric': 'rmse'}\n    #Cross validating with the specified parameters in 5 folds and 70 iterations\n    cv_result = xgb.cv(params, dtrain, num_boost_round=25, nfold=5)\n    #Return the negative RMSE\n    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]","0b0c7687":"#Invoking the Bayesian Optimizer with the specified parameters to tune\nxgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (3, 7), \n                                             'gamma': (0.001, .01),\n                                             'learning_rate':(0,1),\n                                             'n_estimators':(20,50)\n                                            })","75253c67":"\"\"\"performing Bayesian optimization for 5 iterations with 2 steps of random exploration\n   with an acquisition function of expected improvement\"\"\"\nxgb_bo.maximize(n_iter=5, init_points=2, acq='ei')","3f1deb90":"#Extracting the best parameters\nparams = xgb_bo.max['params']\n#Converting the max_depth and n_estimator values from float to int\nparams['max_depth']= int(params['max_depth'])\nparams['n_estimators']= int(params['n_estimators'])\n#Initialize an XGBClassifier with the tuned parameters and fit the training data\nclassifier = XGBClassifier(**params).fit(x_train_vec, y_train)\n\n#predicting for training set\ny_preds = classifier.predict(x_eval_vec)\n\n#Looking at the classification report\nprint(classification_report(y_preds, y_eval))","41142554":"#Attained prediction accuracy on the training set\ncm = confusion_matrix(y_preds, y_eval)\nacc = cm.diagonal().sum()\/cm.sum()\nprint(acc)","0190f567":"# Linear SVC\nsvc = LinearSVC(C=10.0)\nprint((cross_val_score(svc,x_vec,y, cv = 7, verbose = False)).mean())","f482f5b4":"svc = LinearSVC(C=10.0)\nsvc.fit(x_train_vec,y_train)\ny_preds = svc.predict(x_eval_vec)","8f2e940c":"# Assessing performance on validation set\nprint(accuracy_score(y_preds,y_eval))\nprint(precision_score(y_eval,y_preds, average = 'macro'))\nprint(recall_score(y_eval, y_preds, average = 'macro'))","dbe8abe4":"#Looking at the classification report\nprint(classification_report(y_preds, y_eval))","f51e95f6":"# Training the model on whole data\nclassifier = LinearSVC(C=10.0).fit(x_vec, y)","b52a6d7e":"test_data = tfidf.transform(df_test['STORY'])\npreds = classifier.predict(test_data)","2e97235b":"submission_df = pd.DataFrame(preds)\nsubmission_df.to_excel('submission.xlsx', index=False,header = ['SECTION'])","900bafe3":"#### Conversion of text data to numerical features","77cf1092":"#### Submission:","402d937e":"#### SVM Classification","2eec25d3":"There are four distinct sections where each story may fall in to. The Sections are labelled as follows :\n* Politics: 0\n* Technology: 1 (the most prevalent cateogory in the given dataset)\n* Entertainment: 2 \n* Business: 3","1bb254ee":"#### XGBoost Classification:","9e082d96":"#### Exploratory Data Analysis:","5e25ff23":"#### Text Preprocessing:"}}