{"cell_type":{"8f3ea153":"code","f165ba2f":"code","8983f508":"code","b8fe355b":"code","71d95908":"code","0634ddbf":"code","869d314a":"code","b2a0c59c":"code","0055f157":"code","b7434808":"code","03091eee":"code","db051a73":"code","72e260fb":"code","6e5397ad":"code","56f64de3":"code","a529a1da":"code","ad6cc04e":"code","a9d7f039":"code","9088e479":"code","114fbbae":"code","27258748":"code","f17fc9b8":"code","9c95c60d":"code","3bf9c93f":"code","4060b0a2":"code","9c16e62b":"code","bcb55b56":"code","e3e226ec":"code","7b84f6a7":"code","3fbe1eb0":"code","2d524bf2":"code","192d71f1":"code","7df5084e":"code","9edfd6ba":"code","434df310":"code","a9d55568":"code","3ff865da":"code","47876b97":"code","bf24272e":"code","9c9ba30b":"code","6cf8f134":"markdown","1e7c7252":"markdown","e19d7ea7":"markdown","317aadf3":"markdown","0c6418e5":"markdown","704698d8":"markdown","5cd1ea75":"markdown","b19ad74f":"markdown","a8dce273":"markdown","af57e8e9":"markdown","f7ebfcce":"markdown","99333e66":"markdown","9f73ff84":"markdown","3ef97dbc":"markdown","f050b344":"markdown","87443a86":"markdown","ca7dc3ec":"markdown"},"source":{"8f3ea153":"import plotly.offline as pyo\npyo.init_notebook_mode()\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport cv2 \nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfrom sklearn import preprocessing\nimport random\nimport tensorflow as tf\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n!pip install visualkeras","f165ba2f":"def Create_Directory_DataFrame():\n    df =pd.DataFrame(columns=['Class','Location'])\n    basedir = '..\/input\/blood-cells\/dataset2-master\/dataset2-master\/images\/'\n    for folder in os.listdir(basedir):\n        for Class in os.listdir(basedir+folder+'\/'):\n            for location in os.listdir(basedir+folder+'\/'+Class+'\/'):\n                df = df.append({'Class':Class,'Location':basedir+folder+'\/'+Class+'\/'+location},ignore_index=True)\n    df = df.sample(frac = 1) \n    return df\ndf = Create_Directory_DataFrame()\nprint(df.shape)\ndf.head()","8983f508":"count = 1\nf = plt.figure(figsize=(50,13))\nfor Class in df['Class'].unique():\n    seg = df[df['Class']==Class]\n    address =  seg.sample().iloc[0]['Location']\n    img = cv2.imread(address,0)\n    ax = f.add_subplot(2, 5,count)\n    ax = plt.imshow(img)\n    ax = plt.title(Class,fontsize= 30)\n    count = count + 1\nplt.suptitle(\"Blood Cell Type\", size = 32)\nplt.show()","b8fe355b":"w , h= 64,64\nfinal_class = 4","71d95908":"from tqdm import tqdm\nfrom sklearn.preprocessing import OneHotEncoder\ntrain_image = []\nfor location in tqdm(df.iloc[:]['Location']):\n    img = cv2.imread(location,0)\n    img = cv2.resize(img, (w,h), interpolation = cv2.INTER_AREA)\n    img = img.reshape(w,h,1)\n    train_image.append(img)\nX = np.array(train_image)\ny = np.array(df.iloc[:]['Class'])\ny = y.reshape(y.shape[0],1)\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit(y)\nprint(enc.categories_)\ny = enc.transform(y).toarray()\nprint('Data   :   '+str(X.shape))\nprint('Output :   '+str(y.shape))","0634ddbf":"def sample(ind):\n    print(X[ind].reshape(w,h))\n    print('Output'+str(y[ind]))\n    plt.figure(figsize=(25,8))\n    plt.imshow(X[ind].reshape(w,h))\n    plt.title(enc.inverse_transform(y[0].reshape(1,final_class))[0][0],size = 20)\n    plt.show()\n    \nsample(65)","869d314a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)\nprint('Train data    :'+str(X_train.shape))\nprint('Test data     :'+str(X_test.shape))\nprint('Train Output  :'+str(y_train.shape))\nprint('Test Output   :'+str(y_test.shape))","b2a0c59c":"def conv_block(filters):\n    block = tf.keras.Sequential([\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPool2D()\n    ]\n    )\n    return block\ndef dense_block(units, dropout_rate):\n    block = tf.keras.Sequential([\n        tf.keras.layers.Dense(units, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(dropout_rate)\n    ])\n    return block\ndef build_model(act , final_class , w , h ):\n    model = tf.keras.Sequential([\n        tf.keras.Input(shape=(w , h , 1)),\n        \n        tf.keras.layers.Conv2D(16, 3, activation=act, padding='same'),\n        tf.keras.layers.Conv2D(16, 3, activation=act, padding='same'),\n        tf.keras.layers.MaxPool2D(),\n        \n        conv_block(32),\n        conv_block(64),\n        \n        conv_block(128),\n        tf.keras.layers.Dropout(0.2),\n        \n        conv_block(256),\n        tf.keras.layers.Dropout(0.2),\n        \n        tf.keras.layers.Flatten(),\n        dense_block(512, 0.7),\n        dense_block(128, 0.5),\n        dense_block(64, 0.3),\n        \n        tf.keras.layers.Dense(final_class, activation='sigmoid')\n    ])\n    return model\ndef wrap(Training_Output_Results , Opt , Act ,  history):\n    epoch  = len(history.history['loss'])\n    epochs = list(np.arange(1,epoch + 1,1))\n    Optimizer = np.repeat(Opt,epoch).tolist()\n    Activation = np.repeat(Act,epoch).tolist()\n    cumiliated_res = {}\n    cumiliated_res['Epochs']=epochs\n    cumiliated_res['Optimizer']=Optimizer\n    cumiliated_res['Activation_Function']=Activation\n    cumiliated_res['Train_Loss']=history.history['loss']\n    cumiliated_res['Train_Accuracy']=history.history['accuracy']\n    cumiliated_res['Train_Precision']=history.history['precision']\n    cumiliated_res['Train_Recall']=history.history['recall']\n    cumiliated_res['Val_Loss']=history.history['val_loss']\n    cumiliated_res['Val_Accuracy']=history.history['val_accuracy']\n    cumiliated_res['Val_Precision']=history.history['val_precision']\n    cumiliated_res['Val_Recall']=history.history['val_recall']\n    convertDictionary = pd.DataFrame(cumiliated_res)\n    Training_Output_Results = Training_Output_Results.append(convertDictionary)\n    return Training_Output_Results","0055f157":"Optimisers = ['RMSprop','Adam','Adadelta','Adagrad']","b7434808":"Training_Output_Results =pd.DataFrame(columns=['Epochs','Optimizer','Activation_Function','Train_Loss','Train_Accuracy','Train_Precision','Train_Recall',                                             'Val_Loss','Val_Accuracy','Val_Precision','Val_Recall'])\ndef Optimise_verify(Training_Output_Results):\n    for opt in Optimisers:\n        model = build_model('relu', final_class , w , h)\n        METRICS = [\n                'accuracy',\n                tf.keras.metrics.Precision(name='precision'),\n                tf.keras.metrics.Recall(name='recall'),\n                tf.keras.metrics.Hinge(name='hinge', dtype=None),\n                tf.keras.metrics.Poisson(name='poisson', dtype=None)\n            \n        ]  \n        model.compile(\n                optimizer=opt,\n                loss='categorical_crossentropy',\n                metrics=METRICS\n            )\n        history = model.fit(X_train, y_train, epochs=100, validation_split=0.3, batch_size=15,verbose=1,shuffle=True)\n        Training_Output_Results = wrap(Training_Output_Results , opt,'relu',history)\n        print('---------------------Round for '+opt+' Completed-----------------------------------------')\n    return Training_Output_Results\n    \n    \nTraining_Output_Results = Optimise_verify(Training_Output_Results)","03091eee":"Training_Output_Results=Training_Output_Results.sample(frac = 1) \nprint(Training_Output_Results.shape)\nTraining_Output_Results.to_csv('Optimizer_64*64_data.csv', index = False) \nTraining_Output_Results.tail()","db051a73":"opt = pd.read_csv('.\/Optimizer_64*64_data.csv')","72e260fb":"import plotly.express as px\nimport plotly.io as pio\nimport statsmodels.api as sm\nscatterplot = px.scatter(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Train_Accuracy\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"black\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Train Accuracy',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\npyo.iplot(scatterplot, filename = 'Opt_train_acc')","6e5397ad":"scatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Train_Loss\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Train Loss',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_train_loss')","56f64de3":"scatterplot = px.scatter(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Train_Precision\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Train Precision',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_train_prec')","a529a1da":"scatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Val_Accuracy\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Validation Accuracy',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_val_acc')","ad6cc04e":"scatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Val_Precision\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Validation Precision',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_val_prec')\n","a9d7f039":"scatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Val_Recall\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Validation Recall',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_val_recall')","9088e479":"scatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Val_Loss\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Validation Loss',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_val_loss')","114fbbae":"import plotly.graph_objects as go\ntab_opt = opt[opt['Epochs']==100]\nfinal_col = np.delete(tab_opt.columns[0:], [0,2])\nfig = go.Figure(data=[go.Table(\n    header=dict(values=list(final_col),\n                fill_color='paleturquoise',\n                align='left'),\n    cells=dict(values=[tab_opt.Optimizer , tab_opt.Train_Loss,tab_opt.Train_Accuracy,tab_opt.Train_Precision,tab_opt.Train_Recall,tab_opt.Val_Loss,tab_opt.Val_Accuracy,tab_opt.Val_Precision,tab_opt.Val_Recall],\n               fill_color='lavender',\n               align='left'))\n])\n\nfig.show()","27258748":"import plotly.graph_objects as go\nty =opt[opt['Epochs']==10].iloc[:,3:]\nnm = ty.columns\nty = ty.values.tolist()\ndata = []\n\nfor j in range(len(nm)):\n        lt = []\n        for i in range(len(Optimisers)):\n            lt.append(ty[i][j])\n            \n        data.append(go.Bar(name = nm[j],x=Optimisers, y=lt))\nfig = go.Figure(data=data)\n# Change the bar mode\nfig.update_layout(barmode='group')\nfig.show()","f17fc9b8":"def wrap(Training_Output_Results , lr ,  history):\n    epoch  = len(history.history['loss'])\n    epochs = list(np.arange(1,epoch + 1,1))\n    Optimizer = np.repeat(lr,epoch).tolist()\n    cumiliated_res = {}\n    cumiliated_res['Epochs']=epochs\n    cumiliated_res['Learning Rate']=Optimizer\n    cumiliated_res['Train_Loss']=history.history['loss']\n    cumiliated_res['Train_Accuracy']=history.history['accuracy']\n    cumiliated_res['Train_Precision']=history.history['precision']\n    cumiliated_res['Val_Loss']=history.history['val_loss']\n    cumiliated_res['Val_Accuracy']=history.history['val_accuracy']\n    cumiliated_res['Val_Precision']=history.history['val_precision']\n    convertDictionary = pd.DataFrame(cumiliated_res)\n    Training_Output_Results = Training_Output_Results.append(convertDictionary)\n    return Training_Output_Results","9c95c60d":"Training_Output_Results =pd.DataFrame(columns=['Epochs','Learning Rate','Train_Loss','Train_Accuracy','Train_Precision','Val_Loss','Val_Accuracy','Val_Precision'])\ndef LR_verify(Training_Output_Results):\n        model = build_model('relu', final_class , w , h)\n        METRICS = [\n                'accuracy',\n                tf.keras.metrics.Precision(name='precision')\n        ]  \n        model.compile(\n                optimizer=tf.keras.optimizers.Adam(),\n                loss='categorical_crossentropy',\n                metrics=METRICS\n            )\n        history = model.fit(X_train, y_train, epochs=200, validation_split=0.3, batch_size=15,verbose=1,shuffle=True)\n        Training_Output_Results = wrap(Training_Output_Results , 0.00002548,history)\n        return Training_Output_Results,model,history\n    \n    \nTraining_Output_Results ,model,history= LR_verify(Training_Output_Results)","3bf9c93f":"Training_Output_Results.head()","4060b0a2":"def Plot(history , name , model):\n    model.save(name+'.h5')\n    epochs = range(1,len(history.history['loss']) + 1)\n    epochs = list(epochs)\n    fig = make_subplots(rows=2, cols=3,subplot_titles=(\"Train Loss\", \"Train Accuracy\" , \"Train Precision\",\"Train Recall\", \"Validation Loss\", \"Validation Accuracy\",\n                                                      \"Validation Precision\",\"Validation Recall\"))\n    fig.add_trace(go.Scatter(x=epochs, y=history.history['loss']), row=1, col=1)\n    fig.add_trace(go.Scatter(x=epochs, y=history.history['accuracy']), row=1, col=2)\n    fig.add_trace(go.Scatter(x=epochs, y=history.history['precision']), row=1, col=3)\n    fig.add_trace(go.Scatter(x=epochs, y=history.history['val_loss']), row=2, col=1)\n    fig.add_trace(go.Scatter(x=epochs, y=history.history['val_accuracy']), row=2, col=2)\n    fig.add_trace(go.Scatter(x=epochs, y=history.history['val_precision']), row=2, col=3)\n    fig.update_layout(showlegend=False,height=1000, width=1200, title_text=name)\n    pyo.iplot(fig, filename = 'Act_train_rec')","9c16e62b":"import visualkeras\nvisualkeras.layered_view(model)","bcb55b56":"from keras.utils import plot_model\nplot_model(model, to_file='model.png',show_shapes=True)","e3e226ec":"Plot(history , 'final_model',model)","7b84f6a7":"from keras.models import Model\nfrom matplotlib.pyplot import figure\nfrom numpy import expand_dims\ndef image_transform_gray(image):\n    plt.figure(figsize=(25,8))\n    plt.imshow(image.reshape(w,h))\n    plt.title(enc.inverse_transform(y[0].reshape(1,final_class))[0][0],size = 20)\n    plt.show()\n    img = expand_dims(image, axis=0)\n    model1 = Model(inputs=model.inputs, outputs=model.layers[0].output)\n    feature_maps = model1.predict(img)\n    figure(num=None, figsize=(25, 30), dpi=80, facecolor='w', edgecolor='k')\n    square = 4\n    ix = 1\n    for _ in range(square):\n        for _ in range(square):\n            # specify subplot and turn of axis\n            ax = pyplot.subplot(square, square, ix)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            # plot filter channel in grayscale\n            pyplot.imshow(feature_maps[0, :, :, ix-1],cmap='gray')\n            ix += 1\n    # show the figure\n    pyplot.show()","3fbe1eb0":"image_transform_gray(X[89])","2d524bf2":"import plotly.graph_objects as go\nfrom sklearn.metrics import classification_report\ndef binary_classify(y_pred):\n    for inp in y_pred:\n        maximum = 0\n        index = 0\n        for i in range(final_class):\n            if(maximum != max(maximum,inp[i])):\n                maximum = max(maximum,inp[i])\n                index = i\n            inp[i] = 0\n        inp[index]=1\n    return y_pred\ny_prediction  = binary_classify(y_prediction)\ndef create_result(y):\n    y_final = []\n    for i in range(y.shape[0]):\n        y_final.append(enc.inverse_transform(y[i].reshape(1,final_class))[0][0])\n    return y_final \ndef remove_none(y , y_pred):\n    index = []\n    for i in range(len(y)-1,0,-1):\n        if y_pred[i] == None :\n            del y[i]\n            del y_pred[i]\n        \n    return y , y_pred\ndef label_encode(y , y_pred):\n    le = preprocessing.LabelEncoder()\n    le.fit(y_pred)\n    print('Classes   :    '+str(le.classes_))\n    y = le.transform(y)\n    y_pred = le.transform(y_pred)\n    return y , y_pred\ndef Test_Results_compiled(model,history, name =''):\n    print('Results '+name)\n    y_pred = model.evaluate(X_test , y_test,verbose =1)\n    index = len(history.history['loss']) - 1\n    fig = go.Figure(data=[\n        go.Bar(name = 'Accuracy',x=['Training','Validation','Real World Data'], y=[history.history['accuracy'][index] ,history.history['val_accuracy'][index],y_pred[1] ]),\n        go.Bar(name = 'Precision',x=['Training','Validation','Real World Data'], y=[history.history['precision'][index] ,history.history['val_precision'][index],y_pred[2] ]),\n        go.Bar(name = 'Loss',x=['Training','Validation','Real World Data'], y=[history.history['loss'][index] ,history.history['val_loss'][index],y_pred[0] ]),\n\n    ])\n    fig.update_layout(barmode='group')\n    fig.update_yaxes(type = \"log\")\n    pyo.iplot(fig, filename = 'Act_train_rec')\n    y_prediction = model.predict(X_test)\n    y_class_result = create_result(y_prediction)\n    y_class_desired = create_result(y_test)\n    y_label_desired , y_label_result = label_encode(y_class_desired , y_class_result) \n    tn = []\n    for cat in enc.categories_[0].reshape(final_class,1):\n        tn.append(cat[0])\n    target_names = tn\n    print(classification_report(y_label_desired, y_label_result, target_names=target_names))\n    count = 1\n    f = plt.figure(figsize=(20,24))\n    for i in range(20):\n        ind = random.sample(list(y_label_result),1)[0]\n        img = X_test[ind]\n        Class = str(y_class_desired[ind]) + '  vs  '+str(y_class_result[ind])\n        ax = f.add_subplot(5, 4,count)\n        ax = plt.imshow(img.reshape(w,h))\n        ax = plt.title(Class,fontsize= 11)\n        count = count + 1\n    plt.suptitle(\"Blood Samples\", size = 32)\n    plt.show()","192d71f1":"Test_Results_compiled(model,history,'CNN Based Model')","7df5084e":"from keras.applications.vgg16 import VGG16\nmodel = VGG16()\nvisualkeras.layered_view(model)","9edfd6ba":"Training_Output_Results =pd.DataFrame(columns=['Epochs','Learning Rate','Train_Loss','Train_Accuracy','Train_Precision','Val_Loss','Val_Accuracy','Val_Precision'])\n\nmodel = build_model('relu', final_class , w , h)\nMETRICS = [\n            'accuracy',\n            tf.keras.metrics.Precision(name='precision')\n]  \nmodel.compile(\n                optimizer=tf.keras.optimizers.Adam(),\n                loss='categorical_crossentropy',\n                metrics=METRICS\n            )\nhistory = model.fit(X_train, y_train, epochs=200, validation_split=0.3, batch_size=15,verbose=1,shuffle=True)","434df310":"Plot(history , 'vgg16',model)","a9d55568":"Test_Results_compiled(model,history,'CNN Based Model')","3ff865da":"from keras.applications.inception_v3 import InceptionV3\n# load model\nmodel = InceptionV3()\nvisualkeras.layered_view(model)","47876b97":"Training_Output_Results =pd.DataFrame(columns=['Epochs','Learning Rate','Train_Loss','Train_Accuracy','Train_Precision','Val_Loss','Val_Accuracy','Val_Precision'])\n\nmodel = build_model('relu', final_class , w , h)\nMETRICS = [\n            'accuracy',\n            tf.keras.metrics.Precision(name='precision')\n]  \nmodel.compile(\n                optimizer=tf.keras.optimizers.Adam(),\n                loss='categorical_crossentropy',\n                metrics=METRICS\n            )\nhistory = model.fit(X_train, y_train, epochs=200, validation_split=0.3, batch_size=15,verbose=1,shuffle=True)","bf24272e":"Plot(history , 'final_model',model)","9c9ba30b":"Test_Results_compiled(model,history,'Inception Based Model')","6cf8f134":"Hi folks, Lets just briefly see what we intend to achieve here\n1. Load the data to a dataframe( class : image )\n2. Load all the images in a numpy stacked array\n3. Develop a good accuracy , precision and recall using deep learning\n    i We need to choose a good optimizer\n    ii We need to decide upon the learning rate \n    iii Batch size choice\n    iv Number of epochs\n4. Finally after developing the final model we want to understand the layer wise feature detection for the same\n5. Its report on training validation and test data\n6. Finally few glimse on what the output is\n7. Transfer learning Models VGG16 and Inception\n\nIf you find the code helpful and dedactive do upvote and comment for any queries.","1e7c7252":"# Identify Blood Cell Subtypes From Images\nBasophil vs Eosinophil vs Lymphocyte vs Monocyte vs Neutrophil\nMononuclear (Basophil + Lymphocyte vs Monocyte) vs Polynuclear (Neutrophil + Eosinophil)\nAn important problem in blood diagnostics is classifying different types of blood cells. Here we have 410 original images and 12,500 augmented images of blood cells paired with subtype labels (Basophil vs Eosinophil vs Lymphocyte vs Monocyte vs Neutrophil). We want to automatically classify each image according to the subtype of cells within it.\n\nFor more information about blood cells and blood cell subtypes, see the following links: https:\/\/www.ncbi.nlm.nih.gov\/books\/NBK2263\/ and https:\/\/www.ncbi.nlm.nih.gov\/books\/NBK2263\/box\/A26\/?report=objectonly","e19d7ea7":"Note : The research for not going deep into activation function is rather imensely elaborated in the [this paper](https:\/\/www.kaggle.com\/tathagatbanerjee\/hand-recognition-cnn-model-plot-feature-plot)","317aadf3":"# Final Model","0c6418e5":"## Choice of Optimizer to minimize the loss","704698d8":"# Segmentation in Traing and Test Data Sets\n1. Training : 80% \n2. Test : 20%\n3. Validation : 30%","5cd1ea75":"# Test The Results","b19ad74f":"# Model\nTo make our model more modular and easier to understand, let's define some blocks. As we're building a convolution neural network, we'll create a convolution block and a dense layer block.\nThe following method will define the function to build our model for us. The Dropout layers are important as they \"drop out,\" hence the name, certain nodes to reduce the likelikhood of the model overfitting. We want to end the model with a Dense layer of one node\nFor our metrics, we want to include precision and recall as they will provide use with a more informed picture of how good our model is. Accuracy tells us what fractions are the labels are correct. Since our data is not balanced, accuracy might give a skewed sense of a good model (i.e. a model that always predicts PNEUMONIA will be 74% accurate but is not a good model).\n\nPrecision is the number of true positives (TP) over the sum of TP and false positives (FP). It shows what fraction of labeled positives are actually correct.\n\nRecall is the number of TP over the sum of TP and false negatves (FN). It shows what fraction of actual positives are correct.","a8dce273":"# Defining Constraints","af57e8e9":"## Ploting","f7ebfcce":"Finally here we can conclude \n1. Training data : Adam and RMSprop has performed exceptionally well than the other two\n2. Validation data : Adam have proved much rohbust than RMSprop","99333e66":"# Samples","9f73ff84":"# Create A Stacked Numpy Array\nI Brief Idea for Image Loading\n1. Read Images \n2. Resize them\n3. Add to a list\n4. Convert the list to np array\n\nII Output array\n1. Read the classes\n2. reshape to 1 D array\n3. One hot encode the same\n","3ef97dbc":"## Optimizer Selection test","f050b344":"# Lets Create a dataframe with class and location to it","87443a86":"# Visualize Data","ca7dc3ec":"# Transfer Learning"}}