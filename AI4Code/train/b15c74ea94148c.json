{"cell_type":{"8f89bc3f":"code","d95cf48b":"code","4e4dcba5":"code","253d7940":"code","47e47f95":"code","9f5b1b64":"code","cc36b50f":"code","469e88b8":"code","34c43f4e":"code","d275a5c5":"code","a64e0f1c":"code","b787b2d5":"code","45076600":"code","e035dff1":"code","e065c3d4":"code","3781744b":"code","48380930":"code","626fcd28":"code","0f92ca74":"code","595e49e1":"code","2eff611d":"code","8fc18d04":"code","f7b1b33f":"code","f137a97e":"code","b8635818":"code","96a1987f":"code","52ce1c0d":"code","3c619417":"code","53797027":"code","0150c23c":"code","edfc6589":"code","c8390e72":"code","81b7e29f":"code","65850ca7":"code","9a138fbd":"code","cc5a93df":"code","68be18dc":"markdown","7560e3a2":"markdown","1384e009":"markdown","e04c8629":"markdown","3c43d8e0":"markdown","59e0b502":"markdown","dddc9d9f":"markdown","3419cb68":"markdown","d8fb15bb":"markdown","dbb17473":"markdown","040f7147":"markdown","4b7ed682":"markdown","5680027e":"markdown","1cd825ef":"markdown","8c638231":"markdown","7b19412a":"markdown","93ca29ca":"markdown","c3cee1fb":"markdown","ca7d2a1e":"markdown","1b836888":"markdown"},"source":{"8f89bc3f":"# Some basic stuff for EDA:\n\nimport pandas as pd\nimport numpy as np\n\n# for visualizing\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport plotly.express as px\n\n# for adding extra statistical stuff\n\nfrom scipy.stats import skew, norm","d95cf48b":"# Styling graphs with customized color palette.\n\ncust_palt = [\n    '#111d5e','#c70039','#37b448','#B43757', '#ffbd69', '#ffc93c','#FFFF33','#FFFACD',\n]\n\nplt.style.use('ggplot')","4e4dcba5":"# Train, test, targets and submission file:\n\ntrain_feat = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_target = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\n\ntest_feat = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_sub = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","253d7940":"print('Train Feature Samples:')\ndisplay(train_feat.sample(3))\nprint('Test Feature Samples:')\ndisplay(test_feat.sample(3))\nprint('Train Target Samples:')\ndisplay(train_target.sample(3))","47e47f95":"# Checking train and test columns\/rows.\n\nprint(\n    f'Train data has {train_feat.shape[1]} features, {train_feat.shape[0]} observations. {train_feat.sig_id.nunique() } of these are unique.\\nTest data {test_feat.shape[1]} features, {test_feat.shape[0]} observations. {test_feat.sig_id.nunique() } of these are unique.'\n)\n\n","9f5b1b64":"# Checking missing values.\n\ntrain_miss=train_feat.isnull().sum().sum()\ntest_miss=train_feat.isnull().sum().sum()\n\nif train_miss&test_miss == 0:\n    print('There are no missing values in both datasets!')\nelse:\n    print('There are missing values you should check them individually!')","cc36b50f":"# Checking categorical features\nprint(f'Categorical features on dataset are:\\n {train_feat.loc[:,train_feat.nunique()<=10].columns.tolist()}')","469e88b8":"# Displaying categorical distribution:\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 12))\n\n\ngrid = gridspec.GridSpec(ncols=6, nrows=3, figure=fig)\n\nax1 = fig.add_subplot(grid[0, :3])\n\nax1.set_title(f'Train cp_type Distribution',weight='bold')\n\nsns.countplot(x='cp_type',\n                    data=train_feat,\n                    palette=cust_palt,\n                    ax=ax1,\n                    order=train_feat['cp_type'].value_counts().index)\n\ntotal = float(len(train_feat['cp_type']))\n\n\nfor p in ax1.patches:\n    height = p.get_height()\n    ax1.text(p.get_x() + p.get_width() \/ 2.,\n            height + 2,\n            '{:1.2f}%'.format((height \/ total) * 100),\n            ha='center')\n\n\nax2 = fig.add_subplot(grid[0, 3:])\n\n\n\nsns.countplot(x='cp_type',\n                    data=test_feat,\n                    palette=cust_palt,\n                    ax=ax2,\n                    order=test_feat['cp_type'].value_counts().index)\n\ntotal = float(len(test_feat['cp_type']))\n\nax2.set_title(f'Test cp_type Distribution', weight='bold')\n\n\nfor p in ax2.patches:\n    height = p.get_height()\n    ax2.text(p.get_x() + p.get_width() \/ 2.,\n            height + 2,\n            '{:1.2f}%'.format((height \/ total) * 100),\n            ha='center')\nax3 = fig.add_subplot(grid[1, :3])\n\nax3.set_title(f'Train cp_time Distribution', weight='bold')\n\nsns.countplot(x='cp_time',\n                    data=train_feat,\n                    palette=cust_palt,\n                    ax=ax3,\n                    order=train_feat['cp_time'].value_counts().index)\n\ntotal = float(len(train_feat['cp_time']))\n\n\nfor p in ax3.patches:\n    height = p.get_height()\n    ax3.text(p.get_x() + p.get_width() \/ 2.,\n            height + 2,\n            '{:1.2f}%'.format((height \/ total) * 100),\n            ha='center')\n\nax4 = fig.add_subplot(grid[1, 3:])\n\nax4.set_title(f'Test cp_time Distribution', weight='bold')\n\nsns.countplot(x='cp_time',\n                    data=test_feat,\n                    palette=cust_palt,\n                    ax=ax4,\n                    order=train_feat['cp_time'].value_counts().index)\n\ntotal = float(len(test_feat['cp_time']))\n\n\nfor p in ax4.patches:\n    height = p.get_height()\n    ax4.text(p.get_x() + p.get_width() \/ 2.,\n            height + 2,\n            '{:1.2f}%'.format((height \/ total) * 100),\n            ha='center')\n    \nax5 = fig.add_subplot(grid[2, :3])\n\nax5.set_title(f'Train cp_dose Distribution', weight='bold')\n\nsns.countplot(x='cp_dose',\n                    data=train_feat,\n                    palette=cust_palt,\n                    ax=ax5,\n                    order=train_feat['cp_dose'].value_counts().index)\n\ntotal = float(len(train_feat['cp_dose']))\n\n\nfor p in ax5.patches:\n    height = p.get_height()\n    ax5.text(p.get_x() + p.get_width() \/ 2.,\n            height + 2,\n            '{:1.2f}%'.format((height \/ total) * 100),\n            ha='center')\n\nax6 = fig.add_subplot(grid[2, 3:])\n\nax6.set_title(f'Test cp_dose Distribution', weight='bold')\n\nsns.countplot(x='cp_dose',\n                    data=test_feat,\n                    palette=cust_palt,\n                    ax=ax6,\n                    order=train_feat['cp_dose'].value_counts().index)\n\ntotal = float(len(test_feat['cp_dose']))\n\n\nfor p in ax6.patches:\n    height = p.get_height()\n    ax6.text(p.get_x() + p.get_width() \/ 2.,\n            height + 2,\n            '{:1.2f}%'.format((height \/ total) * 100),\n            ha='center')","34c43f4e":"# Label encoding categorical data\n\ntrain_feat['cp_type'] = train_feat['cp_type'].map({'trt_cp':0,'ctl_vehicle':1})\ntrain_feat['cp_time'] = train_feat['cp_time'].map({24:0,48:1,72:2})\ntrain_feat['cp_dose'] = train_feat['cp_dose'].map({'D1':0,'D2':1})\n\ntest_feat['cp_type'] = test_feat['cp_type'].map({'trt_cp':0,'ctl_vehicle':1})\ntest_feat['cp_time'] = test_feat['cp_time'].map({24:0,48:1,72:2})\ntest_feat['cp_dose'] = test_feat['cp_dose'].map({'D1':0,'D2':1})","d275a5c5":"# Counting target values.\n\ntarg_cts=train_target.iloc[:,1:].sum(axis=0)\nfig = plt.figure(figsize=(20,15))\nsns.barplot(y=targ_cts.sort_values(ascending=False)[:30].index, x=targ_cts.sort_values(ascending=False)[:30].values, palette='inferno')\nplt.show()","a64e0f1c":"# Labels per sample.\n\nplt.figure(figsize=(16,6))\nfeatures = train_target.columns.values[1:]\nplt.title('Total Target Score Counts', weight='bold')\nsns.countplot(train_target[features].sum(axis=1), palette=cust_palt)\nplt.xlabel('Total Number of Targets per Sample')\nplt.legend()\nplt.show()","b787b2d5":"# Displaying meta distribution:\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 12))\n\nfeatures = train_feat.columns.values[1:]\n\ngrid = gridspec.GridSpec(ncols=4, nrows=4, figure=fig)\n\nax1 = fig.add_subplot(grid[0, :2])\n\nax1.set_title('Distribution of Mean Values per Column', weight='bold')\n\nsns.kdeplot(train_feat[features].mean(axis=0),color=cust_palt[0], shade=True, label='Train')\nsns.kdeplot(test_feat[features].mean(axis=0),color=cust_palt[1], shade=True, label='Test')\n\n\nax2 = fig.add_subplot(grid[0, 2:])\n\nax2.set_title('Distribution of Median Values per Column', weight='bold')\n\nsns.kdeplot(train_feat[features].median(axis=0),color=cust_palt[0], shade=True, label='Train')\nsns.kdeplot(test_feat[features].median(axis=0),color=cust_palt[1], shade=True, label='Test')\n\nax3 = fig.add_subplot(grid[1, :2])\n\nax3.set_title('Distribution of Minimum Values per Column', weight='bold')\n\nsns.kdeplot(train_feat[features].min(axis=0),color=cust_palt[0], shade=True, label='Train')\nsns.kdeplot(test_feat[features].min(axis=0),color=cust_palt[1], shade=True, label='Test')\n\n\nax4 = fig.add_subplot(grid[1, 2:])\n\nax4.set_title('Distribution of Maximum Values per Column', weight='bold')\n\nsns.kdeplot(train_feat[features].max(axis=0),color=cust_palt[0], shade=True, label='Train')\nsns.kdeplot(test_feat[features].max(axis=0),color=cust_palt[1], shade=True, label='Test')\n\n\nax5 = fig.add_subplot(grid[2, :2])\n\nax5.set_title('Distribution of Std\\'s per Column', weight='bold')\n\nsns.kdeplot(train_feat[features].std(axis=0),color=cust_palt[0], shade=True, label='Train')\nsns.kdeplot(test_feat[features].std(axis=0),color=cust_palt[1], shade=True, label='Test')\n\nax6 = fig.add_subplot(grid[2, 2:])\n\nax6.set_title('Distribution of Variances per Column', weight='bold')\n\nsns.kdeplot(train_feat[features].var(axis=0),color=cust_palt[0], shade=True, label='Train')\nsns.kdeplot(test_feat[features].var(axis=0),color=cust_palt[1], shade=True, label='Test')\n\nax7 = fig.add_subplot(grid[3, :2])\n\nax7.set_title('Distribution of Skew Values per Column', weight='bold')\n\nsns.kdeplot(train_feat[features].skew(axis=0),color=cust_palt[0], shade=True, label='Train')\nsns.kdeplot(test_feat[features].skew(axis=0),color=cust_palt[1], shade=True, label='Test')\n\nax8 = fig.add_subplot(grid[3, 2:])\n\nax8.set_title('Distribution of Kurtosis Values per Column', weight='bold')\n\nsns.kdeplot(train_feat[features].kurtosis(axis=0),color=cust_palt[0], shade=True, label='Train')\nsns.kdeplot(test_feat[features].kurtosis(axis=0),color=cust_palt[1], shade=True, label='Test')\n\nplt.suptitle('Meta Distributions of Train\/Test Set', fontsize=25, weight='bold')\n\nplt.show()","45076600":"# Listing skew and high standard deviation features:\n\nfeatures_std = train_feat.iloc[:,1:].apply(lambda x: x.std()).sort_values(\n    ascending=False)\nf_std = train_feat[features_std.iloc[:20].index.tolist()]\n\nfeatures_skew = np.abs(train_feat.iloc[:,1:].apply(lambda x: skew(x)).sort_values(\n    ascending=False))\nskewed = train_feat[features_skew.iloc[:20].index.tolist()]","e035dff1":"def feat_dist(df, df2, cols, rows=3, columns=3, title=None):\n    \n    '''A function for displaying skew feat distribution'''\n    \n    fig, axes = plt.subplots(rows, columns, figsize=(30, 25), constrained_layout=True)\n    axes = axes.flatten()\n\n    for i, j in zip(cols, axes):\n        sns.distplot(\n                    df[i],\n                    ax=j,\n                    fit=norm,\n                    hist=False,\n                    color='#111d5e',\n                    label=f'Train {i}',\n                    kde_kws={'alpha':0.9})        \n        \n        sns.distplot(\n                    df2[i],\n                    ax=j,\n                    hist=False,\n                    color = '#c70039',\n                    label=f'Test {i}',\n                    kde_kws={'alpha':0.7})\n        \n        (mu, sigma) = norm.fit(df[i])\n        j.set_title('Train Test Dist of {0} Norm Fit: $\\mu=${1:.2g}, $\\sigma=${2:.2f}'.format(i.capitalize(), mu, sigma), weight='bold')\n        fig.suptitle(f'{title}', fontsize=24, weight='bold')","e065c3d4":"# Creating distplot of features which has high std\n\nfeat_dist(train_feat, test_feat, f_std.columns.tolist(), rows=5, columns=4, title='Distribution of High Std Train\/Test Features')","3781744b":"# Creating distplot of features which highly skewed\n\nfeat_dist(train_feat, test_feat, skewed.columns.tolist(), rows=5, columns=4, title='Distribution of Highly Skewed Train\/Test Features')","48380930":"def tail_dist(df, df2, cols, rows=3, columns=3, title=None):\n    \n    '''A function for displaying skew feat distribution'''\n    \n    fig, axes = plt.subplots(rows, columns, figsize=(30, 25), constrained_layout=True)\n    axes = axes.flatten()\n\n    for i, j in zip(cols, axes):\n        sns.distplot(\n                    df[i],\n                    ax=j,                    \n                    hist=False,\n                    color='#111d5e',\n                    label=f'Train {i}',\n                    kde_kws={'alpha':0.9})        \n        \n        sns.distplot(\n                    df2[i],\n                    ax=j,\n                    hist=False,\n                    color = '#c70039',\n                    label=f'Test {i}',\n                    kde_kws={'alpha':0.7})        \n\n        j.set_title(f'Train Test Dist of {i.capitalize()}', weight='bold')\n        if cols == f_std.columns.tolist():\n            j.axis([-11,-0.5,0,0.1])\n        else:\n            j.axis([1,11,0,0.1])\n        fig.suptitle(f'{title}', fontsize=24, weight='bold')","626fcd28":"# Creating distplot of features which has high std \/ tail part\n\ntail_dist(train_feat, test_feat, f_std.columns.tolist(), rows=5, columns=4, title='Distribution of High Std Train\/Test Feature Tails')","0f92ca74":"# Creating distplot of features which highly skewed \/ tail part\n\ntail_dist(train_feat, test_feat, skewed.columns.tolist(), rows=5, columns=4, title='Distribution of Highly Skewed Train\/Test Feature Tails')","595e49e1":"correlations = train_feat.iloc[:,1:].corr().abs().unstack().sort_values(kind=\"quicksort\",ascending=False).reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']] #preventing 1.0 corr\ncorr_max=correlations.level_0.head(150).tolist()\ncorr_max=list(set(corr_max)) #removing duplicates\n\ncorr_min=correlations.level_0.tail(34).tolist()\ncorr_min=list(set(corr_min)) #removing duplicates","2eff611d":"# top corrs\ndisplay(correlations.head(5))","8fc18d04":"display(correlations.tail(5))","f7b1b33f":"correlation_train = train_feat.loc[:,corr_max].corr()\nmask = np.triu(correlation_train.corr())\n\nplt.figure(figsize=(30, 12))\nsns.heatmap(correlation_train,\n            mask=mask,\n            annot=True,\n            fmt='.3f',\n            cmap='Wistia',\n            linewidths=0.01,\n            cbar=True)\n\n\nplt.title('Features with Highest Correlations',  weight='bold')\nplt.show()","f137a97e":"correlation_train = train_feat.loc[:,corr_min].corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(30, 12))\n\nsns.heatmap(correlation_train,\n            mask=mask,\n            annot=True,\n            fmt='.3f',\n            cmap='Wistia',\n            linewidths=0.01,\n            cbar=True)\n\n\nplt.title('Features with Lowest Correlations',  weight='bold')\nplt.show()","b8635818":"from sklearn.decomposition import PCA\npca = PCA()\npca.fit(train_feat.iloc[:,1:])\npca_train = pca.transform(train_feat.iloc[:,1:])\npca_test = pca.transform(test_feat.iloc[:,1:])","96a1987f":"# Explaining variance ratio:\n\nfig, ax = plt.subplots(2,1,figsize=(28, 10))\nax[0].plot(range(train_feat.iloc[:,1:].shape[1]), pca.explained_variance_ratio_.cumsum(), linestyle='--', drawstyle='steps-mid', color=cust_palt[1],\n         label='Cumulative Explained Variance')\nsns.barplot(np.arange(1,train_feat.iloc[:,1:].shape[1]+1), pca.explained_variance_ratio_, alpha=0.85, color=cust_palt[0],\n            label='Individual Explained Variance', ax=ax[0])\n\n\nax[0].set_title('Explained Variance', fontsize = 20, weight='bold')\nax[0].set_ylabel('Explained Variance Ratio', fontsize = 14)\nax[0].set_xlabel('Number of Principal Components', fontsize = 14)\nplt.legend(loc='center right', fontsize = 13);\n\nax[0].set_xticks([])\n\nax[1].plot(range(train_feat.iloc[:,1:].shape[1]), pca.explained_variance_ratio_.cumsum(), linestyle='--', drawstyle='steps-mid', color=cust_palt[1],\n         label='Cumulative Explained Variance')\nsns.barplot(np.arange(1,train_feat.iloc[:,1:].shape[1]+1), pca.explained_variance_ratio_, alpha=0.85, color=cust_palt[0],\n            label='Individual Explained Variance', ax=ax[1])\n\nax[1].axis([0,29,0,1])\nax[1].set_title('First 30 Explained Variances', fontsize = 20, weight='bold')\nax[1].set_ylabel('Explained Variance Ratio', fontsize = 14)\nax[1].set_xlabel('Number of Principal Components', fontsize = 14)\nplt.tight_layout()\n","52ce1c0d":"train_temp = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\npca = PCA(4)\npca.fit(train_feat.iloc[:,1:])\npca_samples = pca.transform(train_feat.iloc[:,1:])","3c619417":"# Displaying 50% of the variance:\n\ntotal_var = pca.explained_variance_ratio_.sum() * 100\n\nlabels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n}\n\n\nfig = px.scatter_matrix(\n    pca_samples,\n    color=train_temp.iloc[:,1:].cp_type,\n    dimensions=range(4),\n    labels=labels,\n    title=f'Total Explained Variance: {total_var:.2f}% vs cp_type',\n    opacity=0.5,\n    color_discrete_sequence=cust_palt[:4],\n)\nfig.update_traces(diagonal_visible=False)\nfig.show()","53797027":"# Displaying 50% of the variance:\n\ntotal_var = pca.explained_variance_ratio_.sum() * 100\n\nlabels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n}\n\nfig = px.scatter_matrix(\n    pca_samples,\n    color=train_temp.iloc[:,1:].cp_dose,\n    dimensions=range(4),\n    labels=labels,\n    title=f'Total Explained Variance: {total_var:.2f}% vs cp_dose',\n    opacity=0.5,\n    color_discrete_sequence=cust_palt[:4]\n)\nfig.update_traces(diagonal_visible=False)\nfig.show()","0150c23c":"train_temp['number_of_moas'] = train_target[list(train_target.columns[1:])].sum(axis=1)\ntrain_temp['number_of_moas'] = train_temp['number_of_moas'].map({0:'No MoA',1:'One MoA',2:'Multiple MoAs', 3:'Multiple MoAs', 4:'Multiple MoAs', 5:'Multiple MoAs', 6:'Multiple MoAs'\n                                                                , 7:'Multiple MoAs'})\ntrain_temp['cp_time'] = train_temp['cp_time'].astype('str')","edfc6589":"# Displaying 50% of the variance:\n\ntotal_var = pca.explained_variance_ratio_.sum() * 100\n\nlabels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n}\n\n\nfig = px.scatter_matrix(\n    pca_samples,\n    color=train_temp.iloc[:,1:].cp_time,\n    dimensions=range(4),\n    labels=labels,\n    title=f'Total Explained Variance: {total_var:.2f}% vs cp_time',\n    opacity=0.3,\n    color_discrete_sequence=cust_palt[:3],\n)\n\nfig.update_traces(diagonal_visible=False)\nfig.show()","c8390e72":"# Displaying 50% of the variance:\n\ntotal_var = pca.explained_variance_ratio_.sum() * 100\n\nlabels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n}\n\n\nfig = px.scatter_matrix(\n    pca_samples,\n    color=train_temp['number_of_moas'],\n    dimensions=range(4),\n    labels=labels,\n    title=f'Total Explained Variance: {total_var:.2f}% vs number of MoA\\'s',\n    opacity=0.3,\n    color_discrete_sequence=cust_palt[:3],\n)\n\nfig.update_traces(diagonal_visible=False)\nfig.show()","81b7e29f":"# duplicating train test features\n\ntrain_adv = train_feat.iloc[:,1:].copy()\ntest_adv = test_feat.iloc[:,1:].copy()\n\n#labelling train test data\n\ntrain_adv.insert(loc=875, value=0, column='dataset_label')\ntest_adv.insert(loc=875, value=1, column='dataset_label')\n\n# merging train test data\nadv_master = pd.concat([train_adv, test_adv], axis=0)\nadv_master.reset_index(inplace=True)\nadv_master.drop('index', axis=1, inplace=True)\n\n# creating x and y's for adversarial validation\nadv_X = adv_master.drop('dataset_label', axis=1)\nadv_y = adv_master['dataset_label']","65850ca7":"# loading some basic packages for testing\n\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nimport math\nfrom sklearn.metrics import plot_roc_curve, auc, roc_auc_score\n\n\n# setting 3 fold cv\n\ncv = StratifiedKFold(3, shuffle=True, random_state=42)\n\n\n# models:\n\nxg_adv = LogisticRegression(\n    random_state=42,\n    n_jobs=-1,\n)\n\nlg_adv = lgb.LGBMClassifier(\n    random_state=42,\n    n_jobs=-1,\n)\n\nestimators = [xg_adv, lg_adv]","9a138fbd":"def adv_roc(estimators, cv, X, y):\n    \n    ''' A function for plotting roc '''\n\n    fig, axes = plt.subplots(math.ceil(len(estimators) \/ 2),\n                             2,\n                             figsize=(16, 6))\n    axes = axes.flatten()\n\n    for ax, estimator in zip(axes, estimators):\n        tprs = []\n        aucs = []\n        mean_fpr = np.linspace(0, 1, 100)\n\n        for i, (train, test) in enumerate(cv.split(X, y)):\n            estimator.fit(X.loc[train], y.loc[train])\n            viz = plot_roc_curve(estimator,\n                                 X.loc[test],\n                                 y.loc[test],\n                                 name='ROC fold {}'.format(i),\n                                 alpha=0.3,\n                                 lw=1,\n                                 ax=ax)\n            interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n            interp_tpr[0] = 0.0\n            tprs.append(interp_tpr)\n            aucs.append(viz.roc_auc)\n\n        ax.plot([0, 1], [0, 1],\n                linestyle='--',\n                lw=2,\n                color='r',\n                label='Chance',\n                alpha=.8)\n\n        mean_tpr = np.mean(tprs, axis=0)\n        mean_tpr[-1] = 1.0\n        mean_auc = auc(mean_fpr, mean_tpr)\n        std_auc = np.std(aucs)\n        ax.plot(mean_fpr,\n                mean_tpr,\n                color='b',\n                label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' %\n                (mean_auc, std_auc),\n                lw=2,\n                alpha=.8)\n\n        std_tpr = np.std(tprs, axis=0)\n        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n        ax.fill_between(mean_fpr,\n                        tprs_lower,\n                        tprs_upper,\n                        color='grey',\n                        alpha=.2,\n                        label=r'$\\pm$ 1 std. dev.')\n\n        ax.set(xlim=[-0.02, 1.02],\n               ylim=[-0.02, 1.02],\n               title=f'{estimator.__class__.__name__} ROC for Adversarial Val.')\n        ax.legend(loc='lower right', prop={'size': 10})\n    plt.show()","cc5a93df":"adv_roc(estimators, cv, adv_X, adv_y)","68be18dc":"## Quality of the Data, Unique Observations and Categorical Features\n\n### Here we take a look general quality of the data we given. We going to find answers for questions like if we have any missing data, how many features and observations we have, is there any categorical variables, if so which ones etc...","7560e3a2":"# Tails of the Interesting Features\n\nHere we can see where the train test outlier differences lies. Even though they are small partition of the data they can have decent impact on our model performances. Especiall on G-229 we can see huge gap between train and test set around -10, these are wort to dig deeper in future...","1384e009":"# Work in Progress!\n\n### There are loads of missing comments and some graphs I want to visualize later and maybe a simple model, but it's mainly EDA work for now, I'll be updating them whenever it's possible. Thanks and I hope you enjoyed while reading it. Happy coding!","e04c8629":"# Meta Feature Distribution\n\n### Here I wanted to take a look at our some statistical values for our train test data, these are meta values but can give us insights for next steps we going to take. We can detect some meta differences between train and test data.\n\n- There are small differences between distribution of mean values but generally they look balanced. We couldn't say same for the median.\n- The min and max values echoes nicely between them but we see some differences between train and test data.\n- The std and var for both datasets looks nice and balanced.\n- By looking at their skewness we could say they both datasets are similar. But generally their distribution looks worth to take a deeper look.\n- Kurtosis indicates test samples having little longer tails, that's interesting and worth to take a look again...","3c43d8e0":"## Results\n\nWe just implemented two basic classifiers to predict our train test sets: LightGBM and LogisticRegression. When we check our results we can see:\n\n- Both models having hard time while distinguish if data comes from train set or test set.\n- With the score of almost 0.50 we can say train and test datasets randomly selected and they are balanced. Which is good!\n\nIf we got higher scores we would have to inspect what features causing it and then try to eliminate that effect for more regularized results...","59e0b502":"## Loading Data","dddc9d9f":"# Distribution of High Std Features","3419cb68":"# Adversarial Validation\n\nAlright, since we testing for train test sampling differences in previous parts I also wanted wanted to implement what is called 'Adversarial Validation'. This method helped me in previous competition while building my model. Basically we going to replace our targets for both datasets (0 for train and 1 for test), then we going build a classifier which tries to predict which observation belongs to train and which one belongs to test set. If datasets randomly selected from similar roots it should be really hard for the classifier to separate them. But if there is systematic selection differences between train and test sets then classifier should be able to capture this trend. So we want our models score lower for the next section (0.50 AUC) because higher detection rate means higher difference between train and test datasets, so let's get started...\n","d8fb15bb":"# Distribution of High Skew Features","dbb17473":"# Dimension Reduction\n\n### We have quite high number of features and since getting good model is iterative process you might want to reduce your dimensions and get faster results for producing your first working baseline models. If you inspect the visuals below you'll see first component explains 40% of the variance and first 30 components explains around 70% of the variance. With these reduced dimensions you might get your first steps of modelling faster, which I found quite useful","040f7147":"# First Look and Overview of the Data\n\n### We have several datasets like: Train features and train targets for training our model, test features for predicting and submission sample for sending our predictions.","4b7ed682":"# Distribution of Some Features\n\n### There are high number of features and we don't need to visualize all of them for now. Instead of choosing features randomly I wanted to go with some *irregular* variables such as highly skewed or high std features. I also fitted normal distribution line so we can see how are our samples are differs from gaussian distribution. We can see that our irregular features are quite similar around central parts with small differences but we can see some differences visible on tails. Next we will check them.","5680027e":"### We can observe that some of the groups diversible easily in components wise, especially control groups are can be detected easily.","1cd825ef":"# Introduction\n\nIn this work I'm going to do some exploratory data analysis for recent [Mechanisms of Action (MoA) Prediction](https:\/\/www.kaggle.com\/c\/lish-moa\/) competition. It's still in progress and I'll try to update it whenever it's possible for me, I hope you find it useful!\n\nIn this task we will be predicting multiple targets of the Mechanism of Action (MoA) response(s) of different samples, given various inputs such as gene expression data and cell viability data. In short Mechanism of Action describes the process by which a molecule, such as a drug, functions to produce a pharmacological effect. A drug\u2019s mechanism of action may refer to its effects on a biological readout such as cell growth, or its interaction and modulation of its direct biomolecular target, for example a protein or nucleic acid.\n\nWe have given info such as:\n-  Gene Expression Data (g-)\n-  Cell Viability Data (c-)\n-  Cp Type (indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle))\n-  Duration and Dose of the Treatment\n-  MoA labels for prediction\n\nJust to be sure this is multi-label classification and one sample might have more than one labels. I tried to explain [here](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/180500) basically.\n\nLet's get started...","8c638231":"# Loading Libraries","7b19412a":"### Features with lowest correlations\n\ng-179 and g-44 seems to have lowest correlation between them followed by g-363 and c-91.","93ca29ca":"# Correlations\n\n### For this part we gonna inspect correlations between features, to see if there is any linear relations between their values. For this we're going to take abs value for each correlation so it only shows how strong the relations is whether it's positive or negative. We see there are lots of highly correlated features, with this info we might thinking about dropping some features or reducing dimensions via other way later...","c3cee1fb":"# Categorical Data\n\n### These are like treatment dose, time and control groups for our samples, we might encode them for modelling later.\n\n### We can observe:\n- Most of the observed treatments are compound for both datasets meanwhile control pertubation are 7-8% for train test set respectively. We can say it's balanced between train test sets.\n\n- Treatment durations are commonly distributed with 48 hour ones slightly (~2%) more than the rest. Again it's pretty balanced for both datasets.\n\n- Doses are evenly distributed, first dose is slightly more than D2 in both datasets(~2%). Both datasets are balanced.","ca7d2a1e":"### Features with highest correlations\n\nc-52 and c-42 seems to have highest correlation between them in our data with the values of 92.46% followed by c-13 and c-73.","1b836888":"# Targets\n\n### It looks like most common MoA labels are nfkb_inhibitor, proteasome_inhibitors followed by cyclooxygenase_inhibitor, all three have more than 400 instances. If we check total label counts per sample we see most of our observations have one MoA meanwhile ~9k of them have none. Multilabel ones are much more rare but still can effect our final model performance...\n\n### There might be some correlation between 1+ MoA labels worth to investigate in future. Using models including these relations might give better results..."}}