{"cell_type":{"42be66c8":"code","c12e9025":"code","9dac10d0":"code","fe90534b":"code","b0e6c38b":"code","c68a4a1e":"code","46605233":"code","7e633b5e":"code","3e43ffe7":"code","4e65a5f5":"code","18420748":"code","beaaf655":"code","a094adcc":"code","45862f7b":"code","2197946f":"code","d8871e2c":"code","eb294894":"code","04bb3d9d":"code","7f25292c":"code","03a5f671":"code","1342a2cb":"code","e8d407b5":"code","57d0a23e":"code","47bc5509":"code","2f9ae7d2":"markdown","a6a2341c":"markdown","dc64527b":"markdown","ab4f4f11":"markdown","8848349a":"markdown","91b9b60f":"markdown","1100441a":"markdown","e790ed13":"markdown","1e82a4b9":"markdown","847f166c":"markdown","36b34bb3":"markdown","336d5521":"markdown","73c78cbf":"markdown","938fb23b":"markdown","97bba5a6":"markdown","3c3aa87f":"markdown","b3937619":"markdown"},"source":{"42be66c8":"import numpy as np \nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport plotly.express as px\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.model_selection import cross_val_score","c12e9025":"# Load the data\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nprint(\"train shape\", train.shape)\nprint(\"test shape\", test.shape)","9dac10d0":"X_train = train.drop('label', axis=1)\ny_train = train[\"label\"]\n\n# free some space\ndel train\n\ng = sns.countplot(y_train)","fe90534b":"index = 7\nprint(\"real value : \", y_train[index])\ndigit = X_train.loc[index,:]\n\n#reshape to a squared shape\nreshape_size = int(np.sqrt(len(digit)))\ndigit = digit.values.reshape(reshape_size,reshape_size)\n#imshow to plot raw digit \nplt.imshow(digit,cmap='binary')\nplt.show()","b0e6c38b":"\n# first scaling:\n# Feature Scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(test)","c68a4a1e":"%%time\npca = PCA(n_components=2)\npca_proj = pca.fit_transform(X_train)","46605233":"plt.figure(figsize=(15,8))\nplt.scatter(pca_proj[:, 0], pca_proj[:, 1], \n            c=y_train, cmap=\"Paired\")\nplt.colorbar()\nplt.show()","7e633b5e":"X_train.shape","3e43ffe7":"pca_test = PCA()\npca_test.fit(X_train)\n\ndef get_right_dimension(pca, threshold=0.9):\n    cumsum = np.cumsum(pca.explained_variance_ratio_)\n    d = np.argmax(cumsum >= threshold) + 1\n    print(f'The {d} first components are sufficient to preserve {threshold * 100}% of the variance')\n    return d","4e65a5f5":"d = get_right_dimension(pca_test)\nprint(f'Original data dimension : {X_train.shape[-1]}')","18420748":"# construct the dataframe corresponding to the new pca_proj\ndf = pd.DataFrame(pca_proj, columns=['axis_{}'.format(i+1) for i in range(pca.n_components)])\ndf['value'] = y_train\n# we have to transform the labels into str so that we can use it for the color fi \ndf[\"value\"] = df[\"value\"].astype(str)\nfig = px.scatter(df, x=\"axis_1\", y=\"axis_2\", color=\"value\", width=800, height=600)\nfig.show()","beaaf655":"# save results\ndf.to_pickle('pca_embedding.pkl')","a094adcc":"#%%time\nif False:\n    tsn_proj = (TSNE(n_components=2)\n                .fit_transform(X_train)\n               )\n    tsn_proj = pd.read_pickle('..\/tsne_2_components_embedding.pkl')\n","45862f7b":"# it takes a loot of time \n# I ve already saved it on the first run  \ntsn_proj = pd.read_pickle(\"..\/input\/digitrecognizertsne\/tsne_embedding.pkl\")\ntsn_proj.head()","2197946f":"plt.figure(figsize=(15,8))\nplt.scatter(tsn_proj.loc[:, 'axis_1'], tsn_proj.loc[:, 'axis_2'], c=y_train, cmap=\"Paired\")\nplt.colorbar()\nplt.show()","d8871e2c":"\ndf = pd.DataFrame(tsn_proj, columns=['axis_1', 'axis_2'])\ndf['value'] = y_train\n\ndf[\"value\"] = df[\"value\"].astype(str)\nfig = px.scatter(df, x=\"axis_1\", y=\"axis_2\", color=\"value\", \n                  width=800, height=600,\n                 title='T-SNE vizualisation of MNIST Data')\nfig.show()","eb294894":"# save data\n#df.to_pickle('tsne_2_components_embedding.pkl')","04bb3d9d":"# let's define our classifiers\nclassifiers = {\"RF\" : RandomForestClassifier(n_estimators=100, criterion = 'entropy', random_state = 42),\n              \"KNN\" : KNeighborsClassifier(n_neighbors = 7),\n              \"NB\" : GaussianNB() }","7f25292c":"%%time\norigin_scores = {}\nfor name, clf in classifiers.items():\n    print(name, 'classifier')\n    origin_scores[name] = cross_val_score(clf, X_train, y_train,\n                             scoring=\"f1_weighted\",\n                             cv=5)\n    print(f'f1_score = {origin_scores[name]}')\n    print(name, \"overall score :\", \n          \"%.3f\"%(np.mean(origin_scores[name])),\n         \"+- %.3f\"%(np.std(origin_scores[name]))\n         )     \n    print(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\")","03a5f671":"%%time\npca_scores = {}\n\nfor name, clf in classifiers.items():\n    print(name, 'classifier')\n    pca_scores[name] = cross_val_score(clf, pca_proj, y_train,\n                             scoring=\"f1_weighted\",\n                             cv=5)\n    print(f'f1_score = {pca_scores[name]}')","1342a2cb":"%%time\ntwo_first_pca_scores = {}\n\n\nfor name, clf in classifiers.items():\n    print(name, 'classifier')\n    two_first_pca_scores[name] = cross_val_score(clf, \n                                                 np.c_[X_train, pca_proj[:, [0, 1]]], \n                                                 y_train,\n                                                 scoring=\"f1_weighted\",\n                                                 cv=5)\n    \n    print(f'CV f1 scores = {two_first_pca_scores[name]}')\n    print(name, \"overall score :\", \n          \"%.3f\"%(np.mean(two_first_pca_scores[name])),\n         \"+- %.3f\"%(np.std(two_first_pca_scores[name]))\n         )     \n    print(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\")","e8d407b5":"%%time\ntsne_scores = {}\n\nfrom sklearn.model_selection import cross_val_score\nfor name, clf in classifiers.items():\n    print(name, 'classifier')\n    tsne_scores[name] = cross_val_score(clf, np.c_[X_train, tsn_proj[['axis_1', 'axis_2']].values], y_train,\n                             scoring=\"f1_weighted\",\n                             cv=5)\n    print(f'f1_score = {tsne_scores[name]}')\n    print(name, \"overall score :\", \n          \"%.3f\"%(np.mean(tsne_scores[name])),\n         \"+- %.3f\"%(np.std(tsne_scores[name]))\n         ) \n    print(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\")","57d0a23e":"\nresults = pd.DataFrame(classifiers.keys(), columns=['Classifier'])\n\nfor colname, dic in zip([\"original_F1_Score\", \"original+2_first_pca_f1_score\", \"original+2_first_tsne_f1_score\"],\n                        [origin_scores, two_first_pca_scores, tsne_scores]):\n    d = {}\n    for k, v in dic.items():\n        d[k] = np.mean(v)\n    results[colname] = d.values()","47bc5509":"results","2f9ae7d2":"To chose the right reducing dimension number, we will compute the number of components that preserve rather than 100% but for example 90% which is a reasonable proportion. (for data viz we select only the 2 or 3 first ones)\n","a6a2341c":"### 1. Oiginal data","dc64527b":"Now lets persist the computed projections for future use","ab4f4f11":"Now let's plot in  2D the two first principal components","8848349a":"### 2. TSNE transformed data\nNow we will apply the same models on `tsn_proj`","91b9b60f":"### 2. PCA transformed data\n\n#### 2.1. All components:\nnow we will apply the same models on `pca_proj`","1100441a":"## DataViz","e790ed13":"#### 2.1. Add the Two first components:\n\nIn order to  fairly compare with the tsne we'll add to the orginal data only the 2 first PCA","1e82a4b9":"Lets take a look to the repartition of the classes","847f166c":"### Save results:","36b34bb3":"### Use Tsne \nThe t-distributed stochastic neighbor embedding [(t-SNE)](http:\/\/www.jmlr.org\/papers\/volume9\/vandermaaten08a\/vandermaaten08a.pdf) algorithm is a dimension reduction technique for data visualization developed by Geo\ufb00rey Hinton and Laurens van der Maaten.\nThe t-SNE algorithm is based on a probabilistic interpretation of proximities. A probability distribution is de\ufb01ned on the pairs of points in the original space\nso that points close to each other have a high probability of being chosen while distant points have a low probability to be selected.  The t-SNE algorithm consists of matching the two probability densities, minimizing the[ Kullback-Leibler divergence](https:\/\/en.wikipedia.org\/wiki\/Kullback%E2%80%93Leibler_divergence) between the two distributions with respect to the\nlocation of the points on the map.\n\nThe t-SNE non-linear \u201dfeature extraction\u201d algorithm constructs a new representation of the data so that the close data in the original space has a high probability of having close representations in the new space. in the other hand, data that are distant in the original space, have a low probability of having close representations in the new space. \nIn practice, the similarity between each pair of data, in both spaces, is measured by means of probabilistic calculations based on distribution hypotheses. And the new representations are constructed in such a way as to minimize the di\ufb00erence between the probability distributions measured in the original space and those of the new space. \n\nIn code block below applies the T-SNE algorithm on our digits, in order to project them into a 2-dimensional space (which are the two axes of the image). \n","336d5521":"As we see the T-sne embedding definitely gives the better encoding for the classifiers","73c78cbf":"### PCA : Plot a projection on the 2 first principal axis:\n\nThe main idea behind PCA is to find out projection vectors called principal axes so that a **maximum ratio of original variance is preserved**.\nThe corresponding vectors minimize the mean squared distance between the original dataset and its projection onto these axes.\n\nGenral approach:\n#### 1. Find out the principal axes : \nBy means of SVD-Singular Value Decomposition technique that lets to decompose X_train into three matrices $U \u03a3 V^T$, where V contains all the principal components that we are looking for. These vectors constitue a new orthonormal basis that will be used as the projection basis \n\n#### 2. Project training data to the principal components basis\nOnce you have identified all the principal components (r axes), the dimensionality reducing is performed by projecting the orginal dataset onto the hyperplane defined by the first r principal components, that will preserve as much variance as possible.\nMathematically, it consists of appling a **dot product** between the training set matrix $X-{train}$ and the matrix $V_r$ , defined as the matrix containing the first r principal components (of the matrix V). \n\nPlease note that the algorithm assumes that dataset is normalized. (BTW The only family of algorithms that are scale-insensitive are tree-based methods : RF, XGB, LGB..) Even though scikit learn implementation integrates this normalisation step\n\nLet's Compute PCA components using `fit_transform()` method of the `sklearn.decomposition.PCA` model","938fb23b":"Lets evaluate different try different algorithms on:\n* original data\n* original data + 2 first PCA\n* original data + 3 first t-sne vectors\n\nfor each case we will test out:\n* RandomForestClassifier\n* KNeighborsClassifier\n* GaussianNB\n\nTo evaluate performance: we will use **f1_weighted** loss function with 5-fold- cross validation strategy\n\nThe [F1_weighted](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) allows to get the average of F1_scores (of each label) weighted by support (the number of true instances for each label).","97bba5a6":"In this notebbok we will see how to reduce dimension on MNIST dataset with two well known technics :\n* PCA \n* T-SNE\n\nThen we'll use the two first components as standalone new features and compare the obtained performances among:\n* original data\n* original data + 2 first pca components\n* original + 2 first t-sne vectors\n\nThe models that wee'll be using :\n* RandomForestClassifier\n* KNeighborsClassifier\n* GaussianNB\n","3c3aa87f":"The image in the center corresponds to the projection of each image in this new space, each digit being associated with\na di\ufb00erent color (in the fig the orange color represents the number \u201d1\u201d, the red color the number \u201d0\u201d). \n\nAs can be seen, the representation provided by the t-SNE algorithm makes it possible to separate and form distinct groups for each of the\ndigits of the data set.\n\nLets use plotly","b3937619":"#### Use Plotly:\nWe will use [Plotly](https:\/\/plotly.com\/)'s python library to make more interactive and high-quality graphs.\n"}}