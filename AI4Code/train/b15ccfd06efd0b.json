{"cell_type":{"3c03669b":"code","7b757f20":"code","3e596baa":"code","eade4c20":"code","c05ac4a1":"code","be61f9ae":"code","8c72a840":"code","7709ce5c":"code","639f201f":"code","2292c726":"code","ac6bb1a7":"code","36094eee":"code","13120512":"code","d8877c47":"code","ab9cf25a":"code","4fe70115":"code","cf863987":"code","fe2180d9":"code","a73407b7":"code","23070678":"code","84fb5246":"code","9996fb5b":"code","f8aa3bbd":"code","54345bcc":"code","7cf9ab02":"code","7e0b8036":"markdown","2a63cfb2":"markdown","e67b2b62":"markdown","2c95e088":"markdown","849e6d36":"markdown","ef2b2c9e":"markdown","01553a48":"markdown","6dd724a1":"markdown","d82247fe":"markdown","f57bb563":"markdown","03edcc37":"markdown","d53bbbf5":"markdown","c29e473a":"markdown","40dbc059":"markdown","f5b12ae0":"markdown","b8cd5441":"markdown"},"source":{"3c03669b":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()\nuser_secrets.set_tensorflow_credential(user_credential)\n\n# Set your own project id here\nPROJECT_ID = 'whats-burning-298707'\nfrom google.cloud import storage\nstorage_client = storage.Client(project=PROJECT_ID)","7b757f20":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport keras\nimport tensorflow.keras.backend as K\n\nimport math\nimport os # accessing directory structure\nimport re","3e596baa":"\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    # detect and init the TPU\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    # instantiate a distribution strategy\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","eade4c20":"AUTO = tf.data.experimental.AUTOTUNE\n\nEPOCHS = 5\nFOLDS = 3\nSEED = 777\nSHARDS = 16\nTARGET_SIZE = [192, 192]\nIMAGE_SIZE = TARGET_SIZE\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\n\nGCS_OUTPUT = 'gs:\/\/fiery\/tfrecords-jpeg-192x192-2'\nGCS_TRAIN_OUTPUT = GCS_OUTPUT + '\/train'\nGCS_TEST_OUTPUT = GCS_OUTPUT + '\/test'\nGCS_PATH = 'gs:\/\/fiery\/tiny-burning'\nGCS_PATTERN_TRAIN = GCS_PATH + '\/train\/*\/*.jpg'\nGCS_PATTERN_TEST = GCS_PATH + '\/test\/*\/*.jpg'\n\nRECORD_PATTERN_TRAIN = GCS_TRAIN_OUTPUT + '\/*.tfrec'\nRECORD_PATTERN_TEST = GCS_TEST_OUTPUT + '\/*.tfrec'\n\nCLASSES = ['fire', 'not_fire'] # fire labels (folder names in the data)\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATTERN_TRAIN)\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATTERN_TEST)","c05ac4a1":"#@title \"display utilities [RUN ME]\"\ndef display_9_images_from_dataset(dataset):\n    plt.figure(figsize=(13,13))\n    subplot=331\n    for i, (image, label) in enumerate(dataset):\n        plt.subplot(subplot)\n        plt.axis('off')\n        plt.imshow(image.numpy().astype(np.uint8))\n        plt.title(label.numpy().decode(\"utf-8\"), fontsize=16)\n        subplot += 1\n        if i==8:\n            break\n    #plt.tight_layout()\n    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n    plt.show()","be61f9ae":"nb_images = len(TRAINING_FILENAMES)\nshard_size = math.ceil(1.0 * nb_images \/ SHARDS)\nprint(\"Pattern matches {} images which will be rewritten as {} .tfrec files containing {} images each.\".format(nb_images, SHARDS, shard_size))\n\ndef decode_jpeg_and_label(filename):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits)\n    # parse image class from containing directory\n    label = tf.strings.split(tf.expand_dims(filename, axis=-1), sep='\/')\n    label = label.values[-2]\n    return image, label\n\ndef is_image(filename, verbose=False):\n\n    data = open(filename,'rb').read(10)\n\n    # check if file is JPG or JPEG\n    if data[:3] == b'\\xff\\xd8\\xff':\n        if verbose == True:\n             print(filename+\" is: JPG\/JPEG.\")\n        return True\n\n    # check if file is PNG\n    if data[:8] == b'\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a':\n        if verbose == True:\n             print(filename+\" is: PNG.\")\n        return True\n\n    # check if file is GIF\n    if data[:6] in [b'\\x47\\x49\\x46\\x38\\x37\\x61', b'\\x47\\x49\\x46\\x38\\x39\\x61']:\n        if verbose == True:\n             print(filename+\" is: GIF.\")\n        return True\n\n    return False\n\nfilenames = tf.data.Dataset.list_files(GCS_PATTERN_TRAIN, seed=35155) # This also shuffles the images\ndataset1 = filenames.map( decode_jpeg_and_label, num_parallel_calls=AUTO) ","8c72a840":"display_9_images_from_dataset(dataset1)","7709ce5c":"def resize_and_crop_image(image, label):\n    # Resize and crop using \"fill\" algorithm:\n    # always make sure the resulting image\n    # is cut out from the source image so that\n    # it fills the TARGET_SIZE entirely with no\n    # black bars and a preserved aspect ratio.\n    w = tf.shape(image)[0]\n    h = tf.shape(image)[1]\n    tw = TARGET_SIZE[1]\n    th = TARGET_SIZE[0]\n    resize_crit = (w * th) \/ (h * tw)\n    image = tf.cond(resize_crit < 1,\n                    lambda: tf.image.resize(image, [w*tw\/w, h*tw\/w]), # if true\n                    lambda: tf.image.resize(image, [w*th\/h, h*th\/h]))  # if false\n    nw = tf.shape(image)[0]\n    nh = tf.shape(image)[1]\n    image = tf.image.crop_to_bounding_box(image, (nw - tw) \/\/ 2, (nh - th) \/\/ 2, tw, th)\n    return image, label\n  \ndataset2 = dataset1.map(resize_and_crop_image, num_parallel_calls=AUTO) ","639f201f":"display_9_images_from_dataset(dataset2)","2292c726":"def recompress_image(image, label):\n    height = tf.shape(image)[0]\n    width = tf.shape(image)[1]\n    image = tf.cast(image, tf.uint8)\n    image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)\n    return image, label, height, width\n\ndataset3 = dataset2.map(recompress_image, num_parallel_calls=AUTO)\ndataset3 = dataset3.batch(shard_size) # sharding: there will be one \"batch\" of images per file ","ac6bb1a7":"# Three types of data can be stored in TFRecords: bytestrings, integers and floats\n# They are always stored as lists, a single data element will be a list of size 1\n\ndef _bytestring_feature(list_of_bytestrings):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n\ndef _int_feature(list_of_ints): # int64\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n\ndef _float_feature(list_of_floats): # float32\n    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))\n  \ndef to_tfrecord(tfrec_filewriter, img_bytes, label, height, width):  \n    class_num = np.argmax(np.array(CLASSES)==label) # 'roses' => 2 (order defined in CLASSES)\n    one_hot_class = np.eye(len(CLASSES))[class_num]     # [0, 0, 1, 0, 0] for class #2, roses\n\n    feature = {\n        \"image\": _bytestring_feature([img_bytes]), # one image in the list\n        \"class\": _int_feature([class_num]),        # one class in the list\n\n        # additional (not very useful) fields to demonstrate TFRecord writing\/reading of different types of data\n        \"label\":         _bytestring_feature([label]),          # fixed length (1) list of strings, the text label\n        \"size\":          _int_feature([height, width]),         # fixed length (2) list of ints\n        \"one_hot_class\": _float_feature(one_hot_class.tolist()) # variable length  list of floats, n=len(CLASSES)\n    }\n    return tf.train.Example(features=tf.train.Features(feature=feature))\n  \nprint(\"Writing TFRecords\")\nfor shard, (image, label, height, width) in enumerate(dataset3):\n    # batch size used as shard size here\n    shard_size = image.numpy().shape[0]\n    # good practice to have the number of records in the filename\n    filename = GCS_TRAIN_OUTPUT + \"{:02d}-{}.tfrec\".format(shard, shard_size)\n\n    with tf.io.TFRecordWriter(filename) as out_file:\n        for i in range(shard_size):\n            example = to_tfrecord(out_file,\n                                  image.numpy()[i], # re-compressed image: already a byte string\n                                  label.numpy()[i],\n                                  height.numpy()[i],\n                                  width.numpy()[i])\n            out_file.write(example.SerializeToString())\n    print(\"Wrote file {} containing {} records\".format(filename, shard_size))","36094eee":"TRAINING_RECORD_NAMES = tf.io.gfile.glob(RECORD_PATTERN_TRAIN)\nTEST_RECORD_NAMES = tf.io.gfile.glob(RECORD_PATTERN_TEST)","13120512":"MIXED_PRECISION = True\nXLA_ACCELERATE = True\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    print('Mixed precision enabled')\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')","d8877c47":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation \/ 180.\n    shear = math.pi * shear \/ 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n        \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one\/height_zoom,zero,zero, zero,one\/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))","ab9cf25a":"def transform(image,label):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = IMAGE_SIZE[0]\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = 15. * tf.random.normal([1],dtype='float32')\n    shr = 5. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    h_shift = 16. * tf.random.normal([1],dtype='float32') \n    w_shift = 16. * tf.random.normal([1],dtype='float32') \n  \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM\/\/2,-DIM\/\/2,-1), DIM )\n    y = tf.tile( tf.range(-DIM\/\/2,DIM\/\/2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM\/\/2+XDIM+1,DIM\/\/2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM,DIM,3]),label","4fe70115":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    return image\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label # returns a dataset of (image, label) pairs\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum # returns a dataset of image(s)\n\ndef load_dataset(filenames, labeled = True, ordered = False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n    \n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n        \n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # use data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO) # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n    return dataset\n\ndef data_augment(image, label):\n    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n    # of the TPU while the TPU itself is computing gradients.\n    image = tf.image.random_flip_left_right(image)\n    return image, label   \n\ndef get_training_dataset(dataset,do_aug=True):\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    if do_aug: dataset = dataset.map(transform, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(dataset):\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\n\nNUM_TRAINING_IMAGES = int( count_data_items(TRAINING_RECORD_NAMES) * (FOLDS-1.)\/FOLDS )\nNUM_VALIDATION_IMAGES = int( count_data_items(TRAINING_RECORD_NAMES) * (1.\/FOLDS) )\nNUM_TEST_IMAGES = count_data_items(TEST_RECORD_NAMES)\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\n\nprint('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))","cf863987":"fire_data = load_dataset(TRAINING_RECORD_NAMES)\ntrain_data = get_training_dataset(fire_data)\nvalidation_data = get_validation_dataset(fire_data)","fe2180d9":"row = 3; col = 4;\nall_elements = get_training_dataset(load_dataset(TRAINING_RECORD_NAMES),do_aug=False).unbatch()\none_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\naugmented_element = one_element.repeat().map(transform).batch(row*col)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row\/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break","a73407b7":"def make_model():\n    input_tensor = keras.layers.Input(shape=(img_height, img_width, 3))\n    v3 = keras.applications.InceptionV3(\n        include_top=False,\n        weights=\"imagenet\",\n        input_tensor=input_tensor,\n        input_shape=None,\n        pooling=None,\n        classes=2,\n        classifier_activation=\"softmax\",\n    )\n\n    model = keras.models.Sequential()\n    model.add(v3)\n#     model.add(keras.layers.Input(shape=(3,img_height,img_width)))\n#     model.add(keras.layers.Flatten())\n    model.add(keras.layers.GlobalAveragePooling2D())\n#     model.add(keras.layers.Dropout(.2))\n    model.add(keras.layers.Dense(4096, activation=\"relu\"))\n#     model.add(keras.layers.Dropout(.2))\n#     model.add(keras.layers.Dense(64, activation=\"relu\"))\n#     model.add(keras.layers.Dropout(.2))\n    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n    return model","23070678":"model.summary()","84fb5246":"train_datagen = keras.preprocessing.image.ImageDataGenerator(\n    rescale=1.\/255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    rotation_range=10,\n    width_shift_range=0.1, \n    height_shift_range=0.1,\n    horizontal_flip=True,\n    validation_split=0.2) # set validation split\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='sparse',\n    subset='training') # set as training data\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_data_dir, # same directory as training data\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='sparse',\n    subset='validation') # set as validation data\n\ntest_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1.\/255)","9996fb5b":"img_height, img_width = IMAGE_SIZE\n\n# train_data_dir = '\/kaggle\/input\/burning\/train\/'\n# test_data_dir = '\/kaggle\/input\/burning\/test\/'\n\nbatch_size = 64\nepochs = 128\ncheckpoint_filepath = '.\/weights.best.hdf5'\n\nwith strategy.scope():\n\n    model = make_model()\n    model.compile(loss=\"binary_crossentropy\", #Loss function\n                  optimizer='adam', \n                  metrics=[\"accuracy\"])\n    \n","f8aa3bbd":"es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\nmc = keras.callbacks.ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', \n                                     mode='max', verbose=1, save_best_only=True)\nmodel.fit(\n    train_data,\n    epochs=EPOCHS,\n    steps_per_epoch = STEPS_PER_EPOCH,\n    validation_data=validation_data, \n    callbacks=[mc,es])","54345bcc":"EPOCHS = 128\ncheckpoint_filepath = '.\/weights.best.hdf5'\nes = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\nmc = keras.callbacks.ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', \n                                     mode='max', verbose=1, save_best_only=True)\n\nmodel.compile(loss=\"binary_crossentropy\", #Loss function\n              optimizer='adam', \n              metrics=[\"accuracy\"])\n\ndatagen = keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=10,  \n    zoom_range = 0.10,  \n    shear_range=0.1, # bendiness (square to parallelogram kind of transformation)\n    width_shift_range=0.1, \n    height_shift_range=0.1,\n    # vertical_flip=True, \n    horizontal_flip=True)\n\nhistory = model.fit_generator(datagen.flow(X_train.reshape(-1, 28, 28, 1), y_train),\n                              epochs=EPOCHS,\n                              validation_data=(X_valid, y_valid), callbacks=[mc,es])","7cf9ab02":"model.evaluate(test_datagen.flow_from_directory(test_data_dir))","7e0b8036":"## Locate TPU","2a63cfb2":"## [Enable mixed-precision and\/or XLA](https:\/\/www.kaggle.com\/cdeotte\/rotation-augmentation-gpu-tpu-0-96)\n","e67b2b62":"## Link GCloud, GCS and add image data","2c95e088":"## Train the Model\n* The best model is saved as a hdf5 checkpoint based on max validation accuracy.","849e6d36":"## [Resize images in dataset](https:\/\/codelabs.developers.google.com\/codelabs\/keras-flowers-data#4)","ef2b2c9e":"## [Write dataset to TFRecord files](https:\/\/codelabs.developers.google.com\/codelabs\/keras-flowers-data#4)","01553a48":"### Update CONFIGURATION with TFRecords","6dd724a1":"## Conclusion\nThis concludes your starter analysis! To go forward from here, click the blue \"Edit Notebook\" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!","d82247fe":"## [Recompress Images](https:\/\/codelabs.developers.google.com\/codelabs\/keras-flowers-data#4)","f57bb563":"## [Inspect Images](https:\/\/codelabs.developers.google.com\/codelabs\/keras-flowers-data#4)","03edcc37":"## CONFIGURATION","d53bbbf5":"## [Configure Dataset and Image Augmentation Ops to be Done on TPU\/GPU](https:\/\/www.kaggle.com\/cdeotte\/rotation-augmentation-gpu-tpu-0-96)","c29e473a":"# To Fire or Not to Fire\n##### This notebook focuses on the detection of fire in images. ","40dbc059":"### Add Augmentation Pipeline\n* Preprocessing is done on-the-fly. This means that images are resized to 256x256, pixels are rescaled to be between 0 and 1. \n* Image augmentation is used to reduce overfitting. Images are randomly shifted (`width_shift_range` and `height_shift_range`), flipped (`horizontal_flip` and maybe `vertical_flip` as well), rotated (`rotation_range`), bent (`shear_range`), and cropped (`zoom_range`).","f5b12ae0":"## Construct the Model\n* A shallow combination of convolutional, pooling, and fully-connected layers are placed on top of [Inception V3](https:\/\/keras.io\/api\/applications\/inceptionv3\/)","b8cd5441":"## Import Packages"}}