{"cell_type":{"f4c3e9fd":"code","c989cb27":"code","9c61d1c1":"code","9c629e85":"code","d12b3629":"code","5039afd5":"code","ddb6de30":"code","fb45627f":"code","fd6e84b5":"code","6eb91424":"code","be758a0e":"code","5b8072f0":"code","b478e309":"code","2c4c4405":"markdown","3cd752e9":"markdown","401c76a1":"markdown","8eb61726":"markdown"},"source":{"f4c3e9fd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","c989cb27":"train_features = pd.read_csv(\"\/kaggle\/input\/dengai-predicting-disease-spread\/dengue_features_train.csv\")\ntest_features = pd.read_csv(\"\/kaggle\/input\/dengai-predicting-disease-spread\/dengue_features_test.csv\")\ny_train = pd.read_csv(\"\/kaggle\/input\/dengai-predicting-disease-spread\/dengue_labels_train.csv\")\nprint( \"train shape :\",train_features.shape, \"\\n\",\n      \"test shape :\", test_features.shape, \"\\n\",\n      \"y_train shape :\", y_train.shape)","9c61d1c1":"def get_data(data):\n    data[\"ndvi_ne_missing\"] = data[\"ndvi_ne\"].isna().astype(int)\n    data[\"ndvi_nw_missing\"] = data[\"ndvi_nw\"].isna().astype(int)\n    city = [\"iq\",\"sj\"]\n    data['week_start_date'] = pd.to_datetime(data['week_start_date'])\n    month_dummies = pd.get_dummies(data['week_start_date'].dt.month, prefix=\"month\")\n    data = data.join(month_dummies)\n    for city_i in city:\n        data.loc[data.city==city_i] = data.loc[data.city==city_i].interpolate(method='slinear')\n    data[\"city\"] = (data[\"city\"]==\"sj\").astype(int)\n    return data","9c629e85":"train_features = get_data(train_features)\ntest_features = get_data(test_features)","d12b3629":"import lightgbm as lgbm\nfrom sklearn import metrics\nfrom scipy.optimize import differential_evolution\nfrom sklearn.model_selection import TimeSeriesSplit","5039afd5":"def lgbm_optimization(x):\n    \n    lr = x[0]\n    max_depth = int(x[1])\n    lambda_l1 = x[2]\n    num_itera = int(x[3])\n    min_data_in_leaf = int(x[4])\n    tscv = TimeSeriesSplit()\n\n    train_LGBM = []\n    params = {\n        'boosting_type': 'gbdt',\n        'objective': 'poisson',\n        'metric': {'mae'},\n        'max_depth' : max_depth,\n        'learning_rate': lr,\n        'lambda_l1' : lambda_l1,\n        'verbose': 0,\n        'min_data_in_leaf' : min_data_in_leaf   }\n    to_drop = [\"year\",'week_start_date','city']\n    index = train_features.loc[train_features.city==city,:].index\n    train_features_ = train_features.loc[index,]\n    y_train_ = y_train.loc[index,]\n    for i,(a,b) in enumerate(tscv.split(train_features_)) :\n        Xt = train_features_.iloc[a,:]\n        Xt = Xt.drop(to_drop,axis = 1)\n        yt = y_train_.loc[Xt.index, \"total_cases\"]\n\n        Xv = train_features_.iloc[b,:]\n        Xv = Xv.drop(to_drop,axis = 1)\n        yv = y_train_.loc[Xv.index, \"total_cases\"]\n        \n        lgb_train = lgbm.Dataset(Xt, yt)\n        lgb_eval = lgbm.Dataset(Xv, yv, reference=lgb_train)\n        \n         \n        learner = lgbm.train(params,\n                lgb_train,\n                num_boost_round=num_itera,\n                valid_sets=lgb_eval)\n\n\n\n        train_LGBM.append(pd.Series(learner.predict(Xv),\n                                    index=Xv.index, name=\"predict\"+ str(i)))\n        \n    train_LGBM = pd.concat(train_LGBM, axis=1).mean(axis=1)\n    \n    mae_cv = metrics.mean_absolute_error(y_train_.loc[train_LGBM.index,].total_cases, train_LGBM.values.astype(int))\n    return mae_cv","ddb6de30":"#bounds = [(0, 0.1), (2, 10),(3,30),(100,1000),(10,1000)]\n#result = differential_evolution(lgbm_optimization, bounds,disp=True,seed=123)\n#result.x, result.fun","fb45627f":"result = [6.34718946e-03, 6.02508835e+00, 9.80686603e+00, 1.62936154e+02, 5.63587429]\nlr = result[0]\nmax_depth = int(result[1])\nlambda_l1 = result[2]\nnum_iterations = int(result[3])\nmin_data_in_leaf = int(result[4])\nparams = {\n        'boosting_type': 'gbdt',\n        'objective': 'gamma',\n        'metric': {'mae'},\n        'max_depth' : max_depth,\n        'learning_rate': lr,\n        'lambda_l1' : lambda_l1,\n        'verbose': 0,\n        'min_data_in_leaf' : min_data_in_leaf\n            }\nto_drop = to_drop = [\"year\",'week_start_date']\ntrain_features_ = train_features.drop(to_drop,axis =1)\nlgb_train = lgbm.Dataset(train_features_, y_train.loc[:, \"total_cases\"])\nlearner = lgbm.train(params,\n                lgb_train,\n                num_boost_round=num_iterations)","fd6e84b5":"submission_format = pd.read_csv(\"\/kaggle\/input\/dengai-predicting-disease-spread\/submission_format.csv\")","6eb91424":"def get_submission(test,submission: pd.DataFrame):\n    city = [0,1]\n    to_drop = [\"year\",'week_start_date']\n    test_ = test.drop(to_drop, axis = 1)\n    submission.loc[test_.index,\"total_cases\"] = learner.predict(test_).astype(int)\n    return submission","be758a0e":"submission_format = get_submission(test_features, submission_format)","5b8072f0":"submission_format.set_index(\"city\").to_csv(\"dengAI_baseline_lightGBM.csv\", header=True)","b478e309":"submission_format","2c4c4405":"# Novas vari\u00e1veis\n\nPara as vari\u00e1veis *ndvi_ne* e *ndvi_nw* foi criado vari\u00e1veis dummies para as observa\u00e7\u00f5es estejam perdidas. Para as imputar as vari\u00e1veis perdidas foi realizado uma interpola\u00e7\u00e3o polinomial. Foi obtido o m\u00eas para cada observa\u00e7\u00e3o, logo, foi criado vari\u00e1veis dummies.","3cd752e9":"[6.34718946e-03, 6.02508835e+00, 9.80686603e+00, 1.62936154e+02,\n        5.63587429]","401c76a1":"# Modelagem\nO modelagem consistiu utilizar lightgbm avaliando o ajuste e otimizando os hyperpar\u00e3metros mediandte valida\u00e7\u00e3o cruzada (k=5) mediante split pelo tempo e utilizando o algoritmo differential_evolution. Os hyperpar\u00e2metros a otimizar foram:\n\n* learning rate: taxa de aprendizagem do algoritmo gradiente\n* max_depth : m\u00e1xima profundidad de cada arvore\n* lambda_l1 : penaliza\u00e7\u00e3o do tipo lasso\n* num_itera : n\u00famero de itera\u00e7\u00f5es at\u00e9 converg\u00eancia\n* min_data_in_leaf : par\u00e2metro utilizado para prevenir overfitting","8eb61726":"# Otimiza\u00e7\u00e3o"}}