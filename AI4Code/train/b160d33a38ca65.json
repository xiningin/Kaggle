{"cell_type":{"32594ae9":"code","9873f306":"code","0bcb478a":"code","ea7119ff":"code","c7d37a8c":"code","02a43d4c":"code","8573797c":"code","1e571212":"code","cac1c361":"code","75cfc027":"code","ce400452":"code","f96ed2a7":"code","61994393":"code","d8f002c9":"code","a5316700":"code","e45a429c":"markdown","680e1c92":"markdown","a0a64b5a":"markdown","2db85a79":"markdown","ca29cfd3":"markdown","45d37675":"markdown","d1dfaefd":"markdown","d03d9945":"markdown","185f6d2c":"markdown","d30f1f82":"markdown","db374bef":"markdown","12fef49d":"markdown","37ee8276":"markdown"},"source":{"32594ae9":"%%time\n!pip install \/kaggle\/input\/pythonmag\/mag > \/dev\/null\n!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null\n!pip install \/kaggle\/input\/sacrebleu\/sacreBLEU-master\/ > \/dev\/null\n!pip install \/kaggle\/input\/fairseq-hacked\/fairseq > \/dev\/null","9873f306":"import os\nimport json\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd","0bcb478a":"%%time\n!python \/kaggle\/input\/old-bert-code\/predict_test.py \\\n  --model_dir \/kaggle\/input\/stackx-80-aux-ep-3       \\\n  --sub_file model1_bert_base_uncased_pred.csv","ea7119ff":"%%time\n!python ..\/input\/bert-base-random-code\/run.py                \\\n  --sub_file=model2_bert_base_cased_pred.csv                  \\\n  --data_path=\/kaggle\/input\/google-quest-challenge\/            \\\n  --max_sequence_length=500                                     \\\n  --max_title_length=26                                          \\\n  --max_question_length=260                                       \\\n  --max_answer_length=210                                          \\\n  --batch_size=8                                                    \\\n  --bert_model=\/kaggle\/input\/bert-base-pretrained\/stackx-base-cased\/","c7d37a8c":"# setups\n\nROBERTA_EXPERIMENT_DIR = \"2-4-roberta-base-saved-5-head_tail-roberta-stackx-base-v2-pl1kksample20k-1e-05-210-260-500-26-roberta-200\"\n!mkdir $ROBERTA_EXPERIMENT_DIR\n!ln -s \/kaggle\/input\/roberta-stackx-base-pl20k\/checkpoints $ROBERTA_EXPERIMENT_DIR\/checkpoints\n\nROBERTA_CONFIG = {\n    \"_seed\": 42,\n    \"batch_accumulation\": 2,\n    \"batch_size\": 4,\n    \"bert_model\": \"roberta-base-saved\",\n    \"folds\": 5,\n    \"head_tail\": True,\n    \"label\": \"roberta-stackx-base-v2-pl1kksample20k\",\n    \"lr\": 1e-05,\n    \"max_answer_length\": 210,\n    \"max_question_length\": 260,\n    \"max_sequence_length\": 500,\n    \"max_title_length\": 26,\n    \"model_type\": \"roberta\",\n    \"warmup\": 200\n}\nwith open(os.path.join(ROBERTA_EXPERIMENT_DIR, \"config.json\"), \"w\") as fp:\n    json.dump(ROBERTA_CONFIG, fp)\n    \n!echo kek > $ROBERTA_EXPERIMENT_DIR\/command","02a43d4c":"%%time\n!python ..\/input\/roberta-base-code\/infer.py                 \\\n  --experiment $ROBERTA_EXPERIMENT_DIR                       \\\n  --checkpoint=best_model.pth                                 \\\n  --bert_model=\/kaggle\/input\/roberta-base-model                \\\n  --dataframe=\/kaggle\/input\/google-quest-challenge\/test.csv     \\\n  --output_dir=roberta-base-output","8573797c":"%%time\n!python ..\/input\/bart-code\/run.py                      \\\n  --sub_file=model4_bart_large_pred.csv                 \\\n  --data_path=\/kaggle\/input\/google-quest-challenge\/      \\\n  --max_sequence_length=500                               \\\n  --max_title_length=26                                    \\\n  --max_question_length=260                                 \\\n  --max_answer_length=210                                    \\\n  --batch_size=4                                              \\\n  --bert_model=bart.large","1e571212":"sample_submission_df = pd.read_csv(\"\/kaggle\/input\/google-quest-challenge\/sample_submission.csv\", \n                             index_col='qa_id')\ntarget_columns = sample_submission_df.columns\nprint(f'There are {len(target_columns)} targets to predict')\n\ntrain_df = pd.read_csv(\"\/kaggle\/input\/google-quest-challenge\/train.csv\")","cac1c361":"model1_bert_base_uncased_pred_df = pd.read_csv(\"model1_bert_base_uncased_pred.csv\")\nmodel2_bert_base_cased_pred_df = pd.read_csv(\"model2_bert_base_cased_pred.csv\")\nmodel4_bart_large_pred_df = pd.read_csv(\"model4_bart_large_pred.csv\")","75cfc027":"roberta_base_dfs = [pd.read_csv(\n                    os.path.join(\"roberta-base-output\", \"fold-{}.csv\".format(fold))) \n                    for fold in range(5)]\n\nmodel3_roberta_pred_df = roberta_base_dfs[0].copy()\n\nfor col in target_columns:\n    model3_roberta_pred_df[col] = np.mean([df[col] for df in roberta_base_dfs], axis=0)","ce400452":"blended_df = model3_roberta_pred_df.copy()\n\nfor col in target_columns:\n    blended_df[col] = (\n        model1_bert_base_uncased_pred_df[col] * 0.1 +\n        model2_bert_base_cased_pred_df[col] * 0.2 + \n        model3_roberta_pred_df[col] * 0.1 + \n        model4_bart_large_pred_df[col] * 0.3\n    )","f96ed2a7":"def postprocess_single(target, ref):\n    \"\"\"\n    The idea here is to make the distribution of a particular predicted column\n    to match the correspoding distribution of the corresponding column in the\n    training dataset (called ref here)\n    \"\"\"\n    \n    ids = np.argsort(target)\n    counts = sorted(Counter(ref).items(), key=lambda s: s[0])\n    scores = np.zeros_like(target)\n    \n    last_pos = 0\n    v = 0\n    \n    for value, count in counts:\n        next_pos = last_pos + int(round(count \/ len(ref) * len(target)))\n        if next_pos == last_pos:\n            next_pos += 1\n\n        cond = ids[last_pos:next_pos]\n        scores[cond] = v\n        last_pos = next_pos\n        v += 1\n        \n    return scores \/ scores.max()","61994393":"def postprocess_prediction(prediction, actual):\n    \n    postprocessed = prediction.copy()\n    \n    for col in target_columns:\n        scores = postprocess_single(prediction[col].values, actual[col].values)\n        # Those are columns where our postprocessing gave substantial improvement.\n        # It also helped for some others, but we didn't include them as the gain was\n        # very marginal (less than 0.01)\n        if col in (\n            \"question_conversational\",\n            \"question_type_compare\",\n            \"question_type_definition\",\n            \"question_type_entity\",\n            \"question_has_commonly_accepted_answer\",\n            \"question_type_consequence\",\n            \"question_type_spelling\"\n        ):\n            postprocessed[col] = scores\n            \n        # scale to 0-1 interval\n        v = postprocessed[col].values\n        postprocessed[col] = (v - v.min()) \/ (v.max() - v.min())\n    \n    return postprocessed","d8f002c9":"postprocessed = postprocess_prediction(blended_df, train_df)","a5316700":"postprocessed.to_csv(\"submission.csv\", index=False)","e45a429c":"**Install necessary packages**\n - [mag](https:\/\/github.com\/ex4sperans\/mag) is a lightweight library to keep track of experiments\n - sacremoses is a dependency for transformers\n - sacreBLEU and fairseq are dependencies for the BART model ","680e1c92":"### Model 4. BART\n\nBART-large, with pseudo-labeling.","a0a64b5a":"# <center> Google Quest Q&A Labeling\n## <center> 1st place solution\n#### <center> by Dmitriy Danevskiy, Oleg Yaroshevskiy, Yury Kashnitsky, and Dmitriy Abulkhanov\n\nThe purpose of this competition is to analyze StackExchange questions & answers predicting whether the question is interesting, whether the answer is helpful or misleading etc. So in theory, top solutions can help Q&A systems in getting more human-like.\n\n\nIn a nutshell, our team trained 4 models: 2 [BERT](https:\/\/arxiv.org\/abs\/1810.04805) ones, one [RoBERTa](https:\/\/arxiv.org\/abs\/1907.11692), and one [BART](https:\/\/arxiv.org\/abs\/1910.13461). Key ideas are:\n- pretraining language models with StackExchange data and auxiliary targets\n- pseudo-labeling\n- postprocessing predictions\n\nDetails are outlined [in this post](https:\/\/www.kaggle.com\/c\/google-quest-challenge\/discussion\/129840), code is shared in [this repository](https:\/\/github.com\/oleg-yaroshevskiy\/quest_qa_labeling). ","2db85a79":"**Blending**","ca29cfd3":"**Reading submission files**","45d37675":"**Applying postprocessing to the final blend, also discussed [here](https:\/\/www.kaggle.com\/c\/google-quest-challenge\/discussion\/129840).**","d1dfaefd":"\n# Inference\n\n### Model 1. BERT base uncased\n\nThis is an uncased BERT model, its LM is finetuned with StackExchange data.","d03d9945":"### Model 3. RoBERTa\n\nHere we're resorting to both LM finetuning and pseudo-labeling.","185f6d2c":"**First, we read the 30 target columns that we need to predict.**","d30f1f82":"**Saving the submission file.**","db374bef":"**For RoBERTa, we average predictions from 5 folds**","12fef49d":"### Model 2. BERT base cased\n\nThis is a cased BERT model, its LM is finetuned with StackExchange data, code has been refactored w.r.t. to the first model.","37ee8276":"# Blending and postprocessing"}}