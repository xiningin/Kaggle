{"cell_type":{"da1f373c":"code","46b23020":"code","3524fca5":"code","8f371ce4":"code","1e7ab9cd":"code","24caa278":"code","880b7c18":"code","c42569cd":"code","23c682bf":"code","1cdec72b":"code","800e57e7":"code","aa0e5473":"code","f49dc788":"code","d9a77ad3":"code","2b18108e":"code","db662db9":"code","ca25e22e":"code","8a18cea1":"code","55ce84c3":"code","2ef7ee2b":"code","b5bc2ec0":"code","6bb03b12":"code","4c87d941":"code","18a35e3c":"code","290012a8":"code","f741c730":"code","a096b7e0":"code","696f323a":"code","c2151f57":"code","c81cdc64":"code","1e08bce2":"code","996fccb0":"code","596af3d5":"code","18afcef8":"code","9b20f05a":"code","629e7770":"code","b0bab5bf":"code","2c713e41":"code","2304c19f":"code","21a3f143":"code","20a04b4e":"code","a03ca304":"code","4f96c7f5":"code","9d85ee5c":"code","a7bfaca2":"code","d5ac35b5":"code","19afbbce":"code","6e3ea2aa":"code","e672f3a1":"code","7b7b61b2":"code","1a445bcc":"code","86b38490":"code","7eafda5e":"code","888481b5":"code","899b5070":"code","42a71d8c":"code","0c93ede0":"code","386b872f":"code","12581c92":"code","4ab37403":"code","afe2f518":"code","39420c7e":"code","0ef52f50":"code","e8d5922f":"code","834a704d":"code","bddbaa56":"code","17c7c5fe":"code","ce6ce66f":"code","7794fe30":"code","e9dde298":"code","6e869593":"code","3daca674":"code","229b0a5c":"code","c7bda703":"code","d6eeb644":"code","ebec136e":"code","42a4f6bf":"code","7a90a1f5":"markdown","d33b486d":"markdown","744ed636":"markdown","6408182e":"markdown","dc27b7e1":"markdown","34967348":"markdown","853f0fa4":"markdown","dfbc7cd0":"markdown","296d79b7":"markdown","b73a8612":"markdown","c6c64252":"markdown","5b64a1b3":"markdown","28b5af97":"markdown","e27bb527":"markdown","0ecee096":"markdown","3a7bef4d":"markdown","e9aa0a37":"markdown","1b4cb14c":"markdown","ab432c52":"markdown","e231e9a8":"markdown","c9774a06":"markdown","b5738d7e":"markdown","841b8324":"markdown","54329d61":"markdown","18e749de":"markdown","dc66f803":"markdown","213c1638":"markdown","548b2666":"markdown","24887df3":"markdown","4f142c1b":"markdown","d3ca5cf4":"markdown","fea4afb9":"markdown","168ae0d6":"markdown","3271aa84":"markdown","67ef5761":"markdown","3c5dcf43":"markdown","c1cd5157":"markdown","fb302939":"markdown","5666b28f":"markdown","8b1d6802":"markdown","c35df4bb":"markdown","b8e4c3b8":"markdown","8f069d7e":"markdown","5e1d7051":"markdown","cf47ddad":"markdown","c64fc669":"markdown","1cd910f6":"markdown"},"source":{"da1f373c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# the path of the dataset\nHOUSING_PATH = \"..\/input\/california-housing-prices\"\n\n\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","46b23020":"def load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)","3524fca5":"housing = load_housing_data()\nhousing.head()","8f371ce4":"# checking summary of the dataset\nhousing.info()","1e7ab9cd":"# getting stats of the dataset\nhousing.describe()","24caa278":"housing[\"ocean_proximity\"].value_counts()","880b7c18":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20, 15))\nplt.show()","c42569cd":"def split_train_test(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]","23c682bf":"train_set, test_set = split_train_test(housing, 0.2)\nprint(len(train_set), \"train +\", len(test_set), \"test\")","1cdec72b":"import hashlib\n# defining functions for checking if the test set remains uncorrupted from the training set\n# if hash of the id is less or equal to 20 percent of the data\ndef test_set_check(identifier, test_ratio, hash):\n    return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio","800e57e7":"def split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5):\n    ids = data[id_column]\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio, hash))\n    return data.loc[~in_test_set], data.loc[in_test_set]","aa0e5473":"housing_with_id = housing.reset_index() # adds an index column\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")","f49dc788":"housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")","d9a77ad3":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","2b18108e":"train_set","db662db9":"# dividing into categories of income and putting income above 5 into category of 5\nhousing[\"income_cat\"] = np.ceil(housing[\"median_income\"] \/ 1.5)\nhousing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace= True)","ca25e22e":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","8a18cea1":"housing[\"income_cat\"].value_counts() \/ len(housing)","55ce84c3":"for set in (strat_train_set, strat_test_set):\n    set.drop([\"income_cat\"], axis=1, inplace=True)","2ef7ee2b":"housing = strat_train_set.copy()","b5bc2ec0":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")","6bb03b12":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)","4c87d941":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n            s=housing[\"population\"]\/100, label=\"population\", c=\"median_house_value\",\n            cmap=plt.get_cmap(\"jet\"), colorbar=True)\nplt.legend()","18a35e3c":"corr_matrix = housing.corr()","290012a8":"# seeing how much each attribute correlates with the median house value\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","f741c730":"# another way to see the correlation of some of the promising attributes\n\n# from pandas.tools.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\npd.plotting.scatter_matrix(housing[attributes], figsize = (12, 8))","a096b7e0":"housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1)","696f323a":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]= housing[\"population\"]\/housing[\"households\"]","c2151f57":"corr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","c81cdc64":"housing = strat_train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","1e08bce2":"# in the latest scikit version it is called SimpleImputer\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")","996fccb0":"housing_num = housing.drop(\"ocean_proximity\", axis=1)\n\n# fit imputer instance to training data using fit method\nimputer.fit(housing_num)","596af3d5":"#We'll check the median values for each of the features now\nimputer.statistics_","18afcef8":"housing_num.median().values","9b20f05a":"# using the trained imputer to fill in missing values of the train set to transform it\n# filling it with trained medians\nX = imputer.transform(housing_num)","629e7770":"# if you wanna put it back to a pandas dataframe just use the following:\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns)","b0bab5bf":"# converting text or categorical values into numerical values\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nhousing_cat = housing[\"ocean_proximity\"]\nhousing_cat_encoded = encoder.fit_transform(housing_cat)","2c713e41":"housing_cat_encoded","2304c19f":"print(encoder.classes_)","21a3f143":"from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nhousing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\nhousing_cat_1hot","20a04b4e":"housing_cat_1hot.toarray()","a03ca304":"from sklearn.preprocessing import LabelBinarizer\nencoder = LabelBinarizer()\nhousing_cat_1hot = encoder.fit_transform(housing_cat)\nhousing_cat_1hot","4f96c7f5":"from sklearn.base import BaseEstimator, TransformerMixin\n\nrooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, household_ix]\n        population_per_household = X[:, population_ix] \/ X[:, household_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n        \nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","9d85ee5c":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy=\"median\")),\n    ('attribs_adder', CombinedAttributesAdder()),\n    ('std_scaler', StandardScaler()),\n])\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","a7bfaca2":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)","d5ac35b5":"housing_prepared.shape","19afbbce":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","6e3ea2aa":"some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\\t\", lin_reg.predict(some_data_prepared))\nprint(\"Labels:\\t\\t\", list(some_labels))","e672f3a1":"some_data_prepared","7b7b61b2":"from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","1a445bcc":"\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)\nhousing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)","86b38490":"tree_rmse","7eafda5e":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","888481b5":"tree_rmse_scores","899b5070":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard Deviation:\", scores.std())\n    \ndisplay_scores(tree_rmse_scores)","42a71d8c":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","0c93ede0":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)","386b872f":"housing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","12581c92":"from sklearn.model_selection import cross_val_score\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","4ab37403":"scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\npd.Series(np.sqrt(-scores)).describe()","afe2f518":"from sklearn.svm import SVR\n\nsvm_reg = SVR(kernel=\"linear\")\nsvm_reg.fit(housing_prepared, housing_labels)\nhousing_predictions = svm_reg.predict(housing_prepared)\nsvm_mse = mean_squared_error(housing_labels, housing_predictions)\nsvm_rmse = np.sqrt(svm_mse)\nsvm_rmse","39420c7e":"from sklearn.model_selection import GridSearchCV\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n]\n\nforest_reg = RandomForestRegressor(random_state=42)\n\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)","0ef52f50":"grid_search.best_params_","e8d5922f":"grid_search.best_estimator_","834a704d":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","bddbaa56":"pd.DataFrame(grid_search.cv_results_)","17c7c5fe":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\nparam_distribs ={\n    'n_estimators': randint(low=1, high=200),\n    'max_features': randint(low = 1, high=8),\n}\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)","ce6ce66f":"cvres = rnd_search.cv_results_\n\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","7794fe30":"feature_importances = grid_search.best_estimator_.feature_importances_","e9dde298":"feature_importances","6e869593":"extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","3daca674":"final_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis =1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","229b0a5c":"final_predictions","c7bda703":"y_test","d6eeb644":"from scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                         loc=squared_errors.mean(),\n                         scale=stats.sem(squared_errors)))","ebec136e":"m = len(squared_errors)\nmean = squared_errors.mean()\ntscore = stats.t.ppf((1 + confidence) \/ 2, df=m - 1)\ntmargin = tscore * squared_errors.std(ddof=1) \/ np.sqrt(m)\nnp.sqrt(mean - tmargin), np.sqrt(mean + tmargin)","42a4f6bf":"zscore = stats.norm.ppf((1 + confidence) \/ 2)\nzmargin = zscore * squared_errors.std(ddof=1) \/ np.sqrt(m)\nnp.sqrt(mean - zmargin), np.sqrt(mean + zmargin)","7a90a1f5":"We will now show these importance scores next to their corresponding attribute names","d33b486d":"we applied imputer to all numeric attribues since new data can have missing values for other attributes too","744ed636":"You also need to apply the LabelBinarizer on the categorical values. You can join all these transformations into a single pipeline by scikit-learn's ColumnTransformer class.","6408182e":"Alternatively, we could use a z-scores rather than t-scores:","dc27b7e1":"From the output above, we can see that rooms per household has a stronger correlation to house value than total rooms. It shows the relevant house feature's correlation with housing price.","34967348":"The above plot shows color according to the intensity of data points","853f0fa4":"# Handling Text and categorical values\nconvert text or categorical values to numbers to work with them","dfbc7cd0":"Checking the Distribution of Data","296d79b7":"From the above values we can see that the median_income has the highest linear correlation with the media_house_value. The correlation close to zero means that there's no linear correlation with that attribute.","b73a8612":"we can apply text categories to integer categories in vice-versa in one shot using LabelBinarizer class","c6c64252":"check if the encoder has learned the mapping using the classes_ attribute","5b64a1b3":"We'll now create some useful attributes from some of the attributes. This way we can feed our ML algortihm some good data that can actually help rather than some useless attributes.","28b5af97":"LabelBinarizer() returns dense Numpy array by default. To get a sparse matrix pass sparse_output=True to the LabelBinarizer constructor.","e27bb527":"Let's try it on some other instances from the training set","0ecee096":"reshape method allows one dimension to be -1, which means unspecified The one-hot encoder returns a sparse matrix. If you wanna convert it to a dense matrix, just call the toarray() method.","3a7bef4d":"The radius of the circle indicates the population and the color represents the median_house_value","e9aa0a37":"Scikit has a Class to take care of missing values named SimpleImputer which comes in handy","1b4cb14c":"# Preparing the Data","ab432c52":"\nWe can conclude that the decision tree is highly overfitting the data to the extent that it has performed worse in comparison with the linear regression model used earlier. So, let's now give another try with the random forest model.","e231e9a8":"# Custom Transformers\nAlthough Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom cleanup operations or combining specific attributes. You will want your transformer to work seamlessly with Scikit-Learn func\u2010 tionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher\u2010 itance), all you need is to create a class and implement three methods: fit() (returning self), transform(), and fit_transform(). You can get the last one for free by simply adding TransformerMixin as a base class. Also, if you add BaseEstima tor as a base class (and avoid *args and kargs in your constructor) you will get two extra methods (get_params() and set_params()) that will be useful for auto\u2010 matic hyperparameter tuning. For example, here is a small transformer class that adds the combined attributes we discussed earlier:","c9774a06":"For your Reference: https:\/\/github.com\/ageron\/handson-ml2\n\nBook reference: https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1491962291\n\n# **Setup & Getting The Data**","b5738d7e":"# Final Model","841b8324":"\nSince most median housing values range between 120k and 265k, so a typical prediction of 68,628 USD is not very satisfying. So, we can conclude that our model is underfitting the data.\n\nThis can mean that the features do not provide enough info or the model is not powerful enough. So, we have to select a more powerful model, to feed the algo with better features, or to reduce the constraints on the model. And also, this model is not regularized.\n\n# Trying a Decision Tree Regressor Model","54329d61":"discard non-numeric values since median only calculates on numeric values","18e749de":"calculating the scores for the linear regression model just to be sure","dc66f803":"scikit-learn has a method named train_test_split for such type of splitting","213c1638":"Load the dataset","548b2666":"# Select and Train a Model\n1. We have framed the problem\n2. Got the data and explored it\n3. sampled a training set and a test set\n4. wrote transformation pipelines to clean up and prepare the data for ML algos automatically\nSo, we are now ready to select and train our ML model.\n\n# Training and evaluating on the training set\nLet's try the Linear regression model first.","24887df3":"Checking the train set to understand the data","4f142c1b":"Checking the categories of Non-categorical Data","d3ca5cf4":"0 percent error could indicate towards overfitting. So, we should now try to validate the model on a part of the training set.\n\n# Better Evaluation using Cross-Validation\nusing scikit learn's cross_val_score","fea4afb9":"# Data Cleaning","168ae0d6":"But this representation assumes nearby values to be more similar than distant values. To fix this issue, we will use a binary attribute per category. For example: if one attribute is equal to one, then that category will be <1H OCEAN (0 otherwise). This is called one-hot encoding. Let's use it.\n\nWe need to reshape housing_cat_encoded because fit_transform() expects a 2D array. So, let's do that first.","3271aa84":"# Discover & Visualize Data to gain insights","67ef5761":"The diagonals from left to right are the histogram of correlations of the attributes with themselves so that we don't get into any confusion and just ignore them. The previous notion for the correlations still hold true from this plot.","3c5dcf43":"The above indicates the best hyperparameter combo found","c1cd5157":"\n# **Feature Scaling**\nFeature scaling means having all the attributes to have the same scale except the target values. ML algos don't perform well when the input numerical attributes have different scales. That's why we use feature scaling. The two common ways of feature scaling are min-max scaling and standardization.\n\nMin-max scaling (many people call this normalization) is quite simple: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max minus the min. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if you don\u2019t want 0\u20131 for some reason.\n\nStandardization is quite different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the variance so that the resulting distribution has unit variance. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standardization is much less affected by outliers. For example, suppose a district had a median income equal to 100 (by mistake). Min-max scaling would then crush all the other values from 0\u201315 down to 0\u20130.15, whereas standardization would not be much affected. Scikit-Learn provides a transformer called StandardScaler for standardization.\n\n# **Transformation Pipelines**\nThere are many steps that need to be executed in the right order for data transformation. The Pipeline class of scikit-learn helps with the secquences of the transformation.","fb302939":"# trying Random Forest Model","5666b28f":"There's a method for stratified sampling in scikit-learn which does sampling from the different categories which we'll use","8b1d6802":"We'll need to know the correlation of between the promising variables to understand the data.","c35df4bb":"The predictions are not great as you can see. Also, the second prediction is off by almost 50%","b8e4c3b8":"We could compute the interval manually like this:","8f069d7e":"# Fine Tuning the model\n**Grid Search**\n\nGrid search is scikit learn's method for trying all possible combinations of the hyperparameters you want to experiment with, and what values to try it with. It will do it using cross-validation.","5e1d7051":"We can compute a 95% confidence interval for the test RMSE:","cf47ddad":"# Splitting the data","c64fc669":"# Randomized Search\nThe grid search approach is fine for relatively few combos but when the hyperparameter search space is large, RandomizedSearchCV is often preferable","1cd910f6":"# Trying the Support Vector Machine Regressor"}}