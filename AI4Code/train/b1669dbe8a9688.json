{"cell_type":{"2426a9f4":"code","9b3c8cba":"code","4cee10e9":"code","9f61da98":"code","9c72b611":"code","7d4d33a2":"code","9b683ede":"code","35966122":"code","3861fbfc":"code","03471296":"code","bd410344":"code","542c9816":"code","647aee2f":"code","3934208b":"code","0ee7d30d":"code","2c1a00d2":"code","fb9694e3":"code","ecf352f5":"code","ecda35f8":"code","7d09602f":"code","a5f1c20b":"code","881ce2ee":"markdown","793d1f90":"markdown","dc40adce":"markdown","6f4cf4da":"markdown","71342021":"markdown","182afb98":"markdown","8f48b835":"markdown","eeef682b":"markdown","e84b2e5c":"markdown","64967e4f":"markdown","00efb0ce":"markdown","d160970b":"markdown","74a619eb":"markdown","40444656":"markdown","409ba774":"markdown","fce02a7a":"markdown","0df905c5":"markdown","dccd13cb":"markdown","d0e720b9":"markdown","948e8d3e":"markdown","b1eade0d":"markdown","35c679b9":"markdown","f70a0412":"markdown","33f433c1":"markdown","96ad6a27":"markdown","264b6c2d":"markdown","3f8f9ac1":"markdown","9b655e8e":"markdown","ed59ee2a":"markdown","7f6cd494":"markdown","a64938ac":"markdown","104d94cf":"markdown","a6de4af3":"markdown","e4c695a7":"markdown","3bc43c12":"markdown","66732b26":"markdown","925d2fde":"markdown"},"source":{"2426a9f4":"from __future__ import print_function\n\nfrom collections import defaultdict, deque\nimport datetime\nimport pickle\nimport time\nimport torch.distributed as dist\nimport errno\nfrom fastai import metrics\n\nimport cv2\nimport collections\nimport os\nimport numpy as np\n\nimport torch.utils.data\nfrom PIL import Image, ImageFile\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nfrom torchvision import transforms\nimport torchvision\nimport random\nfrom torch.utils.data import DataLoader, Dataset, sampler\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport cv2\nimport pdb\nimport time\nimport warnings\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader, Dataset, sampler\nfrom matplotlib import pyplot as plt\nfrom albumentations import (HorizontalFlip,VerticalFlip,Cutout,SmallestMaxSize,\n                            ToGray, ShiftScaleRotate, Blur,Normalize, Resize, Compose, GaussNoise)\nfrom albumentations.pytorch import ToTensor\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nfrom tqdm import tqdm_notebook\nimport cv2\nfrom PIL import Image\n\nfrom torchvision import models\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.utils.data as utils\n\n","9b3c8cba":"import platform\nprint(f'Python version: {platform.python_version()}')\nprint(f'PyTorch version: {torch.__version__}')","4cee10e9":"def seed_everything(seed=43):\n    '''\n      Make PyTorch deterministic.\n    '''    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    torch.backends.cudnn.deterministic = True\n\nseed_everything()\n\nIS_DEBUG = False","9f61da98":"def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n\n    def f(x):\n        if x >= warmup_iters:\n            return 1\n        alpha = float(x) \/ warmup_iters\n        return warmup_factor * (1 - alpha) + alpha\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)","9c72b611":"class DiceLoss(torch.nn.Module):\n    def __init__(self):\n        super(DiceLoss, self).__init__()\n \n    def forward(self, logits, targets):\n        ''' fastai.metrics.dice uses argmax() which is not differentiable, so it \n          can NOT be used in training, however it can be used in prediction.\n          see https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/metrics.py#L53\n        '''\n        N = targets.size(0)\n        preds = torch.sigmoid(logits)\n        #preds = logits.argmax(dim=1) # do NOT use argmax in training, because it is NOT differentiable\n        # https:\/\/github.com\/tensorflow\/tensorflow\/blob\/r1.12\/tensorflow\/python\/keras\/backend.py#L96\n        EPSILON = 1e-7\n \n        preds_flat = preds.view(N, -1)\n        targets_flat = targets.view(N, -1)\n \n        intersection = (preds_flat * targets_flat).sum()#.float()\n        union = (preds_flat + targets_flat).sum()#.float()\n        \n        loss = (2.0 * intersection + EPSILON) \/ (union + EPSILON)\n        loss = 1 - loss \/ N\n        return loss","7d4d33a2":"\n\ndef train_one_epoch(model, optimizer, data_loader, device, epoch):\n    model.train()\n    loss_func = DiceLoss() #nn.BCEWithLogitsLoss() \n\n    lr_scheduler = None\n    if epoch == 0:\n        warmup_factor = 1. \/ 1000\n        warmup_iters = min(1000, len(data_loader) - 1)\n\n        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n\n    lossf=None\n    inner_tq = tqdm(data_loader, total=len(data_loader), leave=False, desc= f'Iteration {epoch}')\n    for images, masks in inner_tq:\n        y_preds = model(images.to(device))\n        y_preds = y_preds['out'][:, 1, :, :] #\n\n        loss = loss_func(y_preds, masks.to(device))\n\n        if torch.cuda.device_count() > 1:\n            loss = loss.mean() # mean() to average on multi-gpu.\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        if lossf:\n            lossf = 0.98*lossf+0.02*loss.item()\n        else:\n            lossf = loss.item()\n        inner_tq.set_postfix(loss = lossf)","9b683ede":"def rle2mask(rle, imgshape):\n    width = imgshape[0]\n    height= imgshape[1]\n    \n    mask= np.zeros( width*height ).astype(np.uint8)\n    \n    array = np.asarray([int(x) for x in rle.split()])\n    starts = array[0::2]\n    lengths = array[1::2]\n\n    current_position = 0\n    for index, start in enumerate(starts):\n        mask[int(start):int(start+lengths[index])] = 1\n        current_position += lengths[index]\n        \n    return np.flipud( np.rot90( mask.reshape(height, width), k=1 ) )","35966122":"print(os.listdir('..\/input\/severstal-steel-defect-detection\/'))","3861fbfc":"\npath = '..\/input\/severstal-steel-defect-detection\/'\n","03471296":"tr = pd.read_csv(path + 'train.csv')\nprint(len(tr))\ntr.head()","bd410344":"df_train = tr[tr['EncodedPixels'].notnull()].reset_index(drop=True)\n\n#df_train1 = df_train[df_train['ImageId_ClassId'].apply(lambda x: x.split('_')[1] == '1')].reset_index(drop=True)\n#df_train2 = df_train[df_train['ImageId_ClassId'].apply(lambda x: x.split('_')[1] == '2')].reset_index(drop=True)\n#df_train3 = df_train[df_train['ImageId_ClassId'].apply(lambda x: x.split('_')[1] == '3')].reset_index(drop=True)\n#df_train4 = df_train[df_train['ImageId_ClassId'].apply(lambda x: x.split('_')[1] == '4')].reset_index(drop=True)\n\n#df_train = tr[tr['EncodedPixels']].reset_index(drop=True)\n#df_train = tr\nprint(len(df_train))\ndf_train.head()","542c9816":"df_train","647aee2f":"class ImageData(Dataset):\n    def __init__(self, df, transform, subset=\"train\"):\n        super().__init__()\n        self.df = df\n        self.transform = transform\n        self.subset = subset\n        \n        if self.subset == \"train\":\n            self.data_path = path + 'train_images\/'\n        elif self.subset == \"test\":\n            self.data_path = path + 'test_images\/'\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):                      \n        fn = self.df['ImageId_ClassId'].iloc[index].split('_')[0]         \n        img = Image.open(self.data_path + fn)\n        img = self.transform(img)\n\n        if self.subset == 'train': \n            mask = rle2mask(self.df['EncodedPixels'].iloc[index], (256, 1600))\n            mask = transforms.ToPILImage()(mask)            \n            mask = self.transform(mask)\n            return img, mask\n        else: \n            mask = None\n            return img  ","3934208b":"data_transf = transforms.Compose([\n                                  transforms.Scale((256, 1600)),\n                                  #HorizontalFlip(p=0.5),\n                                  #VerticalFlip(p = 0.5),\n                                  #Blur(),\n                                  #Cutout(),\n                                  #ShiftScaleRotate(),\n                                  #GaussNoise(),\n                                  #ToGray(),\n                                  transforms.ToTensor()])\ntrain_data = ImageData(df = df_train, transform = data_transf)","0ee7d30d":"model_ft = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=False, num_classes=4)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_ft.to(device)\nNUM_GPUS = torch.cuda.device_count()\nif NUM_GPUS > 1:\n    model_ft = torch.nn.DataParallel(model_ft)\n_ = model_ft.to(device)","2c1a00d2":"data_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=4, shuffle=True, num_workers=NUM_GPUS,drop_last=True\n)","fb9694e3":"# construct an optimizer\nparams = [p for p in model_ft.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.0001, momentum=0.9, weight_decay=0.0005)","ecf352f5":"# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=5,\n                                               gamma=0.1)","ecda35f8":"num_epochs = 2\nfor epoch in range(num_epochs):\n    train_one_epoch(model_ft, optimizer, data_loader, device, epoch)\n    lr_scheduler.step()","7d09602f":"\nfor param in model_ft.parameters():\n    param.requires_grad = False\nmodel_ft.to(torch.device('cuda'))\n#assert model_ft.training == False\n\nmodel_ft.eval()","a5f1c20b":"torch.save(model_ft.state_dict(), 'deeplabv3Resnet101.pth')\ntorch.cuda.empty_cache()","881ce2ee":"**Note : i am completely new in deep learning world and has very little knowledge about this field,if you find any implementation error or bug please let me know in the comment box and it will be highly appreciated**","793d1f90":"**Loss Function**","dc40adce":"![](https:\/\/i0.wp.com\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2019\/01\/semantic_8.png?resize=649%2C333&ssl=1)","6f4cf4da":"**The following shows the effects of three semantic segmentation models in the ADE20K dataset, where ADE20K is a scene resolution dataset published by MIT that contains a variety of scenarios, including people, backgrounds, and objects.**\n\n![](http:\/\/www.programmersought.com\/images\/928\/522173f7eab9d4d27b9bbdd0b833fde8.png)","71342021":"# References \n* [mask-rcnn with augmentation and multiple masks](https:\/\/www.kaggle.com\/abhishek\/mask-rcnn-with-augmentation-and-multiple-masks)\n\n* [SIIM-DeepLabV3](https:\/\/www.kaggle.com\/soulmachine\/siim-deeplabv3)\n\n* [PyTorch Starter (U-Net ResNet)](https:\/\/www.kaggle.com\/ateplyuk\/pytorch-starter-u-net-resnet)","182afb98":"# Why using DEEPLABV3-RESNET101?","8f48b835":" equation of DilatedNet:\n \n![](https:\/\/miro.medium.com\/proxy\/1*mlHFvK6H_wMCyURSZNZWGQ.png) \n\n\nThe left one is the standard convolution. The right one is the dilated convolution. We can see that at the summation, it is s+lt=p that we will skip some points during convolution.","eeef682b":"**Mask Decoder**","e84b2e5c":"# Dilated Convolution","64967e4f":"*For the semantic segmentation model, GluonCV-Torch mainly supports pre-trained FCN, PSPNet and DeepLab-V3. DeepLab-V3 is a very common open source model, which has very good effect on semantic segmentation tasks. The pre-training effects of these three models in the Pascal VOC dataset are shown below, where Pascal VOC contains 20 categories of images:*\n\n![](http:\/\/www.programmersought.com\/images\/427\/d258324e597cccec479b706fa7a2a04b.png)","00efb0ce":"# Understanding the DeepLab Model Architecture\n\nDeepLab V3 uses ImageNet\u2019s pretrained Resnet-101 with atrous convolutions as its main feature extractor. In the modified ResNet model, the last ResNet block uses atrous convolutions with different dilation rates. It uses Atrous Spatial Pyramid Pooling and bilinear upsampling for the decoder module on top of the modified ResNet block.\n\nDeepLab V3+ uses Aligned Xception as its main feature extractor, with the following modifications:\n\n1. All max pooling operations are replaced by depthwise separable convolution with striding\n2. Extra batch normalization and ReLU activation are added after each 3 x 3 depthwise convolution\n3. Depth of the model is increased without changing the entry flow network structure","d160970b":"![](https:\/\/miro.medium.com\/proxy\/1*tnDNIyPePgHvb8JIx8SbqA.png)\n\n**l=1 (left), l=2 (Middle), l=4 (Right)**","74a619eb":"# Reasons of Dilated Convolution?","40444656":" The above figure shows more examples about the receptive field.","409ba774":"**Function for training**","fce02a7a":"**Steel DataLoader**","0df905c5":"1. It is found that with small output feature map obtained at the end of the network, the accuracy is reduced in semantic segmentation.\n\n2. In FCN, it also shows that when 32\u00d7 upsampling is needed, we can only get a very rough segmentation results. Thus, a larger output feature map is desired.\n\n3. A naive approach is to simply remove subsampling (striding) steps in the network in order to increase the resolution of feature map. However, this also reduces the receptive field which severely reduces the amount of context. such reduction in receptive field is an unacceptable price to pay for higher resolution.\n\n4. For this reason, dilated convolutions are used to increase the receptive field of the higher layers, compensating for the reduction in receptive field induced by removing subsampling.\n\n5. And it is found that using dilated convolution can also help for image classification task in this paper.\n","dccd13cb":"**[DeepLabV3 model with a ResNet-101 backbone](https:\/\/pytorch.org\/hub\/pytorch_vision_deeplabv3_resnet101\/)**","d0e720b9":"I  highly recommend you to check this link : [Semantic Segmentation: Introduction to the Deep Learning Technique Behind Google Pixel\u2019s Camera!](https:\/\/www.analyticsvidhya.com\/blog\/2019\/02\/tutorial-semantic-segmentation-google-deeplab\/)\ni have used some of the contents from that link,definitely a great resource for understanding deeplabv3 well","948e8d3e":"SOURCES : \n1. https:\/\/towardsdatascience.com\/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5\n2. https:\/\/towardsdatascience.com\/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5","b1eade0d":"1. In the paper(mentioned below), it uses d as dilation factor.\n\n2. When d=1, it is standard convolution.\n\n3. When d>1, it is dilated convolution.","35c679b9":"#  Algorithms for Image segmentation","f70a0412":"<font size=\"6\" color=\"red\">Please UPVOTE if you find this kernel Helpful!<\/font>","33f433c1":"*  Source of informations above : http:\/\/www.programmersought.com\/article\/5710352893\/","96ad6a27":"1. When l=1, it is standard convolution\n\n2. When l>1, it is dilated convolution.","264b6c2d":"<font size=\"6\" color=\"green\">Please UPVOTE if you find this kernel Helpful!<\/font>","3f8f9ac1":"*  <font size=\"4\" color=\"green\">Google's Deeplabv3 uses dilated convolution,lets first talk little bit about dilated convolutions!<\/font>","9b655e8e":"**Steel Dataset paths**","ed59ee2a":"* The above illustrate an example of dilated convolution when l=2. We can see that the receptive field is larger compared with the standard one.","7f6cd494":"**IMPORTS**","a64938ac":"![](https:\/\/miro.medium.com\/max\/794\/1*-67TMJkhBO3sTtzAg2oUHg.png)","104d94cf":"**In my earlier discussion post titled [DRN \u2014 Dilated Residual Networks (Image Classification & Semantic Segmentation)](https:\/\/www.kaggle.com\/c\/severstal-steel-defect-detection\/discussion\/111546) ** a kaggler name Qishen Ha (@haqishen) said \"If you want to try out dilated convolution, Deeplabv3 is a good sample to learn\" :) so i decided to make this kernel for my fellow kaggle mates and also self teaching deeplabv3 a little bit :)","a6de4af3":"![](https:\/\/miro.medium.com\/max\/1185\/1*btockft7dtKyzwXqfq70_w.gif)\n                  **Standard Convolution (l=1) (Left) Dilated Convolution (l=2) (Right)**","e4c695a7":"Image segmentation is a long standing computer Vision problem. Quite a few algorithms have been designed to solve this task, such as the Watershed algorithm, Image thresholding , K-means clustering, Graph partitioning methods, etc.\n\nMany deep learning architectures (like fully connected networks for image segmentation) have also been proposed, but Google\u2019s DeepLab model has given the best results till date. That\u2019s why we\u2019ll focus on using DeepLab.DeepLab uses atrous convolution with rates 6, 12 and 18.","3bc43c12":"# Motivation","66732b26":"## Dilated Residual Networks (DRN)","925d2fde":"**Utility**"}}