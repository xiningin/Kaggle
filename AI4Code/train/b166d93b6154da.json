{"cell_type":{"e150f5c8":"code","5ecc07d8":"code","056a8877":"code","699c5780":"code","284411e4":"code","ed02411d":"code","84ad3106":"code","5056db59":"code","7953870f":"code","7131d81e":"code","e945b66e":"code","6c7b6fc8":"code","06c3c2bb":"markdown","a20b6bb2":"markdown","39d912a2":"markdown","472c96ec":"markdown","8e8ce279":"markdown","765703b5":"markdown","5c2fbb29":"markdown","cb823298":"markdown","0ad67a41":"markdown","ce9c392e":"markdown","69cfc97d":"markdown","fcc6708b":"markdown"},"source":{"e150f5c8":"import os\nimport numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\nimport seaborn as sns\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nos.environ[\"KMP_SETTINGS\"] = \"false\"","5ecc07d8":"#Plotting ROC-curves for 2 classes (binary classification):\ndef plot_roc(pred1,y_cat):\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(1):\n        fpr[i], tpr[i], _ = roc_curve(y_cat[:, i], pred1[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    for i in range(1):\n        plt.figure()\n        plt.plot(fpr[i], tpr[i], label='ROC-curve (area = %0.2f)' % roc_auc[i])\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title(f'ROC-curve for val_split predictions')\n        plt.legend(loc=\"lower right\")\n        plt.show()\n\n#Plotting confusion matrix\ndef plot_confusion_matrix(cm, names):\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cells\n    # labels, title and ticks\n    ax.set_xlabel('Predicted values', fontsize=10)\n    ax.xaxis.set_label_position('bottom')\n    plt.xticks(rotation=0)\n    ax.xaxis.set_ticklabels(names, fontsize = 10)\n    ax.xaxis.tick_bottom()\n    ax.set_ylabel('True values', fontsize=10)\n    ax.yaxis.set_ticklabels(names, fontsize = 10)\n    plt.yticks(rotation=0)\n    plt.title('confusion matrix', fontsize=20)\n    plt.tight_layout()\n    plt.show()\n","056a8877":"train_file_path = \"..\/input\/titanic\/train.csv\"\n\nX = pd.read_csv(train_file_path)\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"Survived\"]\nX = pd.get_dummies(X[features])\nX = pd.concat([X.drop('Pclass', axis=1), pd.get_dummies(X['Pclass'], prefix=\"Pclass\")], axis=1)\nimputer = SimpleImputer()\nimputed_X = pd.DataFrame(imputer.fit_transform(X))\nimputed_X.columns = X.columns\nX = imputed_X\nlearnoutput = X.Survived\nlearninput = X.drop(\"Survived\", axis=1)\n\ntest_file_path = \"..\/input\/titanic\/test.csv\"\ntest_data = pd.read_csv(test_file_path)\nfeatures = [\"Pclass\", \"Sex\", \"Age\"]\ntest = pd.get_dummies(test_data[features])\ntestinput = pd.concat([test.drop('Pclass', axis=1), pd.get_dummies(test['Pclass'], prefix=\"Pclass\")], axis=1)","699c5780":"x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(learninput, learnoutput, random_state=0)","284411e4":"model = keras.Sequential([\n\n      layers.Dense(\n          units=128,\n          activation = \"tanh\",\n          input_shape = [6]\n      ),                        \n\n      layers.Dropout(0.2),\n\n      layers.Dense(\n          units = 256,\n          activation = \"tanh\",\n      ),\n      \n      layers.Dropout(0.2),\n\n      layers.Dense(\n          units = 512,\n          activation = \"tanh\",\n      ),\n\n      layers.Dropout(0.2),\n\n      layers.Dense(\n          units = 1024,\n          activation = \"tanh\",\n      ),\n\n      layers.Dropout(0.2),\n\n      layers.Dense(\n          units = 1,\n          activation = \"sigmoid\"\n      )\n  ])","ed02411d":"#myOpt=keras.optimizers.Adam(learning_rate=0.1)\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","84ad3106":"print(model.summary())","5056db59":"history = model.fit(x_train_split, y_train_split, epochs=25, verbose=0, validation_data=(x_val_split, y_val_split))\n","7953870f":"plt.plot(history.history['loss'], label='Train sample')\nplt.plot(history.history['val_loss'], label='Validation sample')\nplt.grid(True)\nplt.xlabel('Epochs')\nplt.ylabel('Loss Q')\nplt.title('Loss graph while training NN')\nplt.legend(loc=\"upper right\")\nplt.show()\n\nplt.plot(history.history['accuracy'], label='Train sample')\nplt.plot(history.history['val_accuracy'], label='Validation sample')\nplt.grid(True)\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title ('Accuracy graph while training NN')\nplt.legend(loc=\"lower right\")\nplt.show()","7131d81e":"model.evaluate(x_val_split, y_val_split)","e945b66e":"val_predictions = model.predict(x_val_split)\nval_predictions = np.around(val_predictions).astype('int64') \ny_val_split = np.asarray(y_val_split)\ny_val_split =np.around(y_val_split).astype('int64')\ny_val_split = np.reshape(y_val_split, (len(y_val_split), 1))\n\nscore = metrics.accuracy_score(y_val_split.T[0], val_predictions.T[0]) # Determination of the accuracy of the NN on the validation sample\nprint(\"The accuracy of the neural network: {}\".format(score))\n\nplot_roc(val_predictions, y_val_split)\n\ncm = confusion_matrix(y_val_split.T[0], val_predictions.T[0])\nproducts=['Dead','Alive']\nplot_confusion_matrix(cm, products)","6c7b6fc8":"predictions = model.predict(testinput) \npredictions = predictions.ravel()\npredictions = predictions.round()\npredictions = np.nan_to_num(predictions)\npredictions = predictions.astype(int)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, \"Survived\": predictions})\noutput.to_csv(\".\/file1.csv\", index=False)","06c3c2bb":"# Creating NN model step:\nI'v chosen to use Keras API to create NN model:\nI used this model only because I want to apply dropout to my NN and look how it works.","a20b6bb2":"# Functions for plotting ROC-curves and confusion matrix:\nFunctions configured on binary classification, but it can be multiclass classification.","39d912a2":"# Calculation of the accuracy of the neural network on a test sample:\nsaving results in .csv format","472c96ec":"# Plotting ROC-curves and confusion matrix for our trained model:\nSince the validation sample is not involved in training, we will check the quality of our model on it before submitting the test sample data to the input.","8e8ce279":"# So, let's try to split our train dataset on train_split and val_split:\nThis step is needed in order to:\n1. Control how the validation sample is distributed\n2. Use validation sampling to evaluate neural model training by evaluating metrics such as loss and accuracy, and constructing ROC-curve and confusion matrix\nrandom_state=0, for stability","765703b5":"# Analazing output results of NN\n1. Plotting the graph of NN's losses -> Loss value must decrease with the rise of epochs\n2. Plotting the graph of NN's accuracy -> Accuracy value must increase with the rise of epochs","5c2fbb29":"# Model's fitting:\nI use 25 learning epochs due to the structure of the neural network, this is the most optimal value from those that I picked\n","cb823298":"# Evaluating accuracy of NN by val_split sample","0ad67a41":"# Compiling model step:\nChoice of optimizer and its parameters, loss-function and metrics:","ce9c392e":"# Model summary:\nHere we can see structure of neural networ model, count of its parameters:","69cfc97d":"![](https:\/\/www.looperman.com\/media\/avatars\/lrg\/looperman-avatar-02989330.jpg)\n\n# Hello, my titanic try below:","fcc6708b":"# Reading input train and test datasets from Kaggle's competition:\n\nHere I extract the most informative columns from both of datasets.\n1. Get_dummies helps me to convert categorical variable into dummy\/indicator variables. I used it for \"Pclass\" and \"Sex\" columns.\n2. Also I used \"SimpleImputer\" to remove Nan-values from DF\n3. Singled out survivors in output of NN\n"}}