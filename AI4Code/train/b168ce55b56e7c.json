{"cell_type":{"6a9658d8":"code","e6ef434f":"code","2554e94e":"code","412efe07":"code","c1f69a4d":"code","8a8ca1bc":"code","66413280":"code","c5579342":"code","ebc5fed3":"code","5fd3396c":"code","aef491ec":"code","d332aff4":"code","ebbaf0f4":"code","7d186182":"code","285ee395":"code","b47c98a9":"code","7c7972b7":"code","cb403017":"code","1fdcdba3":"code","99ce5abc":"code","8873ef78":"code","cd6dc1ab":"code","dc1aabe4":"code","7ff6b9b7":"code","875b7311":"code","4f319d32":"code","c54268a6":"code","8a3f7dfd":"code","2282f684":"code","03205dac":"code","9317af10":"code","9af7ec49":"code","32317d50":"code","1dcc92db":"code","f7b187ec":"code","05d509a7":"code","aea12115":"code","e6bae251":"code","5b096203":"code","20983f51":"code","f99b515a":"code","6efd5d46":"code","20389187":"code","54684467":"code","3f3dc2d0":"code","bd025b27":"code","a48a49bf":"code","9eaa75ec":"code","e6d1c819":"code","822b0f79":"code","cb89caf0":"code","73970267":"markdown","c98dfdbf":"markdown","f1a51809":"markdown","4c2e2fd8":"markdown","5f9e85dd":"markdown","a3e180ac":"markdown","df3e24fc":"markdown","7cc95726":"markdown","9ff6a85c":"markdown","b7d4e1e9":"markdown","da1d833a":"markdown","a1a28996":"markdown","f651ef6b":"markdown","ebb7bbfd":"markdown","001a9a1a":"markdown","39f12189":"markdown","16af6453":"markdown","9dc263d8":"markdown","b0b579df":"markdown","c645837d":"markdown","89476a6a":"markdown","7c053ecd":"markdown","db857c70":"markdown"},"source":{"6a9658d8":"# Load the MNIST dataset\nimport numpy as np\n\ntry:\n    from sklearn.datasets import fetch_openml\n    mnist = fetch_openml('mnist_784', version=1)\n    mnist.target = mnist.target.astype(np.int64)\nexcept ImportError:\n    from sklearn.datasets import fetch_mldata\n    mnist = fetch_mldata('MNIST original')","e6ef434f":"# Split 50,000 instances for training, 10,000 for validation, and 10,000 for testing.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train_val, X_test, y_train_val, y_test = train_test_split( mnist.data, mnist.target, test_size=10000, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split( X_train_val, y_train_val, test_size=10000, random_state=42)","2554e94e":"# Train Random Forest classifier, Extra-Trees classifier, SVM and MLP\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neural_network import MLPClassifier\n\nrandom_forest_clf = RandomForestClassifier(n_estimators=10, random_state=42)\nextra_trees_clf = ExtraTreesClassifier(n_estimators=10, random_state=42)\nsvm_clf = LinearSVC(random_state=42)\nmlp_clf = MLPClassifier(random_state=42)\n\nestimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]\nfor estimator in estimators:\n    estimator.fit(X_train, y_train)","412efe07":"# .score() method directly calls sklearn.metrics.accuracy_score method.\n\n[estimator.score(X_val, y_val) for estimator in estimators]","c1f69a4d":"# Combine the classifiers into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier.\n\nfrom sklearn.ensemble import VotingClassifier\n\nnamed_estimators = [ (\"random_forest_clf\", random_forest_clf), (\"extra_trees_clf\", extra_trees_clf), (\"svm_clf\", svm_clf), (\"mlp_clf\", mlp_clf)]","8a8ca1bc":"voting_clf = VotingClassifier(named_estimators)","66413280":"voting_clf.fit(X_train, y_train)","c5579342":"voting_clf.score(X_val, y_val)","ebc5fed3":"[estimator.score(X_val, y_val) for estimator in voting_clf.estimators_]","5fd3396c":"# remove an estimator by setting it to None using set_params()\n\nvoting_clf.set_params(svm_clf=None)","aef491ec":"# Updated list of estimators\n\nvoting_clf.estimators","d332aff4":"# Updated list of trained estimators\n\nvoting_clf.estimators_","ebbaf0f4":"# We can either fit the VotingClassifier again, or just remove the SVM from the list of trained estimators:\n\ndel voting_clf.estimators_[2]","7d186182":"# Recheck\n\nvoting_clf.estimators_","285ee395":"# Evaluate the VotingClassifier again:\n\nvoting_clf.score(X_val, y_val)","b47c98a9":"# Set voting to \"soft\"\n\nvoting_clf.voting = \"soft\"\nvoting_clf.score(X_val, y_val)","7c7972b7":"# Test set\n\nvoting_clf.score(X_test, y_test)","cb403017":"[estimator.score(X_test, y_test) for estimator in voting_clf.estimators_]","1fdcdba3":"from sklearn.tree import DecisionTreeClassifier # Base Classifier for Bagging Method\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Train an ensemble of 500 Decision Tree classifiers.\n# BaggingClassifier automatically performs soft voting if the base classifier has a predict_proba() method.\n# To train each predictor on a random subset of the input features, use max_features(0-1). Useful if training set has high dimentional features.\n# \"n_estimators = 500\" was taking lot of CPU time, I used \"n_estimators = 10\" for demonstration purpose.\n# bootstrap = True (Bagging)\nclf_bagging = BaggingClassifier( DecisionTreeClassifier(), n_estimators = 10, max_samples = 0.8, bootstrap = True, oob_score=True, n_jobs = -1)\n# bootstrap = False (Pasting)\nclf_pasting = BaggingClassifier( DecisionTreeClassifier(), n_estimators = 10, max_samples = 0.8, bootstrap = False, n_jobs = -1)\n\nclf_bagging.fit(X_train, y_train)\nclf_pasting.fit(X_train, y_train)\n\ny_pred_bagging = clf_bagging.predict(X_val)\ny_pred_pasting = clf_bagging.predict(X_val)\n\n\nclf_bagging.oob_score_, accuracy_score(y_val, y_pred_bagging), accuracy_score(y_val, y_pred_pasting)","99ce5abc":"# The decision function returns the class probabilities (if base estimator has a predict_proba() method) for each training instance. \nclf_bagging.oob_decision_function_[112]\n#y_train[112]","8873ef78":"# import matplotlib as mpl\n\n# some_digit = X_train[112]\n# some_digit_image = some_digit.reshape(28, 28)\n# plt.imshow(some_digit_image, cmap = mpl.cm.binary, interpolation=\"nearest\")\n# plt.axis(\"off\")\n\n# plt.show()","cd6dc1ab":"#  Trains a Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using all available CPU cores:\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf_randomforest = RandomForestClassifier(n_estimators=10, n_jobs=-1)\nclf_randomforest.fit(X_train, y_train)\ny_pred_rf = clf_randomforest.predict(X_val)\n\naccuracy_score(y_val, y_pred_rf)","dc1aabe4":"from sklearn.datasets import load_iris\niris = load_iris()\nrnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\nrnd_clf.fit(iris[\"data\"], iris[\"target\"])\nfor name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n    print(name, score)","7ff6b9b7":"# Train a classifier based on 200 Decision Trees \n\nfrom sklearn.ensemble import AdaBoostClassifier\nada_clf = AdaBoostClassifier( DecisionTreeClassifier(max_depth=1), n_estimators=200, algorithm=\"SAMME.R\", learning_rate=0.5)\nada_clf.fit(X_train, y_train)\ny_pred_ada = ada_clf.predict(X_val)\naccuracy_score(y_val, y_pred_ada)","875b7311":"import numpy as np\n\nnp.random.seed(42)\nX = np.random.rand(100, 1) - 0.5\ny = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)","4f319d32":"from sklearn.tree import DecisionTreeRegressor\ntree_reg1 = DecisionTreeRegressor(max_depth=2)\ntree_reg1.fit(X, y)","c54268a6":"y2 = y - tree_reg1.predict(X)\ntree_reg2 = DecisionTreeRegressor(max_depth=2)\ntree_reg2.fit(X, y2)","8a3f7dfd":"y3 = y2 - tree_reg2.predict(X)\ntree_reg3 = DecisionTreeRegressor(max_depth=2)\ntree_reg3.fit(X, y3)","2282f684":"\nX_new = np.array([[0.8]])\n\ny_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\ny_pred","03205dac":"from sklearn.ensemble import GradientBoostingRegressor\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\ngbrt.fit(X, y)","9317af10":"gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42)\ngbrt_slow.fit(X, y)","9af7ec49":"import matplotlib.pyplot as plt\n\n\ndef plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n    x1 = np.linspace(axes[0], axes[1], 500)\n    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n    plt.plot(X[:, 0], y, data_style, label=data_label)\n    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n    if label or data_label:\n        plt.legend(loc=\"upper center\", fontsize=16)\n    plt.axis(axes)\n\nplt.figure(figsize=(11,11))","32317d50":"\nplt.figure(figsize=(11,4))\n\nplt.subplot(121)\nplot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"Ensemble predictions\")\nplt.title(\"learning_rate={}, n_estimators={}\".format(gbrt.learning_rate, gbrt.n_estimators), fontsize=14)\n\nplt.subplot(122)\nplot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\nplt.title(\"learning_rate={}, n_estimators={}\".format(gbrt_slow.learning_rate, gbrt_slow.n_estimators), fontsize=14)\n\n#save_fig(\"gbrt_learning_rate_plot\")\nplt.show()","1dcc92db":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nX_train, X_val, y_train, y_val = train_test_split(X, y)\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\ngbrt.fit(X_train, y_train)\nerrors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\nbst_n_estimators = np.argmin(errors)\ngbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\ngbrt_best.fit(X_train, y_train)","f7b187ec":"min_error = np.min(errors)","05d509a7":"plt.figure(figsize=(11, 4))\n\nplt.subplot(121)\nplt.plot(errors, \"b.-\")\nplt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\")\nplt.plot([0, 120], [min_error, min_error], \"k--\")\nplt.plot(bst_n_estimators, min_error, \"ko\")\nplt.text(bst_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=14)\nplt.axis([0, 120, 0, 0.01])\nplt.xlabel(\"Number of trees\")\nplt.title(\"Validation error\", fontsize=14)\n\nplt.subplot(122)\nplot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\nplt.title(\"Best model (%d trees)\" % bst_n_estimators, fontsize=14)\n\nplt.show()","aea12115":"gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n\nmin_val_error = float(\"inf\")\nerror_going_up = 0\nfor n_estimators in range(1, 120):\n    gbrt.n_estimators = n_estimators\n    gbrt.fit(X_train, y_train)\n    y_pred = gbrt.predict(X_val)\n    val_error = mean_squared_error(y_val, y_pred)\n    if val_error < min_val_error:\n        min_val_error = val_error\n        error_going_up = 0\n    else:\n        error_going_up += 1\n        if error_going_up == 5:\n            break  # early stopping","e6bae251":"print(gbrt.n_estimators)","5b096203":"print(\"Minimum validation MSE:\", min_val_error)","20983f51":"from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier","f99b515a":"X_train_val, X_test, y_train_val, y_test = train_test_split( mnist.data, mnist.target, test_size=10000, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split( X_train_val, y_train_val, test_size=10000, random_state=42)","6efd5d46":"random_forest_clf = RandomForestClassifier(n_estimators=10, random_state=42)\nextra_trees_clf = ExtraTreesClassifier(n_estimators=10, random_state=42)\nmlp_clf = MLPClassifier(random_state=42)","20389187":"estimators = [random_forest_clf, extra_trees_clf, mlp_clf]\nfor estimator in estimators:\n    print(\"Training the\", estimator)\n    estimator.fit(X_train, y_train)","54684467":"[estimator.score(X_val, y_val) for estimator in estimators]","3f3dc2d0":"X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)\n\nfor index, estimator in enumerate(estimators):\n    X_val_predictions[:, index] = estimator.predict(X_val)","bd025b27":"X_val_predictions","a48a49bf":"rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\nrnd_forest_blender.fit(X_val_predictions, y_val)","9eaa75ec":"rnd_forest_blender.oob_score_","e6d1c819":"X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32)\n\nfor index, estimator in enumerate(estimators):\n    X_test_predictions[:, index] = estimator.predict(X_test)","822b0f79":"y_pred = rnd_forest_blender.predict(X_test_predictions)","cb89caf0":"accuracy_score(y_test, y_pred)","73970267":"Then we train a third regressor on the residual errors made by the second predictor:","c98dfdbf":"We can fine-tune this blender or try other types of blenders (e.g., an MLPClassifier), then select the best one using cross-validation, as always.\n\nWe have trained a blender, and together with the classifiers they form a stacking ensemble.\n\nLet's evaluate the ensemble on the test set. For each image in the test set, make predictions with all our classifiers, then feed the predictions to the blender to get the ensemble's predictions.","f1a51809":"The learning_rate hyperparameter scales the contribution of each tree. If we set it to a low value(0.1) we will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This is a regularization technique called shrinkage.\n\nIf Gradient Boosting ensemble overfits the training set, we should try decreasing the learning rate. We could also use early stopping to find the right number of predictors.","4c2e2fd8":"### <u> AdaBoost <\/u>\n\nThe predictors(classifier\/ regressor) fit the training set in sequence. The next predictor corrects its predecessor by paying more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases.\n\nTo build an AdaBoost classifier, each instance's weight is set to an initial value. A base classifier (eg. Decision Tree) is trained and makes predictions on the training set. The relative weight of misclassified training instances is then increased. The second classifier is trained on the training set using the updated weights and again it makes predictions on the training set and update the weights. The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found.\n\n![Screen%20Shot%202019-11-17%20at%2023.41.39.png](attachment:Screen%20Shot%202019-11-17%20at%2023.41.39.png)\n\nThis sequential learning technique is similar to Gradient Descent, except that instead of tweaking a single predictor\u2019s parameters to minimize a cost function, AdaBoost adds more predictors to the ensemble, gradually making it better.\n\n<b><u> Algorithm <\/u><\/b>:\n\n- Each instance weight $w^{(i)}$ is initially set to $\\frac{1}{m}$ (m = number of instances) . The first predictor is trained and its weighted error rate $r_1$ is computed on the training set:\n\n$$\\text{ Weighted error rate for $j^{th}$ predictor:   } r_j = \\frac{\\hat{y_j}^i \\neq y^i}{\\sum_{i=1}^{m} w^i} $$\n\nwhere ${y_j}^i$ is the $j^{th}$ predictor\u2019s prediction for the $i^{th}$ instance.\n\n- The predictor\u2019s weight $\\alpha_j$ is then computed using its weighted error rate , where $\\eta$ is the learning rate hyperparameter (defaults to 1).\n  - Predictor guessing accurately - Higher weight\n  - Predictor guessing randomly - Weight close to 0\n  - Predictor guessing mostly wrong - Negative weights\n\n$$\\text{ Predictor weight:   } \\alpha_j = \\eta \\log\\frac{1 - r_j}{r_j} $$\n\n- The instance weights are updated using above equation and the misclassified instance' weights are boosted.\n\n$$\\text{ Weight update rule:   } w^{i}  \\leftarrow \n  \\begin{cases}\n    w^i       & \\quad \\text{if }  \\hat{y_j}^i = y^i\\\\\n    w^i \\exp(\\alpha_j)  & \\quad \\text{if } \\hat{y_j}^i \\neq y^i\n  \\end{cases}\n$$\n\n- All the instance weights are normalized (divided by $\\sum_{i=1}^{m} w^i$).\n\nA new predictor is trained using the updated weights, and the whole process is repeated (the new predictor\u2019s weight is computed, the instance weights are updated, then another predictor is trained. \n\nTo make predictions, AdaBoost simply computes the predictions of all the predictors and weighs them using their predictor weights $\\alpha_j$. The predicted class is the one that receives the majority of weighted votes.","5f9e85dd":"If AdaBoost ensemble underfits the training data, you can try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator. You may also try slightly increasing the learning rate.","a3e180ac":"Now we have an ensemble containing three trees. It can make predictions on a new instance simply by adding up the predictions of all the trees:","df3e24fc":"## <u>Voting Classifiers<\/u> :\nTrain a few classifiers (Logistic Regression classifier, SVM classifier, Random Forest classifier, K-Nearest Neighbors classifier etc.) on the training set.\n\n![Screen%20Shot%202019-11-19%20at%2010.47.24.png](attachment:Screen%20Shot%202019-11-19%20at%2010.47.24.png)","7cc95726":"Scikit-Learn uses a multiclass version of AdaBoost - SAMME. When there are just two classes, SAMME is equivalent to AdaBoost. If the predictors can estimate class probabilities (if they have a predict_proba() method), Scikit-Learn can use a variant of SAMME called SAMME.R, which relies on class probabilities rather than predictions and generally performs better.\n\nThe following code using Scikit-Learn\u2019s AdaBoostClassifier class (as you might expect, there is also an Ada BoostRegressor class). A Decision Stump is a Decision Tree with max_depth=1\u2014in other words, a tree composed of a single decision node plus two leaf nodes. This is the default base estimator for the AdaBoostClassifier class:","9ff6a85c":"## <u>Boosting<\/u>\nBoosting refers to a family of algorithms that are able to convert weak learners to strong learners. The main principle of boosting is to fit a sequence of weak learners\u2212 models that are only slightly better than random guessing, such as small decision trees to weighted versions of the data. More weight is given to examples that were misclassified by earlier rounds.\n\nThe predictions are then combined through a weighted majority vote (classification) or a weighted sum (regression) to produce the final prediction. \n\nBoosting technique cannot be parallelized (or only partially) because each predictor can only be trained after the previous predictor has been trained and evaluated. As a result, it does not scale as well as bagging \/ pasting.","b7d4e1e9":"In an <b> extremely randomized trees <\/b> algorithm randomness goes one step further: the splitting thresholds are randomized. Instead of looking for the most discriminative threshold, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows reduction of the variance of the model a bit more, at the expense of a slightly greater increase in bias.","da1d833a":"Linear SVM is far outperformed by the other classifiers. Remove the SVM to see if performance improves.","a1a28996":" Aggregate the predictions of each classifier and predict the class that gets the most votes. This majority vote classifier is called a hard voting classifier.\n \n![Screen%20Shot%202019-11-15%20at%2018.25.08.png](attachment:Screen%20Shot%202019-11-15%20at%2018.25.08.png)\n\nThis voting classifier often achieves a higher accuracy than the best classifier in the ensemble. Even if each classifier is a weak learner (it does only slightly better than random guessing), the ensemble can still be a strong learner (achieving high accuracy), provided there are a sufficient number of weak learners and they are sufficiently diverse.","f651ef6b":"# <u>Ensemble Learning<\/u>\n\nIf we aggregate the predictions of a group of predictors (classifiers \/ regressors), we will <u> often<\/u> get better predictions than with the best individual predictor. A group of predictor is called an ensemble, the technique of aggregation is called Ensemble Learning, and an Ensemble Learning algorithm is called an Ensemble method.\n\n<b>Ensemble methods work best when the predictors are as independent from one another as possible. Training the dataset using very different classifiers increases the chance of them making very different types of errors, improving ensemble\u2019s accuracy.<\/b>\n\n<u>Ensemble methods can be divided into two groups: <\/u>\n\n- <b> Sequential ensemble methods <\/b> where the base learners are generated sequentially (e.g. AdaBoost).\n  - The basic motivation of sequential methods is to <u> exploit the dependence between the base learners <\/u>. The overall performance can be boosted by weighing previously mislabeled examples with higher weight.\n\n- <b> Parallel ensemble methods <\/b> where the base learners are generated in parallel (e.g. Random Forest).\n  - The basic motivation of parallel methods is to <u> exploit independence between the base learners <\/u> since the error can be reduced dramatically by averaging.","ebb7bbfd":"### <u>Feature Importance<\/u>\n\nIf we look at a single Decision Tree, important features are likely to appear closer to the root of the tree, while unimportant features will often appear closer to the leaves (or not at all). It is possible to get an estimate of a feature\u2019s importance by computing the average depth at which it appears across all trees in the forest.","001a9a1a":"## <u>Bagging and Pasting<\/u>\nIn Bagging\/ Pasting,  same training algorithm is used for every predictor(Classifier\/ Regressor), but we train them on different random subsets of the training set. When sampling is performed with replacement, the method is called <b> bagging \/ bootstrap aggregating <\/b>. When sampling is performed without replacement, it is called <b> pasting <\/b>.\n\n![Screen%20Shot%202019-11-15%20at%2018.40.20.png](attachment:Screen%20Shot%202019-11-15%20at%2018.40.20.png)\n\nOnce all predictors are trained, the ensemble can make a prediction for a new instance by aggregating the predictions of all predictors. The aggregation function is generally the statistical mode (most frequent prediction) for classification, or the statistical mean for regression.\n\nEach individual predictor has a higher bias(underfit) because only a subset of the trining set is trained on it, aggregation reduces both bias and variance. The ensemble has a similar bias but a lower variance than a single predictor trained on the original training set.\n\nBagging and Pasting scale very well on different CPU cores and servers, all predictors can be trained in parallel, predictions can also be made in parallel.\n\n### <u>Out-of-Bag Evaluation <\/u>\n\nWith bagging, at each iteration, the remaining training instances that are not sampled are called out-of-bag (oob) instances. These instances are not same for all predictors.\nSince a predictor never sees the oob instances during training, it can be evaluated on these instances, without the need for a separate validation set or cross-validation. We can evaluate the ensemble itself by averaging out the oob evaluations of each predictor.","39f12189":"### Stacking\n\nStacking is an ensemble learning technique that uses predictions from multiple models (for example decision tree, knn or svm) to build a new model. This model is used for making predictions on the test set.\n\n![Screen%20Shot%202019-11-17%20at%2014.20.06.png](attachment:Screen%20Shot%202019-11-17%20at%2014.20.06.png)\n\nFirst, the training set is split in two subsets. The first subset is used to train the predictors in the first layer. Next, the predictors in the first layer are used to make predictions on the second(hold-out) set. Now (in example above) for each instance in the hold-out set there are four predicted values. A new training set is created using these predicted values as input features and keeping the target values. The blender is trained on this new training set, it learns to predict the target value where inputs are the the first layer\u2019s predictions.\n\nIt is possible to train several different blenders on the top of one another (e.g., one using Linear Regression, another using Random Forest Regression etc). The training set should be divided equal to the number of layers(see image above).","16af6453":"Now train a second DecisionTreeRegressor on the residual errors made by the first\npredictor:","9dc263d8":"### Gradient Boosting\nGradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost, Gradient Boosting tries to fit the new predictor to the residual errors made by the previous predictor.","b0b579df":"## <u> Random Forests <\/u>\nRandom Forest is an ensemble of Decision Trees.\n\nInstead of building a BaggingClassifier and passing it a DecisionTreeClassifier, we can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees.\n\nRandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier (to control how trees are grown), plus all the hyperparameters of a BaggingClassifier to control the ensemble itself.","c645837d":"The following code creates the same ensemble as the previous one:","89476a6a":"In order to find the optimal number of trees, you can use early stopping (see Chap\u2010 ter 4). A simple way to implement this is to use the staged_predict() method: it returns an iterator over the predictions made by the ensemble at each stage of train\u2010 ing (with one tree, two trees, etc.). The following code trains a GBRT ensemble with 120 trees, then measures the validation error at each stage of training to find the opti\u2010 mal number of trees, and finally trains another GBRT ensemble using the optimal number of trees:","7c053ecd":"### End\nIf you reached this far please comment and upvote this kernel, feel free to make improvements on the kernel and please share if you found anything useful !","db857c70":"A bit better! SVM was hurting performance. Let's try using a soft voting classifier.\n\nIf all classifiers in the ensemble can estimate class probabilities (predict_proba()), we can set \"voting = 'soft'\" to predict the class with the highest class probability, averaged over all the individual classifiers. This is called soft voting. It often achieves higher performance than hard voting because it gives more weight to highly confident votes."}}