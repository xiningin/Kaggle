{"cell_type":{"f2082937":"code","ace5b8c9":"code","46bd9d72":"code","b8ee1245":"code","ecfb341e":"code","6fd41056":"code","f08e6f6b":"code","27a474da":"code","fa729a12":"code","b361734c":"code","9a5978f9":"markdown","4e45dde2":"markdown","a4892082":"markdown","07337dbb":"markdown","40b82343":"markdown","1e812f9d":"markdown","b0349408":"markdown","c0aea353":"markdown","96b278bf":"markdown","d69af4c6":"markdown","367a4afa":"markdown"},"source":{"f2082937":"# Set up code checking\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.ml_intermediate.ex1 import *\nprint(\"Setup Complete\")","ace5b8c9":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX_full = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\nX = X_full[my_cols].copy()","46bd9d72":"X_train.head()","b8ee1245":"from sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Lasso\n\ndef rf_model(n_estimators=100, criterion='mae', random_state=0):\n    return RandomForestRegressor(n_estimators=n_estimators, criterion=criterion, random_state=random_state)\n\ndef xgb_model(n_estimators=1000, learning_rate=0.05, n_jobs=4):\n    return XGBRegressor(n_estimators=n_estimators, learning_rate=learning_rate, n_jobs=n_jobs)\n\ndef lasso_model(alpha=0.0005, random_state=5):\n    return Lasso(alpha=alpha, random_state=random_state)\n","ecfb341e":"## Option 1 : drop cols with missing data\n\n# Number of missing values in each column of training data\n#missing_val_count_by_column = (X.isnull().sum())\n#missing_val_count_by_column_test = (X_test.isnull().sum())\n\n# Fill in the line below: get names of columns with missing values\n#cols_with_missing = missing_val_count_by_column[missing_val_count_by_column > 0].keys()\n#cols_with_missing_test = missing_val_count_by_column_test[missing_val_count_by_column_test > 0].keys()\n#cols_with_missing = cols_with_missing.union(cols_with_missing_test)\n#print(cols_with_missing)\n\n# Fill in the lines below: drop columns in training and validation data\n#X_train = X_train.drop(cols_with_missing, axis=1)\n#X_valid = X_valid.drop(cols_with_missing, axis=1)\n#X_test = X_test.drop(cols_with_missing, axis=1)\n##X = X.drop(cols_with_missing, axis=1)\n\n#X_train.head()\n\n# MEA Only is : 18935\n\n## Option 2 : Imputation of missing data\n\n#from sklearn.impute import SimpleImputer\n\n# Imputation\n#final_imputer = SimpleImputer(strategy='median')\n#X_train = pd.DataFrame(final_imputer.fit_transform(X_train))\n#X_valid = pd.DataFrame(final_imputer.transform(X_valid))\n#X_test = pd.DataFrame(final_imputer.transform(X_test))\n#X = pd.DataFrame(final_imputer.transform(X))\n\n# Imputation removed column names; put them back\n#X_train.columns = X_train.columns\n#X_valid.columns = X_valid.columns\n#X_test.columns = X_test.columns\n#X.columns = X.columns\n\n# MEA is : 18093","6fd41056":"## Option 3 : Label Encoding\n\n#from sklearn.preprocessing import LabelEncoder\n\n# Get list of categorical variables\n#s = (X.dtypes == 'object')\n#object_cols = list(s[s].index)\n\n#print(\"Categorical variables:\")\n#print(object_cols)\n\n# Columns that can be safely label encoded\n#good_label_cols = [col for col in object_cols if set(X_train[col]) == set(X_valid[col])]\n#print(\"good labels:\")\n#print(good_label_cols)\n\n# Problematic columns that will be dropped from the dataset\n#bad_label_cols = list(set(object_cols)-set(good_label_cols))\n#print(\"bad labels:\")\n#print(bad_label_cols)\n\n# Drop categorical columns that will not be encoded\n#X_train = X_train.drop(bad_label_cols, axis=1)\n#X_valid = X_valid.drop(bad_label_cols, axis=1)\n#X_test = X_test.drop(bad_label_cols, axis=1)\n#X = X.drop(bad_label_cols, axis=1)\n\n# Apply label encoder to each column with categorical data\n#label_encoder = LabelEncoder()\n#for col in good_label_cols:\n#    X_train[col] = label_encoder.fit_transform(X_train[col])\n#    X_valid[col] = label_encoder.transform(X_valid[col])\n#    X_test[col] = label_encoder.transform(X_test[col])\n#    X[col] = label_encoder.transform(X[col])\n\n# MAE is : 18607 (combined with drop missing cols)\n","f08e6f6b":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Bundle preprocessing and modeling code in a pipeline\ndef clf(n_estimators=100, criterion='mae', random_state=0):\n    return  Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', rf_model(n_estimators=n_estimators, criterion=criterion, random_state=random_state))\n                     ])\n# MAE : 17772\n\n# Bundle preprocessing and modeling code in a pipeline\ndef xgb_clf(n_estimators=1000, learning_rate=0.05, n_jobs=4):\n    return  Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', xgb_model(n_estimators=n_estimators, learning_rate=learning_rate, n_jobs=n_jobs ))\n                     ])\n\n# MAE: 16499\n\ndef lasso_clf(alpha=0.0005, random_state=5):\n    return  Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', lasso_model(alpha=alpha, random_state=random_state))\n                     ])\n\n# MAE: \n","27a474da":"from sklearn.metrics import mean_absolute_error\n\n# Function for comparing different models\ndef score_model(mdl, X_t=X_train, X_v=X_valid, y_t=y_train, y_v=y_valid):\n    mdl.fit(X_t, y_t)\n    preds = mdl.predict(X_v)\n    return mean_absolute_error(y_v, preds)\n\nmae = score_model(clf(n_estimators=100, criterion='mae', random_state=0))\nprint(\"Random forset Regression Model's MAE: %d\" % (mae))\n\nmae = score_model(xgb_clf(n_estimators=1000, learning_rate=0.05, n_jobs=4))\nprint(\"Gradient boosting Model's MAE: %d\" % (mae))\n\nmae = score_model(lasso_clf(alpha=0.00005, random_state=5))\nprint(\"Lasso Model's MAE: %d\" % (mae))\n","fa729a12":"from sklearn.model_selection import cross_val_score\n\ndef get_score_cross_validation(n_estimators):\n    test_pipeline = xgb_clf(n_estimators=n_estimators)\n    scores = -1 * cross_val_score(test_pipeline, X, y,\n                                  cv=3,\n                                  scoring='neg_mean_absolute_error')\n    return scores.mean()\n    pass\n\nresults = {}\nfor i in range(1,20):\n    results[50*i] = get_score_cross_validation(50*i)\n\n# results for  rf_model are {50: 17989.90902711937, 100: 17951.248890445127, 150: 17905.961725587727, 200: 17873.74381446413, 250: 17912.836598102094, 300: 17939.665974828862, 350: 18000.571310498595, 400: 17993.435347382427}\n# So, we conclude that 200 is the best value of estimators\n\n# results for  xgb_model are {50: 23154.361754126232, 100: 17392.48273273184, 150: 16586.630607658255, 200: 16291.770399811685, 250: 16143.857207045168, 300: 16074.786700873738, 350: 16011.175154920638, 400: 15983.171869889198, 450: 15964.655717794165, 500: 15954.15480163433, 550: 15949.418943565805, 600: 15947.995305519818, 650: 15936.35036284181, 700: 15922.79011287209, 750: 15926.222763224805, 800: 15916.359328436118, 850: 15914.507840491146, 900: 15912.554610106248, 950: 15916.544758285245}\n# So, we conclude that 900 is the best value of estimators\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nprint(results)\nplt.plot(results.keys(), results.values())\nplt.show()\n","b361734c":"# Fit the model to the training data\n#my_pipeline = clf(n_estimators=200)\n#my_pipeline = xgb_clf(n_estimators=900)\nmy_pipeline = lasso_clf(alpha=0.00005, random_state=5)\n\nmy_pipeline.fit(X, y)\n\n# Generate test predictions\npreds_test = my_pipeline.predict(X_test)\n\n# Save predictions in format used for competition scoring\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","9a5978f9":"# Step 2: Preprocessing Data via Piplines\n\n1\/ Deal with missing data (obsolete)\n","4e45dde2":"A brief view on data","a4892082":"2\/ option 2 : using a cross-validation and neg mean absolute error","07337dbb":"1\/ option 1 : Using a Mean Absolute Error","40b82343":"# step 1 : create a models\n\n1\/ Standard methodes ","1e812f9d":"# Step 0 : Get Data from training and test files","b0349408":"2\/ Label Encoding (Obselete)","c0aea353":"# Step 4 : Generate test predictions","96b278bf":"# Setup","d69af4c6":"# Step 3 : Evaluate model using MAE","367a4afa":"3\/ using pipeline > Categorical columns + Numerical columns"}}