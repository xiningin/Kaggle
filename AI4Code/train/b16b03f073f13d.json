{"cell_type":{"fb2743b6":"code","5e049995":"code","ccac9435":"code","079aad7b":"code","529f8cd8":"code","c11c99b6":"markdown","df3a46a7":"markdown","00dc2b35":"markdown","0c1546d0":"markdown","f1d6b432":"markdown","1f1fb976":"markdown","845a3ce4":"markdown","8e6d76c7":"markdown","44f69f12":"markdown","d2d68c63":"markdown","ca61ed46":"markdown","b07fff0c":"markdown","56d2113e":"markdown","daec427a":"markdown","23ca0669":"markdown","f7fe43da":"markdown","d63d881f":"markdown","e9ed64b9":"markdown","4c897f75":"markdown","5bc9cef6":"markdown","17eb1627":"markdown"},"source":{"fb2743b6":"knob_weight = 0.5\ninput_val = 0.5\ngoal_pred = 0.8\n\n# think y = mx from slope-intercept formula (ignore the b or bias part for now)\npred = input_val * knob_weight\n# squaring the error will always make it postive (even if pred - goal_pred is negative)\nerror = (pred - goal_pred) ** 2\n\n# how much you missed by\nprint(error)","5e049995":"weight = 0.5\ninput_val = 0.5\ngoal_prediction = 0.8\n\nstep_amount = 0.001\n\nfor iteration in range(1101):\n    prediction = input_val * weight\n    error = (prediction - goal_prediction) ** 2\n    print(\"Step:\" + str(iteration) + \" Error:\" + str(error) + \" Prediction:\" + str(prediction))\n    \n    # after predicting the error, you add a step amount value to the weight\n    # multiplied by the original input_val to get a new prediction\n    # the up_error is the new error based on you adding the step_amount to the weight\n    up_prediction = input_val * (weight + step_amount)\n    up_error = (goal_prediction - up_prediction) ** 2\n    \n    # after predicting the error, you subtract a step amount value from the weight\n    # multiplied by the original input_val to get a new prediction\n    # the down_error is the new error based on you subtracting the step_amount from the weight\n    down_prediction = input_val * (weight - step_amount)\n    down_error = (goal_prediction - down_prediction) ** 2\n    \n    # finally you check to see which is bigger\n    # based on the evaluation of the if statement, you assign the variable weight\n    # which was declared at the top a new weight value\n    if (down_error < up_error):\n        weight = weight - step_amount\n        \n    if (down_error > up_error):\n        weight = weight + step_amount\n        \n# the for loop is going to run 1101 times in trying to get the error as close to zero as possible\n# with a better prediction and more accurate weight","ccac9435":"weight = 0.5\ngoal_pred = 0.8\ninput_val = 0.5\n\nfor iteration in range(20):\n    pred = input_val * weight\n    error = (pred - goal_pred) ** 2\n    # direction_and_amount is going to tell you which direction your new weight value should move\n    # either postive or negative\n    # and also scale the amount for you through multiplication (or stop it if input_val = 0)\n    direction_and_amount = (pred - goal_pred) * input_val\n    # your new weight value will be the current weight - the direction_and_amount\n    weight = weight - direction_and_amount\n    \n    print(\"Step:\" + str(iteration) + \" Error:\" + str(error) + \" Prediction:\" + str(pred))","079aad7b":"weight, goal_pred, input_val = 0.0, 0.8, 0.5\n\nfor iteration in range(20):\n    pred = input_val * weight\n    # error - how far you are off from actual prediction, positive\n    error = (pred - goal_pred) ** 2\n    # delta - how far you are off from actual prediction\n    delta = pred - goal_pred\n    # weight_delta - how far you off from the actual prediction with proper scaling & negative reversal\n    # with the actual input value\n    weight_delta = delta * input_val\n    # weight - modify weight accordingly to the weight_delta \n    # this is also decreasing the error (or getting it closer to 0) as well\n    # since the new weight value is used to calculate the pred variable\n    weight -= weight_delta\n    \n    print(\"Step:\" + str(iteration) + \" Error:\" + str(error) + \" Prediction:\" + str(pred))","529f8cd8":"weight = 0.5\ngoal_pred = 0.8\ninput_val = 2\nalpha = 0.1\n\nfor iteration in range(20):\n    pred = input_val * weight\n    error = (pred - goal_pred) ** 2\n    # you're calculating your slope\/derivative\n    derivative = input_val * (pred - goal_pred)\n    # because you want your derivative\/slope to not overshoot with the new weight update\n    # you want to scale it (through mulitplication) appropiately based on your input\n    weight = weight - (alpha * derivative)\n    \n    print(\"Step:\" + str(iteration) + \" Error:\" + str(error) + \" Prediction:\" + str(pred))","c11c99b6":"## Overcorrecting Neural Networks","df3a46a7":"A good quote by Trask, \"Changing `weight` means the function *conforms to the patterns in the data*\"[2].","00dc2b35":"## Hot and Cold Learning","0c1546d0":"## Learning is Just Reducing Error","f1d6b432":"# Chapter 4: Introduction to Gradient Descent","1f1fb976":"Weight Delta is your slope (or m) in the equation y = mx. Look at the following parabola (bowl-shaped curve).Error is the dependent variable (y) and the weight is the independent variable (x). Another way of saying it, error is dependent on the value of weight times the slope. (insert drawing of parabola below this cell).","845a3ce4":"\"Gradient descent [allows] you to calculate both the *direction* and the *amount* you should change **weight** to reduce **error**\" [1].","8e6d76c7":"## Resources","44f69f12":"[1] \u201cIntroduction to neural learning: gradient descent\u201d *Grokking Deep Learning*, by Andrew W. Trask, Manning Publications, 2019, p. 190.\n\n[2] \u201cIntroduction to neural learning: gradient descent\u201d *Grokking Deep Learning*, by Andrew W. Trask, Manning Publications, 2019, p. 209.\n\n[3] \u201cIntroduction to neural learning: gradient descent\u201d *Grokking Deep Learning*, by Andrew W. Trask, Manning Publications, 2019, p. 219.\n\n[4] \u201cIntroduction to neural learning: gradient descent\u201d *Grokking Deep Learning*, by Andrew W. Trask, Manning Publications, 2019, p. 224\n\n[5] \u201cIntroduction to neural learning: gradient descent\u201d *Grokking Deep Learning*, by Andrew W. Trask, Manning Publications, 2019, p. 236","d2d68c63":"Trask has a good definiton of derivative. A derivative is, \"the relationship between two variables in a function so you can know how much one changes when you change the other. It's just the sensitivity between two variables\" [4]. For example, y = 2x -> the derivative would be 2. Why? Because the value 2 is telling you (whether positive or negative) how much x changes in relation to y. ","ca61ed46":"Hot and Cold Learning is basically when you change the weight knobs as you are trying to get your error close to 0 as possible.","b07fff0c":"* The code below comes from page 199 but is modified as the range in the for loop should be **20** and not **4** as it is in the book to get the value close to **0.8** in prediction","56d2113e":"![IMG_0181.JPG](attachment:IMG_0181.JPG)","daec427a":"## Calculating both direction and amount from error","23ca0669":"![IMG_0182.JPG](attachment:IMG_0182.JPG)","f7fe43da":"Check out the blog post accompanying this notebook [here](https:\/\/jdridgeway.com\/deep-learning-from-scratch-gradient-descent\/)","d63d881f":"\"The slope's sign gives you direction, and the slope's steepness gives you amount. You can use both of these to help find the goal `weight`\" [3].","e9ed64b9":"Note: As an Amazon Associate I earn from qualifying purchases. I get commissions for purchases made through links in this jupyter notebook. See a more full disclaimer [here](https:\/\/jdridgeway.com\/disclaimer\/)","4c897f75":"The above code sucks because even though you know whether you have to move in the positive (+) or negative (-) direction with the weight (based the error), you don't know by how much. Right now, we're just subtracting or adding the current weight to the step amount  but step amount is not accurate AND we have to do so many iterations (too intensive in real life)","5bc9cef6":"Material that the following code is drawn upon comes from the Grokking Deep Learning book which you can purchase here (ebook: https:\/\/www.manning.com\/books\/grokking-deep-learning) or (physical:https:\/\/amzn.to\/3cEVv7l)","17eb1627":"Neural Networks can explode in value when the input is too big. The best way to overcorrect is to use alpha parameters that are values between 0 and 1. To know which alpha values to use, is to do some guess and check work to see which gets closest to `error == 0`. Or in other words, \"If you have a big input, the prediction is very sensitive to changes in the weight (because `pred = input * weight`). This can cause the network to overcorrect\" [5]."}}