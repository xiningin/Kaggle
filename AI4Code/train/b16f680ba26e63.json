{"cell_type":{"a9f3324b":"code","602f3ba3":"code","a4ec74df":"code","bec6dd9c":"code","31abff2f":"code","c40d0f1f":"code","a8cf9a1b":"code","cff44c00":"code","c50b6e3f":"code","d6acec3c":"code","5720985c":"code","42f4ac3d":"code","2ec2f895":"code","3713de1f":"code","b4843076":"code","fe5ae660":"code","820ffee8":"code","d5f20bea":"code","b4a02ed2":"code","56d0c133":"code","4f17a9ff":"code","be5a1db0":"code","5a5dbfe8":"code","54ac7777":"code","d1d5e2fc":"code","f113f767":"code","5c483c42":"code","1cb86bb2":"code","6f63e6ee":"code","2515f1bf":"code","0c3be6de":"code","7b0eee3f":"code","0611d43a":"code","a79b5117":"markdown","7399b830":"markdown","827e0738":"markdown","41546898":"markdown","734f904d":"markdown","951cff81":"markdown","59c8c519":"markdown","9a132e14":"markdown","0f519df6":"markdown","e0f46e0d":"markdown","75bff99f":"markdown","efd5311d":"markdown","8bcac597":"markdown","6626744f":"markdown","1244a91a":"markdown","27f6667e":"markdown","c95729eb":"markdown","c3db18ff":"markdown","29b32f36":"markdown","55b7ee7a":"markdown"},"source":{"a9f3324b":"# Same old same old\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Fancy library for graphs\nimport seaborn as sns\nsns.set_style('darkgrid')\n\n# Now this is random\nimport random\n\n# Readymade Machine Learning imports\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Doctor Strange feels\nimport time\n\n","602f3ba3":"# Let's see what the csv has in store for us\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain.head()","a4ec74df":"data = [train, test]\nfor dataset in data:\n    dataset.drop(['Name', 'PassengerId', 'Ticket'], axis=1,inplace=True)\ntrain.head()","bec6dd9c":"train.describe(include=['O'])","31abff2f":"train.info()","c40d0f1f":"test.info()","a8cf9a1b":"for dataset in data:\n    dataset.drop(['Cabin'], inplace=True, axis=1)\n    \ntrain.head()","cff44c00":"grid = sns.FacetGrid(train, row='Pclass', col='Sex')\ngrid.map(plt.hist, 'Age', bins=25)\ngrid.add_legend()","c50b6e3f":"for dataset in data:\n    dataset['Sex'] = dataset['Sex'].apply(lambda x:1 if x == 'female' else 0)\nsns.swarmplot(x='Survived', y='Age', hue='Sex', data=train)","d6acec3c":"# Estimating values for missing entries in column 'Age'\n\"\"\"\nWe're going to calculate median Age for every combination of values Sex and Pclass, and replace the missing values with the median\n\"\"\"\nfor dataset in data:\n    for i in range(0, 2):          # Iterating over 'Sex' 0 or 1\n        for j in range(0, 3):      # Iterating over 'Pclass' 1, 2 or 3\n            guess_df = dataset.loc[(dataset['Sex'] == i) & (dataset['Pclass']==j+1)]['Age'].dropna()\n            \n            age_guess = guess_df.median()\n            \n            dataset.loc[(dataset['Age'].isnull()) & (dataset['Sex'] == i) & (dataset['Pclass']==j+1), ['Age']] = age_guess\n            \n    dataset['Age'] = dataset['Age'].astype(int)\n                        \ntrain.head()\n                           \n","5720985c":"train['AgeBand'] = pd.cut(train['Age'], 5)\ntrain[['AgeBand', 'Survived']].groupby(['AgeBand']).mean()","42f4ac3d":"for dataset in data:\n    dataset.loc[dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age']) > 16 & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age']) > 32 & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age']) > 48 & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[(dataset['Age']) > 64 & (dataset['Age'] <= 80), 'Age'] = 4\n\ntrain.head()","2ec2f895":"# We don't need the AgeBand Feature now, so it's gotta go!\ntrain.drop(['AgeBand'], axis=1, inplace=True)","3713de1f":"test['Fare'].fillna(test['Fare'].dropna().median(), inplace=True)\n\ntrain['Fareband'] = pd.qcut(train['Fare'], 4)\ntrain[['Fareband', 'Survived']].groupby(['Fareband']).mean()","b4843076":"for dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\n","fe5ae660":"train.drop(['Fareband'], axis=1, inplace=True)\ntrain.head(10)","820ffee8":"for dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(train['Embarked'].dropna().mode()[0])","d5f20bea":"train[['Embarked', 'Survived']].groupby('Embarked').mean()","b4a02ed2":"for dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n    \ntrain.head()","56d0c133":"X_train = train.drop('Survived', axis=1)\nY_train = train['Survived']\nX_test = test\nX_train.shape, Y_train.shape, X_test.shape\naccuracies = pd.DataFrame()","4f17a9ff":"sgd = SGDClassifier()\n\ntic = time.time()\nsgd.fit(X_train, Y_train)\ntok = time.time()\n\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nsgd_time = round(tok-tic, 4)\n\nalgorithms = ['Stochastic Gradient Decent']\nmodel_accuracy = [acc_sgd]\nexec_time = [sgd_time]\nY_pred = sgd.predict(X_test)","be5a1db0":"knn = KNeighborsClassifier(n_neighbors = 5)\n\ntik = time.time()\nknn.fit(X_train, Y_train)\ntok = time.time()\n\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n\nalgorithms.append('KNN Classifier')\nmodel_accuracy.append(acc_knn)\nexec_time.append(round(tok-tic, 4))","5a5dbfe8":"svc = SVC(kernel='poly', degree=8)\n\ntic = time.time()\nsvc.fit(X_train, Y_train)\ntoc = time.time()\n\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nalgorithms.append('Support Vector Machines')\nmodel_accuracy.append(acc_svc)\nexec_time.append(round(toc-tic, 4))","54ac7777":"random_forest_classifier = RandomForestClassifier(n_estimators=100)\n\ntic = time.time()\nrandom_forest_classifier.fit(X_train, Y_train)\ntoc = time.time()\n\nY_pred = random_forest_classifier.predict(X_test)\nrandom_forest_classifier = round(random_forest_classifier.score(X_train, Y_train) * 100, 2)\nalgorithms.append('Random Forest classifier')\nmodel_accuracy.append(random_forest_classifier)\nexec_time.append(round(toc-tic, 4))","d1d5e2fc":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsubmission = pd.DataFrame({\n    \"PassengerId\": test[\"PassengerId\"],\n    \"Survived\" : Y_pred\n})\nsubmission.to_csv(\"submission.csv\", index=False)","f113f767":"# Bringing out the big guns\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\n# To get reproducible results\ntorch.manual_seed(2)","5c483c42":"# Converting the values to a Pytorch Floar tensor\ndef to_tensor(data):\n    return [torch.FloatTensor(point) for point in data]\n\n# Dataset class for fetching data from the dataset\nclass TitanicDataset(Dataset):\n    def __init__(self, df, X_col, Y_col):\n        self.features = df[X_col].values\n        self.targets = df[Y_col].values.reshape(-1, 1)\n        \n    def __len__(self):\n        return len(self.targets)\n    \n    def __getitem__(self, idx):\n        return to_tensor([self.features[idx], self.targets[idx]])\n    \n\n","1cb86bb2":"# Splitting our training data into 90 % training and 10 % validation data\nsplit = int(0.9*int(len(train)))\n\n# X_cols are features for our NN. The first column is 'Survived'(index 0), which is our target\/prediction Variable. So we neglect that from our features.\nX_col = list(train.columns[1:])\nY_col = 'Survived'\n\ntrain_data = train[:split].reset_index(drop=True)\nvalid_data = train[split:].reset_index(drop=True)\n\nprint(f\"train DF{train.shape}\\nAfter Splitting\\ntrain DF {train_data.shape}\\nvalid_data{valid_data.shape}\")\n\n\n#a, b = train_data[0]\nprint(train_data)\n\ntrain_set = TitanicDataset(train_data, X_col, Y_col)\nvalid_set = TitanicDataset(valid_data, X_col, Y_col)\n\ntrain_loader = DataLoader(train_set, batch_size=64, shuffle=True)\nvalid_loader = DataLoader(valid_set, batch_size=64, shuffle=True)","6f63e6ee":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Sequential(\n                    nn.Linear(7, 128),\n                    # nn.Dropout(0.5),\n                    nn.ReLU(),\n                    nn.BatchNorm1d(128))\n        self.fc2 = nn.Sequential(\n                    nn.Linear(128, 256),\n                    #nn.Dropout(0.5),\n                    nn.ReLU(),\n                    nn.BatchNorm1d(256))\n        self.fc3 = nn.Sequential(\n                    nn.Linear(256, 512),\n                    #nn.Dropout(0.5),\n                    nn.ReLU(),\n                    nn.BatchNorm1d(512))\n        self.fc4 = nn.Sequential(\n                    nn.Linear(512, 256),\n                    #nn.Dropout(0.5),\n                    nn.ReLU(),\n                    nn.BatchNorm1d(256))\n        self.fc5 = nn.Sequential(\n                    nn.Linear(256, 1),\n                    nn.Sigmoid())\n        \n        \n    def forward(self, inputs):\n        x = self.fc1(inputs)\n        \n        x = self.fc2(x)\n        \n        x = self.fc3(x)\n        \n        x = self.fc4(x)\n        \n        x = self.fc5(x)\n        \n        return x\n        \n# set use_gpu as 1 if you want to use gpu\nuse_gpu = 1\n\ndevice = torch.device(\"cuda\" if use_gpu else \"cpu\")\n\n\nmodel = Net().to(device)\nprint(model)\n        ","2515f1bf":"def calc_accuracy(Y_pred, Y_actual):\n    Y_pred = torch.round(Y_pred)\n    #print(f\"Rounded values: {Y_pred[:5]}\\nActual Values: {Y_actual[:5]}\")\n    correct_results_sum = (Y_pred == Y_actual).sum().float()\n    acc = correct_results_sum\/Y_pred.shape[0]\n    return acc","0c3be6de":"num_epochs = 300\n\n\ncriterion = torch.nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters())\nloss = []\naccuracy = []\nvalid_loss = []\nvalid_accuracy = []\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\ntic = time.time()\nfor i in range(1, num_epochs+1):\n    epoch_loss = 0\n    epoch_acc = 0 \n    for current_batch in train_loader:\n        batch_X, batch_Y = current_batch\n        batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n        #batch_Y = batch_Y.long().squeeze()\n        outputs = model.forward(batch_X)\n        # print(f\"Batch_Y shape: {batch_Y.shape}\\noutput Shape:{outputs.shape}\")\n        batch_loss = criterion(outputs, batch_Y)\n        #print(f\"Loss shape:{batch_loss}\")\n        optimizer.zero_grad()\n        batch_loss.backward()\n        optimizer.step()\n        scheduler.step()\n        # print(f\"Batch_Y shape: {batch_Y[:5]}\\noutput Shape:{outputs[:5]}\")\n        batch_acc = calc_accuracy(outputs.squeeze(), batch_Y.squeeze())\n        \"\"\"        \n        print(f\"epoch loss: {epoch_loss:.4f}\\tbatch_loss: {batch_loss:.4f}\")\n        print(f\"epoch acc: {epoch_acc:.4f}\\tbatch_acc: {batch_acc:.4f}\")\"\"\"\n        epoch_loss += batch_loss\n        epoch_acc += batch_acc\n        \n    valid_epoch_loss = 0\n    valid_epoch_acc = 0\n    with torch.no_grad():\n        for valid_batch in valid_loader:\n            valid_X, valid_Y = valid_batch\n            valid_X, valid_Y = valid_X.to(device), valid_Y.to(device)\n            \n            valid_prediction = model.forward(valid_X)\n\n            valid_batch_loss = criterion(valid_prediction, valid_Y)\n            valid_batch_acc = calc_accuracy(valid_prediction.squeeze(), valid_Y.squeeze())\n\n            valid_epoch_loss += valid_batch_loss\n            valid_epoch_acc += valid_batch_acc\n    \n    epoch_loss \/= len(train_loader)\n    epoch_acc \/= len(train_loader)\n    \n    valid_epoch_loss \/= len(valid_loader)\n    valid_epoch_acc \/= len(valid_loader)\n    \n    \n    loss.append(epoch_loss)   \n    accuracy.append(epoch_acc)\n    \n    if i % 25 == 0:\n        print(\"======================================================\")\n        print(f'Yo, this is epoch number {i}')\n        print(f\"Loss : {epoch_loss:.4f}\\t\\t\\tAccuracy: {epoch_acc:.4f}\")\n        print(f\"Validation Loss : {valid_epoch_loss:.4f}\\tValidation Accuracy: {valid_epoch_acc:.4f}\")\ntoc = time.time()","7b0eee3f":"device = 'GPU'\nif not use_gpu:\n    device = 'CPU'\nalgorithms.append('Neural Network (' + device + ')')\n# algorithms.append('Neural Network (gpu)')\nmodel_accuracy.append(round(epoch_acc.item()*100, 2))\nexec_time.append(round(toc-tic, 4))","0611d43a":"accuracies['Algorithms'] = algorithms\naccuracies['Accuracy'] = model_accuracy\naccuracies['Training Time'] = exec_time\naccuracies.reset_index(drop=True, inplace=True)\naccuracies","a79b5117":"# K-nearest neighbours","7399b830":"# Filling Missing values\nAge","827e0738":"> Well well well, looks like Random Forest has beaten the Neural Network.\n\n![download.jfif](attachment:download.jfif)","41546898":"# Completing a Categorical Feature","734f904d":"# Civil War\nIf you've always wanted to compare Machine Learning algorithms and Neural Networks on the basis of accuracy, you're at the right place. We're gonna do exactly that, with the help of Titanic Dataset\n\n> References -<br> This amazing [notebook](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions) by [Manav Sehgal](https:\/\/www.kaggle.com\/startupsci)\n\n> The First step is gonna be the same, regardless of which method we choose. We have to prepare the data, before we can feed it to our algorithms or Neural Network Models","951cff81":"# Let's build some models NOW","59c8c519":"# Neural Network has joined the chat!\n\n","9a132e14":"# Stochastic Gradient Decent","0f519df6":"# Support Vector Machines\n","e0f46e0d":"> That was tiring. Let's make the model now.\n","75bff99f":"> Cabin columns have a lot of null values, we need to get rid of it. Age column has some missing values, so we'll add appropriate values later","efd5311d":"So we have the bands (0-16), (16-32), (32-48), (48-64), (64, 80)","8bcac597":"# Data-Wrangling\n> Since most of our data has very low variance (spread of values), it would be better if we could do that with the column 'Age'\n\nWe can use a 'cut' method of pandas to get some sort of 'Age Band',and then replace these age bands by integral values\n                                                                                                                                ","6626744f":"How did that happen? \nThe Titanic dataset is a small dataset. Neural Networks shine when the amount of data is huge. \nWe can conclude that when a problem comes with less data, and is relatively simple, we should try traditional Machine Learning Algorithms, and not Neural Networks. \n\n\nTraditional ML algorthms beat NN in time, as well as in accuracy for the Titanic Dataset.","1244a91a":"# Coverting Categorical Features to Integral values","27f6667e":"# Random Forest Classifier","c95729eb":"> Preparing our data for the NN","c3db18ff":"Now, I think we could say that passenger names, Passenger ID and Ticket could have very low impact on deciding the outcome of the accident, so we're gonna drop them","29b32f36":"> This clearly indicates that Females had a better chance of surviving the accident when compared to males, also, there were more men when compared to the number of women on Titanic","55b7ee7a":"> We'll do the same thing with fare, but test has a NaN value for the column 'Fare'. We'll replace it with the median"}}