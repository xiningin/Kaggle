{"cell_type":{"de7a2edf":"code","82e67f63":"code","7d56c3a7":"code","1eb443d2":"code","ddf50c77":"code","8c22aef1":"code","abe9608b":"code","09a0d08b":"code","2bcb1f85":"code","46bbcf20":"code","a1a5dfaa":"code","7a232cdb":"code","77f98fbe":"code","5584b4ef":"code","320da97f":"code","e8982b53":"markdown","7a9ca93a":"markdown","32374da2":"markdown","0048e0b7":"markdown","b82e0aa3":"markdown","aa98a36a":"markdown","0379d4c3":"markdown","d032a1d6":"markdown","5a023928":"markdown","d494a8a6":"markdown","1053050d":"markdown","2d28bb61":"markdown"},"source":{"de7a2edf":"#Importing the required libraries\nimport re\nimport math\nimport numpy as np\nimport pandas as pd\nimport string\nimport scipy as sp\nimport nltk\nimport time\nimport operator\nfrom scipy import *\nfrom scipy.sparse import *\nfrom collections import defaultdict\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer \nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport os\nprint(os.listdir(\"..\/input\"))","82e67f63":"#remove words less than length of 4\ndef filterLen(docs, minlen):\n    return [ [t for t in d if len(t) >= minlen ] for d in docs ]","7d56c3a7":"#Building Bag of words\ndef word_bag(docs):\n#     nrows = len(docs)\n    idx = {}\n    tid = 0\n    for d in docs:\n        for w in d:\n            if w not in idx:\n                idx[w] = tid\n                tid += 1\n    return idx\n    \n#Building the sparse matrix\nfrom collections import Counter\nfrom scipy.sparse import csr_matrix\ndef build_matrix(docs, idx):\n    r\"\"\" Build sparse matrix from a list of documents, \n    each of which is a list of word\/terms in the document.  \n    \"\"\"\n    nrows = len(docs)\n    nnz = 0\n    for d in docs:\n        nnz += len(set(d))\n        \n    ncols = len(idx)\n        \n    # set up memory\n    ind = np.zeros(nnz, dtype=np.int)\n    val = np.zeros(nnz, dtype=np.double)\n    ptr = np.zeros(nrows+1, dtype=np.int)\n    i = 0  # document ID \/ row counter\n    n = 0  # non-zero counter\n    # transfer values\n    for d in docs:\n        cnt = Counter(d)\n        keys = list(k for k,_ in cnt.most_common())\n        l = len(keys)\n        for j,k in enumerate(keys):\n            #print(keys)\n            if(k in idx):\n                \n                ind[j+n] = idx[k]\n                val[j+n] = cnt[k]\n        ptr[i+1] = ptr[i] + l\n        n += l\n        i += 1\n            \n    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n    mat.sort_indices()\n    \n    return mat","1eb443d2":"# scale matrix and normalize its rows\ndef csr_idf(mat, copy=False, **kargs):\n    r\"\"\" Scale a CSR matrix by idf. \n    Returns scaling factors as dict. If copy is True, \n    returns scaled matrix and scaling factors.\n    \"\"\"\n    if copy is True:\n        mat = mat.copy()\n    nrows = mat.shape[0]\n    nnz = mat.nnz\n    ind, val, ptr = mat.indices, mat.data, mat.indptr\n    # document frequency\n    df = defaultdict(int)\n    for i in ind:\n        df[i] += 1\n    # inverse document frequency\n    for k,v in df.items():\n        df[k] = np.log(nrows \/ float(v))  ## df turns to idf - reusing memory\n    # scale by idf\n    for i in range(0, nnz):\n        val[i] *= df[ind[i]]\n        \n    return df if copy is False else mat\n\ndef csr_l2normalize(mat, copy=False, **kargs):\n    if copy is True:\n        mat = mat.copy()\n    nrows = mat.shape[0]\n    nnz = mat.nnz\n    ind, val, ptr = mat.indices, mat.data, mat.indptr\n    # normalize\n    for i in range(nrows):\n        rsum = 0.0    \n        for j in range(ptr[i], ptr[i+1]):\n            rsum += val[j]**2\n        if rsum == 0.0:\n            continue  # do not normalize empty rows\n        rsum = 1.0\/np.sqrt(rsum)\n        for j in range(ptr[i], ptr[i+1]):\n            val[j] *= rsum\n            \n    if copy is True:\n        return mat","ddf50c77":"#kNN Function\nclass knn_main():\n    def __init__(self):\n        pass\n    \n    def fit(self, x_train, y_train):\n        self.x_train = x_train\n        self.y_train = y_train\n    \n    def predict(self, x_test,k,eps):\n        self.x_test = x_test\n        y_predict = []\n        print(x_test.shape[0])\n        for i,test_case in enumerate(x_test):\n            temp = self.compute_distances(test_case, self.x_train)\n            c = temp.tocoo(copy=True)\n            d = self.sort_coo(c)\n       #print(d[0])\n            t = d[0:k]\n        #print(t)\n            dict_l ={}\n        #print(k)\n            for z,x in enumerate(t):\n                index = x[1]\n                similarity = x[2]\n                label_t = self.y_train[index]\n                if(z>0):\n                    \n                    if similarity > eps:\n                        if label_t not in dict_l:\n                            dict_l[label_t]=1\n                        else:\n                            dict_l[label_t]+=1\n                else:\n                    dict_l[label_t] =1\n                \n            m = max(dict_l.items(),key=operator.itemgetter(1))[0]\n            y_predict.append(m)\n            #print(\"test case:\", i+1,\"Predicted :\", m)\n        return y_predict\n    def sort_coo(self,m):\n        tuples = zip(m.row, m.col, m.data)\n        return sorted(tuples, key=lambda x: (x[2]),reverse=True)\n    \n    def compute_distances(self, X_test,train):        \n        dot_pro = np.dot(X_test, train.T)\n        return(dot_pro)","8c22aef1":"#Seperation of labels and text data\nlabels = []\ntexts = []\nwith open(\"..\/input\/train.dat\", \"r\") as fh:\n    train_lines = fh.readlines() \nfor line in train_lines:\n    splitline = line.split('\\t')\n    labels.append(splitline[0])\n#     ps = word_tokenize()\n    texts.append(nltk.word_tokenize(splitline[1].lower()))\n\nlen(texts)\n# docs1 = filterLen(texts, 4)\n# docs2 = filterLen(tex,4)","abe9608b":"train_text =  filterLen(texts, 4)\nwordbag = word_bag(train_text)\ntrain_mat = build_matrix(train_text,wordbag)\nmat2 = csr_idf(train_mat, copy=True)\nnorm_train_mat = csr_l2normalize(mat2, copy=True)","09a0d08b":"X_train, X_test, y_train,y_test = train_test_split(texts,labels,test_size = 0.3)\ntest_mat = build_matrix(X_test,wordbag)\nmat3 = csr_idf(test_mat, copy=True)\nnorm_test_mat = csr_l2normalize(mat3, copy=True)","2bcb1f85":"classifier = knn_main()\nclassifier.fit(norm_train_mat,labels)\ng = classifier.predict(norm_test_mat,100,0.1)","46bbcf20":"cm=confusion_matrix(y_test ,g)\nprint(cm)\nprint(classification_report(y_test, g))","a1a5dfaa":"tst_tex = []\nwith open(\"..\/input\/test.dat\", \"r\") as fr:\n    test_lines = fr.readlines() \nfor line in test_lines:\n#         splitline = line.split()\n    tst_tex.append(nltk.word_tokenize(line.lower()))\ntest_text = filterLen(tst_tex, 4)","7a232cdb":"test_mat = build_matrix(test_text,wordbag)\nmat3 = csr_idf(test_mat, copy=True)\nnorm_test_mat = csr_l2normalize(mat3, copy=True)\n# norm_test_mat","77f98fbe":"classifier = knn_main()\nclassifier.fit(norm_train_mat,labels)\n#print(mat5)","5584b4ef":"g = classifier.predict(norm_test_mat,100,0.1)","320da97f":"with open('program.dat', 'w') as f:\n        for cls in g:\n            f.write(\"%s\\n\" % cls)","e8982b53":"#### Writing the predicted class labels into a file","7a9ca93a":"#### Building the confusion matrix and printing the performance report","32374da2":"#### Reading the train data and storing it into list for performing classification","0048e0b7":"#### Train test spilt the train data to find the evaluate the performance of our code and then build csr_matrix for test data using the wordbag of the train data","b82e0aa3":"#### e-kNN Function","aa98a36a":"#### Calling the classifier function and performing predictions for the test split of train data","0379d4c3":"#### Functions to normalize the csr_matrix","d032a1d6":"#### Building word of bags for train data, building the csr_matrix and normalising it","5a023928":"#### Now performing the above steps on the test data file","d494a8a6":"#### Importing the required libraries","1053050d":"## Medical Text Classification using e-kNN \nwhere 'e' is epsilon & 'e' is the minimum similarity value requires to be in the nearest neighbour.","2d28bb61":"#### Functions for building bag of words and csr_matrix ussing the bag of words"}}