{"cell_type":{"5f2ad9a5":"code","a05bd0b2":"code","b2531592":"code","a06b6953":"code","2bb99336":"code","6776d6c8":"code","66d8cc5b":"code","084f7e9e":"code","090ffc7e":"markdown","ff36b5cb":"markdown","343bb87e":"markdown","1e930cf9":"markdown","5b88804c":"markdown"},"source":{"5f2ad9a5":"#imports\nimport os\nimport PIL\nimport torchvision\nimport torchvision.datasets as dset\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nimport xml.etree.ElementTree as ET\nimport numpy as np\n\nimport imgaug as ia\nimport imgaug.augmenters as iaa\n\nimport matplotlib.pyplot as plt\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim as optim\n\n\nimport time\nimport torch\nimport torch.nn as nn\n\nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\nimport matplotlib.image as mpimg","a05bd0b2":"class DataGenerator(Dataset):\n    def __init__(self, directory, transform=None, n_samples=np.inf):\n        self.directory = directory\n        self.transform = transform\n        self.n_samples = n_samples\n\n        self.samples = self._load_subfolders_images(directory)\n        if len(self.samples) == 0:\n            raise RuntimeError(\"Found 0 files in subfolders of: {}\".format(directory))\n\n    def _load_subfolders_images(self, root):\n        IMG_EXTENSIONS = (\n        '.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n\n        def is_valid_file(x):\n            return torchvision.datasets.folder.has_file_allowed_extension(x, IMG_EXTENSIONS)\n\n        required_transforms = torchvision.transforms.Compose([\n                torchvision.transforms.Resize(64),\n                torchvision.transforms.CenterCrop(64),\n        ])\n\n        imgs = []\n        paths = []\n        for root, _, fnames in sorted(os.walk(root)):\n            for fname in sorted(fnames)[:min(self.n_samples, 999999999999999)]:\n                path = os.path.join(root, fname)\n                paths.append(path)\n\n        for path in paths:\n            if is_valid_file(path):\n                # Load image\n                img = dset.folder.default_loader(path)\n\n                # Get bounding boxes\n                annotation_basename = os.path.splitext(os.path.basename(path))[0]\n                annotation_dirname = next(\n                        dirname for dirname in os.listdir('..\/input\/annotation\/Annotation\/') if\n                        dirname.startswith(annotation_basename.split('_')[0]))\n                annotation_filename = os.path.join('..\/input\/annotation\/Annotation\/',\n                                                   annotation_dirname, annotation_basename)\n                tree = ET.parse(annotation_filename)\n                root = tree.getroot()\n                objects = root.findall('object')\n                for o in objects:\n                    bndbox = o.find('bndbox')\n                    xmin = int(bndbox.find('xmin').text)\n                    ymin = int(bndbox.find('ymin').text)\n                    xmax = int(bndbox.find('xmax').text)\n                    ymax = int(bndbox.find('ymax').text)\n\n                    bbox = (xmin, ymin, xmax, ymax)\n\n                    object_img = required_transforms(img.crop(bbox))\n                    imgs.append(object_img)\n        return imgs\n\n    def __getitem__(self, index):\n        sample = self.samples[index]\n        \n        if self.transform is not None:\n            sample = self.transform(sample)\n            \n        return np.asarray(sample)\n    \n    def __len__(self):\n        return len(self.samples)","b2531592":"# util functions\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0.)\n\ndef plot_loss(G_losses, D_losses, epoch):\n    plt.figure(figsize=(10,5))\n    plt.title(\"Generator and Discriminator Loss - EPOCH \"+ str(epoch))\n    plt.plot(G_losses,label=\"G\")\n    plt.plot(D_losses,label=\"D\")\n    plt.xlabel(\"iterations\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n\ndef show_generated_img(n_images=5):\n    sample = []\n    for _ in range(n_images):\n        noise = torch.randn(1, NZ, 1, 1, device=device)\n        gen_image = netG(noise).to(\"cpu\").clone().detach().squeeze(0)\n        gen_image = gen_image.numpy().transpose(1, 2, 0)\n        sample.append(gen_image)\n\n    figure, axes = plt.subplots(1, len(sample), figsize=(64, 64))\n    for index, axis in enumerate(axes):\n        axis.axis('off')\n        image_array = (sample[index] + 1.) \/ 2.\n        axis.imshow(image_array)\n    plt.show()","a06b6953":"# initialize training variables\/load image data\nkernel_start_time = time.perf_counter()\ndatabase = \"..\/input\/all-dogs\/all-dogs\/\"\nn_samples = 2000 #np.inf\nBATCH_SIZE = 16\nEPOCHS = 31\nuse_soft_noisy_labels = True\ninvert_labels = False\nSHOW_DATA = True\n\ntransform = transforms.Compose([#transforms.RandomHorizontalFlip(p=0.5),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrain_data = DataGenerator(database, transform=transform, n_samples=n_samples)\n\n\ntrain_loader = torch.utils.data.DataLoader(train_data, shuffle=True,batch_size=BATCH_SIZE)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nNZ = 100\ncriterion = nn.BCELoss()","2bb99336":"# train model loop\ndef train_loop():\n    G_losses = []\n    D_losses = []\n    print(\"Training Model\")\n    for epoch in range(EPOCHS):\n        if time.perf_counter() - kernel_start_time > 32000:\n            print(\"Time limit reached! Stopping kernel!\"); break\n            \n        start = time.time()\n        for ii, real_images in enumerate(train_loader):\n            if real_images.shape[0]!= BATCH_SIZE:\n                continue\n            \n            # create labels for the data\n            if use_soft_noisy_labels:\n                real_labels = torch.squeeze(torch.empty((BATCH_SIZE, 1), device=device).uniform_(0.80, 0.95))\n                fake_labels = torch.squeeze(torch.empty((BATCH_SIZE, 1), device=device).uniform_(0.05, 0.20))\n            else:\n                real_labels = torch.squeeze(torch.full((BATCH_SIZE, 1), 0.95, device=device))\n                fake_labels = torch.squeeze(torch.full((BATCH_SIZE, 1), 0.05, device=device))\n                \n            if invert_labels:\n                real_labels, fake_labels = fake_labels, real_labels\n            \n            \n            ############################\n            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n            ###########################\n            # train with real samples\n            netD.zero_grad()\n            real_images = real_images.to(device)\n            output = netD(real_images)\n            errD_real = criterion(output, real_labels)\n            errD_real.backward()\n            \n            \n            # train with fake samples\n            noise = torch.randn(BATCH_SIZE, NZ, 1, 1, device=device)\n            fake = netG(noise)\n            output = netD(fake.detach())\n            errD_fake = criterion(output, fake_labels)\n            errD_fake.backward()\n            errD = errD_real + errD_fake\n            optimizerD.step()\n            \n            ############################\n            # (2) Update G network: maximize log(D(G(z)))\n            ###########################\n            netG.zero_grad()\n            output = netD(fake)\n            errG = criterion(output, real_labels)\n            errG.backward()\n            optimizerG.step()\n            \n            G_losses.append(errG.item())\n            D_losses.append(errD.item())\n       \n        #print(f\"{time.time() - start:.2f}s [{epoch + 1}\/{EPOCHS}] Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f}\")\n        \n        if SHOW_DATA and (epoch) % 10 == 0:\n            #plot_loss(G_losses, D_losses, epoch)\n            show_generated_img(10)\n        G_losses = []; D_losses = []","6776d6c8":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n                nn.Conv2d(3, 64, 4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(64),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            \n                nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(128),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            \n                nn.Conv2d(128, 256, 4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(256),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            \n                nn.Conv2d(256, 512, 4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(512),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            \n                nn.Conv2d(512, 1, 4, stride=1, padding=0, bias=False),\n                nn.Sigmoid()\n                )\n        \n    def forward(self, input):\n        output = self.main(input)\n        return output.view(-1)\n\nnetD = Discriminator().to(device)\nweights_init(netD)\noptimizerD = optim.Adam(netD.parameters(), lr=0.0005, betas=(0.5, 0.999))","66d8cc5b":"class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n                nn.ConvTranspose2d(100, 512, 4, stride=1, padding=0),\n                nn.BatchNorm2d(512),\n                nn.LeakyReLU(negative_slope=0.05),\n            \n                nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),\n                nn.BatchNorm2d(256),\n                nn.LeakyReLU(negative_slope=0.05),\n            \n                nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n                nn.BatchNorm2d(128),\n                nn.LeakyReLU(negative_slope=0.05),\n            \n                nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n                nn.BatchNorm2d(64),\n                nn.LeakyReLU(negative_slope=0.05),\n            \n                nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1),\n                nn.Tanh()\n                )\n        \n    def forward(self, input):\n        output = self.main(input)\n        return output\n\nnetG = Generator().to(device)\nweights_init(netG)\noptimizerG = optim.Adam(netG.parameters(), lr=0.0010, betas=(0.5, 0.999))\n\ntrain_loop()","084f7e9e":"class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        \n        def up_conv_block(n_input, n_output, mode='bilinear', k_size=3, stride=1, padding=1):\n            return [nn.Upsample(scale_factor=2, mode=mode),\n                    nn.Conv2d(n_input, n_output, k_size, stride=stride, padding=padding),\n                    nn.BatchNorm2d(n_output),\n                    nn.LeakyReLU(negative_slope=0.05),\n                   ]\n            \n        self.model = nn.Sequential(\n                        nn.ConvTranspose2d(100, 512, 4, stride=1, padding=0),\n                        nn.BatchNorm2d(512),\n            \n                        *up_conv_block(512, 256, mode='bilinear', k_size=3, stride=1, padding=1),\n                        *up_conv_block(256, 128, mode='bilinear', k_size=3, stride=1, padding=1),\n                        *up_conv_block(128, 64,  mode='bilinear', k_size=3, stride=1, padding=1),\n            \n                        nn.Upsample(scale_factor=2, mode='bilinear'),\n                        nn.Conv2d(64,  3,   3, stride=1, padding=1),\n                        nn.Tanh(),\n                        )\n        \n    def forward(self, x):\n        return self.model(x)\n\n\nnetG = Generator().to(device)\nweights_init(netG)\noptimizerG = optim.Adam(netG.parameters(), lr=0.0010, betas=(0.5, 0.999))\n    \ntrain_loop()","090ffc7e":"Try it out! Any comment on the results obtained would really be appreciated.","ff36b5cb":"# Generator -> TransposeConvolutions","343bb87e":"Using TransposeConv for upsampling on the Generator might produce a checkboard effect visible on the output images, this effect can be greatly reduced by using Upsample+Conv instead.\n<br><\/br><br><\/br>\nSource: https:\/\/distill.pub\/2016\/deconv-checkerboard\/\n![image.png](attachment:image.png)","1e930cf9":"# Comparison:   Upsample+Conv   vs   TransposeConv","5b88804c":"# Generator -> Upsample(bilinear) + Convolution(3x3)"}}