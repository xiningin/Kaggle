{"cell_type":{"4e67fa54":"code","d01f7551":"code","647f095e":"code","f7dcbe85":"code","a4c169c0":"code","0ac18da9":"code","2a2e1366":"code","a489ff8c":"code","3a5a4166":"markdown","d3badfc9":"markdown"},"source":{"4e67fa54":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.optimizers import Adam","d01f7551":"TRAINING_DIR = '..\/input\/stanford-dogs-dataset\/images\/Images'","647f095e":"IMG_SIZE = 224\nBATCH_SIZE = 64\nCLASS_NUM = 120\n\n# Training IDG\ntrain_idg = ImageDataGenerator(\n    rescale = 1.\/255, \n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1, \n    shear_range=0.1, \n    zoom_range=0.1,\n    horizontal_flip=True,\n    validation_split=0.2)\n\n# Training Gen\ntrain_gen = train_idg.flow_from_directory(\n    TRAINING_DIR,\n    target_size=(IMG_SIZE,IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    class_mode='categorical',\n    subset='training'\n)\n\n# Test IDG\ntest_idg = ImageDataGenerator(rescale=1.\/255)\n\n# Test Gen\ntest_gen = train_idg.flow_from_directory(\n        TRAINING_DIR,\n        target_size=(IMG_SIZE, IMG_SIZE),\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        class_mode='categorical',\n        subset='validation'\n)","f7dcbe85":"# Iterator\naug_iter = train_idg.flow_from_directory(\n    TRAINING_DIR,\n    target_size=(IMG_SIZE,IMG_SIZE),\n    batch_size=1,\n    class_mode='categorical',\n    subset='training'\n)\n\n# mostrar imagenes horizontales\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,15))\n\n# obtiene los labels de las clases de flow from directory\ndef getLabel(index):\n    return (list(aug_iter.class_indices.keys())[list(aug_iter.class_indices.values()).index(index)])\n\n# mostrar 3 imagenes\nfor i in range(3):\n    \n    obj = next(aug_iter)\n    \n    # obtenemos el index de cada clase\n    class_index = np.argmax(obj[1])\n    \n    # obtener imagenes del generador\n    image = obj[0]\n\n    # mostar 3 imagenes\n    ax[i].imshow(image.reshape(IMG_SIZE, IMG_SIZE, 3))\n    ax[i].set_title(getLabel(class_index))\n    ax[i].axis('off')","a4c169c0":"inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n\n# === InceptionV3 CNN Model ===================\nbase_model = keras.applications.InceptionV3(\n    weights = 'imagenet',\n    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n    include_top=False)\n\nbase_model.trainable = False\n# === InceptionV3 CNN Model ===================\n\n# === Arquitectura ===================\nx = base_model.output\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dropout(0.4)(x)\nx = keras.layers.Dense(2048, activation='relu')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Dropout(0.4)(x)\nx = keras.layers.Dense(2048, activation='relu')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Dropout(0.2)(x)\noutputs = keras.layers.Dense(CLASS_NUM, activation='softmax')(x)\n\n# se acopla el modelo\nmodel = keras.Model(base_model.input, outputs)\n# === Arquitectura ===================\n\n# congelar capas\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# compilar el modelo.\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=Adam(lr=0.001),\n              metrics=['accuracy'])\n\nmodel.summary()","0ac18da9":"history = model.fit(\n    train_gen,\n    steps_per_epoch=80,  \n    epochs=10,\n    verbose=1,\n    validation_data=test_gen,\n    validation_steps=50\n)","2a2e1366":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.ylim([0, 1.1])\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.ylim([0, 1.1])\nplt.show()","a489ff8c":"# iterator\naug_iter = train_idg.flow_from_directory(\n        TRAINING_DIR,\n        target_size=(IMG_SIZE, IMG_SIZE),\n        batch_size=1,\n        class_mode='categorical',\n        subset='validation'\n)\n\n# mostrar imagenes horizontales\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,15))\n\n# obtiene los labels de las clases de flow from directory\ndef getLabel(index):\n    return (list(aug_iter.class_indices.keys())[list(aug_iter.class_indices.values()).index(index)])\n\n# mostrar 3 imagenes\nfor i in range(3):\n\n    obj = next(aug_iter)\n    \n    # obtenemos el index de cada clase\n    class_index = np.argmax(obj[1])\n    \n    # obtener imagenes del generador\n    image = obj[0]\n    \n    # valor real\n    real = getLabel(class_index)\n    # prediccion!\n    pred = getLabel(np.argmax(model.predict([image])))\n    \n    # mostar 3 imagenes\n    ax[i].imshow(image.reshape(IMG_SIZE, IMG_SIZE, 3))\n    ax[i].set_title(real+\"\\n\\n\"+pred+\"\\n\")\n    ax[i].axis('off')","3a5a4166":"# Predictions","d3badfc9":"<img src=\"https:\/\/bingvsdevportalprodgbl.blob.core.windows.net\/preview-images\/17da6816-2e2b-45bb-a0ef-539c7727bff7.png\" \/>\n\n## What breed is that dog?\n\nDebe disenar una red neuronal convolucional para la identificacion de razas de perros.\nLa idea es hacer un modelo capaz de realizar algo similar a : https:\/\/www.bing.com\/visualsearch\/Microsoft\/WhatDog\n\n- En esta tarea se require solo el desarrollo del modelo. Si ud desea hacer una pagina web para simular la pagina de bing, eso es opcional.\n\n- debe crear un cuaderno en Kaggle para el dataset https:\/\/www.kaggle.com\/jessicali9530\/stanford-dogs-dataset (750 Mb)\n- debe utilizar Transfer Learning para entrenar su modelo.\n- si su modelo no sirve con transfer learning, debe intentar crear su propia arq convolucional.\n- el dataset cuenta con 120 diferentes tipos de razas, con 100 imagenes en cada folder. approx 20,580 imagenes"}}