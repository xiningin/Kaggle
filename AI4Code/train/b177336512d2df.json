{"cell_type":{"0bf72a37":"code","187570f6":"code","600ee7ab":"code","1f62e17f":"code","7bc01306":"code","b5dd88e5":"code","fb1f8222":"code","68fec910":"markdown","bb359033":"markdown","614190c8":"markdown","33b807fb":"markdown","072e11e0":"markdown","0ec44990":"markdown","8c130bf4":"markdown","65ab4686":"markdown","cd1a175e":"markdown","14cd403f":"markdown","67dce617":"markdown","a836488a":"markdown","644cc1f8":"markdown","7a567a2d":"markdown"},"source":{"0bf72a37":"from numpy import array\ndata = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\ndata = data.reshape((1, 10, 1))\nprint(data.shape)","187570f6":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf \nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, GRU, Flatten\n\nimg_rows, img_cols = 28, 28 \nnum_classes = 10\n\ndef prep_data(raw): \n    y = raw[:, 0]\n    out_y = keras.utils.to_categorical(y, num_classes)\n    \n    x = raw[:,1:] #gives the data as a numpy array\n    num_images = raw.shape[0]\n    out_x = x.reshape(num_images, img_rows, img_cols)\n    out_x = out_x \/ 255\n    return out_x, out_y\n\nfashion_file = \"..\/input\/fashionmnist\/fashion-mnist_train.csv\"\nfashion_data = np.loadtxt(fashion_file, skiprows=1, delimiter=',')\nx, y = prep_data(fashion_data)\nprint(\"Data Loaded\")","600ee7ab":"print(fashion_data.shape)\nprint(x.shape)\nprint(\"x.shape[1:]\", x.shape[1:])","1f62e17f":"model_LSTM = Sequential()\n\nmodel_LSTM.add(LSTM(units = 128, activation = 'relu', input_shape = (x.shape[1:]), return_sequences = False)) \n#units = 1 for a vanilla RNN as it is only one layer of the vectors described above\n#can have return_sequences=True in case we want to continue onto another LSTM, but False is the default\nmodel_LSTM.add(Dropout(0.2))\n#we are adding dropout to ignore randomly chosen nodes, this reduces overfitting\n#like in the Deep Learning course\n\nmodel_LSTM.add(Dense(12, activation = 'relu'))\nmodel_LSTM.add(Dropout(0.2))\n\nmodel_LSTM.add(Dense(num_classes, activation = 'softmax')) #prediction layer\n\nmodel_LSTM.compile(loss = keras.losses.categorical_crossentropy,\n                   optimizer = 'adam',\n                   metrics = ['accuracy'])\n\nmodel_LSTM.summary()","7bc01306":"model_LSTM.fit(x,y, batch_size = 100, epochs = 4, validation_split = 0.2) #validation_split = 0.2 means that we set \n#20% of the data aside for validation","b5dd88e5":"model_GRU = Sequential()\n\nmodel_GRU.add(GRU(128, activation = 'relu', input_shape = (x.shape[1:]), return_sequences = False)) \nmodel_GRU.add(Dropout(0.2))\n\nmodel_GRU.add(Flatten())\n\nmodel_GRU.add(Dense(12, activation = 'relu'))\nmodel_GRU.add(Dropout(0.2))\n\nmodel_GRU.add(Dense(num_classes, activation = 'softmax')) #prediction layer\n\nopt = tf.keras.optimizers.Adam(lr = 1e-3, decay = 1e-5)\n\nmodel_GRU.compile(loss = keras.losses.categorical_crossentropy,\n                   optimizer = opt,\n                   metrics = ['accuracy'])\n\nmodel_GRU.summary()","fb1f8222":"model_GRU.fit(x,y,batch_size = 100, epochs = 4, validation_split = 0.2)","68fec910":"An example of this is when we ask the neural network 'what time is it?' The RNN breaks the input up into a sequence, because that is how RNNs work: 'what', 'time', 'is', 'it', '?'\n\nSo the first step would be to produce an output for the word 'what', this would be done in the hidden state. Then the RNN remembers this output and produces an output for the word 'time', this is still in the hidden state.","bb359033":"![image.png](attachment:image.png)\nImage taken from: https:\/\/towardsdatascience.com\/recurrent-neural-networks-by-example-in-python-ffd204f99470","614190c8":"# Introduction\n\nI recently finished the Deep Learning Kaggle Course and quickly realised how incredible it is! It covers plenty of deep learning techniques, but I want to build on what I learned. \n\nThe next step mastering deep learning after doing the course seems to be Recurrent Neural Networks (RNN)... so here are my brief notes explaining what RNNs are and how to work with them using Keras.","33b807fb":"![image.png](attachment:image.png)","072e11e0":"The input into every LSTM layer myst be thre-dimensional. The three dimensional inputs are: \n\n* Samples - one or more samples\n* Time steps - one time step is one point of observation in the sample\n* Features - one feature is one observation at a time step\n\nWhen defining the LSTM, the network assumes you have one or more samples, and requires that you specify the number of time steps and features. You can do this by specifying a tuple to the \"input_shape\" argument. E.g. model.add(LSTM(32, input_shape = (50, 2))) means that there are 1 or more samples, 50 time steps and 2 features.\n\nAs we need a 3D array for the LSTM, we can use the *reshape()* function in NumPy:","0ec44990":"# What are Recurring Neural Networks? \n\nRecurring Neural Networks (RNNs) are neural networks that are good at modelling sequence data. Think of it this way: if you see a picture of a ball like the image below, how can you predict which direction it will go in next? \n\n![image.png](attachment:image.png)\nImage taken from: https:\/\/towardsdatascience.com\/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n\nWithout knowing where the ball has been, you can't really tell where the ball is going.\n\nRNNs are good at processing things that a in a sequence, like stock predictions, speech recognition and to describe content in pictures. \n\nRNNs have sequential memory, an example of this within humans is the alphabet. We can easily recite the alphabet in sequence: \nabcdefghijk... \nBut if we try it backwards (without practicing) then it is more difficult. \n\n*How do RNNs replicate sequential memory?*\n\nFeed Forward Neural Networks work in a flow, like this:","8c130bf4":"# References:\nhttps:\/\/towardsdatascience.com\/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n\nhttps:\/\/www.youtube.com\/watch?v=8HyCNIVRbSU \n\nhttps:\/\/towardsdatascience.com\/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n\nhttps:\/\/pythonprogramming.net\/recurrent-neural-network-deep-learning-python-tensorflow-keras\/\n\nhttps:\/\/machinelearningmastery.com\/reshape-input-data-long-short-term-memory-networks-keras\/","65ab4686":"# Conclusion\n\nIt makes more sense to ignore vanilla RNNs and just use LSTMs and GRUs, but RNNs have the key advantage of being able to train faster and use less computational resources as there are fewer tensor operations to compute. \n\nLSTMs and GRUs are standard and people generally try both and see which one works better for the data.","cd1a175e":"# LSTM and GRU\n\nLong Short Term Memory and Gated Recurrent Unit\n\n\nThey are a form of RNN but are capable of learning long-term dependencies using something called 'gates'. The RNNs described above are commonly called 'vanilla' RNNs. LSTMs and GRUs are used more often as they tackle the vanishing gradient problem, as they are useful for modelling longer sequences and long-term dependencies. \n\nThe following brief explanation and images are taken from the video:\nhttps:\/\/www.youtube.com\/watch?v=8HyCNIVRbSU \n\nAs you can see from the image below, both LSTMs and GRUs have 'gates' which regulate the flow of information, these gates learn what information to keep and what to discard. So like in the example mentioned earlier: 'what time is it?', the gate may put the most weight on the word 'time' and forget other filler words like 'is' or 'it'. Sort of like how a human would remember the question. \n\nTanh activation:\n\nTanh activation, shown in the image below, is a way to regulate the values flowing through the network. As the values assigned to the inputs undergo mathematical operations like multiplication, the values can increase exponentially. So running the values through the tanh function keeps the values between 1 and -1 so that the algorithm can have a better understanding of how important an input variable is. \n\nSigmoid activation: \n\nWorks similarly to tanh activation as it regulates the values so that they are between 0 and 1. The values that are closer to 0 get forgotten as they are so small and the values closer to 1 are remembered as they travel through the algorithm.\n\n**LSTMs:**\n\nAs you can see from the image below, LSTMs have three different types of gates: the forget gate, input gate and output gate. The forget gate decides what information should be thrown away or kept. \n\nIn the input gate the information from the previous cell\/hidden state and information from the current input is passed through the sigmoid function to figure out if the information is important. The same information is passed through the tanh function to help regulate the network.\n\nThe cell state gets updated as it has enough information from the forget and input gates, it basically drops more irrelevant information. \n\nThe outputs from the cell state and the input gate are now fed to the output gate which decides what the next hidden state should be - what should be passed onto the next vector. \n\nThe diagram shows one part of the process, there are plenty of these vectors that are strung together, one after another.\n\n![image.png](attachment:image.png)\nImage taken from: https:\/\/towardsdatascience.com\/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n\n**GRUs:**\n\nNewer generation of RNNs and is pretty similar to an LSTM, but it got rid of the cell state and hidden state to transfer information. It has 2 gates: a reset gate and an update gate. These can be seen in the diagram above. \n\nThe update gate decides what information to remember or forget - like combining the LSTM forget and input gates.\n\nThe reset gate is another gate that decides what information to forget. \n\nGRUs have fewer tensor operations - so they are quicker to train than LSTMs.","14cd403f":"Image taken from: https:\/\/towardsdatascience.com\/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n\nRNNs loop back in the hidden state:","67dce617":"We can print the array to check that it is 3D.\n\nThis data is now ready to be used and has input_shape = (10, 1).\n\nNow, we shall move onto a more complex example of the MNIST Fashion data that was used in the 'Deep Learning' course","a836488a":"It should be noted that CuDNNLSTM is a much faster way to run LSTM.\n\n\nGRU Example and some useful tips:\n\nDecay: \n\nYou want to take those larger steps in your learning rate to get to the minima, but if we took smaller steps as time goes by, then we could get to the minima faster. Decay essentially decays the learning rate a little bit.\n\nWe keep the the optimizer as Adam in this case.","644cc1f8":"![image.png](attachment:image.png)\nImage taken from: https:\/\/towardsdatascience.com\/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n\nThis continues until all steps of the input have ran, these loops are all in the hidden layer. A final output is then produced. This can be seen in the image above. \n\nWith each loop through the hidden layer, the RNN forgets a little more about what happened at the start of the sequence. This is because RNNs have **short term memory**. This leads us onto...\n\n# The vanishing gradient problem\n\nNeural networks have three major steps: \n1. a forward pass to make a prediction\n2. comparing the prediction to the ground truth using the loss function - producing an error value to show how well the model is performing\n3. uses the error value from step 2 to do backpropagation and improve the model\n\nTo understand more about backpropagation, check out the amazing video: https:\/\/www.youtube.com\/watch?v=Ilg3gGewQ5U&t=711s\n\nBackpropagation works by using the gradient to adjust the weights of each layer, it works back through the network, i.e. in the direction of the output to the hidden layers to the input layer. As the adjustments start at the end, each adjustmet after gets smaller and smaller, so the layers at the end of the backpropagation have the least learning. \n\nThis means that the earlier layers fail to do a lot of learning because the internal weights are barely being adjusted due to small gradients, this is the **vanishing gradient problem**.\n\nLinking back to the example: the RNN forgets the start of the statement, and the neural network is trying to find an output with the input 'is it?'. This would be very difficult, even for a human!\n\nLSTM and GRU are more commonly used methods that combat the memory problem...","7a567a2d":"# Tensorflow Example Code\n\nJust to help out with understanding the theory, I have wrote some super basic Keras LSTM and GRU examples. I am sure there are ways to get better results, like those mentioned on the 'Deep Learning' course, but this is a useful starting point."}}