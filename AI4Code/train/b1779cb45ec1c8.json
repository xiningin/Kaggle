{"cell_type":{"5553951d":"code","1b0b1c66":"code","2bed8406":"code","047a1bcb":"code","4e6b5be7":"code","c3012abf":"code","1853c5fb":"code","3d20363b":"code","f5706a61":"code","dc62f357":"code","4c482c63":"code","c9397ef9":"code","e5b4487f":"code","b0ca07fb":"code","7ceeaefe":"code","e24717e7":"markdown","9aa9c789":"markdown","4d25a99e":"markdown","97e20819":"markdown","ff2c6401":"markdown","fc3924ac":"markdown"},"source":{"5553951d":"import torch\nfrom torch import nn\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torchaudio\nimport os\nimport copy\nimport random\nfrom random import shuffle\nimport math\nimport IPython\nfrom IPython.display import Audio\n\n#para contar o tempo at\u00e9 o inicio do treinamento\n#import time\n#start_program_time = time.time()\n###############################\n\n#DEFINICAO DE VARIAVEIS\ndevice = 'cuda' #dispositivo no qual o programa sera executado (cuda para rodar em GPU ou cpu para rodar no processador)\n\n#definicao da GAN\nz_dim = 1079 #dimensao do vetor de entrada do gerador\nc_lambda = 7 #usado na funcao de perda do critico\n\n#treinamento\ncrit_repeats = 5 #numero de vezes que o critico repete por epoca antes do gerador treinar\nn_epochs = 19 #numero de epocas\nbatch_size = 20 #tamanho do batch\n\n\n#dados\naudio_len = 1053120\ntext_len = 471\nsample_rate = 24000\ndata_fraction = 15 #use only 1\/data_fraction of the data\n\n#otimizadores\nlr = 0.0002\nbeta_1 = 0.5\nbeta_2 = 0.999\n","1b0b1c66":"#list of paths for training set\npathlist = open('\/kaggle\/input\/tg-training-state\/pathlist.txt').read().split('\\n')\nstart_epoch = int(pathlist[0])\npathlist = pathlist[1:]\npathlist = [p.replace('\\n','') for p in pathlist]\nrandom.shuffle(pathlist)\n#print(pathlist[0])\nlast_len = len(pathlist)\n\n##The code below only needs to be run once\n'''\n#I don't know why some files don't have the corresponding text\n#so I need to ignore these files otherwise it will raise a filenotfound error \n\npathlist = [p for p in pathlist if (os.path.exists(p.replace('wav','normalized.txt')) and (os.path.exists(p)))]\n#original_pathlist = pathlist\nprint('ignoring ' + str(last_len - len(pathlist)) + ' files.')\nprint(len(pathlist))\n'''","2bed8406":"#removing files outside the desired sample\n#import librosa\n#pathlist = [p for p in pathlist if ((librosa.get_duration(filename=p)>5.6) and (librosa.get_duration(filename=p)<5.7))]\n#original_pathlist = pathlist","047a1bcb":"def load_text_files(pathlist):\n    return [open(line.replace('wav','normalized.txt')).read() for line in pathlist]\n\ndef load_wav_files(pathlist):\n    return [list(torchaudio.load_wav(path)[0]) for path in pathlist]\n\ndef get_max_len(text_list):\n    return max([len(s) for s in text_list])\n\ndef preprocess_text(text_content,max_len):\n    char_encoded = np.zeros((len(text_content),max_len))\n    for i in range(len(text_content)):\n        for j in range(len(text_content[i])):\n            char_encoded[i,j] = ord(text_content[i][j])\n    return char_encoded\n\ndef preprocess_audio(audiolist, max_audio_len):\n    ret = np.zeros((len(audiolist),1,max_audio_len))\n    for i in range(len(audiolist)):\n        for j in range(len(audiolist[i])):\n            ret[i,0,j]=audiolist[i][0][j]\n    return ret","4e6b5be7":"def get_text_batch(pathlist,batch_size,batch_index,max_text_len):\n    text_content = load_text_files(pathlist[batch_index*batch_size:min(len(pathlist),batch_index*batch_size+batch_size)])\n    text_list = preprocess_text(text_content,max_text_len)\n    return text_list\n\ndef get_audio_batch(pathlist,batch_size,batch_index,max_audio_len):\n    audiolist = load_wav_files(pathlist[batch_index*batch_size:min(len(pathlist),batch_index*batch_size+batch_size)])\n    audiolist = preprocess_audio(audiolist, max_audio_len)\n    return audiolist\n\ndef get_batches(pathlist,batch_size,max_audio_len,max_text_len):\n    for batch_index in range(math.ceil(len(pathlist)\/batch_size)):\n        text = get_text_batch(pathlist,batch_size,batch_index,max_text_len)\n        audio = get_audio_batch(pathlist,batch_size,batch_index,max_audio_len)\n        yield (text,audio)","c3012abf":"def display_results(text,fake,how_many,sample_rate):\n    tuples = [t for t in zip(text,fake)] #tuples to associate the texts to the corresponding audios\n    sample_to_display = random.sample(tuples,how_many)\n    i=0\n    for txt,audio in sample_to_display:\n        txt_string=''.join([chr(o) for o in txt[0] if o>0])\n        print('text '+str(i)+':')\n        print(txt_string)\n        print()\n        IPython.display.display(Audio(audio.detach().to('cpu').numpy(),rate=sample_rate))\n        i+=1","1853c5fb":"##Definicao do Gerador\nclass Generator(nn.Module):\n    def __init__(self, z_dim=1079):\n        super(Generator, self).__init__()\n        self.z_dim = z_dim\n        # Network that extracts high level features from the text\n        self.text_handler = self.gen = nn.Sequential(\n            self.make_text_handler_block(stride=25,kernel_size=31,final_layer=True)\n        )\n        # Build the neural network\n        self.gen = nn.Sequential(\n            self.make_gen_block(stride=960, kernel_size=960,final_layer=True)\n        )\n        \n    def make_gen_block(self, kernel_size=3, stride=1, final_layer=False):\n        '''\n        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n        Parameters:\n            kernel_size: the size of each convolutional filter\n            stride: the stride of the convolution\n            final_layer: a boolean, true if it is the final layer and false otherwise \n                      (affects activation and batchnorm)\n        '''\n        if not final_layer:\n            return nn.Sequential(\n                nn.ConvTranspose1d(1,1,kernel_size=kernel_size, stride=stride),\n                nn.BatchNorm1d(1),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            return nn.Sequential(\n                nn.ConvTranspose1d(1,1,kernel_size=kernel_size, stride=stride)\n            )\n\n    def make_text_handler_block(self, kernel_size=3, stride=1, final_layer=False):\n        '''\n        Function to extract high level features from the text\n        Parameters:\n            kernel_size: the size of each convolutional filter\n            stride: the stride of the convolution\n            final_layer: a boolean, true if it is the final layer and false otherwise \n                      (affects activation and batchnorm)\n        '''\n        if not final_layer:\n            return nn.Sequential(\n                nn.Conv1d(1,1,kernel_size=kernel_size, stride=stride),\n                nn.BatchNorm1d(1),\n                nn.LeakyReLU(0.2,inplace=True),\n            )\n        else:\n            return nn.Sequential(\n                nn.Conv1d(1,1,kernel_size=kernel_size, stride=stride)\n            )\n\n    def forward(self, text, noise):\n        '''\n        Function for completing a forward pass of the generator: Given a noise tensor,\n        returns generated images.\n        Parameters:\n            noise: a noise tensor with dimensions (n_samples, z_dim)\n        '''\n        text_features = self.text_handler(text)\n        x = torch.cat((text_features,noise),2)\n        return self.gen(x)\n\ndef get_noise(n_samples,z_dim,device='cpu'):\n    return torch.randn((n_samples,1,z_dim),device=device)\n  \n##Definicao do Critico\nclass Critic(nn.Module):\n    def __init__(self):\n        super(Critic, self).__init__()\n        self.crit = nn.Sequential(\n            #se nao me engano nao precisa nem ser um numero real: pode ser qq coisa\n            self.make_crit_block(final_layer=True),\n        )\n\n    def make_crit_block(self, kernel_size=4, stride=2, final_layer=False):\n        '''\n        Function to return a sequence of operations corresponding to a critic block of DCGAN;\n        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n        Parameters:\n            kernel_size: the size of each convolutional filter\n            stride: the stride of the convolution\n            final_layer: a boolean, true if it is the final layer and false otherwise \n                      (affects activation and batchnorm)\n        '''\n        if not final_layer:\n            return nn.Sequential(\n                nn.Conv1d(1,1,kernel_size=kernel_size, stride=stride),\n                nn.BatchNorm1d(1),\n                nn.LeakyReLU(0.2,inplace=True),\n            )\n        else:\n            return nn.Sequential(\n                nn.Conv1d(1,1,kernel_size=kernel_size, stride=stride)\n            )\n\n    def forward(self, text, audio):\n        '''\n        Function for completing a forward pass of the critic: Given an image tensor, \n        returns a 1-dimension tensor representing fake\/real.\n        Parameters:\n            image: a flattened image tensor with dimension (im_chan)\n        '''\n        #print(text.size(),audio.size())\n        x = torch.cat((text,audio),2)\n        crit_pred = self.crit(x)\n        return crit_pred.view(len(crit_pred), -1)","3d20363b":"gen = Generator(z_dim).to(device)\n#gen = torch.load('\/kaggle\/input\/tg-training-state\/gen.pt').to(device)\ngen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\ncrit = Critic().to(device)\n#crit = torch.load('\/kaggle\/input\/tg-training-state\/crit.pt').to(device)\ncrit_opt = torch.optim.Adam(crit.parameters(), lr=lr, betas=(beta_1, beta_2))\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n    if isinstance(m, nn.BatchNorm2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias, 0)\ngen = gen.apply(weights_init)\ncrit = crit.apply(weights_init)","f5706a61":"def get_gradient(crit, real, fake, epsilon, text):\n    '''\n    Return the gradient of the critic's scores with respect to mixes of real and fake images.\n    Parameters:\n        crit: the critic model\n        real: a batch of real images\n        fake: a batch of fake images\n        epsilon: a vector of the uniformly random proportions of real\/fake per mixed image\n    Returns:\n        gradient: the gradient of the critic's scores, with respect to the mixed image\n    '''\n    # Mix the audios together\n    #print(real.size(),fake.size(),epsilon.size())\n    mixed_audio = real * epsilon + fake * (1 - epsilon)\n    #print(text.size(),mixed_audio.size())\n    # Calculate the critic's scores on the mixed images\n    mixed_scores = crit(text,mixed_audio)\n    \n    # Take the gradient of the scores with respect to the images\n    gradient = torch.autograd.grad(\n        inputs=mixed_audio,\n        outputs=mixed_scores,\n        # These other parameters have to do with the pytorch autograd engine works\n        grad_outputs=torch.ones_like(mixed_scores), \n        create_graph=True,\n        retain_graph=True,\n    )[0]\n    return gradient\ndef gradient_penalty(gradient):\n    '''\n    Return the gradient penalty, given a gradient.\n    Given a batch of image gradients, you calculate the magnitude of each image's gradient\n    and penalize the mean quadratic distance of each magnitude to 1.\n    Parameters:\n        gradient: the gradient of the critic's scores, with respect to the mixed image\n    Returns:\n        penalty: the gradient penalty\n    '''\n    # Flatten the gradients so that each row captures one image\n    gradient = gradient.view(len(gradient), -1)\n\n    # Calculate the magnitude of every row\n    gradient_norm = gradient.norm(2, dim=1)\n    \n    # Penalize the mean squared distance of the gradient norms from 1\n    penalty = torch.mean((gradient_norm-torch.ones_like(gradient_norm))**2)\n    return penalty","dc62f357":"def get_gen_loss(crit_fake_pred):\n    '''\n    Return the loss of a generator given the critic's scores of the generator's fake images.\n    Parameters:\n        crit_fake_pred: the critic's scores of the fake images\n    Returns:\n        gen_loss: a scalar loss value for the current batch of the generator\n    '''\n    return -torch.mean(crit_fake_pred)\ndef get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda):\n    '''\n    Return the loss of a critic given the critic's scores for fake and real images,\n    the gradient penalty, and gradient penalty weight.\n    Parameters:\n        crit_fake_pred: the critic's scores of the fake images\n        crit_real_pred: the critic's scores of the real images\n        gp: the unweighted gradient penalty\n        c_lambda: the current weight of the gradient penalty \n    Returns:\n        crit_loss: a scalar for the critic's loss, accounting for the relevant factors\n    '''\n    return -(torch.mean(crit_real_pred)-torch.mean(crit_fake_pred))+gp*c_lambda","4c482c63":"def train_critic(text,audio,gen,crit,crit_opt,cur_batch_size,z_dim):\n    ### Update critic ###\n    crit_opt.zero_grad()\n    fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n    fake = gen(text,fake_noise)\n    crit_fake_pred = crit(text,fake.detach())\n    crit_real_pred = crit(text,audio)\n\n    epsilon = torch.rand(cur_batch_size, 1, 1, device=device, requires_grad=True)\n    gradient = get_gradient(crit, audio, fake.detach(), epsilon, text)\n    gp = gradient_penalty(gradient)\n    crit_loss = get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda)\n    loss = crit_loss.item()\n    # Update gradients\n    crit_loss.backward(retain_graph=True)\n    # Update optimizer\n    crit_opt.step()\n    return loss","c9397ef9":"def train_generator(text,gen,crit,gen_opt,cur_batch_size,z_dim):\n    ### Update generator ###\n    gen_opt.zero_grad()\n    fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n    fake = gen(text,fake_noise)\n    crit_fake_pred = crit(text,fake)\n        \n    gen_loss = get_gen_loss(crit_fake_pred)\n    gen_loss.backward()\n\n    # Update the weights\n    gen_opt.step()\n    return (gen_loss.item(),fake)","e5b4487f":"def train(n_epochs,gen,crit,gen_opt,crit_opt,z_dim,pathlist,batch_size,max_audio_len,max_text_len):\n    generator_losses = []\n    critic_losses = []\n    #epoch=0\n    for epoch in range(n_epochs):\n    #while ((time.time()-start_time)<14400):\n        batches = get_batches(pathlist,batch_size,max_audio_len,max_text_len)\n        for text, audio in tqdm(batches):\n            #print(text)\n            cur_batch_size = len(text)\n            text = torch.Tensor(text).view(cur_batch_size,1,len(text[0])).to(device)\n            #print(audio)\n            audio = torch.Tensor(audio).to(device)\n            #print(audio.size())\n            mean_iteration_critic_loss = 0\n            for _ in range(crit_repeats):\n                crit_loss = train_critic(text,audio,gen,crit,crit_opt,cur_batch_size,z_dim)\n                # Keep track of the average critic loss in this batch\n                mean_iteration_critic_loss += crit_loss \/ crit_repeats\n            \n            critic_losses += [mean_iteration_critic_loss]\n                    \n            gen_loss, fake = train_generator(text,gen,crit,gen_opt,cur_batch_size,z_dim)\n            # Keep track of the average generator loss\n            generator_losses += [gen_loss]\n        #epoch+=1\n    return (generator_losses,critic_losses, text, fake)","b0ca07fb":"def visualize(generator_losses,critic_losses,text,fake,how_many,sample_rate):\n    \n    display_results(text,fake,how_many,sample_rate)\n    \n    gen_mean = sum(generator_losses[-len(generator_losses):]) \/ len(generator_losses)\n    crit_mean = sum(critic_losses[-len(generator_losses):]) \/ len(generator_losses)\n    print(f\"Generator loss: {gen_mean}, critic loss: {crit_mean}\")\n    step_bins = 20\n    num_examples = (len(generator_losses) \/\/ step_bins) * step_bins\n    plt.plot(\n        range(num_examples \/\/ step_bins), \n        torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n        label=\"Generator Loss\"\n    )\n    plt.plot(\n        range(num_examples \/\/ step_bins), \n        torch.Tensor(critic_losses[:num_examples]).view(-1, step_bins).mean(1),\n        label=\"Critic Loss\"\n    )\n    plt.legend()\n    plt.show()","7ceeaefe":"generator_losses,critic_losses, text, fake = train(n_epochs,gen,crit,gen_opt,crit_opt,z_dim,pathlist,batch_size,audio_len,text_len)\n#print('time before training: ' + str(time.time()-start_program_time))\n#start_time=time.time()\n#generator_losses,critic_losses, text, fake, last_epoch = train(start_time,gen,crit,gen_opt,crit_opt,z_dim,pathlist,batch_size,audio_len,text_len)\n\nvisualize(generator_losses,critic_losses,text,fake,1,sample_rate)\n\nlast_epoch = start_epoch + n_epochs\nlist_to_save = [str(last_epoch)]+original_pathlist\n\nwith open('pathlist.txt','w') as f:\n    f.write('\\n'.join(list_to_save))\n\ntorch.save(gen,'gen.pt')\ntorch.save(crit,'crit.pt')","e24717e7":"## Training functions","9aa9c789":"# The actual training code","4d25a99e":"# Generator and Critic definition and initialization","97e20819":"# Auxiliary functions","ff2c6401":"# Imports e vari\u00e1veis globais","fc3924ac":"# Functions to be used during training"}}