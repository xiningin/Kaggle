{"cell_type":{"06318269":"code","4875a0d2":"code","c7a44ec7":"code","223ce1b0":"code","f574b298":"code","cb513055":"code","fcf02a4a":"code","26cde81d":"code","5956e6a0":"code","455c7ef0":"code","be29d96b":"code","9f4942fc":"code","96a17b17":"code","b0a85669":"code","3f418cb0":"code","c70db7c2":"code","c32906f5":"code","3cb13622":"code","d17496aa":"code","52e63e81":"code","5fe04b15":"code","8543af09":"code","28a4829a":"code","df7695fd":"code","50f9a4e1":"code","51a97113":"code","fa045d6a":"code","1944e435":"code","02115a77":"code","f57745d5":"code","bce5a4a0":"code","4470b5e0":"code","41117cf6":"code","d516826f":"code","9d05cd36":"code","980adf67":"code","a5a35190":"code","b93f630b":"code","f961e00a":"code","229f65ff":"code","9c7f0952":"code","e8d1e549":"code","f9127e79":"code","75937e90":"code","9565aedb":"code","53f40a7f":"code","95c9683f":"code","23e7ca4f":"code","2aac8dd9":"code","3318bcb1":"code","a21c3f4d":"code","ec9cba99":"code","cd4772f5":"code","0e5d2a63":"code","5a2f5b2c":"code","4592b36d":"code","d3ae7944":"code","8aaff9c7":"code","6703d9d3":"code","fc843f30":"code","b6a5cad3":"code","2c333714":"code","b047bd50":"code","db522020":"code","ebb4e027":"code","f40cc0ff":"code","e62f64fe":"code","d84758b2":"code","8cbab467":"code","52e544ee":"code","28f1ae12":"code","8ac466ae":"code","f2958c06":"code","ad883599":"code","6d77b524":"code","8473de48":"code","b640979a":"code","eacde0bb":"code","1c5eee0a":"code","6a4cbc8a":"code","cb20a276":"code","7baa5790":"code","90132010":"code","77b28d40":"code","864ab210":"code","1f3857a5":"code","a65d2219":"code","4f954a7c":"code","7598fdcd":"code","7eb1ca79":"code","68593861":"code","82ceec5a":"code","6e98d914":"code","c8a89d78":"code","38eba5d7":"code","d84c7565":"code","bf155a5e":"code","f3402742":"code","804f24f5":"code","2b6e8565":"code","a57dac11":"code","9be01afb":"code","8fc5f004":"code","09c45873":"code","b7278836":"code","0da25f67":"code","3e78f1c0":"code","96dd26f3":"code","55fd9b1b":"code","cbc2a0ca":"code","a1771f55":"code","e9472293":"code","467f8b5e":"code","1a0d51a5":"code","76108154":"code","d7fdd853":"code","a1ba9178":"code","95b90d9b":"code","455b79b4":"markdown","efeab916":"markdown","77ebe6a3":"markdown","091bd3f4":"markdown","407d3de2":"markdown","54706735":"markdown","f6204327":"markdown","b330df76":"markdown","d2f2b1bb":"markdown","52650fb0":"markdown","c78f49a5":"markdown","ae885a8c":"markdown","149d5b76":"markdown","800c4568":"markdown","562e507b":"markdown","5d3a5933":"markdown","786823e2":"markdown","69452312":"markdown","d6ec7eb4":"markdown","62984d53":"markdown","2f313d62":"markdown","bb227677":"markdown","b9294266":"markdown","4c4c9cd2":"markdown","4d783976":"markdown","961c2e33":"markdown","ce7aec47":"markdown","84abcd81":"markdown","91054f9c":"markdown"},"source":{"06318269":"import pandas as pd\nimport numpy as np\nimport math\nimport string\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import GridSearchCV, ParameterGrid\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport category_encoders as ce\n\nfrom sys import getsizeof\nfrom datetime import datetime\nimport os ","4875a0d2":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c7a44ec7":"train_data = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/test.csv')","223ce1b0":"print('Train data size : {} MB'.format(round(getsizeof(train_data) \/ 1024 \/ 1024)))\nprint('Test data size : {} MB'.format(round(getsizeof(test_data) \/ 1024 \/ 1024)))","f574b298":"print(train_data.shape)\nprint(test_data.shape)","cb513055":"train_data.head()","fcf02a4a":"test_data.head()","26cde81d":"# List down the ord_5 variable values with different cases.\n\ntmp_df = pd.DataFrame(train_data['ord_5'].unique(), columns = ['val'])\ntmp_df['trans_val'] = tmp_df['val'].str.upper()\n\nlist_1 = []\n\nfor trans_val in tmp_df['trans_val'].unique():\n    val_cnt = sum(tmp_df['trans_val'] == trans_val)\n    \n    for val in tmp_df.loc[(tmp_df['trans_val'] == trans_val) & (tmp_df['val'] != trans_val), 'val']:\n        if val_cnt == 1:\n            break\n    list_1.append(val)","5956e6a0":"list_1[:20]","455c7ef0":"tmp_df.loc[tmp_df['trans_val'] == 'AP', ]","be29d96b":"# Variables to exclude from change case operation.\nexcl_cols = ['ord_5']\n\nfilter_cond = (train_data.dtypes == 'object') & (~train_data.dtypes.index.isin(excl_cols))\ncols_for_change_case = train_data.dtypes[filter_cond].index.tolist()\ncols_for_change_case","9f4942fc":"# Create a data frame to hold unique value count for the selected variables of train data set before changing values to lower case.\n\nbef_tr_varunq_count = pd.DataFrame(train_data.loc[:, cols_for_change_case].nunique().sort_values(ascending = False))\nbef_tr_varunq_count.reset_index(inplace = True)\nbef_tr_varunq_count.rename(columns = {'index':'variable', 0:'unique_val_count'}, inplace = True)","96a17b17":"# Create a data frame to hold unique value count for the selected variables of test data set before changing values to lower case.\n\nbef_ts_varunq_count = pd.DataFrame(test_data.loc[:, cols_for_change_case].nunique().sort_values(ascending = False))\nbef_ts_varunq_count.reset_index(inplace = True)\nbef_ts_varunq_count.rename(columns = {'index':'variable', 0:'unique_val_count'}, inplace = True)","b0a85669":"# Change values of the train data set variables to lower case.\n\ntrain_data.loc[:, cols_for_change_case] = train_data.loc[:, cols_for_change_case].apply(lambda x:x.astype(str).str.lower())","3f418cb0":"train_data.head()","c70db7c2":"# Change values of the test data set variables to lower case.\n\ntest_data.loc[:, cols_for_change_case] = test_data.loc[:, cols_for_change_case].apply(lambda x:x.astype(str).str.lower())","c32906f5":"test_data.head()","3cb13622":"# Create a data frame to hold unique value count for the selected variables of train data set after changing values to lower case.\n\naft_tr_varunq_count = pd.DataFrame(train_data.loc[:, cols_for_change_case].nunique().sort_values(ascending = False))\naft_tr_varunq_count.reset_index(inplace = True)\naft_tr_varunq_count.rename(columns = {'index':'variable', 0:'unique_val_count'}, inplace = True)","d17496aa":"# Create a data frame to hold unique value count for the selected variables of test data set after changing values to lower case.\n\naft_ts_varunq_count = pd.DataFrame(test_data.loc[:, cols_for_change_case].nunique().sort_values(ascending = False))\naft_ts_varunq_count.reset_index(inplace = True)\naft_ts_varunq_count.rename(columns = {'index':'variable', 0:'unique_val_count'}, inplace = True)","52e63e81":"aft_tr_varunq_count.shape","5fe04b15":"bef_tr_varunq_count.shape","8543af09":"# Joining unique value counts of the train data set variables before and after changing case\n# to find out how many variables do not match on the basis of unique value counts.\n\naft_tr_varunq_count.merge(bef_tr_varunq_count, \n                          right_on = ['variable', 'unique_val_count'], \n                          left_on = ['variable', 'unique_val_count'])['variable'].count()","28a4829a":"# Joining unique value counts of the test data set variables before and after changing case\n# to find out how many variables do not match on the basis of unique value counts.\n\naft_ts_varunq_count.merge(bef_ts_varunq_count, \n                          right_on = ['variable', 'unique_val_count'], \n                          left_on = ['variable', 'unique_val_count'])['variable'].count()","df7695fd":"# Bring unique value count of both train and test data sets together after changing values of the variables to lower case.\n# We are going this to figure out if there are any variables in test data set with fewer unique value counts than train data set.\n\ntmp_df = aft_tr_varunq_count.merge(aft_ts_varunq_count,\n                                   how = 'inner',\n                                   left_on = 'variable',\n                                   right_on = 'variable',\n                                   suffixes = ['_tr', '_ts'])","50f9a4e1":"print('No. of variables with unique value count of train data set less than test data set : {}'.\\\n      format(sum(tmp_df['unique_val_count_tr'] < tmp_df['unique_val_count_ts'])))","51a97113":"print('Train data size : {} MB'.format(round(getsizeof(train_data) \/ 1024 \/ 1024)))\nprint('Test data size : {} MB'.format(round(getsizeof(test_data) \/ 1024 \/ 1024)))\n\n# Before changing case of the data, sizes of train and test data set were 332 MB and 220 MB.","fa045d6a":"# Deleting variables which are no longer required.\n\ndel tmp_df, bef_tr_varunq_count, aft_tr_varunq_count, bef_ts_varunq_count, aft_ts_varunq_count, cols_for_change_case","1944e435":"# Spliting two character value into two single character values and converting them into numbers according\n# to sequence of letters defined in string.ascii_letters did not help me much in improving the score.\n\n# train_data['ord_5_1'] = train_data['ord_5'].apply(lambda x : string.ascii_letters.index(x[0]) + 1)\n# train_data['ord_5_2'] = train_data['ord_5'].apply(lambda x : string.ascii_letters.index(x[1]) + 1)\n\n# test_data['ord_5_1'] = test_data['ord_5'].apply(lambda x : string.ascii_letters.index(x[0]) + 1)\n# test_data['ord_5_2'] = test_data['ord_5'].apply(lambda x : string.ascii_letters.index(x[1]) + 1)\n\n\n# Split two character values of ord_5 variable into two single character values.\n\ntrain_data['ord_5_1'] = train_data['ord_5'].apply(lambda x : x[0])\ntrain_data['ord_5_2'] = train_data['ord_5'].apply(lambda x : x[1])\n\ntrain_data.drop(columns = 'ord_5', inplace = True)\n\ntest_data['ord_5_1'] = test_data['ord_5'].apply(lambda x : x[0])\ntest_data['ord_5_2'] = test_data['ord_5'].apply(lambda x : x[1])\n\ntest_data.drop(columns = 'ord_5', inplace = True)","02115a77":"# Transforming ord_5 two character values into concatenated numeric values of the corresponding \n# string.ascii_letters value did not improve the score of the model. Hence commenting the code.\n\n# train_data['ord_5_1_2'] = train_data['ord_5'].apply(lambda x : str(string.ascii_letters.index(x[0]) + 1) + str(string.ascii_letters.index(x[1]) + 1))\n# test_data['ord_5_1_2'] = test_data['ord_5'].apply(lambda x : str(string.ascii_letters.index(x[0]) + 1) + str(string.ascii_letters.index(x[1]) + 1))\n\n# train_data['ord_5_1_2'] = train_data['ord_5_1_2'].astype('int64')\n# test_data['ord_5_1_2'] = test_data['ord_5_1_2'].astype('int64')","f57745d5":"# Transforming ord_5 two character values into sum of numeric values of the corresponding \n# string.ascii_letters value did not improve the score of the model. Hence commenting the code.\n\n# train_data['ord_5_1plus2'] = train_data['ord_5'].apply(lambda x : (string.ascii_letters.index(x[0]) + 1) + (string.ascii_letters.index(x[1]) + 1))\n# test_data['ord_5_1plus2'] = test_data['ord_5'].apply(lambda x : (string.ascii_letters.index(x[0]) + 1) + (string.ascii_letters.index(x[1]) + 1))","bce5a4a0":"# Transforming ord_5 two character values into product of numeric values of the corresponding \n# string.ascii_letters value did not improve the score of the model. Hence commenting the code.\n\n# train_data['ord_5_1mult2'] = train_data['ord_5'].apply(lambda x : (string.ascii_letters.index(x[0]) + 1) * (string.ascii_letters.index(x[1]) + 1))\n# test_data['ord_5_1mult2'] = test_data['ord_5'].apply(lambda x : (string.ascii_letters.index(x[0]) + 1) * (string.ascii_letters.index(x[1]) + 1))","4470b5e0":"# train_data[['ord_5', 'ord_5_1_2', 'ord_5_1plus2', 'ord_5_1mult2']].head()","41117cf6":"# Transforming day and month variables containing cyclical values into actual numeric values \n# did not improve the score of the model. Hence commenting the code.\n\n# # For train data set\n\n# train_data['day_sin'] = np.sin(train_data['day'] * (2. * np.pi \/ train_data['day'].max()))\n# train_data['day_cos'] = np.cos(train_data['day'] * (2. * np.pi \/ train_data['day'].max()))\n\n# train_data['month_sin'] = np.sin((train_data['month']) * (2. * np.pi \/ train_data['month'].max()))\n# train_data['month_cos'] = np.cos((train_data['month']) * (2. * np.pi \/ train_data['month'].max()))\n\n# # For test data set\n\n# test_data['day_sin'] = np.sin(test_data['day'] * (2. * np.pi \/ test_data['day'].max()))\n# test_data['day_cos'] = np.cos(test_data['day'] * (2. * np.pi \/ test_data['day'].max()))\n\n# test_data['month_sin'] = np.sin((test_data['month']) * (2. * np.pi \/ test_data['month'].max()))\n# test_data['month_cos'] = np.cos((test_data['month']) * (2. * np.pi \/ test_data['month'].max()))","d516826f":"# Print unique value counts for each of the bin* variables for train data set.\n\nfor col in train_data.columns[train_data.columns.str.contains('bin*')]:\n    print(train_data[col].value_counts())\n    print(train_data[col].dtype)","9d05cd36":"# Print unique value counts for each of the bin* variables for test data set.\n\nfor col in test_data.columns[test_data.columns.str.contains('bin*')]:\n    print(test_data[col].value_counts())\n    print(test_data[col].dtype)","980adf67":"# Map value 't' and 'f' to 1 and 0 respectively.\n\ntrain_data['bin_3'] = train_data['bin_3'].apply(lambda x : 1 if x == 't' else 0)\ntest_data['bin_3'] = test_data['bin_3'].apply(lambda x : 1 if x == 't' else 0)","a5a35190":"# Map value 'y' and 'n' to 1 and 0 respectively.\n\ntrain_data['bin_4'] = train_data['bin_4'].apply(lambda x : 1 if x == 'y' else 0)\ntest_data['bin_4'] = test_data['bin_4'].apply(lambda x : 1 if x == 'y' else 0)","b93f630b":"# Converting ord_3 and ord_4 values into corresponding string.ascii_letters numeric values \n# did not help me improve score of the model. Hence, commenting the code.\n\n# train_data['ord_3'] = train_data['ord_3'].apply(lambda x : string.ascii_letters.index(x[0]) + 1)\n# train_data['ord_4'] = train_data['ord_4'].apply(lambda x : string.ascii_letters.index(x[0]) + 1)\n\n# test_data['ord_3'] = test_data['ord_3'].apply(lambda x : string.ascii_letters.index(x[0]) + 1)\n# test_data['ord_4'] = test_data['ord_4'].apply(lambda x : string.ascii_letters.index(x[0]) + 1)","f961e00a":"train_data['ord_3_4'] = train_data['ord_3'] + train_data['ord_4']\ntest_data['ord_3_4'] = test_data['ord_3'] + test_data['ord_4']","229f65ff":"excl_cols = ['id', 'target']\nexcl_cols = excl_cols + ['bin_' + str(i) for i in range(5)]","9c7f0952":"# excl_cols","e8d1e549":"# Create a data frame listing unique value count and data type of each variable of train data set.\n\ntr_unique_val_cnt_df = pd.DataFrame(train_data.loc[:, ~train_data.columns.isin(excl_cols)].nunique().sort_values(ascending = False))\ntr_unique_val_cnt_df.reset_index(inplace =  True)\ntr_unique_val_cnt_df.rename(columns = {'index':'variable', 0:'unique_val_count'}, inplace = True)\ntr_unique_val_cnt_df = tr_unique_val_cnt_df.merge(pd.DataFrame(train_data.dtypes).reset_index().rename(columns = {'index':'variable', 0:'dtype'}),\n                                                 right_on = 'variable', left_on = 'variable')","f9127e79":"# Create a data frame listing unique value count and data type of each variable of test data set.\n\nts_unique_val_cnt_df = pd.DataFrame(test_data.loc[:, ~test_data.columns.isin(excl_cols)].nunique().sort_values(ascending = False))\nts_unique_val_cnt_df.reset_index(inplace = True)\nts_unique_val_cnt_df.rename(columns = {'index':'variable', 0:'unique_val_count'}, inplace = True)\nts_unique_val_cnt_df = ts_unique_val_cnt_df.merge(pd.DataFrame(test_data.dtypes).reset_index().rename(columns = {'index':'variable', 0:'dtype'}),\n                                                 right_on = 'variable', left_on = 'variable')","75937e90":"tr_unique_val_cnt_df","9565aedb":"ts_unique_val_cnt_df","53f40a7f":"tmp_df_1 = pd.DataFrame()\ntmp_df_1 = train_data['id'].copy()\ntmp_df_1_cols = []\n\nfor i in range(len(tr_unique_val_cnt_df)):\n    if tr_unique_val_cnt_df.iloc[i, 1] <= 7:\n        tmp_df_1 = pd.concat([tmp_df_1, pd.get_dummies(data = train_data.loc[:, tr_unique_val_cnt_df.iloc[i, 0]],\n                                                       prefix = tr_unique_val_cnt_df.iloc[i, 0],\n                                                       drop_first = True)], axis = 1)\n        tmp_df_1_cols.append(tr_unique_val_cnt_df.iloc[i, 0])","95c9683f":"sorted(tmp_df_1_cols)","23e7ca4f":"tmp_df_1.shape","2aac8dd9":"# tmp_df_1.head()","3318bcb1":"tmp_df_2 = pd.DataFrame()\ntmp_df_2 = train_data['id'].copy()\ntmp_df_2_cols = []\n\nfor i in range(len(tr_unique_val_cnt_df)):\n    if tr_unique_val_cnt_df.iloc[i, 1] > 7 and tr_unique_val_cnt_df.iloc[i, 1] <= 20:\n        tmp_df_2 = pd.concat([tmp_df_2, pd.get_dummies(data = train_data.loc[:, tr_unique_val_cnt_df.iloc[i, 0]],\n                                                       prefix = tr_unique_val_cnt_df.iloc[i, 0],\n                                                       drop_first = True)], axis = 1)\n        tmp_df_2_cols.append(tr_unique_val_cnt_df.iloc[i, 0])","a21c3f4d":"sorted(tmp_df_2_cols)","ec9cba99":"tmp_df_2.shape","cd4772f5":"# tmp_df_2.head()","0e5d2a63":"filter_cond = (tr_unique_val_cnt_df['unique_val_count'] > 100)\ntmp_df_3_cols = tr_unique_val_cnt_df.loc[filter_cond, 'variable'].tolist()\n\ntmp_df_3_enc = ce.LeaveOneOutEncoder(cols = tmp_df_3_cols)\n\ntmp_df_3 = tmp_df_3_enc.fit_transform(X = train_data.loc[:, ['id'] + tmp_df_3_cols], \n                                      y = train_data['target'])","5a2f5b2c":"sorted(tmp_df_3_cols)","4592b36d":"tmp_df_3.shape","d3ae7944":"# tmp_df_3.head()","8aaff9c7":"filter_cond = (tr_unique_val_cnt_df['unique_val_count'] > 20) & (tr_unique_val_cnt_df['unique_val_count'] <= 100)\n\n# Create a data frame with id and the columns that satisfy above unique value count condition.\ntmp_df_4 = train_data.loc[:, ['id'] + tr_unique_val_cnt_df.loc[filter_cond, 'variable'].tolist()]\n\n# Create a list to store variable names encoded using this encoding technique.\ntmp_df_4_cols = []\n\ncol_grp_main_df = pd.DataFrame()\n\n# Flag used to create either \"odds for\" or \"odds against\" variable.\n# Set to True to create odds for variable. False, otherwise.\npos_odd_flag = False\n\n# Flag used to create either \"odds ratio for\" or \"odds ratio against\" variable.\n# Set to True to create \"odds ratio for\" variable. False, otherwise.\npos_odd_ratio_flag = False\n\n# This loop executes once for each variable.\nfor i in tr_unique_val_cnt_df.loc[filter_cond, ].index:\n    \n    var_name = tr_unique_val_cnt_df.iloc[i, 0]\n\n    # Create variable names dynamically created for original variable.\n    tot_count_var = var_name + '_tot_count'\n    pos_count_var = var_name + '_pos_count'\n    neg_count_var = var_name + '_neg_count'\n    pos_prob_var = var_name + '_pos_prob'\n    neg_prob_var = var_name + '_neg_prob'\n    odds_var = var_name + '_odds'\n    log_odds_var = var_name + '_log_odds'\n    odds_ratio_var = var_name + '_odds_ratio'\n    log_odds_ratio_var = var_name + '_log_odds_ratio'\n    mean_var = var_name + '_mean'\n    variance_var = var_name + '_variance'\n    \n    # Compute unique value count for each variable.\n    grp_main_df = pd.DataFrame(train_data[var_name].value_counts())\n    grp_main_df.reset_index(inplace = True)\n    grp_main_df.rename(columns = {var_name:tot_count_var}, inplace = True)\n    \n    # Compute unique value count for each variable for negative labelled (target == 0) observations.\n    grp_neg_df = pd.DataFrame(train_data.loc[train_data['target'] == 0, var_name].value_counts())\n    grp_neg_df.reset_index(inplace = True)\n    grp_neg_df.rename(columns = {var_name:neg_count_var}, inplace = True)\n\n    # Compute unique value count for each variable for positive labelled (target == 1) observations.\n    grp_pos_df = pd.DataFrame(train_data.loc[train_data['target'] == 1, var_name].value_counts())\n    grp_pos_df.reset_index(inplace = True)\n    grp_pos_df.rename(columns = {var_name:pos_count_var}, inplace = True)\n\n    # Compute variance of the target variable for each unique value of the predictor.\n    grp_variance_df = pd.DataFrame(train_data.groupby([var_name])['target'].var())\n    grp_variance_df.reset_index(inplace = True)\n    grp_variance_df.rename(columns = {var_name:'index', 'target':variance_var}, inplace = True)\n    \n    # Merge above computed values into a single data frame.\n    grp_main_df = grp_main_df.merge(grp_pos_df, on = 'index', how = 'left')     \n    grp_main_df = grp_main_df.merge(grp_neg_df, on = 'index', how = 'left')\n    grp_main_df = grp_main_df.merge(grp_variance_df, on = 'index', how = 'left')\n    grp_main_df.fillna(0, inplace = True)\n\n    # Compute positive probability and negative probability.\n    grp_main_df[pos_prob_var] = grp_main_df[pos_count_var] \/ grp_main_df[tot_count_var]\n    grp_main_df[neg_prob_var] = grp_main_df[neg_count_var] \/ grp_main_df[tot_count_var]\n\n    # Compute odds for or odds against values.\n    if pos_odd_flag:\n        grp_main_df[odds_var] = grp_main_df[pos_prob_var] \/ grp_main_df[neg_prob_var]\n    else:\n        grp_main_df[odds_var] = grp_main_df[neg_prob_var] \/ grp_main_df[pos_prob_var]\n\n    # Handling zero or infinite (+\/-) values resulted from odds value computation.\n    grp_main_df.loc[grp_main_df[odds_var] == 0, odds_var] = 1\n    grp_main_df.loc[grp_main_df[odds_var] == float('inf'), odds_var] = .1\n    grp_main_df.loc[grp_main_df[odds_var] == float('-inf'), odds_var] = .1\n\n    # Compute log-odds value.\n    grp_main_df[log_odds_var] = grp_main_df[odds_var].apply(lambda x : np.log(.1) if math.isinf(x) else np.log(x))\n\n    tot_pos_count = grp_main_df[pos_count_var].sum()\n    tot_neg_count = grp_main_df[neg_count_var].sum()\n\n    # Compute odds for ratio or odds against ratio values.\n    if pos_odd_ratio_flag:\n        grp_main_df[odds_ratio_var] = grp_main_df.apply(lambda x : (x[pos_count_var] \/ (tot_pos_count - x[pos_count_var])) \/ (x[neg_count_var] \/ (tot_neg_count - x[neg_count_var])), axis = 1)\n    else:\n        grp_main_df[odds_ratio_var] = grp_main_df.apply(lambda x : (x[neg_count_var] \/ (tot_neg_count - x[neg_count_var])) \/ (x[pos_count_var] \/ (tot_pos_count - x[pos_count_var])), axis = 1)\n\n    # Handling zero or infinite (+\/-) values resulted from odds ratio value computation.\n    grp_main_df.loc[grp_main_df[odds_ratio_var] == 0, odds_ratio_var] = 1\n    grp_main_df.loc[grp_main_df[odds_ratio_var] == float('inf'), odds_ratio_var] = 1\n    grp_main_df.loc[grp_main_df[odds_ratio_var] == float('-inf'), odds_ratio_var] = 1\n\n    # Compute log-odds ratio value.\n    grp_main_df[log_odds_ratio_var] = grp_main_df[odds_ratio_var].apply(lambda x : np.log(.1) if math.isinf(x) else np.log(x))\n\n    # Rename pos_prob column of a variable to variance of the same variable.\n    grp_main_df.rename(columns = {pos_prob_var:mean_var}, inplace = True)\n\n    # We do not need these variables anymore. Hence, adding these variables to drop list.\n    cols_to_drop = [tot_count_var, pos_count_var, neg_count_var, neg_prob_var]\n\n    if len(cols_to_drop) > 0:\n        grp_main_df.drop(columns = cols_to_drop, inplace = True)\n\n    tmp_df_4 = tmp_df_4.merge(grp_main_df, right_on = 'index', left_on = var_name)\n    tmp_df_4.drop(columns = ['index', var_name], inplace = True)\n\n    grp_main_df.rename(columns = {log_odds_var:'log_odds'}, inplace = True)\n    grp_main_df.rename(columns = {log_odds_ratio_var:'log_odds_ratio'}, inplace = True)\n\n    grp_main_df.rename(columns = {odds_var:'odds'}, inplace = True)\n    grp_main_df.rename(columns = {odds_ratio_var:'odds_ratio'}, inplace = True)\n\n    grp_main_df.rename(columns = {mean_var:'mean'}, inplace = True)\n    grp_main_df.rename(columns = {variance_var:'variance'}, inplace = True)\n\n    col_grp_main_df = pd.concat([col_grp_main_df, \n                                pd.concat([pd.DataFrame([var_name] * grp_main_df.shape[0], columns = ['variable']), grp_main_df], axis = 1)], \n                                axis = 0,\n                                sort = False)\n\n    tmp_df_4_cols.append(var_name)","6703d9d3":"sorted(tmp_df_4_cols)","fc843f30":"# Inf values check.\n(~np.isfinite(col_grp_main_df.iloc[:, 2:]) & ~col_grp_main_df.iloc[:, 2:].isna()).sum()","b6a5cad3":"del grp_main_df, grp_pos_df, grp_neg_df, var_name, tot_count_var, pos_count_var, neg_count_var, pos_prob_var, neg_prob_var\ndel odds_var, log_odds_var, odds_ratio_var, log_odds_ratio_var","2c333714":"tmp_df_4.shape","b047bd50":"# tmp_df_4.head()","db522020":"addnl_cols = ['id', 'target']\naddnl_cols = addnl_cols + ['bin_' + str(i) for i in range(5)]","ebb4e027":"addnl_cols","f40cc0ff":"print(tmp_df_1.shape)\nprint(tmp_df_2.shape)\nprint(tmp_df_3.shape)\nprint(tmp_df_4.shape)","e62f64fe":"tr_enc_data = pd.DataFrame()\ntr_enc_data = tmp_df_1.merge(tmp_df_2, on = 'id')\ntr_enc_data = tr_enc_data.merge(tmp_df_3, on = 'id')\ntr_enc_data = tr_enc_data.merge(tmp_df_4, on = 'id')\ntr_enc_data = tr_enc_data.merge(train_data[addnl_cols], on = 'id')\ntr_enc_data.shape","d84758b2":"train_data['target'].value_counts()","8cbab467":"tr_enc_data['target'].value_counts()","52e544ee":"print('Train dataset size : {} MB'.format(round(getsizeof(train_data) \/ 1024 \/ 1024)))","28f1ae12":"print('Encoded train dataset size : {} MB'.format(round(getsizeof(tr_enc_data) \/ 1024 \/ 1024)))","8ac466ae":"del tmp_df_1, tmp_df_2, tmp_df_3, tmp_df_4","f2958c06":"# Check for columns with duplicate column names.\nif tr_enc_data.columns.duplicated().sum() > 0:\n    print(tr_enc_data.columns[tr_enc_data.columns.duplicated().sum()])","ad883599":"tmp_df_1_cols","6d77b524":"tmp_df_1 = pd.DataFrame()\ntmp_df_1 = test_data['id'].copy()\n\nfor col in tmp_df_1_cols:\n    tmp_df_1 = pd.concat([tmp_df_1, pd.get_dummies(data = test_data.loc[:, col],\n                                                   prefix = col,\n                                                   drop_first = True)], axis = 1)","8473de48":"tmp_df_1.shape","b640979a":"tmp_df_2_cols","eacde0bb":"tmp_df_2 = pd.DataFrame()\ntmp_df_2 = test_data['id'].copy()\n\nfor col in tmp_df_2_cols:\n    tmp_df_2 = pd.concat([tmp_df_2, pd.get_dummies(test_data.loc[:, col],\n                                                   prefix = col,\n                                                   drop_first = True)], axis = 1)","1c5eee0a":"tmp_df_2.shape","6a4cbc8a":"# tmp_df_2.head()","cb20a276":"tmp_df_3_cols","7baa5790":"tmp_df_3 = tmp_df_3_enc.transform(X = test_data.loc[:, ['id'] + tmp_df_3_cols])","90132010":"# Missing value check.\ntmp_df_3.isnull().sum()[tmp_df_3.isnull().sum() > 0]","77b28d40":"# Infinite value check.\n(~np.isfinite(tmp_df_3) & ~tmp_df_3.isna()).sum()[(~np.isfinite(tmp_df_3) & ~tmp_df_3.isna()).sum() > 0]","864ab210":"tmp_df_4_cols","1f3857a5":"tmp_df_4 = test_data.loc[:, ['id'] + tmp_df_4_cols]\n\nfor col in tmp_df_4_cols:\n\n    tmp_col_grp_main_df = col_grp_main_df.loc[col_grp_main_df['variable'] == col, \n                                              ['index', 'log_odds', 'log_odds_ratio', 'odds', 'odds_ratio', 'mean', 'variance']].copy()\n\n    tmp_col_grp_main_df.rename(columns = {'log_odds' : col + '_log_odds',\n                                          'log_odds_ratio' : col + '_log_odds_ratio'}, inplace = True)\n    \n    tmp_col_grp_main_df.rename(columns = {'odds' : col + '_odds',\n                                          'odds_ratio' : col + '_odds_ratio'}, inplace = True)\n    \n    tmp_col_grp_main_df.rename(columns = {'mean' : col + '_mean',\n                                          'variance' : col + '_variance'}, inplace = True)\n    \n    tmp_df_4 = tmp_df_4.merge(tmp_col_grp_main_df, right_on = 'index', left_on = col, how = 'left')\n\n    tmp_df_4.drop(columns = ['index', col], inplace = True)","a65d2219":"# Missing value check.\ntmp_df_4.isnull().sum()[tmp_df_4.isnull().sum() > 0]","4f954a7c":"# Infinite value check.\n(~np.isfinite(tmp_df_4) & ~tmp_df_4.isna()).sum()[(~np.isfinite(tmp_df_4) & ~tmp_df_4.isna()).sum() > 0]","7598fdcd":"tmp_df_4.shape","7eb1ca79":"addnl_cols.remove('target')\n\nts_enc_data = pd.DataFrame()\nts_enc_data = tmp_df_1.merge(tmp_df_2, on = 'id')\nts_enc_data = ts_enc_data.merge(tmp_df_3, on = 'id')\nts_enc_data = ts_enc_data.merge(tmp_df_4, on = 'id')\nts_enc_data = ts_enc_data.merge(test_data[addnl_cols], on = 'id')","68593861":"print('Test dataset size : {} MB'.format(round(getsizeof(test_data) \/ 1024 \/ 1024)))","82ceec5a":"print('Encoded test dataset size : {} MB'.format(round(getsizeof(ts_enc_data) \/ 1024 \/ 1024)))","6e98d914":"print(tr_enc_data.shape)\nprint(ts_enc_data.shape)","c8a89d78":"# Duplicate column name check.\npd.Series(tr_enc_data.columns)[pd.Series(tr_enc_data.columns).duplicated()]","38eba5d7":"# Duplicate column name check.\npd.Series(ts_enc_data.columns)[pd.Series(ts_enc_data.columns).duplicated()]","d84c7565":"print('Test data columns not found in Train data : {}'.format(set(ts_enc_data.columns).difference(set(tr_enc_data.columns))))\nprint('Train data columns not found in Test data : {}'.format(set(tr_enc_data.columns).difference(set(ts_enc_data.columns))))","bf155a5e":"# Missing value check.\nprint(tr_enc_data.isna().sum().sum())\nprint(ts_enc_data.isna().sum().sum())","f3402742":"# train_data.columns","804f24f5":"# tr_enc_data.columns[tr_enc_data.columns.str.contains('ord')]","2b6e8565":"del tmp_df_1, tmp_df_2, tmp_df_3, tmp_df_4, tmp_col_grp_main_df","a57dac11":"def create_submn_file (file_prefix, file_data_df):\n    \n    '''\n    This function is used to create date-time stamped submission file.\n\n    Parameters:\n        file_prefix : String to be used as file prefix.\n        file_data_df : DataFrame containing data to be written to the file.\n    '''\n    curr_date_time = datetime.now()\n    file_name = file_prefix + '_Submn_File_%d%d%d_%d%d%d.csv' % (curr_date_time.year, curr_date_time.month, curr_date_time.day, curr_date_time.hour, curr_date_time.minute, curr_date_time.second)\n    file_data_df.to_csv('.\/submission_files\/' + file_name, index = False)","9be01afb":"def make_predictions (model_estimator, X_test, prediction_type = 'class'):\n\n    '''\n    This function is used to predict either class or class probability based on \n    the estimator and test data passed as input to this function.\n\n    Parameters:\n        model_estimator : Model estimator trained on train data set.\n        X_test : Test data set on which model estimator is applied to make predictions.\n        prediction_type : Possible values are - 'class' and 'proba'.\n                            - 'class' : Default value. Used to make class predictions.\n                            - 'proba' : Used to make class probability predictions.\n    \n    Returns : Class or Class probability preductions.\n    '''\n\n    if (prediction_type == 'class'):\n        test_pred = model_estimator.predict(X_test)\n    elif (prediction_type == 'proba'):\n        test_pred = model_estimator.predict_proba(X_test)\n        if (test_pred.ndim > 1):\n            test_pred = test_pred[:, 1]\n        \n    return test_pred","8fc5f004":"def prepare_file_data (model_estimator, X_test, primary_column_data, opt_threshold = 0, prediction_type = 'proba'):\n\n    '''\n    This function is used to prepare data to be written to the submission file.\n\n    Parameters:\n        model_estimator : Model estimator trained on train data set.\n        X_test : Test data set on which model estimator is applied to make predictions.\n        primary_column_data : Primary key data expected in the first column of the submission file which in this problem is 'id'.\n        opt_threshold : Not relevant for this problem.\n        prediction_type : Possible values are - 'class' and 'proba'.\n\n    Returns a DataFrame with two variables: id, target (class probability predictions).\n    '''\n    \n    test_pred = make_predictions(model_estimator, X_test, prediction_type)\n    \n    if (prediction_type == 'class'):\n        None\n    \n    elif (prediction_type == 'proba'):\n        None\n        \n#         test_pred_list = list(range(len(test_pred)))\n\n#         for i in range(len(test_pred_list)):\n#             test_pred_list[i] = 1 if test_pred[i] > opt_threshold else 0\n            \n#         test_pred = test_pred_list.copy()\n    \n    file_data = pd.DataFrame({'id':primary_column_data, 'target':test_pred})\n    \n    return file_data","09c45873":"def plot_cv_results(estimator):\n\n    '''\n    This function is used to plot validation scores for each fold of K-fold cross validation.\n\n    Parameters:\n        estimator : Model estimator trained on train data set.\n    '''\n\n    plt.figure(figsize = (6, 5))\n\n    for idx, i in enumerate(estimator.scores_[1]):\n        plt.plot(np.log(estimator.Cs_), i, label = 'Fold-' + str(idx + 1))\n\n    plt.legend()\n    plt.show()","b7278836":"cols_to_excl = []\n\ncol_names_list = ['ord_5_1', 'ord_5_2', 'ord_4']\n\n# Comment the any one or more lines below to include them in modeling phase.\n\n# cols_to_excl = cols_to_excl + ['_'.join(i) for i in zip(col_names_list, ['log_odds'] * len(col_names_list))]\ncols_to_excl = cols_to_excl + ['_'.join(i) for i in zip(col_names_list, ['log_odds_ratio'] * len(col_names_list))]\ncols_to_excl = cols_to_excl + ['_'.join(i) for i in zip(col_names_list, ['odds_ratio'] * len(col_names_list))]\ncols_to_excl = cols_to_excl + ['_'.join(i) for i in zip(col_names_list, ['odds'] * len(col_names_list))]\ncols_to_excl = cols_to_excl + ['_'.join(i) for i in zip(col_names_list, ['mean'] * len(col_names_list))]\ncols_to_excl = cols_to_excl + ['_'.join(i) for i in zip(col_names_list, ['variance'] * len(col_names_list))]\n\nmodel_train_data = tr_enc_data.copy()\nmodel_test_data = ts_enc_data.copy()\n\nif (len(cols_to_excl) > 0):\n    model_train_data = model_train_data.drop(columns = cols_to_excl, axis = 1)\n    model_test_data = model_test_data.drop(columns = cols_to_excl, axis = 1)","0da25f67":"# list(model_train_data.columns)","3e78f1c0":"# len(cols_to_excl)","96dd26f3":"# model_train_data.head()","55fd9b1b":"# model_test_data.head()","cbc2a0ca":"print(model_train_data.shape)\nprint(model_test_data.shape)","a1771f55":"X = model_train_data.drop(['id', 'target'], axis = 1)\ny = model_train_data['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 100)","e9472293":"log_reg = LogisticRegression(solver = 'lbfgs', random_state = 100, max_iter = 1000)\nlog_reg.fit(X_train, y_train)","467f8b5e":"test_pred = make_predictions(log_reg, X_test, 'proba')\nprint('Model ROC AUC score: {}'.format(roc_auc_score (y_test, test_pred)))\n\n# Model ROC AUC score: 0.8037375992463995","1a0d51a5":"# Prepare submission file based on class predictions.\n\nfile_data = prepare_file_data (log_reg, model_test_data.iloc[:, range(1,model_test_data.shape[1])], model_test_data['id'])\n# create_submn_file ('logreg', file_data)\n\nfile_data.to_csv('submission.csv', index = False)\n\n# 0.80459","76108154":"lr_l2_cv = LogisticRegressionCV(random_state = 100, \n                                solver = 'liblinear', \n                                scoring = 'roc_auc',\n                                penalty = 'l2',\n                                cv = 10, \n                                max_iter = 1000,\n                                verbose = 3)\n\nlr_l2_cv.fit(X_train, y_train)","d7fdd853":"plot_cv_results(lr_l2_cv)","a1ba9178":"test_pred = make_predictions(lr_l2_cv, X_test, 'proba')\nprint('Model ROC AUC score: {}'.format(roc_auc_score (y_test, test_pred)))\n\n# Model ROC AUC score: 0.8037420416539948\n# Model ROC AUC score: 0.8037513913858887","95b90d9b":"file_data = prepare_file_data (lr_l2_cv, model_test_data.iloc[:, range(1,model_test_data.shape[1])], model_test_data['id'])\n# create_submn_file ('lr_l2_cv', file_data)\n\nfile_data.to_csv('submission.csv', index = False)\n\n# 0.80481","455b79b4":"**Variable : ord_5_1mult2**","efeab916":"**Variables : day and month**","77ebe6a3":"(3) Apply Leave One Out encoding technique for variables with unique value count > 100.","091bd3f4":"**Variable : ord_5_1_2**","407d3de2":"**L2 regularization**","54706735":"(1) Apply dummy variable encoding technique for variables with unique value count > 2 and <= 7.","f6204327":"Let us now look at the unique value count for all the columns once again for train and test data sets to decide on the encoding technique to be applied on each variable.","b330df76":"This is the right time to decide on the encoding techniques we are going to apply on each of the variables. <br>\nHere we are not going to apply encoding techniques based on the kind of variable i.e nominal, ordinal, cyclical etc. Instead, we are going to employ encoding techniques based on the number of unique values we have for each of the given variables. Because, its the number of unique values that hurts the compute memory and other resources really bad when we have more number of unique values.<br>\n\nThis is what worked for me and got me best score that I could get:\n1. Variables with **unique value count <= 2** : **Not** going to apply encoding steps.\n2. Variables with **unique value count > 2 and <= 7** : Apply **Dummy variable encoding** technique.\n3. Variables with **unique value count > 7 and <= 20** : Apply **Dummy variable encoding** technique.\n4. Variables with **unique value count > 20 and <= 100** : Apply **Bin counting encoding** technique where we compute mean, variance, odds, log odds, odds ratio and log odds ratio.\n5. Variables with **unique value count > 100** : Apply **Leave One Out encoding** technique.","d2f2b1bb":"**Handling bin* binary variables**","52650fb0":"#### UDF : create_submn_file ()\n\nI used this function to create date-time stamped submission files to overcome overwriting of submission files. This function is not required in this kernel.","c78f49a5":"**\"ord_5\"** is a variable where we are finding two character values each containing same characters in same sequence but in different cases i.e. lower case and upper case. For e.g. kW and kw, qo and Qo. We assume value of these kinds represent different values. Hence we are not going to change case of the values of this variable.","ae885a8c":"**LogisticRegressionCV**","149d5b76":"**Variables : ord_3 and ord_4**","800c4568":"1. Before applying encoding schemes on the given data sets, let's ensure that we are not encoding same values (for any variable) represented in different cases (combination of Upper-case and Lower-case characters) to get two or more differently encoded values for the same original values. \n2. To do that let's **change the case** of values of all the object type variable to **lower case** and then, **compare the unique value counts** of each of these categorical variables **before** and **after** changing the case. \n3. As explained above, we are going to exclude **ord5** variable from this operation.","562e507b":"### Modeling","5d3a5933":"(1) Apply dummy variable encoding technique for variables with unique value count > 2 and <= 7.","786823e2":"By performing this check, we re-confirmed that the test dataset does not have unique values for any variable in excess compared to the unique values of the corresponding variable in train dataset.","69452312":"**LogisticRegression**","d6ec7eb4":"(2) Apply dummy variable encoding technique for variables with unique value count > 7 and <= 20.","62984d53":"**Test data encoding**","2f313d62":"(2) Apply dummy variable encoding technique for variables with unique value count > 7 and <= 20.","bb227677":"(4) Apply Bin Counting technique to encode variables with unique value count > 20 and <= 100","b9294266":"We are done with performing almost all necessary data checks before applying encoding techniques and there-on to modeling phase.<br>","4c4c9cd2":"(3) Apply Leave One Out encoding technique for variables with unique value count > 100.","4d783976":"**Variable : ord_5_1plus2**","961c2e33":"#### UDF : prepare_file_data()","ce7aec47":"**Variables : ord_5_1 and ord_5_2**","84abcd81":"(4) Apply Bin Counting technique to encode variables with unique value count > 20 and <= 100.","91054f9c":"#### UDF : make_predictions()"}}