{"cell_type":{"cba2123d":"code","c14aa478":"code","22b26dab":"code","a142d053":"code","f25aae04":"code","af655ce8":"code","5f4c2018":"code","ad035891":"code","5c8cb722":"code","30c20ac2":"code","f9617226":"code","8926a22c":"code","12ba968b":"code","f05ed092":"code","bab3874b":"code","f3111b2a":"code","174d849f":"code","0a2dcbcc":"code","164dd95e":"code","6e460110":"code","fa7d0bcd":"code","1057bf7e":"code","60acecbf":"code","a1c951a5":"code","85650f48":"code","515fa2f0":"code","e616ce1f":"code","4f1b6590":"code","965dfd04":"code","15ff6f6c":"code","0e118c13":"code","e3f16c09":"code","84728ba3":"code","d534c7f5":"code","e9c8b1b9":"code","78a5719d":"code","dcd2e96d":"code","3ca4581f":"code","3a3e1d47":"code","24cc7405":"code","2b5ca34d":"code","d7dd4593":"code","6b7f776c":"code","387d5d90":"code","58cc40d9":"code","cbbc48e9":"code","d40f075a":"code","6f224cb5":"code","cec44963":"code","8ff0d160":"code","b653187f":"code","e8c2b3c4":"code","b7eba6cf":"code","3db0b14e":"code","3db7da69":"code","2c28012a":"code","dba24a24":"code","f2519013":"code","be965ffe":"code","20aba694":"code","f57056fa":"code","8da1160e":"code","bb405e0c":"code","70fef429":"code","ce281559":"code","ee0a9f74":"code","214fb274":"code","1f3cea3b":"code","77e3731a":"markdown","18380cc6":"markdown","ca96ef5f":"markdown","e0e7a4a4":"markdown","4a351ba2":"markdown","9ff19f07":"markdown","597da504":"markdown","52170420":"markdown","b0d2542a":"markdown","e51c71e3":"markdown","357ec307":"markdown","9613d44f":"markdown","53762453":"markdown","80f6b243":"markdown","84c7de36":"markdown","6f436a07":"markdown","186242e9":"markdown","70c2dcf1":"markdown","d8264625":"markdown","2102c1f1":"markdown","599c1bd6":"markdown","460ab77b":"markdown","2088bca6":"markdown","1e8427c6":"markdown","b3c4a111":"markdown","9b891b70":"markdown","0a8b1a65":"markdown","c1069722":"markdown","8bbcac81":"markdown","3581a450":"markdown","706c347f":"markdown","388c5dc1":"markdown","75f94c30":"markdown","78bda69b":"markdown","27a9c48d":"markdown","70fa274a":"markdown","8b0e77b1":"markdown","26073f50":"markdown","4c6fa315":"markdown","d00d7865":"markdown","a78b5d7e":"markdown","93e1eefe":"markdown","eaa3b1db":"markdown"},"source":{"cba2123d":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport category_encoders as ce \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import zero_one_loss\nimport optuna\nfrom optuna.samplers import RandomSampler, GridSampler, TPESampler\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb","c14aa478":"dftrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndftest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndftrain['Pclass'] = dftrain['Pclass'].replace({1:'Upper',2:'Middle',3:'Lower'})\ndftest['Pclass'] = dftest['Pclass'].replace({1:'Upper',2:'Middle',3:'Lower'})\ndftrain.head()","22b26dab":"dftrain_ILL = dftrain[['Age','Fare','Survived']] \ndftrain_ILL.isnull().sum()","a142d053":"filter1 = (dftrain['Age'] >= 20) & (dftrain['Age']<=40) & (dftrain['Fare'] >=0) & (dftrain['Fare']<=50)  \ndftrain_ILL2 = dftrain_ILL[filter1]\nX = StandardScaler().fit_transform(dftrain_ILL2[['Age','Fare']])\ny = dftrain_ILL2['Survived']\nprint(X.shape)\nprint(y.shape)","f25aae04":"fig, axes = plt.subplots(1, 2,figsize=(15,5))\nsns.distplot(dftrain['Age'], color='b', bins=40,ax=axes[0])\nsns.distplot(dftrain['Fare'], color='r', bins=40,ax=axes[1])","af655ce8":"cm = plt.cm.RdBu\ncm_bright = ListedColormap(['#FF0000', '#0000FF'])\nplot_step = 0.02\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                     np.arange(y_min, y_max, plot_step))","5f4c2018":"def get_decision_tree(max_depth,seed):\n    DT = DecisionTreeClassifier(max_depth=max_depth,random_state=seed)\n    return DT","ad035891":"fig, axes = plt.subplots(2,2,constrained_layout=True,figsize=(10,5))\nmax_depth = [1,2,4,8]\nDT_models = []\nfor ax, tree_depth in zip(axes.flatten(),max_depth):\n    DT = get_decision_tree(tree_depth,0)\n    DT.fit(X, y)\n    y_pred = DT.predict(np.c_[xx.ravel(), yy.ravel()])\n    y_pred = y_pred.reshape(xx.shape)\n    ax.contourf(xx, yy, y_pred, cmap=cm, alpha=.7)\n    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright,edgecolors='k')\n    DT_models.append(DT)","5c8cb722":"DT_1 = tree.plot_tree(DT_models[0]) ","30c20ac2":"plt.figure(figsize=(8,8))\nDT_2 = tree.plot_tree(DT_models[1]) ","f9617226":"def get_Adaboost(max_depth,seed):\n    Ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=max_depth),\n                         algorithm=\"SAMME\",\n                         n_estimators=100,\n                        random_state=seed)\n    return Ada","8926a22c":"fig, axes = plt.subplots(2,2,constrained_layout=True,figsize=(10,5))\nmax_depth = [1,2,4,8]\nfig.suptitle('Adaboost 1 - Tree Depth (n_estimators=100)')\nfor ax, tree_depth in zip(axes.flatten(),max_depth):\n    Ada = get_Adaboost(tree_depth,0)\n    Ada.fit(X, y)\n    y_pred = Ada.predict(np.c_[xx.ravel(), yy.ravel()])\n    y_pred = y_pred.reshape(xx.shape)\n    ax.set_title('Max_depth='+str(tree_depth))\n    ax.contourf(xx, yy, y_pred, cmap=cm, alpha=.7)\n    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright,edgecolors='k')","12ba968b":"def get_Adaboost2(n_e,seed):\n    Ada2 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n                         algorithm=\"SAMME\",\n                         n_estimators=n_e,\n                        random_state=seed)\n    return Ada2","f05ed092":"fig, axes = plt.subplots(2,2,constrained_layout=True,figsize=(10,5))\nn_estimators = [32,128,256,512]\nfig.suptitle('Adaboost 2 - n_estimators (max_depth)')\nfor ax, ne in zip(axes.flatten(),n_estimators):\n    Ada2 = get_Adaboost2(ne,0)\n    Ada2.fit(X, y)\n    y_pred = Ada2.predict(np.c_[xx.ravel(), yy.ravel()])\n    y_pred = y_pred.reshape(xx.shape)\n    ax.set_title('n_estimators='+str(ne))\n    ax.contourf(xx, yy, y_pred, cmap=cm, alpha=.7)\n    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright,edgecolors='k')","bab3874b":"def get_gradient_boosting(max_depth,seed):\n    Gradient_Boosting = GradientBoostingClassifier(max_depth=max_depth,\n                         n_estimators=100,\n                        random_state=seed)\n    return Gradient_Boosting","f3111b2a":"fig, axes = plt.subplots(2,2,constrained_layout=True,figsize=(10,5))\nmax_depth = [1,2,4,8]\nfig.suptitle('Gradient Boosting 1 - max_depth (n_estimators=100)')\nfor ax, tree_depth in zip(axes.flatten(),max_depth):\n    gra = get_gradient_boosting(tree_depth,0)\n    gra.fit(X, y)\n    y_pred = gra.predict(np.c_[xx.ravel(), yy.ravel()])\n    y_pred = y_pred.reshape(xx.shape)\n    ax.set_title('Max_depth='+str(tree_depth))\n    ax.contourf(xx, yy, y_pred, cmap=cm, alpha=.7)\n    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright,edgecolors='k')","174d849f":"def get_gradient_boosting2(n_e,seed):\n    Gradient_Boosting = GradientBoostingClassifier(max_depth=4,\n                        n_estimators=n_e,\n                        random_state=seed)\n    return Gradient_Boosting","0a2dcbcc":"fig, axes = plt.subplots(2,2,constrained_layout=True,figsize=(10,5))\nn_estimators = [2,16,32,128]\nfig.suptitle('Gradient Boosting 1 - n_estimators (max_depth=4)')\nfor ax, n_e in zip(axes.flatten(),n_estimators):\n    gra2 = get_gradient_boosting2(n_e,0)\n    gra2.fit(X, y)\n    y_pred = gra.predict(np.c_[xx.ravel(), yy.ravel()])\n    y_pred = y_pred.reshape(xx.shape)\n    ax.set_title('n_estimators='+str(n_e))\n    ax.contourf(xx, yy, y_pred, cmap=cm, alpha=.7)\n    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright,edgecolors='k')","164dd95e":"train = dftrain[['Pclass','Sex','Age','SibSp','Parch','Fare']]\nytrain = dftrain['Survived']\ntest = dftest[['Pclass','Sex','Age','SibSp','Parch','Fare']]\n\nprint('Columns: ',train.columns)","6e460110":"train.isnull().sum()","fa7d0bcd":"test.isnull().sum()","1057bf7e":"train_new = train.copy()\ntrain_new['Age'] = train['Age'].fillna(value=train['Age'].median())\n#train_new['Embarked'] = train['Embarked'].fillna(value=train['Embarked'].mode())\ntest_new = test.copy()\ntest_new['Age'] = test['Age'].fillna(value=train['Age'].median())\ntest_new['Fare'] = test['Fare'].fillna(value=train['Fare'].median())","60acecbf":"def get_ct(ordinal_features, nominal_features):\n\n    # Create ordinal encoder\n    ce_ord = ce.OrdinalEncoder(\n                               cols=ordinal_features,\n                               mapping=[{'col': 'Pclass', 'mapping': {'Lower': 1, 'Middle': 2,'Upper':3}},\n                                        {'col': 'Sex','mapping':{'male':0,'female':1}}],\n                               handle_unknown='value',\n                               handle_missing='value'\n                               )\n    # Create nominal encoder\n    ce_nom = ce.OneHotEncoder(cols=nominal_features,handle_unknown='value',handle_missing='value')\n    \n    # Create a column transformer to combine both ordinal and nominal encoders \n    ct1 = ColumnTransformer(\n            transformers=[\n                ('cat_ordinal',ce_ord,ordinal_features),\n                ('cat_nominal',ce_nom,nominal_features),\n                ],remainder = 'passthrough')\n    clf = Pipeline(steps=[('preprocessor', ct1)])\n    return clf ","a1c951a5":"clf_ct = get_ct(['Pclass','Sex'],[]) \ntrain_data = clf_ct.fit_transform(train_new)\ntest_data = clf_ct.transform(test_new)","85650f48":"X_train, y_train = train_data[:700], ytrain[:700]\nX_val, y_val = train_data[700:], ytrain[700:]\nprint(X_train.shape)\nprint(X_val.shape)\nprint(train_new.columns)","515fa2f0":"max_depth = [2,4,6,8,10,12,14,16,18,20,40,70]\nDT_err_val = []\nDT_err_train = []\nfor mx in max_depth:\n    DT = DecisionTreeClassifier(max_depth=mx,random_state=0)\n    DT.fit(X_train, y_train)\n    DT_err_temp = 1.0 - DT.score(X_val, y_val)\n    DT_err_temp2 = 1.0 - DT.score(X_train, y_train)\n    DT_err_val.append(DT_err_temp)\n    DT_err_train.append(DT_err_temp2)","e616ce1f":"plt.figure()\nplt.plot(max_depth,DT_err_val,'-s',linewidth=3,label='Validation')\nplt.plot(max_depth,DT_err_train,'-o',linewidth=3,label='Train')\nplt.xlabel('max depth',fontsize=14)\nplt.ylabel('Classification error',fontsize=14)\nplt.title('Decision Tree',fontsize=14)","4f1b6590":"DEC_stump = DecisionTreeClassifier(max_depth=1,random_state=0)\nDEC_stump.fit(X_train, y_train)\nDEC_stump_err_train = 1.0 - DEC_stump.score(X_train, y_train)\nDEC_stump_err_val = 1.0 - DEC_stump.score(X_val, y_val)\n\nn_estimators = 5000\nAda_stump = AdaBoostClassifier(base_estimator=DEC_stump,n_estimators=n_estimators,learning_rate=0.1,random_state=0)\nAda_stump.fit(X_train, y_train)","965dfd04":"Ada_stump_err_val = []\nAda_stump_err_train = []\n\nfor i, y_pred in enumerate(Ada_stump.staged_predict(X_val)):\n    Ada_stump_err_val.append(zero_one_loss(y_pred, y_val))\nfor i, y_pred in enumerate(Ada_stump.staged_predict(X_train)):\n    Ada_stump_err_train.append(zero_one_loss(y_pred, y_train))","15ff6f6c":"fig, axes = plt.subplots(1, 2,figsize=(15,5))\n\naxes[1].plot([1, n_estimators], [DEC_stump_err_val] * 2, 'r--',label='Decision Stump')\naxes[1].plot(np.arange(n_estimators) + 1, Ada_stump_err_val,label='Adaboost with Decision Stump',color='red')\naxes[1].legend()\naxes[1].set_xlabel('n_estimators',fontsize=14)\naxes[1].set_ylabel('Classification error',fontsize=14)\naxes[1].set_title('Validation',fontsize=14)\n\naxes[0].plot([1, n_estimators], [DEC_stump_err_train] * 2, 'k--',label='Decision Stump')\naxes[0].plot(np.arange(n_estimators) + 1, Ada_stump_err_train,label='Adaboost with Decision Stump',color='k')\naxes[0].set_xlabel('n_estimators',fontsize=14)\naxes[0].set_ylabel('Classification error',fontsize=14)\naxes[0].legend()\naxes[0].set_title('Train',fontsize=14)","0e118c13":"fig, axes = plt.subplots(1, 2,figsize=(15,5))\naxes[0].plot(range(1,len(Ada_stump.estimator_errors_)+1),Ada_stump.estimator_errors_,'-s',markersize=4)\naxes[0].set_xlabel('Individual Estimator',fontsize=14)\naxes[0].set_ylabel('Classification error',fontsize=14)\n\naxes[1].hist(Ada_stump.estimator_errors_)\naxes[1].set_xlabel('Classification error',fontsize=14)\naxes[1].set_ylabel('Count',fontsize=14)","e3f16c09":"n_estimators = 600\nmax_depth = [4,10,14,16]\nerr_ada_tree_val = []\nerr_ada_tree_train = []\nAda_tree_all = []\nfor mx in max_depth:\n    err_val_temp = []\n    err_train_temp = []\n    DT_base = DecisionTreeClassifier(max_depth=mx,random_state=0)\n    Ada_tree = AdaBoostClassifier(base_estimator=DT_base,n_estimators=n_estimators,learning_rate=0.1,random_state=0)\n    Ada_tree.fit(X_train, y_train)\n    for i, y_pred in enumerate(Ada_tree.staged_predict(X_val)):\n        err_val_temp.append(zero_one_loss(y_pred, y_val))\n    for i, y_pred in enumerate(Ada_tree.staged_predict(X_train)):\n        err_train_temp.append(zero_one_loss(y_pred, y_train))\n    err_ada_tree_val.append(err_val_temp)\n    err_ada_tree_train.append(err_train_temp)\n    Ada_tree_all.append(Ada_tree)","84728ba3":"def plot_err_loss(n_estimators,content_train,content_val,y_axis_label,y_labels):\n    fig, axes = plt.subplots(1, 2,figsize=(15,5))\n    for i in range(len(content_train)):\n        axes[0].plot(np.arange(n_estimators) + 1,content_train[i],label=y_labels[i])\n    axes[0].legend()\n    axes[0].set_title('Train',fontsize=14)\n    axes[0].set_xlabel('n_estimators',fontsize=14)\n    axes[0].set_ylabel(y_axis_label,fontsize=14)\n    \n    for i in range(len(content_train)):\n        axes[1].plot(np.arange(n_estimators) + 1,content_val[i],label=y_labels[i])\n    axes[1].legend()\n    axes[1].set_title('Validation',fontsize=14)\n    axes[1].set_xlabel('n_estimators',fontsize=14)\n    axes[1].set_ylabel(y_axis_label,fontsize=14)   ","d534c7f5":"y_labels = ['max_depth=4','max_depth=10',\n           'max_depth=14','max_depth=16']\nplot_err_loss(600,err_ada_tree_train,err_ada_tree_val,'Classification error',y_labels)","e9c8b1b9":"def gradient_booster(n_e,variable,changing_variable='max_depth'):\n    n_estimators = n_e\n    err_gra_tree_val = []\n    err_gra_tree_train = []\n    Gra_tree_all = []\n    train_loss_all = []\n    val_loss_all = []\n    y_logit_train_all = []\n    y_logit_val_all = []\n    for va in variable:\n        err_val_temp = []\n        err_train_temp = []\n        train_loss = []\n        val_loss = []\n        y_logit_train = []\n        y_logit_val = []\n        if changing_variable == 'max_depth':\n            # Learning rate in this case default to be 0.1\n            Gra_tree = GradientBoostingClassifier(max_depth=va,n_estimators=n_estimators,random_state=0)\n        else:\n            # max_depth fixed to be 8 (you can change it to the value you want)\n            Gra_tree = GradientBoostingClassifier(learning_rate=va,max_depth=8,n_estimators=n_estimators,random_state=0)\n        Gra_tree.fit(X_train, y_train)\n        for i, y_pred in enumerate(Gra_tree.staged_predict(X_val)):\n            err_val_temp.append(zero_one_loss(y_pred, y_val))\n        for i, y_pred in enumerate(Gra_tree.staged_predict(X_train)):\n            err_train_temp.append(zero_one_loss(y_pred, y_train))\n        for i, y_pred in enumerate(Gra_tree.staged_decision_function(X_train)):\n            train_loss.append(Gra_tree.loss_(y_train, y_pred))\n            y_logit_train.append(y_pred[:,:])\n        for i, y_pred in enumerate(Gra_tree.staged_decision_function(X_val)):\n            val_loss.append(Gra_tree.loss_(y_val, y_pred))\n            y_logit_val.append(y_pred[:,:])\n\n        y_logit_train_all.append(np.array(y_logit_train)[:,:,0])\n        y_logit_val_all.append(np.array(y_logit_val)[:,:,0])\n        err_gra_tree_val.append(err_val_temp)\n        err_gra_tree_train.append(err_train_temp)\n        train_loss_all.append(train_loss)\n        val_loss_all.append(val_loss)\n        Gra_tree_all.append(Gra_tree)\n        \n    return y_logit_train_all, y_logit_val_all, err_gra_tree_train, err_gra_tree_val, \\\n            train_loss_all, val_loss_all, Gra_tree_all","78a5719d":"max_depth = [4,8,12,16]\nchange_variable = 'max_depth'\ny_logit_train_all, y_logit_val_all, err_gra_tree_train, err_gra_tree_val, \\\n                train_loss_all, val_loss_all, Gra_tree_all = gradient_booster(200,max_depth,change_variable)","dcd2e96d":"y_labels = ['max_depth=4','max_depth=8',\n           'max_depth=12','max_depth=16']\nplot_err_loss(200,err_gra_tree_train,err_gra_tree_val,'Classification error',y_labels)","3ca4581f":"fig, axes = plt.subplots(1, 2,figsize=(15,5))\nn_estimators = 200\naxes[0].plot(y_logit_val_all[0][:, y_val.values == 0][:, 0],label='max_depth=4')\naxes[0].plot(y_logit_val_all[1][:, y_val.values == 0][:, 0],label='max_depth=8')\naxes[0].plot(y_logit_val_all[2][:, y_val.values == 0][:, 0],label='max_depth=12')\naxes[0].plot(y_logit_val_all[3][:, y_val.values == 0][:, 0],label='max_depth=16')\naxes[0].set_xlabel('n_estimators',fontsize=14)\naxes[0].set_ylabel('y_logit',fontsize=14)\naxes[0].legend()\n\naxes[1].plot(y_logit_val_all[0][:, y_val.values == 1][:, 0],label='max_depth=4')\naxes[1].plot(y_logit_val_all[1][:, y_val.values == 1][:, 0],label='max_depth=8')\naxes[1].plot(y_logit_val_all[2][:, y_val.values == 1][:, 0],label='max_depth=12')\naxes[1].plot(y_logit_val_all[3][:, y_val.values == 1][:, 0],label='max_depth=16')\naxes[1].set_xlabel('n_estimators',fontsize=14)\naxes[1].set_ylabel('y_logit',fontsize=14)\naxes[1].legend()","3a3e1d47":"y_labels = ['max_depth=4','max_depth=8',\n           'max_depth=12','max_depth=16']\nplot_err_loss(200,train_loss_all,val_loss_all,'Logistic Loss',y_labels)","24cc7405":"def get_cumul_loss(move_dir,num_to_move,Gra,X_val,y_val):\n    # This moves trees from front\n    if move_dir == 'front':\n        cumul_logit_temp = np.array([x for x in Gra.staged_decision_function(X_val)])[:, :, 0] \n        cumul_loss = Gra.loss_(y_val,cumul_logit_temp[-1,:]-cumul_logit_temp[num_to_move-1,:])\n    \n    # This moves trees from end\n    elif move_dir == 'back':\n        cumul_logit_temp = np.array([x for x in Gra.staged_decision_function(X_val)])[:, :, 0] \n        cumul_loss = Gra.loss_(y_val,cumul_logit_temp[-(num_to_move+1),:])\n    \n    # This does not move trees\n    else:\n        cumul_logit_temp = np.array([x for x in Gra.staged_decision_function(X_val)])[:, :, 0]\n        cumul_loss = Gra.loss_(y_val,cumul_logit_temp[-1,:])\n    return cumul_loss","2b5ca34d":"num_to_move = [1,4,8,16,32,64]\ncumul_loss_base = get_cumul_loss('nothing',1,Gra_tree_all[0],X_val,y_val)\n\ncumul_loss_front = []\ncumul_loss_back = []\nfor ntm in num_to_move:\n    cumul_loss_front_temp = get_cumul_loss('front',ntm,Gra_tree_all[0],X_val,y_val)\n    cumul_loss_back_temp = get_cumul_loss('back',ntm,Gra_tree_all[0],X_val,y_val)\n    cumul_loss_front.append(cumul_loss_front_temp)\n    cumul_loss_back.append(cumul_loss_back_temp)","d7dd4593":"fig, axes = plt.subplots(1, 2,figsize=(15,5))\naxes[0].plot(num_to_move,cumul_loss_front,'-o',c='r',linewidth=3)\naxes[1].plot(num_to_move,cumul_loss_back,'-o',c='k',linewidth=3)\n\naxes[0].set_xlabel('Number of front trees removed',fontsize=14)\naxes[0].set_ylabel('Logistic Loss',fontsize=14)\naxes[1].set_xlabel('Number of end trees removed',fontsize=14)\naxes[1].set_ylabel('Logistic Loss',fontsize=14)","6b7f776c":"lr = [0.01,0.1,0.5]\nchange_variable = 'lr'\ny_logit_train_all, y_logit_val_all, err_gra_tree_train, err_gra_tree_val, \\\n                train_loss_all, val_loss_all, Gra_tree_all = gradient_booster(200,lr,change_variable)","387d5d90":"y_labels = ['learning_rate=0.01','learning_rate=0.1','learning_rate=0.5']\nplot_err_loss(200,err_gra_tree_train,err_gra_tree_val,'Classification error',y_labels)","58cc40d9":"fig, axes = plt.subplots(1, 2,figsize=(15,5))\naxes[0].plot(y_logit_val_all[0][:, y_val.values == 0][:, 0],label='learning_rate=0.01')\naxes[0].plot(y_logit_val_all[1][:, y_val.values == 0][:, 0],label='learning_rate=0.1')\naxes[0].plot(y_logit_val_all[2][:, y_val.values == 0][:, 0],label='learning_rate=0.5')\naxes[0].set_xlabel('n_estimators',fontsize=14)\naxes[0].set_ylabel('y_logit',fontsize=14)\naxes[0].legend()\n\naxes[1].plot(y_logit_val_all[0][:, y_val.values == 1][:, 0],label='learning_rate=0.01')\naxes[1].plot(y_logit_val_all[1][:, y_val.values == 1][:, 0],label='learning_rate=0.1')\naxes[1].plot(y_logit_val_all[2][:, y_val.values == 1][:, 0],label='learning_rate=0.5')\naxes[1].set_xlabel('n_estimators',fontsize=14)\naxes[1].set_ylabel('y_logit',fontsize=14)\naxes[1].legend()","cbbc48e9":"y_labels = ['learning_rate=0.01','learning_rate=0.1','learning_rate=0.5']\nplot_err_loss(200,train_loss_all,val_loss_all,'Logistic Loss',y_labels)","d40f075a":"def get_cumul_loss(move_dir,num_to_move,Gra,X_val,y_val):\n    # This moves trees from front\n    if move_dir == 'front':\n        cumul_logit_temp = np.array([x for x in Gra.staged_decision_function(X_val)])[:, :, 0] \n        cumul_loss = Gra.loss_(y_val,cumul_logit_temp[-1,:]-cumul_logit_temp[num_to_move-1,:])\n    \n    # This moves trees from end\n    elif move_dir == 'back':\n        cumul_logit_temp = np.array([x for x in Gra.staged_decision_function(X_val)])[:, :, 0] \n        cumul_loss = Gra.loss_(y_val,cumul_logit_temp[-(num_to_move+1),:])\n    \n    # This does not move trees\n    else:\n        cumul_logit_temp = np.array([x for x in Gra.staged_decision_function(X_val)])[:, :, 0]\n        cumul_loss = Gra.loss_(y_val,cumul_logit_temp[-1,:])\n    return cumul_loss","6f224cb5":"num_to_move = [1,4,8,16,32,64]\ncumul_loss_base = get_cumul_loss('nothing',1,Gra_tree_all[0],X_val,y_val)\n\ncumul_loss_front = []\ncumul_loss_back = []\nfor ntm in num_to_move:\n    cumul_loss_front_temp = get_cumul_loss('front',ntm,Gra_tree_all[0],X_val,y_val)\n    cumul_loss_back_temp = get_cumul_loss('back',ntm,Gra_tree_all[0],X_val,y_val)\n    cumul_loss_front.append(cumul_loss_front_temp)\n    cumul_loss_back.append(cumul_loss_back_temp)","cec44963":"fig, axes = plt.subplots(1, 2,figsize=(15,5))\naxes[0].plot(num_to_move,cumul_loss_front,'-o',c='r',linewidth=3)\naxes[1].plot(num_to_move,cumul_loss_back,'-o',c='k',linewidth=3)\n\naxes[0].set_xlabel('Number of front trees removed',fontsize=14)\naxes[0].set_ylabel('Logistic Loss',fontsize=14)\naxes[1].set_xlabel('Number of end trees removed',fontsize=14)\naxes[1].set_ylabel('Logistic Loss',fontsize=14)","8ff0d160":"study_name1 = 'xgb'\nstudy_xgb = optuna.create_study(study_name=study_name1,direction='maximize',sampler=TPESampler(0))","b653187f":"def opt_xgb(trial):    \n\n    max_depth = int(trial.suggest_loguniform(\"max_depth\", 3,20))\n    subsample =  trial.suggest_discrete_uniform('bfrac',0.5,1.0,q=0.05)\n    min_child_weight = trial.suggest_loguniform(\"min_child_weight\", 1,10)\n    colsample_bytree = trial.suggest_discrete_uniform('feature',0.5,1.0,q=0.05)\n    reg_lambda = trial.suggest_loguniform(\"lambda_l2\", 1e-7, 10)\n    kFold= StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n    model = xgb.XGBClassifier(max_depth = max_depth,subsample=subsample,min_child_weight=min_child_weight,\n                               colsample_bytree=colsample_bytree,reg_lambda=reg_lambda,random_state=0)\n    scoring = 'accuracy'\n    return cross_val_score(model, train_data, ytrain, n_jobs=-1,scoring=scoring,cv=kFold).mean()","e8c2b3c4":"study_xgb.optimize(opt_xgb, n_trials=50)","b7eba6cf":"print('Total number of trials: ',len(study_xgb.trials))\ntrial_xgb = study_xgb.best_trial\nprint('Best score : {}'.format(trial_xgb.value))\nfor key, value in trial_xgb.params.items():\n    print(\"    {}: {}\".format(key, value))","3db0b14e":"xgb_max_depth = int(list(trial_xgb.params.items())[0][1])\nxgb_bfrac = list(trial_xgb.params.items())[1][1]\nxgb_min_child_weight = list(trial_xgb.params.items())[2][1]\nxgb_feature =  list(trial_xgb.params.items())[3][1]\nxgb_lambda_l2 = list(trial_xgb.params.items())[4][1]","3db7da69":"study_name2 = 'lgb'\nstudy_lgb = optuna.create_study(study_name=study_name2,direction='maximize',sampler=TPESampler(0))","2c28012a":"def opt_lgb(trial):    \n\n    num_leaves = int(trial.suggest_loguniform(\"num_leaves\", 3,20))\n    subsample =  trial.suggest_discrete_uniform('bfrac',0.5,1.0,q=0.05)\n    subsample_freq = int(trial.suggest_discrete_uniform('bfreq',1,5,q=1.0))\n    colsample_bytree = trial.suggest_discrete_uniform('feature',0.5,1.0,q=0.05)\n    reg_lambda = trial.suggest_loguniform(\"lambda_l2\", 1e-7, 10)\n    kFold= StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n    model = lgb.LGBMClassifier(num_leaves = num_leaves,subsample=subsample,subsample_freq=subsample_freq,\n                               colsample_bytree=colsample_bytree,reg_lambda=reg_lambda,random_state=0)\n    scoring = 'accuracy'\n    return cross_val_score(\n        model, train_data, ytrain, n_jobs=-1,scoring=scoring,cv=kFold).mean()","dba24a24":"study_lgb.optimize(opt_lgb, n_trials=50)","f2519013":"print('Total number of trials: ',len(study_lgb.trials))\ntrial_lgb = study_lgb.best_trial\nprint('Best score : {}'.format(trial_lgb.value))\nfor key, value in trial_lgb.params.items():\n    print(\"    {}: {}\".format(key, value))","be965ffe":"lgb_num_leaves = int(list(trial_lgb.params.items())[0][1])\nlgb_bfrac = list(trial_lgb.params.items())[1][1]\nlgb_bfreq = int(list(trial_lgb.params.items())[2][1])\nlgb_feature =  list(trial_lgb.params.items())[3][1]\nlgb_lambda_l2 = list(trial_lgb.params.items())[4][1]","20aba694":"study_name3 = 'gbdt'\nstudy_gbdt = optuna.create_study(study_name=study_name3,direction='maximize',sampler=TPESampler(0))","f57056fa":"def opt_gbdt(trial):    \n\n    max_depth = int(trial.suggest_loguniform(\"max_depth\", 3,20))\n    subsample =  trial.suggest_discrete_uniform('bfrac',0.5,1.0,q=0.05)\n    max_features = trial.suggest_discrete_uniform('feature',0.5,1.0,q=0.05)\n    min_samples_leaf = int(trial.suggest_discrete_uniform('min_sam_leaf',5.,50.,q=5.))\n    kFold= StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n    model = GradientBoostingClassifier(max_depth = max_depth,subsample=subsample,min_samples_leaf=min_samples_leaf,\n                               max_features=max_features,random_state=0)\n    scoring = 'accuracy'\n    return cross_val_score(model, train_data, ytrain, n_jobs=-1,scoring='accuracy',cv=kFold).mean()","8da1160e":"study_gbdt.optimize(opt_gbdt, n_trials=50)","bb405e0c":"print('Total number of trials: ',len(study_gbdt.trials))\ntrial_gbdt = study_gbdt.best_trial\nprint('Best score : {}'.format(trial_gbdt.value))\nfor key, value in trial_gbdt.params.items():\n    print(\"    {}: {}\".format(key, value))","70fef429":"gbdt_max_depth = int(list(trial_gbdt.params.items())[0][1])\ngbdt_bfrac = list(trial_gbdt.params.items())[1][1]\ngbdt_feature = list(trial_gbdt.params.items())[2][1]\ngbdt_min_sam_leaf =  int(list(trial_gbdt.params.items())[3][1])","ce281559":"final_model_xgb = xgb.XGBClassifier(max_depth = xgb_max_depth,subsample=xgb_bfrac,min_child_weight=xgb_min_child_weight,\n                               colsample_bytree=xgb_feature,reg_lambda=xgb_lambda_l2,random_state=0)\nfinal_model_lgb = lgb.LGBMClassifier(num_leaves = lgb_num_leaves,subsample=lgb_bfrac,subsample_freq=lgb_bfreq,\n                               colsample_bytree=lgb_feature,reg_lambda=lgb_lambda_l2,random_state=0)\nfinal_model_gbdt = GradientBoostingClassifier(max_depth = gbdt_max_depth,subsample=gbdt_bfrac,\n                                              min_samples_leaf=gbdt_min_sam_leaf,max_features=gbdt_feature,random_state=0)","ee0a9f74":"final_model_xgb.fit(train_data,ytrain)\nfinal_model_lgb.fit(train_data,ytrain)\nfinal_model_gbdt.fit(train_data,ytrain)","214fb274":"y_pred_xgb = final_model_xgb.predict_proba(test_data)[:,1]\ny_pred_lgb = final_model_lgb.predict_proba(test_data)[:,1]\ny_pred_gbdt = final_model_gbdt.predict_proba(test_data)[:,1]\ny_pred = (y_pred_xgb + y_pred_lgb + y_pred_gbdt) \/ 3\ny_pred = 1* (y_pred >= 0.5)","1f3cea3b":"sample_sub = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\nsample_sub['Survived'] = y_pred\nsample_sub.to_csv('sample_submission.csv',index=False)","77e3731a":"We make a meshgrid which can be used to visualize the decision surface.","18380cc6":"We can further explore Adaboost with decision tree of different depth.","ca96ef5f":"## 5. Gradient Boosting - n_estimators","e0e7a4a4":"Let's visualize decision boundary of Adaboost. You can see\nfor max_depth=1 with 100 n_estimators, the boosting algorithm is\ndrawing a more complex decision boundary than a single decision tree.\nAs we increase the depth the decision tree, the boundary also gets more complex.\nNote, for Adaboost, which is a meta-algorithm, we need to specify a base algorithm. In \nthis case, the base algorithm is decision tree.","4a351ba2":"Let's build a pipeline to encode all categorical variables.","9ff19f07":"Let's see what happens with Adaboost\/Decision stump. This is similar to \nwhat happens to decision tree. Optimal n_estimators in this case \nis around 2000. ","597da504":"This notebook will provide you with some visual understanding of boosting techniques. We will use titanic \ndataset as an example. Boosting belongs to the family of ensemble methods. The general idea is that\nimagine you already have a bag of models, you want to add an additional model to this bag. You want this new model\nto focus on the data points that are not modeled well, i.e. misclassified by previou models. We will look at both \nAdaboost and gradient boosting. The base estimator for Adaboost is typically, e.g. decision stump. Decision stump \nrefers to decision tree with depth 1. In this case, it simply makes a cut of data along certain feature direction. ","52170420":"Look at the magnitude and you can see the difference.","b0d2542a":"## Adaboost + Decision Stump","e51c71e3":"Let's also plot the tree structure for tree depth one, i.e. decision stump. In this\ncase, it simply splits the data in half. How it does the split? It splits based on \nto increase the purity of data in the leaf nodes. In this case, we use Gini index to \nmeasure purity.","357ec307":"We see that we have 177 missing values for \"Age\" and 2 missing values for \"Embarked\". Let's build a \nsimple pipeline to fill in missing values and process our data. We are going to change the ordering of \npclass, lower class to 1, middle class to 2, upper class to 3; We are going to fill missing values of Age\nwith median of age of training dataset, and \"Embarked\" with mode. The single missing value of \"Fare\" in test dataset will be filled with median of \"Fare\" of training dataset.","9613d44f":"## Gradient Boosting","53762453":"Let's take a look at gradient boosting with different tree depth.","80f6b243":"We will plot classification error as a function of n_estimators, as well as\ny_logit as a function of n_estimators, so you can see which estimator contributes\nmostly to y_logit. ","84c7de36":"Finally, let's train three different models on the simpified dataset and make a submission.","6f436a07":"## 1. Decision Tree","186242e9":"## 1. Xgboost","70c2dcf1":"## Decision Tree","d8264625":"Besides plotting the classification error, we can also \nplot out classification error for each individual estimator.","2102c1f1":"We first visualize how the decision boundary changes as \nwe increase the depth of decision tree. You can that for decision tree \nwith depth one, the points can not be correcly classified. As we \nincrease the depth of tree, the decision boundary becomes more complex. The\nclassifier now is able to seperate points belong to two different classes.","599c1bd6":"Let's take a look at how classification error changes\nas a function of tree depth. The training error keeps going down\nuntil it saturates. However, validation error goes down first then goes back up.\nThis means after certain depth, the algorithm is overfitting on the training dataset.","460ab77b":"## In-depth Understanding of Boosting","2088bca6":"# Model Training & Parameter Tuning","1e8427c6":"## 2. LightGBM","b3c4a111":"Let's also make a plot for tree depth 2.","9b891b70":"## Adaboost + Decision Tree","0a8b1a65":"Let's choose an intermediate depth and investigate learning rate 0.5, 0.1 and 0.01. You can play further with more numbers.\nIn general, as learning rate goes down, number of estimators goes up.","c1069722":"Let's further explore the effects of removing front n trees and last n trees. You can\nsee the front trees are the most important. Sometimes, removing last n trees can even help\nreduce logistic loss. Also, be careful with magnitudes, the effect of removing first n trees\nare much stronger than that of last n trees.","8bbcac81":"# Preprocess Data","3581a450":"Let's further split the training data into training set and validation set","706c347f":"Let's take a look at Adaboost with decision stump. This means that \nthe base algorithm is decision tree with depth 1. We increase number\nof estimators, you can see that with a week base learner (decision stump),\nwe can still draw a descent decision boundary. This illustrates the key concept of ensemble\nlearning, which is to combine the strength and weakness of each individual learner.","388c5dc1":"Let's check missing values.","75f94c30":"## 3. GBDT (Scikit-learn)","78bda69b":"## 2. Adaboost - Tree Depth","27a9c48d":"We plot the distribution for \"Age\" and \"Fare\" variable, you can see\nthat their distributions are skewed.","70fa274a":"# Ensemble","8b0e77b1":"Let's take a look at gradient boosting with different n_estimators.","26073f50":"Let's check how logistic loss changes as a function of number of estimators. You can see \noverfitting occurs for all tree depth after certain number of n_estimators.","4c6fa315":"# Playground & Visualization","d00d7865":"## 3. Adaboost\/Decision Stump - n_estimators","a78b5d7e":"## 4. Gradient Boosting - Tree Depth","93e1eefe":"# Motivation & Background","eaa3b1db":"For the data used in Playground, we will only fit the classifier to \"Age\" and \"Fare\" features. This allows\nfor visualization. \"Age\" is restricted between 20 and 40, and fare is restricted between  0 and 50."}}