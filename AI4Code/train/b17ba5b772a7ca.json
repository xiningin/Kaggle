{"cell_type":{"fa3887d7":"code","247e4598":"code","0e773603":"code","c8f7d456":"code","27d4001f":"code","821117d1":"code","e5bd851f":"code","fcc7b050":"code","28e85903":"code","052d3758":"code","12e9c6af":"code","a2251ce3":"code","f8d2a4a4":"code","a8cbf91d":"code","68aeee43":"code","56d3fb96":"code","3bfd938e":"code","d6144342":"code","70e2e555":"code","ead0ffb9":"code","92c93055":"code","4925ad87":"code","06333702":"code","f2c09dab":"markdown"},"source":{"fa3887d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import f_regression, f_classif, SelectKBest, chi2\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder,LabelEncoder\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","247e4598":"por = \"..\/input\/student-alcohol-consumption\/student-por.csv\"\n\nd3 = pd.read_csv(por)\n","0e773603":"print(d3)","c8f7d456":"# Tr\u01b0\u1eddng k\u00fd t\u1ef1 s\u1ed1\nnumberic_data = list(d3.describe().columns)\nprint(numberic_data)","27d4001f":"#  Chuy\u1ec3n category v\u1ec1 s\u1ed1\nle=LabelEncoder()\nfor k in d3.columns:\n    if k not in numberic_data:\n        print(k)\n        d3[k]=le.fit_transform(d3[k])\n","821117d1":"## add illegal drinking\ndef illegal_drinking(alc, age):\n    if alc > 1 and age < 18:\n        return True\n    return False\n\nd3[\"Pedu\"] = d3[[\"Fedu\", \"Medu\"]].max(axis=1)\nd3[\"illegal_drink\"] = d3.apply(lambda x : illegal_drinking(x[\"Walc\"], x[\"age\"]), axis=1)\nd3[\"Drinking\"] = (d3[\"Walc\"] > 1)\ndf_not_use = ['Drinking', 'Walc', 'illegal_drink', 'age','Dalc']","e5bd851f":"def get_label(df):\n    return df[\"Drinking\"].to_numpy(dtype=np.int32)","fcc7b050":"le=LabelEncoder()\nfor k in d3.columns:\n    if k not in numberic_data:\n        print(k)\n        d3[k]=le.fit_transform(d3[k].astype('str'))\nfeatures = [k for k in d3.columns if k not in df_not_use]\nfeatures = d3[features].to_numpy()","28e85903":"import os\nimport pickle\n\nfrom sklearn.manifold import TSNE\nlink_save_tsne_model = f'save_TSNE'\nprint(f'Save at{link_save}')\nos.makedirs(link_save, exist_ok=True)\nn_clusters = [5]\nfrom sklearn.cluster import KMeans\n# parameters for cluster\nkmeans_kwargs = {\n    \"init\": \"random\",\n    \"n_init\": 10,\n    \"max_iter\": 300,\n    \"random_state\": 42,\n}\n\n#  dimensionality reduction\n\n\n\nX = tsne.fit_transform(features)\n\n\nfeatures = X\n\nlabels = None\nfor n_cluster in n_clusters:\n\n    kmeans = KMeans(n_clusters=n_cluster, **kmeans_kwargs)\n    labels = kmeans.fit(features).predict(features)\n\n    plt.scatter(X[:, 0], X[:, 1], c=labels)\n    plt.show()\n    plt.close()","052d3758":"def calculate_chi2_feature_importance(df):\n    print('Start calculate chi2')\n    Y = get_label(df)\n    features = [k for k in df.columns if k not in df_not_use]\n    print(features)\n    X = df[features].to_numpy()\n    chi2_selector = chi2(X, Y)\n    chi2_scores = pd.DataFrame(list(zip([k for k in df.columns if k not in df_not_use], chi2_selector[0], chi2_selector[1])),\n                               columns=['feature', 'score', 'pval'])\n    chi2_scores = chi2_scores.sort_values(by=['score'], ascending=False)\n    print('Complete')\n    return chi2_scores\n","12e9c6af":"calculate_chi2_feature_importance(d3)","a2251ce3":"def calculate_f_regression_feature_importance(df):\n    print('Start calculate f_regression')\n    Y = get_label(df)\n    features = [k for k in df.columns if k not in df_not_use]\n    print(features)\n    X = df[features].to_numpy()\n    f_val, p_val = f_regression(X, Y)\n    f_regression_scores = pd.DataFrame(\n        list(zip([k for k in df.columns if k not in df_not_use], f_val, p_val)),\n        columns=['feature', 'score', 'pval']\n    )\n    f_regression_scores = f_regression_scores.fillna(0)\n    f_regression_scores = f_regression_scores.sort_values(by=['score'], ascending=False)\n    print('Complete')\n    return f_regression_scores\n","f8d2a4a4":" calculate_f_regression_feature_importance (d3)","a8cbf91d":"def calculate_f_classif_feature_importance(df):\n    print('Start calculate f_classif')\n    Y = get_label(df)\n    features = [k for k in df.columns if k not in df_not_use]\n    print(features)\n    X = df[features].to_numpy()\n    f_classif_val, p_classif_val = f_classif(X, Y)\n    f_classif_scores = pd.DataFrame(list(zip([k for k in df.columns if k not in df_not_use], f_classif_val, p_classif_val)),\n                                    columns=['feature', 'score', 'pval'])\n    f_classif_scores = f_classif_scores.fillna(0)\n    f_classif_scores = f_classif_scores.sort_values(by=['score'], ascending=False)\n    print('Complete')\n    return f_classif_scores","68aeee43":"calculate_f_classif_feature_importance(d3)","56d3fb96":"le=LabelEncoder()\nfor k in d3.columns:\n    if k not in numberic_data:\n        print(k)\n        d3[k]=le.fit_transform(d3[k].astype('str'))","3bfd938e":"def calculate_RF_feature_importance(df):\n    print('Start calculate RF')\n    Y = get_label(df)\n    features = [k for k in df.columns if k not in df_not_use]\n    print(features)\n    X = df[features].to_numpy()\n    model = RandomForestClassifier()\n    # fit the model\n    model.fit(X, Y)\n    # get importance\n    importance = model.feature_importances_\n    # summarize feature importance\n    score = []\n    for i, v in enumerate(importance):\n        score.append(v)\n    RF_score = pd.DataFrame(list(zip(list(features), score)), columns=['feature', 'score'])\n    RF_score = RF_score.sort_values(by=['score'], ascending=False)\n    print('Complete')\n    return RF_score\n","d6144342":"calculate_RF_feature_importance(d3)","70e2e555":"def calculate_MLP_feature_importance(df):\n    print('Start calculate MLP')\n    Y = get_label(df)\n    features = [k for k in df.columns if k not in df_not_use]\n    print(features)\n    X = df[features].to_numpy()\n    scaler = StandardScaler()\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    model = MLPClassifier(random_state=0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict_proba(X_test)[:, 1]\n\n    def get_feature_importance(feature_j, n):\n        s = roc_auc_score(y_test, y_pred)\n        total = 0\n        for i in range(n):\n            perm = np.random.permutation(range(X_test.shape[0]))\n            X_test_ = X_test.copy()\n            X_test_[:, feature_j] = X_test[perm, feature_j]\n            y_pred_ = model.predict(X_test_)\n            s_ij = roc_auc_score(y_test, y_pred_)\n            total += s_ij\n        return s - total \/ n\n\n    f = []\n    for j in range(X_test.shape[1]):\n        f_j = get_feature_importance(j, 1)\n        f.append(f_j)\n    MLP_score = pd.DataFrame(list(zip(list(features), f)), columns=['feature', 'score'])\n    MLP_scores = MLP_score.sort_values(by=['score'], ascending=False)\n    print('Complete')\n    return MLP_scores\n","ead0ffb9":"calculate_MLP_feature_importance(d3)","92c93055":"from sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\ndf =d3\nprint(df)\nfeatures = [k for k in df.columns if k not in df_not_use]\nfrom catboost import CatBoostClassifier, Pool, cv\ncatboost_model = CatBoostClassifier(\n    cat_features=[k for k in features if k not in numberic_data],\n    loss_function=\"Logloss\",\n    eval_metric=\"AUC\",\n    iterations=200,\n)\nY = get_label(df)\nfeatures = [k for k in df.columns]\nX = df[features]\ncatboost_model.fit(X,Y)\nimportance = catboost_model.get_feature_importance()\nscore = []\nfor i, v in enumerate(importance):\n    score.append(v)\nres_score = pd.DataFrame(list(zip(list(features), score)), columns=['feature', 'score'])\nres_score = RF_score.sort_values(by=['score'], ascending=False)\nprint(res_score)\nprint('Complete')\n","4925ad87":"def evaluate_top_feature(num_features=None):\n    catboost_model = CatBoostClassifier(\n    cat_features=[k for k in list(res_score['feature'])[:num_features] if k not in numberic_data],\n    loss_function=\"Logloss\",\n    eval_metric=\"AUC\",\n    iterations=200,\n)\n    \n    Y = get_label(df)\n    features = [k for k in list(res_score['feature'])[:num_features]] if num_features !=None else [k for k in list(res_score['feature'])]\n    X = df[features]\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n    catboost_model.fit(X_train,y_train)\n    y_pred = catboost_model.predict(X_test)\n    print(classification_report(y_test, y_pred))","06333702":"evaluate_top_feature(30)","f2c09dab":"### "}}