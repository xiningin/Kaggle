{"cell_type":{"455502a1":"code","64a3c2c6":"code","d49e7261":"code","6faddfbb":"code","326cd0ab":"code","b405aa31":"code","1d90eabe":"code","8f4865fd":"code","0d249422":"code","a5ffc736":"markdown","abc83b17":"markdown","3b2bef0d":"markdown","414682ae":"markdown","7a5223ca":"markdown","9f1e6205":"markdown","8bebee29":"markdown","53aea0c6":"markdown","0dfeea53":"markdown","b245bd1c":"markdown","c1a364f0":"markdown","fbdeb270":"markdown","ed8b7530":"markdown","04d07231":"markdown"},"source":{"455502a1":"import numpy as np\nimport pandas as pd\nimport json\nimport warnings\n#from cuml.linear_model import Ridge, Lasso\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.linear_model import Ridge, RidgeCV, OrthogonalMatchingPursuit\nfrom sklearn.svm import SVR\nimport scipy\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom joblib import Parallel, delayed, load, dump\n\ndef metric(y_true, y_pred):\n    return np.mean(np.sum(np.abs(y_true - y_pred), axis=0)\/np.sum(y_true, axis=0))\n\n# turn cell width to 100%\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))\n\nbase_path = '\/kaggle\/input\/trends-assessment-prediction\/'\nfeature_path = '\/kaggle\/input\/pni-trends-features\/'","64a3c2c6":"# we did a site 1 vs site 2 classification (TPrate 86%) to later regress out the sitedifferences in our features\nprediction_pooled = pd.read_csv(feature_path + \"site_prediction.csv\").drop('Unnamed: 0', axis=1)\ngraphparams = pd.read_csv(feature_path + 'fnc_graph_params.csv')\n# we extracted the Cerebral Spinal Fluid, White Matter and Gray Matter from the spatial maps \ncsf_df = pd.read_csv(feature_path + 'csf_features')\nwm_df = pd.read_csv(feature_path + 'wm_features')\ngm_all_df = pd.read_csv(feature_path + 'gm_all_features')\ngm_df = pd.read_csv(feature_path + 'gm_max_features')\n# the PNI features are a new set of features that are currently in the works for a publication\npni_df = pd.read_csv(feature_path + 'PNI_features')\nloading_df = pd.read_csv(base_path +\"loading.csv\")\n# The assymetry between the two hemispheres is a good predictor for age\n# the assymetry was clipped at zero and winsorized for better performance\nasym_df = pd.read_csv(feature_path +  'asym_winsorized_features')","d49e7261":"# the naming of the fnc features indicate that they are based off from large scale resting state networks\n# the internal and external connectivity between the regions are therefore extracted as new features\n\nfnc_data = pd.read_csv(base_path + 'fnc.csv')\n\"\"\"\nSCN - Sub-cortical Network\nADN - Auditory Network\nSMN - Sensorimotor Network\nVSN - Visual Network\nCON - Cognitive-control Network\nDMN - Default-mode Network\nCBN - Cerebellar Network\n\"\"\"\n\nnetworks = np.array(['SCN', 'ADN', 'SMN', 'VSN', 'CON', 'DMN', 'CBN'])\ninternal_conn = np.zeros((len(fnc_data.Id), len(networks)))\nexternal_conn = np.zeros((len(fnc_data.Id), len(networks)))\n\nnetmat = pd.DataFrame({'Id': fnc_data.Id})\nnetmat_count = pd.DataFrame({'Id': [int(1)]})\n\ninternal_count = np.zeros(len(networks))\nexternal_count = np.zeros(len(networks))\n\nfor c in fnc_data.drop('Id', axis=1).columns:\n    #print(c)\n    splitted=c.split('_')\n    #print(splitted[0][:3], splitted[2][:3])\n    netmat_col=splitted[0][:3] + '_vs_' + splitted[2][:3]\n     \n    if netmat_col not in netmat:\n        netmat[netmat_col]=fnc_data[c].values\n        netmat_count[netmat_col]=1\n    else:\n        netmat[netmat_col]+=fnc_data[c].values\n        netmat_count[netmat_col]+=1\n        \n    ########### internal - external\n    \n    if splitted[0][:3] == splitted[2][:3]:\n        #print(\"within\", np.argwhere(networks==splitted[0][:3])[0])\n        whichnetwork = np.argwhere(networks==splitted[0][:3])[0][0]\n        internal_conn[:,whichnetwork] += fnc_data[c].values\n        internal_count[whichnetwork] += 1\n    else:\n        #print(\"between\", np.argwhere(networks==splitted[0][:3])[0] , np.argwhere(networks==splitted[2][:3])[0] )\n        whichnetwork1 = np.argwhere(networks==splitted[0][:3])[0][0]                 \n        external_conn[:,whichnetwork1] += fnc_data[c].values\n        whichnetwork2 = np.argwhere(networks==splitted[2][:3])[0][0]                \n        external_conn[:,whichnetwork2] += fnc_data[c].values\n        \n        external_count[whichnetwork1] += 1\n        external_count[whichnetwork2] += 1\n\n# normalize netmat\nnetmat \/= netmat_count.values\nnetmat = pd.DataFrame(netmat)\n\n# normalise\ninternal_conn \/= internal_count \nexternal_conn \/= external_count \n                         \ninternal_conn = pd.DataFrame(internal_conn, columns=[ \"int_\" + str(c) for c in networks])\ninternal_conn.insert(0, 'Id', fnc_data.Id)\nexternal_conn = pd.DataFrame(external_conn, columns=[ \"ext_\" + str(c) for c in networks]) \nexternal_conn.insert(0, 'Id', fnc_data.Id)\n\n\n# div\ninternal_conn_tmp = internal_conn.copy()\nexternal_conn_tmp = external_conn.copy()\nexternal_conn_tmp.columns = internal_conn_tmp.columns = ['Id'] + [\"div_\" + str(c) for c in networks]\nintdivext = internal_conn_tmp\/external_conn_tmp\nintdivext.Id=internal_conn.Id\n\n# mult (interaction)\nexternal_conn_tmp.columns = internal_conn_tmp.columns = ['Id'] + [\"mult_\" + str(c) for c in networks]\nintmultext = internal_conn_tmp*external_conn_tmp\nintmultext.Id=internal_conn.Id\n\n","6faddfbb":"fnc=fnc_data\n\nfnc_feat_vecs = fnc.iloc[:, 1:].values\nfnc_feat_vecs.shape\nicn_numbers = pd.read_csv(base_path + \"ICN_numbers.csv\")\nicn_numbers[\"FNC_name\"] = [\"SCN(69)\"] + list(map(lambda x: x.split(\"_\")[0], fnc.columns[1:53]))\ncol_labels = icn_numbers[\"FNC_name\"].values\n\ndegree=pd.DataFrame(np.zeros((fnc_feat_vecs.shape[0], len(col_labels))), columns=['degree_' + c for c in col_labels])\nclustering_r=pd.DataFrame(np.zeros((fnc_feat_vecs.shape[0], len(col_labels))), columns=['clusteringr_' + c for c in col_labels])\nclustering_i=pd.DataFrame(np.zeros((fnc_feat_vecs.shape[0], len(col_labels))), columns=['clusteringi_' + c for c in col_labels])\nbetweenness_centrality=pd.DataFrame(np.zeros((fnc_feat_vecs.shape[0], len(col_labels))), columns=['between_' + c for c in col_labels])\neigenvector_centrality=pd.DataFrame(np.zeros((fnc_feat_vecs.shape[0], len(col_labels))), columns=['eigenvec_' + c for c in col_labels])\n","326cd0ab":"# get colnames for each feature group \ncol_loading_features = list(loading_df.columns[1:])\ncol_fnc_features = list(fnc_data.columns[1:])\ncol_netmat_features = list(netmat.columns[1:])\ncol_internalconn_features = list(internal_conn.columns[1:])\ncol_externalconn_features = list(external_conn.columns[1:])\ncol_intmultext_features = list(intmultext.columns[1:])\ncol_gm_features = list(gm_df.columns[1:])\ncol_wm_features = list(wm_df.columns[1:])\ncol_csf_features = list(csf_df.columns[1:])  \ncol_gm_all = list(gm_all_df.columns[1:])  \ncol_pni = list(pni_df.columns[1:])\ncol_asym_features = list(asym_df.columns[1:]) \ncol_graphparams_features = list(graphparams.columns[1:]) \ncol_degree_features = list(degree)    \ncol_clustering_features = list(clustering_r) + list(clustering_i)\ncol_clustering_r_features = list(clustering_r) \ncol_clustering_i_features = list(clustering_i) \ncol_betweenness_centrality_features = list(betweenness_centrality)\ncol_eigenvector_centrality_features = list(eigenvector_centrality)\n\n# merge dataframes\ndf = loading_df.merge(fnc_data, on=\"Id\").merge(netmat, on=\"Id\").merge(internal_conn, on=\"Id\").merge(external_conn, on=\"Id\").merge(intmultext, on='Id').merge(gm_df, on=\"Id\").merge(wm_df, on=\"Id\").merge(csf_df, on=\"Id\").merge(pni_df, on='Id').merge(asym_df, on='Id').merge(graphparams, on='Id')\n","b405aa31":"# regressing out the site effects\nfrom sklearn.linear_model import LinearRegression\ndf_pandas = df.copy()\ntmp_df=prediction_pooled.merge(df_pandas, on='Id')\nfor c in tmp_df.columns:\n    if c=='Id' or c=='site_predicted_fixed' or c=='site_predicted' or c=='predicted':\n        continue\n    model = LinearRegression()\n    model.fit(tmp_df.site_predicted_fixed.values.reshape(-1,1), tmp_df[c].values)\n    prediction = model.predict(tmp_df.site_predicted_fixed.values.reshape(-1,1))\n    df_pandas.at[:,c] = (df_pandas[c] - prediction)\ndf_sitereg = pd.DataFrame(df_pandas)\n\ndf_sitereg = df_sitereg.merge(gm_all_df , on = 'Id')\n\nlabels_df = pd.read_csv(base_path + \"train_scores.csv\")\nlabels_df[\"is_train\"] = True\n\ndf = df_sitereg.merge(labels_df, on=\"Id\", how=\"left\")\n\ntest_df = df[df[\"is_train\"] != True].copy()\ndf = df[df[\"is_train\"] == True].copy()","1d90eabe":"from sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import cross_val_score\n\n# optimal parameters for each pipeline were obtained with the RandomizedSearchCV of sklearn\n\nwith open(feature_path + 'optpars.json', 'r') as fp:\n    optpars = json.load(fp)\n\ndef down_scale(x, factor):\n    return x*factor\n\ndef square(x):\n    return x*x\n\n\ndef winsorize(x, l=0.0001, h=0.0001):\n    return scipy.stats.mstats.winsorize(x, (l, h))","8f4865fd":"test_result=test_df.copy()\n\nreg = dict()\n\nfeatures = col_loading_features+col_fnc_features+col_netmat_features+col_internalconn_features+col_externalconn_features+col_intmultext_features+col_gm_features+col_wm_features+col_csf_features+col_asym_features+col_graphparams_features+col_gm_all+col_pni\n\n\nfor target in ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']:\n    print(\"*****\",target)\n    \n    dfnona = df[df[target].notnull()]\n\n    estimators = [\n        #0 ##################################################\n        ('sbm+funcdeweighted_win_ridge',\n        Pipeline([('columns', ColumnTransformer([\n                                        ('sbm', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_loading_features),\n                                            ('fnc', FunctionTransformer(down_scale, kw_args={'factor': 1\/600}, validate=True), col_fnc_features),\n                                            ('internal_conn', FunctionTransformer(down_scale, kw_args={'factor': 1\/600}, validate=True), col_internalconn_features),\n                                            ('external_conn', FunctionTransformer(down_scale, kw_args={'factor': 1\/600}, validate=True), col_externalconn_features),\n                                            ('intmultext', FunctionTransformer(down_scale, kw_args={'factor': 1\/600}, validate=True), col_intmultext_features),\n                                          ],remainder='drop')),\n            ('win', FunctionTransformer(winsorize, kw_args={'l': 0.0001, 'h': 0.0001}, validate=True)),\n            ('e', Ridge(alpha=optpars['sbm+funcdeweighted_win_ridge'][target]))\n        ])),\n        #1 ##################################################\n        ('sbm+funcdeweighted_win_ridgebag',\n        Pipeline([('columns', ColumnTransformer([\n                                            ('sbm', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_loading_features),\n                                            ('fnc', FunctionTransformer(down_scale, kw_args={'factor': 1\/600}, validate=True), col_fnc_features),\n                                            ('gm', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_gm_features),\n                                            ('gm2', FunctionTransformer(square, validate=True), col_gm_features),\n                                            ('wm', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_wm_features),\n                                            ('wm2', FunctionTransformer(square, validate=True), col_wm_features),\n                                            ('csf', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_csf_features),\n                                            ('csf2', FunctionTransformer(square, validate=True), col_csf_features),\n                                          ],remainder='drop')),\n            ('win', FunctionTransformer(winsorize, kw_args={'l': 0.0001, 'h': 0.0001}, validate=True)),\n            ('e', BaggingRegressor(Ridge(alpha=optpars['sbm+funcdeweighted_win_ridgebag'][target]),\n                                                  n_estimators=20,\n                                                  random_state=42,\n                                                  max_samples=0.8,\n                                                  max_features=0.8))\n        ])),\n        #2 ##################################################\n        ('sbm+sbm_win_ridge',\n        Pipeline([('columns', ColumnTransformer([('sbm', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_loading_features),\n                                         ('sbm2', FunctionTransformer(square, validate=True), col_loading_features)\n                                         ],remainder='drop')),\n            ('win', FunctionTransformer(winsorize, kw_args={'l': 0.0001, 'h': 0.0001}, validate=True)),\n            ('e', Ridge(alpha=optpars['sbm+sbm_win_ridge_alpha'][target]))\n        ])),\n        #3 ##################################################\n        ('sbm+sbm_win_ridgebag',\n        Pipeline([('columns', ColumnTransformer([('sbm', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_loading_features),\n                                         ('sbm2', FunctionTransformer(square, validate=True), col_loading_features)\n                                         ],remainder='drop')),\n            ('win', FunctionTransformer(winsorize, kw_args={'l': 0.0001, 'h': 0.0001}, validate=True)),\n            ('e', BaggingRegressor(Ridge(alpha=optpars['sbm+sbm_win_ridge_alphabag'][target]),\n                                                  n_estimators=20,\n                                                  random_state=42,\n                                                  max_samples=0.8,\n                                                  max_features=0.8))\n        ])),\n        #4 ##################################################\n        ('fnc_extended_ridge', Pipeline([\n            ('columns', ColumnTransformer([('fnc', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_fnc_features),\n                                           ('internal_conn', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_internalconn_features),\n                                           ('external_conn', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_externalconn_features),\n                                           ('intmultext', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_intmultext_features),\n                                           ('netmat', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_netmat_features),\n                                           ('netmat2', FunctionTransformer(square, validate=True), col_netmat_features)\n                                          ],\n                                           remainder='drop')),\n            ('win', FunctionTransformer(winsorize, validate=True)),\n            ('e', Ridge(alpha=optpars['fnc_extended_ridge'][target]))\n        ])),\n        #5 ##################################################\n        ('fnc_extended_ridgebag', Pipeline([\n            ('columns', ColumnTransformer([('fnc', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_fnc_features),\n                                           ('internal_conn', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_internalconn_features),\n                                           ('external_conn', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_externalconn_features),\n                                           ('intmultext', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_intmultext_features),\n                                           ('netmat', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_netmat_features),\n                                           ('netmat2', FunctionTransformer(square, validate=True), col_netmat_features)\n                                          ],\n                                           remainder='drop')),\n            ('win', FunctionTransformer(winsorize, validate=True)),\n            ('e', BaggingRegressor(Ridge(alpha=optpars['fnc_extended_ridgebag'][target]),\n                                                  n_estimators=20,\n                                                  random_state=42,\n                                                  max_samples=0.8,\n                                                  max_features=0.8))\n        ])),\n        #6 ##################################################\n        ('fnc_simple_ridge', Pipeline([\n            ('columns', ColumnTransformer([('internal_conn', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_internalconn_features),\n                                            ('external_conn', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_externalconn_features),\n                                            ('netmat', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_netmat_features)\n                                            \n                                          ],\n                                           remainder='drop')),\n            ('win', FunctionTransformer(winsorize, validate=True)),\n            ('poly', PolynomialFeatures()),\n            ('e', Ridge(alpha=optpars['fnc_simple_ridge'][target]))\n        ])),\n        #7 ##################################################\n        ('spatial_maps_gmmax_wm_csf', Pipeline([\n            ('columns', ColumnTransformer([('gm', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_gm_features),\n                                            ('gm2', FunctionTransformer(square, validate=True), col_gm_features),\n                                            ('wm', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_wm_features),\n                                            ('wm2', FunctionTransformer(square, validate=True), col_wm_features),\n                                            ('csf', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_csf_features),\n                                            ('csf2', FunctionTransformer(square, validate=True), col_csf_features)                             \n                                          ],\n                                           remainder='drop')),\n            ('win', FunctionTransformer(winsorize, validate=True)),\n            ('e', Ridge(alpha=optpars['spatial_maps_gmmax_wm_csf'][target]))\n        ])),\n        #8 ##################################################\n        ('sbm+funcdeweighted_win_svr',\n        Pipeline([('columns', ColumnTransformer([\n                                            ('sbm', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_loading_features),\n                                            ('fnc', FunctionTransformer(down_scale, kw_args={'factor': 1\/600}, validate=True), col_fnc_features),\n                                            ('internal_conn', FunctionTransformer(down_scale, kw_args={'factor': 1\/600}, validate=True), col_internalconn_features),\n                                            ('external_conn', FunctionTransformer(down_scale, kw_args={'factor': 1\/600}, validate=True), col_externalconn_features),\n                                            ('intmultext', FunctionTransformer(down_scale, kw_args={'factor': 1\/600}, validate=True), col_intmultext_features)\n                                          ],remainder='drop')),\n            ('win', FunctionTransformer(winsorize, kw_args={'l': 0.0001, 'h': 0.0001}, validate=True)),\n            ('e', SVR(C=optpars['sbm+funcdeweighted_win_svr'][target], cache_size=1000, gamma='scale'))\n        ])),\n\n        # 9 ##############################\n        ('fnc_graph_ridge_alpha',\n        Pipeline([\n            ('columns', ColumnTransformer([('sbm', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_loading_features),\n                                            ('fnc', FunctionTransformer(down_scale, kw_args={'factor': 1\/600}, validate=True), col_fnc_features),\n                ('degree', FunctionTransformer(down_scale, kw_args={'factor': optpars['fnc_graph_ridge_degree_scale'][target]}, validate=True), col_degree_features),\n                ('clustering_r', FunctionTransformer(down_scale, kw_args={'factor': optpars['fnc_graph_ridge_clustering_r_scale'][target]}, validate=True), col_clustering_r_features),\n                ('clustering_i', FunctionTransformer(down_scale, kw_args={'factor': optpars['fnc_graph_ridge_clustering_i_scale'][target]}, validate=True), col_clustering_i_features),\n                ('betweenness', FunctionTransformer(down_scale, kw_args={'factor': optpars['fnc_graph_ridge_betweenness_scale'][target]}, validate=True), col_betweenness_centrality_features),  \n                ('eigenvec', FunctionTransformer(down_scale, kw_args={'factor': optpars['fnc_graph_ridge_eigenvec_scale'][target]}, validate=True), col_eigenvector_centrality_features),\n                ('gm', FunctionTransformer(down_scale, kw_args={'factor': optpars['fnc_graph_ridge_gm_scale'][target]}, validate=True), col_gm_features),\n                ('csf', FunctionTransformer(down_scale, kw_args={'factor': optpars['fnc_graph_ridge_csf_scale'][target]}, validate=True), col_csf_features),\n                           \n                                                              ],\n                                           remainder='drop')),\n            ('var', VarianceThreshold(0)),\n            ('e', Ridge(alpha=optpars['fnc_graph_ridge_alpha'][target]))\n        ])),\n        # 10 #######################\n        ('gm_all_ridge',\n        Pipeline([\n            ('columns', ColumnTransformer([ ('wm', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_wm_features),\n                                            ('wm2', FunctionTransformer(square, validate=True), col_wm_features),\n                                            ('csf', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_csf_features),\n                                            ('csf2', FunctionTransformer(square, validate=True), col_csf_features),\n                                            ('gm_all', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_gm_all)                              \n                                          ],\n                                           remainder='drop')),\n            ('win', FunctionTransformer(winsorize, validate=True)),\n            ('e', Ridge(alpha=optpars['gm_all_ridge_alpha'][target]))\n        ])),\n        \n        # 11 ################################\n        # base PNI\n        ('sbm_PNI_ridge',\n        Pipeline([('columns', ColumnTransformer([\n                                        ('sbm', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_loading_features),\n                                        ('PNI', FunctionTransformer(down_scale, kw_args={'factor': 1}, validate=True), col_pni)\n                                          ],remainder='drop')),\n            ('e', Ridge(alpha=optpars['sbm_PNI_ridge_alpha'][target]))\n        ])),\n        \n    ]\n    \n    #drop some estimators:\n    if target=='domain1_var1' or target=='domain2_var2':\n        estimators = estimators[2:] # no ridge with sbm+fnc\/600\n    \n    print([e[0] for e in estimators])\n        \n\n    from sklearn.ensemble import StackingRegressor\n    reg[target] = StackingRegressor(estimators=estimators, final_estimator=RidgeCV(alphas=[500, 1000, 5000, 10000]), passthrough=False, verbose=0, cv=6, n_jobs=1)\n    reg[target].fit(dfnona[features], dfnona[target].values)\n    \n    #####################################################\n    print(metric(reg[target].predict(dfnona[features]), dfnona[target].values))\n    # note that this below is suboptimal in terms of computatioin time:\n    cv_score = cross_val_score(reg[target],\n                                 dfnona[features],\n                                 dfnona[target].values,\n                                 scoring=make_scorer(metric, greater_is_better=False))\n    print(np.mean(cv_score), cv_score)\n    test_result[target] = reg[target].predict(test_result[features])","0d249422":"sub_df = pd.melt(test_result[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=[\"Id\"], value_name=\"Predicted\")\nsub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n\nsub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\nassert sub_df.shape[0] == test_df.shape[0]*5\nsub_df.to_csv(\"submission.csv\", index=False)\nsub_df.head(10)","a5ffc736":"And that's all. The variable \"test_result\" was simply melted and submitted.\nThis submission scores 0.15681 on the public and 0.15753 on the private leaderboard.\nWe are looking forward to see the site-2 leaderboard results!\n\n# We are thankful to the organizers for this most exciting competition!","abc83b17":"# Here we regress out the predicted site affect from all features.","3b2bef0d":"Dr. Tamas Spisak, PhD\n\nRobert Englert, Msc\n\nBalint Kincses, MD, PhD\n\nLaboratory of Predictive Neuroimaging\nInstitute for Artificial Intelligence in Medicine\nInstitute for Diagnostic and Interventional Radiology and Neuroradiology\nUniversity Hospital Essen\n\n![image.png](attachment:image.png)\n\nhttps:\/\/pni-lab.github.io\/\n\n","414682ae":"## Here we compute the RSN-based network transformations","7a5223ca":"Getting column names to easily address feature types later.","9f1e6205":"### Hi, we are Tamas Spisak, Balint Kincses and Robert Englert from the Laboratory of Predictive Neuroimaging UK Essen (Germany). First of all, let us thank the organizers for this most exciting Kaggle competition. This was our first ever Kaggle competition and we landed 9th (0.15681) on the public and 13th (0.15753) on the private leaderboard! \n### Let us start with a bit of \"advertisement\": We are currently expanding our research group, so if you are interested in persuing a careeer in Neuroscience\/Neuroimaging by applying techniques of Machine Learning, click here: https:\/\/pni-lab.github.io\/blog\/postdoc_and_phd_student_job_advert\/\n### What better place to advertise, than on a Kaggle competition with Neuroscience Data? ;)\n\n![image.png](attachment:image.png)","8bebee29":"Here's the ensemble itself. No magic here, mainly ridges (and some SVMs) for various subsets of the fetures and a bit of bagging now-and-than.\nNote that this part wasn't systematically optimised (as we were primarly interested in the performance of our new fearures, rather than the ML-part).","53aea0c6":"Just some data consolidation below:","0dfeea53":"Optimal hyperparameters were computed locally for each model. \nAs mentioned above, we performed a low-key optimization. We are sure a more comprehensive optimization could still considerably imporve the final model performance.\n\nBelow you will see the \"optimal\" hyperparameters for each model simply stored in a map.\nThen we define some helper functions, to be used in scikitlearn pipelines.","b245bd1c":"# Model","c1a364f0":"# Optimal Parameters for the different Pipelines","fbdeb270":"# Here we load all the features from the attached dataset\nSome of the features required a lot of compoutations, so we computed those on local machines.\nBelow some details about each.\n- site_prediction: While we do not detail the classification model here, it was eseentially a simple RidgeClassifier with some manual calibration.\n- fnc_graph_params: Node-level graph-theoretical parameters from the fnc data as calculated by networkx.\n- csf_features: mean \"activity\" in cerebrospinal fluid regions per IC\n- wm_features: mean activity in white matter, per IC\n- gm_all_features and gm_max_features: mean activity values, extracted from the 122-region version of the BASC-atlas (nilearn)\n- asym_winsorized_features: assymetry indeces, computed on the voxel-level and then averaged across activated voxels.\n\n- PNI_features: our secret weapon, a recurrent neural network-based transformation of the functional connectivity matrix, which embeds both connectivity strength and complex network structure into a smaller dataset. This boosted our predictive accuracy significantly! The manuscript describing the approach is currently in prep, but we are happy to share some details upon reasonable request.\n\nAll the algorithms producing the other features are also available upon request.","ed8b7530":"## Just some imports","04d07231":"# Our approach in a nutshell:\n\nWe were essentially using an ensemble with a low-key hyperparameter optimization, with - most probably - a lot of space for improvement...\nWe were more interested in feature engeneering, especially for the functional connectivity data and actually, a novel, dynamic recurrent neural network-based feature transformation approach catapulted us from place ~50 to place 8, without changing anything else. See some more details below, in the notebook.\n\nSome other approaches which were included in the final model:\n- we trained a site classifier and regressed out its predictions form all fetures.\n- some fnc transformations taking advantage of the modular nature of the data:\n    - internal and external connectivities within the resting-state networks (RSNs)\n    - connectivity across RSNs\n- fnc graph-theoretical parameters (node degree, clusteredness, various centralities)\n- brain atlas based mean activations and globally computed percentiles from the spatial maps\n- hemispherical assymetry indices computed from the spatial maps (not used in final model).\n- to account for (or take use of!) potential imaging artifacts, we extracted white-matter and cerebrospinal-fluid signal from these maps, too.\n- we also applied some more traditional feature transfromations, e.g. often added the quadratic terms, as U-shaped (or inverse U-shaped) relationships are often found with age. We also often slightly winsorized the data to mitigate the effect of potential outliers.\n\n\n\n# Let's start with the notebook!"}}