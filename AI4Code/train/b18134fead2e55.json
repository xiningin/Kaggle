{"cell_type":{"76ab4b5b":"code","a7a0771d":"code","36de9c8e":"code","3cd0872d":"code","aa62038c":"code","1f8d3c16":"code","a49f01fe":"code","58ba1bd9":"code","5a1d4e4f":"code","52e11bbc":"code","df55dce0":"code","4602fea1":"code","6492e053":"markdown","ced0c7e0":"markdown","8f582cbe":"markdown","f4d1c4e9":"markdown","caaf2b01":"markdown","8a642b4b":"markdown","4a8c6938":"markdown","07c040ff":"markdown","65b79034":"markdown","e4adfb83":"markdown","d04d09d6":"markdown","a9a5a748":"markdown","e61d5ffc":"markdown","43b1b7d7":"markdown","9fb88da5":"markdown","b9bb528e":"markdown","62a912a4":"markdown","25a866ce":"markdown","40b27727":"markdown"},"source":{"76ab4b5b":"import librosa, IPython\nimport librosa.display\nimport sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline","a7a0771d":"file = '..\/input\/gtzan-dataset-music-genre-classification\/Data\/genres_original\/classical\/classical.00005.wav'\nSignal , sr = librosa.load(file , sr = 22050) # n_samples = 2.6 * 60 * 22050","36de9c8e":"plt.figure(figsize=(15,5))\nlibrosa.display.waveplot(Signal , sr = sr)\nplt.xlabel('Time')\nplt.ylabel('Amplitude')\nplt.title(\"Classical music signal\")\nplt.show()","3cd0872d":"# play sample file\nIPython.display.Audio(Signal, rate=sr)","aa62038c":"#Spectogram\nX = librosa.stft(Signal)\nXdb = librosa.amplitude_to_db(abs(X))\nplt.figure(figsize=(14, 5))\nlibrosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\nplt.title(\"Spectogram\")\nplt.colorbar()","1f8d3c16":"spectral_centroids = librosa.feature.spectral_centroid(Signal, sr=sr)[0]\n#.spectral_centroid will return an array with columns equal to the number of frames present in your sample.\n\n# Computing the time variable for visualization\nplt.figure(figsize=(15, 5))\nframes = range(len(spectral_centroids))\nt = librosa.frames_to_time(frames)\n\n# Normalising the spectral centroid for visualisation\ndef normalize(Signal, axis=0):\n    return sklearn.preprocessing.minmax_scale(Signal, axis=axis)\n\n#Plotting the Spectral Centroid along the waveform\nlibrosa.display.waveplot(Signal, sr=sr, alpha=0.4)\nplt.plot(t, normalize(spectral_centroids), color='r')","a49f01fe":"spectral_rolloff = librosa.feature.spectral_rolloff(Signal+0.01, sr=sr)[0]\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(Signal, sr=sr, alpha=0.4)\nplt.plot(t, normalize(spectral_rolloff), color='r')","58ba1bd9":"spectral_bandwidth_2 = librosa.feature.spectral_bandwidth(Signal+0.01, sr=sr)[0]\nspectral_bandwidth_3 = librosa.feature.spectral_bandwidth(Signal+0.01, sr=sr, p=3)[0]\nspectral_bandwidth_4 = librosa.feature.spectral_bandwidth(Signal+0.01, sr=sr, p=4)[0]\n\nplt.figure(figsize=(15, 6))\nlibrosa.display.waveplot(Signal, sr=sr, alpha=0.4)\nplt.plot(t, normalize(spectral_bandwidth_2), color='r')\nplt.plot(t, normalize(spectral_bandwidth_3), color='g')\nplt.plot(t, normalize(spectral_bandwidth_4), color='y')\nplt.legend(('p = 2', 'p = 3', 'p = 4'))","5a1d4e4f":"n0 = 9000\nn1 = 9100\nzero_crossings = librosa.zero_crossings(Signal[n0:n1], pad=False)\nprint(sum(zero_crossings))#16","52e11bbc":"# Zooming in\nn0 = 9000\nn1 = 9100\nplt.figure(figsize=(14, 5))\nplt.plot(Signal[n0:n1])\nplt.grid()","df55dce0":"mfccs = librosa.feature.mfcc(Signal, sr=sr)\n\n#Displaying  the MFCCs:\nplt.figure(figsize=(15, 7))\nlibrosa.display.specshow(mfccs, sr=sr, x_axis='time')","4602fea1":"hop_length = 512\nchromagram = librosa.feature.chroma_stft(Signal, sr=sr, hop_length=hop_length)\nplt.figure(figsize=(20, 5))\nlibrosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')","6492e053":"### Feature extraction from Audio signal","ced0c7e0":"Audio data analysis is about analyzing and understanding audio signals captured by digital devices, with numerous applications in the enterprise, healthcare, productivity, and smart cities.\n\n## Audio file overview\n\nThe sound excerpts are digital audio files in .wav format. Sound waves are digitized by sampling them at discrete intervals known as the sampling rate (typically 44.1kHz for CD-quality audio meaning samples are taken 44,100 times per second).\n\n\nEach sample is the amplitude of the wave at a particular time interval, where the bit depth determines how detailed the sample will be also known as the dynamic range of the signal (typically 16bit which means a sample can range from 65,536 amplitude values).","8f582cbe":"Now that we understood how we can play around with audio data and extract important features using python. In the next section section, we are going to use these features and build an Neural Network for music genre classification.","f4d1c4e9":"#### play sample file","caaf2b01":"## Reading & Understanding Audio data","8a642b4b":"### 5. Mel-Frequency Cepstral Coefficients(MFCCs)\n\n\nThe Mel frequency cepstral coefficients (**MFCCs**) of a signal are a small set of features (usually about 10\u201320) that concisely describe the overall shape of a spectral envelope. It models the characteristics of the human voice.","4a8c6938":"### 3. Spectral Bandwidth\n\nThe spectral bandwidth is defined as the width of the band of light at one-half the peak maximum (or full width at half maximum [FWHM]) and is represented by the two vertical red lines and \u03bbSB on the wavelength axis.","07c040ff":"## Audio Data Handling using Python\n\nSound is represented in the form of an audio signal having parameters such as frequency, bandwidth, decibel, etc. A typical audio signal can be expressed as a function of Amplitude and Time shown below:\n\n![3eee0b_32a52b2007cb4df692875872b5c92757_mv2.webp](attachment:3eee0b_32a52b2007cb4df692875872b5c92757_mv2.webp)","65b79034":"# How to handle audio data?\n","e4adfb83":"### 1. Spectral Centroid\n\n\nThe spectral centroid indicates at which frequency the energy of a spectrum is centered upon or in other words It indicates where the \u201d center of mass\u201d for a sound is located.","d04d09d6":"### 2. Spectral Rolloff\n\n\nIt is a measure of the shape of the signal. It represents the frequency at which high frequencies decline to 0. To obtain it, we have to calculate the fraction of bins in the power spectrum where 85% of its power is at lower frequencies.","a9a5a748":"There are devices built that help you catch these sounds and represent it in a computer-readable format. Examples of these formats are\n\nwav (Waveform Audio File) format\n\nmp3 (MPEG-1 Audio Layer 3) format\n\nWMA (Windows Media Audio) format\n\nA typical audio processing process involves the extraction of acoustics features relevant to the task at hand, followed by decision-making schemes that involve detection, classification, and knowledge fusion. Thankfully we have some useful python libraries which make this task easier.","e61d5ffc":"### 6. Chroma feature\n\n\nA chroma feature or vector is typically a 12-element feature vector indicating how much energy of each pitch class, {C, C#, D, D#, E, \u2026, B}, is present in the signal. In short, It provides a robust way to describe a similarity measure between music pieces.","43b1b7d7":"We have 7 zero crossings, now lets verify it by visualizing the signal.","9fb88da5":"### Spectrogram\n\nA spectrogram is a visual way of representing the signal strength, or \u201cloudness\u201d, of a signal over time at various frequencies present in a particular waveform. Not only can one see whether there is more or less energy at, for example, 2 Hz vs 10 Hz, but one can also see how energy levels vary over time.\n\n\nA spectrogram is usually depicted as a heat map, i.e., as an image with the intensity shown by varying the color or brightness.","b9bb528e":"### 4. Zero-Crossing Rate\n\n\nA very simple way for measuring the smoothness of a signal is to calculate the number of zero-crossing within a segment of that signal. A voice signal oscillates slowly \u2014 for example, a 100 Hz signal will cross zero 100 per second \u2014 whereas an unvoiced fricative can have 3000 zero crossings per second.\n![3eee0b_583f8545714d49fd85733de547dd323a_mv2.webp](attachment:3eee0b_583f8545714d49fd85733de547dd323a_mv2.webp)\n\nIt usually has higher values for highly percussive sounds like those in metal and rock.","62a912a4":"![3eee0b_afa2dfd450244112879452b5fed350a2_mv2.webp](attachment:3eee0b_afa2dfd450244112879452b5fed350a2_mv2.webp)","25a866ce":"Every audio signal consists of many features. However, we must extract the characteristics that are relevant to the problem we are trying to solve. The process of extracting features to use them for analysis is called feature extraction. Let us study a few of the features in detail.\n\n\nThe spectral features (frequency-based features), which are obtained by converting the time-based signal into the frequency domain using the Fourier Transform, like fundamental frequency, frequency components, spectral centroid, spectral flux, spectral density, spectral roll-off, etc.","40b27727":"Python has some great libraries for audio processing like Librosa and PyAudio.There are also built-in modules for some basic audio functionalities.\n\nWe will mainly use two libraries for audio acquisition and playback:\n\n\n### 1. Librosa\n\n\nIt is a Python module to analyze audio signals in general but geared more towards music. It includes the nuts and bolts to build a MIR(Music information retrieval) system. It has been very well documented along with a lot of examples and tutorials.\n\nInstallation:\n\n```pip install librosa```\n\nor\n\n```conda install -c conda-forge librosa```\n\n\nTo fuel more audio-decoding power, you can install **ffmpeg** which ships with many audio decoders.\n\n\n### 2. IPython.display.Audio\n\n\nIPython.display.Audio lets you play audio directly in a jupyter notebook."}}