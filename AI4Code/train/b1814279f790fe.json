{"cell_type":{"beb7599f":"code","01e8a409":"code","98fcb8af":"code","a44f8ca4":"code","2e11f4f0":"code","f1c800dc":"code","b302517a":"code","c22d8e38":"code","ee4d5e2e":"code","695cf119":"code","caded513":"code","6ace3caa":"code","10661b0a":"code","fb3c1ffb":"code","240c7380":"code","a2754fb3":"code","7a77d59e":"code","90733b50":"code","b73486ac":"code","9e0fb1bd":"code","535f7b3f":"code","38833203":"code","aecad810":"code","bb864597":"code","7bc6d4e6":"code","246c7f4a":"code","a796c147":"code","2a394e1c":"code","487f7995":"code","947fea29":"code","f227c47f":"code","190d91bc":"code","2bdf32ed":"code","05ad79d4":"code","0a15ee62":"code","bc17971b":"code","676d7933":"code","3f830127":"code","fbcf854e":"code","193dce82":"code","22c8f9a6":"code","129cd20c":"code","eb033d3e":"code","a36e86a9":"code","1a6887d0":"code","908503e2":"code","c4cc64cc":"code","3e98e086":"code","87c322c2":"code","8dfced95":"code","b76c112d":"code","2d86c305":"code","3ff959e6":"code","3c324855":"code","c5e45633":"code","8963d4d0":"code","ac9accb1":"code","f3b7ed7f":"code","2197eb08":"code","a98f3977":"code","91d9b583":"code","2aefdb1d":"code","bd8f4f0a":"code","1e0a668d":"code","39b49b2f":"code","08ab6b0c":"code","56a28432":"code","2f78048e":"code","0d1e511a":"code","8c56b6cb":"code","aa8a5605":"code","14c8004f":"code","1021835e":"code","4c4092b9":"code","20bc373e":"code","dcea1210":"code","617860b7":"code","6f0b999a":"code","3710d421":"code","c260301b":"code","4a2505f0":"code","e11943b1":"code","d4f2226f":"code","8ca73ffd":"code","1df95ba4":"code","1ee9ce03":"code","8b92b210":"code","56fa4a34":"code","5ab19dcb":"code","e52b05e8":"code","ab7dfbfa":"code","1908c9c1":"code","3ddd4f6f":"code","81d99165":"code","da964373":"code","f179caf0":"code","5c91c669":"code","d6c05f66":"code","27024eba":"code","41939922":"code","7a9a6fd5":"code","4a7e93d9":"code","da8626dd":"code","e7f7c6df":"code","b21d8e04":"code","14dbb77c":"code","877094e9":"code","189da0c7":"code","450d0635":"code","0be62832":"code","94f71738":"code","ead25f50":"code","ba2a2610":"code","0d8e939e":"code","288ad3a8":"code","89144084":"code","584ac3b1":"code","f91bcbf4":"code","82ee4a8e":"code","702eb73e":"code","69e7f325":"code","08cd7f77":"code","cea4554d":"code","5d018ef6":"code","5661fa8a":"code","450f6beb":"code","bedfc29c":"code","ad1111b4":"code","d2506965":"code","b07813dc":"code","4a254ba0":"code","5104cbbb":"code","54c20b74":"code","3f01546d":"code","75aeda69":"code","d05a0269":"code","d476fe90":"code","62164634":"code","30859cf9":"code","56139aad":"code","94fcb8d8":"code","b514b09b":"code","af7d578c":"code","81fccbd8":"code","12b486fe":"code","adcd5fa8":"code","346a813a":"code","f51e8f74":"code","1f76dc46":"code","92dbc405":"code","75ef92e7":"code","688d4b0f":"code","7d2d647a":"code","3c85b1b1":"code","f15a9ccc":"code","465647f1":"code","1133679b":"code","9879c2e2":"code","8d903521":"code","08a898ad":"code","e8ee75e9":"code","e331dd4e":"code","b74429bf":"code","65cae67d":"code","316fb057":"code","b9f777b3":"code","ea36de4c":"code","563c53c6":"code","98794dc5":"code","9fc56449":"code","1ec0cab9":"code","9259cadb":"code","f839d68e":"markdown","d7d9a7f8":"markdown","f56f292c":"markdown","750eaac4":"markdown","05e2f3a6":"markdown","11193af1":"markdown","9b4f4c3b":"markdown","fad3b357":"markdown","439072da":"markdown","a5762cf8":"markdown","d6cb944b":"markdown","c2aba79c":"markdown","2c915a15":"markdown","7e078e2a":"markdown","11527ee1":"markdown","53340271":"markdown","a0ceaa8e":"markdown","a7abfada":"markdown","2791b624":"markdown","1af630a5":"markdown","2b33b671":"markdown","39d1ae81":"markdown","b3acc6c9":"markdown","079d97e3":"markdown","8c599915":"markdown","1f44f63c":"markdown","7c89e6c8":"markdown","5c973fd8":"markdown","d2ee3843":"markdown","bd33c059":"markdown","db5ce4b3":"markdown","dd171742":"markdown","62fc2268":"markdown","e95f63cb":"markdown","abf52947":"markdown","c07135ee":"markdown","b1970a56":"markdown","e0c81cba":"markdown","4c292d8a":"markdown","a567d247":"markdown","14106106":"markdown","16f72837":"markdown","27bd4459":"markdown","a6327784":"markdown","d556e91b":"markdown","5fdb9d54":"markdown","1784e0be":"markdown","bf849ac8":"markdown","98a51cb7":"markdown","b73ae810":"markdown","4b781a04":"markdown","5179e947":"markdown","5ed1cc3a":"markdown","9022a05b":"markdown","db2b98cb":"markdown","29e3c863":"markdown","c73cceaf":"markdown","b87c0361":"markdown","69d33ea4":"markdown","6f383f85":"markdown","652572af":"markdown","9ee196b2":"markdown","db1a61f8":"markdown","db96cc34":"markdown","967173ca":"markdown","9e7a998c":"markdown","a1dc281e":"markdown","0517ae7e":"markdown","114e544f":"markdown","cef96310":"markdown","6dbe2b20":"markdown","f0f16939":"markdown","2d9a54b0":"markdown","a9cab48f":"markdown","1d3669aa":"markdown","d6204225":"markdown","a512b315":"markdown","a775a735":"markdown","84faa2f9":"markdown"},"source":{"beb7599f":"'''!pip install jovian --upgrade --quiet\n!pip install opendatasets --q'''","01e8a409":"#import jovian","98fcb8af":"# Execute this to save new versions of the notebook\n#jovian.commit(project=\"automobile-insurance\")","a44f8ca4":"# Exploratory Data Analysis\n\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsns.set_style('darkgrid')\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (9, 5)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'","2e11f4f0":"# Xgboost\n\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom xgboost import plot_tree\nfrom matplotlib.pylab import rcParams\n%matplotlib inline","f1c800dc":"# PyTorch computations\n\nimport os\nimport torch\n#import jovian\nimport torchvision\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n#import opendatasets as od\nfrom torch.utils.data import random_split, TensorDataset, DataLoader","b302517a":"'''dataset_url = ('https:\/\/www.kaggle.com\/aashishjhamtani\/automobile-insurance')\nod.download(dataset_url)'''","c22d8e38":"DATA_DIR = '..\/input\/automobile-insurance'\nos.listdir(DATA_DIR)","ee4d5e2e":"insurance_claims = pd.read_csv(DATA_DIR + '\/insurance_claims.csv')\ninsurance_claims.head()","695cf119":"insurance_claims.info()","caded513":"insurance_claims._c39","6ace3caa":"def split_date(df):\n  df['policy_bind_date'] = pd.to_datetime(df['policy_bind_date'])\n  df['policy_bind_year'] = df.policy_bind_date.dt.year\n  df['policy_bind_month'] = df.policy_bind_date.dt.month\n  df['policy_bind_day'] = df.policy_bind_date.dt.day\n  df['policy_bind_week_of_year'] = df.policy_bind_date.dt.isocalendar().week\n  df['incident_date'] = pd.to_datetime(df['incident_date'])\n  df['incident_year'] = df.incident_date.dt.year\n  df['incident_month'] = df.incident_date.dt.month\n  df['incident_day'] = df.incident_date.dt.day\n  df['incident_week_of_year'] = df.incident_date.dt.isocalendar().week\n","10661b0a":"split_date(insurance_claims)","fb3c1ffb":"insurance_claims.head()","240c7380":"def incident_time(df):\n  df['incident_time'] = 12 * (df.incident_year - df.policy_bind_year) + (df.incident_month - df.policy_bind_month)\n  df['incident_time'] = df['incident_time'].map(lambda x: 0 if x < 0 else x).fillna(0)","a2754fb3":"incident_time(insurance_claims)","7a77d59e":"insurance_claims.head()","90733b50":"insurance_claims[['policy_bind_date', 'incident_date', 'incident_time']]","b73486ac":"insurance_claims.shape","9e0fb1bd":"insurance_claims.info()","535f7b3f":"insurance_claims.describe()","38833203":"insurance_claims.columns","aecad810":"insurance_claims.drop('_c39', axis = 1, inplace = True)","bb864597":"insurance_claims.shape","7bc6d4e6":"## Check that there are no more missing values in the dataset\ninsurance_claims.isnull().sum().sum()","246c7f4a":"insurance_claims.incident_type.value_counts()","a796c147":"claims_df = insurance_claims[['total_claim_amount', 'injury_claim', 'property_claim', 'vehicle_claim']]\nclaims_df","2a394e1c":"claims_sum = claims_df.sum()\nclaims_sum","487f7995":"claims_sum[0] == sum(claims_sum[1:])","947fea29":"plt.figure(figsize = (12,7))\nplt.title('Distribution of Claims by Type')\nplt.pie(claims_sum[1:], labels = claims_sum[1:].index, autopct = '%1.1f%%', startangle=0)\nplt.legend(loc = 'upper left', fontsize = 'x-small');","f227c47f":"No_of_vehicles_involved = insurance_claims.number_of_vehicles_involved.value_counts()\nNo_of_vehicles_involved","190d91bc":"plt.figure(figsize=(12,6))\nplt.title('Number of Cars involved in an accident')\nsns.barplot(x=No_of_vehicles_involved.index, y=No_of_vehicles_involved);","2bdf32ed":"insurance_claims.groupby('auto_model')[['total_claim_amount']].mean().sort_values('total_claim_amount', ascending = False).rename(columns = {'total_claim_amount' : 'mean_claim_amount'})","05ad79d4":"plt.figure(figsize=(16, 8))\n\nmy_order = insurance_claims.groupby(by=[\"auto_model\"])[\"total_claim_amount\"].median().iloc[::-1].index\n\nsns.boxplot(x = 'auto_model', y = 'total_claim_amount' ,data = insurance_claims, order = my_order).set(title = 'Box Plot of Auto Model vs Total Claim Amount')\nplt.xticks(rotation = 90);","0a15ee62":"State_claims = insurance_claims.groupby('incident_state')[['total_claim_amount']].sum().sort_values('total_claim_amount', ascending = False)\nState_claims","bc17971b":"State_claims_pct = State_claims.total_claim_amount * 100\/ State_claims.total_claim_amount.sum()\n\nsns.barplot(x=State_claims_pct, y=State_claims_pct.index)\n\nplt.title('State Distribution of Claims Severity')\nplt.ylabel(None);\nplt.xlabel('Percentage');","676d7933":"insurance_claims.incident_year.value_counts()","3f830127":"insurance_claims.groupby('incident_month')[['total_claim_amount']].sum().sort_values('total_claim_amount', ascending = False)","fbcf854e":"monthly_model_claims = insurance_claims.groupby(['incident_month', 'auto_model'])[['total_claim_amount']].sum().sort_values('total_claim_amount', ascending = False)\nmonthly_model_claims","193dce82":"import calendar\n\n#Use calendar library for abbreviations and order\ndd=dict((enumerate(calendar.month_abbr)))\n\n#rename level zero of multiindex\nmonthly_model_claims = monthly_model_claims.rename(index=dd,level=0)\nmonthly_model_claims","22c8f9a6":"sns.heatmap(monthly_model_claims, fmt ='d', cmap='Blues')","129cd20c":"sns.scatterplot(x = 'policy_annual_premium', y = 'total_claim_amount', data = insurance_claims)","eb033d3e":"#jovian.commit()\n","a36e86a9":"insurance_claims.columns","1a6887d0":"insurance_claims.shape","908503e2":"input_cols = ['months_as_customer', 'age', 'policy_number', 'policy_bind_date',\n       'policy_state', 'policy_csl', 'policy_deductable',\n       'policy_annual_premium', 'umbrella_limit', 'insured_zip', 'insured_sex',\n       'insured_education_level', 'insured_occupation', 'insured_hobbies',\n       'insured_relationship', 'capital-gains', 'capital-loss',\n       'incident_date', 'incident_type', 'collision_type', 'incident_severity',\n       'authorities_contacted', 'incident_state', 'incident_city',\n       'incident_location', 'incident_hour_of_the_day',\n       'number_of_vehicles_involved', 'property_damage', 'bodily_injuries',\n       'witnesses', 'police_report_available', 'auto_make',\n       'auto_model', 'auto_year', 'fraud_reported', 'policy_bind_year',\n       'policy_bind_month', 'policy_bind_day', 'policy_bind_week_of_year',\n       'incident_year', 'incident_month', 'incident_day',\n       'incident_week_of_year', 'incident_time']\n       \n       \ntarget_col = ['total_claim_amount']","c4cc64cc":"inputs = insurance_claims[input_cols].copy()\ntargets = insurance_claims[target_col].copy()","3e98e086":"inputs.info()","87c322c2":"inputs.shape, targets.shape","8dfced95":"numeric_cols = inputs.select_dtypes(include=np.number).columns.tolist()\nlen(numeric_cols)","b76c112d":"categorical_cols = (inputs.select_dtypes(include=['object', 'datetime64[ns]']).columns.tolist())\nlen(categorical_cols)","2d86c305":"len(input_cols) == len(numeric_cols) + len(categorical_cols)","3ff959e6":"scaler = MinMaxScaler().fit(inputs[numeric_cols])\ninputs[numeric_cols] = scaler.transform(inputs[numeric_cols])","3c324855":"encoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(inputs[categorical_cols])\nencoded_cols = list(encoder.get_feature_names(categorical_cols))\ninputs[encoded_cols] = encoder.transform(inputs[categorical_cols])","c5e45633":"inputs.shape","8963d4d0":"inputs.describe()","ac9accb1":"#Create training group (with train and validation sets) and test sets\ntrain_inputs, test_inputs, train_targets_g, test_targets = train_test_split(\n    inputs[numeric_cols + encoded_cols], targets, test_size=0.1, random_state=42)","f3b7ed7f":"len(train_inputs), len(test_inputs), len( inputs[numeric_cols + encoded_cols])","2197eb08":"len(train_targets_g), len(targets),len(test_targets)","a98f3977":"model = XGBRegressor(random_state=42, n_jobs=-1, n_estimators=20, max_depth=4)","91d9b583":"%%time\nmodel.fit(train_inputs, train_targets_g)","2aefdb1d":"preds = model.predict(train_inputs)\npreds","bd8f4f0a":"preds.shape","1e0a668d":"def rmse(a,b):\n  return mean_squared_error(a, b, squared=False)","39b49b2f":"rmse(preds, train_targets_g)","08ab6b0c":"rcParams['figure.figsize'] = 30, 30","56a28432":"import os\nos.environ[\"PATH\"] += os.pathsep + 'C:\/Users\/HP\/anaconda3\/Library\/bin\/graphviz'","2f78048e":"plot_tree(model, rankdir='LR');","0d1e511a":"plot_tree(model, rankdir='LR', num_trees=1);","8c56b6cb":"trees = model.get_booster().get_dump()","aa8a5605":"len(trees)","14c8004f":"print(trees[0])","1021835e":"len(train_inputs.columns)","4c4092b9":"len(model.feature_importances_)","20bc373e":"importance_df = pd.DataFrame({\n    'feature': train_inputs.columns,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)","dcea1210":"importance_df.head(10)","617860b7":"plt.figure(figsize = (10, 6))\nplt.title('Feature Importances')\nsns.barplot(data = importance_df.head(10), x = 'importance', y = 'feature')","6f0b999a":"insurance_claims.collision_type.value_counts()","3710d421":"#jovian.commit()","c260301b":"def train_and_evaluate(X_train, train_targets, X_val, val_targets, **params):\n    model = XGBRegressor(random_state=42, n_jobs=-1, **params)\n    model.fit(X_train, train_targets)\n    train_rmse = rmse(model.predict(X_train), train_targets)\n    val_rmse = rmse(model.predict(X_val), val_targets)\n    return model, train_rmse, val_rmse","4a2505f0":"kfold = KFold(n_splits=5)","e11943b1":"models = []\n\nfor train_idxs, val_idxs in kfold.split(train_inputs):\n    X_train, train_targets = train_inputs.iloc[train_idxs], train_targets_g.iloc[train_idxs]\n    X_val, val_targets = train_inputs.iloc[val_idxs], train_targets_g.iloc[val_idxs]\n    model, train_rmse, val_rmse = train_and_evaluate(X_train, \n                                                     train_targets, \n                                                     X_val, \n                                                     val_targets, \n                                                     max_depth=4, \n                                                     n_estimators=20)\n    models.append(model)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, val_rmse))","d4f2226f":"import numpy as np\n\ndef predict_avg(models, inputs):\n    return np.mean([model.predict(inputs) for model in models], axis=0)","8ca73ffd":"preds = predict_avg(models, train_inputs)\npreds","1df95ba4":"def test_params_kfold(n_splits, **params):\n    train_rmses, val_rmses, models = [], [], []\n    kfold = KFold(n_splits)\n    for train_idxs, val_idxs in kfold.split(train_inputs):\n        X_train, train_targets = train_inputs.iloc[train_idxs], train_targets_g.iloc[train_idxs]\n        X_val, val_targets = train_inputs.iloc[val_idxs], train_targets_g.iloc[val_idxs]\n        model, train_rmse, val_rmse = train_and_evaluate(X_train, train_targets, X_val, val_targets, **params)\n        models.append(model)\n        train_rmses.append(train_rmse)\n        val_rmses.append(val_rmse)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(np.mean(train_rmses), np.mean(val_rmses)))\n    return models","1ee9ce03":"X_train, X_val, train_targets, val_targets = train_test_split(train_inputs, train_targets_g, test_size=0.1)","8b92b210":"def test_params(**params):\n    model = XGBRegressor(n_jobs=-1, random_state=42, **params)\n    model.fit(X_train, train_targets)\n    train_rmse = rmse(model.predict(X_train), train_targets)\n    val_rmse = rmse(model.predict(X_val), val_targets)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, val_rmse))","56fa4a34":"test_params(n_estimators=10)","5ab19dcb":"test_params(n_estimators=30)","e52b05e8":"test_params(n_estimators=100)","ab7dfbfa":"test_params(max_depth=2)","1908c9c1":"test_params(max_depth=5)","3ddd4f6f":"test_params(n_estimators=50, learning_rate=0.01)","81d99165":"test_params(n_estimators=50, learning_rate=0.1)","da964373":"test_params(n_estimators=50, learning_rate=0.3)","f179caf0":"test_params(booster='gblinear')","5c91c669":"#jovian.commit()","d6c05f66":"model = XGBRegressor(n_jobs=-1, random_state=42, n_estimators=1000, \n                     learning_rate=0.2, max_depth=10, subsample=0.9, \n                     colsample_bytree=0.7)","27024eba":"%%time\nmodel.fit(train_inputs, train_targets_g)","41939922":"test_preds = model.predict(test_inputs)","7a9a6fd5":"rmse(test_preds, test_targets)","4a7e93d9":"insurance_claims.describe()","da8626dd":"#jovian.commit()","e7f7c6df":"insurance_claims","b21d8e04":"insurance_claims.info()","14dbb77c":"num_rows = len(insurance_claims)\nnum_rows","877094e9":"num_cols = len(insurance_claims.columns)\nnum_cols","189da0c7":"input_cols = ['months_as_customer', 'age', 'policy_number', 'policy_bind_date',\n       'policy_state', 'policy_csl', 'policy_deductable',\n       'policy_annual_premium', 'umbrella_limit', 'insured_zip', 'insured_sex',\n       'insured_education_level', 'insured_occupation', 'insured_hobbies',\n       'insured_relationship', 'capital-gains', 'capital-loss',\n       'incident_date', 'incident_type', 'collision_type', 'incident_severity',\n       'authorities_contacted', 'incident_state', 'incident_city',\n       'incident_location', 'incident_hour_of_the_day',\n       'number_of_vehicles_involved', 'property_damage', 'bodily_injuries',\n       'witnesses', 'police_report_available', 'auto_make',\n       'auto_model', 'auto_year', 'fraud_reported', 'policy_bind_year',\n       'policy_bind_month', 'policy_bind_day', 'policy_bind_week_of_year',\n       'incident_year', 'incident_month', 'incident_day',\n       'incident_week_of_year', 'incident_time']\n       \n       \noutput_cols = ['total_claim_amount']","450d0635":"ins_inputs = insurance_claims[input_cols].copy()\nins_outputs = insurance_claims[output_cols].copy()\nins_inputs.shape, ins_outputs.shape","0be62832":"categorical_cols = (ins_inputs.select_dtypes(include=['object', 'datetime64[ns]']).columns.tolist())\nlen(categorical_cols)","94f71738":"rcParams['figure.figsize'] = 12, 6\nminim = insurance_claims.total_claim_amount.min()\nmaxim = insurance_claims.total_claim_amount.max()\navg = insurance_claims.total_claim_amount.mean()\n\nprint('Minimum Claims {:.4f}, Maximum Claims {:.4f}, Average Claims {:.4f}'.format(minim, maxim,avg))\n\n# Distribution\nsns.distplot(insurance_claims.total_claim_amount,kde = True)\nplt.title('Distribution of Total Claims');","ead25f50":"#jovian.commit()","ba2a2610":"def insurance_claims_to_arrays(insurance_claims):\n  # Make a copy of the original dataframe\n  insurance_claims1 = insurance_claims.copy(deep = True)\n  # Convert non-numeric categorical columns into numbers\n  for col in categorical_cols:\n    insurance_claims1[col] = insurance_claims1[col].astype('category').cat.codes\n  # Extract inputs and outputs as arrays\n  inputs_array = insurance_claims1[input_cols].to_numpy().astype('float32')\n  targets_array = insurance_claims1[output_cols].to_numpy().astype('float32')\n  return inputs_array,targets_array\n","0d8e939e":"inputs_array, targets_array = insurance_claims_to_arrays(insurance_claims)\ninputs_array.shape, targets_array.shape","288ad3a8":"inputs_array","89144084":"inputs_t = torch.from_numpy(inputs_array).type(torch.float32)\ntargets_t = torch.from_numpy(targets_array).type(torch.float32)\ninputs_t.dtype, targets_t.dtype","584ac3b1":"dataset = TensorDataset(inputs_t, targets_t)","f91bcbf4":"# Split test from training group i.e. validation + test\ntest_percent = 0.125 # between 0.1 and 0.2\ntest_size = int(num_rows * test_percent)\ntrain_g_size = num_rows - test_size\n\ntrain_g_ds, test_ds = random_split(dataset, [train_g_size, test_size]) # Use the random_split function to split dataset into 2 parts of the desired length","82ee4a8e":"train_g_size","702eb73e":"# Split validation and test sets from training group\n\nval_percent = 0.1 # between 0.1 and 0.2\nval_size = int(train_g_size * val_percent)\ntrain_size = train_g_size - val_size\n\ntrain_ds, val_ds = random_split(train_g_ds, [train_size, val_size]) # Use the random_split function to split dataset into 2 parts of the desired length","69e7f325":"train_size, val_size","08cd7f77":"batch_size = 32","cea4554d":"train_loader = DataLoader(train_ds, batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size)\ntest_loader = DataLoader(test_ds, batch_size)","5d018ef6":"for xb, yb in train_loader:\n  print('inputs:', xb)\n  print('targets:', yb)\n  break","5661fa8a":"#jovian.commit()","450f6beb":"input_size = len(input_cols)\noutput_size = len(output_cols)","bedfc29c":"class InsuranceModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(input_size, output_size)                 \n        \n    def forward(self, xb):\n        out = self.linear(xb)                      \n        return out\n    \n    def training_step(self, batch):\n        inputs, targets = batch \n        # Generate predictions\n        out = self(inputs)          \n        # Calcuate loss\n        loss = F.l1_loss(out, targets)                          \n        return loss\n    \n    def validation_step(self, batch):\n        inputs, targets = batch\n        # Generate predictions\n        out = self(inputs)\n        # Calculate loss\n        loss = F.l1_loss(out, targets)                              \n        return {'val_loss': loss.detach()}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()  \n        return {'val_loss': epoch_loss.item()}\n    \n    def epoch_end(self, epoch, result, num_epochs):\n        # Print result every 20th epoch\n        if (epoch+1) % 20 == 0 or epoch == num_epochs-1:\n            print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch+1, result['val_loss']))","ad1111b4":"model = InsuranceModel()","d2506965":"list(model.parameters())","b07813dc":"#jovian.commit()","4a254ba0":"def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result, epochs)\n        history.append(result)\n    return history","5104cbbb":"result = evaluate(model, val_loader) \nprint(result)","54c20b74":"epochs = 180\nlr = 1e-7\nhistory1 = fit(epochs, lr, model, train_loader, val_loader)","3f01546d":"epochs = 200\nlr = 1e-8\nhistory2 = fit(epochs, lr, model, train_loader, val_loader)","75aeda69":"epochs = 200\nlr = 1e-9\nhistory3 = fit(epochs, lr, model, train_loader, val_loader)","d05a0269":"losses = [r['val_loss'] for r in [result] + history1 + history2 + history3]\nplt.plot(losses, '-x')\nplt.xlabel('epoch')\nplt.ylabel('val_loss')\nplt.title('val_loss vs. epochs');","d476fe90":"val_loss = 20203.4688","62164634":"#jovian.log_metrics(val_loss = val_loss)","30859cf9":"#jovian.commit()","56139aad":"def predict_single(input, target, model):\n    inputs = input.unsqueeze(0)\n    predictions = model(inputs)              \n    prediction = predictions[0].detach()\n    print(\"Input:\", input)\n    print(\"Target:\", target)\n    print(\"Prediction:\", prediction)","94fcb8d8":"input, target = val_ds[0]\npredict_single(input, target, model)","b514b09b":"input, target = val_ds[10]\npredict_single(input, target, model)","af7d578c":"result_test = evaluate(model, test_loader)\nresult_test","81fccbd8":"#jovian.commit()","12b486fe":"hidden_size = 128","adcd5fa8":"class InsuranceModelF(nn.Module):\n    def __init__(self,input_size, hidden_size, output_size):\n        super().__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size) \n        self.linear2 = nn.Linear(hidden_size, output_size)  \n\n        \n    def forward(self, xb):\n        out = self.linear1(xb) \n        out = F.relu(out)\n        out = self.linear2(out)                   \n        return out\n    \n    def training_step(self, batch):\n        inputs, targets = batch \n        # Generate predictions\n        out = self(inputs)          \n        # Calculate loss\n        loss = F.l1_loss(out, targets)                          \n        return loss\n    \n    def validation_step(self, batch):\n        inputs, targets = batch\n        # Generate predictions\n        out = self(inputs)\n        # Calculate loss\n        loss = F.l1_loss(out, targets)                            \n        return {'val_loss': loss.detach()}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        return {'val_loss': epoch_loss.item()}\n    \n    def epoch_end(self, epoch, result):\n      if (epoch+1) % 20 == 0:\n        print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch + 1, result['val_loss']))","346a813a":"model = InsuranceModelF(input_size = input_size, hidden_size = hidden_size, output_size = output_size)\nmodel","f51e8f74":"for inputs, targets in train_loader:\n  out = model(inputs)\n  loss = F.l1_loss(out, targets)                          \n  print(loss)\n  break","1f76dc46":"for inputs, targets in train_loader:\n    print('inputs.shape:', inputs.shape)\n    break\n\ninput_size = inputs.shape[-1]\ninput_size","92dbc405":"for t in model.parameters():\n    print(t.shape)","75ef92e7":"torch.cuda.is_available()","688d4b0f":"def get_default_device():\n    if torch.cuda.is_available:\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","7d2d647a":"device = get_default_device()\ndevice","3c85b1b1":"def to_device(data, device):\n    if isinstance(data, (list, tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking = True)","f15a9ccc":"for inputs, targets in train_loader:\n    print(inputs.shape)\n    inputs = to_device(inputs, device)\n    print(inputs.device)\n    break","465647f1":"class DeviceDataLoader():\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        for b in self.dl:\n            yield to_device(b, self.device)\n            \n    def __len__(self):\n        return len(self.dl)","1133679b":"train_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)","9879c2e2":"for xb, yb in val_loader:\n    print('xb.device:', xb.device)\n    print('yb:', yb)\n    break","8d903521":"def evaluate(model, val_loader):\n    \"\"\"Evaluate the model's performance on the validation set\"\"\"\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    \"\"\"Train the model using gradient descent\"\"\"\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","08a898ad":"model = InsuranceModelF(input_size = input_size, hidden_size=hidden_size, output_size = output_size)\nto_device(model, device)","e8ee75e9":"history = [evaluate(model, val_loader)]\nhistory","e331dd4e":"history += fit(200, 1e-11, model, train_loader, val_loader)","b74429bf":"history += fit(200, 1e-11, model, train_loader, val_loader)","65cae67d":"history += fit(200, 1e-10, model, train_loader, val_loader)","316fb057":"history += fit(200, 1e-10, model, train_loader, val_loader)","b9f777b3":"history += fit(400, 1e-10, model, train_loader, val_loader)","ea36de4c":"losses = [x['val_loss'] for x in history]\nplt.plot(losses, '-x')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.title('Loss vs No. of Epochs');","563c53c6":"Feed_forward_val_loss = 20573.5781","98794dc5":"#jovian.log_metrics(Feed_forward_val_loss = Feed_forward_val_loss)","9fc56449":"#jovian.log_metrics","1ec0cab9":"#jovian.log_dataset(dataset_url= dataset_url)","9259cadb":"#jovian.commit()","f839d68e":"**Q3: What are the column titles of the input variables?**","d7d9a7f8":"### Claims\n\nLet's look at the distribution of claims by type i.e. injury, vehicle and property","f56f292c":"**Q2: How many columns does the dataset have**","750eaac4":"# Feed Forward Neural Networks on a GPU with PyTorch","05e2f3a6":"**Q9: Complete the class definition below by filling out the constructor (`__init__`), `forward`, `training_step` and `validation_step` methods.**","11193af1":"Most accidents involve 1 car followed by 3. Accidents involving 2 or 4 cars seem to be a rare occurence","9b4f4c3b":"**Q11: Train the model 4-5 times with different learning rates & for different number of epochs.**\n\nHint: Vary learning rates by orders of 10 (e.g. `1e-2`, `1e-3`, `1e-4`, `1e-5`, `1e-6`) to figure out what works.","fad3b357":"Let's save our work before continuing.","439072da":"### Scale numeric and categorical columns","a5762cf8":"### Visualization\n\nWe can visualize individual trees using `plot_tree` (note: this requires the `graphviz` library to be installed).","d6cb944b":"One final commit before we train the model.","c2aba79c":"Let's check out the weights and biases of the model using `model.parameters`.","2c915a15":"Let's view some basic information about the resultant data frame.","7e078e2a":"The claims range between `$100` and `~$115K` with the mean at `~$53K` and a bimodal dataset with most claims at ~`$5k` and others coalescing at `$60k` ","11527ee1":"Let us answer some basic questions about the dataset. \n\n\n**Q1: How many rows does the dataset have?**","53340271":"### One Hot Encode Categorical columns","a0ceaa8e":"New York is leading with the most claims at 28% i.e. $14.7M. Let's save and upload our work before continuing.","a7abfada":"\nWe are now ready to train the model. You may need to run the training loop many times, for different number of epochs and with different learning rates, to get a good result. Also, if your loss becomes too large (or `nan`), you may have to re-initialize the model by running the cell `model = InsuranceModel()`. Experiment with this for a while, and try to get to as low a loss as possible.","2791b624":"Let's first look at the distribution of cars in our dataset. Let's look at the average claim amount by car type","1af630a5":"### Date\n\nFirst, let's convert `Policy Bind Date` and `Incident Date` to a `datecolumn` and extract different parts of the date.","2b33b671":"## Step 4: Train the model to fit the data\n\nTo train our model, we'll use the `fit` function built as a generic training loop. That's the benefit of defining a generic training loop - you can use it for any problem.","39d1ae81":"## Hyperparameter Tuning and Regularization\n\nJust like other machine learning models, there are several hyperparameters we can to adjust the capacity of model and reduce overfitting.\n\n<img src=\"https:\/\/i.imgur.com\/EJCrSZw.png\" width=\"480\">\n\nCheck out the following resources to learn more about hyperparameter supported by XGBoost:\n\n- https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#xgboost.XGBRegressor\n- https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html","b3acc6c9":"**Q10: Use the `evaluate` function to calculate the loss on the validation set before training.**","079d97e3":"**Q6: Convert the numpy arrays `inputs_array` and `targets_array` into PyTorch tensors. Make sure that the data type is `torch.float32`.**","8c599915":"### Which State has the most severe value of claim incidences?\n\nTo answer this question, we'll need to use the groupby data frame method to aggregate the rows for each state.","1f44f63c":"We can identify the number of vehicles involved in an accident by using `value_counts` method","7c89e6c8":"### Evaluation\n\nLet's evaluate the predictions using RMSE error.","5c973fd8":"## Putting it Together and Making Predictions\n\nLet's train a final model on the entire training set with custom hyperparameters. ","d2ee3843":"### Input and Target Columns\n\nLet's select the columns that we'll use for training.","bd33c059":"### Premiums\n\n","db5ce4b3":"## Gradient Boosting\n\nWe're now ready to train our gradient boosting machine (GBM) model. \n\n### Training\n\nTo train a GBM, we can use the `XGBRegressor` class from the [`XGBoost`](https:\/\/xgboost.readthedocs.io\/en\/latest\/) library.","dd171742":"## Preparing the Data for Training\n\nWe'll perform the following steps to prepare the dataset for training:\n\n1. Identify input and target columns\n2. Identify numeric and categorical columns\n3. Scale numeric values to the $(0, 1)$ range\n4. Encode categorical columns to one-hot vectors\n5. Create a train\/test\/validation split","62fc2268":"#### `max_depth`\n\nAs you increase the max depth of each tree, the capacity of the tree increases and it can capture more information about the training set.","e95f63cb":"### Heat Map\n\nA heatmap is used to visualize 2-dimensional data like a matrix or a table using colors. The best way to understand it is by looking at an example. ","abf52947":"Identify numeric and categorical columns","c07135ee":"Let us create a model using the `InsuranceModel` class. You may need to come back later and re-run the next cell to reinitialize the model, in case the loss becomes `nan` or `infinity`.","b1970a56":"#### `n_estimators`\n\nThe number of trees to be created. More trees = greater capacity of the model.","e0c81cba":"The dataset has been downloaded. Let's examine the contents of the csv file","4c292d8a":"Let's also define a function to average predictions from the 5 different models.","a567d247":"Let's look at a batch of data to verify everything is working fine so far.","14106106":"### Prediction\n\nWe can now make predictions and evaluate the model using `model.predict`.","16f72837":"From the dataset we are given, vehicle claims take up 72% of the payouts made by this insurance company with property and injury sharing 14% each. Lets look at the vehicle claims further.\n\nLet's look at\n1. The spread of the number of vehicles involved in an accident\n2. The relationship between the car model and the total claims\n3. The states with the highest incidence rate\n","27bd4459":"We can now use `predict_avg` to make predictions for the test set.","a6327784":"Now, we can use the `KFold` utility to create the different training\/validations splits and train a separate model for each fold.","d556e91b":"Next, we need to create PyTorch datasets & data loaders for training & validation. We'll start by creating a `TensorDataset`.","5fdb9d54":"Lets download and import the libraries we will need","1784e0be":"Let's train the model using `model.fit`.","bf849ac8":"## Exploratory Analysis and Visualization\n\nBefore we split the data and start training, it would help us to understand the nature of claims, the insureds' demographics, i.e., age, gender, type of claims, fraud incidences, etc. ","98a51cb7":"### Feature importance\n\nJust like decision trees and random forests, XGBoost also provides a feature importance score for each column in the input.","b73ae810":"### Create training test and validation sets","4b781a04":"Looking at the nature of incidents, most are collisions i.e. Mutliple vehicles followed closely by Single vehicles","5179e947":"The RMSE at `$16.5K` is quite high for a total claims mean of `$52.7K` and a standard deviation of `$26.4K`. Let's examine further predictions via PyTorch to yield greater accuracy","5ed1cc3a":"Let's define a helper function `train_and_evaluate` which trains a model the given parameters and returns the trained model, training error and validation error.","9022a05b":"** Use the `evaluate` function to calculate the loss on the test set.**","db2b98cb":"**Q12: What is the final validation loss of your model?**","29e3c863":"Let's view the results of the new columns we've created.","c73cceaf":"Finally, we can create data loaders for training & validation.\n\n**Q8: Pick a batch size for the data loader.**","b87c0361":"In this notebook, we will use \n\n1. Xgboost\n2. PyTorch \n\nto predict insurance claims for automobile Insurance and compare the results. \n\nWe have one dataset containing 2015 claims that we will split into train, test and validation datasets for training, hyperparameter tuning and testing the accuracy of the models\n\n![](https:\/\/www.oakentrust.com\/wp-content\/uploads\/2014\/07\/insurance.jpg)\n\nWe will use the [Automobile Insurance](https:\/\/www.kaggle.com\/aashishjhamtani\/automobile-insurance) dataset from Kaggle\n\nLet's save and upload the notebook details to Jovian","69d33ea4":"#### `learning_rate`\n\nThe scaling factor to be applied to the prediction of each tree. A very high learning rate (close to 1) will lead to overfitting, and a low learning rate (close to 0) will lead to underfitting.","6f383f85":"We can visualize this information using a bar chart","652572af":"Here's a helper function to test hyperparameters with K-fold cross validation.","9ee196b2":"#### `booster`\n\nInstead of using Decision Trees, XGBoost can also train a linear model for each iteration. This can be configured using `booster`.","db1a61f8":"## Step 3: Create a Linear Regression Model\n\nOur initial model is a fairly straightforward linear regression (we'll build more complex models hereafter). ","db96cc34":"### Q. Is there a relationship between the car model and the severity of claims on the Insurer?\n\n","967173ca":"Let's drop the `_cd` variable using the `drop` method","9e7a998c":"The data contains 1000 inputs to 39 variables all of which have zero missing values. Column 40 is empty. ","a1dc281e":"## Step 1: Download and explore the data\n\nLet us begin by downloading the data. We'll import the files directly from Kaggle\n\n~use the `opendatasets` function to get the data as a CSV (comma-separated values) file from Kaggle.~\n","0517ae7e":"## Step 5: Make predictions using the trained model\n\n**Q13: Complete the following function definition to make predictions on a single input**","114e544f":"## Incident time\n\nWe can use the newly created features to compute the number of months since the policy came into force and the risk\/incident occuring","cef96310":"As expected, collision types are the most significant contribution to the claim amount","6dbe2b20":"## Step 2: Prepare the dataset for training\n\nWe need to convert the data from the Pandas dataframe into a PyTorch tensors for training. To do this, the first step is to convert it numpy arrays. ","f0f16939":"Split the data into traning, test and validation sets","2d9a54b0":"We'll use another sample dataset from `insurance_claims`, to visualize monthly claim trends at our insurance company over the three months in 2015 given\n","a9cab48f":"# Automobile Insurance\n","1d3669aa":"## Preprocessing and Feature Engineering\n\nLet's take a look at the available columns, and figure out if we can create new columns or apply any useful transformations.","d6204225":"# Insurance Claim Prediction by PyTorch\n\n## Step 1: Explore the data\n\nLet's begin by looking at the dataset downloaded already ","a512b315":"### K Fold Cross Validation\n\nNotice that we didn't create a validation set before training our XGBoost model. We'll use the K-fold cross validation strategy this time:","a775a735":"**Q: What is the minimum, maximum and average value of the charges column? Show the distribution of values in a graph**","84faa2f9":"Let's now view some basic statistics about numeric columns."}}