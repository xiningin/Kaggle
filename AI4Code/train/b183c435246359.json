{"cell_type":{"5e8e9e6f":"code","34f521b2":"code","58d45397":"code","f63f616a":"code","6fff9f40":"code","469c9522":"code","ddc743ed":"code","02d8de57":"code","e8353eb1":"code","c75265fc":"code","3ebe3fd4":"code","d23b9135":"code","b1abbd54":"code","737dfb9f":"code","8ed5ec7e":"code","31d5b4e6":"code","512c0cb5":"markdown","1f7ad8d2":"markdown","60027ba3":"markdown","4c870f25":"markdown","437b45f3":"markdown","450eba98":"markdown","8c7a7610":"markdown","55285724":"markdown","15a3850b":"markdown","0b43aea7":"markdown","1ec3f2c7":"markdown","9c33f5ad":"markdown","7c098589":"markdown"},"source":{"5e8e9e6f":"!pip install timm","34f521b2":"import os\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport random\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.metrics.functional import accuracy\nimport torch\n# from torchvision import models\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as albu\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nimport timm","58d45397":"import random\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything()","f63f616a":"TRAIN_CSV = \"..\/input\/cassava-leaf-disease-classification\/train.csv\"\nTRAIN_IMAGE_FOLDER = '..\/input\/cassava-leaf-disease-classification\/train_images'\nCLASSES = 5","6fff9f40":"class SymmetricCrossEntropy(nn.Module):\n\n    def __init__(self, alpha=0.1, beta=1.0, num_classes= 5):\n        super(SymmetricCrossEntropy, self).__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.num_classes = num_classes\n\n    def forward(self, logits, targets, reduction='mean'):\n        onehot_targets = torch.eye(self.num_classes)[targets].cuda()\n        ce_loss = F.cross_entropy(logits, targets, reduction=reduction)\n        rce_loss = (-onehot_targets*logits.softmax(1).clamp(1e-7, 1.0).log()).sum(1)\n        if reduction == 'mean':\n            rce_loss = rce_loss.mean()\n        elif reduction == 'sum':\n            rce_loss = rce_loss.sum()\n        return self.alpha * ce_loss + self.beta * rce_loss","469c9522":"# https:\/\/www.kaggle.com\/c\/siim-isic-melanoma-classification\/discussion\/173733#965200\n# implementations reference - https:\/\/github.com\/CoinCheung\/pytorch-loss\/blob\/master\/pytorch_loss\/taylor_softmax.py\n# paper - https:\/\/www.ijcai.org\/Proceedings\/2020\/0305.pdf\n\nclass TaylorSoftmax(nn.Module):\n\n    def __init__(self, dim=1, n=2):\n        super(TaylorSoftmax, self).__init__()\n        assert n % 2 == 0\n        self.dim = dim\n        self.n = n\n\n    def forward(self, x):\n        \n        fn = torch.ones_like(x)\n        denor = 1.\n        for i in range(1, self.n+1):\n            denor *= i\n            fn = fn + x.pow(i) \/ denor\n        out = fn \/ fn.sum(dim=self.dim, keepdims=True)\n        return out\n\nclass LabelSmoothingLoss(nn.Module):\n\n    def __init__(self, classes, smoothing=0.0, dim=-1): \n        super(LabelSmoothingLoss, self).__init__() \n        self.confidence = 1.0 - smoothing \n        self.smoothing = smoothing \n        self.cls = classes \n        self.dim = dim \n    def forward(self, pred, target): \n        \"\"\"Taylor Softmax and log are already applied on the logits\"\"\"\n        with torch.no_grad(): \n            true_dist = torch.zeros_like(pred) \n            true_dist.fill_(self.smoothing \/ (self.cls - 1)) \n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) \n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n    \n\nclass TaylorCrossEntropyLoss(nn.Module):\n\n    def __init__(self, n=2, ignore_index=-1, reduction='mean', smoothing=0.2):\n        super(TaylorCrossEntropyLoss, self).__init__()\n        assert n % 2 == 0\n        self.taylor_softmax = TaylorSoftmax(dim=1, n=n)\n        self.reduction = reduction\n        self.ignore_index = ignore_index\n        self.lab_smooth = LabelSmoothingLoss(5, smoothing=smoothing)\n\n    def forward(self, logits, labels):\n\n        log_probs = self.taylor_softmax(logits).log()\n        loss = self.lab_smooth(log_probs, labels)\n        return loss","ddc743ed":"FOLDS = 10\nBATCH_SIZE =24\nLR = 0.0001\nEPOCHS=4\nSMOOTHING = 0.1\n\nLOSS_FUNCTION = nn.CrossEntropyLoss()\nLOSS_FUNCTION = TaylorCrossEntropyLoss(n=2, smoothing=SMOOTHING)\nLOSS_FUNCTION = SymmetricCrossEntropy()\n\nIMG_SIZE = 128\nIMG_SIZE = 240\nIMG_SIZE = 512\n# IMG_SIZE = 380\n\nEARLY_STOPPING = True\n\nMODEL_ARCH = 'resnet50'\nMODEL_ARCH = 'tf_efficientnet_b1_ns'\nMODEL_ARCH = 'efficientnet_b3'\nMODEL_ARCH = 'tf_efficientnet_b4_ns'","02d8de57":"# # These are the available model architectures in timm\n# from pprint import pprint\n# model_names = timm.list_models(pretrained=True)\n# pprint(model_names)","e8353eb1":"class CassavaDataset(Dataset):\n    def __init__(self, train, train_mode=True, transforms=None):\n        self.train = train\n        self.transforms = transforms\n        self.train_mode = train_mode\n    \n    def __len__(self):\n        return self.train.shape[0]\n    \n    def __getitem__(self, index):\n        image_path = os.path.join(TRAIN_IMAGE_FOLDER, self.train.iloc[index].image_id)\n        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if (self.transforms):\n            image = self.transforms(image=image)[\"image\"]\n        \n        if not(self.train_mode):\n            return {\"x\":image}\n        \n        return {\n            \"x\": image,\n            \"y\": torch.tensor(self.train.iloc[index, self.train.columns.str.startswith('label')], dtype=torch.float64)\n        }","c75265fc":"def get_augmentations():\n    \n    mean = (0.485, 0.456, 0.406)\n    std = (0.229, 0.224, 0.225) \n    \n    train_augmentations = albu.Compose([\n        albu.RandomResizedCrop(IMG_SIZE, IMG_SIZE, p=1.0),\n        albu.Transpose(p=0.5),\n        albu.HorizontalFlip(p=0.5),\n#         albu.VerticalFlip(0.5),\n        albu.CoarseDropout (p=0.5),\n        albu.Normalize(always_apply=True),        \n        ToTensorV2(p=1.0)\n    ], p=1.0)\n    \n    valid_augmentations = albu.Compose([\n        albu.Resize(IMG_SIZE, IMG_SIZE),\n        albu.Normalize(always_apply=True),        \n        ToTensorV2(p=1.0)\n    ], p=1.0)   \n    \n    return train_augmentations, valid_augmentations\n\ntrain_augs, val_augs = get_augmentations()","3ebe3fd4":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = timm.create_model(MODEL_ARCH, pretrained=True)\n#         self.model = base_model\n\n#         Efficientnets\n        n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Linear(n_features, CLASSES)\n        \n#         Resnets\n#         n_features = self.model.fc.in_features\n#         self.model.fc = nn.Linear(n_features, CLASSES)\n        \n        self._freeze_batchnorm()  # NEW NEW NEW NEW NEW NEW NEW NEW NEW\n        \n    def _freeze_batchnorm(self):\n        for module in self.model.modules():\n            if isinstance(module, nn.BatchNorm2d):\n                if hasattr(module, 'weight'):\n                    module.weight.requires_grad_(False)\n                if hasattr(module, 'bias'):\n                    module.bias.requires_grad_(False)\n                module.eval()\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x","d23b9135":"traincsv = pd.read_csv(TRAIN_CSV)\ntraincsv['kfold'] = -1\ntraincsv = traincsv.sample(frac=1).reset_index(drop=True)\nstratifier = StratifiedKFold(n_splits=FOLDS)\n\nfor fold, (train_index, val_index) in enumerate(stratifier.split(X=traincsv.image_id.values, y=traincsv.label.values)):\n    traincsv.loc[val_index, \"kfold\"] = fold\n\ntraincsv.to_csv(\"train_folds.csv\", index=False)","b1abbd54":"class CassavaDataModule(pl.LightningDataModule):\n    def __init__(self, fold):\n        super().__init__()\n        self.train_aug, self.valid_aug = get_augmentations()\n        self.fold = fold\n        self.batch_size = BATCH_SIZE\n    \n    def setup(self, stage=None):\n        folds = pd.read_csv('.\/train_folds.csv')\n#         folds = pd.get_dummies(folds, columns=['label'])\n        train_fold = folds.loc[folds[\"kfold\"] != self.fold]\n        val_fold = folds.loc[folds[\"kfold\"] == self.fold]\n        \n        self.train_ds = CassavaDataset(train_fold, transforms=train_augs)\n        self.val_ds = CassavaDataset(val_fold, transforms=val_augs)\n        \n    def train_dataloader(self):\n        return DataLoader(self.train_ds, self.batch_size, num_workers=4, shuffle=True)\n        \n    def val_dataloader(self):\n        return DataLoader(self.val_ds, self.batch_size, num_workers=4, shuffle=False)        \n        ","737dfb9f":"class CassavaPLModule(pl.LightningModule):\n    def __init__(self, hparams, model):\n        super(CassavaPLModule, self).__init__()\n        self.hparams = hparams\n        self.model = model\n        self.criterion = LOSS_FUNCTION\n        self.accuracy = pl.metrics.Accuracy()\n        \n    def forward(self, x):\n        return self.model(x)\n    \n#     def configure_optimizers(self):\n#         optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hparams.lr)\n# #         optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.hparams.lr, weight_decay=0.001)\n#         scheduler = {\n#             'scheduler': \n#                 torch.optim.lr_scheduler.ReduceLROnPlateau(\n#                     optimizer, \n#                     patience=2,\n#                     factor=0.25,\n#                     threshold=0.01,\n#                     mode='min', verbose=True\n#                 ),\n#             'interval': 'epoch',\n#             'monitor' : 'val_loss'\n#         }\n#         return [optimizer], [scheduler]\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hparams.lr)\n        scheduler = {\n            'scheduler': \n                torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n                    optimizer, \n                    15,\n                    verbose=False\n                ),\n            'interval': 'step',\n            'monitor' : 'train_loss'\n        }\n        return [optimizer], [scheduler]    \n    \n    def training_step(self, batch, batch_index):\n        # One batch at a time\n        features = batch['x']\n        targets = batch['y']\n        out = self(features)\n        loss = self.criterion(out, targets.squeeze().long())\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)   \n#         import pdb; pdb.set_trace()        \n        metric_acc = accuracy(out, targets.squeeze().long())\n#         metric_acc = accuracy(torch.argmax(out, dim=1), torch.argmax(targets.squeeze(), dim=1))\n        self.log(\"train_accuracy\", metric_acc, on_step=False, on_epoch=True, prog_bar=True,logger=True)\n        return loss\n        \n    def validation_step(self, batch, batch_index):\n        # One batch at a time\n        features = batch['x']\n        targets = batch['y']\n        out = self(features)\n        loss = self.criterion(out, targets.squeeze().long())\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True) \n        metric_acc = accuracy(out, targets.squeeze().long())\n#         metric_acc = accuracy(torch.argmax(out, dim=1), torch.argmax(targets.squeeze(), dim=1))\n        self.log(\"val_accuracy\", metric_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)        ","8ed5ec7e":"def train(fold): \n    callbacks=[]\n    checkpoint_callback = ModelCheckpoint(\n        dirpath='checkpoints\/',\n        filename='model_{val_loss:.2f}',\n        monitor='val_loss', verbose=True,\n        save_last=False, save_top_k=1, save_weights_only=False,\n        mode='min', period=1, prefix=''\n    )        \n    callbacks.append(checkpoint_callback)\n    \n    early_stopping = EarlyStopping('val_accuracy', patience=3, verbose=True, mode='max')\n\n    if EARLY_STOPPING == True:\n        callbacks.append(early_stopping)    \n    \n    tpu_core = fold + 1\n    \n    trainer = pl.Trainer(\n                        gpus=-1 if torch.cuda.is_available() else None, \n                        precision=16,\n                        max_epochs=EPOCHS,\n#                         accumulate_grad_batches=4, # NEW NEW NEW NEW NEW NEW NEW NEW NEW\n                        callbacks=callbacks)\n    model = Model()\n    pl_dm = CassavaDataModule(fold=fold)\n    pl_module = CassavaPLModule(hparams={'lr':LR, 'batch_size':BATCH_SIZE}, model=model)\n    \n    trainer.use_native_amp = False\n    trainer.fit(pl_module, pl_dm)\n    \n    print(checkpoint_callback.best_model_path, checkpoint_callback.best_model_score)\n    ","31d5b4e6":"train(0)\ntrain(1)\ntrain(2)\ntrain(3)\ntrain(4)","512c0cb5":"### PL Module","1f7ad8d2":"### Training","60027ba3":"### PL Data module","4c870f25":"### K-Fold CV","437b45f3":"### NN Model","450eba98":"Inference at : https:\/\/www.kaggle.com\/krisho007\/simple-pytorch-lightning-inference","8c7a7610":"### Dataset","55285724":"1. Remove error images  \n2. https:\/\/www.kaggle.com\/khyeh0719\/pytorch-efficientnet-baseline-inference-tta - Augments","15a3850b":"### Taylor Smooth cross entropy","0b43aea7":"RandAugment ","1ec3f2c7":"### Symmetric Cross Entropy Loss\nhttps:\/\/www.kaggle.com\/c\/cassava-leaf-disease-classification\/discussion\/208239","9c33f5ad":"### Transforms","7c098589":"### Hyper parameters"}}