{"cell_type":{"6408b2db":"code","38ee2f0a":"code","4bcbf3eb":"code","a0f6b0ea":"code","881656df":"code","c0f1d2e1":"code","d524f73b":"code","2c8d7e93":"code","4506ae76":"code","37513ca8":"code","58b62c61":"code","6a8a5505":"code","162d3602":"code","d2fbb62b":"code","7c6d908c":"code","f6d2f4a4":"code","4a7e806d":"markdown","4347b326":"markdown","f5c4fa4b":"markdown","46942965":"markdown","28572cbd":"markdown"},"source":{"6408b2db":"import lightgbm as lgb\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt","38ee2f0a":"train = pd.read_csv(\"..\/input\/train.csv\").drop(\"ID_code\",axis=1)\ny_train = train['target']\nX_train = train.drop(['target'], axis = 1)\nX_test = pd.read_csv(\"..\/input\/test.csv\").drop(\"ID_code\",axis=1)","4bcbf3eb":"# use sample instead of all data\nsample = 0\nif(sample == 1):\n    sample_train_0 = train[train.target == 0].sample(n = 4000, random_state = 1573456)\n    sample_train_1 = train[train.target == 1].sample(n = 400, random_state = 1573456)\n    \n    train = sample_train_0.append(sample_train_1)\n    y_train = sample_train['target']\n    X_train = sample_train.drop(['target'], axis = 1)","a0f6b0ea":"# Scaling\nmmscale = MinMaxScaler()  \nX_train_scaled = mmscale.fit_transform(X_train)  \nX_test_scaled = mmscale.transform(X_test)","881656df":"# PCA\npca = PCA()  \nfactors_train = pca.fit_transform(X_train_scaled) \nfactors_test = pca.transform(X_test_scaled)","c0f1d2e1":"explained_variance = pca.explained_variance_ratio_  ","d524f73b":"pd.DataFrame(explained_variance,columns=['explained_variance']).plot(kind='box')","2c8d7e93":"with plt.style.context('dark_background'):\n    plt.figure(figsize=(15, 12))\n\n    plt.bar(range(200), explained_variance, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","4506ae76":"sum(explained_variance[:200])","37513ca8":"factors_train = pd.DataFrame(factors_train)\nfactors_test = pd.DataFrame(factors_test)","58b62c61":"train_combined = train.merge(factors_train, left_index = True, right_index = True)\ntest_combined = X_test.merge(factors_test, left_index = True, right_index = True)","6a8a5505":"# Inspiration from [2]\ndef augment(train, num_n = 1, num_p = 2):\n    newtrain=[train]\n    \n    n = train[train.target == 0]\n    for i in range(num_n):\n        newtrain.append( n.apply( lambda x:x.values.take(np.random.permutation(len(n))) ) )\n    \n    p = train[train.target == 1]\n    for i in range(num_p):\n        newtrain.append( p.apply( lambda x:x.values.take(np.random.permutation(len(p))) ) )\n    \n    return pd.concat(newtrain)","162d3602":"param = {\n    #'bagging_freq': 5,\n    #'bagging_fraction': 0.335,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.041,\n    'learning_rate': 0.009,\n    'max_depth': -1,\n    'metric':'auc',\n    'min_data_in_leaf': 70,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 15,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': -1\n}","d2fbb62b":"train_augmented = augment(train_combined)\n#train_augmented = train_combined\ntrain_data = lgb.Dataset(train_augmented.drop('target', axis = 1), train_augmented['target'])\n# with augmentation 12895 -> 0.898\n# without augmentation 12895 -> 0.897\nmodel = lgb.train(param, train_data, 19000)","7c6d908c":"prediction = model.predict(test_combined)","f6d2f4a4":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = prediction\nfilename = \"sub_{:%Y-%m-%d_%H_%M}.csv\".format(datetime.now())\nsubmission.to_csv(filename, index=False)","4a7e806d":"# Inspired by:\n\n[1] [Ole Morten Grod\u00e5s, \"Lightgbm with data augmentation\"](https:\/\/www.kaggle.com\/omgrodas\/lightgbm-with-data-augmentation)\n\n[2] [Jiwei Liu, \"LGB 2 leaves + augment\"](https:\/\/www.kaggle.com\/jiweiliu\/lgb-2-leaves-augment)\n\n[3] [Shrutti_Lyyer, \"Santander customer transaction, PCA and NB\"](https:\/\/www.kaggle.com\/shrutimechlearn\/santander-customer-transaction-pca-and-nb)","4347b326":"Big thanks to:\n* Jiwei Liu for [Augment insight](https:\/\/www.kaggle.com\/jiweiliu\/lgb-2-leaves-augment)\n* [Ole Morten Grod\u00e5s, Lightgbm with data augmentation](https:\/\/www.kaggle.com\/omgrodas\/lightgbm-with-data-augmentation)\n","f5c4fa4b":"# Sampling","46942965":"# PCA features","28572cbd":"# Augmentation"}}