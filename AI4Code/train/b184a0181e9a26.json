{"cell_type":{"1948ddf0":"code","b6fface7":"code","7e02e134":"code","093e5ba1":"code","32e9c8af":"code","d09cc8ab":"code","d732b180":"code","6d1b1d39":"code","620df8dd":"code","03b3f710":"code","6b803c29":"code","c40106c8":"code","83bfad77":"code","bf0b90c8":"code","4beb2ccf":"code","a0b890f6":"code","7e38db4a":"markdown","9bc5ff9e":"markdown","8774159f":"markdown","02e5a98b":"markdown","b522c28e":"markdown","24735790":"markdown","1e9639aa":"markdown","b265aa99":"markdown","930714a4":"markdown","3c7674dd":"markdown","e6079845":"markdown","092d3827":"markdown","8aa7f10c":"markdown","c943108a":"markdown","c79d1cb6":"markdown","692e7c33":"markdown","13ff8636":"markdown","7cd1177f":"markdown","8e5e9b73":"markdown","75fa13b7":"markdown","d26ea2c9":"markdown","850165bd":"markdown","ed95ee7c":"markdown","6856031f":"markdown","dfc5a3a8":"markdown","63378c08":"markdown","94c183e3":"markdown","0f2e52e0":"markdown","721fd608":"markdown","3e8306c3":"markdown","45607564":"markdown","8a393b39":"markdown","62906976":"markdown","01824e85":"markdown","8d883098":"markdown","d4f4a0f7":"markdown","131f526a":"markdown"},"source":{"1948ddf0":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error, r2_score\nfrom sklearn.model_selection import GridSearchCV","b6fface7":"df = pd.read_csv('..\/input\/winequality-red.csv')\ndf.head()","7e02e134":"df.info()","093e5ba1":"df['quality'].sort_values().unique()","32e9c8af":"def plot_wine_quality_histogram(quality):\n    unique_vals = df['quality'].sort_values().unique()\n    plt.xlabel(\"Quality\")\n    plt.ylabel(\"Count\")\n    plt.hist(quality.values, bins=np.append(unique_vals, 9), align='left')\nplot_wine_quality_histogram(df['quality'])","d09cc8ab":"def plot_features_correlation(df):\n    plt.figure(figsize=(7.5,6))\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    sns.set(font_scale=1)\n    corr_mat = df.corr()\n    ax = sns.heatmap(data=corr_mat, annot=True, fmt='0.1f', vmin=-1.0, vmax=1.0, center=0.0, xticklabels=corr_mat.columns, yticklabels=corr_mat.columns, cmap=\"Blues\")\n    colorbar = ax.collections[0].colorbar\n    colorbar.set_ticks([-1, -0.5, 0, 0.5, 1])\nplot_features_correlation(df)","d732b180":"y = df.quality\nX = df.drop('quality', axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)","6d1b1d39":"scaler = preprocessing.StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","620df8dd":"def scores_results(y_train, y_test, y_pred_train, y_pred_test):\n    #this function will provide us with accuracy and mse scores for training and test sets\n    y_pred_train_round = np.round(y_pred_train)\n    y_pred_test_round = np.round(y_pred_test)\n    accuracy = [accuracy_score(y_train, y_pred_train_round), accuracy_score(y_test, y_pred_test_round)]\n    mse_with_rounding = [mean_squared_error(y_train, y_pred_train_round), mean_squared_error(y_test, y_pred_test_round)]\n    results = pd.DataFrame(list(zip(accuracy, mse_with_rounding)), columns = ['accuracy score', 'mse (after rounding)'], index = ['train', 'test'])\n    return results\n\ndef baseline(X_train_scaled, X_test_scaled, y_train, y_test):\n    #dummy regressor which always predicts the mean of the dataset\n    from sklearn.dummy import DummyRegressor\n    baseline = DummyRegressor(strategy='mean')\n    baseline.fit(X_train_scaled, y_train)\n    y_pred_train = baseline.predict(X_train_scaled)\n    y_pred_test = baseline.predict(X_test_scaled)\n    return scores_results(y_train, y_test, y_pred_train, y_pred_test)\n\nbaseline(X_train_scaled, X_test_scaled, y_train, y_test)","03b3f710":"def linear_reg(X_train_scaled, X_test_scaled, y_train, y_test):\n    # basic linear regression\n    from sklearn.linear_model import LinearRegression\n    lm = LinearRegression()\n    lm.fit(X_train_scaled, y_train)\n    y_pred_train = lm.predict(X_train_scaled)\n    y_pred_test = lm.predict(X_test_scaled)\n    global metrics_lr #store this for a later comparison between different methods\n    metrics_lr = [accuracy_score(y_test, np.round(y_pred_test)), mean_squared_error(y_test, y_pred_test), r2_score(y_test, y_pred_test)]\n    return scores_results(y_train, y_test, y_pred_train, y_pred_test)\nlinear_reg(X_train_scaled, X_test_scaled, y_train, y_test)","6b803c29":"def lasso_reg(X_train_scaled, X_test_scaled, y_train, y_test):\n    from sklearn.linear_model import LassoCV\n    n_alphas = 5000\n    alpha_vals = np.logspace(-6, 0, n_alphas)\n    lr = LassoCV(alphas=alpha_vals, cv=10, random_state=0)\n    lr.fit(X_train_scaled, y_train)\n    y_pred_train = lr.predict(X_train_scaled)\n    y_pred_test = lr.predict(X_test_scaled)\n    metrics_lasso = [accuracy_score(y_test, np.round(y_pred_test)), mean_squared_error(y_test, y_pred_test), r2_score(y_test, y_pred_test)]\n    return metrics_lasso\ndef elastic_net_reg(X_train_scaled, X_test_scaled, y_train, y_test):\n    from sklearn.linear_model import ElasticNetCV\n    n_alphas = 300\n    l1_ratio = [.1, .3, .5, .7, .9]\n    rr = ElasticNetCV(n_alphas=n_alphas, l1_ratio=l1_ratio, cv=10, random_state=0)\n    rr.fit(X_train_scaled, y_train)\n    y_pred_train = rr.predict(X_train_scaled)\n    y_pred_test = rr.predict(X_test_scaled)\n    metrics_en = [accuracy_score(y_test, np.round(y_pred_test)), mean_squared_error(y_test, y_pred_test), r2_score(y_test, y_pred_test)]\n    return metrics_en\ndef ridge_reg(X_train_scaled, X_test_scaled, y_train, y_test):\n    from sklearn.linear_model import RidgeCV\n    n_alphas = 100\n    alpha_vals = np.logspace(-1, 3, n_alphas)\n    rr = RidgeCV(alphas=alpha_vals, cv=10)\n    rr.fit(X_train_scaled, y_train)\n    y_pred_train = rr.predict(X_train_scaled)\n    y_pred_test = rr.predict(X_test_scaled)\n    metrics_ridge = [accuracy_score(y_test, np.round(y_pred_test)), mean_squared_error(y_test, y_pred_test), r2_score(y_test, y_pred_test)]\n    return metrics_ridge\n\nmetrics_lasso = lasso_reg(X_train_scaled, X_test_scaled, y_train, y_test)\nmetrics_en = elastic_net_reg(X_train_scaled, X_test_scaled, y_train, y_test)\nmetrics_ridge = ridge_reg(X_train_scaled, X_test_scaled, y_train, y_test)\nfinalscores = pd.DataFrame(list(zip(metrics_lr, metrics_lasso, metrics_en, metrics_ridge)), columns = ['lr', 'lasso', 'el net', 'ridge'], index = ['acc','mse','r2'])\nfinalscores","c40106c8":"from sklearn.preprocessing import PolynomialFeatures\nX_deg2 = PolynomialFeatures(degree=2).fit_transform(X) #this has now 78 feautures\nX_deg3 = PolynomialFeatures(degree=3).fit_transform(X) #this has now 170 features\n\nX_train_deg2, X_test_deg2, y_train_deg2, y_test_deg2 = train_test_split(X_deg2, y, test_size=0.2, random_state=0, stratify=y)\nX_train_deg3, X_test_deg3, y_train_deg3, y_test_deg3 = train_test_split(X_deg3, y, test_size=0.2, random_state=0, stratify=y)\n\nscaler_deg2 = preprocessing.StandardScaler().fit(X_train_deg2)\nscaler_deg3 = preprocessing.StandardScaler().fit(X_train_deg3)\n\nX_train_scaled_deg2 = scaler_deg2.transform(X_train_deg2)\nX_train_scaled_deg3 = scaler_deg3.transform(X_train_deg3)\n\nX_test_scaled_deg2 = scaler_deg2.transform(X_test_deg2)\nX_test_scaled_deg3 = scaler_deg3.transform(X_test_deg3)","83bfad77":"from sklearn.exceptions import ConvergenceWarning\nimport warnings\nwarnings.filterwarnings(action='ignore', category=ConvergenceWarning) #this is just to disable some anoying warnings that sklearn forces to always come up\n\nlinear_reg(X_train_scaled_deg2, X_test_scaled_deg2, y_train_deg2, y_test_deg2)\nmetrics_lasso = lasso_reg(X_train_scaled_deg2, X_test_scaled_deg2, y_train_deg2, y_test_deg2)\nmetrics_en = elastic_net_reg(X_train_scaled_deg2, X_test_scaled_deg2, y_train_deg2, y_test_deg2)\nmetrics_ridge = ridge_reg(X_train_scaled_deg2, X_test_scaled_deg2, y_train_deg2, y_test_deg2)\n\nfinalscores = pd.DataFrame(list(zip(metrics_lr, metrics_lasso, metrics_en, metrics_ridge)), columns = ['lr', 'lasso', 'el net', 'ridge'], index = ['acc','mse','r2'])\nfinalscores","bf0b90c8":"linear_reg(X_train_scaled_deg3, X_test_scaled_deg3, y_train_deg3, y_test_deg3)\nmetrics_lasso = lasso_reg(X_train_scaled_deg3, X_test_scaled_deg3, y_train_deg3, y_test_deg3)\nmetrics_en = elastic_net_reg(X_train_scaled_deg3, X_test_scaled_deg3, y_train_deg3, y_test_deg3)\nmetrics_ridge = ridge_reg(X_train_scaled_deg3, X_test_scaled_deg3, y_train_deg3, y_test_deg3)\n\nfinalscores = pd.DataFrame(list(zip(metrics_lr, metrics_lasso, metrics_en, metrics_ridge)), columns = ['lr', 'lasso', 'el net', 'ridge'], index = ['acc','mse','r2'])\nfinalscores","4beb2ccf":"def svm_reg(X_train_scaled, X_test_scaled, y_train, y_test):\n    from sklearn.svm import SVR\n    parameters = [{'C': [0.1, 1, 10],\n                   'epsilon': [0.01, 0.1],\n                    'gamma': [0.01, 0.1, 0.3, 0.5, 1]}]\n    clf2 = SVR(kernel = 'rbf')\n    clf = GridSearchCV(clf2, parameters, cv=10)\n    clf.fit(X_train_scaled, y_train)\n    y_pred_train = clf.predict(X_train_scaled)\n    y_pred_test = clf.predict(X_test_scaled)\n    best_parameters = clf.best_params_\n    print ('best parameters:', best_parameters)\n    return scores_results(y_train, y_test, y_pred_train, y_pred_test)\nsvm_reg(X_train_scaled, X_test_scaled, y_train, y_test)","a0b890f6":"def nn_reg(X_train_scaled, X_test_scaled, y_train, y_test):\n    from sklearn.neural_network import MLPRegressor\n    parameters = [{'hidden_layer_sizes': [3, 5, 10, 100],\n                   'alpha': [0.01, 1, 10, 100],\n                   'activation': ['relu','logistic','tanh', 'identity']}]\n    nn = MLPRegressor(solver='lbfgs', random_state=0)\n    nn = GridSearchCV(nn, parameters, cv = 10)\n    nn.fit(X_train_scaled, y_train)\n    y_pred_train = nn.predict(X_train_scaled)\n    y_pred_test = nn.predict(X_test_scaled)\n    best_parameters = nn.best_params_\n    print ('best parameters:', best_parameters)\n    return scores_results(y_train, y_test, y_pred_train, y_pred_test)\nnn_reg(X_train_scaled, X_test_scaled, y_train, y_test)","7e38db4a":"#### Baseline regressor","9bc5ff9e":"Neural Networks are booming in the field of Machine Learning and Pattern Recognition. In this section we will be fitting a Multi-Layer Perceptron Neural Network, which performs training using the _backpropagation algoritm_.\n\nOnce again, to avoid overfitting, we will perform (10-fold) cross-validation over three hyperparameters: the number of hidden (fully-connected) layers, alpha (the L2 penalty used for regularisation) and the activation function at the end of each neuron.","8774159f":"There are only 6 unique values for quality scores in our dataset. They range from 3 to 8.","02e5a98b":"### Support Vector Machines (SVM)","b522c28e":"In this section we try to solve the problem using regression. This is a good starting point, since our quality attribute is _discrete numeric_ (ideal for regressions).","24735790":"It is usually a good idea to introduce some regularisation (penalty) in linear regressions, particularly to avoid overfitting. We will consider 3 different popular regularisation methods: __Lasso (L1)__, __Ridge (L2)__ and __Elastic Net\n(L1 and L2)__.\n\nIn theory, we expect the Ridge regression to provide better results than Lasso, since this dataset only has 11 features, and therefore the main advantage of Lasso, its feature selection property, will not really come into play.\n\nWe use 10-fold cross-validation to select the best parameter lambda for each of the three regressions.","1e9639aa":"### Initial dataset observations","b265aa99":"Looks like the distribution of quality scores is heavily centred around 5 and 6. This inbalance is likely to pose a challenge to our learning models, since most algorithms will tend to centre their predictions around 5 and 6, making the rest of the scores harder to predict.","930714a4":"Before completely moving away from linear regression algorithms, it could be interesting to try transforming the features with different polynomials.\n\nRecall that a transformation of a set of 2 features $(X_1, X_2)$ to second-order will yield a new set of features $(1, X_1, X_2, X_1X_2, X_1^2, X_2^2)$.\n\nIn our case, since we have 11 features, a transformation of degree 2 will yield __78 features__, and one of degree 3, __170 features__.","3c7674dd":"#### Feature Transformation","e6079845":"### Neural Networks","092d3827":"Plotting the correlation between all the given variables shows that the most positively correlated with quality is alcohol. Thus, alcohol is likely to have the biggest effect when predicting quality.","8aa7f10c":"The solver used for weight optimization is _lbfgs_, an optimizer in the family of quasi-Newton methods which tends to have one of the best performances.\n\nThis set-up demonstrates a very competitive performance, even ourperforming that of the SVM. What is more, it does not seem to be suffering from _overfitting_ at all (in fact, if anything, it would be suffering from _underfitting_!). This indicates that a deeper focus on this NN approach could potentially through even better results. I welcome feedback on how to take this model from here and improve it!","c943108a":"Interesting! Looks like we finally see an increase in performance. This was somehow expected, since 11 features was quite a low number to begin with, and 78 (and 170) features have the potential of giving us much more information. There is however something slightly controversial: when using a degree of 2, the non-regularised model surprisingly outperforms the regularised ones, but this is not the case when using a degree of 3. A possible explanation could be that the simple regression using degree 2 features does not suffer from overfitting yet (and therefore regularisation is neither necessary nor advised), while when using degree 3 features, the simple model actually begins to overfit, and regularisation starts doing a better job. This would also explain the poor performance of the non-regularised linear regression on the set with degree 3 features.\n\nAnother interesting observation is that Ridge is the best of the regularised regressions in both degree 2 and 3. This is probably because, as we saw earlier, there is some multicollinearity between several of the features. Applying a polynomial transformation does not change this (in fact, if anything, it would worsen it), and one of the main advantages of the L2 penalty is, precisely, that it helps address multicollinearity.","c79d1cb6":"### Background & short intro","692e7c33":"### Initial approaches: Regression","13ff8636":"Our first serious approach is to implement a classic linear regression:","7cd1177f":"Our motivation is to study and implement multiple linear and non-linear machine learning algorithms and evaluate their performance.\n\nThe chosen dataset consists of red wine samples from the north of Portugal. Each instance is composed of 12 variables: 11 physicochemical properties and a quality score which ranges from 1 (lowest) to 10 (highest).\n\nWe will investigate those predictors which, taking the 11 physicochemical properties as inputs, can predict the wine quality with the smallest error possible.","8e5e9b73":"With an accuracy of over 62%, this basic linear regression already improves our baseline of 40% by quite a significant amount. Let's keep exploring this path.","75fa13b7":"# Red Wine Quality","d26ea2c9":"We first split our data into training (80%) and test (20%) sets.","850165bd":"## Discussion of results","ed95ee7c":"We run the same regressions as in the previous section (same 10-fold cross-validation, etc.) but this time with the new sets of features:","6856031f":"Overall, the MLP Neural Network performed the best, and thus becomes our preferred method for implementation. However, the SVM method also left a good impression, and it could be interesting to see future work on it, for example regarding using other types of Neural Networks different than the MLP.\n\nThe results, in general, however, are not very encouraging, especially when we look at accuracy. We only managed to bring it up from 40.00% (baseline case) to 65.31% (using an RBF kernel). We suspect that this might be a consequence of the dataset being particularly sensitive to subjectiveness: the fact that the quality score is formulated by humans, even if these are experts in their field, seems to suggest that __there might not be a clear correlation__ between what society considers to be a _good wine_ and its _physicochemical properties_.\n","dfc5a3a8":"In our models, we would like to capture the idea that, for a wine of quality 5, an incorrect prediction of 2 is inherently somewhat worse than an (also incorrect) prediction of 4.\n\nFor this purpose, we will choose a loss function equal to the squared error. This is a widely used loss function in regression problems. The idea behind squaring the error is that negative values do not cancel positive ones.\n\nThroughout the rest of this study, we will make use of 2 different metrics in order to properly interpret and compare our results: __accuracy (%)__ and __mean squared error (MSE)__. Accuracy is a widely used metric in this field for performance benchmarking, hence why we will be reporting it. However, since this dataset does not have an equal number of instances of each quality score, it loses relevance and fails to capture the whole picture. To overcome this, we will also be using the mean squared error: this will provide us with extra information about the quality of the predictions (distance to the correct label), which the accuracy metric fails to provide.\n\nWhen looking at accuracy, higher is better. When looking at MSE, it will be the opposite.","63378c08":"### Data preprocessing","94c183e3":"Our dataset consists of 1599 wine instances, and does not seem to have any missing value","0f2e52e0":"#### Adding regularisation","721fd608":"Before even attempting a simple linear regression, it is important (and good practice) to consider the very most baseline predictor: one that always predicts the output to be the same number.\n\nIn this case, we choose this to be the mean of the quality attribute (of the training dataset).","3e8306c3":"Since our dataset is quite sparse and has attributes of varying scales, it is important to __standardize our data__ (instead of applying other popular data preprocessing techniques such as Min-Max scaling or a transformation to unit norm)\nTo achieve this, we standardise the training data to have __zero mean__ and __one standard deviation__. We then use these training parameters to scale our test data, which is the correct way of performing these kinds of tasks (for more info, check out this article http:\/\/sebastianraschka.com\/Articles\/2014_about_feature_scaling.html)","45607564":"### Importing libraries","8a393b39":"### Loss function and metrics","62906976":"Interestingly, despite there being 6 classes (the quality scores range from 3 to 8), this dummy algorithm achieves a surprisingly high accuracy of 40%.\n\nThis is, again, due to the fact that most wine instances in our dataset (more than half of them, in fact) have a quality of 5 or 6, as shown earlier.\n\nThis is our baseline model. The next algorithms we try should be __consistently outperforming it__.","01824e85":"These results clearly outperform all of the other methods we have tried so far. It seems like the RBF kernel does indeed quite a good job in separating our data. It is interesting to see how the results for the training data are significantly better from those of the test data. This means that our model is suffering from _overfitting_, even after our hyperparameter tuning.","8d883098":"A _Support Vector Machine (SVM)_ aims to maximize the margin between data points and a separating hyperplane. This hyperplane is defined by the so-called Support Vectors. SVMs use _kernels_ to transform the original data into a new space. Some of the most widely used kernels are: _linear_, _polynomials_, _RBF (Radial Based Function)_ and _sigmoid_.\n\nThe choice of kernel depends on the dataset, however, generally, RBF tends to outperform the others, and, in fact, this also seems to be the case for this dataset (proof not showed here due to space considerations, so you'll have to trust my word).\n\nThe __RBF kernel__ is a nonlinear and flexible one, which might be _prone to overfitting_. To avoid this problem, we need to carefully finetune its hyperparameters, which are $\\gamma$, $\\varepsilon$ and C. We do this using 10-fold cross-validation.\n\n$\\gamma$ is the free parameter of the Gaussian RBF function. A lower $\\gamma$ helps to fight overfitting (by increasing the bias and decreasing the variance).\n\n$\\varepsilon$, despite controlling something different (the size of the tube within which we tolerate prediction errors), ends up having a similar influence.\n\nFinally, C is the cost parameter, which controls the trade-off between the model complexity and the amount up to which deviations greater than $\\varepsilon$ can be tolerated.","d4f4a0f7":"#### Linear regression","131f526a":"Interestingly, adding regularisation did not really outperform our basic linear regression. This could be because it never suffered too much from overfitting in the first place.\nWe can confirm however, that Ridge does indeed (marginally) outperform Lasso, as expected, and that Elastic net is slightly\nthe best of all. This is due to the fact that it combines the advantages of both L1 and L2 penalty functions."}}