{"cell_type":{"b4b9c74e":"code","75a7bed8":"code","6f5c52d6":"code","5d7adf15":"code","3b7a2e5d":"code","78818ed1":"code","ef22ceff":"code","1e945b99":"code","ff8c126f":"code","e0c0c603":"code","d42a7a03":"code","6877d5d0":"code","2c94f445":"code","a7523735":"code","840e7b95":"code","f61eb954":"code","dc91a7db":"code","8abfac24":"code","13b6a5bd":"code","1a43cdf1":"code","88ad124d":"code","faca6b7b":"code","cef2e48c":"code","63235182":"code","42d5f5a3":"code","e2106700":"code","9985a83c":"code","cb6660df":"code","49b993d7":"code","b9cc4673":"code","fc6fcf17":"code","a5cbeab7":"code","c6892044":"code","da9aa566":"code","e9f63125":"code","003efc4c":"code","56385713":"code","9ed242f1":"code","80f62927":"code","71555414":"code","5733b87c":"code","c0b89ff5":"code","dccdf564":"code","80259757":"code","4f75fbf5":"code","a0ce560a":"code","797c3b6a":"code","a1cdfe1f":"code","e626bd18":"code","23ccd753":"code","00c95e2e":"code","64f8daec":"code","0c509c6e":"code","90353bf6":"code","25b88e3a":"code","0a66e2e0":"code","fc24ca77":"code","11ae0fe5":"code","2c9ec75b":"code","1f95c232":"code","462fe2e0":"code","d5927233":"code","98e6f699":"code","b633b6a7":"code","2b9352c4":"code","bd3c796d":"code","9b84212f":"code","8ff3c38c":"code","e513c9b4":"code","fb9de32e":"code","f8aee5c8":"code","f82ecd30":"code","74e99be3":"code","596ae4c5":"code","2b4ae92a":"code","2b64486c":"code","583c9f80":"code","14cf1faa":"code","3dc87edf":"code","eaf4b37d":"code","d2bbad30":"code","4b359f44":"code","6c7c3bb3":"code","386cc401":"code","8a1483df":"code","7a8a2a0e":"code","5bf091ec":"code","b519e1d1":"code","e83f7eb1":"code","c37d46b0":"code","c4f64554":"code","93d273c3":"code","2c136525":"code","c11fc53c":"code","8b260ac9":"code","af10f1cf":"code","170739c7":"code","75be0255":"code","91fb4f61":"code","5b28f37a":"code","c3d98c76":"code","866adc5c":"code","2fe9df5f":"code","5b93bd9f":"code","9d904647":"code","64c38366":"code","4d0adad9":"code","c0eb71b3":"code","b54760ba":"code","9dbad336":"code","fa33677c":"code","12ecc286":"code","54586941":"code","456f26c1":"code","2e92e564":"markdown","d149e086":"markdown","a2cd0caf":"markdown","a34f3b7f":"markdown","6c3fb865":"markdown","ebbdb82b":"markdown","6a95cb80":"markdown","a27d567c":"markdown","54d7a3d9":"markdown","52a1187b":"markdown","1423c8c2":"markdown","76df6349":"markdown","613f4e4e":"markdown","b988a8b2":"markdown","e23b8f15":"markdown","2843b016":"markdown","6263391f":"markdown","e8744887":"markdown","38bf5495":"markdown","723dd911":"markdown","e6120ec3":"markdown","7c8012fa":"markdown","87ebdc12":"markdown","3ffcbd7c":"markdown","2d8bd399":"markdown","d643344e":"markdown","e3bab470":"markdown","6a6c58ef":"markdown","9b32c028":"markdown","98bfa0b0":"markdown","80f37f84":"markdown"},"source":{"b4b9c74e":"pip install missingno","75a7bed8":"pip install catboost","6f5c52d6":"# Import Dependencies\n%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n# Machine learning\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')","5d7adf15":"# Import train & test data \ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ngender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv') # example of what a submission should look like","3b7a2e5d":"train.shape","78818ed1":"train.shape","ef22ceff":"train.head()","1e945b99":"test.head()","ff8c126f":"gender_submission.head()","e0c0c603":"train.head()","d42a7a03":"train.describe()","6877d5d0":"missingno.matrix(train,figsize=(30,10))","2c94f445":"#Alternatively way To See the missing value \ntrain.isnull().sum()","a7523735":"df_bin=pd.DataFrame()#for discretised  continous variable\ndf_con=pd.DataFrame()#for continous variable","840e7b95":"train.dtypes","f61eb954":"# How many people survived?\nfig = plt.figure(figsize=(20,1))\nsns.countplot(y='Survived', data=train);\nprint(train.Survived.value_counts())","dc91a7db":"#lets add this to our dataframe\ndf_bin['Survived']=train['Survived']\ndf_con['Survived']=train['Survived']","8abfac24":"df_bin.head()","13b6a5bd":"df_con.head()","1a43cdf1":"sns.distplot(train.Pclass)","88ad124d":"train.Pclass.isnull().sum()","faca6b7b":"#lets add this to our dataframe\ndf_bin['Pclass']=train['Pclass']\ndf_con['Pclass']=train['Pclass']","cef2e48c":" df_bin.head()","63235182":"df_con.head()","42d5f5a3":"train.Name.value_counts()","e2106700":"train.Name.isnull().sum()","9985a83c":"def sex(x):\n    if 'Miss' in x['Name']:return 1\n    elif 'Mrs' in x['Name']:return 2\n    else:return 3\ndf_bin['Name']=train.apply(sex,axis=1)\ndf_con['Name']=train.apply(sex,axis=1)","cb6660df":"df_bin.head()","49b993d7":"# Let's view the distribution of Sex\nplt.figure(figsize=(20,4))\nsns.countplot(y='Sex',data=train)","b9cc4673":"train.Sex.isnull().sum()","fc6fcf17":"train.Sex.head()","a5cbeab7":"#add sex to the dataframe\ndf_bin['Sex']=train['Sex']\ndf_bin['Sex']=np.where(df_bin['Sex']=='female',1,0)# change sex to 0 for male and 1 for female\ndf_con['Sex'] = train['Sex']","c6892044":"# How does the Sex variable look compared to Survival?\n# We can see this because they're both binarys.\nfig=plt.figure(figsize=(10,8))\nsns.distplot(df_bin.loc[df_bin['Survived'] ==1]['Sex'], kde_kws={'label':'Survived'});\nsns.distplot(df_bin.loc[df_bin['Survived'] ==0]['Sex'], kde_kws={'label':'Did not Survived'});\n","da9aa566":"#How many missing value does the Age have?\ntrain.Age.isnull().sum()","e9f63125":"#finding the average value\nsum_non = (train.Age.sum())\n#sum_non\nnon_miss_cols = len(train['Age']) -sum(train.Age.isnull())\navg_age = sum_non \/ non_miss_cols\navg_age = round(avg_age, 2)\navg_age","003efc4c":"#filling the NAN value with the average value\ndf_bin['Age']=train['Age'].fillna(avg_age)\ndf_bin\n\n","56385713":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train[train['Sex']=='female']\nmen = train[train['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","9ed242f1":"#Defining the function\ndef plot_count_dist(data, bin_df, label_column, target_column, figsize=(20, 5), use_bin_df=False):\n    if use_bin_df: \n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=bin_df);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     kde_kws={\"label\": \"Survived\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     kde_kws={\"label\": \"Did not survive\"});\n    else:\n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=data);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     kde_kws={\"label\": \"Survived\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     kde_kws={\"label\": \"Did not survive\"});","80f62927":"train.SibSp.isnull().sum()","71555414":"train.SibSp.value_counts()","5733b87c":"#add the SibSp to the dataframe\ndf_bin['SibSp']=train['SibSp']\ndf_con['SibSp']=train['SibSp']","c0b89ff5":"# Visualise the counts of SibSp and the distribution of the values\n# against Survived\nplot_count_dist(train, \n                bin_df=df_bin, \n                label_column='Survived', \n                target_column='SibSp', \n                figsize=(20, 10))\n","dccdf564":"train.Parch.isnull().sum()","80259757":"train.Parch.value_counts()","4f75fbf5":"#add the SibSp to the dataframe\ndf_bin['Parch']=train['Parch']\ndf_con['Parch']=train['Parch']","a0ce560a":"# Visualise the counts of Parch and the distribution of the values\n# against Survived\nplot_count_dist(train, \n                bin_df=df_bin,\n                label_column='Survived', \n                target_column='Parch', \n                figsize=(20, 10))","797c3b6a":"df_con.head()","a1cdfe1f":"df_bin.head()","e626bd18":"train.head()","23ccd753":"train.Ticket.isnull().sum()","00c95e2e":"sns.countplot(y='Ticket',data=train)","64f8daec":"#how many kind of ticket are there\ntrain.Ticket.value_counts()","0c509c6e":" #How many unique kinds of Ticket are there?\nprint(\"There are {} unique Ticket values.\".format(len(train.Ticket.unique())))","90353bf6":"train.Fare.isnull().sum()","25b88e3a":"sns.countplot(y='Fare',data=train)","0a66e2e0":"#How many unique kinds of Fare are there?\nprint(\"There are {} unique Fare values.\".format(len(train.Fare.unique())))","fc24ca77":"# Add Fare to sub dataframes\ndf_con['Fare']=train['Fare']\ndf_bin['Fare']=pd.cut(train['Fare'],bins=5)","11ae0fe5":"# What do our Fare bins look like?\ndf_bin.Fare.value_counts()\n\n","2c9ec75b":"# Visualise the counts of Tickets and the distribution of the values\n# against Survived\nplot_count_dist(train, \n                bin_df=df_bin,\n                label_column='Survived', \n                target_column='Fare', \n                figsize=(20, 10))","1f95c232":"train.Cabin.isnull().sum()","462fe2e0":"train.Cabin.value_counts()","d5927233":"#missing value in Embarked\ntrain.Embarked.isnull().sum()","98e6f699":"train.Embarked.value_counts()","b633b6a7":"sns.countplot(y='Embarked',data=train)","2b9352c4":"#adding dataframe to the subDataframe\ndf_bin['Embarked']=train['Embarked']\ndf_con['Embarked']=train['Embarked']","bd3c796d":"print(len(df_con))\ndf_bin=df_bin.dropna(subset=['Embarked'])\ndf_con=df_con.dropna(subset=['Embarked'])\nprint(len(df_con))\n\n","9b84212f":"df_bin.head()","8ff3c38c":"# One-hot encode binned variables\none_hot_cols = df_bin.columns.tolist()\none_hot_cols.remove('Survived')\ndf_bin_enc = pd.get_dummies(df_bin, columns=one_hot_cols)\n\ndf_bin_enc.head()","e513c9b4":"df_con.head(10)","fb9de32e":"# One hot encode the categorical columns\ndf_embarked_one_hot = pd.get_dummies(df_con['Embarked'], \n                                     prefix='embarked')\n\ndf_sex_one_hot = pd.get_dummies(df_con['Sex'], \n                                prefix='sex')\n\ndf_plcass_one_hot = pd.get_dummies(df_con['Pclass'], \n                                   prefix='pclass')","f8aee5c8":"# Combine the one hot encoded columns with df_con_enc\ndf_con_enc = pd.concat([df_con, \n                        df_embarked_one_hot, \n                        df_sex_one_hot, \n                        df_plcass_one_hot], axis=1)\n\n# Drop the original categorical columns (because now they've been one hot encoded)\ndf_con_enc = df_con_enc.drop(['Pclass', 'Sex', 'Embarked'], axis=1)","f82ecd30":"# Let's look at df_con_enc\ndf_con_enc.head(20)\n","74e99be3":"# Seclect the dataframe we want to use first for predictions\nselected_df = df_con_enc","596ae4c5":"selected_df.head()","2b4ae92a":"# Split the dataframe into data and labels\nX_train = selected_df.drop('Survived', axis=1) # data\ny_train = selected_df.Survived # labels","2b64486c":"# Shape of the data (without labels)\nX_train.shape\n","583c9f80":"X_train.head()","14cf1faa":"# Shape of the labels\ny_train.shape","3dc87edf":"# Function that runs the requested algorithm and returns the accuracy metrics\ndef fit_ml_algo(algo, X_train, y_train, cv):\n    \n    # One Pass\n    model = algo.fit(X_train, y_train)\n    acc = round(model.score(X_train, y_train) * 100, 2)\n    \n    # Cross Validation \n    train_pred = model_selection.cross_val_predict(algo, \n                                                  X_train, \n                                                  y_train, \n                                                  cv=cv, \n                                                  n_jobs = -1)\n    # Cross-validation accuracy metric\n    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n    \n    return train_pred, acc, acc_cv","eaf4b37d":"start_time = time.time()\ntrain_pred_log, acc_log, acc_cv_log = fit_ml_algo(LogisticRegression(), \n                                                               X_train, \n                                                               y_train, \n                                                                    10)\nlog_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_log)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_log)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=log_time))","d2bbad30":"# k-Nearest Neighbours\nstart_time = time.time()\ntrain_pred_knn, acc_knn, acc_cv_knn = fit_ml_algo(KNeighborsClassifier(), \n                                                  X_train, \n                                                  y_train, \n                                                  10)\nknn_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_knn)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_knn)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=knn_time))","4b359f44":"# Gaussian Naive Bayes\nstart_time = time.time()\ntrain_pred_gaussian, acc_gaussian, acc_cv_gaussian = fit_ml_algo(GaussianNB(), \n                                                                      X_train, \n                                                                      y_train, \n                                                                           10)\ngaussian_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_gaussian)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gaussian)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gaussian_time))","6c7c3bb3":"# Linear SVC\nstart_time = time.time()\ntrain_pred_svc, acc_linear_svc, acc_cv_linear_svc = fit_ml_algo(LinearSVC(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\nlinear_svc_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_linear_svc)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svc)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=linear_svc_time))","386cc401":"# Stochastic Gradient Descent\nstart_time = time.time()\ntrain_pred_sgd, acc_sgd, acc_cv_sgd = fit_ml_algo(SGDClassifier(), \n                                                  X_train, \n                                                  y_train,\n                                                  10)\nsgd_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_sgd)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_sgd)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=sgd_time))","8a1483df":"# Decision Tree Classifier\nstart_time = time.time()\ntrain_pred_dt, acc_dt, acc_cv_dt = fit_ml_algo(DecisionTreeClassifier(), \n                                                                X_train, \n                                                                y_train,\n                                                                10)\ndt_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_dt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_dt)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=dt_time))","7a8a2a0e":"# Gradient Boosting Trees\nstart_time = time.time()\ntrain_pred_gbt, acc_gbt, acc_cv_gbt = fit_ml_algo(GradientBoostingClassifier(), \n                                                                       X_train, \n                                                                       y_train,\n                                                                       10)\ngbt_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_gbt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gbt)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gbt_time))","5bf091ec":"# View the data for the CatBoost model\nX_train.head()","b519e1d1":"# View the labels for the CatBoost model\ny_train.head()","e83f7eb1":"# Define the categorical features for the CatBoost model\ncat_features = np.where(X_train.dtypes != np.float)[0]\ncat_features","c37d46b0":"# Use the CatBoost Pool() function to pool together the training data and categorical feature labels\ntrain_pool = Pool(X_train, \n                  y_train,\n                  cat_features)","c4f64554":"y_train.head()","93d273c3":"# CatBoost model definition\ncatboost_model = CatBoostClassifier(iterations=1000,\n                                    custom_loss=['Accuracy'],\n                                    loss_function='Logloss')\n\n# Fit CatBoost model\ncatboost_model.fit(train_pool,\n                   plot=True)\n\n# CatBoost accuracy\nacc_catboost = round(catboost_model.score(X_train, y_train) * 100, 2)","2c136525":"start_time = time.time()\n\n# Set params for cross-validation as same as initial model\ncv_params = catboost_model.get_params()\n\n# Run the cross-validation for 10-folds (same as the other models)\ncv_data = cv(train_pool,\n             cv_params,\n             fold_count=10,\n             plot=True)\n\n# How long did it take?\ncatboost_time = (time.time() - start_time)\n\n# CatBoost CV results save into a dataframe (cv_data), let's withdraw the maximum accuracy score\nacc_cv_catboost = round(np.max(cv_data['test-Accuracy-mean']) * 100, 2)\n","c11fc53c":"# Print out the CatBoost model metrics\nprint(\"---CatBoost Metrics---\")\nprint(\"Accuracy: {}\".format(acc_catboost))\nprint(\"Accuracy cross-validation 10-Fold: {}\".format(acc_cv_catboost))\nprint(\"Running Time: {}\".format(datetime.timedelta(seconds=catboost_time)))","8b260ac9":"models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees',\n              'CatBoost'],\n    'Score': [\n        acc_knn, \n        acc_log,  \n        acc_gaussian, \n        acc_sgd, \n        acc_linear_svc, \n        acc_dt,\n        acc_gbt,\n        acc_catboost\n    ]})\nprint(\"---Reuglar Accuracy Scores---\")\nmodels.sort_values(by='Score', ascending=False)\n","af10f1cf":"cv_models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees',\n              'CatBoost'],\n    'Score': [\n        acc_cv_knn, \n        acc_cv_log,      \n        acc_cv_gaussian, \n        acc_cv_sgd, \n        acc_cv_linear_svc, \n        acc_cv_dt,\n        acc_cv_gbt,\n        acc_cv_catboost\n    ]})\nprint('---Cross-validation Accuracy Scores---')\ncv_models.sort_values(by='Score', ascending=False)","170739c7":"# Feature Importance\ndef feature_importance(model, data):\n    \"\"\"\n    Function to show which features are most important in the model.\n    ::param_model:: Which model to use?\n    ::param_data:: What data to use?\n    \"\"\"\n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': data.columns})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\n    _ = fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 10))\n    return fea_imp\n    #plt.savefig('catboost_feature_importance.png')","75be0255":"# Plot the feature importance scores\nfeature_importance(catboost_model, X_train)","91fb4f61":"\nmetrics = ['Precision', 'Recall', 'F1', 'AUC']\n\neval_metrics = catboost_model.eval_metrics(train_pool,\n                                           metrics=metrics,\n                                           plot=True)\n\nfor metric in metrics:\n    print(str(metric)+\": {}\".format(np.mean(eval_metrics[metric])))","5b28f37a":"# We need our test dataframe to look like this one\nX_train.head()\n","c3d98c76":"# Our test dataframe has some columns our model hasn't been trained on\ntest.head()","866adc5c":"# One hot encode the columns in the test data frame (like X_train)\ntest_embarked_one_hot = pd.get_dummies(test['Embarked'], \n                                       prefix='embarked')\n\ntest_sex_one_hot = pd.get_dummies(test['Sex'], \n                                prefix='sex')\n\ntest_plcass_one_hot = pd.get_dummies(test['Pclass'], \n                                   prefix='pclass')","2fe9df5f":"# Combine the test one hot encoded columns with test\ntest = pd.concat([test, \n                  test_embarked_one_hot, \n                  test_sex_one_hot, \n                  test_plcass_one_hot], axis=1)","5b93bd9f":"# Let's look at test, it should have one hot encoded columns now\ntest.head()","9d904647":"# Create a list of columns to be used for the predictions\nwanted_test_columns = X_train.columns\nwanted_test_columns","64c38366":"# Make a prediction using the CatBoost model on the wanted columns\npredictions = catboost_model.predict(test[wanted_test_columns])","4d0adad9":"# Our predictions array is comprised of 0's and 1's (Survived or Did Not Survive)\npredictions[:20]","c0eb71b3":"# Create a submisison dataframe and append the relevant columns\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = test['PassengerId']\nsubmission['Survived'] = predictions # our model predictions on the test dataset\nsubmission.head()","b54760ba":"# What does our submission have to look like?\ngender_submission.head()","9dbad336":"# Let's convert our submission dataframe 'Survived' column to ints\nsubmission['Survived'] = submission['Survived'].astype(int)\nprint('Converted Survived column to integers.')","fa33677c":"# How does our submission dataframe look?\nsubmission.head()","12ecc286":"# Are our test and submission dataframes the same length?\nif len(submission) == len(test):\n    print(\"Submission dataframe is the same length as test ({} rows).\".format(len(submission)))\nelse:\n    print(\"Dataframes mismatched, won't be able to submit to Kaggle.\")\n","54586941":"# Convert submisison dataframe to csv for submission to csv \n# for Kaggle submisison\nsubmission.to_csv('..\/catboost_submission.csv', index=False)\nprint('Submission CSV is ready!')","456f26c1":"# Check the submission csv to make sure it's in the right format\nsubmissions_check = pd.read_csv(\"..\/catboost_submission.csv\")\nsubmissions_check.head()","2e92e564":"# Fare\nDescription: How much the ticket cost.","d149e086":"# Logistic Regression","a2cd0caf":"# Age\nDescription: The age of the passenger.","a34f3b7f":"Now we have our two sub dataframes ready. We can encode the features so they're ready to be used with our machine learning models.\n\nWe will encode our binned dataframe (df_bin) with one-hot encoding and our continuous dataframe (df_con) with the label encoding function from sklearn.","6c3fb865":"# Sex\nDescription: The sex of the passenger (male or female).","ebbdb82b":"# Name\nDescription: The name of the passenger.","6a95cb80":"# What datatypes are in the dataframe?","a27d567c":"# Embarked\nDescription: The port where the passenger boarded the Titanic.\n\nKey: C = Cherbourg, Q = Queenstown, S = Southampton\n\n","54d7a3d9":"Majority of the female survived","52a1187b":"Data Descriptions\nSurvival: 0 = No, 1 = Yes\n\npclass (Ticket class): 1 = 1st, 2 = 2nd, 3 = 3rd\n\nsex: Sex\n\nAge: Age in years\n\nsibsp: number of siblings\/spouses aboard the Titanic\n\nparch: number of parents\/children aboard the Titanic\n\nticket: Ticket number\n\nfare: Passenger fare\n\ncabin: Cabin number\n\nembarked: Port of Embarkation, C = Cherbourg, Q = Queenstown, S = Southampton","1423c8c2":"What missing values are there?\nWhere are the holes in our data?\n\nThese are rows which are missing a value or have NaN instead of something like the rest of the column.\n\n","76df6349":" Now our data has been manipulating and converted to numbers, we can run a series of different machine learning algorithms over it to find which yield the best results.","613f4e4e":"# SibSp\nDescription: The number of siblings\/spouses the passenger has aboard the Titanic.","b988a8b2":"# Survived\nDescription: Whether the passenger survived or not.\n\nKey: 0 = did not survive, 1 = survived\n\nThis is the variable we want our machine learning model to predict based off all the others.","e23b8f15":"# Cabin\nCabin number where the passenger was staying at","2843b016":"# K-Nearest Neighbours","6263391f":"Define a function to fit machine learning algorithms\nSince many of the algorithms we will use are from the sklearn library, they all take similar (practically the same) inputs and produce similar outputs.\n\nTo prevent writing code multiple times, we will functionise fitting the model and returning the accuracy scores.","e8744887":"# Ticket\nDescription: The ticket number of the boarding passenger.","38bf5495":"# Parch\nDescription: The number of parents\/children the passenger has aboard the Titanic.\n\nSince this feature is similar to SibSp, we'll do a similar analysis.","723dd911":"# Let's seperate the data","e6120ec3":"# Start Building Machine Learning Models","7c8012fa":"Another way to find the number of tickets","87ebdc12":"\n# Encoding","3ffcbd7c":"Since there are too many missing values, we won't use Cabin for our initial models and won't add it to our sub dataframes.","2d8bd399":"\nSince this is already binary variable (male or female), let's add it straight to our subset dataframes.\n\n","d643344e":"# Define a function to fit machine learning algorithms","e3bab470":"Ok we can clearly see some missing values here. Especially in the cabin column.\n\nIt's important to visualise missing values early so you know where the major holes are in your dataset.\n\nKnowing this informaiton will help with your EDA and figuring out what kind of data cleaning and preprocessing is needed.","6a6c58ef":"Every row has a unique name. This is equivalent to the passenger ID. But name could be used differently.\n\nCan you think of ways you could reduce the number of different names? Or create new features out of the names?\n\nNote: Because of so many different names and to keep this EDA fast, we won't move forward using the name variable.\n\n","9b32c028":"We've removed the two rows with missing values for Embarked, now we can add Embarked to our sub dataframes","98bfa0b0":"# To perform our data analysis, let's create two new dataframes\nWe'll create one for exploring discretised continuous variables (continuous variables which have been sorted into some kind of category) and another for exploring continuous variables.","80f37f84":"# Pclass\n\nLet's plot the distribution\nWe will look at the distribution of each feature first if we can to understand what kind of spread there is across the dataset.\n\nFor example, if there are values which are completely outside of the distribution, we may not want to include them in our model."}}