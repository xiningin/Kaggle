{"cell_type":{"a9cfea7a":"code","933b18c3":"code","fe344adb":"code","d2641c37":"code","41a7879b":"code","5ed3a1fd":"code","e377f716":"code","966ab7fc":"code","cdfdbcfd":"code","b92c501f":"code","bb542397":"code","b359fc3a":"code","3e6be104":"code","29d85e7d":"code","07c11b22":"code","efa4425f":"code","5427b86e":"code","8a5a42d9":"code","599e2e97":"code","dea21ccd":"code","b19f7d23":"code","b7c37ff9":"code","1d7b505d":"code","3146a799":"code","3ea189a2":"code","984073cc":"code","1f3aaff1":"code","00dd6526":"code","f8f66f85":"code","c988dae8":"code","436bfcc7":"code","8d51d3aa":"code","404bc7a0":"code","48ce1ec1":"code","2d627a78":"code","48a8a284":"code","5b82f483":"code","0f96b894":"code","888b9ccc":"code","38e2a68f":"code","d5a67f4b":"code","64f1ab38":"code","fedaf8eb":"code","ab2321d2":"code","8339daa8":"code","ba2ebbcd":"code","619b6f62":"code","ba48afdb":"markdown","8360d1a2":"markdown","8b7a57de":"markdown","f4abf108":"markdown","9c071fdd":"markdown","dde23f87":"markdown","53466377":"markdown","4aa7b86e":"markdown","4581d847":"markdown","45ded588":"markdown","8b8da1b8":"markdown","6a7cafa9":"markdown","34713e9f":"markdown","8b382a3d":"markdown","14707bab":"markdown","983d6f16":"markdown","1ab6488d":"markdown","996363fe":"markdown","9bf0ef37":"markdown","5d3a648d":"markdown","0ecaf372":"markdown","57899b23":"markdown","f3eb04d8":"markdown","68c6f574":"markdown","54fdcb74":"markdown","993ed341":"markdown","484ce668":"markdown","ad70156f":"markdown","6501fd70":"markdown","e6126bde":"markdown","dc55ebb9":"markdown","66d3d1bb":"markdown","26846461":"markdown","5e74098f":"markdown","c259c811":"markdown","6bd4913e":"markdown","cb91e819":"markdown","8f70cb82":"markdown","0184ecd1":"markdown","15f2a59e":"markdown","6e28f5b7":"markdown","56db7ba1":"markdown","590fedbe":"markdown"},"source":{"a9cfea7a":"import numpy as np \nimport pandas as pd \n\n# Input data files are available in the read-only \"..\/input\/\" directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","933b18c3":"# import libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nimport IPython.display as display\n\n# Graphics libraries\n#!pip install pywaffle\n#!pip install pycomp\n\nimport shap\n\n#from pywaffle import Waffle\nimport xgboost\nimport pickle\n\nimport warnings\n#warnings.filterwarnings(\"ignore\")","fe344adb":"def missing_count(df):\n    missing_count = df.isna().sum()\n    missing_df = (pd.concat([missing_count.rename('Missing count'),\n                     missing_count.div(len(df))\n                          .rename('Missing ratio')],axis = 1)\n             .loc[missing_count.ne(0)])\n    return missing_df.sort_values(by=\"Missing ratio\")","d2641c37":"SampleSubmission = \"..\/input\/widsdatathon2021\/SampleSubmissionWiDS2021.csv\"\nSolutionTemplate = \"..\/input\/widsdatathon2021\/SolutionTemplateWiDS2021.csv\"\nDataDictionary = \"..\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv\"\nUnlabeledWiDS2021 = \"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\"\nTrainingWiDS2021 = \"..\/input\/widsdatathon2021\/TrainingWiDS2021.csv\"","41a7879b":"table_train =  pd.read_csv(TrainingWiDS2021)\ntable_test = pd.read_csv(UnlabeledWiDS2021)\ndata_dictionary = pd.read_csv(DataDictionary)","5ed3a1fd":"missing_df = missing_count(table_train)\n\ndisplay.display(table_train.describe().T)\ndisplay.display(missing_df)\ntable_train.head()","e377f716":"missing_df = missing_count(table_test)\n\ndisplay.display(table_test.describe().T)\ndisplay.display(missing_df)\ntable_test.head()","966ab7fc":"fig = go.Figure(data=[go.Table(\n    header=dict(values=list(data_dictionary.columns),\n                fill_color=\"#551a3a\",\n                line_color='black',\n                font_color = \"white\",\n                align='center'),\n    cells=dict(values=[data_dictionary.Category,data_dictionary['Variable Name'],data_dictionary['Unit of Measure'],data_dictionary['Data Type'],data_dictionary['Description'],data_dictionary['Example']],\n               fill_color=\"#42a385\",\n               line_color='black',\n               align='left'))\n])\n\nfig.show()","cdfdbcfd":"# Droping useless columns\ncol_useless = ['Unnamed: 0','encounter_id', 'hospital_id', 'icu_id']\ntable_train.drop(columns= col_useless, inplace = True)\ntable_test.drop(columns= col_useless, inplace = True)","b92c501f":"column_nan_miss = missing_df[missing_df[\"Missing ratio\"]>.3].index.to_list()\ntable_train.drop(columns=column_nan_miss, inplace = True)\ntable_test.drop(columns=column_nan_miss, inplace = True)","bb542397":"count_values = table_train.dropna().nunique().reset_index()\nonly_one = count_values[count_values[0]<2][\"index\"].to_list()\n\ntable_train.drop(columns=only_one, inplace = True)\ntable_test.drop(columns=only_one, inplace = True)","b359fc3a":"featu_int=[]\nfeatu_float=[]\nfeatu_obj=[]\nfor col in table_train.columns:\n    x=table_train[col].dtype\n    if x=='int64' or x == 'int32':\n        table_train[col] = table_train[col].astype('int32')\n        featu_int.append(col)\n    elif x=='float64' or x== 'float32':\n        table_train[col] = table_train[col].astype('float32')\n        featu_float.append(col)\n    else:\n        featu_obj.append(col)","3e6be104":"df = table_train[[\"gender\", \"age\", \"diabetes_mellitus\"]].dropna()\nfig = px.histogram(df, x=\"age\", y=\"diabetes_mellitus\", color=\"gender\",\n                   marginal=\"box\", # or violin, rug\n                   hover_data=df.columns)\nfig.update_layout(title_text='Gender Distribution by Age - Positive for Diabetes' ,\n                  barmode='overlay',\n                  template= \"plotly_dark\" , \n                  xaxis = dict(title = 'Age'), \n                  yaxis = dict(title = 'Count Positive to Diabetes'))\nfig.show()","29d85e7d":"# Grouping data and count diabetes status by gender\ngender = table_train[[\"gender\", \"diabetes_mellitus\"]].copy()\ngender[\"count\"] = 1\ngender = gender.groupby([\"gender\", \"diabetes_mellitus\"]).count().reset_index()\n\n# Plotly bar char\ntemp_df = gender[gender[\"gender\"]== \"F\"]\ntemp_df.sort_values(by = \"count\", inplace= True )\ntemp_df[\"diabetes_mellitus\"] = temp_df[\"diabetes_mellitus\"].astype(str)\n\ntemp_df2 = gender[gender[\"gender\"]== \"M\"]\ntemp_df2.sort_values(by = \"count\", inplace= True )\ntemp_df2[\"diabetes_mellitus\"] = temp_df2[\"diabetes_mellitus\"].astype(str)\n\n\ntrace1 = go.Bar(\n                x = temp_df['diabetes_mellitus'],\n                y = temp_df['count'], name =\"Female\",\n                marker = dict(color = 'rgb(1,200,15)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=temp_df['count'], textposition='outside')\n\ntrace2 = go.Bar(\n                x = temp_df2['diabetes_mellitus'],\n                y = -temp_df2['count'], name =\"Male\",\n                marker = dict(color = 'rgb(200,12,15)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=temp_df2['count'], textposition='outside')\n\n\n\n\nlayout = go.Layout(template= \"plotly_dark\",title = 'Diabetes Mellitus by Gender' , xaxis = dict(title = 'Diabetes mellitus'), yaxis = dict(title = 'Count'), height=650)\nfig = go.Figure(data = [trace1, trace2], layout = layout)\nfig.update_yaxes(showticklabels=False)\nfig.update_layout(barmode='overlay')\nfig.show()","07c11b22":"trace = go.Pie(labels=table_train[\"gender\"].value_counts().index,\n               values=table_train[\"gender\"].value_counts().values, \n               hoverinfo='percent+value+label', \n               textinfo='percent',\n               textposition='inside',\n               showlegend=True,\n               marker=dict(colors=plt.cm.viridis_r(np.linspace(0, 1, 28)),\n                           line=dict(color='#000000',\n                                     width=2),\n                          )\n                  )\n\nlayout = go.Layout(title=\"Gender Representation - Whole DataSet\",template= \"plotly_dark\")\nfig=go.Figure(data=[trace], layout=layout)\nfig.show()","efa4425f":"temp_df = table_train[\"ethnicity\"].value_counts()\ntemp_df2 = table_train[table_train[\"diabetes_mellitus\"]==1][\"ethnicity\"].value_counts()\n\ntrace = go.Pie(labels=temp_df.index.sort_values(), \n               values=temp_df.values, \n               hoverinfo='percent+value+label', \n               textinfo='percent',\n               textposition='inside',\n               showlegend=True,\n               hole=.2,\n               pull=[0.1 for _ in range(len(temp_df.index))],\n               marker=dict(colors=plt.cm.viridis_r(np.linspace(0, 1, 28)),\n                           line=dict(color='#000000',\n                                     width=2),\n                          )\n                  )\n\ntrace2 = go.Pie(labels=temp_df2.index.sort_values(), \n               values=temp_df2.values, \n               hoverinfo='percent+value+label', \n               textinfo='percent',\n               textposition='inside',\n               showlegend=True,\n               hole=.2,\n               pull=[0.1 for _ in range(len(temp_df2.index))],\n               marker=dict(colors=plt.cm.viridis_r(np.linspace(0, 1, 28)),\n                           line=dict(color='#000000',\n                                     width=2),\n                          )\n                  )\n\nlayout = go.Layout(title=\"Ethnicity Representation - Whole data set\",template= \"plotly_dark\")\nfig=go.Figure(data=[trace], layout=layout)\nfig.show()\n\nlayout = go.Layout(title=\"Ethnicity Representation - Diabetes Positive\",template= \"plotly_dark\")\nfig=go.Figure(data=[trace2], layout=layout)\nfig.show()","5427b86e":"# hospital_admit_source\n\ntemp_df = table_train[\"hospital_admit_source\"].value_counts()\ntemp_df2 = table_train[table_train[\"diabetes_mellitus\"]==1][\"hospital_admit_source\"].value_counts()\n\n\ntrace1 = go.Bar(\n                x = temp_df.index,\n                y = temp_df.values, name =\"Whole Data\",\n                marker = dict(color = 'rgb(1,200,15)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=temp_df.values, textposition='outside')\n\ntrace2 = go.Bar(\n                x = temp_df2.index,\n                y = -temp_df2.values, name =\"Diabetes Positive\",\n                marker = dict(color = 'rgb(200,28,15)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=temp_df2.values, textposition='outside')\n\n\nlayout = go.Layout(template= \"plotly_dark\",title = 'hospital_admit_source' , xaxis = dict(title = 'Diabetes mellitus'), yaxis = dict(title = 'Count'), height=650)\nfig = go.Figure(data = [trace1, trace2], layout = layout)\nfig.update_yaxes(showticklabels=False)\nfig.update_layout(barmode='overlay')\nfig.show()","8a5a42d9":"temp_df = table_train[\"icu_type\"].value_counts()\ntemp_df2 = table_train[table_train[\"diabetes_mellitus\"]==1][\"icu_type\"].value_counts()\n\n\ntrace1 = go.Bar(\n                x = temp_df.index,\n                y = temp_df.values, name =\"Whole Data\",\n                marker = dict(color = 'rgb(1,200,15)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=temp_df.values, textposition='outside')\n\ntrace2 = go.Bar(\n                x = temp_df2.index,\n                y = -temp_df2.values, name =\"Diabetes Positive\",\n                marker = dict(color = 'rgb(200,28,15)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=temp_df2.values, textposition='outside')\n\n\nlayout = go.Layout(template= \"plotly_dark\",title = 'icu_type' , xaxis = dict(title = 'Diabetes mellitus'), yaxis = dict(title = 'Count'), height=650)\nfig = go.Figure(data = [trace1, trace2], layout = layout)\nfig.update_yaxes(showticklabels=False)\nfig.update_layout(barmode='overlay')\nfig.show()","599e2e97":"temp_df = table_train[\"icu_stay_type\"].value_counts()\ntemp_df2 = table_train[table_train[\"diabetes_mellitus\"]==1][\"icu_stay_type\"].value_counts()\n\ntrace1 = go.Bar(\n                x = temp_df.index,\n                y = temp_df.values, name =\"Whole Data\",\n                marker = dict(color = 'rgb(1,200,15)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=temp_df.values, textposition='outside')\n\ntrace2 = go.Bar(\n                x = temp_df2.index,\n                y = -temp_df2.values, name =\"Diabetes Positive\",\n                marker = dict(color = 'rgb(200,28,15)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=temp_df2.values, textposition='outside')\n\n\nlayout = go.Layout(template= \"plotly_dark\",title = 'icu_stay_type' , xaxis = dict(title = 'Diabetes mellitus'), yaxis = dict(title = 'Count'), height=650)\nfig = go.Figure(data = [trace1, trace2], layout = layout)\nfig.update_yaxes(showticklabels=False)\nfig.update_layout(barmode='overlay')\nfig.show()","dea21ccd":"# Create distplot with curve_type set to 'normal'\ndef hist_plotly(df, col,bin_size=5,color = \"slategray\"):\n    colors = [color]\n    fig = ff.create_distplot([df[col].dropna()], [col],bin_size=bin_size,\n                             curve_type='normal', # override default 'kde'\n                             show_rug=False,\n                             colors=colors)\n\n    # Add title\n    fig.update_layout(title_text='{} - Distplot with Normal Distribution'.format(col.upper()) ,\n                  template= \"plotly_dark\" , \n                  xaxis = dict(title = col.upper()))\n    fig.show()","b19f7d23":"hist_plotly(table_train, \"age\", color = \"blue\")","b7c37ff9":"hist_plotly(table_train, \"height\", color=\"magenta\")","1d7b505d":"hist_plotly(table_train, \"weight\")","3146a799":"hist_plotly(table_train, \"bmi\",bin_size=1, color= \"magenta\")","3ea189a2":"bmi_cut = table_train.copy()\nbmi_cut[\"bmi\"] = pd.cut(table_train[\"bmi\"] , labels= [\"Underwaight\", \"Normal\", \"Overweight\", \"Obese\"], bins=[0, 18.5, 24.9,30, 100])","984073cc":"# Plotly bar char\ntemp_df = bmi_cut[[\"bmi\"]].value_counts().reset_index().sort_values(\"bmi\")\ntemp_df[\"bmi\"] = temp_df[\"bmi\"].astype(str)\ntemp_df.columns = [\"bmi\", \"count\"]\n\ntemp_df2 = bmi_cut[bmi_cut[\"diabetes_mellitus\"]==1][[\"bmi\"]].value_counts().reset_index().sort_values(\"bmi\")\ntemp_df2[\"bmi\"] = temp_df2[\"bmi\"].astype(str)\ntemp_df2.columns = [\"bmi\", \"count\"]\n\ntrace1 = go.Bar(\n                x = temp_df['bmi'],\n                y = temp_df['count'], name =\"Whole dataset\",\n                marker = dict(color = 'rgb(1,200,15)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=temp_df['count'], textposition='outside')\n\ntrace2 = go.Bar(\n                x = temp_df2['bmi'],\n                y = -temp_df2['count'], name =\"Diabetes Positive\",\n                marker = dict(color = 'rgb(200,19,15)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=temp_df2['count'], textposition='outside')\n\n\n\nlayout = go.Layout(template= \"plotly_dark\",title = 'BMI status vs Count' , xaxis = dict(title = 'BMI status'), yaxis = dict(title = 'Count'), height=650)\nfig = go.Figure(data = [trace1,trace2], layout = layout)\nfig.update_yaxes(showticklabels=False)\nfig.update_layout(barmode='overlay')\nfig.show()","1f3aaff1":"def pie_categ(df, list_graph):\n    temp = df[list_graph]\n    for col in list_graph:\n        temp[col] = temp[col].astype(str) + \"_\" + col.split(\"_\")[0]\n    fig = px.sunburst(temp,\n                      path=list_graph)\n\n    fig.update_layout(title_text='{} related with {}'.format(*list_graph),\n                      template= \"plotly_dark\" , )\n    fig.show()","00dd6526":"for col in featu_int:\n    if col == 'diabetes_mellitus':\n        continue\n    pie_categ(table_train,['diabetes_mellitus', col ] )","f8f66f85":"table_train[featu_float + [\"diabetes_mellitus\"]].corr(method = \"pearson\").style.background_gradient(cmap='Reds')","c988dae8":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.preprocessing import QuantileTransformer","436bfcc7":"train_model = table_train.drop(columns=[\"diabetes_mellitus\"]).copy()\nlabel_model = table_train[[\"diabetes_mellitus\"]].copy()\ntest_model  = table_test.copy()","8d51d3aa":"imp_simp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputer_int = SimpleImputer(missing_values=np.nan, strategy='mean')#KNNImputer(n_neighbors=2, weights=\"uniform\")\nimputer_float = SimpleImputer(missing_values=np.nan, strategy='mean')#KNNImputer(n_neighbors=2, weights=\"uniform\")\nfeatu_int_tr = featu_int.copy()\nfeatu_int_tr.remove(\"diabetes_mellitus\")\n# Train\ntrain_model[featu_float] = imputer_float.fit_transform(train_model[featu_float]) \ntrain_model[featu_int_tr] = imputer_int.fit_transform(train_model[featu_int_tr]).astype(int) \ntrain_model[featu_obj] = imp_simp.fit_transform(train_model[featu_obj])\n# Test\ntest_model[featu_float] = imputer_float.transform(test_model[featu_float]) \ntest_model[featu_int_tr] = imputer_int.transform(test_model[featu_int_tr]).astype(int) \ntest_model[featu_obj] = imp_simp.transform(test_model[featu_obj])    ","404bc7a0":"def add_new_features(data):\n    data = data.copy()\n\n    def create_features(x):\n        g_e = x['ethnicity']+'_'+x['gender']\n        a_bmi = x['age']*x['bmi']\n        a_glu = x['age']*x['d1_glucose_max']\n        bmi_glu = x['bmi']*x['d1_glucose_max']\n\n        return pd.Series([g_e,a_bmi,a_glu,bmi_glu ], index = ['gend_ethn','age_bmi','a_glu','bmi_glu' ])\n\n    return data.join(data.apply(create_features, axis=1))\n\ntrain_model_agg =  add_new_features(train_model)\ntest_model_agg  =  add_new_features(test_model)\n\ntrain_model_agg.head()","48ce1ec1":"featu_int=[]\nfeatu_float=[]\nfeatu_obj=[]\nfor col in train_model_agg.columns:\n    x=train_model_agg[col].dtype\n    if x=='int64' or x == 'int32':\n        train_model_agg[col] = train_model_agg[col].astype('int32')\n        featu_int.append(col)\n    elif x=='float64' or x== 'float32':\n        train_model_agg[col] = train_model_agg[col].astype('float32')\n        featu_float.append(col)\n    else:\n        featu_obj.append(col)","2d627a78":"import category_encoders as ce\ntarget_enc = ce.CatBoostEncoder(cols=featu_obj)\ntarget_enc.fit(train_model_agg[featu_obj], label_model)\n\n# Transform the features, rename columns with _cb suffix, and join to dataframe\ntrain_model_enc = train_model_agg.join(target_enc.transform(train_model_agg[featu_obj]).add_suffix('_cb'))\ntest_model_enc  = test_model_agg.join(target_enc.transform(test_model_agg[featu_obj]).add_suffix('_cb'))\n\ntrain_model_enc.head()","48a8a284":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit(train_model_agg[featu_obj])\n\ndef add_onehotencoder(df, columns, encoder):\n    df = df.join(pd.DataFrame(encoder.transform(df[columns]).toarray(), columns = encoder.get_feature_names(columns) ).add_suffix('_ohe'))\n    return df.drop(columns = columns)\n    \ntrain_model_enc = add_onehotencoder(train_model_enc, featu_obj, enc)\ntest_model_enc  = add_onehotencoder(test_model_enc, featu_obj, enc)\n\ntrain_model_enc.head()","5b82f483":"from imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN\nimport collections\n\ndef balancingClasses_Smote(x_train, y_train,random_state):\n\n    # Using SMOTE to to balance our training data points\n    sm = SMOTE(random_state=random_state)\n    features_balanced, target_balanced = sm.fit_resample(x_train, y_train)\n\n    print(\"Count for each class value after SMOTE:\", collections.Counter(target_balanced))\n\n    return features_balanced, target_balanced","0f96b894":"train_model_ball, label_model_ball = balancingClasses_Smote(train_model_enc, label_model, 30)","888b9ccc":"train_model_sc = train_model_ball.copy()\ntest_model_sc = test_model_enc.copy()\nqt = QuantileTransformer(n_quantiles=100, random_state=0,output_distribution = \"normal\")\n\ntrain_model_sc[featu_float] = qt.fit_transform(train_model_sc[featu_float])\ntest_model_sc[featu_float] = qt.transform(test_model_sc[featu_float])\n\ntrain_model_sc.head()","38e2a68f":"from xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV , RandomizedSearchCV\nfrom sklearn.metrics import roc_auc_score, classification_report\nX_train, X_test, y_train, y_test = train_test_split(train_model_sc, label_model_ball, train_size=0.8,stratify=label_model_ball)","d5a67f4b":"tuned_parameters_xgb = [{'tree_method': ['gpu_hist'],'booster':['gbtree'], 'predictor':['gpu_predictor'], 'learning_rate':np.linspace(.05, 1, 20),'min_child_weight':np.linspace(0, 1, 20), 'n_estimators':list(range(50, 300, 20)), # 'subsample':[0.1, 0.5, 0.8,], 'n_estimators':[100,150], 'min_child_weight':[0.1, 0.5, 1.0,],\n                    'objective':['binary:logistic'],'max_depth': list(range(1,20)), 'gamma': np.linspace(0,1.,20) }]\n\nscores = {'f1': 'f1_macro', 'roc':'roc_auc_ovr' , 'prec': 'precision'}     \n# GridSearchCV\nclf = RandomizedSearchCV(XGBClassifier(), tuned_parameters_xgb,\n                   scoring=scores, \n                   refit= \"f1\",\n                   cv=4,\n                   verbose=1,\n                   n_jobs=1,\n                   n_iter= 80,)\n\nclf.fit(X_train, y_train)\n# Validation\npred = clf.best_estimator_.predict_proba(X_test)\npred_bin = clf.best_estimator_.predict(X_test)\nroc = roc_auc_score(y_test, pred[:,1])\nprint(\"ROC test split = \", roc)\nprint(classification_report(y_test, pred_bin, digits=4 ))\nprint(clf.best_params_)","64f1ab38":"clf.cv_results_","fedaf8eb":"#params = clf.best_params_#{'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor', 'objective': 'binary:logistic', 'n_estimators': 230, 'min_child_weight': 0.7368421052631579, 'max_depth': 12, 'learning_rate': 0.05, 'gamma': 0.3157894736842105, 'booster': 'gbtree'} #clf.best_params_\nparams= {'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor', 'objective': 'binary:logistic', 'n_estimators': 230, 'min_child_weight': 0.7894736842105263, 'max_depth': 12, 'learning_rate': 0.05, 'gamma': 0.10526315789473684, 'booster': 'gbtree'}","ab2321d2":"model = XGBClassifier(**params)\nmodel.fit(train_model_sc, label_model_ball)","8339daa8":"y_pred     = model.predict_proba(test_model_sc)\n\nencounter_IDs = pd.read_csv(UnlabeledWiDS2021)[[\"encounter_id\"]].values\ndf_sub = {'encounter_id': encounter_IDs[:,0], 'diabetes_mellitus': y_pred[:,1]}\ndf_predictions = pd.DataFrame.from_dict(df_sub).set_index(['encounter_id'])\ndf_predictions.to_csv('Predictions_final.csv')\n\ndf_predictions.head(10)","ba2ebbcd":"import shap\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\",\n                  title = \"\")","619b6f62":"shap.summary_plot(shap_values, X_test)","ba48afdb":"### Encoding data\n\n#### **CatBoost Encoding**\nFinally, we'll look at CatBoost encoding. This is similar to target encoding in that it's based on the target probablity for a given value. However with CatBoost, for each row, the target probability is calculated only from the rows before it.\n","8360d1a2":"### Balancing Classes\n\nCheckout more on [this post.](https:\/\/towardsdatascience.com\/5-smote-techniques-for-oversampling-your-imbalance-data-b8155bdbe2b5)\n\n**SMOTE** or Synthetic Minority Oversampling Technique is an oversampling technique but SMOTE working differently than your typical oversampling.\n\nIn a classic oversampling technique, the minority data is duplicated from the minority data population. While it increases the number of data, it does not give any new information or variation to the machine learning model.\n\nFor a reason above, Nitesh Chawla, et al. (2002) introduce a new technique to create synthetic data for oversampling purposes in their [SMOTE paper](https:\/\/arxiv.org\/abs\/1106.1813).\nSMOTE works by utilizing a k-nearest neighbor algorithm to create synthetic data. SMOTE first start by choosing random data from the minority class, then k-nearest neighbors from the data are set. Synthetic data would then made between the random data and the randomly selected k-nearest neighbor. Let me show you the example below.\n![](https:\/\/miro.medium.com\/max\/347\/1*m0sr75BFm4C3kTbb84iWVQ.png)","8b7a57de":"<a id=\"ch5\"><\/a>\n# CHAPTER 5. Prepare data for consumption\n1. Drop useless columns, like \"Unamed: 0\"\n1. Drop unique identification, like names and ids. It's just noise.\n1. Drop columns with less than 2 unique values.\n1. Drop columns with hight rate of nan values","f4abf108":"<a id=\"ch4\"><\/a>\n# CHAPTER 4. Feature description","9c071fdd":"### BMI\nSo, what about BMI classification, we can review the recomendation from the [WHO](https:\/\/www.researchgate.net\/figure\/A-comparison-of-the-WHO-BMI-classifications-for-the-general-population-and-for-Asian_tbl1_233947928)\n\n![](https:\/\/www.researchgate.net\/profile\/John_Wilkinson5\/publication\/233947928\/figure\/tbl1\/AS:669665554595856@1536672118276\/A-comparison-of-the-WHO-BMI-classifications-for-the-general-population-and-for-Asian.png)","dde23f87":"<a id=\"ch3\"><\/a>\n# CHAPTER 3. Overview Data","53466377":"## Model Time","4aa7b86e":"<a id=\"ch6\"><\/a>\n# CHAPTER 6. Exploratory Data Analysis","4581d847":"### Scaling Data\n1. QuantileTransformer\n\nTransform features using quantiles information.\n\nThis method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme.\n\nThe transformation is applied on each feature independently. First an estimate of the cumulative distribution function of a feature is used to map the original values to a uniform distribution. The obtained values are then mapped to the desired output distribution using the associated quantile function. Features values of new\/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.","45ded588":"## Ethnicity Distribution\n\nThis fast visualization, show us that African Americans have the largerst number of diabete positives cases.","8b8da1b8":"## Exploring Binary Features - int32\n\nfeatu_int","6a7cafa9":"<h1 style=\"color:RED; text-align:left; font-family: 'Oswald', sans-serif \"> ABOUT: <\/h1>\n\n<h3 style=\"text-align:Left\">Hi everyone, I want to share with you all my data exploration, analysis and model selection about this topic, I\u2019m an engineer but the medical topic and research over diseases has always been an interesting topic for me. So, I hope that you enjoy my work and find this useful for your data science journey.\n<\/h3>\n\n\n<h2 style=\" color:GREEN; text-align:center; font-family:'Lobster',\">Let's do it!<\/h2>","34713e9f":"### Fillnan Values","8b382a3d":"### Differentiate between Numerical and Categorical Data.\n\nOn this step I going to classify features on int, float and objects.\n","14707bab":"## Final Training\nAfter run and succesfuly finished the RandomSearch, we need to make the last training with the whole data set, using the best hyperparameters.","983d6f16":"## Icu stay type","1ab6488d":"<a id=\"ch2\"><\/a>\n# CHAPTER 2. Data Science Pipeline\n\n1. **Define the Problem:** We need to predict if a patient have diabetes millitus, based on his features like patient. We face a binary classification.\n\n2. **Gather the Data:** The data provides us is structured data, features like columns. where every row represents a patient. Also is labeled data, i meant we know is a patient have diabetes millitus. Hence we can accomplish the problem like a [supervised learning](https:\/\/machinelearningmastery.com\/supervised-and-unsupervised-machine-learning-algorithms\/).\n\n3. **Prepare Data for Consumption:** This step could overlap exploratory data analysis EDA, while our exploration we'll need to clean, transform, reformat or scale our dataset, all this for extract useful information to clarify our problem and have healthy data, to avoid missing or outlier data points.\n\n4. **Perform Exploratory Analysis:** Without analysis our work could be useless, because \"garbage-in, garbage-out (GIGO)\". This is the step when we going to create our personal perspective of the problem but trying really hard to be objective, sometimes we have to use statistics tools to validate some hypothesis. All this analysis will be support by graphics aims to looking patterns, comparations and correlations. Also, we going to need to differentiate categorical vs numerical features, and balance of those. These aspects of the data are important to create the best hypothesis for our data model.\n\n5. **Model Data:** The data model that we'll chose determine the algorithms available for use. The algorithm is just a fancy tool completely useless in the wrong context. That\u2019s because data modeling came after EDA, first you understand the data, then you model it.\n\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](https:\/\/machinelearningmastery.com\/overfitting-and-underfitting-with-machine-learning-algorithms\/).\n\n7. **Optimize and Strategize:** After all these steps, we'll need to repeat it, creating different hypothesis and models, optimize our process aims to make better predictions.","996363fe":"<head>\n\n<link rel=\"preconnect\" href=\"https:\/\/fonts.gstatic.com\">\n<link href=\"https:\/\/fonts.googleapis.com\/css2?family=Roboto&display=swap\" rel=\"stylesheet\">\n\n<\/head>\n<body>\n    \n<h1 style=\"color:GREEN; font-family: Roboto, sans-serif; text-align:center; font-size:30px\">WIDS Datathon 2021 \ud83d\udcda\ud83d\udcac<\/h1>\n    \n<\/body>\n","9bf0ef37":"## Test file overview","5d3a648d":"### Paths \ud83d\udcda","0ecaf372":"### OneHotEncoder (Review this - maybe is useless)\nEncode categorical features as a one-hot numeric array.\n\nThe input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array (depending on the sparse parameter)","57899b23":"## 6.1. CATEGORICAL FEATURES","f3eb04d8":"## 6.2. NUMERICAL FEATURES\n\nWhile I was creating these chars, I\u2019ve realized that all these numerical features going to need a rescale o preprocessing to create a good model and estimator.\n\nBut for now, please enjoy the native behavior for every numerical feature.\n","68c6f574":"## Diabetes Status by Genders\n\nWe going to visualize the amount of positive and negative cases of diabetes for women and men.\n1. We have unbalance data. So, we'll use a method to up sample the minority class, on the next chapter.\n","54fdcb74":"<center>\n    <img src= \"https:\/\/res.cloudinary.com\/grohealth\/image\/upload\/c_fill,f_auto,fl_lossy,h_650,q_auto,w_1085\/v1581695681\/DCUK\/Content\/causes-of-diabetes.png\" width=\"800\">\n<\/center>","993ed341":"### Updating feature list","484ce668":"### Training - Model Optimization\n\nThis step will help us to chose the best hyperparams for our model.\nBe awere that this process is time computing expensive. so, take a rest and do anything else.\n\n#### **RandomizedSearchCV**\n\nRandomized search on hyper parameters.\n\nRandomizedSearchCV implements a \u201cfit\u201d and a \u201cscore\u201d method. It also implements \u201cscore_samples\u201d, \u201cpredict\u201d, \u201cpredict_proba\u201d, \u201cdecision_function\u201d, \u201ctransform\u201d and \u201cinverse_transform\u201d if they are implemented in the estimator used.\n\nThe parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.\n\nIn contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.\n\nLet's look [xgboost classifier](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.sklearn)","ad70156f":"### File descriptions\n* **TrainingWiDS2021.csv** - the training data. You should see 130,157 encounters represented here. Please view the Data Dictionary file for more information about the columns.\n\n* **UnlabeledWiDS2021.csv**- the unlabeled data (data without diabetes_mellitus provided). You are being asked to predict the diabetes_mellitus variable for these encounters.\n\n* **SampleSubmissionWiDS2021.csv** - a sample submission file in the correct format.\n\n* **SolutionTemplateWiDS2021.csv** - a list of all the rows (and encounters) that should be in your submissions.\n\n* **DataDictionaryWiDS2021.csv** - supplemental information about the data.","6501fd70":"### Drop columns with less than 2 unique values","e6126bde":"<a id=\"ch1\"><\/a>\n# CHAPTER 1. What is the problem?\n\nWomen in data science challenge, is about to predict if a patient have diabetes or not. Every patient have a set of features, around 180 features and we have to use it to made our predictions over the data set. Every problem is unique, so in that way we need to analice and vizualice the features and relations.","dc55ebb9":"### Open files","66d3d1bb":"## Hospital admit source","26846461":"## Train file overview\n\n1. Description table shows us a simple distribution behavior for every feature of the data.\n2. Missing rate values present on our dataset.\n3. Sample train table, only five rows.\n","5e74098f":"## Exploring Float Features - float32\n\nBecause we have too many float features, result more combinient create a correlation matriz for all of those.\n\nfeatu_float","c259c811":"<a id=\"ch7\"><\/a>\n# CHAPTER 7. Model Data","6bd4913e":"## Icu type","cb91e819":"# Table of Contents\n1. [Chapter 1 - What is the problem?](#ch1)\n1. [Chapter 2 - Data Science Pipeline](#ch2)\n1. [Chapter 3 - Overview data](#ch3)\n1. [Chapter 4 - Feature description](#ch4)\n1. [Chapter 5 - Prepare data for consumption](#ch5)\n1. [Chapter 6 - Exploratory Data Analysis](#ch6)\n1. [Chapter 7 - Model Data](#ch7)\n1. [Chapter 8 - What did we find about diabetes?](#ch8)\n1. [Submission File](#ch99)","8f70cb82":"### Feature Generation\n\nOf course, i've created  more features, i'd considered only 5 original features for this step.\n1. Ethnicity\n2. gender\n3. bmi\n4. d1_glucosa_max\n5. age\n\nMixin this features a create 4 additonal features.\n\n        g_e = x['ethnicity']+'_'+x['gender']\n        a_bmi = x['age']*x['bmi']\n        a_glu = x['age']*x['d1_glucose_max']\n        bmi_glu = x['bmi']*x['d1_glucose_max']","0184ecd1":"### Drop columns with hight rate of nan values > 0.3","15f2a59e":"## Diabetes status by Age and Gender\nAge distribution for men and women, counting only patients with diabetes. \n1. We can see that most people diagnosed with diabetes are between 60 and 70 years old.\n2. For positive cases, men have the largest number of cases.\n3. There is one outlier value, with age 0\n","6e28f5b7":"<a id=\"ch99\"><\/a>\n\n## Submission File\n\nWell folks, this is it. Is this was useful for you, plesase give an upvote and let me a comment about how to improve this notebook. Thanks","56db7ba1":"## Feature importance\n\nLet's see what can tell us our model. What features are relevant to predict if a person have or not diabetes melllitus.\n\nThis step will be implemented using SHAP, who use game theory for extract information about our model, we could\u2019ve used the feature importance calculations that come with XGBoost, but it\u2019s not accurate at all or even contradictory.\n\nIf you want to see more about SHAP, please [check the link. ](https:\/\/shap.readthedocs.io\/en\/latest\/example_notebooks\/tabular_examples\/tree_based_models\/Census%20income%20classification%20with%20XGBoost.html)","590fedbe":"<a id=\"ch8\"><\/a>\n\n# Chapter 8 - What did we find about diabetes"}}