{"cell_type":{"c22cdd78":"code","eea18e36":"code","572e6a5c":"code","7a0af749":"code","fc8b8029":"code","9cf62817":"code","291d5db9":"code","03233e12":"code","d0687f12":"code","fed467ec":"code","9b465857":"code","53c381d7":"code","0063db16":"code","e0c8489c":"code","420cce4b":"code","c5551ddd":"code","fea10412":"code","acf09e5b":"code","de0a48c1":"markdown","414e5570":"markdown","0bed5d95":"markdown","c161a6d7":"markdown","21c090bf":"markdown","06447e19":"markdown","20d0bdc6":"markdown","54e869e7":"markdown","cafacea1":"markdown","d78aa9ba":"markdown"},"source":{"c22cdd78":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Dropout, Flatten ,BatchNormalization , MaxPool2D\nfrom keras.layers.convolutional import Conv2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nimport os","eea18e36":"train=pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest=pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","572e6a5c":"train.columns","7a0af749":"train['label'][1:10]","fc8b8029":"train.shape","9cf62817":"x=train.drop(['label'],axis=1)\ny1=y=train['label']\ny = to_categorical(y)","291d5db9":"x.head()","03233e12":"sns.countplot(y1)","d0687f12":"x=x\/255.0\ntest=test\/255.0\n","fed467ec":"x = np.array(x).reshape(-1,28,28,1)\ntest = np.array(test).reshape(-1,28,28,1)\nx.shape","9b465857":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state=4)\nx_train.shape,y_train.shape,x_test.shape,y_test.shape","53c381d7":"fig, axis = plt.subplots(10, 10, figsize=(20, 20))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(x_train[i].reshape(28,28))\n    ax.axis('off')\n    ax.set(title = f\"Real Number is {y_train[i].argmax()}\")","0063db16":"model = Sequential()\n#First\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3) ,activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 56, kernel_size = (3,3),activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\n#Second\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),activation ='relu'))\nmodel.add(Conv2D(filters = 48, kernel_size = (3,3),activation ='relu'))\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3),activation ='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\n#Third\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dense(128, activation = \"relu\"))\nmodel.add(Dense(64, activation = \"relu\"))\nmodel.add(Dropout(0.4))\n\n#Output\nmodel.add(Dense(10, activation = \"softmax\"))\n\n\nmodel.compile(Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n","e0c8489c":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)","420cce4b":"history = model.fit_generator(datagen.flow(x_train,y_train, batch_size=56),\n                              epochs = 10, validation_data = (x_test,y_test),\n                              verbose = 2, steps_per_epoch=660)\n","c5551ddd":"plt.figure()\nfig,(ax1, ax2)=plt.subplots(1,2,figsize=(19,7))\nax1.plot(history.history['loss'])\nax1.plot(history.history['val_loss'])\nax1.legend(['training','validation'])\nax1.set_title('loss')\nax1.set_xlabel('epoch')\n\nax2.plot(history.history['accuracy'])\nax2.plot(history.history['val_accuracy'])\nax2.legend(['training','validation'])\nax2.set_title('Acurracy')\nax2.set_xlabel('epoch')\n\n\n\nscore =model.evaluate(x_test,y_test,verbose=0)\nprint('Test Score:',score[0])\nprint('Test Accuracy:',score[1])","fea10412":"y_pred = model.predict(x_test)\nX_test__ = x_test.reshape(x_test.shape[0], 28, 28)\n\nfig, axis = plt.subplots(5, 5, figsize=(20, 20))\nfor i, ax in enumerate(axis.flat):\n    ax.imshow(X_test__[i])\n    ax.axis('off')\n    ax.set(title = f\"Real Number is {y_test[i].argmax()}\\nPredict Number is {y_pred[i].argmax()}\");\n","acf09e5b":"results = model.predict(test)\n\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"CNN_Digit_Recognizer.csv\",index=False)","de0a48c1":"# NORMILIZATION","414e5570":"If you like my work, please hit upvote ","0bed5d95":"# COUNTPLOT ON LABLE DATA","c161a6d7":"> # CNN(Convolutional Neural Networks)","21c090bf":"* size of image will be height * width * 3(R:red,G:green,B:blue)\n*  cnn is feed-forward artifical neutral net work it has three layer namely mentioned below\n*      1 convlution layer\n*      2 relu layer\n*      3 pooling layer\n*      4 fully connected\n* Now in a traditional convolutional neural network architecture, there are other layers that are interspersed between these conv layers. I\u2019d strongly encourage those interested to read up on them and understand their function and effects, but in a general sense, they provide nonlinearities and preservation of dimension that help to improve the robustness of the network and control overfitting. A classic CNN architecture would look like this.The last layer, however, is an important one and one that we will go into later on. Let\u2019s just take a step back and review what we\u2019ve learned so far. We talked about what the filters in the first conv layer are designed to detect. They detect low level features such as edges and curves. As one would imagine, in order to predict whether an image is a type of object, we need the network to be able to recognize higher level features such as hands or paws or ears. So let\u2019s think about what the output of the network is after the first conv layer. It would be a 28 x 28 x 3 volume (assuming we use three 5 x 5 x 3 filters).  When we go through another conv layer, the output of the first conv layer becomes the input of the 2nd conv layer.  Now, this is a little bit harder to visualize. When we were talking about the first layer, the input was just the original image. However, when we\u2019re talking about the 2nd conv layer, the input is the activation map(s) that result from the first layer. So each layer of the input is basically describing the locations in the original image for where certain low level features appear. Now when you apply a set of filters on top of that (pass it through the 2nd conv layer), the output will be activations that represent higher level features. Types of these features could be semicircles (combination of a curve and straight edge) or squares (combination of several straight edges). As you go through the network and go through more conv layers, you get activation maps that represent more and more complex features. By the end of the network, you may have some filters that activate when there is handwriting in the image, filters that activate when they see pink objects, etc. If you want more information about visualizing filters in ConvNets, Matt Zeiler and Rob Fergus had an excellent research paper discussing the topic. Jason Yosinski also has a video on YouTube that provides a great visual representation. Another interesting thing to note is that as you go deeper into the network, the filters begin to have a larger and larger receptive field, which means that they are able to consider information from a larger area of the original input volume (another way of putting it is that they are more responsive to a larger region of pixel space).\nNow that we can detect these high level features, the icing on the cake is attaching a fully connected layer to the end of the network. This layer basically takes an input volume (whatever the output is of the conv or ReLU or pool layer preceding it) and outputs an N dimensional vector where N is the number of classes that the program has to choose from. For example, if you wanted a digit classification program, N would be 10 since there are 10 digits. Each number in this N dimensional vector represents the probability of a certain class. For example, if the resulting vector for a digit classification program is [0 .1 .1 .75 0 0 0 0 0 .05], then this represents a 10% probability that the image is a 1, a 10% probability that the image is a 2, a 75% probability that the image is a 3, and a 5% probability that the image is a 9 (Side note: There are other ways that you can represent the output, but I am just showing the softmax approach). The way this fully connected layer works is that it looks at the output of the previous layer (which as we remember should represent the activation maps of high level features) and determines which features most correlate to a particular class. For example, if the program is predicting that some image is a dog, it will have high values in the activation maps that represent high level features like a paw or 4 legs, etc. Similarly, if the program is predicting that some image is a bird, it will have high values in the activation maps that represent high level features like wings or a beak, etc. Basically, a FC layer looks at what high level features most strongly correlate to a particular class and has particular weights so that when you compute the products between the weights and the previous layer, you get the correct probabilities for the different classes.\n*","06447e19":"# Reshaping\n","20d0bdc6":"* image in 3 dimensions (height = 28px, width = 28px , canal = 1)","54e869e7":"vil","cafacea1":"# Prediction validation results\n","d78aa9ba":"* WE KONW THAT MAX PIXEL VALUE WILL BE 255.0 ON THAT NOTE WE CAN DIVIDED EACH PIXEL WITH 255.0 TO GET NORMALIZATION"}}