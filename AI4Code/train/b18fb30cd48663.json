{"cell_type":{"a397c95c":"code","9fbf6d07":"code","cb503c77":"code","ddb0a53a":"code","4949b555":"code","d0b3a3b7":"code","10b33ced":"code","b8c08ef7":"code","1a939371":"code","c98e5798":"code","177c007a":"code","08c0d140":"code","56e80bc8":"code","53803c2f":"code","0d46d65b":"code","929162a5":"code","279632d3":"code","b5d3eb03":"code","d8ed801e":"code","dbfcdb49":"code","6d0c28dc":"code","0bfd53cc":"code","140466bd":"code","9373a888":"code","73124c3f":"code","86dd6d97":"code","0c5ef878":"code","406b95e1":"code","d904bd9a":"code","504a274d":"code","5cb5043e":"code","9ee5a80f":"code","8c2e4810":"code","cbc7ffc1":"code","7a35bd17":"code","5b0882e0":"code","fe007a3e":"code","08d11e49":"code","af1651f0":"code","58af49c5":"code","0ac5e592":"code","833fb51c":"code","55f20d65":"code","4e9a421d":"code","4f7ab7aa":"code","53f0b9a6":"code","087dd9e8":"code","7cea6bba":"code","ef31d30a":"code","75d4b71c":"code","45924181":"code","ea2e8b71":"code","794d766e":"code","4230f147":"code","f2290fbc":"code","8e7f18d1":"code","42bbc996":"code","a21e88d9":"code","adcf7992":"code","769966a1":"code","3971c552":"code","b9a6d319":"code","5a064103":"code","eb11e71b":"code","26c59735":"code","cbba6873":"code","82e52427":"code","c48203e9":"code","f67e18fb":"code","c03f32c8":"code","246f396f":"code","f9ac2896":"code","861c9876":"code","e3f2f354":"code","77489ee1":"code","18584aee":"code","6e3f34ef":"code","d1c38f70":"code","70ae1f94":"code","662573c0":"code","d4357344":"code","fe378f24":"code","b4bf01f6":"code","068739ac":"code","3543b188":"code","89c90dd5":"code","3669a39f":"code","954bad72":"code","70b1eee8":"code","77e91b13":"code","c004882b":"code","8a65486f":"code","a6dafb46":"code","ad079509":"code","48fd757a":"code","17331f3c":"code","eaddd054":"code","77ddb919":"code","700657a5":"code","979575b5":"code","822aa190":"code","6babb28d":"code","b3a44883":"code","b429c505":"code","9a009c44":"code","5ffe614b":"code","7b002bae":"code","0b6974e1":"code","7404a669":"code","ea63b779":"code","4b0bbd28":"code","67dd9711":"code","c9e9ab3c":"code","89a3fa20":"code","594ea149":"code","5cb25c97":"code","7a10b787":"code","0f0bc306":"code","314e9af1":"code","fe5bcd6d":"code","09abde4a":"code","575b225e":"code","c5cc37b3":"code","d99e192a":"code","282223ea":"code","a16d6c48":"code","0186677a":"code","dab564a8":"code","c59e5b40":"code","915e05a7":"code","31294976":"code","0d99003a":"code","4267b059":"code","f7944841":"code","c18a9073":"code","49a9d95a":"code","c0ae5526":"code","41df1ab5":"code","6bd201a7":"code","cc1d2d6d":"code","2bbe4a07":"code","893eb0e6":"markdown","507d36b1":"markdown","7bd1d197":"markdown","fe5549c6":"markdown","a0a68033":"markdown","2437dd3c":"markdown","e496dba5":"markdown","15ad00af":"markdown","3c41faa9":"markdown","fdc7db0a":"markdown","9763ff8d":"markdown","818056c4":"markdown","db7c3901":"markdown","108fa400":"markdown","8b041b7c":"markdown","a3a834c1":"markdown","6e1aa7e8":"markdown","ece7136f":"markdown","1d5e85c7":"markdown","37e5c6f8":"markdown","d0d07968":"markdown","a2180aec":"markdown","667445e2":"markdown","e972953a":"markdown","bb064ea1":"markdown","42ef741f":"markdown","b8327a5e":"markdown","7d695443":"markdown","36443324":"markdown","703c2801":"markdown","5c3a576d":"markdown","fa729329":"markdown","344d4f9d":"markdown","04b6b4b6":"markdown","cce2a234":"markdown","7c4d3818":"markdown","00871155":"markdown","271da447":"markdown","9a7474fb":"markdown","9bea5f45":"markdown","c188a897":"markdown","282f849e":"markdown","9076663f":"markdown","7de350ea":"markdown","d21bc2d4":"markdown","fc514ed6":"markdown","8f5a9294":"markdown","252ba73d":"markdown","daf7a8ed":"markdown","49c2747b":"markdown","c3c4a606":"markdown","336ddc9d":"markdown","312e6ce0":"markdown","4fc3197c":"markdown","b43b6b62":"markdown","ae7479f9":"markdown"},"source":{"a397c95c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n# Standard Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom IPython import display\nfrom ipywidgets import interact , widgets\n\n\nimport scipy.stats\nimport scipy.optimize\nimport scipy.spatial\n\nimport re\nimport mailbox\nimport csv\nimport math\nimport io","9fbf6d07":"# 2008 US swing state election results\ndf_swing=pd.read_csv('..\/input\/2008_swing_states.csv')\ndf_all_states=pd.read_csv('..\/input\/2008_all_states.csv')\ndf_swing[['state','county','dem_share']].head()","cb503c77":"_ = plt.hist(df_swing['dem_share'])\n# df_swing['dem_share'].plot.hist() # Same result","ddb0a53a":"_ = plt.hist(df_swing['dem_share'],bins=20)","4949b555":"# All data is shown in beeswarmplot , but not good for large data.\n_ = sns.swarmplot(x='state',y='dem_share',data=df_swing)","d0b3a3b7":"def ecdf(data):\n    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n    # Number of data points: n\n    n = len(data)\n\n    # x-data for the ECDF: x\n    x = np.sort(data)\n\n    # y-data for the ECDF: y\n    y = np.arange(1, n+1) \/ n\n\n    return x, y","10b33ced":"# Making an ECDF\nx=np.sort(df_swing['dem_share'])\ny=np.arange(1,len(x)+1)\/len(x)\n\n_=plt.plot(x,y,marker='.',linestyle='none')","b8c08ef7":"# Comparison of ECDFs\n# ECDFs also allow you to compare two or more distributions (though plots get cluttered if you have too many). \n\n","1a939371":"#pa_dem_share,oh_dem_share,fl_dem_share\npa_dem_share=df_swing.query(\"state=='PA'\")['dem_share']\noh_dem_share=df_swing.query(\"state=='OH'\")['dem_share']\nfl_dem_share=df_swing.query(\"state=='FL'\")['dem_share']\n\n\nx_pa,y_pa=ecdf(pa_dem_share)\nx_oh,y_oh=ecdf(oh_dem_share)\nx_fl,y_fl=ecdf(fl_dem_share)\n\n_=plt.plot(x_pa,y_pa,marker='.',linestyle='none')\n_=plt.plot(x_oh,y_oh,marker='.',linestyle='none')\n_=plt.plot(x_fl,y_fl,marker='.',linestyle='none')\nplt.legend(('PA','OH','FL'))\nplt.show()\n","c98e5798":"# Computing means\n# The mean of all measurements gives an indication of the typical magnitude of a measurement. It is computed using np.mean().\n# Mean is affected by outliers but not median.\n\n","177c007a":"# Percentiles, outliers, and box plots\n# Specify array of percentiles: percentiles\npercentiles=np.array([2.5,25,50,75,97.5])\n\n# Compute percentiles: ptiles_vers\nptiles_vers=np.percentile(df_swing['dem_share'],percentiles)\n","08c0d140":"# Plot the ECDF\n_ = plt.plot(x, y, '.')\n_ = plt.xlabel('Dem Share')\n_ = plt.ylabel('ECDF')\n\n# Overlay percentiles as red diamonds.\n_ = plt.plot(ptiles_vers, percentiles\/100, marker='D', color='red',\n         linestyle='none')\n\n# Show the plot\nplt.show()\n","56e80bc8":"_ = sns.boxplot(x='east_west', y='dem_share', data=df_all_states)\n_ = plt.xlabel('region')\n_ = plt.ylabel('percent of vote for Obama')","53803c2f":"print(np.var(x_fl))\nprint(np.sqrt(np.var(x_fl)))\nprint(np.std(x_fl))\n","0d46d65b":"total_votes=df_swing.total_votes\/1000\ndem_share=df_swing.dem_share\n_ = plt.plot(total_votes, dem_share, marker='.', linestyle='none')\n_ = plt.xlabel('total votes (thousands)')\n_ = plt.ylabel('percent of vote for Obama')\n\n# Indeed, we see some correlation","929162a5":"# Compute the covariance matrix: covariance_matrix\ncovariance_matrix=np.cov(total_votes,dem_share)\n\n# Print covariance matrix\nprint(covariance_matrix)\n","279632d3":"def pearson_r(x, y):\n    \"\"\"Compute Pearson correlation coefficient between two arrays.\"\"\"\n    # Compute correlation matrix: corr_mat\n    corr_mat=np.corrcoef(x,y)\n\n    # Return entry [0,1]\n    return corr_mat[0,1]\n\n# Compute Pearson correlation coefficient \nr=pearson_r(total_votes,dem_share)\n\n# Print the result\nprint(r)","b5d3eb03":"# we'll generate lots of random numbers between zero and one, and then plot a histogram of the results.\n# If the numbers are truly random, all bars in the histogram should be of (close to) equal height.\n\nrandom_numbers=np.random.random(1000)\n_ = plt.hist(random_numbers)\n\n\n# The histogram is almost exactly flat across the top,indicating that \n# there is equal chance that a randomly-generated number is in any of the bins of the histogram.","d8ed801e":"def perform_bernoulli_trials(n, p):\n    \"\"\"Perform n Bernoulli trials with success probability p\n    and return number of successes.\"\"\"\n    # Initialize number of successes: n_success\n    n_success = 0\n\n    # Perform trials\n    for i in range(n):\n        # Choose random number between zero and one: random_number\n        random_number=np.random.random()\n\n        # If less than p, it's a success so add one to n_success\n        if random_number < p:\n            n_success+=1\n\n    return n_success","dbfcdb49":"# Seed random number generator\nnp.random.seed(42)\n\n# Initialize the number of defaults: n_defaults\nn_defaults=np.empty(1000)\n\n# Compute the number of defaults\nfor i in range(1000):\n    n_defaults[i] = perform_bernoulli_trials(100,0.05)\n\n\n# Plot the histogram with default number of bins; label your axes\n_ = plt.hist(n_defaults, density=True)\n_ = plt.xlabel('number of defaults out of 100 loans')\n_ = plt.ylabel('probability')\n\n# Show the plot\nplt.show()\n\n# This is actually not an optimal way to plot a histogram when the results are known to be integers.","6d0c28dc":"# Compute ECDF: x, y\nx,y=ecdf(n_defaults)\n\n# Plot the ECDF with labeled axes\nplt.plot(x,y,marker='.',linestyle='none')\n\n\nplt.xlabel('number of defaults out of 100')\nplt.ylabel('ECDF')\n# Show the plot\nplt.show()\n\n# Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money\nn_lose_money=np.sum(n_defaults >=10)\n\n# Compute and print probability of losing money\nprint('Probability of losing money =', n_lose_money \/ len(n_defaults))\n\n# As we might expect, we most likely get 5\/100 defaults.But we still have about a 2% chance of getting 10 or more defaults out of 100 loans.","0bfd53cc":"# Take 10,000 samples out of the binomial distribution: n_defaults\nn_defaults=np.random.binomial(n=100,p=0.05,size=10000)\n\n# Compute CDF: x, y\nx,y=ecdf(n_defaults)\n\n# Plot the CDF with axis labels\nplt.plot(x,y,marker='.',linestyle='none')\nplt.xlabel('number of defaults out of 100 loans')\nplt.ylabel('CDF')\n\n\n\n# Show the plot\nplt.show()\n","140466bd":"# Compute bin edges: bins\nbins = np.arange(max(n_defaults) + 1.5) - 0.5\n\n# Generate histogram\nplt.hist(n_defaults,density=True,bins=bins)\n\n# Label axes\nplt.xlabel('number of defaults out of 100 loans')\nplt.ylabel('PMF')\n\n\n# Show the plot\nplt.show()\n","9373a888":"'''\nThe means are all about the same, which can be shown to be true by doing some pen-and-paper work.\nThe standard deviation of the Binomial distribution gets closer and closer to that of the Poisson distribution\nas the probability p gets lower and lower.\n\n'''\n\n# Draw 10,000 samples out of Poisson distribution: samples_poisson\nsamples_poisson=np.random.poisson(10,size=10000)\n\n# Print the mean and standard deviation\nprint('Poisson:     ', np.mean(samples_poisson),\n                       np.std(samples_poisson))\n\n# Specify values of n and p to consider for Binomial: n, p\nn = [20, 100, 1000]\np = [0.5, 0.1, 0.01]\n\n\n# Draw 10,000 samples for each n,p pair: samples_binomial\nfor i in range(3):\n    samples_binomial = np.random.binomial(n[i],p[i],size=10000)\n\n    # Print results\n    print('n =', n[i], 'Binom:', np.mean(samples_binomial),\n                                 np.std(samples_binomial))\n","73124c3f":"'''\nWe can see how the different standard deviations result in PDFs of different widths. The peaks are all centered at the mean of 20.\n'''\n# Draw 100000 samples from Normal distribution with stds of interest: samples_std1, samples_std3, samples_std10\nsamples_std1=np.random.normal(20,1,size=100000)\nsamples_std3=np.random.normal(20,3,size=100000)\nsamples_std10=np.random.normal(20,10,size=100000)\n\n# Make histograms\n_=plt.hist(samples_std1,density=True,histtype='step',bins=100)\n_=plt.hist(samples_std3,density=True,histtype='step',bins=100)\n_=plt.hist(samples_std10,density=True,histtype='step',bins=100)\n\n# Make a legend, set limits and show plot\n_ = plt.legend(('std = 1', 'std = 3', 'std = 10'))\nplt.ylim(-0.01, 0.42)\nplt.show()\n","86dd6d97":"'''\nThe CDFs all pass through the mean at the 50th percentile; the mean and median of a Normal distribution are equal.\nThe width of the CDF varies with the standard deviation.\n\n'''\n# Generate CDFs\nx_std1,y_std1=ecdf(samples_std1)\nx_std3,y_std3=ecdf(samples_std3)\nx_std10,y_std10=ecdf(samples_std10)\n\n\n# Plot CDFs\n_=plt.plot(x_std1,y_std1,marker='.',linestyle='none')\n_=plt.plot(x_std3,y_std3,marker='.',linestyle='none')\n_=plt.plot(x_std10,y_std10,marker='.',linestyle='none')\n\n\n# Make a legend and show the plot\n_ = plt.legend(('std = 1', 'std = 3', 'std = 10'), loc='lower right')\nplt.show()\n","0c5ef878":"nohitter_times=[ 843, 1613, 1101,  215,  684,  814,  278,  324,  161,  219,  545,\n        715,  966,  624,   29,  450,  107,   20,   91, 1325,  124, 1468,\n        104, 1309,  429,   62, 1878, 1104,  123,  251,   93,  188,  983,\n        166,   96,  702,   23,  524,   26,  299,   59,   39,   12,    2,\n        308, 1114,  813,  887,  645, 2088,   42, 2090,   11,  886, 1665,\n       1084, 2900, 2432,  750, 4021, 1070, 1765, 1322,   26,  548, 1525,\n         77, 2181, 2752,  127, 2147,  211,   41, 1575,  151,  479,  697,\n        557, 2267,  542,  392,   73,  603,  233,  255,  528,  397, 1529,\n       1023, 1194,  462,  583,   37,  943,  996,  480, 1497,  717,  224,\n        219, 1531,  498,   44,  288,  267,  600,   52,  269, 1086,  386,\n        176, 2199,  216,   54,  675, 1243,  463,  650,  171,  327,  110,\n        774,  509,    8,  197,  136,   12, 1124,   64,  380,  811,  232,\n        192,  731,  715,  226,  605,  539, 1491,  323,  240,  179,  702,\n        156,   82, 1397,  354,  778,  603, 1001,  385,  986,  203,  149,\n        576,  445,  180, 1403,  252,  675, 1351, 2983, 1568,   45,  899,\n       3260, 1025,   31,  100, 2055, 4043,   79,  238, 3931, 2351,  595,\n        110,  215,    0,  563,  206,  660,  242,  577,  179,  157,  192,\n        192, 1848,  792, 1693,   55,  388,  225, 1134, 1172, 1555,   31,\n       1582, 1044,  378, 1687, 2915,  280,  765, 2819,  511, 1521,  745,\n       2491,  580, 2072, 6450,  578,  745, 1075, 1103, 1549, 1520,  138,\n       1202,  296,  277,  351,  391,  950,  459,   62, 1056, 1128,  139,\n        420,   87,   71,  814,  603, 1349,  162, 1027,  783,  326,  101,\n        876,  381,  905,  156,  419,  239,  119,  129,  467]","406b95e1":"# Seed random number generator\nnp.random.seed(42)\n\n'''\nWe see the typical shape of the Exponential distribution, going from a maximum at 0 and decaying to the right.\n'''\n# Compute mean no-hitter time: tau\ntau = np.mean(nohitter_times)\n\n# Draw out of an exponential distribution with parameter tau: inter_nohitter_time\ninter_nohitter_time = np.random.exponential(tau, 100000)\n\n# Plot the PDF and label axes\n_ = plt.hist(inter_nohitter_time,\n             bins=50, density=True,histtype='step')\n_ = plt.xlabel('Games between no-hitters')\n_ = plt.ylabel('PDF')\n\n# Show the plot\nplt.show()","d904bd9a":"'''\nIt looks like no-hitters in the modern era of Major League Baseball are Exponentially distributed.\nBased on the story of the Exponential distribution, this suggests that they are a random process; \nwhen a no-hitter will happen is independent of when the last no-hitter was.\n'''\n\n# Create an ECDF from real data: x, y\nx, y = ecdf(nohitter_times)\n\n# Create a CDF from theoretical samples: x_theor, y_theor\nx_theor, y_theor = ecdf(inter_nohitter_time)\n\n# Overlay the plots\nplt.plot(x_theor, y_theor)\nplt.plot(x, y, marker='.', linestyle='none')\n\n# Margins and axis labels\nplt.margins(.02)\nplt.xlabel('Games between no-hitters')\nplt.ylabel('CDF')\n\n# Show the plot\nplt.show()\n","504a274d":"'''\nNotice how the value of tau given by the mean matches the data best. In this way, tau is an optimal parameter.\n'''\n# Plot the theoretical CDFs\nplt.plot(x_theor, y_theor)\nplt.plot(x, y, marker='.', linestyle='none')\nplt.margins(0.02)\nplt.xlabel('Games between no-hitters')\nplt.ylabel('CDF')\n\n# Take samples with half tau: samples_half\nsamples_half = np.random.exponential(tau\/2,10000)\n\n# Take samples with double tau: samples_double\nsamples_double = np.random.exponential(tau*2,10000)\n\n# Generate CDFs from these samples\nx_half, y_half = ecdf(samples_half)\nx_double, y_double = ecdf(samples_double)\n\n# Plot these CDFs as lines\n_ = plt.plot(x_half, y_half)\n_ = plt.plot(x_double, y_double)\n\n# Show the plot\nplt.show()","5cb5043e":"illiteracy=np.asarray([ 9.5, 49.2,  1. , 11.2,  9.8, 60. , 50.2, 51.2,  0.6,  1. ,  8.5,\n        6.1,  9.8,  1. , 42.2, 77.2, 18.7, 22.8,  8.5, 43.9,  1. ,  1. ,\n        1.5, 10.8, 11.9,  3.4,  0.4,  3.1,  6.6, 33.7, 40.4,  2.3, 17.2,\n        0.7, 36.1,  1. , 33.2, 55.9, 30.8, 87.4, 15.4, 54.6,  5.1,  1.1,\n       10.2, 19.8,  0. , 40.7, 57.2, 59.9,  3.1, 55.7, 22.8, 10.9, 34.7,\n       32.2, 43. ,  1.3,  1. ,  0.5, 78.4, 34.2, 84.9, 29.1, 31.3, 18.3,\n       81.8, 39. , 11.2, 67. ,  4.1,  0.2, 78.1,  1. ,  7.1,  1. , 29. ,\n        1.1, 11.7, 73.6, 33.9, 14. ,  0.3,  1. ,  0.8, 71.9, 40.1,  1. ,\n        2.1,  3.8, 16.5,  4.1,  0.5, 44.4, 46.3, 18.7,  6.5, 36.8, 18.6,\n       11.1, 22.1, 71.1,  1. ,  0. ,  0.9,  0.7, 45.5,  8.4,  0. ,  3.8,\n        8.5,  2. ,  1. , 58.9,  0.3,  1. , 14. , 47. ,  4.1,  2.2,  7.2,\n        0.3,  1.5, 50.5,  1.3,  0.6, 19.1,  6.9,  9.2,  2.2,  0.2, 12.3,\n        4.9,  4.6,  0.3, 16.5, 65.7, 63.5, 16.8,  0.2,  1.8,  9.6, 15.2,\n       14.4,  3.3, 10.6, 61.3, 10.9, 32.2,  9.3, 11.6, 20.7,  6.5,  6.7,\n        3.5,  1. ,  1.6, 20.5,  1.5, 16.7,  2. ,  0.9])\n\nfertility=np.asarray([1.769, 2.682, 2.077, 2.132, 1.827, 3.872, 2.288, 5.173, 1.393,\n       1.262, 2.156, 3.026, 2.033, 1.324, 2.816, 5.211, 2.1  , 1.781,\n       1.822, 5.908, 1.881, 1.852, 1.39 , 2.281, 2.505, 1.224, 1.361,\n       1.468, 2.404, 5.52 , 4.058, 2.223, 4.859, 1.267, 2.342, 1.579,\n       6.254, 2.334, 3.961, 6.505, 2.53 , 2.823, 2.498, 2.248, 2.508,\n       3.04 , 1.854, 4.22 , 5.1  , 4.967, 1.325, 4.514, 3.173, 2.308,\n       4.62 , 4.541, 5.637, 1.926, 1.747, 2.294, 5.841, 5.455, 7.069,\n       2.859, 4.018, 2.513, 5.405, 5.737, 3.363, 4.89 , 1.385, 1.505,\n       6.081, 1.784, 1.378, 1.45 , 1.841, 1.37 , 2.612, 5.329, 5.33 ,\n       3.371, 1.281, 1.871, 2.153, 5.378, 4.45 , 1.46 , 1.436, 1.612,\n       3.19 , 2.752, 3.35 , 4.01 , 4.166, 2.642, 2.977, 3.415, 2.295,\n       3.019, 2.683, 5.165, 1.849, 1.836, 2.518, 2.43 , 4.528, 1.263,\n       1.885, 1.943, 1.899, 1.442, 1.953, 4.697, 1.582, 2.025, 1.841,\n       5.011, 1.212, 1.502, 2.516, 1.367, 2.089, 4.388, 1.854, 1.748,\n       2.978, 2.152, 2.362, 1.988, 1.426, 3.29 , 3.264, 1.436, 1.393,\n       2.822, 4.969, 5.659, 3.24 , 1.693, 1.647, 2.36 , 1.792, 3.45 ,\n       1.516, 2.233, 2.563, 5.283, 3.885, 0.966, 2.373, 2.663, 1.251,\n       2.052, 3.371, 2.093, 2.   , 3.883, 3.852, 3.718, 1.732, 3.928])","9ee5a80f":"'''\nYou can see the correlation between illiteracy and fertility by eye, and by the substantial Pearson correlation coefficient of 0.8.\nIt is difficult to resolve in the scatter plot, but there are many points around near-zero illiteracy and about 1.8 children\/woman.\n'''\n# Plot the illiteracy rate versus fertility\n_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')\n\n# Set the margins and label axes\nplt.margins(.02)\n_ = plt.xlabel('percent illiterate')\n_ = plt.ylabel('fertility')\n\n# Show the plot\nplt.show()\n\n# Show the Pearson correlation coefficient\nprint(pearson_r(illiteracy, fertility))\n","8c2e4810":"# Plot the illiteracy rate versus fertility\n_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')\nplt.margins(0.02)\n_ = plt.xlabel('percent illiterate')\n_ = plt.ylabel('fertility')\n\n# Perform a linear regression using np.polyfit(): a, b\na, b = np.polyfit(illiteracy,fertility ,deg=1)\n\n# Print the results to the screen\nprint('slope =', a, 'children per woman \/ percent illiterate')\nprint('intercept =', b, 'children per woman')\n\n# Make theoretical line to plot\nx = np.array([0,100])\ny = a * x + b\n\n# Add regression line to your plot\n_ = plt.plot(x, y)\n\n# Draw the plot\nplt.show()\n","cbc7ffc1":"'''\nNotice that the minimum on the plot, that is the value of the slope that gives the minimum sum of the square of the residuals,\nis the same value you got when performing the regression.\n'''\n# Specify slopes to consider: a_vals\na_vals = np.linspace(0,0.1,200)\n\n# Initialize sum of square of residuals: rss\nrss = np.empty_like(a_vals)\n\n# Compute sum of square of residuals for each value of a_vals\nfor i, a in enumerate(a_vals):\n    rss[i] = np.sum((fertility - a*illiteracy - b)**2)\n\n# Plot the RSS\nplt.plot(a_vals, rss, '-')\nplt.xlabel('slope (children per woman \/ percent illiterate)')\nplt.ylabel('sum of square of residuals')\n\nplt.show()\n","7a35bd17":"rainfall=np.asarray([ 875.5,  648.2,  788.1,  940.3,  491.1,  743.5,  730.1,  686.5,\n        878.8,  865.6,  654.9,  831.5,  798.1,  681.8,  743.8,  689.1,\n        752.1,  837.2,  710.6,  749.2,  967.1,  701.2,  619. ,  747.6,\n        803.4,  645.6,  804.1,  787.4,  646.8,  997.1,  774. ,  734.5,\n        835. ,  840.7,  659.6,  828.3,  909.7,  856.9,  578.3,  904.2,\n        883.9,  740.1,  773.9,  741.4,  866.8,  871.1,  712.5,  919.2,\n        927.9,  809.4,  633.8,  626.8,  871.3,  774.3,  898.8,  789.6,\n        936.3,  765.4,  882.1,  681.1,  661.3,  847.9,  683.9,  985.7,\n        771.1,  736.6,  713.2,  774.5,  937.7,  694.5,  598.2,  983.8,\n        700.2,  901.3,  733.5,  964.4,  609.3, 1035.2,  718. ,  688.6,\n        736.8,  643.3, 1038.5,  969. ,  802.7,  876.6,  944.7,  786.6,\n        770.4,  808.6,  761.3,  774.2,  559.3,  674.2,  883.6,  823.9,\n        960.4,  877.8,  940.6,  831.8,  906.2,  866.5,  674.1,  998.1,\n        789.3,  915. ,  737.1,  763. ,  666.7,  824.5,  913.8,  905.1,\n        667.8,  747.4,  784.7,  925.4,  880.2, 1086.9,  764.4, 1050.1,\n        595.2,  855.2,  726.9,  785.2,  948.8,  970.6,  896. ,  618.4,\n        572.4, 1146.4,  728.2,  864.2,  793. ])","5b0882e0":"for _ in range(50):\n    # Generate bootstrap sample: bs_sample\n    bs_sample = np.random.choice(rainfall, size=len(rainfall))\n\n    # Compute and plot ECDF from bootstrap sample\n    x, y = ecdf(bs_sample)\n    _ = plt.plot(x, y, marker='.', linestyle='none',\n                 color='gray', alpha=0.1)\n\n# Compute and plot ECDF from original data\nx, y = ecdf(rainfall)\n_ = plt.plot(x, y, marker='.')\n\n# Make margins and label axes\nplt.margins(0.02)\n_ = plt.xlabel('yearly rainfall (mm)')\n_ = plt.ylabel('ECDF')\n\n# Show the plot\nplt.show()","fe007a3e":"def bootstrap_replicate_1d(data, func):\n    return func(np.random.choice(data, size=len(data)))\n\ndef draw_bs_reps(data, func, size=1):\n    \"\"\"Draw bootstrap replicates.\"\"\"\n\n    # Initialize array of replicates: bs_replicates\n    bs_replicates = np.empty(size)\n\n    # Generate replicates\n    for i in range(size):\n        bs_replicates[i] = bootstrap_replicate_1d(data,func)\n\n    return bs_replicates","08d11e49":"'''\nNotice that the SEM we got from the known expression and the bootstrap replicates is the same and the distribution of the \nbootstrap replicates of the mean is Normal.\n'''\n\n# Take 10,000 bootstrap replicates of the mean: bs_replicates\nbs_replicates = draw_bs_reps(rainfall,func=np.mean,size=10000)\n\n# Compute and print SEM\nsem = np.std(rainfall) \/ np.sqrt(len(rainfall))\nprint(sem)\n\n# Compute and print standard deviation of bootstrap replicates\nbs_std = np.std(bs_replicates)\nprint(bs_std)\n\n# Make a histogram of the results\n_ = plt.hist(bs_replicates, bins=50, density=True)\n_ = plt.xlabel('mean annual rainfall (mm)')\n_ = plt.ylabel('PDF')\n\n# Show the plot\nplt.show()","af1651f0":"'''\nThis is not normally distributed, as it has a longer tail to the right.\nNote that you can also compute a confidence interval on the variance, or any other statistic, \nusing np.percentile() with your bootstrap replicates.\n'''\n# Generate 10,000 bootstrap replicates of the variance: bs_replicates\nbs_replicates = draw_bs_reps(rainfall,np.var,size=10000)\n\n# Put the variance in units of square centimeters\nbs_replicates=bs_replicates\/100\n\n# Make a histogram of the results\n_ = plt.hist(bs_replicates, bins=50, normed=True)\n_ = plt.xlabel('variance of annual rainfall (sq. cm)')\n_ = plt.ylabel('PDF')\n\n# Show the plot\nplt.show()","58af49c5":"'''\nThis gives you an estimate of what the typical time between no-hitters is. It could be anywhere between 660 and 870 games.\n'''\n# Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates\nbs_replicates = draw_bs_reps(nohitter_times,np.mean,10000)\n\n# Compute the 95% confidence interval: conf_int\nconf_int = np.percentile(bs_replicates,[2.5,97.5])\n\n# Print the confidence interval\nprint('95% confidence interval =', conf_int, 'games')\n\n# Plot the histogram of the replicates\n_ = plt.hist(bs_replicates, bins=50, normed=True)\n_ = plt.xlabel(r'$\\tau$ (games)')\n_ = plt.ylabel('PDF')\n\n# Show the plot\nplt.show()\n","0ac5e592":"gapminder=pd.read_csv('..\/input\/gapminder.csv')\ngapminder.info()","833fb51c":"gapminder.loc[0:200:20]","55f20d65":"# Plotting a graph for the year 1965 for babies_per_woman vs age5_surviving(% of babies surviving till 5 years of age)\n\n#gapminder[gapminder['year']==1965].plot(x='babies_per_woman',y='age5_surviving',kind='scatter')\n\n# or a neat way is :\n\ngapminder[gapminder.year==1965].plot.scatter('babies_per_woman','age5_surviving')","4e9a421d":"gapminder.iloc[1063,:]","4f7ab7aa":"# Define a function to show it for other years also , not just 1965\n\ndef plotyear(year):\n    data=gapminder[gapminder.year==year]\n    \n    # show points according to population , bigger points for larger population\n    # doing just 'size_param=data.population' creates big circles which cover the whole plot region , so I reduced the size of the \n    # circles exponentially for each points , hence the relative size will be maintained , after few tries 4e-6 seems appropirate\n    size_param=4e-6*data.population\n    \n    # Colours countries by continent , done by creating a disctionary region:colour\n    colour_param=data.region.map({'Africa':'skyblue' , 'Asia':'coral' , 'Europe':'palegreen' , 'America':'gold'})\n    \n    data.plot.scatter('babies_per_woman','age5_surviving',\n                      s=size_param,\n                      c=colour_param,\n                      edgecolors='k',\n                      linewidths=1,\n                      figsize=(12,9))\n    \n    # plotting details\n    plt.axis(ymin=50,ymax=105,xmin=0,xmax=8)\n    plt.xlabel('babie sper vwoman')\n    plt.ylabel('% childern alive at 5')\n    \ninteract(plotyear,year=widgets.IntSlider(min=1950,max=2015,value=1965,step=1))\n    ","53f0b9a6":"china1965 = pd.read_csv('..\/input\/income-1965-china.csv')\nchina2015 = pd.read_csv('..\/input\/income-2015-china.csv')\nusa1965 = pd.read_csv('..\/input\/income-1965-usa.csv')\nusa2015 = pd.read_csv('..\/input\/income-2015-usa.csv')","087dd9e8":"\nchina1965.quantile([0.25,0.75])","7cea6bba":"china1965.quantile(0.5)\n# same as median","ef31d30a":"china1965.median()","75d4b71c":"scipy.stats.percentileofscore(china1965.income,1.5)","45924181":"china1965.describe()","ea2e8b71":"usa1965.describe()","794d766e":"china1965.income.plot(kind='box')","4230f147":"pd.DataFrame({'usa': usa1965.log10_income, 'china': china1965.log10_income}).boxplot()","f2290fbc":"# Both works similar\n#china1965.income.plot(kind='hist',histtype='step',bins=30)\nchina1965.income.plot.hist(bins=30,histtype='step')\nplt.axvline(china1965.income.mean(),c='C1')\nplt.axvline(china1965.income.median(),c='C1',linestyle='--')\nplt.axvline(china1965.income.quantile(0.25),c='C1',linestyle=':')\nplt.axvline(china1965.income.quantile(0.75),c='C1',linestyle=':')","8e7f18d1":"china1965.income.plot(kind='hist',histtype='step',bins=30,density=True)\nchina1965.income.plot.density(bw_method=0.5)\n\nplt.axis(xmin=0,xmax=3)","42bbc996":"china1965.log10_income.plot.hist(histtype='step',bins=20)\nusa1965.log10_income.plot.hist(histtype='step',bins=20)\n\nlevels = [0.25,0.5,1,2,4,8,16,32,64]\nplt.xticks(np.log10(levels),levels);","a21e88d9":"china2015.log10_income.plot.hist(histtype='step',bins=20)\nusa2015.log10_income.plot.hist(histtype='step',bins=20)\n\nlevels = [0.25,0.5,1,2,4,8,16,32,64]\nplt.xticks(np.log10(levels),levels);","adcf7992":"china_pop2015 = float(gapminder.query('country == \"China\" and year == 2015').population)\nusa_pop2015 = float(gapminder.query('country == \"United States\" and year == 2015').population)\nchina2015['weight'] = china_pop2015 \/ len(china2015)\nusa2015['weight'] = usa_pop2015 \/ len(usa2015)","769966a1":"china2015.log10_income.plot.hist(histtype='step',bins=20,weights=china2015.weight)\nusa2015.log10_income.plot.hist(histtype='step',bins=20,weights=usa2015.weight)\n\nlevels = [0.25,0.5,1,2,4,8,16,32,64]\nplt.xticks(np.log10(levels),levels);","3971c552":"italy=gapminder.query(\"country=='Italy'\")\nitaly.head()","b9a6d319":"italy.plot.scatter('year','population')","5a064103":"gapminder.query(\"country=='India'\").plot.scatter('year','population')","eb11e71b":"italy.plot.scatter(\"year\", \"gdp_per_day\")","26c59735":"italy.plot.scatter(\"year\", \"gdp_per_day\", logy=True)","cbba6873":"italy.plot.scatter(\"gdp_per_day\", \"life_expectancy\")","82e52427":"italy.plot.scatter(\"gdp_per_day\", \"life_expectancy\", logx=True)\n# We can see better cluttering around bottom left , when seen on logx scale","c48203e9":"# increasing size for the years which are decade\nsize=np.where(italy.year % 10 ==0 , 30 ,2)\n\nitaly.plot.scatter(\"gdp_per_day\", \"life_expectancy\", logx=True, s=size)\n","f67e18fb":"# Plotting life expectancy over the years for two countries on a single plot\n\ndata=gapminder.query(\"(country=='Italy') or (country=='United States')\")\ncolour_param=np.where(data.country=='Italy','blue','green')\nsize=np.where(data.year % 10 ==0 , 30 ,2)\n\ndata.plot.scatter('gdp_per_day','life_expectancy',c=colour_param,logx=True,s=size)","c03f32c8":"def plotyear(nyear):\n    data=gapminder.query(\"year== @nyear\")\n    #data=gapminder[gapminder['year']== nyear]\n    data.plot.scatter('gdp_per_day','life_expectancy')\n    \nplotyear(1965)","246f396f":"# Repeated to highlight the difference in usage of \n# gapminder.query(\"year== @nyear\") vs gapminder[gapminder['year']== nyear]\ndef plotyear(nyear):\n    #data=gapminder.query(\"year== nyear\")\n    data=gapminder[gapminder['year']== nyear]\n    data.plot.scatter('gdp_per_day','life_expectancy')\n    \nplotyear(1965)","f9ac2896":"def plotyear(nyear):\n    #data=gapminder.query(\"year== nyear\")\n    data=gapminder[gapminder['year']== nyear]\n    data.plot.scatter('gdp_per_day','life_expectancy',logx=True)\n    \nplotyear(1965)","861c9876":"# Making the above visually informative\n# visualizing 4 variables together : population ,age5_surviving , gdp_per_day , life_expectancy\ndef plotyear(nyear):\n    data=gapminder[gapminder['year']== nyear]\n    size_param=4e-6*data.population\n    colour_param=data.age5_surviving\n    data.plot.scatter('gdp_per_day','life_expectancy',logx=True,s=size_param,\n                      c=colour_param,\n                      colormap=matplotlib.cm.get_cmap('Blues_r'),\n                      edgecolors='k',\n                      linewidths=1,sharex=False\n                      )\n    \nplotyear(1965)","e3f2f354":"# Making the above visually informative\n# visualizing 5 variables together : population ,age5_surviving , gdp_per_day , life_expectancy , region\ndef plotyear(year):\n    data=gapminder[gapminder['year']== year]\n    size_param=4e-6*data.population\n    colour_param=data.age5_surviving\n    edgecolor = data.region.map({'Africa': 'skyblue','Europe': 'gold','America': 'palegreen','Asia': 'coral'})\n    data.plot.scatter('gdp_per_day','life_expectancy',logx=True,s=size_param,\n                      c=colour_param,\n                      colormap=matplotlib.cm.get_cmap('Blues_r'),\n                      edgecolors=edgecolor,\n                      linewidths=1,sharex=False,\n                      figsize=(10,6)\n                      )\n    \nplotyear(1965)","77489ee1":"interact(plotyear,year=range(1965,2016,10))","18584aee":"def plotyear(year):\n    data=gapminder[gapminder['year']== year]\n    size_param=4e-6*data.population\n    colour_param=data.age5_surviving\n    edgecolor = data.region.map({'Africa': 'skyblue','Europe': 'gold','America': 'palegreen','Asia': 'coral'})\n    data.plot.scatter('gdp_per_day','life_expectancy',logx=True,s=size_param,\n                      c=colour_param,\n                      colormap=matplotlib.cm.get_cmap('Blues_r'),\n                      edgecolors=edgecolor,\n                      linewidths=1,sharex=False,\n                      figsize=(10,6)\n                      )\n    for level in [4,16,64]:\n        plt.axvline(level,linestyle=':',color='k')\n    \nplotyear(1965)","6e3f34ef":"# gapminder['log10_gdp_per_day'] = np.log10(data['gdp_per_day'])\n# data = gapminder.loc[gapminder.year == 2015,['log10_gdp_per_day','life_expectancy','age5_surviving','babies_per_woman']]\n\n\n# #scatter_matrix(data,figsize=(9,9))\n# #pd.plotting.scatter_matrix(data)\n# data","d1c38f70":"smoking = pd.read_csv('..\/input\/whickham.csv')\nsmoking.head()","70ae1f94":"pd.DataFrame(smoking.smoker.value_counts())","662573c0":"pd.DataFrame(smoking.outcome.value_counts())","d4357344":"pd.DataFrame(smoking.outcome.value_counts(normalize=True))","fe378f24":"bysmoker=smoking.groupby('smoker').outcome.value_counts()\nbysmoker","b4bf01f6":"bysmoker.index","068739ac":"bysmoker.unstack()","3543b188":"smoking['AgeGroup']=pd.cut(smoking.age,[0,30,40,53,64],labels=['0-30','30-40','40-53','53-64'])\nsmoking['AgeGroup'].head()","89c90dd5":"byage=smoking.groupby(['AgeGroup','smoker']).outcome.value_counts(normalize=True)\nbyage.head()","3669a39f":"byage.unstack().drop('Dead',axis=1)","954bad72":"plt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nsmoking.outcome.value_counts().plot.pie()\nplt.title('outcome')\n\nplt.subplot(1,2,2)\nsmoking.smoker.value_counts().plot.pie()\nplt.title('smoker')","70b1eee8":"bysmoker.plot.bar()","77e91b13":"bysmoker.unstack().plot.bar()","c004882b":"bysmoker.unstack().plot.bar(stacked=True)","8a65486f":"byage.unstack().plot.bar(stacked=True)","a6dafb46":"byage.unstack().drop('Dead',axis=1).unstack().plot.bar()\n# Make it further neat","ad079509":"byage2=byage.unstack().drop('Dead',axis=1).unstack()\nbyage2","48fd757a":"byage2.columns","17331f3c":"byage2.columns=['No','Yes']\nbyage2.columns.name='smoker'","eaddd054":"byage2","77ddb919":"byage2.plot.bar()","700657a5":"poll = pd.read_csv('..\/input\/poll.csv')\npoll.head()","979575b5":"poll.vote.value_counts(normalize=True)","822aa190":"np.random.rand(5)","6babb28d":"np.random.rand(5) < 0.51","b3a44883":"np.where(np.random.rand(5) < 0.51 , 'Brown','Green')","b429c505":"def sample(brown,n=1000):\n    return pd.DataFrame({'vote': np.where(np.random.rand(n) < brown , 'Brown','Green')})\n\ns=sample(0.51)","9a009c44":"s.vote.value_counts(normalize=True)","5ffe614b":"dist=pd.DataFrame([sample(0.51).vote.value_counts(normalize=True) for i in range(1000)])\ndist.head()","7b002bae":"dist.Brown.plot.hist(histtype='step',bins=20)","0b6974e1":"# Revisit\ndef samplingdist(brown,n=1000):\n    return pd.DataFrame([sample(brown,n).vote.value_counts(normalize=True) for i in range(1000)])","7404a669":"def quantiles(brown,n=1000):\n    dist = samplingdist(brown,n)\n    return dist.Brown.quantile(0.025), dist.Brown.quantile(0.975)","ea63b779":"quantiles(0.50)","4b0bbd28":"quantiles(0.48)","67dd9711":"dist = samplingdist(0.50,10000)","c9e9ab3c":"dist.Brown.hist(histtype='step')","89a3fa20":"largepoll = pd.read_csv('..\/input\/poll-larger.csv')\nlargepoll.vote.value_counts(normalize=True)","594ea149":"pop = pd.read_csv('..\/input\/grades.csv')","5cb25c97":"pop.head()","7a10b787":"pop.grade.plot.hist(histtype='step')\n","0f0bc306":"pop.describe()\n# The histogram does not have a very well defined structure . It has a mean of 5.5 though.","314e9af1":"'''\nWhat can we say about the true mean value.\nThis time we cannot build a confidence interval by simulating the sampling distribution because we do not know how to describe it. \nAnd, indeed, given the observed histogram it is unlikely that it has a simple form such as a normal distribution.\n\nWhat we'll do is to estimate the uncertainty of our statistic, the mean, by generating a large family of samples from the one we have. \nAnd then, characterizing the distribution of the mean over this family.\n\nEach sample in the family is prepared as follow: we draw grades randomly for our single existing sample allowing the same grade to be drawn more than once.\nTechnically speaking, we are sampling with replacement\n'''","fe5bcd6d":"# We took 100 samples from the given 100 points , but with replacement\npop.sample(100,replace=True).describe()\n\n# The mean for this bootstrapped sample is a bit different from 5.508561 (mean of 100 samples originally)","09abde4a":"'''\nWe repeat the bootstrap sampling for 1000 times and record the means in a dataframe for analysis\n'''\nbootstrap=pd.DataFrame({'meangrade':[pop.sample(100,replace=True).grade.mean() for i in range(1000)]})","575b225e":"bootstrap.meangrade.plot.hist(histtype='step')\nplt.axvline(bootstrap.meangrade.mean(),color='yellow',linestyle='-')\nplt.axvline(pop.grade.mean(),color='red')\n'''\nBoth means are same , but there is a significant spread over the bootstrapped mean\n'''","c5cc37b3":"bootstrap.meangrade.quantile(0.025),bootstrap.meangrade.quantile(0.975)","d99e192a":"# position of 8 water pumps in central london\npumps = pd.read_csv('..\/input\/pumps.csv')\npumps","282223ea":"# Deaths by area and the nearest pump to the area\ncholera = pd.read_csv('..\/input\/cholera.csv')\ncholera.head()","a16d6c48":"# Plotting pumps and deaths\nplt.figure(figsize=(6,6))\nplt.scatter(cholera.x,cholera.y,s=3)\nplt.scatter(pumps.x,pumps.y)","0186677a":"cholera.closest.value_counts()","dab564a8":"'''\nMore deaths near pump zero may be due to the fact that more people might be living near it as compared to other pumps\n\n'''\ncholera.groupby('closest').deaths.sum()\n\n","c59e5b40":"'''\nLet's simulate each death randomly, proportionally to the population of each area.\n0: 340\nTotal : 489\n'''\ndef simulate(n):\n    return pd.DataFrame({'closest': np.random.choice([0,1,4,5],size=n,p=[0.65,0.15,0.10,0.10])})","915e05a7":"simulate(489).closest.value_counts()","31294976":"'''\nSo we get something close to what we actually observed in the true data. What we need now is the sampling distribution \nof the number of deaths in area zero.I will extract the count for area zero, repeat the operation 10,000 times, and \nenclose the result in a DataFrame.\n'''\nsampling = pd.DataFrame({'counts': [simulate(489).closest.value_counts()[0] for i in range(10000)]})","0d99003a":"# Lets look at the histogram\nsampling.counts.plot.hist(histtype='step')","4267b059":"'''\nWe have generated this distribution under the null hypothesis that the pumps have nothing to do with cholera, \nand the deaths occur simply proportionally to population. We can now compare this distribution with the observed \nnumber of 340 deaths in area zero.More precisely, we evaluate at what quantile we find 340 in this null hypothesis sampling distribution.\n\nSo 340 is a very extreme value, which we would not expect from the null scenario. In fact, we'd expect it only 1.70(100-98.3) percent \nof the time.This is known as the P value, the smaller the P value, the more strongly we can reject the null hypothesis.\n'''\nscipy.stats.percentileofscore(sampling.counts,340)\n","f7944841":"poll.vote.value_counts(normalize=True)","c18a9073":"'''\nIn the smaller poll Brown had a seeming majority of votes, so here the null hypothesis will be that Green wins or ties the election, \nso the true Brown fraction would be 0.50 or less. We need to find out whether a Brown proportion of 0.511 is an extreme result \nif the null hypothesis holds. So we compute the sampling distribution of the proportion, and get a true Brown fraction of 0.50. \n\nIf it's lower than that the P value will be even lower. \n'''\ndef sample(brown, n=1000):\n    return pd.DataFrame({'vote': np.where(np.random.rand(n) < brown,'Brown','Green')})","49a9d95a":"dist = pd.DataFrame({'Brown': [sample(0.50,1000).vote.value_counts(normalize=True)['Brown'] for i in range(10000)]})","c0ae5526":"dist.Brown.hist(histtype='step',bins=20)","41df1ab5":"100 - scipy.stats.percentileofscore(dist.Brown,0.511)","6bd201a7":"largepoll.vote.value_counts(normalize=True)","cc1d2d6d":"dist = pd.DataFrame({'Green': [sample(0.50,10000).vote.value_counts(normalize=True)['Green'] for i in range(1000)]})","2bbe4a07":"dist.Green.hist(histtype='step',bins=20)\nplt.axvline(0.5181,c='C1')","893eb0e6":"# Introduction to Statistical Inference","507d36b1":"## Showing how the original dataset grades was created and compare its spread with \n## bootstrapped spread","7bd1d197":"## Confidence Intervals","fe5549c6":"# Thinking probabilistically-- Discrete variables  \nStatistical inference rests upon probability. Because we can very rarely say anything meaningful with absolute certainty from data, we use probabilistic language to make quantitative statements about data.you will learn how to think probabilistically about discrete quantities, those that can only take certain values, like integers. It is an important first step in building the probabilistic language necessary to think statistically.\n","a0a68033":"## Linear regression by least squares  \nThe process of finding the parameters for which the sum of the squares of the residuals is minimal.\n","2437dd3c":"# Quantitative exploratory data analysis\n\n","e496dba5":"## Histograms","15ad00af":"## Hypothesis Testing\n\nAnalyzing cholera deaths by contaminated water pumps","3c41faa9":"## The np.random module and Bernoulli trials\nYou can think of a Bernoulli trial as a flip of a possibly biased coin. Specifically, each coin flip has a probability p of landing heads (success) and probability 1\u2212p of landing tails (failure).Below is  a function to perform n Bernoulli trials which returns the number of successes out of n Bernoulli trials, each of which has probability p of success. To perform each Bernoulli trial, use the np.random.random() function, which returns a random number between zero and one.","fdc7db0a":"## Probabilistic logic and statistical inference\nWhat is the goal of statistical inference?  \nTo draw probabilistic conclusions about what we might expect if we collected the same data again.  \nTo draw actionable conclusions from data.  \nTo draw more general conclusions from relatively few data or observations.\n","9763ff8d":"## Will the bank fail?\nPlot the number of defaults.\nIf interest rates are such that the bank will lose money if 10 or more of its loans are defaulted upon, what is the probability that the bank will lose money?","818056c4":"**EDA of literacy\/fertility data**  \n\nIn the next few exercises, we will look at the correlation between female literacy and fertility (defined as the average number of children born per woman) throughout the world. For ease of analysis and interpretation, we will work with the illiteracy rate.\n\nIt is always a good idea to do some EDA ahead of our analysis. To this end, plot the fertility versus illiteracy and compute the Pearson correlation coefficient. The Numpy array illiteracy has the illiteracy rate among females for most of the world's nations. The array fertility has the corresponding fertility data.","db7c3901":"# Parameter estimation by optimization\nWhen doing statistical inference, we speak the language of probability. A probability distribution that describes your data has parameters. So, a major goal of statistical inference is to estimate the values of these parameters, which allows us to concisely and unambiguously describe our data and draw conclusions from it. You will learn how to find the optimal parameters, those that best describe your data.","108fa400":"# Introduction to Exploratory Data Analysis  \nThe process of organizing, ploting, and summarizing a data set\n","8b041b7c":"**Bootstrap replicates of the mean and the SEM**  \nIn this exercise, you will compute a bootstrap estimate of the probability density function of the mean annual rainfall at the Sheffield Weather Station. Remember, we are estimating the mean annual rainfall we would get if the Sheffield Weather Station could repeat all of the measurements from 1883 to 2015 over and over again. This is a probabilistic estimate of the mean. You will plot the PDF as a histogram, and you will see that it is Normal.\n\nIn fact, it can be shown theoretically that under not-too-restrictive conditions, the value of the mean will always be Normally distributed. (This does not hold in general, just for the mean and a few other statistics.) The standard deviation of this distribution, called the standard error of the mean, or SEM, is given by the standard deviation of the data divided by the square root of the number of data points. I.e., for a data set, sem = np.std(data) \/ np.sqrt(len(data)). Using hacker statistics, you get this same result without the need to derive it, but you will verify this result from your bootstrap replicates.","a3a834c1":"**Linear regression**  \n\nWe will assume that fertility is a linear function of the female illiteracy rate. That is, f=ai+b, where a is the slope and b is the intercept. We can think of the intercept as the minimal fertility rate, probably somewhere between one and two. The slope tells us how the fertility rate varies with illiteracy. We can find the best fit line using np.polyfit().\n\nPlot the data and the best fit line. Print out the slope and intercept. (Think: what are their units?)","6e1aa7e8":"**How often do we get no-hitters?**  \n\nThe number of games played between each no-hitter in the modern era (1901-2015) of Major League Baseball is stored in the array nohitter_times.\n\nIf you assume that no-hitters are described as a Poisson process, then the time between no-hitters is Exponentially distributed. As you have seen, the Exponential distribution has a single parameter, which we will call \u03c4, the typical interval time. The value of the parameter \u03c4 that makes the exponential distribution best match the data is the mean interval time (where time is in units of number of games) between no-hitters.\n\nCompute the value of this parameter from the data. Then, use np.random.exponential() to \"repeat\" the history of Major League Baseball by drawing inter-no-hitter times from an exponential distribution with the \u03c4 you found and plot the histogram as an approximation to the PDF.","ece7136f":"**Confidence intervals of rainfall data**  \nA confidence interval gives upper and lower bounds on the range of parameter values you might expect to get if we repeat our measurements. For named distributions, you can compute them analytically or look them up, but one of the many beautiful properties of the bootstrap method is that you can take percentiles of your bootstrap replicates to get your confidence interval. Conveniently, you can use the np.percentile() function.\n\nUse the bootstrap replicates you just generated to compute the 95% confidence interval. That is, give the 2.5th and 97.5th percentile of your bootstrap replicates stored as bs_replicates.  \n\n**Bootstrap replicates of other statistics**  \nWe saw in a previous exercise that the mean is Normally distributed. This does not necessarily hold for other statistics, but no worry: as hackers, we can always take bootstrap replicates!","1d5e85c7":"Covariance : A measure of how two quantities vary together","37e5c6f8":"Binnig Bias : The same data may be interpreted differently depending on choice of bins.  \nWe are also losing information with histograms as we are combining data into bins.","d0d07968":"## Histogram","a2180aec":"# Visualizing and Describing Data","667445e2":"**Confidence interval on the rate of no-hitters**  \n\nConsider again the inter-no-hitter intervals for the modern era of baseball. Generate 10,000 bootstrap replicates of the optimal parameter \b\u03c4. Plot a histogram of your replicates and report a 95% confidence interval.","e972953a":"**How is this parameter optimal?**  \n\nNow sample out of an exponential distribution with \u03c4 being twice as large as the optimal \u03c4. Do it again for \u03c4 half as large. Make CDFs of these samples and overlay them with your data. You can see that they do not reproduce the data as well. Thus, the \u03c4 you computed from the mean inter-no-hitter times is optimal in that it best reproduces the data.","bb064ea1":"## Exponential Distribution","42ef741f":"## Random number generators and hacker statistics\nHacker Statistics : Uses simulated repeated measurements to compute probabilities.  \nBernouli Trial : An experiment that has two options,\"success\" (True) and \"failure\" (False).\n","b8327a5e":"**How is it optimal?**  \n\nThe function np.polyfit() that you used to get your regression parameters finds the optimal slope and intercept. It is optimizing the sum of the squares of the residuals, also known as RSS (for residual sum of squares). In this exercise, you will plot the function that is being optimized, the RSS, versus the slope parameter a. To do this, fix the intercept to be what you found in the optimization. Then, plot the RSS vs. the slope. Where is it minimal?","7d695443":"## More Variables","36443324":"**Normal CDF** : Now that you have a feel for how the Normal PDF looks, let's consider its CDF.","703c2801":"## Email Analysis","5c3a576d":"## Relationship between Binomial and Poisson distributions\nThe Poisson distribution is a limit of the Binomial distribution for rare events. This makes sense if you think about the stories. Say we do a Bernoulli trial every minute for an hour, each with a success probability of 0.1. We would do 60 trials, and the number of successes is Binomially distributed, and we would expect to get about 6 successes. This is just like the Poisson story we discussed in the video, where we get on average 6 hits on a website per hour. So, the Poisson distribution with arrival rate equal to np approximates a Binomial distribution for n Bernoulli trials with probability p of success (with n large and p small). Importantly, the Poisson distribution is often simpler to work with because it has only one parameter instead of two for the Binomial distribution.\n\nWe will compute the mean and standard deviation of samples from a Poisson distribution with an arrival rate of 10. Then, we will compute the mean and standard deviation of samples from a Binomial distribution with parameters n and p such that np=10.","fa729329":"## How many defaults might we expect?\nLet's say a bank made 100 mortgage loans. It is possible that anywhere between 0 and 100 of the loans will be defaulted upon. You would like to know the probability of getting a given number of defaults, given that the probability of a default is p = 0.05. To investigate this, you will do a simulation. You will perform 100 Bernoulli trials using the perform_bernoulli_trials() function you wrote in the previous exercise and record how many defaults we get. Here, a success is a default. (Remember that the word \"success\" just means that the Bernoulli trial evaluates to True, i.e., did the loan recipient default?) You will do this for another 100 Bernoulli trials. And again and again until we have tried it 1000 times. Then, you will plot a histogram describing the probability of the number of defaults.","344d4f9d":"## Sampling out of the Binomial distribution\nCompute the probability mass function for the number of defaults we would expect for 100 loans as in the last section, but instead of simulating all of the Bernoulli trials, perform the sampling using np.random.binomial(). This is identical to the calculation done earlier using the custom-written perform_bernoulli_trials() function, but far more computationally efficient. Given this extra efficiency, we will take 10,000 samples instead of 1000. After taking the samples, plot the CDF as last time. This CDF that we are plotting is that of the Binomial distribution.","04b6b4b6":"## Covariance and the Pearson correlation coefficient","cce2a234":"**Do the data follow our story?**  \n\nYou have modeled no-hitters using an Exponential distribution. Create an ECDF of the real data. Overlay the theoretical CDF with the ECDF from the data. This helps you to verify that the Exponential distribution describes the observed data.","7c4d3818":"## Introduction to the Normal distribution\nNormal Distribution : Describes a continuous variable whose PDF has a single symmetric peak.  \n**The Normal PDF**  \nBelow we will explore the Normal PDF and also learn a way to plot a PDF of a known distribution using hacker statistics.  \nSpecifically, we will plot a Normal PDF for various values of the variance.\n\n","00871155":"# Bootstrap confidence intervals  \nTo \"pull yourself up by your bootstraps\" is a classic idiom meaning that you achieve a difficult task by yourself with no help at all. In statistical inference, you want to know what would happen if you could repeat your data acquisition an infinite number of times. This task is impossible, but can we use only the data we actually have to get close to the same result as an infinitude of experiments? The answer is yes! The technique to do it is aptly called bootstrapping.  \nBootstraping: The use of resampled data to perform statistical inference  \nBootstrap Sample : A resampled array of the data  \nBootstrap Replicate : A statistic computed from a resampled array  \n\n\n\n","271da447":"## Probability distributions and stories: The Binomial distribution\n","9a7474fb":"## Bootstrap confidence intervals\n**Confidence Interval of a statistics** :  \nIf we repeated measurements over and over again,p% of the observed values would lie within the p% confidence interval.","9bea5f45":"## Pearson correlation coefficient","c188a897":"## Proportions","282f849e":"## Analyzing two variables","9076663f":"## Distribution","7de350ea":"## Bootstraping","d21bc2d4":"## Bee swarm plots","fc514ed6":"## Categorical","8f5a9294":"## Empirical cumulative distribution function (ECDF)","252ba73d":"## P-Value and Confidence Intervals","daf7a8ed":"## Variance and standard deviation","49c2747b":"## Poisson processes and the Poisson distribution","c3c4a606":"I have created this notebook as part of my learning Statistics for Data Science using Python.\nThis is still in building stage and not very well documented , yet I am making it public so as to be a kind of starter for someone looking to explore this topic.  \n\nThis notebook will be updated frequently , with insights\/code as I gain more knowledge in this domain.\n\nHope this is helpful , any comments will be appreciated.","336ddc9d":"Ask people to grade the mayor on scale of 0-10 , but took only 100 samples.","312e6ce0":"## Introduction to summary statistics:The sample mean and median","4fc3197c":"**Visualizing bootstrap samples**  \nIn this exercise, you will generate bootstrap samples from the set of annual rainfall data measured at the Sheffield Weather Station in the UK from 1883 to 2015. The data are stored in the NumPy array rainfall in units of millimeters (mm). By graphically displaying the bootstrap samples with an ECDF, you can get a feel for how bootstrap sampling allows probabilistic descriptions of data.","b43b6b62":"**Generating many bootstrap replicates**  \nFunction draw_bs_reps(data, func, size=1) generates many bootstrap replicates from the data set.","ae7479f9":"# Thinking probabilistically-- Continuous variables\nAfter learning about probability distributions of discrete variables. Now it is time to move on to continuous variables, such as those that can take on any fractional value. Many of the principles are the same, but there are some subtleties.  \nContinuous Variables : Quantities that can take any value, not just discrete values  \nProbability density function (PDF) :  \n  Continuous analog to the PMF\n  Mathematical description of the relative likelihood of observing a value of a continuous variable.  \n  Normal PDF , Normal CDF"}}