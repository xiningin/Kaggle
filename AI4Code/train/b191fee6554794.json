{"cell_type":{"e1fc134e":"code","442ed791":"code","d7a6e98b":"code","531b97ad":"code","bb8424eb":"code","42db6ec8":"code","b1af2590":"code","d0d4679a":"code","b85b6a30":"code","bd9530bb":"code","f3ca09cf":"code","1cfaddb8":"code","e5a827af":"code","e23b9044":"code","f859364d":"code","aff9e453":"code","234df2d7":"code","32c35b9f":"code","485369ea":"code","97f65a8b":"code","e5907b56":"code","7d647967":"code","46d24038":"code","e7f5d007":"code","15c830fe":"code","14d03b3c":"code","a7caaf9e":"code","ba4832c8":"code","3479111e":"code","07cdb8d6":"code","358cb7c4":"code","dba0f9c4":"code","f6f600e9":"code","08c15ada":"code","c14d6418":"code","40eb565d":"code","6541830d":"code","0fb21baa":"code","85a7bf07":"code","45b4389b":"code","a265f1cf":"code","a7bda66d":"code","d073798f":"code","4d14c2dd":"code","f4d8be0c":"code","a1fa2df6":"code","ed6aac6b":"code","a27ae9e6":"code","b357776c":"code","53d9a6e1":"code","4bd0b1c6":"code","8f943150":"code","03255a83":"code","8b5f7345":"code","d30b0cdb":"code","5da9b78d":"code","cba471d2":"code","ee045821":"code","b95c5b64":"code","7b2c0562":"code","a71958c2":"code","e4313d01":"code","109d7e9d":"code","07476a82":"code","1f11f915":"code","f8634cb5":"code","ef2c7a94":"code","7212a4bf":"code","860e0243":"code","8b593b60":"code","1d41520e":"code","9054ec1e":"code","c4d648f2":"code","4da7c271":"code","242e3903":"code","fa700e17":"code","9e02f575":"markdown","21c17eb3":"markdown","950bb734":"markdown","87fb7e3d":"markdown","67f32e9a":"markdown","9b44f187":"markdown","faced28b":"markdown","e6415aaf":"markdown","90df96da":"markdown","e254af89":"markdown","a75c0cb1":"markdown","7587b0c3":"markdown","eaea8c2e":"markdown","cc826416":"markdown","ee8b7f2a":"markdown","b96204d4":"markdown","351dae07":"markdown","ce2b9971":"markdown","df70554c":"markdown","5b298c32":"markdown","5babe23b":"markdown","bcfa1bf2":"markdown","d9b531fb":"markdown","f985f05b":"markdown","9dcff468":"markdown","1c07e885":"markdown","35965046":"markdown","5956362c":"markdown","08597619":"markdown","14cb6869":"markdown","ec75c6d7":"markdown","d2bd96d2":"markdown","b099bf6b":"markdown","2bd10ffc":"markdown","98b52fb7":"markdown","b07b82e7":"markdown","29dd0d99":"markdown","5567bf64":"markdown","661cab81":"markdown","d22bbbc0":"markdown","c4d1e44e":"markdown","58c5256c":"markdown","6a8eb012":"markdown","52b82d50":"markdown","4eb0bcbb":"markdown","52ef0473":"markdown","e92b5ba1":"markdown","cd7a1707":"markdown","f4337505":"markdown","27205b60":"markdown","f0ec73d7":"markdown","acf68501":"markdown","042cbf74":"markdown","1005271b":"markdown","42882d1e":"markdown"},"source":{"e1fc134e":"import numpy as np\nimport pandas as pd\n\ntrain_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nX_train = train_df.drop(['SalePrice'], axis=1)\ny_train = train_df['SalePrice']\ntrain_df\n\n# We will use a \"combined\" set to look across both the train and test to ensure we normalize our values and make our checks across both data sets.\ncombined = pd.concat([train_df,test_df])\nall_data = [train_df,test_df]\n\ntotal = combined.isnull().sum().sort_values(ascending=False)\npercent_1 = combined.isnull().sum()\/combined.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(20)","442ed791":"train_df['SalePrice'].describe()","d7a6e98b":"combined['MSZoning'].unique()","531b97ad":"combined.groupby('MSZoning')['Id'].count()","bb8424eb":"train_df.groupby('MSZoning')['SalePrice'].describe()","42db6ec8":"combined[combined['MSZoning'].isnull()]","b1af2590":"MSZoningMap = {'FV': 1, 'RL': 2, 'RH': 3, 'RM': 4, 'C (all)': 5}\n\nfor d in all_data:\n    d['MSZoning'] = d['MSZoning'].fillna('RL')\n    d['MSZoning'] = d['MSZoning'].map(MSZoningMap)\n\ncombined['MSZoning'] = combined['MSZoning'].fillna('RL')\ncombined['MSZoning'] = combined['MSZoning'].map(MSZoningMap)\n    \ncombined['MSZoning'].unique()","d0d4679a":"all_data[0].groupby('MSZoning')['SalePrice'].describe().sort_values('mean')","b85b6a30":"subclass_map = train_df.groupby('MSSubClass')['SalePrice'].mean().sort_values()\nfor d in all_data:\n    for idx, s in enumerate(subclass_map.index):\n        d.loc[d['MSSubClass'] == s,'MSSubClass'] = idx+1\n\n        \nfor idx, s in enumerate(subclass_map.index):\n    combined.loc[combined['MSSubClass'] == s,'MSSubClass'] = idx+1\n\ntrain_df.groupby('MSSubClass')['SalePrice'].describe().sort_values('mean')","bd9530bb":"combined['MSSubClass'].isnull().sum()","f3ca09cf":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nplt.scatter(y_train, X_train['LotFrontage'])\nplt.show()","1cfaddb8":"# Use mean normalization\ncombined['LotFrontage'] = combined['LotFrontage'].fillna(0)\n\nfor d in all_data:\n    d['LotFrontage'] = d['LotFrontage'].fillna(0)\n    d['LotFrontage'] = (d['LotFrontage']-combined['LotFrontage'].mean())\/combined['LotFrontage'].std()\n    \n    \ncombined['LotFrontage'] = (combined['LotFrontage']-combined['LotFrontage'].mean())\/combined['LotFrontage'].std()","e5a827af":"all_data[0].head()","e23b9044":"plt.plot(y_train, X_train['LotArea'], 'o')\nm, b = np.polyfit(y_train, X_train['LotArea'], 1)\nplt.plot(y_train, m*y_train + b)","f859364d":"# Use mean normalization\ncombined['LotArea'] = combined['LotArea'].fillna(0)\n\nfor d in all_data:\n    d['LotArea'] = d['LotArea'].fillna(0)\n    d['LotArea'] = (d['LotArea']-combined['LotArea'].mean())\/combined['LotArea'].std()\n\ncombined['LotArea'] = (combined['LotArea']-combined['LotArea'].mean())\/combined['LotArea'].std()","aff9e453":"combined['Alley'] = combined['Alley'].fillna('None')\n\nfor d in all_data:\n    d['Alley'] = d['Alley'].fillna('None')","234df2d7":"combined.groupby(['Street', 'Alley']).describe()","32c35b9f":"all_data[0].groupby('Street')['SalePrice'].describe()","485369ea":"combined.drop(['Street','Alley'], axis=1, inplace=True)\n\nfor d in all_data:\n    d.drop(['Street','Alley'], axis=1, inplace=True)","97f65a8b":"combined.drop(['LotShape','LandContour', 'Utilities', 'LandSlope', \\\n               'Condition1', 'Condition2', 'YearBuilt', 'RoofStyle', \\\n               'RoofMatl', 'Heating', 'Electrical', 'Functional', \\\n               'GarageYrBlt', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', \\\n               'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal', \\\n               'SaleType'], axis=1, inplace=True)\n\nfor d in all_data:\n    d.drop(['LotShape','LandContour', 'Utilities', 'LandSlope', \\\n            'Condition1', 'Condition2', 'YearBuilt', 'RoofStyle', \\\n            'RoofMatl', 'Heating', 'Electrical', 'Functional', \\\n            'GarageYrBlt', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', \\\n            'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal', \\\n            'SaleType'], axis=1, inplace=True)","e5907b56":"subclass_map = train_df.groupby('LotConfig')['SalePrice'].mean().sort_values()\nfor d in all_data:\n    for idx, s in enumerate(subclass_map.index):\n        d.loc[d['LotConfig'] == s,'LotConfig'] = idx+1\n\n        \nfor idx, s in enumerate(subclass_map.index):\n    combined.loc[combined['LotConfig'] == s,'LotConfig'] = idx+1\n\ntrain_df.groupby('LotConfig')['SalePrice'].describe().sort_values('mean')","7d647967":"subclass_map = train_df.groupby('Neighborhood')['SalePrice'].mean().sort_values()\nfor d in all_data:\n    for idx, s in enumerate(subclass_map.index):\n        d.loc[d['Neighborhood'] == s,'Neighborhood'] = idx+1\n\n        \nfor idx, s in enumerate(subclass_map.index):\n    combined.loc[combined['Neighborhood'] == s,'Neighborhood'] = idx+1\n\ntrain_df.groupby('Neighborhood')['SalePrice'].describe().sort_values('mean')","46d24038":"# Added new column and used min\/max normalization\n\ncombined['NeighborhoodLotArea'] = combined['Neighborhood'] * combined['LotArea']\n\nfor d in all_data:\n    d['NeighborhoodLotArea'] = d['Neighborhood'] * d['LotArea']\n    d['NeighborhoodLotArea'] = (d['NeighborhoodLotArea'] - combined['NeighborhoodLotArea'].min()) \\\n                                      \/ (combined['NeighborhoodLotArea'].max()-combined['NeighborhoodLotArea'].min())\n\ncombined['NeighborhoodLotArea'] = (combined['NeighborhoodLotArea'] - combined['NeighborhoodLotArea'].min()) \\\n                                  \/ (combined['NeighborhoodLotArea'].max()-combined['NeighborhoodLotArea'].min())","e7f5d007":"subclass_map = train_df.groupby('BldgType')['SalePrice'].mean().sort_values()\nfor d in all_data:\n    for idx, s in enumerate(subclass_map.index):\n        d.loc[d['BldgType'] == s,'BldgType'] = idx+1\n\n        \nfor idx, s in enumerate(subclass_map.index):\n    combined.loc[combined['BldgType'] == s,'BldgType'] = idx+1\n\ntrain_df.groupby('BldgType')['SalePrice'].describe().sort_values('mean')","15c830fe":"# Added new column and used min\/max normalization\n\ncombined['NeighborhoodBldgType'] = combined['Neighborhood'] * combined['BldgType']\n\nfor d in all_data:\n    d['NeighborhoodBldgType'] = d['Neighborhood'] * d['BldgType']\n    d['NeighborhoodBldgType'] = (d['NeighborhoodBldgType'] - combined['NeighborhoodBldgType'].min()) \\\n                                      \/ (combined['NeighborhoodBldgType'].max()-combined['NeighborhoodBldgType'].min())\n\ncombined['NeighborhoodBldgType'] = (combined['NeighborhoodBldgType'] - combined['NeighborhoodBldgType'].min()) \\\n                                  \/ (combined['NeighborhoodBldgType'].max()-combined['NeighborhoodBldgType'].min())","14d03b3c":"combined['TotalBsmtSF'] = combined['TotalBsmtSF'].fillna(0)\ncombined['GarageArea'] = combined['GarageArea'].fillna(0)\n\ncombined['TotalSquareFeet'] = combined['TotalBsmtSF'] + combined['1stFlrSF'] \\\n                            + combined['2ndFlrSF'] + combined['GarageArea'] \\\n                            + combined['WoodDeckSF']\n\nfor d in all_data:\n    d['TotalBsmtSF'] = d['TotalBsmtSF'].fillna(0)\n    d['GarageArea'] = d['GarageArea'].fillna(0)\n    d['TotalSquareFeet'] = d['TotalBsmtSF'] + d['1stFlrSF'] + d['2ndFlrSF'] + d['GarageArea'] + d['WoodDeckSF']\n    d['TotalSquareFeet'] = (d['TotalSquareFeet']-combined['TotalSquareFeet'].mean())\/combined['TotalSquareFeet'].std()\n    \ncombined['TotalSquareFeet'] = (combined['TotalSquareFeet']-combined['TotalSquareFeet'].mean())\/combined['TotalSquareFeet'].std()","a7caaf9e":"subclass_map = train_df.groupby('HouseStyle')['SalePrice'].mean().sort_values()\nfor d in all_data:\n    for idx, s in enumerate(subclass_map.index):\n        d.loc[d['HouseStyle'] == s,'HouseStyle'] = idx+1\n\n        \nfor idx, s in enumerate(subclass_map.index):\n    combined.loc[combined['HouseStyle'] == s,'HouseStyle'] = idx+1\n\ntrain_df.groupby('HouseStyle')['SalePrice'].describe().sort_values('mean')","ba4832c8":"# Added new column and used min\/max normalization\n\ncombined['NeighborhoodHouseStyle'] = combined['Neighborhood'] * combined['HouseStyle']\n\nfor d in all_data:\n    d['NeighborhoodHouseStyle'] = d['Neighborhood'] * d['HouseStyle']\n    d['NeighborhoodHouseStyle'] = (d['NeighborhoodHouseStyle'] - combined['NeighborhoodHouseStyle'].min()) \\\n                                      \/ (combined['NeighborhoodHouseStyle'].max()-combined['NeighborhoodHouseStyle'].min())\n\ncombined['NeighborhoodHouseStyle'] = (combined['NeighborhoodHouseStyle'] - combined['NeighborhoodHouseStyle'].min()) \\\n                                  \/ (combined['NeighborhoodHouseStyle'].max()-combined['NeighborhoodHouseStyle'].min())","3479111e":"# Added new column and used min\/max normalization\n\ncombined['OverallQualNCond'] = combined['OverallQual'] * combined['OverallCond']\n\nfor d in all_data:\n    d['OverallQualNCond'] = d['OverallQual'] * d['OverallCond']\n    d['OverallQualNCond'] = (d['OverallQualNCond'] - combined['OverallQualNCond'].min()) \\\n                                      \/ (combined['OverallQualNCond'].max()-combined['OverallQualNCond'].min())\n\ncombined['OverallQualNCond'] = (combined['OverallQualNCond'] - combined['OverallQualNCond'].min()) \\\n                                  \/ (combined['OverallQualNCond'].max()-combined['OverallQualNCond'].min())","07cdb8d6":"# Added new column and used min\/max normalization\n\ncombined['NeighborhoodOverallQualNCond'] = combined['Neighborhood'] * combined['OverallQualNCond']\n\nfor d in all_data:\n    d['NeighborhoodOverallQualNCond'] = d['Neighborhood'] * d['OverallQualNCond']\n    d['NeighborhoodOverallQualNCond'] = (d['NeighborhoodOverallQualNCond'] - combined['NeighborhoodOverallQualNCond'].min()) \\\n                                      \/ (combined['NeighborhoodOverallQualNCond'].max()-combined['NeighborhoodOverallQualNCond'].min())\n\ncombined['NeighborhoodOverallQualNCond'] = (combined['NeighborhoodOverallQualNCond'] - combined['NeighborhoodOverallQualNCond'].min()) \\\n                                  \/ (combined['NeighborhoodOverallQualNCond'].max()-combined['NeighborhoodOverallQualNCond'].min())","358cb7c4":"max_year = combined['YrSold'].max()\nmax_month = combined.loc[combined['YrSold'] == max_year, 'MoSold'].max()\n\nfor d in all_data:\n    months_ago = (max_year-d['YearRemodAdd'])*12 + max_month\n    d['AdjRemodAdd'] = (months_ago - months_ago.min()) \/ (months_ago.max()-months_ago.min()) \\\n                     * -1 + 1 # reverse the order\n    d.drop(['YearRemodAdd'], axis=1, inplace=True)\n\nmonths_ago = (max_year-combined['YearRemodAdd'])*12 + max_month\ncombined['AdjRemodAdd'] = (months_ago - months_ago.min()) \/ (months_ago.max()-months_ago.min()) \\\n                        * -1 + 1 # reverse the order\ncombined.drop(['YearRemodAdd'], axis=1, inplace=True)","dba0f9c4":"from sklearn.metrics import mean_squared_error\n\nX_train = train_df.drop(['SalePrice'], axis=1)\nplt.plot(y_train, X_train['AdjRemodAdd'], 'o')\nm, b = np.polyfit(y_train, X_train['AdjRemodAdd'], 1)\nplt.plot(y_train, m*y_train + b)\nprint (mean_squared_error(y_train, m*y_train + b))","f6f600e9":"# Get the average of each year\/month\ndf = train_df.groupby(['YrSold','MoSold'])[['SalePrice']].mean()\ndf = df.rename({'SalePrice': 'MonthMean'}, axis=1)\n\n# get the rolling average spanning 3 months\nqt = (df[['MonthMean']] + df[['MonthMean']].shift(1) + df[['MonthMean']].shift(-1)) \/ 3\ndf['QuarterMean'] = qt['MonthMean']\ndf['QuarterMean'] = df['QuarterMean'].fillna(df['MonthMean'])\nprint(df)","08c15ada":"# Merge the averages onto our datasets\nfor idx, d in enumerate(all_data):\n    all_data[idx] = pd.merge(d, df, how='left', left_on=['YrSold','MoSold'], right_on=['YrSold','MoSold'])\n    all_data[idx].drop(['MoSold', 'YrSold'], axis=1, inplace=True)\n    \ncombined = pd.merge(combined, df, how='left', left_on=['YrSold','MoSold'], right_on=['YrSold','MoSold'])\ncombined.drop(['MoSold', 'YrSold'], axis=1, inplace=True)\n\n# Fix the reference since above we generated new references for all_data\ntrain_df = all_data[0]\ntest_df = all_data[1]","c14d6418":"# Normalize the values and drop the old ones\n\nfor d in all_data:\n    d['QuarterMeanNorm'] = (d['QuarterMean']-combined['QuarterMean'].mean())\/combined['QuarterMean'].std()\n    d['MonthMeanNorm'] = (d['MonthMean']-combined['MonthMean'].mean())\/combined['MonthMean'].std()\n    #d.drop(['QuarterMean','MonthMean'], axis=1, inplace=True)\n\ncombined['QuarterMeanNorm'] = (combined['QuarterMean']-combined['QuarterMean'].mean())\/combined['QuarterMean'].std()\ncombined['MonthMeanNorm'] = (combined['MonthMean']-combined['MonthMean'].mean())\/combined['MonthMean'].std()\n#combined.drop(['QuarterMean','MonthMean'], axis=1, inplace=True)","40eb565d":"ex1_map = {'Other': 0, 'BrkFace': 1, 'CemntBd': 2, 'Plywood': 3, 'Wd Sdng': 4, 'MetalSd': 5, 'HdBoard': 6, 'VinylSd': 7}\nex2_map = {'Other': 0, 'AsbShng': 1, 'BrkFace': 2, 'Stucco': 3, 'Wd Shng': 4, 'CmentBd': 5, 'Plywood': 7, 'Wd Sdng': 8, 'HdBoard': 9, 'MetalSd': 10, 'VinylSd': 11}\nmax_map = {'None': 0, 'BrkCmn': 1, 'Stone': 2, 'BrkFace': 3}\nexg_map = {'Po': 0.0, 'Fa': 0.25, 'TA': 0.5, 'Gd': 0.75, 'Ex': 1.0}\n\ncombined['MasVnrArea'] = combined['MasVnrArea'].fillna(0)\n\nfor d in all_data:\n    d['Exterior1st'] = d['Exterior1st'].fillna(combined['Exterior1st'].mode()[0])\n    d['Exterior1st'] = d['Exterior1st'].replace(['AsphShn','CBlock','ImStucc','BrkComm','Stone','AsbShng','Stucco','WdShing'], 'Other')\n    d['Exterior1st'] = d['Exterior1st'].map(ex1_map)\n    d['Exterior2nd'] = d['Exterior2nd'].fillna(combined['Exterior2nd'].mode()[0])\n    d['Exterior2nd'] = d['Exterior2nd'].replace(['CBlock','AsphShn','Stone','Brk Cmn','ImStucc'], 'Other')\n    d['Exterior2nd'] = d['Exterior2nd'].map(ex2_map)\n    d['MasVnrType'] = d['MasVnrType'].fillna('None')\n    d['MasVnrType'] = d['MasVnrType'].map(max_map)\n    d['MasVnrArea'] = d['MasVnrArea'].fillna(0)\n    d['MasVnrArea'] = (d['MasVnrArea'] - combined['MasVnrArea'].min()) \\\n                      \/ (combined['MasVnrArea'].max()-combined['MasVnrArea'].min())\n    d['ExterQual'] = d['ExterQual'].map(exg_map)\n    d['ExterCond'] = d['ExterCond'].map(exg_map)\n\ncombined['Exterior1st'] = combined['Exterior1st'].fillna(combined['Exterior1st'].mode()[0])\ncombined['Exterior1st'] = combined['Exterior1st'].replace(['AsphShn','CBlock','ImStucc','BrkComm','Stone','AsbShng','Stucco','WdShing'], 'Other')\ncombined['Exterior1st'] = combined['Exterior1st'].map(ex1_map)\ncombined['Exterior2nd'] = combined['Exterior2nd'].fillna(combined['Exterior2nd'].mode()[0])\ncombined['Exterior2nd'] = combined['Exterior2nd'].replace(['CBlock','AsphShn','Stone','Brk Cmn','ImStucc'], 'Other')\ncombined['Exterior2nd'] = combined['Exterior2nd'].map(ex2_map)\ncombined['MasVnrType'] = combined['MasVnrType'].fillna('None')\ncombined['MasVnrType'] = combined['MasVnrType'].map(max_map)\ncombined['MasVnrArea'] = (combined['MasVnrArea'] - combined['MasVnrArea'].min()) \\\n                         \/ (combined['MasVnrArea'].max()-combined['MasVnrArea'].min())\ncombined['ExterQual'] = combined['ExterQual'].map(exg_map)\ncombined['ExterCond'] = combined['ExterCond'].map(exg_map)","6541830d":"fnd_map = {'Slab': 0, 'BrkTil': 1, 'CBlock': 2, 'Other': 3, 'PConc': 4}\nbsm_map = {'Po': 0.0, 'Fa': 0.25, 'TA': 0.5, 'Gd': 0.75, 'Ex': 1.0}\nbex_map = {'NA': 0.0, 'No': 0.25, 'Mn': 0.5, 'Av': 0.75, 'Gd': 1.0}\nbfi_map = {'NA': 0.0, 'Unf': 0.17, 'LwQ': 0.33, 'Rec': 0.5, 'BLQ': 0.67, 'ALQ': 0.83, 'GLQ': 1.0}\n\ncombined['BsmtFinSF1'] = combined['BsmtFinSF1'].fillna(0)\ncombined['BsmtFinSF2'] = combined['BsmtFinSF2'].fillna(0)\ncombined['BsmtUnfSF'] = combined['BsmtUnfSF'].fillna(0)\ncombined['TotalBsmtSF'] = combined['TotalBsmtSF'].fillna(0)\ncombined['BsmtFullBath'] = combined['BsmtFullBath'].fillna(0)\ncombined['BsmtHalfBath'] = combined['BsmtFullBath'].fillna(0)\n\nfor d in all_data:\n    d['Foundation'] = d['Foundation'].replace(['Stone', 'Wood'], 'Other')\n    d['Foundation'] = d['Foundation'].map(fnd_map)\n    d['BsmtQual'] = d['BsmtQual'].fillna('Po')\n    d['BsmtQual'] = d['BsmtQual'].map(bsm_map)\n    d['BsmtCond'] = d['BsmtCond'].fillna('Po')\n    d['BsmtCond'] = d['BsmtCond'].map(bsm_map)\n    d['BsmtExposure'] = d['BsmtExposure'].fillna('NA')\n    d['BsmtExposure'] = d['BsmtExposure'].map(bex_map)\n    d['BsmtFinType1'] = d['BsmtFinType1'].fillna('NA')\n    d['BsmtFinType1'] = d['BsmtFinType1'].map(bfi_map)\n    d['BsmtFinType2'] = d['BsmtFinType2'].fillna('NA')\n    d['BsmtFinType2'] = d['BsmtFinType2'].map(bfi_map)\n    d['BsmtFinSF1'] = d['BsmtFinSF1'].fillna(0)\n#     d['BsmtFinSF1'] = (d['BsmtFinSF1'] - combined['BsmtFinSF1'].min()) \\\n#                       \/ (combined['BsmtFinSF1'].max()-combined['BsmtFinSF1'].min())\n    d['BsmtFinSF1'] = (d['BsmtFinSF1']-combined['BsmtFinSF1'].mean())\/combined['BsmtFinSF1'].std()\n    d['BsmtFinSF2'] = d['BsmtFinSF2'].fillna(0)\n#     d['BsmtFinSF2'] = (d['BsmtFinSF2'] - combined['BsmtFinSF2'].min()) \\\n#                       \/ (combined['BsmtFinSF2'].max()-combined['BsmtFinSF2'].min())\n    d['BsmtFinSF2'] = (d['BsmtFinSF2']-combined['BsmtFinSF2'].mean())\/combined['BsmtFinSF2'].std()\n    d['BsmtUnfSF'] = d['BsmtUnfSF'].fillna(0)\n#     d['BsmtUnfSF'] = (d['BsmtUnfSF'] - combined['BsmtUnfSF'].min()) \\\n#                       \/ (combined['BsmtUnfSF'].max()-combined['BsmtUnfSF'].min())\n    d['BsmtUnfSF'] = (d['BsmtUnfSF']-combined['BsmtUnfSF'].mean())\/combined['BsmtUnfSF'].std()\n    d['TotalBsmtSF'] = d['TotalBsmtSF'].fillna(0)\n#     d['TotalBsmtSF'] = (d['TotalBsmtSF'] - combined['TotalBsmtSF'].min()) \\\n#                       \/ (combined['TotalBsmtSF'].max()-combined['TotalBsmtSF'].min())\n    d['TotalBsmtSF'] = (d['TotalBsmtSF']-combined['TotalBsmtSF'].mean())\/combined['TotalBsmtSF'].std()\n    d['BsmtFullBath'] = d['BsmtFullBath'].fillna(0)\n    d['BsmtHalfBath'] = d['BsmtFullBath'].fillna(0)\n    d['BsmtFullBath'] = (d['BsmtFullBath']-combined['BsmtFullBath'].mean())\/combined['BsmtFullBath'].std()\n    d['BsmtHalfBath'] = (d['BsmtHalfBath']-combined['BsmtHalfBath'].mean())\/combined['BsmtHalfBath'].std()\n\ncombined['Foundation'] = combined['Foundation'].replace(['Stone', 'Wood'], 'Other')\ncombined['Foundation'] = combined['Foundation'].map(fnd_map)\ncombined['BsmtQual'] = combined['BsmtQual'].fillna('Po')\ncombined['BsmtQual'] = combined['BsmtQual'].map(bsm_map)\ncombined['BsmtCond'] = combined['BsmtCond'].fillna('Po')\ncombined['BsmtCond'] = combined['BsmtCond'].map(bsm_map)\ncombined['BsmtExposure'] = combined['BsmtExposure'].fillna('NA')\ncombined['BsmtExposure'] = combined['BsmtExposure'].map(bex_map)\ncombined['BsmtFinType1'] = combined['BsmtFinType1'].fillna('NA')\ncombined['BsmtFinType1'] = combined['BsmtFinType1'].map(bfi_map)\ncombined['BsmtFinType2'] = combined['BsmtFinType2'].fillna('NA')\ncombined['BsmtFinType2'] = combined['BsmtFinType2'].map(bfi_map)\n# combined['BsmtFinSF1'] = (combined['BsmtFinSF1'] - combined['BsmtFinSF1'].min()) \\\n#                          \/ (combined['BsmtFinSF1'].max()-combined['BsmtFinSF1'].min())\ncombined['BsmtFinSF1'] = (combined['BsmtFinSF1']-combined['BsmtFinSF1'].mean())\/combined['BsmtFinSF1'].std()\n# combined['BsmtFinSF2'] = (combined['BsmtFinSF2'] - combined['BsmtFinSF2'].min()) \\\n#                          \/ (combined['BsmtFinSF2'].max()-combined['BsmtFinSF2'].min())\ncombined['BsmtFinSF2'] = (combined['BsmtFinSF2']-combined['BsmtFinSF2'].mean())\/combined['BsmtFinSF2'].std()\n# combined['BsmtUnfSF'] = (combined['BsmtUnfSF'] - combined['BsmtUnfSF'].min()) \\\n#                          \/ (combined['BsmtUnfSF'].max()-combined['BsmtUnfSF'].min())\ncombined['BsmtUnfSF'] = (combined['BsmtUnfSF']-combined['BsmtUnfSF'].mean())\/combined['BsmtUnfSF'].std()\n# combined['TotalBsmtSF'] = (combined['TotalBsmtSF'] - combined['TotalBsmtSF'].min()) \\\n#                          \/ (combined['TotalBsmtSF'].max()-combined['TotalBsmtSF'].min())\ncombined['TotalBsmtSF'] = (combined['TotalBsmtSF']-combined['TotalBsmtSF'].mean())\/combined['TotalBsmtSF'].std()\ncombined['BsmtFullBath'] = (combined['BsmtFullBath']-combined['BsmtFullBath'].mean())\/combined['BsmtFullBath'].std()\ncombined['BsmtHalfBath'] = (combined['BsmtHalfBath']-combined['BsmtHalfBath'].mean())\/combined['BsmtHalfBath'].std()","0fb21baa":"ht_map = {'Po': 0.0, 'Fa': 0.25, 'TA': 0.5, 'Gd': 0.75, 'Ex': 1.0}\nyn_map = {'N': 0, 'Y': 1}\n\nfor d in all_data:\n    d['HeatingQC'] = d['HeatingQC'].map(ht_map)\n    d['CentralAir'] = d['CentralAir'].map(yn_map)\n    \ncombined['HeatingQC'] = combined['HeatingQC'].map(ht_map)\ncombined['CentralAir'] = combined['CentralAir'].map(yn_map)","85a7bf07":"rqc_map = {'Po': 0.0, 'Fa': 0.25, 'TA': 0.5, 'Gd': 0.75, 'Ex': 1.0}\n\nfor d in all_data:\n    d['OverallQual'] = (d['OverallQual'] - combined['OverallQual'].min()) \\\n                       \/ (combined['OverallQual'].max()-combined['OverallQual'].min())\n    d['OverallCond'] = (d['OverallCond'] - combined['OverallCond'].min()) \\\n                       \/ (combined['OverallCond'].max()-combined['OverallCond'].min())\n#     d['1stFlrSF'] = (d['1stFlrSF'] - combined['1stFlrSF'].min()) \\\n#                     \/ (combined['1stFlrSF'].max()-combined['1stFlrSF'].min())\n    d['1stFlrSF'] = (d['1stFlrSF']-combined['1stFlrSF'].mean())\/combined['1stFlrSF'].std()\n#     d['2ndFlrSF'] = (d['2ndFlrSF'] - combined['2ndFlrSF'].min()) \\\n#                     \/ (combined['2ndFlrSF'].max()-combined['2ndFlrSF'].min())\n    d['2ndFlrSF'] = (d['2ndFlrSF']-combined['2ndFlrSF'].mean())\/combined['2ndFlrSF'].std()\n#     d['LowQualFinSF'] = (d['LowQualFinSF'] - combined['LowQualFinSF'].min()) \\\n#                     \/ (combined['LowQualFinSF'].max()-combined['LowQualFinSF'].min())\n    d['LowQualFinSF'] = (d['LowQualFinSF']-combined['LowQualFinSF'].mean())\/combined['LowQualFinSF'].std()\n#     d['GrLivArea'] = (d['GrLivArea'] - combined['GrLivArea'].min()) \\\n#                     \/ (combined['GrLivArea'].max()-combined['GrLivArea'].min())\n    d['GrLivArea'] = (d['GrLivArea']-combined['GrLivArea'].mean())\/combined['GrLivArea'].std()\n    d['KitchenQual'] = d['KitchenQual'].fillna(d['KitchenQual'].mode()[0])\n    d['KitchenQual'] = d['KitchenQual'].map(rqc_map)\n    d['FireplaceQu'] = d['FireplaceQu'].fillna('Po')\n    d['FireplaceQu'] = d['FireplaceQu'].map(rqc_map)\n    d['FullBath'] = (d['FullBath']-combined['FullBath'].mean())\/combined['FullBath'].std()\n    d['HalfBath'] = (d['HalfBath']-combined['HalfBath'].mean())\/combined['HalfBath'].std()\n    d['BedroomAbvGr'] = (d['BedroomAbvGr']-combined['BedroomAbvGr'].mean())\/combined['BedroomAbvGr'].std()\n    d['KitchenAbvGr'] = (d['KitchenAbvGr']-combined['KitchenAbvGr'].mean())\/combined['KitchenAbvGr'].std()\n    d['Fireplaces'] = (d['Fireplaces']-combined['Fireplaces'].mean())\/combined['Fireplaces'].std()\n    d['TotRmsAbvGrd'] = (d['TotRmsAbvGrd']-combined['TotRmsAbvGrd'].mean())\/combined['TotRmsAbvGrd'].std()\n\ncombined['OverallQual'] = (combined['OverallQual'] - combined['OverallQual'].min()) \\\n                         \/ (combined['OverallQual'].max()-combined['OverallQual'].min())\ncombined['OverallCond'] = (combined['OverallCond'] - combined['OverallCond'].min()) \\\n                         \/ (combined['OverallCond'].max()-combined['OverallCond'].min())\n# combined['1stFlrSF'] = (combined['1stFlrSF'] - combined['1stFlrSF'].min()) \\\n#                          \/ (combined['1stFlrSF'].max()-combined['1stFlrSF'].min())\ncombined['1stFlrSF'] = (combined['1stFlrSF']-combined['1stFlrSF'].mean())\/combined['1stFlrSF'].std()\n# combined['2ndFlrSF'] = (combined['2ndFlrSF'] - combined['2ndFlrSF'].min()) \\\n#                          \/ (combined['2ndFlrSF'].max()-combined['2ndFlrSF'].min())\ncombined['2ndFlrSF'] = (combined['2ndFlrSF']-combined['2ndFlrSF'].mean())\/combined['2ndFlrSF'].std()\n# combined['LowQualFinSF'] = (combined['LowQualFinSF'] - combined['LowQualFinSF'].min()) \\\n#                          \/ (combined['LowQualFinSF'].max()-combined['LowQualFinSF'].min())\ncombined['LowQualFinSF'] = (combined['LowQualFinSF']-combined['LowQualFinSF'].mean())\/combined['LowQualFinSF'].std()\n# combined['GrLivArea'] = (combined['GrLivArea'] - combined['GrLivArea'].min()) \\\n#                          \/ (combined['GrLivArea'].max()-combined['GrLivArea'].min())\ncombined['GrLivArea'] = (combined['GrLivArea']-combined['GrLivArea'].mean())\/combined['GrLivArea'].std()\ncombined['KitchenQual'] = combined['KitchenQual'].fillna(combined['KitchenQual'].mode()[0])\ncombined['KitchenQual'] = combined['KitchenQual'].map(rqc_map)\ncombined['FireplaceQu'] = combined['FireplaceQu'].fillna('Po')\ncombined['FireplaceQu'] = combined['FireplaceQu'].map(rqc_map)\ncombined['FullBath'] = (combined['FullBath']-combined['FullBath'].mean())\/combined['FullBath'].std()\ncombined['HalfBath'] = (combined['HalfBath']-combined['HalfBath'].mean())\/combined['HalfBath'].std()\ncombined['BedroomAbvGr'] = (combined['BedroomAbvGr']-combined['BedroomAbvGr'].mean())\/combined['BedroomAbvGr'].std()\ncombined['KitchenAbvGr'] = (combined['KitchenAbvGr']-combined['KitchenAbvGr'].mean())\/combined['KitchenAbvGr'].std()\ncombined['Fireplaces'] = (combined['Fireplaces']-combined['Fireplaces'].mean())\/combined['Fireplaces'].std()\ncombined['TotRmsAbvGrd'] = (combined['TotRmsAbvGrd']-combined['TotRmsAbvGrd'].mean())\/combined['TotRmsAbvGrd'].std()","45b4389b":"grt_map = {'None': 0, 'Detchd': 1, 'Other': 2, 'Attchd': 3, 'BuiltIn': 4}\ngrf_map = {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}\ngqc_map = {'Po': 0.0, 'Fa': 0.25, 'TA': 0.5, 'Gd': 0.75, 'Ex': 1.0}\npdr_map = {'N': 0.0, 'P': 0.5, 'Y': 1.0}\n\ncombined['GarageCars'] = combined['GarageCars'].fillna(0)\ncombined['GarageArea'] = combined['GarageArea'].fillna(0)\n\nfor d in all_data:\n    d['GarageType'] = d['GarageType'].fillna('None')\n    d['GarageType'] = d['GarageType'].replace(['CarPort', '2Types', 'Basment'], 'Other')\n    d['GarageType'] = d['GarageType'].map(grt_map)\n    d['GarageFinish'] = d['GarageFinish'].fillna('None')\n    d['GarageFinish'] = d['GarageFinish'].map(grf_map)\n    d['GarageCars'] = d['GarageCars'].fillna(0)\n    d['GarageCars'] = (d['GarageCars']-combined['GarageCars'].mean())\/combined['GarageCars'].std()\n    d['GarageArea'] = d['GarageArea'].fillna(0)\n    d['GarageArea'] = (d['GarageArea']-combined['GarageArea'].mean())\/combined['GarageArea'].std()\n    d['GarageQual'] = d['GarageQual'].fillna('Po')\n    d['GarageQual'] = d['GarageQual'].map(gqc_map)\n    d['GarageCond'] = d['GarageCond'].fillna('Po')\n    d['GarageCond'] = d['GarageCond'].map(gqc_map)\n    d['PavedDrive'] = d['PavedDrive'].map(pdr_map)\n    \ncombined['GarageType'] = combined['GarageType'].fillna('None')\ncombined['GarageType'] = combined['GarageType'].replace(['CarPort', '2Types', 'Basment'], 'Other')\ncombined['GarageType'] = combined['GarageType'].map(grt_map)\ncombined['GarageFinish'] = combined['GarageFinish'].fillna('None')\ncombined['GarageFinish'] = combined['GarageFinish'].map(grf_map)\ncombined['GarageCars'] = (combined['GarageCars']-combined['GarageCars'].mean())\/combined['GarageCars'].std()\ncombined['GarageArea'] = (combined['GarageArea']-combined['GarageArea'].mean())\/combined['GarageArea'].std()\ncombined['GarageQual'] = combined['GarageQual'].fillna('Po')\ncombined['GarageQual'] = combined['GarageQual'].map(gqc_map)\ncombined['GarageCond'] = combined['GarageCond'].fillna('Po')\ncombined['GarageCond'] = combined['GarageCond'].map(gqc_map)\ncombined['PavedDrive'] = combined['PavedDrive'].map(pdr_map)","a265f1cf":"plt.plot(y_train, X_train['WoodDeckSF'], 'o')\nm, b = np.polyfit(y_train, X_train['WoodDeckSF'], 1)\nplt.plot(y_train, m*y_train + b)","a7bda66d":"for d in all_data:\n    d['WoodDeckSF'] = (d['WoodDeckSF']-combined['WoodDeckSF'].mean())\/combined['WoodDeckSF'].std()\n    d['OpenPorchSF'] = (d['OpenPorchSF']-combined['OpenPorchSF'].mean())\/combined['OpenPorchSF'].std()\n    \ncombined['WoodDeckSF'] = (combined['WoodDeckSF']-combined['WoodDeckSF'].mean())\/combined['WoodDeckSF'].std()\ncombined['OpenPorchSF'] = (combined['OpenPorchSF']-combined['OpenPorchSF'].mean())\/combined['OpenPorchSF'].std()","d073798f":"sac_map = {'Other': 0, 'Abnorml': 1, 'Normal': 2, 'Partial': 3}\n\nfor d in all_data:\n    d['SaleCondition'] = d['SaleCondition'].replace(['AdjLand', 'Family', 'Alloca'], 'Other')\n    d['SaleCondition'] = d['SaleCondition'].map(sac_map)\n    \ncombined['SaleCondition'] = combined['SaleCondition'].replace(['AdjLand', 'Family', 'Alloca'], 'Other')\ncombined['SaleCondition'] = combined['SaleCondition'].map(sac_map)","4d14c2dd":"pd.set_option('display.max_columns', None)\ntrain_df.head(10)","f4d8be0c":"field = 'SaleCondition'\nprint(f'{combined[field].isnull().sum()} or {combined[field].isnull().sum() \/ 2919. * 100}% empty')\ntrain_df.groupby(field)['SalePrice'].describe().sort_values('mean')","a1fa2df6":"train_df.to_csv('new_train.csv', index=False)\ntest_df.to_csv('new_test.csv', index=False)\nprint(\"New files have been created.\")","ed6aac6b":"# import numpy as np\n# import pandas as pd\n# from sklearn.metrics import mean_squared_log_error\n\nX = pd.read_csv(\"\/kaggle\/working\/new_train.csv\")\ny = pd.read_csv(\"\/kaggle\/working\/new_test.csv\")\n\nall_data = [X,y]\n\npd.set_option('display.max_columns', None)\nX.head(20)","a27ae9e6":"B = pd.concat([X,y])\n\nB = pd.concat([B, pd.get_dummies(B['Neighborhood'], prefix='Neighborhood', drop_first=True)], axis=1)\nB = pd.concat([B, pd.get_dummies(B['LotConfig'], prefix='LotConfig', drop_first=True)], axis=1)\nB = pd.concat([B, pd.get_dummies(B['BldgType'], prefix='BldgType', drop_first=True)], axis=1)\nB = pd.concat([B, pd.get_dummies(B['HouseStyle'], prefix='HouseStyle', drop_first=True)], axis=1)\nB = pd.concat([B, pd.get_dummies(B['SaleCondition'], prefix='SaleCondition', drop_first=True)], axis=1)\nB = pd.concat([B, pd.get_dummies(B['GarageFinish'], prefix='GarageFinish', drop_first=True)], axis=1)\nB = pd.concat([B, pd.get_dummies(B['GarageType'], prefix='GarageType', drop_first=True)], axis=1)\nB = pd.concat([B, pd.get_dummies(B['Foundation'], prefix='Foundation', drop_first=True)], axis=1)\n\n#B.drop(['Neighborhood'], axis=1, inplace=True)\nB.drop(['LotConfig', 'BldgType', 'HouseStyle', 'SaleCondition', 'GarageFinish', 'GarageType', 'Foundation'], axis=1, inplace=True)\nB.drop(['MasVnrType', 'Exterior2nd', 'Exterior1st', 'MSZoning', 'MSSubClass'], axis=1, inplace=True)\n\nB.head(20)","b357776c":"X = B[:len(X)]\ny = B[len(X):]\ny = y.drop('SalePrice', axis=1)\n\nlen(X)","53d9a6e1":"X.to_csv('new_train_1h.csv', index=False)\ny.to_csv('new_test_1h.csv', index=False)\nprint(\"New files have been created.\")","4bd0b1c6":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_log_error\n\n# X_train = pd.read_csv(\"new_train.csv\")\n# X_test = pd.read_csv(\"new_test.csv\")\nX = pd.read_csv(\"\/kaggle\/working\/new_train_1h.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/working\/new_test_1h.csv\")","8f943150":"train_df = X.sample(frac=0.6,random_state=200)\nevaluate_df = X.drop(train_df.index)\n\nX_train = train_df.drop(['Id','SalePrice'], axis=1)\nY_train = train_df[\"SalePrice\"]\nX_train_full = X.drop(['Id','SalePrice'], axis=1)\nY_train_full = X[\"SalePrice\"]\nX_eval  = evaluate_df.drop(['Id','SalePrice'], axis=1)\nY_eval  = evaluate_df[\"SalePrice\"]\nX_test = test_df.drop('Id', axis=1)\n\nlen(X_train_full.columns)","03255a83":"#from sklearn.ensemble import GradientBoostingRegressor\nimport xgboost as xgb\n\n# grad_boost = GradientBoostingRegressor(random_state=0, n_estimators=500, max_features= 0.3)\n# grad_boost_full = GradientBoostingRegressor(random_state=0, n_estimators=500, max_features= 0.3)\ngrad_boost = xgb.XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=5, min_child_weight=1.5, n_estimators=1900, \n             n_jobs=1, nthread=None, objective='reg:squarederror',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)\ngrad_boost_full = xgb.XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=5, min_child_weight=1.5, n_estimators=1900, \n             n_jobs=1, nthread=None, objective='reg:squarederror',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)\nnum_feat_to_drop = 1\ndropped_features = []\ni = 0\nbest_acc = 1\n\nwhile len(X_train.columns) > 5:\n    # Train our model with the set\n    grad_boost.fit(X_train, Y_train)\n    grad_boost_full.fit(X_train_full, Y_train_full)\n    \n    # Determine our accurancy with this model\n    acc_train = np.sqrt(mean_squared_log_error(Y_train, grad_boost.predict(X_train)))\n    acc_eval = np.sqrt(mean_squared_log_error(Y_eval, grad_boost.predict(X_eval)))\n    acc_full = np.sqrt(mean_squared_log_error(Y_train_full, grad_boost_full.predict(X_train_full)))\n    avg_acc = (acc_train + acc_eval + acc_full) \/ 3\n    if (avg_acc >= best_acc):\n        print (f'{i}: Train = {acc_train}  Evaluate = {acc_eval} Full = {acc_full} Average = {avg_acc}')\n    else:\n        best_acc = avg_acc\n        print (f'{i}: Train = {acc_train}  Evaluate = {acc_eval} Full = {acc_full} Average = {avg_acc} - NEW BEST!!!')\n        print (f'  Dropped: {dropped_features}')\n    \n    # Determine the least important features\n    importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(grad_boost.feature_importances_,3)})\n    importances = importances.sort_values('importance',ascending=True).set_index('feature')\n    \n    # Drop the X least important features\n    features_to_drop = importances[0:num_feat_to_drop].index.tolist()\n    if len(features_to_drop) <= 0:\n        break\n    X_train.drop(features_to_drop, axis=1, inplace=True)\n    X_eval.drop(features_to_drop, axis=1, inplace=True)\n    X_train_full.drop(features_to_drop, axis=1, inplace=True)\n    dropped_features.append(features_to_drop)\n    \n    i += num_feat_to_drop","8b5f7345":"# this is the result of my run on my own machine so it will differ with Kaggle\nbest_dropped = [['LotConfig_4'], ['Foundation_3'], ['Neighborhood_3'], ['BldgType_2'], ['Neighborhood_21'], ['Neighborhood_12'], ['Neighborhood_8'], ['HouseStyle_8'], ['BldgType_3'], ['HouseStyle_2'], ['Neighborhood_10']]\nbest_dropped = [x for inner in best_dropped for x in inner]\nbest_dropped","d30b0cdb":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_log_error\n\nX = pd.read_csv(\"\/kaggle\/working\/new_train_1h.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/working\/new_test_1h.csv\")\n\ntrain_df = X.sample(frac=0.6,random_state=200)\nevaluate_df = X.drop(train_df.index)\n\nX_train = train_df.drop(['Id','SalePrice'], axis=1)\nY_train = train_df[\"SalePrice\"]\nX_train_full = X.drop(['Id','SalePrice'], axis=1)\nY_train_full = X[\"SalePrice\"]\nX_eval  = evaluate_df.drop(['Id','SalePrice'], axis=1)\nY_eval  = evaluate_df[\"SalePrice\"]\nX_test = test_df.drop('Id', axis=1)","5da9b78d":"# Check if any columns have null values\nfor (columnName, columnData) in X_test.iteritems():\n    print (f'{columnName}: {X_test[columnName].isna().sum()}')","cba471d2":"pd.set_option('display.max_columns', None)\nX_test.describe()","ee045821":"drop_list = ['LotConfig_4', 'Foundation_3', 'Neighborhood_3', 'BldgType_2', 'Neighborhood_21', 'Neighborhood_12', 'Neighborhood_8', 'HouseStyle_8', 'BldgType_3', 'HouseStyle_2', 'Neighborhood_10']\n\nX_train.drop(drop_list, axis=1,inplace=True)\nX_eval.drop(drop_list, axis=1,inplace=True)\nX_train_full.drop(drop_list, axis=1,inplace=True)\nX_test.drop(drop_list, axis=1,inplace=True)","b95c5b64":"from sklearn import linear_model\n\nlr = linear_model.LinearRegression()\nlr.fit(X_train, Y_train)\n\nacc_train_lr = np.sqrt(mean_squared_log_error(Y_train, lr.predict(X_train)))\nacc_eval_lr = np.sqrt(mean_squared_log_error(Y_eval, lr.predict(X_eval)))\n\nprint (f'LR: Train = {acc_train_lr}  Evaluate = {acc_eval_lr}')","7b2c0562":"from sklearn import linear_model\n\nsgd = linear_model.SGDClassifier(random_state=1, max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\n\nacc_train_sgd = np.sqrt(mean_squared_log_error(Y_train, sgd.predict(X_train)))\nacc_eval_sgd = np.sqrt(mean_squared_log_error(Y_eval, sgd.predict(X_eval)))\n\nprint (f'SGD: Train = {acc_train_sgd}  Evaluate = {acc_eval_sgd}')","a71958c2":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nacc_train_rf = np.sqrt(mean_squared_log_error(Y_train, random_forest.predict(X_train)))\nacc_eval_rf = np.sqrt(mean_squared_log_error(Y_eval, random_forest.predict(X_eval)))\n\nprint (f'RF: Train = {acc_train_rf}  Evaluate = {acc_eval_rf}')","e4313d01":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly_features = PolynomialFeatures(degree=1)\nX_train_poly = poly_features.fit_transform(X_train)\nX_eval_poly = poly_features.fit_transform(X_eval)\npoly_model = LinearRegression()\npoly_model.fit(X_train_poly, Y_train)\n\nacc_train_pr = np.sqrt(mean_squared_log_error(Y_train, poly_model.predict(X_train_poly)))\nacc_eval_pr = np.sqrt(mean_squared_log_error(Y_eval, abs(poly_model.predict(X_eval_poly))))\n\nprint (f'PR: Train = {acc_train_pr}  Evaluate = {acc_eval_pr}')","109d7e9d":"from sklearn.linear_model import LogisticRegression\n\n#logreg = LogisticRegression(random_state=1, max_iter=1000)\nlogreg = LogisticRegression(random_state=1)\nlogreg.fit(X_train, Y_train)\n\nacc_train_lr = np.sqrt(mean_squared_log_error(Y_train, logreg.predict(X_train)))\nacc_eval_lr = np.sqrt(mean_squared_log_error(Y_eval, logreg.predict(X_eval)))\n\nprint (f'LR: Train = {acc_train_lr}  Evaluate = {acc_eval_lr}')","07476a82":"from sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\n\nacc_train_gnb = np.sqrt(mean_squared_log_error(Y_train, gaussian.predict(X_train)))\nacc_eval_gnb = np.sqrt(mean_squared_log_error(Y_eval, gaussian.predict(X_eval)))\n\nprint (f'GNB: Train = {acc_train_gnb}  Evaluate = {acc_eval_gnb}')","1f11f915":"from sklearn.linear_model import Perceptron\n\nperceptron = Perceptron(random_state=1, max_iter=50)\nperceptron.fit(X_train, Y_train)\n\nacc_train_prc = np.sqrt(mean_squared_log_error(Y_train, perceptron.predict(X_train)))\nacc_eval_prc = np.sqrt(mean_squared_log_error(Y_eval, perceptron.predict(X_eval)))\n\nprint (f'Perceptron: Train = {acc_train_prc}  Evaluate = {acc_eval_prc}')","f8634cb5":"from sklearn.svm import SVC, LinearSVC\n\n#linear_svc = LinearSVC(random_state=1, max_iter=100000)\nlinear_svc = LinearSVC(random_state=1)\nlinear_svc.fit(X_train, Y_train)\n\nacc_train_svm = np.sqrt(mean_squared_log_error(Y_train, linear_svc.predict(X_train)))\nacc_eval_svm = np.sqrt(mean_squared_log_error(Y_eval, linear_svc.predict(X_eval)))\n\nprint (f'SVM: Train = {acc_train_svm}  Evaluate = {acc_eval_svm}')","ef2c7a94":"from sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier(random_state=1)\ndecision_tree.fit(X_train, Y_train)\n\nacc_train_dt = np.sqrt(mean_squared_log_error(Y_train, decision_tree.predict(X_train)))\nacc_eval_dt = np.sqrt(mean_squared_log_error(Y_eval, decision_tree.predict(X_eval)))\n\nprint (f'DT: Train = {acc_train_dt}  Evaluate = {acc_eval_dt}')","7212a4bf":"from sklearn.ensemble import RandomForestRegressor\n\nrandom_forest_reg = RandomForestRegressor(random_state=1, n_estimators=100)\nrandom_forest_reg.fit(X_train, Y_train)\n\nacc_train_rfr = np.sqrt(mean_squared_log_error(Y_train, random_forest_reg.predict(X_train)))\nacc_eval_rfr = np.sqrt(mean_squared_log_error(Y_eval, random_forest_reg.predict(X_eval)))\n\nprint (f'RFR: Train = {acc_train_rfr}  Evaluate = {acc_eval_rfr}')","860e0243":"## Current Best\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngrad_boost = GradientBoostingRegressor(random_state=1, n_estimators=500, max_features= 0.3)\ngrad_boost.fit(X_train, Y_train)\n\nacc_train_gb = np.sqrt(mean_squared_log_error(Y_train, grad_boost.predict(X_train)))\nacc_eval_gb = np.sqrt(mean_squared_log_error(Y_eval, grad_boost.predict(X_eval)))\n\nprint (f'GB: Train = {acc_train_gb}  Evaluate = {acc_eval_gb}')","8b593b60":"from sklearn.ensemble import AdaBoostRegressor\n\nada_boost = AdaBoostRegressor(random_state=1, n_estimators=100)\nada_boost.fit(X_train, Y_train)\n\nacc_train_ab = np.sqrt(mean_squared_log_error(Y_train, ada_boost.predict(X_train)))\nacc_eval_ab = np.sqrt(mean_squared_log_error(Y_eval, ada_boost.predict(X_eval)))\n\nprint (f'AdaBoost: Train = {acc_train_ab}  Evaluate = {acc_eval_ab}')","1d41520e":"from sklearn.ensemble import ExtraTreesRegressor\n\nex_trees = ExtraTreesRegressor(random_state=1, n_estimators=100)\nex_trees.fit(X_train, Y_train)\n\nacc_train_et = np.sqrt(mean_squared_log_error(Y_train, ex_trees.predict(X_train)))\nacc_eval_et = np.sqrt(mean_squared_log_error(Y_eval, ex_trees.predict(X_eval)))\n\nprint (f'ET: Train = {acc_train_et}  Evaluate = {acc_eval_et}')","9054ec1e":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import VotingRegressor\nimport xgboost as xgb\n\nreg1 = GradientBoostingRegressor(random_state=1, n_estimators=100)\nreg2 = RandomForestRegressor(random_state=1, n_estimators=100)\nreg3 = ExtraTreesRegressor(random_state=1, n_estimators=100)\nreg4 = AdaBoostRegressor(random_state=1, n_estimators=100)\nreg5 = xgb.XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=4, min_child_weight=1.5, n_estimators=2400,\n             n_jobs=1, nthread=None, objective='reg:squarederror',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)\nereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('et', reg3), ('ad', reg4), ('xg', reg5)])\nereg = ereg.fit(X_train, Y_train)\n\nacc_train_vr = np.sqrt(mean_squared_log_error(Y_train, ereg.predict(X_train)))\nacc_eval_vr = np.sqrt(mean_squared_log_error(Y_eval, ereg.predict(X_eval)))\n\nprint (f'VR: Train = {acc_train_vr}  Evaluate = {acc_eval_vr}')","c4d648f2":"from sklearn.linear_model import RidgeCV, LassoCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import StackingRegressor\n\nest_cnt = 100\nseed = 42\n\nfinal_estimator = xgb.XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=4, min_child_weight=1.5, n_estimators=2400,\n             n_jobs=1, nthread=None, objective='reg:squarederror',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)\n\nfinal_layer = StackingRegressor(\n    estimators=[\n                ('abar', AdaBoostRegressor(random_state=seed, n_estimators=est_cnt))\n                #,('extr', ExtraTreesRegressor(random_state=seed, n_estimators=est_cnt))\n                ,('adar', AdaBoostRegressor(random_state=seed, n_estimators=est_cnt))\n                ,('gbrt', GradientBoostingRegressor(random_state=seed, n_estimators=est_cnt))\n                #,('rf', RandomForestRegressor(random_state=seed, n_estimators=est_cnt))\n               ],\n    final_estimator=final_estimator\n)\n\nsecond_layer = StackingRegressor(\n    estimators=[\n                ('abar', AdaBoostRegressor(random_state=seed, n_estimators=est_cnt))\n                ,('extr', ExtraTreesRegressor(random_state=seed, n_estimators=est_cnt))\n                ,('adar', AdaBoostRegressor(random_state=seed, n_estimators=est_cnt))\n                ,('gbrt', GradientBoostingRegressor(random_state=seed, n_estimators=est_cnt))\n                ,('rf', RandomForestRegressor(random_state=seed, n_estimators=est_cnt))\n               ],\n    final_estimator=final_layer\n)\n\nmulti_layer_regressor = StackingRegressor(\n    estimators=[\n                #('ridge', RidgeCV())\n                ('lasso', LassoCV(random_state=seed))\n                ,('svr', SVR(C=1, gamma=1e-6, kernel='rbf'))\n                ,('etr', ExtraTreesRegressor(random_state=seed, n_estimators=est_cnt))\n               ],\n    final_estimator=second_layer\n)\n\nmulti_layer_regressor.fit(X_train, Y_train)\n\nacc_train_sg = np.sqrt(mean_squared_log_error(Y_train, multi_layer_regressor.predict(X_train)))\nacc_eval_sg = np.sqrt(mean_squared_log_error(Y_eval, multi_layer_regressor.predict(X_eval)))\n\nprint (f'SG: Train = {acc_train_sg}  Evaluate = {acc_eval_sg}')","4da7c271":"import xgboost as xgb\n\n\ngbm = xgb.XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.7, gamma=2,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=5, min_child_weight=0, n_estimators=1900, \n             n_jobs=1, nthread=None, objective='reg:squarederror',\n             reg_alpha=0.6, reg_lambda=0.3, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1).fit(X_train, Y_train)\nacc_train_xg = np.sqrt(mean_squared_log_error(Y_train, gbm.predict(X_train)))\nacc_eval_xg = np.sqrt(mean_squared_log_error(Y_eval, gbm.predict(X_eval)))\n\nprint (f'XG: Train = {acc_train_xg}  Evaluate = {acc_eval_xg}')","242e3903":"import xgboost as xgb\n\nbest_reg = xgb.XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=5, min_child_weight=1.5, n_estimators=1900, \n             n_jobs=1, nthread=None, objective='reg:squarederror',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)\nbest_reg.fit(X_train_full, Y_train_full)\n\nacc_train_full = np.sqrt(mean_squared_log_error(Y_train_full, best_reg.predict(X_train_full)))\n\nprint (f'Best (previous): Train = {acc_train_xg}  Evaluate = {acc_eval_xg}')\nprint (f'Best FULL Train = {acc_train_full}')\n\npredictions = best_reg.predict(X_test)\noutput = pd.DataFrame({'Id': test_df.Id, 'SalePrice': predictions})\noutput.to_csv('my_submission.csv', index=False)","fa700e17":"import matplotlib.pyplot as plt\n\ntest_predictions = best_reg.predict(X_train_full).flatten()\n\na = plt.axes(aspect='equal')\nplt.scatter(Y_train_full, test_predictions)\nplt.xlabel('True Values')\nplt.ylabel('Predictions')\nlims = [0, 1000000]\nplt.xlim(lims)\nplt.ylim(lims)\n_ = plt.plot(lims, lims)","9e02f575":"<a id=\"subsection-RanForReg\"><\/a>\n## Random Forest Regressor","21c17eb3":"<a id=\"subsection-Results\"><\/a>\n## Results","950bb734":"<a id=\"section-training\"><\/a>\n# Training","87fb7e3d":"<a id=\"subsection-SaleInfo\"><\/a>\n## Sale Info","67f32e9a":"<a id=\"subsection-ExtRanTree\"><\/a>\n## Extremely Randomized Trees","9b44f187":"<a id=\"subsection-GradBooReg\"><\/a>\n## Gradient Boost Regressor","faced28b":"<a id=\"section-onehot\"><\/a>\n# One-Hot Encoding\n\nSo I know some of you might be thinking why I have a whole different section for this.  Well, to be honest, one-hot encoding was an after thought for me.  After I did the above data cleanup, I tested it and got good results and wanted to see what it would look like if I did one-hot encoding and I got better results so I kept it in.","e6415aaf":"<a id=\"subsection-NeighborhoodLotArea\"><\/a>\n## NeighborhoodLotArea","90df96da":"<a id=\"subsection-GNB\"><\/a>\n## Gaussian Naive Bayes","e254af89":"<a id=\"subsection-Garage\"><\/a>\n## Garage","a75c0cb1":"<a id=\"subsection-LogReg\"><\/a>\n## Logistic Regression","7587b0c3":"<a id=\"subsection-HeatCool\"><\/a>\n## Heating \/ Cooling","eaea8c2e":"<a id=\"subsection-FondBase\"><\/a>\n## Foundation and Basement","cc826416":"<a id=\"subsection-LotFrontage\"><\/a>\n## LotFrontage","ee8b7f2a":"<a id=\"subsection-RandFore\"><\/a>\n## Random Forest Classifier\nI don't know why I did this.  This isn't a classifier problem...","b96204d4":"<a id=\"subsection-Exterior\"><\/a>\n## Exterior","351dae07":"### Summary\nI don't see much coorelation directly between this and sale price but not sure at this point.  For now, will check other features to see if this might be useful in some way or if it should be dropped.","ce2b9971":"### Summary:\nLot frontage is loosey coorelated to sale price so we probably have to add some features to take advantage of this.  I've put these into categories for now.","df70554c":"### Summary\nBased on all this, I think we can default the MSZoning null values (which only appear in the test data set) with the most frequent value 'RL'. We can also convert 'RL' to number.","5b298c32":"<a id=\"subsection-Perceptron\"><\/a>\n## Perceptron","5babe23b":"<a id=\"subsection-PolyReg\"><\/a>\n## Polynomial Regression","bcfa1bf2":"<a id=\"subsection-NeighborhoodOverallQualNCond\"><\/a>\n## NeighborhoodOverallQualNCond","d9b531fb":"<a id=\"subsection-OverallQualNCond\"><\/a>\n## OverallQualNCond","f985f05b":"<a id=\"subsection-FloorRoom\"><\/a>\n## Floor Sizes and Room Conditions","9dcff468":"<a id=\"subsection-NeighborhoodHouseStyle\"><\/a>\n## NeighborhoodHouseStyle","1c07e885":"<a id=\"subsection-SVM\"><\/a>\n## Linear Support Vector Machine","35965046":"<a id=\"subsection-DropFields\"><\/a>\n## Drop unused fields\nHere is where I decided which fields I wouldn't use at all.  I did some analysis on these elsewhere but I also trusted my gut on others.","5956362c":"<a id=\"subsection-LinearReg\"><\/a>\n## Linear Regression","08597619":"<a id=\"section-columnselect\"><\/a>\n# Column Selection","14cb6869":"<a id=\"subsection-HouseStyle\"><\/a>\n## HouseStyle","ec75c6d7":"<a id=\"subsection-Other\"><\/a>\n## Other Features","d2bd96d2":"### Summary\nI don't think either of these features really contribute at all so I'm going to drop them.","b099bf6b":"<a id=\"subsection-DesiTree\"><\/a>\n## Decision Tree","2bd10ffc":"<a id=\"subsection-Neighborhood\"><\/a>\n## Neighborhood","98b52fb7":"<a id=\"subsection-AdaBoost\"><\/a>\n## AdaBoost","b07b82e7":"<a id=\"subsection-LotConfig\"><\/a>\n## LotConfig","29dd0d99":"**Final Column Drop**\n\nUse the drop list from above","5567bf64":"# Housing Prices - My Submission\n\nI created this notebook to share my work with my friends over at #kaggle_competitions for the Machine Learning Foundations Scholarship with Microsoft Azure.\n\n### A brief disclamer before I get started\n\nThis is a copy of what I did in Jupyter notebooks on my local machine using Anaconda 4.8.2 (Python 3.7.6).  I know there are some differences between Kaggle's version of python libraries and my own so I don't know if all this will work verbatim in Kaggle and give the same results as I got but I did give it a run through and it seems to return similar figures.  You can find a copy of my Jupyter notebooks here: https:\/\/github.com\/cassova\/Kaggle-HousingPrices\n\n* [Data Cleanup](#section-data-cleanup)\n    - [MSZoning](#subsection-MSZoning)\n    - [MSSubClass](#subsection-MSSubClass)\n    - [LotFrontage](#subsection-LotFrontage)\n    - [LotArea](#subsection-LotArea)\n    - [Street and Alley](#subsection-StreetAlley)\n    - [Drop unused fields](#subsection-DropFields)\n    - [LotConfig](#subsection-LotConfig)\n    - [Neighborhood](#subsection-Neighborhood)\n    - [NeighborhoodLotArea](#subsection-NeighborhoodLotArea)\n    - [BldgType](#subsection-BldgType)\n    - [NeighborhoodBldgType](#subsection-NeighborhoodBldgType)\n    - [Total Square Feet](#subsection-TotalSquareFeet)\n    - [HouseStyle](#subsection-HouseStyle)\n    - [NeighborhoodHouseStyle](#subsection-NeighborhoodHouseStyle)\n    - [OverallQualNCond](#subsection-OverallQualNCond)\n    - [NeighborhoodOverallQualNCond](#subsection-NeighborhoodOverallQualNCond)\n    - [Age](#subsection-Age)\n    - [Average Sale Price](#subsection-AverageSalePrice)\n    - [Exterior](#subsection-Exterior)\n    - [Foundation and Basement](#subsection-FondBase)\n    - [Heating \/ Cooling](#subsection-HeatCool)\n    - [Floor Sizes and Room Conditions](#subsection-FloorRoom)\n    - [Garage](#subsection-Garage)\n    - [Other Features](#Other)\n    - [Sale Info](#subsection-SaleInfo)\n    - [Results](#subsection-Results)\n* [One-Hot Encoding](#section-onehot)\n* [Column Selection](#section-columnselect)\n* [Training](#section-training)\n    - [Linear Regression](#subsection-LinearReg)\n    - [Stochastic Gradient Descent (SGD)](#subsection-StoGradDes)\n    - [Random Forest Classifier](#subsection-RandFore)\n    - [Polynomial Regression](#subsection-PolyReg)\n    - [Logistic Regression](#subsection-LogReg)\n    - [Gaussian Naive Bayes](#subsection-GNB)\n    - [Perceptron](#subsection-Perceptron)\n    - [Linear Support Vector Machine](#subsection-SVM)\n    - [Decision Tree](#subsection-DesiTree)\n    - [Random Forest Regressor](#subsection-RanForReg)\n    - [Gradient Boost Regressor](#subsection-GradBooReg)\n    - [AdaBoost](#subsection-AdaBoost)\n    - [Extremely Randomized Trees](#subsection-ExtRanTree)\n    - [Ensemble: VotingRegessor](#subsection-EnsVotReg)\n    - [Ensemble: Stacked Generalization](#subsection-EnsStakGen)\n    - [XGBoost](#subsection-XGBoost)\n    - [Best](#subsection-Best)\n","661cab81":"<a id=\"subsection-StreetAlley\"><\/a>\n## Street and Alley","d22bbbc0":"### Summary\nThe lot area is correlated to the price so we'll normalize it.","c4d1e44e":"<a id=\"subsection-LotArea\"><\/a>\n## LotArea","58c5256c":"### Output new files","6a8eb012":"<a id=\"subsection-BldgType\"><\/a>\n## BldgType","52b82d50":"<a id=\"section-data-cleanup\"><\/a>\n# Data Cleanup\n\nHere I went through all the columns and either dropped them outright or cleaned them.  I cleaned categorical items by converting them to numbers sorting their means by sale price.  I normalized specifications like number of rooms, area, and others using mean normalization (z-score).  Others were normalized with min-max.","4eb0bcbb":"<a id=\"subsection-NeighborhoodBldgType\"><\/a>\n## NeighborhoodBldgType","52ef0473":"<a id=\"subsection-EnsVotReg\"><\/a>\n## Ensemble: VotingRegessor","e92b5ba1":"<a id=\"subsection-MSZoning\"><\/a>\n## MSZoning","cd7a1707":"<a id=\"subsection-StoGradDes\"><\/a>\n## Stochastic Gradient Descent (SGD)","f4337505":"<a id=\"subsection-MSSubClass\"><\/a>\n## MSSubClass","27205b60":"<a id=\"subsection-TotalSquareFeet\"><\/a>\n## Total Square Feet","f0ec73d7":"<a id=\"subsection-XGBoost\"><\/a>\n## XGBoost","acf68501":"<a id=\"subsection-Best\"><\/a>\n## Best","042cbf74":"<a id=\"subsection-EnsStakGen\"><\/a>\n## Ensemble: Stacked Generalization","1005271b":"<a id=\"subsection-Age\"><\/a>\n## Age\nWe'll determine this by getting the max MoSold\/YrSold and count the months back to last remodelled since we determined build date is less correlated than remodelled date (assuming built & remodelled on 1st month of the year)","42882d1e":"<a id=\"subsection-AverageSalePrice\"><\/a>\n## Average Sale Price"}}