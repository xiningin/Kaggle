{"cell_type":{"610f09f5":"code","047f1410":"code","36afe480":"code","e97ba9fc":"code","d67a46fc":"code","ccee00d3":"code","b0527b21":"code","9d7e8ee7":"code","b5411f3a":"code","b04daada":"code","114ffe9e":"code","d2ea930c":"code","bea1902c":"code","654caa36":"code","e5213a91":"code","a4aa9452":"code","68b9c6be":"code","f8cc6f8c":"code","8e761c56":"code","07e9c655":"code","49c20431":"code","2ccc6c59":"code","7815fb88":"markdown","05220453":"markdown","3c12ab76":"markdown","e1e5475d":"markdown","7eab4be5":"markdown","1483fa09":"markdown","835eb677":"markdown","449699ec":"markdown","c6264d1d":"markdown","f1970ef8":"markdown","76fe1461":"markdown","ebad746b":"markdown","3cbe0154":"markdown","b67927fd":"markdown","2c7bd294":"markdown","60640ea9":"markdown"},"source":{"610f09f5":"!pip install pdfminer","047f1410":"!pip install docxpy","36afe480":"!pip install tika","e97ba9fc":"from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\nfrom pdfminer.converter import TextConverter\nfrom pdfminer.layout import LAParams\nfrom pdfminer.pdfpage import PDFPage\nfrom io import StringIO\nfrom os.path import splitext\nimport os\nimport re\nimport nltk\nimport pandas as pd\nimport numpy as np\nimport docxpy\nfrom tika import parser\nimport warnings\nimport seaborn as sns\nwarnings.filterwarnings('ignore')","d67a46fc":"\ndef splitext_(path):\n    if len(path.split('.')) > 2:\n        return path.split('.')[0],'.'.join(path.split('.')[-2:])\n    return splitext(path)\n\ndef text_preprocess(text):\n    cleaned_text =  re.sub(r\"[^a-zA-Z]\", ' ', text)  \n    return cleaned_text\n\n# extracting text from pdf file\n\ndef convert_pdf_to_txt(path):\n    rsrcmgr = PDFResourceManager()\n    retstr = StringIO()\n    laparams = LAParams()\n    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n    fp = open(path, 'rb')\n    interpreter = PDFPageInterpreter(rsrcmgr, device)\n    password = \"\"\n    maxpages = 0\n    caching = True\n    pagenos=set()\n    try:\n        for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,\\\n                                  caching=caching, check_extractable=True):\n            interpreter.process_page(page)\n    except:\n        print('This pdf won\\'t allow text extraction!')        \n    fp.close()\n    device.close()\n    str = retstr.getvalue()\n    retstr.close()\n    return str\n \n\nextracted = []    \n\n# Based on the extension of file, extracting text\n\nfor foldername,subfolders,files in os.walk(r\"\/kaggle\/input\/health_docs\"):\n    for file_ in files:\n        dict_ = {}\n        file_name,extension = splitext_(file_)\n        if extension == '.pdf':\n            converted = convert_pdf_to_txt(foldername +\"\/\" + file_)\n            converted = text_preprocess(converted)   \n            dict_['Extracted'] = converted\n            dict_['Label'] = foldername.split('\/')[-1]\n            extracted.append(dict_)\n            \n        elif extension == '.docx':\n            doc = docxpy.process(foldername +'\/'+ file_)\n            doc = text_preprocess(doc)\n            \n            dict_['Extracted'] = doc\n            dict_['Label'] = foldername.split('\/')[-1]\n            extracted.append(dict_)\n        elif extension == '.ppt':\n            parsed = parser.from_file(foldername +'\/'+ file_)\n            ppt = parsed[\"content\"]\n            ppt = text_preprocess(ppt)\n            dict_['Extracted'] = ppt\n            dict_['Label'] = foldername.split('\/')[-1]\n            extracted.append(dict_)   \n        \n            \n        df =  pd.DataFrame(extracted)\n        print(df)\n        df.to_csv('labelled_data.csv')\n            \n","ccee00d3":"import pandas as pd\ndata = pd.read_csv(\"\/kaggle\/input\/labelled-data\/bla.csv\")\ndata.head()","b0527b21":"data = data.drop('Unnamed: 0',axis=1)","9d7e8ee7":"data.isna().sum()","b5411f3a":"data=data.dropna(axis=0)\ndata.isna().sum()","b04daada":"import re\n\ndata['cleaned_text'] = data['text'].str.lower()\ndata['cleaned_text'] = data['cleaned_text'].apply(lambda text : re.sub(r'\\d+','', text)) \ndata['cleaned_text'] = data['cleaned_text'].apply(lambda text : re.sub('[^a-zA-Z0-9-_*.]', ' ', text) )\ndata['cleaned_text'] = data['cleaned_text'].str.strip()\ndata.head()","114ffe9e":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))\ndata['cleaned_text'] = data['cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n","d2ea930c":"from nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\nstemmer= PorterStemmer()\ndata['cleaned_text'].apply(lambda x : stemmer.stem(x))\ndata['cleaned_text'].head()\n","bea1902c":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\ntext = str(data['cleaned_text'].values)\nwordcloud  = WordCloud(stopwords=stop_words).generate(text)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","654caa36":"data.head()","e5213a91":"data.label.value_counts()","a4aa9452":"sns.set(font_scale=.9)\nsns.countplot(data['label'])\nsns.set_color_codes(palette='deep')\nplt.show()","68b9c6be":"from sklearn.preprocessing import LabelEncoder\n\ndata['label'] = LabelEncoder().fit_transform(data['label'])","f8cc6f8c":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvector = TfidfVectorizer()\nvector.fit(data['cleaned_text'])\ntfidf_text = vector.transform(data['cleaned_text'])\n","8e761c56":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n\nX = tfidf_text\ny = data['label']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.2,random_state=42)\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\nypred = lr.predict(X_test)\nconfusion_matrix(y_test,ypred)","07e9c655":"accuracy_score(y_test,ypred)","49c20431":"from sklearn.naive_bayes import GaussianNB\nimport seaborn as sns\n\nnb = GaussianNB()\nnb.fit(X_train.toarray(),y_train)\nypred = nb.predict(X_test.toarray())\nsns.heatmap(confusion_matrix(y_test,ypred),annot=True)","2ccc6c59":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier \n\nclassifier_dict = { 'DecisionTree' : DecisionTreeClassifier(), 'Random Forest': RandomForestClassifier(), 'Logistic Regression': LogisticRegression(),'KNN': KNeighborsClassifier()}\n\nl = []\nfor key, clf in classifier_dict.items():\n    dict_ = {}\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    dict_['class'] = clf.__class__.__name__\n    dict_['acc'] = accuracy\n    l.append(dict_)\n    print(key+\" : \"+str(accuracy))\n","7815fb88":"**Importing required libraries **","05220453":"**Vectorization**","3c12ab76":"**Checking if any Imbalance in Label column :**","e1e5475d":"Since the dataset is very less, the accuracy is good as it is overfitting. In order to handle overfitting, LOOCV has to be performed.","7eab4be5":"**Label Encoding the Label column **","1483fa09":"**Text Extraction**","835eb677":"**Removing stopwords and Stemming**","449699ec":"**Data Pre-processing**\n","c6264d1d":"It can be seen that the value counts for all the 4 labels are almost equal. So we need not apply any resampling techniques.","f1970ef8":"** Removing numbers and special characters :**","76fe1461":"![](http:\/\/)**Dropping the 'Unnamed' column**","ebad746b":"**Word Cloud on frequently occuring words**","3cbe0154":"**Checking NaN values**","b67927fd":"**Model Building**","2c7bd294":"** Checking accuracy for different models**","60640ea9":"**** Model Building****"}}