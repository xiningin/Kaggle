{"cell_type":{"f6bc415e":"code","c2622f81":"code","8fbbc1ea":"code","1f1f5dbd":"code","a1dabcc9":"code","a33c753d":"code","da162253":"code","0615ba10":"code","8de247bd":"code","5ea9d7eb":"code","b63026b3":"code","49158bc9":"code","40153194":"code","ea0b59cd":"code","53440b2d":"code","f99faecb":"markdown"},"source":{"f6bc415e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c2622f81":"import time\nimport datetime\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom tqdm import tqdm\nimport scipy as sp\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom numpy import inf\npd.options.display.precision = 15","8fbbc1ea":"import os\nimport gc\nfrom joblib import Parallel, delayed","1f1f5dbd":"def classic_sta_lta(x, Ns, Nl):\n    sta = np.cumsum(x ** 2)\n    sta = np.require(sta, dtype=np.float)\n    lta = sta.copy()\n    lta[Nl:-Ns] = lta[Nl:-Ns] - lta[:-Nl-Ns]\n    lta \/= Nl\n    sta[Nl+Ns-1:] = sta[Nl+Ns-1:] - sta[Nl-1:-Ns]\n    sta \/= Ns\n    sta[:Nl - 1 + Ns] = 0\n    dtiny = np.finfo(0.0).tiny\n    idx = lta < dtiny\n    lta[idx] = dtiny\n    return sta \/ lta","a1dabcc9":"class FeatureGenerator(object):\n    def __init__(self, dtype, n_jobs=1, chunk_size=None):\n        self.chunk_size = chunk_size\n        self.dtype = dtype\n        self.filename = None\n        self.n_jobs = n_jobs\n        self.test_files = []\n        if self.dtype == 'train':\n            self.filename = '..\/input\/train.csv'\n            self.total_data = int(629145481 \/ self.chunk_size)\n        else:\n            submission = pd.read_csv('..\/input\/sample_submission.csv')\n            for seg_id in submission.seg_id.values:\n                self.test_files.append((seg_id, '..\/input\/test\/' + seg_id + '.csv'))\n            self.total_data = int(len(submission))\n\n    def read_chunks(self):\n        if self.dtype == 'train':\n            iter_df = pd.read_csv(self.filename, iterator=True, chunksize=self.chunk_size,\n                                  dtype={'acoustic_data': np.float64, 'time_to_failure': np.float64})\n            for counter, df in enumerate(iter_df):\n                x = df.acoustic_data.values\n                y = df.time_to_failure.values[-1]\n                seg_id = 'train_' + str(counter)\n                del df\n                yield seg_id, x, y\n        else:\n            for seg_id, f in self.test_files:\n                df = pd.read_csv(f, dtype={'acoustic_data': np.float64})\n                x = df.acoustic_data.values[-self.chunk_size:]\n                del df\n                yield seg_id, x, -999\n\n    def features(self, x, y, seg_id):\n        feature_dict = dict()\n        feature_dict['target'] = y\n        feature_dict['seg_id'] = seg_id\n\n        # create features here\n        feature_dict['mean'] = np.mean(x)\n        feature_dict['max'] = np.max(x)\n        feature_dict['min'] = np.min(x)\n        feature_dict['std'] = np.std(x)\n        feature_dict['var'] = np.var(x)\n        feature_dict['quantile_03'] = np.quantile(x, 0.03)\n        feature_dict['skew'] = sp.stats.skew(x)\n        feature_dict['kurtosis'] = sp.stats.kurtosis(x)\n        feature_dict['moment_3'] = sp.stats.moment(x, 3)\n        \n        pct_change = pd.Series(x).pct_change()\n        pct_change[pct_change == -inf] = 0\n        pct_change[pct_change == inf] = 0\n        feature_dict['pct_change_mean'] = pct_change.mean()\n        rate_change = pd.Series(x).pct_change().pct_change()\n        rate_change[rate_change == -inf] = 0\n        rate_change[rate_change == inf] = 0\n        feature_dict['rate_change_max'] = rate_change.max()\n        feature_dict['rate_change_mean'] = rate_change.mean()\n        feature_dict['classic_sta_lta_mean'] = classic_sta_lta(x, 100, 5000).mean()\n        \n        window_size = 10\n        x_roll_std = pd.Series(x).rolling(window_size).std().dropna().values\n        feature_dict['q03_roll_std_' + str(window_size)] = np.quantile(x_roll_std, 0.03)\n        window_size = 150\n        x_roll_std = pd.Series(x).rolling(window_size).std().dropna().values\n        feature_dict['q03_roll_std_' + str(window_size)] = np.quantile(x_roll_std, 0.03)\n        \n        return feature_dict\n    \n    def generate(self):\n        feature_list = []\n        res = Parallel(n_jobs=self.n_jobs,\n                       backend='threading')(delayed(self.features)(x, y, s)\n                                            for s, x, y in tqdm(self.read_chunks(), total=self.total_data))\n        for r in res:\n            feature_list.append(r)\n        return pd.DataFrame(feature_list)\n    \ntraining_fg = FeatureGenerator(dtype='train', n_jobs=10, chunk_size=150000)\ntraining_data = training_fg.generate()\n\ntest_fg = FeatureGenerator(dtype='test', n_jobs=10, chunk_size=150000)\ntest_data = test_fg.generate()\n        ","a33c753d":"X = training_data.drop(['target', 'seg_id'], axis=1)\nX_test = test_data.drop(['target', 'seg_id'], axis=1)\ntest_segs = test_data.seg_id\ny = training_data.target","da162253":"folds = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros((len(X), 1))\ntest_preds = np.zeros((len(X_test), 1))","0615ba10":"params = {\n    \"learning_rate\": 0.01,\n    \"max_depth\": 3,\n    \"n_estimators\": 10000,\n    \"min_child_weight\": 4,\n    \"colsample_bytree\": 1,\n    \"subsample\": 0.9,\n    \"nthread\": 12,\n    \"random_state\": 42\n}","8de247bd":"for fold_, (trn_, val_) in enumerate(folds.split(X)):\n    print(\"Current Fold: {}\".format(fold_))\n    trn_x, trn_y = X.iloc[trn_], y.iloc[trn_]\n    val_x, val_y = X.iloc[val_], y.iloc[val_]\n\n    clf = xgb.XGBRegressor(**params)\n    clf.fit(\n        trn_x, trn_y,\n        eval_set=[(trn_x, trn_y), (val_x, val_y)],\n        eval_metric='mae',\n        verbose=150,\n        early_stopping_rounds=100\n    )\n    val_pred = clf.predict(val_x, ntree_limit=clf.best_ntree_limit)\n    test_fold_pred = clf.predict(X_test, ntree_limit=clf.best_ntree_limit)\n    print(\"MAE = {}\".format(mean_absolute_error(val_y, val_pred)))\n    oof_preds[val_, :] = val_pred.reshape((-1, 1))\n    test_preds += test_fold_pred.reshape((-1, 1))\ntest_preds \/= 5\n\noof_score = mean_absolute_error(y, oof_preds)\nprint(\"Mean MAE = {}\".format(oof_score))","5ea9d7eb":"print(clf.feature_importances_)","b63026b3":"print(training_data.columns)","49158bc9":"feature_importance = pd.concat([pd.Series(list(set(list(training_data)) - set(['seg_id', 'target']))), pd.Series(clf.feature_importances_)], axis = 1, keys = ['feature', 'importance'])","40153194":"feature_importance.sort_values(by = ['importance'], ascending = False, inplace = True)","ea0b59cd":"feature_importance","53440b2d":"submission = pd.DataFrame(columns=['seg_id', 'time_to_failure'])\nsubmission.seg_id = test_segs\nsubmission.time_to_failure = test_preds\nsubmission.to_csv('submission.csv', index=False)","f99faecb":"Taken from [Abhishek's Kernel](https:\/\/www.kaggle.com\/abhishek\/quite-a-few-features-1-51). Features used here are different though."}}