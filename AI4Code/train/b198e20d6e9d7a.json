{"cell_type":{"cc86528a":"code","96448cb8":"code","3b276de5":"code","5c6d4eeb":"code","b1f6ffa7":"code","ed711f53":"code","443055da":"code","d5c0ee49":"code","8593296c":"code","ff14eff8":"code","38217900":"code","4ae7bb21":"code","db4641de":"code","0410bf87":"code","de663d2c":"code","6ee49170":"code","a344aa21":"code","dbe5585b":"code","64d220b8":"code","7b947110":"code","e8813fe1":"code","249a5cb1":"code","d9573b2d":"code","4fa74b67":"markdown","84df6bfc":"markdown","1de37043":"markdown","c190b3f3":"markdown","5d0c4ced":"markdown","49fc6489":"markdown","e3a446f7":"markdown","082b4628":"markdown","a28267f6":"markdown","8f9f665e":"markdown","a70dc93c":"markdown","32b83c2d":"markdown","ba6121d6":"markdown","927d2352":"markdown","abc599e2":"markdown","3d9fb7bb":"markdown","fa37b303":"markdown","8e2cebbf":"markdown"},"source":{"cc86528a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom collections import Counter\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, LinearRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\n\n# setting seaborn theme\nsns.set_theme(style='whitegrid', palette='ch:.25')","96448cb8":"ori = pd.read_csv('\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\ndf = pd.read_csv('\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\ndf.head()","3b276de5":"df.info()","5c6d4eeb":"df.isnull().mean()","b1f6ffa7":"df.nunique()","ed711f53":"correlation = df.corr()\nplt.figure(figsize=(18,18))\nsns.heatmap(correlation, annot=True, square=True)\nplt.show()","443055da":"df.isna().sum()","d5c0ee49":"df = df.drop(['Date','Evaporation','Sunshine','Cloud9am','Cloud3pm'], axis=1)","8593296c":"df.columns","ff14eff8":"cat_list = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm','RainToday', 'RainTomorrow']\nfor column in cat_list:\n    df[column] = pd.Categorical(df[column])\n    df[column] = df[column].cat.codes\n    #-1 represent NaN in .cat.codes. Therefore, I replaced to real NaN value\n    df[column].replace(-1, np.NaN, inplace=True)","38217900":"def filling_null(feature, df=df):\n    \n    #make train set and test set\n    temp_df = df.copy().drop('RainTomorrow', axis=1)\n    df_list = list(temp_df.columns)\n    df_list.remove(feature)\n    temp_df.dropna(subset=df_list, inplace=True)\n    train = temp_df.loc[temp_df.notna()[feature]]\n    train_x = train.drop(feature, axis=1)\n    train_y = train[feature]\n    test = temp_df[temp_df.isnull()[feature]].drop(feature,axis=1)    \n\n    #run machine learning model and predict null values\n    KNN = KNeighborsRegressor(n_jobs=-1)\n    KNN.fit(train_x, train_y)\n    change_NaN = KNN.predict(test)\n    index_list = test.index.tolist()\n    for i in range(len(change_NaN)):\n        df.at[index_list[i], feature]= change_NaN[i]\n\n    #return dataset which had been changed\n    return df","4ae7bb21":"apply_list =['MinTemp', 'MaxTemp', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'Humidity9am',\n             'Humidity3pm', 'Pressure9am', 'Pressure3pm']\n\n\nfor feature in apply_list:\n    df = filling_null(feature = feature)","db4641de":"#getting information of number of null variable changed\ndf_columns = list(df.columns)\nchanged_dict = {}\nfor col in df_columns:\n    changed_dict[\"%s\" %col] = len(df[col].dropna()) - len(ori[col].dropna())\n\n#delet features which did not changed at all\npop_list = ['Location','Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm','Temp9am','Temp3pm','RainToday','RainTomorrow']\nfor feature in pop_list:\n    changed_dict.pop(feature)\n\n#make list of key and value to visualize the graph\nkey_list = []\nvalue_list = []\nfor key, value in changed_dict.items():\n    key_list.append(key)\n    value_list.append(value)\n\ntemp_df = pd.DataFrame()\ntemp_df['key'] = key_list\ntemp_df['value'] = value_list\n\n#visualization\nplt.figure(figsize=(25, 10))\nplot = sns.barplot(x='key',y='value', data=temp_df)\nfor p in plot.patches:\n    plot.annotate(format(p.get_height(), '0.0f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\nplt.yscale('log')\nplot.axes.get_yaxis().set_visible(False)\nplt.title('# of null values which changed to non-null values', fontsize=20)\nplt.show()","0410bf87":"df_Xnul = df.fillna(df.median())","de663d2c":"df_Xnul.info()","6ee49170":"X = df_Xnul.drop(['RainTomorrow'], axis=1)\ny = df_Xnul['RainTomorrow']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)","a344aa21":"classifier_names = [\"Logistic Regression\",'SGDClassifier', \"Random Forest\",\"KNN\",\"Decision\"]\n\nclassifiers = [LogisticRegression(), SGDClassifier(), RandomForestClassifier(), KNeighborsClassifier(), DecisionTreeClassifier()]\n\nzipped_clf = zip(classifier_names,classifiers)","dbe5585b":"def classifier(classifier, t_train, c_train, t_test, c_test):\n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([\n            ('standardize', StandardScaler()),                         \n            ('classifier', c)\n        ])\n        print(\"Validation result for {}\".format(n))\n        print(c)\n        clf_acc = fit_classifier(checker_pipeline, t_train, c_train, t_test,c_test)\n        result.append((n,clf_acc))\n    return result","64d220b8":"def fit_classifier(pipeline, X_train, y_train, X_test, y_test):\n    model_fit = pipeline.fit(X_train, y_train)\n    y_pred = model_fit.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n    print()\n    return accuracy","7b947110":"result = classifier(zipped_clf, X_train, y_train, X_test, y_test)","e8813fe1":"param_list = {'min_samples_leaf':[3,6,9], 'max_depth': [5,10,None], 'criterion' : ['gini', 'entropy']}\nRFC = RandomForestClassifier(n_jobs=-1, random_state=42)\nclf = GridSearchCV(RFC,param_list)\nclf.fit(X_train, y_train)","249a5cb1":"y_pred = clf.best_estimator_.predict(X_test)","d9573b2d":"accuracy_score(y_test,y_pred)*100","4fa74b67":"### Handling Null Values\n\nHandling Null Values is really important to predict the answer. <br\/>\nIf there is null values in dataset, machine learning model will not accept to fit dataset into model.<br\/> Also, it will lead to wrong prediction at the last. \nIn this section, I changed null values by using 2 steps.\n\n1. **Predict null values**<br\/> \nJust replacing null values to median or mean would not increase the accuracy of model.<br\/> Therefore, by using `KNeighborsRegressor`, I am going to predict missing values as much as possible.\n\n2. **Replace to median**<br\/>\nAfter predicting null values, I am going to replace it by using `SimpleImputer`.<br\/>\n","84df6bfc":"# Reference\n\n* https:\/\/towardsdatascience.com\/automate-the-machine-learning-model-implementation-with-sklearn-pipeline-2ef1389062c9","1de37043":"### Checking Basic Information","c190b3f3":"- Since `Evaporation`, `Sunshine`, `Cloud9am`, `Cloud3pm` contain null values more than 30%, it will not help to improve out modes so they will be dropped.\n- `Date` information does not needed, so this will be also dropped.","5d0c4ced":"<img src='https:\/\/t1.daumcdn.net\/cfile\/tistory\/99B3263359928F0F30' width='400'>","49fc6489":"### Null Values in each column","e3a446f7":"### Dropping Some Columns","082b4628":"Reason why I did not apply all the columns to `filling_null` function is that features not in the list, `Rainfall`, `WindGustSpeed`, `WindDir9am`, `WindDir3pm`, `Temp9am`, `Temp3pm`, `RainToday`, will have empty dataset if I drop null values from other features.","a28267f6":"# 1. Load Data & Check Information","8f9f665e":"As you can see, `WindDir9am` feature has changed a lot! <br> Other features also changed, but not like `WindDir9am`. However, it is still worth it","a70dc93c":"By applying 5 machine learning models, `Random Forest` gained highest accuracy score(85.29%) with default hyperparameter. <\/br>\nRandom Forest's hyperparameters will be tuned to increase the accuracy score little bit more.","32b83c2d":"# 3. Train & Test set\nBefore moving on, we need to make train and test set.","ba6121d6":"# 2. Data Engineering","927d2352":"Since we finished first step, we are going to do second step, which is **Replace to median**.","abc599e2":"KNN model does not receive **object** type values, so object type values must be replaced to interger of float type.","3d9fb7bb":"First step will be executed by `filling_null` function. <br\/><br\/>\nThings going on in `filling_null` :\n* Based on the given feature, it drops all the null variable in other features.\n* Split dataset into train and test set. Train set only include non-null values for given feature, and test set only include null values for given feature.\n* Run KNN model to predict null values\n* Return Dataset","fa37b303":"# 4. Model Selection & GridSearch\nNow it's time to select best model and do some hyperparameter tuning!","8e2cebbf":"### Checking Correlationship"}}