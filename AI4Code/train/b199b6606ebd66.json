{"cell_type":{"d567db11":"code","55e23b2d":"code","e38c161e":"code","c4b58c68":"code","d4254e54":"code","e9b6f2c5":"code","767e01af":"code","41d801b2":"code","71d21e69":"code","877d28d7":"code","24fba02f":"code","ba0219cc":"code","48032ad8":"code","46916cca":"code","00b483df":"code","58b0e2fb":"code","d6bafd60":"code","6a5d4548":"code","24f4b50c":"markdown"},"source":{"d567db11":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig\n\n\n# for text cleaning\nimport string\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords","55e23b2d":"class color:\n    PURPLE = '\\033[95m'\n    CYAN = '\\033[96m'\n    DARKCYAN = '\\033[36m'\n    BLUE = '\\033[94m'\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'\n    RED = '\\033[91m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n    END = '\\033[0m'","e38c161e":"TRAIN_DATA_PATH = \"..\/input\/nlp-getting-started\/train.csv\"\nTEST_DATA_PATH = \"..\/input\/nlp-getting-started\/test.csv\"\n\ntrain_data = pd.read_csv(TRAIN_DATA_PATH)\ntest_data = pd.read_csv(TEST_DATA_PATH)\n\nprint(color.YELLOW + 'summary of dataset:\\n' + color.END,\n      train_data.describe())\n\n\nTEXT_FIELD = \"text\"\nLABEL_FIELD = \"target\"\n\n# drop the null entitites\ntrain_data.dropna(inplace=True, subset=[TEXT_FIELD])\ntrain_data.drop_duplicates(inplace=True,subset=[TEXT_FIELD])\n\nprint(color.YELLOW + '\\nsummary of dataset after dropping null rows and duplicate texts:\\n' + color.END,\n      train_data.describe())\n\n","c4b58c68":"train_data['len'] = train_data[TEXT_FIELD].apply(len)\ntrain_data['len'].value_counts().plot.bar()","d4254e54":"contractions = {\n    \"ain't\": \"am not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"i'd\": \"I would\",\n    \"i'd've\": \"I would have\",\n    \"i'll\": \"I will\",\n    \"i'll've\": \"I will have\",\n    \"i'm\": \"I am\",\n    \"i've\": \"I have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so is\",\n    \"that'd\": \"that had\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\",\n    \"you'll've\": \"you will have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\",\n}\n\nstop_words = stopwords.words('english')\nstop_words = [w for w in stop_words if not w in ['not', 'no', 'nor']]\n\ndef clean_text(text):\n    # remove the contractions\n    for word in text.split():\n        if word.lower() in contractions:\n            text = text.replace(word, contractions[word.lower()])\n    \n    tokens = word_tokenize(text) # divide into tokens\n    table = str.maketrans('', '', string.punctuation)\n    words = [w.lower().translate(table) for w in tokens] # remove the punc'ns and convert to lower case\n    words = [w for w in words if w.isalpha()]\n#     words = [w for w in words if not w in stop_words]\n    size = len(words)\n    \n    clean_text = ' '.join(word for word in words)    \n    return clean_text, size\n\n\ndef create_inputs(tweets, tokenizer):\n    input_ids, attention_masks, token_type_ids=[], [], []\n    MAX_LEN = -1\n    clean_tweets = []\n    for tweet in tweets:\n        clean_tweet, size = clean_text(tweet)\n        clean_tweets.append(clean_tweet)\n        MAX_LEN = max(MAX_LEN, size)\n    \n    \n    for tweet in clean_tweets:\n        encoded = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=64,\n                                       pad_to_max_length=True)\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n        token_type_ids.append(encoded['token_type_ids'])\n    \n    return np.asarray(input_ids), np.asarray(attention_masks), np.asarray(token_type_ids)","e9b6f2c5":"# create the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# get BERT essentials\ntrain_input_ids, train_attention_masks, train_token_type_ids = create_inputs(\n    train_data[TEXT_FIELD], tokenizer)\ntest_input_ids,test_attention_masks,test_token_type_ids = create_inputs(test_data[TEXT_FIELD], tokenizer)","767e01af":"train_len = train_input_ids.shape[0]\nval_len = int(0.2 * train_len) # 20% data will be used for validation\n\nval_inp = train_input_ids[:val_len]\ntrain_inp = train_input_ids[val_len : ]\n\nval_out = train_data[LABEL_FIELD][:val_len]\ntrain_out = train_data[LABEL_FIELD][val_len : ]\n\nval_mask = train_attention_masks[:val_len]\ntrain_mask = train_attention_masks[val_len : ]\n\nval_type_ids = train_token_type_ids[:val_len]\ntrain_type_ids = train_token_type_ids[val_len : ]","41d801b2":"def convert_to_features(input_ids,attention_masks,token_type_ids,y):\n#     This funciton will convert examples to FEATURES\n    return {\"input_ids\": input_ids,\n          \"attention_mask\": attention_masks,\n          \"token_type_ids\": token_type_ids},y","71d21e69":"BATCH_SIZE = 128\ntrain_ds = tf.data.Dataset.from_tensor_slices(\n    (train_inp,train_mask,train_type_ids,\n     train_out)).map(convert_to_features).shuffle(100).batch(BATCH_SIZE).repeat(5)\nval_ds = tf.data.Dataset.from_tensor_slices(\n    (val_inp, val_mask, val_type_ids, val_out)).map(convert_to_features).batch(BATCH_SIZE)","877d28d7":"# Get and configure the BERT model\nconfig = BertConfig.from_pretrained(\"bert-base-uncased\", hidden_dropout_prob=0.3, num_labels=2)\nmodel = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config)","24fba02f":"optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4, epsilon=0.0015, clipnorm=0.01)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])\nmodel.summary()","ba0219cc":"earlyStopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\nhistory = model.fit(train_ds, epochs=50, validation_data = val_ds, callbacks=[earlyStopping])","48032ad8":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nepochs = range(1, len(loss) + 1)\nplt.figure()\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","46916cca":"def test_create_feature(input_ids,attention_masks,token_type_ids):\n    return {\"input_ids\": input_ids,\n          \"attention_mask\": attention_masks,\n          \"token_type_ids\": token_type_ids}","00b483df":"inp_ids, attn_msks, tkn_ids = create_inputs(\n    test_data[TEXT_FIELD], tokenizer)\ninp_ds = test_create_feature(inp_ids, attn_msks, tkn_ids)","58b0e2fb":"pre_result = model.predict(inp_ds)","d6bafd60":"res_dict = {}\n\n# print(pre_result[0][0])\n\nfor i in range(len(pre_result[0])):\n    res_dict[test_data['id'][i]] = np.argmax(pre_result[0][i])\n\nprint(res_dict)","6a5d4548":"submission_df = pd.DataFrame.from_dict(res_dict, orient=\"index\")\nsubmission_df.to_csv(\"submission.csv\")","24f4b50c":"# Start Predecting"}}