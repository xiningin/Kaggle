{"cell_type":{"957dcb06":"code","135a03cf":"code","de263806":"code","8685ef8b":"code","f2298fca":"code","8885cb01":"code","41eaf5bf":"code","1a7e887d":"code","17c41ac4":"code","634d681d":"code","4174babe":"code","22a8e0c3":"code","ef7ec786":"code","7e182c9a":"code","fe626fe9":"code","002a41db":"code","83f3397a":"code","6211ab74":"code","f165881d":"code","4a5160ee":"code","f34bc603":"code","e564316f":"code","299e0720":"code","9aa83ca6":"code","345fd538":"code","ff3c1b7b":"code","db4ac446":"code","dbd262c0":"code","666ff954":"code","4060f575":"code","beaba9e3":"code","aac027b6":"code","0a93702d":"code","b4aa72a8":"code","2dcf729c":"code","dd9bc935":"code","08b7927b":"code","cb8fe6e8":"code","a870bb44":"code","4a9ed4fb":"code","f95e79bf":"code","a4dc2a64":"code","4b968f87":"code","8fa6d7e5":"code","ae230938":"code","7dd01479":"code","0160ba09":"code","88df0c12":"code","25d3e339":"code","bccca45f":"code","490da687":"code","2648b452":"code","5b39861c":"code","7e0ba84e":"code","8aade19a":"code","3466bc44":"code","d4893afd":"code","f2024058":"code","817273c3":"code","8ed5ce39":"code","d0c38388":"code","3d81a30b":"code","aeafd722":"code","19b51911":"code","e47c2257":"code","04dfe93c":"code","e8ae0992":"code","5eb2712e":"code","6d0a66ab":"code","e2d1420e":"code","4657b6c9":"code","a9df6518":"code","4bbede1f":"code","1fb32dce":"code","5265b51d":"code","a03d6059":"code","11042784":"code","9c7c5508":"code","cc648bd3":"code","c53f4083":"code","83fda8be":"code","94aebbc9":"code","e9cc8ee2":"code","39ea6ccb":"code","7c5d8c17":"code","9c6e1923":"code","bfa94822":"code","eaaf463a":"code","25ed478a":"code","9effbceb":"code","6e1e4d6f":"code","f0e53a67":"code","f808ec76":"code","4d22df01":"code","56492767":"code","73b4d337":"code","176ade3a":"code","73fc8a19":"code","939f56c0":"code","b440c207":"code","fe945d1a":"code","04fd4aff":"code","9bd882da":"code","0ed6068c":"code","e6099a2c":"code","41c1b862":"code","76632787":"code","d4f37ccd":"code","4b6cceda":"code","03262d2c":"code","21200ce1":"code","d6ef4934":"code","83670b11":"code","c564da83":"code","dff906f0":"markdown","83c7e90d":"markdown","ac5e788e":"markdown","ae3006b4":"markdown","1f950e44":"markdown","7ecfc9c4":"markdown","6a2b97d9":"markdown","a67ccd62":"markdown","ff99618c":"markdown","ef210c39":"markdown","e8d04a0e":"markdown","09fca1fe":"markdown","bd4bab6e":"markdown","632588df":"markdown","ff6087a3":"markdown","b9317d0d":"markdown","f878a90d":"markdown","9f30514c":"markdown","d03f723d":"markdown","c170d4af":"markdown","73a05931":"markdown","43c99617":"markdown","92580332":"markdown","6d2ac715":"markdown","3ffae70b":"markdown","a926f681":"markdown","430f1280":"markdown","7a67affd":"markdown","34d1a412":"markdown","cccf3e60":"markdown","b5402c9d":"markdown","01462570":"markdown","7f5bcdf9":"markdown","7af52b78":"markdown","3aa93a55":"markdown","67d3cbb0":"markdown"},"source":{"957dcb06":"import numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\n\nimport scipy.stats as stats\nimport statsmodels.api as sm\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n# special matplotlib argument for improved plots\nfrom matplotlib import rcParams\n\n\nnp.set_printoptions(precision=4)\n\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nsns.set(font_scale=1.5)","135a03cf":"# Load the Boston housing dataset\ntrain_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data  = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","de263806":"train_data.head()","8685ef8b":"train_data.tail()","f2298fca":"# quike look to get various summary statistics in numeric data.\ntrain_data.describe()","8885cb01":"# quike look to get various summary statistics in categorical data.\ntrain_data.describe(include=['O'])","41eaf5bf":"# grab numeric columns.\nnumeric_features = train_data.select_dtypes(include=[np.number])\n\nnumeric_features.columns","1a7e887d":"# grab object columns.\ncategorical_features = train_data.select_dtypes(include=[np.object])\n\nnumeric_features.columns","17c41ac4":"# shows missing data per column\nplt.figure(figsize=(19,9))\nsns.heatmap(train_data.isnull(),cbar=False)\n","634d681d":"#the skewness of train data is rghite skewnsess\nsns.distplot(train_data.skew(),color='blue',axlabel ='Skewness')","4174babe":"sns.barplot(y='SalePrice',x='Utilities',data=train_data)\nplt.title('Sale price by utilites')","22a8e0c3":"plt.figure(figsize=(19,9))\nsns.barplot(y='SalePrice',x='Neighborhood',data=train_data)\nxt = plt.xticks(rotation=45)\nplt.title('Sale Price by Neighborhood')","ef7ec786":"plt.figure(figsize = (19, 9))\nsns.countplot(x = 'Neighborhood', data = train_data)\nxt = plt.xticks(rotation=45)\nplt.title('Number of Sales per Neighborhood')","7e182c9a":"plt.figure(figsize=(19,9))\nsns.barplot(y='SalePrice',x='SaleType',data=train_data)\nplt.title('Sale Price by Sale Type')","fe626fe9":"plt.figure(figsize=(19,9))\nsns.barplot(y='SalePrice',x='SaleCondition',data=train_data)\nplt.title('Sale Price by Sale Condition')","002a41db":"#to see which kind of relationship we have \nsns.set()\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.pairplot(train_data[columns],size = 2 ,kind ='scatter',diag_kind='kde')\nplt.show()","83f3397a":"fig, ((ax1, ax2), (ax3, ax4),(ax5,ax6)) = plt.subplots(nrows=3, ncols=2, figsize=(14,10))\nOverallQual_scatter_plot = pd.concat([train_data['SalePrice'],train_data['OverallQual']],axis = 1)\nsns.regplot(x='OverallQual',y = 'SalePrice',data = OverallQual_scatter_plot,scatter= True, fit_reg=True, ax=ax1)\nTotalBsmtSF_scatter_plot = pd.concat([train_data['SalePrice'],train_data['TotalBsmtSF']],axis = 1)\nsns.regplot(x='TotalBsmtSF',y = 'SalePrice',data = TotalBsmtSF_scatter_plot,scatter= True, fit_reg=True, ax=ax2)\nGrLivArea_scatter_plot = pd.concat([train_data['SalePrice'],train_data['GrLivArea']],axis = 1)\nsns.regplot(x='GrLivArea',y = 'SalePrice',data = GrLivArea_scatter_plot,scatter= True, fit_reg=True, ax=ax3)\nGarageArea_scatter_plot = pd.concat([train_data['SalePrice'],train_data['GarageArea']],axis = 1)\nsns.regplot(x='GarageArea',y = 'SalePrice',data = GarageArea_scatter_plot,scatter= True, fit_reg=True, ax=ax4)\nFullBath_scatter_plot = pd.concat([train_data['SalePrice'],train_data['FullBath']],axis = 1)\nsns.regplot(x='FullBath',y = 'SalePrice',data = FullBath_scatter_plot,scatter= True, fit_reg=True, ax=ax5)\nYearBuilt_scatter_plot = pd.concat([train_data['SalePrice'],train_data['YearBuilt']],axis = 1)\nsns.regplot(x='YearBuilt',y = 'SalePrice',data = YearBuilt_scatter_plot,scatter= True, fit_reg=True, ax=ax6)\nYearRemodAdd_scatter_plot = pd.concat([train_data['SalePrice'],train_data['YearRemodAdd']],axis = 1)\nYearRemodAdd_scatter_plot.plot.scatter('YearRemodAdd','SalePrice')","6211ab74":"saleprice_overall_quality= train_data.pivot_table(index ='OverallQual',values = 'SalePrice', aggfunc = np.median)\nsaleprice_overall_quality.plot(kind = 'bar',color = 'blue')\nplt.xlabel('Overall Quality')\nplt.ylabel('Median Sale Price')\nplt.show()","f165881d":"var = 'OverallQual'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\nf, ax = plt.subplots(figsize=(12, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)","4a5160ee":"var = 'Neighborhood'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 10))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nxt = plt.xticks(rotation=45)","f34bc603":"k= 11\ncorrelation=train_data.corr()\ncols = correlation.nlargest(k,'SalePrice')['SalePrice'].index\nprint(cols)\ncm = np.corrcoef(train_data[cols].values.T)\nf , ax = plt.subplots(figsize = (14,12))\nsns.heatmap(cm, vmax=.8, linewidths=0.01,square=True,annot=True,cmap='viridis',\n            linecolor=\"white\",xticklabels = cols.values ,annot_kws = {'size':12},yticklabels = cols.values)","e564316f":"train_data.shape","299e0720":"train_data.info()","9aa83ca6":"#we did this to 3 columns, the observation were originally numbers( int) but each number refer to a certain category\n#so we replaced each one to its meaning ( string) we took these definitions from the data description file ","345fd538":"td_MSSubClass_col=     {20: '1-STORY 1946 & NEWER ALL STYLES',\n                        30: '1-STORY 1945 & OLDER',\n                        40: '1-STORY W\/FINISHED ATTIC ALL AGES',\n                        45: '1-1\/2 STORY - UNFINISHED ALL AGES',\n                        50: '1-1\/2 STORY FINISHED ALL AGES',\n                        60: '2-STORY 1946 & NEWER',\n                        70: '2-STORY 1945 & OLDER',\n                        75: '2-1\/2 STORY ALL AGES',\n                        80: 'SPLIT OR MULTI-LEVEL',\n                        85: 'SPLIT FOYER',\n                        90: 'DUPLEX - ALL STYLES AND AGES',\n                        120:'1-STORY PUD (Planned Unit Development) - 1946 & NEWER',\n                        150:'1-1\/2 STORY PUD - ALL AGES',\n                        160:'2-STORY PUD - 1946 & NEWER',\n                        180: 'PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER',\n                        190: '2 FAMILY CONVERSION - ALL STYLES AND AGES'}\n\ntrain_data['MSSubClass']=train_data['MSSubClass'].replace(td_MSSubClass_col)\n\n#train_data.MSSubClass.value_counts()","ff3c1b7b":"OverallQual_col=     {10:\"Very Excellent\",\n                      9:\"Excellent\",\n                       8:\"Very Good\",\n                       7:\"Good\",\n                       6:\"Above Average\",\n                       5:\"Average\",\n                       4:\"Below Average\",\n                       3:\"Fair\",\n                       2:\"Poor\",\n                       1:\"Very Poor\"}\n\n\ntrain_data['OverallQual']=train_data['OverallQual'].replace(OverallQual_col)\n\n#train_data.OverallQual.value_counts()","db4ac446":"OverallCond_col = {10:\"Very Excellent\",\n                   9:\"Excellent\",\n                   8:\"Very Good\",\n                   7:\"Good\",\n                   6:\"Above Average\",\n                   5:\"Average\",\n                   4:\"Below Average\",\n                   3:\"Fair\",\n                   2:\"Poor\",\n                   1:\"Very Poor\"}\n\ntrain_data['OverallCond']=train_data['OverallCond'].replace(OverallCond_col)\n\n\n#train_data.OverallCond.value_counts()","dbd262c0":"# to check how many null cells we have in every column \ntrain_data.isnull().sum().sort_values(ascending = False).head()","666ff954":"# we replaced each null cells with eather the median if the column's type were organnly int or flout.\n#otherwise we replaced it with the mode  if it was catagorical.","4060f575":"train_data.PoolQC.value_counts()","beaba9e3":"#print(\"LotFrontage median = \",train_data.LotFrontage.median())\n#print (\"MasVnrArea median = \",train_data.MasVnrArea.median())","aac027b6":"train_data[\"PoolQC\"].fillna(\"No Pool\",inplace=True)\ntrain_data[\"Electrical\"].fillna(\"SBrkr\",inplace=True)\ntrain_data[\"MasVnrArea\"].fillna(0,inplace=True)\ntrain_data[\"LotFrontage\"].fillna(69,inplace=True)\ntrain_data[\"MasVnrType\"].fillna(\"None\",inplace=True)\ntrain_data[\"FireplaceQu\"].fillna(\"No Fireplace\",inplace=True)\ntrain_data[\"MiscFeature\"].fillna(\"standerd\",inplace=True)\ntrain_data[\"Alley\"].fillna(\"No alley access\",inplace=True)\ntrain_data[\"Fence\"].fillna(\"No Fence\",inplace=True)\ntrain_data[\"GarageCond\"].fillna(\"No Garage\",inplace=True)\ntrain_data[\"GarageType\"].fillna(\"No Garage\",inplace=True)\ntrain_data[\"GarageYrBlt\"].fillna(0,inplace=True)\ntrain_data[\"GarageFinish\"].fillna(\"No Garage\",inplace=True)\ntrain_data[\"BsmtExposure\"].fillna(\"No Basement\",inplace=True)\ntrain_data[\"GarageQual\"].fillna(\"No Garage\",inplace=True)\ntrain_data[\"BsmtFinType2\"].fillna(\"No Basement\",inplace=True)\ntrain_data[\"BsmtCond\"].fillna(\"No Basement\",inplace=True)\ntrain_data[\"BsmtFinType1\"].fillna(\"No Basement\",inplace=True)\ntrain_data[\"BsmtQual\"].fillna(\"No Basement\",inplace=True)","0a93702d":"test_data.head()","b4aa72a8":"test_data.shape","2dcf729c":"test_data.info()","dd9bc935":"# to check how many null cells we have in every column \ntest_data.isnull().sum().sort_values(ascending = False).head() ","08b7927b":"#test_data.MSZoning.value_counts()\ntest_data[\"MSZoning\"].fillna(\"RL\",inplace=True)\n\n#test_data.LotFrontage.median()\ntest_data[\"LotFrontage\"].fillna(67,inplace=True)\n\ntest_data[\"Alley\"].fillna(\"No alley access\",inplace=True)\n\n#test_data.Utilities.value_counts()\ntest_data[\"Utilities\"].fillna(\"AllPub\",inplace=True)\n\n#test_data.Exterior1st.value_counts()\ntest_data[\"Exterior1st\"].fillna(\"VinylSd\",inplace=True)\n\n#test_data.Exterior2nd.value_counts()\ntest_data[\"Exterior2nd\"].fillna(\"VinylSd\",inplace=True)\n\n#test_data.BsmtQual.value_counts()\ntest_data[\"BsmtQual\"].fillna(\"No Basement\",inplace=True)\n\n#test_data.MasVnrType.value_counts()\ntest_data[\"MasVnrType\"].fillna(\"None\",inplace=True)\n\n#test_data.BsmtCond.value_counts()\ntest_data[\"BsmtCond\"].fillna(\"No Basement\",inplace=True)\n\n#test_data.BsmtExposure.value_counts()\ntest_data[\"BsmtExposure\"].fillna(\"No Basement\",inplace=True)\n\n#test_data.PoolQC.value_counts()\ntest_data[\"PoolQC\"].fillna(\"No Pool\",inplace=True)\n\n#test_data.MiscFeature.value_counts()\ntest_data[\"MiscFeature\"].fillna(\"None\",inplace=True)\n\n#test_data.MiscFeature.value_counts()\ntest_data[\"MiscFeature\"].fillna(\"None\",inplace=True)\n\n#test_data.Fence.value_counts()\ntest_data[\"Fence\"].fillna(\"No Fence\",inplace=True)\n\n#test_data.FireplaceQu.value_counts()\ntest_data[\"FireplaceQu\"].fillna(\"No Fireplace\",inplace=True)\n\n#test_data.GarageYrBlt.value_counts()\ntest_data[\"GarageYrBlt\"].fillna(0,inplace=True)\n\n#test_data.GarageCond.value_counts()\ntest_data[\"GarageCond\"].fillna(\"No Garage\",inplace=True)\n\n#test_data.GarageFinish.value_counts()\ntest_data[\"GarageFinish\"].fillna(\"No Garage\",inplace=True)\n\n#test_data.GarageQual.value_counts()\ntest_data[\"GarageQual\"].fillna(\"No Garage\",inplace=True)\n\n#test_data.GarageType.value_counts()\ntest_data[\"GarageType\"].fillna(\"No Garage\",inplace=True)\n\n#test_data.BsmtFinType2.value_counts()\ntest_data[\"BsmtFinType2\"].fillna(\"No Basement\",inplace=True)\n\n#test_data.BsmtFinType1.value_counts()\ntest_data[\"BsmtFinType1\"].fillna(\"No Basement\",inplace=True)\n\n#test_data.MasVnrArea.value_counts()\ntest_data[\"MasVnrArea\"].fillna(0,inplace=True)\n\n#test_data.Functional.value_counts()\ntest_data[\"Functional\"].fillna(\"Typ\",inplace=True)\n\n#test_data.BsmtFullBath.value_counts()\ntest_data[\"BsmtFullBath\"].fillna(0,inplace=True)\n\n#test_data.BsmtUnfSF.value_counts()\ntest_data[\"BsmtUnfSF\"].fillna(0,inplace=True)\n\n#test_data.TotalBsmtSF.value_counts()\ntest_data[\"TotalBsmtSF\"].fillna(0,inplace=True)\n\n#test_data.BsmtHalfBath.value_counts()\ntest_data[\"BsmtHalfBath\"].fillna(0,inplace=True)\n\n#test_data.BsmtFinSF2.value_counts()\ntest_data[\"BsmtFinSF2\"].fillna(0,inplace=True)\n\n#test_data.SaleType.value_counts()\ntest_data[\"SaleType\"].fillna(\"WD\",inplace=True)\n\n#test_data.BsmtFinSF1.value_counts()\ntest_data[\"BsmtFinSF1\"].fillna(0,inplace=True)\n\n#test_data.GarageCars.value_counts()\ntest_data[\"GarageCars\"].fillna(2,inplace=True)\n\n# t=test_data[test_data.GarageArea.isnull()]\n# t[[\"GarageArea\",\"GarageCars\"]]\n#test_data.groupby(by=\"GarageCars\").mean()\n# the mean of GarageArea with 2 cars (just like the null cell here) is 519.042857\n#test_data.GarageArea.value_counts()\ntest_data[\"GarageArea\"].fillna(519.042857,inplace=True)\n\n# k=test_data[test_data.KitchenQual.isnull()]\n# test_data.loc[ :,[\"KitchenQual\",\"KitchenAbvGr\"]].groupby(by=\"KitchenAbvGr\").describe()\n# we saw that TA is the most frequent (KitchenQual) whenver the (KitchenAbvGr) is equal to 1, which is the case here\n# in the null cell\n#test_data.KitchenQual.value_counts()\ntest_data[\"KitchenQual\"].fillna(\"TA\",inplace=True)\n","cb8fe6e8":"td_MSSubClass_col=     {20: '1-STORY 1946 & NEWER ALL STYLES',\n                        30: '1-STORY 1945 & OLDER',\n                        40: '1-STORY W\/FINISHED ATTIC ALL AGES',\n                        45: '1-1\/2 STORY - UNFINISHED ALL AGES',\n                        50: '1-1\/2 STORY FINISHED ALL AGES',\n                        60: '2-STORY 1946 & NEWER',\n                        70: '2-STORY 1945 & OLDER',\n                        75: '2-1\/2 STORY ALL AGES',\n                        80: 'SPLIT OR MULTI-LEVEL',\n                        85: 'SPLIT FOYER',\n                        90: 'DUPLEX - ALL STYLES AND AGES',\n                        120:'1-STORY PUD (Planned Unit Development) - 1946 & NEWER',\n                        150:'1-1\/2 STORY PUD - ALL AGES',\n                        160:'2-STORY PUD - 1946 & NEWER',\n                        180: 'PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER',\n                        190: '2 FAMILY CONVERSION - ALL STYLES AND AGES'}\n\ntest_data['MSSubClass']=test_data['MSSubClass'].replace(td_MSSubClass_col)","a870bb44":"OverallQual_col=     {10:\"Very Excellent\",\n                      9:\"Excellent\",\n                       8:\"Very Good\",\n                       7:\"Good\",\n                       6:\"Above Average\",\n                       5:\"Average\",\n                       4:\"Below Average\",\n                       3:\"Fair\",\n                       2:\"Poor\",\n                       1:\"Very Poor\"}\n\n\ntest_data['OverallQual']=test_data['OverallQual'].replace(OverallQual_col)","4a9ed4fb":"OverallCond_col = {10:\"Very Excellent\",\n                   9:\"Excellent\",\n                   8:\"Very Good\",\n                   7:\"Good\",\n                   6:\"Above Average\",\n                   5:\"Average\",\n                   4:\"Below Average\",\n                   3:\"Fair\",\n                   2:\"Poor\",\n                   1:\"Very Poor\"}\n\ntest_data['OverallCond']=test_data['OverallCond'].replace(OverallCond_col)","f95e79bf":"%matplotlib inline \n\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport sklearn\nimport statsmodels.api as sm\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n# special matplotlib argument for improved plots\nfrom matplotlib import rcParams","a4dc2a64":"# Let\u2019s first plot the distribution of the target variable. \n# We will use the histogram plot function from the matplotlib library.\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nplt.hist(train_data['SalePrice'], bins=30)\nplt.xlabel(\"House prices in $1000\")\nplt.show()","4b968f87":"plt.figure(figsize=(20, 5))\n\nfeatures = ['SaleType', 'SaleCondition']\ntarget = train_data['SalePrice']\n\nfor i, col in enumerate(features):\n    plt.subplot(1, len(features) , i+1)\n    x = train_data[col]\n    y = target\n    plt.scatter(x, y, marker='o')\n    plt.title(\"Variation in House prices\")\n    plt.xlabel(col)\n    plt.ylabel('\"House prices in $1000\"')","8fa6d7e5":"# Jointplots for high correlations - no. of rooms\n\n\nplt.figure (figsize=(10,10))\nsns.jointplot(x = 'TotRmsAbvGrd', y = 'SalePrice', data = train_data, kind = 'hex', color = 'green', height = 10)","ae230938":"# Jointplots for high correlations \n\nplt.figure (figsize=(10,10))\nsns.jointplot(x = 'YearBuilt', y = 'SalePrice', data = train_data, kind = 'reg', height = 10, color = 'orange');","7dd01479":"plt.figure(figsize=(20, 5))\n\nfeatures = ['SaleType', 'SaleCondition']\ntarget = train_data['SalePrice']\n\nfor i, col in enumerate(features):\n    plt.subplot(1, len(features) , i+1)\n    x = train_data[col]\n    y = target\n    plt.scatter(x, y, marker='o')\n    plt.title(\"Variation in House prices\")\n    plt.xlabel(col)\n    plt.ylabel('\"House prices in $1000\"')","0160ba09":"# extracting only the numeric columns from the training data, why?\n# because after getting dummies, and scaling, the dummy columns will have only 2 numbers\n#and we want to remove outliers but we can't rely on those, because they weren't originally numbers.\n#and the numeric values they represent aren't real.\n# later on we do a heatmap to find the most numaric featuers that effect the saleprice (target)\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nr = train_data.select_dtypes(include=numerics)\n\nscaler = StandardScaler()\nr_std = pd.DataFrame(scaler.fit_transform(r),columns = r.columns)\n\nfig = plt.figure(figsize= (15,15))\ncorr=r_std.corr()\nsns.heatmap(corr[['SalePrice']].sort_values(by=['SalePrice'],ascending=False),\n            vmin=-1,\n            cmap='coolwarm',\n            annot=True);\nfont = {\n        'color':  'black',\n        'weight': 'normal',\n        'size': 20,\n        }\nplt.title (\"Correlation\",fontdict=font)\n\n\n\n","88df0c12":"train_data.shape","25d3e339":"X= train_data.iloc[:,0:80]\n\nX_dum=pd.get_dummies(X)\ny=train_data[\"SalePrice\"]\n\n#adding columns that will exist in the testing data if you get dummies in it.\n#make them equal to zero in the trainng data\nu=['MSSubClass_1-1\/2 STORY PUD - ALL AGES', 'MiscFeature_None']\nfor i in u:\n    X_dum[i]=0\n\n\nscaler = StandardScaler()\ntrain_no_std = pd.DataFrame(scaler.fit_transform(X_dum),columns = X_dum.columns)\ntrain_no_std.shape\n","bccca45f":"train_no_std[\"GrLivArea\"]=train_no_std[\"GrLivArea\"].apply(lambda x:np.nan if x<-2.69 or x>2.69 else x)\n#train_no_std[\"TotalBsmtSF\"]=train_no_std[\"TotalBsmtSF\"].apply(lambda x:np.nan if x<-2.69 or x>2.69 else x)\n#train_no_std[\"GarageArea\"]=train_no_std[\"GarageArea\"].apply(lambda x:np.nan if x<-2.69 or x>2.69 else x)\ntrain_no_std[\"GarageCars\"]=train_no_std[\"GarageCars\"].apply(lambda x:np.nan if x<-2.69 or x>2.69 else x)\n#train_no_std[\"1stFlrSF\"]=train_no_std[\"1stFlrSF\"].apply(lambda x:np.nan if x<-2.69 or x>2.69 else x)\n#train_no_std[\"FullBath\"]=train_no_std[\"FullBath\"].apply(lambda x:np.nan if x<-1.96 or x>1.96 else x)\n#train_no_std[\"TotRmsAbvGrd\"]=train_no_std[\"TotRmsAbvGrd\"].apply(lambda x:np.nan if x<-1.96 or x>1.96 else x)\n#train_no_std[\"YearBuilt\"]=train_no_std[\"YearBuilt\"].apply(lambda x:np.nan if x<-1.96 or x>1.96 else x)\n#train_no_std[\"YearRemodAdd\"]=train_no_std[\"YearRemodAdd\"].apply(lambda x:np.nan if x<-1.96 or x>1.96 else x)","490da687":"train_no_std[\"SalePrice\"]=y","2648b452":"train_no_std.shape","5b39861c":"train_no_std.dropna(inplace= True)","7e0ba84e":"train_no_std.shape","8aade19a":"train_no_std.head()","3466bc44":"train_data.head()","d4893afd":"train_data.shape","f2024058":"t= train_data.iloc[:,0:80]\nt_dummy=pd.get_dummies(t)\nz=train_data[\"SalePrice\"]","817273c3":"#adding columns that will exist in the testing data if you get dummies in it.\n#make them equal to zero in the trainng data\n\nu=['MSSubClass_1-1\/2 STORY PUD - ALL AGES', 'MiscFeature_None']\nfor i in u:\n    t_dummy[\"MSSubClass_1-1\/2 STORY PUD - ALL AGES\"]=0\n    t_dummy[\"MiscFeature_None\"]=0","8ed5ce39":"scaler = StandardScaler()\ntrain_standerdized = pd.DataFrame(scaler.fit_transform(t_dummy),columns = t_dummy.columns)\ntrain_standerdized.shape","d0c38388":"train_standerdized[\"SalePrice\"]=z","3d81a30b":"train_standerdized.shape","aeafd722":"# we are testing the no outlier module train_no_std\n\nX=train_no_std.iloc[:,0:336]\ny=train_no_std.SalePrice","19b51911":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=10)","e47c2257":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n# create a LassoCV model instance\nmodel = LassoCV(alphas=np.logspace(-4, 4, 1000), cv=5) \n\n# fit the model\nmodel.fit(X_train, y_train)\n\n# get the best alpha\nprint('Best alpha:', model.alpha_)\n\n# evaluate on the training set\nprint('Training score:', model.score(X_train, y_train))\n\n# evaluate on the test set\nprint(\"Test Score:\", model.score(X_test, y_test))","04dfe93c":"from sklearn.model_selection import cross_val_score, cross_val_predict\n\nlasso_cross=Lasso(alpha=532.9994080844093)\n\nscores = cross_val_score(lasso_cross, X_train,y_train, cv=5)\n\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean of Ccoss-validated scores:\", scores.mean())","e8ae0992":"X=train_no_std.iloc[:,0:336]\ny=train_no_std.SalePrice","5eb2712e":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=10)","6d0a66ab":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n# create a LassoCV model instance\nmodel = RidgeCV(alphas=np.logspace(-4, 4, 1000), cv=5) \n\n# fit the model\nmodel.fit(X_train, y_train)\n\n# get the best alpha\nprint('Best alpha:', model.alpha_)\n\n# evaluate on the training set\nprint('Training score:', model.score(X_train, y_train))\n\n# evaluate on the test set\nprint(\"Test Score:\", model.score(X_test, y_test))","e2d1420e":"from sklearn.model_selection import cross_val_score, cross_val_predict\n\nRidg_cross=Ridge(alpha=136.18652367560827)\n\nscores = cross_val_score(Ridg_cross, X_train,y_train, cv=5)\n\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean of Ccoss-validated scores:\", scores.mean())\n\n\n","4657b6c9":"X=train_no_std.iloc[:,0:336]\ny=train_no_std.SalePrice","a9df6518":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=10)","4bbede1f":"from sklearn.linear_model import ElasticNetCV\n\nmodel__E = ElasticNetCV(alphas=np.logspace(-4, 4, 100),l1_ratio=np.arange(0.1, 1, 0.1) ,\n                       n_jobs=-1,verbose=1,cv=5) \n\n# fit the model\nmodel__E.fit(X_train, y_train)\n\n# get the best alpha\nprint('Best alpha:', model__E.alpha_)\n\nprint('Best l1_ratio:', model__E.l1_ratio_)\n\n# evaluate on the training set\nprint('Training score:', model__E.score(X_train, y_train))\n\n# evaluate on the test set\nprint(\"Test Score:\", model__E.score(X_test, y_test))","1fb32dce":"from sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import cross_val_score\n\nelastic_cross=ElasticNet(alpha=2.782559402207126,l1_ratio=0.9)\n\nscores = cross_val_score(elastic_cross,X_train,y_train, cv=5)\n\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean of Ccoss-validated scores:\", scores.mean())\n","5265b51d":"from sklearn.ensemble import RandomForestRegressor","a03d6059":"X=train_no_std.iloc[:,0:336]\ny=train_no_std.SalePrice","11042784":"rand = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=11,\n                      max_features=150, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, n_estimators=400,\n                      n_jobs=None, oob_score=False, random_state=None,\n                      verbose=0, warm_start=False)\nrand.fit(X,y)","9c7c5508":"from sklearn.model_selection import cross_val_score\n\nkf = KFold(n_splits=5, shuffle=True, random_state=10) # notice shuffle \nprint('Cross Validation Score:',cross_val_score(rand,X,y,cv=kf).mean())","cc648bd3":"from sklearn.ensemble import RandomForestRegressor\nX=train_no_std.iloc[:,0:336]\ny=train_no_std.SalePrice","c53f4083":" rand_fo= RandomForestRegressor()\n\nrand_pa = {'n_estimators': [200, 400],\n     'max_features':np.arange(100,250,10),\n    'max_depth': [5, 6,7,8,9,10,11]}\n","83fda8be":"rand_grid = GridSearchCV(rand_fo, rand_pa, n_jobs = -1, verbose = 1)\nrand_grid.fit(X,y)","94aebbc9":"rand_grid.best_estimator_","e9cc8ee2":"X= train_standerdized.iloc[:,0:336]\ny=train_standerdized.SalePrice","39ea6ccb":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=10)","7c5d8c17":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\nmodel_lass = LassoCV(alphas=np.logspace(-4, 4, 1000), cv=5) \n\n# fit the model\nmodel_lass.fit(X_train, y_train)\n\n# get the best alpha\nprint('Best alpha:', model_lass.alpha_)\n\n# evaluate on the training set\nprint('Training score:', model_lass.score(X_train, y_train))\n\n# evaluate on the test set\nprint(\"Test Score:\", model_lass.score(X_test, y_test))","9c6e1923":"from sklearn.model_selection import cross_val_score, cross_val_predict\n\nlasso_cross=Lasso(alpha=1581.9734815786014)\n\nscores = cross_val_score(lasso_cross, X_train,y_train, cv=5)\n\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean of Ccoss-validated scores:\", scores.mean())","bfa94822":"X= train_standerdized.iloc[:,0:336]\ny=train_standerdized.SalePrice","eaaf463a":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=10)","25ed478a":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV , ElasticNetCV\n\nmodel = RidgeCV(alphas=np.logspace(-4, 4, 1000), cv=5) \n\n# fit the model\nmodel.fit(X_train, y_train)\n\n# get the best alpha\nprint('Best alpha:', model.alpha_)\n\n# evaluate on the training set\nprint('Training score:', model.score(X_train, y_train))\n\n# evaluate on the test set\nprint(\"Test Score:\", model.score(X_test, y_test))","9effbceb":"from sklearn.model_selection import cross_val_score, cross_val_predict\n\nredg_cross=Ridge(alpha=845.1366330684722)\n\nscores = cross_val_score(redg_cross, X_train,y_train, cv=5)\n\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean of Ccoss-validated scores:\", scores.mean())","6e1e4d6f":"from sklearn.linear_model import ElasticNetCV\nX= train_standerdized.iloc[:,0:336]\ny=train_standerdized.SalePrice\n","f0e53a67":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=10)","f808ec76":"model_E = ElasticNetCV(alphas=np.logspace(-4, 4, 100),l1_ratio=np.arange(0.1, 1, 0.1) ,\n                       n_jobs=-1,verbose=1,cv=5) \n\n\n# fit the model\nmodel_E.fit(X_train, y_train)\n\n# get the best alpha\nprint('Best alpha:', model_E.alpha_)\n\nprint('Best l1_ratio:', model_E.l1_ratio_)\n\n# evaluate on the training set\nprint('Training score:', model_E.score(X_train, y_train))\n\n# evaluate on the test set\nprint(\"Test Score:\", model_E.score(X_test, y_test))","4d22df01":"from sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import cross_val_score\n\nelastic_cross=ElasticNet(alpha=8.497534359086455,l1_ratio=0.9)\n\nscores = cross_val_score(elastic_cross,X_train,y_train, cv=5)\n\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean of Ccoss-validated scores:\", scores.mean())\n","56492767":"X=train_standerdized.iloc[:,0:336]\ny=train_standerdized.SalePrice","73b4d337":"rand_fo= RandomForestRegressor()\n\nrand_pa = {'n_estimators': [200, 400],\n     'max_features':np.arange(100,250,10),\n     'max_depth': [5, 6,7,8,9,10,11]}\n","176ade3a":"rand_grid = GridSearchCV(rand_fo, rand_pa, n_jobs = -1, verbose = 1)\nrand_grid.fit(X,y)","73fc8a19":"rand_grid.best_estimator_","939f56c0":"X=train_no_std.iloc[:,0:336]\ny=train_no_std.SalePrice","b440c207":"rand_ = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=11,\n                      max_features=150, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, n_estimators=400,\n                      n_jobs=None, oob_score=False, random_state=None,\n                      verbose=0, warm_start=False)\nrand_.fit(X,y)","fe945d1a":"kf = KFold(n_splits=5, shuffle=True, random_state=10) # notice shuffle \nprint('Cross Validation Score:',cross_val_score(rand_,X,y,cv=kf).mean())","04fd4aff":"test_data.head()","9bd882da":"test_dum=pd.get_dummies(test_data)","0ed6068c":"test_dum.shape","e6099a2c":"# finding columns names that were in the trainng data but not on the testing data,\n# adding them and make them equal to zero  ( not including the target column saleprice)\ndiff = [x for x in train_no_std.columns if x not in test_dum.columns]\ndiff","41c1b862":"r=['Utilities_NoSeWa',\n 'Condition2_RRAe',\n 'Condition2_RRAn',\n 'Condition2_RRNn',\n 'HouseStyle_2.5Fin',\n 'RoofMatl_ClyTile',\n 'RoofMatl_Membran',\n 'RoofMatl_Metal',\n 'RoofMatl_Roll',\n 'Exterior1st_ImStucc',\n 'Exterior1st_Stone',\n 'Exterior2nd_Other',\n 'Heating_Floor',\n 'Heating_OthW',\n 'Electrical_Mix',\n 'GarageQual_Ex',\n 'PoolQC_Fa',\n 'MiscFeature_TenC',\n 'MiscFeature_standerd']\n\n\nfor i in r:\n    test_dum[i]=0\n\n    ","76632787":"train_no_std.shape","d4f37ccd":"test_dum.shape","4b6cceda":"test_dum = pd.DataFrame(scaler.transform(test_dum),columns = test_dum.columns)\n#scaler.transform(test_dum)","03262d2c":"test_dum.shape","21200ce1":"rand.predict(test_dum)","d6ef4934":"submit=pd.DataFrame(columns=['Id',\"SalePrice\"])","83670b11":"submit.Id=test_data.Id\nsubmit.SalePrice=rand.predict(test_dum)\nsubmit.head()","c564da83":"submit.to_csv('randomeforest - 2 featuers - 2.69 outliers jjj.csv', index=False)","dff906f0":"## Randomeforest","83c7e90d":"## A. Getting the right data types ","ac5e788e":"## Elastic Net Cv ","ae3006b4":"# 2. EDA and visualization on the training data","1f950e44":"## lasso ","7ecfc9c4":"# 3. Cleaning trainng data ","6a2b97d9":"**Initial Visualization**","a67ccd62":"## B. Gitting the right data types: Test-data","ff99618c":"Screen Shot 2019-10-11 at 16.41.56![image.png](attachment:image.png)","ef210c39":"### fist of all we have worked with 2 diffrent trainng data, one that we have orignllay recived form kaggle, and the other one where we have selecet 2 numaric fetuers that have a hihgly corlation with the target (Saleprice).\n\n### both of these data are scaled, filled the missing values, and completllt clean, the only diffrence is that in one we have removed the outliers using 2 featuers.\n\n\n### now in both of these trainng dataset, we have tried 4 diffrent approches:\n1-Ridge\n2-Lasso\n3- ElasticNet \n4- randome forest \nin all of these tests, we used the cv virsion before to get the best parameter ( RidgeCV, LassoCV , ElasticNetCV, GridSearchCV)\n\n### the scores of the mean cross valdation were like the following:\n#### after removing the outliers (train_no_std)\n1- ridge: 0.9053094586067507\n2- lasso: 0.9053094586067507\n3- ElasticNet: 0.8987689482673635\n4- randomeforest :  0.0.8833751936703015\n\n\n#### the orignal data  (train_standerdized)\n1- lasso : 0.7605890442091523\n2- ridge : 0.7930685106393112\n3- ElasticNet : 0.79267206152532\n4- randomforest : 0.0.8835427127158999\n\n\n### in all the test (exept the randomforest) the data with the removed outliers preformed better in the cross-valedation and even in the randomforest there wasn't great diffrence.\n\n\n# At the end the randome forest with the removed outliers preforemd better in kaggle with the 0.20624\n\nwe had a better results also using the randomeforest (0.20024)  but we forget which parameter we used with ): \n\n\n","e8d04a0e":"https:\/\/www.kaggle.com\/yazeidalqahtani\/project2-part1?scriptVersionId=22930796","09fca1fe":"## B. Treating missing data in train datasets:","bd4bab6e":"# 11. Conclusion","632588df":"## A. Treating missing data in test datasets:","ff6087a3":"____\n____\n____","b9317d0d":"# 5.cleaning  the test data","f878a90d":"## B. without deleiting the outliers (train_standerdized)","9f30514c":"## Ridge","d03f723d":"# 8. Splittng the train data into two datasets","c170d4af":"## randome forest  GridSearchCV","73a05931":"## Ridge","43c99617":"# 4. EDA on test_data ","92580332":"# 9. Bulding the models ","6d2ac715":"## A. Testing on the data without outliers (train_no_std)\n1- lasso \n\n2- ridge\n\n3- Elastic Net Cv \n\n4- randome forest \n\n5- randome forest  GridSearchCV","3ffae70b":"## A. without outliers ( train_no_std)","a926f681":"## lasso ","430f1280":"# 10. Prepering: the test data (Featuer Engineering )","7a67affd":"# 1. Load Libraries and Data","34d1a412":"# 7. Visualization ( After filling the missing data)","cccf3e60":"### we selected the fetuers that have 0.5 or more in the heatmap to remove the outliers.\n### the approch we followed was the same as the IQR approch but with the standerdevation, see the picture below.","b5402c9d":"## B. Testing on the reguler trainning data  (train_standerdized)\n\n1- lasso \n\n2- ridge\n\n3- Elastic Net Cv \n\n4- randome forest GridSearchCV\n\n5- randome forest","01462570":"## Randome forest ","7f5bcdf9":"## Elastic Net Cv ","7af52b78":"\n\n\n\n<img src=\"http:\/\/imgur.com\/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n\n# Project-2 | Part-1 : Housing Values in Suburbs of Boston\n\n\n**Wejdan, Yazeed and Amal.**\n\n---","3aa93a55":"____\n____\n____","67d3cbb0":"## Randomeforest gridsearch"}}