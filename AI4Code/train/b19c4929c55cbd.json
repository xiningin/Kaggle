{"cell_type":{"bd18e97a":"code","5fb8afef":"code","40c1ad49":"code","f1823719":"code","9deefac1":"code","272ea642":"code","ca600b40":"code","6317146d":"code","34eafee4":"code","eb8e5b88":"code","cc8c631d":"code","b621a626":"code","7ac91a5d":"code","3319683e":"code","2821d9b4":"code","dbeb6a9e":"code","b3292950":"code","8a7f644e":"code","1990dc4c":"code","cb29a73f":"code","bd23b49f":"code","d046d5c8":"code","1ca0af39":"code","a25ffee4":"code","5a75d78c":"code","9bcbb00e":"code","fe0d9193":"code","8ebefe5f":"code","722a1ece":"code","7b5bffc4":"code","da9b39d0":"code","4c19a93e":"code","efbc88ec":"code","6987c542":"code","7d21149f":"markdown","e6f71da5":"markdown","baa7c17f":"markdown","0a06d8ff":"markdown","03617b64":"markdown"},"source":{"bd18e97a":"import numpy as np\nimport pandas as pd\nimport re\nimport string\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score,precision_score,recall_score\nfrom sklearn import svm","5fb8afef":"df = pd.read_csv('..\/input\/all.csv')   #importing csv","40c1ad49":"df.head() #showing the first 5 content","f1823719":"df.shape","9deefac1":"df.info()","272ea642":"df.isnull().sum()","ca600b40":"# removing empty data\ndf.dropna(inplace=True)","6317146d":"df.groupby('type').count()","34eafee4":"content = df['content'].tolist()[:3]","eb8e5b88":"print(content)","cc8c631d":"labels = 'Love', 'Mythology & Folklore', 'Nature'\nsizes = [326, 58, 187]\n\n\nfig1, ax1 = plt.subplots(figsize=(5,5))\nax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=False, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","b621a626":"df.content.str.lower()","7ac91a5d":"# remove list in the document\nremove_list=[\"A\",\n\"An\",\n\"The\",\n\"Aboard\",\n\"About\",\n\"Above\",\n\"Absent\",\n\"Across\",\n\"After\",\n\"Against\",\n\"Along\",\n\"Alongside\",\n\"Amid\",\n\"Among\",\n\"Amongst\",\n\"Anti\",\n\"Around\",\n\"As\",\n\"At\",\n\"Before\",\n\"Behind\",\n\"Below\",\n\"Beneath\",\n\"Beside\",\n\"Besides\",\n\"Between\",\n\"Beyond\",\n\"But\",\n\"By\",\n\"Circa\",\n\"Concerning\",\n\"Considering\",\n\"Despite\",\n\"Down\",\n\"During\",\n\"Except\",\n\"Excepting\",\n\"Excluding\",\n\"Failing\",\n\"Following\",\n\"For\",\n\"From\",\n\"Given\",\n\"In\",\n\"Inside\",\n\"Into\",\n\"Like\",\n\"Minus\",\n\"Near\",\n\"Of\",\n\"Off\",\n\"On\",\n\"Onto\",\n\"Opposite\",\n\"Outside\",\n\"Over\",\n\"Past\",\n\"Per\",\n\"Plus\",\n\"Regarding\",\n\"Round\",\n\"SO\",\n\"Save\",\n\"Since\",\n\"Than\",\n\"Through\",\n\"To\",\n\"Toward\",\n\"Towards\",\n\"Under\",\n\"Underneath\",\n\"Unlike\",\n\"Until\",\n\"Up\",\n\"Upon\",\n\"Versus\",\n\"Via\",\n\"With\",\n\"Within\",\n\"Without\"]\n","3319683e":"# replace those words with space\nfor  value in remove_list:\n    df.content=df.content.str.replace(value,\" \")\ndf.content","2821d9b4":"#function to remove punctuation\ndef removePunctuation(x):\n    x = x.lower()\n    x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n    x = x.replace('\\r','')\n    x = x.replace('\\n','')\n    x = x.replace('  ','')\n    x = x.replace('\\'','')\n    return re.sub(\"[\"+string.punctuation+\"]\", \" \", x)\n\n#getting stop words\nfrom nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\")) \n\n\n#function to remove stopwords\ndef removeStopwords(x):\n    filtered_words = [word for word in x.split() if word not in stops]\n    return \" \".join(filtered_words)\n\n\ndef processText(x):\n    x= removePunctuation(x)\n    x= removeStopwords(x)\n    return x\n\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nX= pd.Series([word_tokenize(processText(x)) for x in df['content']])\nX.head()","dbeb6a9e":"# regular expression, using stemming: try to replace tail of words like ies to y \nimport re\n\ndf.content = df.content.str.replace(\"ing( |$)\", \" \")\ndf.content = df.content.str.replace(\"[^a-zA-Z]\", \" \")\ndf.content = df.content.str.replace(\"ies( |$)\", \"y \")","b3292950":"def sent_to_words(content):\n    return [np.array([x.split() for x in poem.split()]) for poem in content]","8a7f644e":"poem = sent_to_words(content)","1990dc4c":"def build_dict(poem):\n    dictionary = {}\n    rev_dict = {}\n    count = 0\n    for content in poem:\n        for i in content:\n            if i[0] in dictionary:\n                pass\n            else:\n                dictionary[i[0]] = count\n                count += 1\n    rev_dict = dict(zip(dictionary.values(), dictionary.keys()))\n    return dictionary, rev_dict","cb29a73f":"dictionary, rev_dict = build_dict(poem)","bd23b49f":"import tensorflow as tf\nfrom tensorflow.contrib import rnn","d046d5c8":"vocab_size = len(dictionary)","1ca0af39":"# Parameters\nlearning_rate = 0.0001\ntraining_iters = 1600\ndisplay_step = 200\nn_input = 9","a25ffee4":"# number of units in RNN cell\nn_hidden = 512","5a75d78c":"# tf Graph input\ntf.device(\"\/device:GPU:0\")\nx = tf.placeholder(\"float\", [None, n_input, 1])\ny = tf.placeholder(\"float\", [None, vocab_size])","9bcbb00e":"# RNN output node weights and biases\nweights = {\n    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n}\nbiases = {\n    'out': tf.Variable(tf.random_normal([vocab_size]))\n}","fe0d9193":"def RNN(x, weights, biases):\n\n    # reshape to [1, n_input]\n    x = tf.reshape(x, [-1, n_input])\n\n    # Generate a n_input-element sequence of inputs\n    # (eg. [had] [a] [general] -> [20] [6] [33])\n    x = tf.split(x,n_input,1)\n\n    # 2-layer LSTM, each layer has n_hidden units.\n    # Average Accuracy= 95.20% at 50k iter\n    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])\n    # generate prediction\n    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n    # there are n_input outputs but\n    # we only want the last output\n    return tf.matmul(outputs[-1], weights['out']) + biases['out']","8ebefe5f":"pred = RNN(x, weights, biases)","722a1ece":"# Loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)","7b5bffc4":"# Model evaluation\ncorrect_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","da9b39d0":"saver = tf.train.Saver()\ninit = tf.global_variables_initializer()","4c19a93e":"df_train = sent_to_words(content)","efbc88ec":"j = 0\nfor i in df_train:\n    if i.shape[0] <= n_input:\n        df_train = np.delete(df_train, (j), axis = 0)\n        j -= 1\n    j += 1","6987c542":"with tf.Session() as session:\n    session.run(init)\n    step = 0\n    end_offset = n_input + 1\n    acc_total = 0\n    loss_total = 0\n    while step < training_iters:\n        acc_total = 0\n        loss_total = 0\n        j = 0\n        for training_data in df_train:\n            m = training_data.shape[0]\n            windows = m - n_input\n            acc_win = 0\n            for window in range(windows):\n                batch_x = training_data[window : window + n_input]\n                batch_y = training_data[window + n_input]\n                symbols_in_keys = [dictionary[i[0]] for i in batch_x]\n                symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n        \n                symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n                symbols_out_onehot[dictionary[batch_y[0]]] = 1.0\n                symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n\n                _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n                loss_total += loss\n                acc_win += acc\n            acc_total += float(acc_win) \/ m\n        acc_total \/= len(df_train)\n        if (step+1) % display_step == 0:\n            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n                  \"{:.6f}\".format(loss_total\/display_step) + \", Average Accuracy= \" + \\\n                  \"{:.2f}%\".format(100*acc_total))\n        step += 1\n    print(\"Optimization Finished!\")\n    save_path = saver.save(session, \"..\/working\/model.ckpt\")\n    print(\"Model saved in path: %s\" % save_path)\n    while True:\n        prompt = \"%s words: \" % n_input\n        sentence = 'When I Queen Mab within my fancy viewed, My'\n        sentence = sentence.strip()\n        words = sentence.split(' ')\n        if len(words) != n_input:\n            continue\n        try:\n            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n            for i in range(64):\n                keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n                onehot_pred = session.run(pred, feed_dict={x: keys})\n                onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n                sentence = \"%s %s\" % (sentence,rev_dict[onehot_pred_index])\n                symbols_in_keys = symbols_in_keys[1:]\n                symbols_in_keys.append(onehot_pred_index)\n            print(sentence)\n            break\n        except:\n            print(\"Word not in dictionary\")","7d21149f":"**Convert Sentences into List of words**","e6f71da5":"**Convert Char to Number Mapping**","baa7c17f":"**Convert Data into Feature**","0a06d8ff":"Importing Library","03617b64":"**Tensor Starts**"}}