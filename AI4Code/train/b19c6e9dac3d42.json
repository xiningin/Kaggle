{"cell_type":{"d1fb3f94":"code","b53913ad":"code","8c8d1f4f":"code","459ffdd8":"code","279ede5a":"code","183674b2":"code","ba2fdd1e":"code","9281c91e":"code","feac88f2":"code","dd90d9a1":"code","5f7f0fae":"code","93164ab0":"code","f90cd551":"code","da7276e3":"code","b573eb84":"code","9f89f5fe":"code","8000a147":"code","0422de4a":"code","8e0b9aff":"code","5e1898c5":"code","e17442fa":"code","41492649":"code","3eaf03a0":"code","1b8f9c06":"code","f289ccac":"code","05d519d4":"code","df9942a6":"code","7d586e57":"code","d543b981":"code","d08f8599":"code","4a30f5ab":"code","584b85b6":"code","b9ef039a":"code","a35696b3":"code","db3a79de":"code","9c023e52":"code","fff93f1e":"code","483a9ee4":"code","b912e0fb":"code","316cbb2c":"code","1f466b38":"code","7915db6b":"code","58d2e5ed":"code","d4f6dc9e":"code","01716db9":"code","4c9aee8a":"code","80d88d67":"code","6f8f8242":"code","7281d486":"code","f9f6c7bc":"code","da069732":"code","089fbda7":"code","70314f5b":"code","0178f29f":"code","b6761fcd":"code","0bc56ca4":"code","fca51a0d":"code","53c0ef6a":"code","8fedb0cc":"code","7fb8d155":"code","4d427459":"code","331122a2":"code","600d03fb":"code","7df95abe":"code","7c4e0b9c":"code","1abbd7d6":"code","54494050":"code","7cffd140":"code","bdb643f6":"code","747fe370":"code","4540b5f8":"code","840f2245":"code","787ea569":"code","22281dff":"code","65fa9d70":"code","ad6d296b":"code","78a4e3be":"code","6cbdc787":"code","2490b37b":"code","8dbabf4f":"code","fabac3aa":"code","5a6b608b":"code","b59078a0":"code","75bb192b":"markdown","dec4d973":"markdown","b0b308d7":"markdown","f685b464":"markdown","f5f6eb79":"markdown","9a5192c8":"markdown","3e1227f9":"markdown","df1d34cb":"markdown","bf15c5ff":"markdown","b2305d88":"markdown","899e1b38":"markdown","93dc9732":"markdown","b4bc70d5":"markdown","f2aecbdc":"markdown","5c11e7f7":"markdown","eb724828":"markdown","548208b1":"markdown","d13fff69":"markdown","99a02b64":"markdown","f396f15c":"markdown","19d0a375":"markdown","5a3f847c":"markdown","b1ac6b95":"markdown","e41ae6bd":"markdown","25458867":"markdown","8d9afdf9":"markdown","d1975487":"markdown","2e69d9db":"markdown","31717718":"markdown","2e1c8968":"markdown","c5d2ae78":"markdown","2841b58d":"markdown","909224f2":"markdown","559e6ab6":"markdown","b4852e18":"markdown","5bea55f6":"markdown","d5b4c7ab":"markdown","7b407699":"markdown","6c54240a":"markdown","4d592a67":"markdown","11e53155":"markdown","c0b611cc":"markdown","708f7e60":"markdown","b32243ac":"markdown","a0414891":"markdown","d3fe0c3e":"markdown","74dbcfb9":"markdown","67422aee":"markdown","4509dd3c":"markdown","3bad86dc":"markdown","5d4f4ce8":"markdown","d0b284ac":"markdown","f85250e6":"markdown","b96f8866":"markdown","5e821674":"markdown","056e09c6":"markdown","31d7d8ac":"markdown","a64196e6":"markdown","a3d1d97c":"markdown","b52864b2":"markdown","7e450e7c":"markdown","3749f7e6":"markdown","012eb9e0":"markdown","f1c86377":"markdown","e18fe0f1":"markdown","a38740e0":"markdown","fe0fc64d":"markdown"},"source":{"d1fb3f94":"import numpy as np # numpy is base of everything we see in machine leraning using python\nimport pandas as pd # pandas handles the data in a formatted way\nimport torch # it is the pytorch package\nfrom torch import nn # Neural Network Package\nfrom torch import optim # Optimizer\nimport torch.nn.functional as F # We use Functional API for flexibility\nimport torchvision # to deal with images\nfrom torchvision import transforms # performs transformations\nimport matplotlib.pyplot as plt # Our FAV\nfrom tqdm import tqdm # Fancy tool that does not let you get bored while in training\nfrom sklearn.metrics import confusion_matrix # confused? DOn't be. It is just a simple confusion matrix ;)\nimport seaborn as sns # how can we forget seaborn when we want fancy\nfrom torch.utils.tensorboard import SummaryWriter # this is where the magic happens in real time\nimport itertools # you know, some tools to do random things for you","b53913ad":"t = torch.tensor([[1,2],[3,4]]) # create a basic tensor\nprint(t.shape)\nt","8c8d1f4f":"rank = len(t.shape) # length of shape = Rank of a tensor\nrank\n# rank of a tensors tells us that how deep do we have to go before we can access the values. Going deep below the\n# rank will always give you another tensor of a 2-D shape","459ffdd8":"print(t[0][0]) # see the output that it's a tensor\n\n# to access a value from it, use\nitem = t[0][0].item()\nprint(item)\n\n# item object is only for tensors having \" scalar values\" try using t[0].item()","279ede5a":"print(t.device) # get the current operating device\nprint(type(t))\nprint(t.type()) # get the type of a tensor. there are many types of tensors in PyTorch. See the docs\nprint(t.dtype) # what is the type of data inside that specific tensor\nprint(t.layout) # stride is just another name for Dense","183674b2":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # this will automatically set operations on GPU\nprint('Using device:', device)\n\nif device.type == 'cuda':\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)\/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_cached(0)\/1024**3,1), 'GB')","ba2fdd1e":"t1 = torch.tensor([[1,2],[2,3]])\nt2 = torch.tensor([[1.1,2.2],[2.2,3.3]])\n# t3 = t1.cuda() # No CUDA Present error\n\nprint(t1.dtype)\nprint(t2.dtype)","9281c91e":"t1+t2\n# print(t1+t3) # it'll produce an error. Check for yourself","feac88f2":"data = np.array([1,2,3])","dd90d9a1":"\nt1 = torch.Tensor(data)  # pass data inside the Constructor of the Tensor class directly\n# equivalent to t = torch.tensor(data,dtype=torch.float32)\n\nprint(t1)\nprint(f't1 belongs to: {type(t1)}')\nprint(f'Type of t1 is: {t1.type()}')\nprint(f'Type of data inside t1 is {t1.dtype}')\n","5f7f0fae":"t2 = torch.tensor(data) # Factory Function \n#  equivalent to t = torch.tensor(data,dtype=torch.int64)\n\n\nprint(t2)\nprint(type(t2))\nprint(t2.type())\nprint(t2.dtype)","93164ab0":"t3 = torch.from_numpy(data) # (Factory) Function\n\nprint(t3)\nprint(type(t3))\nprint(t3.type())\nprint(t3.dtype)\n","f90cd551":"t4 = torch.as_tensor(data) # Factory Function\n\nprint(t4)\nprint(type(t4))\nprint(t4.type())\nprint(t4.dtype)\n","da7276e3":"data[0] = 5\ndata[2] = 7","b573eb84":"print(t1)\nprint(t2)\nprint(t3)\nprint(t4)","9f89f5fe":"# each value represents a pixel of 4*4 image\none = torch.tensor([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]).reshape(4,4)  # converts into a 4*4 matrix\nprint(one,'\\n')\n\ntwo = torch.tensor([2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2]).reshape(4,4)\nprint(two,'\\n')\n\nthree = torch.tensor([3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3]).reshape(4,4)\nprint(three,'\\n')\n\nstacked = torch.stack((one,two,three)) # make a batch of 3 images so shape is (3,4,4)\nprint('Stacked:\\n',stacked)\n\nstacked_reshaped = stacked.reshape((3,1,4,4)) # batch, color channels, height, width\n\nstacked_reshaped_flattened = stacked_reshaped.flatten(start_dim=1) \n# leave the batch as it is but flatten all the pixels\nstacked_reshaped_flattened","8000a147":"another_method = stacked_reshaped.reshape((3,16)) # Flatten each image\nanother_method","0422de4a":"print(t1*(3))\nprint(t1.mul(3))\n# it is not scaler multiplication but the scaler is broadcasted","8e0b9aff":"t1%2 == 0 ","5e1898c5":"# It is just like making an tensor of same shape as of given tensor but with all the scaler values\nbroadcasted_3 = torch.from_numpy(np.broadcast_to(3,t1.shape))\nprint(f'Broadcasted tensor: {broadcasted_3}')\nprint(t1*broadcasted_3)\nprint(t1.mul(broadcasted_3))","e17442fa":"train_set = torchvision.datasets.FashionMNIST(download=True,root='.\/Data\/FashionMNIST',\n                                              transform=transforms.Compose([transforms.ToTensor()]))\n\ntrain_loader = torch.utils.data.DataLoader(train_set,batch_size=32)","41492649":"print(f'There are {len(train_set.targets)} images in our data  where each one belongs to one of 10 classes and the classes are LabelEncoded as {dict(zip(train_set.classes,range(10)))}')\n","3eaf03a0":"fig = plt.figure(figsize=(5,5))\n\nplt.pie(train_set.targets.bincount().numpy(),\n        labels=dict(zip(train_set.classes,range(10))),\n        autopct='%1.2f%%')\n\nplt.title('Distribution of Classes in the whole dataset',size='x-large')\nplt.show()","1b8f9c06":"sample = next(iter(train_set))\nprint(type(sample))\nprint(len(sample))\n# first entry sample[0] are the pixels and second sample[1] is the label\npixels,label = sample # or pixels,labels = sample[0], sample[1]\nprint(pixels.shape)","f289ccac":"plt.imshow(pixels.reshape(28,28),cmap='gray')\n# plt.imshow(pixels.squeeze()) # there is an extra image axis so we have to remove it\nplt.show()","05d519d4":"batch = next(iter(train_loader)) # iter makes the list as an iterator. Iterator gives you an object one at a time\n# With each time instance a new object is generated (yield) like factory rather than a warehouse (return)\nprint(f'A batch has {len(batch)} elements and type of batch is {type(batch)}')","df9942a6":"images,labels = batch\nprint(f'Number of images in a single batch is {len(images)} and there are {len(labels)} labels corresponding to each image')\nprint(f'Type of Images is {type(images)} which is a collection of images  of (28x28) pixels')\nprint(f'Shape of those Images is {images.shape}')","7d586e57":"grid = torchvision.utils.make_grid(images,nrow=10) #nrow is number of elements in a single row\nplt.figure(figsize=(15,12))\nplt.imshow(np.transpose(grid,(1,2,0))) # transpose to (h,w,c) instead of (c,h,w)\nprint(labels)","d543b981":"class Network(nn.Module):\n    def __init__(self):\n        super(Network,self).__init__() # Super is indeed super\n        \n        self.conv_1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=(5,5),bias=True)\n        # for single grayscale image use 6 different kernls of size (5,5) to produce 6 diff feature maps \n        \n        self.conv_2 = nn.Conv2d(in_channels=6,out_channels=12,kernel_size=(3,3))\n        # from the existing 6 already feat maps, use 12 different kernal filters of size (3,3) to get TOTAL of 12 new feat\n        \n        self.dense_1 = nn.Linear(in_features=12*4*4,out_features=128) # WHYY (12*4*4) it is explained in the end\n        # Flatten the output of conv_2d in 12*4*4\n        \n        self.fc_2 = nn.Linear(in_features=128,out_features=64)\n        # Fully Connected = fc_2 = Dense Layer = dense_2\n        \n        self.out = nn.Linear(in_features=64,out_features=10)\n        # output layer. Output number of neurons = num of classes for classification & 1 for regression\n        \n    \n    def forward(self,t):\n        '''\n        implement a forward pass on a Tensor 't' of rank 'R'\n        '''\n        # input  layer 1 though it is never needed\n        # t = t\n        \n        # second layer. Layer is a mix of functions that has weights \n        t = self.conv_1(t) # works by calling __call__() method inside class\n        t = F.relu(input=t,) # it is not a layer but Function (layers have weights, activations don't)\n        t = F.max_pool2d(t,kernel_size=(3,3),stride=2) # max pooling\n        \n        # third layer\n        t = self.conv_2(t) # works by calling __call__() method inside class\n        t = F.relu(input=t,) # it is not a leyer but Function as layers have weights\n        t = F.max_pool2d(t,kernel_size=(3,3),stride=2) # max pool\n        \n        # fourth layer\n        t = t.reshape(-1,12*4*4) \n        # due to Conv and pooling operations, our image has been reduced from (1,28,28) to (4,4)\n        # use ((input_size - filter_size + 2*padding)\/stride )+1  for each  cov and max_pool \n        # it assumes input and kernel size are square\n        t = self.dense_1(t)\n        t = F.relu(t)\n        \n        # Fifth layer\n        t = self.fc_2(t)\n        t = F.relu(t)\n        \n        # output\n        t = self.out(t)\n        # t = F.softmax(t,dim=1)\n        # commented because loss function used will be cross_entropy which has softmax behind the scenes\n        return t","d08f8599":"# torch.set_grad_enabled(False) # stop making computational graphs it is True by default","4a30f5ab":"tb = SummaryWriter() # instantiate the tensorboard object\n\nnetwork = Network() # instantiate object of Nwtwork\n\nbatch = next(iter(train_loader))\nimages,labels = batch\n\ngrid = torchvision.utils.make_grid(images,nrow=15)\n\ntb.add_image('image_grid',grid)\ntb.add_graph(network,images)\n\ntb.close()\n# after running this, check your local machine's address given by the shell to see the graphs and everything","584b85b6":"sample = next(iter(train_set))\nimage, label = sample\nimage.shape # add a new index to the the image to convert it into a batch of 1","b9ef039a":"batch_image = image.reshape((1,1,28,28))\nprint(batch_image.shape)\n\n# or by using \n\nbatch_image = image.unsqueeze(dim=0)\nprint(batch_image.shape)","a35696b3":"y_pred = network(batch_image) \ny_pred\n# y_pred is NOT the  probabilities for each label. These are final output Tensor because we have not used softmax","db3a79de":"F.softmax(y_pred,dim=1) # these are the probabilities for each class","9c023e52":"y_pred.shape # shape is (1,10) means 1 image and 10 predictions","fff93f1e":"y_pred.argmax(dim=1) # get the index where value is maximum","483a9ee4":"torch.set_grad_enabled(True) # True by default but as we had turned it off so turning it on","b912e0fb":"train_set = torchvision.datasets.FashionMNIST(download=True,root='.\/Data\/FashionMNIST',\n                                              transform=transforms.Compose([transforms.ToTensor()]))\n\ntrain_loader = torch.utils.data.DataLoader(train_set,batch_size=32) ","316cbb2c":"batch = next(iter(train_loader)) # make a new batch from DataLoader\nimages,labels = batch # get images and labels from batch","1f466b38":"print(len(labels)) # 32 labels for 32 images\nprint(images.shape) # 32 grayscale images of size (28,28)","7915db6b":"network = Network() # weights are random everytime you initialize\npred = network(images)\npred.shape # 10 labels for each of 32 images","58d2e5ed":"pred.argmax(dim=1) # get predictions for the images after 1 instance\n","d4f6dc9e":"labels # original labels","01716db9":"pred.argmax(dim=1).eq(labels) # element wise operation. Return True if equal else return False","4c9aee8a":"def get_correct_pred(pred,labels,percent=False):\n    num = pred.argmax(dim=1).eq(labels).sum().item()\n    if percent:\n        return (num\/len(labels))*100 # 4 out of 32 correct predictions\n    else:\n        return num","80d88d67":"print(f'{get_correct_pred(pred,labels,True)}% of labels have been predicted correctly')","6f8f8242":"loss = F.cross_entropy(pred,labels)\nloss.item() # this is our loss function\n\nprint(network.conv_1.weight.grad) # No Gradients  present NOW on the First Pass for any of the layer","7281d486":"loss.backward() # works only if set_grad_enable(True)\nprint(network.conv_1.weight.grad.shape) # Now you can see Updated Gradients for every layer\nprint(network.conv_2.weight.grad.shape)","f9f6c7bc":"optimizer = optim.Adam(network.parameters(),lr=0.01) \n# lr is learning rate. High learning rate is fast but produce less accuracy and vice versa\n\noptimizer.step() # update weights","da069732":"# Predict again and you will see a decrease in loss\n\npred = network(images)\nprint(get_correct_pred(pred,labels))\nloss = F.cross_entropy(pred,labels)\nloss.item()","089fbda7":"BATCH = 128 # set the batch size\nlr = 0.001 # set learning rate\nEPOCH = 5 # set epoch. 1 epoch means the whole data will be presented to the network in batches. So in this case,\n# Whole data will be presented to the network 5 times  in (60000\/\/128) steps per epoch\n\nnetwork = Network() # instantiate network\noptimizer = optim.Adam(network.parameters(),lr=lr) # instantiate optimizer\n\ncomment = f\"Hyper Parameters: BATCH={BATCH}, lr={lr}\" # this is a dynamic representation.You can put code inside {}\ntb = SummaryWriter(comment=comment) # instantiate Tensorboard for live evaluation\n\nfor epoch in range(EPOCH): # train for 5 epoch\n    \n    total_loss = 0\n    total_accuracy = 0\n\n    train_loader = torch.utils.data.DataLoader(train_set,batch_size=BATCH)  # load the data every time for new epoch\n    \n    for batch in tqdm(train_loader): # it's just a fancy thing for visualization\n        images,labels = batch # fet images and corresponding labels\n\n        pred = network(images) # feed the images \n        loss = F.cross_entropy(pred,labels,reduction='sum') # calculate loss on the whole batch\n        # or you can do \n        # F.cross_entropy(pred,labels)* len(images)\n\n        optimizer.zero_grad() # make the previous epoch's gradients as 0 as these can be accumulated\n        # network.zero_grad() # we can use this too\n        loss.backward() # back propogate\n        optimizer.step() # start and update weights for each layer\n\n        total_loss+= loss.item() # calculate total loss\n        total_accuracy+= get_correct_pred(pred,labels) \n    \n    acc = (total_accuracy\/len(train_set))\n    avg_loss = (total_loss\/len(train_set)) # average loss and accuracy after current epoch\n    \n    print(f\"End of Epoch: {epoch+1}, Training Accuracy: {acc}, Average Training Loss: {avg_loss}\")\n    \n    \n    tb.add_scalar('AVG Loss',avg_loss,epoch) # automatically make the epoch vs loss graph using tensorboard\n    tb.add_scalar('Accuracy',acc,epoch)\n    tb.add_scalar('Total Correct',total_accuracy,epoch)\n    \n    # tb.add_histogram('conv_1 Bias',network.conv_1.bias,epoch)\n    # tb.add_histogram('conv_1 Weights',network.conv_1.weight,epoch)\n    # tb.add_histogram('conv_1 Gradients',network.conv_1.weight.grad,epoch)\n    \n    for layer_name,weight in network.named_parameters(): # it is the automated version of manual commented lines above\n        tb.add_histogram(layer_name,weight,epoch) # histogram of weights of each layer per epoch\n        tb.add_histogram(f'{layer_name}.grad',weight.grad,epoch)\n    \ntb.close()","70314f5b":"# @torch.no_grad() # if decorator is used here, we don't need with torch.no_grad() later\ndef get_all_pred_from_loader(model,loader):\n    all_pred = torch.Tensor([])\n    for batch in loader:\n        images,labels = batch\n        pred = model(images)\n        \n        all_pred = torch.cat((all_pred,pred),dim=0)\n    return all_pred","0178f29f":"with torch.no_grad(): # we do not want to compute gradients\n    loader = torch.utils.data.DataLoader(train_set,batch_size=128)\n    preds = get_all_pred_from_loader(network,loader)\n    \ntotal_correct = get_correct_pred(preds,train_set.targets)\naccu = total_correct\/len(train_set)\nprint(acc)","b6761fcd":"stacked = torch.stack((train_set.targets,preds.argmax(dim=1)),dim=1)\nprint(stacked.shape)\nstacked[:5] # original, predicted","0bc56ca4":"concat = torch.cat((train_set.targets,preds.argmax(dim=1)),dim=-1) # or use dim=0 . dim=1 gives error\nprint(concat.shape)\nconcat[:5] # original, predicted","fca51a0d":"conf_mat = torch.zeros((10,10),dtype=torch.int32)\nfor row_val in stacked:\n    true,pred = row_val.tolist() # unpack row value\n    conf_mat[true,pred] = conf_mat[true,pred]+1 # whatever value was there, +1. Because we have 0-9 label\n\nconf_mat","53c0ef6a":"conf_mat2 = confusion_matrix(train_set.targets,preds.argmax(dim=1)) # sklearn\nconf_mat2","8fedb0cc":"# change conf_mat tensor to numpy or directly pass conf_mat2\nconf_df = pd.DataFrame(conf_mat.numpy(),columns=train_set.classes,index=train_set.classes)\n\nf,ax = plt.subplots(1,1,figsize=(8,5))\nsns.heatmap(conf_df,annot=True,lw=0.8,cmap='Pastel1',ax=ax,\n            fmt='d', # fmt='d'\/ fmt='g' for suppressing scientific notation\n            annot_kws={\"size\": 10})\n\nax.set_xlabel('Predicted Labels',size='x-large')\nax.set_ylabel('Actual Labels',size='x-large')","7fb8d155":"torch.set_grad_enabled(True) # this is True by default\n\nbatch_size_list = [32,512]\nlr_list = [0.01,0.001]\n\n\nall_list = [batch_size_list,lr_list] \npermutations = list(itertools.product(*all_list))\n# here are (2*2) combinations to try for model for 2 epochs each\n\nfor BATCH_SIZE,lr in tqdm(permutations):\n    \n    network = Network()\n    optimizer = optim.Adam(network.parameters(),lr=lr)\n\n    comment = f\"Hyper Parameters: BATCH={BATCH_SIZE}, lr={lr}\"\n    tb = SummaryWriter(comment=comment)\n    \n    for epoch in range(2):\n\n        total_loss = 0\n        total_accuracy = 0\n        \n        train_loader = torch.utils.data.DataLoader(train_set,batch_size=BATCH_SIZE) \n\n        for batch in train_loader: \n            images,labels = batch\n\n            pred = network(images)\n            loss = F.cross_entropy(pred,labels,reduction='sum')\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss+= loss.item()\n            total_accuracy+= get_correct_pred(pred,labels)\n\n        acc = (total_accuracy\/len(train_set))\n        avg_loss = (total_loss\/len(train_set))\n\n        tb.add_scalar('AVG Loss',avg_loss,epoch)\n        tb.add_scalar('Accuracy',acc,epoch)\n        tb.add_scalar('Total Correct',total_accuracy,epoch)\n\n        for layer_name,weight in network.named_parameters():\n            tb.add_histogram(layer_name,weight,epoch)\n            tb.add_histogram(f'{layer_name}.grad',weight.grad,epoch)\n\ntb.close()","4d427459":"my_x = [np.array([[1.0,2],[3,4]]),np.array([[5.,6],[7,8]])] # a list of numpy arrays\nprint(my_x,'\\n')\nmy_y = [np.array([4.]), np.array([2.])] # another list of numpy arrays (targets or labels for corresponding X)\nprint(my_y,'\\n')\n\ntensor_x = torch.Tensor(my_x) # transform to torch tensor\nprint(tensor_x)\ntensor_y = torch.Tensor(my_y)\n\nmy_dataset = torch.utils.data.TensorDataset(tensor_x,tensor_y) # create your datset\nmy_dataloader = torch.utils.data.DataLoader(my_dataset) # create your dataloader","331122a2":"def load_dataset(data_path):\n    # data_path = 'data\/train\/' # our your own directory\n    train_dataset = torchvision.datasets.ImageFolder(root=data_path,transform=torchvision.transforms.ToTensor())\n    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=64,num_workers=0,shuffle=True)\n    return train_loader\n\n# use the above method like\n\n# for batch_idx, (data, target) in enumerate(load_dataset()):\n    #train network","600d03fb":"# train_dataset=datasets.ImageFolder(root=\".\/root\/\",transform=train_transforms)","7df95abe":"x = np.zeros((3,10,20))\nprint('Before: ',x.shape)\n\n# %timeit np.moveaxis(x,[0,1,2],[2,0,1]) \nx1 = np.moveaxis(x,[0,1,2],[2,0,1]) # use single digit for a single axis rotation\nprint('Method 1:', x1.shape)\n\n# %timeit np.einsum('ijk->kij',x)\nx2 = np.einsum('ijk->jki',x)\nprint('Method 2: ',x2.shape)\n\nx3 = np.rollaxis(x, 0, 3) # roll any of the two axis or in simple terms, interchange\nprint('Method 3: ',x3.shape)\n\nx4 = np.transpose(x,axes=(1,2,0))\nprint('Method 4: ',x4.shape)\n\nx5 = np.reshape(x,(10,20,3)) # because original shape is 3,10,20\nprint('Method 5: ',x5.shape)","7c4e0b9c":"fig=plt.figure(figsize=(15,5))\nplt.imshow(plt.imread('\/kaggle\/input\/cnn-images\/stack_concat.png'))\n# courtesy of Deeplizard","1abbd7d6":"x = np.zeros((3,10,20))\nprint('Original: ',x.shape)\nprint('Case 1: ',np.reshape(x,(1,3,1,1,10,20,1,1,1)).shape)\nprint('Case 2: ',np.reshape(x,(1,25,1,12,1,2,1)).shape)","54494050":"for i in range(len(x.shape)+1):\n    print('Add new axis at:',i)\n    print('New Shape is: ',np.expand_dims(x,axis=i).shape,'\\n')","7cffd140":"t1,t2,t3 = np.array([1,1,1]),np.array([2,2,2]),np.array([3,3,3])","bdb643f6":"x_simple_cat_dim0 = np.concatenate((t1,t2,t3),)\nx_simple_cat_dim0\n\n\n# Below code will produce error as there is no axis=1\n# x_simple_cat_dim1 = np.concatenate((t1,t2,t3),axis=1)\n# print(x_simple_cat_dim1)","747fe370":"np.concatenate((np.expand_dims(t1,0),\n                np.expand_dims(t2,0),\n                np.expand_dims(t3,0)),\n               axis=0)","4540b5f8":"np.concatenate((np.expand_dims(t1,0),\n                np.expand_dims(t2,0),\n                np.expand_dims(t3,0)),\n               axis=1)","840f2245":"np.concatenate((np.expand_dims(t1,1),\n                np.expand_dims(t2,1),\n                np.expand_dims(t3,1)),\n               axis=0)","787ea569":"np.concatenate((np.expand_dims(t1,1),\n                np.expand_dims(t2,1),\n                np.expand_dims(t3,1)),\n               axis=1)","22281dff":"x_simple_stack_dim0 = np.stack((t1,t2,t3),axis=0)\nx_simple_stack_dim0","65fa9d70":"x_simple_stack_dim1 = np.stack((t1,t2,t3),axis=1)\nx_simple_stack_dim1","ad6d296b":"np.stack((np.concatenate((np.expand_dims(t1,1),np.expand_dims(t2,1),np.expand_dims(t3,1)),axis=1),\n                np.stack((t1,t2,t3))),axis=0)","78a4e3be":"np.stack((np.concatenate((np.expand_dims(t1,1),np.expand_dims(t2,1),np.expand_dims(t3,1)),axis=1),\n                np.stack((t1,t2,t3))),axis=1)","6cbdc787":"np.concatenate((np.concatenate((np.expand_dims(t1,1),np.expand_dims(t2,1),np.expand_dims(t3,1)),axis=1),\n                np.stack((t1,t2,t3))),axis=0)","2490b37b":"np.concatenate((np.concatenate((np.expand_dims(t1,1),np.expand_dims(t2,1),np.expand_dims(t3,1)),axis=1),\n                np.stack((t1,t2,t3))),axis=1)","8dbabf4f":"x = np.zeros((3,3))\nx","fabac3aa":"x[0] = 3\nx","5a6b608b":"x[0,1:] = 1\nx","b59078a0":"x[1:,1:] = [4.4,5.5] \nx","75bb192b":"### You can try to use concat but it won't make columns. Concat works like `list.extend()`","dec4d973":"## Adding an extra dimension as a batch inside a matrix in numpy\nYou can use `np.reshape()` and adding extra one dimension anywhere won't effect because number of elements will always be same","b0b308d7":"# CNN Architecture","f685b464":"### Get class label result for the image","f5f6eb79":"### Difference\nTo see the difference, try to modify values of `data` now","9a5192c8":"### There are basically 4 ways of creating tensors from numpy array or data\n1. Tensor Class\n2. tensor method inside torch module\n3. from_numpy method\n4. as_tensor method","3e1227f9":"### Concat -> {concat(expand_dims),stack}","df1d34cb":"## Plot confusion matrix using Seaborn Heatmap","bf15c5ff":"Let me describe the architecture and working of the Network.\n\nEvery class uses the `nn.Module` to get the functionality of that class. Those who do not know how this works, it is as simple as thinking that you have a very influential family name and you want to make your own place in the society too. So what you do is that you can use you family name `nn.Module` to get all the privilages in your life `Network`. Best thing about this? Suppose there is privilage that your family members can fly 10000 Kms per year for free but you have a special privilage that you have earned to fly 50000 Kms. SO? Don't worry!! You are a self made person now ;)\n\nThis is the exact working of `super(Network,self).__init__()`. It says that, Hey!! I am going to use my family name AND if necessary I can replace the family privilage and\/or add extra things to it so that if my next generation uses my name, they can have all the benifits that I have.\n\nInternal structure of `nn.Module` contains a dynamic graph. YES!! You heard me, A Dynamic graph and this is the best thing about PyTorch that you can test and run every line instead of compiling in Keras or Tensorflow. Graph is generated dynamically and there are weights, gradients and biases with every layer.","b2305d88":"#### Concatenate","899e1b38":"## Getting Predictions after training\nuse `torch.no_grad()` because when we predict, we do not need model to compute gradients. Calculating always causes overheads to system","93dc9732":"# Numpy Extra\nPlease try all the examples by yourself to get the true deeper understanding of working of methods","b4bc70d5":"### Step-5: Make a BackPass or Start Back Propagation to update weights","f2aecbdc":"### Step-3: Pass image to the network object and get predictions\nThese are not probabilities but the final tensor values","5c11e7f7":"### Compare results\nResults do not make sense here. Results are random due to the fact that weights are tuned after training properly","eb724828":"### Layers structure and working\nLayers work in a stacked way like a pipeline. Output of the previous layer is fed to the coming layer and so on. You can see the weights, biases and gradients of a specific layer by using `network.layer_name.weight.grad`.\n\n**PLEASE VISIT [THIS LINK](https:\/\/www.kaggle.com\/deshwalmahesh\/bengali-ai-complete-beginner-tutorial-95-acc\/edit\/run\/26801239)**\nto know the working of each and every layer and activation function used here before proceeding. \n**IT IS A MUST FOR BEGINNERS**\n\n### Network Structure\nIn first part, we define the stack of layers with each layer's input, output, number of neurons, kernal size etc in the constructor. A constructor is called automatically and it means that if a `Network()` will be called, ever, we'll be having atleast these defined layers by default. If nothing has been done with the object, it'll atleast have these layers\n\nSecond part is the `forward()` method. What it does is organizes the whole training operation of the network. How the layers are attaches, whose input will act as whose output, which activation function to use etc etc\n","548208b1":"## Build Confusion Matrix\nYou can make yout own confusion matrix or you can use `sklearn`","d13fff69":"Use these in order `opt.zero_grad()`, `loss.backward()`, `opt.step()`\n\n1. `zero_grad` clears old gradients from the last step (otherwise you\u2019d just accumulate the gradients from all `loss.backward()` calls).\n\n2. `loss.backward()` computes the derivative of the loss w.r.t. the parameters (or anything requiring gradients) using backpropagation.\n\n3. `opt.step()` causes the optimizer to take a step based on the gradients of the parameters.","99a02b64":"## 3. Keras type - Different classes in different folders\n[Link to demo and documentation](https:\/\/pytorch.org\/docs\/master\/torchvision\/datasets.html#imagefolder)","f396f15c":"# Complete Model Training","19d0a375":"# Data Loading Techniques\nThere are different scenarios when you have to load your images. Your images can be stored within an array in forms of pizel matrices, there can be a `DataFrame` which contains the image name and label column, there can be images inside a nested directory where each type of images are in a seperate directory and so on. I'll try to shed light on some.","5a3f847c":"### Plot Random Images\nWe will use `make_grid` to make a grid and then plot images inside the batches uaing `matplotlib`","b1ac6b95":"### Step-2: Reshape the image for compatibility","e41ae6bd":"# END Of PyTorch Tutorial \nbut\n\n**\"Every Finish Line is the Beginning of a New Race\"- Lil Wayne**\n\nBelow are some demonstrations of similarities of `numpy` and `PyTorch` workings. You'll be seeing the deep working of `stack`, `concatenate` operations and much more.","25458867":"#### Stack","8d9afdf9":"### Reshaping, stacking, Flattening\nWe have to perform these operations on a daily basis and are very general for AI and Deep learning.","d1975487":"To get the idea, it is like copying a scaler to same dimension and then checking every element of one tensor to every element of second on the same index. It means that the scaler is converted to a matrix\/tensor which has dimensions equal to the given matrix\/tensor and is full of that scaler value. \nTo have another deep knowledge, look at the output given below","2e69d9db":"### Step-4: Apply Softmax to get probabilities","31717718":"### Step-1: Load Batch","2e1c8968":"## 2. Loading images from Directory","c5d2ae78":"## Everything about CNN\nBefore starting any problem, always have an vision of how your network will look like. I'm asking you to do it with perfection in a single shot but a rough draft that how many layrs will be arranged in which way. So below is the details of layers copied from my another tutorial which you can [check here](https:\/\/www.kaggle.com\/deshwalmahesh\/bengali-ai-complete-beginner-tutorial-95-acc).\n\n## Convolution Neural Network\nIn CNN, we use Convolution layers which extract the feature maps from the input image after applying a kernel. A kernel is a (NxN) function which slides  over the whole image to get a feature map. This feature map can be considered as the extracted features from the image such as border, lines, edges etc. With each convolution layer, some new features are detected from the image. An activation is always attached to the layer which keeps the output value in the range `[0,1]` both included generally and `[-1,1]` in some cases. A value of 0 means that the neuron is inactive for the decision making and 1 means it is active and impact is decided by the weight.\n\nA pooling layer is added often times to reduce the dimension of the feature maps else the whole purpose of using the CNN due to dimensionality explosion will be futile. Sometimes a couple of Convolution layers are used in continuation before using the pooling layers. A pooling such as Average Pooling of (2x2) gets the average of every pixel which come in a 2 by 2 grid for each row and column and save it in a new matrix. Just like Average, a Max Pooling takes the maximum values of those pixels because it is thought that the pixel with the highest value around an area can define the whole area. Working of CNN can be seen by Figure.\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*uAeANQIOQPqWZnnuH-VEyw.jpeg\" width=\"580px\">\n\nThe result is output of a softmax activation function which is the probability of the data belonging to each class. Our goal of this process is to get the loss as minimum as possible where loss id defined in the terms of categorical cross entropy which is in simple terms, a number which tells us that how far is the probability of the expected class from the actual class. Neuron with the maximum probability is the class to which the data point or the test image in our case belongs to. We can see the whole process in the single figure given below. A Flatten layer is applied after the last Pooling and the first dense layer to get the data from 3 Dimensional to 1 Dimension for the Dense layer and then to last output layer.","2841b58d":"### Step-4: Calculating Loss and Gradients","909224f2":"## Tensorboard\n`Tensorboard` is magic. You can see the real time graphs of training for every attribute related to training and then analyse.\n\nuse `tensorboard --logdir=runs` using shell in the same diectory and it'll make a history for every run and save every graph ever created.","559e6ab6":"# Hyper Parameter tuning with Tensorboard and loops\nWorks like Grid Search\n**Note: You should never use this approach in real life as this is very time consuming. This is for demonstration of usefullness of  `tensorboard` **","b4852e18":"#### Concatenate + expand_dims\n`expand_dims()` adds an extra dimension just like the **opposite** of `squeeze()`","5bea55f6":"# Import the libraries\nIf an error comes to you, it is a good thing  because then you'll be eager to know and while getting the answers for exact problems, you'll get a whole lot of other knowledge you didn't even know existed. ","d5b4c7ab":"# Tensors! Tensors! Tensors!","7b407699":"### Step-6:  Updating Weights using Gradients","6c54240a":"# Inserting  and changing values in numpy arrays","4d592a67":"### Stack -> {concat(expand_dims),stack}","11e53155":"###  Operations on Tensors are done IFF they are on same device only and IFF they have same dtype \n(version > 1.3.0 can perform operations on float and int)","c0b611cc":"# Single Batch Training Demo","708f7e60":"### SEE the DIFFERENCE? \nFirst 2 use `COPY` of data. Last 2 `SHARE` the same instances. So based on whether you want to perform an operation on the same tensors (suitable for situation when you are running low on memory and do not need the original tensor later) or you can have a copy of tensors (cases where you want the original data too and memory is not a constraint)","b32243ac":"# Single Image Training Demo\n### Step-1: Load Image\nInput should be always `(BATCH,CHANNELS,HEIGHT,WIDTH)` for Convolution Networks in PyTorch","a0414891":"## Explore Data\nEach image is formed by `(1,28,28)` pixels which means that in order to define a single grayscale image, we have to use (28*28) pixels. Our data in shown in `(Channels,Width,Height)` format\n\nOur train set use the batch so each batch has an extra dimension","d3fe0c3e":"Suppose we have a scenario where we have 3 individual images with `(C,W,H)` and three seperate images as `(B,C,W,H)`. What do we do now? \n\nAns: Stack the upper three images and concat those with the below 3 to make a batch of 6 images","74dbcfb9":"## Stack vs Concat","67422aee":"### Step-2: Pass the batch to the Network object","4509dd3c":"### Enabling GPU if installed\nYou can change the device to CUDA if you have GPU enabled","3bad86dc":"## Basic Operations \nBefore diving deep into the world of PyTorch, remeber one thing that every operation is performed in the form of tensors. In mathematical terms, Tensors are functions that provide bla..bla..bla.. We, as ML engineers tend to reshape everything (literally) and have reshaped the meaning of this one too. So what are tensors?? Any body??\nYes, You!!\nGreat answer!\nFancy names for Matrices!!!! Tensors are just other names for matrices when it comes to AI and the main difference is that they can be operated on GPU which happens to be 50x to 100x faster than CPU.\n\nPytorch handles the data very closely related to numpy. You can see a lot of methods and functionality that is common between the two. If you run into a problem, run to the Pytorch's community and trust me, those are very helpful people.","5d4f4ce8":"To get the meaning of `stride` here, please check the [documentation for stride](https:\/\/pytorch.org\/docs\/stable\/tensor_attributes.html#torch.torch.layout) or this [stackoverflow link](https:\/\/stackoverflow.com\/questions\/56659255\/what-does-layout-torch-strided-mean)","d0b284ac":"If you try to access `t[0]`, it'll be another tensor and depth is 1. Output of a tensor is another tensor be it slicing or a single value.`t[0][0]` means that we have accessed the full depth but the output is still a tensor itself.","f85250e6":"`network.zero_grad()` and `optimizer.zero_grad()` are the same IFF all your model parameters are in that optimizer just like we have `optimizer = optim.Adam(network.parameters(),lr=0.01)`. \n\nI found it is safer to call ```network.zero_grad()``` to make sure all grads are zero, e.g. if you have two or more optimizers for one model.","b96f8866":"# Thank you!!\nFor being there with me till the end (end of Notebook for the least in this case `*_*` ). Please correct any code or mistakes and please give me suggestions for improvements in comments because I too am a learner like every other human being out here.","5e821674":"##  Torchvision Datasets\nWe will use the dataset given with the `torchVision` module. You need to `pip install torchvision` or the `conda` version of it if not already done. \n\nThose who are not in the field of ML, this is the corniest and most used dataset worldwide in my opinion. So used that it'd ne needing a break from all this find about itself had it been a real thing. 10 types of images,6000 per types making a total of 60000 converted into grayscale and resized to (28,28). By training the model about how each image how it looks like, we want mode lto predict the exact for a new image. This is all we do with  the children, all the time. \"Het it's a dog!! What's that? What's that?? Dog!!\" We are basically training our children or making them cram the shapes. Same thing with CNN.","056e09c6":"# Channel First Channel Last Problem Solution.\nSometimes when we have our data in the form of `(B,C,W,H)`and we have to feed the model with `(B,W,H,C)` or vice versa, then we'll have to change the axis. Numpy has a functionality for it and we can do it by using more than one method. \n\n`%timeit` is a magic function which will run the code for thousands of iterations. Not making it up, % functions are called magic functions. You can see the time taken by each code for execution and select the fastest according to your needs.","31d7d8ac":"# Let's go CNN\n\"Why we need CNN but?\"\n\"You kidding\", my brain replied.\n\"No, but, okay!! well tell me why not Fully or Dense?\",asked the silly me.\n\"Imagine what it likes to have an image?? A matrix of (28x28) which have an order that makes an image IMAGE. DNN can't find that order for startes and lets us just assume EVEN IF THEY DO BY MAGIC, it'll blow the memory.\"\n\nNow here is the basic of Digital Images:\n\n**Every color is made up of Red,Green,Blue (3 channels) and in computers we represent these colors by numbers ranging from (0-255) where 0 represents White and 255 represents Black. Each color has it's own values in case of 3 channel images but in grayscale, we just have one channel so we get B&W pictures. To get a better understanding of how does all of this makes sense, just [visit this link](https:\/\/www.youtube.com\/watch?v=bwb4r3UVKko)**","a64196e6":"### Step-7: Predict ","a3d1d97c":"## 1. Load data from Numpy Array or Pandas DataFrame","b52864b2":"### Manual Confusion matrix","7e450e7c":"This what we usually aim for. This is a batch of 2 made by stacking 2 grayscale images","3749f7e6":"## Hey!!! This just *Looks* like a BIIGGGG Notebook because it contains detailed version of everything of what can be done using a simple pre-built code so don't get scared and try *NOT* to finish this notebook in one day.\n\nHi Fellow Beginners. If you have come to this post, I hope you learn from it. I have tried my best to teach you the basics of `Pytorch`. People use `Keras` specially beginners including me but `Keras` does not provide the feel of Neural Networks. Creators of `Pytorch` wanted it to be dynamic like `Python` and as close as possible to the Neural Networks. This is a tutorial about `Pytorch`  basics and CNN and Image Recognition. If you want to learn more about Deep Learning and Keras, please check out [my dedicated notebook](https:\/\/www.kaggle.com\/deshwalmahesh\/bengali-ai-complete-beginner-tutorial-95-acc) on the art of CNN, Image Processing, Theory of AI and find some very very useful and interesting links there. \n\n## NOTE:\n**This notebook is meant to run on local system so download it to see the full benifits of it and Please write and practice every code and line by line using your head and hand and try to implement it. You can make a new cell below the original code and WRITE it yourself. Whatever happens, DO NO COPY AND PASTE**\n\n## Super important Notes of COMPLETE Beginners\nWe'll build a Neural Network and more specifically a Convolution Network. NN, CNN works on finding and memorizing the patterns like we humans do but subconciously. We do not have to apply a lot of load because we have memorized and learned to memorize the patterns from billions of years of evolution and we are trying to teach this this to computers within what, a hundred years or less. Not Fair! And I can not give you a whole lot of Deep Learning in this mere notebook but before we start into the solution, I highly rcommend and I repeat, I HIGHLY RECOMMEND to watch these [Neural Network series](https:\/\/www.youtube.com\/channel\/UCYO_jab_esuFRV4b17AJtAw\/playlists?view=50&sort=dd&shelf_id=20) and [Fundamentals of Deep Learning and Neural Networks](https:\/\/www.youtube.com\/playlist?list=PLZbbT5o_s2xq7LwI2y8_QtvuXZedL6tQU) where mazing people giving you *INSIGHTS* of what happens *ACTUALLY INSIDE* the mathemetics of something , how things work, what are *PRACTICAL SIGNIFICANCE* of formulas we use and all the things. I had a sudden revealation from these such as **Neural Networks JUST find the patterns and we have high hopes that they'll find and to our surprise, they even just do. NN doesn't even know what they havefound.**  And some amazing visual explanations of **Working of Gradients**, **Significance of Eignveectors and EignValues and why and how are they used, How Convolution , Pooling ACTUALLY works** and so on. Few of the videos on Keras are [available here](https:\/\/www.youtube.com\/playlist?list=PLZbbT5o_s2xrwRnXk_yCPtnqqo4_u2YGL). I am just trying to give you the best of I have after searching through tons of tutorials and blogs.","012eb9e0":"This Confusion Matrix says that out of all the 6000 images of T-Shirt\/Top, \n1. 5273 were correctly predicted\n2. 18 were predicted as Trousers, 396 as Shirts and none were predicted as Sneakers or Ankle Boots","f1c86377":"### Tensor broadcasting\nThis one is very important because a lot of peole do not know what is happening behind the scenes when you multiply or divide a scaler with a tensor. Remember one thing that\n**Tensor works only and only on  `Element vise` operations**","e18fe0f1":"### Step-3: Get prediction `tensor`\nWe still do not have the probabilites bt we know that highest value will produce the class label after using softmax so using it directly to find the index where value is maximum as the index is the label for the image.","a38740e0":"### Confusion Matrix using `sklearn`","fe0fc64d":"## Stack and Concatenate\nNumpy has `np.stack()` and `np.concatenate()` functionality just like PyTorch so we can use these too"}}