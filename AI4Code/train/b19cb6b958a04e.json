{"cell_type":{"07717985":"code","ffa1d6f9":"code","3ef1bec7":"code","554a5057":"code","8ed7a035":"code","86e6d31d":"code","ef035254":"code","550cab75":"code","8cbc2187":"code","4d992fc8":"code","d79010e2":"code","7ea7858b":"code","00f9caec":"code","b35ee344":"code","45c869bb":"code","4648426c":"code","76db80e3":"code","b1c76e93":"code","9c8f6ec6":"code","15feb166":"code","c51c2c16":"code","fda3f237":"code","1d8f57a5":"code","d6fe9259":"code","9ba79664":"code","5884a753":"code","335526cd":"code","31638066":"code","91c27f0c":"code","620b813f":"code","d04ac726":"code","f090c0de":"code","fbc7b1c5":"code","1ad4f452":"code","9ad7859d":"code","f77f14b8":"code","35d88624":"code","2bbaac7e":"code","694d40bf":"code","d6b99923":"code","1b39b105":"code","774e877f":"code","98a51f11":"code","cc4011a9":"code","f6ee8505":"code","ec47b6ee":"code","fcaf41dd":"code","e06cedb1":"code","290fc8cf":"code","f3317373":"code","2fc811dd":"code","58239444":"code","8f1d9a66":"code","c515b4d2":"code","f7b1d262":"code","4fb66b44":"code","e9a84995":"code","a3c09944":"code","51e72482":"code","f5f11680":"code","ffb8640d":"code","ed00b1be":"code","be47cbe6":"code","3df64cdb":"code","361837ac":"code","73893257":"code","807bbbbf":"code","979f7e23":"code","983ad489":"code","a719b72b":"code","493460b0":"code","fd36e052":"code","04f896b1":"code","739dd684":"code","6a8dc745":"code","db03bea4":"code","3f942f3e":"code","84392063":"code","125248a8":"code","4b7e9216":"code","80f5c980":"code","937ae03f":"code","648699f8":"code","6ab05342":"code","7b6246d9":"code","909607ae":"code","8c4eb552":"code","c4708ddc":"code","2b65139a":"code","b9313f8c":"code","ff3d28fc":"code","bc4fa26f":"code","6fde7397":"code","739408bd":"code","35df12c9":"code","77a2841c":"code","b6054a50":"code","9d31d174":"code","9efd063d":"code","740d5581":"code","678a7386":"code","71672840":"code","f26b2561":"code","a9ac105c":"code","6ce3d2b5":"code","762647a9":"code","fb41cb18":"code","6ae3f191":"code","f29c4931":"code","2034a99f":"code","2a3c31e5":"code","2741d49f":"code","2f26e96d":"code","1865c4f6":"code","47aedc5d":"code","6732829c":"code","8707eaf5":"code","3f022bb8":"code","13d561f4":"code","34b8b5ee":"code","5ab739a8":"markdown","e58d7a3a":"markdown","cb5eb2e6":"markdown","cfb5af25":"markdown","5d754610":"markdown","b5e6cfae":"markdown","c1050074":"markdown","9c1c1b16":"markdown","2b9b786b":"markdown","183708d9":"markdown","f649154c":"markdown","7776517f":"markdown","7d4b0e09":"markdown","51abd114":"markdown","105de924":"markdown","b8ad8822":"markdown","2539f667":"markdown","643fe76a":"markdown","aea51226":"markdown","4bf73c01":"markdown","0495e101":"markdown","ac1e4344":"markdown","98ffa441":"markdown","04054ca5":"markdown","1e568576":"markdown","0ea84cdd":"markdown","3997d0f2":"markdown","39814ce8":"markdown","f2485555":"markdown","05f5bb9b":"markdown","cbbaddf3":"markdown","d909cd93":"markdown","b0a076da":"markdown","bb22cd7d":"markdown","d821a04e":"markdown","556b82f9":"markdown","e450b40e":"markdown","a9e5ecd8":"markdown"},"source":{"07717985":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport joblib\nimport xgboost\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom math import sqrt\nfrom numpy import concatenate\n\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping","ffa1d6f9":"import os\nprint()","3ef1bec7":"df1 = pd.read_csv('..\/input\/demographics.csv')\ndf1['Year'] = '2017'\nprint(len(df1))\ndf1.head(2)","554a5057":"df2 = pd.read_csv('..\/input\/event_calendar.csv')\nprint(len(df2))\ndf2['YearMonth'] = df2['YearMonth'] .astype(str)\ndf2['YearMonth'] = df2['YearMonth'].apply(lambda x: x[0:4]+'-'+x[4:6])\ndf2['YearMonth']  = pd.to_datetime(df2['YearMonth'])\ndf2.head(2)","8ed7a035":"df3 = pd.read_csv('..\/input\/historical_volume.csv')\nprint(len(df3))\ndf3['YearMonth'] = df3['YearMonth'] .astype(str)\ndf3['YearMonth'] = df3['YearMonth'].apply(lambda x: x[0:4]+'-'+x[4:6])\ndf3['YearMonth']  = pd.to_datetime(df3['YearMonth'])\ndf3 = df3.sort_values('YearMonth', ascending=True).reset_index(drop=True)\ndf3.head(2)","86e6d31d":"df4 = pd.read_csv('..\/input\/industry_soda_sales.csv')\nprint(len(df4))\ndf4['YearMonth'] = df4['YearMonth'] .astype(str)\ndf4['YearMonth'] = df4['YearMonth'].apply(lambda x: x[0:4]+'-'+x[4:6])\ndf4['YearMonth']  = pd.to_datetime(df4['YearMonth'])\ndf4.head(2)","ef035254":"df5 = pd.read_csv('..\/input\/industry_volume.csv')\nprint(len(df5))\ndf5['YearMonth'] = df5['YearMonth'] .astype(str)\ndf5['YearMonth'] = df5['YearMonth'].apply(lambda x: x[0:4]+'-'+x[4:6])\ndf5['YearMonth']  = pd.to_datetime(df5['YearMonth'])\ndf5.head(2)","550cab75":"df6 = pd.read_csv('..\/input\/price_sales_promotion.csv')\nprint(len(df6))\ndf6['YearMonth'] = df6['YearMonth'] .astype(str)\ndf6['YearMonth'] = df6['YearMonth'].apply(lambda x: x[0:4]+'-'+x[4:6])\ndf6['YearMonth']  = pd.to_datetime(df6['YearMonth'])\ndf6 = df6.sort_values('YearMonth', ascending=True).reset_index(drop=True)\ndf6.head(2)","8cbc2187":"df7 = pd.read_csv('..\/input\/weather.csv')\nprint(len(df7))\ndf7['YearMonth'] = df7['YearMonth'] .astype(str)\ndf7['YearMonth'] = df7['YearMonth'].apply(lambda x: x[0:4]+'-'+x[4:6])\ndf7['YearMonth']  = pd.to_datetime(df7['YearMonth'])\ndf7.head(2)","4d992fc8":"dfa = df4.merge(df5, on='YearMonth', how='inner')\ndfa = dfa.merge(df2, on='YearMonth', how='inner')\nprint(len(dfa))\ndfa.head(2)","d79010e2":"df = df3.merge(df6, on=['Agency','SKU','YearMonth'], how='inner')\ndf = df.merge(df7, on=['Agency','YearMonth'], how='inner')\n\ndf['Year'] = df['YearMonth'].dt.year\ndf['Year'] = df['Year'].astype(str)\n\nprint(len(df))\ndf = df.merge(df1, on=['Agency','Year'], how ='left')\n\nprint(len(df))\ndf['SKU'] = df['SKU'].apply(lambda x: x[4:])\ndf['SKU'] = df['SKU'].astype(int)\n\ndf['Agency'] = df['Agency'].apply(lambda x: x[7:])\ndf['Agency'] = df['Agency'].astype(int)\n\ndf = df.merge(dfa, on='YearMonth', how='left')\ndf = df.drop('Year', axis=1)\nprint(len(df))\n\ndf.to_csv('..\/input\/train.csv',index=False)\ndf.head(2)","7ea7858b":"# df7['Agency'] = df7['Agency'].apply(lambda x: x[7:])\n# df7['Agency'] = df7['Agency'].astype(int)\nprint(len(df7))\ndf7.head(3)","00f9caec":"df7 = df7.groupby('Agency')['Avg_Max_Temp'].agg(['mean','median']).reset_index()\nprint(len(df7))\ndf7.head(3)","b35ee344":"agen6 = df7[(df7['mean']>=29.007939)&(df7['mean'] <29.007940)].reset_index(drop=True)\nagen6","45c869bb":"agen14 = df7[(df7['mean']>= 25.085280)&(df7['mean'] <25.085282)].reset_index(drop=True)\nagen14","4648426c":"df1 = df1.drop('Year',axis=1)\ndf1 = df1.sort_values('Agency',ascending=True).reset_index(drop=True)\ndf1.head(2)","76db80e3":"agen_614 = df1[(df1['Agency']=='Agency_06')|(df1['Agency']=='Agency_14')]\nagen_614","b1c76e93":"df1_temp = df1[(df1['Avg_Population_2017'] >= 1800000)]\ndf1_temp = df1_temp[(df1_temp['Avg_Population_2017'] < 2500000)]\ndf1_temp.head(2)","9c8f6ec6":"df1_temp = df1_temp[df1_temp['Avg_Yearly_Household_Income_2017']> 185000]\ndf1_temp = df1_temp[df1_temp['Agency'] != 'Agency_06']\ndf1_temp = df1_temp[df1_temp['Agency'] != 'Agency_14'].reset_index(drop=True)\ndf1_temp.head(2)","15feb166":"fig, ax = plt.subplots(figsize=(15,10), sharex=True, sharey=True)\ng = sns.scatterplot(x='Avg_Population_2017',y='Avg_Yearly_Household_Income_2017', hue='Agency', data=df1, ax=ax)\ng = sns.scatterplot(x='Avg_Population_2017',y='Avg_Yearly_Household_Income_2017', marker='X', s=400 , data=agen_614, ax=ax)\ng = sns.scatterplot(x='Avg_Population_2017',y='Avg_Yearly_Household_Income_2017', marker='X', s=150 , data=df1_temp, ax=ax)\n\n\nax.legend( df1['Agency'].values, loc='upper lef', ncol=2, borderaxespad=0,frameon=False, bbox_to_anchor= (1.01, 1.0))\nplt.savefig('..\/fig\/sku reco analysis 1.png',bbox_inches='tight')","c51c2c16":"agen6 = df3[df3['Agency'].isin(['Agency_55','Agency_60','Agency_5','Agency_40','Agency_8','Agency_50'])]\nagen6 = agen6.sort_values('Agency', ascending=True).reset_index(drop=True)\nagen6 = agen6.drop('YearMonth',axis=1)\nprint(len(agen6))\nagen6.head()","fda3f237":"agen6 = agen6.groupby(['Agency','SKU'])['Volume'].agg(['mean','median']).reset_index()\nagen6 = agen6.drop('Agency',axis=1)\nagen6.head()","1d8f57a5":"# agen6['SKU'].value_counts()","d6fe9259":"agen6 = agen6.groupby('SKU')['mean','median'].mean().reset_index()\nagen6 = agen6.sort_values('mean', ascending=False).reset_index(drop=True)\nagen6.head(6)","9ba79664":"dagen14 = df3[df3['Agency'].isin(['Agency_57','Agency_56','Agency_12','Agency_13','Agency_15',\n                                    'Agency_16','Agency_17','Agency_20','Agency_38','Agency_39',\n                                    'Agency_58','Agency_59','Agency_60'])]\nprint(len(dagen14))\ndagen14.head()","5884a753":"dagen14 = dagen14.sort_values('Agency', ascending=True).reset_index(drop=True)\ndagen14 = dagen14.drop('YearMonth',axis=1)\ndagen14.head()","335526cd":"dagen14 = dagen14.groupby(['Agency','SKU'])['Volume'].agg(['mean','median']).reset_index()\ndagen14.head()","31638066":"dagen14 = dagen14.drop('Agency',axis=1)\ndagen14.head()","91c27f0c":"# dagen14['SKU'].value_counts()","620b813f":"dagen14 = dagen14.groupby('SKU')['mean','median'].mean().reset_index()\ndagen14 = dagen14.sort_values('mean', ascending=False).reset_index(drop=True)\ndagen14.head(6)","d04ac726":"df = pd.read_csv('..\/input\/train.csv')\ndf['Volume'] = df['Volume'].round(2)\ndf['Price'] = df['Price'].round(2)\ndf['Sales'] = df['Sales'].round(2)\ndf['Promotions'] = df['Promotions'].round(2)\ndf['Avg_Max_Temp'] = df['Avg_Max_Temp'].round(2)\ndf.head()","f090c0de":"df = df[['Agency','SKU','Volume']].copy()\ndf.head()","fbc7b1c5":"df.isnull().sum()","1ad4f452":"X = df.iloc[:,0:2]\nX.head(2)","9ad7859d":"y = df.iloc[:,2:3]\ny.head(2)","f77f14b8":"sc_x = StandardScaler()\n\nX = sc_x.fit_transform(X.astype(float))\ny = y.values","35d88624":"X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)\nprint(Y_test.shape)","2bbaac7e":"kfold = KFold(n_splits=5, shuffle=True, random_state=42)","694d40bf":"arr_gb_val_r2 = []\narr_gb_val_mse = []\n\narr_gb_test_r2 = []\narr_gb_test_mse = []\n\nfor train, test in kfold.split(X_train):\n    clf_gb = GradientBoostingRegressor()\n    clf_gb.fit(X[train],y[train])\n\n    Y_pred_val = clf_gb.predict(X[test])\n    val_mse_gb = mean_squared_error(y[test],Y_pred_val)\n    val_r2_gb = r2_score(y[test], Y_pred_val) \n\n    print('r2 Val : ',val_r2_gb)\n    print('mse Val : ',val_mse_gb)\n    arr_gb_val_r2.append(val_r2_gb)\n    arr_gb_val_mse.append(val_mse_gb)\n    \n    Y_pred_test = clf_gb.predict(X_test)\n    test_mse_gb = mean_squared_error(Y_test,Y_pred_test)\n    test_r2_gb = r2_score(Y_test, Y_pred_test) \n\n    print('r2 Test : ',test_r2_gb)\n    print('mse Val : ',test_mse_gb)\n    arr_gb_test_r2.append(test_r2_gb)\n    arr_gb_test_mse.append(test_mse_gb)\n\n","d6b99923":"print(np.mean(arr_gb_val_r2))\nprint(np.mean(arr_gb_val_mse))\nprint(np.mean(arr_gb_test_r2))\nprint(np.mean(arr_gb_test_mse))","1b39b105":"arr_rf_val_r2 = []\narr_rf_val_mse = []\n\narr_rf_test_r2 = []\narr_rf_test_mse = []\n\nfor train, test in kfold.split(X_train):\n    clf_rf = RandomForestRegressor(n_estimators=500, random_state=0,max_depth=2)\n    clf_rf.fit(X[train],y[train])\n\n    Y_pred_val = clf_rf.predict(X[test])\n    val_mse_rf = mean_squared_error(y[test],Y_pred_val)\n    val_r2_rf = r2_score(y[test], Y_pred_val) \n\n    print('r2 Val : ',val_r2_rf)\n    print('mse Val : ',val_mse_rf)\n    arr_rf_val_r2.append(val_r2_rf)\n    arr_rf_val_mse.append(val_mse_rf)\n    \n    Y_pred_test = clf_rf.predict(X_test)\n    test_mse_rf = mean_squared_error(Y_test,Y_pred_test)\n    test_r2_rf = r2_score(Y_test, Y_pred_test) \n\n    print('r2 Test : ',test_r2_rf)\n    print('mse Val : ',test_mse_rf)\n    arr_rf_test_r2.append(test_r2_rf)\n    arr_rf_test_mse.append(test_mse_rf)\n\n","774e877f":"print(np.mean(arr_rf_val_r2))\nprint(np.mean(arr_rf_val_mse))\nprint(np.mean(arr_rf_test_r2))\nprint(np.mean(arr_rf_test_mse))","98a51f11":"arr_svm_val_r2 = []\narr_svm_val_mse = []\n\narr_svm_test_r2 = []\narr_svm_test_mse = []\n\nfor train, test in kfold.split(X_train):\n    clf_svm = SVR(kernel='rbf', C=1e3, gamma=0.1)\n    clf_svm.fit(X[train],y[train])\n\n    Y_pred_val = clf_svm.predict(X[test])\n    val_mse_svm = mean_squared_error(y[test],Y_pred_val)\n    val_r2_svm = r2_score(y[test], Y_pred_val) \n\n    print('r2 Val : ',val_r2_svm)\n    print('mse Val : ',val_mse_svm)\n    arr_svm_val_r2.append(val_r2_svm)\n    arr_svm_val_mse.append(val_mse_svm)\n    \n    Y_pred_test = clf_svm.predict(X_test)\n    test_mse_svm = mean_squared_error(Y_test,Y_pred_test)\n    test_r2_svm = r2_score(Y_test, Y_pred_test) \n\n    print('r2 Test : ',test_r2_svm)\n    print('mse Val : ',test_mse_svm)\n    arr_svm_test_r2.append(test_r2_svm)\n    arr_svm_test_mse.append(test_mse_svm)\n\n","cc4011a9":"print(np.mean(arr_svm_val_r2))\nprint(np.mean(arr_svm_val_mse))\nprint(np.mean(arr_svm_test_r2))\nprint(np.mean(arr_svm_test_mse))","f6ee8505":"arr_knn_val_r2 = []\narr_knn_val_mse = []\n\narr_knn_test_r2 = []\narr_knn_test_mse = []\n\nfor train, test in kfold.split(X_train):\n    clf_knn = KNeighborsRegressor(n_neighbors=2)\n    clf_knn.fit(X[train],y[train])\n\n    Y_pred_val = clf_knn.predict(X[test])\n    val_mse_knn = mean_squared_error(y[test],Y_pred_val)\n    val_r2_knn = r2_score(y[test], Y_pred_val) \n\n    print('r2 Val : ',val_r2_knn)\n    print('mse Val : ',val_mse_knn)\n    arr_knn_val_r2.append(val_r2_knn)\n    arr_knn_val_mse.append(val_mse_knn)\n    \n    Y_pred_test = clf_knn.predict(X_test)\n    test_mse_knn = mean_squared_error(Y_test,Y_pred_test)\n    test_r2_knn = r2_score(Y_test, Y_pred_test) \n\n    print('r2 Test : ',test_r2_knn)\n    print('mse Val : ',test_mse_knn)\n    arr_knn_test_r2.append(test_r2_knn)\n    arr_knn_test_mse.append(test_mse_knn)\n\n","ec47b6ee":"print(np.mean(arr_knn_val_r2))\nprint(np.mean(arr_knn_val_mse))\nprint(np.mean(arr_knn_test_r2))\nprint(np.mean(arr_knn_test_mse))","fcaf41dd":"arr_xgb_val_r2 = []\narr_xgb_val_mse = []\n\narr_xgb_test_r2 = []\narr_xgb_test_mse = []\n\ni = 1\nfor train, test in kfold.split(X_train):\n    clf_xgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,colsample_bytree=1, max_depth=7)\n    clf_xgb.fit(X[train],y[train])\n    #save model\n    joblib.dump(clf_xgb, '..\/model\/xgb_'+str(i)+'.dat') \n    i = i+1\n    \n    Y_pred_val = clf_xgb.predict(X[test])\n    val_mse_xgb = mean_squared_error(y[test],Y_pred_val)\n    val_r2_xgb = r2_score(y[test], Y_pred_val) \n\n    print('r2 Val : ',val_r2_xgb)\n    print('mse Val : ',val_mse_xgb)\n    arr_xgb_val_r2.append(val_r2_xgb)\n    arr_xgb_val_mse.append(val_mse_xgb)\n    \n    Y_pred_test = clf_xgb.predict(X_test)\n    test_mse_xgb = mean_squared_error(Y_test,Y_pred_test)\n    test_r2_xgb = r2_score(Y_test, Y_pred_test) \n\n    print('r2 Test : ',test_r2_xgb)\n    print('mse Val : ',test_mse_xgb)\n    arr_xgb_test_r2.append(test_r2_xgb)\n    arr_xgb_test_mse.append(test_mse_xgb)\n\n","e06cedb1":"print(np.mean(arr_xgb_val_r2))\nprint(np.mean(arr_xgb_val_mse))\nprint(np.mean(arr_xgb_test_r2))\nprint(np.mean(arr_xgb_test_mse))","290fc8cf":"arr_lr_val_r2 = []\narr_lr_val_mse = []\n\narr_lr_test_r2 = []\narr_lr_test_mse = []\n\nfor train, test in kfold.split(X_train):\n    clf_lr = LinearRegression()\n    clf_lr.fit(X[train],y[train])\n\n    Y_pred_val = clf_lr.predict(X[test])\n    val_mse_lr = mean_squared_error(y[test],Y_pred_val)\n    val_r2_lr = r2_score(y[test], Y_pred_val) \n\n    print('r2 Val : ',val_r2_lr)\n    print('mse Val : ',val_mse_lr)\n    arr_lr_val_r2.append(val_r2_lr)\n    arr_lr_val_mse.append(val_mse_lr)\n    \n    Y_pred_test = clf_lr.predict(X_test)\n    test_mse_lr = mean_squared_error(Y_test,Y_pred_test)\n    test_r2_lr = r2_score(Y_test, Y_pred_test) \n\n    print('r2 Test : ',test_r2_lr)\n    print('mse Val : ',test_mse_lr)\n    arr_lr_test_r2.append(test_r2_lr)\n    arr_lr_test_mse.append(test_mse_lr)\n\n","f3317373":"print(np.mean(arr_lr_val_r2))\nprint(np.mean(arr_lr_val_mse))\nprint(np.mean(arr_lr_test_r2))\nprint(np.mean(arr_lr_test_mse))","2fc811dd":"result_modelling = pd.DataFrame({\n    'model': ['GradientBoostingRegressor', 'RandomForestRegressor', 'Support Vector Regression', \n            'KNeighborsRegressor', 'XGBRegressor', 'Linear Regression'\n             ],\n    'val mse': [np.mean(arr_gb_val_mse), np.mean(arr_rf_val_mse), np.mean(arr_svm_val_mse),\n                np.mean(arr_knn_val_mse), np.mean(arr_xgb_val_mse), np.mean(arr_lr_val_mse)\n        \n    ],\n    \n    'val r2': [np.mean(arr_gb_val_r2), np.mean(arr_rf_val_r2), np.mean(arr_svm_val_r2),\n               np.mean(arr_knn_val_r2), np.mean(arr_xgb_val_r2), np.mean(arr_lr_val_r2)\n        \n    ],\n    'test mse': [np.mean(arr_gb_test_mse), np.mean(arr_rf_test_mse), np.mean(arr_svm_test_mse),\n                 np.mean(arr_knn_test_mse), np.mean(arr_xgb_test_mse), np.mean(arr_lr_test_mse)\n        \n    ],\n    \n    'test r2': [np.mean(arr_gb_test_r2), np.mean(arr_rf_test_r2), np.mean(arr_svm_test_r2),\n                np.mean(arr_knn_test_r2), np.mean(arr_xgb_test_r2), np.mean(arr_lr_test_r2)\n        \n    ],\n})\nresult_modelling['val mse'] = np.round(result_modelling['val mse'], decimals = 3)\nresult_modelling['val r2'] = np.round(result_modelling['val r2'], decimals = 3)\nresult_modelling['test mse'] = np.round(result_modelling['test mse'], decimals = 3)\nresult_modelling['test r2'] = np.round(result_modelling['test r2'], decimals = 3)\nresult_modelling = result_modelling.sort_values(by='val r2', ascending=False).reset_index(drop=True)\nresult_modelling","58239444":"result_modelling.to_csv('..\/result\/result_modelling.csv',index=False)","8f1d9a66":"val = result_modelling['val r2']\ntest = result_modelling['test r2']\n\nbars = result_modelling['model']\nbarwidth = 0.3\n\ntotal_pos = np.arange(len(bars))\nval_pos = [x + barwidth for x in total_pos]\ntest_pos = [x + 2*barwidth for x in total_pos]\n","c515b4d2":"fig,ax =plt.subplots(figsize=(15,10))\nplt.bar(val_pos,val,width=barwidth, color = '#800000', alpha=0.9)\nplt.bar(test_pos,test,width=barwidth, color = '#008000', alpha=0.9)\n\nplt.bar(0.3,0,width=barwidth, color = '#800000',label='Val ')\nplt.bar(0.6,0,width=barwidth, color = '#008000', label='Test ')\n# plt.bar(0.3,0.9450,width=barwidth, color = '#800000')\n# plt.bar(0.6,0.946977,width=barwidth, color = '#008000')\n\ntitle = 'Model Analysis (Cross Validation and Test)'\ntxpos = 4 #title x coordinate\ntypos = 1.1 #title y coordinate\nax.text(txpos,typos,title,horizontalalignment='center',color='#800000',fontsize=20,fontweight='bold')\n\n\n# insight = '''\n\n# '''\n# ixpos = 0.1 #insight x coordinate\n# iypos = 1.05 #insight y coordinate\n# ax.text(ixpos,iypos,insight,horizontalalignment='left',color='grey',fontsize=16,fontweight='normal')\n\nplt.xticks(val_pos, bars,rotation=45)\nax.legend(loc='upper left', bbox_to_anchor= (1.01, 1.0), ncol=1, borderaxespad=0,frameon=False)\nax.set_ylim(0,1.2)\n\n\n\n\nplt.savefig('..\/fig\/result_modelling.png',bbox_inches='tight')","f7b1d262":"df_test = pd.read_csv('..\/input\/volume_forecast.csv')\ndf_test.head(2)","4fb66b44":"df_test.isnull().sum()","e9a84995":"df_test['SKU'] = df_test['SKU'].apply(lambda x: x[4:])\ndf_test['SKU'] = df_test['SKU'].astype(int)\n\ndf_test['Agency'] = df_test['Agency'].apply(lambda x: x[7:])\ndf_test['Agency'] = df_test['Agency'].astype(int)\ndf_test.head(2)","a3c09944":"df_test = df_test.drop('Volume', axis=1)","51e72482":"X_test = sc_x.fit_transform(df_test.astype(float))","f5f11680":"model_xgb = joblib.load('..\/model\/xgb_2.dat')","ffb8640d":"y_pred = model_xgb.predict(X_test)\ny_pd = pd.DataFrame({'Volume':y_pred})\nprint('finish predict')\ny_pd.head()","ed00b1be":"df_test = pd.read_csv('..\/input\/volume_forecast.csv') \ndf_test = df_test.drop('Volume', axis=1)\ndf_test.head()","be47cbe6":"frames = [df_test, y_pd]\nresult = pd.concat(frames, axis=1)\nresult.to_csv('..\/result\/volume_forecast.csv',index=False)\nresult.head()","3df64cdb":"df = pd.read_csv('..\/input\/train.csv')\ndf['Volume'] = df['Volume'].round(2)\ndf['Price'] = df['Price'].round(2)\ndf['Sales'] = df['Sales'].round(2)\ndf['Promotions'] = df['Promotions'].round(2)\ndf['Avg_Max_Temp'] = df['Avg_Max_Temp'].round(2)\ndf.head()","361837ac":"df.isnull().sum()","73893257":"# df = df.drop('YearMonth', axis=1)\n# df_temp = df\ncols = list(df)\n\ncorr_ =df[cols].corr()\nplt.figure(figsize=(16,10))\nsns.heatmap(corr_, annot=True, fmt = \".2f\", cmap = \"BuPu\")\nplt.savefig('..\/fig\/Data Numeric Corr.png',bbox_inches='tight')","807bbbbf":"df = df.drop('Soda_Volume',axis=1)\ndf = df.drop('Good Friday',axis=1)\ndf = df.drop('Sales',axis=1)\ndf = df.drop('Revolution Day Memorial',axis=1)\ndf = df.drop('Independence Day',axis=1)\ndf = df.drop('Beer Capital',axis=1)\ndf = df.drop('New Year',axis=1)\ndf = df.drop('Avg_Max_Temp',axis=1)\ndf = df.drop('FIFA U-17 World Cup',axis=1)\ndf = df.drop('Football Gold Cup',axis=1)\ndf = df.drop('Avg_Population_2017',axis=1)\ndf = df.drop('Avg_Yearly_Household_Income_2017',axis=1)","979f7e23":"df.isnull().sum()","983ad489":"corr_ =df[list(df)].corr()\nplt.figure(figsize=(16,10))\nsns.heatmap(corr_, annot=True, fmt = \".2f\", cmap = \"BuPu\")\nplt.savefig('..\/fig\/Data Numeric Corr 2.png',bbox_inches='tight')","a719b72b":"df = df[['YearMonth','Volume','Price','Promotions']].copy()\ndf.head()","493460b0":"df = df.groupby('YearMonth')['Volume','Price','Promotions'].agg(['sum','mean','std']).reset_index()\ndf.head()","fd36e052":"df.columns = ['YearMonth','v_sum','v_mean','v_std','p_sum','p_mean','p_std','pr_sum','pr_mean','pr_std']","04f896b1":"len(df)","739dd684":"df = df.sort_values('YearMonth', ascending=True).reset_index(drop=True)\ndf['YearMonth'] = pd.to_datetime(df['YearMonth'])\ndf = df.merge(df2, on='YearMonth', how='inner')\ndf.head()","6a8dc745":"df = df.drop('Good Friday',axis=1)\ndf = df.drop('Revolution Day Memorial',axis=1)\ndf = df.drop('Independence Day',axis=1)\ndf = df.drop('Beer Capital',axis=1)\ndf = df.drop('New Year',axis=1)\ndf = df.drop('FIFA U-17 World Cup',axis=1)\ndf = df.drop('Football Gold Cup',axis=1)","db03bea4":"data = df\ndata= data.drop('YearMonth', axis=1)\ndata.head(2)","3f942f3e":"data.info()","84392063":"len(list(data))","125248a8":"fig,ax = plt.subplots(14,1,figsize=(20,15))\nfor i,column in enumerate([col for col in data.columns if col != 'wnd_dir']):\n    data[column].plot(ax=ax[i])\n    ax[i].set_title(column)\nplt.savefig('..\/fig\/Dataset1.png',bbox_inches='tight')","4b7e9216":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    # put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","80f5c980":"values = data.values\nprint(values.shape)\nvalues = values.astype('float32')\nseries_to_supervised(values,1,1).head()","937ae03f":"reframed = series_to_supervised(values,1,1)\n# drop columns we don't want to predict\nreframed.drop(reframed.columns[[15,16,17,18,19,20,21,22,23,24,25,26,27]], axis=1, inplace=True)\nprint(len(list(reframed)))\nreframed.head()","648699f8":"scaler = MinMaxScaler(feature_range=(0,1))\nscaled = scaler.fit_transform(values)\nreframed = series_to_supervised(scaled,1,1)\nreframed.drop(reframed.columns[[15,16,17,18,19,20,21,22,23,24,25,26,27]], axis=1, inplace=True)\nreframed.head()","6ab05342":"values = reframed.values\nn_train = 49\ntrain = values[:n_train]\ntest = values[n_train:]\ntrainX,trainY = train[:,:-1],train[:,-1]\ntestX,testY = test[:,:-1],test[:,-1]\n\nprint(trainX.shape,trainY.shape,testX.shape,testY.shape)\n\ntrainX = trainX.reshape(trainX.shape[0],1,trainX.shape[1])\ntestX = testX.reshape(testX.shape[0],1,testX.shape[1])\n\n\nprint(trainX.shape)\nprint(testX.shape)","7b6246d9":"len(testX)","909607ae":"stop_noimprovement = EarlyStopping(patience=10)\nmodel = Sequential()\nmodel.add(LSTM(50,input_shape=(trainX.shape[1],trainX.shape[2]),dropout=0.2))\nmodel.add(Dense(1))\nmodel.compile(loss=\"mae\",optimizer=\"adam\")\n\nhistory= model.fit(trainX,trainY,validation_data=(testX,testY),epochs=100,verbose=2,callbacks=[stop_noimprovement],shuffle=False)\n","8c4eb552":"model.summary()","c4708ddc":"fig,ax =plt.subplots(figsize=(15,10))\n\nplt.plot(history.history['loss'],label='train loss')\nplt.plot(history.history['val_loss'],label='val loss')\nplt.legend()\n# plt.savefig('..\/fig\/trai val forecasting.png',bbox_inches='tight')","2b65139a":"len(testX)","b9313f8c":"predicted = model.predict(testX)","ff3d28fc":"testXRe = testX.reshape(testX.shape[0],testX.shape[2])\npredicted = np.concatenate((predicted,testXRe[:,1:]),axis=1)\nprint('predicted.shape : ',predicted.shape)","bc4fa26f":"predicted = model.predict(testX)\n\ntestXRe = testX.reshape(testX.shape[0],testX.shape[2])\npredicted = np.concatenate((predicted,testXRe[:,1:]),axis=1)\nprint('predicted.shape : ',predicted.shape)\n\npredicted = scaler.inverse_transform(predicted)\ntestY = testY.reshape(len(testY),1)\nprint('testY.shape : ',testY.shape)\n\ntestY = np.concatenate((testY,testXRe[:,1:]),axis=1)\ntestY = scaler.inverse_transform(testY)","6fde7397":"pd.DataFrame(testY)","739408bd":"np.sqrt(mean_squared_error(testY[:,0],predicted[:,0]))","35df12c9":"result = pd.concat([pd.Series(predicted[:,0]),pd.Series(testY[:,0])],axis=1)\nresult.columns = ['thetahat','theta']\nresult['diff'] = result['thetahat'] - result['theta']","77a2841c":"result = pd.concat([pd.Series(predicted[:,0]),pd.Series(testY[:,0])],axis=1)\nresult.columns = ['thetahat','theta']\nresult['diff'] = result['thetahat'] - result['theta']","b6054a50":"result.head()","9d31d174":"df_new = df[['YearMonth','Volume','Price','Promotions']].copy()\ndf_new.head()","9efd063d":"df_new = df_new.groupby('YearMonth')['Volume'].agg(['sum']).reset_index()\ndf_new.head()","740d5581":"df_new['YearMonth'] = pd.to_datetime(df_new['YearMonth'])\ndf_new = df_new.sort_values(by='YearMonth').reset_index(drop=True)\ndf_new = df_new.drop('YearMonth', axis=1)\ndf_new.head()","678a7386":"fig,ax =plt.subplots(figsize=(15,10))\nplt.plot(df_new)\nplt.xlabel('month')\nplt.ylabel('sum of volume')\nplt.savefig('..\/fig\/forecasting unvariate 1.png',bbox_inches='tight')","71672840":"train = df_new[0:50]\ntest = df_new[50:]","f26b2561":"# load the trainset\ntrain = train.values\ntrain = train.astype('float32')\nscaler = MinMaxScaler(feature_range=(0, 1))\ntrain = scaler.fit_transform(train)\n\n# load the testset\ntest = test.values\ntest = test.astype('float32')\nscaler = MinMaxScaler(feature_range=(0, 1))\ntest = scaler.fit_transform(test)\nprint(test.shape)","a9ac105c":"# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)","6ce3d2b5":"look_back = 1\ntestX, testY = create_dataset(test, look_back)\n\n\ntrainX, trainY = create_dataset(train, look_back)","762647a9":"trainX.shape","fb41cb18":"trainY.shape","6ae3f191":"testX.shape","f29c4931":"# reshape input to be [samples, time steps, features]\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))","2034a99f":"model = Sequential()\nmodel.add(LSTM(4, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n# model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n\nhistory= model.fit(trainX,trainY,validation_data=(testX,testY),epochs=100,verbose=2,callbacks=[stop_noimprovement],shuffle=False)\n","2a3c31e5":"model.summary()","2741d49f":"fig,ax =plt.subplots(figsize=(15,10))\n\nplt.plot(history.history['loss'],label='train loss')\nplt.plot(history.history['val_loss'],label='val loss')\nplt.legend()\nplt.savefig('..\/fig\/unvariate 1.png',bbox_inches='tight')","2f26e96d":"testPredict = model.predict(testX)\nprint(testPredict.shape)\ntestPredict = scaler.inverse_transform(testPredict)\ntestPredict","1865c4f6":"len(test)","47aedc5d":"test = df_new[55:]\ntest","6732829c":"test.values","8707eaf5":"# load the testset\ntest = test.values\ntest = test.astype('float32')\nscaler = MinMaxScaler(feature_range=(0, 1))\ntest = scaler.fit_transform(test)","3f022bb8":"test = np.expand_dims(test, axis=1)\ntest.shape","13d561f4":"data= test\n# data = np.expand_dims(data, axis=1)\n\nprint('data n',data)\nprint('\\n')\n\n\npredict = model.predict(data)\nprint('predict data n ',predict)\nprint('\\n')\n\n\nfuture = predict\nfuture = np.expand_dims(future, axis=1)\n\nfuture_predict = model.predict(future)\nprint('predict data n + 1 ',future_predict)","34b8b5ee":"future_predict = scaler.inverse_transform(future_predict)\nfuture_predict","5ab739a8":"## 1. Introduction","e58d7a3a":"### 5.2.3 Support Vector Regression","cb5eb2e6":"### 5.2.7 Result Modeling","cfb5af25":"### 5.2.6 Linear Regression","5d754610":"## 6. Test Algorithm on Test Data","b5e6cfae":"### Agency_14","c1050074":"###  2.1 Load Data","9c1c1b16":"### 5.1 Validation Data","2b9b786b":"### 3.2 by demographis","183708d9":"## 2. Collection of Data","f649154c":"### Data Training","7776517f":"## 7.1 Numerical Data Correlation\u00b6","7d4b0e09":"### 3.3 combination of weather and demographis","51abd114":"## 8. Forecasting Unvariate","105de924":"## 5. Train Algorithm on Train Data","b8ad8822":"## 7.3 Modelling ","2539f667":" ### Predictions","643fe76a":"### 5.2.4 KNeighborsRegressor","aea51226":"## 7. Forecasting Multivariate","4bf73c01":"## 3. SKU Recommendation","0495e101":"### 5.2.1 GradientBoostingRegressor","ac1e4344":"**dari kedua hasil analisa tersebut maka<br>\nAgency_06 : 55, 60, 5, 40, 8, dan 50<br>\nAgency_14 : 57, 56, 12, 13, 15, 16. 17, 20, 38, 39, 57, 58, 59, 60<br><br>\nMencari SKU dengan volume terbaik dari agency diatas di dalam data historical_volume**","98ffa441":"### Agency_06","04054ca5":"### 5.2.2 RandomForestRegressor","1e568576":"1. [**Introduction**](#1.-Introduction)<br>\n2. [**Collection of Data**](#2.-Collection-of-Data)<br>\n    [2.1 Load Data](#2.1-Load-Data)<br>\n3. [**SKU Recommendation**](#3.-SKU-Recommendation)<br>\n    [3.1 by weather data](#3.1-by-weather-data)<br>\n    [3.2 by demographis](#3.2-by-demographis)<br>\n    [3.3 combination of weather and demographis](#3.3-combination-of-weather-and-demographis)<br>\n4. [**Data preparation and Data Distribution**](#4.-Data-Preparation-and-Data-Distribution)<br>\n    [4.1 Check Null and Missing Values](#4.1-Check-Null-and-Missing-Values)<br>\n5. [**Train Algorithm on Train Data**](#5.-Train-Algorithm-on-Train-Data)<br>\n    [5.1 Validation Data](#5.1-Validation-Data)<br>\n    [5.2 Modelling](#5.2-Modelling)<br>\n    [5.2.1 GradientBoostingRegressor](#5.2.1-GradientBoostingRegressor)<br>\n    [5.2.2 RandomForestRegressor](#5.2.3-RandomForestRegressor)<br>\n    [5.2.3 Support Vector Regression](#5.2.3-Support_Vector_Regression)<br>\n    [5.2.4 KNeighborsRegressor](#5.2.4-KNeighborsRegressor)<br>\n    [5.2.5 XGBRegressor](#5.2.5-XGBRegressor)<br>\n    [5.2.6 Linear Regression](#5.2.6-Linear-Regression)<br>\n    [5.2.7 Result Modeling](#5.2.7-Result-Modeling)<br>\n6. [**Test Algorithm on Test Data**](#6.-Test-Algorithm-on-Test-Data)<br> \n7. [**Forecasting Multivariate**](#7.-Forecasting-Multivariate)<br> \n    [7.1 Numerical Data Correlation](#7.1-Numerical-Data-Correlation)<br>\n    [7.2 Feature Engineering](#7.2-Feature-Engineering)<br>\n    [7.3 Modelling](#7.3-Modelling)<br>\n8. [**Forecasting Unvariate**](#8.-Forecasting-Unvariate)<br>\n        ","0ea84cdd":"## 7.2 Feature Engineering","3997d0f2":"**jika kita analisa dari graph tersebut maka,<br>\nAgency_06 dekat dengan agency 55 dan 60<br>\nAgency_14 dekat dengan 57 dan 56**","39814ce8":"## 4. Data Preparation and Data Distribution","f2485555":"### 3.1 by weather data","05f5bb9b":"### Concat","cbbaddf3":"### 4.1 Check Null and Missing Values","d909cd93":"Country Beeristan, a high potential market, accounts for nearly 10% of Stallion & Co.\u2019s global beer sales. Stallion & Co. has a large portfolio of products distributed to retailers through wholesalers (agencies). There are thousands of unique wholesaler-SKU\/products combinations. In order to plan its production and distribution as well as help wholesalers with their planning, it is important for Stallion & Co. to have an accurate estimate of demand at SKU level for each wholesaler.<br>\n\nCurrently demand is estimated by sales executives, who generally have a \u201cfeel\u201d for the market and predict the net effect of forces of supply, demand and other external factors based on past experience. The more experienced a sales exec is in a particular market, the better a job he does at estimating. Joshua, the new Head of S&OP for Stallion & Co. just took an analytics course and realized he can do the forecasts in a much more effective way. He approaches you, the best data scientist at Stallion, to transform the exercise of demand forecasting.<br>\n\n$\\textbf{Arif Romadhan}$ <br>\nemail : arifromadhan19@gmail.com<br><br>\n[Link my kaggle](https:\/\/www.kaggle.com\/utathya\/future-volume-prediction)<br>\n[Link github](https:\/\/github.com\/arifromadhan19\/kaggle\/blob\/master\/Volume%20Forecasting\/Volume%20Forecasting.ipynb)<br>","b0a076da":"**jika kita analisa by mean dan median<br>\nAgency 6 memiliki kesamaan dengan agency 5, 40, 8, dan 50<br>\nAgency 14 memiliki kesamaan dengan agency 12, 13, 15, 16. 17, 20, 38, 39, 57, 58, 59, 60**","bb22cd7d":"### 5.2 Modelling","d821a04e":"#### Merge historical_volume, price_sales_promotion, weather, industry_soda_sales, industry_volume","556b82f9":"#### Merge industry_soda_sales, industry_volume","e450b40e":"# Volume Forecasting : SKU future volume analysis and prediction","a9e5ecd8":"### 5.2.5 XGBRegressor"}}