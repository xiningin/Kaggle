{"cell_type":{"f4c48687":"code","5d916a8f":"code","60a40b9b":"code","e8ead2b3":"code","13e39667":"code","a47b941e":"code","09f00cdd":"code","c18bee1f":"code","e3ef08f0":"code","f5864dbb":"code","df1b82a1":"code","2fae2c02":"code","a0186d70":"code","0f7d64c8":"code","37ab6ccd":"code","5640e314":"code","9610159a":"code","973d0406":"code","7a205737":"code","2b040e99":"code","df8a3500":"code","ec70aff7":"markdown"},"source":{"f4c48687":"RUN_FOLDS = 1\nEPOCHS = 1\nBATCHSIZE = 128\n# please change to the following values offline\n# RUN_FOLDS = 5\n# EPOCHS = 50","5d916a8f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm, tqdm_notebook\nimport time\nimport glob\nimport os\nprint(os.listdir(\"..\/input\"))\nimport gc\n\n\n# Any results you write to the current directory are saved as output.\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom keras import layers\nfrom keras import backend as K\nfrom keras import regularizers\nfrom keras.constraints import max_norm\nfrom keras.models import Sequential\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.models import Model\nfrom keras.initializers import glorot_uniform\nfrom keras.layers import Input,Dense,Activation,ZeroPadding2D,BatchNormalization,Flatten,Conv2D,AveragePooling2D,MaxPooling2D,Dropout,concatenate\nfrom sklearn import preprocessing\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve\n#from sklearn.metrics import auc\nfrom sklearn.metrics import roc_auc_score\nfrom keras.utils import Sequence,to_categorical\n\nGPU = 4\nos.environ['CUDA_VISIBLE_DEVICES'] = str(GPU)\n\nimport datetime\nimport os\nimport sys\nimport time\n\nimport random\nimport multiprocessing\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","60a40b9b":"def reverse(tr,cols=None):\n    reverse_list = [0,1,2,3,4,5,6,7,8,11,15,16,18,19,\n                22,24,25,26,27,41,29,\n                32,35,37,40,48,49,47,\n                55,51,52,53,60,61,62,103,65,66,67,69,\n                70,71,74,78,79,\n                82,84,89,90,91,94,95,96,97,99,\n                105,106,110,111,112,118,119,125,128,\n                130,133,134,135,137,138,\n                140,144,145,147,151,155,157,159,\n                161,162,163,164,167,168,\n                170,171,173,175,176,179,\n                180,181,184,185,187,189,\n                190,191,195,196,199]\n    reverse_list = ['var_%d'%i for i in reverse_list]\n    if cols is not None:\n        for col in cols:\n            colx = col.split('_')\n            colx = '_'.join(colx[:2])\n            if colx in reverse_list:\n                print('reverse',col)\n                tr[col] = tr[col]*(-1)\n        return tr\n    \n    for col in reverse_list:\n        tr[col] = tr[col]*(-1)\n    return tr\n\ndef scale(tr,te=None):\n    for col in tr.columns:\n        if col.startswith('var_'):\n            mean,std = tr[col].mean(),tr[col].std()\n            tr[col] = (tr[col]-mean)\/std\n            if te is not None:\n                te[col] = (te[col]-mean)\/std\n    if te is None:\n        return tr\n    return tr,te\n\ndef getp_vec_sum(x,x_sort,y,std,c=0.5):\n    # x is sorted\n    left = x - std\/c\n    right = x + std\/c\n    p_left = np.searchsorted(x_sort,left)\n    p_right = np.searchsorted(x_sort,right)\n    p_right[p_right>=y.shape[0]] = y.shape[0]-1\n    p_left[p_left>=y.shape[0]] = y.shape[0]-1\n    return (y[p_right]-y[p_left])\n\ndef get_pdf(tr,col,x_query=None,smooth=3):\n    std = tr[col].std()\n    tr = tr.dropna(subset=[col])\n    df = tr.groupby(col).agg({'target':['sum','count']})\n    cols = ['sum_y','count_y']\n    df.columns = cols\n    df = df.reset_index()\n    df = df.sort_values(col)\n    y,c = cols\n    \n    df[y] = df[y].cumsum()\n    df[c] = df[c].cumsum()\n    \n    if x_query is None:\n        rmin,rmax,res = -5.0, 5.0, 501\n        x_query = np.linspace(rmin,rmax,res)\n    \n    dg = pd.DataFrame()\n    tm = getp_vec_sum(x_query,df[col].values,df[y].values,std,c=smooth)\n    cm = getp_vec_sum(x_query,df[col].values,df[c].values,std,c=smooth)+1\n    dg['res'] = tm\/cm\n    dg.loc[cm<500,'res'] = 0.1\n    return dg['res'].values\n\ndef get_pdfs(tr):\n    y = []\n    for i in range(200):\n        name = 'var_%d'%i\n        res = get_pdf(tr,name)\n        y.append(res)\n    return np.vstack(y)\n\ndef print_corr(corr_mat,col,bar=0.95):\n    cols = corr_mat.loc[corr_mat[col]>bar,col].index.values\n    return cols.tolist()\n\ndef get_group(df,cols,reverse=True,bar=0.9):\n    if reverse:\n        df = reverse(df,cols=cols)\n    df = scale(df)\n    pdfs = get_pdfs(df)\n    df_pdf = pd.DataFrame(pdfs.T,columns=cols)\n    corr_mat = df_pdf.corr(method='pearson')\n    groups =[]\n    skip_list = []\n    for i in cols:\n        if i not in skip_list:\n            cols = print_corr(corr_mat,i,bar)\n            if(len(cols)>1):\n                groups.append(cols)\n                for e,v in enumerate(cols):\n                    skip_list.append(i)\n    return groups","e8ead2b3":"class DataGenerator(Sequence):\n    'Generates data for Keras'\n    def __init__(self, **kwargs):\n        'Initialization'\n        self.params = kwargs\n        self.X = self.params['X']\n        self.cols_info = self.params['cols_info']\n        self.groups = self.params['groups']\n        self.shuffle = self.params['shuffle']\n        self.y = self.params['y']\n        self.aug = self.params['aug']\n        self.indexes = np.arange(self.y.shape[0])\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        batch_size = self.params['batch_size']\n        return int(np.floor(self.indexes.shape[0] \/ batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        batch_size = self.params['batch_size']\n        indexes = self.indexes[index*batch_size:(index+1)*batch_size]\n\n        X, y = self.__data_generation(indexes)\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        y = self.y[indexes]\n        X = self.X[indexes]\n        batch_size = self.params['batch_size']\n        if self.aug:\n            X,y = augment_fix_fast(X,y,groups=2,t1=2, t0=2)\n        base_feats, noise_feats = self.cols_info\n        allfeas = base_feats+noise_feats\n        X = pd.DataFrame(X,columns=allfeas)\n        X = get_keras_groups_data(X, self.cols_info,self.groups)\n        return X, y\n    \n    def get_keras_data(self, dataset, cols_info):\n        X = {}\n        base_feats, noise_feats = cols_info\n        X['base'] = np.reshape(dataset[:,:len(base_feats)], (-1, len(base_feats), 1))\n        X['noise1'] = np.reshape(dataset[:,len(base_feats): len(base_feats) + len(noise_feats)], (-1, len(noise_feats), 1))\n        \n        return X\n    \n    def aug_(self,xb,xn1,y,t=2):\n        xb_pos,xb_neg,xn1_pos,xn1_neg = [],[],[],[]\n        for i in range(t):\n            mask = y>0\n            x1 = xb[mask].copy()\n            x2 = xn1[mask].copy()\n            ids = np.arange(x1.shape[0])\n            for c in range(x1.shape[1]):\n                np.random.shuffle(ids)\n                x1[:,c] = x1[ids][:,c]\n                x2[:,c] = x2[ids][:,c]\n            xb_pos.append(x1)\n            xn1_pos.append(x2)\n        \n        for i in range(t):\n            mask = y==0\n            x1 = xb[mask].copy()\n            x2 = xn1[mask].copy()\n            ids = np.arange(x1.shape[0])\n            for c in range(x1.shape[1]):\n                np.random.shuffle(ids)\n                x1[:,c] = x1[ids][:,c]\n                x2[:,c] = x2[ids][:,c]\n            xb_neg.append(x1)\n            xn1_neg.append(x2)\n    \n\n        xb_pos = np.vstack(xb_pos)\n        xb_neg = np.vstack(xb_neg)\n        xn1_pos = np.vstack(xn1_pos)\n        xn1_neg = np.vstack(xn1_neg)\n\n        ys = np.ones(xb_pos.shape[0])\n        yn = np.zeros(xb_neg.shape[0])\n        xb = np.vstack([xb,xb_pos,xb_neg])\n        xn1 = np.vstack([xn1,xn1_pos,xn1_neg])\n        y = np.concatenate([y,ys,yn])\n        return xb,xn1,y","13e39667":"# define helper functions. auc, plot_history\ndef auc(y_true, y_pred):\n    #auc = tf.metrics.auc(y_true, y_pred)[1]\n    y_pred = y_pred.ravel()\n    y_true = y_true.ravel()\n    return roc_auc_score(y_true, y_pred)\n\ndef auc_2(y_true, y_pred):\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n\ndef plot_history(histories, key='binary_crossentropy'):\n    plt.figure(figsize=(16,10))\n    #plt.plot([0, 1], [0, 1], 'k--')\n    for name, history in histories:\n        val = plt.plot(history.epoch, history.history['val_'+key], '--', label=name.title()+' Val')\n\n    plt.plot(history.epoch, history.history[key], color=val[0].get_color(), label=name.title()+' Train')\n\n    plt.xlabel('Epochs')\n    plt.ylabel(key.replace('_',' ').title())\n    plt.legend()\n\n    plt.xlim([0,max(history.epoch)])\n    plt.ylim([0, 0.4])\n    plt.show()","a47b941e":"def shuffle_col_vals_fix(x1, groups):\n    group_size = x1.shape[1]\/\/groups\n    xs = [x1[:, i*group_size:(i+1)*group_size] for i in range(groups)]\n    rand_x = np.array([np.random.choice(x1.shape[0], size=x1.shape[0], replace=False) for i in range(group_size)]).T\n    grid = np.indices(xs[0].shape)\n    rand_y = grid[1]\n    res = [x[(rand_x, rand_y)] for x in xs]\n    return np.hstack(res)\n\ndef augment_fix_fast(x,y,groups,t1=2, t0=2):\n    # In order to make the sync version augment work, the df should be the form of:\n    # var_1, var_2, var_3 | var_1_count, var_2_count, var_3_count | var_1_rolling, var_2_rolling, var_3_rolling\n    # for the example above, 3 groups of feature, groups = 3\n    xs,xn = [],[]\n    for i in range(t1):\n        mask = y>0\n        x1 = x[mask].copy()\n        x1 = shuffle_col_vals_fix(x1, groups)\n        xs.append(x1)\n\n    for i in range(t0):\n        mask = (y==0)\n        x1 = x[mask].copy()\n        x1 = shuffle_col_vals_fix(x1, groups)\n        xn.append(x1)\n\n    xs = np.vstack(xs); xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0]);yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn]); y = np.concatenate([y,ys,yn])\n    return x,y","09f00cdd":"def build_magic_nn():\n    train_df = pd.read_csv('..\/input\/train.csv')\n    test_df =  pd.read_csv(\"..\/input\/test.csv\")\n\n        \n    base_features = [x for x in train_df.columns.values.tolist() if x.startswith('var_')]\n    train_df['real'] = 1\n\n    for col in base_features:\n        test_df[col] = test_df[col].map(test_df[col].value_counts())\n    a = test_df[base_features].min(axis=1)\n\n    test_df = pd.read_csv('..\/input\/test.csv')\n    test_df['real'] = (a == 1).astype('int')\n\n    train = train_df.append(test_df).reset_index(drop=True)\n    del test_df, train_df; gc.collect()\n    for col in tqdm(base_features):\n        train[col + '_size'] = train[col].map(train.loc[train.real==1, col].value_counts())\n    cnt_features = [col + '_size' for col in base_features]\n\n    for col in tqdm(base_features):\n    #        train[col+'size'] = train.groupby(col)['target'].transform('size')\n        train.loc[train[col+'_size']>1,col+'_no_noise'] = train.loc[train[col+'_size']>1,col]\n    noise1_features = [col + '_no_noise' for col in base_features]\n\n    train[noise1_features] = train[noise1_features].fillna(train[noise1_features].mean())\n\n    train_df = train[train['target'].notnull()]\n    test_df = train[train['target'].isnull()]\n    all_features = base_features + noise1_features\n\n    scaler = preprocessing.StandardScaler().fit(train_df[all_features].values)\n    df_trn = pd.DataFrame(scaler.transform(train_df[all_features].values), columns=all_features)\n    df_tst = pd.DataFrame(scaler.transform(test_df[all_features].values), columns=all_features)\n\n    return df_trn,df_tst,train_df,test_df","c18bee1f":"df_trn,df_tst,train_df,test_df = build_magic_nn()","e3ef08f0":"#%%time\ny = train_df['target'].values\nbase_features = ['var_%d'%i for i in range(200)]\nnoise1_features = ['%s_no_noise'%i for i in base_features]\nall_features = base_features + noise1_features\ncols_info = [base_features, noise1_features]","f5864dbb":"df_trn = reverse(df_trn,cols=base_features + noise1_features)\ndf_tst = reverse(df_tst,cols=base_features + noise1_features)","df1b82a1":"def get_keras_data(dataset, cols_info):\n    X = {}\n    base_feats, noise_feats = cols_info\n    X['base'] = np.reshape(np.array(dataset[base_feats].values), (-1, len(base_feats), 1))\n    X['noise1'] = np.reshape(np.array(dataset[noise_feats].values), (-1, len(noise_feats), 1))\n    return X\n\ndef get_keras_groups_data(dataset, cols_info, groups):\n    X = {}\n    base_feats, noise_feats = cols_info\n    #X['base'] = np.reshape(np.array(dataset[base_feats].values), (-1, len(base_feats), 1))\n    for c,g in enumerate(groups):\n        X['group_%d'%c] = np.expand_dims(dataset[g].values,2)\n    X['noise1'] = np.expand_dims(dataset[noise_feats].values,2)\n    return X","2fae2c02":"groups = get_group(train_df[base_features+['target']].copy(),base_features,reverse=False,bar=0.9)","a0186d70":"X_test = get_keras_groups_data(df_tst[all_features], cols_info, groups)","0f7d64c8":"# define network structure -> 2D CNN\ndef Convnet(cols_info, groups, classes=1):\n    base_feats, noise1_feats= cols_info\n    \n    xs = []\n    ins = []\n    for c,i in enumerate(groups):\n        X = Input(shape=(len(i), 1), name='group_%d'%c)\n        ins.append(X)\n        X = Dense(1)(X)\n        X = Activation('relu')(X)\n        X = Flatten(name='group_%d_last'%c)(X)\n        xs.append(X)\n    \n    for i,j in zip(cols_info,['noise1']):\n        X = Input(shape=(len(i), 1), name=j)\n        ins.append(X)\n        X = Dense(16)(X)\n        X = Activation('relu')(X)\n        X = Flatten(name='%s_last'%j)(X)\n        xs.append(X)\n    \n    X = concatenate(xs)\n    X = Dense(classes, activation='sigmoid')(X)\n    \n    model = Model(inputs=ins,outputs=X)\n    return model\n\nmodel = Convnet(cols_info,groups)\nmodel.summary()","37ab6ccd":"try:\n    del train, df_tst \nexcept:\n    pass\ngc.collect()","5640e314":"# parameters\nSEED = 2019\nn_folds = 5\ndebug_flag = True\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)","9610159a":"#%%time\n#transformed_shape = tuple([-1] + list(shape))\n#X_test = np.reshape(X_test, transformed_shape)\n\ni = 0\nresult = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nval_aucs = []\nvalid_X = train_df[['target']]\nvalid_X['predict'] = 0\nfor train_idx, val_idx in skf.split(df_trn, y):\n    if i == RUN_FOLDS:\n        break\n    \n    i += 1    \n    X_train, y_train = df_trn.iloc[train_idx], y[train_idx]\n    X_valid, y_valid = df_trn.iloc[val_idx], y[val_idx]\n    \n    #aug\n    X_train, y_train = augment_fix_fast(X_train.values, y_train, groups=2, t1=2, t0=2)\n    X_train = pd.DataFrame(X_train, columns=all_features)\n    \n    #X_train = get_keras_data(X_train, cols_info)\n    #X_valid = get_keras_data(X_valid, cols_info)\n    #X_train = np.reshape(X_train, transformed_shape)\n    #X_valid = np.reshape(X_valid, transformed_shape)\n    \n    model_name = 'NN_fold{}_{}.h5'.format(str(i),GPU)\n    \n    model = Convnet(cols_info,groups)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'binary_crossentropy', auc_2])\n    checkpoint = ModelCheckpoint(model_name, monitor='val_auc_2', verbose=1, \n                                 save_best_only=True, mode='max', save_weights_only = True)\n    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=4, \n                                       verbose=1, mode='min', epsilon=0.0001)\n    earlystop = EarlyStopping(monitor='val_auc_2', mode='max', patience=5, verbose=1)\n    \n    if 1:\n        training_generator = DataGenerator(X=X_train.values,y=y_train,aug=1,groups=groups,\n                                           batch_size=BATCHSIZE,shuffle=True,cols_info=cols_info)\n        \n        validation_generator = DataGenerator(X=X_valid.values,y=y_valid,aug=0,groups=groups,\n                                             batch_size=BATCHSIZE,shuffle=False,cols_info=cols_info)\n        \n        history = model.fit_generator(generator=training_generator,\n                        validation_data=validation_generator,\n                        epochs=EPOCHS,  \n                        callbacks=[checkpoint, reduceLROnPlat, earlystop])\n    train_history = pd.DataFrame(history.history)\n    train_history.to_csv('train_profile_fold{}_{}.csv'.format(str(i),GPU), index=None)\n    \n    # load and predict\n    model.load_weights(model_name)\n    \n    #predict\n    X_valid = get_keras_groups_data(X_valid, cols_info,groups)\n    y_pred_keras = model.predict(X_valid).ravel()\n    \n    # AUC\n    valid_X['predict'].iloc[val_idx] = y_pred_keras\n    \n    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_valid, y_pred_keras)\n    auc_valid = roc_auc_score(y_valid, y_pred_keras)\n    val_aucs.append(auc_valid)\n    print('Fold %d auc %.4f'%(i,val_aucs[-1]))\n    prediction = model.predict(X_test)\n    result[\"fold{}\".format(str(i))] = prediction","973d0406":"for i in range(len(val_aucs)):\n    print('Fold_%d AUC: %.6f' % (i+1, val_aucs[i]))","7a205737":"val_aucs","2b040e99":"# summary on results\nauc_mean = np.mean(val_aucs)\nauc_std = np.std(val_aucs)\nauc_all = roc_auc_score(valid_X.target, valid_X.predict)\nprint('%d-fold auc mean: %.9f, std: %.9f. All auc: %6f.' % (n_folds, auc_mean, auc_std, auc_all))","df8a3500":"y_all = result.values[:, 1:]\nresult['target'] = np.mean(y_all, axis = 1)\nto_submit = result[['ID_code', 'target']]\nto_submit.to_csv('NN_submission_{}.csv'.format(GPU), index=None)\nresult.to_csv('NN_all_prediction_{}.csv'.format(GPU), index=None)\nvalid_X['ID_code'] = train_df['ID_code']\nvalid_X = valid_X[['ID_code', 'target', 'predict']].to_csv('NN_oof_{}.csv'.format(GPU), index=None)","ec70aff7":"** Somehow it is very slow running on kernel please change the values when running offline.**"}}