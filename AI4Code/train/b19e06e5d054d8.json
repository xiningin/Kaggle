{"cell_type":{"c9df1c2a":"code","acb0866b":"code","5b867874":"code","b01fa9c0":"code","92d44f56":"code","cbf7e86a":"code","28f4bcd1":"code","0a19f980":"code","f2822402":"code","cc8da957":"code","08639b77":"code","db493b6d":"code","d93259cb":"code","1dd7aea1":"code","a9a14c9c":"code","880fc6a1":"code","c7e82603":"code","91bbcef4":"code","cc551209":"code","f470869f":"code","f40a82a7":"code","f6f02faf":"code","bb38d62b":"code","c914351d":"code","a108d53d":"code","d7efcced":"code","5f08a4b0":"code","6352d538":"markdown","3d7a910b":"markdown","a98d90b1":"markdown","5e1583f1":"markdown","ba21ac54":"markdown","a4cc7600":"markdown","0a4e92e4":"markdown","d10d1912":"markdown","3c13fda0":"markdown","d42d088f":"markdown","5a76be4c":"markdown","97e2f511":"markdown","7b2ce004":"markdown","9b5b9497":"markdown","48f8de4e":"markdown","4cfced99":"markdown","7b85061a":"markdown","57e11cc7":"markdown","2f22c14f":"markdown","667f41a6":"markdown","f0f0b5ce":"markdown","a926fdc7":"markdown"},"source":{"c9df1c2a":"import os\nimport glob\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\nimport random\nfrom tqdm.notebook import tqdm\nimport pydicom # Handle MRI images\n\nimport cv2  # OpenCV - https:\/\/docs.opencv.org\/master\/d6\/d00\/tutorial_py_root.html\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.metrics import roc_auc_score\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import layers\n","acb0866b":"data_dir = Path('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/')\n\nmri_types = [\"FLAIR\", \"T1w\", \"T2w\", \"T1wCE\"]\nexcluded_images = [109, 123, 709] # Bad images\n\nSEED_LENGTH = 10 # Number of seeds to use for the model ensemble","5b867874":"train_df = pd.read_csv(data_dir \/ \"train_labels.csv\",\n#                        index='id',\n#                       nrows=100000\n                      )\ntest_df = pd.read_csv(data_dir \/ \"sample_submission.csv\")\nsample_submission = pd.read_csv(data_dir \/ \"sample_submission.csv\")\n\ntrain_df = train_df[~train_df.BraTS21ID.isin(excluded_images)].reset_index()\n\nprint(f\"train data: Rows={train_df.shape[0]}, Columns={train_df.shape[1]}\")\n# print(f\"test data : Rows={test_df.shape[0]}, Columns={test_df.shape[1]}\")","b01fa9c0":"def create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state=42)\n    for f, (t, v) in enumerate(kf.split(X=data)):\n        data.loc[v, \"kfold\"] = f\n    return data\n","92d44f56":"train_df_kf = create_folds(train_df, 5)","cbf7e86a":"train_df_kf.head()","28f4bcd1":"def load_dicom(path, size = 224):\n    ''' \n    Reads a DICOM image, standardizes so that the pixel values are between 0 and 1, then rescales to 0 and 255\n    \n    Not super sure if this kind of scaling is appropriate, but everyone seems to do it. \n    '''\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    # transform data into black and white scale \/ grayscale\n#     data = data - np.min(data)\n    if np.max(data) != 0:\n        data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return cv2.resize(data, (size, size))","0a19f980":"def get_all_image_paths(brats21id, image_type, folder='train'): \n    '''\n    Returns an arry of all the images of a particular type for a particular patient ID\n    '''\n    assert(image_type in mri_types)\n    \n    patient_path = os.path.join(\n        \"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/%s\/\" % folder, \n        str(brats21id).zfill(5),\n    )\n\n    paths = sorted(\n        glob.glob(os.path.join(patient_path, image_type, \"*\")), \n        key=lambda x: int(x[:-4].split(\"-\")[-1]),\n    )\n    \n    num_images = len(paths)\n    \n    start = int(num_images * 0.25)\n    end = int(num_images * 0.75)\n\n    interval = 3\n    \n    if num_images < 10: \n        interval = 1\n    \n    return np.array(paths[start:end:interval])\n\ndef get_all_images(brats21id, image_type, folder='train', size=225):\n    return [load_dicom(path, size) for path in get_all_image_paths(brats21id, image_type, folder)]","f2822402":"def get_all_data_for_train(image_type, image_size=32):\n    global train_df\n    \n    X = []\n    y = []\n    train_ids = []\n\n    for i in tqdm(train_df.index):\n        x = train_df.loc[i]\n        images = get_all_images(int(x['BraTS21ID']), image_type, 'train', image_size)\n        label = x['MGMT_value']\n\n        X += images\n        y += [label] * len(images)\n        train_ids += [int(x['BraTS21ID'])] * len(images)\n        assert(len(X) == len(y))\n    return np.array(X), np.array(y), np.array(train_ids)","cc8da957":"def get_all_data_for_test(image_type, image_size=32):\n    global test_df\n    \n    X = []\n    test_ids = []\n\n    for i in tqdm(test_df.index):\n        x = test_df.loc[i]\n        images = get_all_images(int(x['BraTS21ID']), image_type, 'test', image_size)\n        X += images\n        test_ids += [int(x['BraTS21ID'])] * len(images)\n\n    return np.array(X), np.array(test_ids)","08639b77":"X, y, trainidt = get_all_data_for_train('T1wCE', image_size=32)\nX_test, testidt = get_all_data_for_test('T1wCE', image_size=32)","db493b6d":"X.shape, y.shape, trainidt.shape","d93259cb":"X_train, X_valid, y_train, y_valid, trainidt_train, trainidt_valid = train_test_split(X, y, trainidt, test_size=0.2, random_state=42)","1dd7aea1":"df = train_df","a9a14c9c":"for fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n#     print(xtrain)\n#     print(f\"xvalid {xvalid}\")\n#     xtest = df_test.copy()\n\n#     ytrain = xtrain.target\n#     yvalid = xvalid.target\n       \n  \n#     model = XGBRegressor(random_state=42, tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\")\n#     model.fit(xtrain, ytrain)\n#     preds_valid = model.predict(xvalid)\n#     test_preds = model.predict(xtest)\n#     final_predictions.append(test_preds)\n#     rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n#     print(fold, rmse)\n#     scores.append(rmse)\n\n# print(np.mean(scores), np.std(scores))","880fc6a1":"X_train = tf.expand_dims(X_train, axis=-1)\nX_valid = tf.expand_dims(X_valid, axis=-1)","c7e82603":"y_train = to_categorical(y_train)\ny_valid = to_categorical(y_valid)","91bbcef4":"# Define, train, and evaluate model\n# source: https:\/\/keras.io\/examples\/vision\/3D_image_classification\/\ndef get_model01(width=128, height=128, depth=64, name='3dcnn'):\n    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n\n    inputs = tf.keras.Input((width, height, depth, 1))\n\n    x = tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n    x = tf.keras.layers.MaxPool3D(pool_size=2)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n    x = tf.keras.layers.MaxPool3D(pool_size=2)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n    x = tf.keras.layers.MaxPool3D(pool_size=2)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n    x = tf.keras.layers.MaxPool3D(pool_size=2)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.GlobalAveragePooling3D()(x)\n    x = tf.keras.layers.Dense(units=512, activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(0.3)(x)\n\n    outputs = tf.keras.layers.Dense(units=1, activation=\"sigmoid\")(x)\n\n    # Define the model.\n    model = tf.keras.Model(inputs, outputs, name=name)\n    \n    # Compile model.\n    initial_learning_rate = 0.0001\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n    )\n    model.compile(\n        loss=\"binary_crossentropy\",\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n        metrics=[\"acc\"],\n    )\n    \n    return model\n\n","cc551209":"def get_model02(seed=42, activation=\"relu\"):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n\n    inpt = keras.Input(shape=X_train.shape[1:])\n\n    h = keras.layers.experimental.preprocessing.Rescaling(1.0 \/ 255)(inpt)\n\n    h = keras.layers.Conv2D(64, kernel_size=(4, 4), activation=activation, name=\"Conv_1\")(h)\n    h = keras.layers.MaxPool2D(pool_size=(2, 2))(h)\n\n    h = keras.layers.Conv2D(32, kernel_size=(2, 2), activation=activation, name=\"Conv_2\")(h)\n    h = keras.layers.MaxPool2D(pool_size=(1, 1))(h)\n\n    h = keras.layers.Dropout(0.1)(h)\n\n    h = keras.layers.Flatten()(h)\n    h = keras.layers.Dense(32, activation=activation)(h)\n\n    output = keras.layers.Dense(2, activation=\"softmax\")(h)\n\n    model = keras.Model(inpt, output)\n    \n    initial_learning_rate = 0.0001\n    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n    )\n    model.compile(\n        loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=lr_schedule), metrics=[tf.keras.metrics.AUC()]\n    )\n    return model","f470869f":"def get_seed_list(low=0, high=1000, length=5):\n    np.random.seed(42)\n    return np.random.randint(low=low, high=high, size=length)","f40a82a7":"seed_list = get_seed_list(length=SEED_LENGTH)\nseed_list","f6f02faf":"# Define early stopping callback.\nearly_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode='max', verbose=1, patience=3)\nfor idx, seed in enumerate(seed_list):\n    tf.keras.backend.clear_session()\n    # Set up Model Checkpoint\n\n    checkpoint_filepath = f\"best_model_{seed}.h5\"\n\n    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n        filepath=checkpoint_filepath,\n        save_weights_only=False,\n        monitor=\"val_auc\",\n        mode=\"max\",\n        save_best_only=True,\n        save_freq=\"epoch\",\n        verbose=1,\n    )\n\n    model = get_model02(seed=seed)\n\n    history = model.fit(x=X_train, y = y_train, epochs=200, callbacks=[checkpoint_cb, early_stopping_cb], validation_data= (X_valid, y_valid))","bb38d62b":"best_models = []\nfor seed in seed_list:\n    best_models.append(tf.keras.models.load_model(filepath=f\"best_model_{seed}.h5\"))","c914351d":"for idx, model in enumerate(best_models):\n    y_pred = model.predict(X_valid)\n\n    pred = np.argmax(y_pred, axis=1)\n\n    result = pd.DataFrame(trainidt_valid)\n    result[1] = pred\n\n    result.columns = [\"BraTS21ID\", \"MGMT_value\"]\n    result2 = result.groupby(\"BraTS21ID\", as_index=False).mean()\n\n    result2 = result2.merge(train_df, on=\"BraTS21ID\")\n    auc = roc_auc_score(\n        result2.MGMT_value_y,\n        result2.MGMT_value_x,\n    )\n    print(f\"Validation AUC of model {idx+1} = {auc}\")","a108d53d":"y_pred = best_models[0].predict(X_test)\n\npred = np.argmax(y_pred, axis=1) #\nresult = pd.DataFrame(testidt)\n\nresult[1] = pred\npred","d7efcced":"result = pd.DataFrame(testidt)\nfor idx, model in enumerate(best_models):\n    y_pred = model.predict(X_test)\n\n    pred = np.argmax(y_pred, axis=1) #\n\n    result[idx+1] = pred\nresult","5f08a4b0":"result.columns=['BraTS21ID'] + [f\"MGMT_value_{idx+1}\" for idx, _ in enumerate(best_models)]\n# for idx, _ in enumerate(best_models):\n\nresult2 = result.groupby('BraTS21ID',as_index=False).mean()\nresult2['BraTS21ID'] = sample_submission['BraTS21ID']\n\n# Rounding...\nsample_submission['MGMT_value'] = result2[[f\"MGMT_value_{idx+1}\" for idx, _ in enumerate(best_models)]].mean(axis=1)\n# sample_submission['MGMT_value'] = sample_submission['MGMT_value'].apply(lambda x:round(x*10)\/10)\nsample_submission.to_csv('submission.csv',index=False)\nsample_submission","6352d538":"# Predictions on Validation Set","3d7a910b":"# Submission File","a98d90b1":"# Load Images We Will Need","5e1583f1":"## Model from: https:\/\/www.kaggle.com\/evanyao27\/team-9-second-week\/notebook\n\n- Validation AUC=0.9148664856146349","ba21ac54":"## Model from:  https:\/\/www.kaggle.com\/ohbewise\/dataset-to-model-with-tensorflow","a4cc7600":"# Load Our Best Models","0a4e92e4":"# Train\/Validation Split","d10d1912":"# Configuration, Constants, Setup","3c13fda0":"K-Fold Sources:\n- https:\/\/www.kaggle.com\/abhishek\/same-old-creating-folds\n- https:\/\/www.kaggle.com\/abhishek\/30-days-create-folds","d42d088f":"# Utility Functions","5a76be4c":"### Note that rerunning the cell below will change val_acc to val_acc_N and the model will not be saved.","97e2f511":"# Load Datasets","7b2ce004":"## Add dimension","9b5b9497":"### Set a couple of seeds for ensemble","48f8de4e":"# Tensorflow Models","4cfced99":"# Load Libraries","7b85061a":"# Versions \n- V6: Starting to add Cross Validation","57e11cc7":"# Predictions on the Test Set","2f22c14f":"I'm enhancing this notebook bto use ensemble learning with random seeds.\n\nThe idea is,to generate different models by setting the random seed.  Many people use random=42, for example. Using different values will give slightly different results.\n\n5 produced better results\n10 produced worse results.\n\nI will try other variations after first implementing CV.\n\n***\nOriginal author:\n\nI'm reading through several existing notebooks and trying to distill down the information into a new notebook to help me understand the project.  All help appreciated!\n\n# References\n\n- [Advanced EDA - Brain Tumor Data](https:\/\/www.kaggle.com\/smoschou55\/advanced-eda-brain-tumor-data)\n- [Team 9 Second Week](https:\/\/www.kaggle.com\/evanyao27\/team-9-second-week)\n  - The only model that is working. get_model02()\n- [Dataset to Model with Tensorflow](https:\/\/www.kaggle.com\/ohbewise\/dataset-to-model-with-tensorflow)\n- [Brain Tumer Train Class Flair](https:\/\/www.kaggle.com\/lucamtb\/brain-tumer-train-class-flair)\n  - Uses TPU\n  - Generates a Tensorflow model: Brain_flair_model_effect_3e-05_0.0001.h5\n- [Brain Tumor very basic inference](https:\/\/www.kaggle.com\/lucamtb\/brain-tumor-very-basice-inference)\n  - Uses the above mentioned model: Brain_flair_model_effect_3e-05_0.0001.h5\n  - Add this Kaggle Dataset: https:\/\/www.kaggle.com\/lucamtb\/effect0-brain","667f41a6":"## Set up Model Checkpoint","f0f0b5ce":"### There's a version that converts into grayscale: \n\n- https:\/\/www.kaggle.com\/smoschou55\/advanced-eda-brain-tumor-data\n","a926fdc7":"## One-hot encode labels"}}