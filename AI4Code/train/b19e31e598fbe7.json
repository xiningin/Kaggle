{"cell_type":{"156dcc35":"code","a6b6f960":"code","e495d00a":"code","83872f48":"code","0803cc2a":"code","332d78b7":"code","f4ddfbc9":"code","baf10180":"code","409c1510":"code","43d18ac4":"code","4b65a5c7":"code","1dca896a":"code","558e203c":"code","cde37a17":"code","f0f209d5":"code","2ad6d856":"code","9427c2b1":"code","7f1831ac":"code","925cd8df":"code","1a41bf20":"code","7e4599af":"code","4970a3e0":"code","26143697":"code","81c6a162":"code","5ca54939":"code","35406ddc":"code","1373dcd0":"code","0ca51551":"code","e4809c47":"code","8b9a918a":"code","63a472ed":"code","50f3f9c6":"code","f55d2477":"code","3f923cac":"code","861ec946":"code","2837daba":"code","0ce43c37":"code","2879bc37":"code","5aa3f84b":"code","902a2312":"code","654bb985":"code","5f7a1eca":"markdown","03d05000":"markdown","8057f472":"markdown","7fbf193c":"markdown","66b026a0":"markdown","3b009a30":"markdown","a8a31b55":"markdown","9b2d2284":"markdown","0667a8d6":"markdown","c414e40d":"markdown","5a92dace":"markdown","537811d8":"markdown","78656adc":"markdown","ccca2b64":"markdown","284762e0":"markdown","2e338a15":"markdown","1a3ab96a":"markdown","f8e0e06e":"markdown","d3400190":"markdown","10554307":"markdown","ac0dcc9e":"markdown","324faa9c":"markdown","17b9460f":"markdown","5636c305":"markdown","21f792c6":"markdown","97a64f96":"markdown","f6b3430a":"markdown","5f5b5e36":"markdown","8dcd4165":"markdown","5b8c85ac":"markdown","87565edf":"markdown","f024da0f":"markdown"},"source":{"156dcc35":"import os\nimport numpy as np \nimport pandas as pd \n\n# For data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\n%matplotlib inline\n\n# For data modeling & prediction\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import preprocessing\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.linear_model import BayesianRidge, LinearRegression\nimport xgboost as xgb\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn.model_selection import cross_validate, train_test_split\nfrom statistics import mean\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") ","a6b6f960":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e495d00a":"df_train = pd.read_csv(\"\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip\", compression='zip')\ndf_features = pd.read_csv(\"\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip\", compression='zip')\ndf_test = pd.read_csv(\"\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip\", compression='zip')\ndf_stores = pd.read_csv(\"\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv\")","83872f48":"df_train.tail(3)","0803cc2a":"df_test.tail(3)","332d78b7":"df_stores.tail(3)","f4ddfbc9":"df_features.head(3)","baf10180":"df_train = pd.merge(df_train,df_features, on = ['Store','Date','IsHoliday'],how='inner')\ndf_train = pd.merge(df_train,df_stores, on= 'Store',how='inner')\ndf_test = pd.merge(df_test,df_features, on = ['Store','Date','IsHoliday'],how='inner')\ndf_test = pd.merge(df_test,df_stores, on= 'Store',how='inner')","409c1510":"\"train\", df_train.isna().mean(),\"---------------------\", ","43d18ac4":"def convert_dates(dataframe):\n    dataframe['Date'] = pd.to_datetime(dataframe['Date'])\n    dataframe['year'] = dataframe['Date'].dt.year\n    dataframe['week'] = dataframe.Date.dt.week \n    \n    return dataframe\n\ndf_test = convert_dates(df_test)\ndf_train = convert_dates(df_train)","4b65a5c7":"to_categorical = ['Store', 'Dept', 'IsHoliday', 'Type', 'year', 'week']\nfor column in to_categorical:\n    df_train[column] = df_train[column].astype('category')\n    \ndf_train.dtypes","1dca896a":"df_train[['Dept', 'Store', 'week', 'year', 'IsHoliday']] = df_train[['Dept', 'Store', 'week', 'year', 'IsHoliday']].astype('int')\n\nplt.figure(figsize=(18,12))\ncorr = df_train.corr()\nnp.fill_diagonal(corr.values, np.nan)\n\nsns.heatmap(corr, annot=True, fmt='.2f')","558e203c":"def boxplot(column, x_size=15, y_size=10):\n    fig = plt.figure(figsize=(x_size,y_size))\n    sns.boxplot(y=df_train.Weekly_Sales, x=df_train[column])\n    plt.ylabel('Weekly_Sales')\n    plt.xlabel(column)\n","cde37a17":"boxplot('Store')","f0f209d5":"boxplot('Dept', x_size=25)","2ad6d856":"boxplot('week')","9427c2b1":"boxplot('year')","7f1831ac":"boxplot('Type')","925cd8df":"boxplot('IsHoliday')","1a41bf20":"def correlation(column):\n    print(\"----------------------------Column name: \"+column+\"----------------------------\")\n    print(\"Correlation: \" + str(df_train['Weekly_Sales'].corr(df_train[column])))\n    print(\"\\n\")","7e4599af":"correlation(\"CPI\")\ncorrelation(\"Unemployment\")\ncorrelation(\"Temperature\")\ncorrelation(\"Size\")\ncorrelation(\"Fuel_Price\")\ncorrelation(\"Unemployment\")\ncorrelation(\"MarkDown1\")\ncorrelation(\"MarkDown2\")\ncorrelation(\"MarkDown3\")\ncorrelation(\"MarkDown4\")\ncorrelation(\"MarkDown5\")","4970a3e0":"df_train.groupby('Store')['Size'].nunique()","26143697":"df_train = pd.get_dummies(df_train, columns=['Type'])\ndf_train.columns","81c6a162":"df_test = pd.get_dummies(df_test, columns=['Type'])\ndf_test.columns","5ca54939":"df_train.columns","35406ddc":"df_train.dtypes","1373dcd0":"mean_sales = df_train.groupby([\"Store\", \"Dept\", \"week\"], as_index=False).agg({\"Weekly_Sales\": \"mean\"})\ndf_val = df_test.merge(mean_sales, on=['Store', 'Dept', 'week'], how='left')\nsample_submission = pd.read_csv(\"\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip\")\n\ntest_ids = df_test.Store.astype(str) + '_' + df_test.Dept.astype(str) + '_' + df_test.Date.astype(str)\nsample_submission['Id'] = test_ids.values\nsample_submission[\"Weekly_Sales\"] = df_val[\"Weekly_Sales\"]\n\n# apparently there are some missing values. I will fill the NaN values with 0 (I know I miss some score :( ).\nsample_submission = sample_submission.fillna(0)\nsample_submission.to_csv('submission_simple_mean.csv',index=False)","0ca51551":"kfold = KFold(n_splits=5, random_state=35)\n\nx = df_train.loc[:, df_train.columns != 'Weekly_Sales']\nx = x.drop(['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4','MarkDown5', 'CPI', \n            'Unemployment', 'Size', 'IsHoliday', 'Type_A', 'Type_B', 'Type_C', 'year', 'Date'], axis=1)\n\ny = df_train.loc[:, df_train.columns == 'Weekly_Sales']\n\nmodels = {\n    \n    'xgboost' : xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1, max_depth = 5, alpha = 10, n_estimators = 10),\n    'Bayesian' : BayesianRidge(),\n    'LinearRegression': LinearRegression(),\n    'DecisionTreeRegressor': DecisionTreeRegressor(random_state=1),\n    'AdaBoostRegressor' : AdaBoostRegressor(n_estimators=50, learning_rate=.1, loss='square'),\n    'ExtraTreesRegressor': ExtraTreesRegressor(n_estimators=50, max_features='auto', random_state=35),\n    'RandomForestRegressor': RandomForestRegressor(n_estimators=50, random_state=35),\n}\n\nfor model_name, model in models.items():\n    results = cross_validate(model, x,y , cv=kfold, scoring=['neg_mean_absolute_error'], return_estimator=False)\n    print(model_name, mean(results['test_neg_mean_absolute_error']), mean(results['fit_time']), mean(results['score_time']))","e4809c47":"x = df_train.loc[:, df_train.columns != 'Weekly_Sales']\nx = x.drop(['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4','MarkDown5', 'CPI', \n            'Unemployment', 'Date'], axis=1)\n\ny = df_train.loc[:, df_train.columns == 'Weekly_Sales']\n\nmodels = {\n    \n    'xgboost' : xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1, max_depth = 5, alpha = 10, n_estimators = 10),\n    'Bayesian' : BayesianRidge(),\n    'LinearRegression': LinearRegression(),\n    'DecisionTreeRegressor': DecisionTreeRegressor(random_state=1),\n    'AdaBoostRegressor' : AdaBoostRegressor(n_estimators=50, learning_rate=.1, loss='square'),\n    'ExtraTreesRegressor': ExtraTreesRegressor(n_estimators=50, max_features='auto', random_state=35),\n    'RandomForestRegressor': RandomForestRegressor(n_estimators=50, random_state=35),\n}\n\nres = {}\n\nfor model_name, model in models.items():\n    results = cross_validate(model, x,y , cv=kfold, scoring=['neg_mean_absolute_error'], return_estimator=False)\n    print(model_name, mean(results['test_neg_mean_absolute_error']), mean(results['fit_time']), mean(results['score_time']))","8b9a918a":"x = df_train.loc[:, df_train.columns != 'Weekly_Sales']\nx = x.drop(['Date'], axis=1)\nx = x.fillna(0)\ny = df_train.loc[:, df_train.columns == 'Weekly_Sales']\n\nmodels = {\n    'xgboost' : xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1, max_depth = 5, alpha = 10, n_estimators = 10),\n    'Bayesian' : BayesianRidge(),\n    'LinearRegression': LinearRegression(),\n    'DecisionTreeRegressor': DecisionTreeRegressor(random_state=1),\n    'AdaBoostRegressor' : AdaBoostRegressor(n_estimators=50, learning_rate=.1, loss='square'),\n    'ExtraTreesRegressor': ExtraTreesRegressor(n_estimators=50, max_features='auto', random_state=35),\n    'RandomForestRegressor': RandomForestRegressor(n_estimators=50, random_state=35),\n}\n\nres = {}\n\nfor model_name, model in models.items():\n    results = cross_validate(model, x,y , cv=kfold, scoring=['neg_mean_absolute_error'], return_estimator=False)\n    print(model_name, mean(results['test_neg_mean_absolute_error']), mean(results['fit_time']), mean(results['score_time']))","63a472ed":"x = df_train.loc[:, df_train.columns != 'Weekly_Sales']\nx = x.drop(['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4','MarkDown5', 'CPI', \n            'Unemployment', 'Date'], axis=1)\n\ny = df_train.loc[:, df_train.columns == 'Weekly_Sales']\n\nextratreeregressor = ExtraTreesRegressor(n_estimators=50, max_features='auto', random_state=35)\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=35)\nextratreeregressor.fit(X_train, y_train)\ny_pred = extratreeregressor.predict(X_test)\nmean_absolute_error(y_pred, y_test)","50f3f9c6":"x_val = df_test.drop(['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4','MarkDown5', 'CPI', \n            'Unemployment', 'Date'], axis=1)\n\ny_val = extratreeregressor.predict(x_val)","f55d2477":"test_ids = df_test.Store.astype(str) + '_' + df_test.Dept.astype(str) + '_' + df_test.Date.astype(str)\nsample_submission['Id'] = test_ids.values\nsample_submission[\"Weekly_Sales\"] = y_val\n\nsample_submission = sample_submission.fillna(0)\nsample_submission.to_csv('submission_extratreeregressor.csv',index=False)","3f923cac":"from sklearn.decomposition import PCA\n\nx = df_train.loc[:, df_train.columns != 'Weekly_Sales']\nx = x.drop(['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4','MarkDown5', 'CPI', \n            'Unemployment', 'Date'], axis=1)\nx = x.fillna(0)\ny = df_train.loc[:, df_train.columns == 'Weekly_Sales']\n\npca = PCA(n_components=5)\npca.fit(x)\npca_features = pca.transform(x)\n\ncolumns = ['pca_%i' % i for i in range(5)]\nx = pd.DataFrame(pca_features, columns=columns, index=x.index)","861ec946":"extratreeregressor_pca = ExtraTreesRegressor(n_estimators=50, max_features='auto', random_state=35)\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=35)\nextratreeregressor_pca.fit(X_train, y_train)\ny_pred = extratreeregressor_pca.predict(X_test)\nmean_absolute_error(y_pred, y_test)","2837daba":"x_val = df_test.drop(['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4','MarkDown5', 'CPI', \n            'Unemployment', 'Date'], axis=1)\npca_features_val = pca.transform(x_val)\ncolumns = ['pca_%i' % i for i in range(5)]\n\nx_val = pd.DataFrame(pca_features_val, columns=columns, index=x_val.index)\n\ny_val = extratreeregressor_pca.predict(x_val)","0ce43c37":"test_ids = df_test.Store.astype(str) + '_' + df_test.Dept.astype(str) + '_' + df_test.Date.astype(str)\nsample_submission['Id'] = test_ids.values\nsample_submission[\"Weekly_Sales\"] = y_val\n\nsample_submission = sample_submission.fillna(0)\nsample_submission.to_csv('submission_extratreeregressor_pca.csv',index=False)","2879bc37":"x = df_train.loc[:, df_train.columns != 'Weekly_Sales']\nx = pd.concat([x, pd.DataFrame(pca_features)], axis=1)\n\nx = x.drop(['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4','MarkDown5', 'CPI', \n            'Unemployment', 'Date'], axis=1)\nx = x.fillna(0)\ny = df_train.loc[:, df_train.columns == 'Weekly_Sales']","5aa3f84b":"extratreeregressor_pca_allfeatures = ExtraTreesRegressor(n_estimators=50, max_features='auto', random_state=35)\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=35)\nextratreeregressor_pca_allfeatures.fit(X_train, y_train)\ny_pred = extratreeregressor_pca_allfeatures.predict(X_test)\nmean_absolute_error(y_pred, y_test)","902a2312":"x_val = df_test.drop(['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4','MarkDown5', 'CPI', \n            'Unemployment', 'Date'], axis=1)\npca_features_val = pca.transform(x_val)\nx_val = pd.concat([x_val, pd.DataFrame(pca_features_val)], axis=1)\n\ny_val = extratreeregressor_pca_allfeatures.predict(x_val)","654bb985":"test_ids = df_test.Store.astype(str) + '_' + df_test.Dept.astype(str) + '_' + df_test.Date.astype(str)\nsample_submission['Id'] = test_ids.values\nsample_submission[\"Weekly_Sales\"] = y_val\n\nsample_submission = sample_submission.fillna(0)\nsample_submission.to_csv('submission_extratreeregressor_pca_allfeatures.csv',index=False)","5f7a1eca":"Got 3153.03549 on private score and 3046.07420 on public, which improves my position to closer to the 210th.\nWhat about using PCA to generate some features, and then training using the Extra Tree Regressor?","03d05000":"This submission gave me a 3337.81 on public score and 3460.96 on the private score. Considering I just calculated the mean value on the training set using 3 of the features, it is huge. If this challenge was on, I would be around the 300th position, which I consider very good.","8057f472":"### More training after first prediction\n\nLet's see some machine learning in action. I will perform 3 different tests here: One using the same features used in the mean submission, another using the features I considered more important according to my analysis and a final one using all the features.\n\nIn this comparison I will consider the Mean Absolute Error (MAE) because it is very close to the metric used in this challenge. I will print the time used for training the model (in seconds) too, this could be important if time is short.","7fbf193c":"Another improvement, got 3030.99599 on the private score and 2883.80360 on the public score, placing me under the 180th position.\nAnd if we combined the original features with the 5 PCA generated features? Could that work?","66b026a0":"Type is not clearly described in the challenge description. Here it is possible to see that Type C sells less than types A and B. Type B has more outliers and Type A has the higher mean because of the size of the second and third quartiles. It looks to be an important feature.","3b009a30":"Points to highligth:\n\n- Again, some Depts apparently have negative values (more visible in Depts 6, 45 and 47).\n- Depts 7 and 72 have lots of outliers too.","a8a31b55":"There are some points to highlight here:\n\n- All stores concentrate most of the selling in the third quartile, with several outliers.\n- Apparently store 28 has some values above 0. It is not clear for other stores but apparently it is possible to have negative sellings. It would be the case to ask for the store manager if the information is correct before continues, at least for these negative values. As it is not possible to do it here, I will assume these values are correct.","9b2d2284":"## Machine Learning\n\nI will start with a simple baseline: I will calculate the average week sales according to each week, dept and store. My score will be calculated using this mean historical value.","0667a8d6":"MarkDowns are not strongly correlated to Weekly_Sales. I am highlighting these columns because there is a lot of missing data. It may be a good idea to forget these columns.\nLet's check each variable. Starting with the discrete ones. I will use boxplot to check each quartil difference.","c414e40d":"Let's load and take a little look at the data","5a92dace":"As it is possible to see, there are some missing (NaN) values, at least in the \"Features\" dataframe. Let's take a look at all data frames to search more missing information.","537811d8":"## Conclusion\n\nIn this exercise I tried to generate the best possible prediction according to the data provided by Wallmart.\nI demonstrated how I selected the features I considered more important, according to correlation and other features.\n\nIn this specific case, using just simple math was enough to create a really good baseline (way better than the baseline provided by the challenge). It shows that, in some cases, it is possible to achieve a reasonably good result without using any machine learning technique, just understanding how each feature is relevant for a given problem.\n\nHowever, this is not the case. By comparing results of 7 different machine learning supervised methods, it was possible to significantly improve results. When combined with PCA, the model could still improve a little.\n\n### Further analysis\n\nI am not sure if this challenge really reflects the real world, at least in the description. It would be nice to check other holidays or important selling dates like Easter, Mother's Day and Vallentine's Day, there might be some more hidden information which could help improve our results.\n\nAnother consideration to make is to isolate each store to create better models. Perhaps the data provided by just one store is not enough to generate a good predictor (because of the lack of information in this case) but maybe isolate stores in the same region (one predictor for each city for example) could be a good idea.\n\n### Improving results?\n\nI just used K-fold for deciding which model I would use to generate my final results. It is possible I would achieve better results if I used an ensemble of Extra Tree Regressors.\n\nOther point here is to search for the best parameters for the chosen model. There may be other settings that could improve this results, both on the Extra Tree Regressor and PCA. \n\nOne last thing to try could be the use of Time Series models, like recurrent neural networks (RNN). As it is clear in the description, an analysis over the time could really generate a good model.\n\n\n\nBest Regards,\nClaudio Santos","78656adc":"So it is correct, one size for each store. It may be the case of use only one of these two variables.","ccca2b64":"### Analyse the data\n\nCheck what features are relevant or not for a good prediction.\n\n### Evaluate models\n\nUsing different machine learning models I will choose the best according to the evaluation I will demonstrate.\n\n### Conclusions\n\nA brief conclusion of this work.","284762e0":"Looks better. Let's see the correlation. ","2e338a15":"Something I decided to check: according to the description, each store has one size, is it correct? The following code prints it\n","1a3ab96a":"In each column:\n\nCPI: low correlation, looks not important\n\nTemperature: low correlation, looks not important\n\nUmemployment: low correlation, apparently no importance\n\nTemperature: low correlation, apparently no importance\n\nSize: there is some correlation, maybe with some importance\n\nFuel_Price: low correlation, looks not important\n\nMarkDown-1\/5: low correlation in all cases, apparently no importance","f8e0e06e":"Not a lot of difference in the sellings here, however, this field is important because, as described in the competion, training data is related from 2010-02-05 until 2012-11-01. It means that the lack of outliers in the year 2012 may be related to the lack of information about thanksgiving and christmas, which it has lots of outliers as seen in previous analysis.","d3400190":"# Claudio Santos\n\nThis notebook is part of the hiring process on Ze Delivery.\n\nIf you need some help, please consider asking in the comments or try to contact me.\n\n[Github](http:\/\/github.com\/cfsantos) | [Linkedin](https:\/\/www.linkedin.com\/in\/cfsantos85\/)","10554307":"Holidays definetly influences the Weekly_Sales because of the quantity of outliers and higher mean.\nNow checking the continue columns.","ac0dcc9e":"Last case: using all features","324faa9c":"It appears \"Train\" and \"Test\" dataframes connect to \"Features\" by using Store, Date and IsHoliday columns. \"Stores\" dataframe connects to \"Train\" and \"Test\" by the Store column. Merging all content to make it easier to deal.","17b9460f":"Now using the information I considered more important.","5636c305":"# Pre-processing\n\nSomething bothering me is that this work is evaluated by week but Date column gives the information about some days, apparently every 7 days. I understand this analysis should be done using weeks instead of days, so I will change some information to make more sense of the data.","21f792c6":"Most of the columns are numerical, however, some of them (Store, Dept, IsHoliday, Type, year and week) I rather work as category.","97a64f96":"As described in the challenge, holidays sellings are different. It is clear that weeks closer to them (5, 47 and 51\/52) sell more than other weeks.","f6b3430a":"So the best MAE in the kfold split was using Extra Tree Regressor using the features I considered more important in the analysis. \nI will train one and make a prediction.","5f5b5e36":"First, just using Store, Dept and Wee information","8dcd4165":"From this analysis, I will make two different predictions: one using all features and other just using the ones I considered relevant for this case (Store, Dept, IsHoliday, Size, year, week and Type).","5b8c85ac":"Only \"features\" has missing values, As described in the challenge, Markdown-1\/5 are only available from a given date. Over 50% of this information is missing and around 7% of CPI and Unemployment is missing too.\n\nFirst, I will take a look at the importance of each feature. If necessary, I will deal with the missing information in a future step.","87565edf":"An improvement of 3008.95940 on the private score but a small decline on the public (2884.29135), still I consider as an improvement.","f024da0f":"In this challenge, Wallmart desires to predict sales from stores according to some features, like size, department and so on.\n\nIn this notebook , I will:"}}