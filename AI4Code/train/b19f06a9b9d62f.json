{"cell_type":{"f03d224b":"code","fe9ac8b8":"code","9ea9e912":"code","12b05c98":"code","a1263a7b":"code","e1b36b63":"code","be0cac25":"code","283ae554":"code","e85b09d2":"code","fd724aee":"code","a7c82f23":"code","63911fc7":"code","122a8462":"code","f21585d3":"code","9347ca7b":"code","2af285d0":"code","4fdf92f1":"code","7010270c":"code","dbf572cd":"code","ad1ec35f":"code","452cdb3c":"code","28de0730":"code","3d8baadc":"code","811aa9f5":"code","aab4ec66":"code","9cec3cde":"code","c4c7da0e":"code","4c276864":"code","2409d57a":"code","cf35696e":"markdown","bfa92412":"markdown","54cc422b":"markdown","3cd94ddf":"markdown","23131bb2":"markdown","b5d4f5a9":"markdown","b8edd414":"markdown","fa1d8101":"markdown","2ea1551b":"markdown","2bf42173":"markdown","75d040f7":"markdown","14c047ce":"markdown","7857524e":"markdown","07cd66e4":"markdown","d02c9cb4":"markdown"},"source":{"f03d224b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport random\nimport os","fe9ac8b8":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9ea9e912":"print(\"TF version: \", tf.__version__)\nif tf.__version__ < \"2.0.0\":\n    tf.enable_eager_execution()\n    print(\"Eager execution enabled.\")\nelse:\n    print(\"Eager execution enabled by default.\")\n\nif tf.test.gpu_device_name(): \n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\nelse:\n   print(\"Please install GPU version of TF\")","12b05c98":"def setseed(seed = 0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    \nSEED = 0\nsetseed(SEED)\n\nsetseed()","a1263a7b":"train = pd.read_csv('..\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip', sep = '\\t')\ntest = pd.read_csv('..\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip', sep = '\\t')","e1b36b63":"print(train.shape, test.shape)","be0cac25":"train.head()","283ae554":"test.head()","e85b09d2":"train.info()","fd724aee":"from tqdm import tqdm\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nimport re","a7c82f23":"def clean_sentences(df):\n    reviews = []\n    \n    for sent in tqdm(df['Phrase']):       \n        #remove non-alphabetic characters\n        review_text = re.sub(\"[^a-zA-Z]\",\" \", sent)\n        \n        #tokenize the sentences\n        words = word_tokenize(review_text.lower())\n        \n        #lemmatize each word to its lemma\n        lemmatizer = WordNetLemmatizer()\n        lemma_words = [lemmatizer.lemmatize(i) for i in words]\n        \n        reviews.append(lemma_words)\n    \n    return(reviews)","63911fc7":"%%time\ntrain_sentences = clean_sentences(train)\ntest_sentences = clean_sentences(test)\n\nprint(len(train_sentences))\nprint(len(test_sentences))","122a8462":"print(train['Phrase'][0])\nprint(' '.join(train_sentences[0]))","f21585d3":"from keras.utils import to_categorical\n\ntarget = train.Sentiment.values\ny_target = to_categorical(target)\n\n# number of numerical values exist in y_target's column\nnum_classes = y_target.shape[1]","9347ca7b":"print(num_classes)","2af285d0":"from sklearn.model_selection import train_test_split","4fdf92f1":"X_train, X_val, y_train, y_val = train_test_split(train_sentences,\n                                                  y_target,\n                                                  test_size = 0.2,\n                                                  stratify = y_target)","7010270c":"X_train[0]","dbf572cd":"unique_words = set()\nlen_max = 0\n\nfor sent in tqdm(X_train):\n    unique_words.update(sent)\n    if(len_max < len(sent)):\n        len_max = len(sent)\n\n# length of the list of unique_words \nprint('Number of vocabs: ', len(list(unique_words)))\nprint('Max length of text is: ', len_max)","ad1ec35f":"vocab_size = len(list(unique_words))\nembedding_dim = 300\nmax_length = len_max\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"","452cdb3c":"%%time\ntokenizer = Tokenizer(num_words = vocab_size,\n                      # filters = '#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                      oov_token = oov_tok,\n                      # lower = True,\n                      char_level = False)\n\ntokenizer.fit_on_texts(list(X_train))\n\n# Training\nX_train = tokenizer.texts_to_sequences(X_train)\nX_train = pad_sequences(X_train,\n                        maxlen = max_length,\n                        padding = padding_type,\n                        truncating = trunc_type)\n\n# Validation\nX_val = tokenizer.texts_to_sequences(X_val)\nX_val = pad_sequences(X_val,\n                      maxlen = max_length,\n                      padding = padding_type,\n                      truncating = trunc_type)\n\n# Testing\nX_test = tokenizer.texts_to_sequences(test_sentences)\nX_test = pad_sequences(X_test,\n                       maxlen = max_length,\n                       padding = padding_type,\n                       truncating = trunc_type)","28de0730":"print(\"X_training shape   : \",X_train.shape)\nprint(\"X_validation shape : \",X_val.shape)\nprint(\"X_testing shape    : \",X_test.shape)","3d8baadc":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout = 0.8, recurrent_dropout=0.8, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout = 0.5, recurrent_dropout=0.5, return_sequences=False)),\n    tf.keras.layers.Dense(128, activation = 'relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(num_classes, activation = 'softmax')\n])\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.summary()","811aa9f5":"from keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(min_delta = 0.001,\n                               mode = 'max',\n                               monitor = 'val_acc',\n                               patience = 2)\ncallback = [early_stopping]","aab4ec66":"%%time\n\nnum_epochs = 5\n\nhistory = model.fit(X_train,\n                    y_train,\n                    validation_data = (X_val, y_val),\n                    epochs = num_epochs,\n                    batch_size = 256,\n                    verbose = 1,\n                    callbacks = callback)","9cec3cde":"import matplotlib.pyplot as plt\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\n  \nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')","c4c7da0e":"test_id = test['PhraseId']","4c276864":"%%time\n\n# y_pred = model.predict_classes(X_test)\ny_pred = np.argmax(model.predict(X_test), axis=-1)","2409d57a":"submission = pd.DataFrame({'PhraseId': test_id, 'Sentiment': y_pred})\nsubmission.to_csv('movie_review_prediction_5EP_MLBDLSTM_submission.csv', index=False)\nsubmission.head()","cf35696e":"Turn label into OHE format","bfa92412":"Train a Sentiment Model","54cc422b":"Set Training & Validation set to 80\/20","3cd94ddf":"Take a look at the data","23131bb2":"Tokenize the dataset","b5d4f5a9":"Visualize the training graph","b8edd414":"Get vocab sizes and max length","fa1d8101":"Set random seed so we get consistent result when improveing our model","2ea1551b":"The sentiment labels are:\n\n* 0 - negative\n* 1 - somewhat negative\n* 2 - neutral\n* 3 - somewhat positive\n* 4 - positive","2bf42173":"Prepare for submission","75d040f7":"# Sentiment Analysis with Keras Tokenization & Embeddings + Multilayer Bidirectional LSTM","14c047ce":"Import required libs","7857524e":"Load dataset","07cd66e4":"**Text Preprocessing**\n\nNormalization:\n* converting numbers into words or removing numbers\n* expanding abbreviations\n* removing stop words\n* remove sparse terms and particular words (Stemming\/Lemmatization)\n\nAvailable in the tokenizer:\n* converting all letters to lower case\n* removing punctuations, accent marks and other diacritics\n* removing white spaces","d02c9cb4":"Ensure TF using GPU, and enable eager execution."}}