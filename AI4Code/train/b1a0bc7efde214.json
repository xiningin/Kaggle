{"cell_type":{"4c3a066c":"code","216fb297":"code","12f0ce54":"code","40873174":"code","25e6fa27":"code","19151683":"code","9302bd6d":"code","8f256579":"code","89e391c7":"code","e7e0bd38":"code","093086e5":"code","8b722019":"code","83ff6e18":"code","5a7db81e":"code","0c666fb2":"code","b993fa76":"code","18e1466e":"code","99b9ebdf":"code","009a27c3":"code","1c629042":"code","25a2c761":"code","cac30a76":"code","d678d41c":"code","b2374959":"code","1bd7fba2":"code","69ecfffc":"code","b987f125":"code","1fa2bacb":"code","d39be470":"code","5cc51066":"code","47b3836b":"markdown","ced74377":"markdown","6b9d22d6":"markdown"},"source":{"4c3a066c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","216fb297":"#Load the data\noriginal_train = pd.read_csv('\/kaggle\/input\/4611hw1data\/train.csv')\noriginal_test = pd.read_csv('\/kaggle\/input\/4611hw1data\/train.csv')\nsample_sub = pd.read_csv('\/kaggle\/input\/4611hw1data\/sample_submission.csv')\n\ntrain = original_train.copy()\ntest = original_test.copy()","12f0ce54":"pd.set_option(\"display.max_columns\", None)\ntrain","40873174":"#Check for missing values within the training data\ntrain.info()","25e6fa27":"#Double check the above printout\ntrain.isna().sum().max()","19151683":"train.describe()","9302bd6d":"print(\"Not bankrupt: \",train['Bankrupt'].value_counts()[0])\nprint(\"Bankrupt: \",train['Bankrupt'].value_counts()[1])","8f256579":"import matplotlib.pyplot as plt\n\ntrain.hist(figsize=(40,60))\nplt.show()","89e391c7":"#Check for outliers in training data\n#Define outliers as being outside of the IQR, since std can be\n#susceptible to outliers.\n#Economic data apparently rarely normally distributed, so use IQR\n\noutlier_info = []\noutlier_rows = []\n\nQ1 = train.quantile(0.25)\nQ3 = train.quantile(0.75)\nIQR = Q3-Q1\nlower = (Q1 - 1.5 * IQR)\nupper = (Q3 + 1.5 * IQR)\n\nfor col in range(2,len(train.columns)): #Don't include the bankrupt column\n    entry = []\n    outliers = []\n    \n    for row in range(0,len(train)):\n        if train.iloc[row,col] > upper[col] or train.iloc[row,col] < lower[col]:\n            outliers.append(train.iloc[row,col])\n            outlier_rows.append(row)\n\n    if len(outliers) != 0: #Choosing to fix only variables with 1% of data or more as outliers. \n        print(\"{0}: {1} Potential Outlier(s)\".format(train.columns[col],len(outliers)))\n        entry.append(col)\n        entry.append(outliers)\n        outlier_info.append(entry)        \n        \noutlier_rows = set(outlier_rows)","e7e0bd38":"#If contains outliers, describe and implement an\n#approach to handle them.\n\n#for row in outlier_rows:\n    #train = train[train.index != row] #lmao just drop the row\n#print('Done dropping')","093086e5":"train","8b722019":"train","83ff6e18":"#Build and train a decision tree on the training data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\nX = train.loc[:, ~train.columns.isin(['id', 'Bankrupt'])]\nY = train['Bankrupt']\n\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y,test_size=0.3, random_state=42)\n\ndt = DecisionTreeClassifier(max_depth = 4, random_state=1)\ntree_model = dt.fit(X_train, Y_train)\nbankrupt = dt.predict(X_val)","5a7db81e":"#Report best ROC AUC, F1 Score, and Accuracy\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\n\nprint(accuracy_score(Y_val,bankrupt))\nprint(f1_score(Y_val,bankrupt,average='weighted'))\n\ntree_auc = roc_auc_score(Y_val,bankrupt)\n\nprint(tree_auc)","0c666fb2":"from sklearn.model_selection import GridSearchCV\n\nparam_dict = {\n    \"criterion\":['gini','entropy'],\n    \"max_depth\":range(1,10),\n    \"min_samples_split\":range(1,10),\n    \"min_samples_leaf\":[0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2],\n    \"max_features\": [0.2,0.4,0.6,0.8]\n}\n\ngrid = GridSearchCV(dt,\n                   param_grid=param_dict,\n                   scoring='roc_auc',\n                   cv=10,\n                   verbose=1,\n                   n_jobs=-1)\n\ngrid.fit(X_train,Y_train)\n\nprint(grid.best_estimator_)\nprint(grid.best_score_) #Best cv accuracy\nprint(grid.best_estimator_.score(X_val,Y_val))","b993fa76":"pred = grid.predict_proba(X_val)[:,1]","18e1466e":"#Report best ROC AUC, F1 Score, and Accuracy\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\n\nprint(accuracy_score(Y_val,pred))\nprint(f1_score(Y_val,pred,average='weighted'))\n\ntree_auc = roc_auc_score(Y_val,pred)\n\nprint(tree_auc)","99b9ebdf":"#Train a random forest on the training data\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators = 250, max_depth = 4)\n\nforest_model = rfc.fit(X_train,Y_train)\nbankrupt2 = forest_model.predict(X_val)","009a27c3":"#Report ROC_AUC, F1, and Accuracy Scores\nfrom sklearn.model_selection import cross_val_score\n\nprint(accuracy_score(Y_val,bankrupt2))\nprint(f1_score(Y_val,bankrupt2,average='weighted'))\n\nforest_auc = roc_auc_score(Y_val,bankrupt2)\n\nprint(forest_auc)\n\nprint(cross_val_score(rfc, X_train, Y_train, cv=10))","1c629042":"#Select the best model you genereated and use that model\n#to predict the target vector for the test data\nbest_model = None\n\nif tree_auc > forest_auc:\n    best_model = tree_model\nelse:\n    best_model = forest_model\n    \npre_drop_test = test.copy()\ntest = test.drop('Bankrupt',axis=1)\ntest = test.drop('id',axis=1)\n","25a2c761":"test","cac30a76":"bankrupt_predictions = best_model.predict_proba(test)","d678d41c":"probas = best_model.predict_proba(test)\nclasses = best_model.classes_\n\nfor class_name, proba in zip(classes, probas):\n    print(f\"{class_name}: {proba}\")","b2374959":"bankrupt_predictions","1bd7fba2":"real_pred = []\n\nfor pred in bankrupt_predictions:\n    real_pred.append(pred[1])","69ecfffc":"len(bankrupt_predictions)","b987f125":"len(real_pred)","1fa2bacb":"# Package results into a new dataframe to turn in!\n\nprint(len(pre_drop_test['id']))\nprint(len(bankrupt_predictions))\n\n#prediction = pd.Series(bankrupt_predictions)\n\n#pred = pd.Series(bankrupt_predictions.reshape((bankrupt_predictions.shape[0],)))\n\nsubmission = pd.DataFrame({\n        \"id\": pre_drop_test['id'],\n        \"Bankrupt\": real_pred\n    })\n\nfrom datetime import datetime\n\nsub_name = 'submission-' + datetime.now().strftime(\"%H:%M:%S\") + '.csv'\n# + datetime.now().strftime(\"%d\/%m\/%Y-%H:%M:%S\") + '.csv'\n\nsubmission.to_csv(sub_name, index=False)","d39be470":"submission","5cc51066":"print('Done!')","47b3836b":"According to Chapter 1 of DataCamp's \"MACHINE LEARNING WITH TREE-BASED MODELS IN PYTHON\" course, classification trees don't require feature scaling. As such, no standardization nor normalization is required.","ced74377":"The data has no missing values, so no action required there.","6b9d22d6":"Joshua Nichols"}}