{"cell_type":{"7d09affd":"code","68775e0e":"code","bb5e46be":"code","b03172ca":"code","caed347a":"code","315f8d3b":"code","05b26798":"code","ff91544a":"code","c3c2c06c":"code","77217d08":"code","eda47bac":"code","7698a323":"code","3189a92d":"code","260706cb":"code","1cde84ee":"code","5bac7498":"code","dd31fb90":"code","cd8bf0e5":"code","a2225446":"code","52ce288e":"code","bcdc175d":"code","7c601eaa":"code","83a8b481":"code","bc6bc89c":"code","05a7f623":"code","df8804e3":"code","1b66e02c":"code","9ebfdf48":"code","fcfc379a":"code","588920a1":"code","c657f868":"code","3359e9f7":"code","6957b173":"code","0a594b54":"code","f3fe2a50":"code","d2d42714":"code","4675d9af":"code","d24a11d5":"code","2279d7a9":"code","e27aa98f":"code","87246700":"code","767d201b":"code","e0b806ce":"code","ab3c96d6":"code","185168df":"code","3489769b":"code","c34fa1f2":"code","21548383":"code","ba6695b7":"code","def1ff1a":"code","ea0136a1":"code","354559e2":"code","cd001b3b":"code","be2f700a":"code","c47bfb6f":"code","56f035f9":"code","09a08001":"code","e7c14d1e":"markdown","f18f54a5":"markdown","7d4d6773":"markdown","806b51c0":"markdown","75cd3581":"markdown","ebe1ed3d":"markdown","56fb4364":"markdown","0c280a1a":"markdown","b679a716":"markdown","715e2e20":"markdown","2d8d9f4e":"markdown","fdb69c19":"markdown","cb0eada9":"markdown","2491ea2e":"markdown","db6c26a9":"markdown","800f9be7":"markdown","130c5771":"markdown","3166d7bb":"markdown","a56f81d3":"markdown","de985088":"markdown","2e44ff9d":"markdown","ee9eee03":"markdown","b625d72a":"markdown","49af2f5c":"markdown","585749d7":"markdown","94488468":"markdown","1583a7b5":"markdown","468e4f13":"markdown"},"source":{"7d09affd":"!pip install -Uqq fastbook kaggle waterfallcharts treeinterpreter dtreeviz\nimport fastbook\nfastbook.setup_book()","68775e0e":"import seaborn as sns\nfrom fastbook import *\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG, clear_output\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 10","bb5e46be":"path = Path(\"..\/input\/ventilator-pressure-prediction\")","b03172ca":"df = pd.read_csv(path\/\"train.csv\", low_memory=False)\ndf_test = pd.read_csv(path\/\"test.csv\", low_memory=False)\ndf.columns","caed347a":"df.head()","315f8d3b":"df[\"breath_id\"].unique()","05b26798":"len(df)","ff91544a":"df.R.unique(), df.C.unique()","c3c2c06c":"# https:\/\/www.kaggle.com\/c\/ventilator-pressure-prediction\/discussion\/273974\ndf[\"u_in_cumsum\"] = df.u_in.groupby(df.breath_id).cumsum()\ndf_test[\"u_in_cumsum\"] = df_test.u_in.groupby(df_test.breath_id).cumsum()\ndf.head()","77217d08":"procs = [Categorify, FillMissing]","eda47bac":"np.where(df.time_step == 0)","7698a323":"np.count_nonzero(np.where(df.time_step == 0)[0] % 80)","3189a92d":"df[\"new_time_step\"] = (df.id - 1) % 80\ndf_test[\"new_time_step\"] = (df_test.id - 1) % 80\ndf.head()","260706cb":"df.new_time_step.unique()","1cde84ee":"df.head(82)","5bac7498":"df = df.drop(columns=\"id\")\ndf_test = df_test.drop(columns=\"id\")\ndf","dd31fb90":"df.isnull().values.any(), df.isnull().sum().sum()","cd8bf0e5":"# Split based on GroupKFold so the same group wound't appear on the same set, to imitate test set. \nfrom sklearn.model_selection import GroupKFold\n\ngroups = df.breath_id.to_numpy()\nX = df.drop(columns=[\"pressure\", \"breath_id\"]).to_numpy()\ny = df.pressure.to_numpy()\n\ngkfold = GroupKFold(n_splits=5)\nfor train_idx, valid_idx in gkfold.split(X, y, groups): pass","a2225446":"splits = (list(train_idx),list(valid_idx))\ndep_var = \"pressure\"  # dependent variable\n\n# use fastai function. \ncont, cat = cont_cat_split(df, 3, dep_var=dep_var)\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)","52ce288e":"len(to.train), len(to.valid)","bcdc175d":"to.show(3)","7c601eaa":"TabularPandas(df, procs, cat, [], y_names=dep_var, splits=splits).show(3)","83a8b481":"to.items.head(3)","bc6bc89c":"to.classes[\"R\"]","05a7f623":"df.R.isnull().values.any()","df8804e3":"save_path = Path(\"\/kaggle\/working\")\nsave_pickle(save_path\/\"to.pkl\", to)  # save preprocessing steps. ","1b66e02c":"# to = load_pickle(save_path\/\"to.pkl\")\n\nxs, y = to.train.xs, to.train.y\nvalid_xs, valid_y = to.valid.xs, to.valid.y","9ebfdf48":"m = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(xs, y)","fcfc379a":"draw_tree(m, xs, size=10, leaves_parallel=True, precision=2)","588920a1":"samp_idx = np.random.permutation(len(y))[:500]\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname=\"DejaVu Sans\", scale=1.6, label_fontsize=11, orientation=\"LR\")","c657f868":"cumsum_y = df[df.new_time_step == 79].u_in_cumsum.to_numpy()\ncumsum_y","3359e9f7":"# histogram plot? \nplt.figure(dpi=100)\nsns.distplot(cumsum_y)","6957b173":"cumsum_test_y = df_test[df_test.new_time_step == 79].u_in_cumsum.to_numpy()\nplt.figure(dpi=100)\nsns.distplot(cumsum_test_y)","0a594b54":"sns.distplot(df[df.u_out == 1].u_in.to_numpy())","f3fe2a50":"# df.u_in.where(df.u_out == 0, 0, inplace=True)\n# np.count_nonzero(df[df.u_out == 1].u_in.to_numpy())","d2d42714":"# recalculate cumsum after setting to zero. \n# df[\"u_in_cumsum\"] = df.u_in.groupby(df.breath_id).cumsum()\n# df","4675d9af":"cumsum_y = df[df.new_time_step == 79].u_in_cumsum.to_numpy()\nplt.figure(dpi=100)\nsns.distplot(cumsum_y)","d24a11d5":"# Ideas from https:\/\/www.kaggle.com\/artgor\/ventilator-pressure-prediction-eda-fe-and-models\ndf[\"max_cumsum\"] = df.groupby(\"breath_id\")[\"u_in_cumsum\"].transform(\"last\")\ndf_test[\"max_cumsum\"] = df_test.groupby(\"breath_id\")[\"u_in_cumsum\"].transform(\"last\")\ndf.head()","2279d7a9":"# We use binning = 5. Feel free to change the value. \n# Remember to change the value of `to.pkl` as well to (at least) q so that it treats it\n# as a categorical variable. \ndf[\"binned_max_cumsum_5fold\"] = pd.qcut(df.max_cumsum, q=5)\ndf.head()","e27aa98f":"m = df.binned_max_cumsum_5fold.unique().to_numpy()\ntbins = sorted([g.right for g in m] + [-0.001])\n\n# To avoid NAN, we need to change the smallest and largest bins values to incorporate all.\ntbins[0] = df_test.max_cumsum.min() - 0.01\ntbins[-1] = df_test.max_cumsum.max()\n\ndf_test[\"binned_max_cumsum_5fold\"] = pd.cut(df_test.max_cumsum, tbins)","87246700":"gby = df.groupby(\"breath_id\")\ndf[\"breathId_uIn_max\"] = gby.u_in.transform(\"max\")\ndf[\"breathId_uIn_diffmax\"] = gby.u_in.transform(\"max\") - df.u_in\ndf[\"breathId_uIn_diffmean\"] = gby.u_in.transform(\"mean\") - df.u_in\n\ndf[\"uIn_lag1\"] = gby.u_in.shift(1)\ndf[\"uIn_lag2\"] = gby.u_in.shift(2)\ndf[\"uIn_lag3\"] = gby.u_in.shift(3)\n\ndf[\"uIn_diff1\"] = df.u_in - df.uIn_lag1\ndf[\"uIn_diff2\"] = df.u_in - df.uIn_lag2\ndf[\"uIn_diff3\"] = df.u_in - df.uIn_lag3  # this is NA in the referenced notebook.\n\ndf[\"uIn_lagback1\"] = gby.u_in.shift(-1)\ndf[\"uIn_lagback2\"] = gby.u_in.shift(-2)\ndf[\"uIn_lagback3\"] = gby.u_in.shift(-3)\n\nfor col in df.columns:\n    if df[col].isnull().values.any(): df[col] = df[col].fillna(0)\ndf.isnull().values.any(), df.isnull().sum().sum()","767d201b":"# repeat for test\ngby = df_test.groupby(\"breath_id\").u_in\ndf_test[\"breathId_uIn_max\"] = gby.transform(\"max\")\ndf_test[\"breathId_uIn_diffmax\"] = gby.transform(\"max\") - df_test.u_in\ndf_test[\"breathId_uIn_diffmean\"] = gby.transform(\"mean\") - df_test.u_in\n\nfor i in range(1, 4):\n    df_test[f\"uIn_lag{i}\"] = gby.shift(i)\n    df_test[f\"uIn_lagback{i}\"] = gby.shift(-i)\n    df_test[f\"uIn_diff{i}\"] = df.u_in - df[f\"uIn_lag{i}\"]\n    \nfor col in df_test.columns:\n    if df_test[col].isnull().values.any(): df_test[col] = df_test[col].fillna(0)\ndf_test.isnull().values.any(), df_test.isnull().sum().sum()","e0b806ce":"df.u_in_cumsum.where(df.u_in_cumsum < 1500, 1500, inplace=True)\ndf_test.u_in_cumsum.where(df_test.u_in_cumsum < 1500, 1500, inplace=True)\ndf.u_in_cumsum.describe()","ab3c96d6":"cumsum_y = df[df.new_time_step == 79].u_in_cumsum.to_numpy()\nplt.figure(dpi=100)\nsns.distplot(cumsum_y)","185168df":"tsdist = df[df.new_time_step == 79].time_step.to_numpy()\ntsdist","3489769b":"plt.figure(dpi=100)\nsns.distplot(tsdist)","c34fa1f2":"tsdist.max()","21548383":"tsdist = df_test[df_test.new_time_step == 79].time_step.to_numpy()\nplt.figure(dpi=100)\nsns.distplot(tsdist)","ba6695b7":"def m_mae(m, xs, y): return mean_absolute_error(y, m.predict(xs))","def1ff1a":"m_mae(m, xs, y), m_mae(m, valid_xs, valid_y)","ea0136a1":"splits = (list(train_idx),list(valid_idx))\ndep_var = \"pressure\"  # dependent variable\n\n# use fastai function. \ndf_drop = df.drop(columns=\"new_time_step\")\ncont, cat = cont_cat_split(df_drop, 5, dep_var=dep_var)\nto = TabularPandas(df_drop, procs, cat, cont,\n                   y_names=dep_var, splits=splits)","354559e2":"xs, y = to.train.xs, to.train.y\nvalid_xs, valid_y = to.valid.xs, to.valid.y\n\nm = DecisionTreeRegressor(min_samples_leaf=300)\nm.fit(to.train.xs, to.train.y)\nm_mae(m, xs, y), m_mae(m, valid_xs, valid_y)","cd001b3b":"m.get_n_leaves()","be2f700a":"splits = (list(train_idx),list(valid_idx))\ndep_var = \"pressure\"  # dependent variable\n\n# use fastai function. \ndf_drop = df.drop(columns=\"time_step\")\ncont, cat = cont_cat_split(df_drop, 5, dep_var=dep_var)\nto = TabularPandas(df_drop, procs, cat, cont,\n                   y_names=dep_var, splits=splits)\n\nxs, y = to.train.xs, to.train.y\nvalid_xs, valid_y = to.valid.xs, to.valid.y\n\nm = DecisionTreeRegressor(min_samples_leaf=300)\nm.fit(to.train.xs, to.train.y)\nm_mae(m, xs, y), m_mae(m, valid_xs, valid_y)","c47bfb6f":"m.get_n_leaves()","56f035f9":"# df = df.drop(columns=\"new_time_step\")\ndf.to_csv(\"train_preprocessed.csv\", index=False)\ndf_test.to_csv(\"test_preprocessed.csv\", index=False)\ndf.head()","09a08001":"splits = (list(train_idx),list(valid_idx))\ndep_var = \"pressure\"  # dependent variable\n\n# use fastai function. \ncont, cat = cont_cat_split(df, 5, dep_var=dep_var)\nto = TabularPandas(df, procs, cat, cont,\n                   y_names=dep_var, splits=splits)\n\nsave_pickle(save_path\/\"to.pkl\", to)  # overwrite the original to.pkl\nsave_pickle(save_path\/\"split.pkl\", splits)\n\nto_test = TabularPandas(df_test, procs)\nsave_pickle(save_path\/\"to_test.pkl\", to_test)","e7c14d1e":"Now try with `new_time_step`. ","f18f54a5":"Looks like the time aren't too far apart. We could try both and see how they works. Particularly, some patients have longer breathing cycle some have less, but they are still in the range $2.45 \\leq x \\leq 2.75$ mostly. However, it seems like to have a long tail distribution, so let's see what's the maximum value? ","7d4d6773":"One more thing: what are the distributions of max `time_step`? One remembers they're not equal, so let's see if the distribution is far apart before we could determine whether or not to use `new_time_step`. ","806b51c0":"For now, we will leave `time_step` and `u_in` unremoved. We might decide to remove them later but let's leave it for now. ","75cd3581":"`#na#` is missing value. However, perhaps it might just be there just in case, because we previously checked the dataframe doesn't have any nan values. Let's check again to make sure this class no nan. ","ebe1ed3d":"**Notice that there are some extreme pumping of air into the lungs while breathing out. One cannot be sure, as one isn't a doctor, whether that would prevent the patient from having difficulty breathing out or not.**\n\nSo this might be simulation error again. If not, then the \"artificial bellows test lung\" wouldn't be a safe product? **Note this is AN OPINION. For proper instruction, this requires medical experts to determine whether it is or not**. \n\nFor now, let's continue with our task of setting them == 0. And then, update our `u_in_cumsum`. \n\n### Update\nSince it doesn't do lots of improvement, we are not setting `df.u_in`  == 0 anymore. ","56fb4364":"We can use dtreeviz to show information. ","0c280a1a":"Now that we have the baseline, let's make our dataloaders again using the new pandas dataframe. This time, we shall try our third point: make two models and see if using `time_step` is better or `new_time_step`. ","b679a716":"The second value of `cont_cat_split` means how many `.unique()` values less than defined will be treated as categorical? Here, if we put 1, we treat none as categorical (unless there are class with just one single category, which is not very predictive anyway that should be removed). Here we put 3, beacuse previously we check R and C to have 3 distinct values, hence they are considered categorical. Then, `u_out` is binary so it will also be considered categorical. ","715e2e20":"After the `time_step` 1.065, the patient is breathing out so the pressure stays \"constant\". There looks like some problem with `u_in_cumsum` where some patients have much much more larger lungs than average patients does? One doesn't know what this means lol... Let's try plot the max value we get for `u_in_cumsum()` and see what it means. ","2d8d9f4e":"We shall just use the last split as splitting of data. Additionally, you could train a n-fold model if you would like to. Here, we save the hassle. ","fdb69c19":"We also consider is using the `final_value` of cumulative as a new row. Then, we will make it categorical by binning it. Bin shall be varying depending on values, such that each bin have approximately the same counts. This way, we can get the max values. ","cb0eada9":"# Other things to think about\n- Whether or not to set `u_in` to 0 if `u_out` is True. According to https:\/\/www.kaggle.com\/artgor\/ventilator-pressure-prediction-eda-fe-and-models, it is the most important feature. For this feature, you could say that it makes sense or it makes no sense. It makes sense because if you are breathing out, and if you are pumping air in while breathing out, you \"seems to\" accumulate greater pressure. It makes no sense because why \"the last\" value? \n","2491ea2e":"The reason we did step 1 after step 2 is because, we cannot be sure that the long-tail distributions are due to `u_out`. Seeing the graph above, we know it isn't. Hence, we can cap it. ","db6c26a9":"So the time step is calculated over 80 values each? Let's check. ","800f9be7":"For the \"no stopping criteria\" part of the notebook, we will skip. The data here is (much) much more than the original data used in the book, so it might take (quite a long time) forever. \n\nAnother thing is instead of using our own function for MAE (the competition metric), we shall use `sklearn`'s `mean_absolute_error` instead. We will still define the value `m_rmse` equivalent, though. ","130c5771":"# Experiment","3166d7bb":"There are also other transformations in https:\/\/www.kaggle.com\/artgor\/ventilator-pressure-prediction-eda-fe-and-models which we will be using here. Particularly, based on the chart of LightBGM, we see some columns contains significant importance, which we could use it here. We will include them although some of them doesn't make direct sense of why they are important. \n\n**However, we will check (and welcome you readers to check) whether there are data leakage or not. Report it in the comments if noticed so. **\n\nThen in the second notebook, where we check whether they are \"closely related\" and \"redundant\" or not, we will remove those that are redundant (after we choose which features gives the best result, based on greedy-method (training and compare MAE by retaining one at a time, keeping all else constant)). ","a56f81d3":"Let's check the original cumsum distribution how it looks like. ","de985088":"# Google Brain Ventilator Pressure Competition\nWe can use this competition as a practice of using Fastai Tabular module. Check [Fastbook Chapter 9](https:\/\/nbviewer.jupyter.org\/github\/fastai\/fastbook\/blob\/master\/09_tabular.ipynb) for more details. In fact, we are almost copy and pasting the code except that we are using a different dataset, as practice. And of course, plus several own thoughts, thoughts taken from discussion forums, and etc are integrated into this notebook. ","2e44ff9d":"Second thing: let's try and see if we \"reset\" `u_in_cumsum` when patient is breathing out, so we stop the cumsum according to `time_step == 1.065`, so that might be `new_time_step=40` (or something like that, we have to check). This means that after this time step, (or after this new_time_step), we shall stop cumulating and keep it constant. \n\n**Note, finally this is implemented as setting all values of `u_in` == 0 when `u_out` == 1. This is useful because if you see the plot below:**","ee9eee03":"## Creating Decision Tree","b625d72a":"Perhaps this (artificial) patient knows meditative breathing hehehe... One doesn't know how to deal with this. You can't say \"capped\" it at a value, there's no such thing as \"stop the time of the patient\" or \"ask the patient to breath faster\" that kind of thing. ","49af2f5c":"Looks like using `time_step` gives better result. We shall pass on `new_time_step` instead of deleting it now, just for the tutorial reason to see how it does later on. \n\nFor the rest of the tutorial, please see [notebook 2](https:\/\/www.kaggle.com\/wabinab\/ventpressure2) as this notebook is getting a little bit long. (Also if you decide not to break here and continue, it's about time to free up some RAM). \n\nBefore we end, we shall save final preprocessing steps that could be imported into notebook 2. ","585749d7":"One first decide to cap at 1000, but it seems like there are still a lot of values above 1000, so one changes one's mind to cap at 1500, or else you would see that most values (and by most, one means highest density) value will be at 1000. With 1500, the value is more lenient. \n\nOf course, for this, you won't see any changes in score with Decision Trees (since it is robust). However, for Deep Neural Network (DNN) that we might experiment with later on, this could be useful. \n\n## Explanation on long-tailed of cumulative `u_in_cumsum`\n\n**Note**: In notebook 2 one founds that without capping it will give better results (well, slightly better results, something like 5e-4 less in MAE). Second thing is, the reason some are large and some are small is because when the patient breathe, the machine not necessarily only open up a small hole (since `u_in` is percentage), it could open up fully and let the patient breath hard hard. Hence, this is why most are small (very shallow breath) while some are large (deep breath). If you notice your own breath, usually we breath very shallow (like not even filling half of our lungs, unconscious breathing mostly); while other times we breath very deep \"down to the bottom\" (when we breath consciously). Unless you practice meditation or other breathing exercises, normal human don't breath that deep most of the time. \n\nSo one previously mention the difference in lung isn't quite correct. It's more of *how much of the lungs are filled during breathing*. ","94488468":"Yes, so each time step covers 80 steps. Instead of using real (floating point) time, we could change `time_step` into \"steps\". We could use the ID (which increase incrementally, we assumed) to create this by getting it's modulo. ","1583a7b5":"Some patients have extra huge lungs. Looks like simulated data cannot really represent real data. (It must be simulated from some distribution, that's why there're probabilities to have such long-tailed distributions. Particularly, **although they seem true, they're never true. No one will have such large lungs and this looks very fake**. \n\nIrregardless of whether it's true or fake, we shall limit the max size of patient's lung, one guesses. We won't be doing it immediately here because we still have another step that would overwrite this step. We would like to see the distribution after that step first. See next step. ","468e4f13":"Although we use `show` it show back the original value, the underlying value used for training are numeric. You can see it from `to.items.head(...)` (which is what actually passed to training. `show` actually decodes back by mapping the internal value to their actual value). "}}