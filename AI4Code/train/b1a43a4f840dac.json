{"cell_type":{"22523f2e":"code","49e7010b":"code","b0adc4e7":"code","9cc4a573":"code","d3c476a4":"code","3af84253":"code","253ada9e":"code","3f1cb500":"code","70e61f3e":"code","b18b24b0":"code","e16b1d6a":"code","9dd8302e":"code","574cf6a0":"code","90aa9575":"code","f4d2dbd0":"code","b0cd70fc":"code","b0e45858":"markdown","9d976651":"markdown","7021c55f":"markdown","c844ad8a":"markdown","fcf5d35c":"markdown","7160dcd4":"markdown","924661d9":"markdown","5b484e5a":"markdown"},"source":{"22523f2e":"# hide\n!pip install deepspeed\n!pip install --upgrade wandb\nimport copy\nfrom functools import partial\nimport multiprocessing as mp\nfrom pathlib import Path\nfrom typing import Any, Callable, List, Tuple\n\nfrom deepspeed.ops.adam import FusedAdam\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch.distributions.bernoulli import Bernoulli\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import io, transforms\n# from torchvision.utils import Image, ImageDraw\nfrom torchvision.transforms.functional import to_pil_image\nfrom tqdm.auto import tqdm\n\n# Wandb login:\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\nuser_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"wandb_api_key\")\nwandb.login(key=secret_value)\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n%matplotlib inline\nprint(torch.__version__, pl.__version__, wandb.__version__)","49e7010b":"# Image parameters\nTRAIN_FILES = \"\/kaggle\/input\/coco-2017-dataset\/coco2017\/train2017\/\"\nIMAGE_SIZE = 256\nPATCH_SIZE = 16\nZERO_PCT = 0.1\nPATCHES_PER_ROW = (IMAGE_SIZE \/\/ PATCH_SIZE)\nNUM_PATCHES = PATCHES_PER_ROW ** 2\nRGB_CHANNELS = 3\nNUM_PIXELS = PATCH_SIZE ** 2 * RGB_CHANNELS\nVALID_IMAGES = 5\nTOPK = 5\n\n# Training parameters\nBATCH_SIZE = 16\nEPOCHS = 5\nLR = 1e-4\n\n# Transformer parameters\nN_HEADS = 8\nN_LAYERS = 6\n\n# Update constants\nTEMPERATURE_S = 0.1\nTEMPERATURE_T = 0.05\nCENTER_MOMENTUM = 0.9\nTEACHER_MOMENTUM = 0.995","b0adc4e7":"class ImageData(Dataset):\n    def __init__(self, files: List[str]):\n        self.files = files\n        self.randcrop_big = transforms.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE), scale=(0.5, 1.0))\n        self.randcrop_small = transforms.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE))\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, i):\n        img = io.read_image(self.files[i])\n        img1 = self.randcrop_big(img)\n        img2 = self.randcrop_small(img)\n        if img.shape[0] == 1:\n            img1 = torch.cat([img1]*3)\n            img2 = torch.cat([img2]*3)\n\n        return img1, img2\n\n\nclass CollateFn:\n    def reshape(self, batch):\n        patches = torch.stack(batch)\\\n                    .unfold(2, PATCH_SIZE, PATCH_SIZE)\\\n                    .unfold(3, PATCH_SIZE, PATCH_SIZE)\n\n        num_images = len(patches)\n        patches = patches.reshape(\n            num_images,\n            RGB_CHANNELS, \n            NUM_PATCHES, \n            PATCH_SIZE, \n            PATCH_SIZE\n        )\n        patches.transpose_(1, 2)\n        \n        return patches.reshape(num_images, NUM_PATCHES, -1) \/ 255.0 - 0.5\n        \n    def __call__(\n        self, batch: List[Tuple[torch.Tensor, torch.Tensor]]\n    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n        x1, x2 = zip(*batch)\n\n        return self.reshape(x1), self.reshape(x2)","9cc4a573":"# hide\nclass ImageOriginalData(Dataset):\n    def __init__(self, files: List[str]):\n        self.files = files\n        self.resize = transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, i):\n        img = io.read_image(self.files[i])\n        if img.shape[0] == 1:\n            img = torch.cat([img]*3)\n\n        return self.resize(img)\n    \nclass CollateSingleImage(CollateFn):    \n    def __call__(\n        self, batch: List[torch.Tensor]\n    ) -> torch.FloatTensor:\n        return self.reshape(batch)\n    \nfiles = [str(file) for file in Path(TRAIN_FILES).glob(\"*.jpg\")]\ntrain_files, valid_files = train_test_split(files, test_size=0.15, random_state=42)\n\ntrain_data = ImageData(train_files)\ntrain_dl = DataLoader(\n    train_data, \n    BATCH_SIZE, \n    shuffle=True, \n    drop_last=True, \n    num_workers=4,\n    pin_memory=True,\n    collate_fn=CollateFn(),\n)\n\nvalid_data = ImageOriginalData(valid_files)\nvalid_dl = DataLoader(\n    valid_data, \n    BATCH_SIZE*2, \n    shuffle=False, \n    drop_last=False, \n    num_workers=4,\n    pin_memory=True,\n    collate_fn=CollateSingleImage(),\n)","d3c476a4":"x, y = next(iter(train_dl))\nx2 = next(iter(valid_dl))\n(x.shape, y.shape), (x2.shape)","3af84253":"class Model(nn.Module):\n    def __init__(self, d_model, n_head, n_layers):\n        super().__init__()\n        # transformer\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head)\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n\n        # positional embedding\n        w_pos = torch.randn(NUM_PATCHES, d_model) \/ d_model ** 0.5\n        cls_token = torch.randn(1, d_model) \/ d_model ** 0.5\n        self.register_parameter(\"pos_embed\", nn.Parameter(w_pos))\n        self.register_parameter(\"cls_token\", nn.Parameter(cls_token))\n\n        # pixel projection\n        self.linear = nn.Linear(2 * d_model, d_model)\n        self.norm1 = nn.LayerNorm(2 * d_model, elementwise_affine=False)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        batch_size = len(x)\n        position = torch.stack([self.pos_embed] * batch_size)\n        x = torch.cat([x, position], dim=-1)\n        pixel_proj = self.norm2(F.relu(self.linear(self.norm1(x))))\n        batched_cls_token = torch.stack([self.cls_token]*batch_size)\n        cls_x = torch.cat([batched_cls_token, pixel_proj], dim=1)\n        \n        cls_x.transpose_(0, 1)\n        return F.normalize(self.encoder(cls_x)[0, ...], dim=-1)","253ada9e":"class HLoss:\n    def __init__(self, temperature_t: float, temperature_s: float):\n        self.temperature_t = temperature_t\n        self.temperature_s = temperature_s\n        \n    def __call__(\n        self, \n        t: torch.FloatTensor, \n        s: torch.FloatTensor, \n        center: torch.FloatTensor\n    ) -> torch.FloatTensor:\n        t = F.softmax((t.detach() - center) \/ self.temperature_t, dim=1)\n        log_s = F.log_softmax(s \/ self.temperature_s, dim=1)\n\n        return -(t * log_s).sum(dim=1).mean()","3f1cb500":"# hide\nresize = transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))\n\ndef get_closest(embedding: torch.FloatTensor, i: int):\n    similarity = embedding @ embedding[i,:].T\n    scores, idx = similarity.topk(TOPK)\n    return scores.cpu().numpy(), idx.cpu().numpy()\n\ndef get_closest_wandb_images(embedding: torch.FloatTensor, i: int, files: List[str]):\n    main_img = to_pil_image(resize(io.read_image(files[i])))\n    closest_imgs = [wandb.Image(main_img)]\n    \n    scores, idx = get_closest(embedding, i)\n    \n    for i, score in zip(idx, scores):\n        img = to_pil_image(resize(io.read_image(files[i])))\n        closest_imgs.append(\n            wandb.Image(img, caption=f\"{score:.4f}\")\n        )\n        \n    return closest_imgs","70e61f3e":"class LightningModel(pl.LightningModule):\n    def __init__(\n        self,\n        teacher: nn.Module,\n        lr: float,\n        loss_fn: Callable,\n        valid_files: List[str],\n        dim: int,\n        center_momentum: float,\n        param_momentum: float,\n    ):\n        super().__init__()\n        self.teacher = teacher\n        self.student = copy.deepcopy(teacher)\n        self.lr = lr\n        self.loss_fn = loss_fn\n        self.c_mom = center_momentum\n        self.p_mom = param_momentum\n        self.register_buffer(\"center\", torch.zeros((1, dim)).float())\n        self.valid_files = valid_files\n        \n        for p in self.teacher.parameters():\n            p.requires_grad = False\n\n    def loss_calculation(\n        self,\n        batch: Tuple[torch.FloatTensor, torch.FloatTensor],\n    ) -> torch.FloatTensor:\n        x1, x2 = batch\n        \n        s1, s2 = self.student(x1), self.student(x2)\n        t1, t2 = self.teacher(x1), self.teacher(x2)\n        \n        loss = self.loss_fn(t1, s2, self.center) + self.loss_fn(t2, s1, self.center)\n        \n        emperical_center = F.normalize(\n            torch.cat([t1, t2]).mean(dim=0, keepdims=True),\n            dim=-1,\n        )\n        return loss, emperical_center\n\n    def training_step(\n        self, batch: Tuple[torch.FloatTensor, torch.FloatTensor], *args: List[Any]\n    ) -> torch.Tensor:\n        loss, emperical_center = self.loss_calculation(batch)\n        self.log(name=\"Training loss\", value=loss, on_step=True, on_epoch=True)\n        \n        self.center = F.normalize(\n            self.c_mom * self.center + (1 - self.c_mom) * emperical_center,\n            dim=-1,\n        )\n        for s_p, t_p in zip(self.student.parameters(), self.teacher.parameters()):\n            t_p.data = self.p_mom * t_p.data + (1 - self.p_mom) * s_p.data\n            \n        return loss\n    \n    def validation_step(self, images: torch.FloatTensor, *args: List[Any]) -> None:\n        return self.teacher(images)\n        \n    def validation_epoch_end(self, validation_step_outputs):\n        valid_embeds = torch.cat([pred for pred in validation_step_outputs])\n        columns = [\"image\"] + [f\"closest_{i+1}\" for i in range(TOPK)]\n        indices = np.random.choice(len(self.valid_files), VALID_IMAGES, replace=False)\n        rows = [get_closest_wandb_images(valid_embeds, i, self.valid_files) for i in indices]\n        table = wandb.Table(data=rows, columns=columns)\n        self.logger.experiment.log({f\"epoch {self.current_epoch} results\": table})\n        \n    def on_after_backward(self):\n        if self.trainer.global_step % 50 == 0:  # don't make the tf file huge\n            global_step = self.trainer.global_step\n            for name, param in self.student.named_parameters():\n                if \"weight\" in name and not \"norm\" in name and param.requires_grad:\n                    self.logger.experiment.log(\n                        {f\"{name}_grad\": wandb.Histogram(param.grad.cpu())}\n                    )\n\n    def configure_optimizers(self) -> torch.optim.Optimizer:\n        return FusedAdam(self.student.parameters(), lr=self.lr)","b18b24b0":"!mkdir \/kaggle\/working\/logs\nteacher = Model(NUM_PIXELS, N_HEADS, N_LAYERS)\nh_loss = HLoss(TEMPERATURE_T, TEMPERATURE_S)\nlightning_model = LightningModel(\n    teacher, \n    LR,\n    h_loss,\n    valid_files,\n    NUM_PIXELS,\n    CENTER_MOMENTUM, \n    TEACHER_MOMENTUM,\n)\n\nlogger = WandbLogger(\"DINO\", \"\/kaggle\/working\/logs\/\", project=\"DINO\")\ntrainer = pl.Trainer(\n    max_epochs=EPOCHS,\n    gpus=torch.cuda.device_count(),\n    gradient_clip_val=1.0,\n    logger=logger,\n    precision=16,\n#     limit_train_batches=10,\n    num_sanity_val_steps=0,\n)\ntrainer.fit(lightning_model, train_dl, valid_dl)","e16b1d6a":"# hide\nimage_orig_data = ImageOriginalData(files)\nimage_orig_dl = DataLoader(\n    image_orig_data, \n    BATCH_SIZE*2, \n    shuffle=False, \n    drop_last=False, \n    num_workers=4,\n    pin_memory=True,\n    collate_fn=CollateSingleImage(),\n)","9dd8302e":"teacher = teacher.eval().to(device)\nembedding = []\nwith torch.no_grad():\n    for x in tqdm(image_orig_dl):\n        out = teacher(x.to(device))\n        embedding.append(out.cpu())\n        \n    embedding = torch.cat(embedding, dim=0)","574cf6a0":"def plot_closest_pairs(embedding, i, files):\n    img = to_pil_image(resize(io.read_image(files[i])))\n    plt.imshow(img)\n    scores, idx = get_closest(embedding, i)\n    \n    fig, axs = plt.subplots(1, len(idx), figsize=(12, 5))\n    for i, score, ax in zip(idx, scores, axs):\n        img = to_pil_image(resize(io.read_image(files[i])))\n        ax.imshow(img)\n        ax.set_title(f\"{score:.4f}\")\n\n    plt.show()","90aa9575":"i = 64\nplot_closest_pairs(embedding, i, files)","f4d2dbd0":"i = 42\nplot_closest_pairs(embedding, i, files)","b0cd70fc":"i = 21\nplot_closest_pairs(embedding, i, files)","b0e45858":"## Shameless Self Promotion\nIf you enjoyed the tutorial buy me a coffee, or better yet [buy my course](https:\/\/www.udemy.com\/course\/machine-learning-and-data-science-2021\/?referralCode=E79228C7436D74315787) (usually 90% off).","9d976651":"# \"DINO Self Supervised Vision Transformers\"\n> Getting image embeddings with no negative samples\n\n- toc: true\n- branch: master\n- badges: true\n- comments: true\n- author: Sachin Abeywardana\n- categories: [pytorch, pytorch lightning, loss function]\n- use_math: true\n- image: images\/dino.png\n![](https:\/\/i.imgur.com\/4mdrr4x.png)\nPhoto credit [Nicola Stocchi](https:\/\/www.flickr.com\/photos\/124313328@N08\/26611532154)\n\nThis implementation is based off of [this paper](https:\/\/arxiv.org\/pdf\/2104.14294.pdf) by FAIR. The reason I'm excited about this paper is because 1. I was able to implement this by reading the paper (don't underestimate how convoluted this step is) 2. We have a setting where we ignore any kind of labels altogether, i.e. completely self supervised.\n\nIn a nutshell what the paper attempts to do, is to take two different augmentations of the same image, and try and push these embeddings closer together. Most (and possibly all) other such tasks attempt to do a triplet loss where the image is compared to a similar image (positive example), and different image(s) (negative examples). What's amazing about this paper is that it ignores negative examples altogether and is still able to get a meaningful embedding space for a given image.\n\n\n","7021c55f":"## Cross Entropy Loss, sort of\nWe need a loss function that will push the output vector of the above model towards each other for the two augmented images. The way that this paper does it, is by treating the vector as a (log of) a histogram, and trying to line it up with its augmented version. Why is this interesting? Usually, most authors tend to simply take the dot product and maximise that to indicate that vectors need to line up (be close). This paper achieves the same result but by asking histograms to line up instead.\n\nThe cross entropy loss takes on the form of $-\\sum_{j=1}^J y_j\\log p(x_j)$. This is minimised when $p(x_j)$ tends towards $y_j$. Similarly if we were to replace $y_j$ with a probability distribution $q_j\\in[0, 1]\\text{ s.t. }\\sum_j q_j = 1$, p_j is minimised when the distributions line up.\n\nMy personal intuition as to why this might be better might be as follows: (Feel free to correct me if I am wrong). When trying to maximise dot products, we are asking two points on a sphere to move towards each other. Now there is an obvious shortest path distance when you visualise it by drawing a line between the two points. However, going away from the point will also, eventually get you to the same point but by taking a longer time.\n\nBy using the following equation we need two histograms to line up, which is a simpler derivative, and therefore more likely to get to a local minima faster.\n$$ \\frac{1}{N}\\sum_{n=1}^N\\sum_{j=1}^J q_j\\log p_j$$\n\nNote that in the implementation below, we have extra two concepts of centering, and using a temperature. I'm not sure about the centering, but the temperature variable (which is positive but less than one), sharpens the histogram, making the peaks more prominent.","c844ad8a":"## Training\nThe `LightningModule` below goes through the training step. The main steps are:\n1. Create two copies of the model with the exact same parameters. One would be considered teacher (with the gradients not being calculated at backprop) and the student.\n2. Pass both augmentations through both student and teacher.\n3. Calculate the loss function shown above for the two augmentations, but with one embedding from teacher and the other from the student.\n4. Calculate the new exponentially weighted teacher parameters with the corresponding student parameters.\n5. Calculate a new (exponentially weighted) center parameter from the embeddings passed through the teacher only.\nSteps 1-4 being the most important aspects IMO. 5th step feels more like a trick needed to stabalize optimization.\n\nThe following gif from the [official repo](https:\/\/github.com\/facebookresearch\/dino) is an overview of the algorithm:\n![](https:\/\/github.com\/facebookresearch\/dino\/raw\/main\/.github\/dino.gif)","fcf5d35c":"## Model\nThe model is simply the 256 image patches passed through an encoder transformer with a CLS token. However, a few things that I think newcomers to the transformer\/ pytorch field ought to know is as follows (also I made these mistakes \ud83d\ude05).\n- Make sure to use `self.register_parameter` when declaring trainable variables in pytorch. doing `self.myvar = nn.Parameter(...)` is not enough.\n- I have used `LayerNorm` everywhere possible to keep gradient sizes reasonable for a optimizer with constant learning rate.\n\nThe walkthough of the model operation ignoring normalisations is as follows:\n1. Take the flattened 3x16x16 image patches and append them to a positional encoding.\n2. Pass this through a linear layer and append the CLS token embedding to this.\n3. Pass this through transformer, and take the 0th token since this correspond to the CLS token.\n4. Normalize this vector to unit length so that you have a final image embedding.","7160dcd4":"## Evaluate results\nFinally lets look at some random images with its most closest images according to the model. Note that at this point, we throw away the student and simply take the teacher, even though it is only the student that used gradient information directly.","924661d9":"## Data\n\nThe following shows how a given image is passed through two transforms (`RandomResizedCrop`) with one guaranteed to be atleast half as large as the original image. A collate function is then used to break it up into 16x16 patches so that we can fit it into a transformer. The collate function is a minor detail in the overall picture, but if you wish to read about it you can do so [here](https:\/\/sachinruk.github.io\/blog\/pytorch\/data\/2021\/06\/05\/PyTorch-CollateFn.html).","5b484e5a":"Thanks to PyTorch Lightning and WandB I can easily do half precision training and log the results to a beautiful dashboard, with results in the link below."}}