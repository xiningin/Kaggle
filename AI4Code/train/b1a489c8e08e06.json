{"cell_type":{"9a6759b5":"code","aabfa9e9":"code","210ef2d8":"code","d129b0ee":"code","67087e1c":"code","83c7f942":"code","fc95ce39":"code","8b64c253":"code","58f57a24":"code","220d5558":"code","364aa3bb":"code","dcb8c110":"code","111a2bce":"code","7cd5a086":"code","8cc4013d":"code","46379f4a":"code","ce020cfc":"code","e20bfbcb":"code","0841e5b1":"code","510cb4c7":"code","f103d2c4":"code","b77ba698":"code","dc825deb":"code","6d0c4836":"code","0d695c0d":"code","b29dc4d5":"code","6f8b0fe0":"code","ba81354b":"code","bd063e97":"code","d52fac52":"markdown","b4d99075":"markdown","5bf3f1bb":"markdown","b88f1211":"markdown","73373907":"markdown","498a3c91":"markdown","b7f388e1":"markdown","60aa460d":"markdown","21d5a0fa":"markdown","5d398270":"markdown","317cc728":"markdown","35eb22e9":"markdown","b1b921bb":"markdown","a55656cf":"markdown","76d8e6a7":"markdown","c556ae16":"markdown","df5c494e":"markdown"},"source":{"9a6759b5":"from google.colab import drive\ndrive.mount('\/content\/drive')","aabfa9e9":"import numpy as np\nimport pandas as pd\nimport re\n\ntrain=pd.read_csv('drive\/MyDrive\/kaggle\/train.csv')\ntest=pd.read_csv('drive\/MyDrive\/kaggle\/test.csv')","210ef2d8":"train.loc[pd.isnull(train.keyword), 'keyword'] = ''\ntest.loc[pd.isnull(test.keyword), 'keyword'] = ''\ntrain.loc[pd.isnull(train.location), 'location'] = ''\ntest.loc[pd.isnull(test.location), 'location'] = ''","d129b0ee":"train[:10]","67087e1c":"for i in range(train.shape[0]):\n    train.iloc[i,3]=' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \", train.iloc[i,3]).split())","83c7f942":"for i in range(test.shape[0]):\n    test.iloc[i,3]=' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \", test.iloc[i,3]).split())","fc95ce39":"for i in range(100):\n    print(train.iloc[137+i,3])","8b64c253":"# \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c\nall.shape","58f57a24":"from sklearn.preprocessing import OneHotEncoder\n\nall = pd.concat([train, test])\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit([[x] for x in all.keyword.values])\ntrain_enc = enc.transform([[x] for x in train.keyword.values]).toarray()\ntest_enc = enc.transform([[x] for x in test.keyword.values]).toarray()\nprint(train_enc.shape, test_enc.shape)\nenc_train = pd.DataFrame(train_enc)\nenc_test = pd.DataFrame(test_enc)\nnp.array(enc_train[list(enc_train.columns)])[1000]\ntrain = pd.concat([train, enc_train], axis = 1)\ntest = pd.concat([test, enc_test], axis = 1)\nprint(train.shape, test.shape)","220d5558":"train_enc[1000]","364aa3bb":"!pip3 install tqdm\n!pip3 install transformers","dcb8c110":"import transformers\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')","111a2bce":"import tqdm\n\ndef create_bert_input_features(tokenizer, docs, max_seq_length):\n    \n    all_ids, all_masks = [], []\n    for doc in tqdm.tqdm(docs, desc=\"Converting docs to features\"):\n        tokens = tokenizer.tokenize(doc)\n        if len(tokens) > max_seq_length-2:\n            tokens = tokens[0 : (max_seq_length-2)]\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n        ids = tokenizer.convert_tokens_to_ids(tokens)\n        masks = [1] * len(ids)\n        # Zero-pad up to the sequence length.\n        while len(ids) < max_seq_length:\n            ids.append(0)\n            masks.append(0)\n        all_ids.append(ids)\n        all_masks.append(masks)\n    encoded = np.array([all_ids, all_masks])\n    return encoded","7cd5a086":"train_X, val_X, train_Y, val_Y = train_test_split(train['location'] + ' ' + train['text'], train['target'], test_size=0.05, random_state=42)","8cc4013d":"from sklearn.model_selection import train_test_split\ncols = list(enc_train.columns) + ['text']\ntrain_X, val_X, train_Y, val_Y = train_test_split(train[cols], train['target'], test_size=0.15, random_state=42)\ntrain_X_enc = train_X[list(enc_train.columns)]\nval_X_enc = val_X[list(enc_train.columns)]\ntest_X_enc = test[list(enc_test.columns)]\nprint(val_X_enc.shape, train_X_enc.shape, test_X_enc.shape)\n\ntrain_X, val_X, train_Y, val_Y = train_X['text'].values, val_X['text'].values, train_Y.values, val_Y.values\ntest_X = test['text'].values\nprint(val_X.shape, train_X.shape, test_X.shape)","46379f4a":"train_X[:10]","ce020cfc":"MAX_SEQ_LENGTH = 500\n\ntrain_features_ids, train_features_masks = create_bert_input_features(tokenizer, train_X, \n                                                                      max_seq_length=MAX_SEQ_LENGTH)\nval_features_ids, val_features_masks = create_bert_input_features(tokenizer, val_X, \n                                                                  max_seq_length=MAX_SEQ_LENGTH)\nprint('Train Features:', train_features_ids.shape, train_features_masks.shape)\nprint('Val Features:', val_features_ids.shape, val_features_masks.shape)","e20bfbcb":"test_features_ids, test_features_masks = create_bert_input_features(tokenizer, test_X, \n                                                                    max_seq_length=MAX_SEQ_LENGTH)\nprint('Test Features:', test_features_ids.shape, test_features_masks.shape)","0841e5b1":"import tensorflow as tf","510cb4c7":"inp_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_ids\")\ninp_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_masks\")\ninputs = [inp_id, inp_mask]\n\nkeyword_input = tf.keras.layers.Input(shape=(222,), dtype='int32', name=\"keyword_input\")\nx = tf.keras.layers.Dense(256, activation='relu')(keyword_input)\nx = tf.keras.layers.Dropout(0.3)(x)\nx = tf.keras.layers.Dense(256, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.3)(x)\n\nhidden_state = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')(inputs)[0]\npooled_output = hidden_state[:, 0]\nconcated = tf.keras.layers.concatenate([pooled_output, x], axis = 1)\ndense1 = tf.keras.layers.Dense(256, activation='relu')(concated)\ndrop1 = tf.keras.layers.Dropout(0.3)(dense1)\ndense2 = tf.keras.layers.Dense(256, activation='relu')(drop1)\ndrop2 = tf.keras.layers.Dropout(0.3)(dense2)\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(drop2)\n\n\nmodel = tf.keras.Model(inputs=[inp_id, inp_mask, keyword_input], outputs=output)\nmodel.compile(optimizer=tf.optimizers.Adam(learning_rate=2e-6, \n                                           epsilon=1e-08), \n              loss='binary_crossentropy', metrics=['accuracy',tf.keras.metrics.AUC()])","f103d2c4":"model.summary()","b77ba698":"inp_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_ids\")\ninp_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_masks\")\ninputs = [inp_id, inp_mask]\n\nhidden_state = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')(inputs)[0]\npooled_output = hidden_state[:, 0]    \ndense1 = tf.keras.layers.Dense(256, activation='relu')(pooled_output)\ndrop1 = tf.keras.layers.Dropout(0.3)(dense1)\ndense2 = tf.keras.layers.Dense(256, activation='relu')(drop1)\ndrop2 = tf.keras.layers.Dropout(0.3)(dense2)\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(drop2)\n\n\nmodel = tf.keras.Model(inputs=inputs, outputs=output)\nmodel.compile(optimizer=tf.optimizers.Adam(learning_rate=2e-6, \n                                           epsilon=1e-08), \n              loss='binary_crossentropy', metrics=['accuracy',tf.keras.metrics.AUC()])","dc825deb":"es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                      patience=2,\n                                      restore_best_weights=True,\n                                      verbose=1)","6d0c4836":"model.fit([train_features_ids, \n           train_features_masks, train_X_enc], train_Y, \n          validation_data=([val_features_ids, \n                            val_features_masks, val_X_enc], val_Y),\n          epochs=10, \n          batch_size=20, \n          shuffle=True,\n          callbacks=[es],\n          verbose=1)","0d695c0d":"predictions = [1 if pr > 0.5 else 0 \n                   for pr in model.predict([val_features_ids, \n                                            val_features_masks], batch_size=200, verbose=0).ravel()]","b29dc4d5":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\nprint('null accuracy:', max(sum(val_Y)\/val_Y.shape[0],1-sum(val_Y)\/val_Y.shape[0]))\nprint(\"Accuracy: %.2f%%\" % (accuracy_score(val_Y, predictions)*100))\nprint(\"roc auc:\", roc_auc_score(val_Y, predictions))\nprint(classification_report(val_Y, predictions))\npd.DataFrame(confusion_matrix(val_Y, predictions))","6f8b0fe0":"test_Y=model.predict([test_features_ids, test_features_masks], batch_size=200, verbose=0)\n\ntest_label=[]\n\nfor i in range(test_Y.shape[0]):\n    if test_Y[i]>=0.5:\n        test_label.append(1)\n    else:\n        test_label.append(0)","ba81354b":"submission=pd.DataFrame({'id': test['id'], 'target':test_label})\nprint(submission.head(10))\n\nfilename = 'submission_nlp_tweets_bert.csv'\n\nsubmission.to_csv(filename,index=False)","bd063e97":"from IPython.display import FileLink\nFileLink('.\/submission_nlp_tweets_bert.csv')","d52fac52":"## \u0433\u043e\u0442\u043e\u0432\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 - \u0442\u0435\u043a\u0441\u0442\n","b4d99075":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0442\u0432\u0438\u0442\u044b \u0442\u0435\u043f\u0435\u0440\u044c. \u0421\u0442\u0430\u043b\u043e \u043b\u0443\u0447\u0448\u0435","5bf3f1bb":"## \u0422\u0440\u0435\u0439\u043d\/\u0442\u0435\u0441\u0442 \u0441\u043f\u043b\u0438\u0442","b88f1211":"\u041c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0432\u0445\u043e\u0434\u0430 - text - Bert","73373907":"## \u0414\u0435\u043b\u0430\u0435\u043c \u0441\u0430\u0431\u043c\u0438\u0442 \u043d\u0430 Kaggle ","498a3c91":"\u0412\u0442\u043e\u0440\u043e\u0439 \u0441\u0430\u0431\u043c\u0438\u0442 \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u0441 \u043f\u0440\u0438\u043a\u0440\u0435\u043f\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043a \u0442\u0435\u043a\u0441\u0442\u0443 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438 \u043c\u0435\u0441\u0442\u0430","b7f388e1":"## DistilBertTokenizer \u0438 \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0411\u0435\u0440\u0442\u044b","60aa460d":"## \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438","21d5a0fa":"## \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u043e\u043f\u0442\u0438\u043c\u0430\u0439\u0437\u0435\u0440","5d398270":"## Text embeddings with location","317cc728":"## \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435","35eb22e9":"\u0423\u0434\u0430\u043b\u0438\u043c \u043f\u0443\u043d\u043a\u0442\u0443\u0430\u0446\u0438\u044e \u0438 \u043f\u0440\u043e\u0447\u0438\u0435 \u0441\u0438\u043c\u0432\u043e\u043b\u044b","b1b921bb":"\u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u0438\u043c\u0432\u043e\u043b\u044b \u043d\u0430\u0447\u0430\u043b\u0430 \u0438 \u043a\u043e\u043d\u0446\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439.\n\u0414\u0435\u043b\u0430\u0435\u043c \u043c\u0430\u0441\u043a\u0438 \u0438 \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u044b ","a55656cf":"\u041c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0432\u0445\u043e\u0434\u0430 text + keyword","76d8e6a7":"## OneHotEncoding\n","c556ae16":"## \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438","df5c494e":"## \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445"}}