{"cell_type":{"0151cec8":"code","d0b1d6a1":"code","c4c159f1":"code","3de22143":"code","c52b3816":"code","dba51ca2":"code","d1c1d208":"code","3426e046":"code","be2454f8":"code","9139c87d":"code","fcded982":"code","19b899a2":"code","eb7576b6":"code","36554e1c":"code","46ec4c79":"code","d4be01b5":"code","25335781":"code","6fdb7f29":"code","47e3fbd4":"code","8e41081f":"code","44716395":"code","a003f5ca":"code","86771eea":"code","c8418cb2":"code","e6a6859b":"code","564f18ff":"code","9c72914c":"code","542f11c2":"code","d2a595df":"code","5115834f":"code","28be1abc":"code","4063c666":"code","343836cd":"code","0d8f46fb":"code","802879e6":"code","c0dd80c9":"code","57a52a31":"code","5d4081c4":"code","95b6a2a6":"code","894fb314":"code","260a3e8e":"code","622afc7b":"code","8bdfd8d6":"code","178999dd":"code","c0ad5c3d":"code","2af661d4":"code","a27d5e50":"code","e9c007e3":"code","c2d76540":"code","3c9dc2d8":"code","624fa3a4":"code","5d1ba7f0":"code","6233d844":"code","73b4bed2":"code","30ef5f70":"code","8b175cd0":"code","645dcf0e":"code","3fd4f59c":"code","159d0a12":"markdown","6ff9ec6a":"markdown","8c88b94d":"markdown","f8c938ef":"markdown","390748f9":"markdown","6103f46f":"markdown","8d298865":"markdown","e31ae29c":"markdown","e82cf2c7":"markdown","50a15834":"markdown","b7ff95c9":"markdown","415280a8":"markdown","9c90a86c":"markdown","77bb6010":"markdown","cec855dc":"markdown","533b4e08":"markdown","6157c37a":"markdown","0e2b4acf":"markdown","a0786a7c":"markdown","25748d9f":"markdown","c3e3b668":"markdown","4b17b64e":"markdown","4331268d":"markdown","3de43f15":"markdown","efc2138c":"markdown","5e76117f":"markdown"},"source":{"0151cec8":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nprint(\"--------------------------------------------------\")\n# Other Enviroment Checks\n!python --version\n!pwd","d0b1d6a1":"import pandas as pd\nfrom pandasql import sqldf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom collections import Counter\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\nsns.set(style='white', context='notebook', palette='deep')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c4c159f1":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nIDtest = test[\"PassengerId\"]","3de22143":"train.info()","c52b3816":"def detect_outliers(df, n, features):\n    \n    outlier_i = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        #print(col, Q1)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        #print(col,Q3)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col greater than 1.5x of 75percentile and less than 1.5x of 25percentile\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_i.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_i = Counter(outlier_i)        #this will make fequency dict of outlier_i\n    multiple_outliers = list( k for k, v in outlier_i.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train, 2, [\"SibSp\",\"Parch\",\"Fare\"])","dba51ca2":"Outliers_to_drop","d1c1d208":"# Drop outliers\ntrain = train.drop(Outliers_to_drop, axis = 0).reset_index(drop = True)","3426e046":"## Join train and test datasets in order to obtain the same number of features during categorical conversion\ntrain_len = len(train)\ndataset =  pd.concat(objs=[train, test], axis=0)#.reset_index(drop=True)","be2454f8":"# There are null values in Age and Cabin\ntrain.info()","9139c87d":"# Explore Age vs Sex, Parch , Pclass and SibSP\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\nax = fig.add_axes([0,0,1,1])\nfig.suptitle(\"FEATURE wrt AGE\")\n\n#sns.factorplot(y=\"Age\", x=\"Sex\", data=dataset, kind=\"box\", ax=ax[0,0])\n#sns.factorplot(y=\"Age\", x=\"Sex\", hue=\"Pclass\", data=dataset,kind=\"box\", ax = axes[0,1])\n#sns.factorplot(y=\"Age\", x=\"Parch\", data=dataset, kind=\"box\", ax = axes[1,0])\n#sns.factorplot(y=\"Age\", x=\"SibSp\", data=dataset, kind=\"box\", ax = axes[1,1])\n\n#plt.show()","fcded982":"# convert Sex into categorical value 0 for male and 1 for female\ndataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\": 0, \"female\":1})","19b899a2":"mean_value=dataset['Age'].mean()\ndataset['Age'].fillna(value=mean_value, inplace=True)\n\n","eb7576b6":"dataset.info()","36554e1c":"# Explore SibSp feature vs Survived\ng = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=train, kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","46ec4c79":"# Explore SibSp feature vs Survived\ng = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=train,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","d4be01b5":"# Explore Parch feature vs Survived\ng  = sns.factorplot(x=\"Parch\",y=\"Survived\",data=train,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","25335781":"# Explore Age distibution \ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0) & (train[\"Age\"].notnull())], color=\"Blue\", shade = True)\ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1) & (train[\"Age\"].notnull())], ax =g, color=\"Red\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","6fdb7f29":"#Fill Fare missing values with the median value\ndataset[\"Fare\"].fillna(dataset[\"Fare\"].median(), inplace = True)","47e3fbd4":"g = sns.distplot(dataset[\"Fare\"], color=\"b\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","8e41081f":"sns.barplot(x=\"Sex\",y=\"Survived\",data=train)","44716395":"output = sqldf(\"SELECT Sex, COUNT(Survived) AS Total_Survived FROM train GROUP BY Sex\")\noutput","a003f5ca":"# Filling missing value of Age \n\n## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n# Index of NaN age rows\nindex_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = dataset[\"Age\"].median()\n    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        dataset['Age'].iloc[i] = age_pred\n    else :\n        dataset['Age'].iloc[i] = age_med\n\n","86771eea":"dataset[\"Name\"].head()","c8418cb2":"# Get Title from Name\ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\ndataset[\"Title\"] = pd.Series(dataset_title)\ndataset[\"Title\"].head()","e6a6859b":"g = sns.countplot(x=\"Title\",data=dataset)\ng = plt.setp(g.get_xticklabels(), rotation=45) ","564f18ff":"# Convert to categorical values Title \ndataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndataset[\"Title\"] = dataset[\"Title\"].astype(int)","9c72914c":"g = sns.countplot(dataset[\"Title\"])\ng = g.set_xticklabels([\"Master\",\"Miss\/Ms\/Mme\/Mlle\/Mrs\",\"Mr\",\"Rare\"])","542f11c2":"g = sns.factorplot(x=\"Title\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_xticklabels([\"Master\",\"Miss-Mrs\",\"Mr\",\"Rare\"])\ng = g.set_ylabels(\"survival probability\")","d2a595df":"# Drop Name variable\ndataset.drop(labels = [\"Name\"], axis = 1, inplace = True)","5115834f":"# Create a family size descriptor from SibSp and Parch\ndataset[\"Fsize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1","28be1abc":"g = sns.factorplot(x=\"Fsize\",y=\"Survived\",data = dataset)\ng = g.set_ylabels(\"Survival Probability\")","4063c666":"# Create new feature of family size\ndataset['Single'] = dataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\ndataset['SmallF'] = dataset['Fsize'].map(lambda s: 1 if  s == 2  else 0)\ndataset['MedF'] = dataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndataset['LargeF'] = dataset['Fsize'].map(lambda s: 1 if s >= 5 else 0)","343836cd":"g = sns.factorplot(x=\"Single\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"SmallF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"MedF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"LargeF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")","0d8f46fb":"# convert to indicator values Title and Embarked \ndataset = pd.get_dummies(dataset, columns = [\"Title\"])\ndataset = pd.get_dummies(dataset, columns = [\"Embarked\"], prefix=\"Em\")","802879e6":"dataset.head()","c0dd80c9":"# Replace the Cabin number by the type of cabin 'X' if not\ndataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])","57a52a31":"g = sns.countplot(dataset[\"Cabin\"],order=['A','B','C','D','E','F','G','T','X'])","5d4081c4":"g = sns.factorplot(y=\"Survived\",x=\"Cabin\",data=dataset,kind=\"bar\",order=['A','B','C','D','E','F','G','T','X'])\ng = g.set_ylabels(\"Survival Probability\")\n","95b6a2a6":"dataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\")","894fb314":"## Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X. \n\nTicket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ndataset[\"Ticket\"] = Ticket\ndataset[\"Ticket\"].head()\n","260a3e8e":"dataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\")","622afc7b":"# Create categorical values for Pclass\ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\ndataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")","8bdfd8d6":"# Drop useless variables \ndataset.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)","178999dd":"## Separate train dataset and test dataset\n\ntrain = dataset[:train_len]\ntest = dataset[train_len:]\ntest.drop(labels=[\"Survived\"],axis = 1,inplace=True)","c0ad5c3d":"## Separate train features and label \n\ntrain[\"Survived\"] = train[\"Survived\"].astype(int)\n\nY_train = train[\"Survived\"]\n\nX_train = train.drop(labels = [\"Survived\"],axis = 1)","2af661d4":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)","a27d5e50":"# Modeling step Test differents algorithms \nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\n","e9c007e3":"### META MODELING  WITH ADABOOST, RF, EXTRATREES and GRADIENTBOOSTING\n\n# Adaboost\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaDTC.fit(X_train,Y_train)\n\nada_best = gsadaDTC.best_estimator_\n","c2d76540":"gsadaDTC.best_score_","3c9dc2d8":"#ExtraTrees \nExtC = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(X_train,Y_train)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_\n","624fa3a4":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,Y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","5d1ba7f0":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,Y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_\n","6233d844":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(X_train,Y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","73b4bed2":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X_train,Y_train,cv=kfold)\n\n","30ef5f70":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\nnames_classifiers = [(\"AdaBoosting\", ada_best),(\"ExtraTrees\",ExtC_best),(\"RandomForest\",RFC_best),(\"GradientBoosting\",GBC_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","8b175cd0":"test_Survived_RFC = pd.Series(RFC_best.predict(test), name=\"RFC\")\ntest_Survived_ExtC = pd.Series(ExtC_best.predict(test), name=\"ExtC\")\ntest_Survived_SVMC = pd.Series(SVMC_best.predict(test), name=\"SVC\")\ntest_Survived_AdaC = pd.Series(ada_best.predict(test), name=\"Ada\")\ntest_Survived_GBC = pd.Series(GBC_best.predict(test), name=\"GBC\")\n\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_Survived_RFC,test_Survived_ExtC,test_Survived_AdaC,test_Survived_GBC, test_Survived_SVMC],axis=1)\n\n\ng= sns.heatmap(ensemble_results.corr(),annot=True)","645dcf0e":"votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(X_train, Y_train)","3fd4f59c":"test_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n\nresults = pd.concat([IDtest,test_Survived],axis=1)\n\nresults.to_csv(\"ensemble_python_voting.csv\",index=False)","159d0a12":"Age","6ff9ec6a":"#### 2.3.1 Filling Missing Value","8c88b94d":"## Feature importance of tree based classifiers\n\nIn order to see the most informative features for the prediction of passengers survival, i displayed the feature importance for the 4 tree based classifiers.","f8c938ef":"* **1 Introduction**\n* **2 Data**\n    * 2.1 load data\n    * 2.2 Outlier detection\n    * 2.3 Data Pre-Processing\n        * 2.3.1 Filling Missing Value\n* **3 Features**\n    * 3.1 Numerical values\n    * 3.2 Categorical values\n* **4 Feature Analysis & Engineering**\n    * 4.1 Name\n    * 4.2 Family Size\n    * 5.3 Cabin\n    * 5.4 Ticket\n* **5 ML Modeling**\n    * 5.1 Simple modeling\n        * 5.1.1 Cross validate models\n        * 5.1.2 Hyperparamater tunning for best models\n        * 5.1.3 Plot learning curves\n        * 5.1.4 Feature importance of the tree based classifiers\n    * 5.2 Ensemble modeling\n    * 5.3 Prediction","390748f9":"## Plot learning curves\n\nLearning curves are a good way to see the overfitting effect on the training set and the effect of the training size on the accuracy","6103f46f":"_\"Age\" is not correlated with \"Survived\", we can see that there is age categories of passengers that of have more or less chance to survive._","8d298865":"### 2.3 Data Pre-Processing","e31ae29c":"Fare","e82cf2c7":"### 2.2 Outlier detection","50a15834":"### 3.2 Categorical values","b7ff95c9":"# OUTLINE","415280a8":"## 2. Data","9c90a86c":"##### Age","77bb6010":"Pclass","cec855dc":"SibSP","533b4e08":"# MODELING","6157c37a":"_Small families have more chance to survive, more than single (Parch 0), medium (Parch 3,4) and large families (Parch 5,6 )._","0e2b4acf":"## 3. Features","a0786a7c":"Parch","25748d9f":"_Single passengers (0 SibSP) or with two other persons (SibSP 1 or 2) have more chance to survive._","c3e3b668":"## 1. Introduction \n<b> Hi My name is Yash Gupta. This is my oldest notbook. I have dcided to public my notebooks for gaining some medals. This notebook is highly inspired from other notebooks since it was one of my oldest work. Thanks for reading the notebook!!!!! <\/b>","4b17b64e":"### 2.1 Load Data","4331268d":"I compared 10 popular classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure.\n\n* SVC\n* Decision Tree\n* AdaBoost \n* Random Forest\n* Extra Trees\n* Gradient Boosting\n* Multiple layer perceprton (neural network)\n* KNN\n* Logistic regression\n* Linear Discriminant Analysis","3de43f15":"## Prediction","efc2138c":"## Hyperparameter tunning for best models\n\nI performed a grid search optimization for AdaBoost, ExtraTrees , RandomForest, GradientBoosting and SVC classifiers.\n\nI set the \"n_jobs\" parameter to 4 since i have 4 cpu . The computation time is clearly reduced.\n\nBut be carefull, this step can take a long time, i took me 15 min in total on 4 cpu.","5e76117f":"### 3.1 Numerical Values"}}