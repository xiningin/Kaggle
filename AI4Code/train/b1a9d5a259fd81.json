{"cell_type":{"6349b12f":"code","47a56db2":"code","695fd6b1":"code","a9160f24":"code","2e566ace":"code","4e4a12ee":"code","7afd3433":"code","0116b59a":"code","27137585":"code","95abd89e":"code","8dad50f9":"code","61247308":"code","b7a6b227":"code","808bd683":"code","dfa8fd02":"code","a38b4e41":"code","b4c6d2e7":"code","a79100f3":"code","9991bb65":"code","84d5dfa8":"code","428e669d":"code","5d5c1548":"code","073ce524":"code","693a1ddb":"code","e3f5498f":"code","40653fda":"code","74679b8d":"code","2c605e33":"code","038b8857":"code","ca17145c":"code","f954adf8":"code","a3b6e30b":"code","1e36de52":"code","cad6cd85":"code","d98c45cc":"code","a4a91c54":"code","1cf20256":"code","fa02f9c0":"code","35549fb1":"code","1c47a36c":"code","af267019":"code","0fc0b382":"code","1cf5939e":"code","589d8cf3":"code","105069c0":"code","c69c6d77":"code","91470013":"code","9756dc87":"code","b4d2ea91":"code","69a3435c":"code","88cf4c37":"code","65302039":"code","4eee5300":"code","5020f0d8":"code","3c3ad45f":"code","18b2be16":"code","619cd962":"code","8312dca9":"code","863c4aba":"markdown","3a01b103":"markdown","4c6e71ee":"markdown","64a35f69":"markdown","8fd50eed":"markdown","c8294173":"markdown","5ac45c8d":"markdown","69523121":"markdown","5e23418c":"markdown","e736b619":"markdown","bb810c9d":"markdown","86231884":"markdown","3ba0d18f":"markdown","3e1b5090":"markdown","2d8252df":"markdown","881baca6":"markdown","1aea45bd":"markdown","a1455df8":"markdown","af70c876":"markdown","0b7f4924":"markdown","85fc52c4":"markdown","368c8733":"markdown","c5aaa6a2":"markdown","948d76e9":"markdown","b0bac4ee":"markdown","1f92ce3a":"markdown","10c00c20":"markdown","765153ae":"markdown","9f0e744b":"markdown","1fb80ae6":"markdown","fd4b9ead":"markdown","f3da3049":"markdown","509e825d":"markdown"},"source":{"6349b12f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","47a56db2":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport warnings\n%matplotlib inline\nwarnings.filterwarnings('ignore')","695fd6b1":"df = pd.read_csv(\"..\/input\/bostoncsv\/Boston.csv\")\ndf.drop(columns=['Unnamed: 0'], axis=0, inplace=True)\ndf.head()","a9160f24":"# statistical info\ndf.describe()","2e566ace":"# datatype info\ndf.info()","4e4a12ee":"# check for null values\ndf.isnull().sum()","7afd3433":"# create box plots\nfig, ax = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\nax = ax.flatten()\n\nfor col, value in df.items():\n    sns.boxplot(y=col, data=df, ax=ax[index])\n    index += 1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)","0116b59a":"# create dist plot\nfig, ax = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\nax = ax.flatten()\n\nfor col, value in df.items():\n    sns.distplot(value, ax=ax[index])\n    index += 1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)","27137585":"cols = ['crim', 'zn', 'tax', 'black']\nfor col in cols:\n    # find minimum and maximum of that column\n    minimum = min(df[col])\n    maximum = max(df[col])\n    df[col] = (df[col] - minimum) \/ (maximum - minimum)","95abd89e":"fig, ax = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\nax = ax.flatten()\n\nfor col, value in df.items():\n    sns.distplot(value, ax=ax[index])\n    index += 1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)","8dad50f9":"# standardization\nfrom sklearn import preprocessing\nscalar = preprocessing.StandardScaler()\n\n# fit our data\nscaled_cols = scalar.fit_transform(df[cols])\nscaled_cols = pd.DataFrame(scaled_cols, columns=cols)\nscaled_cols.head()","61247308":"for col in cols:\n    df[col] = scaled_cols[col]","b7a6b227":"fig, ax = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\nax = ax.flatten()\n\nfor col, value in df.items():\n    sns.distplot(value, ax=ax[index])\n    index += 1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)","808bd683":"corr = df.corr()\nplt.figure(figsize=(20,10))\nsns.heatmap(corr, annot=True, cmap='Paired')","dfa8fd02":"sns.regplot(y=df['medv'], x=df['lstat'])","a38b4e41":"sns.regplot(y=df['medv'], x=df['rm'])","b4c6d2e7":"X = df.drop(columns=['medv', 'rad'], axis=1)\ny = df['medv']","a79100f3":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\ndef train(model, X, y):\n    # train the model\n    x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    model.fit(x_train, y_train)\n    \n    # predict the training set\n    pred = model.predict(x_test)\n    \n    # perform cross-validation\n    cv_score = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=5)\n    cv_score = np.abs(np.mean(cv_score))\n    \n    print(\"Model Report\")\n    print(\"MSE:\",mean_squared_error(y_test, pred))\n    print('CV Score:', cv_score)","9991bb65":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression(normalize=True)\ntrain(model, X, y)\ncoef = pd.Series(model.coef_, X.columns).sort_values()\ncoef.plot(kind='bar', title='Model Coefficients')","84d5dfa8":"from sklearn.tree import DecisionTreeRegressor\nmodel = DecisionTreeRegressor()\ntrain(model, X, y)\ncoef = pd.Series(model.feature_importances_, X.columns).sort_values(ascending=False)\ncoef.plot(kind='bar', title='Feature Importance')","428e669d":"from sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor()\ntrain(model, X, y)\ncoef = pd.Series(model.feature_importances_, X.columns).sort_values(ascending=False)\ncoef.plot(kind='bar', title='Feature Importance')","5d5c1548":"from sklearn.ensemble import ExtraTreesRegressor\nmodel = ExtraTreesRegressor()\ntrain(model, X, y)\ncoef = pd.Series(model.feature_importances_, X.columns).sort_values(ascending=False)\ncoef.plot(kind='bar', title='Feature Importance')","073ce524":"import xgboost as xgb\nmodel = xgb.XGBRegressor()\ntrain(model, X, y)\ncoef = pd.Series(model.feature_importances_, X.columns).sort_values(ascending=False)\ncoef.plot(kind='bar', title='Feature Importance')","693a1ddb":"# Splitting to training and testing data\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 4)","e3f5498f":"# Import library for Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\n# Create a Linear regressor\nlm = LinearRegression()\n\n# Train the model using the training sets \nlm.fit(X_train, y_train)","40653fda":"# Value of y intercept\nlm.intercept_","74679b8d":"#Converting the coefficient values to a dataframe\ncoeffcients = pd.DataFrame([X_train.columns,lm.coef_]).T\ncoeffcients = coeffcients.rename(columns={0: 'Attribute', 1: 'Coefficients'})\ncoeffcients","2c605e33":"# Model prediction on train data\ny_pred = lm.predict(X_train)","038b8857":"# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","ca17145c":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()","f954adf8":"# Checking residuals\nplt.scatter(y_pred,y_train-y_pred)\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","a3b6e30b":"# Checking Normality of errors\nsns.distplot(y_train-y_pred)\nplt.title(\"Histogram of Residuals\")\nplt.xlabel(\"Residuals\")\nplt.ylabel(\"Frequency\")\nplt.show()","1e36de52":"# Predicting Test data with the model\ny_test_pred = lm.predict(X_test)","cad6cd85":"# Model Evaluation\nacc_linreg = metrics.r2_score(y_test, y_test_pred)\nprint('R^2:', acc_linreg)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_test_pred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_test_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","d98c45cc":"# Import Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create a Random Forest Regressor\nreg = RandomForestRegressor()\n\n# Train the model using the training sets \nreg.fit(X_train, y_train)","a4a91c54":"# Model prediction on train data\ny_pred = reg.predict(X_train)","1cf20256":"# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","fa02f9c0":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()","35549fb1":"# Checking residuals\nplt.scatter(y_pred,y_train-y_pred)\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","1c47a36c":"# Predicting Test data with the model\ny_test_pred = reg.predict(X_test)","af267019":"# Model Evaluation\nacc_rf = metrics.r2_score(y_test, y_test_pred)\nprint('R^2:', acc_rf)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_test_pred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_test_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","0fc0b382":"# Import XGBoost Regressor\nfrom xgboost import XGBRegressor\n\n#Create a XGBoost Regressor\nreg = XGBRegressor()\n\n# Train the model using the training sets \nreg.fit(X_train, y_train)","1cf5939e":"# Model prediction on train data\ny_pred = reg.predict(X_train)","589d8cf3":"# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","105069c0":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()\n","c69c6d77":"# Checking residuals\nplt.scatter(y_pred,y_train-y_pred)\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","91470013":"#Predicting Test data with the model\ny_test_pred = reg.predict(X_test)","9756dc87":"# Model Evaluation\nacc_xgb = metrics.r2_score(y_test, y_test_pred)\nprint('R^2:', acc_xgb)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_test_pred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_test_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","b4d2ea91":"# Creating scaled set to be used in model to improve our results\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","69a3435c":"# Import SVM Regressor\nfrom sklearn import svm\n\n# Create a SVM Regressor\nreg = svm.SVR()","88cf4c37":"# Train the model using the training sets \nreg.fit(X_train, y_train)","65302039":"# Model prediction on train data\ny_pred = reg.predict(X_train)","4eee5300":"# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","5020f0d8":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()","3c3ad45f":"# Checking residuals\nplt.scatter(y_pred,y_train-y_pred)\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","18b2be16":"# Predicting Test data with the model\ny_test_pred = reg.predict(X_test)","619cd962":"# Model Evaluation\nacc_svm = metrics.r2_score(y_test, y_test_pred)\nprint('R^2:', acc_svm)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_test_pred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_test_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","8312dca9":"models = pd.DataFrame({\n    'Model': ['Linear Regression', 'Random Forest', 'XGBoost', 'Support Vector Machines'],\n    'R-squared Score': [acc_linreg*100, acc_rf*100, acc_xgb*100, acc_svm*100]})\nmodels.sort_values(by='R-squared Score', ascending=False)","863c4aba":"* max_depth (int) \u2013 Maximum tree depth for base learners.\n\n* learning_rate (float) \u2013 Boosting learning rate (xgb\u2019s \u201ceta\u201d)\n\n* n_estimators (int) \u2013 Number of boosted trees to fit.\n\n* gamma (float) \u2013 Minimum loss reduction required to make a further partition on a leaf node of the tree.\n\n* min_child_weight (int) \u2013 Minimum sum of instance weight(hessian) needed in a child.\n\n* subsample (float) \u2013 Subsample ratio of the training instance.\n\n* colsample_bytree (float) \u2013 Subsample ratio of columns when constructing each tree.\n\n* objective (string or callable) \u2013 Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n\n* nthread (int) \u2013 Number of parallel threads used to run xgboost. (Deprecated, please use n_jobs)\n\n* scale_pos_weight (float) \u2013 Balancing of positive and negative weights.","3a01b103":"# Input Split :","4c6e71ee":"**Hence XGBoost Regression works the best for this dataset.**","64a35f69":"* \ud835\udc45^2 : It is a measure of the linear relationship between X and Y. It is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable.\n\n* Adjusted \ud835\udc45^2 :The adjusted R-squared compares the explanatory power of regression models that contain different numbers of predictors.\n\n* MAE : It is the mean of the absolute value of the errors. It measures the difference between two continuous variables, here actual and predicted values of y. \n\n* MSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. \n\n* RMSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. ","8fd50eed":"**Train the model :**","c8294173":"# Import modules :","5ac45c8d":"There is no pattern visible in this plot and values are distributed equally around zero. So Linearity assumption is satisfied","69523121":"**Training the model :**","5e23418c":"**For Test data :**","e736b619":"**For Test data :**","bb810c9d":"**Model Evaluation :**","86231884":"# Evaluation and comparision of all the models :","3ba0d18f":"# Boston House Price Prediction : \n\n**Author : Saumy**\n\n**Date : 24th December 2021**\n\n**The problem that we are going to solve here is that given a set of features that describe a house in Boston, our machine learning model must predict the house price**\n\n# Dataset Information :\n**Boston House Prices Dataset was collected in 1978 and has 506 entries with 14 attributes or features for homes from various suburbs in Boston.**\n\n**Boston Housing Dataset Attribute Information (in order):**\n\n* CRIM : per capita crime rate by town\n* ZN : proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS : proportion of non-retail business acres per town\n* CHAS : Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n* NOX : nitric oxides concentration (parts per 10 million)\n* RM : average number of rooms per dwelling\n* AGE : proportion of owner-occupied units built prior to 1940\n* DIS : weighted distances to five Boston employment centres\n* RAD : index of accessibility to radial highways\n* TAX : full-value property-tax rate per $10,000\n\n* PTRATIO : pupil-teacher ratio by town\n\n* B : 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n\n* LSTAT : % lower status of the population\n\n* MEDV : Median value of owner-occupied homes in $1000's","3e1b5090":"# Exploratory Data Analysis :","2d8252df":"**Model Evaluation :**","881baca6":"**Train the model :**","1aea45bd":"Here the residuals are normally distributed. So normality assumption is satisfied","a1455df8":"**Model Evaluation :**","af70c876":"**For test data :**","0b7f4924":"# Min-Max Normalization :","85fc52c4":"Here the model evaluations scores are almost matching with that of train data. So the model is not overfitting.","368c8733":"# Linear regression :","c5aaa6a2":"# Model Training :","948d76e9":"# Random Forest Regressor :","b0bac4ee":"**Training the model :**","1f92ce3a":"* C : float, optional (default=1.0): The penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly.\n\n* kernel : string, optional (default='rbf\u2019): kernel parameters selects the type of hyperplane used to separate the data. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed\u2019 or a callable.\n\n* degree : int, optional (default=3): Degree of the polynomial kernel function (\u2018poly\u2019). Ignored by all other kernels.\n\n* gamma : float, optional (default='auto\u2019): It is for non linear hyperplanes. The higher the gamma value it tries to exactly fit the training data set. Current default is 'auto' which uses 1 \/ n_features.\n\n* coef0 : float, optional (default=0.0): Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'.\n\n* shrinking : boolean, optional (default=True): Whether to use the shrinking heuristic.","10c00c20":"# Preprocessing the dataset :","765153ae":"**Model Evaluation :**","9f0e744b":"**For test data :**","1fb80ae6":"# XGBoost Regressor :","fd4b9ead":"# Loading the dataset :","f3da3049":"# SVM Regressor :","509e825d":"# Coorelation Matrix :"}}