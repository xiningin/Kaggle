{"cell_type":{"ddecdad6":"code","fd055255":"code","38b9ac79":"code","48c7d3bd":"code","7a646fa8":"code","75f299fd":"code","916d2577":"code","cc7116a2":"code","98017d5e":"code","99ed8ed8":"code","b1582861":"code","10e57d7e":"code","e8fec779":"code","4412f3a6":"code","8de686d2":"code","01ab2f4e":"code","e4f592fa":"code","ea6698cb":"code","f0455ecb":"code","a3c33980":"code","b9f854f5":"code","30244655":"code","93085359":"code","22ecf50d":"code","5195063d":"code","7b8c9d9a":"code","1870e6e0":"code","972eb358":"code","6d529149":"code","52eefa99":"code","7df654d7":"code","2839f7dd":"code","64072d95":"code","5148f4cc":"code","67fd8575":"code","c34dcb56":"code","019a36d5":"code","0b4aa070":"code","88ca17e2":"code","e68ab003":"code","9180452e":"code","42698b35":"code","4bc0d698":"code","b93ea310":"code","a933f733":"code","e37066d9":"code","156eab9e":"code","02437c91":"markdown","988c46db":"markdown","8e80894d":"markdown","a8aa8ae6":"markdown","94b2e0a2":"markdown","7f37216e":"markdown","b58b1933":"markdown","a601788a":"markdown","196b95c4":"markdown","a38f01d9":"markdown","37ca02e0":"markdown","e8a0dea1":"markdown","a98936ce":"markdown","7345f5d6":"markdown","5b43a49b":"markdown","53baa2a2":"markdown","950c206b":"markdown","4e2ea04a":"markdown","8bb7ba14":"markdown","c8bb648a":"markdown","0a0a4e10":"markdown","7e82d220":"markdown","7ec06eeb":"markdown","246c07da":"markdown","a23cd5bf":"markdown","7dce8378":"markdown","f379ae98":"markdown","6c9b9937":"markdown","ae1e2115":"markdown","cdf39e58":"markdown","67fbb4b7":"markdown","f9129aa4":"markdown","4e6f34d2":"markdown","fabc9d0b":"markdown"},"source":{"ddecdad6":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, ElasticNet, Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom scipy.stats import skew\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.float_format', lambda x: \"%3f\"%x)\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n\n# ignore Deprecation Warning\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","fd055255":"# load dataset\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain_shape=train.shape[0]\nprint(train.shape)\ntrain.head()","38b9ac79":"# Load the dataset for submission (the one on which our model will be evaluated by Kaggle)\n# it contains exactly the same variables, but not the target\n\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest.head()","48c7d3bd":"df_columns = test.columns\n\ny = train.SalePrice\ntrain = train[df_columns]\n\ndf = pd.concat([train, test])","7a646fa8":"# divide the data into numerical (\"num\") and categorical (\"cat\") features\nnum = list( test.loc[:,test.dtypes != 'object'].drop('Id',axis=1).columns.values )\ncat = list( test.loc[:,test.dtypes == 'object'].columns.values )","75f299fd":"# Find out how many missing values there are for the numerical and categorical features\n# Data that needs good replacement of NaNs\nnan_data = df[num].isnull().sum()\nnan_data = nan_data[nan_data > 0]\nnan_data = nan_data.sort_values(ascending=False)\nprint(nan_data)\nprint('**'*40)\nnan_data = df[cat].isnull().sum()\nnan_data = nan_data[nan_data > 0]\nnan_data = nan_data.sort_values(ascending=False)\nprint(nan_data)\n","916d2577":"# Filling missing values for numerical features. Most of the NAN should mean that \n# the corresponding facillity\/structure doesn't exist, so I use zero for most cases\n\n# Use zero\ndf.MasVnrArea.fillna(0, inplace=True)\ndf.BsmtHalfBath.fillna(0, inplace=True)\ndf.BsmtFullBath.fillna(0, inplace=True)\ndf.GarageArea.fillna(0, inplace=True)\ndf.GarageCars.fillna(0, inplace=True)\ndf.TotalBsmtSF.fillna(0, inplace=True)\ndf.BsmtUnfSF.fillna(0, inplace=True)\ndf.BsmtFinSF2.fillna(0, inplace=True)\ndf.BsmtFinSF1.fillna(0, inplace=True)\n\n\n# NAN should mean no garage. I temporarily use yr = 0 here. Will come back to this later. \ndf.GarageYrBlt.fillna(0, inplace=True)","cc7116a2":"# leaving these as None is the best value for it - not missing data - just not applicable\nnan_data = df.isnull().sum().sort_values(ascending=False)\nnan_data[nan_data > 0]","98017d5e":"# Fill Nulls with 'NA'\ndf.PoolQC.fillna('NA', inplace=True)\ndf.MiscFeature.fillna('NA', inplace=True)\ndf.Alley.fillna('NA', inplace=True)\ndf.Fence.fillna('NA', inplace=True)\ndf.FireplaceQu.fillna('NA', inplace=True)\ndf.LotFrontage.fillna(0., inplace=True)\ndf.GarageCond.fillna('NA', inplace=True)\ndf.GarageType.fillna('NA', inplace=True)\ndf.GarageFinish.fillna('NA', inplace=True)\ndf.GarageQual.fillna('NA', inplace=True)\ndf.BsmtExposure.fillna('NA', inplace=True)\ndf.BsmtFinType2.fillna('NA', inplace=True)\ndf.BsmtFinType1.fillna('NA', inplace=True)\ndf.BsmtCond.fillna('NA', inplace=True)\ndf.BsmtQual.fillna('NA', inplace=True)\ndf.MasVnrType.fillna('None', inplace=True)\n\n# Fill Nulls with 0\ndf.MasVnrArea.fillna(0, inplace=True)\ndf.GarageArea.fillna(0, inplace=True)\ndf.BsmtUnfSF.fillna(0, inplace=True)\ndf.BsmtFinSF2.fillna(0, inplace=True)\ndf.BsmtFullBath.fillna(0, inplace=True)\ndf.BsmtHalfBath.fillna(0, inplace=True)\ndf.BsmtFinSF1.fillna(0, inplace=True)\ndf.KitchenQual.fillna('TA', inplace=True)\ndf.Functional.fillna(0, inplace=True)\ndf.Utilities.fillna(0, inplace=True)\n# df.MSZoning.fillna(0, inplace=True)\ndf.Exterior2nd.fillna(df.Exterior2nd.mode().values[0], inplace=True)\ndf.Electrical.fillna(df.Electrical.mode().values[0], inplace=True)\ndf.Exterior1st.fillna(df.Exterior1st.mode().values[0], inplace=True)\ndf.GarageCars.fillna(0, inplace=True)\ndf.TotalBsmtSF.fillna(0, inplace=True)\ndf.SaleType.fillna(df.SaleType.mode().values[0], inplace=True)\ndf.GarageType.fillna(\"NoGarage\", inplace=True)","99ed8ed8":"# We should have only MSZoning Left with 4 nulls\nnan_data = df.isnull().sum().sort_values(ascending=False)\nnan_data[nan_data > 0]\n","b1582861":"# Handling missing values in MSZoning\nfor i in df.Neighborhood.unique():\n    if (df.MSZoning[df.Neighborhood == i].isnull().sum()) > 0:\n        df.loc[df.Neighborhood == i,'MSZoning'] = df.loc[df.Neighborhood == i,'MSZoning'].fillna(df.loc[df.Neighborhood == i,'MSZoning'].mode()[0])","10e57d7e":"# Should return an empty series\nnan_data = df.isnull().sum().sort_values(ascending=False)\nnan_data[nan_data > 0]","e8fec779":"# Mapping Categorical variable with corresponding ranks -  Text to Num\n\ndf['ExterQual'] = df.ExterQual.replace(dict(Ex=5, Gd=4, TA=3, Fa=2, Po=1))\ndf['ExterCond'] = df.ExterCond.replace(dict(Ex=5, Gd=4, TA=3, Fa=2, Po=1))\ndf['BsmtQual'] = df.BsmtQual.replace(dict(Ex=5, Gd=4, TA=3, Fa=2, Po=1, NA=0))\ndf['BsmtCond'] = df.BsmtCond.replace(dict(Ex=5, Gd=4, TA=3, Fa=2, Po=1, NA=0))\ndf['BsmtExposure'] = df.BsmtExposure.replace(dict(Gd=4, Av=3, Mn=2, No=1, NA=0))\ndf['BsmtFinType1'] = df.BsmtFinType1.replace(dict(GLQ=6, ALQ=5, BLQ=4, Rec=3, LwQ=2, Unf=1, NA=0))\ndf['BsmtFinType2'] = df.BsmtFinType2.replace(dict(GLQ=6, ALQ=5, BLQ=4, Rec=3, LwQ=2, Unf=1, NA=0))\ndf['HeatingQC'] = df.HeatingQC.replace(dict(Ex=5, Gd=4, TA=3, Fa=2, Po=1))\ndf['KitchenQual'] = df.KitchenQual.replace(dict(Ex=5, Gd=4, TA=3, Fa=2, Po=1))\ndf['Functional'] = df.Functional.replace(dict(Typ=8, Min1=7, Min2=6, Mod=5, Maj1=4, Maj2=3, Sev=2, Sal=1))\ndf['FireplaceQu'] = df.FireplaceQu.replace(dict(Ex=5, Gd=4, TA=3, Fa=2, Po=1, NA=0))\ndf['GarageQual'] = df.GarageQual.replace(dict(Ex=5, Gd=4, TA=3, Fa=2, Po=1, NA=0))\ndf['GarageCond'] = df.GarageCond.replace(dict(Ex=5, Gd=4, TA=3, Fa=2, Po=1, NA=0))\ndf['PoolQC'] = df.PoolQC.replace(dict(Ex=5, Gd=4, TA=3, Fa=2, NA=1))\ndf['Fence'] = df.Fence.replace(dict(GdPrv=5, MnPrv=4, GdWo=3, MnWw=2, NA=1))\ndf['Alley'] = df.Alley.replace(dict(Grvl=3, Pave=2, NA=1))\ndf['LotShape'] = df.LotShape.replace(dict(Reg=4, IR1=3, IR2=2, IR3=1))\ndf['Utilities'] = df.Utilities.replace(dict(AllPub=4, NoSewr=3, NoSeWa=2, ELO=1))\ndf['LotConfig'] = df.LotConfig.replace(dict(Inside=5, Corner=4, CulDSac=3, FR2=2, FR3=1))\ndf['LandContour'] = df.LandContour.replace(dict(Lvl=4, Bnk=3, HLS=2, Low=1))\ndf['LandSlope'] = df.LandSlope.replace(dict(Gtl=3, Mod=2, Sev=1))\ndf['MSZoning']=df.MSZoning.replace(({'RL':1, 'RM':2, 'C (all)':3, 'FV':4, 'RH':5}))\ndf['Street']=df.Street.replace(dict(Grvl=2, Pave=1))\n# df['Neighborhood'] = df.Neighborhood.replace(dict(NAmes=1,CollgCr=2,OldTown=3,Edwards=4,Somerst=5,Gilbert=6,\n# \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 NridgHt=7,Sawyer=8,NWAmes=9,SawyerW=10,BrkSide=11,Crawfor=12,\n# \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Mitchel=13,NoRidge=14,Timber=15,IDOTRR=16,ClearCr=17,StoneBr=18,\n# \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 SWISU=19,MeadowV=20,Blmngtn=21,BrDale=22,Veenker=23,NPkVill=24,Blueste=25))","4412f3a6":"#Add New Features\ndf.loc[(df.PoolArea>0), ['MiscFeature']] = 'Pool'\ndf.loc[(df.PoolArea>0), ['MiscVal']] = df.loc[(df.PoolArea>0),['MiscVal', 'PoolArea']].apply(lambda x: (x.MiscVal + x.PoolArea), axis=1)\n\ndf['TotalExtraPoints'] = df.HeatingQC + df.PoolQC + df.FireplaceQu + df.KitchenQual\ndf['TotalPoints'] = (df.ExterQual + df.FireplaceQu + df.GarageQual + df.KitchenQual +\n                      df.BsmtQual + df.BsmtExposure + df.BsmtFinType1 + df.PoolQC + \n                      df.ExterCond + df.BsmtCond + df.GarageCond + df.OverallCond +\n                      df.BsmtFinType2 + df.HeatingQC ) + df.OverallQual**2\n\n\ndf['GarageArea_x_Car'] = df.GarageArea * df.GarageCars\n\ndf['TotalBsmtSF_x_Bsm'] = df.TotalBsmtSF * df['1stFlrSF']\n\n# We don\u00b4t have a feature with all construct area, maybe it is an interesting feature to create.\ndf['ConstructArea'] = (df.TotalBsmtSF + df.WoodDeckSF + df.GrLivArea +df['3SsnPorch']+\n                       df.OpenPorchSF + df.ScreenPorch + df.EnclosedPorch +\n                       df.MasVnrArea + df.GarageArea + df.PoolArea )\n\n#df['TotalArea'] = df.ConstructArea + df.LotArea\n\ndf['Garage_Newest'] = df.YearBuilt > df.GarageYrBlt\ndf.Garage_Newest = df.Garage_Newest.apply(lambda x: 1 if x else 0)\n\ndf['TotalPorchSF'] = df.OpenPorchSF + df.EnclosedPorch + df.ScreenPorch + df.WoodDeckSF+df['3SsnPorch']\ndf.EnclosedPorch = df.EnclosedPorch.apply(lambda x: 1 if x else 0)\n\ndf['LotAreaMultSlope'] = df.LotArea * df.LandSlope\n\n\ndf['BsmtSFPoints'] = (df.BsmtQual**2 + df.BsmtCond + df.BsmtExposure +\n                      df.BsmtFinType1 + df.BsmtFinType2)\n\n\ndf['BsmtSFMultPoints'] = df.TotalBsmtSF * (df.BsmtQual**2 + df.BsmtCond + df.BsmtExposure +\n                                           df.BsmtFinType1 + df.BsmtFinType2)\n\ndf['TotBathrooms'] = df.FullBath + (df.HalfBath*0.5) + df.BsmtFullBath + (df.BsmtHalfBath*0.5)\ndf.FullBath = df.FullBath.apply(lambda x: 1 if x else 0)\ndf.HalfBath = df.HalfBath.apply(lambda x: 1 if x else 0)\ndf.BsmtFullBath = df.BsmtFullBath.apply(lambda x: 1 if x else 0)\ndf.BsmtHalfBath = df.BsmtHalfBath.apply(lambda x: 1 if x else 0)","8de686d2":"# Create new features from existing data\ndf['TotalBath'] = df.FullBath + df.BsmtFullBath + 0.5 * (df.BsmtHalfBath + df.HalfBath)\ndf['TotalSF'] = df.TotalBsmtSF + df.GrLivArea\n\ndf[\"IsRegularLotShape\"] = (df.LotShape == \"Reg\") * 1\ndf[\"IsLandLevel\"] = (df.LandContour == \"Lvl\") * 1\ndf[\"IsLandSlopeGntl\"] = (df.LandSlope == \"Gtl\") * 1\ndf[\"IsElectricalSBrkr\"] = (df.Electrical == \"SBrkr\") * 1\ndf[\"IsGarageDetached\"] = (df.GarageType == \"Detchd\") * 1\ndf[\"IsPavedDrive\"] = (df.PavedDrive == \"Y\") * 1\ndf[\"HasShed\"] = (df.MiscFeature == \"Shed\") * 1\n\n# these features jumped my score from 0.11875 to 0.11833\n\ndf.loc[df.Neighborhood == 'NridgHt', \"Neighborhood_Good\"] = 1\ndf.loc[df.Neighborhood == 'Crawfor', \"Neighborhood_Good\"] = 1\ndf.loc[df.Neighborhood == 'StoneBr', \"Neighborhood_Good\"] = 1\ndf.loc[df.Neighborhood == 'Somerst', \"Neighborhood_Good\"] = 1\ndf.loc[df.Neighborhood == 'NoRidge', \"Neighborhood_Good\"] = 1\ndf[\"Neighborhood_Good\"].fillna(0, inplace=True)\n\n\n# jumped the score from 0.11833 to 0.11819\ndf[\"SaleCondition_PriceDown\"] = df.SaleCondition.replace({'Abnorml': 1, 'Alloca': 1, 'AdjLand': 1, 'Family': 1, 'Normal': 0, 'Partial': 0})\n\n# House completed before sale or not\ndf[\"BoughtOffPlan\"] = df.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\n\n\narea_cols = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n             'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF',\n             'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'LowQualFinSF', 'PoolArea' ]\ndf[\"TotalArea\"] = df[area_cols].sum(axis=1)\n\n\n# If YearRemodAdd != YearBuilt, then a remodeling took place at some point.\ndf[\"Remodeled\"] = (df[\"YearRemodAdd\"] != df[\"YearBuilt\"]) * 1\n\n# Did a remodeling happen in the year the house was sold?\ndf[\"RecentRemodel\"] = (df[\"YearRemodAdd\"] == df[\"YrSold\"]) * 1\n\ndf[\"Age\"] = 2020 - df[\"YearBuilt\"]\ndf[\"TimeSinceSold\"] = 2020 - df[\"YrSold\"]\n","01ab2f4e":"# Create new features\n# Simplify existing features - squash into smaller groups\n# existing features\ndf['OverallQual_simple'] = df.OverallQual.replace({1:1, 2:1, 3:1,4:2, 5:2, 6:2, 7:3, 8:3, 9:3, 10:3})\ndf['OverallCond_simple'] = df.OverallCond.replace({1:1, 2:1, 3:1,4:2, 5:2, 6:2,7:3, 8:3, 9:3, 10:3})\n# convert from categorical to ordinal with smaller groups\ndf['ExterQual_simple'] = df.ExterQual.replace({5:3, 4:3, 3:2, 2:2, 1:1})\ndf['ExterCond_simple'] = df.ExterCond.replace({5:3, 4:3, 3:2, 2:2, 1:1})\ndf['BsmtQual_simple'] = df.BsmtQual.replace({5:3, 4:3, 3:2, 2:2, 1:1})\ndf['BsmtCond_simple'] = df.BsmtCond.replace({5:3, 4:3, 3:2, 2:2, 1:1})\ndf['BsmtFinType1_simple'] = df.BsmtFinType1.replace({6:3, 5:3, 4:2, 3:2, 2:1, 1:1})\ndf['BsmtFinType2_simple'] = df.BsmtFinType2.replace({6:3, 5:3, 4:2, 3:2, 2:1, 1:1})\ndf['HeatingQC_simple'] = df.HeatingQC.replace({5:3, 4:3, 3:2, 2:2, 1:1})\ndf['KitchenQual_simple'] = df.KitchenQual.replace({5:3, 4:3, 3:2, 2:2, 1:1})\ndf['Functional_simple'] = df.Functional.replace({8:4, 7:3, 7:3, 6:3, 5:2,4:2, 3:1, 2:1})\ndf['GarageQual_simple'] = df.GarageQual.replace({5:3, 4:3, 3:2, 2:2, 1:1})\ndf['GarageCond_simple'] = df.GarageCond.replace({5:3, 4:3, 3:2, 2:2, 1:1})\ndf['PoolQC_simple'] = df.PoolQC.replace({5:3, 4:3, 3:2, 2:2, 1:1})\ndf['FireplaceQu_simple'] = df.FireplaceQu.replace({5:3, 4:3, 3:2, 2:2, 1:1})","e4f592fa":"df['YrBltAndRemod']=df['YearBuilt']+df['YearRemodAdd']\ndf['TotalSF_SF']=df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n\ndf['Total_sqr_footage'] = (df['BsmtFinSF1'] + df['BsmtFinSF2'] +df['1stFlrSF'] + df['2ndFlrSF'])\n\n#df['Total_Bathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) +df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n\n#df['Total_porch_sf'] = (df['OpenPorchSF'] + df['3SsnPorch'] +df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF'])\ndf['haspool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndf['has2ndfloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndf['hasgarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndf['hasbsmt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndf['hasfireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n","ea6698cb":"# Converting categorical data with numerical features into categories \n# NOTE:- pd.get_dummies will one hot encode these columns now\n# MSSubClass, MoSold\ndf['MSSubClass'] = df.MSSubClass.astype('category')\ndf['MoSold'] = df.MoSold.astype('category')\ndf['GarageYrBlt'] = df.GarageYrBlt.astype('category')\ndf['Neighborhood'] = df.Neighborhood.astype('category')\ndf['LotConfig'] = df.LotConfig.astype('category')\ndf['Condition1'] = df.Condition1.astype('category')\ndf['Condition2'] = df.Condition1.astype('category')\ndf['BldgType'] = df.BldgType.astype('category')\ndf['HouseStyle'] = df.HouseStyle.astype('category')\ndf['RoofStyle'] = df.RoofStyle.astype('category')\ndf['Exterior1st'] = df.Exterior1st.astype('category')\ndf['MasVnrArea'] = df.MasVnrArea.astype('category')\ndf['RoofMatl'] = df.RoofMatl.astype('category')\ndf['MasVnrType'] = df.MasVnrType.astype('category')\ndf['Foundation'] = df.Foundation.astype('category')\ndf['SaleCondition'] = df.SaleCondition.astype('category')\ndf['SaleType'] = df.SaleType.astype('category')\ndf['Exterior2nd'] = df.Exterior2nd.astype('category')\ndf['Heating'] = df.Heating.astype('category')\ndf['Electrical'] = df.Electrical.astype('category')\ndf['BedroomAbvGr'] = df.BedroomAbvGr.astype('category')\ndf['KitchenAbvGr'] = df.KitchenAbvGr.astype('category')\ndf['TotRmsAbvGrd'] = df.TotRmsAbvGrd.astype('category')\ndf['Fireplaces'] = df.Fireplaces.astype('category')\ndf['GarageType'] = df.GarageType.astype('category')\ndf['GarageFinish'] = df.GarageFinish.astype('category')\ndf['GarageCars'] = df.GarageCars.astype('category')\ndf['PavedDrive'] = df.PavedDrive.astype('category')","f0455ecb":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = df.select_dtypes(include = [\"object\", \"category\"]).columns\nnumerical_features = df.select_dtypes(exclude = [\"object\", \"category\"]).columns\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\ndf_num = df[numerical_features]\ndf_cat = df[categorical_features]","a3c33980":"## Check for missing values\ndf_num.isnull().values.sum(), df_cat.isnull().values.sum()","b9f854f5":"skewness = df_num.apply(skew)\nskewness = skewness[abs(skewness) > 0.5]\nskewed_features = skewness.index\ndf_num[skewed_features] = df_num[skewed_features].applymap(np.log1p)","30244655":"# # print the skewness of each numerical feature\nfor i in df_num.columns.tolist():\n    print(i+': {}'.format(round(skew(df[i]),2)))","93085359":"# One hot encoding the categorical variables\n\ndf_cat = pd.get_dummies(df_cat)","22ecf50d":"df = pd.concat([df_num, df_cat], axis=1)\n# scaler = StandardScaler()\n# df[numerical_features] = scaler.fit_transform(df[numerical_features])\n\nX_train = df[:train_shape]\nX_test = df[train_shape:]\nprint(X_train.shape, X_test.shape)","5195063d":"# log transform the sale price\n\n\n## After prdictions are made - we need to convert them back to exponential form\ny = np.log1p(y)\n# y.rename(columns={ y.columns[0]: \"y\" }, inplace = True)","7b8c9d9a":"from sklearn.model_selection import KFold\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X_train):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","1870e6e0":"alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","972eb358":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.svm import SVR\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","6d529149":"from sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt',\n                                min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)","52eefa99":"# !pip install lightgbm\nfrom lightgbm import LGBMRegressor\nlightgbm = LGBMRegressor(objective='regression', \n                           num_leaves=4,\n                           learning_rate=0.01, \n                           n_estimators=5000,\n                           max_bin=200, \n                           bagging_fraction=0.75,\n                           bagging_freq=5, \n                           bagging_seed=7,\n                           feature_fraction=0.2,\n                           feature_fraction_seed=7,\n                           verbose=-1)","7df654d7":"import xgboost as xgb\nxgboost = xgb.XGBRegressor(learning_rate=0.01,n_estimators=3460,max_depth=3, min_child_weight=0,\n                           gamma=0, subsample=0.7,colsample_bytree=0.7,objective='reg:linear',\n                           nthread=-1,scale_pos_weight=1,seed=27,reg_alpha=0.00006)","2839f7dd":"from mlxtend.regressor import StackingCVRegressor\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,use_features_in_secondary=True)\n","64072d95":"import datetime\nscore = cv_rmse(ridge)\nprint(\"RIDGE: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.datetime.now(), )\n\nscore = cv_rmse(lasso)\nprint(\"LASSO: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.datetime.now(), )\n\nscore = cv_rmse(elasticnet)\nprint(\"elastic net: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.datetime.now(), )\n\nscore = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.datetime.now(), )\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.datetime.now(), )\n\nscore = cv_rmse(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.datetime.now(), )\n\nscore = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.datetime.now(), )\n\n\nprint('Done')","5148f4cc":"stack_gen_model = stack_gen.fit(X_train, y)\n\npreds_sg = stack_gen_model.predict(np.array(X_test))\n\nsg_preds = pd.DataFrame(dict(SalePrice=np.expm1(preds_sg), Id=test.Id))\nsg_preds.head()","67fd8575":"model_ridge = ridge\nmodel_ridge.fit(X_train, y)\n\npreds_r = model_ridge.predict(X_test)\n\nridge_preds = pd.DataFrame(dict(SalePrice=np.expm1(preds_r), Id=test.Id))\nridge_preds.head()","c34dcb56":"model_lasso = lasso\nmodel_lasso.fit(X_train, y)\n\npreds_l = model_lasso.predict(X_test)\n\nlasso_preds = pd.DataFrame(dict(SalePrice=np.expm1(preds_l), Id=test.Id))\nlasso_preds.head()","019a36d5":"# import xgboost as xgb\n\nregr = xgboost\nregr.fit(X_train,y)\ny_pred_xgb = regr.predict(X_test)\nxgb_preds = pd.DataFrame(dict(SalePrice=np.expm1(y_pred_xgb), Id=test.Id))\n\nxgb_preds.head()","0b4aa070":"# residuals from best performing method for validation\ny_pred_xgb_train = regr.predict(X_train)\nprint('XGB train mse: {}'.format(mean_squared_error(y, y_pred_xgb_train)))\npred_to_plot = pd.DataFrame(dict(best_pred=preds_l, new=np.expm1(y_pred_xgb)))\npred_to_plot.plot(x='best_pred', y='new', kind='scatter')","88ca17e2":"light_gbm_model = lightgbm.fit(X_train, y)\nlightgbm_predictions = light_gbm_model.predict(X_test)\npred_lgbm = pd.DataFrame(dict(SalePrice=np.expm1(lightgbm_predictions), Id=test.Id))\n\npred_lgbm.head()","e68ab003":"gbr_model = gbr.fit(X_train, y)\ngbr_predictions = gbr_model.predict(X_test)\npred_gbr = pd.DataFrame(dict(SalePrice=np.expm1(gbr_predictions), Id=test.Id))\n\npred_gbr.head()","9180452e":"# from sklearn.svm import SVR\nSVR_model = svr\nSVR_model.fit(X_train, y)\n\n# to evaluate the models\nfrom sklearn.metrics import mean_squared_error\n\npred_svr = SVR_model.predict(X_train)\nprint('SVR train mse: {}'.format(mean_squared_error(y, pred_svr)))\npred_svr = SVR_model.predict(X_test)\npreds_svr = pd.DataFrame(dict(SalePrice=np.expm1(pred_svr), Id=test.Id))\n\npreds_svr.head()","42698b35":"elastic_net_model = elasticnet.fit(X_train, y)\nelastic_net_predictions = elastic_net_model.predict(X_test)\npred_enet = pd.DataFrame(dict(SalePrice=np.expm1(elastic_net_predictions), Id=test.Id))\n\npred_enet.head()","4bc0d698":"from sklearn.ensemble import RandomForestRegressor\nregr = RandomForestRegressor(max_depth=2, random_state=0)\nregr.fit(X_train, y)\ny_pred_rf = regr.predict(X_test)\n# residuals from best performing method for validation\ny_pred_rf_train = regr.predict(X_train)\nprint('RF train mse: {}'.format(mean_squared_error(y, y_pred_rf_train)))\npred_to_plot = pd.DataFrame(dict(best_pred=preds_l, new=np.expm1(y_pred_rf)))\npred_to_plot.plot(x='best_pred', y='new', kind='scatter')\n\n\nrf_preds = pd.DataFrame(dict(SalePrice=np.expm1(y_pred_rf), Id=test.Id))\n\nrf_preds.head()\n","b93ea310":"#STACKED REGRESSOR 1\nstack_gen1 = StackingCVRegressor(regressors=(lasso,elasticnet),meta_regressor=xgboost,use_features_in_secondary=True)\n\n\nstack_gen_model1 = stack_gen1.fit(np.array(X_train),np.array(y))\npreds_sg1 = stack_gen_model1.predict(np.array(X_test))\n\nsg_preds1 = pd.DataFrame(dict(SalePrice=np.expm1(preds_sg1), Id=test.Id))\nsg_preds1.head()","a933f733":"#Lasso + SR\npreds_comb1 = 0.6 * preds_l + 0.4 * preds_sg\ncomb_preds1 = pd.DataFrame(dict(SalePrice=np.expm1(preds_comb1), Id=test.Id))\ncomb_preds1.head()","e37066d9":"#SR + ENet\npreds_comb2 = 0.65 * elastic_net_predictions + 0.35 * preds_sg\ncomb_preds2 = pd.DataFrame(dict(SalePrice=np.expm1(preds_comb2), Id=test.Id))\ncomb_preds2.head()","156eab9e":"sg_preds.to_csv('\/kaggle\/working\/submission_Stacked_Reg_LogTranform_Tuned.csv',index=False)\nsg_preds1.to_csv('\/kaggle\/working\/submission_Stacked_Reg1_LogTranform_Tuned.csv',index=False)\ncomb_preds1.to_csv('\/kaggle\/working\/submission_Lasso_Stacked_Reg_LogTransform_Tuned.csv',index=False)\ncomb_preds2.to_csv('\/kaggle\/working\/submission_Stacked_Reg_ENet_LogTransform_Tuned.csv',index=False)","02437c91":"### Stacked Regressor","988c46db":"### Defining some functions to measure performance","8e80894d":"### Elastic Net","a8aa8ae6":"### Apply Log Transform","94b2e0a2":"### Converting categorical data with numerical features into categories ","7f37216e":"### Import Packages","b58b1933":"### Support Vector","a601788a":"### Handling skewed features","196b95c4":"### Create a list of suitable model parameters","a38f01d9":"### XGBoost","37ca02e0":"### Defining Models \/ Pipelines \/ Stacked Regressors","e8a0dea1":"### Random Forest","a98936ce":"# House Prices: Advanced Regression Techniques\n\n## This Notebook has:\n\n### **Data Preprocessing**\n* Data cleaning and preprocessing\n* Handling missing data and anomalies\n* Transformation of variables\n\n### **Feature Engineering**\n* Extensive feature engineering\n* Creating new features\n* Combining existing features\n* Removing insignificant features\n\n### **Modeling**\n* Various tuned and untuned Advanced Regressors\n* Combination of Regressors\n* Stacked Regression technique\n\n\n#### Note:\n* This includes few of my top submissions.","7345f5d6":"### Ridge","5b43a49b":"# PREDICTIONS","53baa2a2":"### Gradient Boost","950c206b":"### Separating Train and Test Data","4e2ea04a":"### Separate data into Numerical and Categorical Variables","8bb7ba14":"### Adding New Features","c8bb648a":"### Saving a few cominations for submission","0a0a4e10":"### Combined Predictions\nTheir weights can be varied","7e82d220":"# MODELING","7ec06eeb":"### Create new features from existing data","246c07da":"# DATA CLEANING AND PREPROCESSING","a23cd5bf":"### Simplify existing features - squash into smaller groups","7dce8378":"### Encoding Categorical Variables","f379ae98":"### Read Data","6c9b9937":"#### Performance scoring","ae1e2115":"# FEATURE ENGINEERING","cdf39e58":"### Stacked Regressor Combination 2","67fbb4b7":"### Lasso","f9129aa4":"### Differentiate numerical features (minus the target) and categorical features","4e6f34d2":"### Light Gradient Boost","fabc9d0b":"### Join Data"}}