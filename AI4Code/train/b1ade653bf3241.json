{"cell_type":{"219e44dd":"code","da7eb205":"code","72e00331":"code","620643c4":"code","25bebcae":"code","b53a9066":"code","cab40804":"code","ba560acc":"code","9c6ac7d5":"code","839d500f":"code","336b3c64":"code","584dc13d":"code","e6a56870":"code","e352b7db":"code","1b3b4afe":"code","a866675a":"code","0a1cad09":"code","e7fee542":"code","3911fcba":"code","780dc597":"code","ce040af6":"code","ee0bd300":"code","c85357d6":"code","878c757e":"code","1921b3ad":"code","c9266bbe":"code","55a6da87":"code","cc10e77a":"code","ef36e3fd":"code","5aa9fe5b":"code","1f560bbd":"code","fc40b23b":"code","e9c1cc3a":"code","25445471":"code","bd0ba442":"code","733b29f0":"code","a94279b7":"code","61bc1a01":"code","6f57a5c0":"code","047d0428":"code","a1aa24f2":"markdown","d1c72412":"markdown","c311b4ae":"markdown","a854352f":"markdown","a8e82759":"markdown","17689611":"markdown","9faed375":"markdown","b39048a1":"markdown","5f074031":"markdown","6cf4bf43":"markdown","5fe40715":"markdown","cb1cd6b4":"markdown","b862332e":"markdown","43e3886d":"markdown"},"source":{"219e44dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da7eb205":"import matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Flatten, LSTM, Embedding, Input, Conv1D, MaxPooling1D\nfrom keras.layers.merge import concatenate\nfrom keras.layers import Dropout\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.callbacks import EarlyStopping\n\nfrom gensim.models import KeyedVectors\nfrom gensim.models import Word2Vec\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nimport time\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize","72e00331":"# load google's pre-trained word2vec embeddings\nfilename = \"\/kaggle\/input\/nlpword2vecembeddingspretrained\/GoogleNews-vectors-negative300.bin\"\n\nstart = time.time()\ngoogle_embeddings = KeyedVectors.load_word2vec_format(filename, binary=True)\n\nprint(\"Load time (seconds): \", (time.time() - start))","620643c4":"# load Stanford's pre-trained GloVe embeddings\nglove_file = \"\/kaggle\/input\/nlpword2vecembeddingspretrained\/glove.6B.300d.txt\"\nglove_word2vec_file = \"glove.6B.300d.txt.word2vec\"\n\nglove2word2vec(glove_file, glove_word2vec_file)","25bebcae":"# glove embeddings\nstart = time.time()\n\nglove_embeddings = KeyedVectors.load_word2vec_format(glove_word2vec_file, binary=False)\n\nprint(\"Load time (seconds): \", (time.time() - start))","b53a9066":"# load data\ndf = pd.read_csv(\"\/kaggle\/input\/enron-email-classification-using-machine-learning\/preprocessed.csv\")\n\n# view first 5 rows of the dataframe 'df'\ndf.head()","cab40804":"# shape of the data\ndf.shape","ba560acc":"def label_encoder(df):\n    class_le = LabelEncoder()\n    # apply label encoder on the 'X-Folder' column\n    y = class_le.fit_transform(df['X-Folder'])\n    return y","9c6ac7d5":"y = label_encoder(df)\ncorpus = df['text']","839d500f":"# split the data into training and test set\nX_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.1, random_state=0)","336b3c64":"all_words = []\n\nfor sent in corpus:\n    tokenize_word = word_tokenize(sent)\n    for word in tokenize_word:\n        all_words.append(word)","584dc13d":"# retrieve all unique words from all_words\nunique_words = set(all_words)\nprint(\"Unique words: \",len(unique_words))","e6a56870":"# prepare tokenizer\nt = Tokenizer()\n\n# fit the tokenizer on the docs\nt.fit_on_texts(corpus)\n\n# integer encode the documents\ntrain_encoded_docs = t.texts_to_sequences(X_train)\ntest_encoded_docs = t.texts_to_sequences(X_test)","e352b7db":"# find the largest doc to make all the docs of uniform size i.e size of largest doc\nword_count = lambda doc: len(word_tokenize(doc))\nlongest_doc = max(corpus, key=word_count)\nlength_longest_doc = len(word_tokenize(longest_doc))\nlength_longest_doc = 500","1b3b4afe":"# to make all the docs of equal size, we will add zeros to empty indexes\ntrain_padded_docs = pad_sequences(train_encoded_docs, length_longest_doc, padding='post')\ntest_padded_docs = pad_sequences(test_encoded_docs, length_longest_doc, padding='post')","a866675a":"# one-hot encode the output labels\nY_train = to_categorical(y_train, 20)\nY_test = to_categorical(y_test, 20)","0a1cad09":"docs = []\n\nfor doc in corpus:\n    li = list(doc.split())\n    docs.append(li)","e7fee542":"start = time.time()\n# train the model\nmodel = Word2Vec(docs, size=300, window=5, min_count=1, workers=4, sg=0)\n# summarize the loaded model\nprint(model)\n# save the model\nmodel.save(\"email_embeddings.bin\")\n\nprint(\"Training time (seconds): \", (time.time() - start))","3911fcba":"# load own word embeddings\nstart = time.time()\n\nfilename = \"email_embeddings.bin\"\n\nemail_embeddings = Word2Vec.load(filename)\n\nprint(\"Load time (seconds): \", (time.time() - start))","780dc597":"vocab_size = len(email_embeddings.wv.vocab)","ce040af6":"# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocab_size, 300))\ncount = 0\n\nfor word, i in t.word_index.items():\n    if word in email_embeddings.wv.vocab.keys():\n        embedding_vector = email_embeddings[word]\n    \n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    else:\n        count += 1","ee0bd300":"embedding_matrix.shape","c85357d6":"print(\"Number of words not present in email_embeddings: \", count)","878c757e":"# define the model\ndef define_model(length_longest_doc, vocab_size, embedding_size):\n    # channel 1\n    inputs1 = Input(shape=(length_longest_doc,))\n    embedding1 = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=False)(inputs1)\n    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n    drop1 = Dropout(0.5)(conv1)\n    pool1 = MaxPooling1D(pool_size=2)(drop1)\n    flat1 = Flatten()(pool1)\n\n    # channel 2\n    inputs2 = Input(shape=(length_longest_doc,))\n    embedding2 = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=False)(inputs2)\n    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n    drop2 = Dropout(0.5)(conv2)\n    pool2 = MaxPooling1D(pool_size=2)(drop2)\n    flat2 = Flatten()(pool2)\n\n    # channel 3\n    inputs3 = Input(shape=(length_longest_doc,))\n    embedding3 = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=False)(inputs3)\n    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n    drop3 = Dropout(0.5)(conv3)\n    pool3 = MaxPooling1D(pool_size=2)(drop3)\n    flat3 = Flatten()(pool3)\n\n    # merge\n    merged = concatenate([flat1, flat2, flat3])\n\n    # interpretation\n    dense1 = Dense(10, activation='relu')(merged)\n    outputs = Dense(20, activation='softmax')(dense1)\n\n    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n\n    # compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    # print model summary\n    model.summary()\n    plot_model(model, show_shapes=True, to_file='model.png')\n    return model","1921b3ad":"# define model_b\ndef define_model_b(length_longest_doc, vocab_size, embedding_size):\n    # channel 1\n    inputs1 = Input(shape=(length_longest_doc,))\n    embedding1 = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=False)(inputs1)\n    lstm1_a = LSTM(256, dropout=0.5, return_sequences=True)(embedding1)\n    lstm1_b = LSTM(128, dropout=0.5)(lstm1_a)\n    flat1 = Flatten()(lstm1_b)\n    \n    # channel 2\n    inputs2 = Input(shape=(length_longest_doc,))\n    embedding2 = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=False)(inputs2)\n    lstm2_a = LSTM(256, dropout=0.5, return_sequences=True)(embedding2)\n    lstm2_b = LSTM(128, dropout=0.5)(lstm2_a)\n    flat2 = Flatten()(lstm2_b)\n    \n    # merge\n    merge = concatenate([flat1, flat2])\n    \n    # interpretation\n    #dense1 = Dense(10, activation='relu')(merge)\n    outputs = Dense(20, activation='softmax')(merge)\n    \n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    \n    # compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    # print model summary\n    model.summary()\n    #plot_model(model, show_shapes=True, to_file='model.png')\n    return model","c9266bbe":"start = time.time()\n\n# define model\nmodel = define_model_b(length_longest_doc, vocab_size, 300)\n\n# patience early stopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4, min_delta=0.001)\n\n# train the model\nhistory = model.fit([train_padded_docs, train_padded_docs], np.array(Y_train), epochs=50, batch_size=16, validation_split=0.1, callbacks=[es])\n\n# save the model\nmodel.save(\"model1.h5\")\nprint(\"Training time (minutes): \", (round((time.time() - start)\/60, 2)))","55a6da87":"# evaluate the model\nmodel_train_eval = model.evaluate([train_padded_docs, train_padded_docs, train_padded_docs], np.array(Y_train), verbose=0)\nmodel_test_eval = model.evaluate([test_padded_docs, test_padded_docs, test_padded_docs], np.array(Y_test), verbose=0)\n\nprint(\"Train Accuracy: {:0.3f}    Loss: {:0.3f}\".format(model_train_eval[1], model_train_eval[0]))\nprint(\"Test Accuracy:  {:0.3f}    Loss: {:0.3f}\".format(model_test_eval[1], model_test_eval[0]))","cc10e77a":"fig, axis = plt.subplots(1,2, figsize=(12, 6))\n# plot the loss\naxis[0].set_title(\"Loss\")\naxis[0].plot(history.history['loss'], label='train')\naxis[0].plot(history.history['val_loss'], label='validation')\naxis[0].legend()\naxis[0].set_xlabel('Epochs')\naxis[0].set_ylabel(\"Loss\")\n\n# plot the accuracy\naxis[1].set_title(\"Accuracy\")\naxis[1].plot(history.history['accuracy'], label='train')\naxis[1].plot(history.history['val_accuracy'], label='validation')\naxis[1].legend()\naxis[1].set_xlabel('Epochs')\naxis[1].set_ylabel(\"Accuracy\")\n\nplt.show()","ef36e3fd":"# create embedding matrix\nembedding_matrix = np.zeros((vocab_size, 300))\n\ncount = 0\n\nfor word, i in t.word_index.items():\n    if word in google_embeddings.wv.vocab.keys():\n        embedding_vector = google_embeddings[word]\n    \n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            \n    elif word in email_embeddings.wv.vocab.keys():\n        embedding_vector = email_embeddings[word]\n        \n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            \n    else: \n        count += 1","5aa9fe5b":"embedding_matrix.shape","1f560bbd":"start = time.time()\n\n# define the model\nmodel2 = define_model(length_longest_doc, vocab_size, 300)\n\n# patience early stopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4, min_delta=0.001)\n\n# train the model\nhistory2 = model2.fit([train_padded_docs, train_padded_docs, train_padded_docs], np.array(Y_train), epochs=50, validation_split=0.1, callbacks=[es])\n\n# save the model2\nmodel2.save(\"model2.h5\")\n\nprint(\"Training time (minutes): \", (round((time.time() - start)\/60, 2)))","fc40b23b":"# evaluate the model\nmodel2_train_eval = model2.evaluate([train_padded_docs,train_padded_docs,train_padded_docs], np.array(Y_train), verbose=0)\nmodel2_test_eval = model2.evaluate([test_padded_docs, test_padded_docs, test_padded_docs], np.array(Y_test), verbose=0)\n\nprint(\"Train Accuracy: {:0.3f}    Loss: {:0.3f}\".format(model2_train_eval[1], model2_train_eval[0]))\nprint(\"Test Accuracy:  {:0.3f}    Loss: {:0.3f}\".format(model2_test_eval[1], model2_test_eval[0]))","e9c1cc3a":"fig, axis = plt.subplots(1,2, figsize=(12, 6))\n# plot the loss\naxis[0].set_title(\"Loss\")\naxis[0].plot(history2.history['loss'], label='train')\naxis[0].plot(history2.history['val_loss'], label='validation')\naxis[0].legend()\naxis[0].set_xlabel('Epochs')\naxis[0].set_ylabel(\"Loss\")\n\n# plot the accuracy\naxis[1].set_title(\"Accuracy\")\naxis[1].plot(history2.history['accuracy'], label='train')\naxis[1].plot(history2.history['val_accuracy'], label='validation')\naxis[1].legend()\naxis[1].set_xlabel('Epochs')\naxis[1].set_ylabel(\"Accuracy\")\n\nplt.show()","25445471":"# create embedding matrix\nembedding_matrix = np.zeros((vocab_size, 300))\n\ncount = 0\n\nfor word, i in t.word_index.items():\n    if word in glove_embeddings.wv.vocab.keys():\n        embedding_vector = glove_embeddings[word]\n    \n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            \n    elif word in email_embeddings.wv.vocab.keys():\n        embedding_vector = email_embeddings[word]\n        \n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            \n    else: \n        count += 1","bd0ba442":"start = time.time()\n\n# define the model\nmodel3 = define_model(length_longest_doc, vocab_size, 300)\n\n# patience early stopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6, min_delta=0.001)\n\n# train the model\nhistory3 = model3.fit([train_padded_docs, train_padded_docs, train_padded_docs], np.array(Y_train), epochs=50, batch_size=16, validation_split=0.1, callbacks=[es])\n\nprint(\"Training time (minutes): \",(round((time.time()-start)\/60, 2)))","733b29f0":"# save the model\nmodel3.save(\"model3.h5\")","a94279b7":"# evaluate the model\nmodel3_train_eval = model3.evaluate([train_padded_docs, train_padded_docs, train_padded_docs], np.array(Y_train), verbose=0)\nmodel3_test_eval = model3.evaluate([test_padded_docs, test_padded_docs, test_padded_docs], np.array(Y_test), verbose=0)\n\nprint(\"Train Accuracy: {:0.3f}    Loss: {:0.3f}\".format(model3_train_eval[1], model3_train_eval[0]))\nprint(\"Test Accuracy:  {:0.3f}    Loss: {:0.3f}\".format(model3_test_eval[1], model3_test_eval[0]))","61bc1a01":"fig, axis = plt.subplots(1,2, figsize=(12, 6))\n# plot the loss\naxis[0].set_title(\"Loss\")\naxis[0].plot(history3.history['loss'], label='train')\naxis[0].plot(history3.history['val_loss'], label='validation')\naxis[0].legend()\naxis[0].set_xlabel('Epochs')\naxis[0].set_ylabel(\"Loss\")\n\n# plot the accuracy\naxis[1].set_title(\"Accuracy\")\naxis[1].plot(history3.history['accuracy'], label='train')\naxis[1].plot(history3.history['val_accuracy'], label='validation')\naxis[1].legend()\naxis[1].set_xlabel('Epochs')\naxis[1].set_ylabel(\"Accuracy\")\n\nplt.show()","6f57a5c0":"# create a results dataframe\n\nres = {\n    \"Embeddings\": [\"Email\", \"Word2Vec + Email\", \"GloVe + Email\"],\n    \"train accuracy\": [model_train_eval[1], model2_train_eval[1], model3_train_eval[1]],\n    \"test accuracy\": [model_test_eval[1], model2_test_eval[1], model3_test_eval[1]],\n    \"train loss\": [model_train_eval[0], model2_train_eval[0], model3_train_eval[0]],\n    \"test loss\": [model_test_eval[0], model2_test_eval[0], model3_test_eval[0]]\n}\n\nresults = pd.DataFrame(res)\nresults","047d0428":"results.to_csv(\"results.csv\", index=False)","a1aa24f2":"#### 2.2 Stanford's Word Embedding","d1c72412":"## 2. Load Pre-trained word embeddings","c311b4ae":"## 4. Prepare Data","a854352f":"## 3. Load Data","a8e82759":"#### 2.1 Google's Word Embedding","17689611":"## Enron Email Classification using Word Embeddings and LSTM","9faed375":"#### Encode class labels","b39048a1":"**Find total number of words in our corpus**","5f074031":"## 6. Email Classification using Email Word Embeddings","6cf4bf43":"## 5. Learn Own Word Embeddings","5fe40715":"## 1. Import necessary libraries","cb1cd6b4":"## 7. Using Word2Vec","b862332e":"## 8. Using GloVe ","43e3886d":"## 9. Results"}}