{"cell_type":{"f39224ec":"code","679a8def":"code","029c295d":"code","b8e7ba3b":"code","2173178f":"code","bb11231a":"code","899f0246":"code","df7a41a4":"code","f6e0b4a4":"code","778c641f":"code","350444cb":"code","3d270486":"code","3f52de5a":"code","165ef13e":"code","9076c38e":"code","a055a5e9":"code","8a35d48b":"code","4deb985e":"code","7344d083":"code","d89e2ec3":"code","37d0f3c3":"code","69183ee0":"code","d47f7139":"code","e10763bd":"code","716fde56":"code","61059b5b":"code","958c8c23":"code","678579a4":"code","bd75dbda":"code","fc30427e":"code","b0dd7a47":"code","effa0275":"code","63e57f30":"code","cb2ba42a":"code","0be221c6":"code","758c441b":"code","79ce53e6":"code","3ce874df":"code","c939abfa":"code","b45b7583":"code","cb199341":"code","8aee0cc1":"code","11928287":"code","b7237457":"code","cfe8703f":"code","c761de79":"code","820c28be":"code","cd9c8578":"code","a19747c5":"code","6fd1ea2c":"code","a72beb47":"code","826199f5":"code","a8bbd81c":"code","fc592961":"code","eebc603c":"code","c51dbf36":"code","a5d07c1e":"code","54918f91":"code","be7ede16":"code","d734179a":"code","a4a95ca2":"code","80d4e840":"code","37bc8799":"code","068c3ba1":"code","23137c06":"code","55650f2b":"code","258d5c24":"code","367f2e4c":"code","e00814c6":"code","3ec220fd":"code","2f7e16ba":"code","41f254c7":"code","785f37de":"code","47d532cb":"code","9d29e2c1":"code","52e5aa7c":"code","7703a054":"code","4a6500f7":"code","8031644b":"code","d49e31d2":"code","093490c3":"code","57e8f926":"code","183c9b78":"code","57e0311d":"code","f4905a43":"code","8cf0cd2a":"code","0b0a2a52":"code","38f32840":"code","4c435d8c":"code","1e8eedde":"code","3746276a":"code","17114e59":"code","2a3a5167":"code","8512a144":"code","6bffd3ea":"code","99db26f2":"code","3d366c36":"code","4ea4d466":"code","6032473f":"code","cfc37d66":"code","ab699b0f":"code","36d108d5":"code","b793ea6d":"code","73f33a34":"code","d2efed3d":"code","8a282f59":"code","4b02bb78":"code","a7e70d3f":"code","7322edcb":"code","cae3d1e7":"code","afdb5273":"code","89db9773":"code","3239482f":"code","582aff2a":"code","64745f28":"code","b4243c81":"code","bd6f94d9":"code","8f91d940":"code","ffc9ba33":"code","48cb7fe0":"code","b6f87330":"code","ad87f70f":"code","52f13a18":"code","e110669d":"code","2f55f72f":"markdown","41a29e71":"markdown","d105ecb1":"markdown","38849dc8":"markdown","e757a02b":"markdown","0cd7d44e":"markdown","4a24328f":"markdown","2272bd5e":"markdown","1f511d89":"markdown","b8784116":"markdown","5d1fcf0b":"markdown","de443619":"markdown","bd9e865f":"markdown","5876f374":"markdown","f7bc61bb":"markdown","564ec57f":"markdown","8f501ca1":"markdown","8e30f611":"markdown","c796e39b":"markdown","53d18d51":"markdown","50fc3979":"markdown","d8393730":"markdown","4dbfafc3":"markdown","3ae2a5c2":"markdown","176d6845":"markdown","53701874":"markdown","d6f3f170":"markdown","670d99be":"markdown","6e16bea8":"markdown","c8d35b70":"markdown","4873a6bf":"markdown","dc272642":"markdown","9674b83a":"markdown","ccd84475":"markdown","2c606604":"markdown","39989e99":"markdown","1475ea8b":"markdown","b0f2ca4b":"markdown","cf90abf5":"markdown"},"source":{"f39224ec":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport glob\nimport os\nfrom tqdm import tqdm\n\n%matplotlib inline","679a8def":"X=[]\nZ=[]\nIMG_SIZE=150\nFLOWER_SUNFLOWER_DIR='..\/input\/flowers-recognition\/flowers\/flowers\/sunflower'","029c295d":"X=[]\nZ=[]\nIMG_SIZE=150\nFLOWER_SUNFLOWER_DIR='..\/input\/flowers-recognition\/flowers\/flowers\/sunflower'\ndef assign_label(img,flower_type):\n    return flower_type\n","b8e7ba3b":"#FUNCTION TO LOAD DATA\ndef make_train_data(flower_type,DIR):\n    for img in tqdm(os.listdir(DIR)):\n        label=assign_label(img,flower_type)\n        path = os.path.join(DIR,img)\n        img = cv2.imread(path,cv2.IMREAD_COLOR)\n        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))#Resizing the image\n        \n        X.append(np.array(img))#we are storing the data in the form of a list\n        Z.append(str(label))# we are also storing the label in the form of a list","2173178f":"#Loading Sunflower Data\nmake_train_data('Sunflower',FLOWER_SUNFLOWER_DIR)\nprint(len(X))","bb11231a":"fix_img = cv2.cvtColor(X[0],cv2.COLOR_BGR2RGB)# THIS IS HOW TO CONVERT BGR COLOR SPACE TO RGB COLOR SPACE","899f0246":"plt.figure(figsize = (12,8))\nplt.imshow(fix_img)","df7a41a4":"new_img_1 = fix_img.copy() \n","f6e0b4a4":"new_img_1.shape\n","778c641f":"new_img_1[:,:,0] = 0 # making R channel zero\nnew_img_1[:,:,1] = 0 #making G channel zero","350444cb":"plt.imshow(new_img_1) # Finally having blue version of that image","3d270486":"new_img_2 = fix_img.copy()\nnew_img_3 = fix_img.copy()","3f52de5a":"#For Red color Channel\nnew_img_2[:,:,1] = 0\nnew_img_2[:,:,2] = 0\n#For Green color channel\nnew_img_3[:,:,0] = 0\nnew_img_3[:,:,2] = 0","165ef13e":"f, axes = plt.subplots(1,3, figsize = (15,15))\nlist = [new_img_1,new_img_2,new_img_3]\ni = 0\nfor ax in axes:\n    ax.imshow(list[i])\n    i+=1","9076c38e":"f, axes = plt.subplots(1,3, figsize = (15,15))\nlist = [fix_img[:,:,0],fix_img[:,:,1],fix_img[:,:,2]]\ni = 0\nfor ax in axes:\n    ax.imshow(list[i],cmap = 'gray')\n    i+=1","a055a5e9":"hsl_img = cv2.cvtColor(X[0],cv2.COLOR_BGR2HLS)\n","8a35d48b":"hsl_img.shape\n","4deb985e":"plt.figure(figsize=(12,10))\nplt.imshow(hsl_img)","7344d083":"hsl_img_1 = hsl_img.copy()\nhsl_img_2 = hsl_img.copy()\nhsl_img_3 = hsl_img.copy()","d89e2ec3":"#HUE --> ZERO\nhsl_img_1[:,:,1] = 0\nhsl_img_1[:,:,2] = 0\n#SATURATION --> ZERO\nhsl_img_2[:,:,0] = 0\nhsl_img_2[:,:,2] = 0\n#LIGHTNESS --> ZERO\nhsl_img_3[:,:,0] = 0\nhsl_img_3[:,:,1] = 0","37d0f3c3":"f, axes = plt.subplots(1,3, figsize = (15,15))\nlist = [hsl_img_1,hsl_img_2,hsl_img_3]\ni = 0\nfor ax in axes:\n    ax.imshow(list[i])\n    i+=1","69183ee0":"f, axes = plt.subplots(1,3, figsize = (15,15))\nlist = [hsl_img[:,:,0],hsl_img[:,:,1],hsl_img[:,:,2]]\ni = 0\nfor ax in axes:\n    ax.imshow(list[i],cmap = \"gray\")\n    i+=1","d47f7139":"hsv_img = cv2.cvtColor(X[0],cv2.COLOR_BGR2HSV)","e10763bd":"hsv_img.shape","716fde56":"plt.figure(figsize = (10,8))\nplt.imshow(hsv_img)","61059b5b":"hsv_img_1 = hsv_img.copy()\nhsv_img_2 = hsv_img.copy()\nhsv_img_3 = hsv_img.copy()","958c8c23":"f, axes = plt.subplots(1,3, figsize = (15,15))\nlist = [hsv_img_1,hsv_img_2,hsv_img_3]\ni = 0\nfor ax in axes:\n    ax.imshow(list[i])\n    i+=1","678579a4":"f, axes = plt.subplots(1,3, figsize = (15,15))\nlist = [hsv_img[:,:,0],hsv_img[:,:,1],hsv_img[:,:,2]]\ni = 0\nfor ax in axes:\n    ax.imshow(list[i],cmap = \"gray\")\n    i+=1","bd75dbda":"!pip install opencv-contrib-python==4.2.0.34","fc30427e":"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\n","b0dd7a47":"pic = cv2.imread('..\/input\/cv-object-detecton-dataset\/CV Object Detecton Dataset\/leaf.jfif')\npic = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\nplt.imshow(pic)","effa0275":"type(pic)","63e57f30":"matrix = np.float32([[1,0,20],[0,1,30]])","cb2ba42a":"rows,cols,chn = pic.shape\nprint('height:{}'.format(rows))\nprint('width:{}'.format(cols))\nprint(\"pic shape: {} \".format(pic.shape))","0be221c6":"translated_pic = cv2.warpAffine(pic,matrix,(cols,rows))","758c441b":"plt.figure(figsize=(10,8))\nplt.subplot(121)\nplt.imshow(translated_pic)\n\n# plt.figure(figsize=(10,8))\nplt.subplot(122)\nplt.imshow(pic)\nplt.show()","79ce53e6":"translated_pic.shape","3ce874df":"pic = cv2.imread('..\/input\/cv-object-detecton-dataset\/CV Object Detecton Dataset\/leaf.jfif')\npic = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\nplt.imshow(pic)","c939abfa":"pic.shape","b45b7583":"resized_pic = cv2.resize(pic,(100,200))\nplt.imshow(resized_pic)","cb199341":"resized_pic.shape","8aee0cc1":"width_ratio = 0.5\nheight_ratio = 0.5\nresized_pic2 = cv2.resize(pic,(0,0),pic,width_ratio,height_ratio)","11928287":"plt.imshow(resized_pic2)\nprint('shape:{}'.format(resized_pic2.shape))# shape reduced by 50%","b7237457":"rotate = cv2.flip(pic,0)#you also can pass value like--> 0,1,-1 etc.","cfe8703f":"plt.imshow(rotate)# Rotated by 180 degree","c761de79":"pic1 = cv2.imread('..\/input\/cv-object-detecton-dataset\/CV Object Detecton Dataset\/gorilla.jpg')\npic1 = cv2.cvtColor(pic1,cv2.COLOR_BGR2RGB)","820c28be":"plt.imshow(pic1)","cd9c8578":"pic1.shape","a19747c5":"crop = pic1[800:3000,1800:3200]","6fd1ea2c":"plt.imshow(crop)#Croped Image","a72beb47":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","826199f5":"img = cv2.imread('..\/input\/cv-object-detecton-dataset\/CV Object Detecton Dataset\/SharedScreenshot1.jpg',0)\nplt.imshow(img,cmap = 'gray')","a8bbd81c":"img.max()\/2#Finding middle value of the distribution of the pixel value","fc592961":"'''1st arg as the gray image itself the 2nd arg as the threshold value(usually the mean value of pixel values) then 3rd \narg as the the max pixel value and as the last arg we pass the method using which we are going to do the thresholding'''\nret3,thresh3 = cv2.threshold(img,123,255,cv2.THRESH_TRUNC)","eebc603c":"ret, thresh1 = cv2.threshold(img,123,255,cv2.THRESH_BINARY)","c51dbf36":"ret, thresh2 = cv2.threshold(img,123,255,cv2.THRESH_TRIANGLE)","a5d07c1e":"plt.imshow(thresh1,cmap = 'gray')","54918f91":"plt.imshow(thresh2,cmap = 'gray')","be7ede16":"diff = thresh1 - thresh2","d734179a":"plt.imshow(diff)","a4a95ca2":"diff1 = thresh1 - thresh3","80d4e840":"sum1 = thresh1 + thresh3","37bc8799":"plt.imshow(sum1,cmap = 'gray')","068c3ba1":"'''1st arg: Gray image itself\n   2nd arg:Max pixel value\n   3rd arg: Type for calculating mean\n   4th arg:thresholding type\n   5th arg: block size(size of the pixel neighbourhood for calculate a threshold, it is needed to be odd like-3,5,7 etc.)\n   6th arg: c constant(Generally a constant value & a odd number)'''\nth2 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,11,8)","23137c06":"th3 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,8)","55650f2b":"th = th3 - th2\nth.shape","258d5c24":"'''import numpy as np\nth1 = np.mean(th)\nprint(th1)'''\n","367f2e4c":"plt.imshow(th,cmap = 'gray')","e00814c6":"#Function for Loading the image\ndef load_img():\n    img = cv2.imread('..\/input\/cv-object-detecton-dataset\/CV Object Detecton Dataset\/blue_brick.jpg')\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    return img","3ec220fd":"#Function for displaying the image\ndef disp_img(img,cmap = None):\n    fig = plt.figure(figsize = (8,8))\n    ax = fig.add_subplot(111)\n    ax.imshow(img,cmap)","2f7e16ba":"img1 = load_img()","41f254c7":"img1.shape","785f37de":"disp_img(img1)","47d532cb":"img2 = load_img()\nfont = cv2.FONT_HERSHEY_COMPLEX\ncv2.putText(img2,text = 'Bricks',org = (10,125),fontFace = font,fontScale = 2,color = (255,0,0),thickness = 1)","9d29e2c1":"disp_img(img2)","52e5aa7c":"#Creating a custom kernel as a filter for our bluring perpous\nimport numpy as np\nkernel = np.ones(shape=(3,3),dtype = np.float32)\/6.07 ### WE ARE DIVIDING THAT BY 25(KERNEL SIZE= 5X5)\n#TO GET FLOTING VALUES IF WE USE THAT FOR MULTIPLYING THE OTHER IMAGE THEN PIXEL VALUE WILL BE DECREASED OF OTHER IMAGE","7703a054":"1\/25","4a6500f7":"dst = cv2.filter2D(img2,-1,kernel)\ndisp_img(dst)","8031644b":"#Loading image again \nimg2 = load_img()\nfont = cv2.FONT_HERSHEY_COMPLEX\ncv2.putText(img2,text = 'Bricks',org = (10,125),fontFace = font,fontScale = 2,color = (255,0,0),thickness = 1)\nprint('reset')","d49e31d2":"disp_img(img2)","093490c3":"blur = cv2.blur(img2,ksize = (2,2))\ndisp_img(blur)","57e8f926":"noisy_img2  = cv2.imread('..\/input\/cv-object-detecton-dataset\/CV Object Detecton Dataset\/balloons_noisy.png')\nnoisy_img2 = cv2.cvtColor(noisy_img2,cv2.COLOR_BGR2RGB)\ndisp_img(noisy_img2)\n","183c9b78":"blur = cv2.blur(noisy_img2,ksize = (3,3))\ndisp_img(blur)","57e0311d":"img2 = load_img()\nfont = cv2.FONT_HERSHEY_COMPLEX\ncv2.putText(img2,text = 'Bricks',org = (10,125),fontFace = font,fontScale = 2,color = (255,0,0),thickness = 1)\nprint('reset')","f4905a43":"disp_img(img2)","8cf0cd2a":"blur_img  = cv2.GaussianBlur(img2,(3,3),0.64)\ndisp_img(blur_img)","0b0a2a52":"noisy_img1  = cv2.imread('..\/input\/cv-object-detecton-dataset\/CV Object Detecton Dataset\/balloons_noisy.png')\nnoisy_img1 = cv2.cvtColor(noisy_img1,cv2.COLOR_BGR2RGB)\ndisp_img(noisy_img1)\n","38f32840":"blur_img  = cv2.GaussianBlur(noisy_img1,(3,3),0.64)\ndisp_img(blur_img)","4c435d8c":"#Gaussian blur bluring the image but not removing noise from this picture.","1e8eedde":"img2 = load_img()\nfont = cv2.FONT_HERSHEY_COMPLEX\ncv2.putText(img2,text = 'Bricks',org = (10,125),fontFace = font,fontScale = 2,color = (255,0,0),thickness = 1)\nprint('reset')","3746276a":"disp_img(img2)","17114e59":"median_blur = cv2.medianBlur(img2,3)\ndisp_img(median_blur)","2a3a5167":"noisy_img  = cv2.imread('..\/input\/cv-object-detecton-dataset\/CV Object Detecton Dataset\/balloons_noisy.png')\nnoisy_img = cv2.cvtColor(noisy_img,cv2.COLOR_BGR2RGB)\ndisp_img(noisy_img)\n","8512a144":"median = cv2.medianBlur(noisy_img,5)\ndisp_img(median)","6bffd3ea":"plt.figure(figsize=(12,10))\nattatiched = np.concatenate([noisy_img,median],axis=1)\nplt.imshow(attatiched)","99db26f2":"#READING THREE DIFFERENT IMAGES\ndark =cv2.imread('..\/input\/cv-object-detecton-dataset\/CV Object Detecton Dataset\/dark_image.jpg')\ndark_img = cv2.cvtColor(dark,cv2.COLOR_BGR2RGB)\n\nrainbo =cv2.imread('..\/input\/cv-object-detecton-dataset\/CV Object Detecton Dataset\/rainbo.jpg')\nrainbo_img = cv2.cvtColor(rainbo,cv2.COLOR_BGR2RGB)\n\nbrick =cv2.imread('..\/input\/cv-object-detecton-dataset\/CV Object Detecton Dataset\/blue_brick.jpg')\nbrick_img = cv2.cvtColor(brick,cv2.COLOR_BGR2RGB)","3d366c36":"#SHOWING THE IMAGE\ndisp_img(brick_img)","4ea4d466":"rainbo_hist = cv2.calcHist([rainbo],channels = [0],mask = None,histSize= [256],ranges = [0,256])","6032473f":"rainbo_hist.shape","cfc37d66":"plt.plot(rainbo_hist)\n#plt.xlim([0,20])","ab699b0f":"color = ('b','g','r')\n\nfor i,col in enumerate(color):\n    hist = cv2.calcHist([brick],[i],None,[256],[0,256])\n    plt.plot(hist,color = col)\n    plt.xlim([0,256])\n    #plt.ylim()\nplt.title('hist for rainbo')","36d108d5":"rainbo =cv2.imread('..\/input\/cv-object-detecton-dataset\/CV Object Detecton Dataset\/rainbo.jpg')\nrainbo_img = cv2.cvtColor(rainbo,cv2.COLOR_BGR2RGB)","b793ea6d":"rainbo_img.shape","73f33a34":"plt.imshow(rainbo_img)","d2efed3d":"mask = np.zeros(rainbo_img.shape[:2],np.uint8)","8a282f59":"plt.imshow(mask,cmap = 'gray')","4b02bb78":"mask[75:100,50:100] = 255","a7e70d3f":"plt.imshow(mask,cmap = 'gray')","7322edcb":"masked_img = cv2.bitwise_and(rainbo,rainbo,mask = mask)\nshow_masked_img = cv2.bitwise_and(rainbo_img,rainbo_img,mask = mask)","cae3d1e7":"plt.imshow(show_masked_img)","afdb5273":"hist_mask = cv2.calcHist([rainbo],[2],mask,[256],[0,256])\n#Notice we are only passing \"[2]\" in the number of color \n#channel which reffers that here we are only interested about \"RED\" color channel, as in the maskind image we only have\n#green color not red so as output we will not have any high picks in the histogram","89db9773":"hist_not_mask = cv2.calcHist([rainbo],[2],None,[256],[0,256])#Performing same thing without mask","3239482f":"plt.plot(hist_mask)","582aff2a":"hist_mask_green = cv2.calcHist([rainbo],[1],mask,[256],[0,256])#trying to plot histogram for only green color channel","64745f28":"plt.plot(hist_mask_green)","b4243c81":"gorilla = cv2.imread('..\/input\/cv-object-detecton-dataset\/CV Object Detecton Dataset\/gorilla.jpg')\ngorilla = cv2.cvtColor(gorilla,cv2.COLOR_BGR2GRAY)\n","bd6f94d9":"disp_img(gorilla,cmap = 'gray')","8f91d940":"eq_gorilla = cv2.equalizeHist(gorilla)","ffc9ba33":"disp_img(eq_gorilla,cmap = 'gray')#After applying histogram equalization","48cb7fe0":"road = cv2.imread('..\/input\/cv-object-detecton-dataset\/CV Object Detecton Dataset\/yello_road.jpg')\nroad = cv2.cvtColor(road,cv2.COLOR_BGR2RGB)\nplt.imshow(road)","b6f87330":"blurry = cv2.GaussianBlur(road,(35,35),0)\nplt.imshow(blurry)\nblur_img = blurry.copy()","ad87f70f":"import numpy as np","52f13a18":"blur_img4 = blurry.copy()","e110669d":"hsv = cv2.cvtColor(blur_img4,cv2.COLOR_RGB2HSV)\nlow_yellow = np.array([18,94,140])\nup_yellow = np.array([48,230,230])\nmask = cv2.inRange(hsv,low_yellow,up_yellow)\nedge = cv2.Canny(mask,75,150)\n\nline = cv2.HoughLinesP(edge,1,np.pi\/180,50,maxLineGap = 50)\n\nfor i in line:\n    x1,y1,x2,y2 = i[0]\n    cv2.line(blur_img4,(x1,y1),(x2,y2),(0,255,0),25)\n    \nplt.imshow(blur_img4)","2f55f72f":"<center><h1>Deep Dive in Image Pre-processing Methods \ud83d\udd25\ud83d\udd25\ud83d\udd25<\/h1><center>","41a29e71":"## How to create the normal Histogram?\nTo create a normal Histogram we need to use **cv2.calcHist()**, now there are different arguments we need to pass in that function to get the distribution,\n- 1st arg: the source image in the form of a list\n- 2nd arg: the number of chennel we need\n- 3rd arg: I am using any mask or not\n- 4th arg: upper limit of the pixel\n- 5th arg: range og the pixel ","d105ecb1":"### Using Fractions --> Like what is the percentage by which you want to resize the image(I wanna resize the image by 50 percent from both height and width)","38849dc8":"So, lets jump to the code, so much of theory\ud83d\ude12.\n#### Concept:\nSo I am going to use a picture of a blue wall and then i will put some text on the top of that, which will make use understand the intensity of the algos or how efficient they are. I am taking `cv2.FONT_HERSHEY_COMPLEX` as my font type, which has a kind of gap inside a letter wich is going to be blured if we use blurring on them.","e757a02b":"### Making Kernels","0cd7d44e":"## cv2.filter2D() :","4a24328f":"## Basic Geometric Transformations:\n* Image Translation\n* Image Resizing\n* Image Rotating\n* Image cropping","2272bd5e":"\n<center><img src=\"https:\/\/i.imgur.com\/cTZgUdS.jpg\" ><\/center>\n<center><p style=\"padding-left:380px;color:red\">Fig3:HSV Color Space<\/p><\/center>","1f511d89":"<h1><span class=\"label label-success\">OpenCV Implimentation:-<\/span><\/h1>\n<p>For implementing thresholding we use the builtin function called <strong>cv2.threshold()<\/strong> which returns the threshold value and the thresholded image in the form of a tuple.Adaptive thresholding also does the same kind of thing just there are some more parameters for playing around(<i>implimented in the below cells using <strong>cv2.adaptiveThreshold()<\/strong><\/i>).There are different ways of creating a thresholded image, like-<\/p>\n<ol style=\"padding-left:235px\">\n    <li>Binary<\/li>\n    <li>Inverted Binary<\/li>\n    <li>Truncated<\/li>\n    <li>To Zero<\/li>\n    <li>To Zero Inverted<\/li>\n    <li>Otsu\u2019s Binarization<\/li>\n<\/ol>\n<p>Last one is pretty effective, the <strong>Otsu\u2019s Binarization<\/strong>.In terms of creating a desired thresholded image, we will be needing a threshold value and finding that is a trial and error process. But, we can use Otsu\u2019s Binarization which does not take any threshold value to create one, it just only needs the min and max pixel values. <\/p>\n\nBut, in other methods we need to pass 1st arg as the gray image itself the 2nd arg as the threshold value(usually the mean value of pixel values) then 3rd arg as the max pixel value and as the last arg we pass the method using which we are going to do the thresholding. Check out the below image to understand different methods of thresholding.\n\n<center><img src=\"https:\/\/i.imgur.com\/FnZOvQK.png\" ><\/center>\n<center><p style=\"padding-left:380px;color:red\">Fig1:Different type of Thresholding<\/p><\/center>","b8784116":"### Image Croping:\n","5d1fcf0b":"We will be using a customized, which is consist of a bule brick wall image and a red text putted in it","de443619":"If I try to explain thresholding in the simplest manner, that will be, it is a method of creating a binary image from a normal image. We need to use a grayscale image in order to apply thresholding on a normal image.\n<h1><span class=\"label label-success\">Applications:-<\/span><\/h1>\n<p>There are so many applications in computer vision,one of the general appliactions are- <\/p>\n<ol style= \"padding-left: 235px\">\n    <li>Segmentation<\/li>\n    <li>Mask creation<\/li>\n    <li>Edge Detection<\/li>\n    <li>Line Detection etc.<\/li>\n<\/ol>\n<h1><span class=\"label label-success\">Methods:-<\/span><\/h1>\n<p>The general method of creating a thresholded image is like that- we take an image then make that a grayscale image so that we can get the range of pixel values from 0 to 255, in the image if the pixel values are greater than the desired value (generally it is called threshold value) we replace those by 1 and pixels those are lesser than that desired value get replaced by 0(zero). Thus we create a desired binary image from a normal image.<\/p>\nBut, there are way more different methods to create a threshold image but the basic intuition is the same as above.","bd9e865f":"## how to perform histogram equalization?\nHistogram Equalization is a method of contrast adjustment based on the image's histogram.\n\nIn this image the pixel values are are between 0-255, but we will not find any pixel values which are exactly 0 or 255, it does not have any image which is pure white or pure black.If we apply the histogram equalization then it will reduce the color depth.Currently the minimum pixel value is 52 and the highest is 255.\nAfter you apply histogram equalization, you will find the the min pixel value now got transformed to zero and the max got converted to 255. So, notice again how the min and max values are equalized between 0 and 255, we also see less shade of gray.(view fig1 to 2)\n\n<img src=\"https:\/\/i.imgur.com\/mWTXqZ4.png\" style=\"width:500px;height:250px;\">\n<center><p style=\"padding-left:380px;color:red\">Fig1:before applying histogram equalization<\/p><\/center>\n\n<img src=\"https:\/\/i.imgur.com\/5diUO7c.png\" style=\"width:500px;height:250px;\">\n<center><p style=\"padding-left:380px;color:red\">Fig2: After applying histogram equalization<\/p><\/center>\n\nAgain, now we have a image on the left hand side and it's coresponding histogram(in red) on the right the black line is nothing but the cumulative of the pixel values.So, after we apply the Histogram equalizer that cumulative changes to linear step function. Notice that we don't literally flatten out the histogram we only just focus on the cumulative linear. And the real mathematics behind the Histogram equalization is just like that.(view fig 3 to 5)\n<img src=\"https:\/\/i.imgur.com\/uIBnzbu.png\" style=\"width:500px;height:250px;\">\n\n<center><p style=\"padding-left:380px;color:red\">Fig3:before applying histogram equalization<\/p><\/center>\n\n<img src=\"https:\/\/i.imgur.com\/fuIGrCi.png\" style=\"width:500px;height:250px;\">\n\n<center><p style=\"padding-left:380px;color:red\">Fig4: After applying histogram equalization<\/p><\/center>\n\n<img src=\"https:\/\/i.imgur.com\/9q3NVIg.png\" style=\"width:500px;height:250px;\">\n<center><p style=\"padding-left:380px;color:red\">Fig5:Main difference <\/p><\/center>\n\nWe mainly use histogram equalization when we need to increase the contrast of the image.","5876f374":"## How to create a histogram for all color channels?\nWe will be doing as the normal histograms(mentioned above) are made,by using **cv2.calcHist()** and passing different color channels as the input and rest of the things will be the same.","f7bc61bb":"<img src=\"https:\/\/i.imgur.com\/s1GeDmW.jpg\" style=\"width:800px;height:800px;\">","564ec57f":"### Image Rotation","8f501ca1":"## cv2.GaussianBlur():\nGaussian blur is another type of blurring technique which helps to reduce Gaussian noise and create blur. In this method, the filter which is used is a \"Gaussian Filter\" which convolves around the image and do the multiplication and summation task, as explained above. As arguments we pass the image then we pass the kernel size and at the last, we pass the standard deviation in the X-direction and the standard deviation in the Y direction, if the only std for X is given the std for Y is taken as equal as the std for X, if it is not given or zero it is computed from the kernel size and kernel size should be positive and odd.We can create an Gaussian kernel using **cv2.getGaussianKernel()**. \nThis is how a Gaussian kernel looks like.\n<img src=\"https:\/\/i.imgur.com\/ZF3OOGr.png\" style=\"width:500px;height:250px;\">\n<center><p style=\"padding-left:380px;color:red\">Fig1:Visualize a Gaussian Filter<\/p><\/center>\n<img src=\"https:\/\/i.imgur.com\/qgAtxnC.png\" style=\"width:700px;height:250px;\">\n<center><p style=\"padding-left:380px;color:red\">Fig2:Methemetical form of Gaussian Filter<\/p><\/center>\n<div>\n<p>The pixel values are normally distributed, at the middle pixel value is the highest and at the edges it is the lowest.\nAnd this is the formula,<\/p>\n<\/div>\n<img src=\"https:\/\/i.imgur.com\/VyPEexH.png\" style=\"width:400px;height:220px;\">\n<center><p style=\"padding-left:380px;color:red\">Fig3: Defining the Gaussian function based on the size of sigma(standard deviation)<\/p><\/center>\n\nwhere x is the distance from the origin in the horizontal axis, y is the distance from the origin in the vertical axis, and \u03c3 is the standard deviation of the Gaussian distribution.","8e30f611":"\n<center><img src=\"https:\/\/i.imgur.com\/8Ul3k79.jpg\" ><\/center>\n<center><p style=\"padding-left:380px;color:red\">Fig2:HSL Color Space<\/p><\/center>","c796e39b":"\n<center><img src=\"https:\/\/i.imgur.com\/RCIr2Vp.png\" ><\/center>\n<center><p style=\"padding-left:380px;color:red\">Fig1:RGB Color Space<\/p><\/center>","53d18d51":"### What are the types of Colorspace?\n--> There are mainly five major color models out there. But, I am going to write about only the frequent ones(RGB, HSV, and HSL).\n\n1. RGB(Red Green Blue)\n2. HSL(Hue Saturation Lightness)\n3. HSV(Hue Saturation Value)\n4. YUV(Luminance, blue\u2013luminance, red\u2013luminance)\n5. CMYK(Cyan, Magenta, Yellow, Key)","50fc3979":"<h1 style=\"background-color: orange;color: rgb(255, 255, 255);text-align: center;padding-top: 30px;padding-bottom: 30px;\">Thresholding<\/h1>","d8393730":"<center><p style=\"padding-left:340px;color:red\">Fig0: Photo by Jeremy Bishop on Unsplash<\/p><center>\n","4dbfafc3":"### References:\n1. https:\/\/opencv-python-tutroals.readthedocs.io\n2. https:\/\/www.kaggle.com\/hrmello\/intro-to-image-processing-colorspaces\n3. https:\/\/www.kaggle.com\/sanikamal\/image-segmentation-using-color-spaces\n<p>If there is any query please let me know.<\/p>","3ae2a5c2":"### Line Detection:\nNow, we will try to detect lines.so we will simply use **cv2.HoughLinesP()** for doing that along with creating a mask through which we detect the edges using canny edge detector, and then apply that o\/p in that Hough line prediction function.","176d6845":"###  Converting RGB ColorSpace in HSL color Space:\n#### HSL Color Space:\n--> The general meaning of HSL is Hue, Saturation, and Lightness. You can visualize HSL in the form of a cylinder as shown in fig:2(a). All around the cylinder will be different colors, Like green, yellow, red, etc. (the actual hue we are looking for). Saturation is how many hue you end up having and lightness is how dark or how bright the color is. As you can see the top of the cylinder is full of white and the bottom part is full black.","53701874":"Here you can see, the blue channel is having two picks around the 150 and the 230 pixel values, which means that the number of pixels which are in majority belongs to the pixel values near to 150 and 230. And for Red and the Green chennel these two values are aproximately 80,210 and 140,210 respectively.","d6f3f170":"<h1><span class=\"label label-success\">Intro:-<\/span><\/h1>\n<p>This thing needs no explanation the title explains everything. Some time for finding features or to remove noise we sometimes need to perform blurring and smoothing. Blurring and smoothing is one of the important toolkits of your image processing toolbox. But, the main problem in doing that is there are so many methods, available in OpenCV for performing blurring and smoothing, like-<\/p>\n<ol style= \"padding-left: 235px\">\n    <li>Gaussion Blurring<\/li>\n    <li>Median Bluring<\/li>\n    <li>Bilateral Blurring<\/li>\n    <li>Gamma Currection<\/li>\n    <li>Using Builtin Kernels(<i>cv2.blur()<\/i>)<\/li>\n    <li>Using user defined Kernels(<i>cv2.filter2D()<\/i>) etc.<\/li>\n<\/ol>\n\n<p>So, what to use \ud83e\udd14? Mainly people use Gaussian Blurring for the first time, if it doesn't work then they go for other methods, mainly I try to follow this pattern. But, one the other hand is true that nothing is sure which one will help you to get your job done, you have to keep trying different methods until you find your desired image, except the in every method, there are multiple parameters to tune, which might give you better results.<\/p>\n<h1><span class=\"label label-success\">How it works?\ud83e\udd14<\/span><\/h1>\n<p>So, generally there will be a kernel(<i>kernels are nothing but array of numbers<\/i>) for perfoeming different kind of operations and we will be multyplying them with the normal image and taking the the sum of that and consider that as a pixel value.This is the kernel for blurring-<\/p>\n\n<div class=\"column\">\n    <img src=\"https:\/\/i.imgur.com\/XUFtFWJ.png\" style=\"width:500px;height:250px;\">\n    <center><p style=\"padding-left:380px;color:red\">Fig1:Filter for blurring<\/p><\/center>\n  <\/div>\n \n<h3 style=\"text-align:center\">And this the result of blurring \ud83d\udc47\ud83d\udc47\ud83d\udc47<\/h3>\n<div class=\"column\">\n    <img src=\"https:\/\/i.imgur.com\/P2qg11V.png\" style=\"width:500px;height:250px;\">\n    <center><p style=\"padding-left:380px;color:red\">Fig2:After applying blurring<\/p><\/center>\n  <\/div>\n<a href=\"https:\/\/setosa.io\/ev\/image-kernels\/\" class=\"btn btn-primary\" style=\"color:white;\">Here<\/a> is the website website which has a kind of visualizasion to understand how blurring and smoothing worksAs I said above that there different kernels for different perpous,this is the kernel for shearpening the image\n<div class=\"column\">\n    <img src=\"https:\/\/i.imgur.com\/mmCCgse.png\" style=\"width:500px;height:250px;\">\n    <center><p style=\"padding-left:380px;color:red\">Fig3:Filter for smoothing<\/p><\/center>\n  <\/div>\n  \n<h3 style=\"text-align:center\">This is how the normal image look like after being sherpen \ud83d\udc47\ud83d\udc47\ud83d\udc47<\/h3>\n<div class=\"column\">\n    <img src=\"https:\/\/i.imgur.com\/XOMTx4w.png\" style=\"width:500px;height:250px;\">\n    <center><p style=\"padding-left:380px;color:red\">Fig4:image after smoothing<\/p><\/center>\n  <\/div>","670d99be":"### RGB COLOR SPACE :\n#### RGB Color Space:\n--> RGB color space is one of the well-known color space represented by Red, Green, and Blue coordinates of the 3-D coordinate system. In more technical terms, RGB describes a color as a tuple of three components. Each component can take a value between 0 and 255, where the tuple (0, 0, 0) represents black and (255, 255, 255) represents white. Zeroth, first and second component of the tuple represents the amount of Red, Green, and Blue respectively.","6e16bea8":"### Resizing","c8d35b70":"### How it works?\nWe use the function `cv2.filter2D()` which will take our filter and apply that on the top of the image as I explained above, that how a blurring and smoothing generally works.\nAs a result of the blurring, you can see that the image has got so much blurred.\n####  Type of Filters: \nThere are two types of filters 1.LPF(Low Pass Filter) and 2. HPF(High Pass Filter). LPFs helps to blur an image by removing high-frequency elements like- noise, edges. But, HPFs are the opposite of LPFs, the help to find edges in an image.These are some fundamental theories that should be in mind forever CV aspirant.  \nN.B- there are also some filters that do blurring without removing edges of an image. \n#### Arguments:\nAs arguments, we are gonna pass,\n1. \"src\" --> input image\n2. \"desired depth\" --> we are gonna use `-1` and this is the value you are gonna use often for, which means keeping the input and output depth the same.\n3. \"Kernel\" --> kernel that we want to use.","4873a6bf":"## cv2.blur():\nIn the case of `cv2.blur()` what happens is that it takes the average of all the pixes of an image that belongs to the under of the filter and replace that value with the center pixel and keep doing that by sliding itself again and again. This filter removes edges from an image for blurring purposes.People sometimes call this \"Average Blurring\". \n##### Analysis of the output:\n1. **For the Blue Brick Image** By looking at the output it is quite clear that it removes the edges and the gap inside each letter is also blended which makes clear that the o\/p image is a blurred image.  \n2. **For the human image**: Output of it is kind of blurry. It is not removing noise.","dc272642":"### Table of content:\n1. Color Space\n      - RGB Color Space\n      - HSV color space\n      - HSL color space\n2. Basic Geometric Transformations\n      - Image Translation\n      - Image Resizing\n      - Image Rotating\n      - Image cropping\n3. Thresholding\n      - Normal Thresholding\n      - Adaptive Thresholding\n4. Bluring  and Smoothing\n      - Gaussian Blurring\n      - Median Blurring\n      - Using Builtin Kernels(cv2.blur())\n      - Using user-defined Kernels(cv2.filter2D())\n5. Color Histograms\n      - How to create a normal Histogram?\n      - How to create a histogram for all color channels?\n      - How to create a histogram for a masked portion an image?\n      - How to perform histogram equalizer?\n6. Line Detection","9674b83a":"## how to create histogram for a masked portion an image?\nSay, you have an image and you want the color histogram of a specific ROI (region of Interest), so to extract that ROI you have to cheate a mask, and then you will do **cv2.bitwise_and()** for getting that ROI. After you get that you just have to follow the above process to get a color histogram and don't forget to pass **mask = the_mask_you_have_created**.","ccd84475":"##   Median Blur\nSo, how Median Blur is different from the other ones? Actually, in the median blur, it takes the average of all pixels which are under the filter and then replace the central pixel with that value. And, if the noise type is salt-paper type then you will find using Median blurring very useful. As you can see in the 2nd image with balloons. in the **cv2.medianBlur()** we pass the source image and kernel size(kernel size must be odd and positive).\n1. you can use it for Salt-paper noise removing\n2. you can also use it for blurring too.","2c606604":"I will be discussing different Image preprocessing techniques, that we can apply in different images.I also have discussed about different object detection techniques like, **Template Matching,Corner Detection,Harris Corner Detection,Shi-Tomasi,Corner Detection,Edge detection,Contour Detection,Furier Transform,Face Detection with OpenCV Haar Cascade. **\nSo, if want to check it out ckick <a href=\"https:\/\/setosa.io\/ev\/image-kernels\/\" class=\"btn btn-primary\" style=\"color:white;\">Here<\/a>","39989e99":"## Learn Image Processing\n### What is Colorspace?\n--> Color is a continuous phenomenon, it means there is an infinite number of colors. But, the human eye and perception are limited. So, to identify those colors we need a medium or representation of those colors and this representation of color is called color space. In technical terms, a color model or color space is a specification of a 3-D coordinate system and a subspace within that system where each color is represented by a single point.","1475ea8b":"<h1 style=\"background-color:rgb(183, 0, 255);color: rgb(255, 255, 255);text-align: center;padding-top: 30px;padding-bottom: 30px;\">Bluring and Smoothing<\/h1>","b0f2ca4b":"## Color Histograms:\nIn the color histogram, we visualize the distribution of pixel values in R, G, and B color channels. This helps us to understand which pixel values are more in number for each color channel. we need to use this when we need to analyze the color distribution of an image. And I hope you guys know the concept behind histograms.\nThis topic will be having three sub-topics,\n1. How to create a normal Histogram?\n2. How to create a histogram for all color channels?\n3. how to create a histogram for a masked portion an image?\n4. how to perform histogram equalizer?","cf90abf5":"### RGB to HSV Colorspace Convertion:\n#### HSV Color Space:\n--> The name HSV is coming from the three coordinates of the color model which are Hue, Saturation, and Value. It is also a cylindrical color model where the radius of that cylinder implies Saturation, Vertical axis implies the Value and the angle represents the Hue. Hue is the dominant color as perceived by as an observer, Saturation is the amount of white light mixed with a hue and Value is the chromic notion of intensity, lower the Value, the color gets more similar to black and higher the value, the color gets more similar to the color itself. By changing these parameters we can generate different colors."}}