{"cell_type":{"1186edb3":"code","e89b6951":"code","e316a8ca":"code","ba7e8c17":"code","710d7384":"code","5126b39f":"code","b89839b7":"code","55701405":"code","b2686066":"code","4fe088fe":"code","d1b23ac0":"code","17e20196":"code","206a9f7f":"code","19f7b7df":"code","782fa724":"code","077ff087":"code","c4f9e42d":"code","3ae895bc":"code","7429258b":"code","b1c150cb":"code","cdf2fef7":"code","0d252501":"code","89173686":"markdown","6727992f":"markdown","11a4a2e4":"markdown","f86409fe":"markdown","9cbbda23":"markdown","83ff9e33":"markdown"},"source":{"1186edb3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e89b6951":"df = pd.read_csv(os.path.join(dirname, 's11.csv'))","e316a8ca":"# Python \u22653.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn \u22650.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\ntry:\n    # %tensorflow_version only exists in Colab.\n    %tensorflow_version 2.x\nexcept Exception:\n    pass\n\n# TensorFlow \u22652.0 is required\nimport tensorflow as tf\nfrom tensorflow import keras\nassert tf.__version__ >= \"2.0\"\n\n%load_ext tensorboard\n\n# Common imports\nimport numpy as np\nimport os\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# Where to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"deep\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)","ba7e8c17":"df.head(5)","710d7384":"temp_df = df[['game_length', 'result', 'team_1', 'team_2']] # Select only interests\nblue = temp_df['team_1']\nred = temp_df['team_2']\nn = len(df)\n\nblue_champs = []\nred_champs = []\nfor i in range(0,n):\n    blue_champs += [blue[i].split(',')]\n    red_champs += [red[i].split(',')]\n    \ntop = []\njg = []\nmid = []\nadc = []\nsup = []\nfor i in range(0, n):\n    top += [blue_champs[i][0]]\n    jg += [blue_champs[i][1]]\n    mid += [blue_champs[i][2]]\n    adc += [blue_champs[i][3]]\n    sup += [blue_champs[i][4]]\n    \ntop_2 = []\njg_2 = []\nmid_2 = []\nadc_2 = []\nsup_2 = []\nfor i in range(0, n):\n    top_2 += [red_champs[i][0]]\n    jg_2 += [red_champs[i][1]]\n    mid_2 += [red_champs[i][2]]\n    adc_2 += [red_champs[i][3]]\n    sup_2 += [red_champs[i][4]]","5126b39f":"data = temp_df.drop(columns=['team_1','team_2'])\n# blue team\ndata['top1'] = top\ndata['jg1'] = jg\ndata['mid1'] = mid\ndata['adc1'] = adc\ndata['sup1'] = sup\n# red team\ndata['top2'] = top_2\ndata['jg2'] = jg_2\ndata['mid2'] = mid_2\ndata['adc2'] = adc_2\ndata['sup2'] = sup_2","b89839b7":"data.head(10)","55701405":"from sklearn.preprocessing import OneHotEncoder\n#y = pd.get_dummies(data.top1, prefix='top1')\nenc = OneHotEncoder()\nonly_champs = data.drop(columns=['game_length', 'result'])\nonly_champs.head(5)\nonly_champs_onehot = enc.fit_transform(only_champs)","b2686066":"# print(only_champs_onehot)\nenc.get_params()","4fe088fe":"# Convert game_length to float and normalize\nimport re\ndate_str = data.game_length\nm = 2717 #longest games are 45m 17s\n\nfor i in range(len(date_str)):\n    if type(date_str[i]) == str:\n        p = re.compile('\\d*')\n        min = float(p.findall(date_str[i][:2])[0])\n        temp = p.findall(date_str[i][-3:])\n        for j in temp:\n            if j != '':\n                sec = float(j)\n                break\n        date_str[i] = (60*min+sec)\/m\n    else: \n        date_str[i] = date_str[i]\/m","d1b23ac0":"# Now we have the X we want\n#except_champs = data.drop(columns=['result','top1','jg1','mid1','adc1','sup1','top2','jg2','mid2','adc2','sup2'])\n# sparse_to_df = pd.DataFrame.sparse.from_spmatrix(only_champs_onehot)\nprint(type(only_champs_onehot))\nsparse_to_df = pd.DataFrame.sparse.from_spmatrix(only_champs_onehot)\nprint(sparse_to_df.shape)\nprint(date_str.shape)\n\nX = date_str.to_frame().join(sparse_to_df).dropna()\nX = np.asarray(X).astype('float32')","17e20196":"print(type(X))\nprint(X.shape)\n# print(X.isnull())","206a9f7f":"y = data['result']\nfor i in range(len(y)):\n    if y[i] == \"Victory\":\n        y[i] = 1\n    else:\n        y[i] = 0\ny = np.asarray(y).astype('float32')","19f7b7df":"from sklearn.model_selection import train_test_split\nimport math\n\nX_train_full, X_test, y_train_full, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n#len(X_train) = 3222\nl = math.floor(3222*0.8)\nX_valid, X_train = X_train_full[:l], X_train_full[l:]\ny_valid, y_train = y_train_full[:l], y_train_full[l:]\nprint(y_valid.shape)\nprint(X_valid.shape)","782fa724":"model = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=(755,)),\n    keras.layers.Dense(30, activation=\"relu\", name=\"layer_1\"),\n    keras.layers.Dropout(rate=0.2),\n    keras.layers.Dense(16, activation=\"relu\", name=\"layer_2\"),\n    keras.layers.Dropout(rate=0.2),\n    keras.layers.Dense(16, activation=\"relu\", name=\"layer_3\"),\n    keras.layers.Dropout(rate=0.2),\n    keras.layers.Dense(1, activation=\"sigmoid\", name=\"layer_4\")\n])","077ff087":"model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()","c4f9e42d":"model.fit(X_train, y_train, epochs=50, batch_size=1)","3ae895bc":"test_loss, test_acc = model.evaluate(X_test, y_test)\nprint('accuracy', test_acc)","7429258b":"from sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=2000, max_leaf_nodes=32, n_jobs=-1)\nrnd_clf.fit(X_train, y_train)\n","b1c150cb":"y_val_pred = rnd_clf.predict(X_valid)","cdf2fef7":"val_acc = np.sum(y_val_pred == y_valid)\/len(y_valid)\nprint(\"validation accuracy: \"+str(val_acc))","0d252501":"y_test_pred = rnd_clf.predict(X_test)\ntest_acc = np.sum(y_test_pred == y_test)\/len(y_test)\nprint(\"test accuracy: \"+str(test_acc))","89173686":"## Let's try Neural Network","6727992f":"With random forest we got an immediate increase of 12% and now we can predict much better.","11a4a2e4":"## Train test set","f86409fe":"## Let's Clean up the dataset\n","9cbbda23":"Neural Network with dropouts does not seem to predict too well with accuracy of only 65.6%. Originally I thought this would be great since my dataset would look a lot like MNIST data with Lots of features.","83ff9e33":"## Let's try Random Forest Instead!"}}