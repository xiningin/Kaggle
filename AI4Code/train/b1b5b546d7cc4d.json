{"cell_type":{"2b640ca3":"code","5636b7df":"code","dea02070":"code","f31392cc":"code","1a90a314":"code","e83f352c":"code","4820debd":"code","bec9331f":"code","8a5b773f":"code","44513c61":"code","491053d8":"code","d0c6b5a3":"code","3e9e347f":"code","024adaa5":"code","2d573c22":"code","7895b6ff":"code","7ee07659":"code","3af8c58b":"code","c36d7495":"code","d9aa1cde":"code","a9ad9153":"code","d458241d":"code","7d65d62e":"code","8c5369c2":"code","c02b891e":"code","0384f576":"code","d62e8f48":"code","6083c12f":"code","4d24db0f":"code","69e418ec":"code","09621759":"code","bcac7e90":"code","14bcbfb7":"code","ba42b1d3":"code","7d2df887":"code","1f7a68d1":"code","9468afed":"code","30e6523b":"code","360430e7":"code","286faf2b":"code","ae636a32":"code","0989df19":"code","206fd8c6":"code","7117acf5":"code","2d62a21e":"code","7a85e73a":"code","323de841":"code","58caee04":"code","d5e67ca0":"markdown","abe15f5f":"markdown","9fd766bf":"markdown","46d19a94":"markdown","e476d0ac":"markdown","096cfc62":"markdown","17f9b85b":"markdown","fcf072a1":"markdown","11e2794f":"markdown","53ed2b4f":"markdown","95d3c2fa":"markdown","7e37ec96":"markdown","6f69dd33":"markdown","9f855751":"markdown","3cbf02f9":"markdown"},"source":{"2b640ca3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/support-ticket-urgency-classification\/\"))\n\n# Any results you write to the current directory are saved as output.","5636b7df":"import os, sys\n\nimport numpy as np\n\nfrom keras.models import Model\n\nfrom keras.layers import Input, Dense, Flatten\nfrom keras.layers import Conv1D, MaxPooling1D,Dropout\nfrom keras.layers import Embedding\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.utils import to_categorical","dea02070":"train = pd.read_csv(\"..\/input\/support-ticket-urgency-classification\/all_tickets-1551435513304.csv\",header=0)","f31392cc":"train.head()","1a90a314":"train.drop([\"ticket_type\",\"category\",\"sub_category1\",\"sub_category2\",\"business_service\",\"impact\"],inplace=True,axis=1)","e83f352c":"train.drop(\"title\",inplace=True,axis = 1)","4820debd":"train.shape","bec9331f":"X = train.body.values\nY = train.urgency.values","8a5b773f":"from sklearn.model_selection import train_test_split","44513c61":"X_train,X_test,y_train,y_test = train_test_split(X,Y, test_size = 0.2 , random_state = 2, stratify=Y)\n# X_train.shape","491053d8":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=250,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nprint(\"Words used in less priority 1 tickets\")\nshow_wordcloud(train.body[train.urgency == 1])\nprint(\"Words used in less priority 2 tickets\")\nshow_wordcloud(train.body[train.urgency == 2])\nprint(\"Words used in less priority 3 tickets\")\nshow_wordcloud(train.body[train.urgency == 3])","d0c6b5a3":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","3e9e347f":"# Prepare tokenizer\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\n\nword_Index = tokenizer.word_index\n\nvocab_Size = len(word_Index) + 1\nprint('Found %s unique tokens.' % vocab_Size)","024adaa5":"# Prepare tokenizer\ntokenizer_1 = Tokenizer()\ntokenizer_1.fit_on_texts(X_test)\n\nword_Index_1 = tokenizer_1.word_index\n\nvocab_Size_1 = len(word_Index_1) + 1\nprint('Found %s unique tokens.' % vocab_Size_1)","2d573c22":"# integer encode the documents\nsequences = tokenizer.texts_to_sequences(X_train)\nprint(X_train[1], sequences[1])\nprint(\"------------------------#################-------------------------\")\nsequences_test = tokenizer.texts_to_sequences(X_test)\nprint(X_test[1], sequences_test[1])\n#for i in sequences:\n#    print (len(i))","7895b6ff":"MAX_SEQUENCE_LENGTH = 1000\n\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\ndata_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n\nprint('Shape of data tensor:', data.shape)\nprint('Shape of data tensor:', data_test.shape)","7ee07659":"# split the data into a training set and a test set\nX_train = data\n\nX_test = data_test\n\ny_train = y_train\n\ny_test = y_test","3af8c58b":"Y_train = to_categorical(y_train)\nY_test = to_categorical(y_test)","c36d7495":"y_train.shape","d9aa1cde":"PATH_glove = \"..\/input\/glove-6b\"\n\nembeddings_index = {}\nf = open(os.path.join(PATH_glove, 'glove.6B.50d.txt'), encoding=\"utf8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","a9ad9153":"embedding_Matrix = np.zeros((vocab_Size, 50))\nfor word, i in word_Index.items():\n    embedding_Vector = embeddings_index.get(word)\n    if embedding_Vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_Matrix[i] = embedding_Vector\n\nprint (embedding_Matrix.shape)","d458241d":"embedding_layer = Embedding(vocab_Size,\n                            50,\n                            weights=[embedding_Matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=True)","7d65d62e":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx = Conv1D(128, 5, activation='relu')(embedded_sequences)\nx = MaxPooling1D(4)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = MaxPooling1D(4)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = MaxPooling1D(4)(x)  # global max pooling\nx = Flatten()(x)\nx = Dense(100, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(50, activation='relu')(x)\nx = Dropout(0.2)(x)\npreds = Dense(4, activation='softmax')(x)\n\nmodel = Model(sequence_input, preds)","8c5369c2":"model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","c02b891e":"# summarize the model\nprint(model.summary())","0384f576":"model.fit(X_train, Y_train, epochs=5,validation_split=0.2,batch_size=100)","d62e8f48":"Y_pred = model.predict(X_test)\nprint(Y_pred)","6083c12f":"scores =model.evaluate(X_test, Y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","4d24db0f":"# print the confusion matrix\nmetrics.confusion_matrix(y_test, y_pred)","69e418ec":"from keras.datasets import imdb #A utility to load a dataset\n\nfrom keras.models import Sequential\n\nfrom keras.layers import Dense, LSTM, Dropout, Embedding,BatchNormalization,Activation,GRU,CuDNNLSTM,CuDNNGRU,Bidirectional\nfrom keras.layers import Conv1D, MaxPooling1D\n\nfrom keras.preprocessing import sequence #To convert a variable length sentence into a prespecified length\n","09621759":"embedding_vector_length = 150\n\nmodel_LSTM = Sequential()\n\nmodel_LSTM.add(Embedding(vocab_Size, embedding_vector_length, input_length=MAX_SEQUENCE_LENGTH))\nmodel_LSTM.add(Dropout(0.2))\nmodel_LSTM.add(LSTM(100))\nmodel_LSTM.add(Dropout(0.2))\nmodel_LSTM.add(BatchNormalization())\nmodel_LSTM.add(Dense(100))\nmodel_LSTM.add(Dropout(0.2))\nmodel_LSTM.add(BatchNormalization())\nmodel_LSTM.add(Activation(activation='relu'))\nmodel_LSTM.add(Dense(50))\nmodel_LSTM.add(Dropout(0.2))\nmodel_LSTM.add(BatchNormalization())\nmodel_LSTM.add(Activation(activation='relu'))\nmodel_LSTM.add(Dense(20))\nmodel_LSTM.add(Dropout(0.2))\nmodel_LSTM.add(BatchNormalization())\nmodel_LSTM.add(Activation(activation='relu'))\nmodel_LSTM.add(Dense(4, activation='softmax'))","bcac7e90":"model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model_LSTM.summary())","14bcbfb7":"model_LSTM.fit(X_train, Y_train, epochs=4, batch_size=256,validation_split=0.2)","ba42b1d3":"scores =model_LSTM.evaluate(X_test, Y_test, verbose=1,batch_size=500)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","7d2df887":"print(\"Accuracy: %.2f%%\" % (scores[1]*100))","1f7a68d1":"Y_pred =model_LSTM.predict(X_test)\nprint(Y_pred)","9468afed":"embedding_vector_length = 150\n\nmodel_GRU = Sequential()\n\nmodel_GRU.add(Embedding(vocab_Size, embedding_vector_length, input_length=MAX_SEQUENCE_LENGTH))\nmodel_GRU.add(Dropout(0.2))\nmodel_GRU.add(GRU(130))\nmodel_GRU.add(Dropout(0.2))\nmodel_GRU.add(BatchNormalization())\nmodel_GRU.add(Dense(100))\nmodel_GRU.add(Dropout(0.2))\nmodel_GRU.add(BatchNormalization())\nmodel_GRU.add(Activation(activation='relu'))\nmodel_GRU.add(Dense(50))\nmodel_GRU.add(Dropout(0.2))\nmodel_GRU.add(BatchNormalization())\nmodel_GRU.add(Activation(activation='relu'))\nmodel_GRU.add(Dense(20))\nmodel_GRU.add(Dropout(0.2))\nmodel_GRU.add(BatchNormalization())\nmodel_GRU.add(Activation(activation='relu'))\nmodel_GRU.add(Dense(4, activation='softmax'))","30e6523b":"model_GRU.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model_GRU.summary())","360430e7":"model_GRU.fit(X_train, Y_train, epochs=4, batch_size=256,validation_split=0.2)","286faf2b":"scores =model_GRU.evaluate(X_test, Y_test, verbose=1,batch_size=500)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","ae636a32":"embedding_vector_length = 300\n\nmodel_BDLSTM = Sequential()\n\nmodel_BDLSTM.add(Embedding(vocab_Size, embedding_vector_length, input_length=MAX_SEQUENCE_LENGTH))\n#model_BDLSTM.add(Bidirectional(CuDNNGRU(100,return_sequences=True)))\nmodel_BDLSTM.add(Bidirectional(CuDNNGRU(75)))\nmodel_BDLSTM.add(Dropout(0.2))\nmodel_BDLSTM.add(Dense(10))\nmodel_BDLSTM.add(Dropout(0.2))\nmodel_BDLSTM.add(Activation(activation='relu'))\nmodel_BDLSTM.add(Dense(10))\nmodel_BDLSTM.add(Dropout(0.2))\nmodel_BDLSTM.add(BatchNormalization())\nmodel_BDLSTM.add(Activation(activation='relu'))\nmodel_BDLSTM.add(Dense(8))\nmodel_BDLSTM.add(Dropout(0.2))\nmodel_BDLSTM.add(Activation(activation='relu'))\nmodel_BDLSTM.add(Dense(8))\nmodel_BDLSTM.add(Dropout(0.2))\nmodel_BDLSTM.add(BatchNormalization())\nmodel_BDLSTM.add(Activation(activation='relu'))\n\nmodel_BDLSTM.add(Dense(4, activation='softmax'))","0989df19":"model_BDLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model_BDLSTM.summary())","206fd8c6":"model_BDLSTM.fit(X_train, Y_train, epochs=5, batch_size=256,validation_split=0.2)","7117acf5":"scores =model_BDLSTM.evaluate(X_test, Y_test, verbose=1,batch_size=500)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","2d62a21e":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx = Bidirectional(CuDNNLSTM(128))(embedded_sequences)\nx = Dropout(0.2)(x)\nx = Dense(32, activation='relu')(x)\nx = Dropout(0.2)(x)\npreds = Dense(4, activation='softmax')(x)\n","7a85e73a":"model = Model(sequence_input, preds)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nprint(model.summary())","323de841":"model.fit(X_train, Y_train, epochs=9,validation_split=0.2,batch_size=128)","58caee04":"scores =model.evaluate(X_test, Y_test, verbose=1,batch_size=500)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","d5e67ca0":"* Before a program can process natural language, we need identify the words that constitute a string of characters. This is important because the meaning of text generally depends on the relations of words in that text.\n* By default text on a computer is represented through String values. These values store a sequence of characters (nowadays mostly in UTF-8 format). The first step of an NLP pipeline is therefore to split the text into smaller units corresponding to the words of the language we are considering. In the context of NLP we often refer to these units as tokens, and the process of extracting these units is called tokenization. Tokenization is considered boring by most, but it's hard to overemphasize its importance, seeing as it's the first step in a long pipeline of NLP processors, and if you get this step wrong, all further steps will suffer.","abe15f5f":"* The similarity metrics used for nearest neighbor evaluations produce a single scalar that quantifies the relatedness of two words. This simplicity can be problematic since two given words almost always exhibit more intricate relationships than can be captured by a single number. For example, man may be regarded as similar to woman in that both words describe human beings; on the other hand, the two words are often considered opposites since they highlight a primary axis along which humans differ from one another.\n\n\n* In order to capture in a quantitative way the nuance necessary to distinguish man from woman, it is necessary for a model to associate more than a single number to the word pair. A natural and simple candidate for an enlarged set of discriminative numbers is the vector difference between the two word vectors. GloVe is designed in order that such vector differences capture as much as possible the meaning specified by the juxtaposition of two words.\n![alt text](https:\/\/nlp.stanford.edu\/projects\/glove\/images\/man_woman.jpg)","9fd766bf":"## Reading Data from CSV file","46d19a94":"* LSTM layer is a verient of Recurrent layer in RNN\n#### What is RNN?\n\n* Recurrent nets are cool, they're useful for learning sequences of data. Input. Hidden state. Output. \n* It has a weight matrix that connects input to hidden state. But also a weight matrix that connects hidden state to hidden state at previous time step.\n* So we could even think of it as the same feedforward network connecting to itself overtime (unrolled) since passing in not just input in next training iteration but input + previous hidden state\n![alt text](http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/RNN-unrolled.png \"Logo Title Text 1\")\n#### Problem with RNN\n* If we want to predict the last word in the sentence \"I am French (2000 words later) i speak fluent French\". We need to be able to remember long range dependencies. RNN's are bad at this. They forget the long term past easily.\n![alt text](http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/RNN-longtermdependencies.png \"Logo Title Text 1\")\n* This is called the \"Vanishing Gradient Problem\". The Gradient exponentially decays as its backpropagated.\n#### The LSTM Cell (Long-Short Term Memory Cell)\n\n* We've placed no constraints on how our model updates, so its knowledge can change pretty chaotically: at one frame it thinks the characters are in the US, at the next frame it sees the characters eating sushi and thinks they're in Japan, and at the next frame it sees polar bears and thinks they're on Hydra Island. \n\n* This chaos means information quickly transforms and vanishes, and it's difficult for the model to keep a long-term memory. So what you\u2019d like is for the network to learn how to update its beliefs (scenes without Bob shouldn't change Bob-related information, scenes with Alice should focus on gathering details about her), in a way that its knowledge of the world evolves more gently.\n\n* It replaces the normal RNN cell and uses an input, forget, and output gate. As well as a cell state\n![alt text](https:\/\/www.researchgate.net\/profile\/Mohsen_Fayyaz\/publication\/306377072\/figure\/fig2\/AS:398082849165314@1471921755580\/Fig-2-An-example-of-a-basic-LSTM-cell-left-and-a-basic-RNN-cell-right-Figure.ppm \"Logo Title Text 1\")\n\n![alt text](https:\/\/kijungyoon.github.io\/assets\/images\/lstm.png \"Logo Title Text 1\")\n* These gates each have their own set of weight values. The whole thing is differentiable (meaning we compute gradients and update the weights using them) so we can backprop through it\n\n* We want our model to be able to know what to forget, what to remember. So when new a input comes in, the model first forgets any long-term information it decides it no longer needs. Then it learns which parts of the new input are worth using, and saves them into its long-term memory.\n* And instead of using the full long-term memory all the time, it learns which parts to focus on instead.\n\n* Basically, we need mechanisms for forgetting, remembering, and attention. That's what the LSTM cell provides us.\n","e476d0ac":"## Vizualizing the most used words in raising a ticket request","096cfc62":"## Global Vectors for Word Representation(GLOVE) Embaddings","17f9b85b":"## Building a 1D Convolution layer based network for Text analysis","fcf072a1":"## Gated Recurrent Unit(GRU) based model ","11e2794f":"## Biderectional LSTM based model","53ed2b4f":"## Bidirectional LSTM With GLOVE Embeddings","95d3c2fa":"* It involves duplicating the first recurrent layer in the network so that there are now two layers side-by-side, then providing the input sequence as-is as input to the first layer and providing a reversed copy of the input sequence to the second.\n* The use of providing the sequence bi-directionally was initially justified in the domain of speech recognition because there is evidence that the context of the whole utterance is used to interpret what is being said rather than a linear interpretation.\n\n![](https:\/\/ars.els-cdn.com\/content\/image\/1-s2.0-S0010482518300738-gr3.jpg)","7e37ec96":"## Tokenization","6f69dd33":"### 1Dimention Convolution layer\n* A CNN works well for identifying simple patterns within your data which will then be used to form more complex patterns within higher layers. A 1D CNN is very effective when you expect to derive interesting features from shorter (fixed-length) segments of the overall data set and where the location of the feature within the segment is not of high relevance.\n\n* This applies well to the analysis of time sequences of sensor data (such as gyroscope or accelerometer data). It also applies to the analysis of any kind of signal data over a fixed-length period (such as audio signals). Another application is NLP (although here LSTM networks are more promising since the proximity of words might not always be a good indicator for a trainable pattern).\n\n![alt text](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*aBN2Ir7y2E-t2AbekOtEIw.png)`","9f855751":"## LongShortTime Memory(LSTM) based Model ","3cbf02f9":"#### GRU\n* As mentioned above to solve the vanishing gradient problem of a standard RNN, GRU uses, so called, update gate and reset gate. Basically, these are two vectors which decide what information should be passed to the output. The special thing about them is that they can be trained to keep information from long ago, without washing it through time or remove information which is irrelevant to the prediction.\n![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*6eNTqLzQ08AABo-STFNiBw.png)"}}