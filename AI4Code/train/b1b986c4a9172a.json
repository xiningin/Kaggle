{"cell_type":{"64f08cb4":"code","90ea3fb8":"code","2b95bdb8":"code","e9420d01":"code","0372ff99":"code","b2606acd":"code","34e68785":"code","7c295b68":"code","82fac016":"code","1952dd8b":"code","8fd66994":"code","9a0c892c":"code","5043004f":"code","aef58159":"code","bfdcf963":"code","d340de69":"markdown","bae54738":"markdown","46716ad8":"markdown","6582ccf4":"markdown","794b428f":"markdown","308d3564":"markdown","8f3347cf":"markdown","b19bae6c":"markdown","9e50f206":"markdown","432e4f2f":"markdown","c72e3952":"markdown","6c6a0bc9":"markdown","f995182f":"markdown","929a0f67":"markdown","9be75d91":"markdown","3401b29e":"markdown","d4f71630":"markdown","ce90b0c6":"markdown","62f10031":"markdown","7614bdfc":"markdown","b9bf9458":"markdown","84b7d300":"markdown","a856b7ca":"markdown","197447d4":"markdown","9f63eebc":"markdown","07110293":"markdown","b607ae15":"markdown","33136889":"markdown","bd22f1f7":"markdown","9b80c2ed":"markdown"},"source":{"64f08cb4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier","90ea3fb8":"original = pd.read_csv('..\/input\/forest-cover-type-prediction\/train.csv')\nsynthetic = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')","2b95bdb8":"print('original', original.shape)\nprint('synthetic', synthetic.shape)","e9420d01":"(original.columns == synthetic.columns).all()","0372ff99":"(original.dtypes == synthetic.dtypes).all()","b2606acd":"target = 'Cover_Type'\ndef plot(df, name):\n    count = df[target].value_counts().sort_index()\n    plt.ticklabel_format(useOffset=False, style='plain')\n    plt.bar(count.index, count)\n    plt.xlabel('label\/class')\n    plt.title(name)\n    plt.show()\nplot(synthetic, 'synthetic')\nplot(original, 'original')","34e68785":"pd.DataFrame(dict(\n    synthetic = synthetic.Aspect.describe(),\n    original = original.Aspect.describe()\n)).round(0)","7c295b68":"def plot(df, name):\n    df.Aspect.value_counts().sort_index().plot(figsize=(11,5))\n    plt.title(name)\n    plt.xlabel('Aspect')\n    plt.show()\nplot(original, 'original')\nplot(synthetic, 'synthetic')","82fac016":"x = synthetic.copy()\nfor cname in ['Elevation', 'Aspect', 'Slope',\n       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']:\n    before = len(x)\n    x = x[x[cname].between(\n        original[cname].min(),\n        original[cname].max(),\n    )]\n    after = len(x)\n    print(cname, after-before, before, after, sep='\\t')\ndef clean_one_hot(cnames, x=x):\n    cnt = x[cnames].sum(axis=1)\n    before = len(x)\n    x = x[cnt==1]\n    after = len(x)\n    print(after-before, before, after, sep='\\t')\n    return x\nx = clean_one_hot([\n    'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4'\n])\nx = clean_one_hot([\n    'Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n    'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n    'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n    'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n    'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n    'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n    'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n    'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n    'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n    'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40',\n])\nsynthetic = x\ndel x","1952dd8b":"high = original.Aspect.between(340,359)\nlow = original.Aspect.between(1,20)\nhigh = original[high].copy()\nhigh['isHigh'] = 1\nlow = original[low].copy()\nlow['isHigh'] = 0\nx = pd.concat([high, low])\ny = x.pop('isHigh')\nprint('total sample', len(x))\nprint('target class balance', y.mean())\nx.pop('Aspect') # important, it's too easy if the classifier can see the Aspect directly\nx.pop('Id')\nx = x.drop(columns=['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']) # explained later\nx = x.to_numpy()\ny = y.to_numpy()\ntrain, val = train_test_split(np.arange(len(x)), stratify=y, random_state=0)\nest = RandomForestClassifier(random_state=0)\nest.fit(x[train], y[train])\nprint('train score', accuracy_score(y[train], est.predict(x[train])))\nprint('val score', accuracy_score(y[val], est.predict(x[val])))","8fd66994":"high = original.Aspect.between(340,359)\nlow = original.Aspect.between(1,20)\nhigh = original[high].copy()\nhigh['isHigh'] = 1\nlow = original[low].copy()\nlow['isHigh'] = 0\nx = pd.concat([high, low])\ny = x.pop('isHigh')\nprint('total sample', len(x))\nprint('target class balance', y.mean())\nx.pop('Aspect') # important, it's too easy if the classifier can see the Aspect directly\nx.pop('Id')\n# Hillshade data are not dropped this time\nx = x.to_numpy()\ny = y.to_numpy()\ntrain, val = train_test_split(np.arange(len(x)), stratify=y, random_state=0)\nest = RandomForestClassifier(random_state=0)\nest.fit(x[train], y[train])\nprint('train score', accuracy_score(y[train], est.predict(x[train])))\nprint('val score', accuracy_score(y[val], est.predict(x[val])))","9a0c892c":"high = synthetic.Aspect.between(340,359)\nlow = synthetic.Aspect.between(1,20)\nhigh = synthetic[high].sample(999, random_state=0).copy()\nhigh['isHigh'] = 1\nlow = synthetic[low].sample(999, random_state=0).copy()\nlow['isHigh'] = 0\nx = pd.concat([high, low])\ny = x.pop('isHigh')\nx.pop('Aspect') # important, it's too easy if the classifier can see the Aspect directly\nx.pop('Id')\nx = x.drop(columns=['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']) # explained above\nx = x.to_numpy()\ny = y.to_numpy()\nprint('total sample', len(x))\nprint('target class balance', y.mean())\ntrain, val = train_test_split(np.arange(len(x)), stratify=y, random_state=0)\nest = RandomForestClassifier(random_state=0)\nest.fit(x[train], y[train])\nprint('train score', accuracy_score(y[train], est.predict(x[train])))\nprint('val score', accuracy_score(y[val], est.predict(x[val])))","5043004f":"high = synthetic.Aspect.between(340,359)\nlow = synthetic.Aspect.between(1,20)\nhigh = synthetic[high].sample(999, random_state=0).copy()\nhigh['isHigh'] = 1\nlow = synthetic[low].sample(999, random_state=0).copy()\nlow['isHigh'] = 0\nx = pd.concat([high, low])\ny = x.pop('isHigh')\nx.pop('Aspect') # important, it's too easy if the classifier can see the Aspect directly\nx.pop('Id')\n# Hillshade data are not dropped this time\nx = x.to_numpy()\ny = y.to_numpy()\nprint('total sample', len(x))\nprint('target class balance', y.mean())\ntrain, val = train_test_split(np.arange(len(x)), stratify=y, random_state=0)\nest = RandomForestClassifier(random_state=0)\nest.fit(x[train], y[train])\nprint('train score', accuracy_score(y[train], est.predict(x[train])))\nprint('val score', accuracy_score(y[val], est.predict(x[val])))\n","aef58159":"east = original.Aspect.between(70,110)\nwest = original.Aspect.between(260,280)\neast = original[east].sample(500, random_state=0).copy()\neast['isEast'] = 1\nwest = original[west].copy()\nwest['isEast'] = 0\nx = pd.concat([east, west])\ny = x.pop('isEast')\nprint('total sample', len(x))\nprint('target class balance', y.mean())\nx.pop('Aspect') # important, it's too easy if the classifier can see the Aspect directly\nx.pop('Id')\nx = x.drop(columns=['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']) # explained above\nx = x.to_numpy()\ny = y.to_numpy()\ntrain, val = train_test_split(np.arange(len(x)), stratify=y, random_state=0)\nest = RandomForestClassifier(random_state=0)\nest.fit(x[train], y[train])\nprint('train score', accuracy_score(y[train], est.predict(x[train])))\nprint('val score', accuracy_score(y[val], est.predict(x[val])))","bfdcf963":"east = synthetic.Aspect.between(70,110)\nwest = synthetic.Aspect.between(260,280)\neast = synthetic[east].sample(500, random_state=0).copy()\neast['isEast'] = 1\nwest = synthetic[west].sample(500, random_state=0).copy()\nwest['isEast'] = 0\nx = pd.concat([east, west])\ny = x.pop('isEast')\nprint('total sample', len(x))\nprint('target class balance', y.mean())\nx.pop('Aspect') # important, it's too easy if the classifier can see the Aspect directly\nx.pop('Id')\nx = x.drop(columns=['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']) # explained above\nx = x.to_numpy()\ny = y.to_numpy()\ntrain, val = train_test_split(np.arange(len(x)), stratify=y, random_state=0)\nest = RandomForestClassifier(random_state=0)\nest.fit(x[train], y[train])\nprint('train score', accuracy_score(y[train], est.predict(x[train])))\nprint('val score', accuracy_score(y[val], est.predict(x[val])))","d340de69":"The validation-accuracy improves from 57% to 98%, hence Hillshade have strong correlation with Aspect ... ***at least in the original dataset***.","bae54738":"I would like to do a experiment to prove the following:\n1. Aspect of 1 degree-angle would closely resemble 359 degree.\n2. Aspect of 90 degree-angle would be very different from 270 degree.\n\nA classifier should have difficulty to separate the 1st case (because of similiarity), but not the 2nd case.","46716ad8":"Please note that the original one looks shaky, but the synthetic one looks smoother, this is simply because the synthetic one has large number of sample.\n\nIf we smooth-out the shakiness, both of them has similar movement which resembles a trigonometric's [sinusoid](https:\/\/en.wikipedia.org\/wiki\/Sine_wave) curve. I would say that the original one definitely forming a sine wave, but not sure about the synthethic one.\n\nI also would like to mention about the sharp spike on `Aspect=0` on the synthethic one that seems pretty strange.","6582ccf4":"### Leaked Test-Label on Original-Competition","794b428f":"Now this is funny. Only 48% accuracy is indication that the classifier is having difficulty separating the sample that facing East from the one that facing West.\n\nWe can get 85% accuracy on the original dataset, but only able to get 48% accuracy on the synthetic dataset.","308d3564":"### ? Aspect of 90 degree-angle would be very different from 270 degree ?","8f3347cf":"#### Original Dataset","b19bae6c":"#### Synthetic Dataset","9e50f206":"# Target Label\/Class Distribution","432e4f2f":"`Aspect` column in the synthetic-dataset is quite different from what the real-data (original-dataset) should be. Seems like the generation of synthethic data wasn't able to properly generate the periodicity of a feature, and it's also failing to generate the same correlation between features.","c72e3952":"With validation-accuracy only 57% it seems the classifier is having hard time separating `Aspect<20` from `Aspect>340`, because those Aspect values are close\/similar.","6c6a0bc9":"In the original-competition, the **leaked test-label** has imbalance distribution (almost half of them are class 2, which resembles the train-set in the synthethic-dataset).\n\nIn the original-competition, a classifier normally would have no access to the information about class imbalance in the test-set, hence can't utilize the imbalance distribution of the label when doing prediction.\n\nContrary, synthetic's training-set contains information about the class imbalance. The classifier should be able to gain benefit from the class imbalance ***under assumption*** that the imbalance in the synthetic's test-set will be similar.","f995182f":"In this notebook, I would like to compare the (synthetic) dataset from [Tabular Playground Series - Dec 2021](https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021) against the  original competition (non-synthetic) dataset [Forest Cover Type Prediction](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction).","929a0f67":"## Periodicity of Aspect","9be75d91":"# Columns Names and Data-Types","3401b29e":"Only 52% accuracy, pretty similar with our finding on original data, the classifier is having hard time distinguishing similar Aspect (`Aspect<20` versus `Aspect>340`) ... **but let's try to use the Hillshade data now**.","d4f71630":"Now this is funny. The validation-accuracy staying around 52%, while in the original-dataset the validation-accuracy improves drastically from 57% to 98%. Hence my conclusion is:\n\n**In the original dataset, Aspect is strongly-correlated with Hillshade. But this doesn't hold true for the synthetic dataset.**","ce90b0c6":"As shown above, the column's names and data-types are similar.","62f10031":"#### Synthetic Dataset","7614bdfc":"# Aspect","b9bf9458":"#### Original Dataset","84b7d300":"A classifier should be able to easily distinguish samples of 90 degree from 270 degree, since facing East would bring a lot of difference compared to facing West.","a856b7ca":"## Take Away","197447d4":"A classifier should have difficulty to separate samples of 1 degree from 359 degree (due to similarity)","9f63eebc":"**Why I drop `Hillshade` columns?**\n\nI believe Hillshade have a strong correlation with Aspect. E.g. if your house is facing East (`Aspect=90`), then your house will be brighter at morning, compared to other houses facing West that will be darker at morning (`Aspect=270`). Large Hillshade value means brighter, and vice-versa, small Hillshade value means darker.\n\nLet's feed the Hillshade columns back into our classifier to prove the correlation.","07110293":"The original Aspect range is [0,360] angle-degree, but the synthethic range is [-33,407].","b607ae15":"The distribution of synthetic dataset is imbalanced, but the distribution of original dataset is balanced.","33136889":"I would say that validation-score 85% is pretty good. The classifier can easily separate the sample that facing East from the one that facing West.","bd22f1f7":"# Shape","9b80c2ed":"### ? Aspect of 1 degree-angle would closely resemble 359 degree ?"}}