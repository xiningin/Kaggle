{"cell_type":{"bfecd24b":"code","6deb39fe":"code","f9a71e72":"code","3c0b1964":"code","1ac1535b":"markdown","c75607e5":"markdown","a1606445":"markdown","08b94ddd":"markdown","7bb056be":"markdown","4e48b1f5":"markdown","5d7bd3bc":"markdown","3d1bc51e":"markdown"},"source":{"bfecd24b":"import warnings\nwarnings.filterwarnings('ignore')\n\n\nimport pandas as pd\ndata = pd.read_csv('..\/input\/melb_data.csv')\ncols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nX = data[cols_to_use]\ny = data.Price","6deb39fe":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\nmy_pipeline = make_pipeline(Imputer(), RandomForestRegressor())","f9a71e72":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(my_pipeline, X, y, scoring='neg_mean_absolute_error')\nprint(scores)","3c0b1964":"print('Mean Absolute Error %2f' %(-1 * scores.mean()))","1ac1535b":"You may notice that we specified an argument for *scoring*.  This specifies to what measure of model quality we need to report.  The docs for scikit-learn show a [list of options](http:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html).  \n\nIt is a little surprising that we specify *negative* mean absolute error in this case. Scikit-learn has a convention where all metrics are defined so a high number is better.  Using negatives here allows them to be consistent with that convention, though negative MAE is almost unheard of elsewhere.\n\nYou typically want a single measure of model quality to compare between models.  So we take the average across experiments.","c75607e5":"# Conclusion\n\nUsing cross-validation gave us much better measures of model quality, with the added benefit of cleaning up our code (no longer need to keep track of separate train and test sets.  So, it's a good win.\nTHANK YOU","a1606445":"## The Cross-Validation Procedure\n\nIn cross-validation, we run our modeling process on different subsets of the data to get multiple measures of model quality. For example, we can  have 5 **folds** or experiments.  We divide the data into 5 pieces, each being 20% of the full dataset.  \n\n![cross-validation-graphic](https:\/\/i.stack.imgur.com\/1fXzJ.png)\n\n\nWe run an experiment called experiment 1 which uses the first fold as a holdout set, and everything else as training data. This gives us a measure of model quality based on a 20% holdout set, much as we got from using the simple train-test split.  \nWe then run a second experiment, where we hold out data from the second fold (using everything except the 2nd fold for training the model.) This gives us a second estimate of model quality.\nWe repeat this process, using every fold once as the holdout.  Putting this together, 100% of the data is used as a holdout at some point.  \n\nReturning to our example above from train-test split, if we have 5000 rows of data, we end up with a measure of model quality based on 5000 rows of holdout (even if we don't use all 5000 rows simultaneously.\n\n## Trade-offs Between Cross-Validation and Train-Test Split\nCross-validation gives a more accurate measure of model quality, which is  important if you are making a lot of modeling decisions.  However, it can take more time to run, because it estimates models once for each fold.  So it does more total work.\nGiven these tradeoffs, when should you use each approach\nOn small datasets, the extra computational burden of running cross-validation isn't a big deal.  These are also the problems where model quality scores would be least reliable with train-test split.  So, if your dataset is smaller, you should run cross-validation.\n\nFor the same reasons, a simple train-test split is sufficient for larger datasets.  It will run faster, and you may have enough data and need to re-use some of it for holdout.\n\nThere's no simple threshold for what constitutes a large vs small dataset.  If your model takes a couple minute or less to run, it's probably worth switching to cross-validation.  If your model takes much longer to run, cross-validation may slow down your workflow more than it's worth.\n\nAlternatively, you can run cross-validation and see if the scores for each experiment seem close. If each experiment gives the same results, train-test split is probably sufficient.","08b94ddd":"# Example","7bb056be":"First we read the data","4e48b1f5":"# Introduction\n- This is a short and concise notebook explaining what is Cross Validation and how to use it using SkLearn.\n- We are using <a href='https:\/\/www.kaggle.com\/dansbecker\/melbourne-housing-snapshot'>Melbourne Housing Snapshot<\/a> which is a snapshot of a dataset created by <a href='https:\/\/www.kaggle.com\/anthonypino\/melbourne-housing-market'>Tony Pino<\/a>.\n\n","5d7bd3bc":"Finally get the cross-validation scores:","3d1bc51e":"Then specify a pipeline of our modeling steps (It can be very difficult to do cross-validation properly if you arent't using [pipelines](https:\/\/www.kaggle.com\/dansbecker\/pipelines))"}}