{"cell_type":{"134b3f1c":"code","dd9bc6fd":"code","34726daf":"code","692686aa":"code","43232f5d":"code","1384e828":"code","3fff7853":"code","718a0cac":"code","c0218746":"code","1310eda9":"code","5582d144":"code","b49ded4c":"code","5a5d90c1":"code","fb527a8a":"code","6d39d5c7":"code","1ec0dfb2":"code","ad45ef8f":"code","e42052e7":"code","bbbece5a":"code","21c0703d":"code","849a6daa":"code","3a4fc9bb":"code","243bc937":"code","b21547da":"code","5d5d69a7":"code","5362c2b8":"code","683e3a2b":"code","8d577d1e":"code","8d17a5cf":"markdown","0de12508":"markdown","62cfe53b":"markdown","788e729d":"markdown","a1d44a79":"markdown","910f5f27":"markdown","9b6b0a0d":"markdown","241e3182":"markdown","d0a771f1":"markdown","7fd61bc3":"markdown"},"source":{"134b3f1c":"#import the necessary libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib.image as mplimg\nfrom matplotlib.pyplot import imshow\nimport zipfile as zf\nimport os\nimport csv\nimport gc\nimport operator\nimport random\nfrom sklearn.cross_validation import train_test_split\nfrom numpy import array\nfrom numpy import argmax\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom random import shuffle\nfrom IPython.display import Image\nfrom pathlib import Path\n\nfrom keras import layers\nfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\nfrom keras.utils import np_utils\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom keras import optimizers\n\n#load the training data\ntrainData = pd.read_csv(\"..\/input\/whale-categorization-playground\/train.csv\")\n\n#See what is in the data\ntrainData.sample(5)\n","dd9bc6fd":"#trainData['Id'].value_counts()","34726daf":"#show sample image\nImage(filename=\"..\/input\/whale-categorization-playground\/train\/\"+random.choice(trainData['Image'])) ","692686aa":"def prepareImages(data, m, dataset):\n    \n    print(\"Preparing images\")\n    \n    X_train = np.zeros((m, 100, 100, 3))\n    \n    count = 0\n    \n    for fig in data['Image']:\n        #load images into images of size 100x100x3\n        img = image.load_img(\"..\/input\/whale-categorization-playground\/\"+dataset+\"\/\"+fig, target_size=(100, 100, 3))\n        x = image.img_to_array(img)\n        #x = np.expand_dims(x, axis=0)\n        x = preprocess_input(x)\n        X_train[count] = x\n        if (count%500 == 0):\n            print(\"Processing image: \", count+1, \", \", fig)\n        count += 1\n    \n    count = 0\n    \n    print(\"Finished!\")\n            \n    return X_train","43232f5d":"#not used version of the image preparation class\n#def prepareImages(data, m, dataset):\n    \n    #print(\"Preparing images\")\n    \n    #X_train = np.zeros((m, 100, 100, 3))\n    \n    #count = 0\n    \n    #for fig in data['Image']:\n        #file = Path(\"..\/input\/whale-categorization-playground\/\"+dataset+\"\/\"+fig)\n        #if(file.is_file()):\n            #img = image.load_img(file, target_size=(100, 100, 3))\n        #else:\n            #img = image.load_img(\"..\/input\/augmented-data-whale\/data\/\"+fig, target_size=(100, 100, 3))\n        #x = image.img_to_array(img)\n        #x = np.expand_dims(x, axis=0)\n        #x = preprocess_input(x)\n        #X_train[count] = x\n        #if (count%500 == 0):\n           # print(\"Processing image: \", count+1, \", \", fig)\n        #count += 1\n    \n    #count = 0\n    \n   # print(\"Finished!\")\n            \n    #return X_train","1384e828":"def prepareY(Y):\n\n    values = array(Y)\n    print(values.shape)\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(values)\n    print(integer_encoded)\n\n    onehot_encoder = OneHotEncoder(sparse=False)\n    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n    print(onehot_encoded)\n\n    y = onehot_encoded\n    print(y.shape)\n    return y, label_encoder\n\n#the next lines are used to test the code and do not need to run when using it\n#inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n#print(inverted)","3fff7853":"mod = Sequential()\n\nmod.add(Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0', input_shape = (100, 100, 3)))\n\nmod.add(BatchNormalization(axis = 3, name = 'bn0'))\nmod.add(Activation('relu'))\n\nmod.add(MaxPooling2D((2, 2), name='max_pool'))\nmod.add(Conv2D(64, (3, 3), strides = (1,1), name=\"conv1\"))\nmod.add(Activation('relu'))\nmod.add(AveragePooling2D((3, 3), name='avg_pool'))\n\nmod.add(Flatten())\nmod.add(Dense(500, activation=\"relu\", name='rl'))\nmod.add(Dropout(0.8))\nmod.add(Dense(4251, activation='softmax', name='sm'))\n\nprint(mod.output_shape)\n\n#opt = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\nmod.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])","718a0cac":"#I have used many variations of the CNN architecture and choice of optimizer.\n#mod = Sequential()\n\n#mod.add(Conv2D(32, (7, 7), strides = (1, 1), padding='same', name = 'conv0', input_shape = (100, 100, 3)))\n#mod.add(BatchNormalization(axis = 3, name = 'bn0'))\n#mod.add(Activation('relu'))\n#print(mod.output_shape)\n\n#mod.add(MaxPooling2D((2, 2), name='max_pool'))\n#print(mod.output_shape)\n\n#mod.add(Conv2D(64, (3, 3), strides = (1,1), padding='same', name=\"conv1\"))\n#mod.add(Activation('relu'))\n#mod.add(BatchNormalization(axis = 3, name = 'bn1'))\n\n#mod.add(AveragePooling2D((3, 3), name='avg_pool'))\n#mod.add(AveragePooling2D((2, 2), name='avg_pool'))\n#print(mod.output_shape)\n\n#mod.add(Conv2D(128, (3, 3), strides = (1,1), padding='same', name=\"conv2\"))\n#mod.add(Activation('relu'))\n#mod.add(BatchNormalization(axis = 3, name = 'bn2'))\n\n#mod.add(MaxPooling2D((2, 2), name='max_pool2'))\n\n#mod.add(Conv2D(128, (4, 4), strides = (1,1), padding='same', name=\"conv3\"))\n#mod.add(Activation('relu'))\n#mod.add(BatchNormalization(axis = 3, name = 'bn3'))\n\n#mod.add(AveragePooling2D((2, 2), name='avg_pool1'))\n#print(mod.output_shape)\n\n#mod.add(Flatten())\n#print(mod.output_shape)\n\n#mod.add(Dense(500, activation=\"relu\", name='rl'))\n#mod.add(Dropout(0.6))\n\n#mod.add(Dense(500, activation=\"relu\", name='r2'))\n#mod.add(Dropout(0.8))\n\n#mod.add(Dense(4251, activation='softmax', name='sm'))\n\n#print(mod.output_shape)\n\n#opt = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n#mod.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])","c0218746":"X = prepareImages(trainData, 9850, \"train\")\n\n#put all the values of the training data in the range between 0 and 1\nX \/= 255\n\nprint(\"Shape X-train: \", X.shape)","1310eda9":"Y = trainData['Id']\n\nprint(\"Shape Y-train: \", Y.shape)\n\n#The next lines are used for testing - not necessary for the code\n#labels = trainData['Id'].unique()\n#print(\"Labels in data set: \", labels.shape)\n#labelsTrain = Y.unique()\n#print(\"Labels in training set: \", labelsTrain.shape)\n\ny, label_encoder = prepareY(Y)","5582d144":"#used to prepare augmented data - not used here\n#train2 = []\n\n#for i in trainData['Image']:\n    #temp = i.split('.')\n    #temp[0] = temp[0]+'f.jpg'\n    #train2.append(temp[0])\n\n#col = ['Image']\n#train2 = pd.DataFrame(train2, columns=col)\n\n#train3 = []\n\n#for i in trainData['Image']:\n    #temp = i.split('.')\n    #temp[0] = temp[0]+'r.jpg'\n    #train3.append(temp[0])\n    \n#train3 = pd.DataFrame(train3, columns=col)\n\n#train4 = []\n\n#for i in trainData['Image']:\n    #temp = i.split('.')\n    #temp[0] = temp[0]+'r2.jpg'\n    #train4.append(temp[0])\n    \n#train4 = pd.DataFrame(train4, columns=col)\n\n#imageData = np.concatenate((np.reshape(trainData['Image'], (9850, 1)), train2), axis=0)\n#imageData = np.concatenate((imageData, train3), axis=0)\n#imageData = np.concatenate((imageData, train4), axis=0)\n#idData = np.concatenate((trainData['Id'], trainData['Id']), axis=0)\n#idData = np.concatenate((idData, trainData['Id']), axis=0)\n#idData = np.concatenate((idData, trainData['Id']), axis=0)\n#tempData = np.concatenate((imageData, np.reshape(idData, (39400, 1))), axis=1)\n\n#cols = ['Image', 'Id']\n#data = pd.DataFrame(tempData, columns=cols)\n\n#print(data.shape)","b49ded4c":"history = mod.fit(X, y, epochs=100, batch_size=100, verbose=1)\ngc.collect()","5a5d90c1":"#training with augmented data\n#for i in range(0, 70000):\n    #batch = data.sample(32)\n    #Ydata = np.zeros((32, 4251))\n    #X = prepareImages(batch, 32, \"train\")\n    #X \/= 255\n    #ytemp = label_encoder.transform(batch['Id'])\n    #for j in range(0,32):\n        #Ydata[j][ytemp[j]] = 1\n    \n    #history = mod.train_on_batch(X, Ydata)\n\n#print(mod.metrics_names)\n#print(history)","fb527a8a":"#continued training\n#for i in range(0, 197):\n    #batch = data[i*50: i*50+50]\n    #Ydata = np.zeros((50, 4251))\n    #X = prepareImages(batch, 50, \"train\")\n    #X \/= 255\n    #ytemp = label_encoder.transform(batch['Id'])\n    #for j in range(0,50):\n        #Ydata[j][ytemp[j]] = 1\n    \n    #history = mod.train_on_batch(X, Ydata)\n    \n#print(mod.metrics_names)\n#print(history)","6d39d5c7":"#another version of the code to train the CNN\n#gc.collect() \n#Y = trainData['Id']\n#X = prepareImages(trainData, 9850, \"whale-categorization-playground\/train\")\n#y, label_encoder = prepareY(Y)\n\n#X \/= 255\n\n#train2 = []\n\n#for i in trainData['Image']:\n    #temp = i.split('.')\n    #temp[0] = temp[0]+'f.jpg'\n    #train2.append(temp[0])\n\n#col = ['Image']\n#train2 = pd.DataFrame(train2, columns=col)\n\n#gc.collect()\n#X2 = prepareImages(train2, 9850, \"humpback-whale-flipped\/train2\")\n#gc.collect()\n\n#X2 \/= 255\n\n#X = np.concatenate((X, X2), axis=0)\n#y = np.concatenate((y, y), axis=0)\n\n#print(X.shape)\n#print(y.shape)\n#gc.collect() ","1ec0dfb2":"#another version of the code to train the CNN, here using Keras for data augmentation\n#gc.collect()\n#datagen = image.ImageDataGenerator( \n    #featurewise_center=True, \n    #featurewise_std_normalization=True, \n    #rotation_range=20, \n    #width_shift_range=0.2, \n    #height_shift_range=0.2, \n    #horizontal_flip=True)\n\n#datagen.fit(X[0:6000])\n\n#gc.collect()","ad45ef8f":"#mod.fit_generator(datagen.flow(X, y, batch_size=100), steps_per_epoch=len(X) \/ 100, epochs=30, initial_epoch=0)\n#history = mod.fit(X, y, epochs=2, batch_size=100, verbose=1, shuffle=True)\n#gc.collect()","e42052e7":"#plot how the accuracy changes as the model was trained\nplt.plot(history.history['acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.show()","bbbece5a":"#open test data\ntest = os.listdir(\"..\/input\/whale-categorization-playground\/test\/\")\nprint(len(test))\n\n#separate data into different DataFrames due to memory constraints\ncol = ['Image']\ntestData1 = pd.DataFrame(test[0:3899], columns=col)\ntestData2 = pd.DataFrame(test[3900:7799], columns=col)\ntestData3 = pd.DataFrame(test[7800:11699], columns=col)\ntestData4 = pd.DataFrame(test[11700:15609], columns=col)\ntestData = pd.DataFrame(test, columns=col)","21c0703d":"#X_test = prepareImages(testData1, 15610, \"test\")\ngc.collect()\nX = prepareImages(testData1, 3900, \"test\")\nX \/= 255","849a6daa":"predictions1 = mod.predict(np.array(X), verbose=1)\ngc.collect()","3a4fc9bb":"X = prepareImages(testData2, 3900, \"test\")\nX \/= 255\npredictions2 = mod.predict(np.array(X), verbose=1)\ngc.collect()","243bc937":"X = prepareImages(testData3, 3900, \"test\")\nX \/= 255\npredictions3 = mod.predict(np.array(X), verbose=1)\ngc.collect()","b21547da":"X = prepareImages(testData4, 3910, \"test\")\nX \/= 255\npredictions4 = mod.predict(np.array(X), verbose=1)\ngc.collect()","5d5d69a7":"#concatenate all the predictions in the same vector\npredictions = np.concatenate((predictions1, predictions2), axis=0)\npredictions = np.concatenate((predictions, predictions3), axis=0)\npredictions = np.concatenate((predictions, predictions4), axis=0)\ngc.collect()\nprint(predictions.shape)\nprint(predictions)","5362c2b8":"#choose predictions with highest probability. For each value I choose, I set the probability to zero, so it can't be picked again.\nprint(predictions.shape)\n\ncopy_pred = np.copy(predictions)\nidx = np.argmax(copy_pred, axis=1)\ncopy_pred[:,idx] = 0\nidx2 = np.argmax(copy_pred, axis=1)\ncopy_pred[:, idx2] = 0\nidx3 = np.argmax(copy_pred, axis=1)\ncopy_pred[:, idx3] = 0\nidx4 = np.argmax(copy_pred, axis=1)\ncopy_pred[:, idx4] = 0\nidx5 = np.argmax(copy_pred, axis=1)","683e3a2b":"#convert the one-hot vectors to their names\nresults = []\n\nprint(idx[0:10])\nprint(idx2[0:10])\nprint(idx3[0:10])\nprint(idx4[0:10])\nprint(idx5[0:10])\nthreshold = 0.05 #threshold - only consider answers with a probability higher than it\nfor i in range(0, predictions.shape[0]):\n#for i in range(0, 10):\n    each = np.zeros((4251, 1))\n    each2 = np.zeros((4251, 1))\n    each3 = np.zeros((4251, 1))\n    each4 = np.zeros((4251, 1))\n    each5 = np.zeros((4251, 1))\n    if((predictions[i, idx5[i]] > threshold)):\n        each5[idx5[i]] = 1\n        each4[idx4[i]] = 1\n        each3[idx3[i]] = 1\n        each2[idx2[i]] = 1\n        each[idx[i]] = 1\n        tags = [label_encoder.inverse_transform([argmax(each)])[0], label_encoder.inverse_transform([argmax(each2)])[0], label_encoder.inverse_transform([argmax(each3)])[0], label_encoder.inverse_transform([argmax(each4)])[0], label_encoder.inverse_transform([argmax(each5)])[0]]\n    else:\n        if((predictions[i, idx4[i]] > threshold)):\n            print(predictions[i, idx4[i]])\n            each4[idx4[i]] = 1\n            each3[idx3[i]] = 1\n            each2[idx2[i]] = 1\n            each[idx[i]] = 1\n            tags = [label_encoder.inverse_transform([argmax(each)])[0], label_encoder.inverse_transform([argmax(each2)])[0], label_encoder.inverse_transform([argmax(each3)])[0], label_encoder.inverse_transform([argmax(each4)])[0]]\n        else:\n            if((predictions[i, idx3[i]] > threshold)):\n                each3[idx3[i]] = 1\n                each2[idx2[i]] = 1\n                each[idx[i]] = 1\n                tags = [label_encoder.inverse_transform([argmax(each)])[0], label_encoder.inverse_transform([argmax(each2)])[0], label_encoder.inverse_transform([argmax(each3)])[0]]\n            else:\n                if((predictions[i, idx2[i]] > threshold)):\n                    each2[idx2[i]] = 1\n                    each[idx[i]] = 1\n                    tags = [label_encoder.inverse_transform([argmax(each)])[0], label_encoder.inverse_transform([argmax(each2)])[0]]\n                else:\n                    each[idx[i]] = 1\n                    tags = label_encoder.inverse_transform([argmax(each)])[0]\n    results.append(tags)","8d577d1e":"#write the predictions in a file to be submitted in the competition.\nmyfile = open('output.csv','w')\n\ncolumn= ['Image', 'Id']\n\nwrtr = csv.writer(myfile, delimiter=',')\nwrtr.writerow(column)\n\nfor i in range(0, testData.shape[0]):\n    pred = \"\"\n    if(len(results[i])==5):\n        if (results[i][4]!=results[i][0]):\n            pred = results[i][0] + \" \" + results[i][1] + \" \" + results[i][2] + \" \" + results[i][3] + \" \" + results[i][4]\n        else:\n            pred = results[i][0] + \" \" + results[i][1] + \" \" + results[i][2] + \" \" + results[i][3]\n    else:\n        if(len(results[i])==4):\n            pred = results[i][0] + \" \" + results[i][1] + \" \" + results[i][2] + \" \" + results[i][3]\n        else:\n            if(len(results[i])==3):\n                pred = results[i][0] + \" \" + results[i][1] + \" \" + results[i][2]\n            else:\n                if(len(results[i])==2):\n                    pred = results[i][0] + \" \" + results[i][1]\n                else:\n                    pred = results[i]\n            \n    result = [testData['Image'][i], pred]\n    #print(result)\n    wrtr.writerow(result)\n    \nmyfile.close()","8d17a5cf":"Now I have to train the CNN. This section contains many versions of the code, depending on how I attempted data augmentation. The version I actually used is the one not commented.","0de12508":"Here I am preparing the labels, by converting them into one-hot vectors.","62cfe53b":"**Humpback Whale Identification - CNN with Keras**\n\nThis is a solution for the challenge \"Humpback Whale Identification\". It is based on a Convolutional Neural Network using Keras.\nDuring the challenge I have created multiple versions of this CNN to try and improve my results. Most of the code for those versions are still in this notebook in hidden sections of the code and commented. The version which is not commented or hidden is the one that produced my best results in the challenge.\n\nIn this code I have chosen not to split the training set to create a test set, because of the characteristics of the data. Many of the classes have only one picture that corresponds to them, so randomly selecting a test set would be harmful for training. I did try data augmentation, as can be seen in previous versions of this code. However, because I was running in the Kaggle kernel, the increase in the amout of data increased the runtime and, due to kernel limitation, that resulted in a decrease in the number of iterations. The results I obtained using the augmented data were, then, worse than the ones simply using the given data.","788e729d":"Preparing test data to get predictions. I had to split the data into parts, because of memory constraints.","a1d44a79":"Now we prepare the data to be used for training.","910f5f27":"The final part of the code choses the predictions with highest probability (up to five options, according to the challenge's rules). I use a threshold to choose how many predictions to make for each image. The one-hot vectors corresponding to the chosen predictions are transformed back to their corresponding names and those are printed in the submission file.","9b6b0a0d":"In the next sections of code, I prepare the images using the same class applied to the training data, and obtain the predictions based on my model.","241e3182":"Now let's open one of the images in the training set to see how they look like.","d0a771f1":"The next set of code is meant to prepare the images to be used for the training. It changes their shape and converts it into an array.","7fd61bc3":"Next, we create the CNN architecture. I have attempted many different variations of it, but the non-commented version generated the best results."}}