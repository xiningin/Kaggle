{"cell_type":{"9f8dcc40":"code","8aff4d01":"code","4cb4c3a6":"code","cf726030":"code","0b58edbc":"code","d1de74a6":"code","a1ad5a91":"code","3525667f":"code","bbe91719":"code","c4c452a2":"code","66e3dfb1":"code","f9ae5712":"code","f33c6c27":"code","92cdb3d7":"markdown","df719083":"markdown","cd6c3dbc":"markdown","4f8a3c26":"markdown","fc79fe00":"markdown"},"source":{"9f8dcc40":"# specifying basic config\nimport transformers\n\nMAX_LEN = 256\n\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 32\nTEST_BATCH_SIZE = 32\n\nEPOCHS = 5\n\nBERT_PATH = \"..\/input\/roberta-base\"\n\nMODEL_PATH = \"..\/input\/roberta-de-nero\/\"\n\ndevice = 'cuda'\n\nTOKENIZER = transformers.RobertaTokenizer.from_pretrained(BERT_PATH)","8aff4d01":"import torch\nclass BERTDataset:\n    def __init__(self, text):\n        self.text = text\n    \n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, item):\n        encode = self.tokenizer(self.text[item],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        \n        return encode","4cb4c3a6":"# attention head\nfrom torch import nn \n\nclass AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector","cf726030":"import transformers\nfrom torch import nn\n\nclass ROBERTA_(nn.Module):\n    def __init__(self):\n        super(ROBERTA_, self).__init__()\n        self.bert = transformers.AutoModelForSequenceClassification.from_pretrained(\n            BERT_PATH,\n            num_labels=1\n        )\n        self.head = AttentionHead(768, 768, 1)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(768, 1)\n\n    def forward(self, **param_mehta):\n        \n        x = self.bert(**param_mehta)\n        x = x[\"logits\"].squeeze(-1)\n        \n#         x = x[0]\n#         x = self.head(x)\n#         x = self.dropout(x)\n#         x = self.linear(x)\n        \n        return x","0b58edbc":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\n\nnum_bins = int(np.floor(1 + np.log2(len(train))))\ntrain.loc[:,'bins'] = pd.cut(train['target'], bins=num_bins, labels=False)\n\nyyy = train.target.values\nbins = train.bins.values\n\ntrain_dataset = BERTDataset(\n    text = train.excerpt.values\n)\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size = TRAIN_BATCH_SIZE,\n)\n\n\ntest_dataset = BERTDataset(\n    text = test.excerpt.values\n)\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size = TEST_BATCH_SIZE,\n)","d1de74a6":"model = ROBERTA_()","a1ad5a91":"def predict(test_data, model, model_path):\n    preds = []\n    \n    state = torch.load(model_path)\n    model.load_state_dict(state['state_dict'])\n    \n    model.to(device)\n    model.eval()\n    \n    with torch.no_grad():\n        for d in test_data:\n            inputs = {key: val.reshape(val.shape[0], -1).to(device) for key, val in d.items()}\n            output = model(**inputs)\n            \n            preds.extend(output.cpu().numpy())\n    \n    return preds","3525667f":"x_train, x_test = [], []\nprint('Fold:', end=' ')\nfor i in range(5):\n    model_path_ = MODEL_PATH + 'model_' + str(i) + '.pth'\n    x_train.append(predict(train_dataloader, model, model_path_))\n    x_test.append(predict(test_dataloader, model, model_path_))\n    print(f'{i}', end=' ')","bbe91719":"preds_sub = None\nfor i in x_test:\n    try:\n        preds_sub += np.array(i)\n    except:\n        preds_sub = np.array(i)\npreds_sub \/= 5\n\nsub_nostack = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsub_nostack['target'] = preds_sub\nprint(sub_nostack)","c4c452a2":"from sklearn.metrics import mean_squared_error\ndef rmse_score(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","66e3dfb1":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import Ridge\nfrom lightgbm import LGBMRegressor\n\n# add model of choice for stacking\nstacking_models_dispatcher = {\n    'ridge': Ridge(alpha=50),\n    'lgbm': LGBMRegressor()\n}\n\ntarget = None\nfor train_data, test_data in zip(x_train, x_test):\n    \n    kfold = StratifiedKFold(n_splits=5)\n    \n    preds = None\n    sum_scores = 0\n    for k, (train_idx, valid_idx) in enumerate(kfold.split(train_data, bins)):\n        \n        train_data = np.array(train_data).reshape(-1, 1)\n        test_data = np.array(test_data).reshape(-1, 1)\n        \n        stacking_model = stacking_models_dispatcher['ridge'] # specify model of choice for stacking\n        X_train, y_train = train_data[train_idx], yyy[train_idx]\n        X_valid, y_valid = train_data[valid_idx], yyy[valid_idx]\n        \n    \n        stacking_model.fit(X_train, y_train)\n        prediction = stacking_model.predict(X_valid)\n        score = rmse_score(prediction,y_valid)\n        print(f'Fold {k}, rmse score: {score}')\n        \n        sum_scores += score\n        try:\n            preds += stacking_model.predict(test_data)\n        except:\n            preds = stacking_model.predict(test_data)\n \n    print(f'MEAN RMSE: {sum_scores \/ 5}')\n        \n    preds \/= 5\n    try:\n        target += preds\n    except:\n        target = preds \ntarget \/= 5","f9ae5712":"sub_stack = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsub_stack['target'] = target\nprint(sub_stack)","f33c6c27":"sub_nostack.to_csv('submission.csv', index=False)","92cdb3d7":"### Approach used:\n* pretraining roberta \n* Tokenizer: RobertaTokenizer\n* model: roberta for sequence classification\n* 5 folds, 7 epochs\n\n### scores upon submission:\n\n| Model | RMSE on LB |\n| --- | --- |\n| RoBERTa for seq classif | 0.481 |\n| RoBERTa for seq classif stacked with lgbm | 0.482 |\n| RoBERTa for seq classif stacked with ridge | 0.483 |","df719083":"### First, I have created a submission dataframe with targets just from RoBERTa model.","cd6c3dbc":"#### If you would like to use this notebook, make sure you use the correct data frame to save the submission file in the next cell.","4f8a3c26":"### Stacking on RoBERTa\n\nThe following cell is for stacking a model on RoBERTa. I have used 5 folds again.","fc79fe00":"## First NLP Competition \nThis is the first NLP competition for me and I had never explored transformers before this. So, firstly, I'd like to mention some resources that helped me to learn and achieve this result.\n\n* World's first 4xGM [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek)'s book [Approaching (Almost) Any Machine Learning Problem](https:\/\/github.com\/abhishekkrthakur\/approachingalmost) and his [youtube](https:\/\/www.youtube.com\/user\/abhisheksvnit) channel.\n* [Notebooks](https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-inference) by [Maunish dave](https:\/\/www.kaggle.com\/maunish)\n* Some YouTube videos:\n    * [BERT Neural Network - EXPLAINED! by CodeEmporium](https:\/\/youtu.be\/xI0HHN5XKDo)\n    * [Grandmaster Series \u2013 Building World-Class NLP Models with Transformers and Hugging Face by NVIDIA Developer](https:\/\/youtu.be\/PXc_SlnT2g0)"}}