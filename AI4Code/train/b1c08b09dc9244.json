{"cell_type":{"2da938c7":"code","d8a9025a":"code","60d34c96":"code","435e2012":"code","850acb23":"code","c0e612c5":"code","d3d751bb":"code","03bccd99":"code","1323641e":"code","2f410a35":"code","5a10fb53":"code","1a39c2b7":"code","f241d793":"code","9e479a91":"code","cb49ad11":"code","d67097b2":"code","716775fe":"code","8067be59":"code","85b3f5e1":"code","841fed53":"code","76270dfc":"code","4f461838":"code","18453be8":"markdown","e49dea5f":"markdown","9ca0cb79":"markdown","d036cbdc":"markdown","b46ff4d5":"markdown","ce65f0a5":"markdown","08ba8835":"markdown","68cb85e3":"markdown","eb87f568":"markdown"},"source":{"2da938c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d8a9025a":"test = pd.read_csv('\/kaggle\/input\/occupancy-detection-data-set-uci\/datatest.txt', header=0, index_col=1, parse_dates=True, squeeze=True)","60d34c96":"test.drop('date', axis=1, inplace=True)","435e2012":"test.head()","850acb23":"train = pd.read_csv('\/kaggle\/input\/occupancy-detection-data-set-uci\/datatraining.txt', header=0, index_col=1, parse_dates=True, squeeze=True)","c0e612c5":"train.drop('date', axis=1, inplace=True)","d3d751bb":"train.head()","03bccd99":"validate = pd.read_csv('\/kaggle\/input\/occupancy-detection-data-set-uci\/datatest2.txt', header=0, index_col=1, parse_dates=True, squeeze=True)","1323641e":"validate.drop('date', axis=1, inplace=True)","2f410a35":"validate.head()","5a10fb53":"validate[['Temperature', 'Humidity']].plot()","1a39c2b7":"validate[['CO2', 'Light']].plot()","f241d793":"validate[['Occupancy']].plot()","9e479a91":"data = pd.concat([test, train, validate])","cb49ad11":"data[\"CO2\"].plot()","d67097b2":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","716775fe":"values = data.values\n# split data into inputs and outputs\nX, y = values[:, :-1], values[:, -1]\n# split the dataset\ntrainX, testX, trainy, testy = train_test_split(X, y, test_size=0.3, shuffle=False, random_state=1)","8067be59":"# make a naive prediction\ndef naive_prediction(testX, value):\n\treturn [value for x in range(len(testX))]\n \n# evaluate skill of predicting each class value\nfor value in [0, 1]:\n\t# forecast\n\tyhat = naive_prediction(testX, value)\n\t# evaluate\n\tscore = accuracy_score(testy, yhat)\n\t# summarize\n\tprint('Naive=%d score=%.3f' % (value, score))","85b3f5e1":"from sklearn.linear_model import LogisticRegression","841fed53":"# define the model\nmodel = LogisticRegression()\n# fit the model on the training set\nmodel.fit(trainX, trainy)\n# predict the test set\nyhat = model.predict(testX)\n# evaluate model skill\nscore = accuracy_score(testy, yhat)\nprint(score)","76270dfc":"# basic feature selection\nfeatures = [0, 1, 2, 3, 4]\nfor f in features:\n\t# split data into inputs and outputs\n\tX, y = values[:, f].reshape((len(values), 1)), values[:, -1]\n\t# split the dataset\n\ttrainX, testX, trainy, testy = train_test_split(X, y, test_size=0.3, shuffle=False, random_state=1)\n\t# define the model\n\tmodel = LogisticRegression()\n\t# fit the model on the training set\n\tmodel.fit(trainX, trainy)\n\t# predict the test set\n\tyhat = model.predict(testX)\n\t# evaluate model skill\n\tscore = accuracy_score(testy, yhat)\n\tprint('feature=%d, name=%s, score=%.3f' % (f, data.columns[f], score))","4f461838":"## Extensions\nThis data may still be interesting for further investigation.\n\nSome ideas include:\n\nPerhaps the problem would be more challenging if the light column was removed.\nPerhaps the problem can be framed as a true multivariate time series classification where lag observations are used in the model.\nPerhaps the clear peaks in the environmental variables can be exploited in the prediction.\nI tried each of these models briefly without exciting results.","18453be8":"## Naive Model\nA simple model for this formulation of the problem is to predict the most prominent class outcome. This is called the Zero Rule, or the naive prediction algorithm. We will evaluate predicting all 0 (unoccupied) and all 1 (occupied) for each example in the test set and evaluate the approach using the accuracy metric.","e49dea5f":"##\u00a0Simple Predictive Models\nThe simplest formulation of the problem is to predict occupancy based on the environmental conditions at the current time.\n\nI refer to this as a direct model as it does not make use of the observations of the environmental measures at prior time steps. Technically, this is not sequence classification, it is just a straight classification problem where the observations are temporally ordered.\n\nThis seems to be the standard formulation of the problem from my skim of the literature, and disappointingly, the papers seem to use the train\/validation\/test data as labeled on the UCI website.\n\nWe will use the combined dataset described in the previous section and evaluate model skill by holding back the last 30% of the data as a test set. For example:","9ca0cb79":"So always predicting unoccupied is 82% accurate on the test set.. For any model to be considered skilful on the problem, it must achieve a skill of 82% or better.","d036cbdc":"Work through https:\/\/machinelearningmastery.com\/how-to-predict-room-occupancy-based-on-environmental-factors\/ although I deviate in some naming\n\nAfter completing this tutorial, you will know:\n\n* The Occupancy Detection standard time series classification problem in machine learning.\n* How to load and visualize multivariate time series classification data.\n* How to develop simple naive and logistic regression models that achieve nearly perfect skill on the problem.\n\nWe are using the dataset [Occupancy Detection Data Set, UCI Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Occupancy+Detection+). This is a binary classification problem which requires that an observation of environmental factors such as temperature and humidity be used to classify whether a room is occupied or unoccupied. *An office room [\u2026] was monitored for the following variables: temperature, humidity, light and CO2 levels. A microcontroller was employed to acquire the data. A ZigBee radio was connected to it and was used to transmit the information to a recording station. A digital camera was used to determine if the room was occupied or not. The camera time stamped pictures every minute and these were studied manually to label the data.*\n\nData is provided with date-time information and six environmental measures taken each minute over multiple days, specifically:\n\n* Temperature in Celsius.\n* Relative humidity as a percentage.\n* Light measured in lux.\n* Carbon dioxide measured in parts per million.\n* Humidity ratio, derived from temperature and relative humidity measured in kilograms of water vapor per kilogram of air.\n* Occupancy as either 1 for occupied or 0 for not occupied.\n\nThe data is available in CSV format in three files, claimed to be a split of data for training, validation and testing.\n\nThe three files are as follows:\n\n* datatest.txt (test): From 2015-02-02 14:19:00 to 2015-02-04 10:43:00\n* datatraining.txt (train): From 2015-02-04 17:51:00 to 2015-02-10 09:33:00\n* datatest2.txt (val): From 2015-02-11 14:48:00 to 2015-02-18 09:19:00\n\nThe published paper is https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0378778815304357\n\n## Data prep","b46ff4d5":"##\u00a0Concatenated Dataset\nWe can simplify the dataset by preserving the temporal consistency of the data and concatenating all three sets into a single dataset, dropping the \u201cno\u201d column.\n\nThis will allow ad hoc testing of simple direct framings of the problem (in the next section) that can be tested on a temporally consistent way with ad hoc train\/test set sizes.\n\nNote: This simplification does not account for the temporal gaps in the data and algorithms that rely on a sequence of observations at prior time steps may require a different organization of the data.","ce65f0a5":"We can see that only the \u201cLight\u201d variable is required in order to achieve 99% accuracy on this dataset.\n\nIt is very likely that the lab rooms in which the environmental variables were recorded had a light sensor that turned internal lights on when the room was occupied.\n\nAlternately, perhaps the light is recorded during the daylight hours (e.g. sunshine through windows), and the rooms are occupied on each day, or perhaps each week day.\n\nAt the very least, the results of this tutorial ask some hard questions about any research papers that use this dataset, as clearly it is not a challenging prediction problem.","08ba8835":"## Extensions\nThis data may still be interesting for further investigation.\n\nSome ideas include:\n\nPerhaps the problem would be more challenging if the light column was removed.\nPerhaps the problem can be framed as a true multivariate time series classification where lag observations are used in the model.\nPerhaps the clear peaks in the environmental variables can be exploited in the prediction.\nI tried each of these models briefly without exciting results.","68cb85e3":"##\u00a0Logistic Regression\nA skim of the literature shows a range of sophisticated neural network models applied on this problem.\n\nTo start with, let\u2019s try a simple logistic regression classification algorithm.","eb87f568":"The skill of the model is about 99% accurate, showing skill over the naive method.\n\n##\u00a0Feature Selection and Logistic Regression\nA closer look at the time series plot shows a clear relationship between the times when the rooms are occupied and peaks in the environmental measures.\n\nThis makes sense and explains why this problem is in fact so easy to model.\n\nWe can further simplify the model by testing a simple logistic regression model on each environment measure in isolation. The idea is that we don\u2019t need all of the data to predict occupancy; that perhaps just one of the measures is sufficient.\n\nThis is the simplest type of feature selection where a model is created and evaluated with each feature in isolation. More advanced methods may consider each subgroup of features."}}