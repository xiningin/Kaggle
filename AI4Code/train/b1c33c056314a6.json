{"cell_type":{"a3ab49cb":"code","68a4259b":"code","05efa3f3":"code","a080623c":"code","d5c9c0d1":"code","d806186b":"code","98ac40eb":"code","32441390":"code","422895d9":"code","2d7387c6":"code","e8241e23":"code","9e07c714":"code","5118e022":"code","3c7fd7a0":"code","d540be6a":"code","eeeb151f":"code","17118dcd":"code","5fd7594f":"code","4bfab209":"code","fe657eb9":"code","93ef13b8":"code","2319065a":"markdown","d7f57584":"markdown","25b0ba64":"markdown","1a969c0c":"markdown"},"source":{"a3ab49cb":"# uncomment below line of code if you want to calculate features and save dataframe\n# this script prints the path at which dataframe with calculated features is saved.\n# train.py calls the DataGenerator class to \n\n# %run .\/train.py WMT original\n\n# this notebook was trained on cloud compute. So use your own paths","68a4259b":"import pandas as pd\nimport pickle \nimport numpy as np\nfrom tqdm import tqdm_notebook as tqdm\nfrom IPython.core.interactiveshell import InteractiveShell\n\n\nnp.random.seed(2)\ncompany_code = 'WMT'\nstrategy_type = 'original'\n# use the path printed in above output cell after running stock_cnn.py. It's in below format\ndf = pd.read_csv(\"..\/input\/stock-wmt-blog\/df_\"+company_code+\".csv\")\ndf['labels'] = df['labels'].astype(np.int8)\nif 'dividend_amount' in df.columns:\n    df.drop(columns=['dividend_amount', 'split_coefficient'], inplace=True)\ndisplay(df.head())","05efa3f3":"from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n# from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\nfrom collections import Counter\n\nlist_features = list(df.loc[:, 'open':'eom_26'].columns)\nprint('Total number of features', len(list_features))\nx_train, x_test, y_train, y_test = train_test_split(df.loc[:, 'open':'eom_26'].values, df['labels'].values, train_size=0.8, \n                                                    test_size=0.2, random_state=2, shuffle=True, stratify=df['labels'].values)\n\n# smote = RandomOverSampler(random_state=42, sampling_strategy='not majority')\n# x_train, y_train = smote.fit_resample(x_train, y_train)\n# print('Resampled dataset shape %s' % Counter(y_train))\n\nif 0.7*x_train.shape[0] < 2500:\n    train_split = 0.8\nelse:\n    train_split = 0.7\n# train_split = 0.7\nprint('train_split =',train_split)\nx_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, train_size=train_split, test_size=1-train_split, \n                                                random_state=2, shuffle=True, stratify=y_train)\nmm_scaler = MinMaxScaler(feature_range=(0, 1)) # or StandardScaler?\nx_train = mm_scaler.fit_transform(x_train)\nx_cv = mm_scaler.transform(x_cv)\nx_test = mm_scaler.transform(x_test)\n\nx_main = x_train.copy()\nprint(\"Shape of x, y train\/cv\/test {} {} {} {} {} {}\".format(x_train.shape, y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))","a080623c":"num_features = 225  # should be a perfect square\nselection_method = 'all'\ntopk = 320 if selection_method == 'all' else num_features\n# if train_split >= 0.8:\n#     topk = 400\n# else:\n#     topk = 300","d5c9c0d1":"%%time\nfrom operator import itemgetter\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n\nif selection_method == 'anova' or selection_method == 'all':\n    select_k_best = SelectKBest(f_classif, k=topk)\n    if selection_method != 'all':\n        x_train = select_k_best.fit_transform(x_main, y_train)\n        x_cv = select_k_best.transform(x_cv)\n        x_test = select_k_best.transform(x_test)\n    else:\n        select_k_best.fit(x_main, y_train)\n    \n    selected_features_anova = itemgetter(*select_k_best.get_support(indices=True))(list_features)\n    print(selected_features_anova)\n    print(select_k_best.get_support(indices=True))\n    print(\"****************************************\")\n    \nif selection_method == 'mutual_info' or selection_method == 'all':\n    select_k_best = SelectKBest(mutual_info_classif, k=topk)\n    if selection_method != 'all':\n        x_train = select_k_best.fit_transform(x_main, y_train)\n        x_cv = select_k_best.transform(x_cv)\n        x_test = select_k_best.transform(x_test)\n    else:\n        select_k_best.fit(x_main, y_train)\n\n    selected_features_mic = itemgetter(*select_k_best.get_support(indices=True))(list_features)\n    print(len(selected_features_mic), selected_features_mic)\n    print(select_k_best.get_support(indices=True))","d806186b":"if selection_method == 'all':\n    common = list(set(selected_features_anova).intersection(selected_features_mic))\n    print(\"common selected featues\", len(common), common)\n    if len(common) < num_features:\n        raise Exception('number of common features found {} < {} required features. Increase \"topk variable\"'.format(len(common), num_features))\n    feat_idx = []\n    for c in common:\n        feat_idx.append(list_features.index(c))\n    feat_idx = sorted(feat_idx[0:225])\n    print(feat_idx)","98ac40eb":"if selection_method == 'all':\n    x_train = x_train[:, feat_idx]\n    x_cv = x_cv[:, feat_idx]\n    x_test = x_test[:, feat_idx]\n\nprint(\"Shape of x, y train\/cv\/test {} {} {} {} {} {}\".format(x_train.shape, \n                                                             y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))","32441390":"_labels, _counts = np.unique(y_train, return_counts=True)\nprint(\"percentage of class 0 = {}, class 1 = {}\".format(_counts[0]\/len(y_train) * 100, _counts[1]\/len(y_train) * 100))","422895d9":"from sklearn.utils.class_weight import compute_class_weight\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import get_custom_objects\n\ndef get_sample_weights(y):\n    \"\"\"\n    calculate the sample weights based on class weights. Used for models with\n    imbalanced data and one hot encoding prediction.\n\n    params:\n        y: class labels as integers\n    \"\"\"\n\n    y = y.astype(int)  # compute_class_weight needs int labels\n    class_weights = compute_class_weight('balanced', np.unique(y), y)\n    \n    print(\"real class weights are {}\".format(class_weights), np.unique(y))\n    print(\"value_counts\", np.unique(y, return_counts=True))\n    sample_weights = y.copy().astype(float)\n    for i in np.unique(y):\n        sample_weights[sample_weights == i] = class_weights[i]  # if i == 2 else 0.8 * class_weights[i]\n        # sample_weights = np.where(sample_weights == i, class_weights[int(i)], y_)\n\n    return sample_weights\n\ndef reshape_as_image(x, img_width, img_height):\n    x_temp = np.zeros((len(x), img_height, img_width))\n    for i in range(x.shape[0]):\n        # print(type(x), type(x_temp), x.shape)\n        x_temp[i] = np.reshape(x[i], (img_height, img_width))\n\n    return x_temp\n\ndef f1_weighted(y_true, y_pred):\n    y_true_class = tf.math.argmax(y_true, axis=1, output_type=tf.dtypes.int32)\n    y_pred_class = tf.math.argmax(y_pred, axis=1, output_type=tf.dtypes.int32)\n    conf_mat = tf.math.confusion_matrix(y_true_class, y_pred_class)  # can use conf_mat[0, :], tf.slice()\n    # precision = TP\/TP+FP, recall = TP\/TP+FN\n    rows, cols = conf_mat.get_shape()\n    size = y_true_class.get_shape()[0]\n    precision = tf.constant([0, 0, 0])  # change this to use rows\/cols as size\n    recall = tf.constant([0, 0, 0])\n    class_counts = tf.constant([0, 0, 0])\n\n    def get_precision(i, conf_mat):\n        print(\"prec check\", conf_mat, conf_mat[i, i], tf.reduce_sum(conf_mat[:, i]))\n        precision[i].assign(conf_mat[i, i] \/ tf.reduce_sum(conf_mat[:, i]))\n        recall[i].assign(conf_mat[i, i] \/ tf.reduce_sum(conf_mat[i, :]))\n        tf.add(i, 1)\n        return i, conf_mat, precision, recall\n\n    def tf_count(i):\n        elements_equal_to_value = tf.equal(y_true_class, i)\n        as_ints = tf.cast(elements_equal_to_value, tf.int32)\n        count = tf.reduce_sum(as_ints)\n        class_counts[i].assign(count)\n        tf.add(i, 1)\n        return count\n\n    def condition(i, conf_mat):\n        return tf.less(i, 3)\n\n    i = tf.constant(3)\n    i, conf_mat = tf.while_loop(condition, get_precision, [i, conf_mat])\n\n    i = tf.constant(3)\n    c = lambda i: tf.less(i, 3)\n    b = tf_count(i)\n    tf.while_loop(c, b, [i])\n\n    weights = tf.math.divide(class_counts, size)\n    numerators = tf.math.multiply(tf.math.multiply(precision, recall), tf.constant(2))\n    denominators = tf.math.add(precision, recall)\n    f1s = tf.math.divide(numerators, denominators)\n    weighted_f1 = tf.reduce_sum(f.math.multiply(f1s, weights))\n    return weighted_f1\n\ndef f1_metric(y_true, y_pred):\n    \"\"\"\n    this calculates precision & recall \n    \"\"\"\n\n    def recall(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))  # mistake: y_pred of 0.3 is also considered 1\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    # y_true_class = tf.math.argmax(y_true, axis=1, output_type=tf.dtypes.int32)\n    # y_pred_class = tf.math.argmax(y_pred, axis=1, output_type=tf.dtypes.int32)\n    # conf_mat = tf.math.confusion_matrix(y_true_class, y_pred_class)\n    # tf.Print(conf_mat, [conf_mat], \"confusion_matrix\")\n\n    return 2 * ((precision * recall) \/ (precision + recall + K.epsilon()))\n\nget_custom_objects().update({\"f1_metric\": f1_metric, \"f1_weighted\": f1_weighted})","2d7387c6":"sample_weights = get_sample_weights(y_train)\nprint(\"Test sample_weights\")\nrand_idx = np.random.randint(0, 1000, 30)\nprint(y_train[rand_idx])\nprint(sample_weights[rand_idx])","e8241e23":"one_hot_enc = OneHotEncoder(sparse=False, categories='auto')  # , categories='auto'\ny_train = one_hot_enc.fit_transform(y_train.reshape(-1, 1))\nprint(\"y_train\",y_train.shape)\ny_cv = one_hot_enc.transform(y_cv.reshape(-1, 1))\ny_test = one_hot_enc.transform(y_test.reshape(-1, 1))","9e07c714":"dim = int(np.sqrt(num_features))\nx_train = reshape_as_image(x_train, dim, dim)\nx_cv = reshape_as_image(x_cv, dim, dim)\nx_test = reshape_as_image(x_test, dim, dim)\n# adding a 1-dim for channels (3)\nx_train = np.stack((x_train,) * 3, axis=-1)\nx_test = np.stack((x_test,) * 3, axis=-1)\nx_cv = np.stack((x_cv,) * 3, axis=-1)\nprint(\"final shape of x, y train\/test {} {} {} {}\".format(x_train.shape, y_train.shape, x_test.shape, y_test.shape))","5118e022":"from matplotlib import pyplot as plt\n%matplotlib inline\n\nfig = plt.figure(figsize=(15, 15))\ncolumns = rows = 3\nfor i in range(1, columns*rows +1):\n    index = np.random.randint(len(x_train))\n    img = x_train[index]\n    fig.add_subplot(rows, columns, i)\n    plt.axis(\"off\")\n    plt.title('image_'+str(index)+'_class_'+str(np.argmax(y_train[index])), fontsize=10)\n    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n    plt.imshow(img)\nplt.show()","3c7fd7a0":"from tensorflow.keras.models import Sequential, load_model, Model\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, LeakyReLU\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger, Callback\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.regularizers import l2, l1, l1_l2\nfrom tensorflow.keras.initializers import RandomUniform, RandomNormal\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras import regularizers\n\nparams = {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.22, 'conv2d_filters_1': 20, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, \n                                              'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.05, \n                                              'conv2d_filters_2': 40, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, \n                                              'kernel_regularizer_2': 0.0, 'layers': 'two'}, \n          'dense_layers': {'dense_do_1': 0.22, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.0, 'layers': 'one'},\n          'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}\n\n# Original paper CNN params: input layer (15x15), two convolutional layers (15x15x32, 15x15x64),\n# a max pooling (7x7x64), two dropout (0.25, 0.50), fully connected layers (128), and an out-\n# put layer (3). stride?\n# params = {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.25, 'conv2d_filters_1': 32, 'conv2d_kernel_size_1': 3, 'conv2d_mp_1': 0, \n#                                                'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.5, \n#                                                'conv2d_filters_2': 64, 'conv2d_kernel_size_2': 3, 'conv2d_mp_2': 7, 'conv2d_strides_2': 1, \n#                                                'kernel_regularizer_2': 0.0, 'layers': 'two'}, \n#            'dense_layers': {'dense_do_1': 0.0, 'dense_nodes_1': 128, 'kernel_regularizer_1': 0.0, 'layers': 'one'},\n#            'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}\n","d540be6a":"from functools import *\nfrom sklearn.metrics import f1_score\nfrom tensorflow.keras.metrics import AUC\n\ndef f1_custom(y_true, y_pred):\n    y_t = np.argmax(y_true, axis=1)\n    y_p = np.argmax(y_pred, axis=1)\n    f1_score(y_t, y_p, labels=None, average='weighted', sample_weight=None, zero_division='warn')\n\ndef create_model_cnn(params):\n    model = Sequential()\n\n    print(\"Training with params {}\".format(params))\n    # (batch_size, timesteps, data_dim)\n    # x_train, y_train = get_data_cnn(df, df.head(1).iloc[0][\"timestamp\"])[0:2]\n    conv2d_layer1 = Conv2D(params[\"conv2d_layers\"][\"conv2d_filters_1\"],\n                           params[\"conv2d_layers\"][\"conv2d_kernel_size_1\"],\n                           strides=params[\"conv2d_layers\"][\"conv2d_strides_1\"],\n                           kernel_regularizer=regularizers.l2(params[\"conv2d_layers\"][\"kernel_regularizer_1\"]), \n                           padding='valid',activation=\"relu\", use_bias=True,\n                           kernel_initializer='glorot_uniform',\n                           input_shape=(x_train[0].shape[0],\n                                        x_train[0].shape[1], x_train[0].shape[2]))\n    model.add(conv2d_layer1)\n    if params[\"conv2d_layers\"]['conv2d_mp_1'] == 1:\n        model.add(MaxPool2D(pool_size=2))\n    model.add(Dropout(params['conv2d_layers']['conv2d_do_1']))\n    if params[\"conv2d_layers\"]['layers'] == 'two':\n        conv2d_layer2 = Conv2D(params[\"conv2d_layers\"][\"conv2d_filters_2\"],\n                               params[\"conv2d_layers\"][\"conv2d_kernel_size_2\"],\n                               strides=params[\"conv2d_layers\"][\"conv2d_strides_2\"],\n                               kernel_regularizer=regularizers.l2(params[\"conv2d_layers\"][\"kernel_regularizer_2\"]),\n                               padding='valid',activation=\"relu\", use_bias=True,\n                               kernel_initializer='glorot_uniform')\n        model.add(conv2d_layer2)\n        if params[\"conv2d_layers\"]['conv2d_mp_2'] == 1:\n            model.add(MaxPool2D(pool_size=2))\n        model.add(Dropout(params['conv2d_layers']['conv2d_do_2']))\n\n    model.add(Flatten())\n\n    model.add(Dense(params['dense_layers'][\"dense_nodes_1\"], activation='relu'))\n    model.add(Dropout(params['dense_layers']['dense_do_1']))\n\n    if params['dense_layers'][\"layers\"] == 'two':\n        model.add(Dense(params['dense_layers'][\"dense_nodes_2\"], activation='relu', \n                        kernel_regularizer=params['dense_layers'][\"kernel_regularizer_1\"]))\n        model.add(Dropout(params['dense_layers']['dense_do_2']))\n\n    model.add(Dense(3, activation='softmax'))\n    if params[\"optimizer\"] == 'rmsprop':\n        optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n    elif params[\"optimizer\"] == 'sgd':\n        optimizer = optimizers.SGD(lr=params[\"lr\"], decay=1e-6, momentum=0.9, nesterov=True)\n    elif params[\"optimizer\"] == 'adam':\n        optimizer = optimizers.Adam(learning_rate=params[\"lr\"], beta_1=0.9, beta_2=0.999, amsgrad=False)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', f1_metric])\n    # from keras.utils.vis_utils import plot_model use this too for diagram with plot\n    # model.summary(print_fn=lambda x: print(x + '\\n'))\n    return model\n\ndef check_baseline(pred, y_test):\n    print(\"size of test set\", len(y_test))\n    e = np.equal(pred, y_test)\n    print(\"TP class counts\", np.unique(y_test[e], return_counts=True))\n    print(\"True class counts\", np.unique(y_test, return_counts=True))\n    print(\"Pred class counts\", np.unique(pred, return_counts=True))\n    holds = np.unique(y_test, return_counts=True)[1][2]  # number 'hold' predictions\n    print(\"baseline acc:\", (holds\/len(y_test)*100))","eeeb151f":"from IPython.display import SVG\nfrom tensorflow.keras.utils import model_to_dot, plot_model\n\nmodel = create_model_cnn(params)\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=False)\n\n# SVG(model_to_dot(model).create(prog='dot', format='svg'))","17118dcd":"import os\n\nbest_model_path = os.path.join('.', 'best_model_keras')\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n                   patience=100, min_delta=0.0001)\n# csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'log_training_batch.log'), append=True)\nrlp = ReduceLROnPlateau(monitor='val_loss', factor=0.02, patience=20, verbose=1, mode='min',\n                        min_delta=0.001, cooldown=1, min_lr=0.0001)\nmcp = ModelCheckpoint(best_model_path, monitor='val_f1_metric', verbose=1,\n                      save_best_only=True, save_weights_only=False, mode='max', period=1)  # val_f1_metric","5fd7594f":"%%time\nhistory = model.fit(x_train, y_train, epochs=params['epochs'], verbose=1,\n                            batch_size=64, shuffle=True,\n                            # validation_split=0.3,\n                            validation_data=(x_cv, y_cv),\n                            callbacks=[mcp, rlp, es]\n                            , sample_weight=sample_weights)","4bfab209":"from matplotlib import pyplot as plt\n%matplotlib inline\nInteractiveShell.ast_node_interactivity = \"last\"\n\nplt.figure()\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['f1_metric'])\nplt.plot(history.history['val_f1_metric'])\n\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train_loss', 'val_loss', 'f1', 'val_f1'], loc='upper left')\nplt.show()","fe657eb9":"from sklearn.metrics import confusion_matrix, roc_auc_score, cohen_kappa_score\nimport seaborn as sns\n\nmodel = load_model(best_model_path)\ntest_res = model.evaluate(x_test, y_test, verbose=0)\nprint(\"keras evaluate=\", test_res)\npred = model.predict(x_test)\npred_classes = np.argmax(pred, axis=1)\ny_test_classes = np.argmax(y_test, axis=1)\ncheck_baseline(pred_classes, y_test_classes)\nconf_mat = confusion_matrix(y_test_classes, pred_classes)\nprint(conf_mat)\nlabels = [0,1,2]\n# ax = sns.heatmap(conf_mat, xticklabels=labels, yticklabels=labels, annot=True)\n# ax.xaxis.set_ticks_position('top')\nf1_weighted = f1_score(y_test_classes, pred_classes, labels=None, \n         average='weighted', sample_weight=None)\nprint(\"F1 score (weighted)\", f1_weighted)\nprint(\"F1 score (macro)\", f1_score(y_test_classes, pred_classes, labels=None, \n         average='macro', sample_weight=None))\nprint(\"F1 score (micro)\", f1_score(y_test_classes, pred_classes, labels=None, \n         average='micro', sample_weight=None))  # weighted and micro preferred in case of imbalance\n# https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#cohen-s-kappa --> supports multiclass; ref: https:\/\/stats.stackexchange.com\/questions\/82162\/cohens-kappa-in-plain-english\nprint(\"cohen's Kappa\", cohen_kappa_score(y_test_classes, pred_classes))\n\nprec = []\nfor i, row in enumerate(conf_mat):\n    prec.append(np.round(row[i]\/np.sum(row), 2))\n    print(\"precision of class {} = {}\".format(i, prec[i]))\nprint(\"precision avg\", sum(prec)\/len(prec))\n","93ef13b8":"# suffix = '_'+str(np.round(test_res[1],2))+'_'+str(np.round(f1_weighted,2))\n# model_path = os.path.join('..', 'outputs', 'model_'+company_code+suffix+'.h5')\n# model.save(model_path)\n# print(\"model save path\", os.path.join('..', 'outputs', 'model_'+company_code+suffix+'.h5'))","2319065a":"Split data into Training, Validation and Test","d7f57584":"This Kernel is a part of my blog [Stock Market Action Prediction](https:\/\/towardsdatascience.com\/stock-market-action-prediction-with-convnet-8689238feae3). It is based on a Research paper titled \"Algorithmic Financial Trading with Deep Convolutional Neural Networks: Time Series to Image Conversion Approach\". The blog explains everything, so I am repeating the explanation here.","25b0ba64":"Out of total 441+ features select top 'N' features (let's include base features like close, adjusted_close etc)","1a969c0c":"Record of various configs and their results\n-------------------------------------------------------------------------\nIBM\n----------------------------------------------------------------------------\n* **{'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.2, 'conv2d_filters_1': 30, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.05, 'conv2d_filters_2': 15, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.0, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.2, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.0, 'layers': 'one'}, 'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}**\n\nbaseline acc: 87.44444444444444\n\n[[ 46   0  12]\n [  0  47   8]\n [ 35  53 699]]\n\nF1 score (weighted) 0.8914500898959538\n\nF1 score (macro) 0.7322029896966632\n\nF1 score (micro) 0.88\n\ncohen's Kappa 0.5845248323352525, sometimes 0.61\n\nprecision of class 0 = 0.79\n\nprecision of class 1 = 0.85\n\nprecision of class 2 = 0.89\n\n--------------------------------------------------\n* {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.2, 'conv2d_filters_1': 30, 'conv2d_kernel_size_1': 2,\n                                               'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, \n                                               'conv2d_do_2': 0.07, 'conv2d_filters_2': 20, 'conv2d_kernel_size_2': 2, \n                                               'conv2d_mp_2': 3, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.0, 'layers': 'two'},\n           'dense_layers': {'dense_do_1': 0.2, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.0, 'layers': 'one'}, 'epochs': 3000,\n           'lr': 0.001, 'optimizer': 'adam'}\n\nbaseline acc: 87.44444444444444\n[[ 42   0  16]\n [  0  44  11]\n [ 31  39 717]]\nF1 score (weighted) 0.8993345798690069\nF1 score (macro) 0.7385150835481354\nF1 score (micro) 0.8922222222222224\ncohen's Kappa 0.5952205422097341\nprecision of class 0 = 0.72\nprecision of class 1 = 0.8\nprecision of class 2 = 0.91\nprecision avg 0.81\n\nWMT\n------------------------------------------------------\n{'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.2, 'conv2d_filters_1': 30, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.05, 'conv2d_filters_2': 15, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.0, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.2, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.0, 'layers': 'one'}, 'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}\n\nbaseline acc: 87.33333333333333\n[[ 52   0   6]\n [  0  44  12]\n [ 47  42 697]]\nF1 score (weighted) 0.8923266236465278\nF1 score (macro) 0.7369509608548\nF1 score (micro) 0.8811111111111111\ncohen's Kappa 0.5944443508582787\nprecision of class 0 = 0.9\nprecision of class 1 = 0.79\nprecision of class 2 = 0.89\nprecision avg 0.86\nNOTE: same config with strides_1=2 gave exceptional (90+%) accuracy on classes 0,1 but class2 84%.\n\n* **{'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.22, 'conv2d_filters_1': 35, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.05, 'conv2d_filters_2': 20, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.0, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.22, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.0, 'layers': 'one'}, 'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}**\n    \nbaseline acc: 87.33333333333333\n[[ 53   0   5]\n [  0  45  11]\n [ 39  30 717]]\nF1 score (weighted) 0.9127522951482708\nF1 score (macro) 0.7792439001374168\nF1 score (micro) 0.9055555555555556\ncohen's Kappa 0.6589784510043419\nprecision of class 0 = 0.91\nprecision of class 1 = 0.8\nprecision of class 2 = 0.91\nprecision avg 0.8733333333333334"}}