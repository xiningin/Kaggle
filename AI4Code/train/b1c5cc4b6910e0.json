{"cell_type":{"206a7739":"code","e5a2fa9d":"code","394396cc":"code","7b089266":"code","37713c61":"code","aac486bf":"code","52b138ed":"code","028707f2":"code","de66962c":"code","4e312d41":"code","91506dea":"code","a47ce24a":"code","bddbfb8d":"code","d4d9801c":"code","ed0b85f5":"code","694d449d":"code","c728257b":"code","c28702b7":"code","8d9c50ba":"code","593790fe":"code","812383aa":"code","e7cac56a":"code","8e66fe52":"code","38d8d1f2":"code","788ad6df":"code","89e1b1c5":"code","42951731":"code","bcddfacb":"code","ea1812b0":"code","39c631a4":"code","e4e30937":"code","0166bbfe":"code","1c5fe761":"code","69ef6c00":"code","555d8aef":"code","0fee3c2a":"code","ebb3bc88":"code","c59a0288":"code","9c61b8b1":"markdown","6a2c5649":"markdown","becb9536":"markdown","a73e6add":"markdown","39f16465":"markdown","08e6a6a6":"markdown","bdd92a83":"markdown","2f1321b5":"markdown","a7c920cf":"markdown","f4570aeb":"markdown","3bb8930f":"markdown","78c9de40":"markdown","d5e39ec3":"markdown"},"source":{"206a7739":"import sys\nimport re\nimport string\nimport pickle\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, recall_score, f1_score\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score\nimport joblib\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\n\nimport tweepy\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport wordcloud\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter","e5a2fa9d":"df = pd.read_csv(\"..\/input\/mbti-type\/mbti_1.csv\")\ndf.head()","394396cc":"df.isnull().any()#Checking if there are any missing or null values present in the dataset.\n","7b089266":"df.shape #The shape of the dataset","37713c61":"df.info() #information about dataset","aac486bf":"types = np.unique(np.array(df['type']))\ntypes","52b138ed":"print(df.type.value_counts())\ndf.type.hist(xrot=90)\nplt.show()","028707f2":"graph = df['type'].value_counts()\nplt.figure(figsize=(15,6))\nsb.barplot(graph.index, graph.values, alpha=1)\nplt.xlabel('Personality types', fontsize=15)\nplt.ylabel('No. of posts', fontsize=15)\nplt.show()","de66962c":"# Swarm Plot\ndf1 = df.copy()\n#this function counts the no of words in each post of a user\ndef var_row(row):\n    l = []\n    for i in row.split('|||'):\n        l.append(len(i.split()))\n    return np.var(l)\n\n#this function counts the no of words per post out of the total 50 posts in the whole row\ndf1['words_per_comment'] = df1['posts'].apply(lambda x: len(x.split())\/50)\ndf1['variance_of_word_counts'] = df1['posts'].apply(lambda x: var_row(x))\n\nplt.figure(figsize=(15,10))\nsb.swarmplot(\"type\", \"words_per_comment\", data=df1)","4e312d41":"#Plotting WordCloud.\n\n#Finding the most common words in all posts.\nwords = list(df1[\"posts\"].apply(lambda x: x.split()))\nwords = [x for y in words for x in y]\nCounter(words).most_common(40)\nwc = wordcloud.WordCloud(width=1200, height=500, \n                         collocations=False, background_color=\"white\", \n                         colormap=\"tab20b\").generate(\" \".join(words))\n\n# collocations to False  is set to ensure that the word cloud doesn't appear as if it contains any duplicate words\nplt.figure(figsize=(25,10))\n# generate word cloud, interpolation \nplt.imshow(wc, interpolation='bilinear')\n_ = plt.axis(\"off\")\n\n","91506dea":"fig, ax = plt.subplots(len(df1['type'].unique()), sharex=True, figsize=(15,len(df1['type'].unique())))\nk = 0\nfor i in df1['type'].unique():\n    df_4 = df[df['type'] == i]\n    wordcloud = WordCloud(max_words=1628,relative_scaling=1,normalize_plurals=False).generate(df_4['posts'].to_string())\n    plt.subplot(4,4,k+1)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(i)\n    ax[k].axis(\"off\")\n    k+=1","a47ce24a":"\n\ndf['ie'] = df.type\ndf['ns'] = df.type\ndf['ft'] = df.type\ndf['pj'] = df.type\n\nfor i, t in enumerate(df.type):\n    if 'I' in t:\n        df.ie[i] = 'I'\n    elif 'E' in t:\n        df.ie[i] = 'E'\n        \n    if 'N' in t:\n        df.ns[i] = 'N'\n    elif 'S' in t:\n        df.ns[i] = 'S'\n        \n    if 'F' in t:\n        df.ft[i] = 'F'\n    elif 'T' in t:\n        df.ft[i] = 'T'\n        \n    if 'P' in t:\n        df.pj[i] = 'P'\n    elif 'J' in t:\n        df.pj[i] = 'J'\n\n\nposts = df.posts.values\nyIE = df.ie.values\nyNS = df.ns.values\nyFT = df.ft.values\nyPJ = df.pj.values\ny = df.type","bddbfb8d":"print(posts.shape)","d4d9801c":"print(df.head(5))","ed0b85f5":"print(df.ie.value_counts(), end='\\n\\n')\nprint(df.ns.value_counts(), end='\\n\\n')\nprint(df.ft.value_counts(), end='\\n\\n')\nprint(df.pj.value_counts(), end='\\n\\n')\n\ndf.ie.hist(); plt.show()\ndf.ns.hist(); plt.show()\ndf.ft.hist(); plt.show()\ndf.pj.hist(); plt.show()","694d449d":"print (\"Introversion (I) \/  Extroversion (E):\\t\", df['ie'].value_counts()['I'], \" \/ \", df['ie'].value_counts()['E'])\nprint (\"Intuition (N) \/ Sensing (S):\\t\\t\", df['ns'].value_counts()['N'], \" \/ \", df['ns'].value_counts()['S'])\nprint (\"Thinking (T) \/ Feeling (F):\\t\\t\", df['ft'].value_counts()['F'], \" \/ \", df['ft'].value_counts()['T'])\nprint (\"Judging (J) \/ Perceiving (P):\\t\\t\", df['pj'].value_counts()['P'], \" \/ \", df['pj'].value_counts()['J'])","c728257b":"#regular expressions for tokenization\nregexes = [\n    #urls\n    #r'http[s]?:\/\/(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\n    \n    #html\n    #r'<[^>]+>',\n    \n    #punctuation\n    r'(?:(\\w+)\\'s)',\n    \n    r'(?:\\s(\\w+)\\.+\\s)',\n    r'(?:\\s(\\w+),+\\s)',\n    r'(?:\\s(\\w+)\\?+\\s)',\n    r'(?:\\s(\\w+)!+\\s)',\n    \n    r'(?:\\'+(\\w+)\\'+)',\n    r'(?:\"+(\\w+)\"+)',\n    r'(?:\\[+(\\w+)\\]+)',\n    r'(?:{+(\\w+)}+)',\n    r'(?:\\(+(\\w+))',\n    r'(?:(\\w+)\\)+)',\n\n    #words containing numbers & special characters & punctuation\n    r'(?:(?:(?:[a-zA-Z])*(?:[0-9!\"#$%&\\'()*+,\\-.\/:;<=>?@\\[\\\\\\]^_`{|}~])+(?:[a-zA-Z])*)+)',\n    \n    #pure words\n    r'([a-zA-Z]+)',\n    \n    #numbers\n    #r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)',\n\n    #emoticons\n    #r\"\"\"(?:[:=;][oO\\-]?[D\\)\\]\\(\\]\/\\\\OpP])\"\"\",\n\n    #other words\n    #r'(?:[\\w_]+)',\n\n    #anything else\n    #r'(?:\\S)'\n]\n\n#compiling regular expression\nregex = re.compile(r'(?:'+'|'.join(regexes)+')', re.VERBOSE | re.IGNORECASE)\n","c28702b7":"def preprocess(documents):\n    lemmatizer = WordNetLemmatizer()\n    stemmer = PorterStemmer()\n    \n    #fetching list of stopwords\n    punctuation = list(string.punctuation)\n    swords = stopwords.words('english') + ['amp'] + ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'january', 'feburary', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december',  'mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun',  'jan', 'feb', 'mar', 'apr', 'may', 'jun' 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'tommorow', 'today', 'yesterday'] + ['mr', 'mrs']\n\n\n    processed_documents = []\n    for i,document in enumerate(documents):\n        print('{0}\/{1}'.format(i+1, len(documents)))\n        \n        #tokenization\n        tokens = regex.findall(document)\n\n        #skipping useless tokens\n        t_regex = re.compile(r\"[^a-zA-Z]\")\n        document = []\n        \n        for token in tokens:\n            token = np.array(token)\n            token = np.unique(token[token != ''])\n            \n            if len(token) > 0:\n                token = token[0].lower()\n            else:\n                continue\n                \n            if re.search(t_regex, token) == None and token not in swords:\n                token = lemmatizer.lemmatize(token)\n                document.append(token)\n                \n        document = ' '.join(document)\n\n        #skipping\n        if len(document) >= 0:\n            processed_documents.append(document)\n\n    print()\n    return np.array(processed_documents)","8d9c50ba":"%%time\nposts = preprocess(posts)","593790fe":"print(posts[0])","812383aa":"%%time\n\n#TF-IDF representation\n# creating document frequency matrix\ncv = CountVectorizer().fit(posts)\nX = cv.transform(posts)","e7cac56a":"tf = TfidfTransformer()","8e66fe52":"X_tf=  tf.fit_transform(X).toarray()","38d8d1f2":"print(X)","788ad6df":"posts.shape, X.shape, X_tf.shape, yIE.shape, yNS.shape # verifying that the shapes match","89e1b1c5":"\n\nprint(\"X: 1st posts in tf-idf representation\\n%s\" % X_tf[0])\n","42951731":"# splitting dataset into training and testing dataset\nxIETrain, xIETest, yIETrain, yIETest = train_test_split(X, yIE)\nxNSTrain, xNSTest, yNSTrain, yNSTest = train_test_split(X, yNS)\nxFTTrain, xFTTest, yFTTrain, yFTTest = train_test_split(X, yFT)\nxPJTrain, xPJTest, yPJTrain, yPJTest = train_test_split(X, yPJ)\nxTrain, xTest, yTrain, yTest = train_test_split(X, y)","bcddfacb":"model = MultinomialNB().fit(xTrain, yTrain)\nieModel = MultinomialNB().fit(xIETrain, yIETrain)\nnsModel = MultinomialNB().fit(xNSTrain, yNSTrain)\nftModel = MultinomialNB().fit(xFTTrain, yFTTrain)\npjModel = MultinomialNB().fit(xPJTrain, yPJTrain)","ea1812b0":"print(model.score(xTest, yTest))\nprint(ieModel.score(xIETest, yIETest))\nprint(nsModel.score(xNSTest, yNSTest))\nprint(ftModel.score(xFTTest, yFTTest))\nprint(pjModel.score(xPJTest, yPJTest))","39c631a4":"# Let's look at the recall score\n\n#minority classes\nprint('MINORITY CLASSES:')\nprint(recall_score(yIETest, ieModel.predict(xIETest), pos_label='E'))\nprint(recall_score(yNSTest, nsModel.predict(xNSTest), pos_label='S'))\nprint(recall_score(yFTTest, ftModel.predict(xFTTest), pos_label='T'))\nprint(recall_score(yPJTest, pjModel.predict(xPJTest), pos_label='J'), end='\\n\\n')\n\n#majority classes\nprint('MAJORITY CLASSES:')\nprint(recall_score(yIETest, ieModel.predict(xIETest), pos_label='I'))\nprint(recall_score(yNSTest, nsModel.predict(xNSTest), pos_label='N'))\nprint(recall_score(yFTTest, ftModel.predict(xFTTest), pos_label='F'))\nprint(recall_score(yPJTest, pjModel.predict(xPJTest), pos_label='P'))","e4e30937":"# Let's look at the f1 score\n\n#minority classes\nprint('MINORITY CLASSES:')\nprint(f1_score(yIETest, ieModel.predict(xIETest), pos_label='E'))\nprint(f1_score(yNSTest, nsModel.predict(xNSTest), pos_label='S'))\nprint(f1_score(yFTTest, ftModel.predict(xFTTest), pos_label='T'))\nprint(f1_score(yPJTest, pjModel.predict(xPJTest), pos_label='J'), end='\\n\\n')\n\n#majority classes\nprint('MAJORITY CLASSES:')\nprint(f1_score(yIETest, ieModel.predict(xIETest), pos_label='I'))\nprint(f1_score(yNSTest, nsModel.predict(xNSTest), pos_label='N'))\nprint(f1_score(yFTTest, ftModel.predict(xFTTest), pos_label='F'))\nprint(f1_score(yPJTest, pjModel.predict(xPJTest), pos_label='P'))","0166bbfe":"scores = []\n\nscores.append(cross_val_score(estimator=model, cv=10, X=X, y=y, scoring='accuracy'))\nscores.append(cross_val_score(estimator=ieModel, cv=10, X=X, y=LabelEncoder().fit_transform(yIE), scoring='recall'))\nscores.append(cross_val_score(estimator=nsModel, cv=10, X=X, y=LabelEncoder().fit_transform(yNS), scoring='recall'))\nscores.append(cross_val_score(estimator=ftModel, cv=10, X=X, y=LabelEncoder().fit_transform(yFT), scoring='recall'))\nscores.append(cross_val_score(estimator=pjModel, cv=10, X=X, y=LabelEncoder().fit_transform(yPJ), scoring='recall'))\n\n#prining mean and standard deviations for each model\nfor score in scores:\n    print(score.mean())\n    print(score.std(), end='\\n\\n')","1c5fe761":"my_posts =''' Enter your text here '''\nmy_posts = [my_posts]\nmydata = pd.DataFrame(data={'type': ['xxxx'], 'posts': [my_posts]})\n\ndocument = cv.transform(my_posts)\n\nprint(document)","69ef6c00":"print(ieModel.predict(document))\nprint(nsModel.predict(document))\nprint(ftModel.predict(document))\nprint(pjModel.predict(document))","555d8aef":"CONSUMER_KEY        = 'XXX'\nCONSUMER_SECRET     = 'XXX'\nACCESS_TOKEN        = 'XXX'\nACCESS_TOKEN_SECRET = 'XXX'\n\nAUTH = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\nAUTH.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n\napi = tweepy.API(AUTH)","0fee3c2a":"#getting user tweets\n\ntweets = api.user_timeline('username', count=200)\ntweets = [tweet.text for tweet in tweets]","ebb3bc88":"document = cv.transform([' '.join(tweets)])\n","c59a0288":"print(ieModel.predict(document))\nprint(nsModel.predict(document))\nprint(ftModel.predict(document))\nprint(pjModel.predict(document))","9c61b8b1":"# Processing","6a2c5649":"# Preprocessing","becb9536":"# Prediction using text","a73e6add":"# Training","39f16465":"As we can see above the first model which tries to predict at at once performs poorly. But the separate model which predicts i or e, n or s, f or t, p or j does good","08e6a6a6":"# EDA","bdd92a83":"How many Introvert posts are present v\/s how many Extrovert posts are presnt, out of all the given entries in our labelled Kaggle dataset. This is done in order to extplore the dataset for all the individual Personality Indices of MBTI\n\nCounting No. of posts in one class \/ Total no. of posts in the other class\n","2f1321b5":"We can also predict a person's personality by looking at his twitter tweets","a7c920cf":"# Prediction using tweets","f4570aeb":"# Testing - Accuracy, Recall, f1","3bb8930f":"To get actual performance of our models, I am using KFold cross validation with k=10 to get actual performance. These values will be pickled along with models, so that these can be used in scripts to allow users to see performance and reliability of each model corresponding to their characteristic pair.","78c9de40":"I will add one column for each MBTI characteristic pair, since we will be training independent classifier model for each pair independently. The reason for this is because of imbalance present in our dataset as seen in the EDA section.","d5e39ec3":"Above we can see that there is great unbalance in Introvert\/Extrovert and Intuition\/Sensing pairs. Whereas Feeling\/Thinking and Perception\/Judgment pairs are quite balanced. Although I have created trained models for each pair. Only last 2 pairs are somewhat reliable in predicting MBTI type. So it is not advised to depend on first 2 pairs i.e. IE and NS pairs."}}