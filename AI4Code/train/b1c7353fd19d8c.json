{"cell_type":{"bfc0d320":"code","fdc35670":"code","d4e6f4e4":"code","d222832b":"code","d31139dc":"code","3d8b2ecf":"code","e3f6fd08":"code","f4089783":"code","613497c9":"code","6da1ad69":"code","2d091c86":"code","7b01ea7d":"code","3b0d9572":"code","9134f697":"code","49538c1d":"code","bdadb01a":"code","ed782556":"code","66cc6502":"code","9ba54e67":"code","eafded98":"code","500a3c76":"code","577119fa":"markdown","39d62438":"markdown","670aa406":"markdown","fff9e692":"markdown","a3f4f944":"markdown","ad61c2c9":"markdown","df21f93e":"markdown","1e0c06c9":"markdown","ebff9652":"markdown","a86ade03":"markdown","eaa8ec2a":"markdown","360ef6db":"markdown","2307d133":"markdown","34c9672d":"markdown","21640fca":"markdown","f1575ef1":"markdown","cca83953":"markdown","240e752e":"markdown","5b65570a":"markdown","5d61a085":"markdown","28b3c7f3":"markdown","497c6268":"markdown","58199132":"markdown","8a5a7f48":"markdown","092f889b":"markdown","615ef92c":"markdown","2be522f9":"markdown"},"source":{"bfc0d320":"import numpy as np\nimport pandas as pd\nimport sklearn \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping\n\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom sklearn.preprocessing import StandardScaler","fdc35670":"train_data_pd = pd.read_csv(\"train_set.csv\", header=0)\ntest_data_pd = pd.read_csv(\"test_set.csv\", header=0)","d4e6f4e4":"train_data_np = train_data_pd.to_numpy()\ntest_data_np = test_data_pd.to_numpy()","d222832b":"train_data_x = train_data_np[:,2:]\ntrain_data_y = train_data_np[:,1]\ntest_data_x = test_data_np[:,1:]","d31139dc":"#returns the data where the categorical features are removed and the n best hot_encoded_cat_features are appended\ndef n_hottest_hot(X, y ,n, sel_k_best = None):\n    cat_indices = [(X.columns[i].endswith(\"cat\") or X.columns[i].endswith(\"cat\")) for i in np.arange(X.shape[1])]\n    cat_data = X.loc[:,cat_indices]\n    #take only the one with less than 10 categories - otherwise it takes to much space...\n    onehot = sklearn.preprocessing.OneHotEncoder(sparse = False)\n    onehot.fit(cat_data)\n    hot_cats = onehot.transform(cat_data)\n    if sel_k_best is not None:\n        k_best = hot_cats[:,sel_k_best.get_support()]\n    else:\n        k_Best = SelectKBest()\n        sel_k_best = SelectKBest(k=n).fit(hot_cats , y)\n        k_best = hot_cats[:,sel_k_best.get_support()]\n    k_best = pd.DataFrame(k_best)\n    not_cat_indices = [ not(X.columns[i].endswith(\"cat\") or X.columns[i].endswith(\"cat\")) for i in np.arange(X.shape[1])]\n    result = X.loc[:,not_cat_indices]\n    result = pd.concat([result, k_best], axis = 1)\n    return result, sel_k_best","3d8b2ecf":"def n_best(data_x, data_y, n):\n  select_k_best = SelectKBest(f_classif, k=n)\n  select_k_best.fit_transform(data_x, data_y)\n  return select_k_best.get_support()","e3f6fd08":"#Returns n_best_result-features with the categorical features one-hot-encoded\n#data_x is a numpy array of features from data_pandas_x (i.e. train_data_x for train_data_pd)\n#data_pandas_x is the original pandas dataset (i.e. train_data_pd)\n#n_best_result is the result of calling the n_best function on the numpy array and corresponding y\n#offset is the first column of the pandas dataset where features start (2 for training, 1 for testing)\ndef n_best_onehot(data_x, data_pandas_x, n_best_result, offset=2):\n  cols = [i for i,v in enumerate(n_best_result) if v == True]\n\n  manuals = []\n  removefromcols = []\n\n  for i in cols:\n    if(data_pandas_x.columns[i+offset].endswith(\"cat\")):\n      manuals.append(pd.get_dummies(data_pandas_x[data_pandas_x.columns[i+offset]]))\n      removefromcols.append(i)\n\n  for i in removefromcols:\n    cols.remove(i)\n\n  data_x_new = data_x[:,cols]\n\n  for m in manuals:\n    m_np = m.to_numpy()\n    data_x_new = np.append(data_x_new, m_np, axis=1)\n\n  return data_x_new\n","f4089783":"with pd.option_context('display.max_columns', 60):\n    #print(df.describe(include='all'))\n    print(train_data_pd.describe(include='all'))","613497c9":"def violin(X,y):       \n  data = pd.concat([y,X],axis=1)\n  data = pd.melt(data,id_vars=\"target\",\n                      var_name=\"features\",\n                      value_name='value')\n  plt.figure(figsize=(20,10))\n  sns.violinplot(x=\"features\", y=\"value\", hue=\"target\", data=data,split=True, inner=\"quart\")\n  plt.xticks(rotation=90)\n","6da1ad69":"cat_indices = [(train_data_pd.columns[i].endswith(\"cat\") or train_data_pd.columns[i].endswith(\"cat\")) for i in np.arange(train_data_pd.shape[1])]\ncat_data = train_data_pd.loc[:,cat_indices]\n\nviolin(cat_data.loc[:,(cat_data.std() < 1)] ,train_data_pd[\"target\"] )\nviolin(cat_data.loc[:,(cat_data.std() >= 1)] ,train_data_pd[\"target\"] )","2d091c86":"bin_indices = [(train_data_pd.columns[i].endswith(\"bin\") or train_data_pd.columns[i].endswith(\"bin\")) for i in np.arange(train_data_pd.shape[1])]\nbin_data = train_data_pd.loc[:,bin_indices]\n\nviolin(bin_data, train_data_pd[\"target\"])","7b01ea7d":"rest_indices = [not(bin_indices[i] or cat_indices[i]) for i in np.arange(train_data_pd.shape[1])]\nrest_data = train_data_pd.loc[:,rest_indices]\nrest_data = rest_data.drop([\"id\", \"target\"], axis = 1)\n\nviolin( rest_data.loc[:,(rest_data.std() > 1)] ,train_data_pd[\"target\"] )\nviolin( rest_data.loc[:,(rest_data.std() <= 1)] ,train_data_pd[\"target\"] )","3b0d9572":"correlation = train_data_pd.corr()\n\nplt.figure(dpi=1200)\nplt.figure(figsize=(40,20))\nsns.heatmap(correlation, annot=True)\n#plt.savefig(\"correlation.svg\")\nplt.show()","9134f697":"#get x and y of train set in pandas form\npandas_x = train_data_pd.drop([\"target\", \"id\"], axis=1)\npandas_y = train_data_pd[\"target\"]\n\n#get dataset with one-hot-encoding\nonehot_pandas_x, selected_cols = n_hottest_hot(pandas_x, pandas_y, 50)\nprint(onehot_pandas_x.describe)\n\n#convert to numpy array\nonehot_np_x = onehot_pandas_x.to_numpy()\n\nprint(onehot_np_x.shape)","49538c1d":"#standardize data\nscaler = StandardScaler().fit(onehot_np_x)\nonehot_scaled = scaler.transform(onehot_np_x)\nbest_feats = n_best(onehot_scaled, train_data_y, 25)\nonehot_scaled = onehot_scaled[:,best_feats]","bdadb01a":"# Create instance of EarlyStopping to stop when loss is no longer decreased for 5 epochs\nes = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=20)","ed782556":"model = Sequential()\nmodel.add(Dense(10, input_dim=25, activation='relu'))\nmodel.add(Dense(1, input_dim=10, activation='sigmoid'))\n#compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'], weighted_metrics=[\"accuracy\"])","66cc6502":"#fit model on large epoch sizes to quickly converge (500 epochs or until no more loss reduction for 20 epochs)\nmodel.fit(onehot_scaled, train_data_y, epochs=500, batch_size=256, class_weight={0:1,1:14.25}, callbacks=[es])","9ba54e67":"# predict test set\npd_test_x = test_data_pd.drop([\"id\"], axis=1)\n#get dataset with one-hot-encoding\nonehot_test, selected_cols = n_hottest_hot(pd_test_x, pandas_y, 50, sel_k_best=selected_cols)\n\n#transform as with the train set\nonehot_test = scaler.transform(onehot_test)\nonehot_test = onehot_test[:,best_feats]\n\n#predict, round the prediction to 0 or 1\npreds = model.predict(onehot_test)\npreds = np.rint(preds)\npreds = preds.astype(int)\n\n# how many are 0 and 1 class\namount_0 = sum(preds == 0) \/ len(preds)\namount_1 = sum(preds == 1) \/ len(preds)\n\nprint(\"Ratio of 0 classified data:\", amount_0)\nprint(\"Ratio of 1 classified data:\", amount_1)","eafded98":"data_out = pd.DataFrame(test_data_pd['id'].copy())\ndata_out.insert(1, \"target\", preds, True)\ndata_out.to_csv('submission.csv',index=False)","500a3c76":"#get standard scaler\nscaler = StandardScaler()\n\n#get n best columns (here, 14) from training set\nn_best_res = n_best(train_data_x, train_data_y, 14)\n\n#fit the training set to the one hot encoding of these columns\ntrain_x = n_best_onehot(train_data_x, train_data_pd, n_best_res, 2)\n\n#fit scaler and scale\nscaler.fit(train_x, train_data_y)\ntrain_x = scaler.transform(train_x)\n\n#train model\nclf = LogisticRegression(C=1.0, class_weight={0:1, 1:14}, dual=False,\n                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                      max_iter=10000, multi_class='auto', n_jobs=None,\n                      penalty='l1', random_state=None, solver='liblinear',\n                      tol=0.0001, verbose=0, warm_start=False)\nclf.fit(train_x, train_data_y)\n\n#take the same columns and encoding for the test set and scale the same way\nx_test = n_best_onehot(test_data_x, test_data_pd, n_best_res, 1)\nx_test = scaler.transform(x_test)\n\n# predict test set\npreds = clf.predict(x_test)\npreds = np.rint(preds)\npreds = preds.astype(int)\n\n# how many are 0 and 1 class\namount_0 = sum(preds == 0) \/ len(preds)\namount_1 = sum(preds == 1) \/ len(preds)\n\nprint(\"Ratio of 0 classified data:\", amount_0)\nprint(\"Ratio of 1 classified data:\", amount_1)\n\n#create file\ndata_out = pd.DataFrame(test_data_pd['id'].copy())\ndata_out.insert(1, \"target\", preds, True)\ndata_out.to_csv('submission.csv',index=False)","577119fa":"To predict the test set, we select the same columns and scaling as for the test set, and feed it to the neural net. We make sure to use selected_cols to select the same one-hot columns, and the 25 best_feats we calculated earlier.","39d62438":"In a second step, we scale the training data using the StandardScaler, then select the 25 features with overall highest correlation out of a mix of basic features and one-hot encoded ones.","670aa406":"# Best Implementation: Neural Network\nThe neural network implementation managed to score a Macro-F1 score of **0.52942** on the test set, which was our best submission. The neural network is not optimized. It is likely possible to score over 0.53 with a better implementation.\n\n**Note that the Neural Network may produce better or worse results than recorded, since the process is not really deterministic and EarlyStopping might stop the learning process at bad epochs.**","fff9e692":"The function n_hottest_hot(X,y,n,sel_k_best = None) does the following:\n*   Finds all categorical columns in **X**(name ending in cat)\n*   Converts them into one-hot encoded version and removes them from dataset **X**\n*   Takes the **n** columns from the one-hot encoded with the highest correlation to the target (**y**), and appends them to the dataset **X**\n  *   if sel_k_best is provided (list of columns), these columns are chosen instead of running correlation (so that test set will select the same as train set)\n*   Returns the resulting dataset and selected one-hot columns.\n\nThis function does not reduce the dataset, but converts categorical data into a more useable format, while discarding categories that do not have much impact on the target. **The function operates on pandas dataframes.**\n\n\n","a3f4f944":"The final submitted implementation first one-hot encodes the categorical data and takes the 50 most impactful categories into the dataset. This data is then scaled using a standard scaler. From the resulting 93 columns total, the 25 best correlating features with the training set are selected. A neural network with one dense hidden layer of 10 neurons is then trained using a batch size of 256 and class weight 1:14.25, for 500 epochs or until loss stops decreasing for 20 epochs in a row. This neural network is then used to classify test data.","ad61c2c9":"Highest correlation is 0.89 where we could still lose some information when deleting one of the two features so we keep all.","df21f93e":"Another alternative is the function n_best_onehot. This function performs feature selection first, and then one-hot encodes any selected categorical features after. This process produces different results, but in theory they are worse, because feature selection is done before one-hot encoding, meaning categories are falsely assumed to be closer or further from each other based on their numerical standing before selection.","1e0c06c9":"Now lets have a look at the correlation and whether we can drop some features due to high correlation","ebff9652":"Then we check out the binary data:","a86ade03":"# Other Implementation: Logistic Regression\nLogistic Regression on a somewhat optimized dataset managed to achieve a Macro-F1 Score of **0.52771**.\n\n\n*   The 14 best features are extracted (This value was determined by testing different feature amounts and comparing macro score in a test subset of the training data)\n*   The columns are selected from the x data, and categorial columns are encoded by a one-hot encoding\n*   A standard scaler is used to standardize features\n*   The model is trained with following non-default values, which were determined by a grid search over possible hyperparameters:\n    * C = 1.0\n    * class_weight:{0:1, 1:14}\n    * solver: 'liblinear'\n    * penalty: 'l1'\n    * max_iter: 10000\n*   The same scaling and feature selection is applied to the test set\n*   The test set is predicted\n*   The test set is output to a file.\n\n","eaa8ec2a":"We fit the model on the prepared training data","360ef6db":"As numpy arrays","2307d133":"In this first step, we separate X and Y from the pandas dataframe, and replace the categorical columns with the 50 one-hot encoded columns of highest correlation in the training data. We store the selected one-hot columns in selected_cols.","34c9672d":"We can clearly see that ps_car_11_cat has over 100 categories - thats a lot and probably would take too much space to onehot-encode this one...","21640fca":"# Necessary Imports (Must Run)","f1575ef1":"Basic pandas dataset of train and test set respectively.","cca83953":"Next we want to generate some violin plots to get a feeling of which categories might be of high value... We start with the categorical features:","240e752e":"Numpy arrays in x and y form","5b65570a":"n_best returns a selector array which selects the best **n** columns from a given dataset **data_x** in correlation to the target **data_y**. This function reduces the dataset through feature selection and **operates on numpy arrays.**","5d61a085":"## Additional Functions (Must Run)","28b3c7f3":"Our Neural Network has 1 hidden layer with 10 neurons, using relu as an activation function. The final activation function for classification is sigmoid, to act as a \"chance\" of belonging to either class.","497c6268":"Output Data to File","58199132":"First we want to have an overview of all columns of the data.","8a5a7f48":"And finally the rest of the data","092f889b":"# Loading Data and Data Manipulation (Must Run)","615ef92c":"The EarlyStopping class from Keras will monitor the loss for us. It gets called as a callback after each iteration, and will stop the learning early if loss has not decreased for a set number of episodes (here: 20)","2be522f9":"# Data Visualization (Optional)"}}