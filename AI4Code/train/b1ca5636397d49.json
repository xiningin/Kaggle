{"cell_type":{"86d5f293":"code","f15e54f7":"code","50fdef4f":"code","93170e48":"code","242e51a5":"code","70b0995a":"code","0eb96ff7":"code","461c0f69":"code","65a2e691":"code","4be6c9eb":"code","52feb6f5":"code","c34a6772":"code","0aa9b6c5":"code","4cd6bc2d":"code","9eb82ef8":"code","5ef7e646":"code","eb331b71":"code","d436f8ab":"code","c931086d":"code","08d31ef1":"code","420299be":"code","a121eb65":"code","6a18c225":"code","22967747":"code","7dbf3542":"code","01d8f781":"code","ecdc6d53":"code","49bcb4ae":"code","329d30c8":"code","6b8d0348":"code","eaaec0c8":"code","b022580b":"code","1d5b1932":"code","b4577a81":"code","919a1eb6":"code","4b530eb7":"code","f7ba214a":"code","425e6d44":"code","f4a972a5":"code","f01e5897":"code","fe052f06":"code","08e235c4":"code","f64bfa49":"code","bb29b330":"code","9a1e6328":"code","efbdff53":"code","bfb3653c":"code","da29ffc6":"code","f3d7467f":"code","da44ed9e":"code","b687a2cc":"code","204524e5":"code","3265a7d0":"code","dbaaffb3":"code","70ff7f78":"markdown","46cfcf6b":"markdown","8c3db998":"markdown","4697bca4":"markdown","014c9de5":"markdown","45adcfac":"markdown","6866436f":"markdown","90e942d3":"markdown","525a58c8":"markdown","7bd0bba9":"markdown","46dd2a1f":"markdown","555c3b6a":"markdown","843c789d":"markdown","d79a389d":"markdown","3987d05c":"markdown","c42c189a":"markdown","05348b65":"markdown","699c7b00":"markdown","7efe1458":"markdown","00c54b9e":"markdown","344a0c32":"markdown","4585f2db":"markdown","af39ab82":"markdown","524b9870":"markdown","75bb2f86":"markdown","7afe6f55":"markdown","31a24e06":"markdown"},"source":{"86d5f293":"# standard data science libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f15e54f7":"pip install openpyxl    # to read excel files on Kaggle. NOT needed if you install anaconda on local machine","50fdef4f":"df = pd.read_excel('..\/input\/crci-ml-tutorial-datasets\/Reynolds_dual_flow_regime.xlsx')","93170e48":"df.head()   # head displays top 5 rows of the dataframe. An interger argument (n) can be given in the () to show the n top rows","242e51a5":"df.tail ()   # tail displays bottom 5 rows of the dataframe. An interger argument (n) can be given in the () to show the n bottom rows","70b0995a":"df.info()","0eb96ff7":"df.describe()","461c0f69":"# we had imported seaborn as sns at the start\nsns.heatmap(df.corr(), annot = True)","65a2e691":"df.head()","4be6c9eb":"from sklearn.preprocessing import LabelEncoder\ndf['Flow Regime'] = LabelEncoder().fit(df['Flow Regime']).transform(df['Flow Regime'])\ndf.head()","52feb6f5":"df['Flow Regime'].value_counts()  # shows count distribution of the unique values in the selected column","c34a6772":"predictions_classification = []   # variable to store the predictions generated by each model ","0aa9b6c5":"df.head()","4cd6bc2d":"X = df[['Density [kg\/m3]', 'Velocity [m\/s]', 'Diameter [m]', 'Mu [Pa s]']] \ny = df['Flow Regime']","9eb82ef8":"# each feature (column) is scaled in a range of 0-1 for realistic physical representation and faster convergence. Such scaling is only required to be done in \n# algorithms that are based on gradient descent. \nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX = scaler.fit(X).transform(X)","5ef7e646":"# Splitting the total data into train and test sets, with test data being 10% of the total data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 20)","eb331b71":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ny_pred_LR = LogisticRegression(random_state = 20, max_iter = 500).fit(X_train, y_train).predict(X_test)\nprint(confusion_matrix(y_test, y_pred_LR))","d436f8ab":"print(classification_report(y_test, y_pred_LR))","c931086d":"predictions_classification.append(y_pred_LR)","08d31ef1":"# reading data again as we do not need to normalise it for tree based algorithms\nX = df[['Density [kg\/m3]', 'Velocity [m\/s]', 'Diameter [m]', 'Mu [Pa s]']] \ny = df['Flow Regime']","420299be":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 20)","a121eb65":"from sklearn.tree import DecisionTreeClassifier\n\ny_pred_DT = DecisionTreeClassifier(random_state = 20).fit(X_train, y_train).predict(X_test)\nprint(confusion_matrix(y_test, y_pred_DT))","6a18c225":"print(classification_report(y_test, y_pred_DT))","22967747":"predictions_classification.append(y_pred_DT)","7dbf3542":"from sklearn.ensemble import RandomForestClassifier\n\ny_pred_RF = RandomForestClassifier(random_state = 20).fit(X_train, y_train).predict(X_test)\nprint(confusion_matrix(y_test, y_pred_RF))","01d8f781":"print(classification_report(y_test, y_pred_RF))","ecdc6d53":"predictions_classification.append(y_pred_RF)","49bcb4ae":"from catboost import CatBoostClassifier\n\ny_pred_CB = CatBoostClassifier(verbose = 0, random_state = 20).fit(X_train, y_train).predict(X_test)\nprint(confusion_matrix(y_test, y_pred_CB))","329d30c8":"print(classification_report(y_test, y_pred_CB))","6b8d0348":"predictions_classification.append(y_pred_CB)","eaaec0c8":"# to see which test cases did our model get wrong for any algorithm\nX_test['actual'] = y_test\nalgorithms = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'CatBoost']\nfor prediction, algorithm in zip(predictions_classification, algorithms):\n    X_test[algorithm] = prediction\nX_test['Re'] = (X_test['Density [kg\/m3]']*X_test['Velocity [m\/s]']*X_test['Diameter [m]']\/X_test['Mu [Pa s]']).astype(int)","b022580b":"X_test[X_test['CatBoost'] != X_test['actual']]","1d5b1932":"df = pd.read_excel('..\/input\/crci-ml-tutorial-datasets\/Reynolds_triple_flow_regime.xlsx')\ndf.head()","b4577a81":"df['Flow Regime'].value_counts()","919a1eb6":"X = df[['Density [kg\/m3]', 'Velocity [m\/s]', 'Diameter [m]', 'Mu [Pa s]']] \ny = df['Flow Regime']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 20)","4b530eb7":"y_pred = CatBoostClassifier(verbose = 0, random_state = 20).fit(X_train, y_train).predict(X_test)\nprint(confusion_matrix(y_test, y_pred))","f7ba214a":"print(classification_report(y_test, y_pred))","425e6d44":"from sklearn.linear_model import LinearRegression   # classifier was LogisticClassifier\nfrom sklearn.tree import DecisionTreeRegressor      # classifier was DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor  # classifier was RandomForestClassifier\nfrom catboost import CatBoostRegressor              # classifier was CatBoostClassifier\nfrom sklearn.metrics import r2_score                # metric to judge the accuracy of our model","f4a972a5":"df.head()","f01e5897":"predictions_regression = []","fe052f06":"X = df[['Density [kg\/m3]', 'Velocity [m\/s]', 'Diameter [m]', 'Mu [Pa s]']] \ny = df['Reynolds Number']","08e235c4":"# each feature (column) is scaled in a range of 0-1 for realistic physical representation and faster convergence. Such scaling is only required to be done in \n# algorithms that are based on gradient descent. \nscaler = MinMaxScaler()\nX = scaler.fit(X).transform(X)","f64bfa49":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 20)","bb29b330":"# Linear regression\ny_pred = LinearRegression().fit(X_train, y_train).predict(X_test)\npredictions_regression.append(y_pred)\nprint(r2_score(y_test, y_pred))","9a1e6328":"len(y_pred[y_pred<0])   # shows number of predictions with negative Re","efbdff53":"X = df[['Density [kg\/m3]', 'Velocity [m\/s]', 'Diameter [m]', 'Mu [Pa s]']] \ny = df['Reynolds Number']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 20)","bfb3653c":"# Decision tree regressor\ny_pred = DecisionTreeRegressor(random_state = 20).fit(X_train, y_train).predict(X_test)\npredictions_regression.append(y_pred)\nprint(r2_score(y_test, y_pred))","da29ffc6":"len(y_pred[y_pred<0])   # shows number of predictions with negative Re","f3d7467f":"# Random forest regressor\ny_pred = RandomForestRegressor(random_state = 20).fit(X_train, y_train).predict(X_test)\npredictions_regression.append(y_pred)\nprint(r2_score(y_test, y_pred))","da44ed9e":"#Catboost regressor\ny_pred = CatBoostRegressor(verbose = 0, random_state = 20).fit(X_train, y_train).predict(X_test)\npredictions_regression.append(y_pred)\nprint(r2_score(y_test, y_pred))","b687a2cc":"# to see Re value prediction for all the models\nX_test['Re'] = y_test.astype(int)\nalgorithms = ['Linear Regression', 'Decision Tree', 'Random Forest', 'CatBoost']\nfor prediction, algorithm in zip(predictions_regression, algorithms):\n    X_test[algorithm] = prediction.astype(int)\nX_test","204524e5":"plt.rc('figure', figsize = (20,8))   # sets figure size\nplt.rc('font', size = 16)            # sets font size of the figure\n\ncols_to_plot = ['Re', 'Linear Regression', 'Decision Tree', 'Random Forest', 'CatBoost']\ncolors = ['black', 'red', 'green', 'orange', 'blue']\n\nax1 = plt.subplot(1,2,1)\nax1.set_xlim(0, 3e6)\nax1.set_ylim(0, 3e6)\nax1.set_xlabel('Actual Re')\nax1.set_ylabel('Predicted Re')\nfor col,color in zip(cols_to_plot, colors):\n    sns.lineplot(X_test['Re'], X_test[col], color = color, legend = 'full', label = col, linewidth = 2)\n    \nax2 = plt.subplot(1,2,2)\nax2.set_xlim(0, 1e6)\nax2.set_ylim(0, 1e6)\nax2.set_xlabel('Actual Re')\nax2.set_ylabel('Predicted Re')\nfor col,color in zip(cols_to_plot, colors):\n    sns.lineplot(X_test['Re'], X_test[col], color = color, legend = 'full', label = col, linewidth = 2)","3265a7d0":"# rf = RandomForestClassifier()\n# y_pred = rf.fit(X_train, y_train).predict(X_test)","dbaaffb3":"# plotting the decision trees of a random forest model\n# from sklearn import tree\n# fn=X.columns\n# cn=y.name\n# fig, axes = plt.subplots(nrows = 1,ncols = 3,figsize = (10,4), dpi=900)\n# for index in range(0, 3):\n#     tree.plot_tree(rf.estimators_[index],\n#                    feature_names = fn, \n#                    class_names=cn,\n#                    filled = True,\n#                    ax = axes[index]);\n\n#     axes[index].set_title('Estimator: ' + str(index), fontsize = 11)\n# fig.savefig('rf_3trees.png')","70ff7f78":"# Binary Classification","46cfcf6b":"**For multiclass classification (like laminar\/transitional\/turbulent), do NOT label encode the flow regimes as the model thinks that 2>1>0.**\n\n**Simply pass the original categorical data to the classifiers, they will handle it internally.**","8c3db998":"# 3. Random Forest Classifier","4697bca4":"# Accuracy, Precision, Recall, F1 Score","014c9de5":"# 4. Boosting Algorithm - CatBoost","45adcfac":"**Regression modeling means predicting the value itself instead of classifying it into a category.**\n\n**For this, we will reuse the above tri flow regime dataframe and try to predict the Reynolds Number of the flow using the regressor modules of the same algorithms that were used for classification.**","6866436f":"# **Data preparation for ML**","90e942d3":"![image.png](attachment:b9e34bbb-2c49-4a4d-a619-893f83744540.png)","525a58c8":"**Gradient Descent is just a fancy name for loss optimisation wherein the loss is a function of the feature weights\/ coefficients**","7bd0bba9":"**We will first do binary classification - predicting if a data point belongs to class A or class B.**\n\n**For this, we will look at pipe flow data and try to predict if the flow is laminar or turbulent based on the values of velocity, density, diameter, and viscosity.**","46dd2a1f":"![dfdg](https:\/\/www.pcigeomatics.com\/geomatica-help\/COMMON\/references\/images\/RT_schematic.png)","555c3b6a":"# Extending binary classification to multiclass classification","843c789d":"# Confusion Matrix","d79a389d":"![image.png](attachment:e6063eb5-5213-45be-a1be-6afd606bb0fc.png)","3987d05c":"# Regression Models","c42c189a":"![df](https:\/\/htechtrends.com\/wp-content\/uploads\/2021\/03\/Machine-Learning-Algorithms.jpg)","05348b65":"![image.png](https:\/\/i1.wp.com\/dataaspirant.com\/wp-content\/uploads\/2017\/01\/B03905_05_01-compressor.png?resize=768%2C424&ssl=1)","699c7b00":"# Extra","7efe1458":"**Non numeric data cannot be given as input to any ML model. Thus, non numeric data has to either converted to numeric form or dropped from the data frame**\n\n**We have one non numeric data column - Flow Regime.**\n\n**We will convert this to a numerical type - 0 for laminar and 1 for turbulent**\n\n**This conversion will be done through sklearn's LabelEncoder module**","00c54b9e":"# 2. Decision Tree Classifier","344a0c32":"![image.png](attachment:873ac56a-81db-479a-8903-007c598b8ae0.png)","4585f2db":"# 1. Logistic Regression","af39ab82":"![image.png](attachment:b64a08c8-3d77-4b84-8c0e-a2d7af97fddc.png)","524b9870":"# Traditional ML problems - Classification and Regression","75bb2f86":"**Laminar flow instances  << turbulent flow instances**\n\n**Simple ML algorithms, such as Logistic regression, struggle with such imbalanced\/ skewed data**","7afe6f55":"**We will try 4 algorithms** \n\n*1. LogisticRegression*\n\n*2. DecisionTreeClassifier*\n\n*3. RandomForestClassifier*\n\n*4. CatBoostClassifier*","31a24e06":"**For a better overall performance, always try to maximize F1 score**"}}