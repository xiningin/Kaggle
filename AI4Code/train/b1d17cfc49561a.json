{"cell_type":{"6950c201":"code","f8483be8":"code","530aeacc":"code","458974ac":"code","91b9f7ce":"code","784ef55e":"code","f73cb2d5":"code","47c0fde3":"code","b1ee108a":"code","be7007ba":"code","46bac664":"code","2b75edc8":"code","4aa3a41b":"code","02bc8428":"code","9de43227":"code","cea5ddf1":"markdown","72b30cc4":"markdown","532e0db3":"markdown","243b4d76":"markdown","f60e8185":"markdown","9b58dbe5":"markdown","b37abd13":"markdown","27dc1191":"markdown","3d434c1e":"markdown","424c70ff":"markdown","fceebf8c":"markdown","59edfb80":"markdown","5f61b212":"markdown","8ac947a1":"markdown","1cf34943":"markdown","cc68ae8d":"markdown","df45b25a":"markdown","5bd77792":"markdown","5a073462":"markdown"},"source":{"6950c201":"\nfrom tensorflow.keras import utils\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n\nimport tensorflow as tf\n\nimport numpy as np\nimport pandas as pd\n\nimport nltk\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nnltk.download('punkt')","f8483be8":"def process(text):\n    with open (text, \"r\", encoding = \"ISO-8859-1\") as file:\n        data=file.readlines()\n        script = \"\"\n        for i in data:\n            i = i.lower().replace('\"', '').replace(\"\\n\", \" \\n \")\n            if i.strip() != \"\":\n                script += \"\".join(i).replace(\"\\n\",\" \\n \")\n        return script","530aeacc":"derrygirls = process(\"..\/input\/full-derry-girls-script-s12\/DERRY-GIRLS-SCRIPT.txt\") \nprint(derrygirls[0:2000])","458974ac":"Text_Data = derrygirls\n\ncharindex = list(set(Text_Data))\ncharindex.sort() \nprint(charindex)\n\nnp.save(\"charindex.npy\", charindex)\n\nprint(len(Text_Data))","91b9f7ce":"chars_size = len(charindex)\nseq_len = 80\n\nx_train = []\ny_train = []\n\nfor i in range(0, len(Text_Data)-seq_len, 1 ): \n    X = Text_Data[i:i + seq_len]\n    Y = Text_Data[i + seq_len]\n    x_train.append([charindex.index(x) for x in X])\n    y_train.append(charindex.index(Y))\n\nx_train = np.reshape(x_train, (len(x_train), seq_len))\n\ny_train = utils.to_categorical(y_train)","784ef55e":"def LSTM_model():\n    model = models.Sequential()\n    inp = layers.Input(shape=(seq_len, ))\n    x = layers.Embedding(chars_size, 80, trainable=False)(inp)\n    x = tf.compat.v1.keras.layers.CuDNNLSTM(1024, return_sequences=True,)(x)\n    x = tf.compat.v1.keras.layers.CuDNNLSTM(512, return_sequences=True,)(x)\n    x = tf.compat.v1.keras.layers.CuDNNLSTM(256,)(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Dense(256, activation=\"elu\")(x)\n    x = layers.Dense(128, activation=\"elu\")(x)\n    x = layers.Dropout(0.2)(x)\n    outp = layers.Dense(chars_size, activation='softmax')(x)\n    \n    model = models.Model(inputs=inp, outputs=outp)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=optimizers.Adam(lr=0.0008),\n                  metrics=['accuracy']\n                 )\n\n    return model\n\nmodel = LSTM_model()","f73cb2d5":"model.fit(x_train, y_train,\n          batch_size=128,\n          epochs=70)","47c0fde3":"pattern = []\noutput = []\nother = []\nfor i in \"[in the quinn living room. granda joe and gerry are watching tv.] \\n gerry: what \":\n    x = charindex.index(i)\n    output.append(x)\n    pattern = np.append(pattern, x)\n    \nfor t in range(4000):\n    if t % 400 == 0:\n        print(\"%\"+str((t\/4000)*100)+\" done\")\n    x = np.reshape(pattern, (1, len(pattern)))\n    pred = model.predict(x, verbose=0)\n    result = np.argmax(pred)\n    result2 = np.random.choice(pred.argsort().flatten()[-3:-1])\n    output.append(result)\n    other.append(result2)\n    pattern = np.append(pattern,result)\n    pattern = pattern[1:len(pattern)]\n    \n    if t % 10 == 0 and t != 0:\n        pattern[-10] = other[-10]\n","b1ee108a":"output = [charindex[x] for x in output]\noutput = ''.join(output)\n\nprint(output)","be7007ba":"Text_Data = derrygirls\nText_Data = Text_Data.replace(\"\\n\", 'newline')\ntokens = nltk.word_tokenize(Text_Data)\nwordindex = list(set(tokens))\nwordindex.sort()\nnp.save(\"wordindex.npy\", wordindex)","46bac664":"word_size = len(wordindex)\nseq_len = 30\n\nx_train = []\ny_train = []\n\nfor i in range(len(tokens)-seq_len): \n    X = tokens[i:i + seq_len]\n    Y = tokens[i + seq_len]\n    x_train.append([wordindex.index(x) for x in X])\n    y_train.append(wordindex.index(Y))\n\nx_train = np.reshape(x_train, (len(x_train), seq_len))\n\ny_train = utils.to_categorical(y_train)","2b75edc8":"def LSTM_model2():\n    model = models.Sequential()\n    inp = layers.Input(shape=(seq_len, ))\n    x = layers.Embedding(word_size, seq_len, trainable=False)(inp)\n    x = tf.compat.v1.keras.layers.CuDNNLSTM(512, return_sequences=True,)(x)\n    x = tf.compat.v1.keras.layers.CuDNNLSTM(512,)(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(512, activation=\"elu\")(x)\n    x = layers.Dense(256, activation=\"elu\")(x)\n    x = layers.Dropout(0.3)(x)\n    outp = layers.Dense(word_size, activation='softmax')(x)\n    \n    model = models.Model(inputs=inp, outputs=outp)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=optimizers.Adam(lr=0.0008),\n                  metrics=['accuracy']\n                 )\n\n    return model\n\nmodel = LSTM_model2()","4aa3a41b":"model.fit(x_train, y_train,\n          batch_size=256,\n          epochs=800)","02bc8428":"pattern = []\noutput = []\nother = []\nfor i in ['[', 'in', 'the', 'quinn', 'kitchen', '.', 'mary', 'is', 'cooking', 'breakfast', '.', 'gerry', 'and', 'joe', 'are', 'watching', 'tv', 'in', 'the', 'living', 'room', '.', ']', 'newline', 'erin', ':', 'mammy', ',', 'i']:\n    x = wordindex.index(i)\n    output.append(x)\n    pattern = np.append(pattern, x)\n    \nfor t in range(500):\n    if t % 50 == 0:\n        print(\"%\"+str((t\/500)*100)+\" done\")\n    x = np.reshape(pattern, (1, len(pattern)))\n    pred = model.predict(x, verbose=0)\n    result = np.argmax(pred)\n    result2 = np.random.choice(pred.argsort().flatten()[-2:-1])\n    output.append(result)\n    other.append(result2)\n    pattern = np.append(pattern,result)\n    pattern = pattern[1:len(pattern)]\n\n    if t % 15 == 0 and t != 0:\n        pattern[-15] = other[-15]","9de43227":"output = [wordindex[x] for x in output]\noutput = TreebankWordDetokenizer().detokenize(output)\noutput = output.replace('newline', '\\n')\nprint(output)","cea5ddf1":"## 5. Developing the model","72b30cc4":"## 7. Generating script by words","532e0db3":"Now to fit the model to the data! Usually, a batch size of 32 or 64 is used in models, but for this sort of exercise it doesn't need to be this small - 128 or 256 is perfectly acceptable. I chose to run it for 70 epochs, because I found that the accuracy didn't really increase much after this.","243b4d76":"## 4. Preparing data","f60e8185":"Repeating the same process but allocating a number to each word instead of each character.","9b58dbe5":"## 3. Loading data ","b37abd13":"![image.png](attachment:image.png)","27dc1191":"## 6. Generating the new script","3d434c1e":"### Why Derry Girls \u2754\u2754","424c70ff":"Now for the most exciting part \u2755 \ud83d\ude01 To start the script, we need to give it some characters to work with. As the model was trained on sequences of 80 characters, we need to choose 80 characters. I'm only generating a script of 4000 characters here, but this can be adjusted to any number. I'm also adding some elements of randomness to prevent repetition.","fceebf8c":"More than 500,000 characters would be ideal, but 350,112 should be enough to generate something decent, especially with the repetitive language in Derry Girls.\n\nNow we need to create sequences that the model will be trained on. Each sequence is 80 characters long, so the first sequence will be:\n\n**'season 1  \n  episode 1  \n  [in erin\u2019s bedroom. orla is reading erin\u2019s diary.]  \n  erin: is that my d'**\n\nThe second sequence will be:\n\n**'eason 1  \n  episode 1  \n  [in erin\u2019s bedroom. orla is reading erin\u2019s diary.]  \n  erin: is that my di'**\n  \n... And so on.\n\nUsing this method means the model will take into account the progression of the sentences.\n\nHowever, instead of training on characters, each character is allocated a number.\n\nLater, we will also try allocating numbers to the words instead of the characters to see which method generates the best script!","59edfb80":"The first step is loading the data. I'll be loading a Text document that I typed up myself (typing 12 episodes of a TV show takes a lot longer than you would expect!). I converted the whole document to lower case; even though this doesn't look great in the resulting script, minimising the number of unique characters simplifies the learning process as the model won't consider the likes of 'Is' and 'is' to be two separate words.","5f61b212":"LSTMs (Long Short Term Memory networks) are excellent for learning and generating text, because they are capable of learning long-term dependencies. Dropout is also super important here to avoid overfitting! We don't want the model to recite whole chunks of the script, but instead to generate something new and hopefully hilarious \ud83e\udd1e\ud83c\udffb","8ac947a1":"![image.png](attachment:image.png)","1cf34943":"## 2. Imports","cc68ae8d":"## 1. Introduction","df45b25a":"At this point, I think most of us have seen at least one AI generated script or screenplay floating around on social media somewhere. From Friends to Harry Potter, these scripts were a real hit at one point. Lines like 'FRIEND ROSS: I made sex with a dinosaur. What can I do? It's my student.\" \ud83d\udc31\u200d\ud83d\udc09 (by Keaton Patti - Twitter @KeatonPatti) and 'The pig of Hufflepuff pulsed like a large bullfrog. Dumbledore smiled at it, and placed his hand on its head: 'You are Hagrid now.'' \ud83e\uddd9\ud83c\udffb\u200d\u2642\ufe0f (Botnik) had us in hysterics. \ud83e\udd23\n\nBut if you're expecting something like that at the end of this, you will probably be disappointed. It would appear that we have been lied to. \ud83d\ude2d Many of the scripts released on social media are actually FAKE! It can be difficult to determine which are fake, because some scripts are a result of really complicated algorithms that produce excellent and hilarious sentences, but usually a big give away is that the fake ones feature words that weren't even included in the movie\/show\/book, or word combinations that aren't used.\n\nNevertheless, even though I'm not expecting the resulting script to make much sense, I have always wanted to give this a go! \ud83d\udcaa\ud83c\udffb","5bd77792":"So you're probably wondering why I chose the show 'Derry Girls'. Turns out sit-coms are an excellent choice for these exercises, because they have a main group of characters and tend to have repetitive catch phrases that the model can easily pick up on - for example in Derry Girls, *'CATCH YOURSELF ON!'*\n\nAlso, Derry Girls is a personal favourite of mine, as I'm a Northern Irish girl myself and I can really relate to it! The language is so unique and is nothing like anything you'd hear in any other sit-com. Words like 'wee' and 'wain' have the potential to make a Natural Language Processing project really interesting - you could say there's actually nothing natural about our language!","5a073462":"This is exactly how we want the script to look \ud83d\udc4d\ud83c\udffb"}}