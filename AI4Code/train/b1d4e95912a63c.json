{"cell_type":{"28b8a463":"code","d071e61b":"code","ce5d5f2b":"code","8a423f2d":"code","d036db22":"code","6a170d33":"code","4dd3b686":"code","df8611ec":"code","7b337b6e":"code","f0d64ec6":"code","0ecb15ef":"code","4da7fc50":"code","6b6d91ee":"code","c2840be0":"code","0c900e52":"code","b7c0e392":"code","889ed73b":"code","31340540":"code","3b0fed35":"code","0fd1426e":"code","804e0598":"code","5e7d8b5c":"code","8f9088d4":"code","f0524c78":"code","b1a74779":"code","2a19842f":"code","47846c60":"code","f4407550":"code","58f8b370":"markdown","a3d0e07f":"markdown","42a0545d":"markdown","d5967dcb":"markdown","61c15743":"markdown","9e4b9d42":"markdown","57862fb5":"markdown","74f8ed1a":"markdown","01bebdae":"markdown","4e819a2d":"markdown","81c0c28d":"markdown","1c3c0ea6":"markdown","77bc2f85":"markdown","6343256a":"markdown","d67043a0":"markdown","8b5005e7":"markdown","7f128653":"markdown","41a9367f":"markdown","72ce6640":"markdown","6fb623ac":"markdown","d690b56a":"markdown","f9198b46":"markdown","82ce70a9":"markdown","84293117":"markdown","8ab7712c":"markdown","08569bd5":"markdown","52a5a640":"markdown"},"source":{"28b8a463":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport gc\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer, Dense, Dropout, Embedding, Input, LayerNormalization, Flatten\nfrom tensorflow.keras.activations import gelu, softmax\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint","d071e61b":"train = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')","ce5d5f2b":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n \n    return df","8a423f2d":"train = train.drop(columns = ['Id', 'Soil_Type7', 'Soil_Type15'])\ntest = test.drop(columns = ['Id', 'Soil_Type7', 'Soil_Type15'])","d036db22":"train['Cover_Type'].value_counts()","6a170d33":"train = train.drop(index = train[train['Cover_Type'] == 5].index).reset_index(drop = True)\ntrain['Cover_Type'].value_counts()","4dd3b686":"train[\"Aspect\"][train[\"Aspect\"] < 0] += 360\ntrain[\"Aspect\"][train[\"Aspect\"] > 359] -= 360\n\ntest[\"Aspect\"][test[\"Aspect\"] < 0] += 360\ntest[\"Aspect\"][test[\"Aspect\"] > 359] -= 360","df8611ec":"train.loc[train[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ntest.loc[test[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\ntrain.loc[train[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ntest.loc[test[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\ntrain.loc[train[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ntest.loc[test[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\ntrain.loc[train[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ntest.loc[test[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\ntrain.loc[train[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ntest.loc[test[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\ntrain.loc[train[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ntest.loc[test[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255","7b337b6e":"train['Sum_Hydrology'] = np.abs(train['Horizontal_Distance_To_Hydrology']) + np.abs(train['Vertical_Distance_To_Hydrology'])\ntrain['Sub_Hydrology'] = np.abs(train['Horizontal_Distance_To_Hydrology']) - np.abs(train['Vertical_Distance_To_Hydrology'])\n\ntest['Sum_Hydrology'] = np.abs(test['Horizontal_Distance_To_Hydrology']) + np.abs(test['Vertical_Distance_To_Hydrology'])\ntest['Sub_Hydrology'] = np.abs(test['Horizontal_Distance_To_Hydrology']) - np.abs(test['Vertical_Distance_To_Hydrology'])","f0d64ec6":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ny = le.fit_transform(train['Cover_Type'].values)","0ecb15ef":"from sklearn.preprocessing import OneHotEncoder\n\n\noe = OneHotEncoder()\ntarget = oe.fit_transform(y.reshape(-1, 1))\ntarget = target.toarray()\n\ntrain = train.drop(columns = 'Cover_Type')\ngc.collect()","4da7fc50":"train['Soil_Type'] = 0\ntrain['Wilderness_Area'] = 0\n\ntest['Soil_Type'] = 0\ntest['Wilderness_Area'] = 0\n\nsoil_columns = train.columns[-42:-4]\narea_columns = train.columns[-46:-42]","6b6d91ee":"train['Soil_Type'] = np.argmax(train[soil_columns].values, axis = -1)\ntrain['Wilderness_Area'] = np.argmax(train[area_columns].values, axis = -1)\ntest['Soil_Type'] = np.argmax(test[soil_columns].values, axis = -1)\ntest['Wilderness_Area'] = np.argmax(test[area_columns].values, axis = -1)","c2840be0":"train.drop(columns = soil_columns, inplace = True)\ntrain.drop(columns = area_columns, inplace = True)\n\ntest.drop(columns = soil_columns, inplace = True)\ntest.drop(columns = area_columns, inplace = True)\n\ndisplay(train, test)","0c900e52":"columns = ['cls', 'Soil_Type', 'Wilderness_Area', 'Elevation', 'Aspect',\n           'Slope', 'Horizontal_Distance_To_Hydrology',\n           'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n           'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n           'Horizontal_Distance_To_Fire_Points', 'Sum_Hydrology', 'Sub_Hydrology']","b7c0e392":"train['cls'] = 0\ntest['cls'] = 0\n\ntrain = train[columns]\ntest = test[columns]\n\ndisplay(train, test)","889ed73b":"num_columns = train.columns[3:]\n\n# Scaling\nsc = StandardScaler()\n\ntrain[num_columns] = sc.fit_transform(train[num_columns].values)\ntest[num_columns] = sc.transform(test[num_columns].values)\n\ndisplay(train, test)","31340540":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\n\ntrain = train.values\ntest = test.values\n\ngc.collect()","3b0fed35":"class MHA(Layer):\n    '''\n    Multi-Head Attention Layer\n    '''\n    \n    def __init__(self, num_head, dropout = 0):\n        super(MHA, self).__init__()\n        \n        # Constants\n        self.num_head = num_head\n        self.dropout_rate = dropout\n        \n    def build(self, input_shape):\n        query_shape = input_shape\n        d_model = query_shape[-1]\n        units = d_model \/\/ self.num_head\n        \n        # Loop for Generate each Attention\n        self.layer_q = []\n        for _ in range(self.num_head):\n            layer = Dense(units, activation = None, use_bias = False)\n            layer.build(query_shape)\n            self.layer_q.append(layer)\n            \n        self.layer_k = []\n        for _ in range(self.num_head):\n            layer = Dense(units, activation = None, use_bias = False)\n            layer.build(query_shape)\n            self.layer_k.append(layer)\n            \n        self.layer_v = []\n        for _ in range(self.num_head):\n            layer = Dense(units, activation = None, use_bias = False)\n            layer.build(query_shape)\n            self.layer_v.append(layer)\n            \n        self.out = Dense(d_model, activation = None, use_bias = False)\n        self.out.build(query_shape)\n        self.dropout = Dropout(self.dropout_rate)\n        self.dropout.build(query_shape)\n        \n    def call(self, x):\n        d_model = x.shape[-1]\n        scale = d_model ** -0.5\n        \n        attention_values = []\n        for i in range(self.num_head):\n            attention_score = softmax(tf.matmul(self.layer_q[i](x), self.layer_k[i](x), transpose_b=True) * scale)\n            attention_final = tf.matmul(attention_score, self.layer_v[i](x))\n            attention_values.append(attention_final)\n            \n        attention_concat = tf.concat(attention_values, axis = -1)\n        out = self.out(self.dropout(attention_concat))\n        \n        return out","0fd1426e":"class IMHA(Layer):\n    '''\n    Intersample Multi Head Attention\n    Attend on row(samples) not column(features)\n    '''\n    \n    def __init__(self, num_head, dropout = 0):\n        super(IMHA, self).__init__()\n        \n        # Constants\n        self.num_head = num_head\n        self.dropout_rate = dropout\n        \n    def build(self, input_shape):\n        b, n, d = input_shape\n        query_shape = input_shape\n        units = (d * n) \/\/ self.num_head\n        # Loop for Generate each Attention\n        self.layer_q = []\n        for _ in range(self.num_head):\n            layer = Dense(units, activation = None, use_bias = False)\n            layer.build([1, b, int(n * d)])\n            self.layer_q.append(layer)\n            \n        self.layer_k = []\n        for _ in range(self.num_head):\n            layer = Dense(units, activation = None, use_bias = False)\n            layer.build([1, b, int(n * d)])\n            self.layer_k.append(layer)\n            \n        self.layer_v = []\n        for _ in range(self.num_head):\n            layer = Dense(units, activation = None, use_bias = False)\n            layer.build([1, b, int(n * d)])\n            self.layer_v.append(layer)\n            \n        self.out = Dense(d, activation = None, use_bias = False)\n        self.out.build(query_shape)\n        self.dropout = Dropout(self.dropout_rate)\n        self.dropout.build(query_shape)\n        \n    def call(self, x):\n        b, n, d = x.shape\n        scale = d ** -0.5\n        x = tf.reshape(x, (1, b, int(n * d)))\n        attention_values = []\n        \n        for i in range(self.num_head):\n            attention_score = softmax(tf.matmul(self.layer_q[i](x), self.layer_k[i](x), transpose_b=True) * scale)\n            attention_final = tf.matmul(attention_score, self.layer_v[i](x))\n            attention_final = tf.reshape(attention_final, (b, n, int(d \/ self.num_head)))\n            attention_values.append(attention_final)\n            \n        attention_concat = tf.concat(attention_values, axis = -1)\n        out = self.out(self.dropout(attention_concat))\n        \n        return out","804e0598":"class FeedForwardNetwork(Layer):\n    def __init__(self, dim, dropout = 0.0):\n        super(FeedForwardNetwork, self).__init__()\n        self.dense = Dense(dim, activation = 'gelu')\n        self.dropout = Dropout(dropout)\n        \n    def call(self, x):\n        return self.dropout(self.dense(x))","5e7d8b5c":"class CustomEmbedding(Layer):\n    def __init__(self, num_categorical, dim):\n        super(CustomEmbedding, self).__init__()\n        self.num_categorical = num_categorical\n        self.dim = dim\n        \n    def build(self, input_shape):\n        b, n = input_shape\n        self.embedding_categorical = Embedding(self.dim * 2, self.dim)\n        self.embedding_categorical.build([b, self.num_categorical])\n        \n        self.embedding_numerical = Dense(self.dim, activation = 'relu')\n        self.embedding_numerical.build([b, int(n - self.num_categorical), 1])\n        \n    def call(self, x):\n        b, n = x.shape\n        categorical_x = x[:, :self.num_categorical]\n        numerical_x = x[:, self.num_categorical:]\n        numerical_x = tf.reshape(numerical_x, (b, int(n - self.num_categorical), 1))\n        \n        embedded_cat = self.embedding_categorical(categorical_x)\n        embedded_num = self.embedding_numerical(numerical_x)\n    \n        embedded_x = tf.concat([embedded_cat, embedded_num], axis = 1)\n        \n        return embedded_x","8f9088d4":"class SAINT(Layer):\n    def __init__(self, repeat, num_categorical, EMB_DIM, MHA_HEADS, IMHA_HEADS):\n        super(SAINT, self).__init__()\n        self.repeat = repeat\n        self.layer_mha = []\n        self.layer_imha = []\n        self.layer_ffn = []\n        self.layer_layernorm = []\n        self.embedding = CustomEmbedding(num_categorical, EMB_DIM)\n        \n        for _ in range(repeat):\n            mha = MHA(MHA_HEADS)\n            imha = IMHA(IMHA_HEADS)\n            ffn_1 = FeedForwardNetwork(EMB_DIM)\n            ffn_2 = FeedForwardNetwork(EMB_DIM)\n            layernorm_1 = LayerNormalization()\n            layernorm_2 = LayerNormalization()\n            layernorm_3 = LayerNormalization()\n            layernorm_4 = LayerNormalization()\n            \n            self.layer_mha.append(mha)\n            self.layer_imha.append(imha)\n            self.layer_ffn.append(ffn_1)\n            self.layer_ffn.append(ffn_2)\n            self.layer_layernorm.append(layernorm_1)\n            self.layer_layernorm.append(layernorm_2)\n            self.layer_layernorm.append(layernorm_3)\n            self.layer_layernorm.append(layernorm_4)\n            \n    def call(self, x):\n        x = self.embedding(x)\n        \n        # Depth of SAINT Layer\n        for i in range(self.repeat):\n            # Multi-Head part\n            x = self.layer_layernorm[i](self.layer_mha[i](x)) + x\n            x = self.layer_layernorm[i+1](self.layer_ffn[i](x)) + x\n            \n            # Intersample Multi-Head part\n            x = self.layer_layernorm[i+2](self.layer_imha[i](x)) + x\n            x = self.layer_layernorm[i+3](self.layer_ffn[i+1](x)) + x\n       \n        # only using cls token for final output\n        out = x[:, 0] # CLS Token\n        \n        return out","f0524c78":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","b1a74779":"def DataLoader(x, y, batch_size, shuffle = False):\n    ds = tf.data.Dataset.from_tensor_slices((x, y))\n    ds = ds.batch(batch_size, drop_remainder = True)\n    if shuffle:\n        ds = ds.shuffle(len(x))\n        \n    return ds","2a19842f":"EPOCH = 5\nBATCH_SIZE = 4096\nNUM_FOLDS = 5\n\nkf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state=2021)\ntest_preds = []\n\nwith tpu_strategy.scope():\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, y)):\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n\n        checkpoint_filepath = f\"folds{fold}.hdf5\"\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = target[train_idx], target[test_idx]\n        \n        train_ds = DataLoader(X_train, y_train, BATCH_SIZE, shuffle = True)\n        valid_ds = DataLoader(X_valid, y_valid, BATCH_SIZE, shuffle = False)\n        \n        model = Sequential([\n            Input(shape = (15,), batch_size = BATCH_SIZE),\n            SAINT(3, 3, 64, 8, 8),\n            Dense(6, activation = 'softmax')\n        ])\n\n        model.compile(optimizer = \"adam\",\n                      loss = \"categorical_crossentropy\",\n                      metrics = ['accuracy'])\n\n        lr = ReduceLROnPlateau(monitor = \"val_loss\",\n                               factor = 0.5,\n                               patience = 2,\n                               verbose = 1)\n\n        es = EarlyStopping(monitor = \"val_loss\",\n                           patience = 3,\n                           verbose = 1,\n                           restore_best_weights = True)\n\n        model.fit(train_ds,\n                  validation_data = valid_ds,\n                  epochs = EPOCH,\n                  batch_size = BATCH_SIZE,\n                  \n                  callbacks = [lr, es])\n\n        test_preds.append(model.predict(test))\n\n        del X_train, X_valid, y_train, y_valid, model\n        gc.collect()","47846c60":"sub = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\nsub['Cover_Type'] = le.inverse_transform(np.argmax(np.array(test_preds).sum(axis = 0), axis = 1))\nsub","f4407550":"sub.to_csv('sub.csv', index = 0)","58f8b370":"# **Memory Reduce Func**\n\n* **If you don't use some memory reducing strategy, you can face some OOM**","a3d0e07f":"# **SAINT Model**","42a0545d":"# **Training**","d5967dcb":"![Saint_Architecture.png](attachment:cc08a78a-039f-4791-8622-122d3f99b550.png)","61c15743":"## **Reference**\n* [Paper](https:\/\/arxiv.org\/abs\/2106.01342)\n* [Github](https:\/\/github.com\/ogunlao\/saint)","9e4b9d42":"# **Target Encoding**\n\n### **Need to use inverse_transform at the end for submission**","57862fb5":"## **Scaling**\n\n* **Standard Scaling Numerical Columns**","74f8ed1a":"## **DataLoader**\n\n* **Because of the remaining last batch, it makes the shape error! So we need to drop it**","01bebdae":"![Supervised_Learning.png](attachment:48ab48d9-e51b-4f3a-9aa6-4c2a6c9ac3ac.png)","4e819a2d":"## **Aspect**\n\n* **Aspect means angle. Good for rescaling**\n* **Hillshade needs to rescale to 0 ~ 255**\n\nFrom [gulshanmishra Kernel](https:\/\/www.kaggle.com\/gulshanmishra\/tps-dec-21-tensorflow-nn-feature-engineering)\n\nThank you for sharing nice notebook :)","81c0c28d":"# **Data Preparing**\n\n* **Add a column named 'cls' to our dataset. The 'cls' column should be the first column as mentioned in paper**\n* **Apply z-transform to numerical columns**\n* **Label encode categorical columns**\n* **It is required that categorical columns are separated from numerical columns.**","1c3c0ea6":"# **SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training**\n\n### **This kernel explains SAINT model architecture**\n### **Contrastive Pre-Taining is not included**","77bc2f85":"## **Thanks a lot for your interest**","6343256a":"## **Multi-Head Attention Layer**","d67043a0":"## **Check Missing Values**\n\n* **Looking good!**","8b5005e7":"## **Label Encoding Categorical Columns**\n### **There are 2 categorical columns!**\n* **Soil_Type**\n* **Wilderness_Area**","7f128653":"# **Feature Engineering**\n\n* **Check Missing Values**\n* **Check Target Column**\n\n* **Soil_Type7 & Soil_Type15 \u2192 useless**","41a9367f":"## **Intersample Multi-Head Attention Layer**","72ce6640":"## **Custom Embedding**\n\n* **Because there are negative values in numerical, we need to seperate numerical columns and categorical columns and apply Dense for numerical embedding, Embedding for categorical columns**","6fb623ac":"# **Modeling**","d690b56a":"## **Done!**","f9198b46":"## **The structure of the layer is as shown in the picture below by original paper.**","82ce70a9":"## **Include 'CLS' columns and move it to first column**\n### **Column order : Categorical \u2192 Numerical**","84293117":"## **FeedForward Layer**","8ab7712c":"## **You can modify some args for better result**\n\n* Decrease Batch Size\n* Increase Epochs\n* Change optimizer and using LR schedular (Paper used AdamW)","08569bd5":"## **Interaction Features**\n\n* **Sum of Hydrology**\n* **Subtraction of Hydrology**","52a5a640":"![InterSample.png](attachment:297d9568-2e92-4337-880b-6ce67e5b53a8.png)"}}