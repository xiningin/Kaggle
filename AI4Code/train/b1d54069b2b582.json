{"cell_type":{"559514e1":"code","b92a331a":"code","d9d57987":"code","74880e07":"code","d4523c97":"code","a446a234":"code","cf4bc90a":"code","f1040b5c":"code","8dcbd468":"code","995aac62":"code","584db73b":"code","5d9ad3fd":"code","9b6d52cd":"code","749a5faf":"code","982acc5c":"code","58842d6c":"code","0b35596b":"code","8106c678":"code","fd28b0b3":"code","400d7768":"code","88181320":"code","49f4460a":"code","34c4d8d1":"code","5cfc12a1":"markdown","ccd26f99":"markdown","924804c7":"markdown","32d0e426":"markdown","ec5c2b84":"markdown","63708ffe":"markdown","b3038eaa":"markdown","eef6acd1":"markdown","b19caadd":"markdown","5541b298":"markdown","54c37c6f":"markdown","4fc2c81c":"markdown","599ad7c5":"markdown","0108273f":"markdown","1b0c3ce4":"markdown","9e47b13e":"markdown","f465acac":"markdown","a78faafe":"markdown","1ba80516":"markdown","762dd6b7":"markdown","ef354410":"markdown","3ac646c4":"markdown"},"source":{"559514e1":"from IPython.display import Image\nImage(filename=\"..\/input\/images\/kagglememe.png\", width=1200, height=300)\n","b92a331a":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\n        \ninput_path = Path('\/kaggle\/input\/tabular-playground-series-jan-2021\/')","d9d57987":"import seaborn as sns","74880e07":"train = pd.read_csv(input_path \/ 'train.csv', index_col='id')\ndisplay(train.head())","d4523c97":"test = pd.read_csv(input_path \/ 'test.csv', index_col='id')\ndisplay(test.head())","a446a234":"target = train.pop('target')","cf4bc90a":"fig = plt.figure(figsize=(14, 8))\ngs = fig.add_gridspec(1, 1)\nax = fig.add_subplot(gs[0, 0])\nsns.distplot(target ,ax=ax, color='#1BCBE5')\n\nbackground_color = \"#FFFFFF\"\n\nax.text(2.9, 0.74, 'Distribution of Target', fontsize=24, fontweight='bold', fontfamily='serif')\nax.get_yaxis().set_visible(False)\nax.set_facecolor(background_color)","f1040b5c":"fig, p = plt.subplots(7, 2, figsize=(20,22))\n#ax = fig.subplots()\n#ax.text(2.9, 0.74, 'Distribution of Target', fontsize=24, fontweight='bold', fontfamily='serif')\nn_bins = 50\n\nr=0\nc=0\ni=\"cont1\"\nfor i in train.columns:\n    \n    tr = train[i]\n    te = test[i]\n    \n    p[r, c].hist(tr, \n                 n_bins, \n                 density=True, \n                 histtype='bar', \n                 color=\"#ff0032\", \n                 label='train', \n                 linestyle='dashed',\n                 edgeColor = 'white')\n    \n    p[r, c].hist(te, \n                 n_bins, \n                 density=True, \n                 histtype='bar', \n                 color=\"#00a8f9\", \n                 label='test', \n                 alpha=0.6)\n    \n    p[r, c].legend(loc='upper right')\n    p[r, c].set_xlabel(i)\n    \n    if c == 0:\n        p[r, c].set_ylabel('frequency')\n    \n    if r == 0:\n        p[r, c].set_title('Histogram')\n    \n    if c < 1:\n        c+=1\n    else:\n        c=0\n        r+=1\n        \nplt.show()  \n    ","8dcbd468":"fig, p = plt.subplots(7, 2, figsize=(20,22))\n#ax = fig.subplots()\n#ax.text(2.9, 0.74, 'Distribution of Target', fontsize=24, fontweight='bold', fontfamily='serif')\nn_bins = 50\n\nr=0\nc=0\ni=\"cont1\"\nfor i in train.columns:\n    \n    tr = train[i]\n    te = target\n    \n    p[r, c].hist((tr*10), \n                 n_bins, \n                 density=True, \n                 histtype='bar', \n                 color=\"#FF25C0\", \n                 label='train', \n                 linestyle='dashed',\n                 edgeColor = 'white')\n    \n    p[r, c].hist(te, \n                 n_bins, \n                 density=True, \n                 histtype='bar', \n                 color=\"#FFCC00\", \n                 label='target', \n                 alpha=0.6)\n    \n    p[r, c].legend(loc='upper right')\n    p[r, c].set_xlabel(i)\n    \n    if c == 0:\n        p[r, c].set_ylabel('frequency')\n    \n    if r == 0:\n        p[r, c].set_title('Histogram')\n    \n    if c < 1:\n        c+=1\n    else:\n        c=0\n        r+=1\n        \nplt.show()  ","995aac62":"corr_mat = train.corr().stack().reset_index(name=\"correlation\")","584db73b":"g = sns.relplot(\n    data=corr_mat,\n    x=\"level_0\", y=\"level_1\", hue=\"correlation\", size=\"correlation\",\n    palette=\"Blues\", hue_norm=(-1, 1), edgecolor=\".7\",\n    height=10, sizes=(50, 250), size_norm=(-.2, .8),\n)\n\ng.set(xlabel=\"Train\", ylabel=\"Train\", aspect=\"equal\")\ng.despine(left=True, bottom=True)\ng.ax.margins(.2)\nfor label in g.ax.get_xticklabels():\n    label.set_rotation(90)","5d9ad3fd":"X_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.80)","9b6d52cd":"input_d=X_train.shape[1]","749a5faf":"linear = LinearRegression()\nscores = cross_val_score(linear, X_train, y_train, scoring='neg_root_mean_squared_error', cv=100)\nlin_rmse_scrs = -scores\n#print('Linear Regression performance:', lin_rmse_scrs)\nprint(min(lin_rmse_scrs))","982acc5c":"tree = DecisionTreeRegressor(max_depth= 10, max_features=9)\nscores = cross_val_score(tree, X_train, y_train, scoring='neg_root_mean_squared_error', cv=100)\ntree_rmse_scrs = -scores\n#print('Decision Tree Regressor performance:', tree_rmse_scrs)\nprint(min(tree_rmse_scrs))","58842d6c":"lgbm = LGBMRegressor(max_depth=11,learning_rate=0.3071070857621833)\nscores = cross_val_score(lgbm, X_train, y_train, scoring='neg_root_mean_squared_error', cv=100)\nlgbm_rmse_scrs = -scores\n#print('LGBM performance:', lgbm_rmse_scrs)\nprint(min(lgbm_rmse_scrs))","0b35596b":"catb = CatBoostRegressor(verbose=False)\nscores = cross_val_score(catb, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10)\ncatb_rmse_scrs = -scores\n#print('CatBoost performance:', catb_rmse_scrs)\nprint(min(catb_rmse_scrs))","8106c678":"xgb_reg = XGBRegressor(random_state=42)\nscores = cross_val_score(xgb_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\nxgb_rmse_scores = np.sqrt(-scores)\n#print('XGBoost performance:', xgb_rmse_scores)\nprint(min(xgb_rmse_scores))","fd28b0b3":"import keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom sklearn.metrics import classification_report\nfrom keras.callbacks import History \nhistory = History()\n\n\nnn  = Sequential()\nnn.add(Dense(units= 28, activation = 'linear', input_dim=14, kernel_initializer=\"uniform\"))\nnn.add(Dense(units= 28, activation = 'linear',kernel_initializer=\"uniform\"))\nnn.add(Dense(units= 1, activation = 'linear',kernel_initializer=\"uniform\"))\nnn.compile(optimizer='adam',\n              loss='mean_squared_error')\nresult=nn.fit(X_train, y_train, validation_split=0.2,\n                       verbose=0, epochs=100,callbacks=[history])\n\n#print(result.history['val_loss'])\nmin(result.history['val_loss'])","400d7768":"target_ans=nn.predict(test)","88181320":"from pandas import DataFrame\nans = DataFrame(target_ans,columns=['target'])\nans.head()","49f4460a":"test1=pd.read_csv(input_path \/ 'test.csv', index_col='id')\ntest1.reset_index(drop=False, inplace=True)\n\ntest1=test1['id']\ntest1.to_frame()\nsub=pd.concat([test1, ans], axis=1, join='inner')","34c4d8d1":"sub.to_csv('submit.csv',index=False)","5cfc12a1":"### Linear Regression","ccd26f99":"<body style=\"color:white\">\n    <h1 style='color:#F12626' align='center'><font size=\"+3\"><font color=#000ECA>Using Regressor Models<\/font><\/font><\/h1>\n\n<h3 align='center'><font>In the follwing cells we are using the regressor models which are as stated earlier , <font color=#85C1E9>Linear Regression<\/font> , <font color=#27AE60>Decision Tree Regressor<\/font>, <font color=#800080>eXtreme Gradient Boosting Regressor<\/font> , <font color=#EC7063>Light Gradient Boosting Regressor<\/font> , <font color=#2E86C1 >Category Boosting Regressor<\/font> and <font color=#FA520E>Regression Artificial Neural Network<\/font>. \nEach model works through 100 epochs , and the minimum loss function has been displayed for each regressor.To make my work easy , I have used the best parameters according to <a href=https:\/\/www.kaggle.com\/dwin183287 class=Sharlto Cope><font color=#EC7063>Sharlto Cope's<\/font><\/a> notebook. It's a great Notebook , please check it out (<a href=https:\/\/www.kaggle.com\/dwin183287\/tps-jan-2021-eda-models class=Notebook >click here<\/a>) and support it by upvoting !!<\/font><\/h3>\n   \n<\/body>","924804c7":"* We see here that the Target has two different peaks of distribution which are separated at the value 8.","32d0e426":"* Converting the list to a dataframe","ec5c2b84":"* Submitting the csv file created.\n","63708ffe":"* Here in the above plot , we can see that there is no similarity between the distributions of Train and Target data. Here the Train data values have been multiplied by 10 , so that the values can be compared , because the ranges of the values of the two data are not the same. ","b3038eaa":"* The above plots shows that more or less the Test and Train data Cont Values are same with little difference and in ***cont5*** the values are **overlapping**. ","eef6acd1":"### Decision Tree ","b19caadd":" <h1 style='color:#FA0048' align='center'><font size=\"+3\">                  The END<\/font><\/h1>","5541b298":"# Training Data","54c37c6f":"### eXtreme Gradient Boosting","4fc2c81c":"<h1><center> <font size=\"+3\"><font color=#440BE3>\u269c\u269c\u269c\u269c<\/font><font color=#0B1AE3>\u269c\u269c\u269c\u269c<\/font><font color=#0B6CE3>\u269c\u269c\u269c\u269c<\/font><font color=#0BE317>\u269c\u269c\u269c\u269c<\/font><font color=#F7DF21>\u269c\u269c\u269c\u269c<\/font><font color=#F73821>\u269c\u269c\u269c\u269c<\/font><font color=#DA0A0A>\u269c\u269c\u269c\u269c<\/font><font color=#29EAEE>\u269c\u269c\u269c\u269c<\/font><\/font><\/center><\/h1>","599ad7c5":"<body style=\"color:white\">\n    <h1 style='color:#F12626' align='center'><font size=\"+3\"><font color=#000ECA>Exploratory Data Analysis<\/font> : Understanding the Data<\/font><\/h1>\n\n<h3 align='center'><font>Here we will see the properties and variations in the data. We know that the data consists of 14 different features named as cont[N] where N->1,2,....13,14. The values of these range from : 0.0 - ~1.2. Where as the <font color=#000ECA>Target<\/font> has a range of approx 0->10.3. In the following cells we will se the Distribution of the values of the data , and also compare the values of the two provided Train and Test data and also compare the values with the Target.\n    Maybe because this is a <font color=#000ECA>Playground Competition<\/font> , the data provided is clean with no missing values.<\/font><\/h3>\n   \n<\/body>","0108273f":"<body style=\"color:white\">\n    <h1 style='color:cyan' align='center'><font size=\"+3\">                   Welcome to the GYM:<\/font><\/h1>\n\n<h3 align='center'><font>Take a Monthly Membership <a href=https:\/\/www.kaggle.com\/competitions\n                                                                              class=https:\/\/www.kaggle.com\/competitions>HERE<\/a> and grind hard .<\/font><\/h3>\n   \n<\/body>\n","1b0c3ce4":"<body style=\"color:white\">\n    <h1 style='color:red' align='center'><font size=\"+3\">AND THE RESULT IS BELOW !!!<\/font><\/h1>\n   \n<\/body>\n","9e47b13e":"* In the above Correlation Heatmap , we can see that there is correlation between cont1, cont6, cont9, cont10, cont11, cont12 and cont13.","f465acac":"<body style=\"color:white\">\n    <h1 style='color:#F12626' align='center'><font size=\"+3\">Introduction<\/font><\/h1>\n\n<h3 align='center'><font>In this notebook, there are <font color=#000ECA>Two<\/font> parts : Understanding the data , Using Regressor models . \n    We have been provided with Tabular data , with <font color=#CA00BA>14<\/font> different independent dataset. In this notebook , 6 different regression techniques have been used , namely , <font color=#85C1E9>Linear Regression<\/font> , <font color=#27AE60>Decision Tree Regressor<\/font> ,<font color=#800080>eXtreme Gradient Boosting Regressor<\/font> , <font color=#EC7063>Light Gradient Boosting Regressor<\/font> , <font color=#2E86C1 >Category Boosting Regressor<\/font> and <font color=#FA520E>Regression Artificial Neural Network<\/font>. \n    I hope you will like this notebook and if you have , then please <font color=#2E86C1 >UPVOTE<\/font> and support the effort !!! I am a beginner , so any suggestion or constructive criticism is appreciated. Please do comment !!<\/font><\/h3>\n   \n<\/body>","a78faafe":"### Categorical Boosting","1ba80516":"* Generating the dataframe with only two columns - \"id\" and \"target\".","762dd6b7":"### Artificial Neural Network","ef354410":"### Light Gradient Boosting Machine ","3ac646c4":"## Making predictions using the Neural Network\n\n* Because the Neural Network generates lesser loss , we are using it to make predictions."}}