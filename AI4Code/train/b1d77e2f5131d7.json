{"cell_type":{"2184d383":"code","f923a6df":"code","0987456b":"code","ece0342e":"code","a4af0be4":"code","a6785e98":"code","8d07a5a8":"code","480cfab6":"code","90516c47":"code","1350430a":"code","38381fd3":"code","e3bd8191":"code","c9c726b3":"code","3e1a33e9":"code","38bac63a":"code","1668ba40":"code","42a8de06":"code","b386d481":"code","db0d917c":"code","5f9a0590":"code","b5aa684e":"markdown","08e3aead":"markdown","f2573877":"markdown","13223af3":"markdown","95299546":"markdown","d79c0677":"markdown","e898a9d6":"markdown","dc243e0b":"markdown","a4c6a19d":"markdown","1dceeb1b":"markdown","6a3b23cf":"markdown","df8ca330":"markdown","1a48ca3f":"markdown","33849262":"markdown","efa79827":"markdown","6832433e":"markdown","bf3c7b90":"markdown","f604490e":"markdown","92c73152":"markdown","0939a096":"markdown","2f91912d":"markdown"},"source":{"2184d383":"import pandas as pd\ngdata=pd.read_csv(\"..\/input\/Admission_Predict_Ver1.1.csv\")\ncol_names=gdata.columns.tolist()\nprint(\"Column names:\")\nprint(col_names)\nprint(\"\\nSample Data:\")\nprint(gdata.head())\n","f923a6df":"gdata=gdata.rename(columns={'Serial No.':'no','GRE Score':'gre','TOEFL Score':'toefl','University Rating':'rating','SOP':'sop','LOR ':'lor',\n                           'CGPA':'gpa','Research':'research','Chance of Admit ':'chance'})","0987456b":"gdata.dtypes","ece0342e":"print('Shape of the data:')\ngdata.shape","a4af0be4":"print('Missing values in columns:')\ngdata.isnull().any()","a6785e98":"gdata.describe()","8d07a5a8":"gdata.groupby('rating').mean()","480cfab6":"gdata[gdata['chance']>0.82].groupby('chance').mean()","90516c47":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12,8))\nplt.hist(gdata['chance'],bins=10,color=\"orange\")\nplt.title('Histogram of Admission Chance')\nplt.xlabel('Admission Chance')\nplt.ylabel('Frequency of Chance')\nplt.show()","1350430a":"plt.figure(figsize=(12,8))\nplt.plot(range(len(gdata[gdata['research']==1])), gdata[gdata['research']==1]['chance'], color='orange')\nplt.plot(range(len(gdata[gdata['research']==0])), gdata[gdata['research']==0]['chance'], color='olive')\nplt.show()","38381fd3":"gdata.boxplot(column='chance',by='rating',grid=False,figsize=(12,8))\nplt.title('The Chance of Admission for University Ratings')\nplt.xlabel('University Rating')\nplt.ylabel('Chance of Admission')\nplt.show()","e3bd8191":"gdata.hist(bins=10, figsize=(20,15))\nplt.show()","c9c726b3":"gdata.drop(['no'],axis=1,inplace=True)\nvar=gdata.columns.values.tolist()\ny=gdata['chance']\nx=[i for i in var if i not in ['chance']]\nx=gdata[x]","3e1a33e9":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2, random_state=0)","38bac63a":"from sklearn.preprocessing import MinMaxScaler\nxs=MinMaxScaler()\nx_train[x_train.columns] = xs.fit_transform(x_train[x_train.columns])\nx_test[x_test.columns] = xs.transform(x_test[x_test.columns])","1668ba40":"import numpy as np\ncy_train=[1 if chance > 0.82 else 0 for chance in y_train]\ncy_train=np.array(cy_train)\n\ncy_test=[1 if chance > 0.82 else 0 for chance in y_test]\ncy_test=np.array(cy_test)\n\n","42a8de06":"# Fitting logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train, cy_train)\n\n\n# Printing accuracy score & confusion matrix\nfrom sklearn.metrics import accuracy_score\nprint('Logistic regression accuracy: {:.3f}'.format(accuracy_score(cy_test, lr.predict(x_test))))\nprint('--------------------------------------')\nfrom sklearn.metrics import classification_report\nprint(classification_report(cy_test, lr.predict(x_test)))\n\ncy = lr.predict(x_test)\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nlr_confm = confusion_matrix(cy, cy_test, [1,0])\nsns.heatmap(lr_confm, annot=True, fmt='.2f',xticklabels = [\"Admitted\", \"Rejected\"] , yticklabels = [\"Admitted\", \"Rejected\"] )\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.title('Logistic Regression')\nplt.show()","b386d481":"# Fitting random forest model\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train, cy_train)\n\n# Printing accuracy score & confusion matrix\nprint('Random Forest Accuracy: {:.3f}'.format(accuracy_score(cy_test, rf.predict(x_test))))\nprint('--------------------------------------')\nprint(classification_report(cy_test, rf.predict(x_test)))\n\ncy = rf.predict(x_test)\nrf_confm = confusion_matrix(cy, cy_test, [1,0])\nsns.heatmap(rf_confm, annot=True, fmt='.2f',xticklabels = [\"Admitted\", \"Rejected\"] , yticklabels = [\"Admitted\", \"Rejected\"] )\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.title('Random Forest')\nplt.show()","db0d917c":"# Fitting support vector machine model\nfrom sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(x_train, cy_train)\n\n# Printing accuracy score & confusion matrix\nprint('Support vector machine accuracy: {:.3f}'.format(accuracy_score(cy_test, svc.predict(x_test))))\nprint('--------------------------------------')\nprint(classification_report(cy_test, svc.predict(x_test)))\n\ncy = svc.predict(x_test)\nsvc_confm = confusion_matrix(cy, cy_test, [1,0])\nsns.heatmap(svc_confm, annot=True, fmt='.2f',xticklabels = [\"Admitted\", \"Rejected\"] , yticklabels = [\"Admitted\", \"Rejected\"] )\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.title('Support Vector Machine')\nplt.show()","5f9a0590":"f_imp=pd.Series(rf.feature_importances_,index=x_train.columns).sort_values(ascending=False)\nprint(f_imp)","b5aa684e":"How often does my classifier predict admitted students correctly? This measurement is called \u201crecall\u201d and a quick look at these diagrams can demonstrate that models ara close but logistic regression and support vector machine models are slightly better for this criteria. Out of 20 True admitted applicants, LR and SVM are correctly classified 18 of them which means the models are approximately have 90% of \"recall\".\n\nHow often does my classifier predict students that will be admitted correctly? This measurement is called \u201cprecision\u201d and Random Forest performs slightly better (17 out of 18) than other classifiers.\n\nAll classifiers have the same accuracy score which is 96%.\n\n# Feature Importance for Random Forest Model","08e3aead":"Fortunately, data has no missing values\n\nThe \"chance\" column is the outcome variable and takes value between 0 and 1. 1 represents that the subject is admitted to the program while 0 represents rejected applications.\nProblem can be classified as a binary classification problem where outcome probability refers to the probability of subject being admitted to the program. Since only chance of admission is provided, \nthe analysis will continue as a prediction analysis of chance of admission.\n\n# Data Exploration\n\nFirst of all, let us see the basic statistics of the data.\n","f2573877":"## Support Vector Machine","13223af3":"## Random Forest","95299546":"The chance of admission depends a great deal on the subjects' research output; hence, research can be a good predictor in predicting the outcome.\n\n## Scatter Plot of University Rating and The Chance of Admission","d79c0677":"Histogram shows us that 'chance of admission' column is well distributed in data.\n\n## Line Plot for Research Output and The Chance of Admission","e898a9d6":"Rename columns to make their use easier. ","dc243e0b":"Average chance of admission of subjects which applied to program with rating 1 is less than that of the subjects which applied to program with higher ratings.\nNow let us analyze the subjects with more than 82% of chance which is the third quartile of the chance data.","a4c6a19d":"# Classification Models & Furher Analysis\n\nIn order to make classification models, outcome of the classification is defined as 'status' which takes value of True for the applicants that have more than 82% chance. Third quartile, 82%, is chosen as threshold since median of the chance data, 72%, is pretty high.","1dceeb1b":"Check shape of the data and whether there exist missing values:","6a3b23cf":"Data normalization is important in order to represent data in comparable scales. ","df8ca330":"Data will be splitted using train_test_split module of scikitlearn library where splitting ratio is chosen as 20% for test data.","1a48ca3f":"# Data Preparation\n\nAs it  can be seen from Data Preprocessing section above, graduate admission data is only include numerical variables. Hence, only the following steps should be implemented before model devolopment:\n\n* The outcome variable is 'chance', and all other features are predictors.\n* 'no' variable should be dropped from dataset since it only indicator of the instances","33849262":"# Data Preprocessing","efa79827":"## *Several observations:*\n\n* Average GPAs of those with higher level of chance to admit is greater than 9 where the data average is 8.57.\n* Average LOR, SOP, GRE and TOEFL grades of those with higher level of chance to admit is greater than the data average.\n\n# Data Visualization\n\n## Histogram of Admission Chance\n\nLet us visualize our data to get a much clearer picture of the data and the significant features.\n","6832433e":"The type of columns can be found as follows:","bf3c7b90":"As it can be seen from the boxplot chart, chance of admission is higher within the applicants of lower rated (5) universities.\n\n## Histogram of Numeric Variables","f604490e":"Second, let us analyze the distribution of subjects' chance of admit in the data. Let us see how rating affects chance of admission:","92c73152":"## Logistic Regression","0939a096":"In order to represent the most important features which influence whether an applicant is admitted in college, random forest classifier is used.","2f91912d":"# Predict Graduate Admissions with Python\n\nThis kernel presents a reference implementation of an graduate admission analysis that is built by using Python\u2019s Scikit-Learn library. Logistic Regression, Random Forest, and Support Vector Machine models will be introduced. Also measure the accuracy of models that are built by using Machine Learning, and further development will be assessed."}}