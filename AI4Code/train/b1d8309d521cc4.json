{"cell_type":{"51e70b83":"code","8f457f81":"code","4e669550":"code","9c500012":"code","b32a3529":"code","470e653c":"code","c8edfc06":"code","2740fedf":"code","97a7a852":"code","a447ecf3":"code","18a79a41":"code","84d5e8de":"code","71863faf":"code","2c26daa5":"code","95cae879":"markdown","b4479d54":"markdown","9c5bb121":"markdown","4d7a8d42":"markdown","937db43b":"markdown","948bb1f6":"markdown","f665869a":"markdown","ae6fcaa1":"markdown","4b15fd89":"markdown","7ad40d9b":"markdown","fd272239":"markdown","57e3a0fd":"markdown","604fe053":"markdown","0da3f2b4":"markdown","22df4866":"markdown"},"source":{"51e70b83":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","8f457f81":"#check the decoration\ntrain.columns","4e669550":"# Relationship with categorical features\n# \uc22b\uc790 \ubcc0\uc218 \uc120\ud0dd\nnum_cols = [col for col in train.columns if train[col].dtype in ['int64', 'float64']]\n\n# Id \ubc0f SalePrice \ubcc0\uc218 \ucd94\ucd9c\nnum_cols.remove('Id')\nnum_cols.remove('SalePrice')\n\n# num_cols\ub97c \ub370\uc774\ud130 \ud504\ub808\uc784\uc73c\ub85c \ubcc0\ud658\nnum_analysis = train[num_cols].copy()\n\nfor col in num_cols:\n    if num_analysis[col].isnull().sum() > 0:\n        num_analysis[col] = SimpleImputer(strategy='median').fit_transform(num_analysis[col].values.reshape(-1,1))\n        \n# Model\nclf = ExtraTreesRegressor(random_state=42)\netreg_model = clf.fit(num_analysis, train.SalePrice)\n\ndef plot_importance(model, features, num=len(num_cols), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features})\n    plt.figure(figsize=(16, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\n\nplot_importance(etreg_model, num_cols)","9c500012":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)","b32a3529":"plt.figure(figsize=(8,8))\nplt.title('Correlation matrix of critical variables')\ncols =['OverallQual', 'GarageCars', 'GrLivArea', 'YearBuilt', \n       'FullBath', '1stFlrSF', 'TotalBsmtSF', 'GarageArea','Fireplaces','GarageYrBlt','SalePrice']\nsns.heatmap(train[cols].corr(),annot=True,square=True);\n\ndef plot_numerical(col, discrete=False):\n    if discrete:\n        fig, ax = plt.subplots(1,2,figsize=(12,6))\n        sns.stripplot(x=col, y='SalePrice', data=train, ax=ax[0])\n        sns.countplot(train[col], ax=ax[1])\n        fig.suptitle(str(col) + ' analysis')\n    else:\n        fig, ax = plt.subplots(1,2,figsize=(12,6))\n        sns.scatterplot(x=col, y='SalePrice', data=train, ax=ax[0])\n        sns.distplot(train[col], kde=False, ax=ax[1])\n        fig.suptitle(str(col) + ' analysis')\n","470e653c":"from scipy import stats\nfrom scipy.stats import norm\n\n#data transformation\ntrain['GrLivArea'] = np.log(train['GrLivArea'])\n#transformed histogram and normal probability plot\nsns.distplot(train['GrLivArea'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(train['GrLivArea'], plot=plt)","c8edfc06":"train.notnull()\ntrain.notnull().sum()\ntrain.notna()\ntrain.notna().sum()","2740fedf":"train.isnull()\ntrain.isnull().sum()","97a7a852":"train1 = train.loc[:, ['OverallQual', 'SalePrice']]\ntrain1.head()","a447ecf3":"from sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\n\nX_data = train.loc[:,['OverallQual', 'GrLivArea']]\nY_data = train.loc[:,['SalePrice']]\n\nlinear_regression_model = linear_model.LinearRegression()\nlinear_regression_model.fit(X=pd.DataFrame(X_data), y=Y_data)\nlinear_regression_model_prediction = linear_regression_model.predict(X=pd.DataFrame(X_data))\n\n# \uc2e4\uc81c\uac12\/\uc608\uce21\uac12 \uc2dc\uac01\ud654\nfig=plt.figure(figsize=(12, 4))\ngraph = fig.add_subplot(1,1,1)\ngraph.plot(Y_data[:88], marker='o', color='blue', label='actual value')\ngraph.plot(linear_regression_model_prediction[:88], marker='^', color='red', label='theoretical value')\ngraph.set_title('Multiple-Regression-Analysis', size=30)\nplt.xlabel(' ', size=5)\nplt.ylabel('SalePrice', size=15)\nplt.legend(loc = 'best')\n\nlin_mod=LinearRegression()\nlin_mod.fit(train1.loc[:, ['OverallQual']], train1.loc[:, ['SalePrice']])\nprint(\"intercept:\", lin_mod.intercept_)\nprint(\"coefficient:\", lin_mod.coef_)","18a79a41":"\"\"\"\n1. batch : 1\ud68c \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub418\ub294 \ub370\uc774\ud130\uc758 \uc218\n\"\"\"\nimport tensorflow as tf\nimport numpy as np\n\nx_train = train1['OverallQual']\ny_train = train1['SalePrice']\nprint(np.shape(x_train))\nprint(np.shape(y_train))\n\n#W = tf.Variable(tf.random.normal([1]), name=\"weight\")\n#b = tf.Variable(tf.random.normal([1]), name=\"bias\")\n#hypothesis = x_train*W+b\n#cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n\nsgd = tf.keras.optimizers.SGD(learning_rate=0.01)\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(5, input_dim=1))\n# => x_train * w\n# input_dim = x_train dimension - 1\nmodel.compile(loss='mean_squared_error', optimizer=sgd)\n\nmodel.fit(x_train, y_train, epochs=10)","84d5e8de":"print(model.predict(np.array([8])))","71863faf":"import xgboost as xgb\nfrom xgboost import XGBRegressor\n\ntrain.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = train.SalePrice\nX = train.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\n\n\nregr = xgb.XGBRegressor(\n                 colsample_bytree=0.2,\n                 gamma=0.0,\n                 learning_rate=0.01,\n                 max_depth=4,\n                 min_child_weight=1.5,\n                 n_estimators=7200,                                                                  \n                 reg_alpha=0.9,\n                 reg_lambda=0.6,\n                 subsample=0.2,\n                 seed=42)\n\nmy_model = regr\nmy_model.fit(X, y, verbose=False)\n\n# make predictions\npredictions = my_model.predict(X)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, y)))\n\n#predictions_XGB = my_model.predict(np.array([8]))\n#print(predictions_XGB)","2c26daa5":"import csv\n\ndef file_write(name, new_list):\n    with open(name, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(new_list[0])\n        for i in new_list[1:]:\n            writer.writerow(i)","95cae879":"SalePrice and OverallQual are highly correlated.","b4479d54":"**7. Training by XGBoost**","9c5bb121":"**5. Data Processing**","4d7a8d42":"There are 259 of Missing value at LotFrontage.","937db43b":"**4. Missing value**","948bb1f6":"GrLivArea is also important.","f665869a":"**8. Results**","ae6fcaa1":"**1. Calling library & Loading data (\ub77c\uc774\ube0c\ub7ec\ub9ac \ud638\ucd9c \ubc0f \ub370\uc774\ud130 \ub85c\ub4dc)**","4b15fd89":"**0. Abstract**\n\nCalculating the value of an output variable using a specific input variable value is called a prediction problem. Among prediction problems, a problem in which the value of the output variable is a continuous value is called a regression or regression problem. By Exploration Data Analysis, The value(or column) of the data set is effected SalePrice and I analyzed and wrinkled the correlation of values by correction matrix. In addition, 'To solve the problem', I tried to process data and to solve the reaction analysis problem using TensorFlow and XGBoost.","7ad40d9b":"The SalePrice of 'OverallQual = 8' may be 272281.53","fd272239":"References\n* https:\/\/www.kaggle.com\/basakcelikdemir\/housepriceprediction#4.Feature-Selection\n* https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python#5.-Getting-hard-core\n* https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition\/notebook#Feature-Engineering\n* https:\/\/www.kaggle.com\/erikbruin\/house-prices-lasso-xgboost-and-a-detailed-eda\n* https:\/\/www.kaggle.com\/marsggbo\/kaggle\n* https:\/\/jaehyeongan.github.io\/2019\/07\/08\/Kaggle-challenge-%EB%B3%B4%EC%8A%A4%ED%84%B4-%EC%A7%91%EA%B0%92-%EC%98%88%EC%B8%A1-House-Prices-Advanced-Regression-Techniques\/\n* https:\/\/www.kaggle.com\/humananalog\/xgboost-lasso\n* https:\/\/www.kdnuggets.com\/2019\/05\/xgboost-algorithm.html\n* https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/index.html\n* https:\/\/wikidocs.net\/43295","57e3a0fd":"I think OverallQual is important. And GarageCars affects SalePrice in one unit, but not much in the second. In other words, GarageCars is not very important.","604fe053":"**3. Correlation matrix (heatmap style)**","0da3f2b4":"**6. Training by Tensorflow**","22df4866":"**2. EDA & data visualization (\ub370\uc774\ud130 \ud0d0\uc0c9 \ubc0f \uc2dc\uac01\ud654)**"}}