{"cell_type":{"11ac3e6b":"code","8b8c98e1":"code","45f22600":"code","2a68c14b":"code","9b74df56":"code","ce2592aa":"code","d97b13bc":"code","a72297c3":"code","02bd0da8":"code","4d2e2f7d":"code","689ed0bd":"code","446cbd18":"code","853ead3d":"code","e26d9ab2":"code","323f38e8":"code","ece94c59":"code","32b93a82":"code","e8543dd9":"code","9be65c05":"code","f46ff45c":"code","1a6262ad":"code","ad5f4d94":"markdown","7f100152":"markdown","ac9cc6a8":"markdown","4e2b314f":"markdown","ad20d520":"markdown","32413a10":"markdown","8b77b103":"markdown","370cb34b":"markdown","187ee914":"markdown","88b0091a":"markdown","e34bbebc":"markdown","91a609cb":"markdown","19b948c6":"markdown","f132b6b6":"markdown","3cacb843":"markdown","62d1b586":"markdown","3293e6c7":"markdown","bdec91b8":"markdown","ebb7e1dd":"markdown","fa784460":"markdown","a5299472":"markdown","49cdce46":"markdown","ab3eff38":"markdown","75dea078":"markdown","cc4c56e3":"markdown","6304e762":"markdown","445a5294":"markdown","de814476":"markdown","8609b569":"markdown","a87ce23f":"markdown"},"source":{"11ac3e6b":"import re\nimport nltk\nimport spacy\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom wordcloud import WordCloud\nfrom unidecode import unidecode\nfrom string import punctuation\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\n\nfrom spacy.lang.pt.stop_words import STOP_WORDS\n\n\n%matplotlib inline","8b8c98e1":"def limpar_texto(text):\n    \n    # Colocando todas as letras do texto em caixa baixa:\n    text = text.lower()\n    # Excluindo cita\u00e7\u00f5es com @:\n    text = re.sub('@[^\\s]+', '', text)\n    # Excluindo acentua\u00e7\u00e3o das palavras:\n    text = unidecode(text)\n    # Excluindo html tags, como <strong><\/strong>:\n    text = re.sub('<[^<]+?>','', text)\n    # Excluindo os n\u00fameros:\n    text = ''.join(c for c in text if not c.isdigit())\n    # Excluindo URL's:\n    text = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+)|(http?:\/\/[^\\s]+))', '', text)\n    # Excluindo pontua\u00e7\u00e3o:\n    text = ''.join(c for c in text if c not in punctuation)\n    \n    # Retornando o texto tratado tokenizado:\n    \n    return word_tokenize(text)","45f22600":"# O texto abaixo cont\u00e9m todas as situa\u00e7\u00f5es para que seja feita a limpeza:\n\ntexto = \"\"\"\n<strong>Ol\u00e1<\/strong> @usuario, vamos testar a fun\u00e7\u00e3o #clean_text?\nCaso tenha d\u00favidas, uma boa pesquisa no www.google.com pode ajudar!\nMesmo que voc\u00ea tenha que pesquisar 100 vezes!\n\"\"\"\n\ntexto_limpo = limpar_texto(texto)\nprint(texto_limpo)","2a68c14b":"# Removendo as stopwords utilizando a lista do nltk e do spacy:\n\nsw = list(set(stopwords.words('portuguese') + list(STOP_WORDS)))\n\ndef remove_stop_words(texts, stopwords = sw):\n      \n    new_texts = list()\n    \n    for word in texts:\n        if word not in stopwords:\n            new_texts.append(''.join(word))\n\n    return new_texts\n","9b74df56":"print(sw)","ce2592aa":"texto_sem_stop_words = remove_stop_words(texto_limpo)\nprint(texto_sem_stop_words)","d97b13bc":"texto_sem_stop_words = remove_stop_words(texto_limpo, sw + ['cleantext'])\nprint(texto_sem_stop_words)","a72297c3":"# primeiramente \u00e9 necess\u00e1rio realizar a instala\u00e7\u00e3o abaixo:\n\n!python -m spacy download pt","02bd0da8":"# Vamos criar uma fun\u00e7\u00e3o que mostra o texto original, a interpreta\u00e7\u00e3o - da fun\u00e7\u00e3o - sem\u00e2ntica dela e o lema\n\nnlp = spacy.load(\"pt\")\n\ndef verificar_lemma(words):\n    \n    text = \"\"\n    pos = \"\"\n    lemma = \"\"\n    for word in nlp(words):\n        text += word.text + \"\\t\"\n        pos += word.pos_ + \"\\t\"\n        lemma += word.lemma_ + \"\\t\"\n\n    print(text)\n    print(pos)\n    print(lemma)","4d2e2f7d":"verificar_lemma('o sentido desta frase est\u00e1 errado')","689ed0bd":"verificar_lemma('voc\u00ea est\u00e1 se sentindo bem?')","446cbd18":"# Vamos criar uma fun\u00e7\u00e3o que mostra o texto original e o stem de cada palavra\n\ndef verificar_radical(words):\n    \n    stemmer = nltk.stem.SnowballStemmer('portuguese')\n    text = \"\"\n    stem = \"\"\n    \n    for word in word_tokenize(words):\n\n        text += word + \"\\t\"\n        stem += stemmer.stem(word) + \"\\t\"\n    \n    print(text)\n    print(stem)","853ead3d":"verificar_radical('o sentido desta frase esta errado')","e26d9ab2":"verificar_radical('voc\u00ea est\u00e1 se sentindo bem?')","323f38e8":"def nuvem_palavras(textos):\n    \n    # Juntando todos os textos na mesma string\n    todas_palavras = ' '.join([texto for texto in textos])\n    # Gerando a nuvem de palavras\n    nuvem_palvras = WordCloud(width= 800, height= 500,\n                              max_font_size = 110,\n                              collocations = False).generate(todas_palavras)\n    # Plotando nuvem de palavras\n    plt.figure(figsize=(24,12))\n    plt.imshow(nuvem_palvras, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","ece94c59":"df = pd.read_csv('..\/input\/imdb-ptbr\/imdb-reviews-pt-br.csv', nrows=1000)","32b93a82":"# vamos ver as primeiras cinco linhas do dataset:\ndf.head(5)","e8543dd9":"# Construindo a nuvem de palavras:\nnuvem_palavras(df[\"text_pt\"])","9be65c05":"def countvectorizer(textos):\n\n    vect = CountVectorizer()\n    text_vect = vect.fit_transform(textos)\n    \n    return text_vect\n\ndef tfidfvectorizer(textos):\n    \n    vect = TfidfVectorizer(max_features=50)\n    text_vect = vect.fit_transform(textos)\n    \n    return text_vect","f46ff45c":"class preprocess_nlp(object):\n    \n    def __init__(self, texts, stopwords = True, lemma=False, stem=False, wordcloud=True, numeric='tfidf'):\n        \n        self.texts = texts\n        self.stopwords = stopwords\n        self.lemma = lemma\n        self.stem = stem\n        self.wordcloud = wordcloud\n        self.numeric = numeric\n        self.new_texts = None\n        self.stopwords_list = list()\n        \n    def clean_text(self):\n\n        new_texts = list()\n\n        for text in self.texts:\n\n            text = text.lower()\n            text = re.sub('@[^\\s]+', '', text)\n            text = unidecode(text)\n            text = re.sub('<[^<]+?>','', text)\n            text = ''.join(c for c in text if not c.isdigit())\n            text = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+)|(http?:\/\/[^\\s]+))', '', text)\n            text = ''.join(c for c in text if c not in punctuation)\n            new_texts.append(text)\n        \n        self.new_texts = new_texts\n\n    def create_stopwords(self):\n        \n        stop_words = list(set(stopwords.words('portuguese') + list(STOP_WORDS)))\n        \n        for word in stop_words:\n\n            self.stopwords_list.append(unidecode(word))\n       \n    \n    def add_stopword(self, word):\n        \n        self.stopwords_list += [word]\n        \n\n    def remove_stopwords(self):\n\n        new_texts = list()\n\n        for text in self.new_texts:\n\n            new_text = ''\n\n            for word in word_tokenize(text):\n\n                if word.lower() not in self.stopwords_list:\n\n                    new_text += ' ' + word\n\n            new_texts.append(new_text)\n\n        self.new_texts = new_texts\n\n\n    def extract_lemma(self):\n        \n        nlp = spacy.load(\"pt\")\n        new_texts = list()\n\n        for text in self.texts:\n\n            new_text = ''\n\n            for word in nlp(text):\n\n                new_text += ' ' + word.lemma_\n\n            new_texts.append(new_text)\n        \n        self.new_texts = new_texts\n    \n\n    def extract_stem(self):\n\n        stemmer = nltk.stem.SnowballStemmer('portuguese')\n        new_texts = list()\n\n        for text in self.texts:\n\n            new_text = ''\n\n            for word in word_tokenize(text):\n\n                new_text += ' ' + stemmer.stem(word)\n\n            new_texts.append(new_text)\n\n        self.new_texts = new_texts\n    \n\n    def word_cloud(self):\n\n        all_words = ' '.join([text for text in self.new_texts])\n        word_cloud = WordCloud(width= 800, height= 500,\n                               max_font_size = 110,\n                               collocations = False).generate(all_words)\n        plt.figure(figsize=(24,12))\n        plt.imshow(word_cloud, interpolation='bilinear')\n        plt.axis(\"off\")\n        plt.show()\n        \n\n    def countvectorizer(self):\n\n        vect = CountVectorizer()\n        text_vect = vect.fit_transform(self.new_texts)\n\n        return text_vect\n    \n\n    def tfidfvectorizer(self):\n\n        vect = TfidfVectorizer(max_features=50)\n        text_vect = vect.fit_transform(self.new_texts)\n\n        return text_vect\n    \n    \n    def preprocess(self):\n\n        self.clean_text()\n        \n        if self.stopwords == True:\n            self.create_stopwords()\n            self.remove_stopwords()\n            \n        if self.lemma == True:\n            self.extract_lemma()\n        \n        if self.stem == True:\n            self.extract_stem() \n        \n        if self.wordcloud == True:\n            self.word_cloud()\n        \n        if self.numeric == 'tfidf':\n            text_vect = self.tfidfvectorizer()\n        elif self.numeric == 'count':\n            text_vect = self.countvectorizer()\n        else:\n            print('metodo nao mapeado!')\n            exit()\n            \n        return text_vect","1a6262ad":"prepro = preprocess_nlp(df['text_pt'], numeric='count')\n#adicionando as palavras filme e filmes na lista de palavras de parada, pois elas s\u00e3o irrelevantes neste contexto\nprepro.add_stopword('filme') \nprepro.add_stopword('filmes') \nsparse_matrix = prepro.preprocess()","ad5f4d94":"# Fun\u00e7\u00f5es para Pr\u00e9-Processamento","7f100152":"Vamos testar as fun\u00e7\u00f5es de nuvem de palavras, CountVectorizer e TfidfVectorizer em um dataset de reviews traduzidos do imdb:","ac9cc6a8":"## Lematiza\u00e7\u00e3o e stemiza\u00e7\u00e3o","4e2b314f":"Ap\u00f3s a realiza\u00e7\u00e3o da limpeza dos textos \u00e9 necess\u00e1rio transformar o dataset n\u00famero para aplicar algum algoritmo de aprendizagem. Para tal, vamos apresentar duas t\u00e9cnicas: **CountVectorizer** e **TfidfVectorizer**.\n\nO CountVectorizer agrupa todas as palavras e faz uma contagem da frequ\u00eancia de cada uma.\n\nO TfidfVectorizer \u00e9 um \u00edndice no qual o valor aumenta proporcionalmente \u00e0 contagem das palavras, mas \u00e9 compensado pela frequ\u00eancia da palavra no corpus (conjunto de todas as palavras do dataset). Este \u00e9 o IDF, que significa *inverse document frequency part* - parte inversa da frequ\u00eancia no documento. A vantagem de us\u00e1-lo est\u00e1 no fato de que ele vai dar menos import\u00e2ncia para palavras como as *stopwords*.\n\nAbaixo temos fun\u00e7\u00f5es para as duas t\u00e9cnicas:","ad20d520":"# Classe de Pr\u00e9-processamento para NLP","32413a10":"Novamente a palavra **sentido** seria agrupada, mas neste caso, como \u00e9 o radical, est\u00e1 correto.","8b77b103":"# Conclus\u00e3o","370cb34b":"Se estiv\u00e9ssemos criando um modelo a palavra 'cleantext' talvez n\u00e3o fosse importante. Vamos adicion\u00e1-la na nossa lista de palavras de palavra rodar novamente a fun\u00e7\u00e3o:","187ee914":"Na cria\u00e7\u00e3o desta classe coloquei alguns argumentos opcionais em rela\u00e7\u00e3o \u00e0 quais fun\u00e7\u00f5es ser\u00e3o executadas no pr\u00e9-processamento. O padr\u00e3o ser\u00e1 remover as palavras de parada, fazer a limpeza do texto, apresentar a nuvem de palavras e aplicar o TfidfVectorizer.\n\nAgora vamos aplicar na base do imdb:","88b0091a":"Um dos problemas do NLP est\u00e1 na qualidade dos dados, principalmente quando se trabalha com textos n\u00e3o formais (como redes sociais). Al\u00e9m disso a quantidade de material replic\u00e1vel em portugu\u00eas \u00e9 escasso, o que atrapalha bastante na modelagem. Sendo assim, o objetivo deste notebook \u00e9 criar ferramentas para o pr\u00e9-processamento dos textos.","e34bbebc":"Vamos ver a nossa lista de palavras de parada:","91a609cb":"Para concluir, utilizar ou n\u00e3o as fun\u00e7\u00f5es de lema e radical \u00e9 de escolha de quem est\u00e1 desenvolvendo. Caso seja escolhido por utilizar, aconselho dar uma aten\u00e7\u00e3o na qualidade das extra\u00e7\u00f5es.","19b948c6":"As palavras de parada s\u00e3o aquelas que, dependendo do caso, podem ser consideradas irrelevantes para o conjunto de resultados a ser exibido em uma busca realizada em uma *search engine*. Por exemplo: o, para, com, foi.\n\n**Quando remov\u00ea-las de um texto?**\n\nQuando se remove as palavras de parada geralmente o texto perde o seu contexto (ou sentido), o que pode atrapalhar alguns algoritmos de descoberta, principalmente aqueles que trabalham com redes neurais. Ent\u00e3o \u00e9 importante ter aten\u00e7\u00e3o.\n\nAgora, quando montamos um **saco de palavras** (Bag-of-Words) as palavras de maior frequ\u00eancia provavelmente ser\u00e3o palavras de parada. Portanto, neste caso, remov\u00ea-las \u00e9 uma boa pr\u00e1tica.\n\n**Diferen\u00e7as de linguagens e bibliotecas**\n\nCada linguagem possui as suas palavras de parada e a qualidade da remo\u00e7\u00e3o depende de v\u00e1rios fatores, dentre eles o qu\u00e3o correto est\u00e1 escrito o texto. Ent\u00e3o, dependendo de onde ele foi coletado (redes sociais por exemplo), poder\u00e3o aparecer ru\u00eddos ap\u00f3s a remo\u00e7\u00e3o.\nAs duas principais bibliotecas que possuem fun\u00e7\u00f5es para remo\u00e7\u00e3o de palavras de parada s\u00e3o spacy e nltk. Al\u00e9m disso, \u00e9 poss\u00edvel (e recomendado pelo menos considerar) criar a pr\u00f3pria lista de palavras para remover do texto nesta etapa.","f132b6b6":"## Nuvem de palavras","3cacb843":"Agora vamos aplic\u00e1-la no texto de sa\u00edda da fun\u00e7\u00e3o de limpeza:","62d1b586":"Neste caso n\u00e3o iremos aplic\u00e1-las, pois n\u00e3o estamos interessados neste notebook em avan\u00e7ar para a etapa de cria\u00e7\u00e3o de modelos, mas sim em preparar um dataset limpo para tal. Ao inv\u00e9s disso, vamos juntar todas as fun\u00e7\u00f5es em uma classe e aplic\u00e1-las ao dataset do imdb.","3293e6c7":"A primeira fun\u00e7\u00e3o, e talvez a mais importante, \u00e9 para limpar o texto. Nesta etapa \u00e9 tratado tudo o que pode atrapalhar no processo de modelagem. Al\u00e9m disso, acredito que ela possa ser utilizada em qualquer situa\u00e7\u00e3o quando se trata de NLP.\n\nNa fun\u00e7\u00e3o abaixo s\u00e3o realizados os seguintes passos para limpeza do texto:\n\n- Colocar o texto em caixa baixa (letras min\u00fasculas);\n- Excluir cita\u00e7\u00f5es - \u00fatil para textos retirados de redes sociais;\n- Excluir acentua\u00e7\u00e3o das palavras;\n- Excluir html tags para textos retirados de f\u00f3runs e afins;\n- Excluir n\u00fameros;\n- Excluir URL's;\n- Excluir pontua\u00e7\u00e3o e caracteres especiais.\n\nExistem outras tratativas que podem ser adicionadas, mas algumas possuem restri\u00e7\u00f5es em quest\u00e3o de instala\u00e7\u00e3o (s\u00f3 funcionar no linux por exemplo) ou s\u00f3 ter dispon\u00edvel em ingl\u00eas. Um exemplo \u00e9 a fun\u00e7\u00e3o Speller da biblioteca autocorrect que corrige erros de digita\u00e7\u00e3o (com um erro logicamente) e s\u00f3 funciona em ingl\u00eas.","bdec91b8":"## Limpeza do texto","ebb7e1dd":"# Introdu\u00e7\u00e3o","fa784460":"## CountVectorizer e TfidfVectorizer","a5299472":"Neste documento abordamos algumas fun\u00e7\u00f5es de pr\u00e9-processamento de textos para an\u00e1lise de sentimento onde foi poss\u00edvel notar que existe uma gama enorme de possibilidades para tal. Ao final, criamos uma classe, sem aprofundar muito em cada fun\u00e7\u00e3o, para realizar a tratativa de um dataframe de textos com algumas op\u00e7\u00f5es e aplicamos em um dataset selecionado.\nA partir da\u00ed \u00e9 poss\u00edvel realizar estudos explorat\u00f3rios e preditivos na base resultante.","49cdce46":"Neste caso a fun\u00e7\u00e3o extraiu o lema corretamente. O problema \u00e9 que em um dataset com as duas frases, a palavra sentido seria agrupada, sendo que o lema das duas n\u00e3o \u00e9 o mesmo, o que poderia acarretar em problemas em um algoritmo de aprendizagem. Agora vamos ver os radicais.","ab3eff38":"Podemos observar que aparecem v\u00e1rias palavras de parada como relevantes no dataset. Portanto seria interessante remov\u00ea-las, talvez adicionando a palavra \"filme\", pois se trata de *reviews* de filmes.","75dea078":"## Remo\u00e7\u00e3o das Palavras de Parada (Stop Words)","cc4c56e3":"Lematiza\u00e7\u00e3o e stemiza\u00e7\u00e3o s\u00e3o processos que reduzem as palavras. A vantagem de utiliz\u00e1-los \u00e9 a redu\u00e7\u00e3o do vocabul\u00e1rio e abstra\u00e7\u00e3o de significado. No caso do texto que estamos usando como exemplo, as palavras 'pesquisa' e 'pesquisar' poderiam ser reduzidas, pois em quest\u00e3o de significado elas agregam de forma igual no processo de modelagem.\n\n- A lematiza\u00e7\u00e3o reduz a palavra ao seu lema, que \u00e9 a forma no masculino e singular. No caso de verbos o lema \u00e9 o infinitivo. Por exemplo, as palavras \"menino\", \"meninos\", \"menininhos\" s\u00e3o todas formas do lema: \"menino\".\n\n- A stemiza\u00e7\u00e3o reduz a palavra ao seu radical. Por exemplo, as palavras \"menino\", \"meninos\", \"menininhos\" s\u00e3o todas formas do lema: \"menin\".\n\nUm dos problemas de utilizar estas fun\u00e7\u00f5es, principalmente a lematiza\u00e7\u00e3o, \u00e9 que ele pode retornar alguns resultados estranhos, podendo perder o contexto do original. Mas mesmo assim o contexto ficar\u00e1 melhor do que extrair o radical.\n\nEm termos de velocidade de processamento, extrair os radicais vai fazer com que mais palavras se agrupem e, consequentemente, uma poss\u00edvel modelagem fique mais r\u00e1pida. Mas n\u00e3o quer dizer que a qualidade da predi\u00e7\u00e3o vai ficar boa.","6304e762":"Uma nuvem de palavras \u00e9 uma boa maneira de verificar quais palavras s\u00e3o mais ou menos relevantes em um dataset. Trata-se de um gr\u00e1fico que *plota* as palavras indicando atrav\u00e9s de seu tamanho a quantidade de vezes que apareceu em todos os textos. Outra utilidade \u00e9 adicionar novas palavras na lista de stop words que por ventura podem aparecer como relevante e n\u00e3o estarem na lista.\n\nAbaixo temos uma fun\u00e7\u00e3o que cria uma nuvem de palavras:","445a5294":"Neste caso a fun\u00e7\u00e3o extraiu erroneamente o lema da palavra **sentido**, pois ela \u00e9 um substantivo neste caso e o lema seria sentido. Al\u00e9m disso, separou a palavra desta em **d** e **esta**, sendo que o lema seria **deste**. Para as demais palavras a extra\u00e7\u00e3o do lema funcionou corretamente.","de814476":"Testando a fun\u00e7\u00e3o:","8609b569":"Vamos verificar a extra\u00e7\u00e3o do radical com as mesmas frases que utilizamos na fun\u00e7\u00e3o de lema:","a87ce23f":"Vamos verificar como ficaria a extra\u00e7\u00e3o do lema em algumas frases:"}}