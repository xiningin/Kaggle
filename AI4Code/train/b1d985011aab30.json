{"cell_type":{"5cea3b0e":"code","95d494b2":"code","5dc9be40":"code","69a43304":"code","72bd6613":"code","817bd28c":"code","afbb4e39":"code","3ce40a73":"code","cc4d8123":"code","66a9a267":"code","257ef1ba":"markdown","518a5897":"markdown","47282d05":"markdown","14b16f37":"markdown","8bc261ea":"markdown","db1e2e10":"markdown","a1d57f4b":"markdown","552d897d":"markdown","7ce63c0b":"markdown","77b28c30":"markdown","c7de2e93":"markdown"},"source":{"5cea3b0e":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import RandomizedSearchCV, KFold\nfrom scipy.stats import loguniform, uniform, randint\n\nRANDOM_STATE = 1563","95d494b2":"df = pd.read_csv('\/kaggle\/input\/cleaning-cord-19-metadata\/cord_metadata_cleaned.csv')\nprint(f'There are {len(df)} studies.')","5dc9be40":"embeddings = np.load('\/kaggle\/input\/biowordvec-precomputed-cord19\/biowordvec.npy')\nprint(f'Embedding matrix has shape: {embeddings.shape}')","69a43304":"estimator = GaussianMixture(\n    n_components=10,\n    covariance_type='full', \n    max_iter=100, \n    n_init=1, \n    init_params='kmeans', \n    random_state=RANDOM_STATE, \n)","72bd6613":"N_ITER = 20\nN_SPLITS = 4\n\nparam_distributions = {\n    \"n_components\": randint(2, 256),\n    \"covariance_type\": ['diag', 'full', 'spherical'],\n}\n\ncv = KFold(\n    n_splits=N_SPLITS, \n    shuffle=True, \n    random_state=RANDOM_STATE\n)\n\nhp_search = RandomizedSearchCV(\n    estimator=estimator,\n    param_distributions=param_distributions,\n    n_iter=N_ITER,\n    n_jobs=N_SPLITS,\n    cv=cv,\n    verbose=1,\n    random_state=RANDOM_STATE,\n    return_train_score=True,\n    refit=True\n)\n\nhp_search.fit(embeddings)\nbest_model = hp_search.best_estimator_","817bd28c":"print(f'Best validation likelihood: {hp_search.best_score_}')","afbb4e39":"print(f'Best params: {hp_search.best_params_}')","3ce40a73":"df['cluster'] = best_model.predict(embeddings)","cc4d8123":"cluster_count = df['cluster'].value_counts().sort_values()\n\nax = cluster_count.plot(kind='bar', figsize=(15, 5))\nax.set_xticks([])\nax.set_xlabel(\"Cluster id\")\nax.set_ylabel(\"Count\")\nax.grid(True)","66a9a267":"(df\n    .drop(columns=['title_lang', 'abstract_lang', 'distance'])\n    .to_csv('\/kaggle\/working\/cord_metadata_word2vec.csv', index=False)\n)","257ef1ba":"## Introduction\n\n**<span style=\"color:red\">The notebook is based upon our previous work. They should be evaluated together!<\/span>** [Link](https:\/\/www.kaggle.com\/quittend\/unsupervised-abstracts-screening-part-1)","518a5897":"## BioWordVec + GMM","47282d05":"### Extract features\nI couldn't extract features using BioWordVec in the kernel due to the out of memory issues, thus I did it on my personal computer using the following snippet. I decided to \"max pool features over time\" as suggested in many papers like [Rethinking Complex Neural Network Architectures forDocument Classification](https:\/\/cs.uwaterloo.ca\/~jimmylin\/publications\/Adhikari_etal_NAACL2019.pdf).\n\n```\nfrom gensim.models.keyedvectors import KeyedVectors\n\nmodel = KeyedVectors.load_word2vec_format(\n    fname='.\/BioWordVec_PubMed_MIMICIII_d200.vec.bin', \n    binary=True\n)\n\nnlp = spacy.load('en_core_sci_sm')\n\ndef vectorize(text: str):\n    features = np.array([\n        model[token.text] \n        for token in nlp(text)\n        if token.text in model.vocab\n    ])\n    \n    \n    return features.max(axis=0) if features.size != 0 else np.zeros(200)\n    \ndf['text_vector'] = df['text'].apply(vectorize)\n```","14b16f37":"### Define model","8bc261ea":"### Hyperparameter search\nA single set of hyperparameters for 4 splits on 4 cores with one KMeans initialization for GMM takes roughly 3.5 min including refiting model on the whole dataset.","db1e2e10":"## Save results\nSuch results are not easily interpretable as TF-IDF, but there are much less clusters, meaning that such approach is worth evaluation.","a1d57f4b":"## Visualize data\n","552d897d":"### How many elements does each cluster have?","7ce63c0b":"## Import packages","77b28c30":"Clusters seem to be very well balanced!","c7de2e93":"## Read data"}}