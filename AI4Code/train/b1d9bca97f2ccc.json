{"cell_type":{"922e22c5":"code","3cbf00e4":"code","5290f73a":"code","f92a408f":"code","35aab770":"code","220642ba":"code","46453e5a":"code","fbca87c5":"code","6cb30af4":"code","d753b389":"code","0cb5ce43":"code","6e9e0169":"code","5adb9455":"code","bff33408":"code","144fa747":"code","1faab838":"code","fe0e764e":"code","af186641":"code","b949ab8c":"code","5a2c7cc6":"code","63762645":"code","da7e00fe":"code","e30656fa":"code","1267a26a":"code","4266fe9a":"code","3972dcfb":"code","1a9a3a8b":"code","c488d7dd":"code","b6327264":"code","58866f1b":"code","64dddd12":"code","cb06a736":"code","4cbd90ab":"code","2eabd584":"code","00132d46":"code","e596627a":"code","7476a0cc":"code","29a0cf20":"code","c2a1c659":"code","45707db4":"code","9b08358a":"code","e79dd606":"code","629d682c":"code","85f28fad":"code","1ef72d49":"code","c409f441":"code","a4e70d66":"code","a4f6f464":"code","8d10a1d4":"code","81808fe7":"code","a82a21bd":"code","5bf4ade8":"code","a9927400":"code","dcf4a102":"code","67ae72b4":"code","408e0b17":"code","604412ed":"code","609c4fe7":"code","3f730719":"code","07f7508f":"code","bd0dd32d":"code","7c79ecf4":"code","bf86f2b8":"code","f737041d":"code","4be96db7":"code","e2441ccf":"code","51d6c2e3":"code","ac4584b7":"code","6d8ac3dc":"code","a16a5b05":"code","c5f79761":"code","875fe59c":"code","8649cc1f":"code","17d42fa7":"code","2dd7fcf2":"code","09066d50":"code","8283ff0c":"code","c7c05b31":"code","cf0fdd2e":"code","5be109b4":"code","f00b5822":"code","4f73fb73":"code","307714c9":"code","a9b74768":"code","1f467bb1":"code","d3dae03c":"code","ea4ef3ae":"code","222011f8":"code","547305e4":"code","d9a18a60":"code","6c7e9f4b":"code","5a722b2a":"code","2240376f":"code","7b4ccdda":"code","0450a489":"code","80889db1":"markdown","a48eb2cd":"markdown","2ab52f34":"markdown","d4caf5c8":"markdown","f4ca1d5e":"markdown","19b3d563":"markdown","1e6b05e1":"markdown","27471362":"markdown","dcfb7af7":"markdown","3b40b638":"markdown","d716e2da":"markdown","5d8f0672":"markdown","8baea2d9":"markdown","b9d31ca2":"markdown","34925b6a":"markdown","120e866d":"markdown","e9f2a9e0":"markdown","b75b4e71":"markdown","587b5b25":"markdown","fd53437d":"markdown","675dd393":"markdown"},"source":{"922e22c5":"import pandas as pd\nimport numpy as np\n\n# Increase visible rows and columns\npd.options.display.max_columns = 100\npd.options.display.max_rows = 100","3cbf00e4":"# Importing movie_metadata.csv\nmoviedata = pd.read_csv('..\/input\/movie_metadata.csv',encoding = \"ISO-8859-1\")","5290f73a":"moviedata.head()","f92a408f":"# information about dataset\nmoviedata.info(max_cols=300)","35aab770":"moviedata.describe(include='all')","220642ba":"# First lets drop the columns which is not going to be useful for our analysis\n\n# the movie link and name are of no use for analysis\n\ncolumns_nouse = ['movie_title','movie_imdb_link']\n\nmoviedata_1 = moviedata.drop(columns_nouse,axis=1)\n\nmoviedata_1.columns","46453e5a":"# Creating dummy variables for genres\n\ngenres_dummy = moviedata_1['genres'].str.get_dummies('|')\ngenres_dummy.columns","fbca87c5":"# Removing columns that make no sense\n\ngenres_dummy = genres_dummy.drop(['-', 'A', 'B', 'C',\n       'D', 'F', 'H', 'M', 'S', 'T',  'W', 'a', 'c', 'd', 'e',\n       'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'y'],axis=1)\n\ngenres_dummy.columns","6cb30af4":"for colname in genres_dummy.columns:\n    genres_dummy = genres_dummy.rename(columns={colname:(\"genre_\" + colname)})","d753b389":"moviedata_1 = pd.concat([moviedata_1,genres_dummy],axis=1)\n\nmoviedata_1.describe(include='all')","0cb5ce43":"# We can see several columns with Null\/NA Values.\n\n# Get columns with NA by finding percentage\ndef get_na_percentage(df):\n    NA_perc = pd.DataFrame(df.isna().sum() \/ len(df) * 100)\n    return NA_perc.sort_values(by=[0], ascending=False)\n\nget_na_percentage(moviedata_1)","6e9e0169":"# For some numerical features we can impute the NA to 0\n\ncolumns_impute_zero = ['facenumber_in_poster','actor_1_facebook_likes',\n                       'actor_2_facebook_likes','num_critic_for_reviews',\n                       'director_facebook_likes','num_user_for_reviews',\n                       'actor_3_facebook_likes']\n\nfor colname in columns_impute_zero:\n    moviedata_1[colname].fillna(0,inplace=True)\n    \nget_na_percentage(moviedata_1)","5adb9455":"# We will replace the NA with a vlaue \"Unknown\" for categorical fields like actor names and director names\n\ncolumns_impute_unknown = ['actor_1_name','actor_2_name','actor_3_name','country','language','color','content_rating',\n                         'plot_keywords','director_name']\n\nfor colname in columns_impute_unknown:\n    moviedata_1[colname].fillna('Unknown',inplace=True)\n    \nget_na_percentage(moviedata_1)","bff33408":"# We will drop the rows where NA values are present for few columns\ncolumns_remove_na_rows = ['aspect_ratio','duration','title_year']\n\n\nmoviedata_1.dropna(subset = columns_remove_na_rows , inplace=True)\n    \nget_na_percentage(moviedata_1)","144fa747":"median_budget = np.nanmedian(moviedata_1['budget'])\nmoviedata_1['budget'].fillna(median_budget,inplace=True)\n\nget_na_percentage(moviedata_1)","1faab838":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# To ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef plot_univariate_num(dfname,colname):\n    plt.figure(figsize=(18,10))\n    plt.xlabel('x',fontsize=15)\n    plt.ylabel('Y ',fontsize=15)\n    plt.xticks(rotation=50,fontsize=12,ha=\"right\")\n    plt.yticks(fontsize=12,ha=\"right\")\n    ax = sns.distplot(dfname[colname].dropna())\n    plt.show()","fe0e764e":"def plot_bivariate_num(colname,colname2):\n    \n    def get_df(dfname,bins=10):\n        df1 = pd.DataFrame(dfname[colname2]).reset_index(drop=True)\n        df2 = pd.DataFrame(pd.cut(dfname[colname],bins=bins)).reset_index(drop=True)\n        df = pd.concat([df2,df1],axis=1)\n        return df\n    \n    #return get_df(df_top_3_purpose_debt_consolidation)\n    plt.figure(figsize = [18,8])\n    \n    plot_1_data = get_df(moviedata_1)\n    plot_1 = sns.barplot(x=colname,y=colname2,data=plot_1_data)\n    plt.xlabel(colname,fontsize=15)\n    plt.title((colname.upper() + \" vs. \" + colname2.upper()),fontsize=15)\n    plt.ylabel(colname2,fontsize=15)\n    plt.xticks(rotation=50,fontsize=12,ha=\"right\")\n    plt.yticks(fontsize=12,ha=\"right\")\n\n    plt.show()\n    ","af186641":"# Top Actors\n\nmoviedata_1[['actor_1_name','actor_1_facebook_likes']].groupby(['actor_1_name']).mean().sort_values(by=['actor_1_facebook_likes'],ascending = False)","b949ab8c":"moviedata_1.describe()","5a2c7cc6":"# Detecting Outliers\n\ndef get_outlier_percentage(x):\n    Q1 = x.quantile(0.25)\n    Q3 = x.quantile(0.75)\n    \n    IQR = Q3 - Q1\n    \n    outlier_min = Q1 - 1.5 * IQR\n    outlier_max = Q3 + 1.5 * IQR\n    \n    return {'%' : round(((x <= outlier_min) | (x >= outlier_max)).sum() \/ len(x) * 100,2), 'min' : round(outlier_min,2) ,'max': round(outlier_max,2)}\n\n\ndef plot_boxplot(x):\n    fig = plt.figure(figsize = [18,2])\n    ax= sns.boxplot(x)\n    plt.show()","63762645":"# director facebook likes\nprint(pd.DataFrame(moviedata_1['director_facebook_likes']).apply(get_outlier_percentage,axis=0))\nplot_boxplot(moviedata_1['director_facebook_likes'])","da7e00fe":"# Capping director facebook likes\nmoviedata_1['director_facebook_likes'] = moviedata_1['director_facebook_likes'].map(lambda x: 178 if x > 178 else x)","e30656fa":"# actor1 facebook likes\nprint(pd.DataFrame(moviedata_1['actor_1_facebook_likes']).apply(get_outlier_percentage,axis=0))\nplot_boxplot(moviedata_1['actor_1_facebook_likes'])\n\n# actor2 facebook likes\nprint(pd.DataFrame(moviedata_1['actor_2_facebook_likes']).apply(get_outlier_percentage,axis=0))\nplot_boxplot(moviedata_1['actor_2_facebook_likes'])\n\n# actor3 facebook likes\nprint(pd.DataFrame(moviedata_1['actor_3_facebook_likes']).apply(get_outlier_percentage,axis=0))\nplot_boxplot(moviedata_1['actor_3_facebook_likes'])\n\n# cast facebook likes\nprint(pd.DataFrame(moviedata_1['cast_total_facebook_likes']).apply(get_outlier_percentage,axis=0))\nplot_boxplot(moviedata_1['cast_total_facebook_likes'])\n","1267a26a":"# Capping actor facebook likes\nmoviedata_1['actor_1_facebook_likes'] = moviedata_1['actor_1_facebook_likes'].map(lambda x: 1248 if x > 1248 else x)\nmoviedata_1['actor_2_facebook_likes'] = moviedata_1['actor_2_facebook_likes'].map(lambda x: 575 if x > 575 else x)\nmoviedata_1['actor_3_facebook_likes'] = moviedata_1['actor_3_facebook_likes'].map(lambda x: 326 if x > 326 else x)\nmoviedata_1['cast_total_facebook_likes'] = moviedata_1['cast_total_facebook_likes'].map(lambda x: 2686 if x > 2686 else x)","4266fe9a":"# movie duration\nprint(pd.DataFrame(moviedata_1['duration']).apply(get_outlier_percentage,axis=0))\nplot_boxplot(moviedata_1['duration'])\n\n# capping duration\nmoviedata_1['duration'] = moviedata_1['duration'].map(lambda x: 1.95 if x > 1.95 else x)","3972dcfb":"# movie facebook like \nprint(pd.DataFrame(moviedata_1['movie_facebook_likes']).apply(get_outlier_percentage,axis=0))\nplot_boxplot(moviedata_1['movie_facebook_likes'])\n\n# movie facebook like  capping\nmoviedata_1['movie_facebook_likes'] = moviedata_1['movie_facebook_likes'].map(lambda x: 687 if x > 687 else x)","1a9a3a8b":"# reviews and votes\n\nprint(pd.DataFrame(moviedata_1['num_voted_users']).apply(get_outlier_percentage,axis=0))\nplot_boxplot(moviedata_1['num_voted_users'])\nprint(pd.DataFrame(moviedata_1['num_user_for_reviews']).apply(get_outlier_percentage,axis=0))\nplot_boxplot(moviedata_1['num_user_for_reviews'])\nprint(pd.DataFrame(moviedata_1['num_critic_for_reviews']).apply(get_outlier_percentage,axis=0))\nplot_boxplot(moviedata_1['num_critic_for_reviews'])\n\n# reviews and votes capping\nmoviedata_1['num_voted_users'] = moviedata_1['num_voted_users'].map(lambda x: 283674.0 if x > 283674.0 else x)\nmoviedata_1['num_user_for_reviews'] = moviedata_1['num_user_for_reviews'].map(lambda x: 820.88 if x > 820.88 else x)\nmoviedata_1['num_critic_for_reviews'] = moviedata_1['num_critic_for_reviews'].map(lambda x: 449.0 if x > 449.0 else x)","c488d7dd":"# facenumber_in_poster\n\nprint(pd.DataFrame(moviedata_1['facenumber_in_poster']).apply(get_outlier_percentage,axis=0))\nplot_boxplot(moviedata_1['facenumber_in_poster'])\n\n# capping\nmoviedata_1['facenumber_in_poster'] = moviedata_1['facenumber_in_poster'].map(lambda x: 5 if x > 5 else x)","b6327264":"# gross and budget\n\n# facenumber_in_poster\n\nprint(\"Gross\",pd.DataFrame(moviedata_1['gross']).apply(get_outlier_percentage,axis=0)['gross'])\nplot_boxplot(moviedata_1['gross'])\n\nprint(\"Budget\",pd.DataFrame(moviedata_1['budget']).apply(get_outlier_percentage,axis=0)['budget'])\nplot_boxplot(moviedata_1['budget'])\n\n# capping\nmoviedata_1['gross'] = moviedata_1['gross'].map(lambda x: 449955182.5 if x > 449955182.5 else x)\nmoviedata_1['budget'] = moviedata_1['budget'].map(lambda x: 113000000.0 if x > 113000000.0 else x)","58866f1b":"cor_1 = moviedata_1.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(cor_1,annot = True)","64dddd12":"# To have a simpler view of high correlation, we will unstack the pairs\n\n# To check correlation either positive or negative, we will use absolute values\n\ncor_unstacked = cor_1.unstack()\n\ncor_unstacked[cor_unstacked < 1].sort_values(ascending = False)\n","cb06a736":"# Number of User Reviews on Movies\n\n# We had seen above that:\n# mean vote is about 305\n# median vote is about 175 \n# whereas the max goes upto 6000\n# there is a standard deviation of about 425\n\nplot_univariate_num(moviedata_1,'num_user_for_reviews')\n\n# We can observe here that most movies have very less number of reviews compared to some popular movies which have revies close to 1000s\n\n# there is surge at end due to capped outliers","4cbd90ab":"# Number of User Votes on Movies\n\n# We had seen above that:\n# mean vote is about 98000\n# median vote is about 41000 \n# whereas the max goes upto 2 million\n# there is a standard deviation of about 160000\n\nplot_univariate_num(moviedata_1,'num_voted_users')\n\n# We can observe here that most movies have very less number of votes compared to some popular movies which have votes in huge numbers\n\n# there is surge at end due to capped outliers","2eabd584":"# imdb score\n\nplot_univariate_num(moviedata_1,'imdb_score')\n\n# Looking at the dependent variable here\n# the IMBD ratings seem to be quite normally distributed with spikes seen at even number probably because people rate in absolute numbers","00132d46":"# Gross box office collection\n\nplot_univariate_num(moviedata_1,'gross')\n\n# the gross collection is towards lower side for most movies, also displaying few movies have got exceptionally high gross collection\n\n# there is surge at end due to capped outliers","e596627a":"# num_critic_for_reviews\n\nplot_univariate_num(moviedata_1,'num_critic_for_reviews')\n\n# unlike user reviews, critic reviews are quite wide spread towards all films, this could be a good measure to use\n\n# there is surge at end due to capped outliers","7476a0cc":"y = moviedata_1['imdb_score']\n\ng1 = moviedata_1['actor_1_facebook_likes']\ng2 = moviedata_1['actor_2_facebook_likes']\ng3 = moviedata_1['actor_3_facebook_likes']\n \ndata = (g1, g2, g3)\ncolors = (\"red\", \"green\", \"blue\")\ngroups = (\"Actor1\", \"Actor2\", \"Actor3\") \n \n# Create plot\n\nfig = plt.figure(figsize = [18,8])\nax = fig.add_subplot(1, 1, 1, facecolor=\"1.0\")\n \nfor data, color, group in zip(data, colors, groups):\n    x = data\n    ax.scatter(x, y, alpha=0.8, c=color, edgecolors='none', s=30, label=group)\n\n\nplt.title('Actors and IMDB Score')\nplt.legend(loc=2)\nplt.show()","29a0cf20":"# The above plot shows that the actor likes is pretty much concentrated and shows high IMDB scores","c2a1c659":"y = moviedata_1['imdb_score']\n\ng2 = moviedata_1['num_user_for_reviews']\ng3 = moviedata_1['num_critic_for_reviews']\n \ndata = (g2, g3)\ncolors = (\"red\", \"green\")\ngroups = (\"User Review\", \"Critic Review\") \n \n# Create plot\n\nfig = plt.figure(figsize = [18,8])\nax = fig.add_subplot(1, 1, 1, facecolor=\"1.0\")\n \nfor data, color, group in zip(data, colors, groups):\n    x = data\n    ax.scatter(x, y, alpha=0.8, c=color, edgecolors='none', s=30, label=group)\n\n\nplt.title('Review Count and IMDB Score')\nplt.legend(loc=2)\nplt.show()","45707db4":"plot_bivariate_num('actor_1_facebook_likes','imdb_score')\n\n#There doesn't seem to be a very high relationship between imdb score and actor facebook likes","9b08358a":"    \nplot_bivariate_num('budget','gross')\n\n# Higher budget films tend to make higher box office gross income","e79dd606":"plot_bivariate_num('actor_1_facebook_likes','cast_total_facebook_likes')  \n\n# As expected from the high correlation, the plot looks uniform","629d682c":"plot_bivariate_num('num_critic_for_reviews','imdb_score')\n\n# Higher rated movies tend to get more critic reviews","85f28fad":"plot_bivariate_num('budget','imdb_score')\n\n# Budget doesn't seem to have a very high effect on imdb score","1ef72d49":"# we will remove few columns which are not useful for linear regression modelling\n\n# plot keyword has too many unique entries, hence ignoring\n\n# Since we have seen gross has a very high correlation with budget for example, hence we will drop the gross\n\ncolumns_nouse = ['actor_1_name','actor_2_name','actor_3_name','director_name','plot_keywords','gross']\n\n\nmoviedata_1 = moviedata_1.drop(columns_nouse,axis=1)\n\nmoviedata_1.columns","c409f441":"# We will create the categorical features as dummy variable\n\nmoviedata_1['language'].value_counts()","a4e70d66":"moviedata_1['language'] = moviedata_1['language'].apply(lambda x : 'Other' if x not in ['English','French','Spanish','Mandarin','Hindi','Japanese','German','Cantonese'] else x)\nmoviedata_1['language'].value_counts()","a4f6f464":"language_dummy = moviedata_1['language'].str.get_dummies()\nlanguage_dummy = language_dummy.loc[:, language_dummy.columns != 'Other']","8d10a1d4":"moviedata_1['country'].value_counts()","81808fe7":"moviedata_1['country'] = moviedata_1['country'].apply(lambda x : 'Other' if x not in ['USA','UK','France','Canada','Germany','Australia',\n'China','Spain','Japan','India','Hong Kong','Italy'] else x)\nmoviedata_1['country'].value_counts()","a82a21bd":"country_dummy = moviedata_1['country'].str.get_dummies()\ncountry_dummy = country_dummy.loc[:, country_dummy.columns != 'Other']","5bf4ade8":"moviedata_1['color'].value_counts()","a9927400":"color_dummy = moviedata_1['color'].str.get_dummies()\ncolor_dummy = color_dummy.loc[:, color_dummy.columns != 'Unknown']","dcf4a102":"moviedata_1['content_rating'].value_counts()","67ae72b4":"content_rating_dummy = moviedata_1['content_rating'].str.get_dummies()\ncontent_rating_dummy = content_rating_dummy.loc[:, content_rating_dummy.columns != 'Unknown']","408e0b17":"for colname in language_dummy.columns:\n    language_dummy = language_dummy.rename(columns={colname:(\"langauge_\" + colname)})\n    \nfor colname in country_dummy.columns:\n    country_dummy = country_dummy.rename(columns={colname:(\"country_\" + colname)})\n    \nfor colname in content_rating_dummy.columns:\n    content_rating_dummy = content_rating_dummy.rename(columns={colname:(\"Rating_\" + colname)})\n\nfor colname in color_dummy.columns:\n    color_dummy = color_dummy.rename(columns={colname:(\"color_\" + colname)})","604412ed":"moviedata_1.columns","609c4fe7":"# Adding the dummy variables and removing the categorical variables for which dummy created\n\nmoviedata_1 = pd.concat([moviedata_1,language_dummy,country_dummy,content_rating_dummy,color_dummy],axis=1)\n\ncolumns_nouse = ['language', 'country', 'genres', 'color', 'content_rating']\n\nmoviedata_1 = moviedata_1.drop(columns_nouse,axis=1)\n\nmoviedata_1.describe(include='all')","3f730719":"# Putting feature variable to X\nX = moviedata_1.drop(['imdb_score'],axis=1)\n\n# Putting response variable to y\ny = moviedata_1['imdb_score']","07f7508f":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7 , random_state=100)","bd0dd32d":"from sklearn.linear_model import LinearRegression","7c79ecf4":"lm = LinearRegression()\n\n# fit the model to the training data\nlm.fit(X_train,y_train)","bf86f2b8":"# print the intercept\nprint(lm.intercept_)","f737041d":"# Let's see the coefficient\ncoeff_df = pd.DataFrame(lm.coef_,X_test.columns,columns=['Coefficient'])\ncoeff_df","4be96db7":"# Making predictions using the model\n\ny_pred = lm.predict(X_test)\n\ny_pred_lm_default = y_pred","e2441ccf":"#Error Terms\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmse = mean_squared_error(y_test, y_pred)\nr_squared = r2_score(y_test, y_pred)\n\nprint('Mean_Squared_Error :' ,mse)\nprint('r_square_value :',r_squared)","51d6c2e3":"import statsmodels.api as sm\n\nX_train_sm = X_train\nX_train_sm = sm.add_constant(X_train_sm)\n\nlm_1 = sm.OLS(y_train,X_train_sm).fit()\n\n","ac4584b7":"#Let's see the summary of our linear model\nprint(lm_1.summary())","6d8ac3dc":"# UDF for calculating vif value\ndef vif_cal(input_data, dependent_col):\n    vif_df = pd.DataFrame( columns = ['Var', 'Vif'])\n    x_vars=input_data.drop([dependent_col], axis=1)\n    xvar_names=x_vars.columns\n    for i in range(0,xvar_names.shape[0]):\n        y=x_vars[xvar_names[i]] \n        x=x_vars[xvar_names.drop(xvar_names[i])]\n        rsq=sm.OLS(y,x).fit().rsquared  \n        vif=round(1\/(1-rsq),2)\n        vif_df.loc[i] = [xvar_names[i], vif]\n    return vif_df.sort_values(by = 'Vif', axis=0, ascending=False, inplace=False)","a16a5b05":"vif_cal(input_data=moviedata_1, dependent_col=\"imdb_score\")\n\n# we can see very high VIFs which denote that there is very high multicolliniarity in existance","c5f79761":"# Using RFE for feature selection\nfrom sklearn.feature_selection import RFE\n\n# Running RFE with number of the variable = 20\nlm_2 = LinearRegression()\nrfe = RFE(lm_2, 20)             \nrfe = rfe.fit(X_train, y_train)","875fe59c":"# Creating X_test dataframe with RFE selected variables\ncol_2 = X_train.columns[rfe.support_]\nprint(col_2)\nX_train_rfe = X_train[col_2]","8649cc1f":"lm_rfe = sm.OLS(y_train,X_train_rfe).fit()\nprint(lm_rfe.summary())\n\n# We can see the Adjusted R-squared has now improved a lot","17d42fa7":"col_rfe = list(col_2.values)\ncol_rfe.append('imdb_score')\n\nvif_cal(input_data=moviedata_1[col_rfe], dependent_col=\"imdb_score\")\n\n# we still have some high VIFs but it is much better than before","2dd7fcf2":"# Making Prediction\n\n# Now let's use our model to make predictions.\n\n# Creating X_test_6 dataframe by dropping variables from X_test\nX_test_rfe = X_test[col_2]\n\n# Making predictions\ny_pred = lm_rfe.predict(X_test_rfe)\ny_pred_lm_rfe = y_pred\n","09066d50":"# a visual inspection of IMDB ratings\n\npd.concat([y_pred,y_test],axis=1)","8283ff0c":"# Actual and Predicted\n\nc = [i for i in range(1,301,1)] # generating index \nfig = plt.figure(figsize=(18,10)) \nplt.plot(c,y_test.head(300), color=\"blue\", linewidth=2.5, linestyle=\"-\") #Plotting Actual\nplt.plot(c,y_pred.head(300), color=\"red\",  linewidth=2.5, linestyle=\"-\") #Plotting predicted\nfig.suptitle('Actual vs Predicted', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                               # X-label\nplt.ylabel('IMDB score', fontsize=16)                       # Y-label","c7c05b31":"# Plotting the error terms to understand the distribution.\n\nfig = plt.figure(figsize=(18,10))\nsns.distplot((y_test-y_pred),bins=50)\nfig.suptitle('Error Terms', fontsize=20)                  # Plot heading \nplt.xlabel('y_test - y_pred', fontsize=18)                  # X-label\nplt.ylabel('Index', fontsize=16)                          # Y-label\n\n","cf0fdd2e":"from sklearn.linear_model import Lasso\n\nlr = Lasso(alpha=1e-10,normalize=True, max_iter=1e5)\nlr.fit(X_train, y_train)\n","5be109b4":"y_pred = lr.predict(X_test)\ny_pred = pd.Series(y_pred)\n\ny_test_0 = y_test.reset_index()\ny_test_0 = y_test_0['imdb_score']\n\ny_pred_lasso = y_pred\n\npd.concat([y_pred,y_test_0],axis=1)  ","f00b5822":"mse = mean_squared_error(y_test_0, y_pred)\nr_squared = r2_score(y_test_0, y_pred)\n\nprint('Mean_Squared_Error :' ,mse)\nprint('r_square_value :',r_squared)","4f73fb73":"# Actual and Predicted\n\nc = [i for i in range(1,301,1)] # generating index \nfig = plt.figure(figsize=(18,10)) \nplt.plot(c,y_test_0.head(300), color=\"blue\", linewidth=2.5, linestyle=\"-\") #Plotting Actual\nplt.plot(c,y_pred.head(300), color=\"red\",  linewidth=2.5, linestyle=\"-\") #Plotting predicted\nfig.suptitle('Actual vs Predicted', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                               # X-label\nplt.ylabel('IMDB score', fontsize=16)                       # Y-label","307714c9":"# Plotting the error terms to understand the distribution.\n\nfig = plt.figure(figsize=(18,10))\nsns.distplot((y_test_0 - y_pred),bins=50)\nfig.suptitle('Error Terms', fontsize=20)                  # Plot heading \nplt.xlabel('y_test - y_pred', fontsize=18)                  # X-label\nplt.ylabel('Index', fontsize=16)                          # Y-label\n","a9b74768":"import xgboost as xgb\nfrom xgboost import XGBRegressor\n\nxgr = XGBRegressor()\nxgr.fit(X_train, y_train)","1f467bb1":"y_pred = xgr.predict(X_test)\ny_pred = pd.Series(y_pred)\n\ny_pred_XGB_default = y_pred\n\ny_test_0 = y_test.reset_index()\ny_test_0 = y_test_0['imdb_score']\n\npd.concat([y_pred,y_test_0],axis=1)","d3dae03c":"mse = mean_squared_error(y_test_0, y_pred)\nr_squared = r2_score(y_test_0, y_pred)\n\nprint('Mean_Squared_Error :' ,mse)\nprint('r_square_value :',r_squared)","ea4ef3ae":"# Actual and Predicted\n\nc = [i for i in range(1,301,1)] # generating index \nfig = plt.figure(figsize=(18,10)) \nplt.plot(c,y_test_0.head(300), color=\"blue\", linewidth=2.5, linestyle=\"-\") #Plotting Actual\nplt.plot(c,y_pred.head(300), color=\"red\",  linewidth=2.5, linestyle=\"-\") #Plotting predicted\nfig.suptitle('Actual vs Predicted', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                               # X-label\nplt.ylabel('IMDB score', fontsize=16)                       # Y-label","222011f8":"# Plotting the error terms to understand the distribution.\n\nfig = plt.figure(figsize=(18,10))\nsns.distplot((y_test_0 - y_pred),bins=50)\nfig.suptitle('Error Terms', fontsize=20)                  # Plot heading \nplt.xlabel('y_test - y_pred', fontsize=18)                  # X-label\nplt.ylabel('Index', fontsize=16)                          # Y-label\n","547305e4":"from sklearn.model_selection import GridSearchCV\n\nparameters = {'objective':['reg:linear'],\n              'learning_rate': [0.03, 0.05, 0.1, 0.5, 0.8],\n              'max_depth': [5, 6, 8],\n              'min_child_weight': [2,4],\n              'silent': [1],\n              'subsample': [0.5,0.7,1],\n              'colsample_bytree': [0.7,1],\n              'n_estimators': [200]}\n\nxgb_grid = GridSearchCV(xgr,\n                        parameters,\n                        cv = 7,\n                        n_jobs = 10,\n                        verbose=True)\n\nxgb_grid.fit(X_train, y_train)\n\nprint(xgb_grid.best_score_)\nprint(xgb_grid.best_params_)","d9a18a60":"y_pred = xgb_grid.predict(X_test)\ny_pred = pd.Series(y_pred)\n\ny_pred_XGB_grid = y_pred\n\ny_test_0 = y_test.reset_index()\ny_test_0 = y_test_0['imdb_score']\n\npd.concat([y_pred,y_test_0],axis=1)","6c7e9f4b":"mse = mean_squared_error(y_test_0, y_pred)\nr_squared = r2_score(y_test_0, y_pred)\n\nprint('Mean_Squared_Error :' ,mse)\nprint('r_square_value :',r_squared)","5a722b2a":"# Actual and Predicted\n\nc = [i for i in range(1,301,1)] # generating index \nfig = plt.figure(figsize=(18,10)) \nplt.plot(c,y_test_0.head(300), color=\"blue\", linewidth=2.5, linestyle=\"-\") #Plotting Actual\nplt.plot(c,y_pred.head(300), color=\"red\",  linewidth=2.5, linestyle=\"-\") #Plotting predicted\nfig.suptitle('Actual vs Predicted', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                               # X-label\nplt.ylabel('IMDB score', fontsize=16)                       # Y-label","2240376f":"# Plotting the error terms to understand the distribution.\n\nfig = plt.figure(figsize=(18,10))\nsns.distplot((y_test_0 - y_pred),bins=50)\nfig.suptitle('Error Terms', fontsize=20)                  # Plot heading \nplt.xlabel('y_test - y_pred', fontsize=18)                  # X-label\nplt.ylabel('Index', fontsize=16)                          # Y-label\n","7b4ccdda":"predictions = pd.concat([pd.Series(y_pred_lm_default),y_pred_lm_rfe.reset_index()[0],y_pred_lasso,y_pred_XGB_default,y_pred_XGB_grid,y_test_0],axis=1)\npredictions.rename(columns={0:\"LinearReg\",1:\"LinearReg_RFE\",2:\"LassoReg\",3:\"XGBoost\",4:\"XGBoost_grid\",\"imdb_score\":\"Actual IMDB Score\"})","0450a489":"y_pred_lm_rfe.reset_index()[0]","80889db1":"#### For Lasso Regression we see that the errors lie between plus minus 3 points also, but is narrower hence less error.\n#### The majority error is between plus minus 1, which is a pretty good prediction","a48eb2cd":"### Correlation and further EDA","2ab52f34":"#### With XGBoost default parameters itself we can see the error-term has become narrower which denotes a better prediction","d4caf5c8":"### XGBoost Regressor","f4ca1d5e":"### Outlier Treatment","19b3d563":"#### Hence we see that the errors lie between plus minus 3 points.\n#### The majority error is between plus minus 1, which is a pretty good prediction","1e6b05e1":"#### above we can see that adjusted r square is 0.523, which is not a very good score\n#### we can also see by p-value that there are quite many insignificant variables\n#### let's check the Variance Inflation Factor","27471362":"#### Missing Value Imputation or Row Removal","dcfb7af7":"## Prediction Comparisions of Different Models with Actual values","3b40b638":"###### We can observe above that there are a lot of missing data for \"gross\" (appx. 2700 i.e about 50%)\n\nThere are missing data for other fields too (approx estimation below)\n- budget : 500\n- director_name : 500\n- director_facebook_likes : 500\n- plot_keywords : 100\n- num_user_for_reviews : 15\n- language : 15\n- country : 3\n- and so on ...","d716e2da":"#### We can see some very high correlations between the cast_total_facebook_likes and actor_1_facebook_likes, actor_2_facebook_likes ,actor_3_facebook_likes \n#### Also votes and reviews (both critic and user) have very high correlations\n#### There are a lot of high correlations among genres as well\n#### There are also some negative correlations among genres","5d8f0672":"There are some fields with a lot of unique values.\n\nSome of them are obvious like movie_title and movie_imdb_link\n\nActor names are also quite unique showing very high variance in actors\n","8baea2d9":"#### Splitting the Data for Training and Testing","b9d31ca2":"## Model Building","34925b6a":"#### We will now use Recursive Feature Elimination to select 20 best features","120e866d":"### Linear Regresssion","e9f2a9e0":"#### Above we can see the (Predicted values in 'red') and (Actual values in 'blue')","b75b4e71":"### Data Understanding and Cleaning","587b5b25":"### Data Import","fd53437d":"### Lasso Regression","675dd393":"The idea is to predict the ratings for a movie based on multipe features scraped from sites like IMDB.com, www.the-numbers.com and also some image processing to count number of faces in posters.\n\nThe web scraping has been done seperately (pretty much like https:\/\/github.com\/sundeepblue\/movie_rating_prediction) and I will add that part in future in github.\n\nThis notebook only contains the analysis, model building and prediction parts, based on the final consolidated data in the csv file movie_metadata.csv\n\n**Your feedback is valuable and will be appreciated."}}