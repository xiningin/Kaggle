{"cell_type":{"8d2a1ce9":"code","41d51286":"code","3050cdb4":"code","b2e80f79":"code","c4d114bf":"code","ede41bc1":"code","e0163334":"code","0b70da70":"code","b615ecdc":"code","f19ab555":"code","45b4511d":"code","648ba918":"code","ea39519c":"code","1ebcb9d0":"markdown","f0023292":"markdown","b632c51e":"markdown","695bc0e3":"markdown","2d223f09":"markdown"},"source":{"8d2a1ce9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport random\n\nfrom matplotlib.lines import Line2D\n\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.ensemble import IsolationForest\n\n\nimport optuna\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=UserWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","41d51286":"# Read the data\ntrain = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv', index_col='id')\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv', index_col='id')\nsample = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv', index_col='id')","3050cdb4":"print(\"(train, test) na --> \",(train.isna().sum().sum(), test.isna().sum().sum()))","b2e80f79":"X = train.drop(columns = \"claim\").isna().astype('int')\ny = train.claim\nX_test = test.isna().astype('int')","c4d114bf":"def train_model_optuna_xgb(trial, X_train, X_valid, y_train, y_valid):\n    \"\"\"\n    A function to train a model using different hyperparamerters combinations provided by Optuna. \n    Loss of validation data predictions is returned to estimate hyperparameters effectiveness.\n    \"\"\"\n    preds = 0\n       \n    #A set of hyperparameters to optimize by optuna\n    xgb_params = {\n                 \"n_estimators\": trial.suggest_categorical('n_estimators', [10000]),\n                 \"learning_rate\": trial.suggest_float('learning_rate', 0.01, 0.8),\n                 \"subsample\": trial.suggest_float('subsample', 0.5, 0.95),\n                 \"colsample_bytree\": trial.suggest_float('colsample_bytree', 0.5, 0.95),\n                 \"max_depth\": trial.suggest_int(\"max_depth\", 5, 16),\n                 \"booster\": trial.suggest_categorical('booster', [\"gbtree\"]),\n                 \"tree_method\": trial.suggest_categorical('tree_method', [\"gpu_hist\"]),\n                 \"reg_lambda\": trial.suggest_float('reg_lambda', 2, 100),\n                 \"reg_alpha\": trial.suggest_float('reg_alpha', 1, 50),\n                 \"random_state\": trial.suggest_categorical('random_state', [42]),\n                 \"n_jobs\": trial.suggest_categorical('n_jobs', [4]),\n                    }\n\n    # Model loading and training\n    model = XGBClassifier(**xgb_params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              verbose=False)\n    \n    print(f\"Number of boosting rounds: {model.best_iteration}\")\n    oof = model.predict(X_valid)\n    oof[oof<0] = 0\n    \n    return np.sqrt(mean_squared_error(y_valid, oof))","ede41bc1":"%%time\n\nskf = StratifiedKFold(n_splits=6, shuffle=True, random_state=42)\n\nfor fold, (train_indicies, valid_indicies) in enumerate(skf.split(X,y)):\n    \n    X_train, X_valid = X.loc[train_indicies], X.loc[valid_indicies]\n    y_train, y_valid = y.loc[train_indicies], y.loc[valid_indicies]\n\n# Setting optuna verbosity to show only warning messages\n# If the line is uncommeted each iteration results will be shown\noptuna.logging.set_verbosity(optuna.logging.WARNING)\ntime_limit = 3600 * 2\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(lambda trial: train_model_optuna_xgb(trial, \n                                                X_train, \n                                                X_valid,\n                                                y_train, \n                                                y_valid),\n               n_trials = 200,\n               timeout=time_limit\n              )\n # Showing optimization results\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial parameters:', study.best_trial.params)\nprint('Best score:', study.best_value)","e0163334":"xgb_params = study.best_params\n\n# xgb_params = {'n_estimators': 10000, \n#               'learning_rate': 0.08625196792060146, \n#               'subsample': 0.5959773829663169, \n#               'colsample_bytree': 0.7603045913120982, \n#               'max_depth': 7, 'booster': 'gbtree', \n#               'tree_method': 'gpu_hist', # comment this line if you don't have a GPU\n#               'reg_lambda': 74.60593770387143, \n#               'reg_alpha': 33.38858560681472, \n#               'random_state': 42, \n#               'n_jobs': 4}\nprint(xgb_params)","0b70da70":"%%time\n\nfrom sklearn.metrics import roc_auc_score\n\n\ndef strati_fit(X, y, X_test, \n               splits=10, random_state=42,\n               model = XGBClassifier(**xgb_params)):\n    \n    splits = splits\n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=random_state)\n    oof_preds = np.zeros((X.shape[0],))\n    preds = 0\n    model_fi = 0\n    total_mean_rmse = 0\n    total_mean_roc_auc_score = 0\n    \n    \n    for fold, (train_indicies, valid_indicies) in enumerate(skf.split(X,y)):\n\n        X_train, X_valid = X.loc[train_indicies], X.loc[valid_indicies]\n        y_train, y_valid = y.loc[train_indicies], y.loc[valid_indicies]\n        print(fold, f\"X_train = {X_train.shape} - y_train: {y_train.shape}\")\n        print(fold, f\"X_valid = {X_valid.shape} - y_valid: {y_valid.shape}\")\n        \n        model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"auc\",\n              early_stopping_rounds=100,\n              verbose=False)\n        # print(\"fitted\")\n        preds += (model.predict_proba(X_test))[:,1] \/ splits\n        # print(preds.shape)\n        # print(\"preds ok\")\n        model_fi += model.feature_importances_\n        # print(\"model_fi ok\")\n        oof_preds[valid_indicies] = model.predict_proba(X_valid)[:,1]\n        oof_preds[oof_preds < 0] = 0\n    #     fold_rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(np.array(y_valid).reshape(-1,1)), y_scaler.inverse_transform(np.array(oof_preds[valid_idx]).reshape(-1,1))))\n        fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_indicies]))\n        fold_roc_auc_score = roc_auc_score(y_valid, oof_preds[valid_indicies])\n        \n        print(f\"\/nFold {fold} ROC AUC Score: {fold_roc_auc_score}\")\n        \n        print(f\"Fold {fold} RMSE: {fold_rmse}\")\n        total_mean_rmse += fold_rmse \/ splits\n        total_mean_roc_auc_score += fold_roc_auc_score \/ splits\n    return preds, model_fi, total_mean_rmse, total_mean_roc_auc_score,oof_preds\n\n\npreds, model_fi, total_mean_rmse, total_mean_roc_auc_score, oof_preds = strati_fit(X, y, X_test, \n       splits=10, model = XGBClassifier(**xgb_params))\n\n\nstudy = model_fi, total_mean_rmse, total_mean_roc_auc_score","b615ecdc":"model_fi_df = pd.DataFrame()\nmodel_fi_df[\"id\"] = X.columns\nmodel_fi_df[\"fi\"] = model_fi\nprint(model_fi_df.sort_values(\"fi\", ascending = False).head(15))\nbest_fi = model_fi_df.sort_values(\"fi\", ascending = False).head(15).id.to_list()\nprint(best_fi)","f19ab555":"oof_preds","45b4511d":"X_predicted","648ba918":"# # xgb predicting X for future tests\n# X_predicted = pd.DataFrame()\n# X_predicted[\"id\"] = X.index\n# X_predicted[\"claim\"] = oof_preds\n\n# X_predicted.to_csv('X_predicted.csv', index=False, header=predictions.columns)\n# X_predicted.head()","ea39519c":"# xgb public Score untuned and fast parameters: 0.76723\npredictions = pd.DataFrame()\npredictions[\"id\"] = test.index\npredictions[\"claim\"] = preds\n\npredictions.to_csv('submission_only_Na.csv', index=False, header=predictions.columns)\npredictions.head()","1ebcb9d0":"# Libraries and Data import","f0023292":"# NA values in train and test","b632c51e":"Quite a lot Na Values to handle. We need to handle all the NA. \nWe have studied them in the EDA session: https:\/\/www.kaggle.com\/sgiuri\/sep21tp-eda-na-handle-xgbc\n","695bc0e3":"[...]\n","2d223f09":"## XGBC without NA handling"}}