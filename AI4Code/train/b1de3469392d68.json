{"cell_type":{"b331a620":"code","b4ddb759":"code","4fa7f894":"code","6fc9b1fc":"code","29ce5e14":"code","e8701fcf":"code","25d23e8e":"code","b17aa8b6":"code","fe920225":"code","6767e443":"code","b436de27":"code","18eba9dd":"code","144bd5f7":"code","6dff7571":"code","791e9bae":"code","3898beb4":"code","0cac0a68":"code","3db7f4da":"code","45ad9f88":"code","011dd55c":"code","c252a57f":"code","610faca2":"code","6d66e138":"code","d29a9670":"code","5609da68":"code","189b135a":"code","cf1d4cdc":"code","7b1c1979":"markdown","d22277c1":"markdown","96e73b3e":"markdown","993a3783":"markdown","b126ed1d":"markdown","9096b743":"markdown","4e67738d":"markdown","f6297117":"markdown","15537917":"markdown","9e002c7e":"markdown","5968211d":"markdown","2bb03129":"markdown","d81c17da":"markdown","5046c694":"markdown","a542ad7f":"markdown","fcaf10e6":"markdown","0f180d87":"markdown","48ea6c4a":"markdown","de4b9b67":"markdown","e0b1c6f8":"markdown","1653bd17":"markdown","9cd34cbe":"markdown","41ccd0a7":"markdown","01b29a3f":"markdown","19ded8f5":"markdown","ba346a4f":"markdown","9d193ad6":"markdown"},"source":{"b331a620":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b4ddb759":"# Import relevant libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom math import sqrt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","4fa7f894":"data = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')\ndata.head()","6fc9b1fc":"data.info()","29ce5e14":"data.describe()","e8701fcf":"#Find the number of unique entries in each column\ndata.nunique()","25d23e8e":"continuous_variables = ['price', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']\ndiscrete_variables = ['yr_built', 'yr_renovated']\nnominal_variables = ['lat', 'long', 'zipcode']\nordinal_variables = ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade']","b17aa8b6":"sns.set_style(\"darkgrid\")\nplt.figure(figsize =(8,5))\nsns.distplot(data['price'], axlabel = 'Price')","fe920225":"print('Skewness : %f' % data['price'].skew())\nprint('Kurtosis : %f' % data['price'].kurt())","6767e443":"sns.pairplot(data[continuous_variables], height = 2 ,kind ='scatter',diag_kind='kde')               ","b436de27":"fig, ax = plt.subplots(7, 3, figsize=(15,30))\n\nfor i, el in enumerate(ordinal_variables):\n    feature_count = data[el].value_counts()\n    sns.set_style(\"darkgrid\")\n    sns.countplot(x=el, data=data,  ax=ax[i,0])\n    sns.boxplot(x=el, y= 'price',data=data, ax=ax[i,1])\n    sns.regplot(x=el, y= 'price',data=data,  ax=ax[i,2])\n    \nplt.show()    ","18eba9dd":"df1 = data.copy() \ndf1.drop(['id','date'], axis = 1, inplace=True)\ndf1['yrs_old_renovated'] = np.where(df1['yr_renovated']!= 0, 2015 - df1['yr_renovated'], 2015 - df1['yr_built'])\ndf1['yrs_old_bins'] = pd.cut(x = df1['yrs_old_renovated'], bins = [-1, 20, 40, 60, 80, 100, 120])\ndf1['price_bins'] = pd.cut(x = df1['price'], bins = [0, 1e6, 2e6, 3e6, 4e6, 5e6, 6e6, 7e6, 8e6])\ndf1.head()     ","144bd5f7":"fig, ax = plt.subplots(ncols=2, figsize=(18,5))\nsns.boxplot(x='yrs_old_bins', y= 'price',data=df1, ax=ax[0])\nsns.countplot(x='yrs_old_bins', data=df1, ax=ax[1])\nplt.show()","6dff7571":"data[(data['bedrooms'] == 33)]","791e9bae":"df1.drop(df1[df1['bedrooms'] == 33].index, axis = 0, inplace = True)","3898beb4":"#plt.figure(figsize=(10,10))\n#sns.scatterplot(x='long', y ='lat',data=df1,  sizes = (50, 300), style = 'price_bins', \n               # hue = 'price_bins', alpha = 0.4,   palette='bright')","0cac0a68":"plt.figure(figsize=(20,15))\ng = sns.pairplot(data=df1[['long','lat','price_bins']], hue='price_bins', corner=True )","3db7f4da":"features = continuous_variables +  ordinal_variables \nk= len(features)\ncols = df1[features].corr().nlargest(k,'price')['price'].index\ncm = np.corrcoef(data[cols].values.T)\nmask = np.zeros_like(df1[cols].corr())\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(15, 10))\n    ax = sns.heatmap(cm, cmap='viridis', mask=mask, vmax=.7, linewidths=0.01, annot = True, square=True, \n                    linecolor=\"white\",xticklabels = cols.values ,annot_kws = {'size':12},yticklabels = cols.values)","45ad9f88":"selected_features = ['sqft_living', 'grade', 'sqft_living15', 'bathrooms', 'view', 'sqft_basement', 'bedrooms',\n                     'waterfront', 'floors', 'long', 'lat']\ntarget = ['price']\nX = data[selected_features]\ny = np.ravel(data[target])","011dd55c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=100)","c252a57f":"regressor = LinearRegression()\nregressor.fit(X_train, y_train)\ny_prediction = regressor.predict(X_test)\nRMSE = sqrt(mean_squared_error(y_true = y_test, y_pred = y_prediction))\nmae = mean_absolute_error(y_test, y_prediction)\nr2score = r2_score(y_test, y_prediction)\n            \nprint('RMSE:', RMSE)\nprint('MAE:' ,mae )\nprint('R2score:', r2score)","610faca2":"sns.regplot(x=y_test, y=  y_prediction)\nplt.xlabel('True Values [Price]')\nplt.ylabel('Predictions [Price]')\nplt.title('Multiple Linear Regression predictions for the test data')","6d66e138":"max_depth = [5,10,15,20,25,30,35,40,45,50]\nRMSE = []\nmae = []\nr2score = []\nfor n in max_depth:\n    regressor = DecisionTreeRegressor(max_depth = n, random_state = 100)\n    regressor.fit(X_train, y_train)\n    y_prediction = regressor.predict(X_test)\n    RMSE.append(sqrt(mean_squared_error(y_true = y_test, y_pred = y_prediction)))\n    mae.append(mean_absolute_error(y_test, y_prediction))\n    r2score.append(r2_score(y_test, y_prediction))\n    \nDTRegressor_results = pd.DataFrame({'max_depth':max_depth,'RMSE':RMSE, 'MAE': mae, 'r2score':r2score})\n\nprint(DTRegressor_results.round(2))\n\nfig, ax1 = plt.subplots()\nax1.plot(DTRegressor_results['max_depth'], DTRegressor_results['r2score'], 'b--')\nax1.set_xlabel('max_depth')\nax1.set_ylabel('r2score')\nax1.legend(['r2score'], loc =\"upper right\")\nax2 = ax1.twinx()\nax2.plot(DTRegressor_results['max_depth'], DTRegressor_results['MAE'], 'r--')\nax2.set_ylabel('MAE')\nax2.legend(['MAE'],loc =\"upper center\") \nplt.show()","d29a9670":"sns.regplot(x=y_test, y=  DecisionTreeRegressor(max_depth = 10, random_state = 100).fit(X_train, y_train).predict(X_test))\nplt.xlabel('True Values [Price]')\nplt.ylabel('Predictions [Price]')\nplt.title('Decision Tree Regression predictions for the test data')","5609da68":"n_estimators = [5,10,15,20,25,30, 35, 40, 45, 50]\nRMSE = []\nmae = []\nr2score = []\nfor n in n_estimators:\n    regressor = RandomForestRegressor(n_estimators = n, random_state = 100)\n    regressor.fit(X_train, y_train)\n    y_prediction = regressor.predict(X_test)\n    RMSE.append(sqrt(mean_squared_error(y_true = y_test, y_pred = y_prediction)))\n    mae.append(mean_absolute_error(y_test, y_prediction))\n    r2score.append(r2_score(y_test, y_prediction))\n    \nRFRegression_results = pd.DataFrame({'n_estimators':n_estimators,'RMSE':RMSE, 'MAE': mae, 'r2score':r2score})\n\nprint(RFRegression_results.round(3))\n\nfig, ax1 = plt.subplots()\nax1.plot(RFRegression_results['n_estimators'], RFRegression_results['r2score'], 'b--')\nax1.set_xlabel('max_depth')\nax1.set_ylabel('r2score')\nax1.legend(['r2score'], loc =\"upper right\")\nax2 = ax1.twinx()\nax2.plot(RFRegression_results['n_estimators'], RFRegression_results['MAE'], 'r--')\nax2.set_ylabel('MAE')\nax2.legend(['MAE'],loc =\"upper center\") \nplt.show()","189b135a":"sns.regplot(x=y_test, y= RandomForestRegressor(n_estimators = 50, random_state = 0).fit(X_train, y_train).predict(X_test))\nplt.xlabel('True Values [Price]')\nplt.ylabel('Predictions [Price]')","cf1d4cdc":"reg1 = LinearRegression()\nreg2 = DecisionTreeRegressor(max_depth = 10,  random_state = 100)\nreg3 = RandomForestRegressor(n_estimators = 40, random_state = 100)\n\nreg1.fit(X_train, y_train)\nreg2.fit(X_train, y_train)\nreg3.fit(X_train, y_train)\n\npred1 = reg1.predict(X_test[:20])\npred2 = reg2.predict(X_test[:20])\npred3 = reg3.predict(X_test[:20])\n\n\nplt.figure(figsize=(20,5))\nplt.plot(pred1, 'gd', label='LinearRegression')\nplt.plot(pred2, 'b^', label='DecisionTreeRegressor')\nplt.plot(pred3, 'ys', label='RandomForestRegressor')\nplt.plot(y_test[:20], 'ro', label = 'True value')\n\nplt.tick_params(axis='x', which='both', bottom=False, top=False,\n                labelbottom=False)\nplt.ylabel('predicted')\nplt.xlabel('training samples')\nplt.legend(loc=\"best\")\nplt.title('Regressor predictions and true value of 20 samples')\n\nplt.show()","7b1c1979":"Classify the variables into 4 categories.\n* Continuous variables: A numeric variable that takes any value between a certain set of real numbers.\n* Discrete variables: A numeric variable that can only take distinct and separate values.\n* Nominal variables:A categorical variable which has no order.\n* Ordinal variables: A categorical variable whose value can be logically ordered or ranked.","d22277c1":"## **Geographic location vs house price**","96e73b3e":"## Distribution of target variable : sales price","993a3783":"## 1. Multiple Linear Regression\nMultiple linear regression (MLR) attempts to model a linear relationship between the several explanatory (independent) variables and the response (dependent) variable. Here we use all the selected independent training variables to predict the house price.","b126ed1d":"Dataframe has 21613 rows and 21 columns. There aren't any missing values in any column.","9096b743":"There is not much change in the median house price with aging. So we will discard yr_built and yr_renovated features from the training data.","4e67738d":"It looks like there is some error in the data as it is illogical to have a 33 bedroom single story house on 6000 sqft_lot and with only 1.75 bathrooms. So I'm dropping this row.","f6297117":"## Observations\n* Bedrooms & Bathrooms:The median house price is going up with increase in the number of bedrooms (upto 7) and bathrooms (upto 5). Thereafter it doesn't show a linear trend.\n* Floors: The median house price increases with an increase in the number of floors (upto 2.5)\n* Waterfront: The houses with waterfront are priced higher.\n* View: The better the view, the higher the price.\n* Condition: The median price for condition 3, 4 and 5 remains almost the same, though price for condition 1 & 2 houses are slightly lower.\n* Grade: The median house price increases almost exponentially with increase in grade.","15537917":"The above scatter plot is almost the shape of King County. It can be seen that higher priced houses are located in some specific regions, especially near the coasts. Specifically, the high priced houses are located between latitudes of  $47.5^{o}$   and  $47.7^{o}$   and longitudes of  $-122.0^{o}$   and  $\u2212122.4^{o}$  . This information may be helpful for a homebuyer when making a purchase decision. This also indicates that geographical location (latitude, longitude) is a key factor that decides house price.","9e002c7e":"The above graph shows the house price predictions with the different regressor models used and the actual price for the first 20 samples in the test dataset.\n\nThe highest r2score (0.877) is obtained with Random Forest Regression model.","5968211d":"# **Comparison of Regression methods for House Price Prediction**","2bb03129":"## Feature selection\nHere we select the variables which are highly correlated with our target variable, price. Let's choose the top 10 varaibles - 'sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms', 'view', 'sqft_basement', 'bedrooms', 'waterfront', 'floors'.\n\n'sqft_living' and 'sqft_above' are highly correlated with a correlation coefficient of 0.88. So keeping one of this variable in the training set is sufficient. 'sqft_living' has a higher correlation with 'price' than 'sqft_above'. Therefore, we will keep 'sqft_living' in the training feature. Also, we will add the geographical location parameters, 'lat' and 'long' in the training features.","d81c17da":"### True Value vs. Predicted value for Multiple Linear Regression model","5046c694":"### True Value vs. Predicted value for the best fitting Decision Tree Regression model","a542ad7f":"Skewness is the degree of distortion from the symmetrical bell curve or the normal distribution. The above distribution curve shows a positive skewness. ie, the peak of the distribution curve is less than the average value. This may be an indication that many houses are sold at less than the average value. Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. High Kurtosis (34.585540) in this case may be because of outliers present in the data.","fcaf10e6":"## 2. Decision Tree Regression","0f180d87":"## 3. Random Forest Regression","48ea6c4a":"### True Value vs. Predicted value for the best fitting Random Forest Regression model","de4b9b67":"## Conclusions\n* Among the models studied in this work, Random Forest regression model gives highest accuracy in house price prediction.\n* High valued houses are located between latitudes of 47.5\u02da and 47.7\u02da and longitudes of \u2212122.0\u02da and \u2212122.4\u02da. This may be an ideal location to invest in King County. However, the investment decision should be based on one\u2019s financial provisions and aspirations.\n* The most sold out homes are either single or two storied houses with 3 or 4 bedrooms. Better view and having waterfront raises the value of the house. It would be good for a builder to keep these in mind while planning a new project.","e0b1c6f8":"The best fitting model in this case has an r2score of 0.80 and MAE of 88514.64 with max_depth = 10.","1653bd17":"The objective of this work is to predict house prices in King County, USA using regression models and to identify the best fitting model. Three regression models are used in the study: Multiple Linear Regression, Decision\nTree Regression and Random Forest Regression. The best regression model is identified by comparing the r2score for the different models. ","9cd34cbe":"Let's have a look at the 33 bedroom house and compare it with mean and median values of the dataset.","41ccd0a7":"## Correlation between variables","01b29a3f":"## Comparing the different regressor models","19ded8f5":"## House age vs house price","ba346a4f":"The best fitting model in this case has an r2score of 0.877 and MAE of 71879.346 with n_estimators = 40.","9d193ad6":"Almost all the continuous variables show a positive skewness. Variables 'sqft_above' and 'sqft_living' are almost linearly related."}}