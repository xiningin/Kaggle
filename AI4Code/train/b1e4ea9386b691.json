{"cell_type":{"e50355c2":"code","a60d7b24":"code","f7b2d562":"code","b3897d30":"code","17432e12":"code","3bff9e33":"code","85d41543":"code","e47ca09a":"code","ca622ff8":"code","4f873c5e":"code","174e1036":"code","277338da":"code","5664c8b9":"code","450e4989":"code","c8470e41":"code","f23175bb":"code","c89cf112":"code","b068b8ef":"code","97e98213":"code","850253b3":"code","9a287b9f":"code","90aef13c":"code","165ad1a5":"code","48c97723":"code","cd9e31df":"code","909e3d48":"code","3cfd2e6e":"code","255fd704":"code","598e7246":"code","23ca8730":"code","60e34910":"code","518c9077":"code","576af487":"code","ce85690e":"code","29f367ce":"code","4e839072":"code","799cb0cf":"code","88e56fa5":"code","222c867e":"code","7ea8a835":"markdown","3a409420":"markdown","1161cdb4":"markdown","4202a180":"markdown","aff51ab6":"markdown","3142765a":"markdown","98da9c12":"markdown","43b79e18":"markdown","1ce7b7f3":"markdown","ada84e51":"markdown","e02beb69":"markdown","bb199a64":"markdown","eeff83f0":"markdown","3a7b5860":"markdown","b0d25d07":"markdown","5abaa2ae":"markdown","b02c84f4":"markdown","d18ea672":"markdown","10a891b9":"markdown","768bd5e0":"markdown","0eeaeba8":"markdown","a586b84d":"markdown","838a720b":"markdown","c3cc12f9":"markdown","8a09a2c4":"markdown"},"source":{"e50355c2":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport os\nfrom tqdm.notebook import tqdm","a60d7b24":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","f7b2d562":"path = \"..\/input\/m5-forecasting-accuracy\"\n\ncalendar = pd.read_csv(os.path.join(path, \"calendar.csv\"))\nselling_prices = pd.read_csv(os.path.join(path, \"sell_prices.csv\"))\nsample_submission = pd.read_csv(os.path.join(path, \"sample_submission.csv\"))","b3897d30":"sales = pd.read_csv(os.path.join(path, \"sales_train_validation.csv\"))","17432e12":"calendar.head()","3bff9e33":"for i, var in enumerate([\"year\", \"weekday\", \"month\", \"event_name_1\", \"event_name_2\", \n                         \"event_type_1\", \"event_type_2\", \"snap_CA\", \"snap_TX\", \"snap_WI\"]):\n    plt.figure()\n    g = sns.countplot(calendar[var])\n    g.set_xticklabels(g.get_xticklabels(), rotation=45)\n    g.set_title(var)","85d41543":"from sklearn.preprocessing import OrdinalEncoder\n\ndef prep_calendar(df):\n    df = df.drop([\"date\", \"weekday\"], axis=1)\n    df = df.assign(d = df.d.str[2:].astype(int))\n    df = df.fillna(\"missing\")\n    cols = list(set(df.columns) - {\"wm_yr_wk\", \"d\"})\n    df[cols] = OrdinalEncoder(dtype=\"int\").fit_transform(df[cols])\n    df = reduce_mem_usage(df)\n    return df\n\ncalendar = prep_calendar(calendar)","e47ca09a":"calendar.head()","ca622ff8":"selling_prices.info()","4f873c5e":"def prep_selling_prices(df):\n    gr = df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n    df[\"sell_price_rel_diff\"] = gr.pct_change()\n    df[\"sell_price_roll_sd7\"] = gr.transform(lambda x: x.rolling(7).std())\n    df[\"sell_price_cumrel\"] = (gr.shift(0) - gr.cummin()) \/ (1 + gr.cummax() - gr.cummin())\n    df = reduce_mem_usage(df)\n    return df\n\nselling_prices = prep_selling_prices(selling_prices)","174e1036":"selling_prices.tail()","277338da":"sales.head()","5664c8b9":"for i, var in enumerate([\"state_id\", \"store_id\", \"cat_id\", \"dept_id\"]):\n    plt.figure()\n    g = sns.countplot(sales[var])\n    g.set_xticklabels(g.get_xticklabels(), rotation=45)\n    g.set_title(var)","450e4989":"sales.item_id.value_counts()","c8470e41":"def reshape_sales(df, drop_d = None):\n    if drop_d is not None:\n        df = df.drop([\"d_\" + str(i + 1) for i in range(drop_d)], axis=1)\n    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n                 var_name='d', value_name='demand')\n    df = df.assign(d=df.d.str[2:].astype(\"int16\"))\n    return df\n\nsales = reshape_sales(sales, 1000)","f23175bb":"sales.head()","c89cf112":"sns.countplot(sales[\"demand\"][sales[\"demand\"] <= 10]);","b068b8ef":"def prep_sales(df):\n    df['lag_t28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n    df['rolling_mean_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n    df['rolling_mean_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n    df['rolling_mean_t60'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(60).mean())\n    df['rolling_mean_t90'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n    df['rolling_mean_t180'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n    df['rolling_std_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n    df['rolling_std_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n\n    # Remove rows with NAs except for submission rows. rolling_mean_t180 was selected as it produces most missings\n    df = df[(df.d >= 1914) | (pd.notna(df.rolling_mean_t180))]\n    df = reduce_mem_usage(df)\n\n    return df\n\nsales = prep_sales(sales)","97e98213":"sales.head()","850253b3":"sales = sales.merge(calendar, how=\"left\", on=\"d\")\ngc.collect()\nsales.head()","9a287b9f":"sales = sales.merge(selling_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\nsales.drop([\"wm_yr_wk\"], axis=1, inplace=True)\ngc.collect()\nsales.head()","90aef13c":"del selling_prices","165ad1a5":"sales.info()","48c97723":"cat_id_cols = [\"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]\ncat_cols = cat_id_cols + [\"wday\", \"month\", \"year\", \"event_name_1\", \n                          \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n\n# In loop to minimize memory use\nfor i, v in tqdm(enumerate(cat_id_cols)):\n    sales[v] = OrdinalEncoder(dtype=\"int\").fit_transform(sales[[v]])\n\nsales = reduce_mem_usage(sales)\nsales.head()\ngc.collect()","cd9e31df":"num_cols = [\"sell_price\", \"sell_price_rel_diff\", \"sell_price_roll_sd7\", \"sell_price_cumrel\",\n            \"lag_t28\", \"rolling_mean_t7\", \"rolling_mean_t30\", \"rolling_mean_t60\", \n            \"rolling_mean_t90\", \"rolling_mean_t180\", \"rolling_std_t7\", \"rolling_std_t30\"]\nbool_cols = [\"snap_CA\", \"snap_TX\", \"snap_WI\"]\ndense_cols = num_cols + bool_cols\n\n# Need to do column by column due to memory constraints\nfor i, v in tqdm(enumerate(num_cols)):\n    sales[v] = sales[v].fillna(sales[v].median())\n    \nsales.head()","909e3d48":"test = sales[sales.d >= 1914]\ntest = test.assign(id=test.id + \"_\" + np.where(test.d <= 1941, \"validation\", \"evaluation\"),\n                   F=\"F\" + (test.d - 1913 - 28 * (test.d > 1941)).astype(\"str\"))\ntest.head()\ngc.collect()","3cfd2e6e":"# Input dict for training with a dense array and separate inputs for each embedding input\ndef make_X(df):\n    X = {\"dense1\": df[dense_cols].to_numpy()}\n    for i, v in enumerate(cat_cols):\n        X[v] = df[[v]].to_numpy()\n    return X\n\n# Submission data\nX_test = make_X(test)\n\n# One month of validation data\nflag = (sales.d < 1914) & (sales.d >= 1914 - 28)\nvalid = (make_X(sales[flag]),\n         sales[\"demand\"][flag])\n\n# Rest is used for training\nflag = sales.d < 1914 - 28\nX_train = make_X(sales[flag])\ny_train = sales[\"demand\"][flag]\n                             \ndel sales, flag\ngc.collect()","255fd704":"np.unique(X_train[\"state_id\"])","598e7246":"import tensorflow as tf\nimport tensorflow.keras as keras\n\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dense, Input, Embedding, Dropout, concatenate, Flatten\nfrom tensorflow.keras.models import Model","23ca8730":"def create_model(lr=0.002):\n    tf.random.set_seed(173)\n\n    tf.keras.backend.clear_session()\n    gc.collect()\n\n    # Dense input\n    dense_input = Input(shape=(len(dense_cols), ), name='dense1')\n\n    # Embedding input\n    wday_input = Input(shape=(1,), name='wday')\n    month_input = Input(shape=(1,), name='month')\n    year_input = Input(shape=(1,), name='year')\n    event_name_1_input = Input(shape=(1,), name='event_name_1')\n    event_type_1_input = Input(shape=(1,), name='event_type_1')\n    event_name_2_input = Input(shape=(1,), name='event_name_2')\n    event_type_2_input = Input(shape=(1,), name='event_type_2')\n    item_id_input = Input(shape=(1,), name='item_id')\n    dept_id_input = Input(shape=(1,), name='dept_id')\n    store_id_input = Input(shape=(1,), name='store_id')\n    cat_id_input = Input(shape=(1,), name='cat_id')\n    state_id_input = Input(shape=(1,), name='state_id')\n\n    wday_emb = Flatten()(Embedding(7, 1)(wday_input))\n    month_emb = Flatten()(Embedding(12, 1)(month_input))\n    year_emb = Flatten()(Embedding(6, 1)(year_input))\n    event_name_1_emb = Flatten()(Embedding(31, 1)(event_name_1_input))\n    event_type_1_emb = Flatten()(Embedding(5, 1)(event_type_1_input))\n    event_name_2_emb = Flatten()(Embedding(5, 1)(event_name_2_input))\n    event_type_2_emb = Flatten()(Embedding(5, 1)(event_type_2_input))\n\n    item_id_emb = Flatten()(Embedding(3049, 3)(item_id_input))\n    dept_id_emb = Flatten()(Embedding(7, 1)(dept_id_input))\n    store_id_emb = Flatten()(Embedding(10, 1)(store_id_input))\n    cat_id_emb = Flatten()(Embedding(3, 1)(cat_id_input))\n    state_id_emb = Flatten()(Embedding(3, 1)(state_id_input))\n\n    # Combine dense and embedding parts and add dense layers. Exit on linear scale.\n    x = concatenate([dense_input, wday_emb, month_emb, year_emb, \n                     event_name_1_emb, event_type_1_emb, \n                     event_name_2_emb, event_type_2_emb, \n                     item_id_emb, dept_id_emb, store_id_emb,\n                     cat_id_emb, state_id_emb])\n    x = Dense(150, activation=\"tanh\")(x)\n    x = Dense(75, activation=\"tanh\")(x)\n    x = Dense(10, activation=\"tanh\")(x)\n    outputs = Dense(1, activation=\"linear\", name='output')(x)\n\n    inputs = {\"dense1\": dense_input, \"wday\": wday_input, \"month\": month_input, \"year\": year_input, \n              \"event_name_1\": event_name_1_input, \"event_type_1\": event_type_1_input,\n              \"event_name_2\": event_name_2_input, \"event_type_2\": event_type_2_input,\n              \"item_id\": item_id_input, \"dept_id\": dept_id_input, \"store_id\": store_id_input, \n              \"cat_id\": cat_id_input, \"state_id\": state_id_input}\n\n    # Connect input and output\n    model = Model(inputs, outputs)\n\n    model.compile(loss=keras.losses.mean_squared_error,\n                  metrics=[\"mse\"],\n                  optimizer=keras.optimizers.RMSprop(learning_rate=lr))\n    return model","60e34910":"model = create_model(0.0002)\nmodel.summary()\nkeras.utils.plot_model(model, 'model.png', show_shapes=True)","518c9077":"history = model.fit(X_train, \n                    y_train,\n                    batch_size=10000,\n                    epochs=30,\n                    shuffle=True,\n                    validation_data=valid)","576af487":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","ce85690e":"history.history[\"val_loss\"]","29f367ce":"model.save('model.h5')","4e839072":"pred = model.predict(X_test, batch_size=10000)","799cb0cf":"test[\"demand\"] = pred.clip(0)\nsubmission = test.pivot(index=\"id\", columns=\"F\", values=\"demand\").reset_index()[sample_submission.columns]\nsubmission = sample_submission[[\"id\"]].merge(submission, how=\"left\", on=\"id\")\nsubmission.head()","88e56fa5":"submission[sample_submission.id==\"FOODS_1_001_TX_2_validation\"].head()","222c867e":"submission.to_csv(\"submission.csv\", index=False)","7ea8a835":"#### Plot the evaluation metrics over epochs","3a409420":"#### Notes for modeling\n\n**Features**\n- \"dept_id\", \"item_id\", \"store_id\": Integer coding & embedding\n- lagged features derived from response: Numeric\n\n**Reshape**:\n- Reshape days as \"d\" from wide to long -> \"demand\" will be response variable\n\n**Merges**:\n1. Join calendar features by \"d\"\n2. Join selling prices by \"store_id\", \"item_id\" and (\"wm_yr_wk\" from calendar)\n\nComment: Submission dates: \"d_1914\" - \"d_1969\"","1161cdb4":"#### Notes for modeling\n\n**Features** deemed to be useful:\n\n- \"wday\", \"year\", \"month\" -> integer coding & embedding\n- \"event_name_1\", \"event_type_1\" -> integer coding & embedding\n- \"snap_XX\" -> numeric (they are dummies)\n\n**Reshape required**: No\n\n**Merge key(s)**: \"d\", \"wm_yr_wk\"","4202a180":"#### Notes for modeling\n\n**Features**:\n\n- sell_price and derived features -> numeric\n\n**Reshape**: No\n\n**Merge key(s)**: to sales data by store_id, item_id, wm_yr_wk (through calendar data)","aff51ab6":"### Selling prices\n\nContains selling prices for each store_id, item_id_wm_yr_wk combination.","3142765a":"## Prepare data for Keras interface","98da9c12":"# M5 Forecast: Keras with Categorical Embeddings V2\n\nThis notebook tries to model expected sales of product groups. Since many of the features are categorical, we use this example to show how embedding layers make life easy when dealing with categoric inputs for neural nets by skipping the step of making dummy variables by hand.\n\nData preprocessing and feature engineering is very similar (but not identical) to this [R kernel](https:\/\/www.kaggle.com\/mayer79\/m5-forecast-keras-embeddings-with-r) and uses ideas from the two excellent kernels [Very fst Model](https:\/\/www.kaggle.com\/ragnar123\/very-fst-model) and [M5 ForecasteR](https:\/\/www.kaggle.com\/kailex\/m5-forecaster-0-57330).\n\nTo gain an extra 3 GB of RAM, we do not use GPU acceleration for the training.","43b79e18":"#### Impute numeric columns","1ce7b7f3":"### Calendar data\n\nFor each date (covering both training and test data), we have access to useful calendar information.","ada84e51":"## Load data","e02beb69":"## Submission","bb199a64":"## Describe and prepare data\n\nWe will now go through all data sets and prepare them for modelling.","eeff83f0":"#### Reshaping\n\nWe now reshape the data from wide to long, using \"id\" as fixed and swapping \"d_x\" columns. Along this process, we also add structure for submission data and reduce data size.","3a7b5860":"### Imports","b0d25d07":"#### Separate submission data and reconstruct id columns","5abaa2ae":"### Architecture with embeddings","b02c84f4":"### Combine data sources","d18ea672":"### Ordinal encoding of remaining categoricals","10a891b9":"Derive some time related features:","768bd5e0":"#### Make training data","0eeaeba8":"### Sales data\n\nContains the number of sold items (= our response) as well as some categorical features.","a586b84d":"#### Add time-lagged features\n\nAdd some of the derived features from kernel https:\/\/www.kaggle.com\/ragnar123\/very-fst-model.","838a720b":"## The model","c3cc12f9":"### Calculate derivatives and fit model","8a09a2c4":"#### Distribution of the response"}}