{"cell_type":{"72df665e":"code","614971be":"code","597a9621":"code","3ee052cb":"code","70b31a1b":"code","61bac8ce":"code","756b973b":"code","d4a9ef66":"code","0fddedf2":"code","8d5eb1c2":"code","bf27295d":"code","0b1438b9":"code","3f304632":"code","384149ea":"code","59f0343e":"code","26dbb1d7":"code","99476b28":"code","5caf17c7":"code","3e3914b9":"code","8f16a795":"code","a3934103":"code","0818d215":"code","92446601":"code","c47e82d4":"code","be95c5c2":"code","a5608de6":"code","6058b671":"code","40739aa3":"code","87130b2d":"code","ab8560d5":"code","03a5e90b":"code","a0df860f":"code","4057d84b":"code","01cb9df2":"code","f38a16ca":"code","e64b5e6b":"code","6ad85988":"code","5e8963a8":"code","b9135514":"code","0f3ed115":"code","c5455858":"code","60de219e":"markdown","7c59b811":"markdown","fa1e5910":"markdown","2d5bcd17":"markdown","ce931988":"markdown","abf97bbf":"markdown","4e61eeb0":"markdown","a2b3cf00":"markdown","b8a572c1":"markdown","014a4ffd":"markdown","57407e8b":"markdown","2f4f81ff":"markdown","e192d35f":"markdown","4f0d785c":"markdown","709b5d1a":"markdown","c666b733":"markdown","0bc19238":"markdown","3e111640":"markdown","d5b8c07b":"markdown","55c81629":"markdown","caf8bad8":"markdown","7c11afe9":"markdown","94ec91fa":"markdown","68c2bb74":"markdown","c0c73102":"markdown","a94cee1b":"markdown"},"source":{"72df665e":"import sklearn\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge ,Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, PolynomialFeatures\nfrom sklearn.metrics import mean_squared_log_error as msle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\n%matplotlib inline","614971be":"train=pd.read_csv('..\/input\/bike-sharing-demand\/train.csv')\ntest=pd.read_csv('..\/input\/bike-sharing-demand\/test.csv')","597a9621":"train.head()","3ee052cb":"test.head()","70b31a1b":"train.drop(['casual','registered'],1,inplace=True)","61bac8ce":"train.info()","756b973b":"train.describe()","d4a9ef66":"#from data describtion we can extract categorcal data and numerical data","0fddedf2":"categorical_cols=['season','holiday','workingday','weather']\nnumerical_cols=['temp','atemp','humidity','windspeed']\nlabel='count'","8d5eb1c2":"train['datetime'] = pd.to_datetime(train['datetime'])\ntest['datetime'] = pd.to_datetime(test['datetime'])\ntrain['Month']=pd.DatetimeIndex(train['datetime']).month\ntest['Month']=pd.DatetimeIndex(test['datetime']).month\ntrain['Year']=pd.DatetimeIndex(train['datetime']).year\ntest['Year']=pd.DatetimeIndex(test['datetime']).year\ntrain['WeekDay']=pd.DatetimeIndex(train['datetime']).weekday\ntest['WeekDay']=pd.DatetimeIndex(test['datetime']).weekday\ntrain['Hour']=pd.DatetimeIndex(train['datetime']).hour\ntest['Hour']=pd.DatetimeIndex(test['datetime']).hour","bf27295d":"categorical_cols.extend(['Month','WeekDay','Hour'])","0b1438b9":"def encodetime(train,test,col,label):\n    d3=train[[col,label]].groupby(col).mean()\n    d3.sort_values(by='count',ascending=False)\n    plt.scatter(x=d3.index,y=d3['count'])\n    d3=d3.sort_values(by='count')\n    d3['w']=np.arange(train[col].nunique())\n    dic=dict(zip(d3.index,d3['w']))\n    train[col]=train[col].map(dic)\n    test[col]=test[col].map(dic)","3f304632":"encodetime(train,test,'Year',label)","384149ea":"encodetime(train,test,'Month',label)","59f0343e":"encodetime(train,test,'Hour',label)","26dbb1d7":"encodetime(train,test,'WeekDay',label)","99476b28":"def boxplot(x,y,**kwargs):\n    sns.boxplot(x=x,y=y)\n    x=plt.xticks(rotation=90)\nf=pd.melt(train,id_vars=['count'],value_vars=categorical_cols)\ng=sns.FacetGrid(f,col='variable',col_wrap=2,sharex=False)\ng.map(boxplot,'value','count')","5caf17c7":"sns.pairplot(train[[*numerical_cols,'count']])","3e3914b9":"f, ax = plt.subplots(figsize=(15, 15))\ncorr = train[[*numerical_cols,'count']].corr()\nsns.heatmap(corr,cmap=sns.diverging_palette(220, 10, as_cmap=True),square=True, ax=ax, annot = True)","8f16a795":"f, ax = plt.subplots(figsize=(15, 15))\ncorr = train.corr()\nsns.heatmap(corr,cmap=sns.diverging_palette(220, 10, as_cmap=True),square=True, ax=ax, annot = True)","a3934103":"sns.displot(train[label] , kde=True, height=8.27, aspect=11.7\/8.27)\nsns.displot(np.log(train[label]) , kde=True, height=8.27, aspect=11.7\/8.27)","0818d215":"def trans(x,l1=0.3,l2=0):\n    if l1!=0:\n        return ((x+l2)**l1-1)\/l1\n    else:\n        return np.log(x+l2)\ndef rev_trans(x,l1=0.3,l2=0):\n    return (x*l1+1)**(1\/l1)-l2\n\nz=train[label].apply(trans)   \nsns.displot(z , kde=True, height=8.27, aspect=11.7\/8.27)","92446601":"#using box cox transform on the label column","c47e82d4":"train","be95c5c2":"#train=pd.get_dummies(train,columns=['season','weather'])\n#test=pd.get_dummies(test,columns=['season','weather'])","a5608de6":"train","6058b671":"#removing atemp column as it has high correlation with temp column\nx=train.drop(['count','datetime','atemp'],1) \nxtest=test.drop(['datetime','atemp'],1)\ny=train['count']\nxt,xv,yt,yv=train_test_split(x,y,test_size=0.2,random_state=101)","40739aa3":"#redundunt function .. useful if you decide not to use label transformation function\ndef redun(x):\n    return x","87130b2d":"def mk_model_RF(xt1,xv1,yt,yv,md=None,func1=redun,func2=redun,mss=2,n_est=100,al=0):\n    \n    ytt=yt.apply(func1)\n    yvt=yv.apply(func2)\n    model=RandomForestRegressor(max_depth=md, random_state=0,min_samples_split=mss,n_estimators=n_est,ccp_alpha=al)\n    model.fit(xt1,ytt)\n    ypt=np.apply_along_axis(func2,arr=model.predict(xt1),axis=0)\n    ypv=np.apply_along_axis(func2,arr=model.predict(xv1),axis=0)\n    print('training r2:',r2_score(yt,ypt))\n    print('Validation r2:',r2_score(yv,ypv))\n    print('training rmsle:',np.sqrt(msle(yt,ypt)))\n    print('validation rmsle:',np.sqrt(msle(yv,ypv)))\n    return model","ab8560d5":"mk_model_RF(xt,xv,yt,yv)#without using label transformation","03a5e90b":"mk_model_RF(xt,xv,yt,yv,func1=trans,func2=rev_trans,n_est=400,md=20)#using label transformation","a0df860f":"def mk_model_xgb(xt,xv,yt,yv,func1=redun,func2=redun,lr=1,min_child_weight =25,colsample_bytree = 0.8,md=None):\n    model =XGBRegressor( colsample_bytree = colsample_bytree, learning_rate = lr,min_child_weight =min_child_weight, max_depth=md )\n    ytt=yt.apply(func1)\n    model.fit(xt,ytt)\n    ypt=np.apply_along_axis(func2,arr=model.predict(xt),axis=0)\n    ypv=np.apply_along_axis(func2,arr=model.predict(xv),axis=0)\n    print('training r2:',r2_score(yt,ypt))\n    print('Validation r2:',r2_score(yv,ypv))\n    print('training rmsle:',np.sqrt(msle(yt,ypt)))\n    print('validation rmsle:',np.sqrt(msle(yv,ypv)))\n    return model","4057d84b":"_=mk_model_xgb(xt,xv,yt,yv,func1=trans,func2=rev_trans,lr=0.2,min_child_weight =20,colsample_bytree = 0.8,md=20)","01cb9df2":"model=XGBRegressor(colsample_bytree = 0.8, learning_rate = 0.2,min_child_weight =20, max_depth=20).fit(x,y.apply(trans))","f38a16ca":"xtest","e64b5e6b":"model.predict(xtest)","6ad85988":"yp=np.apply_along_axis(rev_trans,arr=model.predict(xtest),axis=0)","5e8963a8":"plt.hist(yp)","b9135514":"test['count']=yp","0f3ed115":"test","c5455858":"test[['datetime', 'count']].to_csv('\/kaggle\/working\/submission.csv', index=False)","60de219e":"## using label transformation gives better rmsle","7c59b811":"# Data describtion","fa1e5910":"# Transforming Label distribution","2d5bcd17":"# Spliting Data into Train and Validation","ce931988":"# EDA","abf97bbf":"# Model Training and Testing","4e61eeb0":"we have to drop the 2 columns 'casual' and 'registerd' from the training dataset as they are not present in the test dataset","a2b3cf00":"adding month,weekday and hour data","b8a572c1":"datetime - hourly date + timestamp\n\nseason -  1 = spring, 2 = summer, 3 = fall, 4 = winter \n\nholiday - whether the day is considered a holiday\n\nworkingday - whether the day is neither a weekend nor holiday\n\nweather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n\ntemp - temperature in Celsius\n\natemp - \"feels like\" temperature in Celsius\n\nhumidity - relative humidity\n\nwindspeed - wind speed\n\ncasual - number of non-registered user rentals initiated\n\nregistered - number of registered user rentals initiated\n\ncount - number of total rentals","014a4ffd":"## Using XGradient Boosting Regressor","57407e8b":"# visualizing numiric coumns","2f4f81ff":"# Muhammed Mamdouh - Ahmed Samir","e192d35f":"XGradient Boosting Regressor yielded better results so we trained the model with whole data using these parameters","4f0d785c":"# Encoding time data ordered by mean of label in each category ","709b5d1a":"# Produce Submission File","c666b733":"# prediction Distripution","0bc19238":"# Encoding Weather and Season (One Hot Encoding)","3e111640":"# visualizing Catergorical columns","d5b8c07b":"## Using Random Forest Regressor","55c81629":"# Adding timestamp data","caf8bad8":"# Loading Data","7c11afe9":"# Applying inverse transformation","94ec91fa":"# Team members:","68c2bb74":"using Log results in left skewed distribution","c0c73102":"seems like there is no abnormal or weird values in the minimum or maximum values","a94cee1b":"seems like there is no null values"}}