{"cell_type":{"9deecca8":"code","f84d6396":"code","6c16f0f9":"code","2d10f13f":"code","9d9164e0":"code","27d8dbf0":"code","b76ed548":"code","92bb0a16":"code","7220a5fd":"code","85e4d8e0":"code","5ccb3438":"code","724e88f6":"code","701606da":"code","87875945":"code","24e0ad29":"code","df7ad8ba":"code","a910ee45":"markdown","ca968c16":"markdown","6e42428c":"markdown","878f2d9d":"markdown","07112707":"markdown","82e3e158":"markdown","3611a6e0":"markdown","d2774c46":"markdown"},"source":{"9deecca8":"!pip install tensorflow\nimport os","f84d6396":"train_dir = os.path.join('..\/input\/railway-track-fault-detection\/dataset\/Train')\nvalidation_dir = os.path.join('..\/input\/railway-track-fault-detection\/dataset\/Validation')\n\n# Directory with our training defective\/nondefective pictures\ntrain_defective_dir = os.path.join('..\/input\/railway-track-fault-detection\/dataset\/Train\/Defective')\ntrain_nondefective_dir = os.path.join('..\/input\/railway-track-fault-detection\/dataset\/Train\/Non defective')\n\n# Directory with our validation defective\/nondefective pictures\nvalidation_defective_dir = os.path.join('..\/input\/railway-track-fault-detection\/dataset\/Validation\/Defective')\nvalidation_nondefective_dir = os.path.join('..\/input\/railway-track-fault-detection\/dataset\/Validation\/Non defective')","6c16f0f9":"train_defective_fnames = os.listdir(train_defective_dir )\ntrain_nondefective_fnames = os.listdir( train_nondefective_dir)\n\nprint(train_defective_fnames[:20])\nprint(train_nondefective_fnames[:20])","2d10f13f":"print('total training defective images :', len(os.listdir(train_defective_dir)))\nprint('total training non-defective images :', len(os.listdir(train_nondefective_dir)))\n\n\nprint('total validation defective images :', len(os.listdir( validation_defective_dir ) ))\nprint('total validation non-defective images :', len(os.listdir( validation_nondefective_dir) ))","9d9164e0":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Parameters for our graph; we'll output images in a 10x10 configuration\nnrows = 4\nncols = 4\n\n# Index for iterating over images\npic_index = 0","27d8dbf0":"# Set up matplotlib fig, and size it to fit 4x4 pics\nfig = plt.gcf()\nfig.set_size_inches(ncols * 4, nrows * 4)\n\npic_index += 8\nnext_defective_pix = [os.path.join(train_defective_dir, fname) \n                for fname in train_defective_fnames[pic_index-8:pic_index]]\nnext_nondefective_pix = [os.path.join(train_nondefective_dir, fname) \n                for fname in train_nondefective_fnames[pic_index-8:pic_index]]\n\n\nfor i, img_path in enumerate(next_defective_pix+next_nondefective_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows, ncols, i + 1)\n  sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","b76ed548":"import tensorflow as tf","92bb0a16":"import os\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\nlocal_weights_file = '..\/input\/inceptionv3\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\n\npre_trained_model = InceptionV3(input_shape = (300,300, 3), \n                                include_top = False, \n                                weights = None)\n\npre_trained_model.load_weights(local_weights_file)\n\nfor layer in pre_trained_model.layers:\n    layer.trainable = False\npre_trained_model.summary()\n\nlast_layer = pre_trained_model.get_layer('mixed7')\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output","7220a5fd":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n# Add our data-augmentation parameters to ImageDataGenerator\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255.,\n                                   rotation_range = 40,\n                                   width_shift_range = 0.2,\n                                   height_shift_range = 0.2,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255.,\n                                   rotation_range = 40,\n                                   width_shift_range = 0.2,\n                                   height_shift_range = 0.2,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\n\n# --------------------\n# Flow training images in batches of 20 using train_datagen generator\n# --------------------\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    batch_size=20,\n                                                    class_mode='binary',\n                                                    target_size=(300,300))     \n# --------------------\n# Flow validation images in batches of 20 using test_datagen generator\n# --------------------\nvalidation_generator =  test_datagen.flow_from_directory(validation_dir,\n                                                         batch_size=20,\n                                                         class_mode  = 'binary',\n                                                         target_size = (300,300))","85e4d8e0":"from tensorflow.keras.optimizers import RMSprop\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(128, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)                  \n# Add a final sigmoid layer for classification\nx = layers.Dense(1, activation='sigmoid')(x)           \n\nmodel = Model( pre_trained_model.input, x) \n\nmodel.compile(optimizer = RMSprop(lr=0.0001), \n              loss = 'binary_crossentropy', \n              metrics = ['accuracy'])\n","5ccb3438":"model.summary()","724e88f6":"history = model.fit(train_generator,\n                              validation_data=validation_generator,\n                              steps_per_epoch=10,\n                              epochs=30,\n                              validation_steps=5,\n                              verbose=2)","701606da":"#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nacc      = history.history[     'accuracy' ]\nval_acc  = history.history[ 'val_accuracy' ]\nloss     = history.history[    'loss' ]\nval_loss = history.history['val_loss' ]\n\nepochs   = range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot  ( epochs,     acc )\nplt.plot  ( epochs, val_acc )\nplt.title ('Training and validation accuracy')\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot  ( epochs,     loss )\nplt.plot  ( epochs, val_loss )\nplt.title ('Training and validation loss'  )","87875945":"import cv2\nimport numpy as np\nimg = cv2.imread('..\/input\/railway-track-fault-detection\/dataset\/Test\/Defective\/IMG_20201114_102203.jpg')\nplt.imshow(img)\nimg = cv2.resize(img,(300,300))\nimg = np.reshape(img,[1,300,300,3])\n\nclasses = model.predict(img)\n\nprint(classes)\nif classes>0.5:\n    print(\"This Railway track has no fault\")\nelse:\n    print(\"This Railway track has fault\")","24e0ad29":"import cv2\nimport numpy as np\nimg = cv2.imread('')\nplt.imshow(img)\nimg = cv2.resize(img,(300,300))\nimg = np.reshape(img,[1,300,300,3])\n\nclasses = model.predict(img)\n\nprint(classes)\nif classes>0.5:\n    print(\"This Railway track has no fault\")\nelse:\n    print(\"This Railway track has fault\")","df7ad8ba":"import numpy as np\nimport random\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\n\n# Let's define a new Model that will take an image as input, and will output\n# intermediate representations for all layers in the previous model after\n# the first.\nsuccessive_outputs = [layer.output for layer in model.layers[1:]]\n#visualization_model = Model(img_input, successive_outputs)\nvisualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n# Let's prepare a random input image from the training set.\ndefective_img_files = [os.path.join(train_defective_dir, f) for f in train_defective_fnames]\nnondefective_img_files = [os.path.join(train_nondefective_dir, f) for f in train_nondefective_fnames]\nimg_path = random.choice(defective_img_files + nondefective_img_files)\n\nimg = load_img(img_path, target_size=(300,300))  # this is a PIL image\nx = img_to_array(img)  # Numpy array with shape (150, 150, 3)\nx = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n\n# Rescale by 1\/255\nx \/= 255\n\n# Let's run our image through our network, thus obtaining all\n# intermediate representations for this image.\nsuccessive_feature_maps = visualization_model.predict(x)\n\n# These are the names of the layers, so can have them as part of our plot\nlayer_names = [layer.name for layer in model.layers[1:]]\n\n# Now let's display our representations\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n    if len(feature_map.shape) == 4:\n    # Just do this for the conv \/ maxpool layers, not the fully-connected layers\n        n_features = feature_map.shape[-1]  # number of features in feature map\n    # The feature map has shape (1, size, size, n_features)\n        size = feature_map.shape[1]\n    # We will tile our images in this matrix\n        display_grid = np.zeros((size, size * n_features))\n        for i in range(n_features):\n      # Postprocess the feature to make it visually palatable\n            x = feature_map[0, :, :, i]\n            x -= x.mean()\n            x \/= x.std()\n            x *= 64\n            x += 128\n            x = np.clip(x, 0, 255).astype('uint8')\n      # We'll tile each filter into this big horizontal grid\n            display_grid[:, i * size : (i + 1) * size] = x\n    # Display the grid\n        scale = 20. \/ n_features\n        plt.figure(figsize=(scale * n_features, scale))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='summer')","a910ee45":"**Now let's take a look at a few pictures to get a better sense of what they look like. First, configure the matplot parameters:**","ca968c16":"**Now, let's see what the filenames look like in the training directories:**","6e42428c":"**Visualizing Intermediate Representations\nTo get a feel for what kind of features our convnet has learned, one fun thing to do is to visualize how an input gets transformed as it goes through the convnet.\nLet's pick a random image from the training set, and then generate a figure where each row is the output of a layer, and each image in the row is a specific filter in that output feature map. Rerun this cell to generate intermediate representations for a variety of training images.**","878f2d9d":"**We then add convolutional layers as in the previous example, and flatten the final result to feed into the densely connected layers.**\n**Note that because we are facing a two-class classification problem, i.e. a binary classification problem, we will end our network with a sigmoid activation, so that the output of our network will be a single scalar between 0 and 1, encoding the probability that the current image is class 1 (as opposed to class 0).**","07112707":"# Data Preprocessing","82e3e158":"**The \"output shape\" column shows how the size of your feature map evolves in each successive layer. The convolution layers reduce the size of the feature maps by a bit due to padding, and each pooling layer halves the dimensions.**","3611a6e0":"# Defining each of these directories","d2774c46":"# Pre-Trained Model"}}