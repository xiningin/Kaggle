{"cell_type":{"b3976bd2":"code","729c4614":"code","a68be3da":"code","36dae4d2":"code","d2da8dfd":"markdown","0d1e0c58":"markdown","9f8352f1":"markdown"},"source":{"b3976bd2":"# This is a simple example of using pymc3 for Bayesian inference of the parameter distribution.\n# It was adopted from Simon Ouellette's Intro to PyMC3:\n# https:\/\/github.com\/SimonOuellette35\/Introduction_to_PyMC3\n\nimport pymc3 as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\n","729c4614":"obs_y = np.random.normal(0.5, 0.35, 1000)","a68be3da":"obs_y[0:20]","36dae4d2":"with pm.Model() as exercise1:\n\n    # prior probabilitiy distributions\n    stdev = pm.HalfNormal('stdev', sd=1)\n    mu = pm.Normal('mu', mu=0.0, sd=1)\n\n    # The model  - y is a normal distribution with mean mu and standard deviation  stdev.\n    y = pm.Normal('y', mu=mu, sd=stdev, observed=obs_y)\n\n    # Iterate MCMC\n    trace = pm.sample(500)\n\npm.traceplot(trace, ['mu', 'stdev'])\nplt.show()","d2da8dfd":"We need to have prior probability distributions on the parameters.  Unless we have some prior information on what these should be, we often assume weak or noninformative priors that allow pymc3 to estimate the distribution using repeated iterations using Bayes theorem.\n\nFor this example, suppose that we expect the data to come from a normal distribution, but we do not have much information on the parameters.  Our weak priors are then:\n1. The standard deviation stdev is a half-normal distribution with standard deviation of 1.\n2. The mean mu has a normal distribution with mean 0 and standard deviation 1.\nNOTE: A half-normal distribution is the positive side of a normal distribution, normalized to sum to one.\n\nThe Bayes graphical model for these random variables is:\n<img src=\"attachment:image.png\" width=\"400px\">","0d1e0c58":"The goal of Bayeisan parameter inference is to esitmate the probability distributions for underlying parameters from observed data.\n\nFirst we generate an artificial dataset. The distribution is $\\mu=0.5$, $\\sigma=0.35$, and we generate 1000 observations.","9f8352f1":"Observe that the resulting posterior distributions give good information about the parameters for the distributions of the data.\n\nUse the code above to test the following situations:\n1. What if you have only 100 observations and overly restrictive probabilities, for example a standard deviation of 0.05 in the priors on both $\\mu$ and $\\sigma$.\n2. What if you have the overly restrivitve priors, and then increase your number of observations.\n3. The sample has a tuning stage where is learns the distributions but the iterations in the tuning stage are not displayed.  Try changing the sampling call to \"trace = pm.sample(100, discard_tuned_samples=False)\" to see the tuning samples.  You may need to also use more restricitve priors on mu and std to see the tuning."}}