{"cell_type":{"ba9775b4":"code","df7325f6":"code","7d1b951b":"code","26b458de":"code","793a564d":"code","66d383bc":"code","f7671899":"code","b4165af5":"code","1b11ad74":"code","f779112b":"code","9375f45c":"code","1a64407c":"code","b76a8842":"code","ec45c18d":"code","61d6f6b6":"code","c487ba52":"code","834d22d7":"code","95e68762":"code","533d43d7":"code","9415bdaa":"code","e762cd45":"code","ff1b998b":"code","3b3394b8":"code","04a597f9":"code","cf84fb3f":"code","1cddca49":"code","6e9e3e4b":"code","f3d73620":"code","910f4b61":"code","99377936":"code","54aed9be":"code","71e8584a":"code","ff156724":"code","d188885a":"code","2c14cbc8":"code","8e9bfe0f":"code","b4833b82":"code","e6289b47":"code","75786164":"code","ba4615d0":"code","dfd81e7e":"markdown","6cb025a4":"markdown","22594894":"markdown","1a6472a7":"markdown","cf8b279c":"markdown","692c4aa4":"markdown","39fbb28a":"markdown","be76503f":"markdown","3cd9abb9":"markdown","d722fa02":"markdown","b15a2b10":"markdown","7460d697":"markdown","55346322":"markdown","3d93b1a2":"markdown","5ea20512":"markdown"},"source":{"ba9775b4":" \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nimport gc\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","df7325f6":"train=pd.read_csv(\"..\/input\/X_train.csv\")\ny=pd.read_csv(\"..\/input\/y_train.csv\")\ntest=pd.read_csv(\"..\/input\/X_test.csv\")","7d1b951b":"train.head()","26b458de":"y.head()","793a564d":"train.shape,test.shape","66d383bc":"train.describe()","f7671899":"y.head(),y.shape,y.columns","b4165af5":"target=y['surface'].value_counts().reset_index().rename(columns={\"index\":\"target\"})","1b11ad74":"target.head()","f779112b":"target.plot.bar()","9375f45c":"y['surface'].value_counts().plot.bar()","1a64407c":"train.isnull().sum()","b76a8842":"train['is_duplicate'] = train.duplicated()\ntrain['is_duplicate'].value_counts()","ec45c18d":"train = train.drop(['is_duplicate'], axis = 1)","61d6f6b6":"train_sort_value=train.sort_values(by=['series_id','measurement_number'],ascending=True)","c487ba52":"train_sort_value.head()","834d22d7":"train.max(),train.min()","95e68762":"f,ax=plt.subplots(1,1 ,figsize=(8,8))\nsns.heatmap(train.iloc[:,3:].corr(),annot=True,linewidths=.5,cmap=\"YlGnBu\",ax=ax)","533d43d7":"f,ax=plt.subplots(1,1 ,figsize=(8,8))\nsns.heatmap(test.iloc[:,3:].corr(),annot=True,linewidths=.5,cmap=\"YlGnBu\",ax=ax)","9415bdaa":"def feature_distribution_plot(train, test, label1, label2, features,a=2,b=5):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(a,b,figsize=(17,9))\n\n    for feature in features:\n        i += 1\n        plt.subplot(a,b,i)\n        sns.kdeplot(train[feature], bw=0.5,label=label1)\n        sns.kdeplot(test[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show(); ","e762cd45":"features = train.columns.values[3:]\nfeature_distribution_plot(train, test, 'train', 'test', features)","ff1b998b":"plt.figure(figsize=(26, 16))\nfor i, col in enumerate(train.columns[3:]):\n    ax = plt.subplot(3, 4, i + 1)\n    sns.distplot(train[col], bins=100, label='train')\n    sns.distplot(test[col], bins=100, label='test')\n    ax.legend()   ","3b3394b8":"data=train.merge(y,on=\"series_id\",how='inner')\ntargets=(y['surface'].value_counts()).index","04a597f9":"data.head()","cf84fb3f":"plt.figure(figsize=(26, 16))\nfor i,col in enumerate(data.columns[3:13]):\n    ax = plt.subplot(3,4,i+1)\n    ax = plt.title(col)\n    for surface in targets:\n        surface_feature = data[data['surface'] == surface]\n        sns.kdeplot(surface_feature[col], label = surface)","1cddca49":"def feature_eng1(df):\n    df['norm_q']=df['orientation_X']**2 + df['orientation_Y']**2 + df['orientation_Z']**2 +df['orientation_W'] **2\n    df['mor_q']=df['norm_q']**0.5\n    df['norm_X']=df['orientation_X']\/df['norm_q']\n    df['norm_Y']=df['orientation_Y']\/df['norm_q']\n    df['norm_Z']=df['orientation_Z']\/df['norm_q']\n    df['norm_W']=df['orientation_W']\/df['norm_q']\n    \n    return df","6e9e3e4b":"df=feature_eng1(train)\ntest=feature_eng1(test)\nprint(df.shape,test.shape)\ndf.head()","f3d73620":"\ndef feat_eng2(df):\n    df['totl_anglr_vel'] = (df['angular_velocity_X']**2 + df['angular_velocity_Y']**2 + df['angular_velocity_Z']**2)** 0.5\n    df['totl_linr_acc'] = (df['linear_acceleration_X']**2 + df['linear_acceleration_Y']**2 + df['linear_acceleration_Z']**2)**0.5\n    df['totl_orientation'] = (df['orientation_X']**2 + df['orientation_Y']**2 + df['orientation_Z']**2)**0.5\n    df['acc_vs_vel'] = df['totl_linr_acc'] \/ df['totl_anglr_vel']\n    return df\n","910f4b61":"data = feat_eng2(train)\ntest = feat_eng2(test)\nprint(data.shape, test.shape)\ndata.head()","99377936":"def feat_eng3(df):\n    data = pd.DataFrame()\n    for col in df.columns:\n        if col in ['row_id','series_id','measurement_number']:\n            continue\n        data[col + '_mean'] = df.groupby(['series_id'])[col].mean()\n        data[col + '_median'] = df.groupby(['series_id'])[col].median()\n        data[col + '_max'] = df.groupby(['series_id'])[col].max()\n        data[col + '_min'] = df.groupby(['series_id'])[col].min()\n        data[col + '_std'] = df.groupby(['series_id'])[col].std()\n        data[col + '_range'] = data[col + '_max'] - data[col + '_min']\n        data[col + '_maxtoMin'] = data[col + '_max'] \/ data[col + '_min']\n        #in statistics, the median absolute deviation (MAD) is a robust measure of the variablility of a univariate sample of quantitative data.\n        data[col + '_mad'] = df.groupby(['series_id'])[col].apply(lambda x: np.median(np.abs(np.diff(x))))\n        data[col + '_abs_max'] = df.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        data[col + '_abs_min'] = df.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        data[col + '_abs_avg'] = (data[col + '_abs_min'] + data[col + '_abs_max'])\/2\n    return data","54aed9be":"%%time\ndata = feat_eng3(train)\ntest = feat_eng3(test)\nprint(data.shape, test.shape)\ndata.head()","71e8584a":"data.fillna(0, inplace = True)\ndata.replace(-np.inf, 0, inplace = True)\ndata.replace(np.inf, 0, inplace = True)\ntest.fillna(0, inplace = True)\ntest.replace(-np.inf, 0, inplace = True)\ntest.replace(np.inf, 0, inplace = True)","ff156724":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndata_y = le.fit_transform(y['surface'])\n","d188885a":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf_y= le.fit_transform(y['surface'])\n","2c14cbc8":"folds = StratifiedKFold(n_splits=12, shuffle=True, random_state=60)\npredicted = np.zeros((test.shape[0],9))\nmeasured= np.zeros((data.shape[0]))\nscore = 0","8e9bfe0f":"for times, (trn_idx, val_idx) in enumerate(folds.split(data.values,data_y)):\n    model = RandomForestClassifier(n_estimators=500, n_jobs = -1)\n    #model = RandomForestClassifier(n_estimators=500, max_depth=10, min_samples_split=5, n_jobs=-1)\n    model.fit(data.iloc[trn_idx],data_y[trn_idx])\n    measured[val_idx] = model.predict(data.iloc[val_idx])\n    predicted += model.predict_proba(test)\/folds.n_splits\n    score += model.score(data.iloc[val_idx],data_y[val_idx])\n    print(\"Fold: {} score: {}\".format(times,model.score(data.iloc[val_idx],data_y[val_idx])))\n    \n    gc.collect()","b4833b82":"importances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis = 0)\nindices = np.argsort(importances)[::-1]","e6289b47":"feature_imp = pd.DataFrame(importances, index = data.columns, columns = ['importance'])\nfeature_imp.sort_values('importance', ascending = False)\nfeature_imp.head()","75786164":"less_important_features = feature_imp.loc[feature_imp['importance'] < 0.0025]\nprint('There are {0} features their importance value is less then 0.0025'.format(less_important_features.shape[0]))","ba4615d0":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['surface'] = le.inverse_transform(predicted.argmax(axis=1))\nsubmission.to_csv('rs_surface_submission6.csv', index=False)\nsubmission.head()","dfd81e7e":"numericla features in train data","6cb025a4":"**Feature Engineering **\n\n\n\nEuler angles\nThe Euler angles are three angles introduced by Leonhard Euler to describe the orientation of a rigid body with respect to a fixed coordinate system.","22594894":"No missing values on the train data","1a6472a7":" Correlations of features Test data","cf8b279c":"now our target variable  analysis ","692c4aa4":"**Model Builiding **","39fbb28a":"\n***DATA PreProcessing****","be76503f":"linear_accelaration are normally distributed\/symmetrical distribution but average value is slightly negative for linear_accelaration_Z\nX,Y,Z,W orientation data are not symmetrical or bell shaped distributed.\nX,Y orientation data are distributed un-even between 1 to -1.\nZ,W orientation data are distributed un-even between 1.5 to -1.5\nSince orientation data is not linearly distributed, taking log of the orientation data may improve the results.\n\n","3cd9abb9":"1. Correlations of features Train data","d722fa02":"Min and max values of each feature of the train data ","b15a2b10":"Understanding about important features will help us fine tuning feature enginnering as well accuracy improvement.\n\n","7460d697":"\nwell , some fetures strongly  Correlated to each other\n* Angular_velocity_Y is related to Angular_velocity_Z\n* Linear_Acceleration_Y is related to the Linear_Acceleration_Z","55346322":"Data exploation","3d93b1a2":"* Godd news, our basic features have the same distribution (Normal) on test and training. There are some differences between orientation_X , orientation_Y and linear_acceleration_Y.\n * I willl try StandardScaler to fix this, and remember: orientation , angular velocity and linear acceleration are measured with different units, scaling might be a good choice.","5ea20512":"No duplicates on the train data"}}