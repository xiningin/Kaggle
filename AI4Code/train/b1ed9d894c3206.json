{"cell_type":{"0afd8131":"code","f214ace7":"code","d7dbe661":"code","82367c33":"code","476b5ab1":"code","6a6ccb20":"code","1dc9e439":"code","b4c0a8f7":"code","4c98ce53":"code","0a2b66bb":"code","f80debea":"code","fdff8941":"code","f57e83e6":"code","c99af019":"code","1cb77c41":"code","33f8e947":"code","919db34d":"code","5a1d74ea":"code","e7c59cf2":"code","80b495ce":"code","4554c784":"code","17a444e9":"code","3f76e463":"code","7c642eff":"code","158f89b6":"markdown","d0d496d2":"markdown","40e61932":"markdown","352eb332":"markdown","7736424e":"markdown","6f4e7424":"markdown","ce5ff0d0":"markdown","35d4f56f":"markdown","59db65e8":"markdown","5da90295":"markdown","bdc68972":"markdown","9af37642":"markdown"},"source":{"0afd8131":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f214ace7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.impute import SimpleImputer as SI\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras.models import Model\nimport kerastuner as kt\nfrom kerastuner.tuners import RandomSearch\n\nss = StandardScaler()\nsi = SI(strategy=\"median\")","d7dbe661":"dtypes_dict = {'row_id': 'int64',\n               'timestamp': 'int64',\n               'user_id': 'int32', \n               'content_id': 'int16',\n               'content_type_id': 'int8',\n               'task_container_id': 'int16', \n               'user_answer': 'int8', \n               'answered_correctly': 'int8',\n               'prior_question_elapsed_time': 'float32', \n               'prior_question_had_explanation': 'boolean'\n              }\n\ntrain_df = pd.read_csv('..\/input\/riiid-test-answer-prediction\/train.csv',\n                      nrows=10**7,\n                      dtype=dtypes_dict,\n                      index_col=0)  # as row_id is same as index, I am making it default index col\ntrain_df","82367c33":"target_col = 'answered_correctly'\n\n# let's see with how many classes we are dealing with\ntrain_df[target_col].value_counts()","476b5ab1":"working_data = train_df[train_df[target_col]!=-1]\nworking_data","6a6ccb20":"working_data.isna().sum()","1dc9e439":"working_data","b4c0a8f7":"print(\"Number of unique users: \", working_data.user_id.nunique())\nprint(\"Number of unique content(or unique user interaction): \", working_data.content_id.nunique())\nprint(\"Number of unique tasks(or batch of lectures): \", working_data.task_container_id.nunique())","4c98ce53":"userGroup = working_data.groupby(\"user_id\")[target_col].mean().reset_index()\nuserGroup","0a2b66bb":"contentGroup = working_data.groupby(\"content_id\")[target_col].mean().reset_index()\ncontentGroup","f80debea":"taskGroup = working_data.groupby(\"task_container_id\")[target_col].mean().reset_index()\ntaskGroup","fdff8941":"userGroup.columns = ['user_id', 'user_performance']\ncontentGroup.columns = ['content_id', 'content_performance']\ntaskGroup.columns = ['task_container_id', 'task_performance']","f57e83e6":"working_data = working_data.reset_index()\nworking_data","c99af019":"working_data.loc[:, \"prior_question_elapsed_time\"] = working_data['prior_question_elapsed_time'].fillna(0)\nworking_data.loc[:, \"prior_question_had_explanation\"] = working_data['prior_question_had_explanation'].fillna(0)\nworking_data","1cb77c41":"features = ['timestamp', 'prior_question_elapsed_time', 'prior_question_had_explanation']\ncat_cols = ['user_id', 'content_id', 'task_container_id']\nselected_data = working_data[features + cat_cols + [target_col]].copy()\nselected_data","33f8e947":"def preprocess(df):\n    \"\"\"\n    Merge user, task and content performance and return df with seleted features.\n    \"\"\"\n    df.loc[:, 'timestamp'] = df['timestamp'].rolling(window=5, min_periods=1, center=True).sum()\n    df.loc[:, 'prior_question_elapsed_time'] = df['prior_question_elapsed_time'].rolling(window=5, min_periods=1, center=True).sum()\n    df = df.merge(userGroup, how='left', on='user_id')\n    # deal with possible nan values\n    df.loc[:, 'user_performance'] = df['user_performance'].fillna(0.5)\n    df = df.merge(contentGroup, how='left', on='content_id')    \n    df.loc[:, 'content_performance'] = df['content_performance'].fillna(0.5)\n    df = df.merge(taskGroup, how='left', on='task_container_id') \n    df.loc[:, 'task_performance'] = df['task_performance'].fillna(0.5)\n    \n    # rescale the time values\n    df['timestamp'] = ss.fit_transform(df['timestamp'].values.reshape(-1, 1))\n    df['prior_question_elapsed_time'] = ss.fit_transform(df['prior_question_elapsed_time'].values.reshape(-1, 1))\n\n    df['prior_question_had_explanation'] = df['prior_question_had_explanation'].map({True:1, False: 0})\n    df['prior_question_had_explanation'] = si.fit_transform(df['prior_question_had_explanation'].values.reshape(-1, 1))\n\n    return df","919db34d":"preprocess(selected_data)","5a1d74ea":"final_features = ['timestamp',\n                  'prior_question_elapsed_time',\n                  'prior_question_had_explanation',\n                  'user_performance',\n                  'content_performance',\n                  'task_performance']\n\nfinal_train = preprocess(selected_data)[final_features + [target_col]]\nfinal_train","e7c59cf2":"def build_model(hp):\n    inputs = tf.keras.Input(shape=(6, ))\n    x = inputs\n    x = tf.keras.layers.Dense(hp.Int('hidden_size', 30, 100, step=10, default=50), activation='relu')(x)\n    x = tf.keras.layers.Dropout(hp.Float('dropout', 0, 0.5, step=0.1, default=0.5))(x)\n    x = tf.keras.layers.Dense(hp.Int('hidden_size', 30, 100, step=10, default=50), activation='relu')(x)\n    x = tf.keras.layers.Dropout(hp.Float('dropout', 0, 0.5, step=0.1, default=0.5))(x)\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n                  loss='binary_crossentropy', \n                  metrics=['accuracy'])\n    return model","80b495ce":"tuner = RandomSearch(\n    build_model,\n    objective=\"val_accuracy\",\n    max_trials=5,\n    executions_per_trial=3\n)\ntuner.search_space_summary()","4554c784":"X= final_train.drop([target_col], axis=1).values\ny = final_train[target_col].values\nprint(X.shape)\nprint(y.shape)","17a444e9":"cv = StratifiedKFold(n_splits=5)\nparams = {}\nmodels = {}\nfor i, (tr, val) in enumerate(cv.split(X, y)):\n    print(\"===================\")\n    print(f\"Fold: {i}\")\n    tuner.search(X[tr], y[tr],\n                 validation_data=(X[val], y[val]),\n                 epochs=20,\n                 callbacks=[tf.keras.callbacks.ModelCheckpoint(f\"model_cv{i}.h5\", save_best_only=True)])\n    \n    params[i] = tuner.get_best_models(1)[0]\n    models[i] = tuner.get_best_hyperparameters(1)[0]\n    \n    \n    pass","3f76e463":"import riiideducation\n\nenv = riiideducation.make_env()\niter_test = env.iter_test()","7c642eff":"for test_df, sample_prediction_df in iter_test:\n    y_preds = []\n    test_df = preprocess(test_df)\n    x_test = test_df[final_features].values\n    \n    for model in models:\n        y_pred = model_v1.predict(x_test, verbose=1)\n        y_preds.append(y_pred)\n    \n    y_preds = sum(y_preds) \/ len(y_preds)\n    test_df[target_col] = y_preds\n    env.predict(test_df.loc[test_df['content_type_id'] == 0,\n               ['row_id', target_col]])","158f89b6":"Ah, well! According to the host, `-1` depicts the data represents a nan value. So, we will discard those samples with `answered_correctly = -1`.","d0d496d2":"# Modelling and Hyperparameter tunning with Keras Tuner","40e61932":"# Prediction","352eb332":"Let's check for nan values if there is more.","7736424e":"Still you want to treat them as a categorical feature? Well, I don't. I am thinking of creating features per individual users\/content\/tasks though. ","6f4e7424":"We will just look at the target cols and will make a simple NN starter model using keras.\n\nMore on EDA and interesting findings can be found here(under working): https:\/\/www.kaggle.com\/mrutyunjaybiswal\/understanding-the-answer-correctness-eda","ce5ff0d0":"Now, let's talk about below feature columns:\n\n- user_id\n- content_id\n- task_container_id\n\nLook at their value counts.","35d4f56f":"# Target and Features\n\n> Target\n- Did the `student(user_id)` answered the question `(answered_correctly)` from \n\n> Features\n- When? `(timestamp)`. Well, its actually the time diffrenece between the time of attempt to user's first interaction\n- a particular `content(content_id)` or \n- `type of content(content_type_id)` or \n- `task container(task_container_id)` or \n- `how much time` did S\/he take to answer the previous question (or question bundle) `prior_question_elapsed_time` or \n- had he referred to any explanation or say it any tutorial for ansdwering the previous question bundle`(prior_question_had_explanation)`.","59db65e8":"# Loading Data\n\nWe gotta take care of data loading as its a very huge dataset, hence we have to specify the dtypes of each column so that our RAM won't crash.","5da90295":"# Cross-validation \n\n> I choose to use Stratified KFold cv with 5 splits. As there is 2:1 imbalancement between classes, this my choice of cv strategy as of now.","bdc68972":"As we have seen so far, there are ~39.4K nan values still in our dataset. How can we deal with that? I am thinking of handling it by not handling it. Why did I say so? We might have some way to fill up th time elapsed column, but how will we do the same for `prev_question_had_explanation` col. Though I have an idea to deal with that. Fill the nan value with the `max(no_of_false, no_of_true)` for each individual user. Well, I am skipping it for now.\n\nFor now, I am simply dropping the nan values.","9af37642":"So what I did, I kinda tried to measure performance in terms of how many times did they answer correctly per each user and content and task. Let's rename the cols and make them ready to merge with our working data. "}}