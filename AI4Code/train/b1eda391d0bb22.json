{"cell_type":{"ee8bb529":"code","72935a72":"code","d0d8c66b":"code","998c5b19":"code","df5531ec":"code","0c3a2cfd":"code","1057944a":"code","45fb6f99":"code","84f0d074":"code","83ea9d84":"code","ef5aa515":"code","f9fccbb6":"code","7c3b295b":"code","d8befe89":"code","a59833f3":"code","476202bf":"code","b2408df9":"code","23385639":"code","bb593a73":"code","76538aee":"code","de682c19":"code","fa531a60":"code","5ec60742":"code","d8817fc1":"code","b68a5ec6":"code","9f660256":"code","29d41e56":"code","cf12f2b1":"code","018de70e":"code","abf7b5fb":"markdown","ef24b069":"markdown","ed33437e":"markdown","d26e1b9e":"markdown","cb124167":"markdown","3a1e58e7":"markdown","4ac43d0b":"markdown","d9b480d8":"markdown","6d0ae15d":"markdown","ed0f05f2":"markdown","87779b5f":"markdown","2e9f6cbe":"markdown","dadc0a2c":"markdown","bbd46a22":"markdown","d5e6dbae":"markdown"},"source":{"ee8bb529":"import pandas as pd\nimport numpy as np\nfrom tensorflow import keras\nfrom tqdm import tqdm\nimport nltk","72935a72":"print(keras.__version__)","d0d8c66b":"train = pd.read_csv('..\/input\/yelp-review-dataset\/yelp_review_polarity_csv\/train.csv', names = ['sentiment', 'text'] )\ntest  = pd.read_csv('..\/input\/yelp-review-dataset\/yelp_review_polarity_csv\/test.csv',  names = ['sentiment', 'text'] )","998c5b19":"train = train[:15000]\ntrain.head()","df5531ec":"# pip install bs4\n# from bs4 import BeautifulSoup\n# from nltk.corpus import stopwords\n# from nltk.tokenize import sent_tokenize, word_tokenize\n# from nltk.stem import WordNetLemmatizer \n# import re\n# lm=WordNetLemmatizer()\n\n\n# def ReturnCleanText(text):\n#         # change the text into lower case.(Note: in case of social media text, it is good to leave them as it is)\n#         text = text.lower()\n#         # removing xml tags from tweets\n#         text =BeautifulSoup(text, 'lxml').get_text()\n#         # removing URLS \n#         text =re.sub('https?:\/\/[A-Za-z0-9.\/]+','',text)\n#         # removing words with \"@\"\n#         text =re.sub(r'@[A-Za-z0-9]+','',text)\n#         # removing special characters\n#         text = re.sub(r\"\\W+|_\", ' ', text)\n#         # tokenization of sentences\n#         text = word_tokenize(text)\n#         # lemmatize the text using WordNetn\n#         words = [lm.lemmatize(word) for word in text if word not in set(stopwords.words('english'))]   \n#         return \" \".join(words)\n    \n# train['clean_text'] = train['text'].apply(ReturnCleanText)","0c3a2cfd":"import tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Dropout","1057944a":"# Just for example\nmax_features = 2000\nEncoder = keras.layers.experimental.preprocessing.TextVectorization( max_tokens = max_features)\nEncoder.adapt(train['text'].values)\n\nvocab = np.array(Encoder.get_vocabulary())\nprint(vocab[:20])\n\nexample =\"This is an example to test the encoder that we just created!\"\nprint(Encoder(example).numpy())\nprint(\" \".join(vocab[Encoder(example).numpy()]))","45fb6f99":"max_features = 2000\ntokenizer = Tokenizer(num_words = max_features, )\ntokenizer.fit_on_texts(train['text'].values)\nX = tokenizer.texts_to_sequences(train['text'].values)\nX = pad_sequences(X, padding = 'post' ,maxlen=300)\nY = pd.get_dummies(train['sentiment']).values\n\nvocab_size = len(tokenizer.word_index)+1","84f0d074":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","83ea9d84":"embid_dim = 300\nlstm_out = 128\n\n\nmodel = keras.Sequential()\nmodel.add(Embedding(max_features, embid_dim, input_length = X.shape[1]))\nmodel.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dense(2, activation = 'softmax'))\nmodel.summary()","ef5aa515":"batch_size = 128\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nhistory = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))","f9fccbb6":"from tqdm import tqdm\nembedding_vector = {}\nf = open('..\/input\/glove840b300dtxt\/glove.840B.300d.txt')\nfor line in tqdm(f):\n    value = line.split(' ')\n    word = value[0]\n    coef = np.array(value[1:],dtype = 'float32')\n    embedding_vector[word] = coef","7c3b295b":"embedding_matrix = np.zeros((vocab_size,300))\nfor word,i in tqdm(tokenizer.word_index.items()):\n    embedding_value = embedding_vector.get(word)\n    if embedding_value is not None:\n        embedding_matrix[i] = embedding_value","d8befe89":"embedding_matrix.shape","a59833f3":"embid_dim = 300\nlstm_out = 128\n\n\nmodel = keras.Sequential()\nmodel.add(Embedding(vocab_size, embid_dim, input_length =X.shape[1], weights = [embedding_matrix] , trainable = False))\nmodel.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dense(2, activation = 'softmax'))\nmodel.summary()","476202bf":"batch_size = 128\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nhistory = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))","b2408df9":"sentences =[]\nfor t in  tqdm(range(len(train['text']))):\n    text = nltk.word_tokenize(train['text'][t])\n    sentences.append(text)","23385639":"from gensim.models import Word2Vec\nw2v_model = Word2Vec(sentences, size=300, min_count=2, sg = 0 )","bb593a73":"words = list(w2v_model.wv.vocab)\nprint('Vocabulary size: %d' % len(words))\n\n# save model \nfilename = 'embedding_word2vec.txt'\nw2v_model.wv.save_word2vec_format(filename, binary=False)","76538aee":"embedding_vector = {}\nf = open('.\/embedding_word2vec.txt')\nfor line in tqdm(f):\n    value = line.split(' ')\n    word = value[0]\n    coef = np.array(value[1:],dtype = 'float32')\n    embedding_vector[word] = coef","de682c19":"embedding_matrix = np.zeros((vocab_size,300))\nfor word,i in tqdm(tokenizer.word_index.items()):\n    embedding_value = embedding_vector.get(word)\n    if embedding_value is not None:\n        embedding_matrix[i] = embedding_value    ","fa531a60":"embid_dim = 300\nlstm_out = 128\n\n\nmodel = keras.Sequential()\nmodel.add(Embedding(vocab_size, embid_dim, input_length =X.shape[1], weights = [ embedding_matrix] , trainable = False))\nmodel.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dense(2, activation = 'softmax'))\nmodel.summary()","5ec60742":"batch_size = 128\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nhistory = model.fit(X_train, Y_train, epochs = 50, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))","d8817fc1":"from gensim.models import KeyedVectors\nfilename = '..\/input\/nlpword2vecembeddingspretrained\/GoogleNews-vectors-negative300.bin'\nw2v_pretrained_model = KeyedVectors.load_word2vec_format(filename, binary=True)","b68a5ec6":"embedding_matrix = np.zeros((vocab_size,300))\nfor word,i in tqdm(tokenizer.word_index.items()):\n    try:\n        embedding_value = w2v_pretrained_model[word]\n        if embedding_value is not None:\n            embedding_matrix[i] = embedding_value         \n    except KeyError:\n        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),300)       ","9f660256":"embid_dim = 300\nlstm_out = 128\n\n\nmodel = keras.Sequential()\nmodel.add(Embedding(vocab_size, 300, input_length =300, weights = [embedding_matrix ] , trainable = False))\nmodel.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dense(2, activation = 'softmax'))\nmodel.summary()","29d41e56":"batch_size = 128\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nhistory = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))","cf12f2b1":"embid_dim = 300\nlstm_out = 128\n\n\nmodel = keras.Sequential()\nmodel.add(Embedding(vocab_size, 300, input_length =300, weights = [embedding_matrix ],\n                    trainable = True))\nmodel.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dense(2, activation = 'softmax'))\nmodel.summary()","018de70e":"batch_size = 128\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nhistory = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))","abf7b5fb":"**Word2Vec** is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets.\n\n1. Continuous Bag-of-Words Model which predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.\n2. Continuous Skip-gram Model which predict words within a certain range before and after the current word in the same sentence. A worked example of this is given below.\n\nKnow more here:\n1. https:\/\/jalammar.github.io\/illustrated-word2vec\/\n2. https:\/\/www.tensorflow.org\/tutorials\/text\/word2vec","ef24b069":"#### In this Notebook, I will be using LSTM(Long Short Term Memory) for Text Classification with Yelp Review Dataset.\n\n#### Main purpose is here to use different kind of **word embedding** along with Neural Network(here LSTM), to see how they affect our overall model accuracy.\n---\n##### We will be using these four embedding methods:\n1. Default Keras Embedding\n2. word2vec\n3. fastText \n4. GloVe\n---\nNote: As this dataset is balanced, I am using accuracy as our model evaluation method, also we are using Keras for developing models.","ed33437e":"### Training a Word2Vec Embedding from scratch using Gensim library:","d26e1b9e":"#### Here, Negative polarity is class 1, and positive class 2.\n","cb124167":"# Training with GloVe 300D Embeddings\n\nGloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n\nLink: https:\/\/nlp.stanford.edu\/projects\/glove\/","3a1e58e7":"#### Before diving into that, lets do little text cleaning\/preprocessing.","4ac43d0b":"In this notebook, I have used different word embedding techniques, we can see the validation accuracy with every embedding method, we need to choose one of these according to our problem statement.\n\nAccuracy can be also further increased by changing\/updating the embedding dimension, preprocessing of text, different model architecture, batch size and etc.","d9b480d8":"# Training with Keras default Embedding Layer","6d0ae15d":"# LSTM(Long Short Term Memory):\n> Long short-term memory is an artificial recurrent neural network architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points, but also entire sequences of data.","ed0f05f2":"# Training with Word2Vec Pre-trained and Trained Embeddings\nReference: https:\/\/machinelearningmastery.com\/develop-word-embedding-model-predicting-movie-review-sentiment\/","87779b5f":"### Using Pretrained Word2Vec Embedding\nReference: https:\/\/www.kaggle.com\/jaskarancr\/word2vec-traditional-models","2e9f6cbe":"### Keras Embedding Layer: \n\nEmbedding layers in Keras are trained just like any other layer in your network architecture: they are tuned to minimize the loss function by using the selected optimization method. The major difference with other layers, is that their output is not a mathematical function of the input. Instead the input to the layer is used to index a table with the embedding vectors [1]. However, the underlying automatic differentiation engine has no problem to optimize these vectors to minimize the loss function...\n\nSo, you cannot say that the Embedding layer in Keras is doing the same as word2vec [2]. Remember that word2vec refers to a very specific network setup which tries to learn an embedding which captures the semantics of words. With Keras's embedding layer, you are just trying to minimize the loss function, so if for instance you are working with a sentiment classification problem, the learned embedding will probably not capture complete word semantics but just their emotional polarity.\n\nMore Here: \n1. https:\/\/stats.stackexchange.com\/questions\/324992\/how-the-embedding-layer-is-trained-in-keras-embedding-layer\n2. https:\/\/stats.stackexchange.com\/questions\/270546\/how-does-keras-embedding-layer-work","dadc0a2c":"# Using Pretrained word2vec Embedding with Trainable as True","bbd46a22":"##### sg : Either 0 or 1. Default is 0 or CBOW. One must explicitly define Skip-gram by passing 1.","d5e6dbae":"\n\n Here we are going to do binary classification using Yelp Review Sentiment Dataset.\n- Dataset Link: [Kaggle Link](https:\/\/www.kaggle.com\/ilhamfp31\/yelp-review-dataset)"}}