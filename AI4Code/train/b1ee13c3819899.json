{"cell_type":{"f2af1576":"code","c803f8fc":"code","a17fcce9":"code","4266927f":"code","e56d6ab1":"code","c38f99e5":"code","306825cf":"code","048bc156":"code","fde9f7bb":"code","58f0593a":"code","6ce99844":"code","548d2ca7":"code","154a4a28":"code","8063a78f":"code","8c24779f":"code","81b79897":"code","3a1e12aa":"code","7bf595ab":"code","def3e945":"code","3a5ad7ac":"code","5fdf3fa3":"code","f93daa74":"code","66b85e42":"code","370c2789":"code","0821ff8d":"code","87cae4bb":"code","edf89c16":"code","4df39383":"code","29c6d5f6":"code","27d64c15":"code","3cc963cf":"code","256c94e7":"markdown","e41304ba":"markdown","eb867079":"markdown","c6d4496f":"markdown","698d80cf":"markdown","2aada31a":"markdown","fe9a796e":"markdown","c465338a":"markdown","3211d17c":"markdown","07c4a285":"markdown","34971e44":"markdown","204276a4":"markdown","bb3c324b":"markdown","0962aabb":"markdown","4978c517":"markdown","51464be8":"markdown","5c6d1b64":"markdown","43100d38":"markdown","12fea5fa":"markdown","7d1cacd3":"markdown","3a1bf93e":"markdown","5da57749":"markdown"},"source":{"f2af1576":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c803f8fc":"!pip install skimpy","a17fcce9":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom skimpy import skim\nimport matplotlib.pyplot as plt","4266927f":"train_org = pd.read_csv('\/kaggle\/input\/categorical-data\/train.csv')\ntrain = train_org[['product','age','target']]\ntrain_x = train.drop(['target'], axis=1)\ntrain_y = train['target']\ntest_x = pd.read_csv('\/kaggle\/input\/categorical-data\/test.csv')","e56d6ab1":"train.info()","c38f99e5":"skim(train)","306825cf":"sns.set(font_scale=1.5)\nplt.style.use('seaborn-white')\ntrain_x['product'].value_counts().sort_values().plot(kind='barh',figsize=(10, 8))\nplt.xlabel(\"Count\", labelpad=14)\nplt.ylabel(\"Label\", labelpad=14)\nplt.title(\"Orginal Label for product column\", y=1.02)","048bc156":"train_x_saved = train_x.copy()\ntest_x_saved = test_x.copy()\n\n\ndef load_data():\n    train_x, test_x = train_x_saved.copy(), test_x_saved.copy()\n    return train_x, test_x\n\ncat_cols = ['product']\ntrain.loc[:,'product'].unique()","fde9f7bb":"train_x, test_x = load_data()\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Encoding by OneHotEncoder\nohe = OneHotEncoder(sparse=False, categories='auto')\nohe.fit(train_x[cat_cols])\n\n# Create column names\ncolumns = []\nfor i, c in enumerate(cat_cols):\n    columns += [f'{c}_{v}' for v in ohe.categories_[i]]\n\nprint(columns)\n# Convert the created dummy variable to a data frame\ndummy_vals_train = pd.DataFrame(ohe.transform(train_x[cat_cols]), columns=columns)\ndummy_vals_test = pd.DataFrame(ohe.transform(test_x[cat_cols]), columns=columns)\n\n# Combine with the rest of the variables\ntrain_x = pd.concat([train_x.drop(cat_cols, axis=1), dummy_vals_train], axis=1)\ntest_x = pd.concat([test_x.drop(cat_cols, axis=1), dummy_vals_test], axis=1)","58f0593a":"skim(train_x)","6ce99844":"def highlight_max(s, props=''):\n    return np.where(s == np.nanmax(s.values), props, '')\n\ntrain_x.iloc[:,1:].astype(int).head().style.apply(highlight_max, props='color:Black;background-color:Yellow', axis=1)","548d2ca7":"train_x, test_x = load_data()\n\ntest_x = test_x.loc[:,['product','age']]\n\ncombined = pd.concat([train_x,test_x])\ncombined = pd.get_dummies(combined,columns=cat_cols)\n\n#respliting train and test dataset\ntrain_x = combined.iloc[:train_x.shape[0],:].reset_index(drop=True)\ntest_X = combined.iloc[train_x.shape[0]:,:].reset_index(drop=True)","154a4a28":"skim(train_x)","8063a78f":"def highlight_max(s, props=''):\n    return np.where(s == np.nanmax(s.values), props, '')\n\ntrain_x.iloc[:,1:].astype(int).head().style.apply(highlight_max, props='color:Black;background-color:Yellow', axis=1)","8c24779f":"train_x, test_x = load_data()\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Perform label encoding iteratively by looping a categorical variable\nfor c in cat_cols:\n    # Transform data after defining based on training data\n    le = LabelEncoder()\n    le.fit(train_x[c])\n    train_x[c] = le.transform(train_x[c])\n    test_x[c] = le.transform(test_x[c])\n    print(\"{0} = {1}\".format(c,train_x[c].unique()))","81b79897":"sns.set(font_scale=1.4)\nplt.style.use('seaborn-white')\ntrain_x['product'].value_counts().sort_values().plot(kind='barh',figsize=(10, 8))\nplt.xlabel(\"Count\", labelpad=14)\nplt.ylabel(\"Label\", labelpad=14)\nplt.title(\"LabelEncoder\", y=1.02)","3a1e12aa":"skim(train_x)","7bf595ab":"train_x['product'].head(10)","def3e945":"train_x, test_x = load_data()\n\nfrom sklearn.feature_extraction import FeatureHasher\nnum_of_feature = 5\n# Feature hashing iteratively on categorical variables\nfor c in cat_cols:\n    fh = FeatureHasher(n_features=num_of_feature, input_type='string')\n\n    # Convert variable to string and apply FeatureHasher\n    hash_train = fh.transform(train_x[[c]].astype(str).values)\n    hash_test = fh.transform(test_x[[c]].astype(str).values)\n\n    # convert to dataframe\n    hash_train = pd.DataFrame(hash_train.todense(), columns=[f'{c}_{i}' for i in range(num_of_feature)])\n    hash_test = pd.DataFrame(hash_test.todense(), columns=[f'{c}_{i}' for i in range(num_of_feature)])\n    \n    print(c,\"=>\")\n    print(hash_train.head())\n    \n    # Combine with original dataframe\n    train_x = pd.concat([train_x, hash_train], axis=1)\n    test_x = pd.concat([test_x, hash_test], axis=1)\n\n# Delete the original categorical variable\ntrain_x.drop(cat_cols, axis=1, inplace=True)\ntest_x.drop(cat_cols, axis=1, inplace=True)","3a5ad7ac":"skim(train_x)","5fdf3fa3":"train_x.iloc[:,1:].head()","f93daa74":"train_x, test_x = load_data()\n\n# Repeating the variable using the for statement to perform frequency encoding\nfor c in cat_cols:\n    freq = train_x[c].value_counts()\n    # Replaced by the number of occurrences of a category\n    train_x[c] = train_x[c].map(freq)\n    test_x[c] = test_x[c].map(freq)\n    print(c,\"=>\")\n    print('-'*60)\n    print(train_x[c].unique())","66b85e42":"sns.set(font_scale=1.4)\nplt.style.use('seaborn-white')\ntrain_x['product'].value_counts().sort_values().plot(kind='barh',figsize=(10, 8))\nplt.xlabel(\"Count\", labelpad=14)\nplt.ylabel(\"Label\", labelpad=14)\nplt.title(\"FrequencyEncoder\", y=1.02)","370c2789":"skim(train_x)","0821ff8d":"train_x.loc[:,'product'].head()","87cae4bb":"train_x, test_x = load_data()\n\nfrom sklearn.model_selection import KFold\n\n# Target encoding is performed by repeating the variable using the for statement.\nfor c in cat_cols:\n    # Calculate the target mean for each category across the training data\n    data_tmp = pd.DataFrame({c: train_x[c], 'target': train_y})\n    target_mean = data_tmp.groupby(c)['target'].mean()\n\n    # Change the category of test data\n    test_x[c] = test_x[c].map(target_mean)\n\n    # Prepare an array to store values after transforming the training data\n    tmp = np.repeat(np.nan, train_x.shape[0])\n\n    # Split training data\n    kf = KFold(n_splits=4, shuffle=True, random_state=72)\n    for idx_1, idx_2 in kf.split(train_x):\n        # Calculate the mean of each categorical objective variable out of fold\n        target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n        # Store the converted value in the date array\n        tmp[idx_2] = train_x[c].iloc[idx_2].map(target_mean)\n\n    # Change the original variable to the data after conversion\n    train_x[c] = tmp\n    print(c,\"=>\")\n    print('-'*60)\n    print(train_x[c].unique())","edf89c16":"sns.set(font_scale=0.8)\nplt.style.use('seaborn-white')\ntrain_x['product'].value_counts().sort_values().plot(kind='barh',figsize=(10, 8))\nplt.xlabel(\"Count\", labelpad=14)\nplt.ylabel(\"Label\", labelpad=14)\nplt.title(\"Target Encoding\", y=1.02)","4df39383":"skim(train_x)","29c6d5f6":"train_x, test_x = load_data()\n\nfrom sklearn.model_selection import KFold\n\n# Define the folds of cross-validation\nkf = KFold(n_splits=4, shuffle=True, random_state=71)\n\n# Looping a variable to perform target encoding\nfor c in cat_cols:\n\n    # add target\n    data_tmp = pd.DataFrame({c: train_x[c], 'target': train_y})\n    # Prepare an array to store the values after conversion\n    tmp = np.repeat(np.nan, train_x.shape[0])\n\n    # Divide the validation data from the training data\n    for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n        # Calculating the average of the objective variable \n        # for each category on the training data\n        target_mean = data_tmp.iloc[tr_idx].groupby(c)['target'].mean()\n        # Storing values in temporary array after conversion \n        # for validation data\n        tmp[va_idx] = train_x[c].iloc[va_idx].map(target_mean)\n\n    # Change the original variable to the data after conversion\n    train_x[c] = tmp\n    print(c,\"=>\")\n    print('-'*60)\n    print(train_x[c].unique())","27d64c15":"sns.set(font_scale=0.8)\nplt.style.use('seaborn-white')\ntrain_x['product'].value_counts().sort_values().plot(kind='barh',figsize=(10, 8))\nplt.xlabel(\"Count\", labelpad=14)\nplt.ylabel(\"Label\", labelpad=14)\nplt.title(\"Target Encoding\", y=1.02)","3cc963cf":"skim(train_x)","256c94e7":"---------------------------------------------------------------------------\n# Embedding\n\nIt is a method of converting discrete expressions such as words or categorical variables into real vectors in natural language processing. For more details, please refer to the notebook below.\n\n[notebook](https:\/\/www.kaggle.com\/ohseokkim\/word-vectors)\n","e41304ba":"## Using Pandas get_dummies\n\nAll columns specified in the parameter columns are one-hot encoded, and a dataframe combined with the remaining unconverted columns is returned. This function is convenient because it automatically creates a new column name using the original column name and the label name of the categorical variable and returns a dataframe.","eb867079":"------------------------------------------------\n# Feature hashing\n\nIn one-hot encoding, the number of features becomes equal to the number of levels of categories when transformation is performed. Feature hashing, on the other hand, is a transformation that reduces the state of a category. The number of features after transformation is determined first, and the hash bucket is determined for each level using the hash function.\nIt can be used when the number of levels of categorical variables is large and the number of features generated by one-hot encoding is too large.","c6d4496f":"As above, it can be seen that encoding is performed with the frequency and number of appearances of each level.","698d80cf":"# Target Encoding and Data Leakage\n\nIf you simply do target encoding on the entire train dataset, an error occurs. For example, when there is only one row data belonging to a specific level, the target encoding result of that level becomes the value of the target variable itself.\n\nAs an extreme example, let's say you want to apply target encoding for a column, such as ID, which is unique in each row.\nIn other words, the conversion result completely matches the column of the target variable, so even if you create a model in this state, it will only be a model that sees and returns only the variable. Of course, these transformations cannot be applied to the test data, so this model is meaningless in the end.\n\nThe content just described is of course an extreme example, but if the number of row data at a certain level is small, the return result will strongly reflect the target variable value of the row to be converted. That is, in the learning process, the target variable that was originally to be covered was leaked as a result of target encoding, and some of the answers of the learning data were visible.\n\nTherefore, if target encoding is performed and the model is overfitted, it is necessary to check whether the target encoding is appropriate.\n\n> In statistics and machine learning, leakage (also known as data leakage or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the model's utility when run in a production environment.\n> \n> Leakage is often subtle and indirect, making it hard to detect and eliminate. Leakage can cause a statistician or modeler to select a suboptimal model, which could be outperformed by a leakage-free model.\n\nRef: https:\/\/en.wikipedia.org\/wiki\/Leakage_(machine_learning)\n\n\n**If you want to know more about data leakage, please refer to the notebook below.**\n\n[Notebook](https:\/\/www.kaggle.com\/ohseokkim\/data-leakage-tricky-but-important)\n","2aada31a":"Label encoding simply converts a string to a numeric categorical value. However, there is a problem that these numbers exist in the order of largest and smallest. Because of these characteristics, label encoding should not be applied to machine learning algorithms such as linear regression. Tree-like machine learning algorithms do not have a problem because they do not reflect these characteristics of numbers.","fe9a796e":"------------------------------------------------\n# Target Encoding\n\n![](https:\/\/www.renom.jp\/notebooks\/tutorial\/preprocessing\/category_encoding\/renom_cat_target.png)\n\nReference: https:\/\/miro.medium.com","c465338a":"-----------------------------------------------------------\n# Label encoding","3211d17c":"---------------------------------------------------------\n# One-hot encoding\n\n![](https:\/\/miro.medium.com\/max\/1400\/1*80tflY8LxDFRmkD16u25RQ.png)\nPicture credit: https:\/\/miro.medium.com\n\nOne-Hot encoding is a vector representation method of words in which the size of a word set is the dimension of a vector, a value of 1 is assigned to the index of the word to be expressed, and 0 is assigned to other indices. A vector expressed in this way is called a one-hot vector.\n\nOne-hot encoding consists of two processes.\n1. Give each word a unique index. (integer encoding)\n2. Give 1 to the index position of the word you want to express, and 0 to the index position of other words.\n\nOn the other hand, when there are n levels of categorical variables, multicollinearity occurs when the number of dummy variables are created as many as the number of levels. To prevent this, a method of creating n-1 dummy variables can be used.\n\n\n**Multicollinearity:**\n> In statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy. In this situation, the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors. That is, a multivariate regression model with collinear predictors can indicate how well the entire bundle of predictors predicts the outcome variable, but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others.\n\nRef: https:\/\/en.wikipedia.org\/wiki","07c4a285":"# Target encoding (Version 1)\n\nThis is a method of converting a categorical variable into a numeric variable using a target variable. That is, the average value of the target variable in each level group of the categorical variable is aggregated and replaced with that value.","34971e44":"## Using sklearn OneHotEncoder\n\nIn OneHotEncoder class, the return value of the transform method is returned as a numpy array, so the original column name and level information are lost. So it's a bit tricky to deal with as you have to return it back to the dataframe when concatenating it. On the other hand, since a sparse matrix is \u200b\u200breturned by specifying the sparse parameter value as True, memory can be saved when one-hot encoding of categorical variables with multiple levels is performed.","204276a4":"**In the train data frame, the data type of the 'product' feature is object. Let's encode this feature in various ways.**","bb3c324b":"----------------------------------------------------------------------\n# Frequency encoding","0962aabb":"![](https:\/\/miro.medium.com\/max\/386\/1*Yp6r7m82IoSnnZDPpDpYNw.png)\nPicture Credit: https:\/\/miro.medium.com","4978c517":"# Target Encoding (Version-2)\n\nIn the case of performing cross verification, it must be converted again for each fold of cross verification. This is because conversion is required again to prevent the target variable of the verification data from being included in the variable.","51464be8":"![](https:\/\/www.elastic.co\/guide\/en\/machine-learning\/current\/images\/frequency-encoding.jpg)\n\nPicture Credit: https:\/\/www.elastic.co\n\nFrequency encoding is a method of substituting a categorical variable with the number or frequency of appearance of each level. It is valid when there is a relationship between the frequency of appearance of each level and the target variable.\nAlso, as a variant of label encoding, it can be used to create an index that is listed in the order of appearance frequency rather than the index in alphabetical order.","5c6d1b64":"As above, a new column is not newly created, and the label encoding can be checked for each column level of the product.","43100d38":"![](https:\/\/wush978.github.io\/FeatureHashing\/assets\/img\/hash.png)\n\nPicture Credit: https:\/\/wush978.github.io","12fea5fa":"---------------------------------------------------------------------------------------------\n# Conclusion\n\nEach encoding method has advantages and disadvantages. And, there is an encoding method that cannot be applied according to the machine learning model. Therefore, it seems that the encoding method should be selected in consideration of the model to be applied, the characteristics of the dataset, and the environment such as memory.","7d1cacd3":"-------------------------------------------\n# Introduction\n\n![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/03\/one-hot-vs-label-encoder.png)\n\nPicture Credit: https:\/\/cdn.analyticsvidhya.com\n\nSince categorical variables cannot be used for analysis as they are in many machine learning models, they must be converted into a suitable form for each model. In addition to representing a variable as a string, it should be treated as a categorical type when the size or order of the values is meaningless even if it is a number in terms of data.\n\nIn this notebook, we examine ways to encode these categorical categories.\n","3a1bf93e":"-----------------------------\n# Setting Up","5da57749":"As above, after encoding, you can see that the new columns increase as much as the number of levels of the product feature. Also, it can be seen that only the corresponding column is set to 1 in the level."}}