{"cell_type":{"5854dd56":"code","93efc442":"code","c0daa2b5":"code","33c3e9e9":"code","531a8c31":"code","99cade5a":"code","c3eb6392":"code","59952d08":"code","8f1d1317":"code","718e7e77":"code","116f4527":"code","db4e44ac":"code","5901ccd0":"code","89c39d75":"code","892c3c47":"code","afb3407a":"code","53106367":"code","caf4ba50":"code","869a4ef7":"code","527d214f":"code","38179ea2":"code","0b2c8338":"code","8b01a6be":"code","270342b3":"code","f1eb466a":"code","4772c798":"code","ba2d141c":"code","c70e0996":"code","499f20c2":"code","d9e568ef":"code","34132ea8":"code","f0eab8c6":"code","5a947574":"code","6bbabd06":"code","f55c5722":"code","95195f1b":"code","37a1cb6c":"code","2e713716":"markdown"},"source":{"5854dd56":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport re\nimport tensorflow.keras.backend as K","93efc442":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\n#REPLICAS = 8\nprint(f'REPLICAS: {REPLICAS}')","c0daa2b5":"df_train = pd.read_csv(\"\/kaggle\/input\/siim-isic-melanoma-classification\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/siim-isic-melanoma-classification\/test.csv\")\nsample_sub = pd.read_csv(\"\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv\")","33c3e9e9":"FOLDS = 5\nIMG_SIZE = 512\nBATCH_SIZE = 32\nEPOCHS = 12","531a8c31":"import tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nGCS_PATH2 = KaggleDatasets().get_gcs_path(\"isic2019-512x512\")\nGCS_PATH1 = KaggleDatasets().get_gcs_path(\"melanoma-512x512\")","99cade5a":"train_filenames1 = tf.io.gfile.glob(GCS_PATH1 + '\/train*.tfrec')\ntrain_filenames2 = tf.io.gfile.glob(GCS_PATH2 + '\/train*.tfrec')\ntest_filenames = tf.io.gfile.glob(GCS_PATH1 + '\/test*.tfrec')\ndata_filenames = train_filenames1 + train_filenames2","c3eb6392":"from sklearn.model_selection import train_test_split\ntrain_filenames, valid_filenames = train_test_split(data_filenames, test_size=0.2, random_state=0, shuffle=True)","59952d08":"def read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"target\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = example['image']\n    label = example['target']\n    return image, label # returns a dataset of (image, label) pairs\n\ndef read_unlabeled_tfrecord(example, return_image_name=True):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = example['image']\n    idnum = example['image_name']\n    return image, idnum if return_image_name else 0\n\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset","8f1d1317":" def decode_image(image_data, augment=False):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0  # convert image to floats in [0, 1] range\n    if augment:\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_flip_up_down(image)\n        image = tf.image.random_saturation(image, 0, 2)\n        image = tf.image.rot90(image)\n    image = tf.reshape(image, [IMG_SIZE,IMG_SIZE, 3]) # explicit size needed for TPU\n    return image\n\ndef get_training_dataset():\n    dataset = tf.data.TFRecordDataset(train_filenames, num_parallel_reads=AUTO)\n    dataset = dataset.repeat() \n    dataset = dataset.shuffle(1024*8)\n    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n    dataset = dataset.map(lambda img, imgname_or_label: (decode_image(img, augment=True), imgname_or_label), num_parallel_calls=AUTO)\n    dataset = dataset.batch(BATCH_SIZE * REPLICAS)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_val_dataset():\n    dataset = tf.data.TFRecordDataset(valid_filenames, num_parallel_reads=AUTO)\n    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n    dataset = dataset.map(lambda img, imgname_or_label: (decode_image(img, augment=False), imgname_or_label), num_parallel_calls=AUTO)\n    dataset = dataset.batch(BATCH_SIZE * REPLICAS)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = tf.data.TFRecordDataset(test_filenames, num_parallel_reads=AUTO)\n    dataset = dataset.repeat() \n    dataset = dataset.map(lambda example: read_unlabeled_tfrecord(example, False), num_parallel_calls=AUTO)\n    dataset = dataset.map(lambda img, imgname_or_label: (decode_image(img, augment=False), imgname_or_label), num_parallel_calls=AUTO)\n    dataset = dataset.batch(BATCH_SIZE * REPLICAS)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","718e7e77":"def get_lr_callback(batch_size=8):\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * REPLICAS * batch_size\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback","116f4527":"def build_model(dim=512):\n    inp = tf.keras.layers.Input(shape=(dim,dim,3))\n    base = efn.EfficientNetB3(input_shape=(dim,dim,3),weights='imagenet',include_top=False)\n    x = base(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n    model = tf.keras.Model(inputs=inp,outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05) \n    model.compile(optimizer=opt,loss=loss,metrics=['AUC'])\n    return model","db4e44ac":"!pip install yapl==0.1.2 efficientnet > \/dev\/null","5901ccd0":"#model 3\nimport efficientnet.tfkeras as efn\ninput_shape = (512, 512, 3)\ndef create_model():\n    model = tf.keras.Sequential([\n        efn.EfficientNetB4(\n                        input_shape=input_shape,\n                        weights='imagenet',\n                        include_top=False\n                    ),\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    \n    return model","89c39d75":"# with strategy.scope():\n#     model = create_model()\n# optimizer = tf.keras.optimizers.Adam(lr=0.001)\n# model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n# model.summary()","892c3c47":"import math\ncallbacks = [get_lr_callback(BATCH_SIZE)] \nstep_per_epoch = count_data_items(train_filenames)\/BATCH_SIZE\/\/REPLICAS","afb3407a":"# warmup_history = model.fit(get_training_dataset(), \n#                            steps_per_epoch=step_per_epoch, \n#                            validation_data=get_val_dataset(),\n#                             epochs=1, \n#                             verbose=1, callbacks=callbacks).history","53106367":"test_dataset = get_test_dataset(ordered=True)","caf4ba50":"# test_images_ds = test_dataset.map(lambda image, idnum: image)\n# ct_test = count_data_items(test_filenames)\n# STEPS = ct_test\/BATCH_SIZE\/4\/REPLICAS\n# probabilities = model.predict(test_images_ds, steps=STEPS).flatten()\n# print(probabilities)","869a4ef7":"# test_imgs = test_dataset.map(lambda images, ids: images)\n# img_ids_ds = test_dataset.map(lambda images, ids: ids).unbatch()\n\n# img_ids = []\n# for coutner, ids in enumerate(img_ids_ds):\n#     if coutner%500 == 0:\n#         print(coutner)\n#     img_ids.append(ids.numpy())\n\n# img_ids = np.array(img_ids).astype('U')","527d214f":"# sample_sub = sample_sub.set_index(\"image_name\").transpose().reindex(columns=list(img_ids)).transpose()\n# sample_sub[\"target\"] = probabilities\n# sample_sub.to_csv(\"submission.csv\")","38179ea2":"def get_dataset(filename, shuffle=False, repeat=False, return_img_name=False, augment=False, labeled=False, batch_size=32, dim=512):\n    dataset = tf.data.TFRecordDataset(filename, num_parallel_reads=AUTO)\n    if repeat:\n        dataset = dataset.repeat()\n    if shuffle:\n        dataset = dataset.shuffle(1024*8)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        dataset = dataset.with_options(opt)\n    if labeled:\n        dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n    else:\n         dataset = dataset.map(lambda example: read_unlabeled_tfrecord(example, return_img_name), num_parallel_calls=AUTO)\n    dataset = dataset.map(lambda img, imgname_or_label: (decode_image(img, augment=augment), imgname_or_label), num_parallel_calls=AUTO)\n    dataset = dataset.batch(batch_size * REPLICAS)\n    dataset = dataset.prefetch(AUTO)\n    return dataset","0b2c8338":"len(train_filenames)","8b01a6be":"FOLDS = 5\nIMG_SIZE = 512\nBATCH_SIZE = 32\nEPOCHS = 12\nweights = 1\/6","270342b3":"from sklearn.model_selection import KFold\nkfold = KFold(n_splits=FOLDS, shuffle=True, random_state=0)","f1eb466a":"TTA = 11\npreds = np.zeros((count_data_items(test_filenames),1))\nfor fold, (train_file, valid_file) in enumerate(kfold.split(data_filenames)):\n    print(fold)\n    training_dataset = get_dataset([data_filenames[x] for x in train_file], augment=True, shuffle=True, repeat=True, labeled=True)\n    valid_dataset = get_dataset([data_filenames[x] for x in valid_file], augment=True, shuffle=False, repeat=False, labeled=False)\n    K.clear_session()\n    with strategy.scope():\n        model = create_model()\n    if fold == 3:\n        print(\"ok\")\n    optimizer = tf.keras.optimizers.Adam(lr=0.001)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    callbacks = [get_lr_callback(BATCH_SIZE)] \n    steps_per_epoch = count_data_items([data_filenames[x] for x in train_file])\/BATCH_SIZE\/\/REPLICAS\n    sv = tf.keras.callbacks.ModelCheckpoint('fold-%i.h5'%fold, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n    callbacks.append(sv)\n    warmup_history = model.fit(training_dataset, steps_per_epoch=steps_per_epoch, validation_data=valid_dataset, epochs=12, verbose=1, callbacks=callbacks).history\n    if fold == 3:\n        print(\"whatup\")\n\n\n    test_dataset = get_dataset(test_filenames, augment=True, repeat=True, batch_size=BATCH_SIZE*4)\n    ct_valid = count_data_items(test_filenames)\n    if fold == 3:\n        print(\"hello\")\n    STEPS = TTA * ct_valid\/32\/4\/REPLICAS\n    pred = model.predict(test_dataset,steps=STEPS,verbose=1)[:TTA*ct_valid,]\n    preds[:, 0] += np.mean(pred.reshape((ct_valid,TTA),order='F'),axis=1)","4772c798":"preds = np.concatenate(preds)","ba2d141c":"preds = preds\/6","c70e0996":"ds = get_dataset(test_filenames, augment=False, repeat=False,\n                 labeled=False, return_img_name=True)\n\nimage_names = np.array([img_name.numpy().decode(\"utf-8\") \n                        for img, img_name in iter(ds.unbatch())])","499f20c2":"submission = pd.DataFrame(dict(image_name=list(image_names), target=preds))\nsubmission = submission.sort_values('image_name')\nsubmission = submission.set_index(\"image_name\")\nsubmission.to_csv('submission.csv', index=True)\nsubmission.head()","d9e568ef":"# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n# train_datagen = ImageDataGenerator(\n#         shear_range=0.1,\n#         zoom_range=0.1,\n#         horizontal_flip=True,\n#         rotation_range=10.,\n#         fill_mode='reflect',\n#         width_shift_range = 0.1, \n#         height_shift_range = 0.1)\n# train_datagen.flow(get_dataset(train_filenames, augment=False, shuffle=False, repeat=True, labeled=True))","34132ea8":"# from sklearn.model_selection import train_test_split\n# train_file, valid_file = train_test_split(train_filenames, test_size=0.5, random_state=0)","f0eab8c6":"# training_dataset = get_dataset(train_file, augment=True, shuffle=True, repeat=True, labeled=True)\n# valid_dataset = get_dataset(valid_file, augment=True, shuffle=False, repeat=True, labeled=False, batch_size=32*5)","5a947574":"# TTA = 10\n# oof_pred = []\n# test_dataset = get_dataset(test_filenames, augment=True, repeat=True, batch_size=BATCH_SIZE*4)\n# ct_valid = count_data_items(test_filenames)\n# STEPS = TTA * ct_valid\/32\/4\/REPLICAS\n# pred = model.predict(test_dataset,steps=STEPS,verbose=1)       \n# preds = pred[:TTA*ct_valid,]","6bbabd06":"# preds.shape","f55c5722":"# oof_pred.append( np.mean(preds.reshape((ct_valid,TTA),order='F'),axis=1) ) ","95195f1b":"np.zeros((count_data_items(test_filenames),1))","37a1cb6c":"len(sample_sub)","2e713716":"# KFOLD"}}