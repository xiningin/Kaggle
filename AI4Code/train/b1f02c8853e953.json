{"cell_type":{"f70e0088":"code","5f34e94a":"code","5e7e9421":"code","3ad8bf68":"code","9587791a":"code","d0cdb2d4":"code","ddf00958":"code","8d340977":"code","40bdb9df":"code","108e3867":"code","f524502c":"code","60d0cb26":"code","e2d1d42e":"code","9398bb4d":"code","27017f5d":"code","6e31a6f7":"code","89ebb44a":"code","48b1ee96":"code","b5f93315":"markdown","8a2226bf":"markdown","866c8734":"markdown","3a0b27f4":"markdown","4886376e":"markdown","e2a68445":"markdown"},"source":{"f70e0088":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nprint(\"Load Packages\")\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom fastai.tabular import *  #fast.ai tabular models\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nprint(\"Print Directories\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5f34e94a":"# Any results you write to the current directory are saved as output.\n#read in training data, outcomes and testing  \nprint(\"load train, test and submission\")\ntrain = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-1\/train.csv') \ntest = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-1\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-1\/submission.csv')\nprint(train.shape)\nprint(test.shape)\nprint(submission.shape)","5e7e9421":"train = train.rename(columns={'ConfirmedCases': 'ConfirmedCases_old', 'Fatalities': 'Fatalities_old'})\ntrain.head()","3ad8bf68":"test.head()","9587791a":"submission.head()","d0cdb2d4":"#separate out the first date available to both train and test \n#that is jan 22 2020 for train and march 12 2020 for test \n#first_day_train = train[train.Date == '2020-01-22']\n#first_day_test = train[train.Date == '2020-03-12']\n\n#drop (keep needed) unneeded variables in both datasets \n#first_day_train=pd.DataFrame(first_day_train,columns=[\"Province\/State\",\"Country\/Region\",\"ConfirmedCases\",\"Fatalities\"])\n#first_day_test=pd.DataFrame(first_day_test,columns=[\"Province\/State\",\"Country\/Region\",\"ConfirmedCases\",\"Fatalities\"])\n\n#change names to first day confirmed and first day fatalities \n#first_day_train.rename(columns={'ConfirmedCases': 'FirstDayConfirmed', 'Fatalities': 'FirstDayFatalities'}, inplace=True)\n#first_day_test.rename(columns={'ConfirmedCases': 'FirstDayConfirmed', 'Fatalities': 'FirstDayFatalities'}, inplace=True)\n\n#merge both datasets to add this new variable \n#train = pd.merge(train, first_day_train, on=['Province\/State', 'Country\/Region'])\n#test = pd.merge(test, first_day_test, on=['Province\/State', 'Country\/Region'])\n\n#train.head()","ddf00958":"#Potentially sort the training database and prepare to take a new type of validation data set\nprint(\"sort the train file\")\nmake_date(train, 'Date')\nmake_date(test, 'Date')","8d340977":"print(\"delete columns that might not be useful\")\n#train=train.drop(['Id', 'Province\/State', 'Country\/Region', 'Lat', 'Long', 'Date','ConfirmedCases', 'Fatalities'],axis=1)\n#test=test.drop(['ForecastId', 'Province\/State', 'Country\/Region', 'Lat', 'Long','Date'],axis=1)\n\n#create date variables in train and test \nprint(\"create time variables in both train and test\")\ntrain_data = add_datepart(train, 'Date',drop=False)\ntest_data = add_datepart(test, 'Date',drop=False)\n\n#add fatalities to test\ntest_data['Fatalities_old'] = 0\ntest_data['ConfirmedCases_old'] = 0\n\n#procedures for cleaning data \nprint(\"set the procedures for cleaning\")\nprocs = [FillMissing, Categorify, Normalize]","40bdb9df":"#examine data for train\ntrain_data.dtypes\n","108e3867":"#examine data for test \ntest_data.dtypes","f524502c":"#sort the data \nprint(\"Sort the training data set for validation\")\ntrain_data = train_data.sort_values(by='Date', ascending=True)\ntrain_data = train_data.reset_index(drop=True)","60d0cb26":"##\n#fastLearner\n#takes a train and test dataframe object and outputs the test file with predictions\n#input: train and test pandas dataframe objects, size of validation set as numeric, \n#and the dep variable name \n#output: pandas dataframe object test with predictions \n##\ndef fastLearning(df1,df2,size,dep):\n    #instantiate variables  \n    train_data = df1\n    test_data =df2\n    val_size = size \n    path = ''\n    deep_var=dep\n    \n    #model parameters \n    # 'ConfirmedCases'\n    dep_var = deep_var\n    cat_names = ['Province\/State', 'Country\/Region','Is_month_end',\n             'Is_month_start','Is_quarter_end','Is_quarter_start','Is_year_end']\n    cont_names = ['Lat', 'Long','Year', \n              'Month', 'Week', \n              'Day', 'Dayofweek', \n              'Dayofyear', 'Elapsed']\n    \n    #Start index for creating a validation set from train\n    start_indx = len(train_data) - int(len(train_data) * val_size)\n\n    #End index for creating a validation set from train\n    end_indx = len(train_data)\n    \n    #TabularList for Validation\n    #val = (TabularList.from_df(train_data.iloc[start_indx:end_indx].copy(), path=path, cat_names=cat_names, cont_names=cont_names))\n    test = (TabularList.from_df(test_data, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs))\n    data = (TabularList.from_df(train_data, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs)\n                           .split_by_idx(list(range(start_indx,end_indx)))\n                           .label_from_df(cols=dep_var)\n                           .add_test(test)\n                           .databunch(bs=20))\n    #create learner \n    learn = tabular_learner(data, layers=[300,200], emb_drop=0.04,metrics= [rmse])\n\n    #Exploring the learning rates\n    #learn.lr_find(start_lr = 1e-03,end_lr = 1e+02, num_it = 100)\n    #learn.lr_find()\n    #learn.recorder.plot()\n    \n    #Fitting data and training the network\n    learn.fit_one_cycle(10)\n\n    #save stage 1 learning \n    learn.save('stage-1')\n\n    #unfreeze the learner\n    learn.unfreeze()\n\n    #Fitting data and training the network\n    learn.fit_one_cycle(2)\n\n    #apply learning model to test \n    #print(\"#apply learning model to test \")\n    test_predictions = learn.get_preds(ds_type=DatasetType.Test)[0]\n\n    #Converting the tensor output to a list of predicted values\n    #print(\"Converting the tensor output to a list of predicted values\")\n    test_predictions = [i[0] for i in test_predictions.tolist()]\n\n    #Converting the prediction to . a dataframe\n    test_predictions = pd.DataFrame(test_predictions, columns = [dep_var+\"_new\"])\n    \n    return test_predictions\n","e2d1d42e":"################# Iterate Tablular Learner ##############\n#make state\/country column in both train and test \n#train_data[\"state_country\"] = train_data[\"Province\/State\"].astype(str) + train_data[\"Country\/Region\"].astype(str)\n#test_data[\"state_country\"] = test_data[\"Province\/State\"].astype(str) + test_data[\"Country\/Region\"].astype(str)\n#test_data.head()\n\n#ensure both have state_country column \n#categories=test_data.groupby('state_country')['state_country'].count() #true \n#categories=pd.DataFrame(categories)\n\n#view the categories\n#print(categories.index)\n\n#subset the file\n#train_data=train_data[train_data.state_country==\"BeijingChina\"]\n#test_data=test_data[test_data.state_country==\"BeijingChina\"]\n\n#test_data.head()","9398bb4d":"################# Iterate Tablular Learner ##############\n#make state\/country column in both train and test \n#train_data[\"state_country\"] = train_data[\"Province\/State\"].astype(str) + train_data[\"Country\/Region\"].astype(str)\n#test_data[\"state_country\"] = test_data[\"Province\/State\"].astype(str) + test_data[\"Country\/Region\"].astype(str)\n#test_data.head()\n\n#ensure both have state_country column \ncategories=test_data.groupby(\"Country\/Region\")[\"Country\/Region\"].count() #true \ncategories=pd.DataFrame(categories)\n#print(categories.index)\n\n#create holding dataframe for test\nconfirmed_holding = pd.DataFrame()\n#fatalities_holding = pd.DataFrame()\n#train_data.head(50)","27017f5d":"#for each state_country run the main program \nfor i in categories.index:\n    #print name for testing \n    #print(i)\n    #subset both train and testing data \n    train_temp=train_data[train_data[\"Country\/Region\"]==i]\n    test_temp=test_data[test_data[\"Country\/Region\"]==i]\n    \n    #run main AI function for the portion of data \n    confirmed_file=fastLearning(df1=train_temp,df2=test_temp,size=.1,dep='ConfirmedCases_old')\n    fatalities_file=fastLearning(df1=train_temp,df2=test_temp,size=.1,dep='Fatalities_old')\n    \n    #make test file \n    #test_temp = test_temp.assign(pd.Series(Fatalities_old_new=fatalities_file[\"Fatalities_old_new\"]))\n    fatalities_file = fatalities_file.set_index(test_temp.index)\n    test_temp[\"Fatalities_old_new\"] = fatalities_file\n\n    confirmed_file = confirmed_file.set_index(test_temp.index)\n    test_temp[\"Confirmed_old_new\"] = confirmed_file\n    \n    #append output file to the holding dataframe \n    confirmed_holding=pd.concat([confirmed_holding,test_temp],ignore_index=True)\n    #fatalities_holding=pd.concat([fatalities_holding,fatalities_file],ignore_index=True)\n    \n     \n\n#ensure test and holding dataframe are the same    \n#holding.shape==test_data.shape","6e31a6f7":"confirmed_holding","89ebb44a":"#make submission file \nfinal=confirmed_holding[[\"ForecastId\",\"Confirmed_old_new\",\"Fatalities_old_new\"]]\nfinal = final.rename(columns={'Confirmed_old_new': 'ConfirmedCases', 'Fatalities_old_new': 'Fatalities'})\nfinal.to_csv('submission.csv',index=False)\nfinal.head()\n","48b1ee96":"#make test file \nconfirmed_holding = confirmed_holding.rename(columns={'Confirmed_old_new': 'ConfirmedCases', 'Fatalities_old_new': 'Fatalities'})\ndel confirmed_holding['ConfirmedCases_old']\ndel confirmed_holding['Fatalities_old']\nconfirmed_holding.to_csv('complete_test.csv',index=False)","b5f93315":"# Build Testing Files\n","8a2226bf":"# Read in Data ","866c8734":"# Build the AI function for Looping through Countries\/States","3a0b27f4":"# Train AI and Forecast - One region at a time ","4886376e":"# Create Submission File ","e2a68445":"# Clean Data:\nConvert date into a date variable and turn them. Fill missing variables, categorify and normalize. Sort data for time series forecasting "}}