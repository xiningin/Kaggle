{"cell_type":{"3ee74868":"code","6ea79eb5":"code","a588b913":"code","07f140dc":"code","30060a3f":"code","b8236526":"code","6caedbb7":"code","91e59a7f":"code","73e1d92b":"code","e7403d94":"code","7cdad684":"code","9fe18e94":"code","99a8f9d3":"code","4da1465d":"code","b27fa272":"code","045ba36b":"code","0a97f1c7":"code","334ba5d1":"code","1aa51f33":"code","941e99d3":"code","dd696fa9":"code","d1f85a53":"code","e3e2bf9a":"code","9e6a3d61":"code","fac7f936":"code","9c5acb9e":"code","229bcd70":"code","7723543e":"code","4c6d3a69":"code","a324c357":"code","794c5552":"code","d232a129":"code","7b7392bd":"code","deff1076":"code","e976f5b3":"markdown","c1560c64":"markdown","63179b15":"markdown","472bde60":"markdown","2dd1aa00":"markdown","a0de6691":"markdown","c90ce700":"markdown","db09e47f":"markdown","e768fb54":"markdown","8438c984":"markdown","3b268547":"markdown","593855be":"markdown","fe86a03d":"markdown","a4394e63":"markdown","129de049":"markdown","194ebd72":"markdown","f8a3bb30":"markdown","71d9b7c6":"markdown","eb6b8b0b":"markdown","fd22cbd6":"markdown","bbb05eb5":"markdown","8f626833":"markdown","22a0b957":"markdown","1eac5d47":"markdown","fc0be378":"markdown","90166e23":"markdown","ab46e0a8":"markdown","dcd667e5":"markdown","bc7bc528":"markdown","0b5c55ed":"markdown","7b5aff0e":"markdown","007faf71":"markdown","4d168499":"markdown"},"source":{"3ee74868":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6ea79eb5":"\nimport matplotlib.pyplot as plt\nimport itertools\nfrom wordcloud import WordCloud, STOPWORDS\n\n\nplt.figure(figsize=(20,12))\nfigsize=(12,5)","a588b913":"tweets_df = pd.read_csv('\/kaggle\/input\/tweets-of-us-senators\/senators.csv')","07f140dc":"#Exploring the tweets\nprint(tweets_df.shape)\ntweets_df.head()","30060a3f":"tweets_df.info()","b8236526":"tweets_df['created_at']= pd.to_datetime(tweets_df['created_at'])\nprint(\"First tweet was done at\", tweets_df['created_at'].min())\nprint(\"The latest tweet is \", tweets_df['created_at'].max())","6caedbb7":"tweets_df['activity'] = tweets_df['replies']+tweets_df['favorites']+tweets_df['retweets']","91e59a7f":"#getting the viral tweets - find the value which is greater than 99.99% of the tweets\nprint(tweets_df['activity'].quantile(0.9999))","73e1d92b":"plot_viral = tweets_df.loc[tweets_df['activity']>600641]\nprint(plot_viral.shape)","e7403d94":"fig, axes = plt.subplots(ncols=3,figsize=figsize)\nplot_viral['user'].value_counts().plot(kind='bar', ax = axes[0],subplots=True)\nplot_viral['state'].value_counts().plot(kind='bar',ax = axes[1],subplots=True)\nplot_viral['party'].value_counts().plot(kind='bar',ax = axes[2],subplots=True)","7cdad684":"#Exploring the value counts by Party\ntweets_df['party'].value_counts()\ntweets_df['party'].value_counts().plot(kind='bar')","9fe18e94":"#Exploring the value counts by State\ntweets_df['state'].value_counts()\ntweets_df['state'].value_counts().plot(kind='bar',figsize=figsize)","99a8f9d3":"states_df = pd.read_csv('\/kaggle\/input\/states-latitude\/States.csv', names=['state', 'latitude','longitude', 'desc'])\nstates_df = states_df[1:]\nnew_df = tweets_df.merge(states_df, on='state')\nnew_df.head()","4da1465d":"import folium\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\n# Create a map\nm_1 = folium.Map(location=[37.0902,-95.7129], tiles='cartodbpositron', zoom_start=4)\n\n# Add a heatmap to the base map\nHeatMap(data=new_df[['latitude', 'longitude']], radius=50).add_to(m_1)\n\n# Display the map\nm_1","b27fa272":"# # Create the map\n# import math\n# m_3 = folium.Map(location=[37.0902,-95.7129], tiles='cartodbpositron', zoom_start=13)\n\n# # Add points to the map\n# mc = MarkerCluster()\n# for idx, row in new_df.iterrows():\n#     mc.add_child(Marker([row['latitude'], row['longitude']]))\n# m_3.add_child(mc)\n\n# # Display the map\n# m_3","045ba36b":"tweets_df['user'].unique().size","0a97f1c7":"tweets_by_user = pd.DataFrame(tweets_df['user'].value_counts())\ntweets_by_user.head()","334ba5d1":"tweets_by_user.describe()","1aa51f33":"user_count_tweets = tweets_df.groupby('user').count()['text']\nuser_count_tweets.nlargest(25).plot(kind='bar',figsize=figsize)\nplt.show()\nuser_count_tweets = tweets_df.groupby('user').sum()['activity']\nuser_count_tweets.nlargest(10).plot(kind='bar',figsize=figsize)\n","941e99d3":"def senator_analytics(senatorName):\n    senator_tweets = tweets_df.loc[tweets_df['user']==senatorName]\n    print('The first tweet of Senator Udall is', senator_tweets['created_at'].min())\n    print('The last tweet of Senator Udall is', senator_tweets['created_at'].max())\n    senator_tweets['month_year']= senator_tweets['created_at'].dt.to_period('M')\n    senator_tweets['month_year'].value_counts().plot(kind='bar', figsize=figsize)\n    return senator_tweets","dd696fa9":"tom_udall_df = senator_analytics('SenatorTomUdall')","d1f85a53":"import re\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ndef preprocessing_tweets(tweet):\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    tweet = tweet.replace(\"\ufffd\",\"\")\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https:(\\\/\\\/t\\.co\\\/([A-Za-z0-9]|[A-Za-z]){10})', '', tweet)\n    # remove references\n    tweet = re.sub(r'@[\\s]+', '', tweet)\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    tokens = nlp(tweet)\n    new_tweet = [];\n    for token in tokens:\n        if not (token.is_punct or token.is_stop or token.text in ['amp']):\n            new_tweet.append(token.lemma_)\n    return ' '.join(new_tweet)\ntom_udall_df['processed_tweet']= tom_udall_df['text'].apply(preprocessing_tweets)","e3e2bf9a":"enitre_tweets = tom_udall_df['processed_tweet'].tolist()\ns='!!'\nenitre_tweets = s.join(enitre_tweets)\nfrom collections import Counter\nword_count = Counter(enitre_tweets.split( ))\nimport itertools\n#Take the top N items\ntop_N_dict = dict(itertools.islice(word_count.items(), 15))\ntop_N_dict= {k: v for k, v in sorted(top_N_dict.items(), key=lambda item: item[1], reverse=True)}\n#freq dictionary\nprint(top_N_dict)\nplt.figure(figsize=(20,5))\nplt.bar(x=top_N_dict.keys(),height=top_N_dict.values())\n","9e6a3d61":"stopwords = set(STOPWORDS)\nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10,collocation_threshold=2).generate(enitre_tweets)\nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()","fac7f936":"def advanced_senator_analytics(senatorName,fieldName='processed_tweet',fn=preprocessing_tweets):\n    senator_df = tweets_df.loc[tweets_df['user']==senatorName]\n    senator_df[fieldName]= senator_df['text'].apply(fn)\n    enitre_tweets = senator_df[fieldName].tolist()\n    s='!!'\n    enitre_tweets = s.join(enitre_tweets)\n    from collections import Counter\n    word_count = Counter(enitre_tweets.split( ))\n    wordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10,collocation_threshold=2).generate(enitre_tweets)\n    plt.figure(figsize = (8, 8), facecolor = None)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad = 0)\n    plt.show()\n    #Take the top N items\n    top_N_dict = dict(itertools.islice(word_count.items(), 15))\n    top_N_dict= {k: v for k, v in sorted(top_N_dict.items(), key=lambda item: item[1], reverse=True)}\n    #freq dictionary\n    plt.figure(figsize=(20,5))\n    plt.bar(x=top_N_dict.keys(),height=top_N_dict.values(),)\n    return senator_df\n    ","9c5acb9e":"sanders_df = advanced_senator_analytics('SenSanders')","229bcd70":"def extract_pos(tweet, pos_tags=['NOUN']):\n    tweet = re.sub(r'https:(\\\/\\\/t\\.co\\\/([A-Za-z0-9]|[A-Za-z]){10})', '', tweet)\n    doc = nlp(tweet)\n    new_tweet = [];\n    for token in doc:\n        if token.pos_ in pos_tags and token.text not in ['http','https','httpst','https ','amp']:\n            new_tweet.append(token.text)\n    return ' '.join(new_tweet)","7723543e":"advanced_senator_analytics('SenSanders','pos_tweet',extract_pos)","4c6d3a69":"advanced_senator_analytics('SenKamalaHarris','pos_tweet',extract_pos)","a324c357":"def extract_entities(tweet):\n    tweet = re.sub(r'https:(\\\/\\\/t\\.co\\\/([A-Za-z0-9]|[A-Za-z]){10})', '', tweet)\n    doc = nlp(tweet)\n    new_tweet = [];\n    for token in doc.ents:\n        if (token.label_ not in ['DATE']):\n            new_tweet.append(token.text)\n    return ' '.join(new_tweet)","794c5552":"sanders_df = advanced_senator_analytics('SenSanders','entity_tweet',extract_entities)","d232a129":"!pip install YAKE","7b7392bd":"import yake\ndef extract_keywords(tweet):\n    language = \"en\"\n    max_ngram_size = 3\n    deduplication_threshold = 0.5\n    numOfKeywords = 20\n    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n    keywords = custom_kw_extractor.extract_keywords(tweet)\n    if len(keywords) > 1 :\n        length = 2 if len(keywords) > 1 else 1\n        keywords = list(zip(*keywords[:length]))[0]\n        return ' '.join (keywords)\n    else:\n        return ''","deff1076":"advanced_senator_analytics('SenSanders','key_tweet',extract_keywords)","e976f5b3":"Generally the East Coast is more active than the West Coast. \n<br>You could zoom in and out see by States, by area to get some more insights.\nI have given an image of how it looks when you zoom in on the East Coast.\n\n\nInterestingly California is having much lesser Tweets than Seattle\n![Heatmap.png](attachment:34ebbd9f-6c7b-4a70-b25f-68fdde6bd004.png)","c1560c64":"Even though Republican party has not generated many Viral Tweets, the count of number of tweets by Replublican party is high. \n","63179b15":"\"Text\" is the major attribute in this dataset. We will try to understand how the Text Distribution varies w.r.t all the other attributes. Here we will ignore URL cause it does not convey any meaning. I am skipping Bio_Guide_Id as well cause it looks like the Bio Guide Id and User convey the same meaning. ","472bde60":"Now we will look at attributes party and state before coming back to User. ","2dd1aa00":"## NLP\n\nAll the above analysis that we did before is simple analysis that we do on a structured data. Now we will go to how to apply NLP onto the above dataset. We will be using Spacy for our data preprocessing activities. First we will have to cleanup the tweet. Given below are some common cleanup activities that we do, but you might have to add more based on your data.\n\n* Capture sequence of words, numbers\n* I found a special character \ufffd, so removing that.\n* RT keyword removal if any\n* Remove hyperlinks,references \n* Then #tags\n* Finally, we iterate through all words in the tweet and check if it is a punctuation or a stop word and then add the lemma form. This is done so that we can find the common topics tweeted.\n\nApplying this on the whole dataset can take a while and so we will stick to a single senator's tweets and analyse them in detail\n","a0de6691":"Now we will see how to plot a wordcloud.Word cloud is another very quick way to see the top 'n' things that a user is speaking about. We will use the library Word cloud for the same. Word Cloud takes an attribute as Collocation_threshold that is basically the number of words that appear together. This helps us really understand the topics better. We can increase\/decrease it to see how the words appear.","c90ce700":"## User\n\nWe will do a slightly detailed analysis of the tweets w.r.t to user before moving on to the tweets itself.","db09e47f":"While Sanders is not among the most active users he has generated the most popular Tweets. In fact, there are very few names that appear in both section. Senator Kamala Harris has only tweeted 1390 times when compared to many senators who have tweeted like more than 3000 times but she was able to generate a higher activity through her tweets. But one thing to note is that the activity generated by Sanders is much more than the others.\n\nNow we will take a few senators and look at their tweets in detail. First we start off analysing the time period of tweets. Where there any spikes in specific months?","e768fb54":"Now we understand what are the main topics that the senator has spoken about. But the problem is there are so many words. Now how do we narrow them down. There are two techniques. Rather than extracting all the words , we could just extract the nouns or any pos tags for that matter .","8438c984":"While Senator Sanders talks more about health care. Senator Kamala Harris also speaks about Health care, but there is a lot of mention about women and gender as well. Another way to look at it would be to look at the entities mentioned by ppl. Entities are a word or a chunk of words referring to one single thing and then can be of a type. Say for example, in the sentence,\"I want to book a flight from Sydney to Paris\". Sydney and Paris are locations, another way to look at it would be Sydney is the source and Paris is the destination. \n\nSpacy provides ner models for many languages which come with inbuilt entities or you can even train more entities. We will look at inbuilt entities here. Adding new entities is not covered here.\n\n\n","3b268547":"## Created At","593855be":"It looks like Vermont has most number of popular tweets more than NY.SenatorLeahy , Senator Sanders have the most number of popular tweets. Democratic party has generated the largest number of viral tweets. Independent has one and Republican has None.\n\nNow we will look at attributes party and state before coming back to User","fe86a03d":"Here we can see that Atlanta has very less tweets. We could probably cluster them into 4 clusters based on the number of tweets. Very large , large, medium, small.\n\nFor plotting the details on a map, I have added a new csv called States.csv downloaded from Google to get the latitude and longitude or each state. We can combine both these dataframes using the State column","a4394e63":"Now we can say obviously trend that Tom Udall 2017 had the most tweets, there were periodic spikes in 2016 and 2015 but very little activity before 2013. So we can say that the activity had increased considerably in the recent years.\n\n","129de049":"![US_Count.png](attachment:34182a4d-c047-40a0-998b-2620842fa282.png)\nImages of how the graph looks if u did not run the previous cell","194ebd72":"Now we will fetch all tweets whose activity has been greater than 600641","f8a3bb30":"# Conclusion\nNow that we have a lot of information, we could use them in a variety of ways. Suppose we are trying to predict a chance of user wining elections,along with the other features we could also include tweets. We could directly use the count of tweets as feature. We could cluster them based on theier social media activity.Coming to NLP, We could take  top 10\/50 - keywords, entities across all users and one hot encode the presence or absence of a keyword. We could cluster users\/states based on these new features and user that as one of the features. I am pretty sure there a lot more features that you can think of. I have just listed a few.Hope you are able to apply some of these techniques in you next problem.","71d9b7c6":"There are about 100 users. Now let us see the average number of tweet's and the number of times they have tweeted. That can be fetched using the value counts attribute of Pandas.","eb6b8b0b":"First we will generate a heatmap to understand the distribution of these tweets. Here the Zoom level and radius were set after some trial and error.","fd22cbd6":"## Party","bbb05eb5":"We can see that the minimum number of tweets by a user is 454 and maximum is 3247 and the mean is 2886. Around 75% of the users tweet more than 3190- that is they are highly active.\n\nLet us see who are the top users who generate a lot of tweets and who are the top users who generate a lot of popular tweets.  For this we will use the nlargest property of pandas. This gives the top 'n' values of a column. Groupby() count() work very similar to any SQL group by.","8f626833":"# Introduction\n\nOften when applying NLP to real life problems, we find that every organisation has a lot of unstructured data lying around, not being used anywhere. But including NLP based features on traditional Prediction problems can bring in fresh perspectives that are not obtained looking through structured data alone. How can we find out if the unstructured data has something useful? What are some of the basic steps that you need to find what information your data holds? This is very different from the NLP based problems like entities, summarisation, text generation etc. This notebook talks about some basic steps in which we can use the unstructured data and create new features.","22a0b957":"## State\n\nWhen analysing tweets by State, we will look at Maps as well","1eac5d47":"# Understanding your columns \n\n","fc0be378":"Now the idea is to find the Viral Tweets - i.e the tweets that have gathered more attention than the others. We will use the Pandas Quantile function to acheive this.First I am going to fetch tweets whose activity has been greater than 99.99 % of the other tweets","90166e23":"<div class=\"alert alert-block alert-info\">\nHere this just a map view of the counts. The below cell takes a pretty long time to load. So If you dont want to run the cell, just look at the image below.\n<\/div>","ab46e0a8":"# Feature Understanding","dcd667e5":"So we have 30 tweets which have generated a lot of attention. We could play around with the quantile number. Just going to take a closer look at these viral tweets..","bc7bc528":"Now we will want to see the top 'n' things that a senator tweeted about. So for that we need to combine all the tweets of a single senator take a counter of words and plot the top 'n' words. Counter returns a dict and hence i am using itertools to slice the top 'n' words.","0b5c55ed":"## Feature Addition - Activity\n\nInstead of individually looking at the replies, retweets, favorites- I am going to add all of them and create a new feature called Activity. Any activity is an indication of interest generation. Now if we want we could assign weightages as well - like Favorite could have more weightage than Retweet. But I am going for a simple sum.","7b5aff0e":"Combining both steps to a single method so that we can view them for multiple senators. Now here i have made the field name and the preprocessing fn name as well, so that we can reuse the same function by extracting sub parameters of the tweet.","007faf71":"Now we will look how to extract the keyphrases using the YAKE library. This library returns a keyword and a score. Please note that lower the score more relevant the score is.","4d168499":"This dataset has about 9 columns and none of them have any null values. The Replies, Retweets, Favorites columns are integers while all other columns are text. In Structured Data as a next step we could do df.describe() to understand the distribution of your values. But here i am gonna skip them as there are only 3 integer columns"}}