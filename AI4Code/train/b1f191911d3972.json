{"cell_type":{"46d5ed7d":"code","89de69f2":"code","2e1a2000":"code","fd75e47b":"code","5a76c597":"code","1ec86995":"code","bce14bf5":"code","73ad801b":"code","9b55a305":"code","f7f50888":"code","8d9b874a":"code","ace9535d":"code","1acd4782":"code","46608764":"code","6efb8a03":"code","f7a176e3":"markdown","fc81daf2":"markdown","6686f16e":"markdown","4adee807":"markdown","c13dadd5":"markdown","8fb0ecab":"markdown","5ac5b926":"markdown"},"source":{"46d5ed7d":"import numpy as np\nimport pandas as pd\n#tensorflow version 2.0\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n#reading data\nmnist_train = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\nmnist_test = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')","89de69f2":"#standardization\nmnist_train.iloc[:,1:] \/= 255\nmnist_test.iloc[:,1:] \/= 255\n\n#splitting features and target column\nx_train = mnist_train.iloc[:,1:]\ny_train = mnist_train.iloc[:,0]\nx_test= mnist_test.iloc[:,1:]\ny_test=mnist_test.iloc[:,0]\n\n#further splitting train set into validation and training set\nx_train,x_validate,y_train,y_validate = train_test_split(x_train,y_train,test_size = 0.2,random_state = 12345)","2e1a2000":"print(\"Let's have a look at the images in our dataset.\")\nclass_names = ['T_shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\nplt.figure(figsize=(10, 10))\nfor i in range(36):\n    plt.subplot(6, 6, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(np.array(x_test.iloc[i]).reshape(28,28))\n    label_index = int(y_test[i])\n    plt.title(class_names[label_index])\nplt.show()","fd75e47b":"sns.countplot(y_train)\nplt.title('Classes distribution in train set');","5a76c597":"sns.countplot(y_validate)\nplt.title('Classes distribution in validation set');","1ec86995":"sns.countplot(y_test)\nplt.title('Classes distribution in test set');","bce14bf5":"input_size = 784\noutput_size = 10\nhidden_layer_size = 300\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(input_size, activation='relu'),  # input layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3rd hidden layer\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n])\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n\nmodel.fit(x_train,y_train, epochs=60, validation_data=(x_validate,y_validate),validation_steps=1, verbose =2,callbacks=early_stop)","73ad801b":"test_loss, test_accuracy = model.evaluate(x_test,y_test)\nprint('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))","9b55a305":"pred = model.predict_classes(x_test)\nfrom sklearn.metrics import classification_report\ntarget_names = [\"Class {}\".format(i) for i in range(10)]\nprint(classification_report(y_test,pred, target_names=target_names))","f7f50888":"sns.heatmap(confusion_matrix(y_test,pred),cmap='seismic');","8d9b874a":"image_rows = 28\nimage_cols = 28\nimage_shape = (image_rows,image_cols,1)\nx_train = tf.reshape(x_train,[x_train.shape[0],*image_shape])\nx_test = tf.reshape(x_test,[x_test.shape[0],*image_shape])\nx_validate = tf.reshape(x_validate,[x_validate.shape[0],*image_shape])","ace9535d":"cnn_model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu',input_shape = image_shape),\n    tf.keras.layers.MaxPooling2D(pool_size=2) ,# down sampling the output instead of 28*28 it is 14*14\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Flatten(), # flatten out the layers\n    tf.keras.layers.Dense(196,activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(50,activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(50,activation='relu'),\n    tf.keras.layers.Dense(10,activation = 'softmax')\n])\n\ncnn_model.compile(loss ='sparse_categorical_crossentropy',\n                  optimizer='adam',metrics =['accuracy'])\n\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n\nhistory = cnn_model.fit(\n    x_train,\n    y_train,\n    batch_size=4096,\n    epochs=75,\n    verbose=1,\n    validation_data=(x_validate,y_validate),\n    callbacks=early_stop\n)","1acd4782":"plt.figure(figsize=(10, 10))\n\nplt.subplot(2, 2, 1)\nplt.plot(history.history['loss'], label='Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend()\nplt.title('Training - Loss Function')\n\nplt.subplot(2, 2, 2)\nplt.plot(history.history['accuracy'], label='Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.legend()\nplt.title('Training - Accuracy')","46608764":"cnn_pred = cnn_model.predict_classes(x_test)\ntarget_names = [\"Class {}\".format(i) for i in range(10)]\nprint(classification_report(y_test,cnn_pred, target_names=target_names))","6efb8a03":"score = cnn_model.evaluate(x_test,y_test,verbose=0)\nprint('Test Loss : {:.4f}'.format(score[0]))\nprint('Test Accuracy : {:.4f}'.format(score[1]))","f7a176e3":"Let's see if our data is balanced.","fc81daf2":"## Deep Neural Network for Fashion MNIST Classification\nFashion-MNIST is a dataset of Zalando's article images\u2014consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. It shares the same image size and structure of training and testing splits.\n### Data Description:\n* Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.\n* Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255.\n* The training and test data sets have 785 columns.\n* The first column consists of the class labels (see above), and represents the article of clothing.\n* The rest of the columns contain the pixel-values of the associated image.\n* Each training and test example is assigned to one of the following labels:\n0- T-shirt\/top;\n1- Trouser;\n2- Pullover;\n3- Dress;\n4- Coat;\n5- Sandal;\n6- Shirt;\n7- Sneaker;\n8- Bag;\n9- Ankle boot;\n*  Since the image data in x_train and x_test is from 0 to 255 , we need to rescale this from 0 to 1.To do this we need to divide the x_train and x_test by 255 . It's important that the training set and the testing set be preprocessed in the same way.","6686f16e":"Let us plot the Training Accuracy vs Loss to get a better understanding of the model training.","4adee807":"# Convolutional Neural Network\n\nThe term deep neural nets refers to any neural network with several hidden layers. Convolutional neural nets are a specific type of deep neural net which are especially useful for image recognition. Specifically, convolutional neural nets use convolutional and pooling layers, which reflect the translation-invariant nature of most images.\n\nFor this, we need to reshape our input data.","c13dadd5":"Now we define our model. The first layer in model network (keras.layers.Flatten) transforms the format of the images from a two-dimensional array (of 28 by 28 pixels) to a one-dimensional array (of 28 * 28 = 784 pixels). This layer unstacks rows of pixels in the image and lining them up and has no parameters to learn; it only reformats the data. Pooling layers are then added to further reduce the number of parameters.\n\nAfter the pixels are flattened, the network consists of a sequence of two keras.layers.Dense layers. These are densely connected, or fully connected, neural layers.\n\nA problem with training neural networks is in the choice of the number of training epochs to use. Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset.","8fb0ecab":"Our DNN model is performing worst on samples of class 6 which is shirt and class 2. And it has above 98% accuracy for classes 1(trouser),5(sandal) and 9(ankle boot).","5ac5b926":"So, by adding convolution layer and pooling layer model accuracy improved to almost 93%."}}