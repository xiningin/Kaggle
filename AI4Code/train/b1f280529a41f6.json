{"cell_type":{"1b612536":"code","38b219d1":"code","14afa7bb":"code","4c8961db":"code","fb34724c":"code","c6750601":"code","2af4b75e":"code","ebb584b5":"code","f3d5b269":"code","14d38233":"code","407c0a00":"code","58042108":"code","a3e8b8e4":"code","87523f91":"code","5979126d":"code","5340416f":"code","18dd6f4d":"code","4f426563":"code","9b3d78e5":"code","09a69b62":"code","0b8c4abe":"code","78d93d56":"code","95c4d31d":"code","e5d2366f":"code","3d444b6c":"code","610c2ad0":"code","0377ef21":"code","e4c92a27":"code","79de75bc":"code","5210a002":"markdown"},"source":{"1b612536":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","38b219d1":"test_data = pd.read_csv(\"..\/input\/emotions-dataset-for-nlp\/test.txt\", header=None, sep=\";\", names=[\"Comment\",\"Emotion\"], encoding=\"utf-8\")\ntrain_data = pd.read_csv(\"..\/input\/emotions-dataset-for-nlp\/train.txt\", header=None, sep=\";\", names=[\"Comment\",\"Emotion\"], encoding=\"utf-8\")\nvalidation_data = pd.read_csv(\"..\/input\/emotions-dataset-for-nlp\/val.txt\", header=None, sep=\";\", names=[\"Comment\",\"Emotion\"], encoding=\"utf-8\")","14afa7bb":"print(\"Train : \", train_data.shape)\nprint(\"Test : \", test_data.shape)\nprint(\"Validation : \", validation_data.shape)","4c8961db":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n\"\"\"sns.set()\nsns.countplot(train_data[\"Comment\"])\nplt.show()\"\"\"","fb34724c":"train_data[\"length\"] = [len(i) for i in train_data[\"Comment\"]]","c6750601":"plt.plot(train_data[\"length\"], color = \"green\")","2af4b75e":"sns.kdeplot(x=train_data[\"length\"], hue=train_data[\"Emotion\"])","ebb584b5":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ntrain_data[\"Emotion\"] = lb.fit_transform(train_data[\"Emotion\"])\ntest_data[\"Emotion\"] = lb.fit_transform(test_data[\"Emotion\"])\nvalidation_data[\"Emotion\"] = lb.fit_transform(validation_data[\"Emotion\"])","f3d5b269":"train_data.head()","14d38233":"test_data.head()","407c0a00":"validation_data.head()","58042108":"import re \nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer","a3e8b8e4":"vocab_size = 10000\ntrain_data[\"length\"].max()","87523f91":"train_data[\"length\"].min()","5979126d":"len_sentence = 150","5340416f":"train_data.head()","18dd6f4d":"from tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nnltk.download('stopwords')\nstopwords = set(nltk.corpus.stopwords.words('english'))","4f426563":"def text_prepare(data, column):\n    print(data.shape)\n    stemmer = PorterStemmer()\n    corpus = []\n    \n    for text in data[column]:\n        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n        \n        text = text.lower()\n        text = text.split()\n        \n        text = [stemmer.stem(word) for word in text if word not in stopwords]\n        text = \" \".join(text)\n        \n        corpus.append(text)\n    one_hot_word = [one_hot(input_text=word, n=vocab_size) for word in corpus]\n    embeddec_doc = pad_sequences(sequences=one_hot_word,\n                              maxlen=len_sentence,\n                              padding=\"pre\")\n    print(data.shape)\n    return embeddec_doc\n        ","9b3d78e5":"x_train=text_prepare(train_data, \"Comment\")\nx_validate=text_prepare(validation_data, \"Comment\")\nx_test=text_prepare(test_data, \"Comment\")","09a69b62":"x_train.shape","0b8c4abe":"y_train=train_data[\"Emotion\"]\ny_validate=validation_data[\"Emotion\"]\ny_test=test_data[\"Emotion\"]","78d93d56":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\ny_train = np.array(y_train)\ny_train = enc.fit_transform(y_train.reshape(-1,1)).toarray()","95c4d31d":"y_test = np.array(y_test)\ny_validate = np.array(y_validate)\n\ny_test = enc.fit_transform(y_test.reshape(-1,1)).toarray()\ny_validate = enc.fit_transform(y_validate.reshape(-1,1)).toarray()","e5d2366f":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.optimizers import Adam","3d444b6c":"model = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=150, input_length=len_sentence))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(128))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, activation=\"sigmoid\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(6, activation=\"softmax\"))","610c2ad0":"model.compile(optimizer=\"Adam\", loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","0377ef21":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\n\nes = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 5)\nmc = ModelCheckpoint('.\/model.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)","e4c92a27":"y_train.shape","79de75bc":"hist = model.fit(x_train, y_train, epochs = 25, batch_size = 64, validation_data=(x_validate, y_validate),verbose = 1, callbacks= [es, mc])","5210a002":"# # # ****Data Preprocessing****"}}