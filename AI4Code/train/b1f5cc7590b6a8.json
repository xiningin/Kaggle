{"cell_type":{"dcee5a8f":"code","5e373498":"code","eb6a6632":"code","3a5211fd":"code","074c0b74":"code","af64c481":"code","89fb5efa":"code","be8c87fa":"code","860ae91f":"code","8072a65c":"code","d84d5ab2":"code","c73e4148":"code","fc884150":"code","765c68b9":"code","19634a95":"code","a4c5d87e":"code","a62bb52a":"code","df096bc0":"code","7776f360":"code","d02cf85d":"code","8778ac50":"code","8c2f42e6":"code","933104de":"code","53f90f0d":"code","7824b150":"code","26ec3003":"code","e5f8b207":"code","bbace648":"code","b3d8b162":"code","4f929e93":"code","30a6262d":"code","646b3350":"code","b16afde8":"code","e781468a":"code","791468e7":"code","db7ac74c":"code","ee332816":"code","f9db5a26":"code","005c72c2":"code","1db603dc":"code","2ac9e9f5":"code","9d7873ff":"code","037c73b5":"code","7805362d":"code","4a63c5d9":"code","f81aa79d":"code","2f0aaa55":"code","5c84a8c0":"code","57cd041c":"code","84fea75d":"code","2258b6d0":"code","c2954739":"code","42f16ee9":"code","2894d378":"code","94581f25":"code","6311e026":"code","7fdb0934":"code","f485c134":"code","afadd4af":"code","883e6d53":"code","6d1fe926":"code","871c78c5":"code","5b2cfb5a":"markdown","3807a956":"markdown","436eb222":"markdown","16024366":"markdown","5569260a":"markdown","cdfde3a4":"markdown","c3b0def3":"markdown","57dbaee8":"markdown","2d1ab412":"markdown","f91aa249":"markdown","3c25efc3":"markdown","be8e2521":"markdown","c46ab775":"markdown","3a48463e":"markdown","e5cb4a28":"markdown","a4108ac7":"markdown","38d7c683":"markdown","b9ef6385":"markdown","60762136":"markdown","e72eb150":"markdown","075c0815":"markdown","4062b7e5":"markdown","910a8d62":"markdown","15c88236":"markdown","c8c96ed6":"markdown","1a3bdcbf":"markdown","cb163768":"markdown","428450e8":"markdown","d69b4e82":"markdown","67368446":"markdown","0c55c058":"markdown"},"source":{"dcee5a8f":"# Data Processing\nimport numpy as np\nimport pandas as pd\nfrom pandas import datetime\n\n# Data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns # advanced vizs\nimport matplotlib.gridspec as gridspec\nfrom IPython.display import display\n%matplotlib inline\n\n# Data Modeling\nfrom statsmodels.tsa.ar_model import AR\nfrom statsmodels.tsa.arima_model import ARMA,ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pmdarima import auto_arima # for determining ARIMA orders\nfrom fbprophet import Prophet\n\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\nimport lightgbm\n\n# Data Evaluation\nfrom sklearn.metrics import mean_squared_error\n\n# Statistics\nfrom statsmodels.distributions.empirical_distribution import ECDF\n\n# Time series analysis\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# Warning ignore\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5e373498":"train = pd.read_csv(\"..\/input\/rossmann-store-sales\/train.csv\")\nstore = pd.read_csv(\"..\/input\/rossmann-store-sales\/store.csv\")\ntest = pd.read_csv(\"..\/input\/rossmann-store-sales\/test.csv\", parse_dates = True, index_col = 'Date')","eb6a6632":"train.head()","3a5211fd":"# Change datatype of InvoiceDate as datetime type\ntrain['Date'] = pd.to_datetime(train['Date'])","074c0b74":"# data extraction\ntrain['Year'] = train['Date'].dt.year\ntrain['Month'] = train['Date'].dt.month\ntrain['Day'] = train['Date'].dt.day\ntrain['WeekOfYear'] = train['Date'].dt.weekofyear\n\ntest['Year'] = test.index.year\ntest['Month'] = test.index.month\ntest['Day'] = test.index.day\ntest['WeekOfYear'] = test.index.weekofyear","af64c481":"# adding new variable\ntrain['SalePerCustomer'] = train['Sales']\/train['Customers']\ntrain['SalePerCustomer'].describe()","89fb5efa":"plt.figure(figsize = (12, 6))\n\nplt.subplot(311)\ncdf = ECDF(train['Sales'])\nplt.plot(cdf.x, cdf.y, label = \"statmodels\");\nplt.title('Sales'); plt.ylabel('ECDF');\n\n# plot second ECDF  \nplt.subplot(312)\ncdf = ECDF(train['Customers'])\nplt.plot(cdf.x, cdf.y, label = \"statmodels\");\nplt.title('Customers');\n\n# plot second ECDF  \nplt.subplot(313)\ncdf = ECDF(train['SalePerCustomer'])\nplt.plot(cdf.x, cdf.y, label = \"statmodels\");\nplt.title('Sale per Customer');\nplt.subplots_adjust(hspace = 0.8)","be8c87fa":"sns.distplot(train['Sales'])","860ae91f":"# Closed Stores with zero sales\ntrain[(train.Open == 0)]","8072a65c":"# Opened stores with zero sales\nlen(train[(train.Open == 1) & (train.Sales == 0)])","d84d5ab2":"# Closed stores and days which didn't have any sales won't be counted into the forecasts.\ntrain = train[(train[\"Open\"] != 0) & (train['Sales'] != 0)]\ntrain.head()","c73e4148":"# missing values?\nstore.isnull().sum()","fc884150":"# missing values in CompetitionDistance\nstore[pd.isnull(store.CompetitionDistance)]","765c68b9":"# fill NaN with a median value (skewed distribuion)\nstore['CompetitionDistance'].fillna(store['CompetitionDistance'].median(), inplace = True)","19634a95":"# replace NA's by 0\nstore.fillna(0, inplace = True)","a4c5d87e":"# by specifying inner join we make sure that only those observations \n# that are present in both train and store sets are merged together\ntrain_store = pd.merge(train, store, how = 'inner', on = 'Store')\n\ntest_store = pd.merge(test, store, how = 'inner', on = 'Store')\n\ntrain_store.head()","a62bb52a":"train_store.groupby('StoreType')['Sales'].describe()","df096bc0":"train_store.groupby('StoreType')['Customers', 'Sales'].sum()","7776f360":"# sales trends\nsns.factorplot(data = train_store, x = 'Month', y = \"Sales\", \n               col = 'StoreType', # per store type in cols\n               palette = 'plasma',\n               hue = 'StoreType',\n               row = 'Promo') ","d02cf85d":"# sales trends\nsns.factorplot(data = train_store, x = 'Month', y = \"Customers\", \n               col = 'StoreType', # per store type in cols\n               palette = 'plasma',\n               hue = 'StoreType',\n               row = 'Promo') ","8778ac50":"# sale per customer trends\nsns.factorplot(data = train_store, x = 'Month', y = \"SalePerCustomer\", \n               col = 'StoreType', # per store type in cols\n               palette = 'plasma',\n               hue = 'StoreType',\n               row = 'Promo')","8c2f42e6":"# customers\nsns.factorplot(data = train_store, x = 'Month', y = \"Sales\", \n               col = 'DayOfWeek', # per store type in cols\n               palette = 'plasma',\n               hue = 'StoreType',\n               row = 'StoreType')","933104de":"# competition open time (in months)\ntrain_store['CompetitionOpen'] = 12 * (train_store.Year - train_store.CompetitionOpenSinceYear) + \\\n        (train_store.Month - train_store.CompetitionOpenSinceMonth)  \n# Promo open time\ntrain_store['PromoOpen'] = 12 * (train_store.Year - train_store.Promo2SinceYear) + \\\n        (train_store.WeekOfYear - train_store.Promo2SinceWeek) \/ 4.0\n\n# test_store['CompetitionOpen'] = 12 * (test_store['Year'] - test_store['CompetitionOpenSinceYear']) + (test_store['Month'] - test_store['CompetitionOpenSinceMonth'])\n# test_store['PromoOpen'] = 12 * (test_store['Year'] - test_store['Promo2SinceYear']) + (test_store['WeekOfYear'] - test_store['Promo2SinceWeek']) \/ 4.0\n\n\n# replace NA's by 0\ntrain_store.fillna(0, inplace = True)\n\n# average PromoOpen time and CompetitionOpen time per store type\ntrain_store[['StoreType', 'Sales', 'Customers', 'PromoOpen', 'CompetitionOpen']].groupby('StoreType').mean()","53f90f0d":"# Compute the correlation matrix \n# exclude 'Open' variable\ncorr_all = train_store.drop('Open', axis = 1).corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr_all, dtype = np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize = (11, 9))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_all, mask = mask,\n            square = True, linewidths = .5, ax = ax, cmap = \"BuPu\")      \nplt.show()","7824b150":"# sale per customer trends\nsns.factorplot(data = train_store, x = 'DayOfWeek', y = \"Sales\", \n               col = 'Promo', \n               row = 'Promo2',\n               hue = 'Promo',   # SPECIAL\n               palette = 'RdPu') ","26ec3003":"# to numerical\nmappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n\ntrain['StateHoliday'] = train['StateHoliday'].replace(mappings).astype('int64')\n\ntest['StateHoliday'] = test['StateHoliday'].replace(mappings).astype('int64')\n\nstore['StoreType'] = store['StoreType'].replace(mappings).astype('int64')\nstore['Assortment'] = store['Assortment'].replace(mappings).astype('int64')\nstore.drop('PromoInterval', axis = 1, inplace = True)\n\ntrain_store = pd.merge(train, store, how = 'inner', on = 'Store')\ntest_store = pd.merge(test, store, how = 'inner', on = 'Store')","e5f8b207":"# Choose 1 store with type a, namely store 2\nsales_2 = train[train['Store'] == 2][['Sales', 'Date','StateHoliday','SchoolHoliday']]","bbace648":"sales_2['Date'].sort_index(ascending = False, inplace=True)","b3d8b162":"a = sales_2.set_index('Date').resample('W').sum()\na","4f929e93":"plt.figure(figsize=(12, 5))\nsns.lineplot(x=a.index, y=a['Sales'])","30a6262d":"f, (ax1, ax2) = plt.subplots(2, figsize = (12, 6))\n\n# monthly\ndecomposition_a = seasonal_decompose(sales_2['Sales'], model = 'additive', freq = 365)\ndecomposition_a.observed.plot(ax = ax1)\nax1.set_ylabel('OBSERVED')\ndecomposition_a.trend.plot(ax = ax2)\nax2.set_ylabel('TREND')\nf.subplots_adjust(hspace = 0.5)","646b3350":"fig = plt.figure(constrained_layout=True, figsize=(15, 4))\ngrid = gridspec.GridSpec(nrows=1, ncols=2,  figure=fig)\n\n# acf and pacf for A\nax1 = fig.add_subplot(grid[0, 0])\nplot_acf(sales_2['Sales'], lags = 50, ax=ax1);\n\n# acf and pacf for A\nax1 = fig.add_subplot(grid[0, 1])\nplot_pacf(sales_2['Sales'], lags = 50, ax=ax1);\n\nplt.show();","b16afde8":"# adfuller helps us to determine the right model for analysis. \n# For example, the returned value from adf_test show 'Fail to reject the null hypothesis', it means we should make differencing.\n\nfrom statsmodels.tsa.stattools import adfuller\n\ndef adf_test(series,title=''):\n    \"\"\"\n    Pass in a time series and an optional title, returns an ADF report\n    \"\"\"\n    print(f'Augmented Dickey-Fuller Test: {title}')\n    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data\n    \n    labels = ['ADF test statistic','p-value','# lags used','# observations']\n    out = pd.Series(result[0:4],index=labels)\n\n    for key,val in result[4].items():\n        out[f'critical value ({key})']=val\n        \n    print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n    \n    if result[1] <= 0.05:\n        print(\"Strong evidence against the null hypothesis\")\n        print(\"Reject the null hypothesis\")\n        print(\"Data has no unit root and is stationary\")\n    else:\n        print(\"Weak evidence against the null hypothesis\")\n        print(\"Fail to reject the null hypothesis\")\n        print(\"Data has a unit root and is non-stationary\")","e781468a":"adf_test(sales_2['Sales'])","791468e7":"x = sales_2.set_index('Date').loc[:'2015-06-30']\ny = sales_2.set_index('Date').loc['2015-07-01':]","db7ac74c":"model = AR(x['Sales'])\nAR1fit = model.fit(method='mle')\n#print(f'Lag: {AR1fit.k_ar}')\n#print(f'Coefficients:\\n{AR1fit.params}')\n\n# This is the general format for obtaining predictions\nstart=len(x)\nend=len(x)+len(y)-1\npredictions1 = AR1fit.predict(start=start, end=end, dynamic=False)\n\npredictions1.index = y.index\n\ny['Sales'].plot()\npredictions1.plot(label='prediction');\nplt.legend()\nplt.show()\n\nprint('Root Mean Squared Error: ', np.sqrt(mean_squared_error(y['Sales'], predictions1)))","ee332816":"# auto_arima help us choose the optimal model, sometime manually tweaking model hyperparameters yeild better result.\nauto_arima(x['Sales'],seasonal=False).summary()","f9db5a26":"model = ARMA(x['Sales'], order=(5,5))\nresults = model.fit()\n\nstart=len(x)\nend=len(x)+len(y)-1\n\npredictions1 = results.predict(start=start, end=end, dynamic=False)\npredictions1.index = y.index\n\ny['Sales'].plot()\npredictions1.plot(label='prediction');\n\nprint('Root Mean Squared Error: ', np.sqrt(mean_squared_error(y['Sales'], predictions1)))","005c72c2":"# Adding exorgenous variable may help accuracy improvement. Let's see\n# It doesnt improve as the store usually closed on StateHoliday or SchoolHoliday and sales may not escalated even if it opens.\nauto_arima(x['Sales'], exorgenous=x[['StateHoliday','SchoolHoliday']],seasonal=False).summary()","1db603dc":"model = ARMA(x['Sales'], order=(5,5))\nresults = model.fit()\n\n# This is the general format for obtaining predictions\nstart=len(x)\nend=len(x)+len(y)-1\nexog_forecast = x[['StateHoliday','SchoolHoliday']]\npredictions1 = results.predict(start=start, end=end, exog=exog_forecast, dynamic=False)\n\npredictions1.index = y.index\n\ny['Sales'].plot()\npredictions1.plot(label='prediction');\nplt.legend()\n\nprint('Root Mean Squared Error: ', np.sqrt(mean_squared_error(y['Sales'], predictions1)))","2ac9e9f5":"# As we talk above, we may be interested in the fact that event or seasonality can influence sale of store.\n# However, in this case, adding seasonality worsen model. Thus, there is no clear seasonal component in this case.\n\n# https:\/\/alkaline-ml.com\/pmdarima\/tips_and_tricks.html#setting-m\n# m = 7(daily), 12(monthly), 52(weekly)\nauto_arima(x['Sales'],seasonal=True, m=7).summary()","9d7873ff":"model = SARIMAX(x['Sales'],order=(5, 0, 3),seasonal_order=(0, 0, 1, 7))\nresults = model.fit()\n\nstart=len(x)\nend=len(x)+len(y)-1\npredictions1 = results.predict(start=start, end=end, dynamic=False)\n\npredictions1.index = y.index\n\ny['Sales'].plot()\npredictions1.plot(label='prediction');\nplt.legend()\n\nprint('Root Mean Squared Error: ', np.sqrt(mean_squared_error(y['Sales'], predictions1)))","037c73b5":"state_dates = x[(x.StateHoliday == 0) | (x.StateHoliday == 1) & (x.StateHoliday == 2)].reset_index()['Date'].values\nschool_dates = x[(x['SchoolHoliday'] == 1)].reset_index()['Date'].values\n\nstate = pd.DataFrame()\nstate['ds'] = pd.to_datetime(state_dates)\nstate['holiday'] = 'state_holiday'\n\nschool = pd.DataFrame()\nschool['ds'] = pd.to_datetime(school_dates)\nschool['holiday'] = 'school_holiday'\n\nholidays = pd.concat((state, school))","7805362d":"TRAIN = x.reset_index()[['Date', 'Sales']]\nTRAIN.columns = ['ds', 'y']","4a63c5d9":"# set the uncertainty interval to 95% (the Prophet default is 80%)\nmy_model = Prophet(interval_width = 0.95, \n                   holidays = holidays)\nmy_model.fit(TRAIN)","f81aa79d":"# dataframe that extends into future 6 weeks \nfuture_dates = my_model.make_future_dataframe(periods=31)","2f0aaa55":"print(\"Last day to forecast.\")\nfuture_dates.tail(1)","5c84a8c0":"# predictions\nforecast = my_model.predict(future_dates)\n\n# preditions for last week\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(1)","57cd041c":"a = forecast[['ds','yhat']]\nb = y['Sales'].reset_index()\ntable = a.set_index('ds').join(b.set_index('Date')).dropna()\ntable.columns = ['yhat', 'y']","84fea75d":"display(table.head(1))\n\ntable['yhat'].plot()\ntable['y'].plot()\nplt.legend();\nplt.show()\n\nprint('Root Mean Squared Error: ', np.sqrt(mean_squared_error(table['yhat'], table['y'])))","2258b6d0":"# visualizing predicions\nmy_model.plot(forecast);","c2954739":"my_model.plot_components(forecast);","42f16ee9":"train_store['CompetitionOpen'] = 12 * (train_store['Year'] - train_store['CompetitionOpenSinceYear']) + (train_store['Month'] - train_store['CompetitionOpenSinceMonth'])\ntrain_store['PromoOpen'] = 12 * (train_store['Year'] - train_store['Promo2SinceYear']) + (train_store['WeekOfYear'] - train_store['Promo2SinceWeek']) \/ 4.0\n\ntest_store['CompetitionOpen'] = 12 * (test_store['Year'] - test_store['CompetitionOpenSinceYear']) + (test_store['Month'] - test_store['CompetitionOpenSinceMonth'])\ntest_store['PromoOpen'] = 12 * (test_store['Year'] - test_store['Promo2SinceYear']) + (test_store['WeekOfYear'] - test_store['Promo2SinceWeek']) \/ 4.0","2894d378":"# Sorting dataframe according to datatime, the oldest is on top, the most recent is at the bottom.\ntrain_store['Date'].sort_index(ascending = False, inplace=True)","94581f25":"def rmsle(y_pred, y):\n    return np.sqrt(mean_squared_error(y_pred, y))\n\ndef model_check (estimators):\n    model_table = pd.DataFrame()\n    row_index = 0\n    \n    for est in estimators:\n        MLA_name = est.__class__.__name__\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n        \n        est.fit(x_train, y_train)\n        y_pred = est.predict(x_test)\n        model_table.loc[row_index, 'Test Error'] = rmsle(y_pred, y_test)\n        \n        row_index += 1\n        \n        model_table.sort_values(by=['Test Error'],\n                            ascending=True,\n                            inplace=True)\n    return model_table","6311e026":"# MODELS\nlr = LinearRegression()\nls = Lasso()\nGBoost = GradientBoostingRegressor(random_state = 0)\nXGBoost = XGBRegressor(random_state = 0, n_job=-1)\nLGBM = LGBMRegressor(random_state = 0, n_job=-1)","7fdb0934":"# Training dataset is separated into train_a and test_a.\n# traing_a train data from 2013 till 2015-06-30, while test_a contain  data from 2015-07-01 till 2015-07-31.\n\ntrain_a = train_store.set_index('Date').loc[:'2015-06-30']\ntest_a = train_store.set_index('Date').loc['2015-07-01':]\n\nx_train = train_a.drop(['Sales', 'Customers'], axis=1)\ny_train = train_a['Sales']\nx_test = test_a.drop(['Sales', 'Customers'], axis=1)\ny_test = test_a['Sales']","f485c134":"estimators = [lr, ls, GBoost, XGBoost, LGBM]\nmodel_check(estimators)","afadd4af":"# This part is different from the above. This particularly examinze the prediction power on store 2 only.\n\ntrain_a = train_store[train_store['Store']==2].set_index('Date').loc[:'2015-06-30']\ntest_a = train_store[train_store['Store']==2].set_index('Date').loc['2015-07-01':]\n\nx_train = train_a.drop(['Sales', 'Customers','SalePerCustomer'], axis=1)\ny_train = train_a['Sales']\nx_test = test_a.drop(['Sales', 'Customers', 'SalePerCustomer'], axis=1)\ny_test = test_a['Sales']","883e6d53":"XGBoost = XGBRegressor(random_state = 0, n_job=-1).fit(x_train, y_train)\ny_pred = XGBoost.predict(x_test)\nLGBM = LGBMRegressor(random_state = 0, n_job=-1).fit(x_train, y_train)\ny_pred = LGBM.predict(x_test)","6d1fe926":"xgb.plot_importance(XGBoost)","871c78c5":"lightgbm.plot_importance(LGBM)","5b2cfb5a":"#### Observation:\nAbout 20% of data has zero amount of sales\/customers probably due to the fact that the store is closed for state holidays or school holidays","3807a956":"#### Observation:\n- Trend is clearly downwards overtime\n- Sales peaks on sunday, and hit bottom on tuesday\n- The busiest working period is in Auguest. ","436eb222":"#### SARIMA Model","16024366":"#### Observation:\n- 172817 closed stores with 0 sales.\n- 52 store opened but without sales.\n- We just keep opened stores with sales for analysis.","5569260a":"#### Observation:\n- Seeing both feature importance of XGBoost and LightGBM show similar patterns.\n- Day, DayOfWeek, WeekOfYear, PromoOpen, Promo primarily account for sale amount","cdfde3a4":"# SALE FORECASTING\n## Vu Duong\n### Date: June, 2020","c3b0def3":"# CREDITS\nThis notebook is inspired by multiple great work:\n- https:\/\/www.kaggle.com\/elenapetrova\/time-series-analysis-and-forecasts-with-prophet\n- https:\/\/www.udemy.com\/course\/python-for-time-series-data-analysis\/","57dbaee8":"#### Observation:\n- At the first glance, store 1 show stationary pattern.\n- Unfortunately, the trend experience downward.","2d1ab412":"#### AR Model","f91aa249":"#### Observation:\n- In the case of no promotion either Promo1 or Promo2, sale peaks on sunday.\n- On the contrary, store running Promo1 make the most of sale on monday.\n- Promo2 seems irrelevant to overal","3c25efc3":"#### Observation:\n- There is high relationship between Customers and Sales and Promo, but Promo2","be8e2521":"### Filling Missing Values","c46ab775":"#### Observation:\n- StoreType B has the longest running period of promotion.\n- It is noticed, StoreType B doesn't generate huge sale amount, but got the largest average sales.","3a48463e":"# INTRODUCTION\nRossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied. You are provided with historical sales data for 1,115 Rossmann stores.\n\nDetailed description of dataset content is described in the following link: https:\/\/www.kaggle.com\/c\/rossmann-store-sales\/overview\n\n##### Data Description\n- Id - an Id that represents a (Store, Date) duple within the test set\n- Store - a unique Id for each store\n- Sales - the turnover for any given day (this is what you are predicting)\n- Customers - the number of customers on a given day\n- Open - an indicator for whether the store was open: 0 = closed, 1 = open\n- StateHoliday - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n- SchoolHoliday - indicates if the (Store, Date) was affected by the closure of public schools\n- StoreType - differentiates between 4 different store models: a, b, c, d\n- Assortment - describes an assortment level: a = basic, b = extra, c = extended\n- CompetitionDistance - distance in meters to the nearest competitor store\n- CompetitionOpenSince[Month\/Year] - gives the approximate year and month of the time the nearest competitor was opened\n- Promo - indicates whether a store is running a promo on that day\n- Promo2 - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n- Promo2Since[Year\/Week] - describes the year and calendar week when the store started participating in Promo2\n- PromoInterval - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store","e5cb4a28":"# APRROACH\n- Data Exploration: load and check missing value,\n- Data Visualization: plot ECDF, distribution,\n- Data Manipulation: change data types, impute data, create new features,\n- Time Series extensive Analysis: observe data distribtion over time, seasonal decomposition, plot ACF and PACF\n- Data Modeling: Compare AR, ARMA, SARIMA, Prophet Facebook models and multiple traditional ML algorithms regressions\n","a4108ac7":"### PROPHET FACEBOOK FORECASTING","38d7c683":"# LIBRARY","b9ef6385":"# DATA EXPLORATION","60762136":"#### Observation:\n- StoreType B has the highest average of Sales among all others, however we have much less data for it\n- Clearly, StoreType A come as first, StoreType D goes on the second place in both Sale and Customers","e72eb150":"#### Observation:\n- Clearly any store with promotion attract more customers, leading to higher sales.\n- Interestingly, sale escalates toward the end of the year, especially Christmas eve.\n- SalePerCustomer is observed at StoreType D with the highest amount, $10 without Promo and 11 with Promo.","075c0815":"# DATA MANIPULATION","4062b7e5":"# TIME SERIES ANALYSIS\n- Time series seems an alternative approach when dealing with continuous values, apart from ML regresions\n- Time in time series is dependant while target values is independant.\n- Seasonality, Trends, Residuals are 3 major things worth being explored in seasonal_decompose. Plus, some special event, holidays, ect, are captured as well.\n- Plotting ACF and PACF is the essential step before choosing which time series model is applyied, such as AR, MA, ARMA, ARIMA, SARIMA.\n- Prophet Facebook function is quite easy to use and is similar to ARIMA family.","910a8d62":"### Seasonal_decompose","15c88236":"### Store Type","c8c96ed6":"# REGRESSION MODELS","1a3bdcbf":"### ARIMA FAMILY FORECASTING","cb163768":"#### Observation:\nOn average customers spend about 9.50$ per day.","428450e8":"### Autocorrelaion","d69b4e82":"# TIME SERIES MODELS","67368446":"#### Observation:\n- ACF is a measure of the correlation between the timeseries with a lagged version of itself. I choose lags of 50. ACF help us choose MA model.\n- PACF, on the other hand, measures the correlation between the timeseries with a lagged version of itself, after removing the influence of any variance in the middle. PACF helps us choose AR model.\n- Looking at the Autocorrelation graph shows store 1 has high seasonality at 12 lags, it experienced positive spike at 12 lags, 24 lags,... This store show high correlation between the current time unit with the previous time unit.","0c55c058":"#### ARMA Model"}}