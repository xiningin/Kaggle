{"cell_type":{"bf768e7c":"code","0f01e225":"code","c35c7e6d":"code","a16727e9":"code","84d80bb6":"code","b80fa98f":"code","1dcf2404":"code","82254e79":"code","d87b5853":"code","f0b36517":"code","8430cf93":"code","0bb36b8c":"code","3eeb578a":"code","2fe9f15c":"code","0c12d08f":"code","fcaed836":"code","96a3a07b":"code","16aa02e0":"code","9bb6dbe1":"code","58d6c293":"code","2f9e6965":"code","ea000cb3":"code","b5c7407e":"code","6f7234a9":"markdown","2b53c862":"markdown","22f30f21":"markdown","9aca981f":"markdown"},"source":{"bf768e7c":"import pandas as pd       \nimport matplotlib as mat\nimport matplotlib.pyplot as plt    \nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import callbacks","0f01e225":"#Trying to get reproducible results\nfrom numpy.random import seed\nseed(42)\nfrom tensorflow.random import set_seed\nset_seed(42)","c35c7e6d":"df_train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv', index_col = 'id')\nY_train = df_train['target'].copy()\nX_train = df_train.copy().drop('target', axis = 1)\n\nX_test = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv', index_col = 'id')","a16727e9":"#Update: Applied Log Transformation and Standard Scaling\nscaler = StandardScaler()\n\nX_concat = pd.concat([X_train, X_test]).reset_index(drop=True)\nfor col in X_train.columns:\n    X_concat[col] = np.log(X_concat[col]-(min(X_concat[col]-1)))\nX_concat = pd.DataFrame(scaler.fit_transform(X_concat), columns = X_train.columns)\nX_concat","84d80bb6":"X_train_logscaled = X_concat[:X_train.shape[0]][X_train.columns]\nX_test_logscaled = X_concat[X_train.shape[0]:][X_train.columns].reset_index(drop=True)\n\nX_train_logscaled","b80fa98f":"X_test_logscaled","1dcf2404":"class_map = {'Class_1': 0,\n            'Class_2': 1,\n            'Class_3': 2,\n            'Class_4': 3}\nY_train = Y_train.map(class_map).astype('int')\nY_train","82254e79":"#Converting target series to matrix for multiclass classification on Keras\n\nY_train = to_categorical(Y_train)\nY_train","d87b5853":"#Included a BatchNormalization on the beginning\ndef get_model():\n    model = keras.Sequential([\n        layers.BatchNormalization(input_shape = [50]),\n        layers.Dense(units = 128, activation = 'relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(units = 64, activation = 'relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(units = 32, activation = 'relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),    \n        layers.Dense(4, activation = 'softmax'),\n    ])\n    \n    return model","f0b36517":"keras.backend.clear_session()\n\nmodel = get_model()\nmodel.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(learning_rate=0.002), metrics='accuracy')\n\nmodel.summary()","8430cf93":"X_train_split, X_val_split, Y_train_split, Y_val_split = train_test_split(X_train_logscaled, Y_train, test_size = 0.2, random_state = 42\n                                                    , stratify = Y_train)","0bb36b8c":"early_stopping = callbacks.EarlyStopping(\n    patience=25,\n    min_delta=0.0000001,\n    restore_best_weights=True,\n)\n\n#New callback\nplateau = callbacks.ReduceLROnPlateau(\n    factor = 0.5,                                     \n    patience = 2,                                   \n    min_delt = 0.0000001,                                \n    cooldown = 0,                               \n    verbose = 1\n) ","3eeb578a":"history = model.fit(X_train_split, Y_train_split,\n          batch_size = 256, epochs = 100,\n          validation_data=(X_val_split, Y_val_split),\n          callbacks=[early_stopping, plateau]);","2fe9f15c":"score = model.evaluate(X_val_split, Y_val_split, verbose = 0)\nprint('Test loss: {}'.format(score[0]))\nprint('Test accuracy: {}%'.format(score[1] * 100))","0c12d08f":"fig, ax = plt.subplots(figsize=(20,8))\nsns.lineplot(x = history.epoch, y = history.history['loss'])\nsns.lineplot(x = history.epoch, y = history.history['val_loss'])\nax.set_title('Learning Curve (Loss)')\nax.set_ylabel('Loss')\nax.set_xlabel('Epoch')\nax.legend(['train', 'test'], loc='best')\nplt.show()","fcaed836":"Y_train = df_train['target'].copy()\nY_train = Y_train.map(class_map).astype('int')\nY_train","96a3a07b":"def prediction (X_train, Y_train, X_test):\n    \n    keras.backend.clear_session()\n\n    kfold = StratifiedKFold(n_splits = 10)\n\n    y_pred = np.zeros((50000,4))\n    train_oof = np.zeros((100000,4))\n    \n    for idx in kfold.split(X=X_train, y=Y_train):\n        train_idx, val_idx = idx[0], idx[1]\n        xtrain = X_train.iloc[train_idx]\n        ytrain = Y_train.iloc[train_idx]\n        xval = X_train.iloc[val_idx]\n        yval = Y_train.iloc[val_idx]\n        \n        ytrain = to_categorical(ytrain)\n        yval = to_categorical(yval)\n        \n        # fit model for current fold\n        model = get_model()\n        model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(learning_rate=0.002), metrics='accuracy')\n        \n        model.fit(xtrain, ytrain,\n        batch_size = 256, epochs = 100,\n        validation_data=(xval, yval),\n        callbacks=[early_stopping, plateau]);\n\n        #create predictions\n        y_pred += model.predict(X_test)\/kfold.n_splits\n        print(y_pred)\n               \n        val_pred = model.predict(xval)\n        # getting out-of-fold predictions on training set\n        train_oof[val_idx] = val_pred\n        \n        # calculate and append logloss\n        fold_logloss = metrics.log_loss(yval,val_pred)\n        print(\"Logloss: {0:0.5f}\". format(fold_logloss))\n  \n    return y_pred, train_oof","16aa02e0":"nn_pred, train_oof = prediction (X_train_logscaled, Y_train, X_test_logscaled)","9bb6dbe1":"print(\"Logloss: {0:0.6f}\".format(metrics.log_loss(Y_train,train_oof)))","58d6c293":"train_oof = pd.DataFrame(train_oof, columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4'])\ntrain_oof","2f9e6965":"pred_test = pd.DataFrame(nn_pred, columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4'])\npred_test","ea000cb3":"train_oof.to_csv('nn_train_oof.csv', index=False)\n\ntrain_oof","b5c7407e":"output = pred_test\noutput['id'] = X_test.index\noutput.to_csv('submission.csv', index=False)\n\noutput","6f7234a9":"## Creating and Evaluating the NN","2b53c862":"## Importing Libraries and Datasets","22f30f21":"## Making Predictions","9aca981f":"# <center>Tabular Playground Series - May\/2021<center>\n## <center>Neural Network with Keras<center>\n---\n    \nAs I recently finished Kaggle\u2019s Intro to Deep Learning course, I decided to start practicing on this month\u2019s tabular playground competition. This is my first notebook on Neural Networks outside the exercises from Kaggle\u2019s course. If you spot any mistakes or have any tips to help me building some \u2018intuition\u2019 on the subject, feel free to share.\n \nBased on [@subinium](https:\/\/www.kaggle.com\/subinium) [great notebook for beginners](https:\/\/www.kaggle.com\/subinium\/tps-may-deeplearning-pipeline-for-beginner), I created a similar model and tweaked the number of nodes on hidden layers, dropout rate and the batch size, observing the train vs validation loss plot. Then, I added the early stopping procedure to halt the training and get the best weights.\n    \nUpdate: First, I\u2019ve applied log transformation and standard scaling to the dataset and reduced the number of units in each hidden layer of the NN. Then I changed the batch size, the learning rate and included the callback \u2018ReduceLROnPlateau\u2019, as suggested by [@pourchot](https:\/\/www.kaggle.com\/pourchot) and used in [his notebook](https:\/\/www.kaggle.com\/pourchot\/lb-1-0896-keras-nn-with-20-folds). Public LB score before these changes: 1.09559\n    \n    \nMy other notebooks in this competition:\n- [Tabular Playground Series - May\/2021: LightGBM Tuned with Hyperopt](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps05-21-lgbm-tuned-w-hyperopt)\n- [Tabular Playground Series - May\/2021: Model Stacking using Logistic Regression as Meta-Learner](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps05-21-model-stacking-meta-learner-lr)\n    "}}