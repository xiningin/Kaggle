{"cell_type":{"d95967a2":"code","eb3960c7":"code","8a181eca":"code","a0a9135e":"code","77b1f1a9":"code","b40cb091":"code","029ac118":"code","ef0c7f2b":"code","af11328f":"code","08de6447":"code","4c4ae874":"code","e9bedc57":"code","1714ab2d":"code","2160eba8":"code","c9364068":"code","07685081":"code","fa4c252c":"code","abf36468":"code","68f66fb3":"code","8de302f6":"code","d036ea3f":"code","e369587f":"code","9d10ac79":"code","8abcfd25":"code","a3b11012":"code","aac7e735":"code","83cec247":"code","d6dfc9a1":"code","80f9a8ef":"code","1b1729fa":"code","c29ef061":"code","1af1a013":"code","5a81c748":"code","15f18a3c":"code","1b049e12":"code","cd700821":"code","4172a36a":"code","1b1a6d55":"code","8028bbb8":"code","534f6aa4":"code","87bb4cad":"code","69c12bd3":"markdown","b89e79a4":"markdown","cb32ae10":"markdown","ceffae78":"markdown","c96233c1":"markdown","64727d18":"markdown","294811f8":"markdown","df6863ef":"markdown","fdc7a56a":"markdown","c55b2fe7":"markdown","80a29004":"markdown","2c2faed9":"markdown","37a28ea7":"markdown","d111d206":"markdown","e1085b50":"markdown","4341c2cd":"markdown","725ec47b":"markdown","64810630":"markdown","6bae77d9":"markdown","fd6f37d7":"markdown"},"source":{"d95967a2":"# Importing libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","eb3960c7":"# Configuring visualizations\n%matplotlib inline\nplt.rcParams['figure.figsize'] = 12, 8","8a181eca":"# Loading datasets\ntrain_set = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_set = pd.read_csv('..\/input\/titanic\/test.csv')\nX_train = train_set.iloc[:, [2, 4, 5, 6, 7, 9, 11]].values\nX_test = test_set.iloc[:, [1, 3, 4, 5, 6, 8, 10]].values\ny_train = train_set.iloc[:, 1].values","a0a9135e":"# Exploring train set\ntrain_set.info()\ntrain_set.describe(include='all')","77b1f1a9":"# Exploring test set\ntest_set.info()\ntest_set.describe(include='all')","b40cb091":"# Taking care of missing data (Age, Embarked, Fare)\nfrom sklearn.impute import SimpleImputer\nimputer_age = SimpleImputer(missing_values=np.nan, strategy='mean')\nX_train[:, 2:3] = imputer_age.fit_transform(X_train[:, 2:3])\nX_test[:, 2:3] = imputer_age.fit_transform(X_test[:, 2:3])\nimputer_embarked = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nX_train[:, 6:7] = imputer_embarked.fit_transform(X_train[:, 6:7])\nimputer_fare = SimpleImputer(missing_values=np.nan, strategy='median')\nX_test[:, 5:6] = imputer_fare.fit_transform(X_test[:, 5:6])","029ac118":"# Encoding categorical data (Sex)\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nX_train[:, 1] = labelencoder.fit_transform(X_train[:, 1])\nX_test[:, 1] = labelencoder.transform(X_test[:, 1])","ef0c7f2b":"# Encoding categorical data (PClass, Embarked)\nfrom sklearn.preprocessing import OneHotEncoder\nonehotencoder_pclass = OneHotEncoder(categories='auto', drop='first', sparse=False)\npclass_train = onehotencoder_pclass.fit_transform(X_train[:, 0:1])\npclass_test = onehotencoder_pclass.transform(X_test[:, 0:1])\nonehotencoder_embarked = OneHotEncoder(categories='auto', drop='first', sparse=False)\nembarked_train = onehotencoder_embarked.fit_transform(X_train[:, 6:7])\nembarked_test = onehotencoder_embarked.transform(X_test[:, 6:7])\nX_train = np.concatenate((pclass_train, X_train[:, 1:6], embarked_train), axis=1)\nX_test = np.concatenate((pclass_test, X_test[:, 1:6], embarked_test), axis=1)\ndel pclass_train, pclass_test, embarked_train, embarked_test","af11328f":"# Feature scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler(with_mean=True, with_std=True)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","08de6447":"# Building a logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(penalty='l2', C=1.0, fit_intercept=True,\n                                solver='lbfgs', multi_class='auto')","4c4ae874":"# Cross validation\nfrom sklearn.model_selection import cross_validate\nresults = cross_validate(classifier, X_train, y_train, cv=10, scoring='accuracy',\n                         return_train_score=True, return_estimator=False, n_jobs=-1)\nprint('train score:', results['train_score'].mean())\nprint('test score:', results['test_score'].mean())","e9bedc57":"# Plotting learning curves\nfrom sklearn.model_selection import learning_curve\nm_range = np.linspace(.05, 1.0, 20)\ntrain_sizes, train_scores, test_scores = learning_curve(classifier, X_train, y_train, cv=10,  \n                                                        train_sizes=m_range, shuffle=False,\n                                                        scoring='accuracy', n_jobs=-1)\nplt.figure()\nplt.title('Learning Curves')\nplt.ylim(.7, .9)\nplt.xlabel('Training examples')\nplt.ylabel('Score')\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\nplt.grid()\nplt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')\nplt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score')\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std, \n                 train_scores_mean + train_scores_std, alpha=0.1, color='r')\nplt.fill_between(train_sizes, test_scores_mean - test_scores_std, \n                 test_scores_mean + test_scores_std, alpha=0.1, color='g')\nplt.legend(loc='best')\nplt.show()","1714ab2d":"# Plotting validation curves\nfrom sklearn.model_selection import validation_curve\nC_range = np.geomspace(1e-3, 1e+1, 41)\ntrain_scores, test_scores = validation_curve(classifier, X_train, y_train, cv=10,\n                                             param_name='C', param_range=C_range,\n                                             scoring='accuracy', n_jobs=-1)\nplt.figure()\nplt.title('Validation Curves')\nplt.ylim(.6, .9)\nplt.xlabel('C')\nplt.ylabel('Score')\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\nplt.grid()\nplt.semilogx(C_range, train_scores_mean, label='Training score', color='darkorange', lw=2)\nplt.semilogx(C_range, test_scores_mean, label='Cross-validation score', color='navy', lw=2)\nplt.fill_between(C_range, train_scores_mean - train_scores_std, \n                 train_scores_mean + train_scores_std, alpha=0.2, color='darkorange', lw=2)\nplt.fill_between(C_range, test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std, alpha=0.2, color='navy', lw=2)\nplt.legend(loc='best')\nplt.show()","2160eba8":"# Createing a new feature representing family sizes\ntrain_set['Family'] = train_set['SibSp'] + train_set['Parch'] + 1\ntest_set['Family'] = test_set['SibSp'] + test_set['Parch'] +1","c9364068":"# Conducting preprocessing of the new feature\nfamily_train = train_set['Family'].values\nfamily_train = family_train.reshape(len(family_train), 1)\nfamily_test = test_set['Family'].values\nfamily_test = family_test.reshape(len(family_test), 1)\nscaler_family = StandardScaler(with_mean=True, with_std=True)\nfamily_train = scaler_family.fit_transform(family_train)\nfamily_test = scaler_family.fit_transform(family_test)\nX_train = np.concatenate((X_train[:, :4], family_train, X_train[:, 6:]), axis=1)\nX_test = np.concatenate((X_test[:, :4], family_test, X_test[: ,6:]), axis=1)\ndel family_train, family_test","07685081":"# Building a logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(penalty='l2', C=1.0, fit_intercept=True,\n                                solver='lbfgs', multi_class='auto')","fa4c252c":"# Cross validation\nfrom sklearn.model_selection import cross_validate\nresults = cross_validate(classifier, X_train, y_train, cv=10, scoring='accuracy',\n                         return_train_score=True, return_estimator=False, n_jobs=-1)\nprint('train score:', results['train_score'].mean())\nprint('test score:', results['test_score'].mean())","abf36468":"# Banding family sizes\ntrain_set['FamilyType'] = 0\ntrain_set.loc[train_set['Family'] == 1, 'FamilyType'] = 'Single'\ntrain_set.loc[(train_set['Family'] >= 2) & (train_set['Family'] <= 4), 'FamilyType'] = 'Small'\ntrain_set.loc[(train_set['Family'] >= 5), 'FamilyType'] = 'Large'\ntest_set['FamilyType'] = 0\ntest_set.loc[test_set['Family'] == 1, 'FamilyType'] = 'Single'\ntest_set.loc[(test_set['Family'] >= 2) & (test_set['Family'] <= 4), 'FamilyType'] = 'Small'\ntest_set.loc[(test_set['Family'] >= 5), 'FamilyType'] = 'Large'","68f66fb3":"# Conducting preprocessing of the new feature\nfamily_train = train_set['FamilyType'].values\nfamily_train = family_train.reshape(len(family_train), 1)\nfamily_test = test_set['FamilyType'].values\nfamily_test = family_test.reshape(len(family_test), 1)\nonehotencoder_family = OneHotEncoder(categories='auto', drop='first', sparse=False)\nfamily_train = onehotencoder_family.fit_transform(family_train)\nfamily_test = onehotencoder_family.transform(family_test)\nscaler_family = StandardScaler(with_mean=True, with_std=True)\nfamily_train = scaler_family.fit_transform(family_train)\nfamily_test = scaler_family.fit_transform(family_test)\nX_train = np.concatenate((X_train[:, :4], family_train, X_train[:, 5:]), axis=1)\nX_test = np.concatenate((X_test[:, :4], family_test, X_test[:, 5:]), axis=1)\ndel family_train, family_test","8de302f6":"# Building a logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(penalty='l2', C=1.0, fit_intercept=True,\n                                solver='lbfgs', multi_class='auto')","d036ea3f":"# Cross validation\nfrom sklearn.model_selection import cross_validate\nresults = cross_validate(classifier, X_train, y_train, cv=10, scoring='accuracy',\n                         return_train_score=True, return_estimator=False, n_jobs=-1)\nprint('train score:', results['train_score'].mean())\nprint('test score:', results['test_score'].mean())","e369587f":"# Banding age\ntrain_set['AgeBand'] = 0\ntrain_set.loc[train_set['Age'] <= 16, 'AgeBand'] = 1\ntrain_set.loc[(train_set['Age'] > 16) & (train_set['Age'] <= 32), 'AgeBand'] = 2\ntrain_set.loc[(train_set['Age'] > 32) & (train_set['Age'] <= 48), 'AgeBand'] = 3\ntrain_set.loc[(train_set['Age'] > 48) & (train_set['Age'] <= 64), 'AgeBand'] = 4\ntrain_set.loc[(train_set['Age'] > 64), 'AgeBand'] = 5\ntest_set['AgeBand'] = 0\ntest_set.loc[test_set['Age'] <= 16, 'AgeBand'] = 1\ntest_set.loc[(test_set['Age'] > 16) & (test_set['Age'] <= 32), 'AgeBand'] = 2\ntest_set.loc[(test_set['Age'] > 32) & (test_set['Age'] <= 48), 'AgeBand'] = 3\ntest_set.loc[(test_set['Age'] > 48) & (test_set['Age'] <= 64), 'AgeBand'] = 4\ntest_set.loc[(test_set['Age'] > 64), 'AgeBand'] = 5\n","9d10ac79":"# Conducting preprocessing of the new feature\nage_train = train_set['AgeBand'].values\nage_train = age_train.reshape(len(age_train), 1)\nage_test = test_set['AgeBand'].values\nage_test = age_test.reshape(len(age_test), 1)\nonehotencoder_age = OneHotEncoder(categories='auto', drop='first', sparse=False)\nage_train = onehotencoder_age.fit_transform(age_train)\nage_test = onehotencoder_age.transform(age_test)\nscaler_age = StandardScaler(with_mean=True, with_std=True)\nage_train = scaler_age.fit_transform(age_train)\nage_test = scaler_age.fit_transform(age_test)\nX_train = np.concatenate((X_train[:, :3], age_train, X_train[:, 4:]), axis=1)\nX_test = np.concatenate((X_test[:, :3], age_test, X_test[:, 4:]), axis=1)","8abcfd25":"# Building a logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(penalty='l2', C=1.0, fit_intercept=True,\n                                solver='lbfgs', multi_class='auto')","a3b11012":"# Cross validation\nfrom sklearn.model_selection import cross_validate\nresults = cross_validate(classifier, X_train, y_train, cv=10, scoring='accuracy',\n                         return_train_score=True, return_estimator=False, n_jobs=-1)\nprint('train score:', results['train_score'].mean())\nprint('test score:', results['test_score'].mean())","aac7e735":"# Banding fare\ntrain_set['FareBand'] = 0\ntrain_set.loc[train_set['Fare'] <= 8.5, 'FareBand'] = 1\ntrain_set.loc[(train_set['Fare'] > 8.5) & (train_set['Fare'] <= 16.5), 'FareBand'] = 2\ntrain_set.loc[(train_set['Fare'] > 16.5) & (train_set['Fare'] <= 32.5), 'FareBand'] = 3\ntrain_set.loc[(train_set['Fare'] > 32.5), 'FareBand'] = 4\ntest_set['FareBand'] = 0\ntest_set.loc[test_set['Fare'] <= 8.5, 'FareBand'] = 1\ntest_set.loc[(test_set['Fare'] > 8.5) & (test_set['Fare'] <= 16.5), 'FareBand'] = 2\ntest_set.loc[(test_set['Fare'] > 16.5) & (test_set['Fare'] <= 32.5), 'FareBand'] = 3\ntest_set.loc[(test_set['Fare'] > 32.5), 'FareBand'] = 4\ntest_set.loc[test_set['Fare'].isnull(), 'FareBand'] = 2","83cec247":"# Conducting preprocessing of the new feature\nfare_train = train_set['FareBand'].values\nfare_train = fare_train.reshape(len(fare_train), 1)\nfare_test = test_set['FareBand'].values\nfare_test = fare_test.reshape(len(fare_test), 1)\nonehotencoder_fare = OneHotEncoder(categories='auto', drop='first', sparse=False)\nfare_train = onehotencoder_fare.fit_transform(fare_train)\nfare_test = onehotencoder_fare.transform(fare_test)\nscaler_fare = StandardScaler(with_mean=True, with_std=True)\nfare_train = scaler_fare.fit_transform(fare_train)\nfare_test = scaler_fare.fit_transform(fare_test)\nX_train = np.concatenate((X_train[:, :10], fare_train, X_train[:, 11:]), axis=1)\nX_test = np.concatenate((X_test[:, :10], fare_test, X_test[:, 11:]), axis=1)\ndel fare_train, fare_test","d6dfc9a1":"# Building a logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(penalty='l2', C=1.0, fit_intercept=True,\n                                solver='lbfgs', multi_class='auto')","80f9a8ef":"# Cross validation\nfrom sklearn.model_selection import cross_validate\nresults = cross_validate(classifier, X_train, y_train, cv=10, scoring='accuracy',\n                         return_train_score=True, return_estimator=False, n_jobs=-1)\nprint('train score:', results['train_score'].mean())\nprint('test score:', results['test_score'].mean())","1b1729fa":"# Loading datasets\ntrain_set = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_set = pd.read_csv('..\/input\/titanic\/test.csv')\nX_train = train_set.iloc[:, [2, 4, 5, 6, 7, 9, 11]].values\nX_test = test_set.iloc[:, [1, 3, 4, 5, 6, 8, 10]].values\ny_train = train_set.iloc[:, 1].values","c29ef061":"# Taking care of missing data (Age, Embarked, Fare)\nfrom sklearn.impute import SimpleImputer\nimputer_age = SimpleImputer(missing_values=np.nan, strategy='mean')\nX_train[:, 2:3] = imputer_age.fit_transform(X_train[:, 2:3])\nX_test[:, 2:3] = imputer_age.fit_transform(X_test[:, 2:3])\nimputer_embarked = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nX_train[:, 6:7] = imputer_embarked.fit_transform(X_train[:, 6:7])\nimputer_fare = SimpleImputer(missing_values=np.nan, strategy='median')\nX_test[:, 5:6] = imputer_fare.fit_transform(X_test[:, 5:6])","1af1a013":"# Encoding categorical data (Sex)\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nX_train[:, 1] = labelencoder.fit_transform(X_train[:, 1])\nX_test[:, 1] = labelencoder.transform(X_test[:, 1])","5a81c748":"# Encoding categorical data (PClass, Embarked)\nfrom sklearn.preprocessing import OneHotEncoder\nonehotencoder_pclass = OneHotEncoder(categories='auto', drop='first', sparse=False)\npclass_train = onehotencoder_pclass.fit_transform(X_train[:, 0:1])\npclass_test = onehotencoder_pclass.transform(X_test[:, 0:1])\nonehotencoder_embarked = OneHotEncoder(categories='auto', drop='first', sparse=False)\nembarked_train = onehotencoder_embarked.fit_transform(X_train[:, 6:7])\nembarked_test = onehotencoder_embarked.transform(X_test[:, 6:7])\nX_train = np.concatenate((pclass_train, X_train[:, 1:6], embarked_train), axis=1)\nX_test = np.concatenate((pclass_test, X_test[:, 1:6], embarked_test), axis=1)\ndel pclass_train, pclass_test, embarked_train, embarked_test","15f18a3c":"# Feature mapping for polynomial regression and feature scaling\nfrom sklearn.preprocessing import PolynomialFeatures\nfeatures = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\nX_train = features.fit_transform(X_train)\nX_test = features.fit_transform(X_test)","1b049e12":"# Feature scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler(with_mean=True, with_std=True)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","cd700821":"# Building a logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(penalty='l2', C=1.0, fit_intercept=True,\n                                solver='lbfgs', multi_class='auto')","4172a36a":"# Cross validation\nfrom sklearn.model_selection import cross_validate\nresults = cross_validate(classifier, X_train, y_train, cv=10, scoring='accuracy',\n                         return_train_score=True, return_estimator=False, n_jobs=-1)\nprint('train score:', results['train_score'].mean())\nprint('test score:', results['test_score'].mean())","1b1a6d55":"# Plotting learning curves\nfrom sklearn.model_selection import learning_curve\nm_range = np.linspace(.05, 1.0, 20)\ntrain_sizes, train_scores, test_scores = learning_curve(classifier, X_train, y_train, cv=10,  \n                                                        train_sizes=m_range, shuffle=False,\n                                                        scoring='accuracy', n_jobs=-1)\nplt.figure()\nplt.title('Learning Curves')\nplt.ylim(.7, .96)\nplt.xlabel('Training examples')\nplt.ylabel('Score')\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\nplt.grid()\nplt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')\nplt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score')\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std, \n                 train_scores_mean + train_scores_std, alpha=0.1, color='r')\nplt.fill_between(train_sizes, test_scores_mean - test_scores_std, \n                 test_scores_mean + test_scores_std, alpha=0.1, color='g')\nplt.legend(loc='best')\nplt.show()","8028bbb8":"# Plotting validation curves\nfrom sklearn.model_selection import validation_curve\nC_range = np.geomspace(1e-4, 1e+2, 31)\ntrain_scores, test_scores = validation_curve(classifier, X_train, y_train, cv=10,\n                                             param_name='C', param_range=C_range,\n                                             scoring='accuracy', n_jobs=-1)\nplt.figure()\nplt.title('Validation Curves')\nplt.ylim(.6, .9)\nplt.xlabel('C')\nplt.ylabel('Score')\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\nplt.grid()\nplt.semilogx(C_range, train_scores_mean, label='Training score', color='darkorange', lw=2)\nplt.semilogx(C_range, test_scores_mean, label='Cross-validation score', color='navy', lw=2)\nplt.fill_between(C_range, train_scores_mean - train_scores_std, \n                 train_scores_mean + train_scores_std, alpha=0.2, color='darkorange', lw=2)\nplt.fill_between(C_range, test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std, alpha=0.2, color='navy', lw=2)\nplt.legend(loc='best')\nplt.show()","534f6aa4":"# Grid search\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'C': list(np.geomspace(1e-2, 1e-0, 21))}\ngrid_search = GridSearchCV(classifier, param_grid, iid=False, refit=True, cv=10, \n                           return_train_score=True, scoring='accuracy', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('best parameters:', grid_search.best_params_)\nprint('best score:', grid_search.best_score_)","87bb4cad":"# Making predictions and submitting outputs\ny_pred = grid_search.predict(X_test)\nsubmission = pd.DataFrame({'PassengerId': test_set.iloc[:, 0].values,\n                           'Survived': y_pred})\nsubmission.to_csv('submission_logistic.csv', index=False)","69c12bd3":"The test score is somewhat higher than those we obtained previously. We thus use this model to make our final predictions.  ","b89e79a4":"Next we want to try banding the new feature representing family sizes. The idea is that type of a family (single\/small\/large) may be more informative than the number of family numbers aboard ship. ","cb32ae10":"Now we can submit our predictions.","ceffae78":"Next we want to try banding feature Fare. Again we fit a new model to our new train set and check the validity. The test score is slighty better than before and thus we decide to keep this new feature. We can also use the codes above to plot learning curves and validation curves to get a better understanding of how our model performs.","c96233c1":"The following shell could be run for our various models.","64727d18":"Feature Age in both train set and test set have considerably large number of missing values. Feature Embarked in train set has two missing values. Feature Fare in test set has only one missing value. We need to take care of them before next step.","294811f8":"We want to create a new feature representing family sizes by combining features SibSp and ParCh. The idea is that this new feature may be more imformative and linked with survival probabilities of passengers.","df6863ef":"The following shell could be run repeatedly for our various models.","fdc7a56a":"Now we can fitting a model to our new train set. Then we use cross validation to check out whether this engineering improved our model. The test score is slighty better than before and thus we decide to keep this new feature. We can also use the codes above to plot learning curves and validation curves to get a better understanding of how our model performs.","c55b2fe7":"We can use grid search to choose the optimal choice of C. ","80a29004":"We'd like to quickly build a messy model before heading into further process of model refinement. Obviously, features like Age, Sex and PClass are more likely to be linked with survival probabilities of passengers. We also want to include features like SibSp, ParCh, Fare, Embarked into our first model before deciding whether to keep them. Features like Name, Cabin and Ticket contain may contain information to be find, but processing these features my be kind of uncertain and bothering, and Cabin contains lots of missing values. We want to leave these features for our last considerastion to engineer.","2c2faed9":"Features Sex and Embarked are categorical data and need to be transformed into numerical data. Further more, features PClass and Embarked contains more than two classes and we need to create dummy variables for our purpose. After this, we can do the feature scaling to transform features into comparable sizes.","37a28ea7":"Learning plot is a very helpful tool for diagnostic prupose. In the learning plot below, learning curves for train score and test score converged quickly and showed no sign of overfitting. By this, we may want to reduce bias to elevate performence of our model. Possible remedial measures include adding features that are not previously included in our model or polynomial features, and of course tuning the penalty parameter C. ","d111d206":"To figure out the propriate range for penalty parameter C, we may want to plot validation curves. It turns out that the optimal choice for C is around 1e-2, but default setting C=1.0 performs fairly well too. ","e1085b50":"Now we are ready to build our first logistic regression model. We want to use cross validation to quickly check how our model works. Though cross validation gives us a mean test score that is not too bad, we want to give our model a diagnosis to figure out how we can improve our model.","4341c2cd":"Next we want to try banding feature Age. Again we fit a new model to our new train set and check the validity. The test score is slighty better than before and thus we decide to keep this new feature. We can also use the codes above to plot learning curves and validation curves to get a better understanding of how our model performs.","725ec47b":"References:\n\n[Logistic regression](https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#logistic-regression)\n\n[Plotting Learning Curves](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py)\n\n[Plotting Validation Curves](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_validation_curve.html#sphx-glr-auto-examples-model-selection-plot-validation-curve-py)","64810630":"Now we want to fit a polynomial logistic regression model to see what happens. Note that we have to add polynomial features before conducting feature scaling, which is definitely necessary under this circumstance. ","6bae77d9":"Again we fit a new model to our new train set and check the validity. The test score is slighty better than before and thus we decide to keep this new feature. We can also use the codes above to plot learning curves and validation curves to get a better understanding of how our model performs.","fd6f37d7":"Feature mapping enables us to add polynomial features into the origianl dataset. We can also try different degrees to obtian the optimal model. Note that degrees greater than three are not appropriate considering the relative sizes between number of samples and number of features."}}