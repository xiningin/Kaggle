{"cell_type":{"0cf75832":"code","70c1f8cb":"code","f087221d":"code","d35ea877":"code","c6f83d09":"code","c22d6265":"code","73d970e4":"code","b4bee289":"code","8b88d03a":"code","cc386158":"code","412c6b7a":"code","50e26c5e":"code","d29d8013":"code","feedf131":"code","d25b9296":"code","00897a66":"code","fdac4e00":"code","64f1cad2":"code","51c2a63a":"code","b4c8b67b":"code","fde48594":"code","6b2b60c4":"code","ec7f82ea":"code","06dc099c":"code","64e45970":"code","3ee17373":"code","c03a8d50":"code","20d68096":"code","3ab82900":"code","59a1a894":"code","a8ebaa2b":"code","d2b1a85f":"code","41ddc128":"code","97260f98":"markdown","13082958":"markdown","433d818a":"markdown","c3275bd7":"markdown","2706f567":"markdown","c38006d6":"markdown","7a34cab4":"markdown","eb1de339":"markdown","deee05ee":"markdown","1f11177b":"markdown","e05f3322":"markdown"},"source":{"0cf75832":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()","70c1f8cb":"df = pd.read_csv('https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/adult\/adult.data', sep=', ')\n# \u041d\u0430\u0437\u043d\u0430\u0447\u0430\u0435\u043c \u0438\u043c\u0435\u043d\u0430 \u043a\u043e\u043b\u043e\u043d\u043e\u043a\ncolumns = ('age workclass fnlwgt education educ-num marital-status occupation relationship '\n           'race sex capital-gain capital-loss  hours-per-week native-country salary')\n\nnumeric_indices = np.array([0, 2, 4, 10, 11, 12])\ncategorical_indices = np.array([1, 3, 5, 6, 7, 8, 9, 13])\n\ndf.columns = columns.split() # this method will divide dataset on columns like in massive above\n\ndf = df.replace('?', np.nan)\n\ndf = df.dropna()\n\ndf['salary'] = df['salary'].apply((lambda x: x=='>50K')) # (True) > 50$,(False) < 50$","f087221d":"numeric_data = df[df.columns[numeric_indices]]\n\ncategorial_data = df[df.columns[categorical_indices]]\ncategorial_data.head()","d35ea877":"df['education'].unique(), len(df['education'].unique())","c6f83d09":"dummy_features = pd.get_dummies(categorial_data)","c22d6265":"X = pd.concat([numeric_data, dummy_features], axis=1)\nX.head()","73d970e4":"y = df['salary']\nX.shape","b4bee289":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(X.values, y.values, train_size=0.8)","8b88d03a":"def search_and_draw(X, y, model, param_name, grid, param_scale='ordinary', draw=True):\n    parameters = {param_name: grid}\n    \n    CV_model = GridSearchCV(estimator=model, \n                            param_grid=parameters,\n                            cv=5, \n                            scoring='f1',\n                            n_jobs=-1, \n                            verbose=10)\n    CV_model.fit(X, y)\n    means = CV_model.cv_results_['mean_test_score']\n    error = CV_model.cv_results_['std_test_score']\n    \n    if draw:\n        plt.figure(figsize=(15,8))\n        plt.title('choose ' + param_name)\n\n\n        if (param_scale == 'log'):\n            plt.xscale('log')\n\n        plt.plot(grid, means, label='mean values of score', color='red', lw=3)\n\n        plt.fill_between(grid, means - 2 * error, means + 2 * error, \n                         color='green', label='filled area between errors', alpha=0.5)\n        legend_box = plt.legend(framealpha=1).get_frame()\n        legend_box.set_facecolor(\"white\")\n        legend_box.set_edgecolor(\"black\")\n        plt.xlabel('parameter')\n        plt.ylabel('roc_auc')\n        plt.show()\n        \n    return means, error, CV_model","cc386158":"models = [KNeighborsClassifier(), DecisionTreeClassifier()]\nparam_names = ['n_neighbors', 'max_depth']\ngrids = [np.array(np.linspace(4, 30, 8), dtype='int'), np.arange(1, 30)]\nparam_scales = ['log', 'ordinary']","412c6b7a":"for model, param_name, grid, param_scale in zip(models, \n                                                param_names, \n                                                grids, \n                                                param_scales):\n    search_and_draw(X_train, y_train, model, param_name, grid, param_scale)","50e26c5e":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm.notebook import tqdm","d29d8013":"max_trees = 100\n\nvalues = np.arange(max_trees) + 1\n\nkf = KFold(n_splits=5, shuffle=True, random_state=1234)\n\nglobal_scores = []\n\nfor train_indices, val_indices in tqdm(kf.split(X_train), total=5):\n    scores = []\n    \n    X_train_kf = X_train[train_indices]\n    y_train_kf = y_train[train_indices]\n    \n    X_val_kf = X_train[val_indices]\n    y_val_kf = y_train[val_indices]\n    \n    forest = RandomForestClassifier(n_estimators=max_trees)\n    forest.fit(X_train_kf, y_train_kf)\n    trees = forest.estimators_\n    \n    for number_of_trees in tqdm(values, leave=False):\n        thinned_forest = RandomForestClassifier(n_estimators=number_of_trees)\n        \n        thinned_forest.n_classes_ = 2\n        thinned_forest.estimators_ = trees[:number_of_trees]\n\n        scores.append(roc_auc_score(y_val_kf, thinned_forest.predict_proba(X_val_kf)[:, 1]))\n    \n    scores = np.array(scores)\n    \n    global_scores.append(scores)\n\nglobal_scores = np.stack(global_scores, axis=0)","feedf131":"mean_cross_val_score = global_scores.mean(axis=0)\nstd_cross_val_score = global_scores.std(axis=0)\n\nplt.figure(figsize=(15,8))\nplt.title('Quality of random forest')\n\nplt.plot(values, mean_cross_val_score, label='mean values', color='red', lw=3)\nplt.fill_between(values, \n                 mean_cross_val_score - 2 * std_cross_val_score, \n                 mean_cross_val_score + 2 * std_cross_val_score, \n                 color='green', \n                 label='filled area between errors',\n                 alpha=0.5)\nlegend_box = plt.legend(framealpha=1).get_frame()\nlegend_box.set_facecolor(\"white\")\nlegend_box.set_edgecolor(\"black\")\nplt.xlabel('number of trees')\nplt.ylabel('roc-auc')\n\nplt.show()","d25b9296":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","00897a66":"search_and_draw(X_train[:, numeric_indices], \n                y_train, \n                KNeighborsClassifier(), \n                'n_neighbors', \n                np.array(np.linspace(4, 30, 8), dtype='int'), \n                'ordinary')","fdac4e00":"model = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n\nmodel.fit(X_train, y_train)\ny_train_predicted = model.predict_proba(X_train)[:, 1]\ny_test_predicted = model.predict_proba(X_test)[:, 1]","64f1cad2":"from sklearn.metrics import roc_auc_score, roc_curve","51c2a63a":"train_auc = roc_auc_score(y_train, y_train_predicted)\ntest_auc = roc_auc_score(y_test, y_test_predicted)\n\nplt.figure(figsize=(20,10))\nplt.plot(*roc_curve(y_train, y_train_predicted)[:2], label='train AUC={:.4f}'.format(train_auc))\nplt.plot(*roc_curve(y_test, y_test_predicted)[:2], label='test AUC={:.4f}'.format(test_auc))\nlegend_box = plt.legend(fontsize='large', framealpha=1).get_frame()\nlegend_box.set_facecolor(\"white\")\nlegend_box.set_edgecolor(\"black\")\nplt.plot(np.linspace(0,1,100), np.linspace(0,1,100))\nplt.show()","b4c8b67b":"import xgboost # eXtreme gradient boosting model","fde48594":"boosting_model = xgboost.XGBClassifier(n_estimators=500)\n\nboosting_model.fit(X_train, y_train)\n\ny_train_predicted = boosting_model.predict_proba(X_train)[:, 1]\ny_test_predicted = boosting_model.predict_proba(X_test)[:, 1]","6b2b60c4":"train_auc = roc_auc_score(y_train, y_train_predicted)\ntest_auc = roc_auc_score(y_test, y_test_predicted)\n\nplt.figure(figsize=(10,7))\nplt.plot(*roc_curve(y_train, y_train_predicted)[:2], label='train AUC={:.4f}'.format(train_auc))\nplt.plot(*roc_curve(y_test, y_test_predicted)[:2], label='test AUC={:.4f}'.format(test_auc))\nlegend_box = plt.legend(fontsize='large', framealpha=1).get_frame()\nlegend_box.set_facecolor(\"white\")\nlegend_box.set_edgecolor(\"black\")\nplt.plot(np.linspace(0,1,100), np.linspace(0,1,100))\nplt.show()","ec7f82ea":"import lightgbm # light gradient boosting model","06dc099c":"boosting_model = lightgbm.LGBMClassifier(n_estimators=500)\n\nboosting_model.fit(X_train, y_train)\n\ny_train_predicted = boosting_model.predict_proba(X_train)[:, 1]\ny_test_predicted = boosting_model.predict_proba(X_test)[:, 1]","64e45970":"train_auc = roc_auc_score(y_train, y_train_predicted)\ntest_auc = roc_auc_score(y_test, y_test_predicted)\n\nplt.figure(figsize=(10,7))\nplt.plot(*roc_curve(y_train, y_train_predicted)[:2], label='train AUC={:.4f}'.format(train_auc))\nplt.plot(*roc_curve(y_test, y_test_predicted)[:2], label='test AUC={:.4f}'.format(test_auc))\nlegend_box = plt.legend(fontsize='large', framealpha=1).get_frame()\nlegend_box.set_facecolor(\"white\")\nlegend_box.set_edgecolor(\"black\")\nplt.plot(np.linspace(0,1,100), np.linspace(0,1,100))\nplt.show()","3ee17373":"from  sklearn.ensemble import GradientBoostingClassifier","c03a8d50":"boosting_model = GradientBoostingClassifier(n_estimators=500)\n\nboosting_model.fit(X_train, y_train)\n\ny_train_predicted = boosting_model.predict_proba(X_train)[:, 1]\ny_test_predicted = boosting_model.predict_proba(X_test)[:, 1]","20d68096":"train_auc = roc_auc_score(y_train, y_train_predicted)\ntest_auc = roc_auc_score(y_test, y_test_predicted)\n\nplt.figure(figsize=(10,7))\nplt.plot(*roc_curve(y_train, y_train_predicted)[:2], label='train AUC={:.4f}'.format(train_auc))\nplt.plot(*roc_curve(y_test, y_test_predicted)[:2], label='test AUC={:.4f}'.format(test_auc))\nlegend_box = plt.legend(fontsize='large', framealpha=1).get_frame()\nlegend_box.set_facecolor(\"white\")\nlegend_box.set_edgecolor(\"black\")\nplt.plot(np.linspace(0,1,100), np.linspace(0,1,100))\nplt.show()","3ab82900":"import catboost # categorical boosting, cats have nothing to do with it.","59a1a894":"boosting_model = catboost.CatBoostClassifier(n_estimators=500, silent=True, eval_metric='AUC')\n\nboosting_model.fit(X_train, y_train)\n\ny_train_predicted = boosting_model.predict_proba(X_train)[:, 1]\ny_test_predicted = boosting_model.predict_proba(X_test)[:, 1]","a8ebaa2b":"train_auc = roc_auc_score(y_train, y_train_predicted)\ntest_auc = roc_auc_score(y_test, y_test_predicted)\n\nplt.figure(figsize=(10,7))\nplt.plot(*roc_curve(y_train, y_train_predicted)[:2], label='train AUC={:.4f}'.format(train_auc))\nplt.plot(*roc_curve(y_test, y_test_predicted)[:2], label='test AUC={:.4f}'.format(test_auc))\nlegend_box = plt.legend(fontsize='large', framealpha=1).get_frame()\nlegend_box.set_facecolor(\"white\")\nlegend_box.set_edgecolor(\"black\")\nplt.plot(np.linspace(0,1,100), np.linspace(0,1,100))\nplt.show()","d2b1a85f":"boosting_model = catboost.CatBoostClassifier(n_estimators=500, silent=True, eval_metric='AUC')\nboosting_model.grid_search({'l2_leaf_reg': np.linspace(0, 1, 20)}, X_train, y_train, plot=True, refit=True)","41ddc128":"y_train_predicted = boosting_model.predict_proba(X_train)[:, 1]\ny_test_predicted = boosting_model.predict_proba(X_test)[:, 1]\n\ntrain_auc = roc_auc_score(y_train, y_train_predicted)\ntest_auc = roc_auc_score(y_test, y_test_predicted)\n\nplt.figure(figsize=(10,7))\nplt.plot(*roc_curve(y_train, y_train_predicted)[:2], label='train AUC={:.4f}'.format(train_auc))\nplt.plot(*roc_curve(y_test, y_test_predicted)[:2], label='test AUC={:.4f}'.format(test_auc))\nlegend_box = plt.legend(fontsize='large', framealpha=1).get_frame()\nlegend_box.set_facecolor(\"white\")\nlegend_box.set_edgecolor(\"black\")\nplt.plot(np.linspace(0,1,100), np.linspace(0,1,100))\nplt.show()","97260f98":"# Cross-validation(k-fold)\n![cross_val.png](attachment:cross_val.png)\n# This is guide about \"How to build best machine learning model using cross validation\"\n****SUCH APPROACH ALSO USED IN STACKING\nIDEA OF CROSS VALIDATION IS USED TO USED IN CHOOSE THE BEST MODEL,AND FIGHT WITH RETRAINING****\n\nAuthor:Albert Bagdasarov.","13082958":"Cool, yeah? There are other built-in implementations of gradient boosting besides XGBoost: these are LGBoost (light gradient boosting model) and built into the sklearn framework - GradientBoosting. In addition there is also CatBoost (categorical boosting, no cat was harmed), but more on that later.","433d818a":"# BOOSTING\n\n\n![%D0%91%D0%B5%D0%B7%20%D0%BD%D0%B0%D0%B7%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F.png](attachment:%D0%91%D0%B5%D0%B7%20%D0%BD%D0%B0%D0%B7%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F.png)","c3275bd7":"What also we can do?\n\nWe selected the optimal one-dimensional parameter for the algorithm. \n\nYou can also:\n\n1. Search the grid not only for numerical hyperparameters, but also for categorical ones, for example, a metric in the nearest neighbors algorithm or a branching criterion in a decision tree.\n2. Search for the optimal parameter on a multidimensional grid. It will not work to go over all the possible options here, because it will take too much time. But you can iterate over random points on the grid. This procedure is called Randomized Grid Search.","2706f567":"# ONE HOTE ENCODING\n\n*The main idea of this method consist in next statements.*\n\nLet there one categorical variable(for example \"color\") take n different meanings(Red,Yellow,Green).So we could create n's new variables correspond to different categorical signs,which equal 1 and 0.\n\n![one_hot.png](attachment:one_hot.png)","c38006d6":"Let's try in a couple of lines to beat all the quality that we were looking for so hard.","7a34cab4":"If you found this notebook useful,please upvote.","eb1de339":"`StandardScaler` Perform transformation $$z = \\frac{x - \\mu}{\\sigma}, \\text{\u0433\u0434\u0435 $\\sigma$ - Standart Deviation, \u0430  $\\mu$ - average}$$","deee05ee":"Let's pick up parameters for n_estimators in random foresr algotihm.\nInteresting Fact: Random Forest doesnt getting retraining.\nThats why graph will be monotonously rise.So we need to find the minimum meaning of n_estimators.\nEvery tree train separetly from others,so we just need to train forest.\n","1f11177b":"****SELECTION OF THE OPTIMAL MODEL****\n\n****So now we are gonna practise in educational process****\n1. Estimate and validate model\n2. Recruitment of most optimal hyperparameters\n3. Mixing models\n4. Took data signs from every model\n5. Connect all models\n\nWe are gonna to solve binary classification task,to build algorithm which will identify if person's income are gonna exceed $50k. ","e05f3322":"****Feature normalization****\n\nNormalize features and do the same with random forest."}}