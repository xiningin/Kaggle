{"cell_type":{"162ca44b":"code","1b296447":"code","363e5352":"code","e2f7690a":"code","a669ad24":"code","c975f27e":"code","dc9112c6":"code","1e72cf6f":"code","8b07eda7":"code","e240704e":"code","5a6dea7d":"code","52116ad1":"code","92bf2744":"markdown","b9b8bb2f":"markdown","4aa010ac":"markdown","9903c2d0":"markdown","a480772a":"markdown","595f0afb":"markdown","464cc8e7":"markdown","5556e3bd":"markdown","45be4afa":"markdown","13ce8810":"markdown"},"source":{"162ca44b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1b296447":"import matplotlib.pyplot as plt","363e5352":"def initialize_parameters(train_set):\n    length = train_set.shape[0]\n    w = np.random.randn(1, length)\n    b = 0\n    return w, b","e2f7690a":"def forward_prop(w, b, X):\n    z = np.dot(w, X) + b\n    return z","a669ad24":"def cost_function(z, y):\n    m = y.shape[1]\n    J = (1 \/ (2 * m)) * (np.sum((z - y) ** 2))\n    return J","c975f27e":"def back_prop(X, y, z):\n    m = y.shape[1]\n    dz = (1 \/ m) * (z - y)\n    dw = np.dot(dz, X.T)\n    db = np.sum(dz)\n    return dw, db","dc9112c6":"def gradient_descent(w, b, dw, db, rate):\n    w = w - (rate * dw)\n    b = b - (rate * db)\n    return w, b","1e72cf6f":"def linear_regression(X_train, y_train, X_val, y_val, rate, epochs):\n    \n    y_train = np.array([y_train])\n    y_val = np.array([y_val])\n    X_train = X_train.T\n    X_val = X_val.T\n    m_train = y_train.shape[1]\n    m_val = y_val.shape[1]\n    \n    X_train = (X_train - np.min(X_train)) \/ (np.max(X_train) - np.min(X_train))\n    X_val = (X_val - np.min(X_val)) \/ (np.max(X_val) - np.min(X_val))\n    \n    w, b = initialize_parameters(X_train)\n    \n    rmse_train_list = []\n    rmse_val_list = []\n\n    for i in range(1 , epochs + 1):\n        z_train = forward_prop(w, b, X_train)\n        cost_train = cost_function(z_train, y_train)\n        dw, db = back_prop(X_train, y_train, z_train)\n    \n        rmse_train = (np.sum((z_train - y_train) ** 2) \/ m_train) ** 0.5\n        rmse_train_list.append(rmse_train)\n    \n        z_val = forward_prop(w, b, X_val)\n        cost_val = cost_function(z_val, y_val)\n        rmse_val = (np.sum((z_val - y_val) ** 2) \/ m_val) ** 0.5\n        rmse_val_list.append(rmse_val)\n\n        w, b = gradient_descent(w, b, dw, db, rate)\n        \n    plt.plot(range(1, epochs + 1), rmse_train_list, c = 'blue', label = 'Train Set')\n    plt.plot(range(1, epochs + 1), rmse_val_list, c = 'red', label = 'Test Set')\n    plt.legend()\n    plt.xlabel('Number of iterations')\n    plt.ylabel('RMSE of Test and Train sets')\n    plt.title('Linear Regression with Learning rate ' + str(rate))","8b07eda7":"from sklearn.datasets.samples_generator import make_regression\nX, y = make_regression(n_samples = 500, n_features = 10, n_informative = 5, noise = 20, bias  = 100, random_state = 1)\n\nf = X.shape[1]\n\nfig = plt.figure(figsize = (15, 15))\n\nfor i in range(1, f + 1):\n    ax = fig.add_subplot(5, 5, i)\n    ax.scatter(X[:, (i - 1)], y)","e240704e":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.33, random_state = 5)","5a6dea7d":"linear_regression(X_train, y_train, X_val, y_val, 0.01, 150)","52116ad1":"def linear(X_train, y_train, X_val, y_val, rate, epochs):\n\n    y_train = np.array([y_train])\n    y_val = np.array([y_val])\n    X_train = X_train.T\n    X_val = X_val.T\n    m_train = y_train.shape[1]\n    m_val = y_val.shape[1]\n    \n    X_train = (X_train - np.min(X_train)) \/ (np.max(X_train) - np.min(X_train))\n    X_val = (X_val - np.min(X_val)) \/ (np.max(X_val) - np.min(X_val))\n    \n    w, b = initialize_parameters(X_train)\n\n    for i in range(1 , epochs + 1):\n        z_train = forward_prop(w, b, X_train)\n        cost_train = cost_function(z_train, y_train)\n        dw, db = back_prop(X_train, y_train, z_train)\n        w, b = gradient_descent(w, b, dw, db, rate)\n\n    z_train = forward_prop(w, b, X_train)\n    cost_train = cost_function(z_train, y_train)\n    rmse_train = (np.sum((z_train - y_train) ** 2) \/ m_train) ** 0.5\n\n    z_val = forward_prop(w, b, X_val)\n    cost_val = cost_function(z_val, y_val)\n    rmse_val = (np.sum((z_val - y_val) ** 2) \/ m_val) ** 0.5\n\n    print(X_val, y_val, z_val)\n    print('\/n')\n    print('The linear regression with learning rate ', str(rate), ' and ', str(epochs), ' epochs, produces predictions with train set RMSE as ', str(rmse_train), ' and test set RMSE as ', str(rmse_test))","92bf2744":"# Step 2 - Predict the target with the given parameters","b9b8bb2f":"# Step 5 - Reinitialise parameters taking the gradient into account","4aa010ac":"# Conjure a random sample with samples generator","9903c2d0":"# Step 3 - Calculate the disparity between actual and predicted results","a480772a":"# Step 4 - Calculate the gradient of error","595f0afb":"# Apply linear regression and visualise the RMSE for test and train sets","464cc8e7":"# Step 1 - Initialize parameters randomly","5556e3bd":"# For application purpose","45be4afa":"# Step 6 - Define the linear regression algorithm with a learning rate and number of iterations (epochs)","13ce8810":"# Perform a train-test split "}}