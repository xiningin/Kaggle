{"cell_type":{"f59b29b9":"code","3702973f":"code","2d97ba48":"code","b25e3193":"code","dbaf74dd":"code","a3d45a9a":"code","a04079e4":"code","f1893e1f":"code","39416809":"code","6dbb7cb2":"code","e852950d":"code","947e8721":"code","8d115a82":"code","3d011dc6":"code","fdb02298":"code","2dc52f7e":"code","2d6744e0":"code","a31c783c":"code","c93dcb3b":"code","b57b947f":"code","faa2a766":"code","8d73b82c":"code","d2cce2b7":"code","d6eedc03":"code","c5398da6":"code","35495b5d":"code","8dfaf502":"code","868ddc15":"code","9f06dce5":"code","83683fbc":"code","8c685dc3":"code","20316b3b":"code","76f5855b":"code","d5f03c5b":"code","ce791108":"code","783396e8":"code","4a2a4ea0":"code","b681b60b":"markdown","a802cae7":"markdown","e57fd738":"markdown","96be87f5":"markdown","1b33e110":"markdown","8713719b":"markdown","7835e3db":"markdown","6b3ef6ce":"markdown","32b5d4bf":"markdown","d51efec4":"markdown","709d3dc4":"markdown","9d686fc9":"markdown","8232df3e":"markdown","e9ca4d00":"markdown"},"source":{"f59b29b9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style = 'whitegrid')\nsns.distributions._has_statsmodels = False # To handle RuntimeError: Selected KDE bandwidth is 0.","3702973f":"data_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndata_test = pd.read_csv('..\/input\/titanic\/test.csv')","2d97ba48":"train = data_train.copy()\ntest = data_test.copy()","b25e3193":"train.head()","dbaf74dd":"test.head()","a3d45a9a":"print(train.info())\nprint('\\n')\nprint(test.info())","a04079e4":"# get letter on column 'cabin' value to categorize column 'cabin'\ntrain['Cabin'] = train['Cabin'].str.get(0)\ntest['Cabin'] = test['Cabin'].str.get(0)","f1893e1f":"# split into numeric and categorical data for analysis purpose\nnum_data = train[['Age', 'SibSp', 'Parch', 'Fare']]\ncat_data = train[['Survived', 'Pclass', 'Sex', 'Cabin', 'Embarked']]","39416809":"import statsmodels\n\nfig, ax = plt.subplots(2, 2 ,figsize = (12,8))\nfig.tight_layout(pad=5.0)\n# can use for loop, if to much columns\nsns.distplot(ax = ax[0, 0], a = num_data['Age'].dropna())\nax[0, 0].set_title('Age', fontsize = 18)\n\nsns.distplot(ax = ax[0, 1], a = num_data['SibSp'].dropna())\nax[0, 1].set_title('SibSp', fontsize = 18)\n\nsns.distplot(ax = ax[1, 0], a = num_data['Parch'].dropna())\nax[1, 0].set_title('Parch', fontsize = 18)\n\nsns.distplot(ax = ax[1, 1], a = num_data['Fare'].dropna())\nax[1, 1].set_title('Fare', fontsize = 18)\n\nplt.show()","6dbb7cb2":"# heatmap data numeric\nheatmapdata = train[['Survived', 'Age', 'SibSp', 'Parch', 'Fare']]\n\ncormat = heatmapdata.corr()\nfig, ax = plt.subplots(figsize = (8,4))\nsns.heatmap(data = cormat)\nplt.show()","e852950d":"fig, ax = plt.subplots(cat_data.shape[1], 1, figsize = (8,16))\nfig.tight_layout(pad=5.0)\n\nfor i, n in enumerate(cat_data):\n        sns.barplot(ax = ax[i], x = cat_data[n].fillna('NaN').value_counts().index, y = cat_data[n].fillna('NaN').value_counts())\n        ax[i].set_title(n)\nplt.show()","947e8721":"# create columns survived so that same shape with training data\ntest.insert(1, 'Survived', -1)\ntest.info()","8d115a82":"print('Train :\\n',train.isnull().sum())\nprint('\\n')\nprint('Test :\\n', test.isnull().sum())","3d011dc6":"# handle missing data on column age (do the same on data test, but with median of data train)\ntrain['Age'].fillna(train['Age'].median(), inplace = True)\ntest['Age'].fillna(train['Age'].median(), inplace = True)\n\n# we know test data have nan values on fare (do the same with train data, for better understanding)\ntrain['Fare'].fillna(train['Fare'].median(), inplace = True)\ntest['Fare'].fillna(train['Fare'].median(), inplace = True)\n\n# handle missing data on embarked columns\ntrain.dropna(subset=['Embarked'] , inplace = True)","fdb02298":"# Drop cabin because that's have many null\/nan values\ntrain.drop(['Cabin'], axis = 1, inplace = True)\ntest.drop(['Cabin'], axis = 1, inplace = True)","2dc52f7e":"print('Train :\\n',train.isnull().sum())\nprint('\\n')\nprint('Test :\\n', test.isnull().sum())","2d6744e0":"# Create column family survived & died from column 'Name' (LastName)\ntrain['LastName'] = train['Name'].str.split(',', expand=True)[0]\ntest['LastName'] = test['Name'].str.split(',', expand=True)[0]","a31c783c":"train.head()","c93dcb3b":"train['Train'] = 1\ntest['Train'] = 0\n\nalldata = pd.concat((train, test), sort = False).reset_index(drop = True)\n\n# From Ken Jee (https:\/\/www.youtube.com\/watch?v=I3FBJdiExcg&t=1477s)\nsur_data = []\ndied_data = []\nfor index, row in alldata.iterrows():\n    s = alldata[(alldata['LastName']==row['LastName']) & (alldata['Survived']==1)]\n    d = alldata[(alldata['LastName']==row['LastName']) & (alldata['Survived']==0)]\n    \n    s=len(s)\n    if row['Survived'] == 1:\n        s-=1\n\n    d=len(d)\n    if row['Survived'] == 0:\n        d-=1\n        \n    sur_data.append(s)\n    died_data.append(d)\n    \nalldata['FamilySurvived'] = sur_data\nalldata['FamilyDied'] = died_data","b57b947f":"train = alldata[alldata['Train'] == 1]\ntest = alldata[alldata['Train'] == 0]\n","faa2a766":"# Remove outlier from data train\n# https:\/\/towardsdatascience.com\/ways-to-detect-and-remove-the-outliers-404d16608dba\n\nq1 = train['Age'].quantile(0.25)\nq3 = train['Age'].quantile(0.75)\niqr = q3-q1\ntrain = train[~((train['Age'] < (q1 - 1.5 * iqr)) | (train['Age'] > (q3+1.5*iqr)))]\n\nq1=train['Fare'].quantile(0.25)\nq3 = train['Fare'].quantile(0.75)\niqr = q3-q1\ntrain = train[~ ((train['Fare'] < q1 - 1.5 * iqr) | (train['Fare'] > (q3 + 1.5 * iqr)))]","8d73b82c":"# Do log transform for column fare to make data more close into normal distribution\ntrain['Fare'] = np.log1p(train['Fare']) # the same as np.log(train['Fare'] + 1)\ntest['Fare'] = np.log1p(test['Fare']) # the same as np.log(test['Fare'] + 1)","d2cce2b7":"import seaborn as sns\nfig, ax = plt.subplots(1, 2 ,figsize = (16,4))\nsns.distplot(ax = ax[0], a = train['Age'])\nsns.distplot(ax = ax[1], a = train['Fare'])\nplt.show()","d6eedc03":"train.head()","c5398da6":"test.head()","35495b5d":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train['Pclass'])\ntrain['Pclass'] = le.transform(train['Pclass'])\nfrom sklearn.preprocessing import OneHotEncoder\n# By dropping one of the one-hot encoded columns from each categorical feature, we ensure there are no \"reference\" columns\u2014the remaining columns become linearly independent.\n# https:\/\/kiwidamien.github.io\/are-you-getting-burned-by-one-hot-encoding.html\n# https:\/\/www.youtube.com\/watch?v=g9aLvY8BfRM\nohe = OneHotEncoder(sparse = False, drop = 'first', categories = 'auto')\nohe.fit(train[['Sex', 'Embarked']])\nohecategory_train = ohe.transform(train[['Sex', 'Embarked']])\nohecategory_test = ohe.transform(test[['Sex', 'Embarked']])\n\nfor i in range(ohecategory_train.shape[1]):\n    train['dummy_variable_' + str(i)] = ohecategory_train[:,i]\n\nfor i in range(ohecategory_test.shape[1]):\n    test['dummy_variable_' + str(i)] = ohecategory_test[:,i]\n\n\nprint('Train shape :', train.shape)\nprint('Test shape :', test.shape)","8dfaf502":"# https:\/\/benalexkeen.com\/feature-scaling-with-scikit-learn\/\n# https:\/\/stats.stackexchange.com\/questions\/463690\/multiple-regression-with-mixed-continuous-categorical-variables-dummy-coding-s\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(train[['Age', 'SibSp', 'Parch', 'Fare']])\ntrain[['Age', 'SibSp', 'Parch', 'Fare']] = sc.transform(train[['Age', 'SibSp', 'Parch', 'Fare']])\ntest[['Age', 'SibSp', 'Parch', 'Fare']] = sc.transform(test[['Age', 'SibSp', 'Parch', 'Fare']])\n","868ddc15":"train.head()","9f06dce5":"test.head()","83683fbc":"# See if train and test data have same shape and column position\nprint('Train columns :\\n',train.columns)\nprint('Train shape : ', train.shape)\nprint('\\n')\nprint('Test columns :\\n',test.columns)\nprint('Test shape : ', test.shape)","8c685dc3":"# See & explore the data for dropping unused columns\/features\ntrain.head()","20316b3b":"# Drop columns 'Sex' and 'Embarked' because we haved one hot encode them\ntrain.drop(['PassengerId', 'Name', 'Sex', 'Ticket', 'Embarked', 'LastName', 'Train'], axis = 1, inplace = True)\ntest.drop(['PassengerId', 'Name', 'Sex', 'Ticket', 'Embarked', 'LastName', 'Train'], axis = 1, inplace = True)","76f5855b":"print('Train columns :\\n',train.columns)\nprint('Train shape : ', train.shape)\nprint('\\n')\nprint('Test columns :\\n',test.columns)\nprint('Test shape : ', test.shape)","d5f03c5b":"X_train = train.iloc[:, 1:].values\ny_train = train.iloc[:, 0].values\n\nX_test = test.iloc[:, 1:].values\ny_test = test.iloc[:, 0].values\n\nprint('X_train :\\n', X_train[0:5])\nprint('y_train :\\n', y_train[0:5])","ce791108":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\n\nclf = KNeighborsClassifier(leaf_size = 1, metric = 'minkowski', n_neighbors = 12, p = 1, weights = 'distance')\naccuracies = cross_val_score(clf, X_train, y_train, cv = 10)\nprint('Accuracies : ', accuracies)\nprint('AVG Accuracies : ', accuracies.mean())\nprint('STD:',accuracies.std())\n","783396e8":"clf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\ny_pred = y_pred.astype('int64')\n\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = data_test['PassengerId']\nsubmission['Survived'] = y_pred\nsubmission['Survived'].value_counts()\n","4a2a4ea0":"submission.to_csv(r'Submission.csv', index = False, header = True)","b681b60b":"In this section, U had done some experiment with several machine learning algorithms like Naive Bayes, Logistic Regression, XGBoost, K-Nearest Neighbors, etc. I used GridSearchCV to find best parameter and accuracy from each algorithms, after that I implemented on data test and submitted the results prediction to kaggle. The best score I have is <b>0.79425 (top 7%)<\/b> with K-Nearest Neighbors algorithm (parameter : leaf_size = 1, metric = 'minkowski', n_neighbors = 12, p = 1, weights = 'distance')","a802cae7":"## Cleaning Data on Column 'Age' and 'Fare'","e57fd738":"# Exploratory Data Analysis","96be87f5":"## Create Column 'FamilySurvived' & 'FamilyDied'","1b33e110":"## Standardization Numerical Data","8713719b":"# Model Prediction","7835e3db":"### Analysis Data Category","6b3ef6ce":"## Handle missing data","32b5d4bf":"## Encode Categorical Data","d51efec4":"## Analysis Data Numeric and Category","709d3dc4":"# Feature Selection","9d686fc9":"### K-Nearest Neighbors","8232df3e":"# Preprocessing Data (handle missing data, cleaning data, feature engineering, etc.) on Each Columns","e9ca4d00":"### Analysis Data Numeric"}}