{"cell_type":{"0191e03d":"code","8223dbc4":"code","e9cf8315":"code","5407ffaa":"code","e476e92b":"code","9571a9e8":"code","5036230f":"code","c510c896":"code","8389b4f5":"code","6b8bf1f5":"code","3f0e79d6":"code","d9ddd597":"code","170690a1":"markdown","b169ffd8":"markdown","500c54fe":"markdown"},"source":{"0191e03d":"import os\nimport sys\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFilter\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, mean_squared_error, log_loss, confusion_matrix ,roc_auc_score,roc_curve\nfrom sklearn.model_selection import StratifiedKFold,train_test_split\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport cv2\n\nnp.random.seed(100)\nLEVEL = 'level_3'","8223dbc4":"class SigmoidNeuron:\n\n    def __init__(self):\n        self.w = None\n        self.b = None\n\n    def perceptron(self, x):\n        return np.dot(x, self.w.T) + self.b\n\n    def sigmoid(self, x):\n        return 1.0 \/ (1.0 + np.exp(-x))\n\n    def grad_w_mse(self, x, y):\n        y_pred = self.sigmoid(self.perceptron(x))\n        return (y_pred - y) * y_pred * (1 - y_pred) * x\n\n    def grad_b_mse(self, x, y):\n        y_pred = self.sigmoid(self.perceptron(x))\n        return (y_pred - y) * y_pred * (1 - y_pred)\n\n    def grad_w_ce(self, x, y):\n        y_pred = self.sigmoid(self.perceptron(x))\n        if y == 0:\n            return y_pred * x\n        elif y == 1:\n            return -1 * (1 - y_pred) * x\n        else:\n            raise ValueError(\"y should be 0 or 1\")\n\n    def grad_b_ce(self, x, y):\n        y_pred = self.sigmoid(self.perceptron(x))\n        if y == 0:\n            return y_pred\n        elif y == 1:\n            return -1 * (1 - y_pred)\n        else:\n            raise ValueError(\"y should be 0 or 1\")\n\n    def fit(self, X, Y, epochs=1, learning_rate=1, initialise=True, loss_fn=\"mse\", display_loss=False):\n\n        # initialise w, b\n        if initialise:\n            self.w = np.zeros(X.shape[1])\n            self.b = 0\n\n        if display_loss:\n            loss = {}\n\n        for i in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n            dw = 0\n            db = 0\n            for x, y in zip(X, Y):\n                if loss_fn == \"mse\":\n                    dw += self.grad_w_mse(x, y)\n                    db += self.grad_b_mse(x, y)\n                elif loss_fn == \"ce\":\n                    dw += self.grad_w_ce(x, y)\n                    db += self.grad_b_ce(x, y)\n            self.w -= learning_rate * dw\n            self.b -= learning_rate * db\n\n            if display_loss:\n                Y_pred = self.sigmoid(self.perceptron(X))\n                if loss_fn == \"mse\":\n                    loss[i] = mean_squared_error(Y, Y_pred)\n                elif loss_fn == \"ce\":\n                    loss[i] = log_loss(Y, Y_pred)\n\n        if display_loss:\n            plt.plot(loss.values())\n            plt.xlabel('Epochs')\n            if loss_fn == \"mse\":\n                plt.ylabel('Mean Squared Error')\n            elif loss_fn == \"ce\":\n                plt.ylabel('Log Loss')\n            key_min = min(loss.keys(), key=(lambda k: loss[k]))\n            print('epochs : ' , key_min)\n            print('Minimum Loss : ',loss[key_min])\n            plt.show()\n\n    def predict(self, X):\n        Y_pred = []\n        for x in X:\n            y_pred = self.sigmoid(self.perceptron(x))\n            Y_pred.append(y_pred)\n        return np.array(Y_pred)","e9cf8315":"def read_all(folder_path, key_prefix=\"\"):\n    '''\n    It returns a dictionary with 'file names' as keys and 'flattened image arrays' as values.\n    '''\n    print(\"Reading:\")\n    images = {}\n    files = os.listdir(folder_path)\n    for i, file_name in tqdm_notebook(enumerate(files), total=len(files)):\n        file_path = os.path.join(folder_path, file_name)\n        image_index = key_prefix + file_name[:-4]\n        image = cv2.imread(file_path,1) \n        gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY) # convert color image to gray\n        ret,th = cv2.threshold(gray,13,255,cv2.THRESH_BINARY) # convert pixels having value more then 13 to 255(White) \n        median = cv2.medianBlur(th, 3) # Remove salt and pepper noise\n        images[image_index] = np.array(median.copy()).flatten()\n    return images","5407ffaa":"languages = ['ta', 'hi', 'en']\n\nimages_train = read_all(\"..\/input\/level_3_train\/\"+LEVEL+\"\/\"+\"background\/\", key_prefix='bgr_') # change the path\nfor language in languages:\n    images_train.update(read_all(\"..\/input\/level_3_train\/\"+LEVEL+\"\/\"+language, key_prefix=language+\"_\" ))\nprint(len(images_train))\n\nimages_test = read_all(\"..\/input\/level_3_test\/kaggle_\"+LEVEL, key_prefix='') # change the path\nprint(len(images_test))","e476e92b":"list(images_test.keys())[:5]","9571a9e8":"X = []\ny = []\nfor key, value in images_train.items():\n    X.append(value)\n    if key[:4] == \"bgr_\":\n        y.append(0)\n    else:\n        y.append(1)\n\nID_test = []\nX_test = []\nfor key, value in images_test.items():\n    ID_test.append(int(key))\n    X_test.append(value)\n\nX = np.array(X)\ny = np.array(y)\nX_test = np.array(X_test)\n\nprint(X.shape, y.shape)\nprint(X_test.shape)","5036230f":"scaler = StandardScaler()\nX_scaled_train = scaler.fit_transform(X)\nX_scaled_test = scaler.transform(X_test)","c510c896":"kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","8389b4f5":"for train_index, test_index in kfold.split(X, y):\n    X_kf_train, X_kf_test = X[train_index], X[test_index]\n    y_kf_train, y_kf_test = y[train_index], y[test_index]\n    X_kf_scaled_train = scaler.fit_transform(X_kf_train)\n    X_kf_scaled_test = scaler.transform(X_kf_test)\n    sn_ce = SigmoidNeuron()\n    sn_ce.fit(X_kf_scaled_train, y_kf_train, epochs=2000, learning_rate=0.00001, loss_fn=\"ce\", display_loss=True)\n    Y_pred_train = sn_ce.predict(X_kf_scaled_train)\n    dic = {}\n    thresholds  =  np.linspace(0,1,10000)\n    # For each threshold calculate the accuracy score and store in dic\n    for i in range(len(thresholds)):\n        x = (Y_pred_train >= thresholds[i]).astype(\"int\").ravel() \n        dic[i] = accuracy_score(y_kf_train,x)\n    key_max = max(dic.keys(), key=(lambda k: dic[k]))\n    print('Max Accuracy Score for train : ',dic[key_max])\n    print('Threshold probability : ', thresholds[key_max])\n    Y_pred_train = (Y_pred_train >= thresholds[key_max]).astype(\"int\").ravel()\n    tn, fp, fn, tp = confusion_matrix(y_kf_train,Y_pred_train).ravel()\n    print(\"Confusion matrix for train set\")\n    print(\"tn, fp, fn, tp : \",tn, fp, fn, tp)\n    Y_pred_test = sn_ce.predict(X_kf_scaled_test)\n    Y_pred_binarised_test = (Y_pred_test >= thresholds[key_max]).astype(\"int\").ravel()\n    print('Accuracy Score for test : ',accuracy_score(y_kf_test,Y_pred_binarised_test))","6b8bf1f5":"sn_ce = SigmoidNeuron()\nsn_ce.fit(X_scaled_train, y, epochs=2000, learning_rate= 0.00001, loss_fn=\"ce\", display_loss=True)","3f0e79d6":"Y_pred_train = sn_ce.predict(X_scaled_train)\nthresholds  =  np.linspace(0,1,10000)\ndic = {}\nfor i in range(len(thresholds)):\n    x = (Y_pred_train >= thresholds[i]).astype(\"int\").ravel()\n    dic[i] = accuracy_score(y,x)\nkey_max = max(dic.keys(), key=(lambda k: dic[k]))\nprint('Max Accuracy Score for train : ',dic[key_max])\nprint('Threshold probability : ', thresholds[key_max])\nY_pred_train = (Y_pred_train >= thresholds[key_max]).astype(\"int\").ravel()\ntn, fp, fn, tp = confusion_matrix(y,Y_pred_train).ravel()\nprint(\"Confusion matrix for train set\")\nprint(\"tn, fp, fn, tp :\", tn, fp, fn, tp)","d9ddd597":"Y_pred_test = sn_ce.predict(X_scaled_test)\nY_pred_binarised_test = (Y_pred_test >= thresholds[key_max]).astype(\"int\").ravel()\n\nsubmission = {}\nsubmission['ImageId'] = ID_test\nsubmission['Class'] = Y_pred_binarised_test\n\nsubmission = pd.DataFrame(submission)\nsubmission = submission[['ImageId', 'Class']]\nsubmission = submission.sort_values(['ImageId'])\nsubmission.to_csv(\"submisision.csv\", index=False)","170690a1":"**Let's split our data into folds, we want to make sure that each fold is a good representative of the whole data.\nHere we have used 5 folds to split the data and for each fold we'll train and test the model to overcome overfitting.**","b169ffd8":"## Submission","500c54fe":"**For each fold we are getting the good accuracy for train and test, Let's do not split the data and train the model for whole dataset.**"}}