{"cell_type":{"4eb9645e":"code","2285e23c":"code","d463c4f8":"code","f5e4d772":"code","8c3840c4":"code","5db911f5":"code","f392f04d":"code","dece74c7":"code","d5d1825a":"code","ab8bff6b":"code","e8b384da":"code","98d76d0e":"code","4da0ad01":"code","bdc27ad1":"code","74dba785":"code","5bc90df2":"code","36dbbe38":"code","f1672ef5":"code","758516d3":"code","7809d1e8":"code","c6de1d4c":"code","2ddccf84":"code","77c3df22":"code","2b110443":"code","a1609629":"code","8f1616a5":"code","a99c6535":"code","59bcff45":"code","894f4ec2":"code","2a53f068":"code","818b8b77":"code","0374785f":"code","6b6e85b2":"code","014bf15c":"code","f1bcaaed":"code","cba7e839":"code","9e9aca32":"code","6aed2a1d":"code","c86a9624":"code","75576481":"code","7645201d":"code","b4cc893f":"code","de4f8565":"markdown","a4d702c8":"markdown","5ee78461":"markdown","c4c1a323":"markdown","3c59d693":"markdown","9efdad43":"markdown","4cdcfaaf":"markdown","0a6e42cc":"markdown","2591a65f":"markdown","66f378b8":"markdown","f62828c6":"markdown","3b0d2dcf":"markdown","49faa9d2":"markdown","a1d8e81c":"markdown","8180c407":"markdown","dc0d11c2":"markdown","1f1ac9ed":"markdown","2ffe17e1":"markdown","8bb64172":"markdown","556045fe":"markdown","1b85001a":"markdown","d54c8bbb":"markdown","1d301686":"markdown","4104fe0b":"markdown","374711bd":"markdown","d849aab5":"markdown","3a2777a2":"markdown","2bb65bcf":"markdown","04d96cf1":"markdown"},"source":{"4eb9645e":"import numpy as np \nimport pandas as pd \nimport random\nimport torch\nimport os\n\nrandom.seed(97)\nnp.random.seed(97)\ntorch.manual_seed(97)\ntorch.cuda.manual_seed(97)\ntorch.backends.cudnn.deterministic = True","2285e23c":"# \u041f\u0443\u0442\u044c \u043a \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\nPATH_TO_TRAIN = '..\/input\/plates\/plates\/train'\n# \u041f\u0443\u0442\u044c \u043a \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\nPATH_TO_TEST = '..\/input\/plates\/plates\/test'\n# \u041f\u0443\u0442\u044c \u043a \u0432\u044b\u0431\u043e\u0440\u043a\u0435, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0431\u0443\u0434\u0435\u0442 \u043e\u0431\u0443\u0447\u0430\u0442\u044c\u0441\u044f \u043c\u043e\u0434\u0435\u043b\u044c\nTRAIN_DIR = 'train'\n# \u041f\u0443\u0442\u044c \u043a \u0432\u044b\u0431\u043e\u0440\u043a\u0435, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0431\u0443\u0434\u0435\u0442 \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0442\u044c\u0441\u044f \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438\nVAL_DIR = 'val'\n# \u041f\u0443\u0442\u044c \u043a \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\nTEST_DIR = 'test'","d463c4f8":"labels = ['cleaned', 'dirty']","f5e4d772":"# \u041a\u043e\u043b-\u0432\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432 \u0438\u0437 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u043f\u043e\u0441\u043b\u0435 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0434\u0435\u043b\u0430\u0435\u0442\u0441\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0448\u0430\u0433\nbatch_size = 8\n# \u0421\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438\nlr = 3e-4\n# \u041a\u043e\u043b-\u0432\u043e \u044d\u043f\u043e\u0445 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\nnum_epoch = 100\n\n# \u043a\u0430\u0436\u0434\u043e\u0435 valid_split \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0431\u0443\u0434\u0435\u0442 \u043e\u0442\u043b\u043e\u0436\u0435\u043d\u043e \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\nvalid_split = 6","8c3840c4":"# \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b lr_scheduler.StepLR\n# \u041a\u043e\u043b-\u0432\u043e \u044d\u043f\u043e\u0445, \u043f\u043e\u0441\u043b\u0435 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0435\u0442\u0441\u044f lerning rate\nstep_size = 1000 # \u0434\u043b\u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043c \u043a\u043e\u043b-\u0432\u043e \u0448\u0430\u0433\u043e\u0432 \u0431\u043e\u043b\u044c\u0448\u0435 \u0447\u0435\u043c \n                 # \u043a\u043e\u043b-\u0432\u043e \u044d\u043f\u043e\u0445, \u0447\u0442\u043e \u0431\u044b \u043d\u0435 \u0443\u0441\u043b\u043e\u0436\u043d\u044f\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c\ngamma=0.1","5db911f5":"from PIL import Image\nfrom random import sample\nfrom os.path import join\nfrom os import listdir\nimport matplotlib.pyplot as plt","f392f04d":"for label in labels:\n    print(label)\n    # \u041f\u0443\u0442\u044c \u043a \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u043c \u043a\u043b\u0430\u0441\u0441\u0430 label\n    PATH = join(PATH_TO_TRAIN, label)\n    # \u041a\u043e\u043b-\u0432\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0432 \u0434\u0430\u043d\u043d\u043e\u0439 \u043f\u0430\u043f\u043a\u0435\n    n = len(listdir(PATH))\n    plt.figure(figsize=(10, 10))\n    for i,image in enumerate(listdir(PATH)):\n        plt.subplot(n\/4, 4, i+1)\n        img = Image.open(join(PATH,image))\n        plt.imshow(img)\n        plt.title(label)\n        plt.xticks([])\n        plt.yticks([])\n    plt.show()","dece74c7":"n_samples = 20\n# \u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0435 20 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0439 \u0438\u0437 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\nsub_sample = sample(listdir(PATH_TO_TEST), n_samples)\nplt.figure(figsize=(10, 10))\nprint('Test set')\nfor i,image in enumerate(sub_sample):\n    plt.subplot(n_samples\/4, 4, i+1)\n    img = Image.open(join(PATH_TO_TEST, image))\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\nplt.show()","d5d1825a":"from shutil import copy\nfrom os import makedirs","ab8bff6b":"for dir_name in [TRAIN_DIR, VAL_DIR]:\n    for label in labels:\n        makedirs(join(dir_name, label),\n                exist_ok=True)","e8b384da":"for label in labels:\n    \n    # \u041f\u0430\u043f\u043a\u0430 \u0438\u0437 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0431\u0443\u0434\u0435\u043c \u043a\u043e\u043f\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0444\u0430\u0439\u043b\u044b \n    source_dir = join(PATH_TO_TRAIN, label)\n    \n    for i, file_name in enumerate(listdir(source_dir)):\n        \n        # \u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u043a\u0443\u0434\u0430 \u0441\u043a\u043e\u043f\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0444\u0430\u0439\u043b\n        if i % 6 != 0:\n            dest_dir = join(TRAIN_DIR, label)\n        else:\n            dest_dir = join(VAL_DIR, label)\n            \n        # \u041a\u043e\u043f\u0438\u0440\u0443\u0435\u043c \u0444\u0430\u0439\u043b \u0432 \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u0443\u044e \u043f\u0430\u043f\u043a\u0443\n        copy(join(source_dir, file_name),\n            (join(dest_dir, file_name)))","98d76d0e":"from torchvision import transforms ","4da0ad01":"train_transforms = transforms.Compose([\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], \n                         [0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], \n                          [0.229, 0.224, 0.225])\n])","bdc27ad1":"from torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader","74dba785":"# \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u0442 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0438\u0437 \u0438\u0437\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u043b\u0435\u0436\u0430\u0449\u0438\u0445 \u0432 \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u043e\u0439 \u043f\u0430\u043f\u043a\u0435\ntrain_dataset = ImageFolder(TRAIN_DIR, train_transforms)\nval_dataset = ImageFolder(VAL_DIR, train_transforms)","5bc90df2":"# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u0442 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u0441\u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0440\u0430\u043d\u0435\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u043e\u0432\ntrain_dataloader = DataLoader(train_dataset,\n                              batch_size=batch_size,\n                              shuffle=True,\n                              num_workers=batch_size)\n\nval_dataloader = DataLoader(val_dataset,\n                            batch_size=batch_size,\n                            shuffle=False,\n                            num_workers=batch_size)","36dbbe38":"def show_batch(X_batch, y_batch, batch_size):\n    plt.figure(figsize=(10, 5))\n    for i, (image_tensor, class_index) in enumerate(zip(X_batch, y_batch)):\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = image_tensor.permute(1, 2, 0).numpy()\n        image = std * image + mean\n        plt.subplot(batch_size\/4, 4, i+1)\n        plt.imshow(image)\n        plt.title(labels[class_index])\n        plt.xticks([])\n        plt.yticks([])      \n    plt.show()","f1672ef5":"for i in range(3):\n    print('Batch', i+1)\n    X_batch, y_batch = next(iter(train_dataloader))\n    show_batch(X_batch, y_batch, batch_size)","758516d3":"from torchvision import models\nfrom torch.nn import Linear, CrossEntropyLoss\nfrom torch.optim import Adam, lr_scheduler","7809d1e8":"# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 ResNet18\nmodel = models.resnet18(pretrained=True)\n\n# \"\u0417\u0430\u043c\u043e\u0440\u043e\u0437\u043a\u0430\" \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u044b\u0445 \u0441\u043b\u043e\u0435\u0432 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438\n# (\u0444\u0438\u043a\u0441\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432)\nfor param in model.parameters():\n    param.requires_grad = False\n\n# \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u043e\u0433\u043e \u0441\u043b\u043e\u044f\n# \u0434\u043b\u044f \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438\nmodel.fc = Linear(model.fc.in_features, 2)\n\n# \u041f\u0435\u0440\u0435\u043a\u043b\u0430\u0434\u044b\u0432\u0430\u0435\u043c \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u043d\u0430 GPU\n# \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u0435\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u044f \u043d\u0430 CPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","c6de1d4c":"# \u0417\u0430\u0434\u0430\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043f\u043e\u0442\u0435\u0440\u044c\nloss = CrossEntropyLoss()","2ddccf84":"# \u0417\u0430\u0434\u0430\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\noptimizer = Adam(model.parameters(),\n                 lr=lr)","77c3df22":"scheduler = lr_scheduler.StepLR(optimizer,\n                                step_size=step_size,\n                                gamma=gamma)","2b110443":"def train_model(model,\n                loss,\n                optimizer,\n                scheduler,\n                num_epochs,\n                verbose = False):\n    \n    # \u0411\u0443\u0434\u0435\u043c \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0442\u044c \u043f\u0440\u043e\u043c\u0435\u0436\u0443\u0442\u043e\u0447\u043d\u044b\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b\n    # \u0434\u043b\u044f \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0439 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n    train_accuracy_history = []\n    train_loss_history = []\n    val_accuracy_history = []\n    val_loss_history = []\n    \n    for epoch in range(num_epochs):\n        if verbose == True:\n            print()\n            print('Epoch {}\/{}:'.format(epoch, num_epochs - 1), flush=True)\n        # \u041d\u0430 \u043a\u0430\u0436\u0434\u043e\u0439 \u044d\u043f\u043e\u0445\u0435 \u0431\u0443\u0434\u0435\u043c \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u0441\u0435\u0442\u044c\n        # \u0437\u0430\u0442\u0435\u043c \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0442\u044c \u0435\u0435 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                dataloader = train_dataloader\n                scheduler.step()\n                model.train()\n            else:\n                dataloader = val_dataloader\n                model.eval()\n        \n            running_loss = 0\n            running_acc = 0\n            \n            # \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\n            # \u0438 \u043e\u0431\u043d\u0443\u043b\u044f\u0435\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\n            for inputs, labels in dataloader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                optimizer.zero_grad()\n                \n                # \u041d\u0430 \u0444\u0430\u0437\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0434\u0435\u043b\u0430\u0435\u043c \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u043e\u0448\u0438\u0431\u043a\u0443 \n                # \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0438 \u0434\u0435\u043b\u0430\u0435\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0448\u0430\u0433\n                with torch.set_grad_enabled(phase == 'train'):\n                    preds = model(inputs)\n                    loss_value = loss(preds, labels)\n                    preds_class = preds.argmax(dim=1)\n                    if phase == 'train':\n                        loss_value.backward()\n                        optimizer.step()\n                running_loss += loss_value.data.cpu()\n                running_acc += (preds_class == labels.data).float().mean().data.cpu()\n            \n            # \u0421\u0440\u0435\u0434\u043d\u044f \u043e\u0448\u0438\u0431\u043a\u0430 \u0438 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430 \u044d\u043f\u043e\u0445\u0435\n            epoch_loss = running_loss \/ len(dataloader)\n            epoch_acc = running_acc \/ len(dataloader)\n        \n            # \u0417\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043e\u0448\u0438\u0431\u043a\u0438 \u0438 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \n            # \u0434\u043b\u044f \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0439 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438\n            if phase == 'train':\n                train_accuracy_history.append(epoch_acc)\n                train_loss_history.append(epoch_loss)\n            else:\n                val_accuracy_history.append(epoch_acc)\n                val_loss_history.append(epoch_loss)\n            \n            if verbose == True:\n                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), flush=True)\n            \n    return train_accuracy_history, train_loss_history, val_accuracy_history, val_loss_history","a1609629":"history = train_model(model,\n                      loss,\n                      optimizer,\n                      scheduler,\n                      num_epochs=num_epoch);","8f1616a5":"from torch import nn","a99c6535":"# \u0420\u0438\u0441\u0443\u0435\u043c \u0433\u0440\u0430\u0444\u0438\u043a\u0438  Accuracy \u0438 Loss \u0434\u043b\u044f \u0422\u0440\u0435\u043d\u0435\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\ntitles = ['Train Accuracy', 'Train Loss', 'Val Accuracy', 'Val Loss']\nplt.figure(figsize=(10, 10))\nfor i, image in enumerate(history):\n    plt.subplot(2, 2, i+1)\n    plt.title(titles[i])\n    plt.grid()\n    if i == 0 or i == 1:\n        color = 'b'\n    else:\n        color = 'orange'\n    plt.plot(image,c=color, label= 'mean: %.2f' % np.array(image).mean())\n    plt.legend(loc='best')\nplt.show()\n\n# \u0420\u0438\u0441\u0443\u0435\u043c \u0441\u043e\u0432\u043c\u0435\u0449\u0435\u043d\u043d\u044b\u0435 \u0433\u0440\u0430\u0444\u0438\u043a\u0438  Accuracy \u0438 Loss \u0434\u043b\u044f \u0422\u0440\u0435\u043d\u0435\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\ntitles_ = ['Accuracy','Loss']\nplt.figure(figsize=(10, 5))\nk=0\nfor i in range(2):\n    plt.subplot(1, 2, i+1)\n    plt.plot(history[0+k], label='Tain', c='b')\n    plt.plot(history[2+k], label='Val', c='orange')\n    plt.title(titles_[i])\n    plt.grid()\n    plt.legend(loc='best')\n    k += 1","59bcff45":"model.eval()\n\nfor _ in range(3):\n    val_predictions = []\n    val_img_paths = []\n    for inputs, labels_ in val_dataloader:\n        inputs = inputs.to(device)\n        labels_ = labels_.to(device)\n        with torch.set_grad_enabled(False):\n            preds = model(inputs)\n        val_predictions.append(nn.functional.softmax(preds, dim=1)[:,1].data.cpu())\n        labels_ = labels_.cpu().numpy()\n        predict = val_predictions[0] > 0.5\n        val_predictions = val_predictions[0].numpy()\n        predict = predict.numpy()\n        ind = (labels_ == predict)\n        for i, image_tensor in enumerate(inputs):\n            if ind[i] == False:\n                mean = np.array([0.485, 0.456, 0.406])\n                std = np.array([0.229, 0.224, 0.225])\n                image = image_tensor.cpu().permute(1, 2, 0).numpy()\n                image = std * image + mean\n                plt.imshow(image)\n                plt.title('label: %s \\n  %s: %.2f' % (labels[labels_[i]],\n                                                      labels[predict[i]],\n                                                      val_predictions[i]))\n                                                                   \n                plt.xticks([])\n                plt.yticks([]) \n                plt.show()\n        print()","894f4ec2":"from shutil import copytree","2a53f068":"# \u041a\u043e\u043f\u0438\u0440\u0443\u0435\u043c \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \n# \u0432 \u043f\u0430\u043f\u043a\u0443 'test\/unknown'\n# \u0434\u043b\u044f \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u044b ImageFolder\ncopytree(PATH_TO_TEST,\n         join(TEST_DIR, 'unknown'))","818b8b77":"from torchvision import transforms, models, datasets","0374785f":"# \u041c\u043e\u0434\u0438\u0444\u0438\u0446\u0438\u0440\u0443\u0435\u043c ImageFolder \u0442\u0430\u043a, \u0447\u0442\u043e \u0431\u044b \u043e\u043d \n# \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u043b \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0441 \u0435\u0433\u043e \u043c\u0435\u0442\u043a\u043e\u0439, \u043d\u043e \u0438\n# \u043f\u0443\u0442\u044c \u043a \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044e\nclass ImageFolderWithPaths(datasets.ImageFolder):\n    def __getitem__(self, index):\n        original_tuple = super(ImageFolderWithPaths,self).__getitem__(index)\n        path = self.imgs[index][0]\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path","6b6e85b2":"test_dataset = ImageFolderWithPaths(TEST_DIR, val_transforms)\ntest_dataloader = DataLoader(test_dataset,\n                             batch_size=batch_size,\n                             shuffle=False,\n                             num_workers=batch_size)","014bf15c":"# \u041f\u0435\u0440\u0435\u0432\u0435\u0434\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0432 evaluation \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\n# (\u043d\u0435 \u0431\u0443\u0434\u0443\u0442 \u0438\u0437\u043c\u0435\u043d\u044f\u0442\u0441\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0441\u0435\u0442\u0438)\nmodel.eval()\n\n# \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\ntest_predictions = []\n# \u041f\u0443\u0442\u0438 \u043a \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\ntest_img_paths = []","f1bcaaed":"from tqdm import tqdm\n# \u0412 \u0446\u0438\u043a\u043b\u0435 \u043f\u043e\u043b\u0443\u0441\u0430\u0435\u043c Batch \u0441 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\u0438, \u043c\u0435\u0442\u043a\u0443 \u043a\u043b\u0430\u0441\u0441\u0430 ('unknown') \u0438 \u043f\u0443\u0442\u0438 \u043a \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\nfor inputs, labels, paths in tqdm(test_dataloader):\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    with torch.set_grad_enabled(False):\n        # \u0421\u0447\u0438\u0442\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438\n        preds = model(inputs)\n    # \u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e softmax \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u043c \u0438\u0445 \u043a \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c\n    # \u0412 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 ('\u0433\u0440\u044f\u0437\u043d\u0430\u044f \u0442\u0430\u0440\u0435\u043b\u043a\u0430')\n    test_predictions.append(nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy())\n    # \u041f\u0443\u0442\u0438 \u043d\u0443\u0436\u043d\u044b \u0434\u043b\u044f \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0438 \u0441\u043e\u0437\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439\n    test_img_paths.extend(paths)\n    \ntest_predictions = np.concatenate(test_predictions)","cba7e839":" prediction = dict(zip(test_img_paths, test_predictions))","9e9aca32":"# \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430\nn_samples = 20\nsub_sample = sample(test_img_paths, n_samples)\nplt.figure(figsize=(10, 10))\nprint('Test set')\nfor i,image in enumerate(sub_sample):\n    plt.subplot(n_samples\/4, 4, i+1)\n    img = Image.open(image)\n    plt.imshow(img)\n    title = 'dirty' if prediction[image] > 0.5 else 'cleaned'\n    title = '%s:%.2f' % (title, prediction[image])\n    plt.title(title)\n    plt.xticks([])\n    plt.yticks([])\nplt.show()","6aed2a1d":"# \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c DataFrame \u0438\u0437 \u043f\u0443\u0442\u0435\u0439 \u043a \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c \u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c \u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u043c \u043a\u043b\u0430\u0441\u0441\u0430\u043c\nsubmission_df = pd.DataFrame.from_dict({'id': test_img_paths, \n                                        'label': test_predictions})","c86a9624":"submission_df.head()","75576481":"# \u041e\u0442\u0447\u0438\u0449\u0430\u0435\u043c id \u043e\u0442 \u043b\u0438\u0448\u0435\u0433\u043e, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u043e\u043c\u0435\u0440 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\n# \u0418 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043a \u043a\u043b\u0430\u0441\u0441\u0430\u043c\nsubmission_df['label'] = submission_df['label'].map(lambda pred: 'dirty' if pred > 0.5 else 'cleaned')\nsubmission_df['id'] = submission_df['id'].str.replace('test\/unknown\/', '')\nsubmission_df['id'] = submission_df['id'].str.replace('.jpg', '')\nsubmission_df.set_index('id', inplace=True)\nsubmission_df.head()","7645201d":"# \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0432 csv \u0444\u0430\u0439\u043b\nsubmission_df.to_csv('submission.csv')","b4cc893f":"!rm -rf train val test ","de4f8565":"\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f","a4d702c8":"## 1.6 \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \"\u0437\u0430\u0433\u0440\u0443\u0437\u0447\u0438\u043a\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445\" \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438","5ee78461":"## 1.5. \u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445","c4c1a323":"## 2.2. \u0414\u043e\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438","3c59d693":"## 2.1. \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0431\u0430\u0437\u043e\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438","9efdad43":"## 1.4 \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445. \u0420\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e \u0438  \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443","4cdcfaaf":"# 2. \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438","0a6e42cc":"## 1.7 \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438","2591a65f":"\u0414\u043b\u044f \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0437\u0430\u0444\u0438\u043a\u0441\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0432\u0441\u0435 \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\u044b \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0445 \u0447\u0438\u0441\u0435\u043b.","66f378b8":"\u0420\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445","f62828c6":"# \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f","3b0d2dcf":"\u0414\u0430\u043d\u043d\u044b\u0439 Kernel \u0441\u0442\u0430\u043b \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u043c \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043d\u0438\u044f <a href=\"https:\/\/www.kaggle.com\/yellowduck\/baseline-in-pytorch\">baseline<\/a> \u043e\u0442 <a href=\"https:\/\/www.kaggle.com\/yellowduck\">\u0418\u0433\u043e\u0440\u044f \u0421\u043b\u0438\u043d\u044c\u043a\u043e<\/a>, \u0438 \u043f\u043e\u043f\u044b\u0442\u043a\u0438 \u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u044c <a href=\"http:\/\/karpathy.github.io\/2019\/04\/25\/recipe\/\">\u0441\u043e\u0432\u0435\u0442\u0430\u043c<\/a> \u043f\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044e \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0445 \u0441\u0435\u0442\u0435\u0439 <a href=\"https:\/\/twitter.com\/account\/access\">\u0410\u043d\u0434\u0440\u0435\u044f \u041a\u043e\u0440\u043f\u0430\u0442\u043e\u0433\u043e<\/a>.","49faa9d2":"\u0420\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u0443\u044e \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 **\u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0430\u044e** \u0438 **\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0443\u044e**. \u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043f\u0430\u043f\u043a\u0438 `train` \u0438 `val` \u0438 \u0432 \u043a\u0430\u0436\u0434\u043e\u0439 \u0438\u0437 \u043d\u0438\u0445 \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043f\u0430\u043f\u043a\u0438 `cleaned` \u0438 `dirty`.","a1d8e81c":"# 5. \u0421\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439","8180c407":"\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438","dc0d11c2":"## 1.2 \u0417\u0430\u0434\u0430\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0441\u0435\u0442\u0438","1f1ac9ed":"\u0411\u0443\u0434\u0435\u043c \u043a\u043b\u0430\u0441\u0442\u044c \u043a\u0430\u0436\u0434\u043e\u0435 `valid_split` \u043f\u043e \u0441\u0447\u0435\u0442\u0443 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0438\u0437 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0432 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443 (\u0432 \u043f\u0430\u043f\u043a\u0443 `val`), \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0431\u0443\u0434\u0443\u0442 \u0441\u043a\u043e\u043f\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b \u0432 \u043f\u0430\u043f\u043a\u0443 `train`","2ffe17e1":"# 4. \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438","8bb64172":"# 1. \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430","556045fe":"## 1.1 \u0424\u0438\u043a\u0441\u0438\u0440\u0443\u0435\u043c \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\u044b \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0445 \u0447\u0438\u0441\u0435\u043b","1b85001a":"\u0418\u043c\u0435\u043d\u0430 \u043a\u043b\u0430\u0441\u0441\u043e\u0432","d54c8bbb":"\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0447\u0430\u0441\u0442\u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438","1d301686":"## 4.1. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439","4104fe0b":"# 3. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f","374711bd":"# \u0423\u0434\u0430\u043b\u044f\u0435\u043c \u0441\u043e\u0437\u0434\u0430\u043d\u043d\u044b\u0435 \u0444\u0430\u0439\u043b\u044b","d849aab5":"---","3a2777a2":"## 1.3 \u0418\u0437\u0443\u0447\u0430\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435","2bb65bcf":"\u0417\u0430\u0434\u0430\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0439 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0434\u043b\u044f \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439.","04d96cf1":"\u041e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u0438\u0434\u0435\u0435\u0439 \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u044f\u0432\u043b\u044f\u043b\u043e\u0441\u044c \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u043a\u0430\u0440\u043a\u0430\u0441 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u0441 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0435\u0439 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u044d\u0442\u0430\u043f\u0430. \n\n\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0433\u043b\u0443\u0431\u0436\u0435 \u043f\u043e\u043d\u044f\u0442\u044c \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u043f\u043e\u0432\u0435\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u0440\u0438 \u0435\u0435 \u043c\u043e\u0434\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438. \u041f\u0440\u0438 \u0442\u0430\u043a\u043e\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u0435 \u043d\u0435\u0431\u043e\u0445\u0434\u0438\u043c\u043e \u0430\u043a\u043a\u0443\u0440\u0430\u0442\u043d\u043e \u0432\u043d\u043e\u0441\u0438\u0442\u044c \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0432 \u043c\u043e\u0434\u0435\u043b\u044c, \u043f\u043e \u043e\u0434\u043d\u043e\u043c\u0443 \u0437\u0430 \u0440\u0430\u0437."}}