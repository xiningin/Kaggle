{"cell_type":{"c3ebba8b":"code","ad04b0a3":"code","b275eddd":"code","dd482ca1":"code","10e55bf5":"code","fcafb352":"code","a3917499":"code","937c0f17":"code","9d752b09":"code","86b0e0ed":"code","0a59507a":"code","b39b7ae9":"code","3ba410ff":"code","f74bdf3d":"code","461987d3":"code","80ab8695":"code","c09d861c":"code","9b45cf05":"code","eadf78b1":"code","b1afb9a8":"code","ae7a7fe1":"code","ed8b4ee3":"code","f1a6f3fc":"code","b4336437":"code","b66d39db":"code","708f3d19":"code","0d156300":"code","66cdf235":"code","f7c1639c":"code","61c266ba":"code","90cdb989":"code","670e42bb":"code","e7d74342":"code","c4dfb9e0":"code","b2abbec0":"code","7aa57f14":"code","2539f73f":"code","1c34fb9f":"code","de00327e":"code","005a5793":"code","98d450af":"code","15e4459e":"markdown","f9b6f6dc":"markdown","f8f11f82":"markdown","48821c08":"markdown","2c587ff8":"markdown","ad5e8720":"markdown","755b3d11":"markdown","1f545194":"markdown","d4d89a55":"markdown","ddfdd13a":"markdown","70942e54":"markdown","727f64fc":"markdown","8b376011":"markdown","0e34db52":"markdown","5ea86ca6":"markdown","a2b7633c":"markdown","ec4559c0":"markdown","9cbb4db5":"markdown"},"source":{"c3ebba8b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad04b0a3":"df = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")\ndf.head()","b275eddd":"df['review'] = df['review'].apply(lambda x:x.lower())","dd482ca1":"df['review'][0]","10e55bf5":"from string import punctuation\nprint(punctuation)","fcafb352":"df['clean_text'] = df['review'].apply(lambda x:''.join([c for c in x if c not in punctuation]))","a3917499":"df['clean_text'][0]","937c0f17":"df['len_review'] = df['clean_text'].apply(lambda x:len(x))","9d752b09":"df.head()","86b0e0ed":"all_text2 = df['clean_text'].tolist()","0a59507a":"from collections import Counter\nall_text2 = ' '.join(all_text2)\n# create a list of words\nwords = all_text2.split()","b39b7ae9":"# Count all the words using Counter Method\ncount_words = Counter(words)\n\ntotal_words = len(words)\nsorted_words = count_words.most_common(total_words)","3ba410ff":"vocab_to_int = {w:i for i, (w,c) in enumerate(sorted_words)}","f74bdf3d":"vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}","461987d3":"reviews_split = df['clean_text'].tolist()","80ab8695":"reviews_int = []\nfor review in reviews_split:\n    r = [vocab_to_int[w] for w in review.split()]\n    reviews_int.append(r)\nprint (reviews_int[0:3])","c09d861c":"df.head()","9b45cf05":"labels_split = df['sentiment'].tolist()","eadf78b1":"encoded_labels = [1 if label =='positive' else 0 for label in labels_split]\nencoded_labels = np.array(encoded_labels)","b1afb9a8":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nreviews_len = [len(x) for x in reviews_int]\npd.Series(reviews_len).hist()\nplt.show()\npd.Series(reviews_len).describe()","ae7a7fe1":"reviews_int = [ reviews_int[i] for i, l in enumerate(reviews_len) if l>0 ]\nencoded_labels = [ encoded_labels[i] for i, l in enumerate(reviews_len) if l> 0 ]","ed8b4ee3":"def pad_features(reviews_int, seq_length):\n    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n    '''\n    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n    \n    for i, review in enumerate(reviews_int):\n        review_len = len(review)\n        \n        if review_len <= seq_length:\n            zeroes = list(np.zeros(seq_length-review_len))\n            new = zeroes+review\n        elif review_len > seq_length:\n            new = review[0:seq_length]\n        \n        features[i,:] = np.array(new)\n    \n    return features","f1a6f3fc":"features = pad_features(reviews_int,200)","b4336437":"print (features[:10,:])","b66d39db":"len_feat = len(features)\nsplit_frac = 0.8","708f3d19":"split_frac = 0.8\ntrain_x = features[0:int(split_frac*len_feat)]\ntrain_y = encoded_labels[0:int(split_frac*len_feat)]\nremaining_x = features[int(split_frac*len_feat):]\nremaining_y = encoded_labels[int(split_frac*len_feat):]\nvalid_x = remaining_x[0:int(len(remaining_x)*0.5)]\nvalid_y = remaining_y[0:int(len(remaining_y)*0.5)]\ntest_x = remaining_x[int(len(remaining_x)*0.5):]\ntest_y = remaining_y[int(len(remaining_y)*0.5):]","0d156300":"type(test_y)","66cdf235":"train_y = np.array(train_y)\ntest_y = np.array(test_y)\nvalid_y = np.array(valid_y)","f7c1639c":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n# create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\ntest_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n# dataloaders\nbatch_size = 50\n# make sure to SHUFFLE your data\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)","61c266ba":"# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint()\nprint('Sample label size: ', sample_y.size()) # batch_size\nprint('Sample label: \\n', sample_y)","90cdb989":"import torch.nn as nn\n\nclass SentimentLSTM(nn.Module):\n    \"\"\"\n    The RNN model that will be used to perform Sentiment analysis.\n    \"\"\"\n\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super().__init__()\n\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.3)\n        \n        # linear and sigmoid layers\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n        \n\n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        batch_size = x.size(0)\n\n        # embeddings and lstm_out\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n    \n        # stack up lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        # dropout and fully-connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        # sigmoid function\n        sig_out = self.sig(out)\n        \n        # reshape to be batch_size first\n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        # return last sigmoid output and hidden state\n        return sig_out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden\n        ","670e42bb":"# Instantiate the model w\/ hyperparams\nvocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\noutput_size = 1\nembedding_dim = 400\nhidden_dim = 256\nn_layers = 2\nnet = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n\nprint(net)","e7d74342":"# First checking if GPU is available\ntrain_on_gpu=torch.cuda.is_available()\n\nif(train_on_gpu):\n    print('Training on GPU.')\nelse:\n    print('No GPU available, training on CPU.')","c4dfb9e0":"# loss and optimization functions\nlr=0.001\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)","b2abbec0":"# training params\n\nepochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n\ncounter = 0\nprint_every = 100\nclip=5 # gradient clipping\n\n# move model to GPU, if available\nif(train_on_gpu):\n    net.cuda()\n\nnet.train()\n# train for some number of epochs\nfor e in range(epochs):\n    # initialize hidden state\n    h = net.init_hidden(batch_size)\n\n    # batch loop\n    for inputs, labels in train_loader:\n        counter += 1\n        #print(counter)\n\n        if(train_on_gpu):\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n\n        # zero accumulated gradients\n        net.zero_grad()\n\n        # get the output from the model\n        output, h = net(inputs, h)\n\n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs \/ LSTMs.\n        nn.utils.clip_grad_norm_(net.parameters(), clip)\n        optimizer.step()\n\n        # loss stats\n        if counter % print_every == 0:\n            # Get validation loss\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs, labels in valid_loader:\n\n                # Creating new variables for the hidden state, otherwise\n                # we'd backprop through the entire training history\n                val_h = tuple([each.data for each in val_h])\n\n                if(train_on_gpu):\n                    inputs, labels = inputs.cuda(), labels.cuda()\n\n                output, val_h = net(inputs, val_h)\n                val_loss = criterion(output.squeeze(), labels.float())\n\n                val_losses.append(val_loss.item())\n\n            net.train()\n            print(\"Epoch: {}\/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","7aa57f14":"# Get test data loss and accuracy\n\ntest_losses = [] # track loss\nnum_correct = 0\n\n# init hidden state\nh = net.init_hidden(batch_size)\n\nnet.eval()\n# iterate over test data\nfor inputs, labels in test_loader:\n\n    # Creating new variables for the hidden state, otherwise\n    # we'd backprop through the entire training history\n    h = tuple([each.data for each in h])\n\n    if(train_on_gpu):\n        inputs, labels = inputs.cuda(), labels.cuda()\n    \n    # get predicted outputs\n    output, h = net(inputs, h)\n    \n    # calculate loss\n    test_loss = criterion(output.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    \n    # convert output probabilities to predicted class (0 or 1)\n    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n    \n    # compare predictions to true label\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\n\n# -- stats! -- ##\n# avg test loss\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n\n# accuracy over all test data\ntest_acc = num_correct\/len(test_loader.dataset)\nprint(\"Test accuracy: {:.3f}\".format(test_acc))","2539f73f":"# negative test review\ntest_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'","1c34fb9f":"def preprocess(review, vocab_to_int):\n    review = review.lower()\n    word_list = review.split()\n    num_list = []\n    #list of reviews\n    #though it contains only one review as of now\n    reviews_int = []\n    for word in word_list:\n        if word in vocab_to_int.keys():\n            num_list.append(vocab_to_int[word])\n    reviews_int.append(num_list)\n    return reviews_int","de00327e":"def predict(net, test_review, sequence_length=200):\n    ''' Prints out whether a give review is predicted to be \n        positive or negative in sentiment, using a trained model.\n        \n        params:\n        net - A trained net \n        test_review - a review made of normal text and punctuation\n        sequence_length - the padded length of a review\n        '''\n    #change the reviews to sequence of integers\n    int_rev = preprocess(test_review, vocab_to_int)\n    #pad the reviews as per the sequence length of the feature\n    features = pad_features(int_rev, seq_length=seq_length)\n    \n    #changing the features to PyTorch tensor\n    features = torch.from_numpy(features)\n    \n    #pass the features to the model to get prediction\n    net.eval()\n    val_h = net.init_hidden(1)\n    val_h = tuple([each.data for each in val_h])\n\n    if(train_on_gpu):\n        features = features.cuda()\n\n    output, val_h = net(features, val_h)\n    \n    #rounding the output to nearest 0 or 1\n    pred = torch.round(output)\n    \n    #mapping the numeric values to postive or negative\n    output = [\"Positive\" if pred.item() == 1 else \"Negative\"]\n    \n    # print custom response based on whether test_review is pos\/neg\n    print(output)","005a5793":"# positive test review\ntest_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'","98d450af":"# call function\n# try negative and positive reviews!\nseq_length=200\npredict(net, test_review_pos, seq_length)","15e4459e":"## Training the Network","f9b6f6dc":"# Data Processing \u2014 remove punctuation","f8f11f82":"##  Tokenize \u2014 Encode the words","48821c08":"### Testing\n- On Test Data","2c587ff8":"### Tokenize \u2014 Encode the labels","ad5e8720":"## Instantiate the network","755b3d11":"## Dataloaders and Batching","1f545194":"# Load in and visualize the data","d4d89a55":"### Tokenize \u2014 Create Vocab to Int mapping dictionary","ddfdd13a":"## Define the LSTM Network Architecture\n","70942e54":"### Padding \/ Truncating the remaining data","727f64fc":"#  Data Processing \u2014 create list of reviews","8b376011":"## Analyze Reviews Length","0e34db52":"### On User-generated Data\nFirst, we will define a tokenize function that will take care of pre-processing steps and then we will create a predict function that will give us the final output after parsing the user provided review.\n","5ea86ca6":"# Data Processing \u2014 convert to lower case","a2b7633c":"## Training Loop\nMost of the code in training loop is pretty standard Deep Learning training code that you might see often in all the implementations that\u2019s using PyTorch framework.","ec4559c0":"### Training, Validation, Test Dataset Split","9cbb4db5":"###  Removing Outliers \u2014 Getting rid of extremely long or short reviews"}}