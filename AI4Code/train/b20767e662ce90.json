{"cell_type":{"7add0c5b":"code","1376651a":"code","8b642c30":"code","7d60ad34":"code","42e498a4":"code","c208a579":"code","9a2a52b8":"code","82b30f14":"code","9391bba2":"code","dfb422c6":"code","09f14bf7":"code","f789d545":"code","22099b88":"code","b06f068f":"code","02a65fc9":"code","271e9c28":"code","925f0807":"code","9a0ebf60":"code","91982a3f":"code","ce8f4a8f":"code","f82ca926":"code","14e9ae04":"code","65662429":"code","694bb54d":"code","2a89c651":"code","3d34d533":"code","f7c90daa":"code","d000735e":"code","63cac8de":"code","ba076919":"code","b74b62d9":"code","12068333":"code","2b52cf4c":"code","3405a11c":"code","cb3051cd":"code","b80b522e":"code","76565de9":"code","78da4f29":"code","999b9d29":"code","caa9f90c":"code","a52d3737":"code","a7ce725c":"code","db6bc19f":"code","048f5368":"code","5411628f":"code","c73c245f":"code","c8591ceb":"code","dd3237da":"code","3dadddf2":"code","4af59fed":"code","0008bfee":"code","7511c2cc":"code","2d81adc4":"code","4a9f7267":"code","8fa243e4":"code","1de19822":"code","eab6cbb2":"code","abe22a23":"code","11c86983":"code","88f6fa30":"code","fd911cc9":"code","0ee38bb6":"markdown","db4dfb78":"markdown","815625e9":"markdown","94cbd1a8":"markdown","3c6d0e7b":"markdown","c180e406":"markdown","fd3c19ee":"markdown","929378a8":"markdown","2f63e5c6":"markdown","e06dfba0":"markdown","e1ab031c":"markdown","e0a6fb74":"markdown","618793b9":"markdown","74a49e1e":"markdown","69572ffa":"markdown","0fd67c43":"markdown","cf261c43":"markdown","05c800d1":"markdown","9f5563cc":"markdown","5cad8eb4":"markdown","9b478f8e":"markdown","15c664fc":"markdown","6b61ce57":"markdown","36c7df3c":"markdown","4c5dbb56":"markdown","c739b696":"markdown","29a8f0bb":"markdown","fd8a04e1":"markdown","5bb283d8":"markdown","13ced0cc":"markdown","c6642993":"markdown","bc64f852":"markdown","6f785160":"markdown","4f44628a":"markdown","7eb10cac":"markdown","cdfe9d1a":"markdown","09535bcf":"markdown","89005592":"markdown","5ba93c11":"markdown","b4c34fd9":"markdown","87e70e4f":"markdown","cb81f2c6":"markdown","03ba24a8":"markdown","c3b5019c":"markdown","9d50fd3f":"markdown","8f4abde1":"markdown","50f36202":"markdown","d6d6e4fa":"markdown","246e9bfa":"markdown","4e745e96":"markdown","996ae66c":"markdown","58996948":"markdown","0f143559":"markdown","806bed2b":"markdown","4731f320":"markdown","aa107212":"markdown","3cde889e":"markdown","e0ecee70":"markdown","afb8bf81":"markdown","e983bf60":"markdown","a0a7a192":"markdown","2d0995cc":"markdown","87d52576":"markdown","fade0b20":"markdown","cf998b4e":"markdown","118cc514":"markdown","2c672ca7":"markdown","c61d68e4":"markdown","173d0212":"markdown","0e715b4a":"markdown","3d58c653":"markdown","c5a94d62":"markdown","4f51dffa":"markdown","5340fa92":"markdown","2abe1464":"markdown","11f424a4":"markdown","ae809420":"markdown","40602ca6":"markdown","022fdb70":"markdown","7d4d36ee":"markdown","72d4e4ec":"markdown","e882596c":"markdown","042c5acb":"markdown","772cf252":"markdown","6ee31ed2":"markdown","ce59ad07":"markdown","5225f066":"markdown","0454ffc5":"markdown","91ea3222":"markdown","71328177":"markdown","c1eae25f":"markdown","2f4cdef1":"markdown","71417850":"markdown","d16cbcc8":"markdown","ed7b625b":"markdown","2113c8a4":"markdown","8b74c729":"markdown","86d45369":"markdown","04d58bd8":"markdown","d2b8fac2":"markdown","eb1cd918":"markdown","b653e4d0":"markdown","fa70df1c":"markdown","70465d91":"markdown","db4fedf1":"markdown","9f788af6":"markdown","783ba348":"markdown","2cbe0bc6":"markdown","bbc0a84b":"markdown","a350c42a":"markdown","8906128e":"markdown","baf18715":"markdown"},"source":{"7add0c5b":"# @title Tutorial slides\n\n# @markdown These are the slides for the videos in this tutorial\n\n# @markdown If you want to download locally the slides, click [here](https:\/\/osf.io\/tzfsn\/download)\nfrom IPython.display import IFrame\nIFrame(src=f\"https:\/\/mfr.ca-1.osf.io\/render?url=https:\/\/osf.io\/tzfsn\/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)","1376651a":"# @title Install dependencies\n!pip install Pillow --quiet\n\n!pip install git+https:\/\/github.com\/NeuromatchAcademy\/evaltools --quiet\nfrom evaltools.airtable import AirtableForm\n\n# generate airtable form\natform = AirtableForm('appn7VdPRseSoMXEG','W2D2_T1','https:\/\/portal.neuromatchacademy.org\/api\/redirect\/to\/96ec754b-e76b-43f7-903e-76df0ac63749')","8b642c30":"# Import libraries\nimport os\nimport time\nimport tqdm\nimport torch\nimport IPython\nimport torchvision\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torchvision import transforms\nfrom torchvision.models import AlexNet\nfrom torchvision.utils import make_grid\nfrom torchvision.datasets import ImageFolder\n\nfrom PIL import Image\nfrom io import BytesIO","7d60ad34":"# @title Figure settings\nimport ipywidgets as widgets       # interactive display\n%config InlineBackend.figure_format = 'retina'\nplt.style.use(\"https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/content-creation\/main\/nma.mplstyle\")","42e498a4":"# @title Set random seed\n\n# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n\n# for DL its critical to set the random seed so that students can have a\n# baseline to compare their results to expected results.\n# Read more here: https:\/\/pytorch.org\/docs\/stable\/notes\/randomness.html\n\n# Call `set_seed` function in the exercises to ensure reproducibility.\nimport random\nimport torch\n\ndef set_seed(seed=None, seed_torch=True):\n  if seed is None:\n    seed = np.random.choice(2 ** 32)\n  random.seed(seed)\n  np.random.seed(seed)\n  if seed_torch:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n  print(f'Random seed {seed} has been set.')\n\n\n# In case that `DataLoader` is used\ndef seed_worker(worker_id):\n  worker_seed = torch.initial_seed() % 2**32\n  np.random.seed(worker_seed)\n  random.seed(worker_seed)","c208a579":"# @title Set device (GPU or CPU). Execute `set_device()`\n# especially if torch modules used.\n\n# inform the user if the notebook uses GPU or CPU.\n\ndef set_device():\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  if device != \"cuda\":\n    print(\"WARNING: For this notebook to perform best, \"\n        \"if possible, in the menu under `Runtime` -> \"\n        \"`Change runtime type.`  select `GPU` \")\n  else:\n    print(\"GPU is enabled in this notebook.\")\n\n  return device","9a2a52b8":"SEED = 2021\nset_seed(seed=SEED)\nDEVICE = set_device()","82b30f14":"# @title Video 1: Modern CNNs and Transfer Learning\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1Wf4y157wE\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"mfOd2EKzscM\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 1: Modern CNNs and Transfer Learning')\n\ndisplay(out)","9391bba2":"class FullyConnectedNet(nn.Module):\n  def __init__(self):\n    super(FullyConnectedNet, self).__init__()\n\n    image_width = 128\n    image_channels = 3\n    self.input_size = image_channels * image_width ** 2\n\n    self.fc1 = nn.Linear(self.input_size, 256)\n\n  def forward(self, x):\n    x = x.view(-1, self.input_size)\n    return self.fc1(x)","dfb422c6":"class ConvNet(nn.Module):\n  def __init__(self):\n    super(ConvNet, self).__init__()\n\n    self.conv1 = nn.Conv2d(in_channels=3,\n                            out_channels=256,\n                            kernel_size=(3, 3),\n                            padding=1)\n\n  def forward(self, x):\n    return self.conv1(x)","09f14bf7":"def get_parameter_count(network):\n  \"\"\"\n  Calculate the number of parameters used by the fully connected network.\n  Hint: Casting the result of network.parameters() to a list may make it\n        easier to work with\n\n  Args:\n      network: Network to calculate the parameters of\n\n  Returns:\n      param_count: The number of parameters in the network\n  \"\"\"\n\n  ####################################################################\n  # Fill in all missing code below (...),\n  # then remove or comment the line below to test your function\n  # raise NotImplementedError(\"Convolution math\")\n  ####################################################################\n  # Get the network's parameters\n  parameters = network.parameters()\n\n  param_count = 0\n  # Loop over all layers\n  for layer in parameters:\n    param_count += torch.numel(layer)\n\n  return param_count\n\n\n# add event to airtable\natform.add_event('Coding Exercise 1: Calculate number of parameters in FCNN vs ConvNet')\n\n# Initialize networks\nfccnet = FullyConnectedNet()\nconvnet = ConvNet()\n## Apply the above defined function to both networks by uncommenting the following lines\nprint(f\"FCCN parameter count: {get_parameter_count(fccnet)}\")\nprint(f\"ConvNet parameter count: {get_parameter_count(convnet)}\")","f789d545":"# @title Parameter Calculator\n# @markdown Run this cell to enable the widget!\n\ndef calculate_parameters(filter_count, image_width, fcnn_nodes):\n  # Convnet math: Implement how parameters scale as a function of image size\n  # between convnets and FCNN\n\n  filter_width = 3\n  image_channels = 3\n\n  # Assuming a square, RGB image\n  image_area = image_width ** 2\n  image_volume = image_area * image_channels\n\n  # If we're using padding=same, the output of a convnet will be the same shape\n  # as the original image, but with more features\n  fcnn_parameters = image_volume * fcnn_nodes\n  cnn_parameters = image_channels * filter_count * filter_width ** 2\n\n  # Add bias\n  fcnn_parameters += fcnn_nodes\n  cnn_parameters += filter_count\n\n  print(f\"CNN parameters: {cnn_parameters}\")\n  print(f\"Fully Connected parameters: {fcnn_parameters}\")\n\n  return None\n\n\n_ = widgets.interact(calculate_parameters,\n                     filter_count=(16, 512, 16),\n                     image_width=(16, 512, 16),\n                     fcnn_nodes=(16, 512, 16))","22099b88":"# @title Video 2: History of convnets\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1364y167Qy\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"xtoLjKSPrUQ\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 2: History of convnets')\n\ndisplay(out)","b06f068f":"# @title Student Response\nfrom ipywidgets import widgets\n\n\ntext=widgets.Textarea(\n   value='Type your answer here and click on `Submit!`',\n   placeholder='Type something',\n   description='',\n   disabled=False\n)\n\nbutton = widgets.Button(description=\"Submit!\")\n\ndisplay(text,button)\n\ndef on_button_clicked(b):\n   atform.add_answer('q1', text.value)\n   print(\"Submission successful!\")\n\n\nbutton.on_click(on_button_clicked)","02a65fc9":"# @title Video 3: AlexNet & VGG\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV12U4y1n7q5\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"ZB87qC7yPiE\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 3: AlexNet & VGG')\n\ndisplay(out)","271e9c28":"# @title Import Alexnet\n# @markdown This cell gives you the `alexnet` model as well as the `input_image` and `input_batch` variables used below\nimport requests, urllib\n\n# original link: https:\/\/s3.amazonaws.com\/pytorch\/models\/alexnet-owt-4df8aa71.pth\nstate_dict = torch.hub.load_state_dict_from_url(\"https:\/\/osf.io\/9dzeu\/download\")\n\nalexnet = AlexNet()\nalexnet.load_state_dict(state_dict=state_dict)\n\nurl, filename = (\"https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W2D2_ModernConvnets\/static\/dog.jpg\", \"dog.jpg\")\ntry: urllib.URLopener().retrieve(url, filename)\nexcept: urllib.request.urlretrieve(url, filename)\n\ninput_image = Image.open(filename)\npreprocess = transforms.Compose([\n                                 transforms.Resize(256),\n                                 transforms.CenterCrop(224),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                      std=[0.229, 0.224, 0.225]),\n                                 ])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n\n# move the input and model to GPU for speed if available\nif torch.cuda.is_available():\n  input_batch = input_batch.cuda()\n  alexnet.cuda()","925f0807":"with torch.no_grad():\n  params = list(alexnet.parameters())\n  fig, axs = plt.subplots(8, 8, figsize=(8, 8))\n  filters = []\n  for filter_index in range(params[0].shape[0]):\n    row_index = filter_index \/\/ 8\n    col_index = filter_index % 8\n\n    filter = params[0][filter_index,:,:,:]\n    filter_image = filter.permute(1, 2, 0).cpu()\n    scale = np.abs(filter_image).max()\n    scaled_image = filter_image \/ (2 * scale) + 0.5 # between 0 and 1.\n    filters.append(scaled_image.cpu())\n    axs[row_index, col_index].imshow(scaled_image.cpu())\n    axs[row_index, col_index].axis('off')\n  plt.show()","9a0ebf60":"# @title Student Response\nfrom ipywidgets import widgets\n\n\ntext=widgets.Textarea(\n   value='Type your answer here and click on `Submit!`',\n   placeholder='Type something',\n   description='',\n   disabled=False\n)\n\nbutton = widgets.Button(description=\"Submit!\")\n\ndisplay(text,button)\n\ndef on_button_clicked(b):\n   atform.add_answer('q2', text.value)\n   print(\"Submission successful!\")\n\n\nbutton.on_click(on_button_clicked)","91982a3f":"# @title Image Widget Code\n# @markdown Run this cell to enable the widget\n\ndef alexnet_intermediate_output(net, image):\n    return F.relu(net.features[0](image))\n\n\ndef browse_images(input_batch, input_image):\n  intermediate_output = alexnet_intermediate_output(alexnet, input_batch)\n  n = intermediate_output.shape[1]\n\n  def view_image(i):\n    with torch.no_grad():\n      channel = intermediate_output[0, i, :].squeeze()\n      fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n      ax[0].imshow(input_image)\n      ax[1].imshow(filters[i])\n      ax[1].set_xlim([-22, 33])\n      ax[2].imshow(channel.cpu())\n      ax[0].set_title('Input image')\n      ax[1].set_title(f\"Filter {i}\")\n      ax[2].set_title(f\"Filter {i} on input image\")\n      [axi.set_axis_off() for axi in ax.ravel()]\n\n  widgets.interact(view_image, i=(0, n-1))\n\n\nbrowse_images(input_batch, input_image)","ce8f4a8f":"print(alexnet)","f82ca926":"# @title Student Response\nfrom ipywidgets import widgets\n\n\ntext=widgets.Textarea(\n   value='Type your answer here and click on `Submit!`',\n   placeholder='Type something',\n   description='',\n   disabled=False\n)\n\nbutton = widgets.Button(description=\"Submit!\")\n\ndisplay(text,button)\n\ndef on_button_clicked(b):\n   atform.add_answer('q3', text.value)\n   print(\"Submission successful!\")\n\n\nbutton.on_click(on_button_clicked)","14e9ae04":"# @title Video 4: Residual Networks (ResNets)\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1bf4y1j7od\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"EJSZnJyy4PI\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 4: Residual Networks (ResNets)')\n\ndisplay(out)","65662429":"# @title Download imagenette\nimport requests, tarfile, os\n\nfname = 'imagenette2-320'\nurl = 'https:\/\/osf.io\/mnve4\/download'\n\nif not os.path.exists(fname):\n  print(\"Data is being downloaded...\")\n  r = requests.get(url, stream=True)\n  with open(fname+'tgz', 'wb') as fd:\n    fd.write(r.content)\n\n  with tarfile.open(fname+'tgz', \"r\") as ft:\n    ft.extractall()\n\n  os.remove(fname+'tgz')\n  print(\"The download has been completed.\")\nelse:\n  print(\"Data has already been downloaded.\")","694bb54d":"# @title Set Up Textual ImageNet labels\ndict_map={0: 'tench, Tinca tinca',\n 1: 'goldfish, Carassius auratus',\n 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n 3: 'tiger shark, Galeocerdo cuvieri',\n 4: 'hammerhead, hammerhead shark',\n 5: 'electric ray, crampfish, numbfish, torpedo',\n 6: 'stingray',\n 7: 'cock',\n 8: 'hen',\n 9: 'ostrich, Struthio camelus',\n 10: 'brambling, Fringilla montifringilla',\n 11: 'goldfinch, Carduelis carduelis',\n 12: 'house finch, linnet, Carpodacus mexicanus',\n 13: 'junco, snowbird',\n 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n 15: 'robin, American robin, Turdus migratorius',\n 16: 'bulbul',\n 17: 'jay',\n 18: 'magpie',\n 19: 'chickadee',\n 20: 'water ouzel, dipper',\n 21: 'kite',\n 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n 23: 'vulture',\n 24: 'great grey owl, great gray owl, Strix nebulosa',\n 25: 'European fire salamander, Salamandra salamandra',\n 26: 'common newt, Triturus vulgaris',\n 27: 'eft',\n 28: 'spotted salamander, Ambystoma maculatum',\n 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n 30: 'bullfrog, Rana catesbeiana',\n 31: 'tree frog, tree-frog',\n 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n 35: 'mud turtle',\n 36: 'terrapin',\n 37: 'box turtle, box tortoise',\n 38: 'banded gecko',\n 39: 'common iguana, iguana, Iguana iguana',\n 40: 'American chameleon, anole, Anolis carolinensis',\n 41: 'whiptail, whiptail lizard',\n 42: 'agama',\n 43: 'frilled lizard, Chlamydosaurus kingi',\n 44: 'alligator lizard',\n 45: 'Gila monster, Heloderma suspectum',\n 46: 'green lizard, Lacerta viridis',\n 47: 'African chameleon, Chamaeleo chamaeleon',\n 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n 50: 'American alligator, Alligator mississipiensis',\n 51: 'triceratops',\n 52: 'thunder snake, worm snake, Carphophis amoenus',\n 53: 'ringneck snake, ring-necked snake, ring snake',\n 54: 'hognose snake, puff adder, sand viper',\n 55: 'green snake, grass snake',\n 56: 'king snake, kingsnake',\n 57: 'garter snake, grass snake',\n 58: 'water snake',\n 59: 'vine snake',\n 60: 'night snake, Hypsiglena torquata',\n 61: 'boa constrictor, Constrictor constrictor',\n 62: 'rock python, rock snake, Python sebae',\n 63: 'Indian cobra, Naja naja',\n 64: 'green mamba',\n 65: 'sea snake',\n 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n 69: 'trilobite',\n 70: 'harvestman, daddy longlegs, Phalangium opilio',\n 71: 'scorpion',\n 72: 'black and gold garden spider, Argiope aurantia',\n 73: 'barn spider, Araneus cavaticus',\n 74: 'garden spider, Aranea diademata',\n 75: 'black widow, Latrodectus mactans',\n 76: 'tarantula',\n 77: 'wolf spider, hunting spider',\n 78: 'tick',\n 79: 'centipede',\n 80: 'black grouse',\n 81: 'ptarmigan',\n 82: 'ruffed grouse, partridge, Bonasa umbellus',\n 83: 'prairie chicken, prairie grouse, prairie fowl',\n 84: 'peacock',\n 85: 'quail',\n 86: 'partridge',\n 87: 'African grey, African gray, Psittacus erithacus',\n 88: 'macaw',\n 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n 90: 'lorikeet',\n 91: 'coucal',\n 92: 'bee eater',\n 93: 'hornbill',\n 94: 'hummingbird',\n 95: 'jacamar',\n 96: 'toucan',\n 97: 'drake',\n 98: 'red-breasted merganser, Mergus serrator',\n 99: 'goose',\n 100: 'black swan, Cygnus atratus',\n 101: 'tusker',\n 102: 'echidna, spiny anteater, anteater',\n 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n 104: 'wallaby, brush kangaroo',\n 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n 106: 'wombat',\n 107: 'jellyfish',\n 108: 'sea anemone, anemone',\n 109: 'brain coral',\n 110: 'flatworm, platyhelminth',\n 111: 'nematode, nematode worm, roundworm',\n 112: 'conch',\n 113: 'snail',\n 114: 'slug',\n 115: 'sea slug, nudibranch',\n 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n 117: 'chambered nautilus, pearly nautilus, nautilus',\n 118: 'Dungeness crab, Cancer magister',\n 119: 'rock crab, Cancer irroratus',\n 120: 'fiddler crab',\n 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n 124: 'crayfish, crawfish, crawdad, crawdaddy',\n 125: 'hermit crab',\n 126: 'isopod',\n 127: 'white stork, Ciconia ciconia',\n 128: 'black stork, Ciconia nigra',\n 129: 'spoonbill',\n 130: 'flamingo',\n 131: 'little blue heron, Egretta caerulea',\n 132: 'American egret, great white heron, Egretta albus',\n 133: 'bittern',\n 134: 'crane',\n 135: 'limpkin, Aramus pictus',\n 136: 'European gallinule, Porphyrio porphyrio',\n 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n 138: 'bustard',\n 139: 'ruddy turnstone, Arenaria interpres',\n 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n 141: 'redshank, Tringa totanus',\n 142: 'dowitcher',\n 143: 'oystercatcher, oyster catcher',\n 144: 'pelican',\n 145: 'king penguin, Aptenodytes patagonica',\n 146: 'albatross, mollymawk',\n 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n 149: 'dugong, Dugong dugon',\n 150: 'sea lion',\n 151: 'Chihuahua',\n 152: 'Japanese spaniel',\n 153: 'Maltese dog, Maltese terrier, Maltese',\n 154: 'Pekinese, Pekingese, Peke',\n 155: 'Shih-Tzu',\n 156: 'Blenheim spaniel',\n 157: 'papillon',\n 158: 'toy terrier',\n 159: 'Rhodesian ridgeback',\n 160: 'Afghan hound, Afghan',\n 161: 'basset, basset hound',\n 162: 'beagle',\n 163: 'bloodhound, sleuthhound',\n 164: 'bluetick',\n 165: 'black-and-tan coonhound',\n 166: 'Walker hound, Walker foxhound',\n 167: 'English foxhound',\n 168: 'redbone',\n 169: 'borzoi, Russian wolfhound',\n 170: 'Irish wolfhound',\n 171: 'Italian greyhound',\n 172: 'whippet',\n 173: 'Ibizan hound, Ibizan Podenco',\n 174: 'Norwegian elkhound, elkhound',\n 175: 'otterhound, otter hound',\n 176: 'Saluki, gazelle hound',\n 177: 'Scottish deerhound, deerhound',\n 178: 'Weimaraner',\n 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n 181: 'Bedlington terrier',\n 182: 'Border terrier',\n 183: 'Kerry blue terrier',\n 184: 'Irish terrier',\n 185: 'Norfolk terrier',\n 186: 'Norwich terrier',\n 187: 'Yorkshire terrier',\n 188: 'wire-haired fox terrier',\n 189: 'Lakeland terrier',\n 190: 'Sealyham terrier, Sealyham',\n 191: 'Airedale, Airedale terrier',\n 192: 'cairn, cairn terrier',\n 193: 'Australian terrier',\n 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n 195: 'Boston bull, Boston terrier',\n 196: 'miniature schnauzer',\n 197: 'giant schnauzer',\n 198: 'standard schnauzer',\n 199: 'Scotch terrier, Scottish terrier, Scottie',\n 200: 'Tibetan terrier, chrysanthemum dog',\n 201: 'silky terrier, Sydney silky',\n 202: 'soft-coated wheaten terrier',\n 203: 'West Highland white terrier',\n 204: 'Lhasa, Lhasa apso',\n 205: 'flat-coated retriever',\n 206: 'curly-coated retriever',\n 207: 'golden retriever',\n 208: 'Labrador retriever',\n 209: 'Chesapeake Bay retriever',\n 210: 'German short-haired pointer',\n 211: 'vizsla, Hungarian pointer',\n 212: 'English setter',\n 213: 'Irish setter, red setter',\n 214: 'Gordon setter',\n 215: 'Brittany spaniel',\n 216: 'clumber, clumber spaniel',\n 217: 'English springer, English springer spaniel',\n 218: 'Welsh springer spaniel',\n 219: 'cocker spaniel, English cocker spaniel, cocker',\n 220: 'Sussex spaniel',\n 221: 'Irish water spaniel',\n 222: 'kuvasz',\n 223: 'schipperke',\n 224: 'groenendael',\n 225: 'malinois',\n 226: 'briard',\n 227: 'kelpie',\n 228: 'komondor',\n 229: 'Old English sheepdog, bobtail',\n 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n 231: 'collie',\n 232: 'Border collie',\n 233: 'Bouvier des Flandres, Bouviers des Flandres',\n 234: 'Rottweiler',\n 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n 236: 'Doberman, Doberman pinscher',\n 237: 'miniature pinscher',\n 238: 'Greater Swiss Mountain dog',\n 239: 'Bernese mountain dog',\n 240: 'Appenzeller',\n 241: 'EntleBucher',\n 242: 'boxer',\n 243: 'bull mastiff',\n 244: 'Tibetan mastiff',\n 245: 'French bulldog',\n 246: 'Great Dane',\n 247: 'Saint Bernard, St Bernard',\n 248: 'Eskimo dog, husky',\n 249: 'malamute, malemute, Alaskan malamute',\n 250: 'Siberian husky',\n 251: 'dalmatian, coach dog, carriage dog',\n 252: 'affenpinscher, monkey pinscher, monkey dog',\n 253: 'basenji',\n 254: 'pug, pug-dog',\n 255: 'Leonberg',\n 256: 'Newfoundland, Newfoundland dog',\n 257: 'Great Pyrenees',\n 258: 'Samoyed, Samoyede',\n 259: 'Pomeranian',\n 260: 'chow, chow chow',\n 261: 'keeshond',\n 262: 'Brabancon griffon',\n 263: 'Pembroke, Pembroke Welsh corgi',\n 264: 'Cardigan, Cardigan Welsh corgi',\n 265: 'toy poodle',\n 266: 'miniature poodle',\n 267: 'standard poodle',\n 268: 'Mexican hairless',\n 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n 273: 'dingo, warrigal, warragal, Canis dingo',\n 274: 'dhole, Cuon alpinus',\n 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n 276: 'hyena, hyaena',\n 277: 'red fox, Vulpes vulpes',\n 278: 'kit fox, Vulpes macrotis',\n 279: 'Arctic fox, white fox, Alopex lagopus',\n 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n 281: 'tabby, tabby cat',\n 282: 'tiger cat',\n 283: 'Persian cat',\n 284: 'Siamese cat, Siamese',\n 285: 'Egyptian cat',\n 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n 287: 'lynx, catamount',\n 288: 'leopard, Panthera pardus',\n 289: 'snow leopard, ounce, Panthera uncia',\n 290: 'jaguar, panther, Panthera onca, Felis onca',\n 291: 'lion, king of beasts, Panthera leo',\n 292: 'tiger, Panthera tigris',\n 293: 'cheetah, chetah, Acinonyx jubatus',\n 294: 'brown bear, bruin, Ursus arctos',\n 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n 298: 'mongoose',\n 299: 'meerkat, mierkat',\n 300: 'tiger beetle',\n 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n 302: 'ground beetle, carabid beetle',\n 303: 'long-horned beetle, longicorn, longicorn beetle',\n 304: 'leaf beetle, chrysomelid',\n 305: 'dung beetle',\n 306: 'rhinoceros beetle',\n 307: 'weevil',\n 308: 'fly',\n 309: 'bee',\n 310: 'ant, emmet, pismire',\n 311: 'grasshopper, hopper',\n 312: 'cricket',\n 313: 'walking stick, walkingstick, stick insect',\n 314: 'cockroach, roach',\n 315: 'mantis, mantid',\n 316: 'cicada, cicala',\n 317: 'leafhopper',\n 318: 'lacewing, lacewing fly',\n 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n 320: 'damselfly',\n 321: 'admiral',\n 322: 'ringlet, ringlet butterfly',\n 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n 324: 'cabbage butterfly',\n 325: 'sulphur butterfly, sulfur butterfly',\n 326: 'lycaenid, lycaenid butterfly',\n 327: 'starfish, sea star',\n 328: 'sea urchin',\n 329: 'sea cucumber, holothurian',\n 330: 'wood rabbit, cottontail, cottontail rabbit',\n 331: 'hare',\n 332: 'Angora, Angora rabbit',\n 333: 'hamster',\n 334: 'porcupine, hedgehog',\n 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n 336: 'marmot',\n 337: 'beaver',\n 338: 'guinea pig, Cavia cobaya',\n 339: 'sorrel',\n 340: 'zebra',\n 341: 'hog, pig, grunter, squealer, Sus scrofa',\n 342: 'wild boar, boar, Sus scrofa',\n 343: 'warthog',\n 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n 345: 'ox',\n 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n 347: 'bison',\n 348: 'ram, tup',\n 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n 350: 'ibex, Capra ibex',\n 351: 'hartebeest',\n 352: 'impala, Aepyceros melampus',\n 353: 'gazelle',\n 354: 'Arabian camel, dromedary, Camelus dromedarius',\n 355: 'llama',\n 356: 'weasel',\n 357: 'mink',\n 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n 359: 'black-footed ferret, ferret, Mustela nigripes',\n 360: 'otter',\n 361: 'skunk, polecat, wood pussy',\n 362: 'badger',\n 363: 'armadillo',\n 364: 'three-toed sloth, ai, Bradypus tridactylus',\n 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n 366: 'gorilla, Gorilla gorilla',\n 367: 'chimpanzee, chimp, Pan troglodytes',\n 368: 'gibbon, Hylobates lar',\n 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n 370: 'guenon, guenon monkey',\n 371: 'patas, hussar monkey, Erythrocebus patas',\n 372: 'baboon',\n 373: 'macaque',\n 374: 'langur',\n 375: 'colobus, colobus monkey',\n 376: 'proboscis monkey, Nasalis larvatus',\n 377: 'marmoset',\n 378: 'capuchin, ringtail, Cebus capucinus',\n 379: 'howler monkey, howler',\n 380: 'titi, titi monkey',\n 381: 'spider monkey, Ateles geoffroyi',\n 382: 'squirrel monkey, Saimiri sciureus',\n 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n 385: 'Indian elephant, Elephas maximus',\n 386: 'African elephant, Loxodonta africana',\n 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n 389: 'barracouta, snoek',\n 390: 'eel',\n 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n 392: 'rock beauty, Holocanthus tricolor',\n 393: 'anemone fish',\n 394: 'sturgeon',\n 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n 396: 'lionfish',\n 397: 'puffer, pufferfish, blowfish, globefish',\n 398: 'abacus',\n 399: 'abaya',\n 400: \"academic gown, academic robe, judge's robe\",\n 401: 'accordion, piano accordion, squeeze box',\n 402: 'acoustic guitar',\n 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n 404: 'airliner',\n 405: 'airship, dirigible',\n 406: 'altar',\n 407: 'ambulance',\n 408: 'amphibian, amphibious vehicle',\n 409: 'analog clock',\n 410: 'apiary, bee house',\n 411: 'apron',\n 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n 413: 'assault rifle, assault gun',\n 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n 415: 'bakery, bakeshop, bakehouse',\n 416: 'balance beam, beam',\n 417: 'balloon',\n 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n 419: 'Band Aid',\n 420: 'banjo',\n 421: 'bannister, banister, balustrade, balusters, handrail',\n 422: 'barbell',\n 423: 'barber chair',\n 424: 'barbershop',\n 425: 'barn',\n 426: 'barometer',\n 427: 'barrel, cask',\n 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n 429: 'baseball',\n 430: 'basketball',\n 431: 'bassinet',\n 432: 'bassoon',\n 433: 'bathing cap, swimming cap',\n 434: 'bath towel',\n 435: 'bathtub, bathing tub, bath, tub',\n 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n 437: 'beacon, lighthouse, beacon light, pharos',\n 438: 'beaker',\n 439: 'bearskin, busby, shako',\n 440: 'beer bottle',\n 441: 'beer glass',\n 442: 'bell cote, bell cot',\n 443: 'bib',\n 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n 445: 'bikini, two-piece',\n 446: 'binder, ring-binder',\n 447: 'binoculars, field glasses, opera glasses',\n 448: 'birdhouse',\n 449: 'boathouse',\n 450: 'bobsled, bobsleigh, bob',\n 451: 'bolo tie, bolo, bola tie, bola',\n 452: 'bonnet, poke bonnet',\n 453: 'bookcase',\n 454: 'bookshop, bookstore, bookstall',\n 455: 'bottlecap',\n 456: 'bow',\n 457: 'bow tie, bow-tie, bowtie',\n 458: 'brass, memorial tablet, plaque',\n 459: 'brassiere, bra, bandeau',\n 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n 461: 'breastplate, aegis, egis',\n 462: 'broom',\n 463: 'bucket, pail',\n 464: 'buckle',\n 465: 'bulletproof vest',\n 466: 'bullet train, bullet',\n 467: 'butcher shop, meat market',\n 468: 'cab, hack, taxi, taxicab',\n 469: 'caldron, cauldron',\n 470: 'candle, taper, wax light',\n 471: 'cannon',\n 472: 'canoe',\n 473: 'can opener, tin opener',\n 474: 'cardigan',\n 475: 'car mirror',\n 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n 477: \"carpenter's kit, tool kit\",\n 478: 'carton',\n 479: 'car wheel',\n 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n 481: 'cassette',\n 482: 'cassette player',\n 483: 'castle',\n 484: 'catamaran',\n 485: 'CD player',\n 486: 'cello, violoncello',\n 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n 488: 'chain',\n 489: 'chainlink fence',\n 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n 491: 'chain saw, chainsaw',\n 492: 'chest',\n 493: 'chiffonier, commode',\n 494: 'chime, bell, gong',\n 495: 'china cabinet, china closet',\n 496: 'Christmas stocking',\n 497: 'church, church building',\n 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n 499: 'cleaver, meat cleaver, chopper',\n 500: 'cliff dwelling',\n 501: 'cloak',\n 502: 'clog, geta, patten, sabot',\n 503: 'cocktail shaker',\n 504: 'coffee mug',\n 505: 'coffeepot',\n 506: 'coil, spiral, volute, whorl, helix',\n 507: 'combination lock',\n 508: 'computer keyboard, keypad',\n 509: 'confectionery, confectionary, candy store',\n 510: 'container ship, containership, container vessel',\n 511: 'convertible',\n 512: 'corkscrew, bottle screw',\n 513: 'cornet, horn, trumpet, trump',\n 514: 'cowboy boot',\n 515: 'cowboy hat, ten-gallon hat',\n 516: 'cradle',\n 517: 'crane',\n 518: 'crash helmet',\n 519: 'crate',\n 520: 'crib, cot',\n 521: 'Crock Pot',\n 522: 'croquet ball',\n 523: 'crutch',\n 524: 'cuirass',\n 525: 'dam, dike, dyke',\n 526: 'desk',\n 527: 'desktop computer',\n 528: 'dial telephone, dial phone',\n 529: 'diaper, nappy, napkin',\n 530: 'digital clock',\n 531: 'digital watch',\n 532: 'dining table, board',\n 533: 'dishrag, dishcloth',\n 534: 'dishwasher, dish washer, dishwashing machine',\n 535: 'disk brake, disc brake',\n 536: 'dock, dockage, docking facility',\n 537: 'dogsled, dog sled, dog sleigh',\n 538: 'dome',\n 539: 'doormat, welcome mat',\n 540: 'drilling platform, offshore rig',\n 541: 'drum, membranophone, tympan',\n 542: 'drumstick',\n 543: 'dumbbell',\n 544: 'Dutch oven',\n 545: 'electric fan, blower',\n 546: 'electric guitar',\n 547: 'electric locomotive',\n 548: 'entertainment center',\n 549: 'envelope',\n 550: 'espresso maker',\n 551: 'face powder',\n 552: 'feather boa, boa',\n 553: 'file, file cabinet, filing cabinet',\n 554: 'fireboat',\n 555: 'fire engine, fire truck',\n 556: 'fire screen, fireguard',\n 557: 'flagpole, flagstaff',\n 558: 'flute, transverse flute',\n 559: 'folding chair',\n 560: 'football helmet',\n 561: 'forklift',\n 562: 'fountain',\n 563: 'fountain pen',\n 564: 'four-poster',\n 565: 'freight car',\n 566: 'French horn, horn',\n 567: 'frying pan, frypan, skillet',\n 568: 'fur coat',\n 569: 'garbage truck, dustcart',\n 570: 'gasmask, respirator, gas helmet',\n 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n 572: 'goblet',\n 573: 'go-kart',\n 574: 'golf ball',\n 575: 'golfcart, golf cart',\n 576: 'gondola',\n 577: 'gong, tam-tam',\n 578: 'gown',\n 579: 'grand piano, grand',\n 580: 'greenhouse, nursery, glasshouse',\n 581: 'grille, radiator grille',\n 582: 'grocery store, grocery, food market, market',\n 583: 'guillotine',\n 584: 'hair slide',\n 585: 'hair spray',\n 586: 'half track',\n 587: 'hammer',\n 588: 'hamper',\n 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n 590: 'hand-held computer, hand-held microcomputer',\n 591: 'handkerchief, hankie, hanky, hankey',\n 592: 'hard disc, hard disk, fixed disk',\n 593: 'harmonica, mouth organ, harp, mouth harp',\n 594: 'harp',\n 595: 'harvester, reaper',\n 596: 'hatchet',\n 597: 'holster',\n 598: 'home theater, home theatre',\n 599: 'honeycomb',\n 600: 'hook, claw',\n 601: 'hoopskirt, crinoline',\n 602: 'horizontal bar, high bar',\n 603: 'horse cart, horse-cart',\n 604: 'hourglass',\n 605: 'iPod',\n 606: 'iron, smoothing iron',\n 607: \"jack-o'-lantern\",\n 608: 'jean, blue jean, denim',\n 609: 'jeep, landrover',\n 610: 'jersey, T-shirt, tee shirt',\n 611: 'jigsaw puzzle',\n 612: 'jinrikisha, ricksha, rickshaw',\n 613: 'joystick',\n 614: 'kimono',\n 615: 'knee pad',\n 616: 'knot',\n 617: 'lab coat, laboratory coat',\n 618: 'ladle',\n 619: 'lampshade, lamp shade',\n 620: 'laptop, laptop computer',\n 621: 'lawn mower, mower',\n 622: 'lens cap, lens cover',\n 623: 'letter opener, paper knife, paperknife',\n 624: 'library',\n 625: 'lifeboat',\n 626: 'lighter, light, igniter, ignitor',\n 627: 'limousine, limo',\n 628: 'liner, ocean liner',\n 629: 'lipstick, lip rouge',\n 630: 'Loafer',\n 631: 'lotion',\n 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n 633: \"loupe, jeweler's loupe\",\n 634: 'lumbermill, sawmill',\n 635: 'magnetic compass',\n 636: 'mailbag, postbag',\n 637: 'mailbox, letter box',\n 638: 'maillot',\n 639: 'maillot, tank suit',\n 640: 'manhole cover',\n 641: 'maraca',\n 642: 'marimba, xylophone',\n 643: 'mask',\n 644: 'matchstick',\n 645: 'maypole',\n 646: 'maze, labyrinth',\n 647: 'measuring cup',\n 648: 'medicine chest, medicine cabinet',\n 649: 'megalith, megalithic structure',\n 650: 'microphone, mike',\n 651: 'microwave, microwave oven',\n 652: 'military uniform',\n 653: 'milk can',\n 654: 'minibus',\n 655: 'miniskirt, mini',\n 656: 'minivan',\n 657: 'missile',\n 658: 'mitten',\n 659: 'mixing bowl',\n 660: 'mobile home, manufactured home',\n 661: 'Model T',\n 662: 'modem',\n 663: 'monastery',\n 664: 'monitor',\n 665: 'moped',\n 666: 'mortar',\n 667: 'mortarboard',\n 668: 'mosque',\n 669: 'mosquito net',\n 670: 'motor scooter, scooter',\n 671: 'mountain bike, all-terrain bike, off-roader',\n 672: 'mountain tent',\n 673: 'mouse, computer mouse',\n 674: 'mousetrap',\n 675: 'moving van',\n 676: 'muzzle',\n 677: 'nail',\n 678: 'neck brace',\n 679: 'necklace',\n 680: 'nipple',\n 681: 'notebook, notebook computer',\n 682: 'obelisk',\n 683: 'oboe, hautboy, hautbois',\n 684: 'ocarina, sweet potato',\n 685: 'odometer, hodometer, mileometer, milometer',\n 686: 'oil filter',\n 687: 'organ, pipe organ',\n 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n 689: 'overskirt',\n 690: 'oxcart',\n 691: 'oxygen mask',\n 692: 'packet',\n 693: 'paddle, boat paddle',\n 694: 'paddlewheel, paddle wheel',\n 695: 'padlock',\n 696: 'paintbrush',\n 697: \"pajama, pyjama, pj's, jammies\",\n 698: 'palace',\n 699: 'panpipe, pandean pipe, syrinx',\n 700: 'paper towel',\n 701: 'parachute, chute',\n 702: 'parallel bars, bars',\n 703: 'park bench',\n 704: 'parking meter',\n 705: 'passenger car, coach, carriage',\n 706: 'patio, terrace',\n 707: 'pay-phone, pay-station',\n 708: 'pedestal, plinth, footstall',\n 709: 'pencil box, pencil case',\n 710: 'pencil sharpener',\n 711: 'perfume, essence',\n 712: 'Petri dish',\n 713: 'photocopier',\n 714: 'pick, plectrum, plectron',\n 715: 'pickelhaube',\n 716: 'picket fence, paling',\n 717: 'pickup, pickup truck',\n 718: 'pier',\n 719: 'piggy bank, penny bank',\n 720: 'pill bottle',\n 721: 'pillow',\n 722: 'ping-pong ball',\n 723: 'pinwheel',\n 724: 'pirate, pirate ship',\n 725: 'pitcher, ewer',\n 726: \"plane, carpenter's plane, woodworking plane\",\n 727: 'planetarium',\n 728: 'plastic bag',\n 729: 'plate rack',\n 730: 'plow, plough',\n 731: \"plunger, plumber's helper\",\n 732: 'Polaroid camera, Polaroid Land camera',\n 733: 'pole',\n 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n 735: 'poncho',\n 736: 'pool table, billiard table, snooker table',\n 737: 'pop bottle, soda bottle',\n 738: 'pot, flowerpot',\n 739: \"potter's wheel\",\n 740: 'power drill',\n 741: 'prayer rug, prayer mat',\n 742: 'printer',\n 743: 'prison, prison house',\n 744: 'projectile, missile',\n 745: 'projector',\n 746: 'puck, hockey puck',\n 747: 'punching bag, punch bag, punching ball, punchball',\n 748: 'purse',\n 749: 'quill, quill pen',\n 750: 'quilt, comforter, comfort, puff',\n 751: 'racer, race car, racing car',\n 752: 'racket, racquet',\n 753: 'radiator',\n 754: 'radio, wireless',\n 755: 'radio telescope, radio reflector',\n 756: 'rain barrel',\n 757: 'recreational vehicle, RV, R.V.',\n 758: 'reel',\n 759: 'reflex camera',\n 760: 'refrigerator, icebox',\n 761: 'remote control, remote',\n 762: 'restaurant, eating house, eating place, eatery',\n 763: 'revolver, six-gun, six-shooter',\n 764: 'rifle',\n 765: 'rocking chair, rocker',\n 766: 'rotisserie',\n 767: 'rubber eraser, rubber, pencil eraser',\n 768: 'rugby ball',\n 769: 'rule, ruler',\n 770: 'running shoe',\n 771: 'safe',\n 772: 'safety pin',\n 773: 'saltshaker, salt shaker',\n 774: 'sandal',\n 775: 'sarong',\n 776: 'sax, saxophone',\n 777: 'scabbard',\n 778: 'scale, weighing machine',\n 779: 'school bus',\n 780: 'schooner',\n 781: 'scoreboard',\n 782: 'screen, CRT screen',\n 783: 'screw',\n 784: 'screwdriver',\n 785: 'seat belt, seatbelt',\n 786: 'sewing machine',\n 787: 'shield, buckler',\n 788: 'shoe shop, shoe-shop, shoe store',\n 789: 'shoji',\n 790: 'shopping basket',\n 791: 'shopping cart',\n 792: 'shovel',\n 793: 'shower cap',\n 794: 'shower curtain',\n 795: 'ski',\n 796: 'ski mask',\n 797: 'sleeping bag',\n 798: 'slide rule, slipstick',\n 799: 'sliding door',\n 800: 'slot, one-armed bandit',\n 801: 'snorkel',\n 802: 'snowmobile',\n 803: 'snowplow, snowplough',\n 804: 'soap dispenser',\n 805: 'soccer ball',\n 806: 'sock',\n 807: 'solar dish, solar collector, solar furnace',\n 808: 'sombrero',\n 809: 'soup bowl',\n 810: 'space bar',\n 811: 'space heater',\n 812: 'space shuttle',\n 813: 'spatula',\n 814: 'speedboat',\n 815: \"spider web, spider's web\",\n 816: 'spindle',\n 817: 'sports car, sport car',\n 818: 'spotlight, spot',\n 819: 'stage',\n 820: 'steam locomotive',\n 821: 'steel arch bridge',\n 822: 'steel drum',\n 823: 'stethoscope',\n 824: 'stole',\n 825: 'stone wall',\n 826: 'stopwatch, stop watch',\n 827: 'stove',\n 828: 'strainer',\n 829: 'streetcar, tram, tramcar, trolley, trolley car',\n 830: 'stretcher',\n 831: 'studio couch, day bed',\n 832: 'stupa, tope',\n 833: 'submarine, pigboat, sub, U-boat',\n 834: 'suit, suit of clothes',\n 835: 'sundial',\n 836: 'sunglass',\n 837: 'sunglasses, dark glasses, shades',\n 838: 'sunscreen, sunblock, sun blocker',\n 839: 'suspension bridge',\n 840: 'swab, swob, mop',\n 841: 'sweatshirt',\n 842: 'swimming trunks, bathing trunks',\n 843: 'swing',\n 844: 'switch, electric switch, electrical switch',\n 845: 'syringe',\n 846: 'table lamp',\n 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n 848: 'tape player',\n 849: 'teapot',\n 850: 'teddy, teddy bear',\n 851: 'television, television system',\n 852: 'tennis ball',\n 853: 'thatch, thatched roof',\n 854: 'theater curtain, theatre curtain',\n 855: 'thimble',\n 856: 'thresher, thrasher, threshing machine',\n 857: 'throne',\n 858: 'tile roof',\n 859: 'toaster',\n 860: 'tobacco shop, tobacconist shop, tobacconist',\n 861: 'toilet seat',\n 862: 'torch',\n 863: 'totem pole',\n 864: 'tow truck, tow car, wrecker',\n 865: 'toyshop',\n 866: 'tractor',\n 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n 868: 'tray',\n 869: 'trench coat',\n 870: 'tricycle, trike, velocipede',\n 871: 'trimaran',\n 872: 'tripod',\n 873: 'triumphal arch',\n 874: 'trolleybus, trolley coach, trackless trolley',\n 875: 'trombone',\n 876: 'tub, vat',\n 877: 'turnstile',\n 878: 'typewriter keyboard',\n 879: 'umbrella',\n 880: 'unicycle, monocycle',\n 881: 'upright, upright piano',\n 882: 'vacuum, vacuum cleaner',\n 883: 'vase',\n 884: 'vault',\n 885: 'velvet',\n 886: 'vending machine',\n 887: 'vestment',\n 888: 'viaduct',\n 889: 'violin, fiddle',\n 890: 'volleyball',\n 891: 'waffle iron',\n 892: 'wall clock',\n 893: 'wallet, billfold, notecase, pocketbook',\n 894: 'wardrobe, closet, press',\n 895: 'warplane, military plane',\n 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n 897: 'washer, automatic washer, washing machine',\n 898: 'water bottle',\n 899: 'water jug',\n 900: 'water tower',\n 901: 'whiskey jug',\n 902: 'whistle',\n 903: 'wig',\n 904: 'window screen',\n 905: 'window shade',\n 906: 'Windsor tie',\n 907: 'wine bottle',\n 908: 'wing',\n 909: 'wok',\n 910: 'wooden spoon',\n 911: 'wool, woolen, woollen',\n 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n 913: 'wreck',\n 914: 'yawl',\n 915: 'yurt',\n 916: 'web site, website, internet site, site',\n 917: 'comic book',\n 918: 'crossword puzzle, crossword',\n 919: 'street sign',\n 920: 'traffic light, traffic signal, stoplight',\n 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n 922: 'menu',\n 923: 'plate',\n 924: 'guacamole',\n 925: 'consomme',\n 926: 'hot pot, hotpot',\n 927: 'trifle',\n 928: 'ice cream, icecream',\n 929: 'ice lolly, lolly, lollipop, popsicle',\n 930: 'French loaf',\n 931: 'bagel, beigel',\n 932: 'pretzel',\n 933: 'cheeseburger',\n 934: 'hotdog, hot dog, red hot',\n 935: 'mashed potato',\n 936: 'head cabbage',\n 937: 'broccoli',\n 938: 'cauliflower',\n 939: 'zucchini, courgette',\n 940: 'spaghetti squash',\n 941: 'acorn squash',\n 942: 'butternut squash',\n 943: 'cucumber, cuke',\n 944: 'artichoke, globe artichoke',\n 945: 'bell pepper',\n 946: 'cardoon',\n 947: 'mushroom',\n 948: 'Granny Smith',\n 949: 'strawberry',\n 950: 'orange',\n 951: 'lemon',\n 952: 'fig',\n 953: 'pineapple, ananas',\n 954: 'banana',\n 955: 'jackfruit, jak, jack',\n 956: 'custard apple',\n 957: 'pomegranate',\n 958: 'hay',\n 959: 'carbonara',\n 960: 'chocolate sauce, chocolate syrup',\n 961: 'dough',\n 962: 'meat loaf, meatloaf',\n 963: 'pizza, pizza pie',\n 964: 'potpie',\n 965: 'burrito',\n 966: 'red wine',\n 967: 'espresso',\n 968: 'cup',\n 969: 'eggnog',\n 970: 'alp',\n 971: 'bubble',\n 972: 'cliff, drop, drop-off',\n 973: 'coral reef',\n 974: 'geyser',\n 975: 'lakeside, lakeshore',\n 976: 'promontory, headland, head, foreland',\n 977: 'sandbar, sand bar',\n 978: 'seashore, coast, seacoast, sea-coast',\n 979: 'valley, vale',\n 980: 'volcano',\n 981: 'ballplayer, baseball player',\n 982: 'groom, bridegroom',\n 983: 'scuba diver',\n 984: 'rapeseed',\n 985: 'daisy',\n 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n 987: 'corn',\n 988: 'acorn',\n 989: 'hip, rose hip, rosehip',\n 990: 'buckeye, horse chestnut, conker',\n 991: 'coral fungus',\n 992: 'agaric',\n 993: 'gyromitra',\n 994: 'stinkhorn, carrion fungus',\n 995: 'earthstar',\n 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n 997: 'bolete',\n 998: 'ear, spike, capitulum',\n 999: 'toilet tissue, toilet paper, bathroom tissue'}","2a89c651":"# @title Map Imagenette Labels to Imagenet Labels\ndir_to_imagenet_index = {\n    'n03888257': 1,\n    'n03425413': 571,\n    'n03394916': 566,\n    'n03000684': 491,\n    'n02102040': 217,\n    'n03445777': 574,\n    'n03417042': 569,\n    'n03028079': 497,\n    'n02979186': 482,\n    'n01440764': 701\n    }\n\ndir_index_to_imagenet_label = {}\nordered_dirs = sorted(list(dir_to_imagenet_index.keys()))\n\nfor dir_index, dir_name in enumerate(ordered_dirs):\n  dir_index_to_imagenet_label[dir_index] = dir_to_imagenet_index[dir_name]","3d34d533":"# @title Prepare Imagenette Data\nval_transform = transforms.Compose((transforms.Resize((256, 256)),\n                                    transforms.ToTensor()))\n\nimagenette_val = ImageFolder('imagenette2-320\/val', transform=val_transform)\n\ntrain_transform = transforms.Compose((transforms.Resize((256, 256)),\n                                      transforms.ToTensor()))\n\nimagenette_train = ImageFolder('imagenette2-320\/train',\n                               transform=train_transform)\nrandom.seed(SEED)\nrandom_indices = random.sample(range(len(imagenette_train)), 400)\nimagenette_train_subset = torch.utils.data.Subset(imagenette_train,\n                                                  random_indices)\n\n\n\n\n# Subset to only one tenth of the data for faster runtime\nrandom_indices = random.sample(range(len(imagenette_val)), int(len(imagenette_val) * .1))\nimagenette_val = torch.utils.data.Subset(imagenette_val, random_indices)","f7c90daa":"# To preserve reproducibility\ng_seed = torch.Generator()\ng_seed.manual_seed(SEED)\n\nimagenette_train_loader = torch.utils.data.DataLoader(imagenette_train_subset,\n                                                      batch_size=16,\n                                                      shuffle=True,\n                                                      num_workers=2,\n                                                      worker_init_fn=seed_worker,\n                                                      generator=g_seed\n                                                      )\n\nimagenette_val_loader = torch.utils.data.DataLoader(imagenette_val,\n                                                    batch_size=16,\n                                                    shuffle=False,\n                                                    num_workers=2,\n                                                    worker_init_fn=seed_worker,\n                                                    generator=g_seed)\n\ndataiter = iter(imagenette_val_loader)\nimages, labels = dataiter.next()\n\n# show images\nplt.figure(figsize=(8, 8))\nplt.imshow(make_grid(images, nrow=4).permute(1, 2, 0))\nplt.axis('off')\nplt.show()","d000735e":"# @title eval_imagenette function\ndef eval_imagenette(resnet, data_loader, dataset_length, device):\n  resnet.eval()\n  with torch.no_grad():\n    loss_sum = 0\n    total_1_correct = 0\n    total_5_correct = 0\n    total = dataset_length\n    for batch in tqdm.notebook.tqdm(data_loader):\n      images, labels = batch\n\n      # Map the imagenette labels onto the network's output\n      for i, label in enumerate(labels):\n          labels[i] = dir_index_to_imagenet_label[label.item()]\n\n      images = images.to(device)\n      labels = labels.to(device)\n      output = resnet(images)\n\n      # Calculate top-5 accuracy\n      # Implementation from https:\/\/github.com\/bearpaw\/pytorch-classification\/blob\/cc9106d598ff1fe375cc030873ceacfea0499d77\/utils\/eval.py\n      batch_size = labels.size(0)\n\n      _, predictions = output.topk(5, 1, True, True)\n      predictions = predictions.t()\n\n      top_k_correct = predictions.eq(labels.view(1, -1).expand_as(predictions))\n      top_k_correct = top_k_correct.sum()\n\n      predictions = torch.argmax(output, dim=1)\n      top_1_correct = torch.sum(predictions == labels)\n      total_1_correct += top_1_correct\n      total_5_correct += top_k_correct\n\n    top_1_acc = total_1_correct \/ total\n    top_5_acc = total_5_correct \/ total\n\n    return top_1_acc, top_5_acc","63cac8de":"# @title Imagenette Train Loop\n\ndef imagenette_train_loop(model, optimizer, train_loader, loss_fn, device):\n  for epoch in tqdm.notebook.tqdm(range(5)):\n    # Set model to use the imagenette classifier head\n    model.train()\n    # Train on a batch of images\n    for imagenette_batch in train_loader:\n      images, labels = imagenette_batch\n\n      # Convert labels from imagenette indices to imagenet labels\n      for i, label in enumerate(labels):\n        labels[i] = dir_index_to_imagenet_label[label.item()]\n\n      images = images.to(device)\n      labels = labels.to(device)\n      output = model(images)\n      optimizer.zero_grad()\n      loss = loss_fn(output, labels)\n      loss.backward()\n      optimizer.step()\n\n  return model","ba076919":"# Original network\ntop_1_accuracies = []\ntop_5_accuracies = []\n\n# Instantiate a pretrained resnet model\nset_seed(seed=SEED)\nresnet = torchvision.models.resnet18(pretrained=True).to(DEVICE)\nresnet_opt = torch.optim.Adam(resnet.parameters(), lr=1e-4)\nloss_fn = nn.CrossEntropyLoss()\n\nimagenette_train_loop(resnet,\n                      resnet_opt,\n                      imagenette_train_loader,\n                      loss_fn,\n                      device=DEVICE)\n\ntop_1_acc, top_5_acc = eval_imagenette(resnet,\n                                       imagenette_val_loader,\n                                       len(imagenette_val),\n                                       device=DEVICE)\ntop_1_accuracies.append(top_1_acc.item())\ntop_5_accuracies.append(top_5_acc.item())","b74b62d9":"def predict_top5(images, device, seed):\n  \"\"\"\n  Args:\n    images: torch Tensor with dimensionality B x C x H x W\n      (batch size x number of channels x height x width)\n    device: STRING\n      `cuda` if GPU is available, else `cpu`.\n  Output:\n      top5_probs: torch Tensor (B, 5) with top 5 class probabilities\n      top5_names: list of top 5 class names (B, 5)\n  \"\"\"\n  ####################################################################\n  # Fill in all missing code below (...),\n  # then remove or comment the line below to test your function\n  # raise NotImplementedError(\"Predict top 5\")\n  ####################################################################\n  set_seed(seed=seed)\n\n  B = images.size(0)\n  with torch.no_grad():\n    # Run images through model\n    images = images.to(device) # attach image to device\n    output = resnet(images)\n    # The model output is unnormalized. To get probabilities, run a softmax on it.\n    probs = torch.nn.functional.softmax(output, dim=1) # F.softmax(output)\n    # Fetch output from GPU and convert to numpy array\n    probs = probs.cpu().detach().numpy()\n\n  # Get top 5 predictions\n  _, top5_idcs = output.topk(5, 1, True, True)\n  top5_idcs = top5_idcs.t().cpu().numpy()\n  top5_probs = probs[torch.arange(B), top5_idcs]\n\n  # Convert indices to class names\n  top5_names = []\n  for b in range(B):\n    temp = [dict_map[key].split(',')[0] for key in top5_idcs[:, b]]\n    top5_names.append(temp)\n\n  return top5_names, top5_probs\n\n\n# add event to airtable\natform.add_event('Coding Exercise 4.1: Use the ResNet model')\n\n# get batch of images\ndataiter = iter(imagenette_val_loader)\nimages, labels = dataiter.next()\n\n## Uncomment to test your function and retrieve top 5 predictions\ntop5_names, top5_probs = predict_top5(images, DEVICE, SEED)\nprint(top5_names[1])","12068333":"# visualize probabilities of top 5 predictions\nfig, ax = plt.subplots(5, 2, figsize=(10, 20))\n\nfor i in range(5):\n  ax[i, 0].imshow(np.moveaxis(images[i].numpy(), 0, -1))\n  ax[i, 0].axis('off')\n\n  ax[i, 1].bar(np.arange(5), top5_probs[:, i])\n  ax[i, 1].set_xticks(np.arange(5))\n  ax[i, 1].set_xticklabels(top5_names[i], rotation=30)\n  ax[i,1].set_ylim(0,1)\n\nfig.tight_layout()\nplt.show()","2b52cf4c":"loc = 'https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W2D2_ModernConvnets\/static\/'\n\nfname1 = 'bonsai-svg-5.png'\nresponse = requests.get(loc + fname1)\nimage = Image.open(BytesIO(response.content)).resize((256, 256))\ndata = torch.from_numpy(np.asarray(image)[:, :, :3]) \/ 255.\n\nfname2 = 'Pok\u00e9mon_Pikachu_art.png'\nresponse = requests.get(loc + fname2)\nimage = Image.open(BytesIO(response.content)).resize((256, 256))\ndata2 = torch.from_numpy(np.asarray(image)[:, :, :3]) \/ 255.\n\nfname3 = 'https:\/\/simpsons.fandom.com\/de\/wiki\/Bart_Simpson?file=Bart_Simpson.png'\nresponse = requests.get(fname3)\nimage = Image.open(BytesIO(response.content)).resize((256, 256))\ndata3 = torch.from_numpy(np.asarray(image)[:, :, :3]) \/ 255.\n\nimages = torch.stack([data, data2, data3]).permute(0, 3, 1, 2)","3405a11c":"# retrieve top 5 predictions\ntop5_names, top5_probs  = predict_top5(images, DEVICE, SEED)","cb3051cd":"# visualize probabilities of top 5 predictions\nfig, ax = plt.subplots(2, 2, figsize=(10, 10))\n\nfor i in range(2):\n  ax[i, 0].imshow(np.moveaxis(images[i].numpy(), 0, -1))\n  ax[i, 0].axis('off')\n\n  ax[i, 1].bar(np.arange(5), top5_probs[:, i])\n  ax[i, 1].set_xticks(np.arange(5))\n  ax[i, 1].set_xticklabels(top5_names[i], rotation=30)\n  ax[i,1].set_ylim(0,1)\n\nfig.tight_layout()\nplt.show()","b80b522e":"# @title Video 5: Improving efficiency: Inceptrion and ResNeXt\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1Zq4y1W7Px\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"TDHn7X1wNQ4\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 5: Improving efficiency: Inceptrion and ResNeXt')\n\ndisplay(out)","76565de9":"# @title Parameter Calculator\n# @markdown Run this cell to enable the widget\nfrom IPython.display import display as dis\n\ndef calculate_parameters_resnet(d_in, resnet_channels):\n    # ResNet math: Implement how parameters scale\n    d_out = d_in\n    resnet_parameters = d_in*resnet_channels + 3*3*resnet_channels*resnet_channels + resnet_channels*d_out\n\n    print('ResNet parameters: {}'.format(resnet_parameters))\n    return None\n\n\ndef calculate_parameters_resnext(d_in, resnext_channels, num_paths):\n    # ResNet math: Implement how parameters scale\n    d_out = d_in\n    d = resnext_channels\n\n    resnext_parameters = (d_in*d + 3*3*d*d + d*d_out)*num_paths\n\n    print('ResNeXt parameters: {}'.format(resnext_parameters))\n    return None\n\n\nlabels = ['ResNet', 'ResNeXt']\ndescriptions_resnet = ['Channels in+out', 'Bottleneck channels']\ndescriptions_resnext = ['Channels in+out', 'Bottleneck channels',\n                        'Number of paths (cardinality)']\nlbox_resnet = widgets.VBox([widgets.Label(description) for description in descriptions_resnet])\nlbox_resnext = widgets.VBox([widgets.Label(description) for description in descriptions_resnext])\n\nd_in = widgets.FloatLogSlider(\n    value=256,\n    base=2,\n    min=1, # max exponent of base\n    max=10, # min exponent of base\n    step=1, # exponent step\n)\nresnet_channels = widgets.FloatLogSlider(\n    value=64,\n    base=2,\n    min=5, # max exponent of base\n    max=10, # min exponent of base\n    step=1, # exponent step\n)\nresnext_channels = widgets.FloatLogSlider(\n    value=4,\n    base=2,\n    min=1, # max exponent of base\n    max=10, # min exponent of base\n    step=1, # exponent step\n)\nnum_paths = widgets.FloatLogSlider(\n    value=32,\n    base=2,\n    min=0, # max exponent of base\n    max=7, # min exponent of base\n    step=1, # exponent step\n)\n\nrbox_resnet = widgets.VBox([d_in, resnet_channels])\nrbox_resnext = widgets.VBox([d_in, resnext_channels, num_paths])\nui_resnet = widgets.HBox([lbox_resnet, rbox_resnet])\nui_resnet_labeled = widgets.VBox(\n    [widgets.HTML(value=\"<b>\" + labels[0] + \"<\/b>\"), ui_resnet],\n    layout=widgets.Layout(border='1px solid black'))\nui_resnext = widgets.HBox([lbox_resnext, rbox_resnext])\nui_resnext_labeled = widgets.VBox(\n    [widgets.HTML(value=\"<b>\" + labels[1] + \"<\/b>\"), ui_resnext],\n    layout=widgets.Layout(border='1px solid black'))\nui = widgets.VBox([ui_resnet_labeled, ui_resnext_labeled])\n\nout_resnet = widgets.interactive_output(calculate_parameters_resnet,\n                     {'d_in':d_in,\n                     'resnet_channels':resnet_channels})\n\nout_resnext = widgets.interactive_output(calculate_parameters_resnext,\n                     {'d_in':d_in,\n                     'resnext_channels':resnext_channels,\n                     'num_paths':num_paths})\n\nd1 = dis(ui, out_resnet, out_resnext)","78da4f29":"# @title Student Response\nfrom ipywidgets import widgets\n\n\ntext=widgets.Textarea(\n   value='Type your answer here and click on `Submit!`',\n   placeholder='Type something',\n   description='',\n   disabled=False\n)\n\nbutton = widgets.Button(description=\"Submit!\")\n\ndisplay(text,button)\n\ndef on_button_clicked(b):\n   atform.add_answer('q4', text.value)\n   print(\"Submission successful!\")\n\n\nbutton.on_click(on_button_clicked)","999b9d29":"# @title Video 6: Improving efficiency: MobileNet\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1D44y127fS\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"kdbGpn1JfmU\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 6: Improving efficiency: MobileNet')\n\ndisplay(out)","caa9f90c":"def convolution_math(in_channels, filter_size, out_channels):\n  \"\"\"\n  Convolution math: Implement how parameters scale as a function of feature maps\n  and filter size in convolution vs depthwise separable convolution.\n\n  Args:\n    in_channels : number of input channels\n    filter_size : size of the filter\n    out_channels : number of output channels\n  \"\"\"\n  ####################################################################\n  # Fill in all missing code below (...),\n  # then remove or comment the line below to test your function\n  # raise NotImplementedError(\"Convolution math\")\n  ####################################################################\n  # calculate the number of parameters for regular convolution\n  conv_parameters = in_channels * filter_size * filter_size * out_channels\n  # calculate the number of parameters for depthwise separable convolution\n  depthwise_conv_parameters = in_channels * filter_size * filter_size + in_channels * out_channels\n\n  print(f\"Depthwise separable: {depthwise_conv_parameters} parameters\")\n  print(f\"Regular convolution: {conv_parameters} parameters\")\n\n  return None\n\n\n# add event to airtable\natform.add_event('Coding Exercise 6.1: Calculation of parameters')\n\n## Uncomment to test your function\nconvolution_math(in_channels=4, filter_size=3, out_channels=2)","a52d3737":"# it doesn not matter how many input features.\nconvolution_math(in_channels=64, filter_size=3, out_channels=2)","a7ce725c":"# @title Student Response\nfrom ipywidgets import widgets\n\n\ntext=widgets.Textarea(\n   value='Type your answer here and click on `Submit!`',\n   placeholder='Type something',\n   description='',\n   disabled=False\n)\n\nbutton = widgets.Button(description=\"Submit!\")\n\ndisplay(text,button)\n\ndef on_button_clicked(b):\n   atform.add_answer('q5', text.value)\n   print(\"Submission successful!\")\n\n\nbutton.on_click(on_button_clicked)","db6bc19f":"# @title Video 7: Transfer Learning\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1z54y1E714\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"Qr5l-an5ac4\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 7: Transfer Learning')\n\ndisplay(out)","048f5368":"# @title Download Data\nimport zipfile, io\n\n# original link: https:\/\/github.com\/ben-heil\/cis_522_data.git\nurl = 'https:\/\/osf.io\/u4njm\/download'\n\nfname = 'small_pokemon_dataset'\n\nif not os.path.exists(fname+'zip'):\n  print(\"Data is being downloaded...\")\n  r = requests.get(url, stream=True)\n  z = zipfile.ZipFile(io.BytesIO(r.content))\n  z.extractall()\n  print(\"The download has been completed.\")\nelse:\n  print(\"Data has already been downloaded.\")","5411628f":"# List the different Pokemon\nos.listdir(\"small_pokemon_dataset\/\")","c73c245f":"# @title Determine number of classes\nnum_classes = 0\nfor folders in os.listdir('small_pokemon_dataset\/'):\n  num_classes += 1\nprint(f\"{num_classes} types of Pokemon\")","c8591ceb":"# @title Display Example Images\ntrain_transform = transforms.Compose((transforms.Resize((256, 256)),\n                                      transforms.ToTensor()))\n\npokemon_dataset = ImageFolder('small_pokemon_dataset',\n                              transform=train_transform)\n\nimage_count = len(pokemon_dataset)\ntrain_indices = []\ntest_indices = []\nfor i in range(image_count):\n  # Put ten percent of the images in the test set\n  if random.random() < .1:\n    test_indices.append(i)\n  else:\n    train_indices.append(i)\n\npokemon_test_set = torch.utils.data.Subset(pokemon_dataset, test_indices)\npokemon_train_set = torch.utils.data.Subset(pokemon_dataset, train_indices)\n\npokemon_train_loader = torch.utils.data.DataLoader(pokemon_train_set,\n                                                   batch_size=16,\n                                                   shuffle=True,)\npokemon_test_loader = torch.utils.data.DataLoader(pokemon_test_set,\n                                                  batch_size=16)\n\ndataiter = iter(pokemon_train_loader)\nimages, labels = dataiter.next()\n\n# show images\nplt.imshow(make_grid(images, nrow=4).permute(1, 2, 0))\nplt.axis('off')\nplt.show()","dd3237da":"resnet = torchvision.models.resnet18(pretrained=True)\nnum_ftrs = resnet.fc.in_features\n# reset final fully connected layer, number of classes = types of Pokemon = 9\nresnet.fc = nn.Linear(num_ftrs, num_classes)\nresnet.to(DEVICE)\noptimizer = torch.optim.Adam(resnet.parameters(), lr=1e-4)\nloss_fn = nn.CrossEntropyLoss()","3dadddf2":"# @title Finetune ResNet\n\npretrained_accs = []\nfor epoch in tqdm.tqdm(range(10)):\n  # Train loop\n  for batch in pokemon_train_loader:\n    images, labels = batch\n    images = images.to(DEVICE)\n    labels = labels.to(DEVICE)\n\n    optimizer.zero_grad()\n    output = resnet(images)\n    loss = loss_fn(output, labels)\n    loss.backward()\n    optimizer.step()\n\n  # Eval loop\n  with torch.no_grad():\n    loss_sum = 0\n    total_correct = 0\n    total = len(pokemon_test_set)\n    for batch in pokemon_test_loader:\n      images, labels = batch\n      images = images.to(DEVICE)\n      labels = labels.to(DEVICE)\n      output = resnet(images)\n      loss = loss_fn(output, labels)\n      loss_sum += loss.item()\n\n      predictions = torch.argmax(output, dim=1)\n\n      num_correct = torch.sum(predictions == labels)\n      total_correct += num_correct\n\n    # Plot accuracy\n    pretrained_accs.append(total_correct \/ total)\n    plt.plot(pretrained_accs)\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Pokemon prediction accuracy')\n    IPython.display.clear_output(wait=True)\n    IPython.display.display(plt.gcf())\n  plt.close()","4af59fed":"resnet = torchvision.models.resnet18(pretrained=True)\nfor param in resnet.parameters():\n  param.requires_grad = False\nnum_ftrs = resnet.fc.in_features\n# reset final fully connected layer\nresnet.fc = nn.Linear(num_ftrs, num_classes)\nresnet.to(DEVICE)\noptimizer = torch.optim.Adam(resnet.fc.parameters(), lr=1e-2)\nloss_fn = nn.CrossEntropyLoss()","0008bfee":"# @title Finetune readout of ResNet\nlinreadout_accs = []\nfor epoch in range(10):\n  # Train loop\n  for batch in pokemon_train_loader:\n    images, labels = batch\n    images = images.to(DEVICE)\n    labels = labels.to(DEVICE)\n\n    optimizer.zero_grad()\n    output = resnet(images)\n    loss = loss_fn(output, labels)\n    loss.backward()\n    optimizer.step()\n\n  # Eval loop\n  with torch.no_grad():\n    loss_sum = 0\n    total_correct = 0\n    total = len(pokemon_test_set)\n    for batch in pokemon_test_loader:\n      images, labels = batch\n      images = images.to(DEVICE)\n      labels = labels.to(DEVICE)\n      output = resnet(images)\n      loss = loss_fn(output, labels)\n      loss_sum += loss.item()\n\n      predictions = torch.argmax(output, dim=1)\n\n      num_correct = torch.sum(predictions == labels)\n      total_correct += num_correct\n\n    # Plot accuracy\n    linreadout_accs.append(total_correct \/ total)\n    plt.plot(linreadout_accs)\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Pokemon prediction accuracy')\n    IPython.display.clear_output(wait=True)\n    IPython.display.display(plt.gcf())\n  plt.close()","7511c2cc":"resnet = torchvision.models.resnet18(pretrained=False)\nnum_ftrs = resnet.fc.in_features\n# reset final fully connected layer\nresnet.fc = nn.Linear(num_ftrs, num_classes)\nresnet.to(DEVICE)\noptimizer = torch.optim.Adam(resnet.parameters(), lr=1e-4)\n\nloss_fn = nn.CrossEntropyLoss()","2d81adc4":"# @title Train ResNet from scratch\nscratch_accs = []\nfor epoch in tqdm.tqdm(range(10)):\n  # Train loop\n  for batch in pokemon_train_loader:\n    images, labels = batch\n    images = images.to(DEVICE)\n    labels = labels.to(DEVICE)\n\n    optimizer.zero_grad()\n    output = resnet(images)\n    loss = loss_fn(output, labels)\n    loss.backward()\n    optimizer.step()\n\n  # Eval loop\n  with torch.no_grad():\n    loss_sum = 0\n    total_correct = 0\n    total = len(pokemon_test_set)\n    for batch in pokemon_test_loader:\n      images, labels = batch\n      images = images.to(DEVICE)\n      labels = labels.to(DEVICE)\n      output = resnet(images)\n      loss = loss_fn(output, labels)\n      loss_sum += loss.item()\n\n      predictions = torch.argmax(output, dim=1)\n\n      num_correct = torch.sum(predictions == labels)\n      total_correct += num_correct\n\n    scratch_accs.append(total_correct \/ total)\n    plt.plot(scratch_accs)\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Pokemon prediction accuracy')\n\n    IPython.display.clear_output(wait=True)\n    IPython.display.display(plt.gcf())\n  plt.close()","4a9f7267":"# @title Plot Accuracies\nplt.plot(pretrained_accs, label='Pretrained: fine-tuning')\nplt.plot(linreadout_accs, label='Pretrained: linear Readout')\nplt.plot(scratch_accs, label='Trained from Scratch')\nplt.title('Pokemon prediction accuracy')\nplt.legend()\nplt.show()","8fa243e4":"# @title Video 8: Summary and Outlook\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1So4y1D7Ev\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"sjj0-7i6XfE\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 8: Summary and Outlook')\n\ndisplay(out)","1de19822":"# @title Airtable Submission Link\nfrom IPython import display as IPydisplay\nIPydisplay.HTML(\n   f\"\"\"\n <div>\n   <a href= \"{atform.url()}\" target=\"_blank\">\n   <img src=\"https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/blob\/main\/tutorials\/static\/SurveyButton.png?raw=1\"\n alt=\"button link end of day Survey\" style=\"width:410px\"><\/a>\n   <\/div>\"\"\" )","eab6cbb2":"# @title Video 9: Speed-accuracy trade-off\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1v64y1z7PT\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"9p4gD-QnbIQ\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 8: Speed-accuracy trade-off')\n\ndisplay(out)","abe22a23":"# load three pretrained models from torchvision.models\n# [these are just examples, other models are possible as well]\nmodel1 = ...\nmodel2 = ...\nmodel3 = ...\n\nmodels = {'...': model1, '...': model2, '...-19': model3}\nlearning_rates = [1e-4, 1e-4, 1e-4]\n\ntimes, top_1_accuracies = [], []","11c86983":"# @title Imagenette Train Loop\ndef train_loop(model, optimizer, train_loader, loss_fn, device):\n\n  times = []\n  model.to(device)\n  for epoch in tqdm.notebook.tqdm(range(5)):\n    model.train()\n    t_start = time.time()\n    # Train on a batch of images\n    for imagenette_batch in train_loader:\n      images, labels = imagenette_batch\n\n      # Convert labels from imagenette indices to imagenet labels\n      for i, label in enumerate(labels):\n        labels[i] = dir_index_to_imagenet_label[label.item()]\n\n      images = images.to(device)\n      labels = labels.to(device)\n      output = model(images)\n      optimizer.zero_grad()\n      loss = loss_fn(output, labels)\n      loss.backward()\n      optimizer.step()\n      if torch.cuda.is_available():\n        torch.cuda.synchronize()\n\n      times += [time.time() - t_start]\n\n  return np.mean(times)","88f6fa30":"for (name, model), lr in zip(models.items(), learning_rates):\n\n  print(name, lr)\n\n  model.to(DEVICE)\n  model.aux_logits = False  # only important for googlenet\n\n  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n  loss_fn = nn.CrossEntropyLoss()\n\n  model_time = train_loop(model, optimizer, imagenette_train_loader, loss_fn,\n                          DEVICE)\n  times.append(model_time)\n\n  top_1_acc, _ = eval_imagenette(model, imagenette_val_loader,\n                                 len(imagenette_val), device=DEVICE)\n  top_1_accuracies.append(top_1_acc.item())","fd911cc9":"# @title Plot accuracies vs. training speed\ndef get_parameter_count(model):\n  return sum([torch.numel(p) for p in model.parameters()])\n\n\ndef plot_acc_speed(times, accs, models):\n  ti = [t*1000 for t in times]\n  for i, model in enumerate(list(models.keys())):\n    scale = get_parameter_count(models[model])*1e-6\n    plt.scatter(ti[i], accs[i], s=scale, label=model)\n  plt.grid(True)\n  plt.xlabel('speed [ms]')\n  plt.ylabel('accuracy')\n  plt.title('Accuracy vs. speed')\n  plt.legend()\n\n\nplot_acc_speed(times, top_1_accuracies, models)","0ee38bb6":"###  Train ResNet from scratch\n","db4dfb78":"## Section 3.1: Introduction to AlexNet\n\nAlexNet arguably marked the start of the current age of deep learning.\nIt incorporates a number of the defining characteristics of successful DL today: deep networks, GPU-powered paralellization, and building blocks encoding task-specific priors.\nIn this section you'll have the opportunity to play with AlexNet and see the world through its eyes.","815625e9":"<a href=\"https:\/\/colab.research.google.com\/github\/NeuromatchAcademy\/course-content-dl\/blob\/main\/tutorials\/W2D2_ModernConvnets\/student\/W2D2_Tutorial1.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\"\/><\/a>","94cbd1a8":"##  Tutorial slides\n","3c6d0e7b":"## Section 7.1: Download and prepare the data","c180e406":"###  Imagenette Train Loop\n","fd3c19ee":"## Section 7.5: Head to Head Comparison\nStarting from a randomly initialized network works less well, especially in the case of small datsets. Note that the model converges more slowly and less evenly.","929378a8":"---\n# Tutorial Objectives\n\nIn this tutorial we are going to learn more about Convnets. More specifically, we will:\n\n1. Learn about modern CNNs and Transfer Learning.\n2. Understand how architectures incorporate ideas we have about the world.\n3. Understand the operating principles underlying the basic building blocks of modern CNNs.\n4. Understand the concept of transfer learning and learn to recognize opportunities for applying it.\n5. (Bonus) Understand the speed vs. accuracy trade-off.","2f63e5c6":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D2_ModernConvnets\/solutions\/W2D2_Tutorial1_Solution_8cc92376.py)\n\n","e06dfba0":"##  Video 1: Modern CNNs and Transfer Learning\n","e1ab031c":"As the models got larger and the number of connections increased so did the computational costs involved. In the modern era of image processing, there is a tradeoff between model performance and computational cost. Models can reach extremely high performance on many problems, but achieving state of the art results requires [huge amounts of compute power](https:\/\/arxiv.org\/pdf\/1810.00736.pdf).\n\n\n<img src=\"https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W2D2_ModernConvnets\/static\/compute_vs_performance.png\">","e0a6fb74":"##  eval_imagenette function\n","618793b9":"In this section we'll be working with a state of the art CNN model called [ResNet](https:\/\/arxiv.org\/abs\/1512.03385). ResNet has two particularly interesting features. First, it uses skip connections to avoid the vanishing gradient problem. Second, each block (collection of layers) in a ResNet can be treated as learning a residual function.\n\nMathematically, a neural network can be thought of as a series of operations that maps an input (like an image of a dog) to an output (like the label \"dog\"). In math-speak a mapping from an input to an output is called a function. Neural networks are a flexible way of expressing that function. \n\nIf you were to subtract out the true function mapping images to class labels from the function learned by a network, you'd be left with the residual error or \"residual function\". ResNets try to learn the original function, then the residual function, then the residual of the residual, and so on, using their residual blocks and adding them to the output of the preceeding layers.\n\nIn this section we'll run several images through a pre-trained ResNet and see what happens.","74a49e1e":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D2_ModernConvnets\/solutions\/W2D2_Tutorial1_Solution_4554ddef.py)\n\n","69572ffa":"###  Finetune ResNet\n","0fd67c43":"---\n# Section 7: Transfer Learning\n\n*Time estimate: ~24mins*","cf261c43":"---\n# Section 6: Depthwise separable convolutions\n\n*Time estimate: ~23mins*","05c800d1":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D2_ModernConvnets\/solutions\/W2D2_Tutorial1_Solution_6c4f5536.py)\n\n","9f5563cc":"###  Plot accuracies vs. training speed\n","5cad8eb4":"##  Set random seed\n","9b478f8e":"This cell creates a ResNet model pretrained on [ImageNet](http:\/\/www.image-net.org\/), a 1000 class image prediction dataset. The model is then trained to make predictions on [Imagenette](https:\/\/github.com\/fastai\/imagenette), a small subset of ImageNet classes that is useful for demonstrations and prototyping.","15c664fc":"##  Prepare Imagenette Data\n","6b61ce57":"--- \n# Section 2: The History of Convnets\n\n*Time estimate: ~15mins*","36c7df3c":"##  Set device (GPU or CPU). Execute `set_device()`\n","4c5dbb56":"---\n# Setup","c739b696":"##  Video 9: Speed-accuracy trade-off\n","29a8f0bb":"```\nRandom seed 2021 has been set.\n['gas pump', 'chain saw', 'jinrikisha', 'French horn', 'laptop']\n```","fd8a04e1":"###  Student Response\n","5bb283d8":"Now we want to look at the number of parameters.\n* How does the difference in number of parameters change if we fix the number of channels in the bottleneck of both ResNet and ResNeXt to be 64, but vary the number of paths in ResNeXt? (8 paths with 8 channels each would be one such example)\n* Which number of paths results in the biggest parameter savings?\n","13ced0cc":"## Interactive Demo 5: ResNet vs. ResNeXt\n\nThe widgets below calculate the number of parameters in a ResNet (top) and the parameters in a ResNeXt (bottom). We assume that the number of input and output channels (or feature maps) is the same (labeled \"Channels in+out\" in the widget). We refer to the number of channels after the first and the second layer of one block of either ResNet or ResNeXt as \"bottleneck channels\".\n\nThe sliders are currently in the position that is displayed in the figure above. The goal of the following tasks is to investigate the difference in expressiveness and numbers of parameters in ResNet and ResNeXt.","c6642993":" This cell gives you the `alexnet` model as well as the `input_image` and `input_batch` variables used below\n","bc64f852":" Executing `set_seed(seed=seed)` you are setting the seed\n","6f785160":"###  Parameter Calculator\n","4f44628a":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D2_ModernConvnets\/solutions\/W2D2_Tutorial1_Solution_3840a79d.py)\n\n","7eb10cac":"## Coding Exercise 4.1: Use the ResNet model\n\nComplete the function below that runs a batch of images through the trained ResNet and returns the Top 5 class predictions and their probabilities. Note that the ResNet model returns unnormalized logits$^\\dagger$. To obtain probabilities, you need to normalize the logits using softmax.\n\n<br>\n\n$^\\dagger$ $ \\text{logit}(p) = \\sigma^{-1}(p) = \\text{log} \\left( \\frac{p}{1-p} \\right), \\, \\text{for} \\, p \\in (0,1)$, where $\\sigma(\\cdot)$ is the sigmoid function, i.e., $\\sigma(z) = 1\/(1+e^{-z})$. For more information see [here](http:\/\/machinelearningmechanic.com\/deep_learning\/2019\/09\/04\/cross-entropy-loss-derivative.html).","cdfe9d1a":"---\n# Section 5: Inception + ResNeXt\n\n*Time estimate: ~27mins*","09535bcf":"##  Video 7: Transfer Learning\n","89005592":"## Think! 5: ResNet vs. ResNeXt\n\nIn the figure above, both networks \u2013 ResNet and ResNeXt \u2013 have a similar number of parameters. \n\n1. How many channels are there in the bottleneck of the two networks, respectively?\n1. How are these channels connected to each other from the first to the second layer in the blocks of the two networks, respectively? \n1. What does it mean for the expressiveness of the two models relative to each other?","5ba93c11":"```\nFCCN parameter count: 12583168\nConvNet parameter count: 7168\n```","b4c34fd9":"```\nDepthwise separable: 44 parameters\nRegular convolution: 72 parameters\n```","87e70e4f":"###  Parameter Calculator\n","cb81f2c6":"## Section 7.4: Training ResNet from scratch\n\nAs a baseline and for comparison reasons we will also train the ResNet \"from scratch\" \u2013 that is: initialize the weights randomly and train the entire network exclusively on the Pokemon dataset.","03ba24a8":"##  Video 6: Improving efficiency: MobileNet\n","c3b5019c":"##  Figure settings\n","9d50fd3f":"##  Install dependencies\n","8f4abde1":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D2_ModernConvnets\/solutions\/W2D2_Tutorial1_Solution_9f27445e.py)\n\n","50f36202":"### Exercise 7.5.1\n\nFirst, we compare the Pretrained ResNet with the ResNet trained from scratch. Why might pretrained models outperform models trained from scratch? In what cases would you expect them to be worse?","d6d6e4fa":"## Section 7.3: Train only classification layer\n\nAnother possible way to make use of transfer learning is to take a pre-trained model and replace the last layer, the classification layer (sometimes also called the \"linear readout\"). Instead of fine-tuning the whole model as before, we train only the classification layer.","246e9bfa":"####  Image Widget Code\n","4e745e96":"### Think! 6.1: How do parameter savings depend the on number of input feature maps, 4 vs. 64?","996ae66c":"###  Finetune readout of ResNet\n","58996948":"## Section 3.2: What does AlexNet learn?\nThis code visualizes the top-layer filters learned by AlexNet.\nWhat do these filters remind you of?","0f143559":"##  Download imagenette\n","806bed2b":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D2_ModernConvnets\/solutions\/W2D2_Tutorial1_Solution_85bbf359.py)\n\n","4731f320":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D2_ModernConvnets\/solutions\/W2D2_Tutorial1_Solution_55eef76b.py)\n\n","aa107212":"Images are high dimensional. That is to say that `image_length` * `image_width` * `image_channels` is a big number, and multiplying that big number by a normal sized fully-connected layer leads to a ton of parameters to learn. Yesterday, we learned about convolutional neural networks, one way of working around high dimensionality in images and other domains. \n\nThe widget below (i.e., *Interactive Demo 1*) calculates the parameters required for a single convolutional or fully connected layer that operates on an image of a certain height and width.\n\nRecall that, the number of parameters of a convolutional layer $l$ are calculated as:\n\n\\begin{equation}\n\\text{num_of_params}_l = \\left[ \\left( H \\times W \\times K_{l-1} \\right) + 1 \\right] \\times K_l\n\\end{equation}\n\nwhere $H$ denotes the shape of the height of the filter, $W$ the shape of the width of the filter, and $K_l$ denotes the number of the filters in the $l$-th layer. The added $1$ is because of the bias term for each filter.\n\n\nWhile a fully connected layer contains:\n\n\\begin{equation}\n\\text{num_of_params}_l = \\left[ \\left( N_{l-1} \\times N_l \\right) + 1 \\times N_l \\right]\n\\end{equation}\n\nwhere $N_l$ denotes the number of nodes in the $l$-th layer.\n\n\nAdjust the sliders to gain an intuition for how different model and data characteristics affect the number of parameters your model need to fit.\n\nNote: these classes are designed to show parameter scaling in the first layer of a network, to be actually useful they would need more layers, an activation function, etc.","3cde889e":"---\n# Section 3: Big and Deep Convnets\n\n*Time estimate: 18mins*","e0ecee70":"---\n# Summary\n\nIn this tutorial, you have learned about the modern Convnets (CNNs), their architecture, and operating principles. Also, you are now familiar with the notion of *Transfer Learning*, and you have learned when to apply it. If you have time left, you will learn more about the speed vs. accuracy trade-off. In the next tutorial, we will see the modern convnets in a facial recognition task.","afb8bf81":"###  Determine number of classes\n","e983bf60":"Write a function that calculates the number of parameters of a given network. Apply the function to the above defined fully-connected network and convolutional network and compare the parameter counts.\n\n**Hint:** `torch.numel`","a0a7a192":"## ResNet vs ResNeXt\n\n<img height=300 src=\"https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W2D2_ModernConvnets\/static\/ResNets.png\">\n\n[Xie et al., 2016](https:\/\/arxiv.org\/abs\/1611.05431)\n","2d0995cc":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D2_ModernConvnets\/solutions\/W2D2_Tutorial1_Solution_22704e52.py)\n\n","87d52576":"### Think! 3.2.1: Filter Similarity\n\nWhat do these filters remind you of?","fade0b20":"##  Video 3: AlexNet & VGG\n","cf998b4e":"## Out-of-distribution examples","118cc514":"###  Plot Accuracies\n","2c672ca7":" Run this cell to enable the widget\n","c61d68e4":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D2_ModernConvnets\/solutions\/W2D2_Tutorial1_Solution_01230f70.py)\n\n","173d0212":"---\n# Section 1: Modern CNNs and Transfer Learning\n\n*Time estimate: ~25mins*","0e715b4a":"####  Student Response\n","3d58c653":"**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n\n<p align='center'><img src='https:\/\/github.com\/NeuromatchAcademy\/widgets\/blob\/master\/sponsors.png?raw=True'\/><\/p>","c5a94d62":"### Exercise 7.5.2\n\nSecond, take a look at the different transfer learning methods - fine-tuning the whole network and training only the classification layer. Why might fine-tuning the whole network outperform training only the classification layer? What are the benefits of training only the classification layer? In what cases would you expect a similar performance of both methods?","4f51dffa":"The most common way large image models are trained in practice is via transfer learning. One first pretrains a network on a large classification dataset like ImageNet, then uses the weights of this network as initialization for training (\"fine-tuning\") that network on your task of choice. \n\nWhile training a network twice sounds like a strange thing to do, the model ends up training faster on the target dataset and often outperforms training \"from scratch\". There are also other benefits such as [robustness to noise](https:\/\/arxiv.org\/pdf\/1901.09960.pdf) that are the subject of [active research](https:\/\/arxiv.org\/abs\/2008.11687).\n\nIn this section we will demonstrate transfer learning by taking a model trained on ImageNet and teaching it to classify Pokemon.","5340fa92":"##  Video 8: Summary and Outlook\n","2abe1464":" Run this cell to enable the widget\n","11f424a4":"---\n# Bonus: Speed-Accuracy Trade-Off \/ Different Backbones\n\n*Time estimate: ~ 21mins*","ae809420":"## Section 7.2: Fine-tuning a ResNet\n\nIt is common in computer vision to take a large model trained on a large dataset (often ImageNet), replace the classification layer and fine-tune the entire network to perform a different task. \n\nHere we'll be using a pre-trained ResNet model to classify types of Pokemon.","40602ca6":"####  Student Response\n","022fdb70":"##  Video 4: Residual Networks (ResNets)\n","7d4d36ee":"## Bonus Coding Exercise: Compare accuracy and training speed of different models\n\nThe goal is to load three pretrained models and fine-tune them.\n`models` is a dictionary where the keys are the names of the models and the values are the corresponding model objects.\nCurrently the names are *ResNet18, AlexNet* and *VGG-19*.\nFor a start, load these models from torchvision.models and make sure they are pretrained.\n\nIf you want to try other models, just change the dictionary, or if you want to even try out more than three models, just add them to the dictionary and add their learning rates in the array below.","72d4e4ec":"## Interactive Demo 1: Check your results\nThe widget below calculates the number of parameters in a FCNN and CNN with the same architecture as our models above. Our models had an input image that was 128x128, and used 256 filters (or 256 nodes in the FCNN case). Check that the calculations you made above are correct.\n\nNote how few parameters the convolutional networks take, especially as you increase the input image size.","e882596c":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D2_ModernConvnets\/solutions\/W2D2_Tutorial1_Solution_9d15a19c.py)\n\n","042c5acb":" Run this cell to enable the widget!\n","772cf252":"## Bonus Exercise 1\n\nLook at the plot above.\nIt shows the training speed vs. the accuracy of the models you chose.\nThe training speed is measured as the mean time the training takes per epoch.\nThe size of the marker visualizes the number of parameters of the model.\n\nWhich model seems to be the best for this task and why?\nExplain your conclusion based on speed, accuracy and number of parameters.","6ee31ed2":"##  Imagenette Train Loop\n","ce59ad07":"##  Set Up Textual ImageNet labels\n","5225f066":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D2_ModernConvnets\/solutions\/W2D2_Tutorial1_Solution_71e2a897.py)\n\n","0454ffc5":"### Coding Exercise 6.1: Calculation of parameters\n\nFill in the calculation of the parameters of regular convolution and depthwise separable convolution in the function below.\nAbove you can see the example given in the video for you to check if your calculation is correct.","91ea3222":"###  Display Example Images\n","71328177":"## Further Reading\nIf the question \"what are neural network filters looking for\" is at all interesting to you, or if you like geometric art, you'll enjoy [this post](https:\/\/distill.pub\/2017\/feature-visualization\/) creating images that maximize output of various CNN neurons. There is also a good article showing what the space of images looks like as models train [here](https:\/\/distill.pub\/2020\/grand-tour\/).","c1eae25f":"## Further Reading\nSupervised pretraining as you've seen here is useful, but there are several other ways of using outside data to improve your models. The ones that are particularly popular right now are self-supervised techniques like [contrastive learning](https:\/\/arxiv.org\/pdf\/2002.05709.pdf).\n\nThere is also a [recent paper](https:\/\/arxiv.org\/abs\/2102.01293) that seeks to quantify the relationship between model size, pretraining dataset size, training dataset size, and performance.","2f4cdef1":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D2_ModernConvnets\/solutions\/W2D2_Tutorial1_Solution_84de91ca.py)\n\n","71417850":"###  Import Alexnet\n","d16cbcc8":"##  Video 5: Improving efficiency: Inceptrion and ResNeXt\n","ed7b625b":"## Section 6.1: Depthwise separable convolutions\n\nAnother way to reduce the computational cost of large models is the use of depthwise separable convolutions ([introduced here](https:\/\/www.di.ens.fr\/data\/publications\/papers\/phd_sifre.pdf)). Depthwise separable convolutions are the key component making [MobileNets](https:\/\/arxiv.org\/abs\/1704.04861) efficient.\n\n<img src=\"https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W2D2_ModernConvnets\/static\/SchematicCNN.png\">","2113c8a4":"##  Map Imagenette Labels to Imagenet Labels\n","8b74c729":"##  Airtable Submission Link\n","86d45369":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D2_ModernConvnets\/solutions\/W2D2_Tutorial1_Solution_b3ebfd81.py)\n\n","04d58bd8":"###  Student Response\n","d2b8fac2":"## Coding Exercise 1: Calculate number of parameters in FCNN vs ConvNet ","eb1cd918":"##  Video 2: History of convnets\n","b653e4d0":"Convolutional neural networks have been around for a long time. [The first CNN model](https:\/\/www.rctn.org\/bruno\/public\/papers\/Fukushima1980.pdf) was published in 1980, and was based on ideas in neuroscience that [predated it by decades](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC1359523\/). Why is it then that [AlexNet](https:\/\/proceedings.neurips.cc\/paper\/2012\/hash\/c399862d3b9d6b76c8436e924a68c45b-Abstract.html), a CNN model published in 2012, is generally considered to mark the start of the deep learning revolution?\n\nWatch the video below to get a better idea of the role that hardware and the internet have played in progressing deep learning.","fa70df1c":"### Think! 3.2.2 Filter Purpose\nWhat do these filters appear to be doing? Note that different filters play different roles so there are several good answers.","70465d91":"---\n# Section 4: Convnets After AlexNet\n\n*Time estimate: ~25mins*","db4fedf1":"The code below runs two out-of-distribution examples through the trained ResNet. Look at the predictions and discuss, why the model might fail to make accurate predictions on these images. ","9f788af6":"### Bonus Exercise 2\n\nHow does the speed correlate with the accuracy? Are faster models also more accurate?","783ba348":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D2_ModernConvnets\/solutions\/W2D2_Tutorial1_Solution_40c35f4c.py)\n\n","2cbe0bc6":"## Think! 2: Challenges of improving CNNs\nAs we shall see today, the story of deep learning and CNNs has been one of scaling networks: making them bigger and deeper.\n\nBased on what you know so far from previous days, what challenges might researchers have faced when trying to scale up CNNs and applying them to different visual recognition tasks? Do you already have some ideas how these challenges might have been addressed?\n\nDiscuss this with your group for ~10 minutes.\n\n(Hint: labeled data, compute and memory are all finite)","bbc0a84b":"# Tutorial 1: Learn how to use modern convnets\n\n**Week 2, Day 2: Modern Convnets**\n\n**By Neuromatch Academy**\n\n__Content creators:__ Laura Pede, Richard Vogg, Marissa Weis, Timo L\u00fcddecke, Alexander Ecker (based on an initial version by Ben Heil)\n\n__Content reviewers:__ Arush Tagade, Polina Turishcheva, Yu-Fang Yang, Bettina Hein, Melvin Selim Atay, Kelson Shilling-Scrivo\n\n__Content editors:__ Roberto Guidotti, Spiros Chavlis\n\n__Production editors:__ Anoop Kulkarni, Roberto Guidotti, Cary Murray, Spiros Chavlis","a350c42a":"####  Student Response\n","8906128e":"###  Download Data\n","baf18715":"### Interactive Demo 3.2: What does AlexNet see?\nOne way of visualizing CNNs is to look at the output of individual filters for a given image. Below is a widget that lets you examine the outputs of various filters used in AlexNet."}}