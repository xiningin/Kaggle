{"cell_type":{"cda70f60":"code","b5e2a457":"code","a9d91542":"code","31e217f9":"code","c6475014":"code","26e77fc4":"code","3ce010e5":"code","97c20758":"code","2d85ec5d":"code","7f0d873f":"code","22871882":"code","ba1c9b5b":"code","8aff84c7":"code","84365240":"code","e5a974b5":"code","e86db70d":"code","e7327395":"code","71c583d5":"code","e4bfbfa4":"code","c9fe103e":"code","2eb90b65":"code","0c70ce45":"code","47957fc2":"code","dbf83b5a":"code","ab275f2d":"code","3750e138":"code","60e19373":"code","438bddb6":"code","1ea6644f":"code","2568f84e":"code","f34b81a8":"code","9ef65b78":"code","69769c46":"code","0e47028c":"code","b8207837":"code","0e4d6237":"markdown","bd4a9b44":"markdown","17608844":"markdown","396ced84":"markdown","3ca6a414":"markdown","d57086c5":"markdown","1718f3fb":"markdown","2798383a":"markdown","75a6cd84":"markdown","344a2f1b":"markdown","a0454f34":"markdown","4bac8713":"markdown","6a3c5c74":"markdown","047d226d":"markdown","dc689b5a":"markdown","5b31c249":"markdown","0c0884ff":"markdown","28addf0c":"markdown","635668e1":"markdown","e28160c9":"markdown","9f160808":"markdown","872603ae":"markdown","0227277c":"markdown"},"source":{"cda70f60":"# Developers version with better logging and final model description\n!pip install -U https:\/\/github.com\/sberbank-ai-lab\/LightAutoML\/raw\/fix\/logging\/LightAutoML-0.2.16.2-py3-none-any.whl","b5e2a457":"# Standard python libraries\nimport os\nimport time\n\n# Essential DS libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import rankdata\nimport torch\n\n# LightAutoML presets, task and report generation\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\nfrom lightautoml.report.report_deco import ReportDeco\n\n# Everything for graphs\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a9d91542":"N_THREADS = 4\nN_FOLDS = 5\nRANDOM_STATE = 42\nTEST_SIZE = 0.2\nTIMEOUT = 6*3600\nTARGET_NAME = 'loss'\n\nCUTOFFS = [0, 3, 5, 7, 10, 13, 15, 20]","31e217f9":"np.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","c6475014":"%%time\n\ntrain_data = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntrain_data.head()","26e77fc4":"test_data = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\ntest_data.head()","3ce010e5":"samp_sub = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\nsamp_sub.head()","97c20758":"train_data['NN_preds'] = pd.read_csv('..\/input\/in-python-tabular-denoising-residual-network\/oof_2.csv').iloc[:, 0].values\ntest_data['NN_preds'] = pd.read_csv('..\/input\/in-python-tabular-denoising-residual-network\/submission_v2.csv')[TARGET_NAME].values","2d85ec5d":"%%time\n\nimport xgboost as xgb\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\n\ndef rmse(y_true, y_pred, **kwargs):\n    return mean_squared_error(y_true, y_pred, squared = False, **kwargs)\n\nX = train_data.drop(['id', 'loss', 'NN_preds'], axis=1).values\ny = train_data['loss'].values\nX_test = test_data.drop(['id', 'NN_preds'], axis=1).values\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)\n\n# Fetch the best trial parameters and set some settings for the KFold predictions.\nxgb_params = {\n     'max_depth': 7, \n     'eta': 0.008373136177752354, \n     'subsample': 0.55, \n     'colsample_bytree': 0.65, \n     'min_child_weight': 56, \n     'reg_lambda': 49, \n     'reg_alpha': 43,\n     'tree_method': 'hist',\n     'n_estimators': 3700,\n     'n_jobs': N_THREADS\n}\n\noof_preds = np.array([0.0] * len(train_data))\ntest_preds = np.array([0.0] * len(test_data))\n\nkf = KFold(n_splits=10, shuffle=True, random_state = RANDOM_STATE)\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n    # Fetch the train-validation indices.\n    X_train, X_valid = X[train_idx], X[valid_idx]\n    y_train, y_valid = y[train_idx], y[valid_idx]\n    \n    # Create and fit a new model using the best parameters.\n    model = xgb.XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid)],\n            eval_metric='rmse', verbose=False)\n    \n    # Validation predictions.\n    valid_pred = model.predict(X_valid)\n    oof_preds[valid_idx] = valid_pred\n    print('Fold {} RMSE: {}'.format(fold, rmse(y_valid, valid_pred)))\n    \n    test_preds += model.predict(X_test) \/ 10\n\nprint('Check scores...')\nprint('OOF score: {}'.format(rmse(y, oof_preds)))\n\ntrain_data['XGB_preds'] = oof_preds\ntest_data['XGB_preds'] = test_preds","7f0d873f":"plt.figure(figsize = (10, 10))\nplt.scatter(train_data['XGB_preds'], train_data['NN_preds'])\nplt.plot([0, 18], [0, 18], '--r')\nplt.grid(True)\nplt.title('Train data: XGB vs NN preds', fontsize = 17)\nplt.xlabel('XGB_preds', fontsize = 17)\nplt.ylabel('NN_preds', fontsize = 17)\nplt.show()","22871882":"for algo in ['XGB']:\n    for data in [train_data, test_data]:\n        data['NN_minus_{}'.format(algo)] = data['NN_preds'] - data['{}_preds'.format(algo)]\n        data['NN_mul_{}'.format(algo)] = data['NN_preds'] * data['{}_preds'.format(algo)]\n        data['NN_div_{}'.format(algo)] = data['NN_preds'] \/ data['{}_preds'.format(algo)]","ba1c9b5b":"%%time\n\ntr_data, te_data = train_test_split(train_data, \n                                    test_size=TEST_SIZE, \n                                    stratify=train_data[TARGET_NAME], \n                                    random_state=RANDOM_STATE)\nprint('Data splitted. Parts sizes: tr_data = {}, te_data = {}'.format(tr_data.shape, te_data.shape))","8aff84c7":"tr_data.head()","84365240":"tr_data[TARGET_NAME].value_counts().shape","e5a974b5":"%%time \n\nIMP_arr = []\nOOF_preds = []\nTEST_preds = []\nOOF_preds_parts = []\nTEST_preds_parts = []\nfor i in CUTOFFS:\n    print('Start {}'.format(i))\n    tr_data[TARGET_NAME + '_cl'] = (tr_data[TARGET_NAME] > i).astype(int)\n    te_data[TARGET_NAME + '_cl'] = (te_data[TARGET_NAME] > i).astype(int)\n    \n    # =============================================================\n    \n    task = Task('binary')\n\n    roles = {'target': TARGET_NAME + '_cl',\n             'drop': ['id', TARGET_NAME]\n             }\n\n    automl = TabularAutoML(task = task, \n                           timeout = TIMEOUT,\n                           cpu_limit = N_THREADS,\n                           reader_params = {'n_jobs': N_THREADS, 'cv': 3, 'random_state': RANDOM_STATE},\n                           general_params = {'use_algos': [['lgb', 'cb']], \n                                             'return_all_predictions': True, # return all predictions from the layer before blender\n                                             'weighted_blender_max_nonzero_coef': 0.0}, # no drop for algos during blending phase\n                           verbose = 1 # Available values: 0,1,2,3 (from less detailed to more)\n                          )\n\n    oof_pred = automl.fit_predict(tr_data, roles = roles)\n    IMP_arr.append(automl.get_feature_scores('fast').set_index('Feature')['Importance'].to_dict())\n    te_pred = automl.predict(te_data)\n    \n    # =============================================================\n    \n    OOF_preds_parts.append(oof_pred.data)\n    TEST_preds_parts.append(te_pred.data)\n\n    oof_pred_weighted = np.dot(oof_pred.data, automl.blender.wts) # Create weighted OOF preds based on single algos and blender weights\n    te_pred_weighted = np.dot(te_pred.data, automl.blender.wts) # Create weighted Holdout preds based on single algos and blender weights\n    OOF_preds.append(oof_pred_weighted)\n    TEST_preds.append(te_pred_weighted)\n    \n    # =============================================================\n    \n    print('Check scores {}...'.format(i))\n    print('OOF score: {}'.format(roc_auc_score(tr_data[TARGET_NAME + '_cl'].values, oof_pred_weighted)))\n    print('HOLDOUT score: {}'.format(roc_auc_score(te_data[TARGET_NAME + '_cl'].values, te_pred_weighted)))","e86db70d":"# Drop unnecessary columns created in the cell above\ntr_data.drop(columns = [TARGET_NAME + '_cl'], inplace = True)\nte_data.drop(columns = [TARGET_NAME + '_cl'], inplace = True)","e7327395":"def feat_imp_plot(df, title):\n    plt.figure(figsize=(4,15))\n    ax = sns.heatmap(df.set_index('Feature'), \n                     annot=False, \n                     cmap=\"RdBu\", \n                     annot_kws={\"weight\": \"bold\", \"fontsize\":13})\n    ax.set_title(title, fontsize=17)\n    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n             rotation_mode=\"anchor\", weight=\"normal\")\n    plt.setp(ax.get_yticklabels(), weight=\"normal\",\n             rotation_mode=\"anchor\", rotation=0, ha=\"right\")\n    plt.show();\n\nfeats_imp_df = pd.DataFrame()\nfeats_imp_df['Feature'] = ['f'+str(i) for i in range(100)] + ['NN_preds', 'XGB_preds', 'NN_minus_XGB', 'NN_mul_XGB', 'NN_div_XGB']\nfor cutoff, mapper in zip(CUTOFFS, IMP_arr):\n    feats_imp_df['Imps_'+str(cutoff)] = rankdata(feats_imp_df['Feature'].map(mapper))\n\nfeat_imp_plot(feats_imp_df, \"Feature importances ranks for different target cutoffs\\n100 - best, 0 - worst\")","71c583d5":"# Making a plot\nFeats_imp_df = pd.DataFrame()\nFeats_imp_df['Feature'] = ['f'+str(i) for i in range(100)] + ['NN_preds', 'XGB_preds', 'NN_minus_XGB', 'NN_mul_XGB', 'NN_div_XGB']\nfor cutoff, mapper in zip(CUTOFFS, IMP_arr):\n    Feats_imp_df['Imps_'+str(cutoff)] = Feats_imp_df['Feature'].map(mapper)\n    mx, mn = Feats_imp_df['Imps_'+str(cutoff)].max(), Feats_imp_df['Imps_'+str(cutoff)].min()\n    Feats_imp_df['Imps_'+str(cutoff)] = (Feats_imp_df['Imps_'+str(cutoff)] - mn) \/ (mx - mn)\n\nfeat_imp_plot(Feats_imp_df, \"Feature importances (min-max transform) for different target cutoffs\\n1 - best, 0 - worst\")","e4bfbfa4":"def combine_preds_array_to_df(cutoffs_arr, preds_parts):\n    preds_df = pd.DataFrame()\n    prev_cutoff = None\n    for cutoff, pred in zip(cutoffs_arr, preds_parts):\n        preds_df['LGBM_'+str(cutoff)] = pred[:, 0]\n        preds_df['CB_'+str(cutoff)] = pred[:, 1]\n        \n        if prev_cutoff is not None:\n            preds_df['diff_LGBM_'+str(cutoff)] = preds_df['LGBM_'+str(prev_cutoff)] - preds_df['LGBM_'+str(cutoff)]\n            preds_df['diff_CB_'+str(cutoff)] = preds_df['CB_'+str(prev_cutoff)] - preds_df['CB_'+str(cutoff)]\n        prev_cutoff = cutoff\n    \n    return preds_df\n\ntr_preds_df = combine_preds_array_to_df(CUTOFFS, OOF_preds_parts)\nte_preds_df = combine_preds_array_to_df(CUTOFFS, TEST_preds_parts)\nprint(tr_preds_df.shape, te_preds_df.shape)","c9fe103e":"tr_preds_df.head()","2eb90b65":"for col in tr_preds_df.columns:\n    tr_data[col] = tr_preds_df[col].values\n    te_data[col] = te_preds_df[col].values","0c70ce45":"tr_data.head()","47957fc2":"task = Task('reg', )\n\nroles = {'target': TARGET_NAME,\n         'drop': ['id'] + list(tr_preds_df.columns)\n         }\n\nautoml = TabularAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE},\n                       general_params = {'use_algos': [['lgb', 'cb']]},\n                       verbose = 1 # Available values: 0,1,2,3 (from less detailed to more)\n                      )\n\noof_pred = automl.fit_predict(tr_data, roles = roles)\nfast_fi = automl.get_feature_scores('fast')\nfast_fi.set_index('Feature')['Importance'].plot.bar(figsize = (30, 10), grid = True)\nte_pred = automl.predict(te_data)\n\nprint('Check scores...')\nprint('OOF score: {}'.format(rmse(tr_data[TARGET_NAME].values, oof_pred.data[:, 0])))\nprint('HOLDOUT score: {}'.format(rmse(te_data[TARGET_NAME].values, te_pred.data[:, 0])))","dbf83b5a":"task = Task('reg', )\n\nroles = {'target': TARGET_NAME,\n         'drop': ['id']\n         }\n\nautoml = TabularAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE},\n                       general_params = {'use_algos': [['lgb', 'cb']]},\n                       verbose = 1 # Available values: 0,1,2,3 (from less detailed to more)\n                      )\n\noof_pred = automl.fit_predict(tr_data, roles = roles)\nfast_fi = automl.get_feature_scores('fast')\nfast_fi.set_index('Feature')['Importance'].plot.bar(figsize = (30, 10), grid = True)\nte_pred = automl.predict(te_data)\n\nprint('Check scores...')\nprint('OOF score: {}'.format(rmse(tr_data[TARGET_NAME].values, oof_pred.data[:, 0])))\nprint('HOLDOUT score: {}'.format(rmse(te_data[TARGET_NAME].values, te_pred.data[:, 0])))","ab275f2d":"%%time \n\nOOF_preds = []\nTEST_preds = []\nOOF_preds_parts = []\nTEST_preds_parts = []\nfor i in CUTOFFS:\n    print('Start {}'.format(i))\n    train_data[TARGET_NAME + '_cl'] = (train_data[TARGET_NAME] > i).astype(int)\n    \n    # =============================================================\n    \n    task = Task('binary', )\n\n    roles = {'target': TARGET_NAME + '_cl',\n             'drop': ['id', TARGET_NAME]\n             }\n\n    automl = TabularAutoML(task = task, \n                           timeout = TIMEOUT,\n                           cpu_limit = N_THREADS,\n                           reader_params = {'n_jobs': N_THREADS, 'cv': 10, 'random_state': RANDOM_STATE},\n                           general_params = {'use_algos': [['lgb', 'cb']], \n                                             'return_all_predictions': True, \n                                             'weighted_blender_max_nonzero_coef': 0.0},\n                           verbose = 1 # Available values: 0,1,2,3 (from less detailed to more)\n                          )\n\n    oof_pred = automl.fit_predict(train_data, roles = roles)\n    test_pred = automl.predict(test_data)\n    \n    # =============================================================\n    \n    OOF_preds_parts.append(oof_pred.data)\n    TEST_preds_parts.append(test_pred.data)\n\n    oof_pred_weighted = np.dot(oof_pred.data, automl.blender.wts)\n    test_pred_weighted = np.dot(test_pred.data, automl.blender.wts)\n    OOF_preds.append(oof_pred_weighted)\n    TEST_preds.append(test_pred_weighted)\n    \n    # =============================================================\n    \n    print('Check scores {}...'.format(i))\n    print('OOF score: {}'.format(roc_auc_score(train_data[TARGET_NAME + '_cl'].values, oof_pred_weighted)))","3750e138":"# Drop unnecessary columns created in the cell above\ntrain_data.drop(columns = [TARGET_NAME + '_cl'], inplace = True)","60e19373":"def combine_preds_array_to_df(cutoffs_arr, preds_parts):\n    preds_df = pd.DataFrame()\n    prev_cutoff = None\n    for cutoff, pred in zip(cutoffs_arr, preds_parts):\n        preds_df['LGBM_'+str(cutoff)] = pred[:, 0]\n        preds_df['CB_'+str(cutoff)] = pred[:, 1]\n        preds_df['LGBM_CB_diff_'+str(cutoff)] = pred[:, 0] - pred[:, 1]\n        if prev_cutoff is not None:\n            preds_df['diff_LGBM_'+str(cutoff)] = preds_df['LGBM_'+str(prev_cutoff)] - preds_df['LGBM_'+str(cutoff)]\n            preds_df['diff_CB_'+str(cutoff)] = preds_df['CB_'+str(prev_cutoff)] - preds_df['CB_'+str(cutoff)]\n        prev_cutoff = cutoff\n    \n    return preds_df\n\ntrain_preds_df = combine_preds_array_to_df(CUTOFFS, OOF_preds_parts)\ntest_preds_df = combine_preds_array_to_df(CUTOFFS, TEST_preds_parts)\nprint(train_preds_df.shape, test_preds_df.shape)","438bddb6":"for col in train_preds_df.columns:\n    train_data[col] = train_preds_df[col].values\n    test_data[col] = test_preds_df[col].values","1ea6644f":"lgb_params = {\n    'metric': 'RMSE',\n    'feature_pre_filter': False,\n    'lambda_l1': 0.45,\n    'lambda_l2': 4.8,\n    'learning_rate': 0.005,\n    'num_trees': 80000,\n    'early_stopping_rounds': 200,\n    'num_leaves': 10, \n    'feature_fraction': 0.4, \n    'bagging_fraction': 1.0, \n    'bagging_freq': 0, \n    'min_child_samples': 100,\n    'num_threads': 4\n}\n\ncb_params = {\n    'num_trees': 7000, \n    'od_wait': 600, \n    'learning_rate': 0.015, \n    'l2_leaf_reg': 64, \n    'subsample': 0.83, \n    'random_strength': 17.17, \n    'max_depth': 6, \n    'min_data_in_leaf': 10, \n    'leaf_estimation_iterations': 3,\n    'loss_function': 'RMSE',\n    'eval_metric': 'RMSE',\n    'bootstrap_type': 'Bernoulli',\n    'leaf_estimation_method': 'Newton',\n    'random_seed': 42,\n    \"thread_count\": 4\n}","2568f84e":"%%time \n\nCONF_PATH = '..\/input\/lightautoml-configs\/'\n\ntask = Task('reg', )\n\nroles = {\n    'target': TARGET_NAME,\n    'drop': ['id']\n}\n\nautoml = TabularUtilizedAutoML(task = task, \n                               timeout = TIMEOUT,\n                               cpu_limit = N_THREADS,\n                               reader_params = {'n_jobs': N_THREADS, 'cv': 10, 'random_state': RANDOM_STATE},\n                               general_params = {'use_algos': [['lgb', 'cb']]}, # LGBM and CatBoost algos only\n                               lgb_params = {'default_params': lgb_params, 'freeze_defaults': True}, # LGBM params\n                               cb_params = {'default_params': cb_params, 'freeze_defaults': True}, # CatBoost params\n                               verbose = 2, # Available values: 0,1,2,3 (from less detailed to more)\n                               configs_list = [CONF_PATH + 'conf_0_sel_type_0.yml',\n                                               CONF_PATH + 'conf_2_select_mode_1_no_typ.yml',\n                                               CONF_PATH + 'conf_4_sel_type_0_no_int.yml',\n                                               CONF_PATH + 'conf_6_sel_type_1_tuning_full_no_int_lgbm.yml'],\n                               max_runs_per_config=2,\n                               drop_last = False\n                              )\n\noof_pred = automl.fit_predict(train_data, roles = roles)","f34b81a8":"fast_fi = automl.get_feature_scores('fast')\nfast_fi.set_index('Feature')['Importance'].plot.bar(figsize = (30, 10), grid = True)","9ef65b78":"print(automl.create_model_str_desc())","69769c46":"test_pred = automl.predict(test_data)\nprint('Prediction for test_data:\\n{}\\nShape = {}'.format(test_pred, test_pred.shape))","0e47028c":"samp_sub[TARGET_NAME] = test_pred.data[:, 0]\nsamp_sub.to_csv('LightAutoML_utilized_submission.csv', index = False)","b8207837":"samp_sub","0e4d6237":"# =========== LightAutoML model building ===========","bd4a9b44":"### Great - as we can see above the classifier predictions usage idea works fine and we can reproduce it on the full dataset\n\n# Retrain on the full dataset","17608844":"# It's time to check the feature importances for different target cutoffs","396ced84":"<img src=\"https:\/\/github.com\/sberbank-ai-lab\/LightAutoML\/raw\/master\/imgs\/LightAutoML_logo_big.png\" alt=\"LightAutoML logo\" style=\"width:50%;\"\/>\n\n## Official LightAutoML github repository is [here](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML)\n\n## Upvote is the best motivator \ud83d\udc4d\n\n# Step 0.0. LightAutoML installation","3ca6a414":"# Step 0.1. Import libraries\n\nHere we will import the libraries we use in this kernel:\n- Standard python libraries for timing, working with OS etc.\n- Essential python DS libraries like numpy, pandas, scikit-learn and torch (the last we will use in the next cell)\n- LightAutoML modules: presets for AutoML, task and report generation module","d57086c5":"# Now we are ready for training the model","1718f3fb":"# Predict for test data ","2798383a":"# Comparing usual model with the model on extended dataset ","75a6cd84":"In next the cell we are going to create LightAutoML model with `TabularAutoML` class - preset with default model structure like in the image below:\n\n<img src=\"https:\/\/github.com\/sberbank-ai-lab\/LightAutoML\/raw\/master\/imgs\/tutorial_blackbox_pipeline.png\" alt=\"TabularAutoML preset pipeline\" style=\"width:85%;\"\/>\n\nin just several lines. Let's discuss the params we can setup:\n- `task` - the type of the ML task (the only **must have** parameter)\n- `timeout` - time limit in seconds for model to train\n- `cpu_limit` - vCPU count for model to use\n- `reader_params` - parameter change for Reader object inside preset, which works on the first step of data preparation: automatic feature typization, preliminary almost-constant features, correct CV setup etc. For example, we setup `n_jobs` threads for typization algo, `cv` folds and `random_state` as inside CV seed.\n- `general_params` - we use `use_algos` key to setup the model structure to work with (Linear and LGBM model on the first level and their weighted composition creation on the second). This setup is only to speedup the kernel, you can remove this `general_params` setup if you want the whole LightAutoML model to run.\n\n**Important note**: `reader_params` key is one of the YAML config keys, which is used inside `TabularAutoML` preset. [More details](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML\/blob\/master\/lightautoml\/automl\/presets\/tabular_config.yml) on its structure with explanation comments can be found on the link attached. Each key from this config can be modified with user settings during preset object initialization. To get more info about different parameters setting (for example, ML algos which can be used in `general_params->use_algos`) please take a look at our [article on TowardsDataScience](https:\/\/towardsdatascience.com\/lightautoml-preset-usage-tutorial-2cce7da6f936).\n\n## In the cell below we are going to calculate LightAutoML classifier models for binary targets based on real target and cutoff like \"bigger than X\"","344a2f1b":"### We also add XGB for even better generalization","a0454f34":"# Step 0.4. Data loading\nLet's check the data we have:","4bac8713":"# LightAutoML model creation - TabularAutoML preset","6a3c5c74":"### In both variants we see the known features in the top - `f81`, `f52` and `f25`.  ","047d226d":"# Step 0.2. Constants\n\nHere we setup the constants to use in the kernel:\n- `N_THREADS` - number of vCPUs for LightAutoML model creation\n- `N_FOLDS` - number of folds in LightAutoML inner CV\n- `RANDOM_STATE` - random seed for better reproducibility\n- `TEST_SIZE` - houldout data part size \n- `TIMEOUT` - limit in seconds for model to train\n- `TARGET_NAME` - target column name in dataset","dc689b5a":"# Create new dataframes with classifiers predicts","5b31c249":"# Step 0.3. Imported models setup\n\nFor better reproducibility we fix numpy random seed with max number of threads for Torch (which usually try to use all the threads on server):","0c0884ff":"# Step 0.5. Data splitting for train-holdout\nAs we have only one file with target values, we can split it into 80%-20% for holdout usage:","28addf0c":"# Additional materials\n\n- [Official LightAutoML github repo](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML)\n- [LightAutoML documentation](https:\/\/lightautoml.readthedocs.io\/en\/latest)\n- [LightAutoML starter for TPS August 2021](https:\/\/www.kaggle.com\/alexryzhkov\/aug21-lightautoml-starter)","635668e1":"### We need to append NN predictions as well to make better generalization \n#### Thanks for these predictions goes to @pourchot and [his great notebook](https:\/\/www.kaggle.com\/pourchot\/in-python-tabular-denoising-residual-network) - do not forget to upvote it \ud83d\udc4d","e28160c9":"## Upvote if you like the kernel or find it useful \ud83d\udc4d","9f160808":"This step can be used if you are working inside Google Colab\/Kaggle kernels or want to install LightAutoML on your machine:","872603ae":"# Create submission file","0227277c":"### Received model looks like \ud83d\udc47\ud83d\udc47\ud83d\udc47"}}