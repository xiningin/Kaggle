{"cell_type":{"315e7804":"code","78077c5d":"code","7dbcb442":"code","85efcae9":"code","7713f992":"code","c8cd5444":"code","f40cdb0d":"code","83c7583d":"code","e63fe7ba":"code","c3246785":"code","feafa1b1":"code","94341a53":"code","a65e8f8b":"code","5e0829dd":"code","038451db":"code","7db68fc6":"code","ffe10eec":"code","482bbbdd":"code","c045df04":"code","e48048d7":"code","850a6a83":"code","4f62de3e":"code","df9795cc":"code","f6127bb8":"code","89c32ae9":"code","7f5b6396":"code","2f160d54":"code","42b8d58c":"code","6f0e8442":"code","7c7596ca":"code","9019374a":"code","075f702f":"code","a0e8d2ce":"code","01ac96f0":"code","858822f3":"code","9f1f8be0":"code","ad9fa57e":"code","96a9fc2d":"code","f006a613":"code","7a265691":"code","320302d3":"code","cac3f666":"code","ce235873":"code","6bed9fae":"code","86654122":"code","397d707c":"code","33dd18fc":"code","f210d341":"code","7f0b41fc":"markdown","5dec8200":"markdown","9c26d6b6":"markdown","956d7ddd":"markdown","0a89efac":"markdown","4581ef64":"markdown","29695847":"markdown","39a46c6e":"markdown","35128db9":"markdown","5e25f50a":"markdown","ddcaf340":"markdown","60403983":"markdown","90da81b4":"markdown","394a1e8d":"markdown","38dd9cd1":"markdown","4af75a9e":"markdown","d6dc1267":"markdown","514c001d":"markdown","fbbac7cc":"markdown","55ca2350":"markdown","82467423":"markdown","9f890a4d":"markdown","5abdcc6c":"markdown","dd75ce14":"markdown"},"source":{"315e7804":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)`\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","78077c5d":"# sample_submission = pd.read_csv(\"..\/input\/ieee-fraud-detection\/sample_submission.csv\")\n\ntest_identity = pd.read_csv(\"..\/input\/input\/test_identity.csv\" , index_col = 'TransactionID')\n# test_identity = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_identity.csv\" , index_col='TransactionID')\ntest_transaction = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_transaction.csv\",index_col='TransactionID')\ntrain_identity = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_identity.csv\",index_col='TransactionID')\ntrain_transaction = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_transaction.csv\",index_col='TransactionID')\n\ntrain_data = train_transaction.merge(train_identity, how='left' ,left_index=True , right_index=True)\ntest_data  = test_transaction.merge(test_identity,how='left' , left_index=True, right_index=True)\n\n\ntest_data.to_csv('merged_test_data.csv')\ntrain_data.to_csv('merged_train_data.csv')\n","7dbcb442":"del train_identity,train_transaction,test_identity, test_transaction , test_data , train_data","85efcae9":"import pandas as pd \nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","7713f992":"\ndef resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    return summary\n    \n\n# THIS FUNCTION WILL PLOT A CORRELATION HEATMAP WITH A SET THRESHOLD OF 0.9 CORRELATION.\ndef corrfunc(df , col):\n    color = plt.get_cmap('RdYlGn') \n    color.set_bad('green') \n    correalation =df[col].corr()\n    correalation[np.abs(correalation)<.9] = 0 # This will set all correlations less than 0.9 to 0\n    plt.figure(figsize= (len(col),len(col)))\n    sns.heatmap(correalation, yticklabels= True, annot = True, vmin=-1, vmax=1,cmap = color)\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: \n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n","c8cd5444":"\ndata = pd.read_csv(\"..\/input\/kernel-1\/merged_train_data.csv\" , index_col='TransactionID')\nprint(data.shape)\n\ntrain = reduce_mem_usage(data)\n\n\ndata = pd.read_csv(\"..\/input\/kernel-1\/merged_test_data.csv\" , index_col='TransactionID')\n\nX_test = reduce_mem_usage(data)\n","f40cdb0d":"with pd.option_context('display.max_columns', 433):\n    print(train.describe(include='all'))\n","83c7583d":"resumetable(train)[:25]","e63fe7ba":"print(train.shape , X_test.shape)\ntrain.head()","c3246785":"plt.figure(figsize=(9,6))\ng =sns.countplot(x='isFraud' , data= train )\ng.set_title(\"Fraud Transactions Distribution \\n# 0: No Fraud | 1: Fraud #\", fontsize=22)\ng.set_xlabel(\"Is fraud?\", fontsize=18)\ng.set_ylabel('Count', fontsize=18)\nplt.show()","feafa1b1":"print(\"Transaction Amounts Quantiles:\")\nprint(train['TransactionAmt'].quantile([.01, .025, .1, .25, .5, .75, .9, .975, .99]))","94341a53":"plt.figure(figsize=(16,12))\nplt.suptitle('Transaction Values Distribution', fontsize=22)\nplt.subplot(221)\ng = sns.distplot(train['TransactionAmt'])\ng.set_title(\"Transaction Amount\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\ng1 = sns.distplot(np.log(train['TransactionAmt']))\ng1.set_title(\"Transaction Amount (Log) Distribuition\", fontsize=18)\ng1.set_xlabel(\"\")\ng1.set_ylabel(\"Probability\", fontsize=15)\n\nplt.show()","a65e8f8b":"\nplt.figure(figsize=(14,10))\nplt.title('ProductCD Distributions', fontsize=22)\nplt.subplot(221)\ng = sns.countplot(x='ProductCD', data=train)\ng.set_title(\"ProductCD Distribution\", fontsize=19)\ng.set_xlabel(\"ProductCD Name\", fontsize=17)\ng.set_ylabel(\"Count\", fontsize=17)\n\nplt.subplot(222)\ng1 = sns.countplot(x='ProductCD', hue='isFraud', data=train)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\n\ng1.set_title(\"Product CD by Target(isFraud)\", fontsize=19)\ng1.set_xlabel(\"ProductCD Name\", fontsize=17)\ng1.set_ylabel(\"Count\", fontsize=17)\n\nplt.show()","5e0829dd":"resumetable(train[['card1', 'card2', 'card3','card4', 'card5', 'card6']])","038451db":"corrfunc(train,['card1','card2','card3','card5'])","7db68fc6":"plt.figure(figsize=(14,22))\nplt.subplot(411)\ng = sns.distplot(train[train['isFraud'] == 1]['card1'], label='Fraud')  \ng = sns.distplot(train[train['isFraud'] == 0]['card1'], label='NoFraud')\ng.legend()\ng.set_title(\"Card 1 Values Distribution by Target\", fontsize=16)\ng.set_xlabel(\"Card 1 Values\", fontsize=12)\ng.set_ylabel(\"Probability\", fontsize=18)\n\nplt.subplot(412)\ng1 = sns.distplot(train[train['isFraud'] == 1]['card2'].dropna(), label='Fraud')\ng1 = sns.distplot(train[train['isFraud'] == 0]['card2'].dropna(), label='NoFraud')\ng1.legend()\ng1.set_title(\"Card 2 Values Distribution by Target\", fontsize=18)\ng1.set_xlabel(\"Card 2 Values\", fontsize=12)\ng1.set_ylabel(\"Probability\", fontsize=18)\n\nplt.subplot(413)\ng3 = sns.distplot(train[train['isFraud']==1]['card3'].dropna(),label='Fraud')\ng3 = sns.distplot(train[train['isFraud']==0]['card3'].dropna(),label='NotFraud')\ng3.legend()\ng3.set_title('Card3 values Distibution by Target' , fontsize = 18)\ng3.set_xlabel('Card3 Values' ,fontsize=12)\ng3.set_ylabel('Probability' ,fontsize=18)\n\nplt.subplot(414)\ng4=sns.distplot(train[train['isFraud']==1]['card5'].dropna() , label='Fraud' )\ng4=sns.distplot(train[train['isFraud']==1]['card5'].dropna() , label='Fraud' )\ng4.legend()\ng4.set_title('Card5 values Distibution by Target' , fontsize = 18)\ng4.set_xlabel('Card5 Values' ,fontsize=12)\ng4.set_ylabel('Probability' ,fontsize=18)\n\n\nplt.show()\n\n","ffe10eec":"plt.figure(figsize=(14,12))\nplt.subplot(211)\ng=sns.countplot(x='card4' , data = train)\ng.set_title(\"Card4 Distribution\", fontsize=19)\ng.set_xlabel(\"Card4 Category Names\", fontsize=17)\ng.set_ylabel(\"Count\", fontsize=17)\n\nplt.subplot(212)\ng2=sns.countplot(x='card6' , data = train)\ng2.set_title(\"Card6 Distribution\", fontsize=19)\ng2.set_xlabel(\"Card6 Category Names\", fontsize=17)\ng2.set_ylabel(\"Count\", fontsize=17)\nplt.show()","482bbbdd":"for col in ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']:   \n    plt.figure(figsize=(5,5))\n    g =sns.countplot(x=col , data = train)\n    g.set_title(col+ \" Distribution\", fontsize=19)\n    g.set_xlabel(col+ \" Category Names\", fontsize=17)\n    g.set_ylabel(\"Count\", fontsize=17)\n    plt.show()","c045df04":"data_null = train.isnull().sum()\/len(train) * 100\ndata_null = data_null.drop(data_null[data_null == 0].index).sort_values(ascending=False)[:500]\n\nmissing_data = pd.DataFrame({'Missing Ratio': data_null})\nprint(missing_data.shape)\nmissing_data.head()\n","e48048d7":"# find attributes with more than 90 percent missing vaules \ndef get_useless_columns(data):\n    \n    too_many_null = [col for col in data.columns if data[col].isnull().sum() \/ data.shape[0] > 0.80]\n    print(\"More than 80% null columns: \" + str(len(too_many_null)))\n    \n#     too_many_rpeated_values = [col for col in data.columns if data[col].value_counts(dropna=False \n#                 ,normalize =True).values[0] >0.90]\n    \n#     print(\"More than 90% repeated value columns: \" + str(len(too_many_rpeated_values)))\n    \n    cols_to_drop = list(set(too_many_null))# + too_many_rpeated_values))\n   # cols_to_drop.remove('isFraud')\n    return cols_to_drop\n\n\n\ncols_to_drop = get_useless_columns(train)\nprint(cols_to_drop)","850a6a83":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\nus_emails = ['gmail', 'net', 'edu']\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    X_test[c + '_bin'] = X_test[c].map(emails)\n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    X_test[c + '_suffix'] = X_test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    X_test[c + '_suffix'] = X_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n\n","4f62de3e":"Y =train['isFraud'].copy()\n\ntrain =train.drop(['isFraud'],axis=1)\n\n\nx_train_reduced = train.drop(cols_to_drop , axis=1)\n# x_vaild_reduced = x_valid_full.drop(cols_to_drop, axis=1)\n\nX_test_reduced = X_test.drop(cols_to_drop , axis = 1)\n\n# # del x_train_full\n\n\n","df9795cc":"print(x_train_reduced.shape , X_test_reduced.shape)    \nx_train_reduced.head()\n","f6127bb8":"#  separete categorical values from numerical values\n\ncategorical_col = [cname for cname in x_train_reduced.columns if   #x_train_reduced[cname].nunique()<15 and \n                   x_train_reduced[cname].dtype=='object']\n\nprint(categorical_col)\n\n\nnumerical_col = [cname for cname in x_train_reduced.columns \n                 \n                 if x_train_reduced[cname].dtype!='object']\n# print(numerical_col) \nprint(len(numerical_col)+len(categorical_col))\n\n\nmy_col=categorical_col+numerical_col\n\n#  now we have total cols \nX_train = x_train_reduced[my_col].copy()\n# X_vaild=x_vaild_reduced[my_col].copy()\nX_test = X_test_reduced[my_col].copy()","89c32ae9":"print(X_train.shape , X_test.shape)","7f5b6396":"\nnumircal_imputer = SimpleImputer(strategy='mean')\n\ncat_encoder= LabelEncoder()\ncat_imputer = SimpleImputer(strategy='most_frequent')\n\n\n#impute numerical values  \n\nx_train_imputed_numerical = pd.DataFrame(numircal_imputer.fit_transform(X_train[numerical_col]))\n# x_vaild_imputed_numerical = pd.DataFrame(numircal_imputer.transform(X_vaild[numerical_col]))\nx_test_imputed_numerical = pd.DataFrame(numircal_imputer.transform(X_test[numerical_col]))\n\nx_train_imputed_numerical.columns= X_train[numerical_col].columns\n# x_vaild_imputed_numerical.columns=X_vaild[numerical_col].columns\nx_test_imputed_numerical.columns=X_test[numerical_col].columns\n\n\n# impute cat values \n\nx_train_imputed_cat = pd.DataFrame(cat_imputer.fit_transform(X_train[categorical_col]))\n# x_vaild_imputed_cat = pd.DataFrame(cat_imputer.transform(X_vaild[categorical_col]))\nx_test_imputed_cat = pd.DataFrame(cat_imputer.transform(X_test[categorical_col]))\n\nx_train_imputed_cat.columns = X_train[categorical_col].columns\n# x_vaild_imputed_cat.columns = X_vaild[categorical_col].columns\nx_test_imputed_cat.columns = X_test[categorical_col].columns\n\n# encode categical variables \n\nmy_encoder = LabelEncoder()\n\n\nfor col in categorical_col:\n    my_encoder.fit(list(x_train_imputed_cat[col].values) + list(x_test_imputed_cat[col].values))\n    x_train_imputed_cat[col] = my_encoder.transform(x_train_imputed_cat[col])\n#     x_vaild_imputed_cat[col] = my_encoder.transform(x_vaild_imputed_cat[col])\n    x_test_imputed_cat[col]  = my_encoder.transform(x_test_imputed_cat[col])\ndel X_train ,X_test\n","2f160d54":"X_train = pd.concat([x_train_imputed_numerical , x_train_imputed_cat] ,axis=1)\n\nX_test  = pd.concat([x_test_imputed_numerical , x_test_imputed_cat] ,axis=1)\n\nprint(X_train.shape , X_test.shape)","42b8d58c":"del X_test_reduced , c , cat_encoder ,cat_imputer , categorical_col , col , cols_to_drop , corrfunc ,data \ndel emails , g , g1 , g2 , g3 ,g4 , get_useless_columns ,gridspec , missing_data\ndel my_encoder , my_col ,numerical_col , numircal_imputer , train\ndel x_test_imputed_cat ,x_test_imputed_numerical ,x_train_imputed_cat ,x_train_reduced\ndel data_null","6f0e8442":"whos","7c7596ca":"X_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","9019374a":"X1_train , X1_valid ,y_train , y_vaild = train_test_split(X_train , Y ,train_size = 0.8 ,test_size=0.2)","075f702f":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=100,verbose=1)\nmodel.fit(X1_train , y_train)\n\npred=model.predict(X1_valid)\nprint(accuracy_score(y_vaild, pred))\nsample_submission = pd.read_csv(\"..\/input\/ieee-fraud-detection\/sample_submission.csv\",index_col='TransactionID')\nsample_submission['isFraud']=model.predict(X_test)\nsample_submission.to_csv('randomForest.csv')\n","a0e8d2ce":"from keras.regularizers import l2\nimport tensorflow as tf","01ac96f0":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(10, activation=tf.nn.sigmoid , activity_regularizer=l2(0.1)))\nmodel.add(tf.keras.layers.Dense(4, activation=tf.nn.sigmoid , activity_regularizer = l2(0.01)))\nmodel.add(tf.keras.layers.Dense(1, activation=tf.nn.relu))\n\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\"  , metrics=['accuracy'])\n\nmodel.fit(X1_train.values ,y_train.values, epochs=10 , batch_size=100)\n","858822f3":"print(model.evaluate(X1_valid, y_vaild))","9f1f8be0":"sample_submission['isFraud']=model.predict(X_test)\nsample_submission.to_csv('MLP no PCA with regularization L2-2.csv')\n                         \n","ad9fa57e":"whos","96a9fc2d":"del LabelEncoder ,RandomForestClassifier ,tf ,model","f006a613":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis()\n\nlda.fit(X1_train ,y_train)\n\npred=lda.predict(X1_valid)\nprint(accuracy_score(y_vaild, pred))\n\nsample_submission['isFraud']=lda.predict(X_test)\nsample_submission.to_csv('LDA.csv')\n                         \n\n","7a265691":"X1_train = lda.transform(X1_train)\nX1_test = lda.transform(X_test)","320302d3":"X1_test.shape","cac3f666":"from sklearn import svm\n\nmodel = svm.SVC(kernel='linear' , C=0.3 ,verbose=True)\nmodel.fit(X1_train, y_train)\n# preds = model.predict(X1_valid)\n# print(model.score(y_valid, preds))\nsample_submission['isFraud']=model.predict(X1_test)\nsample_submission.to_csv('SVM.csv')","ce235873":"del lda , X1_test , X1_train ,model","6bed9fae":"from sklearn.decomposition import PCA\n# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()    # normalize data before PCA\n# train_scaled = scaler.fit_transform(X_train)  \n\n\npca = PCA(n_components=260)\nX_train = pca.fit_transform(X_train)\nprint(pca.n_components_ )\n\nX_train = pd.DataFrame(X_train)\nX_train=reduce_mem_usage(X_train)\n\nX_test = pca.transform(X_test)\nX_test = pd.DataFrame(X_test)\nX_test = reduce_mem_usage(X_test)","86654122":"X1_train , X1_valid ,y_train , y_vaild = train_test_split(X_train , Y ,train_size = 0.8 ,test_size=0.2)","397d707c":"%%time\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nmodel = DecisionTreeClassifier()\nmodel.fit(X1_train, y_train)\npreds = model.predict(X1_valid)\nprint(accuracy_score(y_vaild, preds))\n\n# print(model.score(y_vaild, preds))\n# cross_val_score(model, X_train ,Y, cv=10)\n","33dd18fc":"print(accuracy_score(y_vaild, preds))","f210d341":"sample_submission['isFraud']=model.predict(X_test)\nsample_submission.to_csv('decisionTree with PCA 260.csv')","7f0b41fc":"free some memory space","5dec8200":"##  separete categorical values from numerical values\n","9c26d6b6":"# code to load data and merge them \nthis cell will be run only once and we then restart the kernel to save some memory space \nafter restarting the kernel we load the merged data directly and start working \n","956d7ddd":"# exploring the data ","0a89efac":"## find columns with missing data>80%\n\nso we drop these columns as they many nulls","4581ef64":"# Models for classification ","29695847":"## Exploring M1-M9 Features\n","39a46c6e":"# Data\n\nIn this competition we are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n\nThe data is broken into two files **identity ** and **transaction**, which are joined by TransactionID.","35128db9":"## Target distribution (isFraud distripution)","5e25f50a":"## Decision Tree with PCA","ddcaf340":"##  fisher linear discriminant analysis LDA","60403983":"## support vector machines with LDA\n","90da81b4":"notice that it is imbalanced ","394a1e8d":"## Product Feature","38dd9cd1":"## handle email domain ","4af75a9e":"## code to load the merged data","d6dc1267":"## Card Features","514c001d":"# Start from here \nafter restarting the kernel start from here to load merged data \n\n     \n\n### Categorical Features - Transaction\n\n    ProductCD\n    emaildomain\n    card1 - card6\n    addr1, addr2\n    P_emaildomain\n    R_emaildomain\n    M1 - M9   \n###  Categorical Features - Identity\n\n    DeviceType\n    DeviceInfo\n    id_12 - id_38\n\nThe TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).","fbbac7cc":"### import things and functions used ","55ca2350":"## encode categorical columns and impute the missing values in numerical columns \nuse label encoder with most frequent values for categorical columns and imputer with mean values for numerical values","82467423":"## feature reduction with PCA","9f890a4d":"# feature engineering ","5abdcc6c":"## Multi Layer Perceptron","dd75ce14":"## trasaction amount feature"}}