{"cell_type":{"1cdc8e75":"code","d14f632b":"code","3022f8b6":"code","45739ad2":"code","2d01c479":"code","8ffb2608":"code","08b0c643":"code","a30433d6":"code","2bc814d6":"code","6a3802f0":"code","449334bb":"code","66d4c9a7":"markdown","8938ef15":"markdown","e83da710":"markdown","c99bba52":"markdown","de726a47":"markdown","3f5021fc":"markdown"},"source":{"1cdc8e75":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler,LabelEncoder,OneHotEncoder\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,KFold,train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cbt\nimport datetime\nimport time\nimport warnings\nimport gc\nwarnings.filterwarnings(\"ignore\")\nsns.set(style=\"whitegrid\",color_codes=True)\nsns.set(font_scale=1)\n%time","d14f632b":"#reduce mem cost\n%time\ndef reduce_mem_cost(df,verbose = True):\n    num_type = ['int16','int32','int64','float32','float64','object']\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in num_type:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            elif str(col_type)[:5] == 'float':\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df       ","3022f8b6":"%time\ntrain_data = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\")\ntrain_target = train_data.pop(\"target\")\ntrain_data = reduce_mem_cost(train_data)\ntest_data = reduce_mem_cost(test_data)","45739ad2":"#\u67e5\u770b\u6570\u636e\ntrain_data.info()","2d01c479":"%time\nlabel_encoder_cols = []\nno_label_encoder_cols = []\nfor col in train_data.columns:\n    if train_data[col].dtypes == \"int32\" or train_data[col].dtypes == \"int8\":\n        no_label_encoder_cols.append(col)\n    else:\n        label_encoder_cols.append(col)","8ffb2608":"%time\n# label encoder on some cols\ntrain_id = train_data.pop(\"id\")\ntest_id = test_data.pop(\"id\")\nall_data = pd.concat([train_data,test_data],axis=0)\nall_data[label_encoder_cols]","08b0c643":"%time\n#generate  count feats\nbin_3_cnt = all_data[\"bin_3\"].value_counts().to_dict()\nall_data[\"bin_3_cnt\"] = all_data[\"bin_3\"].map(bin_3_cnt)\n\nbin_4_cnt = all_data[\"bin_4\"].value_counts().to_dict()\nall_data[\"bin_4_cnt\"] = all_data[\"bin_4\"].map(bin_4_cnt)\n\nnom_0_cnt = all_data[\"nom_0\"].value_counts().to_dict()\nall_data[\"nom_0_cnt\"] = all_data[\"nom_0\"].map(nom_0_cnt)\n\nnom_1_cnt = all_data[\"nom_1\"].value_counts().to_dict()\nall_data[\"nom_1_cnt\"] = all_data[\"nom_1\"].map(nom_1_cnt)\n\nnom_2_cnt = all_data[\"nom_2\"].value_counts().to_dict()\nall_data[\"nom_2_cnt\"] = all_data[\"nom_2\"].map(nom_2_cnt)\n\nnom_3_cnt = all_data[\"nom_3\"].value_counts().to_dict()\nall_data[\"nom_3_cnt\"] = all_data[\"nom_3\"].map(nom_3_cnt)\n\nnom_4_cnt = all_data[\"nom_4\"].value_counts().to_dict()\nall_data[\"nom_4_cnt\"] = all_data[\"nom_4\"].map(nom_4_cnt)\n\nnom_5_cnt = all_data[\"nom_5\"].value_counts().to_dict()\nall_data[\"nom_5_cnt\"] = all_data[\"nom_5\"].map(nom_5_cnt)\n\nnom_6_cnt = all_data[\"nom_6\"].value_counts().to_dict()\nall_data[\"nom_6_cnt\"] = all_data[\"nom_6\"].map(nom_6_cnt)\n\nnom_7_cnt = all_data[\"nom_7\"].value_counts().to_dict()\nall_data[\"nom_7_cnt\"] = all_data[\"nom_7\"].map(nom_7_cnt)\n\nnom_8_cnt = all_data[\"nom_8\"].value_counts().to_dict()\nall_data[\"nom_8_cnt\"] = all_data[\"nom_8\"].map(nom_8_cnt)\n\nnom_9_cnt = all_data[\"nom_9\"].value_counts().to_dict()\nall_data[\"nom_9_cnt\"] = all_data[\"nom_9\"].map(nom_9_cnt)\n\nord_1_cnt = all_data[\"ord_1\"].value_counts().to_dict()\nall_data[\"ord_1_cnt\"] = all_data[\"ord_1\"].map(ord_1_cnt)\n\nord_2_cnt = all_data[\"ord_2\"].value_counts().to_dict()\nall_data[\"ord_2_cnt\"] = all_data[\"ord_2\"].map(ord_2_cnt)\n\nord_3_cnt = all_data[\"ord_3\"].value_counts().to_dict()\nall_data[\"ord_3_cnt\"] = all_data[\"ord_3\"].map(ord_3_cnt)\n\nord_4_cnt = all_data[\"ord_4\"].value_counts().to_dict()\nall_data[\"ord_4_cnt\"] = all_data[\"ord_4\"].map(ord_4_cnt)\n\nord_5_cnt = all_data[\"ord_5\"].value_counts().to_dict()\nall_data[\"ord_5_cnt\"] = all_data[\"ord_5\"].map(ord_5_cnt)\n\nall_data","a30433d6":"%time\n#label encoder on some cols\nfor le in label_encoder_cols:\n    le_feat = LabelEncoder()\n    le_feat.fit(all_data[le])\n    all_data[le] = le_feat.transform(all_data[le])","2bc814d6":"#split all data into train and test\ntrain_data = all_data[:train_data.shape[0]]\ntest_data = all_data[train_data.shape[0]:]","6a3802f0":"%time\ntrain_x,train_y,testX = train_data.values,train_target.values,test_data.values","449334bb":"start = time.time()\nmodel = lgb.LGBMClassifier(boosting_type=\"gbdt\",num_leaves=48, max_depth=-1, learning_rate=0.05,\n                               n_estimators=3000, subsample_for_bin=50000,objective=\"binary\",min_split_gain=0, min_child_weight=5, min_child_samples=30, #10\n                               subsample=0.8,subsample_freq=1, colsample_bytree=1, reg_alpha=3,reg_lambda=5,\n                               feature_fraction= 0.9, bagging_fraction = 0.9,\n                               seed= 2019,n_jobs=10,slient=True,num_boost_round=3000)\nn_splits = 3\nrandom_seed = 2019\nskf = StratifiedKFold(shuffle = True, random_state = random_seed, n_splits = n_splits)\ncv_pred = []\nval_score = []\nfor idx, (tra_idx,val_idx) in enumerate(skf.split(train_x,train_y)):\n    startTime = time.time()\n    print(\"============================================fold_{}===================================================\".format(str(idx+1)))\n    X_train,Y_train = train_x[tra_idx],train_y[tra_idx]\n    X_val,Y_val = train_x[val_idx], train_y[val_idx]\n    lgb_model = model.fit(X_train,Y_train,eval_names=[\"train\",\"valid\"],eval_metric=[\"logloss\"],eval_set=[(X_train, Y_train),(X_val,Y_val)],early_stopping_rounds=200)\n    val_pred = lgb_model.predict(X_val,num_iteration = lgb_model.best_iteration_)\n    val_score.append(f1_score(Y_val,val_pred))\n    print(\"f1_score:\",f1_score(Y_val, val_pred))\n    test_pred = lgb_model.predict(testX, num_iteration = lgb_model.best_iteration_).astype(int)\n    cv_pred.append(test_pred)\n    endTime = time.time()\n    print(\"fold_{} finished in {}\".format(str(idx+1), datetime.timedelta(seconds= endTime-startTime)))\n    \nend = time.time()\nprint('-'*60)\nprint(\"Training has finished.\")\nprint(\"Total training time is {}\".format(str(datetime.timedelta(seconds=end-start))))\nprint(val_score)\nprint(\"mean f1:\",np.mean(val_score))\nprint('-'*60)\n\nsubmit = []\nfor line in np.array(cv_pred).transpose():\n    submit.append(np.argmax(np.bincount(line)))\nfinal_result = pd.DataFrame(columns=[\"id\",\"target\"])\nfinal_result[\"id\"] = list(test_id.unique())\nfinal_result[\"target\"] = submit\nfinal_result.to_csv(\"submitLGB{0}.csv\".format(datetime.datetime.now().strftime(\"%Y%m%d%H%M\")),index = False)\nprint(final_result.head())       ","66d4c9a7":"### import packages","8938ef15":"### EDA","e83da710":"### split data into train data and test data","c99bba52":"### feature engniees","de726a47":"### load data ","3f5021fc":"### model build"}}