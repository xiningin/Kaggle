{"cell_type":{"8ae6dad9":"code","38a945ac":"code","8eef1b3d":"code","481aecef":"code","8cb57275":"code","1e860009":"code","9743bc4b":"code","bdd0ca39":"code","6c6a2039":"code","77d5dd35":"code","69a57343":"code","cc8cb847":"code","0d17f06e":"code","11c4ee52":"code","bf9de574":"code","23ad9e17":"code","f4ed0a2f":"code","6746cb66":"markdown","f6042051":"markdown","844efce6":"markdown","692f0f48":"markdown","6fd760a2":"markdown","e842dcb0":"markdown","aa9f29ab":"markdown","7976d84a":"markdown","16975914":"markdown","44a70bb9":"markdown","c78b8347":"markdown","a3030415":"markdown","5a55a1b2":"markdown","a3b6fc88":"markdown","c81c533e":"markdown","d24c1a91":"markdown","27ba992e":"markdown","774cb580":"markdown","26451ca3":"markdown","d4d52846":"markdown","c59cf1db":"markdown","e948e270":"markdown","44af8afb":"markdown"},"source":{"8ae6dad9":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import balanced_accuracy_score","38a945ac":"APP_INPUT_LIB  = \"..\/input\/data-for-investing-type-prediction\"\nAPP_INPUT_FILE = \"investing_program_prediction_data.csv\"\n\nAPP_TARGET_FEATURE = \"InvType\"\nAPP_IDENT_FEATURE  = \"ID\"\n\ndata = pd.read_csv(os.path.join(APP_INPUT_LIB,APP_INPUT_FILE))\ndata.insert(0,'ID', ['ID' + str(i)  for i in np.arange(len(data))])","8eef1b3d":"display_settings = {'display.max_rows' : 20 , 'display.max_columns' : 50 , 'display.width' : 200}\nfor op,value in display_settings.items():\n    pd.set_option(op,value)","481aecef":"# Print data dimensions\nprint(\"Data dimensions :\")\nprint(data.shape)\n# List of columns\nprint(\"List of columns :\")\nprint(data.columns)","8cb57275":"# Create feature groups\nse_features = [f for f in data.columns if 'SE' in f]\nba_features = [f for f in data.columns if 'BA' in f]\npe_features = [f for f in data.columns if 'PE' in f]\nia_features = [f for f in data.columns if 'IA' in f]\n\n#### List sample customer data - Age and Geographic location\nprint(\"Sample customer data :\")\nprint(data[se_features].head())\n\n#### List sample banking activity data\nprint(\"Sample banking activity data :\")\nprint(data[ba_features].head())\n\n#### List sample investing history data\nprint(\"Sample investing portfolio data :\")\nprint(data[pe_features].head())\n\n#### List sample investing activity data\nprint(\"Sample investing activity data :\")\nprint(data[ia_features].head())","1e860009":"fig = plt.figure(figsize=(10,10))\nsns.set(style=\"darkgrid\")\nax = sns.countplot(x = APP_TARGET_FEATURE , data = data).set_title(APP_TARGET_FEATURE + \" \" + \"Distribution\")","9743bc4b":"label_encoders = dict()\n\nfor f in data.columns :\n    if (f == APP_IDENT_FEATURE):\n        continue\n    f_type = data[f].dtype.name\n    if (f_type == 'object'):\n        f_enc = LabelEncoder() ; data[f] = f_enc.fit_transform(data[f]) ; label_encoders[f] = f_enc","bdd0ca39":"from sklearn.model_selection import RepeatedStratifiedKFold\nnum_folds = 3 ; num_rep = 1 ; splits = dict()\nrkfs_partitioner  = RepeatedStratifiedKFold(n_splits = num_folds , n_repeats = num_rep , random_state = 1234)\nsplit_cnt = 0\nfor train_idxs , test_idxs in rkfs_partitioner.split(data , data[APP_TARGET_FEATURE]):\n    # print(\"TRAIN:\", len(train_idxs), \"TEST:\", len(test_idxs))\n    i_rep  = split_cnt \/\/ num_folds\n    i_part = split_cnt - i_rep * num_folds\n    # print(cnt) ; print(i_rep) ; print(i_part)\n    splits['R' + str(i_rep) + 'P' + str(i_part)] = {'Train' : train_idxs , 'Test' : test_idxs}\n    split_cnt += 1","6c6a2039":"def report_feature_importance(i_split , feature_names , model):\n    import numpy as np\n    import pandas as pd\n    \n    feature_importance = model.feature_importances_\n    indices = np.argsort(feature_importance)[::-1]\n    i_feature_importance_df               = pd.DataFrame()\n    i_feature_importance_df['Split']      = [i_split for f in range(len(feature_names))]\n    i_feature_importance_df['Rank']       = [f for f in range(len(feature_names))]\n    i_feature_importance_df['Feature']    = feature_names[indices]  \n    i_feature_importance_df['Importance'] = feature_importance[indices]\n    \n    return i_feature_importance_df","77d5dd35":"performance_metrics    = dict()\nfeature_importance_df   = pd.DataFrame()\n\nfor i_split in splits.keys() :\n    train_data = data.loc[splits[i_split]['Train']]\n    X_train    = train_data[[i for i in train_data.columns if i not in [APP_IDENT_FEATURE , APP_TARGET_FEATURE]]]\n    y_train    = train_data[APP_TARGET_FEATURE]\n    i_model    = RandomForestClassifier(n_estimators = 100 ,  max_features = 'sqrt' , max_depth = 15 , random_state = 0) \n    i_model.fit(X_train,y_train)\n    i_feature_importance_df = report_feature_importance(i_split , X_train.columns , i_model)\n    feature_importance_df   = pd.concat([feature_importance_df, i_feature_importance_df])\n    test_data = data.loc[splits[i_split]['Test']]\n    X_test    = test_data[[i for i in train_data.columns if i not in [APP_IDENT_FEATURE , APP_TARGET_FEATURE]]]\n    y_test    = test_data[APP_TARGET_FEATURE]\n    y_test_pred = i_model.predict(X_test).flatten() \n    performance_metrics[i_split] = balanced_accuracy_score(y_test,y_test_pred)","69a57343":"print(\"Average Balanced accuracy is : %1.3f\" % np.mean(list(performance_metrics.values())))\nprint(\"10 features with highest average importance\")\nprint(feature_importance_df.groupby(['Feature'])['Importance'].mean().sort_values(ascending = False).head(10).reset_index())","cc8cb847":"import featuretools as ft\nimport featuretools.variable_types as vtypes\nprint(ft.__version__)\n\nprimitives = ft.list_primitives() # Or https:\/\/primitives.featurelabs.com\/\nprint(primitives[primitives['type'] == 'transform'].head(5))\nprint(primitives[primitives['type'] == 'aggregation'].head(5))","0d17f06e":"def ftfe_create_train_num_op(X_train_num_op , fe_input_num_op , transform_primitives_set):\n    \n    import featuretools as ft\n    import featuretools.variable_types as vtypes\n\n    # Creating train entity set\n    es_train = ft.EntitySet(id = 'invtype')\n    # Adding dataframe for numeric operations\n    es_train.entity_from_dataframe(entity_id = 'invtype_num_op', dataframe = X_train_num_op , index = 'ID')\n    \n    #print(es_train[\"invtype_num_op\"].variables)\n    \n    feature_matrix_train, feature_matrix_names_train = ft.dfs(entityset = es_train, target_entity = 'invtype_num_op', \n                                                                agg_primitives = None ,\n                                                                trans_primitives = transform_primitives_set , \n                                                                max_depth = 2, n_jobs = -1, chunk_size = 100 , max_features = 100 \n                                                               )\n    #print(\"feature_matrix_train :\") ; print(feature_matrix_train)\n    \n    feature_data_train , feature_data_names_train = post_process('TRAIN' , feature_matrix_train)\n    \n        \n    #print(\"feature_data_train after preprocess :\") ; print(feature_data_train)\n    feature_data_train  = feature_data_train[[x for x in feature_data_train.columns if x not in fe_input_num_op]]\n    feature_data_names_train = [x for x in feature_data_train.columns if x not in fe_input_num_op]\n    #print(\"Final feature_data_train :\") ; print(feature_data_train)\n    \n    feature_matrix_train_enc, features_train_enc = ft.encode_features(feature_matrix_train, feature_matrix_names_train, include_unknown = False)\n    \n    return feature_data_train , features_train_enc , feature_data_names_train","11c4ee52":"def post_process(run_mode , feature_matrix, missing_threshold = 0.95, correlation_threshold = 0.95):\n    \n    print('Run mode : {}.'.format(run_mode))\n    if(run_mode == 'TRAIN'):\n        print('Feature matrix post processing missing_threshold : {} , correlation_threshold : {}'.format(missing_threshold,  correlation_threshold))\n    print('Dimensionality before post processing : {}.'.format(feature_matrix.shape))\n    \n    #### Remove duplicated features\n    start_features = feature_matrix.shape[1]\n    feature_matrix = feature_matrix.iloc[:, ~feature_matrix.columns.duplicated()]\n    n_duplicated   = start_features - feature_matrix.shape[1]\n    print(f'Number of duplicated features : {n_duplicated}')\n    \n    #### Replace infinity values with missing values\n    feature_matrix = feature_matrix.replace({np.inf: np.nan, -np.inf:np.nan}).reset_index()\n    \n    #### Treat features with missing values\n    # Missing values statistics\n    missing = pd.DataFrame(feature_matrix.isnull().sum())\n    missing['fraction'] = missing[0] \/ feature_matrix.shape[0]\n    missing.sort_values('fraction', ascending = False, inplace = True)\n    # Missing above threshold\n    missing_cols = list(missing[missing['fraction'] > missing_threshold].index)\n    n_missing_cols = len(missing_cols)\n    # Remove missing columns\n    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in missing_cols]]\n    print('Number of features with missing values above {} : {}'.format(missing_threshold , n_missing_cols))\n    \n    # Fill missing values with 0\n    feature_matrix.fillna(0 , inplace = True)\n    \n    if(run_mode == 'TEST'):\n        return feature_matrix\n\n    #### Treat Zero variance features\n    # Variance statistics\n    unique_counts = pd.DataFrame(feature_matrix.nunique()).sort_values(0, ascending = True)\n    zero_variance_cols = list(unique_counts[unique_counts[0] == 1].index)\n    n_zero_variance_cols = len(zero_variance_cols)\n    # Remove zero variance features\n    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in zero_variance_cols]]\n    print('Number of zero variance features : {}'.format(n_zero_variance_cols))\n    \n    #### Treat highly correlated features\n    # Calculate Correlations\n    corr_matrix = feature_matrix.corr()\n    # Extract the upper triangle of the correlation matrix\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n    # print(upper)\n\n    # Select the features with abolute correlation value above the threshold\n    to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n    # print(to_drop)\n    n_collinear = len(to_drop)\n    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in to_drop]]\n    print('Highly correlated columns removed with correlation above {} : {} '.format(correlation_threshold , n_collinear))\n    \n    total_removed = n_duplicated, n_missing_cols + n_zero_variance_cols + n_collinear\n    \n    print('Total columns removed: ', total_removed)\n    print('Dimensionality after post processing: {}'.format(feature_matrix.shape))\n    \n    feature_names = feature_matrix.columns\n    \n    return feature_matrix ,feature_names\n    ","bf9de574":"def ftfe_create_test_num_op(X_test , features_train_enc , feature_data_names_train):\n    \n    import featuretools as ft\n    import featuretools.variable_types as vtypes\n    \n    # Creating test entity set\n    es_test = ft.EntitySet(id = 'invtype')\n    # Adding dataframe for numeric operations \n    es_test.entity_from_dataframe(entity_id = 'invtype_num_op', dataframe = X_test , index = 'ID')\n    feature_matrix_test = ft.calculate_feature_matrix(features = features_train_enc , entityset = es_test , n_jobs = 1)\n\n    #print(\"feature_matrix_test :\") ; print(feature_matrix_test)\n    \n    feature_data_test = feature_matrix_test[[x for x in feature_matrix_test.columns if x in feature_data_names_train]]\n    #print(feature_data_test)\n    feature_data_test = post_process('TEST' , feature_data_test)\n    \n    \n    #print(\"Final feature_data_test :\") ; print(feature_data_test)\n    \n    return feature_data_test","23ad9e17":"# Top 3 important business activity input features\nfe_input_num_op          = ['BA3','BA7','BA4']\n# Top 4 feaures regardless business meaning\nfe_input_num_op          = ['BA3','BA7','BA4','SE1']\n# Top 2 important business activity input features\nfe_input_num_op          = ['BA3','BA7']\n\ntransform_primitives_set = ['subtract_numeric','divide_numeric']","f4ed0a2f":"performance_metrics    = dict()\nfeature_importance_df   = pd.DataFrame()\nfor i_split in splits.keys() :\n    # i_split = 'R0P0'\n    train_data = data.loc[splits[i_split]['Train']]\n    X_train    = train_data[[i for i in train_data.columns if i not in [APP_TARGET_FEATURE]]]\n    y_train    = train_data[APP_TARGET_FEATURE]\n    \n    # Add train num op features\n    X_train_num_op    = train_data[[APP_IDENT_FEATURE] + fe_input_num_op]\n    ftfe_train_num_op, ftfe_train_num_op_enc, ftfe_train_num_op_fnames  = ftfe_create_train_num_op(X_train_num_op , fe_input_num_op , transform_primitives_set)\n    print(\"num_op :{0} features added\".format(len(ftfe_train_num_op_fnames)))\n    if(len(ftfe_train_num_op_fnames) > 1):\n        X_train  = pd.merge(X_train ,ftfe_train_num_op , on = ['ID'] , how = 'inner' ).drop(APP_IDENT_FEATURE , axis=1)\n\n    i_model  = RandomForestClassifier(n_estimators = 100 ,  max_features = 'sqrt' , max_depth = 15 , random_state = 0) \n    i_model.fit(X_train,y_train)\n    \n    i_feature_importance_df = report_feature_importance(i_split , X_train.columns , i_model)\n    feature_importance_df   = pd.concat([feature_importance_df, i_feature_importance_df])\n    \n    test_data = data.loc[splits[i_split]['Test']]\n    X_test    = test_data[[i for i in test_data.columns if i not in [APP_TARGET_FEATURE]]]\n    y_test    = test_data[APP_TARGET_FEATURE]\n\n    # Add test num op features    \n    X_test_num_op = test_data[[APP_IDENT_FEATURE] + fe_input_num_op]\n    ftfe_test_num_op = ftfe_create_test_num_op(X_test_num_op , ftfe_train_num_op_enc , ftfe_train_num_op_fnames)\n    X_test  = pd.merge(X_test ,ftfe_test_num_op , on = ['ID'] , how = 'inner' ).drop(APP_IDENT_FEATURE , axis = 1)\n    \n    y_test_pred = i_model.predict(X_test).flatten() \n    performance_metrics[i_split] = balanced_accuracy_score(y_test,y_test_pred)\n    \nprint(\"Average Balanced accuracy is : %1.3f\" % np.mean(list(performance_metrics.values())))\nprint(\"10 features with highest average importance\")\nprint(feature_importance_df.groupby(['Feature'])['Importance'].mean().sort_values(ascending = False).head(10).reset_index())","6746cb66":"Conclusion : Two classes are almost balanced","f6042051":"**Setup pandas presentation**","844efce6":"### **Prepare label encoders**","692f0f48":"### **Prepare FE on test data utility **","6fd760a2":"## ** Feature Enginnering with featuretools transform primitives**","e842dcb0":"### **Prepare FE on training data utility **","aa9f29ab":"**Distribution of target feature**","7976d84a":"### **Post process new features utility **","16975914":"### **Setup for feature enginnering **","44a70bb9":"## **Explore Data**","c78b8347":"### **Prepare for feature tools**","a3030415":"### **Prepare 3 - fold cross validation**","5a55a1b2":"### **Feature importance utility**","a3b6fc88":"### **Run new model , report performance and feature importance**","c81c533e":"## **Build baseline model**","d24c1a91":"**Imports**","27ba992e":"**Input data dimensionality**","774cb580":"## **Setup**","26451ca3":"**Describe feature groups**","d4d52846":"## **Summary**","c59cf1db":"### **Run baseline model , report performance and feature importance**","e948e270":"Featuretools as a Automated Feature Enginnering tool easily creates a large number of features that are able to improve your model. In addition , custom primitives , easy pipeline design and parallel processing makes featuretools an important contribution for Data Science and Machine learning toolbox . In this case I didn't have some intra data relationship to try aggregation metrics or categorical encoding and focus on transform primitives . I run several sets of input features for FE and best set include top 2 important features relate to business activity to gain ~ 2.5% in balanced accurscy . List of input features , list of transform primitives and dfs settings can be a part of overall optimization of algorithm along with with well  HPO. . \n\nReferences :\n\n[1] https:\/\/www.kaggle.com\/willkoehrsen\/featuretools-for-good\n\n[2] https:\/\/medium.com\/dataexplorations\/tool-review-can-featuretools-simplify-the-process-of-feature-engineering-5d165100b0c3 \n\n[3] https:\/\/towardsdatascience.com\/automated-feature-engineering-in-python-99baf11cc219 \n\n[4] https:\/\/innovation.alteryx.com\/encode-smarter\/ ","44af8afb":"**Read data**"}}