{"cell_type":{"6e4749ba":"code","16283f8d":"code","b5edb553":"code","565b9f45":"code","bfdf9b56":"code","14160d10":"code","1010afd5":"code","ac61b4d7":"code","bc2b598b":"markdown","2b03ebd2":"markdown","f4b9a8b7":"markdown","c605b37d":"markdown","0a701c46":"markdown","a5ab6d30":"markdown"},"source":{"6e4749ba":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport h2o\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nfrom h2o.grid.grid_search import H2OGridSearch\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os, gc\nprint(os.listdir(\"..\/input\"))\n\nh2o.init()\n\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\nvalid_rate = .15\ntrain_df, valid_df, tr_y, va_y = train_test_split(train_df, train_df['target'], stratify = train_df['target'], test_size=valid_rate, random_state = 42)\n\ntrain = h2o.H2OFrame(train_df)\nvalid = h2o.H2OFrame(valid_df)\ntest = h2o.import_file(\"..\/input\/test.csv\")\n\nimport gc\ndel train_df, valid_df, tr_y, va_y\ngc.collect()","16283f8d":"y = 'target'\nx = train.columns[2:]\ntrain[y] = train[y].asfactor()\n","b5edb553":"# CHANGE THIS PARAMETER to test as many models as you wish\nn_models = 8\ngrid_params = {\n    'max_depth': [2, 3],\n    'col_sample_rate': [.6, .7],\n    'learn_rate': [.09, .1],\n    'learn_rate_annealing': [1],\n    'min_rows': [110, 90],\n    'sample_rate': [.7]\n}\n\ngbm_grid = H2OGridSearch(model=h2o.estimators.H2OGradientBoostingEstimator,\n                grid_id='gbm_grid', \n                hyper_params=grid_params,\n                search_criteria={'strategy': 'RandomDiscrete', 'max_models': n_models})\n","565b9f45":"gbm_grid.train(x=x, y=y, training_frame=train, validation_frame=valid,\n            distribution='bernoulli',\n            ntrees=2500,\n            score_tree_interval = 20,\n            stopping_rounds = 4,\n            stopping_metric = \"AUC\",\n            stopping_tolerance = 1e-4,\n            seed = 1)","bfdf9b56":"\ngridperf = gbm_grid.get_grid(sort_by='auc', decreasing=True)\nbest_model = gridperf.models[0]\nfor par in grid_params:\n    if par in best_model.params:\n        print('par: ' + par); print(best_model.params[par])\n","14160d10":"best_model.scoring_history()","1010afd5":"history = pd.DataFrame(best_model.scoring_history())\nhistory.plot(x='number_of_trees', y = ['validation_auc', 'validation_pr_auc'])\n","ac61b4d7":"preds = best_model.predict(test)\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = preds['p1'].as_data_frame()\nsubmission.to_csv('gbm_submission.csv', index = False)\nsubmission.head()","bc2b598b":"Hi Kagglers ! \n\nThis is a notebook to test what you can do in a few minutes with h2o's migthy GBM. I also included some basic grid search and early stopping to make the model more competitive. There is no CV in this kernel, I simply used a train\/validation split framework. Unfortunately h2o does not allow you to correct for stratification when splitting (to my knowledge), so I rather used sklearn splitting function. \n\nA neat feature is that you can keep an eye on the scoring history along the training with scoring_history() function on many metrics of interest, not only AUC ROC.\n\nFeel free to comment, fork and upvote, happy kaggling, Cheers!\n","2b03ebd2":"# Contents\n1. [Start h2o and load the data](#step1)\n2. [Define a grid and train](#step2)\n3. [Best model](#step3)\n4. [Submission](#step4)\n","f4b9a8b7":"## Start h2o and load the data  <a name=\"step1\"><\/a>\n\nStart the h2o cluster and load train, validation and test datasets as h2oFrames. \n\nTrain\/Validation split is done with sklearn function, to correct for stratification on the target column.","c605b37d":"## Define a grid and train  <a name=\"step2\"><\/a>\n\nWith the RandomDiscrete strategy you test *n_models*, with parameters randomly chosen from the grid. \n\n2500 trees should be enough here to reach the early stopping *AUC* criteria on the validation frame.","0a701c46":"## Best model  <a name=\"step3\"><\/a>\n\nNow sort the models of the grid from decreasing order on the *auc* criteria, and keep the first. You can check what are the parameters that were used to reach the best score. A detailed history on the different metrics is informative, especially for the *AUC* of the Precision Recall curve since target column is not balanced. ","a5ab6d30":"## Submission  <a name=\"step4\"><\/a>\n\nHere's your submission csv that should get you a LB score around .897 ! "}}