{"cell_type":{"306af2db":"code","2561f69d":"markdown","e1ea6ced":"markdown"},"source":{"306af2db":"######################################\n# K\u00fct\u00fcphanelerin import edilmesi\n#####################################\n\nimport numpy as np\nimport pandas as pd\nimport gc\nimport time\nfrom contextlib import contextmanager\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport re\n\n\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n    \n    \n#######################\n# Fonksiyonlar\n#######################\n\n# veri setine genel bak\u0131\u015f\ndef check_df(dataframe):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    \n# De\u011fi\u015fkenlerin t\u00fcrlerinin belirlenmesi\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    #print(f\"Observations: {dataframe.shape[0]}\")\n    #print(f\"Variables: {dataframe.shape[1]}\")\n    #print(f'cat_cols: {len(cat_cols)}')\n    #print(f'num_cols: {len(num_cols)}')\n    #print(f'cat_but_car: {len(cat_but_car)}')\n    #print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\n# Eksik g\u00f6zlemler\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns\n    \n# Kategorik de\u011fi\u015fkenlerin incelenmesi\ndef cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"##########################################\")\n    if plot:\n        plt.figure(figsize=(10,5))\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()\n        \n# N\u00fcmerik de\u011fi\u015fkenlerin incelenmesi\ndef num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    print(dataframe[numerical_col].describe(quantiles).T)\n    if plot:\n        dataframe[numerical_col].hist(bins=20)\n        plt.xlabel(numerical_col)\n        plt.title(numerical_col)\n        plt.show()\n        \n        \n# Korelasyonlar\ndef high_correlation(data, remove=['SK_ID_CURR', 'SK_ID_BUREAU'], corr_coef=\"pearson\", corr_value = 0.7):\n    if len(remove) > 0:\n        cols = [x for x in data.columns if (x not in remove)]\n        c = data[cols].corr(method=corr_coef)\n    else:\n        c = data.corr(method=corr_coef)\n\n    for i in c.columns:\n        cr = c.loc[i].loc[(c.loc[i] >= corr_value) | (c.loc[i] <= -corr_value)].drop(i)\n        if len(cr) > 0:\n            print(i)\n            print(\"-------------------------------\")\n            print(cr.sort_values(ascending=False))\n            print(\"\\n\")\n\n# Rare Encoding\ndef rare_encoder(dataframe, rare_perc, cat_cols):\n   \n    rare_columns = [col for col in cat_cols if\n                    (dataframe[col].value_counts() \/ len(dataframe) < rare_perc).sum() > 1]\n\n    for col in rare_columns:\n        tmp = dataframe[col].value_counts() \/ len(dataframe)\n        rare_labels = tmp[tmp < rare_perc].index\n        dataframe[col] = np.where(dataframe[col].isin(rare_labels), 'Rare', dataframe[col])\n\n    return dataframe\n\n# One-hot Encoding \ndef one_hot_encoder(df, nan_as_category = True):\n    original_columns = list(df.columns)\n    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns\n\n\n#########################\n# Application_train_test\n#########################\n\ndef application_train_test(num_rows = None, nan_as_category = False):\n    # Veri setinin okutulmas\u0131\n    df = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv', nrows= num_rows)\n    test_df = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv', nrows= num_rows)\n    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n    df = df.append(test_df).reset_index()\n    \n    # Cinsiyeti belirtilmeyen 4 ki\u015fi var bunlar\u0131 \u00e7\u0131kar\u0131yoruz.\n    df = df[df['CODE_GENDER'] != 'XNA']\n    # Medeni durumu unknown olan 1 ki\u015fi var bunu droplad\u0131k.\n    df = df[df['NAME_FAMILY_STATUS'] != \"Unknown\" ]\n    # NaN values for DAYS_EMPLOYED: 365243 -> nan\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n    \n    ########\n    # RARE\n    ########\n    # NAME_INCOME_TYPE de\u011fi\u015fkeninin 4 s\u0131n\u0131f\u0131n\u0131n frekans\u0131 di\u011ferlerine g\u00f6re d\u00fc\u015f\u00fck oldu\u011funu g\u00f6zlemledik.\n    # Bu nedenle bu 4 s\u0131n\u0131f\u0131 kendilerine en yak\u0131n olabilecek s\u0131n\u0131f\u0131n i\u00e7in dahil ettik.\n    # Yani rare s\u0131n\u0131f\u0131n\u0131 di\u011ferlerine eklemi\u015f olduk.\n    df.loc[df['NAME_INCOME_TYPE'] == 'Businessman', 'NAME_INCOME_TYPE'] = 'Commercial associate'\n    df.loc[df['NAME_INCOME_TYPE'] == 'Maternity leave', 'NAME_INCOME_TYPE'] = 'Pensioner'\n    df.loc[df['NAME_INCOME_TYPE'] == 'Student', 'NAME_INCOME_TYPE'] = 'State servant'\n    df.loc[df['NAME_INCOME_TYPE'] == 'Unemployed', 'NAME_INCOME_TYPE'] = 'Pensioner'\n    \n    # ORGANIZATION_TYPE\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Business Entity\"), \n                                       \"Business_Entity\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Industry\"), \n                                       \"Industry\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Trade\"),\n                                       \"Trade\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Transport\"),\n                                       \"Transport\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"School\", \"Kindergarten\", \"University\"]),\n                                       \"Education\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Emergency\",\"Police\", \"Medicine\",\"Goverment\", \"Postal\", \"Military\", \"Security Ministries\", \"Legal Services\"]), \"Official\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Bank\", \"Insurance\"]),\n                                       \"Finance\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Goverment\"), \n                                       \"Government\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Realtor\", \"Housing\"]), \"Realty\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Hotel\", \"Restaurant\",\"Services\"]), \"TourismFoodSector\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Cleaning\",\"Electricity\", \"Telecom\", \"Mobile\", \"Advertising\", \"Religion\", \"Culture\"]), \"Other\", df[\"ORGANIZATION_TYPE\"])\n    \n    \n    # OCCUPATION_TYPE\n    df[\"OCCUPATION_TYPE\"] = np.where(df[\"OCCUPATION_TYPE\"].isin([\"Low-skill Laborers\", \"Cooking staff\", \"Security staff\", \"Private service staff\", \"Cleaning staff\", \"Waiters\/barmen staff\"]), \"Low_skill_staff\", df[\"OCCUPATION_TYPE\"])\n    df[\"OCCUPATION_TYPE\"] = np.where(df[\"OCCUPATION_TYPE\"].isin([\"IT staff\", \"High skill tech staff\"]), \"High_skill_staff\", df[\"OCCUPATION_TYPE\"])\n    df[\"OCCUPATION_TYPE\"] = np.where(df[\"OCCUPATION_TYPE\"].isin([\"Secretaries\", \"HR staff\",\"Realty agents\"]), \"Others\", df[\"OCCUPATION_TYPE\"])\n\n    # NAME_TYPE_SUITE\n    rare_list = [\"NAME_TYPE_SUITE\"]\n    rare_encoder(df, 0.01, rare_list)\n    \n    # NAME_EDUCATION_TYPE\n    # Akademik derecenin frekans\u0131 az oldu\u011fu i\u00e7in bununla y\u00fcksek e\u011fitimi ayn\u0131 s\u0131n\u0131fa ald\u0131k.\n    df[\"NAME_EDUCATION_TYPE\"] = np.where(df[\"NAME_EDUCATION_TYPE\"] == \"Academic degree\",\n                                         \"Higher education\", df[\"NAME_EDUCATION_TYPE\"])\n    \n    df[\"NAME_EDUCATION_TYPE\"] = np.where(df[\"NAME_EDUCATION_TYPE\"].str.contains(\"Secondary \/ secondary special\"),\n                                         \"Secondary_secondary_special\", df[\"ORGANIZATION_TYPE\"])\n    \n    # NAME_FAMILY_STATUS\n    df[\"NAME_FAMILY_STATUS\"] = np.where(df[\"NAME_FAMILY_STATUS\"].str.contains(\"Single \/ not married\"),\n                                        \"Single_not_married\", df[\"NAME_FAMILY_STATUS\"])\n    \n    \n    # NAME_HOUSING_TYPE\n    df[\"NAME_HOUSING_TYPE\"] = np.where(df[\"NAME_HOUSING_TYPE\"].str.contains(\"House \/ apartment\"),\n                                       \"House_apartment\", df[\"NAME_HOUSING_TYPE\"])\n    \n    # NAME_TYPE_SUITE\n    df[\"NAME_TYPE_SUITE\"] = np.where(df[\"NAME_TYPE_SUITE\"].str.contains(\"Spouse, partner\"),\n                                       \"Spouse_partner\", df[\"NAME_TYPE_SUITE\"])\n    \n    \n    # NAME_CONTRACT_TYPE\n    # Kategorik olan ama cinsiyet gibi 0 ve 1 olarak kodlanacak de\u011fi\u015fkenlere binary encode yapt\u0131k.\n    for bin_feature in [\"NAME_CONTRACT_TYPE\", 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n    \n    # WEEKDAY_APPR_PROCESS_START\n    # G\u00fcn isimlerini 1,2,3....,7 olarak de\u011fi\u015ftirece\u011fiz.\n    # Daha sonra g\u00fcnler d\u00f6ng\u00fcsel yap\u0131da olduklar\u0131 i\u00e7in bunlara cycle encode uygulayaca\u011f\u0131z.\n    # As\u0131l de\u011fi\u015fkenleri silmedik feature importance da bak!\n    weekday_dict = {'MONDAY': 1, 'TUESDAY': 2, 'WEDNESDAY': 3, 'THURSDAY': 4, 'FRIDAY': 5, 'SATURDAY': 6, 'SUNDAY': 7}\n    df.replace({'WEEKDAY_APPR_PROCESS_START': weekday_dict}, inplace=True)\n    # Cycle encode\n    df['NEW_WEEKDAY_APPR_PROCESS_START' + \"_SIN\"] = np.sin(2 * np.pi * df[\"WEEKDAY_APPR_PROCESS_START\"]\/7)\n    df[\"NEW_WEEKDAY_APPR_PROCESS_START\" + \"_COS\"] = np.cos(2 * np.pi * df[\"WEEKDAY_APPR_PROCESS_START\"]\/7)\n    \n    # HOUR_APPR_PROCESS_START\n    # de\u011fi\u015fken m\u00fc\u015fterinin hangi saatte ba\u015fvurdu\u011fu bilgisini veriyordu.\n    # Saat bilgisi de yine d\u00f6ng\u00fcsel oldu\u011fu i\u00e7in buna da cycle encode yap\u0131yoruz.\n    df['NEW_HOUR_APPR_PROCESS_START' + \"_SIN\"] = np.sin(2 * np.pi * df[\"HOUR_APPR_PROCESS_START\"]\/23)\n    df[\"NEW_HOUR_APPR_PROCESS_START\" + \"_COS\"] = np.cos(2 * np.pi * df[\"HOUR_APPR_PROCESS_START\"]\/23)\n  \n    ###########\n    # DROP\n    ###########\n    # FLAG_MOBIL ve FLAG_CONT_MOBILE de\u011fi\u015fkenlerinde iki alt s\u0131n\u0131f\u0131 var ve birinin frekans\u0131 \u00e7ok az.\n    # Yani bilgi ta\u015f\u0131mayan de\u011fi\u015fken bu nedenle drop ediyoruz.\n    drop_cols = [\"FONDKAPREMONT_MODE\", \"WALLSMATERIAL_MODE\", \"HOUSETYPE_MODE\",\n                 \"EMERGENCYSTATE_MODE\",\"FLAG_MOBIL\", \"FLAG_EMP_PHONE\",\"FLAG_WORK_PHONE\", \"FLAG_CONT_MOBILE\", \"FLAG_PHONE\", \"FLAG_EMAIL\"]\n    df.drop(drop_cols, axis = 1, inplace = True)\n    \n    # OBS_30_CNT_SOCIAL_CIRCLE,OBS_60_CNT_SOCIAL_CIRCLE\n    df.drop(['OBS_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE'], axis = 1, inplace = True)\n    \n    # REGION\n    # Bu de\u011fi\u015fkenler b\u00f6lge ve \u015fehir baz\u0131nda ayr\u0131 ayr\u0131 puan veriyordu\n    # Biz bunlar\u0131 toplayarak tek bir de\u011fi\u015fken elde ettik ve di\u011ferlerini droplad\u0131k.\n    cols = [\"REG_REGION_NOT_LIVE_REGION\",\"REG_REGION_NOT_WORK_REGION\", \"LIVE_REGION_NOT_WORK_REGION\", \n            \"REG_CITY_NOT_LIVE_CITY\",\"REG_CITY_NOT_WORK_CITY\",\"LIVE_CITY_NOT_WORK_CITY\"]\n    df[\"NEW_REGION\"] = df[cols].sum(axis = 1)\n    df.drop(cols, axis = 1, inplace = True)\n    \n    # Flag_DOCUMENT\n    # Bu de\u011fi\u015fkenler her d\u00f6k\u00fcman\u0131n ayr\u0131 ayr\u0131 verilip verilmedi\u011fi bilgisini veriyordu.\n    # Biz bunlar\u0131 toplayarak tek bir de\u011fi\u015fken elde ettik,yani totalde verilen belge say\u0131s\u0131n\u0131 hesaplad\u0131k\n    # ve di\u011ferlerini droplad\u0131k.\n    docs = [col for col in df.columns if 'FLAG_DOC' in col]\n    df['NEW_DOCUMENT'] = df[docs].sum(axis=1)\n    df.drop(docs, axis = 1, inplace = True)\n    \n    ##########################\n    # FEATURE ENGINEERING\n    ##########################\n    # 1. M\u00fc\u015fteri ba\u015fvurudan ne kadar \u00f6nce i\u015fe ba\u015flad\u0131(g\u00fcn) \/ m\u00fc\u015fterinin ya\u015f\u0131(g\u00fcn)\n    df['NEW_DAYS_EMPLOYED_RATIO'] = df['DAYS_EMPLOYED'] \/ df['DAYS_BIRTH']\n\n    # 2. M\u00fc\u015fterinin toplam geliri \/ kredi tutar\u0131\n    df['NEW_INCOME_CREDIT_RATIO'] = df['AMT_INCOME_TOTAL'] \/ df['AMT_CREDIT']\n\n    # 3. M\u00fc\u015fterinin toplam geliri \/ aile \u00fcyesi s\u00fcresi\n    # Ailede ki\u015fi ba\u015f\u0131na ne kadar gelir var bunu ortaya \u00e7\u0131kar\u0131yor.\n    df['NEW_INCOME_PER_RATIO'] = df['AMT_INCOME_TOTAL'] \/ df['CNT_FAM_MEMBERS']\n\n    # 4. Kredinin y\u0131ll\u0131k \u00f6demesi \/ M\u00fc\u015fterinin geliri\n    # E\u011fer bu de\u011fer 0-1 aras\u0131ndaysa iyi yani geliri kredi \u00f6demesinden fazla\n    # E\u011fer bu de\u011fer 1 ise k\u00f6t\u00fc yani geliri kredi \u00f6demesinden daha az.\n    df['NEW_ANNUITY_INCOME_RATIO'] = df['AMT_ANNUITY'] \/ df['AMT_INCOME_TOTAL']\n\n    # 5. Kredinin y\u0131ll\u0131k \u00f6demesi \/ kredi tutar\u0131\n    df['NEW_PAYMENT_RATIO'] = df['AMT_ANNUITY'] \/ df['AMT_CREDIT']\n    \n    # 6. EXT_SOURCE de\u011fi\u015fkenleri d\u0131\u015far\u0131dan al\u0131nan puanlard\u0131.Ortalamalar\u0131 ile yeni bir de\u011fi\u015fken olu\u015fturduk.\n    df[\"NEW_EXTSOURCE_MEAN\"] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n\n    # 7. Bu de\u011fi\u015fkenleri \u00e7arparak a\u011f\u0131rl\u0131kl\u0131 yeni bir de\u011fi\u015fken olu\u015fturduk.\n    df['NEW_EXTSOURCES_WPOINT'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n    \n    # 8. Kredi ile sat\u0131n al\u0131nacak mallar\u0131n fiyat\u0131 \/ toplam kredi tutar\u0131\n    # Bu oran 0-1 aras\u0131nda ise m\u00fc\u015fterinin alaca\u011f\u0131 mal fiyat\u0131ndan daha fazla kredi \u00e7ekmesi demek.\n    # Bu oran\u0131n 1 olmas\u0131 demek m\u00fc\u015fterinin ihtiyac\u0131 kadar kredi \u00e7ekti\u011fi anlam\u0131na gelir.\n    # Bu oran 1 den b\u00fcykse m\u00fc\u015fteri ihtiyac\u0131ndan daha az kredi \u00e7ekmi\u015f demektir.\n    df[\"NEW_GOODS_CREDIT_RATIO\"] = df[\"AMT_GOODS_PRICE\"] \/ df[\"AMT_CREDIT\"]\n    \n    # 9.Yukar\u0131daki de\u011fi\u015fkenle ba\u011flant\u0131l\u0131 olarak:\n    # Bu fark 0 dan b\u00fcy\u00fckse ihtiyac\u0131ndan az kredi \u00e7ekmi\u015f\n    # 0 olursa ihtiyac\u0131 kadar \u00e7ekmi\u015f\n    # 0 dan k\u00fc\u00e7\u00fckse ihtiyac\u0131ndan fazla \u00e7ekmi\u015f\n    df[\"NEW_GOODS_CREDIT_DIFF\"] = df[\"AMT_GOODS_PRICE\"] - df[\"AMT_CREDIT\"]\n    \n    # 10. (Kredi ile sat\u0131n al\u0131nacak mallar\u0131n fiyat\u0131 \/ toplam kredi tutar\u0131) \/ toplam gelir\n    df[\"NEW_GOODS_CREDIT_DIFF_RATIO\"] = (df[\"AMT_GOODS_PRICE\"] - df[\"AMT_CREDIT\"]) \/ df[\"AMT_INCOME_TOTAL\"]\n    \n    # 11. Toplam gelir \/ m\u00fc\u015fterinin ya\u015f\u0131(g\u00fcn cinsinden)\n    df['NEW_INCOME_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] \/ df['DAYS_BIRTH']\n    \n    # 12. Ki\u015finin DAYS_BIRTH de\u011fi\u015fkeni g\u00fcn cinsindn ya\u015f\u0131n\u0131 veriyordu.\n    # Ama de\u011ferler - (\u00e7\u00fcnk\u00fc \u015fu kadar g\u00fcn \u00f6nce do\u011fmu\u015f bilgisini veriyor.)\n    # Bu y\u00fczden m\u00fc\u015fterinin ya\u015f\u0131n\u0131 bulmak i\u00e7in - ile \u00e7arp\u0131p 360 a b\u00f6lece\u011fiz.\n    df[\"NEW_DAYS_BIRTH\"] = round(df[\"DAYS_BIRTH\"]* -1 \/ 365)\n    \n    # 13. Ya\u015flara g\u00f6re m\u00fc\u015fterileri segmentlere ay\u0131rma\n    df.loc[df[\"NEW_DAYS_BIRTH\"] <= 34 ,\"NEW_SEGMENT_AGE\"] = \"Young\"\n    df.loc[(df[\"NEW_DAYS_BIRTH\"] > 34)&(df[\"NEW_DAYS_BIRTH\"] <= 54) ,\"NEW_SEGMENT_AGE\"] = \"Middle_Age\"\n    df.loc[(df[\"NEW_DAYS_BIRTH\"] > 54),\"NEW_SEGMENT_AGE\"] = \"Old\"\n    \n    # 14. Gelire g\u00f6re m\u00fc\u015fterileri segmentlere ay\u0131rma\n    df.loc[df[\"AMT_INCOME_TOTAL\"] <= 112500 ,\"NEW_SEGMENT_INCOME\"] = \"Low_Income\"\n    df.loc[(df[\"AMT_INCOME_TOTAL\"] > 112500)&(df[\"AMT_INCOME_TOTAL\"] <= 225000) ,\"NEW_SEGMENT_INCOME\"] = \"Middle_Income\"\n    df.loc[(df[\"AMT_INCOME_TOTAL\"] > 225000),\"NEW_SEGMENT_INCOME\"] = \"High_Income\"\n    \n    # 15. Ki\u015fi ba\u015fvuru yaparken kimle birlikteydi?\n    df.loc[df['NAME_TYPE_SUITE'] == 'Unaccompanied', 'NEW_TYPE_SUITE_CAT'] = 0\n    df.loc[df['NAME_TYPE_SUITE'] != 'Unaccompanied', 'NEW_TYPE_SUITE_CAT'] = 1\n    df.loc[df['NAME_TYPE_SUITE'].isnull(), 'NEW_TYPE_SUITE_CAT'] = np.nan\n    \n    # 16. Elimizde m\u00fc\u015fterinin \u00e7evresinde 30 ve 60 g\u00fcn temerr\u00fcde d\u00fc\u015fm\u00fc\u015f ki\u015fi say\u0131s\u0131 bilgisini veren iki\n    # de\u011fi\u015fken vard\u0131. Biz 30 ve 60 g\u00fcn\u00fc birle\u015ftirerek;\n    # m\u00fc\u015fterinin \u00e7evresinde temerr\u00fcde d\u00fc\u015fen varsa 1 etiketlesin\n    # temerr\u00fcde d\u00fc\u015fen yoksa 0 etiketlesin dedik.\n    df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] > 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] > 0),\n           'NEW_DEF_30_60_SOCIAL_CIRCLE'] = 1\n    df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] > 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] == 0),\n           'NEW_DEF_30_60_SOCIAL_CIRCLE'] = 1\n    df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] == 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] > 0),\n           'NEW_DEF_30_60_SOCIAL_CIRCLE'] = 1\n    df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] == 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] == 0),\n           'NEW_DEF_30_60_SOCIAL_CIRCLE'] = 0\n    \n    ########################\n    # One-Hot Encoding\n    ########################\n    df, cat_cols = one_hot_encoder(df, nan_as_category=False)\n    \n    # Dropping feature named index\n    df.drop('index', axis=1, inplace=True)\n    \n    del test_df\n    gc.collect()\n    #print(df.columns.tolist())\n    return df\n\n\n###############################\n# Bureau and Bureau Balance\n##############################\n\ndef bureau_and_balance(num_rows = None, nan_as_category = True):\n    df = pd.read_csv(\"..\/input\/home-credit-default-risk\/bureau_balance.csv\", nrows = num_rows)\n    bureau = pd.read_csv('..\/input\/home-credit-default-risk\/bureau.csv')\n    #cat_cols, num_cols, cat_but_car = grab_col_names(df)\n    df,df_cat=one_hot_encoder(df)\n    \n    # Bureau balance: Perform aggregations and merge with bureau.csv\n    agg_list = {'MONTHS_BALANCE': ['min', 'max', 'size'] }\n    for col in df_cat:\n        agg_list[col] = ['mean','sum']\n\n    bb_agg = df.groupby(\"SK_ID_BUREAU\").agg(agg_list)\n    # Degisken isimlerinin yeniden adlandirilmasi \n    bb_agg.columns = pd.Index([col[0] + \"_\" + col[1].upper() for col in bb_agg.columns.tolist()])\n    # New feature\n    bb_agg['NEW_STATUS_SCORE'] = bb_agg['STATUS_1_SUM'] + bb_agg['STATUS_2_SUM']^2 + bb_agg['STATUS_3_SUM']^3 + bb_agg['STATUS_4_SUM']^4 + bb_agg['STATUS_5_SUM']^5\n    \n    bureau_and_bb = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n    del df, bb_agg\n    gc.collect()\n\n    cat_cols, num_cols, cat_but_car = grab_col_names(bureau_and_bb)\n    rare_encoder(bureau_and_bb,0.2,cat_cols)\n    \n    #CREDIT_ACTIVE degiskeninin sinif sayisini 2'ye d\u00fc\u015f\u00fcrd\u00fck (Closed-Active)\n    bureau_and_bb['CREDIT_ACTIVE'] = bureau_and_bb['CREDIT_ACTIVE'].replace(\"Rare\", 'Active')\n    #CREDIT_CURRENCY de\u011fi\u015fkenin 1 s\u0131n\u0131f\u0131, veri setinin %99'unu kaplad\u0131\u011f\u0131 i\u00e7in yani dengesiz veri oldu\u011fu i\u00e7in anlams\u0131z bilgi ta\u015f\u0131r.Bu y\u00fczden \u00e7\u0131karaca\u011f\u0131z.\n    bureau_and_bb.drop(\"CREDIT_CURRENCY\", inplace = True, axis = 1)\n    \n    #######################\n    # FEATURE ENGINEERING\n    #######################\n    # 1. Active ve Closed Krediler i\u00e7in kredi erken kapanm\u0131\u015fm\u0131? \n    # E\u011fer kredi durumu aktifse ve DAYS_CREDIT_ENDDATE de\u011fi\u015fkeni 0 dan k\u00fc\u00e7\u00fckse(yani - olmas\u0131\n    # o kadar g\u00fcn \u00f6nce sona erdi\u011fi anlam\u0131na gelir.) bu ki\u015fi kredisini erken kapatm\u0131\u015ft\u0131r.\n    # Closed da da kredisi kapanm\u0131\u015f ama erken \u00f6demesi erken bitmi\u015f olanlar var.\n    bureau_and_bb.loc[(bureau_and_bb['CREDIT_ACTIVE'] == 'Active') & (bureau_and_bb['DAYS_CREDIT_ENDDATE'] < 0), 'NEW_EARLY_ACT\u0130VE'] = 1\n    bureau_and_bb.loc[(bureau_and_bb['CREDIT_ACTIVE'] == 'Closed') & (abs(bureau_and_bb['DAYS_CREDIT_ENDDATE']) < abs(bureau_and_bb['DAYS_ENDDATE_FACT']) ), 'NEW_EARLY_CLOSED'] = 1\n    \n    # 2. Uzat\u0131lm\u0131\u015f Kredilerin 1 ile de\u011fi\u015ftirilmesi\n    bureau_and_bb[\"NEW_CNT_CREDIT_PROLONG_CAT\"] = bureau_and_bb.loc[:,'CNT_CREDIT_PROLONG']\n    prolong = [1,2,3,4,5,6,7,8,9]\n    bureau_and_bb[\"NEW_CNT_CREDIT_PROLONG_CAT\"] = bureau_and_bb['NEW_CNT_CREDIT_PROLONG_CAT'].replace(prolong, 1)\n    \n    # 3. Ki\u015fi Ka\u00e7 farkl\u0131 kredi tipi alm\u0131\u015f\n    temp_bu = bureau_and_bb[['SK_ID_CURR', 'CREDIT_TYPE']].groupby(by=['SK_ID_CURR'])['CREDIT_TYPE'].nunique().reset_index().rename(index=str, columns={'CREDIT_TYPE': 'NEW_BUREAU_LOAN_TYPES'})\n    bureau_and_bb = bureau_and_bb.merge(temp_bu, on=['SK_ID_CURR'], how='left')\n    \n    # 4. Bor\u00e7 Oran\u0131\n    # Kredi B\u00fcrosuna olan mevcut bor\u00e7 \/ kredi b\u00fcrosu i\u00e7in mevcut kredi \n    # 1 le toplamam\u0131z\u0131n sebebi tan\u0131ms\u0131zl\u0131k olmas\u0131n diye\n    bureau_and_bb['NEW_DEPT_RATIO'] = bureau_and_bb['AMT_CREDIT_SUM_DEBT'] \/ (bureau_and_bb['AMT_CREDIT_SUM']+1)\n\n    # 5.Kredi g\u00fcncellenmesi yenimi ?\n    # 90 g\u00fcn\u00fc baz ald\u0131k. \u00c7\u00fcnk\u00fc bankalarda 3 ay gecikmeden sonra i\u015flem ba\u015flat\u0131l\u0131yor.\n    bureau_and_bb['NEWS_DAYS_CREDIT_UPDATE'] = bureau_and_bb['DAYS_CREDIT_UPDATE'].apply(lambda x : 'old' if x < -90 else 'new')\n    #cat_cols, num_cols, cat_but_car = grab_col_names(bureau_and_bb)\n    \n    ###########################\n    # One-Hot Encoding\n    ###########################\n    bureau_and_bb, bureau_and_bb_cat = one_hot_encoder(bureau_and_bb)\n    \n    # Bureau and bureau_balance numeric features\n    num_aggregations = {'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n                        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n                        'DAYS_CREDIT_UPDATE': ['mean'],\n                        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n                        'DAYS_ENDDATE_FACT':['min', 'max', 'mean'],\n                        'NEW_STATUS_SCORE':['min','mean','max'],\n                        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n                        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n                        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n                        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n                        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n                        'AMT_ANNUITY': ['max', 'mean'],\n                        'CNT_CREDIT_PROLONG': ['sum'],\n                        'MONTHS_BALANCE_MIN': ['min'],\n                        'MONTHS_BALANCE_MAX': ['max'],\n                        'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n                        'NEW_DEPT_RATIO': ['min','max','mean'] }\n\n    # Bureau and bureau_balance categorical features\n    cat_aggregations = {}\n    for cat in bureau_and_bb_cat: cat_aggregations[cat] = ['mean']\n    for cat in df_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n    \n    bureau_agg = bureau_and_bb.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n    \n    # Bureau: Active credits - using only numerical aggregations\n    active = bureau_and_bb[bureau_and_bb['CREDIT_ACTIVE_Active'] == 1]\n    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n    del active, active_agg\n    \n    # Bureau: Closed credits - using only numerical aggregations\n    closed = bureau_and_bb[bureau_and_bb['CREDIT_ACTIVE_Closed'] == 1]\n    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n    del closed, closed_agg, bureau\n    gc.collect()\n    #print(bureau_agg.columns.tolist())\n    return bureau_agg\n\n###########################\n# Previous_application\n###########################\n\ndef previous_applications(num_rows = None, nan_as_category = True):\n    df = pd.read_csv(\"..\/input\/home-credit-default-risk\/previous_application.csv\", nrows = num_rows)\n    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n    \n    \n    # N\u00fcmerik de\u011fi\u015fkenleri incelerken baz\u0131 de\u011fi\u015fkenlerin max de\u011ferinin ger\u00e7ek d\u0131\u015f\u0131 \n    # yani \u00e7ok b\u00fcy\u00fck oldu\u011funu g\u00f6rd\u00fck.365 e b\u00f6ld\u00fck 1000 y\u0131l gibi bir de\u011fer geldi.\n    # Bu sebeple bu kolondaki b\u00f6yle b\u00fcy\u00fck de\u011ferlere Nan de\u011feri atayaca\u011f\u0131m\n    df['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n    df['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n    df['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n    df['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n    df['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n    \n    # Baz\u0131 kategorik de\u011fi\u015fkenlerin alt s\u0131n\u0131flar\u0131 XNA, XAP olarak yaz\u0131lm\u0131\u015f asl\u0131nda bunlar nan de\u011fer.\n    # Bu y\u00fczden bunlara nan at\u0131yoruz.\n    na = ['XNA', 'XAP']\n    for col in cat_cols:\n        for n in na:\n            df.loc[df[col] == n, col] = np.nan\n\n    #########\n    # RARE\n    ########\n    rare_encoder(df, 0.01,cat_cols)\n    \n    # NAME_GOODS_CATEGORY\n    # Bu de\u011fi\u015fkenin baz\u0131 alt s\u0131n\u0131flar\u0131n\u0131 others da toplad\u0131k.\n    a = ['Auto Accessories', 'Jewelry', 'Homewares', 'Medical Supplies', 'Vehicles', 'Sport and Leisure', \n         'Gardening', 'Other', 'Office Appliances', 'Tourism', 'Medicine', 'Direct Sales', 'Fitness', 'Additional Service', \n         'Education', 'Weapon', 'Insurance', 'House Construction', 'Animals'] \n\n    df[\"NAME_GOODS_CATEGORY\"] = df[\"NAME_GOODS_CATEGORY\"].replace(a, 'others')\n    \n    # \"NAME_SELLER_INDUSTRY\n    df[\"NAME_SELLER_INDUSTRY\"] = df[\"NAME_SELLER_INDUSTRY\"].replace(\"Rare\", 'others')\n    \n    # PREV_NAME_GOODS_CATEGORY\n    df[\"NAME_GOODS_CATEGORY\"] = np.where(df[\"NAME_GOODS_CATEGORY\"].str.contains(\"Photo \/ Cinema Equipment\"),\n                                       \"Photo_Cinema_Equipment\", df[\"NAME_GOODS_CATEGORY\"])\n    \n    # CHANNEL_TYPE\n    df[\"CHANNEL_TYPE\"] = np.where(df[\"CHANNEL_TYPE\"].str.contains(\"Regional \/ Local\"),\n                                       \"Regional_Local\", df[\"CHANNEL_TYPE\"])\n    \n    df[\"NAME_GOODS_CATEGORY\"] = np.where(df[\"NAME_GOODS_CATEGORY\"].str.contains(\"Audio\/Video\"),\n                                       \"Audio_Video\", df[\"NAME_GOODS_CATEGORY\"])\n    \n    \n    \n    \n    \n    ############\n    # DROP\n    ############\n    # %99 eksik veri olan ve de\u011fi\u015fkenleri inceledi\u011fimizde bilgi ta\u015f\u0131mad\u0131\u011f\u0131n\u0131 d\u00fc\u015f\u00fcnd\u00fc\u011f\u00fcm\u00fcz \n    # de\u011fi\u015fkenleri verisetinden \u00e7\u0131kar\u0131yoruz.\n\n    del_cols = ['RATE_INTEREST_PRIMARY', 'RATE_INTEREST_PRIVILEGED', 'DAYS_FIRST_DRAWING',\n                'NAME_CASH_LOAN_PURPOSE', 'CODE_REJECT_REASON', 'FLAG_LAST_APPL_PER_CONTRACT',\n                'NFLAG_LAST_APPL_IN_DAY', 'SELLERPLACE_AREA']\n\n    df.drop(del_cols, axis=1, inplace=True)\n    \n    #########################\n    # FEATURE ENGINEERING\n    #########################\n    \n    # 1. \"HOUR_APPR_PROCESS_START\" (M\u00fc\u015fteri \u00f6nceki ba\u015fvurusu i\u00e7in yakla\u015f\u0131k olarak hangi saatte ba\u015fvurdu) \n    # de\u011fi\u015fkenin NEW_WORK_HOURS ve NEW_OFF_HOURS olarak iki s\u0131n\u0131fa ayr\u0131lmas\u0131\n    df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"]= df.loc[:,\"HOUR_APPR_PROCESS_START\"]\n    a = [8,9,10,11,12,13,14,15,16,17]\n    df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"] = df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"].replace(a, \"WORK_HOURS\")\n    b = [18,19,20,21,22,23,0,1,2,3,4,5,6,7]\n    df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"] = df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"].replace(b, 'OFF_HOURS')\n    \n    # 2. \"WEEKDAY_APPR_PROCESS_START\"  de\u011fi\u015fkeninin  WEEK_DAY ve WEEKEND olarak iki s\u0131n\u0131fa  indirdik\n    df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"] = df.loc[:,\"WEEKDAY_APPR_PROCESS_START\"]\n    df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"] = df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"].replace(['MONDAY','TUESDAY', 'WEDNESDAY','THURSDAY','FRIDAY'], 'WEEK_DAY')\n    df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"] = df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"].replace(['SATURDAY', 'SUNDAY'], 'WEEKEND')\n    \n    # 3. \"NAME_TYPE_SUITE\"  de\u011fi\u015fkeninin single ve multiple olarak iki kategoriye ayr\u0131lmas\u0131\n    df[\"NAME_TYPE_SUITE\"] = df[\"NAME_TYPE_SUITE\"].replace('Unaccompanied', 'single')\n    b = ['Family', 'Spouse, partner', 'Children', 'Other_B', 'Other_A', 'Group of people']\n    df[\"NAME_TYPE_SUITE\"] = df[\"NAME_TYPE_SUITE\"].replace(b, 'multiple')\n    \n    # 4. M\u00fc\u015fteri \u00f6ncek ba\u015fvuruda ne kadar kredi istedi \/ \u00f6nceki ba\u015fvurunun nihai kredi tutar\u0131\n    df[\"NEW_AMT_CREDIT_RATIO\"] = df[\"AMT_APPLICATION\"]\/df[\"AMT_CREDIT\"]\n    # 1 e yak\u0131n olmas\u0131 istedi\u011fi krediden az alm\u0131\u015f olmas\u0131\n    # 0 a yak\u0131n olmas\u0131 istedi\u011fi krediden fazla alm\u0131\u015f olmas\u0131\n    # 1 ise istenilen kredi ile verilen kredi miktar\u0131 ayn\u0131 olmas\u0131\n    \n    # 5. x <= 1 ise istedi\u011fi krediyi alm\u0131\u015f veya daha fazlas\u0131n\u0131 alm\u0131\u015f.\n    df[\"NEWX2_FLAG_AMT_CREDIT_RATIO\"] = df[\"NEW_AMT_CREDIT_RATIO\"].apply(lambda x: 1 if(x<=1) else 0)\n    \n    # 6. NFLAG_INSURED_ON_APPROVAL de\u011fi\u015fkeni yerine kullan\u0131lmak izere NEW_INSURANCE de\u011fi\u015fkeni tan\u0131mland\u0131.\n    # NFLAG_INSURED_ON_APPROVAL: M\u00fc\u015fteri \u00f6nceki ba\u015fvuru s\u0131ras\u0131nda sigorta talep etti mi?\n    # AMT_CREDIT: Bankan\u0131n verdi\u011fi nihai kredi tutar\u0131\n    # AMT_GOOD_PRICE: M\u00fc\u015fterinin(varsa) \u00f6nceki uygulamada istedi\u011fi mal\u0131n fiyat\u0131\n    ########### NAN SAYISINI AZALTMAK \u0130\u00c7\u0130N YAPTIK \u00d6NEML\u0130\u0130\u0130 !!!!!!  ############\n    df[(df['AMT_CREDIT'] == 0) | (df['AMT_GOODS_PRICE'] == 0)]['NEW_INSURED_ON_APPROVAL'] = np.nan\n    df['INSURANCE_AMOUNT'] = df['AMT_CREDIT'] - df['AMT_GOODS_PRICE']\n    df['NEW_INSURED_ON_APPROVAL'] = df['INSURANCE_AMOUNT'].apply(lambda x: 1 if x > 0 else (0 if x <= 0 else np.nan))\n    df.drop('INSURANCE_AMOUNT', axis=1, inplace=True)\n    \n    # 7. Ka\u00e7 y\u0131lda \u00f6dedi = Bankan\u0131n verdi\u011fi kredi \/ y\u0131ll\u0131k taksit tutar\u0131\n    df['NEW_HOW_PAID_YEARS'] = df['AMT_CREDIT'] \/ df['AMT_ANNUITY']\n    \n    # 8. Ba\u015fvurdu\u011fu kredi miktar\u0131 \/ almak istedi\u011fi \u00fcr\u00fcn\u00fcn fiyat\u0131 \n    df['NEW_GOODS_RATIO'] = df['AMT_APPLICATION'] \/ df['AMT_GOODS_PRICE']\n    \n    #  \u00d6deme g\u00fcn\u00fcn\u00fc geciktirmi\u015f mi bunu g\u00f6steren de\u011fi\u015fken t\u00fcretelim.\n    # 1= geciktirmi\u015f, 0 = geciktirmemi\u015f, NaN = bo\u015f de\u011fer\n    # (Mevcut ba\u015fvurunun ba\u015fvuru tarihine g\u00f6re, bir \u00f6nceki ba\u015fvurunun ilk vadesi ne zamand\u0131?) - \n    # (Mevcut ba\u015fvurunun ba\u015fvuru tarihine g\u00f6re, bir \u00f6nceki ba\u015fvurunun ilk vadesi ne zaman olmal\u0131yd\u0131?)\n    # 9. N\u00dcMER\u0130K OLAN\n    df['NEW_LATE_DAYS'] =  df['DAYS_LAST_DUE_1ST_VERSION'] - df['DAYS_FIRST_DUE'] \n    # 10 .SINIFLI OLAN\n    k = df[\"DAYS_LAST_DUE_1ST_VERSION\"] - df[\"DAYS_LAST_DUE\"]\n    df[\"NEW_FLAG_LATE_DAYS\"] = [1 if i >= 0 else (0 if i < 0  else \"NaN\") for i in k]\n    \n    # WEEKDAY_APPR_PROCESS_START_DIC\n    # Cycle encoding g\u00fcn, ay, y\u0131l gibi d\u00f6ng\u00fcsel de\u011fi\u015fkenlerde kullan\u0131labilir.\n    df['WEEKDAY_APPR_PROCESS_START_DIC'] = df['WEEKDAY_APPR_PROCESS_START'].map({\n        'MONDAY': 1, 'TUESDAY': 2, 'WEDNESDAY': 3, 'THURSDAY': 4, 'FRIDAY': 5, 'SATURDAY': 6, 'SUNDAY': 7})\n    df['NEW_WEEKDAY_SIN'] = np.sin(2 * np.pi * df['WEEKDAY_APPR_PROCESS_START_DIC'] \/ 7)\n    df['NEW_WEEKDAY_COS'] = np.cos(2 * np.pi * df['WEEKDAY_APPR_PROCESS_START_DIC'] \/ 7)\n    # df.drop('WEEKDAY_APPR_PROCESS_START', axis=1, inplace=True) # feature imp bak!!!!!\n    \n    #######################\n    # One_Hot Encoding\n    #######################\n    df, cat_cols = one_hot_encoder(df, nan_as_category= True)\n\n    #################\n    # Aggregation\n    #################\n    col_list = df.columns.tolist()\n    id_list = [\"SK_ID_CURR\",\"SK_ID_PREV\"]\n    num_list = [col for col in col_list if col not in cat_cols + id_list]\n    \n    # Previous applications numeric features\n    agg_num_prev = {}\n    for num in num_list:\n        agg_num_prev[num] = ['min', 'max', 'mean', 'median']\n        \n    # Previous applications categorical features\n    agg_cat_prev = {}\n    for cat in cat_cols:\n        agg_cat_prev[cat] = ['mean']\n        \n    prev_agg = df.groupby('SK_ID_CURR').agg({**agg_num_prev, **agg_cat_prev})\n    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n    # Previous Applications: Approved Applications - only numerical features\n    approved = df[df['NAME_CONTRACT_STATUS_Approved'] == 1]\n    approved_agg = approved.groupby('SK_ID_CURR').agg(agg_num_prev)\n    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n    # Previous Applications: Refused Applications - only numerical features\n    refused = df[df['NAME_CONTRACT_STATUS_Refused'] == 1]\n    refused_agg = refused.groupby('SK_ID_CURR').agg(agg_num_prev)\n    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n    \n    del refused, refused_agg, approved, approved_agg, df\n    gc.collect()\n    #print(prev_agg.columns.tolist())\n    return prev_agg\n\n##################\n# Pos_cash\n#################\n\ndef pos_cash(num_rows = None, nan_as_category = True):\n    df = pd.read_csv('..\/input\/home-credit-default-risk\/POS_CASH_balance.csv', nrows = num_rows)\n    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n    \n    # Rare\n    rare_encoder(df, 0.01, cat_cols)\n    # One-Hot Encoding\n    df, cat_cols = one_hot_encoder(df, nan_as_category= True)\n    \n    # Numerical Features\n    aggregations = {'MONTHS_BALANCE': ['max', 'mean', 'size'],\n                    'CNT_INSTALMENT': ['max', 'mean', 'std', 'min', 'median'],\n                    'CNT_INSTALMENT_FUTURE': ['max', 'mean', 'sum', 'min', 'median', 'std'],\n                    'SK_DPD': ['max', 'mean'],\n                    'SK_DPD_DEF': ['max', 'mean']\n                   }\n    \n    # Categorical Features\n    original_columns = list(df.columns)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    for cat in new_columns:\n        aggregations[cat] = ['mean']\n        \n    pos_agg = df.groupby('SK_ID_CURR').agg(aggregations)\n    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n    # Count pos cash accounts\n    pos_agg['POS_COUNT'] = df.groupby('SK_ID_CURR').size()\n    del df\n    gc.collect()\n    #print(pos_agg.columns.tolist())\n    return pos_agg\n\n###############################\n# Installments_payments\n###############################\n\ndef installments_payments(num_rows = None, nan_as_category = True):\n    df = pd.read_csv('..\/input\/home-credit-default-risk\/installments_payments.csv', nrows = num_rows)\n    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n    df, cat_cols = one_hot_encoder(df, nan_as_category= True) ### neden cat var?\n    \n    #########################\n    # Feature Engineering\n    #########################\n    # Her kredi taksidi \u00f6demesinde \u00f6dedi\u011fi miktarla asl\u0131 aras\u0131ndaki fark ve bunun y\u00fczdesi\n    df['PAYMENT_PERC'] = df['AMT_PAYMENT'] \/ df['AMT_INSTALMENT']\n    df['PAYMENT_DIFF'] = df['AMT_INSTALMENT'] - df['AMT_PAYMENT']\n\n    # Vadesi ge\u00e7mi\u015f g\u00fcnler ve vadesinden \u00f6nceki g\u00fcnler -- sadece pozitif de\u011ferler al\u0131n\u0131r\n    df['DPD'] = df['DAYS_ENTRY_PAYMENT'] - df['DAYS_INSTALMENT']\n    df['DBD'] = df['DAYS_INSTALMENT'] - df['DAYS_ENTRY_PAYMENT']\n    df['DPD'] = df['DPD'].apply(lambda x: x if x > 0 else 0)\n    df['DBD'] = df['DBD'].apply(lambda x: x if x > 0 else 0)\n    \n    # Her bir taksit \u00f6demesinin gec olup olmama durumu 1: gec \u00f6dedi 0: erken \u00f6demeyi temsil eder\n    df['NEW_DAYS_PAID_EARLIER'] = df['DAYS_INSTALMENT'] - df['DAYS_ENTRY_PAYMENT']\n    df['NEW_NUM_PAID_LATER'] = df['NEW_DAYS_PAID_EARLIER'].map(lambda x: 1 if x<0 else 0)\n    \n    # Numeric Features\n    aggregations = {\n        'NUM_INSTALMENT_VERSION': ['nunique'],\n        'DPD': ['max', 'mean', 'sum'],\n        'DBD': ['max', 'mean', 'sum'],\n        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n    }\n    \n    # Categorical Features\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    ins_agg = df.groupby('SK_ID_CURR').agg(aggregations)\n    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n    # Count installments accounts\n    ins_agg['INSTAL_COUNT'] = df.groupby('SK_ID_CURR').size()\n    del df\n    gc.collect()\n    #print(ins_agg.columns.tolist())\n    return ins_agg\n\n\n#########################\n# Credit_card_balance\n########################\ndef credit_card_balance(num_rows = None, nan_as_category = True):\n    df = pd.read_csv('..\/input\/home-credit-default-risk\/credit_card_balance.csv', nrows = num_rows)\n    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n    \n    # Rare\n    df[\"NAME_CONTRACT_STATUS\"] = np.where(~(df[\"NAME_CONTRACT_STATUS\"].isin([\"Active\", \"Completed\"])),\n                                          \"Rare\", df[\"NAME_CONTRACT_STATUS\"])\n    \n    # One Hot Encoder\n    df, cat_cols = one_hot_encoder(df, nan_as_category=False)\n    \n    ########################\n    # Feature Engineering\n    ########################\n    # ATM den cekilen tutar + mal sat\u0131n alma miktari\n    df[\"TOTAL_SPENDING\"] = df[\"AMT_DRAWINGS_ATM_CURRENT\"] + df[\"AMT_DRAWINGS_POS_CURRENT\"]\n    # M\u00fc\u015fterinin ay boyunca \u00f6dedi\u011fi para - ayl\u0131k asgari taksit\n    df[\"REGULARITY_PAYMENT\"] = df[\"AMT_INST_MIN_REGULARITY\"] - df[\"AMT_PAYMENT_TOTAL_CURRENT\"]\n    \n    # General aggregations\n    df.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n    cc_agg = df.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n    # Count credit card lines\n    cc_agg['CC_COUNT'] = df.groupby('SK_ID_CURR').size()\n    del df\n    gc.collect()\n    return cc_agg\n\n#################\n# MODEL\n#################\n\n# LightGBM GBDT with KFold or Stratified KFold\n# Parameters from Tilii kernel: https:\/\/www.kaggle.com\/tilii7\/olivier-lightgbm-parameters-by-bayesian-opt\/code\ndef kfold_lightgbm(df, num_folds, stratified = False, debug= False):\n    df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n    # Divide in training\/validation and test data\n    train_df = df[df['TARGET'].notnull()]\n    test_df = df[df['TARGET'].isnull()]\n    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n    del df\n    gc.collect()\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n    # Create arrays and dataframes to store results\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds = np.zeros(test_df.shape[0])\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n\n        # LightGBM parameters found by Bayesian optimization\n        clf = LGBMClassifier(\n            boosting_type= 'goss',\n            nthread=4,\n            n_estimators=10000,\n            learning_rate=0.005134,\n            num_leaves=54,\n            colsample_bytree=0.508716,\n            subsample=1,\n            subsample_for_bin= 240000,\n            max_depth=10,\n            reg_alpha=0.436193,\n            reg_lambda=0.436193,\n            min_split_gain=0.024766,\n            min_child_weight=39.3259775,\n            silent=-1,\n            verbose=-1, )\n        \n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n            eval_metric= 'auc', verbose= 200, early_stopping_rounds= 200)\n\n        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] \/ folds.n_splits\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n        del clf, train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n    # Write submission file and plot feature importance\n    if not debug:\n        test_df['TARGET'] = sub_preds\n        test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index= False)\n    display_importances(feature_importance_df)\n    return feature_importance_df\n\n# Display\/plot feature importance\ndef display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:100].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(20, 25))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    #plt.savefig('lgbm_importances01.png')\n    \n\ndef main(debug = False):\n    num_rows = 10000 if debug else None\n    df = application_train_test(num_rows)\n    print(\"application shape:\", df.shape)\n    with timer(\"Process bureau and bureau_balance\"):\n        bureau = bureau_and_balance(num_rows)\n        print(\"Bureau df shape:\", bureau.shape)\n        df = df.join(bureau, how='left', on='SK_ID_CURR')\n        del bureau\n        gc.collect()\n    with timer(\"Process previous_applications\"):\n        prev = previous_applications(num_rows)\n        print(\"Previous applications df shape:\", prev.shape)\n        df = df.join(prev, how='left', on='SK_ID_CURR')\n        del prev\n        gc.collect()\n    with timer(\"Process POS-CASH balance\"):\n        pos = pos_cash(num_rows)\n        print(\"Pos-cash balance df shape:\", pos.shape)\n        df = df.join(pos, how='left', on='SK_ID_CURR')\n        del pos\n        gc.collect()\n    with timer(\"Process installments payments\"):\n        ins = installments_payments(num_rows)\n        print(\"Installments payments df shape:\", ins.shape)\n        df = df.join(ins, how='left', on='SK_ID_CURR')\n        del ins\n        gc.collect()\n    with timer(\"Process credit card balance\"):\n        cc = credit_card_balance(num_rows)\n        print(\"Credit card balance df shape:\", cc.shape)\n        df = df.join(cc, how='left', on='SK_ID_CURR')\n        del cc\n        gc.collect()\n    with timer(\"Run LightGBM with kfold\"):\n        feat_importance = kfold_lightgbm(df, num_folds= 10, stratified= False, debug= debug)\n\nif __name__ == \"__main__\":\n    submission_file_name = \"submission_kernel02.csv\"\n    with timer(\"Full model run\"):\n        main()","2561f69d":"![homecredit1.jpg](attachment:d674c640-78ef-41e4-854a-dc555ed1cf6c.jpg)\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/9120\/logos\/thumb76_76.png?t=2018-04-02-23-45-04\" align=\"left\" width = \"100px\"\/>\n\n<h1> Home Credit Default Risk<\/h1>","e1ea6ced":"## Data Description\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n\nHome Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n\n## Datasets\n\n<center> <div style=\"width:70%\"><img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/home-credit\/home_credit.png\"><\/img><\/div><\/center>"}}