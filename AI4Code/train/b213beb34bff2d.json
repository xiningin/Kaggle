{"cell_type":{"84d93352":"code","a155bd1e":"code","5249cdcb":"code","19877c67":"code","55f022a6":"code","ad9aec05":"code","07a487cd":"code","ecbe15d8":"code","f5d78d7d":"code","99c32e53":"code","f10a93b5":"code","ccea7112":"code","3c3abc13":"code","d8c8fa86":"code","84ef4792":"code","41d839c7":"code","03076918":"code","a64e192c":"code","a9be3390":"code","b326c9d1":"code","6fec12d1":"code","6c3a95bd":"code","af3c2f93":"code","e68bc939":"code","8f1eb97a":"code","4f09ceac":"code","2f9a5961":"code","4a925e81":"code","77788c4b":"code","b56bbd44":"code","ab8028a5":"code","c6aca5d1":"code","a839e6ba":"code","e982c22a":"code","b5ef40f1":"code","c44c63a8":"code","71c5a005":"code","d81f3673":"code","e9c0467f":"code","76366e8a":"code","90159396":"code","d18d3108":"code","18eb7b8a":"code","6ba4f1d9":"code","64699929":"code","6dca3716":"code","7a81028c":"code","18fef2cf":"code","d7e7a73d":"code","68e1a220":"code","fa72a97e":"code","03a08dc0":"code","fc200161":"code","acb1b49c":"code","23147d34":"code","0bcd1fe6":"code","7eb86626":"code","2939d99e":"code","26f94058":"code","e5c7043b":"code","ba378fd3":"code","4de8ed28":"code","6f1e4a59":"code","155302e0":"code","2ea8fd66":"code","fb0b44d4":"code","a428148d":"code","708ac0cc":"code","aab2d5e6":"code","5887610a":"code","db7270bc":"code","19aa814a":"code","6ccb0dae":"code","273d27e4":"code","f6d8cb80":"code","b9599022":"code","b262cdda":"code","7cc8e605":"code","ec0f1319":"code","5727654b":"code","1b44812c":"code","2aaaa3a4":"code","b9aa1474":"code","e3fb629b":"code","7ad23285":"code","c92dd573":"code","9117c9d9":"code","3fa050f5":"code","104c1495":"code","b6f0865e":"code","752826b9":"code","af6c54d9":"code","9f3b2acd":"code","9b783040":"code","0ba290bb":"code","7306733e":"code","44d76d67":"code","b17f1491":"code","b8e00bce":"code","035a277f":"code","237a4811":"code","f968b720":"code","d8894e84":"code","05054f2d":"code","c9c1a666":"code","a77808e0":"code","a676a45c":"code","4991518d":"code","f5ad9af7":"code","c61bf010":"code","1d881b26":"code","52b1c433":"code","c48e2c47":"code","cd8e606c":"markdown","9db7c914":"markdown","89ee70e8":"markdown","27a613a7":"markdown","4fc56e94":"markdown","24185806":"markdown","333648db":"markdown","c5cee13c":"markdown","72a3107c":"markdown","dfc5b0c8":"markdown","4788af8f":"markdown","587a5082":"markdown","1b2997f9":"markdown"},"source":{"84d93352":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nimport missingno as msno\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.impute import KNNImputer\nimport statsmodels.api as sm\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom lightgbm import LGBMRegressor\nimport re","a155bd1e":"#Read the Data\n\ndf=pd.read_csv(\"\/kaggle\/input\/hitters-baseball-data\/Hitters.csv\")","5249cdcb":"data=df.copy()\ndf.head()","19877c67":"def data_understanding(df):\n    print('############shape##############')\n    print(df.shape)\n    print('############types##############')\n    print(df.dtypes)\n    print('############head###############')\n    print(df.head())\n    print('############info###############')\n    print(df.info())\n    print('############nunique###############')\n    print(df.nunique())","55f022a6":"# There are 322 observations and int-float-object types of features in this data set\ndata_understanding(df)","ad9aec05":"print(\"Num of Object Variables:\", df.select_dtypes(object).shape[1])\nprint(\"Num of Integer Variables:\", df.select_dtypes(\"integer\").shape[1])\nprint(\"Num of Float Variables:\", df.select_dtypes(\"float\").shape[1])","07a487cd":"df[\"League\"].value_counts()","ecbe15d8":"df[\"League\"].value_counts().plot.barh()","f5d78d7d":"df[\"NewLeague\"].value_counts()","99c32e53":"df[\"NewLeague\"].value_counts().plot.barh()","f10a93b5":"df[\"Division\"].value_counts()","ccea7112":"df[\"Division\"].value_counts().plot.barh()","3c3abc13":"sns.distplot(df['Salary'])","d8c8fa86":"#There are 59 null values in Hitters data set\ndf.isnull().sum().sum()","84ef4792":"# All these NA values comes from \"Salary\" feature\ndf.isnull().sum()","41d839c7":"df[df.Salary.isnull()==True].head()","03076918":"msno.bar(df)","a64e192c":"#Statistical view for all features\ndf.describe().T","a9be3390":"# Descriptive Analysis\ndf.describe([0.05,0.25,0.50,0.75,0.95,0.99]).T","b326c9d1":"sns.boxplot(x = df[\"Salary\"])\nplt.show()","6fec12d1":"def outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","6c3a95bd":"#When the quarters of 1% and quartiles of 99% were examined first, no outlier was found.\nlower, upper=outlier_thresholds(df, 'Salary', q1=0.01, q3=0.99)\nprint(df[(df['Salary']<lower) | (df['Salary']>upper)].shape[0])","af3c2f93":"#Then, when the quarters of 25% and quarters of 75% were examined, an outlier was found.\n#Conclusion: Observation analysis against the dependent variable is applied according to quartiles of 25 and 75. \n#Business sector information may remain untouched.\nlower, upper=outlier_thresholds(df, 'Salary', q1=0.25, q3=0.75)\nprint(df[(df['Salary']<lower) | (df['Salary']>upper)].shape[0])","e68bc939":"#Later, when quarters of 5% and quarters of 95% were examined, no outlier was found.\nlower, upper=outlier_thresholds(df, 'Salary')\nprint(df[(df['Salary']<lower) | (df['Salary']>upper)].shape[0])","8f1eb97a":"# numerical variables\ndef numeric_cols(df):\n    numeric_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\n    return numeric_cols","4f09ceac":"#Here, how many outlier observations in all variables in quartiles of 25 and 75 are accessed.\nfor col in numeric_cols(df):\n    lower, upper=outlier_thresholds(df, col, 0.25, 0.75)\n    count=df[(df[col]<lower) | (df[col]>upper)].shape[0]\n    if count!=0:\n        print(col, 'yes')\n        print(count)\n    else:\n        print(col, 'no')","2f9a5961":"def replace_with_thresholds(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if low_limit > 0:\n        dataframe.loc[(dataframe[col_name] < low_limit), col_name] = low_limit\n        dataframe.loc[(dataframe[col_name] > up_limit), col_name] = up_limit\n    else:\n        dataframe.loc[(dataframe[col_name] > up_limit), col_name] = up_limit\n        \n    return dataframe","4a925e81":"df=replace_with_thresholds(df, 'Salary')","77788c4b":"sns.boxplot(df['Salary'])","b56bbd44":"for i in numeric_cols(df):\n\n    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 4))\n    sns.histplot(df[i], bins=10, ax=axes[0])\n    axes[0].set_title(i)\n    \n    sns.boxplot(df[i], ax=axes[1])\n    axes[1].set_title(i)\n   \n    sns.kdeplot(df[i], ax=axes[2])\n    axes[2].set_title(i)\n    plt.show()","ab8028a5":"# correlation analysis\ndf.corr()","c6aca5d1":"def correlation(df, size=[20, 15]):\n    f, ax = plt.subplots(figsize= [20,15])\n    sns.heatmap(df.corr(), annot=True, fmt=\".2f\", ax=ax, cmap = \"magma\" )\n    ax.set_title(\"Correlation Matrix\", fontsize=20)\n    plt.show()","a839e6ba":"correlation(df)","e982c22a":"# Correlation analysis of numerical variables was performed.\ndef find_corr(df, num_col_names, limit=0.55):\n    high_corrs={}\n    for col in num_col_names:\n        if col=='Salary':\n            pass\n        else:\n            corr=df[[col, 'Salary']].corr().loc[col, 'Salary']\n            print(col, corr)\n            if abs(corr)>limit:\n                high_corrs[col]=corr\n    return high_corrs","b5ef40f1":"high_corrs = find_corr(df, numeric_cols(df))","c44c63a8":"#Two variables with high correlation.\nprint(high_corrs)","71c5a005":"sns.scatterplot(x= df['CRuns'], y=df.Salary)","d81f3673":"sns.scatterplot(x= df['CRBI'], y=df.Salary)","e9c0467f":"def lof_scores(df):\n    clf=LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n    clf.fit_predict(df)\n    df_scores=clf.negative_outlier_factor_\n    sns.boxplot(df_scores)\n    plt.show()\n    return df_scores\n    \ndef lof(df, df_scores, threshold):\n    not_outlier = df_scores >threshold\n    value= df[df_scores == threshold]\n    outliers = df[~not_outlier] \n    res=outliers.to_records(index=False)\n    res[:] = value.to_records(index = False)\n    not_outlier_df = df[not_outlier]\n    outliers = pd.DataFrame(res, index = df[~not_outlier].index)\n    df_res = pd.concat([not_outlier_df, outliers], ignore_index = True)\n    return df_res","76366e8a":"#drop NA values\ndf1=df.dropna()\ndf1.shape","90159396":"#Min-Max Scaler\ndef minmax_scaler(dataframe, col_names, feature_range=(0,1)):\n    minmax_scaler = MinMaxScaler(feature_range=feature_range)\n    col_names=[col for col in col_names if col !=\"Salary\"]\n    dataframe[col_names] = minmax_scaler.fit_transform(dataframe[col_names])\n    return dataframe","d18d3108":"df1=minmax_scaler(df1, numeric_cols(df1))","18eb7b8a":"df1.isnull().sum().sum()","6ba4f1d9":"# Variables with 2 categories\ndef var_two_cat(df):    \n    bins_cols=[col for col in df.columns if df[col].dtype=='O' and df[col].nunique()==2]\n    return bins_cols","64699929":"print(var_two_cat(df1))","6dca3716":"def label_encoder(df, bins_cols):\n    for col in bins_cols:\n        le=LabelEncoder()\n        df[col]=le.fit_transform(df[col])\n    return df","7a81028c":"df1=label_encoder(df1, var_two_cat(df1))","18fef2cf":"df1.name='df1'\ndf1.head()","d7e7a73d":"#This is second option and method is fill NA values with mean\ndf2=df.copy()","68e1a220":"#New variables were created with the most appropriate variables according to their proportions.\n#The data set includes the data obtained by the players in 1986 and throughout their careers and how many years of experience they have. \n#We add the annual averages of these data and the ratio of the 1986 data to the overall performance.\ndef new_var(df):\n    df['AtBat_new'] = df['AtBat'] \/ df['CAtBat']\n    df['Hits_new'] = df['Hits'] \/ df['CHits']\n    df['HmRun_new'] = (df['HmRun'] \/ df['CHmRun']).fillna(0)\n    df['Runs_new'] = df['Runs'] \/ df['CRuns']\n    df['RBI_new'] = (df['RBI'] \/ df['CRBI']).fillna(0)\n    df['Walks_new'] = (df['Walks'] \/ df['CWalks']).fillna(0)\n\n    df[\"CAtBat_rate\"] = df[\"CAtBat\"] \/ df[\"Years\"]\n    df[\"CHits_rate\"] = df[\"CHits\"] \/ df[\"Years\"]\n    df[\"CHmRun_rate\"] = df[\"CHmRun\"] \/ df[\"Years\"]\n    df[\"Cruns_rate\"] = df[\"CRuns\"] \/ df[\"Years\"]\n    df[\"CRBI_rate\"] = df[\"CRBI\"] \/ df[\"Years\"]\n    df[\"CWalks_rate\"] = df[\"CWalks\"] \/ df[\"Years\"]\n    \n    return df","fa72a97e":"def new_year(df):\n    df['New_Year'] = pd.cut(x=df['Years'], bins=[0, 3, 6, 10, 15, 19, 24], ).astype(\"O\")\n    return df","03a08dc0":"df2=new_year(df2)","fc200161":"df2['New_Year'].value_counts().plot.barh()","acb1b49c":"df2.isnull().sum().sum()","23147d34":"msno.bar(df2)","0bcd1fe6":"df2['Salary']=df2['Salary'].fillna(df2.groupby(['New_Year', \"League\", 'Division'])['Salary'].transform('mean'))","7eb86626":"df2.isnull().sum().sum()","2939d99e":"df2.head()","26f94058":"df2=minmax_scaler(df2, numeric_cols(df2))","e5c7043b":"df2=label_encoder(df2, var_two_cat(df2))","ba378fd3":"df2.head()","4de8ed28":"def one_hot_cols(df): \n    return [col for col in df.columns if 10>=df[col].nunique()>2]\nprint(one_hot_cols(df2))","6f1e4a59":"df2 = pd.get_dummies(df2, columns=one_hot_cols(df2), drop_first=True)","155302e0":"df2.head()\ndf2.name='df2'","2ea8fd66":"df3=df.copy()","fb0b44d4":"df3=new_year(df3)","a428148d":"df3=minmax_scaler(df3, numeric_cols(df3))","708ac0cc":"df3=label_encoder(df3, var_two_cat(df3))","aab2d5e6":"print(one_hot_cols(df3))","5887610a":"df3 = pd.get_dummies(df3, columns=one_hot_cols(df3), drop_first=True)","db7270bc":"df3.head()","19aa814a":"# We fill in the missing observations with the KNN algorithm and create the dataset named 'df_knn_imp':\ndef knn_imputer(df, n):\n    imputer = KNNImputer(n_neighbors = n)\n    df_filled = imputer.fit_transform(df)\n    df_knn_imp = pd.DataFrame(df_filled,columns = df.columns)\n    return df_knn_imp","6ccb0dae":"df3=knn_imputer(df3, 4)\ndf3.head()\ndf3.name='df3'","273d27e4":"df3.isnull().sum().sum()","f6d8cb80":"#Filling Missing Data with KNN and Suppressing Outliers to create 'df4'\ndf4=df.copy()","b9599022":"df4.head()","b262cdda":"df4=new_year(df4)","7cc8e605":"df4=minmax_scaler(df4, numeric_cols(df4))","ec0f1319":"df4=label_encoder(df4, var_two_cat(df4))","5727654b":"df4 = pd.get_dummies(df4, columns=one_hot_cols(df4), drop_first=True)","1b44812c":"df4.head()","2aaaa3a4":"df4=knn_imputer(df4, 4)","b9aa1474":"array=np.sort(lof_scores(df4))\n\narray_res=array[array>array[63]]","e3fb629b":"sns.boxplot(array_res)","7ad23285":"df_scores=lof_scores(df4)\ndf4=lof(df4, df_scores, np.sort(df_scores)[63])","c92dd573":"df4.isnull().sum().sum()","9117c9d9":"df4.name='df4'\ndf4.head()","3fa050f5":"df5=df.copy()","104c1495":"df5=new_year(df5)\ndf5=new_var(df5)","b6f0865e":"df5=label_encoder(df5, var_two_cat(df5))","752826b9":"print(one_hot_cols(df5))","af6c54d9":"df5 = pd.get_dummies(df5, columns=one_hot_cols(df5), drop_first=True)","9f3b2acd":"df5=knn_imputer(df5, 4)","9b783040":"df_scores=lof_scores(df5)","0ba290bb":"df5=lof(df5, df_scores, np.sort(df_scores)[110])   #90","7306733e":"df5=minmax_scaler(df5, numeric_cols(df5))","44d76d67":"df5.isnull().sum().sum()","b17f1491":"df5.name='df5'\ndf5.head()","b8e00bce":"def reg_model(df, Y, algo, test_size=0.20):\n    X=df.drop(Y, axis=1)\n    Y=df[[Y]]\n    X_train, X_test, Y_train, Y_test=train_test_split(X, Y, test_size=test_size, random_state=42)\n    model=algo.fit(X_train, Y_train)\n    Y_train_pred=model.predict(X_train)\n    train_rmse=np.sqrt(mean_squared_error(Y_train, Y_train_pred))\n    print(df.name)\n    print(type(model).__name__)\n    print(\"Train RMSE: {}\".format(train_rmse))\n    \n    Y_test_pred=model.predict(X_test)\n    test_rmse=np.sqrt(mean_squared_error(Y_test, Y_test_pred))\n    print(\"Test RMSE: {}\".format(test_rmse))\n    print('###################################')\n    return (df.name, type(model).__name__, train_rmse, test_rmse)","035a277f":"models=[LinearRegression(), Ridge(), Lasso(), ElasticNet()]\ndataframes=[df1, df2, df3, df4, df5]\nresults={'frame':[], 'model':[], 'train_error':[], 'test_error':[]}","237a4811":"for frame in dataframes:\n    for model in models:\n        res=reg_model(frame, 'Salary', model)\n        results['frame'].append(res[0])\n        results['model'].append(res[1])\n        results['train_error'].append(res[2])\n        results['test_error'].append(res[3])","f968b720":"results=pd.DataFrame(results)\nresults","d8894e84":"sns.barplot(x=results['frame'], y=results['test_error'], hue=results['model'])","05054f2d":"def model_tuning(df, Y, algo_cv, algo, alphas, test_size=0.20, cv=10):\n    X=df.drop(Y, axis=1)\n    Y=df[[Y]]\n    X_train, X_test, Y_train, Y_test=train_test_split(X, Y, random_state=42, test_size=test_size)\n    model_cv=algo_cv(alphas=alphas, cv=cv)\n    model_cv.fit(X_train, Y_train)\n    model_tuned=algo(alpha=model_cv.alpha_)\n    model_tuned.fit(X_train, Y_train)\n    print(df.name)\n    print(type(model_tuned).__name__)\n    Y_train_pred=model_tuned.predict(X_train)\n    train_rmse=np.sqrt(mean_squared_error(Y_train, Y_train_pred))\n    print(\"Train RMSE:{}\".format(train_rmse))\n    Y_test_pred=model_tuned.predict(X_test)\n    test_rmse=np.sqrt(mean_squared_error(Y_test, Y_test_pred))\n    print(\"Test RMSE:{}\".format(test_rmse))\n    print('#####################')\n    return (df.name, type(model_tuned).__name__, train_rmse, test_rmse)","c9c1a666":"models={Ridge: RidgeCV, Lasso:LassoCV, ElasticNet:ElasticNetCV}\nresults_tuned={'frame':[], 'model':[], 'train_rmse':[], 'test_rmse':[]}\nalphas = [0.1,0.01, 0.005, 0.05, 0.001,0.2,0.3,0.5,0.8,0.9,1]","a77808e0":"for frame in dataframes:\n    for model in models:\n        res=model_tuning(frame, 'Salary', models[model], model, alphas)\n        results_tuned['frame'].append(res[0])\n        results_tuned['model'].append(res[1])\n        results_tuned['train_rmse'].append(res[2])\n        results_tuned['test_rmse'].append(res[3])","a676a45c":"results_tuned=pd.DataFrame(results_tuned)\nresults_tuned","4991518d":"def light_gbm(df, Y):\n    df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n    lgbm=LGBMRegressor()\n    X=df.drop(Y, axis=1)\n    Y=df[[Y]]\n    X_train, X_test, Y_train, Y_test=train_test_split(X, Y, random_state=42, test_size=0.20)\n    lgbm.fit(X_train, Y_train)\n\n    Y_pred=lgbm.predict(X_test,num_iteration=lgbm.best_iteration_)\n\n    print((np.sqrt(mean_squared_error(Y_test, Y_pred))))","f5ad9af7":"light_gbm(df4, 'Salary')","c61bf010":"light_gbm(df5, 'Salary')","1d881b26":"def light_gbm_tuning(df, Y):\n    df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n    X=df.drop(Y, axis=1)\n    Y=df[[Y]]\n    X_train, X_test, Y_train, Y_test=train_test_split(X, Y, random_state=42, test_size=0.20)\n    lgbm_grid={\n    'colsample_bytree':[0.4, 0.5, 0.6, 0.9, 1],\n    'learning_rate':[0.01, 0.1, 0.5, 1],\n           'n_estimators':[20, 40, 100, 200, 500, 1000],\n           'max_depth':[1, 2, 3, 4, 5, 6, 7, 8]}\n\n    lgbm=LGBMRegressor()\n\n    lgbm_cv_model=GridSearchCV(lgbm, lgbm_grid, cv=10,\n                           n_jobs=-1, verbose=2)\n\n    lgbm_cv_model.fit(X_train, Y_train)\n\n    #lgbm_cv_model.best_params_\n    lgbm_tuned=LGBMRegressor(learning_rate=0.1,\n                         max_depth=2,\n                         n_estimators=100,\n                         colsample_bytree=0.9)\n\n    lgbm_tuned.fit(X_train, Y_train)\n\n    Y_pred=lgbm_tuned.predict(X_test)\n\n    print(np.sqrt(mean_squared_error(Y_test, Y_pred)))","52b1c433":"light_gbm_tuning(df4, 'Salary')","c48e2c47":"light_gbm_tuning(df5, 'Salary')","cd8e606c":"# First option","9db7c914":"# MODEL TUNING","89ee70e8":"# Outliers","27a613a7":"# Data Preprocessing","4fc56e94":"# DATA UNDERSTANDING","24185806":"# Five option","333648db":"# Local Outlier Factor","c5cee13c":"# Fourth option","72a3107c":"# Second option","dfc5b0c8":"# Libraries","4788af8f":"We will create different data sets for different scenarios that we will apply for salary estimation.","587a5082":"# Model","1b2997f9":"# Third option"}}