{"cell_type":{"1a32a823":"code","0e4ac1cb":"code","eee8fb00":"code","726ea454":"code","6e6f66a2":"code","7d925383":"code","ef0c80e7":"code","436a967d":"code","002db570":"code","b632414e":"code","4e1af358":"code","5517c330":"code","64330d81":"code","cecd7fc1":"code","d74e224d":"code","bbeff652":"code","c0764d9f":"code","80750b04":"code","31133f89":"code","aed9b65a":"code","dbaddd02":"code","52c97c16":"code","0c861624":"code","de3a7213":"code","745b48cc":"code","ad0525f8":"code","500a04ab":"code","05c37ff7":"code","24e927eb":"code","6b90d6ca":"code","82bc2fd5":"code","cb7c84b7":"code","4a9dfcbb":"code","83ca6c64":"code","3d3038d8":"code","64c1ad22":"code","29ea3f32":"code","79ee4da1":"code","f5833dd9":"code","adbcf13c":"code","94891386":"code","6ad9d58f":"code","3e9c87ab":"code","85165c43":"code","1c61fdb9":"code","aa835a88":"markdown","7ef3f92a":"markdown","a8ae869a":"markdown","bb882973":"markdown","4082e297":"markdown","a27b4d3d":"markdown","15feeb5d":"markdown","40e4bd93":"markdown","b0ec84cf":"markdown","4a9745f9":"markdown","2c8a0d98":"markdown","3b8f1fb1":"markdown","7e983435":"markdown","9c8b3f8e":"markdown","4fb76fd2":"markdown","492ec5de":"markdown","99b7a45b":"markdown","12c591dd":"markdown","4c7209ec":"markdown","87b3be7d":"markdown","ccc6306d":"markdown","c6909e9e":"markdown","ba0c2e4c":"markdown","aa8041f6":"markdown","e92373d2":"markdown","dc439dbb":"markdown","e1f5d913":"markdown","6da50a1c":"markdown","917952ef":"markdown","d1eb4396":"markdown","07059d25":"markdown","e3160c3b":"markdown"},"source":{"1a32a823":"x = 3.0\nmessage = f\"x has value {x}\"\nprint(message)","0e4ac1cb":"# standard Python library\nimport math, os, time, copy\n\n# defacto standard numerics package\nimport numpy as np\n\n# plotting\nimport matplotlib.pyplot as plt\n\n# PyTorch\nimport torch","eee8fb00":"a = torch.tensor([1, 2, 3, 4]) # manually specify contents\nb = torch.arange(0.0, 10.0, 1.0) # a range\nc = torch.rand(2, 2) # ~Unif[0,1] of specified size\nd = torch.zeros(3, 3)\ne = torch.ones(2, 3)\n\nprint(a)\nprint(b)\nprint(c)\nprint(d)\nprint(e)","726ea454":"print(a.shape, a.dtype, a.device)\nprint()\nprint(b.shape, b.dtype, b.device)\nprint()\nprint(c.shape, c.dtype, c.device)","6e6f66a2":"a = torch.tensor([1.0,2 ,3 ,4])\nb = torch.tensor([-1 ,3 ,8 ,0.1])\n\nc = a + b\nd = a * b\ne = torch.exp(a)\nf = torch.sin(b)\ng = b**2\n\nc,d,e,f,g","7d925383":"A = torch.rand(2,3)\nB = torch.rand(3,2)\n\nC = torch.mm(A, B) # matrix multiplication\nprint(C)\n\nD = C.svd()\nprint()\nprint(D)\n\ngreater = a.gt(b)\nprint()\nprint(greater)\n\ngreater2 = torch.where(greater, a, torch.tensor(0.0))\nprint(greater2)","ef0c80e7":"%%timeit\n\na = torch.rand(10**8)\nb = a.sin()","436a967d":"%%timeit\n\nc = torch.rand(10**8, device='cuda')\nd = c.sin()","002db570":"%%timeit\n\na = torch.rand(10)\nb = a.sin()","b632414e":"%%timeit\n\nc = torch.rand(10, device='cuda')\nd = c.sin()","4e1af358":"a = torch.rand(10**8)\nb = a.sin()\n\nc = torch.rand(10**8, device='cuda')\nd = c.sin()\n\na.device, b.device, c.device, d.device","5517c330":"a_cu = a.cuda()\n\nc_cpu = c.cpu()\n\nprint(a_cu.device)\nprint(c_cpu.device)\n\ntry:\n    a_cu + c_cpu\nexcept RuntimeError as e:\n    print(e)","64330d81":"# Handy line for using a CUDA device if available but default to the CPU if not\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ndevice","cecd7fc1":"X = torch.arange(0., 1., step=0.02, device=device) # partition of [0,1], which we create on device\n\nY = 0.6 * torch.sin(6*X) * torch.sin(3*X+1) + 0.25 # calculate our function at the positions specified in X\n\n# Our first use of matplotlib\nplt.plot(X.cpu(), Y.cpu(), '.', label=\"data\") # can only plot data on the CPU\nplt.xlim(0.0, 1.0)\nplt.ylim(0.0, 1.0)\nplt.legend()\nplt.show()","d74e224d":"print(X.shape)\nprint(X[0].shape) # empty shape => scalar\n\nX = X.unsqueeze(-1)\nY = Y.unsqueeze(-1)\n\nprint(X.shape)\nprint(X[0].shape) # 1D tensor with 1 element","bbeff652":"# Create a dataset from existing tensors\ndataset = torch.utils.data.TensorDataset(X, Y)\n\n# lets follow best practices and split into training and testing data\n# we will split the dataset 80\/20\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\n\n# now to do the actual splitting\n# using generator=torch.manual_seed(0) fixes the RNG so we get a reproducable split\ntrain_data, test_data = torch.utils.data.random_split(\n    dataset, \n    [train_size, test_size], \n    generator=torch.manual_seed(0)\n)\n\n\nplt.plot(train_data[:][0].cpu().squeeze(), train_data[:][1].cpu().squeeze(), '+', label=\"training\")\nplt.plot(test_data[:][0].cpu().squeeze(), test_data[:][1].cpu().squeeze(), 'x', label=\"testing\")\nplt.legend()\nplt.xlim(0.0, 1.0)\nplt.ylim(0.0, 1.0)\nplt.show()","c0764d9f":"n = 32 # how many hidden units we want\n\nshallow = torch.nn.Sequential(\n    torch.nn.Linear(1, n), # = x A^T + b\n    torch.nn.ReLU(), # = ReLU(x A^T + b)\n    torch.nn.Linear(n, 1, bias=False), # = ReLU(x A^T + b) C^T\n)\n# to get just a linear transform we disable the bias term in Linear with bias=False\n# also note the odd x A^T + b convention rather than our usual Ax + b\n\n# lets move our network to our device of choice, this happens in the same manner as with tensors\nshallow = shallow.to(device)","80750b04":"# Compute the total number of trainable parameters in a model\ndef total_parameters(net):\n    return sum(p.numel() for p in net.parameters() if p.requires_grad)\n\ntotal_parameters(shallow)","31133f89":"print(shallow) # the whole network: consists of the 3 modules executed in sequence\nprint()\n\nprint(shallow[0]) # inspect the first module\nprint()\n\nprint(shallow[0].weight) # inspect the linear weights of the first linear module (i.e. A)\nprint(shallow[0].bias) # the bias tensor (i.e. b)","aed9b65a":" with torch.no_grad():\n    # in this scope, gradient computation is disabled\n    # lets feed in our range of x-coordinates and see what comes out\n    Ypred = shallow(X)\n    \nplt.plot(X.cpu(), Y.cpu(), '-', label=\"function\") # can only plot data on the CPU\nplt.plot(X.cpu(), Ypred.cpu(), '-', label=\"network\")\nplt.legend()\nplt.show()","dbaddd02":"# reset all the parameters of a network\n# we iterate through all the submodules and if a submodule provides a reset_parameters function we call it\ndef reset_network(net):\n    for m in net.children():\n       if hasattr(m, 'reset_parameters'):\n           m.reset_parameters()\n            \nreset_network(shallow)","52c97c16":"EPOCHS = 1000\nBATCH_SIZE = 8\nLEARNING_RATE = 0.005\n\n# A DataLoader will feed us random (shuffled) batches\nloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n\n# An optimizer will modify our network's parameters\n# we use the SGD optimizer and let it know what parameters it has to consider\n# we also specify the learning rate\noptimizer = torch.optim.SGD(shallow.parameters(), lr=LEARNING_RATE)\n\nfrom IPython.display import clear_output # for animating the plot","0c861624":"for epoch in range(1, EPOCHS+1):\n    \n    shallow.train() # Switch model to training mode (gradients are calculated)\n    \n    for x, y in iter(loader):\n        optimizer.zero_grad() # zero out the gradients\n        prediction = shallow(x) # evaluate the batch data\n        loss = (y - prediction).pow(2).sum() # calculate the loss of the actual output vs. the desired output\n        loss.backward() # calculate the gradient of the loss function with respect to the parameters via backpropagation\n        optimizer.step() # now that the gradients are calculated the optimizer can modify the parameters\n    \n    shallow.eval() # Switch model to evaluation mode (no gradients are calculated)\n    Ypred = shallow(X) # network's prediction for the whole interval\n    \n    # Plot every few epochs\n    if epoch % 20 == 0 or epoch == EPOCHS:\n        clear_output(wait=True)\n        plt.figure(figsize=[10, 6])\n        plt.title(f\"Epoch {epoch}\/{EPOCHS}\")\n        plt.xlim(0,1)\n        plt.ylim(0,1)\n        plt.plot(X.cpu(), Y.cpu(), '+',label=\"data\")\n        plt.plot(X.cpu(), Ypred.detach().cpu(), '-', label=\"network\") # I'll explain .detach() below\n        plt.legend()\n        plt.show()","de3a7213":"# again we use detach liberaly since we do not want these calculations to be differentiated\nXinflection = - shallow[0].bias.detach() \/ shallow[0].weight.detach().squeeze()\nYinflection = shallow(Xinflection.unsqueeze(-1)).squeeze().detach()\n\nplt.figure(figsize=[10, 6])\nplt.xlim(-3,4)\nplt.ylim(-3,4)\nplt.plot(X.cpu(), Y.cpu(), '-',label=\"function\")\nplt.plot(X.cpu(), Ypred.detach().cpu(), '-', label=\"network\")\nplt.plot(Xinflection.cpu(), Yinflection.cpu(), 'r+', label=\"inflection points\")\nplt.legend()\nplt.show()","745b48cc":"reset_network(shallow)","ad0525f8":"for epoch in range(1, EPOCHS+1):\n    \n    shallow.train() # Switch model to training mode (gradients are calculated)\n    \n    for x, y in iter(loader):\n        optimizer.zero_grad() # zero out the gradients\n        prediction = shallow(x) # evaluate the batch data\n        loss = (y - prediction).pow(2).sum() # calculate the loss of the actual output vs. the desired output\n        loss.backward() # calculate the gradient of the loss function with respect to the parameters via backpropagation\n        optimizer.step() # now that the gradients are calculated the optimizer can modify the parameters\n    \n    shallow.eval() # Switch model to evaluation mode (no gradients are calculated)\n    Ypred = shallow(X) # network's prediction for the whole interval\n    \n    # Plot every few epochs\n    if epoch % 20 == 0 or epoch == EPOCHS:\n        clear_output(wait=True)\n        plt.figure(figsize=[10, 6])\n        plt.title(f\"Epoch {epoch}\/{EPOCHS}\")\n        plt.xlim(-1,2)\n        plt.ylim(-1,2)\n        plt.plot(X.cpu(), Y.cpu(), '-', label=\"function\")\n        plt.plot(X.cpu(), Ypred.detach().cpu(), '-', label=\"network\") # I'll explain .detach() below\n        \n        Xinflection = - shallow[0].bias.detach() \/ shallow[0].weight.detach().squeeze()\n        Yinflection = shallow(Xinflection.unsqueeze(-1)).squeeze().detach()\n        plt.plot(Xinflection.cpu(), Yinflection.cpu(), 'r+', label=\"inflection points\")\n        \n        plt.legend()\n        plt.show()","500a04ab":"Xinflection = - shallow[0].bias.detach() \/ shallow[0].weight.detach().squeeze()\nuseful = torch.logical_and(Xinflection.greater_equal(0.0), Xinflection.less_equal(1.0)).sum().cpu().item()\n\nprint(f\"{useful} out of {n} inflection points are in a useful position.\")","05c37ff7":"\"\"\"\nWe set the a's first.\nRecall that parameters are tensors wrapped in a torch.nn.parameter.Parameter\nto access the tensor itself we call .data on the Parameter.\nWe call an in-place method of the tensor, in-place means it will modify the existing tensor and not build a new one or generate a copy.\nYou can identify in-place methods by their trailing underscore.\nThe following line will set all values of the weight parameters to 1.0.\n\"\"\"\nshallow[0].weight.data.fill_(1.0)\n\n\"\"\"\nThe b's are next.\nWe can use all of the tensor creation functions we already saw to initialize an existing tensor by specifying the `out` argument.\nWhen `out` is specified the function will not generate a new tensor but fill in the `out` tensor.\nThe following line will fill the b's with values from the range -1 to 0.\n\"\"\"\ntorch.arange(start=-1.0, end=0.0, step=1.0\/n, out=shallow[0].bias.data)\n\n\n\"\"\"\nThe `torch.nn.init` namespace contains a set of common stochastic initialization methods.\nThese function work directly on `Parameter` object so no need to use `.data`.\nThe following line will initialize the weight of the final linear combination with values drawn uniformly from [-1,1].\nNote the trailing underscore indicating this is also an in-place function.\nWe will fix the RNG seed again for reproducability.\n\"\"\"\ntorch.manual_seed(0)\ntorch.nn.init.uniform_(shallow[2].weight, a=-1.0, b=1.0)","24e927eb":"EPOCHS=100\n\nfor epoch in range(1, EPOCHS+1):\n    \n    shallow.train() # Switch model to training mode (gradients are calculated)\n    \n    for x, y in iter(loader):\n        optimizer.zero_grad() # zero out the gradients\n        prediction = shallow(x) # evaluate the batch data\n        loss = (y - prediction).pow(2).sum() # calculate the loss of the actual output vs. the desired output\n        loss.backward() # calculate the gradient of the loss function with respect to the parameters via backpropagation\n        optimizer.step() # now that the gradients are calculated the optimizer can modify the parameters\n    \n    shallow.eval() # Switch model to evaluation mode (no gradients are calculated)\n    Ypred = shallow(X) # network's prediction for the whole interval\n    \n    # Plot every few epochs\n    if epoch % 2 == 0 or epoch == EPOCHS:\n        clear_output(wait=True)\n        plt.figure(figsize=[10, 6])\n        plt.title(f\"Epoch {epoch}\/{EPOCHS}\")\n        plt.xlim(-1,2)\n        plt.ylim(-1,2)\n        plt.plot(X.cpu(), Y.cpu(), '-', label=\"function\")\n        plt.plot(X.cpu(), Ypred.detach().cpu(), '-', label=\"network\") # I'll explain .detach() below\n        \n        Xinflection = - shallow[0].bias.detach() \/ shallow[0].weight.detach().squeeze()\n        Yinflection = shallow(Xinflection.unsqueeze(-1)).squeeze().detach()\n        plt.plot(Xinflection.cpu(), Yinflection.cpu(), 'r+', label=\"inflection points\")\n        \n        plt.legend()\n        plt.show()","6b90d6ca":"from torch.nn.functional import relu\n\ndef f(x):\n    return relu(2*x) - relu(4*x-2) + relu(2*x-2)\n\nxs = torch.arange(0.0, 1.0, 0.002)\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=[16,4])\nax1.plot(xs, f(xs))\nax2.plot(xs, f(f(xs)) )\nax3.plot(xs, f(f(f(xs))))\nax4.plot(xs, f(f(f(f(xs)))))\nplt.show()","82bc2fd5":"X = np.load('..\/input\/signdigitdatasetfixed\/X.npy')\nY = np.load('..\/input\/signdigitdatasetfixed\/Y.npy')\n\n # Lets visualize some of the samples\nshow = (100, 600, 1500, 2000)\nplt.figure(figsize=[4*len(show), 4])\n\nfor i in range(len(show)):\n    plt.subplot(1, len(show), i+1)\n    plt.imshow(X[show[i]], cmap='gray_r')\n    plt.text(4, 4, f\"{Y[show[i]].argmax(-1)}\", va='top' ,color='C1', fontsize=48)\n    plt.axis(\"off\")\n    \nplt.show()\n\nX = torch.tensor(X).unsqueeze(1).to(device)\nY = torch.tensor(Y).argmax(-1).to(device)\n\nprint(X.shape)\nprint(Y.shape)","cb7c84b7":" # Create a dataset from existing tensors\ndataset = torch.utils.data.TensorDataset(X, Y)\n\n# lets follow best practices and split into training and testing data\n# we will split the dataset 80\/20\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\n\n# now to do the actual splitting\ntrain_data, test_data = torch.utils.data.random_split(dataset, [train_size, test_size])","4a9dfcbb":"import torch.nn as nn\n\nnetwork = nn.Sequential(                    # input shape: [B,1,64,64]\n    nn.Conv2d(1, 64, kernel_size=(3, 3)),   # [B,64,62,62]\n    nn.ReLU(), \n    nn.MaxPool2d(2),                        # [B,64,31,31]     \n    \n    nn.Conv2d(64, 64, 3),                   # [B,64,29,29]          \n    nn.ReLU(),\n    nn.MaxPool2d(2),                        # [B,64,14,14]\n    \n    nn.Conv2d(64, 32, 3),                   # [B,32,12,12]     \n    nn.ReLU(),\n    nn.MaxPool2d(2),                        # [B,32,6,6]\n    \n    nn.Conv2d(32, 32, 3),                   # [B,32,4,4]\n    nn.ReLU(),\n    nn.MaxPool2d(2),                        # [B,32,2,2]\n    \n    nn.Flatten(),                           # [B,128]\n    nn.Linear(128, 128),                    # [B,128]\n    nn.ReLU(),\n    \n    nn.Linear(128, 128),                    # [B,128]\n    nn.ReLU(),\n    \n    nn.Linear(128, 10),                     # [B,10]\n)\n\nnetwork = network.to(device)","83ca6c64":"EPOCHS = 100\nBATCH_SIZE = 32\nLEARNING_RATE = 0.0002\nLR_GAMMA = 0.95\n\nloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n\noptimizer = torch.optim.Adam(network.parameters(), lr=LEARNING_RATE)\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, LR_GAMMA)\n\nfrom IPython.display import clear_output # for animating the plot\n\nbatch_loss = []\ntest_accuracy = []","3d3038d8":" for epoch in range(1, EPOCHS+1):\n    \n    network.train() # Switch model to training mode (gradients are calculated)\n    \n    for x, y in iter(loader):\n        optimizer.zero_grad() # zero out the gradients\n        prediction = network(x) # evaluate the batch data\n        loss = nn.functional.cross_entropy(prediction, y) # calculate the loss of the actual output vs. the desired output\n        batch_loss.append(loss.cpu().item())\n        loss.backward() # calculate the gradient of the loss function with respect to the parameters via backpropagation\n        optimizer.step() # now that the gradients are calculated the optimizer can modify the parameters\n    \n    network.eval() # Switch model to evaluation mode (no gradients are calculated)\n     \n    Xtest, Ytest = test_data[:] # retrieve the test data\n    Ypred = network(Xtest).argmax(-1)\n    test_accuracy.append((Ytest==Ypred).to(torch.float).mean().item())\n    \n    # Plot every few epochs\n    if epoch % 1 == 0 or epoch == EPOCHS:\n        clear_output(wait=True)\n        plt.figure(figsize=[10, 8])\n        \n        plt.plot(\n            np.arange(len(batch_loss))*BATCH_SIZE \/ len(train_data), \n            batch_loss\/np.max(batch_loss), \n            \".\", \n            markersize=1.5, \n            label=\"batch loss\", \n            markerfacecolor=(0, 0, 1, 0.3)\n        )\n        \n        plt.plot(test_accuracy, label=\"testing accuracy\")\n        \n        # add some text displaying the current test accuracy\n        x_anchor = 0.3 * plt.xlim()[0] + 0.7 * plt.xlim()[1]\n        y_anchor = 0.5 * plt.ylim()[0] + 0.5 * plt.ylim()[1]\n        plt.text(x_anchor, y_anchor, f\"Test accuracy: {test_accuracy[-1]:.1%}\", ha='center', fontsize=16)\n        \n        plt.legend(ncol=2)\n        plt.show()","64c1ad22":"from sklearn.metrics import confusion_matrix\n\nXtest, Ytest = test_data[:]\nYpred = network(Xtest).argmax(-1)\n\nconf_matrix = confusion_matrix(Ytest.cpu().numpy(), Ypred.cpu().numpy())\n\nfig, ax = plt.subplots(figsize=[8,8])\nax.matshow(conf_matrix, cmap=plt.cm.YlGn, alpha=0.7)\nax.set_xticks(np.arange(10))\nax.set_yticks(np.arange(10))\n\n# Loop over data dimensions and create text annotations.\nfor i in range(10):\n    for j in range(10):\n        if conf_matrix[i,j] > 0:\n            text = ax.text(j, i, conf_matrix[i, j], ha=\"center\", va=\"center\")\n\nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=24)\n\nplt.show()","29ea3f32":"show = np.random.randint(len(dataset), size=10)\n\nXshow = X[show,...]\nYshow = Y[show,...]\nYpred = network(Xshow).argmax(-1)\n\nplt.figure(figsize=[20, 10])\n\nfor i in range(len(show)):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(Xshow[i].cpu().squeeze(), cmap='gray_r')\n    if Ypred[i].cpu() == Yshow[i].cpu():\n        plt.text(48, 4, f\"{Ypred[i].cpu()}\", va='top' ,color='green', fontsize=48)\n    else:\n        plt.text(48, 4, f\"{Ypred[i].cpu()}\", va='top', color='red', fontsize=48)\n        plt.text(48, 24, f\"{Yshow[i].cpu()}\", va='top', color='blue', fontsize=36)\n    plt.axis(\"off\")\n    \nplt.show()","79ee4da1":"wrong = (Y - network(X).argmax(-1)).nonzero(as_tuple=False).squeeze()\n\nshow = wrong[np.random.randint(len(wrong), size=5)]\n\nXshow = X[show,...]\nYshow = Y[show,...]\nYpred = network(Xshow).argmax(-1)\n\nplt.figure(figsize=[20, 10])\n\nfor i in range(len(show)):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(Xshow[i].cpu().squeeze(), cmap='gray_r')\n    if Ypred[i].cpu() == Yshow[i].cpu():\n        plt.text(48, 4, f\"{Ypred[i].cpu()}\", va='top' ,color='green', fontsize=48)\n    else:\n        plt.text(48, 4, f\"{Ypred[i].cpu()}\", va='top', color='red', fontsize=48)\n        plt.text(48, 24, f\"{Yshow[i].cpu()}\", va='top', color='blue', fontsize=36)\n    plt.axis(\"off\")\n    \nplt.show()","f5833dd9":"def extract_parameters(net):\n    kernels = torch.empty(0)\n    weights = torch.empty(0)\n    biases = torch.empty(0)\n\n    for m in net:\n        if isinstance(m, torch.nn.Conv2d):\n            kernels = torch.cat([kernels, m.weight.detach().flatten().cpu()])\n            biases = torch.cat([biases, m.bias.detach().cpu()])\n        elif isinstance(m, torch.nn.Linear):\n            weights = torch.cat([weights, m.weight.detach().flatten().cpu()])\n            biases = torch.cat([biases, m.bias.detach().cpu()])\n            \n    return (kernels, weights, biases)","adbcf13c":"from tqdm import trange\n\nreset_network(network)\n\nparams_before = extract_parameters(network)\n\nfor epoch in trange(1, 400+1):\n\n    network.train() # Switch model to training mode (gradients are calculated)\n\n    for x, y in iter(loader):\n        optimizer.zero_grad() # zero out the gradients\n        prediction = network(x) # evaluate the batch data\n        loss = nn.functional.cross_entropy(prediction, y) # calculate the loss of the actual output vs. the desired output\n        batch_loss.append(loss.cpu().item())\n        loss.backward() # calculate the gradient of the loss function with respect to the parameters via backpropagation\n        optimizer.step() # now that the gradients are calculated the optimizer can modify the parameters\n\n\nparams_after = extract_parameters(network)\n\n\nnetwork.eval()\nXtest, Ytest = test_data[:] # retrieve the test data\nYpred = network(Xtest).argmax(-1)\naccuracy = (Ytest==Ypred).to(torch.float).mean().item()\nprint(f\"Network with {total_parameters(network)} parameters finished with {accuracy:.1%} accuracy.\")","94891386":"plt.figure(figsize=[10, 10])\nid = torch.arange(-0.5,0.5,0.01)\nplt.plot(id, id)\nplt.plot(params_before[0], params_after[0], \".\", markersize=3.0, label='kernels', alpha=0.5)\nplt.plot(params_before[1], params_after[1], \".\", markersize=3.0, label='weights', alpha=0.5)\nplt.plot(params_before[2], params_after[2], \".\", markersize=3.0, label='biases', alpha=0.5)\nplt.legend(markerscale=4)\nplt.show()","6ad9d58f":"plt.figure(figsize=[18,5])\n\nplt.suptitle('Parameter evolution')\n\nr = [[-0.2,0.2],[-0.2,0.2]]\n\nplt.subplot(1,3,1)\nplt.hist2d(params_before[0].numpy(), params_after[0].numpy(), bins=100, cmap='magma', range=r)\nplt.xlim(-0.1, 0.1)\nplt.ylim(-0.1, 0.1)\nplt.colorbar(extend='max')\nplt.title(\"Kernel weights\")\n\nplt.subplot(1,3,2)\nplt.hist2d(params_before[1].numpy(), params_after[1].numpy(), bins=100, cmap='magma', range=r)\nplt.xlim(-0.2, 0.2)\nplt.ylim(-0.2, 0.2)\nplt.colorbar(extend='max')\nplt.title(\"Linear weights\")\n\nplt.subplot(1,3,3)\nplt.hist2d(params_before[2].numpy(), params_after[2].numpy(), bins=100, cmap='magma', range=r)\nplt.xlim(-0.2, 0.2)\nplt.ylim(-0.2, 0.2)\nplt.title(\"Biases\")\n\ncb = plt.colorbar(extend='max')\ncb.set_label('Number of parameters')\n\nplt.show()","3e9c87ab":"changes = torch.cat([(params_after[i]-params_before[i]) for i in range(3)])\n\nplt.hist(changes.numpy(), bins=50)\nplt.show()","85165c43":"reset_network(network)\n\nparams_before2 = extract_parameters(network)\n\nfor epoch in trange(1, 400+1):\n\n    network.train() # Switch model to training mode (gradients are calculated)\n\n    for x, y in iter(loader):\n        optimizer.zero_grad() # zero out the gradients\n        prediction = network(x) # evaluate the batch data\n        loss = nn.functional.cross_entropy(prediction, y) # calculate the loss of the actual output vs. the desired output\n        batch_loss.append(loss.cpu().item())\n        loss.backward() # calculate the gradient of the loss function with respect to the parameters via backpropagation\n        optimizer.step() # now that the gradients are calculated the optimizer can modify the parameters\n\n\nparams_after2 = extract_parameters(network)\n\n\nnetwork.eval()\nXtest, Ytest = test_data[:] # retrieve the test data\nYpred = network(Xtest).argmax(-1)\naccuracy = (Ytest==Ypred).to(torch.float).mean().item()\nprint(f\"Network with {total_parameters(network)} parameters finished with {accuracy:.1%} accuracy.\")","1c61fdb9":"def distance(params1, params2):\n    changes = torch.cat([(params1[i]-params2[i]) for i in range(3)])\n    return changes.pow(2).sum().sqrt()\n\nprint(f\"distance between #1 and #2 at the start: {distance(params_before, params_before2):.2f}\")\nprint(f\"distance between #1 and #2 trained: {distance(params_after, params_after2):.2f}\")\nprint(f\"#1 initial vs trained: {distance(params_before, params_after):.2f}\")\nprint(f\"#2 initial vs trained: {distance(params_before2, params_after2):.2f}\")","aa835a88":"You can see that each module has already initialized its parameters, we will go with these initial values for the moment. Observe that instead of just printing the tensor, each line starts with `Parameter containing:`. Parameters are stored as a `torch.Tensor` wrapped in a `torch.nn.parameter.Parameter`, this wrapper does nothing other than flagging the tensor as being used as a parameter. Note that each of the parameter tensors has been placed on our chosen `device`. Also note the new `requires_grad=True` flag, this indicates to PyTorch that during evaluation of the network it has to collect data to allow it to compute the gradient with respect to these tensors.\n\nLets see what the network outputs at this stage. Since we just want to evaluate the network (inferencing in DL parlace) we do not need the extra work that is required to compute gradients, to temporarily turn gradient computation off we can use the `torch.no_grad` context manager.","7ef3f92a":"## Inspecting the Result\n\nLets take a closer look at the network as it stands now. Let us look at the inflection points of the network's output.","a8ae869a":"Outside of their contents, tensors have 3 important properties:\n* `shape`\n* `dtype` (or datatype)\n* `device`","bb882973":"## A shallow network\n\n$$\n    \\mathcal{N}(x) = \\sum_{i=1}^{n} c_i \\ \\sigma(a_i x + b_i)\n$$\n\nThe basic building blocks of NNs in PyTorch are modules, more specifically subclasses of `torch.nn.Module`. To define a network we create instances of the modules we want and indicated how we want them connected. To construct a shallow ReLU network we need 3 modules:\n- `torch.nn.Linear` : linear and affine transform of its inputs,\n- `torch.nn.ReLU` : element-wise ReLU,\n- `torch.nn.Sequential` : connects its submodules in a sequential or feedforward fashion.\nNow lets put our network together.","4082e297":"### Training","a27b4d3d":"## Sign language digit classification\n\n![](https:\/\/raw.githubusercontent.com\/ardamavi\/Sign-Language-Digits-Dataset\/master\/Examples\/example_0.JPG)\n![](https:\/\/raw.githubusercontent.com\/ardamavi\/Sign-Language-Digits-Dataset\/master\/Examples\/example_1.JPG)\n![](https:\/\/raw.githubusercontent.com\/ardamavi\/Sign-Language-Digits-Dataset\/master\/Examples\/example_2.JPG)\n![](https:\/\/raw.githubusercontent.com\/ardamavi\/Sign-Language-Digits-Dataset\/master\/Examples\/example_3.JPG)\n![](https:\/\/raw.githubusercontent.com\/ardamavi\/Sign-Language-Digits-Dataset\/master\/Examples\/example_4.JPG)\n\n![](https:\/\/raw.githubusercontent.com\/ardamavi\/Sign-Language-Digits-Dataset\/master\/Examples\/example_5.JPG)\n![](https:\/\/raw.githubusercontent.com\/ardamavi\/Sign-Language-Digits-Dataset\/master\/Examples\/example_6.JPG)\n![](https:\/\/raw.githubusercontent.com\/ardamavi\/Sign-Language-Digits-Dataset\/master\/Examples\/example_7.JPG)\n![](https:\/\/raw.githubusercontent.com\/ardamavi\/Sign-Language-Digits-Dataset\/master\/Examples\/example_8.JPG)\n![](https:\/\/raw.githubusercontent.com\/ardamavi\/Sign-Language-Digits-Dataset\/master\/Examples\/example_9.JPG)","15feeb5d":"As well as all the operations you expect in a computing package, such as linear algebra, comparisons and conditionals.","40e4bd93":"### Define a model\n\nArbitrary matrices for high dimensional data are a bit extravagant, so we switch to convolutions as linear operators.\nMaxpooling also helps to quickly reduce the dimensions from $\\mathbb{R}^{64 \\times 64}$ to $\\mathbb{R}^{10}$.\n\n<img src=\"https:\/\/www.bmnsmets.com\/img\/casaday_cnn.png\" alt=\"casaday_cnn\" width=\"900\"\/>","b0ec84cf":"## \"Brute force is subtle in its own way\"","4a9745f9":"The standard way of dealing with overfitting and generalization in supervised learning is by splitting the dataset.","2c8a0d98":"<br\/>\n<br\/>\n<br\/>\n<br\/>","3b8f1fb1":"## Analyzing the parameter evolution","7e983435":"Minor technicality: X and Y have shape [50], meaning when we access an element we get a scalar (i.e. tensor with no shape). Most PyTorch operations require tensors to have shape to operate on. We can fix this by using `.unsqueeuze(-1)`: this will add a dimension of size 1 to a tensor. The argument specifies where the dimension is to be inserted, -1 means at the end. Dimensions of size 1 can also be removed easily with `.squeeze()`, none of these operations change anything about the contents of the tensors, they just change their interpretation.","9c8b3f8e":"### But...","4fb76fd2":"### Load data","492ec5de":"## Using the GPU\n\nSo far we have not done anything that NumPy, Matlab or Mathematica cannot do.\nA major part of any deep learning framework is being able to leverage GPUs.","99b7a45b":"## PyTorch basics\n\nDeep Learning Framework = \n* Highly parallel computations on the GPU \n* Automatic differentation\n* Convenience stuff to make building NNs easier","12c591dd":"<img src=\"https:\/\/www.bmnsmets.com\/img\/cpu_gpu.png\" alt=\"cpu_gpu\" width=\"700\"\/>","4c7209ec":"The usual pointwise mathematical operations are supported.","87b3be7d":"Let us try manually initializing the parameters to control the location of the inflection point.","ccc6306d":"## Going Deep","c6909e9e":"# __CASA Day 2021__: Introduction to Deep Learning with PyTorch\n# _\"brute force is subtle in its own way\"_\n\n<!--\n\n    In this workshop we will take a dive into deep learning using the PyTorch framework.\n    The title is a bit unfair in only mentioning PyTorch, \n    PyTorch is part of a much larger programming ecosystem of dozens of large open source projects centered around the Python programming language.\n    We will be using many of those projects directly or indirectly, in that sense this workshop will be an introduction to that entire ecosystem.\n\n    The workshop is not intended as a programming tutorial and we will not ask you to write any code. \n    But we will provide you with the opportunity to run our code step-by-step yourself as we progress, so we encourage you to bring your laptop.\n    Python is known as being easy to read, even if you lack firsthand experience with the language you should still be able to understand the purpose of the code.\n\n    Most introductions to deep learning are targeted at programmers and computer scientists.\n    This workshop will be an exercise in introducing mathematicians to the subject, where we cover both the technical and mathematical details.\n    We will work through some toy problems to build up intuition about the strengths and weaknesses of neural networks.\n    \n-->\n\n## Jupyter notebooks\n\nThis is a __Jupyter notebook__, much like a Mathematica notebook it is made up of a mix of text and code.\n\nNotebooks consist of cells, this is a __Markdown__ cell. \nMarkdown is a lightweight markup language to add basic formatting to text.\nMarkdown cells also support a subset of $\\LaTeX$:\n$$\n    f(x) = \\int_\\alpha^\\beta \\nabla g(x) \\ dx\n$$\n\nYou can double-click the cell to edit it and then press `Ctrl`+`Enter` to render it.","ba0c2e4c":"Tensors can be copied between devices by using the `.cpu()` and `.cuda()` methods. Copying between devices is relatively slow, so we should not do it more often than necessary.\n\nIt is not possible to perform an operation on tensors that live on different devices.","aa8041f6":"## Supervised learning\n\n__Goal:__ approximate a function $f:X \\to Y$.\n\n#### Three ingredi\u00ebnts:\n1) Data $\\mathcal{D} = \\left\\{ \\left(x_i,y_i\\right) \\right\\}_{i=1}^{N}$ where $y_i = f(x_i)$.\n\n2) A model $F:X \\times W \\to Y$ with some parameter space $W$.\n\n3) A loss function $\\ell : Y \\times Y \\to \\mathbb{R}$ to tell us how good our prediction is.\n\nLoss on a sample: $\\ell_i(w)=\\ell \\left( F(x_i ; w), y_i \\right)$.\n\nLoss on the dataset: $\\ell_{\\text{total}}(w) = \\sum_{i=1}^{N} \\ell_i(w)$.\n\nIdeally we find $w^* = \\arg\\min_{w \\in W} \\ell_{\\text{total}}(w) + \\lambda C(w)$\nwith some optional regularization scheme $C:W\\to \\mathbb{R}$ and $\\lambda>0$.","e92373d2":"The central object in PyTorch (or any deep learning framework) is a _tensor_; a multi-dimensional array.\n\nThere are many ways to create a tensor.","dc439dbb":"## The rest of the system\n\nUnlike _batteries included_ systems like Mathematica or Matlab, Jupyter notebooks provide only that: a _notebook interface_. To make it useful we need to choose a __kernel__ that will execute our code. There are kernels for many languages, not just Python. On here, Kaggle provides us with an environment with a Python kernel and a large selection of packages relevant for deep learning.\n\n<img src=\"https:\/\/www.bmnsmets.com\/img\/jupyter_stack.png\" alt=\"jupyter_stack\" width=\"900\"\/>\n\n\n(+) Open Source (a.e.)\n\n(+) Free\n\n(+) Massive community\n\n(+) Defacto standard for Deep Learning\n\n(-) A bit like Frankenstein's monster","e1f5d913":"We can inspect various parts of the model.","6da50a1c":"## Training\n\nStochastic gradient descent:\n$$\nw_{t+1} = w_{t} - \\eta \\ \\sum_{i \\in I_t}\\nabla \\ell_i(w_{t})\n$$\nwhere $I_t \\subset \\{ 1, \\ldots, N \\}$.\n\nTo do SGD we need to draw random batches from our training data, `torch.utils.data.DataLoader` does exactly that. We also need to make some choices now about how many times we want to go through our dataset (epochs), how big our batches are going to be and what learning rate we want.\n\nFirst we provide a way to reset the network i.e. reinitialize its parameters according to some default distribution.","917952ef":"Below is a __Code__ cell, selecting it and clicking the play button or hitting `Ctrl`+`Enter` will cause the kernel to execute the code contained within. The output produced will be shown below the cell.","d1eb4396":"That is less than optimal, most inflection points are outside of our range of interest, wasting a lot of the capacity of the network. Lets train again with the inflection points visualized to see how they behave.","07059d25":"## An artificial neuron\n\n<img src=\"https:\/\/www.bmnsmets.com\/img\/neuron.png\" alt=\"neuron\" width=\"500\"\/>\n\nLet $x \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$\n$$\n\\mathcal{N}(x) = \\sigma \\left( A x + b \\right)\n$$\nwhere $\\sigma$ is a choice of __activation function__. For today we will stick to the __Rectified Linear Unit__ or ReLU:\n$$\n\\sigma(\\lambda) = ReLU(\\lambda) := \\max \\{ 0, \\lambda \\}.\n$$\nWe abuse notation by applying scalar functions element-wise.","e3160c3b":"### Create dataset"}}