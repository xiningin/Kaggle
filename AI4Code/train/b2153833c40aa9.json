{"cell_type":{"5880f8ee":"code","240cf242":"code","daad84b8":"code","b32f80ec":"code","06d0f3ca":"code","1d7c12dc":"code","9415817b":"code","b6203b52":"code","a4b7b2eb":"code","2885edba":"code","72e71bba":"code","9c86c306":"code","35447500":"code","a4d16686":"code","1e1a8f59":"code","ab06df97":"code","f7502b77":"code","daf4e96c":"code","d431f576":"code","cc69b501":"code","16455f00":"code","f15c8a08":"code","7e7db9be":"code","d90883d2":"code","5cb05696":"markdown","7429146e":"markdown","a0cc3127":"markdown","5ee06421":"markdown"},"source":{"5880f8ee":"# Loads Python libraries used into the Kaggle Kernel.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score","240cf242":"# The data is read in from a CSV file format into a Pandas data frame and is stored in a variable loan_data.\n# Data is stored on Kaggle Kernel by default.\nloan_data = pd.read_csv(\"..\/input\/loan_data.csv\")","daad84b8":"# The data frame structure and format is displayed as well as the formatting of each feature.\n# The loan_data data frame has zero null values and one cateforical feature(purpose) which will be transformed into dummy variables in a later step.\nloan_data.info()","b32f80ec":"# The first 10 rows of the loan_data data frame are displayed.\nloan_data.head(10)","06d0f3ca":"loan_data.describe()","1d7c12dc":"# A histogram of two FICO distributions on top of each other, one for each credit.policy outcome is displayed.\n# A credit policy is extended from businesses to cutomers when credit is issued. We can see below that when credit.policy=0\n# no credit was extended to customers. These cutomers tend to have lower FICO scores when looking at the graph below.\nplt.figure(figsize=(10,6))\nloan_data[loan_data['credit.policy']==1]['fico'].hist(alpha=0.5,color='blue',\n                                              bins=30,label='Has an open credit policy')\nloan_data[loan_data['credit.policy']==0]['fico'].hist(alpha=0.5,color='red',\n                                              bins=30,label='Does not have a credit policy')\nplt.legend()\nplt.xlabel('FICO')","9415817b":"# A histogram of two FICO distributions on top of each other, one for each loan outcome outcome is displayed as a histogram.\n# An interesting trend is those with lower credit scores appear to not pay back the loan balance in full.\nplt.figure(figsize=(10,6))\nloan_data[loan_data['not.fully.paid']==1]['fico'].hist(alpha=0.5,color='red',\n                                              bins=30,label='Loan not paid in full')\nloan_data[loan_data['not.fully.paid']==0]['fico'].hist(alpha=0.5,color='blue',\n                                              bins=30,label='Loan paid in full')\nplt.legend()\nplt.xlabel('FICO')","b6203b52":"# A barplot was created to compare loans that were paid to those which weren't paid back to the described purpose of the loan.\nchart = sns.countplot(data=loan_data,x=\"purpose\",hue=\"not.fully.paid\",palette='Set1',orient='v')\nchart.set_xticklabels(chart.get_xticklabels(), rotation=90)","a4b7b2eb":"# A linear plot is created to show how the number of delinquency payments within the last 2 years have affected interest rates for cusotmers\n# based on FICO score. \n# Customers with no or a few delinquencies that have a higher FICO score get lower interest rates according to the graph below.\nsns.lmplot(y='int.rate',x='fico',data=loan_data,hue='delinq.2yrs',palette=\"cubehelix\")","2885edba":"# A histogram of each feature is created and displayed diagonally. Additionally a scatter plot of each feature is generated to \n# compare the distribution of each features data points for finding relationships between them.\nsns.pairplot(loan_data);","72e71bba":"# The purpose feature is a categorical feature. Using the pandas get dummies function it is turned into dummy features.\ncat_feat=['purpose']\nloan_data = pd.get_dummies(loan_data,columns=cat_feat,drop_first=True)","9c86c306":"# A statistical summary of the loan_data data frame is generated.\nloan_data.describe()","35447500":"# There are features from the above statistical description which have highly variable scales. \n# In order for a machine learning algorithm to work properly the scales of each variable should be normalized.\n# Scikit MinMaxScaler is used to normalize the features in the function below.\nscaler = MinMaxScaler() \nloan_data[['installment','log.annual.inc','dti','fico','days.with.cr.line','revol.bal','revol.util','inq.last.6mths','delinq.2yrs','pub.rec']] = scaler.fit_transform(loan_data[['installment','log.annual.inc','dti','fico','days.with.cr.line','revol.bal','revol.util','inq.last.6mths','delinq.2yrs','pub.rec']])\n\n# loan_data is stored as a scaled_data variable\nscaled_data = loan_data","a4d16686":"# The features within the data frame are now normalized. They each range from 0 to 1 which will allow the machine learning algorithms\n# to work properly and effectively.\n# A statistical summary of scaled_data is generated.\nscaled_data.describe()","1e1a8f59":"scaled_data.head()","ab06df97":"# Displays the count of each class for the not.fully.paid feature. It is apparent that this target feature is imbalance after running the code below.\nscaled_data['not.fully.paid'].value_counts()","f7502b77":"# The scaled_data is split into training and validation sets. 80% is for training and 20% for testing.\n# To create the X variable the not.fully.paid variable is dropped from the data frame.\n# To create the y variable only the not.fully.paid variable is selected.\nX = scaled_data.drop('not.fully.paid',axis=1)\ny = scaled_data['not.fully.paid']\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.20, random_state=1)","daf4e96c":"# The imbalanced not.fully.paid feature has its classes balanced using SMOTE in the code below.\n\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n\n# import SMOTE module from imblearn library \n# Loads the imblearn library which has SMOTE functionality \nfrom imblearn.over_sampling import SMOTE \nsm = SMOTE(random_state = 2) \nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel()) \n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) \n","d431f576":"# A grid Search is preformed for the best parameters for Logistic Regression\nparameters = {\n    'C': np.linspace(1, 50, 50)\n             }\nlr = LogisticRegression(solver='lbfgs', multi_class='auto')\nclf = GridSearchCV(lr, parameters, cv=5, verbose=5, n_jobs=3)\nclf.fit(X_train_res, y_train_res.ravel())","cc69b501":"# The best parameters are shown for LR\nclf.best_params_","16455f00":"# Six algorithm models will be created using the training data.\nmodels = []\nmodels.append(('RFC', RandomForestClassifier()))\nmodels.append(('LR', LogisticRegression(C=47,solver='lbfgs', multi_class='auto',max_iter=1000)))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('DTC', DecisionTreeClassifier()))\nmodels.append(('ADB', AdaBoostClassifier()))\nmodels.append(('SVM', SVC(gamma='auto')))\n# Each model is evaluated using cross validation with 10 splits. \nresults = []\nnames = []\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=10, random_state=1)\n    cv_results = cross_val_score(model, X_train_res, y_train_res, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))","f15c8a08":"# A box & whisker plot is created to compare the six algorithms accuracies against each other.\nplt.boxplot(results, labels=names)\nplt.title('Algorithm Comparison')\nplt.show() #Both logistic regression and support vector algorithms performed the best.","7e7db9be":"# Using the Logistic Regression algorithm the code below will make predictions using the validation dataset.\nmodel = LogisticRegression(C=47,solver='lbfgs', multi_class='auto',max_iter=1000)\nmodel.fit(X_train_res, y_train_res)\npredictions = model.predict(X_validation)","d90883d2":"# The accuracy score, confusion matrix, and classification_report is displayed for the validation data using the Logistic Regression algorithm.\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))\n\n# the model has a 65% accuracy rate in predicting whether or not someone will repay their loan.","5cb05696":"**Feature Engineering**","7429146e":"**Loading and Cleaning Data**","a0cc3127":"**Exploratory Data Analysis**","5ee06421":"**Algorithm Selection & Evaluation**"}}