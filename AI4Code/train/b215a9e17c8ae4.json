{"cell_type":{"a9efbe13":"code","e648b3eb":"code","b6e6e26b":"code","add510a5":"code","ccdd3884":"code","aaf872a1":"code","7bc502c0":"code","21527457":"code","a334724b":"code","3ba6b242":"code","858165c2":"code","72dd9c0e":"code","aff050af":"code","0189cadd":"code","59645fe5":"code","1989f184":"code","33306fb4":"markdown","caf6aba5":"markdown","3c2565fc":"markdown","ea2e06ec":"markdown","3153cc0c":"markdown","4350cae5":"markdown","6afa7999":"markdown","c1ac671f":"markdown","0b43822e":"markdown","b4a2b38f":"markdown","04755db6":"markdown","0f45e38f":"markdown","4a243342":"markdown","66e237c8":"markdown"},"source":{"a9efbe13":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","e648b3eb":"dataset = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndataset","b6e6e26b":"dataset.keys()","add510a5":"import re\n# Importing library to remove stopwords from our text as it will not help to predict our text emotion \n# like all the articles(the, a , an...)\nimport nltk\n\n# Downloading all the stopwords from the nltk library\nnltk.download('stopwords')\n\n# Importing stopwords\nfrom nltk.corpus import stopwords","ccdd3884":"from nltk.stem.porter import PorterStemmer\n\n# Cleaning the texts\n# Creating empty list which will contain all the cleaned texts\n# We will create a for loop to iterate all the texts of our datasets \n# and for each of these review we will apply the cleaning process\n# and after cleaning all the reviews we will add it into created empty list corpus\n\nCleaned_Text = []\nfor i in range(len(dataset)):\n    # Removing every punctuations and commas except a-z or A-Z by space\n    Text = re.sub('[^a-zA-Z]', ' ', dataset['review'][i])\n    \n    # Transforming all the capital letters into lower case letters\n    Text = Text.lower()\n    \n    # Splitting the text into different words so that we can apply stemming \n    Text = Text.split()\n    \n    # Stemming the text\n    ps = PorterStemmer()\n    all_stopwords = stopwords.words('english')\n    \n    # Remove 'not' from the stopwords as it can alter the emotion of expression\n    all_stopwords.remove('not')\n    \n    # Applying Stemming on all words except the stopwords\n    Text = [ps.stem(word) for word in Text if not word in set(all_stopwords)]\n    \n    # Joining all the words together seperating with space\n    Text = ' '.join(Text)\n    \n    # Adding the cleaned text to the empty list\n    Cleaned_Text.append(Text)","aaf872a1":"# Cleaned_Text","7bc502c0":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Creating instance of Count Vectorizer class\ncv = CountVectorizer(max_features = 2000)\n\n# Fit method will take all the words \n# and transform method will put all those in different columns\nX = cv.fit_transform(Cleaned_Text).toarray()\n\n# Creating dependent variable\ny1 = dataset.iloc[:, -1].values","21527457":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y1)","a334724b":"y","3ba6b242":"# Creating dataframe\nDf1 = pd.DataFrame(y1)\nDf1.columns = ['Sentiments']\nDf1['Sentiments Encoded'] = y\nDf1","858165c2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)","72dd9c0e":"# Importing library for naive_bayes classification\nfrom sklearn.naive_bayes import GaussianNB\n\n# Creating instance\nclassifier1 = GaussianNB()\n\n# Traning model as Naive_bayes classification model\nclassifier1.fit(X_train, y_train)\n\n# Predicting test sets results using Model\ny_pred = classifier1.predict(X_test)\n\n# Comparing the predicted y and actual y to ensure accuracy of model\nDf_1 = pd.DataFrame(y_pred)\nDf_1.columns = ['Predicted_Emotions']\nDf_1['Actual_Emotions'] = y_test\nDf_1","aff050af":"# Importing library for creating Confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score","0189cadd":"# Creating Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix :\\n\\n',cm)\n\n# Finding R_score\nR = accuracy_score(y_test, y_pred)\nprint('\\nR score = ',R)","59645fe5":"def Prediction_Single_Text(Text):\n    # Removing every punctuations and commas except a-z or A-Z by space\n    Text = re.sub('[^a-zA-Z]', ' ', Text)\n    \n    # Transforming all the capital letters into lower case letters\n    Text = Text.lower()\n    \n    # Splitting the text into different words so that we can apply stemming \n    Text = Text.split()\n    \n    # Stemming the text\n    ps = PorterStemmer()\n    all_stopwords = stopwords.words('english')\n    \n    # Remove 'not' from the stopwords as it can alter the emotion of expression\n    all_stopwords.remove('not')\n    \n    # Applying Stemming on all words except the stopwords\n    Text = [ps.stem(word) for word in Text if not word in set(all_stopwords)]\n    \n    # Joining all the words together seperating with space\n    Text = ' '.join(Text)\n    \n    Cleaned_Text = [Text]\n    new_X_test = cv.transform(Cleaned_Text).toarray()\n    new_y_pred1 = classifier1.predict(new_X_test)\n    return new_y_pred1","1989f184":"Text = 'I wasted my money in this movie.'\nPrediction_Single_Text(Text)","33306fb4":"### Importing datasets","caf6aba5":"### Creating the Bag of Words model","3c2565fc":"### Algorithm applied : \n+ NLP(Natural Language Processing)","ea2e06ec":"### To predict the number of positive and negative reviews using either classification or deep learning algorithms.","3153cc0c":"### Splitting the datasets into training sets and the test sets","4350cae5":"+ IMDB dataset having 50K movie reviews for natural language processing or Text analytics. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. It provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. \n+ Kaggle datasets: http:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/","6afa7999":"### Cleaning the text\n+ We are going to remove punctuations and different types of special characters, capital letters or lower case letters from our text as these things will create problem while processing the data","c1ac671f":"### Importing Libraries","0b43822e":"### Label encoding the dependent variables containing different sentiments","b4a2b38f":"### Cleaned_Text","04755db6":"### 1. Naive Bayes Classification Model","0f45e38f":"### Stemming of Text : It will convert all the words in their roots.\n\n+ Example:\n- loved as love\n- helped as help\n- hopes as hope\n\n+ Reason: \n- As after cleaning the text when will create the bag of words model we will create sparse matrix with each column will have all the different words all having different emotions. So in order to optimize the dimension of the sparse matrix we need to apply stemming. If we don't apply the stemming then in sparse matrix we would have one column for present tense and other for the past tense that would be same thing so will create redundants and will make sparse matrix more complex with higher dimension. ","4a243342":"# Creating Classification model","66e237c8":"### Project Objective : \n+ To develop a deep learning algorithm to detect different types of sentiments contained in a collection of English sentences or a large paragraph.   "}}