{"cell_type":{"07a5aabe":"code","1d83c04b":"code","6ac4498a":"code","28e3ebed":"code","4da4c378":"code","25a863e4":"code","d1ec674f":"code","757d4179":"code","863766ea":"code","67efcdff":"code","34d15f56":"code","5f19244a":"code","f338a572":"code","6b00c4a6":"code","0dd0983f":"code","1fdcd021":"code","199ed62a":"code","a913e176":"code","47d0148d":"code","3e975221":"code","82431a23":"code","1e52cc4c":"code","b58f6031":"code","40b67c36":"code","6c09711e":"code","e7d90d1e":"code","46b44762":"code","4a8f4eee":"code","2985f13f":"code","9f180778":"code","b63cf2c9":"code","90fdf32b":"code","0338a4fd":"code","72c5e4b7":"code","34097950":"code","82e1689c":"code","5e731d3f":"code","135cc4ca":"code","d3753125":"code","4ca10141":"code","2b008169":"code","6f489829":"code","62e82187":"code","306dc598":"code","68358872":"code","6a23e36d":"code","a8c857b8":"code","95878dd4":"code","2ade292c":"code","36fd0b22":"code","6da57806":"code","6fd2c07c":"code","7d72b747":"code","a15ca867":"markdown","920c0d41":"markdown","7b61cc6c":"markdown","ae6f6164":"markdown","4ed19354":"markdown","f20d0840":"markdown","7bce5f52":"markdown","6d26d688":"markdown","69905ac4":"markdown","1814d11b":"markdown","19831861":"markdown","ed00be74":"markdown","2aa09a2b":"markdown","1eda7d4c":"markdown","f9cd9581":"markdown","871f5525":"markdown","494839b8":"markdown","a3c6262b":"markdown","5387f882":"markdown","31c32c2c":"markdown","41b4e3bb":"markdown","68f3947e":"markdown","a3b59472":"markdown","dcaa2e53":"markdown","c0c26760":"markdown","f0f7b38e":"markdown","ff057d96":"markdown","93e4503f":"markdown","69962619":"markdown","168ea889":"markdown","c4db19fa":"markdown","3e7c961f":"markdown","d2c13190":"markdown","3e211556":"markdown","8c6d8f5a":"markdown"},"source":{"07a5aabe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1d83c04b":"#Importing the data as pandas dataframe. We won't use the test.csv file, as we are\n#concerned with supervised learning here. We rather divide train.csv into train and test set\n#each example then having its own target value known. \n\ntrain = pd.read_csv('..\/input\/train.csv')\n#test = pd.read_csv('..\/input\/test.csv')","6ac4498a":"train.shape","28e3ebed":"train.head()","4da4c378":"# We select 5 random examples (rows) from the train dataset\nrandom_indexes = random.sample(range(train.shape[0]),5)","25a863e4":"#The original images are obtained by reshaping the rowos\noriginal_images = [np.array(train.iloc[element,1:]).reshape(28,28) for element in random_indexes]\n\n#The 784-dimensional array form is instead\narray_representation = [np.array(train.iloc[element,1:]) for element in random_indexes]","d1ec674f":"#Visualizing the orginal images\nfig, axes = plt.subplots(nrows=1, ncols=5)\n\ni=0\nfor ax in axes:\n    ax.imshow(original_images[i], cmap ='gist_gray')\n    i +=1\nfig.tight_layout()","757d4179":"#Visualizing their 784-dimensional array form\n\nfig, axes = plt.subplots(nrows=1, ncols=5)\n\ni=0\nfor ax in axes:\n    ax.imshow(array_representation[0].reshape(784,1), aspect = 0.02,  cmap='gist_gray')\n    i +=1\nfig.tight_layout()","863766ea":"train.describe()","67efcdff":"train.describe().loc['mean'].idxmax()","34d15f56":"temp_im = np.zeros(shape=(28,28))\ntemp_im[407\/\/28, 407%28] = 100\n\nsns.heatmap(temp_im, cmap ='gray')\nplt.show()","5f19244a":"train.groupby('label').pixel0.count().plot.bar()\nplt.show()","f338a572":"#Normalizes each pixel column [-1,1], taking into account that some column is completely filed with 0s\ndef feat_normalize(X):\n    M = X.shape[1]\n    for i in range(M):\n        if np.any(X[:,i]) != 0:\n            min_ = X[:,i].min()\n            max_ = X[:,i].max()\n            X[:,i] =(2*X[:,i]-min_-max_)\/(max_-min_)\n            \n\n            \ndef append_ones(X):\n    \n    s = X.shape[0]\n    \n    ones = np.ones(shape=(s,1))\n    \n    return np.concatenate((ones, X), axis=1)\n\n            \n#functions to calculate precision, recall and F1 score of a model\n            \n\ndef prec_rec_F1(class_rep):\n    precision = []\n    recall = []\n    F1 = []\n\n    for i in range(10):\n        temp = np.zeros(shape=(2,2))\n        temp[0,0] = class_rep.iloc[i,i]\n        temp[0,1] = sum(class_rep.iloc[i,:i]) + sum(class_rep.iloc[i,i+1:])\n        temp[1,0] = sum(class_rep.iloc[:i,i]) + sum(class_rep.iloc[i+1:, i])\n        temp[1,1] = sum(np.diag(class_rep))- class_rep.iloc[i,i]\n    \n        ptemp = temp[0,0]\/(temp[0,0]+ temp[0,1])\n        precision.append([i,ptemp])\n        rectemp = temp[0,0]\/(temp[0,0]+ temp[1,0])\n        recall.append([i,rectemp])\n        F1.append([i,2 * ptemp * rectemp \/(ptemp+rectemp)])\n    \n    return [precision, recall, F1]\n\ndef create_class_rep(prediction, y_test):\n    class_rep =np.zeros(shape=(10,10))\n    \n    for i in range(len(y_test)):\n        x = prediction[i]\n        y = y_test[i]\n        class_rep[x,y] +=1\n        \n    class_rep = pd.DataFrame(class_rep)\n    return class_rep.applymap(int)\n\n\n\n","6b00c4a6":"#Sigmoid function\ndef sigmoid(x):\n    return 1\/(1 + np.exp(-x))\n\n#Cost function of the logistic regression for binary classification, s_i = {0,1} \ndef cost(X, y , theta):\n    dim = X.shape[0]\n    s = sigmoid(np.dot(X,theta))\n    tot = -(np.log(s)*y +np.log(1-s)*(1-y))\n    return 1\/dim *sum(tot)[0]\n    \n#Gradient of the cost function with respect to the parameters theta. To be used in gradient descent below\ndef grad_cost(X, y, theta):\n    \n    dim = X.shape[0]\n    pred = sigmoid(np.dot(X,theta))\n    c1 = 1\/dim * np.transpose(pred-y)\n    return np.transpose(np.dot(c1,X))\n\n#Gradient descent to get the parameter theta\ndef grad_descent(X, y, theta, learning_par, num_iter):\n\n    for i in range(num_iter):\n        #print cost(X,y,theta) to check the cost is monotonically decreasing at each iteration\n        theta = theta - learning_par*grad_cost(X,y,theta)\n        \n    return theta\n\n\n\n","0dd0983f":"#Dividing the training set in train and test set\n\ny = train.iloc[:,0]\nX = train.iloc[:,1:]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 101)\n\n#Normalizing the train and test sets\nX_train = np.array(X_train)\nfeat_normalize(X_train)\nX_train = append_ones(X_train)\n\n\n#Appending the bias column to the train and test matrices\n\nX_test = np.array(X_test)\nfeat_normalize(X_test)\nX_test = append_ones(X_test)\n\n\n\n\n","1fdcd021":"#Create the vector of target lables for each digit 0-9\ny_target = []\nfor i in range(10):\n    y_target.append(y_train.apply(lambda x: 1 if x == i else 0))\n    \n#Initialize the list of training parameters (784+1 (bias) for each digit)\ntheta=[]\n\n#Gradient descent to train the model\nfor i in range(10):\n    ytemp = np.array(y_target[i])\n    ytemp = ytemp.reshape(y_train.shape[0],1)\n\n    thetatemp = np.zeros(shape=(X_train.shape[1],1))\n\n    alpha = 0.03\n    n_iter = 100\n\n    thetatemp = grad_descent(X_train,ytemp,thetatemp,alpha,n_iter)\n    theta.append(thetatemp)\n    print('{}: done!'.format(i))","199ed62a":"plt.imshow(theta[0][1:].reshape(28,28), cmap='gist_gray')\nplt.show()","a913e176":"plt.imshow(theta[1][1:].reshape(28,28), cmap='gist_gray')\nplt.show()","47d0148d":"result = [sigmoid(np.dot(X_test,theta[i])) for i in range(10)]\nresult = np.transpose(np.array(result)).reshape(X_test.shape[0],10)\n\nprediction = (np.array([element.argmax() for element in result])).reshape(X_test.shape[0],1)","3e975221":"#testing accuracy of the prediction\ny_test = np.array(y_test)\ny_test = y_test.reshape(y_test.shape[0],1)\n\n\naccuracy = sum(prediction == y_test)[0]\/(y_test.shape[0])\nprint('Accuracy is: {}%'.format(accuracy))","82431a23":"class_rep = create_class_rep(prediction,y_test)\nclass_rep","1e52cc4c":"precision, recall, F1 = prec_rec_F1(class_rep)","b58f6031":"plt.figure(figsize=(8,8))\n\nplt.xticks(range(10))\nplt.yticks(1\/10*np.array(range(10)))\n\nplt.bar(np.transpose(precision)[0],np.transpose(precision)[1], align='edge', width =-0.25)\nplt.bar(np.transpose(recall)[0],np.transpose(recall)[1],align='center',width = 0.25)\nplt.bar(np.transpose(F1)[0],np.transpose(F1)[1],align='edge',width =0.25)\nplt.legend(labels = ('Precision','Recall','F1'))\n\nplt.tight_layout()","40b67c36":"from sklearn.ensemble import RandomForestClassifier","6c09711e":"#Create a forest with n=100 trees and fot to the model\nforest = RandomForestClassifier(n_estimators=100)\nforest.fit(X_train, y_train)","e7d90d1e":"#Predicting new results\nprediction = forest.predict(X_test)\nprediction = prediction.reshape(prediction.shape[0],1)","46b44762":"#Accuracy\naccuracy = sum(prediction == y_test)[0]\/(y_test.shape[0])\nprint('Accuracy is: {}%'.format(accuracy))","4a8f4eee":"#Classification report\nclass_rep = create_class_rep(prediction,y_test)\nclass_rep","2985f13f":"#And precision, recall, F1\n\nprecision, recall, F1 = prec_rec_F1(class_rep)\n\nplt.xticks(range(10))\nplt.yticks(1\/10*np.array(range(10)))\n\nplt.bar(np.transpose(precision)[0],np.transpose(precision)[1], align='edge', width =-0.25)\nplt.bar(np.transpose(recall)[0],np.transpose(recall)[1],align='center',width = 0.25)\nplt.bar(np.transpose(F1)[0],np.transpose(F1)[1],align='edge',width =0.25)\nplt.legend(labels = ('Precision','Recall','F1'))\n\nplt.tight_layout()\n\n","9f180778":"from sklearn.naive_bayes import GaussianNB","b63cf2c9":"#Create an instance of the NB algorothm and fit to the training set\nclassifier = GaussianNB()\nclassifier.fit(X_test,y_test)","90fdf32b":"#Predicting new results\nprediction = classifier.predict(X_test)\nprediction = prediction.reshape(prediction.shape[0],1)","0338a4fd":"accuracy = sum(prediction == y_test)[0]\/(y_test.shape[0])\nprint('Accuracy is: {}%'.format(accuracy))","72c5e4b7":"#Classification report\nclass_rep = create_class_rep(prediction,y_test)\nclass_rep","34097950":"#Number of examples for each class\nclass_rep['sum'] = class_rep.sum()\n\n#correctly classified examples for each class\ndiag = pd.Series([class_rep.iloc[i,i] for i in range(10)])\n\n#Misclassification rate\nclass_rep['misclass_rate'] = 1 - diag \/ class_rep['sum'] \n\nclass_rep.drop('sum', axis=1)","82e1689c":"#And precision, recall, F1\n\nprecision, recall, F1 = prec_rec_F1(class_rep)\n\nplt.xticks(range(10))\nplt.yticks(1\/10*np.array(range(10)))\n\nplt.bar(np.transpose(precision)[0],np.transpose(precision)[1], align='edge', width =-0.25)\nplt.bar(np.transpose(recall)[0],np.transpose(recall)[1],align='center',width = 0.25)\nplt.bar(np.transpose(F1)[0],np.transpose(F1)[1],align='edge',width =0.25)\nplt.legend(labels = ('Precision','Recall','F1'))\n\nplt.tight_layout()\n\n\n","5e731d3f":"from sklearn.svm import LinearSVC","135cc4ca":"#Create an instance of SVM and cfit to the training set\nclassifier = LinearSVC()\nclassifier.fit(X_train, y_train)","d3753125":"#Predicting\nprediction = classifier.predict(X_test)\nprediction = prediction.reshape(prediction.shape[0],1)","4ca10141":"#Accuracy\naccuracy = sum(prediction == y_test)[0]\/(y_test.shape[0])\nprint('Accuracy is: {}%'.format(accuracy))","2b008169":"class_rep = create_class_rep(prediction,y_test)\nclass_rep","6f489829":"#And precision, recall, F1\n\nprecision, recall, F1 = prec_rec_F1(class_rep)\n\nplt.xticks(range(10))\nplt.yticks(1\/10*np.array(range(10)))\n\nplt.bar(np.transpose(precision)[0],np.transpose(precision)[1], align='edge', width =-0.25)\nplt.bar(np.transpose(recall)[0],np.transpose(recall)[1],align='center',width = 0.25)\nplt.bar(np.transpose(F1)[0],np.transpose(F1)[1],align='edge',width =0.25)\nplt.legend(labels = ('Precision','Recall','F1'))\n\nplt.tight_layout()\n\n","62e82187":"temp_im = np.zeros(shape=(28,28))\ntemp_im[407\/\/28, 407%28] = 100\ntemp_im[380\/\/28, 380%28] = 100\n\nsns.heatmap(temp_im, cmap ='gray')\nplt.show()","306dc598":"#Calculating the covariance matrix, its eigenvalues and eigenvectors\nX_train_trans = np.transpose(X_train)\ncovariance = np.dot(X_train_trans,X_train)\neigenval, eigenvec = np.linalg.eig(covariance)","68358872":"#We pick the first two main eigenvectors. This comes with some advantage concerning visualization.\n#(note their are sorted in descnding order with respect to their associated eigenvalues).\n#Note the imaginary part of the components of those vector is zero.\n#We take the real part just not ot get any error in the following\nPCA_mat = np.real(eigenvec[:,0:2])","6a23e36d":"plt.figure(figsize=(10,10))\nsns.heatmap(PCA_mat, cmap = 'inferno')\nplt.show()","a8c857b8":"X_transf = np.dot(X_train, PCA_mat)","95878dd4":"plt.scatter(X_transf[:,0],X_transf[:,1], c = y_train, cmap ='plasma')\nplt.show()","2ade292c":"y_target = []\nfor i in range(10):\n    y_target.append(y_train.apply(lambda x: 1 if x == i else 0))\n    \n#Initialize the list of training parameters (784+1 (bias) for each digit)\ntheta=[]\n\n#Gradient descent to train the model\nfor i in range(10):\n    ytemp = np.array(y_target[i])\n    ytemp = ytemp.reshape(y_train.shape[0],1)\n\n    thetatemp = np.zeros(shape=(X_transf.shape[1],1))\n\n    alpha = 0.03\n    n_iter = 400\n\n    thetatemp = grad_descent(X_transf,ytemp,thetatemp,alpha,n_iter)\n    theta.append(thetatemp)\n    print('{}: done!'.format(i))","36fd0b22":"X_test_transf = np.dot(X_test, PCA_mat)","6da57806":"result = [sigmoid(np.dot(X_test_transf,theta[i])) for i in range(10)]\nresult = np.transpose(np.array(result)).reshape(X_test.shape[0],10)\n\nprediction = (np.array([element.argmax() for element in result])).reshape(X_test_transf.shape[0],1)","6fd2c07c":"y_test = np.array(y_test)\ny_test = y_test.reshape(y_test.shape[0],1)\n\n\naccuracy = sum(prediction == y_test)[0]\/(y_test.shape[0])\nprint('Accuracy is: {}%'.format(accuracy))","7d72b747":"for k in range(X_train.shape[1]):\n    if 1-sum(eigenval[:k])\/sum(eigenval) < 0.01:\n        print('{} principal components needed'.format(k))\n        break\n    ","a15ca867":"Accuracy is extremely low as compared to what obtained above. It can of course be improved by some optimization technique (beyond the scope of this notebook).","920c0d41":"# Part 1: Import libraries and data + some little EDA","7b61cc6c":"Let us have a look of the general statistical properties of the matrix 'train'. Many pixels have intensity 0, as it is expected: those correspond to the regions at border of the paper where the digit is originally drawn, which are less likely to be filled.","ae6f6164":"We see that the most misclassfied example is \"5\" (95% misclassification rate). ","4ed19354":"\nare most likely highly correlated, in that when drawing a digit, one that hits pixel 380 is likely to hit also pixel 407. We show below a by-hand derivation, although a similar approach can be adopted by making use of PCA from sklearn.decomposition","f20d0840":"Classification is nothing but measuring the overlap of new examples with those \"templates\" theta. This overlap is then passed to the sigmoid function, which in turns yields an output between 1 (perfect overlap) and 0 (no overlap at all). So the larger the overlap of a new unseen example with one of those templates, the larger the probability the example corresponds to the digit the tempalte is representative of.\n\n#### Let us predict new observation now","7bce5f52":"Great, we're all set! We have determined a form for 10 785-dimensional vectors theta[i] which we use to classify new examples. For instance, theta[0] and theta[1] are found as the following (in image form):","6d26d688":"We see it is a $n \\times m$ matrix, with $n = 42000$ examples (images). Note that the first column 'label' denotes the independent variable column, i.e. the true digit corresponding to the image. \n\nEach images is then encoded in a $m = 785-1 = 784$ feature vector, representing the intensity of the pixel. Each element of the vector contains an integer encoding the intensity of a pixel. In other words the original image is decomposed in a $28 \\times 28$ pixel grid, which is reshaped to be represented as a $ 28 \\times 28=784$ column vector.\n\nLet us see below the visual represetation","69905ac4":"That is, this guy","1814d11b":" Now that we have obtained the rotation matrix PCA_mat, we can project the training set along these two main directions","19831861":"#### Question: What's the most frequent digit present in the train set?","ed00be74":"Let us have a look to the \"train\" data:","2aa09a2b":"Where is this low accuracy coming from? Let's see:","1eda7d4c":"We can now test the accuracy of the prediction","f9cd9581":"Transforming the test set as well, and testing the accuracy of the model, we in fact realize the prediction are not optimal...Accuracy ~29%! ","871f5525":"How to choose the number of principal component such that a faithful representation of the original dataset is retained? The rule is to preserve '99%' of the variance of the data. A rough way to check this is to compare the cumulative sum of the k-principal eigenvalues against the full $m$ = 785-list of eigenvalues. In particular  we want to check what's the smallest $k$ such that \n $$ 1- \\frac{\\sum_{k} \\textrm{Eigenvalues}}{\\sum_{m} \\textrm{Eigenvalues}} \\le 0.01 $$","494839b8":"# Part 2: classifying the dataset\n\n## First approach: logistic regression by hand\n\nWe use here the one vs all classification for multiclass problem: \n- take an example ex\n- Consider one of the ten classes an example can be classified into (i.e. class = 3)\n- Get the probabilities for ex to belong to class 3 or not by solving a binary classification model\n- Repeat for each of the ten classes\n- Classify ex -> class =... according to the highest obtained probability\n\nLet us define few functions:","a3c6262b":"## Fourth approach: Support Vector Machine\n\nLet us now turn to the support vector machine case.","5387f882":"# MNIST dataset: a starting-point notebook","31c32c2c":"#### What this components actually are?\n\nThey are linear combinations of the original 784-dimensional vectors. To viusalize them, let's plot a heatmap, where the contribution of each component is encoded in the intensity of the colour","41b4e3bb":"We can plot the obtained result for each class","68f3947e":"For instance the first row is telling us that out of (1257+0+3+13+4+8+21+0+22+1)=1329 examples containing the digit \"0\", 1257 have been correctly classfied, 0 of them have been classified as \"1\", 3 as \"2\", etc. \n\n\nFrom the matrix above we can calculate the precision, recall and F1 score of our model","a3b59472":"Can you verify that by using this number of components the accuracy is actually close to that of the full model?","dcaa2e53":"In this notebook we will classify examples from the MNIST hand-digit dataset according to different classification methods. The accuracy, precision and recall for each classification method will be determined.\n\nWe will use here:\n- Logistic regression (\"by hand\" )\n- Random Forest\n- Naive Bayes\n- Support Vector Machines\n\nFinally we briefly present the result of the principal compnent analysis (PCA), again adopting a \"by-hand\" aproach.\n\nThis notebook might be useful to beginners in ML with Python, but with basic ML knowledge (e.g. linear, polynomial, logistic regression; gradient descent etc.).\nAlso, optimization techniques are beyond the scope of his notebook, and we mainly use classifier from sklearn with standard parameters.","c0c26760":"## Second approach: Random Forest\n\nHere we classify the new examples according the Random Forest algorithm.","f0f7b38e":"Now we can apply the on vs all classification. For each category 0-9 we get the list of target indexes and train the model to classfify whether an example belongs to that category or not, with the respective probability.\n\nEach binary classification is trained by minimizing the cost function via gradient descent","ff057d96":"We can visualize the data. Each colour if a datapoint corresponds to a class. We see it's very hard to distinguish the different clusters. This suggests that by retaining only 2 principal compenents we are losing some substantial amount of information","93e4503f":"## Third approach: Naive Bayes\n\nWe use here the naive Bayes algorithm","69962619":"# Part 3 \/ Appendix: Principal component analysis\n\nAs we have briefly discussed above, most of the pixels have average value ~0, mainly corresponding to the border areas of the original papers where the digits were drawn. This suggests that the 784+1-dimensional vector of features can be reduced, as to account for the dominant components while neglecting those which are not playing a crucial role in the classification. \n\n\nTo achieve that, we use Principal Component Analysis. What we do is basically look for correlation between the pixels. For instance pixels 407 and 380:\n","168ea889":"The most likely pixel to contain part of the hand-written digit is ","c4db19fa":"#### We create a classification matrix\n\nThat is a matrix summarizing the classification for each example:  ","3e7c961f":" To verify this, let us run a standard logistic regression classification","d2c13190":"We see it's number 1. What would we expect? Well, each person who took part to the MNIST experiment had equal probability of writing a digit between 0-9. In practice, some cultural factor might influence their choice. For instance, number one is associated to succes, number 3 to perfection, whereas number 0 might not be so appealing... So if naively one expcets a uniform probability mass distribution for the 0-9 digits, it is not obvious what is going to happen in the infinite-example limit $n -> \\infty$.","3e211556":"### Finally we define few functions for later convenience","8c6d8f5a":" We find an accuracy of ~82%, which can be improved by tuning the number of iterations and the learning parameter in the gradient descent loop."}}