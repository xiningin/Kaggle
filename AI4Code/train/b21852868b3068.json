{"cell_type":{"4ad378de":"code","a9a3b4a0":"code","559ed171":"code","a7ffc0b4":"code","fba1da97":"code","59590f55":"code","1774e56b":"code","6ec93594":"code","b61873b6":"code","7994653a":"code","e22b7838":"code","4b45ea66":"code","dcb44e2e":"code","3f28f45c":"code","5beb9fb2":"code","37c19aa8":"code","d580497f":"markdown","a7bed6b5":"markdown"},"source":{"4ad378de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a9a3b4a0":"import pandas as pd\nimport torch\nfrom torch import nn, optim\nimport torch.utils.data as data_utils\nimport matplotlib.pyplot as plt\n\n\ntest_data = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\ntrain_data = pd.read_csv('..\/input\/digit-recognizer\/train.csv')","559ed171":"print('test image: ', test_data.shape[0])\nprint('train image: ', train_data.shape[0])\nprint('feature: ', test_data.shape[1])","a7ffc0b4":"# convert the data into tensor and normalize the values.\n# For cross entropy loss function, the label values must be integer. (long())\nfeatures_tensor = torch.FloatTensor((train_data.drop(['label'], axis=1).values))\/255\ntargets_tensor = torch.FloatTensor(train_data['label'].values).long()\n\n\nprint(\"train images size: \", features_tensor.shape[0],\n      \"\\nfeatures size: \", features_tensor.shape[1],\n      \"\\nlabels size: \", targets_tensor.shape[0])\n\n# set train_loader with custom data (type: tensor)\ntrain = data_utils.TensorDataset(features_tensor, targets_tensor)\ntrain_loader = data_utils.DataLoader(train, batch_size= 512, shuffle=True)","fba1da97":"# build a network\nmodel = nn.Sequential(nn.Linear(784, 128),\n                      nn.ReLU(),\n                      nn.Linear(128, 64),\n                      nn.ReLU(),\n                      nn.Linear(64, 10),\n                      nn.LogSoftmax(dim=1))\n\n# set the loss\ncriterion = nn.NLLLoss()\n# set the optimizer \/ Stochastic Gradient Descent\noptimizer = optim.SGD(model.parameters(), lr=0.03)\n# set epoch\nepoch = 20","59590f55":"losses = []\n\nfor e in range(epoch):\n    total_loss = 0\n        \n    for images, labels in train_loader:\n        # flatten images \n        images = images.view(images.shape[0], -1)\n        \n        # initialize the gradients\n        optimizer.zero_grad()\n        # calculate the loss\n        output = model(images)\n        loss = criterion(output, labels)\n        \n        total_loss += loss.item()\n        \n        # backpropagation\n        loss.backward()\n        # update the weight\n        optimizer.step()\n                \n        \n    else:\n        epoch_loss = total_loss \/ len(train_loader)\n        # stack the loss\n        losses.append(epoch_loss)\n        print(\"training loss: \", epoch_loss)         ","1774e56b":"%matplotlib inline\nplt.plot(losses)\nplt.ylabel('Training Loss')\nplt.xlabel('training length')\nplt.show()","6ec93594":"import pandas as pd\nimport torch\nfrom torch import nn, optim\nimport torch.utils.data as data_utils\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\n\ndata = pd.read_csv('..\/input\/digit-recognizer\/train.csv')","b61873b6":"# convert the data into tensor and normalize the values.\n# For cross entropy loss function, the label values must be integer. (long())\nfeatures = torch.FloatTensor((data.drop(['label'], axis=1).values))\/255\nlabels = torch.FloatTensor(data['label'].values).long()","7994653a":"# distribute 80 % of the data into the training set \/ 20 % of the data into the validation set.\ntrain_size = int(0.8 * len(data))\nval_size = len(data) - train_size\n\naug_data = data_utils.TensorDataset(features, labels)\ntrain_set, val_set = torch.utils.data.random_split(aug_data, [train_size, val_size])","e22b7838":"# set data loader\ntrain_loader = data_utils.DataLoader(train_set, batch_size= 256, shuffle=True)\ntest_loader = data_utils.DataLoader(val_set, batch_size = 256, shuffle=True)","4b45ea66":"# build a feed-forward neural network.\nclass classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128,64)\n        self.fc4 = nn.Linear(64, 10)\n        \n        self.dropout = nn.Dropout(p = 0.2)\n        \n    def forward(self, x):\n        x = x.view(x.shape[0], -1)\n        \n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = self.dropout(F.relu(self.fc2(x)))\n        x = self.dropout(F.relu(self.fc3(x)))\n        x = F.log_softmax(self.fc4(x), dim=1)\n        \n        return x","dcb44e2e":"# set the configuration \nmodel = classifier()\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr = 0.03)\n\nepochs = 10\nsteps = 0\ntrain_losses, val_losses = [], []","3f28f45c":"for e in range(epochs):\n    running_loss = 0\n    \n    for images, labels in train_loader:\n        \n        optimizer.zero_grad() #initialize gradients\n        \n        output = model(images)\n        loss = criterion(output, labels)\n        loss.backward() # backprogagation\n        optimizer.step() # update the weights\n        \n        running_loss += loss.item()\n    \n    else:\n        accuracy = 0 \n        val_loss = 0\n        \n        # temporarily turn off calculating gradients \n        # to efficiently compute tensors in the validation set.\n        with torch.no_grad(): \n            # turn off dropout\n            # I want to use the same model trained\n            model.eval()\n            for images, labels in test_loader:\n                \n                output = model(images)\n                val_loss += criterion(output, labels)\n                \n                prob = torch.exp(output)\n                top_prob, top_labels = prob.topk(1, dim=1)\n                comparison = top_labels == labels.view(*top_labels.shape)\n                accuracy += torch.mean(comparison.type(torch.FloatTensor)) \n                # torch.mean takes tensors only in float type.\n                \n            # after evaluating the accuracy for validation set,\n            # turn on dropout for the next epoch\n            model.train()\n            \n            train_losses.append(running_loss\/len(train_loader))\n            val_losses.append(val_loss\/len(test_loader))\n            \n            print(\"Epoch: {}\/{}..\".format(e+1, epochs),\n                 \"Training Loss: {:.3f}..\".format(train_losses[-1]),\n                 \"Test Loss: {:.3f}..\".format(val_losses[-1]),\n                 \"Test Accuracy: {:.3f}..\".format(accuracy\/len(test_loader)))\n\n                ","5beb9fb2":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt","37c19aa8":"plt.plot(train_losses, label=\"training loss\")\nplt.plot(val_losses, label='validation loss')\nplt.legend(frameon=False)","d580497f":"### I applied what I learned in \"intro to pytorch\" of Udacity to this problem.\ncourse link: https:\/\/www.udacity.com\/course\/deep-learning-pytorch--ud188\n#### 1. handling with custom data (.csv).\n#### 2. displaying tensor shapes.\n#### 3. building a feed-forward network\n#### 4. backpropagation\n#### 5. loss function (Log Softmax + Negative Log Likelihood loss >> Cross Entropy Loss)\n#### 6. plotting the loss per each epoch.","a7bed6b5":"# update #2\n### Inference (probability) and Validation\n\n\n### evaluate validation set\n##### I used dropout to prevent overfitting."}}