{"cell_type":{"766b9885":"code","18ed2708":"code","94afa7d9":"code","1454b0f8":"code","62d5bcd4":"code","6d2d4589":"code","cf880bf1":"code","6c3fd347":"code","e4924ec9":"code","d679dc7e":"code","2a565ed2":"code","e350421d":"code","916fe0ad":"markdown"},"source":{"766b9885":"# Installing Jovian dependency\n!pip install jovian --upgrade --quiet\n# Importing required dependencies\nimport jovian\nimport os\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F","18ed2708":"# Initial project configuration\nresult = []\nproject_name = 'Human Activity Recognition'\narch = \"Convolution + pooling + convolution + pooling + dense + dense + dense + output\"\nbatch_size = 64\nepochs = 50\nlr = 0.01\nmomentum = 0.9","94afa7d9":"torch.manual_seed(29)","1454b0f8":"# Defining data pre-processing functions and Data_loader class\ndef format_data_x(datafile):\n    x_data = None\n    for item in datafile:\n        item_data = np.loadtxt(item, dtype=np.float)\n        if x_data is None:\n            x_data = np.zeros((len(item_data), 1))\n        x_data = np.hstack((x_data, item_data))\n    x_data = x_data[:, 1:]\n    print(x_data.shape)\n    X = None\n    for i in range(len(x_data)):\n        row = np.asarray(x_data[i, :])\n        row = row.reshape(9, 128).T\n        if X is None:\n            X = np.zeros((len(x_data), 128, 9))\n        X[i] = row\n    print(X.shape)\n    return X\n\ndef format_data_y(datafile):\n    data = np.loadtxt(datafile, dtype=np.int) - 1\n    YY = np.eye(6)[data]\n    return YY\n\ndef load_data():\n    str_folder = '..\/input\/uci-human-activity-recognition\/' + 'UCI HAR Dataset\/'\n    INPUT_SIGNAL_TYPES = [\n        \"body_acc_x_\",\n        \"body_acc_y_\",\n        \"body_acc_z_\",\n        \"body_gyro_x_\",\n        \"body_gyro_y_\",\n        \"body_gyro_z_\",\n        \"total_acc_x_\",\n        \"total_acc_y_\",\n        \"total_acc_z_\"\n    ]\n    str_train_files = [str_folder + 'train\/' + 'Inertial Signals\/' + item + 'train.txt' for item in\n                       INPUT_SIGNAL_TYPES]\n    str_test_files = [str_folder + 'test\/' + 'Inertial Signals\/' +\n                      item + 'test.txt' for item in INPUT_SIGNAL_TYPES]\n    str_train_y = str_folder + 'train\/y_train.txt'\n    str_test_y = str_folder + 'test\/y_test.txt'\n\n    X_train = format_data_x(str_train_files)\n    X_test = format_data_x(str_test_files)\n    Y_train = format_data_y(str_train_y)\n    Y_test = format_data_y(str_test_y)\n\n    return X_train, onehot_to_label(Y_train), X_test, onehot_to_label(Y_test)\n\ndef onehot_to_label(y_onehot):\n    a = np.argwhere(y_onehot == 1)\n    return a[:, -1]\n\nclass Data_loader(Dataset):\n    def __init__(self, samples, labels, t):\n        self.samples = samples\n        self.labels = labels\n        self.T = t\n\n    def __getitem__(self, index):\n        sample, target = self.samples[index], self.labels[index]\n        if self.T:\n            return self.T(sample), target\n        else:\n            return sample, target\n\n    def __len__(self):\n        return len(self.samples)    \n\ndef normalize(x):\n    x_min = x.min(axis=(0, 2, 3), keepdims=True)\n    x_max = x.max(axis=(0, 2, 3), keepdims=True)\n    x_norm = (x - x_min) \/ (x_max - x_min)\n    return x_norm\n\ndef load(batch_size=64):\n    x_train, y_train, x_test, y_test = load_data()\n    x_train, x_test = x_train.reshape(\n        (-1, 9, 1, 128)), x_test.reshape((-1, 9, 1, 128))\n    transform = None\n    train_set = Data_loader(x_train, y_train, transform)\n    test_set = Data_loader(x_test, y_test, transform)\n    train_loader = DataLoader(\n        train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n    return train_loader, test_loader","62d5bcd4":"# Defining network architecture\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=9, out_channels=32, kernel_size=(1, 9)),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(1, 2), stride=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1, 9)),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(1, 2), stride=2)\n        )\n        self.fc1 = nn.Sequential(\n            nn.Linear(in_features=64 * 26, out_features=1000),\n            nn.ReLU()\n        )\n        self.fc2 = nn.Sequential(\n            nn.Linear(in_features=1000, out_features=500),\n            nn.ReLU()\n        )\n        self.fc3 = nn.Sequential(\n            nn.Linear(in_features=500, out_features=6)\n        )\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.conv2(out)\n        out = out.reshape(-1, 64 * 26)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        out = F.softmax(out, dim=1)\n        return out","6d2d4589":"# Train and plot functions\ndef train(model, optimizer, train_loader, test_loader):\n    n_batch = len(train_loader.dataset) \/\/ batch_size\n    criterion = nn.CrossEntropyLoss()\n\n    for e in range(epochs):\n        model.train()\n        correct, total_loss = 0, 0\n        total = 0\n        for index, (sample, target) in enumerate(train_loader):\n            sample, target = sample.to(\n                DEVICE).float(), target.to(DEVICE).long()\n            sample = sample.view(-1, 9, 1, 128)\n            output = model(sample)\n            loss = criterion(output, target)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            _, predicted = torch.max(output.data, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum()\n\n        acc_train = float(correct) * 100.0 \/ (batch_size * n_batch)\n        print(f'Epoch: [{e+1}\/{epochs}], loss: {total_loss}, train acc: {acc_train}%')\n\n        # We proceed now to use the test data to evaluate intermediate results without modifying the model (no training)\n        model.train(False)\n        with torch.no_grad():\n            correct, total = 0, 0\n            for sample, target in test_loader:\n                sample, target = sample.to(\n                    DEVICE).float(), target.to(DEVICE).long()\n                sample = sample.view(-1, 9, 1, 128)\n                output = model(sample)\n                _, predicted = torch.max(output.data, 1)\n                total += target.size(0)\n                correct += (predicted == target).sum()\n        acc_test = float(correct) * 100 \/ total\n        print(f'Epoch: [{e+1}\/{epochs}], test acc: {float(correct) * 100 \/ total}%')\n        result.append([acc_train, acc_test])\n        result_np = np.array(result, dtype=float)\n        np.savetxt('result.csv', result_np, fmt='%.2f', delimiter=',')\n\n\ndef plot():\n    data = np.loadtxt('result.csv', delimiter=',')\n    plt.figure()\n    plt.plot(range(1, len(data[:, 0]) + 1),\n             data[:, 0], color='blue', label='train')\n    plt.plot(range(1, len(data[:, 1]) + 1),\n             data[:, 1], color='red', label='test')\n    plt.legend()\n    plt.xlabel('Epoch', fontsize=14)\n    plt.ylabel('Accuracy (%)', fontsize=14)\n    plt.title('Training and Test Accuracy', fontsize=20)\n    plt.show()","cf880bf1":"#\u00a0Get GPU if available\ndef get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\nDEVICE = get_default_device()\nDEVICE","6c3fd347":"# Loading the data\ntrain_loader, test_loader = load(\n        batch_size=batch_size)","e4924ec9":"# Load to selected device and traing model\nmodel = Network().to(DEVICE)\noptimizer = optim.SGD(params=model.parameters(), lr=lr, momentum=momentum)\ntrain(model, optimizer, train_loader, test_loader)\nresult = np.array(result, dtype=float)\n","d679dc7e":"#\u00a0Saving results to a csv file which will be used by the plot function\nnp.savetxt('result.csv', result, fmt='%.2f', delimiter=',')\n#\u00a0Plotting the accuracy for the train and test data\nplot()","2a565ed2":"# We save the current model\ntorch.save(model.state_dict(), 'human-activity-recognition.pth')","e350421d":"#\u00a0Save hyperparameters, commit this notebook to Jovian.ml\njovian.log_hyperparams(arch=arch, \n                       lrs=lr, \n                       epochs=epochs)\ntrain_acc = result[len(result)-1,0]\ntest_acc = result[len(result)-1,1]\njovian.log_metrics(train_acc=train_acc, test_acc=test_acc)\njovian.commit(project=project_name, outputs=['human-activity-recognition.pth'], environment=None)","916fe0ad":"# Human Acticity Recognition with Deep Learning\nIn this notebook we will use the [Human Activity Recognition Using Smartphones Data Set](https:\/\/archive.ics.uci.edu\/ml\/datasets\/human+activity+recognition+using+smartphones), to train a Convolutional Neural Network and recognise the activity performed by the individuals according to the accelerometer and gyroscope measurements.\n\nSome parts of the below code have been inspired by the starter notebooks from Aakash in Jovian.ml ([link](https:\/\/jovian.ml\/aakashns\/collections\/deep-learning-with-pytorch)) and the example code from Jindong Wang in Github ([link](https:\/\/github.com\/jindongwang\/Deep-learning-activity-recognition))."}}