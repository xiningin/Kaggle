{"cell_type":{"2d05d618":"code","b9ca415d":"code","d67d1b66":"code","2ac129b1":"code","18b7d398":"code","a0f8d693":"code","f09bc778":"code","bb6a3244":"code","23af3888":"code","568f0ee9":"code","b2a10bc7":"code","2da7633a":"code","11196f63":"code","56c3d823":"code","b63cc40b":"code","3711162f":"code","5178c934":"code","6db8b575":"code","b5a850a7":"code","e34b6d74":"code","8379e823":"code","a4ab008e":"code","31661065":"code","d7fad0c1":"code","3e91c51a":"code","abc1dfb3":"code","a3cfe7fe":"code","8194d6b9":"code","76eebe74":"code","1a8be8e8":"code","52704ead":"code","68024167":"code","6483edcc":"code","372b7665":"code","2b55281c":"code","d463bb24":"markdown","99091f2a":"markdown","b0622340":"markdown","d7696e5b":"markdown","94fb4156":"markdown","b6a6bcc6":"markdown"},"source":{"2d05d618":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b9ca415d":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.utils import shuffle\n\nfrom sklearn import ensemble, tree, linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","d67d1b66":"# merger data together without label and stor label variable\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_no_label = train.drop(['SalePrice'], axis = 1)\nlabel = train['SalePrice'].copy()\ndata = pd.concat([train_no_label, test], keys= ['train', 'test'])","2ac129b1":"data.describe()","18b7d398":"label.describe()","a0f8d693":"sns.displot(label)\nplt.show()","f09bc778":"data.info()","bb6a3244":"# check the relationship between numeriacal data\nfor var in ('LotFrontage', 'MasVnrArea', 'GrLivArea', 'TotalBsmtSF'):\n    df = pd.concat([data.loc['train'][var], label], axis=1, keys=[var, 'SalePrice'])\n    df.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));\n    plt.show()","23af3888":"# check the relationship between categorical data\nfor var in ('OverallQual', 'YearBuilt', 'YearRemodAdd'):\n    df = pd.concat([data.loc['train'][var], label], axis=1, keys=[var, 'SalePrice'])\n    f, ax = plt.subplots(figsize=(16,8))\n    fig = sns.boxplot(x=var, y='SalePrice', data=df)\n    fig.axis(ymin=0, ymax=800000)\n    plt.show()","568f0ee9":"# with the label\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=.8, square=True)","b2a10bc7":"k = 11\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\n# or set \"rowvar=False\"\nf, ax = plt.subplots(figsize=(12,9))\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size':10},\n                 yticklabels=cols.values,xticklabels=cols.values)\nplt.show()","2da7633a":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], height = 2.5)\nplt.show();","11196f63":"# check missing value condition \ntotal = data.isnull().sum().sort_values(ascending=False)\npercentage = (data.isnull().sum()\/len(data) * 100).sort_values(ascending=False)\nmissing = pd.concat([total, percentage], keys=['Total', 'Percentage'], axis = 1)\nmissing.head(30)","56c3d823":"# deal with zeros value\nzeros = (data== 0).sum(axis=0).sort_values(ascending=False)\nzeros_percentage = (data== 0).sum(axis=0).sort_values(ascending=False)\/len(data)*100\nzero = pd.concat([zeros, zeros_percentage], axis=1, keys=['Total', 'Percentage'])\nzero.head(10)","b63cc40b":"drop_zeros = zero[zero['Percentage']>10].index\ndrop_missing = missing[missing['Percentage']>0.05].index\ndrops = set([x for x in drop_zeros]+ [x for x in drop_missing])\ndf = data.drop(drops, axis=1)\nfills = missing.loc[(missing['Percentage']<=0.05) & (missing['Percentage']>0)].index\ndf.isnull().sum().sort_values(ascending=False)[:10]","3711162f":"fill_col= []\nfor x in fills.values:\n    if x in df.columns.values:\n        fill_col.append(x)\ncategorical = df[fill_col].dtypes[df[fill_col].dtypes == 'object'].index\nnumerical = df[fill_col].dtypes[df[fill_col].dtypes == 'float64'].index\nlen(categorical) + len(numerical) == len(fill_col)","5178c934":"# fill the missing value\nfor var in numerical :\n    df[var] = df[var].fillna(df[var].median())   \nfor var in categorical:\n    df[var] = df[var] = df[var].fillna(df[var].mode()[0])","6db8b575":"# imputer = SimpleImputer(strategy='most_frequent')\n# imputer.fit(df[fill_col])\n# imputer.statistics_\n# X = imputer.transform(df[fill_col])\n# x = pd.DataFrame(X, index=df.index, columns=fill_col)\n# x1 = df.drop(fill_col, axis =1)\n# x = pd.concat([x1,x], axis=1)\n# x\n# the order of columns are different","b5a850a7":"# Adding total sqfootage feature and removing Basement, 1st and 2nd floor features\ndf['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF']\ndf.drop(['TotalBsmtSF', '1stFlrSF'], axis=1, inplace=True)","e34b6d74":"# figure the categorical features and numerical features\ncate_features = df.dtypes[df.dtypes == 'object'].index.values\nint_features = df.dtypes[df.dtypes == 'int64'].index\nfloat_features = df.dtypes[df.dtypes == 'float64'].index\nnums = list([x for x in int_features]+ [x for x in float_features])","8379e823":"# seprate numerical data and categorical data\nnumeric_features = df[nums]\ncategorical_features = df[cate_features]\nnumeric_features_standardized = (numeric_features - numeric_features.mean())\/numeric_features.std()","a4ab008e":"# categorcial = df.dtypes[df.dtypes == 'object'].index\n# categorcial\n# build function\ndef mulcolumns_dummies(col1, col2):\n    conditions = set([x for x in df[col1]] + [x for x in df[col2]])\n    dummies = pd.DataFrame(data=np.zeros((len(df.index), len(conditions))), index= df.index, columns=conditions)\n    for i, cond in enumerate(zip(df[col1], df[col2])):\n        for x in cond:\n            dummies.iloc[i].loc[x] = 1\n    return dummies\ndummies = mulcolumns_dummies('Condition1', 'Condition2')\ndf = pd.concat([df, dummies.add_prefix('Condition')],axis=1)\ndf.drop(['Condition1', 'Condition2'], axis=1, inplace=True)\ndummies = mulcolumns_dummies('Exterior1st', 'Exterior2nd')\ndf = pd.concat([df, dummies.add_prefix('Exterior')],axis=1)\ndf.drop(['Exterior1st', 'Exterior2nd'], axis=1, inplace=True)\n# OneHoter others\nfor col in df.dtypes[df.dtypes == 'object'].index:\n    for_dummy = df.pop(col)\n    df = pd.concat([df, pd.get_dummies(for_dummy, prefix=col)], axis=1)\ndf","31661065":"# update numercial data\ndf_standardized = df.copy()\ndf_standardized.update(numeric_features_standardized)","d7fad0c1":"### Splitting features\ntrain_features = df.loc['train'].drop('Id', axis=1).select_dtypes(include=[np.number]).values\ntest_features = df.loc['test'].drop('Id', axis=1).select_dtypes(include=[np.number]).values\n\n### Splitting standardized features\ntrain_features_st = df_standardized.loc['train'].drop('Id', axis=1).select_dtypes(include=[np.number]).values\ntest_features_st = df_standardized.loc['test'].drop('Id', axis=1).select_dtypes(include=[np.number]).values","3e91c51a":"## shuffling\ntrain_features_st, train_features, train_labels = shuffle(train_features_st, train_features,label, random_state = 5)","abc1dfb3":"# Prints R2 and RMSE scores\ndef get_score(prediction, lables):    \n    print('R2: {}'.format(r2_score(prediction, lables)))\n    print('RMSE: {}'.format(np.sqrt(mean_squared_error(prediction, lables))))\n\n# Shows scores for train and validation sets    \ndef train_test(estimator, x_trn, x_tst, y_trn, y_tst):\n    prediction_train = estimator.predict(x_trn)\n    # Printing estimator\n    print(estimator)\n    # Printing train scores\n    get_score(prediction_train, y_trn)\n    prediction_test = estimator.predict(x_tst)\n    # Printing test scores\n    print(\"Test\")\n    get_score(prediction_test, y_tst)","a3cfe7fe":"### Splitting train data and test data from the train \nx_train, x_val, y_train, y_val= train_test_split(train_features, train_labels, test_size=0.1, random_state=200)\nx_train_st, x_val_st, y_train_st, y_val_st = train_test_split(train_features_st, train_labels, test_size=0.1, random_state=200)","8194d6b9":"LR = linear_model.LinearRegression().fit(x_train_st,y_train_st)\ntrain_test(LR, x_train_st, x_val_st, y_train_st, y_val_st)","76eebe74":"RDF = ensemble.RandomForestRegressor(bootstrap=True, max_features=8, n_estimators=30, oob_score=True).fit(x_train, y_train)\ntrain_test(RDF, x_train, x_val, y_train, y_val)","1a8be8e8":"GBest = ensemble.GradientBoostingRegressor(random_state=0).fit(x_train, y_train)\ntrain_test(GBest, x_train, x_val, y_train, y_val)","52704ead":"param_grid = [\n    # try 12 (3\u00d74) combinations of hyperparameters\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    # then try 6 (2\u00d73) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n#forest_reg = ensemble.RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(RDF, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(x_train, y_train)","68024167":"grid_search.best_params_\ngrid_search.best_estimator_","6483edcc":"columns = pd.DataFrame(df.columns[1:], columns = ['variable'])\nscore = pd.DataFrame(RDF.feature_importances_, columns = ['score'])\nimportance = pd.concat([score, columns], axis=1).sort_values(ascending=False, by='score')\nimportance[:10]","372b7665":"fig, ax = plt.subplots(figsize=(10, 8))\nsns.barplot(x='score', y='variable', data=importance[:10])\nplt.show()","2b55281c":"results = RDF.predict(test_features)\npd.DataFrame({'Id': test.Id, 'SalePrice': results}).to_csv(\"submission.csv\", index=False)","d463bb24":"## Tune parameters use gridSearch","99091f2a":"## Missing values","b0622340":"## Deal with categorical data and numerical data","d7696e5b":"Thanks to Sergei Neviadomski and Pedro Marcelino\nhttps:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python#2.-First-things-first:-analysing-'SalePrice'\nhttps:\/\/www.kaggle.com\/mynamesp\/how-to-get-to-top-25-with-simple-model-sklearn\/edit\n    ","94fb4156":"## Data Exploration","b6a6bcc6":"## Modeling"}}