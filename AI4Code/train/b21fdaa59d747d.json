{"cell_type":{"b65c88cd":"code","cf63f5b1":"code","6c86c5a2":"code","4f359918":"code","8403b948":"code","5bb6559e":"code","0b0e62e7":"code","c42cc73d":"code","ecad34f1":"code","bee1819c":"code","03e20435":"code","6fb67dda":"code","1e06ee53":"code","c152b9b0":"code","b5b1968b":"code","1c0190a6":"code","0ec072e4":"code","90e82edb":"code","6c9cdb3e":"code","1cbc99ee":"code","4a910894":"code","3510dc59":"code","87dc2606":"code","235afea7":"code","817eb827":"code","194fb457":"code","5d427cd8":"code","19b6c86e":"code","bccc029e":"code","4f6b646c":"code","c8bb16fd":"code","b26985b4":"code","04949e55":"code","63a8ff8a":"code","d9a5fa5a":"code","dfd0e91c":"code","d49eabbe":"code","89121062":"code","defecb5c":"code","c7233b19":"code","28ed2f53":"code","57a243f0":"code","61e32178":"code","fe928429":"code","b81be8e6":"code","0db3ce68":"code","3d4e1d8a":"code","dc3d1c3a":"markdown","28049248":"markdown","3c78305c":"markdown","f14e6f21":"markdown","cf7f18a3":"markdown","81755d68":"markdown","632563fb":"markdown","c7c679d3":"markdown","fd516a88":"markdown","4247a4c0":"markdown","679269bb":"markdown","82e0f191":"markdown","de760dfc":"markdown","250aa4a6":"markdown","54e63c4c":"markdown","8a81ecec":"markdown","ad0f36e6":"markdown"},"source":{"b65c88cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cf63f5b1":"# import the basic libraries we will use in this kernel\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nimport time\nimport datetime\nfrom datetime import datetime\nimport calendar\n\nfrom sklearn import metrics\nfrom math import sqrt\nimport gc\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport itertools\nimport warnings\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nwarnings.filterwarnings(\"ignore\") # specify to ignore warning messages","6c86c5a2":"# Resample the sales by this parameter\nPERIOD = \"M\"\n\nSHOPS = [8, 14, 37, 41, 59]\n\n# this is help us change faster between Kaggle and local machine\nLOCAL = False\n\nif LOCAL:\n    PATH = os.getcwd()\n    FULL_DF_PATH = PATH\n    GB_DF_PATH = PATH\n    OUTPUT_PATH = PATH\nelse:\n    PATH = '..\/input\/competitive-data-science-predict-future-sales\/'\n    FULL_DF_PATH = \"..\/input\/full-df-only-test-all-features\/\"\n    GB_DF_PATH = \"..\/input\/group-by-df\/\"","4f359918":"# prints the local files\ndef print_files():\n    \n    '''\n    Prints the files that are in the current working directory.\n    '''\n    \n    cwd = \"..\/input\/competitive-data-science-predict-future-sales\/\"\n    \n    for f, ff, fff in os.walk(cwd):\n        for file in fff:\n            if file.split(\".\")[1] in [\"pkl\", \"csv\"]:\n                print(file)","8403b948":"# reduces the memory of a dataframe\ndef reduce_mem_usage(df, verbose = True):\n    \n    '''\n    Reduces the space that a DataFrame occupies in memory.\n\n    This function iterates over all columns in a df and downcasts them to lower type to save memory.\n    '''\n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'\\\n              .format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))","5bb6559e":"def generate_shift_features(dataset, lista):\n    objective_column_name = \"_\".join(lista)\n    lista2=lista.copy()\n    gb_df_=dataset.pivot_table(index=lista, values='item_cnt_day', aggfunc=[np.sum,np.mean,np.max]).reset_index()\n    lista2.append(objective_column_name+'_sum')\n    lista2.append(objective_column_name+'_mean')\n    lista2.append(objective_column_name+'_max')\n    gb_df_.columns=lista2    \n    for tipo in ['_sum','_mean','_max']:\n        # Para las agrupaciones de item_cnt_day\n        # Shift 1,2,3,6\n        for x in [1,2,3,4,6]:\n            gb_df_[objective_column_name+tipo+'_shift_'+str(x)]=gb_df_.groupby(lista[1:])[objective_column_name+tipo].shift(x)\n\n        # notice taht in var_3 we use shift(4), we do this because we want to capture the variation of 3 months\n        # and not the variation of month - 3\n        for x in [1,2,3]:\n            gb_df_[objective_column_name+tipo+'_var_'+str(x)] = gb_df_[objective_column_name+tipo+'_shift_1']-\\\n                                                                gb_df_[objective_column_name+tipo+'_shift_'+str(x+1)]\n        for x in [1,2,3]:\n             gb_df_[objective_column_name+tipo+'_var_pct_'+str(x)] = gb_df_[objective_column_name+tipo+'_var_'+str(x)]\/\\\n                                                                    gb_df_[objective_column_name+tipo+'_shift_'+str(x+1)]\n    gb_df_.fillna(-2, inplace = True)  \n    gb_df_.replace([np.inf, -np.inf], -2, inplace = True)\n    return gb_df_","0b0e62e7":"file_dir=\"\/kaggle\/input\/future-sales-part-ii-model-training-2-modelo\/FULL_DF_ANTES_DEL_FEATUREENGENIRERING.pkl\"\nfull_df = pd.read_pickle(file_dir)","c42cc73d":"gb_list = [\"date\", \"shop_id\", \"city\"]\nshop_sales_features =  generate_shift_features(full_df, gb_list)","ecad34f1":"shop_sales_features[shop_sales_features['shop_id']==8 ].head().T","bee1819c":"gb_list = [\"date\", \"item_id\"]\nst = time.time()\nitem_sales_features =  generate_shift_features(full_df, gb_list)\net = time.time()\n(et - st)\/60","03e20435":"item_sales_features[item_sales_features[\"item_id\"] == 30 ].head().T","6fb67dda":"gb_list = [\"date\", \"item_category_id\"]\nmonth_item_category_features =  generate_shift_features(full_df, gb_list)","1e06ee53":"month_item_category_features[month_item_category_features[\"item_category_id\"] == 2].head().T","c152b9b0":"gb_list = [\"date\", \"type_code\"]\nmonth_type_code_features =  generate_shift_features(full_df, gb_list)\n","b5b1968b":"month_type_code_features[month_type_code_features[\"type_code\"] == 1].head().T","1c0190a6":"full_df[\"year\"] = full_df[\"date\"].dt.year\nfull_df[\"month\"] = full_df[\"date\"].dt.month\nfull_df[\"days_in_month\"] = full_df[\"date\"].dt.days_in_month\nfull_df[\"quarter_start\"] = full_df[\"date\"].dt.is_quarter_start\nfull_df[\"quarter_end\"] = full_df[\"date\"].dt.is_quarter_end","0ec072e4":"full_df.head()","90e82edb":"holidays_next_month = {\n    12:8,\n    1:1,\n    2:1,\n    3:0,\n    4:2,\n    5:1,\n    6:0,\n    7:0,\n    8:0,\n    9:0,\n    10:1,\n    11:0\n}\n\nholidays_this_month = {\n    1:8,\n    2:1,\n    3:1,\n    4:0,\n    5:2,\n    6:1,\n    7:0,\n    8:0,\n    9:0,\n    10:0,\n    11:1,\n    12:0\n}\n\nfull_df[\"holidays_next_month\"] = full_df[\"month\"].map(holidays_next_month)\nfull_df[\"holidays_this_month\"] = full_df[\"month\"].map(holidays_this_month)","6c9cdb3e":"def extract_number_weekends(test_month):\n    saturdays = len([1 for i in calendar.monthcalendar(test_month.year, test_month.month) if i[5] != 0])\n    sundays = len([1 for i in calendar.monthcalendar(test_month.year, test_month.month) if i[6] != 0])\n    return saturdays + sundays\n\nfull_df[\"total_weekend_days\"] = full_df[\"date\"].apply(extract_number_weekends)\n\ndate_diff_df = full_df[full_df[\"item_cnt_day\"] > 0][[\"shop_id\", \"item_id\", \"date\", \"item_cnt_day\"]].groupby([\"shop_id\", \"item_id\"])\\\n[\"date\"].diff().apply(lambda timedelta_: timedelta_.days).to_frame()\n\ndate_diff_df.columns = [\"date_diff_sales\"]\n\nfull_df = pd.merge(full_df, date_diff_df, how = \"left\", left_index=True, right_index=True)\n\nfull_df.fillna(-1, inplace = True)","1cbc99ee":"full_df.head()","4a910894":"city_population = {\\\n'\u042f\u043a\u0443\u0442\u0441\u043a':307911, \n'\u0410\u0434\u044b\u0433\u0435\u044f':141970,\n'\u0411\u0430\u043b\u0430\u0448\u0438\u0445\u0430':450771, \n'\u0412\u043e\u043b\u0436\u0441\u043a\u0438\u0439':326055, \n'\u0412\u043e\u043b\u043e\u0433\u0434\u0430':313012, \n'\u0412\u043e\u0440\u043e\u043d\u0435\u0436':1047549,\n'\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f':1228680, \n'\u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439':107560, \n'\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d':1228680, \n'\u041a\u0430\u0437\u0430\u043d\u044c':1257391, \n'\u041a\u0430\u043b\u0443\u0433\u0430':341892,\n'\u041a\u043e\u043b\u043e\u043c\u043d\u0430':140129,\n'\u041a\u0440\u0430\u0441\u043d\u043e\u044f\u0440\u0441\u043a':1083865, \n'\u041a\u0443\u0440\u0441\u043a':452976, \n'\u041c\u043e\u0441\u043a\u0432\u0430':12678079,\n'\u041c\u044b\u0442\u0438\u0449\u0438':205397, \n'\u041d.\u041d\u043e\u0432\u0433\u043e\u0440\u043e\u0434':1252236,\n'\u041d\u043e\u0432\u043e\u0441\u0438\u0431\u0438\u0440\u0441\u043a':1602915 , \n'\u041e\u043c\u0441\u043a':1178391, \n'\u0420\u043e\u0441\u0442\u043e\u0432\u041d\u0430\u0414\u043e\u043d\u0443':1125299, \n'\u0421\u041f\u0431':5398064, \n'\u0421\u0430\u043c\u0430\u0440\u0430':1156659,\n'\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434':104579, \n'\u0421\u0443\u0440\u0433\u0443\u0442':373940, \n'\u0422\u043e\u043c\u0441\u043a':572740, \n'\u0422\u044e\u043c\u0435\u043d\u044c':744554, \n'\u0423\u0444\u0430':1115560, \n'\u0425\u0438\u043c\u043a\u0438':244668,\n'\u0426\u0438\u0444\u0440\u043e\u0432\u043e\u0439':1228680, \n'\u0427\u0435\u0445\u043e\u0432':70548, \n'\u042f\u0440\u043e\u0441\u043b\u0430\u0432\u043b\u044c':608353\n}\n\ncity_income = {\\\n'\u042f\u043a\u0443\u0442\u0441\u043a':70969, \n'\u0410\u0434\u044b\u0433\u0435\u044f':28842,\n'\u0411\u0430\u043b\u0430\u0448\u0438\u0445\u0430':54122, \n'\u0412\u043e\u043b\u0436\u0441\u043a\u0438\u0439':31666, \n'\u0412\u043e\u043b\u043e\u0433\u0434\u0430':38201, \n'\u0412\u043e\u0440\u043e\u043d\u0435\u0436':32504,\n'\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f':46158, \n'\u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439':54122, \n'\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d':46158, \n'\u041a\u0430\u0437\u0430\u043d\u044c':36139, \n'\u041a\u0430\u043b\u0443\u0433\u0430':39776,\n'\u041a\u043e\u043b\u043e\u043c\u043d\u0430':54122,\n'\u041a\u0440\u0430\u0441\u043d\u043e\u044f\u0440\u0441\u043a':48831, \n'\u041a\u0443\u0440\u0441\u043a':31391, \n'\u041c\u043e\u0441\u043a\u0432\u0430':91368,\n'\u041c\u044b\u0442\u0438\u0449\u0438':54122, \n'\u041d.\u041d\u043e\u0432\u0433\u043e\u0440\u043e\u0434':31210,\n'\u041d\u043e\u0432\u043e\u0441\u0438\u0431\u0438\u0440\u0441\u043a':37014 , \n'\u041e\u043c\u0441\u043a':34294, \n'\u0420\u043e\u0441\u0442\u043e\u0432\u041d\u0430\u0414\u043e\u043d\u0443':32067, \n'\u0421\u041f\u0431':61536, \n'\u0421\u0430\u043c\u0430\u0440\u0430':35218,\n'\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434':54122, \n'\u0421\u0443\u0440\u0433\u0443\u0442':73780, \n'\u0422\u043e\u043c\u0441\u043a':43235, \n'\u0422\u044e\u043c\u0435\u043d\u044c':72227, \n'\u0423\u0444\u0430':35257, \n'\u0425\u0438\u043c\u043a\u0438':54122,\n'\u0426\u0438\u0444\u0440\u043e\u0432\u043e\u0439':46158, \n'\u0427\u0435\u0445\u043e\u0432':54122, \n'\u042f\u0440\u043e\u0441\u043b\u0430\u0432\u043b\u044c':34675\n}\n\nfull_df[\"city_population\"] = full_df[\"city\"].map(city_population)\n\nfull_df[\"city_income\"] = full_df[\"city\"].map(city_income)\n\nfull_df[\"price_over_income\"] = full_df[\"item_price\"]\/full_df[\"city_income\"]","3510dc59":"full_df = pd.merge(full_df, shop_sales_features, on = [\"date\", \"shop_id\"], how = \"left\")\n\n\nfull_df = pd.merge(full_df, item_sales_features, on = [\"date\", \"item_id\"], how = \"left\")\n\n\nfull_df = pd.merge(full_df, month_item_category_features, on = [\"date\", \"item_category_id\"], how = \"left\")\n\n\nfull_df = pd.merge(full_df, month_type_code_features, on = [\"date\", \"type_code\"], how = \"left\")","87dc2606":"list(full_df.head().columns)","235afea7":"# delete dfs with features\ndel shop_sales_features, item_sales_features, month_item_category_features, month_type_code_features","817eb827":"# delete all the previous df\n#del shops_df, items_df, items_category_df, sales_df, test_df, cartesian_product, gb_df\ngc.collect()","194fb457":"full_df.rename(columns = {\"item_cnt_day\":\"sales\"}, inplace = True)","5d427cd8":"full_df['sales'].max()","19b6c86e":"st = time.time()\n\nfull_df.to_pickle(\"FULL_DF_ONLY_TEST_ALL_FEATURES.pkl\")\n\net = time.time()\n\n(et - st)\/60","bccc029e":"# Solom para 5 tiendas\nfull_df = full_df[full_df[\"shop_id\"].isin(SHOPS)]","4f6b646c":"# delete all the columns where lags features are - 2 (shift(6))\nfull_df = full_df[full_df[\"date\"] > np.datetime64(\"2013-06-30\")]","c8bb16fd":"cols_to_drop = [\n\n'revenue',\n'shop_name',\n'city_x',\n'city_y',\n'item_name',\n'item_category_name',\n'split',\n'type',\n'subtype',\n    \n\"date_shop_id_city_sum\",\n\"date_shop_id_city_mean\",\n\"date_shop_id_city_max\",\n\n\"date_item_id_sum\",\n\"date_item_id_mean\",\n\"date_item_id_max\",\n    \n\"date_item_category_id_sum\",\n\"date_item_category_id_mean\",\n\"date_item_category_id_max\",\n\n\"date_type_code_sum\",\n\"date_type_code_mean\",\n\"date_type_code_max\"]","b26985b4":"full_df.drop(cols_to_drop, inplace = True, axis = 1)","04949e55":"# split the data into train, validation and test dataset\ntrain_index = sorted(list(full_df[\"date\"].unique()))[:-2]\n\nvalida_index = [sorted(list(full_df[\"date\"].unique()))[-2]]\n\ntest_index = [sorted(list(full_df[\"date\"].unique()))[-1]]","63a8ff8a":"X_train = full_df[full_df[\"date\"].isin(train_index)].drop(['sales', \"date\"], axis=1)\nY_train = full_df[full_df[\"date\"].isin(train_index)]['sales']\n\nX_valida = full_df[full_df[\"date\"].isin(valida_index)].drop(['sales', \"date\"], axis=1)\nY_valida = full_df[full_df[\"date\"].isin(valida_index)]['sales']\n\nX_test = full_df[full_df[\"date\"].isin(test_index)].drop(['sales', \"date\"], axis = 1)\nY_test = full_df[full_df[\"date\"].isin(test_index)]['sales']","d9a5fa5a":"st = time.time()\n\nmodel = XGBRegressor(seed = 175)\n\nmodel_name = str(model).split(\"(\")[0]\n\nday = str(datetime.now()).split()[0].replace(\"-\", \"_\")\nhour = str(datetime.now()).split()[1].replace(\":\", \"_\").split(\".\")[0]\nt = str(day) + \"_\" + str(hour)\n\nmodel.fit(X_train, Y_train, eval_metric = \"rmse\", \n    eval_set = [(X_train, Y_train), (X_valida, Y_valida)], \n    verbose = True, \n    early_stopping_rounds = 10)\n\net = time.time()\n\nprint(\"Training took {} minutes!\".format((et - st)\/60))","dfd0e91c":"pickle.dump(model, open(\"{}_{}.dat\".format(model_name, t), \"wb\"))","d49eabbe":"print(\"{}_{}.dat\".format(model_name, t))","89121062":"importance = model.get_booster().get_score(importance_type = \"gain\")\n\nimportance = {k: v for k, v in sorted(importance.items(), key = lambda item: item[1])}","defecb5c":"fig, ax = plt.subplots(figsize=(15, 30))\nplot_importance(model, importance_type = \"gain\", ax = ax)\nplt.savefig(\"{}_{}_plot_importance.png\".format(model_name, t))","c7233b19":"Y_valida_pred = model.predict(X_valida)","28ed2f53":"metrics.r2_score(Y_valida, Y_valida_pred)","57a243f0":"rmse_valida = sqrt(metrics.mean_squared_error(Y_valida, Y_valida_pred))\nrmse_valida","61e32178":"Y_test_predict = model.predict(X_test)","fe928429":"Y_test_predict.sum()","b81be8e6":"Y_test_predict.max()","0db3ce68":"Y_test.sum()","3d4e1d8a":"Y_test.max()","dc3d1c3a":"# Predict and model evaluation","28049248":"# Date and shop_id features","3c78305c":"Parto de este dataset y empezamos a sacar las variables:","f14e6f21":"# Date and item_category features","cf7f18a3":"Esta ejecucion no tarda ni un seg, mientras que a la clase python le lleva 3 mins. ","81755d68":"# Funciones utiles","632563fb":"# Date and type_code features","c7c679d3":"# Funcion generate_shift_features ()","fd516a88":"# Cargo el Dataset del pickle creado antes del feature engeniering","4247a4c0":"# Basic model train","679269bb":"# Datetime features","82e0f191":"# Date and item_id features","de760dfc":"# Join full sales df with all the features generated","250aa4a6":"# Adding holiday and number of weekends data ","54e63c4c":"#  Feature importance","8a81ecec":"Para entender bien que hacias, he desmontado tu clase python y la he analizado. Me he creado una funcion pyhton normal que hace lo mismo, salvo alguna salvedad: Cuando haces las diversas agrupaciones sobre el campo item_cnt_day (SUM y MEAN), le he metido tambien el MAX() y cuando relleno los nulos y los inf he puesto -2 ya que he visto que hay valores -1 y asi pongo un valor disitnto a estos para marcar los nulos. Con los MAX en cada agrupacion ya me salen unas cuantas variables nuevas.","ad0f36e6":"He intentado optimizar el proceso y cuando calculo las variaciones (_var_ y _var_pct), no calculo de nuevo los shift ,como hace la clase python, sino que uso los shit calculados anteriormente. De esta manera esta funcion es bastante mas rapida que la clase python. Para optimizar mas le proceso me calculo el shift(4), que inicialmente no se calculaba."}}