{"cell_type":{"d16c44e9":"code","9b1c4d48":"code","3b97bfe8":"code","d55f1129":"code","21bb6400":"code","9ca12860":"code","2cf28e9e":"code","a6f69a0f":"code","678b97b9":"code","1d80d629":"code","cb47f9c3":"code","7196e495":"code","9cc8c2f5":"code","02089813":"code","f1ee0161":"code","8d130b1b":"code","86fe188b":"code","7d24924c":"code","6488f8ae":"code","19c47098":"code","397d6567":"code","1af9ce6b":"code","16524e75":"code","e20934fa":"code","5cb72a11":"code","5b9c3146":"code","b021bf48":"code","a60b3db8":"markdown","5817f0aa":"markdown","bab1380e":"markdown","fa2443f1":"markdown","a50cf671":"markdown","71118c95":"markdown","b4144d5f":"markdown","9b7ff23b":"markdown","50783db6":"markdown","4a5c9bba":"markdown","9253b648":"markdown","15ff7103":"markdown","4df49bf5":"markdown"},"source":{"d16c44e9":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, learning_curve, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import f1_score, roc_auc_score, plot_confusion_matrix, accuracy_score\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport os","9b1c4d48":"ds = pd.read_csv('..\/input\/ethereum-frauddetection-dataset\/transaction_dataset.csv', index_col=[0])\nds.drop(columns='Index', inplace=True)","3b97bfe8":"# sampling some observations\nprint(ds.shape)\nds.sample(3)","d55f1129":"# column names\ndisplay(ds.columns)\n\n# describtion of numeratic columns\ndisplay(ds.describe())\n\n# Non-Null Count and type of columns\ndisplay(ds.info())","21bb6400":"# How many non-unique adresses?\n_, uniq_idx, counts = np.unique(ds['Address'].to_numpy(), return_index=True, return_counts=True)\nnon_unique_addresses_idx = uniq_idx[counts > 1]\nprint(\"non-unique adresses count: {}\".format(len(non_unique_addresses_idx), end='\\n\\n'))\n# What are the flags of non-uniqe adresses\nnon_unique_addresses_flags = ds.iloc[non_unique_addresses_idx]['FLAG']\nprint(\"flags of non-unique adresses: \", end='')\nprint(*non_unique_addresses_flags)\n\nds.drop(columns='Address', inplace=True)","9ca12860":"display(np.unique(ds[' ERC20 most sent token type'].astype(str)))\ndisplay(np.unique(ds[' ERC20_most_rec_token_type'].astype(str)))\n\nds.drop(columns=[' ERC20 most sent token type', ' ERC20_most_rec_token_type'], inplace=True)","2cf28e9e":"# are classes balanced?\nprint('class : count : percent')\nprint('0     : {}  : {:.2%}'.format(sum(ds['FLAG']==0), sum(ds['FLAG']==0)\/len(ds['FLAG']) ))\nprint('1     : {}  : {:.2%}'.format(sum(ds['FLAG']==1), sum(ds['FLAG']==1)\/len(ds['FLAG']) ))\n\nsns.heatmap(ds.iloc[:,:1])\nplt.show()","a6f69a0f":"ds.drop(columns=[' ERC20 avg time between rec tnx', ' ERC20 avg time between rec 2 tnx', ' ERC20 avg time between contract tnx',\n                 ' ERC20 min val sent contract', ' ERC20 max val sent contract', ' ERC20 avg val sent contract', ' ERC20 avg time between sent tnx'], inplace=True)","678b97b9":"# missing values\nmissing_values = ds.isna()\nmissing_percent = missing_values.sum() \/ ds.shape[0] * 100\nmissing_df = pd.DataFrame([missing_values.sum(), missing_percent], ['count', 'percent'])\ndisplay(missing_df.sort_values(by='percent', axis=1, ascending=False))\nmissing_df.sort_values(by='percent', axis=1, ascending=False).to_csv('missing.csv')\n\nsns.heatmap(missing_values, cbar=False, cmap='magma')\nplt.show()","1d80d629":"non_fraud_rows, fraud_rows = np.where( [ds.iloc[:,0]==1] )\nprint(ds.iloc[fraud_rows,:].isna().sum()[-20:])","cb47f9c3":"missing_columns = ds.columns[ds.isna().sum() > 0]","7196e495":"# correlation\ncorr = ds.corr()\nplt.figure(figsize=(20,12))\nsns.heatmap(np.abs(corr), cmap='coolwarm')\nplt.show()","9cc8c2f5":"preprocessing_pipeline = Pipeline([\n    ('impoter', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\nX = ds.drop(columns='FLAG').to_numpy()\ny = ds['FLAG'].to_numpy()\n\nrandom_permutation = np.random.permutation(len(X))\nX = X[random_permutation]\ny = y[random_permutation]","02089813":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n\nX_train = preprocessing_pipeline.fit_transform(X_train)\nX_test = preprocessing_pipeline.transform(X_test)","f1ee0161":"def evaluate_models(X, y, models, cv):\n    f1_scores = dict()\n    acc_scores = dict()\n    \n    for i, model in enumerate(models):\n        clf_pipeline = make_pipeline(preprocessing_pipeline, model)\n        results = cross_validate(clf_pipeline, X, y, cv=cv, scoring=['f1', 'accuracy'], n_jobs=-1)\n        avg_f1 = np.mean(results['test_f1'])\n        avg_acc = np.mean(results['test_accuracy'])\n        \n        model_name = model.__class__.__name__\n        f1_scores[model_name] = avg_f1\n        acc_scores[model_name] = avg_acc\n        print('{}-of-{}: {} f1={}, acc={}'.format(i+1, len(models), model_name, avg_f1, avg_acc))\n    return f1_scores, acc_scores","8d130b1b":"cv = StratifiedKFold(5, shuffle=True, random_state=42)\n\nclassifiers = [\n    LogisticRegression(random_state=42),\n    KNeighborsClassifier(),\n    RandomForestClassifier(random_state=42),\n    lgb.LGBMClassifier(random_state=42),\n    xgb.XGBClassifier(random_state=42),\n    SVC(random_state=42),\n    AdaBoostClassifier(random_state=42),\n    GaussianNB(),\n    MLPClassifier(random_state=42),\n]","86fe188b":"f1_scores, acc_scores = evaluate_models(X, y, classifiers, cv)","7d24924c":"def visualize_scores(f1_scores, acc_scores):\n    x = np.arange(len(f1_scores))\n    width = 0.45\n    \n    f1_values = list(f1_scores.values())\n    acc_values = list(acc_scores.values())\n    \n    plt.figure(figsize=(15, 8)).tight_layout()\n    plt.bar(x - width \/ 2, f1_values, width, label='f1')\n    plt.bar(x + width \/ 2, acc_values, width, label='accuracy')\n    \n    for index, value in enumerate(x - width \/ 2):\n        plt.text(value, f1_values[index], '{:.3}'.format(f1_values[index]),\n                 verticalalignment='bottom', horizontalalignment='center', fontsize=10)\n\n    for index, value in enumerate(x + width \/ 2):\n        plt.text(value, acc_values[index], '{:.3}'.format(acc_values[index]),\n                 verticalalignment='bottom', horizontalalignment='center', fontsize=10)    \n    \n    classifiers_names = f1_scores.keys()\n    plt.xticks(x, classifiers_names, rotation=40, horizontalalignment='right', fontsize=10)\n    plt.legend()\n\nvisualize_scores(f1_scores, acc_scores)","6488f8ae":"xgb_parameters = {\n    'xgbclassifier__n_estimators': range(1000, 4001, 1000),\n    'xgbclassifier__gamma': [0, 0.5, 1],\n    'xgbclassifier__max_depth': [5, 6, 7]\n}\n\nxgb_pipeline = make_pipeline(preprocessing_pipeline, xgb.XGBClassifier(random_state=42))\n# xgb_pipeline.steps\nxgb_grid_search = RandomizedSearchCV(\n    xgb_pipeline,\n    param_distributions=xgb_parameters,\n    scoring = 'f1',\n    n_iter = 12,\n    n_jobs = -1,\n    cv = 5,\n    random_state=42\n)\n\nxgb_grid_search.fit(X, y)","19c47098":"#grid_search.grid_scores_\ndisplay(xgb_grid_search.best_score_)\ndisplay(xgb_grid_search.best_params_)","397d6567":"LGBM_parameters = {\n        'lgbmclassifier__bagging_fraction': [0, 0.2, 0.5, 0.8, 1],\n        'lgbmclassifier__feature_fraction': [0.5, 0.8],\n        'lgbmclassifier__max_depth': [6, 10, 13, 16, 20],\n        'lgbmclassifier__min_data_in_leaf': range(40, 180, 20),\n        'lgbmclassifier__num_leaves': range(500, 2500, 300)\n}\n\nLGBM_pipeline = make_pipeline(preprocessing_pipeline, lgb.LGBMClassifier(random_state=42))\nLGBM_grid_search = RandomizedSearchCV(\n    LGBM_pipeline,\n    param_distributions=LGBM_parameters,\n    scoring = 'f1',\n    n_iter = 60,\n    n_jobs = -1,\n    cv = 5,\n    random_state=42\n)\n\nLGBM_grid_search.fit(X, y)","1af9ce6b":"display(LGBM_grid_search.best_score_)\ndisplay(LGBM_grid_search.best_params_)","16524e75":"RFC_parameters = {\n        'randomforestclassifier__n_estimators': range(50, 1050, 100),\n        'randomforestclassifier__max_depth': range(50, 300, 20),\n        'randomforestclassifier__min_samples_split': [2, 5, 10],\n        'randomforestclassifier__min_samples_leaf': [1, 2, 4],\n        'randomforestclassifier__bootstrap': [True, False]\n}\n\nRFC_pipeline = make_pipeline(preprocessing_pipeline, RandomForestClassifier(random_state=42))\nRFC_grid_search = RandomizedSearchCV(\n    RFC_pipeline,\n    param_distributions=RFC_parameters,\n    scoring = 'f1',\n    n_iter = 24,\n    n_jobs = -1,\n    cv = 5,\n    random_state=42\n)\n\nRFC_grid_search.fit(X, y)","e20934fa":"display(RFC_grid_search.best_score_)\ndisplay(RFC_grid_search.best_params_)","5cb72a11":"best_model = lgb.LGBMClassifier(num_leaves=1400, min_data_in_leaf=100, max_depth=10,\n    feature_fraction=0.8, bagging_fraction=0.5, random_state=42)\n\nbest_model.fit(X_train, y_train)\n\npredictions = best_model.predict(X_test)","5b9c3146":"print(\"f1 score = {}\".format(f1_score(y_test, predictions)))\n\nprint(\"ROC AUC score = {}\".format(roc_auc_score(y_test, predictions)))\n\nprint(\"accuracy score = {}\".format(accuracy_score(y_test, predictions)))\n\ndisplay(plot_confusion_matrix(best_model, X_test, y_test))","b021bf48":"def plot_features_importance(feature_importance):\n    column_names = ds.drop(columns='FLAG').columns\n\n    df_feature_importance = pd.DataFrame(sorted(zip(feature_importance, column_names)),\n                                       columns=['Importance value', 'Feature'])\n    df_feature_importance = df_feature_importance.sort_values('Importance value', ascending=False)\n\n    plt.figure(figsize=(9, 7)).tight_layout()\n    sns.barplot(y=\"Feature\", x=\"Importance value\", data=df_feature_importance)\n    plt.show()\n\nplot_features_importance(best_model.feature_importances_)","a60b3db8":"As we expected every missing value is in fraud rows. That means that almost 40% of fraud rows have missing values.","5817f0aa":"Deleting columns that holds only zeros.","bab1380e":"Looks like missing values are highly connected to fraud cases","fa2443f1":"## This dataset contains rows of known fraud and valid transactions made over Ethereum.\n\n### Task: EDA & Prediction of Fraud\/Valid Transaction\n\nHere is a description of the rows of the dataset:\n* Index: the index number of a row\n* Address: the address of the ethereum account\n* FLAG: whether the transaction is fraud or not\n* Avg min between sent tnx: Average time between sent transactions for account in minutes\n* Avgminbetweenreceivedtnx: Average time between received transactions for account in minutes\n* TimeDiffbetweenfirstand_last(Mins): Time difference between the first and last transaction\n* Sent_tnx: Total number of sent normal transactions\n* Received_tnx: Total number of received normal transactions\n* NumberofCreated_Contracts: Total Number of created contract transactions\n* UniqueReceivedFrom_Addresses: Total Unique addresses from which account received transactions\n* UniqueSentTo_Addresses20: Total Unique addresses from which account sent transactions\n* MinValueReceived: Minimum value in Ether ever received\n* MaxValueReceived: Maximum value in Ether ever received\n* AvgValueReceived5Average value in Ether ever received\n* MinValSent: Minimum value of Ether ever sent\n* MaxValSent: Maximum value of Ether ever sent\n* AvgValSent: Average value of Ether ever sent\n* MinValueSentToContract: Minimum value of Ether sent to a contract\n* MaxValueSentToContract: Maximum value of Ether sent to a contract\n* AvgValueSentToContract: Average value of Ether sent to contracts\n* TotalTransactions(IncludingTnxtoCreate_Contract): Total number of transactions\n* TotalEtherSent:Total Ether sent for account address\n* TotalEtherReceived: Total Ether received for account address\n* TotalEtherSent_Contracts: Total Ether sent to Contract addresses\n* TotalEtherBalance: Total Ether Balance following enacted transactions\n* TotalERC20Tnxs: Total number of ERC20 token transfer transactions\n* ERC20TotalEther_Received: Total ERC20 token received transactions in Ether\n* ERC20TotalEther_Sent: Total ERC20token sent transactions in Ether\n* ERC20TotalEtherSentContract: Total ERC20 token transfer to other contracts in Ether\n* ERC20UniqSent_Addr: Number of ERC20 token transactions sent to Unique account addresses\n* ERC20UniqRec_Addr: Number of ERC20 token transactions received from Unique addresses\n* ERC20UniqRecContractAddr: Number of ERC20token transactions received from Unique contract addresses\n* ERC20AvgTimeBetweenSent_Tnx: Average time between ERC20 token sent transactions in minutes\n* ERC20AvgTimeBetweenRec_Tnx: Average time between ERC20 token received transactions in minutes\n* ERC20AvgTimeBetweenContract_Tnx: Average time ERC20 token between sent token transactions\n* ERC20MinVal_Rec: Minimum value in Ether received from ERC20 token transactions for account\n* ERC20MaxVal_Rec: Maximum value in Ether received from ERC20 token transactions for account\n* ERC20AvgVal_Rec: Average value in Ether received from ERC20 token transactions for account\n* ERC20MinVal_Sent: Minimum value in Ether sent from ERC20 token transactions for account\n* ERC20MaxVal_Sent: Maximum value in Ether sent from ERC20 token transactions for account\n* ERC20AvgVal_Sent: Average value in Ether sent from ERC20 token transactions for account\n* ERC20UniqSentTokenName: Number of Unique ERC20 tokens transferred\n* ERC20UniqRecTokenName: Number of Unique ERC20 tokens received\n* ERC20MostSentTokenType: Most sent token for account via ERC20 transaction\n* ERC20MostRecTokenType: Most received token for account via ERC20 transactions","a50cf671":"### XGBClassifier hyperparameter tuning","71118c95":"### Numerical columns","b4144d5f":"### Categorical columns","9b7ff23b":"Columns 'ERC20 most sent token type' and ' ERC20_most_rec_token_type' contain token types. Most of tokens occur only once so they seem irrelevant in fraud detection.","50783db6":"Dataset is unbalanced, we must remember about this when choosing our model metric.","4a5c9bba":"## Data preprocessing","9253b648":"## Model selection","15ff7103":"## Best model evaluation","4df49bf5":"We left only numeratic features so preprocessing is limited to imputing null values with column mean and scaling."}}