{"cell_type":{"7d903dcd":"code","96496913":"code","ea4c260a":"code","25c80108":"code","a236f32b":"code","efaada25":"code","f75c1477":"code","802ba7f3":"code","6c1005d4":"code","0bd6a324":"code","bdc726b0":"code","700957f3":"code","b81aefdf":"code","b80177fd":"code","47aee678":"code","4ed5fc0f":"code","92922529":"code","d9870e3e":"code","2a77988a":"code","346b895d":"code","5c782dec":"code","ec744bd4":"code","fa68873c":"code","72e79b5e":"code","dab605fe":"code","2a7d3f0f":"code","8fdf1845":"code","caa9c8dc":"code","d5aae0ab":"code","f65d91da":"code","bdf414eb":"markdown","a3cb03ab":"markdown","5673da79":"markdown","4d8cc238":"markdown","b7a48c51":"markdown","1d4493e6":"markdown","c471ac5b":"markdown","91e09cc6":"markdown","134f7395":"markdown","22950599":"markdown","b1e24447":"markdown","df1f194e":"markdown","3ad1852b":"markdown","9b2a09fe":"markdown","16210362":"markdown","7ee69dd7":"markdown","228b2870":"markdown","7239c848":"markdown","5b566c48":"markdown","262beac8":"markdown","5f59604f":"markdown","8f863d12":"markdown","1666bdf9":"markdown","8691d591":"markdown","086251ad":"markdown","14dd4e50":"markdown","1df1dd55":"markdown","d9205b7b":"markdown","b235205a":"markdown"},"source":{"7d903dcd":"import pandas as pd\nimport numpy as np\nimport os\nimport warnings; warnings.simplefilter('ignore')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy import stats\nfrom sklearn.externals import joblib","96496913":"RAW_DATA_FILE = 'melb_data.csv'\n# check if the notebook is running on Kaggle or locally\nif os.getcwd() == '\/kaggle\/working':\n    RAW_DATA_PATH = '..\/input'\nelse:\n    RAW_DATA_PATH = os.path.join('..\/data','raw')","ea4c260a":"def load_data(raw_data_path, raw_data_file):\n    cols_to_use = ['Rooms','Price','Method','Date','Distance','Propertycount','Bedroom2','Bathroom','Car','Landsize','BuildingArea','YearBuilt','Lattitude','Longtitude']\n    df = pd.read_csv(os.path.join(raw_data_path, raw_data_file), usecols =[i for i in cols_to_use])\n    df['Date'] = pd.to_datetime(df['Date'])\n    return df\n\ndf = load_data(RAW_DATA_PATH, RAW_DATA_FILE)","25c80108":"train_set, test_set = train_test_split(df, test_size=0.2, random_state=38, shuffle=True)\nprint(train_set.shape)\nprint(test_set.shape)","a236f32b":"def separate_target_input(df):\n    housing = train_set.drop('Price', axis=1)\n    housing_labels = train_set['Price'].copy()\n    return housing, housing_labels\n\nX_train, y_train = separate_target_input(train_set)\nX_test, y_test = separate_target_input(test_set)","efaada25":"def feat_extract(X):\n    X['other_rooms'] = X.Rooms - X.Bathroom\n    X['surface_per_room'] = X.BuildingArea\/X.Rooms\n    X['perc_built'] = X.BuildingArea\/X.Landsize\n    X['house_age'] = X.Date.dt.year - X.YearBuilt\n    return X","f75c1477":"num_pipeline = Pipeline(steps=[\n    ('imputer', Imputer(strategy='median')),\n    ('scaler', StandardScaler())\n])","802ba7f3":"cat_pipeline = Pipeline(steps=[\n    ('ohe', OneHotEncoder(handle_unknown='ignore', categories='auto', sparse=False))\n])","6c1005d4":"cat_vars = X_train.select_dtypes(include=[object]).columns.tolist()\nnum_vars = X_train.select_dtypes(include=[np.number]).columns.tolist()\n\nfull_pipeline = Pipeline(steps=[\n    ('feat_extract', FunctionTransformer(feat_extract, validate=False)),\n    ('union', ColumnTransformer(transformers=[\n        ('num_pipeline', num_pipeline, num_vars),\n        ('cat_pipeline', cat_pipeline, cat_vars)],\n        remainder='drop')\n    )\n])\nX_train_prep = full_pipeline.fit_transform(X_train)","0bd6a324":"def display_scores(scores):\n    print('Scores:', scores)\n    print('Mean:', np.mean(scores))\n    print('Standard Deviation:', np.std(scores))","bdc726b0":"linreg = LinearRegression()\nscores = cross_val_score(linreg, X_train_prep, y_train, cv=10, scoring='neg_mean_squared_error')\nlinreg_scores = np.sqrt(-scores)\ndisplay_scores(linreg_scores)","700957f3":"dectree = DecisionTreeRegressor()\nscores = cross_val_score(dectree, X_train_prep, y_train, cv=10, scoring='neg_mean_squared_error')\ndectree_scores = np.sqrt(-scores)\ndisplay_scores(dectree_scores)","b81aefdf":"forest = RandomForestRegressor(n_estimators=30)\nscores = cross_val_score(forest, X_train_prep, y_train, cv=10, scoring='neg_mean_squared_error')\nforest_scores = np.sqrt(-scores)\ndisplay_scores(forest_scores)","b80177fd":"svr_rbf = SVR(kernel='rbf', gamma='auto')\nscores = cross_val_score(svr_rbf, X_train_prep, y_train, cv=10, scoring='neg_mean_squared_error')\nsvr_rbf_scores = np.sqrt(-scores)\ndisplay_scores(svr_rbf_scores)","47aee678":"ada = AdaBoostRegressor()\nscores = cross_val_score(ada, X_train_prep, y_train, cv=10, scoring='neg_mean_squared_error')\nada_scores = np.sqrt(-scores)\ndisplay_scores(ada_scores)","4ed5fc0f":"param_grid = {\n    'n_estimators': [200],\n    'max_features': [0.5,1.0],\n    'bootstrap': [True, False]\n}\n\nrf = RandomForestRegressor()\ngs = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n\ngs.fit(X_train_prep, y_train)","92922529":"gs.best_params_","d9870e3e":"np.sqrt(-gs.best_score_)","2a77988a":"for score, params in zip(gs.cv_results_['mean_test_score'],gs.cv_results_['params']):\n    print(np.sqrt(-score), params)","346b895d":"extra_vars = ['other_rooms', 'surface_per_room', 'perc_built', 'house_age']\ncat_ohe_vars = list(OneHotEncoder().fit(X_train[cat_vars]).categories_[0])\nprep_vars = extra_vars + num_vars + cat_ohe_vars\nfeature_importances = gs.best_estimator_.feature_importances_\nsorted(zip(feature_importances, prep_vars), reverse=True)","5c782dec":"param_dist = {\n    'n_estimators': stats.randint(low=10, high=200),\n    'max_features': stats.randint(low=2, high=X_train_prep.shape[1]),\n    'bootstrap': [True, False]\n}\n\nrs = RandomizedSearchCV(estimator=RandomForestRegressor(), param_distributions=param_dist, cv=5, n_jobs=-1, scoring='neg_mean_squared_error', n_iter=50)\nrs.fit(X_train_prep, y_train)","ec744bd4":"rs.best_params_","fa68873c":"np.sqrt(-rs.best_score_)","72e79b5e":"for score, params in zip(rs.cv_results_['mean_test_score'],rs.cv_results_['params']):\n    print(np.sqrt(-score), params)","dab605fe":"full_pipeline_with_model = Pipeline([\n    ('prep', full_pipeline),\n    ('model', RandomForestRegressor(**rs.best_params_))\n])\n\nprep_param_grid = {\n    'prep__union__num_pipeline__imputer__strategy': ['most_frequent', 'median', 'mean']\n}\n\ngs2 = GridSearchCV(full_pipeline_with_model, param_grid=prep_param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\ngs2.fit(X_train, y_train)","2a7d3f0f":"gs2.best_params_","8fdf1845":"preds_test = gs2.best_estimator_.predict(X_test)\nmse_test = mean_squared_error(y_test, preds_test)\nprint('RMSE on test set', np.sqrt(mse_test))","caa9c8dc":"squared_errors = (y_test - preds_test)**2\nm = len(squared_errors)\nconfidence = 0.95\nci = np.sqrt(stats.t.interval(confidence, m - 1, loc=np.mean(squared_errors), scale=stats.sem(squared_errors)))\nci","d5aae0ab":"final_pipeline = Pipeline([\n    ('prep', full_pipeline),\n    ('model', RandomForestRegressor(**rs.best_params_))\n])\n\nfinal_pipeline.fit(X_train, y_train)\n\njoblib.dump(final_pipeline, 'final_pipeline.pkl')","f65d91da":"final_pipeline_loaded = joblib.load('final_pipeline.pkl')\nfinal_pipeline_loaded.predict(X_test)","bdf414eb":"### Importing packages","a3cb03ab":"#### Support Vector Machine (kernel=radial basis function)","5673da79":"#### RandomizedSearchCV","4d8cc238":"In our data, we have both numerical and categorical variables. The preprocessing is not always the same for both data types. Therefore, we will split up the preprocessing for numerical and categorical variables in separate Pipelines. We will not further use the Date variable. This variable will be dropped by setting remainder to 'drop' in the ColumnTransformer.","b7a48c51":"### Load the data","1d4493e6":"### Automating data preprocessing\nAs we will be carrying out the same data preprocessing on the train and test set, it is worthwhile to create a pipeline. For some steps, we will create custom transformers which can be added to the pipeline. With FunctionTransformer we can easily transform a function so it can be used in a pipeline.","c471ac5b":"Compute a 95% confidence interval of the RMSE on the test set. stats.sem is the standard error of the mean.","91e09cc6":"#### GridSearch for the best hyperparameter for preprocessing\nWe can also search for the best hyperparameters for the data preprocessing steps. Let's see if the imputation of the numerical variables can be improved.","134f7395":"RandomizedSearchCV gives better results than GridSearchCV. This will be our final model.","22950599":"# Melbourne Housing Market - Data Preparation\nhttps:\/\/www.kaggle.com\/dansbecker\/melbourne-housing-snapshot\n\nThis notebook is the second step in building a model to predict the housing prices in Melbourne. In the [first notebook](https:\/\/www.kaggle.com\/bertcarremans\/data-exploration-melbourne-housing-market), we explored the data from the Melbourne Housing Market dataset. \n\nAs in the notebook for the data exploration, some code is reused from the book [Hands-On Machine Learning with Scikit-Learn and TensorFlow by Aur\u00e9lien G\u00e9ron (O'Reilly). Copyright 2017 Aur\u00e9lien G\u00e9ron, 978-1-491-96229-9](http:\/\/shop.oreilly.com\/product\/0636920052289.do)","b1e24447":"#### Feature extraction\nIn the notebook of Data Exploration, we tried some combinations of the existing variables. Let's create them again.","df1f194e":"### Separate the input variables and target variable","3ad1852b":"#### Random Forest Regressor\nThe averaged RMSE continues to improve.","9b2a09fe":"Imputation with the median gives the best results, so we can keep the full_pipeline as is.","16210362":"#### Adaboost Regressor","7ee69dd7":"### Model selection\nWe are now ready to train models on the prepared input data. The first thing we'll do is try out different algorithms. To evaluate their performance we will apply cross-validation. This gives a more reliable estimation of the model performance than looking at the performance on the whole training set. \n\nCross-validation will split the training set up in a number of folds. Each fold is used only once as a validation set. That way we evaluate the model on data it hasn't seen during training. We will not yet evaluate on the test set as we want to use that data only when we are sure about the model. The cross_val_score method expects a utility function for scoring (meaning, the higher the better). Therefore, we will use the negated mean squared error.\n\nBased on the performance of these models we can select an algorithm which we'll finetune later with hyperparameter tuning.","228b2870":"#### Decision Tree Regressor\nThe averaged RMSE is better than that of the linear regression, but still very high. ","7239c848":"We can have a look at the importance of each feature in the final model.","5b566c48":"### Hyperparameter tuning\nFrom the different algorithms above, we select the random forest regressor for further finetuning. We start with searching for the best hyperparameters for the estimator.","262beac8":"#### One-hot encoding of categorical variables\nCategorical variables cannot be used in a machine learning algorithm. They need to be converted to numbers. We will do so by creating a binary variable for each categorical value.","5f59604f":"### Prepare for production\nTo use the pipeline in production, we will combine the preprocessing and model into one pipeline. This pipeline is then save into a pickle file for later use.","8f863d12":"#### Numerical variables\n\n**Missing values**: In the report created by pandas_profiling in the Data Exploration notebook, we noticed that some variables have missing values. Not all machine learning algorithms can work with missing values. We will impute the missing values with the median. \n\n**Scaling**: not all variables are on the same scale. Therefore, we apply scaling. Because there some of the numerical variables have outliers, we apply StandardScaling instead of min-max scaling. The latter is more sensitive to outliers.","1666bdf9":"### Create a test set","8691d591":"#### GridSearchCV","086251ad":"#### Linear Regression\nThe averaged RMSE is very high for the linear regression. This model underfits the data. Underfitting can be solved by adding more meaningfull features, relaxing regularization or trying more complex models. Let's try the latter with a decision tree and random forest regressor.","14dd4e50":"#### Combining all preprocessing steps in one pipeline","1df1dd55":"Test to see if the final pipeline can be used.","d9205b7b":"### Evaluating on the test set","b235205a":"With these parameters we can further improve the RMSE a bit, compared to the model with the default parameters."}}