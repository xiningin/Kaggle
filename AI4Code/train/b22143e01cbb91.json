{"cell_type":{"6b338267":"code","6e9bbe97":"code","391b8c63":"code","2574f927":"code","ff93f5df":"code","3f2757ab":"code","723201fd":"code","a2beeff7":"code","6b8a81dc":"code","b3c394db":"code","c79aa841":"code","04dede7f":"code","40b55c1a":"code","69d146e2":"code","b5085e8b":"code","0b111146":"code","59c8807e":"code","5230fe2b":"code","7bf70ec6":"code","25999a3b":"code","504be7cc":"code","2fe960bd":"code","4b82d19f":"code","e8de5917":"code","b433acf6":"code","b0fe2a24":"code","6d960a94":"code","4ececf5a":"code","3dbb7d22":"code","19d02490":"code","f831af67":"code","7b1faae7":"code","9de75ae0":"code","f46a2eb6":"code","9e6b7a6d":"code","6bca9ad9":"code","b3e2b8b0":"code","b2fff5c7":"code","82ea5ca8":"code","45d8aa34":"code","218b0202":"code","ac86dc0b":"code","89166551":"code","de625768":"code","5a0a9f77":"code","a2a01970":"code","a41bdbed":"code","4b3c503d":"code","ecc8676b":"code","7feccd41":"code","37633d50":"code","98701f1c":"code","fed36b3d":"code","288ef40f":"code","4f2b98f0":"code","b78f5442":"code","931f7257":"code","678d4ce9":"code","71463d59":"code","19dd0738":"code","95b76c9c":"code","57b308b8":"code","f9928bb0":"code","6f581692":"code","bca4a535":"code","323fdd67":"code","4f9ca6e1":"code","377cb376":"code","8b18dfcf":"code","309d09cd":"code","6a33e4fc":"code","164164e0":"code","901e1057":"code","3f35efe1":"code","3b29ba77":"code","69acc085":"code","60c56dc8":"code","db22458d":"code","ed2029d5":"code","7a66ec54":"code","6e7dafa1":"code","17f71bc7":"code","69653d8d":"code","9cd141b8":"code","08dfc419":"code","a2a233ed":"markdown","7a5e07b1":"markdown","957099c2":"markdown","1f3aefd2":"markdown","dde82784":"markdown","4bba79b2":"markdown","ad55b8f4":"markdown","2d8fd039":"markdown","8a3e8993":"markdown","e2b215ac":"markdown","dc2a8d5f":"markdown","0007c71d":"markdown","a7ab54d2":"markdown","c49beeec":"markdown","b57251cf":"markdown","e45a7017":"markdown","53a90884":"markdown","cb7069f2":"markdown","e9433fd4":"markdown","cc31080b":"markdown","5134f0d2":"markdown","0a698503":"markdown","778eee2d":"markdown","e0980f7b":"markdown","8ed5ef42":"markdown","445c73f3":"markdown","a0d0a59e":"markdown","ff1fc200":"markdown","8e6071c9":"markdown","e73333df":"markdown","9d885b48":"markdown","9bd74ff7":"markdown","b079f631":"markdown","6e700a68":"markdown","ff1bcc7f":"markdown","4ae28d23":"markdown","cce2e030":"markdown","c76bb443":"markdown","18c4a4a9":"markdown","4d87a05a":"markdown","260dbf50":"markdown","7f781cc0":"markdown","6edf2ac8":"markdown","8384b9b2":"markdown","f1d38b19":"markdown","6d05e9bd":"markdown","b715f51e":"markdown","97b47480":"markdown","025155a8":"markdown","f13f6581":"markdown","9950a58c":"markdown","7b6a77cd":"markdown","b3c0760f":"markdown","dec8a48a":"markdown","9fddb800":"markdown","dcf2d4f4":"markdown","32c4e00b":"markdown","42b2533b":"markdown","6b40713c":"markdown","91f7ec94":"markdown","f6c3bde6":"markdown","1d4abf90":"markdown","dad75639":"markdown","4ad6e2b7":"markdown","9224ab31":"markdown","2bc272db":"markdown","ea701a78":"markdown","f3381332":"markdown","ff54251c":"markdown","a580282c":"markdown","462a53e3":"markdown","4e1dc283":"markdown","5590df74":"markdown","4849223b":"markdown","78058fd7":"markdown","64c7d3e5":"markdown","488988d4":"markdown","77578c9c":"markdown","f040934d":"markdown"},"source":{"6b338267":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport networkx as nx\n\n\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom lime import lime_tabular\nimport lime\n\n# classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n# regressors\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n%matplotlib inline\nsns.set()","6e9bbe97":"column_names = ['Id', 'age', 'workclass', 'final_weight', 'education', 'education_num', 'marital_status',\n                'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week',\n                'native_country', 'income']","391b8c63":"data = pd.read_csv('..\/input\/adult-pmr3508\/train_data.csv', names = column_names, na_values='?').drop(0, axis = 0).reset_index(drop = True)","2574f927":"data.head()","ff93f5df":"data.describe()","3f2757ab":"def count_null_values(data):\n    '''\n    Return a DataFrame with count of null values\n    '''\n    \n    counts_null = []\n    for column in data.columns:\n        counts_null.append(data[column].isnull().sum())\n    counts_null = np.asarray(counts_null)\n\n    counts_null = pd.DataFrame({'feature': data.columns, 'count.': counts_null,\n                                'freq. [%]': 100*counts_null\/data.shape[0]}).set_index('feature', drop = True)\n    counts_null = counts_null.sort_values(by = 'count.', ascending = False)\n    \n    return counts_null","723201fd":"count_null_values(data).head()","a2beeff7":"def work_missing_values(data):\n    '''\n    Return new data with no missing values for this problem\n    '''\n    \n    aux = data.copy()\n    # select index of rows that workclass is nan\n    aux_index = aux[aux['workclass'].isna()].index\n    \n    # fill nan with 'unknown'\n    aux['workclass'].loc[aux_index] = 'unknown'\n    aux['occupation'].loc[aux_index] = 'unknown'\n    \n    # complete missing of native_country and occupation with most frequent\n    cols = ['native_country', 'occupation']\n    for col in cols:\n        top = aux[col].value_counts().index[0]\n        aux[col] = aux[col].fillna(top)\n    aux.reset_index(drop = True)\n    \n    return aux","6b8a81dc":"data = work_missing_values(data)","b3c394db":"count_null_values(data).head()","c79aa841":"numeric_columns = ['age', 'final_weight', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\ncategoric_columns = ['workclass', 'education', 'marital_status', 'occupation',\n                     'relationship', 'race', 'sex', 'native_country', 'income']\n\n# change to number variable\nfor column in numeric_columns:\n    data[column] = pd.to_numeric(data[column])","04dede7f":"sns.set()\nsns.pairplot(data, vars = numeric_columns, hue = 'income', palette = 'Wistia')","40b55c1a":"def bar_plot(data, by, hue, normalize_by_index = True):\n    '''\n    Plot count bar for each unique in data[by], using as reference 'hue'\n    \n    obs: if normalize_by_index is True, than for each index, the values will be normalized\n    '''\n    \n    index = data[by].unique()\n    columns = data[hue].unique()\n    \n    data_to_plot = pd.DataFrame({'index': index})\n    \n    for column in columns:\n        temp = []\n        for unique in index:\n            filtered_data = data[data[by] == unique]\n            filtered_data = filtered_data[filtered_data[hue] == column]\n            \n            temp.append(filtered_data.shape[0])\n        data_to_plot = pd.concat([data_to_plot, pd.DataFrame({column: temp})], axis = 1)\n        \n    data_to_plot = data_to_plot.set_index('index', drop = True)\n    \n    if normalize_by_index:\n        print('O gr\u00e1fico est\u00e1 normalizado por index!')\n        for row in index:\n            data_to_plot.loc[row] = data_to_plot.loc[row].values\/data_to_plot.loc[row].values.sum()\n    \n    ax = data_to_plot.plot.bar(rot=0, figsize = (14,7), alpha = 0.9, cmap = 'Wistia')","69d146e2":"bar_plot(data, 'sex', 'income')","b5085e8b":"bar_plot(data, 'race', 'income')","0b111146":"bar_plot(data, 'race', 'occupation')","59c8807e":"bar_plot(data, 'marital_status', 'income')","5230fe2b":"bar_plot(data, 'workclass', 'income')","7bf70ec6":"def pie_plot(data, by, n = 5):\n    '''\n    Plot pie with count of recurrence of each unique of data[by]. \n    \n    n: is for how much uniques to be shown, remainer will be treat as 'Others'\n    '''\n    \n    data_counts = data[by].value_counts()\n    if len(data_counts) < n:\n        n = len(data_counts)\n    \n    all_counts = data_counts[:n]\n    \n    if n != len(data_counts):\n        other_counts = data_counts[n:]\n        \n        all_counts = pd.concat([all_counts, pd.Series({'Others': other_counts.sum()})])\n        del other_counts\n        \n    all_counts = all_counts.sort_values(ascending = False)\n    all_counts.plot(kind = 'pie', cmap = 'Wistia', figsize = (14,7))\n    del all_counts","25999a3b":"pie_plot(data, 'native_country', n = 3)","504be7cc":"pie_plot(data, 'occupation', n = 13)","2fe960bd":"pie_plot(data, 'education', n = 13)","4b82d19f":"pie_plot(data, 'workclass', n = 13)","e8de5917":"def box_plot(data, var_x, var_y, orientation = 'v', rotate_x_label = False):\n\n    df = pd.concat([data[var_y], data[var_x]], axis=1)\n    f, ax = plt.subplots(figsize=(15, 7))\n\n    sns.boxplot(x=var_x, y=var_y, data=df, notch = True, palette = 'Wistia', orient = orientation)\n    plt.title('Boxplot of education num over race')\n    if rotate_x_label:\n        ax.set_xticklabels(data[var_x].unique(), rotation=90)\n    ","b433acf6":"box_plot(data, 'income', 'education_num')","b0fe2a24":"box_plot(data, 'income', 'age')","6d960a94":"box_plot(data, 'income', 'hours_per_week')","4ececf5a":"box_plot(data, 'marital_status', 'hours_per_week', rotate_x_label=True)","3dbb7d22":"box_plot(data, 'marital_status', 'age', rotate_x_label=True)","19d02490":"def findEdges(data, col1, col2):\n    '''\n    return list of edges that connects col1 and col2\n    '''\n    \n    edges = []\n    for unique1 in data[col1].unique():\n        temp1 = data[data[col1] == unique1]\n        for unique2 in data[col2].unique():\n            temp2 = temp1[temp1[col2] == unique2]\n            size = len(temp2)\n            if size == 0:\n                continue\n            edges.append((unique1, unique2, size))\n    \n    del temp1, temp2\n    return edges","f831af67":"def setNodePosition(nodes, r = 7, center = 14):\n    '''\n    return dictionary of nodes with respected position in a circle\n    '''\n    \n    n = len(nodes)\n    positions = {}\n    for k, node in enumerate(nodes):\n        positions[node] = (r*np.cos(k*(2*np.pi)\/n) + center, r*np.sin(k*(2*np.pi)\/n) + center)\n    return positions","7b1faae7":"def drawPairNetwork(data, var1, var2):\n\n    G = nx.Graph()\n\n    G.add_nodes_from(data[var1].unique())\n    positions = setNodePosition(data[var1].unique(), r = 7)\n\n    G.add_nodes_from(data[var2].unique())\n    positions.update(setNodePosition(data[var2].unique(), r = 14))\n\n    edges = findEdges(data, var1, var2)\n    for edge in edges:\n        G.add_edge(edge[0], edge[1], weight = edge[2])\n    del edges\n\n    with plt.style.context('seaborn-whitegrid'):\n        colors = range(20)\n        \n        plt.figure(figsize = (14,14))\n        nx.draw_networkx_nodes(G, pos = positions, nodelist = data[var1].unique(), node_color = 'yellow')\n        nx.draw_networkx_nodes(G, pos = positions, nodelist = data[var2].unique(), node_color = 'coral')\n        \n        edges, weights = zip(*nx.get_edge_attributes(G, 'weight').items())\n        nx.draw_networkx_edges(G, pos = positions, width = 4.0, edgelist = edges, edge_color = weights, edge_cmap = plt.cm.Oranges)\n        nx.draw_networkx_labels(G, pos = positions)\n        plt.grid(alpha = 0)\n    \n    return G","9de75ae0":"G = drawPairNetwork(data, 'race', 'native_country')","f46a2eb6":"G = drawPairNetwork(data, 'sex', 'occupation')","9e6b7a6d":"G = drawPairNetwork(data, 'income', 'race')","6bca9ad9":"G = drawPairNetwork(data, 'income', 'occupation')","b3e2b8b0":"G = drawPairNetwork(data, 'income', 'marital_status')","b2fff5c7":"G = drawPairNetwork(data, 'income', 'education')","82ea5ca8":"def usa_column_prepare(value):\n    '''\n    Return 1 if value is United-States, 0 otherwise\n    '''\n    \n    if value == 'United-States':\n        return 1\n    return 0\n\ndef education_column_prepare(value):\n    '''\n    Return an integer that correspond to education order:\n    \n    Preschool < 1st-4th < 5th-6th < 7th-8th < 9th < 10th < 11th \n    < 12th < HS-grad < Prof-school < Assoc-acdm < Assoc-voc \n    < Some-college < Bachelors < Masters < Doctorate\n    '''\n    \n    if value == 'Preschool':\n        return 0\n    if value == '1st-4th':\n        return 1\n    if value == '5th-6th':\n        return 2\n    if value == '7th-8th':\n        return 3\n    if value == '9th':\n        return 4\n    if value == '10th':\n        return 5\n    if value == '11th':\n        return 6\n    if value == '12th':\n        return 7\n    if value == 'HS-grad':\n        return 8\n    if value == 'Prof-school':\n        return 9\n    if value == 'Assoc-acdm':\n        return 10\n    if value == 'Assoc-voc':\n        return 11\n    if value == 'Some-college':\n        return 12\n    if value == 'Bachelors':\n        return 13\n    if value == 'Masters':\n        return 14\n    return 15\n\ndef married_present_column_prepare(value):\n    '''\n    Returns 1 if marriege has both partners present to each other.\n    '''\n    if value == 'Married-civ-spouse' or value == 'Married-AF-spouse':\n        return 1\n    return 0","45d8aa34":"def pipe_of_data_prepare(data, drop_columns = None, kind = 'train'):\n    '''\n    Return numeric prepared data to train models\n    '''\n    \n    columns = data.columns\n    \n    # remove missing values\n    new_data = work_missing_values(data)\n    \n    # remove Id column\n    new_data = new_data.drop('Id', axis = 1)\n    \n    # add US or not column\n    if 'native_country' in columns:\n        new_data['usa'] = new_data['native_country'].apply(usa_column_prepare)\n        \n    # education ordered column:\n    if 'education' in columns:\n        new_data['education'] = new_data['education'].apply(education_column_prepare)\n        \n    # select important correspondence in marital_status\n    if 'marital_status' in columns:\n        new_data['married_present'] = new_data['marital_status'].apply(married_present_column_prepare)\n        \n    # label encoder for categorical not ordered features    \n    categorical_features = ['workclass', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n    encoder = {}\n    for feature in categorical_features:\n        if feature in columns:\n            encoder[feature] = LabelEncoder()\n            new_data[feature] = encoder[feature].fit_transform(new_data[feature])\n            \n    if drop_columns is not None:\n        new_data = new_data.drop(drop_columns, axis = 1)\n    \n    if kind == 'train':\n        X, y = new_data.drop('income', axis = 1), new_data['income']\n        y = y.values.reshape(-1,1)\n    else:\n        X = new_data.copy()\n    used_columns = X.columns\n    \n    X = X.values\n    \n    del new_data\n            \n    # normalize features with StandardScaler  \n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    \n    # train data has y\n    if kind == 'train':\n        return X, y, used_columns, scaler, encoder\n    \n    # test data\n    return X, used_columns, scaler, encoder","218b0202":"data = {}\ndata['train'] = pd.read_csv('..\/input\/adult-pmr3508\/train_data.csv', names = column_names, na_values='?').drop(0, axis = 0).reset_index(drop = True)\ndata['test'] = pd.read_csv('..\/input\/adult-pmr3508\/test_data.csv', names = column_names[:len(column_names)-1], na_values='?').drop(0, axis = 0).reset_index(drop = True)\n\ndrop_columns = ['final_weight', 'native_country', 'marital_status', 'education_num']\nX_train, y, used_columns_train, scaler_train, encoder_train = pipe_of_data_prepare(data['train'], drop_columns = drop_columns, kind = 'train')\nX_test, used_columns_test, scaler_test, encoder_test = pipe_of_data_prepare(data['test'], drop_columns = drop_columns, kind = 'test')\n\nX_train_unscaled = scaler_train.inverse_transform(X_train)\nX_test_unscaled = scaler_test.inverse_transform(X_test)\n\nused_columns_train, used_columns_test = list(used_columns_train), list(used_columns_test)","ac86dc0b":"def explanation_fn(estimator, instance):\n    '''\n    fixed function for lime explanation for estimator and given example instance\n    '''\n    explainer = lime.lime_tabular.LimeTabularExplainer(X_train, training_labels=y, \n                                                   feature_names=used_columns_train, categorical_features = [1,3,4,5,6,12], \n                                                   class_names = ['<=50K', '>50K'])\n\n    exp = explainer.explain_instance(X_test[instance], estimator.predict_proba, num_features=6, top_labels=None)\n    exp.show_in_notebook(show_table=True, show_all=False)","89166551":"def outputPrediction(ids, predictions):\n    data = pd.DataFrame({'Id': ids, 'income': predictions})\n    return data","de625768":"%%time\ntime_train = [2000]\n\n# train\n\nLogClf = LogisticRegression(solver = 'lbfgs', C = 1.0, penalty = 'l2', warm_start =  True)\n\nLogCV = cross_val_score(LogClf, X_train, y.reshape(-1), cv = 10)\n\nLogClf.fit(X_train, y.reshape(-1))\n\ncv_accuracy = [LogCV.mean()]\ncv_std = [LogCV.std()]\n\ncv_values = {}\ncv_values['Lin'] = LogCV\nprint('Logistic Regression CV accuracy: {0:1.4f} +-{1:2.5f}\\n'.format(LogCV.mean(), LogCV.std()))","5a0a9f77":"%%time\ntime_test = [5]\n\n# test\n\npredictions = {}\npredictions['Log'] = LogClf.predict(X_test)\npredictions['Log'] = outputPrediction(data['test']['Id'].values, predictions['Log'])","a2a01970":"explanation_fn(LogClf, 6)","a41bdbed":"explanation_fn(LogClf, 21)","4b3c503d":"%%time\ntime_train.append(30500)\n\n# train\n\nKNNClf = KNeighborsClassifier(n_neighbors = 19, p = 1, weights = 'uniform')\n\nKNNCV = cross_val_score(KNNClf, X_train, y.reshape(-1), cv = 10)\n\nKNNClf.fit(X_train, y.reshape(-1))\n\ncv_accuracy.append(KNNCV.mean())\ncv_std.append(KNNCV.std())\ncv_values['KNN'] = KNNCV\nprint('K-Nearest Neighboors CV accuracy: {0:1.4f} +-{1:2.5f}\\n'.format(KNNCV.mean(), KNNCV.std()))","ecc8676b":"%%time\ntime_test.append(9350)\n\n# test\n\npredictions['KNN'] = KNNClf.predict(X_test)\npredictions['KNN'] = outputPrediction(data['test']['Id'].values, predictions['KNN'])","7feccd41":"explanation_fn(KNNClf, 6)","37633d50":"explanation_fn(KNNClf, 21)","98701f1c":"%%time\ntime_train.append(170000)\n\n# train\n\nRFClf = RandomForestClassifier(n_estimators = 700, max_depth = 12)\n\nRFCV = cross_val_score(RFClf, X_train, y.reshape(-1), cv = 10)\n\nRFClf.fit(X_train, y.reshape(-1))\n\ncv_accuracy.append(RFCV.mean())\ncv_std.append(RFCV.std())\ncv_values['RF'] = RFCV\nprint('Random Forest CV accuracy: {0:1.4f} +-{1:2.5f}\\n'.format(RFCV.mean(), RFCV.std()))","fed36b3d":"%%time\ntime_test.append(1570)\n\n# test\n\npredictions['RF'] = RFClf.predict(X_test)\npredictions['RF'] = outputPrediction(data['test']['Id'].values, predictions['RF'])","288ef40f":"explanation_fn(RFClf, 6)","4f2b98f0":"explanation_fn(RFClf, 21)","b78f5442":"%%time\ntime_train.append(60000)\n\n# train\n\nXGBClf = XGBClassifier(max_depth = 4, n_estimators = 250)\n\nXGBCV = cross_val_score(XGBClf, X_train, y.reshape(-1), cv = 10)\n\nXGBClf.fit(X_train, y.reshape(-1))\n\ncv_accuracy.append(XGBCV.mean())\ncv_std.append(XGBCV.std())\ncv_values['XGB'] = XGBCV\nprint('XGBoost CV accuracy: {0:1.4f} +-{1:2.5f}\\n'.format(XGBCV.mean(), XGBCV.std()))","931f7257":"%%time\ntime_test.append(155)\n\n# test\n\npredictions['XGB'] = XGBClf.predict(X_test)\npredictions['XGB'] = outputPrediction(data['test']['Id'].values, predictions['XGB'])","678d4ce9":"explanation_fn(XGBClf, 6)","71463d59":"explanation_fn(XGBClf, 21)","19dd0738":"results = pd.DataFrame()\n\nresults['Estimator'] = ['Logistic Regression', 'K-Nearest Neighboors', 'Random Forest', 'XGBoost']\nresults['CV accuracy'] = cv_accuracy\nresults['CV std'] = cv_std\nresults['~Time (Train) [ms]'] = time_train\nresults['~Time (Test) [ms]'] = time_test\n\nresults = results.set_index('Estimator', drop = True)\n\n# setting CV values for visualization\n\ncv_values = pd.DataFrame(cv_values)\ncv_values = np.concatenate([cv_values.columns.values.reshape(-1,1), cv_values.values.transpose()], axis = 1)\ntemp = {}\nfor u in range(cv_values.shape[0]):\n    temp[u] = cv_values[u, :]\ncv_values = pd.DataFrame(temp)\ncv_values = cv_values.rename(columns = {0:'Logistic Regression', 1:'K-Nearest Neighboors', 2:'Random Forest', 3:'XGBoost'})\ncv_values = cv_values.drop(0, axis = 0)","95b76c9c":"plt.figure(figsize=(14,7))\nplt.title('Cross Validation Accuracy', fontsize = 15)\nsns.boxplot(data = cv_values, palette = 'Wistia')","57b308b8":"plt.figure(figsize=(14,7))\nplt.title('~Time Evaluation [ms]', fontsize = 15)\nplt.plot(results.index, results['~Time (Train) [ms]'].values, color = 'gray', lw = 2,\n         marker = 'o', markersize = 12, markerfacecolor = 'orange', label = 'Train')\nplt.plot(results.index, results['~Time (Test) [ms]'].values, color = 'gray', lw = 2,\n         marker = 'o', markersize = 12, markerfacecolor = 'yellow', label = 'Test')\nplt.legend()","f9928bb0":"for classifier in predictions.keys():\n    predictions[classifier].to_csv(classifier.lower() + '_predict.csv', index = False)","6f581692":"data = pd.read_csv('..\/input\/atividade-3\/train.csv', na_values='?').reset_index(drop = True)\ndata.head()","bca4a535":"data.describe()","323fdd67":"def plotMap(data, sizes = None, colors = None, cmap = 'Wistia', alpha = 0.7, title = 'Mapa'):\n    '''\n    plot on cartesian plan, coordinatedes according to lat long, with circle sizes em color scale\n    '''\n    v_sizes, v_colors = None, None\n    if sizes is not None:\n        scaler = MinMaxScaler()\n        v_sizes = scaler.fit_transform(data[sizes].values.reshape(-1,1))*100\n        v_sizes = v_sizes.reshape(-1)\n        \n    if colors is not None:\n        v_colors = data[colors]\n        \n    with plt.style.context('seaborn-whitegrid'):\n        data.plot.scatter('longitude', 'latitude', s = v_sizes, figsize = (11,7), c = v_colors, cmap = cmap, alpha = alpha)\n        plt.title(title)","4f9ca6e1":"plotMap(data, sizes = 'median_income', colors = 'median_house_value', title = 'Income and house value visualization map')","377cb376":"plotMap(data, sizes = 'population', colors = 'median_age', title = 'Population and ages visualization map', alpha = 0.7)","8b18dfcf":"plt.figure(figsize=(14,7))\nsns.distplot(data['median_age'], color = 'Orange', bins = 20)","309d09cd":"plt.figure(figsize=(14,7))\nsns.distplot(data['median_income'], color = 'Orange', bins = 20)","6a33e4fc":"plt.figure(figsize=(14,7))\nsns.distplot(data['population'], color = 'Orange', bins = 30)","164164e0":"plt.figure(figsize=(14,7))\nsns.boxplot(x = data['median_age'], y = data['population'], palette = 'Wistia')","901e1057":"plt.figure(figsize=(14,7))\nsns.boxplot(x = data['median_age'], y = data['median_income'], palette = 'Wistia')","3f35efe1":"corrmat = data.drop('Id', axis = 1).corr()\nplt.figure(figsize=(12,9))\nsns.heatmap(corrmat, cmap = 'Wistia')","3b29ba77":"#Retirando outliers da base\ndata_clean = data[(np.abs(stats.zscore(data)) < 3).all(axis=1)]\n\n#Reindexando para ajustar termos faltantes\ndata_clean = data_clean.assign(index = list(range(0, data_clean.iloc[:,0].size)))\ndata_clean = data_clean.set_index('index')","69acc085":"selected_columns = ['longitude', 'median_income', 'median_age', 'population']\ntarget = 'median_house_value'\n\nselected_base = pd.concat([data_clean[selected_columns], data_clean[target]], axis = 1)","60c56dc8":"scaler = {}\nfor col in selected_columns:\n    scaler[col] = StandardScaler()\n    selected_base[col] = scaler[col].fit_transform(selected_base[col].values.reshape(-1,1))\n    \nscaler[target] = MinMaxScaler()\nselected_base[target] = scaler[target].fit_transform(selected_base[target].values.reshape(-1,1))","db22458d":"X, y = selected_base[selected_columns].values, selected_base[target].values","ed2029d5":"# metrica para avaliar os regressores\n\nfrom sklearn.metrics import mean_squared_log_error, make_scorer\n\nmsle = make_scorer(mean_squared_log_error)","7a66ec54":"%%time\ntime_train = [40]\n\n# train\n\nLinReg = LinearRegression()\n\nLinCV = cross_val_score(LinReg, X, y.reshape(-1), cv = 10, scoring = msle)\n\nLinReg.fit(X, y)\n\ncv_accuracy = [LinCV.mean()]\ncv_std = [LinCV.std()]\ncv_values = {}\ncv_values['Lin'] = LinCV\nprint('Linear Regression CV msle: {0:1.4f} +-{1:2.5f}\\n'.format(LinCV.mean(), LinCV.std()))","6e7dafa1":"%%time\ntime_train.append(310)\n\n# train\n\nKNNReg = KNeighborsRegressor(n_neighbors=30)\n\nKNNCV = cross_val_score(KNNReg, X, y, cv = 10, scoring = msle)\n\nKNNReg.fit(X, y)\n\ncv_accuracy.append(KNNCV.mean())\ncv_std.append(KNNCV.std())\ncv_values['KNN'] = KNNCV\nprint('KNN Regression CV msle: {0:1.4f} +-{1:2.5f}\\n'.format(KNNCV.mean(), KNNCV.std()))","17f71bc7":"%%time\ntime_train.append(18700)\n\n# train\n\nRFReg = RandomForestRegressor(n_estimators = 50, max_depth = 14)\n\nRFCV = cross_val_score(RFReg, X, y.reshape(-1), cv = 10, scoring = msle)\n\nRFReg.fit(X, y.reshape(-1))\n\ncv_accuracy.append(RFCV.mean())\ncv_std.append(RFCV.std())\ncv_values['RF'] = RFCV\nprint('RF Regression CV msle: {0:1.4f} +-{1:2.5f}\\n'.format(RFCV.mean(), RFCV.std()))","69653d8d":"results = pd.DataFrame()\n\nresults['Estimator'] = ['Linear Regression', 'K-Nearest Neighboors Regressor', 'Random Forest Regressor']\nresults['CV accuracy'] = cv_accuracy\nresults['CV std'] = cv_std\nresults['~Time (Train) [ms]'] = [35, 265, 1440]\n\nresults = results.set_index('Estimator', drop = True)\n\n# setting CV values for visualization\n\ncv_values = pd.DataFrame(cv_values)\ncv_values = np.concatenate([cv_values.columns.values.reshape(-1,1), cv_values.values.transpose()], axis = 1)\ntemp = {}\nfor u in range(cv_values.shape[0]):\n    temp[u] = cv_values[u, :]\ncv_values = pd.DataFrame(temp)\ncv_values = cv_values.rename(columns = {0:'Linear Regression', 1:'K-Nearest Neighboors Regressor', 2:'Random Forest Regressor'})\ncv_values = cv_values.drop(0, axis = 0)","9cd141b8":"plt.figure(figsize=(14,7))\nplt.title('Cross Validation MSLE', fontsize = 15)\nsns.boxplot(data = cv_values, palette = 'Wistia')","08dfc419":"plt.figure(figsize=(14,7))\nplt.title('~Time Evaluation [ms]', fontsize = 15)\nplt.plot(results.index, results['~Time (Train) [ms]'].values, color = 'gray', lw = 2,\n         marker = 'o', markersize = 12, markerfacecolor = 'orange', label = 'Train')\nplt.legend()","a2a233ed":"**Conclus\u00e3o**: Para os exemplos 6 e 21, o classificador teve muita certeza absoluta do 21, considerando basicamente o capital_gain. J\u00e1 para o exemplo 6, houve menos certeza da resposta, mas novamente o capital_gain foi o fator determinante para a classe determinada pelo modelo.","7a5e07b1":"**Conclus\u00e3o**: Dos tr\u00eas regressores, a **Floresta Aleat\u00f3ria** teve maior desempenho sobre o MSLE, mas o pior desempenho em tempo de treino.","957099c2":"**Conclus\u00e3o:** Vemos que as ocupa\u00e7\u00f5es variam de acordo com o sexo, por exemplo homens trabalham mais como 'craft-repair' e as mulheres, como 'adm-clerical'.","1f3aefd2":"## 6.3. Distplots","dde82784":"# 1. Primeiros passos\n## 1.1. Importando depend\u00eancias","4bba79b2":"**Conclus\u00e3o**: Para os exemplos 6 e 21, o classificador teve muita certeza do 21, considerando fatores mais relevantes como capital_gain e o estado de casamento, entre outros. J\u00e1 para o exemplo 6, houve menos certeza da resposta, mas novamente o capital_gain e educa\u00e7\u00e3o favoreceram a classifica\u00e7\u00e3o >50K, enquanto que a classe de trabalho e capital_loss (com pouca intensidade) para classe contr\u00e1ria.","ad55b8f4":"**Conclus\u00e3o:** Interessante notar que a distribui\u00e7\u00e3o de ocupa\u00e7\u00e3o por etnia varia. Por exemplo, Asian-Pac-Islander atuam muito em servi\u00e7os privados de casa; enquanto que a popula\u00e7\u00e3o negra, atua principalmente como Adm-clerical e outros servi\u00e7os.","2d8fd039":"# 7. Limpeza e prepara\u00e7\u00e3o dos dados\n## 7.1. Outliers","8a3e8993":"# 4. Aplica\u00e7\u00e3o de modelos","e2b215ac":"## 4.5. Resultados","dc2a8d5f":"**Conclus\u00e3o:** A grande parte dos pa\u00edses nativos das pessoas intrevistadas \u00e9 estadounidense.","0007c71d":"# 5. Discuss\u00f5es","a7ab54d2":"**Conclus\u00e3o**: Vemos uma grande correla\u00e7\u00e3o entre alguns fatores como (lat, long); entre os pares de [salas, quartos, popula\u00e7\u00e3o, casas]; al\u00e9m disso, forte correla\u00e7\u00e3o positiva entre a o sal\u00e1rio e o valor das casas.","c49beeec":"**Conclus\u00e3o:** Pessoas que recebem mais, em m\u00e9dia, trabalham mais horas.","b57251cf":"### 4.4.2. Testing","e45a7017":"**Conclus\u00e3o**: Para os exemplos 6 e 21, o classificador teve muita certeza do 6, considerando fatores mais relevantes como capital_gain e educa\u00e7\u00e3o. J\u00e1 para o exemplo 21, houve uma indecis\u00e3o de resposta, as probabilidades associadas \u00e0s classes eram muito pr\u00f3ximas.","53a90884":"### 2.1.5 Graph Visualization","cb7069f2":"## 8.3. Random Forest Regressor\n### 8.3.1. Training with CV","e9433fd4":"**Conclus\u00e3o:** Pessoas que recebem mais, em m\u00e9dia, tiveram mais tempo de estudo.","cc31080b":"# 2. Explora\u00e7\u00e3o\n## 2.1 Visualizando os dados","5134f0d2":"**Conclus\u00e3o:** Nessa visualiza\u00e7\u00e3o, notamos que a grande parte das pessoas tem ensino m\u00e9dio completo e recebem menos de 50k. A maior parte das pessoas que recebe acima de 50k tem alguma faculdade ou um bacharel ou ensino m\u00e9dio completo.","0a698503":"### 4.2.3. Lime explanation example","778eee2d":"### 2.1.4 Boxplot","e0980f7b":"### 4.1.3. Lime explanation example","8ed5ef42":"## 3.2. Pipeline de prepara\u00e7\u00e3o das bases X, y","445c73f3":"## 4.4. XGBoost\n### 4.4.1. Training with CV","a0d0a59e":"### 4.5.2. Tempos","ff1fc200":"**Conclus\u00e3o:** Podemos notar que alguns atributos correlacionados aparentemente separam as classes de interesse. Observe que 'age' e 'education_num', 'hours_per_week' e 'education_num', etc.","8e6071c9":"### 4.1.2. Testing","e73333df":"**Conclus\u00e3o**: Regi\u00f5es de mediana de idade muito baixa, costumam ter uma popula\u00e7\u00e3o maior do que aquelas que possuem mediana de idade alta; isso pode significar que regi\u00f5es de mediana de idade baixa h\u00e1 alta taxa de natalidade.","9d885b48":"### 2.1.3 Pieplot","9bd74ff7":"**Conclus\u00e3o:** Pessoas que recebem mais, em m\u00e9dia, s\u00e3o mais velhas.","b079f631":"                           Escola Polit\u00e9cnica da Universidade de S\u00e3o Paulo\n                                         Data: 08\/11\/2019\n#       PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es\n### An\u00e1lise e aplica\u00e7\u00e3o de diferentes classificadores a base *adult*\n#### Autor: Lucas Nunes Sequeira","6e700a68":"**Conclus\u00e3o:** Aqui podemos notar que a etnia que proporcionalmente menos possui pessoas que recebem acima de 50k \u00e9 'Black', isso pode sugerir pol\u00edticas\/cultura racista.","ff1bcc7f":"### 2.1.1 Pairplot","4ae28d23":"## 4.2. K-Nearest Neighboor\n### 4.2.1. Training with CV","cce2e030":"### 4.2.2. Testing","c76bb443":"### 8.1.1 Training with CV","18c4a4a9":"Atribuirei para as linhas que 'occupation' e 'workclass' s\u00e3o faltantes m\u00fatuos o valor: 'unknown' como um valor significante.","4d87a05a":"**Conclus\u00e3o:** De fato pessoas vi\u00favas s\u00e3o mais velhas.","260dbf50":"# Extra - California Household Prices\n# 6. Leitura e visualiza\u00e7\u00e3o dos dados\n## 6.1. Dataframe","7f781cc0":"**Conclus\u00e3o:** Nesse grafo, notamos que pessoas nunca casadas em maioria recebem menos de 50k, enquanto que a maior parte das pessoas que recebem mais de 50k, possuem um casamento civil.","6edf2ac8":"**Conclus\u00e3o:** Grande parte da popula\u00e7\u00e3o \u00e9 branca e estadounidense. Al\u00e9m disso, vemos que 'amer-indian-eskimo' \u00e9 a etnia com menor diversidade de pa\u00edses de origem.","8384b9b2":"## 1.2. Leitura dos dados ","f1d38b19":"**Conclus\u00e3o:** Nessa representa\u00e7\u00e3o, podemos ver que certamente aqueles que n\u00e3o recebem pagamento, n\u00e3o recebem sal\u00e1rio acima de 50K, mas vemos tambpem que a \u00fanica classe de trabalho que a maior parcela das pessoas recebe acima de 50K s\u00e3o empreendedores.","6d05e9bd":"# 8. Testando regressores","b715f51e":"## 4.3. Random Forest\n### 4.3.1. Training with CV","97b47480":"## 7.2. Base para treino","025155a8":"**Conclus\u00e3o**: A popula\u00e7\u00e3o litor\u00e2nea al\u00e9m de receberem mais do que a popula\u00e7\u00e3o do interior, o valor das casas tab\u00e9m \u00e9 superior.","f13f6581":"**Conclus\u00e3o**: A popula\u00e7\u00e3o varia pouco com rela\u00e7\u00e3o \u00e0 m\u00e9dia, exceto alguns locais que s\u00e3o como 'outliers'.","9950a58c":"**Conclus\u00e3o:** Mais da metade das pessoas tem ou algum col\u00e9gio ou ensino m\u00e9dio conclu\u00eddo. Aproximadamente 1\/5 das pessoas possui bacharelado.","7b6a77cd":"# 3. Prepara\u00e7\u00e3o dos dados\n## 3.1. M\u00e9todos de prepara\u00e7\u00e3o de atributos espec\u00edficos","b3c0760f":"**Conclus\u00e3o:** As ocupa\u00e7\u00f5es s\u00e3o diversas pela popula\u00e7\u00e3o intrevistada.","dec8a48a":"## 7.3. Escalonamento dos dados","9fddb800":"**Conclus\u00e3o:** Devido a distribui\u00e7\u00e3o de cor de pele ser em grande parte, branca isso indica as arestas mais azul; e no caso, vemos que dentre a popula\u00e7\u00e3o negra, a maior parte recebe menos de 50k.","dcf2d4f4":"### 4.4.3. Lime explanation examples","32c4e00b":"## 6.2. Visualiza\u00e7\u00e3o em 'mapa'","42b2533b":"### 4.3.3. Lime explanation example","6b40713c":"### 4.5.1. Acur\u00e1cia","91f7ec94":"Nos pr\u00f3ximos par\u00e1grafos, farei uma breve descri\u00e7\u00e3o dos classificadores utilizados, considerando quest\u00f5es como tempo de processamento para o teste e treino, acur\u00e1cia e explicabilidade\/interpretabilidade para os 4 classificadores utilizados nesse exerc\u00edcio: **Regress\u00e3o Log\u00edstica**, **K-Nearest Neighboors**, **Floresta Aleat\u00f3ria** e **XGBoost**.\n\nPrimeiro, com rela\u00e7\u00e3o ao classificador dito o mais simples: **Regress\u00e3o Log\u00edstica**, o que observamos \u00e9 que tanto o tempo de processamento para o treino como para os testes, teve o melhor desempenho; por outro lado, j\u00e1 com rela\u00e7\u00e3o \u00e0 acur\u00e1cia, deve o pior desempenho dentre os classificadores utilizados. Embora a regress\u00e3o linear tivera o pior desempenho de acur\u00e1cia, \u00e9 o classificador com maior facilidade de interpretabilidade\/explicabilidade; isso pois devido a forma como \u00e9 definido:\n\n$$g(X) = \\begin{cases}0\\text{, se }sig(\\sum_{i = 1}^n \\beta_ix_i) < 1\/2\\\\ 1\\text{, caso contr\u00e1rio}\\end{cases}$$\n\nEm que $sig$ \u00e9 a fun\u00e7\u00e3o sigmoid e $X$ \u00e9 um exemplo observado. Podemos inferir sobre a relev\u00e2ncia dos par\u00e2metros associados aos atributos, e uma maneira de entend\u00ea-los \u00e9 avaliando sua magnitude, quando maior seu valor, mais dominante \u00e9 o atributo associado com rela\u00e7\u00e3o aos demais.\n\nSegundo, agora com rela\u00e7\u00e3o ao classificador **K-Nearest Neighboors**, notamos que seu tempo de treino foi razoavelmente baixo, mas que seu tempo de teste foi bastante alto; sendo portanto um modelo ruim em quest\u00e3o de tempo de processamento para aplicar nos dados, caso seja mais importante o tempo de teste de exemplos. Al\u00e9m disso, sua acur\u00e1cia na valida\u00e7\u00e3o deixou a desejar, pois \u00e9 pouco acima da **Regress\u00e3o Log\u00edstica** mas o tempo de treino \u00e9 muito maior. A sua explicabilidade pode ser interessante; o classificador \u00e9 definido como:\n\n*Dado um exemplo, uma m\u00e9trica e um n\u00famero k de vizinhos, o classificador classifica um exemplo considerando a classe mais frequente observada nos k vizinhos do ponto em quest\u00e3o.*\n\nNesse ponto, explicar um exemplo com base nos vizinhos do mesmo, \u00e9 interessante pois tem os atributos dos vizinhos que os determinam, e dessa forma, \u00e9 poss\u00edvel associar os valores observados nos vizinhos com os valores dos atributos do exemplo a ser testado.\n\nO terceiro classificador analisado foi a **Floresta Aleat\u00f3ria**, um classificador bem mais complexo que os anteriores, apresentou um tempo de treino bastante superior aos demais, mas um tempo de teste bem baixo; como tamb\u00e9m uma acur\u00e1cia bastante interessante, pois supera consideravelmente a dos classificadoes mais simples supracitados. A **Floresta Aleat\u00f3ria** \u00e9 uma composi\u00e7\u00e3o de \u00e1rvores de decis\u00e3o em que a cada *split* s\u00e3o sorteados atributos que definem um exemplo, como tamb\u00e9m a cada \u00e1rvore, \u00e9 feito uma amostra com reposi\u00e7\u00e3o da base. Um clasificador muito mais complexo em termos de explicabilidade; mas que alternativas como tentativas de descrever um resumo dos votos das dezenas de \u00e1rvores utilizadas e como cada \u00e1rvore decidiu aquele voto, pode ser interessante. Isso pois, uma \u00e1rvore de decis\u00e3o (que comp\u00f5e uma floresta aleat\u00f3ria) \u00e9 um classificador muito interpret\u00e1vel.\n\nPor fim, o **XGBoost**, uma t\u00e9cinica de *boosting*, em que o classificador \u00e9 treinado sob diferentes classificadores mais simples, e a cada classificador gerado, o pr\u00f3ximo se ajusta em especial aos erros dos anteriores, uma forma de cobrir um conjunto cosider\u00e1vel de verdadeiros positivos e negativos. No caso, o classificador teve um tempo de treino pr\u00f3ximo ao do **KNN**, mas um tempo de teste compar\u00e1vel a **Regress\u00e3o Log\u00edstica**. N\u00e3o s\u00f3 isso como se destaca por ser o classificador com maior taxa de acerto (acur\u00e1cia). J\u00e1 com respeito a interpretabilidade\/explicabilidade, \u00e9 um modelo bastante caixa preta, n\u00e3o conhe\u00e7o uma forma simples de interpretar os resultados das classifica\u00e7\u00f5es, e por isso, no caso, considero o modelo com pior explicabilidade\/interpretabilidade.","f6c3bde6":"## 6.5. Boxplots","1d4abf90":"## 6.6. Heatmap","dad75639":"### 2.1.2 Barplot","4ad6e2b7":"## 8.1. Linear Regressor","9224ab31":"## 1.3. Tratando dos dados faltantes","2bc272db":"**Conclus\u00e3o**: Uma distribui\u00e7\u00e3o do tipo beta, com uma cauda longa \u00e0 direita. Poucos recebem muito, e a grande parte da popula\u00e7\u00e3o recebe pr\u00f3ximo \u00e0 mediana.","ea701a78":"**Conslus\u00e3o**: Ao sul podemos observar um cluster, que nele \u00e0 noroeste, a popula\u00e7\u00e3o \u00e9 mais velha que \u00e0 sudeste. A popula\u00e7\u00e3o central \u00e9 muito baixa (vemos grandes vazios -- desertos) e bolas muito pequenas. Ao norte, temos uma popula\u00e7\u00e3o bastante mista em idades, mas com densidade superior no litoral.","f3381332":"## 4.1. Regress\u00e3o Log\u00edstica\n### 4.1.1. Training with CV","ff54251c":"**Conclus\u00e3o:** Grande parte da popula\u00e7\u00e3o entrevistada trabalha no setor privado","a580282c":"## 8.2. K-Nearest Neighboors Regressor\n### 8.2.1. Training with CV","462a53e3":"**Conclus\u00e3o:** Com esse gr\u00e1fico, observamos que proporcionalmente, mulheres recebem menos que os homens. Isso sugere desigualdade de g\u00eanero.","4e1dc283":"**Conclus\u00e3o**: Pir\u00e2mide et\u00e1ria com base infantil (0 - 13) muito pequena, grande maioria adultos (21 - 55).","5590df74":"**Conclus\u00e3o:** Nesse gr\u00e1fico, vemos por exemplo, que pessoas casadas costumam receber mais do que pessoas divorciadas, separadas, vi\u00favas ou nunca casadas.\n\n**Ideia:** Separar atributos em 'married_present': [True, False]","4849223b":"**Conclus\u00e3o:** Nessa visualiza\u00e7\u00e3o, vemos que aqueles que recebem mais de 50k, distribuiem-se especialmente em 'prof-specialty' e 'exec-managering'.","78058fd7":"**Conclus\u00e3o**: Para os exemplos 6 e 21, o classificador teve muita certeza do 21, considerando fatores mais relevantes como capital_gain e educa\u00e7\u00e3o. J\u00e1 para o exemplo 6, houve menos certeza da resposta, mas novamente o capital_gain e educa\u00e7\u00e3o favoreceram a classifica\u00e7\u00e3o >50K, enquanto que a classe de trabalho e capital_loss para classe contr\u00e1ria.","64c7d3e5":"**Conclus\u00e3o:** Vemos nesse caso, que pessoas vi\u00favas trabalham muito menos horas por semana (provavelmente pela idade), mas que pessoas casadas costumam trabalhar mais do que as 40 horas semanais.","488988d4":"**Conclus\u00e3o**: Os 3\/4 quartiles diminuem sutilmente de amplitude para mediana de idades maiores; mas na maioria das idades, a quantidade de outliers \u00e9 grande.","77578c9c":"### 4.5.3. Outputs","f040934d":"### 4.3.2. Testing"}}