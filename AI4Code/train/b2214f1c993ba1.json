{"cell_type":{"1b9ad691":"code","9fe12f2e":"code","94aaf7d1":"code","5143bcf4":"code","c685952f":"code","d8f51143":"code","35acf841":"code","63de08ed":"code","b2c0548c":"code","4803651b":"code","07115144":"code","161b0793":"code","00f74e68":"code","72922655":"code","bbcc30b4":"code","1798b009":"code","b95ffd14":"code","809336bf":"code","fd8f1d5c":"code","5413825a":"code","70f8cfa1":"code","28c2e8ea":"code","419ccb85":"code","c81a2a89":"code","6bcd4342":"code","c08a746e":"code","414de9a2":"code","f882c47b":"code","4451c14c":"code","131868b9":"code","aec12df5":"code","a4beed28":"code","6eec1b90":"code","e46ce042":"code","bc0cd989":"code","f2c74eac":"code","7db35fb9":"code","c8d4f5da":"code","3e6d1226":"code","55c863e2":"code","61030380":"code","03e5639e":"code","419f03b0":"code","2f6d2f2a":"code","be14ae89":"code","b540f2da":"code","befb9d53":"code","be51134e":"code","e96c387c":"code","90390e75":"code","8b9b6901":"code","ee22e122":"code","628fb108":"code","bfffdc7a":"code","727b17f6":"code","c10a7ec2":"markdown","ab08dd98":"markdown","094f19a8":"markdown","ae6eb316":"markdown","91c878e0":"markdown","afcf7392":"markdown","01e0ff66":"markdown"},"source":{"1b9ad691":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\n\n\ndata = pd.read_csv('..\/input\/car-evaluation-data-set\/car_evaluation.csv', header = None)  ","9fe12f2e":"data.head()","94aaf7d1":"col_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n\n\ndata.columns = col_names\n\ncol_names","5143bcf4":"data.describe()","c685952f":"data.drop_duplicates()","d8f51143":"data.info()","35acf841":"for col in data.columns:\n    print(data[col].value_counts())","63de08ed":"X = data.drop(['class'], axis=1)\ny = data['class']","b2c0548c":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42, stratify=y)","4803651b":"print(y_test.value_counts()) #we secured the 20% in the classes represenation","07115144":"mapping = [{'col':'buying', 'mapping':{'low':0, 'med':1, 'high':2, 'vhigh':3}},\n          {'col':'maint', 'mapping':{'low':0, 'med':1, 'high':2, 'vhigh':3}},\n          {'col':'doors', 'mapping':{'2':0, '3':1, '4':2, '5more':3}},\n          {'col':'persons', 'mapping':{'2':0, '4':1, 'more':2}},\n          {'col':'lug_boot', 'mapping':{'small':0, 'med':1, 'big':2}},\n          {'col':'safety', 'mapping':{'low':0, 'med':1, 'high':2}}]","161b0793":"import category_encoders as ce\n\nencoder = ce.OrdinalEncoder(cols=['byuing', 'maint', 'doors', 'persons', 'lug_boot', 'safety'], mapping = mapping)\n\nX_train = encoder.fit_transform(X_train)\nX_test = encoder.fit_transform(X_test)","00f74e68":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state = 42)\n           \nX_train, y_train = sm.fit_sample(X_train, y_train) ","72922655":"print(y_train.value_counts())","bbcc30b4":"from sklearn.metrics import log_loss\nfrom sklearn.tree import DecisionTreeClassifier\n\ntraining_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=i, random_state = 42)\n    \n    \n    clf_gini.fit(X_train, y_train)\n    y_pred_gini = clf_gini.predict_proba(X_test)\n    y_pred_train_gini = clf_gini.predict_proba(X_train)\n    \n     \n\n    training_loss.append(log_loss(y_train, y_pred_train_gini))\n\n    test_loss.append(log_loss(y_test, y_pred_gini))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","1798b009":"y = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('Max Depth')\n# Set the y axis label of the current axis.\nplt.ylabel('Log-Loss')\n# Set a title of the current axes.\nplt.title('Log-Loss plot ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","b95ffd14":"from sklearn.model_selection import cross_val_score\n\nclf_gini = DecisionTreeClassifier(criterion='gini', max_depth=7, random_state = 42)\n\nprint('Cross-Validation Score:',np.mean(cross_val_score(clf_gini, X_train, y_train, cv=10)))","809336bf":"\nclf_gini = DecisionTreeClassifier(criterion='gini', max_depth=7, random_state = 42)\n\nclf_gini.fit(X_train, y_train)","fd8f1d5c":"y_pred_gini = clf_gini.predict(X_test)","5413825a":"print('Training set score: {:.4f}'.format(clf_gini.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_gini.score(X_test, y_test)))","70f8cfa1":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport graphviz\nfrom sklearn import tree\n\n\n\n\ndot_data = tree.export_graphviz(clf_gini, out_file=None, \n                              feature_names=X_train.columns,  \n                              class_names=y_train,  \n                              filled=True, rounded=True,  \n                              special_characters=True)\n\ngraph = graphviz.Source(dot_data) \n\ngraph ","28c2e8ea":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\ncf_matrix = confusion_matrix(y_test, y_pred_gini)\n\ncf_matrix","419ccb85":"plt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cf_matrix, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(y_test.unique())\nax.yaxis.set_ticklabels(y_test.unique())","c81a2a89":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_gini))","6bcd4342":"training_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=i, random_state = 42)\n    \n    \n    clf_en.fit(X_train, y_train)\n    y_pred_en = clf_en.predict_proba(X_test)\n    y_pred_train_en = clf_en.predict_proba(X_train)\n    \n     \n\n    training_loss.append(log_loss(y_train, y_pred_train_en))\n\n\n    test_loss.append(log_loss(y_test, y_pred_en))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","c08a746e":"y = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('Max Depth')\n# Set the y axis label of the current axis.\nplt.ylabel('Log-Loss')\n# Set a title of the current axes.\nplt.title('Log-Loss plot ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","414de9a2":"clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=6, random_state = 42)\n\nprint('Cross-Validation Score:',np.mean(cross_val_score(clf_en, X_train, y_train, cv=20)))","f882c47b":"clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=6, random_state = 42)\n\nclf_en.fit(X_train, y_train)","4451c14c":"y_pred_en = clf_en.predict(X_test)","131868b9":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(clf_en.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_en.score(X_test, y_test)))","aec12df5":"dot_data = tree.export_graphviz(clf_en, out_file=None, \n                              feature_names=X_train.columns,  \n                              class_names=y_train,  \n                              filled=True, rounded=True,  \n                              special_characters=True)\n\ngraph = graphviz.Source(dot_data) \n\ngraph ","a4beed28":"cm = confusion_matrix(y_test, y_pred_en)\n\nprint('Confusion matrix\\n\\n', cm)","6eec1b90":"plt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(y_test.unique())\nax.yaxis.set_ticklabels(y_test.unique())","e46ce042":"print(classification_report(y_test, y_pred_en))","bc0cd989":"from sklearn.ensemble import GradientBoostingClassifier\n\ntraining_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    clf_gb = GradientBoostingClassifier( max_depth=i, random_state = 42)\n    \n    \n    clf_gb.fit(X_train, y_train.values.ravel())\n    y_pred_gb = clf_gb.predict_proba(X_test)\n    y_pred_train_gb = clf_gb.predict_proba(X_train)\n    \n     \n\n    training_loss.append(log_loss(y_train, y_pred_train_gb))\n\n\n    test_loss.append(log_loss(y_test, y_pred_gb))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","f2c74eac":"y = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('Max Depth')\n# Set the y axis label of the current axis.\nplt.ylabel('Log-Loss')\n# Set a title of the current axes.\nplt.title('Log-Loss plot ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","7db35fb9":"clf_gb = GradientBoostingClassifier( max_depth=5, random_state = 42)\n\nprint('Cross-Validation Score:',np.mean(cross_val_score(clf_gb, X_train, y_train.values.ravel(), cv=10)))","c8d4f5da":"clf_gb = GradientBoostingClassifier( max_depth=5, random_state = 42)\n\nclf_gb.fit(X_train, y_train.values.ravel())","3e6d1226":"y_pred_gb = clf_gb.predict(X_test)","55c863e2":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(clf_gb.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_gb.score(X_test, y_test)))","61030380":"dot_data = tree.export_graphviz(clf_gb.estimators_[0, 0], out_file=None, \n                              feature_names=X_train.columns,  \n                              class_names=['1','2','3','4'],  \n                              filled=True, rounded=True,  \n                              special_characters=True)\n\ngraph = graphviz.Source(dot_data) \n\ngraph ","03e5639e":"cm = confusion_matrix(y_test, y_pred_gb)\n\nprint('Confusion matrix\\n\\n', cm)","419f03b0":"plt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(pd.unique(y_test.values.ravel()))\nax.yaxis.set_ticklabels(pd.unique(y_test.values.ravel()))","2f6d2f2a":"print(classification_report(y_test, y_pred_gb))","be14ae89":"import xgboost as xgb\n\nclf = xgb.XGBClassifier(max_depth=2, n_jobs = 4)","b540f2da":"training_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    clf_xgb = xgb.XGBClassifier( max_depth=i, random_state = 42, n_jobs = 4)\n    \n    \n    clf_xgb.fit(X_train, y_train.values.ravel())\n    y_pred_xgb = clf_xgb.predict_proba(X_test)\n    y_pred_train_xgb = clf_xgb.predict_proba(X_train)\n    \n     \n\n    training_loss.append(log_loss(y_train, y_pred_train_xgb))\n\n    test_loss.append(log_loss(y_test, y_pred_xgb))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","befb9d53":"y = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('Max Depth')\n# Set the y axis label of the current axis.\nplt.ylabel('Log-Loss')\n# Set a title of the current axes.\nplt.title('Log-Loss plot ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","be51134e":"clf_xgb = xgb.XGBClassifier( max_depth=4, random_state = 42, n_jobs = 4)\n\nprint('Cross-Validation Score:',np.mean(cross_val_score(clf_xgb, X_train, y_train.values.ravel(), cv=10)))","e96c387c":"clf_xgb = xgb.XGBClassifier( max_depth=4, random_state = 42, n_jobs = 4)\n\nclf_xgb.fit(X_train, y_train.values.ravel())","90390e75":"y_pred_xgb = clf_xgb.predict(X_test)","8b9b6901":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(clf_xgb.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_xgb.score(X_test, y_test)))","ee22e122":"fig, ax = plt.subplots(figsize=(30, 30))\nxgb.plot_tree(clf_xgb, num_trees=4, ax=ax)\nplt.show()","628fb108":"cm = confusion_matrix(y_test, y_pred_xgb)\n\nprint('Confusion matrix\\n\\n', cm)","bfffdc7a":"plt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(pd.unique(y_test.values.ravel()))\nax.yaxis.set_ticklabels(pd.unique(y_test.values.ravel()))","727b17f6":"print(classification_report(y_test, y_pred_xgb))","c10a7ec2":"# GRADIENT BOOSTING","ab08dd98":"# Decision Tree: ENTROPY criterion","094f19a8":"We will use an ordinal encoder since there is ordinality in our data","ae6eb316":"### **In this notebook I will try to do an exploratory data analysis and classification for the Car Evaluation dataset, found in the UCI machine learning repository.**","91c878e0":"# Decision Tree: GINI criterion","afcf7392":"We will produce synthetic data to balance the classes","01e0ff66":"# XGBOOST"}}