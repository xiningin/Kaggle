{"cell_type":{"322d2ded":"code","89924315":"code","20176a92":"code","1a8e9165":"code","8726f549":"code","a752ba9b":"code","b3ec0dcd":"code","373873c3":"code","067f5671":"code","1c501b80":"code","8acba9cd":"code","da14f663":"code","722ea7ee":"code","9b9f4f84":"code","f835559a":"code","86f87a7c":"code","2b6e4c83":"code","45785d4c":"code","0da40f26":"code","92d9299d":"code","41cdbdb4":"code","fd01155d":"code","0a0e294e":"code","58a079e3":"code","47b2584f":"code","c86d9749":"code","85f677d7":"code","44343adc":"code","d3f0de72":"code","0138ecb3":"code","49aceb5a":"code","7e79feaa":"code","72c3be84":"code","434fd625":"code","9917b213":"code","df85eb92":"code","21b2413d":"code","2ec7b553":"code","0a002f4d":"code","ef2feec7":"code","2cb49944":"code","8660e540":"code","34ab7492":"code","d194c43f":"code","16101e55":"code","a5f8d42d":"code","0dfa1b7c":"code","932f4f16":"code","cba7097c":"code","7f62424d":"code","ff7e553c":"code","17d4ca15":"code","529e58ff":"code","378cc71c":"code","57648014":"code","0352945d":"code","8d6b6fd9":"code","1b359d6e":"code","5726b9d9":"code","5f5ea9ed":"code","1b7c6822":"code","a3d7953f":"code","e275b340":"code","2f124b83":"code","f4861129":"code","7091ecfc":"code","aac71614":"code","4257689d":"code","8080e59d":"code","dd2f2fba":"code","13967430":"code","3c3f688a":"code","e69f389b":"code","7f6e9e77":"code","88d0bb72":"code","a9aaa1fd":"code","294cb820":"code","38fe320a":"code","ec74fd97":"code","5926ccfc":"code","19f715d7":"code","29fcb41a":"code","4d314643":"code","3582084e":"markdown","94988283":"markdown","c00757a8":"markdown","df44cff1":"markdown","256a6d19":"markdown","8f034bbc":"markdown","c122c735":"markdown","3687bca9":"markdown","994b80f0":"markdown","d79c6797":"markdown","bb7689ea":"markdown","705f64fd":"markdown","2a464cb0":"markdown","3a0fd7ae":"markdown","b24ba85a":"markdown","b5d0b01a":"markdown","5086cb66":"markdown","25830038":"markdown","35341b7e":"markdown","b2d3d5cb":"markdown","a989a171":"markdown","37bea887":"markdown","ed9a8f5e":"markdown","7c3f9939":"markdown"},"source":{"322d2ded":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","89924315":"# Import Exploring and Visualizing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","20176a92":"# Import training and validaiton datasets\n\ndf = pd.read_csv(\"..\/input\/bluebook-for-bulldozers\/TrainAndValid.csv\",\n                 low_memory=False)","1a8e9165":"df.head()","8726f549":"df.info()","a752ba9b":"df.isna().sum()","b3ec0dcd":"sns.set_style(\"darkgrid\")\nfig, ax =plt.subplots()\nax.scatter(df[\"saledate\"][:1000],df[\"SalePrice\"][:1000])\n","373873c3":"df.SalePrice.plot.hist()","067f5671":"# Import data again but this time parse date\ndf = pd.read_csv(\"..\/input\/bluebook-for-bulldozers\/TrainAndValid.csv\",\n                 low_memory=False,\n                 parse_dates=[\"saledate\"])","1c501b80":"df.saledate[:100]","8acba9cd":"fig, ax = plt.subplots()\nax.scatter(df['saledate'][:1000],df['SalePrice'][:1000])","da14f663":"df.head().T # To check all columns in the dataset","722ea7ee":"df.saledate.head(20)","9b9f4f84":"# Sort DataFrame in date order\ndf.sort_values(by=[\"saledate\"],inplace=True, ascending=True)\ndf.saledate.head(20)","f835559a":"df.head()","86f87a7c":"# Make a copy \ndf_temp = df.copy()","2b6e4c83":"df_temp","45785d4c":"# Let's add some date features extracting from saledate column\ndf_temp[\"saleYear\"] = df_temp.saledate.dt.year\ndf_temp[\"saleMonth\"] = df_temp.saledate.dt.month\ndf_temp[\"saleDay\"] = df_temp.saledate.dt.day\ndf_temp[\"saleDayOfWeek\"] = df_temp.saledate.dt.dayofweek\ndf_temp[\"saleDayOfYear\"] = df_temp.saledate.dt.dayofyear","0da40f26":"df_temp.head()","92d9299d":"# Now we've enriched our DataFrame with date time features, now we can remove saledate column\ndf_temp.drop(\"saledate\",axis=1,inplace=True)","41cdbdb4":"# Check the values of different columns \ndf_temp.state.value_counts()","fd01155d":"df_temp.info()","0a0e294e":"df_temp.head().T","58a079e3":"pd.api.types.is_string_dtype(df_temp[\"UsageBand\"])","47b2584f":"# Find the columns which contain strings\nfor labels, content in df_temp.items():\n    if pd.api.types.is_string_dtype(content):\n        print(labels)\n# It will loop through all features and print all those column names which contains\n# the string datatype","c86d9749":"# If you're wondering what df.items() does, here's an example below\nrandom_dict = {\"key1\": \"hello\",\n               \"key2\": \"world!\"}\nfor key, value in random_dict.items():\n    print(f\"This is the key:{key}\")\n    print(f\"This is the value:{value}\")    ","85f677d7":"# This will turn all of the string value into category values \nfor label, content in df_temp.items():\n    if pd.api.types.is_string_dtype(content):\n        df_temp[label] = content.astype(\"category\").cat.as_ordered()","44343adc":"df_temp.info()","d3f0de72":"# Let's check out the state column as ordered\ndf_temp.state.cat.categories","0138ecb3":"# It still have string in it but pandas has assigned hidden numbers to its values \n\n# Let's check those numbers\n\ndf_temp.state.cat.codes","49aceb5a":"# Check the missing data (in percentage)\n\ndf_temp.isnull().sum()\/len(df_temp)","7e79feaa":"df_temp.isna().sum()","72c3be84":"# Let's check which columns are numberic first\nfor label, content in df_temp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        print(label)","434fd625":"# Check which numeric columns have null values\nfor label, content in df_temp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","9917b213":"# Fill numberic rows with the median\nfor label, content in df_temp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            # Add a binary column which tells us if the data was missing\n            df_temp[label+\"_is_missing\"] = pd.isnull(content)\n            # Fill missing numeric values with median\n            df_temp[label] = content.fillna(content.median())","df85eb92":"df_temp.head()","21b2413d":"# Let's Demonstrate how median is more robust (essential) than mean\n# In most cases outliers in datasets really impact the mean which really doesn't\n# make any sense but median will never be affected with outliers\n\nhundreds = np.full((1000), 100)\nhundreds_billion = np.append(hundreds, 1000000000)\n\nnp.mean(hundreds),np.mean(hundreds_billion), np.median(hundreds),np.median(hundreds_billion)","2ec7b553":"# Chech if there is any null numeric values\nfor label, content in df_temp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)\n# There will no output because there is no numeric missing values","0a002f4d":"# Check to see how many examples were missing\ndf_temp.auctioneerID_is_missing.value_counts()","ef2feec7":"df_temp.isna().sum()","2cb49944":"df_temp.info()","8660e540":"# Check for columns which are numeric\nfor label, content in df_temp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        print(label)     ","34ab7492":"# Turn categorical variables into numbers and fill missing\nfor label, content in df_temp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        # Add binary column to indicate whether sample had missing values\n        df_temp[label+\"_is_missing\"] = pd.isnull(content)\n        # Turn categories into numbers and add +1\n        df_temp[label] = pd.Categorical(content).codes+1","d194c43f":"pd.Categorical(df_temp[\"state\"]).codes+1","16101e55":"df_temp.info()","a5f8d42d":"df_temp.head().transpose()","0dfa1b7c":"df_temp.describe()","932f4f16":"df_temp.dtypes","cba7097c":"df_temp.columns","7f62424d":"df_temp.head()","ff7e553c":"%%time\n# Instantiate model \nmodel = RandomForestRegressor(random_state=42)\n\n# Fit the model \nmodel.fit(df_temp.drop(\"SalePrice\",axis =1), df_temp[\"SalePrice\"])","17d4ca15":"# Score the model\nmodel.score(df_temp.drop(\"SalePrice\",axis=1), df_temp[\"SalePrice\"])","529e58ff":"df_temp.saleYear","378cc71c":"df_temp.saleYear.value_counts()","57648014":"# Split data into training and validation\ndf_val = df_temp[df_temp.saleYear == 2012]\ndf_train = df_temp[df_temp.saleYear!= 2012]\n\nlen(df_val), len(df_train)","0352945d":"# Split data into X, y\nX_train, y_train = df_train.drop(\"SalePrice\", axis=1), df_train.SalePrice\nX_valid, y_valid = df_val.drop(\"SalePrice\",axis= 1), df_val.SalePrice\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape","8d6b6fd9":"# Create evaluation function(The competition uses RMSLE)\nfrom sklearn.metrics import mean_squared_log_error, mean_absolute_error, r2_score\n\ndef rmsle(y_test, y_preds):\n    \"\"\"\n    Calculates root mean squared log error between predictions and true labels\n    \"\"\"\n    return np.sqrt(mean_squared_log_error(y_test, y_preds))\n\n# Create function to evaluate model on a few different levels\ndef show_scores(model):\n    train_preds = model.predict(X_train)\n    val_preds = model.predict(X_valid)\n    scores = {\"Training MAE\": mean_absolute_error(y_train, train_preds),\n              \"Valid MAE\": mean_absolute_error (y_valid, val_preds),\n              \"Training RMSLE\": rmsle(y_train, train_preds),\n              \"Valid RMSLE\": rmsle(y_valid, val_preds),\n              \"Training R^2\": r2_score(y_train, train_preds),\n              \"Valid R^2\": r2_score(y_valid, val_preds)}\n    return scores\n    ","1b359d6e":"# # This takes far too long... for experimenting \n\n# %%time\n# model = RandomForestRegressor(n_jobs =-1,\n#                               random_state=42)\n# model.fit(X_train, y_train)","5726b9d9":"# One method is that we can slice tran sets into eg.10K to train on \n# model.fit(X_train[:10000], y_train[:10000])","5f5ea9ed":"# another way is to Change max_samples value \nmodel = RandomForestRegressor(random_state=42,\n                              max_samples=10000)","1b7c6822":"%%time\n# Cutting down on the max number of samples each estimator can see improves training time\nmodel.fit(X_train, y_train)","a3d7953f":"show_scores(model)","e275b340":"%%time\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Different RandomForestRegressor hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10,100,10),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2),\n           \"max_features\": [0.5, 1,\"sqrt\", \"auto\"],\n           \"max_samples\": [10000]}\n# Instantiate randomized search cv\nrs_model = RandomizedSearchCV(RandomForestRegressor(random_state=42),\n                             param_distributions=rf_grid,\n                             n_iter=2,\n                             cv=5,\n                             verbose=True)\n# Fit the RandomizedSearchCV model\nrs_model.fit(X_train, y_train)","2f124b83":"# Find the best model hyperparameters\n\nrs_model.best_params_","f4861129":"# Evaluate the RandomizedSearch model\nshow_scores(rs_model)","7091ecfc":"%%time\n\n# Most ideal hyperparameters\nideal_model = RandomForestRegressor(n_estimators =40,\n                                    min_samples_leaf=1,\n                                    min_samples_split=14,\n                                    max_features=0.5,\n                                    max_samples=None,\n                                    random_state=42)\n# Fit the ideal model\nideal_model.fit(X_train, y_train)","aac71614":"# Show scores for ideal model (trained on all data)\nshow_scores(ideal_model)","4257689d":"# Show scores on rs_model (Only trained on ~10,000 examples)\nshow_scores(rs_model)","8080e59d":"# Import Test dataset\ndf_test = pd.read_csv(\"..\/input\/bluebook-for-bulldozers\/Test.csv\",\n                      low_memory=False,\n                      parse_dates=[\"saledate\"])\ndf_test.head()","dd2f2fba":"df_test.isna().sum()","13967430":"df_test.info()","3c3f688a":"df_test.columns","e69f389b":"def preprocess_data(df):\n    \"\"\"\n    Performs transformations on df and returns transformed df.\n    \"\"\"\n    # Let's add some date features extracting from saledate column\n    df[\"saleYear\"] = df.saledate.dt.year\n    df[\"saleMonth\"] = df.saledate.dt.month\n    df[\"saleDay\"] = df.saledate.dt.day\n    df[\"saleDayOfWeek\"] = df.saledate.dt.dayofweek\n    df[\"saleDayOfYear\"] = df.saledate.dt.dayofyear\n    \n    df.drop(\"saledate\",axis=1, inplace=True)\n    \n    # Fill numeric row with median\n    for label, content in df.items():\n        if pd.api.types.is_numeric_dtype(content):\n            if pd.isnull(content).sum():\n                # Add a binary column which tells us if the data was missing\n                df[label+\"_is_missing\"] = pd.isnull(content)\n                # Fill missing numeric values with median\n                df[label] = content.fillna(content.median())\n    \n         # Fill the categorical missing data and turned categorical into numbers\n        if not pd.api.types.is_numeric_dtype(content):\n                df[label+\"_is_missing\"] = pd.isnull(content)\n                # We add +1 to the category code because pandas encodes missing values as -1\n                df[label] = pd.Categorical(content).codes+1\n                \n    \n    return df","7f6e9e77":"# Processed test data\ndf_test = preprocess_data(df_test)\n\ndf_test.head()","88d0bb72":"# # Make predictions on updated test data\n# test_preds = ideal_model.predict(df_test)","a9aaa1fd":"# We can find how the columns differ using sets\nset(X_train.columns) - set(df_test.columns)","294cb820":"# Manually adjust df_test to have auctioneerID_is_missing columns\ndf_test[\"auctioneerID_is_missing\"] = False\ndf_test.head()","38fe320a":"# Make predictions on the test data\ntest_preds = ideal_model.predict(df_test)","ec74fd97":"test_preds","5926ccfc":"# Format predicitons into the same format Kaggle is after\ndf_preds = pd.DataFrame()\ndf_preds[\"SalesID\"] = df_test[\"SalesID\"]\ndf_preds[\"SalesPrice\"] = test_preds\ndf_preds","19f715d7":"# Find Feature importance of our model  \nideal_model.feature_importances_","29fcb41a":"# Helper function for plotting feature importance\ndef plot_features(columns, importances, n=20):\n    df = (pd.DataFrame({\"features\":columns,\n                        \"features_importances\": importances})\n          .sort_values(\"features_importances\",ascending=False)\n          .reset_index(drop=True))\n    \n    # Plot the dataframe we created\n    fig, ax = plt.subplots()\n    ax.barh(df[\"features\"][:n], df[\"features_importances\"][:20])\n    ax.set_ylabel(\"Features\")\n    ax.set_xlabel(\"Features Importance\")\n    ax.invert_yaxis()","4d314643":"plot_features(X_train.columns, ideal_model.feature_importances_)","3582084e":"### Add datetime parameters for `saledate` column","94988283":"## Testing our model on a subset (to tune the hyperparameters)","c00757a8":"We've made some predictions but they're not in the same format Kaggle is asking for:\nhttps:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/overview\/evaluation","df44cff1":"Finally now our test data dataframe has the same features as our training dataframe, we can make predictions!","256a6d19":"Thanks to pandas categories we now have a way to access all our data in the form of numbers\n\nBut we still have bunch of missing values...","8f034bbc":"## 5. Modelling\n\nWe have done enough EDA(We could always do more) but let's start to do some model-driven EDA","c122c735":"### Splitting data into train\/validation sets","3687bca9":"#### As we saw for ideal_model we had good decrease in RMSLE after finding best parameters\n","994b80f0":"# Make Predictions On Test Dataset","d79c6797":"Now that all of data is numeric as well as our dataframe has no missing values, we should be able to build a machine learing model.","bb7689ea":"### Preprocessing the data (getting the test dataset in the same format as our training dataset)","705f64fd":"# Features Importance\n\nFeature importance seeks to figure out which different attributes of the data were most importance when it comes to predicting the **Target Varaiable**(SalePrice).","2a464cb0":"### Parsing dates\n\nWhen we work time series data, we want to enrich the time & data component as much as possible.\n\nWe can do that by telling pandas which of our columns has dates in it using the `parse_dates` parameter. ","3a0fd7ae":"### Make a copy of original DataFrame\n\nWe make a copy of original dataframe so when we manipluate the copy, we've still got our original data.","b24ba85a":"**Question:** Why doesn't the above metric hold water? (why isn't the metric reliable)","b5d0b01a":"> An error will occur if we run below cell because we have a missing feature","5086cb66":"### Sort Dataframe by saledate\n\nWhen working with time series data, it's a good idea to sort it by date.","25830038":"## Train a model with the best hyperparameters\n\n**Note:** These were found after 100 iterations of `RandomizedSearchCV`","35341b7e":"## Fill missing values\n\n### 1. Fill numeric missing values first","b2d3d5cb":"# Predicting The Sale Price of Bulldozers Using Machine Learning\n\nIn this notebook we're going to go through an example of machine learning project with the goal of predicting the sale price of bulldozers.\n\n## 1. Problem Definition\n\n> How well can we predict the future sale price of a bulldozer, given its characteristic and previous examples of how much similar bulldozers have been sold for?\n\n## 2. Data\n\nThe Data is Downloaded from the kaggle bluebook for bulldozers competition: https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/data\n\nThere are 3 main datasets:\n\n* **Train.csv** is the training set, which contains data through the end of 2011.\n* **Valid.csv** is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.\n* **Test.csv** is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition\n\n## 3. Evaluation \n\nThe evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices.\n\nFor more on the evaluation of this project check:\nhttps:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/overview\/evaluation\n\nNote: The goal for most regression evaluation is metrics is to minimize the error. For example, our goal for this project will be to build a machine learning model which minimises the RMSLE(root mean squared log error).\n\n\n## 4. Features\n\nKaggle provides a data Dictionary detailing all of the features of the dataset. You can view this data dictionary on Kaggle https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/data\n\n## 5. Modelling\n\n## 6. Experimentaion ","a989a171":"### Filling and turning categorical variables into numbers","37bea887":"### Building an evaluation function","ed9a8f5e":"### Hyperparameter tunning with RandomizedSearchCV","7c3f9939":"### Convert string (object) into categories\n\nOne way we can turn all of our data  into numbers is by converting them into pandas categories.\n\nWe can check the different datatypes compatible with pandas here:\nhttps:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.api.types.is_object_dtype.html"}}