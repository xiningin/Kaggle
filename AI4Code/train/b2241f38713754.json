{"cell_type":{"17a731de":"code","7864fd37":"code","944f372f":"code","0408a208":"code","0faffa85":"code","fa5451ba":"code","9910bf06":"code","bfadf99d":"code","b87b7c35":"code","aceaa6bf":"code","6759ee30":"code","d5d6b75a":"code","532d9697":"code","ced605c2":"code","4c72a3fa":"code","53020680":"code","2e7b2319":"code","b022fc6d":"code","fc6a84b5":"code","15cf96c6":"code","069d79d7":"code","520b516f":"code","3374e8da":"code","d7174bec":"code","fa33027f":"code","f8a8e268":"code","a7bb0cca":"code","06a9df43":"code","710b0779":"code","6a50f165":"code","17ef1d8d":"code","66e96c56":"code","8c1578b9":"code","08a845f2":"code","f49abf80":"code","62ed1e7d":"code","14e03340":"code","c76c1861":"code","eb020c9b":"code","f5c46898":"code","c1a83441":"code","bcce4bf5":"code","960326d9":"code","3416b2ea":"code","a80ed1e7":"code","77f07c99":"code","d8478b17":"code","42a9df0b":"code","ac5edfed":"code","d840e3de":"code","464bf903":"code","4b4319b0":"code","a54791de":"code","e34f1aed":"code","3d80e25f":"code","70a7ccac":"code","ffbddf41":"code","199cc6de":"code","e7c79442":"code","78308d64":"code","5404fea8":"code","6b634361":"code","9a6e7ddc":"code","d25695fa":"code","7a0ca24d":"code","7f2dfcec":"code","0c8ceb0f":"code","d627c65a":"code","32def1d2":"code","724e142e":"code","4c1c2e95":"code","650e2d9c":"code","3862909f":"code","137b3eeb":"code","f18a2975":"code","d1010a7f":"code","a88ea7a7":"code","c00c6e92":"code","639f3259":"code","7498c7f4":"code","aa1d5fe4":"code","ba92f512":"code","ce4f43fe":"code","47bb89e5":"code","7edb248d":"code","2d9f3860":"code","5347a702":"code","6cd296f3":"code","7cba8ef1":"code","5692fad6":"code","61ea8124":"code","f11c0a89":"code","19f22bbe":"code","25dee514":"code","5e7c16c8":"code","42fd691a":"code","9b418f84":"code","0f40dcec":"markdown","281cbed6":"markdown","1a52844e":"markdown","8c98129f":"markdown","e19ab031":"markdown","661278de":"markdown","a53bb117":"markdown","1272a913":"markdown","d3ecd6a5":"markdown","17ee21a9":"markdown","5363ef0e":"markdown","f9a087da":"markdown","d8cdc17f":"markdown","717a8c52":"markdown","4fd4db06":"markdown","f0eb2858":"markdown","e7030384":"markdown","1e835225":"markdown","174984b4":"markdown","e450ab84":"markdown","799069b1":"markdown","d0055318":"markdown","c6239d95":"markdown","1b251101":"markdown","c83a3352":"markdown","d2099e43":"markdown","0918ca3a":"markdown","3b4c1c1d":"markdown","ecc76f36":"markdown","d7217188":"markdown","63386287":"markdown","e3f0b557":"markdown","d2635b34":"markdown","43713249":"markdown","100352f1":"markdown"},"source":{"17a731de":"import os\nimport re\nimport pickle\nimport string\nimport unicodedata\nfrom random import randint\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom nltk.corpus import stopwords\nfrom wordcloud import STOPWORDS, WordCloud\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding, TimeDistributed","7864fd37":"!pip install -q contractions==0.0.48","944f372f":"from contractions import contractions_dict\n\nfor key, value in list(contractions_dict.items())[:10]:\n    print(f'{key} == {value}')","0408a208":"# Using TPU\n\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","0faffa85":"filename1 = '..\/input\/news-summary\/news_summary.csv'\nfilename2 = '..\/input\/news-summary\/news_summary_more.csv'\n\ndf1 = pd.read_csv(filename1, encoding='iso-8859-1').reset_index(drop=True)\ndf2 = pd.read_csv(filename2, encoding='iso-8859-1').reset_index(drop=True)","fa5451ba":"df1.sample(5)","9910bf06":"df2.sample(5)","bfadf99d":"df1_columns = df1.columns.tolist()\ndf1_columns.remove('headlines')\ndf1_columns.remove('text')\ndf1.drop(df1_columns, axis='columns', inplace=True)\n\ndf = pd.concat([df1, df2], axis='rows')\ndel df1, df2\n\n# Shuffling the df\ndf = df.sample(frac=1).reset_index(drop=True)\n\nprint(f'Dataset size: {len(df)}')\ndf.sample(5)","b87b7c35":"def expand_contractions(text, contraction_map=contractions_dict):\n    # Using regex for getting all contracted words\n    contractions_keys = '|'.join(contraction_map.keys())\n    contractions_pattern = re.compile(f'({contractions_keys})', flags=re.DOTALL)\n\n    def expand_match(contraction):\n        # Getting entire matched sub-string\n        match = contraction.group(0)\n        expanded_contraction = contraction_map.get(match)\n        if not expand_contractions:\n            print(match)\n            return match\n        return expanded_contraction\n\n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text\n\n\nexpand_contractions(\"y'all can't expand contractions i'd think\")","aceaa6bf":"# Converting to lowercase\ndf.text = df.text.apply(str.lower)\ndf.headlines = df.headlines.apply(str.lower)\n\ndf.sample(5)","6759ee30":"df.headlines = df.headlines.apply(expand_contractions)\ndf.text = df.text.apply(expand_contractions)\ndf.sample(5)","d5d6b75a":"# Remove puncuation from word\ndef rm_punc_from_word(word):\n    clean_alphabet_list = [\n        alphabet for alphabet in word if alphabet not in string.punctuation\n    ]\n    return ''.join(clean_alphabet_list)\n\nprint(rm_punc_from_word('#cool!'))\n\n\n# Remove puncuation from text\ndef rm_punc_from_text(text):\n    clean_word_list = [rm_punc_from_word(word) for word in text]\n    return ''.join(clean_word_list)\n\nprint(rm_punc_from_text(\"Frankly, my dear, I don't give a damn\"))","532d9697":"# Remove numbers from text\ndef rm_number_from_text(text):\n    text = re.sub('[0-9]+', '', text)\n    return ' '.join(text.split())  # to rm `extra` white space\n\nprint(rm_number_from_text('You are 100times more sexier than me'))\nprint(rm_number_from_text('If you taught yes then you are 10 times more delusional than me'))","ced605c2":"# Remove stopwords from text\ndef rm_stopwords_from_text(text):\n    _stopwords = stopwords.words('english')\n    text = text.split()\n    word_list = [word for word in text if word not in _stopwords]\n    return ' '.join(word_list)\n\nrm_stopwords_from_text(\"Love means never having to say you're sorry\")","4c72a3fa":"# Cleaning text\ndef clean_text(text):\n    text = text.lower()\n    text = rm_punc_from_text(text)\n    text = rm_number_from_text(text)\n    text = rm_stopwords_from_text(text)\n\n    # there are hyphen(\u2013) in many titles, so replacing it with empty str\n    # this hyphen(\u2013) is different from normal hyphen(-)\n    text = re.sub('\u2013', '', text)\n    text = ' '.join(text.split())  # removing `extra` white spaces\n\n    # Removing unnecessary characters from text\n    text = re.sub(\"(\\\\t)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\\\r)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\\\n)\", ' ', str(text)).lower()\n\n    # remove accented chars ('S\u00f3m\u011b \u00c1cc\u011bnt\u011bd t\u011bxt' => 'Some Accented text')\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode(\n        'utf-8', 'ignore'\n    )\n\n    text = re.sub(\"(__+)\", ' ', str(text)).lower()\n    text = re.sub(\"(--+)\", ' ', str(text)).lower()\n    text = re.sub(\"(~~+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\+\\++)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\.\\.+)\", ' ', str(text)).lower()\n\n    text = re.sub(r\"[<>()|&\u00a9\u00f8\\[\\]\\'\\\",;?~*!]\", ' ', str(text)).lower()\n\n    text = re.sub(\"(mailto:)\", ' ', str(text)).lower()\n    text = re.sub(r\"(\\\\x9\\d)\", ' ', str(text)).lower()\n    text = re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(text)).lower()\n    text = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM',\n                  str(text)).lower()\n\n    text = re.sub(\"(\\.\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\-\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\:\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\s+.\\s+)\", ' ', str(text)).lower()\n\n    try:\n        url = re.search(r'((https*:\\\/*)([^\\\/\\s]+))(.[^\\s]+)', str(text))\n        repl_url = url.group(3)\n        text = re.sub(r'((https*:\\\/*)([^\\\/\\s]+))(.[^\\s]+)', repl_url, str(text))\n    except Exception as e:\n        pass\n\n    text = re.sub(\"(\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\s+.\\s+)\", ' ', str(text)).lower()\n\n    return text\n\nclean_text(\"Mrs. Robinson, you're trying to seduce me, aren't you?\")","53020680":"df.text = df.text.apply(clean_text)\ndf.headlines = df.headlines.apply(clean_text)\ndf.sample(5)","2e7b2319":"# saving the cleaned data\ndf.to_csv('cleaned_data.csv')","b022fc6d":"# To customize colours of wordcloud texts\ndef wc_blue_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n    return \"hsl(214, 67%%, %d%%)\" % randint(60, 100)\n\n\n# stopwords for wordcloud\ndef get_wc_stopwords():\n    wc_stopwords = set(STOPWORDS)\n\n    # Adding words to stopwords\n    # these words showed up while plotting wordcloud for text\n    wc_stopwords.add('s')\n    wc_stopwords.add('one')\n    wc_stopwords.add('using')\n    wc_stopwords.add('example')\n    wc_stopwords.add('work')\n    wc_stopwords.add('use')\n    wc_stopwords.add('make')\n\n    return wc_stopwords\n\n\n# plot wordcloud\ndef plot_wordcloud(text, color_func):\n    wc_stopwords = get_wc_stopwords()\n    wc = WordCloud(stopwords=wc_stopwords, width=1200, height=600, random_state=0).generate(text)\n\n    f, axs = plt.subplots(figsize=(20, 10))\n    with sns.axes_style(\"ticks\"):\n        sns.despine(offset=10, trim=True)\n        plt.imshow(wc.recolor(color_func=color_func, random_state=0), interpolation=\"bilinear\")\n        plt.xlabel('WordCloud')","fc6a84b5":"plot_wordcloud(' '.join(df.headlines.values.tolist()), wc_blue_color_func)","15cf96c6":"plot_wordcloud(' '.join(df.text.values.tolist()), wc_blue_color_func)","069d79d7":"df.headlines = df.headlines.apply(lambda x: f'_START_ {x} _END_')","520b516f":"start_token = 'sostok'\nend_token = 'eostok'\ndf.headlines = df.headlines.apply(lambda x: f'{start_token} {x} {end_token}')","3374e8da":"df.sample(5)","d7174bec":"text_count = [len(sentence.split()) for sentence in df.text]\nheadlines_count = [len(sentence.split()) for sentence in df.headlines]\n\npd.DataFrame({'text': text_count, 'headlines': headlines_count}).hist(bins=100, figsize=(16, 4), range=[0, 50])\nplt.show()","fa33027f":"# To check how many rows in a column has length (of the text) <= limit\ndef get_word_percent(column, limit):\n    count = 0\n    for sentence in column:\n        if len(sentence.split()) <= limit:\n            count += 1\n\n    return round(count \/ len(column), 2)\n\n\n# Check how many % of headlines have 0-13 words\nprint(get_word_percent(df.headlines, 13))\n\n# Check how many % of summary have 0-42 words\nprint(get_word_percent(df.text, 42))","f8a8e268":"max_text_len = 42\nmax_summary_len = 13","a7bb0cca":"# select the summary and text between their defined max lens respectively\ndef trim_text_and_summary(df, max_text_len, max_summary_len):\n    cleaned_text = np.array(df['text'])\n    cleaned_summary = np.array(df['headlines'])\n\n    short_text = []\n    short_summary = []\n\n    for i in range(len(cleaned_text)):\n        if len(cleaned_text[i].split()) <= max_text_len and len(\n            cleaned_summary[i].split()\n        ) <= max_summary_len:\n            short_text.append(cleaned_text[i])\n            short_summary.append(cleaned_summary[i])\n\n    df = pd.DataFrame({'text': short_text, 'summary': short_summary})\n    return df\n\n\ndf = trim_text_and_summary(df, max_text_len, max_summary_len)\nprint(f'Dataset size: {len(df)}')\ndf.sample(5)","06a9df43":"# rare word analysis\ndef get_rare_word_percent(tokenizer, threshold):\n    # threshold: if the word's occurrence is less than this then it's rare word\n\n    count = 0\n    total_count = 0\n    frequency = 0\n    total_frequency = 0\n\n    for key, value in tokenizer.word_counts.items():\n        total_count += 1\n        total_frequency += value\n        if value < threshold:\n            count += 1\n            frequency += value\n\n    return {\n        'percent': round((count \/ total_count) * 100, 2),\n        'total_coverage': round(frequency \/ total_frequency * 100, 2),\n        'count': count,\n        'total_count': total_count\n    }","710b0779":"# Splitting the training and validation sets\nx_train, x_val, y_train, y_val = train_test_split(\n    np.array(df['text']),\n    np.array(df['summary']),\n    test_size=0.1,\n    random_state=1,\n    shuffle=True\n)","6a50f165":"x_tokenizer = Tokenizer()\nx_tokenizer.fit_on_texts(list(x_train))\n\nx_tokens_data = get_rare_word_percent(x_tokenizer, 4)\nprint(x_tokens_data)","17ef1d8d":"# else use this\nx_tokenizer = Tokenizer()\nx_tokenizer.fit_on_texts(list(x_train))","66e96c56":"# save tokenizer\nwith open('x_tokenizer', 'wb') as f:\n    pickle.dump(x_tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)","8c1578b9":"# one-hot-encoding\nx_train_sequence = x_tokenizer.texts_to_sequences(x_train)\nx_val_sequence = x_tokenizer.texts_to_sequences(x_val)\n\n# padding upto max_text_len\nx_train_padded = pad_sequences(x_train_sequence, maxlen=max_text_len, padding='post')\nx_val_padded = pad_sequences(x_val_sequence, maxlen=max_text_len, padding='post')\n\n# if you're not using num_words parameter in Tokenizer then use this\nx_vocab_size = len(x_tokenizer.word_index) + 1\n\n# else use this\n# x_vocab_size = x_tokenizer.num_words + 1\n\nprint(x_vocab_size)","08a845f2":"y_tokenizer = Tokenizer()\ny_tokenizer.fit_on_texts(list(y_train))\n\ny_tokens_data = get_rare_word_percent(y_tokenizer, 6)\nprint(y_tokens_data)","f49abf80":"# else use this\ny_tokenizer = Tokenizer()\ny_tokenizer.fit_on_texts(list(y_train))","62ed1e7d":"# save tokenizer\nwith open('y_tokenizer', 'wb') as f:\n    pickle.dump(y_tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)","14e03340":"# one-hot-encoding\ny_train_sequence = y_tokenizer.texts_to_sequences(y_train)\ny_val_sequence = y_tokenizer.texts_to_sequences(y_val)\n\n# padding upto max_summary_len\ny_train_padded = pad_sequences(y_train_sequence, maxlen=max_summary_len, padding='post')\ny_val_padded = pad_sequences(y_val_sequence, maxlen=max_summary_len, padding='post')\n\n# if you're not using num_words parameter in Tokenizer then use this\ny_vocab_size = len(y_tokenizer.word_index) + 1\n\n# else use this\n# y_vocab_size = y_tokenizer.num_words + 1\n\nprint(y_vocab_size)","c76c1861":"# removing summary which only has sostok & eostok\ndef remove_indexes(summary_array):\n    remove_indexes = []\n    for i in range(len(summary_array)):\n        count = 0\n        for j in summary_array[i]:\n            if j != 0:\n                count += 1\n        if count == 2:\n            remove_indexes.append(i)\n    return remove_indexes\n\n\nremove_train_indexes = remove_indexes(y_train_padded)\nremove_val_indexes = remove_indexes(y_val_padded)\n\ny_train_padded = np.delete(y_train_padded, remove_train_indexes, axis=0)\nx_train_padded = np.delete(x_train_padded, remove_train_indexes, axis=0)\n\ny_val_padded = np.delete(y_val_padded, remove_val_indexes, axis=0)\nx_val_padded = np.delete(x_val_padded, remove_val_indexes, axis=0)","eb020c9b":"latent_dim = 240\nembedding_dim = 300\nnum_epochs = 50","f5c46898":"def get_embedding_matrix(tokenizer, embedding_dim, vocab_size=None):\n    word_index = tokenizer.word_index\n    voc = list(word_index.keys())\n\n    path_to_glove_file = '..\/input\/glove6b\/glove.6B.300d.txt'\n\n    embeddings_index = {}\n    with open(path_to_glove_file) as f:\n        for line in f:\n            word, coefs = line.split(maxsplit=1)\n            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n            embeddings_index[word] = coefs\n\n    print(\"Found %s word vectors.\" % len(embeddings_index))\n\n    num_tokens = len(voc) + 2 if not vocab_size else vocab_size\n    hits = 0\n    misses = 0\n\n    # Prepare embedding matrix\n    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # Words not found in embedding index will be all-zeros.\n            # This includes the representation for \"padding\" and \"OOV\"\n            embedding_matrix[i] = embedding_vector\n            hits += 1\n        else:\n            misses += 1\n    print(\"Converted %d words (%d misses)\" % (hits, misses))\n\n    return embedding_matrix\n\n\nx_embedding_matrix = get_embedding_matrix(x_tokenizer, embedding_dim, x_vocab_size)\ny_embedding_matrix = get_embedding_matrix(y_tokenizer, embedding_dim, y_vocab_size)","c1a83441":"print(x_embedding_matrix.shape)\nprint(y_embedding_matrix.shape)","bcce4bf5":"def build_seq2seq_model_with_just_lstm(\n    embedding_dim, latent_dim, max_text_len, \n    x_vocab_size, y_vocab_size,\n    x_embedding_matrix, y_embedding_matrix\n):\n    # instantiating the model in the strategy scope creates the model on the TPU\n    with tpu_strategy.scope():\n\n        # =====================\n        # \ud83d\udd25 Encoder\n        # =====================\n        encoder_input = Input(shape=(max_text_len, ))\n\n        # encoder embedding layer\n        encoder_embedding = Embedding(\n            x_vocab_size,\n            embedding_dim,\n            embeddings_initializer=tf.keras.initializers.Constant(x_embedding_matrix),\n            trainable=False\n        )(encoder_input)\n\n        # encoder lstm 1\n        encoder_lstm1 = LSTM(\n            latent_dim,\n            return_sequences=True,\n            return_state=True,\n            dropout=0.4,\n            recurrent_dropout=0.4\n        )\n        encoder_output1, state_h1, state_c1 = encoder_lstm1(encoder_embedding)\n\n        # encoder lstm 2\n        encoder_lstm2 = LSTM(\n            latent_dim,\n            return_sequences=True,\n            return_state=True,\n            dropout=0.4,\n            recurrent_dropout=0.4\n        )\n        encoder_output, *encoder_final_states = encoder_lstm2(encoder_output1)\n\n        # =====================\n        # \ud83c\udf08 Decoder\n        # =====================\n\n        # Set up the decoder, using `encoder_states` as initial state.\n\n        decoder_input = Input(shape=(None, ))\n\n        # decoder embedding layer\n        decoder_embedding_layer = Embedding(\n            y_vocab_size,\n            embedding_dim,\n            embeddings_initializer=tf.keras.initializers.Constant(y_embedding_matrix),\n            trainable=True\n        )\n        decoder_embedding = decoder_embedding_layer(decoder_input)\n\n        # decoder lstm 1\n        decoder_lstm = LSTM(\n            latent_dim,\n            return_sequences=True,\n            return_state=True,\n            dropout=0.4,\n            recurrent_dropout=0.4\n        )\n        decoder_output, *decoder_final_states = decoder_lstm(\n            decoder_embedding, initial_state=encoder_final_states\n        )\n\n        # dense layer\n        decoder_dense = TimeDistributed(\n            Dense(y_vocab_size, activation='softmax')\n        )\n        decoder_output = decoder_dense(decoder_output)\n\n        # =====================\n        # \u26a1\ufe0f Model\n        # =====================\n        model = Model([encoder_input, decoder_input], decoder_output)\n        model.summary()\n\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return {\n            'model': model,\n            'inputs': {\n                'encoder': encoder_input,\n                'decoder': decoder_input\n            },\n            'outputs': {\n                'encoder': encoder_output,\n                'decoder': decoder_output\n            },\n            'states': {\n                'encoder': encoder_final_states,\n                'decoder': decoder_final_states\n            },\n            'layers': {\n                'decoder': {\n                    'embedding': decoder_embedding_layer,\n                    'last_decoder_lstm': decoder_lstm,\n                    'dense': decoder_dense\n                }\n            }\n        }","960326d9":"def build_seq2seq_model_with_bidirectional_lstm(\n    embedding_dim, latent_dim, max_text_len, \n    x_vocab_size, y_vocab_size,\n    x_embedding_matrix, y_embedding_matrix\n):\n    # instantiating the model in the strategy scope creates the model on the TPU\n    with tpu_strategy.scope():\n\n        # =====================\n        # \ud83d\udd25 Encoder\n        # =====================\n        encoder_input = Input(shape=(max_text_len, ))\n\n        # encoder embedding layer\n        encoder_embedding = Embedding(\n            x_vocab_size,\n            embedding_dim,\n            embeddings_initializer=tf.keras.initializers.Constant(x_embedding_matrix),\n            trainable=False,\n            name='encoder_embedding'\n        )(encoder_input)\n\n        # encoder lstm1\n        encoder_bi_lstm1 = Bidirectional(\n            LSTM(\n                latent_dim,\n                return_sequences=True,\n                return_state=True,\n                dropout=0.4,\n                recurrent_dropout=0.4,\n                name='encoder_lstm_1'\n            ),\n            name='encoder_bidirectional_lstm_1'\n        )\n        encoder_output1, forward_h1, forward_c1, backward_h1, backward_c1 = encoder_bi_lstm1(\n            encoder_embedding\n        )\n        encoder_bi_lstm1_output = [\n            encoder_output1, forward_h1, forward_c1, backward_h1, backward_c1\n        ]\n\n        # encoder lstm 2\n        encoder_bi_lstm2 = Bidirectional(\n            LSTM(\n                latent_dim,\n                return_sequences=True,\n                return_state=True,\n                dropout=0.4,\n                recurrent_dropout=0.4,\n                name='encoder_lstm_2'\n            ),\n            name='encoder_bidirectional_lstm_2'\n        )\n        encoder_output2, forward_h2, forward_c2, backward_h2, backward_c2 = encoder_bi_lstm2(\n            encoder_output1\n        )\n        encoder_bi_lstm2_output = [\n            encoder_output2, forward_h2, forward_c2, backward_h2, backward_c2\n        ]\n\n        # encoder lstm 3\n        encoder_bi_lstm = Bidirectional(\n            LSTM(\n                latent_dim,\n                return_sequences=True,\n                return_state=True,\n                dropout=0.4,\n                recurrent_dropout=0.4,\n                name='encoder_lstm_3'\n            ),\n            name='encoder_bidirectional_lstm_3'\n        )\n        encoder_output, *encoder_final_states = encoder_bi_lstm(encoder_output2)\n\n        # =====================\n        # \ud83c\udf08 Decoder\n        # =====================\n\n        # Set up the decoder, using `encoder_states` as initial state.\n\n        decoder_input = Input(shape=(None, ))\n\n        # decoder embedding layer\n        decoder_embedding_layer = Embedding(\n            y_vocab_size,\n            embedding_dim,\n            embeddings_initializer=tf.keras.initializers.Constant(y_embedding_matrix),\n            trainable=False,\n            name='decoder_embedding'\n        )\n        decoder_embedding = decoder_embedding_layer(decoder_input)\n        \n        decoder_bi_lstm = Bidirectional(\n            LSTM(\n                latent_dim,\n                return_sequences=True,\n                return_state=True,\n                dropout=0.4,\n                recurrent_dropout=0.2,\n                name='decoder_lstm_1'\n            ),\n            name='decoder_bidirectional_lstm_1'\n        )\n        decoder_output, *decoder_final_states = decoder_bi_lstm(\n            decoder_embedding, initial_state=encoder_final_states\n            # decoder_embedding, initial_state=encoder_final_states[:2]\n        )  # taking only the forward states\n\n        # dense layer\n        decoder_dense = TimeDistributed(\n            Dense(y_vocab_size, activation='softmax')\n        )\n        decoder_output = decoder_dense(decoder_output)\n\n        # =====================\n        # \u26a1\ufe0f Model\n        # =====================\n        model = Model([encoder_input, decoder_input], decoder_output, name='seq2seq_model_with_bidirectional_lstm')\n        model.summary()\n\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return {\n            'model': model,\n            'inputs': {\n                'encoder': encoder_input,\n                'decoder': decoder_input\n            },\n            'outputs': {\n                'encoder': encoder_output,\n                'decoder': decoder_output\n            },\n            'states': {\n                'encoder': encoder_final_states,\n                'decoder': decoder_final_states\n            },\n            'layers': {\n                'decoder': {\n                    'embedding': decoder_embedding_layer,\n                    'last_decoder_lstm': decoder_bi_lstm,\n                    'dense': decoder_dense\n                }\n            }\n        }","3416b2ea":"def build_hybrid_seq2seq_model(\n    embedding_dim, latent_dim, max_text_len, \n    x_vocab_size, y_vocab_size,\n    x_embedding_matrix, y_embedding_matrix\n):\n    # instantiating the model in the strategy scope creates the model on the TPU\n    with tpu_strategy.scope():\n\n        # =====================\n        # \ud83d\udd25 Encoder\n        # =====================\n        encoder_input = Input(shape=(max_text_len, ))\n\n        # encoder embedding layer\n        encoder_embedding = Embedding(\n            x_vocab_size,\n            embedding_dim,\n            embeddings_initializer=tf.keras.initializers.Constant(x_embedding_matrix),\n            trainable=False,\n            name='encoder_embedding'\n        )(encoder_input)\n\n        # encoder lstm1\n        encoder_bi_lstm1 = Bidirectional(\n            LSTM(\n                latent_dim,\n                return_sequences=True,\n                return_state=True,\n                dropout=0.4,\n                recurrent_dropout=0.4,\n                name='encoder_lstm_1'\n            ),\n            name='encoder_bidirectional_lstm_1'\n        )\n        encoder_output1, forward_h1, forward_c1, backward_h1, backward_c1 = encoder_bi_lstm1(\n            encoder_embedding\n        )\n        encoder_bi_lstm1_output = [\n            encoder_output1, forward_h1, forward_c1, backward_h1, backward_c1\n        ]\n\n        # encoder lstm 2\n        encoder_bi_lstm2 = Bidirectional(\n            LSTM(\n                latent_dim,\n                return_sequences=True,\n                return_state=True,\n                dropout=0.4,\n                recurrent_dropout=0.4,\n                name='encoder_lstm_2'\n            ),\n            name='encoder_bidirectional_lstm_2'\n        )\n        encoder_output2, forward_h2, forward_c2, backward_h2, backward_c2 = encoder_bi_lstm2(\n            encoder_output1\n        )\n        encoder_bi_lstm2_output = [\n            encoder_output2, forward_h2, forward_c2, backward_h2, backward_c2\n        ]\n\n        # encoder lstm 3\n        encoder_bi_lstm = Bidirectional(\n            LSTM(\n                latent_dim,\n                return_sequences=True,\n                return_state=True,\n                dropout=0.4,\n                recurrent_dropout=0.4,\n                name='encoder_lstm_3'\n            ),\n            name='encoder_bidirectional_lstm_3'\n        )\n        encoder_output, *encoder_final_states = encoder_bi_lstm(encoder_output2)\n\n        # =====================\n        # \ud83c\udf08 Decoder\n        # =====================\n\n        # Set up the decoder, using `encoder_states` as initial state.\n\n        decoder_input = Input(shape=(None, ))\n\n        # decoder embedding layer\n        decoder_embedding_layer = Embedding(\n            y_vocab_size,\n            embedding_dim,\n            embeddings_initializer=tf.keras.initializers.Constant(y_embedding_matrix),\n            trainable=False,\n            name='decoder_embedding'\n        )\n        decoder_embedding = decoder_embedding_layer(decoder_input)\n        \n        decoder_lstm = LSTM(\n            latent_dim,\n            return_sequences=True,\n            return_state=True,\n            dropout=0.4,\n            recurrent_dropout=0.2,\n            name='decoder_lstm_1'\n        )\n        decoder_output, *decoder_final_states = decoder_lstm(\n            decoder_embedding, initial_state=encoder_final_states[:2]\n        )  # taking only the forward states\n\n        # dense layer\n        decoder_dense = TimeDistributed(\n            Dense(y_vocab_size, activation='softmax')\n        )\n        decoder_output = decoder_dense(decoder_output)\n\n        # =====================\n        # \u26a1\ufe0f Model\n        # =====================\n        model = Model([encoder_input, decoder_input], decoder_output, name='seq2seq_model_with_bidirectional_lstm')\n        model.summary()\n\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return {\n            'model': model,\n            'inputs': {\n                'encoder': encoder_input,\n                'decoder': decoder_input\n            },\n            'outputs': {\n                'encoder': encoder_output,\n                'decoder': decoder_output\n            },\n            'states': {\n                'encoder': encoder_final_states,\n                'decoder': decoder_final_states\n            },\n            'layers': {\n                'decoder': {\n                    'embedding': decoder_embedding_layer,\n                    'last_decoder_lstm': decoder_lstm,\n                    'dense': decoder_dense\n                }\n            }\n        }","a80ed1e7":"seq2seq = build_seq2seq_model_with_just_lstm(\n    embedding_dim, latent_dim, max_text_len, \n    x_vocab_size, y_vocab_size,\n    x_embedding_matrix, y_embedding_matrix\n)","77f07c99":"model = seq2seq['model']\n\nencoder_input = seq2seq['inputs']['encoder']\ndecoder_input = seq2seq['inputs']['decoder']\n\nencoder_output = seq2seq['outputs']['encoder']\ndecoder_output = seq2seq['outputs']['decoder']\n\nencoder_final_states = seq2seq['states']['encoder']\ndecoder_final_states = seq2seq['states']['decoder']\n\ndecoder_embedding_layer = seq2seq['layers']['decoder']['embedding']\nlast_decoder_lstm = seq2seq['layers']['decoder']['last_decoder_lstm']\ndecoder_dense = seq2seq['layers']['decoder']['dense']","d8478b17":"model.layers[-2].input","42a9df0b":"callbacks = [\n    EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=0.000001, verbose=1),\n]","ac5edfed":"history = model.fit(\n    [x_train_padded, y_train_padded[:, :-1]],\n    y_train_padded.reshape(y_train_padded.shape[0], y_train_padded.shape[1], 1)[:, 1:],\n    epochs=num_epochs,\n    batch_size=128 * tpu_strategy.num_replicas_in_sync,\n    callbacks=callbacks,\n    validation_data=(\n        [x_val_padded, y_val_padded[:, :-1]],\n        y_val_padded.reshape(y_val_padded.shape[0], y_val_padded.shape[1], 1)[:, 1:]\n    )\n)","d840e3de":"# Accuracy\nplt.plot(history.history['accuracy'][1:], label='train acc')\nplt.plot(history.history['val_accuracy'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","464bf903":"# Loss\nplt.plot(history.history['loss'][1:], label='train loss')\nplt.plot(history.history['val_loss'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='lower right')","4b4319b0":"# Next, let\u2019s build the dictionary to convert the index to word for target and source vocabulary:\nreverse_target_word_index = y_tokenizer.index_word\nreverse_source_word_index = x_tokenizer.index_word\ntarget_word_index = y_tokenizer.word_index","a54791de":"def build_seq2seq_model_with_just_lstm_inference(\n    max_text_len, latent_dim, encoder_input, encoder_output,\n    encoder_final_states, decoder_input, decoder_output,\n    decoder_embedding_layer, decoder_dense, last_decoder_lstm\n):\n    # Encode the input sequence to get the feature vector\n    encoder_model = Model(\n        inputs=encoder_input, outputs=[encoder_output] + encoder_final_states\n    )\n\n    # Decoder setup\n    # Below tensors will hold the states of the previous time step\n    decoder_state_input_h = Input(shape=(latent_dim, ))\n    decoder_state_input_c = Input(shape=(latent_dim, ))\n    decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim))\n\n    # Get the embeddings of the decoder sequence\n    decoder_embedding = decoder_embedding_layer(decoder_input)\n\n    # To predict the next word in the sequence, set the initial\n    # states to the states from the previous time step\n    decoder_output, *decoder_states = last_decoder_lstm(\n        decoder_embedding,\n        initial_state=[decoder_state_input_h, decoder_state_input_c]\n    )\n\n    # A dense softmax layer to generate prob dist. over the target vocabulary\n    decoder_output = decoder_dense(decoder_output)\n\n    # Final decoder model\n    decoder_model = Model(\n        [decoder_input] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c], \n        [decoder_output] + decoder_states\n    )\n\n    return (encoder_model, decoder_model)","e34f1aed":"def build_seq2seq_model_with_bidirectional_lstm_inference(\n    max_text_len, latent_dim, encoder_input, encoder_output,\n    encoder_final_states, decoder_input, decoder_output,\n    decoder_embedding_layer, decoder_dense, last_decoder_bi_lstm\n):\n\n    # Encode the input sequence to get the feature vector\n    encoder_model = Model(\n        inputs=encoder_input, outputs=[encoder_output] + encoder_final_states\n    )\n\n    # Decoder setup\n    # Below tensors will hold the states of the previous time step\n    decoder_state_forward_input_h = Input(shape=(latent_dim, ))\n    decoder_state_forward_input_c = Input(shape=(latent_dim, ))\n    decoder_state_backward_input_h = Input(shape=(latent_dim, ))\n    decoder_state_backward_input_c = Input(shape=(latent_dim, ))\n\n    # Create the hidden input layer with twice the latent dimension,\n    # since we are using bi - directional LSTM's we will get \n    # two hidden states and two cell states\n    decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim * 2))\n\n    decoder_initial_state = [\n        decoder_state_forward_input_h, decoder_state_forward_input_c,\n        decoder_state_backward_input_h, decoder_state_backward_input_c\n    ]\n\n    # Get the embeddings of the decoder sequence\n    decoder_embedding = decoder_embedding_layer(decoder_input)\n\n    # To predict the next word in the sequence, set the initial\n    # states to the states from the previous time step\n    decoder_output, *decoder_states = last_decoder_bi_lstm(\n        decoder_embedding, initial_state=decoder_initial_state\n    )\n\n    # A dense softmax layer to generate prob dist. over the target vocabulary\n    decoder_output = decoder_dense(decoder_output)\n\n    # Final decoder model\n    decoder_model = Model(\n        [decoder_input] + [decoder_hidden_state_input] + decoder_initial_state,\n        [decoder_output] + decoder_states\n    )\n\n    return (encoder_model, decoder_model)","3d80e25f":"def build_hybrid_seq2seq_model_inference(\n    max_text_len, latent_dim, encoder_input, encoder_output,\n    encoder_final_states, decoder_input, decoder_output,\n    decoder_embedding_layer, decoder_dense, last_decoder_bi_lstm\n):\n\n    # Encode the input sequence to get the feature vector\n    encoder_model = Model(\n        inputs=encoder_input, outputs=[encoder_output] + encoder_final_states\n    )\n\n    # Decoder setup\n    # Below tensors will hold the states of the previous time step\n    decoder_state_forward_input_h = Input(shape=(latent_dim, ))\n    decoder_state_forward_input_c = Input(shape=(latent_dim, ))\n    # decoder_state_backward_input_h = Input(shape=(latent_dim, ))\n    # decoder_state_backward_input_c = Input(shape=(latent_dim, ))\n\n    # Create the hidden input layer with twice the latent dimension,\n    # since we are using bi - directional LSTM's we will get \n    # two hidden states and two cell states\n    decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim * 2))\n\n    decoder_initial_state = [\n        decoder_state_forward_input_h, decoder_state_forward_input_c,\n        #decoder_state_backward_input_h, decoder_state_backward_input_c\n    ]\n\n    # Get the embeddings of the decoder sequence\n    decoder_embedding = decoder_embedding_layer(decoder_input)\n\n    # To predict the next word in the sequence, set the initial\n    # states to the states from the previous time step\n    decoder_output, *decoder_states = last_decoder_bi_lstm(\n        decoder_embedding, initial_state=decoder_initial_state\n    )\n\n    # A dense softmax layer to generate prob dist. over the target vocabulary\n    decoder_output = decoder_dense(decoder_output)\n\n    # Final decoder model\n    decoder_model = Model(\n        [decoder_input] + [decoder_hidden_state_input] + decoder_initial_state,\n        [decoder_output] + decoder_states\n    )\n\n    return (encoder_model, decoder_model)","70a7ccac":"encoder_model, decoder_model = build_seq2seq_model_with_just_lstm_inference(\n    max_text_len, latent_dim, encoder_input, encoder_output,\n    encoder_final_states, decoder_input, decoder_output,\n    decoder_embedding_layer, decoder_dense, last_decoder_lstm\n)","ffbddf41":"encoder_model.summary()","199cc6de":"decoder_model.summary()","e7c79442":"decoder_model.layers[-3].input","78308d64":"def decode_sequence_seq2seq_model_with_just_lstm(\n    input_sequence, encoder_model, decoder_model\n):\n    # Encode the input as state vectors.\n    e_out, e_h, e_c = encoder_model.predict(input_sequence)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1))\n\n    # Populate the first word of target sequence with the start word.\n    target_seq[0, 0] = target_word_index[start_token]\n\n    stop_condition = False\n    decoded_sentence = ''\n\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + [e_out, e_h, e_c]\n        )\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = reverse_target_word_index[sampled_token_index]\n\n        if sampled_token != end_token:\n            decoded_sentence += ' ' + sampled_token\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == end_token) or (len(decoded_sentence.split()) >= (max_summary_len - 1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        e_h, e_c = h, c\n\n    return decoded_sentence","5404fea8":"def decode_sequence_seq2seq_model_with_bidirectional_lstm(\n    input_sequence, encoder_model, decoder_model\n):\n    # Encode the input as state vectors.\n    e_out, *state_values = encoder_model.predict(input_sequence)\n    \n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1))\n\n    # Populate the first word of target sequence with the start word.\n    target_seq[0, 0] = target_word_index[start_token]\n\n    stop_condition = False\n    decoded_sentence = ''\n    \n    while not stop_condition:\n        output_tokens, *decoder_states = decoder_model.predict(\n            [target_seq] + [e_out] + state_values\n        )\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :]) # Greedy Search\n        sampled_token = reverse_target_word_index[sampled_token_index + 1]\n        \n        if sampled_token != end_token:\n            decoded_sentence += ' ' + sampled_token\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == end_token) or (len(decoded_sentence.split()) >= (max_summary_len - 1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        state_values = decoder_states\n\n    return decoded_sentence","6b634361":"def decode_sequence_hybrid_seq2seq_model(\n    input_sequence, encoder_model, decoder_model\n):\n    # Encode the input as state vectors.\n    e_out, *state_values = encoder_model.predict(input_sequence)\n    \n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1))\n\n    # Populate the first word of target sequence with the start word.\n    target_seq[0, 0] = target_word_index[start_token]\n\n    stop_condition = False\n    decoded_sentence = ''\n    \n    while not stop_condition:\n        output_tokens, *decoder_states = decoder_model.predict(\n            [target_seq] + [e_out] + state_values[:2]\n        )\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :]) # Greedy Search\n        sampled_token = reverse_target_word_index[sampled_token_index + 1]\n        \n        if sampled_token != end_token:\n            decoded_sentence += ' ' + sampled_token\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == end_token) or (len(decoded_sentence.split()) >= (max_summary_len - 1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        state_values = decoder_states\n\n    return decoded_sentence","9a6e7ddc":"def seq2summary(input_sequence):\n    new_string = ''\n    for i in input_sequence:\n        if (\n            (i != 0 and i != target_word_index[start_token]) and\n            (i != target_word_index[end_token])\n        ):\n            new_string = new_string + reverse_target_word_index[i] + ' '\n    return new_string","d25695fa":"def seq2text(input_sequence):\n    new_string = ''\n    for i in input_sequence:\n        if i != 0:\n            new_string = new_string + reverse_source_word_index[i] + ' '\n    return new_string","7a0ca24d":"l = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nif len(l) % 3 != 0:\n    while len(l) % 3 != 0:\n        l.append(0)\nprint(l)\n\nlst_i = 3\nfor i in range(0, len(l), 3):\n    print(l[i:i + lst_i])\n\nprint(' '.join(['', 'james', 'ethan', '', 'tony']))\nprint(' '.join(' '.join(['', 'james', 'ethan', '', 'tony']).split()))","7f2dfcec":"def predict_text(text, decode_sequence, encoder_model, decoder_model):\n    original_text = text\n    text = clean_text([text])  # generator\n    text_list = original_text.split()\n\n    if len(text_list) <= max_text_len:\n        text = expand_contractions(text)\n        text = clean_text(text)\n        text = f'_START_ {text} _END_'\n        text = f'{start_token} {text} {end_token}'\n\n        seq = x_tokenizer.texts_to_sequences([' '.join(text_list)])\n        padded = pad_sequences(seq, maxlen=max_text_len, padding='post')\n        pred_summary = decode_sequence(\n            padded.reshape(1, max_text_len), encoder_model, decoder_model\n        )\n        return pred_summary\n    else:\n        pred_summary = ''\n\n        # breaking long texts to individual max_text_len texts and predicting on them\n        while len(text_list) % max_text_len == 0:\n            text_list.append('')\n\n        lst_i = max_text_len\n        for i in range(0, len(text_list), max_text_len):\n            _text_list = original_text.split()[i:i + lst_i]\n            _text = ' '.join(_text_list)\n            _text = ' '.join(\n                _text.split()\n            )  # to remove spaces that were added to make len(text_list) % max_text_len == 0\n\n            _text = expand_contractions(_text)\n            _text = clean_text(_text)  # generator\n            _text = f'_START_ {_text} _END_'\n            _text = f'{start_token} {_text} {end_token}'\n            # print(_text, '\\n')\n\n            _seq = x_tokenizer.texts_to_sequences([_text])\n            _padded = pad_sequences(_seq, maxlen=max_text_len, padding='post')\n            _pred = decode_sequence(\n                _padded.reshape(1, max_text_len), encoder_model, decoder_model\n            )\n            pred_summary += ' ' + ' '.join(_pred.split()[1:-2])\n            pred_summary = ' '.join(pred_summary.split())\n\n        return pred_summary","0c8ceb0f":"# Testing on training data\nfor i in range(0, 15):\n    print(f\"# {i+1} News: \", seq2text(x_train_padded[i]))\n    print(\"Original summary: \", seq2summary(y_train_padded[i]))\n    print(\n        \"Predicted summary: \",\n        decode_sequence_seq2seq_model_with_just_lstm(\n            x_train_padded[i].reshape(1, max_text_len), encoder_model,\n            decoder_model\n        )\n    )\n    print()","d627c65a":"# Testing on validation data\nfor i in range(0, 15):\n    print(f\"# {i+1} News: \", seq2text(x_val_padded[i]))\n    print(\"Original summary: \", seq2summary(y_val_padded[i]))\n    print(\n        \"Predicted summary: \",\n        decode_sequence_seq2seq_model_with_just_lstm(\n            x_val_padded[i].reshape(1, max_text_len), encoder_model,\n            decoder_model\n        )\n    )\n    print()","32def1d2":"# HDF5 format\nmodel.save('model.h5')    \nencoder_model.save('encoder_model.h5')\ndecoder_model.save('decoder_model.h5')","724e142e":"models_info = {\n    'just_lstm': {\n        'model': build_seq2seq_model_with_just_lstm,\n        'inference': build_seq2seq_model_with_just_lstm_inference,\n        'decode_sequence': decode_sequence_seq2seq_model_with_just_lstm\n    },\n    'bidirectional_lstm': {\n        'model': build_seq2seq_model_with_bidirectional_lstm,\n        'inference': build_seq2seq_model_with_bidirectional_lstm_inference,\n        'decode_sequence': decode_sequence_seq2seq_model_with_bidirectional_lstm\n    },\n    'hybrid_model': {\n        'model': build_hybrid_seq2seq_model,\n        'inference': build_hybrid_seq2seq_model_inference,\n        'decode_sequence': decode_sequence_hybrid_seq2seq_model\n    }\n}","4c1c2e95":"model_func = models_info['just_lstm']['model']\ninference_func = models_info['just_lstm']['inference']\ndecode_sequence_func = models_info['just_lstm']['decode_sequence']","650e2d9c":"seq2seq = model_func(\n    embedding_dim, latent_dim, max_text_len, \n    x_vocab_size, y_vocab_size,\n    x_embedding_matrix, y_embedding_matrix\n)\n\nmodel = seq2seq['model']\n\nencoder_input = seq2seq['inputs']['encoder']\ndecoder_input = seq2seq['inputs']['decoder']\n\nencoder_output = seq2seq['outputs']['encoder']\ndecoder_output = seq2seq['outputs']['decoder']\n\nencoder_final_states = seq2seq['states']['encoder']\ndecoder_final_states = seq2seq['states']['decoder']\n\ndecoder_embedding_layer = seq2seq['layers']['decoder']['embedding']\nlast_decoder_lstm = seq2seq['layers']['decoder']['last_decoder_lstm']\ndecoder_dense = seq2seq['layers']['decoder']['dense']\n\nmodel.summary()","3862909f":"history = model.fit(\n    [x_train_padded, y_train_padded[:, :-1]],\n    y_train_padded.reshape(y_train_padded.shape[0], y_train_padded.shape[1], 1)[:, 1:],\n    epochs=num_epochs,\n    batch_size=128 * tpu_strategy.num_replicas_in_sync,\n    callbacks=callbacks,\n    validation_data=(\n        [x_val_padded, y_val_padded[:, :-1]],\n        y_val_padded.reshape(y_val_padded.shape[0], y_val_padded.shape[1], 1)[:, 1:]\n    )\n)","137b3eeb":"# Accuracy\nplt.plot(history.history['accuracy'][1:], label='train acc')\nplt.plot(history.history['val_accuracy'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","f18a2975":"# Loss\nplt.plot(history.history['loss'][1:], label='train loss')\nplt.plot(history.history['val_loss'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='lower right')","d1010a7f":"# Inference\nencoder_model, decoder_model = inference_func(\n    max_text_len, latent_dim, encoder_input, encoder_output,\n    encoder_final_states, decoder_input, decoder_output,\n    decoder_embedding_layer, decoder_dense, last_decoder_lstm\n)","a88ea7a7":"encoder_model.summary()","c00c6e92":"decoder_model.summary()","639f3259":"# Testing on training data\nfor i in range(0, 10):\n    print(f\"# {i+1} News: \", seq2text(x_train_padded[i]))\n    print(\"Original summary: \", seq2summary(y_train_padded[i]))\n    print(\n        \"Predicted summary: \",\n        decode_sequence_func(\n            x_train_padded[i].reshape(1, max_text_len), encoder_model,\n            decoder_model\n        )\n    )\n    print()","7498c7f4":"model_func = models_info['bidirectional_lstm']['model']\ninference_func = models_info['bidirectional_lstm']['inference']\ndecode_sequence_func = models_info['bidirectional_lstm']['decode_sequence']","aa1d5fe4":"seq2seq = model_func(\n    embedding_dim, latent_dim, max_text_len, \n    x_vocab_size, y_vocab_size,\n    x_embedding_matrix, y_embedding_matrix\n)\n\nmodel = seq2seq['model']\n\nencoder_input = seq2seq['inputs']['encoder']\ndecoder_input = seq2seq['inputs']['decoder']\n\nencoder_output = seq2seq['outputs']['encoder']\ndecoder_output = seq2seq['outputs']['decoder']\n\nencoder_final_states = seq2seq['states']['encoder']\ndecoder_final_states = seq2seq['states']['decoder']\n\ndecoder_embedding_layer = seq2seq['layers']['decoder']['embedding']\nlast_decoder_lstm = seq2seq['layers']['decoder']['last_decoder_lstm']\ndecoder_dense = seq2seq['layers']['decoder']['dense']\n\nmodel.summary()","ba92f512":"history = model.fit(\n    [x_train_padded, y_train_padded[:, :-1]],\n    y_train_padded.reshape(y_train_padded.shape[0], y_train_padded.shape[1], 1)[:, 1:],\n    epochs=num_epochs,\n    batch_size=128 * tpu_strategy.num_replicas_in_sync,\n    callbacks=callbacks,\n    validation_data=(\n        [x_val_padded, y_val_padded[:, :-1]],\n        y_val_padded.reshape(y_val_padded.shape[0], y_val_padded.shape[1], 1)[:, 1:]\n    )\n)","ce4f43fe":"# Accuracy\nplt.plot(history.history['accuracy'][1:], label='train acc')\nplt.plot(history.history['val_accuracy'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","47bb89e5":"# Loss\nplt.plot(history.history['loss'][1:], label='train loss')\nplt.plot(history.history['val_loss'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='lower right')","7edb248d":"# Inference\nencoder_model, decoder_model = inference_func(\n    max_text_len, latent_dim, encoder_input, encoder_output,\n    encoder_final_states, decoder_input, decoder_output,\n    decoder_embedding_layer, decoder_dense, last_decoder_lstm\n)","2d9f3860":"encoder_model.summary()","5347a702":"decoder_model.summary()","6cd296f3":"# Testing on training data\nfor i in range(0, 10):\n    print(f\"# {i+1} News: \", seq2text(x_train_padded[i]))\n    print(\"Original summary: \", seq2summary(y_train_padded[i]))\n    print(\n        \"Predicted summary: \",\n        decode_sequence_func(\n            x_train_padded[i].reshape(1, max_text_len), encoder_model,\n            decoder_model\n        )\n    )\n    print()","7cba8ef1":"model_func = models_info['hybrid_model']['model']\ninference_func = models_info['hybrid_model']['inference']\ndecode_sequence_func = models_info['hybrid_model']['decode_sequence']","5692fad6":"seq2seq = model_func(\n    embedding_dim, latent_dim, max_text_len, \n    x_vocab_size, y_vocab_size,\n    x_embedding_matrix, y_embedding_matrix\n)\n\nmodel = seq2seq['model']\n\nencoder_input = seq2seq['inputs']['encoder']\ndecoder_input = seq2seq['inputs']['decoder']\n\nencoder_output = seq2seq['outputs']['encoder']\ndecoder_output = seq2seq['outputs']['decoder']\n\nencoder_final_states = seq2seq['states']['encoder']\ndecoder_final_states = seq2seq['states']['decoder']\n\ndecoder_embedding_layer = seq2seq['layers']['decoder']['embedding']\nlast_decoder_lstm = seq2seq['layers']['decoder']['last_decoder_lstm']\ndecoder_dense = seq2seq['layers']['decoder']['dense']\n\nmodel.summary()","61ea8124":"history = model.fit(\n    [x_train_padded, y_train_padded[:, :-1]],\n    y_train_padded.reshape(y_train_padded.shape[0], y_train_padded.shape[1], 1)[:, 1:],\n    epochs=num_epochs,\n    batch_size=128 * tpu_strategy.num_replicas_in_sync,\n    callbacks=callbacks,\n    validation_data=(\n        [x_val_padded, y_val_padded[:, :-1]],\n        y_val_padded.reshape(y_val_padded.shape[0], y_val_padded.shape[1], 1)[:, 1:]\n    )\n)","f11c0a89":"# Accuracy\nplt.plot(history.history['accuracy'][1:], label='train acc')\nplt.plot(history.history['val_accuracy'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","19f22bbe":"# Loss\nplt.plot(history.history['loss'][1:], label='train loss')\nplt.plot(history.history['val_loss'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='lower right')","25dee514":"# Inference\nencoder_model, decoder_model = inference_func(\n    max_text_len, latent_dim, encoder_input, encoder_output,\n    encoder_final_states, decoder_input, decoder_output,\n    decoder_embedding_layer, decoder_dense, last_decoder_lstm\n)","5e7c16c8":"encoder_model.summary()","42fd691a":"decoder_model.summary()","9b418f84":"# Testing on training data\nfor i in range(0, 10):\n    print(f\"# {i+1} News: \", seq2text(x_train_padded[i]))\n    print(\"Original summary: \", seq2summary(y_train_padded[i]))\n    print(\n        \"Predicted summary: \",\n        decode_sequence_func(\n            x_train_padded[i].reshape(1, max_text_len), encoder_model,\n            decoder_model\n        )\n    )\n    print()","0f40dcec":"## \ud83e\udd38\u200d\u2642\ufe0f Inference\n\n![](https:\/\/media.giphy.com\/media\/5FJcVeYFVysda\/giphy.gif)","281cbed6":"If you want to change `model` then just change the `function name` above.","1a52844e":"Use a `tuple` instead of `list` in `validation_parameter` in `model.fit()`, to know the reason reading this [post](https:\/\/stackoverflow.com\/questions\/61586981\/valueerror-layer-sequential-20-expects-1-inputs-but-it-received-2-input-tensor).","8c98129f":"**Tokenizing text \ud83d\udc49 x**","e19ab031":"For predicting `unseen` data pass `decode_sequence` function for which you want to decode.","661278de":"Useful `stackoverflow` [post](https:\/\/stackoverflow.com\/questions\/60697843\/tensorflow-keras-bidirectional-lstm-for-text-summarization) to understand `inference` process when using `bidirectional lstms` in `encoder` and `decoder` in the training model.","a53bb117":"Converting from `sequence to text` for model `with just LSTM's` and for model `with Bidirectional LSTM's`.","1272a913":"### Model with hybrid architecture","d3ecd6a5":"![](https:\/\/media.giphy.com\/media\/3o72EUwmrRFtyT1Vhm\/giphy.gif)","17ee21a9":"If the length of headlines or the text is kept large the deep learning model will face issues with performance and also training will slower.\n\nOne solution for creating summary for long sentences can be break a paragraph into sentences and then create a summary for them, this way the summary will make sence instead of giving random piece of text and creating summary for it.","5363ef0e":"## \ud83e\udd3c\u200d\u2642\ufe0f Modelling\n\n![](https:\/\/media.giphy.com\/media\/YTJXDIivNMPuNSMgc0\/giphy.gif)","f9a087da":"---\n\nI'll wrap things up there. If you want to find some other answers then go ahead `edit` this kernel. If you have any `questions` then do let me know.\n\nIf this kernel helped you then don't forget to \ud83d\udd3c `upvote` and share your \ud83c\udf99 `feedback` on improvements of the kernel.\n\n![](https:\/\/media.giphy.com\/media\/3o85xAYQLOhSrmINHO\/giphy.gif)\n\n---","d8cdc17f":"**Seq2Seq model with just LSTMs**. Both `encoder` and `decoder` have just `LSTM`s.","717a8c52":"## \ud83e\ude82 Getting the data","4fd4db06":"## \ud83c\udf81 Saving the model","f0eb2858":"![](https:\/\/media.giphy.com\/media\/XKSa6XxpmHh1NEBvvl\/giphy.gif)","e7030384":"Using a `start` and `end` tokens in `headlines(summary)` to let the learning algorithm know from where the headlines start's and end's.","1e835225":"The `headlines` column will be treated as `summary` for the text.","174984b4":"**Model with Bidirectional LSTMs**","e450ab84":"\ud83d\udd25 to `increase computation speed` use this\n\n```python\ny_tokenizer = Tokenizer(num_words=y_tokens_data['total_count'] - y_tokens_data['count'])\n```","799069b1":"\ud83d\udd25 to `increase computation speed` use this\n\n```python\nx_tokenizer = Tokenizer(num_words=x_tokens_data['total_count'] - x_tokens_data['count'])\n```","d0055318":"**Model with just LSTMs**","c6239d95":"**Seq2Seq model with Bidirectional LSTMs**. Both `encoder` and `decoder` have `Bidirectional LSTM`s.","1b251101":"It's important to use `sostok` and `eostok` as start and end tokens respectively as later while using `tensorflow's Tokenizer` will filter the tokens and covert them to lowercase.\n\n**sostok** & **eostok** tokens are for us to know where to start & stop the summary because using `_START_` & `_END_`, tf's tokenizer with convert them to **start** & **end** respectively.\n\nSo while decoding the summary sequences of sentences like **'everything is going to end in 2012'** if use `_START_` & `_END_` tokens (which will make the sentence like **'start everything is going to end in 2012 end'** this) whome tf's tokenizer will convert to start and end then we will stop decoding as we hit first **end**, so this is bad and therefore **sostok** & **eostok** these tokens are used.\n\nSo we can just use these **sostok** & **eostok** instead of `_START_` & `_END_`, well you can but I tried both ways and while not using these `_START_` & `_END_` I was getting `undesired results` \ud83e\udd2f \ud83d\ude05 i.e. model's `results weren't good`.","c83a3352":"Finding what should be the `maximum length` of text and headlines that will be feed or accepted by the learning algorithm.","d2099e43":"## \ud83c\udfcc\ufe0f\u200d\u2642\ufe0f Running all the 3 different models\n\nAfter understanding how all the pieces work, running all the `3 models` to understand how it `performs` and its `results`.\n\n**Here there 3 different training models**\n- `build_seq2seq_model_with_just_lstm` - **Seq2Seq model with just LSTMs**. Both `encoder` and `decoder` have just `LSTM`s.\n- `build_seq2seq_model_with_bidirectional_lstm` - **Seq2Seq model with Bidirectional LSTMs**. Both `encoder` and `decoder` have `Bidirectional LSTM`s.\n- `build_hybrid_seq2seq_model` - **Seq2Seq model with hybrid architecture**. Here `encoder` has `Bidirectional LSTM`s while `decoder` has just `LSTM`s.\n\n**Inference methods for the 3 different learning models - just add `_inference` as `prefix`**\n- `build_seq2seq_model_with_just_lstm_inference`\n- `build_seq2seq_model_with_bidirectional_lstm_inference`\n- `build_hybrid_seq2seq_model_inference`\n\n**Decoding sequence for the 3 different learning models - just add `decode_sequence_` as `suffix`**\n- `decode_sequence_build_seq2seq_model_with_just_lstm`\n- `decode_sequence_build_seq2seq_model_with_bidirectional_lstm`\n- `decode_sequence_build_hybrid_seq2seq_model`\n\n![](https:\/\/media.giphy.com\/media\/3ogmaPGsQOruw\/giphy.gif)","0918ca3a":"## \ud83d\udd2e Predictions\n\n![](https:\/\/media.giphy.com\/media\/1wqqlaQ7IX3TXibXZE\/giphy.gif)","3b4c1c1d":"**Seq2Seq model with hybrid architecture**. Here `encoder` has `Bidirectional LSTM`s while `decoder` has just `LSTM`s.","ecc76f36":"## \ud83c\udfc2 Data preparation","d7217188":"**Tokenizing headlines(summary) \ud83d\udc49 y**","63386287":"**Here there 3 different training models**\n- `build_seq2seq_model_with_just_lstm` - **Seq2Seq model with just LSTMs**. Both `encoder` and `decoder` have just `LSTM`s.\n- `build_seq2seq_model_with_bidirectional_lstm` - **Seq2Seq model with Bidirectional LSTMs**. Both `encoder` and `decoder` have `Bidirectional LSTM`s.\n- `build_hybrid_seq2seq_model` - **Seq2Seq model with hybrid architecture**. Here `encoder` has `Bidirectional LSTM`s while `decoder` has just `LSTM`s.\n\n**Inference methods for the 3 different learning models - just add `_inference` as `prefix`**\n- `build_seq2seq_model_with_just_lstm_inference`\n- `build_seq2seq_model_with_bidirectional_lstm_inference`\n- `build_hybrid_seq2seq_model_inference`\n\n**Decoding sequence for the 3 different learning models - just add `decode_sequence_` as `suffix`**\n- `decode_sequence_build_seq2seq_model_with_just_lstm`\n- `decode_sequence_build_seq2seq_model_with_bidirectional_lstm`\n- `decode_sequence_build_hybrid_seq2seq_model`","e3f0b557":"**Plotting model's performance**","d2635b34":"Using `pre-trained` embeddings and keeping the `Embedding` layer `non-trainable` we get increase in computation speed as don't need to compute the embedding matrix.","43713249":"# Abstractive Text Summarization\n\n[Post](https:\/\/towardsdatascience.com\/data-scientists-guide-to-summarization-fc0db952e363) on getting started with `text summarization`, their pros and cons and much more.\n\n**There 3 different training models used here**\n- `build_seq2seq_model_with_just_lstm` - **Seq2Seq model with just LSTMs**. Both `encoder` and `decoder` have just `LSTM`s.\n- `build_seq2seq_model_with_bidirectional_lstm` - **Seq2Seq model with Bidirectional LSTMs**. Both `encoder` and `decoder` have `Bidirectional LSTM`s.\n- `build_hybrid_seq2seq_model` - **Seq2Seq model with hybrid architecture**. Here `encoder` has `Bidirectional LSTM`s while `decoder` has just `LSTM`s.\n\n**To see the full learning and results of all the 3 model go to the end of the notebook in the `Running all the 3 different models` section**\n\nThe `model (the trained model)`, `encoder_model (for inference)` and `decoder_model (for inference)` for **Seq2Seq with just LSTMs** are only saved.\n\n![](https:\/\/media.giphy.com\/media\/dsKnRuALlWsZG\/giphy.gif)","100352f1":"Again adding `tokens` ... but different ones."}}