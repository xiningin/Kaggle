{"cell_type":{"955caa65":"code","3f66e2ab":"code","8bc44086":"code","01f78554":"code","161d6c53":"code","b8aefe6b":"code","e82325aa":"code","a79894ac":"code","5b6db714":"code","760b0f70":"code","9e23ad22":"code","85154a08":"code","fb0de221":"code","160a43b7":"code","7c49f52f":"code","aad37bfc":"code","0184dd9a":"code","6c8900df":"code","0203c1f5":"code","8839472f":"code","ca1ccc9e":"code","73dddc3a":"code","db0d15ce":"code","e44d9945":"code","f0a0da3a":"code","b1887350":"code","44601d29":"code","1cd91395":"code","927b8812":"code","a2fc36be":"code","68c7495f":"code","f8c9c9bb":"code","44563269":"code","39f5d376":"code","779154e3":"code","e14c25eb":"code","bcacacfb":"code","4e7c87b9":"code","c39d0ac5":"code","e422b9ce":"code","1b9d7f88":"code","096008e9":"code","16fd5269":"code","843589d6":"code","89476450":"code","bd599a09":"code","aeb33520":"code","d7369598":"code","55b28367":"code","434fc4ea":"code","0e2b4299":"code","33b2ca75":"code","4cff3fb0":"code","23fd713b":"code","aa0eec3e":"code","73d32ecf":"code","b6b949c0":"code","18a38187":"code","4f1fc153":"code","96353e88":"code","1357f873":"code","8ee31b69":"code","e25a04d1":"code","b5737f1f":"code","8cd14458":"code","15be793a":"code","5313ece4":"code","64a6bb9c":"code","9897a033":"code","1e2c91a8":"code","15641b0f":"code","69945a7e":"code","74344422":"code","34123c1c":"code","42d887a5":"code","9a981667":"code","382123ab":"code","987c5546":"code","01c5856d":"code","7beb78cd":"code","2248e6d5":"code","3122e319":"code","8900feed":"code","73ca6bec":"code","ac15e4f9":"code","e03c2be5":"code","00edc49b":"code","1cc50ee0":"code","599d7d00":"code","9c48162f":"code","b7f79f22":"code","b1cdcdde":"code","c9248c85":"code","7fbdcfdf":"code","6027e7bd":"code","53867a22":"code","e9ff49ad":"code","cc432799":"code","832138b2":"code","701e8ab3":"code","130a5ab9":"code","af6a399a":"markdown","0059b2b8":"markdown","7a749ca6":"markdown","5e35b471":"markdown","9f30e857":"markdown","dffa2ba3":"markdown","92f03318":"markdown","6e1b51fe":"markdown","99465cae":"markdown","92342008":"markdown","9dd123ee":"markdown","f75fdba8":"markdown","8aaa8809":"markdown","f83a695a":"markdown","f078db8c":"markdown","b19fd825":"markdown","f9498ec5":"markdown","e7558265":"markdown","d17372b5":"markdown","2c577e5e":"markdown","e73c3bce":"markdown","ebd9e3a8":"markdown","809af3cd":"markdown","e957b7a6":"markdown","74155ee4":"markdown","5f82fe12":"markdown","db871b78":"markdown","dc33cda3":"markdown","680b5bbd":"markdown","2dea8545":"markdown","e55e12c0":"markdown","06fad272":"markdown","b1975b8b":"markdown","5931dd9d":"markdown","36734c39":"markdown","fe4270d1":"markdown","019dc3bf":"markdown","c3aec0d6":"markdown","32020763":"markdown","7d1181f0":"markdown","3b6924f0":"markdown","58c92334":"markdown","e50fb255":"markdown","720dc6fa":"markdown","140fe3e0":"markdown","13622d91":"markdown","2cb59052":"markdown","5baf8f59":"markdown","4c40d81e":"markdown","652a260b":"markdown","72749a6a":"markdown","f3380088":"markdown","e00e209e":"markdown","6be1e83a":"markdown","d8cc7c4e":"markdown","f6677de2":"markdown","5fe74ea5":"markdown","b9c98b9b":"markdown","3009828e":"markdown","6b92ae72":"markdown","af3e9b21":"markdown"},"source":{"955caa65":"import numpy as np\nimport pandas as pd\nfrom hyperopt import hp, tpe, Trials\nfrom hyperopt.fmin import fmin\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport hyperopt\nfrom sklearn.preprocessing import LabelEncoder","3f66e2ab":"!ls ..\/input","8bc44086":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")\ndf_train.head()","01f78554":"df_train.shape","161d6c53":"df_test.shape","b8aefe6b":"df_train[\"train\"] = 1\ndf_test[\"train\"] = 0\ndf_all = pd.concat([df_train, df_test], sort=False)\ndf_all.head()","e82325aa":"df_all.tail()","a79894ac":"y = df_train[\"Survived\"]\ny.head()","5b6db714":"def parse_cabin_type(x):\n    if pd.isnull(x):\n        return None\n    #print(\"X:\"+x[0])\n    #cabin id consists of letter+numbers. letter is the type\/deck, numbers are cabin number on deck\n    return x[0]","760b0f70":"def parse_cabin_number(x):\n    if pd.isnull(x):\n        return -1\n#        return np.nan\n    cabs = x.split()\n    cab = cabs[0]\n    num = cab[1:]\n    if len(num) < 2:\n        return -1\n        #return np.nan\n    return num","9e23ad22":"def parse_cabin_count(x):\n    if pd.isnull(x):\n        return np.nan\n    #a typical passenger has a single cabin but some had multiple. multiple cabin ids are space separated\n    cabs = x.split()\n    return len(cabs)","85154a08":"df_train.dtypes","fb0de221":"cabin_types = df_all[\"Cabin\"].apply(lambda x: parse_cabin_type(x))\ncabin_types = cabin_types.unique()\n#drop the nan value from list of cabin types\ncabin_types = np.delete(cabin_types, np.where(cabin_types == None))\ncabin_types","160a43b7":"df_all[\"cabin_type\"] = df_all[\"Cabin\"].apply(lambda x: parse_cabin_type(x))\ndf_all[\"cabin_num\"] = df_all[\"Cabin\"].apply(lambda x: parse_cabin_number(x))\ndf_all[\"cabin_count\"] = df_all[\"Cabin\"].apply(lambda x: parse_cabin_count(x))\ndf_all[\"cabin_num\"] = df_all[\"cabin_num\"].astype(int)\ndf_all[[\"Cabin\", \"cabin_type\", \"cabin_count\", \"cabin_num\"]].head()","7c49f52f":"embarked_dummies = pd.get_dummies(df_all[\"Embarked\"], prefix=\"embarked_\", dummy_na=True)\n#TODO: see if imputing embardked makes a difference\ndf_all = pd.concat([df_all, embarked_dummies], axis=1)\ndf_all[[col for col in df_all.columns if 'embarked_' in col]].head()","aad37bfc":"cabin_type_dummies = pd.get_dummies(df_all[\"cabin_type\"], prefix=\"cabin_type_\", dummy_na=True)\ndf_all = pd.concat([df_all, cabin_type_dummies], axis=1)\ndf_all[[col for col in df_all.columns if 'cabin_type_' in col]].head()","0184dd9a":"l_enc = LabelEncoder()\ndf_all[\"sex_label\"] = l_enc.fit_transform(df_all[\"Sex\"])\ndf_all[[\"Sex\", \"sex_label\"]].head()","6c8900df":"df_all[\"family_size\"] = df_all[\"SibSp\"] + df_all[\"Parch\"] + 1\ndf_all[[\"SibSp\", \"Parch\", \"family_size\"]].head()","0203c1f5":"# Cleaning name and extracting Title\nfor name_string in df_all['Name']:\n    df_all['Title'] = df_all['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\ndf_all[[\"Name\", \"Title\"]].head()","8839472f":"df_all[\"Title\"].unique()","ca1ccc9e":"# Replacing rare titles \nmapping = {'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs', 'Major': 'Other', \n           'Col': 'Other', 'Dr' : 'Other', 'Rev' : 'Other', 'Capt': 'Other', \n           'Jonkheer': 'Royal', 'Sir': 'Royal', 'Lady': 'Royal', \n           'Don': 'Royal', 'Countess': 'Royal', 'Dona': 'Royal'}\n           \ndf_all.replace({'Title': mapping}, inplace=True)\n#titles = ['Miss', 'Mr', 'Mrs', 'Royal', 'Other', 'Master']","73dddc3a":"titles = df_all[\"Title\"].unique()\ntitles","db0d15ce":"title_dummies = pd.get_dummies(df_all[\"Title\"], prefix=\"title\", dummy_na=True)\ndf_all = pd.concat([df_all, title_dummies], axis=1)\ndf_all.head()","e44d9945":"df_all['Age'].isnull().sum()","f0a0da3a":"df_all[\"Age\"].value_counts().count()","b1887350":"titles = list(titles)\n# Replacing missing age by median age for title \nfor title in titles:\n    age_to_impute = df_all.groupby('Title')['Age'].median()[titles.index(title)]\n    df_all.loc[(df_all['Age'].isnull()) & (df_all['Title'] == title), 'Age'] = age_to_impute","44601d29":"df_all['Age'].isnull().sum()","1cd91395":"df_all[\"Age\"].value_counts().count()","927b8812":"df_all.groupby('Pclass').agg({'Fare': lambda x: x.isnull().sum()})","a2fc36be":"df_all[df_all[\"Fare\"].isnull()]","68c7495f":"df_all.loc[152]","f8c9c9bb":"p3_median_fare = df_all[df_all[\"Pclass\"] == 3][\"Fare\"].median()\np3_median_fare","44563269":"df_all[\"Fare\"].fillna(p3_median_fare, inplace=True)","39f5d376":"df_all.loc[152]","779154e3":"#name col seems to be in format \"last name, first names\". \n#so split by comma and take first item in resulting list should give last name..\ndf_all['Last_Name'] = df_all['Name'].apply(lambda x: str.split(x, \",\")[0])\ndf_all[['Name', 'Last_Name']].head()","e14c25eb":"#this would be the default value if no family member is found\nDEFAULT_SURVIVAL_VALUE = 0.5\ndf_all['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\nwhatisthis1 = None\nwhatisthis2 = None\nfor grp, grp_df in df_all[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n\n    if (len(grp_df) != 1):\n        # A Family group is found.\n        for ind, row in grp_df.iterrows():\n            if ind == 312: #570\n                print(\"t1\")\n                whatisthis1 = grp_df\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                df_all.loc[df_all['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin == 0.0):\n                df_all.loc[df_all['PassengerId'] == passID, 'Family_Survival'] = 0\n\nfor _, grp_df in df_all.groupby('Ticket'):\n    if grp_df.iloc[0][\"Ticket\"] == \"LINE\":\n        continue\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if ind == 312:\n                print(\"t2\")\n                whatisthis2 = grp_df\n                \n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    df_all.loc[df_all['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin == 0.0):\n                    df_all.loc[df_all['PassengerId'] == passID, 'Family_Survival'] = 0","bcacacfb":"#whatisthis1","4e7c87b9":"#whatisthis2","c39d0ac5":"df_all.isnull().sum()","e422b9ce":"df_all['cabin_type'].unique()","1b9d7f88":"df_all['Embarked'] = df_all['Embarked'].fillna('S')\ndf_all['cabin_type'] = df_all['cabin_type'].fillna('unknown')\ndf_all['Cabin'] = df_all['Cabin'].fillna('unknown')\ndf_all['cabin_count'] = df_all['cabin_count'].fillna(0)\n","096008e9":"df_train = df_all[df_all[\"train\"] == 1]\ndf_test = df_all[df_all[\"train\"] == 0]","16fd5269":"X_cols = set(df_train.columns)\n\nX_cols -= set(['PassengerId', 'Survived', 'Sex', 'Name', 'Ticket', 'Cabin', \n               'Embarked', 'cabin_type', 'Title', 'train', 'Last_Name'])\nX_cols = list(X_cols)\nX_cols\n","843589d6":"df_train[X_cols].head()","89476450":"class OptimizerResult:\n    avg_accuracy = None,\n    misclassified_indices = None,\n    misclassified_expected = None,\n    misclassified_actual = None,\n    oof_predictions = None,\n    predictions = None,\n    df_misses = None,\n    all_accuracies = None,\n    all_losses = None,\n    all_params = None,\n","bd599a09":"from sklearn.model_selection import cross_val_score, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\ndef stratified_test_prediction_avg_vote(clf, X_train, X_test, y, n_folds, n_classes, fit_params):\n    folds = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=69)\n    #N columns, one per target label. each contains probability of that value\n    sub_preds = np.zeros((X_test.shape[0], n_classes))\n    oof_preds = np.zeros((X_train.shape[0]))\n    use_eval_set = fit_params.pop(\"use_eval_set\")\n    score = 0\n    acc_score = 0\n    acc_score_total = 0\n    misclassified_indices = []\n    misclassified_expected = []\n    misclassified_actual = []\n    for i, (train_index, test_index) in enumerate(folds.split(X_train, y)):\n        print('-' * 20, i, '-' * 20)\n\n        X_val, y_val = X_train.iloc[test_index], y[test_index]\n        if use_eval_set:\n            clf.fit(X_train.iloc[train_index], y[train_index], eval_set=([(X_val, y_val)]), **fit_params)\n        else:\n            #random forest does not know parameter \"eval_set\" or \"verbose\"\n            clf.fit(X_train.iloc[train_index], y[train_index], **fit_params)\n        #could directly do predict() here instead of predict_proba() but then mismatch comparison would not be possible\n        oof_preds[test_index] = clf.predict_proba(X_train.iloc[test_index])[:,1].flatten()\n        #we predict on whole test set, thus split by n_splits, not n_splits - 1\n        sub_preds += clf.predict_proba(X_test) \/ folds.n_splits\n#        sub_preds += clf.predict(X_test) \/ folds.n_splits\n#        score += clf.score(X_train.iloc[test_index], y[test_index])\n        preds_this_round = oof_preds[test_index] >= 0.5\n        acc_score = accuracy_score(y[test_index], preds_this_round)\n        acc_score_total += acc_score\n        print('accuracy score ', acc_score)\n        if hasattr(clf, 'feature_importances_'):\n            importances = clf.feature_importances_\n            features = X_train.columns\n\n            feat_importances = pd.Series(importances, index=features)\n            feat_importances.nlargest(30).sort_values().plot(kind='barh', color='#86bf91', figsize=(10, 8))\n            plt.show()\n        else:\n            print(\"classifier has no feature importances: skipping feature plot\")\n\n        missed = y[test_index] != preds_this_round\n        misclassified_indices.extend(test_index[missed])\n        m1 = y[test_index][missed]\n        misclassified_expected.append(m1)\n        m2 = oof_preds[test_index][missed].astype(\"int\")\n        misclassified_actual.append(m2)\n\n    print(f\"acc_score: {acc_score}\")\n    sub_sub = sub_preds[:5]\n    print(f\"sub_preds: {sub_sub}\")\n    avg_accuracy = acc_score_total \/ folds.n_splits\n    print('Avg Accuracy', avg_accuracy)\n    result = OptimizerResult()\n    result.avg_accuracy = avg_accuracy\n    result.misclassified_indices = misclassified_indices\n    result.misclassified_expected = misclassified_expected\n    result.misclassified_actual = misclassified_actual\n    result.oof_predictions = oof_preds\n    result.predictions = sub_preds\n    return result\n","aeb33520":"#check if given parameter can be interpreted as a numerical value\ndef is_number(s):\n    if s is None:\n        return False\n    try:\n        float(s)\n        return True\n    except ValueError:\n        return False\n\n#convert given set of paramaters to integer values\n#this at least cuts the excess float decimals if they are there\ndef convert_int_params(names, params):\n    for int_type in names:\n        #sometimes the parameters can be choices between options or numerical values. like \"log2\" vs \"1-10\"\n        raw_val = params[int_type]\n        if is_number(raw_val):\n            params[int_type] = int(raw_val)\n    return params\n\n#convert float parameters to 3 digit precision strings\n#just for simpler diplay and all\ndef convert_float_params(names, params):\n    for float_type in names:\n        raw_val = params[float_type]\n        if is_number(raw_val):\n            params[float_type] = '{:.3f}'.format(raw_val)\n    return params\n","d7369598":"import numpy as np\nimport pandas as pd\nfrom hyperopt import hp, tpe, Trials\nfrom hyperopt.fmin import fmin\nimport catboost\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport hyperopt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import accuracy_score, log_loss\n\n# run n_folds of cross validation on the data\n# averages fold results\ndef fit_cv(X, y, params, fit_params, n_classes, classifier, max_n, n_folds, print_summary, verbosity):\n    # cut the data if max_n is set\n    if max_n is not None:\n        X = X[:max_n]\n        y = y[:max_n]\n\n    fit_params = fit_params.copy()\n    use_eval = fit_params.pop(\"use_eval_set\")\n    score = 0\n    acc_score = 0\n    folds = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=69)\n\n    if print_summary:\n        print(f\"Running {n_folds} folds...\")\n    oof_preds = np.zeros((X.shape[0], n_classes))\n    for i, (train_index, test_index) in enumerate(folds.split(X, y)):\n        if verbosity > 0:\n            print('-' * 20, f\"RUNNING FOLD: {i}\/{n_folds}\", '-' * 20)\n\n        X_train, y_train = X.iloc[train_index], y[train_index]\n        X_test, y_test = X.iloc[test_index], y[test_index]\n        #https:\/\/catboost.ai\/docs\/concepts\/python-reference_parameters-list.html#python-reference_parameters-list\n        #clf = catboost.CatBoostClassifier(**params)\n        clf = classifier(**params)\n        # verbose = print loss at every \"verbose\" rounds.\n        #if 100 it prints progress 100,200,300,... iterations\n        if use_eval:\n            clf.fit(X_train, y_train, eval_set=(X_test, y_test), **fit_params)\n        else:\n            clf.fit(X_train, y_train, **fit_params)\n        oof_preds[test_index] = clf.predict_proba(X.iloc[test_index])\n        #score += clf.score(X.iloc[test_index], y[test_index])\n        acc_score += accuracy_score(y[test_index], oof_preds[test_index][:,1] >= 0.5)\n        # print('score ', clf.score(X.iloc[test_index], y[test_index]))\n        #importances = clf.feature_importances_\n        features = X.columns\n    #accuracy is calculated each fold so divide by n_folds.\n    #not n_folds -1 because it is not sum by row but overall sum of accuracy of all test indices\n    total_acc_score = acc_score \/ n_folds\n    logloss = log_loss(y, oof_preds)\n    if print_summary:\n        print(f\"total acc: {total_acc_score}, logloss={logloss}\")\n    return total_acc_score, logloss","55b28367":"def create_misclassified_dataframe(result, y):\n    oof_series = pd.Series(result.oof_predictions[result.misclassified_indices])\n    oof_series.index = y[result.misclassified_indices].index\n    miss_scale_raw = y[result.misclassified_indices] - result.oof_predictions[result.misclassified_indices]\n    miss_scale_abs = abs(miss_scale_raw)\n    df_miss_scale = pd.concat([miss_scale_raw, miss_scale_abs, oof_series, y[result.misclassified_indices]], axis=1)\n    df_miss_scale.columns = [\"Raw_Diff\", \"Abs_Diff\", \"Prediction\", \"Actual\"]\n    result.df_misses = df_miss_scale\n","434fc4ea":"class CatboostOptimizer:\n    # how many CV folds to do on the data\n    n_folds = 5\n    # max number of rows to use for X and y. to reduce time and compare options faster\n    max_n = None\n    # max number of trials hyperopt runs\n    n_trials = 200\n    #verbosity in LGBM is how often progress is printed. with 100=print progress every 100 rounds. 0 is quite?\n    verbosity = 0\n    #if true, print summary accuracy\/loss after each round\n    print_summary = False\n\n    all_accuracies = []\n    all_losses = []\n    all_params = []\n\n    def objective_sklearn(self, params):\n        int_types = [\"depth\"]\n        params = convert_int_params(int_types, params)\n        params[\"iterations\"] = 1000\n        params[\"early_stopping_rounds\"] = 10\n        if params['bootstrap_type'].lower() != \"bayesian\":\n            #catboost gives error if bootstrap option defined with bootstrap disabled\n            del params['bagging_temperature']\n\n    #    n_classes = params[\"num_class\"]\n        n_classes = params.pop(\"num_class\")\n        \n        score, logloss = fit_cv(self.X, self.y, params, self.fit_params, n_classes, catboost.CatBoostClassifier, \n                                self.max_n, self.n_folds, self.print_summary, verbosity=self.verbosity)\n        self.all_params.append(params)\n        self.all_accuracies.append(score)\n        self.all_losses.append(logloss)\n        if self.verbosity == 0:\n            if self.print_summary:\n                print(\"Score {:.3f}\".format(score))\n        else:\n            print(\"Score {:.3f} params {}\".format(score, params))\n        #using logloss here for the loss but uncommenting line below calculates it from average accuracy\n    #    loss = 1 - score\n        loss = logloss\n        result = {\"loss\": loss, \"score\": score, \"params\": params, 'status': hyperopt.STATUS_OK}\n        return result\n\n    def optimize_catboost(self, n_classes, max_n_search):\n        # https:\/\/github.com\/Microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst\n        #https:\/\/indico.cern.ch\/event\/617754\/contributions\/2590694\/attachments\/1459648\/2254154\/catboost_for_CMS.pdf\n        space = {\n            #'shrinkage': hp.loguniform('shrinkage', -7, 0),\n            'depth': hp.quniform('depth', 2, 10, 1),\n            'rsm': hp.uniform('rsm', 0.5, 1),\n            'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n            'border_count': hp.qloguniform('border_count', np.log(32), np.log(255), 1),\n            #'ctr_border_count': hp.qloguniform('ctr_border_count', np.log(32), np.log(255), 1),\n            'l2_leaf_reg': hp.quniform('l2_leaf_reg', 0, 5, 1),\n            'leaf_estimation_method': hp.choice('leaf_estimation_method', ['Newton', 'Gradient']),\n            'bootstrap_type': hp.choice('bootstrap_type', ['Bayesian', 'Bernoulli', 'No']), #Poisson also possible for GPU\n            'bagging_temperature': hp.loguniform('bagging_temperature', np.log(1), np.log(3)),\n            'use_best_model': True\n            #'gradient_iterations': hp.quniform('gradient_iterations', 1, 100, 1),\n        }\n\n        self.max_n = max_n_search\n\n        if n_classes > 2:\n            space['objective'] = \"multiclass\"\n            space[\"num_class\"] = n_classes\n            space[\"eval_metric\"] = \"multi_logloss\"\n        else:\n            space['objective'] = \"Logloss\"\n            space[\"num_class\"] = 2\n            #space[\"eval_metric\"] = [\"Logloss\"]\n            #space[\"num_class\"] = 1\n\n        trials = Trials()\n        best = fmin(fn=self.objective_sklearn,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=self.n_trials,\n                   trials=trials)\n\n        # find the trial with lowest loss value. this is what we consider the best one\n        idx = np.argmin(trials.losses())\n        print(idx)\n\n        print(trials.trials[idx])\n\n        params = trials.trials[idx][\"result\"][\"params\"]\n        print(params)\n        return params\n\n    # run a search for binary classification\n    def classify_binary(self, X_cols, df_train, df_test, y_param):\n        self.y = y_param\n\n        self.X = df_train[X_cols]\n        self.X_test = df_test[X_cols]\n\n        self.fit_params = {'verbose': self.verbosity, \n                         'use_eval_set': True}\n\n        # use 2 classes as this is a binary classification\n        # the second param is the number of rows to use for training\n        params = self.optimize_catboost(2, 5000)\n        print(params)\n\n        clf = catboost.CatBoostClassifier(**params)\n\n        search_results = stratified_test_prediction_avg_vote(clf, self.X, self.X_test, self.y,\n                                                             n_folds=self.n_folds, n_classes=2, fit_params=self.fit_params)\n        search_results.all_accuracies = self.all_accuracies\n        search_results.all_losses = self.all_losses\n        search_results.all_params = self.all_params\n        return search_results\n","0e2b4299":"import numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgbm\nfrom hyperopt import hp, tpe, Trials\nfrom hyperopt.fmin import fmin\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport hyperopt\n\nclass LGBMOptimizer:\n    # how many CV folds to do on the data\n    n_folds = 5\n    # max number of rows to use for training (from X and y). to reduce time and compare options faster\n    max_n = None\n    # max number of trials hyperopt runs\n    n_trials = 200\n    #verbosity in LGBM is how often progress is printed. with 100=print progress every 100 rounds. 0 is quite?\n    verbosity = 0\n    #if true, print summary accuracy\/loss after each round\n    print_summary = False\n\n    from sklearn.metrics import accuracy_score, log_loss\n\n    all_accuracies = []\n    all_losses = []\n    all_params = []\n\n    def create_fit_params(self, params):\n        using_dart = params['boosting_type'] == \"dart\"\n        if params[\"objective\"] == \"binary\":\n            # https:\/\/github.com\/Microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst\n            fit_params = {\"eval_metric\": [\"binary_logloss\", \"auc\"]}\n        else:\n            fit_params = {\"eval_metric\": \"multi_logloss\"}\n        if using_dart:\n            n_estimators = 2000\n        else:\n            n_estimators = 15000\n            fit_params[\"early_stopping_rounds\"] = 100\n        params[\"n_estimators\"] = n_estimators\n        fit_params['use_eval_set'] = True\n        fit_params['verbose'] = self.verbosity\n        return fit_params\n\n    # this is the objective function the hyperopt aims to minimize\n    # i call it objective_sklearn because the lgbm functions called use sklearn API\n    def objective_sklearn(self, params):\n        int_types = [\"num_leaves\", \"min_child_samples\", \"subsample_for_bin\", \"min_data_in_leaf\"]\n        params = convert_int_params(int_types, params)\n\n        # Extract the boosting type\n        params['boosting_type'] = params['boosting_type']['boosting_type']\n        #    print(\"running with params:\"+str(params))\n\n        fit_params = self.create_fit_params(params)\n        if params['objective'] == \"binary\":\n            n_classes = 2\n        else:\n            n_classes = params[\"num_class\"]\n\n        score, logloss = fit_cv(self.X, self.y, params, fit_params, n_classes, lgbm.LGBMClassifier, \n                                self.max_n, self.n_folds, self.print_summary, verbosity=self.verbosity)\n        self.all_params.append(params)\n        self.all_accuracies.append(score)\n        self.all_losses.append(logloss)\n        if self.verbosity == 0:\n            if self.print_summary:\n                print(\"Score {:.3f}\".format(score))\n        else:\n            print(\"Score {:.3f} params {}\".format(score, params))\n    #using logloss here for the loss but uncommenting line below calculates it from average accuracy\n    #    loss = 1 - score\n        loss = logloss\n        result = {\"loss\": loss, \"score\": score, \"params\": params, 'status': hyperopt.STATUS_OK}\n        return result\n\n    def optimize_lgbm(self, n_classes):\n        # https:\/\/github.com\/Microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst\n        # https:\/\/indico.cern.ch\/event\/617754\/contributions\/2590694\/attachments\/1459648\/2254154\/catboost_for_CMS.pdf\n        space = {\n            #this is just piling on most of the possible parameter values for LGBM\n            #some of them apparently don't make sense together, but works for now.. :)\n            'class_weight': hp.choice('class_weight', [None, 'balanced']),\n            'boosting_type': hp.choice('boosting_type',\n                                       [{'boosting_type': 'gbdt',\n    #                                     'subsample': hp.uniform('dart_subsample', 0.5, 1)\n                                         },\n                                        {'boosting_type': 'dart',\n    #                                     'subsample': hp.uniform('dart_subsample', 0.5, 1)\n                                         },\n                                        {'boosting_type': 'goss'}]),\n            'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n            'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n            'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n            'feature_fraction': hp.uniform('feature_fraction', 0.5, 1),\n            'bagging_fraction': hp.uniform('bagging_fraction', 0.5, 1), #alias \"subsample\"\n            'min_data_in_leaf': hp.qloguniform('min_data_in_leaf', 0, 6, 1),\n            'lambda_l1': hp.choice('lambda_l1', [0, hp.loguniform('lambda_l1_positive', -16, 2)]),\n            'lambda_l2': hp.choice('lambda_l2', [0, hp.loguniform('lambda_l2_positive', -16, 2)]),\n            'verbose': -1,\n            #the LGBM parameters docs list various aliases, and the LGBM implementation seems to complain about\n            #the following not being used due to other params, so trying to silence the complaints by setting to None\n            'subsample': None, #overridden by bagging_fraction\n            'reg_alpha': None, #overridden by lambda_l1\n            'reg_lambda': None, #overridden by lambda_l2\n            'min_sum_hessian_in_leaf': None, #overrides min_child_weight\n            'min_child_samples': None, #overridden by min_data_in_leaf\n            'colsample_bytree': None, #overridden by feature_fraction\n    #        'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n            'min_child_weight': hp.loguniform('min_child_weight', -16, 5), #also aliases to min_sum_hessian\n    #        'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    #        'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    #        'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n        }\n        if n_classes > 2:\n            space['objective'] = \"multiclass\"\n            space[\"num_class\"] = n_classes\n        else:\n            space['objective'] = \"binary\"\n            #space[\"num_class\"] = 1\n\n        trials = Trials()\n        best = fmin(fn=self.objective_sklearn,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=self.n_trials,\n                    trials=trials,\n                   verbose= 1)\n\n        # find the trial with lowest loss value. this is what we consider the best one\n        idx = np.argmin(trials.losses())\n        print(idx)\n\n        print(trials.trials[idx])\n\n        # these should be the training parameters to use to achieve the best score in best trial\n        params = trials.trials[idx][\"result\"][\"params\"]\n        max_n = None\n\n        print(params)\n        return params\n\n    # run a search for binary classification\n    def classify_binary(self, X_cols, df_train, df_test, y_param):\n        self.y = y_param\n\n        self.X = df_train[X_cols]\n        self.X_test = df_test[X_cols]\n\n        # use 2 classes as this is a binary classification\n        params = self.optimize_lgbm(2)\n        print(params)\n\n        clf = lgbm.LGBMClassifier(**params)\n\n        fit_params = self.create_fit_params(params)\n\n        search_results = stratified_test_prediction_avg_vote(clf, self.X, self.X_test, self.y,\n                                                             n_folds=self.n_folds, n_classes=2, fit_params=fit_params)\n        search_results.all_accuracies = self.all_accuracies\n        search_results.all_losses = self.all_losses\n        search_results.all_params = self.all_params\n        return search_results","33b2ca75":"import numpy as np\nimport pandas as pd\nfrom hyperopt import hp, tpe, Trials\nfrom hyperopt.fmin import fmin\nimport lightgbm as lgbm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nimport hyperopt\nfrom sklearn.metrics import accuracy_score, log_loss\n\nclass RFOptimizer:\n    # how many CV folds to do on the data\n    n_folds = 5\n    # max number of rows to use for training (from X and y). to reduce time and compare options faster\n    max_n = None\n    # max number of trials hyperopt runs\n    n_trials = 200\n    #verbosity 0 in RF is quite, 1 = print epoch, 2 = print within epoch\n    #https:\/\/stackoverflow.com\/questions\/31952991\/what-does-the-verbosity-parameter-of-a-random-forest-mean-sklearn\n    verbosity = 0\n    #if true, print summary accuracy\/loss after each round\n    print_summary = False\n\n    all_accuracies = []\n    all_losses = []\n    all_params = []\n\n    def objective_sklearn(self, params):\n        int_types = [\"n_estimators\", \"min_samples_leaf\", \"min_samples_split\", \"max_features\"]\n        n_classes = params.pop(\"num_class\")\n        params = convert_int_params(int_types, params)\n        score, logloss = fit_cv(self.X, self.y, params, self.fit_params, n_classes, RandomForestClassifier, \n                                self.max_n, self.n_folds, self.print_summary, verbosity=self.verbosity)\n        self.all_params.append(params)\n        self.all_accuracies.append(score)\n        self.all_losses.append(logloss)\n\n        loss = logloss\n        result = {\"loss\": loss, \"score\": score, \"params\": params, 'status': hyperopt.STATUS_OK}\n        return result\n\n    def optimize_rf(self, n_classes):\n        # https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n        space = {\n            'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n            # 'scale': hp.choice('scale', [0, 1]),\n            # 'normalize': hp.choice('normalize', [0, 1]),\n            'bootstrap': hp.choice('bootstrap', [True, False]),\n            # nested choice: https:\/\/medium.com\/vooban-ai\/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters-e3102814b919\n            'max_depth': hp.choice('max_depth', [None, hp.quniform('max_depth_num', 10, 100, 10)]),\n            'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2', None, hp.quniform('max_features_num', 1, 5, 1)]),\n            'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 4, 1),\n            'min_samples_split': hp.quniform('min_samples_split', 2, 10, 4),\n            'class_weight': hp.choice('class_weight', [\"balanced\", None]),\n            'n_estimators': hp.quniform('n_estimators', 200, 2000, 200),\n            'n_jobs': -1,\n            'num_class': n_classes,\n            'verbose': self.verbosity\n        }\n        # save and reload trials for hyperopt state: https:\/\/github.com\/Vooban\/Hyperopt-Keras-CNN-CIFAR-100\/blob\/master\/hyperopt_optimize.py\n\n        trials = Trials()\n        best = fmin(fn=self.objective_sklearn,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=self.n_trials,\n                   trials=trials)\n\n        idx = np.argmin(trials.losses())\n        print(idx)\n\n        print(trials.trials[idx])\n\n        params = trials.trials[idx][\"result\"][\"params\"]\n        print(params)\n        return params\n\n    # run a search for binary classification\n    def classify_binary(self, X_cols, df_train, df_test, y_param):\n        self.y = y_param\n\n        self.X = df_train[X_cols]\n        self.X_test = df_test[X_cols]\n\n        self.fit_params = {'use_eval_set': False}\n\n        # use 2 classes as this is a binary classification\n        params = self.optimize_rf(2)\n        print(params)\n\n        clf = RandomForestClassifier(**params)\n\n        search_results = stratified_test_prediction_avg_vote(clf, self.X, self.X_test, self.y,\n                                                             n_folds=self.n_folds, n_classes=2, fit_params=self.fit_params)\n        search_results.all_accuracies = self.all_accuracies\n        search_results.all_losses = self.all_losses\n        search_results.all_params = self.all_params\n        return search_results","4cff3fb0":"import numpy as np\nimport pandas as pd\nfrom hyperopt import hp, tpe, Trials\nfrom hyperopt.fmin import fmin\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport hyperopt\nfrom sklearn.metrics import accuracy_score, log_loss\n\nclass XGBOptimizer:\n    # how many CV folds to do on the data\n    n_folds = 5\n    # max number of rows to use for training (from X and y). to reduce time and compare options faster\n    max_n = None\n    # max number of trials hyperopt runs\n    n_trials = 200\n    #verbosity 0 in RF is quite, 1 = print epoch, 2 = print within epoch\n    #https:\/\/stackoverflow.com\/questions\/31952991\/what-does-the-verbosity-parameter-of-a-random-forest-mean-sklearn\n    verbosity = 0\n    #if true, print summary accuracy\/loss after each round\n    print_summary = False\n\n    all_accuracies = []\n    all_losses = []\n    all_params = []\n\n    def objective_sklearn(self, params):\n        int_params = ['max_depth']\n        params = convert_int_params(int_params, params)\n        float_params = ['gamma', 'colsample_bytree']\n        params = convert_float_params(float_params, params)\n        n_classes = params.pop(\"num_class\")\n\n        score, logloss = fit_cv(self.X, self.y, params, self.fit_params, n_classes, xgb.XGBClassifier, \n                                self.max_n, self.n_folds, self.print_summary, verbosity=self.verbosity)\n        self.all_params.append(params)\n        self.all_accuracies.append(score)\n        self.all_losses.append(logloss)\n\n        loss = logloss\n        result = {\"loss\": loss, \"score\": score, \"params\": params, 'status': hyperopt.STATUS_OK}\n        return result\n\n    def optimize_xgb(self, n_classes):\n        #https:\/\/indico.cern.ch\/event\/617754\/contributions\/2590694\/attachments\/1459648\/2254154\/catboost_for_CMS.pdf\n        space = {\n            'max_depth': hp.quniform('max_depth', 2, 10, 1),\n            #removed gblinear since it does not support early stopping and it was getting tricky\n            'booster': hp.choice('booster', ['gbtree', 'dart']),\n            #'booster': hp.choice('booster', ['gbtree', 'gblinear', 'dart']),\n            'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.3)),\n            #nthread defaults to maximum so not setting it\n            'subsample': hp.uniform('subsample', 0.75, 1.0),\n            'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n            'colsample_bylevel': hp.uniform('colsample_bylevel', 0.3, 1.0),\n            #'gamma': hp.uniform('gamma', 0.0, 0.5),\n            'min_child_weight': hp.loguniform('min_child_weight', -16, 5),\n            'alpha': hp.choice('alpha', [0, hp.loguniform('alpha_positive', -16, 2)]),\n            'lambda': hp.choice('lambda', [0, hp.loguniform('lambda_positive', -16, 2)]),\n            'gamma': hp.choice('gamma', [0, hp.loguniform('gamma_positive', -16, 2)]),\n            'num_class': n_classes,\n            'verbose': self.verbosity\n            #'n_estimators': 1000   #n_estimators = n_trees -> get error this only valid for gbtree\n            #https:\/\/github.com\/dmlc\/xgboost\/issues\/3789\n        }\n\n        trials = Trials()\n        best = fmin(fn=self.objective_sklearn,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=self.n_trials,\n                   trials=trials)\n\n        idx = np.argmin(trials.losses())\n        print(idx)\n\n        print(trials.trials[idx])\n\n        params = trials.trials[idx][\"result\"][\"params\"]\n        print(params)\n        return params \n\n    # run a search for binary classification\n    def classify_binary(self, X_cols, df_train, df_test, y_param):\n        self.y = y_param\n\n        self.X = df_train[X_cols]\n        self.X_test = df_test[X_cols]\n\n        self.fit_params = {'use_eval_set': False}\n\n        # use 2 classes as this is a binary classification\n        params = self.optimize_xgb(2)\n        print(params)\n\n        clf = xgb.XGBClassifier(**params)\n\n        search_results = stratified_test_prediction_avg_vote(clf, self.X, self.X_test, self.y,\n                                                             n_folds=self.n_folds, n_classes=2, fit_params=self.fit_params)\n        search_results.all_accuracies = self.all_accuracies\n        search_results.all_losses = self.all_losses\n        search_results.all_params = self.all_params\n        return search_results","23fd713b":"xgb_opt = XGBOptimizer()\nxgb_opt.n_trials = 200\nxgb_results = xgb_opt.classify_binary(X_cols, df_train, df_test, y)\ncreate_misclassified_dataframe(xgb_results, y)\n","aa0eec3e":"lgopt = LGBMOptimizer()\nlgopt.n_trials = 200\nlgbm_results = lgopt.classify_binary(X_cols, df_train, df_test, y)\ncreate_misclassified_dataframe(lgbm_results, y)\n","73d32ecf":"copt = CatboostOptimizer()\ncopt.n_trials = 200\ncb_results = copt.classify_binary(X_cols, df_train, df_test, y)\ncreate_misclassified_dataframe(cb_results, y)\n","b6b949c0":"rf_opt = RFOptimizer()\nrf_opt.n_trials = 200\nrf_results = rf_opt.classify_binary(X_cols, df_train, df_test, y)\ncreate_misclassified_dataframe(rf_results, y)","18a38187":"df_losses = pd.DataFrame()\ndf_losses[\"lgbm_loss\"] = lgbm_results.all_losses\ndf_losses[\"lgbm_accuracy\"] = lgbm_results.all_accuracies\ndf_losses[\"cb_loss\"] = cb_results.all_losses\ndf_losses[\"cb_accuracy\"] = cb_results.all_accuracies\ndf_losses[\"rf_loss\"] = rf_results.all_losses\ndf_losses[\"rf_accuracy\"] = rf_results.all_accuracies\ndf_losses[\"xgb_loss\"] = xgb_results.all_losses\ndf_losses[\"xgb_accuracy\"] = xgb_results.all_accuracies\ndf_losses.plot(figsize=(14,8))","4f1fc153":"df_losses.drop(\"rf_loss\", axis=1).plot(figsize=(14,8))","96353e88":"df_losses.sort_values(by=\"lgbm_accuracy\", ascending=False)[[\"lgbm_loss\", \"lgbm_accuracy\"]].head(10)","1357f873":"df_losses.sort_values(by=\"cb_accuracy\", ascending=False)[[\"cb_loss\", \"cb_accuracy\"]].head(10)","8ee31b69":"df_losses.sort_values(by=\"rf_accuracy\", ascending=False)[[\"rf_loss\", \"rf_accuracy\"]].head(10)","e25a04d1":"df_losses.sort_values(by=\"xgb_accuracy\", ascending=False)[[\"xgb_loss\", \"xgb_accuracy\"]].head(10)","b5737f1f":"df_losses.sort_values(by=\"lgbm_loss\", ascending=True)[[\"lgbm_loss\", \"lgbm_accuracy\"]].head(10)","8cd14458":"df_losses.sort_values(by=\"cb_loss\", ascending=True)[[\"cb_loss\", \"cb_accuracy\"]].head(10)","15be793a":"df_losses.sort_values(by=\"rf_loss\", ascending=True)[[\"rf_loss\", \"rf_accuracy\"]].head(10)","5313ece4":"df_losses.sort_values(by=\"xgb_loss\", ascending=True)[[\"xgb_loss\", \"xgb_accuracy\"]].head(10)","64a6bb9c":"ss = pd.read_csv('..\/input\/gender_submission.csv')\n# predicting only true values, so take column 1 (0 is false column)\nnp_preds = np.array(cb_results.predictions)[: ,1]\nss[\"Survived\"] = np.where(np_preds > 0.5, 1, 0)\nss.to_csv('catboost.csv', index=False)\nss.head(10)","9897a033":"ss = pd.read_csv('..\/input\/gender_submission.csv')\n# predicting only true values, so take column 1 (0 is false column)\nnp_preds = np.array(lgbm_results.predictions)[: ,1]\nss[\"Survived\"] = np.where(np_preds > 0.5, 1, 0)\nss.to_csv('lgbm.csv', index=False)\nss.head(10)","1e2c91a8":"ss = pd.read_csv('..\/input\/gender_submission.csv')\n# predicting only true values, so take column 1 (0 is false column)\nnp_preds = np.array(rf_results.predictions)[: ,1]\nss[\"Survived\"] = np.where(np_preds > 0.5, 1, 0)\nss.to_csv('rf.csv', index=False)\nss.head(10)","15641b0f":"ss = pd.read_csv('..\/input\/gender_submission.csv')\n# predicting only true values, so take column 1 (0 is false column)\nnp_preds = np.array(xgb_results.predictions)[: ,1]\nss[\"Survived\"] = np.where(np_preds > 0.5, 1, 0)\nss.to_csv('xgb.csv', index=False)\nss.head(10)","69945a7e":"print(len(lgbm_results.misclassified_indices))\nprint(len(cb_results.misclassified_indices))\nprint(len(rf_results.misclassified_indices))\nprint(len(xgb_results.misclassified_indices))","74344422":"df_top_misses_lgbm = lgbm_results.df_misses.sort_values(by=\"Abs_Diff\", ascending=False)\ndf_top_misses_lgbm.head()","34123c1c":"df_top_misses_cb = cb_results.df_misses.sort_values(by=\"Abs_Diff\", ascending=False)\ndf_top_misses_cb.head()","42d887a5":"df_top_misses_rf = rf_results.df_misses.sort_values(by=\"Abs_Diff\", ascending=False)\ndf_top_misses_rf.head()","9a981667":"df_top_misses_xgb = xgb_results.df_misses.sort_values(by=\"Abs_Diff\", ascending=False)\ndf_top_misses_xgb.head()","382123ab":"#capture the probabilities of True (1) classification for each classifier, to use as inputs for ensembling:\nensemble_input_df = pd.DataFrame()\nensemble_input_df[\"lgbm\"] = lgbm_results.predictions[:,1]\nensemble_input_df[\"xgb\"] = xgb_results.predictions[:,1]\nensemble_input_df[\"catboost\"] = cb_results.predictions[:,1]\nensemble_input_df[\"randomforest\"] = rf_results.predictions[:,1]\nensemble_input_df.head()","987c5546":"ensemble_input_df[\"avg\"] = (ensemble_input_df[\"lgbm\"]+ensemble_input_df[\"xgb\"]+ensemble_input_df[\"catboost\"]+ensemble_input_df[\"randomforest\"])\/4\nensemble_input_df.head()","01c5856d":"ensemble_input_df[\"lgbm_01\"] = np.where(ensemble_input_df[\"lgbm\"] > 0.5, 1, 0)\nensemble_input_df[\"xgb_01\"] = np.where(ensemble_input_df[\"xgb\"] > 0.5, 1, 0)\nensemble_input_df[\"cat_01\"] = np.where(ensemble_input_df[\"catboost\"] > 0.5, 1, 0)\nensemble_input_df[\"rf_01\"] = np.where(ensemble_input_df[\"randomforest\"] > 0.5, 1, 0)\nensemble_input_df.head()","7beb78cd":"ensemble_input_df.shape","2248e6d5":"from scipy.stats import mode\n\n#TODO: majority, avg, stacked ensemble methods\n\ndata = [ensemble_input_df[\"lgbm_01\"].values, ensemble_input_df[\"xgb_01\"], ensemble_input_df[\"cat_01\"], ensemble_input_df[\"rf_01\"]]\nmajority = mode(data, axis=0)\n#the \"majority\" variable is actually now a list of two lists. majority[0][0] is the mode (1 or 0, the majority class), and majority[0][1] is how many times the \"mode\" appears\n#majority","3122e319":"len(majority[0][0])","8900feed":"ensemble_input_df[\"majority\"] = majority[0][0]\nensemble_input_df.head()","73ca6bec":"ss = pd.read_csv('..\/input\/gender_submission.csv')\nss[\"Survived\"] = np.where(ensemble_input_df[\"avg\"] > 0.5, 1, 0)\n#ss[\"Survived\"] = ensemble_input_df[\"avg\"]\nss.to_csv('avg.csv', index=False)\nss.head(10)","ac15e4f9":"ss = pd.read_csv('..\/input\/gender_submission.csv')\nss[\"Survived\"] = ensemble_input_df[\"majority\"]\nss.to_csv('majority.csv', index=False)\nss.head(10)","e03c2be5":"oof_df = pd.DataFrame()\noof_df[\"lgbm\"] = lgbm_results.oof_predictions\noof_df[\"xgb\"] = xgb_results.oof_predictions\noof_df[\"cat\"] = cb_results.oof_predictions\noof_df[\"rf\"] = rf_results.oof_predictions\noof_df[\"target\"] = y\noof_df.head()","00edc49b":"import numpy as np\nimport pandas as pd\nfrom hyperopt import hp, tpe, Trials\nfrom hyperopt.fmin import fmin\nimport lightgbm as lgbm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nimport hyperopt\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.linear_model import LogisticRegression\n\nclass LogRegOptimizer:\n    # how many CV folds to do on the data\n    n_folds = 5\n    # max number of rows to use for training (from X and y). to reduce time and compare options faster\n    max_n = None\n    # max number of trials hyperopt runs\n    n_trials = 200\n    # ?\n    verbosity = 0\n    #if true, print summary accuracy\/loss after each round\n    print_summary = False\n\n    all_accuracies = []\n    all_losses = []\n    all_params = []\n\n    def objective_sklearn(self, params):\n        #print(params)\n        params.update(params[\"solver_params\"]) #pop nested dict to top level\n        del params[\"solver_params\"] #delete the original nested dict after pop (could pop() above too..)\n        if params[\"penalty\"] == \"none\":\n            del params[\"C\"]\n            del params[\"l1_ratio\"]\n        elif params[\"penalty\"] != \"elasticnet\":\n            del params[\"l1_ratio\"]\n        if params[\"solver\"] == \"liblinear\":\n            params[\"n_jobs\"] = 1\n        n_classes = params.pop(\"num_class\")\n#        params = convert_int_params(int_types, params)\n        score, logloss = fit_cv(self.X, self.y, params, self.fit_params, n_classes, LogisticRegression, \n                                self.max_n, self.n_folds, self.print_summary, verbosity=self.verbosity)\n        self.all_params.append(params)\n        self.all_accuracies.append(score)\n        self.all_losses.append(logloss)\n\n        loss = logloss\n        result = {\"loss\": loss, \"score\": score, \"params\": params, 'status': hyperopt.STATUS_OK}\n        return result\n\n    def optimize_logreg(self, n_classes):\n        # https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n        space = {\n            'solver_params': hp.choice('solver_params', [\n                {'solver': 'newton-cg', \n                 'penalty': hp.choice('penalty-ncg', [\"l2\", 'none'])}, #also multiclass loss supported\n                 {'solver': 'lbfgs', \n                 'penalty': hp.choice('penalty-lbfgs', [\"l2\", 'none'])},\n                 {'solver': 'liblinear',\n                 'penalty': hp.choice('penalty-liblin', [\"l1\", \"l2\"])},\n                 {'solver': 'sag',\n                 'penalty': hp.choice('penalty-sag', [\"l2\", 'none'])},\n                 {'solver': 'saga',\n                 'penalty': hp.choice('penalty-saga', [\"elasticnet\", \"l1\", \"l2\", 'none'])},\n            ]),\n            'C': hp.uniform('C', 1e-5,10),\n            'tol': hp.uniform('tol', 1e-5, 10),\n            'fit_intercept': hp.choice(\"fit_intercept\", [True, False]),\n            'class_weight': hp.choice(\"class_weight\", [\"balanced\", None]),\n            #multi-class jos ei bianry\n            'l1_ratio': hp.uniform('l1_ratio', 0.00001, 0.99999), #vain jos elasticnet penalty\n            'n_jobs': -1,\n            'num_class': n_classes,\n            'verbose': self.verbosity\n        }\n        # save and reload trials for hyperopt state: https:\/\/github.com\/Vooban\/Hyperopt-Keras-CNN-CIFAR-100\/blob\/master\/hyperopt_optimize.py\n\n        trials = Trials()\n        best = fmin(fn=self.objective_sklearn,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=self.n_trials,\n                   trials=trials)\n\n        idx = np.argmin(trials.losses())\n        print(idx)\n\n        print(trials.trials[idx])\n\n        params = trials.trials[idx][\"result\"][\"params\"]\n        print(params)\n        return params\n\n    # run a search for binary classification\n    def classify_binary(self, X_cols, df_train, df_test, y_param):\n        self.y = y_param\n\n        self.X = df_train[X_cols]\n        self.X_test = df_test[X_cols]\n\n        self.fit_params = {'use_eval_set': False}\n\n        # use 2 classes as this is a binary classification\n        params = self.optimize_logreg(2)\n        print(params)\n\n        clf = LogisticRegression(**params)\n\n        search_results = stratified_test_prediction_avg_vote(clf, self.X, self.X_test, self.y,\n                                                             n_folds=self.n_folds, n_classes=2, fit_params=self.fit_params)\n        search_results.all_accuracies = self.all_accuracies\n        search_results.all_losses = self.all_losses\n        search_results.all_params = self.all_params\n        return search_results\n","1cc50ee0":"df_stack_train = oof_df.drop(\"target\", axis=1)\ndf_stack_train.head()","599d7d00":"X_stack_cols = df_stack_train.columns\ndf_stacked_X = ensemble_input_df[[\"lgbm\", \"xgb\", \"catboost\", \"randomforest\"]]\ndf_stacked_X.columns = X_stack_cols\nlogreg_opt = LogRegOptimizer()\nlr_results = logreg_opt.classify_binary(X_stack_cols, df_stack_train, df_stacked_X, y)\ncreate_misclassified_dataframe(lr_results, y)","9c48162f":"from sklearn.linear_model import LogisticRegression\n\nss = pd.read_csv('..\/input\/gender_submission.csv')\n# predicting only true values, so take column 1 (0 is false column)\nnp_preds = np.array(lr_results.predictions)[: ,1]\nss[\"Survived\"] = np.where(np_preds > 0.5, 1, 0)\nss.to_csv('stacked.csv', index=False)\nss.head(10)","b7f79f22":"#https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-for-ensemble-models\/","b1cdcdde":"all_misses = pd.concat([df_top_misses_lgbm,df_top_misses_cb,df_top_misses_xgb,df_top_misses_rf], axis=0)\nall_misses.head()","c9248c85":"all_misses_count = all_misses.index.value_counts()\nall_misses_count.head()","7fbdcfdf":"all_misses_count.value_counts()","6027e7bd":"all_misses.head()","53867a22":"#group by index, count sum of abs","e9ff49ad":"all_misses = all_misses.sort_index()\n\nmiss_counts = all_misses.groupby(all_misses.index)['Abs_Diff'].sum().sort_values(ascending=False)\nmiss_counts.head(10)","cc432799":"df_train[df_train[\"Ticket\"] == \"LINE\"]","832138b2":"top10 = miss_counts.head(10).index\ndf_train.iloc[top10]\n","701e8ab3":"miss_counts.tail(10)","130a5ab9":"top10 = miss_counts.tail(10).index\ndf_train.iloc[top10]","af6a399a":"## Family Survival Feature","0059b2b8":"I found the following to be an interesting way to impute age: group by title, use median age per title. Quite often getting good (accuracy) results is not about fitting the most complex model and ensemble but figuring out the data. Like this trick. Again, this is from the kernel I linked above, not my own magic:","7a749ca6":"Some of these classifiers seem very stable in their performances with different hyperparameters, while others seem to fluctuate quite widely. For example, Catboost is quite consistent, while Random Forest shows large spikes with certain values. Not sure why or what it means, but more investigations would likely help learn a bit more about these classifiers and their hyperpamaters. But if it works... :)\n\nAs for loss vs accuracy, I would expect to start with higher loss and lower accuracy, going towards overall higher average accuracy and lower loss as hyperopt would focus the search on parameters. Well, actually looking at the pic above, the hyperparameter search does not seem to make such as huge difference. Maybe the problem\/data is not complex enough?","5e35b471":"## Fit A Classifier with Given Parameters, Cross-Validate over N-Folds\n\n*fit_cv* runs hyperopt for *n_trials* iterations to try different hyperparameter combinations. Each iteration consists of *n_folds* cross-validation splits. So *n_folds*=5 results in splitting to 5 folds, using each at a turn as the validation set and the other 4 for training:","9f30e857":"Plotting the accuracy vs loss. Generally when loss goes up, accuracy should go down. The plot changes on runs due to not fixing random seed. But almost always there are spots in the plot where loss goes much higher while accuracy stays up. This would be the case where overall accuracy is good but the error in misclassifications is very high. So expect maybe not to generalize so great on unseen instances, rather like overfitting on this data.","dffa2ba3":"### LGBM","92f03318":"## Averaging based ensemble","6e1b51fe":"### XGBoost","99465cae":"### Catboost","92342008":"# Build Ensemble Classifiers (Average and Majority)\n\nTry building some ensemble models. Averaging, majority voting, and stacking ones. \n","9dd123ee":"*Family survival* is another feature I got from the [kernel](https:\/\/www.kaggle.com\/vincentlugat\/200-lines-randomized-search-lgbm-82-3) I linked above. Another good example of a very clever way to look at the data. Group people into families (or any other group travelling together) by matching their last name and fare. Create a feature to indicate if others in this same group survived or not. This turns out to be a very nice predictor feature, as my initial model with similar parameters would always score around max 77% accuracy. Adding this feature upped the accuracy to over 81%. I suppose people in a group\/family are more likely to move and act together.\n\nIn a way, this feature feels a little like cheating. If I wanted to figure what features on the ship contributed to survival, for cases where the ship has not sunk yet, this kind of information would not be available. Or you would already know the result, since you know who survived or not. But this is Kaggle, and insights such as these are often what really makes a solution rise in the results. And it shows how to think about the data and relations within it in very clever ways. In other cases it could certainly be much more realistic. Cool!\n\nI guess in other cases, other information could also be useful, such as shared ticket number, or [subsequent ticket numbers](https:\/\/www.kaggle.com\/c\/titanic\/discussion\/11127), cabins, etc. Anyway..","f75fdba8":"# Stacking Ensemble\n\nFinally, try a stacking ensemble based on raw predictions of all the other classifiers in the ensemble:","8aaa8809":"## Family-Size \/ Group-Size\n\nCreate new feature from combined family size:","f83a695a":"Find missing fare values (there is just one..):","f078db8c":"Just some helper functions:","b19fd825":"The above columns in the data are quite self-explanatory with two exceptions:\n\n- sibsp:\tnumber of siblings \/ spouses aboard the Titanic\t\n- parch:\tnumber of parents \/ children aboard the Titanic\n","f9498ec5":"## Accuracy over tuning iterations (per classifier)\n\nLets see the iterations with highest accuracy per classifier:","e7558265":"### Random Forest","d17372b5":"# Hyperopt over the actual classifiers\n","2c577e5e":"The above list has some quite obvious samples (at least when I ran it before writing this..):\n- 62 year old male in second class, whose family members did not survive but he did\n- 25 year old male in third class, whose family members did not survive but he did\n- 26 year old female in second class, with 2 other family members and unknown survival for them. did not survive herself\n- 2 year old baby girl who had both parents on board, of which at least one survived, but the baby did not\n- and so on...\n\nSome pointers I learned from the above table was to look more closely into the passenger family definition and whether they all really match or not. In the end I have no idea, but it seemed to give interesting insight to look into those.\n\nWhich ones do the classifiers most disagree on?","e73c3bce":"To create the features for cabin type, cabin number, and number of cabins:","ebd9e3a8":"## Look at Worst Misclassifications for Stacked Ensemble","809af3cd":"### XGBoost","e957b7a6":"Mr Storey above is from the test set, so *survived* is NaN.","74155ee4":"### LGBM","5f82fe12":"Index 152 repeats twice, from the train set and from the test set. This is visible in *survival*=0 vs *survival*=NaN. And in *train* = 1 vs *train* = 0. Of course, looking back now, just the *survival*=NaN would have been enough to differentiate the two without needing a custom *train* feature.\n\nAnyway, there is only one missing fare so just impute with median of their passenger class:","db871b78":"# Common Functions for all Classifiers Prediction\/Tuning\n\n## Stratified Average Prediction over N-Folds\n\nThe following function does an N-way stratified split of the training data, uses these splits to build a cross-validation set, and performs cross-validation N times with different set combinations.\n\nExample:\nSplit training data to 5 parts. Take 4 parts to train on, 1 part to validate. Change these around so each of the 5 parts is used as validation set once. This gives 5 different models, each trained on 80% of the data. Finally, use each of these 5 models to predict the real test-set. Average the results of all 5 predictions. Other applications would include majority voting etc. Maybe later.","dc33cda3":"Again some top ones:\n- Baby boy of 4 years age, first class, did not survive. Family survival unknown. Strangely there seemed to have been no family on board? Might explain the non-survival.. Maybe too few examples of such in the dataset for the classifier to learn?\n- Another boy of 4 years age with similar properties and unknown family. In first class but seemingly on deck C vs A for first boy?\n- Female in third class, age 28, did not survive. Family unknown, although listed as +2.\n\nSince the misclassification strength in the above seems strong but only for one classifiers, maybe it would be useful to further plot which classifier is in disagreement? A majority classifier would likely get these right, but then it must get some others equally wrong. There would be a few more topics to investigate..\n\n","680b5bbd":"Grab the misclassifications in order of absolute misclassification confidense, see how far off the worts onces are:","2dea8545":"### XGBoost","e55e12c0":"At 4 classifiers the above shows how it picks the lower mode values. Perhaps an odd number as ensemble input would be better to get a \"real\" mode?\n\n## Submission Files for Average and Majority Ensembles\n\nWrite out the average and majority ensemble results for submission:","06fad272":"# Optimizer Result Data Structure\n\nNeed a datastructure to pass multiple parameters around more easily:","b1975b8b":"## Cabin Types\n\nTo see what types of cabins there are:","5931dd9d":"## Collect Misclassifications and their Strengths (How far off)\n\nFollowing function compares predictions for validation set vs gold standard in target column for each row in validation set. Stores misclassified row indices in results:","36734c39":"All the lowest loss ones have good accuracy as well. Even if a lower loss is not the highest accuracy in all cases.\n\n# Create Submission Files for Base Classifiers\n\nSo just to make some submissions to try out the leaderboard:","fe4270d1":"## Majority vote ensemble","019dc3bf":"Now would be a good time to stop, but we can actually have some more *fun* by looking at the worst misclassifications. That would be the ones where the model makes the biggest mistakes in predicting high confidence of survivor when not surviving and the other way around.\n\n# A Look at Classifiers Most Confident Mistakes\n\nCounts of misclassifications per classifier:","c3aec0d6":"# Preprocessing\n\n## Helper Functions to Parse Features from Data\n\nFunctions to create features from cabin type, cabin number, and number of cabins a passenger has reserved:","32020763":"## Remove Unneeded Features\n\nSelect the features to use for prediction. Drop the ones not useful (ones used only for processing earlier):","7d1181f0":"### Catboost","3b6924f0":"## Impute Null\/NaN values","58c92334":"Will be predicting survival. Take that as target:","e50fb255":"The above shows the parameters found for the best iteration (lowest loss) by hyperopt, for each classifier, as well as a plots of the most important features for each of the 5 folds as given by each classifier (where the information is available).\n\nOverall, it seems to consistently rank fare, age, family survival, passenger class, and title of Mr (and sex_label) as highest features. I guess higher passenger class meant paying a higher fare, and being located at a higher deck. Or perhaps they were given priority to escape? I am sure more internet searches would help but going with this for now..","720dc6fa":"## Loss over tuning iterations\n\nLeave out Random Forest, because it skews the graph too much. Because the hyperparameters for RF seem to have much bigger impact, causing very high loss vs the other classifiers in some combinations. The loss over iterations then:","140fe3e0":"### Random Forest","13622d91":"## Impute Remaining Null\/NaN","2cb59052":"# Tuning Results","5baf8f59":"Not much data but that is actually good for trying out the approach with fast training times.\n\n## Combine Data for Preprocessing\n\nCombine train and test sets to create new features for both sets at once. Mark them with 1 and 0 to split them back later:","4c40d81e":"# Split Combined Data Back to Train\/Test\n\nNow that all the features are created, split the data back to train and test sets:","652a260b":"Overall, which passengers did all the classifiers agree to misclassify most?","72749a6a":"# Hyperoptimizer classes for Catboos, LGBM, XGBoost, Random Forest\n\nFollowing are classes to run hyperopt optimization and results prediction using Catboost, LGBM, XGBoost, and Random Forest. Later comes the same for Logistic Regression.","f3380088":"## HyperOpt class for Logistic Regression\n\nUse logistic regression as the stacking ensemble classifier, and optimize it for hyperparameters as well:","e00e209e":"## Parse and Combine Titles","6be1e83a":"The above should show how not always having higher accuracy means having lower loss.\n\n## Print Best Iterations per Classifier (smallest loss)\n\nAnd the final goal of checking the ordered list of smallest losses (so best iterations according to this metric..):","d8cc7c4e":"The following title parsing and family survival borrowed from this [kernel](https:\/\/www.kaggle.com\/vincentlugat\/200-lines-randomized-search-lgbm-82-3). Don't know if that is the original source, but thanks all Kagglers :)","f6677de2":"### Catboost","5fe74ea5":"### LGBM","b9c98b9b":"# Ensemble of Trees with HyperOpt tuning\n\nI built this to try develop and try out some utility functions for hyperparameter tuning of some sklearn classifiers. Uses LGBM, Catboost, XGBoost, and Random Forest classifiers to build an ensemble, and stacks those with Logistic Regression. All five have their hyper-parameters tuned with Hyperopt. Besides stacking, also averaging and majority vote ensembles are applied.\n\nAlso features some helper functions to see what are the biggest misclassifications the model makes. All functions also found on my [Github](https:\/\/github.com\/mukatee\/ml-experiments\/tree\/master\/utils).","3009828e":"## One-Hot Encode Categorical Features\n\nOne-hot encode categorical variables (embarked, cabin type, gender):","6b92ae72":"The graph above might vary a bit but generally towards the right side, the fluctuation of the results using the selected hyperparameters should be smaller. Because the search should be focusing more on the part of the hyperparameter value space that gives better results..","af3e9b21":"### Random Forest"}}