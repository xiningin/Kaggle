{"cell_type":{"62af7b93":"code","c3a3b7fa":"code","0e6677ea":"code","1bd9a8bc":"code","ac0e1f2d":"code","9089f136":"code","3f3d8e09":"code","9a56a0b2":"code","0fc1abba":"markdown"},"source":{"62af7b93":"#Basic Headers\nimport numpy as np\nimport pandas as pd\n\n#Understanding the system\nimport os\nprint(os.name)\nprint(os.getcwd())\n\n#Looking at the dataset.\ndf = pd.read_csv(\"..\/input\/Absenteeism_at_work.csv\")\nprint(df, \"\\n\\n\\n\")\nprint(df.describe(), \"\\n\\n\\n\")\nprint(df.info())","c3a3b7fa":"#Mean Shift Clustering\nfrom sklearn.cluster import MeanShift\nfrom sklearn.cluster import MeanShift, estimate_bandwidth\nX = df\n\n#Performing MeanShift Clustering\n#bandwidth = estimate_bandwidth(X)\nms = MeanShift(bandwidth = 2.001, bin_seeding=True).fit(X) #Returns labels for each row(check using len(clustering))\nlabels = ms.labels_  #Retrive the labels for each datapoint\ncluster_centers = ms.cluster_centers_\n\nlabels_unique = np.unique(labels)\nn_clusters_ = len(labels_unique)\n\nprint(\"Estimated cluster centers : \\n\", cluster_centers, \"\\n\")\nprint(\"Number of estimated clusters : %d\" % n_clusters_, \"\\n\")\nprint(\"Labels are : \", labels_unique)","0e6677ea":"#DBSCAN\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\n\n# #############################################################################\n# Compute DBSCAN\ndb = DBSCAN(eps = 8.5, min_samples = 5, metric = 'euclidean').fit(X)\n#db = DBSCAN().fit(X)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n\nprint('Estimated number of clusters: %d' % n_clusters_)","1bd9a8bc":"#KNN\nimport numpy as np  \nimport matplotlib.pyplot as plt  \nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nX = df.iloc[:, :-1].values  \ny = df.iloc[:, 14].values\n\nfrom sklearn.model_selection import train_test_split  \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\nfrom sklearn.preprocessing import StandardScaler  \nscaler = StandardScaler()  \nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)  \nX_test = scaler.transform(X_test)  \n\nfrom sklearn.neighbors import KNeighborsClassifier\n# Calculating error for K values between 1 and 50\nerror = []\nfor i in range(1, 50):  \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error.append(np.mean(pred_i != y_test))\n    \nplt.figure(figsize=(12, 6))  \nplt.plot(range(1, 50), error, color='red', linestyle='dashed', marker='o',  \n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')  \nplt.xlabel('K Value')  \nplt.ylabel('Mean Error')\nplt.show()\n\nindex_min = np.argmin(error)\nclassifier = KNeighborsClassifier(n_neighbors = index_min+1)  \nclassifier.fit(X_train, y_train)  \n\ny_pred = classifier.predict(X_test)  \n\ncount = 0\nfor i in range(len(y_pred)):\n    if(y_pred[i] == y_test[i]):\n        count += 1\n\nprint(\"Accuracy = \", count\/len(y_pred), \"\\n\\n\")\n        \nfrom sklearn.metrics import classification_report, confusion_matrix \nprint(\"Confusion matrix:\")\nprint(confusion_matrix(y_test, y_pred),\"\\n\\n Report:\")  \nprint(classification_report(y_test, y_pred)) ","ac0e1f2d":"#Random Forest\n# Import train_test_split function\nfrom sklearn.model_selection import train_test_split\n\nX = df.iloc[:, :-1].values  \ny = df.iloc[:, 14].values\n\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\n#Scaling the data\nfrom sklearn.preprocessing import StandardScaler  \nscaler = StandardScaler()  \nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)  \nX_test = scaler.transform(X_test) \n\n#Import Random Forest Model\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Calculating error for n_estimators values between 300 and 500\nerror = []\nfor i in range(300, 500):  \n    clf = RandomForestClassifier(n_estimators = i)\n    clf.fit(X_train, y_train)\n    pred_i = clf.predict(X_test)\n    error.append(np.mean(pred_i != y_test))\n    \nplt.figure(figsize=(12, 6))  \nplt.plot(range(300, 500), error, color='red', linestyle='dashed', marker='o',  \n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate Plot')  \nplt.xlabel('n_estimators Value')  \nplt.ylabel('Mean Error')\nplt.show()\n\n#Create a Gaussian Classifier\nindex_min = np.argmin(error)\nclf=RandomForestClassifier(n_estimators = index_min+300)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_test)\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred), \"\\n\\n\")\n\nfrom sklearn.metrics import classification_report, confusion_matrix \nprint(\"Confusion matrix:\")\nprint(confusion_matrix(y_test, y_pred),\"\\n\\n Report:\")  \nprint(classification_report(y_test, y_pred)) ","9089f136":"#SVM\nimport pandas as pd\nfrom sklearn.svm import SVC\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n\ndf = pd.read_csv(\"..\/input\/Absenteeism_at_work.csv\")\n\nX = df.iloc[:, :-1].values  \ny = df.iloc[:, 14].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\n#Scaling the data\nfrom sklearn.preprocessing import StandardScaler  \nscaler = StandardScaler()  \nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)  \nX_test = scaler.transform(X_test) \n\n#####Training, prediction\nsvclassifier = SVC(kernel='sigmoid')  \nsvclassifier.fit(X_train, y_train) \ny_pred = svclassifier.predict(X_test)  \n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred), \"\\n\\n\")\n\nfrom sklearn.metrics import classification_report, confusion_matrix \nprint(\"Confusion matrix:\")\nprint(confusion_matrix(y_test, y_pred),\"\\n\\n Report:\")  \nprint(classification_report(y_test, y_pred)) ","3f3d8e09":"#Some comments on Unsupervised Clustering Algorithms:\n\n'''Here we have used two clustering algorithms, Mean Shift Clustering and DBSCAN.\nThese methods were chosen because the the number of clusters are identified by the algorithms themselves.\nHowever we must understand that clustering algorithms are used to gain more insight about the data and find\nstructure\/patterns within the datapoints if there is any.\n\nClustering methods are not used for prediction or classification purposes and hence there is no meaning in evaluating\nthe performance of such models in tasks such as prediction of hours of absenteeism.\nEven if we were to come up with a way to evaluate the models, their accuracies would be much greater than the accuracy of\nany other classification algorithm as there is a significant decrease in the number of classes while clustering. Also\nsplitting the data for a clustering algorithm into training and testing sets is not meaningful.\n\nHere due to a large number of attributes\/features in the dataset it is hard to visualize the clusters. This inturn makes it\ndifficult to tune the parameters of the model being fit. Untill somekind of dimensionality reduction is not applied on the\ndata it would be hard to come up with a good clustering algorithm.\n\nAs the focus of the assignment was on predection, we have not focused much on the clustering techniques. Nevertheless, we have\ntried performing some basic clustering just to get a feel of how to go about using different algorithms.'''","9a56a0b2":"#Conclusion:\n\n'''Random Forest builds multiple decision trees and merges them together to get a more stable and accurate result.\nThe major advantage of Random Forest is that it can be used for both regression and classification problems. Random Forest\ncan handle categorical variables well and high dimensional spaces. Random Forest adds additional randomness to the model,\nwhile growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best\nfeature among a random subset of features. This results in wide diversity that generally results in a better model. Random Forest\nwill also not overfit the model, as there are enough trees in the forest. Random Forest works very well with a large number of training\nexamples, unlike SVMs, which are inefficient to train in such a case (due to the large time required to train a large number of samples).\n\nSVMs can be used with a non linear kernel when the problem cannot be linearly separable. SVMs can be used for text classification.\nKNN is robust to noisy training data and is effective in case of a large number of training examples. However, in KNN, we would have to\ndetermine the value of K and the type of distance to be used, which is computationally taxing as we would have to find the distances for each query\nto come up with he most optimum distance. Unlike KNN or SVMs, Random Forest works \u201cout of the box\u201d and is therefore popular. '''","0fc1abba":"**Assignment 6 - Kernels in Kaggle<br>\nShailesh Sridhar - 01FB16ECS349<br>\nShashank Prabhakar - 01FB16ECS356<br>\nShrey Tiwari - 01FB16ECS368**<br>"}}