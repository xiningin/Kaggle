{"cell_type":{"dbca02b1":"code","23a72b32":"code","7d352dba":"code","225f202f":"code","37b2ffc4":"code","a250031e":"code","915b497d":"code","98d239d8":"code","e27022fb":"code","5320a3c5":"code","d7dde730":"code","1b6b2544":"code","9a05c944":"code","e0067200":"code","aa0957c6":"code","27611af8":"code","97f04e8b":"code","bca28ae4":"code","e6c7b587":"code","48d37822":"code","be2e1452":"code","29c0818b":"code","ba2f59cc":"code","79fe9df4":"code","c7e8c60b":"code","b8af90e0":"code","8b4dd835":"code","2a484f46":"markdown","a03b4cea":"markdown","1fc63481":"markdown","6e35cc99":"markdown"},"source":{"dbca02b1":"#preprocessing libraries\nimport pandas as pd\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\n","23a72b32":"#Load the dataset and combine the dataset for EDA and preprocessing","7d352dba":"tweet= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","225f202f":"tweet.shape,test.shape","37b2ffc4":"target = tweet[\"target\"]\ntweet.drop([\"target\"],axis=1,inplace=True)\n#test[\"target\"] = 0","a250031e":"df_train = pd.concat([tweet,test])","915b497d":"#convert multiline sentence in to single line sentence\n","98d239d8":"import re\nimport string\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punctuation(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndef remove_pattern(input_txt ):\n    pattern = \"@[\\w]*\"\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n        \n    return input_txt \n\ndef remove_stopWord(text):\n        new_text = [word.lower() for word in text.split() if((word.isalpha()==1) & (word not in stop))]\n        return ' '.join(new_text)\n    \n\n\n\nstop = set(stopwords.words('english')) \ndf_train[\"cleaned_tweet\"] = df_train[\"text\"].map(remove_URL)\ndf_train[\"cleaned_tweet\"] = df_train[\"cleaned_tweet\"].map(remove_html)\ndf_train[\"cleaned_tweet\"] = df_train[\"cleaned_tweet\"].map(remove_emoji)\ndf_train[\"cleaned_tweet\"] = df_train[\"cleaned_tweet\"].map(remove_punctuation)\ndf_train[\"cleaned_tweet\"] = df_train[\"cleaned_tweet\"].map(remove_pattern)\ndf_train[\"cleaned_tweet\"] = df_train[\"cleaned_tweet\"].map(remove_stopWord)\n","e27022fb":"#Tokanized all the cleaaned tweet in our dataset\n#tokenized_tweet = df_train['cleaned_tweet'].apply(lambda x: x.split())\n","5320a3c5":"df_train.head()","d7dde730":"import spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndef lemmatization(text):\n    #Lemmatization:\n    \n    #It is a process of grouping together the inflected forms of a word so they can be analyzed as a single item, identified by the word\u2019s lemma, or dictionary form.\n\n     \n\n    # Load English tokenizer, tagger, \n    # parser, NER and word vectors \n     \n\n    # Process whole documents \n    #text = (\"\"\"My name is Shaurya Uppal. I enjoy writing articles on GeeksforGeeks checkout my other article by going to my profile section.\"\"\") \n\n    doc = nlp(text) \n    sent =''\n    for token in doc: \n      sent +=token.lemma_+' '\n    return sent\n\ndf_train[\"cleaned_tweet\"]=df_train[\"cleaned_tweet\"].map(lemmatization)\n#lemmatization(\"My name is Shaurya Uppal. I enjoying writing articles on GeeksforGeeks checkout my other article by going to my profile section.\")","1b6b2544":"tweet.shape[0]","9a05c944":"train=df_train[:tweet.shape[0]]\ntest=df_train[tweet.shape[0]:]\n","e0067200":"train[\"target\"]= target","aa0957c6":"from wordcloud import WordCloud\nfrom matplotlib import pyplot as plt\nnormal_words =' '.join([text for text in train['cleaned_tweet'][train['target'] == 0]])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","27611af8":"\nnormal_words =' '.join([text for text in train['cleaned_tweet'][train['target'] == 1]])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show() ","97f04e8b":"from sklearn.model_selection import train_test_split\nx_train,x_valid,y_train,y_valid=train_test_split(train.drop([\"target\"],axis=1),train[\"target\"].values,test_size=0.15,random_state=23)","bca28ae4":"x_train.shape","e6c7b587":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n# TF-IDF feature matrix\ntrain_tf = tfidf_vectorizer.fit_transform(x_train['cleaned_tweet'])\nvalid_tf = tfidf_vectorizer.transform(x_valid['cleaned_tweet'])","48d37822":"train_tf.shape,valid_tf.shape","be2e1452":"#Model building libraries\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom keras.layers import LeakyReLU,Dropout","29c0818b":"\nmodel = Sequential()\nmodel.add(Dense(128, kernel_initializer ='glorot_uniform',input_dim=train_tf.shape[1]))\nmodel.add(LeakyReLU(alpha=0.01))\nmodel.add(Dropout(0.50))\nmodel.add(Dense(128, kernel_initializer ='glorot_uniform'))\nmodel.add(LeakyReLU(alpha=0.01))\nmodel.add(Dropout(0.50))\nmodel.add(Dense(output_dim = 1, kernel_initializer ='glorot_uniform', activation = 'sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adamax',metrics=['acc'])","ba2f59cc":"model.summary()","79fe9df4":"from keras.callbacks import ModelCheckpoint","c7e8c60b":"# checkpoint\nfilepath=\"best_weights.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n# Fit the model\nmodel.fit(train_tf, y_train, validation_split=0.15, epochs=100, batch_size=10, callbacks=callbacks_list, verbose=1)","b8af90e0":"test_tf = tfidf_vectorizer.transform(test['cleaned_tweet'])","8b4dd835":"y_pre=model.predict(test_tf)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':test['id'].values.tolist(),'target':y_pre})\nsub.to_csv('submission.csv',index=False)","2a484f46":"\n\nAs You know this is Tweets dataset. It may possibly contain following things.\n\n\n1) URl(it contains Hyper links while user try to reference somethig)\n\n2) Html tags (possibly while coping from web source's)\n\n3) emoji (emoticons, symbols and pictographs etc)\n\n4) punctuation(the marks, such as full stop, comma, and brackets, used in writing to separate sentences and their elements and to clarify meaning.)\n\n\n5)@User tag( we have no requirement of @user )","a03b4cea":"# Start Preprocessing the Dataset","1fc63481":"\nWe are challenged to build a machine learning model that predicts which Tweets\nare about real disasters and which one\u2019s aren\u2019t. \n\nYou\u2019ll have access to a dataset of 10,000 tweets that were hand classified","6e35cc99":"# Problem Statment"}}