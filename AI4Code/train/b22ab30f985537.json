{"cell_type":{"0dc439cc":"code","ada99a60":"code","2ccee079":"code","3a92d42b":"code","56f02616":"code","fd06e9bb":"code","f9f274eb":"code","911f489c":"code","ba800e84":"code","b5bd60b0":"code","0d359cf7":"code","003e2f71":"code","08d74c15":"code","7bf425d9":"code","b1a77715":"code","e061eef9":"code","8ee4b8f2":"code","388484f8":"markdown","3337d457":"markdown","b6f0912d":"markdown","d6fcdd98":"markdown","08e26a21":"markdown","0735ab27":"markdown","a8b1dc99":"markdown","b881535a":"markdown","6737aa2d":"markdown","29280254":"markdown","5402ef78":"markdown","8e50545a":"markdown","76e7078c":"markdown","8285b5c0":"markdown","b95322b5":"markdown","324cff61":"markdown"},"source":{"0dc439cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ada99a60":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nsubmission_df = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","2ccee079":"df = pd.concat([train, test], axis=0)\ndf = df.set_index('PassengerId')\ndf.info()","3a92d42b":"df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n","56f02616":"import seaborn as sns\nsns.set_theme(font_scale=1.5)\nsns.displot(data=df,hue='Sex', x='Age',row='Survived',col='Pclass')","fd06e9bb":"inds = df[df['Age']>15].index\ndf.loc[inds, 'Kind'] = 'Adult'\ninds = df[df['Age']<=15].index\ndf.loc[inds, 'Kind'] = 'Child'\nsns.catplot(data=df,hue='Sex', x='Kind',row='Survived',col='Pclass', kind='count')","f9f274eb":"df['Embarked'] = df['Embarked'].fillna('S')\nsns.catplot(data=df,hue='Sex', x='Embarked',col='Survived',kind='count')","911f489c":"sns.catplot(data=df,x='Fare', col='Pclass',row='Survived',kind='violin',cut=0, bw=.2)","ba800e84":"df = df.drop('Fare', axis=1)","b5bd60b0":"def fillna_using_knn_imputer(df):\n    from sklearn.impute import KNNImputer\n    import pandas as pd\n    # initialize imputer:\n    imputer = KNNImputer(n_neighbors=5,\n                         weights='uniform',\n                         metric='nan_euclidean')\n    # use integer coding on the string cols to be filled:\n    df['Sex'] = df['Sex'].factorize()[0]\n    df['Embarked'] = df['Embarked'].factorize()[0]\n    df['Kind'] = df['Kind'].factorize()[0]\n    #  divide to train, test and drop survived so it won't be filled:\n    X_train = df[~df['Survived'].isnull()].drop('Survived', axis=1)\n    X_test = df[df['Survived'].isnull()].drop('Survived', axis=1)\n    # fit_transform on X_train and transform on test:\n    X_train_trans = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n    X_test_trans = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns, index=X_test.index)\n    dff = pd.concat([X_train_trans, X_test_trans], axis=0)\n    dff['Survived'] = df['Survived']\n    dff = dff.sort_index()\n    return dff\n\ndf = fillna_using_knn_imputer(df)\ndf.info()","0d359cf7":"def encode_cols(df, cols=['Pclass', 'Embarked']):\n    import pandas as pd\n    for col in cols:\n        dumm = pd.get_dummies(data=df[col], prefix=col)\n        df = pd.concat([df, dumm], axis=1)\n        df = df.drop(col, axis=1)\n    return df\n\n\ndef scale_all_features(df):\n    from sklearn.preprocessing import MinMaxScaler\n    import pandas as pd\n    X = df.drop('Survived', axis=1)\n    scaler = MinMaxScaler()\n    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n    df = pd.concat([X, df['Survived']], axis=1)\n    return df, scaler\n\ndf = encode_cols(df)\ndf, _ = scale_all_features(df)\ndf.info()","003e2f71":"def split_to_train_test_X_y(df):\n    X = df[~df['Survived'].isna()].drop('Survived', axis=1)\n    y = df[~df['Survived'].isna()]['Survived']\n    X_test = df[df['Survived'].isna()].drop('Survived', axis=1)\n    return X, y, X_test\nX, y, X_test = split_to_train_test_X_y(df)","08d74c15":"class ML_Classifier_Switcher(object):\n\n    def pick_model(self, model_name):\n        \"\"\"Dispatch method\"\"\"\n        self.param_grid = None\n        method_name = str(model_name)\n        # Get the method from 'self'. Default to a lambda.\n        method = getattr(self, method_name, lambda: \"Invalid ML Model\")\n        return method()\n\n    def SVM(self):\n        from sklearn.svm import SVC\n        import numpy as np\n        self.param_grid = {'kernel': ['rbf', 'sigmoid', 'linear'],\n                           'C': np.logspace(-2, 2, 10),\n                           'gamma': np.logspace(-5, 1, 14)}\n\n        return SVC()\n\n    def XGR(self):\n        import numpy as np\n        from xgboost import XGBClassifier\n        self.param_grid = {'gamma': np.logspace(-5, 1, 7),\n                           'subsample': [0.5, 0.75, 1.0],\n                           'colsample_bytree': [0.5, 0.75, 1.0],\n                           'eta': [0.1, 0.5, 0.9],\n                           'max_depth': [3, 5]}\n        return XGBClassifier(random_state=42, nthread=7, use_label_encoder=False,\n                             eval_metric='error', tree_method = \"hist\")\n\n    def RF(self):\n        from sklearn.ensemble import RandomForestClassifier\n        # import numpy as np\n        self.param_grid = {\n            'n_estimators': [50, 100, 200, 300],\n            'max_features': ['auto'],\n            'criterion': ['entropy'],\n            'max_depth': [5, 10],\n            'min_samples_split': [5],\n            'min_samples_leaf': [1]\n        }\n\n        return RandomForestClassifier(random_state=42, n_jobs=-1)\n\n    def LR(self):\n        from sklearn.linear_model import LogisticRegression\n        self.param_grid = {'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n                           'penalty': ['l2'],\n                           'C': [100, 10, 1.0, 0.1, 0.01]}\n        return LogisticRegression(n_jobs=None, random_state=42)\n\n    def KNN(self):\n        from sklearn.neighbors import KNeighborsClassifier\n        self.param_grid = {\n            'n_neighbors': list(range(1, 5)),\n            'weights': ['uniform', 'distance'],\n            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n            'leaf_size': list(range(1, 10)),\n            'p': [1, 2]\n        }\n        return KNeighborsClassifier()","7bf425d9":"\ndef cross_validate(X, y, model_name='RF', cv=5, scoring='accuracy',\n                   gridsearch=True):\n    \"\"\"Options for model_name are : RF, LR, KNN and SVM\"\"\"\n    import pandas as pd\n    from sklearn.model_selection import cross_val_score\n    from sklearn.model_selection import GridSearchCV\n    # we instantiate an ML model switcher:\n    switcher = ML_Classifier_Switcher()\n    model = switcher.pick_model(model_name)\n    # we do gridsearch in the HP space and pick the best model:\n    if gridsearch:\n        gr = GridSearchCV(model, switcher.param_grid,\n                          scoring=scoring, cv=cv, n_jobs=-1)\n        gr.fit(X, y)\n        model = gr.best_estimator_\n    # we do another CV with the best model and report the scores via cvr dataframe:\n    cvr = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n    cvr = pd.DataFrame(cvr).T\n    cvr.index = [model_name]\n    cvr.columns = ['fold_{}'.format(x+1) for x in cvr.columns]\n    cvr['mean'] = cvr.mean(axis=1)\n    cvr['std'] = cvr.std(axis=1)\n    return cvr, model","b1a77715":"def cross_validate_models(X, y, cv=5, scoring='accuracy'):\n    import pandas as pd\n    models = ['LR', 'KNN', 'SVM', 'RF']\n    cvrs = []\n    bests = []\n    for model_name in models:\n        print('Optimizing {} model'.format(model_name))\n        cvr, best = cross_validate(X, y, model_name=model_name, cv=cv, scoring=scoring,\n                                   gridsearch=True)\n        cvrs.append(cvr)\n        bests.append(best)\n        cvr = pd.concat(cvrs, axis=0)\n    return cvr, bests","e061eef9":"cvr, bests = cross_validate_models(X,y)\nprint(cvr)","8ee4b8f2":"def predict_on_test(X_test, model, submission_df, target='Survived'):\n    preds_test = model.predict(X_test)\n    submission_df.loc[:, target] = [int(x) for x in preds_test]\n    submission_df.to_csv('submission.csv', index=False)\n    return\npredict_on_test(X_test, bests[-1], submission_df)","388484f8":"So, top panels are passengers who did not survive, and it seems that are mostly males from higher passenger classes.\nSurvivers are mostly females (bottom panels). so Pclass and Sex are strong indicators to survival, Age less so, although it seems that children as a whole (Age<=15 or so) survived better than adults. Let's add an adult feature:","3337d457":"Looks ok, now before we model it, we'll preprocces.","b6f0912d":"It looks as though passenger pay more to be on first class but it does not help them too much with the survival, so we'll drop this feature (Fare).","d6fcdd98":"Now, we'd like to test various model's success at predicting the positive class (Survived). We introduce a cross-validation function so that we could employ hyper-parameters tuning for each model. For this option, i've written an new ML model class which is able to switch between ML models and their HP param grid:","08e26a21":"So i've implemented 5 models but the XGR takes too long to optimize so we'll skip it. Now, we'll define the cross validation function:","0735ab27":"Cabin will be dropped since it has too many NaNs, Embarked will be filled (but it won't make a lot of differernce), and Age will be challanging to fill, i'll try KNN imputer later on. Also, Ticket and Name will be dropped as well since I'm not going NLP on them:-). For now, let's do some EDA.","a8b1dc99":"<a id=\"modelling\"><\/a>\n## 5. Modelling\nFirst we split the data to train and test again:","b881535a":"<a id=\"eda\"><\/a>\n## 2. EDA","6737aa2d":"First we combine the train and test datasets and check for NaNs, where the NaNs in the target variable (Survived) will distinguish between the train and test sets:","29280254":"<a id=\"nans\"><\/a>\n## 3. Filling missing values\nNow, let's define a function for filling the missing values in Age (and in Embarked):","5402ef78":"So all of the children in the Pclass 1 and 2 survived, maybe it will help a bit, so we'll keep this feature, next we look at Embarked:","8e50545a":"<a id=\"preprocess\"><\/a>\n## 4. Preprocessing\nwe'll use One Hot Encoding and scale all features to [0,1] using MinMax scaler:","76e7078c":"So, RF is the best with 0.83 accuracy, let's plug it in the predict function and submit:","8285b5c0":"So, males that embarked from Southampton survived the less (maybe they were in the 3rd class?), but ~75% of females from Southhampton survived and for the people embarked from Cherbourg the relashonship revereses. So Embarked could be important as well. Let's look at Fare next:","b95322b5":"<a id=\"dataload\"><\/a>\n## 1. Load datasets and missing value analysis","324cff61":"And another function to run the cross_validate in a loop for all models:"}}