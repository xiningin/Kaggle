{"cell_type":{"d157462c":"code","395c273e":"code","ad867abc":"code","a538ea66":"code","2bd1aaa7":"code","424c15fe":"code","cbadb8d1":"code","1efe7040":"code","183146c2":"code","0a875596":"code","d5da62ba":"code","aec5d80c":"code","7e799fec":"code","060c0231":"code","3fbd4f08":"code","24452637":"code","b2d5275b":"code","c955e714":"code","20939b70":"code","ac79abc7":"code","e295fe7a":"code","f0efd098":"code","275f3a09":"code","ea7828df":"code","6654bb88":"code","ee1094ab":"code","1aafa27e":"code","37370dd5":"code","42bdd535":"code","e7171eb9":"code","e93a0035":"code","f87f08d0":"code","f8c6d6f5":"code","cffe893b":"code","4b40b038":"code","b6f3e03b":"code","6e2cf1a8":"code","aa92e9d7":"code","614fd0db":"code","2dea6fc6":"code","9dd6a086":"code","257151ea":"code","6e2beef9":"code","30eff8b6":"code","80b3b81d":"code","09056b49":"code","41bdca3d":"code","0b49c1de":"code","a7a54bcb":"code","7dd6a3f3":"code","98b42170":"code","d95bae0e":"code","5cd6df75":"code","dc783c0a":"code","a61d894d":"code","ef41ac4d":"markdown","771155fa":"markdown","baaf8e4b":"markdown","78d6b964":"markdown","1ffc216d":"markdown","c6c9af88":"markdown","632593b0":"markdown","32db5f6e":"markdown","0d685b2b":"markdown","38d25072":"markdown","2fd83c5d":"markdown","156fc651":"markdown","41a47a1d":"markdown","840b4546":"markdown","e7ab2165":"markdown","1c4edd5f":"markdown","55efae64":"markdown","3e8722d4":"markdown","9e90bcaf":"markdown","5715513b":"markdown","e80ce560":"markdown","dc441e54":"markdown","550ce706":"markdown","9a1514cd":"markdown","54cb4d55":"markdown","179ae70a":"markdown","f9c8e1b6":"markdown"},"source":{"d157462c":"!pip install IQA_pytorch #For SSIM Score","395c273e":"!pip install torchsummaryX","ad867abc":"!pip install pytorch-msssim","a538ea66":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torchvision\nfrom torch.optim import *\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport torch.nn.functional as F\nfrom IQA_pytorch import DISTS, utils\nfrom pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\nfrom torchsummaryX import summary\n\nimport math\nimport time\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport albumentations\nimport albumentations.pytorch\nfrom matplotlib import pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import font_manager, rc\nfrom IPython import display\nimport random\nimport glob\nimport os\nfrom os import listdir\nfrom os.path import isfile, join\nimport warnings\nimport sys\nfrom tqdm import tqdm\nimport pickle\nimport gc\nimport random\nimport urllib.request\n\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Version of Torch : {0}\".format(torch.__version__))\nprint(\"Version of TorchVision : {0}\".format(torchvision.__version__))","2bd1aaa7":"gc.collect()\ntorch.cuda.empty_cache()","424c15fe":"%matplotlib inline\n\nplt.rcParams['axes.unicode_minus'] = False\nfontpath = \"..\/input\/koreanfont\/NanumBrush.ttf\"\nfontprop = font_manager.FontProperties(fname=fontpath)\n\nplt.rcParams[\"animation.html\"] = \"jshtml\"\nplt.rcParams['figure.dpi'] = 150  \nplt.ioff()","cbadb8d1":"# Device\nUSE_CUDA = torch.cuda.is_available()\n\nprint(\"Device : {0}\".format(\"GPU\" if USE_CUDA else \"CPU\"))\ndevice = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\ncpu_device = torch.device(\"cpu\")\n\nDEBUG = False\n\nRANDOM_SEED = 2004\n\n# Train\nonly_d_train_step = 5\n\nstart_epoch_only_d = 0\nall_epochs_only_d = 0\nstart_epoch = 0\nall_epochs = 1\nbatch_size = 14\n\nlrG = 0.0002\nlrD = 0.0002\nbeta1 = 0.5\nbeta2 = 0.999\n\nL1lambda = 100\nGAMMA = 0.65\nMODIFIER = 5\nALPHA = 1 #0.83\n\nINPUT_NUM = 4\nTIME_STEP = 4\nTEST_TIME_STEP = 6\nIMAGE_SIZE = 128\n\npatch = (1,256\/\/2**4,256\/\/2**4)\n\n# Path\nDATASET1_PATH = '..\/input\/the-cloudcast-dataset'\n\n# Checkpoint\nUSE_CHECKPOINT = False\n\nOLD_PATH = '..\/input\/fastoraclegan-multiple-input'\nOLD_GENERATOR_MODEL = os.path.join(OLD_PATH, 'Generator.pth')\nOLD_DISCRIMINATOR_MODEL = os.path.join(OLD_PATH, 'Discriminator.pth')\nOLD_G_LOSS = os.path.join(OLD_PATH, 'gloss.txt')\nOLD_D_LOSS = os.path.join(OLD_PATH, 'dloss.txt')","1efe7040":"torch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed_all(RANDOM_SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\n\nprint('Random Seed : {0}'.format(RANDOM_SEED))","183146c2":"def log(text):\n    global DEBUG\n    if DEBUG:\n        print(text)","0a875596":"def torch_tensor_to_plt(img):\n    img = img.detach().numpy()[0]\n    img = np.transpose(img, (1, 2, 0))\n    return img ","d5da62ba":"def show_video_in_jupyter_nb(width, height, video_url):\n    from IPython.display import HTML\n    return HTML(\"\"\"<video width=\"{}\" height=\"{}\" controls>\n    <source src={} type=\"video\/mp4\">\n    <\/video>\"\"\".format(width, height, video_url))","aec5d80c":"def plt_image_animation(frames, update_func):\n    fig, ax = plt.subplots(figsize=(4,4))\n    plt.axis('off')\n    anim = animation.FuncAnimation(fig, update_func, frames=frames)\n    video = anim.to_html5_video()\n    html = display.HTML(video)\n    display.display(html)\n    plt.close()","7e799fec":"plt_image_animation(15, lambda t : plt.imshow(np.load(join(DATASET1_PATH, '2017M01', '{0}.npy'.format(t))), cmap='gray'))","060c0231":"transformer = transforms.Compose([transforms.ToTensor(),\n                                  torchvision.transforms.Resize(IMAGE_SIZE),\n                                  transforms.Normalize((0.5), (0.5)), #GrayScale\n                                 ])\n","3fbd4f08":"nowpath = \"\"\n\nclass TimeStepImageDataset(Dataset):\n    def __init__(self, date, input_num, time_step, transform=None):\n        self.date = date\n        self.input_num = input_num\n        self.time_step = time_step\n        self.transformer = transform\n        self.file = []\n        \n        file_list = glob.glob(join(self.date, '*'))\n        self.file = [file for file in file_list if (file.endswith(\".npy\") and not file.endswith('TIMESTAMPS.npy'))]\n        \n    def __len__(self):\n        return len(self.file)-self.time_step-self.input_num\n    \n    def transform(self, image):\n        if self.transformer:\n            return self.transformer(image)\n        else :\n            return image\n\n    def __getitem__(self, idx):\n        global nowpath\n        \n        log(join(self.date, str(idx)+'.npy'))\n        X_list = []\n        for i in range(0, self.input_num):\n            X_list.append(self.transform(np.load(join(self.date, str(idx+i)+'.npy'))).unsqueeze(0))\n        X = torch.cat(X_list)\n        nowpath = join(self.date, str(idx)+'.npy')\n\n        Y_list = []\n        for i in range(self.input_num, self.input_num+self.time_step):\n            Y_list.append(self.transform(np.load(join(self.date, str(idx+i)+'.npy'))).unsqueeze(0))\n        Y = torch.cat(Y_list)    \n\n        return X, Y","24452637":"DATASET1_DIRS = glob.glob(join(DATASET1_PATH, '*'))\n\nrandom.shuffle(DATASET1_DIRS)\n\ntraindatasetlist = []\nfor ind, name in enumerate(DATASET1_DIRS[:20]):\n    traindatasetlist.append(TimeStepImageDataset(name, INPUT_NUM, TIME_STEP, transform=transformer))\ntrain_dataset = torch.utils.data.ConcatDataset(traindatasetlist)\n\ntestdatasetlist = []\nfor ind, name in enumerate(DATASET1_DIRS[20:]):\n    testdatasetlist.append(TimeStepImageDataset(name, INPUT_NUM, TEST_TIME_STEP, transform=transformer))\ntest_dataset = torch.utils.data.ConcatDataset(testdatasetlist)","b2d5275b":"train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataloader_bs1_shuffle = DataLoader(test_dataset, batch_size=1, shuffle=True) \ntest_dataloader_bs1_noshuffle = DataLoader(test_dataset, batch_size=1, shuffle=False) ","c955e714":"def ShowDatasetImage(x, y):\n    grid = torchvision.utils.make_grid(y)\n    \n    fig = plt.figure(figsize=(8, 2.5))\n    #plt.imshow(torch_tensor_to_plt(x.unsqueeze(0)), cmap='gray')\n    plt.axis('off')\n    plt.title('Input', fontproperties=fontprop)\n    for i in range(1, INPUT_NUM+1):\n        ax = fig.add_subplot(1, INPUT_NUM, i)\n        ax.axis('off')\n        ax.imshow(torch_tensor_to_plt(x[i-1].unsqueeze(0)), cmap='gray')\n        #ax.set_title('after {0} minutes'.format(15*i), fontproperties=fontprop)\n    plt.show()   \n\n    \n    fig = plt.figure(figsize=(8, 2.5))\n    plt.title('Real Weather Image', fontproperties=fontprop)\n    plt.axis('off')\n    for i in range(1, TIME_STEP+1):\n        ax = fig.add_subplot(1, TIME_STEP, i)\n        ax.axis('off')\n        ax.imshow(torch_tensor_to_plt(y[i-1].unsqueeze(0)), cmap='gray')\n        ax.set_title('after {0} minutes'.format(15*i), fontproperties=fontprop)\n    plt.show()\n\n    del x, y","20939b70":"for ind, (x, y) in enumerate(train_dataset):\n    if ind != 0:\n        continue\n    ShowDatasetImage(x, y)\n    break","ac79abc7":"class EmptyLayer(nn.Module):\n    def __init__(self):\n        super(EmptyLayer, self).__init__()\n\n    def forward(self, x):\n        return x","e295fe7a":"class depthwise_separable_conv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n        super(depthwise_separable_conv, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels, bias=False)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n\n    def forward(self, x):\n        out = self.depthwise(x)\n        out = self.pointwise(out)\n        return out","f0efd098":"class UNetDown(nn.Module):\n    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0, dsconv=True):\n        super().__init__()\n    \n        if dsconv:\n            layers = [depthwise_separable_conv(in_channels, out_channels, 4, stride=2, padding=1, bias=False)]\n        else :\n            layers = [nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False)]\n            \n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_channels)),\n\n        layers.append(nn.LeakyReLU(0.2))\n\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.down = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.down(x)\n        return x","275f3a09":"class UNetUp(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout=0.0, use_skip_conv=True):\n        super().__init__()\n\n        layers = [\n            nn.ConvTranspose2d(in_channels, out_channels,4,2,1,bias=False),\n            nn.InstanceNorm2d(out_channels),\n            nn.LeakyReLU()\n        ]\n\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.up = nn.Sequential(*layers)\n        self.use_skip_conv = use_skip_conv\n        \n        if use_skip_conv:\n            self.skip_conv = nn.Conv2d(out_channels, out_channels, 1, bias=False) #Skip Convolution\n\n    def forward(self,x,skip):\n        x = self.up(x)\n        if self.use_skip_conv:\n            skip = self.skip_conv(skip)\n        x = torch.cat((x, skip),1)\n        return x","ea7828df":"class GeneratorUNet(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1):\n        super().__init__()\n\n        self.down1 = UNetDown(in_channels, 64, normalize=False, dsconv=False)\n        self.down2 = UNetDown(64,128)                 \n        self.down3 = UNetDown(128,256)               \n        self.down4 = UNetDown(256,512,dropout=0.5) \n        self.down5 = UNetDown(512,512,dropout=0.5)      \n        self.down6 = UNetDown(512,512,dropout=0.5)             \n        #self.down7 = UNetDown(512,512,dropout=0.5)              \n        self.down8 = UNetDown(512,512,normalize=False,dropout=0.5, dsconv=False)\n\n        #self.up1 = UNetUp(512,512,dropout=0.5)\n        self.up2 = UNetUp(1024\/\/2,512,dropout=0.5)\n        self.up3 = UNetUp(1024,512,dropout=0.5)\n        self.up4 = UNetUp(1024,512,dropout=0.5)\n        self.up5 = UNetUp(1024,256)\n        self.up6 = UNetUp(512,128)\n        self.up7 = UNetUp(256,64)\n        self.up8 = nn.Sequential(\n            nn.ConvTranspose2d(128,out_channels,4,stride=2,padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down8(d6)\n        u1 = d7\n        u2 = self.up2(u1,d6)\n        u3 = self.up3(u2,d5)\n        u4 = self.up4(u3,d4)\n        u5 = self.up5(u4,d3)\n        u6 = self.up6(u5,d2)\n        u7 = self.up7(u6,d1)\n        u8 = self.up8(u7)\n        \n        return u8","6654bb88":"def _make_divisible(v, divisor, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor \/ 2) \/\/ divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef conv_3x3_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.identity = stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.identity:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, num_classes, width_mult=1.):\n        super(Discriminator, self).__init__()\n        # setting of inverted residual blocks\n        self.cfgs = [\n            # t, c, n, s\n            [1,  16, 1, 1],\n            [6,  24, 2, 2],\n            [6,  32, 3, 2],\n            [6,  64, 4, 2],\n            [6,  96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        input_channel = _make_divisible(32 * width_mult, 4 if width_mult == 0.1 else 8)\n        layers = [conv_3x3_bn(INPUT_NUM+1, input_channel, 2)]\n        # building inverted residual blocks\n        block = InvertedResidual\n        for t, c, n, s in self.cfgs:\n            output_channel = _make_divisible(c * width_mult, 4 if width_mult == 0.1 else 8)\n            for i in range(n):\n                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t))\n                input_channel = output_channel\n        self.features = nn.Sequential(*layers)\n        # building last several layers\n        output_channel = _make_divisible(1280 * width_mult, 4 if width_mult == 0.1 else 8) if width_mult > 1.0 else 1280\n        self.conv = conv_1x1_bn(input_channel, output_channel)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        \n        self.d_classifier = nn.Linear(output_channel, 1)\n        self.frame_classifier = nn.Linear(output_channel, num_classes)\n        self.sigmoid = nn.Sigmoid()\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.conv(x)\n        x = self.avgpool(x)\n        feature = x.view(x.size(0), -1)\n        dresult = self.d_classifier(feature) #real or fate\n        frame = self.frame_classifier(feature) #What time step?\n        #x = self.sigmoid(x)\n        return (dresult, frame)\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. \/ n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()","ee1094ab":"def weights_init(m):\n    classname = m.__class__.__name__\n    if type(m) == nn.Conv2d:\n        m.weight.data.normal_(0.0, 0.02)\n    elif type(m) == nn.BatchNorm2d:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)","1aafa27e":"Generator = GeneratorUNet(in_channels=INPUT_NUM).to(device)\nDiscriminator = Discriminator(num_classes=TIME_STEP).to(device) \n\nsummary_g = Generator.apply(weights_init)\nsummary_d = Discriminator.apply(weights_init)","37370dd5":"summary(Generator, torch.rand((batch_size, INPUT_NUM, IMAGE_SIZE, IMAGE_SIZE)).to(device))","42bdd535":"summary(Discriminator, torch.rand((batch_size, INPUT_NUM+1, IMAGE_SIZE, IMAGE_SIZE)).to(device))","e7171eb9":"optimizerG = Adam(Generator.parameters(), lr=lrG, betas=(beta1, beta2))\noptimizerD = Adam(Discriminator.parameters(), lr=lrD, betas=(beta1, beta2))","e93a0035":"img_list = []\nG_loss = []\nD_loss = []\n\nFAKE_LABEL = 0.0\nREAL_LABEL = 1.0","f87f08d0":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):    \n        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n\n        pt = torch.exp(-ce_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss","f8c6d6f5":"class MS_SSIM_L1_LOSS(nn.Module):\n    # Have to use cuda, otherwise the speed is too slow.\n    def __init__(self, gaussian_sigmas=[0.5, 1.0, 2.0, 4.0, 8.0],\n                 data_range = 1.0,\n                 K=(0.01, 0.03),\n                 alpha=0.025,\n                 compensation=200.0,\n                 cuda_dev=0,):\n        super(MS_SSIM_L1_LOSS, self).__init__()\n        self.DR = data_range\n        self.C1 = (K[0] * data_range) ** 2\n        self.C2 = (K[1] * data_range) ** 2\n        self.pad = int(2 * gaussian_sigmas[-1])\n        self.alpha = alpha\n        self.compensation=compensation\n        filter_size = int(4 * gaussian_sigmas[-1] + 1)\n        g_masks = torch.zeros((3*len(gaussian_sigmas), 1, filter_size, filter_size))\n        for idx, sigma in enumerate(gaussian_sigmas):\n            # r0,g0,b0,r1,g1,b1,...,rM,gM,bM\n            g_masks[3*idx+0, 0, :, :] = self._fspecial_gauss_2d(filter_size, sigma)\n            g_masks[3*idx+1, 0, :, :] = self._fspecial_gauss_2d(filter_size, sigma)\n            g_masks[3*idx+2, 0, :, :] = self._fspecial_gauss_2d(filter_size, sigma)\n        #self.g_masks = g_masks.cuda(cuda_dev)\n        self.g_masks = g_masks.to(device)\n\n    def _fspecial_gauss_1d(self, size, sigma):\n        \"\"\"Create 1-D gauss kernel\n        Args:\n            size (int): the size of gauss kernel\n            sigma (float): sigma of normal distribution\n        Returns:\n            torch.Tensor: 1D kernel (size)\n        \"\"\"\n        coords = torch.arange(size).to(dtype=torch.float)\n        coords -= size \/\/ 2\n        g = torch.exp(-(coords ** 2) \/ (2 * sigma ** 2))\n        g \/= g.sum()\n        return g.reshape(-1)\n\n    def _fspecial_gauss_2d(self, size, sigma):\n        \"\"\"Create 2-D gauss kernel\n        Args:\n            size (int): the size of gauss kernel\n            sigma (float): sigma of normal distribution\n        Returns:\n            torch.Tensor: 2D kernel (size x size)\n        \"\"\"\n        gaussian_vec = self._fspecial_gauss_1d(size, sigma)\n        return torch.outer(gaussian_vec, gaussian_vec)\n\n    def forward(self, x, y):\n        b, c, h, w = x.shape\n        mux = F.conv2d(x, self.g_masks, groups=1, padding=self.pad)\n        muy = F.conv2d(y, self.g_masks, groups=1, padding=self.pad)\n\n        mux2 = mux * mux\n        muy2 = muy * muy\n        muxy = mux * muy\n\n        sigmax2 = F.conv2d(x * x, self.g_masks, groups=1, padding=self.pad) - mux2\n        sigmay2 = F.conv2d(y * y, self.g_masks, groups=1, padding=self.pad) - muy2\n        sigmaxy = F.conv2d(x * y, self.g_masks, groups=1, padding=self.pad) - muxy\n\n        # l(j), cs(j) in MS-SSIM\n        l  = (2 * muxy    + self.C1) \/ (mux2    + muy2    + self.C1)  # [B, 15, H, W]\n        cs = (2 * sigmaxy + self.C2) \/ (sigmax2 + sigmay2 + self.C2)\n\n        lM = l[:, -1, :, :] * l[:, -2, :, :] * l[:, -3, :, :]\n        PIcs = cs.prod(dim=1)\n\n        loss_ms_ssim = 1 - lM*PIcs  # [B, H, W]\n\n        loss_l1 = F.l1_loss(x, y, reduction='none')  # [B, 3, H, W]\n        # average l1 loss in 3 channels\n        gaussian_l1 = F.conv2d(loss_l1, self.g_masks.narrow(dim=0, start=-3, length=3),\n                               groups=1, padding=self.pad).mean(1)  # [B, H, W]\n\n        loss_mix = self.alpha * loss_ms_ssim + (1 - self.alpha) * gaussian_l1 \/ self.DR\n        loss_mix = self.compensation*loss_mix\n\n        return loss_mix.mean()","cffe893b":"class VGGPerceptualLoss(torch.nn.Module):\n    def __init__(self, alpha, device, resize=True, loss=MS_SSIM_L1_LOSS(), perloss=nn.L1Loss()):\n        super(VGGPerceptualLoss, self).__init__()\n        self.alpha = alpha\n        self.loss = loss\n        self.perloss = perloss\n        self.device = device\n        blocks = []\n        blocks.append(torchvision.models.vgg16(pretrained=True).features[:4].eval().to(self.device))\n        blocks.append(torchvision.models.vgg16(pretrained=True).features[4:9].eval().to(self.device))\n        blocks.append(torchvision.models.vgg16(pretrained=True).features[9:16].eval().to(self.device))\n        blocks.append(torchvision.models.vgg16(pretrained=True).features[16:23].eval().to(self.device))\n        for bl in blocks:\n            for p in bl.parameters():\n                p.requires_grad = False\n        self.blocks = torch.nn.ModuleList(blocks)\n        self.transform = torch.nn.functional.interpolate\n        self.resize = resize\n        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(self.device))\n        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(self.device))\n\n    def forward(self, input, target, feature_layers=[0, 1, 2, 3], style_layers=[]):\n        oinput = input.to(self.device).clone()\n        otarget = target.to(self.device).clone()\n        \n        if input.shape[1] != 3:\n            input = input.repeat(1, 3, 1, 1)\n            target = target.repeat(1, 3, 1, 1)\n        input = (input.to(self.device)-self.mean) \/ self.std\n        target = (target.to(self.device)-self.mean) \/ self.std\n        if self.resize:\n            input = self.transform(input, mode='bilinear', size=(224, 224), align_corners=False)\n            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n        loss = 0.0\n        x = input\n        y = target\n        for i, block in enumerate(self.blocks):\n            x = block(x)\n            y = block(y)\n            if i in feature_layers:\n                loss += self.perloss(x, y)\n            if i in style_layers:\n                act_x = x.reshape(x.shape[0], x.shape[1], -1)\n                act_y = y.reshape(y.shape[0], y.shape[1], -1)\n                gram_x = act_x @ act_x.permute(0, 2, 1)\n                gram_y = act_y @ act_y.permute(0, 2, 1)\n                loss += self.perloss(gram_x, gram_y)\n        loss *= (1-self.alpha)\/(len(feature_layers)+len(style_layers))\n        loss += self.loss(oinput, otarget)*self.alpha\n        return loss","4b40b038":"l1loss = nn.L1Loss()\nl2loss = nn.MSELoss()\nsmoothl1loss = nn.SmoothL1Loss()\n\nms_ssim_l1_loss = MS_SSIM_L1_LOSS()\nvgg_perceptual_loss = VGGPerceptualLoss(ALPHA, device)\n\nbceloss = nn.BCEWithLogitsLoss() #nn.BCELoss()\nceloss = nn.CrossEntropyLoss()\nfocalloss = FocalLoss()","b6f3e03b":"def generator_error(netG, netD, sketch, real, real_label, fake_label, gamma=0.0, b_size=batch_size):\n    def G_error(iter_input, G_output, real, D_output, D_target):\n        return smoothl1loss((G_output-iter_input), (real-iter_input))*L1lambda + focalloss(D_output[1], D_target) + l2loss(D_output[0].view(-1), real_label)\n    \n    next_input = sketch\n    error = None\n    \n    real_list = []\n    for i in range(TIME_STEP):\n        real_list.append(real[:,i,:,:,:])\n    \n    for ind, y in enumerate(real_list):\n        iter_input = next_input[:, -1, :, :].clone().detach()\n        G_output = netG(next_input)\n        next_input = G_output.clone().detach()\n        if INPUT_NUM > 1:\n            next_input = torch.cat((sketch[:, 1:, :, :].clone().detach(), next_input), dim=1)\n        else :\n            next_input = G_output.clone().detach()\n        D_output = netD(torch.cat([sketch, G_output], dim=1))\n        \n        class_label = torch.full((b_size,), ind, dtype=torch.long, device=device)\n        \n        if ind==0:\n            error = G_error(iter_input, G_output, y, D_output, class_label)\n        else :\n            error += (gamma ** ind) * G_error(iter_input, G_output, y, D_output, class_label)\n            \n        del G_output, D_output\n        gc.collect()\n        torch.cuda.empty_cache()\n            \n    return error","6e2cf1a8":"def discriminator_error_only_d(netD, sketch, real, real_label, fake_label, b_size=batch_size, avg=True): \n    errD = 0.0    \n    for i in range(0, TIME_STEP):\n        outputs_real = netD(torch.cat([sketch, real[:,i,:,:]], dim=1))\n        \n        class_label = torch.full((b_size,), i, dtype=torch.long, device=device)\n        if avg:\n            errD += (focalloss(outputs_real[1], class_label) + l2loss(outputs_real[0].view(-1), real_label))\/TIME_STEP\n        else:\n            errD += focalloss(outputs_real[1], class_label) + l2loss(outputs_real[0].view(-1), real_label)\n        del outputs_real\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n    return errD","aa92e9d7":"def discriminator_error_in_gan(netG, netD, sketch, real, real_label, fake_label, b_size=batch_size, avg=True):\n    output_g = netG(sketch)\n    outputs_fake = netD(torch.cat([sketch, output_g.detach()], dim=1))    \n    errD = bceloss(outputs_fake[0].view(-1), fake_label)\n    \n    del output_g, outputs_fake\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    for i in range(0, TIME_STEP):\n        outputs_real = netD(torch.cat([sketch, real[:,i,:,:]], dim=1))\n        \n        class_label = torch.full((b_size,), i, dtype=torch.long, device=device)\n        if avg:\n            errD += (focalloss(outputs_real[1], class_label) + l2loss(outputs_real[0].view(-1), real_label))\/TIME_STEP\n        else:\n            errD += focalloss(outputs_real[1], class_label) + l2loss(outputs_real[0].view(-1), real_label)\n        del outputs_real\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n    return errD","614fd0db":"def apply_checkpoint(use_checkpoint=True):\n    global Generator, Discriminator, optimizerG, optimizerD, G_Loss, D_Loss, start_epoch, all_epochs_only_d\n    \n    if os.path.isdir(OLD_PATH) and use_checkpoint:        \n        checkpoint = torch.load(OLD_GENERATOR_MODEL)\n        start_epoch = checkpoint['epoch']\n        Generator.load_state_dict(checkpoint['model_state_dict'])\n        optimizerG.load_state_dict(checkpoint['optimizer_state_dict'])\n        \n        checkpoint = torch.load(OLD_DISCRIMINATOR_MODEL)\n        start_epoch = checkpoint['epoch']\n        Discriminator.load_state_dict(checkpoint['model_state_dict'])\n        optimizerD.load_state_dict(checkpoint['optimizer_state_dict'])\n        \n        with open(OLD_G_LOSS, 'rb') as f:\n            G_loss = pickle.load(f)\n            \n        with open(OLD_D_LOSS, 'rb') as f:\n            D_loss = pickle.load(f)\n            \n        all_epochs_only_d = 0\n        \n        print('Continue training. (Epoch : {0})'.format(start_epoch))\n    else :\n        print('Begin training newly.')","2dea6fc6":"nowepoch_only_d = 0\nnowepoch = 0","9dd6a086":"# Only Discriminator Train\n\ndef fit_only_d(device, num_epochs_only_d=1):\n    global nowepoch_only_d\n    iters = 0\n    for epoch in range(start_epoch_only_d+1, num_epochs_only_d+start_epoch_only_d+1):\n        nowepoch_only_d = epoch\n        print(\"< EPOCH{0} >\".format(epoch))\n        result = train_one_epoch_only_d(device, train_dataloader, Discriminator, optimizerD, epoch, num_epochs_only_d)\n        if not result:\n            return\n        \ndef train_one_epoch_only_d(device, dataloader, netD, optimizerD, epoch, num_epochs, iters=0):\n    global nowpath, strange_error_num, strange_error_limit\n    with torch.autograd.set_detect_anomaly(True):\n        for i, data in enumerate(dataloader):   \n            if i%only_d_train_step != 0 :\n                continue\n            start = time.time()\n            sketch, real = data\n            sketch, real = sketch.to(device), real.to(device)\n            \n            sketch_list = []\n            for ind in range(0, INPUT_NUM):\n                sketch_list.append(sketch[:, ind, :, :, :])\n            sketch = torch.cat(sketch_list, dim=1)\n            \n            b_size = sketch.size(0)\n            real_label = torch.full((b_size,), REAL_LABEL, dtype=torch.float, device=device)\n            fake_label = torch.full((b_size,), FAKE_LABEL, dtype=torch.float, device=device)\n\n            netD.train()\n            netD.zero_grad()\n            \n            errD = discriminator_error_only_d(netD, sketch, real, real_label, fake_label, b_size=b_size)\n            \n            log('Complete calcuating of Discriminator')\n            errD.backward()\n            log('Complete backprogration of Discriminator')\n            optimizerD.step()\n            log('Complete stepping OptimizerD')\n\n            \n            del b_size, real_label, fake_label, sketch, real\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            #Log\n            if i % 1 == 0:\n                print('[%d\/%d][%d\/%d]\\tLoss_D: %.4f\\tTime: %.6f'\n                      % (epoch, num_epochs, i, len(dataloader),\n                         errD.item(), time.time() - start))\n                \n            D_loss.append(errD.item())\n            \n            del errD\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            iters += 1\n    return True","257151ea":"# GAN Train\n\ndef fit_gan(device, num_epochs=1):\n    global nowepoch\n    iters = 0\n    for epoch in range(start_epoch+1, num_epochs+start_epoch+1):\n        nowepoch = epoch\n        print(\"< EPOCH{0} >\".format(epoch))\n        result = train_one_epoch_gan(device, train_dataloader, Generator, Discriminator, optimizerG, optimizerD, epoch, num_epochs)\n        if not result:\n            return\n    \n\ndef train_one_epoch_gan(device, dataloader, netG, netD, optimizerG, optimizerD, epoch, num_epochs, iters=0):\n    global nowpath, strange_error_num, strange_error_limit\n    with torch.autograd.set_detect_anomaly(True):\n        for i, data in enumerate(dataloader):   \n            start = time.time()\n            sketch, real = data\n            sketch, real = sketch.to(device), real.to(device)\n            \n            sketch_list = []\n            for ind in range(0, INPUT_NUM):\n                sketch_list.append(sketch[:, ind, :, :, :])\n            sketch = torch.cat(sketch_list, dim=1)\n            \n            b_size = sketch.size(0)\n            real_label = torch.full((b_size,), REAL_LABEL, dtype=torch.float, device=device)\n            fake_label = torch.full((b_size,), FAKE_LABEL, dtype=torch.float, device=device)\n            \n            #Train Discriminator\n            netG.eval()\n            netD.train()\n            netD.zero_grad()\n            \n            errD = discriminator_error_in_gan(netG, netD, sketch, real, real_label, fake_label, b_size=b_size)\n            \n            log('Complete calcuating of Discriminator')\n            errD.backward()\n            log('Complete backprogration of Discriminator')\n            optimizerD.step()\n            log('Complete stepping OptimizerD')\n        \n            #Train Generator\n            netG.train()\n            netD.eval()\n            netG.zero_grad()\n            \n            errG = generator_error(netG, netD, sketch, real, real_label, fake_label, gamma=GAMMA, b_size=b_size)\n \n            log('Complete calcuating of Generator')\n            errG.backward()\n            log('Complete backprogration of Genereator')\n            optimizerG.step()\n            log('Complete stepping OptimizerG')\n            \n            del b_size, real_label, fake_label, sketch, real\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            #Log\n            if i % 1 == 0:\n                print('[%d\/%d][%d\/%d]\\tLoss_G: %.4f\\tLoss_D: %.4f\\tTime: %.6f'\n                      % (epoch, num_epochs, i, len(dataloader),\n                         errG.item(), errD.item(), time.time() - start))\n                \n            \n\n            G_loss.append(errG.item())\n            D_loss.append(errD.item())\n            \n            del errG, errD\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            iters += 1\n    return True","6e2beef9":"apply_checkpoint(use_checkpoint=USE_CHECKPOINT)","30eff8b6":"summary = Discriminator.train()\n\nif all_epochs_only_d>0:\n    fit_only_d(device, num_epochs_only_d=all_epochs_only_d)\n\nsummary = Discriminator.eval()","80b3b81d":"summary = Generator.train()\nsummary = Discriminator.train()\n\nif all_epochs>0:\n    fit_gan(device, num_epochs=all_epochs)\n\nsummary = Generator.eval()\nsummary = Discriminator.eval()","09056b49":"plt.figure(figsize=(10,5))\nplt.title('Loss of Generator')\nplt.plot(G_loss,label=\"\")\nplt.xlabel(\"Iter\")\nplt.legend()\nplt.show()","41bdca3d":"plt.figure(figsize=(10,5))\nplt.title('Loss of Discriminator')\nplt.plot(D_loss,label=\"train\")\nplt.xlabel(\"Iter\")\nplt.legend()\nplt.show()","0b49c1de":"def model_predict(model, time, input):\n    if time%15==0 and time!=0:\n        model.eval()\n        num = time\/\/15\n        \n        final_answer = None\n        next_input = input\n        for i in range(num):\n            final_answer = model(next_input).clone().detach()\n            if INPUT_NUM > 1:\n                next_input = torch.cat((next_input[:, 1:, :, :], final_answer), dim=1)\n            else:\n                next_input = model(next_input).clone().detach()\n        return final_answer\n    else:\n        raise ValueError('Please set the time to a multiple of 15.')","a7a54bcb":"from IQA_pytorch import SSIM, utils\n\ntoPILImage = transforms.ToPILImage()\nssim_model = SSIM(channels=1)\n\ndef one_time_step_ssim_score(dataloader, model, time_step, num=-1):\n    model.eval()\n    score = 0\n    total = 0\n    for ind, (x, y) in enumerate(test_dataloader_bs1_shuffle):\n        x, y = x.squeeze(0).to(device), y.squeeze(0).to(device)\n        x_list = []\n        for i in range(0, INPUT_NUM):\n            x_list.append(x[i, :, :, :])\n        x = torch.cat(x_list, dim=0).to(device)\n        outputG = model_predict(model, time_step*15, x.unsqueeze(0))\n\n        sketch = utils.prepare_image(toPILImage(outputG.squeeze(0))).to(device)\n        real = utils.prepare_image(toPILImage(y[time_step-1])).to(device)\n\n        score += ssim_model(sketch, real, as_loss=False).item()\n        total += 1\n\n        del x, y, outputG, sketch, real\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n        if num != -1:\n            if ind+1 >= num:\n                break\n            \n    print(\"SSIM Score of the prediction {0} minutes later : {1}\".format(time_step*15, score\/total))\n    return score\/total\n\nfor ind in range(1, TEST_TIME_STEP+1):\n    one_time_step_ssim_score(test_dataloader_bs1_shuffle, Generator, ind, num=2000)","7dd6a3f3":"import zipfile\n\ny_nums = 40\niter = 0\n\nai_noseries_ls = []\nreal_noseries_ls = []\n\nstart_ind = 200\n\nfor ind, (x, y) in enumerate(test_dataloader_bs1_shuffle):\n    if ind < start_ind:\n        continue\n        \n    iter += 1\n    \n    x, y = x.to(device), y[0].to(device)\n    \n    x_list = []\n    for i in range(0, INPUT_NUM):\n        x_list.append(x[:, i, :, :, :])\n    x = torch.cat(x_list, dim=1).to(device)\n        \n    outputg = Generator(x).to(cpu_device)\n    \n    outputg = outputg*127.5+127.5\n    realimage = y*127.5+127.5\n\n    cv2.imwrite('.\/AI_NOSERIES_Answer{0}.png'.format(ind+1), torch_tensor_to_plt(outputg)*30)\n    cv2.imwrite('.\/Real_NOSERIES{0}.png'.format(ind+1), torch_tensor_to_plt(realimage.to(cpu_device))*30)\n    \n    ai_noseries_ls.append('.\/AI_NOSERIES_Answer{0}.png'.format(ind+1))\n    real_noseries_ls.append('.\/Real_NOSERIES{0}.png'.format(ind+1))\n    \n    if iter > y_nums:\n        break\n\nwith zipfile.ZipFile(\"ai_noseries.zip\", 'w') as my_zip:\n    for i in ai_noseries_ls:\n        my_zip.write(i)\n    my_zip.close()\n\n\nwith zipfile.ZipFile(\"real_noseries.zip\", 'w') as my_zip:\n    for i in real_noseries_ls:\n        my_zip.write(i)\n    my_zip.close()\n    \nfor file in (ai_noseries_ls + real_noseries_ls):\n    os.remove(file)\n    \nprint('NOSERIES Images are generated.')","98b42170":"import zipfile\n\ny_nums = 40\niter = 0\n\nai_series_ls = []\nreal_series_ls = []\n\nnext_input = None\nstart_ind = 200\n\nfor ind, (x, y) in enumerate(test_dataloader_bs1_noshuffle):\n    if ind < start_ind:\n        continue\n        \n    iter += 1\n    \n    x, y = x.to(device), y[0].to(device)\n    \n    x_list = []\n    for i in range(0, INPUT_NUM):\n        x_list.append(x[:, i, :, :, :])\n    x = torch.cat(x_list, dim=1).to(device)\n    \n    if ind == start_ind:\n        next_input = x.clone().detach().to(device)\n        #cv2.imwrite('.\/Input_SERIES.png', torch_tensor_to_plt(next_input[].to(cpu_device)*127.5+127.5)*30)\n    \n    outputg_series = Generator(next_input).to(cpu_device)\n    if INPUT_NUM > 1:\n        next_input = torch.cat((next_input[:, 1:, :, :].clone().detach(), outputg_series.clone().detach().to(device)), dim=1).to(device)\n    else:\n        next_input = outputg_series.clone().detach().to(device)\n        \n    outputg_series = outputg_series * 127.5 + 127.5\n    realimage = y*127.5+127.5\n\n    cv2.imwrite('.\/AI_SERIES_Answer{0}.png'.format(ind+1), torch_tensor_to_plt(outputg_series)*30)\n    cv2.imwrite('.\/Real_SERIES{0}.png'.format(ind+1), torch_tensor_to_plt(realimage.to(cpu_device))*30)\n    \n    ai_series_ls.append('.\/AI_SERIES_Answer{0}.png'.format(ind+1))\n    real_series_ls.append('.\/Real_SERIES{0}.png'.format(ind+1))\n    \n    if iter > y_nums:\n        break\n\nwith zipfile.ZipFile(\"ai_series.zip\", 'w') as my_zip:\n    for i in ai_series_ls:\n        my_zip.write(i)\n    my_zip.close()\n\nwith zipfile.ZipFile(\"real_series.zip\", 'w') as my_zip:\n    for i in real_series_ls:\n        my_zip.write(i)\n    my_zip.close()\n    \n\n    \nprint('SERIES Images are generated')","d95bae0e":"v1 = cv2.VideoWriter('oraclegan_series.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 3, (128, 128))\nfor name in ai_series_ls:\n    img = cv2.imread(name)\n    v1.write(img)\nv1.release()\n\nv2 = cv2.VideoWriter('real_series.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 3, (128, 128))\nfor name in real_series_ls:\n    img = cv2.imread(name)\n    v2.write(img)\nv2.release()\n\nprint('Videos are generated')\nprint('video path : \".\/oraclegan_series.mp4\" and \".\/real_series.mp4\"')","5cd6df75":"for file in (ai_series_ls + real_series_ls):\n    os.remove(file)","dc783c0a":"torch.save({\n            'epoch': nowepoch,\n            'epoch_only_d': nowepoch_only_d,\n            'model_state_dict': Generator.state_dict(),\n            'optimizer_state_dict': optimizerG.state_dict(),\n            }, 'Generator.pth')\n\ntorch.save({\n            'epoch': nowepoch,\n            'epoch_only_d': nowepoch_only_d,\n            'model_state_dict': Discriminator.state_dict(),\n            'optimizer_state_dict': optimizerD.state_dict(),\n            }, 'Discriminator.pth')","a61d894d":"with open('.\/gloss.txt', 'wb') as f:\n    pickle.dump(G_loss, f)\nwith open('.\/dloss.txt', 'wb') as f:\n    pickle.dump(D_loss, f)","ef41ac4d":"# **Key Featues of FastOracleGAN**\n-------------------------------------\n- [Depthwise Separable Convolution](#depthwise_separable_convolution)\n- [MobileNet v2](#mobilenetv2)\n- [Skip Convolution](#skip_convolution)","771155fa":"# **Goal**\n- Predict future weather images using current weather images.","baaf8e4b":"# Define Cost Functions","78d6b964":"# Save Checkpoint","1ffc216d":"<a id=\"cost_of_generator\"><\/a>\n## **Cost Function of Generator**\n\n------------------------------------------------\n\n**$$ \\mathbf{Loss_G(x, y) = \\sum_{i=1}^{t}\\gamma ^ {i-1}\\times \\left \\{ \\lambda _1 \\times  E_{x,y_i}\\left [ \\left \\| y_i - G^i(x) \\right \\|_1 \\right ] + E_{x}\\left [ log(1-D(G^i(x))) \\right ] \\right \\} } $$**\n\n$t$ is Time Step. $\\gamma$ is discount factor(GAMMA). $\\lambda _1$ is L1Lambda.","c6c9af88":"# Define Neural Networks and Optimizers\n|Name|Sort|\n|----|----|\n|Generator|UNet|\n|Discriminator|ResNet|\n|Optimizer of Generator|Adam|\n|Optimizer of Disciminator|Adam|","632593b0":"# Apply Checkpoint","32db5f6e":"<a id=\"time_step_image_dataset\"><\/a>\n## **Time Step Image Dataset**\n\n------------------------------------------------\n\nOracleGAN calculates loss between predicted image and real image not only after 15 minutes but also **after 15\u00d7TimeStep minutes**.\n \nSo, dataset need to have **multiple output** images.","0d685b2b":"# Define Hyperparameters\n|Name of Hyperparameter|Explanation|\n|-----|-----|\n|USE_CUDA|whether to use GPU|\n|DEBUG|whether to print specific logs|\n|RANDOM_SEED|random seed of pytorch, random, numpy|\n|start_epoch|this is used to continuing train from checkpoint|\n|all_epochs|Epochs|\n|batch_size|Batch Size|\n|lrG|the learning rate of Generator|\n|lrD|the learning rate of Discriminator|\n|beta1, beta2|the beta1 and beta2 of Generator and Discriminator|\n|**L1Lambda**|lambda of pix2pix objective function|\n|**GAMMA**|factor similar to discount factor of DQN. (0<$\\gamma$<1) (check cost function of OracleGAN Generator)|\n|**INPUT_NUM**|the number of input images. (If INPUT_NUM is 1, this code works like FastOracleGAN of single input version)|\n|**TIME_STEP**|the number of future images which is used to calculate loss (check cost function of OracleGAN Generator)|\n|**ALPHA**|how much focus to l1 loss than perspectual loss|\n\n","38d25072":"<a id=\"skip_convolution\"><\/a>\n## Skip Convolution\n------------------------\n**1x1 Convolutions are added to Generator's Skip.**\n\nAt first, I tried adding **Attention Blocks**.\nBut, they are too **heavy** for the purpose of FastOracleGAN. Also, I thought the purpose to Generator is different to the original purpose of UNet.\nThe original purpose of UNet is Semantic Segmentation. Preventing to disturb input is important.\nHowever, the purpose of Generator is converting image. I estimated skip which adds original image to decoder layers can **interrupt** rather achieving the original purpose.\n**So I thought it was necessary to set Generator to be more interested in Up-sampling than Skip, instead of leaving it to artificial intelligence learning about which things to be interested in, such as Attention Blocks.** Therefore, I judged most of Generator's Attention Blocks can be substituted with just manipulating Skip with skip convolution.","2fd83c5d":"## Empty Layer\n------------------------\n$Layer(x) = x$","156fc651":"# **FastOracleGAN** - Multiple Input Version\n--------------\nThis study explores how to make **lighter, faster [OracleGAN](https:\/\/www.kaggle.com\/lapl04\/oraclegan-pix2pix-for-time-series-image)**.\n\nMobileNet v2 is adopted to Discriminator.\nAlso, part of Generator's convolution layers are replaced with Depthwise Separable Convolution.\nConvolution is added to Generator's skips.","41a47a1d":"# **Conclusion**\n------------------\n**the time required of 1 iter training with FastOracleGAN(about 7.5s) is reduced more twice times than its OracleGAN(about 3.36s).**","840b4546":"# Train","e7ab2165":"<a id=\"mobilenetv2\"><\/a>\n## MobileNet v2\n------------------------\nMobileNet v2 is adopted to Discriminator for better, lighter, and faster.","1c4edd5f":"# Define Train Function","55efae64":"<a id=\"depthwise_separable_convolution\"><\/a>\n## Depthwise Separable Convolution\n------------------------\nGenerator's second~sixth convolution layers are replaced with Depthwise Separable Convolution.","3e8722d4":"<a id=\"cost_of_discriminator\"><\/a>\n## **Cost Function of Discriminator**\n\n------------------------------------------------\n\n**$$ \\mathbf{Loss_D(x, y) = E_x\\left [ log D(G(x)) \\right ] + \\frac{1}{t} \\sum_{i=1}^{t} E_{y_i}\\left [ log(1-D(G(y_i)))) \\right ]} $$**\n\n$t$ is Time Step.","9e90bcaf":"> **< SSIM Score of OracleGAN >**  *(check \"[OracleGAN - Pix2Pix for Time Series Image](https:\/\/www.kaggle.com\/lapl04\/oraclegan-pix2pix-for-time-series-image)\")*\n>\n> |Prediction|SSIM Score|\n> |-------------------|----------------|\n> |prediction 15 minutes later|0.8025623009409756|\n> |prediction 30 minutes later|0.7757316670715809|\n> |prediction 45 minutes later|0.7566662636697292|\n> |prediction 60 minutes later|0.7422156925499439|\n> |prediction 75 minutes later|0.7312239380329847|\n> |prediction 90 minutes later|0.720590250596404|","5715513b":"# Install additional libraries\n\nIQA_pytorch is a library which is used to calculate SSIM Score","e80ce560":"# Initiate Weights and Biases","dc441e54":"# Visualize Data\n|Name of Function|Explanation|\n|-----|-----|\n|torch_tensor_to_plt|Convert torch image to matplotlib image|\n|plt_image_animation|show a video by update_function|","550ce706":"# Import Libraries","9a1514cd":"> **< SSIM Score of normal Pix2Pix >**  *(check \"[Pix2Pix (Compared to OracleNet)](https:\/\/www.kaggle.com\/lapl04\/pix2pix-compared-to-oraclenet)\")*\n>\n> |Prediction|SSIM Score|\n> |-------------------|----------------|\n> |prediction 15 minutes later|0.8199273004531861|\n> |prediction 30 minutes later|0.7006081487536431|\n> |prediction 45 minutes later|0.6030721757411956|\n> |prediction 60 minutes later|0.5150686911344529|","54cb4d55":"# **Key Featues of OracleGAN**\n-------------------------------------\n- [Time Step Image Dataset](#time_step_image_dataset)\n- [Cost Function of Generator](#cost_of_generator)\n- [Cost Function of Discriminator](#cost_of_discriminator)","179ae70a":"# Test\n\n1. Calculate SSIM Score each Time Steps\n2. Generate test predicted images.\n3. Generate video which consist of series predicted images.","f9c8e1b6":"# Preprocess Dataset"}}