{"cell_type":{"e5713d3a":"code","d6bc526b":"code","a3ac19cf":"code","502c3fdc":"code","32d206d4":"code","40cbf047":"code","2be1ee25":"code","d4722da2":"code","75069785":"code","a051e5b0":"code","b69444a0":"code","5bb6f438":"code","fb8786c0":"code","ea8fe459":"code","ee93ef20":"code","73fdd52d":"code","eb9f7d1f":"code","f3d7b353":"code","401bda57":"code","5ff22840":"code","69389457":"code","a9ddf6b9":"code","b78f09cf":"code","bd6f6fac":"code","8b89e9dc":"code","27391a73":"code","c2575f74":"code","9d180453":"code","e54d97ae":"code","8a553909":"code","96af07c9":"code","57498629":"code","1ada5127":"code","e3ccbbca":"code","21d1716f":"code","c876fa96":"code","37b22393":"markdown","90f00e8e":"markdown","6ebf0f96":"markdown","4e1c5128":"markdown","030f99d3":"markdown","172ff19e":"markdown","661f29ae":"markdown","58ac62cc":"markdown","604fd1ba":"markdown","79dd4f56":"markdown","e3172a9a":"markdown","78341956":"markdown","cbca2b49":"markdown","6de91d50":"markdown","6b7bc4bb":"markdown","d7173f5a":"markdown","3a3b27a0":"markdown","8c16bfde":"markdown","7ae7cc81":"markdown"},"source":{"e5713d3a":"!pip install textstat\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error as mse\nimport os\nimport textstat\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag","d6bc526b":"data_dir = '\/kaggle\/input\/commonlitreadabilityprize'\ntrain_data_path = os.path.join(data_dir, 'train.csv')\ntest_data_path = os.path.join(data_dir, 'test.csv')\n\ntrain_df = pd.read_csv(train_data_path)\ntest_df = pd.read_csv(test_data_path)\n\nprint(train_df.shape)\nprint(test_df.shape)","a3ac19cf":"train_df.head()","502c3fdc":"train_df.info()","32d206d4":"all_chars = set()\ntrain_df['excerpt'].apply(lambda x: [all_chars.add(c) for c in x])\nfor c in sorted(all_chars):\n    print(c + ' ', end='')\n    \nprint('\\n\\n')\n    \nfor c in sorted(all_chars):\n    print(f'({c}, {str(ord(c))}) ', end='')","40cbf047":"# Make a boolean column for excerpts with \"hard characters\"\n\nhard_chars = set()\nfor c in all_chars:\n    if ord(c) >= 176 and ord(c) <= 339:\n        hard_chars.add(c)\n        \nprint(hard_chars)\n\ntrain_df['has_hard_char'] = train_df['excerpt'].apply(lambda x: any([c in hard_chars for c in x]))\nsum(train_df['has_hard_char'])","2be1ee25":"train_df['len_excerpt'] = train_df['excerpt'].apply(len)","d4722da2":"sns.distplot(train_df['len_excerpt'], kde=False)","75069785":"sns.distplot(train_df['target'], kde=False)","a051e5b0":"sns.violinplot(train_df['has_hard_char'], train_df['target'], palette=['b', 'r'])","b69444a0":"correlation_matrix = np.corrcoef(train_df['len_excerpt'], train_df['target'])\ncorrelation_xy = correlation_matrix[0,1]\nr_squared = correlation_xy**2\n\nprint('Linear fit r^2:', r_squared)","5bb6f438":"correlation_matrix[0,1]\ncorrelation_matrix","fb8786c0":"sns.scatterplot(train_df['target'], train_df['standard_error'], alpha=0.4)","ea8fe459":"# removing the values with zero as target\ntrain_df = train_df[train_df['target'] != 0]","ee93ef20":"train_df.shape","73fdd52d":"# preprocess text\ndef preprocess(data):\n    excerpt_processed=[]\n    for e in data['excerpt']:\n        \n        # find alphabets\n        e = re.sub(\"[^a-zA-Z]\", \" \", e)\n        \n        # convert to lower case\n        e = e.lower()\n        \n        # tokenize words\n        e = nltk.word_tokenize(e)\n        \n        # remove stopwords\n        e = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \n        # lemmatization\n        lemma = nltk.WordNetLemmatizer()\n        e = [lemma.lemmatize(word) for word in e]\n        e=\" \".join(e)\n        \n        excerpt_processed.append(e)\n        \n    return excerpt_processed \n\ntrain_df[\"excerpt_preprocessed\"] = preprocess(train_df)\ntest_df[\"excerpt_preprocessed\"] = preprocess(test_df)","eb9f7d1f":"def get_top_n_words(corpus, n=None):\n    vec = CV().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_bigram(corpus, n=None):\n    vec = CV(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_trigram(corpus, n=None):\n    vec = CV(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef plot_bt(x,w,p):\n    common_words = x(train_df['excerpt_preprocessed'], 20)\n    common_words_df = pd.DataFrame(common_words,columns=['word','freq'])\n    plt.figure(figsize=(16,8))\n    sns.barplot(x='freq', y='word', data=common_words_df,facecolor=(0, 0, 0, 0),linewidth=3,edgecolor=sns.color_palette(p,20))\n    plt.title(\"Top 20 \"+ w,font='Serif')\n    plt.xlabel(\"Frequency\", fontsize=14)\n    plt.yticks(fontsize=13)\n    plt.xticks(rotation=45, fontsize=13)\n    plt.ylabel(\"\");\n    return common_words_df","f3d7b353":"common_words = get_top_n_words(train_df['excerpt_preprocessed'], 20)\ncommon_words_df1 = pd.DataFrame(common_words,columns=['word','freq'])\nplt.figure(figsize=(16, 8))\nax = sns.barplot(x='freq', y='word', data=common_words_df1,facecolor=(0, 0, 0, 0),linewidth=3,edgecolor=sns.color_palette(\"ch:start=3, rot=.1\",20))\n\nplt.title(\"Top 20 unigrams\",font='Serif')\nplt.xlabel(\"Frequency\", fontsize=14)\nplt.yticks(fontsize=13)\nplt.xticks(rotation=45, fontsize=13)\nplt.ylabel(\"\");\n\ncommon_words_df2 = plot_bt(get_top_n_bigram,\"bigrams\",\"ch:rot=-.5\")\ncommon_words_df3 = plot_bt(get_top_n_trigram,\"trigrams\",\"ch:start=-1, rot=-.6\")\n","401bda57":"train_df.reset_index(inplace=True, drop=True)\ntext_props = train_df.copy()\n\ndef avg_word_len(df):\n    df = df.str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n    return df\n\ntext_len = train_df['excerpt'].str.len()\ntext_len_pre = train_df['excerpt_preprocessed'].str.len()\navg_text = avg_word_len(train_df['excerpt'])\navg_text_pre = avg_word_len(train_df['excerpt_preprocessed'])\nlexicon_count = []\nlexicon_count_pre = []\nsentence_count = []\nfor i in range(len(train_df)):\n    lc = textstat.lexicon_count(train_df['excerpt'][i])\n    lcp = textstat.lexicon_count(train_df['excerpt_preprocessed'][i])\n    sc = textstat.sentence_count(train_df['excerpt'][i])\n    lexicon_count.append(lc)\n    lexicon_count_pre.append(lcp)\n    sentence_count.append(sc)\n    \ntext_props['text_len'] = text_len\ntext_props['text_len_pre'] = text_len_pre\ntext_props['lexicon_count'] = lexicon_count\ntext_props['lexicon_count_pre'] = lexicon_count_pre\ntext_props['avg_text'] = avg_text\ntext_props['avg_text_pre'] = avg_text_pre\ntext_props['sentence_count'] = sentence_count","5ff22840":"text_len = test_df['excerpt'].str.len()\ntext_len_pre = test_df['excerpt_preprocessed'].str.len()\navg_text = avg_word_len(test_df['excerpt'])\navg_text_pre = avg_word_len(test_df['excerpt_preprocessed'])\nlexicon_count = []\nlexicon_count_pre = []\nsentence_count = []\nfor i in range(len(test_df)):\n    lc = textstat.lexicon_count(test_df['excerpt'][i])\n    lcp = textstat.lexicon_count(test_df['excerpt_preprocessed'][i])\n    sc = textstat.sentence_count(test_df['excerpt'][i])\n    lexicon_count.append(lc)\n    lexicon_count_pre.append(lcp)\n    sentence_count.append(sc)\n    \ntest_df['text_len'] = text_len\ntest_df['text_len_pre'] = text_len_pre\ntest_df['lexicon_count'] = lexicon_count\ntest_df['lexicon_count_pre'] = lexicon_count_pre\ntest_df['avg_text'] = avg_text\ntest_df['avg_text_pre'] = avg_text_pre\ntest_df['sentence_count'] = sentence_count","69389457":"num_cols = ['text_len','text_len_pre','lexicon_count','lexicon_count_pre','avg_text','avg_text_pre','sentence_count','target']\ncorr = text_props[num_cols].corr().abs()\n\nfig = plt.figure(figsize=(12,12),dpi=80)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='BuPu', robust=True, center=0,\n            square=True, linewidths=.5, annot=True)\nplt.title('Correlation of text properties', fontsize=15,font=\"Serif\")\nplt.show()","a9ddf6b9":"filtered_cols = []\nfiltered_cols.append(\"avg_text_pre\")","b78f09cf":"text_props['pos_tags'] = text_props['excerpt_preprocessed'].str.split().map(pos_tag)\n\ndef count_tags(pos_tags):\n    tag_count = {}\n    for word,tag in pos_tags:\n        if tag in tag_count:\n            tag_count[tag] += 1\n        else:\n            tag_count[tag] = 1\n    return tag_count\n\ntext_props['tag_counts'] = text_props['pos_tags'].map(count_tags)\n\nset_pos = set([tag for tags in text_props['tag_counts'] for tag in tags])\ntag_cols = list(set_pos)\n\nfor tag in tag_cols:\n    text_props[tag] = text_props['tag_counts'].map(lambda x: x.get(tag, 0))","bd6f6fac":"test_df['pos_tags'] = test_df['excerpt_preprocessed'].str.split().map(pos_tag)\n\ndef count_tags(pos_tags):\n    tag_count = {}\n    for word,tag in pos_tags:\n        if tag in tag_count:\n            tag_count[tag] += 1\n        else:\n            tag_count[tag] = 1\n    return tag_count\n\ntest_df['tag_counts'] = test_df['pos_tags'].map(count_tags)\n\nfor tag in tag_cols:\n    test_df[tag] = test_df['tag_counts'].map(lambda x: x.get(tag, 0))","8b89e9dc":"text_props[tag_cols]","27391a73":"corr_tags = text_props[tag_cols + ['target']].corr().abs()\n\nfig = plt.figure(figsize=(30,24),dpi=80)\nmask_tags = np.triu(np.ones_like(corr_tags, dtype=bool))\nsns.heatmap(corr_tags, mask=mask_tags, cmap='BuPu', robust=True, center=0,\n            square=True, linewidths=.5, annot=True)\nplt.title('Correlation of POS tags', fontsize=15,font=\"Serif\")\nplt.show()\n\n# sentence_count is highly correlated to target ","c2575f74":"filtered_cols += \"VBD\", \"NN\", \"VB\",\"JJ\"\nfiltered_cols","9d180453":"# POS tags frequency across the text\npos = text_props[tag_cols].sum().sort_values(ascending = False)\nplt.figure(figsize=(16,10))\nax = sns.barplot(x=pos.index, y=pos.values,palette=\"Wistia\")\nplt.xticks(rotation = 50)\nax.set_yscale('log')\nplt.title('POS tags frequency',fontsize=15,font=\"Serif\")\nplt.show()","e54d97ae":"flesch_re, flesch_kg, fog_scale, automated_r,coleman, linsear, text_standard  = ([] for i in range(7))\nfor i in range(len(text_props)):\n    flr = textstat.flesch_reading_ease(train_df['excerpt'][i])\n    flkg = textstat.flesch_kincaid_grade(train_df['excerpt'][i])\n    fs = textstat.gunning_fog(train_df['excerpt'][i])\n    ar = textstat.automated_readability_index(train_df['excerpt'][i])\n    cole = textstat.coleman_liau_index(train_df['excerpt'][i])\n    lins = textstat.linsear_write_formula(train_df['excerpt'][i])\n    ts = textstat.text_standard(train_df['excerpt'][i])\n    \n    flesch_re.append(flr)\n    flesch_kg.append(flkg)\n    fog_scale.append(fs)\n    automated_r.append(ar)\n    coleman.append(cole)\n    linsear.append(lins)\n    text_standard.append(ts)\n    \ntext_props['flesch_re'] = flesch_re\ntext_props['flesch_kg'] = flesch_kg\ntext_props['fog_scale'] = fog_scale\ntext_props['automated_r'] = automated_r\ntext_props['coleman'] = coleman\ntext_props['linsear'] = linsear\ntext_props['text_standard'] = text_standard","8a553909":"flesch_re = []\nfor i in range(len(test_df)):\n    flr = textstat.flesch_reading_ease(test_df['excerpt'][i])\n    flesch_re.append(flr)\n    \ntest_df['flesch_re'] = flesch_re","96af07c9":"readability_cols = ['flesch_re','flesch_kg','fog_scale','automated_r','coleman','linsear','text_standard','target']\n\ncorr_read = text_props[readability_cols].corr().abs()\nfig = plt.figure(figsize=(12,12),dpi=80)\nmask_read = np.triu(np.ones_like(corr_read, dtype=bool))\nsns.heatmap(corr_read, mask=mask_read, cmap='PuBuGn', robust=True, center=0,\n            square=True, linewidths=.5,annot=True)\nplt.title('Correlation of readability tests', fontsize=15,font=\"Serif\")\nplt.show()","57498629":"filtered_cols.append(\"flesch_re\") \nfiltered_cols","1ada5127":"plt.figure(figsize=(10,8))\nsns.kdeplot(text_props[\"flesch_re\"],shade=True)\nplt.title(\"Distribution of Flesch Reading Ease test\")\nplt.show()","e3ccbbca":"# More than 70% of excerpts can be easily understood by 13-15 year olds.\ntext_props.loc[text_props['flesch_re'] > 60]['flesch_re'].count() \/ len(text_props) *100","21d1716f":"filtered_cols += \"target\", \"excerpt_preprocessed\", \"excerpt\"","c876fa96":"final_train_df = text_props.loc[:,filtered_cols]\nfiltered_cols.remove(\"target\")\nfinal_test_df = test_df.loc[:, filtered_cols]","37b22393":"**Target Correlation with Standard Error**","90f00e8e":"**VBD, NN, VB, JJ are the most correlated ones with target** <br>\n**Tags REF: https:\/\/cs.nyu.edu\/~grishman\/jet\/guide\/PennPOS.html**","6ebf0f96":"**Some of the key resources that helped me out in formulating this:**\n- https:\/\/www.kaggle.com\/ejmejm\/commonlit-eda-video-tutorial?scriptVersionId=64939768 : There is a beautiful explanation via youtube video as well. \n- https:\/\/www.kaggle.com\/ruchi798\/commonlit-readability-prize-eda-baseline","4e1c5128":"**Finding the outlier**","030f99d3":"**Part-of-Speech tagging**","172ff19e":"**Excerpt len variation**","661f29ae":"**avg_text_pre is highly correlated to target**","58ac62cc":"**Unique Characters**","604fd1ba":"- **Input Variable:** *excerpt*\n- **Target Variable:** *target*","79dd4f56":"**Correlation between features**","e3172a9a":"### EDA","78341956":"**Extracting Top n-grams (1, 2, 3)**","cbca2b49":"**Target Correlation with Excerpt Length**","6de91d50":"**Common Readability Tests**","6b7bc4bb":"**Target Correlation with \"Hard Characters\"**","d7173f5a":"### Feature Engineering","3a3b27a0":"### Feature Selection","8c16bfde":"**Target Column Distribution**","7ae7cc81":"**flesch_re is highly correlated to target**"}}