{"cell_type":{"4d2fc2a9":"code","de25600c":"code","5acb2faf":"code","7c8d2cfc":"code","01fcb5ad":"code","ad684b44":"code","157faf21":"code","b8c8db9d":"code","f1865433":"code","cc30cd1e":"code","4cd973a1":"code","fd462242":"code","d5b6e9f6":"code","b9ce5f04":"code","57797271":"code","9103eaae":"code","b04af7e4":"code","290b7e72":"code","e2b648bf":"code","95e8e154":"code","291d37f8":"markdown","70ca1ab9":"markdown","0ffa5264":"markdown","3a87dcd1":"markdown","ea320e3e":"markdown","da67c4e3":"markdown","d617d883":"markdown","f1cd4a41":"markdown","c0d79261":"markdown","58ee6921":"markdown","015106b0":"markdown","34129d1b":"markdown","9ed7c1e1":"markdown","1ace61dc":"markdown","e956f641":"markdown","d0a64acc":"markdown","0e5b851c":"markdown","ef30f81a":"markdown","ffeae306":"markdown","97fcf127":"markdown","99a09e6c":"markdown","13a1cd24":"markdown","58e7c668":"markdown","db161cac":"markdown"},"source":{"4d2fc2a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","de25600c":"# load data set\ntrain = pd.read_csv('..\/input\/cnn-train\/train.csv')\ntrain.shape","5acb2faf":"test = pd.read_csv('..\/input\/cnn-test\/test.csv')\ntest.shape","7c8d2cfc":"train.head()","01fcb5ad":"test.head()","ad684b44":"#drop label column from train data set and create X_train:\nX_train = train.drop('label', axis=1)\n#y_train is the label column of train set:\ny_train = train['label']","157faf21":"#distribution graph:\nsns.countplot(y_train)","b8c8db9d":"#example of image representation:\n\nimg = X_train.iloc[0].as_matrix()\nimg = img.reshape((28, 28))\n\nplt.imshow(img, cmap='gray')\nplt.title(X_train.iloc[0,0])\nplt.axis('off')\nplt.show()","f1865433":"#we change the values to 0-1 scale:\nX_train = X_train \/ 255.0\ntest = test \/ 255.0","cc30cd1e":"X_train = X_train.values.reshape(-1,28,28,1)\n\ntest = test.values.reshape(-1,28,28,1)\n\nprint(X_train.shape)\nprint(test.shape)","4cd973a1":"from keras.utils.np_utils import to_categorical #convert to one-hot-encoding\n\ny_train = to_categorical(y_train, num_classes = 10)","fd462242":"#Split the train and the validation set for the fitting:\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=2)\n#Burada bizim main datam\u0131z X ve y de\u011fil, X_train ve y_train oldu\u011fu i\u00e7in X_train, X_val, Y_train, Y_val \u015feklinde b\u00f6ld\u00fck.\n#yani asl\u0131nda test datas\u0131na hi\u00e7 dokunmad\u0131k.\nprint('X_train shape:', X_train.shape)\nprint('X_val shape:', X_val.shape)\nprint('y_train shape:', y_train.shape)\nprint('y_val shape:', y_val.shape)\n","d5b6e9f6":"#import methods:\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical #convert to one-hot coding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel = Sequential()\n\n#conv -> max pool -> dropout -> conv -> max pool -> dropout -> fully connected(2 layers)\n#Convolutional layer'\u0131 ekliyoruz:\nmodel.add(Conv2D(filters=8, kernel_size = (5,5), padding='Same', activation='relu', input_shape=(28,28,1)))\n#filters=8 adet filtre kullanaca\u011f\u0131z dedik, \n#kernel_size filtre size'\u0131m\u0131z, \n#padding i\u00e7in 'same pedding' kullan\u0131yoruz\n\n#max pool -> dropout -> conv -> max pool -> dropout -> fully connected(2 layers)\n#max pool ekliyoruz:\nmodel.add(MaxPool2D(pool_size=(2,2)))\n#pool size yukar\u0131da da 2x2 kullanm\u0131\u015ft\u0131k.\n\n#dropout -> conv -> max pool -> dropout -> fully connected(2 layers)\n#drop out'ta s\u0131ra:\nmodel.add(Dropout(0.25))\n#0.25 oranla ignore et dedik.\n\n#conv -> max pool -> dropout -> fully connected(2 layers)\n#Convolutional layer'\u0131 ekliyoruz:\nmodel.add(Conv2D(filters=16, kernel_size=(3,3), padding='Same', activation='relu'))\n\n#max pool -> dropout -> fully connected(2 layers)\n#max pool ekliyoruz:\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n#bu sefer stride ekliyoruz.\n\n#dropout -> fully connected(2 layers)\n#dropout ekliyoruz:\nmodel.add(Dropout(0.25))\n\n#fully connected(2 layers):\n#fully connected yaparken \u00f6nce flatten yap\u0131caz:\nmodel.add(Flatten())\n#1 adet hidden layer ekliyoruz:\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\n#1 adet de output ekliyoruz: (2 layers bir hidden bir outputtan olu\u015fuyordu)\nmodel.add(Dense(10, activation='softmax'))\n","b9ce5f04":"#Define the optimizer:\noptimizer = Adam(lr=0.001, beta_1= 0.9, beta_2= 0.999)","57797271":"#compile the model:\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])","9103eaae":"epochs = 10\nbatch_size = 250","b04af7e4":"datagen = ImageDataGenerator(\n        featurewise_center=False, #set input mean to 0 over the dataset\n        samplewise_center=False, #set each sample mean to 0\n        featurewise_std_normalization=False, #divide inputs by std of the dataset\n        samplewise_std_normalization=False, #divide each input by its std\n        zca_whitening=False, #dimension reduction\n        rotation_range=0.5, #randomly rotate images in the range 5 degrees\n        zoom_range=0.5, #randomly zoom image 5%\n        width_shift_range=0.5, #randomly shift images horizontally 5%\n        height_shift_range=0.5, #randomly shift images vertically 5%\n        horizontal_flip=False, #randomly flip images\n        vertical_flip=False) #randomly flip images\n\ndatagen.fit(X_train)","290b7e72":"#fit the model:\nhistory = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size), epochs=epochs, validation_data=(X_val, y_val), steps_per_epoch=X_train.shape[0]\/\/batch_size)","e2b648bf":"# Plot the loss and accuracy curves for training and validation \nplt.plot(history.history['val_loss'], color='b', label=\"validation loss\")\nplt.title(\"Test Loss\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","95e8e154":"# confusion matrix\nimport seaborn as sns\n# Predict the values from the validation dataset\ny_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \ny_pred_classes = np.argmax(y_pred,axis = 1) \n# Convert validation observations to one hot vectors\ny_true = np.argmax(y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_true, y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","291d37f8":"<a id=\"18\"><\/a>\n**Epochs and Batch Size:**\n\n* Say you have a dataset of 10 examples (or samples). You have a **batch size** of 2, and you've specified you want the algorithm to run for 3 **epochs**. Therefore, in each epoch, you have 5 **batches** (10\/2 = 5). Each batch gets passed through the algorithm, therefore you have 5 iterations **per epoch**.\n\n* reference: https:\/\/stackoverflow.com\/questions\/4752626\/epoch-vs-iteration-when-training-neural-networks","70ca1ab9":"<a id=\"13\"><\/a>\n**Full Connection**\n\n* Neurons in a fully connected layer have connections to all activations in the previous layer. Yani, flatten edildikten sonra olu\u015fan layer bizim inputumuz ve bu input t\u00fcm\u00fcyle hidden layer'lara connected oluyor: \n\n<a href=\"https:\/\/ibb.co\/hsS14p\"><img src=\"https:\/\/preview.ibb.co\/evzsAU\/fullyc.jpg\" alt=\"fullyc\" border=\"0\"><\/a>","0ffa5264":"<a id=\"14\"><\/a>\n**CNN Implementation with KERAS**","3a87dcd1":"# Convolutional Neural Networks (CNN)\n<br>Content: \n* [Loading the Data Set](#1)\n* [Overview Data Set](#2)\n* [Data Preprocessing](#3)\n    * [Normalization](#4)\n    * [Reshape](#5)\n    * [Label Encoding](#6)\n    * [Train Test Split](#7)\n* [Convolutional Neural Network](#8)\n    * [What is Convolution Operation?](#9)\n    * [Same Padding](#10)\n    * [Max Pooling](#11)\n    * [Flattening](#12)\n    * [Full Connection](#13)\n* [Implementing with Keras](#14)\n    * [Create Model](#15)\n    * [Define Optimizer](#16)\n    * [Compile Model](#17)\n    * [Epochs and Batch Size](#18)\n    * [Data Augmentation](#19)\n    * [Fit the Model](#20)\n    * [Evaluate the Model](#21)\n","ea320e3e":"<a id=\"19\"><\/a>\n**Data Augmentation:**\n\n* To avoid overfitting, we need to expand our dataset.\n\n* Alter training data with small transformations to reproduce the variance.\n\n* For example, the image is rotated, the scale is changed, the image is not centered,...\n\n<a href=\"https:\/\/ibb.co\/k24CUp\"><img src=\"https:\/\/preview.ibb.co\/nMxXUp\/augment.jpg\" alt=\"augment\" border=\"0\"><\/a>","da67c4e3":"**CNN with PYTORCH**\n\n* https:\/\/www.kaggle.com\/kanncaa1\/pytorch-tutorial-for-deep-learning-lovers","d617d883":"<a id=\"20\"><\/a>\n**Fit the Model:**\n\n* Epochs u art\u0131rarak, \n* Data augmentation parametrelerini de\u011fi\u015ftirerek,\n* batch_size \u0131 de\u011fi\u015ftirerek,\n* layer say\u0131s\u0131n\u0131 de\u011fi\u015ftirerek,\n* filters say\u0131s\u0131n\u0131(8) de\u011fi\u015ftirerek\n* kernel_size \u0131 de\u011fi\u015ftirerek accuracy'yi de\u011fi\u015ftirebiliriz.!!","f1cd4a41":"<a id=\"9\"><\/a>\n**What is Convolution Operation?**\n\n* We have image and feature detector(3x3); it can be 5x5 or 7x7, it is hyperparameter.\n\n* Feature detector is matrix which detects features like edges or convex shapes. \u00d6rne\u011fin, yukar\u0131daki 5'in \u00fcst \u00e7izgisi, oval k\u0131sm\u0131 gibi..\n\n* Feature detector is named as kernel or filter.\n\n* Feature map: element wise multiplication of matrixes. yani, convolute input image with feature detector:\n\n<a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/image.ibb.co\/m4FQC9\/gec.jpg\" alt=\"gec\" border=\"0\"><\/a>\n\n* Stride: navigating in input image.\n\n* We reduce the size of image with this process (striding the image). So, the code runs faster but we lose information.\n\n* We create multiple feature maps and we use multiple feature detectors(filters).\n\n* After having convolution layer, we use ReLU to break up linearity. Because images are non linear:\n\n<a href=\"https:\/\/ibb.co\/mVZih9\"><img src=\"https:\/\/preview.ibb.co\/gbcQvU\/RELU.jpg\" alt=\"RELU\" border=\"0\"><\/a>\n\n","c0d79261":"<a id=\"5\"><\/a>\n**Reshape:**\n\n* Train and test set's format is 28x28, but Keras works 3D matrixes, so we will change format to 28x28x1. \n\n* If the pictures are colored (rgb), we will use x3 (for r, g and b)","58ee6921":"<a id=\"10\"><\/a>\n**Same Pedding**\n\n* As we apply convolution layers, the size of the volume will decrease faster than we don't want. \n\n* In the early layers of our network, we want to preserve information from the original input volume so that we can extract those low level features.\n\n* Image size (input) is the same with the matrix after applying pedding to this image (output)\n\n* Green area is the image, we manually set frame with zeros to this image, so after striding with the filter (the orange matrix) we have the same sized matrix with the image:\n\n<a href=\"https:\/\/ibb.co\/jUPkUp\"><img src=\"https:\/\/preview.ibb.co\/noH5Up\/padding.jpg\" alt=\"padding\" border=\"0\"><\/a>","015106b0":"<a id=\"8\"><\/a>\n**Convolutional Neural Network**\n\n* CNN is used for image classification and object detection.\n\n* General structure of CNN:\n\n<a href=\"https:\/\/ibb.co\/kV1j9p\"><img src=\"https:\/\/preview.ibb.co\/nRkBpp\/gec2.jpg\" alt=\"gec2\" border=\"0\"><\/a>","34129d1b":"<a id=\"1\"><\/a>\n**Loading Data Set**","9ed7c1e1":"<a id=\"15\"><\/a>\n**Create Model:**\n\n* Define any model: conv -> max pool -> dropout -> conv -> max pool -> dropout -> fully connected(2 layers)\n\n* Dropout: is a technique where randomly selected neurons are ignored during training which is used to prevent overfitting.\n\n<a href=\"https:\/\/ibb.co\/jGcvVU\"><img src=\"https:\/\/preview.ibb.co\/e7yPPp\/dropout.jpg\" alt=\"dropout\" border=\"0\"><\/a>\n\n","1ace61dc":"<a id=\"11\"><\/a>\n**Max Pooling**\n\n* We preserve the size with Same Pedding, but size is disadvantage also. \n\n* We want not to lose information, but with the same time we want to reduce the number of parameters also.\n\n* So, we will apply sub-sampling (down-sampling) with max pooling.\n\n* Also, max pooling makes the detection of features invariant to orientation changes.\n\n* Also, max pooling reduce the amount of parameters and computation in network, hence to control overfitting.\n\n* A\u015fa\u011f\u0131daki \u00f6rnekte, image 6x6'l\u0131k, biz 2x2'lik max pooling ile 2x2'lik matrix'lere ay\u0131rd\u0131\u011f\u0131m\u0131z image i\u00e7in her 2x2'lik matrix'in max value's\u0131n\u0131 al\u0131yor, bu \u015fekilde daha k\u00fc\u00e7\u00fck bir matrix olu\u015fturuyor: bu, yukar\u0131daki pooling layer:\n\n<a href=\"https:\/\/ibb.co\/ckTjN9\"><img src=\"https:\/\/preview.ibb.co\/gsNYFU\/maxpool.jpg\" alt=\"maxpool\" border=\"0\"><\/a>","e956f641":"<a id=\"12\"><\/a>\n**Flattening**\n\n* Pooling yapt\u0131ktan sonra elde etti\u011fimiz matrix'i uygulayaca\u011f\u0131m\u0131z ANN i\u00e7in input olu\u015fturmas\u0131 i\u00e7in flatten yap\u0131yoruz:\n\n<a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/image.ibb.co\/c7eVvU\/flattenigng.jpg\" alt=\"flattenigng\" border=\"0\"><\/a>","d0a64acc":"<a id=\"21\"><\/a>\n**Evaluate the Model:**","0e5b851c":"<a id=\"7\"><\/a>\n**Train Test Split:**\n\n* Split the data into train and test sets. Test size=0.1, train size=0.9","ef30f81a":"<a id=\"16\"><\/a>\n**Define Optimizer:**\n\n* Adam optimizer is an optimizer which is the most common optimizer. It changes the learning rate.","ffeae306":"<a id=\"17\"><\/a>\n**Compile Model:**\n\n* We have multiple classes so we use categorical cross entropy as loss function.","97fcf127":"<a id=\"3\"><\/a>\n**Data Preprocessing**","99a09e6c":"<a id=\"6\"><\/a>\n**Label Encoding:**\n\n* Encode labels to one hot vectors:\n    * 2-->> [0,0,1,0,0,0,0,0,0,0]\n    * 4-->> [0,0,0,0,1,0,0,0,0,0]","13a1cd24":"<a id=\"2\"><\/a>\n**Overview The Data**","58e7c668":"<a id=\"4\"><\/a>\n**Normalization:**\n\n* We perform a grayscale normalization to reduce the effect of illumination(ayd\u0131nl\u0131k), and with normalization model will work faster.","db161cac":"* The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n\n* Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. \n\n* Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n\n* The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n\n* The test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column."}}