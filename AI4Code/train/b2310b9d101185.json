{"cell_type":{"69e1dc99":"code","35bb390c":"code","79d621e3":"code","3b181d5f":"code","26e4919b":"code","7121cea4":"code","7e35e9f8":"code","486cfe04":"code","84ad75ae":"code","aba50db2":"code","21ee0f43":"code","74b02b05":"code","c12a3ccf":"code","5f9802b9":"code","569385bb":"code","f725cd70":"code","a3376e71":"code","c3fc9f2e":"code","1de097b3":"code","06539273":"markdown","c0e77cac":"markdown","22fe5b89":"markdown","01816fab":"markdown","b5bf729a":"markdown","9b031729":"markdown","42c3f970":"markdown","dc27ce0a":"markdown","6df483b5":"markdown","7d92d7e5":"markdown","a9fda739":"markdown","2da73baf":"markdown","03a65181":"markdown","4290c579":"markdown","61e01f9f":"markdown","e4497008":"markdown","9c8a1c0b":"markdown","c6ea80f1":"markdown","ebf7cff5":"markdown","4608a48e":"markdown","dd373cc5":"markdown"},"source":{"69e1dc99":"!pip install praat-textgrids\n","35bb390c":"\nimport numpy as np \nimport textgrids\n\nFRAME_DURATION = 30 # 30 msec\nOVERLAP_RATE = 0.0 # overlap rate between frames: if 0.0 two frames don't overlap, if 1.0 two frames completely overlap\n\n# costants of the labels\nNONSPEECH = 0\nSPEECH = 1\n","79d621e3":"import matplotlib.pyplot as plt\n\ndef plotLabels(data, t, t_label, fs, label_list):\n\n    figure = plt.Figure(figsize=(10, 7), dpi=85)\n    \n    plt.plot(t, data)\n    \n    shift = 1 - OVERLAP_RATE\n    frame_length = get_frame_length(FRAME_DURATION, fs) # frame length in sample\n    frame_shift = round(frame_length * shift)# frame shift in sample\n\n    for i, frame_labeled in enumerate(label_list):\n        idx = i * frame_shift\n\n        if (frame_labeled == SPEECH):\n            plt.axvspan(xmin= t_label[idx], xmax=t_label[idx + frame_length-1], ymin=-1000, ymax=1000, alpha=0.4, zorder=-100, facecolor='g', label='Speech')\n\n    plt.title(\"Labels\")\n    plt.legend(['Signal', 'Speech'])\n    plt.show()","3b181d5f":"\n\ndef readFile(path):\n    '''\n    Read the file and return the list of SPEECH\/NONSPEECH labels for each frame\n    '''\n        \n    labeled_list  = []\n    grid = textgrids.TextGrid(path)\n\n    for interval in grid['silences']:\n        label = int(interval.text)\n\n        dur = interval.dur\n        dur_msec = dur * 1000 # sec -> msec\n        num_frames = int(round(dur_msec \/FRAME_DURATION)) # the audio is divided into 30 msec frames\n        for i in range(num_frames):\n            \n            labeled_list.append(label)\n\n    return np.array(labeled_list)\n","26e4919b":"import librosa as lb # very useful library to process audio signals\n\ndef get_frame_length(duration, fs):\n    return int(np.floor(duration * fs \/ 1000))\n\ndef split_into_frames(data, frame_length, overlap_rate):\n    # This function divide the singnal into frames\n    \n    shift = 1.0 - overlap_rate\n    frame_shift = round(frame_length * shift)  # shift length in samples: hop_length\n    frame_list = lb.util.frame(x = data, frame_length = frame_length, hop_length = frame_shift, axis=0)  # matrix: each row represents a frame\n\n    return frame_list\n    \ndef windowed_frames(data, fs, duration=30, overlap_rate=0.0, window_type=\"hamming\"):\n    \"\"\"\n    The audio is divided into frames and a windows is applied to each of them\n\n    :param duration: frame duration in msec\n    :param overlap_rate: how much a frame overlap with the next one (percentage)\n    :param window_type: the type of of window applied to each frame\n    \"\"\"\n\n    frame_length =  get_frame_length(duration, fs) # frame length (in samples) s -> msec\n\n    frame_list = split_into_frames(data, frame_length, overlap_rate)\n\n    window_filter = lb.filters.get_window(window =  window_type, Nx = frame_length, fftbins = False) # create the filter\n    \n    return np.multiply(frame_list, window_filter)\n        \ndef energyPerFrame(data, fs):\n    \n    filtered_frame_list = windowed_frames(data = data, fs = fs, duration = FRAME_DURATION, overlap_rate = OVERLAP_RATE, window_type=\"hamming\") # to attenuate the noise an hamming window is applied to each frame\n    return np.array([ np.sum(frame ** 2, axis=0) for frame in filtered_frame_list ])\n","7121cea4":"from scipy.io import wavfile\n\nroot ='\/Female\/TIMIT\/SA2'\nannotation_path = \"\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Annotation\/Female\/TMIT\/SI2220.TextGrid\"\naudio_path = \"\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Audio\/Female\/TMIT\/SI2220.wav\"\n\n# read annotaion\nlabel_list = readFile(annotation_path)\n\n# read wav file\nfs, data = wavfile.read(audio_path)\ndata = np.array(data, dtype=np.float)\n\n# define time axis\nNs = len(data)  # number of sample\nTs = 1 \/ fs  # sampling period\nt = np.arange(Ns) * 1000 * Ts  # time axis\n\nplotLabels(data, t, t, fs, label_list)","7e35e9f8":"\nenergy_frames = energyPerFrame(data, fs)\n\ndiff = len(label_list) - len(energy_frames)\nprint(\"Difference: \", diff)","486cfe04":"label_list = label_list[: len(label_list) - diff]\n\nframe_length = get_frame_length(FRAME_DURATION, fs)\n\ntime = []\nfor i in range(len(energy_frames)):\n    time.append((i * frame_length) * Ts * 1000)\n    ","84ad75ae":"plotLabels(energy_frames, time, t, fs, label_list)","aba50db2":"print(\"Minimum energy: \", min(energy_frames))\nprint(\"Maximum energy: \", max(energy_frames))\n\n# normalize the energy\nenergy_frames = energy_frames \/ max(energy_frames) # now new range is [0, 1]\nplotLabels(energy_frames, time, t, fs, label_list)","21ee0f43":"threshold = 0.005\npredicted_label = np.zeros(label_list.shape)\n\n# thresholding \npredicted_label[energy_frames >= threshold] = SPEECH","74b02b05":"plotLabels(data, t, t, fs, predicted_label)","c12a3ccf":"from sklearn.metrics import balanced_accuracy_score\n\nprint(\"Balance accuracy: \", balanced_accuracy_score(predicted_label, label_list))","5f9802b9":"\nroot ='\/Female\/TIMIT\/SA2'\nannotation_path = \"\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Annotation\/Male\/TMIT\/SI562.TextGrid\"\naudio_path = \"\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Audio\/Male\/TMIT\/SI562.wav\"\n\n# read annotaion\nlabel_list = readFile(annotation_path)\n\n# read wav file\nfs, data = wavfile.read(audio_path)\ndata = np.array(data, dtype=np.float)\n\n# define time axis\nNs = len(data)  # number of sample\nTs = 1 \/ fs  # sampling period\nt = np.arange(Ns) * 1000 * Ts  # time axis\n\nplotLabels(data, t, t, fs, label_list)","569385bb":"energy_frames = energyPerFrame(data, fs)\n\ndiff = len(label_list) - len(energy_frames)\nprint(\"Difference: \", diff)\n\nenergy_frames = energy_frames[: len(energy_frames) - abs(diff)]","f725cd70":"\nprint(\"Minimum energy: \", min(energy_frames))\nprint(\"Maximum energy: \", max(energy_frames))\n\n# normalize the energy\nenergy_frames = energy_frames \/ max(energy_frames) # now new range is [0, 1]","a3376e71":"\n\nframe_length = get_frame_length(FRAME_DURATION, fs)\n\ntime = []\nfor i in range(len(energy_frames)):\n    time.append((i * frame_length) * Ts * 1000)\n    \n    \nplotLabels(energy_frames, time, t, fs, label_list)","c3fc9f2e":"threshold = 0.005\npredicted_label = np.zeros(label_list.shape)\n\n# thresholding \npredicted_label[energy_frames >= threshold] = SPEECH","1de097b3":"print(\"Balance accuracy: \", balanced_accuracy_score(predicted_label, label_list))","06539273":"## Define useful functions","c0e77cac":"Load a file with a male speaker","22fe5b89":"Compute the balance accuracy since we don't know how many SPEECH\/NONSPEECH segments there are ","01816fab":"Try classify the frames using their energy: It is a very common feature for speech\/silence detection but it loses its effectiveness in noisy scenario.\n\n$$ \\Sigma_{n}x(n)^2 $$\n\n![image.png](attachment:image.png)\n\n* If the energy of the incoming frame is high, the frame is classified as a SPEECH frame.\n* If the energy of the incoming frame is low, it is labeled as NON-SPEECH.\n\nIn order to calculate the feature a Hamming window is previously applied to the frame.","b5bf729a":"#### Function extracting the energy of the signal for each frame","9b031729":"### Set the threshold and compute labels","42c3f970":"Thresholding the energy","dc27ce0a":"#### Function extracting the ground truth labels","6df483b5":"#### function to plot the signal with the labels","7d92d7e5":"And plot them with the ground trouth labels","a9fda739":"Compare the goundtruth labels with the predicted one","2da73baf":"### Normalizing the features","03a65181":"### Estracting the energy features","4290c579":"### Load an audio from the TIMIT corpus","61e01f9f":"Due to the precision of the intervals, there might be more or less labeles than features. We need to get rid of the additional elements.","e4497008":"Compute energy and normalize","9c8a1c0b":"With the same threshold we get lower accuracy wtr the first audio signal and considering that such signals are \"noiseless\" it's not a very good result. This means we need to compute the threshold value for each different signal in order to get higher performances. Other features can be considered, such as the **Zero-Crossing-Rate** which is a meaningful feature in the speech processing field.\nHowever the thresholding approach is simple but non very effective when the signals are quite affected by noise (white noise and background noise), the neural networks can provided pretty good results within the right set of short-time features.","c6ea80f1":"### Install library","ebf7cff5":"# Find speech segments","4608a48e":"Choose a threshold so that\n* energy >= threshold, the frame is classified as SPEECH\n* energy < threshold, the frame is classified as NONSPEECH","dd373cc5":"## Try same threshold with a different audio"}}