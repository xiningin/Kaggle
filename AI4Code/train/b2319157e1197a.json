{"cell_type":{"81b2bacb":"code","1ba4996f":"code","dd10e96c":"code","0fe98909":"code","4181b5e9":"code","969c1a47":"code","d61afcc4":"code","f518cbcf":"code","70021262":"code","221408f1":"code","d813edb8":"code","383e72a7":"code","a02bd8d2":"code","7881c6e2":"code","c83b7d52":"code","5b1db9c1":"code","de1eb5ee":"code","ba002779":"code","5a7c98d2":"code","a1e13777":"code","3199f93c":"code","6d5af209":"code","e59cc37e":"markdown","c656c015":"markdown","2871bd21":"markdown","5cf37569":"markdown","5dee69f7":"markdown","14c4c70f":"markdown","ebd2fa66":"markdown","e6333581":"markdown","d65f2b1b":"markdown","9b49a26f":"markdown","5779f66b":"markdown","873a1d37":"markdown","d3a1774a":"markdown","c74ff787":"markdown","a21a88a6":"markdown","1db50894":"markdown","5b1562f3":"markdown","0d43e224":"markdown"},"source":{"81b2bacb":"# Import the models needed \n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv) \n\n\nimport tensorflow as tf\n\n#get the version of tensorflow\nprint(\"Version: \", tf.__version__)\n#eager mode\n\nprint(\"Eager mode: \", tf.executing_eagerly())\n\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","1ba4996f":"file = \"\/kaggle\/input\/toxic-arabic-tweets-classification\/toxic arabic tweets classification.txt\"\ntweets = pd.read_csv(file,sep=\"\\t\")","dd10e96c":"tweets.head()","0fe98909":"tweets.isnull().sum()","4181b5e9":"tweets.describe()","969c1a47":"tweets['Class'].unique()","d61afcc4":"tweets.shape","f518cbcf":"import matplotlib.pyplot as plt\nclasses= ['Abusive tweets','Normal tweets','Hate tweets']\nvalues =[len ( tweets[tweets['Class']=='abusive'].index ),len ( tweets[tweets['Class']=='normal'].index ),len ( tweets[tweets['Class']=='normal'].index )]\nplt.title('Occurrences of type of tweets')\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.bar(classes,values)\n","70021262":"def decodeValues(value):\n    #'abusive', 'normal', 'hate'\n    if value == 'abusive':\n        return 1\n    elif value == 'normal':\n        return 2\n    elif value == 'hate':\n        return 3\n\n#Map each class into a numerical value\ntweets['Class'] = tweets['Class'].apply(decodeValues)\n\ntweets.head()","221408f1":"import re\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\" # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n","d813edb8":"def split_white_space(text):\n    text = text.lower().split()\n    return text\n","383e72a7":"import string\n\ndef remove_punctuation(text):\n    result = string.punctuation\n    listText=[]\n    for words in text:\n        String =\"\"\n        for word in words:\n            if word not in result:\n                String+=word\n            else:\n                break\n        if (String!=\"\") :\n            listText.append(String)    \n    return listText","a02bd8d2":"def clean_data(text):\n    text = remove_emoji(text)\n    #text = split_white_space(text)\n    #text = remove_punctuation(text)\n    return text\n\ntweets['Tweet'] = tweets['Tweet'].apply(clean_data)\n\ntweets.head()","7881c6e2":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split( tweets['Tweet'], tweets['Class'], test_size=0.2)\n\n","c83b7d52":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nprint('Number of Unique Tokens',len(tokenizer.word_index))\n\nvocab_size = len(tokenizer.word_index) + 1\n\nmaxlen = 200\n\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\nprint( X_train.shape )\nprint(X_train,y_train)","5b1db9c1":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Flatten, MaxPooling1D, Input, Concatenate\nvocab_size = 10000\nembedding_dim = 1000\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\nmodel = tf.keras.Sequential([\n    #embedding layer(input)\n    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n    tf.keras.layers.SimpleRNN(50),\n    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n    tf.keras.layers.Dense(145, activation='relu'),\n    #output layer\n    tf.keras.layers.Dense(4, activation=\"softmax\")\n    ])\nmodel.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\" , metrics=[\"accuracy\"])\n# train model normally\nmodel.fit(X_train, y_train)","de1eb5ee":"test_lost , test_acc_rnn = model.evaluate(X_test, y_test)\n\n\nprint(\"The accuracy of the RNN model is:\",(test_acc_rnn*100))","ba002779":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding\nvocab_size = 10000\nembedding_dim = 1000\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n     model = tf.keras.Sequential([\n        #Word embdading layer (Input layer)\n        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n        tf.keras.layers.LSTM(44),\n        tf.keras.layers.Dense(embedding_dim, activation='softmax'),\n        tf.keras.layers.Dense(140, activation='relu'),\n        tf.keras.layers.Dense(150, activation='softmax'),\n        #Output layer(We use softmax activation function in multiple classification)\n        tf.keras.layers.Dense(4, activation=\"softmax\")\n    ])\n\nmodel.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\" , metrics=[\"accuracy\"])\n# train model normally\nmodel.fit(X_train, y_train,epochs=15)","5a7c98d2":"test_lost , test_acc_rnn = model.evaluate(X_test, y_test)\n\n\nprint(\"The accuracy of the LSTM model is:\",(test_acc_rnn*100))","a1e13777":"model.save(r'.\/LSTM.h5')","3199f93c":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding\nvocab_size = 10000\nembedding_dim = 1000\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nfrom tensorflow.keras import layers\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\n\nwith tpu_strategy.scope():\n     model = tf.keras.Sequential([\n        #Word embdading layer (Input layer)\n        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(10)),\n        tf.keras.layers.Dense(embedding_dim, activation='relu'),\n        tf.keras.layers.Dense(140, activation='relu'),\n        tf.keras.layers.Dense(150, activation='relu'),\n        #Output layer(We use softmax activation function in multiple classification)\n        tf.keras.layers.Dense(4, activation=\"softmax\")\n    ])\n\nmodel.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\" , metrics=[\"accuracy\"])\n# train model normally\nmodel.fit(X_train, y_train)","6d5af209":"test_lost , test_acc_rnn = model.evaluate(X_test, y_test)\n\n\nprint(\"The accuracy of the Bi-LSTM model is:\",(test_acc_rnn*100))","e59cc37e":"Decode the value of the class column to make it traitable with DL algorithmes","c656c015":" ## show the nullable data","2871bd21":"## Get the distinct values of classes of tweets","5cf37569":"## remove pnctuation","5dee69f7":"We will internsetted to prepare data by fellowing the steps:\n* Decode the value of the class column\n* clean the data of tweets column\n* Decode the tweets column","14c4c70f":"# EDA","ebd2fa66":" ## show the head of data","e6333581":"# Data preparation","d65f2b1b":"# Vectorizing the words using hot encoding","9b49a26f":"# Load data","5779f66b":"# Apply LSTM architecture","873a1d37":"# Getting the number of values","d3a1774a":"## clean the data of tweets column\nWe will intressted in this section in data cleaning:\nfirst we need to delete all the non alphabetics values: \n*  deduplicate \n*  Removing puctuations\n*  Removing URL data\n*  removing emojies\n","c74ff787":"## removing emojies","a21a88a6":"## describe the data","1db50894":"## Visualise each class","5b1562f3":"## split text by space","0d43e224":" # Apply RNN architecture"}}