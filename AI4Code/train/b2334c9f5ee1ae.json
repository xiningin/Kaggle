{"cell_type":{"47bc0dcb":"code","c7e7110a":"code","c5a2f7f3":"code","6f38fa19":"code","3469dcc8":"code","ba6cb512":"code","1bee9e19":"code","5c8f2bbf":"code","cd580225":"code","3216f270":"code","bb621bbb":"code","354b75f3":"code","c041c0ca":"code","6afd4811":"code","7f87c6a4":"code","98e4622f":"code","6f894a7d":"code","55b09ceb":"code","84346f01":"code","aa1d9029":"code","0174496a":"code","a5a2b78f":"code","8e87bab4":"code","0a58588e":"code","1ddef27a":"code","d2c13b3e":"code","491aa948":"code","50fb5d39":"code","ae6518c9":"code","3162f6f0":"code","8afb5632":"code","d7693cd3":"code","fc5984a8":"code","45198707":"code","7682fe5a":"code","19367dce":"code","78efab62":"code","9a337490":"code","5a636c66":"code","7dd31036":"code","83750289":"code","0943db6a":"code","1bb7280a":"code","1df76223":"code","9686b05e":"code","1e705241":"code","542f41b2":"code","3d01a59a":"code","54144a3b":"code","fbccf32c":"markdown","24ba5d21":"markdown","eb984bef":"markdown","62e2a6c9":"markdown","dd032480":"markdown","1c9990e6":"markdown","1a46765a":"markdown","ea989159":"markdown","3c02ad89":"markdown","7b7ba6dc":"markdown","bb9a71cf":"markdown","1ec631b4":"markdown","646e03bd":"markdown","9e66889f":"markdown","ccf2e177":"markdown","93766c30":"markdown","2c245361":"markdown","4cfb3fd7":"markdown","54d9ae47":"markdown","055a3aea":"markdown","456ec8a0":"markdown"},"source":{"47bc0dcb":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\n# import datetime\nimport warnings\nwarnings.filterwarnings('ignore')","c7e7110a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndata_dir = Path('\/kaggle\/input\/house-prices-advanced-regression-techniques\/')\npd.read_csv(data_dir \/ 'train.csv').head(2)","c5a2f7f3":"#jupyter nbconvert src\/startups_study.ipynb --no-input --to html\n\n%matplotlib inline\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option(\"display.precision\", 3)\npd.set_option(\"display.expand_frame_repr\", False)","6f38fa19":"target_variable = 'SalePrice'\n\ncat_columns = [\n    'MSSubClass', 'category', 'Street', 'Alley', 'LotShape',\n    'MSZoning', 'LandContour', 'LotConfig', 'Neighborhood',\n    'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle',\n    'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n    'BsmtQual', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n    'Heating', 'HeatingQC', 'Electrical', 'KitchenQual', 'Functional',\n    'FireplaceQu', 'GarageType', 'GarageQual', 'GarageFinish', 'PavedDrive', 'MiscFeature',\n    'SaleType', 'SaleCondition',\n    'ExterQual', 'ExterCond', 'BsmtCond', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', # <- ordered categories \n    # 'Utilities', \n\n]\ndate_columns = [\n    'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold', \n]\nbool_columns = [\n    'CentralAir', \n]\n\ncat_dict = {\n    c: 'category' for c in cat_columns\n}\nbool_dict = {\n    b: 'bool' for b in bool_columns\n}\ncolumns_types = cat_dict.update(bool_dict)\n","3469dcc8":"train = pd.read_csv(\n    data_dir \/ 'train.csv',\n    true_values = ['Y'],\n    false_values = ['N'],\n    dtype=columns_types,\n    # parse_dates=date_columns,\n)\n\ntest = pd.read_csv(\n    data_dir \/ 'test.csv',\n    true_values = ['Y'],\n    false_values = ['N'],\n    dtype=columns_types,\n    # parse_dates=date_columns,\n)\n\nprint(f'train shape: {train.shape}')\nprint(f'test shape: {test.shape}')\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\nntrain = train.shape[0]\nntest = test.shape[0]\nY_train = train['SalePrice'].values\n\nall_df = pd.concat((train, test)).reset_index(drop=True)\nall_df.drop(['SalePrice'], axis=1, inplace=True)\n\nprint(f'all_df shape: {all_df.shape}')","ba6cb512":"plt.figure(figsize=(20,4))\nsns.heatmap(\n    all_df.isnull(), \n    yticklabels=False, \n    cbar=False, \n    cmap='viridis',\n)","1bee9e19":"for cat in all_df.columns[all_df.isnull().any()]:\n    if cat in all_df.select_dtypes(include=['category']).columns:\n        if 'None' not in all_df[cat].cat.categories:\n            all_df[cat] = all_df[cat].cat.add_categories('None')\n        all_df[cat] = all_df[cat].fillna('None')\nall_df['PoolQC'] = all_df['PoolQC'].fillna('None')\nall_df['MiscFeature'] = all_df['MiscFeature'].fillna('None')\nall_df['GarageQual'] = all_df['GarageQual'].fillna('None')\nall_df[\"MasVnrArea\"] = all_df[\"MasVnrArea\"].fillna(0)","5c8f2bbf":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_df[\"LotFrontage\"] = all_df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","cd580225":"all_df = all_df.drop(['Utilities'], axis=1)","3216f270":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_df[col] = all_df[col].fillna(0)","bb621bbb":"plt.figure(figsize=(20,4))\nsns.heatmap(\n    all_df.isnull(), \n    yticklabels=False, \n    cbar=False, \n    cmap='viridis',\n)","354b75f3":"sns.distplot(train['SalePrice']);","c041c0ca":"#skewness and kurtosis\nprint(f'Skewness: {train[\"SalePrice\"].skew()}')\nprint(f'Kurtosis: {train[\"SalePrice\"].kurt()}')","6afd4811":"corr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(\n    corr,\n    cmap=cmap,\n    vmax=.8,\n    square=True,\n    mask=mask\n);","7f87c6a4":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corr.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(\n    cm,\n    cbar=True,\n    annot=True,\n    square=True,\n    cmap=cmap,\n    fmt='.2f',\n    annot_kws={'size': 10},\n    yticklabels=cols.values,\n    xticklabels=cols.values\n)\nplt.show()","98e4622f":"cols = ['OverallQual', 'GarageCars', 'FullBath']\nfor feature in cols:\n    train.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","6f894a7d":"#scatterplot\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(\n    train[cols],\n    size = 2.5\n)\nplt.show();","55b09ceb":"train.plot.scatter(\n    x='GrLivArea',\n    y='SalePrice',\n    color='b',\n);","84346f01":"# boxplots are better than scatterplots for categorical values\nsns.boxplot(\n    data=train,\n    x='OverallQual',\n    y='SalePrice',\n)","aa1d9029":"train.plot.scatter(\n    # train,\n    x='1stFlrSF',\n    y='SalePrice',\n    color='b',\n);","0174496a":"#standardizing data\nsaleprice_scaled = ((Y_train-Y_train.mean())\/Y_train.std())\nsaleprice_scaled.sort()\nlow_range = saleprice_scaled[:10]\nhigh_range = saleprice_scaled[-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","a5a2b78f":"#deleting points\ntrain.sort_values(by = 'GrLivArea', ascending = False)[:2]","8e87bab4":"\ntrain = train.drop(train[train_ID == 1299].index)\ntrain = train.drop(train[train_ID == 524].index)","0a58588e":"numeric_feats = all_df.dtypes[all_df.dtypes == 'int64'].index\n\n# Check the skew of all numerical features\nskewed_feats = all_df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint('Skew in numerical features:')\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","1ddef27a":"def plot_dist(data) -> None:\n    fig = plt.figure()\n    sns.distplot(data, fit=norm);\n    fig = plt.figure()\n    res = stats.probplot(data, plot=plt)","d2c13b3e":"plot_dist(Y_train)","491aa948":"# first lets see if works\nplot_dist(np.log1p(Y_train))","50fb5d39":"from scipy.special import boxcox1p\nplot_dist(1\/Y_train)\nplot_dist(np.sqrt(Y_train))\nplot_dist(Y_train**(1\/1.2))\nplot_dist(boxcox1p(Y_train, 0.15))","ae6518c9":"Y_train = np.log1p(Y_train)","3162f6f0":"#histogram and normal probability plot\nplot_dist(np.log(all_df['GrLivArea']))","8afb5632":"all_df['GrLivArea'] = np.log(all_df['GrLivArea'])","d7693cd3":"skewness = skewness[abs(skewness) > 0.75]\nskewed_features = skewness.index\nfor feat in skewed_features:\n    if feat is 'GrLivArea': continue\n    all_df[feat] = boxcox1p(all_df[feat], 0.15)","fc5984a8":"all_df_dummies = pd.get_dummies(all_df)\nall_df_dummies.sample(5)","45198707":"train_dummies = all_df_dummies[:ntrain]\ntest_dummies = all_df_dummies[ntrain:]\n\nprint(f'train shape: {train_dummies.shape}')\nprint(f'test shape: {test_dummies.shape}')\nprint(f'Y_train shape: {Y_train.shape}')","7682fe5a":"# label-encode features?\n# scikit pipelines?","19367dce":"import xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","78efab62":"# split the train set in train\/test so we can test the models with new rows\nX_train, X_test, y_train, y_test = train_test_split(\n    train_dummies, Y_train, test_size=0.10, random_state=42\n)\nprint(f'X_train: {X_train.shape}')\nprint(f'X_test: {X_test.shape}')\nprint(f'y_train: {y_train.shape}')\nprint(f'y_test: {y_test.shape}')","9a337490":"# The error metric: RMSE on the log of the sale prices.\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","5a636c66":"D_train = xgb.DMatrix(X_train, label=y_train)\nD_test = xgb.DMatrix(X_test, label=y_test)\nsteps = 20  # The number of training iterations\nparam = {\n    'colsample_bytree': 0.2, \n    'max_depth': 4,  \n    # 'objective': 'multi:softprob',  \n    'gamma': 0.0,\n    'learning_rate': 0.01,\n    'min_child_weight': 1.5,\n    'n_estimators': 7200,\n    'reg_alpha': 0.9,\n    'reg_lambda': 0.6,\n    'subsample': 0.2,\n    'seed': 42,\n    # 'silent': 1,\n} \n","7dd31036":"regr = xgb.XGBRegressor(\n    **param\n)\n\nregr.fit(X_train, y_train)\n\n# Run prediction on training set to get a rough idea of how well it does.\ny_pred = regr.predict(X_test)\nprint(f'XGBoost score on training set: {rmse(y_test, y_pred)}')","83750289":"# from sklearn.model_selection import GridSearchCV\n\n# clf = xgb.XGBClassifier()\n\n# parameters = {\n#      'eta'    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n#      'max_depth'        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n#      'min_child_weight' : [ 1, 1.5, 2, 2.5, 3, 5, 7 ],\n#      'gamma'            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n#      'colsample_bytree' : [ 0.1, 0.2, 0.3, 0.4, 0.5 , 0.7 ],\n#      'learning_rate'    : [ 0.01, 0.015, 0.02, 0.025],\n#      'min_child_weight' : [ 1.0, 1.5, 2 ],\n#      'n_estimators'     : [ 7200 ],\n#      'reg_alpha'        : [ 0.8, 0.9 ],\n#      'reg_lambda'       : [ 0.5, 0.6 ],\n#      'subsample'        : [ 0.1, 0.2, 0.3 ],\n#      'seed'             : [42],\n#      'silent'           : [1],\n# }\n\n# grid = GridSearchCV(\n#      clf,\n#      parameters, n_jobs=4,\n#      scoring=\"neg_log_loss\",\n#      cv=3,\n# )\n\n# grid.fit(X_train, y_train)\n\n# y_pred = regr.predict(X_test)\n# print(f'xgboost score on training set: {rmse(y_test, y_pred)}')\n# regr.dump_model('dump.raw.txt')","0943db6a":"n_folds = 5\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train_dummies.values, Y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","1bb7280a":"model_xgb = xgb.XGBRegressor(\n    **param\n)","1df76223":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","9686b05e":"model_xgb.fit(train_dummies.values, Y_train)","1e705241":"y_pred = model_xgb.predict(X_test)\nprint(f'XGBoost score on training set: {rmse(y_test, y_pred)}')","542f41b2":"# Run prediction on the Kaggle test set.\ny_pred_xgb = model_xgb.predict(test_dummies)\nxgb_pred = np.expm1(y_pred_xgb)","3d01a59a":"print(y_pred_xgb[:5])\nprint(xgb_pred[:5])","54144a3b":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = xgb_pred\nsub.to_csv('submission.csv',index=False)","fbccf32c":"Some columns like *Alley* and *PoolQC* look like have almost all values missing.  \nHowever, if you take a look at the data_description.txt file you'll see those values are not missing but just means this houses have no pool (for example).  \nSo in this case I opted for adding a new category (to those columns where NaN does not mean missing value) for these cases.  \n","24ba5d21":"It worked, so let's apply the transformation","eb984bef":"The SalePrice distribution doesn't have a normal distribution, but we know that positive skewed dsitributions often can be corrected with a logaritmic transformation","62e2a6c9":"## Modelling\n","dd032480":"Fill Garage Information with 0 for houses without a garage.","1c9990e6":"## Handle missing values\n\nBefore doing any analysis, lets try to figure it out how to handle missing values (NaN).  \nFirst, lets take a look at the missing values in each column in a graphical way:","1a46765a":"Convert the default datatype (object) to more appropiate datatypes like category and datetime.","ea989159":"Let's continue with other variables distribution","3c02ad89":"## Data exploration\nFirst, let's start by taking a look at the target variable SalePrice:","7b7ba6dc":"We can see that SalePrice deviates from the normal dsitribution and have a high positive skewness","bb9a71cf":"Now lets create a correlation matrix to get the variables that have a higher impact in the variation of the SalePrice:","1ec631b4":"## Modelling the data\n\n> Data with a normal distribution works better in ML models.\n\n- Histogram -> Kurtosis and skewness.\n- Normal probability plot -> Data distribution should closely follow the diagonal that represents the normal distribution.","646e03bd":"First step is to load the dataset, indentify the target (dependant) variable (SalePrice), and take a look at the different variables and its datatypes.","9e66889f":"# House Prices Prediction","ccf2e177":"Now that we don\u00b4t have any missing values, let's start the data exploration.","93766c30":"Now lets take a closer look to the most correlated variables to SalePrice","2c245361":"Let's try other common transformations:  \n- Reciprocal Transformation\n- Square Root Transformation\n- Exponential Transformation\n- Box-Cox Transformation\n    ","4cfb3fd7":"## Handling Outliers\nFirst, standarize the data to have 0 mean and 1 stdv,\nthen take a look at the higher and lower ranges of values.","54d9ae47":"Almost all rows have the same utilities value so it doesn't provide much information, we can drop this column","055a3aea":"LotFrontage is most likely similar for all houses in the same neightbourhood so we can fill NaNs with the neighbourhood median.","456ec8a0":"Quick look to the scatter plots of the relationship of the most correlated variables with SalePrice "}}