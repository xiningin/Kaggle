{"cell_type":{"8e7b4ca5":"code","5912727d":"code","db8afab1":"code","c6a93d93":"code","a59bd646":"code","1baebe3b":"code","b142b61f":"code","4476f172":"code","0886ba7c":"code","36e8f1ac":"code","4e4ae1ea":"code","d2211a5e":"code","37a01d11":"code","79dee5fd":"code","2eea32bb":"code","c55c6749":"code","d32dc952":"code","4656a64d":"code","091cc03a":"code","20940edc":"code","186aedb3":"code","00775190":"code","2650b33b":"code","84c0ba32":"code","0b59d952":"code","2716946d":"code","7209e9ff":"code","6a5f52ce":"code","b2382389":"code","68a59b22":"code","1d6332e4":"code","5647d5b2":"code","7ef55c56":"code","cdcefdd5":"code","efe68251":"code","f7a8bb9f":"code","3283ac5a":"code","1175458e":"code","177fa4ff":"code","29b3e352":"code","a4d07c18":"code","33946b78":"code","d5adbce7":"code","be74ac5c":"code","de02486b":"code","3f68babb":"code","992bf036":"code","3c23fd79":"code","4578a9c8":"code","23b0ebe5":"code","a070f99f":"code","15ff6e1f":"code","aad44ffd":"code","459ac01b":"code","b99eb510":"code","191bd8e5":"code","45284f4f":"code","33281001":"code","a018700e":"code","01b348af":"code","df89371c":"code","3cc36572":"code","cd4a88a1":"code","68e6c57f":"code","33abfadc":"code","d5677791":"code","4d602ca8":"code","58dc118f":"code","7e9bf612":"code","9969fb9a":"code","302cf726":"code","e09c6232":"code","e61e108f":"code","6ad7d8ee":"code","0f29c2f0":"code","b3081ad5":"code","f520fe4c":"code","8156f7df":"code","2671dafd":"code","78006a5c":"code","0f11980c":"code","a8b5ae2b":"code","e81644af":"code","b80274be":"code","40affe35":"code","93e464d2":"code","44845e73":"code","a8b8c3bf":"code","c734b807":"code","075b0353":"code","755ac28c":"code","3d8279f9":"code","9bcd7d57":"code","28c2d822":"code","eba56b72":"code","fa259c04":"code","9fddfadc":"code","591ac640":"code","5826fb7c":"code","1c40339d":"code","daf6fa1e":"code","d108141b":"code","7d7ca610":"code","d3c0a3d3":"code","4c2118ad":"code","b9df8b61":"code","3308ff50":"code","f7532826":"code","f75abfb8":"code","8f9aa3c4":"code","55e9d149":"code","598c3b84":"code","30bbbb04":"code","fa347406":"code","32ac0d0a":"code","0a2165be":"code","35130b7c":"code","7318bd21":"code","9169fd14":"code","3837a223":"code","f2cb2d6a":"code","e2abbc8c":"code","b68b07ae":"code","8012b7b0":"code","c6a2f6f8":"code","4cb22662":"code","88c6d32c":"code","ad0aaa84":"code","2ecbb5a8":"code","6810b6a7":"code","fefb8e90":"code","08c6a82b":"code","b087b79d":"code","3de77b06":"code","d46b13d9":"code","6bac9f22":"code","c23fbb12":"code","6ece028e":"code","a6a57d74":"code","2923156c":"code","d23f7a1b":"code","affdf8fe":"code","6ffe115a":"code","48d360f4":"code","a4f8adc9":"code","aad6f396":"code","5451203c":"code","228c69fc":"code","8ada9a7d":"code","2e4ba832":"code","a588a5bb":"code","6d79e97c":"code","a8bdb5bf":"code","d348ffd6":"code","9298a4c3":"code","f4b9449b":"code","89880438":"code","1f9776fc":"code","ac9b1d5e":"code","a204cd15":"code","9ef46861":"code","0175f8b6":"code","746a861a":"code","e14f9fbf":"code","3964d3b5":"code","ba8c069d":"code","9000815a":"code","f667e7a2":"code","f94323cb":"code","097d824c":"code","66f25751":"code","e2ef191a":"code","0c72db2f":"code","c0984504":"code","c9beb5a5":"code","df5211f8":"code","92591a5c":"code","e77d41cf":"code","9e9bc7bb":"code","abb874ed":"code","bb659571":"code","656c0c65":"code","5ccfb31a":"code","af1bf5ba":"code","5c3e49f4":"code","cc1c4aed":"code","4dc7a071":"code","bd9c304f":"code","6ef18d5a":"code","2ef99602":"code","7624387f":"code","aa9365e2":"code","b43e146d":"code","3c49169f":"code","561cf4f1":"code","ec9d1aac":"code","cfb7a13b":"code","424c4b28":"code","0c5349e4":"code","84ec4da3":"code","06c994bf":"code","4030fac7":"code","9aafe38f":"code","c36dc7a0":"code","05f1ffe1":"code","70cf46a9":"code","7be48d18":"code","a73cb5c8":"code","618feb1c":"code","d37ed269":"code","b052aa03":"code","d17f00ba":"code","35be0b9c":"code","f50efdf2":"code","f11c001c":"code","3fb7bfd3":"code","ca66b30a":"code","87bab4e7":"code","f6f9a825":"code","8be19520":"code","3eece254":"code","04cbab3f":"code","248f7cf4":"code","c292e8e4":"code","b7c159ee":"code","f24a84fd":"code","48a3c81e":"code","b73272a5":"code","f8aeb1a9":"code","f7cccfff":"code","28960525":"code","85ff0c64":"code","3dad8e63":"code","530606bf":"code","9bc995ca":"code","0b36b07d":"code","72bb6750":"code","5c4ccc4c":"code","70b80f2b":"code","4f18ee19":"code","491c527e":"code","8c3e7b53":"code","da0206e5":"code","e2899c1f":"code","d8750c4d":"code","7f738258":"code","3b76e0ed":"code","d256758c":"code","6b521f8c":"code","c59891bb":"code","a643fb54":"code","5c45e523":"code","42a4c031":"code","6f4f6b10":"code","3b030ab6":"code","4b096c62":"code","38eb3279":"code","d8ffb481":"code","c4c24c39":"code","946a9f60":"code","569e72b9":"code","2312a8d2":"code","119566b1":"code","5563dbbd":"code","f6b319a5":"code","48f71cad":"code","08135146":"code","70bab5dc":"code","4b29674a":"code","68cc91e7":"code","722f7321":"code","e24d9a2e":"code","5436074b":"code","a4a068f8":"code","6fc15755":"code","a6c8eb4f":"code","e84b83d8":"code","f553cbe4":"code","25d4d4d3":"code","c28fba9c":"code","bd50d443":"code","b722bbb4":"code","14395c63":"code","61cf864d":"code","86a43187":"code","ab4951e2":"code","ea0ba0df":"code","a9f13960":"code","b638d963":"code","1c7c2c44":"code","9d4b4dcc":"code","1ed46283":"code","668169e3":"code","46a053c5":"code","11731024":"code","da4d7113":"code","f5933f28":"code","c13f3b54":"code","e6f1e5ea":"code","01f4c0eb":"code","ef2f5f6b":"code","bb8def9c":"code","3b880b10":"code","6542a54a":"code","a86b5e30":"code","b15741f0":"code","8038e125":"code","dfd28485":"code","89af59d2":"code","cc39a1b3":"code","d6e50788":"markdown","f35d0d63":"markdown","3907cb87":"markdown","706ea828":"markdown","3611ec0e":"markdown","484dcfba":"markdown","819eb92c":"markdown","e5c56be8":"markdown","a580cd9f":"markdown","c50dec47":"markdown","940c4e76":"markdown","ce6ea089":"markdown","1c5e6b75":"markdown","ad1d700f":"markdown","2840fb00":"markdown","dc393ae6":"markdown","ec790899":"markdown","77a076fc":"markdown","299245c1":"markdown","53f03414":"markdown","b3c52131":"markdown","0e9ac707":"markdown","0c4f8023":"markdown","32fe40b0":"markdown","c5740542":"markdown","e15dde24":"markdown","cf0e1373":"markdown","f4653245":"markdown","6b3758e5":"markdown","7400ffa1":"markdown","2554e2d5":"markdown","decb4ec5":"markdown","ff6049ed":"markdown","5793f7b7":"markdown","e0842724":"markdown","36f0f18f":"markdown","4f295960":"markdown","f91ac16a":"markdown","2a6e8596":"markdown","ba9b53de":"markdown","7fd4740a":"markdown","d06977a3":"markdown","92d649b9":"markdown","3e48bd38":"markdown","c7fc1cc6":"markdown","7cdaa43e":"markdown","72d77962":"markdown","252a122b":"markdown","52a53932":"markdown","abf33e80":"markdown","6368c272":"markdown","d737feec":"markdown","e54106f1":"markdown","26066220":"markdown","b97ff540":"markdown","79b2df77":"markdown","15ab0319":"markdown","cf0e90a2":"markdown","29658f50":"markdown","d317f11f":"markdown","c78a91c0":"markdown","c022bcd4":"markdown","0a46ea52":"markdown","1dd47430":"markdown","5d82a0ba":"markdown","8572fd6a":"markdown","5fe0ec58":"markdown","15884252":"markdown","89ec0979":"markdown","eb4840e7":"markdown","ba43611f":"markdown","b42efc40":"markdown","ad944e8b":"markdown","d7904540":"markdown","3b4735af":"markdown","abd3538b":"markdown","162e052c":"markdown","d477275a":"markdown","87e4443b":"markdown","3776512f":"markdown","6b632d32":"markdown","4d6d8b78":"markdown","fdaef796":"markdown","5a059975":"markdown","31e9dd68":"markdown","55b1de11":"markdown","8b781cdd":"markdown","1c4fefee":"markdown","fde787e6":"markdown","604d24b4":"markdown","6bb342e0":"markdown","d88cad25":"markdown","6c8ae232":"markdown","35fcbcf6":"markdown","445245a9":"markdown","ade7c0f7":"markdown","28d09627":"markdown","1c3971be":"markdown","dbfca96a":"markdown","eae847de":"markdown","e865cfdd":"markdown","3ee22792":"markdown","7688c50e":"markdown","5e43e1b2":"markdown","7ae2da0e":"markdown","3d6c5f37":"markdown","0c062793":"markdown","f5c5357c":"markdown","6db43c9b":"markdown","eaf8e95e":"markdown","d8e0c149":"markdown","81aad13b":"markdown","42310f6a":"markdown","d31e43dc":"markdown","2a23937a":"markdown","edaf9b0d":"markdown","d7eb31e8":"markdown","053c34d1":"markdown","d2faa768":"markdown","ffe2386a":"markdown","10a2efab":"markdown","7dcc0d57":"markdown","63630a0c":"markdown","0158e041":"markdown","09dd86ce":"markdown","bbf774bb":"markdown","8be3e9a6":"markdown","50a8cd78":"markdown","e36d3b01":"markdown","099d2b15":"markdown","a7190f9a":"markdown","88b9646f":"markdown","5e4c592b":"markdown","0210e6b4":"markdown","27d81a9b":"markdown","9e7e346c":"markdown","ab60eec8":"markdown","fdaed75f":"markdown","321d9fc8":"markdown","3a9e12fd":"markdown","1fd0ac7d":"markdown","be89625f":"markdown","6594cb64":"markdown","fa24f6e7":"markdown","f5d7a125":"markdown","aa485b7f":"markdown","3908356a":"markdown","d6640772":"markdown","83f31322":"markdown","9382a054":"markdown","db5b245e":"markdown","5563d883":"markdown","69b9ecb9":"markdown","1b8be421":"markdown","aff69627":"markdown","c1bf3082":"markdown","d8f36a00":"markdown","f112c3c0":"markdown","55d32844":"markdown","76477f3e":"markdown","dcdf7109":"markdown","7b5619fb":"markdown","53e1274a":"markdown","446fd051":"markdown","c00b5e98":"markdown"},"source":{"8e7b4ca5":"from google.colab import drive\ndrive.mount('\/gdrive')\n%cd \/gdrive","5912727d":"# Import our libraries we are going to use for our data analysis.\nimport tensorflow as tf\nimport pickle\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import preprocessing\nfrom sklearn.utils import class_weight\n\n\n# For oversampling Library (Dealing with Imbalanced Datasets)\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\n# Other Libraries\nimport time\n\n% matplotlib inline\n#NLP\nimport nltk\nnltk.download('words')\nnltk.download(\"stopwords\") \nnltk.download('punkt')\nnltk.download('wordnet')\n# Importing the necessary functions\nimport nltk, re\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nwords = set(nltk.corpus.words.words())\nimport string\n\n#libraries for machine learning algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.svm import SVC\nfrom sklearn_pandas import DataFrameMapper, CategoricalImputer\nfrom sklearn.svm import LinearSVR\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n#importing necessary Decision Tree libraries\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\n#importing necessary Random Forest Classifier library\nfrom sklearn.ensemble import RandomForestClassifier\n#importing necessary MLP library for Neural Network\nfrom sklearn.neural_network import MLPClassifier\n#importing necessary library for Naiye Bayes\nfrom sklearn.naive_bayes import GaussianNB\n#importing necessary library for LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n#importing necessary library for Gradient Boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\n#importing necessary library for Support Vector Machines\nfrom sklearn.svm import SVC\n#importing necessary libraries for KNN classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score\n","db8afab1":"start_df = pd.read_csv(\"\/gdrive\/My Drive\/Capstone Project - NLP\/fullacc.csv\",low_memory=False)","c6a93d93":"df = start_df.copy(deep=True)\ndf.head()","a59bd646":"print('There are {} rows and {} columns in the dataset.'.format(df.shape[0],df.shape[1]))","1baebe3b":"#printing the name of columns\ndf.columns","b142b61f":"# This will print basic statistics for numerical columns\ndf.describe()","4476f172":"#Removing all other predictors and their associated predictor columns containing amount related data other than loan amount which we will be using for modelling\ndf.drop(['funded_amnt','funded_amnt_inv','last_pymnt_amnt','delinq_amnt'],axis=1,inplace=True)","0886ba7c":"#removing grade, sub grade and interest columns\ndf.drop(['grade','sub_grade','int_rate'],axis=1,inplace=True)","36e8f1ac":"df_description = pd.read_excel(r'\/gdrive\/My Drive\/Capstone Project - NLP\/Harsh\/LCDataDictionary.xlsx').dropna()\nprint(df_description.shape[0])\ndf_description.style.set_properties(subset=['Description'], **{'width': '1000px'})","4e4ae1ea":"drop_list = ['id','member_id','issue_d']\ndf = df.drop(drop_list,axis=1)","d2211a5e":"df['emp_title'].value_counts()","37a01d11":"#Dropping another column that I deem unnecessary. It contains data in an unorganized way which will not be very helpful for modeling. \ndrop_list2 = ['emp_title']\ndf = df.drop(drop_list2,axis=1)","79dee5fd":"df.head(1)","2eea32bb":"#Url is also not much of importance to us. Dropping it. \ndrop_list3 = ['url']\ndf = df.drop(drop_list3,axis=1)","c55c6749":"#leaving description field as I will be trying to apply NLP techniques on it for feature engineering. ","d32dc952":"#exploring what is there in purpose predictor\ndf['purpose'].unique()","4656a64d":"# recode loan purpose \ndf['purpose_n'] = np.nan #Creating new column and filling it with nan values\n\n#filter by debt consolidation, CC and storing them in new columns with a common name \"DEBT\"\ndf.loc[(df['purpose'] == 'debt_consolidation')|(df['purpose'] ==\"credit_card\"), 'purpose_n'] = 'debt' \n#filter by home improvement, major purchase, car, house, vacation, renewable energy \n#and storing them in new columns with a common name \"major purchases\"\ndf.loc[(df['purpose'] == 'home_improvement')|(df['purpose'] ==\"major_purchase\")|\n                 (df['purpose'] == 'car')|(df['purpose'] ==\"house\")|\n                 (df['purpose'] == 'vacation')|(df['purpose'] ==\"renewable_energy\"),\n                 'purpose_n'] = 'major_purchases' \n#filter by small business, medical, moving, wedding, educational \n#and storing them in new columns with a common name \"life events\"                 \ndf.loc[(df['purpose'] == 'small_business')|(df['purpose'] ==\"medical\")|\n                 (df['purpose'] == 'moving')|(df['purpose'] ==\"wedding\")|\n                 (df['purpose'] == 'educational'),\n                 'purpose_n'] = 'life_events'\n#the remaining category will remain with the same name 'other' in new columns                 \ndf.loc[(df['purpose'] == 'other'), 'purpose_n'] = 'other'","091cc03a":"df['title'].unique()#.tolist()\n#There are too many unique values to get a meaning out of it. Also, it contains more or less similar information as Purpose predictor which is much cleaner. So, we will drop the title field.","20940edc":"#As we have created a new column after recoding the values under purpose, dropping the original purpose predictor. \n#Also, the title predictor contains too many unique values and is more or less similar to purpose predictor which we recoded above\ndrop_list4 = ['purpose','title']\ndf = df.drop(drop_list4,axis=1)","186aedb3":"print(df['zip_code'].head())\nprint(df['addr_state'].value_counts().head(5))","00775190":"#Zip code and State contains similar information. Also, zip code does not even have the entire zip code value and just 3 digits. So, dropping it and keeping state. \ndrop_list5 = ['zip_code']\ndf = df.drop(drop_list5,axis=1)","2650b33b":"df.earliest_cr_line.head(3)","84c0ba32":"import datetime\n\n# calculate time since first credit line\nnow = datetime.datetime.today() #prints current date\ndef credit_age (x):\n    if x != 'nan': #filter non null\n        c1 = datetime.datetime.strptime(x, '%b-%y') #strips the present date in mon-year format \n        #b-Abbreviated month name.\ty-Year without century as a zero-padded decimal number.\t#Reference: https:\/\/www.programiz.com\/python-programming\/datetime\/strftime\n        return (now-c1).days\/365.25\n        #return c1\n    else:\n        return None\n\ndf['earliest_cr_line_n'] = df['earliest_cr_line'].astype(str)\ndf['earliest_cr_line_n'] = df['earliest_cr_line_n'].apply(credit_age)\n","0b59d952":"df['earliest_cr_line_n'].head()","2716946d":"#Removing the original columns \"earliest_6cr_line\" because we have transformed it to a new one\ndrop_list6 = ['earliest_cr_line']\ndf = df.drop(drop_list6,axis=1)","7209e9ff":"def null_values(df): #creates a function with below logic\n        mis_val = df.isnull().sum() #gives sum of missing values (null values)\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df) #getting percentage of missing values\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1) #creating a DF containing missing value count and it's percentage\n        mis_val_table_ren_columns = mis_val_table.rename( \n        columns = {0 : 'Missing Values', 1 : '% of Total Values'}) #renaming the columns\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1) #getting the percentage of missing values in descending order and rounding it to 1 decimal\n        print (\"Dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\") #printing number of columns and missing value count\n        return mis_val_table_ren_columns","6a5f52ce":"# Missing values statistics\nmiss_values = null_values(df)\nmiss_values.head(20)","b2382389":"#First calculate the percentage of missing data for each feature:\nmissing_features = df.isnull().mean().sort_values(ascending=False)","68a59b22":"#Let's visualize the distribution of missing data percentages:\nplt.figure(figsize=(6,3), dpi=90)\nmissing_features.plot.hist(bins=20)\nplt.title('Histogram of Missing Features')\nplt.xlabel('Fraction of data missing')\nplt.ylabel('Feature count')","1d6332e4":"drop_list = sorted(list(missing_features[missing_features > 0.3].index)) #creating list to store the column names with over 30% missing values\nprint(drop_list)","5647d5b2":"#number of features to be dropped?\nlen(drop_list)","7ef55c56":"#Dropping these features\ndf.drop(drop_list, axis=1, inplace=True)","cdcefdd5":"print('Now we are left with {} columns'.format(df.shape[1]))\nprint('The remaining columns are as follows:')\nprint(df.columns)","efe68251":"df.head(1)","f7a8bb9f":"print(df['pymnt_plan'].unique())\nprint(df['pymnt_plan'].value_counts())","3283ac5a":"#As there are mostly 'no' values and very few number of 'yes' values. Dropping it. \ndrop_list7 = ['pymnt_plan']\ndf = df.drop(drop_list7,axis=1)","1175458e":"cor = df.corr() #Checking corelation between features\nplt.subplots(figsize=(20,15)) #giving figure size parameters\nsns.heatmap(cor, square = True) #plotting heatmap to check corelation","177fa4ff":"# calcualte mean fico score\ndf['fico_avg'] = (df['fico_range_high'] + df['fico_range_low'])\/2\n# calcualte mean last_fico score\ndf['last_fico_avf'] = (df['last_fico_range_high'] + df['last_fico_range_low'])\/2\n#Dropping the columns that are now transformed to new columns\ndrop_list8 = ['fico_range_high','last_fico_range_high','last_fico_range_low','last_fico_range_high']\ndf = df.drop(drop_list8,axis=1)","29b3e352":"#Opps. I missed one column. \ndf = df.drop(['fico_range_low'],axis=1)","a4d07c18":"df.head(1)","33946b78":"df['open_acc'].head()","d5adbce7":"DROP_LIST = ['funded_amnt', 'funded_amnt_inv', 'pymnt_plan', 'delinq_2yrs', 'inq_last_6mths', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', 'last_credit_pull_d', 'last_fico_range_high', 'last_fico_range_low', 'collections_12_mths_ex_med', 'policy_code', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim', 'acc_open_past_24mths', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mths_since_recent_bc', 'mths_since_recent_inq', 'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'tax_liens', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit', 'hardship_flag', 'disbursement_method', 'debt_settlement_flag']","be74ac5c":"drop_list9 = ['out_prncp','out_prncp_inv','total_pymnt','total_pymnt_inv','total_rec_prncp','total_rec_int',\n        'total_rec_late_fee','recoveries','collection_recovery_fee','last_pymnt_d',\n        'last_credit_pull_d']\ndf = df.drop(drop_list9,axis=1)","de02486b":"drop_list10 = ['inq_last_6mths']\ndf = df.drop(drop_list10,axis=1)","3f68babb":"df.head(1)","992bf036":"print(df['initial_list_status'].head(3))\nprint(df['initial_list_status'].unique())\nprint(df['initial_list_status'].count())","3c23fd79":"df['initial_list_status'].value_counts().plot.bar()\nplt.show()","4578a9c8":"df['policy_code'].value_counts().plot.bar()\nplt.show()","23b0ebe5":"print(df.policy_code.value_counts())\nprint(df.initial_list_status.value_counts())","a070f99f":"drop_list11 =['policy_code']\ndf = df.drop(drop_list11,axis=1)","15ff6e1f":"df['collections_12_mths_ex_med'].value_counts()","aad44ffd":"df = df.drop(['collections_12_mths_ex_med'],axis=1)","459ac01b":"df['application_type'].value_counts()","b99eb510":"df.groupby('application_type')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off']","191bd8e5":"df['acc_now_delinq'].value_counts()","45284f4f":"df['acc_now_delinq'].isna().sum()","33281001":"#Not many missing values and majority of the class contains 0. So, imputing with 0.\ndf['acc_now_delinq'] = df['acc_now_delinq'].fillna(0)","a018700e":"df.head(1)","01b348af":"#drop_list12 = ['open_acc_6m','open_il_12m','open_il_24m','open_act_il']\n#df = df.drop(drop_list12,axis=1)","df89371c":"print(df['tot_coll_amt'].value_counts())\nprint(df['tot_coll_amt'].describe())\nprint(df['tot_cur_bal'].value_counts())\nprint(df['tot_cur_bal'].describe())","3cc36572":"#As majority of the class contains 0. So, imputing with 0.\ndf['tot_coll_amt'] = df['tot_coll_amt'].fillna(0)\ndf['tot_cur_bal'] = df['tot_cur_bal'].fillna(0)","cd4a88a1":"df.head(1)","68e6c57f":"def check_stats(col):\n  print(df[col].head())\n  print(df[col].describe())\n  print(df[col].value_counts())","33abfadc":"check_stats('total_rev_hi_lim')","d5677791":"#From the dictionary, it is clear that this will not be avaiable at initial state\ndf = df.drop(['total_rev_hi_lim'],axis=1)","4d602ca8":"drop_list13 = ['acc_open_past_24mths','avg_cur_bal','bc_open_to_buy','bc_util','mo_sin_old_il_acct','mo_sin_old_rev_tl_op','mo_sin_rcnt_rev_tl_op','mo_sin_rcnt_tl']\ndf = df.drop(drop_list13,axis=1)","58dc118f":"check_stats('chargeoff_within_12_mths')","7e9bf612":"#As dictionary suggests, this attribute will not be available for investors initially. \n#Also, 'chargeoff_within_12_mths' have almost all values as 0 and will therefore not be very useful for modeling. Removing it.\ndrop_list14 = ['chargeoff_within_12_mths']\ndf = df.drop(drop_list14,axis=1)","9969fb9a":"df.head(1)","302cf726":"new_list_to_check = ['mort_acc',\t'mths_since_recent_inq',\t'num_accts_ever_120_pd',\t'num_actv_bc_tl',\t'num_actv_rev_tl',\t'num_bc_sats',\t'num_bc_tl',\t'num_il_tl',\t'num_op_rev_tl',\t'num_rev_accts',\t'num_rev_tl_bal_gt_0',\t'num_sats',\t'num_tl_120dpd_2m',\t'num_tl_30dpd',\t'num_tl_90g_dpd_24m',\t'num_tl_op_past_12m']\nfor col in new_list_to_check:\n  print(check_stats(col))","e09c6232":"df[\"mort_acc\"] = df[\"mort_acc\"].fillna(0) #Imputing with zero as most number of people do not have a mortgage account\n#Too many missing values, mostyly biased to one value and not that relevant of a feature. \ndrop_list15 = ['mths_since_recent_bc','mths_since_recent_inq','num_accts_ever_120_pd','num_actv_bc_tl','num_actv_rev_tl','num_bc_sats','num_bc_tl','num_il_tl','num_op_rev_tl','num_rev_accts','num_rev_tl_bal_gt_0','num_sats','num_tl_120dpd_2m','num_tl_30dpd','num_tl_90g_dpd_24m','num_tl_op_past_12m']\ndf = df.drop(drop_list15,axis=1)","e61e108f":"df.head(1)","6ad7d8ee":"list_next_10_till_debt_settlement = ['pct_tl_nvr_dlq','percent_bc_gt_75',\t'pub_rec_bankruptcies',\t'tax_liens',\t'tot_hi_cred_lim',\t'total_bal_ex_mort',\t'total_bc_limit',\t'total_il_high_credit_limit','hardship_flag',\t'debt_settlement_flag']","0f29c2f0":"for col in list_next_10_till_debt_settlement:\n  print(check_stats(col))","b3081ad5":"# majority of the trades never delinquent (can been seen from mean). Therefore, removing pct_tl_nvr_dlq.\n# Most of the values are missing and not that relevant of a feature. Therefore, removing them.\ndrop_list16 = ['pct_tl_nvr_dlq','percent_bc_gt_75','tot_hi_cred_lim','total_bal_ex_mort','total_bc_limit','total_il_high_credit_limit','hardship_flag','debt_settlement_flag']\ndf.drop(drop_list16, axis=1, inplace=True) ","f520fe4c":"# The column labeled 'tax_liens' have almost all values as 0 and will therefore not be very useful for modeling. Removing it.\ndf = df.drop(['tax_liens'],axis=1)","8156f7df":"df.head(1)","2671dafd":"df.info()","78006a5c":"#import seaborn as sns\n#from matplotlib import pyplot as plt\ncor = df.corr() \nplt.subplots(figsize=(20,15))\nsns.heatmap(cor, square = True)","0f11980c":"# Missing values statistics\nmiss_values = null_values(df)\nmiss_values.head(20)","a8b5ae2b":"# Copy Dataframe\ncomplete_df = df.copy()","e81644af":"#What are the value counts for this variable?\ncomplete_df['loan_status'].value_counts()","b80274be":"type(complete_df['loan_status'][0])","40affe35":"#Dependent Variable =  Loan_Status \n\n#Charged Off = 1\n#Default = 1\n#Late (31-120 days) = 1\n#Does not meet credit policy. Status Charged Off = 1\n\n#Current = 0\n#Fully Paid = 0\n#In Grace Period = 0\n#Late (16-30 days) = 0\n#Does not meet credit policy. Status Fully Paid = 0","93e464d2":"complete_df['loan_status'] = complete_df['loan_status'].replace(\"Default\", \"Charged Off\") #renaming the Charged off rows to Charged Off","44845e73":"complete_df['loan_status'] = complete_df['loan_status'].replace(\"Late (31-120 days)\", \"Charged Off\") #renaming the \"Late (31-120 days)\" rows to Charged Off","a8b8c3bf":"complete_df['loan_status'] = complete_df['loan_status'].replace(\"Does not meet credit policy. Status Charged Off\", \"Charged Off\") \n#renaming the \"Does not meet credit policy. Status Charged Off\" rows to Charged Off","c734b807":"complete_df['loan_status'].value_counts(dropna=False)","075b0353":"complete_df['loan_status'] = ['Charged Off' if i=='Charged Off' else 'Fully Paid' for i in complete_df['loan_status']]","755ac28c":"complete_df['loan_status'].value_counts(dropna=False)","3d8279f9":"complete_df['loan_status'].value_counts(normalize=True, dropna=False)","9bcd7d57":"fig, axs = plt.subplots(1,2,figsize=(14,7))\nsns.countplot(x='loan_status',data=complete_df,ax=axs[0])\naxs[0].set_title(\"Frequency of each Loan Status\")\ncomplete_df.loan_status.value_counts().plot(x=None,y=None, kind='pie', ax=axs[1],autopct='%1.2f%%')\naxs[1].set_title(\"Percentage of each Loan status\")\nplt.show()","28c2d822":"#fig, axs = plt.subplots(1,2,figsize=(14,7))\n#sns.countplot(x='TARGET',data=complete_df,ax=axs[0])\n#axs[0].set_title(\"Frequency of each Loan Status\")\n#complete_df.TARGET.value_counts().plot(x=None,y=None, kind='pie', ax=axs[1],autopct='%1.2f%%')\n#axs[1].set_title(\"Percentage of each Loan status\")\n#plt.show()","eba56b72":"#removing the loan_status predicor as we have recoded the values from it to a new column (\"Target\") which is out depenednt variable\n#complete_df = complete_df.drop('loan_status',axis=1,inplace=True) \n#complete_df = complete_df.drop(['loan_status'],axis=1)","fa259c04":"complete_df.head(1)","9fddfadc":"#Print the remaining predictos for future reference:\nprint(list(complete_df.columns))","591ac640":"complete_df['loan_amnt'].describe()","5826fb7c":"complete_df.groupby('loan_status')['loan_amnt'].describe()","1c40339d":"complete_df['term'].value_counts(dropna=False)","daf6fa1e":"complete_df['term'] = complete_df['term'].apply(lambda x: np.int8(x.split()[0]))","d108141b":"complete_df['term'].value_counts(normalize=True)","7d7ca610":"#Compare the charge-off rate by loan period:\ncomplete_df.groupby('term')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off']","d3c0a3d3":"complete_df['installment'].describe()","4c2118ad":"complete_df.groupby('loan_status')['installment'].describe()","b9df8b61":"complete_df['emp_length'].head(3)","3308ff50":"complete_df['emp_length'].fillna(value=0,inplace=True) #filling the missing values with 0 \ncomplete_df['emp_length'].replace(to_replace='[^0-9]+', value='', inplace=True, regex=True ) #checking not numeric value and then replacng it with '' to removing string\ncomplete_df['emp_length'].value_counts().sort_values().plot(kind='barh',figsize=(18,8)) #plotting the bar to see the emp_length\nplt.title('Number of loans distributed by Employment Years',fontsize=20) #plotting the title\nplt.xlabel('Number of loans',fontsize=15) #plotting number of loans for x axis\nplt.ylabel('Years worked',fontsize=15); #plotting years worked for y axis","f7532826":"complete_df['home_ownership'].value_counts(dropna=False)","f75abfb8":"#any and none are not very relevant for modeling. Therefore, clubbing them with Other. \ncomplete_df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER', inplace=True)","8f9aa3c4":"complete_df['home_ownership'].value_counts(dropna=False)","55e9d149":"complete_df.groupby('home_ownership')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off']","598c3b84":"complete_df['annual_inc'].describe()","30bbbb04":"complete_df['log_annual_inc'] = complete_df['annual_inc'].apply(lambda x: np.log10(x+1))\ncomplete_df.drop('annual_inc', axis=1, inplace=True)","fa347406":"complete_df['log_annual_inc'].describe()","32ac0d0a":"complete_df.groupby('loan_status')['log_annual_inc'].describe()","0a2165be":"complete_df['verification_status'].value_counts()","35130b7c":"complete_df['addr_state'].unique() #seeing the unique values of address state column","7318bd21":"# Make a list with each of the regions by state.\n\nwest = ['CA', 'OR', 'UT','WA', 'CO', 'NV', 'AK', 'MT', 'HI', 'WY', 'ID'] #all western states\nsouth_west = ['AZ', 'TX', 'NM', 'OK'] #all south western states\nsouth_east = ['GA', 'NC', 'VA', 'FL', 'KY', 'SC', 'LA', 'AL', 'WV', 'DC', 'AR', 'DE', 'MS', 'TN' ] #all south eastern states\nmid_west = ['IL', 'MO', 'MN', 'OH', 'WI', 'KS', 'MI', 'SD', 'IA', 'NE', 'IN', 'ND'] #all mid western states\nnorth_east = ['CT', 'NY', 'PA', 'NJ', 'RI','MA', 'MD', 'VT', 'NH', 'ME'] #all north eastern states","9169fd14":"complete_df['region'] = np.nan #creating new column region with all nan values\ndef finding_regions(state): #creating fuction to recode states into region\n    if state in west:\n        return 'West'\n    elif state in south_west:\n        return 'SouthWest'\n    elif state in south_east:\n        return 'SouthEast'\n    elif state in mid_west:\n        return 'MidWest'\n    elif state in north_east:\n        return 'NorthEast'\n    \ncomplete_df['region'] = complete_df['addr_state'].apply(finding_regions) #apply function to the new column","3837a223":"#Calculate the charge-off rates by region:\ncomplete_df.groupby('region')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off'].sort_values()","f2cb2d6a":"#Calculate the charge-off rates by state:\ncomplete_df.groupby('addr_state')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off'].sort_values()","e2abbc8c":"complete_df['dti'].describe()","b68b07ae":"plt.figure(figsize=(8,3), dpi=90)\nsns.distplot(complete_df.loc[complete_df['dti'].notnull() & (complete_df['dti']<60), 'dti'], kde=False)\nplt.xlabel('Debt-to-income Ratio')\nplt.ylabel('Count')\nplt.title('Debt-to-income Ratio')","8012b7b0":"complete_df.groupby('loan_status')['dti'].describe()","c6a2f6f8":"complete_df.groupby('loan_status')['delinq_2yrs'].describe()","4cb22662":"complete_df = complete_df.drop(['delinq_2yrs'],axis=1)","88c6d32c":"complete_df.columns","ad0aaa84":"plt.figure(figsize=(10,3), dpi=90)\nsns.countplot(complete_df['open_acc'], order=sorted(complete_df['open_acc'].unique()), color='#5975A4', saturation=1)\n_, _ = plt.xticks(np.arange(0, 90, 5), np.arange(0, 90, 5))\nplt.title('Number of Open Credit Lines')","2ecbb5a8":"#let's see the difference in number of credit lines between fully paid loans and charged-off loans\ncomplete_df.groupby('loan_status')['open_acc'].describe()","6810b6a7":"complete_df['pub_rec'].value_counts().sort_index()","fefb8e90":"complete_df.groupby('loan_status')['pub_rec'].describe()","08c6a82b":"complete_df['revol_bal'].describe()","b087b79d":"#doing log transform\ncomplete_df['revol_bal'] = complete_df['revol_bal'].apply(lambda x: np.log10(x+1))","3de77b06":"complete_df.groupby('loan_status')['revol_bal'].describe()","d46b13d9":"complete_df['revol_util'].head()","6bac9f22":"#We see that term, emp_length, revol_util columns contains numeric values, but is formatted as object. \ncomplete_df['revol_util'] = complete_df['revol_util'].str.rstrip('%').astype('float') #stripping the % symbol and converting the type to \"Float\"","c23fbb12":"complete_df['revol_util'].describe()","6ece028e":"complete_df.groupby('loan_status')['revol_util'].describe()","a6a57d74":"plt.figure(figsize=(12,3), dpi=90)\nsns.countplot(complete_df['total_acc'], order=sorted(complete_df['total_acc'].unique()), color='#5975A4', saturation=1)\n_, _ = plt.xticks(np.arange(0, 176, 10), np.arange(0, 176, 10))\nplt.title('Total Number of Credit Lines')","2923156c":"complete_df.groupby('loan_status')['total_acc'].describe()","d23f7a1b":"#not really sure what W, F means here. ","affdf8fe":"complete_df.columns","6ffe115a":"complete_df['application_type'].value_counts()","48d360f4":"complete_df.groupby('application_type')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off']","a4f8adc9":"complete_df['acc_now_delinq'].head()","aad6f396":"complete_df['acc_now_delinq'].value_counts()","5451203c":"complete_df.groupby('acc_now_delinq')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off']","228c69fc":"complete_df = complete_df.drop(['application_type'],axis=1)","8ada9a7d":"complete_df = complete_df.drop(['acc_now_delinq'],axis=1)","2e4ba832":"complete_df['tot_coll_amt'].head()","a588a5bb":"complete_df['tot_coll_amt'].value_counts()","6d79e97c":"complete_df['tot_cur_bal'].value_counts()","a8bdb5bf":"complete_df['mort_acc'].describe()","d348ffd6":"complete_df['mort_acc'].value_counts().head(10)","9298a4c3":"#comparing statistics by target variable:\ncomplete_df.groupby('loan_status')['mort_acc'].describe()","f4b9449b":"complete_df['pub_rec_bankruptcies'].value_counts().sort_index()","89880438":"#comparing statistics by target variable:\ncomplete_df.groupby('loan_status')['pub_rec_bankruptcies'].describe()","1f9776fc":"complete_df['purpose_n'].unique()","ac9b1d5e":"complete_df['earliest_cr_line_n'].head()","a204cd15":"complete_df['earliest_cr_line_n'].describe()","9ef46861":"#comparing statistics by target variable:\ncomplete_df.groupby('loan_status')['earliest_cr_line_n'].describe()","0175f8b6":"complete_df.groupby('loan_status')['fico_avg'].describe()","746a861a":"complete_df = complete_df.drop(['last_fico_avf'],axis=1)","e14f9fbf":"complete_df['log_annual_inc'].describe()","3964d3b5":"complete_df.groupby('loan_status')['log_annual_inc'].describe()","ba8c069d":"complete_df = complete_df.drop(['region'],axis=1)","9000815a":"complete_df.columns","f667e7a2":"obj_cols = complete_df.columns[complete_df.dtypes==object]\n\n#Imputer function\nimputer = lambda x:x.fillna(x.value_counts().index[0]) \n\n#Impute dtype=object with most frequent value\ncomplete_df[obj_cols] = complete_df[obj_cols].apply(imputer) \n\n#Impute the rest of df with median\ncomplete_df = complete_df.fillna(df.median(axis=0)) ","f94323cb":"missing_fractions = complete_df.isnull().mean().sort_values(ascending=False) # Fraction of data missing for each variable\nprint(missing_fractions[missing_fractions > 0]) # Print variables that are missing data","097d824c":"#print(complete_df.isnull().sum())\ncomplete_df.fillna(complete_df.median(), inplace=True)\n","66f25751":"complete_df.head(1)","e2ef191a":"complete_df.to_csv(\"\/content\/drive\/My Drive\/Lending_Club\/clean_df_23_col.csv\",index=False)","0c72db2f":"#complete_df.to_csv('clean_df_23_col_.csv') #save csv to my drive by the name lending_club_cleaned1\n#!cp lending_club_cleaned1.csv \"\/content\/drive\/My Drive\/Lending_Club\/\"","c0984504":"#import pickle\n#complete_df.to_pickle(\"clean_final_23_col.pkl\")\n#df_filtered = pd.read_pickle(\"\/content\/drive\/My Drive\/lending_loan_df.pkl\")","c9beb5a5":"#complete_df = pd.read_csv(\"\/content\/drive\/My Drive\/lending_club_cleaned1.csv\")","df5211f8":"corr = complete_df.corr()['TARGET'].sort_values()\n# Display correlations\nprint('Most Positive Correlations:\\n', corr.tail(10))\nprint('\\nMost Negative Correlations:\\n', corr.head(10))","92591a5c":"complete_df = complete_df.drop(['addr_state'],axis=1)","e77d41cf":"target_list = [1 if i=='Charged Off' else 0 for i in complete_df['loan_status']]\ncomplete_df['charged_off'] = target_list\ncomplete_df['charged_off'].value_counts()\ncomplete_df.drop('loan_status', axis=1, inplace=True)","9e9bc7bb":"#how many variable we have now\nprint(complete_df.shape)\ncomplete_df.head(1)","abb874ed":"null_counts = complete_df.isnull().sum().sort_index()\nprint(\"Number of null values in each column:\\n{}\".format(null_counts))","bb659571":"print(\"Data types and their frequency\\n{}\".format(complete_df.dtypes.value_counts()))","656c0c65":"object_columns_df = complete_df.select_dtypes(include=['object'])\nprint(object_columns_df.iloc[0])","5ccfb31a":"cols = ['emp_length','home_ownership','verification_status', 'purpose_n', 'initial_list_status']\nfor name in cols:\n    print(name,':')\n    print(object_columns_df[name].value_counts(),'\\n')","af1bf5ba":"#converting the type of emp_length column to int64 from string\ncomplete_df['emp_length'] = complete_df['emp_length'].astype('int64') ","5c3e49f4":"object_columns_df = complete_df.select_dtypes(include=['object'])\nprint(object_columns_df.iloc[0])","cc1c4aed":"#Converting nominal features into numerical features requires encoding them as dummy variables.\nnominal_columns = [\"home_ownership\", \"verification_status\", \"purpose_n\", \"initial_list_status\"]\ndummy_df = pd.get_dummies(complete_df[nominal_columns], drop_first=True) #greating dummies for the above nominal columns and removing first dummy variable to \n#drop the first one to avoid linear dependency between the resulted features since some algorithms may struggle with this issue.\ncomplete_df = pd.concat([complete_df, dummy_df], axis=1) #merging the newly created dummy columns with the working dataset\ncomplete_df = complete_df.drop(nominal_columns, axis=1) #dropping the original nominal columns as they are not required anymore\n\n#df = pd.get_dummies(df, columns=[\"purpose\"], drop_first=True)","4dc7a071":"complete_df.info()","bd9c304f":"#let's see how many predictors are there now\ncomplete_df.shape","6ef18d5a":"#let's see the dataframe now with dummy variables:\ncomplete_df.head()","2ef99602":"#complete_df['region'].unique()","7624387f":"#complete_df['region'].value_counts()","aa9365e2":"#Converting region into dummy variable too.\n#nominal_columns2 = [\"region\"]\n#dummy_df2 = pd.get_dummies(complete_df[nominal_columns2], drop_first=True) #greating dummies for the above nominal columns and removing first dummy variable to \n#avoid linear dependency between the resulted features since some algorithms may struggle with this issue.\n#complete_df = pd.concat([complete_df, dummy_df2], axis=1) #merging the newly created dummy columns with the working dataset\n#complete_df = complete_df.drop(nominal_columns2, axis=1) #dropping the original nominal columns as they are not required anymore\n\n#df = pd.get_dummies(df, columns=[\"purpose\"], drop_first=True)","b43e146d":"complete_df.to_pickle(\"\/content\/drive\/My Drive\/Lending_Club\/loan_clean_final.pkl\")","3c49169f":"DATA_PATH = \"\/content\/drive\/My Drive\/Capstone Project - NLP\"\ninfile = open(DATA_PATH+'\/lending_loan_df_clean_complete.pkl','rb')\ncomplete_df = pickle.load(infile)","561cf4f1":"#complete_df['desc2'] = complete_df['desc1'].str.len() #creating a column with length of the cleaned description","ec9d1aac":"print(f\"{complete_df.dtypes}\\n\")","cfb7a13b":"var_cor = pd.DataFrame((complete_df.corr()['TARGET'])).reset_index()\nvar_cor.columns=['index', 'correlation']\nvar_cor","424c4b28":"# Copy Dataframe\nnew_df = complete_df.copy()\nnew_df.head(1)","0c5349e4":"new_df = new_df.drop(['Unnamed: 0'],axis=1)","84ec4da3":"import matplotlib.pyplot as plt\nimport seaborn as sb\ndf2 = new_df\nplt.figure()\nax = sb.countplot(x=new_df[\"TARGET\"], y = None, palette = \"Reds\")\nfor p in ax.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax.annotate('{:.0f}'.format(y), (x.mean(), y), ha='center', va='bottom') \nplt.title('Loan Status (Target Variable)')\nplt.show()","06c994bf":"from sklearn.utils import shuffle, resample\n\n# Let's seperate our data into two based on the Score (True or False). \ndf_zero = df2[df2[\"TARGET\"] == 0]\ndf_one = df2[df2[\"TARGET\"] == 1]\n\nprint(\"Number of records before upsampling: \")\nprint(\"One:\", len(df_one), \"Zero:\", len(df_zero))\n\n# Let's use the resample function for upsampling.\ndf_one = resample(df_one, replace=True, n_samples=len(df_zero))\n\n# Let's put the separated data frames together. \ndf2 = pd.concat([df_zero, df_one], axis=0)\n\n# Let's shuffle the data\ndf2 = shuffle(df2)\n\nprint(\"Number of records after upsampling: \")\nprint(\"One:\", len(df2[df2[\"TARGET\"] == 0]), \"Zero:\", len(df2[df2[\"TARGET\"] == 1]))","4030fac7":"import matplotlib.pyplot as plt\nimport seaborn as sb\n\nplt.figure()\nax = sb.countplot(x=df2[\"TARGET\"], y = None, palette = \"Reds\")\nfor p in ax.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax.annotate('{:.0f}'.format(y), (x.mean(), y), ha='center', va='bottom') \nplt.title('Loan Status (Target Variable) after upsampling the 1 class')\nplt.show()","9aafe38f":"#df2 = df2.drop(['desc'],axis=1)","c36dc7a0":"df2.to_pickle(\"\/content\/drive\/My Drive\/Lending_Club\/loan_ready_for_ml.pkl\")","05f1ffe1":"#Reading pickled df\nimport pickle\n#drive.mount('\/content\/drive')\nDATA_PATH = \"\/content\/drive\/My Drive\/Capstone Project - NLP\"\ninfile = open(DATA_PATH+'\/loan_dfready_for_model.pkl','rb')\ndf2 = picklenull_counts = df2.isnull().sum().sort_index()\nprint(\"Number of null values in each column:\\n{}\".format(null_counts)).load(infile)","70cf46a9":"null_counts = df2.isnull().sum().sort_index()\nprint(\"Number of null values in each column:\\n{}\".format(null_counts))","7be48d18":"df2['desc'].head()","a73cb5c8":"#Impute the rest of df with median\n#df2 = df2.fillna(df2.median(axis=0)) ","618feb1c":"snow = SnowballStemmer('english') #Initializing snowball stemmer from NLTK library","d37ed269":"stop_words = stopwords.words(\"english\")\ndef process_text(texts): \n    final_text_list=[]\n    for sent in texts:\n        filtered_sentence=[]\n        \n        sent = sent.lower() # Lowercase \n        sent = sent.strip() # Remove leading\/trailing whitespace\n        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\n        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags\/markups:\n        \n        for w in word_tokenize(sent):\n            # We are applying some custom filtering here.\n            # Check if it is not numeric and its length>2 and not in stop words\n            if(not w.isnumeric()) and (len(w)>2) and (w not in stop_words):  \n                # Stem and add to filtered list\n                filtered_sentence.append(snow.stem(w))\n        final_string = \" \".join(filtered_sentence) #final string of cleaned words\n \n        final_text_list.append(final_string)\n    \n    return final_text_list","b052aa03":"print(\"Pre-processing the training text field\")\ndf2[\"desc\"] = process_text(df2[\"desc\"].tolist()) #applying above function on desc column","d17f00ba":"df2.columns","35be0b9c":"X_train, X_val, y_train, y_val = train_test_split(df2.drop([\"TARGET\"], axis=1), # Input\n                                                  df2['TARGET'].tolist(), # Target field\n                                                  test_size=0.2, # 20% val, 80% tranining\n                                                  shuffle=True) # Shuffle the whole dataset","f50efdf2":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n \n# Initialize the binary count vectorizer\ntfidf_vectorizer = CountVectorizer(binary=True,\n                                   max_features=50    # Limit the vocabulary size\n                                  )\n# Fit and transform\nX_train_text_vectors = tfidf_vectorizer.fit_transform(X_train[\"desc\"].tolist())\n# Only transform\nX_val_text_vectors = tfidf_vectorizer.transform(X_val[\"desc\"].tolist())","f11c001c":"print(tfidf_vectorizer.vocabulary_)","3fb7bfd3":"X_train = X_train.drop(['desc'],axis=1) #dropping desc from X_train as we have processed it in X_train_text_vectors\nX_val = X_val.drop(['desc'],axis=1) #dropping desc from X_val as well as we have processed it in X_val_text_vectors","ca66b30a":"# Let' merge our features\nX_train_features = np.column_stack((X_train_text_vectors.toarray(), \n                                    X_train)\n                                  )\n# Let' merge our features\nX_val_features = np.column_stack((X_val_text_vectors.toarray(), \n                                    X_val)\n                                  )","87bab4e7":"#importing necessary libraries for logistic regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import make_scorer, accuracy_score, f1_score\n\nlrClassifier = LogisticRegression()\nlrClassifier.fit(X_train_features, y_train)\npredicted = lrClassifier.predict(X_val_features)\nprint(\"LogisticRegression on Validation: Accuracy Score: %f, F1-score: %f\" % \n      (accuracy_score(y_val, predicted), f1_score(y_val, predicted)))","f6f9a825":"# generate evaluation metrics\nprobs = lrClassifier.predict_proba(X_val_features)\npredicted = lrClassifier.predict(X_val_features)\naccuracy = accuracy_score(y_val, predicted)\nauc = metrics.roc_auc_score(y_val, probs[:, 1])\n\nprint(f'AUC estimate: {auc:.3}')\nprint(f'Mean accuracy score: {accuracy:.3}')\nprint(metrics.confusion_matrix(y_val, predicted))\nprint(metrics.classification_report(y_val, predicted))","8be19520":"lrClassifier = LogisticRegression(penalty = 'l2', #calling the object with l2 NORM penalty and alpha value of 0.1\n                                  C = 0.1)\nlrClassifier.fit(X_train_features, y_train) #fitting the logistic regression on the training part of the data\npredicted = lrClassifier.predict(X_val_features) #predicting on validation set\n\nprint(\"LogisticRegression on Validation: Accuracy Score: %f, F1-score: %f\" %  \n      (accuracy_score(y_val, predicted), f1_score(y_val, predicted)))#printing the accuracy and f1 score ","3eece254":"# generate evaluation metrics\nprobs = lrClassifier.predict_proba(X_val_features)\npredicted = lrClassifier.predict(X_val_features)\naccuracy = accuracy_score(y_val, predicted)\nauc = metrics.roc_auc_score(y_val, probs[:, 1])\n\nprint(f'AUC estimate: {auc:.3}')\nprint(f'Mean accuracy score: {accuracy:.3}')\nprint(metrics.confusion_matrix(y_val, predicted))\nprint(metrics.classification_report(y_val, predicted))","04cbab3f":"lrClassifier.predict(X_val_features)[0:5] #checking first 5 predicted value via usual binary format","248f7cf4":"lrClassifier.predict_proba(X_val_features)[0:5] #checking first 5 predicted values via probablity format","c292e8e4":"%matplotlib inline \nimport numpy as np #importing numpy library\nimport matplotlib.pyplot as plt #importing matplotlib library for plotting the results\n\n# Calculate the accuracy using different values for the classification threshold, \n# and pick the threshold that resulted in the highest accuracy.\nhighest_accuracy = 0 #initializing highest_accuracy variable\nthreshold_highest_accuracy = 0 #initializing threshold_highest_accuracy variable\n\nthresholds = np.arange(0,1,0.01) #\nscores = []\nfor t in thresholds:\n    # set threshold to 't' instead of 0.5\n    y_val_other = (lrClassifier.predict_proba(X_val_features)[:,1] >= t).astype(float)\n    score = accuracy_score(y_val, y_val_other)\n    scores.append(score)\n    if(score > highest_accuracy):\n        highest_accuracy = score\n        threshold_highest_accuracy = t\nprint(\"Highest Accuracy on Validation:\", highest_accuracy, \\\n      \", Threshold for the highest Accuracy:\", threshold_highest_accuracy)   \n\n# Let's plot the accuracy versus different choices of thresholds\nplt.plot([0.5, 0.5], [np.min(scores), np.max(scores)], linestyle='--')\nplt.plot(thresholds, scores, marker='.')\nplt.title('Accuracy versus different choices of thresholds')\nplt.xlabel('Threshold')\nplt.ylabel('Accuracy')\nplt.show()","b7c159ee":"%matplotlib inline \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_curve\n\n# Calculate the precision and recall using different values for the classification threshold\nval_predictions_probs = lrClassifier.predict_proba(X_val_features)\nprecisions, recalls, thresholds = precision_recall_curve(y_val, val_predictions_probs[:, 1])","f24a84fd":"%matplotlib inline \nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Calculate the F1 score using different values for the classification threshold, \n# and pick the threshold that resulted in the highest F1 score.\nhighest_f1 = 0\nthreshold_highest_f1 = 0\n\nf1_scores = []\nfor id, threhold in enumerate(thresholds):\n    f1_score = 2*precisions[id]*recalls[id]\/(precisions[id]+recalls[id])\n    f1_scores.append(f1_score)\n    if(f1_score > highest_f1):\n        highest_f1 = f1_score\n        threshold_highest_f1 = threhold\nprint(\"Highest F1 score on Validation:\", highest_f1, \\\n      \", Threshold for the highest F1 score:\", threshold_highest_f1)\n\n# Let's plot the F1 score versus different choices of thresholds\nplt.plot([0.5, 0.5], [np.min(f1_scores), np.max(f1_scores)], linestyle='--')\nplt.plot(thresholds, f1_scores, marker='.')\nplt.title('F1 Score versus different choices of thresholds')\nplt.xlabel('Threshold')\nplt.ylabel('F1 Score')\nplt.show()","48a3c81e":"df2.select_dtypes([np.number]).info()","b73272a5":"#def min_max(col): #creating a function to apply min max scaling on a column\n # df2[col] = (df2[col] - df2[col].min())\/(df2[col].max()-df2[col].min())\n  \n#col_to_scale = df2.columns #creating a list of columns in our working dataframe\n#for col in col_to_scale: #running for loop to apply min_max function on all the columns mentioned in the above list\n # min_max(col)","f8aeb1a9":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_features = sc.fit_transform(X_train_features)\nX_val_features = sc.transform(X_val_features)","f7cccfff":"#X_train, X_val, y_train, y_val = train_test_split(df2.drop(\"charged_off\", axis=1), # Input\n#                                                  df2['charged_off'].tolist(), # Target field\n##                                                  test_size=0.2, # 20% val, 80% tranining\n #                                                 shuffle=True) # Shuffle the whole dataset","28960525":"%%time\nlr = LogisticRegression()\nlr.fit(X_train_features, y_train)","85ff0c64":"# generate evaluation metrics\n\nprobs = lr.predict_proba(X_val_features)\npredicted = lr.predict(X_val_features)\naccuracy = accuracy_score(y_val, predicted)\nauc = metrics.roc_auc_score(y_val, probs[:, 1])\n\nprint(f'AUC estimate: {auc:.3}')\nprint(f'Mean accuracy score: {accuracy:.3}')\nprint(metrics.confusion_matrix(y_val, predicted))\nprint(metrics.classification_report(y_val, predicted))","3dad8e63":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_val, lr.predict_proba(X_val_features)[:,1])\nplt.figure(figsize=(6,6))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","530606bf":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import GridSearchCV\n\n#The machine learning pipeline:\npipeline_sgdlogreg = Pipeline([\n    #('imputer', Imputer(copy=False)), # Mean imputation by default\n    ('scaler', StandardScaler(copy=False)),\n    ('model', SGDClassifier(loss='log', max_iter=1000, tol=1e-3, random_state=1, warm_start=True))\n])\n#A small grid of hyperparameters to search over:\n\nparam_grid_sgdlogreg = {\n    'model__alpha': [10**-5, 10**-2, 10**1],\n    'model__penalty': ['l1', 'l2']\n}\n#Create the search grid object:\ngrid_sgdlogreg = GridSearchCV(estimator=pipeline_sgdlogreg, param_grid=param_grid_sgdlogreg, scoring='roc_auc', n_jobs=-1, pre_dispatch='2*n_jobs', cv=5, verbose=1, return_train_score=False)\n","9bc995ca":"#Conduct the grid search and train the final model on the whole dataset:\n\ngrid_sgdlogreg.fit(X_train_features, y_train)","0b36b07d":"#Mean cross-validated AUROC score of the best model:\n\ngrid_sgdlogreg.best_score_","72bb6750":"#Best hyperparameters:\n\ngrid_sgdlogreg.best_params_","5c4ccc4c":"# ROC curve\nfpr, tpr, thresholds = roc_curve(y_val, grid_sgdlogreg.predict_proba(X_val_features)[:,1])\nplt.figure(figsize=(6,6))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","70b80f2b":"pipeline_knn = Pipeline([\n    ('scaler', StandardScaler(copy=False)),\n    ('lda', LinearDiscriminantAnalysis()),\n    ('model', KNeighborsClassifier(n_jobs=-1))\n])","4f18ee19":"param_grid_knn = {\n    'lda__n_components': [3, 9], # Number of LDA components to keep\n    'model__n_neighbors': [5, 25, 125] # The 'k' in k-nearest neighbors\n}","491c527e":"grid_knn = GridSearchCV(estimator=pipeline_knn, param_grid=param_grid_knn, scoring='roc_auc', n_jobs=-1, pre_dispatch='2*n_jobs', cv=5, verbose=1, return_train_score=False)","8c3e7b53":"grid_knn.fit(X_train_features, y_train)","da0206e5":"#Mean cross-validated AUROC score of the best model:\ngrid_knn.best_score_","e2899c1f":"#Best hyperparameters:\ngrid_knn.best_params_","d8750c4d":"#K_values = [3, 5, 10, 20, 30]\n \n#for K in K_values:\n#    knnClassifier = KNeighborsClassifier(n_neighbors=K)\n#    knnClassifier.fit(X_train, y_train)\n#    val_predictions = knnClassifier.predict(X_val)\n#    print(\"F1 Score for K:\", K, \"is\", f1_score(y_val, val_predictions))","7f738258":"#K_values = [3, 5, 10, 20, 30]\n \n#for K in K_values:\n#knnClassifier = KNeighborsClassifier(n_neighbors=3)\n#knnClassifier.fit(X_train, y_train)\n\n#print(\"F1 Score for K=3 is\", f1_score(y_val, predicted))","3b76e0ed":"probs = grid_knn.predict_proba(X_val_features)\npredicted = grid_knn.predict(X_val_features)\n#print(\"knnClassifier on Validation: Accuracy Score: %f, F1-score: %f\" % \n      #(accuracy_score(y_val, predicted), f1_score(y_val, predicted)))\naccuracy = accuracy_score(y_val, predicted)\nauc = metrics.roc_auc_score(y_val, probs[:, 1])\n\n# generate evaluation metrics\nprint(f'AUC estimate: {auc:.3}')\nprint(f'Mean accuracy score: {accuracy:.3}')\nprint(metrics.confusion_matrix(y_val, predicted))\nprint(metrics.classification_report(y_val, predicted))","d256758c":"# ROC curve\n\nfpr, tpr, thresholds = roc_curve(y_val, grid_knn.predict_proba(X_val_features)[:,1])\nplt.figure(figsize=(6,6))\nplt.plot(fpr, tpr, label='KNN (area = %0.2f)' % auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","6b521f8c":"#model_save_name = 'knn_Classifier.pt'\n#path = F\"\/content\/gdrive\/My Drive\/{model_save_name}\" \n#torch.save(knnClassifier, path)","c59891bb":"%%time\nparam_grid={'max_depth': [5, 10, 20],\n            'min_samples_leaf': [5, 10, 15],\n            'min_samples_split': [2, 5, 15, 25] \n           }\n\ndt = DecisionTreeClassifier()\n\ngrid_search = GridSearchCV(dt, # Base model\n                           param_grid, # Parameters to try\n                           cv = 5, # Apply 5-fold cross validation\n                           verbose = 1, # Print summary\n                           n_jobs = -1 # Use all available processors \n                          )\n\ngrid_search.fit(X_train_features, y_train)","a643fb54":"grid_search.best_params_ #{'max_depth': 50, 'min_samples_leaf': 5, 'min_samples_split': 5}","5c45e523":"# Let's get the input and output data for testing the classifier\npredicted = grid_search.predict(X_val_features)","42a4c031":"print(metrics.confusion_matrix(y_val, predicted))","6f4f6b10":"print(metrics.classification_report(y_val, predicted))\nprint(\"Accuracy:\", accuracy_score(y_val, predicted))","3b030ab6":"grid_search.best_estimator_","4b096c62":"#model_save_name = 'Decision_tree.pt'\n#path = F\"\/content\/gdrive\/My Drive\/{model_save_name}\" \n#torch.save(dt, path)","38eb3279":"#Next we train a random forest model. Note that data standardization is not necessary for a random forest.\n\npipeline_rfc = Pipeline([\n    #('imputer', Imputer(copy=False)),\n    ('model', RandomForestClassifier(n_jobs=-1, random_state=1))\n])","d8ffb481":"#The random forest takes very long to train, so we don't test different hyperparameter choices. We'll still use GridSearchCV for the sake of consistency.\n\nparam_grid_rfc = {\n    'model__n_estimators': [50] # The number of randomized trees to build\n}","c4c24c39":"#The AUROC will always improve (with decreasing gains) as the number of estimators increases, but it's not necessarily worth the extra training time and model complexity.\n\ngrid_rfc = GridSearchCV(estimator=pipeline_rfc, param_grid=param_grid_rfc, scoring='roc_auc', n_jobs=-1, pre_dispatch='2*n_jobs', cv=5, verbose=1, return_train_score=False)","946a9f60":"grid_rfc.fit(X_train_features, y_train)","569e72b9":"#Mean cross-validated AUROC score of the random forest:\n\ngrid_rfc.best_score_","2312a8d2":"%%time\nrf = RandomForestClassifier(n_estimators=50, max_depth=3, max_features='log2', oob_score=True,  random_state=0)\nrf.fit(X_train_features, y_train)","119566b1":"# generate evaluation metrics\n\nprobs = rf.predict_proba(X_val_features)\npredicted = rf.predict(X_val_features)\naccuracy = accuracy_score(y_val, predicted)\nauc = metrics.roc_auc_score(y_val, probs[:, 1])\n\nprint(f'AUC estimate: {auc:.3}')\nprint(f'Out-of-bag score estimate: {rf.oob_score_:.3}')\nprint(f'Mean accuracy score: {accuracy:.3}')\nprint(metrics.confusion_matrix(y_val, predicted))\nprint(metrics.classification_report(y_val, predicted))","5563dbbd":"# ROC curve\n\nfpr, tpr, thresholds = roc_curve(y_val, rf.predict_proba(X_val_features)[:,1])\nplt.figure(figsize=(6,6))\nplt.plot(fpr, tpr, label='Random forest (area = %0.2f)' % auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","f6b319a5":"#model_save_name = 'Random_forest.pt'\n#path = F\"\/content\/gdrive\/My Drive\/{model_save_name}\" \n#torch.save(rf, path)","48f71cad":"%%time\n\nnn = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\nnn.fit(X_train_features, y_train)","08135146":"probs = nn.predict_proba(X_val_features)\npredicted = nn.predict(X_val_features)\naccuracy = accuracy_score(y_val, predicted)\nauc = metrics.roc_auc_score(y_val, probs[:, 1])\n\n# generate evaluation metrics\nprint(f'AUC estimate: {auc:.3}')\nprint(f'Mean accuracy score: {accuracy:.3}')\nprint(metrics.confusion_matrix(y_val, predicted))\nprint(metrics.classification_report(y_val, predicted))","70bab5dc":"# ROC curve\n\nfpr, tpr, thresholds = roc_curve(y_val, nn.predict_proba(X_val_features)[:,1])\nplt.figure(figsize=(6,6))\nplt.plot(fpr, tpr, label='Neural network (area = %0.2f)' % auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","4b29674a":"#model_save_name = 'Neural_Network.pt'\n#path = F\"\/content\/gdrive\/My Drive\/{model_save_name}\" \n#torch.save(nn, path)","68cc91e7":"%%time\ngnb = GaussianNB()\ngnb.fit(X_train_features, y_train)","722f7321":"probs = gnb.predict_proba(X_val_features)\npredicted = gnb.predict(X_val_features)\naccuracy = accuracy_score(y_val, predicted)\nauc = metrics.roc_auc_score(y_val, probs[:, 1])\n\n# generate evaluation metrics\nprint(f'AUC estimate: {auc:.3}')\nprint(f'Mean accuracy score: {accuracy:.3}')\nprint(metrics.confusion_matrix(y_val, predicted))\nprint(metrics.classification_report(y_val, predicted))","e24d9a2e":"# ROC curve\n\nfpr, tpr, thresholds = roc_curve(y_val, gnb.predict_proba(X_val_features)[:,1])\nplt.figure(figsize=(6,6))\nplt.plot(fpr, tpr, label='Naive Bayes (area = %0.2f)' % auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","5436074b":"#model_save_name = 'NaiyeBayes.pt'\n#path = F\"\/content\/gdrive\/My Drive\/{model_save_name}\" \n#torch.save(gnb, path)","a4a068f8":"%%time\nlda = LinearDiscriminantAnalysis()\nlda.fit(X_train_features, y_train)","6fc15755":"probs = lda.predict_proba(X_val_features)\npredicted = lda.predict(X_val_features)\naccuracy = accuracy_score(y_val, predicted)\nauc = metrics.roc_auc_score(y_val, probs[:, 1])\n\n# generate evaluation metrics\nprint(f'AUC estimate: {auc:.3}')\nprint(f'Mean accuracy score: {accuracy:.3}')\nprint(metrics.confusion_matrix(y_val, predicted))\nprint(metrics.classification_report(y_val, predicted))","a6c8eb4f":"# ROC curve\n\nfpr, tpr, thresholds = roc_curve(y_val, lda.predict_proba(X_val_features)[:,1])\nplt.figure(figsize=(6,6))\nplt.plot(fpr, tpr, label='Linear Discriminant Analysis (area = %0.2f)' % auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","e84b83d8":"#model_save_name = 'LDA.pt'\n#path = F\"\/content\/gdrive\/My Drive\/{model_save_name}\" \n#torch.save(lda, path)","f553cbe4":"%%time\ngradboost = GradientBoostingClassifier(n_estimators=200,max_depth=3)\ngradboost.fit(X_train_features, y_train)","25d4d4d3":"probs = gradboost.predict_proba(X_val_features)\npredicted = gradboost.predict(X_val_features)\naccuracy = accuracy_score(y_val, predicted)\nauc = metrics.roc_auc_score(y_val, probs[:, 1])\n\n# generate evaluation metrics\nprint(f'AUC estimate: {auc:.3}')\nprint(f'Mean accuracy score: {accuracy:.3}')\nprint(metrics.confusion_matrix(y_val, predicted))\nprint(metrics.classification_report(y_val, predicted))","c28fba9c":"# ROC curve\n\nfpr, tpr, thresholds = roc_curve(y_val, gradboost.predict_proba(X_val_features)[:,1])\nplt.figure(figsize=(6,6))\nplt.plot(fpr, tpr, label='Gradient boosting (area = %0.2f)' % auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","bd50d443":"gradboost.feature_importances_","b722bbb4":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier","14395c63":"clf=BaggingClassifier(oob_score=True,n_jobs=-1,n_estimators=20,random_state=400,\n                      base_estimator=DecisionTreeClassifier())","61cf864d":"clf.fit(X_train_features,y_train)","86a43187":"clf.oob_score_","ab4951e2":"clf.score(X_val_features,y_val)","ea0ba0df":"#for w in range(10,300,20):\n#    clf=BaggingClassifier(oob_score=True,n_jobs=-1,n_estimators=w,random_state=400,\n#                          base_estimator=DecisionTreeClassifier())\n#    clf.fit(X_train_features,y_train)\n##    oob=clf.oob_score_\n #   print('For n_estimators = '+str(w))\n#    print('OOB score is '+str(oob))\n#    print('************************')","a9f13960":"# Feature Importance\nclf.estimators_","b638d963":"print(clf.estimators_[0]) #first tree model","1c7c2c44":"print(clf.estimators_[0].feature_importances_)","9d4b4dcc":"# We can extract feature importance from each tree then take a mean for all trees\nimport numpy as np\nimp=[]\nfor i in clf.estimators_:\n    imp.append(i.feature_importances_)\nimp=np.mean(imp,axis=0)\nimp","1ed46283":"feature_importance=pd.Series(imp,index=X_train_features.tolist())","668169e3":"feature_importance.sort_values(ascending=False)","46a053c5":"feature_importance.sort_values(ascending=False).plot(kind='bar')","11731024":"#model_save_name = 'gradient_boosting.pt'\n#path = F\"\/content\/gdrive\/My Drive\/{model_save_name}\" \n#torch.save(gradboost, path)","da4d7113":"#!pip install mxnet","f5933f28":"#svm = SVC(kernel='linear', C=1E10)\n#svm.fit(X_train, y_train)","c13f3b54":"#probs = svm.predict_proba(X_val)\n#predicted = svm.predict(X_val)\n#accuracy = accuracy_score(y_val, predicted)\n#auc = metrics.roc_auc_score(y_val, probs[:, 1])\n\n# generate evaluation metrics\n#print(f'AUC estimate: {auc:.3}')\n#print(f'Mean accuracy score: {accuracy:.3}')\n#print(metrics.confusion_matrix(y_val, predicted))\n#print(metrics.classification_report(y_val, predicted))","e6f1e5ea":"from keras.models import Sequential\nfrom keras.layers import Dense\nnp.random.seed(777)","01f4c0eb":"X_train_features.shape","ef2f5f6b":"# create model\nmodel = Sequential()\nmodel.add(Dense(12, input_dim=93, activation='relu'))\nmodel.add(Dense(93, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# Fit the model\nmodel.fit(X_train_features, y_train, epochs=5, batch_size=10)\n# evaluate the model\nscores = model.evaluate(X_val_features, y_val)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n\npredictions = model.predict(X_val_features)\ny_rounded = [round(x[0]) for x in predictions]\nscores_test = model.evaluate(X_val_features, y_val)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_test[1]*100))\n\naccuracy_score(y_val, y_rounded)","bb8def9c":"# Phase 2: Making the Neural Network (NN)\n\n# Importing the Keras libraries and packages\nimport keras\nfrom keras.models import Sequential # sequential module reqd to initialize the NN\nfrom keras.layers import Dense # dense module reqd to build the layers of the NN","3b880b10":"# Initialising the ANN\nloans_predictor = Sequential() \n# creating object of Sequential class","6542a54a":"# Adding the input layer and the hidden layer\nloans_predictor.add(Dense(input_dim=93, activation=\"relu\", kernel_initializer=\"uniform\", units=5))","a86b5e30":"# Adding the output layer\nloans_predictor.add(Dense(activation = 'sigmoid', kernel_initializer = \"uniform\", units = 1))","b15741f0":"# Compiling the ANN\nloans_predictor.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","8038e125":"# Fitting the ANN to the Training set\nloans_predictor.fit(X_train_features, y_train, batch_size = 10, nb_epoch = 10)","dfd28485":"# Making the predictions and evaluating the model\n\n# Predicting the Test set results\ny_pred = loans_predictor.predict(X_val_features)","89af59d2":"y_pred = (y_pred > 0.5)","cc39a1b3":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_val, y_pred)\n\ncm","d6e50788":"#### PICKLE AFTER CATEGORICAL CLEANING","f35d0d63":"## 5.21 mort_acc","3907cb87":"#### Also, look at the __classification report__ and __accuracy__.","706ea828":"## 5.16 initial_list_status","3611ec0e":"#Seeing next 5 columns:\n\n- inq_last_6mths:\nThe number of inquiries in past 6 months (excluding auto and mortgage inquiries)\n\n- mths_since_last_delinq: \nThe number of months since the borrower's last delinquency.\n\n- open_acc:\nThe number of open credit lines in the borrower\u2019s credit file.\n\n- pub_rec: Number of derogatory public records\n\n- revol_bal : Total credit revolving balance\n\n\n\n","484dcfba":"Woah! I didn't expect this much change. So, the accuracy improved because: \n\nThe implementation of logistic regression has a penalty on coefficent size (L1 or L2 norm). In this case, feature scaling matters, because coefficients of features with large variance are small and thus less penalized.\n\nI got this from here: https:\/\/www.quora.com\/How-does-feature-scaling-affect-logistic-regression-model\n\nMany algorithms are sensitive to features being on different scales. Example: Gradient Descent, KNN and as we saw logistic regression too. \n\nWhereas, random forest, decision trees are not sensitive to features on different scales. \n\nSolution: Bring features to same scale\n\nCommon Choices: \n\n- Mean Max Scaling\n- Mean\/Variance Standardization\n","819eb92c":">8.8 Gradient boosting - WINNER!","e5c56be8":"# 4. Target Variable","a580cd9f":"> Threshold calibration to improve model F1 score\n\nSimilarly, various choices of classification thresholds would affect the Precision and Recall metrics. Precision and Recall are usually trade offs of each other, so when you can improve both at the same time, your model's overall performance is undeniably improved. To choose a threshold that balances Precision and Recall, we can plot the Precision-Recall curve and pick the point with the highest F1 score. **bold text**","c50dec47":"We need to drop the following columns as they contain information that will not be present while applying for a loan. Meaning, all of them contains data from the future which will not be present at the initial state for the unseen data (people applying for loan)\n\n- out_prncp\n- out_prncp_inv\n- total_pymnt\n- total_pymnt_inv\n- total_rec_prncp\n- total_rec_int\n- total_rec_late_fee\n- recoveries\n- collection_recovery_fee\n- last_pymnt_d\n- last_pymnt_amnt\n- last_credit_pull_d\n\n\n","940c4e76":"## 5.9 dti","ce6ea089":"The column looks fine. Also, it can be seen that people who have worked for 10 or more years are more likely to take a loan.","1c5e6b75":"There are 3,34,262 loan records. Check that the statuses are as expected:","ad1d700f":"It is almost similar by region. Let's see the same by state:","2840fb00":"Can't be very sure. But, by definition, all of these seems important. So, for now, I am letting it be. ","dc393ae6":"So, we've got a lot of columns that we need to understand. Knowing what the columns mean can help us a lot for better results.","ec790899":"> 8. 10 Multi Layer ANN - Keras","77a076fc":"I'll be filling the null values (lesser than 1 year) with 0 assuming that the borrower hasn't worked many years and data is not recorded. Also, I'll be using regex to extract the number of years from all of the data.","299245c1":"The payment plan column (pymnt_plan) has two unique values, 'y' and 'n', with 'y' occurring only 30 times compared to 334232. Let\u2019s drop this column.","53f03414":"Great. With Stochastic Gradient Descent, we are able to increase the AUC score by 1%. AUC with SGD logistic regression is: 92%","b3c52131":"#### READING CLEANED FILE FOR FURTHER MODELING","0e9ac707":"# 3. Exploratory data analysis and missing value imputation","0c4f8023":"> 8.7 Linear discriminant analysis (separate from KNN)","32fe40b0":"It has very few data points for joint application. So, Let's just compare the charge-off rates by application type:","c5740542":"#### Again, a one percent increase in the f1-score from last algorithm.","e15dde24":"## 5.20 tot_cur_bal","cf0e1373":"Loan amounts ranges from \\$500 to \\$40,000, with a median of \\$12,000.","f4653245":"> 8.2 k-nearest neighbors (Very slow)\n\nNext we try k-nearest neighbors. We need to reduce the number of variables to 10 or fewer (reference) for kNN to perform well. We'll use LDA for dimension reduction. The number of component variables to keep is a hyperparameter.","6b3758e5":"Awesome. Just took 9 seconds to give great performance. ","7400ffa1":"> We see that there is no improvement at all but score decreased due to sparse features which became very low with the regularization","2554e2d5":"From the above heatmap we can observe that there is a high correlation between quite a few variables. For example: \n1. fico_range_high and fico_range_low\n2. last_fico_range_high and last_fico_range_low\n\nTherefore, instead of keeping both. I am taking average of such values to eliminate one columns and reduce multicollinearity. \n\n","decb4ec5":"> Threshold calibration to improve model accuracy\n\nWe calculate the accuracy using different values for the classification threshold, and pick the threshold that resulted in the highest accuracy.","ff6049ed":"# 2. Importing the Data","5793f7b7":"People who pay off their loans have several mortgage accounts.","e0842724":"Checking the count of missing values and percentage","36f0f18f":"## 5.11 open_acc","4f295960":"From the above histogram, we see there's a large gap between features missing \"some\" data (<25%) and those missing \"lots\" of data (>40%). \n\nAssuming that it will be very difficult to accurately impute data with more than 30% missing values, dropping such columns. Let's create a list of all columns missing more than 30% data in an alphabetical list:","f91ac16a":"## 5.3 installment","2a6e8596":"> Bagging Classifier : Too Slow","ba9b53de":"### NLP Continued ","7fd4740a":"## 5.25 fico_avg","d06977a3":"## 5.17 application_type","92d649b9":"## 5.13 revol_bal","3e48bd38":"Not much difference","c7fc1cc6":"It can be seen that there are more number of loans taken amount from the same states where there are more number of defaulted risk. This is why the state cannot be taken as a major feature for knowing if a loan will be defaulted or not.\n\n","7cdaa43e":"# 6 Processing Categorical Variables\n\nHere\u2019s an outline of what we\u2019ll be doing in this stage:\n\n- Investigate Categorical Columns\n- Convert Categorical Columns To Numeric Features\n- Map Ordinal Values To Integers\n- Encode Nominal Values As Dummy *Variables*","72d77962":"By now, we are almost done with all the pre-processes required before Machine Learning Modeling. \n\nWe have two more categorical precitors to deal with \"Region\" and \"Desc\". ","252a122b":"# 1. Importing Libraries","52a53932":"### Natural Language Processing Implementation on 'desc' column!","abf33e80":"That's great improvement from what we got from our first algorithm. But it came with a tradeoff. The amount of time taken was pretty high. ","6368c272":"Checking corelation again to see how far have we come. ","d737feec":"#### Currently, this column contains text values that need to be converted to numerical values to be eligible for training a model. ","e54106f1":"3 LDA components are necessary for kNN to perform better than logistic regression.","26066220":"We have reduced the number of features to 26. That's a long way from where we started. I am feeling good about it. Let's see if there are any more opportunities with our dataset. ","b97ff540":"## 5.10 delinq_2yrs :\n\nThe number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years","79b2df77":"> Logistic regression after Scaling","15ab0319":"Joint loans are slightly less likely to be charged-off. So, it is important. Keeping it. ","cf0e90a2":"## 5.27 log_annual_inc","29658f50":"Working on a copy of the dataframe so that I do not have to re-read the entire dataset again in order to save memory.","d317f11f":"Printing the Data Dictionary to refer and understand the data better","c78a91c0":"- delinq_2yrs: The number of 30+ days past-due incidences of delinquency in the borrower\u2019s credit file for the past 2 years\nI am not sure what delinquency is. Let's google it. \n\n- delinquency:neglect of one's duty.\nminor crime, especially that committed by young people.\n\nAgain, looks important as it clearly states individual's chances of being a defaulter. ","c022bcd4":"> 8. 11 Single Layer ANN","0a46ea52":"As we can see there are values like credit card, debt_consolidation etc which conveys more of less similar information (debt). Therefore, reducing it by recoding similar attributes to a standard value. ","1dd47430":"### READING THE SAVED PICKLE","5d82a0ba":"It does not tell us much as mean values for both status are same. Also, it is showing contradictory results that people who have more 30+ days past due incidences are more likely to fully pay the loan. Therefore, dropping it. ","8572fd6a":"\"Our goal here is to predict whether a person will be able to pay off a loan or  he\/she will default.\nWe can see from data dictionary that loan_status is the only field that describes a loan status.\n\nTherefore, we will be using it as target column.\"\n\n","5fe0ec58":"- initial_list_status\t: The initial listing status of the loan. Possible values are \u2013 W, F (not sure what W, F means)\n- collections_12_mths_ex_med\t: Number of collections in 12 months excluding medical collections (need to see further)\n- policy_code\t: publicly available policy_code=1\\nnew products not publicly available policy_code=2\n- application_type\t: Indicates whether the loan is an individual application or a joint application with two co-borrowers\n- acc_now_delinq : The number of accounts on which the borrower is now delinquent.\n","15884252":"## The accuracy score is 67% (which ain't great) with F1 score of 61% but it's a start.","89ec0979":"### CATEGORICAL FEATURES","eb4840e7":"- inq_last_12m : Number of credit inquiries in past 12 months (similar to inq_fi, therefore, removing)\n- chargeoff_within_12_mths : Number of charge-offs within 12 months\t\n\nBy definition, removign following as they don't seem much relevant for predictive modelling. \n\n- acc_open_past_24mths : Number of trades opened in past 24 months.\n- avg_cur_bal\t: Average current balance of all accounts\n- bc_open_to_buy :\tTotal open to buy on revolving bankcards.\n- bc_util\t: Ratio of total current balance to high credit\/credit limit for all bankcard accounts.\n- mo_sin_old_il_acct\t: Months since oldest bank installment account opened\n- mo_sin_old_rev_tl_op : Months since oldest revolving account opened\t\n- mo_sin_rcnt_rev_tl_op\t: Months since most recent revolving account opened\n- mo_sin_rcnt_tl : Months since most recent account opened\n","ba43611f":"#### Convert loan status to 0\/1 charge-off indicator\nChange the response variable loan_status to a 0\/1 variable, where 0 indicates fully paid and 1 indicates charge-off:","b42efc40":"Loans that charge off have a FICO score 9 points lesser on mean.","ad944e8b":"> Testing: Let's see how the classifier performs val **data**. We will use the predict() function this time.","d7904540":"Ensemble methods can be defined as combining several different models (base learners) into final model (meta learner) to reduce the generalization error. \n\nIt relies on the assumption that each model would look at a different aspect of the data which yield to capturing part of the truth. Combining good performing models the were trained independently will capture more of the truth than a single model. Therefore, this would result in more accurate predictions and lower generalization errors.","3b4735af":"> Improvement Ideas:\n\nThe performance improved with this model, but there is still a lot of room for improvement. We can:\n* Utilize the text fields to improve our performance (Bag of Words Representation) (done)\n* Try target enconding for categorical variables (done)\n* Feature selection: Not all variables may be important, we can achieve higher performance by selecting a subset of our vairables.(yet to try)\n* Try ensemble models such as Random Forests (below)","abd3538b":"> 8.5 Neural network","162e052c":"## 5.6 annual_inc","d477275a":"# 7. Training and test subsets\nWe will split our dataset into training (80%) and test (20%) subsets. Sklearn's \"train_test_split()\" function is a useful function to use here.","87e4443b":"## 5.22 pub_rec_bankruptcies","3776512f":"Charged-off loans have higher installments. ","6b632d32":"> 8.6 Naive Bayes","4d6d8b78":"Not much difference. dropping this too. Individual are more likely to charge off but there are more individuals in dataset then joint applications","fdaef796":"People with more bankruptcy records are more likely to charge off. ","5a059975":"let's see the difference in average public records between fully paid loans and charged-off loans\n\n","31e9dd68":"### 3.1 Drop features missing more than 30% data","55b1de11":"Last fico score is not applicable for modeling because it is not available at start of application","8b781cdd":"## 5.1 loan_amnt","1c4fefee":"Obviously, people with higher income are more likely to pay off their loans.\n\n","fde787e6":"#### Convert Categorical Columns to Numeric Features\n\nOrdinal values: these categorical values are in natural order. We can sort or order them either in increasing or decreasing order. \nFor example: S < M < L\n\nNominal Values: these are regular categorical values. We can\u2019t order nominal values. For instance, while we can order loan applicants in the employment length column (emp_length) based on years spent in the workforce:\nyear 1 < year 2 < year 3 \u2026 < year N,\n\nwe can\u2019t do that with the column purpose. It wouldn\u2019t make sense to say:\n\ncar < wedding < education < moving < house\n\nThese are the columns we now have in our dataset:\n\nOrdinal Values\nemp_length\nNominal Values _ home_ownership\nverification_status\npurpose_n\nterm","604d24b4":"## 5.23 purpose_n","6bb342e0":"Let's proceed and contninue to improve financial jargon vocabulary even more. \n\n- revol_util\t- \nRevolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit. In simpler terms, the credit card debt to limit ratio as a percentage. (seems important) (however, need further clearning)\n\n- total_acc - The total number of credit lines currently in the borrower\u2019s credit file (seems important)\n\n- initial_list_status - The initial listing status of the loan. Possible values are \u2013 W, F (not able to find what W, F means) (Not sure) (need to see separately)\n\n- out_prncp\t: Remaining outstanding principal for total amount funded\n\n- out_prncp_inv\t: Remaining outstanding principal for portion of total amount funded by investors\n\n- total_pymnt\t: Payments received to date for total amount funded\n\n- total_pymnt_inv\t: Payments received to date for portion of total amount funded by investors\n\n- total_rec_prncp\t: Principal received to date\n\n- total_rec_int\t: Interest received to date\n\n- total_rec_late_fee\t: Late fees received to date\n\n- recoveries\t: post charge off gross recovery\n\n- collection_recovery_fee\t: post charge off collection fee\n\n- last_pymnt_d : Last month payment was received","d88cad25":"There is slight difference in mean which suggests people with better income are more likely to pay","6c8ae232":"Scaling numerical fields:\n\nWe will apply min-max scaling to our rating field so that they will be between 0-1.","35fcbcf6":"From the dictionary and head of the dataset, we can see that some columns like annual_inc, funded_amt, etc. may be much useful for building our model but at the same time, some columns like id, member_id, etc. will not be helping.\n\nAlso, I will drop the column 'issue_d' which denotes the date the loan was funded since it does not convey any information about the borrower","445245a9":"## 5.4 emp_length","ade7c0f7":"## 5.7 verification_status","28d09627":"## 5.8 addr_state","1c3971be":"## 5.24 earliest_cr_line_n","dbfca96a":"People with higher Revolving line utilization rate are more likely to charge off","eae847de":"**How are the predicted probabilities used to decide class membership?** On each row of predict_proba output, the probabilities values sum to 1. There are two columns, one for each response class: column 0 - predicted probability that each observation is a member of class 0; column 1 - predicted probability that each observation is a member of class 1. From the predicted probabilities, choose the class with the highest probability.\n\nThe key here is that a **threshold of 0.5** is used by default (for binary problems) to convert predicted probabilities into class predictions: class 0, if predicted probability is less than 0.5; class 1, if predicted probability is greater than 0.5.\n\n**Can we improve classifier performance by changing the classification threshold?** Let's **adjust** the classification threshold to influence the performance of the classifier. ","e865cfdd":"## Upsampling the smaller class: \n\nWe will use sklearn's __resample()__ method for upsampling. Here, we will upsample 1 values in the data to match the 0 values (282.5 K)\n\n__Important note:__ For upsampling, we have to use __replace=True__ ","3ee22792":"## 5.18 acc_now_delinq","7688c50e":"However, Accuracy can be a misleading metric. We should make a class based perfomance assessment.","5e43e1b2":"<a href=\"https:\/\/colab.research.google.com\/github\/solharsh\/Data_Understanding_And_Preparation\/blob\/master\/Lending_CLUB_with_NLP.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","7ae2da0e":"To deal with missing values in each column, I will do some data imputation dealing with each dtype differently. For dtype = object, I will use the most frequent values while for numerical dtypes, I will use the median","3d6c5f37":"## PICKLE BEFORE MODELING","0c062793":"There is not much difference in terms of home ownership. However,people who rent and own a house have a higher chance of charge-off. ","f5c5357c":"> Logistic regression with SGD training\n\nThe SGDClassifier estimator in scikit-learn implements linear classifiers (SVM, logistic regression, and others) with stochastic gradient descent (SGD) training. A particular linear classifier is chosen through the loss hyperparameter. Because we want to predict the probability of charge-off, we choose logistic regression (a probabilistic classifier) by setting loss = 'log'.","6db43c9b":"We still have 5 object columns that contain text which need to be converted into numeric features. \n\nLet\u2019s select just the object columns using the DataFrame method select_dtype, then display a sample row to get a better sense of how the values in each column are formatted.","eaf8e95e":"## 5.15 total_acc","d8e0c149":"> 8.4 Random forest classifier","81aad13b":"Using the Precision and Recall values from the curve above, we calculate the F1 scores using:\n\n$$\\text{F1_score} = \\frac{2*(\\text{Precision} * \\text{Recall})}{(\\text{Precision} + \\text{Recall})}$$\n\nand pick the threshold that gives the highest F1 score.","42310f6a":"Status counts as percentages:","d31e43dc":"We will use __Confusion matrix:__ to see the results.\n\n__Confusion matrix:__ The diagonals show us correct classifications. Each row and column belongs to a class (default or no default). The first column and row correspond to \"no default\" case, the second column-rows are \"default\" case.\n\n\n","2a23937a":"### Quick but the AUC score is not good enough ","edaf9b0d":"dti: A ratio calculated using the borrower\u2019s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower\u2019s self-reported monthly income. \n\nLooks important to me. Keeping it. ","d7eb31e8":"We can see many columns are either empty or have very less data compared to the features to be used. \nFor example, member_id, revol_bal_joint, sec_app_fico_range_low, sec_app_fico_range_high and so on. \n\nTherefore, those can be removed upfront. So, I am assuming that predictors that contain more than 70% missing values are not useful and removing them.","053c34d1":"> 8.9 SVM (Support Vector Machines) \n\nThe code is in comments below but not running it as it it taking hours and even then no results. ","d2faa768":"It is clear that loan with longer terms are more likely to charge off. ","ffe2386a":"That's great. We have considerably reduced the features from 136 to 79. ","10a2efab":"With this, I am done with data cleaning. FINALLY!\n\nI\u2019ve been able to reduce the number of columns from 150 to 26 without losing any meaningful data from our model. \n\nData Cleaning Summary: \n- Dropped Data that must have been captured after the application form. This would have messed up our model. \n\n- Imputed missing values with either 0 or median\n\n- Created new feature out of existing (taking average of FICO_low and FICO_high)\n","7dcc0d57":"# 5. Further Pre-processing and Feature Engineering\n\nWe'll inspect each feature separately as it is not in readable range, and do the following:\n\n- Drop the feature if it is not useful for predicting the Target variable.\n- View summary statistics and visualize, plot against the Target variable.\n- Modify the feature to make it useful for modeling, if necessary.\n- We define a function for plotting a variable and comparing with the Target variable.","63630a0c":"We are getting best f1 score of 87% with K = 3 and ROC of 93% with KNN","0158e041":"## 5.5 home_ownership","09dd86ce":"## 5.2 term","bbf774bb":"Columns with categorical values:\n\n- emp_length \u2014 number of years the borrower was employed upon time of application.\n- home_ownership \u2014 home ownership status, can only be 1 of 4 categorical values according to the data dictionary.\n- verification_status \u2014 indicates if income was verified by LendingClub.\n- initial_list_status \u2014 Initial listing status (W,F)\n- purpose_n \u2014 a category provided by the borrower for the loan request.","8be3e9a6":"> To improve the performance of LogisticRegression we can tune its parameters, for example:\n* regularization type: penalty = {l1, l2, elasticnet}\n* regularization strength: C = {smaller values specify stronger regularization} \n*!!! LogisticRegression regularized cost function: C*Cost(w) + penalty(w), \nwhere w is the weights vector !!!\n","50a8cd78":"The columns 'policy_code' has only one class and do not provide any information that would be useful in building a predictive model. Removing it!","e36d3b01":"### We see that there is slight improvement in performance","099d2b15":"## 5.12 pub_rec","a7190f9a":"Visualizing the Target Column Outcomes","88b9646f":"> Trying to improve: Probability threshold calibration\n\nBesides tuning __LogisticRegression__ hyperparameter values, one other path to improve a classifier's performance is to dig deeper into how the classifier actually assigns class membership.\n\n**Binary predictions versus probability predictions.** We often use __classifier.predict()__ to examine classifier binary predictions, while in fact the outputs of most classifiers are real-valued, not binary. For most classifiers in sklearn, the method __classifier.predict_proba()__ returns class probabilities as a two-dimensional numpy array of shape (n_samples, n_classes) where the classes are lexicographically ordered. \n\nFor our example, let's look at the first 5 predictions we made, in binary format and in real-valued probability format:\n\n\n","5e4c592b":"#### There is even more improvement as the F1 score has been increased to 71%.\n\n","0210e6b4":"About 84% of the loans have been fully paid and 16% have charged off, so we can see that we are dealing with an imbalanced dataset. This means one result type is dominating the other one(s). In this case, we have a lot of \"fully paid\" records but not many records of other 6 (charged off) records. ","27d81a9b":"## 8. Fitting the classifier\n\n> Logistic Regression","9e7e346c":"- tot_coll_amt\t: Total collection amounts ever owed (Seems important as how much was owed can definitely tell if a person is likely to default or not)\n- tot_cur_bal\t: Total current balance of all accounts (how much a person keeps in his account surely can be a predictor to tell if he will default or not) (Trust me on this, I have and obviously, I am close to nil almost all the time)\n\nBelow 4 are more or less related to open_acc (The number of open credit lines in the borrower's credit file).Therefore, removing them. \n- open_acc_6m :\tNumber of open trades in last 6 months\n- open_il_12m\t: Number of installment accounts opened in past 12 months\n- open_il_24m\t: Number of installment accounts opened in past 24 months\n- open_act_il\t: Number of currently active installment trades","ab60eec8":"## 5.14 revol_util\n\nRevolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.","fdaed75f":"Too many 0 values and apart from that, outliers are present too. Therefore, dropping. ","321d9fc8":"With KNN, we are able to increase the AUC score with 1% more and the AUC with KNN is 93%.","3a9e12fd":"## 5.28 region","1fd0ac7d":"I'm using my best available knowledge after going through the dictionary and few google pages to determine which loan features are known to potential investors. I am not an investor, so my knowledge loan process is not too great. ","be89625f":"Somebody has 61 mortgage accounts. This could be an outlier or may be some one can. \n\n","6594cb64":"The column 'earliest_cr_line' describes the date when the first credit line was established. Usually, the longer one has held a credit line, the more desirable as a borrower. This feature will therefore be more useful if converted to a measure of how long one has held a credit line. To do this, I will convert the column from object to datetime and calculate the time duration from the establishment of the first credit line to the current date.","fa24f6e7":"Installments range from \\$4.93 to \\$1,584, with a median of \\$369.","f5d7a125":"Let's get our binary vectors for the text field","aa485b7f":"Exploring Next 10 columns:\n- pct_tl_nvr_dlq\t: Percent of trades never delinquent\n\n- percent_bc_gt_75\t: Percentage of all bankcard accounts > 75% of limit.\n\n- pub_rec_bankruptcies\t: Number of public record bankruptcies (seems like important attribute)\n\n- tax_liens\t: Number of tax liens\n\n- tot_hi_cred_lim\t: Total high credit\/credit limit\n\n- total_bal_ex_mort\t: Total credit balance excluding mortgage\n\n- total_bc_limit\t: Total bankcard high credit\/credit limit\n\n- total_il_high_credit_limit\t: Total installment high credit\/credit limit\n\n- hardship_flag\t: Flags whether or not the borrower is on a hardship plan\n\n- debt_settlement_flag : Flags whether or not the borrower, who has charged-off, is working with a debt-settlement company.","3908356a":"## 5.26 last_fico_avf","d6640772":"People with higher number of credit lines are more likely to fully pay. ","83f31322":"### Dealing with Remaining missing values: Data imputation\n ","9382a054":"'collections_12_mths_ex_med' has almost all values as 0 and will therefore not be very useful for modeling. Removing it.","db5b245e":"The variables most linearly correlated with charged_off are the term, FICO score, debt-to-income ratio, number of mortgages, income, and the loan amount.","5563d883":"- mths_since_rcnt_il\t: Months since most recent installment accounts opened\n- total_bal_il :Total current balance of all installment accounts\t \n- open_rv_12m\t: Number of revolving trades opened in past 12 months\n- open_rv_24m :\tNumber of revolving trades opened in past 24 months\n- max_bal_bc : Maximum current balance owed on all revolving accounts\t\n- all_util : Balance to credit limit on all trades\t\n- total_rev_hi_lim : Total revolving high credit\/credit limit\t\n- inq_fi : Number of personal finance inquiries\t\n- total_cu_tl : Number of finance trades","69b9ecb9":"> 8.3 Decision Tree","1b8be421":"#### get correlation with binary outcome","aff69627":"Annual income ranges from \\$0 to \\$7,141,778, with a median of \\$6.324000e+04. Because of the large range of incomes, we can take a log of annual income variable.","c1bf3082":"initital_list_status: \"The initial listing status of the loan. Possible values are \u2013 W, F.\" I'm not sure what this means.","d8f36a00":"### Scaling","f112c3c0":"Reference: https:\/\/machinelearningmastery.com\/tutorial-first-neural-network-python-keras\/","55d32844":"- mort_acc : Number of mortgage accounts.\n- mths_since_recent_bc\t: Months since most recent bankcard account opened.\n- mths_since_recent_inq\t: Months since most recent inquiry.\n- num_accts_ever_120_pd\t: Number of accounts ever 120 or more days past due\n- num_actv_bc_tl : Number of currently active bankcard accounts\n- num_actv_rev_tl\t: Number of currently active revolving trades\n- num_bc_sats\t: Number of satisfactory bankcard accounts\n- num_bc_tl\t: Number of bankcard accounts\n- num_il_tl\t: Number of installment accounts\n- num_op_rev_tl\t: Number of open revolving accounts\n- num_rev_accts\t: Number of revolving accounts\n- num_rev_tl_bal_gt_0\t: Number of revolving trades with balance >0\n- num_sats\t: Number of satisfactory accounts\n- num_tl_120dpd_2m\t: Number of accounts currently 120 days past due (updated in past 2 months)\n- num_tl_30dpd\t: Number of accounts currently 30 days past due (updated in past 2 months)\n- num_tl_90g_dpd_24m\t: Number of accounts 90 or more days past due in last 24 months\n- num_tl_op_past_12m : Number of accounts opened in past 12 months","76477f3e":"not much difference","dcdf7109":"#### Grid search is also time taking procedure. It took the same time as it took me to make and finish my coffee!!!","7b5619fb":"### Saving CSV with the processing until now. ","53e1274a":"not much different in means","446fd051":"## 5.19 tot_coll_amt","c00b5e98":"These are the columns that we are going to use for modeling."}}