{"cell_type":{"df4fafd5":"code","c9b98c0d":"code","40a86196":"code","bc27a970":"code","cf28bf5e":"code","b8cdbe30":"code","7b3fff7a":"code","591cbc81":"code","87ead054":"code","e397c3ea":"code","82ca6ba4":"code","de3b91d6":"code","29343a34":"code","42deee72":"code","8e8e780a":"code","68d06934":"code","90a09323":"code","1b5cb450":"code","c523626a":"code","c0845d1d":"code","4f126920":"code","ba076c63":"code","3479f292":"code","79968001":"code","5effb4b9":"code","92a077da":"code","27cc4c79":"code","910a4ad7":"code","531776b7":"markdown","e086ed4e":"markdown","e4576cd2":"markdown","e953df63":"markdown","9961ba46":"markdown","9fbc1794":"markdown","b2528c89":"markdown","d9ffeea8":"markdown","aabeab14":"markdown","99529c11":"markdown","65b0bd11":"markdown","7f6acbb1":"markdown","cce58e57":"markdown","602782b9":"markdown","5f03d1a7":"markdown","29466703":"markdown","e221b27b":"markdown","5bfb5db1":"markdown","a079150e":"markdown","968337bc":"markdown","59ae79ff":"markdown","b3143c21":"markdown","68e9d624":"markdown","ae8fa3aa":"markdown"},"source":{"df4fafd5":"%matplotlib inline\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport warnings\nimport seaborn as sns \nimport statsmodels.formula.api as smf\nimport statsmodels.api         as sm\nfrom sklearn.utils import shuffle\nfrom scipy.stats import yeojohnson, yeojohnson_normplot, probplot, boxcox\nfrom statsmodels.stats.outliers_influence import OLSInfluence\nwarnings.filterwarnings(\"ignore\")\n\n# Display all columns\npd.set_option('display.max_columns', None)\n\n# Use a ggplot style for graphics\nplt.style.use('ggplot')","c9b98c0d":"# Load data\nfiles = ['\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv', \n         '\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv',\n         '\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv']\ndata = [pd.read_csv(f) for f in files]\ndt_calendar, dt_sales, dt_prices = data\n\n# Merge calendar and prices\ndt_prices = shuffle(dt_prices, n_samples = 3000000)\ndt_complementary = dt_prices.merge(dt_calendar, how='left', on='wm_yr_wk')\ndel dt_prices\ndel dt_calendar\n\n# Shuffle data (it is originally ordered) and take n rows (if you don't have enough RAM)\n#dt_complementary = shuffle(dt_complementary, n_samples=10000000, random_state=0)","40a86196":"# Count the number of zeros in data \ndt_sales['num_zeros'] = (dt_sales == 0).sum(axis=1)\n_ = plt.hist(dt_sales.num_zeros)\nplt.xlabel('Number of zeros')\nplt.ylabel('Frequency')\nplt.title(\"Distribution of number of zeros by observation\")","bc27a970":"# Transform date variable to datetime\ndt_complementary.date = pd.to_datetime(dt_complementary.date)\n\n# Append the first day of sales to each item \nfirst_date = dt_complementary.groupby(['store_id','item_id']).agg({'date':'min'}).reset_index().rename(columns={'date':'date_first_sale'})\ndt_sales = dt_sales.merge(first_date, how='left', on=['store_id','item_id'])\n\n# Delete data to save RAM\ndel first_date\n\n# Difference in days between date_first_sales and d_1\ndt_sales['since_d_1'] = dt_sales.date_first_sale - pd.to_datetime('2011-01-29')\ndt_sales['since_d_1'] = dt_sales['since_d_1'].apply(lambda x: x.days)","cf28bf5e":"# Percentage of zeros since first day of sale\ndt_sales['%_zeros_of_total'] = round(((dt_sales.num_zeros - dt_sales.since_d_1) \/ (1913 - dt_sales.since_d_1)) * 100, 2)\n_ = sns.distplot(dt_sales['%_zeros_of_total'])\n#_ = plt.hist(dt_sales['%_zeros_of_total'], bins=10)\nplt.xlabel('% of zeros')\nplt.ylabel('Frequency')\nplt.title(\"Distribution of % of zeros by item\")","b8cdbe30":"def perc_bin(num:int):\n    if num <= 20:\n        output = 'perc_bin_1'\n    elif num <= 40:\n        output = 'perc_bin_2'\n    elif num <= 60:\n        output = 'perc_bin_3'\n    elif num <= 80:\n        output = 'perc_bin_4'\n    else:\n        output = 'perc_bin_5'\n    return output\n\ndt_sales['perc_zeros_bin'] = dt_sales['%_zeros_of_total'].apply(lambda x: perc_bin(x))","7b3fff7a":"# Items with less percentage of zeros\ndt_sales_bin1 = dt_sales[dt_sales.perc_zeros_bin == 'perc_bin_1'].drop(columns=['id','num_zeros','date_first_sale','since_d_1','%_zeros_of_total','perc_zeros_bin'])\ndel dt_sales\n\n# Melt sales data\nindicators = [f'd_{i}' for i in range(1,1914)]\n\ndt_sales_bin1_melt = pd.melt(dt_sales_bin1, \n                             id_vars = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n                             value_vars = indicators, \n                             var_name = 'day_key', \n                             value_name = 'sales_day')\ndel dt_sales_bin1\n\n# Extract the number of day from the day_key variable\ndt_sales_bin1_melt['day'] = dt_sales_bin1_melt['day_key'].apply(lambda x: x[2:]).astype(int)","591cbc81":"# Data to work with\ncolumns = ['store_id','item_id','sell_price','date','year','d','event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']\ndt_work = dt_sales_bin1_melt.merge(dt_complementary[columns], how = 'inner', left_on=['item_id','store_id','day_key'], right_on=['item_id','store_id','d'])\ndel dt_complementary\nprint(dt_work.shape)","87ead054":"# If there are null values, print the unique values of the column \nfor k,v in dict(dt_work.isnull().sum()).items():\n    if v > 0:\n        print(f\"The unique values for the column {k} are:\", dt_work[k].unique(), \"\\n\")","e397c3ea":"dt_work['event_name_1'] = dt_work['event_name_1'].fillna('Normal')\ndt_work['event_name_2'] = dt_work['event_name_2'].fillna('Normal')\ndt_work['event_type_1'] = dt_work['event_type_1'].fillna('Non-Special')\ndt_work['event_type_2'] = dt_work['event_type_2'].fillna('Non-Special')","82ca6ba4":"# Taken from https:\/\/datascience.stackexchange.com\/questions\/10459\/calculation-and-visualization-of-correlation-matrix-with-pandas\ncorr = dt_work.corr()\n\ncmap = cmap=sns.diverging_palette(5, 250, as_cmap=True)\n\ndef magnify():\n    return [dict(selector=\"th\",\n                 props=[(\"font-size\", \"7pt\")]),\n            dict(selector=\"td\",\n                 props=[('padding', \"0em 0em\")]),\n            dict(selector=\"th:hover\",\n                 props=[(\"font-size\", \"12pt\")]),\n            dict(selector=\"tr:hover td:hover\",\n                 props=[('max-width', '200px'),\n                        ('font-size', '12pt')])\n]\n\ncorr.style.background_gradient(cmap, axis=1)\\\n    .set_properties(**{'max-width': '100px', 'font-size': '10pt'})\\\n    .set_caption(\"Correlation between variables\")\\\n    .set_precision(2)\\\n    .set_table_styles(magnify())","de3b91d6":"print(dt_work.sales_day.describe(),\n      f\"The 1th percentile is {dt_work.sales_day.quantile(.01)}\", \"\\n\",\n      f\"The 5th percentile is {dt_work.sales_day.quantile(.05)}\", \"\\n\",\n      f\"The 10th percentile is {dt_work.sales_day.quantile(.1)}\", \"\\n\",\n      f\"The 15th percentile is {dt_work.sales_day.quantile(.15)}\", \"\\n\",\n      f\"The 20th percentile is {dt_work.sales_day.quantile(.15)}\", \"\\n\",\n      f\"The 90th percentile is {dt_work.sales_day.quantile(.90)}\", \"\\n\",\n      f\"The 98th percentile is {dt_work.sales_day.quantile(.98)}\", \"\\n\",\n      f\"The 99th percentile is {dt_work.sales_day.quantile(.99)}\", \"\\n\",\n      f\"The 99th percentile is {dt_work.sales_day.quantile(.995)}\", \"\\n\",\n      f\"The 99th percentile is {dt_work.sales_day.quantile(.999)}\", \"\\n\")","29343a34":"fig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111)\n\n# Yeo-Johson Normality Plot \nlmbd_yj = yeojohnson_normplot(dt_work.sales_day, -10, 10, plot=ax)\nsales_transformed, maxlmbd = yeojohnson(dt_work.sales_day)\n\nax.axvline(maxlmbd, color='r')\nplt.show()","42deee72":"# QQ-plot of sales transformation \nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111)\nqq_plot = probplot(sales_transformed, dist=\"norm\", plot=ax)\nax.set_title(\"QQ-plot for normal distribution\")\nplt.show()","8e8e780a":"plt.figure(figsize=(12, 6))\n\n# Left side figure\nplt.subplot(1,2,1)\n_ = sns.distplot(dt_work.sales_day)\nplt.title(\"Original Distribution\")\n\n# Right side figure\nplt.subplot(1,2,2)\n_ = sns.distplot(sales_transformed, rug=True)\nplt.title(\"With Yeo-Johnson Transformation\")\nplt.tight_layout(pad=1)\nplt.show()","68d06934":"dt_work['sales_day_yj'] = sales_transformed\n\n# Filter by columns of interest\ncols_to_drop = ['item_id', 'day_key', 'day', 'date', 'd']\ndt_reg = dt_work.drop(columns=cols_to_drop)\n\n# Covert to category type \nfor k, v in dict(dt_reg.dtypes).items(): \n    if v == 'object':\n        dt_reg[k] = dt_reg[k].astype('category')\n \n# Dummy variables    \ndt_reg = pd.get_dummies(dt_reg)","90a09323":"# Model with Yeo-Johnson transformation\nformula_yj=\"sales_day_yj ~ \"\n\nfor col in dt_reg.columns[8:]:\n    formula_yj+='Q(\"'+col+'\")+'\n    \nformula_yj = formula_yj + 'sell_price + year'\n\nmodel_yj = smf.ols(formula = formula_yj, data = dt_reg).fit()\nmodel_yj.summary()","1b5cb450":"# Model with Yeo-Johnson transformation\nformula=\"sales_day ~ \"\n\nfor col in dt_reg.columns[8:]:\n    formula+='Q(\"'+col+'\")+'\n    \nformula = formula + 'sell_price + year'\n\nmodel_orig = smf.ols(formula = formula, data = dt_reg).fit()\nmodel_orig.summary()","c523626a":"# Residuals vs fitted values \nmodel_fitted = model_yj.fittedvalues\nmodel_residuals = model_yj.resid\nfig = plt.figure(figsize = (8, 6))\nsns.scatterplot(model_fitted, \n                model_residuals,\n                alpha=0.5\n                  )\nplt.title('Residuals vs Fitted')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')","c0845d1d":"# Cook's distance values\noutlierInfluence = OLSInfluence(model_yj)\n(c, p) = outlierInfluence.cooks_distance\n\n# Leverage and normalized residuals\nmodel_leverage = model_yj.get_influence().hat_matrix_diag\nmodel_norm_residuals = model_yj.get_influence().resid_studentized_internal\nmodel_cooks = model_yj.get_influence().cooks_distance[0]","4f126920":"plt.figure(figsize=(12, 6))\n\n# Cook's distance plot\nplt.subplot(1,2,1)\nplt.stem(np.arange(20000), c[:20000], markerfmt=\",\")\nplt.title(\"Cook's distance plot for the residuals\",fontsize=16)\nplt.grid(False)\n\n\n# Scatterplot of leverage vs normalized residuals\nplt.subplot(1,2,2)\nplt.scatter(model_leverage[:200000], \n            model_norm_residuals[:200000], alpha=0.5)\nsns.regplot(model_leverage[:200000], \n            model_norm_residuals[:200000],\n            scatter=False,\n            ci=False,\n            lowess=True,\n            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8});\nplt.xlim(-0.0005, 0.0010)\nplt.title('Residuals vs Leverage')\nplt.xlabel('Leverage')\nplt.ylabel('Standardized Residuals')\n\nplt.tight_layout(1.0)\nplt.show()","ba076c63":"# Taken from https:\/\/stackoverflow.com\/questions\/42658379\/variance-inflation-factor-in-python\/54857466\n\ndef variance_inflation_factors(exog_df):\n    '''\n    Parameters\n    ----------\n    exog_df : dataframe, (nobs, k_vars)\n        design matrix with all explanatory variables, as for example used in\n        regression. One recommendation is that if VIF is greater than 5, then \n        the explanatory variable given by exog_idx is highly collinear with the \n        other explanatory variables, and the parameter estimates will have large \n        standard errors because of this.\n\n    Returns\n    -------\n    vif : Series\n        variance inflation factors\n    '''\n    exog_df = sm.add_constant(exog_df)\n    vifs = pd.Series([1 \/ (1. - sm.OLS(exog_df[col].values, \n                                       exog_df.loc[:, exog_df.columns != col].values) \\\n                                   .fit() \\\n                                   .rsquared\n                           ) \n                           for col in exog_df],\n        index=exog_df.columns,\n        name='VIF'\n    )\n    return vifs\n\ncols = ['sell_price', 'year', 'snap_CA', 'snap_TX', 'snap_WI', 'sales_day_yj']\nvariance_inflation_factors(dt_reg[cols])","3479f292":"iterables = [('state_id_CA','snap_CA'), ('state_id_TX', 'snap_TX'), ('state_id_WI', 'snap_WI')]\n\n\nfor i in iterables:\n    state, snap = i\n    sns.lmplot(x='year',\n               y='sales_day_yj', \n               data=dt_reg[dt_reg[state]==1], \n               hue=snap,\n               col=snap,\n               height=8, \n               scatter_kws={\"s\": 10},\n               x_jitter=.25,\n               y_jitter=.05\n              )\n    \nplt.tight_layout(1)\nplt.show()","79968001":"for i in iterables:\n    state, snap = i\n    formula = (f'sales_day_yj ~ year*C({snap})')\n    model = smf.ols(formula=formula, data=dt_reg[dt_reg[state]==1]).fit()\n    print(f\"Model for {state}\",\n          \"\\n\",\n          \"-----------------------------------------------------------\",\n          \"\\n\",\n          model.summary(),\n          \"\\n\",\n          \"-----------------------------------------------------------\")","5effb4b9":"dt_reg['log_sell_price'] = np.log(dt_reg.sell_price + 1)\n\nfor i in iterables:\n    state, snap = i\n    sns.lmplot(x='year',\n               y='log_sell_price', \n               data=dt_reg[dt_reg[state]==1], \n               hue=snap,\n               col=snap,\n               height=8, \n               scatter_kws={\"s\": 10},\n               x_jitter=.15,\n               y_jitter=.05)\n    \nplt.tight_layout(1)\nplt.show()","92a077da":"for i in iterables:\n    state, snap = i\n    formula = (f'log_sell_price ~ year*C({snap})')\n    model = smf.ols(formula=formula, data=dt_reg[dt_reg[state]==1]).fit()\n    print(f\"Model for {state}\",\n          \"\\n\",\n          \"-----------------------------------------------------------\",\n          \"\\n\",\n          model.summary(),\n          \"\\n\",\n          \"-----------------------------------------------------------\")","27cc4c79":"for i in iterables:\n    state, snap = i\n    formula = (f'log_sell_price ~ dept_id_FOODS_1*C({snap}) + dept_id_FOODS_2*C({snap}) + dept_id_FOODS_3*C({snap}) + dept_id_HOBBIES_1*C({snap}) + dept_id_HOUSEHOLD_1*C({snap}) + dept_id_HOUSEHOLD_2*C({snap})')\n    model = smf.ols(formula=formula, data=dt_reg[dt_reg[state]==1]).fit()\n    print(f\"Model for {state}\",\n          \"\\n\",\n          \"-----------------------------------------------------------\",\n          \"\\n\",\n          model.summary(),\n          \"\\n\",\n          \"-----------------------------------------------------------\")","910a4ad7":"iterables = [('CA','snap_CA'), ('TX', 'snap_TX'), ('WI', 'snap_WI')]\n\nfor i in iterables:\n    state, snap = i\n    formula = (f'np.log(sell_price+1) ~ C(dept_id)*C({snap})')\n    model = smf.ols(formula=formula, data=dt_work[dt_work.state_id==state]).fit()\n    print(f\"Model for {state}\",\n          \"\\n\",\n          \"-----------------------------------------------------------\",\n          \"\\n\",\n          model.summary(),\n          \"\\n\",\n          \"-----------------------------------------------------------\")","531776b7":"***B. Melt data and dataframe to work***","e086ed4e":"Based on the graph, we're considering the following bins:\n\n1. `perc_bin_1` --> 0 to 20% zeros.\n2. `perc_bin_2` --> 21 to 40% zeros.\n3. `perc_bin_3` --> 41 to 60% zeros.\n4. `perc_bin_4` --> 61 to 80% zeros.\n5. `perc_bin_5` --> 81 to 100% zeros.","e4576cd2":"#### B. Leverage Points\n\nThe `Cook's distance plot` and the `leverage vs residuals` plot are informative. We see that there are some points with high Cook's distance (left plot) that appear frequently (apparently every 7500 units). Furthermore, in the right plot that there's a clear distinction between normal points and leverage points. When building the forecasting model, we should be cautious about them because they will **highly leverage the model** that we choose.","e953df63":"Assessing **Multicollinearity** is key, because knowing that one variable could be expressed as a linear combination of other(s), let us avoid the sensity of the coefficient of the model to this phenomenon. \n\nIt is interesting to notice that there is nearly a perfect correlation between `day` and `year`. This is due to the fact that as year increases the day increases, for example, the first 365 observations of the day are for the first year, the next 365 for the second year and so.","9961ba46":"### 3. Correlation ","9fbc1794":"***Notes***\n\nThe results may vary because for Kaggle version I'm considering only a shuffle version of 3,000,000 observations for the data of prices to avoid running out of memory. For the results with the complete data see [link](https:\/\/github.com\/TremaMiguel\/KaggleCompetitions\/blob\/master\/M5_Forecasting\/notebooks\/M5_Forecasting_CDA.ipynb).\n\n1. ```dt_complementary``` merges the original calendar and price data to know the ```date``` when the ```product_id``` of the ```store_id``` was saled at. Additionaly to know if they were particular events like ```SNAP``` purchases or major events, check the documentation.\n\n2. ```dt_sales_s```. In case you don't have enough RAM take initial n rows of the original dataframe and shuffle the data, because it is ordered.\n\n3. ```dt_sales_melt```. Melt the dataframe so that each sale by product can be seen as row.\n\n4. ```dt_work```. Merges ```dt_sales_melt``` with ```dt_complementary``` to know for each day of sale the price and relevant events associated to that day.","b2528c89":"**C. What about the `sell_price` on SNAP days across different departments?**\n\nWe conclude that there is no statistical significance difference for the prices across different departments on SNAP days.","d9ffeea8":"### 5. Multiple Linear Regression","aabeab14":"#### A. All variables model","99529c11":"#### C. Variance Inflation Factor\n\nWith the `VIF` factor we try to understand how much the variance is going to increase because of collinearity. It is recommended is that if VIF is greater than 5, then the explanatory variable given is highly collinear with the other explanatory variables, and the parameter estimates will have large standard errors because of this. In our case (we're not considering dummy variables), the values for the choosen variables are near 1. ","65b0bd11":"### 1. Preprocess Data","7f6acbb1":"We're not surprise to see that our model achieves to explain only 6% of the variability in the sale. Notwithstanding, we notice the following\n\n* All Coefficients are statistically significant. The `p-value` is close to zero for all the coefficient associated to each variable. \n\n* There is higher sale in `HOBBIES_1` compared to the base category `FOODS_1`. However, when considering categories, the `FOODS` get more sales. \n\n* Across stores the sales appears to be the same. In fact, the coefficient associated to each store is very close to two. \n\n* There are some events that leverage positively the sale like `NBA finals` or the `SuperBowl`. By contrast, it is surprising to see that `Christmas day` is associated with a reduction in sale, maybe these days the stores are not at all open or not the whole day. \n\n* `Cultural_events` tend to foster sales. \n\n* It appears that sale decrease with the years and with price increases. This confirms the negative correlation that we saw before, but we should be cautious about affirming this.","cce58e57":"We see a staggered distribution, that is, as the number of zeros increases so the frequency. But remember that we're not yet considered when does each item started to being sell. For example, a certain product could have zero sell from `d_1` up to `d_1000` because it started to been sold on `d_1001`. Thus, it is unnecessary to consider the information prior to this day for this item. \n\nTherefore, we're going to obtain the first day of sale form `dt_complementary` and then measure the percentage of zeros by the total number of sale day = `d_1913` - `first_day_of_sale`.","602782b9":"### 6. Model Diagnosis ","5f03d1a7":"## Conclusions","29466703":"***A. Zeros in absolute and percentage terms***\n\nIf we plot the distribution of the `sales_day` variable we're going to notice that most of the observations are zero, in fact, even the median is zero. For this reason, we're going to analyse the number of zeros in `dt_sales`, because they could represent noise for forecasting.","e221b27b":"# Kaggle's M5 forecasting Competition\n\nAuthor: `Armando Miguel Trejo Marrufo`","5bfb5db1":"### 2. Data Imputation","a079150e":"#### A.Evaluate Homokesdasticity (Residuals vs Fitted values)\n\nThe zero values are causing distortions in the regression. However, we see a **handfan** form, that is, as the value to fit increases so the residual, thus `Heterokedasticity`. ","968337bc":"We notice that we got missing values for the events, and that this is due to the fact that in that particular day was not tagged as an special event day. Thus, we're simply considering this days as `Normal` and the event type associated with them as `Non-Special`.","59ae79ff":"### 7. Interaction Effects\n\n**A. Is the sale different on SNAP days across the years?**\n\nWe notice a slight negative slope for the three states, in other words, it seems to be a decrease in sales in the recent years. In fact, we see that in the first three year 2011, 2012 and 2013 the maximum values are greater than the other years 2014, 2015 and 2016.\n\nHowever, by visual comparison is not at all clear if snap sales leverage sales. For this reason we run an `OLS estimation` and evaluate the coefficients and p-values. \n\n1. **California**. The coefficients are statistically significiant. We read the output as follows\n    * The sale increases by 12.2571% among SNAP days, but, \n    \n    * for each year of increment, ***the sale should decrease by 0.0315 percent among not SNAP days and decrease by 0.0376 among SNAP days***.\n    \n    \n2. **Texas**. The interpretation is similar as California, but ***SNAP days have a greater impact on sales by 17.0033%***.\n\n\n3. **Wisconsin**. In this state, the situtation is totally the opposite. The p-values are greater than .05, thus, ***there is no difference in sales between SNAP days and not SNAP days***. Furthermore, even though the sale decreases by 0.0235% each year, there is no effect of SNAP days on it.   \n    ","b3143c21":"**B. Are prices higher on SNAP days?**\n\nWe would like to answer the prior question and evaluate this behavior across the years. The conclusion is that there is very little statistical evidence of an interaction between `snap` and `year` variables to explain prices. In other words, the general view is that ***the prices are not higher on SNAP days***. Now, let's evaluate this across the different departments. ","68e9d624":"The purpose of the Confirmatory Data Analysis was to statistically proof the possible relationships between the variables. We've learned the following:\n\n1. The presence of zeros among items vary. This is highly important because we would going to apply different techniques and models to these items, not necessarily solely intermetting models.\n\n\n2. Sales does not follow a normal distribution. For the purposes of fitting a multiple regression we applied a Yeo-Johnson transformation so that the data approximates a normal distribution. However, we saw that zero values present in data makes this difficult. We would need other techniques or feature engineer to deal with this.\n\n\n3. The dependent variables do not inflate the variance. In other words, we can incorporate variables such as prices and snap days to predict the sale. \n\n\n4. It is possible to divide leverage points from normal. This is highly valuable when forecasting, because leverage points have a high influence in the coefficients of the model. Thus, we could develop an strategy to discard or incorporate these points. \n\n\n5. Behavior on SNAP days. We saw that SNAP prices increase the sales and that there are not changes in prices on SNAP days. ","ae8fa3aa":"### 4. `sales_day` distribution"}}