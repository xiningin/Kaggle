{"cell_type":{"064aca43":"code","f139a2b2":"code","6d7e78f4":"code","58dac66f":"code","690ab438":"code","828e82ed":"code","be64c552":"code","d651100b":"markdown","86c575c1":"markdown","24a0dfec":"markdown","95c82d5c":"markdown","fe2f27ae":"markdown"},"source":{"064aca43":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f139a2b2":"import pandas as pd \nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/text-mining-preprocessing\/data-artikel.csv\")\n\ndata.head()","6d7e78f4":"data['Kalimat'] = data['Kalimat'].str.lower()\n\n\nprint('Case Folding Result : \\n')\nprint(data['Kalimat'].head())\nprint('\\n\\n\\n')","58dac66f":"import string \nimport re #regex library\n\n# import word_tokenize & FreqDist from NLTK\nfrom nltk.tokenize import word_tokenize \nfrom nltk.probability import FreqDist\n\ndef remove_text_special(text):\n    # remove tab, new line, ans back slice\n    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n    # remove non ASCII (emoticon, chinese word, .etc)\n    text = text.encode('ascii', 'replace').decode('ascii')\n    # remove mention, link, hashtag\n    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\\/\\\/\\S+)\",\" \", text).split())\n    # remove incomplete URL\n    return text.replace(\"http:\/\/\", \" \").replace(\"https:\/\/\", \" \")\n                \ndata['Kalimat'] = data['Kalimat'].apply(remove_text_special)\n\n#remove number\ndef remove_number(text):\n    return  re.sub(r\"\\d+\", \"\", text)\n\ndata['Kalimat'] = data['Kalimat'].apply(remove_number)\n\n#remove punctuation\ndef remove_punctuation(text):\n    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n\ndata['Kalimat'] = data['Kalimat'].apply(remove_punctuation)\n\n#remove whitespace leading & trailing\ndef remove_whitespace_LT(text):\n    return text.strip()\n\ndata['Kalimat'] = data['Kalimat'].apply(remove_whitespace_LT)\n\n#remove multiple whitespace into single whitespace\ndef remove_whitespace_multiple(text):\n    return re.sub('\\s+',' ',text)\n\ndata['Kalimat'] = data['Kalimat'].apply(remove_whitespace_multiple)\n\n# remove single char\ndef remove_singl_char(text):\n    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n\ndata['Kalimat'] = data['Kalimat'].apply(remove_singl_char)\n\n# NLTK word rokenize \ndef word_tokenize_wrapper(text):\n    return word_tokenize(text)\n\ndata['Kalimat_tokens'] = data['Kalimat'].apply(word_tokenize_wrapper)\n\nprint('Tokenizing Result : \\n') \nprint(data['Kalimat_tokens'].head())\nprint('\\n\\n\\n')","690ab438":"def freqDist_wrapper(text):\n    return FreqDist(text)\n\ndata['Kalimat_tokens_fdist'] = data['Kalimat_tokens'].apply(freqDist_wrapper)\n\nprint('Frequency Tokens : \\n') \nprint(data['Kalimat_tokens_fdist'].head().apply(lambda x : x.most_common()))","828e82ed":"from nltk.corpus import stopwords\n\n# ----------------------- get stopword from NLTK stopword -------------------------------\n# get stopword indonesia\nlist_stopwords = stopwords.words('indonesian')\n\n\n# ---------------------------- manualy add stopword  ------------------------------------\n# append additional stopword\nlist_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n                       '&amp', 'yah'])\n\n# ---------------------------------------------------------------------------------------\n\n# convert list to dictionary\nlist_stopwords = set(list_stopwords)\n\n\n#remove stopword pada list token\ndef stopwords_removal(words):\n    return [word for word in words if word not in list_stopwords]\n\ndata['Kalimat_tokens_WSW'] = data['Kalimat_tokens'].apply(stopwords_removal) \n\n\nprint(data['Kalimat_tokens_WSW'].head())","be64c552":"data.to_csv(\"hasil-preprocessing.csv\", index = False)","d651100b":"NLTK calc frequency distribution","86c575c1":"Filtering (Stopword Removal)","24a0dfec":"Case Folding","95c82d5c":"Tokenizing","fe2f27ae":"Save Preprocessing Data"}}