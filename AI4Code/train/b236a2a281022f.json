{"cell_type":{"134d34ad":"code","8efd2d28":"code","e31f1cb7":"code","d1373ae4":"code","a827306b":"code","1332fc46":"code","431ec99b":"code","2e030943":"code","10deba3a":"code","173e54f8":"code","54c2727c":"code","49041b54":"code","635ba4f3":"code","11ebd5d2":"code","87e0e49d":"code","ebd63cd6":"code","2c51e4e8":"code","0b119c75":"markdown","1560dc4e":"markdown","920f147b":"markdown","bd1d4763":"markdown","da582fff":"markdown","05b9e2d3":"markdown","a05888c5":"markdown","b814d6f1":"markdown","22738207":"markdown","916cada4":"markdown","33867d59":"markdown"},"source":{"134d34ad":"!pip install alibi","8efd2d28":"import shap\nshap.initjs()\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom alibi.explainers import KernelShap\nfrom scipy.special import logit\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n","e31f1cb7":"data = pd.read_csv('..\/input\/heart-disease-cleveland-uci\/heart_cleveland_upload.csv')\n# To display the top 5 rows\ndata.head(5)","d1373ae4":"heart = data.copy()","a827306b":"target = 'condition'\nfeatures_list = list(heart.columns)\nfeatures_list.remove(target)","1332fc46":"y = heart.pop('condition')","431ec99b":"X_train, X_test, y_train, y_test = train_test_split(heart, y, test_size=0.2, random_state=33)","2e030943":"print(\"Training records: {}\".format(X_train.shape[0]))\nprint(\"Testing records: {}\".format(X_test.shape[0]))","10deba3a":"scaler = StandardScaler().fit(X_train)\nX_train_norm = scaler.transform(X_train)\nX_test_norm = scaler.transform(X_test)","173e54f8":"classifier = LogisticRegression(random_state=0)\nclassifier.fit(X_train_norm, y_train)","54c2727c":"y_pred = classifier.predict(X_test_norm)","49041b54":"cm = confusion_matrix(y_test, y_pred)\ntitle = 'Confusion matrix for the logistic regression classifier'\ndisp = plot_confusion_matrix(classifier,\n                             X_test_norm,\n                             y_test,\n                             #display_labels=target,\n                             cmap=plt.cm.Blues,\n                             normalize=None,\n                            )\ndisp.ax_.set_title(title)","635ba4f3":"pred = classifier.predict_proba\nlr_explainer = KernelShap(pred, link='logit') \n#The purpose of the logit link is to take a linear combination of the values (which may take any value between \u00b1\u221e) and convert those values to the scale of a probability, i.e., between 0 and 1.\nlr_explainer.fit(X_train_norm)","11ebd5d2":"lr_explanation = lr_explainer.explain(X_test_norm, l1_reg=False)","87e0e49d":"idx =  4\ninstance = X_test_norm[idx][None, :]\npred = classifier.predict(instance)\nclass_idx = pred.item()\nprint(\"The predicted class for the X_test_norm[{}] is {}.\".format(idx, *pred))","ebd63cd6":"shap.initjs()\nshap.force_plot(lr_explanation.expected_value[class_idx], lr_explanation.shap_values[class_idx][idx,:], X_test_norm[idx][None, :],features_list)","2c51e4e8":"shap.summary_plot(lr_explanation.shap_values[1], X_test_norm, features_list)","0b119c75":"Checking for accuracy","1560dc4e":"The base value is the average of all output values of the model on the training data(here : -0.3148).\n\nPink values drag\/push the prediction towards 1(pushes the prediction higher i.e. towards having heart disease) and the blue towards 0(pushes the predicion lower i.e. towards no disease).\n\nThe magnitude of influence is determined by the length of the features on the horizontal line. The value shown corresponding to the feature are the values of feature at the particular index(eg. 2.583 for ca). Here, the highest influence is of ca for increasing the prediction value and of sex for decreasing the value.","920f147b":"# KERNEL SHAP","bd1d4763":"The goal of SHAP is to calculate the impact of every feature on the prediction.\n\n\n\nHow is Kernel SHAP different from other permutation importance methods -\n\nIn Kernel SHAP, instead of retraining models with permutations of features, we can use the full model that is already trained, and replace \"missing features\" with \"samples from the data\" that are estimated from a formula.\nThis means that we equate \"absent feature value\" with \"feature value replaced by random feature value from data\".\n\nNow, this changed feature space is fitted to the linear model and the coefficients of this model act as Shapley values.\n\nSHAP has the capability of both local and global interpretations. SHAP can compute the importance of each feature on the prediction for an individual instance and for the overall model as well.\n\nSHAP values are consistent and reliable because if a model changes so that the marginal contribution(i.e. percentage out of the total) of a feature value increases or stays the same (regardless of other features), they increase or remain the same respectively.\n\nThus, SHAP values are mathematically more accurate.","da582fff":"Loading and preparing data ","05b9e2d3":"Applying Kernel SHAP","a05888c5":"The above plot visualizes the impact of features on the prediction class 1. The features are arranged such that the highest influence is of the topmost feature. Thus, ca is the feature that influences the prediction the most followed by thal and so on. \n\nThe colour shades show the direction in which the feature impacts the prediction. For example, higher shap values of ca are shown in red colour which means high feature value. The higher the value of ca, the higher is the SHAP value i.e. more towards 1 . High ca ---> Heart Disease.\n\n\nAlmost all features show this pattern. However, it is the opposite for some features: High thalach will indicate less chances of Heart disease.","b814d6f1":"**Do consider upvoting if you found this notebook helpful. Happy learning!**","22738207":"LOCAL EXPLANATION -","916cada4":"GLOBAL EXPLANATION -","33867d59":"Training data"}}