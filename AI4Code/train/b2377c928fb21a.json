{"cell_type":{"28ee8c36":"code","9bf098a8":"code","05590140":"code","c356bda2":"code","ed07758e":"code","c938a5e2":"code","3557c273":"code","69fc3bb1":"code","2ada9368":"code","c1f5425d":"code","dba59871":"code","82ca244b":"code","1415fb94":"code","67e7fa9e":"code","bc49d547":"code","573b4522":"code","9893cfb3":"code","54e9cf59":"code","8b2b9851":"code","a994c8d9":"code","cb3f5ce0":"code","04b1ab63":"markdown","2ac320e7":"markdown","ee31c9ed":"markdown"},"source":{"28ee8c36":"import numpy as np,pandas as pd,pylab as pl\nimport h5py,torch\nfrom torchvision.datasets import MNIST as tmnist\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader as tdl\nfrom torch.utils.data import Dataset as tds\nimport torch.nn.functional as tnnf\nfrom sklearn.datasets import make_classification\ndev=torch.device(\"cuda:0\" if torch.cuda.is_available() \n                 else \"cpu\")","9bf098a8":"# artificial data\nN=500; n=int(.2*N)\nX,y=make_classification(n_samples=N,n_features=2,\n                        n_redundant=0,n_informative=2)\nmu,std=np.mean(X,axis=0),np.std(X,axis=0)\nX=(X-mu)\/std\nX,y=X.astype('float32'),y.astype('int32')\npl.figure(figsize=(11,3)); pl.grid()\npl.scatter(X[:,0],X[:,1],marker='o',\n           s=10,c=y,cmap='cool');","05590140":"# shuffling & splitting\nshuffle_ids=np.arange(N)\nnp.random.RandomState(23).shuffle(shuffle_ids)\nX,y=X[shuffle_ids],y[shuffle_ids]\nX_test,X_train=X[:n],X[n:]\ny_test,y_train=y[:n],y[n:]\npl.figure(figsize=(11,3)); pl.grid()\npl.scatter(X_test[:,0],X_test[:,1],marker='o',\n           s=10,c=y_test,cmap='cool');","c356bda2":"class Perceptron():\n    def __init__(self,num_features):\n        self.num_features=num_features\n        self.weights=torch.zeros(num_features,1, \n                                 dtype=torch.float32,device=dev)\n        self.bias=torch.zeros(1,dtype=torch.float32,device=dev)\n    def forward(self,x):\n        values=torch.add(torch.mm(x,self.weights),self.bias)\n        a,b=torch.ones(values.size()[0],1),torch.zeros(values.size()[0],1)\n        predictions=torch.where(values>0.,a,b).float()\n        return predictions        \n    def backward(self,x,y):  \n        predictions=self.forward(x)\n        errors=y-predictions\n        return errors        \n    def train(self,x,y,epochs):\n        for e in range(epochs):            \n            for i in range(y.size()[0]):\n                errors=self.backward(x[i].view(1,self.num_features),\n                                     y[i]).view(-1)\n                self.weights+=(errors*x[i]).view(self.num_features,1)\n                self.bias+=errors                \n    def acc(self,x,y):\n        predictions=self.forward(x).view(-1)\n        accuracy=torch.sum(predictions==y).float()\/y.size()[0]\n        return accuracy","ed07758e":"model=Perceptron(num_features=2)\ntX_train=torch.tensor(X_train,dtype=torch.float32,\n                      device=dev)\nty_train=torch.tensor(y_train,dtype=torch.float32,\n                      device=dev)\nmodel.train(tX_train,ty_train,epochs=5)\nprint('Weights: %s'%model.weights)\nprint('Bias: %s'%model.bias)","c938a5e2":"# evaluating\ntX_test=torch.tensor(X_test,dtype=torch.float32,\n                     device=dev)\nty_test=torch.tensor(y_test,dtype=torch.float32,\n                     device=dev)\nacc_test=model.acc(tX_test,ty_test)\nprint('Test accuracy: %.2f%%'%(acc_test*100))","3557c273":"W,b=model.weights,model.bias\nx_min=-2; x_max=2\ny_min=((-(W[0]*x_min)-b[0])\/W[1])\ny_max=((-(W[0]*x_max)-b[0])\/W[1])\nfig,ax=pl.subplots(1,2,sharex=True,figsize=(11,3))\nax[0].plot([x_min,x_max],[y_min,y_max],c='red')\nax[1].plot([x_min,x_max],[y_min,y_max],c='red')\nax[0].scatter(X_train[:,0],X_train[:,1],\n              c=y_train,s=10,cmap=pl.cm.cool)\nax[1].scatter(X_test[:,0], X_test[:,1],\n              c=y_test,s=10,cmap=pl.cm.cool)\nax[0].grid(); ax[1].grid()","69fc3bb1":"class LogisticRegression():\n    def __init__(self,num_features):\n        self.num_features=num_features\n        self.weights=torch.zeros(num_features,1, \n                                dtype=torch.float32,device=dev)\n        self.bias=torch.zeros(1,dtype=torch.float32,device=dev)\n    def forward(self,x):\n        values=torch.add(torch.mm(x,self.weights),self.bias)\n        probs=self._sigmoid(values)\n        return probs       \n    def backward(self,probs,y):  \n        errors=y-probs.view(-1)\n        return errors            \n    def predict_labels(self,x):\n        probs=self.forward(x)\n        a=torch.ones(probs.size()[0],1)\n        b=torch.zeros(probs.size()[0],1)\n        labels=torch.where(probs>=.5,a,b)\n        return labels                \n    def acc(self,x,y):\n        labels=self.predict_labels(x).float()\n        accuracy=torch.sum(labels.view(-1)==y).float()\/y.size()[0]\n        return accuracy    \n    def _sigmoid(self,z):\n        return 1.\/(1.+torch.exp(-z))    \n    def _logit_cost(self,y,prob):\n        tmp1=torch.mm(-y.view(1,-1),torch.log(prob))\n        tmp2=torch.mm((1-y).view(1,-1),torch.log(1-prob))\n        return tmp1-tmp2\n    def train(self,x,y,epochs,learning_rate=.01):\n        for e in range(epochs):\n            probs=self.forward(x)\n            errors=self.backward(probs,y)\n            neg_grad=torch.mm(x.transpose(0,1),errors.view(-1,1))\n            self.weights+=learning_rate*neg_grad\n            self.bias+=learning_rate*torch.sum(errors)\n            print('Epoch: %03d'%(e+1),end=\"\")\n            print(' | Train accuracy: %.3f'%self.acc(x,y),end=\"\")\n            print(' | Cost: %.3f'%self._logit_cost(y,self.forward(x)))","2ada9368":"model=LogisticRegression(num_features=2)\nmodel.train(tX_train,ty_train,epochs=10,learning_rate=.02)\nprint('Weights: %s'%model.weights)\nprint('Bias: %s'%model.bias)","c1f5425d":"# evaluating\nacc_test=model.acc(tX_test,ty_test)\nprint('Test accuracy: %.2f%%'%(acc_test*100))","dba59871":"W,b=model.weights,model.bias\nx_min=-2; x_max=2\ny_min=((-(W[0]*x_min)-b[0])\/W[1])\ny_max=((-(W[0]*x_max)-b[0])\/W[1])\nfig,ax=pl.subplots(1,2,sharex=True,figsize=(11,3))\nax[0].plot([x_min,x_max],[y_min,y_max],c='red')\nax[1].plot([x_min,x_max],[y_min,y_max],c='red')\nax[0].scatter(X_train[:,0],X_train[:,1],\n              c=y_train,s=10,cmap=pl.cm.cool)\nax[1].scatter(X_test[:,0], X_test[:,1],\n              c=y_test,s=10,cmap=pl.cm.cool)\nax[0].grid(); ax[1].grid()","82ca244b":"random_seed=23; batch_size=128\ntrain=tmnist(root='data',train=True,download=True,\n            transform=transforms.ToTensor())\ntest=tmnist(root='data',train=False, \n            transform=transforms.ToTensor())\ntrain_loader=tdl(dataset=train,shuffle=True, \n                 batch_size=batch_size)\ntest_loader=tdl(dataset=test,shuffle=False, \n                batch_size=batch_size)\nfor images,labels in train_loader:  \n    print('Image dimensions: %s'%str(images.shape))\n    print('Label dimensions: %s'%str(labels.shape))\n    break","1415fb94":"learning_rate=.1; epochs=15\nnum_features=784; num_classes=10\nclass SoftmaxRegression(torch.nn.Module):\n    def __init__(self,num_features,num_classes):\n        super(SoftmaxRegression,self).__init__()\n        self.linear=torch.nn.Linear(num_features,num_classes)        \n        self.linear.weight.detach().zero_()\n        self.linear.bias.detach().zero_()     \n    def forward(self,x):\n        logits=self.linear(x)\n        probs=tnnf.softmax(logits,dim=1)\n        return logits,probs\nmodel=SoftmaxRegression(num_features=num_features,\n                        num_classes=num_classes)\nmodel.to(dev)\noptimizer=torch.optim.SGD(model.parameters(),\n                          lr=learning_rate) ","67e7fa9e":"def model_acc(model,data_loader,num_features):\n    correct_preds,num_examples=0,0    \n    for features,targets in data_loader:\n        features=features.view(-1,num_features).to(dev)\n        targets=targets.to(dev)\n        logits,probs=model(features)\n        _,pred_labels=torch.max(probs,1)\n        num_examples+=targets.size(0)\n        correct_preds+=(pred_labels==targets).sum()        \n    return correct_preds.float()\/num_examples*100","bc49d547":"for epoch in range(epochs):\n    for batch_ids,(features,targets) in enumerate(train_loader):        \n        features=features.view(-1,num_features).to(dev)\n        targets=targets.to(dev)\n        logits,probs=model(features)\n        cost=tnnf.cross_entropy(logits,targets)\n        optimizer.zero_grad(); cost.backward()\n        optimizer.step()\n        if not batch_ids%200:\n            print ('Epoch: %03d\/%03d | Batch %03d\/%03d | Cost: %.4f' \n                   %(epoch+1,epochs,batch_ids, \n                     len(train)\/\/batch_size,cost))           \n    with torch.set_grad_enabled(False):\n        print('Epoch: %03d\/%03d train accuracy: %.2f%%'%\\\n              (epoch+1,epochs,model_acc(model,train_loader,num_features)))","573b4522":"print('Test accuracy: %.2f%%'%(model_acc(model,test_loader,num_features)))","9893cfb3":"fpath='..\/input\/flower-color-images\/'\nf=h5py.File(fpath+'FlowerColorImages.h5','r')\nkeys=list(f.keys()); print(keys)\nX=np.array(f[keys[0]],dtype='float32')\/255\ny=np.array(f[keys[1]],dtype='int32')\nN=len(y); n=int(.2*N); batch_size=16\nshuffle_ids=np.arange(N)\nnp.random.RandomState(23).shuffle(shuffle_ids)\nX,y=X[shuffle_ids],y[shuffle_ids]\nX_test,X_train=X[:n],X[n:]\ny_test,y_train=y[:n],y[n:]\nX_train.shape,y_train.shape","54e9cf59":"class TData(tds):\n    def __init__(self,X,y):   \n        self.X=torch.tensor(X,dtype=torch.float32)\n        self.y=torch.tensor(y,dtype=torch.int32)\n    def __getitem__(self,index):\n        train_img,train_lbl=self.X[index],self.y[index]\n        return train_img,train_lbl\n    def __len__(self):\n        return self.y.shape[0]\ntrain=TData(X_train,y_train)\ntest=TData(X_test,y_test)\ntrain_loader=tdl(dataset=train,batch_size=batch_size,shuffle=True)\ntest_loader=tdl(dataset=test,batch_size=batch_size,shuffle=False)\nfor images,labels in train_loader:  \n    print('Image dimensions: %s'%str(images.shape))\n    print('Label dimensions: %s'%str(labels.shape))\n    break","8b2b9851":"learning_rate=.01; epochs=25\nnum_features=49152; num_classes=10\ntorch.manual_seed(random_seed)\nmodel=SoftmaxRegression(num_features=num_features,\n                         num_classes=num_classes)\n\nmodel.to(dev)\noptimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)","a994c8d9":"for epoch in range(epochs):\n    for batch_ids,(features,targets) in enumerate(train_loader):        \n        features=features.view(-1,num_features).to(dev)\n        targets=targets.to(dev)\n        logits,probs=model(features)\n        cost=tnnf.cross_entropy(logits,targets.long())\n        optimizer.zero_grad(); cost.backward()\n        optimizer.step()\n        if not batch_ids%10:\n            print ('Epoch: %03d\/%03d | Batch %03d\/%03d | Cost: %.4f' \n                   %(epoch+1,epochs,batch_ids, \n                     len(train)\/\/batch_size,cost))           \n    with torch.set_grad_enabled(False):\n        print('Epoch: %03d\/%03d train accuracy: %.2f%%'%\\\n              (epoch+1,epochs,model_acc(model,train_loader,num_features)))","cb3f5ce0":"print('Test accuracy: %.2f%%'%(model_acc(model,test_loader,num_features)))","04b1ab63":"Reading classics [Deep Learning Models](https:\/\/github.com\/rasbt\/deeplearning-models)\n## Basic Examples","2ac320e7":"## Softmax Regression","ee31c9ed":"## Applying to Color Images"}}