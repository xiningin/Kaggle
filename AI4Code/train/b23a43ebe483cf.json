{"cell_type":{"bb3d1bb1":"code","323369ee":"code","bd8cf56e":"code","c3b5dd34":"code","68477ca7":"code","fd7fc6ee":"code","ad28cccf":"code","52b5ab06":"code","fad42f92":"code","0fa067b3":"code","398b02d6":"code","b50e0f90":"code","e421fa1f":"code","050125dd":"code","5e3b78a3":"code","0beddaf3":"code","dd93e382":"code","00b138c3":"code","3bfd6701":"code","8ba8d384":"code","e5e08a03":"code","031cd275":"code","5d681b85":"code","cd2a47bb":"code","2869b709":"code","ae415df6":"code","ecaf5af3":"code","eeee7246":"code","29f033a0":"code","0307962c":"code","5f892578":"code","a021654b":"code","7d2471d8":"code","6db0c489":"code","13573a58":"code","c753c207":"code","fba1e50e":"code","214a9aec":"code","73ba40c1":"code","ab5fe17a":"code","b5cfbb9e":"code","51eac3f3":"code","bf9e3c86":"code","ed252c86":"code","11993b0d":"code","897442eb":"code","2df39676":"code","23b42447":"code","3c052dc3":"code","d90b20d5":"markdown","698c61e4":"markdown","eab669ea":"markdown","fc50044b":"markdown","9ce8068d":"markdown","34788ec1":"markdown","fe80f67a":"markdown","82a96ce6":"markdown","9af931b1":"markdown","2f9ad2e4":"markdown","a642e7cf":"markdown","d5b32345":"markdown","152e3672":"markdown","b20826c4":"markdown","b0083e31":"markdown","144aa4f3":"markdown","8cc0827c":"markdown","c202fcd5":"markdown","cfbbb0aa":"markdown","1fcbb990":"markdown","5737f670":"markdown","7d8fe0ee":"markdown","5c153615":"markdown","2d06c824":"markdown","7ce31684":"markdown","623dcb45":"markdown","6f923b12":"markdown","3ae56218":"markdown","a380a74d":"markdown","8667616f":"markdown","6b538377":"markdown"},"source":{"bb3d1bb1":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","323369ee":"df= pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head()\n","bd8cf56e":"df.shape","c3b5dd34":"df.info()","68477ca7":"df.describe()","fd7fc6ee":"features_with_na=[features for features in df.columns if df[features].isnull().sum()>1]\n## 2- step print the feature name and the percentage of missing values\nfor feature in features_with_na:\n    print(feature, np.round(df[feature].isnull().mean(), 4),  ' % missing values')","ad28cccf":"numerical_features = [feature for feature in df.columns if df[feature].dtypes != 'O']\nlen(numerical_features)","52b5ab06":"numerical_features","fad42f92":"numerical_with_nan=[feature for feature in df.columns if df[feature].isnull().sum()>1 and df[feature].dtypes!='O']\n\n## We will print the numerical nan variables and percentage of missing values\n\nfor feature in numerical_with_nan:\n    print(\"{}: {}% missing value\".format(feature,np.around(df[feature].isnull().mean(),4)))","0fa067b3":"for feature in numerical_with_nan:\n    ## We will replace by using median since there are outliers\n    median_value=df[feature].median()\n    \n    ## create a new feature to capture nan values\n    df[feature+'nan']=np.where(df[feature].isnull(),1,0)\n    df[feature].fillna(median_value,inplace=True)\n    \ndf[numerical_with_nan].isnull().sum()","398b02d6":"discrete_feature=[feature for feature in numerical_features if len(df[feature].unique())<25]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","b50e0f90":"discrete_feature","e421fa1f":"continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))","050125dd":"for feature in continuous_feature:\n    data=df.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","5e3b78a3":"fig, ax = plt.subplots(figsize=(10,8))\nsns.heatmap(df.corr(),annot=True,ax=ax)","0beddaf3":"sns.displot(x='age', hue='stroke', data=df, alpha=0.6)\nplt.show()","dd93e382":"stroke = df[df['stroke']==1]\nsns.displot(stroke.age, kind='kde')\nplt.show()","00b138c3":"sns.displot(stroke.age, kind='ecdf')\nplt.grid(True)\nplt.show()","3bfd6701":"categorical_variables=['gender','ever_married','work_type','Residence_type','smoking_status']\nfrom sklearn import preprocessing\n  \n# label_encoder object knows how to understand word labels.\nlabel_encoder = preprocessing.LabelEncoder()\nfor feature in categorical_variables:\n    df[feature]= label_encoder.fit_transform(df[feature])\n    df[feature].unique()\n    \n# Encode labels in column 'species'.\n\n  \ndf.head()","8ba8d384":"stroke = df[df['stroke']==1]","e5e08a03":"ranges = [0, 30, 40, 50, 60, 70, np.inf]\nlabels = ['0-30', '30-40', '40-50', '50-60', '60-70', '70+']\n\nstroke['age'] = pd.cut(stroke['age'], bins=ranges, labels=labels)\nstroke['age'].head()","031cd275":"sns.countplot(stroke.age)","5d681b85":"fig, ax = plt.subplots(figsize=(8, 5))\nsns.countplot(x='gender', hue='age', data=stroke, ax=ax)\n\n","cd2a47bb":"sns.countplot(stroke['age'])","2869b709":"for feature in continuous_feature:\n    data=df.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","ae415df6":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import  BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","ecaf5af3":"#Creating a copy\ndata= df","eeee7246":"data = data.drop(['id'],axis=1)\n","29f033a0":"data.head()","0307962c":"categorical_vars = ['gender','hypertension','heart_disease','ever_married','work_type','Residence_type','smoking_status','bminan']","5f892578":"columns = data.columns\ncolumns","a021654b":"continuous_vars= np.setdiff1d(columns, categorical_vars, assume_unique=False)","7d2471d8":"continuous_vars = np.setdiff1d(continuous_vars,['stroke'],assume_unique=False)","6db0c489":"continuous_vars","13573a58":"\nscaler = StandardScaler()\n\n# define the columns to be encoded and scaled\n\n\n# encoding the categorical columns\ndata = pd.get_dummies(data, columns = categorical_vars, drop_first = True)\n\nX = data.drop(['stroke'],axis=1)\ny = data[['stroke']]\n\ndata[continuous_vars] = scaler.fit_transform(X[continuous_vars])\n\n# defining the features and target\nX = data.drop(['stroke'],axis=1)\ny = data[['stroke']]\n\n","c753c207":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1)","fba1e50e":"lr = LogisticRegression(random_state=42)\n\nknn = KNeighborsClassifier()\npara_knn = {'n_neighbors':np.arange(1, 50)}\n\ngrid_knn = GridSearchCV(knn, param_grid=para_knn, cv=5)\n\ndt = DecisionTreeClassifier()\npara_dt = {'criterion':['gini','entropy'],'max_depth':np.arange(1, 50), 'min_samples_leaf':[1,2,4,5,10,20,30,40,80,100]}\ngrid_dt = GridSearchCV(dt, param_grid=para_dt, cv=5)\n\nrf = RandomForestClassifier()\n\n# Define the dictionary 'params_rf'\nparams_rf = {\n    'n_estimators':[100, 350, 500],\n    'min_samples_leaf':[2, 10, 30]\n}\ngrid_rf = GridSearchCV(rf, param_grid=params_rf, cv=5)","214a9aec":"dt = DecisionTreeClassifier(criterion='gini', max_depth=9, min_samples_leaf=10, random_state=42)\nknn = KNeighborsClassifier(n_neighbors=3)\nrf = RandomForestClassifier(n_estimators=500, min_samples_leaf=2, random_state=42)","73ba40c1":"# Define the list classifiers\nclassifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt), ('Random Forest', rf)]","ab5fe17a":"for clf_name, clf in classifiers:    \n \n    # Fit clf to the training set\n    clf.fit(X_train, y_train)    \n   \n    # Predict y_pred\n    y_pred = clf.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_pred, y_test) \n   \n    # Evaluate clf's accuracy on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy))","b5cfbb9e":"from sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(base_estimator=rf, n_estimators=100, random_state=1)\n\nada.fit(X_train, y_train)\n\ny_pred = ada.predict(X_test)\n\naccuracy_score(y_pred, y_test)","51eac3f3":"importances = pd.Series(data=rf.feature_importances_,\n                        index= X_train.columns)\n\n# Sort importances\nimportances_sorted = importances.sort_values()\n\n# Draw a horizontal barplot of importances_sorted\nplt.figure(figsize=(10, 10))\nimportances_sorted.plot(kind='bar',color='orange')\nplt.title('Features Importances')\nplt.show()","bf9e3c86":"from tensorflow.keras.layers import Dense,Dropout,Flatten\nfrom tensorflow.keras.layers import MaxPooling2D,GlobalAveragePooling2D,BatchNormalization,Activation\nfrom tensorflow import keras\nimport tensorflow as tf","ed252c86":"X_train.shape","11993b0d":"\nmodel = tf.keras.Sequential()\nmodel.add(Dense(1024, input_dim=17, activation= \"relu\"))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(512, activation= \"relu\"))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(128, activation= \"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32, activation= \"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.summary() #Print model Summary","897442eb":"model.compile(loss= \"binary_crossentropy\" , optimizer=\"adam\", metrics=[\"accuracy\"])","2df39676":"Performance = model.fit(X_train, y_train, validation_split =0.1,epochs=5)","23b42447":"model.evaluate(X_test,y_test)","3c052dc3":"my_dpi = 50 # dots per inch .. (resolution)\nplt.figure(figsize=(400\/my_dpi, 400\/my_dpi), dpi = my_dpi)\nplt.plot(Performance.history['accuracy'], label='train accuracy')\nplt.plot(Performance.history['val_accuracy'], label='val accuracy')\nplt.legend()\nplt.show()\nplt.savefig('AccVal_acc')","d90b20d5":"**Number of Numerical Variables**","698c61e4":"**NOTICE THAT BMI IS A NUMERICAL VARIABLE AND WE NEED TO REPLACE THE NaN VALUES**","eab669ea":"# THANK YOU , IF YOU LIKE THE NOTEBOOK PLEASE DO UP VOTE","fc50044b":"**Let Us Know if We Have any missing values**","9ce8068d":"# How will we proceed ?","34788ec1":"**Results against the Age**","fe80f67a":"# Including Required Packages ","82a96ce6":"**IMPORTING THE NECESSARY LIBRARIES**","9af931b1":"So we have found that missing values are presnet in the feature 'bmi'. We will have to take care of it otherwise it will cause problems","2f9ad2e4":"**DESCRIPTION OF THE DATASET**","a642e7cf":"**WE NOTICE THAT MALE HAVE A HIGHER TENDENCY TO HAVE HEART ATTACK**","d5b32345":"**So we see that the most important factor which leads to stroke is age, so it is advisable to the general people to take proper care of the aged people as much as they can and following are the few guidelines that help them.\nTrouble speaking and understanding what others are saying. You may experience confusion, slur your words or have difficulty understanding speech.\nParalysis or numbness of the face, arm or leg. You may develop sudden numbness, weakness or paralysis in your face, arm or leg. This often affects just one side of your body. Try to raise both your arms over your head at the same time. If one arm begins to fall, you may be having a stroke. Also, one side of your mouth may droop when you try to smile.\nProblems seeing in one or both eyes. You may suddenly have blurred or blackened vision in one or both eyes, or you may see double.\nHeadache. A sudden, severe headache, which may be accompanied by vomiting, dizziness or altered consciousness, may indicate that you're having a stroke.\nTrouble walking. You may stumble or lose your balance. You may also have sudden dizziness or a loss of coordination.**\n\n\n","152e3672":"# Inference","b20826c4":"Wow!! We got to know all of the features are numerical variables ! ","b0083e31":"1. **Understanding the Data**\n\n2. **EDA**\n\n3. **Model Building**\n\n4. **Model Performance**\n\n5. **Inference**\n","144aa4f3":"**READING THE DATA**","8cc0827c":"# **STROKE DATASET EDA & PREDICTION PERFORMANCE**","c202fcd5":"**WE SEE THAT LOGISTIC REGRESSION PERFORMS THE BEST**","cfbbb0aa":"So we know that there are 12 features that has been included in the dataset needed to determine Heart Attack","1fcbb990":"**WE SEE THAT AGES BETWEEN 50-60 ARE THE MOST PRONE TO HEART ATTACKS**","5737f670":"# NEURAL NETWORK APPROACH","7d8fe0ee":"# **Models**","5c153615":"**PREPARING THE DATASET FOR MODEL**","2d06c824":"**We need to know the number of discrete variables, Let us find it out !**","7ce31684":"# **EDA**","623dcb45":"# **UNDERSTANDING THE DATA**","6f923b12":"**Now let's deal with the Continuous Variables**","3ae56218":"![](https:\/\/www.heart.org\/-\/media\/images\/news\/2019\/october-2019\/1017strokeptsd_sc.jpg)","a380a74d":"A stroke occurs when the blood supply to part of your brain is interrupted or reduced, preventing brain tissue from getting oxygen and nutrients. Brain cells begin to die in minutes.\n\nA stroke is a medical emergency, and prompt treatment is crucial. Early action can reduce brain damage and other complications.\nStroke Statistics\nIn 2018, 1 in every 6 deaths from cardiovascular disease was due to stroke.1\nSomeone in the United States has a stroke every 40 seconds. Every 4 minutes, someone dies of stroke.2\nEvery year, more than 795,000 people in the United States have a stroke. About 610,000 of these are first or new strokes.2\nAbout 185,000 strokes\u2014nearly 1 of 4\u2014are in people who have had a previous stroke.2\nAbout 87% of all strokes are ischemic strokes, in which blood flow to the brain is blocked.2\nStroke-related costs in the United States came to nearly 46 billion dollars between 2014 and 2015.2 This total includes the cost of health care services, medicines to treat stroke, and missed days of work.\nStroke is a leading cause of serious long-term disability.2 Stroke reduces mobility in more than half of stroke survivors age 65 and over.2","8667616f":"# Overview","6b538377":"The accuracy of the following models are \n1. **Logistic Regression : 0.965**\n2. **K Nearest Neighbours : 0.963**\n3. **Classification Tree : 0.963**\n4. **Random Forest : 0.967**\n5. **Adaboost Classifier: 0.964**\n6. **ANN : 0.964**"}}