{"cell_type":{"04177460":"code","a720d179":"code","14442be7":"code","a86d8a05":"code","1d3fc6f8":"code","d1d43c2c":"code","b8e9a7a6":"code","46f4283d":"code","85bdf3ce":"code","6d310d81":"code","6a7b3f1d":"code","d8f64c33":"code","7ee28e45":"code","9ca7669e":"code","46e441de":"code","8b17310f":"code","482dea9f":"code","aae7d1e8":"code","cdee4a8b":"code","31c514c4":"code","1f9ff19f":"code","8004bec6":"code","b08b408f":"code","4017a441":"code","e305e4a3":"code","e2973e7c":"code","1b22e6fe":"code","ce8a4d7d":"code","881d6eac":"code","ee3d8af8":"code","9a42ee9a":"code","9fb00c78":"code","7a31c310":"code","60208f41":"code","9f44cb7c":"code","741cd97b":"code","3e42ba94":"code","046bec43":"code","a2bf5f88":"code","61c3ae96":"code","d0aaa835":"code","e566c409":"code","c039eadc":"markdown","68f0e17f":"markdown","2c5d6b13":"markdown","423eb4f4":"markdown","5a4d358e":"markdown","0ba8ec1e":"markdown","ed028037":"markdown","4ecbbd82":"markdown","fbaec0e7":"markdown","2b89e4b1":"markdown","3cdd6a90":"markdown","d1dab41b":"markdown","44759741":"markdown","45613d53":"markdown","d438932d":"markdown","6244c1ab":"markdown","96e483c0":"markdown","fd0f8ff3":"markdown","913448ca":"markdown","ab62849e":"markdown","f1f730ed":"markdown","cc4ddc63":"markdown","114caa9b":"markdown","1b2fb347":"markdown","668ba7fd":"markdown","e252a814":"markdown","508b9b34":"markdown","a34fab6a":"markdown","76711e32":"markdown"},"source":{"04177460":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skew\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","a720d179":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","14442be7":"train_df.head()","a86d8a05":"test_df.head()","1d3fc6f8":"print(train_df.info())\nprint(test_df.info())","d1d43c2c":"ids_test = test_df['Id']","b8e9a7a6":"train_df['SalePrice'].describe()","46f4283d":"f, ax = plt.subplots(figsize=(12, 9))\nsns.distplot(train_df['SalePrice']).set_title('Distribution of SalePrice')","85bdf3ce":"train_df['SalePrice'] = np.log1p(train_df['SalePrice'])","6d310d81":"f, ax = plt.subplots(figsize=(12, 9))\nsns.distplot(train_df['SalePrice']).set_title('Distribution of log[SalePrice]')","6a7b3f1d":"corrmatrix = train_df.corr() #Create correlation matrix\nf, ax = plt.subplots(figsize=(14, 10))\nsns.heatmap(corrmatrix, vmax=.8, square=True, cmap='BuPu')","d8f64c33":"pd.DataFrame(corrmatrix['SalePrice'].abs().sort_values(ascending=False))","7ee28e45":"#GrLivArea - high correlation\nwith sns.axes_style('white'):\n    sns.jointplot(x=train_df['GrLivArea'],y=train_df['SalePrice'], color='firebrick')","9ca7669e":"#GarageArea - high correlation\nwith sns.axes_style('white'):\n    sns.jointplot(y=train_df['SalePrice'], x=train_df['GarageArea'], color='firebrick')","46e441de":"#TotalBsmtSF - high correlation\nwith sns.axes_style('white'):\n    sns.jointplot(y=train_df['SalePrice'], x=train_df['TotalBsmtSF'], color='firebrick')","8b17310f":"#PoolArea - low correlation\nwith sns.axes_style('white'):\n    sns.jointplot(x=train_df['SalePrice'], y=train_df['PoolArea'], color='teal')","482dea9f":"highcorr = pd.DataFrame(corrmatrix.abs().unstack().transpose().sort_values(ascending=False).drop_duplicates())\nhighcorr.head(15)","aae7d1e8":"#Neighborhood - high correlation\nax=sns.catplot(x='Neighborhood', y='SalePrice', kind='boxen',data=train_df.sort_values('Neighborhood'),height=12,aspect=2)\nax.set_xticklabels(size=15,rotation=30)\nax.set_yticklabels(size=15,rotation=30)\nplt.xlabel('Neighborhood',size=25)\nplt.ylabel('SalePrice',size=25)\nplt.show()","cdee4a8b":"#ExterQual - high correlation\nax=sns.catplot(x='ExterQual', y='SalePrice',kind='boxen',data=train_df.sort_values('BldgType'),height=12,aspect=2)\nax.set_xticklabels(size=15,rotation=30)\nax.set_yticklabels(size=15,rotation=30)\nplt.xlabel('ExterQual',size=25)\nplt.ylabel('SalePrice',size=25)\nplt.show()","31c514c4":"#BsmtQual - high correlation\nax=sns.catplot(x='BsmtQual', y='SalePrice', kind='boxen',data=train_df.sort_values('BsmtQual'),height=12,aspect=2)\nax.set_xticklabels(size=15,rotation=30)\nax.set_yticklabels(size=15,rotation=30)\nplt.xlabel('BsmtQual',size=25)\nplt.ylabel('SalePrice',size=25)","1f9ff19f":"#Heating - low correlation\nax=sns.catplot(x='Heating', y='SalePrice', kind='boxen',data=train_df.sort_values('Heating'),height=12,aspect=2)\nax.set_xticklabels(size=15,rotation=30)\nax.set_yticklabels(size=15,rotation=30)\nplt.xlabel('Heating',size=25)\nplt.ylabel('SalePrice',size=25)","8004bec6":"#GrLivArea vs SalePrice - before\nwith sns.axes_style('white'):\n    sns.jointplot(x=train_df['GrLivArea'],y=train_df['SalePrice'], color='firebrick')","b08b408f":"train_df = train_df[train_df['GrLivArea'] < 4000]","4017a441":"#GrLivArea vs SalePrice - after\nwith sns.axes_style('white'):\n    sns.jointplot(x=train_df['GrLivArea'],y=train_df['SalePrice'], color='firebrick')","e305e4a3":"df = pd.concat([train_df, test_df])","e2973e7c":"percent_null = pd.DataFrame((df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False))\npercent_null.head(20)","1b22e6fe":"to_drop = ['Id', 'PoolQC', 'MiscVal', 'MiscFeature', 'Alley', 'LandContour', 'Utilities', 'FireplaceQu', 'GarageCond', 'Fence']\ndf.drop(to_drop, axis=1, inplace=True)","ce8a4d7d":"#Categorical features where null indicates the feature is NOT present - will be replaced with the str 'None'\ncat_none = ['MasVnrType', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'GarageType', 'GarageFinish']\nfor col in cat_none:\n    df[col].fillna('None', inplace=True)\n    \n#Categorical features which must be replaced by mode (nulls don't indicate feature is absent)\ncat_mode = ['MSZoning', 'Exterior1st', 'Exterior2nd', 'Electrical', 'KitchenQual', 'SaleType', 'Functional', 'GarageQual']\nfor col in cat_mode:\n    df[col].fillna(df[col].mode()[0], inplace=True)\n    \n#Numerical features where null indicates feature is not present - will be replaced by 0\nnum_zero = ['MasVnrArea', 'BsmtHalfBath', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'TotalBsmtSF', 'BsmtUnfSF', 'GarageYrBlt', 'GarageArea', 'GarageCars']\nfor col in num_zero:\n    df[col].fillna(0, inplace=True)\n    \n#Continuous numerical feature - nulls to be replaced by median (Grouped by neighborhood as the LotFrontage is expected to be similar in a particular neighborhood)\ndf[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n    ","881d6eac":"#Convert discrete numerical features into categorical\nnumerical_to_cat=['BedroomAbvGr', 'Fireplaces', 'FullBath', 'GarageCars', 'HalfBath', 'KitchenAbvGr', 'OverallQual']\n\ndf[numerical_to_cat] = df[numerical_to_cat].apply(lambda x: x.astype(\"str\"))","ee3d8af8":"#Correct feature skewness - only log those features which show a decrease in skewness after the transformation\nnumeric_features = df.dtypes[df.dtypes != 'object'].index.drop('SalePrice')\nskew_b = df[numeric_features].apply(lambda x: skew(x.dropna()))\nlog = np.log1p(df[numeric_features])\nskew_a = log[numeric_features].apply(lambda x: skew(x.dropna()))\nskew_diff = (abs(skew_b)-abs(skew_a)).sort_values(ascending=False)\ndf[skew_diff[skew_diff > 0].index] = np.log1p(df[skew_diff[skew_diff > 0].index])","9a42ee9a":"#Encode categorical variables\ndf = pd.get_dummies(df, drop_first=True)","9fb00c78":"X_train = df.iloc[:1456].drop('SalePrice', axis=1)\nX_test = df.iloc[1456:].drop('SalePrice', axis=1)\ny_train = df['SalePrice'].dropna().values","7a31c310":"print(X_train.info())\nprint(X_test.info())","60208f41":"#Define function to extract best train RMSE from GridSearchCV\ndef best_rmse(grid):\n    \n    best_score = np.sqrt(-grid.best_score_)\n    print(best_score)    \n    print(grid.best_estimator_)\n    \n    return best_score","9f44cb7c":"#Linear Regression\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\n\nlm = LinearRegression()\nparameters_lm = {'fit_intercept':[True,False]}\ngrid_lm = GridSearchCV(lm, parameters_lm, cv=5, verbose=1 , scoring ='neg_mean_squared_error')\ngrid_lm.fit(X_train, y_train)\nscore_lm = best_rmse(grid_lm)","741cd97b":"#Ridge\n\nfrom sklearn.linear_model import Ridge\n\nridge = Ridge()\nparameters_ridge = {'alpha': [4, 4.1, 4.2, 4.3, 4.4, 4.5], 'tol': [0.001, 0.01, 0.1]}\ngrid_ridge = GridSearchCV(ridge, parameters_ridge, cv=5, verbose=1, scoring='neg_mean_squared_error')\ngrid_ridge.fit(X_train, y_train)\nscore_ridge = best_rmse(grid_ridge)","3e42ba94":"#Lasso\n\nfrom sklearn.linear_model import Lasso\n\nlasso = Lasso()\nparameters_lasso = {'alpha': [1e-4, 0.001, 0.01, 0.1, 0.5, 1], 'tol':[1e-06, 1e-05, 1e-04, 1e-03, 0.01, 0.1]}\ngrid_lasso = GridSearchCV(lasso, parameters_lasso, cv=5, verbose=1, scoring='neg_mean_squared_error')\ngrid_lasso.fit(X_train, y_train)\nscore_lasso = best_rmse(grid_lasso)","046bec43":"#RandomForest\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor()\nparameters_rf = {'min_samples_split' : [2, 3, 4], 'n_estimators' : [50, 100, 500]}\ngrid_rf = GridSearchCV(rf, parameters_rf, cv=5, verbose=1, scoring='neg_mean_squared_error')\ngrid_rf.fit(X_train, y_train)\nscore_rf = best_rmse(grid_rf)","a2bf5f88":"rmse_x = ['linear', 'ridge', 'lasso', 'randomforest']\nrmse_y = [score_lm, score_ridge, score_lasso, score_rf]\nsns.set(style='whitegrid')\nsns.barplot(x=rmse_x, y=rmse_y)","61c3ae96":"pred_lasso = np.expm1(grid_lasso.predict(X_test))\npred_ridge = np.expm1(grid_ridge.predict(X_test))\n\npred_combined = (pred_lasso + pred_ridge)\/2","d0aaa835":"#Creating dataframe of submission data\nsubmission = pd.DataFrame()\nsubmission['Id'] = ids_test.values\nsubmission['SalePrice'] = pred_combined","e566c409":"#Save the output as a csv\nsubmission.to_csv('submission_final_edsateam18.csv', index=False)","c039eadc":"**Based on the train RMSE of the models, the top performing two (lasso and ridge) were used to make predictions for the test data and the average of these used for the final submission**","68f0e17f":"We can also look at the values of the correlation coefficients","2c5d6b13":"**Finally, we split the data back into train and test sets**","423eb4f4":"# Overview","5a4d358e":"**...And do one last check to make sure our datasets are in order**","0ba8ec1e":"**This notebook presents the steps followed in creating our team's submission for the House Prices: Advanced Regression Techniques competiton, which we completed as part of an assignment**\n\n![](https:\/\/cdn.pixabay.com\/photo\/2015\/10\/26\/21\/11\/houses-1007932_960_720.jpg)\n*Have you ever wondered what determines a house's sale price? Join us as we attempt to find out and build a predictive model for house prices based on the Ames dataset!*\n\n**Sections contained within the notebook are as follows:**\n\n**1) Exploratory Data Analysis**\n* **Importing the data**\n* **The target variable: SalePrice**\n* **Relationship of numerical features to SalePrice**\n* **Relationship of categorical features to SalePrice**\n\n**2) Data Cleaning**\n* **Removal of outliers**\n* **Dealing with null values**\n* **Preparing data for model fitting**\n\n**3) Regression Models and Prediction of SalePrice**\n* **Linear, Ridge, Lasso and RandomForest**","ed028037":"**For further data manipulation, the test and train datasets are combined to ensure uniform changes to both**","4ecbbd82":"Some variables highly correlated with SalePrice are 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea' and 'TotalBsmtSF'. Many of these make intuitive sense. For example, we would expect houses with higher overall quality and size to sell for higher. For now, we take note of variables that show very low correlations, but will only decide which to remove from the dataset after further analysis.\n\n**Further investigating the relationship between SalePrice and continuous numerical variables can be done using scatter plots**\n\nNote: Only selected plots are shown below to illustrate the point (3 variables with high correlation to SalePrice and 1 with low correlation)","fbaec0e7":"# 3) Regression Models and Prediction of SalePrice\n\n**Four regression models were evaluated:**\n* Linear regression\n* Ridge\n* Lasso\n* RandomForest\n\n**Hyperparameter optimization was performed using GridSearchCV** (note in some cases the value ranges showed here for each model are those after a few rounds of optimization)\n\n**Performance of models was evaluated using the root mean square error (RMSE) for the training set,** as this is comparable to the scoring metric used for the competition (root mean square log error)","2b89e4b1":"**As a first step, we look at the summary statistics**","3cdd6a90":"The distribution plot after log transformation shows that the target variable is now approximately normally distributed","d1dab41b":"## Relationship of categorical variables to SalePrice\n\n**Similarly to scatter plots for numerical variables, box plots provide a way to visualize the relationship between the categorical variables and SalePrice** \n\nNote: Only selected plots are shown below to illustrate the point (3 variables with high correlation to SalePrice and 1 with low correlation)","44759741":"## Relationship of numerical features to SalePrice","45613d53":"## The target variable: SalePrice","d438932d":"**Remaining null values were imputed as follows:**\n\n**Categorical Features:**\n\n* If a null indicates that the feature is absent, it is replaced by 'None'\n* If a null does not indicate such, it is replaced by the most common value (mode)\n\n**Numerical Features:**\n\n* If a null indicates that a feature is absent, it is replaced by 0\n* If a null does not indicate such, it is replaced by the median (chosen as opposed to the mean as it is not sensitive to outliers)","6244c1ab":"## Preparing data for model fitting\n\n**The data needs to be in the proper format for fitting regression models. The following remains to be done:**\n* Correcting feature skewness\n* Converting discrete numerical features to categorical\n* Encoding categorical variables","96e483c0":"**From this first look at the dataset, we can see the following:**\n* The dependent variable we are trying to predict is SalePrice (absent from test set)\n* There are 79 features (independent variables) given to use for building the model, plus Id columns for each dataset\n* Number of **categorical** features = **43**\n* Number of **numerical** features = **36**\n* Full descriptions of each feature are given in the text file provided for the competition\n\n**We store the Ids of the test dataset for later in order to compile the submission file:**","fd0f8ff3":"**Next, we assess the distribution of SalePrice to check normality**\n\nFrom the distribution plot below, it is clear that the variable is skewed to the right. We will correct the skewness by log transforming the variable","913448ca":"**To get a broad overview of relationships between variables, a heatmap of the correlations is plotted**\n\nThe colour of each square represents the correlation coefficient between a variable on the vertical axis and its corresponding variable on the horizontal axis. This enables us to visually identify high and low correlations","ab62849e":"## Library Imports ","f1f730ed":"Although there are no definitive rules for going about the removal of outliers, they can lead to overfitting\n\n\nA conservative approach was taken to remove outliers from only the target variable, as removal of too much data during initial model tests showed poorer performance. We decided to remove entries based on their position when plotting SalePrice against its most correlated numerical variables (see charts above). In this case we removed entries with 'GrLivArea' greater than 4000 square feet. This decision was backed up by [information about the dataset](https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf) from the authors, who recommend the removal of these points.","cc4ddc63":"# 2) Data Cleaning","114caa9b":"## Dealing with null values\n\n**The percentage of null values for each feature is calculated below**","1b2fb347":"**Comparison of RMSE of the 4 models**","668ba7fd":"## Removal of outliers","e252a814":"## Importing the data","508b9b34":"**Based on this, the following will be performed:**\n\n* Features with a high percentage of nulls will be removed \n* Remaining nulls will be replaced, based on feature characteristics\n\n**We decided to remove columns only if they had 2 or more of the following undesirable characteristics:**\n1. Low correlation to SalePrice\n2. High correlation with another variable in the dataset\n3. High percentage of null values\n","a34fab6a":"# 1) Exploratory Data Analysis","76711e32":"**Correlations between independent variables should also be investigated to ensure that redundant variables are removed**"}}