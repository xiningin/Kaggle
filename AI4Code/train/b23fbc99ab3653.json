{"cell_type":{"342c9153":"code","e6582c84":"code","3d9f01a4":"code","04fb6d66":"code","5a843683":"code","c8221e63":"code","f370aeb5":"code","6ba69012":"code","4d4b2b6b":"code","f4e80308":"code","c58a4484":"code","931d7960":"code","15f85f14":"code","0ea693a9":"code","0e7393db":"code","d0c1098c":"code","5052754d":"code","c7079c73":"markdown","ef0de0b9":"markdown"},"source":{"342c9153":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e6582c84":"import pandas as pd\nimport numpy as np \nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.metrics import accuracy_score\nimport time\n#device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","3d9f01a4":"device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","04fb6d66":"\n\ndef preprocessing(word):\n   # replace digits with no space\n     word = re.sub(r\"\\d\", '', word)\n   # Replace all runs of whitespaces with no space\n     word = re.sub(r\"\\s+\", '', word)\n     return word\n\nlines=[]\nwith open('\/kaggle\/input\/alicetxt\/alice.txt','r') as f:\n    for line in f:\n        line=line.lower().split()\n    \n        for i in line:\n            i=re.findall(r\"[\\w]+|[.,!?;]\", i) #we separate punctuation marks from words\n            lines.append(i)\n\ntext=np.concatenate(np.array(lines,dtype=object))","5a843683":"for i,s in enumerate(text):\n    text[i]=preprocessing(s)","c8221e63":"word_list=[]\nfor i in text:\n    if i not in word_list:\n        word_list.append(i)\n        \none_hot_dict={w:i+1 for i,w in enumerate(word_list) }","f370aeb5":"for i,s in enumerate(text):\n    text[i]=one_hot_dict[s]","6ba69012":"text[:300] #text list preview","4d4b2b6b":"#model hyper parameters\nbatch_size=32\ntimestep=30 #each time steps occuring 30 words len\n\nvocab_size=len(one_hot_dict)+1 #extra 1 for padding\nembed_size=128 #Input features to the LSTM\nhidden_size=512  #Number of LSTM units\n\nrep_tensor=torch.LongTensor(text.astype('int')) #text list converting to  tensor \nnum_batch=rep_tensor.shape[0]\/\/batch_size #net number of batches\nrep_tensor=rep_tensor[:num_batch*batch_size] #rep tensor \nrep_tensor=rep_tensor.view(32,-1) \n\nnum_batches=rep_tensor.shape[1]\/\/timestep\nprint(num_batches)","f4e80308":"rep_tensor #preview","c58a4484":"class textGenerator(nn.Module):\n    def __init__(self,vocab_size,embed_size,hidden_size):\n        super(textGenerator,self).__init__()\n\n        self.embed=nn.Embedding(vocab_size,embed_size)\n\n        self.fc=nn.Linear(hidden_size,vocab_size)\n\n        self.lstm=nn.LSTM(input_size=embed_size,\n                          hidden_size=hidden_size,\n                          num_layers=1,\n                          batch_first=True)\n        \n        self.drop=nn.Dropout(0.3)\n\n        self.relu=nn.ReLU()\n\n    \n    def forward(self,x):\n        input=x.clone()\n        \n        # Perform Word Embedding \n        x=self.embed(x)\n        #x = x.view(batch_size,timesteps,embed_size)\n        x,_=self.lstm(x)\n        # (batch_size*timesteps, hidden_size)\n        x=x.contiguous().view(-1, hidden_size)\n        x=self.drop(x)\n        out=self.fc(x)\n        #Decode hidden states of all time steps\n        out=out.view(input.shape[0],input.shape[1],vocab_size)\n        \n        return out[:,-1]\n    \n    \n    ","931d7960":"model=textGenerator(vocab_size,embed_size,hidden_size)\noptimizer=torch.optim.Adam(model.parameters(),lr=0.001)\ncriterion=nn.CrossEntropyLoss()\n\ntrain_loss=[]\n\nmodel.train()\n\nfor epoch in range(10):\n    t0 = time.time()\n    train_l=0\n\n    for i in range(0 ,rep_tensor.shape[1]-timestep):\n    \n        inputs=rep_tensor[:,i:i+timestep]\n        labels=rep_tensor[:,i+timestep]\n\n        outputs=model(inputs)\n        loss = criterion(outputs, labels.reshape(-1))\n        \n        train_l+=loss.item()\n        \n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        acc=accuracy_score(torch.argmax(outputs,dim=1),labels)\n        \n    print('epoch {:} acc {:.3f},  seconds {:.2f}'.format(epoch,acc,(time.time() - t0)\/60))","15f85f14":"#torch.save(model.state_dict(), '\/kaggle\/working\/textGen.pt') #save to model ","0ea693a9":"#model.load_state_dict(torch.load('\/kaggle\/working\/textGen.pt'))#load to model \n","0e7393db":"dum_text=np.concatenate(np.array(lines[-10:],dtype=object))  # we got the last 10 words \nfor i,s in  enumerate(dum_text):\n    dum_text[i]=one_hot_dict[preprocessing(s)] #and preprocessing\n\ndum_text","d0c1098c":"model.eval()\nwith torch.no_grad():\n  with open('\/kaggle\/working\/result.txt','w'):\n    input=torch.tensor(dum_text.astype('int').reshape(1,-1))\n    for i in range(1000):\n\n        output=model(input)\n        output_item=torch.argmax(output,dim=1)\n        output=torch.cat((input.reshape(1,-1),output_item.reshape(1,-1) ),1).reshape(1,-1)\n        input=output.clone()\n    listt=[]\n    for i in output[0]:\n      listt.append(word_list[i-1]+' ')\n    result=''.join(listt)","5052754d":"result","c7079c73":"### conclusion\nAs a result, we trained the model we trained using the alice.txt data in the code blocks above, and then we generated a new story by using the last 10 words of alice.txt.","ef0de0b9":"\nmodel=textGenerator(vocab_size,embed_size,hidden_size)\noptimizer=torch.optim.Adam(model.parameters(),lr=0.1)\ncriterion=nn.CrossEntropyLoss()\n\ntrain_loss=[]\ntrain_l=0\n\nmodel.train()\n\nfor epoch in range(10):\n    t0 = time.time()\n    train_loss=0\n    train_preds=[]\n    train_labels=[]\n    for i in range(0 ,rep_tensor.shape[1]-timestep):\n        model.zero_grad()\n\n        train_input=rep_tensor[:,i:i+timestep]\n        train_label=rep_tensor[:,i+timestep]\n\n        train_pred=model(train_input)\n        loss = criterion(train_pred, train_label.reshape(-1))\n        \n        train_loss+=loss.item()\n        \n        loss.backward()\n        optimizer.step()\n        \n        train_labels.append(train_label.cpu().detach())\n        train_preds.append(train_pred.cpu().detach())\n    p=torch.cat([torch.argmax(train_pred,dim=1) for i in train_preds]).reshape(-1,1)   \n    l=torch.cat(train_labels).reshape(-1,1)\n    acc=accuracy_score(p,l)\n    #acc=accuracy_score(torch.argmax(train_preds,dim=1),train_labels)    \n    print('epoch {:} acc {:.4f},  seconds {:.2f}'.format(epoch,acc,(time.time() - t0)\/60))\n        \n\n    "}}