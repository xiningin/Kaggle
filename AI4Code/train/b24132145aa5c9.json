{"cell_type":{"1bc397f0":"code","ce603e99":"code","7cbceec2":"code","a50c8c48":"code","ddc3a102":"code","9b4e7b68":"code","0607648b":"code","875a2865":"code","e8205f47":"code","b48cfcab":"code","5c5f2800":"code","fd7aad1c":"code","0f1b479c":"code","826b85be":"code","d3f7f7da":"code","cc8edfcc":"code","1c6444d9":"code","8c68c851":"code","73b2d786":"code","6fd7d40a":"code","45d3c84c":"code","ad566176":"code","f7f5d0d2":"code","182a851d":"code","0878e133":"code","4671a00d":"code","833d82f8":"code","96e17f7d":"markdown","aebda06e":"markdown","66e93299":"markdown","b273eaad":"markdown","49111105":"markdown","634504e4":"markdown","f7333d99":"markdown","6c9ab0a1":"markdown","32bf3836":"markdown","37987dc6":"markdown","9c86174f":"markdown","d7a9b0de":"markdown","912f7b1e":"markdown","ed939f71":"markdown","aca53533":"markdown","377aaa2e":"markdown","6542ef51":"markdown","60e0543b":"markdown","b6a2f7e4":"markdown"},"source":{"1bc397f0":"import pandas as pd\ndf=pd.read_csv(\"\/kaggle\/input\/covid19-research-preprint-data\/COVID-19-Preprint-Data_ver2.csv\")","ce603e99":"\nprint(df.columns)\ndf.head()","7cbceec2":"from datetime import datetime\ndf[\"day\"]=df[\"Date of Upload\"].apply(lambda x: int(datetime.strptime(x,'%Y-%m-%d').day))\ndf[\"month\"]=df[\"Date of Upload\"].apply(lambda x:int( datetime.strptime(x,'%Y-%m-%d').month))\ndf[\"year\"]=df[\"Date of Upload\"].apply(lambda x: int( datetime.strptime(x,'%Y-%m-%d').year))\ndf[\"day_in_year\"]=df[\"Date of Upload\"].apply(lambda x:int( datetime.strptime(x,'%Y-%m-%d').timetuple().tm_yday))\n","a50c8c48":"df.drop([\"Preprint Link\",\"DOI\",\"Date of Upload\"],axis=1,inplace=True)","ddc3a102":"import altair_render_script\nimport altair as alt\nalt.Chart(df.groupby([\"day_in_year\"]).count().reset_index()).mark_point().encode(\n    x='day_in_year',\n    y='Abstract',\n    tooltip=[\"Abstract\",\"day_in_year\"],\n    size=\"Abstract\"\n).interactive()\n\n","9b4e7b68":"df[df[\"day_in_year\"]==137]","0607648b":"\nalt.Chart(df[df[\"year\"]==2020].groupby([\"month\"]).count().reset_index()).mark_area(\n    line={'color':'darkblue'},\n    color=alt.Gradient(\n        gradient='linear',\n        stops=[alt.GradientStop(color='white', offset=0),\n               alt.GradientStop(color='blue', offset=1)],\n        x1=1,\n        x2=1,\n        y1=1,\n        y2=0\n\n    )\n).encode(\n    alt.X('month'),\n    alt.Y('Abstract',title=\"Abstract Count published\"),\n    tooltip=[\"month\",\"Abstract\"]\n).interactive()\n","875a2865":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nstop_words=set(stopwords.words('english'))\ndef removeSW(x):\n    x=x.lower()\n    word_tokens = word_tokenize(x) \n    \n    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n    return \" \".join(filtered_sentence)\n\n\ndf[\"Abstract\"]=df[\"Abstract\"].apply(removeSW)\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef get_top_n_words(corpus, n=None):\n  \n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","e8205f47":"unigrams=get_top_n_words(df[\"Abstract\"],20)\nunigrams_title=get_top_n_words(df[\"Title of preprint\"],20)","b48cfcab":"unigrams","5c5f2800":"unigrams_title","fd7aad1c":"d={\"word\":[],\"count\":[],\"type\":[]}\nfor i in unigrams:\n    d[\"word\"].append(i[0])\n    d[\"count\"].append(i[1])\n    d[\"type\"].append(\"Abstract\")\nfor i in unigrams_title:\n    d[\"word\"].append(i[0])\n    d[\"count\"].append(i[1])\n    d[\"type\"].append(\"Title\")\ncount_df=pd.DataFrame(d)\n","0f1b479c":"\nsource = count_df\n\nalt.Chart(source).mark_bar().encode(\n    tooltip=[\"word\",\"count\"],\n    column='type',\n    x='word',\n    y='count',\n    color='type'\n)","826b85be":"def get_top_gram(corpus,grams, n=None):\n  \n    vec = CountVectorizer(ngram_range=grams,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","d3f7f7da":"bigrams=get_top_gram(df[\"Abstract\"],(2,2),20)\nbigrams_title=get_top_gram(df[\"Title of preprint\"],(2,2),20)\nd={\"word\":[],\"count\":[],\"type\":[]}\nfor i in bigrams:\n    d[\"word\"].append(i[0])\n    d[\"count\"].append(i[1])\n    d[\"type\"].append(\"Abstract\")\nfor i in bigrams_title:\n    d[\"word\"].append(i[0])\n    d[\"count\"].append(i[1])\n    d[\"type\"].append(\"Title\")\ncount_df=pd.DataFrame(d)\n\nsource = count_df\n\nalt.Chart(source).mark_bar().encode(\n    tooltip=[\"word\",\"count\"],\n    column='type',\n    x='word',\n    y='count',\n    color='type'\n)\n","cc8edfcc":"fivegrams=get_top_gram(df[\"Abstract\"],(5,5),20)\nfivegrams_title=get_top_gram(df[\"Title of preprint\"],(5,5),20)\nd={\"word\":[],\"count\":[],\"type\":[]}\nfor i in fivegrams:\n    d[\"word\"].append(i[0])\n    d[\"count\"].append(i[1])\n    d[\"type\"].append(\"Abstract\")\nfor i in fivegrams_title:\n    d[\"word\"].append(i[0])\n    d[\"count\"].append(i[1])\n    d[\"type\"].append(\"Title\")\ncount_df=pd.DataFrame(d)\n\nsource = count_df\n\nalt.Chart(source).mark_bar().encode(\n    tooltip=[\"word\",\"count\"],\n    column='type',\n    x='word',\n    y='count',\n    color='type'\n)\n","1c6444d9":"import json\nmyset=set()\nmylist=[]\nfor i in df[\"Author(s) Institutions\"].index:\n    l=set(json.loads(df.loc[i,\"Author(s) Institutions\"]).keys())\n    for j in l:\n        myset.add(j)\n        mylist.append(j)\n","8c68c851":"len(myset)","73b2d786":"len(mylist)","6fd7d40a":"def CountFrequency(my_list): \n      \n  \n   count = {} \n   for i in my_list: \n       count[i] = count.get(i, 0) + 1\n       \n   \n    \n   return count \nfrequency=CountFrequency(mylist)\n  \nd={\"UNI\":[],\"Publishes\":[]}\nfor i in frequency.keys():\n    d[\"UNI\"].append(i)\n    d[\"Publishes\"].append(frequency[i])\n\nuni_df=pd.DataFrame(d)\n\nuni_df=uni_df.sort_values(by=[\"Publishes\"],ascending=False)\nfor i in uni_df.index:\n    if len(uni_df.loc[i].UNI)<4:\n        uni_df.drop(i,axis=0,inplace=True)","45d3c84c":"source = uni_df.iloc[:50,:]\n\nalt.Chart(source).mark_bar().encode(\n    x='Publishes',\n    y=\"UNI\",\n    tooltip=[\"Publishes\"]\n).properties(height=700)","ad566176":"for i in frequency.keys():\n   frequency[i]=[frequency[i],0]\n","f7f5d0d2":"df[\"Author(s) Institutions\"].head()","182a851d":"\nmylist=[]\nfor i in df[\"Author(s) Institutions\"].index:\n    l=set(json.loads(df.loc[i,\"Author(s) Institutions\"]).keys())\n    for j in l:\n        frequency[j][1]=frequency[j][1]+json.loads(df.loc[i,\"Author(s) Institutions\"])[j]","0878e133":" \nd={\"UNI\":[],\"AuthorCount\":[]}\nfor i in frequency.keys():\n    d[\"UNI\"].append(i)\n    d[\"AuthorCount\"].append(frequency[i][1])\n\nuni_df=pd.DataFrame(d)\n\nuni_df=uni_df.sort_values(by=[\"AuthorCount\"],ascending=False)\nfor i in uni_df.index:\n    if len(uni_df.loc[i].UNI)<4:\n        uni_df.drop(i,axis=0,inplace=True)","4671a00d":"uni_df","833d82f8":"source = uni_df.iloc[:50,:]\n\nalt.Chart(source).mark_bar().encode(\n    x='AuthorCount',\n    y=\"UNI\",\n    tooltip=[\"AuthorCount\",\"UNI\"]\n).properties(height=700)","96e17f7d":"<h4>Another function phew!!!, well this is quite simple we use the list and count how many times each intistute has appeared and then a dictionary example ({\"university\":10})<\/h4>\n<p> basically what we want to do is that the frequency represents how many times the institution has published a paper since its occurence in the dataframe in each row is a count for its publications!!!<\/p>","aebda06e":"<h4> We now again use json to get the values of author institution column but we will extract the names of the Uni as a key for our dictionary and will upadte the auhors value to it<\/h4>","66e93299":"<h3> Welcome to this notebook, Here lets get a glimpse of the data and list the columns<\/h3>\n","b273eaad":"<h4>We did the same procdeure and created a new dataframe for university and their authors count!<\/h4>","49111105":"<h4> Now we modify the function before and create a new function called<b> get_top_gram<\/b> It takes one more argument called grams, which will be the number of grams we want, bigram trigram etc and after that I will plot the bigram as similarly as the unigrams, and then I plot the pentagrams!!<h4>","634504e4":"<h4> Voila! the graph above shows how many publications done by the institutions(here I plotted only the top 50 )<p>HOVER!!! and we find out <b> Oxford University<\/b> has published 61 times!!! \n<\/p>\n<p>Now, lets check for the authors , below I used the previous dictionary of frequencies we outputed and change the values to list of values example\n<p>{\"uni\":1}===>{\"uni\":[1,0]}<\/p>\nhere we add the 0 to all as we are going to store the authors count for this value!!!\n<\/p>\n<\/h4>","f7333d99":"<h1>Thank You!<\/h1>","6c9ab0a1":"<h4> We plot!!<\/h4>","32bf3836":"<h1> Please do Upvote this kernel!!<\/h1>","37987dc6":"<h4> Ah of course the first thing I always would do is to split the Datetime into day,month,year and day in year(like 1st febuary is the 32nd day of the year , this helps in plotting :D)<\/h4>","9c86174f":"<h4>Lets get our unigrams of the abstract column and the title column using this function <\/h4>","d7a9b0de":"<h4>See the name of the institue is given and also the value with it are the authors number! which we did not consider above , well now we are!!<\/h4>","912f7b1e":"<h4> Now lets get onto the authors and the Universities!!<\/h4>\n<p> Here I use json to extract the dictionary present in the column <b>Author(s) Institutions<\/b><\/p>\n<p> after that I show the length of the set and list created, basically the set contains the number of unique Institutions and the list contains everytime the Institution is mentioned in the dataframe, we will use it ahead<\/p>\n","ed939f71":"<h4> Our first plot in the notebook is a simple one and I have done it using Altair, here I plot the number of abstracts(or generally papers ) submitted per day \n<ul><li> -X= day in the year<\/li>\n<li> -Y= the count of the number of abstracts(research papers) published in that particular day<\/li><\/ul>\n<h4> the below plot is a bubble plot which increases its size as the number of abstracts per day, you can hover for more info<\/h4>","aca53533":"<h4> we can see above that after a certain day (about 137th day of the year) which is 16th May 2020<\/h4>\n<p><h4>Below we are going to plot the number of abstracts(research papers) published in the given months of 2020, we first filter out only months of 2020<\/h4><\/p>","377aaa2e":"<h4>Over here we will import nltk stopwords and remove the stopwords from the abstract and I have made a function for it, next I have done is as follows:\n<ul><li> I creat a function(get_top_n_words) for doing all in one <\/li>\n<li>it first removes stopwords using Countvectorerizer<\/li>\n<li>our function which takes n as input is the top n word frequencies we want<\/li>\n<li>using vecotor transform and bag of words we get the frequencies and the words of the whole <b>abstract column<\/b> which we will input as the agrument <b>corpus<\/b><\/li>\n<li> we then sort it according to the most number of frequencies and return the top n words and their frequencies in a <b>list of tuples<\/b><\/li>\n<\/ul>\n<\/h4>","6542ef51":"<h4> Finally we plot it!!, and hovering over the data we can see<b> Icahn School of Medicine<\/b> has the highest amount of authors who have published for covid-19 around 470<\/h4>","60e0543b":"<h4> I am going to drop the columns below as I do not aim to use it in my EDA<\/h4>","b6a2f7e4":"<h4> Yes! I know both the list of tuples looks very much same since many of the title and abstract words are column, now lets get down in making a seperate dataframe and join this 2 list of tuples as rows and their values as column ,also a column for <b>type<\/b> indicates that wether it is abstract or title<\/h4>"}}