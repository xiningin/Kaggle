{"cell_type":{"40b2d1d1":"code","4e4f8c5d":"code","87d2c449":"code","bcccb21b":"code","7708a31f":"code","fca73b74":"code","d6eaecf8":"code","de7ab450":"code","873d690c":"code","dd6ea971":"code","f0c170f9":"code","538326f5":"code","cfdebb91":"code","45b7bac7":"code","49969201":"code","a2529ce6":"code","8e939c65":"code","19084107":"code","f11342d8":"markdown","b58bec6c":"markdown","753c9bd7":"markdown","2a18d16e":"markdown","5407a0b0":"markdown","638ae84d":"markdown","d2414280":"markdown","e920af3c":"markdown","4aa477e8":"markdown","732a8b31":"markdown","bff42a41":"markdown","d3c61815":"markdown","bd43d35b":"markdown","c9a55d9d":"markdown","8c5781d0":"markdown"},"source":{"40b2d1d1":"!conda install gdcm -c conda-forge -y","4e4f8c5d":"import sys\nimport pydicom\nfrom pydicom import dcmread\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport hashlib\nimport os\nfrom io import BytesIO\nfrom PIL import Image, ImageFont, ImageDraw\nimport cv2\nfrom tqdm.auto import tqdm\n#import warnings\n#warnings.filterwarnings('ignore')\n%matplotlib inline","87d2c449":"raw_df = pd.read_csv('..\/input\/updated-csv\/combined_train_data.csv')\nraw_df.class_id = raw_df.class_id - 1\nraw_df.head()\n\n","bcccb21b":"IMAGE_SIZE = 1024 # change this to desired value\nCLIP_LIMIT = 2.\nGRID_SIZE = (8,8)\n\nNUM_CLASSES = 3\n\n# https:\/\/sashat.me\/2017\/01\/11\/list-of-20-simple-distinct-colors\/\nLABEL_COLORS = [(230, 25, 75), (60, 180, 75), (255, 225, 25), (0, 130, 200)]\n\ndef read_image(path, voi_lut = True, target_size=IMAGE_SIZE, use_clahe=True):\n    # Original from: https:\/\/www.kaggle.com\/raddar\/convert-dicom-to-np-array-the-correct-way\n    ds = dcmread(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(ds.pixel_array, ds)\n    else:\n        data = ds.pixel_array\n    im = data - np.min(data)\n    im = 255. * im \/ np.max(im)\n    if ds.PhotometricInterpretation == \"MONOCHROME1\": # check for inverted image\n        im = 255. - im\n    if use_clahe:\n        clahe = cv2.createCLAHE(clipLimit=CLIP_LIMIT, tileGridSize=GRID_SIZE)\n        climg = clahe.apply(im.astype('uint8'))\n        img = Image.fromarray(climg.astype('uint8'), 'L')\n    else:\n        img = Image.fromarray(im.astype('uint8'), 'L')\n    org_size = img.size\n    if max(img.size) > target_size:\n        img.thumbnail((target_size, target_size), Image.ANTIALIAS)\n    return img, org_size","7708a31f":"findings = raw_df[raw_df.class_id != -1]\nclass_names = []\nfor i in range(NUM_CLASSES):\n    class_names.append(findings[findings.class_id == i].class_name.iloc[0])\nclass_names","fca73b74":"wbf = []\n\nfor _, row in tqdm(findings.iterrows()):\n    image_name = row['id'].replace('_image','')        \n    label = row['label'].split()\n    length = row['No_of_findings']\n    class_id = row['class_id']\n    class_name = row['class_name']\n    path = row['img_path']\n    \n    j = 0\n    for i in range(length):\n        x_min = float(label[j+2])\n        y_min = float(label[j+3])            \n        x_max = float(label[j+4])           \n        y_max = float(label[j+5])\n        # Hard code below class_name and class_id to opacity and 0 resp. if you want to train on 2-class objects (disease or no disease)\n        #\n        wbf.append([image_name, class_name, class_id, x_min, y_min, x_max, y_max, path])\n        j += 6\n    \nwbf_df = pd.DataFrame (wbf, columns=['image_id', 'class_name', 'class_id', 'x_min', 'y_min', 'x_max', 'y_max', 'path'])\nwbf_df.to_csv('wbf_objects.csv')","d6eaecf8":"wbf_df.head()","de7ab450":"wbf_df.class_id.value_counts()","873d690c":"from sklearn.model_selection import StratifiedKFold\n\nNUM_SHARDS = 20\n\nskf = StratifiedKFold(n_splits=NUM_SHARDS, shuffle=True, random_state=42)\ndf_folds = wbf_df[['image_id']].copy()\n\ndf_folds.loc[:, 'bbox_count'] = 1\ndf_folds = df_folds.groupby('image_id').count()\ndf_folds.loc[:, 'object_count'] = wbf_df.groupby('image_id')['class_id'].nunique()\n\ndf_folds.loc[:, 'stratify_group'] = np.char.add(\n    df_folds['object_count'].values.astype(str),\n    df_folds['bbox_count'].apply(lambda x: f'_{x \/\/ 15}').values.astype(str))\n\ndf_folds.loc[:, 'fold'] = 0\nfor fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number\ndf_folds.reset_index(inplace=True)","dd6ea971":"df_folds","f0c170f9":"df_shard = pd.merge(wbf_df, df_folds[df_folds['fold'] == 0], on='image_id')\ndfs = df_shard.class_name.value_counts().to_frame('S0').sort_index()\nfor i in range(1,20):\n    df_shard = pd.merge(wbf_df, df_folds[df_folds['fold'] == i], on='image_id')\n    dfs['S'+str(i)] = df_shard.class_name.value_counts().to_frame().sort_index()\ndfs","538326f5":"# Create example for TensorFlow Object Detection API\ndef create_tf_example(imagedf, longest_edge=IMAGE_SIZE):  \n    fname = imagedf.path.iloc[0]\n    filename=fname.split('\/')[-1] # exclude path    \n    img, org_size = read_image(fname, target_size=IMAGE_SIZE, use_clahe=True)\n    height = img.size[1] # Image height\n    width = img.size[0] # Image width\n    buf= BytesIO()\n    img.save(buf, format= 'JPEG') # encode to jpeg in memory\n    encoded_image_data= buf.getvalue()\n    image_format = b'jpeg'\n    source_id = imagedf.image_id.iloc[0]\n    # A hash of the image is used in some frameworks\n    key = hashlib.sha256(encoded_image_data).hexdigest()   \n    # object bounding boxes \n    xmins = imagedf.x_min.values\/org_size[0] # List of normalized left x coordinates in bounding box \n    xmaxs = imagedf.x_max.values\/org_size[0] # List of normalized right x coordinates in bounding box\n    ymins = imagedf.y_min.values\/org_size[1] # List of normalized top y coordinates in bounding box \n    ymaxs = imagedf.y_max.values\/org_size[1] # List of normalized bottom y coordinates in bounding box\n    # List of string class name & id of bounding box (1 per box)\n    object_cnt = len(imagedf)\n    classes_text = []\n    classes = []\n    for i in range(object_cnt):\n        classes_text.append(imagedf.class_name.iloc[i].encode())\n        classes.append(1+imagedf.class_id.iloc[i]) # 0 is not a valid class\n        \n    # unused features from Open Image \n    depiction = np.zeros(object_cnt, dtype=int)\n    group_of = np.zeros(object_cnt, dtype=int)\n    occluded = np.zeros(object_cnt, dtype=int) #also Pascal VOC\n    truncated = np.zeros(object_cnt, dtype=int) # also Pascal VOC\n    # Pascal VOC\n    view_text = []\n    for i in range(object_cnt):\n        view_text.append('frontal'.encode())\n    difficult = np.zeros(object_cnt, dtype=int)\n\n    tf_record = tf.train.Example(features=tf.train.Features(feature={\n        'image\/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n        'image\/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n        'image\/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename.encode()])),\n        'image\/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[source_id.encode()])),\n        'image\/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_image_data])),\n        'image\/key\/sha256': tf.train.Feature(bytes_list=tf.train.BytesList(value=[key.encode()])),\n        'image\/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n        'image\/object\/bbox\/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=xmins)),\n        'image\/object\/bbox\/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=xmaxs)),\n        'image\/object\/bbox\/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=ymins)),\n        'image\/object\/bbox\/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=ymaxs)),\n        'image\/object\/class\/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=classes_text)),\n        'image\/object\/class\/label': tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n        'image\/object\/depiction': tf.train.Feature(int64_list=tf.train.Int64List(value=depiction)),\n        'image\/object\/group_of': tf.train.Feature(int64_list=tf.train.Int64List(value=group_of)),\n        'image\/object\/occluded': tf.train.Feature(int64_list=tf.train.Int64List(value=occluded)),\n        'image\/object\/truncated': tf.train.Feature(int64_list=tf.train.Int64List(value=truncated)),\n        'image\/object\/difficult': tf.train.Feature(int64_list=tf.train.Int64List(value=difficult)),\n        'image\/object\/view': tf.train.Feature(bytes_list=tf.train.BytesList(value=view_text))\n    }))\n    return tf_record","cfdebb91":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport contextlib2\n\ndef open_sharded_tfrecords(exit_stack, base_path, num_shards):\n    tf_record_output_filenames = [\n        '{}-{:03d}-of-{:03}.tfrecord'.format(base_path, idx, num_shards)\n        for idx in range(num_shards)\n        ]\n    tfrecords = [\n        exit_stack.enter_context(tf.io.TFRecordWriter(file_name))\n        for file_name in tf_record_output_filenames\n    ]\n    return tfrecords\n\noutput_filebase='.\/SiimCovid19'\n\nimg_cnt = np.zeros(NUM_SHARDS, dtype=int)\nwith contextlib2.ExitStack() as tf_record_close_stack:\n    output_tfrecords = open_sharded_tfrecords(tf_record_close_stack, output_filebase, NUM_SHARDS)\n    for i in tqdm(range(NUM_SHARDS)):\n        df_shard = pd.merge(wbf_df, df_folds[df_folds['fold'] == i], on='image_id')\n        ids = df_shard.image_id.unique()\n        for j in range (len(ids)):\n            imagedf = df_shard[df_shard.image_id == ids[j]]\n            tf_record = create_tf_example(imagedf, longest_edge=IMAGE_SIZE)            \n            output_tfrecords[i].write(tf_record.SerializeToString())\n            img_cnt[i] += 1\nprint(\"Converted {} images\".format(np.sum(img_cnt)))\nprint(\"Images per shard: {}\".format(img_cnt))","45b7bac7":"%%time\n!tar -zcf siim_data.tar.gz -C \"\/kaggle\/working\/\" .","49969201":"import json\n\ndparams = {\n    \"IMAGE_SIZE\": IMAGE_SIZE,\n    \"CLIP_LIMIT\": CLIP_LIMIT,\n    \"GRID_SIZE\": GRID_SIZE}\nwith open(\"dparams.json\", \"w\") as json_file:\n    json_file.write(json.dumps(dparams, indent = 4))","a2529ce6":"labels = ['Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']\n\nwith open('.\/SiimCovid19.pbtxt', 'w') as f:\n    for i in range (len(labels)): \n        f.write('item {{\\n id: {}\\n name:\\'{}\\'\\n}}\\n\\n'.format(i+1, labels[i]))","8e939c65":"# Some helper functions to draw image with object boundary boxes\nfontname = '\/usr\/share\/fonts\/truetype\/dejavu\/DejaVuSans.ttf'\nfont = ImageFont.truetype(fontname, 40) if os.path.isfile(fontname) else ImageFont.load_default()\n\ndef bbox(img, xmin, ymin, xmax, ymax, color, width, label, score):\n    draw = ImageDraw.Draw(img)\n    xres, yres = img.size[0], img.size[1]\n    box = np.multiply([xmin, ymin, xmax, ymax], [xres, yres, xres, yres]).astype(int).tolist()\n    txt = \" {}: {}%\" if score >= 0. else \" {}\"\n    txt = txt.format(label, round(score, 1))\n    ts = draw.textsize(txt, font=font)\n    draw.rectangle(box, outline=color, width=width)\n    if len(label) > 0:\n        if box[1] >= ts[1]+3:\n            xsmin, ysmin = box[0], box[1]-ts[1]-3\n            xsmax, ysmax = box[0]+ts[0]+2, box[1]\n        else:\n            xsmin, ysmin = box[0], box[3]\n            xsmax, ysmax = box[0]+ts[0]+2, box[3]+ts[1]+1\n        draw.rectangle([xsmin, ysmin, xsmax, ysmax], fill=color)\n        draw.text((xsmin, ysmin), txt, font=font, fill='white')\n\ndef plot_img(img, axes, xmin, ymin, xmax, ymax, classes, class_label, by):\n    img = img.convert(\"RGB\")\n    for i in range(len(xmin)):\n        color = LABEL_COLORS[class_label[i]]\n        bbox(img, xmin[i], ymin[i], xmax[i], ymax[i], color, 5, classes[i].decode(), -1)\n    plt.setp(axes, xticks=[], yticks=[])\n    axes.set_title(by)\n    plt.imshow(img)","19084107":"fname='.\/SiimCovid19-018-of-020.tfrecord' \ndataset3 = tf.data.TFRecordDataset(fname)\nfig = plt.figure(figsize=(20,30))\nidx=1\nfor raw_record in dataset3.take(6):\n    axes = fig.add_subplot(3, 2, idx)\n    example = tf.train.Example()\n    example.ParseFromString(raw_record.numpy())\n    xmin=example.features.feature['image\/object\/bbox\/xmin'].float_list.value[:]\n    xmax=example.features.feature['image\/object\/bbox\/xmax'].float_list.value[:]\n    ymin=example.features.feature['image\/object\/bbox\/ymin'].float_list.value[:]\n    ymax=example.features.feature['image\/object\/bbox\/ymax'].float_list.value[:]\n    classes=example.features.feature['image\/object\/class\/text'].bytes_list.value[:]\n    class_label=example.features.feature['image\/object\/class\/label'].int64_list.value[:]\n    img_encoded=example.features.feature['image\/encoded'].bytes_list.value[0]\n    img = Image.open(BytesIO(img_encoded))\n    plot_img(img, axes, xmin, ymin, xmax, ymax, classes, class_label, \"\")\n    idx=idx+1","f11342d8":"A TFRecord file stores your data as a sequence of binary strings. Binary data takes up less space on disk, takes less time to copy and can be read much more efficiently from disk. However, pure performance isn\u2019t the only advantage of the TFRecord file format. It is optimized for use with Tensorflow in multiple ways. One use being, it makes it easy to combine multiple datasets and integrates seamlessly with the data import and preprocessing functionality provided by the library. Especially for datasets that are too large to be stored fully in memory this is an advantage as only the data that is required at the time (e.g. a batch) is loaded from disk and then processed. ","b58bec6c":"# **SIIM-FISABIO-RSNA COVID-19 Detection**","753c9bd7":"Copied from the below mentioned notebook and modified according to the given data for this competition","2a18d16e":"# **Tensorflow Records**","5407a0b0":"# **Loading Packages**","638ae84d":"# Checking TFRecords","d2414280":"**Task:** To identify and localize COVID-19 abnormalities on chest radiographs","e920af3c":"# Credits","4aa477e8":"# **Reading csv file**","732a8b31":"https:\/\/www.kaggle.com\/mistag\/data-create-tfrecords-of-vinbigdata-chest-x-rays\n\nThankyou so much sir.","bff42a41":"Here I'm using an updated csv file not the original file provided in the competition.","d3c61815":"Download these files too (json.dumps and the pbtxt file) for training purpose","bd43d35b":"# **Helpers Functions**","c9a55d9d":"Download it from the output folder","8c5781d0":"# **Exporting the TFRecords**"}}