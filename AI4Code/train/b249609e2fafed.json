{"cell_type":{"446b8ff7":"code","03e33ff6":"code","eb1c232e":"code","42dc4e02":"code","3901c576":"code","5533d52b":"code","9c4e165e":"code","585675da":"code","04893f11":"code","925077e6":"code","d1327065":"code","067be482":"code","adb589b3":"code","a2183b0b":"code","4250cfcb":"code","1de8ef08":"code","57b1313c":"code","eb961369":"code","c99a3a3a":"code","7690f952":"code","9cf66a90":"code","13dd3ed2":"markdown","5d524267":"markdown"},"source":{"446b8ff7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport re\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\n\n# For visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.impute import KNNImputer\n\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.decomposition import FactorAnalysis\n\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom keras.constraints import maxnorm\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation,  Flatten, Input\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras import metrics\n\nfrom sklearn.ensemble import StackingClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","03e33ff6":"# Read the data\ndf = pd.read_csv('..\/input\/titanic\/gender_submission.csv', index_col='PassengerId')\n\ndf_train  = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')","eb1c232e":"df_train.describe()","42dc4e02":"print('Train columns with null values:\\n', df_train.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test\/Validation columns with null values:\\n', df_test.isnull().sum())\nprint(\"-\"*10)\n\ndf_train.describe(include = 'all')","3901c576":"df_train['Embarked'] = df_train['Embarked'].fillna('S')\ndf_test['Embarked'] = df_test['Embarked'].fillna(df_test['Fare'].mean())","5533d52b":"# Features extraction from \"Name\" col such as 'Title' and 'Name length'(I think it make sence)\n\ndef title(row): \n    try:\n        row.Title = re.findall(r'^.*, (M[a-z]*).', row.Name)[0]\n    except IndexError:\n        row.Title = 'others'\n    return row\n\ndef name_len(row): \n    try:\n        row.Name_len = len(row.Name)\n    except IndexError:\n        pass\n    return row\n\ndef title_n(data):\n    data = data.assign(Title=\"\")\n    data = data.assign(Name_len=\"\")\n    data = data.apply(title, axis='columns')\n    data = data.apply(name_len, axis='columns')\n    return data\n    \ndf_test = title_n(df_test)\ndf_train = title_n(df_train)","9c4e165e":"# Converting data \n\ndef converter(dataset):\n    \n    # Rare title converting\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Major', 'Master')\n    \n    # Cabin encoding\n    dataset = dataset.fillna(0)\n    dataset.loc[ dataset['Cabin'] !=0, 'Cabin'] = 1\n\n    #Age imputation\n    imputer = KNNImputer(n_neighbors=5)\n    age = imputer.fit_transform(dataset['Age'].values.reshape(-1, 1))\n    age_1 = pd.DataFrame({'New_Age': age[:, 0]})\n    dataset = dataset.join(age_1)\n    \n    #Embarked imputation\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n    \n    #Sex, Title encoding\n    label = LabelEncoder()\n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'].fillna('S'))\n    \n    #Age encoding\n    dataset.loc[ dataset['New_Age'] <= 16, 'New_Age'] = 0\n    dataset.loc[(dataset['New_Age'] > 16) & (dataset['New_Age'] <= 32), 'New_Age'] = 1\n    dataset.loc[(dataset['New_Age'] > 32) & (dataset['New_Age'] <= 48), 'New_Age'] = 2\n    dataset.loc[(dataset['New_Age'] > 48) & (dataset['New_Age'] <= 64), 'New_Age'] = 3\n    dataset.loc[ dataset['New_Age'] > 64, 'New_Age'] = 4\n    \n    #Name_len encoding\n    dataset.loc[ dataset['Name_len'] <= 20, 'Name_len'] = 0\n    dataset.loc[(dataset['Name_len'] > 20) & (dataset['Name_len'] <= 25), 'Name_len'] = 1\n    dataset.loc[(dataset['Name_len'] > 25) & (dataset['Name_len'] <= 30), 'Name_len'] = 2\n    dataset.loc[ dataset['Name_len'] > 30, 'Name_len'] = 3\n    \n    #Fare encoding\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    \n    # Drop unnecessary cols\n    dataset = dataset.drop(['Name', 'Ticket', 'Age', 'Sex', 'Title', 'Embarked'], axis = 1)\n    \n    dataset = dataset.astype(int)\n    \n    # Cols normalization\n    dataset['Embarked_Code'] = dataset['Embarked_Code'] \/ dataset['Embarked_Code'].max()\n    dataset['Title_Code'] = dataset['Title_Code'] \/ dataset['Title_Code'].max()\n    dataset['Parch'] = dataset['Parch'] \/ dataset['Parch'].max()\n    dataset['SibSp'] = dataset['SibSp'] \/ dataset['SibSp'].max()\n    dataset['Pclass'] = dataset['Pclass'] \/ dataset['Pclass'].max()\n    dataset['Fare'] = dataset['Fare'] \/ dataset['Fare'].max()\n    dataset['New_Age'] = dataset['New_Age'] \/ dataset['New_Age'].max()\n    dataset['Name_len'] = dataset['Name_len'] \/ dataset['Name_len'].max()\n    \n    return dataset\n\ndf_test = converter(df_test)\ndf_train = converter(df_train)\n\ndf_test.sample(10)","585675da":"df_train = df_train.astype(float)\ndf_test = df_test.astype(float)","04893f11":"df_train.describe()","925077e6":"new_df = df_train.drop(['PassengerId'], axis = 1)\ndf_scaled = scale(new_df)\nFacAn = FactorAnalysis(n_components = 3)\nFacAn.fit(df_scaled)","d1327065":"df_fa = pd.DataFrame(FacAn.components_, columns = new_df.columns)\ndf_fa","067be482":"set(df_train.columns.to_list()) - set(df_test.columns.to_list())","adb589b3":"set(df_test.columns.to_list()) - set(df_train.columns.to_list())","a2183b0b":"# Prepare datasets\n\nX_train = df_train.drop(labels = ['Survived', 'PassengerId'], axis=1)\nY_train = df_train[\"Survived\"].astype('int')\nX_test  = df_test.drop(\"PassengerId\", axis=1).copy()\nY_test = df.squeeze()\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape","4250cfcb":"#RandomForest + GridSearchCV  model\n\n#cv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )\n\n#grid_n_estimator = [300]\n#grid_seed = [0]\n#grid_learn = [.1, .2, .3]\n#\n#RandomForest = RandomForestClassifier(criterion = 'gini', oob_score = True, warm_start = False, class_weight = None)\n\nRandomForest = RandomForestClassifier(random_state = 0,\n                                       n_estimators = 300,\n                                       criterion = 'gini',\n                                       max_depth = 6,\n                                       oob_score = True,\n                                       warm_start = False,\n                                      class_weight = None)\n\n#RandomForest = GridSearchCV(RandomForestClassifier(), param_grid= {'max_depth': [4,5,6], \n#                                                            'n_estimators': grid_n_estimator, \n#                                                            'random_state': grid_seed}, \n#                     scoring = 'roc_auc', cv = cv_split)\n\n\nRandomForest.fit(X_train, Y_train)\n#print('Best Parameters: ', RandomForest.best_params_)\nY_pred_train = RandomForest.predict(X_train)\nprint( classification_report(Y_train, Y_pred_train))\n\nY_pred = RandomForest.predict(X_test)\n\nprint(\"Model score: %.3f\" % RandomForest.score(X_test, Y_test))","1de8ef08":"#LGBMClassifier + GridSearchCV  model\n\n#cv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )\n\n#grid_n_estimator = [480]\n#grid_seed = [0]\n#grid_learn = [.01]\n\n#LGBM = LGBMClassifier(boosting_type='gbdt',\n#                       objective='binary',\n#                       metric='auc')\nLGBM = LGBMClassifier(boosting_type='gbdt',\n                      objective='binary',\n                      metric='auc',\n                      seed = 0,\n                      n_estimators = 480,\n                      num_leaves = 5,\n                      max_depth = 3,\n                      learning_rate = 0.01)\n#LGBM = GridSearchCV(LGBMClassifier(), param_grid= {'learning_rate': grid_learn,\n#                                                    'num_leaves': [3,4,5],\n#                                                    'n_estimators': grid_n_estimator,\n#                                                    'random_state': grid_seed,\n#                                                    'max_depth': [3],},\n#                    cv=cv_split)\n\nLGBM.fit(X_train, Y_train)\n#print('Best Parameters: ', LGBM.best_params_)\nY_pred_train = LGBM.predict(X_train)\nprint( classification_report(Y_train, Y_pred_train))\n\nY_pred = LGBM.predict(X_test)\n\nprint(\"Stacking model score: %.3f\" % LGBM.score(X_test, Y_test))","57b1313c":"#CatBoost + GridSearchCV  model\n\n#cv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )\n\n#grid_n_estimator = [35, 50, 80]\n#grid_learn = [.015, .2, .25, .3]\n\n#Cat = CatBoostClassifier(iterations = 1, loss_function='Logloss')\n\nCat = CatBoostClassifier(loss_function = 'Logloss',\n                         n_estimators = 80,\n                         depth = 5,\n                         learning_rate = 0.2)\n\n#Cat = GridSearchCV(CatBoostClassifier(), param_grid= {'learning_rate': grid_learn,\n#                                                        'depth': [4,5,6],\n#                                                        'n_estimators': grid_n_estimator},\n#                     scoring = 'roc_auc', cv = cv_split)\n\nCat.fit(X_train, Y_train)\n#print('Best Parameters: ', Cat.best_params_)\nY_pred_train = Cat.predict(X_train)\nprint( classification_report(Y_train, Y_pred_train))\n\nY_pred = Cat.predict(X_test)\n\nprint(\"Stacking model score: %.3f\" % Cat.score(X_test, Y_test))","eb961369":"#XGBoost + GridSearchCV  model\n\n#cv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )\n\n#grid_n_estimator = [35, 50, 80]\n#grid_seed = [0]\n#grid_learn = [.08, .1, .13]\n\n#XGB = XGBClassifier()\nXGB = XGBClassifier(seed = 0,\n                    n_estimators = 50,\n                    max_depth = 3,\n                    learning_rate = 0.013)\n#XGB = GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, \n#                                                   'max_depth': [3,4], \n #                                                  'n_estimators': grid_n_estimator, \n#                                                   'seed': grid_seed}, \n#                     scoring = 'roc_auc', cv = cv_split)\n\n\nXGB.fit(X_train, Y_train)\n#print('Best Parameters: ', XGB.best_params_)\nY_pred_train = XGB.predict(X_train)\nprint( classification_report(Y_train, Y_pred_train))\n\nY_pred = XGB.predict(X_test)\n\nprint(\"Stacking model score: %.3f\" % XGB.score(X_test, Y_test))","c99a3a3a":"def create_model ():\n    # create model\n    model = Sequential()\n    model.add(Dense(9, input_dim=10, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(11, activation='relu'))\n    model.add(Flatten())\n    optimizer= keras.optimizers.RMSprop(lr=0.008)\n    model.add(Dense(units = 1, activation = 'sigmoid'))  # Compile model\n    model.compile(loss='binary_crossentropy',\n                  optimizer=optimizer, metrics=[keras.metrics.AUC(), 'accuracy'])\n    return model\n\nKerasC=KerasClassifier(build_fn=create_model, epochs=50, batch_size= 30)\nKerasC._estimator_type = \"classifier\"\n\nKerasC.fit(X_train, Y_train)\n#print('Best Parameters: ', KerasC.best_params_)\nY_pred_train = KerasC.predict(X_train)\nprint( classification_report(Y_train, Y_pred_train))\n\nY_pred = KerasC.predict(X_test)\n\nprint(\"Stacking model score: %.3f\" % KerasC.score(X_test, Y_test))","7690f952":"cv_split = StratifiedKFold(n_splits = 3, shuffle = True)\n\nestimators = [('XGB', XGB),\n              ('Cat', Cat), \n              ('LGBM', LGBM), \n              ('RandomForest', RandomForest),\n              ('KerasC', RandomForest)] \n\nstacked = StackingClassifier(estimators = estimators, \n                             final_estimator = LogisticRegression(),\n                             verbose = 4, \n                             cv = cv_split)\n\nstacked.fit(X_train, Y_train)\n\nY_pred_train = stacked.predict(X_train)\nprint( classification_report(Y_train, Y_pred_train))\n\nY_pred = stacked.predict(X_test)\n\nprint(\"Stacking model score: %.3f\" % stacked.score(X_test, Y_test))","9cf66a90":"submission = pd.DataFrame({\n        \"PassengerId\": df_test[\"PassengerId\"].astype(int),\n        \"Survived\": Y_pred\n    })\n\nsubmission.to_csv(\"Stack.csv\", index=False)","13dd3ed2":"To better understanding and make some plots we need to convert \"object\" into \"numbers\" ","5d524267":"The first factor describes rich people more of them women(who have a high chance of being saved):\n- have a low Pclass (first class)\n- paid a lot for the ticket\n- have a cabin\n- have a title that is high enough\n- mostly older people \n\nThe second factor describes young people(mostly girls) with family (who have a middle chance of being saved):\n- have a second Pclass or various Pclass's\n- paid enough for the ticket(maybe kids fare lower than older people)\n- mostly under 20 years old(assumption)\n- have low title (Mr or Miss)\n- have low sex code (female)\n\nThe third factor describes mans from poor\/middle class (who have a low chance of being saved):\n- have a second Pclass or various Pclass's\n- paid low price for the ticket\n- have no cabin\n- have middle title \n- have high sex code (male)\n- "}}