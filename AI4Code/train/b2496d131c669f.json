{"cell_type":{"c5d191d3":"code","e33bc6e7":"code","0118d323":"code","a6a02644":"code","4b6b3621":"code","dcf117cf":"code","e139e979":"code","f713c95d":"code","496418ad":"code","700c851e":"code","33490e61":"code","9696571f":"code","a8ae6791":"code","860e1d6e":"code","9ebea240":"code","2ca09196":"code","87ffc127":"code","991d1003":"code","a72ed651":"code","871327fa":"code","968f70b8":"code","6a0a8779":"code","58c301c3":"code","43e01097":"code","e44a223d":"code","d2419a9e":"code","8a3902ba":"code","8be1e1eb":"code","afbd798a":"code","b780311f":"code","011e474a":"code","5014a8a8":"code","8ae93a23":"code","ac37c754":"code","807015ae":"code","6ced0900":"code","0dc15a1b":"code","153a7c9a":"code","02c97db6":"code","3fe70058":"code","e3453e4b":"code","8ca9c416":"code","eb131d3c":"code","0223fa0d":"code","3d1fc8d6":"code","321f1efe":"markdown","4a16bd9a":"markdown","90c5da0f":"markdown","59913d59":"markdown"},"source":{"c5d191d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e33bc6e7":"emp_atr=pd.read_csv(\"..\/input\/HR-Employee-Attrition.csv\")\nemp_atr.shape","0118d323":"emp_atr.info()","a6a02644":"emp_atr.describe().transpose()","4b6b3621":"emp_atr.Attrition.value_counts()","dcf117cf":"emp_atr.head(10)","e139e979":"emp_atr.tail(10)","f713c95d":"emp_atr.isna().sum()","496418ad":"emp_atr[emp_atr.duplicated()]","700c851e":"emp_atr.columns","33490e61":"cat_col = emp_atr.select_dtypes(exclude=np.number).columns\nnum_col = emp_atr.select_dtypes(include=np.number).columns\nprint(cat_col)\nprint(num_col)","9696571f":"for i in cat_col:\n    print(emp_atr[i].value_counts())","a8ae6791":"# Get discrete numerical value\nnum_col_disc=[]\nnum_col_medium=[]\nnum_col_cont=[]\nprint(\"Attributes with their distinct count\")\nfor i in num_col:\n    if emp_atr[i].nunique() <=10:\n        print(i,\"==\",emp_atr[i].nunique(),\"== disc\")\n        num_col_disc.append(i)\n    elif (emp_atr[i].nunique() >10 and emp_atr[i].nunique() <100):\n        num_col_medium.append(i)    \n        print(i,\"==\",emp_atr[i].nunique(),\"== medium\")\n    else:\n        num_col_cont.append(i)\n        print(i,\"==\",emp_atr[i].nunique(),\"== cont\")\n#print(num_col_disc)\n#print(num_col_medium)\n#print(num_col_cont)","860e1d6e":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","9ebea240":"fig, ax = plt.subplots(3, 3, figsize=(30, 40))\nfor variable, subplot in zip(cat_col, ax.flatten()):\n    cp=sns.countplot(emp_atr[variable], ax=subplot,order = emp_atr[variable].value_counts().index,hue=emp_atr['Attrition'])\n    cp.set_title(variable,fontsize=40)\n    cp.legend(fontsize=30)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)\n        label.set_fontsize(36)                \n    for label in subplot.get_yticklabels():\n        label.set_fontsize(36)        \n        cp.set_ylabel('Count',fontsize=40)    \nplt.tight_layout()\n","2ca09196":"fig, ax = plt.subplots(3, 3, figsize=(30, 40))\nfor variable, subplot in zip(num_col_disc, ax.flatten()):\n    cp=sns.countplot(emp_atr[variable], ax=subplot,order = emp_atr[variable].value_counts().index,hue=emp_atr['Attrition'])\n    cp.set_title(variable,fontsize=40)\n    cp.legend(fontsize=30)\n    for label in subplot.get_xticklabels():\n        #label.set_rotation(90)\n        label.set_fontsize(36)                \n    for label in subplot.get_yticklabels():\n        label.set_fontsize(36)        \n        cp.set_ylabel('Count',fontsize=40)\nplt.tight_layout()","87ffc127":"plt.figure(figsize=(20, 10))\nsns.countplot(emp_atr[\"WorkLifeBalance\"],hue=emp_atr[\"Attrition\"])\n#emp_atr[\"WorkLifeBalance\"]","991d1003":"plt.figure(figsize=(20, 10))\nsns.boxplot(data=emp_atr[num_col_cont],orient=\"h\")","a72ed651":"plt.figure(figsize=(20, 10))\nsns.barplot(data=emp_atr[num_col_cont],orient=\"h\")","871327fa":"#fill_num_attrition=lambda x: 1 if x==\"Yes\" else 0\n#type(fill_num_attrition)\nemp_atr[\"num_attrition\"]=emp_atr[\"Attrition\"].apply(lambda x: 1 if x==\"Yes\" else 0)\nemp_atr[\"num_attrition\"].value_counts()","968f70b8":"emp_atr_cov=emp_atr.cov()\nemp_atr_cov","6a0a8779":"plt.figure(figsize=(40,20))\nsns.heatmap(emp_atr_cov,vmin=-1,vmax=1,center=0,annot=True)","58c301c3":"# Importing necessary package for creating model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","43e01097":"# one hot encoding num_attrition\ncat_col_rm_tgt=cat_col[1:]\nnum_col=emp_atr.select_dtypes(include=np.number).columns\none_hot=pd.get_dummies(emp_atr[cat_col_rm_tgt])\nemp_atr_df=pd.concat([emp_atr[num_col],one_hot],axis=1)\nemp_atr_df.head(10)\n","e44a223d":"X=emp_atr_df.drop(columns=['num_attrition'])\ny=emp_atr_df[['num_attrition']]","d2419a9e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","8a3902ba":"train_Pred = logreg.predict(X_train)","8be1e1eb":"metrics.confusion_matrix(y_train,train_Pred)","afbd798a":"metrics.accuracy_score(y_train,train_Pred)","b780311f":"test_Pred = logreg.predict(X_test)","011e474a":"metrics.confusion_matrix(y_test,test_Pred)","5014a8a8":"metrics.accuracy_score(y_test,test_Pred)","8ae93a23":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, test_Pred))","ac37c754":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","807015ae":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom math import sqrt","6ced0900":"X_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size = 0.3, random_state = 100)\ny_train=np.ravel(y_train)\ny_test=np.ravel(y_test)\n#y_train = y_train.ravel()\n#y_test = y_test.ravel()","0dc15a1b":"accuracy_train_dict={}\naccuracy_test_dict={}\ndf_len=round(sqrt(len(emp_atr_df)))\nfor k in range(3,df_len):\n    K_value = k+1\n    neigh = KNeighborsClassifier(n_neighbors = K_value, weights='uniform', algorithm='auto')\n    neigh.fit(X_train, y_train) \n    y_pred_train = neigh.predict(X_train)\n    y_pred_test = neigh.predict(X_test)    \n    train_accuracy=accuracy_score(y_train,y_pred_train)*100\n    test_accuracy=accuracy_score(y_test,y_pred_test)*100\n    accuracy_train_dict.update(({k:train_accuracy}))\n    accuracy_test_dict.update(({k:test_accuracy}))\n    print (\"Accuracy for train :\",train_accuracy ,\" and test :\",test_accuracy,\"% for K-Value:\",K_value)","153a7c9a":"elbow_curve_train = pd.Series(accuracy_train_dict,index=accuracy_train_dict.keys())\nelbow_curve_test = pd.Series(accuracy_test_dict,index=accuracy_test_dict.keys())\nelbow_curve_train.head(10)","02c97db6":"ax=elbow_curve_train.plot(title=\"Accuracy of train VS Value of K \")\nax.set_xlabel(\"K\")\nax.set_ylabel(\"Accuracy of train\")\n","3fe70058":"ax=elbow_curve_test.plot(title=\"Accuracy of test VS Value of K \")\nax.set_xlabel(\"K\")\nax.set_ylabel(\"Accuracy of test\")","e3453e4b":"from sklearn.naive_bayes import GaussianNB","8ca9c416":"NB=GaussianNB()\nNB.fit(X_train, y_train)","eb131d3c":"GaussianNB(priors=None,var_smoothing=1e-09)","0223fa0d":"train_pred=NB.predict(X_train)\naccuracy_score(train_pred,y_train)","3d1fc8d6":"test_pred=NB.predict(X_test)\naccuracy_score(test_pred,y_test)","321f1efe":"**We will try to implement KNN for this problem to see accuracy is better than logistic regression**","4a16bd9a":"**From the above iteration we see K=12 had better accuracy**","90c5da0f":"**From the above boxplot and barplot, we see that outlier exist for monthly income ,Daily rate and Employee number. Also data is distributed wider for monthly income and monthly rate variable **","59913d59":"**You can clearly see that attrition value for No is quite high than Yes value. We get this biased data having more data for attrition=\"No\" . We try to see that we can get try to get something out of model **\n\n**Also with above plot we see that data is not evenly distributed for discrete values**"}}