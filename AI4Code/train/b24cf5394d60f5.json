{"cell_type":{"e0ef03d0":"code","f8729c9b":"code","5a8d329d":"code","062c2fa1":"code","c254e167":"code","6b0d985c":"code","573700fd":"code","66cecc6e":"code","092ee976":"code","7361865a":"code","b0314665":"code","c9c98800":"code","f79a8712":"code","2fd7f4d4":"code","09304d34":"code","cfee0cbe":"code","47962892":"code","ac059c06":"code","1e7c062e":"code","500857c1":"code","384cf719":"code","5bfbf76b":"code","d46f1897":"markdown","3264644e":"markdown","55098f9b":"markdown","c74d6116":"markdown","aa2df455":"markdown","ae285878":"markdown","b9bc47fb":"markdown","840312bf":"markdown","b87fb099":"markdown","2dd7c359":"markdown","892f1c79":"markdown","32f71e6d":"markdown","6c46a6b6":"markdown","0ce7c410":"markdown","fc55c80f":"markdown"},"source":{"e0ef03d0":"## Uploading my kaggle.json (required for accessing Kaggle APIs)\nfrom google.colab import files\nfiles.upload()","f8729c9b":"## Install Kaggle API\n!pip install -q kaggle\n## Moving the json to appropriate place\n!mkdir -p ~\/.kaggle\n!cp kaggle.json ~\/.kaggle\/\n!chmod 600 \/root\/.kaggle\/kaggle.json","5a8d329d":"!kaggle datasets download mohansacharya\/graduate-admissions\n!echo \"=========================================================\"\n!ls\n!unzip graduate-admissions.zip\n!echo \"=========================================================\"\n!ls","062c2fa1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\npd.set_option('display.max_columns', 60)\n%matplotlib inline","c254e167":"FILE_NAME = \"..\/input\/Admission_Predict_Ver1.1.csv\"\n\nraw_data = pd.read_csv(FILE_NAME)\nraw_data.head()","6b0d985c":"## Are any null values persent ?\nraw_data.isnull().values.any()","573700fd":"## So no NaNs apparently\n## Let's just quickly rename the dataframe columns to make easy references\n## Notice the blankspace after the end of 'Chance of Admit' column name\nraw_data.rename(columns = {\n    'Serial No.' : 'srn',\n    'GRE Score'  : 'gre',\n    'TOEFL Score': 'toefl',\n    'University Rating' : 'unirating',\n    'SOP'        : 'sop',\n    'LOR '        : 'lor',\n    'CGPA'       : 'cgpa',\n    'Research'   : 'research',\n    'Chance of Admit ': 'chance'\n}, inplace=True)\nraw_data.describe()","66cecc6e":"fig, ax = plt.subplots(ncols = 2)\nsns.regplot(x='chance', y='cgpa', data=raw_data, ax=ax[0])\nsns.regplot(x='chance', y='unirating', data=raw_data, ax=ax[1])","092ee976":"fig, ax = plt.subplots(ncols = 2)\nsns.regplot(x='chance', y='gre', data=raw_data, ax=ax[0])\nsns.regplot(x='chance', y='toefl', data=raw_data, ax=ax[1])","7361865a":"fig, ax = plt.subplots(ncols = 3)\nsns.regplot(x='chance', y='sop', data=raw_data, ax=ax[0])\nsns.regplot(x='chance', y='lor', data=raw_data, ax=ax[1])\nsns.regplot(x='chance', y='research', data=raw_data, ax=ax[2])","b0314665":"THRESH = 0.6\n# I think we can also drop srn as it is not doing absolutely anything\nraw_data.drop('srn', axis=1, inplace=True)\nraw_data['chance'] = np.where(raw_data['chance'] > THRESH, 1, 0)\nraw_data.head()","c9c98800":"raw_data.describe()","f79a8712":"X = raw_data.drop(columns='chance')\nY = raw_data['chance'].values.reshape(raw_data.shape[0], 1)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n\n\nprint(\"Training set ...\")\nprint(\"X_train.shape = {}, Y_train.shape =  is {}\".format(X_train.shape, \n                                                          Y_train.shape))\n\nprint(\"Test set ...\")\nprint(\"X_test.shape = {}, Y_test.shape =  is {}\".format(X_test.shape, \n                                                          Y_test.shape))","2fd7f4d4":"from sklearn.metrics import mean_absolute_error as mae","09304d34":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(penalty='l1', solver='liblinear', max_iter=3000000, \n                         tol=1e-8)\nclf.fit(X_train, Y_train)","cfee0cbe":"Y_pred = clf.predict(X_test)\nprint(\"Mean Absolute Error = \", mae(Y_test, Y_pred))","47962892":"from sklearn.svm import LinearSVC\nclf = LinearSVC(verbose=1, max_iter=3000000, tol=1e-8, C=1.25)\nclf.fit(X_train, Y_train)","ac059c06":"Y_pred = clf.predict(X_test)\nprint(\"Mean Absolute Error = \", mae(Y_test, Y_pred))","1e7c062e":"from sklearn.naive_bayes import BernoulliNB\nclf = BernoulliNB()\nclf.fit(X_train, Y_train)","500857c1":"Y_pred = clf.predict(X_test)\nprint(\"Mean Absolute Error = \", mae(Y_test, Y_pred))","384cf719":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(max_depth=10)\nclf.fit(X_train, Y_train)","5bfbf76b":"Y_pred = clf.predict(X_test)\nprint(\"Mean Absolute Error = \", mae(Y_test, Y_pred))","d46f1897":"### Effect of GRE\/TOEFL :\nLet's see if GRE\/TOEFL score matters at all. From what I've heard from my seniors, relatives, friends; these exams don't matter if your score is above some threshold.","3264644e":"## Setting up Google Colab\n\nYou can skip next 2 sections if you're not using Google Colab.","55098f9b":"### Trying out Bernoulli Naive Bayes\nAs described [here](https:\/\/towardsdatascience.com\/naive-bayes-classifier-81d512f50a7c), Bernoulli Naive Bayes can be used for binary classification assuming all factors equally influence the output.","c74d6116":"### Effect of SOP \/ LOR \/ Research :\nI decided to analyze these separately since these are not academic related.and count mostly as an extra curricular skill \/ writing.","aa2df455":"### Imports","ae285878":"### Trying out LinearSVC :\nLinearity of data with respect to 3 features suggest us to use Linear SVC.","b9bc47fb":"### Getting the data\nI'll use Kaggle API for getting the data directly into this instead of pulling the data from Google Drive.","840312bf":"### Trying out Decision Tree Classifier","b87fb099":"### Data exploration","2dd7c359":"### Conclusion :\n\nMost of the classifiers don't work very well on the data with the currently chosen hyper-parameters. This maybe due to smaller size of dataset. \n\nI'm still new to machine learning, if you think I've done something wrong and you want to correct me, you're most welcome.\n\n","892f1c79":"### Conclusions :\nCGPA, GRE and TOEFL are extremely important and they vary almost linearly. (TOEFL varies almost scaringly linearly to chance of admit). On other factors, you need to have _just enough_ score to get admission.\n\nWe will convert the 'Chance of Admission' column into 0 or 1 and then use binary classification algorithms to see if you can get an admission or not.","32f71e6d":"### Trying out Logistic Regression :\nLet's apply good old logistic regression to see if it acn classify the dataset properly","6c46a6b6":"### Train-Test split :\nSince we have less data, I am goign to use traditional 70-30 train test split. May update this in future if author adds more data.","0ce7c410":"### Analyzing the factors influencing the admission :\nFrom what I've heard from my relatives, seniors, friends is that you need an excellent CGPA from a good university , let's verify that first.","fc55c80f":"## Binary Classification using Graduate Admission Dataset\n\nThis notebook compares performance of various Machine Learning classifiers on the \"Graduate Admission\" data. I'm still just a naive student implementing Machine Learning techniques. You're most welcome to suggest me edits on this kernel, I am happy to learn."}}