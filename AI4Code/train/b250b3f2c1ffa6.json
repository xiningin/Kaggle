{"cell_type":{"49d670d9":"code","48b58517":"code","5baeb447":"code","b88e98be":"code","24303653":"code","46bb8116":"code","facc3145":"code","d3dc012c":"code","78753186":"code","96bf0f2c":"code","a681e7d2":"markdown","c40e012e":"markdown","bc99ae09":"markdown","04cc43f1":"markdown","4c7b1373":"markdown","c04918fd":"markdown","4447acbe":"markdown","e0bc44dd":"markdown","7d0a3582":"markdown","f1108b00":"markdown","43a5be94":"markdown"},"source":{"49d670d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # to plot the data for the visualization of SOM\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","48b58517":"! pip install MiniSom","5baeb447":"\"\"\"## Importing the dataset\"\"\"\n\ndataset = pd.read_csv('..\/input\/credit-card-applications\/Credit_Card_Applications.csv')\nX = dataset.iloc[:, :-1].values \nX","b88e98be":"y = dataset.iloc[:, -1].values\ny","24303653":"\"\"\"## Feature Scaling\"\"\"\n\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range = (0,1))\nX = sc.fit_transform(X)","46bb8116":"\"\"\"##Training the SOM\"\"\"\n\nfrom minisom import MiniSom\nsom = MiniSom(x=10, y=10, input_len= 15, sigma= 1.0, learning_rate = 0.5)\nsom.random_weights_init(X)\n# And this above method is random weights underscore in it.So that's the method that will initialize the weights.\n# And inside this method, we just need to input the data.That is x, our data on which the model will be trained.\nsom.train_random(data = X, num_iteration = 100)","facc3145":"\"\"\"##Visualizing the results\"\"\"\n\nfrom pylab import bone, pcolor, colorbar, plot, show\nbone()\npcolor(som.distance_map().T)\ncolorbar()\nmarkers = ['o', 's']\ncolors = ['r', 'g']\nfor i, x in enumerate(X):\n    w = som.winner(x)\n    plot(w[0] + 0.5,\n         w[1] + 0.5,\n         markers[y[i]],\n         markeredgecolor = colors[y[i]],\n         markerfacecolor = 'None',\n         markersize = 10,\n         markeredgewidth = 2)\nshow()\n\n","d3dc012c":"\"\"\"## Finding the frauds\"\"\"\n\nmappings = som.win_map(X)\nfrauds = np.concatenate((mappings[(5,3)], mappings[(1,8)]), axis = 0)\n# this above line corresponds to the white boxes in our map which shows the value between them i.r MID to be\n# 1 which is definately are the customers with the fraud of credit card.\nfrauds = sc.inverse_transform(frauds)\n# the inverse transfoem is applied because we have scaled our dataset and when we see the frauds here you will\n# get the scaled customer id which is actually is not true one so inverse transform is applied.","78753186":"print(frauds)","96bf0f2c":"\"\"\"##Printing the Fraunch Clients\"\"\"\n\nprint('Fraud Customer IDs')\nfor i in frauds[:, 0]:\n  print(int(i))","a681e7d2":"Ok so abive in the image the white corresponds to high MID's\nSo basically it's very simple, we have created two markers, some red circles and some green squares.\nThe red circles are going to correspond to the customers who didn't get approval.And the green squares will correspond to the customers who got approval.We're going to created a new variable, markers and created a vector of two elements corresponding to the two markers. So we said first a circle that is coded by **o** here and then a square that is coded by **s**.Alright so that's the markers,but it's also good to color these markers.So, have added a new variable here that will contain our colors,n equal and same I'm going to create a vector of two elements,which are going to be first the red color,coded by **r****** and the green color,coded by **g******.","c40e012e":"And thus the above is the list of the customers who have cheated and now its bank duty to analyse this data and give their data analyst to perform some logic to indentify it correctly ","bc99ae09":"![image.png](attachment:image.png)","04cc43f1":"# #But Why Scaling ?\n\nML algorithm works better when features are relatively on a similar scale and close to Normal Distribution.\n\n# # # # # **Let us understand the meaning of SCALE, STANDARDIZE AND NORMALIZE**\nSCALE- It means to change the range of values but without changing the shape of distribution. Range is often set to 0 to 1.\n\nSTANDARDIZE-It means changing values so that distribution standard deviation from mean equals to one,output will be very close to normal distribution.\n\nNORMALIZE-It can be used either of above things, it can be a confusion to use this word so i personally do not use this often.\n\nWHY DO WE NEED TO STANDARDIZE OR NORMALIZE OUR FEATURE?\nAlgorithm converge faster when features are relatively smaller or closer to normal distribution.\n\n\n**Example of such algorithms are:**\n1. Linear and Logistic Regression\n2. k nearest neighbor\n3. Neural Network\n4. PCA\n5. LDA\n6. SVM with radial bias kernel function\n\n*Scikit-Learn library gives us some good options to scale or normalize our features.*****\n* MinMaxScaler()\n* RobustScaler()\n* StandardScaler()\n* Normalizer()\n\nYou can read about those more on medium many articles are available there :)","4c7b1373":"> I will be always focusing on that you should scale your data before giving to any deep neural network because scaling is very important for any deep neural network ","c04918fd":"### **A brief introduction to Self Orgnanizing Maps **\nIn the Class SOM we have:\n* x,y : which is grid size of the som(you can play with that)\n* input_len : which tells the no of features we have in our dataset which is exactly 15 if you look into x variable above\n* sigma : it is the radius of the different neighbour in the grid(default its 1)\n* learning rate : This is this hyper-parameter that decides by how much the weights are updated during each iteration.So the higher is the learning rate,the faster there will be convergence and the lower is the learning rate,the longer the self-organizing map will take time to be built(default is 0.5)","4447acbe":"Thus our work is done hope you like this kernel and will surely do **upvote** for my work\n\nThanks with happy learning :)","e0bc44dd":"****** *          Fig 1:- This above diagram shows the self organising Maps","7d0a3582":"In this way you can install Minisom or any other libaries.\nThe SOM is the very basic deep learning unsupervised model used for the credit card fraud detection where in you can easily by seeing the graphs can tell about the results\n\nOk.. So lets start hope you will enjoy :)","f1108b00":"![image.png](attachment:image.png)","43a5be94":"This youtube vidoe help you visualise the SOM in 2D and 3D. Hope you will enjoy!!!!\n\nhttps:\/\/www.youtube.com\/watch?v=b3nG4c2NECI"}}