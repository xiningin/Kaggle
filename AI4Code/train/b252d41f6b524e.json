{"cell_type":{"47fc6a9d":"code","7144dca2":"code","12470c54":"code","401038f9":"code","0e9b08bd":"code","ea0ded37":"code","9fd1cfda":"code","782d58ca":"code","2dd4652a":"code","1a410cb9":"code","e41e54ea":"code","738974e2":"code","4f6cef72":"code","4237cb73":"markdown","41e7a14f":"markdown","e76da21c":"markdown","4f2a8084":"markdown","b0b24135":"markdown","053c1f29":"markdown","ef664f21":"markdown","f2e34f42":"markdown","fbe684d9":"markdown","1a8ef432":"markdown","4b2f962f":"markdown","47d6326c":"markdown","7d45db60":"markdown","148d379d":"markdown","8c87d414":"markdown","e1b6a5e9":"markdown"},"source":{"47fc6a9d":"# Import required modules\nimport json\nimport numpy as np\nimport pandas as pd\nimport time\nimport warnings\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline, FeatureUnion\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')","7144dca2":"# MAP2\ndef MAP2(y_true, y_pred):\n    \n    # Ensure number of rows are the same\n    if len(y_true) != y_pred.shape[0]:\n        \n        # Throw an error\n        raise Exception(\"Length of ground truth vector and predictions differ.\")\n    \n    # Compute Average Precision (AP)\n    ap = ( (y_pred[:, 0] == y_true).astype(int) +\n           (y_pred[:, 1] == y_true).astype(int) \/ 2 )\n    \n    # Compute mean of AP across all observations\n    output = np.mean(ap)\n    \n    # Return\n    return output","12470c54":"# Logistic regression that outputs 2 recommendations\nclass lr_map2(BaseEstimator, ClassifierMixin):\n    \n    def __init__(self, multi_class='ovr', solver='saga', max_iter=100, C=1.0, random_state=123, n_jobs=4, class_weight=None):\n        \n        self.multi_class=multi_class\n        self.solver=solver\n        self.C=C\n        self.class_weight = class_weight\n        self.max_iter=max_iter\n        self.random_state=random_state\n        self.n_jobs=n_jobs\n    \n    def fit(self, X, y=None):\n        \n        self.model = LogisticRegression(\n            multi_class=self.multi_class,\n            solver=self.solver,\n            C=self.C,\n            class_weight=self.class_weight,\n            max_iter=self.max_iter,\n            random_state=self.random_state,\n            n_jobs=self.n_jobs\n        )\n        \n        self.model.fit(X, y)\n        \n        return self\n    \n    def predict(self, X, y=None):\n        \n        # Predict probability of all classes\n        pred_probs = self.model.predict_proba(X)\n        \n        # Extract top two classes\n        pred = self.model.classes_[np.apply_along_axis(lambda x: x.argsort()[-2:][::-1], axis=1, arr=pred_probs)]\n        \n        return pred","401038f9":"# Transformer to fit TF-IDF vectorizer with target labels and transform titles\nclass LabelTransform(TransformerMixin):\n    \n    def __init__(self, labels_tgt, ngram_range, max_df, min_df):\n        \n        self.labels_tgt = labels_tgt\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n    \n    def set_params(self, labels_tgt=None, ngram_range=None, max_df= None, min_df=None):\n        \n        if labels_tgt:\n            self.labels_tgt = labels_tgt\n        \n        if ngram_range:\n            self.ngram_range = ngram_range\n        \n        if max_df:\n            self.max_df = max_df\n        \n        if min_df:\n            self.min_df = min_df\n    \n    def fit(self, X, y=None):\n        \n        # Initialise TF-IDF vectorizer for target labels\n        self.vect_labels = TfidfVectorizer(\n            \n            # USE COUNTS\n            use_idf=False, norm=False, binary=True,\n            \n            # ALLOW SINGLE ALPHANUMERICS\n            token_pattern='(?u)\\\\b\\\\w+\\\\b',\n            \n            # TUNE THESE\n            ngram_range=self.ngram_range,\n            max_df=self.max_df,\n            min_df=self.min_df\n        )\n        \n        # Fit to target labels\n        self.vect_labels.fit(self.labels_tgt)\n        \n        return self\n\n    def transform(self, X, y=None):\n        \n        # Transform\n        output = self.vect_labels.transform(X)\n        \n        # Return\n        return output","0e9b08bd":"# Function for extracting top 2 recommendations\ndef recommend_two(model, X_val):\n    \n    # Obtain predictions\n    pred_probs = model.predict_proba(X_val)\n    \n    # Extract top two classes\n    pred = pd.DataFrame(model.classes_[np.apply_along_axis(lambda x: x.argsort()[-2:][::-1], axis=1, arr=pred_probs)])\n    \n    return pred","ea0ded37":"# Function for preparing submissions\ndef submit_kaggle(itemid, var, pred):\n    \n    # Establish output\n    output = pred.copy()\n    \n    # Set itemid\n    output = pd.DataFrame(itemid.astype(str) + '_' + str(var))\n    output.rename(columns = {'itemid': 'id'}, inplace=True)\n    \n    # Set tagging\n    output['tagging'] = pred.iloc[:, 0].astype(int).astype(str) + ' ' + pred.iloc[:, 1].astype(int).astype(str)\n    \n    # Return\n    return output","9fd1cfda":"# INPUT ATTRIBUTE AND LABEL HERE:\nVAR = 'Camera'\nLABEL = VAR\nDATASET = 'mobile'\n\n# Read data\nmain = pd.read_csv('..\/input\/mobile_data_info_train_competition.csv')\nval = pd.read_csv('..\/input\/mobile_data_info_val_competition.csv')\n\n# Import codebook\nwith open('..\/input\/mobile_profile_train.json', 'r') as f:\n    main_cb = json.load(f)\n\n# Configure stopwords\nstop_words = set([\n    'promo','diskon','baik','terbaik', 'murah',\n    'termurah', 'harga', 'price', 'best', 'seller',\n    'bestseller', 'ready', 'stock', 'stok', 'limited',\n    'bagus', 'kualitas', 'berkualitas', 'hari', 'ini',\n    'jadi', 'gratis'\n])","782d58ca":"# Set output csv name\nfilename = 'mobile_' + VAR.lower().replace(' ', '_') + '.csv'\n\n# Get all titles - BIG MISTAKE\nall_titles = main['title'].append(val['title'])\n\n# Drop image\nmain.drop('image_path', axis = 1, inplace=True)\n\n# Delete missing values\nmain = main[~main[VAR].isnull()]\n\n# Translate words\nmain['title'] = main['title'].str.replace('tahun', 'year')\nmain['title'] = main['title'].str.replace('bulan', 'month')\nmain['title'] = main['title'].str.replace('hitam', 'black')\nmain['title'] = main['title'].str.replace('putih', 'white')\nmain['title'] = main['title'].str.replace('hijau', 'green')\nmain['title'] = main['title'].str.replace('merah', 'red')\nmain['title'] = main['title'].str.replace('ungu', 'purple')\nmain['title'] = main['title'].str.replace(' abu', 'gray')\nmain['title'] = main['title'].str.replace('perak', 'silver')\nmain['title'] = main['title'].str.replace('kuning', 'yellow')\nmain['title'] = main['title'].str.replace('coklat', 'brown')\nmain['title'] = main['title'].str.replace('emas', 'gold')\nmain['title'] = main['title'].str.replace('biru', 'blue')\nmain['title'] = main['title'].str.replace('tahan air', 'waterproof')\nmain['title'] = main['title'].str.replace('layar', 'touchscreen')\n\n# Translate words\nval['title'] = val['title'].str.replace('tahun', 'year')\nval['title'] = val['title'].str.replace('bulan', 'month')\nval['title'] = val['title'].str.replace('hitam', 'black')\nval['title'] = val['title'].str.replace('putih', 'white')\nval['title'] = val['title'].str.replace('hijau', 'green')\nval['title'] = val['title'].str.replace('merah', 'red')\nval['title'] = val['title'].str.replace('ungu', 'purple')\nval['title'] = val['title'].str.replace(' abu', 'gray')\nval['title'] = val['title'].str.replace('perak', 'silver')\nval['title'] = val['title'].str.replace('kuning', 'yellow')\nval['title'] = val['title'].str.replace('coklat', 'brown')\nval['title'] = val['title'].str.replace('emas', 'gold')\nval['title'] = val['title'].str.replace('biru', 'blue')\nval['title'] = val['title'].str.replace('tahan air', 'waterproof')\nval['title'] = val['title'].str.replace('layar', 'touchscreen')\n\n# Configure target labels\nlabels_tgt = pd.Series(list(main_cb[LABEL].keys()))\n\n# Rename data\nX_data = main['title']\ny_data = main[VAR]","2dd4652a":"# Set up feature union\nopt_feats = FeatureUnion(\n    [\n        ('labels', LabelTransform(\n            labels_tgt=labels_tgt,\n            ngram_range=(1,1),\n            max_df=0.2,\n            min_df=1\n        )),\n        \n        # Using LabelTransform to speed up code\n        ('titles', LabelTransform(\n            labels_tgt=all_titles,\n            ngram_range=(1,3),\n            max_df=0.3,\n            min_df=1\n        ))\n    ]\n)","1a410cb9":"# Create temporary Pipeline\ntemp_pipe = Pipeline([('test_feats', opt_feats)])\n\n# Fit to data\ntemp_pipe.fit(X_data, y_data)\n\n# Transform\ntemp_pipe_output = temp_pipe.transform(X_data)\n\n# Select rows\npd.DataFrame(temp_pipe_output[:20, :10].toarray())","e41e54ea":"# Params\nparams_test = {\n    'lr__C': [1]\n}\n\n# Test pipe\ntest_pipe = Pipeline(\n    [\n        ('tfidf', opt_feats),\n        ('lr', lr_map2(multi_class='ovr', solver='saga', C=1))\n    ]\n)","738974e2":"# Initialise GridSearchCV\nopt_test = GridSearchCV(\n    estimator = test_pipe,\n    param_grid = params_test,\n    cv=5,\n    scoring=make_scorer(MAP2, greater_is_better=True),\n    iid=False,\n    verbose=20,\n    n_jobs=4,\n)\n\n# Start timer\nstart_time = time.time()\n\n# Fit\nopt_test.fit(X_data, y_data)\n\n# Stop timer\nend_time = time.time()\nprint('Time taken: %s mins.' % ('{:.2f}'.format((end_time-start_time)\/60)))\n\n# Extract parameters\ncv_results = pd.DataFrame(opt_test.cv_results_['params'])\n\n# Extract mean test score\ncv_results['mean_test_score'] = pd.Series(opt_test.cv_results_['mean_test_score'])\ncv_results['std_test_score'] = pd.Series(opt_test.cv_results_['std_test_score'])\n\n# Display\ncv_results.sort_values('mean_test_score', ascending=False)","4f6cef72":"# Set up feature union\nopt_feats = FeatureUnion(\n    [\n        ('labels', LabelTransform(\n            labels_tgt=labels_tgt,\n            ngram_range=(1,1),\n            max_df=0.2,\n            min_df=1\n        )),\n        \n        ('titles', LabelTransform(\n            labels_tgt=all_titles,\n            ngram_range=(1,3),\n            max_df=0.3,\n            min_df=1\n        ))\n    ]\n)\n\n# Set up Logistic Regression\nopt_clf = LogisticRegression(\n    multi_class='ovr',\n    solver='saga',\n    C=1,\n    random_state=123,\n    n_jobs=4\n)\n\n# Set up full pipeline\nopt_pipe = Pipeline(\n    [\n        ('tfidf', opt_feats),\n        ('lr', opt_clf)\n    ]\n)\n\n# Train on dataset\nopt_pipe.fit(X_data, y_data)\n\n# Obtain predictions\npred = recommend_two(opt_pipe, val['title'])\n\n# Prepare dataset\nsubmission = submit_kaggle(val.itemid, VAR, pred)\n\n# Output\nsubmission.to_csv(filename, index=False)","4237cb73":"# 4th Place Solution Using TF-IDF and Logistic Regression\nWarwick Alumni would like to congratulate the winning teams and all teams who made it to the end of the competition! Although the discussion board was not particularly focused on developing better and more novel solutions, it was nice to see teams helping each other out with respect to the data leak and standing up for one another when it came to the final ranking.\n  \n# Table of Contents\n  \n1. [Overview of Approach](#Overview)\n2. [Import Modules and Define Custom Functions](#importModules)  \n    * [Mean Average Precision @ 2 (MAP@2)](#map2)  \n    * [Logistic Regression with Two Predictions](#lr_map2)\n    * [Label Transformer](#labelTransform)\n    * [Generating Predictions](#recommendTwo)\n    * [Preparing the Submission](#submitKaggle)\n3. [User Parameters](#userParams)\n4. [Basic Data Cleaning](#dataCleaning)\n5. [Model Training](#modelTraining)\n    * [Parameter Optimisation](#paramOpt)\n    * [Generating the Submission](#genSub)\n6. [Conclusion](#conclusion)\n","41e7a14f":"# Conclusion <div id=conclusion><\/div>\nWe used extremely simple techniques to achieve our private leaderboard score of 0.46673, which earned us the 4th-place spot. We wished to have tested more complex techniques (e.g. deep learning), but we were pleasantly surprised by how far this simple approach brought us. Although we may not have achieved the highest leaderboard score, we recommend Shopee to consider the tradeoff between model training time and performance. Our approach offers a good balance of both.  \n  \nFor the data scientists out there, our key learning points are:\n  \n1. No model is too simple to be tested.\n2. Exhaust the possibilities of (1) what features you extract, (2) how you process them, and (3) which ML algorithms you use them with.  \n  \nBy sharing this solution, we hope to encourage aspiring data scientists, folks who are uninitiated but want to know more, and leaders who wish to push data science in their organisation with the simple message that **data science does not need to be as complex as people say**. Sometimes, good intuition and a simple approach is all you need.","e76da21c":"# 2. Model Training <div id=modelTraining><\/div>\nIn this section, we present to sets of code. The first set of code is for parameter optimisation, and the second is for generating the submission.  \n  \n## Parameter Optimisation <div id=paramOpt><\/div>\nFirst, we generate a `FeatureUnion` object for the two sets of extracted features. The first set of extracted features involved a `TfidfVectorizer` trained on **keywords**, and used to transform the combined titles. Intuitively, this represented the **direct matches** between a title and the target feature's classes. The second set of extracted features involved another `TfidfVectorizer` trained on **titles**, and used to transform the combined titles. This represented more **complex relationships** between a title and the target feature's classes.","4f2a8084":"### Preparing the Submission <div id=submitKaggle><\/div>\nFinally, we defined a simple function for formatting the predictions from `recommend_two` for submission on Kaggle.","b0b24135":"To demonstrate what this looked like:","053c1f29":"### Generating Predictions <div id=recommendTwo><\/div>\nNext, we defined a function for providing the required 2 predictions. It takes as its input a `Pipeline` object (with `LogisticRegression`, not `lr_map2` as the estimator) and the test set titles, and returns a dataframe containing the 2 predictions.","ef664f21":"# 1. Basic Data Cleaning <div id=dataCleaning><\/div>\n  \n## Translations\nThe only thing we did to clean the titles was to perform translations. The set of keywords below were words that were (1) keywords and (2) frequently used. Note also that we cleaned the test set titles because we used the test set titles to train the model. As we mentioned earlier, the models did not seem to have sufficient vocabulary. Hence, we used the test set titles to augment the vocabulary provided in the training set.  \n  \nTranslations on the test set were also made for consistency. Training our classifier on features extracted from translated titles and predicting on **un-translated test titles** caused our score to stagnate toward the ending phase of the competition. Only when we rectified this did we jump from 9th to 4th place on the final day.  \n  \nNote that we used different sets of translations for different datasets. For example, in the Beauty dataset, the Colour Group feature contained more colours in Bahasa Indonesian than in the Mobile Colour Family feature.   \n  \n## Images\nWe did not bother to use the images. They were not standardised and would have contributed noise to our models.  \n  \n## Mistake in Sequence of Data Processing\nTo use the test set titles, we accidentally extracted them along with the training set titles **before** translations were made. Had we done the extraction after the translations, our score could have been better.","f2e34f42":"### Label Transformer <div id=labelTransform><\/div>\nThis function was used to process titles in our `Pipeline`. It takes several `TfidfVectorizer` parameters, and generates binary term frequencies (BTFs) for the input data. Another input is `labels_tgt`, which essentially represents **keywords** (words in the class labels of the target feature). The function returns a `csr_matrix` containing only binary features.  \n  \n*Note: We also had a variant of `LabelTransform` called `TitleTransform` that we used to input stopwords. Implementing it is easy, so we won't convolute the post by providing the code for that function.*","fbe684d9":"Next, we set up the parameter grid (`params_test`) and the testing pipeline (`test_pipe`) with the `FeatureUnion` object above as the only `Transformer` and our custom Logistic Regression function with 2 predictions (`lr_map2`) as the `estimator`.","1a8ef432":"# Import Modules and Define Custom Functions <div id=importModules><\/div>\nFirst, we import the required modules and define several custom functions that we used.","4b2f962f":"### Mean Average Precision @ 2 (MAP@2) <div id=map2><\/div>\nFirst, we defined a function to compute the MAP@2 metric. Note that optimising for accuracy and optimising for MAP@2 are not quite the same thing. Accuracy only scores the class with the highest predicted probability, while MAP@2 scores the **classes with the two highest predicted probabilities**. Hence, we used this function in cross validation for parameter optimisation.","47d6326c":"# 0. Setting User Parameters <div id=userParams><\/div>\nWe used one cell to input user parameters:  \n  \n* `VAR`: Which target feature to train model for.\n* `LABEL`: Which target feature to use for keywords. For some target features like Mobile Operating System, it made no sense to use the corresponding keywords, because sellers typically do not write a phone's OS in a sales listing. In this case, we used Phone Model instead. The key idea is to choose a set of keyword that has a strong link to the target feature **and** appears in the titles.  \n* `main`: The training set.\n* `val`: The test set.\n* `main_cb`: Dictionary of mappings from encoded labels to keywords.  \n* `stop_words`: User-defined stopwords.\n  \nThis enabled us to use a single Jupyter notebook as a template, and modify these parameters as required to test models and generate predictions from target features from any dataset.  \n  \nAs you can see, we used a small set of stopwords for our final solution. This is because removing stopwords actually *decreased* performance on the test set. We suspect that there were insufficient words used in the models - increasing the bank of vocabulary should have improved model performance.","7d45db60":"### Logistic Regression with Two Predictions <div id=lr_map2><\/div>\nNext, we defined** a function for our `Pipeline` that outputs a dataframe \/ Numpy array with 2 columns that represent the 1st and 2nd predictions, respectively. We needed this because the scorer that we fed into `GridSearchCV` (function for performing a grid search for optimal parameters using cross validation) requires exactly that: a dataframe \/ Numpy array with 2 columns.","148d379d":"## Generating the Submission <div id=genSub><\/div>\nThe code below was used to set up a `Pipeline` with the optimal settings and generate a submission.","8c87d414":"Next, we set up `GridSearchCV` to search the parameter grid for an optimal setting using MAP@2 as the evaluation metric. We put the results into a dataframe for easy viewing.  \n  \n**Note:** This took a very long time for target features with many categories like Phone Model. Hence, we eventually gave up and used default settings for the Mobile features with more samples and all Beauty and Fashion features.","e1b6a5e9":"# Overview of Approach <div id=Overview><\/div>\nIn this kernel, we present a walkthrough of our solution for the National Data Science Challenge 2019 (Advanced Category) task of predicting product attributes from text and images. We will use the Camera feature from the Mobile dataset simply because it is the smallest and would require less time to train.\n\nOur approach involved the following steps:  \n  \n1. **Basic Data Cleaning:** Translation of keywords in **training and test set titles**. We defined **keywords** as the words in class labels for the target features.\n2. **Feature Extraction:** Generation of binary term frequency (BTF) using `TfidfVectorizer` for (1) keywords and (2) titles.\n3. **Model Training:** Training of a Logistic Regression model using the OneVsRest scheme.  \n  \nWe used the same approach for **all target features**, with minimal tweaks to the `TfidfVectorizer` and `LogisticRegression` parameters.  "}}