{"cell_type":{"79460f9d":"code","c0000faf":"code","876b33ae":"code","b96f6238":"code","3db673bb":"code","46454d29":"code","e4c4cd4b":"code","8b577db0":"code","f6954cb1":"code","9fc0d776":"code","c9f735e4":"code","68baa8aa":"code","35998923":"code","5b645ad4":"code","a48d4df4":"code","600a8ba1":"code","fc71dd3a":"code","c3e5bc92":"code","2fa7ced2":"code","c48c35f5":"code","50965783":"code","4080edcf":"code","19312a2f":"code","f994900f":"code","37a4e4fb":"code","71001c64":"code","52a9a1d3":"code","c2a6208a":"code","5440631b":"code","a9df8eb9":"code","5f8229a9":"code","6a5f52fc":"code","292d7408":"code","ecba85df":"code","67f1f1fe":"code","95b6ec14":"code","d4468008":"code","0007e028":"code","aeaa653c":"code","fdaa7802":"code","e3814b12":"code","08fff059":"code","d497fe65":"code","7f07e5d2":"code","f22bdcbc":"code","c25e25ae":"code","9c1ec34f":"code","4190aa85":"code","a5395942":"code","aca5e052":"code","a15241bc":"code","fc9d5465":"code","ffe60aac":"code","116b31a2":"code","d133c7ba":"code","992a788f":"code","ef840581":"code","24c3cb37":"code","eb0e73e3":"code","f65e691d":"code","2bfa3500":"markdown","df9d4342":"markdown","32ae709f":"markdown","5a03e6c2":"markdown","f911984e":"markdown","ca1ce897":"markdown","9129902d":"markdown","b8a68a23":"markdown","db908a7e":"markdown","29a33e03":"markdown","78b82a8c":"markdown","e0f77aab":"markdown","418834b8":"markdown","1f8f52e5":"markdown","7077b3a7":"markdown","88354546":"markdown","bec70e22":"markdown","0ab060b3":"markdown","3ee471a2":"markdown","8ce8a9c3":"markdown","574b7c88":"markdown","78b7e923":"markdown","a5ea088f":"markdown","da90e40f":"markdown","f3bb9cf2":"markdown"},"source":{"79460f9d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport glob\nimport json\nimport pickle","c0000faf":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 50)\npd.set_option('display.width', 100)\npd.set_option('display.max_colwidth', -1)","876b33ae":"# Load metadata from Kaggle\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df = meta_df.drop_duplicates().reset_index(drop=True)","b96f6238":"# Clean numbers and special characters\nregex_arr = ['\\d','\\t','\\r','\\)','\\(','\\\/', ':', ';', '&', '#', '-', '\\.']\ndef clean_numbers_and_special_characters (df, col):\n    meta_df[col] = meta_df[col].replace(regex=regex_arr, value='')\n    meta_df[col] = meta_df[col].str.strip()\n    meta_df[col] = meta_df[col].str.lower()\n    return meta_df    \n\nmeta_df = clean_numbers_and_special_characters(meta_df, 'title')\nmeta_df = clean_numbers_and_special_characters(meta_df, 'abstract')\nmeta_df = clean_numbers_and_special_characters(meta_df, 'authors')","3db673bb":"# Obtain file paths for all .json files\nall_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nlen(all_json)","46454d29":"# Remove .xml.json files because most of the these files have duplicate information compared to the .json files\nall_json = [x for x in all_json if 'xml.json' not in x ]\nlen(all_json)","e4c4cd4b":"# Method is needed to group doi into a list to avoid duplicate rows for title, abstract, and body_text\ndef agg_doi(df):\n    no_doi = list(df.copy())\n    no_doi.remove('doi')\n    df_doi = df.groupby(no_doi)['doi'].apply(list).reset_index(name = \"doi\")\n    return df_doi","8b577db0":"class FileReader:\n    def __init__(self, all_json):\n        self.all_json = all_json\n    \n    def extract_all_json(self, meta_df):\n        meta_df = meta_df.fillna('')\n        sha_meta = agg_doi(meta_df[['sha', 'doi']])\n        pmc_meta = agg_doi(meta_df[['pmcid', 'doi']])\n        json_dict = []\n        all_json_div = len(all_json) \/\/ 10\n        for file_path in all_json:\n            with open(file_path) as file:\n                if len(json_dict) % all_json_div == 0:\n                    print(f'{len(json_dict)} of {len(all_json)} json files processed')\n                data = json.load(file)\n                paper_id = data['paper_id']\n                title = data['metadata']['title'].lower().strip()\n                author_list = self._extract_name_list(data)\n                body_text = self._extract_json_text(data, 'body_text')\n                abstract= self._extract_json_text(data, 'abstract')\n                doi = []\n                if len(meta_df[meta_df.sha == paper_id]):\n                    doi = sha_meta[sha_meta.sha == paper_id]['doi'].tolist()[0]\n                    if not title:\n                        if len(meta_df[(meta_df.sha == paper_id) & (~meta_df.abstract.isna())]):\n                            title = meta_df[(meta_df.sha == paper_id) & (~meta_df.title.isna())]['title'].tolist()[0]\n                    if not abstract:\n                        if len(meta_df[(meta_df.sha == paper_id) & (~meta_df.abstract.isna())]):\n                            abstract = meta_df[(meta_df.sha == paper_id) & (~meta_df.abstract.isna())]['abstract'].tolist()[0]\n                elif len(meta_df[meta_df.pmcid == paper_id]):\n                    doi = pmc_meta[pmc_meta.pmcid == paper_id]['doi'].tolist()[0]\n                    if not title:\n                        if len(meta_df[(meta_df.pmcid == paper_id) & (~meta_df.title.isna())]):\n                            title = meta_df[(meta_df.pmcid == paper_id) & (~meta_df.title.isna())]['title'].tolist()[0]\n                    if not abstract:\n                        if len(meta_df[(meta_df.pmcid == paper_id) & (~meta_df.title.isna())]):\n                            abstract = meta_df[(meta_df.pmcid == paper_id) & (~meta_df.title.isna())]['abstract'].tolist()[0]\n                json_dict.append({'paper_id': paper_id,\n                                  'title': title,\n                                  'author_list': author_list,\n                                  'abstract': abstract,\n                                  'body_text': body_text,\n                                  'doi': doi})\n        return pd.DataFrame(json_dict)\n\n    def _extract_name_list(self, data):\n        name_list = []\n        for a in data['metadata']['authors']:\n            first = a['first']\n            middle = ''\n            if a['middle']:\n                middle = a['middle'][0] + ' '\n            last = a['last']\n            name_list.append(f'{first} {middle}{last}')\n        return name_list\n    \n    def _extract_json_text(self, data, key):\n        return ' '.join(str(item['text']).lower().strip() for item in data[key])\n\nfiles = FileReader(all_json)\ndf_covid = files.extract_all_json(meta_df)","f6954cb1":"keywords = ['incident command system',\n            'emergency operations',\n            'joint information center',\n            'social distancing',\n            'childcare closers',\n            'travel advisory',\n            'travel warning',\n            'isolation',\n            'quarantine',\n            'mass gathering cancellations',\n            'school closures',\n            'facility closures'\n            'evacuation',\n            'relocation',\n            'restricting travel',\n            'travel ban',\n            'patient cohort',\n            'npi']","9fc0d776":"all_json_df = df_covid[df_covid['abstract'].str.contains('|'.join(keywords), na=False, regex=True)].reset_index(drop=True)","c9f735e4":"len(all_json_df)","68baa8aa":"all_json_df.to_pickle('data.pkl')","35998923":"# Run this cell to read a previously created pickle file instead of re-running the script\n# all_json_df = pd.read_pickle('\/kaggle\/working\/data.pkl')","5b645ad4":"import nltk\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom collections import Counter\nimport re","a48d4df4":"def remove_punc(df, columns):\n    for col in columns:\n        df[col] = df[col].str.replace('[^a-zA-Z\\s]+','')\n    return df","600a8ba1":"def remove_stopwords(df, columns):\n    stop = stopwords.words('english')\n    for col in columns:\n        df[col] = df[col].astype(str).apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    return df","fc71dd3a":"all_json_df = remove_punc(all_json_df, ['body_text','abstract'])\nall_json_df = remove_stopwords(all_json_df, ['body_text','abstract'])","c3e5bc92":"from sklearn.feature_extraction.text import TfidfVectorizer","2fa7ced2":"def to_tfidf(df, columns):\n    for col in columns:\n        tfidfv = TfidfVectorizer()\n        df[col + '_tfidf'] = list(tfidfv.fit_transform(df[col]).toarray())\n    return df","c48c35f5":"all_json_df = to_tfidf(all_json_df, ['body_text','abstract'])","50965783":"def tokenize(row):\n    title_tokens = []\n    title = row['title']\n    if title == title:\n        title = re.sub('(\/|\\|:|&|#|-|\\.)', '', title)\n        tokens = word_tokenize(title)\n        remove_sw = [word for word in tokens if word not in stopwords.words('english')]\n        remove_numbers = [word for word in remove_sw if not word.isnumeric()]\n        remove_comas = [word for word in remove_numbers if not word in [',', '(', ')', '\"', ':', '``', '.', '?']]\n        title_tokens.extend(remove_comas)\n    return [value[0] for value in Counter(title_tokens).most_common()[0:30]]","4080edcf":"all_json_df['tokens'] = all_json_df.apply(tokenize, axis=1)","19312a2f":"model_df = all_json_df.copy()","f994900f":"LABELED_FILE = '\/kaggle\/input\/labeled-data\/labeled_npi.csv'\ndf_labels = pd.read_csv(LABELED_FILE)","37a4e4fb":"model_df = model_df.merge(df_labels, on=\"title\", how=\"inner\")\nmodel_df = model_df.loc[model_df['isNPI'].notna()]","71001c64":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","52a9a1d3":"def pca_apply(df, columns, n_comp):\n    new_df = df.copy()\n    for col in columns:\n        pca = PCA(n_components=n_comp, random_state=1)\n        new_df[col+'_pca'] = list(pca.fit_transform(np.stack(df[col].to_numpy())))\n    return new_df.reset_index(drop=True)","c2a6208a":"def apply_scaler(df, columns):\n    new_df = df.copy()\n    for col in columns:\n        scaler = StandardScaler()\n        new_df[col + '_scaled'] = list(scaler.fit_transform(np.stack(df[col].to_numpy())))\n    return new_df.reset_index(drop=True)","5440631b":"model_df = pca_apply(model_df, ['abstract_tfidf','body_text_tfidf'], 10)\nmodel_df = apply_scaler(model_df,['abstract_tfidf_pca','body_text_tfidf_pca'])","a9df8eb9":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score","5f8229a9":"# Set X and y for model development\nX = np.stack(model_df['body_text_tfidf_pca_scaled'].to_numpy())\ny = model_df[\"isNPI\"]\n\n# Set the training and testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10, stratify=y)","6a5f52fc":"# Set params and XGBoost classifier\nclf_xgb = xgb.XGBClassifier(max_depth=6, learning_rate=0.1,silent=False, objective='binary:logistic', \\\n                  booster='gbtree', n_jobs=8, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, \\\n                  subsample=0.8, colsample_bytree=0.8, colsample_bylevel=1, reg_alpha=0, reg_lambda=1)\n\n# Fit training sets into the model\nclf_xgb.fit(X_train, y_train)","292d7408":"y_pred = clf_xgb.predict(X_test)\nprecision_recall_fscore_support(y_test, y_pred, average='macro')","ecba85df":"score = accuracy_score(y_test, y_pred)\nprint(f'Accuracy Score: {score}')","67f1f1fe":"core_booster = clf_xgb.get_booster()","95b6ec14":"xgb.plot_importance(core_booster)\nplt.show()","d4468008":"from sklearn.metrics import plot_roc_curve","0007e028":"plot_roc_curve(clf_xgb, X_test, y_test)\nplt.show()","aeaa653c":"final_df = all_json_df.copy()","fdaa7802":"def npi_slice(df):\n    def get_count(row):\n        return sum([row['abstract'].count(keyword) for keyword in keywords])\n    df = df[df.apply(get_count, axis=1) > 1]\n    return df\nfinal_df = npi_slice(final_df)\nlen(final_df)","e3814b12":"# Apply PCE without merging the pre-labeled dataframe. Slicing is done on the full dataset\nfinal_df = pca_apply(final_df, ['abstract_tfidf','body_text_tfidf'], 10)\nfinal_df = apply_scaler(final_df,['abstract_tfidf_pca','body_text_tfidf_pca'])","08fff059":"def npi_col(row):\n    x = np.array([row['body_text_tfidf_pca_scaled']])\n    y_pred = clf_xgb.predict(x)[0]\n    if y_pred > 0:\n        return True\n    return False","d497fe65":"final_df['npi_pred'] = final_df.apply(npi_col, axis=1)\nfinal_df = final_df[final_df['npi_pred']].reset_index(drop=True)\nfinal_df = final_df.drop(columns=['npi_pred'])","7f07e5d2":"len(final_df)","f22bdcbc":"from sklearn.cluster import KMeans","c25e25ae":"def cluster(df, columns, clust_nums):\n    new_df = df.copy()\n    for col in columns:\n        kmeans = KMeans(n_clusters = clust_nums)\n        new_df[col + \"_clusterID\"] = list(kmeans.fit_predict(np.stack(df[col].to_numpy())))\n    return new_df","9c1ec34f":"# Let's try to create 10 clusters\nclustered_df = cluster(final_df, ['abstract_tfidf_pca_scaled', 'body_text_tfidf_pca_scaled'], 10)","4190aa85":"clustered_df.body_text_tfidf_pca_scaled_clusterID.value_counts()","a5395942":"clustered_df[clustered_df.body_text_tfidf_pca_scaled_clusterID == 3][['title']].head()","aca5e052":"# Reduce dimensions for plotting later\ndef reduce_dimension(row):\n    return row[:3]\nclustered_df['abstract_tfidf_pca_scaled'] = clustered_df['abstract_tfidf_pca_scaled'].apply(reduce_dimension)\nclustered_df['body_text_tfidf_pca_scaled'] = clustered_df['body_text_tfidf_pca_scaled'].apply(reduce_dimension)","a15241bc":"!pip install country_list\nfrom country_list import countries_for_language","fc9d5465":"countries = set([k.lower() for k in dict(countries_for_language('en')).values()])","ffe60aac":"def set_country_columns(df):\n    def get_country(row):\n        text_set = set(row['body_text'].split(' '))\n        return list(countries.intersection(text_set))\n    df['countries'] = df.apply(get_country, axis=1)\n    return df\nclustered_df = set_country_columns(clustered_df)","116b31a2":"def get_country_df(df):\n    country = []\n    count = []\n    for k in dict(countries_for_language('en')).values():\n        len_country = len(df[df['countries'].map(set([k.lower()]).issubset)])\n        country.append(k.lower())\n        count.append(len_country)\n    return pd.DataFrame({'country': country, 'count': count})\ncountry_frequency = get_country_df(clustered_df)","d133c7ba":"country_frequency.sort_values(by='count', ascending=False)","992a788f":"import plotly.express as px","ef840581":"fig = px.scatter_geo(country_frequency,\n                     locationmode='country names',\n                     locations='country',\n                     hover_name='country',\n                     size='count',\n                     projection='natural earth')\nfig.update_layout(title='Research Papers Mentioned by Country',\n                  autosize=False,\n                  width=500,\n                  height=250,\n                  paper_bgcolor='rgba(0,0,0,0)'\n                 )\nfig.show()","24c3cb37":"clustered_df[['x', 'y', 'z']] = pd.DataFrame(clustered_df['body_text_tfidf_pca_scaled'].values.tolist(),\n                                             index = clustered_df.index)","eb0e73e3":"fig = px.scatter(clustered_df, x='x', y='y',\n                 color='body_text_tfidf_pca_scaled_clusterID',\n                 hover_name='title',\n                 hover_data=['paper_id', 'doi'])\nfig.update_layout(title = '2D cluster of research papers',\n                  xaxis = dict(dtick=1, range=[-5,5], scaleratio = 1),\n                  yaxis = dict(dtick=1, range=[-5,5], scaleratio = 1),\n                  hoverlabel=dict(\n                    bgcolor='white', \n                    font_size=8, \n                    font_family='Rockwell'\n                  ),\n                  coloraxis=dict(\n                    colorbar=dict(title='Cluster ID')\n                  ))\nfig.show()","f65e691d":"fig = px.scatter_3d(clustered_df, x='x', y='y', z='z',\n                    color='body_text_tfidf_pca_scaled_clusterID',\n                    hover_name='title',\n                    hover_data=['paper_id', 'doi'])\nfig.update_layout(title = '3D cluster of research papers by body_text',\n                  paper_bgcolor='rgba(0,0,0,0)',\n                  scene = dict(\n                    xaxis = dict(dtick=1, range=[-5,5],),\n                    yaxis = dict(dtick=1, range=[-5,5],),\n                    zaxis = dict(dtick=1, range=[-5,5],),),\n                  hoverlabel=dict(\n                    bgcolor='white', \n                    font_size=8, \n                    font_family='Rockwell'\n                  ),\n                  coloraxis=dict(\n                    colorbar=dict(title='Cluster ID')\n                  ))\nfig.show()","2bfa3500":"<a id=\"remove-stopwords\"><\/a>\n### *Remove Stop Words*\nFinding and removing stopwords is vital to removing noise as these words most likely will not contribute to the summary, meaning, scope of the text.","df9d4342":"The visualizations above can be located on the following website. https:\/\/kaggle-covid-npi.herokuapp.com\/\n\nThis site can do the following:\n* Select a 2d or 3d visualize\n* Select Number of clusters to display\n* Select the cluster by cluster id\n* Search by keyword\n* Display text data after click\n* Display geographical map and query 2d\/3d visualization by geo-graph click","32ae709f":"<a id=\"slice-npi\"><\/a>\n### *Slice NPI*\nThe slicing of `all_json_df` will be done in two steps.\n1. Remove all rows that do not have more than 1 occurance of a keyword in the abstract\n2. Remove all rows that are predicted to not be an NPI by the XGBoost model","5a03e6c2":"The code above is to pickle the data for use in an external notebook. As json parsing may take a long time, this notebook can be started from this point on with old data.","f911984e":"<a id=\"tfidf\"><\/a>\n### *TF-IDF Vectorization*\nText data cannot be used in a model as is, so this data needs to be converted. We will use TF-IDF to convert the text into a measure of the relative importance of the words in the text.","ca1ce897":"<a id=\"clustering\"><\/a>\n# **Clustering**","9129902d":"<a id=\"first-slice\"><\/a>\n### *Slice by Keywords*\nThrough online research and interviewing healthcare providers, we generated a list of keywords to train a classifier to identify NPI-related papers. This first pass of data slicing was performed on paper abstracts, as titles were not consistently populated, and body text contained a lower signal to noise ratio.","b8a68a23":"<a id=\"text-filtering\"><\/a>\n# **Text Filtering**","db908a7e":"<a id=\"conclusion\"><\/a>\n# **Conclusion**\nMachine learning methods have enabled us to structure and make explorable vast amounts of data. As detailed above, we have produced a classifier-fed clustering alogrithm which enables users to specify the category across which to find clusters (in this case, the body text or abstract of papers), specify number of clusters, and visually represent each cluster spatially amongst the others. By enabling researchers to search for specific terms and see matching and adjacent publications to provide context to their understandings, we produce natural and interpretable groupings of knowledge.\n\nProgress in the fight against COVID-19 will be fought on many fronts. First responders, manufacturers, essential workers, researchers, governmental bodies, and more all each have their part to play. Thankfully, we are seeing global collaboration at all time highs, but alongside this, we are seeing masses of data being produced. Unfortunately, the sheer volume of this research means that it cannot possible be digested and synthesized by any individual. Researchers, government officials, and medical professionals might benefit from having certain information, but the forest is too dense. By allowing these parties to aggregate, visualize, and parse this mass of data, we hope to enable new insights and initiatives to be taken.","29a33e03":"<a id=\"introduction\"><\/a>\n# **Introduction**\n**Coronavirus Disease 2019 (COVID-19)** was first identified in December 2019 in Wuhan, the capital of China's Hubei province and has since then spread to be identified as a Global Pandemic.\n\n### *What is COVID-19*:\nCOVID-19 is a novel coronavirus which expresses itself in humans as a respiratory illness.\n\n### *How does it spread:*\nAccording to the World Health Organization as of 15\/04\/2020 \"This disease can spread from person to person through small droplets from the nose or mouth which are spread when a person with COVID-19 coughs or exhales. These droplets land on objects and surfaces around the person. Other people then catch COVID-19 by touching these objects or surfaces, then touching their eyes, nose or mouth. People can also catch COVID-19 if they breathe in droplets from a person with COVID-19 who coughs out or exhales droplets.\"\n\nThe question that is on everyones mind is **\"How could we have prevented the spread?\"**\nWe hope to allow the research to answer this.\n\n### *What can we do to stop it*:\n*For the purpose of this challenge we will be focusing on Non-pharmaceutical Interventions in preventing the spread of COVID-19.*\n\nAs per the CDC: **Non-pharmaceutical Interventions(NPIs)** are actions, apart from getting vaccinated and taking medicine, that people and communities can take to help slow the spread of illnesses like pandemic influenza (flu).\n\nNPI's are typically broken up into 3 categories listed with examples:\n\n    1. Personal: \n        -Staying home when you are sick\n        -Covering coughs and sneezes with a tissue\n        -Washing hands with soap and water or using hand sanitizer when soap and water is not available\n    2. Community:\n        -Social Distancing\n        -Closures (child care centers, schools, places of worship, sporting events, etc)\n    3. Environmental:\n        -Routine surface cleaning \n\nThe goal of our submission is to analyze the research done on the implementation, efficacy and scaling of NPI's, isolate these papers from other COVID-19 research articles and derive meaningful insights to help us mitigate the existing situaiton and prepare our systems for any future pandemic which may come our way.\n\n<a id=\"goals\"><\/a>\n# Goals:\nThe goal of our submission is to analyze the research done on the implementation, efficacy and scaling of NPI's, isolate these papers from other COVID-19 research articles and derive meaningful insights to help us mitigate the existing situation and prepare our systems for any future pandemic which may come our way.\n\nWe aimed to create a tool to allow interested parties to visualize and investigate patterns in published data, in this case focusing on non-pharmaceutical interventions to address pandemics. This tool can be adapted to other subject areas by adapting the training of the classifier and keywords used. Through a series of clustering methods, we hope to provide users a more focused means of asking questions of large datasets, and understanding relationships between publications. Ultimately, researchers should be able to learn from the vast amounts of papers being published, not get lost in the flood, or have their work go unused.\n\n<a id=\"approach\"><\/a>\n# Approach:\nTo tackle this challenge, we created a classifier to front-load the work of sorting NPI vs. non-NPI articles. This aided us in refining our dataset in preparation for cleaning and standardization, as well as improved the outcomes of our clustering model. As a tool which needs to be trusted in crisis situations, we embedded a human-in-the-loop mechanism, in the form of label definition for the classifier. By using a supervised and human-driven method up front, we can train the model to ensure that only relevant articles are being aggregated. Clustering can then begin, with the goal of placing cluster centers as far apart as possible. We then allow a second step of human guidance by enabling users to investigate the components and metadata of clusters, so they understand what they are looking at on both micro and macro levels. Users can further refine model output by setting cluster parameters, as described below.\n","78b82a8c":"https:\/\/www.cdc.gov\/nonpharmaceutical-interventions\/index.html\n\nhttps:\/\/www.who.int\/news-room\/q-a-detail\/q-a-coronaviruses\n\nhttps:\/\/www.cdc.gov\/nonpharmaceutical-interventions\/tools-resources\/published-research.html\n\nhttps:\/\/www.azdhs.gov\/documents\/preparedness\/emergency-preparedness\/response-plans\/adhs-npi-playbook.pdf\n\nhttps:\/\/www.imperial.ac.uk\/media\/imperial-college\/medicine\/sph\/ide\/gida-fellowships\/Imperial-College-COVID19-NPI-modelling-16-03-2020.pdf\n\nhttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3291414\/\n\nhttps:\/\/www.medrxiv.org\/content\/10.1101\/2020.03.03.20029843v3\n\nhttps:\/\/bmcpublichealth.biomedcentral.com\/articles\/10.1186\/1471-2458-14-1328\n\nhttps:\/\/www.who.int\/influenza\/resources\/documents\/RapidContProtOct15.pdf\n\nhttps:\/\/stacks.cdc.gov\/view\/cdc\/11425","e0f77aab":"# **COVID-19 NPI Clustering**\nYou can find the complete version of our visualization tool here https:\/\/kaggle-covid-npi.herokuapp.com\/","418834b8":"<a id=\"npi-predict\"><\/a>\n# **NPI Prediction**\nIn this section, a copy of `all_json_df` is created to be used to predict NPI. With the help of several people, 1300+ papers were identified as NPI, non-NPI or NULL. We will merge this dataset with the copied dataframe to model NPI. Thereafter, the data from all_json_df will be used to predict NPI based on that model.","1f8f52e5":"The title, abstract and author columns have characters that are not needs for this analysis and the remove of such characters (e.g. :, &, -) is imperative to run the models in this notebook","7077b3a7":"<a id=\"visualization\"><\/a>\n# **Visualization**","88354546":"<a id=\"load-metadata\"><\/a>\n### *Load Metadata*\nThe code to load the metadata is based on the notebook by Ivan Ega Pratama, from Kaggle.\n#### Source: [COVID EDA: Initial Exploration Tool](https:\/\/www.kaggle.com\/ivanegapratama\/covid-eda-initial-exploration-tool)","bec70e22":"<a id=\"label-npi\"><\/a>\n### *Label NPI*","0ab060b3":"<a id=\"xgboost\"><\/a>\n### *XGBoost Model*","3ee471a2":"<a id=\"citation\"><\/a>\n# **Citation\/Sources**","8ce8a9c3":"<a id=\"load-jsondata\"><\/a>\n### *Load JSON Papers*\nThe code to fetch all the json data and load into a dataframe is loosely based on the same notebook by Ivan Ega Pratama. JSON files were not only parsed but were merged with the metadata to create a dataframe to be used for the rest of this notebook.","574b7c88":"<a id=\"tokens\"><\/a>\n### *Tokenize Title*\nLet's create a list of the top keywords from each paper. The tokenize method below will do just that.","78b7e923":"<a id=\"pca\"><\/a>\n### *PCA*\nFor the vectorized data, we will use Principle Component Analysis (PCA) to reduce the dimensions of the data.","a5ea088f":"<a id=\"load-data\"><\/a>\n# **Load the Data** ","da90e40f":"<a id=\"geolocation\"><\/a>\n# **Geolocation**","f3bb9cf2":"# **Table of Contents**\n\n1. [Introduction](#introduction)\n1. [Goals](#goals)\n1. [Approach](#approach)\n1. [Load the Data](#load-data)\n    * [Load Metadata](#load-metadata)\n    * [Load JSON Papers](#load-jsondata)\n    * [Slice by Keywords](#first-slice)\n1. [Text Filtering](#text-filtering)\n    * [Remove Stop Words](#remove-stopwords)\n    * [TF-IDF Vectorization](#tfidf)\n    * [Tokenize Title](#tokens)\n1. [NPI Prediction](#npi-predict)\n    * [Label NPI](#label-npi)\n    * [PCA](#pca)\n    * [XG Boost Model](#xgboost)\n    * [Slice NPI](#slice-npi)\n1. [Clustering](#clustering)\n1. [Geolocation](#geolocation)\n1. [Visualization](#visualization)\n1. [Conclusion](#conclusion)\n1. [Citation\/Sources](#citation)"}}