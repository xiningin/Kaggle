{"cell_type":{"ca6bcedb":"code","053d7112":"code","6e87b3b1":"code","c9f0378b":"code","baf3837e":"code","ed9be19c":"code","b750c433":"code","953c0dcf":"code","96e38138":"code","6c6a5296":"code","36565366":"code","b4547f4d":"code","da54e506":"code","1121b71c":"code","30b94194":"code","9593c79e":"code","36374c77":"code","bfc7c775":"code","fe8520f2":"code","4c45202d":"code","abbdd848":"code","9277c085":"code","6baa8bed":"code","c2d18550":"code","b2a93644":"code","07a6324c":"code","75ae8f31":"code","770df299":"code","7a9b932c":"code","7f5fceb9":"code","50d1b836":"code","fe327341":"code","2621f511":"code","df37377c":"code","37edb038":"code","62f16794":"code","4d3c7b8b":"code","3b52986b":"code","bcfe9f9a":"code","40952b29":"code","3df7fabd":"markdown","be659301":"markdown","87077ef7":"markdown","44e00f70":"markdown","a443e041":"markdown","137a1c1c":"markdown","64a96089":"markdown","06345852":"markdown"},"source":{"ca6bcedb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error","053d7112":"\n# Loading the dataset\ndf = pd.read_csv(\"\/kaggle\/input\/vehicle-dataset-from-cardekho\/CAR DETAILS FROM CAR DEKHO.csv\")","6e87b3b1":"df.shape","c9f0378b":"df.dtypes","baf3837e":"final_df = df[['year', 'selling_price', 'km_driven', 'fuel', 'seller_type', 'transmission', 'owner']]","ed9be19c":"final_df['owner'].value_counts()","b750c433":"final_df.drop(final_df[final_df['owner']=='Test Drive Car'].index,axis=0,inplace=True)","953c0dcf":"final_df['No_of_previous_owner'] = final_df['owner'].map({'First Owner':1,'Second Owner':2,'Third Owner':3,\"Fourth & Above\":4})","96e38138":"final_df.head()","6c6a5296":"final_df.drop(final_df[final_df['seller_type']=='Trustmark Dealer'].index,axis=0,inplace=True)","36565366":"final_df['Current Year'] = 2020\nfinal_df['No_of_Years'] = final_df['Current Year'] - final_df['year']\nfinal_df.drop(['year','Current Year'],axis=1,inplace=True)","b4547f4d":"# percentage of missing values in each column\nround(100*(final_df.isnull().sum()\/len(final_df)),2).sort_values(ascending = False)","da54e506":"# percentage of missing values in each row\nround(100*(final_df.isnull().sum(axis=1)\/len(final_df)),2).sort_values(ascending = False)","1121b71c":"final_df_dup=final_df.copy()\n# Checking for duplicates and dropping the entire duplicate row if any\nfinal_df_dup.drop_duplicates(subset=None, inplace=True)","30b94194":"final_df.shape","9593c79e":"final_df_dup.shape","36374c77":"final_df=final_df_dup\nfinal_df.head()","bfc7c775":"final_df = pd.get_dummies(final_df,drop_first=True)","fe8520f2":"final_df.head()","4c45202d":"# Seprating the dependent variable and target variable\ny = final_df['selling_price']\nX = final_df.drop('selling_price',axis=1)","abbdd848":"# Splitting training and testing data\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=5)","9277c085":"# Building a machine learning model using Random Forest Regressor\nregressor = RandomForestRegressor()","6baa8bed":"from sklearn.model_selection import RandomizedSearchCV","c2d18550":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","b2a93644":"random_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)","07a6324c":"# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = regressor, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = 1)","75ae8f31":"final_df.isnull().sum()","770df299":"# Training the data\nX_train = X_train.fillna(X_train.mean())\ny_train = y_train.fillna(y_train.mean())\nX_test = X_test.fillna(X_test.mean())\ny_test = y_test.fillna(y_test.mean())\nrf_random.fit(X_train,y_train)","7a9b932c":"rf_random.best_params_","7f5fceb9":"y_pred = rf_random.predict(X_test)\nerrors = abs(y_pred - y_test)\nmape = 100 * np.mean(errors \/ y_test)\naccuracy = 100 - mape\nprint('Model Performance')\nprint('MAE:',mean_absolute_error(y_test,y_pred))\nprint('MSE:', mean_squared_error(y_test,y_pred))\nprint('RMSE:', np.sqrt(mean_squared_error(y_test,y_pred)))\nprint('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\nprint('Accuracy = {:0.2f}%.'.format(accuracy))\n    ","50d1b836":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors \/ test_labels)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('MAE:',mean_absolute_error(y_test,y_pred))\n    print('MSE:', mean_squared_error(y_test,y_pred))\n    print('RMSE:', np.sqrt(mean_squared_error(y_test,y_pred)))\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy","fe327341":"base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\nbase_model.fit(X_train, y_train)\nbase_accuracy = evaluate(base_model, X_test, y_test)","2621f511":"best_random = rf_random.best_estimator_\nrandom_accuracy = evaluate(best_random, X_test, y_test)","df37377c":"print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) \/ base_accuracy))","37edb038":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","62f16794":"# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_","4d3c7b8b":"best_grid = grid_search.best_estimator_\ngrid_accuracy = evaluate(best_grid, X_test, y_test)","3b52986b":"print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) \/ base_accuracy))","bcfe9f9a":"# Create a new dataframe of only numeric variables:\n\ncar_n=final_df[[ 'selling_price', 'km_driven', 'No_of_Years']]\n\nsns.pairplot(car_n, diag_kind='kde')\nplt.show()","40952b29":"# Let's check the correlation coefficients to see which variables are highly correlated. Note:\n# here we are considering only those variables (dataframe: car) that were chosen for analysis\n\nplt.figure(figsize = (25,20))\nsns.heatmap(final_df.corr(), annot = True, cmap=\"RdBu\")\nplt.show()","3df7fabd":"# Correlation Matrix","be659301":"On each iteration, the algorithm will choose a difference combination of the features. Altogether, there are 2 * 12 * 2 * 3 * 3 * 10 = 4320 settings! However, the benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values.","87077ef7":"# Plots","44e00f70":"# Hyperparameter optimization using Randomized Search CV","a443e041":"# Evaluate Random Search\nTo determine if random search yielded a better model, we compare the base model with the best random search model.","137a1c1c":"# Hyperparameter Optimization with Grid Search Cross Validation\nRandom search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define.","64a96089":"The most important arguments in RandomizedSearchCV are n_iter, which controls the number of different combinations to try, and cv which is the number of folds to use for cross validation (we use 100 and 3 respectively). More iterations will cover a wider search space and more cv folds reduces the chances of overfitting, but raising each will increase the run time. Machine learning is a field of trade-offs, and performance vs time is one of the most fundamental.","06345852":"This will try out 1 * 4 * 2 * 3 * 3 * 4 = 288 combinations of settings. We can fit the model, display the best hyperparameters, and evaluate performance:"}}