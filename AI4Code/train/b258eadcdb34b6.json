{"cell_type":{"8fc6a2ef":"code","c4515677":"code","ec69d62f":"code","61e58d9c":"code","baab7851":"code","7b536639":"code","88e5cdb9":"code","1d729a18":"code","9b909f98":"code","90b005e7":"code","9c5ce3f7":"code","fa4ca405":"code","964bc2a4":"code","31cb5cc9":"markdown","f0a8cf54":"markdown","c3301432":"markdown","698077eb":"markdown","2aed658c":"markdown","59d5cdf6":"markdown","4fcecc5f":"markdown","7b173ef7":"markdown","abd45747":"markdown","efeed125":"markdown","fd9d470b":"markdown","f5906846":"markdown","e5234a69":"markdown","e2efb25e":"markdown"},"source":{"8fc6a2ef":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\nks = pd.read_csv('..\/input\/kickstarter-projects\/ks-projects-201801.csv',\n                 parse_dates=['deadline', 'launched'])\n\n# Drop live projects\nks = ks.query('state != \"live\"')\n\n# Add outcome column, \"successful\" == 1, others are 0\nks = ks.assign(outcome=(ks['state'] == 'successful').astype(int))\n\n# Timestamp features\nks = ks.assign(hour=ks.launched.dt.hour,\n               day=ks.launched.dt.day,\n               month=ks.launched.dt.month,\n               year=ks.launched.dt.year)\n\n# Label encoding\ncat_features = ['category', 'currency', 'country']\nencoder = LabelEncoder()\nencoded = ks[cat_features].apply(encoder.fit_transform)\n\ndata_cols = ['goal', 'hour', 'day', 'month', 'year', 'outcome']\nbaseline_data = ks[data_cols].join(encoded)","c4515677":"# Defining  functions that will help us test our encodings\nimport lightgbm as lgb\nfrom sklearn import metrics\n\ndef get_data_splits(dataframe, valid_fraction=0.1):\n    valid_fraction = 0.1\n    valid_size = int(len(dataframe) * valid_fraction)\n\n    train = dataframe[:-valid_size * 2]\n    # valid size == test size, last two sections of the data\n    valid = dataframe[-valid_size * 2:-valid_size]\n    test = dataframe[-valid_size:]\n    \n    return train, valid, test\n\ndef train_model(train, valid):\n    feature_cols = train.columns.drop('outcome')\n\n    dtrain = lgb.Dataset(train[feature_cols], label=train['outcome'])\n    dvalid = lgb.Dataset(valid[feature_cols], label=valid['outcome'])\n\n    param = {'num_leaves': 64, 'objective': 'binary', \n             'metric': 'auc', 'seed': 7}\n    print(\"Training model!\")\n    bst = lgb.train(param, dtrain, num_boost_round=1000, valid_sets=[dvalid], \n                    early_stopping_rounds=10, verbose_eval=False)\n\n    valid_pred = bst.predict(valid[feature_cols])\n    valid_score = metrics.roc_auc_score(valid['outcome'], valid_pred)\n    print(f\"Validation AUC score: {valid_score:.4f}\")\n    return bst","ec69d62f":"# Training a model on the baseline data\ntrain, valid, _ = get_data_splits(baseline_data)\nbst = train_model(train, valid)","61e58d9c":"import category_encoders as ce\ncat_features = ['category', 'currency', 'country']\ncount_enc = ce.CountEncoder()\ncount_encoded = count_enc.fit_transform(ks[cat_features])\n\ndata = baseline_data.join(count_encoded.add_suffix(\"_count\"))\n\n# Training a model on the baseline data\ntrain, valid, test = get_data_splits(data)\nbst = train_model(train, valid)","baab7851":"import category_encoders as ce\ncat_features = ['category', 'currency', 'country']\n\n# Create the encoder itself\ntarget_enc = ce.TargetEncoder(cols=cat_features)\n\ntrain, valid, _ = get_data_splits(data)\n\n# Fit the encoder using the categorical features and target\ntarget_enc.fit(train[cat_features], train['outcome'])\n\n# Transform the features, rename the columns with _target suffix, and join to dataframe\ntrain = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\nvalid = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_target'))\n\ntrain.head()\nbst = train_model(train, valid)","7b536639":"cat_features = ['category', 'currency', 'country']\ntarget_enc = ce.CatBoostEncoder(cols=cat_features)\n\ntrain, valid, _ = get_data_splits(data)\ntarget_enc.fit(train[cat_features], train['outcome'])\n\ntrain = train.join(target_enc.transform(train[cat_features]).add_suffix('_cb'))\nvalid = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_cb'))\n\nbst = train_model(train, valid)","88e5cdb9":"from sklearn.decomposition import TruncatedSVD\n\n# Use 3 components in the latent vectors\nsvd = TruncatedSVD(n_components=3)","1d729a18":"train, valid, _ = get_data_splits(data)\n\n# Create a sparse matrix with cooccurence counts\npair_counts = train.groupby(['country', 'category'])['outcome'].count()\npair_counts.head(10)","9b909f98":"pair_matrix = pair_counts.unstack(fill_value=0)\npair_matrix","90b005e7":"svd_encoding = pd.DataFrame(svd.fit_transform(pair_matrix))\nsvd_encoding.head(10)","9c5ce3f7":"encoded = svd_encoding.reindex(data['country']).set_index(data.index)\nencoded.head(10)","fa4ca405":"# Join encoded feature to the dataframe, with info in the column names\ndata_svd = data.join(encoded.add_prefix(\"country_category_svd_\"))\ndata_svd.head()","964bc2a4":"train, valid, _ = get_data_splits(data_svd)\nbst = train_model(train, valid)","31cb5cc9":"# Count Encoding\n\nCount encoding replaces each categorical value with the number of times it appears in the dataset. For example, if the value \"GB\" occured 10 times in the country feature, then each \"GB\" would be replaced with the number 10.\n\nWe'll use the [`categorical-encodings` package](https:\/\/github.com\/scikit-learn-contrib\/categorical-encoding) to get this encoding. The encoder itself is available as `CountEncoder`. This encoder and the others in `categorical-encodings` work like scikit-learn transformers with `.fit` and `.transform` methods.","f0a8cf54":"**[Feature Engineering Home Page](https:\/\/www.kaggle.com\/learn\/feature-engineering)**\n\n---\n","c3301432":"# Introduction\n\nNow that you've built a baseline model, you are ready to improve it with some clever ways to convert categorical variables into numerical features. These encodings will be learned from the data itself. The most basic encodings are one-hot encoding (covered in the [Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning) course) and label encoding which you saw in the first tutorial. \n\nHere, you'll learn count encoding, target encoding (and variations), and singular value decomposition.\n\nHere is the code to rebuild the baseline model from the first tutorial.","698077eb":"The validation score is higher again, from 0.7467 to 0.7491.","2aed658c":"This is the best score yet, 0.7495 compared to baseline of 0.7467. In practice you'd create these encodings for each pair of categorical variables, likely improving the score even more.\n\n# Your Turn\nTry **[encoding categorical features](https:\/\/www.kaggle.com\/kernels\/fork\/1)** yourself\n","59d5cdf6":"First we can use `.groupby` to count up co-occurences for any pair of features.","4fcecc5f":"# Encoding with Singular Value Decomposition\n\nNow we'll use singular value decomposition (SVD) to learn encodings from pairs of categorical features. SVD is more complex than the other encodings you've learned, but it can also be the most effective. We'll construct a matrix of co-occurences for each pair of categorical features. Each row corresponds to a value in feature A, while each column corresponds to a value in feature B. Each element is the count of rows where the value in A appears together with the value in B.\n\nYou then use singular value decomposition to find two smaller matrices that equal the count matrix when multiplied.\n\n<center><img src=\"https:\/\/i.imgur.com\/mnnsBKJ.png\" width=600px><\/center>\n\nYou can choose how long each factor vector will be. Longer vectors will contain more information at the cost of more memory\/computation. To get the encodings for feature A, you multiply the count matrix by the small matrix for feature B.\n\nI'll show you how you can do this for one pair of features using scikit-learn's `TruncatedSVD` class.","7b173ef7":"---\n**[Feature Engineering Home Page](https:\/\/www.kaggle.com\/learn\/feature-engineering)**\n\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https:\/\/www.kaggle.com\/learn-forum) to chat with other Learners.*","abd45747":"# CatBoost Encoding\n\nFinally, we'll look at CatBoost encoding. This is similar to target encoding in that it's based on the target probablity for a given value. However with CatBoost, for each row, the target probability is calculated only from the rows before it.","efeed125":"Now we have a series with a two-level index. We want to convert this into a matrix with `country` on one axis and `category` on the other. To do this, we can use `.unstack`. By default it'll put `NaN`s where data doesn't exist, but we can tell it to fill those spots with zeros.","fd9d470b":"This gives us a mapping of the values in the country feature, the index of the dataframe, to our encoded vectors. Next, we need to replace the values in our data with these vectors. We can do this using the `.reindex` method. This method takes the values in the country column and creates a new dataframe from from `svd_encoding` using those values as the index. Then we need to set the index back to the original index. Note that I learned the encodings from the training data, but I'm applying them to the whole dataset.","f5906846":"Adding the count encoding features increase the validation score from 0.7467 to 0.7486, only a slight improvement.","e5234a69":"This does slightly better than target encoding.","e2efb25e":"# Target Encoding\n\nTarget encoding replaces a categorical value with the average value of the target for that value of the feature. For example, given the country value \"CA\", you'd calculate the average outcome for all the rows with `country == 'CA'`, around 0.28. This is often blended with the target probability over the entire dataset to reduce the variance of values with few occurences.\n\nThis technique uses the targets to create new features. So including the validation or test data in the target encodings would be a form of target leakage. Instead, you should learn the target encodings from the training dataset only and apply it to the other datasets.\n\nThe `category_encoders` package provides `TargetEncoder` for target encoding. The implementation is similar to `CountEncoder`."}}