{"cell_type":{"1933fc5f":"code","333a86bf":"code","0019edd7":"code","c9ea6bd0":"code","3fb6d742":"code","fb1ae276":"code","e1c4fd7f":"code","62aa5887":"code","df91c547":"code","7fb3f07f":"code","aeaea547":"code","ebf816a1":"code","a8bb76b5":"code","4d52b582":"code","5327a394":"code","26a890e5":"code","35f773dc":"code","1f7a26f2":"code","33ad21c6":"code","cec84eda":"code","b9ed2a24":"code","7c352204":"code","941b504d":"code","81112b01":"code","30f7bca6":"code","ae18cfa5":"code","104d6f49":"code","67361c66":"code","8ebf39c6":"code","373efc53":"code","24bf17e0":"markdown","c523df88":"markdown","f76afcf1":"markdown","4e598032":"markdown","e08f2135":"markdown","b09d5201":"markdown","3bd3b8a2":"markdown","10c908e2":"markdown","b1252c97":"markdown","1c0b4a6c":"markdown","9397b3ba":"markdown","2f163401":"markdown","cf198e2d":"markdown","c4d9ce5c":"markdown","399dca7c":"markdown","988a712b":"markdown","bc278210":"markdown","c97f3ea0":"markdown","fe7ce4a1":"markdown","4beaeffd":"markdown","984e5f36":"markdown","18b61ef4":"markdown","29d96cdf":"markdown","a654e03a":"markdown","42e7e0ef":"markdown","c2f09cee":"markdown","db23677e":"markdown","0c0797f5":"markdown","13a67205":"markdown","29a810d1":"markdown","24d2e7d4":"markdown"},"source":{"1933fc5f":"! pip install textstat > \/dev\/null\n! pip install gensim==3.8.3 > \/dev\/null\n! pip install pyLDAvis==2.1.2 > \/dev\/null\n! pip install spacy==2.3.0 > \/dev\/null\n! pip install textacy > \/dev\/null\n! python -m spacy download en_core_web_lg","333a86bf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport spacy\nimport textstat\n\nfrom spacy import displacy\nfrom collections import Counter\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nfrom sklearn import linear_model\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.feature_extraction import text\n\nfrom scipy.sparse import csc_matrix\n\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\nimport pyLDAvis\nimport pyLDAvis.gensim\n\nimport textacy\nfrom textacy import text_stats\n\npyLDAvis.enable_notebook()\nnlp = spacy.load('en_core_web_lg')","0019edd7":"df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ndf.tail()","c9ea6bd0":"df.info()","3fb6d742":"fig = go.Figure()\n\nfig.add_trace(go.Violin(y=df.target[(df.url_legal.notnull()) & (df.license.notnull())],\n                        side='negative',\n                        name='URL & License present',\n                        marker_color='#00CC96'))\n\nfig.add_trace(go.Violin(y=df.target[(df.url_legal.isnull()) & (df.license.isnull())],\n                        side='positive',\n                        name='URL & License not present',\n                        marker_color='#FF6692'))\n\nfig.update_traces(box_visible=True,\n                  meanline_visible=True,\n                  points='all')\n\nfig.update_layout(violinmode='overlay',\n                  violingap=0,\n                  title={\n                      'text': 'Impact of Presence of URL and license on readability score',\n                      'x': 0.5,\n                      'y': 0.9\n                  },\n                  yaxis_title='Readability Score')\n\nfig.show()","fb1ae276":"fig = ff.create_distplot([df.target.values], group_labels=['Readability Score'], bin_size=.2, colors=['lightseagreen'])\nfig.update_layout(title={\n                     'text': 'Distribution of Readability Score',\n                     'x': 0.5,\n                     'y': 0.9\n                 })\nfig.show()","e1c4fd7f":"print(df.id[df.target == 0])\ndf.excerpt[df.target == 0].values","62aa5887":"fig = go.Figure()\nfig.add_trace(go.Violin(y=df.target,\n                     box_visible=True,\n                     points='all',\n                     meanline_visible=True,\n                     marker_color='#E377C2',\n                     name='Target'))\nfig.update_layout(yaxis_title='Readability Score',\n                 title={\n                     'text': 'Distribution of Readability Score',\n                     'x': 0.5,\n                     'y': 0.9\n                 })\nfig.show()","df91c547":"fig = px.scatter(df[df.target != 0], x='target', y='standard_error', color='standard_error')\n\nfig.update_layout(yaxis_title='Standard Error',\n                  xaxis_title='Readability Score',\n                 title={\n                     'text': 'Readability Score Vs Standard Error',\n                     'x': 0.5,\n                     'y': 0.95\n                 },\n                 legend_title='Standard Error')\nfig.show()","7fb3f07f":"def get_word_count(text):\n    return textstat.lexicon_count(text, removepunct=True)\n\ndf['word_count'] = df.excerpt.apply(get_word_count)\ndf['sentence_count'] = df.excerpt.apply(textstat.sentence_count)","aeaea547":"fig = make_subplots(rows=1, cols=2)\n\nfig.add_trace(go.Histogram(x=df['word_count'],\n                           name='Word Count',\n                           marker_color='#AB63FA'), row=1, col=1)\n\nfig.add_trace(go.Histogram(x=df['sentence_count'],\n                           name='Sentence Count',\n                           marker_color='#FF6692'), row=1, col=2)\n\nfig.update_layout(title={\n                    'text': 'Count of Words and Sentences across Corpus',\n                    'x': 0.5,\n                    'y': 0.9    \n                  },\n                  xaxis_title='Count',\n                  yaxis_title='Frequency')\nfig.show()","ebf816a1":"# Average number of words per sentence in each of the excerpts\ndef avg_word_count(text):\n    return textstat.lexicon_count(text, removepunct=True) \/ textstat.sentence_count(text)\n\ndf['avg_word_count'] = df.excerpt.apply(avg_word_count)","a8bb76b5":"fig = go.Figure()\n\nfig.add_trace(go.Histogram(x=df['avg_word_count'][df.target >= df.target.median()],\n                           name='Readable Texts'))\n\nfig.add_trace(go.Histogram(x=df['avg_word_count'][df.target < df.target.median()],\n                           name='Difficult Texts'))\n\nfig.update_layout(title={\n                    'text': 'Average Number of Words per Sentence',\n                    'x': 0.5,\n                    'y': 0.9    \n                  },\n                  xaxis_title='Average number of words',\n                  yaxis_title='Frequency')\nfig.show()","4d52b582":"# Count of Rare\/Difficult words in excerpts\ndf['difficult_words'] = df.excerpt.apply(textstat.difficult_words)\ndf['difficult_word_ratio'] = df['difficult_words'] \/ df['word_count']","5327a394":"fig = go.Figure()\n\nfig.add_trace(go.Histogram(x=df['difficult_word_ratio'][df.target >= df.target.median()],\n                           name='Readable Texts',\n                           marker_color='#54A24B'))\n\nfig.add_trace(go.Histogram(x=df['difficult_word_ratio'][df.target < df.target.median()],\n                           name='Difficult Texts',\n                           marker_color='#F58518'))\n\nfig.update_traces(opacity=0.75)\nfig.update_layout(title={\n                    'text': 'Impact of Presence of Sophisticated Words',\n                    'x': 0.5,\n                    'y': 0.9    \n                  },\n                  xaxis_title='Sophisticated Word Ratio',\n                  yaxis_title='Frequency',\n                  barmode='overlay')\nfig.show()","26a890e5":"df['flesch_reading_ease'] = df.excerpt.apply(textstat.flesch_reading_ease)\ndf['smog_index'] = df.excerpt.apply(textstat.smog_index)\ndf['flesch_kincaid_grade'] = df.excerpt.apply(textstat.flesch_kincaid_grade)\ndf['coleman_liau_index'] = df.excerpt.apply(textstat.coleman_liau_index)\ndf['automated_readability_index'] = df.excerpt.apply(textstat.automated_readability_index)\ndf['dale_chall_readability_score'] = df.excerpt.apply(textstat.dale_chall_readability_score)\ndf['linsear_write_formula'] = df.excerpt.apply(textstat.linsear_write_formula)\ndf['gunning_fog'] = df.excerpt.apply(textstat.gunning_fog)\ndf['text_standard'] = df.excerpt.apply(textstat.text_standard)\ndf['szigriszt_pazos'] = df.excerpt.apply(textstat.szigriszt_pazos)\ndf['fernandez_huerta'] = df.excerpt.apply(textstat.fernandez_huerta)\ndf['gutierrez_polini'] = df.excerpt.apply(textstat.gutierrez_polini)\ndf['crawford'] = df.excerpt.apply(textstat.crawford)","35f773dc":"fig = px.scatter_matrix(df,\n                        dimensions=['target', 'difficult_word_ratio', 'avg_word_count', 'dale_chall_readability_score'],\n                        color='target')\n\nfig.update_layout(title={\n                    'text': 'Prominent Traditional Features Vs Readability Score',\n                    'x': 0.5\n                  })\nfig.show()","1f7a26f2":"px.imshow(df.corr(), width=800, height=800, color_continuous_scale='Aggrnyl')","33ad21c6":"def add_misc_ling_features(df):\n    for idx, row in df.iterrows():\n        doc = textacy.make_spacy_doc(row['excerpt'], lang='en_core_web_lg')\n        ts = text_stats.TextStats(doc)\n        df.loc[idx, 'n_unique_words'] = ts.n_unique_words\n        df.loc[idx, 'n_unique_words_per_sent'] = ts.n_unique_words \/ ts.n_sents\n        df.loc[idx, 'n_chars_per_word'] = ts.n_chars \/ ts.n_words\n        df.loc[idx, 'n_syllables'] = ts.n_syllables\n        df.loc[idx, 'n_syllables_per_word'] = ts.n_syllables \/ ts.n_words\n        df.loc[idx, 'n_syllables_per_sent'] = ts.n_syllables \/ ts.n_sents\n        df.loc[idx, 'n_monosyllable_words'] = ts.n_monosyllable_words\n        df.loc[idx, 'n_polysyllable_words'] = ts.n_polysyllable_words\n        df.loc[idx, 'n_long_words'] = ts.n_long_words\n        df.loc[idx, 'entropy'] = ts.entropy\n        \n    return df\n\ndf = add_misc_ling_features(df)\n# NLP aug - augmentation\n# Topics ","cec84eda":"df.head()","b9ed2a24":"def tree_height(root):\n    if not list(root.children):\n        return 1\n    else:\n        return 1 + max(tree_height(x) for x in root.children)\n\n\ndef get_average_height(paragraph):\n    if type(paragraph) == str:\n        doc = nlp(paragraph)\n    else:\n        doc = paragraph\n    roots = [sent.root for sent in doc.sents]\n    return np.mean([tree_height(root) for root in roots])\n\n\ndef count_subtrees(root):\n    if not list(root.children):\n        return 0\n    else:\n        return 1 + sum(count_subtrees(x) for x in root.children)\n\n\ndef get_mean_subtrees(paragraph):\n    if type(paragraph) == str:\n        doc = nlp(paragraph)\n    else:\n        doc = paragraph\n    roots = [sent.root for sent in doc.sents]\n    return np.mean([count_subtrees(root) for root in roots])\n\n\ndef get_averge_noun_chunks(paragraph):\n    if type(paragraph) == str:\n        doc = nlp(paragraph)\n    else:\n        doc = paragraph\n    return len(list(doc.noun_chunks))\n    \ndef get_noun_chunks_size(paragraph):\n    if type(paragraph) == str:\n        doc = nlp(paragraph)\n    else:\n        doc = paragraph\n    noun_chunks_size = [len(chunk) for chunk in doc.noun_chunks]\n    return np.mean(noun_chunks_size)\n    ","7c352204":"df['avg_parse_tree_height'] = df.excerpt.apply(get_average_height)\ndf['mean_parse_subtrees'] = df.excerpt.apply(get_mean_subtrees)\ndf['noun_chunks'] = df.excerpt.apply(get_averge_noun_chunks)\ndf['avg_noun_chunks'] = df['noun_chunks'] \/ df['sentence_count']\ndf['noun_chunk_size'] = df.excerpt.apply(get_noun_chunks_size)\ndf['mean_noun_chunk_size'] = df['noun_chunk_size'] \/ df['avg_noun_chunks']","941b504d":"fig = px.scatter_matrix(df,\n                        dimensions=['target', 'avg_parse_tree_height', 'mean_parse_subtrees', 'noun_chunk_size'],\n                        color='target',\n                        color_continuous_scale='Plotly3')\n\nfig.update_layout(title={\n                    'text': 'Prominent Synactically Parsed Features Vs Readability Score',\n                    'x': 0.5\n                  })\nfig.show()","81112b01":"px.imshow(df[['target', 'avg_parse_tree_height', 'mean_parse_subtrees', 'noun_chunks', 'avg_noun_chunks', 'noun_chunk_size', 'mean_noun_chunk_size']].corr(),\n          color_continuous_scale='Aggrnyl')","30f7bca6":"doc = nlp(df.excerpt.values[1])\ndisplacy.render(list(doc.sents)[0], style=\"dep\")","ae18cfa5":"def get_pos_freq_per_word(paragraph, tag):\n    if type(paragraph) == str:\n        doc = nlp(paragraph)\n    else:\n        doc = paragraph\n    pos_counter = Counter(([token.pos_ for token in doc]))\n    pos_count_by_tag = pos_counter[tag]\n    total_pos_counts = sum(pos_counter.values())\n    return pos_count_by_tag \/ total_pos_counts","104d6f49":"df['nouns_per_word'] = df.excerpt.apply(lambda x: get_pos_freq_per_word(x, 'NOUN'))\ndf['proper_nouns_per_word'] = df.excerpt.apply(lambda x: get_pos_freq_per_word(x, 'PROPN'))\ndf['pronouns_per_word'] = df.excerpt.apply(lambda x: get_pos_freq_per_word(x, 'PRON'))\ndf['adj_per_word'] = df.excerpt.apply(lambda x: get_pos_freq_per_word(x, 'ADJ'))\ndf['adv_per_word'] = df.excerpt.apply(lambda x: get_pos_freq_per_word(x, 'ADV'))\ndf['verbs_per_word'] = df.excerpt.apply(lambda x: get_pos_freq_per_word(x, 'VERB'))\ndf['cconj_per_word'] = df.excerpt.apply(lambda x: get_pos_freq_per_word(x, 'CCONJ'))\ndf['sconj_per_word'] = df.excerpt.apply(lambda x: get_pos_freq_per_word(x, 'SCONJ'))","67361c66":"fig = px.scatter_matrix(df,\n                        dimensions=['target', 'nouns_per_word', 'verbs_per_word', 'pronouns_per_word'],\n                        color='target',\n                        color_continuous_scale='matter')\n\nfig.update_layout(title={\n                    'text': 'Prominent POS Based Features Vs Readability Score',\n                    'x': 0.5\n                  })\nfig.show()","8ebf39c6":"px.imshow(df[['target', 'nouns_per_word', 'proper_nouns_per_word', 'pronouns_per_word', 'adj_per_word', 'adv_per_word', 'verbs_per_word', 'cconj_per_word', 'sconj_per_word']].corr(),\n          color_continuous_scale='Aggrnyl')","373efc53":"df.to_csv('interim_df.csv')","24bf17e0":"Lets add a few more experimental miscellaneous linguistic features...","c523df88":"## Exploratory Data Analysis <a class=\"anchor\" id=\"eda\"><\/a>","f76afcf1":"## CommonLit Readability Prize EDA + Feature Engineering\n\nIn this competition, we're predicting the reading ease of excerpts from literature - readability score. **Text readability** is best defined as the ease with which a text can be read and understood in terms of the linguistic features found within a text. So the task is to build algorithms to **rate the complexity of reading passages for grade 3-12 classroom use**.\n\nIn this post we'll first extensively focus on visualizing the data we have at our disposal and uncover the subtle patterns hiding in the data. While we're working on exploring and analysing the data, we'd also gradually incorporate feature engineering and introduce what could be some of the most crucial indicators and predictors of the readability score. So without further ado, lets get started!\n\n","4e598032":"To put all these new features into perspective, lets visualize some of the prominent ones and get a sense of how significantly they manipulate the readability score.","e08f2135":"#### Count of words and sentences","b09d5201":"### Column Meaning and Interpretations\n**id** - unique ID for excerpt\n\n**url_legal** - URL of source - this is blank in the test set.\n\n**license** - license of source material - this is blank in the test set.\n\n**excerpt** - text to predict reading ease of\n\n**target** - reading ease\n\n**standard_error** - measure of spread of scores among multiple raters for each excerpt. Not included for test data.","3bd3b8a2":"***pronouns_per_word, verbs_per_word*** tend to have decent positive influence on readability score - our inital guess can be texts with larger proportion of verbs and pronouns seem to be more easily readable than their other counterparts. Also, higher the number adjectives and nouns per word - ***adj_per_word and nouns_per_word*** the less readable the text becomes","10c908e2":"### Traditional Features <a class=\"anchor\" id=\"fe1\"><\/a>\nLets add more classical traditional formulas as features to our dataset and visualize what value they're bringing to the table","b1252c97":"### Understanding Readability Score Distribution <a class=\"anchor\" id=\"eda2\"><\/a>","1c0b4a6c":"### Understanding Linguistic Features <a class=\"anchor\" id=\"eda4\"><\/a>\nLets start by extracting and visualizing some of the basic linguistic features and then drive towards more statistical features","9397b3ba":"## Table of Contents\n\n* [Importing Libraries](#lib)\n* [Reading Data](#data)\n* [Exploratory Data Analysis](#eda)\n    1. [Understanding Significance of URL legal and License](#eda1)\n    2. [Understanding Readability Score Distribution](#eda2)\n    3. [Understanding Readability Score and Standard Error Relationship](#eda3)\n    4. [Understanding Linguistic Features](#eda4)\n* [Feature Engineering](#fe)\n    1. [Traditional Features](#fe1)\n    2. [Syntactic Parse Based Features](#fe2)\n    3. [POS Tag Based Features](#fe3)\n    4. [Text Based Features](#fe4)","2f163401":"### Understanding Readability Score and Standard Error Relationship <a class=\"anchor\" id=\"eda3\"><\/a>","cf198e2d":"### POS Tag Based Features <a class=\"anchor\" id=\"fe3\"><\/a>\n\nNow that we have extracted quite some essential features from classical formulas and tree parsing, lets take a step forward in determining features based on parts-of-speech tagging. The features would be answering our questions if the proportion of nouns, proper nouns, interjections, etc. affect the readability score at all.","c4d9ce5c":"## Importing Libraries <a class=\"anchor\" id=\"lib\"><\/a>","399dca7c":"Going by the plot, our first guesses can be that the excerpts which have valid license and legal url attributed to them have got higher mean and median readability score (difference of ~0.4 in readability scores). However, the distributions are close to normal in both cases.","988a712b":"There's a **strong negative correlation** between target (readability score) and ***difficult_word_ratio, dale_chall_readability_score, crawford, etc.***. And ***fernandez_huerta*** also seems to have **sound positive relation** with the reoported readability scores. It'd be worth considering these features in our future analysis and models.","bc278210":"Just a fun visualization of dependency tree for one the sentences in an excerpt!","c97f3ea0":"### Understanding Significance of URL legal and License <a class=\"anchor\" id=\"eda1\"><\/a>\n**url_legal** and **license** seem to have a high percentage of null entries and as mentioned in the competition's description that all the values of both the fields would be blank in test set, so we can drop them from our further analysis. Just to be entirely sure that their presence has no impact on target variable, readability score, let's draw violin plots to confirm if it stands true or not.","fe7ce4a1":"## Feature Engineering <a class=\"anchor\" id=\"fe\"><\/a>","4beaeffd":"The mean and median seem very close to each other with values around **-0.96 and -0.91** respectively. Overall, there are no signs of any outliers present in the scores reported which validates that the entries added in the dataset were scrutinized before hand and the ones above a certain standard deviation threshold were discarded.","984e5f36":"### Syntactic Parse Based Features <a class=\"anchor\" id=\"fe2\"><\/a>\n\nNow lets dive into engineering features based on syntactic dependency parsing","18b61ef4":"The distinction here in more evident and powerful compared to our previous observation regarding sentence lengths. There is clearly a pattern that is being followed - **readable texts tend to have less sophisticated words than difficult texts**","29d96cdf":"#### Lexical Sophistication\nA lot of traditional studies have shown a correlation between the readability score and the amount to which difficult or sophisticated words are used in a given text. Lets see how it unfolds in our dataset.","a654e03a":"#### Average Words per Sentence in all Excerpts","42e7e0ef":"### Text Based Features - To Be Continued<a class=\"anchor\" id=\"tf1\"><\/a>\n","c2f09cee":"The following syntactic dependency tree parsed features seem to have a considerable negative correlation with readability score - ***avg_parse_tree_height, mean_parse_subtrees, noun_chunk_size***. Intuitively speaking, the findings make a lot of sense as for example with an increase in tree depth, meaning more complex sentence structure, leading to a lower readability score ","db23677e":"There's a **clear and obvious shift** in the average number of words a sentence possesses when we compare the excerpts scored higher or lower than the median readability score. The blue bars denote all the excerpts scored above median score while the red ones scored less than median value **-0.91**. Although there's not enough distinction between the two, it does make sense that longer sentences are likely to be difficult to process while reading than shorter ones.","0c0797f5":"Lets visualize how all our additional features added so co-relate with our target variable readability score and with each other as well","13a67205":"The distribution looks fairly normal with **min value at -3.67 and maximum at 1.71**. The scores seem to have been baselined around the excerpt with id **436ce79fe** which has 0 readability score and 0 standard error.","29a810d1":"The deviations reported in readability scores tend to increase as we drive toward extreme ends - for highly readable texts and for extraordinarily difficult texts. Humans usually are inclined towards having strong opinions on extreme matters - be it in any field. I guess that is precisely the case here.\nThat poses an important question to us - whether to handle such datapoints differently?\n","24d2e7d4":"## Reading Data <a class=\"anchor\" id=\"data\"><\/a>"}}