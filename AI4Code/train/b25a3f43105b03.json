{"cell_type":{"83c558f5":"code","f213339d":"code","e980c8dd":"code","6f67aacd":"code","7ddf3624":"code","20110967":"code","57cfc70b":"code","6fe1a77b":"code","ccd820c5":"code","38c51d69":"code","dbedd213":"code","c4bf8c71":"code","65692de9":"code","820be463":"code","6ae826db":"code","ede01966":"code","a071d4ad":"code","d0a8d0bb":"code","dae1dd6b":"code","09a6d219":"code","16718741":"code","0f29c7cb":"code","0eed5754":"code","95ed547a":"code","b805d2d5":"code","74d9925f":"code","8f203287":"code","ae56c32d":"code","16545591":"code","963e7013":"markdown","b227a7f6":"markdown","a098eaa4":"markdown","7a773e92":"markdown","78f3754a":"markdown","8fb79f54":"markdown","52f82774":"markdown","b26f1a06":"markdown","6d7e7791":"markdown","78e00707":"markdown","3f85b6b4":"markdown"},"source":{"83c558f5":"import os\nimport cv2\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport json\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Activation, Dropout, Conv2D,MaxPooling2D\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications import VGG16\nfrom keras.applications import ResNet50\nfrom keras import optimizers","f213339d":"ann_file = '..\/input\/train2019.json'\nwith open(ann_file) as data_file:\n        train_anns = json.load(data_file)","e980c8dd":"train_anns_df = pd.DataFrame(train_anns['annotations'])[['image_id','category_id']]\ntrain_img_df = pd.DataFrame(train_anns['images'])[['id', 'file_name']].rename(columns={'id':'image_id'})\ndf_train_file_cat = pd.merge(train_img_df, train_anns_df, on='image_id')\ndf_train_file_cat['category_id']=df_train_file_cat['category_id'].astype(str)\ndf_train_file_cat.head()","6f67aacd":"len(df_train_file_cat['category_id'].unique())","7ddf3624":"# Example of images for category_id = 400\nimg_names = df_train_file_cat[df_train_file_cat['category_id']=='400']['file_name'][:30]\n\nplt.figure(figsize=[15,15])\ni = 1\nfor img_name in img_names:\n    img = cv2.imread(\"..\/input\/train_val2019\/%s\" % img_name)[...,[2, 1, 0]]\n    plt.subplot(6, 5, i)\n    plt.imshow(img)\n    i += 1\nplt.show()","20110967":"valid_ann_file = '..\/input\/val2019.json'\nwith open(valid_ann_file) as data_file:\n        valid_anns = json.load(data_file)","57cfc70b":"valid_anns_df = pd.DataFrame(valid_anns['annotations'])[['image_id','category_id']]\nvalid_anns_df.head()","6fe1a77b":"valid_img_df = pd.DataFrame(valid_anns['images'])[['id', 'file_name']].rename(columns={'id':'image_id'})\nvalid_img_df.head()","ccd820c5":"df_valid_file_cat = pd.merge(valid_img_df, valid_anns_df, on='image_id')\ndf_valid_file_cat['category_id']=df_valid_file_cat['category_id'].astype(str)\ndf_valid_file_cat.head()","38c51d69":"nb_classes = 1010\nbatch_size = 128\nimg_size = 128\nnb_epochs = 10","dbedd213":"%%time\ntrain_datagen=ImageDataGenerator(rescale=1.\/255, rotation_range=45, \n                    width_shift_range=.15, \n                    height_shift_range=.15, \n                    horizontal_flip=True, \n                    zoom_range=0.5)\n\ntrain_generator=train_datagen.flow_from_dataframe(\n    dataframe=df_train_file_cat,\n    directory=\"..\/input\/train_val2019\",\n    x_col=\"file_name\",\n    y_col=\"category_id\",\n    batch_size=batch_size,\n    shuffle=True,\n    class_mode=\"sparse\",    \n    target_size=(img_size,img_size))","c4bf8c71":"# udacity_intro_to_tensorflow_for_deep_learning\/l05c04_exercise_flowers_with_data_augmentation_solution.ipynb#scrollTo=jqb9OGoVKIOi\n# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\ndef plotImages(images_arr):\n    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip( images_arr, axes):\n        ax.imshow(img)\n    plt.tight_layout()\n    plt.show()\n    \n    \naugmented_images = [train_generator[0][0][0] for i in range(5)]\nplotImages(augmented_images)","65692de9":"%%time\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\nvalid_generator=test_datagen.flow_from_dataframe(\n    dataframe=df_valid_file_cat,\n    directory=\"..\/input\/train_val2019\",\n    x_col=\"file_name\",\n    y_col=\"category_id\",\n    batch_size=batch_size,\n    class_mode=\"sparse\",    \n    target_size=(img_size,img_size))","820be463":"vgg16_net = VGG16(weights='imagenet', \n                  include_top=False, \n                  input_shape=(img_size, img_size, 3))\nvgg16_net.trainable = False","6ae826db":"resnet = ResNet50(include_top=False, weights='imagenet',\n               input_shape=(img_size,img_size,3))\nresnet.trainable = False","ede01966":"model = Sequential()\nmodel.add(vgg16_net)\nmodel.add(Flatten())\nmodel.add(Dense(1024))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(nb_classes, activation='softmax'))\n\nmodel.compile(optimizers.rmsprop(lr=0.0001, decay=1e-6),loss='sparse_categorical_crossentropy',metrics=['accuracy'])","a071d4ad":"%%time\nhistory = model.fit_generator(generator=train_generator, \n                              \n                    steps_per_epoch=500,\n                              \n                    validation_data=valid_generator, \n                              \n                    validation_steps=100,\n                              \n                    epochs=nb_epochs,\n                    verbose=0)","d0a8d0bb":"with open('history.json', 'w') as f:\n    json.dump(history.history, f)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot()\nhistory_df[['acc', 'val_acc']].plot()","dae1dd6b":"acc = history.history['acc']\nval_acc = history.history['val_acc']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(nb_epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","09a6d219":"test_ann_file = '..\/input\/test2019.json'\nwith open(test_ann_file) as data_file:\n        test_anns = json.load(data_file)","16718741":"test_img_df = pd.DataFrame(test_anns['images'])[['id', 'file_name']].rename(columns={'id':'image_id'})\ntest_img_df.head()","0f29c7cb":"%%time\ntest_datagen = ImageDataGenerator(rescale=1.\/255.)\ntest_generator = test_datagen.flow_from_dataframe(      \n    \n        dataframe=test_img_df,    \n    \n        directory = \"..\/input\/test2019\",    \n        x_col=\"file_name\",\n        target_size = (img_size,img_size),\n        batch_size = 1,\n        shuffle=False,\n        class_mode = None\n        )","0eed5754":"%%time\ntest_generator.reset()\npredict=model.predict_generator(test_generator, steps = len(test_generator.filenames),verbose=1)","95ed547a":"len(predict)","b805d2d5":"predicted_class_indices=np.argmax(predict,axis=1)","74d9925f":"labels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]","8f203287":"sam_sub_df = pd.read_csv('..\/input\/kaggle_sample_submission.csv')\nsam_sub_df.head()","ae56c32d":"filenames=test_generator.filenames\nresults=pd.DataFrame({\"file_name\":filenames,\n                      \"predicted\":predictions})\ndf_res = pd.merge(test_img_df, results, on='file_name')[['image_id','predicted']]\\\n    .rename(columns={'image_id':'id'})\n\ndf_res.head()","16545591":"df_res.to_csv(\"submission.csv\",index=False)","963e7013":"<img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/5408\/media\/bigleaves.jpg\" width=\"600\"><\/img>","b227a7f6":"> * Model: vgg16  \n Apply transfer learning skill from pretrained model using Keras.\n> Using vgg16 with a flatten layer followed by 2 fully connected layer with 1024 units and 1010 units. The output class probabilities based on 1010 classes which is done by the softmax activation function. Add a layer use a relu activation function. Add Dropout layers with a probability of 50%, where appropriate.","a098eaa4":"### Model","7a773e92":"### This kernel is base on [Alexander Teplyuk](https:\/\/www.kaggle.com\/ateplyuk\/inat2019-starter-keras\/output) here I applied Data Augmentation technic from [Udacity](https:\/\/colab.research.google.com\/github\/tensorflow\/examples\/blob\/master\/courses\/udacity_intro_to_tensorflow_for_deep_learning\/l05c04_exercise_flowers_with_data_augmentation_solution.ipynb#scrollTo=UOoVpxFwVrWy) as following:\n* random 45 degree rotation\n* random zoom of up to 50%\n* random horizontal flip\n* width shift of 0.15\n* height shfit of 0.15","78f3754a":"#### Hope you like it and finds this kernel helpful :)!","8fb79f54":"### In the cell below, use ImageDataGenerator to create a transformation that rescales the images by 255 here I applied Data Augmentation as following:\n* random 45 degree rotation\n* random zoom of up to 50%\n* random horizontal flip\n* width shift of 0.15\n* height shfit of 0.15","52f82774":"# Reference\n[Alexander Teplyuk](https:\/\/www.kaggle.com\/ateplyuk\/inat2019-starter-keras\/output)  \n[Udacity](https:\/\/colab.research.google.com\/github\/tensorflow\/examples\/blob\/master\/courses\/udacity_intro_to_tensorflow_for_deep_learning\/l05c04_exercise_flowers_with_data_augmentation_solution.ipynb#scrollTo=08rRJ0sn3Tb1)  \nhttps:\/\/medium.com\/@vijayabhaskar96\/tutorial-on-keras-flow-from-dataframe-1fd4493d237c","b26f1a06":"### Prediction","6d7e7791":"### Test data","78e00707":"### Validation data","3f85b6b4":"### Train data"}}