{"cell_type":{"1374ea66":"code","e24c1271":"code","ce13ebdb":"code","f2c520b2":"code","233eb181":"code","b6bcdb34":"code","f78bcd1b":"code","1572506b":"code","f82ee021":"code","09905165":"code","fadb11d3":"markdown","acdb0cc7":"markdown","62fbb79f":"markdown","6223bb45":"markdown","23212acb":"markdown"},"source":{"1374ea66":"import requests\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms as pth_transforms\nimport numpy as np\nfrom PIL import Image","e24c1271":"patch_size = 8\n#model = torch.hub.load('facebookresearch\/dino:main', 'dino_deits8')\nmodel = torch.hub.load('facebookresearch\/dino:main', 'dino_vits16')","ce13ebdb":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\ndevice","f2c520b2":"for p in model.parameters():\n        p.requires_grad = False\n        \nmodel.eval()\nmodel.to(device)","233eb181":"response = requests.get(\"https:\/\/dl.fbaipublicfiles.com\/dino\/img.png\")\nimg_npy = Image.open(BytesIO(response.content))\nimg_npy = img_npy.convert('RGB')","b6bcdb34":"plt.imshow(img_npy)\nprint(type(img_npy))","f78bcd1b":"transform = pth_transforms.Compose([\n    pth_transforms.ToTensor(),\n    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\nimg = transform(img_npy)\n\n# make the image divisible by the patch size\nw, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - img.shape[2] % patch_size\nimg = img[:, :w, :h].unsqueeze(0)\n\nw_featmap = img.shape[-2] \/\/ patch_size\nh_featmap = img.shape[-1] \/\/ patch_size\n\n#attentions = model.forward_selfattention(img.to(device))\nattentions = model.get_last_selfattention(img)   #img.cuda()\n\nprint(type(img))\nprint(img.shape)","1572506b":"nh = attentions.shape[1] # number of head\n\n# we keep only the output patch attention\nattentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n\n# we keep only a certain percentage of the mass\nval, idx = torch.sort(attentions)\nval \/= torch.sum(val, dim=1, keepdim=True)\ncumval = torch.cumsum(val, dim=1)","f82ee021":"threshold = 0.6 # We visualize masks obtained by thresholding the self-attention maps to keep xx% of the mass.\nth_attn = cumval > (1 - threshold)\nidx2 = torch.argsort(idx)\nfor head in range(nh):\n    th_attn[head] = th_attn[head][idx2[head]]\n    \nth_attn = th_attn.reshape(nh, w_featmap\/\/2, h_featmap\/\/2).float()\n\n# interpolate\nth_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\")[0].cpu().numpy()\n\nattentions = attentions.reshape(nh, w_featmap\/\/2, h_featmap\/\/2)\nattentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\")[0].cpu().numpy()\nattentions_mean = np.mean(attentions, axis=0)","09905165":"plt.figure(figsize=(6,6), dpi=200)\nplt.subplot(3, 3, 1)\nplt.title(\"Input\")\nplt.imshow(img_npy)\nplt.axis(\"off\")\n# visualize self-attention of each head\n\nfor i in range(6):\n    plt.subplot(3, 3, i+4)\n    plt.title(\"Head \"+str(i+1))\n    plt.imshow(attentions[i])\n    plt.axis(\"off\")\n\nplt.subplot(3, 3, 2)\nplt.title(\"Head Mean\")\nplt.imshow(attentions_mean)\nplt.axis(\"off\")\nplt.tight_layout()","fadb11d3":"This notebook referred to the following Colab notebook.<br\/>\nhttps:\/\/colab.research.google.com\/github\/oumpy\/hp_management\/blob\/master\/content\/articles\/2021sy\/blog\/attach\/dino\/dino_visualize_attention.ipynb","acdb0cc7":"## Visualize Self-attention","62fbb79f":"# DINO Visualize Self-Attention Sample\nhttps:\/\/github.com\/facebookresearch\/dino\/blob\/main\/visualize_attention.py","6223bb45":"## Input Image","23212acb":"## Get attention map"}}