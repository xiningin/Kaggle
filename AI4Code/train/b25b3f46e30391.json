{"cell_type":{"97c02b9c":"code","73c6ba7e":"code","20319fdc":"code","ef5b0810":"code","6a04047b":"code","5f107bdd":"code","a51a5a28":"code","cc70adc1":"code","8a0b8928":"code","24e2b339":"code","a8fa960a":"code","cf892438":"code","8c6e2009":"code","3fd6b08e":"code","a6c961ee":"code","e4d34599":"code","1dc39d7f":"code","638ba627":"code","7471113f":"code","79370cd7":"code","9bb182f6":"code","32c75c34":"code","869445e1":"code","f0dc7623":"code","e83069e4":"code","e2b77ec8":"code","5e619736":"code","2f6adca3":"code","2c35bdcc":"code","d6560cd4":"code","b6942cc0":"code","22ae3338":"code","d83dab5d":"code","25ebcce0":"code","de9d15eb":"code","7e946bb8":"code","d16807a3":"code","1b4d2ce0":"code","59b1184b":"code","71c22f45":"code","200e3fe4":"code","a98673b6":"code","5913d67e":"code","e654516d":"code","97122dd2":"code","b66dfd6a":"code","571d2384":"code","6e95f1ca":"code","a24b3648":"code","7de5f86b":"code","1fbe235b":"code","4dc0ec76":"code","9fcda611":"code","692b5504":"code","a47edc2b":"code","2737b13f":"code","62d53e81":"code","e666e258":"code","8caadbf4":"markdown","5ba49942":"markdown","1a6846f6":"markdown","ede7d9a9":"markdown","f0258793":"markdown","871918d5":"markdown","0dd9cd93":"markdown","246503b8":"markdown","9cfdc318":"markdown","a292b55e":"markdown","cad6b962":"markdown","b00679c2":"markdown","8a327ca9":"markdown","0c9e27bb":"markdown","e3bee115":"markdown","95d02ab5":"markdown","1b371e23":"markdown","7e0414bd":"markdown","a1350b55":"markdown","a502a2ec":"markdown"},"source":{"97c02b9c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf","73c6ba7e":"data_path = \"..\/input\/entity-annotated-corpus\/ner_dataset.csv\"\n\ndata = pd.read_csv(data_path, encoding= 'unicode_escape')\n# filling the first column that determines which sentence each word belongs to.\ndata.fillna(method = 'ffill', inplace = True)\ndata.head()","20319fdc":"ready_dist_path = \"..\/input\/named-entity-recognition-ner-corpus\/ner.csv\"\nready_data = pd.read_csv(ready_dist_path)\nready_data.head()","ef5b0810":"def join_a_sentence(sentence_number):\n\n    \"\"\"\n    Args.:\n          sentence_number: sentence number we want to join and return. \n          \n    Returns:\n          The joined sentence.\n    \"\"\"\n    \n    sentence_number = str(sentence_number)\n    the_sentence_words_list = list(data[data['Sentence #'] == 'Sentence: {}'.format(sentence_number)]['Word'])\n    \n    return ' '.join(the_sentence_words_list)","6a04047b":"join_a_sentence(sentence_number = 1)","5f107bdd":"join_a_sentence(sentence_number = 100)","a51a5a28":"# Data Shape\ndata.shape","cc70adc1":"# Number of unique sentences\nlen(np.unique(data['Sentence #']))","8a0b8928":"print(\"Number of unique words in the dataset: {}\".format(data.Word.nunique()))\nprint(\"Number of unique tags in the dataset: {}\".format(data.Tag.nunique()))","24e2b339":"tags = data.Tag.unique()\ntags","a8fa960a":"def num_words_tags (tags, data):\n    \n    \"\"\"This functions takes the tags we want to count and the datafram \n    and return a dict where the key is the tag and the value is the frequency\n    of that tag\"\"\"\n    \n    tags_count = {}\n    \n    for tag in tags:\n        len_tag = len(data[data['Tag'] == tag])\n        tags_count[tag] = len_tag\n    \n    return tags_count","cf892438":"tags_count = num_words_tags(tags, data)\ntags_count","8c6e2009":"plt.figure(figsize = (10, 6))\nplt.hist(data.Tag, log = True, label = 'Tags', color = 'olive', bins = 50)\nplt.xlabel('Tags', fontsize = 16)\nplt.ylabel('Count', fontsize = 16)\nplt.title(\"Tags Frequency\", fontsize = 20)\nplt.grid(alpha=0.3)\nplt.legend()\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xticks(rotation=90)\nplt.show()","3fd6b08e":"# def process_Data():\n    \n#     data_dict = {}\n    \n#     for sn in range(1, len(np.unique(data['Sentence #']))+1):\n        \n#         all_sen_data = []\n        \n#         se_data = data[data['Sentence #']  == 'Sentence: {}'.format(sn)]\n#         sentence = ' '.join(list(se_data['Word']))\n#         all_sen_data.append(sentence)\n        \n#         sen_pos = list(se_data['POS'])\n#         all_sen_data.append(sen_pos)\n        \n#         sen_tags = list(se_data['Tag'])\n#         all_sen_data.append(sen_tags)\n        \n#         data_dict['Sentence: {}'.format(sn)] = all_sen_data\n        \n#         if sn % 10000 == 0:\n#             print(\"{} sentences are processed\".format(sn))\n        \n#     return data_dict","a6c961ee":"ready_data.head()","e4d34599":"X = list(ready_data['Sentence'])\nY = list(ready_data['Tag'])","1dc39d7f":"from ast import literal_eval\nY_ready = []\n\nfor sen_tags in Y:\n    Y_ready.append(literal_eval(sen_tags))","638ba627":"print(\"First three sentences: \\n\")\nprint(X[:3])","7471113f":"print(\"First three Tags: \\n\")\nprint(Y_ready[:3])","79370cd7":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","9bb182f6":"print(\"Number of examples: {}\".format(len(X)))","32c75c34":"# cutoff reviews after 110 words\nmaxlen = 110\n\n# consider the top 36000 words in the dataset\nmax_words = 36000\n\n# tokenize each sentence in the dataset\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(X)\nsequences = tokenizer.texts_to_sequences(X)","869445e1":"word_index = tokenizer.word_index\nprint(\"Found {} unique tokens.\".format(len(word_index)))\nind2word = dict([(value, key) for (key, value) in word_index.items()])","f0dc7623":"word2id = word_index","e83069e4":"# dict. that map each identifier to its word\nid2word = {}\nfor key, value in word2id.items():\n    id2word[value] = key","e2b77ec8":"# pad the sequences so that all sequences are of the same size\nX_preprocessed = pad_sequences(sequences, maxlen=maxlen, padding='post')","5e619736":"# first example after tokenization and padding. \nX_preprocessed[0]","2f6adca3":"# 22479 example after tokenization and padding. \nX_preprocessed[22479]","2c35bdcc":"# dict. that map each tag to its identifier\ntags2id = {}\nfor i, tag in enumerate(tags):\n    tags2id[tag] = i","d6560cd4":"tags2id","b6942cc0":"# dict. that map each identifier to its tag\nid2tag = {}\nfor key, value in tags2id.items():\n    id2tag[value] = key","22ae3338":"id2tag","d83dab5d":"def preprocess_tags(tags2id, Y_ready):\n    \n    Y_preprocessed = []\n    maxlen = 110\n    # for each target \n    for y in Y_ready:\n        \n        # place holder to store the new preprocessed tag list\n        Y_place_holder = []\n        \n        # for each tag in rhe tag list \n        for tag in y:\n            # append the id of the tag in the place holder list\n            Y_place_holder.append(tags2id[tag])\n        \n        # find the lenght of the new preprocessed tag list \n        len_new_tag_list = len(Y_place_holder)\n        # find the differance in length between the len of tag list and padded sentences\n        num_O_to_add = maxlen - len_new_tag_list\n        \n        # add 'O's to padd the tag lists\n        padded_tags = Y_place_holder + ([tags2id['O']] * num_O_to_add)\n        Y_preprocessed.append(padded_tags)\n        \n    return Y_preprocessed","25ebcce0":"Y_preprocessed = preprocess_tags(tags2id, Y_ready)","de9d15eb":"print(Y_preprocessed[0])","7e946bb8":"print(Y_ready[0])","d16807a3":"print(\"The Lenght of training examples: {}\".format(len(X_preprocessed)))\nprint(\"The Lenght of training targets: {}\".format(len(Y_preprocessed)))","1b4d2ce0":"X_preprocessed = np.asarray(X_preprocessed)\nY_preprocessed = np.asarray(Y_preprocessed)","59b1184b":"# 70% of the datat will be used for training \ntraining_samples = 0.7\n# 15% of the datat will be used for validation \nvalidation_samples = 0.15\n# 15% of the datat will be used for testing \ntesting_samples = 0.15","71c22f45":"indices = np.arange(len(Y_preprocessed))","200e3fe4":"np.random.seed(seed=555)\nnp.random.shuffle(indices)","a98673b6":"X_preprocessed = X_preprocessed[indices]\nY_preprocessed = Y_preprocessed[indices]","5913d67e":"X_train = X_preprocessed[: int(0.7 * len(X_preprocessed))]\nprint(\"Number of training examples: {}\".format(len(X_train)))\n\n\nX_val = X_preprocessed[int(0.7 * len(X_preprocessed)) : int(0.7 * len(X_preprocessed)) + (int(0.15 * len(X_preprocessed)) + 1)]\nprint(\"Number of validation examples: {}\".format(len(X_val)))\n\n\nX_test = X_preprocessed[int(0.7 * len(X_preprocessed)) + (int(0.15 * len(X_preprocessed)) + 1) : ]\nprint(\"Number of testing examples: {}\".format(len(X_test)))\n\n\n\nY_train = Y_preprocessed[: int(0.7 * len(X_preprocessed))]\nY_val = Y_preprocessed[int(0.7 * len(X_preprocessed)) : int(0.7 * len(X_preprocessed)) + (int(0.15 * len(X_preprocessed)) + 1)]\nY_test = Y_preprocessed[int(0.7 * len(X_preprocessed)) + (int(0.15 * len(X_preprocessed)) + 1) : ]\n\nprint(\"Total number of examples after shuffling and splitting: {}\".format(len(X_train) + len(X_val) + len(X_test)))","e654516d":"X_train[1000]","97122dd2":"Y_train[1000]","b66dfd6a":"id2word[729]","571d2384":"train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\nval_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val))\ntest_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test))","6e95f1ca":"BATCH_SIZE = 132\nSHUFFLE_BUFFER_SIZE = 132\n\ntrain_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\nval_dataset = val_dataset.batch(BATCH_SIZE)\ntest_dataset = test_dataset.batch(BATCH_SIZE)","a24b3648":"embedding_dim = 300\nmaxlen = 110\nmax_words = 36000\nnum_tags = len(tags)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(max_words, embedding_dim, input_length=maxlen),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=100, activation='tanh', return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=100, activation='tanh', return_sequences=True)),\n    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(num_tags, activation='softmax'))\n])","7de5f86b":"model.summary()","1fbe235b":"model.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","4dc0ec76":"history = model.fit(train_dataset,\n                    validation_data=val_dataset,\n                    epochs=15)","9fcda611":"model.evaluate(test_dataset)","692b5504":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nfig, ax = plt.subplots(1, 2, constrained_layout=True, figsize=(6, 4), dpi=80)\n\nax[0].plot(epochs, acc, label = \"Training Accuracy\", color='darkblue')\nax[0].plot(epochs, val_acc, label = \"Validation Accuracy\", color='darkgreen')\nax[0].grid(alpha=0.3)\nax[0].title.set_text('Training Vs Validation Accuracy')\nax[0].fill_between(epochs, acc, val_acc, color='crimson', alpha=0.3)\nplt.setp(ax[0], xlabel='Epochs')\nplt.setp(ax[0], ylabel='Accuracy')\n\n\nax[1].plot(epochs, loss, label = \"Training Loss\", color='darkblue')\nax[1].plot(epochs, val_loss, label = \"Validation Loss\", color='darkgreen')\nax[1].grid(alpha=0.3)\nax[1].title.set_text('Training Vs Validation Loss')\nax[1].fill_between(epochs,loss, val_loss, color='crimson', alpha=0.3)\nplt.setp(ax[1], xlabel='Epochs')\nplt.setp(ax[1], ylabel='Loss')\n\nplt.show()","a47edc2b":"def make_prediction(model, preprocessed_sentence, id2word, id2tag):\n    \n    #if preprocessed_sentence.shape() != (1, 110):\n    preprocessed_sentence = preprocessed_sentence.reshape((1, 110))\n     \n    # return preprocessed sentence to its orginal form\n    sentence = preprocessed_sentence[preprocessed_sentence > 0]\n    word_list = []\n    for word in list(sentence):\n        word_list.append(id2word[word])\n    orginal_sententce = ' '.join(word_list)\n    \n    len_orginal_sententce = len(word_list)\n    \n    # make prediction\n    prediction = model.predict(preprocessed_sentence)\n    prediction = np.argmax(prediction[0], axis=1)\n    \n    # return the prediction to its orginal form\n    prediction = list(prediction)[ : len_orginal_sententce] \n    \n    pred_tag_list = []\n    for tag_id in prediction:\n        pred_tag_list.append(id2tag[tag_id])\n    \n    return orginal_sententce,  pred_tag_list","2737b13f":"orginal_sententce,  pred_tag_list = make_prediction(model=model,\n                                                    preprocessed_sentence=X_test[520],\n                                                    id2word=id2word,\n                                                    id2tag=id2tag)","62d53e81":"print(orginal_sententce)","e666e258":"print(pred_tag_list)","8caadbf4":"# 6- Thank you","5ba49942":"The las step is to **split** the data into: \n\n- Training dataset \n- Valisdation dataset \n- testing dataset ","1a6846f6":"- **Data shuffling and splitting**","ede7d9a9":"We need to tokenize the sentences by mapping each word to a unique identifier, then we need to pad them because NN need the input sentences to have the same lenght.","f0258793":"**Named Entity Recognition:** is the task of identifying and categorizing key information (entities) in text. An entity can be any word or series of words that consistently refers to the same thing.","871918d5":"## Load dataset to the model using train_dataset = tf.data.Dataset \n","0dd9cd93":"- **Sentences padding**","246503b8":"# 4- Data Preprocessing","9cfdc318":"we need to preprocess tags by assigning a unique identifier for each one of them. \n\nSince also tags for each example have different lenght we need to fine a way to slove this problem.  \n\n- We can assign a new tag for the zeros that we used in padding \n- We can use the O tag for them. \n\nI will try the second choice of using the O tag to pad the tag list. ","a292b55e":"- **Toknize sentences**","cad6b962":"### By now we have the data ready for training our model \n\n\n**We have X_preprocessed and Y_preprocessed that we will use to train our model**","b00679c2":"- **Preprocess tags**","8a327ca9":"# 3- Get to know our data","0c9e27bb":"### All what you need to apply Named Entity Recognition (NER) ","e3bee115":"**Code that I used to produce  ready_data** ","95d02ab5":"# 2- Importing Data ","1b371e23":"# 1- Importing Libraries","7e0414bd":"Find the Aggregated dataset at: https:\/\/www.kaggle.com\/naseralqaydeh\/named-entity-recognition-ner-corpus","a1350b55":"**Thank you for reading, I hope you enjoyed and benefited from it.**\n\n**If you have any questions or notes please leave it in the comment section.**\n\n**If you like this notebook please press upvote and thanks again.**","a502a2ec":"# 5- Model Training and Evaluation"}}