{"cell_type":{"5d4fd906":"code","4136418e":"code","47494bab":"code","e9e51a3b":"code","45260822":"code","8c4c6c3c":"code","0904665f":"code","70240f8c":"code","4e16aab9":"code","99398414":"code","df4a4765":"code","932da8c0":"code","7c9ef190":"markdown","f1a49948":"markdown","9a4ddb60":"markdown","2faa81ac":"markdown","dc555fe7":"markdown","ff5ae986":"markdown","3a3e339f":"markdown","f2d9da05":"markdown","331b37fe":"markdown","f4377259":"markdown","f0fcab54":"markdown","1afdf645":"markdown","3553d5c0":"markdown","e25b810f":"markdown","5279cb32":"markdown","5ed7c5dd":"markdown"},"source":{"5d4fd906":"# Importing Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline","4136418e":"# Reading Dataset\ndf = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndf.head()","47494bab":"# Diciding data into train and crossvalidation set\nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values.reshape(len(X), 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","e9e51a3b":"# Converting shapes for neural network\nX_train = X_train.T\ny_train = y_train.T\nX_test = X_test.T\ny_test = y_test.T\nprint(f\"Training data input shape (n, m) --> {X_train.shape}\")\nprint(f\"Training data output shape (1, m) --> {y_train.shape}\")\nprint(f\"Test data input shape (n, m) --> {X_test.shape}\")\nprint(f\"Test data output shape (1, m) --> {y_test.shape}\")","45260822":"def initialize_weights(n):\n    w = np.zeros((n, 1))\n    b = 0\n    return w, b","8c4c6c3c":"def forward_propagation(X, Y, w, b):\n    m = len(X)\n    Z = np.dot(w.T, X) + b\n    A = 1\/(1 + np.exp(-Z))\n    cost = -(1\/m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n    \n    return A, cost","0904665f":"def backward_propagation(X, Y, A):\n    m = len(X)\n    dz = A - Y\n    dw = (1\/m) * np.dot(X, dz.T)\n    db = (1\/m) * np.sum(dz)\n    return dw, db","70240f8c":"def optimizer(X, Y, num_iterations = 100, learning_rate = 0.01):\n    w, b = initialize_weights(X_train.shape[0])\n    costs = []\n    for i in range(num_iterations):\n        A, cost = forward_propagation(X, Y, w, b)\n        dw, db = backward_propagation(X, Y, A)\n        w = w - learning_rate * dw        \n        b = b - learning_rate * db\n        \n        costs.append(cost)\n    return w, b","4e16aab9":"w, b = optimizer(X_train, y_train, num_iterations = 100, learning_rate = 0.5)","99398414":"def predictions(X, Y, w, b):\n    preds = np.zeros((1, X.shape[1]))\n    m = len(X)\n    Z = np.dot(w.T, X) + b\n    A = 1\/(1 + np.exp(-Z))\n    for i in range(A.shape[1]):\n        if A[0][i] >= 0.5:\n            preds[0][i] = 1\n        else:\n            preds[0][i] = 0\n    return preds\n            \npreds = predictions(X_test, y_test, w, b)","df4a4765":"accuracy = (len(preds[preds == y_test])\/len(y_test[0])) * 100\naccuracy","932da8c0":"preds","7c9ef190":"Hope you understand the input and output shape with above diagram","f1a49948":"# Sigmoid Activation","9a4ddb60":"![Untitled%20drawing%20%281%29.jpg](attachment:Untitled%20drawing%20%281%29.jpg)","2faa81ac":"# Backward Propagation","dc555fe7":"There are many types of activation functions available. For e.g ReLU, Sigmoid, tanh, Leaky ReLU. For classification problems we use sigmoid activation function.\n\n![](https:\/\/miro.medium.com\/max\/970\/1*Xu7B5y9gp0iL5ooBj7LtWw.png)","ff5ae986":"# Preparing feature and target shape\n\nSome terminologies to keep in mind\n\nm --> no of training\/test examples \n\nn --> no of input features\n\nw --> weights corresponding to each input feature. Dimension (n, 1)\n\nX --> input matrix. Dimension (n, m).  X is a matrix with each column represents one training example\n\ny --> Output vector. Dimension (1, m)","3a3e339f":"# Forward Propagation","f2d9da05":"Let's understand line by line how the forward propagation is working.\n**Z = np.dot(w.T, X) + b** --> In this we have taken the dot product of (w Transpose) and X and add the bias term.\nwe know that the shape of w --> (n, 1) so it's transpose shape will be (1, n)\nWe know that the shape of X --> (n, m).\n\nso the dot product of w.T and X will give shape of (1, m).\n\nZ = w1x1 + w2x2 + ___ + Wnxn\nZ will have a shape of (1, m)\n\n\n**A = 1\/(1 + np.exp(-Z))** --> After calculating Z (linear function) Sigmoid function is used to convert Z into probabilities. If A >= 0.5 we predict our example belong to class 1 else class 0. A has a shape of (1, m)\n\n\n**cost = -(1\/m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))** --> This is what you have using using as **binary_crossentropy** loss function. This fucntion calculates the average error over the entire training example at once","331b37fe":"Main motive of this notebook is to show how to implement NN from scratch. Though due to imbalance dataset, this model is predicting all values as 0.","f4377259":"# Implementing Neural Network from scratch using numpy","f0fcab54":"For backprop please refer this link:\n[LINK](https:\/\/www.youtube.com\/watch?v=yXcQ4B-YSjQ)","1afdf645":"We can see from above that there are 30 input features for both training and test data. There are 190820 training examples in training data and 93987 examples in test data.","3553d5c0":"Initially we required random weights to make some prediction then we optimize weights using Gradient descent to reduce the overall error in prediction. As we discussed above the shape of weight vectors should be (n, 1). I have used initial values of 0 for both weight and bias.","e25b810f":"Well, If you are new to neural networks, you are implementing ANN (Artificial Neural Network) using Keras, Tensorflow etc. Though you get better results with neural network as compare to conventional machine learning algorithms. If you don't understand the maths that's working behind neural network, it's a kind of Black Box (that takes some input and gives you output but you don't know how). The main motive of this Notebook is make you understand how things are working behind the scene. Let's get started","5279cb32":"# Things you will learn:\n1. Sigmoid activation\n2. Forward Propagation\n3. Cost and Loss Function\n4. Back Propagation\n5. Gradient Descent (Optimizer)","5ed7c5dd":"# Initializing weights"}}