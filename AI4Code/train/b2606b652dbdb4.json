{"cell_type":{"406eedbd":"code","f4568fc9":"code","b8dd5781":"code","86075b04":"code","7b5618ad":"code","df032b8a":"code","79337e1d":"code","4494c8ac":"code","1bef8acf":"code","162ccc16":"code","9c988b6d":"code","5451b175":"code","75cd8d6d":"code","96cf4325":"code","75373172":"code","878f3473":"code","17090e71":"code","f4cd8dbe":"code","164a617a":"code","4d5ec5b6":"code","3feb3f20":"code","6d4c3412":"code","eb750a0c":"code","d191df46":"code","ec96d71d":"code","b2ecc106":"code","b81bc67b":"code","d4b0a433":"code","4650d783":"code","39500477":"code","b2bce346":"code","98606cfe":"code","7efc9cb1":"code","433ffb47":"code","aa514410":"code","0d145437":"code","a7d0731e":"code","8b770434":"code","7fa2ecca":"code","69390063":"code","7b307cb2":"code","f1920426":"code","a0143e9f":"code","91be0895":"code","530c2a73":"code","f19f388e":"code","8fb6908f":"code","06d33c7a":"code","0940649c":"code","5da04869":"code","2c92419d":"code","5bba0a39":"code","86e404d2":"code","f96a85c8":"code","8aba630f":"code","c469419f":"code","6f531a1f":"code","5183cf6f":"markdown","6dbdfc7a":"markdown","f8658394":"markdown","e2295643":"markdown","7c3117da":"markdown","163e7c79":"markdown","f0bdc5f0":"markdown","63d26145":"markdown"},"source":{"406eedbd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Input data files are available in the \"..\/input\/\" directory.O\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f4568fc9":"train_data=pd.read_csv(r'..\/input\/train.csv')\ntest_data=pd.read_csv(r'..\/input\/test.csv')","b8dd5781":"!..\/input\/Data_Dictionary.xlsx","86075b04":"dict1=pd.read_excel(r'..\/input\/Data_Dictionary.xlsx')\ndict1","7b5618ad":"train_data.head(20)","df032b8a":"train_data.tail()","79337e1d":"train_data.info()","4494c8ac":"train_data.describe(include='all')","1bef8acf":"train_data.isnull().sum()","162ccc16":"plt.figure(figsize=(8,6),dpi=80)\nsns.violinplot(x=train_data['feature_1'],y=train_data['target'],data=train_data,)\nplt.xlabel('Target',size=18,color='r')\nplt.ylabel('Feature_1',size=18,color='r')\nplt.title('Feature_1 Vs Target',size=20,color='blue')\nplt.legend();","9c988b6d":"plt.figure(figsize=(8,6),dpi=80)\nsns.violinplot(x=train_data['feature_2'],y=train_data['target'],data=train_data,)\nplt.xlabel('Target',size=18,color='r')\nplt.ylabel('Feature_2',size=18,color='r')\nplt.title('Feature_2 Vs Target',size=20,color='blue')\nplt.legend();","5451b175":"plt.figure(figsize=(8,6),dpi=80)\nsns.violinplot(x=train_data['feature_3'],y=train_data['target'],data=train_data)\nplt.xlabel('Target',size=18,color='r')\nplt.ylabel('Feature_3',size=18,color='r')\nplt.title('Feature_3 Vs Target',size=20,color='blue')\nplt.legend();","75cd8d6d":"plt.figure(figsize=(8,6))\nsns.distplot(train_data['target'],bins=50,hist=True,color='#F71212',kde=False)","96cf4325":"print('there are {0} sample in target below -30'.format(train_data.loc[train_data.target<-30].shape[0]))","75373172":"feature1=train_data['feature_1'].value_counts().sort_index(ascending=False)\nsns.barplot(x=feature1.index,y=feature1.values,ci=100,color='#FFFF00')\nplt.title('Feature_1',color='red',size=18);","878f3473":"feature2=train_data['feature_2'].value_counts().sort_index(ascending=False)\nsns.barplot(x=feature2.index,y=feature2.values,ci=100,color='#00FF7F')\nplt.title('Feature_2',color='red',size=18);","17090e71":"feature3=train_data['feature_3'].value_counts().sort_index(ascending=False)\nsns.barplot(x=feature3.index,y=feature3.values,ci=100,color='#556B2F')\nplt.title('Feature_3',color='red',size=18);","f4cd8dbe":"train_active_months=train_data['first_active_month'].value_counts().sort_index(ascending=False)\ntest_active_months=test_data['first_active_month'].value_counts().sort_index(ascending=False)","164a617a":"data = [go.Scatter(x=train_active_months.index, y=train_active_months.values, name='train',opacity=1), \n        go.Scatter(x=test_active_months.index, y=test_active_months.values, name='test',opacity=1)]\nlayout = go.Layout(dict(title = \"Counts of first active month\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Count'),\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","4d5ec5b6":"test_data.head()","3feb3f20":"test_data.describe()","6d4c3412":"test_data.info()","eb750a0c":"test_data.isnull().sum()","d191df46":"historical_data=pd.read_csv(r'..\/input\/historical_transactions.csv')","ec96d71d":"historical_data.head()","b2ecc106":"historical_data.tail()","b81bc67b":"historical_data.info()","d4b0a433":"historical_data.isnull().sum()","4650d783":"historical_data['installments'].value_counts()","39500477":"plt.figure(figsize=(8,6),dpi=100)\ninstall=historical_data['installments'].value_counts().sort_index(ascending=False)\nsns.barplot(x=install.index,y=install.values,ci=100,color='#900C3F')\nplt.title('installment detail',color='red',size=18);\n","b2bce346":"trai=historical_data.groupby(['installments'])['authorized_flag'].value_counts()\ntrai.head()","98606cfe":"new_merchant_data=pd.read_csv(r'..\/input\/new_merchant_transactions.csv')","7efc9cb1":"new_merchant_data.head()","433ffb47":"new_merchant_data.info()","aa514410":"new_merchant_data.isna().sum()","0d145437":"new_merchant_data['category_2'].fillna(value=1.0,inplace=True)\nnew_merchant_data['category_3'].fillna('A',inplace=True)\nnew_merchant_data['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)","a7d0731e":"historical_data['category_2'].fillna(value=1.0,inplace=True)\nhistorical_data['category_3'].fillna('A',inplace=True)\nhistorical_data['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)","8b770434":"new_merchant_data['authorized_flag']=new_merchant_data['authorized_flag'].apply(lambda x:1 if x=='Y'else 0)","7fa2ecca":"new_merchant_data['authorized_flag'].value_counts().plot(kind='bar',title='authorize_flag value',color='red')","69390063":"card_total=new_merchant_data.groupby(['card_id'])['purchase_amount'].mean().sort_values()\ncard_total.head()","7b307cb2":"def get_new_columns(name,aggs):\n    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]","f1920426":"import datetime\nfor df in [historical_data,new_merchant_data]:\n    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n    df['year'] = df['purchase_date'].dt.year\n    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n    df['month'] = df['purchase_date'].dt.month\n    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n    df['hour'] = df['purchase_date'].dt.hour\n    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)\/\/30\n    df['month_diff'] += df['month_lag']","a0143e9f":"aggs = {}\nfor col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n    aggs[col] = ['nunique']\n\naggs['purchase_amount'] = ['sum','max','min','mean','var']\naggs['installments'] = ['sum','max','min','mean','var']\naggs['purchase_date'] = ['max','min']\naggs['month_lag'] = ['max','min','mean','var']\naggs['month_diff'] = ['mean']\naggs['authorized_flag'] = ['sum', 'mean']\naggs['weekend'] = ['sum', 'mean']\naggs['category_1'] = ['sum', 'mean']\naggs['card_id'] = ['size']\n\nfor col in ['category_2','category_3']:\n    historical_data[col+'_mean'] = historical_data.groupby([col])['purchase_amount'].transform('mean')\n    aggs[col+'_mean'] = ['mean']    \n\nnew_columns = get_new_columns('hist',aggs)\nhistorical_data_group = historical_data.groupby('card_id').agg(aggs)\nhistorical_data_group.columns = new_columns\nhistorical_data_group.reset_index(drop=False,inplace=True)\nhistorical_data_group['hist_purchase_date_diff'] = (historical_data_group['hist_purchase_date_max'] -historical_data_group['hist_purchase_date_min']).dt.days\nhistorical_data_group['hist_purchase_date_average'] = historical_data_group['hist_purchase_date_diff']\/historical_data_group['hist_card_id_size']\nhistorical_data_group['hist_purchase_date_uptonow'] = (datetime.datetime.today() - historical_data_group['hist_purchase_date_max']).dt.days","91be0895":"#merge with train, test\ntrain_data = train_data.merge(historical_data_group,on='card_id',how='left')\ntest_data = test_data.merge(historical_data_group,on='card_id',how='left')\n\n#cleanup memory\ndel historical_data_group;","530c2a73":"aggre = {}\nfor col in ['subsector_id','merchant_id','merchant_category_id','state_id', 'city_id']:\n    aggre[col] = ['nunique']\nfor col in ['month', 'hour', 'weekofyear', 'dayofweek']:\n    aggre[col] = ['nunique', 'mean', 'min', 'max']    \naggre['purchase_amount'] = ['sum','max','min','mean','var']\naggre['installments'] = ['sum','max','min','mean','var']\naggre['purchase_date'] = ['max','min']\naggre['month_lag'] = ['max','min','mean','var']\naggre['month_diff'] = ['mean']\naggre['weekend'] = ['sum', 'mean']\naggre['category_1'] = ['sum', 'mean']\naggre['card_id'] = ['size']","f19f388e":"for col in ['category_2','category_3']:\n    new_merchant_data[col+'_mean'] = new_merchant_data.groupby([col])['purchase_amount'].transform('mean')\n    new_merchant_data[col+'_min'] = new_merchant_data.groupby([col])['purchase_amount'].transform('min')\n    new_merchant_data[col+'_max'] = new_merchant_data.groupby([col])['purchase_amount'].transform('max')\n    new_merchant_data[col+'_sum'] = new_merchant_data.groupby([col])['purchase_amount'].transform('sum')\n    new_merchant_data[col+'_std'] = new_merchant_data.groupby([col])['purchase_amount'].transform('std')\n    aggre[col+'_mean'] = ['mean']","8fb6908f":"new_columns = get_new_columns('new_hist',aggre)\nnew_merchant_data_group = new_merchant_data.groupby('card_id').agg(aggre)\nnew_merchant_data_group.columns = new_columns\nnew_merchant_data_group.reset_index(drop=False,inplace=True)\nnew_merchant_data_group['new_hist_purchase_date_diff'] = (new_merchant_data_group['new_hist_purchase_date_max'] - new_merchant_data_group['new_hist_purchase_date_min']).dt.days\nnew_merchant_data_group['new_hist_purchase_date_average'] = new_merchant_data_group['new_hist_purchase_date_diff']\/new_merchant_data_group['new_hist_card_id_size']\nnew_merchant_data_group['new_hist_purchase_date_uptonow'] = (datetime.datetime.today() - new_merchant_data_group['new_hist_purchase_date_max']).dt.days\nnew_merchant_data_group['new_hist_purchase_date_uptomin'] = (datetime.datetime.today() - new_merchant_data_group['new_hist_purchase_date_min']).dt.days\n#merge with train, test","06d33c7a":"train_data = train_data.merge(new_merchant_data_group,on='card_id',how='left')\ntest_data = test_data.merge(new_merchant_data_group,on='card_id',how='left')","0940649c":"del new_merchant_data_group;\ndel historical_data;\ndel new_merchant_data;","5da04869":"train_data.head()","2c92419d":"train_data['outliers'] = 0\ntrain_data.loc[train_data['target'] < -30, 'outliers'] = 1\ntrain_data['outliers'].value_counts()","5bba0a39":"for df in [train_data,test_data]:\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n    df['month'] = df['first_active_month'].dt.month\n    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n    df['quarter'] = df['first_active_month'].dt.quarter\n    df['is_month_start'] = df['first_active_month'].dt.is_month_start\n    df['hist_last_buy'] = (df['hist_purchase_date_max'] - df['first_active_month']).dt.days\n    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n    df['new_hist_last_buy'] = (df['new_hist_purchase_date_max'] - df['first_active_month']).dt.days\n    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n                     'new_hist_purchase_date_min']:\n        df[f] = df[f].astype(np.int64) * 1e-9","86e404d2":"df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']\ndf['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\ndf['purchase_amount_mean'] = df['new_hist_purchase_amount_mean']+df['hist_purchase_amount_mean']\ndf['purchase_amount_max'] = df['new_hist_purchase_amount_max']+df['hist_purchase_amount_max']\nfor f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n                     'new_hist_purchase_date_min']:\n    df[f] = df[f].astype(np.int64) * 1e-9\ndf['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']\ndf['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\n\nfor f in ['feature_1','feature_2','feature_3']:\n    order_label = train_data.groupby([f])['outliers'].mean()\n    train_data[f] = train_data[f].map(order_label)\n    test_data[f] = test_data[f].map(order_label)","f96a85c8":"train_data_columns = [c for c in train_data.columns if c not in ['card_id', 'first_active_month','target','outliers']]\ntarget = train_data['target']\ndel train_data['target']","8aba630f":"train_data.head()","c469419f":"param = {'num_leaves': 51,\n         'min_data_in_leaf': 35, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.008,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.85,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.82,\n         \"bagging_seed\": 42,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.11,\n         \"verbosity\": -1,\n         \"nthread\": 4,\n         \"random_state\": 2019}\n#prepare fit model with cross-validation\nfolds = StratifiedKFold(n_splits=9, shuffle=True, random_state=2019)\noof = np.zeros(len(train_data))\npredictions = np.zeros(len(test_data))\nfeature_importance_df = pd.DataFrame()\n#run model\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_data,train_data['outliers'].values)):\n    strLog = \"fold {}\".format(fold_)\n    print(strLog)\n    trn_data = lgb.Dataset(train_data.iloc[trn_idx][train_data_columns], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train_data.iloc[val_idx][train_data_columns], label=target.iloc[val_idx])\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 150)\n    oof[val_idx] = clf.predict(train_data.iloc[val_idx][train_data_columns], num_iteration=clf.best_iteration)\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = train_data_columns\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    #predictions\n    predictions += clf.predict(test_data[train_data_columns], num_iteration=clf.best_iteration) \/ folds.n_splits\n    \nstrRMSE = \"\".format(np.sqrt(mean_squared_error(oof, target)))\nprint(strRMSE)\n","6f531a1f":"submission_data = pd.DataFrame({\"card_id\":test_data[\"card_id\"].values})\nsubmission_data[\"target\"] = predictions\nsubmission_data.to_csv(\"baseline_lgb1.csv\", index=False)","5183cf6f":"All Transcation looks authorized ","6dbdfc7a":"As we can see that both train and test active_month data follow the same trends and thats great !!","f8658394":"Ahh!!! one missing value in first_active_month in test data ","e2295643":"As we can see that some targets are below** (-)30** lets check how many are there ","7c3117da":"category_2 follow trends of **->** **1.0 **\ncategory_3 follow ->A,B or C","163e7c79":"As we can see that different feature have different distribution of data according to target \n1. feature_1 value varied from 1 to 5 in target \n2. feature_1 value varied from 1 to 3 in target \n3. feature_1 value varied from 0 to 1 in target ","f0bdc5f0":" *** The field descriptions are as follow:**\n1. card_id - Card identifier\n2. month_lag - month lag to reference date\n1. category_1 - anonymized category\n1. category_2 - anonymized category\n1. category_3 - anonymized category\n3. purchase_date - Purchase date\n4. authorized_flag - 'Y' if approved, 'N' if denied\n1. installments - number of installments of purchase\n1. merchant_category_id - Merchant category identifier (anonymized )\n1. merchant_id - Merchant identifier (anonymized)\n1. purchase_amount - Normalized purchase amount\n1. city_id - City identifier (anonymized )\n1. state_id - State identifier (anonymized )\n1.  subsector_id - Merchant category group identifier (anonymized )\n","63d26145":"****Many more to come stay tuned!!!**\n\n**DON'T FORGET TO UPVOTE :)****"}}