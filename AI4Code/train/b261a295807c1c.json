{"cell_type":{"efb612c7":"code","0f6bfd00":"code","7e13c49e":"code","7a7ef6ce":"code","0436c2ea":"code","532b1f2e":"code","8dbdc577":"code","4c092398":"code","9ad953c5":"code","d5adb221":"code","fbc632e5":"code","53191768":"code","83abb52b":"code","447b327f":"code","fe6f4e84":"code","0b7fd724":"code","f0cf56d5":"code","d965884c":"code","4a8c98fd":"code","3f08cbcf":"code","8eaec17a":"code","5fc2e891":"code","1599d909":"code","f2e2fbc0":"code","726658f2":"code","08aefa61":"code","3117f853":"code","dfd1312b":"code","5b631154":"code","ca7d3e20":"code","8f843980":"code","e14aad8e":"code","96fd1597":"code","093b8890":"code","b4e239aa":"code","550a3848":"code","6e9b9932":"code","108b57e0":"code","a3289dcd":"code","027ce39a":"code","b6218d49":"code","8a340d55":"code","e72d7713":"code","ca069c4b":"code","1184c486":"code","b3a7af3f":"code","4584197d":"code","10c8593a":"code","c4d643a2":"code","0b11dba5":"code","cd8b888f":"markdown","c1b8e361":"markdown","71f7d2dc":"markdown","e2815eac":"markdown","43115e19":"markdown","8b38249b":"markdown","a090e995":"markdown","a560c11e":"markdown","9b293c06":"markdown","b6b67cdb":"markdown","2e1ca759":"markdown","cf51cfcc":"markdown","0f5bf266":"markdown","39d3efdf":"markdown","4bc2b1f4":"markdown","f0781d69":"markdown","87ebc21b":"markdown","b764f1f8":"markdown","f4d41942":"markdown","90b19d07":"markdown","c6e7b153":"markdown"},"source":{"efb612c7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport json\nimport seaborn as sns","0f6bfd00":"print(os.listdir('..\/input\/data-science-bowl-2019\/'))","7e13c49e":"df_train = pd.read_csv(\"..\/input\/data-science-bowl-2019\/train.csv\")\ndf_train_labels = pd.read_csv(\"..\/input\/data-science-bowl-2019\/train_labels.csv\")\ndf_specs = pd.read_csv(\"..\/input\/data-science-bowl-2019\/specs.csv\")\ndf_test = pd.read_csv(\"..\/input\/data-science-bowl-2019\/test.csv\")","7a7ef6ce":"print(\"Shape of training data:{}\".format(df_train.shape))\nprint(\"Shape of training labels data:{}\".format(df_train_labels.shape))\nprint(\"Shape of specs data:{}\".format(df_specs.shape))\nprint(\"Shape of test data:{}\".format(df_test.shape))","0436c2ea":"df_test.installation_id.unique()","532b1f2e":"df_test.query('installation_id==\"0de6863d\"').head(5)","8dbdc577":"df_test.query('installation_id==\"0de6863d\"').tail(5)","4c092398":"print(df_train.columns)\nprint(len(df_train.columns))","9ad953c5":"df_train.head(3)","d5adb221":"#joing train and specs datafrmae to get train_data.\n#result = pd.concat([df_train, df_specs], axis=1, sort=False)\n\n#result = pd.merge(df_train, df_specs, how='outer', on='event_id', left_on=None, right_on=None,\n#         left_index=False, right_index=False, sort=False,\n#         suffixes=('_x', '_y'), copy=True, indicator=False,\n#         validate=None)","fbc632e5":"#result_test = pd.merge(df_test, df_specs, how='outer', on='event_id', left_on=None, right_on=None,\n#         left_index=False, right_index=False, sort=False,\n#         suffixes=('_x', '_y'), copy=True, indicator=False,\n#         validate=None)\n#result_test.head()","53191768":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","83abb52b":"missing_data(df_train)","447b327f":"missing_data(df_test)","fe6f4e84":"missing_data(df_specs)","0b7fd724":"missing_data(df_train_labels)","f0cf56d5":"for col in df_train.columns:\n    print(\"Column name:\", col)\n    print(\"Unique values--->\",df_train[col].nunique())","d965884c":"for col in df_train_labels.columns:\n    print(\"Column Name:\", col)\n    print(\"Unique values--->\", df_train_labels[col].nunique())","4a8c98fd":"for col in df_specs.columns:\n    print(\"Column Name:\", col)\n    print(\"Unique values--->\",df_specs[col].nunique())","3f08cbcf":"for col in df_test.columns:\n    print(\"Column Name:\", col)\n    print(\"Unique values--->\",df_test[col].nunique())","8eaec17a":"%%time\n#extracted_event_data = pd.io.json.json_normalize(df_train.event_data.apply(json.loads))\n#extracted_event_data = pd.io.json.json_normalize(train_df.event_data.apply(json.loads))","5fc2e891":"df_train_labels.columns","1599d909":"df_train_labels.head()","f2e2fbc0":"temp_accuracy_group = df_train_labels.accuracy_group\n\n#sns.barplot(temp_accuracy_group.index, temp_accuracy_group)","726658f2":"df_test.columns","08aefa61":"min_viable_col = ['event_id', 'game_session', 'timestamp', 'event_data',\n       'installation_id', 'event_count', 'event_code', 'game_time', 'title',\n       'type', 'world']","3117f853":"train_data = df_train[min_viable_col]\ntrain_data.columns","dfd1312b":"train_data.drop(['event_data'], axis=1, inplace = True)\ndf_test.drop(['event_data'], axis=1, inplace= True)","5b631154":"train_data.head(10)","ca7d3e20":"df_test.head(10)","8f843980":"def extract_time_features(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['year'] = df['timestamp'].dt.year\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    df['weekofyear'] = df['timestamp'].dt.weekofyear\n    df.drop(['timestamp'], axis=1, inplace=True)\n    return df","e14aad8e":"train_data.shape, df_test.shape","96fd1597":"train_tmp = extract_time_features(train_data)","093b8890":"test_tmp = extract_time_features(df_test)","b4e239aa":"train_tmp.head()","550a3848":"import matplotlib.pyplot as plt","6e9b9932":"#sns.barplot(x=\"month\", y=\"world\", data=train_tmp)\n\nfig, ax =plt.subplots(1, 2, figsize=(12,8))\nsns.countplot(x=\"world\",ax=ax[0], data=train_tmp)\nsns.countplot(x=\"world\", ax=ax[1], data=df_test)\nfig.show()","108b57e0":"fig, ax =plt.subplots(1, 2, figsize=(12,8))\nsns.countplot(x=\"type\",ax=ax[0], data=train_tmp)\nsns.countplot(x=\"type\", ax=ax[1], data=df_test)\nfig.show()","a3289dcd":"fig, ax =plt.subplots(1, 2, figsize=(18,12))\nchart1 = sns.countplot(x=\"title\",ax=ax[0], data=train_tmp)\nchart1.set_xticklabels(chart1.get_xticklabels(), rotation=90)\nchart2 = sns.countplot(x=\"title\", ax=ax[1], data=df_test)\nchart2.set_xticklabels(chart2.get_xticklabels(), rotation=90)\nfig.show()","027ce39a":"fig, ax =plt.subplots(1, 2, figsize=(12,8))\nsns.countplot(x=\"month\",ax=ax[0], data=train_tmp)\nsns.countplot(x=\"month\", ax=ax[1], data=df_test)\nfig.show()","b6218d49":"fig, ax =plt.subplots(1, 2, figsize=(12,8))\nsns.countplot(x=\"year\",ax=ax[0], data=train_tmp)\nsns.countplot(x=\"year\", ax=ax[1], data=df_test)\nfig.show()","8a340d55":"fig, ax =plt.subplots(1, 2, figsize=(12,8))\nsns.countplot(x=\"hour\",ax=ax[0], data=train_tmp)\nsns.countplot(x=\"hour\", ax=ax[1], data=df_test)\nfig.show()","e72d7713":"fig, ax =plt.subplots(1, 2, figsize=(12,8))\nsns.countplot(x=\"dayofweek\",ax=ax[0], data=train_tmp)\nsns.countplot(x=\"dayofweek\", ax=ax[1], data=df_test)\nfig.show()","ca069c4b":"fig, ax =plt.subplots(1, 2, figsize=(12,8))\nsns.countplot(x=\"weekofyear\",ax=ax[0], data=train_tmp)\nsns.countplot(x=\"weekofyear\", ax=ax[1], data=df_test)\nfig.show()","1184c486":"train_tmp.head()","b3a7af3f":"test_tmp.head()","4584197d":"train_tmp.shape, test_tmp.shape","10c8593a":"df_train_labels.head()","c4d643a2":"df_train_labels.shape","0b11dba5":"len(df_train_labels.game_session.unique())","cd8b888f":"## Time feature\n\nBelow snippet of code is directly taken from @shahules768 kernel [here](https:\/\/www.kaggle.com\/shahules\/xgboost-feature-selection-dsbowl). Added a line of code to drop the original timestamp after extracting month, hour, year, day of week, and week of year.","c1b8e361":"Be clear the ```installation_id``` is unique. Now, group all the rows together with same ```installation_id```.","71f7d2dc":"**Finding:**\nHmm.. event_id column is important while joining the tables. We can see two table joins:\n1. Join df_train and df_train_labels,\n2. Join the resulting table from 1st step with df_specs, and\n3. Join the df_test and df_specs,\n\ndf_train has 384 unique event_ids, while df_test has 365 event_ids, and df_specs has 386 unique event_ids. \n\nLearning: Extract Json format \n","e2815eac":"#### Load data","43115e19":"# Data preparation\n\nAs a part of competition, based on our understanding of train, test, specs, and train_labels data, let us join the train and specs to get comprehensive train_data. In addition, join test and specs to get comprehensive comprehensive test_data. \n\nWe have to decide which 'join' strategy to use:\n* Take the union of them all, join='outer'. This is the default option as it results in zero information loss.\n* Take the intersection, join='inner'.\n\nHere, we choose to take \\union of df_train and df_specs, so that we don't loose any information as shown below.","8b38249b":"Let us add labels from train_label. But how?","a090e995":"How I found the objective of this competition?\n* list the unique ```installation_id``` in the test dataset.\n* Select a random ```installation_id```, and do a head and tail operation to see the entires.\n\n### Objective description:\nUnder ```type``` column, last entry is ```Assessment```, which one needs to predict based on the previous sequence of activities ```type``` carried out by a kind on a device. \n\nTake ```installation_id``` of 1st row (i.e. 0001e90f) and group the rows in a new dataframe. This will help in tracking sequence of activities a child carried out on that device.  ","a560c11e":"Unique test values","9b293c06":"Good, done and dusted, seems we have reduced 2 rows, which were containing null value entries. \n\n### Joining test and specs datasets","b6b67cdb":"## Actual data understanding and data prepration part\n\nBelow is a helper function to check missing values and data type. It is taken from @gabriel's kernel here ([source kernel](https:\/\/www.kaggle.com\/gpreda\/2019-data-science-bowl-eda)).  ","2e1ca759":"# Data understanding: \n#### Please upvote, comment, and revisit; I am still improving this kernel notebook.\n\n# Objective:\n\nKnowing the objective of a featured coding competition:\n\n![image.png](attachment:image.png)\n\nCredit to @massoud for his clear [baseline kernel](https:\/\/www.kaggle.com\/mhviraf\/a-baseline-for-dsb-2019). \n","cf51cfcc":"#### Unique value check","0f5bf266":"Unique values in specs dataset","39d3efdf":"Unique value in df_train|","4bc2b1f4":"### Joining train and specs datasets","f0781d69":"Let us drop a difficult to process column for now i.e. 'event_data'. This column has important information about nature of game-play by each child, however, it is in JSON format and difficult to process. For the baseline model, we will not use this column for now. Let us drop 'event_data' from train and test data.","87ebc21b":"Unique value in df_train_label","b764f1f8":"We will need all the columns present in the test.csv to be present the final train dataset for training and predicting. Let us make a col name array which would be easy to store and call required columns","f4d41942":"### What is in the train data?","90b19d07":"Let us explore each table i.e. train, train_labels, specs, and test.","c6e7b153":"Accuracy_group is our target feature in this competition. Let us see it distribution. "}}