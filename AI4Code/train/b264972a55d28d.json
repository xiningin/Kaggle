{"cell_type":{"70f154b0":"code","c9817570":"code","fa03537e":"code","f7042741":"code","27a2a984":"code","c4624952":"code","0514f911":"code","3bc8138e":"code","424a826b":"code","5e0850b5":"code","434ae83d":"code","828b07dd":"code","3db44cae":"code","11b67845":"code","57f41fff":"code","218a7954":"code","4e0837a6":"code","4830e0d9":"code","e9e3f11a":"code","f0eafc9d":"code","5b580a4a":"code","7b5b28ce":"code","b9a859a7":"code","8f27e636":"code","4791033d":"code","82fe0c65":"code","1e847c1d":"code","f088315a":"code","b501d9ae":"code","90bf54fc":"code","876da1b9":"code","ad362774":"code","d1e4c08c":"code","f65a38af":"code","8b3c5e9d":"code","97b4ebe4":"code","de750411":"code","4b229aa6":"code","79f3d728":"code","fe9dc9fd":"code","63d76a37":"markdown","8668356e":"markdown","cc86fcee":"markdown","f110b478":"markdown","1d5c50cf":"markdown","216a355e":"markdown","22950e1e":"markdown","3471314b":"markdown","de2ead05":"markdown","3e06b7cd":"markdown","041d0825":"markdown","8fac7908":"markdown","a36ed20e":"markdown","73b144a9":"markdown","7992a13c":"markdown"},"source":{"70f154b0":"import os\nfrom tqdm.auto import tqdm\nimport time, gc\n\nimport numpy as np\nimport pandas as pd\n# pd.set_option('display.max_columns', None)\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport cv2\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model, Input, load_model\nfrom keras.layers import Dense, Conv2D, Flatten, Activation, Concatenate\nfrom keras.layers import MaxPool2D, AveragePooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom keras.initializers import RandomNormal\nfrom keras.applications import DenseNet121\n\nfrom sklearn.model_selection import train_test_split\n\nstart_time = time.time()","c9817570":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fa03537e":"### Kaggle or Local-PC ###\nKAGGLE = True       # <==== SET ============\nif KAGGLE:\n    DIR = '..\/input\/bengaliai-cv19'\n    DIRwork = '..\/input\/mywork'\nelse:               # local PC\n    DIR = '.\/bengaliai-cv19'\n    DIRwork = '.\/'","f7042741":"### Train or Predict ###\nTRAIN = False        # <==== SET ============\n# True : Do training (GPU time limit over), model save.\n# False: Load trained model and do prediction.\n\nif TRAIN:\n    print('==TRAIN mode==\\n Training Model takes several hours and does not meet the GPU time limit for competition.')\nelse:\n    # Check file existence\n    if os.path.isfile(os.path.join(DIRwork,\"dense.h5\")) \\\n            and os.path.isfile(os.path.join(DIRwork,\"hist.csv\")):\n        print('==Not TRAIN mode (Predict only)==')\n    else:\n        print(\"==ERROR==\\n No 'Model file' and\/or 'history file'.\\n Upload the files to '..\/input\/bengaliai-cv19\/mywork'.\")","27a2a984":"# read CSV files\ntrain_df = pd.read_csv(os.path.join(DIR,'train.csv'))\ntest_df = pd.read_csv(os.path.join(DIR,'test.csv'))\nclass_map_df = pd.read_csv(os.path.join(DIR,'class_map.csv'))\nsample_sub_df = pd.read_csv(os.path.join(DIR,'sample_submission.csv'))\n                            \n# read Parquet Format Image file (example)\nimg_df = pd.read_parquet(os.path.join(DIR,'train_image_data_0.parquet'))","c4624952":"print(train_df.shape)\ntrain_df.head()","0514f911":"print(test_df.shape)\ntest_df.head()","3bc8138e":"print(class_map_df.shape)\nclass_map_df.head()","424a826b":"print(sample_sub_df.shape)\nsample_sub_df.head()","5e0850b5":"print(img_df.shape)\nimg_df.head()","434ae83d":"# target columns\ntgt_cols = ['grapheme_root','vowel_diacritic','consonant_diacritic']","828b07dd":"def get_top(tgt, n, top=True):\n    top_df = train_df.groupby([tgt]).size().reset_index(name='counts') \\\n                            ['counts'].sort_values(ascending=not top)[:n].copy()\n    top_ids = top_df.index\n    top_vals = top_df.values\n    \n    top_df = class_map_df.iloc[top_ids].copy()\n    top_df.drop(['component_type', 'label'], axis=1, inplace=True)\n    top_df['count'] = top_vals\n    return top_df","3db44cae":"def disp_img(df, ids):\n    r_n = len(ids)  # character count\n    c_n = 5         # num of examples for each character\n    plt.figure()\n    fig, ax = plt.subplots(r_n, c_n, figsize=(12, 10))\n    for r, id in enumerate(ids[:r_n]):\n        sumple_ids = train_df[train_df['grapheme_root'] == id].index\n        for c, sumple_id in enumerate(sumple_ids[:c_n]):\n            flattened_image = df.iloc[sumple_id].drop('image_id').values.astype(np.uint8)\n            ax[r, c%c_n].imshow(flattened_image.reshape([137, 236]))\n            ax[r, c%c_n].set_title(str(id)+'(Train_'+str(sumple_id)+')')","11b67845":"desc_df = train_df[tgt_cols].astype('str').describe()\ndesc_df","57f41fff":"# Number of unique types\ntypes = desc_df.loc['unique',:]","218a7954":"top_roots = get_top('grapheme_root', 10)\ntop_roots","4e0837a6":"# Top 5 example\ndisp_img(img_df, top_roots.index[:5])","4830e0d9":"top_vowels = get_top('vowel_diacritic', 10)\ntop_vowels","e9e3f11a":"# Top 5 example\ndisp_img(img_df, top_vowels.index[:5])","f0eafc9d":"top_consonants = get_top('consonant_diacritic', 10)\ntop_consonants","5b580a4a":"disp_img(img_df, top_consonants.index[:5])","7b5b28ce":"SIZE = 64    # input image size\nN_ch = 1","b9a859a7":"def build_densenet():\n    densenet = DenseNet121(weights='imagenet', include_top=False)\n\n    input = Input(shape=(SIZE, SIZE, N_ch))\n    x = Conv2D(3, (3, 3), padding='same')(input)\n    \n    x = densenet(x)\n    \n    x = GlobalAveragePooling2D()(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(256, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n\n    # multi output\n    grapheme_root = Dense(types['grapheme_root'],\n                          activation = 'softmax', name='root')(x)\n    vowel_diacritic = Dense(types['vowel_diacritic'],\n                            activation = 'softmax', name='vowel')(x)\n    consonant_diacritic = Dense(types['consonant_diacritic'],\n                                activation = 'softmax', name='consonant')(x)\n\n    # model\n    model = Model(input,\n                  [grapheme_root, vowel_diacritic, consonant_diacritic])\n    \n    return model","8f27e636":"if TRAIN:\n    # build model\n    model = build_denseNet()\n    \n    # compile\n    model.compile(Adam(lr=0.002),\n              loss={'root': 'categorical_crossentropy',\n                    'vowel': 'categorical_crossentropy',\n                    'consonant': 'categorical_crossentropy'},\n              loss_weights={'root': 0.333,        ## Set weights\n                            'vowel': 0.333,\n                            'consonant': 0.333},\n              metrics={'root': 'accuracy',\n                       'vowel': 'accuracy',\n                       'consonant': 'accuracy'}\n             )\nelse:\n    # Load pretrained-Model\n    model = load_model(os.path.join(DIRwork, 'dense.h5'))","4791033d":"model.summary()","82fe0c65":"# Clip horizontally\ndef clip(img):\n    cols = np.any(img < 200, axis=0)\n    xleft, xright = np.where(cols)[0][[0, -1]]\n    width = xright - xleft\n    center = int((xleft + xright) \/ 2)\n    \n    if width < 137:\n        img = img[:, max(0, center - 70):min(center + 70, 236)]\n    else:\n        img = img[:, max(0, xleft - 2):min(xright + 2, 236)]\n        \n    return img","1e847c1d":"# Resize image size\ndef resize(df, size=64):\n    resized = {}\n    for i in range(df.shape[0]):\n        img = clip(df.loc[df.index[i]].values.reshape(137,236))\n        image = cv2.resize(img,(size,size))\n        resized[df.index[i]] = image.reshape(-1)\n    resized_df = pd.DataFrame(resized).T\n    return resized_df","f088315a":"if TRAIN:\n    # prepare X\n    img_df = img_df.drop(['image_id'], axis = 1)\n    X_df = (resize(img_df, SIZE) \/ 255.).astype('float32')\n    del img_df\n    gc.collect()\n    for i in tqdm(range(1,4)):\n        img_df = pd.read_parquet(os.path.join(\n            DIR, 'train_image_data_'+str(i)+'.parquet'))\n        img_df = img_df.drop(['image_id'], axis = 1)\n        img_df = (resize(img_df, SIZE) \/ 255.).astype('float32')\n        X_df = pd.concat([X_df, img_df], axis = 0)\n        del img_df\n        gc.collect()\n    \n    X_train = X_df.values.reshape(-1, SIZE, SIZE, N_ch)\n    del X_df\n    gc.collect()","b501d9ae":"if TRAIN:\n    # prepare Y\n    train_df = train_df[tgt_cols].astype('uint8')\n    for col in tgt_cols:\n        train_df[col] = train_df[col].map('{:03}'.format)\n    Y_train = pd.get_dummies(train_df)\n\n    del train_df\n    gc.collect()","90bf54fc":"if TRAIN:\n    # Divide the data into train and val set\n    x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train,\n                                                test_size=0.1, random_state=42)\n    y_train_root = y_train.iloc[:,0:types['grapheme_root']]\n    y_train_vowel = y_train.iloc[:,types['grapheme_root']:types['grapheme_root']+types['vowel_diacritic']]\n    y_train_consonant = y_train.iloc[:,types['grapheme_root']+types['vowel_diacritic']:]\n    y_test_root = y_test.iloc[:,0:types['grapheme_root']]\n    y_test_vowel = y_test.iloc[:,types['grapheme_root']:types['grapheme_root']+types['vowel_diacritic']]\n    y_test_consonant = y_test.iloc[:,types['grapheme_root']+types['vowel_diacritic']:]\n    \n    del X_train, Y_train\n    gc.collect()","876da1b9":"print(time.time() - start_time,'[sec]')","ad362774":"batch_size = 128\nepochs = 30           ## set epochs","d1e4c08c":"# Callback : Learning Rate annealer\nreduceLR = ReduceLROnPlateau(monitor = 'val_root_loss',\n                             patience = 2,\n                             factor = 0.5,\n                             min_lr = 1e-5,\n                             verbose = 1)\n# Callback : Save best model\nchkPoint = ModelCheckpoint('dense.h5',\n                           monitor = 'val_root_accuracy',\n                           save_best_only = True,\n                           save_weights_only = False,\n                           mode = 'auto',\n                           period = 1,\n                           verbose = 0)\n# Callback : Early Stop\nearlyStop = EarlyStopping(monitor='val_root_accuracy',\n                          mode = 'auto',\n                          patience = 4,\n                          min_delta = 0,\n                          verbose = 1)","f65a38af":"if TRAIN:\n    history = model.fit(x_train,\n                    {'root': y_train_root,\n                     'vowel': y_train_vowel,\n                     'consonant': y_train_consonant},\n                    batch_size = batch_size,\n                    epochs = epochs,\n                    shuffle = True,\n                    validation_data = (x_test,\n                                       {'root': y_test_root,\n                                        'vowel': y_test_vowel,\n                                        'consonant': y_test_consonant}),\n                    callbacks = [reduceLR, chkPoint, earlyStop],\n                    verbose = 1)\n\n    del x_train, x_test, y_train, y_test\n    gc.collect()","8b3c5e9d":"if TRAIN:\n    df = pd.DataFrame(history.history)\n    df.to_csv('hist.csv',index=False)\nelse:\n    df = pd.read_csv(os.path.join(DIRwork,'hist.csv'))\n    \n# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(1, 2, figsize = (12, 4))\n\nax[0].plot(df[['root_loss','vowel_loss','consonant_loss',\n               'val_root_loss','val_vowel_loss','val_consonant_loss']])\nax[0].set_ylim(0, 2)\nax[0].set_title('Loss')\nax[0].legend(['train_root_loss','train_vowel_loss','train_conso_loss',\n              'val_root_loss','val_vowel_loss','val_conso_loss'],\n             loc='upper right')\nax[0].grid()\nax[1].plot(df[['root_acc','vowel_acc','consonant_acc',\n               'val_root_acc','val_vowel_acc','val_consonant_acc']])\nax[1].set_ylim(0.5, 1)\nax[1].set_title('Accuracy')\nax[1].legend(['train_root_acc','train_vowel_acc','train_conso_acc',\n              'val_root_acc','val_vowel_acc','val_conso_acc'],\n             loc='lower right')\nax[1].grid()","97b4ebe4":"print(time.time() - start_time, '[sec]')","de750411":"row_ids = []\ntargets = []      # prediction result\nid = 0\nfor i in range(4):\n    img_df = pd.read_parquet(os.path.join(\n                            DIR, 'test_image_data_'+str(i)+'.parquet'))\n    img_df = img_df.drop('image_id', axis = 1)\n    img_df = resize(img_df, SIZE) \/ 255.\n    X_test = img_df.values.reshape(-1, SIZE, SIZE, N_ch)\n\n    preds = model.predict(X_test)\n    for j in range(len(X_test)):\n        for k in range(3):\n            row_ids.append('Test_'+str(id)+'_'+tgt_cols[k])\n            targets.append(np.argmax(preds[k][j]))\n        id += 1","4b229aa6":"submit_df = pd.DataFrame({'row_id':row_ids,'target':targets},\n                         columns = ['row_id','target'])\nsubmit_df.head(10)","79f3d728":"print(time.time() - start_time,'[sec]')","fe9dc9fd":"submit_df.to_csv('submission.csv',index=False)","63d76a37":"### Top used Vowel Diacritic","8668356e":"## Observe each data","cc86fcee":"## Multi Output DenseNet Models\nPrepare DenceNet models with three-outputs (for 'grapheme root', 'vowel diacritic' and 'consonant_diacritic')","f110b478":"### Top used Consonant Diacritic","1d5c50cf":"Training Model takes several hours and does not meet the **GPU time limit** for competition. Therefore, this Karnel uses a pretrained-Model.<BR>If you want to Training yourself, set `TRAIN = True`.","216a355e":"`Column['0': '32331']` of img_df has flattened image size `137 * 236 (= 32332)`","22950e1e":"### Setting execution conditions","3471314b":"### Summarize training data","de2ead05":"### Top used Grapheme Root","3e06b7cd":"## Bengali.Ai : Multi_Output_DenseNet121 (keras)","041d0825":"## Predict and Submit","8fac7908":"## Training\n### Prepare Training Data","a36ed20e":"### Fit","73b144a9":"### Read Data Files","7992a13c":"![image.png](attachment:image.png)<BR>\n<BR>\n    I referred to [kaushl Shah : Starter EDA+ Multi Output CNN](https:\/\/www.kaggle.com\/kaushal2896\/bengali-graphemes-starter-eda-multi-output-cnn). Thanks.<BR><BR>\nThis kernel has changed the Model from my [previous kernel](https:\/\/www.kaggle.com\/amanooo\/bengali-ai-multi-output-densenet-keras). \n- Change a small DenseNet built by myself to **DenseNet121**.\n- And I used locally trained model to avoid GPU time limits."}}