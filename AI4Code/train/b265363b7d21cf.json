{"cell_type":{"14cd3844":"code","e2e1e365":"code","064da116":"code","75c281f9":"code","15096fd5":"code","12ba2780":"code","36f36666":"code","0fe8a061":"code","3d752a43":"code","7a53094a":"code","c9f17f1d":"code","51bbe49c":"code","138f1f67":"markdown","2ace9c03":"markdown","ac118708":"markdown","c0e3b5bc":"markdown","43136051":"markdown","5bbb84ab":"markdown"},"source":{"14cd3844":"import  numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets.samples_generator import make_blobs\n\nX1, y1 = make_blobs(n_samples=300, centers=2,random_state=0, cluster_std=0.99)\n\n\nplt.scatter(X1[:,0],X1[:,1],c=y1,cmap='cool')\nplt.show()","e2e1e365":"X1.shape","064da116":"from sklearn.svm import LinearSVC","75c281f9":"clf = LinearSVC()\nclf.fit(X1,y1)","15096fd5":"clf.coef_","12ba2780":"clf.intercept_","36f36666":"w = clf.coef_[0]\na = -w[0] \/ w[1]\n","0fe8a061":"x1 = np.linspace(-1,4.5,50)","3d752a43":"x2 =  a * x1 + (-clf.intercept_[0]) \/ w[1]","7a53094a":"plt.plot(x1,x2)\nplt.scatter(X1[:,0],X1[:,1],c=y1,cmap='winter')\nplt.title('when C={}'.format(1.0))\nplt.xlabel('feature_1')\nplt.ylabel('feature_2')\nplt.show()","c9f17f1d":"def plot_line(clf):\n    w = clf.coef_[0]\n    a = -w[0] \/ w[1]\n    x1 = np.linspace(-1,4.5,50)\n    x2 =  a * x1 + (-clf.intercept_[0]) \/ w[1]\n    return x1,x2\n\n    ","51bbe49c":"\n#plt.figure(figsize=(8,6))\nfig,subplt = plt.subplots(1,2,figsize=(12,4))\np=[.01,10]\nfor i,sub in zip(p,subplt):\n    clf = LinearSVC(C=i)\n    clf.fit(X1,y1)\n    \n    x1,x2=plot_line(clf)\n    \n    title='When C={}'.format(i)\n    sub.plot(x1,x2)\n    sub.set_title(title)\n    sub.scatter(X1[:,0],X1[:,1],c=y1,cmap='winter')\n    sub.set_xlabel('feature_1')\n    sub.set_ylabel('feature_2')\n   \n    \n    ","138f1f67":"### Why margin should be maximum..?","2ace9c03":"**SVM provides a parameter called 'C' to our Linear SVM classifier**","ac118708":"### Support Vector Machine(SVM)\n\n**The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N\u200a\u2014\u200athe number of features) that distinctly classifies the data points.**","c0e3b5bc":"To separate the two classes of data points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane that has the maximum margin..\n\nThe margin is defined as the distance between the separating hyperplane(decision boundary) and the training samples that are closest to this hyperplane, which are called support vectors.","43136051":"**The reason  behind having decision boundaries with large margin is that they tend to have a lower generalization error i.e lower error on testing data  whereas models with small margins are more prone to overfitting..**","5bbb84ab":"### C is a regularization parameter\n\nFor large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly.\n\n\na very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies some points"}}