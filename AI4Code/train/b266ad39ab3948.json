{"cell_type":{"7b6b87cf":"code","5ef71108":"code","b8b39c76":"code","a8d2478a":"code","467c6a4d":"code","e2e52b5c":"code","cc3d7ec3":"code","4f5ebe04":"code","8b5a4bfa":"code","881763ef":"code","77823478":"code","dd591ff9":"code","9d6ecedb":"code","cd05ef4c":"code","b11705f8":"code","f8b7a50c":"code","53232931":"code","6e2dcbf4":"code","8bd0a486":"code","8ed93e11":"code","d9330b6f":"code","9ef2c559":"code","8a78beb8":"code","593c742c":"code","a0c35b93":"code","4e9e91c2":"code","4bfb34b3":"code","7c8cec98":"code","a61b7b25":"code","0e4d4d53":"code","edf2ca60":"code","f407ebd1":"code","c11f7a52":"code","cdc64cb2":"code","423307ae":"code","323e17d8":"code","1294525b":"code","c6e1d836":"code","742eed98":"code","8178b6c4":"code","67e0f48c":"code","fb2001fd":"code","8c5d2641":"markdown","04814468":"markdown","4508d825":"markdown","da630035":"markdown","76268cd0":"markdown","404faf26":"markdown","9f41d7e8":"markdown","ce0c1909":"markdown","6f0cda8d":"markdown","c6e07093":"markdown","1188f687":"markdown","2b71c91c":"markdown","ced1eb52":"markdown","2ad6c901":"markdown","5c342bf8":"markdown","efc3b89f":"markdown","b0430acf":"markdown","ca80db18":"markdown","4254bce9":"markdown","d1e87846":"markdown","fa1b1eb7":"markdown","3d03228f":"markdown","b9c4a3c9":"markdown","cd7891a9":"markdown","b6e5aa2a":"markdown","a0839fb7":"markdown","4892aa8e":"markdown","aea15610":"markdown","724176b2":"markdown","df9f7b06":"markdown","b35eb2e9":"markdown","4ede3c75":"markdown","ea91b5c9":"markdown","da7f5cd3":"markdown","f1b17f5a":"markdown","45940590":"markdown","af15de63":"markdown"},"source":{"7b6b87cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5ef71108":"train = pd.read_csv('\/kaggle\/input\/bigmart-sales-data\/Train.csv')","b8b39c76":"train.shape\n\n#so the TRAIN data has 8523 rows and 12 columns","a8d2478a":"# get the list of all columns in TRAIN data\ntrain.columns","467c6a4d":"# finding out the datatype of each column\ntrain.info()","e2e52b5c":"#we shall separate the categorical and numeric columns\ncat_data = []\nnum_data = []\n\nfor i,c in enumerate(train.dtypes):\n    print(i,c)\n    if c == object or c == int:\n        cat_data.append(train.iloc[:, i])\n    else :\n        num_data.append(train.iloc[:, i])\n\ncat_data = pd.DataFrame(cat_data).transpose()\nnum_data = pd.DataFrame(num_data).transpose()\ncat_data.head()","cc3d7ec3":"import pandas_profiling as prof\n\n#report = prof.ProfileReport(train)\n#report","4f5ebe04":"cat_data.Item_Fat_Content.unique()\ncat_data['Item_Fat_Content'] = cat_data['Item_Fat_Content'].replace({'low fat': 'Low Fat', 'LF': 'Low Fat','reg': 'Regular'})\nprint(cat_data.Item_Fat_Content.unique())\ncat_data.head()\n##now we see that we now have only two unique Fat_content types\n\ntrain = pd.concat([cat_data,num_data], axis = 1)\ntrain.head()\ntrain.shape","8b5a4bfa":"## to start with a pairplot\nimport seaborn as sns\nimport matplotlib.pyplot as plt","881763ef":"sns.pairplot(train)\nplt.show()","77823478":"num_data.corr()\nsns.heatmap(num_data.corr(),annot=True)","dd591ff9":"# lets get the basic statistical measures for num_data\nnum_data.describe()","9d6ecedb":"cat_data.describe()","cd05ef4c":"cat_data['Item_Identifier'].value_counts()\ncat_data = cat_data.drop(['Item_Identifier'], axis=1)\ncat_data.columns","b11705f8":"for i in cat_data.columns:\n    print()\n    print(i)\n    print(cat_data[i].value_counts())\n    print()\n    print()","f8b7a50c":"cat_columns = cat_data.columns\ncat_columns = list(cat_columns)\ncat_columns.remove('Outlet_Establishment_Year')\ncat_columns_new = cat_columns\n\n\nfor cat in cat_columns_new:\n    print(cat)\n    print()\n    for i in num_data.columns:\n        print(i, \"vs\", cat)\n        sns.set(style=\"whitegrid\")\n        sns.boxplot(train[i], train[cat])\n        plt.show()","53232931":"train.columns","6e2dcbf4":"## 1. Which class in each category corresponds to maximum sales\n\nfor cat in cat_data.columns:\n    print(\"Item_Outlet_Sales in Thousands ('000)\")\n    print()\n    print(\"-\"*20 + cat + '  vs' + '  Item_Outlet_Sales' + \"-\"*20)\n    output = train[[cat,'Item_Outlet_Sales']].groupby([cat]).apply(lambda x: x['Item_Outlet_Sales'].sum()\/1000).sort_values(ascending=False)\n    output = pd.DataFrame(output)\n    output.columns = ['Item_Outlet_Sales']\n    ax = sns.barplot(output.index,'Item_Outlet_Sales', data =output)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n    for p in ax.patches:\n        ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width()\/ 2., p.get_height()),ha='center', va='center', rotation=90, xytext=(0,40), textcoords='offset points')  #vertical bars\n    plt.tight_layout()\n    plt.show()\n    print()\n    print(\"Maximum Sales : \")\n    print(output.head(1))\n    print()\n    print(\"-\" *50)","8bd0a486":"cat_data.describe()\ntrain.groupby(['Outlet_Identifier','Outlet_Establishment_Year']).size().reset_index(name='Freq')\n#train['Outlet_Identifier'].unique()\n#train['Outlet_Establishment_Year'].unique()","8ed93e11":"train.columns\nyear_store_sales = train[['Outlet_Identifier','Outlet_Establishment_Year','Item_Outlet_Sales']].groupby(['Outlet_Identifier','Outlet_Establishment_Year']).apply(lambda x: x['Item_Outlet_Sales'].sum()\/1000).sort_values(ascending=False)\nyear_store_sales = pd.DataFrame(year_store_sales)\nyear_store_sales.columns = ['Outlet_Sales']\nyear_store_sales","d9330b6f":"#To find out the reason for the above case we can act smart and subset data on the Outlet_Identifier and\n#then remove duplicates (assuming that an outlet will be at only one location)\n\noutlets = train[['Outlet_Identifier',\"Outlet_Size\",\"Outlet_Location_Type\",\"Outlet_Type\"]]\nprint(outlets.head())\nprint()\nprint(\"Before removing NA : \" , outlets.shape)","9ef2c559":"# removing duplicates\noutlets_new = outlets.drop_duplicates(subset=['Outlet_Identifier'])\noutlets_new","8a78beb8":"train[train.Outlet_Identifier.isin(['OUT010', \"OUT045\", \"OUT017\"])].shape[0] \/ train.shape[0]","593c742c":"ld = train[[\"Outlet_Identifier\",\"Outlet_Location_Type\",\"Outlet_Size\",\"Outlet_Type\"]].groupby(['Outlet_Location_Type',\"Outlet_Type\",\"Outlet_Identifier\"]).size()\nld = pd.DataFrame(ld)\nld.columns = ['Count']\nld","a0c35b93":"## finding the null values and treating them\nheat = sns.heatmap(train.isnull(), cbar=False)\nplt.show()\nNull_percent = train.isna().mean().round(4)*100\n\nNull_percent = pd.DataFrame({'Null_percentage' : Null_percent})\nNull_percent.head()\nNull_percent = Null_percent[Null_percent.Null_percentage > 0].sort_values(by = 'Null_percentage', ascending = False)\nprint(\"Percentage of Null cells : \\n \\n \" , Null_percent)","4e9e91c2":"print(cat_data.columns)\nprint(num_data.columns)\n\ncat_data_new = cat_data.drop(['Outlet_Size'], axis =1)\nnum_data_new = num_data.drop(['Item_Weight'], axis =1) \ncat_data_new.head()\nnum_data_new.head()\n\nNull_percent_cat = cat_data_new.isna().mean().round(4)*100\nprint(Null_percent_cat)\nNull_percent_num = num_data_new.isna().mean().round(4)*100\nprint(Null_percent_num)","4bfb34b3":"## the columns we have to encode\n## we shall not encode Outlet Establishment Year column\ncat_data_new['Outlet_Establishment_Year'] = pd.to_numeric(cat_data_new['Outlet_Establishment_Year'])\ncat_data_new.info()","7c8cec98":"## loading Standardscaler and OneHotEncoder from Sklearn\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder","a61b7b25":"num_data_new.columns","0e4d4d53":"## num_data has certain columns with some very high valued columns and some very low, thus we \n#should standardize the values of these columns\n\ny = num_data_new['Item_Outlet_Sales']\nnum_data_new = num_data_new.drop(['Item_Outlet_Sales'], axis = 1)\nprint(num_data_new.columns)\n\nfor col in num_data_new.columns:\n    num_data_new[col] = (num_data_new[col]-num_data_new[col].min())\/(num_data_new[col].max() - num_data_new[col].min())\n    \nnum_data_new.head()","edf2ca60":"##Label Encoding\nfrom sklearn.preprocessing import LabelEncoder  \nle = LabelEncoder()\ncat_data_new.head()","f407ebd1":"# transform categorical columns columns\n\nfor i in cat_data_new.loc[:,~cat_data_new.columns.isin(['Outlet_Establishment_Year'])]:\n    cat_data_new[i] = le.fit_transform(cat_data_new[i])\n\ncat_data_new.head()\ncat_data_new.shape","c11f7a52":"cat_data_new.columns\ncat_data_new = pd.get_dummies(cat_data_new, columns=['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier','Outlet_Location_Type', 'Outlet_Type'])\ncat_data_new.head()\ncat_data_new.shape","cdc64cb2":"x = pd.concat([num_data_new,cat_data_new], axis = 1)\nx.head()\ny","423307ae":"## I have an idea, why not convert the Establishment year \n#column to Years of existence by subtracting it by the current year\n\nx['Outlet_Establishment_Year'] = 2020 - x['Outlet_Establishment_Year']\n\nx.rename(columns = {\"Outlet_Establishment_Year\" : \"Years_since\"}, inplace = True)\n \nx['Years_since'].describe()","323e17d8":"data_dummy = pd.concat([x,y], axis =1)\ndata_dummy.columns","1294525b":"from sklearn.model_selection import train_test_split\ntrain,test = train_test_split(data_dummy,test_size=0.20,random_state=2019)\nprint(train.shape)\nprint(test.shape)","c6e1d836":"train_label=train['Item_Outlet_Sales']\ntest_label=test['Item_Outlet_Sales']\ndel train['Item_Outlet_Sales']\ndel test['Item_Outlet_Sales']","742eed98":"# algos to be used\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# evaluating the model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n","8178b6c4":"model_df = {'Name':['LR', 'Ridge', 'Lasso', 'E_Net','SVR','Dec_Tree','RF','Bagging_Reg','AdaBoost','Grad_Boost'],\n             'Model' : [LinearRegression(), Ridge(alpha=0.05,solver='cholesky'), Lasso(alpha=0.01) ,ElasticNet(alpha=0.01,l1_ratio=0.5),\n                     SVR(epsilon=15,kernel='linear'),DecisionTreeRegressor(),\n                     RandomForestRegressor(),BaggingRegressor(max_samples=70),AdaBoostRegressor(),GradientBoostingRegressor()]}\n\nmodel_df = pd.DataFrame(model_df)\nmodel_df['Cross_val_score_mean'], model_df['Cross_val_score_STD'] = 0,0\nmodel_df\n","67e0f48c":"for m in range(0,model_df.shape[0]):\n    print(model_df['Name'][m])\n    score=cross_val_score(model_df['Model'][m] , train , train_label , cv=10 , scoring='neg_mean_squared_error')\n    score_cross=np.sqrt(-score)\n    model_df['Cross_val_score_mean'][m] = np.mean(score_cross)\n    model_df['Cross_val_score_STD'][m] = np.std(score_cross)\n    \nmodel_df","fb2001fd":"model_df.sort_values(by=['Cross_val_score_mean'])","8c5d2641":"# I am unable to make anything but the Skewness in certain features on the num_data out of this pairplot .. Let us try and infer something from a correlation plot","04814468":"Now, we have Null free data. \n\nNow we shall encode our categorical variables and create dummy variables of them, to get the data ready for modelling.","4508d825":"Relationship between 'Item_Weight', 'Item_Visibility', 'Item_MRP', 'Item_Outlet_Sales' and Outlet_Location_Type","da630035":"We shall apply the following Regression techniques\n1. Linear Regression\n2. Ridge Regression\n3. Lasso Regression\n4. Elastic net\n5. Stochastic Gradient\n6. SVR (Support vector regression)\n7. Decision Tree\n8. Random Forest\n9. Bagging Regression\n10. Adaptive Boosting (Ada Boost) \n11. Gradient Boosting\n\nWe shall apply Cross Validation to these","76268cd0":"*In the above summary, I smell an opportunity to try and bin the Item_weight columns and even Item_visibility.\nWe shall not bin Item_MRP because in our prior analysis we observed a good correlation between Tem_MRP and Outlet_Sales.*","404faf26":"## From the above Correlation heat map of Numerical data we infer that :\n-- The MRP of a product has quiet a strong affect on the Outlet sales of the products\n>     i.e. **The Total_Sales of Outlets is Price sensitive**","9f41d7e8":"# Now, we will have to create dummy variables of the cat data which we have","ce0c1909":"Now we have the dataframe of Model and model_names ready. We shall now proceed by training the data on each model using a for loop also in the process extract their cross val score (mean and std) to evaluate the model performance","6f0cda8d":"# I try and use the best and optimized techniques which have quality as well as save you some time. Check out my work below.\n \n# Do upvote if you find the notebok useful.\n \n# Also, please feel free to ask questions and do let me know about my work.","c6e07093":"In the above dataframe we can see the result of our regression models.\n\n- Gradient Boosting technique worked the best. Hence one can try and improve the model and further predict the values of the Test dataset.\n","1188f687":"# **It is a point to be noted that one can see many questions and try find an answer to those, but one should be smart enough to ask good question and seek their answers.\n\n# **Now we shall move forward and attend the problem of missing values**","2b71c91c":"# Creating Boxplots for each Numerical column to understand the spread of the data","ced1eb52":"# Null Value handling","2ad6c901":"# Now, I shall list down the possible questions which we can answer using the 4 Numerical features and some Categorical variables\n\n1. Which class in each category corresponds to maximum sales\n2. The Outlet corresponding to max and min sales and why\n3. The number of outlets in a Location type\n4. The relation between Outlet_Sales, Outlet Location type and Outlet size\n5. The relation between Outlet_Sales, Outlet Location type and Outlet type\n6.","5c342bf8":"# Applying Regression models","efc3b89f":"From the above output we can infer about the most common classes in each Categorical column","b0430acf":"The table above answers ques 3rd, 4th and 5th .. \nIt gives us the count of Outlets w.r.t. the Outlet_type and Location_type\nNOTE: I have ommited Outlet_Size because we dont have Outlet_size for Outlet 10,45 and 17","ca80db18":"# # The 3rd question\n# # \"The number of outlets in a Location type\"","4254bce9":"Now, we shall use visualizations to understand and take meaning out of the data","d1e87846":"From the above analysis we infer that among Oultlet 17,19,35,17\n-- Other than Outlet 19 all the other 3 top Sales outlets are Supermarkets (2 of type 1 and 1 of Type 3)\n-- Also, along with this, we now have an the info that Outlet10, Outlet45, Outlet17  doesnt have a mentioned Outlet Size, and these correspond t0 2410 rows i.e around 28% of the total rows.\n\nNow we have two options to solve this situation. Either we can drop Outlet_Size or try and impute the blank cells with some logical values\n\nWe shall tackle this while imputation","fa1b1eb7":"Before getting on with EDA activity, we shall first segregate the train dataset on the basis of their datatypes. \n- Cat_data (categorical data)\n- Num_data (Numeric data)  \nNote : The Year column will be included in Num_data","3d03228f":"From the analysis above, we have the following information :\n1. Item_Weight has 1463 (17.2%) missing values\n2. Outlet_Size has 2410 (28.3%) missing values\n3. Item Fat content has a naming issue, we need to convert LF & low fat to Low Fat, and reg to Regular\n4. Cramer's V statistics shows that Outlet_Size, Outlet_type, Outlet_location_type and Outlet Identifier are well correlated","b9c4a3c9":"From the above analysis we infer that Outlet OUT027 and OUT019 were established in the same year but 27 has max sales whereas 19 is at the bottom.\n\n*Furthermore, we can see that OUT035,OUT017 has good sales given their Establishment year.* \n\n# Let us try and find out why ?","cd7891a9":"# Now we have the data ready to be modelled","b6e5aa2a":"Relationship between 'Item_Weight', 'Item_Visibility', 'Item_MRP', 'Item_Outlet_Sales' and Outlet_Location_Type","a0839fb7":"Segregating train data into dependent and independent variable dataset","4892aa8e":"The visuals above gives us the information above the Total sales for each class of each category.\n\nSome of the key information is stated below:\n1. SuperMarket Type 1 correspond to Max. sales i.e 12917.34 (in thousands)\n2. Tier 3 locations have max sales to the tune of 7636.75 (in thousands)\n3. Outlets with medium size have max sales to the tune of 7489.72 (in thousands)\n4. Outlet OUT027 has the max sales of 3453.9 (in thousands)\n5. People purchase Fruits and Vegetables the most and second comes Snacks\n6. Low fat goods sell the most","aea15610":"# **Lets answer our 2nd ques.\n# \"The Outlet corresponding to max and min sales and the possible reason for those sales\"**","724176b2":"The above plots speak a lot of the data. Do give it a look and understand the data and its features well.","df9f7b06":"We shall take a look at the data first\n- shape\n- data types\n- columns","b35eb2e9":"# Lets do some Pre imputation visualization","4ede3c75":"# Lets perform some Univariate analysis","ea91b5c9":"Scaling and encoding","da7f5cd3":"Treating the Item_Fat_Content column","f1b17f5a":"# Standard scaling and Label Encoding","45940590":"Lets take a look at the columns with null values","af15de63":"From the analysis above we understand that only 2 columns have null values.\n\nNow we will have to take a decision either to keep them and impute or drop them from the data and move ahead.\n\nI prefer dropping a feature if the Percentage of null values in it is greater than 15% . Also we have a small dataset at our disposal.\nSo, I shall drop these two columns."}}