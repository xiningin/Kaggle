{"cell_type":{"c4483503":"code","ac63467e":"code","2ef22acc":"code","2e723abc":"code","46d56699":"code","8aee4960":"code","fc44b29d":"code","fa7a807a":"code","4e25cc53":"code","497ebcda":"code","666819d3":"code","1bc11cef":"code","3949fe70":"code","7ea92f16":"code","0db74e12":"code","f51ec153":"code","48de426b":"code","6069e8e7":"code","8d53d998":"code","7a7e23bf":"code","368832ec":"code","4e2f19a0":"code","0cbc5b99":"code","a250007f":"code","5de17e56":"code","fa59a1e9":"code","4bcdec1e":"code","354cf143":"code","8a487b3e":"code","2d245634":"code","6fe396f3":"code","3ad38d67":"code","3c5942ac":"code","a7e4517d":"code","a7785c6c":"code","a5083650":"code","1e14fedd":"code","f84db943":"code","a60ad6a1":"code","1b110490":"code","1cc7e673":"code","abf0ceb9":"code","915972cf":"code","38d847d2":"code","a8db0f74":"code","f2f02594":"code","4232fd33":"code","db1fa5ee":"code","bfe86097":"code","b87fb0b8":"code","99e100ab":"code","438a9477":"code","d57ec626":"code","19bf360f":"code","8f2fb738":"code","075e6010":"code","6a489ad7":"code","20da0522":"code","70ac1653":"code","5c407926":"code","71eb0d8b":"code","e63d9d7e":"code","594b6b9a":"code","2563ece6":"code","870b4609":"code","5b1d063f":"code","c08f5c02":"code","4b4756b6":"code","2995f4b3":"code","05acfb70":"code","303b8d29":"code","b7c25886":"code","bbf58165":"code","e67dc452":"code","65ee6a66":"code","4754aa43":"code","71d2c9a0":"code","fd60b2aa":"code","f6646ee9":"code","49339fd8":"code","c003307b":"code","2b1c1843":"code","1d889727":"markdown","125cc4b0":"markdown","9b2ae1cb":"markdown","dd74713d":"markdown","d1af2574":"markdown","01f715ee":"markdown","1abbef1f":"markdown","d538e880":"markdown","36773d5b":"markdown","24e89147":"markdown","9fa6efbc":"markdown","dd1985b2":"markdown","2a908129":"markdown","a8a21fc7":"markdown","1fd8a025":"markdown","d7960117":"markdown","afbba41c":"markdown","345a94e6":"markdown","ed62d44a":"markdown","53c6b71b":"markdown","20e33123":"markdown","32d027b0":"markdown","fb56a72d":"markdown","75228c91":"markdown","da2c82ce":"markdown","4d098edb":"markdown","f505e592":"markdown","3921070f":"markdown","420a6914":"markdown","5f3bc9a6":"markdown","b0749320":"markdown","03160d21":"markdown","3bbeaba4":"markdown","16f722a4":"markdown","d2d300aa":"markdown","b93096d9":"markdown","f6f7c7b2":"markdown","46c87743":"markdown","123acdff":"markdown","86ca3e84":"markdown","68e2afb3":"markdown","d2ce46b1":"markdown","48dfca8e":"markdown","b2f09984":"markdown","a5b978ff":"markdown","e3038c48":"markdown","987c135b":"markdown","d3954aa2":"markdown","b06983c7":"markdown","60da3dd0":"markdown","36ab8565":"markdown","cd441036":"markdown","17a8ca17":"markdown","5988e191":"markdown","7a561a86":"markdown","7dd3d211":"markdown","1a2ea806":"markdown"},"source":{"c4483503":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom collections import Counter\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n","ac63467e":"df_train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\nIDtest = df_test[\"PassengerId\"]","2ef22acc":"df_train.shape","2e723abc":"df_test.shape","46d56699":"df_train.head()","8aee4960":"df_test.head()","fc44b29d":"df_train[\"Survived\"].value_counts()","fa7a807a":"df_train.info()","4e25cc53":"dataset =  pd.concat(objs=[df_train, df_test], axis=0).reset_index(drop=True)","497ebcda":"# Fill empty and NaNs values with NaN\ndataset = dataset.fillna(np.nan)\n\n# Check for Null values\ndataset.isnull().sum()","666819d3":"df_train.isnull().sum()","1bc11cef":"df_train.dtypes","3949fe70":"#Summarize and statistics\ndf_train.describe()","7ea92f16":"g = sns.heatmap(df_train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","0db74e12":"g = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=df_train,kind=\"bar\", size = 6 ,palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Survival Probability\")","f51ec153":"g = sns.factorplot(x=\"Parch\",y=\"Survived\",data=df_train,kind=\"bar\", size = 6 ,palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Survival Probability\")","48de426b":"g = sns.FacetGrid(df_train, col='Survived')\ng = g.map(sns.distplot, \"Age\", color=\"c\")","6069e8e7":"# Explore Age Distibution\ng = sns.kdeplot(df_train[\"Age\"][(df_train[\"Survived\"] == 0) & (df_train[\"Age\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(df_train[\"Age\"][(df_train[\"Survived\"] == 1) & (df_train[\"Age\"].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","8d53d998":"df_train['Fare'].isnull().sum()","7a7e23bf":"dataset['Fare'].isnull().sum()","368832ec":"#Let's fill the Fare missing values with the median value\ndataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].median())","4e2f19a0":"# Explore Fare distribution \ng = sns.distplot(dataset[\"Fare\"], color=\"b\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","0cbc5b99":"# Apply log to Fare to reduce skewness distribution\ndataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","a250007f":"#After log transformation to reduce the skewness\ng = sns.distplot(dataset[\"Fare\"], color=\"b\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","5de17e56":"g = sns.barplot(x=\"Sex\",y=\"Survived\",data=df_train)\ng = g.set_ylabel(\"Survival Probability\")","fa59a1e9":"df_train[[\"Sex\",\"Survived\"]].groupby('Sex').mean()","4bcdec1e":"g = sns.factorplot(x=\"Pclass\",y=\"Survived\", data=df_train, kind=\"bar\", size = 6 ,palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Survival Probability\")","354cf143":"# Explore Pclass vs Survived by Sex\ng = sns.factorplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=df_train,\n                   size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Survival Probability\")","8a487b3e":"df_train[\"Embarked\"].isnull().sum()","2d245634":"dataset[\"Embarked\"].isnull().sum()","6fe396f3":"dataset['Embarked'].value_counts()","3ad38d67":"#Let's fill Embarked missing values of dataset set with 'S' most frequent value\ndataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\")","3c5942ac":"# Explore Embarked vs Survived \ng = sns.factorplot(x=\"Embarked\", y=\"Survived\",  data=df_train,\n                   size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","a7e4517d":"dataset.isnull().sum()","a7785c6c":"# Explore Age vs Sex, Parch, Pclass and SibSP.\ng = sns.catplot(y=\"Age\",x=\"Sex\",data=dataset,kind=\"box\")\ng = sns.catplot(y=\"Age\",x=\"Sex\",hue=\"Pclass\", data=dataset,kind=\"box\")\ng = sns.catplot(y=\"Age\",x=\"Parch\", data=dataset,kind=\"box\")\ng = sns.catplot(y=\"Age\",x=\"SibSp\", data=dataset,kind=\"box\")","a5083650":"# Let's convert Sex into 0 for male and 1 for female\ndataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\": 0, \"female\":1})","1e14fedd":"g = sns.heatmap(dataset[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(), cmap=\"BrBG\", annot=True)","f84db943":"# Filling missing value of Age \n\nIndex_Missing_Age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n\nfor i in Index_Missing_Age:\n    age_med = dataset[\"Age\"].median()\n    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        dataset['Age'].iloc[i] = age_pred\n    else :\n        dataset['Age'].iloc[i] = age_med","a60ad6a1":"g = sns.catplot(x=\"Survived\", y = \"Age\",data = df_train, kind=\"box\")\ng = sns.catplot(x=\"Survived\", y = \"Age\",data = df_train, kind=\"violin\")","1b110490":"dataset[\"Name\"].head()","1cc7e673":"# Get Title from Name\ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\ndataset[\"Title\"] = pd.Series(dataset_title)\n\ndataset[\"Title\"].head()","abf0ceb9":"g = sns.countplot(x=\"Title\",data=dataset)\ng = plt.setp(g.get_xticklabels(), rotation=90) ","915972cf":"# Convert to categorical values Title \ndataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndataset[\"Title\"] = dataset[\"Title\"].astype(int)","38d847d2":"g = sns.countplot(dataset[\"Title\"])\ng = g.set_xticklabels([\"Master\",\"Miss\/Ms\/Mme\/Mlle\/Mrs\",\"Mr\",\"Rare\"])","a8db0f74":"#Surviving probabilities of this groups\ng = sns.catplot(x=\"Title\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_xticklabels([\"Master\",\"Miss-Mrs\",\"Mr\",\"Rare\"])\ng = g.set_ylabels(\"Survival Probability\")","f2f02594":"# Drop Name variable\ndataset.drop(labels = [\"Name\"], axis = 1, inplace = True)","4232fd33":"# Create a family size descriptor from SibSp and Parch\ndataset[\"Family_Size\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1","db1fa5ee":"g = sns.catplot(x=\"Family_Size\",y=\"Survived\",data = dataset, kind = \"point\")\ng = g.set_ylabels(\"Survival Probability\")","bfe86097":"#Create categories of the family size\ndataset['Single'] = dataset['Family_Size'].map(lambda s: 1 if s == 1 else 0)\ndataset['Small_Family'] = dataset['Family_Size'].map(lambda s: 1 if  s == 2  else 0)\ndataset['Medium_Family'] = dataset['Family_Size'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndataset['Large_Family'] = dataset['Family_Size'].map(lambda s: 1 if s >= 5 else 0)","b87fb0b8":"g = sns.catplot(x=\"Single\", y=\"Survived\", data=dataset, kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\n\ng = sns.catplot(x=\"Small_Family\", y=\"Survived\", data=dataset, kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\n\ng = sns.catplot(x=\"Medium_Family\", y=\"Survived\", data=dataset, kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\n\ng = sns.catplot(x=\"Large_Family\", y=\"Survived\", data=dataset, kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")","99e100ab":"#Convert to itle and Embarked (from long format to wide format)\ndataset = pd.get_dummies(dataset, columns = [\"Title\"])\ndataset = pd.get_dummies(dataset, columns = [\"Embarked\"], prefix=\"Em\")","438a9477":"dataset.head()","d57ec626":"dataset[\"Cabin\"].head()","19bf360f":"dataset[\"Cabin\"].describe()","8f2fb738":"dataset[\"Cabin\"].isnull().sum()","075e6010":"# Replace the Cabin number by the type of cabin 'X' if it is null\ndataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])","6a489ad7":"g = sns.countplot(dataset[\"Cabin\"],order=['A','B','C','D','E','F','G','T','X'])","20da0522":"g = sns.catplot(y=\"Survived\",x=\"Cabin\",data=dataset,kind=\"bar\",order=['A','B','C','D','E','F','G','T','X'])\ng = g.set_ylabels(\"Survival Probability\")","70ac1653":"dataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\")","5c407926":"dataset[\"Ticket\"].head()","71eb0d8b":"# Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X. \n\nTicket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ndataset[\"Ticket\"] = Ticket\ndataset[\"Ticket\"].head()","e63d9d7e":"dataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\")","594b6b9a":"# Create categorical values for Pclass\ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\ndataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")","2563ece6":"# Drop unnecessary variables \ndataset.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)","870b4609":"dataset.columns","5b1d063f":"dataset.head()","c08f5c02":"dataset['Survived'].value_counts()","4b4756b6":"# Separate the dataset into train and test.\ntrain_len = len(df_train)\n\ndf_train = dataset[:train_len]\ndf_test = dataset[train_len:]\ndf_test.drop(labels=[\"Survived\"],axis = 1,inplace=True)","2995f4b3":"df_train.shape","05acfb70":"df_test.shape","303b8d29":"# Separate train features and label \n\ndf_train[\"Survived\"] = df_train[\"Survived\"].astype(int)\n\ny_train = df_train[\"Survived\"]\n\nX_train = df_train.drop(labels = [\"Survived\"],axis = 1)","b7c25886":"# Cross validate model with Kfold stratified cross validation\nkfold = StratifiedKFold(n_splits=10)","bbf58165":"# Modeling step Test differents algorithms \nrandom_state = 1992\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","e67dc452":"# AdaBoost\n\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaDTC.fit(X_train,y_train)\n\nada_best = gsadaDTC.best_estimator_\n\n\n# Best score\ngsadaDTC.best_score_","65ee6a66":"#ExtraTrees \nExtC = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(X_train,y_train)\n\nExtC_best = gsExtC.best_estimator_\n\n\n# Best score\ngsExtC.best_score_","4754aa43":"# Random Forest \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [4,5,7],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[80,100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n\n# Best score\ngsRFC.best_score_","71d2c9a0":"# Gradient Boosting\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [80,100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4,5,6,8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","fd60b2aa":"# SVC\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(X_train,y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\n\n# Best score\ngsSVMC.best_score_","f6646ee9":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF Learning Curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees Learning Curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVC Learning Curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost Learning Curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting Learning Curves\",X_train,y_train,cv=kfold)","49339fd8":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\nnames_classifiers = [(\"AdaBoosting\", ada_best),(\"ExtraTrees\",ExtC_best),(\"RandomForest\",RFC_best),(\"GradientBoosting\",GBC_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" Feature Importance\")\n        nclassifier += 1","c003307b":"voting_clas = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nvoting_clas = voting_clas.fit(X_train, y_train)","2b1c1843":"test_Survived = pd.Series(voting_clas.predict(df_test), name=\"Survived\")\n\nresults = pd.concat([IDtest,test_Survived],axis=1)\n\nresults.to_csv(\"Ensemble_Python_Voting_Results.csv\",index=False)","1d889727":"> It seems that skewness is reduced after the log transformation","125cc4b0":"### Joining Train and Test Data Sets","9b2ae1cb":"> It seems that the four classifiers have different top features according to the relative importance. \n\n> Nevertheless, they have some common important features for the classification, such as 'Fare', 'Title_2', 'Age' and 'Sex'.\n\n> **According to the feature importance, we can note that the prediction of the survival seems to be more associated with the Age, the Sex, the family size and the social standing of the passengers more than the location in the boat.**","dd74713d":"> We notice that age distributions are different in the survived and not survived subpopulations. \n\n> There is a peak corresponding to the young passengers, that have survived. We also see that passengers more than 60 of the age have less survived.","d1af2574":"## Categorical Features","01f715ee":"> Let's perform a grid search optimization for AdaBoost, ExtraTrees , RandomForest, GradientBoosting and SVC classifiers.","1abbef1f":"# Feature Engineering","d538e880":"# Modeling","36773d5b":"> It shows that small and medium families have more chance to survive than single passengers and large families.","24e89147":"> We can see that the family size has an important role, survival probability is worst for large families.\n\n> Let's create 4 categories based on family size.","9fa6efbc":"> I will prefer a voting classifier to combine the predictions coming from the 5 classifiers.","dd1985b2":"### Sex","2a908129":"> It seems that age is not correlated with Sex, but it is negatively correlated with Pclass, Parch and SibSp.\n\n> Let's fill Age with the median age of similar rows according to Class, Patch and SibSp.","a8a21fc7":"## Feature Importance of Tree Based Classifiers","1fd8a025":"### SibSP vs Survived","d7960117":"> Ticket class is a significant feature for prediction of survival. First class passengers have more chance to survive than second class and third class passengers. \n\n> This trend is conserved when we look at both male and female passengers.","afbba41c":"# Dealing with Missing Values","345a94e6":"## Cross Validate Models","ed62d44a":"### Correlation Matrix for Numerical Features","53c6b71b":"> It seems that small families have more chance to survive, more than single, medium and large families.","20e33123":"> The first letter of the cabin represents the desk. That's why I just prefer to keep this information, since it indicates the probable location of the passenger in the Titanic.","32d027b0":"> There are many missing values in Age and Cabin features. The reason of survived missing values is the joining testing dataset into training data.","fb56a72d":"## Cabin","75228c91":"> It is obvious that female have more chance to survive than male. That's why sex plays big role in the prediction of the survival.","da2c82ce":"## Family Size","4d098edb":"## Title ","f505e592":"## Numerical Features","3921070f":"> Age distribution seems to be the similar in Male and Female subpopulations that's why Sex is not informative to predict Age.\n\n> It seems that 1rst class passengers are older than 2nd class passengers who are also older than 3rd class passengers.","420a6914":"> Tickets with same prefixes may have a similar class and survival.\n\n> Let's replace the Ticket feature column by the ticket prefixe  and see which may be more informative!","5f3bc9a6":"> Let's predict and submit results.","b0749320":"### Check for Null and Missing Values","03160d21":"## Ticket","3bbeaba4":"# Import and Check Data","16f722a4":"### Age vs Survived","d2d300aa":"# Prediction","b93096d9":"> Let's compared 10 popular classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure.","f6f7c7b2":"> There is no significant difference between median value of age in survive and not survived subpopulation.\n\n> However, we still notice that young passengers have higher survival rate in the violin plot of the survived passengers.","46c87743":"> It seems that there is no imbalanced data problem.","123acdff":"> The Name column includes information of the passenger's title. ","86ca3e84":"> It seems that having more sub sibling or parent reduces survival rate. According to the graph, Single passengers or with two other persons have more chance to survive.","68e2afb3":"> As we can see in the graph, Fare distribution is very skewed. \n\n> In this case, it could be better to transform it with the log function to reduce this skew.","d2ce46b1":"### Parch vs Survived","48dfca8e":"# EDA","b2f09984":"## Hyperparameter Tunning for Best Models","a5b978ff":"### Embarked","e3038c48":"> Passengers coming from Cherbourg (C) have more chance to survive than others.","987c135b":"> Let's check learning curves to see the overfitting effect on the training set and the effect of the training size on the accuracy.","d3954aa2":"> It seems that there is no significance correlation between Survived and the other numerical features according to the matrix. To determine this, let's explore in detail these features.","b06983c7":"> Age column contains 263 missing values in the whole dataset.\n\n> It looks that age is an important feature to survive (for example children). Let's keep the age feature and to impute the missing values.","60da3dd0":"### Age","36ab8565":"## Ensemble Modeling","cd441036":"> It seems that GradientBoosting and Adaboost classifiers tend to overfit the training set.\n\n> The training and cross-validation curves are close together that's why SVC and ExtraTrees classifiers seem to better generalize the prediction since.","17a8ca17":"> We can see that passengers with a cabin have generally more chance to survive than passengers without (X).","5988e191":"### Fare","7a561a86":"> There are 17 unique titles in the dataset, most of them are very rare and we can group them as below.","7dd3d211":"> We can notice that women and children have priority.","1a2ea806":"### Pclass"}}