{"cell_type":{"98de7d1d":"code","9f8dc71b":"code","4fd558ff":"code","db60bbfe":"code","c319c916":"code","bfc71334":"code","6edb55fb":"code","06f8a6cb":"code","253881ef":"code","9c3fcdb1":"code","67b88d88":"code","9deedfdf":"code","cb6bef2e":"code","a0684f21":"code","442a52ec":"code","37bc59e1":"code","791ef317":"markdown","b09e81e6":"markdown","00c3ed2d":"markdown","bbcfa1c7":"markdown","5949ad1e":"markdown","c09708f2":"markdown"},"source":{"98de7d1d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9f8dc71b":"df = pd.read_csv('\/kaggle\/input\/etp-outcomebased-covid\/Test_Covid.csv')","4fd558ff":"df.head()","db60bbfe":"print(\"Shape:\", df.shape)","c319c916":"print(f\"Null values present: {any(df.isnull().sum())}\")","bfc71334":"df.drop('Unnamed: 0', inplace=True, axis=1)","6edb55fb":"df.describe().T","06f8a6cb":"# setting up X and y for training\ny = df.Y\nX = df.drop('Y', axis = 1)","253881ef":"from sklearn.linear_model import LogisticRegression","9c3fcdb1":"# Normalize X to speed up convergence\nX \/= X.max()","67b88d88":"log_res = clf = LogisticRegression(penalty='l1',\n                                   solver='liblinear',\n                                   tol=1e-6,\n                                   max_iter=int(1e6),\n                                   warm_start=True,\n                                   intercept_scaling=100,\n                                   C=5,\n                                   fit_intercept=True)","9deedfdf":"log_res.fit(X,y)","cb6bef2e":"from sklearn.feature_selection import SelectFromModel\n\nl1_sel_features = SelectFromModel(log_res, prefit=True)\nfor features in X.columns[l1_sel_features.get_support()]:\n    print(features)","a0684f21":"X_sel = X[X.columns[l1_sel_features.get_support()]]","442a52ec":"def build_model(X, y):\n\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report, confusion_matrix\n\n    print(\"------------------ Unregularised Model -----------------\")\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25)\n    lr = LogisticRegression()\n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_test)\n    print(classification_report(y_test, y_pred))\n    \n    print(\"------------------- Regularised Model -------------------\")\n    lr = LogisticRegression()\n    X_sel = X_train[X_train.columns[l1_sel_features.get_support()]]\n    lr.fit(X_sel, y_train)\n    y_pred = lr.predict(X_test[X_test.columns[l1_sel_features.get_support()]])\n    print(classification_report(y_test, y_pred))","37bc59e1":"build_model(X, y)","791ef317":"The goal of this notebook is to select the relevant features (feature selection) for the classification problem consisting of 98 features using regularisation techniques.","b09e81e6":"Our regularised model has provided the above features as important or relevant features for the dataset","00c3ed2d":"##### Building our model based on different parameters\n\n1. **penalty**: We will prefer Lasso (l1) since there are a lot of features and we need to perform feature selection.\n2. **solver**: For small datasets (20 samples in our case), \u2018liblinear\u2019 is a good choice.\n3. **tol**: Tolerance for stopping criteria. default 1e-4\n4. **max_ter**: Maximum number of iterations taken for the solvers to converge. default 100\n5. **warm_state**: When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. default=False\n6. **intercept_scaling**: Useful only when the solver \u2018liblinear\u2019 is used and self.fit_intercept is set to True. default=1","bbcfa1c7":"##### Exploring Data","5949ad1e":"**Summary**\n1. The dataset does not contain any null values.\n2. We have dropped the Unnamed: 0 column since it is a categorical variable representing row nos and of no use to us.\n3. There are 20 samples and 96 features.","c09708f2":"##### Get the important features from the Lasso Regularization\n**Note:** The no. of important features will be minimum(m,n) where 'm' is the number of samples and 'n is the number of features'. Therefore, for our case the number of important features will be equal to or less than 20."}}