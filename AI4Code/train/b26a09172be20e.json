{"cell_type":{"38b7a1e3":"code","78fd099e":"code","4f25e5dc":"code","d62d7d71":"code","4d8e17b1":"code","d5d574c3":"code","3b48f66b":"code","67444bc1":"code","44f89289":"code","2b9f6a61":"code","9e1be9dd":"markdown","77c8b387":"markdown","8dd7b3c8":"markdown","d26bb022":"markdown","937dbff1":"markdown","e7b82bc5":"markdown","fd092d33":"markdown","4917daa2":"markdown","c6f48fbb":"markdown","3f335b3f":"markdown"},"source":{"38b7a1e3":"from transformers import set_seed, GPT2Config, GPT2Tokenizer, GPT2ForSequenceClassification\n\nset_seed(731) # My Birthday!, you should get train_loss: 0.773, train_acc: 0.567 in epoch 0.\n\nmodel_config = GPT2Config.from_pretrained('gpt2', num_labels=2) # Binary Classification\nmodel = GPT2ForSequenceClassification.from_pretrained('gpt2', config=model_config)\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.padding_side = \"left\" # Very Important\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.config.pad_token_id = model.config.eos_token_id","78fd099e":"import os\nimport pandas as pd\nfrom torch.utils.data import Dataset\n\nclass TweetDataset(Dataset):\n    def __init__(self, train=True):\n        super().__init__()\n        self.train = train\n        self.data = pd.read_csv(os.path.join('\/kaggle\/input\/nlp-getting-started\/', 'train.csv' if train else 'test.csv'))\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        record = self.data.iloc[index]\n        text = record['text']\n        if self.train:\n            return {'text': text, 'label': record['target']}\n        else:\n            return {'text': text, 'label': '0'}\n\ntrain_dataset = TweetDataset(train=True)\ntest_dataset = TweetDataset(train=False)","4f25e5dc":"for i in range(10):\n    print(train_dataset.__getitem__(i)['text'])","d62d7d71":"class Gpt2ClassificationCollator(object):\n    def __init__(self, tokenizer, max_seq_len=None):\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n        \n        return\n    \n    def __call__(self, sequences):\n        texts = [sequence['text'] for sequence in sequences]\n        labels = [int(sequence['label']) for sequence in sequences]\n        inputs = self.tokenizer(text=texts,\n                                return_tensors='pt',\n                                padding=True,\n                                truncation=True,\n                                max_length=self.max_seq_len)\n        inputs.update({'labels': torch.tensor(labels)})\n        \n        return inputs\n\ngpt2classificationcollator = Gpt2ClassificationCollator(tokenizer=tokenizer,\n                                                        max_seq_len=60)","4d8e17b1":"from torch.utils.data import DataLoader, random_split\n\ntrain_size = int(len(train_dataset) * 0.8)\nval_size = len(train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\ntrain_dataloader = DataLoader(dataset=train_dataset,\n                              batch_size=32,\n                              shuffle=True,\n                              collate_fn=gpt2classificationcollator)\nval_dataloader = DataLoader(dataset=val_dataset,\n                            batch_size=32,\n                            shuffle=False,\n                            collate_fn=gpt2classificationcollator)\ntest_dataloader = DataLoader(dataset=test_dataset,\n                             batch_size=32,\n                             shuffle=False,\n                             collate_fn=gpt2classificationcollator)","d5d574c3":"from transformers import AdamW, get_cosine_schedule_with_warmup\n\ntotal_epochs = 10\n\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]\noptimizer = AdamW(optimizer_grouped_parameters,\n                  lr=1e-5,\n                  eps=1e-8)\n\nnum_train_steps = len(train_dataloader) * total_epochs\nnum_warmup_steps = int(num_train_steps * 0.1) \n\nlr_scheduler = get_cosine_schedule_with_warmup(optimizer,\n                                               num_warmup_steps=num_warmup_steps,\n                                               num_training_steps = num_train_steps)","3b48f66b":"import torch\n\ndef train(dataloader, optimizer, scheduler, device_):\n    global model\n    model.train()\n    \n    prediction_labels = []\n    true_labels = []\n    \n    total_loss = []\n    \n    for batch in dataloader:\n        true_labels += batch['labels'].numpy().flatten().tolist()\n        batch = {k:v.type(torch.long).to(device_) for k, v in batch.items()}\n        \n        \n        outputs = model(**batch)\n        loss, logits = outputs[:2]\n        logits = logits.detach().cpu().numpy()\n        total_loss.append(loss.item())\n        \n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # prevent exploding gradient\n\n        optimizer.step()\n        scheduler.step()\n        \n        prediction_labels += logits.argmax(axis=-1).flatten().tolist()\n    \n    return true_labels, prediction_labels, total_loss\n\ndef validation(dataloader, device_):\n    global model\n    model.eval()\n    \n    prediction_labels = []\n    true_labels = []\n    \n    total_loss = []\n    \n    for batch in dataloader:\n        true_labels += batch['labels'].numpy().flatten().tolist()\n        batch = {k:v.type(torch.long).to(device_) for k, v in batch.items()}\n        \n        with torch.no_grad():\n            outputs = model(**batch)\n            loss, logits = outputs[:2]\n            logits = logits.detach().cpu().numpy()\n            total_loss.append(loss.item())\n\n            prediction_labels += logits.argmax(axis=-1).flatten().tolist()\n        \n    return true_labels, prediction_labels, total_loss","67444bc1":"from sklearn.metrics import classification_report, accuracy_score\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)\n\nall_loss = {'train_loss': [], 'val_loss': []}\nall_acc = {'train_acc': [], 'val_acc': []}\n\nfor epoch in range(total_epochs):\n    y, y_pred, train_loss = train(train_dataloader, optimizer, lr_scheduler, device)\n    train_acc = accuracy_score(y, y_pred)\n    \n    y, y_pred, val_loss = validation(val_dataloader, device)\n    val_acc = accuracy_score(y, y_pred)\n    \n    all_loss['train_loss'] += train_loss\n    all_loss['val_loss'] += val_loss\n    \n    all_acc['train_acc'].append(train_acc)\n    all_acc['val_acc'].append(val_acc)\n    \n    print(f'Epoch: {epoch}, train_loss: {torch.tensor(train_loss).mean():.3f}, train_acc: {train_acc:.3f}, val_loss: {torch.tensor(val_loss).mean():.3f}, val_acc: {val_acc:.3f}') ","44f89289":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfig = plt.figure(figsize=(20,20))\na = fig.add_subplot(4, 1, 1)\nb = fig.add_subplot(4, 1, 2)\nc = fig.add_subplot(2, 1, 2)\na.plot(all_loss['train_loss'])\nb.plot(all_loss['val_loss'])\nc.plot(all_acc['train_acc'])\nc.plot(all_acc['val_acc'])\nc.set(xlabel='epoch', ylabel='accuracy')\nc.legend(['train', 'val'])\n\npass","2b9f6a61":"_, y_pred, _ = validation(test_dataloader, device)\n\nsubmit = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nsubmit['target'] = y_pred\n\nsubmit.to_csv('submission.csv', index=False)","9e1be9dd":"### 1. Model and Tokenizer\n\nIn \ud83e\udd17, they prepared GPT2 model for classification in advance. Very Thankful! <br>\nHere's link: https:\/\/huggingface.co\/transformers\/model_doc\/gpt2.html#transformers.GPT2ForSequenceClassification","77c8b387":"### 2. Build Dataset","8dd7b3c8":"I used GPT-2 model in HuggingFace Library. <br>\nI refered to following links, [\ud83c\udfbbFine-tune Transformers in PyTorch using \ud83e\udd17 Transformers](https:\/\/gmihaila.medium.com\/fine-tune-transformers-in-pytorch-using-transformers-57b40450635).","d26bb022":"### 4. DataLoader","937dbff1":"### 6. Train & Validation","e7b82bc5":"### 8. Run on Test Data","fd092d33":"### 3. Data Collator","4917daa2":"### 7.1. Check Loss with Graph","c6f48fbb":"### 7. Run!","3f335b3f":"### 5. Optimizer & Lr Scheduler"}}