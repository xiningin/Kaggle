{"cell_type":{"a38a4ae7":"code","0a901923":"code","d522c329":"code","3bd755ef":"code","195c3662":"code","b424f630":"code","3e5df828":"code","86b52467":"code","0d3228dc":"code","804e6dbc":"code","c1b7db38":"code","3cafa5b1":"code","2ed669b3":"code","86d33278":"code","fe65a1c6":"code","ba356b28":"code","11a704b3":"code","5f311feb":"code","5d7e300e":"code","eb5f46b6":"code","d8dca37c":"code","fa2af57c":"code","83efca7a":"code","b0a340ed":"code","20c92e1c":"code","210e216c":"code","9b1f35da":"code","0e9a84be":"code","d6c2444c":"markdown","8d05c8a2":"markdown","3dbcc482":"markdown","40aabb74":"markdown","86e27eeb":"markdown","d1f67d27":"markdown","da7c322f":"markdown","64ebda48":"markdown","2faf49fa":"markdown","68ef4f09":"markdown","fb160d40":"markdown","ae3bd451":"markdown","ef527bbb":"markdown","a278ae6b":"markdown","21040f27":"markdown"},"source":{"a38a4ae7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0a901923":"#Read Data\ndf = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","d522c329":"#First 5 rows of data\ndf.head()","3bd755ef":"#Dimension of data\ndf.shape","195c3662":"#Getting the data types of the variable\ndf.info()","b424f630":"#statistical properties of dataset\ndf.describe()","3e5df828":"#columns present in dataset\ndf.columns","86b52467":"#view the unique value in target variable\ndf['target'].unique()","0d3228dc":"#frequency distribution of target variable\ndf['target'].value_counts()","804e6dbc":"#graphical representation of target variable\nf, ax = plt.subplots(figsize=(6, 4))\nax = sns.countplot(x=\"target\", data=df)\nplt.show()","c1b7db38":"profile = pandas_profiling.ProfileReport(df)\nprofile","3cafa5b1":"#Correlation matrix to see how features are correlated with target\nplt.rcParams['figure.figsize'] = (20, 15)\nplt.style.use('ggplot')\n\ncorrmat = df.corr()\nsns.heatmap(corrmat, cmap = 'Wistia', annot=True)\nplt.show()\n","2ed669b3":"Num=corrmat['target'].sort_values(ascending=False).head(20).to_frame()\n\nNum","86d33278":"#We can visualize the value counts of the cp variable wrt target as follows -\nf, ax = plt.subplots(figsize=(8, 6))\nax = sns.countplot(x=\"cp\", hue=\"target\", data=df)\nplt.show()","fe65a1c6":"#We can visualize the value counts of the thalach variable wrt target as follows -\nf, ax = plt.subplots(figsize=(8, 6))\nsns.stripplot(x=\"target\", y=\"thalach\", data=df)\nplt.show()","ba356b28":"df.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']\n\ndf.columns","11a704b3":"df['sex'][df['sex'] == 0] = 'female'\ndf['sex'][df['sex'] == 1] = 'male'\n\ndf['chest_pain_type'][df['chest_pain_type'] == 1] = 'typical angina'\ndf['chest_pain_type'][df['chest_pain_type'] == 2] = 'atypical angina'\ndf['chest_pain_type'][df['chest_pain_type'] == 3] = 'non-anginal pain'\ndf['chest_pain_type'][df['chest_pain_type'] == 4] = 'asymptomatic'\n\ndf['fasting_blood_sugar'][df['fasting_blood_sugar'] == 0] = 'lower than 120mg\/ml'\ndf['fasting_blood_sugar'][df['fasting_blood_sugar'] == 1] = 'greater than 120mg\/ml'\n\ndf['rest_ecg'][df['rest_ecg'] == 0] = 'normal'\ndf['rest_ecg'][df['rest_ecg'] == 1] = 'ST-T wave abnormality'\ndf['rest_ecg'][df['rest_ecg'] == 2] = 'left ventricular hypertrophy'\n\ndf['exercise_induced_angina'][df['exercise_induced_angina'] == 0] = 'no'\ndf['exercise_induced_angina'][df['exercise_induced_angina'] == 1] = 'yes'\n\ndf['st_slope'][df['st_slope'] == 1] = 'upsloping'\ndf['st_slope'][df['st_slope'] == 2] = 'flat'\ndf['st_slope'][df['st_slope'] == 3] = 'downsloping'\n\ndf['thalassemia'][df['thalassemia'] == 1] = 'normal'\ndf['thalassemia'][df['thalassemia'] == 2] = 'fixed defect'\ndf['thalassemia'][df['thalassemia'] == 3] = 'reversable defect'","5f311feb":"df['sex'] = df['sex'].astype('object')\ndf['chest_pain_type'] = df['chest_pain_type'].astype('object')\ndf['fasting_blood_sugar'] = df['fasting_blood_sugar'].astype('object')\ndf['rest_ecg'] = df['rest_ecg'].astype('object')\ndf['exercise_induced_angina'] = df['exercise_induced_angina'].astype('object')\ndf['st_slope'] = df['st_slope'].astype('object')\ndf['thalassemia'] = df['thalassemia'].astype('object')","5d7e300e":"df = pd.get_dummies(df, drop_first=True)","eb5f46b6":"# splitting the dependent and independent variables from the data\n\nx = df.drop('target', axis=1)\ny = df.target\n\n# checking the shapes of x and y\nprint(\"Shape of x:\", x.shape)\nprint(\"Shape of y:\", y.shape)","d8dca37c":"# splitting the sets into training and test sets\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\n# getting the shapes\nprint(\"Shape of x_train :\", x_train.shape)\nprint(\"Shape of x_test :\", x_test.shape)\nprint(\"Shape of y_train :\", y_train.shape)\nprint(\"Shape of y_test :\", y_test.shape)","fa2af57c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score ","83efca7a":"model1 = LogisticRegression()\nmodel1.fit(x_train, y_train)\nmodel1.score(x_train, y_train)","b0a340ed":"model2 = RandomForestClassifier(n_estimators = 50, max_depth = 5)\nmodel2.fit(x_train, y_train)\nmodel2.score(x_train, y_train)","20c92e1c":"model3 = xgb.XGBClassifier(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213,\n                             random_state =7, nthread = -1)\nmodel3.fit(x_train, y_train)\nmodel3.score(x_train, y_train)","210e216c":"from sklearn.ensemble import VotingClassifier \nestimator = [] \nestimator.append(('LR',  \n                  LogisticRegression(solver ='lbfgs',  \n                                     multi_class ='multinomial',  \n                                     max_iter = 200))) \nestimator.append(('RFC', RandomForestClassifier())) \nestimator.append(('XGB', XGBClassifier())) ","9b1f35da":"# Voting Classifier with soft voting \nvot_soft = VotingClassifier(estimators = estimator, voting ='soft') \nvot_soft.fit(x_train, y_train) \ny_pred = vot_soft.predict(x_test) \ny_pred","0e9a84be":"# using accuracy_score \nscore = accuracy_score(y_test, y_pred) \nprint(\"Soft Voting Score % d\" % score) ","d6c2444c":"# Model Building","8d05c8a2":"1. Logistic Regression","3dbcc482":"# Bivariate Analysis","40aabb74":"# Data Profiling","86e27eeb":"3. XgBoost","d1f67d27":"Dummy Variable Creation","da7c322f":"# Voting","64ebda48":"**Interpretation of correlation coefficient**\n\n* The correlation coefficient ranges from -1 to +1.\n\n* When it is close to +1, this signifies that there is a strong positive correlation. So, we can see that there is no variable which has strong positive correlation with target variable.\n\n* When it is close to -1, it means that there is a strong negative correlation. So, we can see that there is no variable which has strong negative correlation with target variable.\n\n* When it is close to 0, it means that there is no correlation. So, there is no correlation between target and fbs.\n\nWe can see that the cp and thalach variables are mildly positively correlated with target variable.\nAnd exang, oldpeak, ca, thal are negatively correlated. So, I will analyze the interaction between these positively correlated features and target variable.\n","2faf49fa":"# Data Exploration","68ef4f09":"From Data profiling, we found that there is no missing value in the data. So we don't have to do missing value treatment.","fb160d40":"let's change the names of the  columns for better understanding","ae3bd451":"2. Random Classifier","ef527bbb":"# Importing Library","a278ae6b":"# Analyse the target variable (Univariate Analysis)","21040f27":"# Splitting the Data"}}