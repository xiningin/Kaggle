{"cell_type":{"8db22ebe":"code","f10e1464":"code","885b4408":"code","97531db9":"code","debea860":"code","3cc3e8c3":"code","52e0c9f5":"code","dcfb523f":"code","332d4934":"code","db95e6b7":"code","4beace55":"code","9f6cd797":"code","2b0487a4":"code","6d11221b":"code","5a065da5":"code","af3dad74":"code","d1d2a7d5":"code","2272351e":"code","64b4e2c1":"code","34d33f4d":"code","17de4d6c":"code","774a63da":"code","fbaf0cc2":"code","d80efc0d":"code","4bb3d475":"code","6810a3ba":"code","d3531b94":"code","59634961":"code","f0f4c347":"code","be3aa2eb":"code","06e1ba19":"code","433cdecc":"code","96e73713":"code","f3a37486":"code","8037ef7e":"code","e7f996d5":"code","7cc4f9a2":"code","a8872123":"code","8d64c30e":"code","76746b75":"code","870828fa":"code","4a18055e":"code","325b5217":"code","1d19a4f5":"code","15dee6f1":"code","42a56070":"code","f135e760":"code","34ab8004":"code","931f4be9":"code","6ec5af99":"code","00367c87":"code","3774bedb":"code","cd7d4b6a":"code","70c4d52b":"code","b9d56a26":"code","449302fc":"code","9782ba7f":"code","908c4097":"code","2319fb45":"code","c423e7be":"code","b02491f1":"code","8786aca0":"code","be86b153":"code","20aec36b":"code","a963e25b":"code","5601201f":"code","e77a4de6":"code","c3c5099f":"code","5c00dc9d":"code","198de664":"code","eb7e1faa":"code","54342bae":"code","02b24b1a":"code","cebee1f1":"code","c1fd60c9":"code","df192e8d":"code","f9889db4":"code","ddaf575d":"code","b5419b6b":"code","324b2f0e":"code","5f9cdd8b":"code","49bb2d4d":"code","ffe3ab79":"code","490a0f18":"code","f551ef1d":"code","09684c2a":"code","dfea5f73":"code","76f3e0ca":"code","63e93608":"code","1a2b27ae":"code","4f5e90d3":"code","88eba21c":"code","acd5a282":"code","8f57413c":"code","16961bcd":"code","f4ebf5dd":"code","a26d6da8":"code","f0b07b72":"code","dbdd0bde":"code","7a0faa17":"code","4efae71e":"code","9eebc695":"code","d6161547":"code","a1ff6a86":"code","ffadf447":"code","3ff8f747":"code","9d511ab7":"markdown","e626cd72":"markdown","0db7de28":"markdown","c462ecc8":"markdown","35a41ab3":"markdown","6d966475":"markdown","402d9a41":"markdown","119416f2":"markdown","08defe68":"markdown","34fc4eee":"markdown","8e0cc85c":"markdown","d06ca1da":"markdown","203b4610":"markdown","4afd460a":"markdown","a7eebbe0":"markdown","2b63d2b9":"markdown","ae97e913":"markdown","94791bef":"markdown","d4172bfd":"markdown","dbd054af":"markdown","2a679151":"markdown","0fe823e9":"markdown","bed4888c":"markdown","d6c53304":"markdown","68df7707":"markdown","c43466c7":"markdown","4775e2fe":"markdown","52111429":"markdown","8ec8ae9b":"markdown","32fa0943":"markdown","5c355f34":"markdown","490e9cfc":"markdown","4339fd5c":"markdown","a9258ec5":"markdown","a4784d2c":"markdown","48d0f8d1":"markdown","e7cabeb2":"markdown","7abb227c":"markdown","0d17a78d":"markdown","8186c996":"markdown","07125d25":"markdown","b14c7139":"markdown","933761a9":"markdown","fdbadc68":"markdown","40468671":"markdown"},"source":{"8db22ebe":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","f10e1464":"df = pd.read_excel('..\/input\/covid19\/dataset.xlsx')","885b4408":"df.head()","97531db9":"df.describe()","debea860":"# All the features\n[col for col in df.columns]","3cc3e8c3":"all_null = [feature for feature in df.columns if df[feature].isnull().sum() == len(df)] ","52e0c9f5":"# all these features are completelly null, so they are not important for the analysis\nall_null.append('Patient ID')","dcfb523f":"print(all_null)","332d4934":"df.drop(all_null, inplace=True, axis=1)","db95e6b7":"# all the not numerical features\n[feature for feature in df.columns if not np.issubdtype(df[feature].dtype, np.number) if feature not in 'SARS-Cov-2 exam result']","4beace55":"# saves the non numeric features\nnot_numeric = [feature for feature in df.columns if not np.issubdtype(df[feature].dtype, np.number) if feature not in 'SARS-Cov-2 exam result']","9f6cd797":"# transforms some possible string features into numeric, if the string is in numerical form\ndf.update(df.apply(pd.to_numeric, errors='coerce'))","2b0487a4":"# shows the unique values of all the features, which are not numbers\ndf_subset = df.select_dtypes(exclude=[np.number]).copy()\n[print(f'{col}: {df_subset[col].value_counts().index}') for col in df_subset.columns]","6d11221b":"# turn nao realizado into NaN\ndf['Urine - pH'][df['Urine - pH'] == 'N\u00e3o Realizado'] = np.nan\ndf['Urine - pH'].value_counts()","5a065da5":"# transforms the string into zero\ndf['Urine - Leukocytes'][df['Urine - Leukocytes'] == '<1000'] = 0\ndf['Urine - Leukocytes'].value_counts()","af3dad74":" # defines the largest Leukocytes values to be 300.000\ndf['Urine - Leukocytes'][df['Urine - Leukocytes'] > 300000] = 300000","d1d2a7d5":"sns.distplot(df['Urine - Leukocytes'][df['Urine - Leukocytes'] < 500000].apply(int))","2272351e":"# separates the Leukocytes into categories accordingly to the distribution of it\ndf['Urine - Leukocytes'] = pd.qcut(df['Urine - Leukocytes'], q = [0.1, 0.2, 0.3, 0.4, 0.75, 1])","64b4e2c1":"# creating dummy\n#not_numeric = [feature for feature in df.columns if not np.issubdtype(df[feature].dtype, np.number) if feature not in 'SARS-Cov-2 exam result']\nis_string = [col for col in df_subset.columns if col not in 'SARS-Cov-2 exam result']\nfor feature in is_string:\n    df[[(str(feature) + '_' + str(col)) for col in pd.get_dummies(df[feature]).columns]] = pd.get_dummies(df[feature])\n    df.drop(feature,axis=1,inplace=True)","34d33f4d":"df.head()","17de4d6c":"# shows the feature title and it's type\n[print(f'{col} {type(col)}') for col in df.columns]","774a63da":"# defines the layout to the plotting\nsns.set_context('paper', font_scale=1.5)\nsns.set_style('whitegrid')","fbaf0cc2":"df['Patient age quantile'].max()","d80efc0d":"df['Patient age quantile'].min()","4bb3d475":"# plot about the age distribution for someone with Cov-2 and without it\nfig = plt.figure(figsize=(10,7))\nfig = sns.distplot(df['Patient age quantile'][df['SARS-Cov-2 exam result']=='negative'], kde=True, hist=False, color='blue', label='negative')\nfig = sns.distplot(df['Patient age quantile'][df['SARS-Cov-2 exam result']=='positive'], kde=True, hist=False, color='red', label='positive')\nfig = plt.xticks(np.arange(0, 20, step=1))\nfig = plt.xlim(df['Patient age quantile'].min(),df['Patient age quantile'].max())\nfig = plt.legend(loc='best')\n\n","6810a3ba":"fig = plt.figure(figsize=(12,7))\nfig = sns.countplot(x='Patient age quantile',data=df[df['Patient age quantile']<=4], palette='viridis',\n             hue='SARS-Cov-2 exam result')\nfor p in fig.patches:\n    fig.annotate('{:.0f}'.format(round(p.get_height())), (p.get_x()+0.1, p.get_height()+0.2), fontsize=20)\nfig = plt.ylim(0,420)","d3531b94":"# gets the numerical features\nnum_features = [feature for feature in df.columns if np.issubdtype(df[feature].dtype, np.number)]","59634961":"print(f'length numerical features: {len(num_features)}')\nprint(f'length total features: {len(df.columns)}')","f0f4c347":"def plot_pie(data, feature):\n    plt.pie(data[feature].value_counts(), autopct='%1.1f%%',startangle=90, colors=['#aaffd5', '#fec0cb'],\n        labels=data[feature].value_counts().index)\n    plt.legend()\n    print(df[feature].value_counts())","be3aa2eb":"plot_pie(df,'Patient addmited to intensive care unit (1=yes, 0=no)' )","06e1ba19":"plot_pie(df,'Patient addmited to semi-intensive unit (1=yes, 0=no)' )","433cdecc":"plot_pie(df,'Patient addmited to regular ward (1=yes, 0=no)')","96e73713":"patient_cols = [col for col in df.columns if 'Patient addmited' in col]","f3a37486":"df['Medical_care_needed'] = (df[patient_cols]==1).any(axis=1).astype(int)","8037ef7e":"df['Medical_care_needed'].sum()","e7f996d5":"plot_pie(df, 'Medical_care_needed')","7cc4f9a2":"fig = plt.figure(figsize=(5,8))\nfig = sns.countplot(x='Medical_care_needed',data=df[df['SARS-Cov-2 exam result']=='positive'], palette='viridis')\nfor p in fig.patches:\n    fig.annotate('{:.1f}%'.format(round(p.get_height()\/len(df[df['SARS-Cov-2 exam result']=='positive'])*100)), (p.get_x()+0.2, p.get_height()+0.2), fontsize=20)\n#plt.ylim(0,420)","a8872123":"# feature engineering about the people, which are going to need semi-intensive and intensive care\ndf['Medical_semi_int'] = (df[patient_cols[1:]]==1).any(axis=1).astype(int)","8d64c30e":"df['Medical_semi_int'].sum()","76746b75":"plot_pie(df, 'Medical_semi_int')","870828fa":"# plot about the missing values from the dataset\nfig, ax = plt.subplots(nrows=4, ncols=1)\nfig.set_figheight(40)\nfig.set_figwidth(15)\nfor i in range(len(patient_cols)):\n    ax[i].set_title(patient_cols[i])\n    sns.heatmap(data=df[df[patient_cols[i]]==1].isna(), cmap='viridis', annot=False, cbar=False, ax=ax[i], \n               xticklabels=True, yticklabels=False)\nax[i+1].set_title('Whole dataset')\nsns.heatmap(data=df.isna(), cmap='viridis', annot=False, cbar=False, ax=ax[i+1], \n               xticklabels=True, yticklabels=False)","4a18055e":"def get_corr(data,features,base_feature):\n    '''\n    gets the correlation between the two input features\n    features => features from which the correlation is going to be calculated\n    base_feature => the base feature to be correlated\n    data => the data from there the calculations are going to be done\n    '''\n    data_to = pd.Series()\n    for feature in features:\n        try:\n            data_to[feature] = data[[base_feature,feature]].dropna().corr().iloc[1,0]\n        except:\n            continue\n    return data_to.dropna().sort_values(ascending=False)\n    ","325b5217":"med_care = get_corr(df,num_features,'Medical_care_needed')\nmed_reg = get_corr(df,num_features,patient_cols[0])\nmed_semi = get_corr(df,num_features,patient_cols[1])\nmed_int = get_corr(df,num_features,patient_cols[2])","1d19a4f5":"def get_tops(top, med_reg, med_semi, med_int):\n    '''\n    function gets the top correlated features from regular, semi-intensive and intensive care\n    \n    med_reg => correlations to medical care needed\n    med_semi => correlations to semi-intensive care needed\n    med_semi => correlations to intensive care needed\n    '''\n    top_reg_semi_int = [feature for feature in med_care.index if feature in med_reg.index[:top] and feature in med_semi.index[:top] and feature in med_int.index[:top]]\n    top_semi_int = [feature for feature in med_care.index if feature in med_semi.index[:top] and feature in med_int.index[:top]]\n    return top_reg_semi_int, top_semi_int","15dee6f1":"top10_reg_semi_int, top10_semi_int = get_tops(10, med_reg, med_semi, med_int)\ntop30_reg_semi_int, top30_semi_int = get_tops(30, med_reg, med_semi, med_int)\ntop50_reg_semi_int, top50_semi_int = get_tops(50, med_reg, med_semi, med_int)\ntop70_reg_semi_int, top70_semi_int = get_tops(70, med_reg, med_semi, med_int)","42a56070":"def print_tops(top_reg_semi_int):\n    '''\n    shows the top correlations\n    \n    top_reg_semi_int => calculated most important correlations\n    '''\n    for feature in top_reg_semi_int:\n        print(feature)\n        print(f'regular: {med_reg[feature]}')\n        print(f'semi: {med_semi[feature]}')\n        print(f'intensive: {med_int[feature]}\\n')\n    print('#########################\\n')","f135e760":"print_tops(top10_reg_semi_int)\nprint_tops(top30_reg_semi_int)\nprint_tops(top50_reg_semi_int)\nprint_tops(top70_reg_semi_int)","34ab8004":"# check tencences\ndef get_tend_tops(top_reg_semi_int):\n    '''\n    gets the risk tendence of each feature\n    \n    top_reg_semi_int => calculated most important correlations\n    '''\n    tendences = pd.Series()\n    for feature in top_reg_semi_int:\n        if med_reg[feature] < med_semi[feature] and med_semi[feature] < med_int[feature]:\n            tendences[feature] = med_reg[feature], med_semi[feature] ,med_int[feature]\n        elif med_reg[feature] > med_semi[feature] and med_semi[feature] > med_int[feature]:\n            tendences[feature] = med_reg[feature], med_semi[feature] ,med_int[feature]\n    return tendences","931f4be9":"# shows the tendences\ntendences = get_tend_tops(top50_reg_semi_int)\ntendences2 = get_tend_tops(top70_reg_semi_int)\nprint(tendences)\nprint(f'\\nnumber of features: {len(tendences.index)}')\nprint('\\n')\nprint(tendences2)\nprint(f'\\nnumber of features: {len(tendences2.index)}')","6ec5af99":"print(df[tendences.index].isna().sum().sort_values(ascending=False))\nprint(f'\\nlength of df_pos dataset: {len(df)}\\n')\nprint( 1 - df[tendences.index].isna().sum().sort_values(ascending=False) \/ len(df))","00367c87":"print(df[tendences2.index].isna().sum().sort_values(ascending=False))\nprint(f'\\nlength of df_pos dataset: {len(df)}\\n')\nprint( 1 - df[tendences2.index].isna().sum().sort_values(ascending=False) \/ len(df))","3774bedb":"fig = plt.figure(figsize=(20,3))\nsns.heatmap(df[list(tendences.index)+patient_cols].corr().iloc[-3:,:-3], annot=True, cmap='viridis', cbar=False)","cd7d4b6a":"fig = plt.figure(figsize=(25,3))\nsns.heatmap(df[list(tendences2.index)+patient_cols].corr().iloc[-3:,:-3], annot=True, cmap='viridis', cbar=False)","70c4d52b":"list(tendences.index)+patient_cols","b9d56a26":"list(tendences2.index)+patient_cols","449302fc":"# getting the best features from the dataset - 2 \nusable_features2 = [feature for feature in list(tendences2.index) if feature not in patient_cols]\nusable_features2","9782ba7f":"len(df[usable_features2].dropna())","908c4097":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\nscaler = MinMaxScaler()\nscaler2 = StandardScaler()","2319fb45":"# splitting the data\nfrom sklearn.model_selection import train_test_split","c423e7be":"from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,BaggingClassifier\nfrom sklearn.metrics import confusion_matrix,classification_report","b02491f1":"import lightgbm as lgb","8786aca0":"# creating the model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout,BatchNormalization\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras import regularizers","be86b153":"# imputing median values on the missing values\ndf_imp = df.copy()\nneg = df_imp[df_imp['SARS-Cov-2 exam result']=='negative']\nneg.fillna(neg.median(), inplace=True)\npos = df_imp[df_imp['SARS-Cov-2 exam result']=='positive']\npos.fillna(pos.median(), inplace= True)\n\ndf_median = pd.concat([neg,pos])","20aec36b":"from imblearn.over_sampling import ADASYN, SMOTE","a963e25b":"from sklearn import linear_model","5601201f":"def get_nan_features(data):\n    nan = data.isna().sum().sort_values(ascending=False)\n    nan_data = [(nan.index[i], nan.values[i]) for i in range(len(nan)) if nan[i] > 0]\n    nan = df_median.isna().sum().sort_values(ascending=False)\n    l = []\n    if nan[0] > 0:\n        print('these features still have NaN values:')\n        for feature, val in nan_data:\n            print(f'{feature}: {val}')\n            l.append(feature)\n            \n    else:\n        print('no nan values')\n    return l","e77a4de6":"df_median.fillna(df_median[get_nan_features(df_median)].median(), inplace= True)\nget_nan_features(df_median)","c3c5099f":"# checking the existence of missing values\ndf_median.isna().sum().sort_values(ascending=False)[:10]","5c00dc9d":"drop_features = [feature for feature in df_median.columns if 'Medical' in feature or 'Patient addmi' in feature or 'SARS-Cov-2 exam result' in feature]\nprint('Features, which are going to be droped')\ndrop_features","198de664":"X = df_median.drop(drop_features, axis=1)\nX['SARS-Cov-2 exam result'] = pd.get_dummies(df_median['SARS-Cov-2 exam result'],drop_first=True)\ny = df_median['Medical_semi_int']","eb7e1faa":"X, y = SMOTE().fit_resample(X, y)","54342bae":"for feature in X.columns:\n    print(feature)","02b24b1a":"print('Features, which are going to be droped for the training')\nto_drop = [feature for feature in df_median.columns if 'detected' in feature or 'positive' in feature or 'negative' in feature]\nto_drop","cebee1f1":"X.drop(to_drop, axis=1, inplace=True)","c1fd60c9":"# All the features which are going to be used to train\nfeatures = [col for col in X.columns]\nprint(len(features))\nfeatures","df192e8d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\nX_train = scaler2.fit_transform(X_train)\nX_test = scaler2.transform(X_test)\nprint(f'features X_train: {len(X_train[1])}\\nfeatures X_test: {len(X_test[1])}')","f9889db4":"plot_pie(pd.DataFrame(y), pd.DataFrame(y).columns[0])","ddaf575d":"adac = AdaBoostClassifier()\nadac.fit(X_train,y_train)\nadac_pred = adac.predict(X_test)\nprint(classification_report(y_test, adac_pred))","b5419b6b":"lgbc = lgb.LGBMClassifier()\nlgbc.fit(X_train, y_train,)\nlgbc_pred=lgbc.predict(X_test)\nprint(classification_report(y_test, np.round(lgbc_pred)))","324b2f0e":"from xgboost import XGBClassifier, plot_importance\nxgbc = XGBClassifier()\nxgbc.fit(X_train, y_train)\nxgbc_pred=xgbc.predict(X_test)\nprint(classification_report(y_test, np.round(xgbc_pred)))","5f9cdd8b":"def lr_scheduler(epoch, lr):\n    decay_rate = 0.999\n    decay_step = 10\n    if epoch % decay_step == 0 and epoch:\n        return lr * pow(decay_rate, np.floor(epoch \/ decay_step))\n    return lr","49bb2d4d":"import tensorflow.keras.backend as K\n\ndef get_f1(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","ffe3ab79":"def build_model(hp):\n    '''\n    function that creates the model for the random search\n    \n    input:\n    hp - objetc from the HyperParameter class from the kerastuner library\n    \n    output:\n    model - created model with the random hyperparameters for the random search'''\n    \n    model = Sequential()\n\n    model.add(Dense(hp.Int('units_1',min_value=30, max_value=100, sampling='linear', default=75),\n                    activation='relu', kernel_regularizer=regularizers.l2(l=0.1)))\n    model.add(Dropout(rate=hp.Choice('drop_out_1',values=[0.,0.1,0.2,0.3,0.4,0.5], default=0.3)))\n\n    model.add(BatchNormalization())\n    model.add(Dense(hp.Int('units_2',min_value=30, max_value=100, sampling='linear', default=75),\n                    activation='relu', kernel_regularizer=regularizers.l2(l=0.1)))\n    model.add(Dropout(rate=hp.Choice('drop_out_2',values=[0.,0.1,0.2,0.3,0.4,0.5], default=0.3)))\n\n    model.add(BatchNormalization())\n    model.add(Dense(hp.Int('units_3',min_value=20, max_value=50, sampling='linear', default=50),\n                    activation='relu', kernel_regularizer=regularizers.l2(l=0.1)))\n    model.add(Dropout(rate=hp.Choice('drop_out_3',values=[0.,0.1,0.2,0.3], default=0.3)))\n\n    model.add(BatchNormalization())\n    model.add(Dense(hp.Int('units_4',min_value=5, max_value=20, sampling='linear', default=20),\n                    activation='relu', kernel_regularizer=regularizers.l2(l=0.1)))\n    model.add(Dropout(rate=hp.Choice('drop_out_4',values=[0.,0.1,0.2,0.3], default=0.3)))\n\n    model.add(BatchNormalization())\n    model.add(Dense(1, activation=hp.Choice('last_activation',['sigmoid','hard_sigmoid'])))\n\n    adam = optimizers.Adam(learning_rate=hp.Float( 'learning_rate',\n                                                    min_value=1e-6,\n                                                    max_value=1e-1,\n                                                    sampling='LOG',\n                                                    default=1e-3), \n                           beta_1=0.9, beta_2=0.999, amsgrad=True)\n              \n    model.compile(optimizer=adam, loss=hp.Choice('loss_function', ['binary_crossentropy','hinge','squared_hinge']), metrics=[get_f1])\n    return model","490a0f18":"# definition of the early stop parameters\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\nclass_weight = {0: 1., 1: 1.}","f551ef1d":"model = Sequential()\n\nmodel.add(Dense(90,activation='relu', kernel_regularizer=regularizers.l2(l=0.1)))\nmodel.add(Dropout(0.3))\n\nmodel.add(BatchNormalization())\nmodel.add(Dense(70,activation='relu', kernel_regularizer=regularizers.l2(l=0.1)))\nmodel.add(Dropout(0.3))\n\nmodel.add(BatchNormalization())\nmodel.add(Dense(25,activation='relu', kernel_regularizer=regularizers.l2(l=0.1)))\nmodel.add(Dropout(0.1))\n\nmodel.add(BatchNormalization())\nmodel.add(Dense(15,activation='relu', kernel_regularizer=regularizers.l2(l=0.1)))\nmodel.add(Dropout(0.1))\n\nmodel.add(BatchNormalization())\nmodel.add(Dense(1, activation='sigmoid'))\n\nadam = optimizers.Adam(learning_rate=3e-5, \n                       beta_1=0.9, beta_2=0.999, amsgrad=True)\nmodel.compile(optimizer=adam, loss='binary_crossentropy', metrics=[get_f1])","09684c2a":"model.fit(x=np.array(X_train), y=np.array(y_train), validation_data=(np.array(X_test), np.array(y_test)),\n             batch_size = 128, epochs = 500)#, callbacks=[LearningRateScheduler(lr_scheduler, verbose=1)])","dfea5f73":"model_0 = model\n# model_1 = tuner.get_best_models(num_models=5)[0]\n# model_2 = tuner.get_best_models(num_models=5)[1]\n# model_3 = tuner.get_best_models(num_models=5)[2]\n# model_4 = tuner.get_best_models(num_models=5)[3]\n# model_5 = tuner.get_best_models(num_models=5)[4]\nmodels = [model_0]#, model_1, model_2, model_3, model_4, model_5]","76f3e0ca":"print('#############################################################################')\nprint('model_0')\nprint('#############################################################################')\npredictions = model_0.predict_classes(X_test)\nprint(classification_report(y_test, predictions))\nprint('#############################################################################')\n# print('model_1')\n# print('#############################################################################')\n# predictions = model_1.predict_classes(X_test)\n# print(classification_report(y_test, predictions))\n# print('#############################################################################')\n# print('model_2')\n# print('#############################################################################')\n# predictions = model_2.predict_classes(X_test)\n# print(classification_report(y_test, predictions))\n# print('#############################################################################')\n# print('model_3')\n# print('#############################################################################')\n# predictions = model_3.predict_classes(X_test)\n# print(classification_report(y_test, predictions))\n# print('#############################################################################')\n# print('model_4')\n# print('#############################################################################')\n# predictions = model_4.predict_classes(X_test)\n# print(classification_report(y_test, predictions))\n# print('#############################################################################')\n# print('model_5')\n# print('#############################################################################')\n# predictions = model_5.predict_classes(X_test)\n# print(classification_report(y_test, predictions))\n# print('#############################################################################')\n# print('AdaBoost')\n# print('#############################################################################')\n# predictions = adac.predict(X_test)\n# print(classification_report(y_test, predictions))\n# print('#############################################################################')\n# print('LightGBM')\n# print('#############################################################################')\n# predictions = lgbc.predict(X_test)\n# print(classification_report(y_test, predictions))\n# print('#############################################################################')\n","63e93608":"from sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.metrics import average_precision_score, precision_score,average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score","1a2b27ae":"test_data = {}\nfor i in range(len(models)):\n    test_data['model_'+str(i)] = np.array(models[i].predict_classes(X_test))\ntest_data['AdaBoost'] = np.array(adac.predict(X_test))\ntest_data['LightGBM'] = np.array(lgbc.predict(X_test))\ntest_data['XGBoost'] = np.array(xgbc.predict(X_test))","4f5e90d3":"test_data_prob = {}\nfor i in range(len(models)):\n    test_data_prob['model_'+str(i)] = np.array(models[i].predict_proba(X_test))\ntest_data_prob['AdaBoost'] = np.array(adac.predict_proba(X_test)[:,1])\ntest_data_prob['LightGBM'] = np.array(lgbc.predict_proba(X_test)[:,1])\ntest_data_prob['XGBoost'] = np.array(xgbc.predict_proba(X_test)[:,1])","88eba21c":"def get_scores(y_test, y_pred):\n    cache = {}\n    cache['accuracy'] = accuracy_score(y_test, y_pred)\n    cache['precision'] = precision_score(y_test, y_pred)\n    cache['recall'] = recall_score(y_test, y_pred)\n    cache['roc'] = roc_auc_score(y_test, y_pred)\n    cache['f1'] = f1_score(y_test, y_pred)\n    return cache","acd5a282":"def performances(models, y_test):\n    perf_data={}\n    for model,y_pred in models.items():\n        perf_data[model] = get_scores(y_test,y_pred)\n    return pd.DataFrame(perf_data)","8f57413c":"def plot_roc_curve(dict_pred,y_test):\n    f, ax = plt.subplots(figsize=(8, 8))\n    plt.plot([0, 1], [0, 1], '--', color='silver')\n    plt.title('ROC Curve', fontsize=20)\n    plt.xlabel('False Positive Rate', fontsize=20)\n    plt.ylabel('True Positive Rate', fontsize=20)\n    i=0\n    for model, pred in dict_pred.items():\n        i+=1\n        roc_score = roc_auc_score(y_test, pred)\n        fpr, tpr, thresholds = roc_curve(y_test, pred) \n        sns.lineplot(x=fpr, y=tpr, color=sns.color_palette(\"magma\", 10)[-i], \n                     linewidth=2, label= f\"ROC-AUC {model}= {round(roc_score*100,2)}%\", ax=ax)","16961bcd":"def plot_pr_curve(dict_pred,y_test):\n    f, ax = plt.subplots(figsize=(8, 8))\n    plt.title('PR-Curve', fontsize=20)\n    plt.xlabel('Precision', fontsize=20)\n    plt.ylabel('Recall', fontsize=20)\n    i=0\n    for model, pred in dict_pred.items():\n        i+=1\n        average_precision = average_precision_score(y_test, pred)\n        fpr, tpr, thresholds = precision_recall_curve(y_test, pred)\n        sns.lineplot(x=fpr, y=tpr, color=sns.color_palette(\"magma\", 10)[-i],\n                     linewidth=2, label= f\"PR-AUC {model}= {round(average_precision*100,2)}%\", ax=ax)","f4ebf5dd":"perf_data = performances(test_data,y_test)","a26d6da8":"plt.figure(figsize=(15,10))\nsns.heatmap(perf_data*100, annot=True, cmap='coolwarm',fmt='.3g')","f0b07b72":"plot_roc_curve(test_data_prob,y_test)","dbdd0bde":"plot_pr_curve(test_data_prob,y_test)","7a0faa17":"n = 50\ncache_test = {'model_'+str(i):[] for i in range(len(models))}\ncache_test['AdaBoost'] = []\ncache_test['LightGBM'] = []\ncache_test['XGBoost'] = []\n\nfor j in range(n):\n    X_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, test_size=0.30, random_state=j)\n    X_train_ = scaler2.transform(X_train_)\n    X_test_ = scaler2.transform(X_test_)\n    \n    # getting the performance of the 5 best models from the random search\n    for i in range(len(models)):\n        predictions = models[i].predict_classes(X_test_)\n        cache_test['model_'+str(i)].append(float(classification_report(y_test_, predictions)[148:153]))\n        \n    adac_pred = adac.predict(X_test_)\n    cache_test['AdaBoost'].append(float(classification_report(y_test_, adac_pred)[148:153]))\n    lgbc_pred = lgbc.predict(X_test_)\n    cache_test['LightGBM'].append(float(classification_report(y_test_, lgbc_pred)[148:153]))\n    xgbc_pred = xgbc.predict(X_test_)\n    cache_test['XGBoost'].append(float(classification_report(y_test_, xgbc_pred)[148:153]))\n       \n# transforming the generated dictionaries into pandas dataframes\ncache_test = pd.DataFrame(cache_test)","4efae71e":"fig, ax = plt.subplots(nrows=2, ncols=1)\nfig.set_figheight(20)\nfig.set_figwidth(15)\nbest_test = f'Best f1-Score from {np.argmax(cache_test.mean())}: {round(cache_test.mean().max()*100,2)}%'\nax[0].set_title('Cross Validation along with multiple test_set Samples\\n' + best_test)\nsns.lineplot(data=cache_test, ax=ax[0], dashes=False)\nax[1].set_title(f'Mean F1-Score values along {n} sample tests\\n' + best_test)\nax[1].bar(x=cache_test.mean().index, height=cache_test.mean().values)\n\nfor i in range(len(ax)):\n    if i%2==1:\n        for p in ax[i].patches:\n            ax[i].annotate(f'{round(p.get_height()*100,2)}%', (p.get_x()+0.3, p.get_height()+0.02), fontsize=15)","9eebc695":"n=50\nperformance_values = []\nfor j in range(n):\n    X_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, test_size=0.30, random_state=j)\n    X_train_ = scaler2.transform(X_train_)\n    X_test_ = scaler2.transform(X_test_)\n    test_data = {}\n    for i in range(len(models)):\n        test_data['model_'+str(i)] = np.array(models[i].predict_classes(X_test_))\n    test_data['AdaBoost'] = np.array(adac.predict(X_test_))\n    test_data['LightGBM'] = np.array(lgbc.predict(X_test_))\n    test_data['XGBoost'] = np.array(xgbc.predict(X_test_))\n    performance_values.append(performances(test_data,y_test_))\nperf_data = pd.concat(performance_values)","d6161547":"perf_data.groupby(perf_data.index).mean()","a1ff6a86":"plt.figure(figsize=(15,10))\nsns.heatmap(perf_data.groupby(perf_data.index).mean()*100, annot=True, cmap='coolwarm',fmt='.4g')","ffadf447":"abstract = np.mean(perf_data.groupby(perf_data.index).mean(),axis=0).sort_values(ascending=False)*100\nprint(abstract)","3ff8f747":"model_0.save(\"model_MedCare_random_search_v8.h5\")\nlgbc.booster_.save_model(\"LightGBM_model.txt\")\nxgbc.save_model('XGBoost_model.model')","9d511ab7":"# Creating a model based on the results of a ranndom search performed before","e626cd72":"#### getting the best 5 models based on the f1-score performance","0db7de28":"The f1-score from the algorithms was almost the same in every sample","c462ecc8":"#### showing the results of the random search","35a41ab3":"# Performing upsamplig with the SMOTE (Synthetic Minority Over-sampling Technique)\nThe dataset was very umbalanced and that made the learning process of the algorithm difficult to perform and the performance was at it's best about 74% f1-score.","6d966475":"#### Creating a funtion to build the model architecture for each random search iteration","402d9a41":"### Creating a Random Search for hyperparameter tuning","119416f2":"#### plotting the F1-Score of the 8 best models along the n-times tests based on the redistribution of the testset","08defe68":"#### saving the best model (normaly commented to avoid overwriting the models)","34fc4eee":"# COVID-19_Medical_care_need","8e0cc85c":"# Creation of the RandomSearch object 'tuner' to perform the random search\n\n```python\n\nimport os\ntuner = RandomSearch(\n    build_model,\n    objective = Objective('val_get_f1', 'max'),\n    max_trials=10,\n    executions_per_trial=3,\n    directory= os.path.normpath('C:\/'),\n    project_name='MedCare_v8')\n\n```","d06ca1da":"#### Checking all the features, which are going to be used for training the model","203b4610":"#### The whole dataset is going to be used, after discovering which are the most important features to predict the need of intensive health care","4afd460a":"#### Creating the random search tuner","a7eebbe0":"The main point to achieve the performace of more than 97% (f1-score, accuracy, roc and recall) in the trained models was the data preparation process. The same setup performed with NN (with class wheithing) 74% f1-score and 50% with LightGBM and XGBoost. The key point for the performance rise was the over-sampling algorithm SMOTE which was used to balance the dataset. With the balanced dataset the algorithms learned much better.","2b63d2b9":"#### Getting the correlation of the most important features","ae97e913":"## IMPORTANT!!!\n### The RandomSearch was performed locally with the code that is shown here as python looking markdowns.\n### The results were used to define the parameters for the Keras sequential model trained in this notebook","94791bef":"#### Droping all the specific information, which is not available in conventional blood and urine exams","d4172bfd":"# Performance Analysis","dbd054af":"#### Droping the information about the medical care needs from the training set and one-hot encoding the feature \"SARS-CoV-2 exam result\"","2a679151":"#### performing upsampling from the positive data in order to balance the dataset\nUpsampling was chosen becouse of the ammount of data.","0fe823e9":"#### printing the classification report of the 5 best models which came from the random search","bed4888c":"#### Performing the random search ","d6c53304":"Model developed to predict the **need of semi-intensive or intensive medical care of patients based on regular blood and urine exams**.\n\nIn the COVID-19 pandemic crisis, a main issue is to manage the healthcare infra structure in order to serve the infected people. Not every person need semi-intensive or intensive medical care and these medical ressources are scarse due to the ammount of infected people.\n\nIn that way this project was created with **the target to predict the need of semi-intensive or intensive care of patients infected with the novel SARS-CoV-2 virus**, to help the healthcare workers to **manage the infrastructure** and to **improve the possibilities to sava lifes**.","68df7707":"# Conclusion","c43466c7":"```python\nfrom kerastuner.tuners import RandomSearch\nfrom kerastuner import HyperParameters,Objective\n```","4775e2fe":"## Filling all the values with the median from the Infection group, based on the information if the patient has tested positive or negative to the SARS-Cov-2 exam","52111429":"#### function that gets the f1-score as metrics","8ec8ae9b":"Showing all the features, which are going to be used to the training","32fa0943":"# Thank you!","5c355f34":"#### function which determines the learn decay rate","490e9cfc":"#### Splitting the data set into training_set and test_set","4339fd5c":"#### Calculating the F1-Score for 50 different samples","a9258ec5":"Creating a kerastuner HyperPararmeter objetct to perform the random search","a4784d2c":"```python\ntuner.search_space_summary()\n```","48d0f8d1":"### Crossvalidation performance Analysis","e7cabeb2":"```python\ntuner.results_summary()\n```","7abb227c":"```python\ntuner.search(x=np.array(X_train), y=np.array(y_train), validation_data=(np.array(X_test), np.array(y_test)),\n             batch_size = 128, epochs = 500)#, callbacks=[LearningRateScheduler(lr_scheduler, verbose=1)])\n```","0d17a78d":"#### Testing the performance with AdaBoost Classifier and LightGBM algorithms","8186c996":"The AdaBoost and LightGBM model perform better on the training_set but considerably poorer on the test set","07125d25":"#### Checking the balance of the data after pergorming over-sampling with the SMOTE algorithm","b14c7139":"# Creating models which need less parameters to make predictions\n\nThe main objective of the model is to predict if someone, which is infected with the SARS-CoV-2 is going to need semi-intensive or intensive medical care.\n\nBut in order to make the model as usable as possible, just the normal blood and urine exams features are going to be used, so that with simple conventional blood and urine exams it is possible to predict the need of semi-intensive or intensive care.\n\nThe idea is that the model is going to be used to help the healthcare organization to manage better the infra structure ressources","933761a9":"Analysis about the correlation between the medical care need and the features of the dataset","fdbadc68":"#### performing a cross validation process with the reassembling and redistributing test_set n-times","40468671":"```python\nhp = HyperParameters()\n```"}}