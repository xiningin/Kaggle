{"cell_type":{"32b92b65":"code","65933469":"code","e0042d40":"code","729dba4b":"code","485c4890":"code","4b203dde":"code","8610f137":"code","5d0245bb":"code","a76c5862":"code","68f97ffc":"code","8998932d":"code","819e09ce":"code","2a9cec2f":"code","d352f07d":"code","ff54dc31":"code","9a039fcb":"code","c64da635":"code","f0b08f73":"code","6af62045":"code","e5344949":"code","2e3724b5":"code","433a550c":"code","ca624bbe":"code","2a5e36fc":"code","ba37fe19":"code","2d1eeafa":"code","62645b48":"code","dd00345e":"code","001700a1":"code","1e2914b4":"code","02f41952":"code","cc0ab5cf":"code","df7f0d70":"code","a7a557b0":"code","a69e3087":"code","2264c192":"code","d73d9862":"code","cfff69d2":"code","545889a1":"code","8378cd56":"code","0bcf90c0":"code","576a4cea":"code","130f4161":"code","308b5a8b":"code","08bf3cb4":"code","71c99d08":"code","83c9bf4f":"code","bbecc044":"code","bc6ab75b":"code","cce8a419":"code","ba389cb9":"code","dfff7be0":"code","d683c5e8":"markdown","cd7171cd":"markdown","c30710e2":"markdown","113cb5e9":"markdown","afc28d84":"markdown","33c83897":"markdown","1d3bab7a":"markdown","50611262":"markdown","3fb2507d":"markdown","dcaebe72":"markdown","1bfd61bc":"markdown","658d073d":"markdown","3a90d61e":"markdown","c4177038":"markdown","dd08f266":"markdown","e1d6f5dc":"markdown","af3bc00d":"markdown","c6fef386":"markdown","81ebfddb":"markdown","f7ca9b6e":"markdown"},"source":{"32b92b65":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport urllib.request\nfrom PIL import Image\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nimport pickle\n\nfrom catboost import CatBoostRegressor","65933469":"df = pd.read_csv(\"data\/diamonds_train.csv\", index_col=0)\ndf","e0042d40":"df.info()","729dba4b":"df.head()","485c4890":"df[\"cut\"].unique()","4b203dde":"# The cut order from high to low is: Ideal, Premium, very good, good, fair\n# LabelEncoder assigns values by alphabetical order so instead a lambda is used to encode this column\n\n\ndf[\"cut\"] = df[\"cut\"].replace({\"Ideal\": 5, \"Premium\": 4, \"Very Good\": 3, \"Good\": 2, \"Fair\": 1})","8610f137":"df[\"color\"].unique()","5d0245bb":"# A higher price is linked to the alphabetical order of the property 'color'. \n\ndf[\"color\"] = df[\"color\"].replace({\"D\": 7, \"E\": 6, \"F\": 5, \"G\": 4, \"H\": 3, \"I\":2, \"J\":1})","a76c5862":"df[\"clarity\"].unique()","68f97ffc":"# Order considering a higer to lower price: IF, VVS1, VVS2, VS1, VS2, SI1, SI2, I1\ndf[\"clarity\"] = df[\"clarity\"].replace({\"I1\": 1, \"VVS1\": 2, \"SI2\": 3, \"SI1\": 4, \"IF\": 5, \"VS1\":6, \"VVS2\":7, \"VS2\":8})","8998932d":"# Correlation between columns\nplt.figure(figsize = (10,5))\nsns.heatmap(df.corr(),annot = True , cmap = \"coolwarm\")\n\n# Carat, x, y and z are highly correlated to the price","819e09ce":"# As cut will be used when training the model, we can ignore the table and depth as this column is based on those two","2a9cec2f":"X = df.drop([ \"price\"], 1)\ny = df[\"price\"]","d352f07d":"X.shape","ff54dc31":"y.shape","9a039fcb":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 4)","c64da635":"# Voting\nfrom sklearn.ensemble import VotingRegressor\n\nrnd_clf = RandomForestRegressor(n_estimators=100, random_state=4)\nsvr_clf = SVR()\ncat_clf = CatBoostRegressor(random_state=4)\n\nestimators = [(\"forest\", rnd_clf), (\"svr\", svr_clf), (\"cat\", cat_clf)]\n\nvoting_clf = VotingRegressor(estimators)\nvoting_clf.fit(X_train, y_train)\n\nfrom sklearn.metrics import accuracy_score\n\nfor clf in (rnd_clf, svr_clf, cat_clf, voting_clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(clf.__class__.__name__, np.sqrt(mean_squared_error(y_test, y_pred)))\n","f0b08f73":"# VotingRegressor suggests that CatBoostRegressor is the most accurate model. In any case, we will still try different models without parameters for comparison","6af62045":"pipe = Pipeline(steps=[\n    (\"classifier\", RandomForestRegressor())\n])\n\nrandom_forest_params = {\n    \"classifier\": [RandomForestRegressor()],\n    \"classifier__random_state\": [4]\n    #\"classifier__n_estimators\": [1, 10, 100],\n    #\"classifier__max_features\": [1,2,3]\n}\n\nsearch_space = [random_forest_params]\n\nclf = GridSearchCV(estimator = pipe,\n                  param_grid = search_space,\n                  cv = 10,\n                  verbose=1,\n                  n_jobs=-1)\n\nclf.fit(X_train, y_train)","e5344949":"clf = GridSearchCV(estimator=pipe, param_grid=search_space)","2e3724b5":"%%time\n# Fit grid search\nbest_model = clf.fit(X_train, y_train)\n# View best model\nprint(\"best estimator:\", best_model.best_estimator_.get_params()['classifier'])\nprint(\"clf.best_params_\", clf.best_params_)\n# Mean cross-validated score of the best_estimator\nprint(\"clf.best_score\", clf.best_score_)\n#SAVE MODEL\n# save the model to disk\nfilename = 'model_random_forest_1.sav'\npickle.dump(best_model, open(filename, 'wb'))","433a550c":"from sklearn.pipeline import make_pipeline\n\ndef PolynomialRegression(degree=2, **kwargs):\n    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))\n\nparam_grid = {\n    \"polynomialfeatures__degree\": np.arange(10),\n    #\"linearregression__fit_intercept\": [True, False], \n    #\"linearregression__normalize\": [True, False]\n}\n\npoly_grid = GridSearchCV(PolynomialRegression(), \n                param_grid, \n                cv=10, \n                scoring=\"neg_mean_squared_error\",\n                verbose=1,\n                n_jobs=-1)\n\nclf_poly.fit(X_train, y_train)\nclf_poly = GridSearchCV(PolynomialRegression(), param_grid)","ca624bbe":"%%time\n# Fit grid search\nbest_model_poly = clf_poly.fit(X_train, y_train)\n# View best model\nprint(\"best estimator:\", best_model.best_estimator_.get_params()['classifier'])\nprint(\"clf.best_params_\", clf_poly.best_params_)\n# Mean cross-validated score of the best_estimator\nprint(\"clf.best_score\", clf_poly.best_score_)\n#SAVE MODEL\n# save the model to disk\nfilename = 'model_poly_1.sav'\npickle.dump(best_model_poly, open(filename, 'wb'))","2a5e36fc":"pipe = Pipeline(steps=[\n    (\"classifier\", SVR)\n])\n\nsvr_params = {\n    \"classifier\": [SVR()],\n    \"classifier__kernel\": (\"linear\", \"rbf\", \"poly\")\n    #\"classifier__C\": [1, 10, 100, 1000],\n    \n}\n\nclf_svr = GridSearchCV(estimator = pipe,\n                  param_grid = svr_params,\n                  cv = 10,\n                  verbose=1,\n                  n_jobs=-1)\n\nclf_svr.fit(X_train, y_train)\nclf_svr = GridSearchCV(estimator=pipe, param_grid=search_space)","ba37fe19":"%%time\n# Fit grid search\nbest_model_svr = clf_svr.fit(X_train, y_train)\n# View best model\nprint(\"best estimator:\", best_model.best_estimator_.get_params()['classifier'])\nprint(\"clf.best_params_\", clf_svr.best_params_)\n# Mean cross-validated score of the best_estimator\nprint(\"clf.best_score\", clf_svr.best_score_)\n#SAVE MODEL\n# save the model to disk\nfilename = 'model_svr_1.sav'\npickle.dump(best_model_svr, open(filename, 'wb'))","2d1eeafa":"pipe = Pipeline(steps=[\n    (\"classifier\", CatBoostRegressor())\n])\n\ncatboost_params = {\n    \"classifier\": [CatBoostRegressor()],\n    \"classifier__iterations\": [100,200],\n    \"classifier__depth\": [6,10],\n    \"classifier__l2_leaf_reg\": [5,10,100],\n    #\"classifier__border_count\": [10,20,50,100],\n    #\"classifier__random_strength\": [0.2,0.5,0.8],\n    \"classifier__eval_metric\": [\"r2\", \"RMSE\"],\n    \"classifier__od_type\": [\"IncToDec\", \"Iter\"],\n    \"classifier__random_state\": [4,42] \n\n}\n                \nclf_cat = GridSearchCV(estimator = pipe,\n                  param_grid = catboost_params,\n                  cv = 10,\n                  verbose=2,\n                  n_jobs=-1)\n\nclf_cat.fit(X_train, y_train)","62645b48":"%%time\n# Fit grid search\nbest_model_cat = clf_cat.fit(X_train, y_train)\n# View best model\nprint(\"best estimator:\", best_model_cat.best_estimator_.get_params()['classifier'])\nprint(\"clf.best_params_\", clf_cat.best_params_)\n# Mean cross-validated score of the best_estimator\nprint(\"clf.best_score\", clf_cat.best_score_)\n#SAVE MODEL\n# save the model to disk\nfilename = 'model_cat_1.sav'\npickle.dump(best_model_cat, open(filename, 'wb'))","dd00345e":"# GridSearch suggests that the best model is RandomForestRegressor.","001700a1":"with open(\"model_random_forest_1.sav\",\"rb\") as f:\n    loaded_model = pickle.load(f)","1e2914b4":"predictions = loaded_model.predict(X_test)\nprint(predictions)\nnp.sqrt(mean_squared_error(y_test, predictions))","02f41952":"print(f\"Score with no parameters: {loaded_model.score(X_train, y_train)}\")","cc0ab5cf":"# TRYING hyperparameters with a smaller train size to reduce the waiting time. If results are desirable the 80% of train data will be used to train the model. - STILL VSC KEPT CRASHING SO THIS ANALYSIS COULD NEVER BE COMPLETED.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 4)\n\npipe = Pipeline(steps=[\n    (\"classifier\", RandomForestRegressor())\n])\n\nrandom_forest_params = {\n    \"classifier\": [RandomForestRegressor()],\n    \"classifier__random_state\": [4],\n    \"classifier__n_estimators\": [50, 100, 200],\n    \"classifier__max_features\": [1,2,3],\n    \"classifier__warm_start\": [True],\n    \"classifier__criterion\": [\"mse\", \"mae\"]\n\n}\n\nsearch_space = [random_forest_params]\n\nclf_rf = GridSearchCV(estimator = pipe,\n                  param_grid = random_forest_params,\n                  cv = 10,\n                  verbose=1,\n                  n_jobs=-1)\n\nclf_rf.fit(X_train, y_train)\nclf_rf = GridSearchCV(estimator=pipe, param_grid=search_space)","df7f0d70":"%%time\n# Fit grid search\nbest_model_rf = clf_rf.fit(X_train, y_train)\n# View best model\nprint(\"best estimator:\", best_model.best_estimator_.get_params()['classifier'])\nprint(\"clf.best_params_\", clf_rf.best_params_)\n# Mean cross-validated score of the best_estimator\nprint(\"clf.best_score\", clf_rf.best_score_)\n#SAVE MODEL\n# save the model to disk\nfilename = 'model_random_forest_3.sav'\npickle.dump(best_model_rf, open(filename, 'wb'))","a7a557b0":"# normalization\nfrom sklearn.preprocessing import Normalizer\nscaler = Normalizer()\nscaler.fit(X)\nX_normalized = scaler.transform(X)\nX_normalized\n\n# train\/test split\nX_train_n, X_test_n, y_train_n, y_test_n = train_test_split(X_normalized, y, test_size=0.20, random_state=4)","a69e3087":"loaded_model_n = loaded_model\nloaded_model_n.fit = (X_train_n, y_train_n)\nprint(f\"Score with no parameters but normalization: {loaded_model.score(X_train_n, y_train_n)}\")\npredictions_n = loaded_model_n.predict(X_test_n)\nnp.sqrt(mean_squared_error(y_test_n, predictions_n))\n\n# Worse score with normalization, so it won't be used. ","2264c192":"# As RandomForestRegressor couldn't be completed with hyperparamters other methods are tried.","d73d9862":"# Bagging\nfrom sklearn.ensemble import BaggingRegressor\nestimator = RandomForestRegressor(random_state=4)\n\nbag_clf = BaggingRegressor(\n            base_estimator = estimator,\n            n_estimators = 500,\n            max_samples = 500,\n            bootstrap = True,\n            warm_start = True,\n            random_state=4          \n)\n\nbag_clf.fit(X_train, y_train)\n\nbag_clf.score(X_train, y_train)\n\npredictions_b = loaded_model_n.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, predictions_b))","cfff69d2":"# Adaboosting","545889a1":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn import model_selection\n\nnum_trees = 30\n\nkfold = model_selection.KFold(n_splits=10)\nmodel = AdaBoostRegressor(n_estimators = num_trees, random_state=4)\nres_abc = model_selection.cross_val_score(model, X, y, cv=kfold).mean()\nmodel.fit(X_train, y_train)\nmodel.score(X_train, y_train)\n\n# less than other methods","8378cd56":"# GradientBoosting\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nnum_trees = 100\n\nmodel = GradientBoostingRegressor(n_estimators = num_trees, random_state=4)\nres_gbc = model_selection.cross_val_score(model, X, y, cv=kfold).mean()\nmodel.fit(X_train, y_train)\nmodel.score(X_train, y_train)\n","0bcf90c0":"#XGBoost\nfrom xgboost import XGBRegressor\nfrom sklearn import model_selection\n\nmodel_x = XGBRegressor(n_estimators = 100)\nres_xgb = model_selection.cross_val_score(model_x, X, y, cv=10).mean()\nmodel_x.fit(X_train, y_train)\nmodel_x.score(X_train, y_train)\ny_pred_x = model_x.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred_x))","576a4cea":"# XGBoost is the best model in this analysis","130f4161":"predictions = model_x.predict(X_test)\nprint(predictions)","308b5a8b":"np.sqrt(mean_squared_error(y_test, predictions))","08bf3cb4":"X_pred = pd.read_csv(\"data\/diamonds_test.csv\", index_col = 0)\nX_pred.head()","71c99d08":"# Same transformations than before to X_pred are carried out\n\nX_pred[\"cut\"] = X_pred[\"cut\"].replace({\"Ideal\": 5, \"Premium\": 4, \"Very Good\": 3, \"Good\": 2, \"Fair\": 1})\n\nX_pred[\"color\"] = X_pred[\"color\"].replace({\"D\": 7, \"E\": 6, \"F\": 5, \"G\": 4, \"H\": 3, \"I\":2, \"J\":1})\n\nX_pred[\"clarity\"] = X_pred[\"clarity\"].replace({\"I1\": 1, \"VVS1\": 2, \"SI2\": 3, \"SI1\": 4, \"IF\": 5, \"VS1\":6, \"VVS2\":7, \"VS2\":8})\n","83c9bf4f":"predictions_submit = model_x.predict(X_pred)\npredictions_submit","bbecc044":"sample = pd.read_csv(\"data\/sample_submission.csv\")","bc6ab75b":"sample.head()","cce8a419":"submission = pd.DataFrame({\"id\": range(len(predictions_submit)), \"price\": predictions_submit})\nsubmission","ba389cb9":"def chequeator(df_to_submit):\n    \"\"\"\n    Esta funci\u00f3n se asegura de que tu submission tenga la forma requerida por Kaggle.\n    \n    Si es as\u00ed, se guardar\u00e1 el dataframe en un `csv` y estar\u00e1 listo para subir a Kaggle.\n    \n    Si no, LEE EL MENSAJE Y HAZLE CASO.\n    \n    Si a\u00fan no:\n    - apaga tu ordenador, \n    - date una vuelta, \n    - enciendelo otra vez, \n    - abre este notebook y \n    - leelo todo de nuevo. \n    Todos nos merecemos una segunda oportunidad. Tambi\u00e9n t\u00fa.\n    \"\"\"\n    if df_to_submit.shape == sample.shape:\n        if df_to_submit.columns.all() == sample.columns.all():\n            if df_to_submit.id.all() == sample.id.all():\n                print(\"You're ready to submit!\")\n                submission.to_csv(\"submission.csv\", index = False) #muy importante el index = False\n                urllib.request.urlretrieve(\"https:\/\/i.kym-cdn.com\/photos\/images\/facebook\/000\/747\/556\/27a.jpg\", \"gfg.png\")     \n                img = Image.open(\"gfg.png\")\n                img.show()   \n            else:\n                print(\"Check the ids and try again\")\n        else:\n            print(\"Check the names of the columns and try again\")\n    else:\n        print(\"Check the number of rows and\/or columns and try again\")\n        print(\"\\nMensaje secreto de Clara: No me puedo creer que despu\u00e9s de todo este notebook hayas hecho alg\u00fan cambio en las filas de `diamonds_test.csv`. Lloro.\")","dfff7be0":"chequeator(submission)","d683c5e8":"## 5. Predictions with X_test","cd7171cd":"### Correlation","c30710e2":"## 3. Dividing X_train, X_test, y_train, y_test","113cb5e9":"## 6. RMSE","afc28d84":"## 8. Prediction with all data","33c83897":"## 4. Model selection","1d3bab7a":"### SVR","50611262":"### Color property","3fb2507d":"### clarity ","dcaebe72":"## 7. Train the model with all available data","1bfd61bc":"### Cut property","658d073d":"### Catboost","3a90d61e":"### Polynominal ","c4177038":"## 2. Defining X and y","dd08f266":"## 5. P\u00e1sale el CHEQUEATOR para comprobar que efectivamente est\u00e1 listo para subir a Kaggle.\n","e1d6f5dc":"### **!! This model takes 1:30h to process !!**","af3bc00d":"### Ensembling","c6fef386":"### Random Forest","81ebfddb":"## **Submission to Kaggle**\n","f7ca9b6e":"## 1. Data transformation"}}