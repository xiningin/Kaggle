{"cell_type":{"881fb2a6":"code","cb34a70c":"code","3b57b4bd":"code","105fdb9c":"code","bc395326":"code","2b1c92d7":"code","9e9f9b3b":"code","624db6c7":"code","ab5d23d5":"code","d0248fcc":"code","1269f4a3":"code","1003d856":"markdown","3f0a25b6":"markdown","1139cc06":"markdown","ddeba9d2":"markdown","9df3fce8":"markdown"},"source":{"881fb2a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cb34a70c":"# Author - Gowtham Ch\n# https:\/\/www.linkedin.com\/in\/gauthamchowta\/","3b57b4bd":"from sklearn.preprocessing import LabelBinarizer\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\nimport warnings\nimport numpy as np\nfrom sklearn.naive_bayes import GaussianNB","105fdb9c":"X  = np.array([[-0.8, -0.8],[-1.8, -0.8],[-2.8, -1.8],[ 1.2,  1.2], [ 2.2,  1.2],[ 3.2,  2.2]])\ny = np.array([1, 1, 1, 2, 2, 2])","bc395326":"def preprocess_target(y):\n    \n    y  = np.array(y)\n    lb = LabelBinarizer()\n    y = lb.fit_transform(y)\n    if y.shape[1] == 1:\n        y = np.concatenate((1 - y, y), axis=1)\n    return y\n    ","2b1c92d7":"y = preprocess_target(y)\nn_classes = y.shape[1]","9e9f9b3b":"y","624db6c7":"def cal_mean_var_based_on_class():\n    n_classes = y.shape[1]\n    mean_array = []\n    var_array = []\n    class_count = []\n    # For each class calculate the mean and variance\n    for j in range(n_classes):\n        mask = y[:,j].astype(bool)\n        X_feature = X[mask]\n        mean_array.append(np.mean(X_feature,axis = 0))\n        var_array.append(np.var(X_feature,axis = 0))\n        # Appending the class count for calculating prior probabilities\n        class_count.append(sum(mask))\n        \n    return mean_array,var_array,class_count\n    ","ab5d23d5":"mean_array,var_array,class_count = cal_mean_var_based_on_class()","d0248fcc":"def calculate_posterior_probs(query_point, mean_array,var_array,class_count,n_classes):\n    \n    probs = []\n    for i in range(n_classes):\n\n        mean = mean_array[i]\n        var = var_array[i]\n        \n        log_li = -0.5*(np.log(2*np.pi*var))  - 0.5*((query_point-mean)**2)\/var\n        prior_prob = np.log(class_count[i]\/sum(class_count))\n        probs.append(log_li.sum() + prior_prob)\n\n    return probs\n        \n    ","1269f4a3":"\nX  = np.array([[-0.8, -0.8],[-1.8, -0.8],[-2.8, -1.8],[ 1.2,  1.2], [ 2.2,  1.2],[ 3.2,  2.2]])\ny = np.array([1, 1, 1, 2, 2, 2])\n\n\nclf = GaussianNB()\nclf.fit(X, y)\n\n\n# Manual implementation:\n# Step -1 preprocessing the target variables\ny = preprocess_target(y)\n\n# Step -2 Calculating the mean and variance based on the class values.\nmean_array,var_array,class_count = cal_mean_var_based_on_class()\n# Step -3 Calculating the likelihood probabilities.\nout =  calculate_posterior_probs(X[0],mean_array,var_array,class_count,n_classes)\n\nprint('Sklearn values:')\nprint('Mean is',clf.theta_)\nprint('Variance is',clf.sigma_)\nprint('Probabilities')\nprint(clf._joint_log_likelihood(X[0:1]))\nprint('*'*50)\nprint('Manual values:')\nprint('Mean is',mean_array)\nprint('Var is',var_array)\nprint('Probabilities')\nprint(out)\n","1003d856":"Calculating posterior probability\n* Here in Gaussian naive bayes the likelihood is assumed to be coming from Gaussian distribution, which means the likelihood probabilities follow the below equation.\n![](https:\/\/miro.medium.com\/max\/932\/1*vYa22Lw1dhHsLfL0URKeKw.png)\nWhen we apply log on both sides we get the following relation:  \n![](https:\/\/miro.medium.com\/max\/1050\/1*PFlyj3uturutjox2ZXZQfA.jpeg)","3f0a25b6":"Bingo!! \u2014 The results are matching, you can find the max of the probabilities to get the predict class value.","1139cc06":"1. Preprocess the data.\n1. Calculate Mean and variance for each feature based on class.\n1. Calculate posterior probabilities.","ddeba9d2":"Calculate Mean and variance for each feature based on class.\n* First, extract the data points of a particular class and then calculate the mean and variance for each of the features.\n* I am also calculating the counts of the target variables which we will use shortly for calculating prior probability.\nAssume you are doing a binary classification data, the below is the process for calculating mean and variance for a feature \u2014 fj in class \u2014 0  \n![](https:\/\/miro.medium.com\/max\/1050\/1*JcNlYBzJIZeDHDJUyYXjHQ.png)","9df3fce8":"Preprocess the data\n\nI am doing one-hot encoding of the target variables like we did in MultinomialNB and CategoricalNB implementation.\n\n"}}