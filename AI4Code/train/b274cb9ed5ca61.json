{"cell_type":{"0602d265":"code","75c5548f":"code","7e3e25ef":"code","92a45f2f":"code","23d8874d":"code","1af659e4":"code","b69f7ad4":"code","0a480d74":"code","bd792112":"code","3e6a022c":"code","2db2f41d":"markdown","660ffe73":"markdown","fe86bfaf":"markdown","293a9ef1":"markdown","8e7898e6":"markdown","ada07ad9":"markdown","8582ed5e":"markdown","48f815f3":"markdown","32091303":"markdown","e45fe1aa":"markdown","6ca696b0":"markdown","a03c7f99":"markdown","df043721":"markdown","3f6d2668":"markdown","0ef13594":"markdown"},"source":{"0602d265":"! ls ..\/input\/severstalmodels","75c5548f":"! python ..\/input\/mlcomp\/mlcomp\/mlcomp\/setup.py","7e3e25ef":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport cv2\nimport albumentations as A\nfrom tqdm import tqdm_notebook\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.jit import load\n\nfrom mlcomp.contrib.transform.albumentations import ChannelTranspose\nfrom mlcomp.contrib.dataset.classify import ImageDataset\nfrom mlcomp.contrib.transform.rle import rle2mask, mask2rle\nfrom mlcomp.contrib.transform.tta import TtaWrap","92a45f2f":"unet_se_resnext50_32x4d = \\\n    load('\/kaggle\/input\/severstalmodels\/unet_se_resnext50_32x4d.pth').cuda()\nunet_mobilenet2 = load('\/kaggle\/input\/severstalmodels\/unet_mobilenet2.pth').cuda()\nunet_resnet34 = load('\/kaggle\/input\/severstalmodels\/unet_resnet34.pth').cuda()","23d8874d":"class Model:\n    def __init__(self, models):\n        self.models = models\n    \n    def __call__(self, x):\n        res = []\n        x = x.cuda()\n        with torch.no_grad():\n            for m in self.models:\n                res.append(m(x))\n        res = torch.stack(res)\n        return torch.mean(res, dim=0)\n\nmodel = Model([unet_se_resnext50_32x4d, unet_mobilenet2, unet_resnet34])","1af659e4":"def create_transforms(additional):\n    res = list(additional)\n    # add necessary transformations\n    res.extend([\n        A.Normalize(\n            mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n        ),\n        ChannelTranspose()\n    ])\n    res = A.Compose(res)\n    return res\n\nimg_folder = '\/kaggle\/input\/severstal-steel-defect-detection\/test_images'\nbatch_size = 2\nnum_workers = 0\n\n# Different transforms for TTA wrapper\ntransforms = [\n    [],\n    [A.HorizontalFlip(p=1)]\n]\n\ntransforms = [create_transforms(t) for t in transforms]\ndatasets = [TtaWrap(ImageDataset(img_folder=img_folder, transforms=t), tfms=t) for t in transforms]\nloaders = [DataLoader(d, num_workers=num_workers, batch_size=batch_size, shuffle=False) for d in datasets]","b69f7ad4":"thresholds = [0.5, 0.5, 0.5, 0.5]\nmin_area = [600, 600, 1000, 2000]\n\nres = []\n# Iterate over all TTA loaders\ntotal = len(datasets[0])\/\/batch_size\nfor loaders_batch in tqdm_notebook(zip(*loaders), total=total):\n    preds = []\n    image_file = []\n    for i, batch in enumerate(loaders_batch):\n        features = batch['features'].cuda()\n        p = torch.sigmoid(model(features))\n        # inverse operations for TTA\n        p = datasets[i].inverse(p)\n        preds.append(p)\n        image_file = batch['image_file']\n    \n    # TTA mean\n    preds = torch.stack(preds)\n    preds = torch.mean(preds, dim=0)\n    preds = preds.detach().cpu().numpy()\n    \n    # Batch post processing\n    for p, file in zip(preds, image_file):\n        file = os.path.basename(file)\n        # Image postprocessing\n        for i in range(4):\n            p_channel = p[i]\n            imageid_classid = file+'_'+str(i+1)\n            p_channel = (p_channel>thresholds[i]).astype(np.uint8)\n            if p_channel.sum() < min_area[i]:\n                p_channel = np.zeros(p_channel.shape, dtype=p_channel.dtype)\n\n            res.append({\n                'ImageId_ClassId': imageid_classid,\n                'EncodedPixels': mask2rle(p_channel)\n            })\n        \ndf = pd.DataFrame(res)\ndf.to_csv('submission.csv', index=False)\t","0a480d74":"df = pd.DataFrame(res)\ndf = df.fillna('')\ndf.to_csv('submission.csv', index=False)","bd792112":"df['Image'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[0])\ndf['Class'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[1])\ndf['empty'] = df['EncodedPixels'].map(lambda x: not x)\ndf[df['empty'] == False]['Class'].value_counts()","3e6a022c":"%matplotlib inline\n\ndf = pd.read_csv('submission.csv')[:40]\ndf['Image'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[0])\ndf['Class'] = df['ImageId_ClassId'].map(lambda x: x.split('_')[1])\n\nfor row in df.itertuples():\n    img_path = os.path.join(img_folder, row.Image)\n    img = cv2.imread(img_path)\n    mask = rle2mask(row.EncodedPixels, (1600, 256)) \\\n        if isinstance(row.EncodedPixels, str) else np.zeros((256, 1600))\n    if mask.sum() == 0:\n        continue\n    \n    fig, axes = plt.subplots(1, 2, figsize=(20, 60))\n    axes[0].imshow(img\/255)\n    axes[1].imshow(mask*60)\n    axes[0].set_title(row.Image)\n    axes[1].set_title(row.Class)\n    plt.show()","2db2f41d":"### Create TTA transforms, datasets, loaders","660ffe73":"### Load models","fe86bfaf":"Catalyst allows to trace models. That is an extremely useful features in Pytorch since 1.0 version: \n\nhttps:\/\/pytorch.org\/docs\/stable\/jit.html\n\nNow we can load models without re-defining them","293a9ef1":"### Models' mean aggregator","8e7898e6":"This kernel demonstrates:\n\n1. Results of training models with [the training kernel](https:\/\/www.kaggle.com\/lightforever\/severstal-mlcomp-catalyst-train-0-90672-offline) and achieves 0.90672 score on public LB\n\n2. Useful code in MLComp library: TtaWrapp, ImageDataset, ChannelTranspose, rle utilities\n\n3. Output statistics and basic visualization","ada07ad9":"As the competition does not allow commit with the kernel that uses internet connection, we use offline installation","8582ed5e":"### Visualization","48f815f3":"About the libraries:\n\n1. [MLComp](https:\/\/github.com\/catalyst-team\/mlcomp) is a distributed DAG  (Directed acyclic graph)  framework for machine learning with UI. It helps to train, manipulate, and visualize. All models in this kernel were trained offline via MLComp + Catalyst libraries. \n\nYou can control an execution process via Web-site\n\nDags\n![Dags](https:\/\/github.com\/catalyst-team\/mlcomp\/blob\/master\/docs\/imgs\/dags.png?raw=true)\n\nComputers\n![Computers](https:\/\/github.com\/catalyst-team\/mlcomp\/blob\/master\/docs\/imgs\/computers.png?raw=true)\n\nReports\n![Reports](https:\/\/github.com\/catalyst-team\/mlcomp\/blob\/master\/docs\/imgs\/reports.png?raw=true)\n\nCode\n![Code](https:\/\/github.com\/catalyst-team\/mlcomp\/blob\/master\/docs\/imgs\/code.png?raw=true)\n\nPlease follow [the web site](https:\/\/github.com\/catalyst-team\/mlcomp) to get the details.\n\nhttps:\/\/github.com\/catalyst-team\/mlcomp\n\n2. Catalys: High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code\/ideas reusing. Being able to research\/develop something new, rather then write another regular train loop. Break the cycle - use the Catalyst!\n\nhttps:\/\/github.com\/catalyst-team\/catalyst\n\nDocs and examples\n- Detailed [classification tutorial](https:\/\/github.com\/catalyst-team\/catalyst\/blob\/master\/examples\/notebooks\/classification-tutorial.ipynb) [![Open In Colab](https:\/\/colab.research.google.com\/assets\/colab-badge.svg)](https:\/\/colab.research.google.com\/github\/catalyst-team\/catalyst\/blob\/master\/examples\/notebooks\/classification-tutorial.ipynb)\n- Comprehensive [classification pipeline](https:\/\/github.com\/catalyst-team\/classification).\n\nAPI documentation and an overview of the library can be found here\n[![Docs](https:\/\/img.shields.io\/badge\/dynamic\/json.svg?label=docs&url=https%3A%2F%2Fpypi.org%2Fpypi%2Fcatalyst%2Fjson&query=%24.info.version&colorB=brightgreen&prefix=v)](https:\/\/catalyst-team.github.io\/catalyst\/index.html)","32091303":"### Loaders' mean aggregator","e45fe1aa":"Save predictions","6ca696b0":"### Import required libraries","a03c7f99":"Histogram of predictions","df043721":"![MLComp](https:\/\/raw.githubusercontent.com\/catalyst-team\/catalyst-pics\/master\/pics\/MLcomp.png)\n![Catalyst](https:\/\/raw.githubusercontent.com\/catalyst-team\/catalyst-pics\/master\/pics\/catalyst_logo.png)","3f6d2668":"Approach descripton:\n\n1. Segmentation via 3 Unet networks. The predictions are being averaged. \n\n2. Thresholding and removeing small areas. This method gives 0.90672 on public LB.\n\n**Improving**:\n\n1. As many participations have seen, that is the key to remove false positives from your predictions.\n\n2. To cope with that, a classification network may be used. \n\n3. Heng CherKeng posted a classifier here: https:\/\/www.kaggle.com\/c\/severstal-steel-defect-detection\/discussion\/106462#latest-634450 resent34_cls_01, **if you remove false positives with it you should get 0.9117 on LB**","0ef13594":"### Install MLComp library(offline version):"}}