{"cell_type":{"4f04d63d":"code","6d5251d1":"code","8495c911":"code","d64fc757":"code","8d13499c":"code","8399e1ba":"code","05288993":"code","80d404a0":"code","a6581943":"code","de5f4f3c":"code","b534b59f":"code","ca931d25":"code","ccdb2632":"code","6986eee0":"code","32ebe5c9":"code","5cd3f385":"code","4a612248":"code","9b97c505":"code","6131cf02":"code","15d1fa76":"code","4aa4f793":"code","573328b8":"code","746b215d":"code","ab8e3cf0":"code","1ea88f23":"code","6e3c53c1":"code","296ba17c":"code","0de6589b":"code","f034dc16":"code","3b91afa5":"code","5864ad6a":"code","1844f79e":"code","f5613dce":"code","a3393893":"code","60d2268a":"code","6e09fbb7":"code","d237e755":"code","46dc94c7":"code","1fad61de":"code","f1380844":"code","08fc8375":"code","5a346544":"code","b43ecd6e":"code","8e68943e":"code","927ce87f":"code","571ad535":"code","95eb8d78":"markdown","8d7c10aa":"markdown","29461d45":"markdown"},"source":{"4f04d63d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6d5251d1":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder","8495c911":"app_train = pd.read_csv('..\/input\/application_train.csv')\nprint('Training data shape: ',app_train.shape)\napp_train.head()","d64fc757":"app_test = pd.read_csv('..\/input\/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","8d13499c":"app_train['TARGET'].value_counts()","8399e1ba":"app_train['TARGET'].astype(int).plot.hist()","05288993":"#function to calculate missing values by columns# function\ndef missing_values_table(df):\n    #total missing values\n    miss_val = df.isnull().sum()\n    \n    #percentage of missing values\n    miss_val_percent = 100*df.isnull().sum()\/len(df)\n    \n    #Make a table with the results\n    miss_val_table = pd.concat([miss_val,miss_val_percent],axis=1)\n\n    #rename the columns\n    miss_val_table_ren_columns = miss_val_table.rename(columns={0:'Missing Values',\n                                                                 1: '% of Total Values'})\n    #sort the table by percent of missing descending\n    miss_val_table_ren_columns = miss_val_table_ren_columns[\n        miss_val_table_ren_columns.iloc[:,1]!=0].sort_values(\n    '% of Total Values', ascending=False).round(1)\n    \n    #print same summary information\n    print(\"Your selected dataframe has \"+str(df.shape[1]) +\n         \"columns.\\n\" \n         \"There are \"+str(miss_val_table_ren_columns.shape[0])+\n         \"columns that have missing values.\")\n    return miss_val_table_ren_columns","80d404a0":"missing_values = missing_values_table(app_train)\nmissing_values.head(20)","a6581943":"app_train.dtypes.value_counts()","de5f4f3c":"# number of unique classes in each object column\npd.DataFrame(app_train.select_dtypes('object').apply(pd.Series.nunique,axis=0))","b534b59f":"# Create a label encoder object\nle = LabelEncoder()\nle_count=0\n\n#Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        #if 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            le.fit(app_train[col])\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            le_count +=1\nprint('%d columns were label encoded. '% le_count)","ca931d25":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test =pd.get_dummies(app_test)\n\nprint('Training Features shape: ',app_train.shape)\nprint('Test Features shape: ',app_test.shape)","ccdb2632":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both data frames\n\napp_train,app_test = app_train.align(app_test,join='inner',axis=1)\n\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ',app_train.shape)\nprint('Testing Features shape: ',app_test.shape)","6986eee0":"(app_train['DAYS_BIRTH']\/-365).describe()","32ebe5c9":"app_train['DAYS_EMPLOYED'].describe()","5cd3f385":"app_train['DAYS_EMPLOYED'].plot.hist(title='Days Employmnet Histogram')\nplt.xlabel('Days Employment')","4a612248":"anom = app_train[app_train['DAYS_EMPLOYED']==365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED']!=365243]\nprint('The non-anomalies default on %0.2f%% of loans' %(100*non_anom['TARGET'].mean()))\n\nprint('The anomalies default on %0.2f%% of loans'%(100*anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment'%len(anom))","9b97c505":"#app_train[app_train['DAYS_EMPLOYED']>0 and app_train['DAYS_EMPLOYED']<365243]\n#app_train['DAYS_EMPLOYED']>0 & app_train['DAYS_EMPLOYED']<365243\napp_train[(app_train.DAYS_EMPLOYED>0)]['DAYS_EMPLOYED'].plot.hist() #== app_train[(app_train.DAYS_EMPLOYED<365243)]","6131cf02":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","15d1fa76":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","4aa4f793":"correlations = app_train.corr()['TARGET'].sort_values()\n\nprint(\"Most positive correlations:\\n\",correlations.tail(15))\nprint(\"Most negative correlations:\\n\",correlations.head(15))","573328b8":"app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","746b215d":"plt.style.use('fivethirtyeight')\n\nplt.hist(app_train['DAYS_BIRTH']\/365,edgecolor='k',bins=25)\nplt.title('Age of Client');plt.xlabel('AGE(years)');plt.ylabel('Count');","ab8e3cf0":"plt.figure(figsize=(10,8))\n\n# KDE plot of loans that were \n\nsns.kdeplot(app_train.loc[app_train['TARGET']==0,'DAYS_BIRTH']\/365, label='target==0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET']==1,'DAYS_BIRTH']\/365,\n           label='target==1')\n\nplt.xlabel('Age (years)');plt.ylabel('Density');plt.title('Distribution of Ages');","1ea88f23":"age_data = app_train[['TARGET','DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH']\/365\n\n#Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'],\n                                 bins = np.linspace(20,70,num=11))\nage_data.head(20)","6e3c53c1":"# group by the bin and calculat averages\nage_groups = age_data.groupby('YEARS_BINNED').mean()\nage_groups","296ba17c":"plt.figure(figsize=(8,8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str),100*age_groups['TARGET'])\n\nplt.xticks(rotation=75);plt.xlabel('Age Group (years)'); plt.ylabel('Failure to replay (%)')\nplt.title('Failure to Repay by age group')\n","0de6589b":"# most negatively correlated\next_data = app_train[['TARGET','EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3',\n                     'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","f034dc16":"plt.figure(figsize=(8,6))\n\n#heat map of correlations\nsns.heatmap(ext_data_corrs,cmap=plt.cm.RdYlBu_r,vmin=-0.25,annot=True,\n           vmax=0.6,)\nplt.title('Correlation Heatmap');","3b91afa5":"plt.figure(figsize=(10,12))\n\nfor i,source in enumerate(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']):\n    #create a new plot for each source\n    plt.subplot(3,1,i+1)\n    \n    #plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET']==0,source],label='target==0')\n    \n    #plot loans not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET']==1,source],label='target==1')\n    \n    #label the plot\n    plt.title('Distribution of %s by Target Value'%source)\n    plt.xlabel('%s'%source);plt.ylabel('Density');\nplt.tight_layout(h_pad=2.5)","5864ad6a":"# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","1844f79e":"\"r={:.2f}\".format(0.023434)","f5613dce":"# Make a new dataframe for polynomila features\npoly_features = app_train[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_2',\n                          'DAYS_BIRTH','TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_2',\n                          'DAYS_BIRTH']]\n\n#imputer for handling missing values\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy='median')\n\npoly_target = poly_features['TARGET']\npoly_features = poly_features.drop(columns=['TARGET'])\n\n#need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.fit_transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly_transformer = PolynomialFeatures(degree=3)","a3393893":"#Train the polynomial features\n#poly_transformer.fit(poly_features)\n\n#Transform the features\npoly_features = poly_transformer.fit_transform(poly_features)\npoly_features_test = poly_transformer.fit_transform(poly_features_test)\nprint(\"Plynomial Features shape: \",poly_features.shape)","60d2268a":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1','EXT_SOURCE_2',\n                                                    'EXT_SOURCE_3','DAYS_BIRTH'])[:15]","6e09fbb7":"poly_features = pd.DataFrame(poly_features,columns = poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']))\n#Add in the target\npoly_features['TARGET'] = poly_target\n\n#find the correlations with the target\n\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n#Display most negative and most positive\nprint(poly_corrs.head(10))\nprint()\nprint(poly_corrs.tail(5))\n","d237e755":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","46dc94c7":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT']\/app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY']\/app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY']\/app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED']\/app_train_domain['DAYS_BIRTH']","1fad61de":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] \/ app_test_domain['DAYS_BIRTH']","f1380844":"plt.figure(figsize=(12,20))\n#iterate through the new features\nfor i,feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n    plt.subplot(4,1,i+1)\n    \n    #plot repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET']==0,feature],\n               label='target==0')\n    #plot loans that were not paid\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET']==1,feature],\n               label='target==1')\n    #label the plots\n    plt.title('Distribution of %s by Target Value'%feature)\n    plt.xlabel('%s'%feature);plt.ylabel('Density')\nplt.tight_layout(h_pad=2.5)","08fc8375":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns=['TARGET'])\nelse:\n    train = app_train.copy()\n    \n#feature names\nfeatures = list(train.columns)\n\n#copy of the testing data\ntest = app_test.copy()\n\nimputer = Imputer(strategy='median')\n\nscaler = MinMaxScaler(feature_range=(0,1))\n\nimputer.fit(train)\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n#Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)\n","5a346544":"a = np.array([[1,2],[4,5]])\nb = np.array([[6,7],[8,9]])","b43ecd6e":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(C=0.0001)\n\nlog_reg.fit(train,train_labels)","8e68943e":"log_reg_pred = log_reg.predict_proba(test)[:,1]","927ce87f":"submit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","571ad535":"submit.to_csv('log_reg_baseline.csv', index = False)\n","95eb8d78":"# Label Encoding and One-Hot Encoding","8d7c10aa":"# Effect of Age on Repayment","29461d45":"# Examine Missing Values"}}