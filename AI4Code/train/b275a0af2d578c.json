{"cell_type":{"340b81ca":"code","ab37f7f0":"code","f96c51d3":"code","5d275683":"code","1658700e":"code","563145f2":"code","2fa80afb":"code","6e72b373":"code","fa717312":"code","ac46c4d3":"code","2f2b3eaf":"code","f4695f85":"code","afe2dfe6":"code","9fc173f6":"code","ece9d73f":"code","c04410ea":"code","102748f2":"code","4b14588e":"code","88633330":"code","a2df4195":"code","e980be05":"code","e35cee1c":"code","63a1472e":"code","6d2a3fb7":"code","b166548f":"code","8216f114":"code","87e915ef":"code","ea07aee3":"code","d6b55f42":"code","87aa2695":"code","61346c87":"code","70f99eaf":"code","29038ba3":"code","e46bbe9e":"code","e175bb58":"code","1950e06e":"code","79d778c3":"markdown","af5bed25":"markdown","618b57c1":"markdown","12d32510":"markdown","8f66ccb5":"markdown","0e238bd8":"markdown","ca5d2d65":"markdown","3c579f2d":"markdown","b58c33b3":"markdown","62d1b3d7":"markdown","18b7a9d2":"markdown","73e5688f":"markdown"},"source":{"340b81ca":"import pandas as pd\nimport numpy as np\nimport matplotlib.dates as md\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import host_subplot\nimport mpl_toolkits.axisartist as AA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.covariance import EllipticEnvelope\n\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM\nfrom mpl_toolkits.mplot3d import Axes3D\n\n%matplotlib inline","ab37f7f0":"from load_kaggle import load_kaggle","f96c51d3":"subm, train, test = load_kaggle()","5d275683":"test.columns","1658700e":"!pip install featurewiz --upgrade \n#!python3 -m pip install git+https:\/\/github.com\/AutoViML\/featurewiz.git","563145f2":"from featurewiz import FE_kmeans_resampler, FE_find_and_cap_outliers, EDA_find_outliers\nfrom featurewiz import FE_convert_all_object_columns_to_numeric, split_data_n_ways, FE_create_categorical_feature_crosses\nfrom featurewiz import FE_create_time_series_features, FE_concatenate_multiple_columns\nfrom featurewiz import simple_XGBoost_model\nimport featurewiz as FW","2fa80afb":"target = 'target'\nidcols = ['id']","6e72b373":"### In regression problems, you might want to convert target into log(target)\n#df[target] = (df[target] - np.mean(df[target]))\/np.std(df[target])\n#train[target] = np.log(train[target].values)","fa717312":"### you can manually set features you want included in dataset here or automatically\nfeatures = test.columns.tolist()","ac46c4d3":"pd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.max_columns', 500)","2f2b3eaf":"df = train[features+[target]]\ntrain = train[features+[target]]\nprint(train.shape)\ntrain.head(1)","f4695f85":"test = test[features]\nprint(test.shape)\ntest.head(1)","afe2dfe6":"nums = [x for x in df.select_dtypes(include='float').columns.tolist() if x not in [target]]\nnums","9fc173f6":"cats = [x for x in df.select_dtypes(include='object').columns.tolist() if x not in [target]]\ncats","ece9d73f":"#!pip install autoviz","c04410ea":"#from autoviz.AutoViz_Class import AutoViz_Class\n#AV = AutoViz_Class()\n#filename = \"\"\n#sep = \",\"\n#dft = AV.AutoViz(\n#    filename,\n#    sep=\",\",\n#    depVar=target,\n#    dfte=train,\n#    header=0,\n#    verbose=0,\n#    lowess=False,\n#    chart_format=\"svg\",\n#    max_rows_analyzed=50000,\n#    max_cols_analyzed=30,\n#)","102748f2":"### You can plot to see how this numeric column would be good for binning\n#df['cont2'].hist(bins=30);","4b14588e":"## This dictionary tells Featurewiz how many bins for each feature\nbin_dict = {'cont1':4, 'cont2':9, 'cont3':3, 'cont4':3, 'cont6':3,\n            'cont9':4, 'cont10':2, 'cont11':3, \n            'cont12':9, 'cont13':4, 'cont14':2}","88633330":"train, test = FW.FE_discretize_numeric_variables(train, bin_dict, test)","a2df4195":"binned_cols = [x for x in train.columns if x.endswith('_discrete')]\nlen(binned_cols)","e980be05":"train_copy = train.copy(deep=True)\ntest_copy = test.copy(deep=True)","e35cee1c":"### This handy function subtracts one list from another\ndef left_subtract(l1,l2):\n    lst = []\n    for i in l1:\n        if i not in l2:\n            lst.append(i)\n    return lst","63a1472e":"col_dict = {'cont12':'log', 'cont8':'log'}\ntrain_copy = FW.FE_transform_numeric_columns(train_copy, col_dict)\ntest_copy = FW.FE_transform_numeric_columns(test_copy, col_dict)\ntrain_copy.head(2)","6d2a3fb7":"train_copy, test_copy = FW.FE_add_groupby_features_aggregated_to_dataframe(train, agg_types=['mean', 'std'],\n                                    groupby_columns=binned_cols,\n                                    ignore_variables=idcols+[target], test=test)","b166548f":"print(train_copy.shape)\ntrain_copy.head(1)","8216f114":"print(test_copy.shape)\ntest_copy.head(1)","87e915ef":"### Take a look at whether it makes sense to cap outliers ###\n_ = FE_find_and_cap_outliers(train,[target],verbose=1)\n#test = FE_find_and_cap_outliers(test,nums,verbose=0)","ea07aee3":"train_copy = train_copy[~(train[target]<5)]\n#train_copy[target].hist(bins=30);","d6b55f42":"train_best, test_best = FW.featurewiz(train_copy, \n                        target, test_data=test_copy)","87aa2695":"preds = test_best.columns.tolist()\npreds","61346c87":"preds = preds[1:]","70f99eaf":"train_best = train_copy[preds+[target]]\ntest_best = test_copy[preds]\nprint(train_best.shape, test_best.shape)","29038ba3":"y_preds = simple_XGBoost_model(X_XGB=train_best[preds], Y_XGB=train_best[target],\n                               X_XGB_test=test_best[preds], modeltype='Regression')","e46bbe9e":"#subm = test[idcols]\n#subm = pd.DataFrame()\nsubm[target] = y_preds\nsubm.head()","e175bb58":" #!pip3 install --upgrade Pillow","1950e06e":"subm.to_csv(target+'_Regression_submission.csv',index=False)","79d778c3":"## This simple XGBoost model works wonders since it is highly effective in many competitions","af5bed25":"# Remember to install featurewiz first","618b57c1":"## This Featurewiz+SimpleXGBModel Template has Received Top Scores in Multiple Hackathons - Current Ranks in the following Hackathons are:\nAnalyticsVidhya Hackathons: https:\/\/datahack.analyticsvidhya.com\/contest\/all\/\n1.  Big_Mart Sales Prediction Score: 1147  -- Rank 250 out of 41,361 = That's a Top <1% Rank!!\n1.  Loan Status Predictions Score 0.791  -- Rank 850 out of 67,424 - Top 1.25% Rank\n\nMachine Hack Hackathons: https:\/\/www.machinehack.com\/hackathon\n1.  Machine Hack Flight Ticket Score 0.9389 -- Rank 165 out of 2723 - Top 6% Rank!\n1.  Machine Hack Data Scientist Salary class Score 0.417 -- Rank 58 out of 1547 - Top 3.7% Rank! (Autoviml Score was 0.329 -- less than 0.417 of Featurewiz+Simple even though an NLP problem!)\n1.  MCHACK Book Price NLP Score 0.7336 -- Rank 104 Autoviml NLP problem and should have done better\n","12d32510":"## In some cases it makes sense to drop training rows where target is less than a certain number - so it removes outliers","8f66ccb5":"## Add groupby features based on binned columns above","0e238bd8":"## You can do some binning of numeric columns here using Featurewiz","ca5d2d65":"# You can use AutoViz to first get some insights and do additional feature selection or engineering","3c579f2d":"# Now find outliers and cap them to their second highest values","b58c33b3":"## These are what I learned from AutoViz Plots on the Tabular Playground Jan 2021 dataset:\nCreate new categorical variables based on:\ncont2 into 9 clusters\ncont3 into 2 clusters\ncont14 into 2 clusters\ncont10 into 2 clusters\ncont4 into 3 clusters\n\nCreate new interaction variables based on:\ncont3, cont14\ncont6, cont7\ncont6, cont9\ncont6, cont12\ncont11, cont12\n\n\nCreate new binning variables based on distributions:\ncont1 4 modes \ncont3 3 modes\ncont11 3 modes\ncont13 might need 4 binning\ncont9 might need 4 binning\ncont 12 might need 9 bins\ncont4 might need 3 bins\n\nIn-situ transform these variables:\ncont12 needs log transformation\ncont8 needs log transformation","62d1b3d7":"# Now let us select the best variables using featurewiz","18b7a9d2":"## Remember you can transform highly skewed variables using log_transform","73e5688f":"## Now create some groupby aggregates based on the binned variables"}}