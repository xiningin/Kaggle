{"cell_type":{"b502b7e3":"code","6333d184":"code","70fbc655":"code","a7e8fc7d":"code","fa85c894":"code","36706834":"code","653bc4e2":"code","9f45670f":"code","772b0322":"code","7a94c236":"code","0288037c":"code","1824098b":"code","2297c4ae":"code","75d89b3d":"code","4a7245cb":"code","2708d4e0":"code","8a06284b":"code","b5e2c06c":"code","03fd75c1":"code","a0d56515":"code","cbfd7ae1":"code","a521e2a2":"code","fd00c6d0":"code","5381a0df":"code","f0060bbd":"code","bff6e33a":"code","2afd4023":"code","a4366a01":"code","3cf923d4":"code","5f61f7ba":"code","3b990df3":"code","c546b332":"code","695445dc":"code","0938d420":"code","7da1a978":"code","53d6fc0a":"code","41b2e834":"code","f6c0b29b":"code","b49c73c5":"code","d68795f6":"code","d88e1b9c":"code","c61df498":"code","4aa139f3":"code","f649bf0c":"code","76e61eec":"code","a6febb21":"code","09e5bbf9":"code","a22d1bd1":"code","37fdd764":"code","7beff866":"code","41192597":"code","97f760c0":"code","f338dae9":"code","b931e70f":"code","3a1a9e39":"code","28a6287b":"code","8b0b122f":"code","e453c996":"code","322c70b5":"code","92253578":"code","2fcae00b":"code","220dcc29":"code","0f8065ec":"code","b66f4148":"code","70d2e1fc":"code","1f1eaa81":"code","c9f8ba40":"markdown","602afc1d":"markdown","c8860cf5":"markdown"},"source":{"b502b7e3":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\nimport time\nimport pickle\nimport re\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\ntqdm.pandas()","6333d184":"# We use Pkl as it takes less time to load\n\nCRAWL_EMBEDDING_PATH = '..\/input\/pickled-crawl300d2m-for-kernel-competitions\/crawl-300d-2M.pkl'\nGLOVE_EMBEDDING_PATH = '..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl'","70fbc655":"train = pd.read_csv('..\/input\/innoplexus-online-hiring-hackathon\/train_F3WbcTw.csv',low_memory=True)\ntest = pd.read_csv('..\/input\/innoplexus-online-hiring-hackathon\/test_tOlRoBf.csv',low_memory=True)","a7e8fc7d":"train.shape","fa85c894":"train.head()","36706834":"# Lets use the other column\n\ntrain['text'] = train['text'] + \"This observation is for \" + train['drug']\ntest['text'] = test['text'] + \"This observation is for \" + test['drug']","653bc4e2":"# You can use this pre-processing steps at later stage\n\nimport re\ndef pre_process(text):\n    new_text =re.sub('[0-9]', '', text)\n    new_text = re.sub(r\"\\u200b\",\"\",new_text)\n    new_text = re.sub(r\"\\.+\",\".\",new_text)\n    new_text = re.sub(r'(https|http)?:\\\/\\\/(\\w|\\.|\\\/|\\?|\\=|\\&|\\%)*\\b', '',new_text, flags=re.MULTILINE)\n    new_text = re.sub(\"'\", \"\", new_text)\n    new_text = re.sub(r'\u2191', '', new_text)\n    new_text = re.sub(\"\\t\", \"\", new_text)\n    new_text = re.sub(\"\\xa0\", \"\", new_text)\n    new_text = re.sub(\"\\(|\\)|\\[|\\]\", \"\", new_text)\n    new_text = re.sub(\"\\n\", \"\", new_text)\n    new_text = re.sub(\"\\.\", \"\", new_text)\n    new_text = re.sub(\"\\,\", \" \", new_text)\n    new_text = re.sub(\"[\/%]\", \" \", new_text)\n    new_text = re.sub('[\/%:;]', '', new_text)\n    new_text = re.sub(' +', ' ', new_text)\n    return new_text","9f45670f":"# remove URL's from train and test\n#for index, row in train['text'].iteritems():\n#    train['text'][index] = pre_process(row)","772b0322":"#for index, row in test['text'].iteritems():\n#    test['text'][index] = pre_process(row)","7a94c236":"# remove URL's from train and test\ntrain['text'] = train['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\ntest['text'] = test['text'].apply(lambda x: re.sub(r'http\\S+', '', x))","0288037c":"# remove numbers\ntrain['text'] = train['text'].str.replace(\"[0-9]\", \" \")\ntest['text'] = test['text'].str.replace(\"[0-9]\", \" \")","1824098b":"# Adjusting the load_embeddings function, to now handle the pickled dict.\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words","2297c4ae":"#Below Functions are to check the embedding\u2019s coverage and building vocabulary \nimport operator \n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x\n\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","75d89b3d":"# Lets load the embeddings \ntic = time.time()\nglove_embeddings = load_embeddings(GLOVE_EMBEDDING_PATH)\nprint(f'loaded {len(glove_embeddings)} word vectors in {time.time()-tic}s')","4a7245cb":"# Lets check how many words we got covered \nvocab = build_vocab(list(train['text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,glove_embeddings)\noov[:20]","2708d4e0":"# Lets replace few words which is not covered in our embedding\u2019s , i have build this over time.\n\nreplaceWords1 = { \"won't\":\"will not\",\"$&@*#\":\"in most profane vulgar shitty terms\",\"#$&@*#\":\"shitty\",\n \"can't\":\"cannot\",\"aren't\": 'are not',\n \"Aren't\": 'Are not',\n \"AREN'T\": 'ARE NOT',\n \"C'est\": \"C'est\",\n \"C'mon\": \"C'mon\",\n \"c'mon\": \"c'mon\",\n \"can't\": 'cannot',\n \"Can't\": 'Cannot',\n \"CAN'T\": 'CANNOT',\n \"con't\": 'continued',\n \"cont'd\": 'continued',\n \"could've\": 'could have',\n \"couldn't\": 'could not',\n \"Couldn't\": 'Could not',\n \"didn't\": 'did not',\n \"Didn't\": 'Did not',\n \"DIDN'T\": 'DID NOT',\n \"don't\": 'do not',\n \"Don't\": 'Do not',\n \"DON'T\": 'DO NOT',\n \"doesn't\": 'does not',\n \"Doesn't\": 'Does not',\n \"else's\": 'else',\n \"gov's\": 'government',\n \"Gov's\": 'government',\n \"gov't\": 'government',\n \"Gov't\": 'government',\n \"govt's\": 'government',\n \"gov'ts\": 'governments',\n \"hadn't\": 'had not',\n \"hasn't\": 'has not',\n \"Hasn't\": 'Has not',\n \"haven't\": 'have not',\n \"Haven't\": 'Have not',\n \"he's\": 'he is',\n \"He's\": 'He is',\n \"he'll\": 'he will',\n \"He'll\": 'He will',\n \"he'd\": 'he would',\n \"He'd\": 'He would',\n \"Here's\": 'Here is',\n \"here's\": 'here is',\n \"I'm\": 'I am',\n \"i'm\": 'i am',\n \"I'M\": 'I am',\n \"I've\": 'I have',\n \"i've\": 'i have',\n \"I'll\": 'I will',\n \"i'll\": 'i will',\n \"I'd\": 'I would',\n \"i'd\": 'i would',\n \"ain't\": 'is not',\n \"isn't\": 'is not',\n \"Isn't\": 'Is not',\n \"ISN'T\": 'IS NOT',\n \"it's\": 'it is',\n \"It's\": 'It is',\n \"IT'S\": 'IT IS',\n \"I's\": 'It is',\n \"i's\": 'it is',\n \"it'll\": 'it will',\n \"It'll\": 'It will',\n \"it'd\": 'it would',\n \"It'd\": 'It would',\n \"Let's\": \"Let's\",\n \"let's\": 'let us',\n \"ma'am\": 'madam',\n \"Ma'am\": \"Madam\",\n \"she's\": 'she is',\n \"She's\": 'She is',\n \"she'll\": 'she will',\n \"She'll\": 'She will',\n \"she'd\": 'she would',\n \"She'd\": 'She would',\n \"shouldn't\": 'should not',\n \"that's\": 'that is',\n \"That's\": 'That is',\n \"THAT'S\": 'THAT IS',\n \"THAT's\": 'THAT IS',\n \"that'll\": 'that will',\n \"That'll\": 'That will',\n \"there's\": 'there is',\n \"There's\": 'There is',\n \"there'll\": 'there will',\n \"There'll\": 'There will',\n \"there'd\": 'there would',\n \"they're\": 'they are',\n \"They're\": 'They are',\n \"they've\": 'they have',\n \"They've\": 'They Have',\n \"they'll\": 'they will',\n \"They'll\": 'They will',\n \"they'd\": 'they would',\n \"They'd\": 'They would',\n \"wasn't\": 'was not',\n \"we're\": 'we are',\n \"We're\": 'We are',\n \"we've\": 'we have',\n \"We've\": 'We have',\n \"we'll\": 'we will',\n \"We'll\": 'We will',\n \"we'd\": 'we would',\n \"We'd\": 'We would',\n \"What'll\": 'What will',\n \"weren't\": 'were not',\n \"Weren't\": 'Were not',\n \"what's\": 'what is',\n \"What's\": 'What is',\n \"When's\": 'When is',\n \"Where's\": 'Where is',\n \"where's\": 'where is',\n \"Where'd\": 'Where would',\n \"who're\": 'who are',\n \"who've\": 'who have',\n \"who's\": 'who is',\n \"Who's\": 'Who is',\n \"who'll\": 'who will',\n \"who'd\": 'Who would',\n \"Who'd\": 'Who would',\n \"won't\": 'will not',\n \"Won't\": 'will not',\n \"WON'T\": 'WILL NOT',\n \"would've\": 'would have',\n \"wouldn't\": 'would not',\n \"Wouldn't\": 'Would not',\n \"would't\": 'would not',\n \"Would't\": 'Would not',\n \"y'all\": 'you all',\n \"Y'all\": 'You all',\n \"you're\": 'you are',\n \"You're\": 'You are',\n \"YOU'RE\": 'YOU ARE',\n \"you've\": 'you have',\n \"You've\": 'You have',\n \"y'know\": 'you know',\n \"Y'know\": 'You know',\n \"ya'll\": 'you will',\n \"you'll\": 'you will',\n \"You'll\": 'You will',\n \"you'd\": 'you would',\n \"You'd\": 'You would',\n \"Y'got\": 'You got',\n 'cause': 'because',\n \"had'nt\": 'had not',\n \"Had'nt\": 'Had not',\n \"how'd\": 'how did',\n \"how'd'y\": 'how do you',\n \"how'll\": 'how will',\n \"how's\": 'how is',\n \"I'd've\": 'I would have',\n \"I'll've\": 'I will have',\n \"i'd've\": 'i would have',\n \"i'll've\": 'i will have',\n \"it'd've\": 'it would have',\n \"it'll've\": 'it will have',\n \"mayn't\": 'may not',\n \"might've\": 'might have',\n \"mightn't\": 'might not',\n \"mightn't've\": 'might not have',\n \"must've\": 'must have',\n \"mustn't\": 'must not',\n \"mustn't've\": 'must not have',\n \"needn't\": 'need not',\n \"needn't've\": 'need not have',\n \"o'clock\": 'of the clock',\n \"oughtn't\": 'ought not',\n \"oughtn't've\": 'ought not have',\n \"shan't\": 'shall not',\n \"sha'n't\": 'shall not',\n \"shan't've\": 'shall not have',\n \"she'd've\": 'she would have',\n \"she'll've\": 'she will have',\n \"should've\": 'should have',\n \"shouldn't've\": 'should not have',\n \"so've\": 'so have',\n \"so's\": 'so as',\n \"this's\": 'this is',\n \"that'd\": 'that would',\n \"that'd've\": 'that would have',\n \"there'd've\": 'there would have',\n \"they'd've\": 'they would have',\n \"they'll've\": 'they will have',\n \"to've\": 'to have',\n \"we'd've\": 'we would have',\n \"we'll've\": 'we will have',\n \"what'll\": 'what will',\n \"what'll've\": 'what will have',\n \"what're\": 'what are',\n \"what've\": 'what have',\n \"when's\": 'when is',\n \"when've\": 'when have',\n \"where'd\": 'where did',\n \"where've\": 'where have',\n \"who'll've\": 'who will have',\n \"why's\": 'why is',\n \"why've\": 'why have',\n \"will've\": 'will have',\n \"won't've\": 'will not have',\n \"wouldn't've\": 'would not have',\n \"y'all'd\": 'you all would',\n \"y'all'd've\": 'you all would have',\n \"y'all're\": 'you all are',\n \"y'all've\": 'you all have',\n \"you'd've\": 'you would have',\n \"you'll've\": 'you will have',\n'bebecause':'be because',\n'I\u2019m':'I am',\n              'it\u2019s':'it is',\n                 'I\u2019ve':'I have',\n                 'don\u2019t':'do not',\n                'However':'but',\n                 'It\u2019s':'It is',\n                 'didn\u2019t':'did not',\n                 'can\u2019t':'can not',\n                 'that\u2019s':'that is',\n'doesn\u2019t':'does not',\n'I\u2019d':'I had',\n'isn\u2019t':'is not',\n'wasn\u2019t':'was not'\n                \n                }\n\ndef wordreplace(tweet,replaceWords):\n    for key in replaceWords:\n        tweet = tweet.replace(key,replaceWords[key])\n    return tweet\n\nfor index, row in train['text'].iteritems():\n    train['text'][index] = wordreplace(row,replaceWords1)\n    \nfor index, row in test['text'].iteritems():\n    test['text'][index] = wordreplace(row,replaceWords1)","8a06284b":"# Now lets check if we have improved on our coverage \nvocab = build_vocab(list(train['text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,glove_embeddings)\noov[:20]","b5e2c06c":"import string\nlatin_similar = \"\u2019'\u2018\u00c6\u00d0\u018e\u018f\u0190\u0194\u0132\u014a\u0152\u1e9e\u00de\u01f7\u021c\u00e6\u00f0\u01dd\u0259\u025b\u0263\u0133\u014b\u0153\u0138\u017f\u00df\u00fe\u01bf\u021d\u0104\u0181\u00c7\u0110\u018a\u0118\u0126\u012e\u0198\u0141\u00d8\u01a0\u015e\u0218\u0162\u021a\u0166\u0172\u01afY\u0328\u01b3\u0105\u0253\u00e7\u0111\u0257\u0119\u0127\u012f\u0199\u0142\u00f8\u01a1\u015f\u0219\u0163\u021b\u0167\u0173\u01b0y\u0328\u01b4\u00c1\u00c0\u00c2\u00c4\u01cd\u0102\u0100\u00c3\u00c5\u01fa\u0104\u00c6\u01fc\u01e2\u0181\u0106\u010a\u0108\u010c\u00c7\u010e\u1e0c\u0110\u018a\u00d0\u00c9\u00c8\u0116\u00ca\u00cb\u011a\u0114\u0112\u0118\u1eb8\u018e\u018f\u0190\u0120\u011c\u01e6\u011e\u0122\u0194\u00e1\u00e0\u00e2\u00e4\u01ce\u0103\u0101\u00e3\u00e5\u01fb\u0105\u00e6\u01fd\u01e3\u0253\u0107\u010b\u0109\u010d\u00e7\u010f\u1e0d\u0111\u0257\u00f0\u00e9\u00e8\u0117\u00ea\u00eb\u011b\u0115\u0113\u0119\u1eb9\u01dd\u0259\u025b\u0121\u011d\u01e7\u011f\u0123\u0263\u0124\u1e24\u0126I\u00cd\u00cc\u0130\u00ce\u00cf\u01cf\u012c\u012a\u0128\u012e\u1eca\u0132\u0134\u0136\u0198\u0139\u013b\u0141\u013d\u013f\u02bcN\u0143N\u0308\u0147\u00d1\u0145\u014a\u00d3\u00d2\u00d4\u00d6\u01d1\u014e\u014c\u00d5\u0150\u1ecc\u00d8\u01fe\u01a0\u0152\u0125\u1e25\u0127\u0131\u00ed\u00eci\u00ee\u00ef\u01d0\u012d\u012b\u0129\u012f\u1ecb\u0133\u0135\u0137\u0199\u0138\u013a\u013c\u0142\u013e\u0140\u0149\u0144n\u0308\u0148\u00f1\u0146\u014b\u00f3\u00f2\u00f4\u00f6\u01d2\u014f\u014d\u00f5\u0151\u1ecd\u00f8\u01ff\u01a1\u0153\u0154\u0158\u0156\u015a\u015c\u0160\u015e\u0218\u1e62\u1e9e\u0164\u0162\u1e6c\u0166\u00de\u00da\u00d9\u00db\u00dc\u01d3\u016c\u016a\u0168\u0170\u016e\u0172\u1ee4\u01af\u1e82\u1e80\u0174\u1e84\u01f7\u00dd\u1ef2\u0176\u0178\u0232\u1ef8\u01b3\u0179\u017b\u017d\u1e92\u0155\u0159\u0157\u017f\u015b\u015d\u0161\u015f\u0219\u1e63\u00df\u0165\u0163\u1e6d\u0167\u00fe\u00fa\u00f9\u00fb\u00fc\u01d4\u016d\u016b\u0169\u0171\u016f\u0173\u1ee5\u01b0\u1e83\u1e81\u0175\u1e85\u01bf\u00fd\u1ef3\u0177\u00ff\u0233\u1ef9\u01b4\u017a\u017c\u017e\u1e93\"\nwhite_list = string.ascii_letters + string.digits + latin_similar + ' '\nwhite_list += \"'\"","03fd75c1":"glove_chars = ''.join([c for c in tqdm(glove_embeddings) if len(c) == 1])\nglove_symbols = ''.join([c for c in glove_chars if not c in white_list])\nglove_symbols","a0d56515":"# Chars available in the embedding\u2019s\njigsaw_chars = build_vocab(list(train[\"text\"]))\njigsaw_symbols = ''.join([c for c in jigsaw_chars if not c in white_list])\njigsaw_symbols","cbfd7ae1":"# Basically we can delete all symbols we have no embeddings for:\nsymbols_to_delete = ''.join([c for c in jigsaw_symbols if not c in glove_symbols])\nsymbols_to_delete","a521e2a2":"# The symbols we want to keep we need to isolate from our words. So lets setup a list of those to isolate.\nsymbols_to_isolate = ''.join([c for c in jigsaw_symbols if c in glove_symbols])\nsymbols_to_isolate","fd00c6d0":"# Note : Next comes the next trick. Instead of using an inefficient loop of replace we use translate. \n# I find the syntax a bit weird, but the improvement in speed is worth the worse readablity. \n\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\n\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x","5381a0df":"#So lets apply that function to our text and reasses the coverage\n\ntrain['text'] = train['text'].progress_apply(lambda x:handle_punctuation(x))\ntest['text'] = test['text'].progress_apply(lambda x:handle_punctuation(x))","f0060bbd":"vocab = build_vocab(list(train['text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,glove_embeddings)\noov[:20]","bff6e33a":"# Lets apply pre-processing before we tokenize the words\n\nfor index, row in train['text'].iteritems():\n    train['text'][index] = pre_process(row)\nfor index, row in test['text'].iteritems():\n    test['text'][index] = pre_process(row)","2afd4023":"from nltk.tokenize.treebank import TreebankWordTokenizer\ntokenizer = TreebankWordTokenizer()","a4366a01":"def handle_contractions(x):\n    x = tokenizer.tokenize(x)\n    x = ' '.join(x)\n    return x","3cf923d4":"train['text'] = train['text'].progress_apply(lambda x:handle_contractions(x))\ntest['text'] = test['text'].progress_apply(lambda x:handle_contractions(x))","5f61f7ba":"# Lets check after we have tokenize\n\nvocab = build_vocab(list(train['text'].apply(lambda x:x.split())),verbose=False)\noov = check_coverage(vocab,glove_embeddings)\noov[:20]","3b990df3":"def fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x","c546b332":"train['text'] = train['text'].progress_apply(lambda x:fix_quote(x.split()))\ntest['text'] = test['text'].progress_apply(lambda x:fix_quote(x.split()))","695445dc":"vocab = build_vocab(list(train['text'].apply(lambda x:x.split())),verbose=False)\noov = check_coverage(vocab,glove_embeddings)\noov[:10]","0938d420":"# Lets also check test data has equal coverage\nvocab = build_vocab(list(test['text'].apply(lambda x:x.split())),verbose=False)\noov = check_coverage(vocab,glove_embeddings)\noov[:10]","7da1a978":"tic = time.time()\ncrawl_embeddings = load_embeddings(CRAWL_EMBEDDING_PATH)\nprint(f'loaded {len(glove_embeddings)} word vectors in {time.time()-tic}s')","53d6fc0a":"vocab = build_vocab(list(train['text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,crawl_embeddings)\noov[:20]","41b2e834":"punctuation = '_`'\n\ntrain['text'] = train['text'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))\ntest['text'] = test['text'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))","f6c0b29b":"#Lets check the embeddings now\n\nvocab = build_vocab(list(train['text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,crawl_embeddings)\noov[:10]","b49c73c5":"X = train['text']\ny = train['sentiment']\ntest_pred = test['text']","d68795f6":"NUM_MODELS = 2\nLSTM_UNITS = 250\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nMAX_LEN = 300\nmax_features = 500000\n\nBATCH_SIZE = 90\nEPOCHS = 2","d88e1b9c":"# Its really important that you intitialize the keras tokenizer correctly. Per default it does lower case and removes a lot of symbols. We want neither of that!\n\ntokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)","c61df498":"tokenizer.fit_on_texts(list(X) + list(test_pred))","4aa139f3":"crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))\n\nmax_features = max_features or len(tokenizer.word_index) + 1\nmax_features\n\nembedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix.shape\n\nimport gc\ndel crawl_matrix\ndel glove_matrix\ngc.collect()","f649bf0c":"X = tokenizer.texts_to_sequences(X)\ntest_pred = tokenizer.texts_to_sequences(test_pred)","76e61eec":"X = sequence.pad_sequences(X, maxlen=MAX_LEN)\ntest_pred = sequence.pad_sequences(test_pred, maxlen=MAX_LEN)","a6febb21":"checkpoint_predictions = []\nweights = []","09e5bbf9":"# Check F1 score\n\nfrom keras import backend as K\n\ndef recall_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\ndef precision_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","a22d1bd1":"from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate,Flatten,Lambda\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D,PReLU,LSTM\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.models import Sequential\nfrom keras.preprocessing import text, sequence\nfrom keras import regularizers\nimport keras\nimport tensorflow as tf\nimport keras.backend as K\nfrom sklearn.model_selection import train_test_split\nfrom keras.engine.topology import Layer\nimport tensorflow_hub as hub\nfrom keras.layers.normalization import BatchNormalization","37fdd764":"X_train , X_val, y_train  , y_val = train_test_split(X , \n                                                     y , \n                                                     stratify = y.values , \n                                                     train_size = 0.8,\n                                                     random_state = 100)","7beff866":"from keras.callbacks import EarlyStopping \nes = EarlyStopping(monitor='val_loss', mode ='min' ,verbose =1,patience=0.1)","41192597":"def build_model(embedding_matrix, num_aux_targets):\n    words = Input(shape=(MAX_LEN,),name = 'input')\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words) #Finds word embeddings for each word\n    x = SpatialDropout1D(0.3)(x) #This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x), \n        GlobalAveragePooling1D()(x),#layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input \n        #of variable length in the simplest way possible.\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)]) #This fixed-length output vector is piped through a fully-connected (Dense) layer with x hidden units.\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(3, activation='softmax')(hidden)\n    model = Model(inputs=words, outputs= result)\n    model.compile(loss='sparse_categorical_crossentropy',metrics = ['accuracy',f1_m], optimizer='adam')\n    \n    return model","97f760c0":"len(X_train[1])","f338dae9":"embedding_matrix.shape","b931e70f":"from sklearn.utils import class_weight\n\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(y_train),\n                                                 y_train)","3a1a9e39":"for model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix,1)\n    for global_epoch in range(EPOCHS):\n        model.fit(\n            X_train,\n            y_train,\n            validation_data = (X_val, y_val),\n            batch_size=BATCH_SIZE,\n            epochs=100,\n            verbose=2,\n            class_weight=class_weights,\n            callbacks=[\n                LearningRateScheduler(lambda epoch: 1e-3 * (0.4 ** global_epoch)),\n                es\n            ]\n        )\n        checkpoint_predictions.append(model.predict(test_pred))\n        weights.append(2 ** global_epoch)","28a6287b":"predictions_x = np.average(checkpoint_predictions, weights=weights, axis=0)","8b0b122f":"pred_avg =np.argmax(predictions_x,axis=1) ","e453c996":"pred_avg","322c70b5":"predictions=model.predict(test_pred)","92253578":"prediction_round = np.argmax(predictions,axis=1)","2fcae00b":"from sklearn.metrics import classification_report\n\ny_pred = model.predict(X_val, batch_size=64, verbose=1)\ny_pred_bool = np.argmax(y_pred, axis=1)\n\nprint(classification_report(y_val, y_pred_bool))","220dcc29":"sub = pd.read_csv(\"..\/input\/innoplexus-online-hiring-hackathon\/sample_submission_i5xnIZD.csv\")","0f8065ec":"sub.head()","b66f4148":"sub['sentiment'] = pred_avg","70d2e1fc":"sub.to_csv(\"Glove_Sub2.csv\",index=False)","1f1eaa81":"prob = pd.DataFrame(predictions)\n\nprob.to_csv(\"prob_glove.csv\",index=False)","c9f8ba40":"# Analytics vidhya Innoplexus Online Hiring Hackathon Sentiment Analysis\n\nAnalytics Vidya Innoplexus Online Hiring Hackathon Sentiment Analysis\n\n![](https:\/\/github.com\/rajat5ranjan\/AV-Innoplexus-Online-Hiring-Hackathon-Sentiment-Analysis\/raw\/master\/inn.png)\n\n# Problem Statement\n## Sentiment Analysis for drugs\/medicines\n\nNowadays the narrative of a brand is not only built and controlled by the company that owns the brand. For this reason, companies are constantly looking out across Blogs, Forums, and other social media platforms, etc for checking the sentiment for their various products and also competitor products to learn how their brand resonates in the market. This kind of analysis helps them as part of their post-launch market research. This is relevant for a lot of industries including pharma and their drugs.\n\n<b> The challenge is that the language used in this type of content is not strictly grammatically correct. Some use sarcasm. Others cover several topics with different sentiments in one post. Other users post comments and reply and thereby indicating his\/her sentiment around the topic. <\/b>\n\nSentiment can be clubbed into 3 major buckets - <b> Positive, Negative and Neutral Sentiments <\/b>\n\nYou are provided with data containing samples of text. This text can contain one or more drug mentions. Each row contains a unique combination of the text and the drug mention. Note that the same text can also have different sentiment for a different drug.\n\nGiven the text and drug name, the task is to predict the sentiment for texts contained in the test dataset. Given below is an example of text from the dataset:\n\n# Example:\n\nStelara is still fairly new to Crohn's treatment. This is why you might not get a lot of replies. I've done some research, but most of the \"time to work\" answers are from Psoriasis boards. For Psoriasis, it seems to be about 4-12 weeks to reach a strong therapeutic level. The good news is, Stelara seems to be getting rave reviews from Crohn's patients. It seems to be the best med to come along since Remicade. I hope you have good success with it. My daughter was diagnosed Feb. 19\/07, (13 yrs. old at the time of diagnosis), with Crohn's of the Terminal Illium. Has used Prednisone and Pentasa. Started Imuran (02\/09), had an abdominal abscess (12\/08). 2cm of Stricture. Started \u200bRemicade in Feb. 2014, along with 100mgs. of Imuran.\n\nFor Stelara the above text is positive while for Remicade the above text is negative.\n\n## Data Description\n\ntrain.csv Contains the labelled texts with sentiment values for a given drug\n\n<table>\n<thead>\n<tr>\n<th>Variable<\/th>\n<th>Definition<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>unique_hash<\/td>\n<td>Unique ID<\/td>\n<\/tr>\n<tr>\n<td>text<\/td>\n<td>text pertaining to the drugs<\/td>\n<\/tr>\n<tr>\n<td>drug<\/td>\n<td>drug name for which the sentiment is provided<\/td>\n<\/tr>\n<tr>\n<td>sentiment<\/td>\n<td>(Target) 0-positive, 1-negative, 2-neutral<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n\ntest.csv test.csv contains texts with drug names for which the participants are expected to predict the correct sentiment\n\n## Evaluation Metric\n\nThe metric used for evaluating the performance of the classification model would be macro F1-Score.\n\n# Public and Private Split\nThe texts in the test data are further randomly divided into Public (40%) and Private (60%) data. Your initial responses will be checked and scored on the Public data. The final rankings would be based on your private score which will be published once the competition is over.\n\nRefer Github link for other implementation like BERT and ELMO [Github](https:\/\/github.com\/buntys2010\/AV-Innoplexus-Online-Hiring-Hackathon-Sentiment-Analysis) ","602afc1d":"# Best way to handle special characters and emoji\u2019s in an embedding\n\nOne of the thing to note is that your embeddings might contains few special characters embeddings which if you delete then will lost information.\n\nSo lets try and check how many embedding\u2019s we have coverage","c8860cf5":"## Checking CRAWL embeddings Coverage"}}