{"cell_type":{"e8a47b61":"code","7b1eebc6":"code","4b5283e1":"code","42223c3a":"code","d703b5ef":"code","84165593":"code","90b7ea91":"code","ca0b3e60":"code","84a58834":"code","b0000e22":"code","e5476ac0":"code","3257aec3":"code","4e171c8f":"code","4344bb35":"code","1d398fc5":"code","7ae977ac":"code","332eeb6a":"code","34eeb908":"code","ac186578":"code","da404314":"code","3161c7ef":"code","c136d934":"code","8bf7c417":"code","88baed71":"code","fc8113ce":"code","ec1d0e16":"markdown","dc455097":"markdown","dfe6813b":"markdown","ae6bbd7d":"markdown","775435ad":"markdown","540825ea":"markdown","36ab0922":"markdown","8a33e7fa":"markdown","fe20207d":"markdown","f3581535":"markdown","82f3026e":"markdown","3cdbd369":"markdown","1c2bd1b2":"markdown","12aba954":"markdown","4863922b":"markdown","6be59967":"markdown","ff8ca06c":"markdown","2323ecac":"markdown","b541d869":"markdown","5ec49237":"markdown","b5f96b90":"markdown"},"source":{"e8a47b61":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nfrom statsmodels.formula.api import ols\npd.set_option('precision', 3)\nimport seaborn as sns","7b1eebc6":"wine = pd.read_csv(\"..\/input\/winequality-red.csv\")\nwine.head()","4b5283e1":"wine[pd.isnull(wine).any(axis=1)]","42223c3a":"# Dropping the null rows\nwine.dropna(axis=0,inplace=True)","d703b5ef":"wine.describe()","84165593":"wine.quality.value_counts()\n#plt.hist(wine.quality)","90b7ea91":"plt.figure(figsize=(10,5))\n\ncor = wine.corr()\nmask = np.zeros_like(cor, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(cor,mask=mask,annot=True)","ca0b3e60":"wine.groupby('quality').mean()","84a58834":"plt.figure(figsize=(25,35))\ni=1\nfor col in wine.columns[:-1] :\n    plt.subplot(4,3,i)\n    sns.boxplot(x='quality',y=col,data=wine)\n    i = i+1","b0000e22":"from sklearn.model_selection import train_test_split\nx = wine.drop('quality',axis=1)\ny = wine.quality","e5476ac0":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)\n\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler().fit(x_train)\nx_train_std = scale.transform(x_train)\nx_test_std = scale.transform(x_test)\n\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=0)\nx_train_std_os,y_train_os = sm.fit_sample(x_train_std,y_train)","3257aec3":"import mord\nquality = np.array([3,4,5,6,7,8])\n\nord_model_IT = mord.LogisticIT(alpha=0).fit(x_train_std,y_train.astype('int'))\ny_pred_IT = ord_model_IT.predict_proba(x_test_std)\nord_model_AT = mord.LogisticAT(alpha=0).fit(x_train_std,y_train.astype('int'))\ny_pred_AT = ord_model_AT.predict_proba(x_test_std)\n\norder_IT = np.argmax(y_pred_IT,axis=1)\npredicted_IT = np.zeros((order_IT.size))\nfor i in range(len(predicted_IT)):\n    predicted_IT[i] = order_IT[i]+3\n    \norder_AT = np.argmax(y_pred_AT,axis=1)\npredicted_AT = np.zeros((order_AT.size))\nfor i in range(len(predicted_AT)):\n    predicted_AT[i] = order_AT[i]+3","4e171c8f":"from sklearn.metrics import mean_absolute_error,mean_squared_error,precision_score,f1_score\n\ndf1 = pd.DataFrame(y_test)\ndf2 = pd.DataFrame(predicted_IT,columns=['predicted'])\n\nres_ord = pd.concat([df1.reset_index(),df2.reset_index()],axis=1).drop(['index'],axis=1)\nprint ('Macro precision = ',precision_score(res_ord.quality,res_ord.predicted,average='macro'))\nprint ('Micro precision = ',precision_score(res_ord.quality,res_ord.predicted,average='micro'))\n\nprint ('Macro f1 score = ',f1_score(res_ord.quality,res_ord.predicted,average='macro'))\nprint ('Micro f1_score = ',f1_score(res_ord.quality,res_ord.predicted,average='micro'))","4344bb35":"from sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(random_state=0).fit(x_train_std_os,y_train_os)\ny_pred_RF = rf_model.predict(x_test_std)","1d398fc5":"df1 = pd.DataFrame(y_test)\ndf2 = pd.DataFrame(y_pred_RF,columns=['predicted'])\n\nres_rf = pd.concat([df1.reset_index(),df2.reset_index()],axis=1).drop(['index'],axis=1)\nprint ('Macro precision = ',precision_score(res_rf.quality,res_rf.predicted,average='macro'))\nprint ('Micro precision = ',precision_score(res_rf.quality,res_rf.predicted,average='micro'))\n\nprint ('Macro f1 score = ',f1_score(res_rf.quality,res_rf.predicted,average='macro'))\nprint ('Micro f1_score = ',f1_score(res_rf.quality,res_rf.predicted,average='micro'))","7ae977ac":"good = y.apply(lambda x: int(x\/7))\ngood.value_counts()","332eeb6a":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,good,test_size=0.2,random_state=0)\n\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler().fit(x_train)\nx_train_std = scale.transform(x_train)\nx_test_std = scale.transform(x_test)\n\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=0)\nx_train_std_os,y_train_os = sm.fit_sample(x_train_std,y_train)","34eeb908":"# Logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=0).fit(x_train_std_os,y_train_os)\nlr_pred = lr.predict_proba(x_test_std)\n\n# Decision tree\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state=0).fit(x_train_std_os,y_train_os)\ntree_pred = tree.predict_proba(x_test_std)\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state=0).fit(x_train_std_os,y_train_os)\nrf_pred = rf.predict_proba(x_test_std)\n\n# KNN classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier().fit(x_train_std_os,y_train_os)\nknn_pred = knn.predict_proba(x_test_std)\n\n# SVC : Linear kernel\nfrom sklearn.svm import SVC\nsv_lin = SVC(kernel='linear',random_state=0,probability=True).fit(x_train_std_os,y_train_os)\nsv_lin_pred = sv_lin.predict_proba(x_test_std)\n\n# SVC : RBF kernel\nfrom sklearn.svm import SVC\nsv_rbf = SVC(kernel='rbf',random_state=0,probability=True).fit(x_train_std_os,y_train_os)\nsv_rbf_pred = sv_rbf.predict_proba(x_test_std)","ac186578":"from sklearn.metrics import precision_recall_curve,average_precision_score,auc,roc_auc_score\n\nmodels = ['LR','Decision Tree','Random Forest','KNN','Linear kernel SVM','RBF kernel SVM']\npreds = [lr_pred,tree_pred,rf_pred,knn_pred,sv_lin_pred,sv_rbf_pred]\nfor pred,model in zip(preds,models):\n    precision,recall,thresholds = precision_recall_curve(y_test,pred[:,1])\n    print ('Area under precision recall curve for %s model = '%(model),round(auc(recall,precision),3))\n    print ('Area under ROC curve for %s model = '%(model),round(roc_auc_score(y_test,pred[:,1]),3),'\\n') ","da404314":"from sklearn.model_selection import cross_val_score\nprint ('Train set AUC for Random Forest model : ',roc_auc_score(y_train,rf.predict_proba(x_train_std)[:,1]))\nprint ('Cross validation AUC for Random Forest model : ',np.mean(cross_val_score(rf,x_train_std,y_train,scoring='roc_auc',cv=10)))\nprint ('Test set AUC for Random Forest model : ',roc_auc_score(y_test,rf.predict_proba(x_test_std)[:,1]))","3161c7ef":"# We will first use RandomizedSearchCV to narrow down the sample space for GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nn_estimators = [i for i in range(100,1100,100)]\nmax_depth = [i for i in range(10,110,10)]\nmax_depth.append(None)\nmax_features = ['auto','sqrt']\n\nrandom_grid = {'n_estimators' : n_estimators,\n               'max_depth' : max_depth,\n               'max_features' : max_features}\n\nrf_new = RandomForestClassifier(random_state=0)\nrf_rand = RandomizedSearchCV(rf_new,random_grid,n_iter=100,cv=10,verbose=0,random_state=0,n_jobs=-1)\nrf_rand.fit(x_train_std_os,y_train_os)","c136d934":"print ('Best params obtained via random search : ',rf_rand.best_params_)\nprint ('AUC achieved using the base model : ',round(roc_auc_score(y_test,rf.predict_proba(x_test_std)[:,1]),3))\nbest_rand = rf_rand.best_estimator_\nprint ('AUC achieved using the best params achieved in Randomized search : ',round(roc_auc_score(y_test,best_rand.predict_proba(x_test_std)[:,1]),3))","8bf7c417":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'n_estimators' : [100,200,300,400,500,600,1000],\n              'max_depth' : [10,20,30,40,50,60,None],\n              'max_features' : ['auto','sqrt']}\n\nrf_new = RandomForestClassifier(random_state = 0)\nrf_grid = GridSearchCV(rf_new,param_grid,verbose=0,n_jobs=-1,cv=10)\nrf_grid.fit(x_train_std_os,y_train_os)","88baed71":"print ('Best params achieved via GridSearch : ',rf_grid.best_params_)\nrf_best = rf_grid.best_estimator_\nprint ('Best AUC achieved using best params : ',round(roc_auc_score(y_test,rf_best.predict_proba(x_test_std)[:,1]),3))","fc8113ce":"feature_importances = pd.DataFrame(rf.feature_importances_,index = x_train.columns,\n                                   columns=['importance']).sort_values('importance',ascending=True)  \n\nplt.figure(figsize=(15,5))\nplt.barh(feature_importances.index,feature_importances.importance)","ec1d0e16":"### Standardise the training and testing data and oversampling using SMOTE","dc455097":"### Using ordinal logit to predict the wine quality","dfe6813b":"### Create a binary quality variable","ae6bbd7d":"#### COMMENT : A few variables like fixed acidity, pH, total & fixed sulphur dioxide are correlated as expected but correlation coefficient ain't really high so as to drop them from the dataset. So we will consider all the variables to be the part of our training data","775435ad":"### Splitting data into train and test","540825ea":"### Loading the data","36ab0922":"### Data exploration","8a33e7fa":"### Load necessary libraries","fe20207d":"#### COMMENT : Since now we have the best parameters obtained via random search and that shows an improvement over base model, we will fine tune our search using GridSearchCV ","f3581535":"#### COMMENT : Random forest model does the best among all. We will tune it's hyperparameters now using GridSearchCV","82f3026e":"#### EVALUATION METRIC : Since it's a highly class imbalance data, looking at accuracy will be misguiding. So we are gonna look at area under precision recall curve as our evaluation metric for different models.","3cdbd369":"#### COMMENT : Highly imbalance classes with majority being 5 & 6","1c2bd1b2":"#### COMMENT : Except for all 3 kinds of acidities & alcohol, there ain't much trend between quality of wine and the variables. There are, though, huge number of outliers in a few variables.","12aba954":"### Tuning RF model's hyperparameters using GridSearchCV","4863922b":"### Training multiple models on train data","6be59967":"### Split the data, standardise and oversample using SMOTE","ff8ca06c":"#### Evalutating the performance of ordinal logit","2323ecac":"### Evaluating the models using AUPRC","b541d869":"### Using an RF model for the predictions","5ec49237":"#### OBSERVATION : We get very low values of precision and f1 score if we try to predict the exact quality bucket of wine. So a better approach, as per the guidelines, would be to split the wine quality in 2 buckets, i.e. good(above 6) and bad(below or equal to 6)","b5f96b90":"### Plot the feature importances"}}