{"cell_type":{"3444c681":"code","abdbaacc":"code","01c65efe":"code","8a006c4c":"code","ee79682e":"code","eb321062":"code","1e9ccba7":"code","172a990d":"code","68d91203":"code","b61a768d":"code","22c7ca4a":"code","f23c4355":"code","b17f66c0":"code","c965b2d8":"code","5187ad0f":"code","f7379e7d":"code","8e11cc07":"code","09b0782d":"code","e2900938":"code","4ce07d07":"code","259eb107":"code","6f5060a2":"code","2c2247a5":"code","cc4de3f6":"code","edd3bf03":"code","678fbeb8":"code","a5bf2435":"code","b65e5f06":"code","b782bfc3":"code","b1d136e7":"code","3d235d44":"code","85f7dac2":"code","0a15808d":"code","b789352a":"code","1e9aa24e":"code","68e9c881":"code","05bd02fc":"code","b05f5314":"code","149aa988":"code","b975ce45":"code","bc11e20a":"code","5a604d9d":"code","f085b9fa":"code","eb86a3a3":"code","7f1aa486":"markdown","82323c38":"markdown","21171f22":"markdown","7732bc35":"markdown","96385b52":"markdown","955eaf51":"markdown","543c8279":"markdown","56efd85d":"markdown","95f2d8df":"markdown","0cf99f7c":"markdown","e6a74ef1":"markdown","e04a16e3":"markdown","6be9286a":"markdown","c78ce163":"markdown","4b576565":"markdown","32b0a75f":"markdown","7501b370":"markdown","94e1f1dd":"markdown","abf8e969":"markdown","6f348d80":"markdown","268c8d19":"markdown","038b04db":"markdown","509dcbc1":"markdown","04f0e612":"markdown","f6ff70a8":"markdown","bb2afd08":"markdown","b2495ec7":"markdown"},"source":{"3444c681":"import numpy as np \nimport pandas as pd \nimport json\nimport bq_helper\nfrom pandas.io.json import json_normalize\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nimport lightgbm as lgb\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import TimeSeriesSplit, KFold\nfrom sklearn.metrics import mean_squared_error","abdbaacc":"# https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields\n\ndef load_df(csv_path='..\/input\/train.csv', JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource'], load_all=False):\n    if not load_all:\n        df = pd.read_csv(csv_path, \n                         converters={column: json.loads for column in JSON_COLUMNS}, \n                         dtype={'fullVisitorId': 'str'}, nrows=250000)\n    else:\n        df = pd.read_csv(csv_path, \n                         converters={column: json.loads for column in JSON_COLUMNS}, \n                         dtype={'fullVisitorId': 'str'})\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n\n    return df","01c65efe":"%%time\ntrain = load_df(\"..\/input\/train_v2.csv\")","8a006c4c":"%%time\ntest = load_df(\"..\/input\/test_v2.csv\", load_all=True)","ee79682e":"# some data processing\ntrain['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True, inplace=True)\ntest['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True, inplace=True)\ntrain['trafficSource.isTrueDirect'].fillna(False, inplace=True)\ntest['trafficSource.isTrueDirect'].fillna(False, inplace=True)\n\ntrain['date'] = pd.to_datetime(train['date'].apply(lambda x: str(x)[:4] + '-' + str(x)[4:6] + '-' + str(x)[6:]))\ntest['date'] = pd.to_datetime(test['date'].apply(lambda x: str(x)[:4] + '-' + str(x)[4:6] + '-' + str(x)[6:]))","eb321062":"cols_to_drop = [col for col in train.columns if train[col].nunique(dropna=False) == 1]\ntrain.drop(cols_to_drop, axis=1, inplace=True)\ntest.drop([col for col in cols_to_drop if col in test.columns], axis=1, inplace=True)\n\n#only one not null value\n# train.drop(['trafficSource.campaignCode'], axis=1, inplace=True)\n# test.drop(['trafficSource.campaignCode'], axis=1, inplace=True)\n\nprint(f'Dropped {len(cols_to_drop)} columns.')","1e9ccba7":"train[['visitNumber', 'totals.hits', 'totals.pageviews', 'totals.transactionRevenue']].info()","172a990d":"# converting columns into more reasonable format\nfor col in ['visitNumber', 'totals.hits', 'totals.pageviews', 'totals.transactionRevenue']:\n    train[col] = train[col].astype(float)","68d91203":"train.drop(['customDimensions', 'hits', 'trafficSource.referralPath', 'trafficSource.source', 'totals.totalTransactionRevenue'], axis=1, inplace=True)\ntest.drop(['customDimensions', 'hits', 'trafficSource.referralPath', 'trafficSource.source', 'totals.totalTransactionRevenue'], axis=1, inplace=True)","b61a768d":"train.head()","22c7ca4a":"for col in train.columns:\n    if train[col].isnull().sum() > 0:\n        rate = train[col].isnull().sum() * 100 \/ train.shape[0]\n        print(f'Column {col} has {rate:.4f}% missing values.')\n    if train[col].dtype == 'object':\n        if (train[col] == 'not available in demo dataset').sum() > 0:\n            rate = (train[col] == 'not available in demo dataset').sum() * 100 \/ train.shape[0]\n            print(f'Column {col} has {rate:.4f}% values not available in dataset.')","f23c4355":"plt.hist(np.log1p(train.loc[train['totals.transactionRevenue'].isna() == False, 'totals.transactionRevenue']));\nplt.title('Distribution of revenue');","b17f66c0":"grouped = train.groupby('fullVisitorId')['totals.transactionRevenue'].sum().reset_index()\ngrouped = grouped.loc[grouped['totals.transactionRevenue'].isna() == False]\nplt.hist(np.log(grouped.loc[grouped['totals.transactionRevenue'] > 0, 'totals.transactionRevenue']));\nplt.title('Distribution of total revenue per user');","c965b2d8":"counts = train.loc[train['totals.transactionRevenue'] > 0, 'fullVisitorId'].value_counts()\nprint('There are {0} paying users ({1} total) in train data.'.format(len(counts), train['fullVisitorId'].nunique()))\nprint('{0} users ({1:.4f}% of paying) have 1 paid transaction.'.format(np.sum(counts == 1), 100 * np.sum(counts == 1) \/ len(counts)))\nprint('{0} users ({1:.4f}% of paying) have 2 paid transaction.'.format(np.sum(counts == 2), 100 * np.sum(counts == 2) \/ len(counts)))\nprint('')\nprint('Count of non-zero transactions per user:')\ncounts.head(10)","5187ad0f":"train['totals.transactionRevenue'] = train['totals.transactionRevenue'].fillna(0)\ntarget = train['totals.transactionRevenue']\ntrain['totals.transactionRevenue'] = np.log1p(train['totals.transactionRevenue'])\nsns.set(rc={'figure.figsize':(20, 16)})\ntrain_ = train.loc[train['totals.transactionRevenue'] > 0.0]\nsns.boxplot(x=\"device.deviceCategory\", y=\"totals.transactionRevenue\", hue=\"channelGrouping\",  data=train_)\nplt.title(\"Total revenue by device category and channel.\");\nplt.xticks(rotation='vertical')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()\n","f7379e7d":"fig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Trends of transactions number by paying and non-paying users\");\ntrain.groupby(['date'])['totals.transactionRevenue'].count().plot(color='brown')\nax1.set_ylabel('Transaction count', color='b')\nplt.legend(['Non-paying users'])\nax2 = ax1.twinx()\ntrain_.groupby(['date'])['totals.transactionRevenue'].count().plot(color='gold')\nax2.set_ylabel('Transaction count', color='g')\nplt.legend(['Paying users'], loc=(0.875, 0.9))\nplt.grid(False)","8e11cc07":"fig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Trends transaction count and total value by paying users\")\ntrain_.groupby(['date'])['totals.transactionRevenue'].count().plot(color='purple')\nax1.set_ylabel('Transaction count', color='b')\nplt.legend(['Transaction count'])\nax2 = ax1.twinx()\ntrain_.groupby(['date'])['totals.transactionRevenue'].sum().plot(color='gold')\nax2.set_ylabel('Natural logarithm of sum of transactions', color='g')\nplt.legend(['Transaction sum'], loc=(0.875, 0.9))\nplt.grid(False)","09b0782d":"print(f'First date in train set is {train[\"date\"].min()}. Last date in train set is {train[\"date\"].max()}.')\nprint(f'First date in test set is {test[\"date\"].min()}. Last date in test set is {test[\"date\"].max()}.')","e2900938":"fig, ax = plt.subplots(2, 2, figsize = (16, 12))\nprint('Mean revenue per transaction')\nsns.pointplot(x=\"device.browser\", y=\"totals.transactionRevenue\", hue=\"device.isMobile\", data=train_, ax = ax[0, 0]);\nsns.pointplot(x=\"device.deviceCategory\", y=\"totals.transactionRevenue\", hue=\"device.isMobile\", data=train_, ax = ax[0, 1]);\nsns.pointplot(x=\"device.operatingSystem\", y=\"totals.transactionRevenue\", hue=\"device.isMobile\", data=train_, ax = ax[1, 0]);\nsns.pointplot(x=\"device.isMobile\", y=\"totals.transactionRevenue\", data=train_, ax = ax[1, 1]);\nax[0, 0].xaxis.set_tick_params(rotation=30);\nax[0, 1].xaxis.set_tick_params(rotation=30);\nax[1, 0].xaxis.set_tick_params(rotation=30);\nax[1, 1].xaxis.set_tick_params(rotation=30);","4ce07d07":"def show_count_sum(df, col):\n    return df.groupby(col).agg({'totals.transactionRevenue': ['count', 'mean']}).sort_values(('totals.transactionRevenue', 'count'), ascending=False).head()\nshow_count_sum(train_, 'geoNetwork.subContinent')","259eb107":"show_count_sum(train_.loc[train_['geoNetwork.subContinent'] == 'Northern America'], 'geoNetwork.metro')","6f5060a2":"show_count_sum(train_.loc[train_['geoNetwork.subContinent'] == 'Northern America'], 'geoNetwork.networkDomain')","2c2247a5":"show_count_sum(train_.loc[train_['geoNetwork.subContinent'] == 'Northern America'], 'geoNetwork.region')","cc4de3f6":"show_count_sum(train_, 'trafficSource.medium')","edd3bf03":"fig, ax = plt.subplots(2, 2, figsize = (12, 12));\nsns.countplot('trafficSource.adwordsClickInfo.isVideoAd', data=train, ax = ax[0, 0]);\nax[0, 0].set_title('All users count');\nsns.countplot('trafficSource.adwordsClickInfo.isVideoAd', data=train_, ax = ax[0, 1]);\nax[0, 1].set_title('Paying users count');\ntrain.groupby('trafficSource.adwordsClickInfo.isVideoAd')['totals.transactionRevenue'].mean().plot('bar', ax = ax[1, 0]);\nax[1, 0].set_title('Mean revenue');\ntrain.groupby('trafficSource.adwordsClickInfo.isVideoAd')['totals.transactionRevenue'].sum().plot('bar', ax = ax[1, 1]);\nax[1, 1].set_title('Total revenue');","678fbeb8":"fig, ax = plt.subplots(2, 2, figsize = (12, 12));\nsns.countplot('trafficSource.isTrueDirect', data=train, ax = ax[0, 0]);\nax[0, 0].set_title('All users count');\nsns.countplot('trafficSource.isTrueDirect', data=train_, ax = ax[0, 1]);\nax[0, 1].set_title('Paying users count');\ntrain.groupby('trafficSource.isTrueDirect')['totals.transactionRevenue'].mean().plot('bar', ax = ax[1, 0]);\nax[1, 0].set_title('Mean revenue');\ntrain.groupby('trafficSource.isTrueDirect')['totals.transactionRevenue'].sum().plot('bar', ax = ax[1, 1]);\nax[1, 1].set_title('Total revenue');","a5bf2435":"plt.figure(figsize=(12, 8))\nplt.title('Distribution of pageviews');\nplt.hist(np.log1p(train['totals.pageviews'].fillna(0)));","b65e5f06":"plt.figure(figsize=(12, 8))\nplt.title('Distribution of hits');\nplt.hist(np.log1p(train['totals.hits'].fillna(0)));","b782bfc3":"plt.figure(figsize=(12, 8))\nplt.title('pageviews vs revenue');\nplt.scatter(train_['totals.pageviews'], train_['totals.transactionRevenue']);\nplt.xlabel('Pageviews');\nplt.ylabel('Revenue');","b1d136e7":"del grouped, counts, train_","3d235d44":"# time based\ntrain['month'] = train['date'].dt.month\ntrain['day'] = train['date'].dt.day\ntrain['weekday'] = train['date'].dt.weekday\ntrain['weekofyear'] = train['date'].dt.weekofyear\n\ntrain['month_unique_user_count'] = train.groupby('month')['fullVisitorId'].transform('nunique')\ntrain['day_unique_user_count'] = train.groupby('day')['fullVisitorId'].transform('nunique')\ntrain['weekday_unique_user_count'] = train.groupby('weekday')['fullVisitorId'].transform('nunique')\ntrain['weekofyear_unique_user_count'] = train.groupby('weekofyear')['fullVisitorId'].transform('nunique')\n\ntest['month'] = test['date'].dt.month\ntest['day'] = test['date'].dt.day\ntest['weekday'] = test['date'].dt.weekday\ntest['weekofyear'] = test['date'].dt.weekofyear\n\ntest['month_unique_user_count'] = test.groupby('month')['fullVisitorId'].transform('nunique')\ntest['day_unique_user_count'] = test.groupby('day')['fullVisitorId'].transform('nunique')\ntest['weekday_unique_user_count'] = test.groupby('weekday')['fullVisitorId'].transform('nunique')\ntest['weekofyear_unique_user_count'] = test.groupby('weekofyear')['fullVisitorId'].transform('nunique')","85f7dac2":"# device based\n\ntrain['browser_category'] = train['device.browser'] + '_' + train['device.deviceCategory']\ntrain['browser_operatingSystem'] = train['device.browser'] + '_' + train['device.operatingSystem']\n\ntest['browser_category'] = test['device.browser'] + '_' + test['device.deviceCategory']\ntest['browser_operatingSystem'] = test['device.browser'] + '_' + test['device.operatingSystem']\n","0a15808d":"train['visitNumber'] = np.log1p(train['visitNumber'])\ntest['visitNumber'] = np.log1p(test['visitNumber'])\n\ntrain['totals.hits'] = np.log1p(train['totals.hits'])\ntest['totals.hits'] = np.log1p(test['totals.hits'].astype(int))\n\ntrain['totals.pageviews'] = np.log1p(train['totals.pageviews'].fillna(0))\ntest['totals.pageviews'] = np.log1p(test['totals.pageviews'].astype(float).fillna(0))\n\ntrain['sum_pageviews_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('sum')\ntrain['count_pageviews_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('count')\ntrain['mean_pageviews_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('mean')\ntrain['sum_hits_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.hits'].transform('sum')\ntrain['count_hits_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.hits'].transform('count')\ntrain['mean_hits_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.hits'].transform('mean')\n\ntest['sum_pageviews_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('sum')\ntest['count_pageviews_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('count')\ntest['mean_pageviews_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('mean')\ntest['sum_hits_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.hits'].transform('sum')\ntest['count_hits_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.hits'].transform('count')\ntest['mean_hits_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.hits'].transform('mean')\n\ntrain['mean_hits_per_day'] = train.groupby(['day'])['totals.hits'].transform('mean')\ntrain['sum_hits_per_day'] = train.groupby(['day'])['totals.hits'].transform('sum')\ntest['mean_hits_per_day'] = test.groupby(['day'])['totals.hits'].transform('mean')\ntest['sum_hits_per_day'] = test.groupby(['day'])['totals.hits'].transform('sum')","b789352a":"train['sum_pageviews_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('sum')\ntrain['count_pageviews_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('count')\ntrain['mean_pageviews_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('mean')\n\ntrain['sum_pageviews_per_region'] = train.groupby('geoNetwork.region')['totals.pageviews'].transform('sum')\ntrain['count_pageviews_per_region'] = train.groupby('geoNetwork.region')['totals.pageviews'].transform('count')\ntrain['mean_pageviews_per_region'] = train.groupby('geoNetwork.region')['totals.pageviews'].transform('mean')\n\ntest['sum_pageviews_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('sum')\ntest['count_pageviews_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('count')\ntest['mean_pageviews_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('mean')\n\ntest['sum_pageviews_per_region'] = test.groupby('geoNetwork.region')['totals.pageviews'].transform('sum')\ntest['count_pageviews_per_region'] = test.groupby('geoNetwork.region')['totals.pageviews'].transform('count')\ntest['mean_pageviews_per_region'] = test.groupby('geoNetwork.region')['totals.pageviews'].transform('mean')","1e9aa24e":"train['sum_hits_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.hits'].transform('sum')\ntrain['count_hits_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.hits'].transform('count')\ntrain['mean_hits_per_network_domain'] = train.groupby('geoNetwork.networkDomain')['totals.hits'].transform('mean')\n\ntrain['sum_hits_per_region'] = train.groupby('geoNetwork.region')['totals.hits'].transform('sum')\ntrain['count_hits_per_region'] = train.groupby('geoNetwork.region')['totals.hits'].transform('count')\ntrain['mean_hits_per_region'] = train.groupby('geoNetwork.region')['totals.hits'].transform('mean')\n\ntrain['sum_hits_per_country'] = train.groupby('geoNetwork.country')['totals.hits'].transform('sum')\ntrain['count_hits_per_country'] = train.groupby('geoNetwork.country')['totals.hits'].transform('count')\ntrain['mean_hits_per_country'] = train.groupby('geoNetwork.country')['totals.hits'].transform('mean')\n\ntest['sum_hits_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.hits'].transform('sum')\ntest['count_hits_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.hits'].transform('count')\ntest['mean_hits_per_network_domain'] = test.groupby('geoNetwork.networkDomain')['totals.hits'].transform('mean')\n\ntest['sum_hits_per_region'] = test.groupby('geoNetwork.region')['totals.hits'].transform('sum')\ntest['count_hits_per_region'] = test.groupby('geoNetwork.region')['totals.hits'].transform('count')\ntest['mean_hits_per_region'] = test.groupby('geoNetwork.region')['totals.hits'].transform('mean')\n\ntest['sum_hits_per_country'] = test.groupby('geoNetwork.country')['totals.hits'].transform('sum')\ntest['count_hits_per_country'] = test.groupby('geoNetwork.country')['totals.hits'].transform('count')\ntest['mean_hits_per_country'] = test.groupby('geoNetwork.country')['totals.hits'].transform('mean')","68e9c881":"train['user_pageviews_sum'] = train.groupby('fullVisitorId')['totals.pageviews'].transform('sum')\ntrain['user_hits_sum'] = train.groupby('fullVisitorId')['totals.hits'].transform('sum')\ntest['user_pageviews_sum'] = test.groupby('fullVisitorId')['totals.pageviews'].transform('sum')\ntest['user_hits_sum'] = test.groupby('fullVisitorId')['totals.hits'].transform('sum')\n\ntrain['user_pageviews_count'] = train.groupby('fullVisitorId')['totals.pageviews'].transform('count')\ntrain['user_hits_count'] = train.groupby('fullVisitorId')['totals.hits'].transform('count')\ntest['user_pageviews_count'] = test.groupby('fullVisitorId')['totals.pageviews'].transform('count')\ntest['user_hits_count'] = test.groupby('fullVisitorId')['totals.hits'].transform('count')\n\ntrain['user_pageviews_sum_to_mean'] = train['user_pageviews_sum'] \/ train['user_pageviews_sum'].mean()\ntrain['user_hits_sum_to_mean'] = train['user_hits_sum'] \/ train['user_hits_sum'].mean()\ntest['user_pageviews_sum_to_mean'] = test['user_pageviews_sum'] \/ test['user_pageviews_sum'].mean()\ntest['user_hits_sum_to_mean'] = test['user_hits_sum'] \/ test['user_hits_sum'].mean()","05bd02fc":"train['user_pageviews_to_region'] = train['user_pageviews_sum'] \/ train['mean_pageviews_per_region']\ntrain['user_hits_to_region'] = train['user_hits_sum'] \/ train['mean_hits_per_region']\n\ntest['user_pageviews_to_region'] = test['user_pageviews_sum'] \/ test['mean_pageviews_per_region']\ntest['user_hits_to_region'] = test['user_hits_sum'] \/ test['mean_hits_per_region']","b05f5314":"num_cols = ['visitNumber', 'totals.hits', 'totals.pageviews', 'month_unique_user_count', 'day_unique_user_count', 'mean_hits_per_day'\n           'sum_pageviews_per_network_domain', 'sum_hits_per_network_domain', 'count_hits_per_network_domain', 'sum_hits_per_region',\n           'sum_hits_per_day', 'count_pageviews_per_network_domain', 'mean_pageviews_per_network_domain', 'weekday_unique_user_count',\n           'sum_pageviews_per_region', 'count_pageviews_per_region', 'mean_pageviews_per_region', 'user_pageviews_count', 'user_hits_count',\n           'count_hits_per_region', 'mean_hits_per_region', 'user_pageviews_sum', 'user_hits_sum', 'user_pageviews_sum_to_mean',\n            'user_hits_sum_to_mean', 'user_pageviews_to_region', 'user_hits_to_region', 'mean_pageviews_per_network_domain',\n           'mean_hits_per_network_domain']\n\nno_use = [\"visitNumber\", \"date\", \"fullVisitorId\", \"sessionId\", \"visitId\", \"visitStartTime\", 'totals.transactionRevenue', 'trafficSource.referralPath']\ncat_cols = [col for col in train.columns if col not in num_cols and col not in no_use]","149aa988":"for col in cat_cols:\n    if col != 'trafficSource.campaignCode':\n        print(col)\n        lbl = LabelEncoder()\n        lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n        train[col] = lbl.transform(list(train[col].values.astype('str')))\n        test[col] = lbl.transform(list(test[col].values.astype('str')))","b975ce45":"no_use.append('trafficSource.campaignCode')\ntrain = train.sort_values('date')\nX = train.drop([col for col in no_use if col in train.columns], axis=1)\ny = train['totals.transactionRevenue']\nX_test = test.drop([col for col in no_use if col in test.columns], axis=1)\n# I use TimeSeriesSplit as we have time series\ntscv = TimeSeriesSplit(n_splits=5)","bc11e20a":"params = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \"max_depth\": 12, \"min_child_samples\": 20, \"reg_alpha\": 0.1, \"reg_lambda\": 0.1,\n        \"num_leaves\" : 1024, \"learning_rate\" : 0.01, \"subsample\" : 0.9, \"colsample_bytree\" : 0.9, \"subsample_freq \": 10}\nn_fold = 10\nfolds = KFold(n_splits=n_fold, shuffle=False, random_state=42)\n# Cleaning and defining parameters for LGBM\nmodel = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)","5a604d9d":"prediction = np.zeros(test.shape[0])\n\nfor fold_n, (train_index, test_index) in enumerate(folds.split(X)):\n    print('Fold:', fold_n)\n    #print(f'Train samples: {len(train_index)}. Valid samples: {len(test_index)}')\n    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n    \n\n    model.fit(X_train, y_train, \n            eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n            verbose=500, early_stopping_rounds=100)\n    \n    y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n    prediction += y_pred\nprediction \/= n_fold","f085b9fa":"lgb.plot_importance(model, max_num_features=30);","eb86a3a3":"test[\"PredictedLogRevenue\"] = prediction\nsub = test.groupby(\"fullVisitorId\").agg({\"PredictedLogRevenue\" : \"sum\"}).reset_index()\nsub[\"PredictedLogRevenue\"] = np.log1p(sub[\"PredictedLogRevenue\"])\nsub[\"PredictedLogRevenue\"] = sub[\"PredictedLogRevenue\"].apply(lambda x : 0.0 if x < 0 else x)\nsub[\"PredictedLogRevenue\"] = sub[\"PredictedLogRevenue\"].fillna(0.0)\nsub.to_csv(\"lgb.csv\", index=False)\n\n\n# from this kernel: https:\/\/www.kaggle.com\/fabiendaniel\/lgbm-rf-starter-lb-1-70\n#submission = test[['fullVisitorId']].copy()\n#submission.loc[:, 'PredictedLogRevenue'] = np.expm1(predictions)\n#submission.loc[:, 'PredictedLogRevenue'] = prediction\n#submission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"].apply(lambda x : 0.0 if x < 0 else x)\n#submission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"].fillna(0.0)\n#grouped_test = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\n#grouped_test[\"PredictedLogRevenue\"] = np.log1p(grouped_test[\"PredictedLogRevenue\"])\n#grouped_test.to_csv('lgb.csv',index=False)","7f1aa486":"Obviously most transactions come from America","82323c38":"The plots are quite similar.","21171f22":"We can see several thing from this overview:\n- less than 2% of all transactions bring revenue. This percentage is more or less reasonable;\n- some columns have full representation only in BigQuery, so it is necessary to use it;\n- traffic sourse in some cases is unknown, we'll need to find a way to fill these values;\n- there several groups of features: visitor activity, geodata and device info, source of traffic;","7732bc35":"## Feature engineering","96385b52":"Here we can see the distribution of transaction revenue. But it would be more useful to see a distribution of total revenue per user.","955eaf51":"### geoNetwork","543c8279":"At first let's create some features","56efd85d":"### Traffic source","95f2d8df":"Let's see how video ads influence revenue. There are for graphs, two of them show count of all users or only paying users, another two show mean and total revenue for transactions based on wheather there were video ads.","0cf99f7c":"## Feature analysis","e6a74ef1":"## Data exploration","e04a16e3":"Let's see the same graphs for trafficSource.adwordsClickInfo.isTrueDirect feature","6be9286a":"Most transactions are from California or New York.","c78ce163":"## General information\nThis kernel is dedicated to EDA of Google Analytics Customer Revenue Prediction  competition as well as feature engineering. For now basic data is used and not data from BigQuery.\n\nIn this dataset we can see customers which went to Google Merchandise Store, info about them and their transactions. We need to predict the natural log of the sum of all transactions per user.\n\nA continuation of this kernel can be found in [this one](https:\/\/www.kaggle.com\/artgor\/lgb-and-feature-generation) it has more features, CV calculation and other things.","4b576565":"### Totals","32b0a75f":"Previously I made a submission following the officially suggested logic: calculating logarithm of sum of predicted revenues. But some people say that it is better to simply submit the sum of predicted values (which are already logarithms). Let's try!","7501b370":"### Revenue","94e1f1dd":"### Devices\n\nLet's see which devices bring most revenue!","abf8e969":"It seems that devices on Chrome OS and Macs bring most profit.\n\nThere is an interesting fact - there are desktop devices which are considered to be mobile devices. Is it an error?","6f348d80":"We can see that the ratio of users arriving from video ads is almost the same for paid and non-paid transactions.\n\nAnd total revenue from transactions from video ads is much higher.","268c8d19":"\nIt isn't surprising that trends of sum and count of paid transactions are almost similar. It is worth noticing that there are several periods when the number of non-paying users was significantly higher that the number of paying users, but it didin't influence total sums.\n\nTrain and test period don't intersect.","038b04db":"Some of columns aren't available in this dataset, let's drop them.","509dcbc1":"### Feature processing","04f0e612":"True directing seems to be better in all regards.","f6ff70a8":"Most paying users made only 1 transaction, but there are several users, who had a lot of transactions. Regular customers?","bb2afd08":"In fact it seems that it will take some time to find a good validation - TimeSeriesSplit gives a high variance in scores, so I'll try kfold for now.","b2495ec7":"We can see that revenue comes mostly from desktops. Social, Affiliates and others aren't as profitable as other channels."}}