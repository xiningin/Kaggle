{"cell_type":{"a3d5ee14":"code","298acb92":"code","8002c6e6":"code","344e7895":"code","fdfecb57":"code","296d54ef":"code","9c057297":"code","9b321e43":"code","5c35b4fc":"code","fd12ec88":"code","e4452b1e":"code","b327b94d":"code","b47e77bc":"code","c6f782b2":"code","f7f3c4d3":"code","72d4b4d7":"code","45ced08d":"code","500effc1":"code","8044fc2d":"code","3e32edc4":"code","05342d49":"code","edf4f943":"code","503762ea":"code","5a108428":"code","d7595b55":"code","df5b99bf":"code","081c1c61":"code","ed3d1ea1":"code","8b13ee06":"code","5e1571dd":"code","2107cbff":"code","eb620da0":"code","72a81262":"code","bb00c3f4":"code","74ea4db6":"code","5f11c9f4":"code","d27de9ed":"code","236ab163":"code","0e3296ad":"code","47554305":"code","b9f28061":"code","be85063c":"code","fe94fb95":"code","d7cb40e9":"code","451a59f9":"code","13a9cf52":"markdown","e29f9187":"markdown","4f5ade89":"markdown","761967ff":"markdown","a28756bf":"markdown","c9e49ea3":"markdown","de7f0fc4":"markdown","9f75ef87":"markdown","1145d229":"markdown","cae3ee50":"markdown","8868c1ce":"markdown","0eb7d1fc":"markdown"},"source":{"a3d5ee14":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","298acb92":"import tensorflow as tf\nimport pandas as pd\nimport string\nimport re\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import PercentFormatter\nimport nltk\nimport spacy\nimport sys\nfrom spacy.lang.en import English\nimport en_core_web_sm\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nprint(tf.__version__)  # 2.0.0-beta0","8002c6e6":"# Run this code for the first time, to install the libraries and download wordnet\n# %reset\n# !{sys.executable} -m pip install spacy\n# !{sys.executable} -m spacy download en\n# !{sys.executable} -m pip install pyLDAvis\n# !{sys.executable} -m pip install gensim\n# nltk.download('stopwords')\n# nltk.download('wordnet')","344e7895":"df = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\", lines=True)\ndf = df[['headline', 'is_sarcastic']]\ndf.head()","fdfecb57":"# check for columns with null values\ndf.is_sarcastic.isnull().any() # no missing values in is_sarcastic column\ndf.headline.isnull().any() # no missing values in headline column","296d54ef":"df['headline'] = df.headline.apply(lambda x:x.lower())  # convert all words in headline into lower case \ndf['headline'] = df.headline.apply(lambda x: ' '.join(word.strip(string.punctuation) for word in x.split()))  # remove all punctuations in headline","9c057297":"df['headline_count'] = df.headline.apply(lambda x: len(list(x.split())))\ndf['headline_unique_word_count'] = df.headline.apply(lambda x: len(set(x.split())))\ndf['headline_has_digits'] = df.headline.apply(lambda x: bool(re.search(r'\\d', x)))\ndf","9b321e43":"sarcastic_dat = df.groupby('is_sarcastic').count()\nsarcastic_dat.index = ['Non-sarcastic','Sarcastic']\nplt.xlabel('Type of headlines (Sarcastic & Non-sarcastic)')\nplt.ylabel('Frequencies of headlines')\nplt.xticks(fontsize=10)\nplt.title('Frequencies of Sarcastic vs Non-sarcastic headlines')\nbar_graph = plt.bar(sarcastic_dat.index, sarcastic_dat.headline_count)\nbar_graph[1].set_color('r')\nplt.show()\n\n\nplt.xlabel('Type of headlines (Sarcastic & Non-sarcastic)')\nplt.ylabel('Proportion of headlines')\nplt.xticks(fontsize=10)\nplt.title('Proportion of Sarcastic vs Non-sarcastic headlines')\nbar_graph = plt.bar(sarcastic_dat.index, sarcastic_dat.headline_count \/ sarcastic_dat.headline_count.sum())\nbar_graph[1].set_color('r')\nplt.gca().yaxis.set_major_formatter(PercentFormatter(1))\nplt.show()\n\n# This is not an imbalanced class dataset\n# Non-sarcastic    0.56\n# Sarcastic        0.44\nround(sarcastic_dat.headline_count \/ sarcastic_dat.headline_count.sum(), 2)","5c35b4fc":"all_dat = df.groupby('headline_count').count()\nsarcastic_dat1 = df[df.is_sarcastic==1]\nsarcastic_dat = sarcastic_dat1.groupby('headline_count').count()\nnot_sarcastic_dat1 = df[df.is_sarcastic==0]\nnot_sarcastic_dat = not_sarcastic_dat1.groupby('headline_count').count()\n\nplt.xlabel('Different lengths of headline')\nplt.ylabel('Frequencies of headline length')\nplt.xticks(fontsize=10)\nplt.title('Distribution of headline length for entire dataset')\nbar_graph = plt.bar(all_dat.index, all_dat.headline)\nbar_graph[8].set_color('r')\nplt.axvline(df.headline_count.mean(), color='k', linestyle='dashed', linewidth=1)  # median is 10 words in a headline\nplt.show()\n\nplt.xlabel('Different lengths of sarcastic headline')\nplt.ylabel('Frequencies of sarcastic headline length')\nplt.xticks(fontsize=10)\nplt.title('Distribution of headline length for sarcastic dataset')\nbar_graph = plt.bar(sarcastic_dat.index, sarcastic_dat.headline)\nbar_graph[7].set_color('r')\nplt.axvline(sarcastic_dat1.headline_count.mean(), color='k', linestyle='dashed', linewidth=1)  # median is 10 words in a headline\nplt.show()\n\n\nplt.xlabel('Different lengths of non-sarcastic headline')\nplt.ylabel('Frequencies of non-sarcastic headline length')\nplt.xticks(fontsize=10)\nplt.title('Distribution of headline length for non-sarcastic dataset')\nbar_graph = plt.bar(not_sarcastic_dat.index, not_sarcastic_dat.headline)\nbar_graph[8].set_color('r')\nplt.axvline(not_sarcastic_dat1.headline_count.mean(), color='k', linestyle='dashed', linewidth=1)  # median is 10 words in a headline\nplt.show()\n\n# difference in the length of sarcastic and non-sarcastic headlines is not significant. \n# median and mean length of headlines is around 10 words","fd12ec88":"digits_dat = df.groupby('headline_has_digits').count()\ndigits_dat.index = ['Has Numbers in Headline','Does not have Numbers in Headline']\n\n\nplt.xlabel('Type of headlines')\nplt.ylabel('Frequencies of headlines')\nplt.xticks(fontsize=10)\nplt.title('Frequencies of headlines with Numbers vs No numbers')\nbar_graph = plt.bar(digits_dat.index, digits_dat.headline \/ digits_dat.headline_count.sum())\nbar_graph[1].set_color('r')\nplt.gca().yaxis.set_major_formatter(PercentFormatter(1))\nplt.show()\n\n\nsarcastic_digits_dat = df[df.is_sarcastic==1].groupby('headline_has_digits').count()\nsarcastic_digits_dat.index = ['Has Numbers in Headline','Does not have Numbers in Headline']\n\n\nplt.xlabel('Type of headlines')\nplt.ylabel('Frequencies of headlines')\nplt.xticks(fontsize=10)\nplt.title('Frequencies of Sarcastic headlines with Numbers vs No numbers')\nbar_graph = plt.bar(sarcastic_digits_dat.index, sarcastic_digits_dat.headline \/ sarcastic_digits_dat.headline_count.sum())\nbar_graph[1].set_color('r')\nplt.gca().yaxis.set_major_formatter(PercentFormatter(1))\nplt.show()\n\n\nnot_sarcastic_digits_dat = df[df.is_sarcastic==0].groupby('headline_has_digits').count()\nnot_sarcastic_digits_dat.index = ['Has Numbers in Headline','Does not have Numbers in Headline']\n\n\nplt.xlabel('Type of headlines')\nplt.ylabel('Frequencies of headlines')\nplt.xticks(fontsize=10)\nplt.title('Frequencies of Non-sarcastic headlines with Numbers vs No numbers')\nbar_graph = plt.bar(not_sarcastic_digits_dat.index, not_sarcastic_digits_dat.headline \/ not_sarcastic_digits_dat.headline_count.sum())\nbar_graph[1].set_color('r')\nplt.gca().yaxis.set_major_formatter(PercentFormatter(1))\nplt.show()\n\nprint(round(digits_dat.headline \/ digits_dat.headline_count.sum(),2))\nprint(round(sarcastic_digits_dat.headline \/ sarcastic_digits_dat.headline_count.sum(),2))\nprint(round(not_sarcastic_digits_dat.headline \/ not_sarcastic_digits_dat.headline_count.sum(),2))\n\n# difference in the use of numbers\/statistics in sarcastic and non-sarcastic headlines is not significant. \n# ~85% headlines uses numbers","e4452b1e":"nlp = en_core_web_sm.load()\nparser = English()\nen_stop = set(nltk.corpus.stopwords.words('english'))\n\n\ndef tokenize(text):\n    \"\"\"this function is to tokenize the headline into a list of individual words\"\"\"\n    lda_tokens = []\n    tokens = parser(text)  # need to use parser for python to treat the list as words\n    for token in tokens:\n        if token.orth_.isspace():  # to ignore any whitespaces in the headline, so that token list does not contain whitespaces \n            continue\n        elif token.like_url:\n            lda_tokens.append('URL')\n        elif token.orth_.startswith('@'):\n            lda_tokens.append('SCREEN_NAME')\n        else:\n            lda_tokens.append(token.lower_)   # tokens (headlines) are already in lowercase\n    return lda_tokens\n\n\ndef get_lemma(word):\n    \"\"\"this function is to lemmatize the words in a headline into its root form\"\"\"\n    lemma = wn.morphy(word)  # converts the word into root form from wordnet\n    if lemma is None:\n        return word\n    else:\n        return lemma\n    \n\ndef prepare_text_for_lda(text):\n    tokens = tokenize(text)  # parse and tokenize the headline into a list of words\n    tokens = [token for token in tokens if len(token) > 4]  # remove headlines with only length of 4 words or less\n    tokens = [token for token in tokens if token not in en_stop]  # remove stopwords in the headline\n    tokens = [get_lemma(token) for token in tokens]  # lemmatize the words in the headline\n    return tokens","b327b94d":"text_data = []\nfor headline in df.headline:\n    tokens = prepare_text_for_lda(headline)\n    text_data.append(tokens)","b47e77bc":"from gensim import corpora\nimport pickle\n\ndictionary = corpora.Dictionary(text_data)  # Convert all headlines into a corpus of words, with each word as a token\ncorpus = [dictionary.doc2bow(text) for text in text_data]  # Convert each headline (a list of words) into the bag-of-words format. (Word ID, Count of word)\npickle.dump(corpus, open('corpus.pkl', 'wb'))  \ndictionary.save('dictionary.gensim')  # takes a while to run the dictionary and corpus","c6f782b2":"import gensim\n\nNUM_TOPICS = [3, 5, 10]\n# passes: Number of passes through the corpus during training\n# alpha: priori on the distribution of the topics in each document.\n# The higher the alpha, the higher the likelihood that document contains a wide range of topics, vice versa. \n# beta: priori on the distribution of the words in each topic.\n# The higher the beta, the higher the likelihood that topic contains a wide range of words, vice versa.\n# we do not alter \/ fine tune the default values of alpha and beta\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS[1], id2word=dictionary, passes=15)\nldamodel.save('model5.gensim')\ntopics = ldamodel.print_topics(num_words=5)\ntopics","f7f3c4d3":"ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 3, id2word=dictionary, passes=15)\nldamodel.save('model3.gensim')\ntopics = ldamodel.print_topics(num_words=5)\ntopics","72d4b4d7":"ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word=dictionary, passes=15)\nldamodel.save('model10.gensim')\ntopics = ldamodel.print_topics(num_words=5)\ntopics","45ced08d":"dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\ncorpus = pickle.load(open('corpus.pkl', 'rb'))\nlda = gensim.models.ldamodel.LdaModel.load('model5.gensim')\nimport pyLDAvis.gensim\nlda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\npyLDAvis.display(lda_display)","500effc1":"lda3 = gensim.models.ldamodel.LdaModel.load('model3.gensim')\nlda_display3 = pyLDAvis.gensim.prepare(lda3, corpus, dictionary, sort_topics=False)\npyLDAvis.display(lda_display3)","8044fc2d":"lda10 = gensim.models.ldamodel.LdaModel.load('model10.gensim')\nlda_display10 = pyLDAvis.gensim.prepare(lda10, corpus, dictionary, sort_topics=False)\npyLDAvis.display(lda_display10)","3e32edc4":"from numpy import mean\n\nsarcastic = list(df.is_sarcastic == 1)\ntuple_list = []\nfor headline in sarcastic:\n    sarcastic = lda10[corpus[headline]]\n    for tuple_ in sarcastic:\n        tuple_list.append(tuple_)\n\nprint('For LDA model with 10 clusters:')\nprint('\\nFor Sarcastic Dataset:')\nprint([(uk, mean([vv for kk,vv in tuple_list if kk==uk])) for uk in set([k for k,v in tuple_list])])\n\nnot_sarcastic = list(df.is_sarcastic == 0)\ntuple_list = []\nfor headline in not_sarcastic:\n    not_sarcastic = lda10[corpus[headline]]\n    for tuple_ in not_sarcastic:\n        tuple_list.append(tuple_)\n        \n\nprint('\\nFor Non-sarcastic Dataset:')\nprint([(uk, mean([vv for kk,vv in tuple_list if kk==uk])) for uk in set([k for k,v in tuple_list])])\n\n# LDA model with 10 clusters not differentiable between sarcastic and not sarcastic headlines.\n# Not very interpretable","05342d49":"sarcastic = list(df.is_sarcastic == 1)\ntuple_list = []\nfor headline in sarcastic:\n    sarcastic = lda[corpus[headline]]\n    for tuple_ in sarcastic:\n        tuple_list.append(tuple_)\n\nprint('For LDA model with 5 clusters:')\nprint('For Sarcastic Dataset:')\nprint([(uk, mean([vv for kk,vv in tuple_list if kk==uk])) for uk in set([k for k,v in tuple_list])])\n\nnot_sarcastic = list(df.is_sarcastic == 0)\ntuple_list = []\nfor headline in not_sarcastic:\n    not_sarcastic = lda[corpus[headline]]\n    for tuple_ in not_sarcastic:\n        tuple_list.append(tuple_)\n        \n\nprint('\\nFor Non-sarcastic Dataset:')\nprint([(uk, mean([vv for kk,vv in tuple_list if kk==uk])) for uk in set([k for k,v in tuple_list])])\n\n# LDA model with 5 clusters not differentiable between sarcastic and not sarcastic headlines.\n# Not very interpretable","edf4f943":"sarcastic = list(df.is_sarcastic == 1)\ntuple_list = []\nfor headline in sarcastic:\n    sarcastic = lda3[corpus[headline]]\n    for tuple_ in sarcastic:\n        tuple_list.append(tuple_)\n\nprint('For LDA model with 3 clusters:')\nprint('For Sarcastic Dataset:')\nprint([(uk, mean([vv for kk,vv in tuple_list if kk==uk])) for uk in set([k for k,v in tuple_list])])\n\nnot_sarcastic = list(df.is_sarcastic == 0)\ntuple_list = []\nfor headline in not_sarcastic:\n    not_sarcastic = lda3[corpus[headline]]\n    for tuple_ in not_sarcastic:\n        tuple_list.append(tuple_)\n        \n\nprint('\\nFor Non-sarcastic Dataset:')\nprint([(uk, mean([vv for kk,vv in tuple_list if kk==uk])) for uk in set([k for k,v in tuple_list])])\n\n# LDA model with 3 clusters not differentiable between sarcastic and not sarcastic headlines.\n# Not very interpretable","503762ea":"train_data, test_data = train_test_split(df[['headline', 'is_sarcastic']], test_size=0.1)  # randomly splitting 10% of dataset to be training dataset \n\ntraining_sentences = list(train_data['headline'])\ntraining_labels = list(train_data['is_sarcastic'])\n\ntesting_sentences = list(test_data['headline'])\ntesting_labels = list(test_data['is_sarcastic'])\ntraining_labels_final = np.array(training_labels)\ntesting_labels_final = np.array(testing_labels)","5a108428":"vocab_size = 10000   # limit vector of words to the top 10,000 words\nembedding_dim = 16\nmax_length = 120\ntrunc_type='post'\noov_tok = \"<OOV>\"\n\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(training_sentences)\npadded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length)\n\n# no lemmatization, removal of stop words and stemming of headlines as we would like to maintain the syntax, literature integrity, sequence of words in LSTM.","d7595b55":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])","df5b99bf":"# Model Definition with BiRNN (GRU)\n# with L1 Lasso Regularization, for feature selection\n# Dropout, for robustness of recurrent neural networks\n# Batch Normalization, to stabilize and perhaps accelerate the learning process\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n    tf.keras.layers.Dense(100, kernel_regularizer=tf.keras.regularizers.l1(0.003), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l1(0.003), activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","081c1c61":"num_epochs = 10\nhistory = model.fit(padded, training_labels_final, epochs=num_epochs, batch_size=64, validation_data=(testing_padded, testing_labels_final))","ed3d1ea1":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\n\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')\nplt.show()","8b13ee06":"# Model Definition with BiRNN (GRU)\n# with L2 Ridge Regularization\n# Dropout, for robustness of recurrent neural networks\n# Batch Normalization, to stabilize and perhaps accelerate the learning process\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n    tf.keras.layers.Dense(100, kernel_regularizer=tf.keras.regularizers.l2(0.003), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.003), activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","5e1571dd":"num_epochs = 10\nhistory = model.fit(padded, training_labels_final, epochs=num_epochs, batch_size=64, validation_data=(testing_padded, testing_labels_final))","2107cbff":"plot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')\nplt.show()","eb620da0":"# Model Definition with BiRNN (LSTM)\n# with L1 Lasso Regularization, for feature selection\n# Dropout, for robustness of recurrent neural networks\n# Batch Normalization, to stabilize and perhaps accelerate the learning process\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(100, kernel_regularizer=tf.keras.regularizers.l1(0.003), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l1(0.003), activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","72a81262":"num_epochs = 10\nhistory = model.fit(padded, training_labels_final, epochs=num_epochs, batch_size=64, validation_data=(testing_padded, testing_labels_final))","bb00c3f4":"plot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')\nplt.show()","74ea4db6":"# Model Definition with BiRNN (LSTM)\n# with L2 Ridge Regularization\n# Dropout, for robustness of recurrent neural networks\n# Batch Normalization, to stabilize and perhaps accelerate the learning process\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(100, kernel_regularizer=tf.keras.regularizers.l2(0.003), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.003), activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","5f11c9f4":"num_epochs = 10\nhistory = model.fit(padded, training_labels_final, epochs=num_epochs, batch_size=64, validation_data=(testing_padded, testing_labels_final))","d27de9ed":"plot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')\nplt.show()","236ab163":"# Model Definition with CNN (Conv1D)\n# with L1 Lasso Regularization, for feature selection\n# Dropout, for robustness\n# Batch Normalization, to stabilize and perhaps accelerate the learning process\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(100, kernel_regularizer=tf.keras.regularizers.l1(0.003), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l1(0.003), activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","0e3296ad":"num_epochs = 10\nhistory = model.fit(padded, training_labels_final, epochs=num_epochs, batch_size=64, validation_data=(testing_padded, testing_labels_final))","47554305":"plot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')\nplt.show()","b9f28061":"# Model Definition with CNN (Conv1D)\n# with L2 Ridge Regularization\n# Dropout, for robustness\n# Batch Normalization, to stabilize and perhaps accelerate the learning process\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(100, kernel_regularizer=tf.keras.regularizers.l2(0.003), activation='relu'),\n    # tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.003), activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n","be85063c":"num_epochs = 10\nhistory = model.fit(padded, training_labels_final, epochs=num_epochs, batch_size=64, validation_data=(testing_padded, testing_labels_final))","fe94fb95":"plot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')\nplt.show()","d7cb40e9":"# Model Definition with CNN (Conv1D)\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Conv1D(128, 1, activation='relu'),\n    tf.keras.layers.MaxPooling1D(2, padding=\"same\"),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(100, kernel_regularizer=tf.keras.regularizers.l1(0.005), activation='relu'),\n    # tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l1(0.005), activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","451a59f9":"plot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')\nplt.show()","13a9cf52":"<a name=\"EDA\"><\/a>\n### Exploratory Data Analysis\n\nWe would first like to understand the news headline dataset and identify the factors that causes a headline to be sarcastic news report. We would also like to identify if there is an imbalanced class, the frequencies and distribution of the different type of words. This will enable us to know if additional data processing steps are required (e.g. sampling of datapoints from sarcastic class if there is an imbalanced class in the dataset)  \n\nData processing steps: \n- Check for missing values in headline, is_sarcastic\n- Convert all words into lowercase \n- Check for imbalanced classes in dataset\n- Removal of punctuation\n\nSome questions we would like to investigate:\n- What are the frequencies of sarcastic headlines against the non-sarcastic headlines?\n- What is the word length of headlines? For sarcastic and non-sarcastic headlines?\n- Does the sarcastic headlines uses statistics (digits\/numbers) in their wording? Compared to non-sarcastic headlines?\n- Topic Modelling. What are the topics in news headline for entire\/sarcastic\/non-sarcastic datasets?\n- What are all the topics in the news headline? Most popular topic?\n- What are all the topics involved in the news headline which are sarcastic? Most popular topic?\n- What are all the topics involved in the news headline which are not sarcastic? Most popular topic?","e29f9187":"# News Headlines Dataset For Sarcasm Detection\n\n\n### Objective\nGiven a news headline, predict whether a news headline is a sarcastic remark or not. We used unsupervised and supervised learning methods, such as Latent Dirichlet Allocation for topic extraction and Deep Learning models (CNN, LSTM with pre-trained word embeddings and self-trained word embeddings) for prediction of sarcasm in news headline. \n\n---\n\n### Context\nPast studies in Sarcasm Detection mostly make use of Twitter datasets collected using hashtag based supervision but such datasets are noisy in terms of labels and language. Furthermore, many tweets are replies to other tweets and detecting sarcasm in these requires the availability of contextual tweets.\n\nTo overcome the limitations related to noise in Twitter datasets, this News Headlines dataset for Sarcasm Detection is collected from two news website. TheOnion aims at producing sarcastic versions of current events and we collected all the headlines from News in Brief and News in Photos categories (which are sarcastic). We collect real (and non-sarcastic) news headlines from HuffPost.\n\nThis new dataset has following advantages over the existing Twitter datasets:\n\n- Since news headlines are written by professionals in a formal manner, there are no spelling mistakes and informal usage. This reduces the sparsity and also increases the chance of finding pre-trained embeddings.\n\n- Furthermore, since the sole purpose of TheOnion is to publish sarcastic news, we get high-quality labels with much less noise as compared to Twitter datasets.\n\n- Unlike tweets which are replies to other tweets, the news headlines we obtained are self-contained. This would help us in teasing apart the real sarcastic elements.\n\n---\n\n### Content\nEach record consists of three attributes:\n\n`is_sarcastic` : 1 if the record is sarcastic otherwise 0\n\n`headline`     : the headline of the news article\n\n`article_link`: link to the original news article. Useful in collecting supplementary data\n\n---\n\n### Motivation\nGiven the rise of fake news on social media as well as on news outlets, it is important for people to identify sarcastic news reports from legitimate news reports. This will allow people to know when to take the news at face value and not spread false rumours.","4f5ade89":"### Conclusion of insights from EDA:\n- What are the frequencies of sarcastic headlines against the non-sarcastic headlines?\n\n\n<span style=\"color:blue\"> This is not an imbalanced class dataset. 56% of the headlines are non-sarcastic and 44% of the headlines are sarcastic. Hence, there is no requirements for conduct sampling to ensure an equal proportion of datasets from each class. <\/span>\n\n\n- What is the word length of headlines? For sarcastic and non-sarcastic headlines?\n\n\n<span style=\"color:blue\"> In the entire dataset, the mean and median length of headlines is around 10 words. There are some headlines with 2 \/ 3 \/ 4 words. These headlines need to be removed as they are too short. Short headlines will have a higher likelihood of being similar to other headlines without providing meaningful information in the topics.\n\nThere is no significant difference in the length of headlines for sarcastic and non-sarcastic datatset. <\/span>\n\n\n- Does the sarcastic headlines uses statistics (digits\/numbers) in their wording? Compared to non-sarcastic headlines?\n\n\n<span style=\"color:blue\"> In the entire dataset, 85% of all the headlines uses numbers. There is no significant difference in the use of numbers for sarcastic and non-sarcastic datatset. This suggests that most headlines uses numbers to attract viewership. <\/span>\n\n- Topic Modelling. What are the topics in news headline for entire\/sarcastic\/non-sarcastic datasets?\n\n- What are all the topics in the news headline? Most popular topic?\n\n\n<span style=\"color:blue\"> For topic modelling, based on the dataset, the higher the number of topics, the more specialized the topics become. We have performed pre-processing of the data and subsequently applied LDA to create 3 \/ 5 \/ 10 topics. LDA for 10 topics helps us to understand the topics better. For example, for LDA10, topic 8 is generally about social media. Topic 9 is about politicians, namely Donald Trump. Topic 2 is about children, violence, climate change. \nFor sarcastic and non-sarcastic datasets, the ratio of topics seems to be similar and there is no significant difference. The sizes of each topic is also similar. All these information suggests that sarcastic headlines is pervasive and appears throughout different genres of news.<\/span>\n\n- What are all the topics involved in the news headline which are sarcastic? Most popular topic?\n- What are all the topics involved in the news headline which are not sarcastic? Most popular topic?\n","761967ff":"<a name=\"cnn\"><\/a>\n### CNN\n\nFor the prediction of sarcasm of headlines, we will use CNN with the following architectures:\n\n- CNN with Conv1D","a28756bf":"# References:\n[1] Using LDA for topic extraction: https:\/\/towardsdatascience.com\/the-complete-guide-for-topics-extraction-in-python-a6aaa6cedbbc\n\n[2] Guidelines for the use of LDA: https:\/\/towardsdatascience.com\/the-complete-guide-for-topics-extraction-in-python-a6aaa6cedbbc\n\n[3] Topic modelling in python with nltk and gensim: https:\/\/towardsdatascience.com\/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21","c9e49ea3":"<a name=\"deeplearning\"><\/a>\n### Prediction of Sarcasm in headlines using Deep Learning methods\n\nFor the prediction of sarcasm of headlines, we will use RNN with the following architectures:\n\n- RNN with Gated Recurrent Units (with Lasso Regularization, Dropout, Batch Normalization)\n- RNN with Gated Recurrent Units (with Ridge Regularization, Dropout, Batch Normalization)\n- RNN with Long Short Term Memory Units (with Lasso Regularization, Dropout, Batch Normalization)\n- RNN with Long Short Term Memory Units (with Ridge Regularization, Dropout, Batch Normalization)\n- CNN with Conv1D\n- Combination of CNN-RNN (LSTM)\n    - Output from CNN with Conv1D is used as input for RNN with LSTM (with Lasso Regularization, Dropout, Batch Normalization)","de7f0fc4":"# Kaggle Link:\nhttps:\/\/www.kaggle.com\/rmisra\/news-headlines-dataset-for-sarcasm-detection\/kernels","9f75ef87":"<a name=\"lstm\"><\/a>\n### RNN with LSTM Architecture \n\nFor the prediction of sarcasm of headlines, we will use RNN with the following architectures:\n\n- RNN with Long Short Term Memory Units (with Lasso Regularization, Dropout, Batch Normalization)\n- RNN with Long Short Term Memory Units (with Ridge Regularization, Dropout, Batch Normalization)","1145d229":"<a name=\"cnnrnn\"><\/a>\n### CNN-RNN combined architecture\n\nFor the prediction of sarcasm of headlines, we will use the following architecture:\n\n- Combination of CNN-RNN (LSTM)\n   - Output from CNN with Conv1D is used as input for RNN with LSTM (with Lasso Regularization, Dropout)","cae3ee50":"<a name=\"gru\"><\/a>\n#### RNN with GRU","8868c1ce":"<a name=\"LDA\"><\/a>\n### Identifying Topics in dataset via LDA\n\n#### Background\nLDA is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions. Each document is modeled as a multinomial distribution of topics and each topic is modeled as a multinomial distribution of words. LDA assumes that the every chunk of text we feed into it will contain words that are somehow related. Therefore choosing the right corpus of data is crucial. It also assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. LDA assumes that each document (i.e. headline) consists of a mixture of topics (multinomial distribution) and each topic consists of a mixture of words (multinomial distribution).\n\n### Parameters\nLDA (short for Latent Dirichlet Allocation) is an unsupervised machine-learning model that takes documents as input and finds topics as output. The model also says in what percentage each document talks about each topic.\nThere are 3 main parameters of the model:\n- the number of topics\n- the number of words per topic\n- the number of topics per document\n\n### Data processing\nBefore we can perform LDA, we will need to process the text in the following steps:\n\n- Tokenization: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n- Removal of stop words.\n- Removing Headlines that contains very few words. This helps to reduce the likelihood of headlines (that comprise of very but commonly-used words) matching together.  \n- Lemmatization: words in third person are changed to first person and verbs in past and future tenses are changed into present.\n- Stemming: words are reduced to their root form.\n\nWhilst labelling each clusters give us an intuition of the meaning of each cluster,\nit is not necessary as the goal is not to label each document\/headline into a cluster, but to measure the similarity between 2 documents\/headlines.\nWe can do so simply using similarity measures like Jensen-Shanon distance matric.\n\n### Limitations\n- Need to pre-specify number of topics\/clusters in advance. \n- Heuristics to determine the optimal number of topics\/clusters is largely based on domain knowledge and human interpretability.\n- Cannot capture correlations between topics\/clusters.","0eb7d1fc":"### Table of Contents\n\n* [ Exploratory Data Analysis and Data Processing ](#EDA)\n\n* [ Identifying Topics in dataset via LDA ](#LDA)\n\n* [ Prediction via Deep Learning Methods](#deeplearning)\n    * [ RNN with GRU ](#gru)\n    * [ RNN with LSTM ](#lstm)\n    * [ CNN with Conv1D ](#cnn)\n    * [ Combined Architecture of CNN with RNN ](#cnnrnn)\n\nAll models are trained with Regularization, Dropout layer and Batch Normalization to stabilize and improve the performance of the models."}}