{"cell_type":{"1602f14f":"code","15e6bcfe":"code","cc95f14c":"code","781339c1":"code","5c5c0689":"code","26f5b852":"code","20563ab5":"code","cd3ee9e4":"code","cd8ee326":"markdown","fa78e249":"markdown","7e0808e8":"markdown","f4a150e6":"markdown","36d6fb13":"markdown"},"source":{"1602f14f":"# Basic libraries\nimport numpy as np\nimport pandas as pd\nimport random\nimport time\nimport datetime\nimport os\n\n# preprocessing\nfrom sklearn.model_selection import train_test_split\n\n# Deeplearning\nfrom tensorflow.keras.layers import Activation, Dropout, BatchNormalization, Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.utils import to_categorical\n# from tensorflow.keras.utils import np_utils\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","15e6bcfe":"# For bayesian optimization\n# Gpy\n!pip install Gpy","cc95f14c":"# Gpy opt\n!pip install GpyOpt","781339c1":"# import Gpy and GpyOpt\nimport GPy, GPyOpt","5c5c0689":"# Define mnist learning and evaluation class\nclass MNIST():\n    def __init__(self, input_=784, output_=10,\n                 layer1_out=512,\n                 layer2_out=512,\n                 layer1_drop=0.2,\n                 layer2_drop=0.2,\n                 batch_size=32,\n                 epochs=10,\n                 validation_split=0.1):\n        self.input_ = input_\n        self.output_ = output_\n        self.layer1_out = layer1_out\n        self.layer2_out = layer2_out\n        self.layer1_drop = layer1_drop\n        self.layer2_drop = layer2_drop\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.validation_split = validation_split\n        self.X_train, self.X_test, self.y_train, self.y_test = self.mnist_data()\n        self.model = self.mnist_model()\n        \n    # Mnist dataset\n    def mnist_data(self):\n        # dataset\n        train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\n        # train dataset\n        X = train.iloc[:,1:].values\n        X = X.astype(np.float32)\n        y = train[\"label\"].values\n        y = to_categorical(y)\n\n        # train and valid split\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n        \n        # img data\n        X_train, X_test = X_train\/255, X_test\/255\n        \n        return X_train, X_test, y_train, y_test\n    \n    # DNN model\n    def mnist_model(self):\n        model = Sequential()\n        model.add(Dense(self.layer1_out, input_shape=(self.input_, )))\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(self.layer1_drop))\n        model.add(Dense(self.layer2_out))\n        model.add(Activation('relu'))\n        model.add(Dropout(self.layer2_drop))\n        model.add(Dense(self.output_))\n        model.add(Activation('softmax'))\n        model.compile(loss='categorical_crossentropy',\n                      optimizer=Adam(),\n                      metrics=['accuracy'])\n        return model\n    \n    # fit mnist model\n    def mnist_fit(self):\n        # model file\n        time_stp = datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\")\n        # callbacks\n        es = EarlyStopping(patience=0, verbose=1)\n        mc = ModelCheckpoint(\"mnist_model_{}.h5\".format(time_stp),\n                             monitor=\"val_loss\",\n                             verbose=1,\n                             period=2,\n                             save_best_only=True)\n        \n        # save information of model into text file\n        with open(\"parameters_{}.txt\".format(time_stp), mode=\"w\") as f:\n            f.write(\"-\"*20)\n            f.write(\"mnist_model : {}\".format(time_stp))\n            f.write(\"layer1_out : {}\".format(self.layer1_out))\n            f.write(\"layer1_drop : {}\".format(self.layer1_drop))\n            f.write(\"layer2_out : {}\".format(self.layer2_out))\n            f.write(\"layer2_drop : {}\".format(self.layer2_drop))\n            f.write(\"validation_split : {}\".format(self.validation_split))\n            f.write(\"batch_size : {}\".format(self.batch_size))\n            f.write(\"epochs : {}\".format(self.epochs))\n            f.close()\n        \n        # fit\n        self.model.fit(self.X_train, self.y_train, \n                         batch_size=self.batch_size,\n                         epochs=self.epochs,\n                         verbose=0,\n                         validation_split=self.validation_split,\n                         callbacks=[es, mc])\n    # evaluation\n    def mnist_evaluate(self):\n        self.mnist_fit()\n        \n        evaluation = self.model.evaluate(self.X_test, self.y_test, batch_size=self.batch_size, verbose=0)\n        \n        return evaluation\n    \n# run\ndef run_mnist(input_=784, output_=10,\n              layer1_out=512,\n              layer2_out=512,\n              layer1_drop=0.2,\n              layer2_drop=0.2,\n              batch_size=16,\n              epochs=16,\n              validation_split=0.1):\n        \n    _mnist = MNIST(input_=input_, output_=output_,\n                  layer1_out=layer1_out,\n                  layer2_out=layer2_out,\n                  layer1_drop=layer1_drop,\n                  layer2_drop=layer2_drop,\n                  batch_size=batch_size,\n                  epochs=epochs,\n                  validation_split=validation_split)\n    \n    mnist_evaluation = _mnist.mnist_evaluate()\n    return mnist_evaluation","26f5b852":"# Bayesian Optimization\n# Define parmeter range\nbounds = [{\"name\":\"validation_split\", \"type\":\"continuous\", \"domain\":(0.1, 0.3)},\n          {\"name\":\"layer1_drop\", \"type\":\"continuous\", \"domain\":(0.0, 0.3)},\n          {'name': 'layer2_drop', 'type': 'continuous', 'domain': (0.0, 0.3)},\n          {'name': 'layer1_out', 'type': 'discrete', 'domain': (64, 128, 256, 512)},\n          {'name': 'layer2_out', 'type': 'discrete', 'domain': (64, 128, 256, 512)},\n          {'name': 'batch_size', 'type': 'discrete', 'domain': (32, 64, 128)},\n          {'name': 'epochs', 'type': 'discrete', 'domain': (10, 50, 100)}\n         ]\n\n# define function\ndef f(x):\n    t0 = time.time()\n    print(x)\n    evaluation = run_mnist(layer1_drop = float(x[:,1]), # input parameter to run execution function\n                           layer2_drop = float(x[:,2]), \n                           layer1_out = int(x[:,3]),\n                           layer2_out = int(x[:,4]), \n                           batch_size = int(x[:,5]), \n                           epochs = int(x[:,6]), \n                           validation_split = float(x[:,0]))\n    print(\"-\"*20)\n    print(\"loss:{0} \\t accuracy:{1}\".format(evaluation[0], evaluation[1]))\n    print(\"-\"*20)\n    print(evaluation)\n    t1 = time.time()\n    print(\"calc time:{}\".format(t1-t0))\n    return evaluation[0] # only one parameter","20563ab5":"%%time\n# primary explore\nopt_mnist = GPyOpt.methods.BayesianOptimization(f=f, domain=bounds) # f is optimization target function, bounds is parameter range.","cd3ee9e4":"%%time\n# bayesian optimization\nopt_mnist.run_optimization(max_iter=10)\n\n# last result\nprint(\"\")\nprint(\"Optimization result\")\nprint(\"optimized parameters: {}\".format(opt_mnist.x_opt))\nprint(\"optimized loss: {}\".format(opt_mnist.fx_opt))","cd8ee326":"# Mnist class","fa78e249":"In the label prediction task of mnist, hyperparameter optimization was performed by Bayesian optimization using GpyOpt. ","7e0808e8":"Define parameters for mnist data, load data, define DNN network, learn and evaluate functions as class.","f4a150e6":"# Parameter optimization by bayesian optimization with GPyOpt","36d6fb13":"## Libraries"}}