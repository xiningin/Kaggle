{"cell_type":{"2c8d6164":"code","7f4b24ed":"code","12e07d95":"code","3f7dea3b":"code","9947dba1":"code","080cf287":"code","66330c67":"code","52139b09":"code","63c4b534":"code","f6601676":"code","75c58201":"code","2805a50d":"code","54088313":"code","edf6de8f":"code","849198a6":"code","fde17cce":"code","b9a91cbd":"code","44920ef7":"code","4dd797d3":"code","9a9b845e":"code","99cf6731":"code","58d31bef":"code","0a94d821":"code","7e11fad2":"code","7e2fbbe4":"code","3b2bf3fd":"code","e6d91036":"code","3d6af6f6":"code","f857ef0e":"code","e2e74a77":"code","dd84e04f":"code","4e27ba45":"code","58f1cf9a":"code","fe40f6ad":"code","e62d2a21":"code","dd6b039c":"code","fe2de406":"code","4d23dcea":"code","32d7ada7":"code","a9440675":"code","207f5e6d":"code","bed1944d":"code","a7b12067":"code","84c0b6dc":"code","a7699e2b":"code","23d2b20d":"code","5289c06a":"code","adf50969":"code","d0bd73d1":"code","124a68db":"code","6797e611":"code","0042752e":"code","f2d9b3ff":"code","37c9cb14":"code","1f8b8458":"code","d2a72a3e":"code","5bd28501":"code","7af1de69":"code","444cfac8":"code","81dc863a":"code","c1e83a6c":"code","36b61e8d":"code","8a861b0c":"code","584cdda4":"code","5f46815c":"code","4188e274":"code","18c7b314":"code","ffae7b71":"code","eb918e12":"code","ffd197ec":"code","7c8ba9c4":"code","225a3bfa":"code","c24ce3e8":"code","36e3094e":"code","b8d33351":"code","c97fcfa4":"code","5c8cfea6":"code","2bd75e91":"code","cab1d6c8":"code","bb4a5f17":"code","6c89c864":"code","17888221":"code","30efce64":"code","a1105eb6":"code","cb7f3f18":"code","466ad63c":"code","3cadf1c2":"markdown","795647b8":"markdown","0dddb994":"markdown","da4be65e":"markdown","be91ca8e":"markdown","073c3fb2":"markdown","7073e9c3":"markdown","2a9e646d":"markdown","6707f096":"markdown","0592f59d":"markdown","1c715fb5":"markdown","2e286da6":"markdown","2467c2ef":"markdown","55470bb8":"markdown","4b6da04c":"markdown","d60ad0e5":"markdown","128f3639":"markdown","3c375e1f":"markdown","ba823092":"markdown","e71613ee":"markdown","6e6c1741":"markdown","22f3d8f5":"markdown","a9b6d99e":"markdown","ed0cde43":"markdown","698694e4":"markdown","34aa67bf":"markdown","61f5e693":"markdown","6eaf9338":"markdown","72325da4":"markdown","4231fdbd":"markdown","362564c5":"markdown","630f253f":"markdown","d1c9ad63":"markdown","eeb07cfc":"markdown","f0ad45b7":"markdown","50957940":"markdown","a936f561":"markdown","a18092aa":"markdown","f49d2408":"markdown","0e66c2ae":"markdown","f6dd315a":"markdown","bc135a0b":"markdown","97a19b9a":"markdown","6bb505c4":"markdown","addd051d":"markdown","68beb18d":"markdown","b18ace86":"markdown","6202e42a":"markdown","403b9928":"markdown","831f79a5":"markdown","0caeb3bf":"markdown","7479e616":"markdown","630ce12e":"markdown","8cc1d812":"markdown","e8133dec":"markdown","d767f345":"markdown","a63c9a97":"markdown","9605f35a":"markdown","bfcb30a3":"markdown","57ea40ae":"markdown","3e528cbb":"markdown","d3e3b68a":"markdown","17cea2c9":"markdown","93032ba0":"markdown","a666fece":"markdown","e01661c0":"markdown","7fc708be":"markdown","45347450":"markdown","fc59f608":"markdown","6ea86681":"markdown","f2efedff":"markdown","6ba35d49":"markdown","8d31d16f":"markdown","ac2abfd8":"markdown"},"source":{"2c8d6164":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7f4b24ed":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","12e07d95":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    #filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()","3f7dea3b":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()","9947dba1":"import pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency, normaltest","080cf287":"#Wczytanie bazy danych do lokalnej zmiennej\ndata = pd.read_csv(\"..\/input\/videogamesales\/vgsales.csv\")\ndata.head()","66330c67":"data.info()","52139b09":"data.isna().sum()","63c4b534":"quant_col = ['NA_Sales', 'EU_Sales', 'JP_Sales','Other_Sales', 'Global_Sales']\nquant_stats = data[quant_col].agg([\"count\",\"mean\",\"median\",\"min\", \"max\", \"std\", \"var\",])\nquant_stats = quant_stats.append(data[quant_col].mode().rename(index={0:\"mode\"}))\nquant_stats","f6601676":"quali_cols = ['Name', 'Platform','Year', 'Genre']\nfig, axes = plt.subplots(len(quali_cols), 1, figsize=(10,30))\nfor i, col in enumerate(quali_cols):\n    axes[i].set_title(f\"Wykres rozk\u0142adu liczno\u015bci dla {col}\")\n    sns.histplot(data[col], ax=axes[i])\n    print(f\"\"\"Dla kolumny {col} tabela liczno\u015bci wygl\u0105da nast\u0119puj\u0105co:\\n\\n\\n{pd.DataFrame(data[col].value_counts())}\\n\"\"\")","75c58201":"plotPerColumnDistribution(data, 10, 5)","2805a50d":"plotCorrelationMatrix(data, 8)","54088313":"plotScatterMatrix(data, 18, 10)","edf6de8f":"mean = data['Year'].mean()\ndata['Year'].fillna(mean, inplace=True)\nplt.xticks(rotation = 75)\nx_axis = data['Year'].astype(int)\nsns.countplot(x= x_axis, data = data)\nplt.title('Total Game Sales Each Year')\nplt.show()","849198a6":"plt.xticks(rotation = 75)\nx_axis = data['Genre']\nsns.countplot(x= x_axis, data = data)\nplt.title('Total Game Sales of Each Genre')\nplt.show()","fde17cce":"game_sales = data[['Genre','Name']].groupby(['Genre']).agg(lambda x:x.value_counts().index[0])\ngenre_games = game_sales.rename(columns = {'Name' : 'Game'}, inplace = False)\ngenre_games","b9a91cbd":"plt.xticks(rotation = 75)\nx_axis = data['Platform']\nsns.countplot(x= x_axis, data = data)\nplt.title('Total Game Sales on Each Platform')\nplt.show()","44920ef7":"correlation_vg_sales = data.corr()\n\naxis_corr = sns.heatmap(\ncorrelation_vg_sales,\nvmin=-1, vmax=1, center=0,\ncmap=sns.diverging_palette(50, 500, n=500),\nsquare=True\n)\n\nplt.show()","4dd797d3":"data['Publisher'] = data['Publisher'].fillna('Unknown')\nnumber_df = data.groupby('Publisher')[['Name']].count().sort_values('Name', ascending = False).head(50)\nnumber_clean = number_df.rename(columns = {'Name' : 'Number'}, inplace = False)\nnumber_clean","9a9b845e":"number_df.plot(kind = 'bar', figsize = (25, 10));\nplt.xlabel('Publisher', fontsize = 20);\nplt.ylabel('Number of video games released', fontsize = 20);\nplt.title('Top Publishers of Games', fontsize = 40)","99cf6731":"top_games_NA = data.sort_values('NA_Sales',ascending = False).head(5)\nexplode = [0.1, 0, 0, 0, 0]\nplt.pie(top_games_NA['NA_Sales'], labels = top_games_NA['Name'], explode = explode)\nplt.show()","58d31bef":"top_games_EU = data.sort_values('EU_Sales',ascending = False).head(5)\nexplode = [0.1, 0, 0, 0, 0]\nplt.pie(top_games_EU['EU_Sales'], labels = top_games_EU['Name'], explode = explode)\nplt.show()","0a94d821":"top_games_JP = data.sort_values('JP_Sales',ascending = False).head(5)\nexplode = [0.1, 0, 0, 0, 0]\nplt.pie(top_games_JP['JP_Sales'], labels = top_games_JP['Name'], explode = explode)\nplt.show()","7e11fad2":"top_games_Other = data.sort_values('Other_Sales',ascending = False).head(5)\nexplode = [0.1, 0, 0, 0, 0]\nplt.pie(top_games_Other['Other_Sales'], labels = top_games_Other['Name'], explode = explode)\nplt.show()","7e2fbbe4":"top_games_Global = data.sort_values('Global_Sales',ascending = False).head(5)\nexplode = [0.1, 0, 0, 0, 0]\nplt.pie(top_games_Global['Global_Sales'], labels = top_games_Global['Name'], explode = explode)\nplt.show()","3b2bf3fd":"plt.figure(figsize=(12,5))\nplt.scatter(data.Genre,data.NA_Sales,color='b',alpha=.5)\nplt.xlabel('Typ gier')           \nplt.ylabel('Ilo\u015b\u0107')\nplt.title('Rozk\u0142ad ')           \nplt.show()","e6d91036":"total_sales = pd.DataFrame({'Country':['NA', 'EU', 'JP', 'Other'], 'Sales':[sum(data['NA_Sales']), sum(data['EU_Sales']), sum(data['JP_Sales']), sum(data['Other_Sales'])]})\ntotal_sales['Percentages'] = total_sales['Sales']\/sum(total_sales['Sales'])*100\ntotal_sales.sort_values(by='Percentages', inplace=True)","3d6af6f6":"fig = plt.figure(facecolor='whitesmoke')\n\naxes1 = fig.add_axes([0, 0, 1, 1])\naxes2 = fig.add_axes([0.7, 0, 1, 1])\naxes3 = fig.add_axes([1.4, 0, 1, 1])\naxes4 = fig.add_axes([2.1, 0, 1, 1])\n\naxes1.pie([total_sales['Percentages'][0], 100-total_sales['Percentages'][0]], startangle=180,\n         colors=['crimson', 'white'])\naxes1.text(-0.27, -0.85,f\"{round(total_sales['Percentages'][0],2)}%\", fontweight='bold', fontsize=16)\naxes1.text(-0.5, 1.3, 'North America', fontweight='bold', fontsize=16)\n\naxes2.pie([total_sales['Percentages'][1], 100-total_sales['Percentages'][1]], startangle=-4,\n         colors=['crimson', 'white'])\naxes2.text(0.26, 0.355, f\"{round(total_sales['Percentages'][1],2)}%\", fontweight='bold', fontsize=16)\naxes2.text(-0.3, 1.3, 'Europe', fontweight='bold', fontsize=16)\n\naxes3.pie([total_sales['Percentages'][2], 100-total_sales['Percentages'][2]], startangle=95,\n         colors=['crimson', 'white'])\naxes3.text(-0.62, 0.5, f\"{round(total_sales['Percentages'][2],2)}%\", fontweight='bold', fontsize=16)\naxes3.text(-0.25, 1.3, 'Japan', fontweight='bold', fontsize=16)\n\naxes4.pie([total_sales['Percentages'][3], 100-total_sales['Percentages'][3]], startangle=130,\n         colors=['crimson', 'white'])\naxes4.text(-0.77, 0.3, f\"{round(total_sales['Percentages'][3],2)}%\", fontweight='bold', fontsize=16)\naxes4.text(-0.5, 1.3, 'Other countries', fontweight='bold', fontsize=16)\n\naxes2.text(-0.3,1.8, 'Percentage of sales by country', fontweight='bold', color='crimson', fontsize=24)\nfig.show()","f857ef0e":"sales_by_year = data.groupby('Year')[['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales']].sum().reset_index()\n\nfig = plt.figure(facecolor='whitesmoke', figsize=(6,5))\nsns.set_style('white')\naxes1 = fig.add_axes([0, 0, 1.5, 1.5]) \naxes2 = fig.add_axes([1.6, 0, 1, 1.5]) \naxes3 = fig.add_axes([0, -1.8, 1.5, 1.5])\naxes4 = fig.add_axes([1.6, -1.8, 1, 1.5]) \n\n# figure\nsns.lineplot(x=sales_by_year['Year'], y=sales_by_year['NA_Sales'],color='crimson',lw=4, ax=axes1)\naxes1.scatter(x=[2000, 2003, 2012, 2013, 2008], y=[94.49, 193.59, 154.93, 154.77, 351.44], color='black', lw=4)\naxes1.set_facecolor('whitesmoke')\naxes1.set_xlabel('Year', fontsize=18, color='black')\naxes1.set_ylabel('Sales', fontsize=18, color='black')\naxes1.text(1979, 340, 'Sales in', color='black', fontsize=20, fontweight='bold')\naxes1.text(1985, 340, 'North America', color='crimson', fontsize=24, fontweight='bold')\n\n# growth\naxes1.annotate('', xy=(2006.9, 350), xytext=(1993.6, 60),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('Strong growth with 2 downturns in', xy=(1993.6, 100), xytext=(1993.6, 100), rotation=61.5, color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2000', xy=(2000.6, 250), xytext=(2000.6, 250), rotation=61.5, color='crimson', fontweight='bold', fontsize=12)\naxes1.annotate('and', xy=(2001.8, 276), xytext=(2001.8, 276), rotation=61.5, color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2002', xy=(2002.8, 298), xytext=(2002.8, 298), rotation=61.5, color='crimson', fontweight='bold', fontsize=12)\n\n\n# stagnation\naxes1.annotate('', xy=(2012, 149), xytext=(2008, 100),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('', xy=(2013, 149), xytext=(2012, 100),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('Stagnation in', xy=(2000.5, 90), xytext=(2000.5, 90), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2012', xy=(2006.3, 90), xytext=(2006.3, 90),color='crimson', fontweight='bold', fontsize=12)\naxes1.annotate('and', xy=(2008.5, 90), xytext=(2008.5, 90),color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2013', xy=(2010.3, 90), xytext=(2010.3, 90),color='crimson', fontweight='bold', fontsize=12)\n\n# max\naxes1.annotate('', xy=(2009, 353.8), xytext=(2015, 325),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes1.annotate('Maximum sales', xy=(2009, 353.8), xytext=(2011, 317.5), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('in', xy=(2011, 303.5), xytext=(2011, 303.5), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2008', xy=(2012, 303.5), xytext=(2012, 303.5), color='crimson', fontweight='bold', fontsize=12)\n\n# conclusion \u2116 1\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'On the chart we can see that', color='black', fontsize=14)\naxes2.text(0.48, 0.8, 'sales before 1996', color='crimson', fontsize=14)\naxes2.text(0, 0.75, 'were highly volatile -', color='black', fontsize=14)\naxes2.text(0.35, 0.75, 'growth was followed by donwturn.', color='crimson', fontsize=14)\naxes2.text(0, 0.7, 'After 1996', color='crimson', fontsize=14)\naxes2.text(0.18, 0.7, 'we observe', color='black', fontsize=14)\naxes2.text(0.375, 0.7, 'strong growth', color='crimson', fontsize=14)\naxes2.text(0.6, 0.7, ', but were two ', color='black', fontsize=14)\naxes2.text(0, 0.65, 'downturns in 2000 and 2002.', color='crimson', fontsize=14)\naxes2.text(0, 0.6, 'It should be noted that', color='black', fontsize=14)\naxes2.text(0.383, 0.6, 'in 2008', color='crimson', fontsize=14)\naxes2.text(0.518, 0.6, 'were the most sales -', color='black', fontsize=14)\naxes2.text(0, 0.55, '351.44 millions.', color='crimson', fontsize=14)\naxes2.text(0, 0.5, 'After 2008', color='crimson', fontsize=14)\naxes2.text(0.185, 0.5, 'we can see a', color='black', fontsize=14)\naxes2.text(0.41, 0.5, 'strong decrease in sales.', color='crimson', fontsize=14)\naxes2.text(0, 0.45, 'In the period', color='black', fontsize=14)\naxes2.text(0.22, 0.45, '2012 - 2013', color='crimson', fontsize=14)\naxes2.text(0.43, 0.45, 'was', color='black', fontsize=14)\naxes2.text(0.51, 0.45, 'stagnation.', color='crimson', fontsize=14)\n\n# calculation of sales growth rates\nlist_NA = []\nfor n in range(1, 37):\n    d = ((sales_by_year['NA_Sales'][n] - sales_by_year['NA_Sales'][n-1])\/sales_by_year['NA_Sales'][n-1])*100\n    list_NA.append(d)\n\n# visualisation growth rates sales in NA by years\ncolors = ['crimson' if _ > 0 else 'darksalmon' for _ in list_NA]\naxes3.bar(height=list_NA, x=sales_by_year['Year'][range(1, 37)], color=colors)\nfor p in axes3.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy()\n    if height >= 0:\n        axes3.annotate('{:.0f}'.format(height), (x + width\/2, y + height*1.02), ha='center')\n    elif height <= -55:\n        axes3.annotate('{:.0f}'.format(height), (x + width\/2, y + height*1.2), ha='center')\n    else:\n        axes3.annotate('{:.0f}'.format(height), (x + width\/2, y + height*1.4), ha='center')\naxes3.set_facecolor('whitesmoke')\naxes3.set_xlabel('Year', fontsize=18, color='black')\naxes3.set_ylabel('Growth rate, %', fontsize=18, color='black')\naxes3.text(1997, 315, 'Growth rate sales(%) in', color='black', fontsize=20, fontweight='bold')\naxes3.text(2001.39, 280, 'North America', color='crimson', fontsize=24, fontweight='bold')\n\n# max +\naxes3.annotate('', xy=(1985, 330), xytext=(1992, 285),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes3.annotate('Max', xy=(1990, 268), xytext=(1990, 268), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('+', xy=(1991.8, 268), xytext=(1991.8, 268), color='crimson', fontweight='bold', fontsize=24)\n\n# max -\naxes3.annotate('', xy=(2015, -79), xytext=(2008, -63),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes3.annotate('Max', xy=(2006, -46), xytext=(2006, -46), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('-', xy=(2007.8, -46), xytext=(2007.8, -46), color='darksalmon', fontweight='bold', fontsize=24)\n\n# conclusion \u2116 2\naxes4.set_facecolor('whitesmoke')\naxes4.axis('off')\naxes4.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes4.text(0, 0.8, 'We can see that', color='black', fontsize=14)\naxes4.text(0.27, 0.8, 'maximum positive growth rate', color='crimson', fontsize=14)\naxes4.text(0, 0.75, 'sales in NA was', color='black', fontsize=14)\naxes4.text(0.27, 0.75, 'in 1983 - 1984', color='crimson', fontsize=14)\naxes4.text(0.52, 0.75, 'and', color='black', fontsize=14)\naxes4.text(0.6, 0.75, 'maximum negative', color='crimson', fontsize=14)\naxes4.text(0, 0.70, 'growth rate', color='crimson', fontsize=14)\naxes4.text(0.21, 0.70, 'sales in NA was', color='black', fontsize=14)\naxes4.text(0.48, 0.70, 'in 2015 - 2016.', color='crimson', fontsize=14)\naxes4.text(0, 0.65, 'Average growth rates by period:', color='crimson', fontsize=14)\naxes4.text(0, 0.60, f'* 1980-1984 - {round(statistics.mean(list_NA[:5]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.55, f'* 1985-1989 - {round(statistics.mean(list_NA[5:10]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.50, f'* 1990-1994 - {round(statistics.mean(list_NA[10:15]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.45, f'* 1995-1999 - {round(statistics.mean(list_NA[15:20]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.40, f'* 2000-2004 - {round(statistics.mean(list_NA[20:25]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.35, f'* 2005-2009 - {round(statistics.mean(list_NA[25:30]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.30, f'* 2010-2016 - {round(statistics.mean(list_NA[30:]),2)}%.', color='black', fontsize=14)\nfig.show()","e2e74a77":"fig = plt.figure(facecolor='whitesmoke')\nsns.set_style('white')\naxes1 = fig.add_axes([0, 0, 1.5, 1.5]) \naxes2 = fig.add_axes([1.6, 0, 1, 1.5]) \naxes3 = fig.add_axes([0, -1.8, 1.5, 1.5]) \naxes4 = fig.add_axes([1.6, -1.8, 1, 1.5]) \n\nsns.lineplot(x=sales_by_year['Year'], y=sales_by_year['EU_Sales'],color='crimson',lw=4, ax=axes1)\naxes1.scatter(x=[2000, 2003, 2009, 2013, 2014], y=[52.75, 103.81, 191.59, 125.80, 125.63], color='black', lw=4)\naxes1.set_facecolor('whitesmoke')\naxes1.set_xlabel('Year', color='black', fontsize=18)\naxes1.set_ylabel('Sales', color='black', fontsize=18)\naxes1.text(1979, 180, 'Sales in', color='black', fontsize=20, fontweight='bold')\naxes1.text(1985, 180, 'Europe', color='crimson', fontsize=24, fontweight='bold')\n\n# growth\naxes1.annotate('', xy=(2007, 193), xytext=(1993, 23),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('Strong growth with 2 downturns in', xy=(1992.4, 40), xytext=(1992.4, 40), rotation=56, color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2000', xy=(2000.7, 139.1), xytext=(2000.7, 139.1), rotation=56, color='crimson', fontweight='bold', fontsize=12)\naxes1.annotate('and', xy=(2001.9, 154.2), xytext=(2001.9, 154.2), rotation=56, color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2003', xy=(2003, 166.7), xytext=(2003, 166.7), rotation=56, color='crimson', fontweight='bold', fontsize=12)\n\n# stagnation\naxes1.annotate('', xy=(2012.4, 116), xytext=(2008, 71),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('', xy=(2013.4, 116), xytext=(2012, 71),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('Stagnation in', xy=(2000.5, 61), xytext=(2000.5, 61), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2013', xy=(2006.3, 61), xytext=(2006.3, 61),color='crimson', fontweight='bold', fontsize=12)\naxes1.annotate('and', xy=(2008.5, 61), xytext=(2008.5, 61),color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2014', xy=(2010.3, 61), xytext=(2010.3, 61),color='crimson', fontweight='bold', fontsize=12)\n\n# max\naxes1.annotate('', xy=(2010, 194), xytext=(2016, 169.2),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes1.annotate('Maximum sales', xy=(2011.5, 161.7), xytext=(2011.5, 161.7), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('in', xy=(2011.5, 152), xytext=(2011.5, 152), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2009', xy=(2012.5, 152), xytext=(2012.5, 152), color='crimson', fontweight='bold', fontsize=12)\n\n# conclusion \u2116 1\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'On the chart we can see', color='black', fontsize=14)\naxes2.text(0.41, 0.8, 'strong growth after 1994', color='crimson', fontsize=14)\naxes2.text(0.825, 0.8, 'and two', color='black', fontsize=14)\naxes2.text(0, 0.75, 'downturns in 2000 and 2003.', color='crimson', fontsize=14)\naxes2.text(0, 0.7, 'And', color='black', fontsize=14)\naxes2.text(0.07, 0.7, 'max sales', color='crimson', fontsize=14)\naxes2.text(0.245, 0.7, 'were', color='black', fontsize=14)\naxes2.text(0.333, 0.7, 'in 2009.', color='crimson', fontsize=14)\naxes2.text(0.472, 0.7, 'We observe', color='black', fontsize=14)\naxes2.text(0.675, 0.7, 'decrease sales', color='crimson', fontsize=14)\naxes2.text(0, 0.65, 'after 2009', color='crimson', fontsize=14)\naxes2.text(0.18, 0.65, 'with a', color='black', fontsize=14)\naxes2.text(0.29, 0.65, 'small increase in 2011-2012.', color='crimson', fontsize=14)\naxes2.text(0, 0.6, 'In period', color='black', fontsize=14)\naxes2.text(0.155, 0.6, '2013 - 2014 sales almost unchanged.', color='crimson', fontsize=14)\naxes2.text(0, 0.55, '----------------------------------------------------------------------------------------', color='black', fontsize=14)\naxes2.text(0, 0.50, 'In comprasion with NA', color='crimson', fontsize=14)\naxes2.text(0.375, 0.50, 'we can say that the', color='black', fontsize=14)\naxes2.text(0.71, 0.50, 'situation', color='crimson', fontsize=14)\naxes2.text(0, 0.45, 'is almost', color='black', fontsize=14)\naxes2.text(0.16, 0.45, 'identical.', color='crimson', fontsize=14)\naxes2.text(0, 0.4, 'Interestingly that', color='black', fontsize=14)\naxes2.text(0.3, 0.4, 'maximum sales, decrease and stagnation', color='crimson', fontsize=14)\naxes2.text(0, 0.35, 'in EU', color='crimson', fontsize=14)\naxes2.text(0, 0.35, 'in EU', color='crimson', fontsize=14)\naxes2.text(0.1, 0.35, 'were a', color='black', fontsize=14)\naxes2.text(0.22, 0.35, 'year later', color='crimson', fontsize=14)\naxes2.text(0.39, 0.35, 'tnan', color='black', fontsize=14)\naxes2.text(0.47, 0.35, 'in NA.', color='crimson', fontsize=14)\nfig.show()\n\n# calculation of sales growth rates\nlist_EU = []\nfor n in range(1, 37):\n    d = ((sales_by_year['EU_Sales'][n] - sales_by_year['EU_Sales'][n-1])\/sales_by_year['EU_Sales'][n-1])*100\n    list_EU.append(d)\n\n# visualisation growth rates sales in EU by years\ncolors = ['crimson' if _ > 0 else 'darksalmon' for _ in list_EU]\naxes3.bar(height=list_EU, x=sales_by_year['Year'][range(1, 37)], color=colors)\nfor p in axes3.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy()\n    if height >= 0:\n        axes3.annotate('{:.0f}'.format(height), (x + width\/2, y + height*1.02), ha='center')\n    elif height <= -55:\n        axes3.annotate('{:.0f}'.format(height), (x + width\/2, y + height*1.2), ha='center')\n    else:\n        axes3.annotate('{:.0f}'.format(height), (x + width\/2, y + height*1.4), ha='center')\naxes3.set_facecolor('whitesmoke')\naxes3.set_xlabel('Year', fontsize=18, color='black')\naxes3.set_ylabel('Growth rate, %', fontsize=18, color='black')\naxes3.text(1997, 315, 'Growth rate sales(%) in', color='black', fontsize=20, fontweight='bold')\naxes3.text(2007.48, 280, 'Europe', color='crimson', fontsize=24, fontweight='bold')\n\n# max +\naxes3.annotate('', xy=(1989, 368), xytext=(1996, 353),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes3.annotate('Max', xy=(1994, 336), xytext=(1994, 336), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('+', xy=(1995.8, 336), xytext=(1995.8, 336), color='crimson', fontweight='bold', fontsize=24)\n\n# max -\naxes3.annotate('', xy=(2015, -73), xytext=(2008, -57),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes3.annotate('Max', xy=(2006, -40), xytext=(2006, -40), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('-', xy=(2007.8, -40), xytext=(2007.8, -40), color='darksalmon', fontweight='bold', fontsize=24)\n\n# conclusion \u2116 2\naxes4.set_facecolor('whitesmoke')\naxes4.axis('off')\naxes4.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes4.text(0, 0.8, 'From chart we can see that', color='black', fontsize=14)\naxes4.text(0.455, 0.8, 'maximum positive growth', color='crimson', fontsize=14)\naxes4.text(0, 0.75, 'was', color='black', fontsize=14)\naxes4.text(0.07, 0.75, 'in 1987 - 1988,', color='crimson', fontsize=14)\naxes4.text(0.32, 0.75, 'and', color='black', fontsize=14)\naxes4.text(0.395, 0.75, 'negative in 2015 - 2016.', color='crimson', fontsize=14)\naxes4.text(0, 0.7, 'Average growth rates by period:', color='crimson', fontsize=14)\naxes4.text(0, 0.65, f'* 1980-1984 - {round(statistics.mean(list_EU[:5]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.6, f'* 1985-1989 - {round(statistics.mean(list_EU[5:10]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.55, f'* 1990-1994 - {round(statistics.mean(list_EU[10:15]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.5, f'* 1995-1999 - {round(statistics.mean(list_EU[15:20]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.45, f'* 2000-2004 - {round(statistics.mean(list_EU[20:25]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.4, f'* 2005-2009 -{round(statistics.mean(list_EU[25:30]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.35, f'* 2010-2016 - {round(statistics.mean(list_EU[30:]),2)}%.', color='black', fontsize=14)\nfig.show()","dd84e04f":"fig = plt.figure(facecolor='whitesmoke')\nsns.set_style('white')\naxes1 = fig.add_axes([0, 0, 1.5, 1.5]) \naxes2 = fig.add_axes([1.6, 0, 1, 1.5]) \naxes3 = fig.add_axes([0, -1.8, 1.5, 1.5]) \naxes4 = fig.add_axes([1.6, -1.8, 1, 1.5]) \n\nsns.lineplot(x=sales_by_year['Year'], y=sales_by_year['JP_Sales'],color='crimson',lw=4, ax=axes1)\naxes1.scatter(x=[1996, 2003, 2006], y=[57.44,34.20, 73.73], color='black', lw=4)\naxes1.set_facecolor('whitesmoke')\naxes1.set_xlabel('Year', color='black', fontsize=18)\naxes1.set_ylabel('Sales', color='black', fontsize=18)\naxes1.text(1979, 70, 'Sales in', color='black', fontsize=20, fontweight='bold')\naxes1.text(1985, 70, 'Japan', color='crimson', fontsize=24, fontweight='bold')\n\n# 1st max\naxes1.annotate('', xy=(1995, 57.44), xytext=(1988, 35.6),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=-.3'))\naxes1.annotate('1st max sales in', xy=(1982, 32.6), xytext=(1982, 32.6), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('1996', xy=(1989, 32.6), xytext=(1989, 32.6), color='crimson', fontweight='bold', fontsize=12)\n\n# 2nd max\naxes1.annotate('', xy=(2007, 74.23), xytext=(2016, 64.73),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes1.annotate('2nd max sales in', xy=(2009.5, 61.6), xytext=(2009.5, 61.6), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2006', xy=(2014.4, 59), xytext=(2014.4, 59), color='crimson', fontweight='bold', fontsize=12)\n\n# downturns\naxes1.annotate('', xy=(2003, 33.6), xytext=(2003, 20.2),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=0'))\naxes1.annotate('Downturn in', xy=(1997, 18.8), xytext=(1999, 18.8), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2003', xy=(2004.3, 18.8), xytext=(2004.3, 18.8), color='crimson', fontweight='bold', fontsize=12)\n\n# conclusion \u2116 1\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'On the chart we see', color='black', fontsize=14)\naxes2.text(0.34, 0.8, '2 periods of growth', color='crimson', fontsize=14)\naxes2.text(0.67, 0.8, 'the maximum', color='black', fontsize=14)\naxes2.text(0, 0.75, 'points of which were', color='black', fontsize=14)\naxes2.text(0.35, 0.75, 'in 1996 and 2006.', color='crimson', fontsize=14)\naxes2.text(0, 0.7, 'After 1993 was', color='black', fontsize=14)\naxes2.text(0.255, 0.7, 'strong donwturn,', color='crimson', fontsize=14)\naxes2.text(0.545, 0.7, 'which stopped', color='black', fontsize=14)\naxes2.text(0, 0.65, 'in 2003.', color='crimson', fontsize=14)\naxes2.text(0, 0.6, '----------------------------------------------------------------------------------------', color='black', fontsize=14)\naxes2.text(0, 0.55, 'In comparison with the previous charts', color='black', fontsize=14)\naxes2.text(0.65, 0.55, 'this chart is', color='crimson', fontsize=14)\naxes2.text(0, 0.5, 'more variable.', color='crimson', fontsize=14)\naxes2.text(0.25, 0.5, 'We observe', color='black', fontsize=14)\naxes2.text(0.45, 0.5, '2 \"ridges\" with maximum sales.', color='crimson', fontsize=14)\n\n# calculation of sales growth rates\nlist_JP = []\nfor n in range(4, 37):\n    d = ((sales_by_year['JP_Sales'][n] - sales_by_year['JP_Sales'][n-1])\/sales_by_year['JP_Sales'][n-1])*100\n    list_JP.append(d)\n\n# visualisation growth rates sales in EU by years\ncolors = ['crimson' if _ > 0 else 'darksalmon' for _ in list_JP]\naxes3.bar(height=list_JP, x=sales_by_year['Year'][range(4, 37)], color=colors)\nfor p in axes3.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy()\n    if height >= 0:\n        axes3.annotate('{:.0f}'.format(height), (x + width\/2, y + height*1.02), ha='center')\n    elif height <= -40:\n        axes3.annotate('{:.0f}'.format(height), (x + width\/2, y + height*1.1), ha='center')\n    else:\n        axes3.annotate('{:.0f}'.format(height), (x + width\/2, y + height*1.4), ha='center')\naxes3.set_facecolor('whitesmoke')\naxes3.set_xlabel('Year', fontsize=18, color='black')\naxes3.set_ylabel('Growth rate, %', fontsize=18, color='black')\naxes3.text(1997, 95, 'Growth rate sales(%) in', color='black', fontsize=20, fontweight='bold')\naxes3.text(2007.68, 83, 'Japan', color='crimson', fontsize=24, fontweight='bold')\n\n# max +\naxes3.annotate('', xy=(1990, 97), xytext=(1986, 73),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=-.3'))\naxes3.annotate('Max', xy=(1985, 69), xytext=(1985, 69), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('+', xy=(1986.5, 69), xytext=(1986.5, 69), color='crimson', fontweight='bold', fontsize=24)\n\n# max -\naxes3.annotate('', xy=(2015, -60), xytext=(2010, -52),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes3.annotate('Max', xy=(2009, -51), xytext=(2009, -51), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('-', xy=(2010.8, -51), xytext=(2010.8, -51), color='darksalmon', fontweight='bold', fontsize=24)\n\n# conclusion \u2116 2\naxes4.set_facecolor('whitesmoke')\naxes4.axis('off')\naxes4.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes4.text(0, 0.8, 'From chart we can see that', color='black', fontsize=14)\naxes4.text(0.455, 0.8, 'maximum positive growth', color='crimson', fontsize=14)\naxes4.text(0, 0.75, 'was', color='black', fontsize=14)\naxes4.text(0.07, 0.75, 'in 1991 - 1992,', color='crimson', fontsize=14)\naxes4.text(0.32, 0.75, 'and', color='black', fontsize=14)\naxes4.text(0.395, 0.75, 'negative in 2015 - 2016.', color='crimson', fontsize=14)\naxes4.text(0, 0.7, 'Average growth rates by period:', color='crimson', fontsize=14)\naxes4.text(0, 0.65, f'* 1983-1984 - {round(statistics.mean(list_JP[:1]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.6, f'* 1985-1989 - {round(statistics.mean(list_JP[1:6]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.55, f'* 1990-1994 - {round(statistics.mean(list_JP[6:11]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.5, f'* 1995-1999 - {round(statistics.mean(list_JP[11:16]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.45, f'* 2000-2004 - {round(statistics.mean(list_JP[16:21]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.4, f'* 2005-2009 - {round(statistics.mean(list_JP[21:26]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.35, f'* 2010-2016 - {round(statistics.mean(list_JP[26:]),2)}%.', color='black', fontsize=14)\nfig.show()\n","4e27ba45":"fig = plt.figure(facecolor='whitesmoke')\nsns.set_style('white')\naxes1 = fig.add_axes([0, 0, 1.5, 1.5]) \naxes2 = fig.add_axes([1.6, 0, 1, 1.5]) \naxes3 = fig.add_axes([0, -1.8, 1.5, 1.5]) \naxes4 = fig.add_axes([1.6, -1.8, 1, 1.5]) \n\nsns.lineplot(x=sales_by_year['Year'], y=sales_by_year['Other_Sales'],color='crimson',lw=4, ax=axes1)\naxes1.scatter(x=[1999, 2003, 2005, 2008, 2013, 2014], y=[10.05, 26.01, 40.55, 82.39, 39.82, 40.02], color='black', lw=4)\naxes1.set_facecolor('whitesmoke')\naxes1.set_xlabel('Year', color='black', fontsize=18)\naxes1.set_ylabel('Sales', color='black', fontsize=18)\naxes1.text(1979, 80, 'Sales in', color='black', fontsize=20, fontweight='bold')\naxes1.text(1985, 80, 'Other countries', color='crimson', fontsize=24, fontweight='bold')\n\n# growth\naxes1.annotate('', xy=(2006, 81), xytext=(1995, 7),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('Strong growth with 3 downturns in', xy=(1994.1, 10), xytext=(1994.1, 10), rotation=62.5, color='black', fontweight='bold', fontsize=12)\naxes1.annotate('1999, 2003', xy=(2001, 55.5), xytext=(2001, 55.5), rotation=62.5, color='crimson', fontweight='bold', fontsize=12)\naxes1.annotate('and', xy=(2003.2, 70.6), xytext=(2003.2, 70.6), rotation=62.5, color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2005', xy=(2004, 76.4), xytext=(2004, 76.4), rotation=62.5, color='crimson', fontweight='bold', fontsize=12)\n\n# stagnation\naxes1.annotate('', xy=(2012.9, 38.5), xytext=(2008, 21),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('', xy=(2013.9, 38.5), xytext=(2012, 21),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('Stagnation in', xy=(2001.5, 18.5), xytext=(2001.5, 18.5), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2013', xy=(2007.35, 18.5), xytext=(2007.35, 18.5),color='crimson', fontweight='bold', fontsize=12)\naxes1.annotate('and', xy=(2009.5, 18.5), xytext=(2009.5, 18.5),color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2014', xy=(2011.2, 18.5), xytext=(2011.2, 18.5),color='crimson', fontweight='bold', fontsize=12)\n\n# max\naxes1.annotate('', xy=(2009, 84), xytext=(2015, 67),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes1.annotate('Maximum sales in', xy=(2009.8, 65), xytext=(2009.8, 65), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2009', xy=(2015.2, 62), xytext=(2015.2, 62), color='crimson', fontweight='bold', fontsize=12)\n\n# conclusion \u2116 1\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'Strong growth', color='crimson', fontsize=14)\naxes2.text(0.24, 0.8, 'began', color='black', fontsize=14)\naxes2.text(0.353, 0.8, 'after 1995', color='crimson', fontsize=14)\naxes2.text(0.535, 0.8, 'and continued', color='black', fontsize=14)\naxes2.text(0.787, 0.8, 'until 2008.', color='crimson', fontsize=14)\naxes2.text(0, 0.75, 'Else we can see', color='black', fontsize=14)\naxes2.text(0.27, 0.75, '3 downturns in 1999, 2003 and 2005.', color='crimson', fontsize=14)\naxes2.text(0, 0.7, 'We observe that', color='black', fontsize=14)\naxes2.text(0.28, 0.7, 'sales decreased after 2008.', color='crimson', fontsize=14)\naxes2.text(0, 0.65, 'Sales downturns stopped in 2013-2014', color='crimson', fontsize=14)\naxes2.text(0.64, 0.65, 'and then began again', color='black', fontsize=14)\naxes2.text(0, 0.6, '----------------------------------------------------------------------------------------', color='black', fontsize=14)\naxes2.text(0, 0.55, 'On this chart we see familiar picture, which was', color='black', fontsize=14)\naxes2.text(0, 0.50, 'in North America and Europe.', color='black', fontsize=14)\n\n\n# calculation of sales growth rates\nlist_Other = []\nfor n in range(1, 37):\n    d = ((sales_by_year['Other_Sales'][n] - sales_by_year['Other_Sales'][n-1])\/sales_by_year['Other_Sales'][n-1])*100\n    list_Other.append(d)\n\n# visualisation growth rates sales in Other by years\ncolors = ['crimson' if _ > 0 else 'darksalmon' for _ in list_Other]\naxes3.bar(height=list_Other, x=sales_by_year['Year'][range(1, 37)], color=colors)\nfor p in axes3.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy()\n    if height >= 0:\n        axes3.annotate('{:.0f}'.format(height), (x + width\/2, y + height*1.02), ha='center')\n    elif height <= -55:\n        axes3.annotate('{:.0f}'.format(height), (x + width\/2, y + height*1.2), ha='center')\n    else:\n        axes3.annotate('{:.0f}'.format(height), (x + width\/2, y + height*1.4), ha='center')\naxes3.set_facecolor('whitesmoke')\naxes3.set_xlabel('Year', fontsize=18, color='black')\naxes3.set_ylabel('Growth rate, %', fontsize=18, color='black')\naxes3.text(1997, 365, 'Growth rate sales(%) in', color='black', fontsize=20, fontweight='bold')\naxes3.text(2000.5, 330, 'Other countries', color='crimson', fontsize=24, fontweight='bold')\n\n# max +\naxes3.annotate('', xy=(1983, 401), xytext=(1980, 322),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=-.3'))\naxes3.annotate('Max', xy=(1979, 312), xytext=(1979, 312), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('+', xy=(1981, 312), xytext=(1981, 312), color='crimson', fontweight='bold', fontsize=24)\n\n# max -\naxes3.annotate('', xy=(1987, -91), xytext=(1995, -72),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=-.3'))\naxes3.annotate('Max', xy=(1994, -62), xytext=(1994, -62), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('-', xy=(1996, -62), xytext=(1996, -62), color='darksalmon', fontweight='bold', fontsize=24)\n\n# conclusion \u2116 2\naxes4.set_facecolor('whitesmoke')\naxes4.axis('off')\naxes4.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes4.text(0, 0.8, 'Maximum positive growth', color='crimson', fontsize=14)\naxes4.text(0.445, 0.8, '-', color='black', fontsize=14)\naxes4.text(0.475, 0.8, '1983-1984.', color='crimson', fontsize=14)\naxes4.text(0.0, 0.75, 'Maximum negative growth', color='crimson', fontsize=14)\naxes4.text(0.445, 0.75, '-', color='black', fontsize=14)\naxes4.text(0.475, 0.75, '1983-1984.', color='crimson', fontsize=14)\naxes4.text(0, 0.7, 'Average growth rates by period:', color='crimson', fontsize=14)\naxes4.text(0, 0.65, f'* 1980-1984 - {round(statistics.mean(list_Other[:5]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.6, f'* 1985-1989 - {round(statistics.mean(list_Other[5:10]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.55, f'* 1990-1994 - {round(statistics.mean(list_Other[10:15]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.5, f'* 1995-1999 - {round(statistics.mean(list_Other[15:20]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.45, f'* 2000-2004 - {round(statistics.mean(list_Other[20:25]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.4, f'* 2005-2009 - {round(statistics.mean(list_Other[25:30]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.35, f'* 2010-2016 - {round(statistics.mean(list_Other[30:]),2)}%.', color='black', fontsize=14)\nfig.show()","58f1cf9a":"fig = plt.figure(facecolor='whitesmoke')\nsns.set_style('white')\naxes1 = fig.add_axes([0, 0, 1.5, 1.5]) \naxes2 = fig.add_axes([1.6, 0, 1, 1.5]) \naxes3 = fig.add_axes([0, -1.8, 1.5, 1.5]) \naxes4 = fig.add_axes([1.6, -1.8, 1, 1.5]) \n\nsns.lineplot(x=sales_by_year['Year'], y=sales_by_year['Global_Sales'],color='crimson',lw=4, ax=axes1)\naxes1.scatter(x=[2000, 2003, 2008, 2012, 2013], y=[ 201.56, 357.85, 678.90, 363.53, 368.11], color='black', lw=4)\naxes1.set_facecolor('whitesmoke')\naxes1.set_xlabel('Year', fontsize=18, color='black')\naxes1.set_ylabel('Sales', fontsize=18, color='black')\naxes1.text(1979, 640, 'Global sales', color='crimson', fontsize=24, fontweight='bold')\n\n# growth\naxes1.annotate('', xy=(2007, 681), xytext=(1992, 100),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('Strong growth with 2 downturns in', xy=(1991.2, 135), xytext=(1991.2, 135), rotation=54, color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2000', xy=(2000, 476), xytext=(2000, 476), rotation=54, color='crimson', fontweight='bold', fontsize=12)\naxes1.annotate('and', xy=(2001.3, 529), xytext=(2001.3, 529), rotation=54, color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2003', xy=(2002.3, 570), xytext=(2002.3, 570), rotation=54, color='crimson', fontweight='bold', fontsize=12)\n\n# stagnation\naxes1.annotate('', xy=(2011.9, 357), xytext=(2010, 183),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('', xy=(2012.9, 357), xytext=(2012, 183),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('Stagnation in', xy=(2001, 165), xytext=(2001, 165), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2013', xy=(2007, 165), xytext=(2007, 165),color='crimson', fontweight='bold', fontsize=12)\naxes1.annotate('and', xy=(2009.3, 165), xytext=(2009.3, 165),color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2014', xy=(2011.2, 165), xytext=(2011.2, 165),color='crimson', fontweight='bold', fontsize=12)\n\n# # max\naxes1.annotate('', xy=(2008, 690), xytext=(2015, 580),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.6'))\naxes1.annotate('Maximum sales', xy=(2011, 565), xytext=(2011, 565), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('in 2008', xy=(2014.2, 540), xytext=(2014.2, 540), color='crimson', fontweight='bold', fontsize=12)\n\n# conclusion \u2116 1\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'Between 1993 and 2008', color='crimson', fontsize=14)\naxes2.text(0.422, 0.8, 'was', color='black', fontsize=14)\naxes2.text(0.505, 0.8, 'strong growth', color='crimson', fontsize=14)\naxes2.text(0, 0.75, 'with', color='black', fontsize=14)\naxes2.text(0.08, 0.75, '2 downturns in 2000 and 2003.', color='crimson', fontsize=14)\naxes2.text(0, 0.7, 'After 2008', color='crimson', fontsize=14)\naxes2.text(0.185, 0.7, 'we see', color='black', fontsize=14)\naxes2.text(0.32, 0.7, 'decreased sales.', color='crimson', fontsize=14)\naxes2.text(0, 0.65, 'Also we observe', color='black', fontsize=14)\naxes2.text(0.278, 0.65, 'stagnation in 2013-2014.', color='crimson', fontsize=14)\n\n# calculation of sales growth rates\nlist_Global = []\nfor n in range(1, 37):\n    d = ((sales_by_year['Global_Sales'][n] - sales_by_year['Global_Sales'][n-1])\/sales_by_year['Global_Sales'][n-1])*100\n    list_Global.append(d)\n\n# visualisation growth rates sales in Other by years\ncolors = ['crimson' if _ > 0 else 'darksalmon' for _ in list_Global]\naxes3.bar(height=list_Global, x=sales_by_year['Year'][range(1, 37)], color=colors)\nfor p in axes3.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy()\n    if height >= 0:\n        axes3.annotate('{:.0f}'.format(height), (x + width\/2, y + height*1.02), ha='center')\n    elif height <= -55:\n        axes3.annotate('{:.0f}'.format(height), (x + width\/2, y + height*1.1), ha='center')\n    else:\n        axes3.annotate('{:.0f}'.format(height), (x + width\/2, y + height*1.4), ha='center')\naxes3.set_facecolor('whitesmoke')\naxes3.set_xlabel('Year', fontsize=18, color='black')\naxes3.set_ylabel('Growth rate, %', fontsize=18, color='black')\naxes3.text(1994, 180, 'Global growth rate sales(%)', color='crimson', fontsize=24, fontweight='bold')\n\n# max +\naxes3.annotate('', xy=(1981.5, 215), xytext=(1989, 180),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes3.annotate('Max', xy=(1987, 172), xytext=(1987, 172), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('+', xy=(1988.8, 171), xytext=(1988.8, 171), color='crimson', fontweight='bold', fontsize=24)\n\n# max -\naxes3.annotate('', xy=(2015, -73), xytext=(2008, -57),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes3.annotate('Max', xy=(2006, -46), xytext=(2006, -46), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('-', xy=(2007.8, -46), xytext=(2007.8, -46), color='darksalmon', fontweight='bold', fontsize=24)\n\n# conclusion \u2116 2\naxes4.set_facecolor('whitesmoke')\naxes4.axis('off')\naxes4.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes4.text(0, 0.8, 'Maximum positive growth', color='crimson', fontsize=14)\naxes4.text(0.438, 0.8, 'was', color='black', fontsize=14)\naxes4.text(0.52, 0.8, 'in 1980-1981,', color='crimson', fontsize=14)\naxes4.text(0.76, 0.8, 'and', color='black', fontsize=14)\naxes4.text(0, 0.75, 'maximum negative growth', color='crimson', fontsize=14)\naxes4.text(0.45, 0.75, '-', color='black', fontsize=14)\naxes4.text(0.48, 0.75, 'in 2015-2016.', color='crimson', fontsize=14)\naxes4.text(0, 0.7, 'Average growth rates by period:', color='crimson', fontsize=14)\naxes4.text(0, 0.65, f'* 1980-1984 - {round(statistics.mean(list_Global[:5]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.6, f'* 1985-1989 - {round(statistics.mean(list_Global[5:10]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.55, f'* 1990-1994 - {round(statistics.mean(list_Global[10:15]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.5, f'* 1995-1999 - {round(statistics.mean(list_Global[15:20]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.45, f'* 2000-2004 - {round(statistics.mean(list_Global[20:25]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.4, f'* 2005-2009 - {round(statistics.mean(list_Global[25:30]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.35, f'* 2010-2016 - {round(statistics.mean(list_Global[30:]),2)}%.', color='black', fontsize=14)\n\nfig.show()","fe40f6ad":"# NA\ngroup_genre_na = data.groupby('Genre')['NA_Sales'].sum().reset_index()\ngroup_genre_na['NA_Sales'] = round(group_genre_na['NA_Sales'],2)\ngroup_genre_na['Percentages'] = group_genre_na['NA_Sales']\/sum(group_genre_na['NA_Sales'])*100\ngroup_genre_na['Percentages'] = round(group_genre_na['Percentages'],2)\ngroup_genre_na.sort_values(by='Percentages', ascending=False, inplace=True)\ngroup_genre_na.reset_index(inplace=True)\ngroup_genre_na.drop('index', axis=1, inplace=True)\n\n# EU\ngroup_genre_eu = data.groupby('Genre')['EU_Sales'].sum().reset_index()\ngroup_genre_eu['EU_Sales'] = round(group_genre_eu['EU_Sales'],2)\ngroup_genre_eu['Percentages'] = group_genre_eu['EU_Sales']\/sum(group_genre_eu['EU_Sales'])*100\ngroup_genre_eu['Percentages'] = round(group_genre_eu['Percentages'],2)\ngroup_genre_eu.sort_values(by='Percentages', ascending=False, inplace=True)\ngroup_genre_eu.reset_index(inplace=True)\ngroup_genre_eu.drop('index', axis=1, inplace=True)\n\n# JP\ngroup_genre_jp = data.groupby('Genre')['JP_Sales'].sum().reset_index()\ngroup_genre_jp['JP_Sales'] = round(group_genre_jp['JP_Sales'],2)\ngroup_genre_jp['Percentages'] = group_genre_jp['JP_Sales']\/sum(group_genre_jp['JP_Sales'])*100\ngroup_genre_jp['Percentages'] = round(group_genre_jp['Percentages'],2)\ngroup_genre_jp.sort_values(by='Percentages', ascending=False, inplace=True)\ngroup_genre_jp.reset_index(inplace=True)\ngroup_genre_jp.drop('index', axis=1, inplace=True)\n\n# Other\ngroup_genre_other = data.groupby('Genre')['Other_Sales'].sum().reset_index()\ngroup_genre_other['Other_Sales'] = round(group_genre_other['Other_Sales'],2)\ngroup_genre_other['Percentages'] = group_genre_other['Other_Sales']\/sum(group_genre_other['Other_Sales'])*100\ngroup_genre_other['Percentages'] = round(group_genre_other['Percentages'],2)\ngroup_genre_other.sort_values(by='Percentages', ascending=False, inplace=True)\ngroup_genre_other.reset_index(inplace=True)\ngroup_genre_other.drop('index', axis=1, inplace=True)\n\n# Global\ngroup_genre_global = data.groupby('Genre')['Global_Sales'].sum().reset_index()\ngroup_genre_global['Global_Sales'] = round(group_genre_global['Global_Sales'],2)\ngroup_genre_global['Percentages'] = group_genre_global['Global_Sales']\/sum(group_genre_global['Global_Sales'])*100\ngroup_genre_global['Percentages'] = round(group_genre_global['Percentages'],2)\ngroup_genre_global.sort_values(by='Percentages', ascending=False, inplace=True)\ngroup_genre_global.reset_index(inplace=True)\ngroup_genre_global.drop('index', axis=1, inplace=True)\n","e62d2a21":"list_na=[]\nfor n in range(len(group_genre_na['Genre'])):\n    x = group_genre_na.loc[n,:]\n    list_na.append(x)\ncolor_list=[['whitesmoke', 'white', 'white']]\nfig = plt.figure(facecolor='whitesmoke')\naxes1 = fig.add_axes([0, 0, 1, 1]) \naxes2 = fig.add_axes([1.6, 0, 1, 1]) \n\naxes1.set_axis_off() \ntable=axes1.table(cellColours=color_list*12,cellText = list_na, cellLoc ='left', loc ='upper left', colWidths=[0.3,0.3,0.3],\n                  colLabels=group_genre_na.columns,colColours=['crimson']*3)           \ntable.auto_set_font_size(False) \ntable.set_fontsize(16)  \ntable.scale(1.5, 2.7) \naxes1.text(0.1, 1.15, 'Sales by genre in', color='black', fontsize=20, fontweight='bold')\naxes1.text(0.63, 1.15, 'North America', color='crimson', fontsize=24, fontweight='bold')\n\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'Top 3 genres in North America:', color='crimson', fontsize=14)\naxes2.text(0, 0.72, f\"* {group_genre_na['Genre'][0]};\", color='black', fontsize=14)\naxes2.text(0, 0.64, f\"* {group_genre_na['Genre'][1]};\", color='black', fontsize=14)\naxes2.text(0, 0.56, f\"* {group_genre_na['Genre'][2]};\", color='black', fontsize=14)\naxes2.text(0, 0.48, f\"Top 3 genres = {sum(group_genre_na['Percentages'][:3])}% of the total sales amount\", color='crimson', fontsize=14)\naxes2.text(0, 0.4, '3 least popular genres in North America', color='crimson', fontsize=14)\naxes2.text(0, 0.32, f\"* {group_genre_na['Genre'][9]};\", color='black', fontsize=14)\naxes2.text(0, 0.24, f\"* {group_genre_na['Genre'][10]};\", color='black', fontsize=14)\naxes2.text(0, 0.16, f\"* {group_genre_na['Genre'][11]};\", color='black', fontsize=14)\naxes2.text(0, 0.08, f\"3 least popular genres = {sum(group_genre_na['Percentages'][9:])}% of the total sales amount\", color='crimson', fontsize=14)\n\nfig.show()","dd6b039c":"list_eu=[]\nfor n in range(len(group_genre_eu['Genre'])):\n    x = group_genre_eu.loc[n,:]\n    list_eu.append(x)\ncolor_list=[['whitesmoke', 'white', 'white']]\nfig = plt.figure(facecolor='whitesmoke')\naxes1 = fig.add_axes([0, 0, 1, 1]) \naxes2 = fig.add_axes([1.6, 0, 1, 1]) \n\naxes1.set_axis_off() \ntable=axes1.table(cellColours=color_list*12,cellText = list_eu, cellLoc ='left', loc ='upper left', colWidths=[0.3,0.3,0.3],\n                  colLabels=group_genre_eu.columns,colColours=['crimson']*3)      \ntable.auto_set_font_size(False) \ntable.set_fontsize(16)  \ntable.scale(1.5, 2.7) \naxes1.text(0.1, 1.15, 'Sales by genre in', color='black', fontsize=20, fontweight='bold')\naxes1.text(0.63, 1.15, 'Europe', color='crimson', fontsize=24, fontweight='bold')\n\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'Top 3 genres in Europe:', color='crimson', fontsize=14)\naxes2.text(0, 0.72, f\"* {group_genre_eu['Genre'][0]};\", color='black', fontsize=14)\naxes2.text(0, 0.64, f\"* {group_genre_eu['Genre'][1]};\", color='black', fontsize=14)\naxes2.text(0, 0.56, f\"* {group_genre_eu['Genre'][2]};\", color='black', fontsize=14)\naxes2.text(0, 0.48, f\"Top 3 genres = {round(sum(group_genre_eu['Percentages'][:3]),2)}% of the total sales amount\", color='crimson', fontsize=14)\naxes2.text(0, 0.4, '3 least popular genres in Europe', color='crimson', fontsize=14)\naxes2.text(0, 0.32, f\"* {group_genre_eu['Genre'][9]};\", color='black', fontsize=14)\naxes2.text(0, 0.24, f\"* {group_genre_eu['Genre'][10]};\", color='black', fontsize=14)\naxes2.text(0, 0.16, f\"* {group_genre_eu['Genre'][11]};\", color='black', fontsize=14)\naxes2.text(0, 0.08, f\"3 least popular genres = {sum(group_genre_eu['Percentages'][9:])}% of the total sales amount\", color='crimson', fontsize=14)\n\nfig.show()","fe2de406":"list_jp=[]\nfor n in range(len(group_genre_jp['Genre'])):\n    x = group_genre_jp.loc[n,:]\n    list_jp.append(x)\ncolor_list=[['whitesmoke', 'white', 'white']]\nfig = plt.figure(facecolor='whitesmoke')\naxes1 = fig.add_axes([0, 0, 1, 1]) \naxes2 = fig.add_axes([1.6, 0, 1, 1]) \n\naxes1.set_axis_off() \ntable=axes1.table(cellColours=color_list*12,cellText = list_jp, cellLoc ='left', loc ='upper left', colWidths=[0.3,0.3,0.3],\n                  colLabels=group_genre_jp.columns,colColours=['crimson']*3)      \ntable.auto_set_font_size(False) \ntable.set_fontsize(16)  \ntable.scale(1.5, 2.7) \naxes1.text(0.1, 1.15, 'Sales by genre in', color='black', fontsize=20, fontweight='bold')\naxes1.text(0.63, 1.15, 'Japan', color='crimson', fontsize=24, fontweight='bold')\n\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'Top 3 genres in Japan:', color='crimson', fontsize=14)\naxes2.text(0, 0.72, f\"* {group_genre_jp['Genre'][0]};\", color='black', fontsize=14)\naxes2.text(0, 0.64, f\"* {group_genre_jp['Genre'][1]};\", color='black', fontsize=14)\naxes2.text(0, 0.56, f\"* {group_genre_jp['Genre'][2]};\", color='black', fontsize=14)\naxes2.text(0, 0.48, f\"Top 3 genres = {round(sum(group_genre_jp['Percentages'][:3]),2)}% of the total sales amount\", color='crimson', fontsize=14)\naxes2.text(0, 0.4, '3 least popular genres in Japan', color='crimson', fontsize=14)\naxes2.text(0, 0.32, f\"* {group_genre_jp['Genre'][9]};\", color='black', fontsize=14)\naxes2.text(0, 0.24, f\"* {group_genre_jp['Genre'][10]};\", color='black', fontsize=14)\naxes2.text(0, 0.16, f\"* {group_genre_jp['Genre'][11]};\", color='black', fontsize=14)\naxes2.text(0, 0.08, f\"3 least popular genres = {sum(group_genre_jp['Percentages'][9:])}% of the total sales amount\", color='crimson', fontsize=14)\n\nfig.show()","4d23dcea":"list_other=[]\nfor n in range(len(group_genre_other['Genre'])):\n    x = group_genre_other.loc[n,:]\n    list_other.append(x)\ncolor_list=[['whitesmoke', 'white', 'white']]\nfig = plt.figure(facecolor='whitesmoke')\naxes1 = fig.add_axes([0, 0, 1, 1]) \naxes2 = fig.add_axes([1.6, 0, 1, 1]) \n\naxes1.set_axis_off() \ntable=axes1.table(cellColours=color_list*12,cellText = list_other, cellLoc ='left', loc ='upper left', colWidths=[0.3,0.3,0.3],\n                  colLabels=group_genre_other.columns,colColours=['crimson']*3)      \ntable.auto_set_font_size(False) \ntable.set_fontsize(16)  \ntable.scale(1.5, 2.7) \naxes1.text(0.1, 1.15, 'Sales by genre in', color='black', fontsize=20, fontweight='bold')\naxes1.text(0.63, 1.15, 'Other countries', color='crimson', fontsize=24, fontweight='bold')\n\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'Top 3 genres in Other countries:', color='crimson', fontsize=14)\naxes2.text(0, 0.72, f\"* {group_genre_other['Genre'][0]};\", color='black', fontsize=14)\naxes2.text(0, 0.64, f\"* {group_genre_other['Genre'][1]};\", color='black', fontsize=14)\naxes2.text(0, 0.56, f\"* {group_genre_other['Genre'][2]};\", color='black', fontsize=14)\naxes2.text(0, 0.48, f\"Top 3 genres = {round(sum(group_genre_other['Percentages'][:3]),2)}% of the total sales amount\", color='crimson', fontsize=14)\naxes2.text(0, 0.4, '3 least popular genres in Other countries', color='crimson', fontsize=14)\naxes2.text(0, 0.32, f\"* {group_genre_other['Genre'][9]};\", color='black', fontsize=14)\naxes2.text(0, 0.24, f\"* {group_genre_other['Genre'][10]};\", color='black', fontsize=14)\naxes2.text(0, 0.16, f\"* {group_genre_other['Genre'][11]};\", color='black', fontsize=14)\naxes2.text(0, 0.08, f\"3 least popular genres = {sum(group_genre_other['Percentages'][9:])}% of the total sales amount\", color='crimson', fontsize=14)\n\nfig.show()","32d7ada7":"list_global=[]\nfor n in range(len(group_genre_global['Genre'])):\n    x = group_genre_global.loc[n,:]\n    list_global.append(x)\ncolor_list=[['whitesmoke', 'white', 'white']]\nfig = plt.figure(facecolor='whitesmoke')\naxes1 = fig.add_axes([0, 0, 1, 1]) \naxes2 = fig.add_axes([1.6, 0, 1, 1]) \n\naxes1.set_axis_off() \ntable=axes1.table(cellColours=color_list*12,cellText = list_global, cellLoc ='left', loc ='upper left', colWidths=[0.3,0.3,0.3],\n                  colLabels=group_genre_global.columns,colColours=['crimson']*3)      \ntable.auto_set_font_size(False) \ntable.set_fontsize(16)  \ntable.scale(1.5, 2.7) \naxes1.text(0.1, 1.15, 'Global sales by genre', color='crimson', fontsize=24, fontweight='bold')\n\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'Top 3 genres:', color='crimson', fontsize=14)\naxes2.text(0, 0.72, f\"* {group_genre_global['Genre'][0]};\", color='black', fontsize=14)\naxes2.text(0, 0.64, f\"* {group_genre_global['Genre'][1]};\", color='black', fontsize=14)\naxes2.text(0, 0.56, f\"* {group_genre_global['Genre'][2]};\", color='black', fontsize=14)\naxes2.text(0, 0.48, f\"Top 3 genres = {round(sum(group_genre_global['Percentages'][:3]),2)}% of the total sales amount\", color='crimson', fontsize=14)\naxes2.text(0, 0.4, '3 least popular genres', color='crimson', fontsize=14)\naxes2.text(0, 0.32, f\"* {group_genre_global['Genre'][9]};\", color='black', fontsize=14)\naxes2.text(0, 0.24, f\"* {group_genre_global['Genre'][10]};\", color='black', fontsize=14)\naxes2.text(0, 0.16, f\"* {group_genre_global['Genre'][11]};\", color='black', fontsize=14)\naxes2.text(0, 0.08, f\"3 least popular genres = {sum(group_genre_global['Percentages'][9:])}% of the total sales amount\", color='crimson', fontsize=14)\n\nfig.show()","a9440675":"north_usa_highest_sold_game = data.NA_Sales.max()\ndata[data[\"NA_Sales\"] == north_usa_highest_sold_game][[\"Name\",\"NA_Sales\"]]","207f5e6d":"sns.histplot(x=data[\"Platform\"],hue=data[\"Year\"])\ndata[[\"Platform\", \"Year\", \"Rank\"]].groupby([\"Platform\", \"Year\"]).count()","bed1944d":"hipo_1 = data.copy()\nhipo_1.NA_Sales = hipo_1.NA_Sales.astype(str)\nhipo_1.NA_Sales.values[data[\"NA_Sales\"].values < 5] = \"<5\"\nhipo_1.NA_Sales.values[(data[\"NA_Sales\"].values < 15) & (data[\"NA_Sales\"].values >= 5)] = \"5<=x<15\"\nhipo_1.NA_Sales.values[(data[\"NA_Sales\"].values < 25) & (data[\"NA_Sales\"].values >= 15)] = \"15<=x<25\"\nhipo_1.NA_Sales.values[(data[\"NA_Sales\"].values <= 45) & (data[\"NA_Sales\"].values >= 25)] = \"25<=x<=45\"\n\nfor col in quali_cols:\n    print(pd.crosstab(hipo_1[col], hipo_1['NA_Sales'], normalize=\"index\"))\n    print(\"\\n\\n\")","a7b12067":"fig, axes = plt.subplots(len(quali_cols), 1, figsize=(10,30))\nfor i, col in enumerate(quali_cols):\n    axes[i].set_title(f\"Wykres histogramu skategoryzowanego {col}\")\n    sns.histplot(data = hipo_1, x=hipo_1[col], hue='NA_Sales', ax=axes[i], multiple=\"dodge\")","84c0b6dc":"hipo_2 = data.copy()\nhipo_2.JP_Sales = hipo_1.NA_Sales.astype(str)\nhipo_2.JP_Sales.values[data[\"JP_Sales\"].values < 5] = \"<5\"\nhipo_2.JP_Sales.values[(data[\"JP_Sales\"].values < 15) & (data[\"JP_Sales\"].values >= 5)] = \"5<=x<15\"\nhipo_2.JP_Sales.values[(data[\"JP_Sales\"].values < 25) & (data[\"JP_Sales\"].values >= 15)] = \"15<=x<25\"\nhipo_2.JP_Sales.values[(data[\"JP_Sales\"].values <= 45) & (data[\"JP_Sales\"].values >= 25)] = \"25<=x<=45\"\n\nfor col in quali_cols:\n    print(pd.crosstab(hipo_2[col], hipo_2['JP_Sales'], normalize=\"index\"))\n    print(\"\\n\\n\")","a7699e2b":"fig, axes = plt.subplots(len(quali_cols), 1, figsize=(10,30))\nfor i, col in enumerate(quali_cols):\n    axes[i].set_title(f\"Wykres histogramu skategoryzowanego {col}\")\n    sns.histplot(data = hipo_2, x=hipo_2[col], hue='JP_Sales', ax=axes[i], multiple=\"dodge\")","23d2b20d":"hipo_3 = data.copy()\nhipo_3.EU_Sales = hipo_1.NA_Sales.astype(str)\nhipo_3.EU_Sales.values[data[\"EU_Sales\"].values < 5] = \"<5\"\nhipo_3.EU_Sales.values[(data[\"EU_Sales\"].values < 15) & (data[\"EU_Sales\"].values >= 5)] = \"5<=x<15\"\nhipo_3.EU_Sales.values[(data[\"EU_Sales\"].values < 25) & (data[\"EU_Sales\"].values >= 15)] = \"15<=x<25\"\nhipo_3.EU_Sales.values[(data[\"EU_Sales\"].values <= 45) & (data[\"EU_Sales\"].values >= 25)] = \"25<=x<=45\"\n\nfor col in quali_cols:\n    print(pd.crosstab(hipo_3[col], hipo_3['EU_Sales'], normalize=\"index\"))\n    print(\"\\n\\n\")","5289c06a":"fig, axes = plt.subplots(len(quali_cols), 1, figsize=(10,30))\nfor i, col in enumerate(quali_cols):\n    axes[i].set_title(f\"Wykres histogramu skategoryzowanego {col}\")\n    sns.histplot(data = hipo_3, x=hipo_3[col], hue='EU_Sales', ax=axes[i], multiple=\"dodge\")","adf50969":"fig, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(data.corr(), annot=True, ax=ax)\ndata.corr()","d0bd73d1":"cols_to_drop = [\"Rank\", \"NA_Sales\"]\nfor col in data.columns:\n    if col not in cols_to_drop:\n        contigency = pd.crosstab(data[col], data[\"NA_Sales\"])\n        chi,p_value,degrees_of_freedom , expected_freq = chi2_contingency(contigency)\n        print(f\"Dla kolumny {col} warto\u015b\u0107 p testu niezale\u017cno\u015bci wynosi {p_value}\")\n        if p_value <= 0.05:\n            print(f\"Warto\u015b\u0107 p jest mniejsza dla za\u0142o\u017conego poziomu istotno\u015bci co pozwala na odrzucenie hipotezy zerowej - zmienne s\u0105 zale\u017cne\\n\")\n        else:\n            print(f\"Warto\u015b\u0107 p jest wi\u0119ksza dla za\u0142o\u017conego poziomu istotno\u015bci co potwierdza hipotez\u0119 zerow\u0105 - zmienne s\u0105 niezale\u017cne\\n\")","124a68db":"cols_to_drop = [\"Rank\", \"JP_Sales\"]\nfor col in data.columns:\n    if col not in cols_to_drop:\n        contigency = pd.crosstab(data[col], data[\"JP_Sales\"])\n        chi,p_value,degrees_of_freedom , expected_freq = chi2_contingency(contigency)\n        print(f\"Dla kolumny {col} warto\u015b\u0107 p testu niezale\u017cno\u015bci wynosi {p_value}\")\n        if p_value <= 0.05:\n            print(f\"Warto\u015b\u0107 p jest mniejsza dla za\u0142o\u017conego poziomu istotno\u015bci co pozwala na odrzucenie hipotezy zerowej - zmienne s\u0105 zale\u017cne\\n\")\n        else:\n            print(f\"Warto\u015b\u0107 p jest wi\u0119ksza dla za\u0142o\u017conego poziomu istotno\u015bci co potwierdza hipotez\u0119 zerow\u0105 - zmienne s\u0105 niezale\u017cne\\n\")","6797e611":"cols_to_drop = [\"Rank\", \"EU_Sales\"]\nfor col in data.columns:\n    if col not in cols_to_drop:\n        contigency = pd.crosstab(data[col], data[\"EU_Sales\"])\n        chi,p_value,degrees_of_freedom , expected_freq = chi2_contingency(contigency)\n        print(f\"Dla kolumny {col} warto\u015b\u0107 p testu niezale\u017cno\u015bci wynosi {p_value}\")\n        if p_value <= 0.05:\n            print(f\"Warto\u015b\u0107 p jest mniejsza dla za\u0142o\u017conego poziomu istotno\u015bci co pozwala na odrzucenie hipotezy zerowej - zmienne s\u0105 zale\u017cne\\n\")\n        else:\n            print(f\"Warto\u015b\u0107 p jest wi\u0119ksza dla za\u0142o\u017conego poziomu istotno\u015bci co potwierdza hipotez\u0119 zerow\u0105 - zmienne s\u0105 niezale\u017cne\\n\")","0042752e":"fig, axes = plt.subplots(len(quant_col),1,  figsize=(10,30))\nfor i, col in enumerate(quant_col):\n    axes[i].set_title(f\"Wykres pude\u0142kowy kolumny {col}\")\n    sns.boxplot(data = data, x=data[col], ax=axes[i], orient = \"h\")","f2d9b3ff":"data[data[\"NA_Sales\"]>=1]","37c9cb14":"import random\ndata.loc[data[\"NA_Sales\"] >= 1,\"NA_Sales\"] = data[\"NA_Sales\"].apply(lambda x: random.randrange(0,1))","1f8b8458":"sns.boxplot(data = data, x=data[\"NA_Sales\"], orient = \"h\")","d2a72a3e":"sns.boxplot(data = data, x=\"Year\", y = \"JP_Sales\", orient=\"v\")","5bd28501":"sns.boxplot(data = data, x=\"Genre\", y = \"EU_Sales\", orient=\"v\")","7af1de69":"fig, axes = plt.subplots(len(quali_cols), 1, figsize=(10,30))\nfor i, col in enumerate(quant_col):\n    axes[i].set_title(f\"Wykres rozk\u0142adu liczno\u015bci dla {col}\")\n    sns.histplot(data[col], ax=axes[i], kde=True)","444cfac8":"for i, col in enumerate(quant_col):\n    print(f\"Warto\u015b\u0107 p dla testu normalnego dla kolumny {col} wynosi {normaltest(data[col].values).pvalue}\\n\")","81dc863a":"from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text, DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\n\n# oddzielenie potrzebnych danych\nhipo_1_data = data[[\"Global_Sales\"]]\nhipo_1_y = data[\"NA_Sales\"]\n\n\n# rozdzielenie danych na sety treningowe i testowe\nX_train, X_test, y_train, y_test = train_test_split(hipo_1_data, hipo_1_y, train_size=0.7, random_state=1)\n\n# \u0142adowanie modelu\ntree_model = DecisionTreeRegressor(min_samples_leaf=500)\n\n# trenowanie modelu\ntree_model.fit(X_train, y_train)\n\n# sprawdzanie dok\u0142\u0105dno\u015bci modelu na danych testowych\n\npreds = tree_model.predict(X_test)\nr2score = r2_score(y_test, preds)\nprint(f\"Wsp\u00f3\u0142czynnik zbie\u017cno\u015bci wynosi {((1 - r2score) * 100):.2f}% - Dopasowanie modelu jest tym lepsze im bardziej wsp\u00f3\u0142czynnik zbie\u017cno\u015bci jest bli\u017cej 0%\")","c1e83a6c":"var_importances = tree_model.feature_importances_\nstd = np.std(var_importances,axis=0)\nindices = np.argsort(var_importances)\nplt.figure()\nplt.title(\"Wa\u017cno\u015b\u0107 predyktor\u00f3w\")\nplt.barh(range(hipo_1_data.shape[1]), var_importances[indices],\n       color=\"r\", xerr=std, align=\"center\")\nplt.yticks(range(hipo_1_data.shape[1]), hipo_1_data.columns)\nplt.ylim([-1, hipo_1_data.shape[1]])\nplt.grid(b=True)\nplt.xlim(0, 1)\nplt.show()","36b61e8d":"plt.figure(figsize=(25,20))\nplot_tree(tree_model, filled=True, rounded=True, feature_names=X_train.columns)\nplt.show()","8a861b0c":"from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix, precision_recall_fscore_support\nfrom sklearn import preprocessing\npd.options.mode.chained_assignment = None\n\n# oddzielenie potrzebnych danych\nhipo_2_data = data[[\"Year\", \"Platform\"]]\nhipo_2_y = data[\"JP_Sales\"]\n\ndecode = {\n    \"2600\" : 1,\n    \"3DO\" : 2,\n    \"3DS\" : 3,\n    \"DC\" : 4,\n    \"DS\" : 5,\n    \"GB\" : 6,\n    \"GBA\" : 7,\n    \"GC\" : 8,\n    \"GEN\" : 9,\n    \"GG\" : 10,\n    \"N64\" : 11,\n    \"NES\" : 12,\n    \"NG\" : 13,\n    \"PC\" : 14,\n    \"PCFX\" : 15,\n    \"PS\" : 16,\n    \"PS2\" : 17,\n    \"PS3\" : 18,\n    \"PS4\" : 19,\n    \"PSP\" : 20,\n    \"PSV\" : 21,\n    \"SAT\" : 22,\n    \"SCD\" : 23,\n    \"SNES\" : 24,\n    \"TG16\" : 25,\n    \"WS\" : 26,\n    \"Wii\" : 27,\n    \"WiiU\" : 28,\n    \"X360\" : 29,\n    \"XB\" : 30,\n    \"XOne\" : 31\n}\n\nhipo_2_data.loc[:, \"Platform\"] = hipo_2_data.Platform.map(decode).values\n\n\n","584cdda4":"lab_enc = preprocessing.LabelEncoder()\nhipo_2_y = lab_enc.fit_transform(hipo_2_y)","5f46815c":"# rozdzielenie danych na sety treningowe i testowe\nX_train, X_test, y_train, y_test = train_test_split(hipo_2_data, hipo_2_y, train_size=0.7, random_state=1)\nX_train\n\n# \u0142adowanie modelu\ntree_model = DecisionTreeClassifier(min_samples_leaf=200)\n\n# trenowanie modelu\ntree_model.fit(X_train, y_train)\n\n# sprawdzanie dok\u0142\u0105dno\u015bci modelu na danych testowych\n\npreds = tree_model.predict(X_test)\nacc = accuracy_score(y_test, preds)\nprint(f\"Dok\u0142adno\u015b\u0107 modelu wynosi {100*acc:.2f}%\")","4188e274":"var_importances = tree_model.feature_importances_\nstd = np.std(var_importances,axis=0)\nindices = np.argsort(var_importances)\nplt.figure()\nplt.title(\"Wa\u017cno\u015b\u0107 predyktor\u00f3w\")\nplt.barh(range(hipo_2_data.shape[1]), var_importances[indices],\n       color=\"r\", xerr=std, align=\"center\")\nplt.yticks(range(hipo_2_data.shape[1]), hipo_2_data.columns)\nplt.ylim([-1, hipo_2_data.shape[1]])\nplt.grid(b=True)\nplt.xlim(0, 1)\nplt.show()","18c7b314":"plt.figure(figsize=(25,20))\nplot_tree(tree_model, filled=True, rounded=True, feature_names=X_train.columns)\nplt.show()","ffae7b71":"\n# oddzielenie potrzebnych danych\nhipo_3_data = data[[\"Genre\"]]\nhipo_3_y = data[\"EU_Sales\"]\n\ndecode = {\n    \"Action\" : 1,\n    \"Adventure\" : 2,\n    \"Fighting\" : 3,\n    \"Misc\" : 4,\n    \"Platform\" : 5,\n    \"Puzzle\" : 6,\n    \"Racing\" : 7,\n    \"Role-Playing\" : 8,\n    \"Shooter\" : 9,\n    \"Simulation\" : 10,\n    \"Sports\" : 11,\n    \"Strategy\" : 12\n}\n\nhipo_3_data.loc[:, \"Genre\"] = hipo_3_data.Genre.map(decode).values","eb918e12":"lab_enc = preprocessing.LabelEncoder()\nhipo_3_y = lab_enc.fit_transform(hipo_3_y)","ffd197ec":"# rozdzielenie danych na sety treningowe i testowe\nX_train, X_test, y_train, y_test = train_test_split(hipo_3_data, hipo_3_y, train_size=0.7, random_state=1)\n\n# \u0142adowanie modelu\ntree_model = DecisionTreeClassifier(min_samples_leaf=500)\n\n# trenowanie modelu\ntree_model.fit(X_train, y_train)\n\n# sprawdzanie dok\u0142\u0105dno\u015bci modelu na danych testowych\n\npreds = tree_model.predict(X_test)\nacc = accuracy_score(y_test, preds)\nprint(f\"Dok\u0142adno\u015b\u0107 modelu wynosi {100*acc:.2f}%\")","7c8ba9c4":"var_importances = tree_model.feature_importances_\nstd = np.std(var_importances,axis=0)\nindices = np.argsort(var_importances)\nplt.figure()\nplt.title(\"Wa\u017cno\u015b\u0107 predyktor\u00f3w\")\nplt.barh(range(hipo_3_data.shape[1]), var_importances[indices],\n       color=\"r\", xerr=std, align=\"center\")\nplt.yticks(range(hipo_3_data.shape[1]), hipo_3_data.columns)\nplt.ylim([-1, hipo_3_data.shape[1]])\nplt.grid(b=True)\nplt.xlim(0, 1)\nplt.show()","225a3bfa":"plt.figure(figsize=(25,20))\nplot_tree(tree_model, filled=True, rounded=True, feature_names=X_train.columns, class_names=[\"1\",\"2\", \"3\", \"4\", \"5\", \"6\",\"7\", \"8\", \"9\", \"10\", \"11\", \"12\"])\nplt.show()","c24ce3e8":"from sklearn.cluster import KMeans\npd.options.mode.chained_assignment = None\nfrom sklearn.decomposition import PCA\n\ndecode = {\n    \"Action\" : 1,\n    \"Adventure\" : 2,\n    \"Fighting\" : 3,\n    \"Misc\" : 4,\n    \"Platform\" : 5,\n    \"Puzzle\" : 6,\n    \"Racing\" : 7,\n    \"Role-Playing\" : 8,\n    \"Shooter\" : 9,\n    \"Simulation\" : 10,\n    \"Sports\" : 11,\n    \"Strategy\" : 12\n}\n\n# oddzielenie potrzebnych danych\ncluster_data = data[[\"Genre\", \"Year\", \"Global_Sales\"]]\ncluster_data.loc[:, \"Genre\"] = cluster_data.Genre.map(decode).values\n\nsse = []\n\n# \u0142adowanie modelu\nfor k in range(1,11):\n    cluster_model = KMeans(n_clusters=k)\n    cluster_model.fit(cluster_data)\n    sse.append(cluster_model.inertia_)\n    \n# sprawdzanie jak\u0105 warto\u015b\u0107 k wybra\u0107 - metoda \u0142okcia\n\nplt.style.use(\"fivethirtyeight\")\nplt.plot(range(1, 11), sse)\nplt.xticks(range(1, 11))\nplt.xlabel(\"Warto\u015b\u0107 k\")\nplt.ylabel(\"SSE\")\nplt.show()","36e3094e":"from sklearn.metrics import silhouette_score\n\nsil_score_max = -1\nbest_n_clusters = 0\n\nfor k in range(2,11):\n  cluster_model = KMeans(n_clusters = k)\n  labels = cluster_model.fit_predict(cluster_data)\n  sil_score = silhouette_score(cluster_data, labels)\n  print(f\"\u015arednia warto\u015b\u0107 silhouette score dla {k} klastr\u00f3w wynosi {sil_score}\")\n  if sil_score > sil_score_max:\n    sil_score_max = sil_score\n    best_n_clusters = k\n    \nprint(f\"Najlepsza ilo\u015b\u0107 klastr\u00f3w to: {best_n_clusters}\")","b8d33351":"cluster_model = KMeans(n_clusters = best_n_clusters)\n# trenowanie modelu\nresult = cluster_model.fit_predict(cluster_data)\n\nlabels = cluster_model.labels_\n\nresult_data = cluster_data.copy()\nresult_data[\"labels\"] = labels\n\nresults_0 = cluster_data[result_data.labels == 0]\nresults_1 = cluster_data[result_data.labels == 1]","c97fcfa4":"results_0.describe()","5c8cfea6":"results_1.describe()","2bd75e91":"# za\u0142adowanie modu\u0142u do analizy metod\u0105 EM\nfrom sklearn.mixture import GaussianMixture\n\n# wczytanie modelu z dwoma klastrami\nem_model = GaussianMixture(n_components=2, random_state=0)\n\nlabels = em_model.fit_predict(cluster_data)\n\n#labels = em_model.labels_\n\nresult_data = cluster_data.copy()\nresult_data[\"labels\"] = labels\n\nresults_0 = cluster_data[result_data.labels == 0]\nresults_1 = cluster_data[result_data.labels == 1]","cab1d6c8":"results_0.describe()","bb4a5f17":"results_1.describe()","6c89c864":"import scipy.stats as stats\nimport math\n\nfor col in results_1:\n    mu_0 = results_0[col].mean()\n    variance_0 = results_0[col].var()\n    mu_1 = results_1[col].mean()\n    variance_1 = results_1[col].var()\n    sigma_0 = math.sqrt(variance_0)\n    sigma_1 = math.sqrt(variance_1)\n    x_0 = np.linspace(mu_0 - 3*sigma_0, mu_0 + 3*sigma_0, 100)\n    x_1 = np.linspace(mu_1 - 3*sigma_1, mu_1 + 3*sigma_1, 100)\n    plt.figure(figsize=(8,5))\n    plt.title(f\"Wykres dystrybucji zmiennych w klastrach dla zmiennej {col}\")\n    plt.plot(x_0, stats.norm.pdf(x_0, mu_0, sigma_0))\n    plt.plot(x_1, stats.norm.pdf(x_1, mu_1, sigma_1))\n    plt.show()","17888221":"# zapisywanie danych w postaci tensora\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nhipo_1_data = data[[\"Year\", \"Genre\"]]\nhipo_1_y = data[\"Global_Sales\"]\n\ndecode = {\n    \"Action\" : 1,\n    \"Adventure\" : 2,\n    \"Fighting\" : 3,\n    \"Misc\" : 4,\n    \"Platform\" : 5,\n    \"Puzzle\" : 6,\n    \"Racing\" : 7,\n    \"Role-Playing\" : 8,\n    \"Shooter\" : 9,\n    \"Simulation\" : 10,\n    \"Sports\" : 11,\n    \"Strategy\" : 12\n}\n\nhipo_1_data.loc[:, \"Genre\"] = hipo_1_data.Genre.map(decode).values\n\n# rozdzielenie danych na sety treningowe i testowe\nX_train, X_test, y_train, y_test = train_test_split(hipo_1_data, hipo_1_y, train_size=0.7, random_state=1)\n\nmodel = keras.models.Sequential([\n    layers.Input(shape=(X_train.shape[1],)),\n    layers.Dense(10, activation=\"relu\"),\n    layers.Dropout(0.2),\n    layers.Dense(10, activation=\"relu\"),\n    layers.Dense(100, activation=\"softmax\")\n])\n\nmodel.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",\n             metrics=[\"accuracy\"])\n\nmodel.build()\nmodel.summary()","30efce64":"#trenowanie modelu\nhistory = model.fit(X_train, y_train, validation_split=0.33, epochs=20)\n# summarize history for accuracy\nplt.figure(figsize=(8,5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Dok\u0142adno\u015b\u0107 modelu')\nplt.ylabel('Dok\u0142adno\u015b\u0107')\nplt.xlabel('Epoka')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.figure(figsize=(8,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Funkcja koszt\u00f3w modelu')\nplt.ylabel('Strata')\nplt.xlabel('Epoka')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","a1105eb6":"from sklearn import preprocessing","cb7f3f18":"scaler = preprocessing.StandardScaler()\nX_train_scal = scaler.fit_transform(X_train)\nX_test_scal = scaler.transform(X_test)\n\n#trenowanie modelu\nhistory = model.fit(X_train, y_train, validation_split=0.33, epochs=20)\n# summarize history for accuracy\nplt.figure(figsize=(8,5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Dok\u0142adno\u015b\u0107 modelu')\nplt.ylabel('Dok\u0142adno\u015b\u0107')\nplt.xlabel('Epoka')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.figure(figsize=(8,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Funkcja koszt\u00f3w modelu')\nplt.ylabel('Strata')\nplt.xlabel('Epoka')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","466ad63c":"model.evaluate(X_test, y_test)","3cadf1c2":"Sprzeda\u017c w regionach z podzia\u0142em na typy gier","795647b8":"Jak wida\u0107 dla ka\u017cdej kolumny zmiennej ilo\u015bciowej nie posiadamy rozk\u0142adu normalnego. Jest to cecha tego zbioru danych i nie powinno si\u0119 zmienia\u0107 jego warto\u015bci, aby rozk\u0142ad zmiennych by\u0142 bliski do rozk\u0142adu normalnego. Niestety z tego powodu nie mo\u017cemy stosowa\u0107 parametrycznych metod statystycznych w celu przewidywania zmiennych zale\u017cnych.","0dddb994":"Do sprawdzenia hipotezy drugiej dotycz\u0105cej sprzeda\u017cy gier na rynek japo\u0144ski w kolejnych latach wybrano nast\u0119puj\u0105ce zmienne niezale\u017cne:\n\nYear, JP_Sales, Publisher\n","da4be65e":"Pozosta\u0142e:","be91ca8e":"Zast\u0105pmy te 910 rekord\u00f3w warto\u015bciami sprzeda\u017cy mi\u0119dzy 0 a 1 milionem sztuk","073c3fb2":"Okre\u015blenie wa\u017cno\u015bci predykat\u00f3w z u\u017cyciem wykresu","7073e9c3":"Ze wzgl\u0119du na ilo\u015b\u0107 platform dost\u0119pnych w datasecie oraz ilo\u015b\u0107 lat, w kt\u00f3rych gry by\u0142y sprzedawane, powy\u017cszy wykres nie pozwala na odczytanie interesuj\u0105cych nas warto\u015bci. Po kolorach mo\u017cemy tylko stwierdzi\u0107, \u017ce w prawej cz\u0119\u015bci drzewa jest dobre dopasowanie modelu. Jednak ca\u0142o\u015bciowo ma si\u0119 to odpowiednio z obliczonym (63.45%) dok\u0142adno\u015bci\u0105 modelu - drzewo te\u017c tylko w nieco wi\u0119kszej po\u0142owie jest zakolorowane \"dobrymi\" kolorami.  ","2a9e646d":"Wizualizacja wynik\u00f3w","6707f096":"Standaryzacja cech","0592f59d":"Sprawd\u017amy jak zmieni\u0142 si\u0119 rozk\u0142ad zmiennych","1c715fb5":"Do wykonania wykres\u00f3w pude\u0142kowych skategoryzowanych wybrano pary: Zmienna jako\u015bciowa: Year, Zmienna ilo\u015bciowa: JP_Sales Zmienna jako\u015bciowa: Genre, Zmianna ilo\u015bciowa: EU_Sales","2e286da6":"Tabele wielodzielcze dla zmiennych jako\u015bciowych dla hipotezy 1. Przez to, \u017ce kolumna NA_Sales ma wiele r\u00f3\u017cnych warto\u015bci, postanowi\u0142em podzieli\u0107 j\u0105 na przedzia\u0142y","2467c2ef":"W ka\u017cdym typie gier, najlepiej sprzedaje si\u0119 ","55470bb8":"Test niezale\u017cno\u015bci dla hipotezy 3 (EU_Sales)","4b6da04c":"Wizualizacja wynik\u00f3w:","d60ad0e5":"Do wykonania analizy skupie\u0144 wybrano zmienne:\n\n* Genre\n* Year\n* Global_Sales\n\nCelem analizy jest sprawdzenie, czy jeste\u015bmy w stanie wyznaczy\u0107 jakie\u015b konkretne grupy gier kupowanych przez konsument\u00f3w w zale\u017cno\u015bci od roku.","128f3639":"# Indukcja drzew decyzyjnych","3c375e1f":"Test niezale\u017cno\u015bci dla hipotezy 1 (NA_Sales)","ba823092":"Macierz korelacji, uwzgl\u0119dniaj\u0105ca tak\u017ce rok powstania gry","e71613ee":"Odczytanie i sformalizowanie na podstawie drzewa 3-5 regu\u0142 dla najbardziej wyrazistych klas lub dla li\u015bci o najmniejszej wariancji","6e6c1741":"Macierz korelacji","22f3d8f5":"Japonia:","a9b6d99e":"P\u00f3\u0142nocna Ameryka:","ed0cde43":"Wykonanie macierzy korelacji","698694e4":"Jako dodatkowy algorytm wybrano sieci neuronowe implementowane przy pomocy biblioteki tensorflow. Model ten zosta\u0142 wybrany, poniewa\u017c sieci neuronowe s\u0105 szeroko wykorzystywane w data science.","34aa67bf":"# Analiza skupie\u0144","61f5e693":"Tabele liczno\u015bci dla zmiennych jako\u015bciowych ","6eaf9338":"Analiza klastra 2","72325da4":"Z powodu jednolitego rozk\u0142adu zmiennych badanie skupie\u0144 nie przynosi dodatkowych informacji, poniewa\u017c metody typu najni\u017cszych s\u0105siad\u00f3w dziel\u0105 te zbiory na podobne klatry o podobnym wygl\u0105dzie. Jedyna r\u00f3\u017cnica pomi\u0119dzy klastrami jest w sprzeda\u017cy globalnej, gdzie pierwszy klaster skupia w sobie gry, kt\u00f3rych sprzedano wi\u0119cej, a drugi klaster te gry, kt\u00f3rych sprzedano mniej. Patrz\u0105c na typ gry, b\u0105d\u017a rok publikacji, to nie ma ona wi\u0119kszego wp\u0142ywu na grupy gier.","4231fdbd":"Podobny zabieg mo\u017cna by\u0142oby wykona\u0107 na ka\u017cdej zmiennej sprzeda\u017cowej \"Sales\".","362564c5":"Do sprawdzenia hipotezy trzeciej dotycz\u0105cej gier akcji na rynku europejskim wybrano nast\u0119puj\u0105ce zmienne niezale\u017cne:\n\nGenre","630f253f":"Analiza klastra 1","d1c9ad63":"Gry najlepiej sprzedawa\u0142y si\u0119 na platform\u0119 PS2 oraz DS","eeb07cfc":"Test niezale\u017cno\u015bci dla hipotezy 2 (JP_Sales)","f0ad45b7":"Wykresy ramka-w\u0105sy dla wszystkich zmiennych ilo\u015bciowych z hipotez. ","50957940":"W analizie korelacji mo\u017cemy wykluczy\u0107 pole \"Rank\" gdy\u017c jest to traktowane jako ID. Zatem korelacja wyst\u0119puje pomi\u0119dzy:\n* \"Year\" a \"Sales\". S\u0105 to s\u0142abe korelacje, cz\u0119\u015bciowo ujemne, cz\u0119\u015bciowo pozorne. Jednak pole \"Year\" nie mo\u017ce zosta\u0107 poddane korelacji gdy\u017c wraz ze wzrostem sprzeda\u017cy, ros\u0142aby warto\u015b\u0107 roku. To jest mo\u017cliwe ale nie w ka\u017cdym przypadku a na pewno nie jest to czysto logiczne\n* \"Sales\" a \"Sales\". Pomi\u0119dzy wszystkimi warto\u015bciami sprzeda\u017cy istnieje korelacja dodatnia co jest jak najbardziej sensowne. W momemncie gdy ro\u015bnie sprzeda\u017c w kt\u00f3rym\u015b regionie \u015bwiata, automatycznie ro\u015bnie sprzeda\u017c w aspekcie ca\u0142ego globu\n\nNajmocniejsz\u0105 korelacj\u0105 wykazuje si\u0119 para \"NA_Sales\" oraz \"Global_Sales\" (0.94) co jest bardzo siln\u0105 korelacj\u0105. Oznacza to najwi\u0119kszy wp\u0142yw na wzrost globalnej sprzeda\u017cy artyku\u0142\u00f3w na rynek Ameryki P\u00f3\u0142nocnej","a936f561":"Histogram analizy kolumn","a18092aa":"Wizualizacja wynik\u00f3w: ","f49d2408":"Dla pustych p\u00f3l wydawcy gier \"Publisher\" uzupe\u0142nione zosta\u0142y one warto\u015bciami \"Unknown\"","0e66c2ae":"Uzupe\u0142nienie kolumny \"Year\" warto\u015bciami tak, aby pozby\u0107 si\u0119 nulli. Nie zdecydowa\u0142em si\u0119 ich usun\u0105\u0107, lecz uzupe\u0142ni\u0107 \u015bredni\u0105 warto\u015bci\u0105. Dzi\u0119ki temu wiemy, \u017ce najwi\u0119cej gier spprzedano w 2008 oraz 2009 roku. ","f6dd315a":"Model jaki zbudowa\u0142em sk\u0142ada si\u0119 z 5 warstw:\n\n* Warstwa wej\u015bciowa - sk\u0142ada si\u0119 z 3 neuron\u00f3w (po jednym na ka\u017cd\u0105 zmienn\u0105)\n* Pierwsza warstwa wewn\u0119trzna - sk\u0142ada si\u0119 z 10 neuron\u00f3w, ka\u017cdy po\u0142\u0105czony z neuronami z warstwy wej\u015bciowej. Zastosowano w nim funkcj\u0119 aktywacyjn\u0105 relu. Neuron z funkcj\u0105 aktywacji ReLU przyjmuje dowolne warto\u015bci rzeczywiste jako swoje wej\u015bcie (a), ale aktywuje si\u0119 tylko wtedy, gdy te wej\u015bcie (a) s\u0105 wi\u0119ksze ni\u017c 0.\n* Druga warstwa wewn\u0119trzna - warstwa przej\u015bciowa, kt\u00f3ra losowo ustawia jednostki wej\u015bciowe na 0 z cz\u0119stotliwo\u015bci\u0105 na ka\u017cdym kroku podczas treningu, co pomaga zapobiega\u0107 nadmiernemu dopasowaniu. Wej\u015bcia nie ustawione na 0 s\u0105 skalowane w g\u00f3r\u0119 o 1 \/ (1 - stawka) tak, \u017ce suma wszystkich wej\u015b\u0107 pozostaje niezmieniona.\n* Trzecia warstwa wewn\u0119trzna - sk\u0142ada si\u0119 z 10 neuron\u00f3w, kt\u00f3ry ka\u017cdy po\u0142\u0105czony jest z neuronem drugiej warstwy wewn\u0119trznej. W warstwie tej zastosowan\u0105 funkcj\u0119 aktywacyjn\u0105 relu.\n* Warstwa wyj\u015bciowa - sk\u0142ada si\u0119 z dw\u00f3ch neuron\u00f3w, kt\u00f3ry ka\u017cdy jest po\u0142\u0105czony z trzeci\u0105 warstw\u0105 wewn\u0119trzn\u0105. Zastosowano tutaj funkcj\u0119 aktywacyjn\u0105 softmax, kt\u00f3ra przetwarza warto\u015bci na ko\u0144cu neuron\u00f3w i zamienia je na warto\u015bci mi\u0119dzy 0 a 1\n\nDodatkowo sie\u0107 zosta\u0142a zbudowana z okre\u015bleniem optymalizatora Adam, kt\u00f3ry implementuje wyk\u0142adnicz\u0105 \u015bredni\u0105 ruchom\u0105 gradient\u00f3w, aby skalowa\u0107 tempo uczenia. Utrzymuje wyk\u0142adniczo malej\u0105c\u0105 \u015bredni\u0105 poprzednich gradient\u00f3w. Adam jest wydajny obliczeniowo i ma bardzo ma\u0142e wymagania dotycz\u0105ce pami\u0119ci. Adam Optimizer jest jednym z najpopularniejszych algorytm\u00f3w optymalizacji zst\u0119powania gradientu. Opr\u00f3cz optymalizatora jako funkcj\u0119 koszt\u00f3w wybrano sparse_categorical_crossentropy, a do pomiar\u00f3w jako\u015bci sieci wyprano parametr accuracy.","bc135a0b":"Dla Hipotezy 2 stworzono podobne tabele wielodzielcze","97a19b9a":"Zamiast wariancji sklearn wykorzystuje MSE (Mean Square Error) do okre\u015blenia jako\u015bci drzewa. Jak wida\u0107 prawa cz\u0119\u015b\u0107 drzewa wygl\u0105da ca\u0142kiem dobrze (MSE w li\u015bciach na niskim poziomie ok 0.08) co wskazuje na bardzo dobre dopasowanie \u015bredniej warto\u015bci w li\u015bciach do warto\u015bci rzeczywistych (ilo\u015bci sprzedanych gier w wyra\u017conych w milionach). Lewa strona drzewa natomiast posiada bardzo niskie warto\u015b\u0107i MSE (na poziomie oko\u0142o 0.01) co powoduje, \u017ce drzewo jest bardzo ma\u0142o wiarygodne. Potwierdza to warto\u015bc dopasowania modelu (47.33%) wskazuj\u0105c, \u017ce model jest dobrze dopasowany tylko w po\u0142owie. Dlatego hipotez\u0119 t\u0105 mo\u017cna przyj\u0105\u0107 tylko dla gier, kt\u00f3rych nak\u0142ad wyni\u00f3s\u0142 powy\u017cej 0.08 miliona sprzedanych sztuk. Dla ca\u0142ej populacji nale\u017cy odrzuci\u0107 hipotez\u0119.","6bb505c4":"Oraz podobnie dla hipotezy 3","addd051d":"Test normalny dla zmiennych - warto\u015bci odstaj\u0105ce","68beb18d":"W powy\u017cszych tabelach wielodzielczych widzimy udzia\u0142 w statystykach gier o sprzeda\u017cy 25-45 mln egzemplarzy. ","b18ace86":"W powy\u017cszych tabelach widzimy zale\u017cno\u015b\u0107, \u017ce z ka\u017cdym rokiem ro\u015bnie liczba sprzedanych gier. Jednak ten trend przestaje obowi\u0105zywa\u0107 po 2010 roku.","6202e42a":"Analiza skupie\u0144 metod\u0105 EM","403b9928":"Por\u00f3wnuj\u0105c wyniki pomi\u0119dzy analiz\u0105 skupie\u0144 metod\u0105 K-najbli\u017cszych s\u0105siad\u00f3w a metod\u0105 EM, mo\u017cna zauwa\u017cy\u0107 niewielkie r\u00f3\u017cnice. Analiza metod\u0105 EM podzieli\u0142a zbi\u00f3r danych g\u0142\u00f3wnie ze wzgl\u0119du na globaln\u0105 sprzeda\u017c (dla klastra 1 gry sprzedane mi\u0119dzy 0.01 a 0.73 miliona sztuk, a dla klastra 2 od 0.02 do 82.74 miliona sztuk), ale w odr\u00f3\u017cnieniu od K-najbli\u017cszych s\u0105siad\u00f3w wzi\u0119\u0142a r\u00f3wnie\u017c pod uwag\u0119 rok publikacji, gdzie klaster 2 jest nieco bardziej przesuni\u0119ty w praw\u0105 stron\u0119 i \u015brednia warto\u015b\u0107 wynosi dla niego 2004 rok sprzeda\u017cy, natomiast dla klastra 1 wynosi 2006 rok.","831f79a5":"Analiza klastra 1:","0caeb3bf":"Por\u00f3wnuj\u0105c modele drzewa decyzyjnego i sieci neuronowej w celu potwierdzenia hipotezy 1 model drzewa sprawdza\u0142 si\u0119 du\u017co lepiej ni\u017c sie\u0107 neuronowa nie (r\u00f3\u017cnica o 10%). Oznacza to, \u017ce nie zawsze skomplikowane rozwi\u0105zania s\u0105 dobre dla ka\u017cdego rozwi\u0105zania. W sytuacji, kiedy jest niewielka korelacja zmiennych, a rozk\u0142ady nie s\u0105 normalne modele nieparametryczne mog\u0105 sprawdza\u0107 si\u0119 lepiej ni\u017c modele parametryczne. Jednak\u017ce hipotez\u0119 mo\u017cna potwierdzi\u0107 przy wykorzystaniu modelu sieci neuronowej","7479e616":"# Algorytm Data Mining","630ce12e":"Wykres rozproszenia i g\u0119sto\u015bci","8cc1d812":"A najcz\u0119\u015bciej sprzedaj\u0105cym si\u0119 typem gier by\u0142y gry akcji. ","e8133dec":"W ka\u017cdym regionie \u015bwiata podzia\u0142 sprzeda\u017cy gier ma si\u0119 nast\u0119puj\u0105co: ","d767f345":"Okre\u015blenie wa\u017cno\u015bci predyktor\u00f3w z u\u017cyciem wykresu.","a63c9a97":"Przeanalizujmy sprzeda\u017c \"NA_Sales\"","9605f35a":"Gdzie najpopularniejszym producentem gier by\u0142 zdecydowanie \"Eloctronic Arts\"","bfcb30a3":"Okre\u015blenie wa\u017cno\u015bci predykat\u00f3w","57ea40ae":"Tabele rozdzielcze nie do ko\u0144ca pokazuj\u0105, \u017ce gry akcji stanowi\u0105 najwi\u0119ksz\u0105 cz\u0119\u015b\u0107 w Europie. Wyprzedzaj\u0105 je m.in gry strzelankowe ze sprzeda\u017c\u0105 0.23% egzemplarzy pomi\u0119dzy 5 a 15 mln sztuk","3e528cbb":"Globalna sprzeda\u017c","d3e3b68a":"Na samym pocz\u0105tku sporz\u0105dzono wykresy rozk\u0142adu warto\u015bci zmiennych ilo\u015bciowych, aby sprawdzi\u0107 czy dana analiza jest potrzebna i przydatna do analizy.","17cea2c9":"Najdro\u017cej sprzedana gra w p\u00f3\u0142nocnej Ameryce:","93032ba0":"# Na podstawie tak obszernej i dok\u0142adnej analizy zmiennych ilo\u015bciowych oraz jako\u015bciowych, okre\u015blono trzy hipotezy kt\u00f3rym mo\u017cna si\u0119 dok\u0142adniej przyjrze\u0107 w tej pracy. \n\n* **Hipoteza 1:** Znaczn\u0105 cz\u0119\u015b\u0107 sprzeda\u017cy globalnej stanowi sprzeda\u017c gier na rynek Ameryka\u0144ski\n* **Hipoteza 2:** Sprzeda\u017c gier na PS2 ro\u015bnie z ka\u017cdym rokiem na rynku japo\u0144skim\n* **Hipoteza 3:** Dominuj\u0105cym gatunkiem gier w Europie jest gatunek gry akcji","a666fece":"Procentowy wykres sprzeda\u017cy na regiony \u015bwiata","e01661c0":"Wykresy pude\u0142kowe pozwalaj\u0105 lepiej zapozna\u0107 sie z rozk\u0142adem zmiennych w zespole danych oraz dodatkowo pozwalaj\u0105 w \u0142atwy spos\u00f3b zauwa\u017cy\u0107 warto\u015bci odstaj\u0105ce w postaci punkt\u00f3w wystaj\u0105cych poza w\u0105sy (punkty te znajduj\u0105 si\u0119 w zakresach kwantyli 0-25 oraz 75-100). Jednak zawsze przed zakwalifikowaniem jaki\u015b danych do warto\u015bci odstaj\u0105cych nale\u017cy si\u0119 zastanowi\u0107, co powinno si\u0119 z nimi zrobi\u0107 i czy wp\u0142ywaj\u0105 one w du\u017cym stopniu negatywnie na modele predykcyjne. W naszym przypadku wykresy pude\u0142kowe nie obrazuj\u0105 rozk\u0142adu zmiennych tak dok\u0142adnie, jak mo\u017cna by\u0142o si\u0119 tego spodziewa\u0107. Jest to spowodowane tym, \u017ce w ka\u017cdym rekordzie danych, kolumna \"Sales\" ma inn\u0105 warto\u015b\u0107 ilo\u015bci sprzedanych gier. Poza warto\u015bciami 0 wszystkie mo\u017cna uzna\u0107 jako odstaj\u0105ce","7fc708be":"Na powy\u017cszym wykresie  nie jest oczywista najrozs\u0105dniejsza ilo\u015b\u0107 klastr\u00f3w jak\u0105 powinno si\u0119 wybra\u0107. Podejrzewamy warto\u015b\u0107 2 lub 3 lecz do automatycznego doboru ilo\u015bci klastr\u00f3w mo\u017cna wykorzysta\u0107 silhouette coefficient score.\n\nSilhouette Coefficient jest obliczany przy u\u017cyciu \u015bredniej odleg\u0142o\u015bci wewn\u0105trz klastra (a) i \u015bredniej odleg\u0142o\u015bci do najbli\u017cszego klastra (b) dla ka\u017cdej pr\u00f3bki. Wsp\u00f3\u0142czynnik sylwetki dla pr\u00f3bki to (b - a) \/ max (a, b). Aby wyja\u015bni\u0107, b to odleg\u0142o\u015b\u0107 mi\u0119dzy pr\u00f3bk\u0105 a najbli\u017csz\u0105 gromad\u0105, kt\u00f3rej pr\u00f3bka nie jest cz\u0119\u015bci\u0105. Zwr\u00f3\u0107 uwag\u0119, \u017ce silhouette coefficient jest definiowany tylko wtedy, gdy liczba etykiet wynosi 2 <= n_labels <= n_samples -","45347450":"Europa","fc59f608":"Test niezale\u017cno\u015bci przy u\u017cyciu chi^2 przy za\u0142o\u017ceniu poziomu istotno\u015bci alfa=0.05, zak\u0142adaj\u0105c za hipotez\u0119 zerow\u0105, \u017ce zmienne s\u0105 niezale\u017cne","6ea86681":"Do sprawdzenia hipotezy pierwszej dotycz\u0105cej udzia\u0142u sprzedanych gier w P\u00f3\u0142nocnej Ameryce wzgl\u0119dem ca\u0142ego globu wybrano zmienne niezale\u017cne:\n\nNA_Sales,\nGlobal_Sales\nZmienne zale\u017cne wybrano dzi\u0119ki obserwacjom z podpunktu drugiego. W tym wypadku mamy doczynienia z przewidywania warto\u015bci dlatego, nale\u017cy wykorzysta\u0107 drzewo regresyjne","f2efedff":"Wykres dla ka\u017cdej zmiennej","6ba35d49":"Z powy\u017cszych histogram\u00f3w mo\u017cna wywnioskowa\u0107, \u017ce najwi\u0119cej gier sprzedano w latach 2008 oraz 2009 gdzie najpopularniejsz\u0105 platform\u0105 docelow\u0105 gry by\u0142o DS oraz PS2. Dominuj\u0105 gry akcji.","8d31d16f":"Statystyki dotycz\u0105ce ka\u017cdej z dost\u0119pnych zmiennych ilo\u015bciowych","ac2abfd8":"Analiza klastra 2:"}}