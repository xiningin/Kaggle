{"cell_type":{"421b52c6":"code","bb521ad9":"code","e0ab5236":"code","a53b6760":"code","965b781d":"markdown","16c083e3":"markdown","aab23529":"markdown","2799da1b":"markdown"},"source":{"421b52c6":"# Imports\nimport numpy as np\nimport pandas as pd\nimport folium as fm # For the map, will only work in Jupyter since it needs a browser\nfrom folium.plugins import FastMarkerCluster # In order to add map markers\nfrom pyproj import Proj, transform\nimport matplotlib.pyplot as plt\nimport time\n\nnp.set_printoptions(threshold=np.inf, linewidth = 500, suppress=True)\npd.set_option('display.max_rows', 3000)\npd.set_option('display.max_columns', 3000)\npd.set_option('display.width', 1000)\npd.set_option('display.float_format', lambda x: '%.3f' % x)","bb521ad9":"\"\"\"\ntrimData\nAbout: The csv that we're trying to load here is much too large to bring into Python in one shot (about 9-million x \n19). Another thing is that I'd like to throw away all entries that are from before 2018 - the data isn't in perfect \nchronological order though so I can't just look for where 2018 begins and throw everything else out. This function will \ngrab all of the 2018\/2019 incidents and save them in a new .csv.\nInput: The name of the csv that contains the full data\nOutput: A csv containing only data from 2018\/2019\n\"\"\"\ndef trimData(file_in, file_out, batchsize):\n    # This is going to be the new DataFrame that will contain only the rows that we want. We'll use the header from the\n    # original dataframe\n    d = pd.read_csv(file_in, delimiter = ',', header = 0, nrows = 0, dtype = str)\n    dat_recent = pd.DataFrame(data = d)\n\n    # The columns that we won't be using can be dropped\n    dat_recent = dat_recent.drop(['Ticket number', 'Meter Id', 'Marked Time', 'RP State Plate', 'Plate Expiry Date',\n                                'VIN', 'Make', 'Route', 'Agency', 'Violation code'], axis=1)\n\n    i = 0\n    not_done = True\n    while not_done:\n\n        print('processing batch', i, ', samples processed: ', i * batchsize)\n\n        # load in batches of 1-million entries for processing per pass\n        dat = pd.read_csv(file_in, delimiter = ',', header = 0, nrows = batchsize, skiprows = range(1, i*batchsize),\n                          dtype = str)\n\n        # Drop the columns that we don't need to save on space\n        dat = dat.drop(['Ticket number', 'Meter Id', 'Marked Time', 'RP State Plate', 'Plate Expiry Date', 'VIN',\n                         'Make', 'Route', 'Agency', 'Violation code'], axis = 1)\n\n        # if the batch has less than 1-million entries then we know that this is the last pass\n        i+=1\n        if len(dat) < batchsize:\n            not_done = False\n\n        # replace the emply fields with \"0000\", I chose that so that checking for the year can be done by checking one\n        # condition rather than two\n        dat = dat.replace(np.nan, '0000')\n\n        remove = [] # a list containing the index values to remove\n\n        # Using the .at method in a for loop is 20x quicker than using iterrows\n        for idx in dat.index:\n            if dat.at[idx, 'Issue Date'][3] != '8':\n                remove.append(idx)\n\n        dat = dat.drop(dat.index[remove])\n        dat_recent = dat_recent.append(dat)\n\n    print(dat_recent)\n    dat_recent.to_csv(file_out)\n    print('done, new .csv saved as', file_out)\n\n\"\"\"\ncleanData\nAbout: Once I have a .csv containing only the dates of interest I'd like to break up the date which is stored in a \nsingle cell as text into three cells saved as int values, along with getting rid of some other columns that I don't \nneed. I'd also like to change the original format that the time was saved as.\nInput: The name of the csv that contains the 2018 data\nOutput: A csv containing the same data in a more usable format\n\"\"\"\ndef cleanData(file_in, file_out, batchsize):\n\n    i = 0\n    not_done = True\n\n    # This will contain the newly formatted year\/month\/day stuff in three columns rather than just the one\n    cols1 = ['Year', 'Month', 'Day']\n    cols2 = ['Year', 'Month', 'Day', 'Issue time', 'Body Style', 'Color', 'Location', 'Violation Description',\n             'Fine amount',\t'Latitude',\t'Longitude']\n\n    data_cleaned = pd.DataFrame(columns = cols2)\n\n\n    while not_done:\n\n        newcols = pd.DataFrame(columns=cols1)\n\n        print('processing batch', i, ', samples processed: ', i*batchsize)\n\n        # load in batches of 1-million entries for processing per pass\n        dat = pd.read_csv(file_in, delimiter=',', header=0, nrows=batchsize, skiprows=range(1, i * batchsize),\n                          dtype = object)\n\n        # if the batch has less than 1-million entries then we know that this is the last pass\n        i += 1\n        if len(dat) < batchsize:\n            not_done = False\n\n        hold_dict = {}\n\n        for idx in dat.index:\n            hold_dict[idx] = [int(dat.at[idx,'Issue Date'][3]), int(dat.at[idx,'Issue Date'][5:7]), int(dat.at[idx,\n                                                                                            'Issue Date'][8:10])]\n\n            # Tack \"Los Angeles\" on to the location just in case we end up using that column to designate location\n            # and there is another street address with the same name somewhere else\n            dat.at[idx,'Location'] = dat.at[idx,'Location'] + ' Los Angeles'\n            dat.at[idx, 'Issue time'] = np.floor(float(dat.at[idx, 'Issue time']) \/ 100)\n\n        hold_df = pd.DataFrame.from_dict(columns = cols1, data = hold_dict, orient = 'index')\n        newcols = newcols.append(hold_df, ignore_index = True)\n\n        dat = dat.drop(['Unnamed: 0','Issue Date'], axis = 1)\n\n        data_cleaned = data_cleaned.append(pd.concat([newcols, dat], axis=1, sort = False), ignore_index = True)\n\n    print(data_cleaned)\n    data_cleaned.to_csv(file_out)\n    print('done, new .csv saved as', file_out)","e0ab5236":"# To keep track of how long it takes\nstart_time = time.time()\n\n# No need to run either if the csv's are already made\nbatchsize = 100000\ntrim_data = False\nclean_data = False\n\n# Paths don't work on Kaggle... This was written in Pycharm with the original file saved locally.\nif trim_data == True:\n    trimData('parking-citations.csv', 'parking-citations-2018-present.csv', batchsize)\n\nif clean_data == True:\n    cleanData('parking-citations-2018-present.csv', '2018-parking-citations-cleaned.csv', batchsize)\n\n# Load in the cleaned data as its own dataframe to work with\nprint('Loading data...')\n# The original path replaced with Kaggle's\nworking_data = pd.read_csv('..\/input\/new-data\/2018-parking-citations-cleaned.csv', delimiter=',', header=0, dtype = object)\n\n# Rename the first column which is duplicated upon loading the csv (csv saves the index and loading it in adds an index)\nworking_data = working_data.rename(columns = {'Unnamed: 0' : 'Index'})\n\nprint('Data loaded, producing figures and maps...')\n\n# - - - - - - - - - - - - - - - - For the map: - - - - - - - - - - - - - - - -\n\n# Take the coorinates as their own dataframe so it can be manipulated, no need to throw out the rows with bad\n# coordinate data from the full data set since they may contain other useful info\n\ncbatch = 40000 # Number of previous incidents to plot, anything over this will bog down the map\ncoords = (working_data.loc[(len(working_data) - cbatch):, 'Latitude':'Longitude']).astype(float)\n\n# Remove the cols with the 99999 values, if they're in one col they're in the other, no need to search both\ncoords = coords[coords['Latitude'] != 99999.0]\n\n# coords are in x\/y and we want lat\/long, this is from the pyproj documentation\npm = '+proj=lcc +lat_1=34.03333333333333 +lat_2=35.46666666666667 +lat_0=33.5 +lon_0=-118 +x_0=2000000 ' \\\n     '+y_0=500000.0000000002 +ellps=GRS80 +datum=NAD83 +to_meter=0.3048006096012192 +no_defs'\n\n# convert to lat\/long\nx_in,y_in = coords['Latitude'].values, coords['Longitude'].values\nlat,long = transform(Proj(pm, preserve_units = True), Proj(\"+init=epsg:4326\"), x_in,y_in)\n\nLA_coords = [34.05 , -118.24]\nm = fm.Map(location=LA_coords, zoom_start=10.5)\n\n# add map markers, plots as \"long\/lat\" rather than \"lat\/long\"\nFastMarkerCluster(data=list(zip(long, lat))).add_to(m)\ndisplay(m) \n","a53b6760":"# - - - - - - - - - - - - - - - - For the figures - - - - - - - - - - - - - - - -\n# Taking pieces of the data so I can manipulate them without affecting the original data, since I might want to use\n# it for new things at some point\n\n# Count the incidents per month\nmonth_counts = working_data.groupby(by = 'Month', as_index=False).agg({'Index' : pd.Series.nunique})\nmonth_counts = month_counts.astype(int)\nmonth_counts = month_counts.sort_values(by = ['Month'], ascending = True)\n\nf1 = plt.figure(figsize=(16, 7))\nplt.bar(month_counts['Month'], month_counts['Index'])\nplt.title('Violations Per Month (2018)')\nplt.xlabel('Month')\nplt.ylabel('Number of Violations')\n\n# Violations according to hour\ntime_counts = working_data.groupby(by = 'Issue time', as_index=False).agg({'Index' : pd.Series.nunique})\ntime_counts = time_counts.astype(float)\ntime_counts = time_counts.sort_values(by = ['Issue time'], ascending = True)\n\nf2 = plt.figure(figsize=(16, 7))\nplt.bar(time_counts['Issue time'], time_counts['Index'])\nplt.title('Violations Grouped by Hour of Occurrence (2018)')\nplt.xlabel('Time (24 hr. clock)')\nplt.ylabel('Number of Violations')\n\n# Reasons for violations\nreason_counts = working_data.groupby(by = 'Violation Description', as_index=False).agg({'Index' : pd.Series.nunique})\nreason_counts = reason_counts.sort_values(by = ['Index'], ascending = False)\nreason_counts = reason_counts[reason_counts['Index'] > 20000]\n\nf3 = plt.figure(figsize=(16, 7))\nplt.bar(reason_counts['Violation Description'], reason_counts['Index'])\nplt.title('Top Violation Reasons')\nplt.xlabel('Violation Reason')\nplt.xticks(rotation=70)\nplt.tight_layout()\nplt.ylabel('Number of Violations')\n\n\nplt.show()\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","965b781d":"### Functions (not necessary to run unless running from scratch from the raw .csv)","16c083e3":"# Visualizing LA Parking Violations from 2018\n## For the map and the visualizations, scroll to the bottom of the page.\nThe point of this side project is to create a map of LA according to parking tickets received from last year (2018)\nand to produce some helpful visualizations. I want to start with the full dataset (~9-million x 19, saved as a .csv > 1gb) to gain more experience handling large .csv files that are cumbersome to work with in excel and are slow to work with in Python unless processed in batches (especially when you don't have much RAM available). \n\n### Note for people viewing on Kaggle: I'm new to Kaggle, I had to make some modifications to the original code since as far as I can tell the input folder doesn't allow write permissions. The functions trimData and cleanData both write to the directory, so keep the variables trim_data and clean_data as false. I've seen some great work on here, I just thought I'd upload my own work since I hadn't seen anyone look at a whole year's worth of data and I figured some may be interested in the batch processing technique when dealing with large .csv's. Please give me some feedback if you have any!","aab23529":"### Main Program","2799da1b":"### Imports and Settings"}}