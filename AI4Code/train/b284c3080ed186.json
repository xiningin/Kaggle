{"cell_type":{"f1311291":"code","ea202f8f":"code","3fe59bd8":"code","40ec6efb":"code","a2b31d6c":"code","beaa7b49":"code","58bb2856":"code","71c732a8":"code","9b352533":"code","5e70c533":"code","9d6b456d":"code","6998aa1e":"code","008ab496":"code","c88b8f6f":"code","442c1fad":"code","75a9596b":"code","a751405c":"code","d63a362f":"code","2c3c8146":"code","4b9c1767":"code","b0bc7fec":"code","43fc7615":"code","4f80c637":"code","7721ce10":"code","aed40dcf":"code","d8520a1e":"code","b5371e1b":"code","3b4f4f1e":"code","581dc9fc":"code","68d46771":"code","41f78021":"code","74e1011e":"code","eb055a86":"code","6d3ba4f8":"code","a56fa530":"code","d9d2129e":"code","5598368f":"code","dc5fa205":"code","c04848f0":"code","2b1822b6":"code","a6b08c57":"code","9df7c684":"code","4873eea3":"code","64477d78":"code","4c20fbb3":"code","f168becc":"code","d7db2ef2":"code","2e732e35":"code","f3452302":"code","f9db2bee":"code","836182b5":"code","8904832e":"code","db8f4340":"code","565c0cea":"markdown","2565fc6c":"markdown","f06d73ab":"markdown","b373a20a":"markdown","294abd7b":"markdown","a3d3e5ad":"markdown","7c2ccc23":"markdown","c2113481":"markdown","35405045":"markdown","95171ccb":"markdown","1b74aadd":"markdown","3a2fcc6a":"markdown","1b49b2fb":"markdown","e8a4a981":"markdown","6b135945":"markdown","0698d83d":"markdown","cdd1f7b8":"markdown","79f22881":"markdown","a1166198":"markdown","fd21c46a":"markdown","7acc90d7":"markdown","e10580ef":"markdown","53f3796a":"markdown","7f16c268":"markdown","517157ee":"markdown","bdad0a92":"markdown","3136d02e":"markdown"},"source":{"f1311291":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport pickle\nimport plotly.express as px\nimport seaborn as sns\nimport shutil\n\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import MaxPooling2D, Conv2D, Dense, Flatten, Dropout, GlobalAveragePooling2D\nfrom keras.models import Sequential, load_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing import image\n\nfrom pathlib import Path\nfrom PIL.ExifTags import TAGS, GPSTAGS\nfrom PIL import Image\n\nfrom skimage.feature import hog\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\nfrom skimage import exposure\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n\nimport tensorflow as tf\nimport keras.backend as keras","ea202f8f":"np.random.seed(1)\ntf.random.set_seed(1)","3fe59bd8":"# create directory paths\nold_base_dir = os.path.join('\/kaggle\/input\/chest-xray-pneumonia', 'chest_xray')\nold_train_dir = os.path.join(old_base_dir, 'train')\nold_val_dir = os.path.join(old_base_dir, 'val')\nold_test_dir = os.path.join(old_base_dir, 'test')","40ec6efb":"def count_data(base_dir, directories):\n    \"\"\" Count number of files in selected sub-dirs, where directories \n        is a list of strings for each sub-directory path. Also returns a \n        dictionary of all img paths (values) for each sub-dir (keys).\n    \"\"\"\n    \n    # list to store img counts, and dict to store image paths\n    file_counts = []\n    img_paths = {}\n    \n    for directory in directories:\n        \n        img_files = [x for x in os.listdir(os.path.join(base_dir, directory)) \n                 if x.endswith('.jpeg')]\n        \n        # find paths to all imgs\n        path_names = [os.path.join(base_dir, directory, x) for x in img_files]\n        \n        # count img no. and append to file counts\n        num_files = len(img_files)\n        file_counts.append(num_files)\n    \n        # update dict of paths with the given imgs for the sub-dir\n        key_name = directory.replace('\/', '_').lower()\n        img_paths[key_name] = path_names\n    \n    return file_counts, img_paths","a2b31d6c":"split_type = ['train', 'val', 'test']\nclass_type = ['PNEUMONIA', 'NORMAL']\ndirectories = [f\"{x}\/{y}\" for x in split_type for y in class_type]\n\ncounts, img_paths = count_data(old_base_dir, directories)\n\nfor subdir, count in zip(directories, counts):\n    print(f\"{subdir} : {count}\")\n    \nsns.barplot(y=directories, x=counts)\nplt.show()","beaa7b49":"# create new directory paths\nbase_dir = os.path.join('\/kaggle\/working', 'chest_xray')\ntrain_dir = os.path.join(base_dir, 'train')\nvalidation_dir = os.path.join(base_dir, 'val')\ntest_dir = os.path.join(base_dir, 'test')","58bb2856":"%%time\n\n# iterate through each sub-dir - count imgs and form dict of paths\nfor directory in directories:\n    \n    # create new directory structure in kaggle working dir\n    new_dir = os.path.join(base_dir, directory)\n    Path(new_dir).mkdir(parents=True, exist_ok=True)\n    \n    # gather img files in kaggle read-only dir\n    img_files = [x for x in os.listdir(os.path.join(old_base_dir, directory)) \n                 if x.endswith('.jpeg')]\n    \n    # find paths to old and new paths for images in current directory\n    old_path_names = [os.path.join(old_base_dir, directory, x) for x in img_files]\n    new_path_names = [os.path.join(new_dir, x) for x in img_files]\n    \n    print(f\"Moving and resizing directory: {directory}\\n\")\n    \n    for i in range(len(old_path_names)):\n        \n        # load img, resize, and save to new location\n        img = Image.open(old_path_names[i])\n        img_new = img.resize((360,320), Image.ANTIALIAS)\n        img_new.save(new_path_names[i], 'JPEG', quality=90)","71c732a8":"def move_img_data(source_dir, destination_dir, proportion=0.2, suffix='.jpeg'):\n    \"\"\" Move a random proportion of img data from a source to destination directory \"\"\"\n    \n    img_files = [x for x in os.listdir(source_dir) if x.endswith(suffix)]\n    \n    move_num = int(np.ceil(len(img_files)*proportion))\n    \n    # select random proportion of images to move\n    random_indices = np.random.permutation(len(img_files))[:move_num]\n    \n    print(f\"Moving a total of {move_num} images from \"\n          f\"{source_dir} to {destination_dir}\\n\")\n    \n    # move selected images to destination loc\n    for index in random_indices:\n        src_path = os.path.join(source_dir, img_files[index])\n        dest_path = os.path.join(destination_dir, img_files[index])\n        shutil.copyfile(src_path, dest_path)","9b352533":"# move 20% of training samples from train to val dir for both classes - ONLY RUN ONCE\nmove_img_data(os.path.join(train_dir, 'NORMAL'), \n              os.path.join(validation_dir, 'NORMAL'),\n              proportion=0.2)\nmove_img_data(os.path.join(train_dir, 'PNEUMONIA'), \n              os.path.join(validation_dir, 'PNEUMONIA'),\n              proportion=0.2)","5e70c533":"counts, img_paths = count_data(base_dir, directories)\n\nfor subdir, count in zip(directories, counts):\n    print(f\"{subdir} : {count}\")\n    \nsns.barplot(y=directories, x=counts)\nplt.show()","9d6b456d":"def create_dataframe(data_dir):\n    \"\"\" Returns a dataframe consisting of img path and label, where\n        0 is normal and 1 is pneumonia \"\"\"\n    data = []\n    labels = []\n    \n    # obtain image paths for all training data\n    normal_dir = os.path.join(data_dir, 'NORMAL')\n    pneunomia_dir = os.path.join(data_dir, 'PNEUMONIA')\n    normal_data = [x for x in os.listdir(normal_dir) if x.endswith('.jpeg')]\n    pneunomia_data = [x for x in os.listdir(pneunomia_dir) if x.endswith('.jpeg')]\n    \n    # append img path and labels for each\n    for normal in normal_data:\n        data.append(os.path.join(normal_dir, normal))\n        labels.append(0) \n    for pneumonia in pneunomia_data:\n        data.append(os.path.join(pneunomia_dir, pneunomia_dir))\n        labels.append(1)\n        \n    # return pandas dataframe\n    return pd.DataFrame({'Image_path' : data, 'Label' : labels})","6998aa1e":"train_df = create_dataframe(train_dir)\nval_df = create_dataframe(validation_dir)\ntest_df = create_dataframe(test_dir)","008ab496":"train_df['Label'].value_counts().plot.bar()\nplt.show()","c88b8f6f":"def duplicate_data(file_dir, suffix='.jpeg'):\n    \"\"\" duplicate img data within destination directory \"\"\"\n    \n    img_files = [x for x in os.listdir(file_dir) if x.endswith(suffix)]\n    \n    for img in img_files:\n        src_path = os.path.join(file_dir, img)\n        dup_img = f\"{img[:-len(suffix)]}_2{suffix}\"\n        dest_path = os.path.join(file_dir, dup_img)\n        shutil.copyfile(src_path, dest_path)","442c1fad":"duplicate_data(os.path.join(train_dir, 'NORMAL'))","75a9596b":"train_df = create_dataframe(train_dir)\ntrain_df['Label'].value_counts().plot.bar()\nplt.show()","a751405c":"fig = plt.figure(figsize=(12, 6))\n\nfor i, example in enumerate(img_paths['train_pneumonia'][:5]):\n    \n    ax = fig.add_subplot(2, 5, i+1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    \n    # read image and plot\n    example_img = tf.io.read_file(example)\n    example_img = tf.image.decode_jpeg(example_img, channels=3)\n    example_img = tf.image.resize(example_img, [360, 320])\n    example_img \/= 255.0\n    ax.imshow(example_img)\n    ax.set_title(f\"Pneumonia {i}\")\n    \nfor i, example in enumerate(img_paths['train_normal'][:5]):\n    \n    ax = fig.add_subplot(2, 5, i+6)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    \n    # read image and plot\n    example_img = tf.io.read_file(example)\n    example_img = tf.image.decode_jpeg(example_img, channels=3)\n    example_img = tf.image.resize(example_img, [360, 320])\n    example_img \/= 255.0\n    ax.imshow(example_img)\n    ax.set_title(f\"Normal {i}\")","d63a362f":"img_height, img_width = 150, 150\nbatch_size = 10\n\n# training data augmentation - rotate, shear, zoom and flip\ntrain_datagen = ImageDataGenerator(\n    rotation_range = 30,\n    rescale = 1.0 \/ 255.0,\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip = True,\n    vertical_flip=True)\n\n# no augmentation for test data - only rescale\ntest_datagen = ImageDataGenerator(rescale = 1. \/ 255.0)\n\n# generate batches of augmented data from training data\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary')\n\n# generate val data from val dir\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary')\n\nnb_train_samples = len(train_generator.classes)\nnb_validation_samples = len(validation_generator.classes)","2c3c8146":"# get class labels dict containing index of each class for decoding predictions\nclass_labels = train_generator.class_indices\n\nclass_labels","4b9c1767":"def create_CNN(input_size=(150, 150)):\n    \"\"\" Basic CNN with 4 Conv layers, each followed by a max pooling \"\"\"\n    cnn_model = Sequential()\n    \n    # four Conv layers with max pooling\n    cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n    cnn_model.add(MaxPooling2D(2, 2))\n    cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n    cnn_model.add(MaxPooling2D(2, 2))\n    cnn_model.add(Conv2D(128, (3, 3), activation='relu'))\n    cnn_model.add(MaxPooling2D(2, 2))\n    cnn_model.add(Conv2D(128, (3, 3), activation='relu'))\n    cnn_model.add(MaxPooling2D(2, 2))\n    \n    # flatten output and feed to dense layer, via dropout layer\n    cnn_model.add(Flatten())\n    cnn_model.add(Dropout(0.5))\n    cnn_model.add(Dense(512, activation='relu'))\n    \n    # add output layer - sigmoid since we only have 2 outputs\n    cnn_model.add(Dense(1, activation='sigmoid'))\n    \n    cnn_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    \n    return cnn_model","b0bc7fec":"CNN_model = create_CNN()\nCNN_model.summary()","43fc7615":"# set up a check point for our model - save only the best val performance\nsave_path =\"basic_cnn_best_weights.hdf5\"\n\ntrg_checkpoint = ModelCheckpoint(save_path, monitor='val_accuracy', \n                                 verbose=1, save_best_only=True, mode='max')\n\ntrg_callbacks = [trg_checkpoint]","4f80c637":"# batch steps before an epoch is considered complete (trg_size \/ batch_size):\nsteps_per_epoch = np.ceil(nb_train_samples\/batch_size)\n\n# validation batch steps (val_size \/ batch_size):\nval_steps_per_epoch = np.ceil(nb_validation_samples\/batch_size)","7721ce10":"history = CNN_model.fit(train_generator, epochs=50, \n                        steps_per_epoch=steps_per_epoch, \n                        validation_data=validation_generator, \n                        validation_steps=val_steps_per_epoch,\n                        callbacks=trg_callbacks,\n                        shuffle=True)","aed40dcf":"# save model as a HDF5 file with weights + architecture\nCNN_model.save('basic_cnn_model_1.hdf5')\n\n# save the history of training to a datafile for later retrieval\nwith open('history_basic_cnn_model_1.pickle', \n          'wb') as pickle_file:\n    pickle.dump(history.history, pickle_file)\n    \nloaded_model = False","d8520a1e":"# if already trained - import history file and best training weights\nCNN_model = load_model('basic_cnn_best_weights.hdf5')","b5371e1b":"# if already trained - import history file and training weights\n#CNN_model = load_model('models\/basic_cnn_model_1.hdf5')\n\n# get history of trained model\n#with open('models\/history_basic_cnn_model_1.pickle', 'rb') as handle:\n#    history = pickle.load(handle)\n    \n#loaded_model = True","3b4f4f1e":"# if loaded model set history accordingly\nif loaded_model:\n    trg_hist = history\nelse:\n    trg_hist = history.history\n\ntrg_loss = trg_hist['loss']\nval_loss = trg_hist['val_loss']\n\ntrg_acc = trg_hist['accuracy']\nval_acc = trg_hist['val_accuracy']\n\nepochs = range(1, len(trg_acc) + 1)\n\n# plot losses and accuracies for training and validation \nfig = plt.figure(figsize=(12,6))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss, marker='o', label='Training Loss')\nplt.plot(epochs, val_loss, marker='x', label='Validation Loss')\nplt.title(\"Training \/ Validation Loss\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n\nax = fig.add_subplot(1, 2, 2)\nplt.plot(epochs, trg_acc, marker='o', label='Training Accuracy')\nplt.plot(epochs, val_acc, marker='^', label='Validation Accuracy')\nplt.title(\"Training \/ Validation Accuracy\")\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()","581dc9fc":"test_generator = test_datagen.flow_from_directory(test_dir, target_size=(img_height, img_width), \n                                                  batch_size=4, class_mode='binary')\n\ntest_loss, test_accuracy = CNN_model.evaluate_generator(test_generator)\nprint(f\"Test accuracy: {test_accuracy}\")","68d46771":"# generate val data from val dir\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary',\n    shuffle=False)","41f78021":"y_labels = np.expand_dims(test_generator.classes, axis=1)\nnb_test_samples = len(y_labels)\n\ny_preds = CNN_model.predict(test_generator, \n                            steps=np.ceil(nb_test_samples\/batch_size))\n\n# round predictions to 0 (Normal) or 1 (Pneumonia)\ny_preds = np.rint(y_preds)","74e1011e":"# number of incorrect labels\nincorrect = (y_labels[:, 0] != y_preds[:, 0]).sum()\n\n# print the basic results of the model\nprint(f\"Accuracy: {accuracy_score(y_labels[:, 0], y_preds[:, 0])*100:.2f}%\")\nprint(f\"F1 Score: {f1_score(y_labels[:, 0], y_preds[:, 0]):.2f}\")\nprint(f\"Samples incorrectly classified: {incorrect} out of {len(y_labels)}\")","eb055a86":"# print recall, precision and f1 score results\nprint(classification_report(y_labels[:, 0], y_preds[:, 0]))","6d3ba4f8":"def plot_confusion_matrix(true_y, pred_y, title='Confusion Matrix', figsize=(8,6)):\n    \"\"\" Custom function for plotting a confusion matrix for predicted results \"\"\"\n    conf_matrix = confusion_matrix(true_y, pred_y)\n    conf_df = pd.DataFrame(conf_matrix, columns=np.unique(true_y), index = np.unique(true_y))\n    conf_df.index.name = 'Actual'\n    conf_df.columns.name = 'Predicted'\n    plt.figure(figsize = figsize)\n    plt.title(title)\n    sns.set(font_scale=1.4)\n    sns.heatmap(conf_df, cmap=\"Blues\", annot=True, \n                annot_kws={\"size\": 16}, fmt='g')\n    plt.show()\n    return\n\n\n# plot a confusion matrix of our results\nplot_confusion_matrix(y_labels[:, 0], y_preds[:, 0], title=\"Basic ConvNet Confusion Matrix\")","a56fa530":"# get class labels dict containing index of each class for decoding predictions\nclass_labels = train_generator.class_indices\n\n# obtain a reverse dict to convert index into class labels\nreverse_class_index = {i : class_label for class_label, i in class_labels.items()}","d9d2129e":"class_labels","5598368f":"def process_and_predict_img(image_path, model, img_size=(150, 150)):\n    \"\"\" Utility function for making predictions for an image. \"\"\"\n    img_path = image_path\n    img = image.load_img(img_path, target_size=img_size)\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = test_datagen.standardize(x)\n    predictions = model.predict(x)\n    return img, predictions","dc5fa205":"img, prediction = process_and_predict_img(img_paths['test_normal'][0], \n                                          model=CNN_model)\nplt.imshow(img)\nplt.title(f\"Prediction: {reverse_class_index[np.argmax(prediction)]}\\n\")\nplt.show()","c04848f0":"def f1_score(y_true, y_pred):\n    \"\"\" Find and return the F1 Score \"\"\"\n    y_pred = keras.round(y_pred)\n    \n    # calculate true pos, true neg, false pos, false neg\n    true_pos = keras.sum(keras.cast(y_true*y_pred, 'float'), axis=0)\n    true_neg = keras.sum(keras.cast((1 - y_true)*(1 - y_pred), 'float'), axis=0)\n    false_pos = keras.sum(keras.cast((1- y_true)*y_pred, 'float'), axis=0)\n    false_neg = keras.sum(keras.cast(y_true*(1 - y_pred), 'float'), axis=0)\n\n    # calculate precision \/ recall, adding epsilon to prevent zero div error(s)\n    precision = true_pos \/ (true_pos + false_pos + keras.epsilon())\n    recall = true_pos \/ (true_pos + false_neg + keras.epsilon())\n\n    # calculate f1 score and return\n    f1_score = (2.0 * precision * recall) \/ (precision + recall + keras.epsilon())\n    f1_score = tf.where(tf.math.is_nan(f1_score), tf.zeros_like(f1_score), f1_score)\n    return keras.mean(f1_score)\n\n\ndef f1_loss(y_true, y_pred):\n    \"\"\" Calculate mean F1 and return minimising function to approximate a loss equivalent. \"\"\"\n    return 1 - f1_score(y_true, y_pred)","2b1822b6":"def basic_CNN_2(input_size=(150, 150)):\n    \"\"\" Basic CNN with 4 Conv and max pooling layers, with custom F1 loss \"\"\"\n    cnn_model = Sequential()\n    \n    # four Conv layers with max pooling\n    cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n    cnn_model.add(MaxPooling2D(2, 2))\n    cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n    cnn_model.add(MaxPooling2D(2, 2))\n    cnn_model.add(Conv2D(128, (3, 3), activation='relu'))\n    cnn_model.add(MaxPooling2D(2, 2))\n    cnn_model.add(Conv2D(128, (3, 3), activation='relu'))\n    cnn_model.add(MaxPooling2D(2, 2))\n    \n    # flatten output and feed to dense layer, via dropout layer\n    cnn_model.add(Flatten())\n    cnn_model.add(Dropout(0.5))\n    cnn_model.add(Dense(512, activation='relu'))\n    \n    # add output layer - sigmoid since we only have 2 outputs\n    cnn_model.add(Dense(1, activation='sigmoid'))\n    \n    cnn_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', f1_score])\n    \n    return cnn_model","a6b08c57":"CNN_model_2 = basic_CNN_2()\nCNN_model_2.summary()","9df7c684":"# set up a check point for our model - save only the best val performance\nsave_path =\"basic_cnn_2_best_weights.hdf5\"\n\ntrg_checkpoint = ModelCheckpoint(save_path, monitor='val_f1_score', \n                                 verbose=1, save_best_only=True, mode='max')\n\ntrg_callbacks = [trg_checkpoint]\n\n# batch steps before an epoch is considered complete (trg_size \/ batch_size):\nsteps_per_epoch = np.ceil(nb_train_samples\/batch_size)\nval_steps_per_epoch = np.ceil(nb_validation_samples\/batch_size)","4873eea3":"history = CNN_model_2.fit(train_generator, epochs=50, \n                          steps_per_epoch=steps_per_epoch, \n                          validation_data=validation_generator, \n                          validation_steps=val_steps_per_epoch,\n                          callbacks=trg_callbacks,\n                          shuffle=True)","64477d78":"# save model as a HDF5 file with weights + architecture\nCNN_model_2.save('basic_cnn_model_2.hdf5')\n\n# save the history of training to a datafile for later retrieval\nwith open('history_basic_cnn_model_2.pickle', \n          'wb') as pickle_file:\n    pickle.dump(history.history, pickle_file)\n    \nloaded_model = False","4c20fbb3":"# if already trained - import history file and best training weights\nCNN_model_2 = load_model('basic_cnn_2_best_weights.hdf5', custom_objects={'f1_score' : f1_score})","f168becc":"# get history of trained model\n#with open('history_basic_cnn_model_2.pickle', 'rb') as handle:\n#    history = pickle.load(handle)\n    \n#loaded_model = True","d7db2ef2":"# if loaded model set history accordingly\nif loaded_model:\n    trg_hist = history\nelse:\n    trg_hist = history.history\n\ntrg_loss = trg_hist['loss']\nval_loss = trg_hist['val_loss']\n\ntrg_acc = trg_hist['accuracy']\nval_acc = trg_hist['val_accuracy']\n\nepochs = range(1, len(trg_acc) + 1)\n\ntrg_loss = trg_hist['loss']\nval_loss = trg_hist['val_loss']\n\ntrg_acc = trg_hist['accuracy']\nval_acc = trg_hist['val_accuracy']\n\ntrg_f1 = trg_hist['f1_score']\nval_f1 = trg_hist['val_f1_score']\n\nepochs = range(1, len(trg_acc) + 1)\n\n# plot losses and accuracies for training and validation \nfig = plt.figure(figsize=(15,5))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss, marker='o', label='Training Loss')\nplt.plot(epochs, val_loss, marker='x', label='Validation Loss')\nplt.title(\"Training \/ Validation Loss\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n\nax = fig.add_subplot(1, 2, 2)\nplt.plot(epochs, trg_acc, marker='o', label='Training Accuracy')\nplt.plot(epochs, val_acc, marker='^', label='Validation Accuracy')\nplt.title(\"Training \/ Validation Accuracy\")\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()\n\n# plot F1 scores\nfig = plt.figure(figsize=(7,5))\nax = fig.add_subplot(1, 1, 1)\nplt.plot(epochs, trg_acc, marker='o', label='Training F1')\nplt.plot(epochs, val_acc, marker='^', label='Validation F1')\nplt.title(\"Training \/ Validation F1 Score\")\nax.set_ylabel(\"F1 Score\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()","2e732e35":"# generate val data from val dir\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary',\n    shuffle=False)\n\ny_labels = np.expand_dims(test_generator.classes, axis=1)\nnb_test_samples = len(y_labels)","f3452302":"y_preds = CNN_model_2.predict(test_generator, \n                              steps=np.ceil(nb_test_samples\/batch_size))\n\n# round predictions to 0 (Normal) or 1 (Pneumonia)\ny_preds = np.rint(y_preds)","f9db2bee":"# number of incorrect labels\nincorrect = (y_labels[:, 0] != y_preds[:, 0]).sum()\n\n# print the basic results of the model\nprint(f\"Accuracy: {accuracy_score(y_labels[:, 0], y_preds[:, 0])*100:.2f}%\")\nprint(f\"F1 Score: {f1_score(y_labels[:, 0], y_preds[:, 0]):.2f}\")\nprint(f\"Samples incorrectly classified: {incorrect} out of {len(y_labels)}\")","836182b5":"# print recall, precision and f1 score results\nprint(classification_report(y_labels[:, 0], y_preds[:, 0]))","8904832e":"def plot_confusion_matrix(true_y, pred_y, title='Confusion Matrix', figsize=(8,6)):\n    \"\"\" Custom function for plotting a confusion matrix for predicted results \"\"\"\n    conf_matrix = confusion_matrix(true_y, pred_y)\n    conf_df = pd.DataFrame(conf_matrix, columns=np.unique(true_y), index = np.unique(true_y))\n    conf_df.index.name = 'Actual'\n    conf_df.columns.name = 'Predicted'\n    plt.figure(figsize = figsize)\n    plt.title(title)\n    sns.set(font_scale=1.4)\n    sns.heatmap(conf_df, cmap=\"Blues\", annot=True, \n                annot_kws={\"size\": 16}, fmt='g')\n    plt.show()\n    return\n\n\n# plot a confusion matrix of our results\nplot_confusion_matrix(y_labels[:, 0], y_preds[:, 0], title=\"ConvNet (F1 loss) Confusion Matrix\")","db8f4340":"try:\n    shutil.rmtree(base_dir)\nexcept OSError as e:\n    print(\"Error: %s : %s\" % (base_dir, e.strerror))","565c0cea":"---\n\n## 4. Further experimentation - Training our previous CNN with a custom F1 Metric\n\nKeras, by default, does not support the optimisation of F1 score directly during training like it does with accruracy. We'll compensate for this by producing a set of custom F1 Score and F1 loss functions to perform this, and evaluate the difference in performance between the previous model and this newly optimised model.\n\nOne thing to be careful with in this case is that Keras works in batches for calculating metrics, which in the case of precision, recall and F1 score can lead to inconsistent and misleading end-products. Never the less, we'll proceed and create a custom F1 score and loss function, and evaluate how well (or not) our model improves compared to previously.","2565fc6c":"###### Form a dataframe for each of our data splits","f06d73ab":"We can also visually represent these results using a **confusion matrix**:","b373a20a":"###### Train model - only conduct once, and import model thereafter (long training time) ","294abd7b":"###### Plot model training results\n\nNote - these results could be improved by importing the best weights found during training (as saved above) instead.","a3d3e5ad":"#### Lets visualise some normal and pneunomia examples from our data","7c2ccc23":"Let's find the basic performance - accuracy, F1 score and number of samples incorrectly classified:","c2113481":"### Limitations with accuracy as a metric\n\nAs shown, our model weights that achieved a peak validation score of around 96% also score a test accuracy of 91.5%. This is good, however accuracy is actually a poor metric in this case. Due to the great imbalance in data, achieving a high scoring accuracy on this dataset is not difficult if we simply predict pneumonia everytime.\n\nFor prediction of pneumonia, we are much more interested in positively identifying a pneumonia case in order to help patients with the appropriate care. This means we should maximise our True Positive Rate (TPR) as much as possible, and therefore our recall is extremely important in this case:\n\n$ Recall = TPR = \\displaystyle\\frac{TP}{FN + TP} $\n\nWhere TP and FN are True Positives and False Negatives respectively.\n\nRecall isn't the only important metric though - precision is also important to reduce inadvertent classifications of patients as having pneumonia when in fact they do not:\n\n$ Precision = \\displaystyle\\frac{TP}{FP + TP} $\n\nWhere FP is the number of False Positives.\n\nSince both of these metrics are important for our model, we can combine both through calculating the F1 score. This is obtained using a combination of Precision and Recall, like so:\n\n$ F1 = 2 \\times \\displaystyle\\frac{Precision \\times Recall}{Precision + Recall} $","35405045":"All we need to do now is repeat the same training performed previously for the basic CNN, but this time include F1 score in the metrics, and use our custom F1 loss function above as the selected loss.\n\nDue to these changes that have taken place, let's redefine our CNN creation function:","95171ccb":"# Analysing Chest X-Rays for Classification of Pneumonia\n\nBasic analysis and prediction of the dataset, with formation and training of basic ConvNets. The first ConvNet uses basic accuracy as an evaluation metric, whilst the second improves on this by usage of a custom F1 Score metric.\n\nDuring initial data exploration, the original directory of x-ray images is copied to the Kaggle working directory after re-sizing the images. In addition, 20% of the original training dataset is added to the validation set to correct the issues with the original data provided (only 8 of each class in the validation set).","1b74aadd":"#### Data preprocessing and augmentation of our images using Keras ImageDataGenerator\n\nTo start, we'll take the built-in Keras ImageDataGenerator to help carry out data augmentation during training.","3a2fcc6a":"###### WARNING - only run this once per notebook session, since it will relocate images from the training to validation directories.","1b49b2fb":"Classification report of performance, including precision, recall, and F1 scores across classes:","e8a4a981":"---\n\n## 2. Model Training - Basic Convolutional Neural Network\n\n**Model features:**\n- Four Conv Layers with max pooling\n- Flatten layer with 50% dropout\n- Dense layer with 512 hidden units and relu activation\n- Output sigmoid\n- RMSPROP optimiser with binary cross-entropy loss\n\n**Results obtained:**\n- Accuracy of 91.51%\n- F1 Score: 0.93\n- Precision: 0.92 weighted average, 0.95 normal, 0.90 pneumonia\n- Recall: 0.92 weighted average, 0.81 normal, 0.98 pneumonia\n\n**Brief overview of process:**\n\nWe need to take our dataset of jpeg images and preprocess them accordingly prior to use in our deep learning model. In summary, we need to perform the following:\n\n- Read in each of our images as a jpeg file\n- Decode each image into floating-point tensor form, with RGB grids of pixels for each image\n- Standardise our images through rescaling of the pixel values.\n\nThese functions are performed automatically using the data generators we created previously. For our first convolutional neural network, we'll form a custom smaller sized ConvNet.","6b135945":"###### Load best weights model (if the above steps have already been conducted once)\n\n","0698d83d":"###### Evaluate results\n\nNote - could improve on these results through use of the best weights saved during training (above). We can simply import this 'best' model and use that instead.","cdd1f7b8":"Much better - although we've only duplicated our normal data, it helps prevent our model being over-fitted to the dominant class during training.","79f22881":"---\n\n## 3. Making predictions on chosen invididual test set images\n\nAbove we made predictions on the entire test dataset and evaluated our performance. \n\nHowever, to actually use a model such as this, we need to be able to easily make predictions on individual chosen images. This is effectively what we want to achieve by forming a model such as this for classifying pneumonia. \n\nIn the next few samples of code we'll make some helper functions to do precisely this.","a1166198":"Much better! Now make sure the above code for moving the images from the training to validation directory is commented out, otherwise we'll loose more and more data from the training directory each time our notebook is run.\n\nWe've still got issues with an imbalance of data classes, however this can be rectified later on. We can choose to either duplicate under-represented data, or apply data augmentation to our images.\n\n**Note:** Before moving on in this notebook, consider downloading this new re-balanced dataset, and then uploading it to the kernel as new input data. Then you can comment out all the movement and resizing code above. This will prevent the kernel outputing thousands of images each time you run it in the future. At the bottom of this kernel, we will remove this dataset from the kaggle output working directory to prevent thousands of output files.","fd21c46a":"## Finally - remove dataset from kaggle output dir (prevents thousands of images being output - uncomment if you want to keep the dataset formed)","7acc90d7":"To compensate for the large imbalance, we'll do the simplest option - oversample the under-represented class.","e10580ef":"###### Move original x-ray data to Kaggle working directory after resizing","53f3796a":"The default distribution of labels, as shown above, is fundamentally flawed. Having only 8 validation samples for each class is not enough to ensure sufficient validation of our model and adjustment of hyper-parameters during training. To correct this, we'll rebalance the dataset accordingly. We'll move 20% of samples currently in the training data into the validation data.\n\nTo do this in a kaggle kernel, we'll have to move the read-only input data into our working directory, but during this move we'll rebalance the data as required.","7f16c268":"When compared to the initial results obtained using the default data, no data augmentation, no regularisation (using dropout), our model is significantly better. This highlights the increases in performance that can be obtained through over-sampling of under-represented classes and performing image data-augmentation when we have a dataset with imbalanced classes or a low number of samples.","517157ee":"---\n \n## 1. Dataset exploration and pre-processing","bdad0a92":"###### Load model with best weights from training conducted above (assuming the above steps have already been conducted once)","3136d02e":"###### Re-run the previous code to count the number of labels distributed throughout our directories"}}