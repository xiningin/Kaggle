{"cell_type":{"0991703d":"code","9408f69e":"code","27394959":"code","e703204d":"code","abb7bbe3":"code","3fa27e13":"code","d9049b60":"code","35caa29b":"code","308ac3d4":"code","8b83c714":"code","4373886d":"code","04474a80":"code","74f03e58":"code","0209ceef":"code","d8ff50b9":"code","74b06dfa":"code","5d90e1be":"code","c4766452":"code","03c567be":"code","213bb9e1":"code","a0e7f612":"code","59bcc3a1":"code","133b881f":"markdown","5669a089":"markdown","fab47832":"markdown","ec3a78be":"markdown","c4b30ab7":"markdown","f92acdd8":"markdown","0b5cadaa":"markdown","f22f837d":"markdown","3b382978":"markdown","1631436b":"markdown","920f54ed":"markdown","bb0dafdd":"markdown","7f873db3":"markdown","af1b00f2":"markdown","de585e72":"markdown","2cbe6bf0":"markdown","61a76524":"markdown","10817ba8":"markdown","0730fb32":"markdown","08ce9404":"markdown","f4b7cf7b":"markdown","84c78ae4":"markdown","c1771dda":"markdown","59813143":"markdown","e0918328":"markdown","edbe4827":"markdown","daef8f0f":"markdown","f18f7fa7":"markdown","24f53f8f":"markdown","1c75bd7b":"markdown","f0fa600c":"markdown","113cffb4":"markdown","09c23d3e":"markdown","304b47a0":"markdown","2730b817":"markdown","2c155a0a":"markdown","5eb0b4d8":"markdown","67cd8f1e":"markdown","e5229bdd":"markdown","38709172":"markdown","37f51215":"markdown","b751c2e2":"markdown","dfec0179":"markdown","37cfc227":"markdown","869f822c":"markdown"},"source":{"0991703d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9408f69e":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\n#sns.set(color_codes=True)\n#sns.set_style({\"axes.facecolor\": \"1.0\", 'grid.linestyle': '--', 'grid.color': '.8'})\nsns.set_style(\"whitegrid\")\n#colors = [\"#F28E2B\", \"#4E79A7\",\"#79706E\"]\n\ncolors = {'Data Science': \"#F28E2B\", 'Data\/SW Engineering': \"#4E79A7\", 'Business\/Management': \"#79706E\"}\ncolors_entr = {'Large Enterprise': \"#17BECF\", 'SME': \"#BCBD22\", 'SMB': \"#C7C7C7\", 'NA': \"#FF7F0E\"}\n\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom IPython.display import display, HTML\ninit_notebook_mode(connected=True)\ndisplay(HTML(\"\"\"\n<style>\n.output {\n    display: flex;\n    align-items: left;\n    text-align: center;\n}\n<\/style>\n\"\"\"))\n\ndata_19 = pd.read_csv(\"\/kaggle\/input\/kaggle-survey-2019\/multiple_choice_responses.csv\", skiprows = range(1,2))\nother_responses = pd.read_csv('\/kaggle\/input\/kaggle-survey-2019\/other_text_responses.csv')\n\nconditions = [\n    (data_19['Q5'] == 'Data Scientist') | (data_19['Q5'] == 'Statistician') | (data_19['Q5'] == 'Data Analyst') | (data_19['Q5'] == 'Research Scientist'), \n    (data_19['Q5'] == 'Software Engineer') | (data_19['Q5'] == 'Data Engineer') | (data_19['Q5'] == 'DBA\/Database Engineer'),\n    (data_19['Q5'] == 'Business Analyst') | (data_19['Q5'] == 'Product\/Project Manager'),\n    (data_19['Q5'] == 'Student') | (data_19['Q5'] == 'Not employed')]\nchoices = ['Data Science', 'Data\/SW Engineering', 'Business\/Management','Student']\ndata_19['JobDomain'] = np.select(conditions, choices, default='Others')\n\n\nconditions = [\n    (data_19['Q6'] == '1000-9,999 employees') | (data_19['Q6'] == '> 10,000 employees') , \n    (data_19['Q6'] == '50-249 employees') | (data_19['Q6'] == '250-999 employees') ,\n    (data_19['Q6'] == '0-49 employees') ]\nchoices = ['Large Enterprise','SME', 'SMB', ]\ndata_19['Vertical'] = np.select(conditions, choices, default='NA')\n\nconditions = [\n    (data_19['Q3'] == 'United States of America') , \n    (data_19['Q3'] == 'India') ,\n    (data_19['Q3'] == 'Russia'),\n    (data_19['Q3'] == 'Japan'),\n    (data_19['Q3'] == 'Brazil'),]\nchoices = ['USA','India', 'Russia','Japan','Brazil' ]\ndata_19['CountryGroup'] = np.select(conditions, choices, default='NA')\n\ncompensation_replace_dict = {\n    '$0-999': '< 10,000','1,000-1,999': '< 10,000','2,000-2,999': '< 10,000','3,000-3,999': '< 10,000',\n    '4,000-4,999': '< 10,000','5,000-7,499': '< 10,000','7,500-9,999': '< 10,000','10,000-14,999': '10,000 - 50,000',\n    '15,000-19,999': '10,000 - 50,000','20,000-24,999': '10,000 - 50,000','25,000-29,999': '10,000 - 50,000',\n    '30,000-39,999': '10,000 - 50,000','40,000-49,999': '10,000 - 50,000','50,000-59,999': '50,000 - 99,000',\n    '60,000-69,999': '50,000 - 99,000','70,000-79,999': '50,000 - 99,000','80,000-89,999': '50,000 - 99,000',\n    '90,000-99,999': '50,000 - 99,000','100,000-124,999': '> 100,000','125,000-149,999': '> 100,000',\n    '150,000-199,999': '> 100,000','200,000-249,999': '> 100,000','250,000-299,999': '> 100,000',\n    '300,000-500,000': '> 100,000','> $500,000': '> 100,000'}\n\ndata_19['Q10'] = data_19['Q10'].replace(compensation_replace_dict)\n\ndf = data_19.query(\" JobDomain != 'Student' & JobDomain != 'Others'\")","27394959":"ax = sns.countplot(data=df, x=\"JobDomain\",palette=colors)#sns.color_palette(colors))\n\nax.set_title('Number of Respondents by Title\/Job Category\\n')\nax.set_ylabel('')\nax.set_xlabel('')\n\n\nplt.show()","e703204d":"fig, axs = plt.subplots(figsize=(10, 6),sharey=True)\ncountry = (df.groupby(['JobDomain'])['CountryGroup']\n                     .value_counts(normalize=True)\n                     .rename('Percentage')\n                     .mul(100)\n                     .reset_index()\n                     .sort_values('CountryGroup'))\n\nr = sns.barplot(x=\"CountryGroup\", y=\"Percentage\", hue=\"JobDomain\", data=country[country['CountryGroup']!= 'NA'], palette=colors)\nr.set_title('Job Groups Compared by Countries\\n')\nr.legend(loc='upper center', bbox_to_anchor=(0.5, -0.4),ncol=2)\n_ = plt.setp(r.get_xticklabels(), rotation=90)","abb7bbe3":"edu = (df.groupby(['JobDomain'])['Q4']\n                     .value_counts(normalize=True)\n                     .rename('Percentage')\n                     .mul(100)\n                     .reset_index()\n                     .sort_values('Q4'))\n\ncoding = (df.groupby(['JobDomain'])['Q15']\n                     .value_counts(normalize=True)\n                     .rename('Percentage')\n                     .mul(100)\n                     .reset_index()\n                     .sort_values('Q15'))\n\n\n\nfig, axs = plt.subplots(ncols=2,figsize=(20, 6),sharey=True)\nplt.subplots_adjust(wspace=0.4)\np = sns.barplot(x=\"Q4\", y=\"Percentage\", hue=\"JobDomain\", data=edu, ax=axs[0],palette=colors)\nq = sns.barplot(x=\"Q15\", y=\"Percentage\", hue=\"JobDomain\", data=coding, ax=axs[1],palette=colors)\n\np.set_title('Comparison by Education \\n')\nq.set_title('Years of coding experience for data analysis\\n')\n_ = plt.setp(p.get_xticklabels(), rotation=90)\n_ = plt.setp(q.get_xticklabels(), rotation=90)","3fa27e13":"fig, axs = plt.subplots(figsize=(10, 6),sharey=True)\npay = (df.groupby(['JobDomain'])['Q10']\n                     .value_counts(normalize=True)\n                     .rename('Percentage')\n                     .mul(100)\n                     .reset_index()\n                     .sort_values('Q10'))\n\nr = sns.barplot(x=\"Q10\", y=\"Percentage\", hue=\"JobDomain\", data=pay[:-1], palette=colors)\nr.set_title('Annual Compensation\\n')\nr.legend(loc='upper center', bbox_to_anchor=(0.5, -0.4),ncol=2)\n_ = plt.setp(r.get_xticklabels(), rotation=90)\n","d9049b60":"#will only work with data science community created in previous sections\ndf_ds = data_19.query(\" JobDomain == 'Data Science' \")\n\nax = sns.countplot(data=df_ds, x=\"Vertical\",palette=colors_entr)\n\nax.set_title('Number people from DS Cohort working in different businesses\\n')\nax.set_ylabel('')\nax.set_xlabel('')\n\n\nplt.show()","35caa29b":"df_ds_Excl = df_ds.query(\" Vertical != 'NA' \")\n\nfig, axs = plt.subplots(figsize=(10, 6),sharey=True)\ncountry1 = (df_ds_Excl.groupby(['Vertical'])['CountryGroup']\n                     .value_counts(normalize=True)\n                     .rename('Percentage')\n                     .mul(100)\n                     .reset_index()\n                     .sort_values('CountryGroup'))\n\nr = sns.barplot(x=\"CountryGroup\", y=\"Percentage\", hue=\"Vertical\", data=country1[country1['CountryGroup']!= 'NA'], palette=colors_entr)\nr.set_title('Job Cohorts Compared by Top 5 Countries\\n')\nr.legend(loc='upper center', bbox_to_anchor=(0.5, -0.4),ncol=4)\n_ = plt.setp(r.get_xticklabels(), rotation=90)","308ac3d4":"educ = (df_ds_Excl.groupby(['Vertical'])['Q4']\n                     .value_counts(normalize=True)\n                     .rename('Percentage')\n                     .mul(100)\n                     .reset_index()\n                     .sort_values('Q4'))\n\nsal = (df_ds_Excl.groupby(['Vertical'])['Q10']\n                     .value_counts(normalize=True)\n                     .rename('Percentage')\n                     .mul(100)\n                     .reset_index()\n                     .sort_values('Q10'))\n\n\n\nfig, axs = plt.subplots(ncols=2,figsize=(18, 6),sharey=True)\nplt.subplots_adjust(wspace=0.2)\np = sns.barplot(x=\"Q4\", y=\"Percentage\", hue=\"Vertical\", data=educ, ax=axs[0],palette=colors_entr)\nq = sns.barplot(x=\"Q10\", y=\"Percentage\", hue=\"Vertical\", data=sal, ax=axs[1],palette=colors_entr)\n\np.set_title('Education difference in DS community by Org type\\n')\nq.set_title('Salary difference in DS community by Org type\\n')\n_ = plt.setp(p.get_xticklabels(), rotation=90)\n_ = plt.setp(q.get_xticklabels(), rotation=90)","8b83c714":"df_ds_Excl = df_ds.query(\" Vertical != 'NA' \")\n\n\nML = (df_ds_Excl.groupby(['Vertical'])['Q8']\n                     .value_counts(normalize=True)\n                     .rename('Percentage')\n                     .mul(100)\n                     .reset_index()\n                     .sort_values('Q8'))\n\ntool = (df_ds_Excl.groupby(['Vertical'])['Q14']\n                     .value_counts(normalize=True)\n                     .rename('Percentage')\n                     .mul(100)\n                     .reset_index()\n                     .sort_values('Q14'))\n\n\n\nfig, axs = plt.subplots(ncols=2,figsize=(18, 6),sharey=True)\nplt.subplots_adjust(wspace=0.2)\np = sns.barplot(x=\"Q8\", y=\"Percentage\", hue=\"Vertical\", data=ML, ax=axs[0],palette=colors_entr)\nq = sns.barplot(x=\"Q14\", y=\"Percentage\", hue=\"Vertical\", data=tool, ax=axs[1],palette=colors_entr)\n\np.set_title('ML methods adoptation by Org type\\n')\nq.set_title('Tools used to analyze data by Org type\\n')\n_ = plt.setp(p.get_xticklabels(), rotation=90)\n_ = plt.setp(q.get_xticklabels(), rotation=90)","4373886d":"mysql_ds = 100 * df_ds_Excl.groupby(['Q34_Part_1']).size()\/len(df_ds_Excl)\nsql_ds = 100 * df_ds_Excl.groupby(['Q34_Part_4']).size()\/len(df_ds_Excl)\norc_ds = 100 * df_ds_Excl.groupby(['Q34_Part_5']).size()\/len(df_ds_Excl)\npost_ds = 100 * df_ds_Excl.groupby(['Q34_Part_2']).size()\/len(df_ds_Excl)\nlite_ds = 100 * df_ds_Excl.groupby(['Q34_Part_3']).size()\/len(df_ds_Excl)\ndb_perc_all = pd.concat([mysql_ds, sql_ds,orc_ds,post_ds,lite_ds], axis=0)\n\ndb_perc_all_srt = db_perc_all.sort_values(ascending=False)\n\nq = sns.barplot(db_perc_all_srt.index, db_perc_all_srt.values)\nq.set_title('Top DB Engines usage by DS Community\\n')\nq.set(ylabel='Percentage')\n_ = plt.setp(q.get_xticklabels(), rotation=90)","04474a80":"ora_ds = 100 * df_ds_Excl.groupby(['Q34_Part_5']).size()\/len(df_ds_Excl)\nsql_ds = 100 * df_ds_Excl.groupby(['Q34_Part_4']).size()\/len(df_ds_Excl)\ndb_perc = pd.concat([ora_ds, sql_ds], axis=0)\n\ndf_ds_large = df_ds_Excl.query(\" Vertical == 'Large Enterprise' \")\n\nora_ds_large = 100 * df_ds_large.groupby(['Q34_Part_5']).size()\/len(df_ds_large)\nsql_ds_large = 100 * df_ds_large.groupby(['Q34_Part_4']).size()\/len(df_ds_large)\ndb_perc_large = pd.concat([ora_ds_large, sql_ds_large], axis=0)\n\n\nfig, axs = plt.subplots(ncols=2,figsize=(18, 6),sharey=True)\nplt.subplots_adjust(wspace=0.2)\np = sns.barplot(db_perc.index, db_perc.values, ax=axs[0])\nq = sns.barplot(db_perc_large.index, db_perc_large.values, ax=axs[1])\n\np.set_title('SQL Server\/Oracle DB usage by DS Community\\n')\nq.set_title('SQL Server\/Oracle DB usage by DS Community in Large Organisations\\n')\n\np.set(ylabel='Percentage')\n\n_ = plt.setp(p.get_xticklabels(), rotation=0)\n_ = plt.setp(q.get_xticklabels(), rotation=0)","74f03e58":"post_ds = 100 * df_ds_Excl.groupby(['Q34_Part_2']).size()\/len(df_ds_Excl)\n\n\ndf_ds_med = df_ds_Excl.query(\" Vertical == 'SME' \")\npost_ds_med = 100 * df_ds_med.groupby(['Q34_Part_2']).size()\/len(df_ds_med)\n\nfig, axs = plt.subplots(ncols=2,figsize=(18, 6),sharey=True)\nplt.subplots_adjust(wspace=0.2)\n\np = sns.barplot(post_ds.index, post_ds.values, ax=axs[0])\nq = sns.barplot(post_ds_med.index, post_ds_med.values, ax=axs[1])\n\n\n\np.set_title('Enterprise DB usage by DS Community\\n')\nq.set_title('Enterprise DB usage by DS Community in SME Organisations\\n')\n\np.set(ylabel='Percentage')\np.set(xlabel='')\nq.set(xlabel='')\n\n_ = plt.setp(p.get_xticklabels(), rotation=0)\n_ = plt.setp(q.get_xticklabels(), rotation=0)","0209ceef":"mysql_ds = 100 * df_ds_Excl.groupby(['Q34_Part_1']).size()\/len(df_ds_Excl)\nlite_ds = 100 * df_ds_Excl.groupby(['Q34_Part_3']).size()\/len(df_ds_Excl)\ndb_perc = pd.concat([mysql_ds, lite_ds], axis=0)\n\ndf_ds_small = df_ds_Excl.query(\" Vertical == 'SMB' \")\n\nmysql_small = 100 * df_ds_small.groupby(['Q34_Part_1']).size()\/len(df_ds_small)\nlite_ds_small = 100 * df_ds_small.groupby(['Q34_Part_3']).size()\/len(df_ds_small)\ndb_perc_small = pd.concat([mysql_small, lite_ds_small], axis=0)\n\n\nfig, axs = plt.subplots(ncols=2,figsize=(18, 6),sharey=True)\n#plt.subplots_adjust(wspace=0.2)\np = sns.barplot(db_perc.index, db_perc.values, ax=axs[0])\nq = sns.barplot(db_perc_small.index, db_perc_small.values, ax=axs[1])\n\np.set_title('Enterprise DB usage by DS Community\\n')\nq.set_title('Enterprise DB usage by DS Community in SMB (Small Size) Organisations\\n')\n\np.set(ylabel='Percentage')\n\n_ = plt.setp(p.get_xticklabels(), rotation=0)\n_ = plt.setp(q.get_xticklabels(), rotation=0)","d8ff50b9":"gcp_ds = 100 * df_ds_Excl.groupby(['Q29_Part_1']).size()\/len(df_ds_Excl)\naws_ds = 100 * df_ds_Excl.groupby(['Q29_Part_2']).size()\/len(df_ds_Excl)\nazure_ds = 100 * df_ds_Excl.groupby(['Q29_Part_3']).size()\/len(df_ds_Excl)\ndb_perc_all = pd.concat([gcp_ds, aws_ds,azure_ds], axis=0)\n\ndb_perc_all_srt = db_perc_all.sort_values(ascending=False)\n\nq = sns.barplot(db_perc_all_srt.index, db_perc_all_srt.values)\nq.set_title('Top 3 Cloud Computing Platforms usage by DS Community\\n')\nq.set(ylabel='Percentage')\n_ = plt.setp(q.get_xticklabels(), rotation=90)","74b06dfa":"gcp_ds_lg = 100 * df_ds_large.groupby(['Q29_Part_1']).size()\/len(df_ds_large)\naws_ds_lg = 100 * df_ds_large.groupby(['Q29_Part_2']).size()\/len(df_ds_large)\nazure_ds_lg = 100 * df_ds_large.groupby(['Q29_Part_3']).size()\/len(df_ds_large)\ndb_perc_lg = pd.concat([gcp_ds_lg, aws_ds_lg,azure_ds_lg], axis=0)\n\ndb_perc_lg = db_perc_lg.sort_values(ascending=False)\n\ngcp_ds_med = 100 * df_ds_med.groupby(['Q29_Part_1']).size()\/len(df_ds_med)\naws_ds_med = 100 * df_ds_med.groupby(['Q29_Part_2']).size()\/len(df_ds_med)\nazure_ds_med = 100 * df_ds_med.groupby(['Q29_Part_3']).size()\/len(df_ds_med)\ndb_perc_med = pd.concat([gcp_ds_med, aws_ds_med,azure_ds_med], axis=0)\n\ndb_perc_med = db_perc_med.sort_values(ascending=False)\n\ngcp_ds_sm = 100 * df_ds_small.groupby(['Q29_Part_1']).size()\/len(df_ds_small)\naws_ds_sm = 100 * df_ds_small.groupby(['Q29_Part_2']).size()\/len(df_ds_small)\nazure_ds_sm = 100 * df_ds_small.groupby(['Q29_Part_3']).size()\/len(df_ds_small)\ndb_perc_sm = pd.concat([gcp_ds_sm, aws_ds_sm,azure_ds_sm], axis=0)\n\ndb_perc_sm = db_perc_sm.sort_values(ascending=False)\n\nfig, axs = plt.subplots(ncols=3,figsize=(18, 6),sharey=True)\n#plt.subplots_adjust(wspace=0.2)\nl = sns.barplot(db_perc_lg.index, db_perc_lg.values, ax=axs[0])\nm = sns.barplot(db_perc_med.index, db_perc_med.values, ax=axs[1])\ns = sns.barplot(db_perc_sm.index, db_perc_sm.values, ax=axs[2])\n\n\nl.set_title('CCP usage by DS Community in Large Enterprise\\n')\nm.set_title('CCP usage by DS Community in SME\\n')\ns.set_title('CCP usage by DS Community in SMB\\n')\n\nl.set(ylabel='Percentage')\n\n_ = plt.setp(l.get_xticklabels(), rotation=90)\n_ = plt.setp(m.get_xticklabels(), rotation=90)\n_ = plt.setp(s.get_xticklabels(), rotation=90)","5d90e1be":"ec2_ds = 100 * df_ds_Excl.groupby(['Q30_Part_1']).size()\/len(df_ds_Excl)\ngce_ds = 100 * df_ds_Excl.groupby(['Q30_Part_2']).size()\/len(df_ds_Excl)\n#lamb_ds = 100 * df_ds_Excl.groupby(['Q30_Part_3']).size()\/len(df_ds_Excl)\nazure_vm_ds = 100 * df_ds_Excl.groupby(['Q30_Part_4']).size()\/len(df_ds_Excl)\n#g_ae_ds = 100 * df_ds_Excl.groupby(['Q30_Part_5']).size()\/len(df_ds_Excl)\n#g_cf_ds = 100 * df_ds_Excl.groupby(['Q30_Part_6']).size()\/len(df_ds_Excl)\n#aws_eb_ds = 100 * df_ds_Excl.groupby(['Q30_Part_7']).size()\/len(df_ds_Excl)\n#gk_ds = 100 * df_ds_Excl.groupby(['Q30_Part_8']).size()\/len(df_ds_Excl)\n#aws_b_ds = 100 * df_ds_Excl.groupby(['Q30_Part_9']).size()\/len(df_ds_Excl)\n#azure_c_ds = 100 * df_ds_Excl.groupby(['Q30_Part_10']).size()\/len(df_ds_Excl)\n\ncc_prod_all = pd.concat([ec2_ds, gce_ds,azure_vm_ds], axis=0)\n\ncc_prod_all_srt = cc_prod_all.sort_values(ascending=False)\n\nq = sns.barplot(cc_prod_all_srt.index, cc_prod_all_srt.values)\nq.set_title('Top 3 Cloud Computing Products usage by DS Community\\n')\nq.set(ylabel='Percentage')\n_ = plt.setp(q.get_xticklabels(), rotation=90)","c4766452":"ec2_ds_lg = 100 * df_ds_large.groupby(['Q30_Part_1']).size()\/len(df_ds_large)\ngce_ds_lg = 100 * df_ds_large.groupby(['Q30_Part_2']).size()\/len(df_ds_large)\nazure_vm_ds_lg = 100 * df_ds_large.groupby(['Q30_Part_4']).size()\/len(df_ds_large)\ndb_perc_lg = pd.concat([ec2_ds_lg, gce_ds_lg,azure_vm_ds_lg], axis=0)\n\ndb_perc_lg = db_perc_lg.sort_values(ascending=False)\n\nec2_ds_med = 100 * df_ds_med.groupby(['Q30_Part_1']).size()\/len(df_ds_med)\ngce_ds_med = 100 * df_ds_med.groupby(['Q30_Part_2']).size()\/len(df_ds_med)\nazure_vm_ds_med = 100 * df_ds_med.groupby(['Q30_Part_4']).size()\/len(df_ds_med)\ndb_perc_med = pd.concat([ec2_ds_med, gce_ds_med,azure_vm_ds_med], axis=0)\n\ndb_perc_med = db_perc_med.sort_values(ascending=False)\n\nec2_ds_sm = 100 * df_ds_small.groupby(['Q30_Part_1']).size()\/len(df_ds_small)\ngce_ds_sm = 100 * df_ds_small.groupby(['Q30_Part_2']).size()\/len(df_ds_small)\nazure_vm_ds_sm = 100 * df_ds_small.groupby(['Q30_Part_4']).size()\/len(df_ds_small)\ndb_perc_sm = pd.concat([ec2_ds_sm, gce_ds_sm,azure_vm_ds_sm], axis=0)\n\ndb_perc_sm = db_perc_sm.sort_values(ascending=False)\n\nfig, axs = plt.subplots(ncols=3,figsize=(18, 6),sharey=True)\n#plt.subplots_adjust(wspace=0.2)\nl = sns.barplot(db_perc_lg.index, db_perc_lg.values, ax=axs[0])\nm = sns.barplot(db_perc_med.index, db_perc_med.values, ax=axs[1])\ns = sns.barplot(db_perc_sm.index, db_perc_sm.values, ax=axs[2])\n\n\nl.set_title('Cloud products used by DS Community in Large Enterprise\\n')\nm.set_title('Cloud products used by DS Community in SME\\n')\ns.set_title('Cloud products used by DS Community in SMB\\n')\n\nl.set(ylabel='Percentage')\n\n_ = plt.setp(l.get_xticklabels(), rotation=90)\n_ = plt.setp(m.get_xticklabels(), rotation=90)\n_ = plt.setp(s.get_xticklabels(), rotation=90)","03c567be":"q1_ds = 100 * df_ds_Excl.groupby(['Q24_Part_1']).size()\/len(df_ds_Excl)\nq2_ds = 100 * df_ds_Excl.groupby(['Q24_Part_2']).size()\/len(df_ds_Excl)\nq3_ds = 100 * df_ds_Excl.groupby(['Q24_Part_3']).size()\/len(df_ds_Excl)\nq4_ds = 100 * df_ds_Excl.groupby(['Q24_Part_4']).size()\/len(df_ds_Excl)\nq5_ds = 100 * df_ds_Excl.groupby(['Q24_Part_5']).size()\/len(df_ds_Excl)\nq6_ds = 100 * df_ds_Excl.groupby(['Q24_Part_6']).size()\/len(df_ds_Excl)\nq7_ds = 100 * df_ds_Excl.groupby(['Q24_Part_7']).size()\/len(df_ds_Excl)\nq8_ds = 100 * df_ds_Excl.groupby(['Q24_Part_8']).size()\/len(df_ds_Excl)\nq9_ds = 100 * df_ds_Excl.groupby(['Q24_Part_9']).size()\/len(df_ds_Excl)\nq10_ds = 100 * df_ds_Excl.groupby(['Q24_Part_10']).size()\/len(df_ds_Excl)\n\nalgo_prod_all = pd.concat([q1_ds, q2_ds,q3_ds,q4_ds,q5_ds,q6_ds,q7_ds,q8_ds,q9_ds,q10_ds], axis=0)\n\nalgo_prod_all_srt = algo_prod_all.sort_values(ascending=False)\n\nq = sns.barplot(algo_prod_all_srt.index, algo_prod_all_srt.values)\nq.set_title('Usage of ML Algos by DS Community\\n')\nq.set(ylabel='Percentage')\n_ = plt.setp(q.get_xticklabels(), rotation=90)","213bb9e1":"q1_ds = 100 * df_ds_large.groupby(['Q24_Part_1']).size()\/len(df_ds_large)\nq2_ds = 100 * df_ds_large.groupby(['Q24_Part_2']).size()\/len(df_ds_large)\nq3_ds = 100 * df_ds_large.groupby(['Q24_Part_3']).size()\/len(df_ds_large)\nq4_ds = 100 * df_ds_large.groupby(['Q24_Part_4']).size()\/len(df_ds_large)\nq5_ds = 100 * df_ds_large.groupby(['Q24_Part_5']).size()\/len(df_ds_large)\nq6_ds = 100 * df_ds_large.groupby(['Q24_Part_6']).size()\/len(df_ds_large)\nq7_ds = 100 * df_ds_large.groupby(['Q24_Part_7']).size()\/len(df_ds_large)\nq8_ds = 100 * df_ds_large.groupby(['Q24_Part_8']).size()\/len(df_ds_large)\nq9_ds = 100 * df_ds_large.groupby(['Q24_Part_9']).size()\/len(df_ds_large)\nq10_ds = 100 * df_ds_large.groupby(['Q24_Part_10']).size()\/len(df_ds_large)\n\nalgo_prod_lg = pd.concat([q1_ds, q2_ds,q3_ds,q4_ds,q5_ds,q6_ds,q7_ds,q8_ds,q9_ds,q10_ds], axis=0)\n\n\nq1_ds = 100 * df_ds_med.groupby(['Q24_Part_1']).size()\/len(df_ds_med)\nq2_ds = 100 * df_ds_med.groupby(['Q24_Part_2']).size()\/len(df_ds_med)\nq3_ds = 100 * df_ds_med.groupby(['Q24_Part_3']).size()\/len(df_ds_med)\nq4_ds = 100 * df_ds_med.groupby(['Q24_Part_4']).size()\/len(df_ds_med)\nq5_ds = 100 * df_ds_med.groupby(['Q24_Part_5']).size()\/len(df_ds_med)\nq6_ds = 100 * df_ds_med.groupby(['Q24_Part_6']).size()\/len(df_ds_med)\nq7_ds = 100 * df_ds_med.groupby(['Q24_Part_7']).size()\/len(df_ds_med)\nq8_ds = 100 * df_ds_med.groupby(['Q24_Part_8']).size()\/len(df_ds_med)\nq9_ds = 100 * df_ds_med.groupby(['Q24_Part_9']).size()\/len(df_ds_med)\nq10_ds = 100 * df_ds_med.groupby(['Q24_Part_10']).size()\/len(df_ds_med)\n\nalgo_prod_med = pd.concat([q1_ds, q2_ds,q3_ds,q4_ds,q5_ds,q6_ds,q7_ds,q8_ds,q9_ds,q10_ds], axis=0)\n\n\nq1_ds = 100 * df_ds_small.groupby(['Q24_Part_1']).size()\/len(df_ds_small)\nq2_ds = 100 * df_ds_small.groupby(['Q24_Part_2']).size()\/len(df_ds_small)\nq3_ds = 100 * df_ds_small.groupby(['Q24_Part_3']).size()\/len(df_ds_small)\nq4_ds = 100 * df_ds_small.groupby(['Q24_Part_4']).size()\/len(df_ds_small)\nq5_ds = 100 * df_ds_small.groupby(['Q24_Part_5']).size()\/len(df_ds_small)\nq6_ds = 100 * df_ds_small.groupby(['Q24_Part_6']).size()\/len(df_ds_small)\nq7_ds = 100 * df_ds_small.groupby(['Q24_Part_7']).size()\/len(df_ds_small)\nq8_ds = 100 * df_ds_small.groupby(['Q24_Part_8']).size()\/len(df_ds_small)\nq9_ds = 100 * df_ds_small.groupby(['Q24_Part_9']).size()\/len(df_ds_small)\nq10_ds = 100 * df_ds_small.groupby(['Q24_Part_10']).size()\/len(df_ds_small)\n\nalgo_prod_sm = pd.concat([q1_ds, q2_ds,q3_ds,q4_ds,q5_ds,q6_ds,q7_ds,q8_ds,q9_ds,q10_ds], axis=0)\n\n\nal_usg = pd.concat([algo_prod_all,algo_prod_lg,algo_prod_med,algo_prod_sm], axis=1)\nal_usg.columns = ['All', 'Large', 'SME','SMB']\n\nfrom matplotlib.colors import ListedColormap\nfig, axs = plt.subplots(ncols=2,figsize=(14, 8),sharey=True)\n\nwith sns.axes_style('white'):\n      p = sns.heatmap(al_usg,\n                cbar=False,\n                square=False,\n                annot=True,\n                fmt='g',\n                cmap=ListedColormap(['white']),\n                linewidths=0.2,ax=axs[0])\n\ntab_n = al_usg.div(al_usg.max(axis=1), axis=0)\nq = sns.heatmap(tab_n,annot=False,cmap=\"YlGnBu\", cbar=False, linewidths=0.5,ax=axs[1])\nbottom, top = q.get_ylim()\nq.set_ylim(bottom + 0.5, top - 0.5)\n\np.set(ylabel='Percentage')\np.set(title='% of Usage of Algos by Enterprise Type')\n\n_ = plt.setp(p.get_xticklabels(), rotation=90)\n_ = plt.setp(q.get_xticklabels(), rotation=90)","a0e7f612":"q1_ds = 100 * df_ds_Excl.groupby(['Q28_Part_1']).size()\/len(df_ds_Excl)\nq2_ds = 100 * df_ds_Excl.groupby(['Q28_Part_2']).size()\/len(df_ds_Excl)\nq3_ds = 100 * df_ds_Excl.groupby(['Q28_Part_3']).size()\/len(df_ds_Excl)\nq4_ds = 100 * df_ds_Excl.groupby(['Q28_Part_4']).size()\/len(df_ds_Excl)\nq5_ds = 100 * df_ds_Excl.groupby(['Q28_Part_5']).size()\/len(df_ds_Excl)\nq6_ds = 100 * df_ds_Excl.groupby(['Q28_Part_6']).size()\/len(df_ds_Excl)\nq7_ds = 100 * df_ds_Excl.groupby(['Q28_Part_7']).size()\/len(df_ds_Excl)\nq8_ds = 100 * df_ds_Excl.groupby(['Q28_Part_8']).size()\/len(df_ds_Excl)\nq9_ds = 100 * df_ds_Excl.groupby(['Q28_Part_9']).size()\/len(df_ds_Excl)\nq10_ds = 100 * df_ds_Excl.groupby(['Q28_Part_10']).size()\/len(df_ds_Excl)\n\nml_fw_all = pd.concat([q1_ds, q2_ds,q3_ds,q4_ds,q5_ds,q6_ds,q7_ds,q8_ds,q9_ds,q10_ds], axis=0)\n\nml_fw_all_srt = ml_fw_all.sort_values(ascending=False)\n\nq = sns.barplot(ml_fw_all_srt.index, ml_fw_all_srt.values)\nq.set_title('Usage of ML Frameworks by DS Community\\n')\nq.set(ylabel='Percentage')\n_ = plt.setp(q.get_xticklabels(), rotation=90)","59bcc3a1":"q1_ds = 100 * df_ds_large.groupby(['Q28_Part_1']).size()\/len(df_ds_large)\nq2_ds = 100 * df_ds_large.groupby(['Q28_Part_2']).size()\/len(df_ds_large)\nq3_ds = 100 * df_ds_large.groupby(['Q28_Part_3']).size()\/len(df_ds_large)\nq4_ds = 100 * df_ds_large.groupby(['Q28_Part_4']).size()\/len(df_ds_large)\nq5_ds = 100 * df_ds_large.groupby(['Q28_Part_5']).size()\/len(df_ds_large)\nq6_ds = 100 * df_ds_large.groupby(['Q28_Part_6']).size()\/len(df_ds_large)\nq7_ds = 100 * df_ds_large.groupby(['Q28_Part_7']).size()\/len(df_ds_large)\nq8_ds = 100 * df_ds_large.groupby(['Q28_Part_8']).size()\/len(df_ds_large)\nq9_ds = 100 * df_ds_large.groupby(['Q28_Part_9']).size()\/len(df_ds_large)\nq10_ds = 100 * df_ds_large.groupby(['Q28_Part_10']).size()\/len(df_ds_large)\n\nml_fw_lg = pd.concat([q1_ds, q2_ds,q3_ds,q4_ds,q5_ds,q6_ds,q7_ds,q8_ds,q9_ds,q10_ds], axis=0)\n\n\nq1_ds = 100 * df_ds_med.groupby(['Q28_Part_1']).size()\/len(df_ds_med)\nq2_ds = 100 * df_ds_med.groupby(['Q28_Part_2']).size()\/len(df_ds_med)\nq3_ds = 100 * df_ds_med.groupby(['Q28_Part_3']).size()\/len(df_ds_med)\nq4_ds = 100 * df_ds_med.groupby(['Q28_Part_4']).size()\/len(df_ds_med)\nq5_ds = 100 * df_ds_med.groupby(['Q28_Part_5']).size()\/len(df_ds_med)\nq6_ds = 100 * df_ds_med.groupby(['Q28_Part_6']).size()\/len(df_ds_med)\nq7_ds = 100 * df_ds_med.groupby(['Q28_Part_7']).size()\/len(df_ds_med)\nq8_ds = 100 * df_ds_med.groupby(['Q28_Part_8']).size()\/len(df_ds_med)\nq9_ds = 100 * df_ds_med.groupby(['Q28_Part_9']).size()\/len(df_ds_med)\nq10_ds = 100 * df_ds_med.groupby(['Q28_Part_10']).size()\/len(df_ds_med)\n\nml_fw_med = pd.concat([q1_ds, q2_ds,q3_ds,q4_ds,q5_ds,q6_ds,q7_ds,q8_ds,q9_ds,q10_ds], axis=0)\n\n\nq1_ds = 100 * df_ds_small.groupby(['Q28_Part_1']).size()\/len(df_ds_small)\nq2_ds = 100 * df_ds_small.groupby(['Q28_Part_2']).size()\/len(df_ds_small)\nq3_ds = 100 * df_ds_small.groupby(['Q28_Part_3']).size()\/len(df_ds_small)\nq4_ds = 100 * df_ds_small.groupby(['Q28_Part_4']).size()\/len(df_ds_small)\nq5_ds = 100 * df_ds_small.groupby(['Q28_Part_5']).size()\/len(df_ds_small)\nq6_ds = 100 * df_ds_small.groupby(['Q28_Part_6']).size()\/len(df_ds_small)\nq7_ds = 100 * df_ds_small.groupby(['Q28_Part_7']).size()\/len(df_ds_small)\nq8_ds = 100 * df_ds_small.groupby(['Q28_Part_8']).size()\/len(df_ds_small)\nq9_ds = 100 * df_ds_small.groupby(['Q28_Part_9']).size()\/len(df_ds_small)\nq10_ds = 100 * df_ds_small.groupby(['Q28_Part_10']).size()\/len(df_ds_small)\n\nml_fw_sm = pd.concat([q1_ds, q2_ds,q3_ds,q4_ds,q5_ds,q6_ds,q7_ds,q8_ds,q9_ds,q10_ds], axis=0)\n\n\nal_usg = pd.concat([ml_fw_all,ml_fw_lg,ml_fw_med,ml_fw_sm], axis=1)\nal_usg.columns = ['All', 'Large', 'SME','SMB']\n\nfrom matplotlib.colors import ListedColormap\nfig, axs = plt.subplots(ncols=2,figsize=(14, 8),sharey=True)\n\nwith sns.axes_style('white'):\n      p = sns.heatmap(al_usg,\n                cbar=False,\n                square=False,\n                annot=True,\n                fmt='g',\n                cmap=ListedColormap(['white']),\n                linewidths=0.2,ax=axs[0])\n\ntab_n = al_usg.div(al_usg.max(axis=1), axis=0)\nq = sns.heatmap(tab_n,annot=False,cmap=\"YlGnBu\", cbar=False, linewidths=0.5,ax=axs[1])\nbottom, top = q.get_ylim()\nq.set_ylim(bottom + 0.5, top - 0.5)\n\np.set(ylabel='Percentage')\np.set(title='% of Usage of ML Framwork by Enterprise Type')\n\n_ = plt.setp(p.get_xticklabels(), rotation=90)\n_ = plt.setp(q.get_xticklabels(), rotation=90)","133b881f":"If we try to see the above distribution by different enterprise category, we will more or less see the same distribution. So we have to analyze by each algorithms type, with it we can identify if one algorithm is more in use at a particular organization type or not. ","5669a089":"#### When it comes to Enterprise level Cloud Computing Products\nTop most cloud computing products used by our DS Community in this survey is AWS EC2","fab47832":"For SME (Mid sized Org), their distribution is similar to total DS community. However, there is one DB Engine where SME Enterprises over-index, that is PostgreSQL Engine.","ec3a78be":"#### SMB (Small and Medium-Sized Businesses)\n<ul>\n<li>Again we are seeing usage of Google product more in this group<\/li>\n<li>SMB under-index when it come to EC2 usage by 1.5%<\/li>\n<\/ul>\n\n#### SME (Small and Medium Enterprises)\n<ul>\n<li>Over-index in GCE usage<\/li>\n<li>Slightly under-index in EC2 usage<\/li>\n<\/ul>\n\n#### Large Enterprise\n<ul>\n<li>Over-index when it comes to EC2 compare to average distribution among DS community<\/li>\n<li>Slightly over-indexed when it comes to Azure VM usage<\/li>\n<\/ul>\n\n\nWhen analysing the number about cloud computing questions, we can conclude\n\n<ul>\n<li>All organizations are more inclined towards AWS because its older of all and is more mature.<\/li>\n<li>Large organizations prefer Azure more compare to other group (SMB & SME) because of its expensive and only large organizations are more likely to afford it. Secondly, its integration with other Microsoft products can be a plus like MS SQL Server, Power BI etc.<\/li>\n<li>Google Platform\/Products is popular in small business because if free for first 12 months<\/li>\n<\/ul>","c4b30ab7":"#### Lets move towards ML\/DL Frameworks","f92acdd8":"From data science community data available in survery (categorized in section 1) \n<ul>\n<li>39.6% working in Large Enterprises<\/li>\n<li>29.7% working in SME<\/li>\n<li>29.2% working in SMB<\/li>\n<\/ul>","0b5cadaa":"#### Job Categories by Education, Coding Experience & Difference in Compensation","f22f837d":"From a novice to an experienced data scientist, there\u2019s a vast range of potential employers out there, from small start-ups to the biggest multinationals. Which type best suits you is for you to decide, based on your personality, your likes and dislikes and, of course, what the company has to offer, for example\n\n1. **Work for a company with great data** -- In determining what will be a great company to work for, data-science-as-a-strategic-differentiator is a necessary criterion, but it is not sufficient. The company must also have world-class data to work with.\n\n1. **Work for a Company with Greenfield Opportunities** -- When evaluating opportunities, find a company that doesn\u2019t have it all figured out yet.\n\n\nWith this notebook, using the data available in Kaggle survey I will try to explain what are the traits of different organizations when it comes to data science community and to answer questions like \n\n    - What kind of data science tools and practices are used in different type of organizations?\n    - How mature are these organizations when it comes to machine learning and what kind of frameworks are being used?\n    - What should you expect when looking for a job opportunity? Will the role allow you to hone skills you already have or add new ones?\n ","3b382978":"#### Lets start with ML Algorithms\n\nIn the sections we will analyze how Data Science Community's exposure to ML Algos is different if they are part of different organization","1631436b":"#### Enterprises by Education & Difference in Compensation","920f54ed":"***\n# 2. Enterprise Classification and Data Science Community\n<a id=\"enterprise\"><\/a>\n***\n\nIn this section we will only work with our data science community cohort and will analyze how being part of a different organization can impact their nature of work\n\n1. Classification of enterprise based on their employee size?\n2. Examine if geography has any role to play to make them different?","bb0dafdd":"### 3.4. Deep dive into ML & DL Framework\n<a id=\"mldl\"><\/a>\n\nLets deep dive into machine learning and deep learning framework usage among these organizations focused on Data Science community","7f873db3":"##### Business & Management Cohort\n<ul>\n<li>Majority holds a masters degree, most probably MBA or Project Management degree<\/li>\n<li>Majority has less number of years in coding to analyze data <\/li>\n<li>This group has higher percentage in 50-99k and 100k+, this is because of product\/project managers in this group and around 45% of them have yearly compensation > 50k <\/li>\n<li>According to payscale.com median salary of product\/project managers and data scientist is almost same<\/li>\n<\/ul>\n\n##### Data Science Cohort\n<ul>\n<li>Highest in terms of doctoral degree holders<\/li>\n<li>Have high number of years in coding experience<\/li>\n<li>This group has equal contribution compare to others in each annual compensation band, this is because of number of years of experience. The more experience you have the higher you get paid <\/li>\n<\/ul>\n\n##### Data\/SW Engineering Cohort\n<ul>\n<li>Similar to business group, most of the people are master degree holder followed by bachelors<\/li>\n<li>As Q15 is about the coding experience to analyze data that's why this group is second to DS when it comes to > 3 years <\/li>\n<li>Majority lies under 50k yearly compensation, around 60%. This came as a surprise because payscale.com and glassdoor.com show that their salary range is 70k+<\/li>\n<\/ul>\n\nBy looking at the results form this survey and based on experience gained from industry, it is important that people belonging from data science group should understand about engineering or project management area and vice versa. At the end of the day, we all have to work together and as a team\n\n<i>If everyone is moving forward together, then success takes care of itself.\" --Henry Ford<\/i>","af1b00f2":"***\n# Introduction\n#### What I am targeting to achieve?\n<a id=\"introduction\"><\/a>\n***\n\nEvaluating the companies that you consider to work for based on their brand image, annual compensation, long term pay off and location etc. is important but while keeping these thing in consideration we usually tend to forget or we don't know whether the company I am considering to join is good for my professional development and future growth or not. \n\n<i>\"Data scientist\" is often used as a blanket title to describe jobs that are drastically different<\/i>\n\nUnderstanding a company's culture and its approach to data is very important for your career growth in data science. I have heard about companies where data scientists are employed to confirm the opinions of interested parties. This may be true in some professions, however a data scientist is expected to form theories, test hunches, and find patterns eventually creating actionable insights.\n\nWe will be using subset of Kaggle survey 2019 data, people who have titles belonging to data science community only. To understand their nature of work, exposure to ML\/AI methods, and what technology is being used, based on different type of organization that they are working for\n\nPlan of action: \n\n1.\tFirst we will separate our cohort under analysis from Kaggle survey and will also examine how they differ from other cohorts\n2.\tEnterprise classification: Based on survey data classification of businesses\n3.\tExamine the characteristics of these enterprises from the point of view of data science community ","de585e72":"### 2.1. How to classify and why?\n<a id=\"classify\"><\/a>\n\nDepending on whom you ask, there are several definitions and key differentiators that influence the classification into which a business can be classified. The widely accepted definition of each business size classification is based on the number of employees and annual revenue. Since we only have no. of employee identifier availble in our survey data, so we will be using it to create our enterprise classifier. \n\n> Q6: What is the size of the company where you are employed?\n\nThere are three main types of businesses that you can work for in the private sector, each with it's own pros and cons:\n\n1. SMB (Small and Medium-Sized Businesses)\n**0-49 employees**\n2. SME (Small and Medium Enterprises)\n**50-999 employees**\n3. Large Enterprise\n**more than 1000 employees**","2cbe6bf0":"### 3.1. Educational & Compensation difference\n<a id=\"experience\"><\/a>\n\nIn this section lets look at some basic differences by enterprise types each as \n\n1. Education level of the data science employees in the company \n2. Difference in income level ","61a76524":"Large enterprises still rely on traditional database engines (such as Oracle, DB2 \u2013 and even SQL Server) to run their mission critical systems. They over-index when it some comes enterprise level DB storage platform\n<ul>\n<li>For Oracle: they over-index by 4%<\/li>\n<li>For SQL Server: they over-index by 3.3%<\/li>\n<\/ul>\n\nreason being\n\n<ul>\n<li>The up-front cost doesn\u2019t bother these large companies, even a million dollars isn\u2019t very much compared to the costs of running a $10bn company.    <\/li>\n<li>Large organizations have to deal with large volume of structure data, so relying on a platform which is enterprise ready and is being used in industry for a long time is a reasonable choice.<\/li>\n<li>The support is a big deal, large companies want to pay for those support contracts, they want 24\/7 support, SLAs, and reassurance that when things go wrong. Support is available to sort things out.<\/li>\n<\/ul>","10817ba8":"#### SMB (Small and Medium-Sized Businesses)\n<ul>\n<li>High on local environment for data analysis and lower on advanced\/enterprise ready tool. This can be because of the reason that since they are exploring the possibilities of ML or either have limited use, so they use open source\/less expensive tools<\/li>\n<li>36% of this group have ML in production, which is lower of all 3 categories of companies<\/li>\n<\/ul>\n\n#### SME (Small and Medium Enterprises)\n<ul>\n<li>These group of companies\u2019 over-index on cloud-based data software compare to other 2 groups (8.4%), which is 1% higher compare to other 2 groups<\/li>\n<li>40% of the companies from this group are using ML in production<\/li>\n<\/ul>\n\n#### Large Enterprise\n<ul>\n<li>Companies in this group over-index on Advanced Statistics Software (by 1.8%) and BI Software (by 1%)<\/li>\n<li>55% of companies in this group are using ML in production<\/li>\n<\/ul>","0730fb32":"<ul>\n<li>Majority of Data Science community who is working in large organizations are in USA at 24%, and India being the second highest at 16.6%<\/li>\n<li>SME are equally distributed between Indian and USA at around 15%<\/li>\n<li>Contrary to large enterprises, Data Science community from SMB are more in India at 19.6% compare to any other country<\/li>\n<\/ul>\n\n\n***Let's step out of the survey and look at overall footprint of digital\/tech companies on the globe***\n\nUSA is the obvious leader when it comes to technology innovation, based on below [figure](https:\/\/www.brookings.edu\/research\/trends-in-the-information-technology-sector\/). Our survey data is skewed towards India responders, due to which we don't see much enterprises from Europe and China.\n<\/ul>\n\nWhen looking at top 100 digital [companies](https:\/\/www.forbes.com\/top-digital-companies\/list\/2\/#tab:rank)\n\n<ul>\n<li>Out of 100 Digital companies, 38 are from USA<\/li>\n<li>10 are from China and only 2 are from India<\/li>\n<li>17 are from Europe<\/li>\n<\/ul>\n\n\n\nFigure: Global distribution of top 100 digital companies and market capitalization (US $billion)\n<img src=\"kaggle-survey-2019\/pics\/digital companies.jpg\" height=\"600\" width=\"600\">","08ce9404":"#### When it comes to Enterprise level Cloud Computing Platforms\n\nTop most cloud computing platform used by our DS Community in this survey is AWS","f4b7cf7b":"#### Job Categories by Country\nOverall survey distribution by country\n<ul>\n<li>Majority of the people belong to India who have taken part in survey, which 24.6% of total<\/li>\n<li>Followed by USA, which is 15.4% <\/li>\n<\/ul>\n\nHowever, when we analyze the distribution by countries for our 3 cohort under study, we see a different picture","84c78ae4":"![](https:\/\/multithreaded.stitchfix.com\/assets\/images\/blog\/three_data_scientists.jpg)\n*Pic Credits:[blog](https:\/\/multithreaded.stitchfix.com\/blog\/2015\/03\/31\/advice-for-data-scientists\/)*","c1771dda":"Of course, there are a lot of considerations: domain, company's brand, the culture, the people, location and the specific technology in use and so forth. All of these are equally important. We analyzed based on the survey results from data science community working out there in different organizations, to help the readers looking for data science opportunities when it comes to choosing what is right for them.\n\nIn this notebook, we have focus on Kaggle survey of 2019 to analyze the difference between difference companies when it comes to Data Science community exposure to data analytics tools, ML methods and frameworks. We have seen clear difference between our 3 groups (Large, SME, & SMB). \n\nThe biggest plus with a large company is usually security and benefits , together with ample opportunity to move your career in the direction you want. Machine learning practices are pretty mature in large organizations, majorly dealing with structured data as RDBMS systems use is over indexing with BI tools. ML frameworks like Scikit-learn, GBM and RF also over index in large organizations.\n\nMid Sized companies (SME) holds highest percentage of PhD holders compare to other 2 groups from Data Science community. You can expect receive better benefits than in a small company, such as health care or a contributory pension depending on your experience and qualification. We have seen Data Science employees working in these organizations are equally exposed to all technologies, ranging from RDBMS systems, to cloud computing platform, to deep learning frameworks when compared with overall usage of data science community.\n\nCompanies with employee size up to 50 are considered as small business (SMB). In our case, we have seen these companies employ ML usage lowest compared to other 2 groups. However, the ones who are using ML are more inclined towards deep learning frameworks.  Generally smaller revenues and profits, pay and benefits are often lower in smaller companies. However, on the other hand, you'll almost certainly have more involvement in a wider range of tasks than in a bigger company, along with the chance of quick promotion if you prove yourself. Working for a small company is also an excellent way of acquiring new, transferable skills.\n\nIf you are at all unsure about what you're getting into, it's a good idea to arrange a trial period before committing yourself. An internship or temporary position offers the perfect opportunity to get a feel for people and the company. Part-time work is also an option, as it gives you the chance to try out two companies at once.","59813143":"### 3.3. Data Storage & Cloud Computing\n<a id=\"storage\"><\/a>\n\nHere we will examine what to expect from these enterprises when it comes to Data Storage and Cloud Computing platforms\/products","e0918328":"***\n# Conclusion\n<a id=\"conclusion\"><\/a>\n*** ","edbe4827":"No Surprise here: Since Kaggle is a data science platform, so most of the people who use this platform must be from data science domain. People like me, who are part of business & management group, but due to interest in data science use this platform to either keep their skillset up to date or they don't have any social life :)\n\nIt is trivial for managers, marketers and business leaders that even if they don\u2019t have hands on experience in DS, but they should be aware of new improvements and developments in this field. This will make their life easy at their workplace or at least they can sound smart in front of their peers.","daef8f0f":"SQLite is mostly used in [small organisations](https:\/\/enlyft.com\/tech\/products\/sqlite)\n\nMySQL is deployment in small to medium sized enterprises and at the departmental level in large enterprises\n\n<ul>\n<li>For MySQL: they over-index by 1.4%<\/li>\n<li>For SQLite: they over-index by 1.8%<\/li>\n<\/ul>\n\nreason being\n\n<ul>\n<li>Both are open source DB engines<\/li>\n<li>Mysql DB is used in small scale and medium scale web softwares for database management<\/li>\n<li>MySQL is the world\u2019s most used client-server RDBMS (SQLite has more installations, because it is bundled and distributed with smaller client-application databases that reside on personal computing devices) and the de-facto standard for Linux-based systems<\/li>\n<\/ul>\n","f18f7fa7":"Differences are evident when compared by Education & Compensation levels of Data Science Community among these enterprises\n\n#### SMB (Small and Medium-Sized Businesses)\n<ul>\n<li>Holds higher percentage (68%) of Masters and Bachelor degree holders<\/li>\n<li>Majority (63%) of data science community working in these companies are paid under 50k a year. It seems that fresh graduate after doing their bachelors\/masters degree who have less experience in coding are being employed by these companies<\/li>\n<\/ul>\n\n#### SME (Small and Medium Enterprises)\n<ul>\n<li>SME holds the highest percentage by doctoral degree holders compare to other enterprises. <\/li>\n<li>Although most people (56%) are paid under 50k a year. But because they are employing PHDs and people with more years of experience compare to SMB, so some are paid more the 50K or even 100k+ a year <\/li>\n<\/ul>\n\n#### Large Enterprise\n<ul>\n<li>50% of data science community working in these companies have master\u2019s degree<\/li>\n<li>Because these organizations employee people with sound background and experience, so annual compensation of this group is higher compare to other 2 groups<\/li>\n<\/ul>","24f53f8f":"#### Enterprises compared by Data Analytics Tools & ML Usage Tenure","1c75bd7b":"### 3.2. Exposure to Data Analytics Tools & ML\n<a id=\"analytics\"><\/a>\n\nIn this section lets look at very high level traits\n\n1. Tools used for data analysis \n2. Usage of ML ","f0fa600c":"Clear difference can be seen above \n\n#### SMB (Small and Medium-Sized Businesses)\n<ul>\n<li>SMB in our dataset are over-indexed on NN usage, highlighting the fact that the data science community working in these companies are dealing with image processing or NLP related problems more compare to other 2 categories<\/li>\n<\/ul>\n\n#### SME (Small and Medium Enterprises)\n<ul>\n<li>Not much different as compare to overall distribution, but slightly high on Bayesian & Evolutionary approaches<\/li>\n<\/ul>\n\n#### Large Enterprise\n<ul>\n<li>Large Enterprise over-index when it come to the usage of algorithms like regression, RF, and GBM. This explains the fact that more of the Large Enterprises are using ML to problems related to structured data and like RF or GBM perform pretty well when it comes to structured data<\/li>\n<li>Also over-index for MLP (possible again for tabular data), and BERT (for NLP)<\/li>\n<\/ul>","113cffb4":"***\n# Table of contents\n***\n\n* [Introduction](#introduction)\n\n\n* [1. Job Categorization](#job)\n    * [1.1. Data Science Community: Our Cohort under study](#title)\n    * [1.2. Similarities & Difference: DS Community Vs. Others](#snd)\n\n\n* [2. Enterprise Classification and Data Science Community](#enterprise)\n   * [2.1. How to classify and why?](#classify)\n   * [2.2. Geographical Distribution of Enterprises](#geo)\n   \n   \n* [3. Data Science Community at work](#dsc)\n    * [3.1. Educational & Compensation difference](#experience)\n    * [3.2 Exposure to Data Analytics Tools & ML](#analytics)\n    * [3.3 Data Storage & Cloud Computing](#storage)\n    * [3.4. Deep dive into ML & DL Framework](#mldl)\n    \n    \n\n* [Conclusion](#conclusion)\n\n* [References](#references)","09c23d3e":"Medium size Enterprise over-index when it comes to PostgreSQL by 1.2%\n\nreason being\n\n<ul>\n<li>According to DB-Engines research (shared above), PostgreSQL has significantly increased its score over the last year with an almost 50 point increase. <\/li>\n<li>It\u2019s a favorite because it\u2019s open-source, free to use, community-driven without being owned by a single company, standards-compliant, filled with useful features, and very extensible.<\/li>\n<li>Many companies have been built around Postgres itself like CitusDB, Timescale, PipelineDB and others. Even AWS Redshift is built on Postgres code.<\/li>\n<li>Recently it has gotten significantly better with features like full-text search, JSON columns, logical replication, upsert, and better scalability.<\/li>\n<\/ul>\n\nAdd all that together and you have a powerful data platform that\u2019s hard to beat, especially for startups and smaller organizations that need a reliable choice without tons of effort or cost","304b47a0":"### 1.1. Data Science Community: Our Cohort under study\n<a id=\"title\"><\/a>\n<i>\"Data scientist\" is often used as a blanket title to describe jobs that are drastically different<\/i>\n\nData science combines several disciplines, including statistics, data analysis, machine learning, and computer science. This can be daunting if you\u2019re new to data science, but keep in mind that different roles and companies will emphasize some skills over others, so you don\u2019t have to be an expert at everything.\n\n\n<img src=\"https:\/\/i2.wp.com\/blog.udacity.com\/wp-content\/uploads\/2014\/11\/Data-Science-Skills-Udacity-Matrix.png?resize=640%2C521&ssl=1\" height=\"500\" width=\"500\">\n\n\nDue to similarities in their role and responsibilities (example shown in skill matrix), we will using titles from Kaggle survey for grouping and below are the cohorts which will be used in this chapter  \n\n> Q5: Select the title most similar to your current role (or most recent title if retired) \n\n\n<img src=\"https:\/\/i.ibb.co\/m6SHLqc\/cohort.png\" height=\"700\" width=\"700\">\n\n\n<ul>\n<li>Data Science cohort --> 39% of total survey data<\/li>\n<li>Data\/SW Engineering cohort --> 17.6% of total survey data<\/li>\n<li>Business and Management cohort --> 7.6% of total survey data<\/li>\n<\/ul>\n","2730b817":"We see a major difference when it comes to geographical distribution\n<ul>\n<li>When it comes to Data Science cohort, top 2 countries are still India & USA. However, both have similar contribution. USA being slightly higher 17.7% Vs. India 17.4%<\/li>\n<li>Majority of the Data\/SW Engineering group people are residing in India, which is 25% compare to USA 13.4% at second place <\/li>\n<li>Business Management group is highest in India (23%), compare to second place USA (16%)<\/li>\n<\/ul>\n\nIts not surprise that India is among the top 5 counties when it comes to scientific research and technological investment. The country has improved its reputation in terms of the risk posed to foreign investments and, in 2019, ranked third in the world in terms of [attracting investment for technology transactions.](https:\/\/www.ibef.org\/industry\/science-and-technology.aspx)\n\n\n\nHigher Data\/SW Engineering percentage in India can be because of the fact that most of the American & European companies have their offshore development and technical support offices in India. The country\u2019s outsourcing industry was recently [valued at 150 billion.](https:\/\/economictimes.indiatimes.com\/tech\/ites\/indias-technology-vendors-paddling-shaky-boats\/articleshow\/56543653.cms) ","2c155a0a":"#### SMB (Small and Medium-Sized Businesses)\n<ul>\n<li>SMB uses GCP among our groups, 2% higher compare to average usage<\/li>\n<li>SMB under-index when it come to MS Azure usage, 2% lower compare to average distribution<\/li>\n<\/ul>\n\n#### SME (Small and Medium Enterprises)\n<ul>\n<li>Over-index in AWS usage by 1% compare to Large & SMB organizations<\/li>\n<\/ul>\n\n#### Large Enterprise\n<ul>\n<li>Over-index when it comes to MS Azure usage, by 11.5% compare to 9.8%<\/li>\n<li>Under-index in Google Cloud Platform usage by 1.2%<\/li>\n<\/ul>","5eb0b4d8":"### 1.2. Similarities & Difference: DS Community Vs. Others\n<a id=\"snd\"><\/a>\n\nNow that we have defined our groups, lets examine some attributes avaiable in survey data to understand them better","67cd8f1e":"#### When it comes to Enterprise level Data Storage platform\n\nTop most DB Engines used by our DS Community in this survey is MySQL, where as according to [DB Engine Ranking](https:\/\/db-engines.com\/en\/ranking)\n","e5229bdd":"***\n# References \n<a id=\"references\"><\/a>\n***\n\n[1] Different data science jobs : https:\/\/blog.udacity.com\/2018\/01\/4-types-data-science-jobs.html\n\n[2] Business size classification : https:\/\/www.sangoma.com\/articles\/smb-sme-large-enterprise-size-business-matters\/                    \n\n[3] Choosing the right company based on business size  : https:\/\/www.monster.ie\/career-advice\/article\/how-can-i-choose-the-right-company  \n\n[4] Stats about India's tech industry : https:\/\/www.ibef.org\/industry\/science-and-technology.aspx\n\n[5] DB Engines : https:\/\/db-engines.com\/en\/\n\n[6] Trends in the Information Technology sector : https:\/\/www.brookings.edu\/research\/trends-in-the-information-technology-sector\/\n\n[7] Top 100 digital companies : https:\/\/www.forbes.com\/top-digital-companies\/list\/3\/#tab:rank","38709172":"***\n# Choosing the right company\n***","37f51215":"By looking at the characteristics mentioned above, everyone would want to join Large Enterprises as they pay more and actively use ML. \n\nBut at a large, established enterprises, most roles are highly specialized, and when a new problem occurs, it will be addressed by the person or team with that specific set of skills. If you try to solve a problem that isn\u2019t \u201cowned\u201d by your department, you\u2019re likely to step on some toes.\nAt a startup\u2014especially a very small one\u2014nearly every problem is an opportunity for you to step in and add value. Your coworkers are more likely to appreciate an action-oriented, problem-solving approach.\n","b751c2e2":"#### SMB (Small and Medium-Sized Businesses)\n<ul>\n<li>Over-indexing on deep learning libraries, stating what we have seen before that these companies are dealing with Image processing & NLP based problem more compare to other categories<\/li>\n<\/ul>\n\n#### SME (Small and Medium Enterprises)\n<ul>\n<li>Not much different as compare to overall distribution<\/li>\n<\/ul>\n\n#### Large Enterprise\n<ul>\n<li>Large Enterprise over-index when it comes to frameworks which are widely used for structured data.<\/li>\n<li>Although we can see that some of large companies are also using DL frameworks as well<\/li>\n<\/ul>\n\n\nAbove results are similar to what we have seen in the analysis about algorithm usage. This has further cleared the difference among these organizations regarding the kind of work they do in machine learning. ","dfec0179":"***\n# 1. Job Categorization\n<a id=\"job\"><\/a>\n***\n\nIn this section we will analyze different groups of people based on their job title available in Kaggle 2019 survey. This will be achieved by looking at \n\n1. How titles can be categorized and why?\n2. What are the factors making them similar or different?\n3. How they differ from other group based on survey data?","37cfc227":"### 2.2. Geographical Distribution of Enterprises\n<a id=\"geo\"><\/a>\n\nHere we are exploring where are these enterprise location by analyzing the Data Science Cohort only.","869f822c":"***\n# 3. Data Science Community at work\n<a id=\"dsc\"><\/a>\n***\n\nIn this section we analyze \n\n1. What is the difference of education level between DS community peers employed in different organization?\n2. Difference in compensation they can experience in small to large enterprise.\n3. What kind of exposure our data science community gets on different ML & DL frameworks, data storage & computing etc.? Based on the type pf organization they are working in.\n\n"}}