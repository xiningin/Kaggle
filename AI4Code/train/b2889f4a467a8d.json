{"cell_type":{"a178fbaf":"code","a322225d":"code","6ba4b2a3":"code","ab02a2a2":"code","9e5a5194":"code","3ab602f6":"code","6421a871":"code","b3022e09":"code","3abafdab":"code","cfbb671d":"code","e12d71f1":"code","d77c2776":"code","8b04ddb9":"code","2d946eaa":"code","1d7b36ad":"markdown","64471869":"markdown","ffa45631":"markdown","0135254a":"markdown","6aa3c1b5":"markdown","13bfe596":"markdown","460378cf":"markdown","446fa97f":"markdown","6d914ad8":"markdown","75bbc60e":"markdown","2f83e42c":"markdown","f4903b35":"markdown"},"source":{"a178fbaf":"import numpy as np\n\nfrom scipy.stats import uniform, randint\n\nfrom sklearn.datasets import load_breast_cancer, load_diabetes, load_wine\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n\nimport xgboost as xgb","a322225d":"def display_scores(scores):\n    print(\"Scores: {0}\\nMean: {1:.3f}\\nStd: {2:.3f}\".format(scores, np.mean(scores), np.std(scores)))","6ba4b2a3":"def report_best_scores(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")","ab02a2a2":"diabetes = load_diabetes()\n\nX = diabetes.data\ny = diabetes.target\n\nxgb_model = xgb.XGBRegressor(objective=\"reg:linear\", random_state=42)\n\nxgb_model.fit(X, y)\n\ny_pred = xgb_model.predict(X)\n\nmse=mean_squared_error(y, y_pred)\n\nprint(np.sqrt(mse))","9e5a5194":"xgb_model","3ab602f6":"cancer = load_breast_cancer()\n\nX = cancer.data\ny = cancer.target\n\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\nxgb_model.fit(X, y)\n\ny_pred = xgb_model.predict(X)\n\nprint(confusion_matrix(y, y_pred))","6421a871":"wine = load_wine()\n\nX = wine.data\ny = wine.target\n\nxgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\nxgb_model.fit(X, y)\n\ny_pred = xgb_model.predict(X)\n\nprint(confusion_matrix(y, y_pred))","b3022e09":"diabetes = load_diabetes()\n\nX = diabetes.data\ny = diabetes.target\n\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\n\nscores = []\n\nfor train_index, test_index in kfold.split(X):   \n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    xgb_model = xgb.XGBRegressor(objective=\"reg:linear\")\n    xgb_model.fit(X_train, y_train)\n    \n    y_pred = xgb_model.predict(X_test)\n    \n    scores.append(mean_squared_error(y_test, y_pred))\n    \ndisplay_scores(np.sqrt(scores))","3abafdab":"xgb_model = xgb.XGBRegressor(objective=\"reg:linear\", random_state=42)\n\nscores = cross_val_score(xgb_model, X, y, scoring=\"neg_mean_squared_error\", cv=5)\n\ndisplay_scores(np.sqrt(-scores))","cfbb671d":"diabetes = load_diabetes()\n\nX = diabetes.data\ny = diabetes.target\n\nxgb_model = xgb.XGBRegressor()\n\nparams = {\n    \"colsample_bytree\": uniform(0.7, 0.3),\n    \"gamma\": uniform(0, 0.5),\n    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n    \"max_depth\": randint(2, 6), # default 3\n    \"n_estimators\": randint(100, 150), # default 100\n    \"subsample\": uniform(0.6, 0.4)\n}\n\nsearch = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, cv=3, verbose=1, n_jobs=1, return_train_score=True)\n\nsearch.fit(X, y)\n\nreport_best_scores(search.cv_results_, 1)","e12d71f1":"cancer = load_breast_cancer()\n\nX = cancer.data\ny = cancer.target\n\n# if more than one evaluation metric are given the last one is used for early stopping\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, eval_metric=\"auc\")\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nxgb_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_test, y_test)])\n\ny_pred = xgb_model.predict(X_test)\n\naccuracy_score(y_test, y_pred)","d77c2776":"print(\"best score: {0}, best iteration: {1}, best ntree limit {2}\".format(xgb_model.best_score, xgb_model.best_iteration, xgb_model.best_ntree_limit))","8b04ddb9":"cancer = load_breast_cancer()\n\nX = cancer.data\ny = cancer.target\n\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", n_estimators=20, random_state=42, eval_metric=[\"auc\", \"error\", \"error@0.6\"])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nxgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n\ny_pred = xgb_model.predict(X_test)","2d946eaa":"# requires graphviz and python-graphviz conda packages\nimport graphviz\n\ncancer = load_breast_cancer()\n\nX = cancer.data\ny = cancer.target\n\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, eval_metric=\"auc\")\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nxgb_model.fit(X_train, y_train, early_stopping_rounds=10, eval_set=[(X_test, y_test)], verbose=False)\n\nxgb.plot_importance(xgb_model)\n\n# plot the output tree via matplotlib, specifying the ordinal number of the target tree\n# xgb.plot_tree(xgb_model, num_trees=xgb_model.best_iteration)\n\n# converts the target tree to a graphviz instance\nxgb.to_graphviz(xgb_model, num_trees=xgb_model.best_iteration)","1d7b36ad":"# XGBoost\n\nExploring the use of XGBoost and its integration with Scikit-Learn.\n\nSome useful links:\n* [XGBoost documentation](https:\/\/xgboost.readthedocs.io\/en\/latest\/index.html)\n* [Parameters](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html)\n* [Python package](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_intro.html)\n* [Python examples](https:\/\/github.com\/dmlc\/xgboost\/tree\/master\/demo\/guide-python)\n* [scikit-learn examples](https:\/\/github.com\/dmlc\/xgboost\/blob\/master\/demo\/guide-python\/sklearn_examples.py)\n* [Diabetes dataset](http:\/\/scikit-learn.org\/stable\/datasets\/index.html#diabetes-dataset)\n* [Breast cancer dataset](http:\/\/scikit-learn.org\/stable\/datasets\/index.html#breast-cancer-wisconsin-diagnostic-database)\n\nObjective is to demonstrate:\n* regression \u2713\n* binary classification \u2713\n* multiclass classification \u2713\n* cross-validation \u2713\n* hyperparameter searching \u2713\n* feature importance \u2713\n* early stopping \u2713\n* evaluations\n* plotting \u2713","64471869":"Cross-validation using `cross_val_score`","ffa45631":"## Early stopping","0135254a":"## Plotting","6aa3c1b5":"## Hyperparameter searching","13bfe596":"The number of boosted trees (`n_estimators`) to train is uncapped, rather training continues until validation has not improved in *n* rounds","460378cf":"## Multiclass classification","446fa97f":"`xgb_model.fit()` will produce a model from the last iteration, not the best one, so to get the optimum model consider retraining over `xgb_model.best_iteration` rounds.","6d914ad8":"## Binary classification","75bbc60e":"## Cross validation\n\nCross-validation using `KFold`","2f83e42c":"## Evaluations","f4903b35":"## Regression"}}