{"cell_type":{"19de8ed1":"code","34644ae0":"code","fc409919":"code","a40173ee":"code","8f27bec7":"code","f64abb16":"code","d2f74469":"code","2a2273b1":"code","e4e73e01":"code","d6f4b6ad":"code","3f7a9465":"code","24e3757d":"code","447cd39f":"code","ca746bc4":"code","67d8c061":"code","6f46dcaa":"code","f93496c1":"code","48a9f206":"code","a8d2a635":"code","b0cf04d5":"code","39f2ec5c":"code","2af51fa7":"code","71784ff4":"code","ea65e6a7":"code","88b4339e":"code","993e1b51":"code","ea6b7239":"code","53d3d519":"code","91306c9a":"code","a8e2581e":"code","cb9da8ce":"code","3afd86a7":"code","fda4e154":"code","7f579249":"code","75af2188":"code","7071f297":"code","2046119b":"code","3562b8a5":"code","1c0e5b66":"code","0538d280":"code","3be24924":"code","d85e842b":"code","205fc639":"code","c44a2585":"code","dfe8b874":"code","e3e35774":"code","94546156":"code","c366ef2f":"markdown","fb2fd040":"markdown","2697578f":"markdown","1919b52f":"markdown","8b6a6de3":"markdown","70e4d464":"markdown","977657e6":"markdown","826f3867":"markdown","c15cd659":"markdown","43579cbf":"markdown","1e9206a1":"markdown","c31fa7ba":"markdown","d02955a2":"markdown","140330ac":"markdown","3d67543b":"markdown","f807559d":"markdown","f7b723d8":"markdown","2611f6bf":"markdown","5aba402a":"markdown","d4a5894a":"markdown","433119e9":"markdown","05fd2766":"markdown"},"source":{"19de8ed1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import r2_score,mean_squared_error, \\\nmean_absolute_error,explained_variance_score\nsns.set()","34644ae0":"original=pd.read_csv('..\/input\/avocado.csv')","fc409919":"print(original.head())\nprint(original.shape)\nprint(original.isnull().sum())\nprint(original.dtypes)\nprint(original.columns)\nprint(original['type'].unique())","a40173ee":"original.drop(columns='Unnamed: 0',inplace=True)","8f27bec7":"original.columns","f64abb16":"original['Date']=pd.to_datetime(original['Date'])","d2f74469":"original.dtypes","2a2273b1":"original['Month']=[i.month for i in original['Date']]","e4e73e01":"original.head()","d6f4b6ad":"original['4046']=original['4046'].astype('str')\noriginal['4225']=original['4225'].astype('str')\noriginal['4770']=original['4770'].astype('str')","3f7a9465":"original.describe()","24e3757d":"def cv(data):\n    x=data.mean()\n    y=data.std()\n    coefficientofvariation=(round((y\/x)*100,2))\n    return coefficientofvariation","447cd39f":"print('AveragePrice cv:%1.2f%%'%(cv(original['AveragePrice'])))\nprint('Total Volume cv:%1.2f%%'%(cv(original['Total Volume'])))\nprint('Total Bags cv:%1.2f%%'%(cv(original['Total Bags'])))\nprint('Small Bags cv:%1.2f%%'%(cv(original['Small Bags'])))\nprint('Large Bags cv:%1.2f%%'%(cv(original['Large Bags'])))\nprint('XLarge Bags cv:%1.2f%%'%(cv(original['XLarge Bags'])))","ca746bc4":"x=['AveragePrice','Total Volume','Total Bags','Small Bags','Large Bags','XLarge Bags']\nprint('skewness')\nprint('=============================')\nprint(original.skew())\nprint('=============================')\nprint('kurtosis')\nprint('=============================')\nprint(original.kurt())\nprint('=============================')\nprint('mode')\nprint('=============================')\nprint('average Price mode:',original['AveragePrice'].mode())\nprint('Total Volume mode:',original['Total Volume'].mode())\nprint('Total Bags mode:',original['Total Bags'].mode())\nprint('Small Bags mode:',original['Small Bags'].mode())\nprint('Large Bags mode:',original['Large Bags'].mode())\nprint('XLarge Bags mode:',original['XLarge Bags'].mode())\n","67d8c061":"z0=plt.figure(figsize=(15,14))\nfor i in range(len(x)):\n    z0.add_subplot(3,3,i+1)\n    original[str(x[i])].plot(kind='kde',title='%s distribution plot'%x[i])\nelse:\n    pass\nplt.show()","6f46dcaa":"bagGroup=original.groupby(by='year')\nbagsum=bagGroup.sum()\nbagSum=pd.DataFrame(bagsum.loc[:,'Total Bags'])\nbagSum.iloc[0:3,0]","f93496c1":"z1=bagSum.iloc[0:3].plot.bar(figsize=(15,14),legend=False)\nz1.get_figure()\nx=np.arange(bagsum.index.shape[0]-1)\ny=np.array(bagSum.iloc[0:3,0])\nplt.title('2013-2017 total bags by year bar graph',fontsize='large')\nplt.ylabel('bags(hundred million)')\nplt.xticks(x,bagSum.index[0:3],rotation=0)\nfor i,j in zip(x,y):\n    plt.text(i,j+10000000,format(round(j,2),','),color='blue',ha='center')\nelse:\n    pass\nplt.show()","48a9f206":"pricePivot=pd.pivot_table(original,index='year',columns='Month',values='AveragePrice',aggfunc=np.mean)\npricePivot","a8d2a635":"pricePivot.index=pricePivot.index.astype('str')\npricePivot=pricePivot.drop(index='2018')","b0cf04d5":"pricePivot","39f2ec5c":"z2=plt.figure(figsize=(25,10))\ny=np.array(list([pricePivot.iloc[0,:],pricePivot.iloc[1,:],pricePivot.iloc[2,:]]))\ny=y.reshape((36,1))\nplt.plot(y,linestyle='--',marker='o')\nyear1=np.array(['2015','2016','2017'])\nmonth1=np.array([str(i) for i in range(1,13)])\nyearmonth=[]\nfor i in range(3):\n    for j in range(12):\n        yearmonth.append(year1[i]+'\/'+month1[j])\n    else:\n        pass\nelse:\n    pass\nx=np.arange(36)\nfor i,j in zip(x,y):\n    plt.text(i,j+0.005,'%1.2f'%j,ha='center')\nelse:\n    pass\nplt.xticks(range(len(yearmonth)),yearmonth,rotation=45)\nplt.title('2015-2017 average price plot',fontsize='large')\nplt.show()","2af51fa7":"DbagSum=bagsum.loc[:,['Small Bags','Large Bags','XLarge Bags']]\nDbagSum","71784ff4":"z3=plt.figure(figsize=(15,14))\nlabels=DbagSum.columns\nexplode=[0.0,0.0,0.0]\nfor i in range(DbagSum.index.shape[0]):\n    z3.add_subplot(2,2,i+1)\n    plt.title('%s Avocade bags pie chart'%DbagSum.index[i])\n    plt.pie(DbagSum.iloc[i,:],labels=labels,explode=explode,autopct='%1.2f%%')\nelse:\n    pass\nplt.show()","ea65e6a7":"original.head()","88b4339e":"data=original.loc[:,['AveragePrice','4046','4225','4770','type','region']]","993e1b51":"data.head()","ea6b7239":"print(data['type'].unique())\nprint(data['region'].unique())","53d3d519":"for i in range(data.index.shape[0]):\n    if data.loc[i,'type']=='conventional':\n        data.loc[i,'type']=0\n    else:\n        data.loc[i,'type']=1","91306c9a":"data.head()","a8e2581e":"data['region']=LabelEncoder().fit_transform(data['region'])","cb9da8ce":"data.dtypes","3afd86a7":"data['4046']=data['4046'].astype('float')\ndata['4225']=data['4225'].astype('float')\ndata['4770']=data['4770'].astype('float')","fda4e154":"pd.DataFrame(data.corr(method='pearson'))","7f579249":"Data=data.iloc[:,1:]\nTarget=data.loc[:,'AveragePrice']","75af2188":"pca=PCA(n_components=5).fit(Data)\npca.explained_variance_ratio_","7071f297":"Pca=PCA(n_components=3).fit(Data)\npData=Pca.transform(Data)\npData","2046119b":"data_train,data_test, \\\ntarget_train,target_test = \\\ntrain_test_split(pData,Target,train_size=0.6)","3562b8a5":"GBR=GradientBoostingRegressor(learning_rate=0.2).fit(data_train,target_train)\nGBR","1c0e5b66":"pre=GBR.predict(data_train)\nprint('explained_variance_score:%1.2f'%(explained_variance_score(target_train,pre)))\nprint('r2_score:%1.2f'%r2_score(target_train,pre))\nprint('mean_absolute_error:%1.2f'%mean_absolute_error(target_train,pre))\nprint('mean_squared_error:%1.2f'%mean_squared_error(target_train,pre))","0538d280":"predict=GBR.predict(data_test)\nprint('explained_variance_score:%1.2f'%(explained_variance_score(target_test,predict)))\nprint('r2_score:%1.2f'%r2_score(target_test,predict))\nprint('mean_absolute_error:%1.2f'%mean_absolute_error(target_test,predict))\nprint('mean_squared_error:%1.2f'%mean_squared_error(target_test,predict))","3be24924":"data.head()","d85e842b":"Fdata=data.loc[:,['4046','4225','4770','type']]\nFtarget=data.loc[:,'AveragePrice']","205fc639":"dataTrain,dataTest, \\\ntargetTrain,targetTest = \\\ntrain_test_split(Fdata,Ftarget,train_size=0.6)","c44a2585":"FGBR=GradientBoostingRegressor(learning_rate=0.3).fit(dataTrain,targetTrain)","dfe8b874":"FGBR","e3e35774":"Fpre=FGBR.predict(dataTrain)\nprint('explained_variance_score:%1.2f'%(explained_variance_score(targetTrain,Fpre)))\nprint('r2_score:%1.2f'%r2_score(targetTrain,Fpre))\nprint('mean_absolute_error:%1.2f'%mean_absolute_error(targetTrain,Fpre))\nprint('mean_squared_error:%1.2f'%mean_squared_error(targetTrain,Fpre))","94546156":"Fpredict=FGBR.predict(dataTest)\nprint('explained_variance_score:%1.2f'%(explained_variance_score(targetTest,Fpredict)))\nprint('r2_score:%1.2f'%r2_score(targetTest,Fpredict))\nprint('mean_absolute_error:%1.2f'%mean_absolute_error(targetTest,Fpredict))\nprint('mean_squared_error:%1.2f'%mean_squared_error(targetTest,Fpredict))","c366ef2f":"## Principal component analysis","fb2fd040":"## Question Defintion\n\n\n- Which year has the most number of bags?\n\n\n- What is the trend of Average price?\n\n\n- What is the percentage of different size bags of each year?\n\n\n- Can we predict the average price with this data?","2697578f":"## Predict--PCA","1919b52f":"## Predict\n\nThe correlation between features is normally ok,\n\nand PCA n_components is 3\n\nWith using two data,\n\npredicting average price with Gradient Boosting Regressor","8b6a6de3":"## Predict--Feature Selection","70e4d464":"## Summary\n\n\n- 2017 has the most number of bags\n\n\n- 2018's data is not complete(below this,pricePivot has shown the 2018's data),\n\n  so show the bar graph without 2018.\n  \n  \n- Between 2015 and 2016,rising of numbers is large.\n\n\n- Number of total bags is growing.","977657e6":"## Summary\n\nWith using PCA and feature selection,\n\nfinding that using feature selection has higher r2_score\n\n- using feature selection is better than PCA data\n\n\n- With using feature selection,r2_score is 0.52\n","826f3867":"## Summary\n\n- Mean of average Price is 1.4,mode is 1.15\n\n\n- Mode of total volume is not unique,\n\n  and mode of Xlarge Bags is 0,\n  \n  other data has no mode.\n\n\n- Coefficient of variation of data except average price is big,\n\n  and other data's coefficient of variation is over 100%,\n  \n  means other data is much more dispersive than average price.\n  \n\n\n- The skewness of average price is possitive distribution,\n\n  and the kurtosis of average price is platykurtic.\n  \n\n\n- The skewness values of other data is big,\n\n  which means other data's distribution has much more different from\n  \n  normal distribution.\n  \n  And skewness values are all possitive,which means all of data is \n  \n  possitive distribution.\n\n\n- The kurtosis of other data is leptokurtic.\n\n  ","c15cd659":"## Percentage of different size bags\n\nWhat is the percentage of different size bags of each year?","43579cbf":"# Kaggle--Avocado Prices dataset\n\n# 2019-04-13 start","1e9206a1":"## Summary\n\n- Percentage of different size bags\n\nYear | Bag Size | Percentage |\n----- | -------- | ------------ |\n2015 | Small Bags | 82.19% |\n     | Large Bags | 17.10% |\n     | XLarge Bags | 0.7% |\n2016 | Small Bags | 75.62% |\n     | Large Bags | 23.01% |\n     | XLarge Bags | 1.37% |\n2017 | Small Bags | 74.29% |\n     | Large Bags | 24.26% |\n     | XLarge Bags | 1.46% |\n2018 | Small Bags | 73.39% |\n     | Large Bags | 25.14% |\n     | XLarge Bags | 1.47% |\n  \n  \n- The percentage of small bags is decreasing\n\n\n- The percentage of large bags is increasing fast\n\n\n- The percentage of XLarge bags is increasing\n\n\n- Between 2015 and 2016,the percentage of XLarge bags suddenly increase.\n","c31fa7ba":"## Describe Statistics","d02955a2":"## Predict","140330ac":"## Summary\n\nWith using PCA,and show the explained variance ratio,\n\nn_components is choosing 3\n\nAnd transform the data.","3d67543b":"## Summary\n\n- The average price of each month seems to be cyclical\n\n\n- 2015's average price is smooth,and 2017's average price has much more fluctuation\n\n\n- December~May's average price is normally low,and rising after this section\n\n\n- 2017\/9 has the highest price,but declining after this month\n\n\n- Winter's average price is normally low,\n\n  and autumn's average price is normally high.\n\n\n- 2016\/5 has the lowest price,but rising after this month.\n\n\n- 2017's average price is normally higher than other year.","f807559d":"## Bags\n\nWhich year has the most number of bags?","f7b723d8":"## Data preprocessing","2611f6bf":"## Data Preprocessing","5aba402a":"## Price\n\nWhat is the trend of Average price?","d4a5894a":"# Summary\n\n## Describe Statistics\n\n- Mean of average price is 1.4,mode is 1.15\n\n\n- Mode of total volume is not unique.\n\n\n- The other data(except average price) is dispersion,\n\n  and large different from normal distribution\n\n\n- All of data is possitive distribution\n\n\n## Total bags of each year\n\n- 2017 has the most number of total bags than other year\n\n\n- 2018's data is not completed\n\n\n- Number of bags is increasing.\n\n\n## Trend of average price\n\n- The trend of average price seems to be cyclical\n\n\n- 2017's average price has much more fluctuation\n\n\n- 2017\/9 has the highest average price,\n\n  and 2016\/5 has the lowest average price\n\n\n- Winter's price is normally low,\n\n  and autumn's price is normally high.\n  \n  \n## Percentage of different size bags\n\n- percentage of differnet size bags each year\n\n\nYear | Bag Size | Percentage |\n----- | -------- | ------------ |\n2015 | Small Bags | 82.19% |\n     | Large Bags | 17.10% |\n     | XLarge Bags | 0.7% |\n2016 | Small Bags | 75.62% |\n     | Large Bags | 23.01% |\n     | XLarge Bags | 1.37% |\n2017 | Small Bags | 74.29% |\n     | Large Bags | 24.26% |\n     | XLarge Bags | 1.46% |\n2018 | Small Bags | 73.39% |\n     | Large Bags | 25.14% |\n     | XLarge Bags | 1.47% |\n\n\n- The percentage of small bags is decreasing.\n\n\n- Between 2015 and 2016,XLarge bags suddenly increase.\n\n\n- The percentage of large bags and Xlarge bags is increasing.\n\n## Correlation analysis\n\n- Category is negative correlation with average price\n\n\n- Region is less correlation with average price\n\n\n- Feature selection is feasibility\n\n\n## Principal component analysis\n\n- Finding the explained variance ratio\n\n\n- n_components choose 3\n\n## Predict\n\n- Using feature selection is better than using PCA\n\n\n- With using feature selection,r2_score is 0.52\n\n\n- With using this data,training model is hard\n","433119e9":"## Summary\n\nWith pearson coefficient matrix,\n\n4046.4225.4770 is negative correlation with average price,\n\nand type is possitive correlation\n\nRegion has less correlation with average price.","05fd2766":"## Correlation analysis--Pearson coefficient matrix"}}