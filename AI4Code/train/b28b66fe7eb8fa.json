{"cell_type":{"f033114d":"code","4c198cc2":"code","563ec6d9":"code","5d71ac5d":"code","e3eb4aa4":"code","2f1f6cc0":"code","ac897d61":"code","1a042e7b":"code","e74347d0":"code","4211d35a":"code","8da9aec0":"code","57385369":"code","8f494eef":"code","abcc790d":"code","04de94e0":"code","2b760081":"code","07bd0791":"code","45c5e505":"code","3ec16e0c":"code","104cb635":"code","7c032646":"code","11988143":"code","24d290cb":"code","1db8391c":"code","a8f9287e":"code","e2e5afcd":"code","444fb309":"code","e5aa636b":"code","58f2ef05":"code","f125d1fb":"code","8f469d06":"code","f1af45c5":"code","ff9d31ea":"code","0a48d516":"code","d188d989":"code","850d8a51":"code","28dbe75b":"code","023e6d56":"code","bde042a9":"code","ef8e1810":"code","1d783359":"code","e244a1f0":"code","06bf8cec":"code","166b3c16":"code","64b3a5d1":"code","ec132a3d":"code","59524683":"code","0a25eea2":"code","eba97f33":"markdown","dc89cde4":"markdown","dc664b0a":"markdown","a80fdca8":"markdown","813a9a6f":"markdown","d153d45f":"markdown","fe533705":"markdown","e66822c7":"markdown","3bab5e8a":"markdown"},"source":{"f033114d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4c198cc2":"df=pd.read_csv(\"\/kaggle\/input\/trump-tweets\/trumptweets.csv\")\ndf","563ec6d9":"import nltk","5d71ac5d":"from nltk.corpus import stopwords","e3eb4aa4":"import re","2f1f6cc0":"from nltk.stem import WordNetLemmatizer ","ac897d61":"clean=[]","1a042e7b":"for i in range(0, 15000):\n    review = re.sub('(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)|^rt|http.+?\"', ' ', df['content'][i])\n    review = review.lower()\n    review = review.split()\n    lm= WordNetLemmatizer() \n    review = [lm.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    clean.append(review)","e74347d0":"df['content'][0]","4211d35a":"clean[0]","8da9aec0":"df_new=pd.DataFrame(df['content'][0:15000])\ndf_new","57385369":"df_new['tweets']=clean","8f494eef":"df_new","abcc790d":"from nltk.tokenize import word_tokenize","04de94e0":"df_new['tokens']=df_new['tweets'].apply(word_tokenize)","2b760081":"df_new","07bd0791":"df_new['tokens'][90]","45c5e505":"from sklearn.feature_extraction.text import CountVectorizer","3ec16e0c":"df_new['tokens'][90]","104cb635":"vect = CountVectorizer().fit(df_new['tokens'][90])\nbag_of_words = vect.transform(df_new['tokens'][90])\nsum_words = bag_of_words.sum(axis=0) ","7c032646":"sum_words","11988143":"def most_freq_words(s, n=None):\n    vect = CountVectorizer().fit(s)\n    bag_of_words = vect.transform(s)\n    sum_words = bag_of_words.sum(axis=0) \n    freq = [(word, sum_words[0, idx]) for word, idx in vect.vocabulary_.items()]\n    freq =sorted(freq, key = lambda x: x[1], reverse=True)\n    return freq[:n]","24d290cb":"most_freq_words([ word for tweet in df_new.tokens for word in tweet],20)","1db8391c":"def least_freq_words(s, n=None):\n    vect = CountVectorizer().fit(s)\n    bag_of_words = vect.transform(s)\n    sum_words = bag_of_words.sum(axis=0) \n    freq = [(word, sum_words[0, idx]) for word, idx in vect.vocabulary_.items()]\n    freq =sorted(freq, key = lambda x: x[1], reverse=False)\n    return freq[:n]","a8f9287e":"least_freq_words([ word for tweet in df_new.tokens for word in tweet],20)","e2e5afcd":"df_new['tokens'][1]","444fb309":"df_new","e5aa636b":"vectorizer = CountVectorizer(min_df=0)# Here \"min_df\" in the parameter refers to the minimum document frequency and the vectorizer will simply drop all words that occur less than that value set (either integer or in fraction form)\nsentence_transform = vectorizer.fit_transform(df_new['tweets'])","58f2ef05":"sentence_transform","f125d1fb":"sentence_transform.shape","8f469d06":"print(\"\\nThe vectorized array looks like:\\n {}\".format(sentence_transform.toarray()))","f1af45c5":"from sklearn.decomposition import LatentDirichletAllocation","ff9d31ea":"lda = LatentDirichletAllocation(n_components=8, max_iter=5,\n                                learning_method = 'online',\n                                learning_offset = 50.,\n                                random_state = 0)\n#n_components are the number of topics you want to classify.","0a48d516":"lda.fit(sentence_transform)","d188d989":"words=[]","850d8a51":"in_arr = np.array([ 2, 0,  1, 5, 4, 1, 9]) \nprint (\"Input unsorted array : \", in_arr)  \n  \nout_arr = np.argsort(in_arr) \nprint (\"Output sorted array indices : \", out_arr) \nprint(\"Output sorted array in Ascending Order: \", in_arr[out_arr]) \n","28dbe75b":"out_arr_new=np.argsort(in_arr)[::-1]\nprint(\"Output Sorted Array in Descending Order\",in_arr[out_arr_new])","023e6d56":"out_arr_new=np.argsort(in_arr)[::-1][:4]\nprint(\"Output Sorted Array in Descending Order\",in_arr[out_arr_new])","bde042a9":"out_arr_new=np.argsort(in_arr)[:-4-1:-1]\nprint(\"Output Sorted Array in Descending Order\",in_arr[out_arr_new])","ef8e1810":"def print_top_words(model, feature_names, n_top_words):\n    for index, topic in enumerate(model.components_):\n        message = \"\\nTopic{}:\".format(index)\n        message += \" \".join([feature_names[i] for i in topic.argsort()[::-1][:n_top_words]])\n        print(message)\n        words.append(message)\n        print(\"=\"*170)","1d783359":"n_top_words = 40\nprint(\"\\nTopics in LDA model: \")\ntf_feature_names = vectorizer.get_feature_names()\nprint_top_words(lda, tf_feature_names, n_top_words)","e244a1f0":"first_topic = lda.components_[0]\nsecond_topic = lda.components_[1]\nthird_topic = lda.components_[2]","06bf8cec":"first_topic","166b3c16":"first_topic.shape","64b3a5d1":"second_topic.shape","ec132a3d":"words[0]","59524683":"from wordcloud import WordCloud ","0a25eea2":"for i in range(0,8):\n    wordcloud = WordCloud(max_font_size=40, max_words=40).generate(words[i])\n\n# Display the generated image:\n    plt.figure(figsize=(10,10))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","eba97f33":"# This is another way to get top 4 numbers","dc89cde4":"# Argsort gives the indices in ascending order by default. To make it descending order we have to add [::-1] with the array.","dc664b0a":"A type of statistical modelling for discovering the abstract \"topics\" that occur in a collection of documents.\nA document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.","a80fdca8":" # Now if we want top 4 numbers in descending order..","813a9a6f":"# Least Frequent Words..","d153d45f":"# All the contents are cleaned..","fe533705":"# Get most Frequent Words..","e66822c7":"# Arg Sort-->","3bab5e8a":"# WHAT IS A TOPIC MODELLING?"}}