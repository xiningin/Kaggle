{"cell_type":{"17c76dc1":"code","552ad60f":"code","da262c3b":"code","dcbe4918":"code","e43da1a1":"code","cbebee87":"code","3a02287d":"code","ddc3ce63":"code","da6638ee":"code","a6053b8e":"code","ccd144e7":"code","20b10bb3":"code","a13d458c":"code","59b3ad77":"code","64888a15":"code","3c9dac21":"code","bd3050dd":"code","ada9873c":"code","755ece95":"code","f9d50519":"code","710d1ea7":"code","0c0ae998":"code","0b025c7b":"code","8721c44e":"code","8bba028d":"code","100264c5":"code","1de9ac03":"code","2c9b5e22":"code","bf8d152e":"code","d86da85d":"code","77edcaf5":"code","3575c436":"code","29008dcc":"code","92d09a42":"code","0555620c":"code","32e3604b":"code","193211aa":"code","c15e512d":"code","fcb26a51":"code","70177534":"code","04cd0653":"code","3497c48c":"code","15169d90":"code","b3e79669":"code","291a73b1":"code","aff00be8":"code","6e6fc73c":"markdown","b53e75df":"markdown","61f7fe0d":"markdown","8894a9d2":"markdown","7a8c7bb4":"markdown","0dfb6f55":"markdown","7a3be491":"markdown","912d208d":"markdown","2ebff379":"markdown","b3a60f8d":"markdown","5d85f2e1":"markdown","cfeeb0d7":"markdown","9f3ef3a8":"markdown","851782e1":"markdown","e4e0e95e":"markdown","e43221d1":"markdown","51694991":"markdown","09d807c2":"markdown","a7cc7347":"markdown","ad1fb2fd":"markdown","4becfc6c":"markdown","d64e709d":"markdown","99c4c958":"markdown","e4bd8d89":"markdown","ccb648d7":"markdown","1a819664":"markdown","32c09c0e":"markdown","2b3459d2":"markdown","781be1e9":"markdown","1ef0c099":"markdown","66d3778e":"markdown","4c017303":"markdown","b77ab087":"markdown","79dc51f5":"markdown","ea5c8e88":"markdown"},"source":{"17c76dc1":"fit_gaussians = False\nuse_plotly=True","552ad60f":"# data manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n# sklearn models & tools\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nimport os\nprint(os.listdir(\"..\/input\"))","da262c3b":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")","dcbe4918":"train.shape","e43da1a1":"train.head(10)","cbebee87":"train.target.dtype","3a02287d":"org_vars = train.drop([\"target\", \"ID_code\"], axis=1).columns.values\nlen(org_vars)","ddc3ce63":"train[\"Id\"] = train.index.values\noriginal_trainid = train.ID_code.values\n\ntrain.drop(\"ID_code\", axis=1, inplace=True)","da6638ee":"train.isnull().sum().sum()","a6053b8e":"test.head(10)","ccd144e7":"test.isnull().sum().sum()","20b10bb3":"test.shape","a13d458c":"test[\"Id\"] = test.index.values\noriginal_testid = test.ID_code.values\n\ntest.drop(\"ID_code\", axis=1, inplace=True)","59b3ad77":"submission.head()","64888a15":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.countplot(train.target.values, ax=ax[0], palette=\"husl\")\nsns.violinplot(x=train.target.values, y=train.index.values, ax=ax[1], palette=\"husl\")\nsns.stripplot(x=train.target.values, y=train.index.values,\n              jitter=True, ax=ax[1], color=\"black\", size=0.5, alpha=0.5)\nax[1].set_xlabel(\"Target\")\nax[1].set_ylabel(\"Index\");\nax[0].set_xlabel(\"Target\")\nax[0].set_ylabel(\"Counts\");","3c9dac21":"train.loc[train.target==1].shape[0] \/ train.loc[train.target==0].shape[0]","bd3050dd":"train_correlations = train.drop([\"target\"], axis=1).corr()\ntrain_correlations = train_correlations.values.flatten()\ntrain_correlations = train_correlations[train_correlations != 1]\n\ntest_correlations = test.corr()\ntest_correlations = test_correlations.values.flatten()\ntest_correlations = test_correlations[test_correlations != 1]\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_correlations, color=\"Red\", label=\"train\")\nsns.distplot(test_correlations, color=\"Green\", label=\"test\")\nplt.xlabel(\"Correlation values found in train (except 1)\")\nplt.ylabel(\"Density\")\nplt.title(\"Are there correlations between features?\"); \nplt.legend();","ada9873c":"parameters = {'min_samples_leaf': [20, 25]}\nforest = RandomForestClassifier(max_depth=15, n_estimators=15)\ngrid = GridSearchCV(forest, parameters, cv=3, n_jobs=-1, verbose=2, scoring=make_scorer(roc_auc_score))","755ece95":"grid.fit(train.drop(\"target\", axis=1).values, train.target.values)","f9d50519":"grid.best_score_","710d1ea7":"grid.best_params_","0c0ae998":"n_top = 5 ","0b025c7b":"importances = grid.best_estimator_.feature_importances_\nidx = np.argsort(importances)[::-1][0:n_top]\nfeature_names = train.drop(\"target\", axis=1).columns.values\n\nplt.figure(figsize=(20,5))\nsns.barplot(x=feature_names[idx], y=importances[idx]);\nplt.title(\"What are the top important features to start with?\");","8721c44e":"fig, ax = plt.subplots(n_top,2,figsize=(20,5*n_top))\n\nfor n in range(n_top):\n    sns.distplot(train.loc[train.target==0, feature_names[idx][n]], ax=ax[n,0], color=\"Orange\", norm_hist=True)\n    sns.distplot(train.loc[train.target==1, feature_names[idx][n]], ax=ax[n,0], color=\"Red\", norm_hist=True)\n    sns.distplot(test.loc[:, feature_names[idx][n]], ax=ax[n,1], color=\"Mediumseagreen\", norm_hist=True)\n    ax[n,0].set_title(\"Train {}\".format(feature_names[idx][n]))\n    ax[n,1].set_title(\"Test {}\".format(feature_names[idx][n]))\n    ax[n,0].set_xlabel(\"\")\n    ax[n,1].set_xlabel(\"\")","8bba028d":"top = train.loc[:, feature_names[idx]]\ntop.describe()","100264c5":"top = top.join(train.target)\nsns.pairplot(top, hue=\"target\")","1de9ac03":"y_proba = grid.predict_proba(test.values)\ny_proba_train = grid.predict_proba(train.drop(\"target\", axis=1).values)","2c9b5e22":"fig, ax = plt.subplots(2,1,figsize=(20,8))\nsns.distplot(y_proba_train[train.target==1,1], norm_hist=True, color=\"mediumseagreen\",\n             ax=ax[0], label=\"1\")\nsns.distplot(y_proba_train[train.target==0,1], norm_hist=True, color=\"coral\",\n             ax=ax[0], label=\"0\")\nsns.distplot(y_proba[:,1], norm_hist=True,\n             ax=ax[1], color=\"purple\")\nax[1].set_xlabel(\"Predicted probability for test data\");\nax[1].set_ylabel(\"Density\");\nax[0].set_xlabel(\"Predicted probability for train data\");\nax[0].set_ylabel(\"Density\");\nax[0].legend();","bf8d152e":"submission[\"target\"] = y_proba","d86da85d":"submission.to_csv(\"submission_baseline_forest.csv\", index=False)","77edcaf5":"original_features = train.drop([\"target\", \"Id\"], axis=1).columns.values\noriginal_features","3575c436":"encoder = LabelEncoder()\nfor your_feature in top.drop(\"target\", axis=1).columns.values:\n    train[your_feature + \"_qbinned\"] = pd.qcut(\n        train.loc[:, your_feature].values,\n        q=10,\n        labels=False\n    )\n    train[your_feature + \"_qbinned\"] = encoder.fit_transform(\n        train[your_feature + \"_qbinned\"].values.reshape(-1, 1)\n    )\n    \n    \n    train[your_feature + \"_rounded\"] = np.round(train.loc[:, your_feature].values)\n    train[your_feature + \"_rounded_10\"] = np.round(10*train.loc[:, your_feature].values)\n    train[your_feature + \"_rounded_100\"] = np.round(100*train.loc[:, your_feature].values)","29008dcc":"cv = StratifiedKFold(n_splits=3, random_state=0)\nforest = RandomForestClassifier(max_depth=15, n_estimators=15, min_samples_leaf=20,\n                                n_jobs=-1)\n\nscores = []\nX = train.drop(\"target\", axis=1).values\ny = train.target.values\n\nfor train_idx, test_idx in cv.split(X, y):\n    x_train = X[train_idx]\n    x_test = X[test_idx]\n    y_train = y[train_idx]\n    y_test = y[test_idx]\n    \n    forest.fit(x_train, y_train)\n    y_proba = forest.predict_proba(x_test)\n    y_pred = np.zeros(y_proba.shape[0])\n    y_pred[y_proba[:,1] >= 0.166] = 1\n    \n    score = roc_auc_score(y_test, y_pred)\n    print(score)\n    scores.append(score)\n\nprint(np.round(np.mean(scores),4))\nprint(np.round(np.std(scores), 4))","92d09a42":"importances = forest.feature_importances_\nfeature_names = train.drop(\"target\", axis=1).columns.values\nidx = np.argsort(importances)[::-1][0:30]\n\nplt.figure(figsize=(20,5))\nsns.barplot(x=feature_names[idx], y=importances[idx]);\nplt.xticks(rotation=90);","0555620c":"col1 = \"var_81\"\ncol2 = \"var_12\"\nN=70000","32e3604b":"fig, ax = plt.subplots(1,1, figsize=(20,10))\nsns.kdeplot(train[col1].values[0:N], train[col2].values[0:N])\nax.scatter(train[col1].values[0:N], train[col2].values[0:N],\n           s=2, c=train.target.values[0:N], cmap=\"coolwarm\", alpha=0.5)\nax.set_xlabel(col1)\nax.set_xlabel(col2);","193211aa":"combined = train.drop([\"target\", \"Id\"], axis=1).append(test.drop(\"Id\", axis=1))\ncombined.shape","c15e512d":"max_components = 10\nstart_components = 3\nn_splits = 3\nK = train.shape[0]\n\nX = train.loc[:, original_features].values[0:K]\ny = train.target.values[0:K]","fcb26a51":"seeds = np.random.RandomState(0).randint(0,100, size=(max_components-start_components))\nseeds","70177534":"scaler = RobustScaler()\nX_scaled = scaler.fit_transform(X)","04cd0653":"if fit_gaussians:\n    components = np.arange(start_components, max_components, 1)\n    kf = StratifiedKFold(random_state=0, n_splits=n_splits)\n    \n    scores = np.zeros(shape=(max_components-start_components, n_splits))\n\n    for m in components:\n        split=0\n        print(\"Components \" + str(m))\n        for train_index, test_index in kf.split(X_scaled, y):\n            print(\"Split \" + str(split))\n            x_train, x_test = X_scaled[train_index], X_scaled[test_index]\n            gm = GaussianMixture(n_components=m, random_state=seeds[m-start_components])\n            gm.fit(x_train)\n            score = gm.score(x_test)\n            scores[m-start_components,split] = score\n            split +=1\n    \n    print(np.round(np.mean(scores, axis=1), 2))\n    print(np.round(np.std(scores, axis=1), 2))\n    best_idx = np.argmax(np.mean(scores, axis=1))\n    best_component = components[best_idx]\n    best_seed = seeds[best_idx]\n    print(\"Best component found \" + str(best_component))\n    \nelse:\n    best_seed = seeds[0]\n    best_component = 3","3497c48c":"X = train.loc[:, original_features].values\n\ngm = GaussianMixture(n_components=best_component, random_state=best_seed)\nX_scaled = scaler.transform(X)\ngm.fit(X_scaled)","15169d90":"train[\"cluster\"] = gm.predict(X_scaled)\ntrain[\"logL\"] = gm.score_samples(X_scaled)\ntest[\"cluster\"] = gm.predict(test.loc[:, original_features].values)\ntest[\"logL\"] = gm.score_samples(test.loc[:, original_features].values)","b3e79669":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.countplot(train.cluster, palette=\"Set2\", ax=ax[0])\nsns.distplot(train.logL, color=\"Dodgerblue\", ax=ax[1]);","291a73b1":"cluster_occupation = train.groupby(\"cluster\").target.value_counts() \/ train.groupby(\"cluster\").size() * 100\ncluster_occupation = cluster_occupation.loc[:, 1]\n\ntarget_occupation = train.groupby(\"target\").cluster.value_counts() \/ train.groupby(\"target\").size() * 100\ntarget_occupation = target_occupation.loc[1, :]\ntarget_occupation.index = target_occupation.index.droplevel(\"target\")\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nax[0].set_title(\"How many % of the data per cluster has hot targets?\")\nsns.barplot(cluster_occupation.index, cluster_occupation.values, ax=ax[0], color=\"cornflowerblue\")\nax[0].set_ylabel(\"% of cluster data\")\nax[0].set_ylim([0,100])\n\nax[1].set_title(\"How many % of total hot targets are in one cluster?\")\nsns.barplot(target_occupation.index, target_occupation.values, ax=ax[1], color=\"tomato\")\nax[1].set_ylabel(\"% of hot targets\")\nax[1].set_ylim([0,100]);","aff00be8":"plt.figure(figsize=(20,5))\nfor n in range(gm.means_.shape[0]):\n    plt.plot(gm.means_[n,:], 'o')\nplt.title(\"How do the gaussian means look like?\")\nplt.ylabel(\"Cluster mean value\")\nplt.xlabel(\"Feature\")","6e6fc73c":"Crazy! Can you see the sharp limits of several variables where the samples with target 1 suddenly accumulate and seldomly pass over. Look at var 81 and 12 for example. You can see that there are limits close to 10 (var 81) and 13.5 (var 12). This finding could be a nice entry point for further feature engineering.","b53e75df":"## Kernel settings","61f7fe0d":"Let's take a look at n_top features of your choice:","8894a9d2":"* At least one big gaussians with one or two small, very thin but long gaussians.\n* It's very interesting that we can still find outliers beside sharp lines. \n\nLet's assume now that the data was generated using a mixture of gaussians and let's try to cluster them. Perhaps we can see that some clusters occupy more hot targets than others.","7a8c7bb4":"### Take Away\n\n* Interestingly there are some peeks inside the distributions, especially for variables 81, 12 and 53. Why do these data points accumulate on these values?\n* We can observe that the accumulations are less dense in the test data. \n* Variable 174 seem to miss the bulb on the right hand side of the distribution in the test data. ","0dfb6f55":"## What can we say about the target? <a class=\"anchor\" id=\"target\"><\/a>","7a3be491":"You can see that the score is not as good as some other scores of public kernels but nontheless my attempt is to understand the data by improving this score. We can use more powerful models later on.","912d208d":"### Exploring top features\n\nFirst of all: How do the distributions of the variables look like with respect to the targets in train? Can we observe discrepancies between train and test features for selected top features?","2ebff379":"Ok, not much to say about it.","b3a60f8d":"Woooow! :-O All features seem to have no linear correlation!!! Neither in train nor in test. Very strange. We know that they are anonymized and perhaps they are decorrelated by some transformation as well. ","5d85f2e1":"### How do the scatter plots look like?","cfeeb0d7":"### Train","9f3ef3a8":"Good luck for the last days! :-)","851782e1":"* As we have much more cold-targets (zero) that hot (ones), I'm not surprised that hot targets occupy only a small part of the data per cluster. Nonetheless we can see that cluster 1 has significantly more hot targets than the others.\n* The second plot shows that most hot targets are located in cluster 1 followed by cluster 2. This confirms our assumption that the big gaussian in the middle (cluster 0) has the smallest amount of hot targets and that the small, thin side distributions are more likely to have hot targets. ","e4e0e95e":"## Sneak a peek at the data <a class=\"anchor\" id=\"data\"><\/a>","e43221d1":"Ok, 200.000 rows and 202 features. ","51694991":"Only some features are important to separate the structure of the data. ","09d807c2":"### Rounding & quantile based binning","a7cc7347":"### Test","ad1fb2fd":"The target as well as the ID-Code of a sample are 2 special variables. Consequently 200 features are left. Browsing through the columns we can see that they look really numeric. It seems that there are no counter or integer variables. In addition it looks like if there are no missing values. Let's check it out:","4becfc6c":"At a first glance this looks similar to train except from the missing target.","d64e709d":"Yields 0.662 on public LB.","99c4c958":"### Submission\n\nBefore we start, let's look at the sample submission as well:","e4bd8d89":"Ok, that's enough to start with the \"data-understanding-journey\".","ccb648d7":"### Take Away\n\n* We have to solve an imbalanced class problem. The number of customers that will not make a transaction is much higher than those that will. \n* It seem that there is no relationship of the target with the index of the train dataframe. This is more empressend by the zero targets than for the ones. \n* Take a look at the jitter plots within the violinplots. We can see that the targets look uniformly distributed over the indexes. It seems that the competitors were careful during the process of ordering the data. Once more this indicates that the data is simulated.","1a819664":"## Can we find relationships between features? <a class=\"anchor\" id=\"correlation\"><\/a>","32c09c0e":"### Take-Away\n\n* By fitting the gaussian mixture model we are maximizing the log likelihood. The higher, the better the gaussians suite to our data. As it's difficult to choose the right number of components (gaussians) I decided to use a stratified k fold of the train data. This way we can fit gaussians to a train subset, and test how big the log likelihood is on the test subset. By doing so three times for each selected component, we gain some more information about the stability of our solution. **We can see that 3 gaussians seem to be sufficient as the log likelihood values decrease with more components**. \n* This need not be true as the **solution depends on the initialization of the gaussians (the seeds I used) and with more data, the result may be different**. \n* But we can say: There are **at least 3 gaussians**. This is what we have already found by visual exploration of the data. \n* The individual score per data spot can be understood as a measure of density. If it's low, the data spot lives in a region with other data points far away. If it's high, it should have a lot of neighbors. Consequently the individual logL-score can tell us something about outliers in the data.","2b3459d2":"### Linear correlations\n\nI have already seen some correlation heatmaps in public kernels and it seems as if there is almost no correlation between features. Let's check this out by computing all correlation values and plotting the overall distribution:","781be1e9":"### New feature importances","1ef0c099":"## Gaussian Mixture Clustering  <a class=\"anchor\" id=\"clustering\"><\/a>\n\nThe majority of the data looks like a big gaussian distribution. Besides that there seems to be at least one or two more gaussians that could explain the second and third mode that we can find for important features. Let's motivate this even further by looking at scatter and kde-plots of some top-features:","66d3778e":"## Feature engineering \n\nLet's do some basic feature engineering. Perhaps it helps to improve:","4c017303":"## Our goal\n\nIn this competition we are asked to predict if a customer will make a transaction or not regardless of the amount of money transacted. Hence our goal is to solve a binary classification problem. In the data description you can see that the features given are numeric and anonymized. Furthermore the data seems to be artificial as they state that \"the data has the same structure as our real data\". \n\n### Table of contents\n\n1. [Loading packages](#load) (complete)\n2. [Sneak a peek at the data](#data) (complete)\n2. [What can we say about the target?](#target) (complete)\n3. [Can we find relationships between features?](#correlation) (complete)\n4. [Baseline submission](#baselines) (complete)\n5. [Basic feature engineering](#engineering) (complete)\n6. [Gaussian Mixture Clustering](#clustering) (complete)","b77ab087":"### Random Forest Top Features\n\nTo start easy, let's use a random forest to select top 10 features. They can serve as a starting point to discover their nature and for trying to understand the data. In addition they may yield some ideas on how to generate new features. I am going to use stratified KFold as a cross validation strategy. It's somehow arbitrary to use KFold as we don't know if we have time series data given, but it may serve as a good starting point. \n\nTo start simple I like to use a random forest that helps us to select important features. As there are no linear correlations it's a good idea to start with a nonlinear model that allows us to discover features, their importances as well as interactions. Let's start! :-)","79dc51f5":"## Baseline submissions \n\n### What score does the forest yield on public LB?","ea5c8e88":"## Loading packages <a class=\"anchor\" id=\"load\"><\/a>"}}