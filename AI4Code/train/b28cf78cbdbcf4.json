{"cell_type":{"bb328534":"code","8e1495d7":"code","135201a0":"code","ac23cc2d":"code","43f7c87d":"code","fe0ed5c3":"code","2496ea48":"code","aec4b94e":"code","0e7124f3":"code","e3b166c8":"code","5169b1e3":"code","f01f3327":"code","c61307e9":"code","5ab9e574":"code","63362467":"code","f3a84f65":"code","77fced78":"code","5823889e":"code","e59e40a9":"code","edad8e02":"code","282c0ba6":"code","36cabe33":"code","e7669ba5":"code","cbdd57f3":"code","6d38899e":"code","056a37ac":"code","f16c6082":"code","b9c250a3":"code","bd5e2774":"code","0b4ae49a":"code","4375391c":"code","f6de99f0":"code","1559c579":"code","24d943ac":"code","2b9fb03e":"code","51aef3ae":"code","eca4aa00":"code","6cad6e6e":"code","54d38109":"code","a3165502":"code","e3106a2c":"code","522f0f8c":"code","f79c2db3":"code","fbf06a95":"code","09aff602":"code","fc680bdd":"code","65deb5ce":"code","9ad46a1a":"code","f75bb330":"code","f6fd1b37":"code","212180b7":"markdown","45396375":"markdown","2051cbd1":"markdown","3c7524db":"markdown","2b56425e":"markdown","4b9d5a62":"markdown","1ac1ebb7":"markdown","364a042b":"markdown","dfc49903":"markdown","80c6f29a":"markdown","56afd044":"markdown","fbdf9ca5":"markdown","1633542c":"markdown","623f9774":"markdown","054790f3":"markdown","9629b1cb":"markdown","6136b103":"markdown","4ed7e64a":"markdown","99c6aeb0":"markdown","c24c9656":"markdown","24d5d648":"markdown","2d870b29":"markdown","32533ab1":"markdown","5e8f5679":"markdown","5cfc66c6":"markdown","842086e5":"markdown","2b64e47b":"markdown","c2680acd":"markdown","dc11a3ff":"markdown","12ef0261":"markdown","39f83c7a":"markdown","00a663d9":"markdown","eef35875":"markdown","1bba9c73":"markdown","30595bd5":"markdown","c124d1a1":"markdown","456eb19e":"markdown","dcb84ab6":"markdown","bd056d0f":"markdown","037ae04e":"markdown","7fea04e4":"markdown","047a7ac9":"markdown","07845216":"markdown","471ef2ac":"markdown","fb3fffa4":"markdown","5c640a34":"markdown","a539e3ff":"markdown","be026899":"markdown","b8c70825":"markdown","600fbf55":"markdown","777d345b":"markdown","e430ca6c":"markdown","741a1a02":"markdown","3df134c7":"markdown","226c0e72":"markdown"},"source":{"bb328534":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8e1495d7":"sales        = pd.read_csv(os.path.join(dirname, 'sales_train.csv'))\nitems        = pd.read_csv(os.path.join(dirname, 'items.csv'))\nitem_cat     = pd.read_csv(os.path.join(dirname, 'item_categories.csv'))\nshops        = pd.read_csv(os.path.join(dirname, 'shops.csv'))\ntest         = pd.read_csv(os.path.join(dirname, 'test.csv'))\nsubmission   = pd.read_csv(os.path.join(dirname, 'sample_submission.csv'))\n","135201a0":"import sklearn\nimport scipy\nimport seaborn\nimport gc\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nfrom tqdm import notebook\nfrom math import sqrt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\n\nfrom itertools import product\nimport joblib","ac23cc2d":"for p in [np, pd, scipy,sklearn, seaborn, lgb]:\n    print (p.__name__, p.__version__)","43f7c87d":"def downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df","fe0ed5c3":"def get_rmse(actual, predicted):\n    '''\n        Input: \n                actual, predicted: series object type\n        Output:\n                root mean squared error: float\n    '''\n    \n    # Select columns to downcast\n    mse = mean_squared_error(actual, predicted)\n    rmse = sqrt(mse)\n        \n    return rmse","2496ea48":"import warnings\nwarnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\nwarnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\nseaborn.set_style(\"whitegrid\")","aec4b94e":"def make_figure_and_axis(x_label, y_label, title, figsize=(10, 8)):\n    \"\"\"make a matplotlib figure\n\n    Args:\n     x_label (str): label for the x-axis\n     y_label (str): label for the y-axis\n     title (str): title for the plot\n     figsize: tuple of width, height\n    Returns:\n     tuple: figure, axis\n    \"\"\"\n    fig = plt.figure(figsize=figsize)\n    axe = fig.gca()\n    axe.set_xlabel(x_label)\n    axe.set_ylabel(y_label)\n    axe.set_title(title)\n    return fig, axe","0e7124f3":"# Trasaction dataset \"sales\" and \"test\"\n\npd.options.display.float_format = '{:,.2f}'.format\nprint(\"Sales\\n\", sales.describe(), \"\\n\")\n\nprint(\"Test\\n\",test.describe())","e3b166c8":"print(sales.item_price.value_counts())\nprint(sales[sales[\"item_price\"] == -1].item_price.value_counts())\nprint(sales[sales[\"item_price\"] == 307980].item_price.value_counts())\nitem1 = sales[sales[\"item_price\"] == -1].item_id.max()\nitem2 = sales[sales[\"item_price\"] == 307980].item_id.max()\n\nprint(\"Item: \", item1, \"\\n\", sales[sales[\"item_id\"] == item1].item_price.value_counts())\nprint(\"Item: \", item2, \"\\n\", sales[sales[\"item_id\"] == item2].item_price.value_counts())","5169b1e3":"print(test[test[\"item_id\"] == item2].item_id.count())","f01f3327":"fig=plt.plot(sales.item_price, \".\")","c61307e9":"print(sales.loc[sales[\"item_price\"] == -1, [\"item_id\", \"item_price\"]]) \nsales.loc[sales[\"item_price\"] == -1, [\"item_price\"]] = sales[sales[\"item_id\"] == item1].item_price.mean()\nprint(sales.loc[sales[\"item_price\"] == -1, [\"item_id\"]]) ","5ab9e574":"sales = sales[sales.item_price != 307980]\nprint(sales[\"item_price\"].describe())","63362467":"fig=plt.plot(sales.item_cnt_day, \".\")","f3a84f65":"plt.hist(sales.date_block_num, sales.date_block_num.max())","77fced78":"plt.hist(sales.shop_id, len(sales.shop_id.unique()))","5823889e":"plt.hist(test.shop_id, len(test.shop_id.unique()))","e59e40a9":"sales_test = sales.copy()\nsales_test[\"item_revenue\"] = sales_test.item_price*sales_test.item_cnt_day \n\nsales_test[\"day\"] = sales_test.date.str[0:2]\nsales_test[\"month\"] = sales_test.date.str[3:5]\nsales_test[\"year\"] = sales_test.date.str[6:10]\n\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\ngrid = [] \nfor block_num in sales_test['date_block_num'].unique():\n    cur_shops = sales_test.loc[sales_test['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales_test.loc[sales_test['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# Groupby data to get shop-item-month aggregates\ngb = sales_test.groupby(index_cols,as_index=False).item_cnt_day.sum()\ngb.columns = index_cols + [\"month_qty\"]\n\n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\nall_data[\"month\"]=(all_data[\"date_block_num\"]+1)%12\nall_data[\"year\"]=(all_data[\"date_block_num\"]+1)\/\/12\nall_data.loc[all_data[\"year\"] == 0, [\"year\"]] = 2013\nall_data.loc[all_data[\"year\"] == 1, [\"year\"]] = 2014\nall_data.loc[all_data[\"year\"] == 2, [\"year\"]] = 2015\n\nitem_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\nall_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n\nall_data.head()\n\n# Downcast dtypes from 64 to 32 bit to save memory\nall_data = downcast_dtypes(all_data)\ndel grid, gb \ngc.collect();","edad8e02":"grid = seaborn.catplot(x=\"date_block_num\", data=all_data, kind=\"count\")","282c0ba6":"date_group = all_data.groupby(\"date_block_num\")\nsummed = date_group.sum()\nsummed = summed.reset_index()\n\ngrid = seaborn.relplot(x=\"date_block_num\", y=\"month_qty\", data=summed, kind=\"line\")","36cabe33":"all_data[\"Date\"] = all_data.year.apply(str) + \"-\" + all_data.month.apply(str)\nmonth_grouped = all_data.groupby(\"Date\")\nmonth_summed = month_grouped.sum().reset_index()\n\ntop_two = month_summed.sort_values(\"month_qty\", ascending=False)[:2]\n\ngrid = seaborn.relplot(x=\"Date\", y=\"month_qty\", data=month_summed, kind=\"line\")\n","e7669ba5":"print(top_two[[\"Date\", \"month_qty\"]])","cbdd57f3":"group = all_data.groupby(\"shop_id\").sum().reset_index().sort_values(\"month_qty\")\n\ngrid = seaborn.relplot(x=\"shop_id\", y=\"month_qty\", data=group)","6d38899e":"group = all_data.groupby(\"item_category_id\").sum().reset_index()\n\ngrid = seaborn.relplot(x=\"item_category_id\", y=\"month_qty\", data=group)","056a37ac":"category_group = all_data.groupby([\"Date\", \"item_category_id\"]).sum().reset_index()\nbiggest = category_group.iloc[category_group[\"month_qty\"].idxmax()]\n\nbiggest_category = category_group[category_group.item_category_id == biggest.item_category_id]\n\ngrid = seaborn.relplot(x=\"Date\", y=\"month_qty\", data=biggest_category, kind=\"line\")","f16c6082":"print(\"Biggest category\\n\", biggest)","b9c250a3":"sales_test = sales.copy()\n# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops\/items combinations from that month\ngrid = [] \nfor block_num in sales_test['date_block_num'].unique():\n    cur_shops = sales_test.loc[sales_test['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales_test.loc[sales_test['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# Groupby data to get shop-item-month aggregates\ngb = sales_test.groupby(index_cols,as_index=False).item_cnt_day.sum()\ngb.columns = index_cols + [\"target\"]\n\n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n# Same as above but with shop-month aggregates\n\n#gb = sales_test.groupby(['shop_id', 'date_block_num'],as_index=False).item_cnt_day.sum()\n#gb.columns = [\"shop_id\", \"date_block_num\", \"target_shop\"]\n\ngb = sales_test.groupby(['shop_id', 'date_block_num'],as_index=False).agg({\"item_cnt_day\":\"sum\",\"item_price\":\"max\"})\ngb.columns = [\"shop_id\", \"date_block_num\", \"target_shop\", \"max_price\"]\n\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n# Same as above but with item-month aggregates\ngb = sales_test.groupby(['item_id', 'date_block_num'],as_index=False).item_cnt_day.sum()\ngb.columns = [\"item_id\", \"date_block_num\", \"target_item\"]\n\nall_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\n# Downcast dtypes from 64 to 32 bit to save memory\nall_data = downcast_dtypes(all_data)\ndel grid, gb \ngc.collect();","bd5e2774":"# List of columns that we will use to create lags\ncols_to_rename = list(all_data.columns.difference(index_cols)) \n\nshift_range = [1, 2, 3, 4, 5, 12]\n\nfor month_shift in notebook.tqdm(shift_range):\n    train_shift = all_data[index_cols + cols_to_rename].copy()\n    \n    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n    train_shift = train_shift.rename(columns=foo)\n\n    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\ndel train_shift\n\n# Don't use old data from previous month block\n#all_data = all_data[all_data['date_block_num'] >= 12] \n\n# I am going to use whole data available for testing\n\n# List of all lagged features\nfit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n\n#This line is needed with 2 \u00b4date_block_num\u00b4  -- was used to fix error when you choose only 2 items witnin range\n#fit_cols += [col for col in all_data.columns if col[-2] in [str(item) for item in shift_range]]\n\n# We will drop these at fitting stage\nto_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n\n# Category for each item\nitem_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\nall_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n\nall_data = downcast_dtypes(all_data)\ngc.collect();","0b4ae49a":"shop_first_block = sales_test.groupby(\"shop_id\").date_block_num.min()\nall_data[\"shop_1st\"] = all_data[\"shop_id\"].map(shop_first_block)\nall_data[\"shop_1st\"] = all_data[\"date_block_num\"] - all_data[\"shop_1st\"]\n\nitem_first_block = sales_test.groupby(\"item_id\").date_block_num.min()\nall_data[\"item_1st\"] = all_data[\"item_id\"].map(item_first_block)\nall_data[\"item_1st\"] = all_data[\"date_block_num\"] - all_data[\"item_1st\"]\n\nall_data.shop_1st.value_counts()","4375391c":"all_data[\"month\"]=(all_data[\"date_block_num\"]+1)%12\nall_data.head(5)","f6de99f0":"dates = all_data['date_block_num']\n\nlast_block = dates.max()   #This will be our validation set\n\nprint('Test `date_block_num` is %d' % last_block)","1559c579":"dates_train = dates[dates <  last_block]\ndates_test  = dates[dates == last_block]\n\nX_train = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)\nX_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n\ny_train = all_data.loc[dates <  last_block, 'target'].values.clip(0,20)\ny_test =  all_data.loc[dates == last_block, 'target'].values.clip(0,20)\n\nprint(X_train.head())","24d943ac":"lr = LinearRegression()\nlr.fit(X_train.values, y_train)\npred_lr = lr.predict(X_test.values).clip(0,20)","2b9fb03e":"rmse_lr = get_rmse(y_test, pred_lr)\nprint(\"rmse: \", rmse_lr)\nprint(\"r2 train: \", r2_score(y_train, lr.predict(X_train.values).clip(0,20)))\nprint(\"r2 test: \", r2_score(y_test, pred_lr))","51aef3ae":"filename = 'lr_model_l1.sav'\njoblib.dump(lr, filename)","eca4aa00":"reg = make_pipeline(StandardScaler(), SGDRegressor(loss=\"epsilon_insensitive\"))\nreg.fit(X_train.values, y_train)\npred_reg = reg.predict(X_test.values).clip(0,20)","6cad6e6e":"rmse_reg = get_rmse(y_test, pred_reg)\nprint(\"rmse: \", rmse_reg)\nprint(\"r2 train: \", r2_score(y_train, reg.predict(X_train.values).clip(0,20)))\nprint(\"r2 test: \", r2_score(y_test, pred_reg))","54d38109":"filename = 'reg_model_l1.sav'\njoblib.dump(reg, filename)","a3165502":"lgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0 \n              }\n\nlgb1 = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)    \npred_lgb = lgb1.predict(X_test).clip(0,20)","e3106a2c":"rmse_lgb = get_rmse(y_test, pred_lgb)\nprint(\"rmse: \", rmse_lgb)\nprint(\"r2 train: \", r2_score(y_train, lgb1.predict(X_train.values).clip(0,20)))\nprint(\"r2 test: \", r2_score(y_test, pred_lgb))","522f0f8c":"filename = 'lgb003_model_l1.sav'\njoblib.dump(lgb1, filename)","f79c2db3":"varimp = lgb1.feature_importance()\nnames = X_train.columns.values\nvar_array = pd.DataFrame(list(zip(names, varimp)))\nvar_array.columns=[\"names\", \"varimp\"]\nvar_array.plot(kind=\"bar\", x=\"names\", y=\"varimp\")","fbf06a95":"X_test_level2 = np.c_[pred_reg, pred_lgb]\n\nprint(X_test_level2.shape)","09aff602":"months_f=np.array([i for i in range(11,last_block)])\ndates_train_level2 = dates_train[dates_train.isin(months_f)]\n\n# That is how we get target for the 2nd level dataset\ny_train_level2 = y_train[dates_train.isin(months_f)]\n\nprint('shape of y_train_level2: {}'.format(y_train_level2.shape))","fc680bdd":"# And here we create 2nd level feature matrix, init it with zeros first\nX_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n\n# Now fill `X_train_level2` with metafeatures\nfor cur_block_num in notebook.tqdm(months_f):\n    \n    print(cur_block_num, end='')\n    \n    '''\n        1. Split `X_train` into parts\n           Remember, that corresponding dates are stored in `dates_train` \n        2. Fit linear regression \n        3. Fit LightGBM and put predictions          \n        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n           You can use `dates_train_level2` for it\n           Make sure the order of the meta-features is the same as in `X_test_level2`\n    '''      \n    \n    #  YOUR CODE GOES HERE\n    X_train_block = all_data.loc[dates < cur_block_num].drop(to_drop_cols, axis=1)\n    X_test_block = all_data.loc[dates == cur_block_num].drop(to_drop_cols, axis=1)\n    \n    y_train_block = all_data.loc[dates <  cur_block_num, 'target'].values.clip(0,20)\n    y_test_block = all_data.loc[dates == cur_block_num, 'target'].values.clip(0,20)\n    \n    print(':  X_train_block.shape={}'.format(X_train_block.shape), end='')\n    print(',  X_test_block.shape={}'.format(X_test_block.shape), end='')\n    print(',   Total Size={}'.format(X_train_block.shape[0] + X_test_block.shape[0]), end='')\n    print()\n    \n    reg.fit(X_train_block, y_train_block)\n    X_train_level2[dates_train_level2 == cur_block_num, 0] = reg.predict(X_test_block.values).clip(0,20)\n    \n    model = lgb.train(lgb_params, lgb.Dataset(X_train_block, label=y_train_block), 100)\n    X_train_level2[dates_train_level2 == cur_block_num, 1] = model.predict(X_test_block).clip(0,20)\n    \n    ","65deb5ce":"lr.fit(X_train_level2, y_train_level2)\n\nprint('Coefficient:            {}'.format(lr.coef_))\nprint('Normalized Coefficient: {}'.format(lr.coef_ \/ lr.coef_.sum()))","9ad46a1a":"test_preds_stacking_lr = lr.predict(np.vstack((pred_lr, pred_lgb)).T).clip(0,20)\ntest_preds_stacking_lr.shape","f75bb330":"rmse_lr_stack = get_rmse(y_test, test_preds_stacking_lr)\nprint(\"rmse: \", rmse_lr_stack)\nprint(\"r2 train: \", r2_score(y_train_level2, lr.predict(X_train_level2).clip(0,20)))\nprint(\"r2 test: \", r2_score(y_test, test_preds_stacking_lr))","f6fd1b37":"filename = 'stack_model_lr_l2.sav'\njoblib.dump(lr, filename)","212180b7":"Adding Mean Encoding for shop_id, item_id and item_category_id.  \n\nIn this case I test applying expanding mean scheme for regualization purpose but model metrics was not favorable, then for final version was omitted.","45396375":"SGD Regressor","2051cbd1":"Shop analysis","3c7524db":"We tested with whole types and parameters of SGDRegressor in order to obtain better performance than linear regression.\nI tried other models like SVR from svm and Logistic Regression but none could improve linear regression.","2b56425e":"For memory issues on this platform, I have to split the Final Project in 2 parts:\n\nPart 1 (This kernel)\n* EDA\n* Features \n* Training models for validation\n\nPart 2\n* Training models for predictions\n* Generating submission file","4b9d5a62":"Serialize","1ac1ebb7":"This distribution show that test data was sintetic, hardly any validation set sustracted from sales data will be aproximated to this.","364a042b":"Plot feature importance","dfc49903":"Train 2nd level model, Linear regression in this case","80c6f29a":"Now we split the data as explained but we must delete from the trainset all columns related with our target to predict.","56afd044":"I am going to use \"KFold scheme in time series\" to validate 2nd. level model from stacking procedure.","fbdf9ca5":"Once again its confirms sales decreasing during the time","1633542c":"Split trasaction dataframes into train and test, taking in a count that this is a regression over the time then prediction are going to be done by month, then we reserve the last block\/month to be the validation set (test set).","623f9774":"Serialize","054790f3":"Again, I used routine from programming assignment from course, I did some adjusts in params and code to be usable here.","9629b1cb":"Test level 2 are taking directly from predictions of two models L1","6136b103":"Exploring shop_id distribution beetwen sales and test data","4ed7e64a":"Function to downsize types from 64 to 32 - took from luliu31415926 on github","99c6aeb0":"Serialize the model","c24c9656":"Actions, replace by mean in the first case and drop unique row for special item just to not distort de model","24d5d648":"See item sold per month","2d870b29":"Look for item_id with price outlier in test set, to determine if it is possible to drop","32533ab1":"Function to calc RMSE","5e8f5679":"Confirm outlayers with plot","5cfc66c6":"Calc RMSE an R2 for model Light GBM.  This metrics were the key for optimize learning rate and iterations, we test over 20 combinations to obtain better performance.","842086e5":"Load Python libraries","2b64e47b":"Now I am going to add month to grid based on date_block_num, this for include in this new feature seasonality that its represent the month in the year - this seasonality for sales was viewed in a histogram data.","c2680acd":"Now we are going to add the most important features, lags for target, item, shop and price -- adapted from \"Ensembling implementation\" programming assigment.  I tested manually different lags ranges and at the end best metrics was obtained selecting 1:5 and 12 range.","dc11a3ff":"Look for taget variable trends in time","12ef0261":"*** TRAINING STAGE ***","39f83c7a":"Function to plot data - took from Cloistered Monkey","00a663d9":"Here we are going to add new features related with first time when appear in training set item_id and shop_id. Then I substract from date_block_num in each row to mean seniority in training set.","eef35875":"Item category analysis","1bba9c73":"**EDA STAGE**","30595bd5":"BEGINS 2nd LEVEL FOR ENSEMBLING","c124d1a1":"Then we must encoding month, the same as before, because this columns is likehood to categorical feature.  At the end was ommitted for model metrics results.","456eb19e":"Continue with \"Final Project CUribe.co Part 2\/2\" kernel \n\nhttps:\/\/www.kaggle.com\/curibe10\/final-project-curibe-co-part-2-2","dcb84ab6":"# Mean Encoding for item_id\ncumsum = all_data.groupby('item_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_id').cumcount()\nall_data['item_target_enc'] = cumsum \/ cumcnt\nall_data['item_target_enc'].fillna(0.3343, inplace=True) \n\n# Mean Encoding for shop_id\ncumsum = all_data.groupby('shop_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('shop_id').cumcount()\nall_data['shop_target_enc'] = cumsum \/ cumcnt\nall_data['shop_target_enc'].fillna(0.3343, inplace=True) \n\n# Mean Encoding for item_category_id\ncumsum = all_data.groupby('item_category_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_category_id').cumcount()\nall_data['category_target_enc'] = cumsum \/ cumcnt\nall_data['category_target_enc'].fillna(0.3343, inplace=True) \n\nall_data = all_data.drop([\"item_id\", \"shop_id\", \"item_category_id\"], axis=1)  # \"item_category_id\"\n\nall_data = downcast_dtypes(all_data)\ndel cumsum, cumcnt\ngc.collect();","bd056d0f":"**Linear Regression**","037ae04e":"Calc RMSE for model SGD Regressor","7fea04e4":"APPLYING MODELS OF THE 1st LEVEL OF ENSEMBLING","047a7ac9":"Looking for outliers","07845216":"Here I used the code from \"Ensembling implementation\" programming assigment, from the course, to generate grid matrix of features.  We did some adjust to run the code and add item_price as a new feature.","471ef2ac":"LightGBM Gradient Boosting with decision trees.","fb3fffa4":"Calc RMSE and R2 for model Linear Regression. this will be used manually to select best features conformation until obtain best R2.","5c640a34":"List Versions used","a539e3ff":"** END OF VALIDATION MODEL ***","be026899":"You can observe feature importance of model, IDs are most important, then season feature (month of the year), then lag 1 pack and seniority measure for shop and item.","b8c70825":"Category most frecuent sold","600fbf55":"# Mean Encoding for month\ncumsum = all_data.groupby('month')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('month').cumcount()\nall_data['month_enc'] = cumsum \/ cumcnt\nall_data['month_enc'].fillna(0.3343, inplace=True) \n\nall_data = all_data.drop([\"month\"], axis=1)\n\nall_data = downcast_dtypes(all_data)\ndel cumsum, cumcnt\ngc.collect();","777d345b":"Basic anlysis of data - complementing those posted by Kaggle Data.","e430ca6c":"Calc RMSE an R2 for this Stack Model - level 2","741a1a02":"Serialize","3df134c7":"Prepare data for specific analysis based on competition's target","226c0e72":"I don't take any action in this case"}}