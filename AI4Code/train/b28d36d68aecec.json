{"cell_type":{"4190e770":"code","2e4c3949":"code","d89b5e59":"code","1346dc78":"code","1a16e93a":"code","03653158":"code","77d38f91":"code","0eebc033":"code","7e0a5516":"code","cdee29c9":"code","b2080095":"code","5a889d30":"code","587bdc79":"code","6fda5364":"code","7da8691a":"code","09323ee4":"code","03461d37":"code","cde63439":"code","fe075d8d":"code","b2d459cb":"code","905b7665":"code","9d0bfd0e":"code","f731f8ba":"code","21e2ee91":"code","71fbc1e0":"code","d0d32b71":"code","46c13e19":"code","bcfc623f":"code","cf21ddf5":"code","b139fb73":"code","8f18a73c":"code","8ad32d1e":"code","67d5f32a":"code","3ece7b32":"code","93fa12ce":"markdown","55c87645":"markdown","94484913":"markdown","4b0aae77":"markdown","26406d64":"markdown","4228da82":"markdown","d414f6cc":"markdown","b710c7a2":"markdown","61f68d89":"markdown","1c9b9413":"markdown","8fcea74f":"markdown"},"source":{"4190e770":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\nimport xgboost as xgb\n%matplotlib inline","2e4c3949":"# Read Dataset\n\ndf = pd.read_csv('..\/input\/Car_sales.csv')\ndf = df.dropna()\nprint(df.shape)\ndf.head()","d89b5e59":"# Change 'Passenger' to binary indicators\n\ndf['Passenger'] = (df['Vehicle_type']=='Passenger')\ndf['Car'] = (df['Vehicle_type']!='Passenger')\ndf.drop('Vehicle_type', inplace=True, axis=1)\ndf.head()","1346dc78":"df.corr()","1a16e93a":"X_train, X_test, y_train, y_test = train_test_split(df.drop('Sales_in_thousands', axis=1), df['Sales_in_thousands'], random_state = 0)\nlinreg = LinearRegression().fit(X_train, y_train)\n\nprint('linear model coeff (w): {}'.format(linreg.coef_))\nprint('linear model intercept (b): {:.3f}'.format(linreg.intercept_))\nprint('R-squared score (training): {:.3f}'.format(linreg.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(linreg.score(X_test, y_test)))","03653158":"knn = KNeighborsRegressor().fit(X_train, y_train)\nprint('R-squared score (training): {:.3f}'.format(knn.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(knn.score(X_test, y_test)))","77d38f91":"grid_values = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\ngrid_knn_mse = GridSearchCV(knn, param_grid = grid_values, scoring = 'neg_mean_squared_error', cv=5, iid=False)\ngrid_knn_mse.fit(X_train, y_train)\n\nprint('Grid best parameter (min. MSE): ', grid_knn_mse.best_params_)\nprint('Grid best score (MSE): ', grid_knn_mse.best_score_)","0eebc033":"knn = grid_knn_mse.best_estimator_\nprint('R-squared score (training): {:.3f}'.format(knn.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(knn.score(X_test, y_test)))","7e0a5516":"svm = SVR(gamma='scale').fit(X_train, y_train)\nprint('R-squared score (training): {:.3f}'.format(svm.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(svm.score(X_test, y_test)))","cdee29c9":"grid_values = {'gamma': [0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10],\\\n               'C': [0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]}\ngrid_svm_mse = GridSearchCV(svm, param_grid = grid_values, scoring = 'neg_mean_squared_error', cv=5, iid=False)\ngrid_svm_mse.fit(X_train, y_train)\n\nprint('Grid best parameter (min. MSE): ', grid_svm_mse.best_params_)\nprint('Grid best score (MSE): ', grid_svm_mse.best_score_)","b2080095":"svm = grid_svm_mse.best_estimator_\nprint('R-squared score (training): {:.3f}'.format(svm.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(svm.score(X_test, y_test)))","5a889d30":"dt = DecisionTreeRegressor().fit(X_train, y_train)\nprint('R-squared score (training): {:.3f}'.format(dt.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(dt.score(X_test, y_test)))","587bdc79":"plt.figure(figsize=(10,10), dpi=80)\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame(feature_names, columns=['features'])\nfeature_importance['importance'] = pd.DataFrame(dt.feature_importances_)\nfeature_importance.sort_values(by='importance', ascending=False, inplace=True)\nfeature_importance.reset_index(drop=True, inplace=True)\nplt.barh(feature_importance['features'], feature_importance['importance'])\nplt.xlabel('Feature importance')\nplt.ylabel('Feature name')\nplt.show()","6fda5364":"grid_values = {'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\\\n               'min_samples_split': [2, 4, 8, 16, 32, 64, 100],\\\n               'min_samples_leaf': [1, 2, 4, 8, 16, 32, 64, 100]}\ngrid_dt_mse = GridSearchCV(dt, param_grid = grid_values, scoring = 'neg_mean_squared_error', cv=5, iid=False)\ngrid_dt_mse.fit(X_train, y_train)\n\nprint('Grid best parameter (min. MSE): ', grid_dt_mse.best_params_)\nprint('Grid best score (MSE): ', grid_dt_mse.best_score_)","7da8691a":"dt = grid_dt_mse.best_estimator_\nprint('R-squared score (training): {:.3f}'.format(dt.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(dt.score(X_test, y_test)))","09323ee4":"plt.figure(figsize=(10,10), dpi=80)\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame(feature_names, columns=['features'])\nfeature_importance['importance'] = pd.DataFrame(dt.feature_importances_)\nfeature_importance.sort_values(by='importance', ascending=False, inplace=True)\nfeature_importance.reset_index(drop=True, inplace=True)\nplt.barh(feature_importance['features'], feature_importance['importance'])\nplt.xlabel('Feature importance')\nplt.ylabel('Feature name')\nplt.show()","03461d37":"rf = RandomForestRegressor(n_estimators=100).fit(X_train, y_train)\nprint('R-squared score (training): {:.3f}'.format(rf.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(rf.score(X_test, y_test)))","cde63439":"grid_values = {'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\\\n               'min_samples_split': [2, 4, 8, 16, 32, 64, 100],\\\n               'min_samples_leaf': [1, 2, 4, 8, 16, 32, 64, 100]}\ngrid_rf_mse = GridSearchCV(rf, param_grid = grid_values, scoring = 'neg_mean_squared_error', cv=5, iid=False)\ngrid_rf_mse.fit(X_train, y_train)\n\nprint('Grid best parameter (min. MSE): ', grid_rf_mse.best_params_)\nprint('Grid best score (MSE): ', grid_rf_mse.best_score_)","fe075d8d":"rf = grid_rf_mse.best_estimator_\nprint('R-squared score (training): {:.3f}'.format(rf.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(rf.score(X_test, y_test)))","b2d459cb":"plt.figure(figsize=(10,10), dpi=80)\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame(feature_names, columns=['features'])\nfeature_importance['importance'] = pd.DataFrame(rf.feature_importances_)\nfeature_importance.sort_values(by='importance', ascending=False, inplace=True)\nfeature_importance.reset_index(drop=True, inplace=True)\nplt.barh(feature_importance['features'], feature_importance['importance'])\nplt.xlabel('Feature importance')\nplt.ylabel('Feature name')\nplt.show()","905b7665":"nn = MLPRegressor().fit(X_train, y_train)\nprint('R-squared score (training): {:.3f}'.format(nn.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(nn.score(X_test, y_test)))","9d0bfd0e":"grid_values = {'hidden_layer_sizes': np.arange(1, 201),\\\n               'learning_rate': ['constant', 'invscaling', 'adaptive']}\ngrid_nn_mse = GridSearchCV(nn, param_grid = grid_values, scoring = 'neg_mean_squared_error', cv=5, iid=False)\ngrid_nn_mse.fit(X_train, y_train)\n\nprint('Grid best parameter (min. MSE): ', grid_nn_mse.best_params_)\nprint('Grid best score (MSE): ', grid_nn_mse.best_score_)","f731f8ba":"nn = grid_nn_mse.best_estimator_\nprint('R-squared score (training): {:.3f}'.format(nn.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(nn.score(X_test, y_test)))","21e2ee91":"xg_reg = xgb.XGBRegressor().fit(X_train, y_train)\nprint('R-squared score (training): {:.3f}'.format(xg_reg.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(xg_reg.score(X_test, y_test)))","71fbc1e0":"grid_values = {'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\\\n               'min_child_weight': [1, 2, 4, 8, 16, 32, 64, 100]}\ngrid_xgb_mse = GridSearchCV(xg_reg, param_grid = grid_values, scoring = 'neg_mean_squared_error', cv=5, iid=False)\ngrid_xgb_mse.fit(X_train, y_train)\n\nprint('Grid best parameter (min. MSE): ', grid_xgb_mse.best_params_)\nprint('Grid best score (MSE): ', grid_xgb_mse.best_score_)","d0d32b71":"xg_reg = grid_xgb_mse.best_estimator_\nprint('R-squared score (training): {:.3f}'.format(xg_reg.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(xg_reg.score(X_test, y_test)))","46c13e19":"gb = GradientBoostingRegressor().fit(X_train, y_train)\nprint('R-squared score (training): {:.3f}'.format(gb.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(gb.score(X_test, y_test)))","bcfc623f":"grid_values = {'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\\\n               'min_samples_split': [2, 4, 8, 16, 32, 64, 100],\\\n               'min_samples_leaf': [1, 2, 4, 8, 16, 32, 64, 100]}\ngrid_gb_mse = GridSearchCV(gb, param_grid = grid_values, scoring = 'neg_mean_squared_error', cv=5, iid=False)\ngrid_gb_mse.fit(X_train, y_train)\n\nprint('Grid best parameter (min. MSE): ', grid_gb_mse.best_params_)\nprint('Grid best score (MSE): ', grid_gb_mse.best_score_)","cf21ddf5":"gb = grid_gb_mse.best_estimator_\nprint('R-squared score (training): {:.3f}'.format(gb.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(gb.score(X_test, y_test)))","b139fb73":"plt.figure(figsize=(10,10), dpi=80)\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame(feature_names, columns=['features'])\nfeature_importance['importance'] = pd.DataFrame(gb.feature_importances_)\nfeature_importance.sort_values(by='importance', ascending=False, inplace=True)\nfeature_importance.reset_index(drop=True, inplace=True)\nplt.barh(feature_importance['features'], feature_importance['importance'])\nplt.xlabel('Feature importance')\nplt.ylabel('Feature name')\nplt.show()","8f18a73c":"stacking = pd.DataFrame(y_train)\nmodels = [linreg, knn, svm, dt, nn, rf, gb, xg_reg]\nfor i in models:\n    stacking[str(i)] = i.predict(X_train)\nstacking.columns = ['Sales_in_thousands', 'LinReg', 'kNN', 'SVM', 'Decision Tree', 'Neural Network', 'Random Forest', 'Gradient Boosting', 'XGB']\nstacking.sort_values('Sales_in_thousands', inplace=True)\nlinreg_s = LinearRegression().fit(stacking.drop('Sales_in_thousands', axis=1), stacking['Sales_in_thousands'])\ncoef = linreg_s.coef_\nprint(coef)","8ad32d1e":"prediction = df.iloc[:, :1]\nmodels = [linreg, knn, svm, dt, nn, rf, gb, xg_reg]\nfor i in models:\n    prediction[str(i)] = i.predict(df.drop('Sales_in_thousands', axis=1))\nprediction.columns = ['Sales_in_thousands', 'LinReg', 'kNN', 'SVM', 'Decision Tree', 'Neural Network', 'Random Forest', 'Gradient Boosting', 'XGB']\nprediction['Stacking'] = coef[0]*prediction['LinReg'] + coef[1]*prediction['kNN'] + coef[2]*prediction['SVM']\\\n                       + coef[3]*prediction['Decision Tree'] + coef[4]*prediction['Neural Network']\\\n                       + coef[5]*prediction['Random Forest'] + coef[6]*prediction['Gradient Boosting']\\\n                       + coef[7]*prediction['XGB']\nprediction.sort_values('Sales_in_thousands', inplace=True)\nprediction.head()","67d5f32a":"score = []\nfor i in range(1, 10):\n    score += [((prediction.iloc[:, i] - prediction['Sales_in_thousands'])**2).mean()]\nbest_model = score.index(min(score))\nprint(score)","3ece7b32":"fig = plt.figure(figsize=(10, 6))\nplt.plot(np.arange(prediction.shape[0]), prediction['Sales_in_thousands'], label='True Value', linewidth=3)\nfor i in range(len(prediction.columns[1:])):\n    if i == best_model:\n        plt.plot(np.arange(prediction.shape[0]), prediction.iloc[:, i+1], label=prediction.columns[i+1]+'('+str(round(score[i], 2))+')', linewidth=3)\n    else:\n        plt.plot(np.arange(prediction.shape[0]), prediction.iloc[:, i+1], label=prediction.columns[i+1]+'('+str(round(score[i], 2))+')')\nplt.title('Prediction Results', fontsize=16)\nplt.xticks(fontsize=13)\nplt.xlabel('Instances', fontsize=16)\nplt.yticks(fontsize=13)\nplt.ylabel('Sales in thousands', fontsize=16)\nplt.xlim(0, prediction.shape[0]-1)\nplt.grid()\nplt.legend(loc=2)\nplt.show()","93fa12ce":"**Random Forest Regression**","55c87645":"**XGB Gradient Boosting Regression**","94484913":"---\n\nHere is the prediction results.","4b0aae77":"**Support Vector Machine Regression**","26406d64":"**sklearn Gradient Boosting Regression**","4228da82":"**Neural Network Regression**","d414f6cc":"**kNN Regression**","b710c7a2":"Baseline Model **LinearRegression**","61f68d89":"**Stacking (Experiment)**","1c9b9413":"**Decision Tree Regression**","8fcea74f":"The performance of our stacking model is not realistic, since this graph contains both the training set and the test set instances."}}