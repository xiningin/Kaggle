{"cell_type":{"68b95d65":"code","531af861":"code","1bf4f4ec":"code","b67b05df":"code","37b7adba":"code","c9dd4ccf":"code","d8bdf630":"code","b14c5f55":"code","837681d2":"code","5344b4e5":"code","293e9da9":"code","e3c671d1":"code","47ad54c2":"code","854dab74":"code","c545902d":"code","c80868e4":"code","d0f92c91":"code","caba6f60":"code","9e9403ad":"code","c6348196":"code","4923bdfa":"code","f02a9564":"code","f6e2e8b7":"code","3cd90a60":"code","9bd30626":"code","25f5dc6e":"code","715b826f":"code","02b3414c":"code","04b5b36c":"code","790c409d":"code","78dc05f5":"code","f869a6ad":"code","d69b06eb":"code","2abb265b":"code","e238cdf3":"code","a1266f90":"code","49b69a99":"code","c504f04f":"code","9628e9f9":"code","6e78cc2d":"code","519a262f":"code","cece64b1":"code","c42eb7f5":"code","fd590801":"code","81b3e8dd":"code","b724d69e":"code","900f7266":"code","c7c445e9":"code","d83c1c0c":"code","97650dbc":"code","1ba8020b":"code","7c887eae":"code","28e08cc0":"code","7e5dfc61":"code","4b5ec50d":"code","4ce37cc1":"markdown","07d48c4f":"markdown","51cc7dc6":"markdown","9a0349bd":"markdown","07c0f01f":"markdown","4d256ae6":"markdown","c1336b20":"markdown","59026b05":"markdown","9dd80844":"markdown","e525aee6":"markdown","aca801df":"markdown","4b915246":"markdown","fbaf8698":"markdown","8ed8bea4":"markdown","7e1bafbc":"markdown","8dff2739":"markdown","bae62005":"markdown","fda9b690":"markdown","bd002f8f":"markdown","e0f24ce8":"markdown","74949ed2":"markdown","591e8a46":"markdown","87c53157":"markdown","5b28732f":"markdown","917c48b2":"markdown","62cb8c72":"markdown","42f6a191":"markdown","15179882":"markdown","ae73c971":"markdown","24c1c198":"markdown","fa3a7897":"markdown","0240fe83":"markdown","c3962d8f":"markdown","719638a1":"markdown","e7b8ee6d":"markdown","478e68d8":"markdown","1b46009a":"markdown","05738c3e":"markdown","1ff56f6e":"markdown","94f524d7":"markdown","65b1ab0c":"markdown","3717dca0":"markdown","2a9c4597":"markdown","1b0aad80":"markdown","ae5b6b73":"markdown"},"source":{"68b95d65":"import numpy as np\nimport pandas as pd\npd.set_option(\"max_colwidth\", -1)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n\nimport nltk\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\nfrom nltk.tokenize import word_tokenize\n\nimport tensorflow as tf\nimport keras\nfrom keras import preprocessing\n\nfrom sklearn.model_selection import KFold\n\nimport string\nimport time\nimport os ,re\nimport functools\nfrom collections import Counter\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","531af861":"train_tweet = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_tweet = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\n\n# I prefer indexing train and test with dictionary\n# rather than define train and tweet one by one\ndf_map = {\"train\": train_tweet, \"test\": test_tweet}","1bf4f4ec":"def get_df_info(df_map):\n    for key, df in df_map.items():\n        print(\"{} dataframe info: \".format(key))\n        print(\"-\" * 30)\n        print(df.info(), end=\"\\n\\n\")\n        \n        \ndef check_null_values(df_map):\n    for key, df in df_map.items():\n        print(\"{} dataframe missing values: \".format(key))\n        print(\"-\" * 30)\n        print(df.isnull().sum(axis=0), end=\"\\n\\n\")","b67b05df":"get_df_info(df_map)","37b7adba":"check_null_values(df_map)","c9dd4ccf":"def plot_class_count(train):\n    \n    fig, ax = plt.subplots(figsize=(5, 5))\n    plt.suptitle(\"Class count\")\n    sns.countplot(x=\"target\", data=train, ax=ax)\n        \n    \nplot_class_count(train_tweet)","d8bdf630":"def word_list(sentences):\n    result = [word.lower() for sentence in sentences \n                           for word in word_tokenize(sentence)]\n    return result\n\ndef create_corpus(train_tweet):\n    df = train_tweet.copy()\n        \n    # convert pandas series => list of words\n    class_0_corpus = word_list(df.query(\"target == 0\")[\"text\"]) \n    class_1_corpus = word_list(df.query(\"target == 1\")[\"text\"])\n\n    return class_0_corpus, class_1_corpus","b14c5f55":"# Define corpus, punc_list and stopwords\nclass_0_corpus, class_1_corpus = create_corpus(train_tweet)\npunc_list = [punct for punct in string.punctuation]\nstop = list(stop)","837681d2":"def sum_all():\n    start = time.time()\n    \n    result = 0\n    for num in range(100):\n        result += num\n    \n    end = time.time()\n    print(\"Elapsed time: {:4f} sec\".format(end - start))\n    print(\"The sum is {}\".format(result))\n\n\nsum_all()","5344b4e5":"def mul_all():\n    start = time.time()\n    \n    result = 0\n    for num in range(100):\n        result *= num\n    \n    end = time.time()\n    print(\"Elapsed time: {:4f} sec\".format(end - start))\n    print(\"The product is {}\".format(result))\n    \nmul_all()","293e9da9":"# Create the wrapper function first, \n# :parameter: func here is our base function like: sum_all and mul_all\ndef simple_decorator(func):\n    \n    # 2. Our function will goes inside here\n    def decorated():\n        start = time.time()  # 3. Timer start\n        result = func()      # 4. Function Executed\n        end = time.time()    # 5. Timer end\n        \n        print(\"{} function\".format(func.__name__))\n        print(\"Elapsed time: {:4f} sec\".format(end - start))\n        print(\"The result is {}\".format(result))\n        print()\n        \n        return result # 6. (Optional) Return the function result here\n    \n    # 1. This function will be called first, then\n    return decorated\n\n","e3c671d1":"@simple_decorator\ndef sum_all():\n    result = 0\n    for x in range(100):\n        result += x\n    return result\n\n@simple_decorator\ndef mul_all():\n    result = 0\n    for x in range(100):\n        result *= x\n    return result","47ad54c2":"mul_all()\nsum_all()","854dab74":"def plot_common_words(func):\n    \n    def decorated(corpus, name=\"dataset\"):\n        # Get word_list from decorated function\n        word_list = func(corpus)\n        \n        # Count words inside the list\n        word_counts = Counter(word_list)\n    \n        # Get top 10 most frequent word\n        top_10_words = word_counts.most_common(10)\n        \n        # Plot the result\n        plt.figure(figsize=(8, 5))\n        plt.suptitle(\"{} in {}\".format(func.__name__, name))\n        \n        x, y = zip(*top_10_words)\n        \n        labels = list(x)\n        bplot = sns.barplot(x=labels, y=y)\n        bplot.set_xticklabels(labels=labels, rotation=30)\n        \n        # Return nothing here, because we just want the plot\n        return\n    \n    return decorated\n\n\n@plot_common_words\ndef common_words(corpus):\n    word_list = [word for word in corpus \n                      if word not in stop and word.isalpha()]\n    return word_list\n\n@plot_common_words\ndef common_puncts(corpus):\n    punc_regex = r\"[{}]+\".format(string.punctuation)\n    word_list = [word for word in corpus if re.fullmatch(punc_regex, word)]\n    return word_list\n\n@plot_common_words\ndef common_nonalpha(corpus):\n    punc_regex = r\"[{}]+\".format(string.punctuation)\n    word_list = [word for word in corpus \n                      if not re.fullmatch(punc_regex, word) and \n                         not word.isalpha() and\n                         not word.isdigit()]\n    return word_list\n\n@plot_common_words\ndef common_stops(corpus):\n    word_list = [word for word in corpus if word in stop]\n    return word_list","c545902d":"common_words(class_0_corpus, name=\"Class 0 Train\")\ncommon_words(class_1_corpus, name=\"Class 1 Train\")","c80868e4":"common_puncts(class_0_corpus, name=\"Class 0 Train\")\ncommon_puncts(class_1_corpus, name=\"Class 1 Train\")","d0f92c91":"common_nonalpha(class_0_corpus, name=\"Class 0 Train\")\ncommon_nonalpha(class_1_corpus, name=\"Class 1 Train\")","caba6f60":"common_stops(class_0_corpus, name=\"Class 0 Train\")\ncommon_stops(class_1_corpus, name=\"Class 1 Train\")","9e9403ad":"noise_patterns = { \n        \"url\"            : r\"https?(:\/\/\\S+|\\S+)|www\\.\\S+\",\n        \"html_tag\"       : r\"<.*?>\",          \n        \"non_ascii\"      : r\"[^\\x00-\\x7f]+\",\n        \"RT word\"        : r\"\\b[Rr][Tt]\\b\",  # RT commonly appear in retweeted tweet\n        \"amp word\"       : r\"\\bamp\\b\",       # what is \"amp\" ? why this is so common in tweets\n}\n\n\ndef noise_check(df_map, patterns):\n    \n    for key, df in df_map.items():\n        print(\"{} DataFrame noises: \".format(key))\n        print(\"-\" * 30)\n        for indicator, pattern in patterns.items():\n            count = df[\"text\"].str.contains(pattern).sum()\n            print(\"There were {:5} rows with {}\".format(count, indicator))\n        print()\n    \n\nnoise_check(df_map, noise_patterns)","c6348196":"# Insert pattern into parentheses\ndef add_paren(text):\n    return \"({})\".format(text)\n\n# Every noise pattern will be inserted into parentheses, then\n# join all noise patterns with '|' \nnoises_all = \"|\".join(add_paren(pattern) for pattern in noise_patterns.values())\n\n# Result is in format: (pattern1)|(pattern2)|(pattern3)|(pattern4)\nprint(noises_all)","4923bdfa":"# Clear all noises simultaneously\ndef clean_noise(text):\n    return re.sub(noises_all, r\" \", text)\n\ntrain_tweet[\"text\"] = train_tweet[\"text\"].apply(clean_noise)\ntest_tweet[\"text\"] = test_tweet[\"text\"].apply(clean_noise)\n\nnoise_check(df_map, noise_patterns)","f02a9564":"def random_sampling(df_map, n=5):\n    for key, df in df_map.items():\n        print(\"{} sample from {} dataset\".format(n, key))\n        print(\"-\" * 30)\n        print(df[\"text\"].sample(n))\n        print()\n        \nrandom_sampling(df_map)","f6e2e8b7":"word1 = \"The\"\nword2 = \"the\"\n\nprint(word1.lower() == word2)","3cd90a60":"def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n# testing\nprint( decontracted(\"I'm Lord of Darkness\") ) \nprint( decontracted(\"Who've made this burger?\") )","9bd30626":"def punct2espace(text): # comma to empty space\n    return re.sub(\",\", \"\", text)\n\ndef punct2wspace(text): # punc to white space\n    return re.sub(r\"[{}]+\".format(string.punctuation), \" \", text)\n\ndef residual_punc(text): # remove remaining bacward slash\n    table = str.maketrans(\"\", \"\", string.punctuation)\n    return text.translate(table)\n\ndef normalize_wspace(text): # normalize multiple whitespace\n    return re.sub(r\"\\s+\", \" \", text)\n\ndef replace_punctuations(text):\n    text = punct2espace(text)\n    text = punct2wspace(text)\n    text = residual_punc(text)\n    text = normalize_wspace(text)\n    \n    return text.strip()\n\n\nreplace_punctuations(r\"@@rakka@@ alhazimi@@hai typhoon--devastation \\\\\\\\\\conclusively\")","25f5dc6e":"!pip install pyspellchecker","715b826f":"from spellchecker import SpellChecker\n\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"speling correctin\"\ncorrect_spellings(text)","02b3414c":"from nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\nlemmatize_words(\"rakka alhazimi are cool\")","04b5b36c":"def normalize_text(text):\n    text = text.lower()                                         # lowercase\n    text = decontracted(text)                                   # unravel apostrophe words\n    text = replace_punctuations(text)                           # remove punctuations\n    text = lemmatize_words(text)                                # lemmatize word \n    \n    return text\n\nnormalize_text(\"Hello andrew, I'm from kuvukiland nuce to meet you\")","790c409d":"!pip install pandarallel # Use pandarallel for fast apply","78dc05f5":"from pandarallel import pandarallel\n\npandarallel.initialize()\n\nstart = time.time()\ntrain_tweet[\"text\"] = train_tweet[\"text\"].parallel_apply(normalize_text)\ntest_tweet[\"text\"] = test_tweet[\"text\"].parallel_apply(normalize_text)\n\nelapsed = time.time() - start\nprint(\"Elapsed time: {:.4f} min\".format(elapsed \/ 60))","f869a6ad":"train_tweet.to_csv(\"train_tweet.csv\")\ntest_tweet.to_csv(\"test_tweet.csv\")","d69b06eb":"train_tweet[[\"text\", \"target\"]].head(10)","2abb265b":"english_vocab = nltk.corpus.words.words(\"en\")\nenglish_vocab = set(english_vocab)\n\ncorpus_0, corpus_1 = create_corpus(train_tweet)\ncorpus = corpus_0 + corpus_1\n\ntweet_words = set(corpus)","e238cdf3":"unknown_words = list(tweet_words.difference(english_vocab))\nprint(\"There were {} unknown words. \".format(len(unknown_words)))","a1266f90":"MAX_LEN = 50  # max sentence length\n\ndef tokenize(df_map):\n    sentence_list = []\n    for key, df in df_map.items():\n        \n        # From sentence -> filter(word) -> sentence\n        # What we filter is : digits\n        sentence = [\" \".join(word for word in sen.split() if not word.isdigit()) # <- here's the digits\n                                  for sen in df[\"text\"]]\n        \n        length = df.shape[0] # Store test tweet length for splitting                    \n        \n        sentence_list += sentence\n        \n    # Index all train and test tweets words\n    tokenizer = preprocessing.text.Tokenizer()\n    tokenizer.fit_on_texts(sentence_list)\n    \n    # Convert text into sequences of integer\n    tensor = tokenizer.texts_to_sequences(sentence_list)\n    tensor = preprocessing.sequence.pad_sequences(tensor, padding=\"post\", maxlen=MAX_LEN)\n    \n    # Split train and test tweets\n    input_tensor_train, input_tensor_test = tensor[:-length], tensor[-length:]\n    \n    return input_tensor_train, input_tensor_test, tokenizer\n\n\n# Input tensor train\/test and tokenizer\nX_train, X_test, tokenizer = tokenize(df_map)\n\n# Target tensor train\ny_train = train_tweet[\"target\"].values\ny_train = y_train.reshape(-1, 1)","49b69a99":"# Fasttext\nwith open(\"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\", \"r\") as vector:\n    fasttext = vector.readlines()\n    \nlen(fasttext)","c504f04f":"# GloVe\nwith open(\"..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt\", \"r\") as vector:\n    glove = vector.readlines()\n    \nlen(glove)","9628e9f9":"def embed2dict(tokenizer, embedding):\n    word_index = tokenizer.word_index\n    embedding_dict = {line.split()[0]: line.split()[1:] for line in embedding[1:]\n                                                        if word_index.get(line.split()[0])}\n    return embedding_dict\n    \nfasttext_dict = embed2dict(tokenizer, fasttext)\nglove_dict = embed2dict(tokenizer, glove)","6e78cc2d":"fasttext_word = set(fasttext_dict.keys())\nglove_word = set(glove_dict.keys())\n\nintersection_word = fasttext_word.intersection(glove_word)\n\nprint(\"Similar embedding words:\", len(intersection_word))","519a262f":"# update glove_dict\ndef update_embed_dict(embed_dict):\n    result = {word: vector for word, vector in embed_dict.items()\n                           if word in intersection_word}\n    return result\n\nfasttext_dict = update_embed_dict(fasttext_dict)\nglove_dict = update_embed_dict(glove_dict)","cece64b1":"fasttext_dim = 300\nglove_dim = 200\n\nnum_words = len(tokenizer.word_index) + 1 # word_index starts at 1\n\ndef dict2vector(tokenizer, embed_dict, embed_dim):\n    \n    vector = np.zeros(shape=(num_words, embed_dim))\n    word_index = tokenizer.word_index\n    \n    for word, index in word_index.items():\n        if index > num_words:\n            continue\n        \n        if embed_dict.get(word):\n            vector[index] = embed_dict.get(word)\n    \n    return vector\n    \nfasttext_vector = dict2vector(tokenizer, fasttext_dict, fasttext_dim)\nglove_vector = dict2vector(tokenizer, glove_dict, glove_dim)","c42eb7f5":"def concat_vector(vectors):\n    # result shape (num_words, fasttext_dim + glove_dim)\n    result = np.hstack(vectors)\n    return result\n\nembedding_vector = concat_vector([fasttext_vector, glove_vector])","fd590801":"embedding_vector.shape","81b3e8dd":"BATCH_SIZE = 32\nEPOCHS = 8\n\nembedding_dim = embedding_vector.shape[1]                                               ","b724d69e":"def build_model(emb_init=None, emb_train=True):\n    \n    embed_params = {\"input_dim\": num_words, \n                    \"output_dim\": embedding_dim,\n                    \"embeddings_initializer\": keras.initializers.Constant(\n                        embedding_vector if emb_init is None else emb_init),\n                    \"trainable\": emb_train,\n                    \"mask_zero\": True, # ignore \"zero\" paddings\n                   }\n    \n    model = keras.Sequential([\n                keras.layers.Embedding(**embed_params),\n\n                keras.layers.Bidirectional(keras.layers.LSTM(128, \n                                                             dropout=0.3,\n                                                             recurrent_dropout=0.3,\n                                                             return_sequences=True\n                                                            )),\n                # Reshape size (x, y, hidden) where x * y = MAX_LEN\n                keras.layers.Reshape((5, 10, 256)),\n                keras.layers.Conv2D(32, 2),\n                keras.layers.GlobalMaxPooling2D(),\n                keras.layers.Dropout(0.3),\n                keras.layers.Dense(1, activation=\"sigmoid\")])\n\n    optimizer = keras.optimizers.Adam(1e-4, clipvalue=0.5)\n\n    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"acc\"])\n\n    return model","900f7266":"test_model = build_model(emb_train=True)\ntest_model.summary()","c7c445e9":"test_model.fit(X_train, y_train,\n              epochs=10,\n              batch_size=BATCH_SIZE,\n              validation_split=0.2,)","d83c1c0c":"oov_embedding = test_model.get_weights()[0]\noov_embedding","97650dbc":"def merge_embeddings(oov_embed, embed_vector):\n    new_vector = embed_vector.copy()\n    \n    for index, row in enumerate(embed_vector):\n        if row.sum() == 0:\n            new_vector[index] = oov_embed[index]\n    \n    return new_vector\n\nnew_embedding = merge_embeddings(oov_embedding, embedding_vector)","1ba8020b":"n_splits = 5\nsplits = list(KFold(n_splits=n_splits, shuffle=True, random_state=0).split(X_train,y_train))\nmodel_record = {}\npredictions = np.zeros((X_test.shape[0], 1))\n\nfor n, fold in enumerate(splits):\n    \n    # Current Fold Status\n    print()\n    print(\"Fold {}\".format(n + 1))\n    \n    # Define Model Name\n    model_fold = \"lstm_fold0{}.h5\".format(n + 1)\n    \n    # Callback List\n    model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=model_fold,\n                                                       monitor=\"val_acc\",\n                                                       save_best_only=True,)\n    callbacks_list = [model_checkpoint]\n    \n    # Split data into train and val using fold index\n    X_train_fold, y_train_fold = X_train[fold[0]], y_train[fold[0]]\n    X_val_fold, y_val_fold = X_train[fold[1]], y_train[fold[1]]\n    \n    # Build model\n    model = build_model(emb_init=new_embedding, emb_train=False) # Embedding is not trainable\n    \n    history = model.fit(X_train_fold, y_train_fold,\n                          batch_size=BATCH_SIZE,\n                          epochs=EPOCHS,\n                          validation_data=(X_val_fold, y_val_fold),\n                          callbacks=callbacks_list,)\n\n    # Record model with best val_accuracy    \n    model_record[model_fold] = max(history.history[\"val_acc\"])\n    \n    # Use the best model\n    model = keras.models.load_model(model_fold)\n    \n    predictions += model.predict(X_test)\n\npredictions \/= n_splits","7c887eae":"model_record","28e08cc0":"top_acc = max(model_record.keys(), key=model_record.get)\ntop_acc","7e5dfc61":"# class_prediction = np.where(predictions > 0.5, 1, 0)\nbest_model = keras.models.load_model(top_acc)\nclass_prediction = best_model.predict_classes(X_test)\nsubmission = pd.DataFrame({\"id\": test_tweet[\"id\"], \n                           \"target\": class_prediction.flatten()})\n\nsubmission.to_csv(\"submission.csv\", index=False)","4b5ec50d":"submission","4ce37cc1":"### Get Dataset Information","07d48c4f":"# Submission","51cc7dc6":"### Transform Embedding into Dict","9a0349bd":"# Build NN Model","07c0f01f":"Step 3. Execute","4d256ae6":"### Similar Words between Two Words Embedding\nWords vector in both fasttext and glove is sure not similar. We need to find the intersect using python set() data structure.","c1336b20":"Let's update the dictionaies","59026b05":"There were no missing values on text columns, we're good to go","9dd80844":"### Unravel Apostrophe Words\n\nreference: https:\/\/stackoverflow.com\/questions\/43018030\/replace-apostrophe-short-words-in-python","e525aee6":"### Decorator Mini Guide\nLet's say you want to measure how long your function running time. You will do the following...","aca801df":"# How many Unknown Words?","4b915246":"### Wrap all function into one.","fbaf8698":"### GloVe","8ed8bea4":"You must copy-paste those start and end variable into the new function. Imagine if you do that on another 10 functions. Time waster isn't it? Now this is the time for **Decorator** to shine. This is how you create a simple decorator.","7e1bafbc":"### Remove Punctuations\nI didn't place this section in noise removal because I need to unravel the apostrophe words before begin removing all punctuations. In this function I replace **\",@#\"** with empty space and replace the rest with whitespace","8dff2739":"### Lowercase all word\nBy using lower(), we have normalized the text to lowercase so that the distinction between The and the is ignored.\n\nreference: https:\/\/www.nltk.org\/book\/ch03.html","bae62005":"# Noise Removal","fda9b690":"### Common Noises\nRemove common noises found on the above EDA","bd002f8f":"# Unique Pattern Investigation\nTo handle these unique patterns, we need to identify what the patterns are. This still in test, and we can skip it.","e0f24ce8":"# Word Embedding","74949ed2":"### End of Decorator Guide","591e8a46":"### Change Embedding Dict into Vector","87c53157":"### Fasttext","5b28732f":"# Train OOV Words Embedding","917c48b2":"# Text Representation","62cb8c72":"# Import Modules","42f6a191":"In order to plot the common words, I will use **decorator**. **Decorator** is used to expand our defined function without explicitly modify it. Just bear with me, it may looks complicated but the concept is simple. \n\nNote: (Skip to the plotting section if you already understand Decorator)","15179882":"Step 2. Add @wrapper_function on top of Base Function","ae73c971":"# Load Dataset","24c1c198":"### Correct Mispelling","fa3a7897":"### Convert text into array of integer","0240fe83":"Quite simple right? What if, you want to measure the execution time on another function?","c3962d8f":"### Concatenate Two Embedding Vectors into One","719638a1":"# Text Normalization","e7b8ee6d":"# Kfold 5","478e68d8":"# Define Parameter","1b46009a":"The model architecture is Bidirectional LSTM with MaxPooling2D.\n1. Embeddings\n2. BLSTM\n3. Reshape into Image dim like\n4. Conv2D\n5. GlobalMaxPooling2D\n6. Dropout\n7. Dense\n\nreference: https:\/\/arxiv.org\/abs\/1611.06639","05738c3e":"### Class Distribution","1ff56f6e":"### Common Words","94f524d7":"### Lemmatization\nText lemmatization is the process of eliminating redundant prefix or suffix of a word and extract the base word (lemma).\n\nreference:\n\nhttps:\/\/medium.com\/text-classification-algorithms\/text-classification-algorithms-a-survey-a215b7ab7e2d\nhttps:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-text-preprocessing#Removal-of-Frequent-words","65b1ab0c":"Let's do it for plotting common words","3717dca0":"Step 1. Build Wrapper Function","2a9c4597":"### Save to CSV","1b0aad80":"# Exploratory Data Analysis (EDA)","ae5b6b73":"# Insert OOV vector into Original Embeddings"}}