{"cell_type":{"4ddc4993":"code","e49c08f6":"code","908ad04a":"code","5e88be9d":"code","e1e28037":"code","5113e8b6":"code","4fa0fdc4":"code","a3aa4501":"code","cb058260":"code","9d344aad":"code","b0ee5328":"code","2047e23f":"code","d2586133":"code","f46f297d":"code","52536c25":"code","b0acdacc":"code","d7a13b58":"code","eef93ad1":"code","952d9fdc":"code","513ef539":"code","f17d78d8":"code","875815e8":"code","56666cd2":"code","f9c66d09":"code","b5dfce4a":"code","6445c291":"code","5fc22353":"code","52386b4e":"code","0027d585":"code","5da400bd":"code","5d864047":"code","543bd794":"code","8fb3b19a":"code","f7ad8b2a":"code","7b6bf7f0":"code","37eeefe7":"code","efc098ec":"code","2c9dc04e":"code","bb56f355":"code","15eb2d43":"code","01a86a22":"code","cf93d2c0":"code","ee6bb875":"code","a5335ca6":"code","04eeb5a5":"markdown","11bd674f":"markdown"},"source":{"4ddc4993":"!python -m spacy download pt\n!pip install unidecode\n!pip install spacymoji","e49c08f6":"import pandas as pd\nimport numpy as np\nimport spacy\nfrom tqdm import tqdm\nimport nltk\nimport keras\nimport math\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport re\nimport string\nimport unidecode\nimport unicodedata\n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM,concatenate, SpatialDropout1D, GlobalAveragePooling1D, Input, Bidirectional, MaxPooling1D, Activation,Conv1D,GRU, CuDNNGRU, CuDNNLSTM, Dropout, GlobalMaxPooling1D,Embedding\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras import backend\n\nfrom sklearn.model_selection import ShuffleSplit\nimport itertools\n\ntqdm.pandas()\n%matplotlib inline","908ad04a":"# Create folder to save embeddings\n!mkdir word_embeddings\/","5e88be9d":"#Embeddings download\n\n#DOWNLOAD GLOVE\n!wget -c http:\/\/143.107.183.175:22980\/download.php?file=embeddings\/glove\/glove_s300.zip -O glove_s300.zip\n!unzip glove_s300.zip\n!mv glove_s300.txt word_embeddings\/g.txt\n!rm glove_s300.zip\n\n#DOWNLOAD WORD2VEC\n#!wget -c http:\/\/143.107.183.175:22980\/download.php?file=embeddings\/word2vec\/cbow_s300.zip -O word_s300.zip\n#!unzip word_s300.zip\n#!mv cbow_s300.txt word_embeddings\/w.txt\n\n#DOWNLOAD FAST\n#!wget -c http:\/\/143.107.183.175:22980\/download.php?file=embeddings\/fasttext\/skip_s300.zip -O fast_s300.zip\n#!unzip fast_s300.zip\n#!mv skip_s300.txt word_embeddings\/f.txt","e1e28037":"#Load Data\n\ntrain_df = pd.read_csv('\/kaggle\/input\/e-commerce-reviews\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/e-commerce-reviews\/test.csv')","5113e8b6":"# Score count\ntrain_df['review_score'].value_counts()","4fa0fdc4":"train_df.groupby('review_score').describe()","a3aa4501":"train_df.info()","cb058260":"# Look for null values\ntrain_df.isnull().sum()","9d344aad":"# List rows containing null columns\ntrain_df[train_df.isna().any(axis=1)]","b0ee5328":"# Remove null values\ntrain_df = train_df.dropna()\n\n# New shape\ntrain_df.shape","2047e23f":"# Count words that appears most in the text\nqtd_words = pd.Series(' '.join(train_df['review_comment_message']).lower().split()).value_counts()\nqtd_words[:10]","d2586133":"# Transcription given for each one emojis on spacy, and his translation\nemojis = {'smiling face with heart-eyes' : 'amei',\n 'grinning face with big eyes' : 'legal',\n 'clapping hands' : 'palmas',\n 'beaming face with smiling eyes' : 'legal',\n 'disappointed face' : 'desapontado',\n 'thumbs up medium-light skin tone' : 'gostei',\n 'clapping hands medium-dark skin tone' : 'palmas',\n 'elephant' : 'elefante',\n 'OK hand medium-light skin tone' : 'ok',\n 'thumbs up' : 'gostei',\n 'clapping hands light skin tone' : 'palmas',\n 'thumbs up light skin tone' : 'gostei',\n 'hugging face' : 'gostei',\n 'tulip' : 'flor',\n 'call me hand light skin tone' : 'legal',\n 'thumbs down' : 'n\u00e3o gostei',\n 'grinning face' : 'feliz',\n 'thinking face' : 'pensativo',\n 'dog' : 'cachorro',\n 'kiss mark' : 'beijo',\n 'two hearts' : 'cora\u00e7\u00e3o',\n 'revolving hearts' : 'cora\u00e7\u00e3o',\n 'face blowing a kiss' : 'beijo',\n 'green heart' : 'cora\u00e7\u00e3o verde',\n 'smiling face with smiling eyes' : 'legal',\n 'thumbs up medium skin tone' : 'gostei',\n 'heart decoration' : 'cora\u00e7\u00e3o',\n 'sparkling heart': 'cora\u00e7\u00e3o',\n 'heart with ribbon': 'cora\u00e7\u00e3o',\n 'angry face' : 'raiva',\n 'folded hands' : 'obrigado',\n 'clapping hands medium skin tone' : 'palmas',\n 'persevering face' : 'perseverante',\n 'neutral face' : 'neutro',\n 'pouting face' : 'raiva',\n 'glowing star' : 'estrela',\n 'winking face' : 'piscar',\n 'pensive face' : 'pensativo',\n 'hundred points' : 'cem pontos',\n 'grinning squinting face' : 'legal',\n 'smirking face' : 'sorrindo',\n 'girl' : 'garota',\n 'backhand index pointing up' : 'acima',\n 'backhand index pointing right' : 'na frente',\n 'loudly crying face' : 'chorando',\n 'sad but relieved face' : 'preocupado',\n 'crying face' : 'chorando',\n 'clapping hands medium-light skin tone' : 'palmas',\n 'television' : 'televis\u00e3o',\n 'smiling face with sunglasses' : 'sorrindo',\n 'drooling face' : '\u00e1gua na boca',\n 'weary face' : 'cansado',\n 'oncoming fist' : 'punho',\n 'confused face' : 'confuso',\n 'thumbs down light skin tone' : 'n\u00e3o gostei',\n 'beating heart' : 'cora\u00e7\u00e3o',\n 'horse racing' : 'cavalo',\n 'wrapped gift' : 'presente',\n 'downcast face with sweat' : 'desanimado',\n 'TOP arrow' : 'top',\n 'slightly frowning face' : 'desanimado',\n 'four leaf clover' : 'trevo',\n 'oncoming fist medium skin tone' : 'punho',\n 'OK hand light skin tone' : 'ok'}","f46f297d":"import spacy\nfrom spacymoji import Emoji\n\nnlp = spacy.load('pt')\nemoji = Emoji(nlp)\nnlp.add_pipe(emoji, first=True)\n\ndef replace_emoji(x):\n  doc = nlp(x)\n  words = []\n  if doc._.has_emoji:\n    for index, item in enumerate(doc):\n      emo_ind = 0\n      if doc[index]._.is_emoji:\n        words.append(doc._.emoji[emo_ind][2])\n        emo_ind = emo_ind + 1\n      else:\n        words.append(doc[index].text)\n    return ' '.join(words)\n      \n  else:\n    return x","52536c25":"def translate_emoji(x):\n  for item in emojis:\n    x = x.replace(item, emojis[item])\n  return x","b0acdacc":"#replace emoji\ntrain_df['review_comment_message'] = train_df['review_comment_message'].progress_map(replace_emoji)\ntrain_df['review_comment_message'] = train_df['review_comment_message'].progress_map(translate_emoji)\n\ntest_df['review_comment_message'] = test_df['review_comment_message'].progress_map(replace_emoji)\ntest_df['review_comment_message'] = test_df['review_comment_message'].progress_map(translate_emoji)","d7a13b58":"misspeling = {\n    \"\u00f1\": \"n\u00e3o\",\n    \"n\": \"n\u00e3o\",\n    \"e-mail\" : \"email\",\n    \"e-mails\": \"emails\"\n}\n\nmisspeling_embed = {\n    \"td\": \"tudo\",\n    \"smp\": \"sempre\",\n    \"masss\": \"mas\",\n    \"ecxelente\": \"excelente\",\n    \"cx\": \"caixa\",\n    \"\u00f1\": \"n\u00e3o\",\n    \"n\": \"n\u00e3o\",\n    \"e-mail\" : \"email\",\n    \"e-mails\": \"emails\",\n    \"qdo\": \"quando\",\n    \"q\": \"que\",\n    \"pq\": \"porque\",\n    \"msm\": \"mesmo\",\n    \"geito\": \"jeito\",\n    \"vcs\": \"voc\u00eas\",\n    \"vc\": \"voc\u00ea\",\n    \"obg\": \"obrigado\",\n    \"cm\": \"cent\u00edmetros\",\n    \"entega\" : \"entrega\",\n    \"und\": \"unidades\",\n    \"adaquirido\": \"adquirido\",\n    \"pedente\": \"pendente\",\n    \"eh\": \"\u00e9\",\n    \"mto\": \"muito\",\n    \"p\": \"para\",\n    \"d\": \"de\",\n    \"p\/\": \"para\",\n    \"nf\": \"nota fiscal\",\n    \"c\": \"com\",\n    \"c\/\": \"com\",\n    \"hj\": \"hoje\",\n    \"plazo\": \"prazo\",\n    \"corsspindente\": \"correspondente\",\n    \"gustei\": \"gostei\",\n    \"oque\": \"o que\",\n    \"r$\": \"reais\",\n    \"on-line\": \"online\",\n    \"talves\": \"talvez\",\n    \"tb\": \"tamb\u00e9m\",\n    \":)\": \"legal\",\n    \":))\": \"legal\",\n}","eef93ad1":"# Define preprocessing function\nimport string\nimport unidecode\nimport unicodedata\nimport re\n\n\nnlp = spacy.load('pt', parser=False, entity=False)\n\ndef preprocessing(x,embed):\n    \n    x = x.lower()\n    \n    # misspelling\n    miss_res = []\n    misspel = {}\n    if embed:\n        misspel = misspeling_embed\n    else:\n        misspel = misspeling\n    for item in x.split():\n        # iterate by keys\n        if item in misspel.keys():\n            # look up and replace\n            miss_res.append(misspel[item])\n        else:\n            miss_res.append(item)\n    \n    x = ' '.join(miss_res)\n    \n    # Remove links\n    x = re.sub(r'http\\S+', ' ', x)\n    \n    # Remove punct\n    x = re.sub(r'[^\\w\\s]',' punct ', x )\n    \n    # Remove extra chars\n    x = re.sub(r'([a-z])\\1+', r'\\1', x)\n    \n    # Remove accents\n    x = unicodedata.normalize('NFD', x)\n    x = x.encode('ascii', 'ignore')\n    x = x.decode(\"utf-8\")\n    \n    x = ' '.join(x.split())\n    \n    return x","952d9fdc":"#Generate embedding\nfrom gensim.models.fasttext import FastText as FT_gensim\nfrom gensim.test.utils import datapath, common_texts, get_tmpfile\nfrom gensim.models import Word2Vec\n\n#Pre process text\ntrain_new = train_df\ntest_new = test_df\ntrain_new['review_comment_message'] = train_df['review_comment_message'].progress_map(lambda x :preprocessing(x, True))\ntest_new['review_comment_message'] = test_df['review_comment_message'].progress_map(lambda x : preprocessing(x, True))\nsentences_emb = (train_new['review_comment_message'].tolist() + test_new[\"review_comment_message\"].tolist())","513ef539":"#Generate corpus\nwith open('word_embeddings\/corpus', 'w', encoding='utf-8') as f:\n        for sentence in tqdm(sentences_emb):\n            f.write(sentence + '\\n')","f17d78d8":"#Generate Model\ndef generate_model():\n    model_gensim = FT_gensim(size=300)\n\n    # build the vocabulary\n    model_gensim.build_vocab(corpus_file='word_embeddings\/corpus')\n\n    # train the model\n    model_gensim.train(\n        corpus_file='word_embeddings\/corpus', epochs=model_gensim.epochs,\n        total_examples=model_gensim.corpus_count, total_words=model_gensim.corpus_total_words\n    )\n\n    model_gensim.save('word_embeddings\/fasttext.vec')\n    \n#generate_model()","875815e8":"# Preprocessing\ntrain_df['review_comment_message'] = train_df['review_comment_message'].progress_map(lambda x :preprocessing(x, False))\ntrain_df.head()","56666cd2":"test_df['review_comment_message'] = test_df['review_comment_message'].progress_map(lambda x :preprocessing(x, False))","f9c66d09":"# Define values\nseq_size     = 50\nmax_tokens   = 15274\nembed_dim    = 300","b5dfce4a":"#Generate X e Y\n## ** Do not use text_fit for texts to sequecen, because X and Y are different\ntokenizer = Tokenizer(num_words=max_tokens, split=' ')\n\n# Join text from train and test for fit\ntext_fit = list(train_df['review_comment_message'].values) + list(test_df['review_comment_message'].values)\n\n# Generate text with only train text, for sequences\ntext = train_df['review_comment_message'].values\n\ntokenizer.fit_on_texts(text_fit)\n\nX = tokenizer.texts_to_sequences(text)  \n\nX = pad_sequences(X, maxlen=seq_size)\n\nY = train_df['review_score'].values","6445c291":"# Generate embeddings from glove, fast and word2vec\n# ** Due limitation of disk on Kaggle, was not possible download all embeddings\n\ndef embedding(tok,embedding_file,max_features,embed_size):\n    print(\"Gerando Embedding\")\n\n\n    embeddings_index = {}\n    with open(embedding_file,encoding='utf8') as f:\n        for line in f:\n            values = line.rstrip().rsplit(' ')\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n\n    word_index = tok.word_index\n    #prepare embedding matrix\n    num_words = min(max_features, len(word_index) + 1)\n    embedding_matrix = np.zeros((num_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n\n    return embedding_matrix\n\n#embedding_f = embedding(tokenizer, 'word_embeddings\/skip_s300.txt', max_tokens,embed_dim)\nembedding_g = embedding(tokenizer, 'word_embeddings\/g.txt', max_tokens,embed_dim)\n#embedding_w = embedding(tokenizer, 'word_embeddings\/cbow_s300.txt', max_tokens,embed_dim)\n#embedding_matrix = np.concatenate((embedding_f, embedding_g,embedding_w), axis=1)","5fc22353":"# Make load of embedding gerated previosly\n\nfrom gensim.models.fasttext import FastText as FT_gensim\ndef generated_embedding(tok,max_features,embed_size):\n    print(\"Fasttext Embedding\")\n\n    loaded_model = FT_gensim.load('\/kaggle\/input\/ecembedding\/fasttext.vec')\n\n    word_index = tok.word_index\n    # prepare embedding matrix\n    num_words = min(max_features, len(word_index) + 1)\n    embedding_matrix = np.zeros((num_words, embed_size))\n    unknown_vector = np.random.normal(size=embed_size)\n    for key, i in word_index.items():\n        if i >= max_features:\n            continue\n        word = key\n        embedding_vector = loaded_model[word]\n        embedding_matrix[i] = embedding_vector\n\n    return embedding_matrix\n\n#embedding_o = generated_embedding(tokenizer, max_tokens,embed_dim)","52386b4e":"#embedding_matrix = np.concatenate((embedding_g, embedding_o), axis=1)\nembedding_matrix = embedding_g","0027d585":"# Multiply o embed_dim by quantity of embeddings\nfrom keras.models import Model\ndef get_model(embedding):\n    sequence_input = Input(shape=(seq_size,))\n    x = Embedding(max_tokens, embed_dim, weights=[embedding], trainable=True, input_length = seq_size)(sequence_input)\n    x = SpatialDropout1D(0.3)(x)\n    \n    x1 = Bidirectional(CuDNNLSTM(256, return_sequences=True))(x)\n    x2 = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x1)\n    x1_conv = Conv1D(256, kernel_size=2, padding=\"valid\", kernel_initializer=\"he_uniform\")(x1)\n    \n    max_pool1 = GlobalMaxPooling1D()(x1)\n    max_pool2 = GlobalMaxPooling1D()(x2)\n    max_pool3 = GlobalMaxPooling1D()(x1_conv)\n    concat_1 = concatenate([max_pool1, max_pool2,max_pool3])\n    \n    dense = Dense(256, activation='relu')(concat_1)\n    \n    preds = Dense(1, activation='relu')(dense)\n    \n    model = Model(sequence_input, preds)\n    model.compile(loss =tf.keras.losses.Huber(), optimizer='rmsprop', metrics=['accuracy'])\n    \n    #model.summary()\n    return model","5da400bd":"from sklearn.model_selection import ShuffleSplit\nfrom imblearn.over_sampling import SMOTE,RandomOverSampler\nimport itertools\n\nskf = ShuffleSplit(n_splits=5, test_size=0.15, random_state=133)\n\nfor n_fold, (train_indices, val_indices) in enumerate(skf.split(X,Y)):\n    \n    base_model = get_model(embedding_matrix)\n    \n    X_train = X[train_indices]\n    Y_train = Y[train_indices]\n    X_valid = X[val_indices]\n    Y_valid = Y[val_indices]\n    \n    filepath = 'best_model_' + str(n_fold) + '.h5'\n\n    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\n    class_weights = class_weight.compute_class_weight('balanced', train_df[\"review_score\"].unique(), Y_train)\n\n    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n\n    reduce_lr = ReduceLROnPlateau(\n                    monitor  = 'val_loss',\n                    factor   = 0.3,\n                    patience = 1,\n                    verbose  = 1,\n                    mode     = 'auto',\n                    epsilon  = 0.0001,\n                    cooldown = 0,\n                    min_lr   = 0\n                )\n\n    callbacks_list = [reduce_lr, early, checkpoint]\n\n    hist = base_model.fit(X_train, Y_train, \n                  validation_data =(X_valid, Y_valid),\n                  class_weight=class_weights,\n                  batch_size=8, nb_epoch = 30,  verbose = 1, callbacks=callbacks_list)","5d864047":"plt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])","543bd794":"# Evaluate one fold\n\ndef eval_fold():\n    \n    base_model = get_model(embedding_matrix)\n    base_model.load_weights('best_model_4.h5')\n    \n    final_predict = base_model.predict(X_valid)\n    \n    return final_predict","8fb3b19a":"val_predict = eval_fold()\nval_predict =  val_predict[:,0]\nval_predict","f7ad8b2a":"Y_valid","7b6bf7f0":"rms = sqrt(mean_squared_error(Y_valid, val_predict))\nrms","37eeefe7":"#Gerar melhores valores para arredondar\n# Generate best values for round\ndef arrendondar(n_min,n_max, score):\n    n = int(str(score).split('.')[1][:2])\n    if n < n_min:\n        return math.floor(score)\n    elif n >= n_max:\n        return math.ceil(score)\n    else:\n        return score","efc098ec":"old_rms = 1\nbest_result = ''\nval_n_min = 0\nval_n_max = 0\nfor n_min in np.arange(0, 50, 1):\n    for n_max in np.arange(50, 100, 1):\n        new_val = np.array([arrendondar(n_min,n_max,x) for x in val_predict])\n        rms = sqrt(mean_squared_error(Y_valid, new_val))\n        if rms < old_rms:\n            val_n_min = n_min\n            val_n_max = n_max\n            best_result = 'N_min {}, N_max {}, rms {}'.format(n_min,n_max,rms)\n            old_rms = rms\nprint(best_result)","2c9dc04e":"def predict(text):\n    new_text = tokenizer.texts_to_sequences(text)\n    new_text = pad_sequences(new_text, maxlen=seq_size)\n    pred     = base_model.predict(new_text)\n    \n    return pred","bb56f355":"def predict_fold(text):\n    new_text = tokenizer.texts_to_sequences(text)\n    new_text = pad_sequences(new_text, maxlen=seq_size)\n    predict_folds=[]\n    for n in range(5):\n        base_model = get_model(embedding_matrix)\n        base_model.load_weights('best_model_' + str(n) + '.h5')\n        predict_folds.append(base_model.predict(new_text))\n    \n    final_predict = np.mean(predict_folds, axis=0)\n    \n    return final_predict","15eb2d43":"# Predict for all folds\npred     = predict_fold(test_df.review_comment_message)\npred     = pred[:,0]\npred[:5] ","01a86a22":"test_df.head()","cf93d2c0":"test_df['review_score'] = pred\ntest_df.head()","ee6bb875":"test_df['review_score'] = test_df['review_score'].apply(lambda x: arrendondar(val_n_min,val_n_max,x))\ntest_df.head()","a5335ca6":"test_df[['review_id', 'review_score']].to_csv('submission.csv', index=False)","04eeb5a5":"**ATTENTION**\n\nWas not possible use all embeddings, and either training my own. This happen because o disk limitation on Kaggle. But all code is available.","11bd674f":"*Brief Solution Comment*\n\n**What have been done?**\n\n1.  Remove emojis and replace with a choosen word\n2.  Preprocessing\n    * Lower\n    * Misspeling\n    * Remove links\n    * Remove accents\n    * Normalize punctuation\n    * Remove special characters\n3.  In tokenizer fit, combine text of train_df and test_df\n4.  Generate own embedding with fasttext\n5.  Generate embedding with glove, fast and word2vec\n6.  Concatenate all 4 embeddings\n7.  Use optimizer rmsprop\n8.  Loss used is huber > https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/losses\/Huber\n9.  Cross Validation with ShuffleSplit\n10. Calculate min e max for rouding"}}