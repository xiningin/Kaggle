{"cell_type":{"1bcb06f1":"code","334913d9":"code","cbc3b0bd":"code","ab667cc9":"code","45ca9787":"code","329d20f7":"code","ff097160":"code","6cc11a46":"code","91aecabe":"code","a4ac1431":"code","e6a179a1":"code","e49eef63":"code","1724fe8e":"code","1558341c":"code","fe5c8628":"code","819f3afe":"code","83b90591":"code","be3543f6":"code","c4229f1b":"code","fa87b41a":"code","a63d46d8":"code","d6b50812":"code","6d877396":"markdown","2e1fdc0e":"markdown","a1b1e50c":"markdown","30b5ef34":"markdown","4be94f4c":"markdown","ee0a8630":"markdown"},"source":{"1bcb06f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","334913d9":"import pandas as pd\n\nimport scikitplot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\nfrom keras.callbacks import Callback, EarlyStopping, TerminateOnNaN, ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator","cbc3b0bd":"train_data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\ntrain_label = train_data['label']\ntrain_data.drop('label', axis=1, inplace=True)\n\ntrain_data.shape, test_data.shape","ab667cc9":"print(f'frequency of labels :\\n{train_label.value_counts() \/ len(train_label)}')\ntrain_label.value_counts().sort_index().plot.bar()\nplt.show()","45ca9787":"X_train_df, X_valid_df, y_train_s, y_valid_s = train_test_split(train_data, train_label,\n                                                    shuffle=True, stratify=train_label,\n                                                    test_size=0.2, random_state=42\n                                            )\nX_train_df.shape, X_valid_df.shape, y_train_s.shape, y_valid_s.shape","329d20f7":"sample_train_img = X_train_df.iloc[0]\nsample_valid_img = X_valid_df.iloc[0]\n\nfig = plt.figure(0, (12, 4))\n\nax1 = plt.subplot(1,2,1)\nax1.imshow(np.array(sample_train_img).reshape(28, 28), cmap='gray')\nax1.set_title('train sample image')\n\nax2 = plt.subplot(1,2,2)\nax2.imshow(np.array(sample_valid_img).reshape(28, 28), cmap='gray')\nax2.set_title('validation sample image')\n\nplt.show()","ff097160":"def data_preprocessing(df, to_normalize=True, **kwargs):\n    X = df.astype('float32').values\n    \n    if to_normalize:\n        X \/= 255.\n        \n    if 'noise' in kwargs:\n        rng = np.random.RandomState(42)\n        X += kwargs['noise'] * rng.normal(loc=0.0, scale=1.0, size=X.shape)\n        X = np.clip(X, 0., 1.)\n        \n    return X.reshape(-1, 28, 28, 1)","6cc11a46":"X_train = data_preprocessing(X_train_df)# noise=0.2) # Training on noisy data didn't helped\nX_valid = data_preprocessing(X_valid_df)\n\nX_train.shape, X_valid.shape","91aecabe":"y_train = np_utils.to_categorical(y_train_s)\ny_valid = np_utils.to_categorical(y_valid_s)\n\ny_valid.shape, y_train.shape","a4ac1431":"img_width = X_train.shape[1]\nimg_height = X_train.shape[2]\nimg_depth = X_train.shape[3]\nnum_classes = y_train.shape[1]","e6a179a1":"def build_cnn():        \n    net = Sequential()\n\n    net.add(\n        Conv2D(\n            filters=32,\n            kernel_size=(3,3),\n            input_shape=(img_width, img_height, img_depth),\n            activation='relu'\n        )\n    )\n    net.add(\n        Conv2D(\n            filters=32,\n            kernel_size=(3,3),\n            activation='relu'\n        )\n    )\n    net.add(BatchNormalization())\n    net.add(MaxPooling2D(pool_size=(2,2)))\n    net.add(Dropout(0.2))\n\n    net.add(\n        Conv2D(\n            filters=64,\n            kernel_size=(3,3),\n            activation='relu'\n        )\n    )\n    net.add(\n        Conv2D(\n            filters=64,\n            kernel_size=(3,3),\n            activation='relu'\n        )\n    )\n    net.add(BatchNormalization())\n    net.add(MaxPooling2D(pool_size=(2,2)))\n    net.add(Dropout(0.2))\n\n    net.add(Flatten())\n    \n    net.add(Dense(128, activation='relu'))\n    net.add(BatchNormalization())\n    net.add(Dropout(0.4))\n    \n    net.add(Dense(num_classes, activation='softmax'))\n    \n    net.compile(\n        loss='categorical_crossentropy',\n        optimizer='adam',\n        metrics=['accuracy']\n    )\n    \n    net.summary()\n    \n    return net","e49eef63":"early_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    mode='max',\n    min_delta=0.0001,\n    baseline=0.98,\n    patience=6,\n    restore_best_weights=True,\n    verbose=1\n)\n\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_accuracy',\n    patience=3,\n    factor=0.25,\n    min_lr=1e-5\n)\n\ncallbacks = [\n    early_stopping,\n    lr_scheduler,\n    TerminateOnNaN(),\n]","1724fe8e":"model = build_cnn()\nhistory = model.fit(\n    x=X_train,\n    y=y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=64,\n    epochs=30,\n    callbacks=callbacks,\n    use_multiprocessing=True\n)","1558341c":"sns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(history.epoch, history.history['accuracy'], label='train')\nsns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(history.epoch, history.history['loss'], label='train')\nsns.lineplot(history.epoch, history.history['val_loss'], label='valid')\nplt.title('Loss')\nplt.tight_layout()\n\nplt.show()","fe5c8628":"df_accu = pd.DataFrame({'train': history.history['accuracy'], 'valid': history.history['val_accuracy']})\ndf_loss = pd.DataFrame({'train': history.history['loss'], 'valid': history.history['val_loss']})\n\nfig = plt.figure(0, (14, 4))\nax = plt.subplot(1, 2, 1)\nsns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_accu), showfliers=False)\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_loss), showfliers=False)\nplt.title('Loss')\nplt.tight_layout()\n\nplt.show()","819f3afe":"yhat_valid = model.predict_classes(X_valid)\nscikitplot.metrics.plot_confusion_matrix(np.argmax(y_valid, axis=1), yhat_valid, figsize=(7,7))\nprint(f'total wrong validation predictions: {np.sum(np.argmax(y_valid, axis=1) != yhat_valid)}\\n\\n')\nprint(classification_report(np.argmax(y_valid, axis=1), yhat_valid))","83b90591":"train_datagen = ImageDataGenerator(\n    shear_range=0.1,\n    zoom_range=0.1,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n)\ntrain_datagen.fit(X_train)\n\nmodel = build_cnn()\nhistory = model.fit_generator(\n    train_datagen.flow(X_train, y_train, batch_size=64),\n    validation_data=(X_valid, y_valid),\n    steps_per_epoch=len(X_train) \/ 64,\n    epochs=25,\n    callbacks=callbacks,\n    use_multiprocessing=True\n)","be3543f6":"fig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(history.epoch, history.history['accuracy'], label='train')\nsns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(history.epoch, history.history['loss'], label='train')\nsns.lineplot(history.epoch, history.history['val_loss'], label='valid')\nplt.title('Loss')\nplt.tight_layout()\n\nplt.show()","c4229f1b":"df_accu = pd.DataFrame({'train': history.history['accuracy'], 'valid': history.history['val_accuracy']})\ndf_loss = pd.DataFrame({'train': history.history['loss'], 'valid': history.history['val_loss']})\n\nfig = plt.figure(0, (14, 4))\nax = plt.subplot(1, 2, 1)\nsns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_accu), showfliers=False)\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_loss), showfliers=False)\nplt.title('Loss')\nplt.tight_layout()\n\nplt.show()","fa87b41a":"yhat_valid = model.predict_classes(X_valid)\nscikitplot.metrics.plot_confusion_matrix(np.argmax(y_valid, axis=1), yhat_valid, figsize=(7,7))\nprint(f'total wrong validation predictions: {np.sum(np.argmax(y_valid, axis=1) != yhat_valid)}\\n\\n')\nprint(classification_report(np.argmax(y_valid, axis=1), yhat_valid))","a63d46d8":"early_stopping = EarlyStopping(\n    monitor='accuracy',\n    mode='max',\n    min_delta=0.0001,\n    baseline=0.98,\n    patience=6,\n    restore_best_weights=True,\n    verbose=1\n)\n\nlr_scheduler = ReduceLROnPlateau(\n    monitor='accuracy',\n    patience=3,\n    factor=0.25,\n    min_lr=1e-5\n)\n\ncallbacks = [\n    early_stopping,\n    lr_scheduler,\n    TerminateOnNaN(),\n]\n\ntrain_datagen = ImageDataGenerator(\n    shear_range=0.1,\n    zoom_range=0.1,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n)\ntrain_datagen.fit(X_train)\n\ntrain_data_pp = data_preprocessing(train_data)\ntrain_label_pp = np_utils.to_categorical(train_label)\n\nmodel = build_cnn()\n\n# changing the steps as data is increased now.\nhistory = model.fit_generator(\n    train_datagen.flow(train_data_pp, train_label_pp, batch_size=32),\n    steps_per_epoch=len(train_data_pp) \/ 32,\n    epochs=25,\n    callbacks=callbacks,\n    use_multiprocessing=True\n)","d6b50812":"submission_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')\nX_test = data_preprocessing(test_data)\nyhat_test = model.predict_classes(X_test)\nsubmission_df['Label'] = yhat_test\nsubmission_df.to_csv('my_submission.csv', index=False)","6d877396":"`The below plots shows that our model generalizes really well while having slightly better performance as earlier.`","2e1fdc0e":"`Now we will use ImageDataGenerator to train on more images for better performance.`","a1b1e50c":"Digit-MNIST is the most fundamental and easiest dataset possible for deep learning purposes. And we have seen that without much efforts we achieved almost 99.5% accuracy on this dataset (which is not even the standard 60k MNIST). Also with this same model and configuration we may have achieved even higher accuracy with the 60k MNIST.\n\nAlthough we have achieved pretty good accuracy in such an easy dataset but for such we an easy dataset our model has parameters in the range of 100k-200k, which is way more for such an easy task. So can we do better i.e., reducing the model size significantly and simultaneously retaining the accuracy in the same range.\n\nRecently I was given a task by some organization to achieve an accuracy of atleast 99.4% on the MNIST. You might think what's good about that, even handicapped models can achieve an accuracy of around 98-99% in MNIST without doing anything. But the constraint was to achieve such an high accuracy using a model having atmost 8k parameters. Although I wasn't able to touch the 99.4% bar but I got an best accuracy of around 99.2% having less than 8k parameters in the given time limit. And I was sure if the model have given enough time then 99.4% accuracy is achievable afterall no one design a task which is un-achievable. The organization may already achieved that and that's why given us such a task.\n\nBut the idea here is that before jumping to deep networks we first try to achieve the goal in minimal model possible. So using such low number of parameters we achieved almost the same accuracy. The benefits are many like easy to inspect, debug, load, train etc..\n\nI thought it's worth sharing the notebook, it's very simple and I didn't do much. If you are interested [here](https:\/\/www.kaggle.com\/gauravsharma99\/mnist-8k-params\/) is the link to the notebook.","30b5ef34":"`Now we will train on the entire train data.`","4be94f4c":"`The below plots clearly shows that we overfit the train data as the validation accuracy is considerably low.`","ee0a8630":"So this is not the standard MNIST data with 60k train and 10k test images."}}